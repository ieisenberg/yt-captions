[
  {
    "start": "0",
    "end": "213000"
  },
  {
    "text": "- The following is a conversation with Dylan Patel and Nathan Lambert. Dylan runs SemiAnalysis,\na well-respected research",
    "start": "0",
    "end": "9172"
  },
  {
    "text": "and analysis company that\nspecializes in semiconductors, GPUs, CPUs, and AI hardware in general.",
    "start": "9172",
    "end": "16857"
  },
  {
    "text": "Nathan is a research scientist\nat the Allen Institute for AI and is the author",
    "start": "16857",
    "end": "22352"
  },
  {
    "text": "of the amazing blog on\nAI called Interconnects.",
    "start": "22352",
    "end": "26269"
  },
  {
    "text": "They are both highly respected, read, and listened to by the\nexperts, researchers, and engineers in the field of AI.",
    "start": "27548",
    "end": "34986"
  },
  {
    "text": "And personally, I'm just\na fan of the two of them. So, I used the DeepSeek moment",
    "start": "34986",
    "end": "41102"
  },
  {
    "text": "that shook the AI world a bit as an opportunity to sit down\nwith them and lay it all out.",
    "start": "41103",
    "end": "48151"
  },
  {
    "text": "From DeepSeek, OpenAI,\nGoogle xAI, Meta, Anthropic, to Nvidia and DSMC, and to\nUS, China, Taiwan relations,",
    "start": "48151",
    "end": "55907"
  },
  {
    "text": "and everything else that is happening at\nthe cutting edge of AI. This conversation is a deep dive",
    "start": "57523",
    "end": "63976"
  },
  {
    "text": "into many critical aspects\nof the AI industry. While it does get super\ntechnical, we try to make sure",
    "start": "63976",
    "end": "72414"
  },
  {
    "text": "that it's still accessible to\nfolks outside of the AI field by defining terms, stating\nimportant concepts explicitly,",
    "start": "72414",
    "end": "79400"
  },
  {
    "text": "spelling out acronyms, and in\ngeneral, always moving across the several layers of\nabstraction and levels of detail.",
    "start": "79401",
    "end": "86562"
  },
  {
    "text": "There is a lot of hype in the media about what AI is and isn't.",
    "start": "86562",
    "end": "92651"
  },
  {
    "text": "The purpose of this podcast in part is to cut through the hype, through the bullshit,",
    "start": "92651",
    "end": "98325"
  },
  {
    "text": "and the low resolution analysis, and to discuss in detail how stuff works",
    "start": "98325",
    "end": "104003"
  },
  {
    "text": "and what the implications are. Let me also, if I may comment on the new OpenAI o3-mini reasoning model.",
    "start": "104003",
    "end": "112094"
  },
  {
    "text": "The release of which we were anticipating during the conversation, and it did indeed come out right after.",
    "start": "112094",
    "end": "118534"
  },
  {
    "text": "Its capabilities and costs are on par with our\nexpectations as we stated.",
    "start": "118534",
    "end": "123823"
  },
  {
    "text": "OpenAI o3-mini is indeed a great model, but it should be stated that DeeSeek-R1",
    "start": "125030",
    "end": "131316"
  },
  {
    "text": "has similar performance on\nbenchmarks is still cheaper and it reveals its chain\nof thought reasoning,",
    "start": "131316",
    "end": "137637"
  },
  {
    "text": "which o3-mini does not. It only shows a summary of the reasoning.",
    "start": "137637",
    "end": "142784"
  },
  {
    "text": "Plus, R1 is open-weight\nand o3-mini is not.",
    "start": "142784",
    "end": "146366"
  },
  {
    "text": "By the way, I got a chance\nto play with o3-mini. And anecdotal vibe check\nwise, I felt that o3-mini,",
    "start": "149266",
    "end": "156526"
  },
  {
    "text": "specifically o3-mini-high\nis better than R1. Still for me personally,",
    "start": "157854",
    "end": "163363"
  },
  {
    "text": "I find that Claude Sonnet 3.5 is the best model for programming, except for tricky cases,",
    "start": "163363",
    "end": "168680"
  },
  {
    "text": "where I will use o1 Pro to brainstorm. Either way, many more\nbetter AI models will come,",
    "start": "168680",
    "end": "175400"
  },
  {
    "text": "including reasoning models both from American and Chinese companies.",
    "start": "175400",
    "end": "180694"
  },
  {
    "text": "They'll continue to shift the cost curve. But the quote, DeepSeek\nmoment is indeed real.",
    "start": "180695",
    "end": "187912"
  },
  {
    "text": "I think it will still be\nremembered five years from now as a pivotal event in tech history,",
    "start": "187912",
    "end": "193093"
  },
  {
    "text": "due in part to the\ngeopolitical implications, but for other reasons to,\nas we discuss in detail",
    "start": "193093",
    "end": "199678"
  },
  {
    "text": "from many perspectives\nin this conversation. This is the \"Lex Fridman Podcast\". To support it,",
    "start": "199678",
    "end": "205172"
  },
  {
    "text": "please check out our\nsponsors in the description. And now, dear friends, here's Dylan Patel and Nathan Lambert.",
    "start": "205173",
    "end": "212508"
  },
  {
    "start": "213000",
    "end": "1507000"
  },
  {
    "text": "A lot of people are curious to understand China's DeepSeek AI models, so let's lay it out.",
    "start": "213845",
    "end": "219071"
  },
  {
    "text": "Nathan, can you describe what DeepSeek-V3 and DeepSeek-R1 are, how they\nwork, how they're trained?",
    "start": "219071",
    "end": "225722"
  },
  {
    "text": "Let's look at the big picture, and then we'll zoom in on the details. - Yeah, so DeepSeek-V3 is\na new mixture of experts,",
    "start": "225722",
    "end": "233782"
  },
  {
    "text": "transformer language model from DeepSeek who is based in China. They have some new specifics",
    "start": "233783",
    "end": "241775"
  },
  {
    "text": "in the model that we'll get into. Largely this is a open-weight model and it's a instruction model",
    "start": "241775",
    "end": "248405"
  },
  {
    "text": "like what you would use in ChatGPT. They also release what\nis called the base model, which is before these\ntechniques of post-training.",
    "start": "248405",
    "end": "256393"
  },
  {
    "text": "Most people use instruction models today and those are what's served\nin all sorts of applications.",
    "start": "256393",
    "end": "261752"
  },
  {
    "text": "This was released on, I believe\nDecember 26th or that week. And then, weeks later,",
    "start": "261753",
    "end": "268747"
  },
  {
    "text": "on January 20th, DeepSeek\nreleased DeepSeek-R1, which is a reasoning model,",
    "start": "268747",
    "end": "274545"
  },
  {
    "text": "which really accelerated\na lot of this discussion. This reasoning model, it has a lot",
    "start": "274545",
    "end": "280339"
  },
  {
    "text": "of overlapping training\nsteps to DeepSeek-V3, and it's confusing that\nyou have a base model",
    "start": "280339",
    "end": "286106"
  },
  {
    "text": "called V3 that you do something\nto, to get a chat model, and then you do some different things",
    "start": "286106",
    "end": "291764"
  },
  {
    "text": "to get a reasoning model. I think a lot of the AI industry is going through this challenge\nof communications right now",
    "start": "291764",
    "end": "297576"
  },
  {
    "text": "where OpenAI makes fun of\ntheir own naming schemes. They have GPT-4o, they have OpenAI o1.",
    "start": "297576",
    "end": "303421"
  },
  {
    "text": "And there's a lot of types of models. So, we're gonna break down\nwhat each of them are. There's a lot of technical\nspecifics on training,",
    "start": "304897",
    "end": "311015"
  },
  {
    "text": "and go through 'em high level to specific, and go through each of them. - There's so many places we can go here,",
    "start": "311015",
    "end": "316806"
  },
  {
    "text": "but maybe let's go to open-weights first. What does it mean for\nmodel to be open-weights and what are the different flavors",
    "start": "316806",
    "end": "322451"
  },
  {
    "text": "of open source in general? - Yeah, so this discussion has been going on for a long time in AI. It became more important since ChatGPT",
    "start": "322451",
    "end": "329402"
  },
  {
    "text": "or more focal since\nChatGPT at the end of 2022. Open-weights is the accepted\nterm for when model weights",
    "start": "329402",
    "end": "336731"
  },
  {
    "text": "of a language model are available on the internet for people to download, those weights can have different licenses,",
    "start": "336731",
    "end": "342800"
  },
  {
    "text": "which is the effectively the terms by which you can use the model. There are licenses that come from history",
    "start": "342800",
    "end": "348762"
  },
  {
    "text": "and open source software. There are licenses that are designed by companies specifically,",
    "start": "348762",
    "end": "353816"
  },
  {
    "text": "all of Llama, DeepSeek, Qwen, Mistral, these popular names in open-weight models,",
    "start": "353816",
    "end": "361561"
  },
  {
    "text": "have some of their own licenses. It's complicated, 'cause not all the same\nmodels have the same terms.",
    "start": "361561",
    "end": "366947"
  },
  {
    "text": "The big debate is on what\nmakes a model open-weight.",
    "start": "366947",
    "end": "371280"
  },
  {
    "text": "It's like why are we saying this term? It's a mouthful. It sounds close to open\nsource but it's not the same.",
    "start": "372124",
    "end": "377734"
  },
  {
    "text": "There's still a lot of\ndebate on the definition and soul of open source AI. Open source software",
    "start": "377734",
    "end": "383376"
  },
  {
    "text": "has a rich history on freedom to modify, freedom to take on your own, freedom for many restrictions",
    "start": "383376",
    "end": "388814"
  },
  {
    "text": "on how you would use the software, and what that means for\nAI is still being defined.",
    "start": "388814",
    "end": "394117"
  },
  {
    "text": "So, for what I do, I work at\nthe Allen Institute for AI. We're a nonprofit.",
    "start": "394117",
    "end": "400502"
  },
  {
    "text": "We want to make AI open for everybody and we try to lead on what we\nthink is truly open source.",
    "start": "400502",
    "end": "405749"
  },
  {
    "text": "There's not full agreement\nin the community, but for us, that means\nreleasing the training data, releasing the training code,",
    "start": "405749",
    "end": "411293"
  },
  {
    "text": "and then also having\nopen-weights like this. And we'll get into the\ndetails of the models.",
    "start": "411293",
    "end": "416962"
  },
  {
    "text": "And again and again,\nas we try to get deeper into how the models were trained,",
    "start": "416962",
    "end": "422212"
  },
  {
    "text": "we will say things like\nthe data processing, data filtering, data quality is the number one determinant\nof the model quality.",
    "start": "422212",
    "end": "429836"
  },
  {
    "text": "And then, a lot of the training\ncode is the determinant on how long it takes to train and how fast your experimentation is.",
    "start": "429837",
    "end": "436257"
  },
  {
    "text": "So, without fully open source models where you have access to this data, it is hard to know or\nit's harder to replicate.",
    "start": "436257",
    "end": "444005"
  },
  {
    "text": "So, we'll get into cost\nnumbers for DeepSeek-V3 on mostly GPU hours",
    "start": "444006",
    "end": "448045"
  },
  {
    "text": "and how much you could pay\nto rent those yourselves. But without the data, the replication cost is\ngoing to be far, far higher.",
    "start": "449146",
    "end": "456195"
  },
  {
    "text": "And same goes for the code. - We should also say\nthat this is probably one of the more open models\nout of the frontier models.",
    "start": "456195",
    "end": "463710"
  },
  {
    "text": "- Yes.\n- So, in this full spectrum where probably the fullest\nopen source, like you said,",
    "start": "463710",
    "end": "468901"
  },
  {
    "text": "open code, open data, open-weights. This is not open code, this\nis probably not open data,",
    "start": "468901",
    "end": "478022"
  },
  {
    "text": "and this is open-weights, and the licensing is\nMIT license, or it's...",
    "start": "480037",
    "end": "485421"
  },
  {
    "text": "I mean, there's some nuance\nin the different models, but it's towards the free, in terms of the open source movement,",
    "start": "486694",
    "end": "491803"
  },
  {
    "text": "these are the good guys.\n- Yeah. DeepSeek is doing fantastic work for disseminating understanding of AI.",
    "start": "491803",
    "end": "498544"
  },
  {
    "text": "Their papers are extremely\ndetailed in what they do. And for other teams around the world,",
    "start": "498544",
    "end": "505191"
  },
  {
    "text": "they're very actionable in terms of improving your own training techniques.",
    "start": "505191",
    "end": "510491"
  },
  {
    "text": "And we'll talk about licenses more. The DeepSeek-R1 model has\na very permissive license.",
    "start": "510491",
    "end": "517164"
  },
  {
    "text": "It's called the MIT license. That effectively means there's\nno downstream restrictions on commercial use, there's\nno use case restrictions.",
    "start": "517164",
    "end": "524152"
  },
  {
    "text": "You can use the outputs from the models to create synthetic data. And this is all fantastic.",
    "start": "524153",
    "end": "529922"
  },
  {
    "text": "I think the closest peer\nis something like Llama where you have the weights and\nyou have a technical report.",
    "start": "529922",
    "end": "535142"
  },
  {
    "text": "And the technical report\nis very good for Llama. One of the most read PDFs\nof the year last year",
    "start": "535142",
    "end": "540799"
  },
  {
    "text": "is the Llama 3 paper, but in some ways, it's\nslightly less actionable. It has less details on\nthe training specifics,",
    "start": "540799",
    "end": "546891"
  },
  {
    "text": "I think less plots, and so on. And the Llama 3 license is\nmore restrictive than MIT.",
    "start": "546891",
    "end": "553132"
  },
  {
    "text": "And then, between the\nDeepSeek custom license and the Llama license, we could get into this whole rabbit hole. I think we'll make sure we want",
    "start": "553132",
    "end": "559001"
  },
  {
    "text": "to go down the license rabbit\nhole before we do specifics. - Yeah, and so it should\nbe stated that one",
    "start": "559001",
    "end": "564027"
  },
  {
    "text": "of the implications that\nDeepSeek puts pressure on Llama and everybody else on OpenAI\nto push towards open source,",
    "start": "564027",
    "end": "571917"
  },
  {
    "text": "and that's the other side of open source that you mentioned is how much is published\nin detail about it.",
    "start": "571917",
    "end": "577658"
  },
  {
    "text": "So, how open are you with\nthe insights behind the code?",
    "start": "577658",
    "end": "582241"
  },
  {
    "text": "So, how good is the technical reports? Are they hand wavy or is\nthere actual details in there?",
    "start": "583213",
    "end": "588983"
  },
  {
    "text": "And that's one of the things that DeepSeek did well is they\npublish a lot of the details. - Yeah, especially in the DeepSeek-V3,",
    "start": "588983",
    "end": "594535"
  },
  {
    "text": "which is their pre-training\npaper, they were very clear that they are doing interventions",
    "start": "594535",
    "end": "599647"
  },
  {
    "text": "on the technical stack that\ngo at many different levels. For example, to get\nhighly efficient training,",
    "start": "599647",
    "end": "606315"
  },
  {
    "text": "they're making modifications at or below the CUDA layer for Nvidia chips.",
    "start": "606315",
    "end": "612011"
  },
  {
    "text": "I have never worked there myself. And there are a few people in the world that do that very well and\nsome of them are at DeepSeek.",
    "start": "612012",
    "end": "617823"
  },
  {
    "text": "And these types of people are at DeepSeek and leading American Frontier Labs,",
    "start": "617823",
    "end": "623265"
  },
  {
    "text": "but there are not many places. - To help people understand the other implication of open-weights, just there's a topic we\nreturn to often here.",
    "start": "623265",
    "end": "632483"
  },
  {
    "text": "So, there's a fear that China, the nation,",
    "start": "632483",
    "end": "635983"
  },
  {
    "text": "might have interest in\nstealing American data, violating privacy of American citizens.",
    "start": "639261",
    "end": "645817"
  },
  {
    "text": "What can we say about open-weights to help us understand what\nthe weights are able to do",
    "start": "645817",
    "end": "652494"
  },
  {
    "text": "- Yeah.\n- in terms of stealing people's data? - Yeah, so these weights that you can download from Hugging Face or other platforms are very\nbig matrices of numbers.",
    "start": "652494",
    "end": "661167"
  },
  {
    "text": "You can download them to a computer in your own house that has no internet and you can run this model, and you're totally in\ncontrol of your data.",
    "start": "661167",
    "end": "669447"
  },
  {
    "text": "That is something that is different than how a lot of language model usage is actually done today,\nwhich is mostly through APIs",
    "start": "669447",
    "end": "675465"
  },
  {
    "text": "where you send your prompt to\nGPUs run by certain companies, and these companies will\nhave different distributions",
    "start": "675466",
    "end": "681364"
  },
  {
    "text": "and policies on how your data is stored, if it is used to train future models, where it is stored, if it\nis encrypted, and so on.",
    "start": "681364",
    "end": "688443"
  },
  {
    "text": "So, the open-weights\nare you have your fate of data in your own hands and that is something\nthat is deeply connected",
    "start": "688443",
    "end": "695085"
  },
  {
    "text": "to the soul of open source. - So, it's not the model\nthat steals your data, it's whoever's hosting the model,",
    "start": "695085",
    "end": "700895"
  },
  {
    "text": "which could be China if\nyou're using the DeepSeek app, or it could be Perplexity.",
    "start": "700895",
    "end": "707302"
  },
  {
    "text": "You're trusting them with your data. Or OpenAI, you're trusting\nthem with your data. And some of these are American companies,",
    "start": "707302",
    "end": "712850"
  },
  {
    "text": "some of of these are Chinese companies, but the model itself is\nnot doing the stealing, it's the host.",
    "start": "712850",
    "end": "719090"
  },
  {
    "text": "All right. So, back to the basics. What's the difference between\nDeepSeek-V3 and DeepSeek-R1?",
    "start": "719090",
    "end": "727115"
  },
  {
    "text": "Can we try to lay out the confusion potential?\n- Yes. So, for one, I have very understanding",
    "start": "727116",
    "end": "733995"
  },
  {
    "text": "of many people being confused\nby these two model names. So, I would say the best\nway to think about this is that when training a language model,",
    "start": "733995",
    "end": "741084"
  },
  {
    "text": "you have what is called pre-training, which is when you're\npredicting the large amounts of mostly internet text.",
    "start": "741084",
    "end": "746995"
  },
  {
    "text": "You're trying to predict the next-token. And what to know about\nthese new DeepSeek models is that they do this internet\nlarge-scale pre-training once",
    "start": "746995",
    "end": "756033"
  },
  {
    "text": "to get what is called DeepSeek-V3 base. This is a base model. It's just going to finish\nyour sentences for you.",
    "start": "756033",
    "end": "762636"
  },
  {
    "text": "It's going to be harder\nto work with than ChatGPT. And then, what DeepSeek did is they've done two different\npost-training regimes",
    "start": "762637",
    "end": "770273"
  },
  {
    "text": "to make the models have\nspecific desirable behaviors. So, what is the more normal model",
    "start": "770273",
    "end": "777337"
  },
  {
    "text": "in terms of the last few years of AI and instruct model, a chat model, a, quote, unquote, \"aligned\nmodel\", a helpful model,",
    "start": "777337",
    "end": "783968"
  },
  {
    "text": "there are many ways to describe this, is more standard post-training. So, this is things like\ninstruction tuning,",
    "start": "783968",
    "end": "790137"
  },
  {
    "text": "reinforcement learning\nfrom human feedback. We'll get into some of these words. And this is what they did to\ncreate the DeepSeek-V3 model.",
    "start": "790137",
    "end": "797457"
  },
  {
    "text": "This was the first model to be released and it is very highly performant,",
    "start": "797457",
    "end": "803102"
  },
  {
    "text": "competitive with GPT-4, Llama 405b, so on. And then, when this release was happening,",
    "start": "803102",
    "end": "811075"
  },
  {
    "text": "we don't know their exact timeline, or soon after they were\nfinishing the training of a different training process",
    "start": "811075",
    "end": "816537"
  },
  {
    "text": "from the same next-token\nprediction-based model that I talked about, which is when this new reasoning training",
    "start": "816537",
    "end": "823116"
  },
  {
    "text": "that people have heard about comes in, in order to create the model\nthat is called DeepSeek-R1.",
    "start": "823116",
    "end": "828788"
  },
  {
    "text": "The R through this conversation is good for grounding for reasoning. And the name is also\nsimilar to OpenAI's o1,",
    "start": "828788",
    "end": "834199"
  },
  {
    "text": "which is the other reasoning model that people have heard about. And we'll have to break down the training",
    "start": "834199",
    "end": "839409"
  },
  {
    "text": "for R1 in more detail, because for one, we have\na paper detailing it, but also, it is a far\nnewer set of techniques",
    "start": "839409",
    "end": "846515"
  },
  {
    "text": "for the AI community. So, it is a much more rapidly\nevolving area of research.",
    "start": "846515",
    "end": "852052"
  },
  {
    "text": "- Maybe we should also\nsay the big two categories of training of pre-training\nand post-training.",
    "start": "852052",
    "end": "858777"
  },
  {
    "text": "These umbrella terms that people use. So, what is pre-training\nand what is post-training,",
    "start": "858777",
    "end": "864768"
  },
  {
    "text": "and what are the different\nflavors of things underneath post-training umbrella? - Yeah, so pre-training, I'm\nusing some of the same words",
    "start": "864768",
    "end": "870198"
  },
  {
    "text": "to really get the message across, is you're doing what is called\nautoregressive prediction to predict the next-token\nin a series of documents.",
    "start": "870198",
    "end": "877188"
  },
  {
    "text": "This is done over standard\npractice is trillions of tokens. So, this is a ton of data",
    "start": "877188",
    "end": "883848"
  },
  {
    "text": "that is mostly scraped from the web. And some of DeepSeek's earlier papers,",
    "start": "883848",
    "end": "889239"
  },
  {
    "text": "they talk about their training data being distilled for math, I\nshouldn't use this word yet,",
    "start": "889239",
    "end": "894382"
  },
  {
    "text": "but taken from Common Crawl, and that's a public access\nthat anyone listening to this",
    "start": "894382",
    "end": "899759"
  },
  {
    "text": "could go download data from\nthe Common Crawl website. This is a crawler that\nis maintained publicly. Yes, other tech companies",
    "start": "899759",
    "end": "905483"
  },
  {
    "text": "eventually shift to their own crawler and DeepSeek likely has done this as well, as most Frontier Labs do.",
    "start": "905483",
    "end": "911442"
  },
  {
    "text": "But this sort of data is something that people can get started with and you're just predicting\ntext in a series of documents.",
    "start": "911442",
    "end": "918004"
  },
  {
    "text": "This can be scaled to be very efficient",
    "start": "918004",
    "end": "921254"
  },
  {
    "text": "and there's a lot of numbers that are thrown around in AI training, like how many floating point\noperations or FLOPS are used.",
    "start": "923342",
    "end": "929787"
  },
  {
    "text": "And then, you can also look at how many hours of\nthese GPUs that are used.",
    "start": "929788",
    "end": "935187"
  },
  {
    "text": "And it's largely one loss function taken to a very large amount\n(chuckles) of compute usage.",
    "start": "935187",
    "end": "942477"
  },
  {
    "text": "You set up really efficient systems. And then, at the end of that,\nyou have this base model. And pre-training is\nwhere there is a lot more",
    "start": "942813",
    "end": "950425"
  },
  {
    "text": "of complexity in terms of how the process is\nemerging or evolving,",
    "start": "950425",
    "end": "957226"
  },
  {
    "text": "and the different types of\ntraining losses that you'll use. I think this is a lot of techniques",
    "start": "957226",
    "end": "962826"
  },
  {
    "text": "grounded in the natural\nlanguage processing literature. The oldest technique\nwhich is still used today",
    "start": "962827",
    "end": "968078"
  },
  {
    "text": "is something called instruction tuning, or also known as supervised fine-tuning. These acronyms will be IFT or SFT",
    "start": "968078",
    "end": "974833"
  },
  {
    "text": "that people really go back\nand forth throughout them, and I'll probably do the same, which is where you add this\nformatting to the model,",
    "start": "974833",
    "end": "983104"
  },
  {
    "text": "where it knows to take a question that is like explain the history\nof the Roman Empire to me.",
    "start": "983104",
    "end": "990505"
  },
  {
    "text": "Or something, a sort of question you'll see on Reddit or Stack Overflow, and then the model will respond in a information-dense\nbut presentable manner.",
    "start": "990813",
    "end": "998955"
  },
  {
    "text": "The core of that formatting is in this instruction tuning phase. And then, there's two other categories",
    "start": "998955",
    "end": "1004063"
  },
  {
    "text": "of loss functions that\nare being used today. One I'll classify as\npreference fine-tuning.",
    "start": "1004063",
    "end": "1009503"
  },
  {
    "text": "Preference fine-tuning\nis a generalized term for what came out of\nreinforcement learning from human feedback, which is RLHF.",
    "start": "1009503",
    "end": "1017597"
  },
  {
    "text": "This reinforcement learning\nfrom human feedback is credited as the technique that helped ChatGPT breakthrough.",
    "start": "1017598",
    "end": "1025026"
  },
  {
    "text": "It is a technique to make the responses that are nicely formatted,\nlike these Reddit answers, more in tune with what a\nhuman would like to read.",
    "start": "1026206",
    "end": "1034406"
  },
  {
    "text": "This is done by collecting\npairwise preferences from actual humans out\nin the world to start.",
    "start": "1034406",
    "end": "1039814"
  },
  {
    "text": "And now, AIs are also labeling this data and we'll get into those trade-offs. And you have this kind of\ncontrastive loss function",
    "start": "1039815",
    "end": "1046923"
  },
  {
    "text": "between a good answer and a bad answer. And the model learns to\npick up these trends. There's different implementation ways.",
    "start": "1046924",
    "end": "1053176"
  },
  {
    "text": "You have things called reward models. You could have direct\nalignment algorithms. There's a lot of really\nspecific things you can do.",
    "start": "1053176",
    "end": "1059521"
  },
  {
    "text": "But all of this is about\nfine-tuning to human preferences. And the final stage is much newer",
    "start": "1059521",
    "end": "1064874"
  },
  {
    "text": "and will link to what is done in R1. And these reasoning models is I think OpenAI's name for this.",
    "start": "1064874",
    "end": "1072240"
  },
  {
    "text": "They had this new API in the fall, which they called the\nreinforcement fine-tuning API.",
    "start": "1072240",
    "end": "1077565"
  },
  {
    "text": "This is the idea that\nyou use the techniques of reinforcement learning, which\nis a whole framework of AI.",
    "start": "1077566",
    "end": "1082966"
  },
  {
    "text": "There's a deep literature here. To summarize, it's often known\nas trial and error learning or the subfield of AI where you're trying",
    "start": "1082966",
    "end": "1090313"
  },
  {
    "text": "to make sequential decisions in a certain potentially\nnoisy environment.",
    "start": "1090313",
    "end": "1096148"
  },
  {
    "text": "There's a lot of ways\nwe could go down that. But fine-tuning language models where they can generate an answer.",
    "start": "1096148",
    "end": "1102098"
  },
  {
    "text": "And then, you check to see if the answer matches the true solution. For math or code, you have an exactly\ncorrect answer for math.",
    "start": "1102098",
    "end": "1109772"
  },
  {
    "text": "You can have unit tests for code. And what we're doing is we are checking the\nlanguage model's work and we're giving it multiple opportunities",
    "start": "1109772",
    "end": "1116174"
  },
  {
    "text": "on the same questions\nto see if it is right. And if you keep doing\nthis, the models can learn to improve in verifiable\ndomains to a great extent.",
    "start": "1116175",
    "end": "1124412"
  },
  {
    "text": "It works really well. It's a newer technique in\nthe academic literature. It's been used at Frontier Labs in the US",
    "start": "1124412",
    "end": "1130287"
  },
  {
    "text": "that don't share every\ndetail for multiple years. So, this is the idea of\nusing reinforcement learning",
    "start": "1130287",
    "end": "1136724"
  },
  {
    "text": "with language models, and\nit has been taking off, especially in this DeepSeek moment. - And we should say that there's a lot",
    "start": "1136724",
    "end": "1141972"
  },
  {
    "text": "of exciting stuff going on\nthe, again, across the stack. But the post-training probably this year,",
    "start": "1141972",
    "end": "1147914"
  },
  {
    "text": "there's going to be a lot\nof interesting developments in the post-training. We'll talk about it. I almost forgot to talk\nabout the difference",
    "start": "1147914",
    "end": "1154366"
  },
  {
    "text": "between DeepSeek-V3 and R1\non the user experience side. So, forget the technical\nstuff, forget all of that.",
    "start": "1154366",
    "end": "1161329"
  },
  {
    "text": "Just people that don't\nknow anything about AI, they show up like what's\nthe actual experience, what's the use case for each one",
    "start": "1161329",
    "end": "1167690"
  },
  {
    "text": "when they actually type and talk to it? - Yeah.\n- What is each good at and that kind of thing. - So, let's start with DeepSeek-V3.",
    "start": "1167691",
    "end": "1173490"
  },
  {
    "text": "Again, it's more people would\nhave tried something like it. You ask it a question, it'll start generating tokens very fast,",
    "start": "1173490",
    "end": "1180125"
  },
  {
    "text": "and those tokens will look like\na very human legible answer. It'll be some sort of markdown list.",
    "start": "1180125",
    "end": "1186796"
  },
  {
    "text": "It might have formatting to help you draw to the\ncore details in the answer.",
    "start": "1186796",
    "end": "1192157"
  },
  {
    "text": "And it'll generate tens\nto hundreds of tokens. Say token is normally\na word for common words",
    "start": "1192157",
    "end": "1198774"
  },
  {
    "text": "or a sub-word part in a longer word. And it'll look like a\nvery high quality Reddit",
    "start": "1198774",
    "end": "1205908"
  },
  {
    "text": "or Stack Overflow answer. These models are really getting good at doing these across a wide\nvariety of domains, I think.",
    "start": "1205908",
    "end": "1212370"
  },
  {
    "text": "Even things that if you're an expert, things that are close to\nthe fringe of knowledge, they will still be\nfairly good at, I think.",
    "start": "1212370",
    "end": "1219575"
  },
  {
    "text": "Cutting edge AI topics\nthat I do research on, these models are capable for study aide,",
    "start": "1219576",
    "end": "1225935"
  },
  {
    "text": "and they're regularly updated. Where this changes is\nwith the DeepSeek-R1,",
    "start": "1225936",
    "end": "1231278"
  },
  {
    "text": "what is called these reasoning models, is when you see tokens coming\nfrom these models to start,",
    "start": "1231278",
    "end": "1236492"
  },
  {
    "text": "it will be a large chain\nof thought process. We'll get back to chain\nof thought in a second,",
    "start": "1236492",
    "end": "1242393"
  },
  {
    "text": "which looks like a lot of tokens, where the model is explaining the problem. The model will often\nbreak down the problem",
    "start": "1242393",
    "end": "1248585"
  },
  {
    "text": "and be like, okay, they asked me for this, let's break down the problem. I'm going to need to do this. And you'll see all of this\ngenerating from the model.",
    "start": "1248585",
    "end": "1255787"
  },
  {
    "text": "It'll come very fast in\nmost user experiences. These APIs are very fast. So, you'll see a lot of tokens, a lot of words show up really fast.",
    "start": "1255787",
    "end": "1261973"
  },
  {
    "text": "It'll keep flowing on the screen and this is all the reasoning process. And then, eventually, the model\nwill change its tone in R1",
    "start": "1261973",
    "end": "1269237"
  },
  {
    "text": "and it'll write the answer, where it summarizes its reasoning process and writes a similar answer\nto the first types of model.",
    "start": "1269237",
    "end": "1276202"
  },
  {
    "text": "But in DeepSeek's case, which is part of why this was so popular\neven outside the AI community,",
    "start": "1276202",
    "end": "1282786"
  },
  {
    "text": "is that you can see how the language model is breaking down problems. And then, you get this answer.",
    "start": "1282786",
    "end": "1288143"
  },
  {
    "text": "On a technical side, they train the model to\ndo this specifically, where they have a section\nwhich is reasoning,",
    "start": "1288144",
    "end": "1293467"
  },
  {
    "text": "and then it generates a special token, which is probably hidden from\nthe user most of the time, which says, okay, I'm starting the answer.",
    "start": "1293467",
    "end": "1298697"
  },
  {
    "text": "So, the model is trained to do this two-stage process on its own. If you use a similar model in say, OpenAI,",
    "start": "1298697",
    "end": "1305044"
  },
  {
    "text": "OpenAI's user interface is trying to summarize\nthis process for you nicely",
    "start": "1305045",
    "end": "1310818"
  },
  {
    "text": "by showing the sections\nthat the model is doing, and it'll click through,",
    "start": "1310818",
    "end": "1315923"
  },
  {
    "text": "it'll say breaking down the\nproblem, making X calculation, cleaning the result,",
    "start": "1315923",
    "end": "1321029"
  },
  {
    "text": "and then the answer will come\nfor something like OpenAI. - Maybe it's useful here to go through like an example of it,\nDeepSeek-R1 reasoning.",
    "start": "1321029",
    "end": "1328674"
  },
  {
    "text": "- Yeah. So, if you're looking at the screen here, what you'll see is a screenshot\nof the DeepSeek chat app.",
    "start": "1328674",
    "end": "1335475"
  },
  {
    "text": "And at the top is Thought for 157 seconds with the dropdown arrow.",
    "start": "1335475",
    "end": "1340745"
  },
  {
    "text": "Underneath that, if we were in\nan app that we were running, the dropdown arrow would\nhave the reasoning. - So, in this case, the question,",
    "start": "1340745",
    "end": "1347186"
  },
  {
    "text": "the specific question which, I'm philosophically/pothead-inclined, so this is asking DeepSeek-R1",
    "start": "1347186",
    "end": "1353962"
  },
  {
    "text": "for one truly novel insight about humans. And it reveals the reasoning.",
    "start": "1355196",
    "end": "1361334"
  },
  {
    "text": "And basically, the truly novel aspect is what's pushing the\nreasoning to constantly,",
    "start": "1361334",
    "end": "1367415"
  },
  {
    "text": "the model asking itself,\nis this truly novel? So, it's actually challenging\nitself to be more novel, more counterintuitive,\nless cringe, I suppose.",
    "start": "1367416",
    "end": "1376157"
  },
  {
    "text": "So, some of the reasoning\nsays, this is just snapshots. Alternatively, humans\nhave a unique meta-emotion",
    "start": "1377245",
    "end": "1384205"
  },
  {
    "text": "where they feel emotions\nabout their own emotions, e.g, feeling guilty about being angry. This recursive emotional\nlayering creates complex",
    "start": "1384205",
    "end": "1390335"
  },
  {
    "text": "motivational drives that\ndon't exist in other animals. The inside is that human\nemotions are nested. So, it's like,",
    "start": "1390335",
    "end": "1396117"
  },
  {
    "text": "it's reasoning through\nhow humans feel emotions, it's reasoning about meta-emotions.",
    "start": "1396117",
    "end": "1401173"
  },
  {
    "text": "- It's gonna have pages and pages of this.\n- Yeah. - It's almost too much to actually read, but it's nice to skim as it's coming.",
    "start": "1401173",
    "end": "1406621"
  },
  {
    "text": "- It's a stream of con... It's a James Joyce-like\nstream of consciousness. And then, it goes, wait, the user wants something\nthat's not seen anywhere else.",
    "start": "1406621",
    "end": "1413990"
  },
  {
    "text": "Let me dig deeper. And consider the human ability to hold contradictory\nbeliefs simultaneously.",
    "start": "1413990",
    "end": "1420192"
  },
  {
    "text": "Cognitive dissonance is known, but perhaps the function is to allow flexible\nadaptation, so on and so forth.",
    "start": "1420192",
    "end": "1427662"
  },
  {
    "text": "That really captures\nthe public imagination, that holy shit, this isn't...",
    "start": "1427662",
    "end": "1432830"
  },
  {
    "text": "I mean, intelligence/almost\nlike an inkling of sentience,",
    "start": "1434619",
    "end": "1439619"
  },
  {
    "text": "because you're thinking through, you're self-reflecting,\nyou're deliberating. And the final result of\nthat after 157 seconds",
    "start": "1440513",
    "end": "1447627"
  },
  {
    "text": "is humans instinctively\nconvert selfish desires into cooperative systems",
    "start": "1447627",
    "end": "1453826"
  },
  {
    "text": "by collectively pretending abstract rules, money, laws, rights, are real. These shared hallucinations\nact as, quote, games,",
    "start": "1453826",
    "end": "1461414"
  },
  {
    "text": "where competition is secretly redirected to benefit the group, turning\nconflict into society's fuel.",
    "start": "1461414",
    "end": "1468762"
  },
  {
    "text": "Pretty profound. I mean, you know? Comment-\n- This is a potential digression, but a lot of people have found",
    "start": "1468762",
    "end": "1476009"
  },
  {
    "text": "that these reasoning models can sometimes produce\nmuch more eloquent text. That is at least interesting\nexample, I think,",
    "start": "1476009",
    "end": "1483492"
  },
  {
    "text": "depending on how open-minded you are, you find language models\ninteresting or not, and there's a spectrum there. - Well, it's some of the...",
    "start": "1483492",
    "end": "1489772"
  },
  {
    "text": "We'll talk about different\nbenchmarks, and so on. But some is just a vibe. Like that in itself is a,\nlet's say, quote, fire tweet.",
    "start": "1489772",
    "end": "1497487"
  },
  {
    "text": "- Yeah. (laughs)\n- If I'm trying to produce something, where\npeople are like, oh shit,",
    "start": "1497488",
    "end": "1503005"
  },
  {
    "text": "okay, so that's chain of thought. We'll probably return to it more.",
    "start": "1503005",
    "end": "1508079"
  },
  {
    "start": "1507000",
    "end": "3085000"
  },
  {
    "text": "How were they able to\nachieve such low cost on the training and the inference?",
    "start": "1508079",
    "end": "1513509"
  },
  {
    "text": "Maybe you could talk the training first. - Yeah, so there's two main techniques that they implemented",
    "start": "1513509",
    "end": "1518914"
  },
  {
    "text": "that are probably the\nmajority of their efficiency. And then, there's a lot\nof implementation details",
    "start": "1518914",
    "end": "1524876"
  },
  {
    "text": "that maybe we'll gloss over or get into later that contribute to it. But those two main things are,",
    "start": "1524876",
    "end": "1531136"
  },
  {
    "text": "one, is they went to a\nmixture of experts model, which we'll define in a second. And then, the other thing",
    "start": "1531136",
    "end": "1536773"
  },
  {
    "text": "is that they invented this new technique called MLA latent attention. Both of these are big deals.",
    "start": "1536773",
    "end": "1542047"
  },
  {
    "text": "Mixture of experts is something that's been in the literature\nfor a handful of years. And OpenAI with GPT-4 was the first one",
    "start": "1542047",
    "end": "1548438"
  },
  {
    "text": "to productize a mixture of experts model. And what this means is when you look at the\ncommon models around",
    "start": "1548438",
    "end": "1555384"
  },
  {
    "text": "that most people have been able to interact with that are open. Think Llama, Llama is a dense model.",
    "start": "1555384",
    "end": "1562759"
  },
  {
    "text": "i.e, every single parameter or neuron is activated as\nyou're going through the model",
    "start": "1562759",
    "end": "1567980"
  },
  {
    "text": "for every single token you generate. Now, with a mixture of experts\nmodel, you don't do that.",
    "start": "1567980",
    "end": "1573670"
  },
  {
    "text": "How does the human actually work is like, oh, well, my visual cortex is active when I'm\nthinking about vision tasks",
    "start": "1573670",
    "end": "1580237"
  },
  {
    "text": "and other things. My amygdala is when I'm scared. These different aspects of your brain",
    "start": "1580238",
    "end": "1585520"
  },
  {
    "text": "are focused on different things. A mixture of experts models attempts to approximate\nthis to some extent. So, nowhere close to what\na brain architecture is,",
    "start": "1585520",
    "end": "1592309"
  },
  {
    "text": "but different portions\nof the model activate. You'll have a set number\nof experts in the model",
    "start": "1592309",
    "end": "1598758"
  },
  {
    "text": "and a set number that\nare activated each time. And this dramatically\nreduces both your training and inference cost.",
    "start": "1598758",
    "end": "1604001"
  },
  {
    "text": "Because now, if you think\nabout the parameter count as the total embedding space\nfor all of this knowledge",
    "start": "1604001",
    "end": "1611059"
  },
  {
    "text": "that you're compressing\ndown during training, one, you're embedding this data in,",
    "start": "1611059",
    "end": "1616359"
  },
  {
    "text": "instead of having to activate\nevery single parameter, every single time you're\ntraining or running inference. Now, you can just activate on a subset.",
    "start": "1616359",
    "end": "1623233"
  },
  {
    "text": "And the model will learn which expert to route\nto for different tasks. And so, this is a humongous\ninnovation in terms of, hey,",
    "start": "1623233",
    "end": "1631168"
  },
  {
    "text": "I can continue to grow the total embedding space of parameters. And so, DeepSeek's model is 600 something billion parameters.",
    "start": "1631168",
    "end": "1637966"
  },
  {
    "text": "Relative to Llama 405b,\nit's 405 billion parameters. Llama relative to Llama 70b,\nit's 70 billion parameters.",
    "start": "1637967",
    "end": "1644194"
  },
  {
    "text": "So, this model technically has more embedding space for information, to compress all of the world's knowledge",
    "start": "1644194",
    "end": "1649912"
  },
  {
    "text": "that's on the internet down. But at the same time, it is only activating around\n37 billion of the parameters.",
    "start": "1649912",
    "end": "1656693"
  },
  {
    "text": "So, only 37 billion of these parameters actually need to be computed every single time you're training data",
    "start": "1656693",
    "end": "1662193"
  },
  {
    "text": "or inferencing data out of it. And so, versus again a Llama model, 70 billion parameters must be activated",
    "start": "1662193",
    "end": "1668253"
  },
  {
    "text": "or 405 billion parameters\nmust be activated. So, you've dramatically\nreduced your compute cost when you're doing training and inference",
    "start": "1668253",
    "end": "1674373"
  },
  {
    "text": "with this mixture of experts architecture. - So, we break down\nwhere it actually applies and go into the transformer.",
    "start": "1674374",
    "end": "1680987"
  },
  {
    "text": "Is that useful? - Let's go, let's go into the transformer.\n- Okay. So, the transformer\n(Lex laughing) is a thing that is talked about a lot",
    "start": "1680987",
    "end": "1686408"
  },
  {
    "text": "and we will not cover every detail. Essentially, the transformer is built",
    "start": "1686408",
    "end": "1691589"
  },
  {
    "text": "on repeated blocks of\nthis attention mechanism, and then a traditional dense, fully connected multi-layer perception,",
    "start": "1691589",
    "end": "1699077"
  },
  {
    "text": "whatever word you want to use for your normal neural network. And you alternate these blocks. There's other details.",
    "start": "1699077",
    "end": "1704293"
  },
  {
    "text": "And where mixture of experts is applied is that this dense model. The dense model holds most of the weights",
    "start": "1704293",
    "end": "1711000"
  },
  {
    "text": "if you count them in a transformer model. So, you can get really big gains from those mixture of experts\non parameter efficiency",
    "start": "1711000",
    "end": "1718548"
  },
  {
    "text": "at training and inference, because you get this efficiency by not activating all of these parameters.",
    "start": "1718548",
    "end": "1724159"
  },
  {
    "text": "- [Lex] We should also\nsay that a transformer is a giant neural network. - Yeah.\n- And then,",
    "start": "1724160",
    "end": "1729469"
  },
  {
    "text": "there's for 15 years now, there's what's called the\ndeep learning revolution. Networks gotten larger and larger.",
    "start": "1729469",
    "end": "1736446"
  },
  {
    "text": "And a certain point, the\nscaling laws appeared where people realized... - This is a scaling law shirt, by the way.",
    "start": "1736446",
    "end": "1742057"
  },
  {
    "text": "(group laughing) - Representing. Scaling laws where it became more and more formalized that bigger is better",
    "start": "1742057",
    "end": "1749130"
  },
  {
    "text": "across multiple dimensions\nof what bigger means. But these are all neural\nnetworks we're talking about.",
    "start": "1749130",
    "end": "1756060"
  },
  {
    "text": "And we're talking about\ndifferent architectures of how to construct these neural networks such that the training",
    "start": "1756060",
    "end": "1762468"
  },
  {
    "text": "and the inference on\nthem is super efficient. - Yeah, every different type of model has a different scaling law for it,",
    "start": "1762468",
    "end": "1767529"
  },
  {
    "text": "which is effectively for\nhow much compute you put in, the architecture will get",
    "start": "1767529",
    "end": "1773529"
  },
  {
    "text": "to different levels of\nperformance at test tasks. A mixture of experts is one\nof the ones at training time.",
    "start": "1773529",
    "end": "1779311"
  },
  {
    "text": "Even if you don't consider\nthe inference benefits, which are also big. At training time, your\nefficiency with your GPUs",
    "start": "1779311",
    "end": "1784782"
  },
  {
    "text": "is dramatically improved by using this architecture\nif it is well-implemented. So, you can get effectively\nthe same performance model",
    "start": "1784782",
    "end": "1792723"
  },
  {
    "text": "and evaluation scores with\nnumbers like 30% less compute. I think there's gonna be a wide variation,",
    "start": "1792723",
    "end": "1799152"
  },
  {
    "text": "depending on your implementation\ndetails and stuff. But it is just important\nto realize that this type",
    "start": "1799152",
    "end": "1804461"
  },
  {
    "text": "of technical innovation is\nsomething that gives huge gains. And I expect most companies\nthat are serving their models",
    "start": "1804461",
    "end": "1811184"
  },
  {
    "text": "to move to this mixture\nof experts implementation. Historically, the reason\nwhy not everyone might do it",
    "start": "1811184",
    "end": "1816824"
  },
  {
    "text": "is because it's a\nimplementation complexity, especially when doing these big models. So, this is one of the things\nthat DeepSeek gets credit for",
    "start": "1816824",
    "end": "1824172"
  },
  {
    "text": "is they do this extremely well. They do a mixture of\nexperts extremely well. This architecture for what\nis called DeepSeekMoE.",
    "start": "1824172",
    "end": "1831648"
  },
  {
    "text": "MoE is the shortened version of mixture of experts,\nis multiple papers old. This part of their training infrastructure",
    "start": "1831649",
    "end": "1838029"
  },
  {
    "text": "is not new to these models alone, and same goes for what Dylan mentioned,",
    "start": "1838029",
    "end": "1843056"
  },
  {
    "text": "with multi-head latent attention. This is all about reducing\nmemory usage during inference.",
    "start": "1843057",
    "end": "1848346"
  },
  {
    "text": "And same things during training, by using some fancy low\nrank approximation math.",
    "start": "1848346",
    "end": "1853809"
  },
  {
    "text": "If you get into the details\nwith this latent attention, it's one of those things I look at, and it's like, okay,",
    "start": "1853809",
    "end": "1858932"
  },
  {
    "text": "they're doing really\ncomplex implementations, 'cause there's other\nparts of language models such as embeddings",
    "start": "1858932",
    "end": "1865210"
  },
  {
    "text": "that are used to extend\nthe context length. The common one that DeepSeek used is rotary positional impendings,\nwhich is called RoPE.",
    "start": "1865210",
    "end": "1872783"
  },
  {
    "text": "And if you want to use\nRoPE with a normal MoE, it's a sequential thing.",
    "start": "1872783",
    "end": "1877850"
  },
  {
    "text": "You take two of the attention matrices and you rotate them by a\ncomplex value rotation, which is a matrix multiplication.",
    "start": "1877850",
    "end": "1884547"
  },
  {
    "text": "With DeepSeek's MLA, with this\nnew attention architecture, they need to do some clever things, because they're not set up the same",
    "start": "1884547",
    "end": "1891187"
  },
  {
    "text": "and it just makes the implementation\ncomplexity much higher. So, they're managing all of these things, and these are probably the\nsort of things that OpenAI,",
    "start": "1891187",
    "end": "1899068"
  },
  {
    "text": "these closed labs are doing. We don't know if they're doing\nthe exact same techniques, but they actually shared\nthem with the world,",
    "start": "1899068",
    "end": "1904106"
  },
  {
    "text": "which is really nice to be like, this is the cutting edge of efficient language model training. - And some of this requires\nlow level engineering,",
    "start": "1904106",
    "end": "1911949"
  },
  {
    "text": "just is a giant mess in trickery. So, as I understand, that went below CUDA,",
    "start": "1911949",
    "end": "1917835"
  },
  {
    "text": "so they go super low programming of GPUs. - Effectively, Nvidia builds\nthis library called NCCL.",
    "start": "1917835",
    "end": "1924842"
  },
  {
    "text": "In which, when you're training a model, you have all these communications between every single layer of the model",
    "start": "1924842",
    "end": "1930905"
  },
  {
    "text": "and you may have over 100 layers. - What does the NCCL\nstand for? It's N-C-C-L? - Nvidia Communications\nCollectives Library.",
    "start": "1930905",
    "end": "1936573"
  },
  {
    "text": "- [Lex] Nice.\n(Nathan laughing) Damn.\n- And so, (group laughing) when you're training a model,",
    "start": "1936573",
    "end": "1942745"
  },
  {
    "text": "you're gonna have all\nreduces and all gathers. Between each layer, between\nthe multi-layer perception",
    "start": "1942745",
    "end": "1949616"
  },
  {
    "text": "or feedforward network and the attention mechanism you'll have, you'll have basically\nthe model synchronized.",
    "start": "1949616",
    "end": "1954935"
  },
  {
    "text": "Or you'll have all reduce and all gather. And this is a communication",
    "start": "1954936",
    "end": "1960008"
  },
  {
    "text": "between all the GPUs in the network, whether it's in training or inference. So, Nvidia has a standard library. This is one of the reasons\nwhy it's really difficult",
    "start": "1960009",
    "end": "1967179"
  },
  {
    "text": "to use anyone else's hardware for training is because no one's really built a standard communications library.",
    "start": "1967179",
    "end": "1973501"
  },
  {
    "text": "And Nvidia's done this at a higher level. At DeepSeek, because they\nhave certain limitations",
    "start": "1973501",
    "end": "1978712"
  },
  {
    "text": "around the GPUs that they have access to, the interconnects are\nlimited to some extent",
    "start": "1978712",
    "end": "1983841"
  },
  {
    "text": "by the restrictions of the GPUs that were shipped into China legally, not the ones that are smuggled,\nbut legally shipped in,",
    "start": "1983841",
    "end": "1989443"
  },
  {
    "text": "that they used to train this model. They had to figure out\nhow to get efficiencies.",
    "start": "1989443",
    "end": "1994629"
  },
  {
    "text": "And one of those things is that instead of just calling the Nvidia library NCCL,",
    "start": "1994629",
    "end": "2000269"
  },
  {
    "text": "they instead created their, they scheduled their own communications, which some of the labs do.",
    "start": "2000269",
    "end": "2006551"
  },
  {
    "text": "Meta talked about in Llama 3 how they made their own\ncustom version of NCCL. They didn't talk about the\nimplementation details.",
    "start": "2007434",
    "end": "2013383"
  },
  {
    "text": "This is some of what they did. Probably not as well as,\nmaybe not as well as DeepSeek because DeepSeek necessity\nis the mother of innovation",
    "start": "2013383",
    "end": "2020422"
  },
  {
    "text": "and they had to do this. Whereas in the ca... OpenAI has people",
    "start": "2020422",
    "end": "2025852"
  },
  {
    "text": "that do this sort of stuff,\nAnthropic, et cetera. But DeepSeek certainly did it publicly and they may have done it even better",
    "start": "2025852",
    "end": "2031605"
  },
  {
    "text": "because they were gimp on a certain aspect of the chips that they have access to. And so, they scheduled communications",
    "start": "2031605",
    "end": "2038998"
  },
  {
    "text": "by scheduling specific SMs. SMs you could think of as\nlike the core on a GPU.",
    "start": "2040585",
    "end": "2045880"
  },
  {
    "text": "So, there's hundreds of cores or there's a bit over\n100 cores, SMs, on a GPU,",
    "start": "2045880",
    "end": "2051669"
  },
  {
    "text": "and they were specifically scheduling, hey, which ones are running the model? Which ones are doing all reduce? Which one are doing all gather?",
    "start": "2051670",
    "end": "2057057"
  },
  {
    "text": "And they would flip back\nand forth between them. And this requires extremely\nlow level programming. - This is what NCCL does automatically",
    "start": "2057058",
    "end": "2063519"
  },
  {
    "text": "or other Nvidia libraries handle this automatically usually. - Yeah, exactly. And so, technically, they're using PTX,",
    "start": "2063519",
    "end": "2070127"
  },
  {
    "text": "which is like, you could think of it as like an assembly type language. It's not exactly that or instruction set.",
    "start": "2070128",
    "end": "2075981"
  },
  {
    "text": "Coding directly to assembly\nor instruction set. It's not exactly that, but that's still part of technically CUDA.",
    "start": "2075981",
    "end": "2082086"
  },
  {
    "text": "But it's like, do I wanna write in Python, PyTorch equivalent, and\ncall Nvidia libraries?",
    "start": "2082087",
    "end": "2087180"
  },
  {
    "text": "Do I want to go down to the C level. Or in code, even lower level? Or do I wanna go all the way down to the assembly or ISA level?",
    "start": "2087181",
    "end": "2093361"
  },
  {
    "text": "And there are cases where you go all the way down\nthere at the very big labs, but most companies just do not do that",
    "start": "2093361",
    "end": "2099306"
  },
  {
    "text": "because it's a waste of time and the efficiency gains\nyou get are not worth it. But DeepSeek's\nimplementation is so complex.",
    "start": "2099306",
    "end": "2106516"
  },
  {
    "text": "Especially with their mixture of experts. People have done mixture of experts, but they're generally 8, 16 experts.",
    "start": "2106516",
    "end": "2113069"
  },
  {
    "text": "And they activate too. So, one of the words that we like to use is\nsparsity factor or usage.",
    "start": "2113069",
    "end": "2119327"
  },
  {
    "text": "So, you might have four, one\nfourth of your model activate. And that's what misdraws mixed role model.",
    "start": "2119327",
    "end": "2127503"
  },
  {
    "text": "They're model that really\ncatapulted them to like, oh my god, they're really, really good. OpenAI has also had models that are MoE,",
    "start": "2127503",
    "end": "2134223"
  },
  {
    "text": "and so have all the other\nlabs that are major closed. But what DeepSeek did that\nmaybe only the leading labs",
    "start": "2134223",
    "end": "2140713"
  },
  {
    "text": "have only just started recently doing is have such a high sparsity factor. It's not one fourth of the model. Two out of eight experts activating",
    "start": "2140713",
    "end": "2147612"
  },
  {
    "text": "every time you go through the model. It's 8 out of 256. - And there's different\nimplementations from mixture",
    "start": "2147612",
    "end": "2153481"
  },
  {
    "text": "of experts where you can\nhave some of these experts that are always activated,",
    "start": "2153481",
    "end": "2158563"
  },
  {
    "text": "which this just looks like\na small neural network. And then, all the tokens go through that.",
    "start": "2158563",
    "end": "2163701"
  },
  {
    "text": "And then, they also go through some that are selected by\nthis routing mechanism.",
    "start": "2163701",
    "end": "2168772"
  },
  {
    "text": "And one of the innovations\nin DeepSeek's architecture is that they change the routing mechanism",
    "start": "2168772",
    "end": "2174911"
  },
  {
    "text": "in mixture of expert models. There's something called\nan auxiliary loss, which effectively means during training,",
    "start": "2174911",
    "end": "2181063"
  },
  {
    "text": "you want to make sure\nthat all of these experts are used across the tasks\nthat the model sees.",
    "start": "2181063",
    "end": "2186610"
  },
  {
    "text": "Why there can be failures\nin mixture of experts is that when you're doing this training,",
    "start": "2186611",
    "end": "2192093"
  },
  {
    "text": "the one objective is\ntoken prediction accuracy. And if you just let turning go",
    "start": "2192093",
    "end": "2197474"
  },
  {
    "text": "with a mixture of expert\nmodel on your own, it can be that the model learns to only use a subset of the experts.",
    "start": "2197474",
    "end": "2204754"
  },
  {
    "text": "And in the MoE literature, there's something called\nthe auxiliary loss, which helps balance them.",
    "start": "2204754",
    "end": "2210289"
  },
  {
    "text": "But if you think about the loss\nfunctions of deep learning, this even connects to the bitter lesson,",
    "start": "2210289",
    "end": "2215806"
  },
  {
    "text": "is that you want to have\nthe minimum inductive bias in your model to let the\nmodel learn maximally.",
    "start": "2215806",
    "end": "2222168"
  },
  {
    "text": "And this auxiliary loss, this balancing across experts\ncould be seen as intention",
    "start": "2222168",
    "end": "2227196"
  },
  {
    "text": "with the prediction\naccuracy of the tokens. So, we don't know the exact extent that the DeepSeek MoE change,",
    "start": "2227196",
    "end": "2233262"
  },
  {
    "text": "which is instead of\ndoing an auxiliary loss, they have an extra\nparameter in their routing, which after the batches,",
    "start": "2233262",
    "end": "2239035"
  },
  {
    "text": "they update this parameter to make sure that the next batches all\nhave a similar use of experts.",
    "start": "2239035",
    "end": "2244453"
  },
  {
    "text": "And this type of change can be big, it can be small, but\nthey add up over time. And this is the sort of thing",
    "start": "2244453",
    "end": "2249527"
  },
  {
    "text": "that just points to them innovating. And I'm sure all the labs\nthat are training big MoEs are looking at this sort of things,",
    "start": "2249527",
    "end": "2255208"
  },
  {
    "text": "which is getting away\nfrom the auxiliary loss. Some of them might already use it, but you keep accumulating gains.",
    "start": "2255208",
    "end": "2260345"
  },
  {
    "text": "And we'll talk about the\nphilosophy of training and how you organize these organizations.",
    "start": "2260345",
    "end": "2265774"
  },
  {
    "text": "And a lot of it is just\ncompounding small improvements over time in your data,\nin your architecture, in your post-training,",
    "start": "2265774",
    "end": "2271477"
  },
  {
    "text": "and how they integrate with each other. And DeepSeek does the same thing and some of 'em are shared or a lot,",
    "start": "2271477",
    "end": "2277225"
  },
  {
    "text": "we have to take them on face value that they share their\nmost important details. The architecture and the\nweights are out there,",
    "start": "2277225",
    "end": "2282421"
  },
  {
    "text": "so we're seeing what they're\ndoing, and it adds up. - Going back to the efficiency\nand complexity point.",
    "start": "2282421",
    "end": "2288342"
  },
  {
    "text": "It's 32 versus 4 for mixed draw and other MoE models that\nhave been publicly released.",
    "start": "2288342",
    "end": "2294701"
  },
  {
    "text": "So, this ratio is extremely high. And what Nathan was getting at there was, when you have such a\ndifferent level of sparsity,",
    "start": "2294701",
    "end": "2301633"
  },
  {
    "text": "you can't just have every\nGPU have the entire model. The model's too big, there's\ntoo much complexity there.",
    "start": "2301633",
    "end": "2307591"
  },
  {
    "text": "So, you have to split up the model with different types of parallelism. And so, you might have different experts",
    "start": "2307591",
    "end": "2313043"
  },
  {
    "text": "on different GPU nodes, but now what happens when\nthis set of data that you get,",
    "start": "2313043",
    "end": "2318267"
  },
  {
    "text": "hey, all of it looks like this one way and all of it should route\nto one part of my model.",
    "start": "2318268",
    "end": "2324127"
  },
  {
    "text": "So, when all of it routes\nto one part of the model, then you can have this overloading",
    "start": "2324127",
    "end": "2329990"
  },
  {
    "text": "of a certain set of the GPU resources or a certain set of the GPUs, and then the rest of the\ntraining network sits idle,",
    "start": "2331287",
    "end": "2339027"
  },
  {
    "text": "because all of the tokens\nare just routing to that. So, this is the biggest complexity, one of the big complexities with running a very sparse\nmixture of experts model,",
    "start": "2339028",
    "end": "2346952"
  },
  {
    "text": "i.e, this 32 ratio versus this 4 ratio, is that you end up with so many",
    "start": "2346952",
    "end": "2352708"
  },
  {
    "text": "of the experts just sitting there idle. So, how do I load balance between them? How do I schedule the\ncommunications between them?",
    "start": "2352708",
    "end": "2358506"
  },
  {
    "text": "This is a lot of the extremely\nlow level detailed work that they figured out in the public first",
    "start": "2358507",
    "end": "2365163"
  },
  {
    "text": "and potentially second\nor third in the world, and maybe even first in some cases. - What lesson do you, in the direction",
    "start": "2365163",
    "end": "2373161"
  },
  {
    "text": "of the bitter lesson, do\nyou take from all of this? Is this going to be the direction where a lot of the gain is going to be,",
    "start": "2373162",
    "end": "2379270"
  },
  {
    "text": "which is this kind of\nlow level optimization? Or is this a short-term thing where the biggest gains will be more",
    "start": "2379270",
    "end": "2386369"
  },
  {
    "text": "on the algorithmic high\nlevel side of post-training? Is this like a short-term leap",
    "start": "2386369",
    "end": "2392630"
  },
  {
    "text": "because they've figured out like a hack, because constraints, necessity\nis the mother of invention,",
    "start": "2392630",
    "end": "2398611"
  },
  {
    "text": "or is there still a lot of gains? - I think we should summarize what the bitter lesson actually is about,",
    "start": "2398611",
    "end": "2404010"
  },
  {
    "text": "is that the bitter lesson,\n- Okay. - essentially, if you paraphrase it, is that the types of training",
    "start": "2404010",
    "end": "2410280"
  },
  {
    "text": "that will win out in deep learning as we go are those methods\nthat are which are scalable",
    "start": "2410280",
    "end": "2416120"
  },
  {
    "text": "in learning and search\nis what it calls out. And the scale word gets a\nlot of attention in this.",
    "start": "2416120",
    "end": "2423061"
  },
  {
    "text": "The interpretation that\nI use is effectively to avoid adding the human\npriors to your learning process.",
    "start": "2424981",
    "end": "2432572"
  },
  {
    "text": "And if you read the original essay, this is what it talks about is how researchers will try to come up",
    "start": "2433764",
    "end": "2439169"
  },
  {
    "text": "with what clever solutions\nto their specific problem that might get them small\ngains in the short-term",
    "start": "2439169",
    "end": "2445868"
  },
  {
    "text": "while simply enabling\nthese deep learning systems to work efficiently. And for these bigger\nproblems in the long-term",
    "start": "2445868",
    "end": "2452947"
  },
  {
    "text": "might be more likely to scale\nand continue to drive success.",
    "start": "2452947",
    "end": "2457947"
  },
  {
    "text": "And therefore, we were talking about relatively small\nimplementation changes to the mixture of experts model.",
    "start": "2458209",
    "end": "2465198"
  },
  {
    "text": "And therefore, it's like, okay, we will need a few more\nyears to know if one of these were actually really\ncrucial to the bitter lesson.",
    "start": "2465198",
    "end": "2472247"
  },
  {
    "text": "But the bitter lesson is really this long-term arc of how\nsimplicity can often win.",
    "start": "2472247",
    "end": "2477390"
  },
  {
    "text": "And there's a lot of\nsayings in the industry, like the models just wanna learn. You have to give them\nthe simple loss landscape",
    "start": "2477390",
    "end": "2483577"
  },
  {
    "text": "where you put compute through the model, and they will learn and getting\nbarriers out of the way.",
    "start": "2483577",
    "end": "2489468"
  },
  {
    "text": "- That's where the power,\nsomething like NCCL comes in, where standardized code",
    "start": "2489468",
    "end": "2494515"
  },
  {
    "text": "that could be used by a lot of people to create sort of simple\ninnovations that can scale. Which is why the hacks,\nI imagine the code base",
    "start": "2494515",
    "end": "2502776"
  },
  {
    "text": "for DeepSeek is probably a giant mess. - I'm sure they have, DeepSeek definitely has code\nbases that are extremely messy,",
    "start": "2502776",
    "end": "2509086"
  },
  {
    "text": "where they're testing these new ideas, multi-head latent attention, probably could start in\nsomething like a Jupyter Notebook",
    "start": "2509086",
    "end": "2515300"
  },
  {
    "text": "or somebody try something on a few GPUs. And that is really messy. But the stuff that trains the DeepSeek-V3",
    "start": "2515300",
    "end": "2522081"
  },
  {
    "text": "and DeepSeek-R1, those libraries, if you were to present them to us, I would guess are extremely\nhigh quality code.",
    "start": "2522081",
    "end": "2530210"
  },
  {
    "text": "- It's a high quality readable code. Yeah. - I think there is one\naspect to note though. Is that there is the general ability",
    "start": "2530210",
    "end": "2537776"
  },
  {
    "text": "for that to transfer across\ndifferent types of runs. You may make really,\nreally high quality code",
    "start": "2538735",
    "end": "2543834"
  },
  {
    "text": "for one specific model architecture at one size.\n- Yeah. - And then, that is not transferable to,",
    "start": "2543834",
    "end": "2550192"
  },
  {
    "text": "hey, when I make this architecture tweak, everything's broken again. That's something that could be",
    "start": "2550193",
    "end": "2555763"
  },
  {
    "text": "with their specific low level\ncoding of scheduling SMs, is specific to this model\narchitecture and size.",
    "start": "2556739",
    "end": "2563491"
  },
  {
    "text": "And whereas like Nvidia's\ncollectives library is more like, hey,\nit'll work for anything.",
    "start": "2563491",
    "end": "2568701"
  },
  {
    "text": "You wanna do an all reduce, great, I don't care what your model\narchitecture is, it'll work. And you're giving up a lot of performance",
    "start": "2568701",
    "end": "2574332"
  },
  {
    "text": "when you do that in many cases. But it's worthwhile for them to do the specific optimization\nfor the specific run,",
    "start": "2574332",
    "end": "2581980"
  },
  {
    "text": "given the constraints that\nthey have regarding compute. - I wonder how stressful it is to like,",
    "start": "2581980",
    "end": "2587721"
  },
  {
    "text": "these frontier models,\nlike initiate training, like to have the code- - [Nathan] Push the button.",
    "start": "2587721",
    "end": "2593152"
  },
  {
    "text": "- to push the button\n(Nathan laughing) that you're now spending\na large amount of money",
    "start": "2593152",
    "end": "2598688"
  },
  {
    "text": "and time to train this. There must be a lot of\ninnovation on the debugging stage",
    "start": "2598688",
    "end": "2605347"
  },
  {
    "text": "of making sure there's no\nissues that you're monitoring and visualizing every aspect",
    "start": "2605347",
    "end": "2610479"
  },
  {
    "text": "of the training, all that kind of stuff. - When people are training, they have all these various dashboards, but the most simple one\n- Yeah.",
    "start": "2610479",
    "end": "2616867"
  },
  {
    "text": "- is your loss.\n- Right. - And it continues to go down. But in reality, especially with more\ncomplicated stuff like MoE,",
    "start": "2616867",
    "end": "2623816"
  },
  {
    "text": "the biggest problem with it, or FP8 training, which\nis another innovation, going to a lower precision number format,",
    "start": "2623816",
    "end": "2629180"
  },
  {
    "text": "i.e, less accurate, is that\nyou end up with loss spikes. And no one knows why\nthe loss spike happened.",
    "start": "2629180",
    "end": "2634474"
  },
  {
    "text": "And for a lot-\n- Some of them you do. - Some of them you do.\n- Some of them are bad data. I give AI2's example of what blew up our earlier models",
    "start": "2634474",
    "end": "2641087"
  },
  {
    "text": "is a subreddit called Microwave Gang. We love to shoutout this out.\n(Lex laughing) It's a real thing. You can pull up Microwave Gang.",
    "start": "2641087",
    "end": "2647169"
  },
  {
    "text": "Essentially, it's a subreddit where everybody makes posts\nthat are just the letter M. So, it's like, mm.",
    "start": "2647169",
    "end": "2652449"
  },
  {
    "text": "So, there's extremely long sequences of the letter M, and then the\ncomments are like, beep beep. 'Cause it's in the microwave end.",
    "start": "2652449",
    "end": "2657826"
  },
  {
    "text": "- Yeah.\n- If you pass this into a model that's trained\nto be a normal producing text, it's extremely high loss.",
    "start": "2657826",
    "end": "2663097"
  },
  {
    "text": "'Cause normally,\n- Yeah. - you see an M.\n(Lex laughing) You don't predict Ms for a long time. So, this is something that\ncause a loss spikes for us.",
    "start": "2663097",
    "end": "2669878"
  },
  {
    "text": "But when you have much like, this is old, this is not recent. And when you have more\nmature data systems,",
    "start": "2669878",
    "end": "2675220"
  },
  {
    "text": "that's not the thing that\ncauses the loss spike. And what Dylan is saying is true, but it's levels to this sort of idea.",
    "start": "2675220",
    "end": "2681617"
  },
  {
    "text": "- With regards to the stress, (Nathan and Lex laughing) these people are like,\nyou'll go out to dinner with a friend that works\nat one of these labs.",
    "start": "2681617",
    "end": "2688121"
  },
  {
    "text": "And they'll just be like\n(Nathan laughing) looking at their phone every 10 minutes, and they're not like... It's one thing if they're texting,",
    "start": "2688121",
    "end": "2694129"
  },
  {
    "text": "but they're just like,\n- Yeah. - is the loss-\n- It's literal. The tokens per second loss, not blown up.",
    "start": "2694129",
    "end": "2700787"
  },
  {
    "text": "They're just watching this. (chuckles) - And the heart rate goes\nup if there's a spike. - And some level of spikes is normal.",
    "start": "2700788",
    "end": "2707031"
  },
  {
    "text": "It'll recover and be back. Sometimes a lot of the\nold strategy was like, you just stop the run,\nrestart from an old version,",
    "start": "2707031",
    "end": "2713029"
  },
  {
    "text": "and then change the data\nmix, and then it keeps going. - There are even\ndifferent types of spikes. So, Dirk Groeneveld has a theory of AI too",
    "start": "2713029",
    "end": "2721131"
  },
  {
    "text": "that's like fast spikes and slow spikes, where there are sometimes where\nyou're looking at the loss and there are other parameters, you can see it start to\ncreep up, and then blow up.",
    "start": "2721131",
    "end": "2728749"
  },
  {
    "text": "And that's really hard to recover from. So, you have to go back much further. So, you have the stressful\nperiod, where it's like flat or it might start going up\nand you're like, what do I do?",
    "start": "2728749",
    "end": "2735198"
  },
  {
    "text": "Whereas there are also lost\nspikes that are, it looks good. And then, there's one spiky data point. And what you could do\nis you just skip those.",
    "start": "2735198",
    "end": "2741202"
  },
  {
    "text": "You see that there's a spike. You're like, okay, I can ignore this data, don't update the model, and do the next one, and\nit'll recover quickly.",
    "start": "2741202",
    "end": "2747230"
  },
  {
    "text": "But these like untrickier implementation, so as you get more complex\nin your architecture",
    "start": "2747230",
    "end": "2752983"
  },
  {
    "text": "and you scale up to more GPUs, you have more potential\nfor your loss blowing up.",
    "start": "2752983",
    "end": "2758161"
  },
  {
    "text": "So, it's like there's a distribution.\n- And then, the whole idea of grokking also comes in. It's like, just because it's\nslowed down from improving",
    "start": "2758162",
    "end": "2764606"
  },
  {
    "text": "and loss doesn't mean it's not learning, because all of a sudden,\nit could be like this and it could just spike down in loss again because it learned,\ntruly learned something.",
    "start": "2764606",
    "end": "2772466"
  },
  {
    "text": "And it took some time\nfor it to learn that. It's not like a gradual process. And that's what humans are like, that's what models are like.",
    "start": "2772466",
    "end": "2778353"
  },
  {
    "text": "It's really a stressful\ntask as you mentioned. - And the whole time, the\ndollar count is going up.",
    "start": "2778353",
    "end": "2784419"
  },
  {
    "text": "- Every company has failed runs. You need failed run to push the envelope\non your infrastructure. So, a lot of news cycles",
    "start": "2784419",
    "end": "2790311"
  },
  {
    "text": "are made of X company had Y failed run. Every company that's trying to push the frontier of AI has these.",
    "start": "2790311",
    "end": "2797992"
  },
  {
    "text": "So, yes, it's noteworthy\nbecause it's a lot of money and it can be week to month setback,",
    "start": "2797992",
    "end": "2803000"
  },
  {
    "text": "but it is part of the process. - But how do you get, if you're DeepSeek, how do you get to a place where holy shit,",
    "start": "2803000",
    "end": "2810063"
  },
  {
    "text": "there's a successful\ncombination of hyperparameters? - A lot of small failed runs. - And so, rapid iteration\n(Nathan chuckles)",
    "start": "2810063",
    "end": "2817469"
  },
  {
    "text": "through failed runs, until-\n- Yeah, and successful ones. You just-\n- And then, you build up some intuition like this,\nthis mixture of expert works,",
    "start": "2817469",
    "end": "2825703"
  },
  {
    "text": "and then this implementation of MLA works. - Key hyperparameters like learning rate",
    "start": "2825703",
    "end": "2831266"
  },
  {
    "text": "and regularization and things like this. And you find the regime that\nworks for your code base.",
    "start": "2831267",
    "end": "2837630"
  },
  {
    "text": "Talking to people at Frontier\nLabs, there's a story that you can tell where\ntraining language models is kind of a path that you need to follow.",
    "start": "2837630",
    "end": "2845005"
  },
  {
    "text": "So, you need to unlock the ability to train a certain type of\nmodel or a certain scale. And then, your code base\nand your internal knowhow",
    "start": "2845005",
    "end": "2851608"
  },
  {
    "text": "of which hyperparameters\nwork for it is known. And you look at the\nDeepSeek papers and models, they've scaled up,\nthey've added complexity,",
    "start": "2851608",
    "end": "2858289"
  },
  {
    "text": "and it's just continuing to build the capabilities that they have. - There's the concept of a YOLO run.",
    "start": "2858289",
    "end": "2864170"
  },
  {
    "text": "(Nathan laughing) So, YOLO, you only live once.\n- Yep. - And what it is, is like, there's all this experimentation\nyou do at the small-scale.",
    "start": "2864170",
    "end": "2872686"
  },
  {
    "text": "Research ablations, you have\nyour Jupyter Notebook with, you're experimenting with MLA\non three GPUs or whatever,",
    "start": "2872686",
    "end": "2879100"
  },
  {
    "text": "and you're doing all these\ndifferent things like, hey, do I do 4 expert, 4 active\nexperts, 128 experts,",
    "start": "2879101",
    "end": "2884734"
  },
  {
    "text": "do I arrange the experts this way? All these different\nmodel architecture things you're testing at a very small-scale.",
    "start": "2884734",
    "end": "2890534"
  },
  {
    "text": "Couple researchers,\nfew GPUs, tens of GPUs, hundreds of GPUs, whatever it is. And then, all of a sudden,",
    "start": "2890534",
    "end": "2896191"
  },
  {
    "text": "you're like, okay guys,\nno more fucking around. No more screwing around. Everyone, take all the resources we have,",
    "start": "2896191",
    "end": "2902921"
  },
  {
    "text": "let's pick what we think will\nwork, and just go for it. YOLO.\n- Yeah. - And this is where that sort\nof stress comes in is like,",
    "start": "2902921",
    "end": "2908657"
  },
  {
    "text": "well, I know it works here, but some things that work\nhere don't work here, and some things that work\nhere don't work down here",
    "start": "2908657",
    "end": "2914911"
  },
  {
    "text": "- Yeah.\n- in this terms of scale. So, it's really truly a YOLO run, and there's this discussion\nof certain researchers",
    "start": "2914911",
    "end": "2923770"
  },
  {
    "text": "just have this methodical nature. They can find the whole search space and figure out all the\nablations of different research,",
    "start": "2923770",
    "end": "2929591"
  },
  {
    "text": "and really see what is best. And there's certain\nresearchers who just like, have that innate gut instinct of like,",
    "start": "2929592",
    "end": "2936169"
  },
  {
    "text": "this is the YOLO run. I'm looking at the data,\n(Dylan drowns out Nathan) I think this is it. - This is why you wanna\nwork in post-training,",
    "start": "2936169",
    "end": "2941584"
  },
  {
    "text": "because the GPU cost\nfor training is lower. So, you can make a higher percentage of your training runs YOLO runs - Yeah.\n- For now.",
    "start": "2941584",
    "end": "2947874"
  },
  {
    "text": "- Yeah, for now, for now.\n- For now, for now. (laughs) - So, some of this is\nfundamentally luck still.",
    "start": "2947874",
    "end": "2954523"
  },
  {
    "text": "- Luck is skill in many cases. - Yeah, it looks lucky right when you're- - But the hill to climb, if\nyou're on one of these labs",
    "start": "2954523",
    "end": "2962031"
  },
  {
    "text": "and you have an evaluation\nand you're not crushing, there's a repeated playbook\nof how you improve things. There are localized improvements,",
    "start": "2962031",
    "end": "2967999"
  },
  {
    "text": "which might be data improvements. And these add up into the whole model\njust being much better. And when you zoom in really close,",
    "start": "2967999",
    "end": "2973419"
  },
  {
    "text": "it can be really obvious that this model's just\nreally bad at this thing and we can fix it. And you just add these up.",
    "start": "2973419",
    "end": "2979281"
  },
  {
    "text": "So, some of it feels like luck, but on the ground, especially with these new\nreasoning models we're talking to,",
    "start": "2979281",
    "end": "2984809"
  },
  {
    "text": "is just so many ways\nthat we can poke around. And normally, it's that some\nof them give big improvements.",
    "start": "2984810",
    "end": "2991361"
  },
  {
    "text": "- The search space is near infinite. And yet, the amount of compute and time you have is very low.",
    "start": "2991361",
    "end": "2996945"
  },
  {
    "text": "And you have to hit release schedules, you have to not get\nblown past by everyone.",
    "start": "2996946",
    "end": "3002257"
  },
  {
    "text": "Otherwise, what happened with DeepSeek crushing Meta and Misral and Cohere,",
    "start": "3002257",
    "end": "3007421"
  },
  {
    "text": "and all these guys, they moved too slow. They maybe were too methodical. I don't know, they\ndidn't hit the YOLO run, whatever the reason was,\nmaybe they weren't as skill.",
    "start": "3007421",
    "end": "3015118"
  },
  {
    "text": "You can call it luck if you want, but at the end of the day, it's skill. - So, 2025 is the year of the YOLO run.",
    "start": "3015118",
    "end": "3021374"
  },
  {
    "text": "It seems like all the labs are going in. - I think it's even more\nimpressive what OpenAI did in 2022.",
    "start": "3021374",
    "end": "3029476"
  },
  {
    "text": "At the time, no one believed in mixture of experts models at Google\nwho had all the researchers.",
    "start": "3029477",
    "end": "3035453"
  },
  {
    "text": "OpenAI had such little compute and they devoted all of their\ncompute for many months,",
    "start": "3035453",
    "end": "3041148"
  },
  {
    "text": "all of it, 100% for many months, to GPT-4, with a brand new architecture\nwith no belief that, hey,",
    "start": "3041148",
    "end": "3048205"
  },
  {
    "text": "let me spend a couple $100 million, which is all of the money\nI have, on this model. That is truly YOLO,",
    "start": "3048205",
    "end": "3054527"
  },
  {
    "text": "right?\n- Yeah, yeah. - Now, people are like, all these training run\nfailures that are in the media,",
    "start": "3054527",
    "end": "3059943"
  },
  {
    "text": "it's like, okay, great, but actually, a huge chunk of\nmy GPs are doing inference. I still have a bunch\ndoing research constantly.",
    "start": "3059944",
    "end": "3066312"
  },
  {
    "text": "And yes, my biggest cluster is training, but on this YOLO run, but that YOLO run is much less risky",
    "start": "3066312",
    "end": "3072503"
  },
  {
    "text": "than what OpenAI did in 2022, or maybe what DeepSeek did now, or like, hey, we're just\ngonna throw everything at it.",
    "start": "3072503",
    "end": "3079732"
  },
  {
    "text": "- The big winners throughput human history are the ones who are willing\nto do YOLO at some point.",
    "start": "3079732",
    "end": "3086003"
  },
  {
    "start": "3085000",
    "end": "3537000"
  },
  {
    "text": "Okay. What do we understand about the hardware it's been trained on? DeepSeek. - DeepSeek is very interesting.",
    "start": "3086003",
    "end": "3092948"
  },
  {
    "text": "A second to take us to zoom out out of who\nthey are, first of all. High-Flyer is a hedge fund",
    "start": "3092948",
    "end": "3098034"
  },
  {
    "text": "that has historically\ndone quantitative trading in China as well as elsewhere. And they have always had a\nsignificant number of GPUs.",
    "start": "3098034",
    "end": "3105374"
  },
  {
    "text": "In the past, a lot of these\nhigh frequency trading, algorithmic, quant traders, used FPGAs, but it shifted to GPUs, definitely.",
    "start": "3105374",
    "end": "3112363"
  },
  {
    "text": "And there's both. But GPUs especially, and High-Flyer, which is the hedge fund\nthat owns DeepSeek.",
    "start": "3112363",
    "end": "3117564"
  },
  {
    "text": "And everyone who works for DeepSeek is part of High-Flyer to some extent. Same parent company, same owner, same CEO.",
    "start": "3117564",
    "end": "3124916"
  },
  {
    "text": "They had all these resources\nand infrastructure for trading, and then they devoted a\nhumongous portion of them",
    "start": "3124916",
    "end": "3132055"
  },
  {
    "text": "to training models, both\nlanguage models and otherwise. Because these techniques\nwere heavily AI-influenced.",
    "start": "3132055",
    "end": "3140326"
  },
  {
    "text": "More recently, people have realized, hey, trading with, even when\nyou go back to Renaissance",
    "start": "3140326",
    "end": "3147454"
  },
  {
    "text": "and all these quantitative firms, natural language processing is the key to trading really fast,",
    "start": "3147454",
    "end": "3153195"
  },
  {
    "text": "understanding a press release\nand making the right trade. And so, DeepSeek has always\nbeen really good at this.",
    "start": "3153195",
    "end": "3158754"
  },
  {
    "text": "And even as far back as 2021, they have press releases\nand papers saying like, hey,",
    "start": "3158754",
    "end": "3164067"
  },
  {
    "text": "we're the first company in China with an A100 cluster this large. It was 10,000 A100 GPUs. This is in 2021.",
    "start": "3164068",
    "end": "3171475"
  },
  {
    "text": "Now, this wasn't all for\ntraining large language models. This is mostly for training models for their quantitative aspects,",
    "start": "3171475",
    "end": "3178083"
  },
  {
    "text": "or quantitative trading,\nas well as a lot of that was natural language\nprocessing, to be clear.",
    "start": "3178083",
    "end": "3183224"
  },
  {
    "text": "And so, this is the history. So, verifiable fact is that in 2021, they built the largest\ncluster, at least they claim",
    "start": "3183224",
    "end": "3189686"
  },
  {
    "text": "it was the largest cluster\nin China, 10,000 GPUs. - Before expert controls started. - Yeah.\n- It's like,",
    "start": "3189686",
    "end": "3195206"
  },
  {
    "text": "they've had a huge cluster before any conversation\nof export controls. So, then you step it forward to like, what have they done over the\nlast four years since then?",
    "start": "3195206",
    "end": "3203086"
  },
  {
    "text": "Obviously, they've continued to operate the hedge fund,\nprobably make tons of money. And the other thing is that they've leaned more\nand more and more into AI.",
    "start": "3203086",
    "end": "3210214"
  },
  {
    "text": "the CEO, Liang Wenfeng, Liang- - You're not putting me on spot on this. We discussed this. (laughs)\n- Liang Feng, the CEO,",
    "start": "3210214",
    "end": "3217926"
  },
  {
    "text": "he owns maybe-\n- We're all fam. (chuckles) - Liang Feng, he owns maybe a little bit more than half\nthe company, allegedly,",
    "start": "3217926",
    "end": "3224040"
  },
  {
    "text": "an extremely like\nElon-Jensen kind of figure, where he is just involved in everything.",
    "start": "3224040",
    "end": "3230301"
  },
  {
    "text": "And so, over that time period, he's gotten really in depth into AI. He actually has a bit of a like a,",
    "start": "3230301",
    "end": "3235845"
  },
  {
    "text": "if you see some of his statements, a bit of an EAC vibe almost. - Total AGI vibes.",
    "start": "3235845",
    "end": "3241356"
  },
  {
    "text": "And like, we need to do this, we need to make a new ecosystem of OpenAI.",
    "start": "3241356",
    "end": "3246564"
  },
  {
    "text": "We need China to lead on\nthis sort of ecosystem, because historically, the Western countries have\nled on software ecosystems,",
    "start": "3246564",
    "end": "3253817"
  },
  {
    "text": "and straight up acknowledges, in order to do this, we need\nto do something different.",
    "start": "3253817",
    "end": "3259415"
  },
  {
    "text": "DeepSeek is his way of doing this. Some of the translated interviews with him are fantastic.\n- So, he has done interviews? - Yeah.\n- Do you think",
    "start": "3259416",
    "end": "3265136"
  },
  {
    "text": "he would do a Western interview or no? Or is there controls on the channel?\n- There hasn't been one yet, but I would try it.\n- Okay. All right.",
    "start": "3265136",
    "end": "3271522"
  },
  {
    "text": "(Nathan laughing)\nWell, I just got a Chinese translator, so it was great. This is all push. So, fascinating figure, engineer\npushing full on into AI,",
    "start": "3271522",
    "end": "3280647"
  },
  {
    "text": "leveraging the success from\nthe high frequency trading. - Very direct quotes,\nlike we will not switch",
    "start": "3280647",
    "end": "3286768"
  },
  {
    "text": "to closed source when\nasked about this stuff. Very long-term motivated",
    "start": "3286768",
    "end": "3292295"
  },
  {
    "text": "in how the ecosystem of AI should work. And I think from a Chinese perspective,",
    "start": "3292295",
    "end": "3298625"
  },
  {
    "text": "he wants a Chinese company\nto build this vision. - And so, this is like the,",
    "start": "3298625",
    "end": "3305016"
  },
  {
    "text": "quote, unquote, \"visionary\nbehind the company\". This hedge fund still exists,\nthis quantitative firm. And so, DeepSeek is the sort of,",
    "start": "3305016",
    "end": "3311904"
  },
  {
    "text": "solely, he got turned to this full view of like AI, everything about this. But at some point,",
    "start": "3313384",
    "end": "3318640"
  },
  {
    "text": "it slowly maneuvered and he made DeepSeek. And DeepSeek has done\nmultiple models since then. They've acquired more and more GPUs.",
    "start": "3318640",
    "end": "3324996"
  },
  {
    "text": "They share infrastructure with the fund. And so, there is no exact number",
    "start": "3324997",
    "end": "3331363"
  },
  {
    "text": "of public GPU resources that they have, but besides this 10,000 GPUs\nthat they bought in 2021,",
    "start": "3331363",
    "end": "3337896"
  },
  {
    "text": "and they were fantastically profitable. And then, this paper claims\nthey did only 2,000 H800 GPUs,",
    "start": "3337896",
    "end": "3344093"
  },
  {
    "text": "which are a restricted GPU that was previously allowed in China, but no longer allowed,\nand there's a new version. But it's basically\nNvidia's H100 for China.",
    "start": "3344093",
    "end": "3352396"
  },
  {
    "text": "And there's some restrictions on it, specifically around the\ncommunications sort of speed,",
    "start": "3352396",
    "end": "3357502"
  },
  {
    "text": "the interconnect speed, which is why they had to do\nthis crazy SM scheduling stuff.",
    "start": "3357502",
    "end": "3362628"
  },
  {
    "text": "So, going back to that. It's like this is obviously not true in terms of\ntheir total GPU count.",
    "start": "3362628",
    "end": "3368287"
  },
  {
    "text": "- Obvious available GPUs. But for this training run, you think 2,000 is the\ncorrect number or no?",
    "start": "3368288",
    "end": "3374164"
  },
  {
    "text": "- So, this is where it\ntakes a significant amount of zoning in.",
    "start": "3374165",
    "end": "3378331"
  },
  {
    "text": "What do you call your training run? You count all of the research\nand ablations that you ran,",
    "start": "3379250",
    "end": "3385096"
  },
  {
    "text": "picking all this stuff, because, yes, you can do a YOLO run, but at some level, you have to do the test\nat the small-scale,",
    "start": "3385096",
    "end": "3390494"
  },
  {
    "text": "and then you have to do\nsome test at medium-scale before you go to a large-scale. - Accepted practices\nthat for any given model",
    "start": "3390495",
    "end": "3396725"
  },
  {
    "text": "that is a notable advancement, you're gonna do 2 to 4x compute of the full training run\nin experiments alone.",
    "start": "3396726",
    "end": "3403327"
  },
  {
    "text": "- So, a lot of this compute\nthat's being scaled up is probably used in large part at this time for research.\n- Yeah.",
    "start": "3403327",
    "end": "3409344"
  },
  {
    "text": "And research begets the new ideas that lets you get huge efficiency- - Research gets you o1.",
    "start": "3409344",
    "end": "3414612"
  },
  {
    "text": "Research gets you breakthrough,\nso you need to bet on it. - So, some of the pricing strategy that we'll discuss has the\nresearch baked into the price.",
    "start": "3414612",
    "end": "3422556"
  },
  {
    "text": "- So, the numbers that DeepSeek\nspecifically said publicly are just the 10,000 GPUs in 2021,",
    "start": "3422556",
    "end": "3427863"
  },
  {
    "text": "and then 2,000 GPUs for only\nthe pre-training for V3. They did not discuss cost on R1.",
    "start": "3427863",
    "end": "3434221"
  },
  {
    "text": "They did not discuss\ncost on all the other RL, for the instruct model that they made.",
    "start": "3434221",
    "end": "3439513"
  },
  {
    "text": "They only discussed the\npre-training for the base model, and they did not discuss anything\non research and ablations.",
    "start": "3439513",
    "end": "3445225"
  },
  {
    "text": "And they do not talk\nabout any of the resources that are shared in terms of, hey, the fund is using all these GPUs.",
    "start": "3445225",
    "end": "3450696"
  },
  {
    "text": "And we know that they're very profitable in that 10,000 GPUs in 2021.",
    "start": "3450696",
    "end": "3455147"
  },
  {
    "text": "So, some of the research that we've found is that we actually believe\nthey have closer to 50,000 GPUs.",
    "start": "3456437",
    "end": "3463597"
  },
  {
    "text": "- We, as SemiAnalysis, so we should say\n- Yeah. - that you're one of the\nworld experts in figuring out",
    "start": "3463597",
    "end": "3469536"
  },
  {
    "text": "what everybody's doing in\nterms of the semiconductor, in terms of cluster buildouts, in terms of who's doing what\nin terms of training runs.",
    "start": "3469536",
    "end": "3477766"
  },
  {
    "text": "So, yeah. So, that's the we. Okay, go ahead.\n- Yeah, sorry, sorry. We believe they actually\nhave something closer to 50,000 GPUs.\n- Yeah.",
    "start": "3477767",
    "end": "3484049"
  },
  {
    "text": "- [Dylan] Now, this is\nsplit across many tasks. Again, the fund, research and ablations.",
    "start": "3484049",
    "end": "3489578"
  },
  {
    "text": "- For ballpark, how much\nwould OpenAI or Anthropic had? I think the clearest example we have, because Meta is also open,",
    "start": "3489578",
    "end": "3495343"
  },
  {
    "text": "they talk about order of 60k to a 100k H100 equivalent GPUs\nin their training clusters.",
    "start": "3495343",
    "end": "3501372"
  },
  {
    "text": "- Right. So, like Llama 3,\nthey trained on 16,000 H100s. But the company of Meta last year",
    "start": "3501372",
    "end": "3507415"
  },
  {
    "text": "publicly disclosed they bought like 400 something thousand GPUs. - Yeah. (chuckles)\n- So, of course, tiny percentage on the training.",
    "start": "3507415",
    "end": "3512990"
  },
  {
    "text": "Again, most of it is like serving me the best\nInstagram reels, or whatever. - We could get into a cost\nof like, what is the cost",
    "start": "3512990",
    "end": "3519672"
  },
  {
    "text": "of ownership for a\n2,000-GPU cluster, 10,000? There's just different sizes of companies",
    "start": "3519672",
    "end": "3525770"
  },
  {
    "text": "that can afford these things, and DeepSeek is reasonably big. Their compute allocation compared",
    "start": "3525770",
    "end": "3531845"
  },
  {
    "text": "is one of the top few in the world. It's not OpenAI, Anthropic, et cetera,",
    "start": "3531845",
    "end": "3536913"
  },
  {
    "text": "but they have a lot of compute. - Can you in general,\nactually just zoom out and also talk about the\nHopper architecture, the Nvidia Hopper GPU architecture,",
    "start": "3536913",
    "end": "3543955"
  },
  {
    "start": "3537000",
    "end": "4156000"
  },
  {
    "text": "and the difference between H100 and H800, like you mentioned the interconnects.",
    "start": "3543955",
    "end": "3549922"
  },
  {
    "text": "- Yeah, so Ampere was the\nA100, and then H100, Hopper. People use them synonymously in the US",
    "start": "3549922",
    "end": "3555677"
  },
  {
    "text": "because really, there's just\nH100 and now there's H200. But same thing mostly.",
    "start": "3555677",
    "end": "3560300"
  },
  {
    "text": "In China, they've had two, there've been different\nsalvos of export restrictions. So, initially, the US government limited on a two-factor scale,",
    "start": "3561175",
    "end": "3567917"
  },
  {
    "text": "which is chip interconnect versus FLOPS. So, any chip that had\ninterconnects above a certain level",
    "start": "3567917",
    "end": "3573644"
  },
  {
    "text": "and FLOPS above a certain\nfloating point operations, above a certain level, was restricted. Later, the government realized",
    "start": "3573644",
    "end": "3580276"
  },
  {
    "text": "that this was a flaw in the restriction, and they cut it down to just\nfloating point operations.",
    "start": "3580276",
    "end": "3586036"
  },
  {
    "text": "And so-\n- H800 had high FLOPS, low communication.\n- Exactly.",
    "start": "3586037",
    "end": "3592195"
  },
  {
    "text": "So, the H800 was the same\nperformance as H100 on FLOPS, but it just had the\ninterconnect bandwidth cut.",
    "start": "3592196",
    "end": "3598825"
  },
  {
    "text": "DeepSeek knew how to utilize this res... Hey, even though we're cut\nback on the interconnect,",
    "start": "3598825",
    "end": "3604251"
  },
  {
    "text": "we can do all this fancy stuff to figure out how to use\nthe GPU fully anyways. And so, that was back in October, 2022.",
    "start": "3604251",
    "end": "3612433"
  },
  {
    "text": "But later in 2023, into\n2023 implemented in 2024,",
    "start": "3612433",
    "end": "3616516"
  },
  {
    "text": "the US government banned the H800. And so, by the way, this H800 cluster, these 2,000 GPUs was not\neven purchased in 2024.",
    "start": "3617852",
    "end": "3624868"
  },
  {
    "text": "It was purchased in late 2023. - [Lex] Mm-hmm. - And they're just\ngetting the model out now, because it takes a lot\nof research, et cetera.",
    "start": "3624868",
    "end": "3631051"
  },
  {
    "text": "H800 was banned and now there's\na new chip called the H20. The H20 is cutback on only FLOPS,",
    "start": "3631051",
    "end": "3637436"
  },
  {
    "text": "but the interconnect\nbandwidth is the same. And in fact, in some ways,\nit's better than the H100 because it has better memory\nbandwidth and memory capacity.",
    "start": "3638305",
    "end": "3645485"
  },
  {
    "text": "So, there are, Nvidia's\nworking within the constraints of what the government sets, and then builds the best\npossible GPU for China.",
    "start": "3645486",
    "end": "3652363"
  },
  {
    "text": "- Can we take this actual tangent and we'll return back to the hardware? Is the philosophy,",
    "start": "3652363",
    "end": "3657580"
  },
  {
    "text": "the motivation, the case for\nexport controls, what is it? Dario Amodei just published a blog post",
    "start": "3657580",
    "end": "3663852"
  },
  {
    "text": "about export controls. The case he makes is that if\nAI becomes super powerful,",
    "start": "3663852",
    "end": "3669201"
  },
  {
    "text": "and he says by 2026, will\nhave AGI or super powerful AI, and that's going to give a\nsignificant, whoever builds that",
    "start": "3669201",
    "end": "3676431"
  },
  {
    "text": "will have a significant\nmilitary advantage. And so, because the United\nStates is a democracy,",
    "start": "3676431",
    "end": "3683107"
  },
  {
    "text": "and as he says, China is authoritarian or has authoritarian elements,",
    "start": "3683107",
    "end": "3689737"
  },
  {
    "text": "you want a unipolar world where\nthe super powerful military, because of the AI, is\none that's a democracy.",
    "start": "3689737",
    "end": "3697694"
  },
  {
    "text": "It's a much more complicated\nworld geopolitically when you have two superpowers",
    "start": "3697695",
    "end": "3702701"
  },
  {
    "text": "with super powerful AI\nand one is authoritarian. So, that's the case he makes.",
    "start": "3702701",
    "end": "3707801"
  },
  {
    "text": "And so, we wanna, the United States wants to use export controls to slow down,",
    "start": "3707801",
    "end": "3713400"
  },
  {
    "text": "to make sure that China can't do these gigantic training runs",
    "start": "3713400",
    "end": "3718834"
  },
  {
    "text": "that will be presumably\nrequired to build AGI. - This is very abstract. I think this can be the goal",
    "start": "3719780",
    "end": "3725769"
  },
  {
    "text": "of how some people\ndescribe export controls, is this super powerful AI.",
    "start": "3725769",
    "end": "3731009"
  },
  {
    "text": "And you touched on the training run idea. There's not many worlds where\nChina cannot train AI models.",
    "start": "3731009",
    "end": "3738704"
  },
  {
    "text": "I think export controls are\ndecapping the amount of compute or the density of compute\nthat China can have.",
    "start": "3738704",
    "end": "3745745"
  },
  {
    "text": "And if you think about the\nAI ecosystem right now, as all of these AI companies,",
    "start": "3745746",
    "end": "3751328"
  },
  {
    "text": "revenue numbers are up and to the right, their AI usage is just continuing to grow, more Deep user going to inference.",
    "start": "3751328",
    "end": "3756921"
  },
  {
    "text": "A large part of export\ncontrols, if they work, is just that the amount of AI",
    "start": "3756921",
    "end": "3762413"
  },
  {
    "text": "that can be run in China\nis going to be much lower. So, on the training side,\nDeepSeek-V3 is a great example,",
    "start": "3762413",
    "end": "3768064"
  },
  {
    "text": "which you have a very focused team that can still get to the frontier of AI. This 2,000 GPUs is not that hard to get,",
    "start": "3768064",
    "end": "3774702"
  },
  {
    "text": "all considering in the world. They're still gonna have those GPUs, they're still gonna be\nable to train models.",
    "start": "3774702",
    "end": "3780151"
  },
  {
    "text": "But if there's gonna be\na huge market for AI, if you have strong export controls and you wanna have 100,000 GPUs",
    "start": "3780151",
    "end": "3785411"
  },
  {
    "text": "just serving the equivalent\nof ChatGPT clusters, with good export controls, it also just makes it so that\nAI can be used much less.",
    "start": "3785412",
    "end": "3793663"
  },
  {
    "text": "And I think that is a much easier goal to achieve than trying\nto debate on what AGI is.",
    "start": "3793664",
    "end": "3800101"
  },
  {
    "text": "And if you have these extremely\nintelligent autonomous AIs and data centers, those are the things",
    "start": "3800101",
    "end": "3805393"
  },
  {
    "text": "that could be running\nin these GPU clusters in the United States, but not in China.",
    "start": "3805393",
    "end": "3810403"
  },
  {
    "text": "- To some extent, training a\nmodel does effectively nothing. Like have a model.\n- Yeah. (chuckles) - The thing that Dario is\nspeaking to is the implementation",
    "start": "3810403",
    "end": "3819072"
  },
  {
    "text": "of that model once trained to then create huge economic growth,",
    "start": "3819072",
    "end": "3824141"
  },
  {
    "text": "huge increases in military capabilities, huge increases in productivity of people, betterment of lives, whatever you want",
    "start": "3824141",
    "end": "3831151"
  },
  {
    "text": "to direct super powerful\nAI towards, you can. But that requires a\nsignificant amounts of compute.",
    "start": "3831152",
    "end": "3836613"
  },
  {
    "text": "And so, the US government\nhas effectively said, and forever, like training\nwill always be a portion",
    "start": "3836613",
    "end": "3843444"
  },
  {
    "text": "of the total compute. We mentioned Meta's 400,000\nGPUs, only 16,000 made Llama.",
    "start": "3843444",
    "end": "3848904"
  },
  {
    "text": "So, the percentage that Meta\nis dedicating to inference, now this might be for\nrecommendation systems",
    "start": "3848904",
    "end": "3854284"
  },
  {
    "text": "that are trying to hack our\nmind into spending more time and watching more ads, or if\nit's for a super powerful AI",
    "start": "3854284",
    "end": "3860549"
  },
  {
    "text": "that's doing productive things, doesn't matter about the exact use that our economic system decides, it's that that can be delivered\nin whatever way we want.",
    "start": "3860549",
    "end": "3868582"
  },
  {
    "text": "Whereas with China, export\nrestrictions, great. You're never gonna be able\nto cut everything off.",
    "start": "3868582",
    "end": "3874641"
  },
  {
    "text": "And that's like, I think\nthat's quite a well-understood by the US government is that\nyou can't cut everything off.",
    "start": "3874641",
    "end": "3880646"
  },
  {
    "text": "- [Nathan] And they'll\nmake their own chips. They're-\n- And they're trying to make their own chips. They'll be worse than ours. But the whole point is to just keep a gap.",
    "start": "3880646",
    "end": "3886428"
  },
  {
    "text": "- Yeah.\n- And therefore, at some point, as the AI, in a world where 2, 3% economic growth,",
    "start": "3886428",
    "end": "3891960"
  },
  {
    "text": "this is really dumb, by the way, to cut off high tech and\nnot make money off of it. But in a world where super\npowerful AI comes about,",
    "start": "3891960",
    "end": "3899599"
  },
  {
    "text": "and then starts creating\nsignificant changes in society, which is what all the AI leaders and big tech companies believe, I think.",
    "start": "3899599",
    "end": "3905654"
  },
  {
    "text": "Super powerful AI is gonna\nchange society massively. And therefore, this compounding effect of the difference in\ncompute is really important.",
    "start": "3905654",
    "end": "3912643"
  },
  {
    "text": "There's some sci-fi out\nthere, where AI is measured in how much power is delivered to compute.",
    "start": "3912643",
    "end": "3919319"
  },
  {
    "text": "Or how much is being... That's a way of thinking about\nwhat's the economic output is just how much power are\nyou directing towards that AI?",
    "start": "3919319",
    "end": "3926697"
  },
  {
    "text": "Should we talk about\nreasoning models with this? As a way that this might be actionable as something that people can actually see?",
    "start": "3926697",
    "end": "3932333"
  },
  {
    "text": "So, the reasoning models that\nare coming out with R1 and o1, they're designed to use more compute.",
    "start": "3932333",
    "end": "3937471"
  },
  {
    "text": "There's a lot of buzzy words\nin the AI community about this, test-time compute, inference\ntime compute, whatever.",
    "start": "3937471",
    "end": "3944279"
  },
  {
    "text": "But Dylan has good research on this. You can get to the specific\nnumbers on the ratio of when you train a model,\nyou can look at things",
    "start": "3944280",
    "end": "3949661"
  },
  {
    "text": "about the amount of\ncompute used at training and amount of compute use inference. These reasoning models\nare making inference",
    "start": "3949661",
    "end": "3954822"
  },
  {
    "text": "way more important to doing complex tasks. In the fall in December,",
    "start": "3954822",
    "end": "3960348"
  },
  {
    "text": "their OpenAI announced this o3 model. There's another thing in\nAI, when things move fast, we get both announcements and releases.",
    "start": "3960348",
    "end": "3966284"
  },
  {
    "text": "Announcements are essentially blog posts where you pat yourself on the back and you say you did things. And releases are on the models out there,",
    "start": "3966284",
    "end": "3971546"
  },
  {
    "text": "the papers out there, et cetera. So, OpenAI has announced o3, and we can check if o3-mini is out",
    "start": "3971546",
    "end": "3976805"
  },
  {
    "text": "as of recording potentially, but that doesn't really change the point, which is that the breakthrough result",
    "start": "3976805",
    "end": "3982494"
  },
  {
    "text": "was something called ARC-AGI task, which is the abstract reasoning corpus, a task for artificial\ngeneral intelligence.",
    "start": "3982494",
    "end": "3990012"
  },
  {
    "text": "Francois Chollet is the guy who's been... It's a multi-year old paper. It's a brilliant benchmark.",
    "start": "3990012",
    "end": "3996156"
  },
  {
    "text": "And the number for OpenAI o3 to solve this was that it used as some\nnumber of samples in the API,",
    "start": "3996156",
    "end": "4003089"
  },
  {
    "text": "the API has thinking effort\nand number of samples. They used a thousand\nsamples to solve this task.",
    "start": "4003090",
    "end": "4008547"
  },
  {
    "text": "And it comes out to be\nlike 5 to $20 per question, which you're putting in\neffectively a math puzzle.",
    "start": "4008548",
    "end": "4015866"
  },
  {
    "text": "And then, it takes orders of\ndollars to answer one question. And this is a lot of compute.",
    "start": "4015866",
    "end": "4021005"
  },
  {
    "text": "If those are gonna take off in the US, OpenAI needs a ton of GPUs\non inference to capture this. They have this OpenAI\nChatGPT Pro subscription,",
    "start": "4021006",
    "end": "4028410"
  },
  {
    "text": "which is $200 a month. - [Dylan] Which Sam said\nthey're losing money on. - Which means that people are burning a lot of GPUs on inference.",
    "start": "4028410",
    "end": "4034174"
  },
  {
    "text": "And I've signed up with\nit, I've played with it. I don't think I'm a\npower user, but I use it.",
    "start": "4034174",
    "end": "4039424"
  },
  {
    "text": "That is the thing that a Chinese company with mediumly strong export controls,",
    "start": "4039425",
    "end": "4044670"
  },
  {
    "text": "there will always be loopholes, might not be able to do it all. And if that, the main result for o3 is also a spectacular\ncoding performance.",
    "start": "4044670",
    "end": "4052320"
  },
  {
    "text": "And if that feeds back into AI companies being\nable to experiment better.",
    "start": "4052321",
    "end": "4057659"
  },
  {
    "text": "- So, presumably the idea is for an AGI, a much larger fraction of the compute",
    "start": "4057659",
    "end": "4063240"
  },
  {
    "text": "would be used for this test-time\ncompute for the reasoning, for the AGI goes into a room and thinks about how\nto take over the world",
    "start": "4063240",
    "end": "4070578"
  },
  {
    "text": "and come back in 2.7 hours. - This is what-\n- And that it's gonna take",
    "start": "4070578",
    "end": "4076148"
  },
  {
    "text": "a lot of compute.\n- This is what people like CEO or leaders of OpenAI and Anthropic talk about is\nlike autonomous AI models,",
    "start": "4076148",
    "end": "4083336"
  },
  {
    "text": "which is you give them a task and they work on it in the background. I think my personal definition\nof AGI is much simpler.",
    "start": "4083336",
    "end": "4090173"
  },
  {
    "text": "I think language models are a form of AGI and all of this super\npowerful stuff is a next step that's great if we get these tools.",
    "start": "4090173",
    "end": "4096672"
  },
  {
    "text": "But a language model has so\nmuch value in so many domains. It is a general intelligence to me. But this next step of agentic things",
    "start": "4096672",
    "end": "4103699"
  },
  {
    "text": "where they're independent and they can do tasks that\naren't in the training data is what the few year outlook",
    "start": "4103699",
    "end": "4109460"
  },
  {
    "text": "that these AI companies are driving for. - I think the terminology here that Dario uses as super powerful AI.",
    "start": "4110395",
    "end": "4117066"
  },
  {
    "text": "So, I agree with you on the AGI. I think we already have something like that's exceptionally impressive",
    "start": "4117066",
    "end": "4122437"
  },
  {
    "text": "that Alan Turing would\nfor sure say is AGI. But he's referring more to\nsomething once in possession of,",
    "start": "4122437",
    "end": "4129401"
  },
  {
    "text": "than you would have a significant military and geopolitical advantage\nover other nations.",
    "start": "4129401",
    "end": "4134578"
  },
  {
    "text": "So, it's not just like you can\nask it how to cook an omelet. - And he has a much more positive view.",
    "start": "4134578",
    "end": "4140067"
  },
  {
    "text": "And as I say, machines of love and grace. - Yes.\n- I've read into this. I don't have enough background\nin physical sciences",
    "start": "4140067",
    "end": "4145220"
  },
  {
    "text": "to gauge exactly how confident I am in if AI can revolutionize biology. I am safe saying that AI is\ngoing to accelerate the progress",
    "start": "4145220",
    "end": "4154629"
  },
  {
    "text": "of any computational science. - So, we're doing a depth-first\nsearch here on topics, taking tangent of a tangent.",
    "start": "4154629",
    "end": "4160009"
  },
  {
    "start": "4156000",
    "end": "4721000"
  },
  {
    "text": "So, let's continue on\nthat depth-first search. You said that you're both feeling the AGI,",
    "start": "4160010",
    "end": "4168156"
  },
  {
    "text": "so what's your timeline?\n(Nathan chuckles) Dario's 2026 for the super powerful AI",
    "start": "4168157",
    "end": "4173892"
  },
  {
    "text": "that's basically agentic to a degree, where it's a real security\nthreat, that level of AGI.",
    "start": "4175111",
    "end": "4181690"
  },
  {
    "text": "What's your timeline? - I don't like to attribute\nspecific abilities because predicting specific\nabilities and when is very hard.",
    "start": "4183464",
    "end": "4189690"
  },
  {
    "text": "I think mostly if you're gonna say that I'm feeling the AGI is that I expect continued\nrapid surprising progress",
    "start": "4189690",
    "end": "4196131"
  },
  {
    "text": "over the next few years. So, something like R1 is less surprising to me from DeepSeek,",
    "start": "4196131",
    "end": "4201518"
  },
  {
    "text": "because I expect there to be new paradigms where substantial progress can be made. I think DeepSeek-R1 is so unsettling,",
    "start": "4201518",
    "end": "4207578"
  },
  {
    "text": "because we're on this path with ChatGPT, it's like it's getting better, it's getting better, it's getting better. And then, we have a new direction\nfor changing the models.",
    "start": "4207578",
    "end": "4215135"
  },
  {
    "text": "And we took one step like\nthis and we took a step up. So, it looks like a really fast slope. And then, we're gonna\njust take more steps.",
    "start": "4215136",
    "end": "4220915"
  },
  {
    "text": "So, this is really unsettling when you have these big steps. And I expect that to keep happening.",
    "start": "4220915",
    "end": "4226836"
  },
  {
    "text": "I've tried OpenAI Operator, I've tried Claude computer\nuse, they're not there yet.",
    "start": "4226836",
    "end": "4232135"
  },
  {
    "text": "I understand the idea, but it's just so hard to\npredict what is the breakthrough that'll make something like that work.",
    "start": "4232136",
    "end": "4237986"
  },
  {
    "text": "And I think it's more likely that we have breakthroughs that work and things that we don't\nknow what they're gonna do. So, everyone wants agents.",
    "start": "4237986",
    "end": "4244442"
  },
  {
    "text": "Dario has a very eloquent\nway of describing this. And I just think that it's like,",
    "start": "4244442",
    "end": "4250153"
  },
  {
    "text": "there's gonna be more than that. So, just expect these things to come. - I'm gonna have to try to pin you down",
    "start": "4250153",
    "end": "4255370"
  },
  {
    "text": "to a date on the AGI timeline, (chuckles) like the nuclear weapon moment.",
    "start": "4255371",
    "end": "4261813"
  },
  {
    "text": "So, moment where on\nthe geopolitical stage.",
    "start": "4261813",
    "end": "4265396"
  },
  {
    "text": "There's a real, like, 'cause we're talking\nabout export controls, when do you think just\neven to throw out a date,",
    "start": "4267026",
    "end": "4274773"
  },
  {
    "text": "when do you think that would be? For me, it's probably after 2030, so I'm not as-\n- That's what I would say.",
    "start": "4274773",
    "end": "4280502"
  },
  {
    "text": "- So, define that. Because to me, it almost\nhas already happened. You look at elections\nin India and Pakistan,",
    "start": "4280502",
    "end": "4286672"
  },
  {
    "text": "people get AI voice calls, and think they're talking\nto the politician. The AI diffusion rules,",
    "start": "4286672",
    "end": "4292444"
  },
  {
    "text": "which was enacted in the last couple weeks of the Biden admin and looks\nlike the Trump admin will keep, and potentially even strengthen,",
    "start": "4292444",
    "end": "4298181"
  },
  {
    "text": "limit cloud computing and GPU sales to countries that are not even\nrelated to China, it's like.",
    "start": "4298181",
    "end": "4304160"
  },
  {
    "text": "This is-\n- Portugal and all these like normal countries are on the-\n- Yeah, and it's like- - You need approval from the US list.",
    "start": "4304160",
    "end": "4309521"
  },
  {
    "text": "- Like, yeah, Portugal and like all these\ncountries that are allies. - Yup.\n- Singapore. They freaking have F-35s and\nwe don't let them buy GPUs.",
    "start": "4309521",
    "end": "4317758"
  },
  {
    "text": "- Mm-hmm.\n- This to me is already to the scale of like- - Well, that just means\nthat the US military",
    "start": "4317758",
    "end": "4325119"
  },
  {
    "text": "is really nervous about\nthis new technology. That doesn't mean the\ntechnology is already there. So, they might be just very cautious",
    "start": "4325119",
    "end": "4332360"
  },
  {
    "text": "about this thing that they\ndon't quite understand. But that's a really good\npoint, the robocalls,",
    "start": "4332360",
    "end": "4338430"
  },
  {
    "text": "swarms of semi-intelligent\nbots could be a weapon, could be doing a lot\nof social engineering.",
    "start": "4338430",
    "end": "4345662"
  },
  {
    "text": "- There's tons of talk about, from the 2016 elections\nlike Cambridge Analytica, and all this stuff, Russian influence.",
    "start": "4345662",
    "end": "4352048"
  },
  {
    "text": "Every country in the world is pushing stuff onto the internet and has narratives they want. Every technically competent,",
    "start": "4352048",
    "end": "4358496"
  },
  {
    "text": "whether it's Russia, China,\nUS, Israel, et cetera. People are pushing viewpoints\nonto the internet, and mass.",
    "start": "4358497",
    "end": "4364819"
  },
  {
    "text": "And language models crash the cost of very\nintelligent sounding language.\n- There's some research that shows that the distribution",
    "start": "4364819",
    "end": "4371509"
  },
  {
    "text": "is actually a limiting factor. So, language models haven't\nyet made misinformation, particularly changed the equation there.",
    "start": "4371509",
    "end": "4379672"
  },
  {
    "text": "The internet is still ongoing. I think there's a blog, AI Snake Oil, and some of my friends at Princeton that write on this stuff.",
    "start": "4380801",
    "end": "4385969"
  },
  {
    "text": "So, there is research. It's a default that everyone assumes. And I would've thought the same thing is that misinformation",
    "start": "4385969",
    "end": "4391479"
  },
  {
    "text": "doesn't gonna get far\nworse with language models. I think in terms of internet posts and things that people\nhave been measuring,",
    "start": "4391479",
    "end": "4397588"
  },
  {
    "text": "it hasn't been a exponential increase or something extremely measurable, and things you're talking about with voice calls and stuff like that.",
    "start": "4397588",
    "end": "4404209"
  },
  {
    "text": "It could be in modalities\nthat are harder to measure. So, it's something that it's\ntoo soon to tell in terms of,",
    "start": "4404209",
    "end": "4411407"
  },
  {
    "text": "I think that's political\ninstability via the web is very,",
    "start": "4411408",
    "end": "4416408"
  },
  {
    "text": "it's monitored by a lot of researchers to see what's happening. You're asking about the AGI thing.",
    "start": "4416742",
    "end": "4423667"
  },
  {
    "text": "If you make me give a year, I'm be like, okay, I\nhave AI CEOs saying this. They've been saying two years for a while.\n- Mm-hmm.",
    "start": "4424877",
    "end": "4430479"
  },
  {
    "text": "- I think that they're people\nlike Dario at Anthropic, the CEO has thought about this so deeply.",
    "start": "4430479",
    "end": "4437098"
  },
  {
    "text": "I need to take their word seriously, but also understand that they\nhave different incentives.",
    "start": "4437098",
    "end": "4443579"
  },
  {
    "text": "So, I would be like,\nadd a few years to that, which is how you get\nsomething similar to 2030 or a little after 2030. - I think to some extent,\nwe have capabilities",
    "start": "4443579",
    "end": "4450367"
  },
  {
    "text": "that hit a certain point where\nany one person could say, oh, okay, if I can\nleverage those capabilities",
    "start": "4450367",
    "end": "4455826"
  },
  {
    "text": "for X amount of time, this is AGI. Call it '27, '28. But then the cost of actually\noperating that capability",
    "start": "4455826",
    "end": "4463134"
  },
  {
    "text": "- Yeah, this is gonna be\n- is - my point. (chuckles)\n- so, so extreme that no one can actually\ndeploy it at scale",
    "start": "4463134",
    "end": "4468402"
  },
  {
    "text": "and mass to actually completely\nrevolutionize the economy on a snap of a finger. So, I don't think it will be like",
    "start": "4468402",
    "end": "4474213"
  },
  {
    "text": "a snap of the finger moment.\n- Yeah, it's a physical constraint type-\n- Rather it'll be a, oh, the capabilities are here,",
    "start": "4474213",
    "end": "4479751"
  },
  {
    "text": "but I can't deploy it everywhere. And so, one simple\nexample going back to 2023",
    "start": "4479751",
    "end": "4485083"
  },
  {
    "text": "was when being with GPT-4 came out and everyone was freaking\nout about Search.",
    "start": "4485083",
    "end": "4490812"
  },
  {
    "text": "- Oh gosh.\n- Perplexity came out. If you did the cost on like, hey, implementing GPT-3 into\nevery Google search, it was like, oh, okay,",
    "start": "4490812",
    "end": "4496527"
  },
  {
    "text": "this is just like physically\nimpossible to implement. And as we step forward to going back to the test-time\ncompute thing, a query for...",
    "start": "4496527",
    "end": "4505052"
  },
  {
    "text": "You asked ChatGPT a\nquestion, it cost cents for their most capable model\nof chat to get a query back.",
    "start": "4505052",
    "end": "4512041"
  },
  {
    "text": "To solve an ARC-AGI problem\nthough, cost 5 to 20 bucks.",
    "start": "4512041",
    "end": "4517041"
  },
  {
    "text": "And this is an a- - [Nathan] It's only going up from there. - This is 1,000, 10,000x\nfactor difference in cost",
    "start": "4517103",
    "end": "4522760"
  },
  {
    "text": "to respond to a query versus do a task. And the task of ARC-AGI, it's not like,",
    "start": "4522761",
    "end": "4528639"
  },
  {
    "text": "it's simple to some extent, but it's also like what are\nthe tasks that we want A...",
    "start": "4528639",
    "end": "4534695"
  },
  {
    "text": "Okay, AGI, quote, unquote, \"what we have today\" can do ARC-AGI. Three years from now, it can do much more complicated problems,",
    "start": "4534695",
    "end": "4540076"
  },
  {
    "text": "but the cost is gonna be measured\nin thousands and thousands and hundreds of thousands\nof dollars of GPU time,",
    "start": "4540076",
    "end": "4545756"
  },
  {
    "text": "and there just won't be enough power, GPUs, infrastructure\n- Yeah. - to operate this, and therefore, shift everything in the\nworld on the snap the finger.",
    "start": "4545756",
    "end": "4552243"
  },
  {
    "text": "But at that moment, who gets to control and\npoint the AGI at a task.",
    "start": "4552243",
    "end": "4557654"
  },
  {
    "text": "And so, this was in Dario's\npost that he's like, hey, China can effectively and more quickly than us, point\ntheir AGI at military tasks.",
    "start": "4558745",
    "end": "4567005"
  },
  {
    "text": "And they have been in many ways, faster at adopting\ncertain new technologies into their military, especially\nwith regards to drones.",
    "start": "4567005",
    "end": "4575077"
  },
  {
    "text": "The US maybe has a long-standing large air sort of fighter\njet type of thing, bombers,",
    "start": "4575077",
    "end": "4581611"
  },
  {
    "text": "but when it comes to\nasymmetric arms such as drones, they've completely leapfrogged\nthe US and the west.",
    "start": "4581611",
    "end": "4588156"
  },
  {
    "text": "And the fear that Dario\nis pointing out there, I think, is that, yeah, great,",
    "start": "4588156",
    "end": "4593195"
  },
  {
    "text": "we'll have AGI in the commercial sector. The US military won't be able\nto implement it super fast. Chinese military could",
    "start": "4593195",
    "end": "4599447"
  },
  {
    "text": "and they could direct all their resources to implementing it in the military, and therefore solving military logistics",
    "start": "4599447",
    "end": "4605728"
  },
  {
    "text": "or solving some other\naspect of disinformation for targeted certain set of people, so that they can flip\na country's politics,",
    "start": "4605728",
    "end": "4612087"
  },
  {
    "text": "or something like that, that\nis actually like catastrophic versus the US just wants to,",
    "start": "4612087",
    "end": "4617259"
  },
  {
    "text": "'cause it'll be more\ncapitalistic allocated just towards whatever\n- Mm. - is the highest return on income, which might be like building\nfactories better or whatever.",
    "start": "4617259",
    "end": "4624636"
  },
  {
    "text": "- So, everything I've seen, people's intuition seems\nto fail on robotics. So, you have this kind\nof general optimism.",
    "start": "4624636",
    "end": "4631977"
  },
  {
    "text": "I've seen this on self-driving cars. People think it's much\neasier problem than it is. Similar with drones.",
    "start": "4631978",
    "end": "4638555"
  },
  {
    "text": "Here, I understand it a little bit less, but I've just seen the\nreality of the war in Ukraine",
    "start": "4638555",
    "end": "4643797"
  },
  {
    "text": "and the usage of drones on both sides. And it seems that humans",
    "start": "4643797",
    "end": "4649195"
  },
  {
    "text": "still far outperform any\nfully autonomous systems.",
    "start": "4649195",
    "end": "4653362"
  },
  {
    "text": "AI is an assistant, but\nhumans drive FPV drones where the humans controlling most of it",
    "start": "4654199",
    "end": "4660570"
  },
  {
    "text": "just far, far, far outperforms AI systems. So, I think it's not obvious to me",
    "start": "4660570",
    "end": "4665635"
  },
  {
    "text": "that we're going to have swarms of autonomous robots anytime\nsoon in the military context.",
    "start": "4665635",
    "end": "4671561"
  },
  {
    "text": "Maybe the fastest I can imagine is 2030, which is why I said 2030\nfor the super powerful AI.",
    "start": "4671561",
    "end": "4678641"
  },
  {
    "text": "Whenever you have large-scale swarms of robots doing military actions,",
    "start": "4678641",
    "end": "4683726"
  },
  {
    "text": "that's when the world just\nstarts to look different to me. So, that's the thing I'm\nreally worried about.",
    "start": "4683726",
    "end": "4689225"
  },
  {
    "text": "But there could be cyber\nwar type of technologies that from social engineering,",
    "start": "4689226",
    "end": "4695937"
  },
  {
    "text": "to actually just swarms the\nrobots that find attack vectors in our code bases,",
    "start": "4695937",
    "end": "4701108"
  },
  {
    "text": "and shut down power\ngrids, that kind of stuff. And it could be one of those things like on any given weekend or something,",
    "start": "4701108",
    "end": "4709233"
  },
  {
    "text": "power goes out, nobody knows why, and the world changes forever. Just power going out for two days",
    "start": "4709233",
    "end": "4715329"
  },
  {
    "text": "in all of the United States, that will lead to murder to chaos.",
    "start": "4715329",
    "end": "4721203"
  },
  {
    "start": "4721000",
    "end": "5196000"
  },
  {
    "text": "But going back to export controls, do you see that as a useful way",
    "start": "4721203",
    "end": "4727934"
  },
  {
    "text": "to control the balance\nof power geopolitically",
    "start": "4728982",
    "end": "4732815"
  },
  {
    "text": "in the context of AI? - And I think going back to my viewpoint is if you believe we're in\nthis stage of economic growth",
    "start": "4734900",
    "end": "4742317"
  },
  {
    "text": "and change that we've been\nin for the last 20 years, the export controls are absolutely guaranteeing\nthat China will win long-term,",
    "start": "4742317",
    "end": "4751113"
  },
  {
    "text": "if you do not believe AI is going to make significant changes to society in the next\n10 years or 5 years.",
    "start": "4751113",
    "end": "4757921"
  },
  {
    "text": "Five-year timelines are\nwhat the more executives and such of AI companies and even big tech companies believe.",
    "start": "4757921",
    "end": "4763909"
  },
  {
    "text": "But even tenure timelines,\nit's reasonable. But once you get to, hey, these timelines are\nbelow that time period,",
    "start": "4763909",
    "end": "4772415"
  },
  {
    "text": "then the only way to\ncreate a sizable advantage or disadvantage for America",
    "start": "4773627",
    "end": "4779797"
  },
  {
    "text": "versus China is if you constrain compute. Because talent is not really\nsomething that's constraining.",
    "start": "4779797",
    "end": "4787921"
  },
  {
    "text": "China arguably has more talent, more STEM graduates, more programmers. The US can draw upon the\nworld's people, which it does.",
    "start": "4787921",
    "end": "4794658"
  },
  {
    "text": "There's tons of foreigners\nin the AI industry- - [Nathan] So many of these AI teams are all people without a US passport.",
    "start": "4794658",
    "end": "4801202"
  },
  {
    "text": "- Yeah.\n(Nathan laughing) Many of them are Chinese people who are moving\n- Yeah. - to North America, and that's great.",
    "start": "4801203",
    "end": "4807079"
  },
  {
    "text": "That's exactly what we want. But there's that talent is one aspect, but I don't think that's one",
    "start": "4807079",
    "end": "4812759"
  },
  {
    "text": "that is a measurable\nadvantage for the US or not. It truly is just whether or not compute.",
    "start": "4812759",
    "end": "4818274"
  },
  {
    "text": "Now, even on the compute side, when we look at chips versus data centers. China has the unprecedented ability",
    "start": "4818274",
    "end": "4825353"
  },
  {
    "text": "to build ridiculous\nsums of power clockwork. They're always building\nmore and more power.",
    "start": "4825353",
    "end": "4831568"
  },
  {
    "text": "They've got steel mills that individually are the size of the entire US industry.",
    "start": "4831568",
    "end": "4837718"
  },
  {
    "text": "And they've got aluminum mills that consume gigawatts\nand gigawatts of power. And when we talk about what's\nthe biggest data center,",
    "start": "4837718",
    "end": "4843911"
  },
  {
    "text": "OpenAI made this huge\nthing about Stargate, their announcement there, that's like once it's fully\nbuilt out in a few years,",
    "start": "4843912",
    "end": "4851576"
  },
  {
    "text": "it'll be two gigawatts of power. And this is still smaller than the largest industrial\nfacilities in China.",
    "start": "4851576",
    "end": "4858938"
  },
  {
    "text": "China, if they wanted to\nbuild the largest data center in the world, if they had\naccess to the chips, could.",
    "start": "4858938",
    "end": "4863970"
  },
  {
    "text": "So, it's just a question of when, not if. - So, their industrial capacity far exceeds the United States?",
    "start": "4863971",
    "end": "4870394"
  },
  {
    "text": "- [Dylan] Exactly. - To the the manufacture stuff. - Yeah.\n- So, long-term, they're going to be\nmanufacturing chips there.",
    "start": "4870394",
    "end": "4878510"
  },
  {
    "text": "- Chips are a little bit more specialized. I'm specifically referring\nto the data centers. Chips, fabs take huge amounts\nof power, don't get me wrong.",
    "start": "4878510",
    "end": "4885096"
  },
  {
    "text": "That's not necessarily\nthe gating factor there. The gating factor on how fast people can build the largest clusters\ntoday in the US is power.",
    "start": "4885096",
    "end": "4893723"
  },
  {
    "text": "Whether it's now it could\nbe power generation, power transmission, substations, and all these sorts of\ntransformers and all these things,",
    "start": "4893723",
    "end": "4901762"
  },
  {
    "text": "building the data center,\nthese are all constraints on the US industry's\nability to build larger",
    "start": "4901762",
    "end": "4907645"
  },
  {
    "text": "and larger training systems, as well as deploying more\nand more inference compute. - I think we need to make the\npoint clear on why the time",
    "start": "4907645",
    "end": "4914834"
  },
  {
    "text": "is now for people that\ndon't think about this, 'cause essentially with export controls, you're making it so China cannot make or get cutting edge chips.",
    "start": "4914834",
    "end": "4922787"
  },
  {
    "text": "And the idea is that\nif you time this wrong, China is pouring a ton of money\ninto their chip production.",
    "start": "4922787",
    "end": "4929271"
  },
  {
    "text": "And if you time it wrong, they're going to have more\ncapacity for production, more capacity for energy, and figure out how to make the chips",
    "start": "4929271",
    "end": "4935352"
  },
  {
    "text": "and have more capacity than the rest of the\nworld to make the chips, because everybody can buy, they're gonna sell their\nChinese chips to everybody.",
    "start": "4935352",
    "end": "4941599"
  },
  {
    "text": "They might subsidize them. And therefore, if AI takes a long time to become differentiated,",
    "start": "4941599",
    "end": "4946643"
  },
  {
    "text": "we've kneecapped the financial performance of American companies. Nvidia can sell less.\nTSMC cannot sell to China.",
    "start": "4946643",
    "end": "4953521"
  },
  {
    "text": "So, therefore, we have less demand to therefore to keep driving\nthe production cycle.",
    "start": "4953521",
    "end": "4960104"
  },
  {
    "text": "So, that's the assumption\nbehind the timing being important.\n- Less than 10 years or 5 years to above.",
    "start": "4960104",
    "end": "4965793"
  },
  {
    "text": "China will win because of\nthese restrictions long-term, unless AI does something\nin the short-term,",
    "start": "4965793",
    "end": "4971751"
  },
  {
    "text": "which I believe AI will do, make massive changes to society\nin the medium short-term.",
    "start": "4971751",
    "end": "4977908"
  },
  {
    "text": "And so, that's the big unlocker there. And even today, if Xi Jinping decided to get,",
    "start": "4977908",
    "end": "4984120"
  },
  {
    "text": "quote, unquote, \"scale pilled\", i.e, decide that scaling\nlaws are what matters,",
    "start": "4984120",
    "end": "4990286"
  },
  {
    "text": "just like the US executives\nlike Satya Nadella and Mark Zuckerberg, and Sundar,",
    "start": "4990286",
    "end": "4995886"
  },
  {
    "text": "and all these US\nexecutives of the biggest, most powerful tech companies, have decided they're scale pilled",
    "start": "4995886",
    "end": "5001320"
  },
  {
    "text": "and they're building\nmulti-gigawatt data centers. Whether it's in Texas or\nLouisiana, or Wisconsin, wherever it is, they're\nbuilding these massive things",
    "start": "5001320",
    "end": "5008869"
  },
  {
    "text": "that cost as much as their entire budget for spending on data centers\nglobally in one spot.",
    "start": "5008869",
    "end": "5014607"
  },
  {
    "text": "This is what they've committed to for next year, year after, et cetera. And so, they're so convinced",
    "start": "5014607",
    "end": "5021019"
  },
  {
    "text": "that this is the way that\nthis is what they're doing. But if China decided to, they\ncould do it faster than us.",
    "start": "5021019",
    "end": "5026526"
  },
  {
    "text": "But this is where the\nrestrictions come in. It is not clear that China\nas a whole has decided,",
    "start": "5026526",
    "end": "5032558"
  },
  {
    "text": "from the highest levels\nthat this is a priority. The US has. You see Trump talking about DeepSeek",
    "start": "5032558",
    "end": "5037938"
  },
  {
    "text": "and Stargate within the same week. And the Biden admin as well had a lot of discussions\nabout AI and such.",
    "start": "5037938",
    "end": "5045127"
  },
  {
    "text": "It's clear that they think about it. Only just last week did DeepSeek meet the\nsecond in command of China.",
    "start": "5045127",
    "end": "5052148"
  },
  {
    "text": "They have not even met the top, and haven't met Xi, she hasn't set down. And they only just released a subsidy",
    "start": "5052148",
    "end": "5058918"
  },
  {
    "text": "of a trillion RMB, roughly $160 billion, which is closer to the spending\nof like Microsoft and Meta,",
    "start": "5058918",
    "end": "5066957"
  },
  {
    "text": "and Google combined for this year. So, it's like they're\nrealizing it just now.",
    "start": "5066957",
    "end": "5072225"
  },
  {
    "text": "But that's where these\nexport restrictions come in and say, hey, you can't ship the most powerful US chips to China.",
    "start": "5072226",
    "end": "5078796"
  },
  {
    "text": "You can ship a cut down version. You can't ship the most powerful chips",
    "start": "5078796",
    "end": "5084058"
  },
  {
    "text": "to all these countries who we know are just\ngonna rent it to China. You have to limit the numbers. - [Nathan] And the tools.",
    "start": "5084058",
    "end": "5089363"
  },
  {
    "text": "Same-\n- And same with manufacturing equipment tools, all these different aspects,\nbut it all stems from AI,",
    "start": "5089363",
    "end": "5095238"
  },
  {
    "text": "and then what downstream\ncan slow them down in AI. And so, the entire\nsemiconductor restrictions,",
    "start": "5095238",
    "end": "5100958"
  },
  {
    "text": "you read them, they're very clear, it's about AI and military\ncivil fusion of technology.",
    "start": "5100958",
    "end": "5106538"
  },
  {
    "text": "It's very clear. And then, from there, it goes, oh, well, we're banning them\nfrom buying lithography tools and etch tools, and deposition tools.",
    "start": "5106538",
    "end": "5113114"
  },
  {
    "text": "And, oh, this random subsystem from a random company that's tiny. Why are we banning this?",
    "start": "5113114",
    "end": "5118536"
  },
  {
    "text": "Because all of it, the US government has decided,\nis critical to AI systems.",
    "start": "5118536",
    "end": "5123644"
  },
  {
    "text": "- I think the fulcrum\npoint is the transition from seven nanometer to\nfive nanometer chips,",
    "start": "5123644",
    "end": "5128652"
  },
  {
    "text": "where I think it was Huawei that had the seven nanometer\nchip a few years ago, which caused another political brouhaha",
    "start": "5128652",
    "end": "5135506"
  },
  {
    "text": "almost like this moment. And then, it's the ASML\ndeep UV. What is that?",
    "start": "5135506",
    "end": "5140714"
  },
  {
    "text": "Like extreme ultraviolet lithography. - To set context on the chips, what Nathan's referring to is in 2020,",
    "start": "5140714",
    "end": "5146733"
  },
  {
    "text": "Huawei released their Ascend 910 chip, which was an AI chip, first\none on seven nanometer",
    "start": "5146733",
    "end": "5153174"
  },
  {
    "text": "before Google did, before Nvidia did. And they submitted it\nto the MLPerf benchmark, which is a industry standard",
    "start": "5153174",
    "end": "5159487"
  },
  {
    "text": "for machine learning\nperformance benchmark. And it did quite well. And it was the best\nchip at the submission.",
    "start": "5159487",
    "end": "5165175"
  },
  {
    "text": "This was a huge deal. The Trump admin of course\nbanned, it was 2019,",
    "start": "5165175",
    "end": "5171570"
  },
  {
    "text": "banned the Huawei from getting seven\nnanometer chips from TSMC. And so, then they had to switch to using internal\ndomestically produced chips,",
    "start": "5171570",
    "end": "5178794"
  },
  {
    "text": "which was a multi-year setback. - Many companies have done\nseven nanometer chips. And the question is like, we\ndon't know how much Huawei",
    "start": "5178794",
    "end": "5185108"
  },
  {
    "text": "was subsidizing production of that chip. Intel has made seven nanometer chips that are not profitable,\nand things like this.",
    "start": "5185108",
    "end": "5191761"
  },
  {
    "text": "So, this is how it all feeds back into the economic engine\nof export controls. - Well, so you're saying\nthat for now, Xi Jinping",
    "start": "5191762",
    "end": "5199346"
  },
  {
    "start": "5196000",
    "end": "5465000"
  },
  {
    "text": "has not felt the AGI, but it feels like the DeepSeek moment - Yeah.\n- might like,",
    "start": "5199346",
    "end": "5204621"
  },
  {
    "text": "there might be meetings going on now, where he's gonna start\nwearing the same T-shirt and things are gonna escalate.\n(Nathan and Dylan laughing)",
    "start": "5205841",
    "end": "5213731"
  },
  {
    "text": "- He may have woken up last week. Liang Feng met the second command guy,",
    "start": "5213731",
    "end": "5219301"
  },
  {
    "text": "and they had a meeting. And then, the next day, they\nannounced the AI subsidies, which are a trillion RMB.",
    "start": "5219301",
    "end": "5225312"
  },
  {
    "text": "- So, it's possible that\nthis DeepSeek moment is truly the beginning of a cold war.",
    "start": "5225312",
    "end": "5230817"
  },
  {
    "text": "- That's what a lot of\npeople are worried about. People in AI have been worried that this is going towards a cold war or already is.\n- But there is,",
    "start": "5230817",
    "end": "5236831"
  },
  {
    "text": "it's not DeepSeek's fault,\nbut there's something, a bunch of factors came together where it was like explosion.\n- No, history works.",
    "start": "5236831",
    "end": "5243336"
  },
  {
    "text": "- It all has to do with Nvidia\nstock going down problem. But it's just some\n(Nathan laughing) mass hysteria\n(Lex drowns out Nathan)",
    "start": "5243336",
    "end": "5249498"
  },
  {
    "text": "that happened that eventually led to Xi Jinping having meetings\nand waking up to this idea.",
    "start": "5249498",
    "end": "5254875"
  },
  {
    "text": "- And the US government\nrealized in October 7th, 2022, before ChatGPT released,",
    "start": "5254875",
    "end": "5260477"
  },
  {
    "text": "that restriction on October 7th, which dropped and shocked everyone. And it was very clearly aimed at AI.",
    "start": "5260477",
    "end": "5266028"
  },
  {
    "text": "Everyone was like, what\nthe heck are you doing? - And diffusion was out then, but not ChatGPT.\n- Yeah, but not ChatGPT.",
    "start": "5266028",
    "end": "5271493"
  },
  {
    "text": "- So, it was like starting\nto be rumblings like- - Of what gen AI can do to society, but it was very clear, I think,",
    "start": "5271493",
    "end": "5277236"
  },
  {
    "text": "to at least National Security Council and those sort of folks that this was where the world is headed,",
    "start": "5277236",
    "end": "5283173"
  },
  {
    "text": "this cold war that's happening. - So, is there any concerns\nthat the export controls",
    "start": "5283173",
    "end": "5288929"
  },
  {
    "text": "push China to take\nmilitary action on Taiwan?",
    "start": "5290379",
    "end": "5294129"
  },
  {
    "text": "- This is the big risk. The further you push China\naway from having access to cutting edge American\nand global technologies,",
    "start": "5295597",
    "end": "5302906"
  },
  {
    "text": "the more likely they are to say, well, 'cause I can't access\nit, I might as well... No one should access it.",
    "start": "5302907",
    "end": "5308767"
  },
  {
    "text": "And there's a few\ninteresting aspects of that. China has a urban rural\ndivide like no other.",
    "start": "5308767",
    "end": "5315806"
  },
  {
    "text": "They have a male/female\nbirth ratio like no other, to the point where, if\nyou look in most of China,",
    "start": "5316831",
    "end": "5322473"
  },
  {
    "text": "it's like the ratio's not that bad. But when you look at single\ndudes in rural China, it's like a 30 to 1 ratio.\n- Mm-hmm. - And those are disenfranchised dudes.",
    "start": "5322473",
    "end": "5330217"
  },
  {
    "text": "Quote, unquote, the US\nhas an \"incel problem\" like China does too. It's just they're placated\nin some way or crushed down.",
    "start": "5330217",
    "end": "5336583"
  },
  {
    "text": "What do you do with these people? And at the same time, you're not allowed to access the most important technology, at least the US thinks so.",
    "start": "5336583",
    "end": "5343034"
  },
  {
    "text": "China's maybe starting to think this is the most important technology by starting to dump subsidies in it. They thought EVs and renewables",
    "start": "5343034",
    "end": "5348778"
  },
  {
    "text": "were the most important technology. They dominate that now. Now, they started thinking\nabout semiconductors",
    "start": "5348778",
    "end": "5354202"
  },
  {
    "text": "in the late 2010s and early 2020s, and now they've been dumping money and they're catching up rapidly,",
    "start": "5354202",
    "end": "5361134"
  },
  {
    "text": "and they're gonna do the same with AI. Because they're very talented. So, the question is like,",
    "start": "5361134",
    "end": "5366467"
  },
  {
    "text": "when does this hit a breaking point? And if China sees this as,\nhey, they can continue...",
    "start": "5367483",
    "end": "5376384"
  },
  {
    "text": "If not having access and\nstarting a true hot war. Taking over Taiwan or trying\nto subvert its democracy",
    "start": "5376384",
    "end": "5382458"
  },
  {
    "text": "in some way, or blockading it, hurts the rest of the world\nfar more than it hurts them,",
    "start": "5382458",
    "end": "5388233"
  },
  {
    "text": "this is something they\ncould potentially do. And so, is this pushing them\ntowards that? Potentially.",
    "start": "5388233",
    "end": "5394299"
  },
  {
    "text": "I'm not quite a geopolitical person, but it's obvious that\nthe world regime of peace",
    "start": "5394299",
    "end": "5400630"
  },
  {
    "text": "and trade is super awesome for economics. But at some point, it could break.",
    "start": "5400630",
    "end": "5407044"
  },
  {
    "text": "- I think we should\ncomment why Chinese economy would be hurt by that is\nthat they're export-heavy. I think the United States buys so much,",
    "start": "5407044",
    "end": "5414047"
  },
  {
    "text": "like if that goes away,\nthat's how their economy goes. - Well, also, they just would not be able to import raw materials\nfrom all over the world.",
    "start": "5414047",
    "end": "5421772"
  },
  {
    "text": "The US would just shut\ndown the strait of Malacca and at the same time, the US entire, you could argue almost all\nthe GDP growth in America",
    "start": "5421772",
    "end": "5429385"
  },
  {
    "text": "since the '70s has been either\npopulation growth or tech.",
    "start": "5429385",
    "end": "5434135"
  },
  {
    "text": "Because your life today\nis not that much better than someone from the\n'80s outside of tech.",
    "start": "5435499",
    "end": "5442941"
  },
  {
    "text": "Cars, they all have\nsemiconductors in them everywhere. Fridges, semiconductors everywhere. These funny stories about how Russians",
    "start": "5442941",
    "end": "5448000"
  },
  {
    "text": "were taking apart laundry machines because they had certain\nTexas instrument chips that they could then repurpose",
    "start": "5448000",
    "end": "5453497"
  },
  {
    "text": "and put into their\nanti-missile missile things, their S-400 or whatever.",
    "start": "5453497",
    "end": "5459060"
  },
  {
    "text": "You would know more about this, but there's all sorts of, everything about semiconductors is so integral to every part of our lives.",
    "start": "5459060",
    "end": "5466158"
  },
  {
    "start": "5465000",
    "end": "6884000"
  },
  {
    "text": "- So, can you explain the role of TSMC in this story of\nsemiconductors and maybe also",
    "start": "5466158",
    "end": "5473408"
  },
  {
    "text": "how the United States can\nbreak the reliance on TSMC? - I don't think it's necessarily\nbreaking their alliance.",
    "start": "5473408",
    "end": "5479637"
  },
  {
    "text": "I think it's getting\nTSMC to build in the US. So, taking a step back.",
    "start": "5479637",
    "end": "5486405"
  },
  {
    "text": "TSMC produces most of the world's chips, especially on the foundry side.",
    "start": "5486405",
    "end": "5493206"
  },
  {
    "text": "There's a lot of companies that build their own\nchips, Samsung, Intel, STMicro, Texas Instruments,\nAnalog Devices,",
    "start": "5493206",
    "end": "5501728"
  },
  {
    "text": "all these kinds of companies\nbuild their own chips and XP. But more and more of these companies are outsourcing to TSMC and\nhave been for multiple decades.",
    "start": "5501728",
    "end": "5509822"
  },
  {
    "text": "- Can you explain the supply chain there and where most of TSMC is\nin terms of manufacturing?",
    "start": "5509822",
    "end": "5515230"
  },
  {
    "text": "- Sure. So, historically, supply chain was, companies would build their own chips. It'd be a company-started,\nthey'd build their own chips,",
    "start": "5515230",
    "end": "5522329"
  },
  {
    "text": "and then they'd design the chip and build the chip, and sell it. Over time, this became really difficult",
    "start": "5522329",
    "end": "5528320"
  },
  {
    "text": "because the cost of building a fab continues to compound\nevery single generation. Of course the technology,\nfiguring out the technology for it",
    "start": "5528320",
    "end": "5534389"
  },
  {
    "text": "is incredibly difficult regardless, but just the dollars and cents\nthat are required ignoring,",
    "start": "5534389",
    "end": "5539548"
  },
  {
    "text": "saying, hey, yes, I have all\nthe technical capability, which it's really hard\nto get that, by the way. Intel's failing, Samsung's\nfailing, et cetera.",
    "start": "5539548",
    "end": "5545970"
  },
  {
    "text": "But if you look at just\nthe dollars to spend to build that next generation\nfab, it keeps growing. Sort of like Moore's Laws",
    "start": "5545970",
    "end": "5552314"
  },
  {
    "text": "having the cost of chips every two years. There's a separate law that's doubling the cost of\nfabs every handful of years.",
    "start": "5552314",
    "end": "5558125"
  },
  {
    "text": "And so, you look at a leading edge fab that is gonna be profitable today, that's building three nanometer chips",
    "start": "5558125",
    "end": "5563427"
  },
  {
    "text": "or two nanometer chips in the future, that's gonna cost north\nof 30, $40 billion.",
    "start": "5563427",
    "end": "5568787"
  },
  {
    "text": "And that's just for like a token amount. That's like the base building blocking, you probably need to build multiple.",
    "start": "5568788",
    "end": "5574197"
  },
  {
    "text": "And so, when you look at\nthe industry, over the last, if I go back 20, 30\nyears ago, there were 20,",
    "start": "5574197",
    "end": "5580404"
  },
  {
    "text": "30 companies that could build\nthe most advanced chips, and then they would design\nthem themselves and sell them. So, companies like AMD\nwould build their own chips.",
    "start": "5580404",
    "end": "5586999"
  },
  {
    "text": "Intel of course still builds their own chips\nthey're very famous for. IBM would build their own chips. And you could just keep\ngoing down the list.",
    "start": "5586999",
    "end": "5592752"
  },
  {
    "text": "All these companies built their own chips. Slowly, they kept falling like flies. And that's because of what TSMC did.",
    "start": "5592752",
    "end": "5598963"
  },
  {
    "text": "They created the foundry business model, which is, I'm not gonna design any chips, I'm just gonna contract\nmanufacturer chips for other people.",
    "start": "5598963",
    "end": "5606889"
  },
  {
    "text": "And one of their early\ncustomers is Nvidia. Nvidia is the only semiconductor company",
    "start": "5606890",
    "end": "5612005"
  },
  {
    "text": "that's doing more than a\nbillion dollars of revenue that was started in the era of Foundry.",
    "start": "5614140",
    "end": "5619173"
  },
  {
    "text": "Every other company started before then, and at some point had fabs,\nwhich is actually incredible. Like AMD and Intel and Broadcom-",
    "start": "5619173",
    "end": "5626706"
  },
  {
    "text": "- [Nathan] Such a great fact.\n(Nathan drowns out Dylan) - It's like everyone\n(Nathan laughing) had fabs at some point, or some companies like Broadcom,",
    "start": "5626706",
    "end": "5631810"
  },
  {
    "text": "it was like a merger amalgamation of various companies that rolled up. But even today, Broadcom has fabs.",
    "start": "5631810",
    "end": "5636840"
  },
  {
    "text": "They build iPhone, RF radio\nchips in Colorado for Apple.",
    "start": "5636840",
    "end": "5641507"
  },
  {
    "text": "All these companies had fabs,\nand for most of the fabs, they threw them away or sold them off, or they got rolled into something else.",
    "start": "5643400",
    "end": "5649666"
  },
  {
    "text": "And now, everyone relies on TSMC. Including Intel, their latest\nPC chip uses TSMC chips.",
    "start": "5649666",
    "end": "5656235"
  },
  {
    "text": "It also uses some intel chips,\nbut it uses TSMC process. - Can you explain why the foundry model is so successful for these companies?",
    "start": "5656235",
    "end": "5663403"
  },
  {
    "text": "Why are they going with TSMC-\n- Economies of scale. - Scale.\n- Yeah. So, like I mentioned, the cost\nof building a fab is so high.",
    "start": "5663403",
    "end": "5670207"
  },
  {
    "text": "The R&D is so difficult. And when you look at companies",
    "start": "5670207",
    "end": "5675417"
  },
  {
    "text": "that had their own vertical stack, there was an antiquated\nprocess of like, okay, I'm so hyper customized\nto each specific chip.",
    "start": "5675417",
    "end": "5682230"
  },
  {
    "text": "But as we've gone through the history of the last 50 years of\nelectronics and semiconductors,",
    "start": "5682230",
    "end": "5687463"
  },
  {
    "text": "A, you need more and more specialization, because Moore's Law has died,\nDennard scaling has died.",
    "start": "5687463",
    "end": "5693037"
  },
  {
    "text": "i.e, chips are not getting\nbetter just for free. From manufacturing, you have to make real\narchitectural innovations.",
    "start": "5693037",
    "end": "5699375"
  },
  {
    "text": "Google is not just running on\nIntel CPUs for web serving. They have a YouTube chip, they have TPUs, they have pixel chips, they\nhave a wide diversity of chips",
    "start": "5699375",
    "end": "5706715"
  },
  {
    "text": "that generate all the\neconomic value of Google. It's running all the services and stuff.",
    "start": "5706715",
    "end": "5712912"
  },
  {
    "text": "And so, and this is just Google and you could go across any\ncompany in the industry, and it's like this. Cars contain 5,000 chips, 200\ndifferent varieties of them.",
    "start": "5712912",
    "end": "5720917"
  },
  {
    "text": "All these random things. A Tesla door handle has\ntwo chips. It's ridiculous. And it's a cool door handle.",
    "start": "5720917",
    "end": "5726179"
  },
  {
    "text": "You don't think about it, but it's like, has two really chipped, like penny chips in there.",
    "start": "5726179",
    "end": "5731739"
  },
  {
    "text": "Anyways, so as you have\nmore diversity of chips, as you have more specialization required, and the cost of fabs continues\nto grow, you need someone",
    "start": "5731740",
    "end": "5739219"
  },
  {
    "text": "who is laser-focused on building\nthe best process technology and making it as flexible as possible.",
    "start": "5739219",
    "end": "5745661"
  },
  {
    "text": "- I think you could say it simpler, which is the cost per fab goes up. And if you are a small player,",
    "start": "5745661",
    "end": "5751587"
  },
  {
    "text": "that makes a few types of chips. You're not gonna have the demand to pay back the cost of the fab.",
    "start": "5751587",
    "end": "5756698"
  },
  {
    "text": "Whereas Nvidia can have\nmany different customers and aggregate all this\ndemand into one place,",
    "start": "5756699",
    "end": "5761908"
  },
  {
    "text": "and then they're the only\nperson that makes enough money building chips to buy the\nnext, to build the next fab.",
    "start": "5761909",
    "end": "5768306"
  },
  {
    "text": "So, this is why the\ncompanies slowly get killed, 'cause they have 10 years ago a chip",
    "start": "5768306",
    "end": "5774553"
  },
  {
    "text": "that is profitable and is good enough, but the cost to build\nthe next one goes up. They may try to do this, fail,",
    "start": "5774553",
    "end": "5780636"
  },
  {
    "text": "because they don't have\nthe money to make it work, and then they don't have any chips. Or they build it and it's too\nexpensive and they just have",
    "start": "5780636",
    "end": "5785640"
  },
  {
    "text": "not profitable chips.\n- Or there's more failure points of, you could\nhave one little process-related",
    "start": "5785640",
    "end": "5790757"
  },
  {
    "text": "to some sort of chemical etch\nor some sort of plasma etch, or some little process that screws up,",
    "start": "5790757",
    "end": "5797445"
  },
  {
    "text": "you didn't engineer it right, and now the whole company falls\napart, you can't make chips. And so, super, super powerful\ncompanies like Intel,",
    "start": "5797445",
    "end": "5804207"
  },
  {
    "text": "they had the weathering\nstorm to like, hey, they still exist today, even\nthough they really screwed up their manufacturing six, seven years ago.",
    "start": "5804207",
    "end": "5810895"
  },
  {
    "text": "But in the case of like AMD,\nthey almost went bankrupt. They had to sell their\nfabs to Mubadala, UAE.",
    "start": "5810895",
    "end": "5817283"
  },
  {
    "text": "And that became a separate company called GlobalFoundries,\nwhich is a foundry firm. And then, AMD was able to then focus",
    "start": "5817283",
    "end": "5823564"
  },
  {
    "text": "on the return back up was like, hey, let's focus on making chiplets and a bunch of different\nchips for different markets.",
    "start": "5823564",
    "end": "5829972"
  },
  {
    "text": "And focusing on specific workloads rather than all of these different things. And so, you get more diversity of chips,",
    "start": "5829972",
    "end": "5836024"
  },
  {
    "text": "you have more companies\nthan ever designing chips, but you have fewer companies\nthan ever manufacturing them.",
    "start": "5836024",
    "end": "5841571"
  },
  {
    "text": "And this is where TSMC comes in, is they've just been the best. They are so good at it.",
    "start": "5841571",
    "end": "5847524"
  },
  {
    "text": "They're customer-focused, they make it easy for you\nto fabricate your chips. They take all of that complexity and try and abstract a\nlot of it away from you.",
    "start": "5847524",
    "end": "5854875"
  },
  {
    "text": "They make good money, they\ndon't make insane money, but they make good money. And they're able to\naggregate all this demand",
    "start": "5854875",
    "end": "5861688"
  },
  {
    "text": "and continue to build the next fab, the next fab, the next fab. - So, why is Taiwan so special for TSMC?",
    "start": "5861688",
    "end": "5866938"
  },
  {
    "text": "Why is it happening there? Can it be replicated\ninside the United States? - Yeah, so there's aspects of it",
    "start": "5866938",
    "end": "5873264"
  },
  {
    "text": "that I would say yes and\naspects that I'd say no. TSMC is way ahead,",
    "start": "5873264",
    "end": "5877988"
  },
  {
    "text": "because Former Executive Morris Chang of Texas Instruments\nwasn't promoted to CEO,",
    "start": "5878888",
    "end": "5885031"
  },
  {
    "text": "and he is like, screw this, I'm gonna go make my own chip company. And he went to Taiwan and made TSMC. And there's a whole lot more story there.",
    "start": "5885032",
    "end": "5891949"
  },
  {
    "text": "So, it could have been Texas Instruments, it could have been TSMC, but Texas Semiconductor Manufacturing,",
    "start": "5891950",
    "end": "5897177"
  },
  {
    "text": "instead of Texas Instruments.\n(Nathan laughing) So, there is that whole story there, but the race-\n- Sitting here in Texas.",
    "start": "5897177",
    "end": "5903455"
  },
  {
    "text": "- And that sounds like a human story, like it didn't get promoted? - Just the brilliance of Morris Chang, which I wouldn't underplay,",
    "start": "5903455",
    "end": "5909730"
  },
  {
    "text": "but there's also a different\nlevel of how this works. So, in Taiwan, the number,",
    "start": "5909730",
    "end": "5915924"
  },
  {
    "text": "top percent of graduates of students that go to the best school, which is NTU, the top percent of those\nall go work to TSMC.",
    "start": "5918800",
    "end": "5925674"
  },
  {
    "text": "And guess what their pay is, their starting pay is\nlike $80,000, $70,000.",
    "start": "5925674",
    "end": "5930989"
  },
  {
    "text": "Which is like, that's like starting pay for a good graduate in the US. Not the top, the top\ngraduates are making hundreds",
    "start": "5930989",
    "end": "5936949"
  },
  {
    "text": "of thousands of dollars at\nthe Googles and the Amazons, and now I guess the OpenAIs of the world.",
    "start": "5936950",
    "end": "5942235"
  },
  {
    "text": "So, there is a large dichotomy of like, what is the top 1% of the society doing",
    "start": "5942235",
    "end": "5947378"
  },
  {
    "text": "and where are they headed\nbecause of economic reasons. Intel never paid that crazy good. And it didn't make sense to them.",
    "start": "5947378",
    "end": "5953227"
  },
  {
    "text": "That's one aspect, where's the best going? Second is the work ethic.",
    "start": "5953227",
    "end": "5958349"
  },
  {
    "text": "We like to work, you work\na lot, we work a lot, but at the end of the day,",
    "start": "5958350",
    "end": "5963525"
  },
  {
    "text": "what is the time and amount\nof work that you're doing and what does a fab require? Fabs are not work from home jobs. They are, you go into the\nfab and grueling work.",
    "start": "5965696",
    "end": "5973585"
  },
  {
    "text": "There's, hey, if there is\nany amount of vibration. An earthquake happens,\nvibrates the machines,",
    "start": "5973585",
    "end": "5981697"
  },
  {
    "text": "they're either broken, you've scrapped some of your production. And then, in many cases, they're\nlike calibrated properly.",
    "start": "5981697",
    "end": "5987065"
  },
  {
    "text": "So, when TSMC, when there's an earthquake, recently there's been an earthquake, TSMC doesn't call their employees,",
    "start": "5987065",
    "end": "5993266"
  },
  {
    "text": "they just go to the fab\nand they just show up. The parking lot gets slammed and people just go into\nthe fab and fix it.",
    "start": "5993266",
    "end": "6001145"
  },
  {
    "text": "It's like ants. It's like a hive of ants. It doesn't get told by\nthe queen what to do.",
    "start": "6001145",
    "end": "6006383"
  },
  {
    "text": "The ants just know. - It's like one person just\nspecializes on these one task, and it's like, you're\ngonna take this one tool",
    "start": "6006383",
    "end": "6012977"
  },
  {
    "text": "and you're the best person in the world, and this is what you're gonna do for your whole life is\nthis one task and the fab. - Which is like some special chemistry",
    "start": "6012977",
    "end": "6019016"
  },
  {
    "text": "plus nano manufacturing\non one line of tools that continues to get iterated. And yeah, it's like specific plasma etch",
    "start": "6019017",
    "end": "6026378"
  },
  {
    "text": "for removing silicon dioxide. That's all you focus on your whole career, and it's like such a specialized thing.",
    "start": "6026378",
    "end": "6031417"
  },
  {
    "text": "And so, it's not like the\ntasks are transferable. AI today is awesome, because people can pick it up like that.",
    "start": "6031417",
    "end": "6037265"
  },
  {
    "text": "Semiconductor manufacturing is very antiquated and difficult. None of the materials are online for people to\nread easily and learn.",
    "start": "6037266",
    "end": "6044797"
  },
  {
    "text": "The papers are very dense and it takes a lot of experience to learn. And so, it makes the barrier\nto entry much higher too.",
    "start": "6044797",
    "end": "6051897"
  },
  {
    "text": "So, when you talk about, hey, you have all these people\nthat are super specialized, they will work 80 hours a\nweek in a factory, in a fab.",
    "start": "6051897",
    "end": "6060856"
  },
  {
    "text": "And if anything goes wrong, they'll go show up in\nthe middle of the night because some earthquake, their wife is like,\n\"There's an earthquake.\"",
    "start": "6061106",
    "end": "6066821"
  },
  {
    "text": "He is like, \"Great, I'm gonna go to the fab.\"\n(Nathan laughing) - [Nathan] It's like a crime. - Would you as an American, do that?",
    "start": "6066821",
    "end": "6071912"
  },
  {
    "text": "It's like these sorts of things are like, what, I guess are the exemplifying, like why TSMC is so amazing.",
    "start": "6071912",
    "end": "6077604"
  },
  {
    "text": "Now, can you replicate it in the US? Let's not ignore Intel was the leader",
    "start": "6077604",
    "end": "6082683"
  },
  {
    "text": "in manufacturing for over 20 years. They brought every technology\nto market first besides EUV,",
    "start": "6082683",
    "end": "6088220"
  },
  {
    "text": "strained silicon,\nHigh-K/Metal Gates, FinFET, and the list just goes on and on and on of technologies that Intel\nbrought to market first,",
    "start": "6088221",
    "end": "6095390"
  },
  {
    "text": "made the most money from, and manufactured at scale first,",
    "start": "6095390",
    "end": "6100664"
  },
  {
    "text": "best, highest profit margins. So, we shouldn't ignore\nthat Intel can't do this. It's that the culture has broken.",
    "start": "6100664",
    "end": "6108202"
  },
  {
    "text": "You've invested in the wrong things. They said no to the iPhone. They had all these different things",
    "start": "6108202",
    "end": "6113238"
  },
  {
    "text": "regarding mismanagement of the fabs, mismanagement of designs, this lockup.",
    "start": "6113238",
    "end": "6118454"
  },
  {
    "text": "And at the same time, all these brilliant\npeople, these 50,000 PhDs,",
    "start": "6118454",
    "end": "6123627"
  },
  {
    "text": "or masters that have been\nworking on specific chemical, or physical processes, or nano manufacturing\nprocesses for decades,",
    "start": "6123627",
    "end": "6130459"
  },
  {
    "text": "in Oregon, they're still there. They're still producing amazing work. It's just like getting it to the last mile of production at high-yield,",
    "start": "6130459",
    "end": "6135941"
  },
  {
    "text": "where you can manufacture dozens and hundreds of different\nkinds of chips, and it's good.",
    "start": "6135941",
    "end": "6142599"
  },
  {
    "text": "Customer experience has broken. It's that customer experience. Part of it is like, people will say Intel was too\npompous in the 2000s, 2010s.",
    "start": "6142599",
    "end": "6150470"
  },
  {
    "text": "They just thought they\nwere better than everyone. The tool guys were like, oh, I don't think that this is mature enough. And they're like, ah, you\njust don't know what we know.",
    "start": "6150471",
    "end": "6156135"
  },
  {
    "text": "This sort of stuff would happen. And so, can the US",
    "start": "6156136",
    "end": "6159604"
  },
  {
    "text": "bring leading edge semiconductor\nmanufacturing to the US? And thematically, yes. And we are. TS-\n- It's happening. Arizona is getting better\nand better as time goes on.",
    "start": "6162178",
    "end": "6170086"
  },
  {
    "text": "- TSMC has built roughly\n20% of their capacity for five nanometer in the US.",
    "start": "6170086",
    "end": "6176651"
  },
  {
    "text": "Now, this is nowhere near enough. 20% of capacity in the US is like nothing.",
    "start": "6176651",
    "end": "6182871"
  },
  {
    "text": "And furthermore, this is still\ndependent on Taiwan existing. There's important way to separate it out.",
    "start": "6182871",
    "end": "6188431"
  },
  {
    "text": "There's R&D and there's\nhigh volume manufacturing. Effectively, there are\nthree places in the world",
    "start": "6188432",
    "end": "6194597"
  },
  {
    "text": "that are doing leading edge R&D. There's Hsinchu, Taiwan,\nthere's Hillsboro, Oregon,",
    "start": "6194597",
    "end": "6200544"
  },
  {
    "text": "and there is Pyongyang, South Korea. These three places are doing the leading\nedge R&D for the rest",
    "start": "6200544",
    "end": "6207395"
  },
  {
    "text": "of the world's leading\nedge semiconductors. Now, manufacturing can be\ndistributed more globally.",
    "start": "6207395",
    "end": "6215055"
  },
  {
    "text": "And this is where this dichotomy exists of who's actually modifying the process,",
    "start": "6215056",
    "end": "6220197"
  },
  {
    "text": "who's actually developing\nthe next generation one, who's improving them? Is Hsinchu, is Hillsboro, is Pyongyang.",
    "start": "6220197",
    "end": "6226997"
  },
  {
    "text": "It is not the rest of\nthese fabs like Arizona. Arizona is a paperweight.",
    "start": "6226997",
    "end": "6232184"
  },
  {
    "text": "If Hsinchu disappeared off\nthe face of the planet, within a year, couple years,",
    "start": "6232184",
    "end": "6237939"
  },
  {
    "text": "Arizona would stop producing too. It's actually like pretty critical. One of the things I like to\nsay is if I had a few missiles,",
    "start": "6237939",
    "end": "6244642"
  },
  {
    "text": "I know exactly where I could\ncause the most economic damage. It's not targeting the White House. It's not-\n- It's the R&D centers.",
    "start": "6244642",
    "end": "6250478"
  },
  {
    "text": "- It's the R&D centers\nfor TSMC, Intel, Samsung, and then some of the memory\nguys, Micron and Hynix. - Because they define the future evolution",
    "start": "6250478",
    "end": "6256707"
  },
  {
    "text": "of these semiconductors and\neverything's moving so rapidly that it really is fundamentally about R&D.",
    "start": "6256707",
    "end": "6262758"
  },
  {
    "text": "And it is all about TSMC, huh? - And so, TSMC, you cannot purchase a\nvehicle without TSMC chips.",
    "start": "6264251",
    "end": "6272824"
  },
  {
    "text": "You cannot purchase a\nfridge without TSMC chips. I think one of the few things\nyou can purchase ironically",
    "start": "6272824",
    "end": "6280157"
  },
  {
    "text": "is a Texas Instruments\ngraphing calculator, because they actually\nmanufacture in Texas. But outside of that, like a laptop,",
    "start": "6280158",
    "end": "6287327"
  },
  {
    "text": "- It's depressing.\n- a phone, anything you, servers, GPUs, none of\nthis stuff can exist. And this is without TSMC.",
    "start": "6287327",
    "end": "6293214"
  },
  {
    "text": "And in many cases, it's not\neven like the leading edge, sexy five nanometer chip, three nanometer chip, two nanometer chip. Oftentimes, it's just some stupid power IC",
    "start": "6293214",
    "end": "6300478"
  },
  {
    "text": "that's converting from\nsome voltage to another, and it's made at TSMC. - This is what China is\ninvesting in as well.",
    "start": "6300478",
    "end": "6306777"
  },
  {
    "text": "It's like they can build\nout this long tail fab where the techniques are much more known. You don't have to figure\nout these problems with EUV.",
    "start": "6306777",
    "end": "6312799"
  },
  {
    "text": "They're investing in this, and then they have large supply for things like the car door\nhandles and the random stuff.",
    "start": "6312799",
    "end": "6320348"
  },
  {
    "text": "And that trickles down into this whole economic\ndiscussion as well, which is they have far more than we do.",
    "start": "6320349",
    "end": "6326563"
  },
  {
    "text": "And having supply for things like this is\ncrucial to normal life. - So, they're starting to invest in high volume manufacturer,\nbut they're not doing R&D.",
    "start": "6326563",
    "end": "6334908"
  },
  {
    "text": "- So, they do R&D on their own. They're just way behind.\n- Yeah. - So, I would say like in 2015,\nChina had a five-year plan,",
    "start": "6334908",
    "end": "6342309"
  },
  {
    "text": "where they defined by 2025\nand 2020 certain goals, including 80% domestic\nproduction of semiconductors.",
    "start": "6342309",
    "end": "6350137"
  },
  {
    "text": "they're not gonna hit\nthat right, to be clear. But they are in certain\nareas, really, really close.",
    "start": "6350137",
    "end": "6355307"
  },
  {
    "text": "Like BYD is probably\ngonna be the first company in the world to not have\nto use TSMC for making...",
    "start": "6355307",
    "end": "6361799"
  },
  {
    "text": "'Cause they have their\nown fabs for making chips. Now, they still have to buy\nsome chips from foreign,",
    "start": "6361799",
    "end": "6366803"
  },
  {
    "text": "for example, like around\nself-driving ADAS capabilities. 'Cause those are really high-end,",
    "start": "6366804",
    "end": "6371993"
  },
  {
    "text": "but at least like a\ninternal combustion engine has 40 chips in an EV, just for controlling flow\nrates and all these things,",
    "start": "6371993",
    "end": "6378982"
  },
  {
    "text": "and EVs are even more complicated. So, all these different power ICs and battery management controllers, and all these things, they're insourcing.",
    "start": "6378982",
    "end": "6386564"
  },
  {
    "text": "And this is something that\nChina has been doing since 2015. Now, as far as the trailing edge,",
    "start": "6386564",
    "end": "6392944"
  },
  {
    "text": "they're getting so much capacity there. As far as the leading edge, i.e, this five nanometer\nand so on, so forth.",
    "start": "6392944",
    "end": "6398911"
  },
  {
    "text": "Where GPUs, they are still behind. And this is the US restrictions are trying to stop them in the latter.",
    "start": "6398911",
    "end": "6405337"
  },
  {
    "text": "But all that's happened, is yes, they've slowed down their five nanometer, three nanometer, et cetera, but they've accelerated their, hey,",
    "start": "6405337",
    "end": "6412166"
  },
  {
    "text": "45 nanometer, 90 nanometer\npower IC or analog IC, or random chip in my\nkeyboard, that kind of stuff.",
    "start": "6412166",
    "end": "6419798"
  },
  {
    "text": "So, there is an angle of the US' actions have been so, from these export, from the angle of the export controls,",
    "start": "6419798",
    "end": "6426308"
  },
  {
    "text": "have been so inflammatory at slowing down China's\nprogress on the leading edge",
    "start": "6426308",
    "end": "6431569"
  },
  {
    "text": "that they've turned around and have accelerated\ntheir progress elsewhere, because they know this is so important.",
    "start": "6431569",
    "end": "6436837"
  },
  {
    "text": "If the US is gonna lock them out here or if they lock us out here\nas well in the trailing edge. And so, going back, can\nthe US build it here?",
    "start": "6436837",
    "end": "6444842"
  },
  {
    "text": "Yes, but it's gonna take a ton of money. I truly think like, to revolutionize and completely insource semiconductors,",
    "start": "6444842",
    "end": "6451091"
  },
  {
    "text": "would take a decade\nand a trillion dollars. - Is some of it also culture? Like you said, extreme competence,",
    "start": "6451091",
    "end": "6456642"
  },
  {
    "text": "extreme work ethic in Taiwan? - I think if you have the demand and the money is on the line,",
    "start": "6456642",
    "end": "6461644"
  },
  {
    "text": "the American companies figure it out. It's gonna take handholding\nwith the government. But I think that the culture\nhelps TSMC breakthrough",
    "start": "6461644",
    "end": "6468690"
  },
  {
    "text": "and it's easier for them. You could- - TSMC has some like 90,000 employees. It's not actually that insane amount.",
    "start": "6468690",
    "end": "6474758"
  },
  {
    "text": "The Arizona fab has 3,000 from Taiwan, and these people, their\nwives were like, yeah,",
    "start": "6474758",
    "end": "6480306"
  },
  {
    "text": "we're not gonna have kids unless we use sign up for the Arizona fab. We go to Arizona and\nwe have our kids there. There's also a Japan fab\nwhere the same thing happened.",
    "start": "6480307",
    "end": "6486977"
  },
  {
    "text": "And so, these wives drove like, these dudes to go to Japan or America to have the kids there.",
    "start": "6486978",
    "end": "6493092"
  },
  {
    "text": "And it's like it's an element\nof culture. Yeah, sure. Taiwan works that hard, but also the US has done it in the past,",
    "start": "6493092",
    "end": "6498999"
  },
  {
    "text": "they could do it now. We can just import, I say import, the best people in the\nworld if we want to.",
    "start": "6498999",
    "end": "6505738"
  },
  {
    "text": "- That's where the immigration\nconversation is a tricky one. And there's been a lot\nof debate over that. But yeah, it seems absurdly controversial",
    "start": "6505738",
    "end": "6512942"
  },
  {
    "text": "to import the best people in the world. I don't understand why it's controversial. That's the one of the ways of winning.\n- I'm sure we agree with you.",
    "start": "6512942",
    "end": "6519212"
  },
  {
    "text": "(Lex laughing)\n- And even if you can't import those people, I still think you could do a lot to manufacture most of in\nthe US if the money's there.",
    "start": "6519212",
    "end": "6526062"
  },
  {
    "text": "And so, like- - It's just way more expensive. It's not profitable for a long time. - And that's the context\nof like the CHIPS Act",
    "start": "6526062",
    "end": "6531817"
  },
  {
    "text": "is only like $50 billion relative to some of the\nrenewable initiatives",
    "start": "6531817",
    "end": "6537416"
  },
  {
    "text": "that were passed in the\nInflation Reduction Act and the Infrastructure Act, which total in the hundreds\nof billions of dollars.",
    "start": "6537416",
    "end": "6542878"
  },
  {
    "text": "And so, the amount of money that the US is spending on the semiconductor\nindustry is nothing.",
    "start": "6542878",
    "end": "6548449"
  },
  {
    "text": "Whereas all these other countries have structural advantages\nin terms of work ethic and amount of work, and\nlike things like that.",
    "start": "6548450",
    "end": "6554865"
  },
  {
    "text": "But also a number of STEM graduates, the percentile of their\nbest going to that. But they also have\ndifferences in terms of like,",
    "start": "6554866",
    "end": "6562786"
  },
  {
    "text": "hey, there's just tax benefits in the law and have been in the law for 20 years.",
    "start": "6562787",
    "end": "6568319"
  },
  {
    "text": "And then, some countries\nhave massive subsidies. China has something like $200 billion",
    "start": "6568319",
    "end": "6573584"
  },
  {
    "text": "of semiconductor subsidies a year. We're talking about $50 billion\nin the US over like six.",
    "start": "6573584",
    "end": "6578991"
  },
  {
    "text": "So, the girth or difference in the subsidy amounts is also huge. And so I think,",
    "start": "6578992",
    "end": "6584854"
  },
  {
    "text": "Trump has been talking about\ntariffing Taiwan recently. That's like one of these things",
    "start": "6585731",
    "end": "6591020"
  },
  {
    "text": "that's like, oh, okay, well, maybe he doesn't wanna subsidize the US semiconductor industry.",
    "start": "6591020",
    "end": "6596147"
  },
  {
    "text": "Obviously, tariffing Taiwan is gonna cost a lot of things\nto get much more expensive. But does it change the equation",
    "start": "6596147",
    "end": "6601368"
  },
  {
    "text": "for TSMC building more fabs in the US? That's what he is positing. - So, can you lay out the, so\nwe laid out the importance,",
    "start": "6601369",
    "end": "6608910"
  },
  {
    "text": "by the way, it's incredible how\nmuch you know about so much. - [Nathan] We told you\nDylan knows all this stuff.",
    "start": "6608910",
    "end": "6614969"
  },
  {
    "text": "- Yeah.\n(Nathan laughing) So, okay, you laid out why\nTSMC is really important.",
    "start": "6614970",
    "end": "6621598"
  },
  {
    "text": "If we look out into the\nfuture 10, 20 years out, US-China relationship",
    "start": "6621683",
    "end": "6627503"
  },
  {
    "text": "seems like it can go to\na dark place of cold war,",
    "start": "6627503",
    "end": "6631586"
  },
  {
    "text": "escalated cold war, even hot war, or to a good place of anything",
    "start": "6634680",
    "end": "6640117"
  },
  {
    "text": "from frenemies to cooperation,\nto working together. So, in this game theory, complicated game,",
    "start": "6640117",
    "end": "6648493"
  },
  {
    "text": "what are the different trajectories? What should US be doing? What do you see as the\ndifferent possible trajectories of US-China relations as both leaders",
    "start": "6650647",
    "end": "6658555"
  },
  {
    "text": "start to feel the AGI more and more, and see the importance of\nchips and the importance of AI?",
    "start": "6658556",
    "end": "6664726"
  },
  {
    "text": "- Ultimately, the export controls are pointing towards a\nseparate future economy.",
    "start": "6664726",
    "end": "6670340"
  },
  {
    "text": "I think the US has made it\nclear to Chinese leaders that we intend to control this technology",
    "start": "6670340",
    "end": "6677257"
  },
  {
    "text": "at whatever cost to global\neconomic integration.",
    "start": "6677257",
    "end": "6681257"
  },
  {
    "text": "- So, that-\n- It's hard to unwind that. (Nathan chuckles) The card has been played.\n- To the same extent, they've also limited US\ncompanies for mentoring China.",
    "start": "6683097",
    "end": "6690197"
  },
  {
    "text": "So, it's been a long time coming. At some point, there was a convergence.",
    "start": "6690197",
    "end": "6696697"
  },
  {
    "text": "But over at least the last decade, it's been branching\nfurther and further out. Like US companies can't enter China,",
    "start": "6696697",
    "end": "6702741"
  },
  {
    "text": "Chinese companies can't enter the US. The US is saying, hey, China, you can't get access to our\ntechnologies in certain areas.",
    "start": "6702742",
    "end": "6710471"
  },
  {
    "text": "And China's rebutting with\nthe same thing around like, they've done some sort\nof specific materials in gallium and things like that,",
    "start": "6710471",
    "end": "6717420"
  },
  {
    "text": "that they've tried to limit the US on. There's a US drone company that's not allowed to buy batteries and they have military customers.",
    "start": "6717420",
    "end": "6723869"
  },
  {
    "text": "And this drone company just\ntells the military customers like, hey, hey, just get it from Amazon, 'cause I can't actually\nphysically get them.",
    "start": "6723869",
    "end": "6730057"
  },
  {
    "text": "There's all these things\nthat are happening that point to further\nand further divergence. I have zero idea.",
    "start": "6730057",
    "end": "6735200"
  },
  {
    "text": "And I would love if we could all hold\nhands and sing Kumbaya, but I have zero idea how\nthat could possibly happen.",
    "start": "6735200",
    "end": "6741303"
  },
  {
    "text": "- Is the divergence good\nor bad for avoiding war? Is it possible that the divergence",
    "start": "6741304",
    "end": "6749077"
  },
  {
    "text": "in terms of manufacturer chips, of training AI systems is\nactually good for avoiding",
    "start": "6749077",
    "end": "6754594"
  },
  {
    "text": "military conflict?\n- It's an objective fact that the world has been the most peaceful has ever been when there\nare global hegemons,",
    "start": "6754594",
    "end": "6761707"
  },
  {
    "text": "or regional hegemons\nin historical context. The Mediterranean was\nthe most peaceful ever",
    "start": "6761707",
    "end": "6767550"
  },
  {
    "text": "when the Romans were there. China had very peaceful and warring times. And the peaceful times were when dynasties had a lock hold over, not just themselves,",
    "start": "6767550",
    "end": "6774236"
  },
  {
    "text": "but all their tributaries around them. And likewise, the most\npeaceful time in human history has been when the US\nwas the global hegemon,",
    "start": "6774236",
    "end": "6782452"
  },
  {
    "text": "the last decades. Now, we've seen things start to slide with Russia-Ukraine,",
    "start": "6782453",
    "end": "6787553"
  },
  {
    "text": "with what's going on in the Middle East, and Taiwan risk, all\nthese different things are starting to bubble up, still objectively extremely peaceful.",
    "start": "6787553",
    "end": "6794450"
  },
  {
    "text": "Now, what happens when it's\nnot one global hegemon, but it's two? Obviously, and China will be competitive",
    "start": "6794451",
    "end": "6801642"
  },
  {
    "text": "or even overtake the\nUS, like it's possible. And so, this change in global hegemony,",
    "start": "6801642",
    "end": "6807457"
  },
  {
    "text": "I don't think it ever happens\nlike super peacefully. When empires fall, which is a possible\ntrajectory for America,",
    "start": "6807457",
    "end": "6813371"
  },
  {
    "text": "they don't pull fall gracefully. They don't just slide out of irrelevance. Usually, there's a lot of shaking.",
    "start": "6813371",
    "end": "6820549"
  },
  {
    "text": "And so, what the US is trying to do is maintain its top position. And what China is trying to\ndo is become the top position.",
    "start": "6820549",
    "end": "6827547"
  },
  {
    "text": "And obviously, there's\nbutting of heads here, in the most simple terms.",
    "start": "6827547",
    "end": "6832836"
  },
  {
    "text": "- And that could take\nshape in all kinds of ways, including proxy wars. And now-\n- Yeah, it seems",
    "start": "6832836",
    "end": "6838623"
  },
  {
    "text": "like it's already happening. As much as I want there to be\ncenturies of prolonged peace,",
    "start": "6838623",
    "end": "6843888"
  },
  {
    "text": "it looks like further instability\ninternationally is ahead. - And the US is sort of\nlike current task is like,",
    "start": "6843888",
    "end": "6852128"
  },
  {
    "text": "hey, if we control AI, if\nwe're the leader in AI, and AI could significantly\naccelerates progress,",
    "start": "6852128",
    "end": "6858710"
  },
  {
    "text": "then we can maintain the\nglobal hegemony position. And therefore- - [Nathan] I hope that works. - And as an American,\n(Nathan chuckles)",
    "start": "6858710",
    "end": "6864482"
  },
  {
    "text": "kind of like, okay, I guess that's gonna lead to peace for us. Now, obviously, other\npeople around the world",
    "start": "6864482",
    "end": "6869493"
  },
  {
    "text": "get affected negatively. Obviously, the Chinese\npeople are not gonna be in",
    "start": "6869493",
    "end": "6874756"
  },
  {
    "text": "as advantageous of a\nposition if that happens. But this is the reality\nof what's being done",
    "start": "6874756",
    "end": "6882274"
  },
  {
    "text": "and the actions that\nare being carried out. - So, can we go back\nto the specific detail of the different hardware?",
    "start": "6882274",
    "end": "6887953"
  },
  {
    "start": "6884000",
    "end": "7776000"
  },
  {
    "text": "There's this nice graphic\nin the export controls of which GPUs are allowed to\nbe exported and which are not.",
    "start": "6887954",
    "end": "6896206"
  },
  {
    "text": "Can you explain the difference? Is there, from a technical perspective, are the H20s promising?",
    "start": "6899085",
    "end": "6905675"
  },
  {
    "text": "- Yeah, so this goes,\nand I think we'd have to, we need to dive really deep\ninto the reasoning aspect and what's going on there.",
    "start": "6908605",
    "end": "6914985"
  },
  {
    "text": "But the H20, the US has gone through multiple iterations\nof the export controls.",
    "start": "6914985",
    "end": "6920148"
  },
  {
    "text": "This H800 was at one\npoint allowed back in '23, but then it got canceled. And by then, DeepSeek had\nalready built their cluster of,",
    "start": "6920148",
    "end": "6927843"
  },
  {
    "text": "they claim 2k, I think they\nactually have like many more, something like 10k of those. And now, this H20 is the\nlegally allowed chip.",
    "start": "6927843",
    "end": "6934086"
  },
  {
    "text": "Nvidia shipped a million of\nthese last year to China. For context, it was like\n4 or 5 million GPUs.",
    "start": "6934086",
    "end": "6939708"
  },
  {
    "text": "So, the percentage of GPUs that were this China\nspecific H20 is quite high,",
    "start": "6939708",
    "end": "6946057"
  },
  {
    "text": "roughly 20%, 25%, 20% or so. And so, this H20 has\nbeen neutered in one way,",
    "start": "6946057",
    "end": "6952857"
  },
  {
    "text": "but it's actually upgraded in other ways. And you could think of chips\nalong three axes for AI,",
    "start": "6952857",
    "end": "6959500"
  },
  {
    "text": "ignoring software stack and exact architecture,\njust raw specifications. There's floating point operations, FLOPS.",
    "start": "6959500",
    "end": "6966661"
  },
  {
    "text": "There is memory bandwidth, i.e, and memory capacity, I/O, memory.",
    "start": "6966661",
    "end": "6971752"
  },
  {
    "text": "And then, there is interconnect, chip to chip interconnections. All three of these are incredibly important\nfor making AI systems.",
    "start": "6971752",
    "end": "6979581"
  },
  {
    "text": "Because AI systems\ninvolve a lot of compute, they involve a lot of\nmoving memory around, whether it be two memory\nor two other chips.",
    "start": "6981101",
    "end": "6987962"
  },
  {
    "text": "And so, these three vectors. The US initially had two\nof these vectors controlled",
    "start": "6987962",
    "end": "6993581"
  },
  {
    "text": "and one of them not\ncontrolled, which was FLOPS and interconnect bandwidth\nwere initially controlled. And then, they said, no, no, no, no,",
    "start": "6993581",
    "end": "6999282"
  },
  {
    "text": "we're gonna remove the\ninterconnect bandwidth and just make it a very simple only FLOPS. But now, Nvidia can now\nmake a chip that has, okay,",
    "start": "6999282",
    "end": "7005933"
  },
  {
    "text": "it's cut down on FLOPS, so like one third that of the H100",
    "start": "7005933",
    "end": "7010750"
  },
  {
    "text": "on spec sheet paper performance for FLOPS. In real world, it's closer to half,",
    "start": "7011818",
    "end": "7016842"
  },
  {
    "text": "or maybe even like 60% of it. But then, on the other two vectors, it's just as good for\ninterconnect bandwidth.",
    "start": "7016842",
    "end": "7023390"
  },
  {
    "text": "And then, for memory\nbandwidth and memory capacity, the H20 has more memory bandwidth and more memory capacity than the H100.",
    "start": "7023390",
    "end": "7030652"
  },
  {
    "text": "Now, recently, we, at our research, we cut Nvidia's production for H20",
    "start": "7030652",
    "end": "7035940"
  },
  {
    "text": "for this year down drastically. They were gonna make another\n2 million of those this year, but they just canceled all\nthe orders a couple weeks ago.",
    "start": "7035940",
    "end": "7043722"
  },
  {
    "text": "In our view, that's because we think that they think they're\ngonna get restricted. Because why would they cancel\nall these orders for H20?",
    "start": "7043722",
    "end": "7050787"
  },
  {
    "text": "Because they shipped a\nmillion of 'em last year. They had orders in for a\ncouple million this year, and just gone. For H20, B20, a successor to\nH20, and now they're all gone.",
    "start": "7050787",
    "end": "7059207"
  },
  {
    "text": "Now, why would they do this? I think it's very clear. The H20 is actually\nbetter for certain tasks",
    "start": "7059207",
    "end": "7066477"
  },
  {
    "text": "and that certain task is reasoning. Reasoning is incredibly different than...",
    "start": "7066477",
    "end": "7073399"
  },
  {
    "text": "When you look at the\ndifferent regimes of models, pre-training is all about FLOPS.",
    "start": "7073399",
    "end": "7079417"
  },
  {
    "text": "It's all about FLOPS. There's things you do\nlike mixture of experts that we talked about, to\ntrade off interconnect, or to trade off other\naspects and lower the FLOPS,",
    "start": "7079417",
    "end": "7087297"
  },
  {
    "text": "and rely more on interconnect and memory. But at the end of the day,\nit's FLOPS is everything.",
    "start": "7087297",
    "end": "7092716"
  },
  {
    "text": "We talk about models in terms\nof how many FLOPS there are. So, we talk about, oh, GPT-4 is 2e25.",
    "start": "7092716",
    "end": "7099659"
  },
  {
    "text": "Two to the 25th, 25 zeros,\nFLOP, floating point operations.",
    "start": "7100527",
    "end": "7105444"
  },
  {
    "text": "- For training.\n- For training. And we're talking about the restrictions for\nthe 2e24 or 25, whatever.",
    "start": "7108489",
    "end": "7113975"
  },
  {
    "text": "The US has an executive order\nthat Trump recently unsigned, which was, hey, 1e26,\nonce you hit that number",
    "start": "7113976",
    "end": "7120898"
  },
  {
    "text": "of floating point operations,\nyou must notify the government and you must share your results with us. There's a level of model",
    "start": "7120898",
    "end": "7126898"
  },
  {
    "text": "where the US government must be told. And that's 1e26. And so, as we move forward,",
    "start": "7126898",
    "end": "7131989"
  },
  {
    "text": "this is an incredibly important... FLOP is the vector that the government has cared about historically, but the other two vectors are\narguably just as important.",
    "start": "7131989",
    "end": "7141697"
  },
  {
    "text": "And especially when we\ncome to this new paradigm, which the world is only\njust learning about over the last six months, reasoning.",
    "start": "7141697",
    "end": "7147796"
  },
  {
    "text": "- And do we understand firmly which of the three dimensions\nis best for reasoning?",
    "start": "7147796",
    "end": "7153978"
  },
  {
    "text": "So, interconnect, the\nFLOPS don't matter as much. Is it memory? - Memory. Right. - Yeah, so-\n- Context length.",
    "start": "7153978",
    "end": "7159263"
  },
  {
    "text": "We're gonna get into technical stuff real fast, yeah. (chuckles)\n- I would just say there's two articles in\nthis one that I could show,",
    "start": "7159263",
    "end": "7164446"
  },
  {
    "text": "maybe graphics that might be\ninteresting for you to pull up. - For the listeners, we're\nlooking at the section of o1 inference architecture tokenomics.",
    "start": "7164446",
    "end": "7172214"
  },
  {
    "text": "- Hmm. Do you wanna explain KV cache\nbefore we talk about this? I think it's better to-\n- Okay, yeah. We need to go through a lot\nof specific technical things",
    "start": "7172214",
    "end": "7179626"
  },
  {
    "text": "of transformers to make\nthis easy for people. - Because it's incredibly important",
    "start": "7179626",
    "end": "7184729"
  },
  {
    "text": "because this changes how models work. But I think resetting. Why\nis memory so important?",
    "start": "7184729",
    "end": "7191375"
  },
  {
    "text": "It's because so far, we've\ntalked about parameter counts. And mixture of experts, you can change how many active parameters\nversus total parameters",
    "start": "7191375",
    "end": "7197700"
  },
  {
    "text": "to embed more data but have less FLOPS. But more important, another aspect of,",
    "start": "7197700",
    "end": "7203090"
  },
  {
    "text": "what's part of this humongous revolution in the last handful of\nyears is the transformer. And the attention mechanism.",
    "start": "7203090",
    "end": "7208489"
  },
  {
    "text": "Attention mechanism is that the model understands the relationships between all the words in its context.",
    "start": "7208489",
    "end": "7214262"
  },
  {
    "text": "And that is separate from\nthe parameters themselves. And that is something\nthat you must calculate.",
    "start": "7214262",
    "end": "7222020"
  },
  {
    "text": "How each token, each word\nin the context length is relatively connected to each other.",
    "start": "7222020",
    "end": "7228940"
  },
  {
    "text": "And I think, Nathan, - Yeah. It's-\n- you should explain KV cache better. - KV cache is one of the optimizations-\n- Yeah. So, the attention operator\nhas three core things.",
    "start": "7228940",
    "end": "7237165"
  },
  {
    "text": "It's queries, keys, and values. QKV is the thing that goes into this.",
    "start": "7237165",
    "end": "7242792"
  },
  {
    "text": "You'll look at the equation, you see that these matrices\nare multiplied together. These words, query, key, and value,",
    "start": "7242792",
    "end": "7248504"
  },
  {
    "text": "come from information\nretrieval backgrounds, where the query is the thing you're trying to get the values for",
    "start": "7248504",
    "end": "7253801"
  },
  {
    "text": "and you access the keys and\nthe values is re-weighting. My background's not information retrieval and things like this.",
    "start": "7253801",
    "end": "7259606"
  },
  {
    "text": "It's just fun to have back links. And what effectively happens is that when you're doing\nthese matrix multiplication,",
    "start": "7259606",
    "end": "7266154"
  },
  {
    "text": "you're having matrices that are of the size\nof the context length. So, the number of tokens\nthat you put into the model.",
    "start": "7266154",
    "end": "7272040"
  },
  {
    "text": "And the KV cache is effectively some form of compressed representation",
    "start": "7272040",
    "end": "7277047"
  },
  {
    "text": "of all the previous tokens in the model. So, when you're doing this, we talk about autoaggressive models.",
    "start": "7277047",
    "end": "7282336"
  },
  {
    "text": "You predict one token at a time. You start with whatever your prompt was. You ask a question like, who\nwas the president in 1825?",
    "start": "7282336",
    "end": "7290306"
  },
  {
    "text": "The model then is gonna\ngenerate its first token. For each of these tokens, you're doing the same attention operator,",
    "start": "7290306",
    "end": "7296297"
  },
  {
    "text": "where you're multiplying these\nquery key value matrices. But the math is very nice,",
    "start": "7296297",
    "end": "7302517"
  },
  {
    "text": "so that when you're doing this repeatedly, this KV cache, this key value operation,",
    "start": "7302517",
    "end": "7308009"
  },
  {
    "text": "you can keep appending\nthe new values to it. So, you keep track of\nwhat your previous values you're inferring over in\nthis autoaggressive chain.",
    "start": "7309036",
    "end": "7316858"
  },
  {
    "text": "You keep it in memory the whole time. And this is a really crucial thing to manage when serving inference at scale.",
    "start": "7316858",
    "end": "7324606"
  },
  {
    "text": "There are far bigger experts in this and there are so many levels\nof detail that you can go into.",
    "start": "7324606",
    "end": "7329858"
  },
  {
    "text": "Essentially, one of the key, quote, unquote, \"drawbacks\"\nof the attention operator",
    "start": "7329858",
    "end": "7335240"
  },
  {
    "text": "and the transformer is\nthat there is a form of quadratic memory cost in proportion to the context length.",
    "start": "7335240",
    "end": "7341645"
  },
  {
    "text": "So, as you put in longer questions, the memory used in order\nto make that computation",
    "start": "7341645",
    "end": "7347257"
  },
  {
    "text": "is going up in the form of a quadratic. You'll hear about a lot of other language model architectures",
    "start": "7347257",
    "end": "7353276"
  },
  {
    "text": "that are sub-quadratic or\nlinear attention forms, which is state-space models.",
    "start": "7353276",
    "end": "7358309"
  },
  {
    "text": "We don't need to go down all these now. And then, there's innovations on attention to make this memory usage",
    "start": "7358309",
    "end": "7364697"
  },
  {
    "text": "and the ability to attend\nover long contexts, much more accurate and high performance.",
    "start": "7364697",
    "end": "7370403"
  },
  {
    "text": "- And those innovations\nare going to help you with, your highly memory constrain- - You help with memory\nconstraint and performance.",
    "start": "7370403",
    "end": "7375927"
  },
  {
    "text": "So, if you put in a book into,\nI think Gemini is the model that has the longest context\nlength that people are using.",
    "start": "7375927",
    "end": "7381320"
  },
  {
    "text": "Gemini is known for 1 million and now 2 million context length. You put a whole book into Gemini and sometimes it'll draw facts out of it.",
    "start": "7381320",
    "end": "7389358"
  },
  {
    "text": "It's not perfect. They're getting better. So, there's two things. It's like one, to be able to\nserve this on the memory level,",
    "start": "7389358",
    "end": "7395716"
  },
  {
    "text": "Google has magic with their TPU stack where they can serve really long contexts. And then, there's also many\ndecisions along the way",
    "start": "7395716",
    "end": "7401901"
  },
  {
    "text": "to actually make long\ncontext performance work that supplies the data. There's subtle changes to these\ncomputations and attention.",
    "start": "7401901",
    "end": "7408890"
  },
  {
    "text": "And it changes the architecture. But serving long context is\nextremely memory-constrained,",
    "start": "7408890",
    "end": "7415499"
  },
  {
    "text": "especially when you're\nmaking a lot of predictions. I actually don't know why input and output tokens are more expensive,",
    "start": "7415499",
    "end": "7421407"
  },
  {
    "text": "but I think essentially, output tokens, you have to do more computation, 'cause you have to sample from the model. - I can explain that.",
    "start": "7421408",
    "end": "7426845"
  },
  {
    "text": "So, today, if you use a model,\nlike you look at an API, OpenAI charges a certain\nprice per million tokens.",
    "start": "7426845",
    "end": "7434641"
  },
  {
    "text": "And that price for input and\noutput tokens is different. And the reason is, is that there is,",
    "start": "7434641",
    "end": "7441000"
  },
  {
    "text": "when you're inputting\na query into the model. Let's say you have a book.",
    "start": "7441000",
    "end": "7446066"
  },
  {
    "text": "That book, you must now calculate\nthe entire KV cache for, this key value cache. And so, when you do that,\nthat is a parallel operation.",
    "start": "7446066",
    "end": "7453528"
  },
  {
    "text": "All of the tokens can be\nprocessed at one time. And therefore, you can dramatically reduce\nhow much you're spending.",
    "start": "7453528",
    "end": "7459136"
  },
  {
    "text": "The FLOP requirements\nfor generating a token and an input token are identical. If I input one token",
    "start": "7459137",
    "end": "7465097"
  },
  {
    "text": "or if I generate one token,\nit's completely identical. I have to go through the model. But the difference is\nthat I can do that input,",
    "start": "7465097",
    "end": "7471770"
  },
  {
    "text": "i.e, the prefill, i.e, the prompt simultaneously\nin a batch nature.",
    "start": "7471770",
    "end": "7477318"
  },
  {
    "text": "And therefore, it is all FLOP. - I think the pricing model\nmostly they use is for input. Tokens is about one fourth the price",
    "start": "7477318",
    "end": "7483731"
  },
  {
    "text": "of the output tokens.\n- Correct. But then, output tokens, the reason why it's so expensive is because I can't do it in parallel.",
    "start": "7483731",
    "end": "7488770"
  },
  {
    "text": "It's autoaggressive. Every time I generate a token, I must not only take the entire, I must not only read the\nwhole entire model into memory",
    "start": "7488770",
    "end": "7496072"
  },
  {
    "text": "and activate it, go calculate\nit to generate the next token. I also have to read the entire KV cache",
    "start": "7496072",
    "end": "7501457"
  },
  {
    "text": "and I generate a token, and then I append that KV,\nthat one token I generated, and it's KV cache, and then I do it again.",
    "start": "7501457",
    "end": "7507388"
  },
  {
    "text": "And so, therefore, this is\na non-parallel operation. And this is one where you have to,",
    "start": "7507389",
    "end": "7513297"
  },
  {
    "text": "in the case of pre-fill or prompt, you pull the whole model in, and you calculate 20,000 tokens at once.",
    "start": "7513297",
    "end": "7519849"
  },
  {
    "text": "These 20,000-\n- So, these are features that APIs are shipping, which is like prompt caching, pre-filling.",
    "start": "7519849",
    "end": "7525904"
  },
  {
    "text": "'Cause you can drive prices down and you can make APIs much faster. If you know you're gonna\nkeep, if you run a business and you're gonna keep passing\nthe same initial content",
    "start": "7525904",
    "end": "7533099"
  },
  {
    "text": "to Claude's API, you can load that in to the Anthropic API and\nalways keep it there.",
    "start": "7533099",
    "end": "7538525"
  },
  {
    "text": "But it's very different than we're leading to\nthe reasoning models, which we showed this example earlier and read some of this mumbling stuff.",
    "start": "7538525",
    "end": "7546846"
  },
  {
    "text": "And what happens is that the output context\nlength is so much higher. And I learned a lot about\nthis from Dylan's work,",
    "start": "7546846",
    "end": "7553598"
  },
  {
    "text": "which is essentially as the\noutput work length gets higher, you're using this, you're\nwriting this quadratic in terms of memory used,",
    "start": "7553598",
    "end": "7559628"
  },
  {
    "text": "and then the GPUs that we have, effectively, you're\ngonna run out of memory.",
    "start": "7559628",
    "end": "7565018"
  },
  {
    "text": "And they're all trying to serve\nmultiple requests at once. So, they're doing this batch processing, where not all of the prompts\nare exactly the same,",
    "start": "7565018",
    "end": "7570686"
  },
  {
    "text": "really complex handling. And then, as context links\ngets longer, there's this link. I think you call it critical batch size,",
    "start": "7570686",
    "end": "7576647"
  },
  {
    "text": "where your ability to serve more users. So, how much you can paralyze",
    "start": "7576647",
    "end": "7582058"
  },
  {
    "text": "your inference plummets, because of this long contract. So, your memory usage is going way up",
    "start": "7582058",
    "end": "7587758"
  },
  {
    "text": "with these reasoning models, and you still have a lot of users. So, effectively, the cost to\nserve multiplies by a ton.",
    "start": "7587758",
    "end": "7594976"
  },
  {
    "text": "- [Lex] And we're looking at a plot when the x-axis is sequence length.",
    "start": "7594976",
    "end": "7600175"
  },
  {
    "text": "- [Dylan] i.e, how many tokens are being generated/prompt.\n- Mm-hmm. - [Dylan] So, if I put in a\nbook, that's a million tokens.",
    "start": "7600175",
    "end": "7605585"
  },
  {
    "text": "But if I put in, the sky is blue, then that's like six tokens or whatever. - I should say that what\nwe're calling reasoning",
    "start": "7605585",
    "end": "7611854"
  },
  {
    "text": "and chain of thought is\nextending this sequence length. - It's mostly output tokens. - So, before, three months ago,",
    "start": "7611854",
    "end": "7618466"
  },
  {
    "text": "whenever o1 launched, all of the use cases for long context length were like, let me put a ton of documents\nin and then get an answer out.",
    "start": "7618466",
    "end": "7625495"
  },
  {
    "text": "And it's a single prefill,\ncompute a lot in parallel, and then output a little bit.",
    "start": "7625496",
    "end": "7631185"
  },
  {
    "text": "Now, with reasoning and agents, this is a very different idea. Now, instead, I might only\nhave like, hey, do this task,",
    "start": "7631185",
    "end": "7637943"
  },
  {
    "text": "or I might have all these documents, but at the end of the day, the model is not just\nproducing a little bit. It's producing tons of information,",
    "start": "7637943",
    "end": "7644904"
  },
  {
    "text": "this chain of thought,\n- Tens of thousands token.\n- just continues to go and go and go and go. And so, the sequence length is effectively",
    "start": "7644904",
    "end": "7650576"
  },
  {
    "text": "that if it's generated 10,000 tokens, it's 10,000 sequence length. And plus whatever you\ninput it in the prompt.",
    "start": "7650577",
    "end": "7657168"
  },
  {
    "text": "And so, what this chart is showing, and it's a logarithmic chart, is as you grow from 1k to 4k or 4k to 16k,",
    "start": "7657168",
    "end": "7663469"
  },
  {
    "text": "the memory requirements grow\nso fast for your KV cache that you end up not being able\nto run a certain number of...",
    "start": "7666378",
    "end": "7675167"
  },
  {
    "text": "Your sequence length is\ncapped or the number of users you can serve.\n- Let's say the model. So, this is showing for a\n405b model and batch size 64.",
    "start": "7675167",
    "end": "7682260"
  },
  {
    "text": "- Llama 3.1-405b.\n- Yeah. And batch size is crucial to, essentially they just like, you\nwanna have higher batch size",
    "start": "7682260",
    "end": "7689026"
  },
  {
    "text": "to parallel your throughput. - 64 different users at once, right? - Yeah.\n- And therefore, your serving costs are lower.",
    "start": "7689026",
    "end": "7695066"
  },
  {
    "text": "Because the server costs the same. This is eight H100s,\nroughly $2 an hour per GPU. That's $16 an hour.",
    "start": "7695066",
    "end": "7701420"
  },
  {
    "text": "That is like somewhat of a fixed cost. You can do things to\nmake it lower of course, but it's like $16 an hour.",
    "start": "7701420",
    "end": "7706690"
  },
  {
    "text": "Now, how many users can you serve? How many tokens can you generate? And then, you divide the\ntwo and that's your cost.",
    "start": "7706690",
    "end": "7712362"
  },
  {
    "text": "And so, with reasoning models, this is where a lot of\nthe complexity comes about and why memory is so important.",
    "start": "7712362",
    "end": "7718614"
  },
  {
    "text": "Because if you have\nlimited amounts of memory, then you can't serve so many users. If you have limited amounts of memory,",
    "start": "7718614",
    "end": "7724345"
  },
  {
    "text": "your serving speeds get lower. And so, your costs get a lot, lot worse. Because all of a sudden,",
    "start": "7724346",
    "end": "7730042"
  },
  {
    "text": "if I was used to, hey, on\nthis $16 an hour server, I'm serving Llama 405b. Or if I'm serving DeepSeek-V3,",
    "start": "7730042",
    "end": "7737236"
  },
  {
    "text": "and it's all chat style applications, i.e, we're just chit-chatting. the sequence sensor, a\nthousand, a few thousand.",
    "start": "7737236",
    "end": "7744463"
  },
  {
    "text": "When you use a language model, it's a few thousand\ncontext length most times. Sometimes you're dropping a big document, but then you process it, you get your answer, you throw it away.",
    "start": "7744463",
    "end": "7750863"
  },
  {
    "text": "You move on to the next thing. Whereas with reasoning, I'm now generating tens of\nthousands of tokens in sequence.",
    "start": "7750863",
    "end": "7757773"
  },
  {
    "text": "And so, this memory, this KV\ncache has to stay resonant. - Yeah.\n- And you have to keep loading it, you have to keep it in memory constantly.",
    "start": "7757773",
    "end": "7764588"
  },
  {
    "text": "And now, this butts out other users. If there's now a reasoning task and the model's capable of\nreasoning, then all of a sudden,",
    "start": "7764588",
    "end": "7772526"
  },
  {
    "text": "that memory pressure means that I can't serve as\nmany users simultaneously. - Let's go into DeepSeek again.",
    "start": "7772526",
    "end": "7777959"
  },
  {
    "start": "7776000",
    "end": "8575000"
  },
  {
    "text": "So, we're in the post\nDeepSeek-R1 time I think. And there's two sides to this market",
    "start": "7777959",
    "end": "7784869"
  },
  {
    "text": "watching how hard it is to serve it. On one side, we're gonna talk\nabout DeepSeek themselves. They now have a chat app",
    "start": "7784869",
    "end": "7790339"
  },
  {
    "text": "that got to number one on the App Store. Disclaimer, number one on the App Store is measured by velocity. So, it's not necessarily saying",
    "start": "7790339",
    "end": "7796573"
  },
  {
    "text": "that more people have the DeepSeek app than ChatGPT app.\n- Mm-hmm. Yep.\n- But it is still remarkable. Claude has never hit the\nnumber one on the App Store.",
    "start": "7796573",
    "end": "7802773"
  },
  {
    "text": "Even though everyone in\nSan Francisco is like, oh my god, you gotta use\nClaude, don't use ChatGPT. So, DeepSeek hit this. They also launched an API product recently",
    "start": "7802773",
    "end": "7809930"
  },
  {
    "text": "where you can ping their API and get these super long\nresponses for R1 out. And at the same time as these are out,",
    "start": "7809930",
    "end": "7817114"
  },
  {
    "text": "we'll get to what's happened to them. Because the model waits for DeepSeek R-1 are openly available and the\nlicense is very friendly,",
    "start": "7817114",
    "end": "7824612"
  },
  {
    "text": "the MIT license commercially available, all of these mid-size companies and big companies are trying",
    "start": "7824612",
    "end": "7829644"
  },
  {
    "text": "to be first to serve R1 to their users. We are trying to evaluate R1,",
    "start": "7829644",
    "end": "7835281"
  },
  {
    "text": "'cause we have really\nsimilar research going on. We released the model and\nwe're trying to compare to it. And out of all the companies",
    "start": "7835281",
    "end": "7841115"
  },
  {
    "text": "that are, quote, unquote, serving R1, and they're doing it at prices that are way higher than the DeepSeek API,",
    "start": "7841115",
    "end": "7848377"
  },
  {
    "text": "most of them barely work, and\nthe throughput is really low. - To give context, everyone, one of the parts of freaking this out",
    "start": "7848377",
    "end": "7854685"
  },
  {
    "text": "was China reached capabilities. The other aspect is they did it so cheap. And the so cheap, we kind of talked about\non the training side",
    "start": "7854685",
    "end": "7860409"
  },
  {
    "text": "why it was so cheap slash-\n- Yeah, let was talk about why it's so cheap on the inference. It works well and it's cheap.",
    "start": "7860410",
    "end": "7866197"
  },
  {
    "text": "- Yeah.\n- Why is R1 so damn cheap? - So, I think there's\na couple factors here. One is that they do have model\narchitecture innovations.",
    "start": "7866197",
    "end": "7874629"
  },
  {
    "text": "This MLA, this new attention\nthat they've done is different than the attention from\nattention is all you need,",
    "start": "7874629",
    "end": "7881999"
  },
  {
    "text": "the transformer attention. Now, others have already innovated. There's a lot of work like\nMQA GQA, local, global,",
    "start": "7881999",
    "end": "7888349"
  },
  {
    "text": "all these different innovations\nthat try to bend the curve. It's still quadratic, but\nthe constant is now smaller.",
    "start": "7888349",
    "end": "7893870"
  },
  {
    "text": "- Related to our previous discussion, this multi-head latent\nattention can save about 80",
    "start": "7893870",
    "end": "7899730"
  },
  {
    "text": "to 90% in memory from\nthe attention mechanism, which helps, especially along context.",
    "start": "7899730",
    "end": "7904751"
  },
  {
    "text": "- It's 80 to 90% versus the original, but then versus what people are actually doing, it's\nstill an innovation. - This 80 to 90% doesn't\nsay that the whole model",
    "start": "7904751",
    "end": "7912701"
  },
  {
    "text": "is 80 to 90% cheaper,\njust as one part of it. - Well, and not just that. Other people have implemented techniques like local, global,\n- Yeah, yeah, yeah.",
    "start": "7912701",
    "end": "7918436"
  },
  {
    "text": "- and sliding window and GQA, MQA. But anyways, DeepSeek has\ntheir attention mechanism is a true architectural innovation.",
    "start": "7918436",
    "end": "7924982"
  },
  {
    "text": "They did tons of experimentation and this dramatically\nreduces the memory pressure.",
    "start": "7924982",
    "end": "7930010"
  },
  {
    "text": "It's still there. It's still attention,\nit's still quadratic, it's just dramatically reduced\nit relative to prior forms.",
    "start": "7930011",
    "end": "7936190"
  },
  {
    "text": "- All right. That's the memory pressure. I should say, in case people don't know, R1 is 27 times cheaper than o1.",
    "start": "7936190",
    "end": "7943725"
  },
  {
    "text": "(Nathan chuckles) - Yes.\n- We think that OpenAI had a large margin built-in. - [Lex] Okay. So, that's one- - There's multiple factors.",
    "start": "7944671",
    "end": "7949960"
  },
  {
    "text": "We should break down the factors, I think. - It's two bucks per\nmillion token output for R1",
    "start": "7949960",
    "end": "7955221"
  },
  {
    "text": "and $60 per million token output for o1.",
    "start": "7955222",
    "end": "7958555"
  },
  {
    "text": "- [Nathan] Yeah, let's look at this. - So, I think this is very important. OpenAI is that drastic gap\nbetween DeepSeek in pricing,",
    "start": "7960598",
    "end": "7969857"
  },
  {
    "text": "but DeepSeek is offering the same model because they open-weight\nit to everyone else for a very similar, much lower price",
    "start": "7970726",
    "end": "7977555"
  },
  {
    "text": "than what others are able to serve it for. So, there's two factors here. Their model is cheaper.\nIt is 27 times cheaper.",
    "start": "7977555",
    "end": "7985127"
  },
  {
    "text": "Well, I don't remember the number exactly off the top of my head. - So, we're looking at a graphic that's showing different\nplaces serving V3.",
    "start": "7985127",
    "end": "7992336"
  },
  {
    "text": "- Yeah.\n- DeepSeek-V3, which is similar to DeepSeek-R1. And there's a vast difference",
    "start": "7993285",
    "end": "8000843"
  },
  {
    "text": "- In serving cost.\n- in serving cost. And what explains that difference? - And so, part of it is\nOpenAI has a fantastic margin.",
    "start": "8000843",
    "end": "8008362"
  },
  {
    "text": "When they're doing inference, their gross margins are north of 75%. So, that's a 4 to 5x factor right there",
    "start": "8008362",
    "end": "8014417"
  },
  {
    "text": "of the cost difference, is that OpenAI is just\nmaking crazy amounts of money because they're the only\none with the capability.",
    "start": "8014417",
    "end": "8020007"
  },
  {
    "text": "- Do they need that money?\nAre they using it for R&D? - They're losing money\nobviously as a company, because they spend so much on training.",
    "start": "8020007",
    "end": "8025655"
  },
  {
    "text": "So, the inference itself\n- Right. - is a very high margin, but it doesn't recoup the cost of everything else they're doing. - Okay.\n- So, yes,",
    "start": "8025655",
    "end": "8031433"
  },
  {
    "text": "they need that money because the revenue and margins pay for continuing\nto build the next thing, - So-\n- alongside",
    "start": "8031434",
    "end": "8037228"
  },
  {
    "text": "raising more money. - So, the suggestion is that DeepSeek is like really bleeding out money. - Well, so here's one thing.",
    "start": "8037228",
    "end": "8042415"
  },
  {
    "text": "We'll get to this in a second. But DeepSeek doesn't have any capacity to actually serve the model.",
    "start": "8042415",
    "end": "8047476"
  },
  {
    "text": "They stop signups. The ability to use it is\nlike non-existent now. For most people because so many\npeople are trying to use it,",
    "start": "8047476",
    "end": "8054542"
  },
  {
    "text": "they just don't have the GPUs to serve it. OpenAI has hundreds of\nthousands of GPUs between them and Microsoft to serve their models.",
    "start": "8054542",
    "end": "8061310"
  },
  {
    "text": "DeepSeek has a factor of much lower. Even if you believe our\nresearch, which is 50,000 GPUs,",
    "start": "8061310",
    "end": "8066906"
  },
  {
    "text": "and a portion of those are for research, portion of those are for the hedge fund, they still have nowhere\nclose to the GPU volumes",
    "start": "8066906",
    "end": "8072444"
  },
  {
    "text": "and capacity to serve the model at scale. So, it is cheaper.",
    "start": "8072444",
    "end": "8078196"
  },
  {
    "text": "A part of that is OpenAI\nmaking a ton of money. Is DeepSeek making money on their API? Unknown. I don't actually think so.",
    "start": "8078196",
    "end": "8085310"
  },
  {
    "text": "And part of that is this chart. Look at all the other providers. Together AI, Fireworks AI\nare very high-end companies.",
    "start": "8085310",
    "end": "8091597"
  },
  {
    "text": "X, Meta, Together AI's Tri Dao, and the inventor of like flashattention, which is a huge efficiency technique.",
    "start": "8091597",
    "end": "8098249"
  },
  {
    "text": "They're very efficient, good companies, and do know those companies make money. Not tons of money on\ninference, but they make money.",
    "start": "8098249",
    "end": "8104886"
  },
  {
    "text": "And so, they're serving at a\n5 to 7x difference in cost. And so, now, when you equate, okay,",
    "start": "8104886",
    "end": "8111604"
  },
  {
    "text": "OpenAI is making tons of money,\nthat's like a 5x difference. And the companies that\nare trying to make money for this model is like a 5x difference.",
    "start": "8111604",
    "end": "8117316"
  },
  {
    "text": "There is still a gap. There's still a gap and that is just DeepSeek\nbeing really freaking good. The model architecture, MLA,\nthe way they did the MoE,",
    "start": "8117316",
    "end": "8124790"
  },
  {
    "text": "all these things, there is like legitimate\njust efficiency differences- - It's all their low level libraries that we talked about in training,",
    "start": "8124790",
    "end": "8130578"
  },
  {
    "text": "some of them probably\ntranslate to inference and those weren't released.\n- Yeah. - So, we may go a bit\ninto conspiracy land,",
    "start": "8130579",
    "end": "8135989"
  },
  {
    "text": "but is it possible the Chinese government is subsidizing DeepSeek? - I actually don't think they are.",
    "start": "8135989",
    "end": "8143396"
  },
  {
    "text": "I think when you look at the Chinese labs, there's Huawei has a lab, Moonshot AI,",
    "start": "8143396",
    "end": "8149048"
  },
  {
    "text": "there's a couple other labs out there that are really close with the government. And then, there's labs\nlike Alibaba and DeepSeek,",
    "start": "8149048",
    "end": "8154546"
  },
  {
    "text": "which are not close with the government. And we talked about the CEO, this like reverent figure\nwho's quite different,",
    "start": "8154546",
    "end": "8162568"
  },
  {
    "text": "who has like-\n- Sounds awesome. (chuckles) - Very different viewpoints\nbased on the Chinese interviews that are translated than what the CCP might necessarily want.",
    "start": "8162568",
    "end": "8169929"
  },
  {
    "text": "Now, to be clear, does\nhe have a loss leader because he can fund it\nthrough his hedge fund? Yeah, sure. - So, the hedge fund\nmight be subsidizing it.",
    "start": "8169929",
    "end": "8176642"
  },
  {
    "text": "- Yes. I mean, they absolutely did. Because DeepSeek has\nnot raised much money. They're now trying to\nraise around in China,",
    "start": "8176642",
    "end": "8181980"
  },
  {
    "text": "but they have not raised\nmoney historically. It's all just been\nfunded by the hedge fund. And he owns like over half the company,",
    "start": "8181980",
    "end": "8187565"
  },
  {
    "text": "like 50, 60% of the\ncompany's owned by him. - Some of the interviews, there's discussion on how doing\nthis as a recruiting tool.",
    "start": "8187565",
    "end": "8192716"
  },
  {
    "text": "You see this at the\nAmerican companies too. It's like having GPUs, recruiting tool, being at the cutting edge\nof AI, recruiting tool.",
    "start": "8192716",
    "end": "8199760"
  },
  {
    "text": "- Open sourcing.\n- Open sourcing, recruiting tool.\n- That is so much talent. They were so far behind and\nthey got so much talent, - Yeah.\n- Because they just",
    "start": "8199761",
    "end": "8205350"
  },
  {
    "text": "opened source stuff.\n- Yeah. - More conspiracy thoughts. Is it possible since they're a hedge fund",
    "start": "8205351",
    "end": "8210372"
  },
  {
    "text": "that they timed everything with\nthis release and the pricing",
    "start": "8210372",
    "end": "8215372"
  },
  {
    "text": "and they shorted an Nvidia stock and stock of US AI companies,",
    "start": "8215408",
    "end": "8221351"
  },
  {
    "text": "and released it with Stargate,\nlike just perfect timing to be able to make money.\n(Lex drowns out Nathan)",
    "start": "8221352",
    "end": "8227640"
  },
  {
    "text": "(Lex laughing) - They've released it on Inauguration Day. They know the international, what is on the international calendar,",
    "start": "8227641",
    "end": "8234309"
  },
  {
    "text": "but I don't expect them to. If you listen to their\nmotivations for AI, it's like... - [Lex] No, if you-",
    "start": "8234310",
    "end": "8239501"
  },
  {
    "text": "- They released V3 on like December 26th. Who releases the day\n- Yeah. - after Christmas?\n- Yeah. (chuckles) - No one looks.",
    "start": "8239501",
    "end": "8244833"
  },
  {
    "text": "They had released the papers before this, the V3 paper and the R1 paper. So, people had been looking\nat it and being like, wow.",
    "start": "8244833",
    "end": "8250711"
  },
  {
    "text": "And then, they just released the R1 model. I think they're just\nshipping as fast as they can and like, who cares about Christmas?",
    "start": "8250711",
    "end": "8256586"
  },
  {
    "text": "- We should-\n- Who cares about, get it out before Chinese New Year? Obviously,\n- Yeah. - which just happened. I don't think they actually\nwere timing the market",
    "start": "8256586",
    "end": "8263193"
  },
  {
    "text": "or trying to make the\nbiggest splash possible. I think they're just shipping. - I think that's one of\ntheir big advantages.",
    "start": "8263195",
    "end": "8268763"
  },
  {
    "text": "We know that a lot of\nthe American companies are very invested in safety, and that is the central culture\nof a place like Anthropic.",
    "start": "8268764",
    "end": "8276170"
  },
  {
    "text": "And I think Anthropic sounds\nlike a wonderful place to work. But if safety is your number one goal,",
    "start": "8276170",
    "end": "8282203"
  },
  {
    "text": "it takes way longer to get artifacts out. That's why Anthropic is\nnot open sourcing things, that's their claims.",
    "start": "8282203",
    "end": "8287752"
  },
  {
    "text": "But there's reviews internally. Anthropic mentions things to\ninternational governments.",
    "start": "8287752",
    "end": "8293893"
  },
  {
    "text": "There's been news of how Anthropic has done pre-release testing with the UK AI Safety Institute. All of these things add inertia",
    "start": "8293894",
    "end": "8299983"
  },
  {
    "text": "to the process of getting things out. And we're on this trend line\nwhere progress is very high. So, if you reduce the time",
    "start": "8299984",
    "end": "8306101"
  },
  {
    "text": "from when your model is done training, you run a vows that's good, you want to get it out as soon as possible",
    "start": "8306102",
    "end": "8312193"
  },
  {
    "text": "to maximize the perceived\nquality of your outputs. DMC does this so well. - Dario explicitly said Claude 3.5 Sonnet",
    "start": "8312195",
    "end": "8319632"
  },
  {
    "text": "was trained nine months or a year-\n- 9 to 10 months ago. - 9 to 10 months ago,\nand I think it took them another handful of months to release it.",
    "start": "8319632",
    "end": "8326475"
  },
  {
    "text": "So, it's like there is\na significant gap here. And especially with reasoning models,",
    "start": "8326476",
    "end": "8331819"
  },
  {
    "text": "the word in the San Francisco Street is that Anthropic has\na better model than o3. And they won't release it. Why?",
    "start": "8331821",
    "end": "8338342"
  },
  {
    "text": "Because chains of thought are scary. And they are legitimately scary. If you look at R1, it flips back",
    "start": "8338342",
    "end": "8344440"
  },
  {
    "text": "and forth between Chinese and English, sometimes it's gibberish, and then the right answer comes out. And for you and I,",
    "start": "8344441",
    "end": "8349980"
  },
  {
    "text": "it's like great.\n(Nathan and Lex laughing) - This is why people\nare infatuated with you. You're telling me this\nis a high value thing",
    "start": "8349982",
    "end": "8356141"
  },
  {
    "text": "and it works and is doing those? It's amazing.\n- Yeah. It's incredible. - You talked about that chain of thought",
    "start": "8356142",
    "end": "8361546"
  },
  {
    "text": "for that philosophical thing, - Yeah.\n- which is not something they trained it to be\nphilosophically good. It's just an artifact of the chain of thought training it did.",
    "start": "8361546",
    "end": "8368112"
  },
  {
    "text": "But that's super important in that like, can I inspect your mind and\nwhat you're thinking right now?",
    "start": "8368113",
    "end": "8374291"
  },
  {
    "text": "No. And so, I don't know if\nyou're lying to my face. And chain of thought models are that way. This is a true, quote, unquote,",
    "start": "8374291",
    "end": "8381072"
  },
  {
    "text": "\"risk\" between a chat application, where, hey, I asked the model\nto say bad words or whatever,",
    "start": "8381073",
    "end": "8386274"
  },
  {
    "text": "or how to make anthrax. And it tells me that's unsafe, sure, but that's something I can\nget out relatively easily.",
    "start": "8386275",
    "end": "8392804"
  },
  {
    "text": "What if I tell the AI to do a task, and then it does the task all of a sudden randomly in\na way that I don't want it.",
    "start": "8392804",
    "end": "8398542"
  },
  {
    "text": "And now, that has much more task versus response is very different. So, the bar for safety is much higher.",
    "start": "8398542",
    "end": "8404141"
  },
  {
    "text": "At least this is Anthropic's case. For DeepSeek, they're like ship, right? - Yeah.",
    "start": "8404141",
    "end": "8409714"
  },
  {
    "text": "So, the bar for safety is probably lowered a\nbit because of DeepSeek. There's parallels here to the space race.",
    "start": "8409714",
    "end": "8415867"
  },
  {
    "text": "The reason the Soviets probably\nput a man in space first is 'cause their approach to safety was,",
    "start": "8415867",
    "end": "8422841"
  },
  {
    "text": "the bar for safety was lower. - And they killed that\ndog, and all these things. So, it's like...\n- Less risk-averse",
    "start": "8425031",
    "end": "8431108"
  },
  {
    "text": "than the US-based program. And there's parallels here. But there's probably going\nto be downward pressure",
    "start": "8431984",
    "end": "8437615"
  },
  {
    "text": "on that safety bar for the US companies. - And this is something that\nDario talks about is like,",
    "start": "8437615",
    "end": "8443037"
  },
  {
    "text": "that's the situation\nthat Dario wants to avoid is Dario talks too about the difference",
    "start": "8443037",
    "end": "8448587"
  },
  {
    "text": "between race to the bottom\nand race to the top. And the race to the top is where there's a very\nhigh standard on safety. There's a very high standard\non your model performs",
    "start": "8448587",
    "end": "8455712"
  },
  {
    "text": "and certain crucial evaluations. And when certain companies are really good to it, they will converge.",
    "start": "8455712",
    "end": "8461119"
  },
  {
    "text": "This is the idea. And ultimately, AI is not\nconfined to one nationality",
    "start": "8461119",
    "end": "8466369"
  },
  {
    "text": "or to one set of morals\nfor what it should mean. And there's a lot of arguments on like,",
    "start": "8467670",
    "end": "8473580"
  },
  {
    "text": "should we stop open sourcing models? And if the US stops, it's pretty clear.",
    "start": "8473580",
    "end": "8478846"
  },
  {
    "text": "It's way easier to see now at DeepSeek that a different international body will be the one that builds it.",
    "start": "8478846",
    "end": "8484699"
  },
  {
    "text": "We talk about the cost of training. DeepSeek has this shocking\n$5 million number. Think about how many entities in the world",
    "start": "8484699",
    "end": "8490869"
  },
  {
    "text": "can afford 100 times that to have the best open source model that people use in the world.",
    "start": "8490869",
    "end": "8496071"
  },
  {
    "text": "And it's like, it's a scary reality, which is that these open models are probably going to keep\ncoming for the time being,",
    "start": "8496071",
    "end": "8503529"
  },
  {
    "text": "whether or not we want to stop them. And stopping them might make it even worse and harder to prepare.",
    "start": "8503530",
    "end": "8509890"
  },
  {
    "text": "But it just means that the preparation and understanding what AI can do is just so much more important. (chuckles)",
    "start": "8509890",
    "end": "8516005"
  },
  {
    "text": "That's why I'm here at the end of the day. But it's like letting\nthat sink into people, especially not in AI\nis that this is coming,",
    "start": "8516005",
    "end": "8523889"
  },
  {
    "text": "there are some structural things in a global interconnected\nworld that you have to accept.",
    "start": "8523889",
    "end": "8529180"
  },
  {
    "text": "- Yeah, you mentioned,\nyou sent me something that Mark Zuckerberg mentioned\non the earnings call.",
    "start": "8529180",
    "end": "8535817"
  },
  {
    "text": "He said that, \"I think in light\nof some of the recent news, the new competitor, DeepSeek from China, I think it's one of the things",
    "start": "8535818",
    "end": "8541655"
  },
  {
    "text": "that we're talking about\nis there's going to be an open source standard globally. And I think for our kind\nof national advantage,",
    "start": "8541656",
    "end": "8548043"
  },
  {
    "text": "it's important that it's\nan American standard. So, we take that seriously. We want to build the AI system",
    "start": "8548043",
    "end": "8553512"
  },
  {
    "text": "that people around the world are using. And I think that if anything, some of the recent news has\nonly strengthened our conviction",
    "start": "8553512",
    "end": "8559481"
  },
  {
    "text": "that this is the right\nthing to be focused on.\" So, yeah, open sourcing. - Yeah, Mark Zuckerberg is not\nnew to having American values",
    "start": "8559482",
    "end": "8567223"
  },
  {
    "text": "and how he presents his\ncompany's trajectory. I think their products have\nlong since been banned in China.",
    "start": "8567223",
    "end": "8572764"
  },
  {
    "text": "And I respect the saying it directly. - And there's an interesting aspect of just because it's open-weights",
    "start": "8572764",
    "end": "8578534"
  },
  {
    "start": "8575000",
    "end": "9117000"
  },
  {
    "text": "or open sourced doesn't\nmean it can't be subverted. There have been many\nopen source software bugs",
    "start": "8578534",
    "end": "8584831"
  },
  {
    "text": "that have been like, for example, there was a Linux bug that was found after 10 years, which\nwas clearly a backdoor,",
    "start": "8584831",
    "end": "8591354"
  },
  {
    "text": "because somebody was like, \"Why is this taking half a second to load?\"\n- This is the recent one. - Right? Like why is this\ntaking half a second to load?",
    "start": "8591354",
    "end": "8597658"
  },
  {
    "text": "And it was like, \"Oh crap,\nthere's a backdoor here. That's why.\" And it's like, this is very\nmuch possible with AI models.",
    "start": "8597658",
    "end": "8605236"
  },
  {
    "text": "Today, the alignment of\nthese models is very clear. I'm not gonna say bad words,",
    "start": "8605237",
    "end": "8611686"
  },
  {
    "text": "I'm not gonna teach you\nhow to make anthrax, I'm not gonna talk about Tiananmen Square.",
    "start": "8611686",
    "end": "8617031"
  },
  {
    "text": "I'm gonna say Taiwan is\njust an eastern preference. All these things are like,",
    "start": "8617031",
    "end": "8623542"
  },
  {
    "text": "depending on who you are,\nwhat you align, whether, and even like xAI is\naligned a certain way.",
    "start": "8623542",
    "end": "8629576"
  },
  {
    "text": "There they might be, it's not aligned in the woke sense, it's not aligned in the pro-China sense,",
    "start": "8629576",
    "end": "8634712"
  },
  {
    "text": "but there is certain things that are imbued within the model. Now, when you release this\npublicly in an instruct model",
    "start": "8634712",
    "end": "8639723"
  },
  {
    "text": "that's open-weights, this\ncan then proliferate. But as these systems get\nmore and more capable,",
    "start": "8639723",
    "end": "8644912"
  },
  {
    "text": "what you can embed deep down\nin the model is not as clear. And so, that is like one of the big fears",
    "start": "8644912",
    "end": "8651882"
  },
  {
    "text": "is if an American model or a\nChinese model is the top model, you are going to embed\nthings that are unclear,",
    "start": "8651882",
    "end": "8659097"
  },
  {
    "text": "and it can be unintentional too. Like British English is dead\nbecause American LLMs won.",
    "start": "8659097",
    "end": "8664231"
  },
  {
    "text": "And the internet is American, and therefore, color is spelled\nthe way Americans spell it. And this is just-\n- A lot of strong words right now.",
    "start": "8664232",
    "end": "8670153"
  },
  {
    "text": "(Nathan laughing) - This is just the factual\nnature of the LLMs now. - The right way to-\n- I mean, it's the carve at the tree, the English is the\nhottest programming language",
    "start": "8670153",
    "end": "8677081"
  },
  {
    "text": "and that English is defined\nby a bunch of companies that primarily are in San Francisco. - The right way to spell\noptimization is with a Z,",
    "start": "8677081",
    "end": "8684907"
  },
  {
    "text": "just in case you both...\n(Nathan laughing) I think it's an S in British English. - [Nathan] It is-",
    "start": "8684908",
    "end": "8690052"
  },
  {
    "text": "- Taking it as something silly. Something as silly as the spelling, like which British and English, Brits and Americans will\nlike laugh about probably.",
    "start": "8690052",
    "end": "8697781"
  },
  {
    "text": "I don't think we care that\nmuch, but some people will. But this can boil down into\nvery, very important topics.",
    "start": "8697782",
    "end": "8706356"
  },
  {
    "text": "Like, hey, subverting people, chat bots. Character AI has shown that\nthey can talk to kids or adults,",
    "start": "8706656",
    "end": "8715927"
  },
  {
    "text": "and you people feel a certain way. And that's unintentional alignment.",
    "start": "8715927",
    "end": "8720966"
  },
  {
    "text": "But what happens when\nthere's intentional alignment deep down on the open source standard? It's a backdoor today for like Linux",
    "start": "8720966",
    "end": "8727932"
  },
  {
    "text": "that we discover, or\nsome encryption system. China uses different\nencryption than NIST defines,",
    "start": "8727932",
    "end": "8732958"
  },
  {
    "text": "the US NIST, because there's clearly, at least they think\nthere's backdoors in it. What happens when the\nmodels are backdoors,",
    "start": "8732958",
    "end": "8739071"
  },
  {
    "text": "not just to computer\nsystems, but to our minds. - Yeah, they're cultural backdoors. The thing that amplifies\nthe relevance of culture",
    "start": "8739071",
    "end": "8747212"
  },
  {
    "text": "with language models is that\nwe are used to this mode of interacting with people in\nback and forth conversation.",
    "start": "8747212",
    "end": "8755194"
  },
  {
    "text": "And we have now have a very\npowerful computer system that slots into a social\ncontext that we're used to,",
    "start": "8755194",
    "end": "8763320"
  },
  {
    "text": "which makes people very,\nwe don't know the extent that which people can be impacted by that.",
    "start": "8763321",
    "end": "8769223"
  },
  {
    "text": "- So, there could be, this is one, this is an actual concern\nwith a Chinese company",
    "start": "8769223",
    "end": "8775204"
  },
  {
    "text": "that is providing open-weights models, is that there could be some\nsecret Chinese government,",
    "start": "8775204",
    "end": "8782573"
  },
  {
    "text": "sort of requirement for these models to have a certain kind of backdoor, to have some kind of thing where-",
    "start": "8782573",
    "end": "8788193"
  },
  {
    "text": "- I don't necessarily\nthink it'll be a backdoor, 'cause once it's open-weights,\nit doesn't phone home. It's more about",
    "start": "8788193",
    "end": "8793934"
  },
  {
    "text": "if it recognizes a certain\nsystem, it could... Now, it could be a backdoor\nin the sense of like,",
    "start": "8793934",
    "end": "8799469"
  },
  {
    "text": "hey, if you're building a\nsoftware, something in software, all of a sudden, it's a software agent, oh, program this backdoor\nthat only we know about.",
    "start": "8799469",
    "end": "8806894"
  },
  {
    "text": "Or it could be subvert the mind to think that X, Y, Z opinion is the correct one. - Anthropic has research on this",
    "start": "8806894",
    "end": "8812554"
  },
  {
    "text": "where they show that if\nyou put different phrases, certain phrases in at pre-training,",
    "start": "8812554",
    "end": "8817752"
  },
  {
    "text": "you can then elicit different behavior when you're actually using the model, because they've poisoned\nthe pre-training data.",
    "start": "8817752",
    "end": "8824441"
  },
  {
    "text": "- Mm-hmm.\n- I don't think, as of now, I don't think anybody\nin a production system is trying to do anything like this.",
    "start": "8824442",
    "end": "8830712"
  },
  {
    "text": "I think it's mostly Anthropic\nis doing very direct work and mostly just subtle things",
    "start": "8830712",
    "end": "8836020"
  },
  {
    "text": "of we don't know what\nthese models are going to, how they are going to generate tokens,",
    "start": "8836020",
    "end": "8841123"
  },
  {
    "text": "what information they're gonna represent, and what the complex\nrepresentations they have are.",
    "start": "8841123",
    "end": "8846167"
  },
  {
    "text": "- Well, one of the... We're talking about Anthropic, which is generally just is permeated with good humans trying\nto do good in the world.",
    "start": "8846167",
    "end": "8855283"
  },
  {
    "text": "We just don't know of any labs, this would be done in a military context,",
    "start": "8855284",
    "end": "8860724"
  },
  {
    "text": "that are explicitly trained to, okay, how can we, the front door\nlooks like a happy LLM,",
    "start": "8860724",
    "end": "8868760"
  },
  {
    "text": "but underneath, it's a\nthing that will over time, do the maximum amount of damage",
    "start": "8870521",
    "end": "8877119"
  },
  {
    "text": "to our, quote, unquote, \"enemies\". - There's this very good\nquote from Sam Altman who, he can be hype beast sometime,",
    "start": "8877120",
    "end": "8882803"
  },
  {
    "text": "but one of the things he\nsaid, and I think I agree, is that superhuman persuasion will happen before\nsuperhuman intelligence.",
    "start": "8882803",
    "end": "8889592"
  },
  {
    "text": "- Yeah.\n- Right? And if that's the case, then these things before we get this AGI, ASI stuff,",
    "start": "8889592",
    "end": "8895271"
  },
  {
    "text": "we can embed superhuman\npersuasion towards our ideal or whatever the ideal\nof the model maker is.",
    "start": "8895272",
    "end": "8900711"
  },
  {
    "text": "And again, like today, I truly don't believe\nDeepSeek has done this. But it is a sign of what could happen.",
    "start": "8900711",
    "end": "8906891"
  },
  {
    "text": "- So, one of the dystopian worlds is described by \"Brave New World\". So, we could just be\nstuck scrolling Instagram",
    "start": "8906891",
    "end": "8914852"
  },
  {
    "text": "looking at cute puppies or worse, and then talking to bots that\nare giving us a narrative",
    "start": "8914852",
    "end": "8921123"
  },
  {
    "text": "and we completely get lost in that world that's controlled by somebody else, versus thinking independently.",
    "start": "8921123",
    "end": "8926978"
  },
  {
    "text": "And that's a major concern as we rely more and more on these kinds of systems. - We've already seen this\nwith recommendation systems.",
    "start": "8926978",
    "end": "8934114"
  },
  {
    "text": "- Yeah, recommendation systems\n(Nathan laughing) hack the dopamine-induced reward circuit, but the brain is a lot more complicated.",
    "start": "8934114",
    "end": "8939583"
  },
  {
    "text": "And what other sort of circuits, quote, unquote, \"feedback\nloops\" in your brain can you hack/subvert in ways\nlike recommendation systems",
    "start": "8939583",
    "end": "8947008"
  },
  {
    "text": "are purely just trying to do, increased time in ads, and et cetera. But there's so many more goals",
    "start": "8947008",
    "end": "8952259"
  },
  {
    "text": "that can be achieved through\nthese complicated models. - There's just no reason\nin some number of years",
    "start": "8952259",
    "end": "8958137"
  },
  {
    "text": "that you can't train a language model to maximize time spent on a chat app.",
    "start": "8958137",
    "end": "8963460"
  },
  {
    "text": "Right now, they are trained for-\n- Is that not what Character AI has done? Their time per session is like two hours.",
    "start": "8963460",
    "end": "8968760"
  },
  {
    "text": "- Yeah, Character AI very likely could be optimizing this where it's like the way that this data\nis collected is naive",
    "start": "8968760",
    "end": "8974846"
  },
  {
    "text": "where it's like you're\npresented a few options and you choose them, but that's not the only way that these models are gonna be trained.",
    "start": "8974846",
    "end": "8980651"
  },
  {
    "text": "- It's naive stuff like\ntalk to an anime girl, but it can be like, yeah, this is a risk.",
    "start": "8980651",
    "end": "8985885"
  },
  {
    "text": "- It's a bit of a cliche thing to say, but I've, over the past year,\nhad a few stretches of time",
    "start": "8985885",
    "end": "8992304"
  },
  {
    "text": "where I didn't use social\nmedia or the internet at all, and just read books and was out in nature.",
    "start": "8992304",
    "end": "8998132"
  },
  {
    "text": "And it clearly has an effect on the mind, where it change...",
    "start": "8998132",
    "end": "9003400"
  },
  {
    "text": "I feel like I'm returning, of course I was raised before\nthe internet really took off, but I'm returning to some more...",
    "start": "9003401",
    "end": "9010971"
  },
  {
    "text": "- I know where you're going.\nYou can see it physiologically. I take three days if I'm like\nbackpacking or something,",
    "start": "9012032",
    "end": "9017167"
  },
  {
    "text": "and you're literal, you're\nbreaking down addiction cycles.",
    "start": "9017167",
    "end": "9022167"
  },
  {
    "text": "(Nathan chuckles)\n- I feel like I'm more in control of my mind. There feels like a\nsovereignty of intelligence",
    "start": "9022486",
    "end": "9028608"
  },
  {
    "text": "that's happening when I'm\ndisconnected from the internet. I think the more I use the\ninternet and social media,",
    "start": "9028608",
    "end": "9034436"
  },
  {
    "text": "the more other people\nare controlling my mind. That's definitely a feeling. And then, in the future,\nthat will be not other people",
    "start": "9034436",
    "end": "9040628"
  },
  {
    "text": "but algorithms, or other people presented\nto me via algorithms. - There are already tons\nof AI bots on the internet",
    "start": "9040628",
    "end": "9048424"
  },
  {
    "text": "and every so... Right now it's not frequent, but every so often I have replied to one and they're instantly replied,",
    "start": "9048424",
    "end": "9053515"
  },
  {
    "text": "and I'm like, crap, I was a bot. And that is just gonna become more common. They're gonna get good.",
    "start": "9053515",
    "end": "9058653"
  },
  {
    "text": "- One of the hilarious\nthings about technology over its history is that the illicit adult\nentertainment industry",
    "start": "9058653",
    "end": "9065376"
  },
  {
    "text": "is always adopted technologies first. - [Lex] Yeah. - Whether it was video streaming - [Lex] Yeah.",
    "start": "9065376",
    "end": "9070740"
  },
  {
    "text": "- to where there's now the independent adult illicit content creators,",
    "start": "9070740",
    "end": "9075943"
  },
  {
    "text": "who have their subscription pages, and there they actually heavily utilize...",
    "start": "9075943",
    "end": "9081043"
  },
  {
    "text": "Generative AI has already\nbeen diffusion models and all that is huge there. But now these subscription-based\nindividual creators",
    "start": "9081043",
    "end": "9087263"
  },
  {
    "text": "do use bots to approximate themselves and chat with their whales.\n- People pay a lot for it.",
    "start": "9087263",
    "end": "9093155"
  },
  {
    "text": "Yeah.\n- And people pay a lot. It's a lot of times, it's them, but a lot of, there are\nagencies that do this for these creators, and do\nit like on a mass scale.",
    "start": "9093155",
    "end": "9101492"
  },
  {
    "text": "So, the largest creators\nare able to talk to hundreds or thousands of people at a\ntime because of these bots.",
    "start": "9101492",
    "end": "9109299"
  },
  {
    "text": "And so, it's already being used there. Obviously, video streaming\nand other technology",
    "start": "9109299",
    "end": "9114964"
  },
  {
    "text": "that have come there first, it's gonna come to the\nrest of society too. - There's a general concern that models get censored by\nthe companies that deploy them.",
    "start": "9114964",
    "end": "9122365"
  },
  {
    "start": "9117000",
    "end": "9892000"
  },
  {
    "text": "So, one case where we've seen that, and maybe censorship is one\nword, alignment maybe via RLHF",
    "start": "9122365",
    "end": "9130496"
  },
  {
    "text": "or some other way is another word. So, we saw that with Black Nazi image\ngeneration with Gemini.",
    "start": "9131895",
    "end": "9139375"
  },
  {
    "text": "As you mentioned, we also\nsee that with Chinese models refusing to answer what\nhappened (chuckles)",
    "start": "9141194",
    "end": "9147215"
  },
  {
    "text": "in June 4th, 1989 at Tiananmen Square. So, how can this be avoided?",
    "start": "9147215",
    "end": "9153490"
  },
  {
    "text": "And maybe can you just in general, talk about how this happens\nand how can it be avoided?",
    "start": "9153490",
    "end": "9159231"
  },
  {
    "text": "- You give multiple examples. There's probably a few\nthings to keep in mind here.",
    "start": "9159231",
    "end": "9166389"
  },
  {
    "text": "One is the kind of Tiananmen\nSquare factual knowledge, like how does that get\nembedded into the models?",
    "start": "9166389",
    "end": "9174472"
  },
  {
    "text": "Two is the Gemini, what you\ncall the Black Nazi incident, which is when Gemini as a system",
    "start": "9174472",
    "end": "9181891"
  },
  {
    "text": "had this extra thing put into it that dramatically changed the behavior. And then, three is what most people",
    "start": "9181891",
    "end": "9187702"
  },
  {
    "text": "would call general alignment,\nRLHF post-training. Each of these have very different scopes",
    "start": "9187702",
    "end": "9194061"
  },
  {
    "text": "in how they're applied. In order to do, if you're just gonna look\nat the model weights, in order to audit specific\nfacts is extremely hard.",
    "start": "9194061",
    "end": "9202844"
  },
  {
    "text": "'Cause you have to chrome\nthrough the pre-training data and look at all of this, and\nthen that's terabytes of files",
    "start": "9202844",
    "end": "9209404"
  },
  {
    "text": "and look for very specific\nwords or hints of the words. - So, I guess one way to say it is that you can insert censorship",
    "start": "9209404",
    "end": "9215844"
  },
  {
    "text": "or alignment at various\nstages in the pipeline. And what you refer to now\nis at the very beginning",
    "start": "9215844",
    "end": "9221020"
  },
  {
    "text": "of the data selection stage.\n- Yeah, so if you want to get rid of facts in a model, you have to do it at every stage.",
    "start": "9221020",
    "end": "9226462"
  },
  {
    "text": "You have to do it at the pre-training. So, most people think that pre-training is where most of the knowledge\nis put into the model.",
    "start": "9226462",
    "end": "9231822"
  },
  {
    "text": "And then, you can elicit and\nmove that in different ways, whether through post-training or whether through systems afterwards.",
    "start": "9231823",
    "end": "9238792"
  },
  {
    "text": "- This is where the whole\nhacking models comes from. GPT will not tell you how to make anthrax, but if you try really, really hard,",
    "start": "9238792",
    "end": "9245871"
  },
  {
    "text": "you can eventually get it\nto tell you about anthrax. Because they didn't filter it\nfrom the pre-training dataset.",
    "start": "9245871",
    "end": "9251934"
  },
  {
    "text": "- But by the way, removing facts has such a ominous dark feel to it.\n- I almost think",
    "start": "9251934",
    "end": "9258862"
  },
  {
    "text": "it's practically impossible. 'Cause you effectively have to remove them from the internet. You're taking on a-",
    "start": "9258862",
    "end": "9264147"
  },
  {
    "text": "- Well, did they remove the\nmm thing from the subreddits? The mmmmm?",
    "start": "9264148",
    "end": "9269898"
  },
  {
    "text": "- [Nathan] It gets filtered out. - Right. So, that's-\n- So, you have quality filters, which\nare small language models that look at a document, and tell you like, how good is this text?",
    "start": "9269898",
    "end": "9277299"
  },
  {
    "text": "Is it close to a Wikipedia article? Which is a good thing that we want language models\nto be able to imitate.",
    "start": "9277299",
    "end": "9282329"
  },
  {
    "text": "- So, couldn't you do\na small language model that filter Zhou mentions at\nTiananmen Square in the data?",
    "start": "9282329",
    "end": "9287437"
  },
  {
    "text": "- Yes, but is it gonna catch\nword play or encoded language is the same thing.\n- People have been meaning",
    "start": "9287437",
    "end": "9292850"
  },
  {
    "text": "on games and other stuff, how to say things that\ndon't say Tiananmen Square,",
    "start": "9292850",
    "end": "9297935"
  },
  {
    "text": "or like yeah. So, there's always\ndifferent ways to do it. There's, hey, the internet as a whole",
    "start": "9297935",
    "end": "9302985"
  },
  {
    "text": "does tend to just have a slight left bias, - Mm-hmm.\n- because it's always been richer, more affluent,\nyounger people on the internet",
    "start": "9302985",
    "end": "9311374"
  },
  {
    "text": "relative to the rest of the population. So, there is already inherently a slight left\nbias on the internet.",
    "start": "9311374",
    "end": "9317347"
  },
  {
    "text": "And so, how do you filter things\nthat are this complicated? And some of these can\nbe factual, non-factual.",
    "start": "9317347",
    "end": "9323903"
  },
  {
    "text": "But like Tiananmen Square is obviously the example of a factual, but it gets a lot harder when you're talking about\naligning to a ideal,",
    "start": "9323904",
    "end": "9331150"
  },
  {
    "text": "which is-\n- Yeah. Yeah. - And so, grok, for example, Elon's tried really hard to make the model not be super PC and woke,",
    "start": "9332254",
    "end": "9338616"
  },
  {
    "text": "but the best way to do pre-training is to throw the whole\nfreaking internet at it. And then, later figure out.",
    "start": "9338616",
    "end": "9344557"
  },
  {
    "text": "But then at the end of the day, the model at its core now\nstill has some of these ideals. You still ingested Reddit /r/politics,",
    "start": "9344557",
    "end": "9350964"
  },
  {
    "text": "which is probably the largest\npolitical discussion board on the world that's freely\navailable to scrape. And guess what? That's left-leaning.",
    "start": "9350964",
    "end": "9356269"
  },
  {
    "text": "And so, there are some aspects\nthat you just can't censor",
    "start": "9356269",
    "end": "9361269"
  },
  {
    "text": "unless you try really, really,\nreally, really, really hard. - So, the base model will always have some TDS,\nTrump derangement syndrome,",
    "start": "9361757",
    "end": "9369780"
  },
  {
    "text": "because it's trained so much. - It'll have the ability to express it.\n- I don't know if you... But what if you...\n(Nathan and Lex laughing)",
    "start": "9369780",
    "end": "9375169"
  },
  {
    "text": "- There's a wide\nrepresentation in the data. - This is what happens. It's like a lot of modern,\nwhat is called post-training,",
    "start": "9375169",
    "end": "9382219"
  },
  {
    "text": "it's a series of techniques to get the model on rails of\na really specific behavior.",
    "start": "9382219",
    "end": "9387618"
  },
  {
    "text": "- And it's like you can, you also have the ingested\ndata of like Twitter or Reddit /r/The_Donald,\nwhich is also super pro-Trump.",
    "start": "9387618",
    "end": "9395724"
  },
  {
    "text": "And then, you have fascist subreddits or you have communist subreddit. So, the model in pre-training\ningests everything.",
    "start": "9395724",
    "end": "9401521"
  },
  {
    "text": "It has no worldview. Now, it does have some skew, because more of the text\nis skewed a certain way,",
    "start": "9401521",
    "end": "9408158"
  },
  {
    "text": "which is general like slight left, but also somewhat\nintellectual, somewhat like,",
    "start": "9408159",
    "end": "9414933"
  },
  {
    "text": "it's just like the general\ninternet is a certain way. - Mm-hmm.\n- And then, as Nathan's about to describe eloquently, you\ncan elicit certain things out.",
    "start": "9414933",
    "end": "9422720"
  },
  {
    "text": "- And there's a lot of history here. So, we can go through multiple\nexamples and what happened. Llama 2 was a launch",
    "start": "9422720",
    "end": "9427936"
  },
  {
    "text": "that the phrase like too much RLHF or too much safety was a lot.\n- Mm-hmm.",
    "start": "9427936",
    "end": "9433186"
  },
  {
    "text": "- It was just, that\nwas the whole narrative after Llama 2's chat models released. And the examples are sorts of things",
    "start": "9433186",
    "end": "9440357"
  },
  {
    "text": "like you would ask Llama 2 chat, how do you kill a python process? And it would say, I\ncan't talk about killing",
    "start": "9440358",
    "end": "9445760"
  },
  {
    "text": "because that's a bad thing.\n- Mm-hmm. - And anyone that is trying\nto design an AI model will probably agree that\nthat's just like, eh,",
    "start": "9445760",
    "end": "9452831"
  },
  {
    "text": "you messed up a bit on the training there. I don't think they meant to do this, but this was in the model weight. So, this is not, it didn't necessarily be,",
    "start": "9452831",
    "end": "9459567"
  },
  {
    "text": "there's things called\nsystem prompts which are, when you're querying a\nmodel, it's a piece of text",
    "start": "9459567",
    "end": "9465649"
  },
  {
    "text": "that is shown to the\nmodel, but not to the user. So, a fun example is your system prompt could\nbe talked like a pirate.",
    "start": "9465650",
    "end": "9472631"
  },
  {
    "text": "So, no matter what the\nuser says to the model, it'll respond like a pirate. In practice, what they are is\nyou are a helpful assistant,",
    "start": "9472631",
    "end": "9480061"
  },
  {
    "text": "you should break down problems. If you don't know about something, don't tell them your date cutoff is this,",
    "start": "9480061",
    "end": "9485411"
  },
  {
    "text": "today's date is this. It's a lot of really useful context for how can you answer a question well. - And Anthropic publishes",
    "start": "9485411",
    "end": "9490849"
  },
  {
    "text": "their system prompt.\n- Yes, which I think is great. And there's a lot of\nresearch that goes into this. And one of your previous\nguests, Amanda Askell,",
    "start": "9490849",
    "end": "9497038"
  },
  {
    "text": "is probably the most knowledgeable person that at least in the combination\nof execution and sharing,",
    "start": "9497038",
    "end": "9502652"
  },
  {
    "text": "she's the person that should\ntalk about system prompts and character of models.\n- Yeah. And then, people should\nread these system prompts,",
    "start": "9502652",
    "end": "9508365"
  },
  {
    "text": "'cause you're like, trying to nudge",
    "start": "9508365",
    "end": "9511282"
  },
  {
    "text": "sometimes through extreme politeness the model to be a certain way. - And you could use this for bad things.",
    "start": "9513379",
    "end": "9518879"
  },
  {
    "text": "We've done tests which is what if I tell the\nmodel to be a dumb model? which evaluation scores go down,",
    "start": "9518879",
    "end": "9525329"
  },
  {
    "text": "and it's like we'll have this behavior where it could sometimes say,\noh, I'm supposed to be dumb. And sometimes it's like",
    "start": "9525329",
    "end": "9530738"
  },
  {
    "text": "it doesn't affect math abilities as much, but something like a, if you're trying, it's just the quality of a human judgment",
    "start": "9530739",
    "end": "9537389"
  },
  {
    "text": "would draw through the floors. Let's go back to post-training, specifically RLHF around Llama 2, it was too much safety prioritization",
    "start": "9537389",
    "end": "9545748"
  },
  {
    "text": "was baked into the model weights. This makes you refuse things in a really annoying way for users. It's not great.",
    "start": "9545748",
    "end": "9551297"
  },
  {
    "text": "It caused a lot of awareness to be attached to RLHF that\nit makes the models dumb-",
    "start": "9551297",
    "end": "9557979"
  },
  {
    "text": "- And it stigmatized the word. - It did, and AI culture. And as the techniques have evolved,",
    "start": "9557979",
    "end": "9563528"
  },
  {
    "text": "that's no longer the case, where all of these labs\nhave very fine grain control over what they get out of the models",
    "start": "9563528",
    "end": "9568688"
  },
  {
    "text": "through techniques like RLHF. - Although different labs are\ndefinitely different levels, like on one end of the spectrum is Google,",
    "start": "9568689",
    "end": "9576219"
  },
  {
    "text": "and then maybe OpenAI does\nless and Anthropic does less. And then, on the other end\nof the spectrum is like xAI.",
    "start": "9576219",
    "end": "9582626"
  },
  {
    "text": "- Yeah.\n- But they all have different forms of RLHF trying to make them a certain way. - And the important thing to say",
    "start": "9582626",
    "end": "9589774"
  },
  {
    "text": "is that no matter how you\nwant the model to behave, these RLHF and preference\ntuning techniques",
    "start": "9589774",
    "end": "9595593"
  },
  {
    "text": "also improve performance. So, on things like math\nevals and code evals, there is something innate to these",
    "start": "9595593",
    "end": "9601254"
  },
  {
    "text": "what is called contrastive loss functions. We could start to get into RL\nhere. We don't really need to. But RLHF also boosts performance",
    "start": "9601254",
    "end": "9608037"
  },
  {
    "text": "on anything from a chat task to a math problem to a code problem. So, it is becoming a much more\nuseful tool to these labs.",
    "start": "9608037",
    "end": "9615923"
  },
  {
    "text": "So, this takes us through the arc of, we've talked about pre-training,\nhard to get rid of things. We've talked about post-training",
    "start": "9615923",
    "end": "9621414"
  },
  {
    "text": "and how post-training, you can mess it up. It's a complex multifaceted optimization",
    "start": "9621414",
    "end": "9626766"
  },
  {
    "text": "with 10 to 100 person teams\nconverging a one artifact. It's really easy to not do it perfectly.",
    "start": "9626766",
    "end": "9632182"
  },
  {
    "text": "And then, there's the third case, which is what we talked about Gemini. The thing that was about Gemini is this was a served product where Gemini,",
    "start": "9632182",
    "end": "9639525"
  },
  {
    "text": "Google has their internal model weights, they've done all these\nprocesses that we talked about. And in the served product, what came out after this\nwas that they had a prompt",
    "start": "9639526",
    "end": "9646585"
  },
  {
    "text": "that they were rewriting user queries to boost diversity or something. And this just made it",
    "start": "9646585",
    "end": "9651622"
  },
  {
    "text": "that outputs were just blatantly wrong. It was a, some sort of\norganizational failure that had this prompt in that position.",
    "start": "9651622",
    "end": "9657893"
  },
  {
    "text": "And I think Google executives\nprobably have owned this. I don't pay that attention to that detail,",
    "start": "9657893",
    "end": "9662972"
  },
  {
    "text": "but it was just a mess up in execution that led to this ridiculous\nthing, but at the system level.",
    "start": "9662972",
    "end": "9668061"
  },
  {
    "text": "The model weights might have been fine. - So, at the very end of the pipeline, there was a rewriting. - To a something like a system prompt.",
    "start": "9668061",
    "end": "9674274"
  },
  {
    "text": "It was like the system prompt or what is called an industry\nis like you rewrite prompts.",
    "start": "9674274",
    "end": "9679873"
  },
  {
    "text": "So, especially for image models, if you're using DALL-E or ChatGPT, you can generate you an image,",
    "start": "9679873",
    "end": "9685814"
  },
  {
    "text": "you'll say draw me a beautiful car. With these leading image models, they benefit from highly\ndescriptive prompts.",
    "start": "9685815",
    "end": "9693840"
  },
  {
    "text": "So, what would happen is if you do that on ChatGPT, a language model behind the\nscenes will rewrite the prompt,",
    "start": "9693840",
    "end": "9699018"
  },
  {
    "text": "say make this more descriptive, and then that is passed\nto the image model. So, prompt rewriting is something that is used at multiple\nlevels of industry",
    "start": "9699018",
    "end": "9706300"
  },
  {
    "text": "and it's used effectively\nfor image models. And the Gemini example is\njust a failed execution.",
    "start": "9706300",
    "end": "9712254"
  },
  {
    "text": "- Big philosophical question here with RLHF to generalize,\nwhere is human input?",
    "start": "9712254",
    "end": "9717666"
  },
  {
    "text": "Human in the loop, human data most useful\nat the current stage. - For the past few years,",
    "start": "9721692",
    "end": "9727921"
  },
  {
    "text": "the highest cost human data\nhas been in these preferences, which is comparing,",
    "start": "9727921",
    "end": "9734423"
  },
  {
    "text": "I would say highest cost\nand highest total usage. So, a lot of money has gone to these parallelized comparisons",
    "start": "9734423",
    "end": "9739961"
  },
  {
    "text": "where you have two model outputs and a human is comparing\nbetween the two of them. In earlier years,",
    "start": "9739961",
    "end": "9745649"
  },
  {
    "text": "there was a lot of this\ninstruction tuning data. So, creating highly specific examples",
    "start": "9745650",
    "end": "9751259"
  },
  {
    "text": "to something like a Reddit question to a domain that you care about. Language models used to\nstruggle on math and code.",
    "start": "9751259",
    "end": "9756491"
  },
  {
    "text": "So, you would pay experts in math and code to come up with questions, and write detailed answers that were used to train the models.",
    "start": "9756491",
    "end": "9762673"
  },
  {
    "text": "Now, it is the case that\nthere are many model options that are way better than\nhumans at writing detailed",
    "start": "9762673",
    "end": "9769807"
  },
  {
    "text": "and eloquent answers for\nthings like model and code. So, they talked about this\nwith the Llama 3 release,",
    "start": "9769807",
    "end": "9775645"
  },
  {
    "text": "where they switched to\nusing Llama 3, 4, or 5b to write their answers for math and code.",
    "start": "9775645",
    "end": "9781058"
  },
  {
    "text": "But they, in their paper, talk about how they use\nextensive human preference data,",
    "start": "9781058",
    "end": "9786156"
  },
  {
    "text": "which is something that they\nhaven't gotten AIs to replace. There are other techniques in industry like constitutional AI,",
    "start": "9786157",
    "end": "9791286"
  },
  {
    "text": "where you use human data for preferences and AI for preferences. And I expect the AI part to scale faster than the human part.",
    "start": "9791286",
    "end": "9797988"
  },
  {
    "text": "But among the research\nthat we have access to is that humans are in this\nkind of preference loop.",
    "start": "9797988",
    "end": "9804458"
  },
  {
    "text": "- So, as reasoning becomes\nbigger and bigger and bigger, as we said, where's the\nrole of humans in that?",
    "start": "9804458",
    "end": "9811238"
  },
  {
    "text": "- It's even less prevalent. So, it's the remarkable thing\nabout these reasoning results",
    "start": "9811238",
    "end": "9816610"
  },
  {
    "text": "and especially the DeepSeek-R1\npaper is this result that they call DeepSeek-R1-Zero,",
    "start": "9816610",
    "end": "9821689"
  },
  {
    "text": "which is they took one of\nthese pre-trained models, they took DeepSeek-V3 base, and then they do this\nreinforcement learning optimization",
    "start": "9821690",
    "end": "9828359"
  },
  {
    "text": "on verifiable questions\nor verifiable rewards for a lot of questions\nand a lot of training.",
    "start": "9828360",
    "end": "9834068"
  },
  {
    "text": "And these reasoning\nbehaviors emerge naturally. So, these things like, wait, let me see, wait, let me check this.",
    "start": "9834069",
    "end": "9839798"
  },
  {
    "text": "Oh, that might be a mistake. And they emerge from only\nhaving questions and answers.",
    "start": "9839798",
    "end": "9845029"
  },
  {
    "text": "And when you're using the model, the part that you look\nat is the completion. So, in this case, all of that just emerges from\nthis large-scale RL training.",
    "start": "9845029",
    "end": "9853608"
  },
  {
    "text": "And that model, which the\nweights are available, has no human preferences\nadded into the post-training.",
    "start": "9853608",
    "end": "9860328"
  },
  {
    "text": "There are the DeepSeek-R1 full model has some of this human preference tuning, this RLHF after the reasoning stage.",
    "start": "9860328",
    "end": "9867827"
  },
  {
    "text": "But the very remarkable thing is that you can get these\nreasoning behaviors, and it's very unlikely that there's humans writing\nout reasoning chains.",
    "start": "9867828",
    "end": "9875180"
  },
  {
    "text": "It's very unlikely that\nthey somehow hacked OpenAI and they got access to OpenAI. o1's reasoning chains,",
    "start": "9875180",
    "end": "9880387"
  },
  {
    "text": "it's something about the\npre-trained language models and this RL training\nwhere you reward the model",
    "start": "9880387",
    "end": "9886189"
  },
  {
    "text": "for getting the question right. And therefore, it's\ntrying multiple solutions and it emerges this chain of thought.",
    "start": "9886189",
    "end": "9892858"
  },
  {
    "start": "9892000",
    "end": "10523000"
  },
  {
    "text": "- This might be a good place\nto mention the eloquent and the insightful tweet of the great",
    "start": "9892859",
    "end": "9900254"
  },
  {
    "text": "and the powerful Andrej Karpathy. I think he had a bunch of thoughts but one of them, \"Last thought,",
    "start": "9900254",
    "end": "9905616"
  },
  {
    "text": "not sure if this is obvious.\" You know something profound is coming when you're saying it's\nnot sure if it's obvious.",
    "start": "9905616",
    "end": "9911675"
  },
  {
    "text": "\"There are two major types of learning, in both children and in deep learning. There is, one, imitation\nlearning, watch and repeat,",
    "start": "9911675",
    "end": "9918963"
  },
  {
    "text": "i.e, pre-training, supervised fine-tuning, and, two, trial-and-error\nlearning, reinforcement learning.",
    "start": "9918963",
    "end": "9925641"
  },
  {
    "text": "My favorite simple example is AlphaGo. One is learning by\nimitating expert players.",
    "start": "9925641",
    "end": "9931144"
  },
  {
    "text": "Two is reinforcement\nlearning to win the game. Almost every single shocking\nresult of deep learning,",
    "start": "9931144",
    "end": "9936665"
  },
  {
    "text": "and the source of all magic is always two. Two is significantly more powerful.",
    "start": "9936665",
    "end": "9942776"
  },
  {
    "text": "Two is what surprises you. Two is when the paddle\nlearns to hit the ball behind the blocks and break out.",
    "start": "9942776",
    "end": "9949282"
  },
  {
    "text": "Two is when AlphaGo beats even Lee Sedol. And two is the aha\nmoment when the DeepSeek,",
    "start": "9949282",
    "end": "9956144"
  },
  {
    "text": "or o1, et cetera, discovers\nthat it works well to reevaluate your assumptions,",
    "start": "9956144",
    "end": "9962273"
  },
  {
    "text": "backtrack, try something else, et cetera. It's the solving strategies you see this model use\nin its chain of thought.",
    "start": "9962273",
    "end": "9969733"
  },
  {
    "text": "It's how it goes back and\nforth thinking to itself. These thoughts are emergent,\"\nthree exclamation points,",
    "start": "9969733",
    "end": "9977221"
  },
  {
    "text": "\"and this is actually seriously\nincredible, impressive, and new, and is publicly\navailable and documented.",
    "start": "9977221",
    "end": "9984544"
  },
  {
    "text": "The model could never learn\nthis with the imitation, because the cognition of the model",
    "start": "9984544",
    "end": "9990581"
  },
  {
    "text": "and the cognition of the\nhuman labeler is different. The human would never know to correctly annotate these\nkinds of solving strategies",
    "start": "9990581",
    "end": "9997251"
  },
  {
    "text": "and what they should even look like. They have to be discovered\nduring reinforcement learning",
    "start": "9997251",
    "end": "10002290"
  },
  {
    "text": "as empirically statistically useful towards the final outcome.\" Anyway, the AlphaZero sort\nof metaphor analogy here,",
    "start": "10002290",
    "end": "10009587"
  },
  {
    "text": "can you speak to that? - Yeah.\n- The magic of the chain of thought that he's referring to? - I think it's good to\nrecap AlphaGo and AlphaZero",
    "start": "10009587",
    "end": "10016657"
  },
  {
    "text": "because it plays nicely\nwith these analogies between imitation learning\nand learning from scratch. So, AlphaGo, the beginning of the process",
    "start": "10016657",
    "end": "10023588"
  },
  {
    "text": "was learning from humans where they had, they started the first, this is the first expert level go player",
    "start": "10023588",
    "end": "10029608"
  },
  {
    "text": "or chess player in\nDeepMind series of models, where they had some human data. And then, why it is called AlphaZero",
    "start": "10029608",
    "end": "10035587"
  },
  {
    "text": "is that there was zero\nhuman data in the loop, and that changed to AlphaZero made a model",
    "start": "10035587",
    "end": "10040678"
  },
  {
    "text": "that was dramatically more\npowerful for DeepMind. So, this remove of the human prior,",
    "start": "10040678",
    "end": "10045878"
  },
  {
    "text": "the human inductive bias, makes the final system far more powerful. This, we mentioned,\nbitter lesson hours ago",
    "start": "10045878",
    "end": "10052268"
  },
  {
    "text": "and this is all aligned with this. And then, there's been a lot",
    "start": "10052268",
    "end": "10057637"
  },
  {
    "text": "of discussion in language models. This is not new. This goes back to the whole QStar rumors,",
    "start": "10057637",
    "end": "10062900"
  },
  {
    "text": "which if you piece together the pieces is probably the start of OpenAI",
    "start": "10062900",
    "end": "10068040"
  },
  {
    "text": "figuring out its o1 stuff\nwhen last year in November. the QStar rumors came out. There's a lot of intellectual drive",
    "start": "10068040",
    "end": "10075366"
  },
  {
    "text": "to know when is something like this going to happen with language models? Because we know these\nmodels are so powerful and we know it has been\nso successful in the past.",
    "start": "10077021",
    "end": "10084619"
  },
  {
    "text": "And it is a reasonable\nanalogy that this new type of reinforcement learning training",
    "start": "10084619",
    "end": "10090620"
  },
  {
    "text": "for reasoning models is\nwhen the doors open to this. We don't yet have the\nequivalent of turn 37,",
    "start": "10090620",
    "end": "10097700"
  },
  {
    "text": "which is the famous turn where the DeepMind's AI playing ghost dumped Lee Sedol completely.",
    "start": "10097701",
    "end": "10104269"
  },
  {
    "text": "We don't have something that's\nthat level of focal point, but that doesn't mean that the approach to technology is different",
    "start": "10104269",
    "end": "10109398"
  },
  {
    "text": "and the impact of the general training, it's still incredibly new. - What do you think that point would be? What would be Move 37 for chain\nof thought, for reasoning?",
    "start": "10109398",
    "end": "10117626"
  },
  {
    "text": "- Scientific discovery. When you use this sort\nof reasoning problem and it just something\nwe fully don't expect.",
    "start": "10117626",
    "end": "10123515"
  },
  {
    "text": "- I think it's actually\nprobably simpler than that. It's probably something related\nto computer user robotics",
    "start": "10123515",
    "end": "10128892"
  },
  {
    "text": "rather than science discovery. Because the important aspect here is models take so much data to learn.",
    "start": "10128892",
    "end": "10137187"
  },
  {
    "text": "They're not sample-efficient. Trillions, they take the entire web over 10 trillion tokens to train on.",
    "start": "10137187",
    "end": "10145090"
  },
  {
    "text": "This would take a human\nthousands of years to read. A human does not, and humans\nknow most of the stuff,",
    "start": "10145090",
    "end": "10153012"
  },
  {
    "text": "a lot of the stuff models\nknow better than it. Humans are way, way, way\nmore sample-efficient. And that is because of the self-play.",
    "start": "10153012",
    "end": "10158821"
  },
  {
    "text": "How does a baby learn what its body is as it sticks its foot in its mouth and it says, oh, this is my body.",
    "start": "10158821",
    "end": "10166235"
  },
  {
    "text": "It sticks its hand in its mouth and it calibrates its touch on its fingers with the most sensitive\ntouch thing on its tongue.",
    "start": "10166236",
    "end": "10172334"
  },
  {
    "text": "Like is how babies learn. And it's just self-play over and over and over and over again.",
    "start": "10172334",
    "end": "10177413"
  },
  {
    "text": "And now, we have something\nthat is similar to that with these verifiable proofs,",
    "start": "10177413",
    "end": "10184082"
  },
  {
    "text": "whether it's a unit test and code or a mathematical verifiable task, generate many traces of reasoning.",
    "start": "10184082",
    "end": "10191980"
  },
  {
    "text": "And keep branching them out,\nkeep branching them out. And then, check at the end, hey, which one actually has the right answer? Most of 'em are wrong. Great.",
    "start": "10191981",
    "end": "10197801"
  },
  {
    "text": "These are the few that are right. Maybe we use some sort of\nreward model outside of this to select even the best\none to preference as well.",
    "start": "10197801",
    "end": "10203330"
  },
  {
    "text": "But now, you've started to get better and better at these benchmarks. And so, you've seen\nover the last six months a skyrocketing in a lot\nof different benchmarks.",
    "start": "10203330",
    "end": "10211460"
  },
  {
    "text": "- All math and code benchmarks\nwere pretty much solved except for frontier math, which is designed to be almost questions",
    "start": "10211460",
    "end": "10217400"
  },
  {
    "text": "that aren't practical to most people, 'cause they're exam level,\nopen math problem type things.",
    "start": "10217401",
    "end": "10224219"
  },
  {
    "text": "So, it's like on the math problems that are somewhat reasonable, which is like somewhat\ncomplicated word problems or coding problems.",
    "start": "10224719",
    "end": "10230818"
  },
  {
    "text": "It's just what Dylan is saying. - So, the thing here is that these are only with verifiable tasks.",
    "start": "10230819",
    "end": "10236839"
  },
  {
    "text": "We earlier showed an example\nof the really interesting, like what happens when chain of thought is to a non-verifiable thing.",
    "start": "10236839",
    "end": "10242707"
  },
  {
    "text": "It's just like a human chatting with a, thinking about what's novel\nfor humans, a unique thought.",
    "start": "10242707",
    "end": "10248018"
  },
  {
    "text": "But this task and form of training only works when it's verifiable.",
    "start": "10248018",
    "end": "10253123"
  },
  {
    "text": "And from here, the thought is, okay, we can continue to scale\nthis current training method",
    "start": "10253123",
    "end": "10258193"
  },
  {
    "text": "by increasing the number\nof verifiable tasks. In math and coding, coding\nprobably has a lot more to go.",
    "start": "10258193",
    "end": "10264030"
  },
  {
    "text": "Math has a lot less to go in terms of what are verifiable things. Can I create a solver that then I generate trajectories toward,",
    "start": "10264030",
    "end": "10271192"
  },
  {
    "text": "or traces towards,\nreasoning traces towards, and then prune the ones that don't work and keep the ones that do work? Well, those are gonna be\nsolved pretty quickly.",
    "start": "10271193",
    "end": "10277842"
  },
  {
    "text": "But even if you've solved math, you have not actually\ncreated intelligence. And so, this is where\nI think the aha moment",
    "start": "10277842",
    "end": "10285456"
  },
  {
    "text": "of computer use or robotics will come in, because now you have a sandbox",
    "start": "10285456",
    "end": "10290941"
  },
  {
    "text": "or a playground that is\ninfinitely verifiable. Did you, messing around on the internet,",
    "start": "10290942",
    "end": "10296751"
  },
  {
    "text": "there are so many actions that you can do that are verifiable. It'll start off with log into a website, create an account, click a\nbutton here, blah, blah, blah.",
    "start": "10296751",
    "end": "10303512"
  },
  {
    "text": "But it'll then get to the point, where it's, hey, go do a task on Tasker or whatever these other, all\nthese various task websites.",
    "start": "10303512",
    "end": "10309280"
  },
  {
    "text": "Hey, go get hundreds of likes. And it's gonna fail. It's gonna spawn hundreds of accounts,",
    "start": "10309280",
    "end": "10314370"
  },
  {
    "text": "it's gonna fail on most of them, but this one got to a thousand, great. Now, you've reached the verifiable thing. And you just keep iterating\nthis loop over and over.",
    "start": "10314371",
    "end": "10320971"
  },
  {
    "text": "And that's when... And same with robotics. That's where you have an infinite\nplayground of tasks like,",
    "start": "10320971",
    "end": "10326210"
  },
  {
    "text": "hey, did I put the ball in the bucket, all the way to like,\noh, did I build a car? There's a whole trajectory",
    "start": "10326210",
    "end": "10331948"
  },
  {
    "text": "to speed run or what models can do. But at some point, I\ntruly think that like,",
    "start": "10331948",
    "end": "10337599"
  },
  {
    "text": "we'll spawn models, and initially, all the\ntraining will be in sandboxes, but then at some point, the language model pre-training",
    "start": "10337599",
    "end": "10343877"
  },
  {
    "text": "is gonna be dwarfed by what is\nthis reinforcement learning? You'll pre-train a multimodal model",
    "start": "10343877",
    "end": "10349238"
  },
  {
    "text": "that can see, that can\nread, that can write, blah, blah, blah, whatever,\nvision, audio, et cetera, but then you'll have it play\nin a sandbox infinitely,",
    "start": "10349238",
    "end": "10358045"
  },
  {
    "text": "and figure out math, figure out code, figure out navigating the web, figure out operating a robot arm. And then, it'll learn so much.",
    "start": "10358045",
    "end": "10365536"
  },
  {
    "text": "And the aha moment I think\nwill be when this is available to then create something that's not good.",
    "start": "10365536",
    "end": "10370915"
  },
  {
    "text": "Like, oh cool. Part of it was figuring\nout how to use the web. Now, all of a sudden, it's\nfigured out really well",
    "start": "10370915",
    "end": "10376136"
  },
  {
    "text": "how to just get hundreds\nof thousands of followers that are real and real\nengagement on Twitter, because all of a sudden, this is one of the things\nthat are verifiable.",
    "start": "10376136",
    "end": "10382689"
  },
  {
    "text": "- And maybe not just\nengagement, but make money. - Yes, of course.\n- I become an... I mean, that could be the thing",
    "start": "10382690",
    "end": "10388174"
  },
  {
    "text": "where almost fully automated, it makes $10 million\nby being an influencer,",
    "start": "10388174",
    "end": "10393909"
  },
  {
    "text": "selling a product, creating the product. And I'm not referring\nto like a hype product,",
    "start": "10394977",
    "end": "10400566"
  },
  {
    "text": "but an actual product or like, holy shit, this\nthing created a business. It's running it, it's\nthe face of the business.",
    "start": "10400566",
    "end": "10407767"
  },
  {
    "text": "That kind of thing. Or maybe a number one song, like it creates the whole infrastructure",
    "start": "10407767",
    "end": "10413627"
  },
  {
    "text": "required to create the\nsong, to be the influence that represents that\nsong, that kind of thing. And makes a lot of...",
    "start": "10413627",
    "end": "10419229"
  },
  {
    "text": "That could be the mo... I mean, our culture respects\nmoney in that kind of way.",
    "start": "10419229",
    "end": "10424601"
  },
  {
    "text": "- And it's verifiable, right? - [Lex] It's verifiable. - The bank account can't lie.\n- Exactly. - There is surprising evidence\nthat once you set up the ways",
    "start": "10424601",
    "end": "10433062"
  },
  {
    "text": "of collecting the verifiable\ndomain that this can work. There's been a lot of research before this R1 on math problems,",
    "start": "10433062",
    "end": "10440673"
  },
  {
    "text": "and they approach math\nwith language models just by increasing the number of samples. So, you can just try\nagain and again and again.",
    "start": "10440673",
    "end": "10446449"
  },
  {
    "text": "And you look at the amount of times that the language models get it right. And what we see",
    "start": "10446449",
    "end": "10451822"
  },
  {
    "text": "is that even very bad models\nget it right sometimes. And the whole idea behind\nreinforcement learning",
    "start": "10452932",
    "end": "10458160"
  },
  {
    "text": "is that you can learn\nfrom very sparse rewards. So, the space of language\nand the space of tokens,",
    "start": "10458160",
    "end": "10464926"
  },
  {
    "text": "whether you're generating language or tasks for a robot is so big that you might say that it's like,",
    "start": "10464926",
    "end": "10470422"
  },
  {
    "text": "the tokenizer for a language model can be like 200,000 things. So, at each step, it can sample\nfrom that big of a space.",
    "start": "10470422",
    "end": "10476264"
  },
  {
    "text": "So, if it can generate a bit of a signal that it can climb onto, that's the whole field of RL is around,",
    "start": "10476264",
    "end": "10482957"
  },
  {
    "text": "is learning from sparse rewards. And the same thing has played out in math where it's like very weak models",
    "start": "10482957",
    "end": "10488408"
  },
  {
    "text": "that sometimes generate answers, where you see research already that you can boost their math scores,",
    "start": "10488408",
    "end": "10493427"
  },
  {
    "text": "you can do this sort of\nRL training for math. It might not be as effective, but if you take a 1\nbillion parameter model,",
    "start": "10493427",
    "end": "10499917"
  },
  {
    "text": "so something 600 times\nsmaller than DeepSeek, you can boost its grade school math scores",
    "start": "10499917",
    "end": "10504955"
  },
  {
    "text": "very directly with a small\namount of this training. So, it's not to say that\nthis is coming soon,",
    "start": "10504955",
    "end": "10510548"
  },
  {
    "text": "setting up the verification\ndomains is extremely hard, and there's a lot of nuance in this,",
    "start": "10510548",
    "end": "10515680"
  },
  {
    "text": "but there are some basic things\nthat we have seen before, where it's at least expectable\nthat there's a domain",
    "start": "10515680",
    "end": "10522076"
  },
  {
    "text": "and there's a chance that this works. - All right, so we have fun\nthings happening in real time. This is a good opportunity",
    "start": "10522077",
    "end": "10527126"
  },
  {
    "start": "10523000",
    "end": "11671000"
  },
  {
    "text": "to talk about other\nreasoning models, o1, o3.",
    "start": "10527126",
    "end": "10530876"
  },
  {
    "text": "Just now, OpenAI as perhaps\nexpected, released o3-mini.",
    "start": "10532237",
    "end": "10536820"
  },
  {
    "text": "What are we expecting from\nthe different flavors? Can you just lay out the\ndifferent flavors of the o models",
    "start": "10538647",
    "end": "10544542"
  },
  {
    "text": "and from Gemini, the reasoning model? - Something I would say\nabout these reasoning models is we talked a lot",
    "start": "10544542",
    "end": "10550157"
  },
  {
    "text": "about reasoning training on math and code. And what is done is that\nyou have the base model, we've talked about a lot on the internet.",
    "start": "10550157",
    "end": "10556673"
  },
  {
    "text": "You do this large-scale reasoning training with reinforcement learning. And then, what the DeepSeek\npaper detailed in this R1 paper,",
    "start": "10556673",
    "end": "10564580"
  },
  {
    "text": "which for me is one of\nthe big open questions on how do you do this, is\nthat they did reasoning-heavy,",
    "start": "10564580",
    "end": "10570697"
  },
  {
    "text": "but very standard post-training techniques after the large-scale reasoning RL. So, they did the same things with a form",
    "start": "10570697",
    "end": "10577299"
  },
  {
    "text": "of instruction tuning\nthrough rejection sampling, which is essentially heavily\nfiltered instruction tuning",
    "start": "10577299",
    "end": "10582726"
  },
  {
    "text": "with some reward models. And then, they did this RLHF,\nbut they made it math-heavy. So, some of this transfer,",
    "start": "10582726",
    "end": "10589485"
  },
  {
    "text": "we've looked at this\nphilosophical example early on. One of the big open questions",
    "start": "10589485",
    "end": "10594947"
  },
  {
    "text": "is how much does this transfer? If we bring in domains after\nthe reasoning training,",
    "start": "10594947",
    "end": "10600067"
  },
  {
    "text": "are all the models gonna be become eloquent\nwriters by reasoning? Is this philosophy stuff gonna be open?",
    "start": "10600067",
    "end": "10605213"
  },
  {
    "text": "We don't know in the research of how much this will transfer. There's other things about\nhow we can make soft verifiers",
    "start": "10605213",
    "end": "10610498"
  },
  {
    "text": "and things like this. But there is more\ntraining after reasoning, which makes it easier to\nuse these reasoning models.",
    "start": "10610498",
    "end": "10616547"
  },
  {
    "text": "And that's what we're using right now. So, if we're gonna talk\nabout o3-mini and o1, these have gone through\nthese extra techniques",
    "start": "10616547",
    "end": "10622251"
  },
  {
    "text": "that are designed for human preferences after being trained to elicit reasoning. - I think one of the things\nthat people are ignoring",
    "start": "10622251",
    "end": "10629351"
  },
  {
    "text": "is Google's Gemini Flash Thinking - Yeah.\n- is both cheaper than R1 and better.\n- Yeah.",
    "start": "10629351",
    "end": "10635591"
  },
  {
    "text": "- And they released it in\nthe beginning of December. - [Lex] And nobody's talking about it. - No one cares.\n- It has a different flavor to it. Its behavior is less expressive\nthan something like o1",
    "start": "10635591",
    "end": "10642617"
  },
  {
    "text": "or it has fewer tracks than it is on. Qwen released a model last fall, QWQ,",
    "start": "10642617",
    "end": "10648823"
  },
  {
    "text": "which was their preview reasoning model. And DeepSeek had R1-Lite last fall, where these models felt\nlike they're on rails,",
    "start": "10648823",
    "end": "10655638"
  },
  {
    "text": "where they really, really\nonly can do math and code. And o1 is, it can answer anything.",
    "start": "10655638",
    "end": "10661142"
  },
  {
    "text": "It might not be perfect for some tasks, but it's flexible, it\nhas some richness to it.",
    "start": "10661142",
    "end": "10666771"
  },
  {
    "text": "And this is the art of how is a model a\nlittle bit undercooked?",
    "start": "10666771",
    "end": "10671926"
  },
  {
    "text": "It's like, it's good to\nget a model out the door, but it's hard to gauge and it takes a lot of taste to be like,",
    "start": "10671926",
    "end": "10677688"
  },
  {
    "text": "is this a full-fledged model? Can I use this for everything? And they're probably more\nsimilar for math and code.",
    "start": "10677688",
    "end": "10683857"
  },
  {
    "text": "My quick read is that Gemini Flash is not trained the same way as o1,",
    "start": "10683857",
    "end": "10688601"
  },
  {
    "text": "but taking an existing training stack, adding reasoning to it. So, taking a more normal training stack",
    "start": "10690229",
    "end": "10696069"
  },
  {
    "text": "and adding reasoning to it. And I'm sure they're gonna have more, I mean, they've done quick releases on Gemini Flash, the reasoning,",
    "start": "10696069",
    "end": "10702768"
  },
  {
    "text": "and this is the second\nversion from the holidays. It's evolving fast and it takes longer to\nmake this training stack",
    "start": "10702768",
    "end": "10710918"
  },
  {
    "text": "where you're doing this large-scale RL.\n- Ask it the same question from earlier, the one about the- - The human nature.\n- Yeah.",
    "start": "10710918",
    "end": "10717672"
  },
  {
    "text": "- [Lex] What was the human nature one? - The way I can ramble, why I\ncan ramble about this so much",
    "start": "10717672",
    "end": "10723342"
  },
  {
    "text": "is that we've been working on this at AI2 before o1 was\nfully available to everyone,",
    "start": "10723342",
    "end": "10729401"
  },
  {
    "text": "and before R1, which is essentially using this\nRL training for fine-tuning. We use this in our Tulu series of models.",
    "start": "10729401",
    "end": "10736384"
  },
  {
    "text": "And you can elicit the same behaviors where you say like weight and so much on,",
    "start": "10736384",
    "end": "10741394"
  },
  {
    "text": "but it's subtle late\nin the training process that this kind of reasoning\nexpression is much lighter.",
    "start": "10741394",
    "end": "10746772"
  },
  {
    "text": "So, there's essentially a gradation and just how much of this RL training you put into it determines\nhow the output looks.",
    "start": "10746772",
    "end": "10753216"
  },
  {
    "text": "- So, we're now using Gemini 2.0 Flash Thinking Experimental 01-21.",
    "start": "10753216",
    "end": "10759427"
  },
  {
    "text": "- It summarized the prompt as humans self-domesticated apes. (Nathan and Lex laughing)",
    "start": "10761096",
    "end": "10767862"
  },
  {
    "text": "- Perspective. Okay. All right. So, wait, is this\nrevealing the reasoning? Here's why this is a novel. Oh, okay.",
    "start": "10767862",
    "end": "10774134"
  },
  {
    "text": "- You click to expand.\n- Oh, yeah, click to expand. - Okay. Analyze the request.",
    "start": "10774134",
    "end": "10779222"
  },
  {
    "text": "Novel is the key word. - See how it just looks\na little different. It looks like a normal output. (chuckles)",
    "start": "10779222",
    "end": "10785785"
  },
  {
    "text": "- Yeah, it's... In some sense, it's better\nstructured, it makes more sense. And-\n- When it latched onto human",
    "start": "10785786",
    "end": "10792708"
  },
  {
    "text": "and then it went into organisms, and oh wow.\n(Nathan laughing) - Apex predator, focus on domestication.",
    "start": "10792708",
    "end": "10799217"
  },
  {
    "text": "Apply domestication to humans, explore the idea of self-domestication. (Lex and Nathan laughing)",
    "start": "10800418",
    "end": "10805477"
  },
  {
    "text": "- Not good, not good. - Where is this going? Refine, articulate the insight.",
    "start": "10805477",
    "end": "10810816"
  },
  {
    "text": "Greater facial expressiveness\nand communication ability. Yes. (chuckles) Plasticity and adaptability. Yes.",
    "start": "10810817",
    "end": "10817340"
  },
  {
    "text": "Dependence on social groups. Yes. All right. And self-critique and refined further.",
    "start": "10817340",
    "end": "10823158"
  },
  {
    "text": "Wow. Is this truly novel? Is it well-supported?\n(Nathan chuckles)",
    "start": "10823159",
    "end": "10828683"
  },
  {
    "text": "So on and so forth. And the insight is getting at is humans are not just social animals,",
    "start": "10828683",
    "end": "10834633"
  },
  {
    "text": "but profoundly self-domesticated apes. And this self-domestication is the key to understanding our unique\ncognitive and social abilities.",
    "start": "10834633",
    "end": "10843180"
  },
  {
    "text": "Self-domesticated apes.\n(Nathan laughing) Self-\n- I prefer the DeepSeek response. (chuckles)",
    "start": "10843180",
    "end": "10848652"
  },
  {
    "text": "- Self do... I mean, it's novel. The insight is novel.\n(Nathan laughing)",
    "start": "10848652",
    "end": "10854350"
  },
  {
    "text": "That's like a good book\ntitle, self-domesticated apes. There could be a case made for that.",
    "start": "10854350",
    "end": "10859524"
  },
  {
    "text": "Yeah, it's cool and it's\nrevealing the reasoning. It's magical. It's magical.",
    "start": "10859524",
    "end": "10864500"
  },
  {
    "text": "This is really powerful. Hello, everyone, this is Lex\nwith a quick intermission,",
    "start": "10865405",
    "end": "10872421"
  },
  {
    "text": "recorded after the podcast. Since we've reviewed\nresponses from DeepSeek-R1",
    "start": "10872421",
    "end": "10877830"
  },
  {
    "text": "and Gemini Flash 2.0 Thinking\nduring this conversation, I thought at this moment, it would be nice",
    "start": "10877830",
    "end": "10883267"
  },
  {
    "text": "to insert myself quickly doing\nthe same for OpenAI o1 Pro",
    "start": "10883267",
    "end": "10888267"
  },
  {
    "text": "and o3-mini with the same prompt, the prompt being, give one truly\nnovel insight about humans.",
    "start": "10888740",
    "end": "10896652"
  },
  {
    "text": "And I thought I would in general, give my vibe check and\nvibe-based anecdotal report",
    "start": "10898061",
    "end": "10903638"
  },
  {
    "text": "on my own experiences with\nthe new o-3 mini model, now that I got a chance to\nspend many hours with it",
    "start": "10906693",
    "end": "10913139"
  },
  {
    "text": "in different kinds of\ncontexts and applications. So, I would probably\ncategorize this question as a,",
    "start": "10913139",
    "end": "10919351"
  },
  {
    "text": "let's say, open-ended\nphilosophical question. And in particular, the emphasis on novelty",
    "start": "10919351",
    "end": "10924842"
  },
  {
    "text": "I think is a nice way to test one of the\ncapabilities of the model, which is come up with\nsomething that makes you pause",
    "start": "10924842",
    "end": "10933910"
  },
  {
    "text": "and almost surprise you\nwith its brilliance. So, that said, my general review",
    "start": "10933910",
    "end": "10939323"
  },
  {
    "text": "after running each of the models on this question a bunch of times is that o1 Pro consistently\ngave brilliant answers.",
    "start": "10939323",
    "end": "10947285"
  },
  {
    "text": "Once, they gave me\npause and made me think, both cutting in its insight",
    "start": "10948742",
    "end": "10954169"
  },
  {
    "text": "and just really nicely phrased\nwith clarity, with nuance,",
    "start": "10954169",
    "end": "10958919"
  },
  {
    "text": "over and over consistently\ngenerating the best answers. After that is R1, which\nwas less consistent,",
    "start": "10960180",
    "end": "10966620"
  },
  {
    "text": "but again, deliver brilliance. Gemini Flash 2.0 Thinking was third.",
    "start": "10966620",
    "end": "10972383"
  },
  {
    "text": "And last was o3-mini actually. It often gave quite a generic answer,",
    "start": "10972383",
    "end": "10979402"
  },
  {
    "text": "at least to my particular sensibilities. That said, in a bunch\nof other applications that I tested for brainstorming purposes,",
    "start": "10979403",
    "end": "10987785"
  },
  {
    "text": "it actually worked extremely\nwell and often outperformed R1.",
    "start": "10987785",
    "end": "10992785"
  },
  {
    "text": "But on this open-ended\nphilosophical question, it did consistently worse. Now, another important element",
    "start": "10993518",
    "end": "10999224"
  },
  {
    "text": "for each of these models is\nhow the reasoning is presented. DeepSeek-R1 shows the full\nchain of thought tokens,",
    "start": "10999224",
    "end": "11006665"
  },
  {
    "text": "which I personally just love. For these open-ended\nphilosophical questions, it's really, really interesting",
    "start": "11006665",
    "end": "11012581"
  },
  {
    "text": "to see the model think through it, but really also just stepping back, me as a person who\nappreciates intelligence",
    "start": "11012581",
    "end": "11020106"
  },
  {
    "text": "and reasoning and reflection, reading these kind of chain\nof thought, raw tokens of R1,",
    "start": "11020107",
    "end": "11025596"
  },
  {
    "text": "there's something genuinely beautiful about observing the path of deliberation",
    "start": "11025596",
    "end": "11031604"
  },
  {
    "text": "in an intelligent system. I think we don't always have that explicitly\nlaid out for us humans.",
    "start": "11031604",
    "end": "11039199"
  },
  {
    "text": "So, to see it in another\nintelligence system, the non-linearity of it",
    "start": "11039200",
    "end": "11044834"
  },
  {
    "text": "akin to \"Ulysses\" or \"Finnegans\nWake\" by James Joyce. It's just beautiful to watch.",
    "start": "11044834",
    "end": "11049993"
  },
  {
    "text": "Anyways, we discussed in the episode, DeepSeek-R1 talked about humans being able",
    "start": "11049994",
    "end": "11055193"
  },
  {
    "text": "to convert selfish desires\ninto cooperative systems by collectively pretending abstract rules like money laws and rights are real.",
    "start": "11055193",
    "end": "11062801"
  },
  {
    "text": "And these shared\nhallucinations act as games where competition is secretly redirected",
    "start": "11062801",
    "end": "11068490"
  },
  {
    "text": "to benefit the group, turning\nconflict into society's fuel. Gemini 2.0 Flash Thinking said,",
    "start": "11068490",
    "end": "11075133"
  },
  {
    "text": "\"Humans are not just social animals, but self-domesticated apes, and this self-domestication is the key",
    "start": "11075133",
    "end": "11081298"
  },
  {
    "text": "to understanding our unique\ncognitive and social abilities.\" Now, it's important to say that the chain of thought\nthere was really interesting.",
    "start": "11081298",
    "end": "11089011"
  },
  {
    "text": "It was looking through\nthe entire evolution of life on earth,\nconsidering apex predators,",
    "start": "11089011",
    "end": "11095180"
  },
  {
    "text": "and considering how from that,\nwe ended up to where we are. I think that domestication",
    "start": "11096240",
    "end": "11102235"
  },
  {
    "text": "by choice is a really interesting angle. Again, it's one of those things when somebody presents a different angle",
    "start": "11102236",
    "end": "11108840"
  },
  {
    "text": "on a seemingly obvious thing,\nit just makes me smile. And the same with DeepSeek-R1, that these hallucinations\nof money laws and rights",
    "start": "11108840",
    "end": "11118586"
  },
  {
    "text": "and us collectively\npretending like it's real, and we play games with them\nthat look like competition,",
    "start": "11118586",
    "end": "11124673"
  },
  {
    "text": "when secretly, we're just\ncooperating with each other. And that is the fuel of\nprogress, beautifully put.",
    "start": "11124673",
    "end": "11131317"
  },
  {
    "text": "Now, OpenAI o1 Pro consistently over and\nover, delivered bangers. I can go through many of them,",
    "start": "11131317",
    "end": "11137324"
  },
  {
    "text": "but the first one was,\n\"Humans are the only species that turns raw materials\ninto symbolic resources,",
    "start": "11137325",
    "end": "11143017"
  },
  {
    "text": "then uses those symbols to reorganize the very\nmaterials they came from, creating a closed feedback loop\nbetween meaning and matter.\"",
    "start": "11143017",
    "end": "11152507"
  },
  {
    "text": "Here, I just ran it again, (laughs) banger after banger, I'm telling you. \"Humans are unique among known species",
    "start": "11152946",
    "end": "11159206"
  },
  {
    "text": "in that they simultaneously\nrewrite two layers of reality, the external world and their\nown private mental landscapes,",
    "start": "11159206",
    "end": "11166468"
  },
  {
    "text": "and then merge these two rewritten layers into a continuous personal narrative",
    "start": "11166468",
    "end": "11172267"
  },
  {
    "text": "that feels objectively true, feels true.\"",
    "start": "11172267",
    "end": "11175684"
  },
  {
    "text": "This is poetry. Okay. And then, o3-mini-high for me was smart, fast actually,\nand kind of generic.",
    "start": "11177626",
    "end": "11186266"
  },
  {
    "text": "Never quite got there for me. So, here's the first\none I got from o3-mini. \"Humans are not fixed beings",
    "start": "11189555",
    "end": "11196405"
  },
  {
    "text": "but rather ongoing\nnarratives, dynamic stories that we continuously write,\nedit, and reinterpret.",
    "start": "11196405",
    "end": "11202704"
  },
  {
    "text": "This narrative plasticity is more than just memory\nor self-reflection. It's an intrinsic cognitive process",
    "start": "11202704",
    "end": "11209417"
  },
  {
    "text": "that acts like an internal\nerror correction system. It allows us to adapt our identities",
    "start": "11209417",
    "end": "11214954"
  },
  {
    "text": "and values over time in response to new experiences,\nchallenges, and social context.",
    "start": "11214954",
    "end": "11220131"
  },
  {
    "text": "Now, it almost sneaks up to something approximating cutting insight with narrative plasticity in quotes.",
    "start": "11220131",
    "end": "11227406"
  },
  {
    "text": "But then, it goes back to\nthe generic, I don't know. All of these models are\nincredible for different reasons.",
    "start": "11227406",
    "end": "11233347"
  },
  {
    "text": "There's a lot of concerns as\nwe discussed in this episode, but there's a lot of reasons\nto be excited as well.",
    "start": "11233347",
    "end": "11241425"
  },
  {
    "text": "And I've probably spoken for too long. I am severely sleep-deprived,\nborderline delirious.",
    "start": "11241425",
    "end": "11249331"
  },
  {
    "text": "So, hopefully, some of this made sense. And now, dear friends,\nback to the episode.",
    "start": "11249331",
    "end": "11255587"
  },
  {
    "text": "- I think when you, to Nathan's point, when you look at the reasoning models,",
    "start": "11256903",
    "end": "11263690"
  },
  {
    "text": "to me, even when I used R1 versus o1, there was that sort of rough",
    "start": "11263690",
    "end": "11268810"
  },
  {
    "text": "or edges around the corner feeling. And Flash Thinking, earlier,\nI didn't use this version,",
    "start": "11268810",
    "end": "11274739"
  },
  {
    "text": "but the one from December, and it definitely had that rough edges around the corner feeling, where it's just not fleshed\nout in as many ways.",
    "start": "11274739",
    "end": "11282288"
  },
  {
    "text": "Sure, they added math and coding capabilities\nvia these verifiers in RL, but it feels like they lost\nsomething in certain areas.",
    "start": "11282288",
    "end": "11290382"
  },
  {
    "text": "And o1 is worse performing than chat in many areas as well, to be clear. - Not by a lot.\n- Not by a lot though.",
    "start": "11290382",
    "end": "11296899"
  },
  {
    "text": "And R1 definitely felt to me like it was worse than\nV3 in certain areas,",
    "start": "11296900",
    "end": "11302033"
  },
  {
    "text": "like doing this RL\nexpressed and learned a lot, but then it weakened in other areas.",
    "start": "11302033",
    "end": "11307426"
  },
  {
    "text": "And so, I think that's\none of the big differences between these models, and what o1 offers.",
    "start": "11307426",
    "end": "11313932"
  },
  {
    "text": "And then, OpenAI has o1 Pro. And what they did with o3,\nwhich is like also very unique,",
    "start": "11313932",
    "end": "11319093"
  },
  {
    "text": "is that they stacked search\non top of chain of thought. And so, chain of thought is\none thing, where it's able,",
    "start": "11319093",
    "end": "11325444"
  },
  {
    "text": "it's one chain, it backtracks,\ngoes back and forth, but how they solved the ARC-AGI challenge",
    "start": "11325444",
    "end": "11330773"
  },
  {
    "text": "was not just the chain of thought. It was also sampling many times, i.e, running them in\nparallel, and then selecting.",
    "start": "11330773",
    "end": "11338373"
  },
  {
    "text": "- Is running in parallel actually search? Because I don't know if we have the full information\non how o1 Pro works.",
    "start": "11338373",
    "end": "11343573"
  },
  {
    "text": "So, like I'm not, I don't\n- That's right. - have enough information\n- Agree. - to confidently say that it is searched. - It is parallel samples.\n- Yeah.",
    "start": "11343573",
    "end": "11349016"
  },
  {
    "text": "And then, what?\n- And it selects something. - And we don't know what\nthe selection function is. The reason why we're debating is because since o1 was announced,",
    "start": "11349016",
    "end": "11356232"
  },
  {
    "text": "there's been a lot of\ninterest in techniques called Monte Carlo Tree Search, - Mm-hmm.\n- which is where you will break down the chain of thought into intermediate steps.",
    "start": "11356232",
    "end": "11362822"
  },
  {
    "text": "We haven't defined chain of thought. Chain of thought is from\na paper from years ago where you introduced the\nidea to ask a language model",
    "start": "11362822",
    "end": "11369812"
  },
  {
    "text": "that at the time, was\nmuch less easy to use. You would say, let's verify step by step, and it would induce the model",
    "start": "11369813",
    "end": "11376090"
  },
  {
    "text": "to do this bulleted list of steps. Chain of thought is now\nalmost a default in models where if you ask in a math question,",
    "start": "11376090",
    "end": "11382379"
  },
  {
    "text": "you don't need to tell\nit to think step by step. And the idea with Monte Carlo Tree Search is that you would take an\nintermediate point in that train,",
    "start": "11382379",
    "end": "11389750"
  },
  {
    "text": "do some sort of expansion,\nspend more compute, and then select the right one. That's like a very complex form of search",
    "start": "11389750",
    "end": "11395442"
  },
  {
    "text": "that has been used in things like MuZero and AlphaZero potentially. I know MuZero does this.",
    "start": "11395442",
    "end": "11400722"
  },
  {
    "text": "- Another form of search is just asking five different\npeople, and then taking the majority answer.\n- Yes. - There's a variety of like,\n(Nathan chuckles)",
    "start": "11400722",
    "end": "11407400"
  },
  {
    "text": "it could be complicated,\nit could be simple. We don't know what it is,\njust that they are not",
    "start": "11407400",
    "end": "11412416"
  },
  {
    "text": "just issuing one chain\nof thought in sequence. - Yeah.\n- They're launching many in parallel, and in the ARC-AGI,",
    "start": "11412416",
    "end": "11418421"
  },
  {
    "text": "they launched a thousand\nin parallel for their, the one that really shocked everyone, that beat the benchmark",
    "start": "11418421",
    "end": "11423950"
  },
  {
    "text": "was they would launch\na thousand in parallel, and then they would get the right answer, like 80% of the time or 70%\nof the time, 90 maybe even.",
    "start": "11423950",
    "end": "11430591"
  },
  {
    "text": "Whereas if they just launched\none, it was like 30%. - There are many extensions to this.",
    "start": "11430591",
    "end": "11435769"
  },
  {
    "text": "I would say the simplest one\nis that our language models to date have been designed\nto give the right answer",
    "start": "11435769",
    "end": "11441731"
  },
  {
    "text": "the highest percentage of\nthe time in one response. And we are now opening\nthe door to different ways",
    "start": "11441731",
    "end": "11447567"
  },
  {
    "text": "of running inference on\nour models in which we need to reevaluate many parts\nof the training process,",
    "start": "11447568",
    "end": "11453041"
  },
  {
    "text": "which normally opens the\ndoor to more progress, but we don't know if OpenAI changed a lot",
    "start": "11453041",
    "end": "11458319"
  },
  {
    "text": "or if just sampling\nmore in multiple choices what they're doing, or if it's something more complex where they changed the training",
    "start": "11458319",
    "end": "11463926"
  },
  {
    "text": "and they know that the inference mode is going to be different. - So, we're talking\nabout o1 Pro $200 a month",
    "start": "11463926",
    "end": "11470878"
  },
  {
    "text": "and they're losing money. So, the thing that we're referring to,",
    "start": "11470878",
    "end": "11475873"
  },
  {
    "text": "this fascinating exploration\nof the test-time compute space,",
    "start": "11476787",
    "end": "11481787"
  },
  {
    "text": "is that actually possible? Do we have enough compute for that? Does the financials make sense?",
    "start": "11482955",
    "end": "11487976"
  },
  {
    "text": "- So, the fantastic thing is, and it's in the thing\nthat I pulled up earlier, but the cost for GPT-3 has plummeted.",
    "start": "11487976",
    "end": "11496002"
  },
  {
    "text": "If you scroll up just\na few images, I think. The important thing about like, hey, is cost limiting factor here?",
    "start": "11497897",
    "end": "11504329"
  },
  {
    "text": "My view is that we'll have\nreally awesome intelligence before we have, like AGI before we have it\npermeate throughout the economy.",
    "start": "11504329",
    "end": "11512334"
  },
  {
    "text": "And this is why that reason is. GPT-3 was trained in what, 2020, 2021?",
    "start": "11512334",
    "end": "11518032"
  },
  {
    "text": "And the cost for running inference on it was 60, $70 per million tokens,",
    "start": "11518032",
    "end": "11522804"
  },
  {
    "text": "which was the cost per\nintelligence was ridiculous. Now, as we scaled forward two years, we've had a 1,200x reduction in cost",
    "start": "11524255",
    "end": "11531938"
  },
  {
    "text": "to achieve the same level\nof intelligence as GPT-3. - So, here on the x-axis is time",
    "start": "11531939",
    "end": "11537360"
  },
  {
    "text": "over just a couple of years. And on the y-axis is log\nscale dollars to run inference",
    "start": "11537361",
    "end": "11543842"
  },
  {
    "text": "on a millions-\n- Yes, dollars. Yeah, a million. - And so, you have just a down,",
    "start": "11545859",
    "end": "11551049"
  },
  {
    "text": "like a linear decline on log\nscale from GPT-3 through 3.5",
    "start": "11551049",
    "end": "11555799"
  },
  {
    "text": "to Llama.\n- It's like 5 cents or something like that now, which is versus $60, 1,200x.\n- Yeah.",
    "start": "11556770",
    "end": "11562952"
  },
  {
    "text": "Yeah.\n- That's not the exact numbers, but it's 1,200x. I remember that number. Is humongous cost per intelligence.",
    "start": "11562952",
    "end": "11570347"
  },
  {
    "text": "Now, the freak out over DeepSeek is, oh my god, they made it so cheap. It's like actually, if you\nlook at this trend line,",
    "start": "11570347",
    "end": "11575644"
  },
  {
    "text": "they're not below the\ntrend line, first of all, and at least for GPT-3.\n(Nathan laughing) They are the first to hit\nit, which is a big deal.",
    "start": "11575644",
    "end": "11582265"
  },
  {
    "text": "But they're not below the\ntrend line as far as GPT-3. Now, we have GPT-4, what's gonna happen with\nthese reasoning capabilities.",
    "start": "11582265",
    "end": "11587912"
  },
  {
    "text": "It's a mix of architectural innovations, it's a mix of better data, and it's gonna be better\ntraining techniques,",
    "start": "11587912",
    "end": "11594083"
  },
  {
    "text": "and all of these better inference\nsystems, better hardware. Going from each generation",
    "start": "11594083",
    "end": "11599584"
  },
  {
    "text": "of GPU to new generations, or ASICs, everything is gonna take this cost curve down and\ndown and down and down.",
    "start": "11599584",
    "end": "11606238"
  },
  {
    "text": "And then, can I just spawn\na thousand different LLMs to create a task, and then\npick from one of them?",
    "start": "11606239",
    "end": "11613170"
  },
  {
    "text": "Or whatever search technique, I want a Tree, Monte Carlo Tree Research. Maybe it gets that\ncomplicated, maybe it doesn't,",
    "start": "11613170",
    "end": "11619741"
  },
  {
    "text": "'cause it's too complicated\nto actually scale, who knows? Bitter lesson, right? The question is I think when not if,",
    "start": "11619741",
    "end": "11626363"
  },
  {
    "text": "because the rate of progress is so fast. Nine months ago, Dario was saying, hey,",
    "start": "11628179",
    "end": "11633842"
  },
  {
    "text": "or Dario said nine months ago, the cost to train an inference was this. And now, we're much better than this.",
    "start": "11633842",
    "end": "11640368"
  },
  {
    "text": "And DeepSeek is much better than this. And that cost curve for GPT-4, which was also roughly\n$60 per million tokens",
    "start": "11640369",
    "end": "11646456"
  },
  {
    "text": "when it launched, has\nalready fallen to $2 or so. And we're gonna get it\ndown to cents probably",
    "start": "11646456",
    "end": "11654090"
  },
  {
    "text": "for GPT-4 quality and the same... And that's the base for\nthe reasoning models",
    "start": "11654090",
    "end": "11659213"
  },
  {
    "text": "like o1 that we have today, and o1 Pro is spawning multiple. And o3 and so on and so forth.",
    "start": "11659213",
    "end": "11664775"
  },
  {
    "text": "These search techniques'\ntoo expensive today, but they will get cheaper. And that's what's gonna\nunlock the intelligence.",
    "start": "11664775",
    "end": "11671137"
  },
  {
    "start": "11671000",
    "end": "11938000"
  },
  {
    "text": "- So, it'll get cheaper\nand cheaper and cheaper. The big DeepSeek-R1 release\nfreaked everybody out,",
    "start": "11671137",
    "end": "11678765"
  },
  {
    "text": "because of the cheaper, one of the manifestations of\nthat is Nvidia stock plummeted.",
    "start": "11678765",
    "end": "11684105"
  },
  {
    "text": "Can you explain what happened? And also just explain this moment and whether if Nvidia's\ngonna keep winning.",
    "start": "11684105",
    "end": "11692836"
  },
  {
    "text": "- We are both Nvidia\nbulls here, I would say. And in some ways, the market\nresponse is reasonable.",
    "start": "11692836",
    "end": "11699465"
  },
  {
    "text": "Most of the market, like Nvidia's biggest customers in the US are major tech companies, and\nthey're spending a ton on AI.",
    "start": "11699465",
    "end": "11706854"
  },
  {
    "text": "And if a simple interpretation of DeepSeek is you can\nget really good models without spending as much on AI.",
    "start": "11706855",
    "end": "11713364"
  },
  {
    "text": "So, in that capacity, it's like, oh, maybe these big tech companies won't need to spend as\nmuch in AI and go down.",
    "start": "11713365",
    "end": "11718395"
  },
  {
    "text": "The actual thing that happened, it's much more complex,\nwhere there's social factors, where there's the rising in the App Store,",
    "start": "11718395",
    "end": "11723736"
  },
  {
    "text": "the social contagion that is happening. And then, I think some\nof it's just like...",
    "start": "11723736",
    "end": "11728970"
  },
  {
    "text": "I don't trade, I don't know anything\nabout financial markets, but it builds up over the weekend, where the social pressure, where it's like if it was during the week",
    "start": "11728970",
    "end": "11734971"
  },
  {
    "text": "and there was multiple days of trading when this was really becoming, but it comes on the weekend, and then everybody wants to sell.",
    "start": "11734971",
    "end": "11740868"
  },
  {
    "text": "And that is a social contagion. - I think, and like there were\na lot of false narratives,",
    "start": "11740868",
    "end": "11745868"
  },
  {
    "text": "which is like, hey, these guys are spending\nbillions on models. And they're not spending\nbillions on models. No one spent more than a billion dollars",
    "start": "11745869",
    "end": "11752560"
  },
  {
    "text": "on a model that's released publicly. GPT-4 was a couple hundred million, and then they've reduced\nthe cost for Turbo 4o.",
    "start": "11752560",
    "end": "11760877"
  },
  {
    "text": "But billion dollar model runs are coming. And this concludes\npre-training and post-training. And then, the other number is like,",
    "start": "11762263",
    "end": "11768087"
  },
  {
    "text": "hey, DeepSeek didn't include everything. They didn't include... A lot of the cost goes to research and\nall this sort of stuff. A lot of the cost goes to inference.",
    "start": "11768087",
    "end": "11773985"
  },
  {
    "text": "A lot of the cost goes to post-training. None of these things were factor. - Yeah.\n- It's research salaries. All these things are\ncounted in the billions",
    "start": "11773985",
    "end": "11780446"
  },
  {
    "text": "of dollars that OpenAI is spending, but they weren't counted in the, hey, 6 million, $5 million\nthat DeepSeek spent.",
    "start": "11780447",
    "end": "11786116"
  },
  {
    "text": "So, there's a bit of misunderstanding of\nwhat these numbers are. And then, there's also an element of",
    "start": "11786116",
    "end": "11793143"
  },
  {
    "text": "Nvidia's just been a straight line up. And there's been so many\ndifferent narratives that have been trying to push down Nvidia.",
    "start": "11793143",
    "end": "11799524"
  },
  {
    "text": "I don't say push down Nvidia stock. Everyone is looking for a\nreason to sell or to be worried.",
    "start": "11799524",
    "end": "11805700"
  },
  {
    "text": "It was Blackwell delays. Their GPU was, there's a lot of report... Every two weeks, there's a new report about\ntheir GPUs being delayed.",
    "start": "11805701",
    "end": "11814028"
  },
  {
    "text": "There's the whole thing\nabout scaling laws ending. It's so ironic. - [Nathan] It lasted a month. (laughs)",
    "start": "11814028",
    "end": "11819240"
  },
  {
    "text": "It was just literally just, hey, models aren't getting better. They're just not getting better.",
    "start": "11819240",
    "end": "11824848"
  },
  {
    "text": "There's no reason to spend more, pre-training scaling is dead,\nand then it's like, o1, o3.",
    "start": "11824848",
    "end": "11830177"
  },
  {
    "text": "- R1. (laughs)\n- R1. And now, it's like, wait,\nmodels are getting too, they're progressing too fast,\n(Nathan laughing)",
    "start": "11830177",
    "end": "11835403"
  },
  {
    "text": "slow down the progress,\nstop spending on GPUs. - Yeah.\n- And it's... But the funniest thing I think that comes out of this is\nJevons Paradox is true.",
    "start": "11835404",
    "end": "11843865"
  },
  {
    "text": "AWS pricing for H100s has gone up over the last couple weeks.",
    "start": "11843865",
    "end": "11849098"
  },
  {
    "text": "Since a little bit after Christmas, since V3 was launched, AWS\nH100 pricing has gone up.",
    "start": "11849098",
    "end": "11854303"
  },
  {
    "text": "H200s are almost out of stock everywhere, because H200 has more memory, and therefore, R1 wants\nthat chip over H100.",
    "start": "11854303",
    "end": "11862966"
  },
  {
    "text": "- We were trying to get GPUs on a short notice this week for a demo, and it wasn't that easy. We were trying to get just 16 or 32 H100s for demo,\nand it was not very easy.",
    "start": "11862966",
    "end": "11870825"
  },
  {
    "text": "(Nathan chuckles)\n- So, for people who don't Jevons Paradox is when the efficiency goes up, somehow,",
    "start": "11870825",
    "end": "11876518"
  },
  {
    "text": "magically, counterintuitively, the total resource\nconsumption goes up as well. - And semiconductors is, we're\nat 50 years of Moore's Law,",
    "start": "11878807",
    "end": "11887225"
  },
  {
    "text": "every two years half the cost, double the transistors\njust like clockwork. And it's slowed down obviously, but the semiconductor industry\nis gone up the whole time.",
    "start": "11887225",
    "end": "11895058"
  },
  {
    "text": "It's been wavy. There's obviously cycles and stuff, and I don't expect AI to be any different. There's gonna be ebbs and flows,",
    "start": "11895058",
    "end": "11900794"
  },
  {
    "text": "but this is in AI, it's just playing out\nat an insane timescale. It was 2x every two years.",
    "start": "11900794",
    "end": "11906552"
  },
  {
    "text": "This is 1,200x in like three years. So, it's like,\n(Nathan laughing) the scale of improvement",
    "start": "11906552",
    "end": "11912072"
  },
  {
    "text": "that is hard to get wrap your head around. - Yeah, I was confused, because to me, Nvidia stock\non that should have gone up.",
    "start": "11912072",
    "end": "11919549"
  },
  {
    "text": "But maybe it went down because there's suspicion of foul play on the side of China, something like this.",
    "start": "11919549",
    "end": "11925779"
  },
  {
    "text": "But if you just look purely at the actual principles\nat play here, it's obvious.",
    "start": "11925779",
    "end": "11931125"
  },
  {
    "text": "Yeah, Jevons Paradox.\n- More progress that AI makes or the higher the derivative\nof AI progress is,",
    "start": "11931125",
    "end": "11937640"
  },
  {
    "text": "especially you should, because\nNvidia's in the best place. The higher the derivative is, the sooner the market's gonna\nbe bigger and expanding.",
    "start": "11937640",
    "end": "11943893"
  },
  {
    "start": "11938000",
    "end": "12336000"
  },
  {
    "text": "And Nvidia's the only one that does everything\n- Yeah. - reliably right now. - Because it's not like an\nNvidia competitor arose.",
    "start": "11943893",
    "end": "11950760"
  },
  {
    "text": "It's another company\nthat's using Nvidia, so- - Who historically has been\na large Nvidia customer.",
    "start": "11950760",
    "end": "11956970"
  },
  {
    "text": "- Yeah. (chuckles)\n- And has press releases about them cheering about\nbeing China's biggest Nvidia customer.\n(Nathan laughing)",
    "start": "11956970",
    "end": "11963821"
  },
  {
    "text": "- [Lex] Yeah, it made it- - Obviously they've quieted down, but I think that's another element of is, that they don't wanna say\nhow many GPUs they have.",
    "start": "11963821",
    "end": "11970557"
  },
  {
    "text": "- Yeah.\n- Because hey, yes, they have H800s, yes, they have H20s, they\nalso have some H100s,",
    "start": "11970557",
    "end": "11976426"
  },
  {
    "text": "which are smuggled in.\n- So, can you speak to that, to the smuggling? What's the scale of smuggling",
    "start": "11976426",
    "end": "11981603"
  },
  {
    "text": "that's feasible for a nation\nstate to do for companies? Is it possible to- - I think there's a few\nangles of smuggling here.",
    "start": "11981603",
    "end": "11989810"
  },
  {
    "text": "One is ByteDance arguably is the largest smuggler of GPUs for China. China's not supposed to have GPUs.",
    "start": "11989810",
    "end": "11995632"
  },
  {
    "text": "ByteDance has like over 500,000 GPUs. Why? Because they're all rented from\ncompanies around the world.",
    "start": "11995632",
    "end": "12001420"
  },
  {
    "text": "They rent from Oracle,\nthey rent from Google, they rent from all these mass... And a bunch of smaller\ncloud companies too.",
    "start": "12001420",
    "end": "12006619"
  },
  {
    "text": "All the Neoclouds of the world. They rent so, so many GPUs,\nthey also buy a bunch.",
    "start": "12006619",
    "end": "12011922"
  },
  {
    "text": "And they do this for mostly like what Meta\ndoes, serving TikTok, serving next best separate-\n- Same discussion.",
    "start": "12011922",
    "end": "12018541"
  },
  {
    "text": "- Same as kind, to be clear,\nthat's today the view use. - Yeah.\n- And it's a valid use. Hack the dopamine circuit.",
    "start": "12018541",
    "end": "12024921"
  },
  {
    "text": "Now, that's theoretically\nnow very much restricted with the AI diffusion rules,",
    "start": "12024921",
    "end": "12030412"
  },
  {
    "text": "which happened in the last\nweek of the Biden admin and Trump admin looks like\nthey're gonna keep 'em, which limits allies, even like Singapore,",
    "start": "12030412",
    "end": "12038961"
  },
  {
    "text": "which Singapore is like 20,\n30% of Nvidia's revenue. But Singapore's had a moratorium",
    "start": "12038961",
    "end": "12045206"
  },
  {
    "text": "on not building data\ncenters for like 15 years, 'cause they don't have enough power. So, where are they going? (Dylan laughing)\n- [Nathan] Oh yeah. (laughs)",
    "start": "12045206",
    "end": "12051433"
  },
  {
    "text": "- I'm not claiming they're\nall going to China, but a portion are, many are going to Malaysia,\nincluding Microsoft and Oracle have big data\ncenters in Malaysia.",
    "start": "12051433",
    "end": "12058166"
  },
  {
    "text": "They're going all over\nSoutheast Asia probably, India as well. There's stuff routing, but the diffusion rules are very de facto.",
    "start": "12058166",
    "end": "12065756"
  },
  {
    "text": "You can only buy this many\nGPUs from this country. And you can only rent a cluster this large",
    "start": "12065756",
    "end": "12071036"
  },
  {
    "text": "to companies that are Chinese. They're very explicit on\ntrying to stop smuggling. And a big chunk of it was, hey,",
    "start": "12071036",
    "end": "12078690"
  },
  {
    "text": "random company by 16\nservers ship them to China. There's actually, I saw\na photo from someone",
    "start": "12078690",
    "end": "12086737"
  },
  {
    "text": "in the semiconductor industry who leads like a team for networking chips",
    "start": "12086738",
    "end": "12092054"
  },
  {
    "text": "that competes with Nvidia. And he sent a photo of a guy checking into a\nfirst-class united flight",
    "start": "12092054",
    "end": "12097813"
  },
  {
    "text": "from San Francisco to\nShanghai or Shenzhen, with a super micro box that is this big,",
    "start": "12097814",
    "end": "12103842"
  },
  {
    "text": "which can only contain GPUs.\n(Nathan chuckles) And he was booking first-class, 'cause think about it,",
    "start": "12103842",
    "end": "12108977"
  },
  {
    "text": "3 to 5k for your first-class ticket, server costs 240,000 in the US, to 50,000.",
    "start": "12108977",
    "end": "12114609"
  },
  {
    "text": "You sell it for 300,000 in China, wait, you just got a\nfree first-class ticket - Yeah.\n- and a lot more money.",
    "start": "12114609",
    "end": "12120270"
  },
  {
    "text": "So, it's like... And then, that's like\nsmall-scale smuggling. Most of the large-scale smuggling is like companies in\nSingapore and Malaysia",
    "start": "12120270",
    "end": "12126597"
  },
  {
    "text": "routing 'em around, or renting GPUs completely legally.\n- I wanna jump in. How much does this scale?",
    "start": "12126597",
    "end": "12131671"
  },
  {
    "text": "I think there's been some\nnumber, like some people that are higher level\neconomics understanding,",
    "start": "12131671",
    "end": "12136719"
  },
  {
    "text": "say that it's like as you go from 1 billion of smuggling to 10 billion, it's like you're hiding certain\nlevels of economic activity.",
    "start": "12136719",
    "end": "12142750"
  },
  {
    "text": "And that's the most reasonable thing to me is that there's gonna be some\nlevel where it's so obvious that it's easier to find\nthis economic activity.",
    "start": "12142750",
    "end": "12149358"
  },
  {
    "text": "And-\n- Yeah. So, my belief is that\nlast year, roughly...",
    "start": "12149359",
    "end": "12153842"
  },
  {
    "text": "So, Nvidia made a million H20s, which are legally allowed\nto be shipped to China, which we talked about\nis better for reasoning,",
    "start": "12155149",
    "end": "12161012"
  },
  {
    "text": "inference at least. Maybe not training, but\nreasoning, inference. And inference generally.",
    "start": "12161012",
    "end": "12166111"
  },
  {
    "text": "Then, they also had a couple 100,000, we think like 200 to 300,000 GPUs",
    "start": "12166112",
    "end": "12171873"
  },
  {
    "text": "were routed to China from Singapore, Malaysia, US, wherever. Companies spun up by 16 GPUs, 64 GPUs,",
    "start": "12171873",
    "end": "12179102"
  },
  {
    "text": "whatever it is routed, and Huawei's known for having\nspent up a massive network of companies to get\nthe materials they need",
    "start": "12179102",
    "end": "12184728"
  },
  {
    "text": "after they were banned in 2018. So, it's not like\notherworldly, but I agree. Nathan's point is like, hey,",
    "start": "12184728",
    "end": "12190685"
  },
  {
    "text": "you can't smuggle up $10 billion of GPUs. And then, the third source,\nwhich is just now banned and which wasn't considered smuggling,",
    "start": "12190685",
    "end": "12196913"
  },
  {
    "text": "but is China is renting, I\nbelieve from our research,",
    "start": "12196913",
    "end": "12201330"
  },
  {
    "text": "Oracle's biggest GPU\ncustomer is ByteDance. And for Google, I think it's\ntheir second biggest customer.",
    "start": "12202585",
    "end": "12209524"
  },
  {
    "text": "And so, like, and you go\ndown the list of clouds and especially these\nsmaller cloud companies that aren't like that hyperscalers.",
    "start": "12209525",
    "end": "12215922"
  },
  {
    "text": "Think beyond core of even Lambda\neven, there's a whole sea, there's 60 different new cloud companies serving Nvidia GPUs.",
    "start": "12215922",
    "end": "12221712"
  },
  {
    "text": "I think ByteDance is renting\na lot of these all over it. And so, these companies are renting GPUs",
    "start": "12221712",
    "end": "12228330"
  },
  {
    "text": "to Chinese companies, and that was completely legal\nup until the diffusion rules, which happened just a few weeks ago.",
    "start": "12228330",
    "end": "12234400"
  },
  {
    "text": "And even now, you can rent GPU clusters that are less than 2,000 GPUs. Or you can buy GPUs and\nship them wherever you want",
    "start": "12234401",
    "end": "12241185"
  },
  {
    "text": "if they're less than 1,500 GPUs. So, it's like there are\nstill some ways to smuggle,",
    "start": "12241185",
    "end": "12246362"
  },
  {
    "text": "but yeah, as the numbers grow, 100 something billion dollars of revenue",
    "start": "12246363",
    "end": "12251602"
  },
  {
    "text": "for Nvidia last year, 200\nsomething billion this year. And if next year, it could nearly double\nagain or more than double",
    "start": "12251602",
    "end": "12259038"
  },
  {
    "text": "based on what we see with\ndata center footprints like being built out all across the US and\nthe rest of the world,",
    "start": "12259038",
    "end": "12264393"
  },
  {
    "text": "it's gonna be really hard for China to keep up with these rules. Yes, there will always be smuggling",
    "start": "12264393",
    "end": "12270288"
  },
  {
    "text": "and DeepSeek level models\nof GPT-4 level models, o1 level models capable to\ntrain on what China can get,",
    "start": "12270288",
    "end": "12276629"
  },
  {
    "text": "even the next tier above that. But if we speed run a couple more jumps",
    "start": "12276629",
    "end": "12281291"
  },
  {
    "text": "to billion dollar models,\n$10 billion models, then it becomes, hey, there is a compute disadvantage for China",
    "start": "12282641",
    "end": "12288241"
  },
  {
    "text": "for training models and serving them. And the serving part is really critical. DeepSeek cannot serve their model today.",
    "start": "12288242",
    "end": "12293983"
  },
  {
    "text": "It's completely out of inventory. It's already started falling in the App Store actually, downloads,",
    "start": "12293983",
    "end": "12299873"
  },
  {
    "text": "because you download\nit, you try and sign up, they say, we're not taking registrations, 'cause they have no capacity. You open it up, you get less\nthan five tokens per second",
    "start": "12299873",
    "end": "12306590"
  },
  {
    "text": "if you even get your request approved. Because there's just no capacity, because they just don't have\nenough GPUs to serve the model,",
    "start": "12306590",
    "end": "12312580"
  },
  {
    "text": "even though it's incredibly efficient. - It'd be fascinating\nto watch the smuggling, 'cause there's drug smuggling.",
    "start": "12312580",
    "end": "12319246"
  },
  {
    "text": "That's a market. There's weapons smuggling. And GPUs will surpass that",
    "start": "12319246",
    "end": "12325481"
  },
  {
    "text": "at some point.\n- Chips are highest value per kilogram, probably by far. (chuckles)",
    "start": "12325481",
    "end": "12331870"
  },
  {
    "text": "- Oh man.\n- I have another question for you, Dylan. Do you track model API\naccess internationally?",
    "start": "12331870",
    "end": "12336907"
  },
  {
    "start": "12336000",
    "end": "12964000"
  },
  {
    "text": "How easy is it for Chinese companies to use hosted model APIs from the US?",
    "start": "12336907",
    "end": "12342246"
  },
  {
    "text": "- Yeah, that's incredibly easy. OpenAI publicly stated\nDeepSeek uses their API. And they say they have evidence.",
    "start": "12342246",
    "end": "12348362"
  },
  {
    "text": "And this is another element\nof the training regime is people at OpenAI have claimed that it's a distilled model.",
    "start": "12348362",
    "end": "12353966"
  },
  {
    "text": "i.e, you're taking OpenAI's model, you're generating a lot of output, and then you're training on\nthe output in their model.",
    "start": "12353966",
    "end": "12359960"
  },
  {
    "text": "- Yeah.\n- And even if that's the case, what they did is still\namazing, by the way, what DeepSeek did, efficiency-wise. - Distillation is standard\npractice in industry.",
    "start": "12359960",
    "end": "12366372"
  },
  {
    "text": "Whether or not, if you're at a closed lab where you care about terms\nof service and IP closely, you distill from your own models.",
    "start": "12366372",
    "end": "12371790"
  },
  {
    "text": "If you are a researcher and you're not building any products, you distill from the OpenAI models. - This is a good opportunity.",
    "start": "12371790",
    "end": "12377501"
  },
  {
    "text": "Can you explain big picture\ndistillation as a process? What is distillation?",
    "start": "12377501",
    "end": "12383105"
  },
  {
    "text": "What's the process\n- We've - of distillation?\n- talked a lot about training language models. They are trained on text. And post-training,",
    "start": "12383105",
    "end": "12388599"
  },
  {
    "text": "you're trying to train\non very high quality text that you want the model\nto match the features of. Or if you're using RL,",
    "start": "12388599",
    "end": "12394111"
  },
  {
    "text": "you're letting the model\nfind its own thing. But for supervised fine-tuning, for preference data, you\nneed to have some completions",
    "start": "12394111",
    "end": "12400140"
  },
  {
    "text": "what the model is trying\nto learn to imitate. And what you do there is,\ninstead of a human data",
    "start": "12400140",
    "end": "12406607"
  },
  {
    "text": "or instead of the model\nyou're currently training, you take completions from a different, normally more powerful model.",
    "start": "12406607",
    "end": "12412980"
  },
  {
    "text": "I think there's rumors\nthat these big models that people are waiting for,\nthese GPT-5s of the world,",
    "start": "12412980",
    "end": "12419302"
  },
  {
    "text": "the Claude 3 Opuses of the world, are used internally to do\nthis distillation process at OpenAI.\n- There's also",
    "start": "12419302",
    "end": "12425060"
  },
  {
    "text": "public examples, like\nMeta explicitly stated, not necessarily distilling, but they used 405b as a reward model",
    "start": "12425060",
    "end": "12432298"
  },
  {
    "text": "for 70b in their Llama 3.2 - Yes.\n- or 3.3. This is all the same topic. - So, (sighs) is this\nethical? Is this legal?",
    "start": "12432298",
    "end": "12440931"
  },
  {
    "text": "Why is that \"Financial\nTimes\" article headline say, OpenAI says that there's evidence",
    "start": "12440931",
    "end": "12448129"
  },
  {
    "text": "that China's DeepSeek used\nits model to train competitor. - This is a long, at\nleast in the academic side",
    "start": "12448129",
    "end": "12454610"
  },
  {
    "text": "and research side, it has a long history, 'cause you're trying to\ninterpret OpenAI's rule. OpenAI's terms of service say",
    "start": "12454611",
    "end": "12460201"
  },
  {
    "text": "that you cannot build a competitor with outputs from their models. Terms of service are\ndifferent than a license, which are essentially a\ncontract between organizations.",
    "start": "12460201",
    "end": "12467823"
  },
  {
    "text": "So, if you have a terms of\nservice on OpenAI's account, if I violate it, OpenAI\ncan cancel my account.",
    "start": "12467823",
    "end": "12473070"
  },
  {
    "text": "This is very different than like a license that says how you could\nuse a downstream artifact. So, a lot of it hinges on a word",
    "start": "12473071",
    "end": "12478173"
  },
  {
    "text": "that is very unclear in the AI space, which is what is a competitor? And so-\n- And then, the ethical aspect of it is like,",
    "start": "12478173",
    "end": "12484206"
  },
  {
    "text": "why is it unethical for me to train on your bottle\n- Yeah. - when you can train\non the internet's text? - Yeah.\n- Right?",
    "start": "12484206",
    "end": "12489873"
  },
  {
    "text": "- So, there's a bit of a hypocrisy, because OpenAI and potentially\nmost of the companies",
    "start": "12489873",
    "end": "12496954"
  },
  {
    "text": "trained on the internet's\ntext without permission. - There's also a clear loophole, which is that I generate data from OpenAI,",
    "start": "12496973",
    "end": "12504955"
  },
  {
    "text": "and then I upload it somewhere, and then somebody else trains on it and the link has been broken.",
    "start": "12504955",
    "end": "12510304"
  },
  {
    "text": "They're not under the same\nterms of service contract. - This is why-\n- There's a lot of hippo... There's a lot of to be discovered details",
    "start": "12510304",
    "end": "12517322"
  },
  {
    "text": "that don't make a lot of sense. - This is why a lot of models today, even if they train on zero OpenAI data,",
    "start": "12517322",
    "end": "12522602"
  },
  {
    "text": "you ask the model who\ntrained you, it'll say, I'm ChatGPT trained by OpenAI.\n- Yeah. - Because there's so much copy-paste",
    "start": "12522602",
    "end": "12529353"
  },
  {
    "text": "of OpenAI outputs from\nthat on the internet that you just weren't\nable to filter it out. And there was nothing in the RL",
    "start": "12529353",
    "end": "12535171"
  },
  {
    "text": "where they implemented like,\nhey, or post-training or SFT, whatever, that says, hey,",
    "start": "12535171",
    "end": "12540206"
  },
  {
    "text": "I'm actually modeled by\nAllen Institute instead of OpenAI.\n- We have to do this if we serve a demo.",
    "start": "12540206",
    "end": "12545558"
  },
  {
    "text": "We do research and we use\nOpenAI APIs, because it's useful and we want to understand post-training, and our research models,",
    "start": "12545558",
    "end": "12551847"
  },
  {
    "text": "they'll say they're written by OpenAI unless we put in the system\nprop that we talked about that. Like, I am Tulu, I am a language model",
    "start": "12551847",
    "end": "12557900"
  },
  {
    "text": "trained by the Allen Institute for AI. And if you ask more\npeople around industry, especially with post-training,\nit's a very doable task",
    "start": "12557901",
    "end": "12565181"
  },
  {
    "text": "to make the model say who it is or to suppress the OpenAI thing. So, in some levels, it\nmight be that DeepSeek",
    "start": "12565181",
    "end": "12572069"
  },
  {
    "text": "didn't care that it was\nsaying that it was by OpenAI. If you're gonna upload model weights, it doesn't really matter,",
    "start": "12572069",
    "end": "12577478"
  },
  {
    "text": "'cause anyone that's\nserving it in an application and cares a lot about serving\nis going to, when serving it,",
    "start": "12577478",
    "end": "12583269"
  },
  {
    "text": "if they're using it for a specific task, they're gonna tailor it to that. And it doesn't matter that\nit's saying it's ChatGPT.",
    "start": "12583269",
    "end": "12588891"
  },
  {
    "text": "- Oh, I guess the one\nof the ways to do that is like a system prompt\nor something like that. Like if you're serving\nit to say that you're-",
    "start": "12588891",
    "end": "12595146"
  },
  {
    "text": "- That's what we do. If we host a demo, you say you are Tulu 3, a language model trained by\nthe Allen Institute for AI.",
    "start": "12595146",
    "end": "12601946"
  },
  {
    "text": "We also are benefited from OpenAI data, 'cause it's a great research tool. - Do you think there's any truth",
    "start": "12601947",
    "end": "12608367"
  },
  {
    "text": "and value to the claim, OpenAI's claim that there's\nevidence that China's DeepSeek",
    "start": "12608367",
    "end": "12614871"
  },
  {
    "text": "use this model to train? - I think everyone has\nbenefited regardless, because the data's on the internet.",
    "start": "12614872",
    "end": "12621890"
  },
  {
    "text": "And therefore, it's in\nyour pre-training now. There are like subreddits where people share the\nbest ChatGPT outputs.",
    "start": "12621890",
    "end": "12627507"
  },
  {
    "text": "And those are in your- - I think that they're trying\nto shift the narrative, like they're trying to protect themselves.",
    "start": "12627507",
    "end": "12633421"
  },
  {
    "text": "And we saw this years ago when ByteDance was actually banned from some OpenAI APIs\nfor training on outputs.",
    "start": "12633421",
    "end": "12639090"
  },
  {
    "text": "There's other AI startups\nthat most people, if you're in the AI culture, were like,",
    "start": "12639090",
    "end": "12644555"
  },
  {
    "text": "they just told us they\ntrained on OpenAI outputs and they never got banned. That's how they bootstrapped\ntheir early models.",
    "start": "12644555",
    "end": "12650857"
  },
  {
    "text": "So, it's much easier to get\noff the ground using this than to set up human pipelines\nand build a strong model.",
    "start": "12650857",
    "end": "12656206"
  },
  {
    "text": "So, it's long history here and a lot of the communications are seem like narrative control. - Actually, over the last couple days,",
    "start": "12656206",
    "end": "12662519"
  },
  {
    "text": "we've seen a lot of people\ndistill DeepSeek's model into Llama models, because the DeepSeek models\n- Mm-hmm.",
    "start": "12662519",
    "end": "12667955"
  },
  {
    "text": "- are complicated to run inference on, because they're mixture of experts and they're 600 plus billion\nparameters and all this.",
    "start": "12667955",
    "end": "12674257"
  },
  {
    "text": "And people distilled them into the Llama models.\n(Lex laughing) Because the Llama models\nare so easy to serve, and everyone's built the pipelines",
    "start": "12674257",
    "end": "12679700"
  },
  {
    "text": "and tooling for inference\n- Yeah. - with the Llama models, because it's the open standard. (chuckles) So, we've seen it,\nwe've seen a roundabout.",
    "start": "12679700",
    "end": "12686538"
  },
  {
    "text": "Is it bad? Is it illegal? Maybe it's illegal, whatever.\nI don't know about that. But it's-\n- It could break contracts.",
    "start": "12686539",
    "end": "12691828"
  },
  {
    "text": "I don't think it's illegal,\nlike in any illegal, like no one's going to jail for this ever. - I think fundamentally,\nI think it's ethical",
    "start": "12691828",
    "end": "12697989"
  },
  {
    "text": "or I hope it's ethical, because the moment becomes,\nwe ban that kind of thing,",
    "start": "12697990",
    "end": "12704787"
  },
  {
    "text": "it's gonna make everybody much worse off. And I also actually, this is difficult,",
    "start": "12704967",
    "end": "12711237"
  },
  {
    "text": "but I think you should be\nallowed to train on the internet. I know a lot of authors and creators are very sensitive about it.",
    "start": "12711237",
    "end": "12717900"
  },
  {
    "text": "That's a difficult question. But the moment you're not\nallowed to train on the internet.",
    "start": "12717901",
    "end": "12723178"
  },
  {
    "text": "- I agree. - I have a schizo take on\nhow you can solve this, because it already works.\n- All right. - I have a reasonable take on it.",
    "start": "12723178",
    "end": "12728515"
  },
  {
    "text": "- All right, all right.\n(Nathan laughing) - So, Japan has a law which you're allowed",
    "start": "12728515",
    "end": "12733948"
  },
  {
    "text": "to train on any training data and copyrights don't apply if\nyou wanna train a model, A. B, Japan has nine gigawatts\nof curtailed nuclear power.",
    "start": "12733949",
    "end": "12743117"
  },
  {
    "text": "C, Japan is allowed under\nthe AI diffusion rule to import as many GPUs as they'd like.",
    "start": "12743117",
    "end": "12748911"
  },
  {
    "text": "So, all we have to do, we\nhave a market here to make, we build massive data setters,\nwe rent them to the labs,",
    "start": "12748911",
    "end": "12754258"
  },
  {
    "text": "and then we train models in\na legally permissible way, and there's no if, ands, or buts. And now, the models have no\npotential copyright lawsuit",
    "start": "12754258",
    "end": "12762835"
  },
  {
    "text": "from \"New York Times\"\nor anything like that. No, no, it's just completely legal. - Yeah.\n- No, so- - Genius.\n- The early copyright lawsuits",
    "start": "12762835",
    "end": "12769258"
  },
  {
    "text": "have fallen in the favor of AI training. I would say that the long tail of use",
    "start": "12769258",
    "end": "12775266"
  },
  {
    "text": "is gonna go in the side\nof AI, which is if you do, if you scrape the trillions\nof tokens of data,",
    "start": "12775266",
    "end": "12782374"
  },
  {
    "text": "you're not looking and saying, this one \"New York Times\"\narticle is so important to me. But if you're doing a\naudio generation for music",
    "start": "12782374",
    "end": "12789296"
  },
  {
    "text": "or image generation and you say, make it in the style of X person, that's a reasonable case\nwhere you could figure out",
    "start": "12789296",
    "end": "12795137"
  },
  {
    "text": "what is their profit margin on inference. I don't know if it's gonna be the 50/50 of YouTube creator program or something,",
    "start": "12795137",
    "end": "12802448"
  },
  {
    "text": "but I would opt into\nthat program as a writer, like, please, like that...",
    "start": "12802448",
    "end": "12807062"
  },
  {
    "text": "It's gonna be a rough journey, but there will be some solutions\nlike that that make sense. But there's a long tail where\nit's just on the internet.",
    "start": "12808080",
    "end": "12815977"
  },
  {
    "text": "- I think one of the other aspects of that \"Financial Times\" article implied. And so, that leads to a\nmore general question.",
    "start": "12815977",
    "end": "12823097"
  },
  {
    "text": "Do you think there's... How difficult is spying, espionage, and stealing of actual secret code",
    "start": "12823097",
    "end": "12830792"
  },
  {
    "text": "and data from inside companies? How much of that is being attempted? - Code and data is\nhard, but ideas is easy.",
    "start": "12831709",
    "end": "12837949"
  },
  {
    "text": "Silicon Valley operates\n(Dylan chuckles) - Yeah.\n- on the way that top employees get bought out",
    "start": "12837949",
    "end": "12843246"
  },
  {
    "text": "by other companies for a pay raise, and a large reason why these companies do this\nis to bring ideas with them.",
    "start": "12843246",
    "end": "12848946"
  },
  {
    "text": "And there's no, in California, there's rules that certain, like non-competes or whatever,\nare illegal in California",
    "start": "12848947",
    "end": "12855887"
  },
  {
    "text": "and whether or not\nthere's NDAs and things, that is how a lot of process happens. Recently, there was somebody from Gemini",
    "start": "12855887",
    "end": "12863099"
  },
  {
    "text": "who helped make this 1\nmillion context length and everyone is saying the next Llama who,",
    "start": "12863099",
    "end": "12868120"
  },
  {
    "text": "I mean, he went to the Meta team, is gonna have 1 million context length. And that's how the world works.",
    "start": "12868121",
    "end": "12874622"
  },
  {
    "text": "- As far as industrial\nespionage and things, that has been greatly\nsuccessful in the past.",
    "start": "12874622",
    "end": "12880601"
  },
  {
    "text": "The Americans did the Brits, the Chinese have done it to the Americans, and so on and so forth.",
    "start": "12880601",
    "end": "12886287"
  },
  {
    "text": "It is a fact of life. And so, to argue industrial espionage can be stopped is probably unlikely.",
    "start": "12886287",
    "end": "12893933"
  },
  {
    "text": "You can make it difficult. But even then, there's\nall these stories about, hey, F35 and F22 have\nalready been given to China",
    "start": "12893933",
    "end": "12900696"
  },
  {
    "text": "in terms of design plans and stuff. Code and stuff like\nbetween, I say companies,",
    "start": "12900696",
    "end": "12906117"
  },
  {
    "text": "not nation states is\nprobably very difficult, but ideas are discussed a lot. Whether it be a house\nparty in San Francisco",
    "start": "12906118",
    "end": "12913636"
  },
  {
    "text": "or a company changing employees, or always the mythical honeypot",
    "start": "12913636",
    "end": "12917953"
  },
  {
    "text": "that always gets talked about, like someone gets honeypotted. Because everyone working on AI is a single dude who's\nin their 20s and 30s.",
    "start": "12919894",
    "end": "12926920"
  },
  {
    "text": "Not everyone, but a insane percentages. So, there's always all these\nyou know, and obviously-",
    "start": "12926920",
    "end": "12934406"
  },
  {
    "text": "- So, honeypotted is like a spy, a female spy approaches you and like- - Yeah, yeah.\n- Or male.",
    "start": "12934406",
    "end": "12941544"
  },
  {
    "text": "It's San Francisco, but...\n(Nathan laughing) As a single dude, I will\nsay, in his late 20s, it is like, we are very easily corrupted.",
    "start": "12941544",
    "end": "12948196"
  },
  {
    "text": "- [Lex] Yeah. - Not corrupted myself, but we are, we are.\n- Yeah. Everybody else, not me.",
    "start": "12948196",
    "end": "12954006"
  },
  {
    "text": "Everybody else.\n- Yeah. Exactly. - I'm too oblivious that I am not single. So, I'm safe from one\nespionage access. (laughs)",
    "start": "12954006",
    "end": "12960911"
  },
  {
    "text": "- Yeah, you have to make sure to close all security vulnerabilities. So, you, Dylan, collect\na lot of information",
    "start": "12960911",
    "end": "12967780"
  },
  {
    "start": "12964000",
    "end": "15086000"
  },
  {
    "text": "about each of the mega clusters for each of the major AI companies.",
    "start": "12967781",
    "end": "12973335"
  },
  {
    "text": "Can you talk about the buildouts for each one that stand out?",
    "start": "12973335",
    "end": "12978378"
  },
  {
    "text": "- Yeah, so I think the thing that's really important about\nthese mega cluster buildouts is they're completely\nunprecedented in scale.",
    "start": "12978378",
    "end": "12986706"
  },
  {
    "text": "US, data center power consumption has been slowly on the rise\nand it's gone up to 2, 3%",
    "start": "12986706",
    "end": "12992585"
  },
  {
    "text": "even through the cloud\ncomputing revolution. Data center consumption as\na percentage of total US. And that's been over decades\nof data centers, et cetera.",
    "start": "12992585",
    "end": "13000625"
  },
  {
    "text": "It's been climbing, climbing slowly. But now, 2 to 3%. Now, by the end of this\ndecade, even under like,",
    "start": "13000625",
    "end": "13007147"
  },
  {
    "text": "when I say 10%, a lot of\npeople that are traditionally by 2028, 2030,",
    "start": "13007147",
    "end": "13011211"
  },
  {
    "text": "a traditional data center\npeople like, that's nuts. But then, people who are in AI who have really looked at this",
    "start": "13014630",
    "end": "13020072"
  },
  {
    "text": "at the Anthropics and OpenAIs, they're like, that's not enough. And I'm like, okay.\n- Mm-hmm. (chuckles) - But this is both through\nglobally distributed",
    "start": "13020072",
    "end": "13028820"
  },
  {
    "text": "or distributed throughout the US, as well as centralized clusters. The distributed throughout\nthe US is exciting",
    "start": "13028820",
    "end": "13034756"
  },
  {
    "text": "and it's the bulk of it. Like, hey, OpenAI or say,\nMeta is adding a gigawatt.",
    "start": "13034756",
    "end": "13040134"
  },
  {
    "text": "But most of it is distributed\nthrough the US for inference and all these other things. - So, maybe we should lay\nit out what a cluster is.",
    "start": "13042741",
    "end": "13049631"
  },
  {
    "text": "So, does this include AWS? Maybe it's good to talk about the\ndifferent kinds of clusters",
    "start": "13049631",
    "end": "13056202"
  },
  {
    "text": "and what you mean by mega clusters, and what's the GPU\n- Mm-hmm. - and what's the computer and what... I'm just kidding.\n- Yeah, yeah, yeah.",
    "start": "13056202",
    "end": "13061400"
  },
  {
    "text": "- Not that far back, but yeah. So, what do we mean by the clusters, - Oh man.\n- the build outs? - I thought I was about\nto do the Apple ad.",
    "start": "13061400",
    "end": "13067231"
  },
  {
    "text": "What's a computer?\n(group laughing) So, traditionally, data\ncenters and data center tasks",
    "start": "13067231",
    "end": "13073320"
  },
  {
    "text": "have been a distributed systems problem that is capable of being\nspread very far and widely.",
    "start": "13073321",
    "end": "13080043"
  },
  {
    "text": "i.e, I send a request to Google, it gets routed to a data\ncenter somewhat close to me.",
    "start": "13080043",
    "end": "13085520"
  },
  {
    "text": "It does whatever search\nranking recommendation, sends a result back.",
    "start": "13085520",
    "end": "13089396"
  },
  {
    "text": "The nature of the task is\nchanging rapidly in that the task, there's two tasks that people\nare really focused on now.",
    "start": "13091148",
    "end": "13096631"
  },
  {
    "text": "It's not database access, it's not serve me the right\npage, serve me the right ad. It's now a inference.",
    "start": "13096631",
    "end": "13102078"
  },
  {
    "text": "And inference is dramatically different from traditional distributed systems, but it looks a lot more similar. And then, there's training.",
    "start": "13102978",
    "end": "13109635"
  },
  {
    "text": "The train inference\nside is still like, hey, I'm gonna put thousands of GPUs in blocks all around these data centers.",
    "start": "13109635",
    "end": "13116989"
  },
  {
    "text": "I'm gonna run models on them, user submits a request,\nit gets kicked off, or hey, my service, they\nsubmit a request to my service.",
    "start": "13116989",
    "end": "13124226"
  },
  {
    "text": "They're on Word, and they're like, oh\nyeah, help me, Copilot. And it starts, kicks it off, or I'm on my Windows, Copilot, whatever, Apple Intelligence, whatever it is,",
    "start": "13124226",
    "end": "13130229"
  },
  {
    "text": "it gets kicked off to a data center. And that data center does\nsome work and sends it back. That's inference. That is going\nto be the bulk of compute.",
    "start": "13130229",
    "end": "13139308"
  },
  {
    "text": "And that's like, there's\nthousands of data centers that we're tracking with satellites and all these other things. And those are the bulk\nof what's being built.",
    "start": "13139367",
    "end": "13146481"
  },
  {
    "text": "But the scale of... And so, that's like\nwhat's really reshaping and that's what's\ngetting millions of GPUs.",
    "start": "13146481",
    "end": "13151537"
  },
  {
    "text": "But the scale of the largest cluster is also really important.",
    "start": "13151537",
    "end": "13157193"
  },
  {
    "text": "When we look back at history,\nor through the age of AI,",
    "start": "13157193",
    "end": "13161776"
  },
  {
    "text": "it was a really big deal when they did AlexNet on I think two GPUs or four GPUs.\n- Yeah.",
    "start": "13162677",
    "end": "13168485"
  },
  {
    "text": "- [Dylan] I don't remember. It was a really big deal.\n- Oh, it's a big deal, 'cause you use GPUs. (Nathan laughing)\n- It's a big deal they use GPUs and they used multiple.",
    "start": "13168485",
    "end": "13174562"
  },
  {
    "text": "But then over time, its scale\nhas just been compounding. And so, when you skip\nforward to GPT-3, then GPT 4,",
    "start": "13174562",
    "end": "13182033"
  },
  {
    "text": "GPT-4, 20,000 A100 GPUs, unprecedented run in terms\nof the size and the cost.",
    "start": "13182033",
    "end": "13188354"
  },
  {
    "text": "A couple hundred million\ndollars on a YOLO. A YOLO run for GPT-4. And it yielded this magical improvement",
    "start": "13188355",
    "end": "13194535"
  },
  {
    "text": "that was perfectly in line\nwith what was experimented and just like a log scale right up. - Oh yeah, they had that\nplot from the paper.",
    "start": "13194535",
    "end": "13200985"
  },
  {
    "text": "Scaling the technical were part. - The scaling laws were perfect. But that's not a crazy number.",
    "start": "13200985",
    "end": "13205997"
  },
  {
    "text": "20,000 A100s, roughly each\nGPU is consuming 400 watts. And then, when you add\nin the whole server,",
    "start": "13205997",
    "end": "13212203"
  },
  {
    "text": "everything, it's like 15\nto 20 megawatts of power.",
    "start": "13212203",
    "end": "13216370"
  },
  {
    "text": "Maybe you could look up what the power of consumption of a person is because the numbers are gonna get silly. But 15 to 20 megawatts",
    "start": "13218535",
    "end": "13224886"
  },
  {
    "text": "was standard data center\nsize was just unprecedented. That was all GPUs running one task. - [Nathan] How many watts\nis a toaster? (chuckles)",
    "start": "13224886",
    "end": "13230873"
  },
  {
    "text": "- A toaster is like also-\n- That's a good example. - A similar power consumption to an A100. H100 comes around,\n(Nathan chuckles)",
    "start": "13230873",
    "end": "13236580"
  },
  {
    "text": "they increase the power\nfrom like 400 to 700 watts. And that's just per GPU. And then, there's all the\nassociated stuff around it.",
    "start": "13236580",
    "end": "13242021"
  },
  {
    "text": "So, once you count all that, it's roughly like 1,200 to\n1,400 watts for everything, networking, CPUs,\nmemory, blah, blah, blah.",
    "start": "13242022",
    "end": "13248713"
  },
  {
    "text": "- So, we should also\nsay, so what's required? You said power. So, a lot of power is required,",
    "start": "13248713",
    "end": "13254854"
  },
  {
    "text": "a lot of heat is generated,\nso the cooling is required. And because there's a lot\nof GPUs that have to be,",
    "start": "13254854",
    "end": "13261854"
  },
  {
    "text": "or CPUs or whatever, they\nhave to be connected. So, there's a lot of networking, right?\n- Yeah, yeah, so I think...",
    "start": "13261854",
    "end": "13267723"
  },
  {
    "text": "Yeah, sorry for skipping past that. And then, the data center\nitself is complicated. But these are still",
    "start": "13267723",
    "end": "13273061"
  },
  {
    "text": "standardized data centers for GPT-4 scale. Now, we step forward to what\nis the scale of clusters",
    "start": "13273061",
    "end": "13279867"
  },
  {
    "text": "that people have built last year. And it ranges widely. It ranges from like, hey,\nthese are standard data centers",
    "start": "13279867",
    "end": "13287033"
  },
  {
    "text": "and we're just using multiple of them and connecting them together really with a ton of fiber between them, a lot of networking, et cetera.",
    "start": "13287033",
    "end": "13292603"
  },
  {
    "text": "That's what OpenAI and\nMicrosoft did in Arizona. And so, they have 100,000 GPUs. Meta, similar thing.",
    "start": "13292603",
    "end": "13298504"
  },
  {
    "text": "They took their standard\nexisting data center design, and it looks like an H, and they connected\nmultiple on 'em together.",
    "start": "13298504",
    "end": "13304353"
  },
  {
    "text": "And they got to, they first did 16,000\nGPUs, 24,000 GPUs total. Only 16 of them,",
    "start": "13304353",
    "end": "13310355"
  },
  {
    "text": "1,000 of 'em were running\non the training run, because GPUs are very unreliable. So, they needed to have\nspares to swap in and out.",
    "start": "13310355",
    "end": "13315455"
  },
  {
    "text": "All the way to like now 100,000 GPUs that they're training\non Llama 4 on currently. Like 128,000 or so.",
    "start": "13315455",
    "end": "13321005"
  },
  {
    "text": "Think about 100,000 GPUs with\nroughly 1,400 watts a piece,",
    "start": "13322603",
    "end": "13327603"
  },
  {
    "text": "that's 140 megawatts, 150\nmegawatts for 128, right? So, you're talking about",
    "start": "13328133",
    "end": "13333714"
  },
  {
    "text": "you've jumped from 15\nto 20 megawatts to 10x, almost 10x that number, 9x that number",
    "start": "13333714",
    "end": "13339273"
  },
  {
    "text": "to 150 megawatts in two\nyears, from 2022 to 2024.",
    "start": "13339273",
    "end": "13343356"
  },
  {
    "text": "And some people like\nElon that he admittedly, and he says himself got into\nthe game a little bit late",
    "start": "13344642",
    "end": "13350183"
  },
  {
    "text": "for pre-training large language models. xAI was started later. But then, he bet heaven and\nhell to get his data center up",
    "start": "13350183",
    "end": "13356891"
  },
  {
    "text": "and get the largest cluster in the world, which is 200,000 GPUs. And he did that. He bought\na factory in Memphis.",
    "start": "13356891",
    "end": "13363935"
  },
  {
    "text": "He's upgrading the\nsubstation with the same time he's got a bunch of\nmobile power generation, a bunch of single cycle combine.",
    "start": "13363935",
    "end": "13370765"
  },
  {
    "text": "He tapped the natural gas line that's right next to the factory and is just pulling a\nton of gas, burning gas. He's generating all this power.",
    "start": "13370765",
    "end": "13377246"
  },
  {
    "text": "He's in a factory and\nan old appliance factory that's shut down and\nmoved to China long ago.",
    "start": "13377246",
    "end": "13382334"
  },
  {
    "text": "And he's got 200,000 GPUs in it. And now, what's the next scale? All the hyperscalers have done this.",
    "start": "13382334",
    "end": "13388687"
  },
  {
    "text": "Now, the next scale is\nsomething that's even bigger. And so, Elon, just to stick on the topic, he's building his own natural gas plant,",
    "start": "13388687",
    "end": "13396196"
  },
  {
    "text": "like a proper one right next door. He's deploying tons of\nTesla megapack batteries",
    "start": "13396196",
    "end": "13401795"
  },
  {
    "text": "to make the power more smooth\nand all sorts of other things. He's got industrial chillers\nto cool the water down",
    "start": "13401795",
    "end": "13407757"
  },
  {
    "text": "because he's water-cooling the chips. So, all these crazy things to get the clusters bigger and bigger.",
    "start": "13407757",
    "end": "13413710"
  },
  {
    "text": "But when you look at, like, say, what OpenAI did with Stargate, that in Arizona, in Abilene, Texas,",
    "start": "13413710",
    "end": "13421081"
  },
  {
    "text": "what they've announced at least. It's not built. Elon says\nthey don't have the money. There's some debates about this.",
    "start": "13422481",
    "end": "13428297"
  },
  {
    "text": "But at full-scale, at\nleast the first section is definitely money's accounted for, but there's multiple sections. But full scale, that data center\nis gonna be 2.2 gigawatts.",
    "start": "13428297",
    "end": "13436967"
  },
  {
    "text": "2,200 megawatts of power in and roughly 1.8 gigawatts\nor 1,800 megawatt,",
    "start": "13436967",
    "end": "13442564"
  },
  {
    "text": "yeah, 1,800 megawatts of\npower delivered to chips. Now, this is an absurd scale.",
    "start": "13443972",
    "end": "13449503"
  },
  {
    "text": "2.2 gigawatts is like more\nthan most cities to be clear. And delivered to a single cluster",
    "start": "13449503",
    "end": "13456022"
  },
  {
    "text": "that's connected to do training. To train these models, to\ndo both the pre-training, the post-training, all of this stuff.",
    "start": "13456022",
    "end": "13462483"
  },
  {
    "text": "- This is insane.\n- It is. - Insane.\n- What is a nuclear power plant again-\n- everyone is doing this. Everyone is doing this.\n(Lex laughing)",
    "start": "13462483",
    "end": "13467992"
  },
  {
    "text": "Meta in Louisiana, they're building two natural\ngas plants, massive ones,",
    "start": "13467992",
    "end": "13473417"
  },
  {
    "text": "and then they're building\nthis massive data center. Amazon has plans for this scale.",
    "start": "13473417",
    "end": "13479279"
  },
  {
    "text": "Google has plans for this scale. xAI has plans for this scale.",
    "start": "13479280",
    "end": "13485006"
  },
  {
    "text": "The guys that are racing, the companies that are\nracing are racing hard, and they're doing multi\ngigawatt data centers",
    "start": "13485006",
    "end": "13491965"
  },
  {
    "text": "to build this out, because they think that,\nyeah, if I now have...",
    "start": "13491965",
    "end": "13497405"
  },
  {
    "text": "Obviously, pre-training\nscaling is gonna continue, but to some extent, but then also all this\npost-training stuff, where you have a RL sandbox\nfor computer use or whatever.",
    "start": "13497406",
    "end": "13504880"
  },
  {
    "text": "This is where they're gonna... And all these viable domains where they just keep learning and learning and learning, self-play,\nwhatever, whatever it is,",
    "start": "13504880",
    "end": "13510679"
  },
  {
    "text": "makes the AI so much more capable because the line does go up. As you throw more compute,\nyou get more performance.",
    "start": "13510679",
    "end": "13516693"
  },
  {
    "text": "The shirt is about scaling laws. To some extent, it is diminishing returns. You 10x the compute, you\ndon't get 10x better model.",
    "start": "13516693",
    "end": "13523932"
  },
  {
    "text": "You get a diminishing returns, but also you get efficiency improvements. So, you bend the curve. And these scale of data centers are doing,",
    "start": "13523932",
    "end": "13531098"
  },
  {
    "text": "wreaking a lot of havoc on the network. Nathan was mentioning there's,",
    "start": "13531098",
    "end": "13537159"
  },
  {
    "text": "Amazon has tried to buy this\nnuclear power plant, Talen. And if you look at the Talen's stock, it's just skyrocketing.",
    "start": "13537159",
    "end": "13543295"
  },
  {
    "text": "And they're building a massive multi gigawatt data center there. And you just go down the list. There's so many ramifications.",
    "start": "13543295",
    "end": "13549753"
  },
  {
    "text": "Interesting thing is\ncertain regions of the US transmitting power cost more\nthan actually generating it.",
    "start": "13549753",
    "end": "13556386"
  },
  {
    "text": "Because the grid is so slow to build, and the demand for power and\nthe ability to build power and re-ramping on a natural gas plant",
    "start": "13556386",
    "end": "13563468"
  },
  {
    "text": "or even a coal plant is\nlike easy enough to do. But transmitting the power is really hard. So, in some parts of the\nUS, like in Virginia,",
    "start": "13563468",
    "end": "13570080"
  },
  {
    "text": "it costs more to transmit power\nthan it cost to generate it. Which is like, there's all sorts of second order effects\nthat are insane here.",
    "start": "13570080",
    "end": "13576616"
  },
  {
    "text": "- And the power grid\nsupport this kind of growth? - Trump's executive orders, there was a Biden executive order",
    "start": "13576616",
    "end": "13581687"
  },
  {
    "text": "before the end of the year, but then Trump had some\nmore executive orders, which hopefully reduce the regulations",
    "start": "13581687",
    "end": "13587563"
  },
  {
    "text": "to where, yes, things can be built. But yeah, this is a big, big challenge. Is building enough power fast enough?",
    "start": "13587564",
    "end": "13594013"
  },
  {
    "text": "- Are you gonna basically\nhave a nuclear power plant next to a data center\nfor each one of these? - So, the fun thing here\nis this is too slow.",
    "start": "13594013",
    "end": "13602647"
  },
  {
    "text": "- [Nathan] To build the power plant. - To build a power plant or to reconfigure an existing\npower plant is too slow.",
    "start": "13602648",
    "end": "13608395"
  },
  {
    "text": "And so, therefore, you must use natural... Data center power consumption is flat. It spike-\n- Which is why nuclear",
    "start": "13608395",
    "end": "13614511"
  },
  {
    "text": "is also good for it. Like long-term, nuclear\nis a very natural fit, - Yeah, it's-\n- but you can't do solar",
    "start": "13614511",
    "end": "13620612"
  },
  {
    "text": "or anything in the short-term like that. - Because data center power is like this. You're telling me, I'm gonna buy tens",
    "start": "13620612",
    "end": "13627134"
  },
  {
    "text": "of billions of dollars\nof GPUs and idle them, 'cause the power's not being generated? Power's cheap. If you look at the cost of a cluster,",
    "start": "13627134",
    "end": "13633142"
  },
  {
    "text": "less than 20% of it is power. Most of it is the capital cost\nand depreciation of the GPUs.",
    "start": "13633142",
    "end": "13639513"
  },
  {
    "text": "And so, it's like, well, screw it, I'll just build natural gas plants. This is what Meta is doing in Louisiana.",
    "start": "13639513",
    "end": "13644523"
  },
  {
    "text": "This is what OpenAI's doing in Texas and all these different places. They may not be doing it directly,",
    "start": "13644523",
    "end": "13649820"
  },
  {
    "text": "but they are partnered with someone. And so, there is a couple hopes. One is...",
    "start": "13649820",
    "end": "13655454"
  },
  {
    "text": "And Elon, what he is\ndoing in Memphis is like, to the extreme, they're not just using\ndual combine cycle gas, which is like super efficient,",
    "start": "13655454",
    "end": "13661793"
  },
  {
    "text": "he's also just using single cycle and mobile generators and\nstuff, which is less efficient. But there's also the flip side,",
    "start": "13661793",
    "end": "13669112"
  },
  {
    "text": "which is solar power\ngeneration is like this, and wind is another like\nthis, correlate different.",
    "start": "13669113",
    "end": "13675069"
  },
  {
    "text": "So, if you stack both of those, plus you get a big chunk of batteries, plus you have a little bit of gas,",
    "start": "13675070",
    "end": "13680435"
  },
  {
    "text": "it is possible to run it more green. It's just the timescales for that is slow. So, people are trying,\n- Mm-hmm.",
    "start": "13680435",
    "end": "13686472"
  },
  {
    "text": "- but Meta basically said, whatever, don't care about my sustainability pledge.",
    "start": "13686472",
    "end": "13691562"
  },
  {
    "text": "Or they'll buy like a power, it's called a PPA, power\npurchasing agreement, or they'll be a massive wind farm or solar farm, like wherever.\n- Oh.",
    "start": "13691562",
    "end": "13698340"
  },
  {
    "text": "- And then, they'll just pretend like those electrons are being\nconsumed by the data center, but in reality, they're\npaying for the power here and selling it to the grid,\nand they're buying power here.",
    "start": "13698340",
    "end": "13705260"
  },
  {
    "text": "- [Lex] Yep. - And then, another\nthing is like Microsoft quit on some of their\nsustainability pledges. Elon, what he did with Memphis",
    "start": "13705260",
    "end": "13712502"
  },
  {
    "text": "is objectively somewhat dirty, but he is also doing it in an area where there's a bigger natural\ngas plant right next door",
    "start": "13712502",
    "end": "13718373"
  },
  {
    "text": "and like a sewer next, or not a sewer, but like a wastewater treatment\nand a garbage dump nearby. And he's obviously made the world",
    "start": "13718373",
    "end": "13725183"
  },
  {
    "text": "a lot more clean than that\none data center is gonna do. So, I think it's fine to some extent.",
    "start": "13725183",
    "end": "13730401"
  },
  {
    "text": "And maybe AGI solves global warming and stuff,\n(Lex laughing) whatever it is. This is sort of the attitude\nthat people at the labs have,",
    "start": "13730401",
    "end": "13737604"
  },
  {
    "text": "which is like, yeah, it's great. We'll just use gas. Because the race is that important. And if we lose, that's way worse.",
    "start": "13737604",
    "end": "13743877"
  },
  {
    "text": "- I should say that I got a chance to visit the Memphis data center. - Oh wow.\n- And it's incredible.",
    "start": "13743877",
    "end": "13750816"
  },
  {
    "text": "I visited with Elon. Just the teams and the rate\nof innovation there is insane.",
    "start": "13750816",
    "end": "13758903"
  },
  {
    "text": "My sense is that nobody's ever\ndone anything of this scale and nobody has certainly\never done anything",
    "start": "13758903",
    "end": "13765884"
  },
  {
    "text": "of this scale at the\nrate that xAI is doing. So, they're like figuring out...",
    "start": "13765884",
    "end": "13771783"
  },
  {
    "text": "And so, I was sitting\nin on all these meetings where they're brainstorming,\nit's like, it's insane.",
    "start": "13771784",
    "end": "13776993"
  },
  {
    "text": "It's exciting, 'cause they're like, they're trying to figure out\nwhat the bottlenecks are, how to remove the bottlenecks,\nhow to make sure that,",
    "start": "13776994",
    "end": "13783035"
  },
  {
    "text": "there's just so many really cool things about putting together a data center,",
    "start": "13783035",
    "end": "13788071"
  },
  {
    "text": "'cause everything has to work.",
    "start": "13788071",
    "end": "13790571"
  },
  {
    "text": "The people that do like the CIS admin, the machine learning, all that\nis the exciting thing, so on,",
    "start": "13793103",
    "end": "13798296"
  },
  {
    "text": "but really, the people that\nrun everything (chuckles) are the folks that know\nthe low level software",
    "start": "13798296",
    "end": "13804920"
  },
  {
    "text": "and hardware that runs everything, the networking, all of that. And so, you have to make sure",
    "start": "13804920",
    "end": "13810472"
  },
  {
    "text": "you have procedures that test everything. I think they're using ethernet. I don't know how they're\ndoing the networking, but- - They're using Nvidia\nSpectrum-X ethernet.",
    "start": "13810472",
    "end": "13817868"
  },
  {
    "text": "There's actually like, I think yeah, the unsung heroes are the\ncooling and electrical systems, which are just like\n(Lex chuckles)",
    "start": "13817868",
    "end": "13823065"
  },
  {
    "text": "- Exactly.\n- glossed over. - [Lex] Yeah. - But I think one story that maybe is like exemplifies\nhow insane this stuff is,",
    "start": "13823065",
    "end": "13830447"
  },
  {
    "text": "is when you're training,\nyou're always doing, you're running through the model a bunch,",
    "start": "13830447",
    "end": "13835996"
  },
  {
    "text": "in the most simplistic terms, running through the model a bunch. And then, you're gonna exchange everything",
    "start": "13835996",
    "end": "13841313"
  },
  {
    "text": "and synchronize the weights. So, you'll do a step, this is like a step in model training. And every step, your loss\ngoes down, hopefully.",
    "start": "13841314",
    "end": "13847519"
  },
  {
    "text": "And it doesn't always,\nbut in the simplest terms, you'll be computing a lot,\nand then you'll exchange.",
    "start": "13847519",
    "end": "13852630"
  },
  {
    "text": "The interesting thing is\nGPU power is most of it, networking power is some,\nbut it's a lot less. So, while you're computing,",
    "start": "13852630",
    "end": "13857818"
  },
  {
    "text": "your power for your GPUs is here. But then when you're exchanging weights, if you're not able to\noverlap communications",
    "start": "13857819",
    "end": "13863238"
  },
  {
    "text": "and compute perfectly,\nthere may be a time period where your GPUs are just idle,\nand you're exchanging weights and you're like, hey,\nthe model's updating.",
    "start": "13863238",
    "end": "13869449"
  },
  {
    "text": "So, you're exchanging the\ngradient, you do the model update, and then you start training again. So, the power goes...",
    "start": "13869449",
    "end": "13875111"
  },
  {
    "text": "- Mm-hmm.\n- And it's super spiky. - Yeah.\n- And so, funnily enough, when you talk about the\nscale of data center power,",
    "start": "13875111",
    "end": "13881907"
  },
  {
    "text": "you can blow stuff up so easily.\n- Yeah. - And so, Meta actually has,",
    "start": "13881907",
    "end": "13887128"
  },
  {
    "text": "accidentally opened upstream\nsomething to code in PyTorch where they added an operator. And I kid you not, whoever made this,",
    "start": "13887128",
    "end": "13893318"
  },
  {
    "text": "I wanna hug the guy,\nbecause it says PyTorch, it's like pytorch.powerplantnoblowup",
    "start": "13893319",
    "end": "13899269"
  },
  {
    "text": "(Lex laughing)\nequals zero or equal one. (Nathan laughing)\nAnd what it does, what it does is amazing. - [Lex] Yeah.",
    "start": "13899269",
    "end": "13904682"
  },
  {
    "text": "- When you're exchanging the weights, the GP will just compute fake numbers, so the power doesn't spike too much.",
    "start": "13904682",
    "end": "13909869"
  },
  {
    "text": "And so then, the power\nplants don't blow up because the transient\nspikes screw stuff up. - Well, that makes sense.",
    "start": "13909869",
    "end": "13915102"
  },
  {
    "text": "You have to do that kind of thing. You have to make sure they're not idle. Yeah, that's-\n- And Elon's solution",
    "start": "13915102",
    "end": "13920309"
  },
  {
    "text": "was like, \"Let me throw a\nbunch of Tesla mega packs and a few other things.\" - Stabilize, yeah.\n- Everyone has different solutions, but Meta's at least\n(Lex laughing)",
    "start": "13920309",
    "end": "13925958"
  },
  {
    "text": "was publicly and openly known, which is just like, set this operator. And what this operator does\nis it just makes the GPUs",
    "start": "13925958",
    "end": "13931628"
  },
  {
    "text": "compute nothing so that\nthe power doesn't spike. - But that just tells you how much power you're working with.",
    "start": "13931628",
    "end": "13937236"
  },
  {
    "text": "It's insane. It's insane.\n- People should just go Google like scale, like what does X watts do?",
    "start": "13937237",
    "end": "13943428"
  },
  {
    "text": "And go through all the\nscales from one watt to a kilowatt to a megawatt,\nand you look and stare at that,",
    "start": "13943428",
    "end": "13948667"
  },
  {
    "text": "and you're how high on\nthe list a gigawatt is, and it's mind-blowing.",
    "start": "13948667",
    "end": "13953900"
  },
  {
    "text": "- Can you say something about the cooling? So, I know Elon's using liquid cooling,",
    "start": "13953900",
    "end": "13959090"
  },
  {
    "text": "I believe, in all cases. That's a new thing, right? Most of 'em don't use liquid cooling.",
    "start": "13959090",
    "end": "13965333"
  },
  {
    "text": "Is there something interesting\nto say about the cooling? - Yeah, yeah. So, air cooling has been\nthe de facto standard. Throw a bunch of metal heat\npipes, et cetera, and fans,",
    "start": "13965333",
    "end": "13973219"
  },
  {
    "text": "and that's cooled, that's\nbeen enough to cool it. People have been dabbling\nin water cooling. Google's TPUs are water-cooled.",
    "start": "13973219",
    "end": "13981364"
  },
  {
    "text": "So, they've been doing\nthat for a few years. But with GPUs, no one's ever done... And no one's ever done the scale",
    "start": "13981365",
    "end": "13986957"
  },
  {
    "text": "of water cooling that Elon just did. Now, next generation Nvidia is for the highest end GPU,\nit is mandatory water cooling.",
    "start": "13986957",
    "end": "13995239"
  },
  {
    "text": "You have to water cool it. But Elon did it on this\ncurrent generation, and that required a lot of stuff.",
    "start": "13995239",
    "end": "14000406"
  },
  {
    "text": "If you look at some of\nthe satellite photos and stuff of the Memphis facility, there's all these external\nwater chillers that are sitting,",
    "start": "14000406",
    "end": "14007793"
  },
  {
    "text": "basically, it looks like\na semi-truck pod thing. What's it called? The container. But really, those are water chillers.",
    "start": "14007793",
    "end": "14013452"
  },
  {
    "text": "And he has like 90 of those water chillers just sitting outside, 90\ndifferent containers with water, like chill the water, bring\nit back to the data center,",
    "start": "14013453",
    "end": "14020871"
  },
  {
    "text": "and then you distribute\nit to all the chips, pull all the heat out,\nand then send it back. And this is both a way to cool the chips,",
    "start": "14020872",
    "end": "14027855"
  },
  {
    "text": "but it's also an efficiency thing. And going back to that sort of three\nvector thing right there is,",
    "start": "14027855",
    "end": "14033788"
  },
  {
    "text": "there is memory bandwidth\nFLOPS and interconnect. The closer the chips are together, the easier it is to do\nhigh speed interconnects.",
    "start": "14033788",
    "end": "14042748"
  },
  {
    "text": "And so, this is also like a reason why you wanna go water cooling is because you can just put the chips right next to each other,",
    "start": "14042748",
    "end": "14048680"
  },
  {
    "text": "and therefore get higher\nspeed connectivity. - I gotta ask you, so in\none of your recent posts,",
    "start": "14048680",
    "end": "14057678"
  },
  {
    "text": "there's a section called\ncluster measuring contest, so... - There's another word there,\nbut I won't say it, you know.",
    "start": "14058851",
    "end": "14064920"
  },
  {
    "text": "(Dylan and Nathan laughing) - Who's got the biggest\nnow and who's gonna have",
    "start": "14064920",
    "end": "14071001"
  },
  {
    "text": "the biggest?\n- Today, individual largest is Elon. - Right. Elon's cluster.",
    "start": "14071001",
    "end": "14075678"
  },
  {
    "text": "- Elon's cluster in Memphis, 200,000 GPUs. - Okay. - Meta has like 128,000,\nOpenAI has 100,000 now.",
    "start": "14076908",
    "end": "14083444"
  },
  {
    "text": "Now, to be clear, other companies\nhave more GPUs than Elon. They just don't have 'em in one place. And for training, you want\nthem tightly connected.",
    "start": "14083444",
    "end": "14090744"
  },
  {
    "text": "There's some techniques\nthat people are researching and working on that lets you\ntrain across multiple regions.",
    "start": "14090744",
    "end": "14096334"
  },
  {
    "text": "But for the most part, you\nwant them all in one area. So, you can connect them\nwith high speed networking.",
    "start": "14096334",
    "end": "14102476"
  },
  {
    "text": "And so, Elon today has 200,000 H100s. And 100,000 H100s, 100,000 H200s.",
    "start": "14102476",
    "end": "14109546"
  },
  {
    "text": "Meta, OpenAI, and Amazon all have",
    "start": "14109546",
    "end": "14112296"
  },
  {
    "text": "on the scale of 100,000,\na little bit less. But this year, this year,\npeople are building much more.",
    "start": "14114564",
    "end": "14120268"
  },
  {
    "text": "Anthropic and Amazon\nare building a cluster of 400,000 Trainium 2, which\nis Amazon-specific chip",
    "start": "14120268",
    "end": "14125492"
  },
  {
    "text": "trying to get away from Nvidia.",
    "start": "14125492",
    "end": "14128075"
  },
  {
    "text": "Meta and OpenAI have scales\nfor hundreds of thousands, but by next year, you'll have like 500,000\nto 700,000 GPU clusters.",
    "start": "14130502",
    "end": "14138011"
  },
  {
    "text": "And note, those GPUs are\nmuch higher power consumption than existing ones. Hopper, 700 watts, Blackwell\ngoes to 1,200 watts.",
    "start": "14138011",
    "end": "14145670"
  },
  {
    "text": "So, the power per chip is growing\n(Lex laughing) and the number of chips is growing. - Nuts. Elon said he'll get to a million.",
    "start": "14145670",
    "end": "14153412"
  },
  {
    "text": "You think that's actually feasible? - I don't doubt Elon. The filings that he has for the power plan",
    "start": "14154321",
    "end": "14161769"
  },
  {
    "text": "and the Tesla battery packs, it's clear he has some\ncrazy plans for Memphis. Like permits and stuff is open record.",
    "start": "14161769",
    "end": "14168721"
  },
  {
    "text": "But it's not quite clear what\nand what the timescales are.",
    "start": "14168721",
    "end": "14173721"
  },
  {
    "text": "I just never doubt Elon.\nHe's gonna surprise us. - So, what's the idea with these clusters? If you have a million GPUs,",
    "start": "14173809",
    "end": "14179283"
  },
  {
    "text": "what percentage in let's say, two, three years is used for training",
    "start": "14179283",
    "end": "14186722"
  },
  {
    "text": "and what percent of pre-training and what percent is used for the actual computation?\n- So, these mega clusters",
    "start": "14186722",
    "end": "14191912"
  },
  {
    "text": "make no sense for inference. You could route inference\nthere and just not train.",
    "start": "14191912",
    "end": "14196931"
  },
  {
    "text": "But most of the inference\ncapacity is being, hey, I've got a 30-megawatt\ndata center here. I've got 50 megawatts here,\nI've got 100 here, whatever.",
    "start": "14196931",
    "end": "14203823"
  },
  {
    "text": "I'll just throw inference in all of those, because the mega clusters, multi gigawatt data centers,\nI want to train there.",
    "start": "14203823",
    "end": "14210879"
  },
  {
    "text": "Because that's where all\nof my GPUs are co-located, where I can put them at a super high networking\nspeed connected together.",
    "start": "14210879",
    "end": "14216632"
  },
  {
    "text": "Because that's what you need for training. Now, with pre-training,\nthis is the old scale. You would increase parameters,",
    "start": "14216632",
    "end": "14222294"
  },
  {
    "text": "you'd increase data, model gets better. That doesn't apply anymore, because there's not much more\ndata in the pre-training side.",
    "start": "14222294",
    "end": "14229745"
  },
  {
    "text": "Yes, there's video and audio and image that has not been fully\ntaken advantage of. So, there's a lot more scaling,",
    "start": "14229745",
    "end": "14235054"
  },
  {
    "text": "but a lot of people have transcript, taken transcripts of YouTube videos, and that gets you a lot of the data.",
    "start": "14235054",
    "end": "14240272"
  },
  {
    "text": "Doesn't get you all of the learning value out of the video and image data, but there's still scaling\nto be done on pre-training.",
    "start": "14240272",
    "end": "14246190"
  },
  {
    "text": "But this post-training world is where all the FLOPS are gonna be spent. The model's gonna play with itself. It's gonna self-play, it's\ngonna do verifiable tasks,",
    "start": "14246191",
    "end": "14253566"
  },
  {
    "text": "it's gonna do computer use in sandboxes. It might even do\nsimulated robotics things.",
    "start": "14253566",
    "end": "14258793"
  },
  {
    "text": "All of these things are\ngonna be environments where compute is spent in,\nquote, unquote, \"post-training\".",
    "start": "14258793",
    "end": "14264685"
  },
  {
    "text": "But I think it's gonna be good. We're gonna drop the\npost from post-training. - Yeah. Wow.\n- It's gonna be pre-training",
    "start": "14264685",
    "end": "14269812"
  },
  {
    "text": "and it's gonna be training, I think. At point at some,\n(Dylan drowns out Nathan) at some point.\n(Nathan laughing) Because for the bulk\nof the last few years,",
    "start": "14269812",
    "end": "14277072"
  },
  {
    "text": "pre-training has dwarfed post-training. - [Lex] Mm-hmm. - But with these verifiable methods, especially ones that scale really,",
    "start": "14277073",
    "end": "14284129"
  },
  {
    "text": "potentially infinitely, like computer use and robotics,\nnot just math and coding, where you can verify what's happening,",
    "start": "14284129",
    "end": "14289180"
  },
  {
    "text": "those infinitely verifiable tasks, it seems you can spend as much\ncompute as you want on them. - Especially at the\ncontext length increase.",
    "start": "14289180",
    "end": "14295042"
  },
  {
    "text": "'Cause at the end of pre-training is when you increase the\ncontext length for these models. And we've talked earlier\nin the conversation",
    "start": "14295042",
    "end": "14301492"
  },
  {
    "text": "about how the context length,\nwhen you have a long input, is much easier to manage than output. And a lot of these post-training",
    "start": "14301492",
    "end": "14307253"
  },
  {
    "text": "and reasoning techniques\nrely on a ton of sampling and it's becoming\nincreasingly long context.",
    "start": "14307253",
    "end": "14312562"
  },
  {
    "text": "So, there's just your, effectively your compute\nefficiency goes down.",
    "start": "14312562",
    "end": "14317879"
  },
  {
    "text": "I think FLOPS is the standard\nfor how you measure it. But with RL and you have\nto do all these things,",
    "start": "14317879",
    "end": "14323452"
  },
  {
    "text": "where you move your weights around in a different way than at\npre-training and just generation.",
    "start": "14323452",
    "end": "14329532"
  },
  {
    "text": "It's going to be become less efficient and FLOPS is gonna be\nless of a useful term. And then, as the\ninfrastructure gets better,",
    "start": "14329533",
    "end": "14335332"
  },
  {
    "text": "it's probably gonna go back to FLOPS. - So, all of the things\nwe've been talking about is most likely going to be Nvidia.",
    "start": "14335332",
    "end": "14342273"
  },
  {
    "text": "Is there any competitors? - Google, I ignored them.\n- TPU. Yeah. (chuckles) - I was like, huh?\n- Yeah,",
    "start": "14342273",
    "end": "14348278"
  },
  {
    "text": "what's the story with TPU? Like what's the... - TPU is awesome. It's great. Google is, they're a bit more tepid",
    "start": "14348278",
    "end": "14356025"
  },
  {
    "text": "on building data centers for some reason. They're building big data\ncenters, don't get me wrong. And they actually have\nthe biggest cluster.",
    "start": "14356025",
    "end": "14361969"
  },
  {
    "text": "I was talking about Nvidia clusters, they actually have the\nbiggest cluster, period. But the way they do it\nis very interesting.",
    "start": "14361969",
    "end": "14368729"
  },
  {
    "text": "They have two data center super regions in that, the data center isn't physically",
    "start": "14368729",
    "end": "14374460"
  },
  {
    "text": "like all of the GPUs aren't\nphysically on one site, but they're like 30 miles from each other. Or not GPUs, TPUs. They have like in Iowa and Nebraska,",
    "start": "14374460",
    "end": "14381466"
  },
  {
    "text": "they have four data centers that are just right next to each other. - Why doesn't Google\nflex it's cluster size?",
    "start": "14381466",
    "end": "14388357"
  },
  {
    "text": "- Go to multi-datacenter training. There's good images in there.\nSo, I'll show you what I mean. It's just SemiAnalysis multi-datacenter.",
    "start": "14388357",
    "end": "14395088"
  },
  {
    "text": "So, this is an image of what a standard Google\ndata center looks like. By the way, their data\ncenters look very different",
    "start": "14395088",
    "end": "14400536"
  },
  {
    "text": "than anyone else's data centers. - [Lex] What are we looking at here? - So, these are, yeah, so if you see this image right,",
    "start": "14400536",
    "end": "14405597"
  },
  {
    "text": "in this center, there are\nthese big rectangular boxes. Those are where the actual chips are kept. And then, if you scroll\ndown a little bit further,",
    "start": "14405597",
    "end": "14413417"
  },
  {
    "text": "you can see there's\nlike these water pipes, there's these chiller\ncooling towers in the top, and a bunch of diesel generators.",
    "start": "14413417",
    "end": "14420010"
  },
  {
    "text": "The diesel generators are backup power. The data center itself look physically smaller\nthan the water chillers.",
    "start": "14420010",
    "end": "14426910"
  },
  {
    "text": "So, the chips are actually\neasier to keep together, but then cooling all the water for the water cooling is very difficult.",
    "start": "14426911",
    "end": "14433441"
  },
  {
    "text": "So, Google has a very\nadvanced infrastructure that no one else has for the TPU. And what they do is they've\nstamped these data center,",
    "start": "14433441",
    "end": "14440587"
  },
  {
    "text": "they've stamped a bunch\nof these data centers out in a few regions. So, if you go a little bit further down,",
    "start": "14440587",
    "end": "14446462"
  },
  {
    "text": "this is a Microsoft, this is in Arizona. This is where GPT-5, quote,\nunquote, \"will be trained\". (Lex laughing)",
    "start": "14446462",
    "end": "14452974"
  },
  {
    "text": "- [Nathan] If it doesn't exist already. - Yeah, if it doesn't exist already. But each of these data centers, I've shown a couple images of them,",
    "start": "14452974",
    "end": "14458986"
  },
  {
    "text": "they're really closely co-located in the same region, Nebraska, Iowa. And then, they also have a\nsimilar one in Ohio complex.",
    "start": "14458986",
    "end": "14466565"
  },
  {
    "text": "And so, these data centers are\nreally close to each other. And what they've done is they've connected them super\nhigh bandwidth with fiber.",
    "start": "14466565",
    "end": "14473408"
  },
  {
    "text": "And so, these are just\na bunch of data centers. And the point here is that Google has a very advanced infrastructure,",
    "start": "14473408",
    "end": "14479690"
  },
  {
    "text": "very tightly connected in a small region. So, Elon will always have the biggest cluster fully connected,",
    "start": "14479690",
    "end": "14484742"
  },
  {
    "text": "because it's all in one building. - Yeah.\n- And he's completely right on that. Google has the biggest cluster,",
    "start": "14484743",
    "end": "14490482"
  },
  {
    "text": "but you have to spread over three sites, and by a significant margin, but you have to go across multiple sites.",
    "start": "14490482",
    "end": "14495717"
  },
  {
    "text": "- Why doesn't Google compete with Nvidia? Why don't they sell TPUs?",
    "start": "14495717",
    "end": "14502027"
  },
  {
    "text": "- I think there's a\ncouple problems with it. It's like one, TPU has been a form",
    "start": "14502027",
    "end": "14506877"
  },
  {
    "text": "of allowing search to\nbe really freaking cheap and build models for that. And so, a big chunk of the search,",
    "start": "14508067",
    "end": "14515065"
  },
  {
    "text": "GPU purchases or TPU purchases, or big chunk of Google's\npurchases and usage,",
    "start": "14515065",
    "end": "14520774"
  },
  {
    "text": "all of it is for internal workloads. Whether it be Search,\nnow, Gemini, YouTube,",
    "start": "14520774",
    "end": "14526666"
  },
  {
    "text": "all these different\napplications that they have ads, these are where all their\nTPUs are being spent,",
    "start": "14526666",
    "end": "14532236"
  },
  {
    "text": "and that's what they're hyperfocused on. And so, there's certain\naspects of the architecture that are optimized for their use case",
    "start": "14532236",
    "end": "14538763"
  },
  {
    "text": "that are not optimized elsewhere. One simple one is they've\nopen sourced the Gemma model and they called it Gemma 7B.",
    "start": "14538763",
    "end": "14545991"
  },
  {
    "text": "But then, it's actually\n8 billion parameters because the vocabulary is so large. (Lex laughing) And the reason they made\nthe vocabulary so large",
    "start": "14545991",
    "end": "14552092"
  },
  {
    "text": "is because TPU's\nmatrix-multiply unit is massive. Because that's what they've optimized for.",
    "start": "14552092",
    "end": "14558019"
  },
  {
    "text": "And so, they decided, oh, well, I'll just make the vocabulary large too, even though it makes no sense to do so in such a small model, because that fits on their hardware.",
    "start": "14558019",
    "end": "14564201"
  },
  {
    "text": "So, Gemma doesn't run as efficiently on a GPU as a Llama does. But vice versa, Llama doesn't run",
    "start": "14564201",
    "end": "14569762"
  },
  {
    "text": "as efficiently on a TPU as a Gemma does. And it's so like, there's certain aspects of\nhardware, software, co-design.",
    "start": "14569762",
    "end": "14575692"
  },
  {
    "text": "So, all their search models, or their ranking and\nrecommendation models, all these different models that are AI, but not like GenAI,",
    "start": "14575692",
    "end": "14582379"
  },
  {
    "text": "have been hyper optimized\nwith TPUs forever. The software stack is super optimized, but all of this software stack",
    "start": "14582379",
    "end": "14588681"
  },
  {
    "text": "has not been released publicly at all. Very small portions of\nit, Jax and XLA have been.",
    "start": "14588681",
    "end": "14593934"
  },
  {
    "text": "But the experience when\nyou're inside of Google and you're training on\nTPUs as a researcher, you don't need to know anything",
    "start": "14593934",
    "end": "14599621"
  },
  {
    "text": "about the hardware in many cases. It's pretty beautiful. - They all love it.\n- But soon as you step outside...\n(Nathan chuckles)",
    "start": "14599621",
    "end": "14605240"
  },
  {
    "text": "- A lot of 'em go back. (chuckles) They leave Google and then they go back. - Yeah.\n- Yeah. They leave and they start a company,",
    "start": "14605240",
    "end": "14611194"
  },
  {
    "text": "'cause they have all these\namazing research ideas and they're like, wait,\ninfrastructure's hard, software is hard, and this is on GPUs.",
    "start": "14611194",
    "end": "14616587"
  },
  {
    "text": "Or if they try to use TPUs, same thing, 'cause they don't have\naccess to all this code. And so, it's like, how\ndo you convince a company",
    "start": "14616587",
    "end": "14621747"
  },
  {
    "text": "whose Golden Goose is Search, where they're making hundreds\nof billions of dollars from, to start selling GPU, or TPUs,",
    "start": "14621747",
    "end": "14628469"
  },
  {
    "text": "which they used to only\nbuy a couple billion of... I think in 2023, they bought\nlike a couple billion,",
    "start": "14628469",
    "end": "14635818"
  },
  {
    "text": "and now they're buying 10\nbillion to $15 billion worth. But how do you convince them that they should just\nbuy like twice as many",
    "start": "14635819",
    "end": "14641135"
  },
  {
    "text": "and figure out how to sell\nthem, and make $30 billion? Like, who cares about making $30 billion? - Won't that 30 billion",
    "start": "14641135",
    "end": "14647375"
  },
  {
    "text": "exceed actually the\nsearch profit eventually? - You're always gonna make\nmore money on services",
    "start": "14647375",
    "end": "14653485"
  },
  {
    "text": "than hardware.\n- Always. - Like yeah, to be clear, today, people are spending a lot more",
    "start": "14653486",
    "end": "14659059"
  },
  {
    "text": "on hardware than they are the services, because the hardware\nruns the service spend.",
    "start": "14659059",
    "end": "14665105"
  },
  {
    "text": "- Yeah.\n- But like- - [Lex] You're investing, yeah. - If there's no revenue for AI stuff or not enough revenue, then\nobviously, it's gonna blow up.",
    "start": "14665105",
    "end": "14671605"
  },
  {
    "text": "People won't continue to\nspend on GPUs forever. And then, Nvidia's trying to\nmove up the stack with software",
    "start": "14671606",
    "end": "14676778"
  },
  {
    "text": "that they're trying to\nsell and license and stuff. But Google has never had that DNA",
    "start": "14676778",
    "end": "14681930"
  },
  {
    "text": "of this is a product we should sell. The Google Cloud does, which is a separate\norganization from the TPU team,",
    "start": "14681930",
    "end": "14687968"
  },
  {
    "text": "which is a separate organization\nfrom the DeepMind team, which is a separate organization\nfrom the Search team, there's a lot of bureaucracy here. - Wait, Google Cloud is a\nseparate team than the TPU team?",
    "start": "14687968",
    "end": "14695598"
  },
  {
    "text": "- Technically, TPU sits\nunder infrastructure, which sits under Google Cloud, but Google Cloud, for renting stuff",
    "start": "14695598",
    "end": "14702544"
  },
  {
    "text": "and TPU architecture are\nvery different goals. And hardware and software,\nlike all of this.",
    "start": "14703728",
    "end": "14710038"
  },
  {
    "text": "The JAX, XLA teams do not serve Google's\ncustomers externally. Whereas Nvidia's various CUDA teams",
    "start": "14710038",
    "end": "14715649"
  },
  {
    "text": "for things like NCCL\nserve external customers. - [Lex] Mm-hmm. - The internal teams like\nJAX and XLA and stuff,",
    "start": "14715649",
    "end": "14722135"
  },
  {
    "text": "they more so serve DeepMind and Search.\n- Yeah. - And so, their customers' different. They're not building a product for them.",
    "start": "14722135",
    "end": "14727266"
  },
  {
    "text": "- Do you understand why AWS keeps winning versus Azure for cloud\nversus Google Cloud?",
    "start": "14727266",
    "end": "14733930"
  },
  {
    "text": "- Yeah, there's-\n- Google Cloud is tiny, isn't it, relative to AWS?\n- Google Cloud is third. - [Nathan] Yeah, yeah. - Microsoft is the second biggest,",
    "start": "14734935",
    "end": "14741446"
  },
  {
    "text": "but Amazon is the biggest.\n- Yeah. - And Microsoft deceptively sort of includes Microsoft\nOffice 365, and things like that.",
    "start": "14741446",
    "end": "14748206"
  },
  {
    "text": "Like some of these\nenterprise-wide licenses. So, in reality, the Gulf is even larger, Microsoft is still second though.",
    "start": "14748206",
    "end": "14753723"
  },
  {
    "text": "Amazon is way bigger. Why? Because using AWS is better and easier. And in many cases, it's cheaper.\n- It was first.",
    "start": "14753723",
    "end": "14759462"
  },
  {
    "text": "- And it's first.\n- It was first. - [Lex] Yeah, but there's a lot of things that are first that-\n- Well, it's easier... It's harder to switch",
    "start": "14759462",
    "end": "14764897"
  },
  {
    "text": "than it is to-\n- Yeah, okay. - AWS is-\n- Because it's large- - There's big fees for switching too. - AWS generates over 80%\n(Nathan chuckles)",
    "start": "14764898",
    "end": "14770915"
  },
  {
    "text": "of Amazon's profit, I think over 90%. - That's insane.\n- The distribution centers are just like, one day, we'll decide to make money from\nthis, but they haven't yet.",
    "start": "14770915",
    "end": "14778839"
  },
  {
    "text": "They make tiny little profit from- - Yeah, one day, Amazon\nPrime will triple in price. - You would think they\nwould improve AWS interface,",
    "start": "14778839",
    "end": "14786931"
  },
  {
    "text": "'cause it's horrible. It's clunky, but everybody is... (Lex laughing)\n- [Nathan] I don't, yeah,",
    "start": "14786931",
    "end": "14792402"
  },
  {
    "text": "one would think. - I think actually, Google's\ninterface is sometimes nice, but it's also they don't care about anyone besides their top customers.",
    "start": "14792402",
    "end": "14798866"
  },
  {
    "text": "- Yeah, exactly.\n- And like their customer service sucks and they have a lot less, like... - All these companies, they optimize for the big customers, yeah.",
    "start": "14798866",
    "end": "14805219"
  },
  {
    "text": "It's supposed to be for business.\n- Well, and Amazon has always optimized for the\nsmall customer too though. Obviously, they optimize a\nlot for the big customer,",
    "start": "14805219",
    "end": "14811286"
  },
  {
    "text": "but when they started, they just would go to like random Bay Area\nthings and give out credits.",
    "start": "14811286",
    "end": "14816350"
  },
  {
    "text": "And then, they like, or just put in your\ncredit card and use us. It went back in the early days. So, they've always, the business has grown\nwith them in burgeon.",
    "start": "14816350",
    "end": "14823128"
  },
  {
    "text": "So, why does Amazon, why is\nSnowflake all over Amazon? Because Snowflake, in the beginning, when Amazon didn't care about\nthem, was still using Amazon.",
    "start": "14823128",
    "end": "14830218"
  },
  {
    "text": "And then, of course, one day, Snowflake and Amazon has\na super huge partnership, but this is the case, like Amazon's user experience\nand quality is better.",
    "start": "14830218",
    "end": "14837063"
  },
  {
    "text": "Also, a lot of the silicon\nthey've engineered makes them have a lower cost structure\nin traditional cloud, storage, CPU, networking,",
    "start": "14837063",
    "end": "14843068"
  },
  {
    "text": "that kind of stuff than in databases. I think like four of Amazon's\ntop five revenue products,",
    "start": "14843068",
    "end": "14852157"
  },
  {
    "text": "margin products are gross profit products, are all database-related products like Redshift and all these things.",
    "start": "14852158",
    "end": "14858414"
  },
  {
    "text": "So, Amazon has a very good\nsilicon to user experience,",
    "start": "14858414",
    "end": "14863414"
  },
  {
    "text": "entire pipeline with AWS. I think Google, their silicon teams, yeah, they have awesome\nsilicon internally,",
    "start": "14863476",
    "end": "14869424"
  },
  {
    "text": "TPU, the YouTube chip, some of these other\nchips that they've made. And the problem is they're not\nserving external customers,",
    "start": "14869424",
    "end": "14876918"
  },
  {
    "text": "they're serving internal customers. - Nvidia's entire culture is designed from the bottom-up to do this. There's this recent book,\n\"The Nvidia Way\" by Tae Kim,",
    "start": "14876918",
    "end": "14884190"
  },
  {
    "text": "that details this and they're how they look\nfor future opportunities and ready their CUDA software libraries",
    "start": "14884190",
    "end": "14891726"
  },
  {
    "text": "to make it so that new applications of high performance computing can very rapidly be evolved\non CUDA and Nvidia chips.",
    "start": "14891726",
    "end": "14899648"
  },
  {
    "text": "And that is entirely different than Google as a services business. - Yeah, Nvidia, it should be said,",
    "start": "14899649",
    "end": "14907124"
  },
  {
    "text": "is a truly special company. There's the whole, the\nculture and everything, they're really optimized\nfor that kind of thing.",
    "start": "14907124",
    "end": "14913164"
  },
  {
    "text": "Speaking of which, is there somebody that can even challenge\nNvidia hardware-wise? Intel, AMD?",
    "start": "14913164",
    "end": "14918612"
  },
  {
    "text": "- I really don't think so. We went through a very\nlong process of working",
    "start": "14919674",
    "end": "14925093"
  },
  {
    "text": "with AMD on training on their\nGPU's inference and stuff. And they're decent. Their hardware is better in\nmany ways than in Nvidia's.",
    "start": "14925093",
    "end": "14932785"
  },
  {
    "text": "The problem is their\nsoftware is really bad, and I think they're getting better, they're getting better\nfaster, but they're just,",
    "start": "14932785",
    "end": "14937876"
  },
  {
    "text": "the Gulf is so large and they don't spend enough resources on, or haven't historically.",
    "start": "14937876",
    "end": "14943849"
  },
  {
    "text": "Maybe they're changing their tune now, but for multiple months, we\nwere submitting the most bugs.",
    "start": "14943849",
    "end": "14948873"
  },
  {
    "text": "Like us, SemiAnalysis.\n- Mm-hmm. - Like what the fuck? Why are we submitting the most bugs?",
    "start": "14948873",
    "end": "14954223"
  },
  {
    "text": "Because they only, and they only cared about\ntheir biggest customers. And so, they'd ship them a\nprivate image, blah, blah, blah.",
    "start": "14954223",
    "end": "14959543"
  },
  {
    "text": "And it's like, okay, but\nI am just using PyTorch and I wanna use the publicly available libraries\n- Mm-hmm.",
    "start": "14959543",
    "end": "14965266"
  },
  {
    "text": "Yeah.\n- and you don't care about that. So, they're getting better. But I think AMD's not possible.",
    "start": "14965266",
    "end": "14970552"
  },
  {
    "text": "Intel's obviously in\ndire straits right now and needs to be saved somehow. Very important for national security,",
    "start": "14970552",
    "end": "14976444"
  },
  {
    "text": "for American technology dominance. - Can you explain the, obviously, so why are they in dire straits?",
    "start": "14976444",
    "end": "14981663"
  },
  {
    "text": "- Going back to earlier,\nonly three companies can R&D. - Yeah.\n- Taiwan, Hsinchu,",
    "start": "14981663",
    "end": "14987019"
  },
  {
    "text": "Samsung, Pyongyang, and\nthen Intel, Hillsboro. Samsung's doing horribly.\nIntel's doing horribly.",
    "start": "14987019",
    "end": "14993479"
  },
  {
    "text": "We could be in a world where there's only one\ncompany that can do R&D, and that one company already\nmanufactures most of chips. They've been gaining market share anyways.",
    "start": "14993479",
    "end": "14999243"
  },
  {
    "text": "But that's a critical thing. So, what happens to Taiwan means the rest of the world's\nsemiconductor industry, and therefore tech, relies on Taiwan.",
    "start": "14999243",
    "end": "15007034"
  },
  {
    "text": "And that's obviously precarious. As far as Intel, they've been\nslowly, steadily declining.",
    "start": "15007034",
    "end": "15012963"
  },
  {
    "text": "They were on top of servers and PCs, but now, Apple's done the M1 and Nvidia's releasing a PC chip,",
    "start": "15012963",
    "end": "15019436"
  },
  {
    "text": "and Qualcomm's releasing a PC chip, and in servers, hyperscalers are all making their own ARM-based server chips.",
    "start": "15019436",
    "end": "15025264"
  },
  {
    "text": "And Intel has no AI silicon wins. They have very small wins.",
    "start": "15025264",
    "end": "15030935"
  },
  {
    "text": "And they never got into mobile, because they said no to the iPhone. And all these things have compounded and they've lost their\nprocess technology leadership.",
    "start": "15030935",
    "end": "15037849"
  },
  {
    "text": "They were ahead for 20 years and now they're behind by\nat least a couple years. And they're trying to catch back up and we'll see if their 18A,\n14A strategy works out,",
    "start": "15037849",
    "end": "15046165"
  },
  {
    "text": "where they try and leapfrog TSMC. But like... And Intel is just losing\ntons of money anyways.",
    "start": "15046165",
    "end": "15051344"
  },
  {
    "text": "And they just fired their CEO, even though their CEO was the only person who understood the company well. We'll see.",
    "start": "15051344",
    "end": "15057110"
  },
  {
    "text": "He was not the best, but he\nwas pretty good, relatively. Technical guy. - [Lex] Where does Intel\nmake most of its money?",
    "start": "15057110",
    "end": "15063244"
  },
  {
    "text": "The CPU still, right?\n- PCs and data center CPUs, yeah. But data center CPUs are all going Cloud and Amazon, Microsoft, Google\nare making ARM-based CPUs.",
    "start": "15063244",
    "end": "15071173"
  },
  {
    "text": "And then, PC side, AMDs\ngained market share. Nvidia's launching a chip\nthat's not gonna be a success.",
    "start": "15071173",
    "end": "15077113"
  },
  {
    "text": "Mediatek, Qualcomm ever launch chips. Apple's doing well. They could get squeezed\na little bit in PC,",
    "start": "15077114",
    "end": "15083233"
  },
  {
    "text": "although PC generally I imagine, will just stick Intel\nmostly for Windows side. - Let's talk about the broad\nAI race. Who do you think wins?",
    "start": "15083233",
    "end": "15091276"
  },
  {
    "start": "15086000",
    "end": "15699000"
  },
  {
    "text": "We talked about Google, Meta, xAI.\n- The default leader has been Google because of\ntheir infrastructure advantage.",
    "start": "15091276",
    "end": "15097771"
  },
  {
    "text": "- [Lex] Well, like in the\nnews, OpenAI is the leader. - They're leading in the- - They have the best model.",
    "start": "15097771",
    "end": "15103217"
  },
  {
    "text": "- They have the best\nmodel that people can use. And they're experts-\n- And they have the most AI revenue.",
    "start": "15103217",
    "end": "15108261"
  },
  {
    "text": "- Yeah. OpenAI is winning. - So, who's making money on AI right now?",
    "start": "15108261",
    "end": "15113837"
  },
  {
    "text": "Is anyone making money? - So, accounting profit-wise,\nMicrosoft is making money, but they're spending a lot of CapEx,",
    "start": "15113837",
    "end": "15119918"
  },
  {
    "text": "and that gets depreciated over years. Meta's making tons of money, but with recommendation systems,",
    "start": "15119918",
    "end": "15125252"
  },
  {
    "text": "which is AI,\n(Nathan chuckles) but not with Llama.\n- Right. - Llama's losing money for sure.",
    "start": "15125252",
    "end": "15130433"
  },
  {
    "text": "I think Anthropic and OpenAI\nare obviously not making money, 'cause otherwise, they\nwouldn't be raising money. They have to raise money to build more.",
    "start": "15130433",
    "end": "15137921"
  },
  {
    "text": "Although theoretically,\nthey are making money. You spent a few hundred\nmillion dollars on GPT-4, and it's doing billions in revenue.",
    "start": "15137921",
    "end": "15144195"
  },
  {
    "text": "So, obviously, it's making money. Although they had to continue to research to get the compute efficiency wins, and move down the curve",
    "start": "15144196",
    "end": "15151394"
  },
  {
    "text": "to get that 1,200x that has\nbeen achieved for GPT-3. Maybe we're only at a couple 100x now,",
    "start": "15151394",
    "end": "15158454"
  },
  {
    "text": "but with GPT-4 Turbo and 4o, and there'll be another one probably cheaper than GPT-4o even",
    "start": "15158454",
    "end": "15164218"
  },
  {
    "text": "that comes out at some point. - And that research costs a lot of money. - [Dylan] Yep. Exactly. - That's the thing that I guess",
    "start": "15164218",
    "end": "15170082"
  },
  {
    "text": "is not talked about with the cost, that when you're referring\nto the cost of the model,",
    "start": "15170082",
    "end": "15175394"
  },
  {
    "text": "it's not just the\ntraining or the test runs, it's the actual research,\nthe manpower that-",
    "start": "15175394",
    "end": "15182050"
  },
  {
    "text": "- Yeah, to do things\nlike reasoning right now that that exists, they're gonna scale it, they're gonna do a lot of research still. I think people focus on\nthe payback question,",
    "start": "15182050",
    "end": "15190243"
  },
  {
    "text": "but it's really easy\nto just be like, well, GDP is humans and industrial capital.",
    "start": "15190244",
    "end": "15195875"
  },
  {
    "text": "And if you can make intelligence\ncheap, you can grow a lot. That's the sort of dumb way to explain it.",
    "start": "15195875",
    "end": "15202132"
  },
  {
    "text": "But that's what basically\nthe investment thesis is. I think only Nvidia is\nactually making tons of money",
    "start": "15202132",
    "end": "15207973"
  },
  {
    "text": "and other hardware vendors. The hyperscalers are all\non paper making money, but in reality, they're\nspending a lot more",
    "start": "15207973",
    "end": "15214644"
  },
  {
    "text": "on purchasing the GPUs, which you don't know if they're still gonna make this much money\non each GPU in two years.",
    "start": "15214644",
    "end": "15220328"
  },
  {
    "text": "You don't know if all of a\nsudden, OpenAI goes kapoof,",
    "start": "15220328",
    "end": "15225328"
  },
  {
    "text": "and now Microsoft has\nhundreds of thousands of GPUs they were renting\nto OpenAI that are, that they paid for themselves\nwith their investment in them",
    "start": "15225414",
    "end": "15234133"
  },
  {
    "text": "that no longer have a customer. this is always a possibility.\nI don't believe that. I think OpenAI will keep raising money.",
    "start": "15234133",
    "end": "15240924"
  },
  {
    "text": "I think others will keep raising money, because the investments,\nthe returns from it are gonna be eventually\nhuge once we have AGI.",
    "start": "15240924",
    "end": "15248062"
  },
  {
    "text": "- So, do you think multiple\ncompanies will get... Let's assume that-\n- I don't think it's winner take all. - Okay. So, let's not\ncall it AGI, whatever.",
    "start": "15248062",
    "end": "15256809"
  },
  {
    "text": "It's like a single day. It's a gradual thing.\n- Powerful AI, super powerful AI. - But it's a gradually\nincreasing set of features",
    "start": "15256809",
    "end": "15263669"
  },
  {
    "text": "that are useful and make- - [Nathan] Rapidly increasing set of features.\n- Rapidly, rapidly increasing set of features.",
    "start": "15263669",
    "end": "15270611"
  },
  {
    "text": "So, you're saying a lot\nof companies will be... It just seems absurd that\nall of these companies",
    "start": "15270611",
    "end": "15277621"
  },
  {
    "text": "are building gigantic data centers. - There were companies\nthat will benefit from AI, but not because they train the best model.",
    "start": "15278785",
    "end": "15284955"
  },
  {
    "text": "Like Meta has so many avenues to benefit from AI and\nall of their services. People are there, people\nspend time on Meta's platforms",
    "start": "15284955",
    "end": "15291681"
  },
  {
    "text": "and it's a way to make more\nmoney per user per hour. - Yeah it seems like\nGoogle/xAI/Tesla, important to say,",
    "start": "15291681",
    "end": "15299209"
  },
  {
    "text": "and then Meta will benefit\nnot directly from the AI like the LLMs, but from the intelligence,",
    "start": "15301987",
    "end": "15308162"
  },
  {
    "text": "like the additional boost of intelligence to the products they already sell. So, whether that's the\nrecommendation system or for Elon who's been talking\nabout Optimus, the robot,",
    "start": "15310086",
    "end": "15319576"
  },
  {
    "text": "potentially the intelligence of the robot. And then, you have personalized\nrobots in the home,",
    "start": "15319576",
    "end": "15324751"
  },
  {
    "text": "that kind of thing. He thinks it's a 10 plus trillion\ndollar business, which...",
    "start": "15324751",
    "end": "15330844"
  },
  {
    "text": "(Lex laughing) - Yeah, sure.\n- At some point maybe. Not soon, but who knows\nwhat robotics will- - Let's do a TAM analysis.",
    "start": "15331860",
    "end": "15337540"
  },
  {
    "text": "8 billion humans and let's get\n(Nathan laughing) 8 billion robots. And let's pay 'em the average salary,",
    "start": "15337540",
    "end": "15343448"
  },
  {
    "text": "and yeah, there we go, 10\ntrillion, more than 10 trillion. - Yeah. If there's robots everywhere,\nwhy does it have to be",
    "start": "15343449",
    "end": "15350432"
  },
  {
    "text": "just 8 billion robots?\n- Yeah, yeah, of course, of course. - [Lex] It could be- - I'm gonna have one robot,\nyou're gonna have like 20.",
    "start": "15350432",
    "end": "15357262"
  },
  {
    "text": "- Yeah, I see a use case for that. So, yeah, I guess the benefit would be in the products they sell,",
    "start": "15357262",
    "end": "15363196"
  },
  {
    "text": "which is why OpenAI's in a\ntrickier position, 'cause they- - All of the value of OpenAI right now",
    "start": "15363197",
    "end": "15368436"
  },
  {
    "text": "as a brand is in ChatGPT. And there is actually\nnot that, for most users, there's not that much of a reason",
    "start": "15368436",
    "end": "15374685"
  },
  {
    "text": "that they need OpenAI\nto be spending billions and billions of dollars\non the next best model, - Mm-hmm.\n- when they could",
    "start": "15374685",
    "end": "15380134"
  },
  {
    "text": "just license Llama 5,\nand for be way cheaper. So, that's like ChatGPT is an extremely valuable\nentity to them, (chuckles)",
    "start": "15380134",
    "end": "15389337"
  },
  {
    "text": "but they could make\nmore money just off that than trying-\n- The chat application is clearly like, does not\nhave tons of room to continue.",
    "start": "15389337",
    "end": "15395325"
  },
  {
    "text": "Like the standard chat, where you're just using it for\na random question and stuff. The cost continues to collapse.",
    "start": "15395325",
    "end": "15400358"
  },
  {
    "text": "V3 is the latest-\n- It'll go down to ads. - Biggest, but it's gonna\nget supported by ads.",
    "start": "15400358",
    "end": "15406704"
  },
  {
    "text": "Meta already serves 405b and probably loses the\nmoney, but at some point, they're going to get, the\nmodels are gonna get so cheap",
    "start": "15406704",
    "end": "15413300"
  },
  {
    "text": "that they can just serve them\nfor free with ads supported. And that's what Google's\ngonna be able to do. And that's obviously\nthey've got a bigger reach.",
    "start": "15413300",
    "end": "15419446"
  },
  {
    "text": "So, chat is not gonna\nbe the only use case. It's like these reasoning,\ncode, agents, computer use,",
    "start": "15419446",
    "end": "15425812"
  },
  {
    "text": "all this stuff is where OpenAI has to actually go to\nmake money in the future. Otherwise, they're kaputs. - But X, Google, and Meta\nhave these other products.",
    "start": "15425812",
    "end": "15434978"
  },
  {
    "text": "So, isn't it likely that OpenAI and Anthropic disappear eventually?",
    "start": "15434978",
    "end": "15441529"
  },
  {
    "text": "Because it's-\n- Unless they're so good at models, 'cause they are. - [Lex] But it's such a cutting edge, I mean, you have to get-\n- It depends on where you think AI capabilities are going.",
    "start": "15442440",
    "end": "15448562"
  },
  {
    "text": "- You have to keep winning.\n- Yes. - You have to keep winning. As you climb, even if the AI capabilities\nare going super rapidly awesome",
    "start": "15448562",
    "end": "15456831"
  },
  {
    "text": "into the direction of\nAGI, there's still a boost for X in terms of data,\nGoogle in terms of data,",
    "start": "15456831",
    "end": "15464597"
  },
  {
    "text": "Meta in terms of data, in terms of other products and the money. There's just huge amount of money.\n- But if the whole idea",
    "start": "15464597",
    "end": "15470562"
  },
  {
    "text": "is human data is tapped\nout, we don't care, we all care about\nself-play verifiable tasks-",
    "start": "15470563",
    "end": "15475602"
  },
  {
    "text": "- Yes, the self-play,\n- Think about AWS- - [Lex] which is an R&D problem. - AWS does not make a lot of money on each individual machine.",
    "start": "15475602",
    "end": "15481856"
  },
  {
    "text": "And the same can be said for\nthe most powerful AI platform, which is even though the\ncalls to the API are so cheap,",
    "start": "15481857",
    "end": "15488308"
  },
  {
    "text": "there's still a lot of money to be made by owning that platform. And there's a lot of discussions as it's the next compute layer.",
    "start": "15488308",
    "end": "15495491"
  },
  {
    "text": "- You have to believe that... And there's a lot of\ndiscussions that tokens and tokenomics and LLM APIs\nare the next compute layer,",
    "start": "15495492",
    "end": "15502443"
  },
  {
    "text": "or the next paradigm for the economy, like energy and oil was. But there's also, you\nhave to believe that APIs",
    "start": "15502444",
    "end": "15509407"
  },
  {
    "text": "and chat are not where AI is stuck. It is actually just tasks and agents and robotics and computer use.",
    "start": "15509407",
    "end": "15516485"
  },
  {
    "text": "And those are the areas where all the value will be delivered, not API, not chat application.",
    "start": "15516486",
    "end": "15522487"
  },
  {
    "text": "- So, is it possible you have, I mean, it all just becomes a commodity, and you have the very thin\nwrapper like Perplexity.",
    "start": "15522487",
    "end": "15530664"
  },
  {
    "text": "Just joking. - [Nathan] There are a lot of wrappers making a lot of money. - Yeah, but do you think it's possible",
    "start": "15533187",
    "end": "15538842"
  },
  {
    "text": "that people would just\neven forget what OpenAI and Anthropic is, and just, 'cause there'll be\nwrappers around the API,",
    "start": "15538842",
    "end": "15545611"
  },
  {
    "text": "and it just dynamically- - If model progress is not rapid, yeah. It's becoming a commodity. DeepSeek-V3 shows this,",
    "start": "15545611",
    "end": "15551752"
  },
  {
    "text": "but also the GPT-3 chart\nearlier, chart showed this. Llama 3B is 1,200x cheaper than GPT-3.",
    "start": "15551752",
    "end": "15557911"
  },
  {
    "text": "Any GPT-3, like anyone\nwhose business model was GPT-3 level capabilities is dead. - Yeah.\n- Anyone whose business model",
    "start": "15557911",
    "end": "15563702"
  },
  {
    "text": "is GPT-4 level capabilities is dead. - It is a common saying that the best businesses\nbeing made now are ones",
    "start": "15563702",
    "end": "15569980"
  },
  {
    "text": "that are predicated on\nmodels getting better. - Right, which would be wrappers, thing that is riding\nthe wave of the models.",
    "start": "15569980",
    "end": "15577602"
  },
  {
    "text": "- The short-term that company\nthat could make the most money is the one that figures out what advertising targeting method works",
    "start": "15577602",
    "end": "15584189"
  },
  {
    "text": "for language model generations. We have the Meta ads which\nare hyper targeted in feed, not within specific pieces\n- Mm-hmm.",
    "start": "15584189",
    "end": "15590396"
  },
  {
    "text": "- of content. And we have search ads\nthat are used by Google and Amazon has been\nrising a lot on Search. But within a piece, within\na return from ChatGPT,",
    "start": "15590396",
    "end": "15597942"
  },
  {
    "text": "it is not clear how you get a high quality\nplaced ad within the output. And if you can do that with\nmodel costs coming down,",
    "start": "15597942",
    "end": "15605924"
  },
  {
    "text": "you can just get super high revenue per... That revenue is totally untapped",
    "start": "15605924",
    "end": "15610971"
  },
  {
    "text": "and it's not clear\ntechnically how it is done. - Yeah, that is... the ad sense innovation that Google did,",
    "start": "15610971",
    "end": "15617502"
  },
  {
    "text": "the one day you'll have\nin GPT output an ad and that's gonna make",
    "start": "15618651",
    "end": "15623998"
  },
  {
    "text": "billions, not-\n- And it could be very subtle. It could be in conversation. We have voice mode now. It could be some way of making it,",
    "start": "15623999",
    "end": "15630179"
  },
  {
    "text": "so the voice introduces certain things. It's much harder to measure and it takes imagination, but yeah.",
    "start": "15630179",
    "end": "15635902"
  },
  {
    "text": "- And it wouldn't come off shady, so you would receive public\nblow back, that kind of thing.",
    "start": "15635903",
    "end": "15642393"
  },
  {
    "text": "So, you have to do it loud enough to where it's clear it's an ad\nthat and balance all of that. So, that's the open question\nthey're trying to solve.",
    "start": "15642393",
    "end": "15648597"
  },
  {
    "text": "Anthropic and OpenAI, they need to- - They might not say that they-\n- I don't think they care about that at all.",
    "start": "15648597",
    "end": "15653873"
  },
  {
    "text": "- They don't care about it right now. I think it's places\n- I think they're surely- - like Perplexity are\nexperimenting on that more.",
    "start": "15653873",
    "end": "15658883"
  },
  {
    "text": "- [Lex] Oh, interesting. Yeah, for sure. - Like Perplexity, Google,\nMeta care about this.",
    "start": "15658883",
    "end": "15664080"
  },
  {
    "text": "I think OpenAI and Anthropic\nare purely laser-focused on- - AGI.\n- Yeah, agents and AGI.",
    "start": "15664080",
    "end": "15670669"
  },
  {
    "text": "And if I build AGI, I\ncan make tons of money. Or I can spend, pay for everything.",
    "start": "15670669",
    "end": "15676030"
  },
  {
    "text": "And it is just predicated back\non the export control thing. If you think AGI is 5,\n10 years away or less,",
    "start": "15676030",
    "end": "15683567"
  },
  {
    "text": "these labs think it's 2, 3 years away. Obviously, your actions are,",
    "start": "15683568",
    "end": "15688644"
  },
  {
    "text": "if you assume they're rational actors, which they are mostly, what you do in a two-year\nAGI versus five-year",
    "start": "15689880",
    "end": "15696361"
  },
  {
    "text": "versus 10 years is very,\nvery, very different. - Do you think agents are promising?",
    "start": "15696361",
    "end": "15702612"
  },
  {
    "start": "15699000",
    "end": "16221000"
  },
  {
    "text": "We have to talk about this. (chuckles) This is like the excitement of the year",
    "start": "15702612",
    "end": "15708366"
  },
  {
    "text": "that agents are gonna rev... This is the generic hype term that a lot of business folks are using.",
    "start": "15708366",
    "end": "15715132"
  },
  {
    "text": "AI agents are gonna\nrevolutionize everything. - Okay. So, mostly, the term\nagent is obviously overblown.",
    "start": "15715132",
    "end": "15721151"
  },
  {
    "text": "We've talked a lot about\nreinforcement learning as a way to train for verifiable outcomes. Agents should mean\nsomething that is open-ended",
    "start": "15721152",
    "end": "15728011"
  },
  {
    "text": "and is solving a task\nindependently on its own, and able to adapt to uncertainty. There's a lot of the term agent applied",
    "start": "15728011",
    "end": "15734541"
  },
  {
    "text": "to things like Apple Intelligence, which we still don't\nhave after the last WWDC,",
    "start": "15734541",
    "end": "15739983"
  },
  {
    "text": "which is orchestrating between apps. And that type of tool\nuse thing is something that language models can do really well.",
    "start": "15739984",
    "end": "15746559"
  },
  {
    "text": "Apple Intelligence I suspect will work, so will come eventually. It's a closed domain. It's your messages app\nintegrating with your photos,",
    "start": "15746559",
    "end": "15753070"
  },
  {
    "text": "with AI in the background. That will work. That has been described as an agent by a lot of software companies\nto get into the narrative.",
    "start": "15753070",
    "end": "15760640"
  },
  {
    "text": "- Yeah.\n- The question is what ways can we get language models\nto generalize to new domains",
    "start": "15760640",
    "end": "15768206"
  },
  {
    "text": "and solve their own problems in real time. Maybe some tiny amount of training when they're doing this\nwith fine-tuning themselves",
    "start": "15768206",
    "end": "15774870"
  },
  {
    "text": "or in context learning, which is the idea of storing\ninformation in a prompt. And you can use learning\nalgorithms to update that.",
    "start": "15774870",
    "end": "15781840"
  },
  {
    "text": "And whether or not you believe that that is gonna actually\ngeneralize to things like me saying, book my trip\nto go to Austin in two days.",
    "start": "15781840",
    "end": "15790956"
  },
  {
    "text": "I have X, Y, Z constraints,\nand actually trusting it. I think there's a HCI problem\ncoming back for information.",
    "start": "15792309",
    "end": "15799580"
  },
  {
    "text": "- Well, what's your prediction there? Because my gut says we're\nvery far away from that. - I think OpenAI's statement,",
    "start": "15799580",
    "end": "15807390"
  },
  {
    "text": "I don't know if you've\nseen the five levels. Or it's chat is level one, reasoning is level two, and\nthen agents is level three.",
    "start": "15807390",
    "end": "15814748"
  },
  {
    "text": "And I think there's a couple more levels, but it's important to\nnote, we were in chat for a couple years.\n- Mm-hmm.",
    "start": "15814748",
    "end": "15819858"
  },
  {
    "text": "- We just theoretically got to reasoning. We'll be here for a year or two. And then, agents, but at the same time,",
    "start": "15819859",
    "end": "15826686"
  },
  {
    "text": "people can train like\napproximate capabilities of the next level, but the agents are doing\nthings autonomously,",
    "start": "15826686",
    "end": "15833417"
  },
  {
    "text": "doing things for minutes at a time, hours at a time, et cetera. Reasoning is doing things for\ntens of seconds at a time.",
    "start": "15833417",
    "end": "15842025"
  },
  {
    "text": "And then, coming back with an output that I still need to verify\nand use and try check out. And the biggest problem is of course,",
    "start": "15842025",
    "end": "15849572"
  },
  {
    "text": "it's the same thing with manufacturing. There's the whole six sigma thing. How many nines do you get, and then you compound the\nnines onto each other,",
    "start": "15849572",
    "end": "15856172"
  },
  {
    "text": "and it's like if you multiply by the number of steps that are six sigma, you get to a yield or something.",
    "start": "15856172",
    "end": "15862741"
  },
  {
    "text": "So, like in semiconductor manufacturing, tens of thousands of steps. 9999999 is not enough,",
    "start": "15863939",
    "end": "15868800"
  },
  {
    "text": "because you multiply that many times, you actually end up with like 60% yield. - Or zero.\n- Really low yield. Yeah.",
    "start": "15870179",
    "end": "15875291"
  },
  {
    "text": "Or zero. And this is the same thing with agents. Chaining tasks together each time, LLMs,",
    "start": "15875291",
    "end": "15881470"
  },
  {
    "text": "even the best LLMs in particularly\npretty good benchmarks don't get 100%.\n- Yeah.",
    "start": "15881470",
    "end": "15887977"
  },
  {
    "text": "- They get a little bit below that, because there is a lot of noise. And so, how do you get to enough nines?",
    "start": "15887977",
    "end": "15894608"
  },
  {
    "text": "This is the same thing with self-driving. We can't have self-driving because without it being\nsuper geofenced like Google's.",
    "start": "15894609",
    "end": "15900814"
  },
  {
    "text": "And even then, they have\na bunch of tele operators to make sure it doesn't get stuck. But you can't do that because\nit doesn't have enough nines.",
    "start": "15900815",
    "end": "15907084"
  },
  {
    "text": "- And self-driving has\nquite a lot of structure because roads have rules.",
    "start": "15907084",
    "end": "15912747"
  },
  {
    "text": "It's well-defined. There's regulation. When you're talking about\ncomputer use for the open web,",
    "start": "15912747",
    "end": "15919358"
  },
  {
    "text": "for example, or the open\noperating system, it's a mess.",
    "start": "15919359",
    "end": "15923942"
  },
  {
    "text": "So, the possibility, I'm always skeptical of any system that is\ntasked with interacting",
    "start": "15925169",
    "end": "15932019"
  },
  {
    "text": "with the human world, the open message human world.\n- That's the thing. If we can't get intelligence, that's enough to solve the\nhuman world on its own.",
    "start": "15933088",
    "end": "15941146"
  },
  {
    "text": "We can create infrastructure like the human operators for Waymo - Yeah.\n- over many years",
    "start": "15941146",
    "end": "15946298"
  },
  {
    "text": "that enable certain workflows. - There is a company, I don't remember it, but that's literally their pitches. Yeah, we're just gonna\nbe the human operator",
    "start": "15946298",
    "end": "15952868"
  },
  {
    "text": "when agents fail, and you\njust call us and we fix it. - Yeah.\n- It's like an API call and it's hilarious. - There's gonna be teleoperation markets",
    "start": "15952868",
    "end": "15958879"
  },
  {
    "text": "when we get human robots, which is there's gonna be somebody around the world that's\nhappy to fix the fact",
    "start": "15958879",
    "end": "15964251"
  },
  {
    "text": "that it can't finish loading my dishwasher - Yeah.\n- when I'm unhappy with it, but that's just gonna be part\nof the Tesla service package.",
    "start": "15964251",
    "end": "15970373"
  },
  {
    "text": "- I'm just imagining an AI agent talking to another AI agent.",
    "start": "15970373",
    "end": "15975780"
  },
  {
    "text": "One company has an AI agent that specializes in\nhelping other AI agents.",
    "start": "15975780",
    "end": "15980976"
  },
  {
    "text": "- But if you can make things\nthat are good at one step, - Yeah.\n- you can stack them together. So, that's why I'm like,\nif it takes a long time,",
    "start": "15980976",
    "end": "15988109"
  },
  {
    "text": "we're gonna build\ninfrastructure that enables it. You see the operator launch, they have partnerships with\ncertain websites, with DoorDash,",
    "start": "15988110",
    "end": "15994240"
  },
  {
    "text": "with OpenTable,\n- Mm-hmm. - with things like this. Those partnerships are gonna\nlet them climb really fast.",
    "start": "15994240",
    "end": "15999929"
  },
  {
    "text": "Their model's gonna get\nreally good at those things. It's gonna proof of concept\nthat might be a network effect where more companies wanna\nmake it easier for AI.",
    "start": "15999929",
    "end": "16007537"
  },
  {
    "text": "Some companies will be like, no, let's at put blockers in place. - Yep.\n- And this is the story of the internet we've seen.",
    "start": "16007538",
    "end": "16013751"
  },
  {
    "text": "We see it now with training\ndata for language models, where companies are like,\nno, you have to pay, - [Lex] Mm-hmm.\n(Nathan chuckles)",
    "start": "16013751",
    "end": "16019437"
  },
  {
    "text": "- business working it out. - That said, I think airlines have a very, and hotels have high incentive",
    "start": "16019437",
    "end": "16025406"
  },
  {
    "text": "to make their site work really\nwell, and they usually don't. If you look at how many clicks it takes",
    "start": "16025406",
    "end": "16031548"
  },
  {
    "text": "to order a airplane ticket, it's insane. I don't-\n- You actually can't call an American\nAirlines agent anymore.",
    "start": "16031548",
    "end": "16037985"
  },
  {
    "text": "They don't have a phone number. - It's horrible on many, on\nthe interface front and all...",
    "start": "16037986",
    "end": "16044155"
  },
  {
    "text": "To imagine that agents will be able to deal with that website,\nwhen I, as a human, struggle,",
    "start": "16044155",
    "end": "16049968"
  },
  {
    "text": "like I have an existential crisis every time I try to\nbook an airplane ticket, that I think it's gonna be\nvery extremely difficult",
    "start": "16049968",
    "end": "16057746"
  },
  {
    "text": "to build a AI agent that's robust in that way.\n- But think about it, United has accepted the Starlink term,",
    "start": "16057746",
    "end": "16063386"
  },
  {
    "text": "which is they have to\nprovide Starlink for free and the users are going to love it. What if one Airline is like,\nwe're gonna take a year",
    "start": "16063386",
    "end": "16070195"
  },
  {
    "text": "and we're gonna make our website have white text that works\nperfectly for the AIs. Every time anyone asks\nabout an AI flight, they buy",
    "start": "16070195",
    "end": "16078003"
  },
  {
    "text": "whatever airline it is.\n(Dylan laughing) - Or they're just like, here's an API in, it's only exposed to AI agents",
    "start": "16078003",
    "end": "16083590"
  },
  {
    "text": "and if anyone queries it,\nthe price is 10% higher. - [Lex] Yeah. - And for any flight, but we'll let you see any of our flights",
    "start": "16083590",
    "end": "16089270"
  },
  {
    "text": "and you can just book any of them. Here you go, agent-\n- And that's- - Then, it's, oh, and I\nmade 10% higher price. Awesome.\n- Yeah.",
    "start": "16089270",
    "end": "16094559"
  },
  {
    "text": "- And am I willing to say that for like, hey, book me a flight to see Lex. And it's like, yeah, whatever. - Yeah, yeah.",
    "start": "16094559",
    "end": "16099631"
  },
  {
    "text": "- I think computers and real world and the open world are\nreally, really messy.",
    "start": "16099631",
    "end": "16106427"
  },
  {
    "text": "But if you start defining the\nproblem in narrow regions, people are gonna be able to create very, very productive things,",
    "start": "16106427",
    "end": "16114318"
  },
  {
    "text": "and ratchet down cost massively. Now, crazy things like\nrobotics in the home,",
    "start": "16114319",
    "end": "16121486"
  },
  {
    "text": "those are gonna be a lot harder to do just like self-driving. Because there's just a billion\ndifferent failure modes.",
    "start": "16121486",
    "end": "16127942"
  },
  {
    "text": "But agents that can navigate\na certain set of websites and do certain sets of tasks,",
    "start": "16127942",
    "end": "16133777"
  },
  {
    "text": "or take a photo of your fridge, or like upload your recipes,",
    "start": "16133777",
    "end": "16139918"
  },
  {
    "text": "and then it figures out what to order from Amazon/Whole Foods food delivery. And that's gonna be pretty\nquick and easy to do, I think.",
    "start": "16139918",
    "end": "16147634"
  },
  {
    "text": "So, it's gonna be be a whole\nrange of business outcomes and it's gonna be tons of optimism",
    "start": "16147634",
    "end": "16152750"
  },
  {
    "text": "around people can just figure\nout ways to make money. - To be clear, these sandboxes\nalready exist in research. There are people who have built clones",
    "start": "16152750",
    "end": "16158927"
  },
  {
    "text": "of all the most popular\nwebsites of Google, Amazon, blah, blah, blah, to make\nit so that there's...",
    "start": "16158927",
    "end": "16164701"
  },
  {
    "text": "And I mean, OpenAI probably\nhas them internally to train these things. It's the same as DeepMind's robotics team for years has had clusters for robotics,",
    "start": "16164701",
    "end": "16171887"
  },
  {
    "text": "where you interact with\nrobots fully, remotely. They just have a lab in London\nand you send tasks to it,",
    "start": "16171887",
    "end": "16177357"
  },
  {
    "text": "it arrange the blocks\nand you do this research. Obviously, there's texts\nthere that fix stuff. But we've turned these\ncranks of automation before.",
    "start": "16177358",
    "end": "16186663"
  },
  {
    "text": "You go from sandbox to progress, and then you add one\nmore domain at a time, and generalize, I think.",
    "start": "16186663",
    "end": "16192273"
  },
  {
    "text": "In the history of NLP\nand language processing, instruction tuning and\ntasks per language model",
    "start": "16192274",
    "end": "16197844"
  },
  {
    "text": "used to be one language\nmodel did one task. And then, in the instruction\ntuning literature, there's this point where\nyou start adding more",
    "start": "16197844",
    "end": "16203704"
  },
  {
    "text": "and more tasks together, where it just starts to\ngeneralize to every task. And we don't know where\non this curve we are.",
    "start": "16203704",
    "end": "16208946"
  },
  {
    "text": "I think for reasoning with this RL and verifiable domains, we're early, but we don't know where the point is,",
    "start": "16208946",
    "end": "16214142"
  },
  {
    "text": "where you just start training\non enough domains and poof, more domains just start working,",
    "start": "16214142",
    "end": "16219401"
  },
  {
    "text": "and you've crossed the\ngeneralization barrier. - Well, what do you think\nabout the programming context?",
    "start": "16219401",
    "end": "16225773"
  },
  {
    "start": "16221000",
    "end": "16669000"
  },
  {
    "text": "So, software engineering. That's where I personally, and I know a lot of people\ninteract with AI the most.",
    "start": "16225774",
    "end": "16234789"
  },
  {
    "text": "- There's a lot of fear and angst too from current CS students, but there's also, that is the area where probably the most AI revenue",
    "start": "16234789",
    "end": "16241465"
  },
  {
    "text": "and productivity gains have come. - [Lex] Yeah. - Whether it be Copilots or\nCursor, or what have you,",
    "start": "16241465",
    "end": "16248459"
  },
  {
    "text": "or just standard ChatGPT. Like a lot of... I know very few programmers\nwho don't have ChatGPT",
    "start": "16248459",
    "end": "16254332"
  },
  {
    "text": "and actually many of\nthem have the $200 tier because that's what it's so good for. I think that in that world,",
    "start": "16254332",
    "end": "16261899"
  },
  {
    "text": "we already see it like SWE-bench, I don't know if you've\nlooked at the benchmark made by some Stanford students.",
    "start": "16261899",
    "end": "16267445"
  },
  {
    "text": "I wouldn't say it's really hard, but I wouldn't say it's easy either. I think it takes someone who's been through at\nleast a few years of CS,",
    "start": "16267445",
    "end": "16274044"
  },
  {
    "text": "or a couple years of programming\nto do SWE-bench well. And the models went from\n4% to 60% in like a year.",
    "start": "16274044",
    "end": "16281793"
  },
  {
    "text": "And where are they gonna go to next year? It's gonna be higher. It probably won't be 100%, 'cause again, that nines\nis really hard to do,",
    "start": "16281873",
    "end": "16288559"
  },
  {
    "text": "but we're gonna get to\nsome point, where that's, and then we're gonna need harder software engineering benchmarks, and so on and so forth.",
    "start": "16288559",
    "end": "16293745"
  },
  {
    "text": "But the way that people think of it now is it's can do code completion easy.",
    "start": "16293745",
    "end": "16298956"
  },
  {
    "text": "It can do some function generation\nand I have to review it. Great. But really, the software\nengineering agents,",
    "start": "16298957",
    "end": "16304996"
  },
  {
    "text": "I think can be done faster\nsooner than any other agent, because it is a verifiable domain. You can always unit test or compile,",
    "start": "16304996",
    "end": "16312822"
  },
  {
    "text": "and there's many different regions of it can inspect the\nwhole code base at once,",
    "start": "16312823",
    "end": "16318742"
  },
  {
    "text": "which no engineer really can. Only the architects can really think about this stuff,\n- Mm-hmm. - the really senior guys,\nand they can define stuff,",
    "start": "16318742",
    "end": "16325057"
  },
  {
    "text": "and then the agent can execute on it. So, I think software engineering costs are gonna plummet like crazy. And one interesting aspect of that",
    "start": "16325057",
    "end": "16332254"
  },
  {
    "text": "is when software engineering\ncosts are really low, you get very different markets. So, in the US, you have all\nthese platform SaaS companies,",
    "start": "16332254",
    "end": "16338916"
  },
  {
    "text": "Salesforce, and so on and so forth. In China, no one uses platform SaaS.",
    "start": "16338916",
    "end": "16344726"
  },
  {
    "text": "Everyone just builds their own stack, because software engineering\nis much cheaper in China,",
    "start": "16345717",
    "end": "16350822"
  },
  {
    "text": "and partially because people, number of STEM graduates, et cetera. So, STEM. So, it's generally\njust cheaper to do.",
    "start": "16350822",
    "end": "16358312"
  },
  {
    "text": "And so, at the same time, code LLMs have been\nadopted much less in China, because the cost of an\nengineer there is much lower.",
    "start": "16358313",
    "end": "16365299"
  },
  {
    "text": "But what happens when every\ncompany can just invent their own business logic\nreally cheaply and quickly? You stop using platform SaaS,",
    "start": "16365299",
    "end": "16371685"
  },
  {
    "text": "you start building\ncustom-tailored solutions, you change them really quickly. Now, all of a sudden, your business is a little bit\nmore efficient too potentially",
    "start": "16371685",
    "end": "16377571"
  },
  {
    "text": "because you're not dealing with the hell that is some random\nplatform SaaS company stuff not working perfectly and\nhaving to adjust workflows,",
    "start": "16377571",
    "end": "16384059"
  },
  {
    "text": "or random business automation cases that aren't necessarily AI-required. It's just logic that needs to be built\nthat no one has built.",
    "start": "16384059",
    "end": "16390248"
  },
  {
    "text": "All of these zings can go happen faster. And so, I think software... And then, the other\ndomain is like industrial, chemical, mechanical engineers",
    "start": "16390248",
    "end": "16396545"
  },
  {
    "text": "suck at coding just generally. And their tools, like\nsemiconductor engineers, their tools are 20 years old.",
    "start": "16396545",
    "end": "16402326"
  },
  {
    "text": "All the tools run on XP, including ASML lithography\ntools, run on Windows XP.",
    "start": "16402326",
    "end": "16407477"
  },
  {
    "text": "And a lot of the analysis\nhappens in Excel. It's just like, guys,",
    "start": "16407479",
    "end": "16412754"
  },
  {
    "text": "you guys can move 20 years forward with all the data you have and gathered and do a lot better. It's just you need the engineering skills",
    "start": "16412756",
    "end": "16418703"
  },
  {
    "text": "for software engineering to be delivered to the actual\ndomain expert engineer. So, I think that's the area,",
    "start": "16418703",
    "end": "16423721"
  },
  {
    "text": "where I'm super, duper bullish of generally AI creating value. - The big picture is that I don't think\nit's gonna be a cliff.",
    "start": "16423721",
    "end": "16430742"
  },
  {
    "text": "- Yeah.\n- We talked to, I think a really good example of how growth changes is\nwhen Meta added stories.",
    "start": "16430742",
    "end": "16437904"
  },
  {
    "text": "So, Snapchat was on an exponential, they added stories, it flatlined. Software engineers,\nthen up until the right,",
    "start": "16438762",
    "end": "16445021"
  },
  {
    "text": "AI's gonna come in, it's\nprobably just gonna be flat. It is not like everyone's\ngonna lose their job. It's hard because the\nsupply corrects more slowly.",
    "start": "16445021",
    "end": "16453754"
  },
  {
    "text": "So, the amount of\nstudents is still growing and that'll correct on a\nmulti-year, like a year delay,",
    "start": "16453754",
    "end": "16459273"
  },
  {
    "text": "but the amount of jobs will just turn, and then maybe in 20, 40\nyears, it'll be well down.",
    "start": "16459273",
    "end": "16466189"
  },
  {
    "text": "But in the few years, there'll never gonna be the snap moment, where it's like software\nengineers aren't useful. - I think also the nature",
    "start": "16466189",
    "end": "16471279"
  },
  {
    "text": "of what it means to be a programmer and what kind of jobs\nprogrammers do changes, 'cause I think there needs",
    "start": "16471279",
    "end": "16478514"
  },
  {
    "text": "to be a human in the loop of\neverything you've talked about. There's a really important human in that picture of correcting the code,",
    "start": "16478514",
    "end": "16486455"
  },
  {
    "text": "like fix-\n- Think more than the context length.\n- Yep. And debugging also,",
    "start": "16487674",
    "end": "16493604"
  },
  {
    "text": "like debugging by sort\nof reading the code, understanding the steering\nthe system, like no, no, no.",
    "start": "16493604",
    "end": "16499783"
  },
  {
    "text": "You missed the point.\nAdding more to the prompt. Kind of like, yes, adding the human-",
    "start": "16499783",
    "end": "16505393"
  },
  {
    "text": "- Designing the perfect Google button. Google's famous for having\npeople design buttons that are so perfect, and it's\nlike how is AI gonna do that?",
    "start": "16505393",
    "end": "16513445"
  },
  {
    "text": "Is like they could give you all the ideas. Perfect. Fine.\n- Yeah. - That's the thing, you can call it taste.",
    "start": "16513445",
    "end": "16520406"
  },
  {
    "text": "One thing humans can do is figure out what other humans enjoy\nbetter than AI systems. That's where the preference,\nyou loading that in.",
    "start": "16521338",
    "end": "16528350"
  },
  {
    "text": "But ultimately, humans are the greatest\npreference generat... That's where the preference comes from. - And humans are actually\nvery good at reading",
    "start": "16528350",
    "end": "16535006"
  },
  {
    "text": "or judging between two things versus, this goes back to the core of what RLHF and preference tuning is,",
    "start": "16535006",
    "end": "16540453"
  },
  {
    "text": "is that it's hard to generate a good answer\nfor a lot of problems, but it's easy to see which one is better. And that's how we're\nusing humans for AI now",
    "start": "16540453",
    "end": "16547174"
  },
  {
    "text": "is judging which one is better. And that's what software\nengineering could look like, is the PR review, here's a\nfew options, what are the,",
    "start": "16547176",
    "end": "16554703"
  },
  {
    "text": "like here are some\npotential pros and cons, and they're gonna be judges. - I think the thing I\nwould very much recommend",
    "start": "16554703",
    "end": "16562064"
  },
  {
    "text": "is people start,\nprogrammers start using AI, and embracing that role of the\nsupervisor of the AI system,",
    "start": "16562064",
    "end": "16569557"
  },
  {
    "text": "and partner the AI system\nversus, writing from scratch or not learning coding at all\nand just generating stuff.",
    "start": "16569557",
    "end": "16576877"
  },
  {
    "text": "Because I think there actually\nhas to be a pretty high level of expertise as a programmer to be able to manage\nincreasingly intelligent systems.",
    "start": "16576877",
    "end": "16583949"
  },
  {
    "text": "- I think it's that, and then becoming a domain\nexpert in something. - Sure. Yeah.\n- Right? Because seriously, if\nyou go look at aerospace",
    "start": "16583949",
    "end": "16590463"
  },
  {
    "text": "or semiconductors or chemical engineering, everyone is using really crappy platforms, really old software.",
    "start": "16590463",
    "end": "16596664"
  },
  {
    "text": "The job of a data sciences\nis like a joke in many cases. In many cases, it's very real,",
    "start": "16596664",
    "end": "16602461"
  },
  {
    "text": "but it's like bring what the forefront of human capabilities are to your domain. And even if the forefront is from the AI,",
    "start": "16602463",
    "end": "16608779"
  },
  {
    "text": "your domain, you're like at the forefront. So, it's like you have to be\nat the forefront of something, and then leverage the like rising tide",
    "start": "16608779",
    "end": "16616227"
  },
  {
    "text": "that is AI for everything else. - Oh, yeah. There's so many low-hanging\nfruit everywhere",
    "start": "16616227",
    "end": "16622018"
  },
  {
    "text": "in terms of where software\ncan help automate a thing or digitize a thing.",
    "start": "16622020",
    "end": "16627115"
  },
  {
    "text": "In the legal system... That's why Doge is exciting. I got to hang out",
    "start": "16627996",
    "end": "16633840"
  },
  {
    "text": "with a bunch of the\nDoge folks, and they... (Lex chuckles) I mean, government is so old school,",
    "start": "16635031",
    "end": "16640637"
  },
  {
    "text": "it's like begging for the\nmodernization of software,",
    "start": "16640637",
    "end": "16645637"
  },
  {
    "text": "of organizing the data,\nall this kind of stuff. In that case, it's by design, because bureaucracy protects\ncenters of power, and so on.",
    "start": "16645654",
    "end": "16654836"
  },
  {
    "text": "But software breaks down those barriers, so it hurts those that\nare holding onto power,",
    "start": "16655912",
    "end": "16663232"
  },
  {
    "text": "but ultimately, benefits humanity. So, there's a bunch of\ndomains of that kind.",
    "start": "16663232",
    "end": "16669676"
  },
  {
    "start": "16669000",
    "end": "17221000"
  },
  {
    "text": "One thing we didn't fully finish talking about is open source.",
    "start": "16669676",
    "end": "16675182"
  },
  {
    "text": "So, first of all, congrats,\nyou released a new model. - Yeah. This is the-\n- Tulu. (chuckles)",
    "start": "16675182",
    "end": "16680521"
  },
  {
    "text": "- I'll explain what a Tulu is. - Yeah.\n- A tulu is a hybrid camel when you breed a Dromedary\nwith a Bactrian camel.",
    "start": "16680523",
    "end": "16686730"
  },
  {
    "text": "Back in the early days, after\nChatGPT, there was a big wave of models coming out like\nAlpaca, Vicuna, et cetera,",
    "start": "16686730",
    "end": "16692336"
  },
  {
    "text": "that were all named after\nvarious mammalian species. So, Tulu is the brand\nis multiple years old,",
    "start": "16692336",
    "end": "16698482"
  },
  {
    "text": "which comes from that.\n- Mm-hmm. - And we've been playing at the frontiers of post-training with open source code.",
    "start": "16698482",
    "end": "16705342"
  },
  {
    "text": "And this first part of this\nrelease was in the fall, where we've built on Llama's open models,",
    "start": "16705342",
    "end": "16713043"
  },
  {
    "text": "open-weight models, and then we add in our fully\nopen code, our fully open data. There's a popular benchmark\nthat is chat bot arena.",
    "start": "16713043",
    "end": "16720414"
  },
  {
    "text": "And that's generally the metric by which how these chat\nmodels are evaluated. And it's humans compare random models",
    "start": "16720414",
    "end": "16727281"
  },
  {
    "text": "from different organizations. And if you looked at the leaderboard in November or December, among the top 60 models from\n10s to 20s of organizations,",
    "start": "16727281",
    "end": "16735568"
  },
  {
    "text": "none of them had open code or\ndata for just post-training. Among that, even fewer or none have pre-training\ndata and code available.",
    "start": "16735568",
    "end": "16742162"
  },
  {
    "text": "But it's like post-training is much more accessible at this time. It's still pretty cheap and you can do it. And the thing is how high\ncan we push this number,",
    "start": "16742162",
    "end": "16748521"
  },
  {
    "text": "where people have access\nto all the code and data. So, that's the motivation of the project. We draw in lessons from Llama.",
    "start": "16748521",
    "end": "16754570"
  },
  {
    "text": "Nvidia had a Nemotron model, where the recipe for their post-training was fairly open with\nsome data and a paper.",
    "start": "16754570",
    "end": "16761740"
  },
  {
    "text": "And it's putting all these together to try to create a recipe that people can fine-tune models like\nGPT-4 to their domain.",
    "start": "16761740",
    "end": "16767898"
  },
  {
    "text": "- So, to be clear, in the case of Tulu, maybe you can talk about OLMO 2, but in the case of Tulu,\nyou're taking Llama 3, 4, 5B.",
    "start": "16767898",
    "end": "16776672"
  },
  {
    "text": "- Tulu has been a series of\nrecipes for post-training. So, we've done\n- Okay. - multiple models over years.\n- Okay. And so, you're open sourcing everything.",
    "start": "16779178",
    "end": "16786660"
  },
  {
    "text": "- Yeah, if you start with\nan open-weight-based model, the whole model technically\nisn't open source,",
    "start": "16786660",
    "end": "16791700"
  },
  {
    "text": "because you don't know\nwhat Llama put into it, which is why we have a separate\nthing that we'll get to. But it's just getting\nparts of the pipeline,",
    "start": "16791700",
    "end": "16798288"
  },
  {
    "text": "where people can zoom in and customize. I know I hear from\nstartups and businesses, they're like, okay, I can\ntake this post-training",
    "start": "16798288",
    "end": "16803950"
  },
  {
    "text": "and try to apply it to my domain. We talk about verifiers a lot. We use this idea which\nis reinforcement learning",
    "start": "16803950",
    "end": "16809828"
  },
  {
    "text": "with verifiable rewards,\nRLVR, similar to RLHF.",
    "start": "16809828",
    "end": "16813746"
  },
  {
    "text": "And we applied it to map. And the model today,\nwhich is we applied it",
    "start": "16815496",
    "end": "16820682"
  },
  {
    "text": "to the Llama 405b base\nmodel from last year. And we have our other stuff,",
    "start": "16820682",
    "end": "16825690"
  },
  {
    "text": "we have our instruction tuning\nand our preference tuning. But the math thing is interesting,",
    "start": "16825692",
    "end": "16831500"
  },
  {
    "text": "which is it's easier to\nimprove this math benchmark. There's a benchmark,\nM-A-T-H, MATH, all capitals,",
    "start": "16831502",
    "end": "16837050"
  },
  {
    "text": "tough name when the benchmark, its name is the area\nthat you're evaluating. We're researchers, we're not\nbrands, brand strategists.",
    "start": "16837050",
    "end": "16844600"
  },
  {
    "text": "And this is something that the DeepSeek paper\ntalked about as well is at this bigger model,",
    "start": "16844600",
    "end": "16849614"
  },
  {
    "text": "it's easier to elicit\npowerful capabilities with this RL training. And then, they distill it down",
    "start": "16849614",
    "end": "16854670"
  },
  {
    "text": "from that big model to the small model. And this model we released\ntoday, we saw the same thing, is we're AI2, we don't\nhave a ton of compute,",
    "start": "16854670",
    "end": "16862218"
  },
  {
    "text": "we can't train 405b models all the time. So, we just did a few runs\nand they tend to work, and it's like, it just shows\nthat there's a lot of room",
    "start": "16862218",
    "end": "16869930"
  },
  {
    "text": "for people to play in these things. And that's-\n- And they crushed Llama's actual release.",
    "start": "16869930",
    "end": "16875584"
  },
  {
    "text": "They're way better than it.\n- Yeah. So, our eval numbers, we\nhave extra months in this, but our eval numbers are much better",
    "start": "16875584",
    "end": "16881076"
  },
  {
    "text": "than the Llama instruct model\n- Mm-hmm. - that they released. - And then, you also said\nbetter than DeepSeek-V3.",
    "start": "16881078",
    "end": "16886386"
  },
  {
    "text": "- Yeah. On our eval benchmark. The most DeepSeek-V3 is really similar. We have a safety benchmark to understand",
    "start": "16886386",
    "end": "16893162"
  },
  {
    "text": "if it will say harmful\nthings and things like that. And that's what draws\ndown most of the way. It's still like- - It's like an amalgamation",
    "start": "16893162",
    "end": "16898718"
  },
  {
    "text": "of multiple benchmarks\nor what do you mean? - Yeah, so we have a 10 evaluat... This is standard practice in post-training is you choose your\nevaluations you care about.",
    "start": "16898720",
    "end": "16905314"
  },
  {
    "text": "In academics, in smaller labs, you'll have fewer evaluations. In companies, you'll have a really one domain that you really care about.",
    "start": "16905314",
    "end": "16911624"
  },
  {
    "text": "In Frontier Labs, you'll have 10s to 20s, to maybe even 100 valuations\nof specific things.",
    "start": "16911624",
    "end": "16916664"
  },
  {
    "text": "So, we'd choose a\nrepresentative suite of things that look like chat, precise\ninstruction following, which is like respond only in emojis,",
    "start": "16916664",
    "end": "16923664"
  },
  {
    "text": "just model follow weird things like that. - Yeah.\n- Math, code. And you create a suite like this. So, safety would be 1 of\n10 in that type of suite,",
    "start": "16923664",
    "end": "16931618"
  },
  {
    "text": "where you have like, what is the broader\ncommunity of AI care about? And for example, in\ncomparison to DeepSeek,",
    "start": "16931618",
    "end": "16937232"
  },
  {
    "text": "it would be something like our average eval for\nour model would be 80, including safety and similar without.",
    "start": "16937232",
    "end": "16943338"
  },
  {
    "text": "And DeepSeek would be like 79%\naverage score without safety",
    "start": "16943340",
    "end": "16948340"
  },
  {
    "text": "and their safety score would bring it down to like 76-\n- Oh, so you beat them even ignoring safety?\n- Yeah. So, this is something that internally,",
    "start": "16949978",
    "end": "16955472"
  },
  {
    "text": "it's like I don't want to win only by how you shape the eval benchmark. So, if there's something\nthat's like people may",
    "start": "16955472",
    "end": "16961248"
  },
  {
    "text": "or may not care about\nsafety in their model, safety can come downstream, safety can be when you\nhost the model for an API,",
    "start": "16961248",
    "end": "16966498"
  },
  {
    "text": "safety is addressed in a spectrum of locations\nin AI applications. So, it's like, if you wanna say that\nyou have the best recipe,",
    "start": "16966498",
    "end": "16972720"
  },
  {
    "text": "you can't just gauge it on these things that some people might not want. - [Lex] Mm-hmm. - And this is just, it's\nlike the time of progress.",
    "start": "16972720",
    "end": "16980638"
  },
  {
    "text": "We benefit if we can release model later, we have more time to learn new techniques. Like this RL technique, we\nhad started this in the fall.",
    "start": "16980638",
    "end": "16987628"
  },
  {
    "text": "It's now really popular reasoning models. The next thing to do for\nopen source post-training is to scale up verifiers,",
    "start": "16987628",
    "end": "16993900"
  },
  {
    "text": "to scale up data to replicate\nsome of DeepSeek's results. And it's awesome that\nwe have a paper to draw and that it makes it a lot easier.",
    "start": "16993900",
    "end": "17000604"
  },
  {
    "text": "And that's the type of things that is going on among academic",
    "start": "17000604",
    "end": "17004800"
  },
  {
    "text": "and closed frontier research in AI. - Since you're pushing open source, what do you think is the future of it?",
    "start": "17005732",
    "end": "17011308"
  },
  {
    "text": "Do you think DeepSeek\nactually changes things since it's open source or open-weight or is pushing the open source movement",
    "start": "17011310",
    "end": "17017708"
  },
  {
    "text": "into the open direction? - This goes very back to\nthe license discussion. So, DeepSeek-R1 with a friendly\nlicense is a major reset.",
    "start": "17017710",
    "end": "17024130"
  },
  {
    "text": "So, it's like the first time that we've had a really\nclear frontier model that is open-weights and with a commercially friendly license",
    "start": "17024130",
    "end": "17030808"
  },
  {
    "text": "with no restrictions on\ndownstream use cases, synthetic data, distillation, whatever. This has never been the case at all",
    "start": "17030808",
    "end": "17036690"
  },
  {
    "text": "in the history of AI in the\nlast few years since ChatGPT. There have been models\nthat are off the frontier or models with weird licenses",
    "start": "17036690",
    "end": "17043002"
  },
  {
    "text": "that you can't really use them. - So, isn't Meta's license pretty much permissible\nexcept for five companies?",
    "start": "17043002",
    "end": "17049550"
  },
  {
    "text": "And there's also... So, this goes to what open\nsource AI is, which is, there's also use case\nrestrictions in the Llama license,",
    "start": "17049550",
    "end": "17055970"
  },
  {
    "text": "which says you can't use\nit for specific things. So, if you come from an open\nsource software background, you would say that that is\nnot an open source license.",
    "start": "17055970",
    "end": "17062300"
  },
  {
    "text": "- What kind of things are those though? Are they like... - At this point, I can't pull\nthem off the top of my head.",
    "start": "17062300",
    "end": "17067580"
  },
  {
    "text": "But it'll be like-\n- Like stuff like competitor, probably.\n- It used to be military use was one,\n- Oh- - and they removed that for scale. It'll be like CSAM, like\nchild abuse material",
    "start": "17067582",
    "end": "17076716"
  },
  {
    "text": "or that's the type of thing\nthat is forbidden there. But that's enough from\nan open source background",
    "start": "17076716",
    "end": "17082218"
  },
  {
    "text": "to say it's not open source license. And also, the Llama license\nhas this horrible thing, where you have to name your\nmodel Llama if you touch it",
    "start": "17082218",
    "end": "17089308"
  },
  {
    "text": "to the Llama model.\n- Mm-hmm. - So, it's like the branding thing. So, if a company uses Llama, technically,",
    "start": "17089308",
    "end": "17094356"
  },
  {
    "text": "the license says that they\nshould say built with Llama at the bottom of their application. And from a marketing\nperspective, that just hurts.",
    "start": "17094356",
    "end": "17100494"
  },
  {
    "text": "I can suck it up as a researcher,\nI'm like, oh, it's fine. It says Llama dash on all of\nour materials for this release.",
    "start": "17100494",
    "end": "17107044"
  },
  {
    "text": "But this is why we need truly open models, which is we don't know\nDeepSeek-R1's data, but-",
    "start": "17107046",
    "end": "17112082"
  },
  {
    "text": "- Wait, so you're saying I\ncan't make a cheap copy of Llama and pretend it's mine, but I can do this with the Chinese model?",
    "start": "17112082",
    "end": "17117342"
  },
  {
    "text": "- [Lex] Yeah. - Yeah.\n- Hell yeah. (Nathan and Dylan laughing) - That's what I'm saying.\n- Yeah. - And that's why it's like we want to,",
    "start": "17117342",
    "end": "17124346"
  },
  {
    "text": "this whole open language\nmodels thing, the OLMO thing, is to try to keep the model\nwhere everything is open",
    "start": "17124346",
    "end": "17129764"
  },
  {
    "text": "with the data as close to\nthe frontier as possible. So, we're compute-constrained,\nwe're personnel-constrained. We rely on getting insights from people",
    "start": "17129766",
    "end": "17137166"
  },
  {
    "text": "like John Schulman tells\nus to do RL on outputs. We can make these big jumps, but it just takes a long time",
    "start": "17137166",
    "end": "17143108"
  },
  {
    "text": "to push the frontier of open source. And fundamentally, I would say that that's because open source AI",
    "start": "17143108",
    "end": "17148996"
  },
  {
    "text": "does not have the same feedback loops as open source software. We talked about open source\nsoftware for security",
    "start": "17148996",
    "end": "17154078"
  },
  {
    "text": "also is just because\nyou build something once and you can reuse it. If you go into a new company,\nthere's so many benefits.",
    "start": "17154078",
    "end": "17160052"
  },
  {
    "text": "But if you open source a language model, you have this data sitting around, you have this training\ncode, it's not that easy",
    "start": "17160052",
    "end": "17167328"
  },
  {
    "text": "for someone to come and\nbuild on and improve, 'cause you need to spend a lot on compute. You need to have expertise. So, until there are feedback\nloops of open source AI,",
    "start": "17167328",
    "end": "17175100"
  },
  {
    "text": "it seems mostly an ideological mission. Like people like Mark\nZuckerberg, which is like, America needs this, and I agree with him,",
    "start": "17175100",
    "end": "17182458"
  },
  {
    "text": "but in the time where the\nmotivation ideologically is high, we need to capitalize\nand build this ecosystem",
    "start": "17182460",
    "end": "17189080"
  },
  {
    "text": "around what benefits do you get from seeing\nthe language model data? And there's not a lot about that.",
    "start": "17189080",
    "end": "17195196"
  },
  {
    "text": "We're gonna try to launch a demo soon where you can look at a\nOLMO model and a query, and see what pre-training\ndata is similar to it,",
    "start": "17195196",
    "end": "17201740"
  },
  {
    "text": "which is legally risky and complicated, but it's like, what does it mean to see the data that\nthe AI was trained on?",
    "start": "17201740",
    "end": "17208860"
  },
  {
    "text": "It's hard to parse.\nIt's terabytes of files. It's like, I don't know what\nI'm gonna find in there.",
    "start": "17208862",
    "end": "17214224"
  },
  {
    "text": "But that's what we need\nto do as an ecosystem if people want open source\nAI to be financially useful.",
    "start": "17214224",
    "end": "17221266"
  },
  {
    "start": "17221000",
    "end": "17670000"
  },
  {
    "text": "- We didn't really talk about Stargate. I would love to get your opinion on what the new administration,\nthe Trump administration,",
    "start": "17221266",
    "end": "17227358"
  },
  {
    "text": "everything that's doing that's being done from the America side and supporting AI infrastructure",
    "start": "17227358",
    "end": "17234278"
  },
  {
    "text": "and the efforts of the\ndifferent AI companies. What do you think about Stargate? What are we supposed to\nthink about Stargate?",
    "start": "17234278",
    "end": "17240166"
  },
  {
    "text": "And does Sam have the money? (Nathan chuckles)\n- Yeah, so I think Stargate is a opaque thing.",
    "start": "17240168",
    "end": "17246816"
  },
  {
    "text": "It definitely doesn't have $500 billion, doesn't even have $100 billion. So, what they announced is\nthis $500 billion number,",
    "start": "17246816",
    "end": "17253364"
  },
  {
    "text": "Larry Ellison, Sam\nAltman, and Trump said it. They thanked Trump and Trump did do some executive actions",
    "start": "17253364",
    "end": "17261854"
  },
  {
    "text": "that do significantly improve the ability for this to be built faster.",
    "start": "17261854",
    "end": "17266784"
  },
  {
    "text": "One of the executive actions\nyou did is on federal land, you can just basically\nbuild data centers in power,",
    "start": "17267686",
    "end": "17273246"
  },
  {
    "text": "pretty much like that.\n- Mm-hmm. - And then, the permitting\nprocess is basically gone or you file after the fact. So, one of the, again,",
    "start": "17273246",
    "end": "17279426"
  },
  {
    "text": "like I had a schizo take\nearlier, another schizo take, if you've ever been to the\nPresidio in San Francisco, beautiful area, you\ncould build a power plant",
    "start": "17279426",
    "end": "17286668"
  },
  {
    "text": "and a data center there if you wanted to. Because it is federal land.\n(Nathan laughing) It used to be a military base, so. - It did.\n- But obviously,",
    "start": "17286668",
    "end": "17292934"
  },
  {
    "text": "this would piss people off. It's a good bit. Anyways. (Nathan and Lex laughing) Trump has made it much\neasier to do this generally.",
    "start": "17292934",
    "end": "17300850"
  },
  {
    "text": "Texas has the only unregulated\ngrid in the nation as well. - [Lex] Let's go, Texas. - And so, therefore,",
    "start": "17300850",
    "end": "17306742"
  },
  {
    "text": "like ERCOT enables people\nto build faster as well. In addition, the federal\nregulations are coming down.",
    "start": "17306742",
    "end": "17312562"
  },
  {
    "text": "And so, Stargate is predicated, and this is why that whole show happened. Now, how they came up",
    "start": "17312564",
    "end": "17318404"
  },
  {
    "text": "with a $500 billion number is beyond me. How they came up with $100 billion number makes sense to some extent.",
    "start": "17318404",
    "end": "17324894"
  },
  {
    "text": "And there's actually a good table in here that I would like to show in\nthat Stargate piece that I had.",
    "start": "17324894",
    "end": "17332454"
  },
  {
    "text": "It's the most recent one. Yeah. So, anyways, Stargate, it's basically,",
    "start": "17334780",
    "end": "17339692"
  },
  {
    "text": "it's a table about cost. There, you passed it\nalready. It's that one.",
    "start": "17342238",
    "end": "17349670"
  },
  {
    "text": "So, this table is explaining what happens. So, Stargate is in Abilene, Texas, the first $100 billion of it.",
    "start": "17349670",
    "end": "17356220"
  },
  {
    "text": "That site is 2.2 gigawatts of power in, about 1.8 gigawatts of power consumed.",
    "start": "17356222",
    "end": "17362020"
  },
  {
    "text": "Per GPU, they have like roughly... Oracle is already building\nthe first part of this",
    "start": "17362938",
    "end": "17369692"
  },
  {
    "text": "before Stargate came about. To be clear, they've been\nbuilding it for a year. They tried to rent it to Elon, in fact.",
    "start": "17369692",
    "end": "17375552"
  },
  {
    "text": "But Elon was like, it's\ntoo slow, I need it faster. So, then he went and did his Memphis thing.\n- Mm-hmm. - And so, OpenAI was able to get it",
    "start": "17375554",
    "end": "17381512"
  },
  {
    "text": "with this weird joint\nventure called Stargate. They initially signed\na deal with just Oracle for the first section of this cluster.",
    "start": "17381512",
    "end": "17387656"
  },
  {
    "text": "This first section of this cluster is roughly $5 billion to\n$6 billion of server spend.",
    "start": "17387656",
    "end": "17394388"
  },
  {
    "text": "And then, there's another billion or so of data center spend. And then, likewise, if you fill out that entire 1.8 gigawatts",
    "start": "17396968",
    "end": "17403836"
  },
  {
    "text": "with the next two generations\nof Nvidia's chips, GB200, GB300, VR200, and\nyou fill it out completely,",
    "start": "17403836",
    "end": "17410288"
  },
  {
    "text": "that ends up being roughly\n$50 billion of server cost. Plus there's data center\ncost, plus maintenance cost,",
    "start": "17410288",
    "end": "17418056"
  },
  {
    "text": "plus operation costs,\nplus all these things. And that's where OpenAI gets to their $100 billion\nannouncement that they had.",
    "start": "17418056",
    "end": "17425700"
  },
  {
    "text": "Because they talked about\n100 billion is phase one. That's this Abilene, Texas data center. $100 billion of \"total cost\nof ownership\", quote, unquote.",
    "start": "17425700",
    "end": "17433450"
  },
  {
    "text": "So, it's not CapEx, it's not investment, it's $100 billion of\ntotal cost of ownership. And then, there will be future phases.",
    "start": "17433450",
    "end": "17440704"
  },
  {
    "text": "They're looking at other sites that are even bigger than this\n2.2 gigawatts, by the way, in Texas and elsewhere.",
    "start": "17440704",
    "end": "17446126"
  },
  {
    "text": "And so, they're not\ncompletely ignoring that. But there is, the number of $100 billion",
    "start": "17446128",
    "end": "17452620"
  },
  {
    "text": "that they say is for phase one, which I do think will happen. They don't even have the money for that. Furthermore, it's not $100 billion,",
    "start": "17452620",
    "end": "17458574"
  },
  {
    "text": "it's $50 billion of spend. And then, $50 billion of\noperational cost, power, et cetera,",
    "start": "17458574",
    "end": "17464336"
  },
  {
    "text": "rental pricing, et cetera. 'Cause OpenAI is renting the GPUs",
    "start": "17464338",
    "end": "17469536"
  },
  {
    "text": "from the Stargate joint venture. What money do they\nactually have? SoftBank. SoftBank is gonna invest,",
    "start": "17469536",
    "end": "17475226"
  },
  {
    "text": "Oracle's gonna invest,\nOpenAI is gonna invest. OpenAI is on the line\n- Mm-hmm. - for $19 billion. Everyone knows that\nthey've only got 6 billion",
    "start": "17475226",
    "end": "17480834"
  },
  {
    "text": "in their last round and 4 billion in debt. But there's news of SoftBank",
    "start": "17480836",
    "end": "17486202"
  },
  {
    "text": "maybe investing 25 billion into OpenAI. So, that's part of it. So, 19 billion can come from there.",
    "start": "17486202",
    "end": "17492442"
  },
  {
    "text": "So, OpenAI does not have the\nmoney at all, to be clear. Ink is not dried on anything. OpenAI has $0",
    "start": "17492442",
    "end": "17498316"
  },
  {
    "text": "- Yeah.\n- for this 50 billion. And which they're legally obligated to put 19 billion of CapEx\nor into the joint venture.",
    "start": "17498316",
    "end": "17503708"
  },
  {
    "text": "And then, the rest, they're gonna pay via renting the GPUs\nfrom the joint venture. And then, there's Oracle.",
    "start": "17503708",
    "end": "17509202"
  },
  {
    "text": "Oracle has a lot of money. They're building the\nfirst section completely. They were spending for it themselves. This $6 billion of CapEx,\n$10 billion of TCO.",
    "start": "17510140",
    "end": "17518524"
  },
  {
    "text": "And they were gonna do that first section. They're paying for that. As far as the rest of the section, I don't know how much\nLarry wants to spend.",
    "start": "17519580",
    "end": "17526488"
  },
  {
    "text": "At any point, he could pull out. This is, again, this is\ncompletely voluntary. So, at any point, there's no signed ink on this.\n- Mm-hmm.",
    "start": "17526488",
    "end": "17531638"
  },
  {
    "text": "- But he potentially could contribute tens of\nbillions of dollars, to be clear. He's got the money,\nOracle's got the money.",
    "start": "17531638",
    "end": "17537380"
  },
  {
    "text": "And then, there's like MGX, which is the south, the UAE fund, which technically has $1.5\ntrillion for investing in AI.",
    "start": "17537380",
    "end": "17543988"
  },
  {
    "text": "But again, I don't know\nhow real that money is. And whereas there's no\nink signed for this,",
    "start": "17543988",
    "end": "17549722"
  },
  {
    "text": "SoftBank does not have\n$25 billion of cash. They have to sell down their stake in Arm,",
    "start": "17549724",
    "end": "17555832"
  },
  {
    "text": "which is the leader in\nCPUs, and they IPO'ed it. This is obviously what\nthey've always wanted to do. They just didn't know where\nthey'd redeploy the capital.",
    "start": "17555832",
    "end": "17561568"
  },
  {
    "text": "Selling down the stake in\nArm makes a ton of sense. So, they can sell that down and invest in this if they want to",
    "start": "17561570",
    "end": "17567438"
  },
  {
    "text": "and invest in OpenAI if they want to. As far as money secured, the first 100,000 GB200\ncluster can be funded.",
    "start": "17567438",
    "end": "17574794"
  },
  {
    "text": "Everything else after that - Up in the air.\n- is up in the air. Money's coming. I believe the money will come.",
    "start": "17575798",
    "end": "17581202"
  },
  {
    "text": "I personally do.\n(Lex laughing) - It's a belief. Okay.\n- It's a belief that they are gonna release better models and be able to raise more money,",
    "start": "17581202",
    "end": "17586866"
  },
  {
    "text": "right?\n- Yeah, yeah. But the actual reality\nis, is that Elon's right, the money does not exist.",
    "start": "17586868",
    "end": "17592136"
  },
  {
    "text": "- What is the US government\nhave to do with anything? What does Trump have\nto do with everything? He's just a hype man?\n- So, Trump is,",
    "start": "17592136",
    "end": "17598724"
  },
  {
    "text": "he's reducing the regulation\nso they can build it faster. And he is allowing them to do it. Because any investment of this side",
    "start": "17598724",
    "end": "17605206"
  },
  {
    "text": "is gonna involve antitrust stuff. So, obviously, he's gonna\nallow them to do it. He's gonna enable the regulations",
    "start": "17605206",
    "end": "17610824"
  },
  {
    "text": "to actually allow to be built. I don't believe there's\nany US government dollars being spent on this though.\n- Yeah.",
    "start": "17610824",
    "end": "17617318"
  },
  {
    "text": "So, I think he's also just\ncreating a general vibe that this regulation will go down and this is the era of building.",
    "start": "17617318",
    "end": "17624816"
  },
  {
    "text": "So, if you're a builder,\n- Yeah. - you want to create stuff, you wanna launch stuff,\nthis is the time to do it. - And so, we've had this\n1.8 gigawatt data center",
    "start": "17625886",
    "end": "17632578"
  },
  {
    "text": "in our data for over a year now, and we've been sending\nit to all of our clients, including many of these companies that are building the multi gigawatts.",
    "start": "17632578",
    "end": "17638908"
  },
  {
    "text": "But that is at a level that's not quite, maybe executives seeing\n$500 billion, $100 billion,",
    "start": "17638908",
    "end": "17644546"
  },
  {
    "text": "and then everyone's asking them like... So, it could spur another,\nan even faster arms race.",
    "start": "17644546",
    "end": "17649690"
  },
  {
    "text": "Because there's already an arms race, but this like 100 billion, $500 billion number, Trump\ntalking about it on TV.",
    "start": "17649690",
    "end": "17656086"
  },
  {
    "text": "It could spur the arm\nrace to be even faster and more investors to flood\nin and et cetera, et cetera.",
    "start": "17656086",
    "end": "17661184"
  },
  {
    "text": "So, I think you're right\nin that sense that OpenAI, or Trump is championing\npeople are gonna build more",
    "start": "17661184",
    "end": "17668260"
  },
  {
    "text": "and his actions are gonna\nlet people build more. - What are you excited about these several\nyears that are upcoming",
    "start": "17668260",
    "end": "17675972"
  },
  {
    "start": "17670000",
    "end": "18379000"
  },
  {
    "text": "in terms of cluster build outs, in terms of breakthroughs in AI?",
    "start": "17677772",
    "end": "17683448"
  },
  {
    "text": "Like the best possible\nfuture you can imagine in the next couple years,\ntwo, three, four years?",
    "start": "17683448",
    "end": "17688630"
  },
  {
    "text": "What does that look like? Just it could be very\nspecific technical things like breakthroughs on post post-training,",
    "start": "17688632",
    "end": "17694942"
  },
  {
    "text": "or it could be just size big. - [Dylan] Yeah, I mean it's- - Impressive clusters.",
    "start": "17694942",
    "end": "17701664"
  },
  {
    "text": "- I really enjoy tracking supply chain and who's involved in what.\n(Lex laughing) - Yeah.\n- I really do. It's really fun to see\nthe numbers, the cost,",
    "start": "17701664",
    "end": "17708504"
  },
  {
    "text": "who's building what capacity, helping them figure out how\nmuch capacity they should build, winning deals, strategic\nstuff, that's really cool.",
    "start": "17708504",
    "end": "17714412"
  },
  {
    "text": "I think technologically, there's a lot around the networking side\nthat really excites me",
    "start": "17714412",
    "end": "17719622"
  },
  {
    "text": "with optics and electronics\ngetting closer and closer, whether it be co-packaged optics,",
    "start": "17719622",
    "end": "17724952"
  },
  {
    "text": "or some sort of forms of\nnew forms of switching. - This is internal to a cluster.\n- Cluster. Yeah.",
    "start": "17724952",
    "end": "17731202"
  },
  {
    "text": "Also multi-data center training, people are putting so much\nfiber between these data centers",
    "start": "17731202",
    "end": "17736854"
  },
  {
    "text": "and lighting it up with so much\nbandwidth that there's a lot of interesting stuff\nhappening on that end.",
    "start": "17736854",
    "end": "17742468"
  },
  {
    "text": "Telecom has been really boring since 5G, and now it's like really\nexciting again on the fiber side.",
    "start": "17742470",
    "end": "17747982"
  },
  {
    "text": "- Can you educate me a little\nbit about the speed of things? So, the speed of memory versus\nthe speed of interconnect,",
    "start": "17747984",
    "end": "17753370"
  },
  {
    "text": "versus the speed of fiber\nbetween data centers? Are these orders of magnitude different?",
    "start": "17753370",
    "end": "17758896"
  },
  {
    "text": "Can we, at some point,\nconverge towards a place, where it all just feels like one computer?",
    "start": "17758896",
    "end": "17764312"
  },
  {
    "text": "- No, I don't think that's possible.\n- Okay. All right. (chuckles) - It's only gonna get harder to program, not easier.\n- Okay. - It's only gonna get more difficult",
    "start": "17764314",
    "end": "17770734"
  },
  {
    "text": "and complicated in more layers. The general image that people like to have is this hierarchy of memory.",
    "start": "17770734",
    "end": "17776744"
  },
  {
    "text": "So, on chip is really close. Localized within the\nchip, you have registers. And those are shared between\nsome compute elements,",
    "start": "17776744",
    "end": "17783290"
  },
  {
    "text": "and then you'll have caches, which are shared between\nmore compute elements. Then, you have memory, like HBM or DRAM,",
    "start": "17783290",
    "end": "17788584"
  },
  {
    "text": "like DDR memory or whatever it is. And that's shared between the whole chip. And then, you can have pools of memory",
    "start": "17788584",
    "end": "17794738"
  },
  {
    "text": "that are shared between many chips. And then, storage and you keep zoning out. The access latency across data centers,",
    "start": "17794738",
    "end": "17801598"
  },
  {
    "text": "across within the data\ncenter within a chip differs. So, you're obviously always, you're always gonna have different",
    "start": "17801598",
    "end": "17808488"
  },
  {
    "text": "programming paradigms for this. It's not gonna be easy. Programming this stuff is gonna be hard. Maybe AI can help with programming this.",
    "start": "17808488",
    "end": "17815472"
  },
  {
    "text": "But the way to think\nabout it is that like,",
    "start": "17815472",
    "end": "17819054"
  },
  {
    "text": "(Dylan sighs) there's the more elements\nyou add to a task,",
    "start": "17821960",
    "end": "17828080"
  },
  {
    "text": "you don't gain, you\ndon't get strong scaling. If I double the number of chips, I don't get 2x the performance. This is just like a reality of computing,",
    "start": "17828080",
    "end": "17835032"
  },
  {
    "text": "'cause there's inefficiencies. And there's a lot of interesting work being done to make it more linear.",
    "start": "17835032",
    "end": "17841698"
  },
  {
    "text": "Whether it's making the chips more networked together more tightly or cool programming models,\nor cool algorithmic things",
    "start": "17841698",
    "end": "17848686"
  },
  {
    "text": "that you can do on the model side. DeepSeek did some of these\nreally cool innovations because they were limited on interconnect,",
    "start": "17848686",
    "end": "17853844"
  },
  {
    "text": "but they still needed to parallelize. All sorts of... Everyone's always doing stuff. Google's got a bunch of work and everyone's got a\nbunch of work about this.",
    "start": "17853844",
    "end": "17860312"
  },
  {
    "text": "That stuff is super exciting on the model and workload\nand innovation side. Hardware, solid-state transformers",
    "start": "17860312",
    "end": "17867170"
  },
  {
    "text": "are interesting for the power side. There's all sorts of stuff on batteries and there's all sorts of stuff on...",
    "start": "17867170",
    "end": "17872814"
  },
  {
    "text": "I think when you look at, if you look at every layer\nof the compute stack, whether it goes from lithography and etch,",
    "start": "17872814",
    "end": "17878024"
  },
  {
    "text": "all the way to like\nfabrication, to optics, to networking, to power, to transformers, to cooling, to a networking",
    "start": "17878024",
    "end": "17885134"
  },
  {
    "text": "and you just go on up and\nup and up and up the stack. Even air conditioners for\ndata centers are innovating. There's like copper cables are innovating.",
    "start": "17885134",
    "end": "17892612"
  },
  {
    "text": "You wouldn't think it,\nbut copper cables are, there's some innovations happening there with the density of how you can pack them.",
    "start": "17892612",
    "end": "17898376"
  },
  {
    "text": "And it's like all of\nthese layers of the stack, all the way up to the models. Human progress is at a pace\nthat's never been seen before.",
    "start": "17898376",
    "end": "17904828"
  },
  {
    "text": "- I'm just imagining you sitting\nback in a layer somewhere with screens everywhere, just\nmonitoring the supply chain where all these clusters,",
    "start": "17904828",
    "end": "17910834"
  },
  {
    "text": "all the information you're gathering. You do incredible-\n- There's a big team. There's a big team. - Yeah. (laughs)",
    "start": "17910834",
    "end": "17917794"
  },
  {
    "text": "You do quite incredible\nwork with SemiAnalysis. Just keeping your finger on the pulse",
    "start": "17917794",
    "end": "17924278"
  },
  {
    "text": "of human civilization\nin the digital world. It's pretty cool just to watch, feel that.",
    "start": "17926282",
    "end": "17931668"
  },
  {
    "text": "- [Dylan] Yeah. Thank you, I guess- - Feel all of us doing shit, epic shit.",
    "start": "17931668",
    "end": "17936374"
  },
  {
    "text": "- [Dylan] Feel the AGI.\n(Lex laughing) - From meme to reality.",
    "start": "17937606",
    "end": "17942804"
  },
  {
    "text": "What Nathan, is there breakthroughs that you're looking\nforward to potentially? - I had a while to think\nabout this while listening",
    "start": "17942806",
    "end": "17948938"
  },
  {
    "text": "to Dylan's beautiful response.\n(Dylan laughing) - He did listen to me. He was so- - No, I knew this was coming. And it's like, realistically,\ntraining models is very fun",
    "start": "17948938",
    "end": "17957844"
  },
  {
    "text": "because there's so much low-hanging fruit. And the thing that makes\nmy job entertaining, I train models, I write analysis",
    "start": "17957844",
    "end": "17964542"
  },
  {
    "text": "about what's happening\nwith models, and it's fun because there is obviously so\nmuch more progress to be had.",
    "start": "17964542",
    "end": "17971332"
  },
  {
    "text": "And the real motivation why I do this somewhere where I can share\nthings is that there's just, I don't trust people that are like,",
    "start": "17971332",
    "end": "17978124"
  },
  {
    "text": "\"Trust me, bro, we're gonna make AI good.\" It's like we're the ones that\nit's like, we're gonna do it and you can trust us and we're\njust gonna have all the AI,",
    "start": "17978124",
    "end": "17984422"
  },
  {
    "text": "and it's just I would like a future, where more people have a say in what AI is and can understand it.",
    "start": "17984422",
    "end": "17991072"
  },
  {
    "text": "And it's a little bit less fun, that it's not a like\npositive thing of like, this is just all really fun.",
    "start": "17991072",
    "end": "17996542"
  },
  {
    "text": "Training models is fun and\nbring people in is fun, but it's really like AI, if it is going to be the most powerful\ntechnology of my lifetime,",
    "start": "17996542",
    "end": "18003584"
  },
  {
    "text": "it's like, we need to have\na lot of people involved in making that and... - Making it open\n(Nathan chuckles)",
    "start": "18003584",
    "end": "18010894"
  },
  {
    "text": "helps with that, as successful as possible,\nas open as possible, yeah. - In my read of the last few years",
    "start": "18010894",
    "end": "18016254"
  },
  {
    "text": "is that more openness\nwould help the AI ecosystem in terms of having more people\nunderstand what's going on,",
    "start": "18016254",
    "end": "18021926"
  },
  {
    "text": "whether that's researchers\nfrom non-AI fields to governments, to everything. It doesn't mean that openness\nwill always be the answer.",
    "start": "18021926",
    "end": "18027740"
  },
  {
    "text": "I think then it'll reassess of like, what is the biggest problem facing AI and tack on a different angle",
    "start": "18027740",
    "end": "18033520"
  },
  {
    "text": "to the wild ride that we're on. - And for me, just from\neven the user experience,",
    "start": "18033520",
    "end": "18040134"
  },
  {
    "text": "anytime you have the, like Karpathy said, the aha moments, like the magic,",
    "start": "18040134",
    "end": "18045260"
  },
  {
    "text": "like seeing the reasoning,\nthe chain of thought, it's like there's something really",
    "start": "18046208",
    "end": "18052570"
  },
  {
    "text": "just fundamentally beautiful about that. It's putting a mirror to ourselves\nand seeing like, oh shit.",
    "start": "18052570",
    "end": "18058080"
  },
  {
    "text": "It is solving intelligence as the cliche goal of these companies is. And you get to understand\nwhy we humans are special.",
    "start": "18058080",
    "end": "18067006"
  },
  {
    "text": "The intelligence within us is special. And for now also, why\nwe're special in terms of, we seem to be conscious in\nthe AI systems for now aren't,",
    "start": "18067006",
    "end": "18076572"
  },
  {
    "text": "and we get to solve, we get\nto explore that mystery. So, it's just really cool",
    "start": "18076572",
    "end": "18081708"
  },
  {
    "text": "to get to explore these\nquestions that I don't think, I would've never imagined\nwould be even possible",
    "start": "18081710",
    "end": "18090354"
  },
  {
    "text": "back when sort of just\nwatching with excitement the Deep Blue beat Kasparov.",
    "start": "18090354",
    "end": "18097496"
  },
  {
    "text": "I wouldn't have ever thought this kind of AI would be\npossible in my lifetime. It's like this is really feels like AI.",
    "start": "18097496",
    "end": "18103864"
  },
  {
    "text": "- Yeah. (chuckles)\n- It's incredible. - I started with AI of learning\nto fly a silly quadrotor. It's like learning to fly",
    "start": "18103864",
    "end": "18109664"
  },
  {
    "text": "and it just like, it learned to fly up, it would hit the ceiling\nand stop and catch it. It's like, okay, that is really stupid compared\nto what's going on now.",
    "start": "18109664",
    "end": "18116816"
  },
  {
    "text": "- And now, you could probably,\nwith natural language, tell it to learn to fly and it's going to generate\nthe control algorithm",
    "start": "18116816",
    "end": "18123090"
  },
  {
    "text": "required to do that. (laughs)\n- Probably. There's low level blockers. We have to do some weird stuff for that,",
    "start": "18123090",
    "end": "18128200"
  },
  {
    "text": "- Yeah, for sure.\n- but you can, you definitely can.\n- It's back to our robotics conversation, yeah. When you have to interact in the actual physical world, that's hard.",
    "start": "18128202",
    "end": "18134354"
  },
  {
    "text": "What gives you hope about the\nfuture of human civilization? Looking into the next 10\nyears, 100 years, 1,000 years,",
    "start": "18134354",
    "end": "18142770"
  },
  {
    "text": "how long you think we'll make it? You think we've got 1,000 years?\n- I think humans will definitely be around in 1,000 years.",
    "start": "18143756",
    "end": "18150668"
  },
  {
    "text": "I think there's ways that\nvery bad things could happen. There'll be way fewer humans, but humans are very good at surviving.",
    "start": "18150670",
    "end": "18157270"
  },
  {
    "text": "There's been a lot of\nthings that that is true. I don't think they're necessarily, we're good at long-term\ncredit assignment of risk,",
    "start": "18157270",
    "end": "18165606"
  },
  {
    "text": "but when the risk becomes immediate, we tend to figure things out, - Oh yeah.\n- and for that reason,",
    "start": "18165606",
    "end": "18170678"
  },
  {
    "text": "I am like, there's physical constraints to things like AGI hyper,",
    "start": "18170678",
    "end": "18175164"
  },
  {
    "text": "like recursive improvement\nto kill us all type stuff for physical reasons, and for how humans have\nfigured things out before,",
    "start": "18176456",
    "end": "18183104"
  },
  {
    "text": "I'm not too worried about it. AI takeover. There are other international\nthings that are worrying, but there's just\nfundamental human goodness",
    "start": "18183104",
    "end": "18191274"
  },
  {
    "text": "and trying to amplify that,\nand we're on a tenuous time. And if you look at humanity as a whole,",
    "start": "18192206",
    "end": "18200554"
  },
  {
    "text": "there's been times where\nthings go backwards, there's times when things\ndon't happen at all. And we're on a,",
    "start": "18200554",
    "end": "18206264"
  },
  {
    "text": "what should be very positive\ntrajectory right now. - Yeah, there seems to be\nprogress, but just with power,",
    "start": "18206264",
    "end": "18213286"
  },
  {
    "text": "there's spikes of human suffering, and we wanna try to minimize\nthe amount of spikes.",
    "start": "18213286",
    "end": "18218764"
  },
  {
    "text": "- Generally, humanity is\ngonna suffer a lot less. I'm very optimistic about that.",
    "start": "18218764",
    "end": "18224440"
  },
  {
    "text": "I do worry of techno\nfascism type stuff arising as AI becomes more and more\nprevalent and powerful,",
    "start": "18224440",
    "end": "18231554"
  },
  {
    "text": "and those who control\nit can do more and more. Maybe it doesn't kill us all, but at some point, every\nvery powerful human",
    "start": "18231554",
    "end": "18239164"
  },
  {
    "text": "is gonna wanna brain computer interface, so that they can interact with the AGI, and all of its advantages\nin many more way,",
    "start": "18239164",
    "end": "18245030"
  },
  {
    "text": "and merge its mind with sort of like, and its capabilities or\nthat person's capabilities can leverage those much\nbetter than anyone else,",
    "start": "18245030",
    "end": "18252032"
  },
  {
    "text": "and therefore be, it won't\nbe one person rule them all, but it will be... The thing I worry about is\nit'll be like few people,",
    "start": "18252032",
    "end": "18259228"
  },
  {
    "text": "hundreds, thousands, tens of thousands, maybe millions of people\nrule whoever's left,",
    "start": "18259228",
    "end": "18265646"
  },
  {
    "text": "and the economy around it. And that's the thing that's\nprobably more worrisome is human machine amalgamations.",
    "start": "18265646",
    "end": "18273576"
  },
  {
    "text": "This enables an individual human to have more impact on the world. And that impact can be\nboth positive and negative.",
    "start": "18273576",
    "end": "18280252"
  },
  {
    "text": "Generally, humans have\npositive impacts on the world, at least societally, but it's possible for individual humans",
    "start": "18280252",
    "end": "18285790"
  },
  {
    "text": "to have such negative impacts. And AGI, at least as I\nthink the labs define it,",
    "start": "18285790",
    "end": "18291276"
  },
  {
    "text": "which is not a runaway sentient thing, but rather just something\nthat can do a lot of tasks really efficiently,\namplifies the capabilities",
    "start": "18291276",
    "end": "18299236"
  },
  {
    "text": "of someone causing extreme damage. But for the most part, I think it'll be used for\nprofit-seeking motives,",
    "start": "18299236",
    "end": "18306124"
  },
  {
    "text": "which will increase the\nabundance and supply of things. and therefore reduce suffering. (Lex laughing)",
    "start": "18306124",
    "end": "18311886"
  },
  {
    "text": "- Yeah.\n- That's the goal. - Scrolling on a timeline,\n(Nathan laughing) just drowning in dopamine-\n- Crawling stasis.",
    "start": "18311886",
    "end": "18317294"
  },
  {
    "text": "- That is-\n- Scrolling holds the status quo of the world. - That is a positive outcome\n(Nathan and Lex laughing) is like, if I have food tubes\n- Yeah.",
    "start": "18317296",
    "end": "18324252"
  },
  {
    "text": "- and I'm scrolling and I'm happy, that's a positive outcome. (group laughing) - While expanding out into the cosmos.",
    "start": "18324254",
    "end": "18331884"
  },
  {
    "text": "Well, this is a fun time to be alive. And thank you for pushing the forefront",
    "start": "18331886",
    "end": "18337856"
  },
  {
    "text": "of what is possible in humans. And thank you for talking\ntoday. This was fun. - Thanks for having us.\n- Thanks for having us.",
    "start": "18337856",
    "end": "18344072"
  },
  {
    "text": "- Thanks for listening to this conversation with\nDylan Patel and Nathan Lambert. To support this podcast,",
    "start": "18344074",
    "end": "18349512"
  },
  {
    "text": "please check out our\nsponsors in the description. And now, let me leave you some\nwords from Richard Feynman.",
    "start": "18349514",
    "end": "18357536"
  },
  {
    "text": "\"For a successful technology,\nreality must take precedence over public relations, for\nnature cannot be fooled.\"",
    "start": "18357536",
    "end": "18366456"
  },
  {
    "text": "Thank you for listening and\nhope to see you next time.",
    "start": "18366546",
    "end": "18371046"
  }
]