[
  {
    "start": "0",
    "end": "44000"
  },
  {
    "text": "All right. So, we have talked about regular neural networks,",
    "start": "0",
    "end": "6140"
  },
  {
    "text": "fully connected neural networks, we have talked about\nconvolutional neural networks that work with images,",
    "start": "6380",
    "end": "12639"
  },
  {
    "text": "we have talked about Reinforcement, Deeper Reinforcement Learning, where we plug in a neural network",
    "start": "12700",
    "end": "18860"
  },
  {
    "text": "into a Reinforcement\nLearning Algorithm, when a system has to not only",
    "start": "18940",
    "end": "25500"
  },
  {
    "text": "perceive the world but also act in it, and collect a reward. And today we will talk about,",
    "start": "25560",
    "end": "31080"
  },
  {
    "text": "perhaps the least understood but the most exciting neural network out there,",
    "start": "32880",
    "end": "37880"
  },
  {
    "text": "flavor of neural networks,\nis Recurrent Neural Networks.",
    "start": "38880",
    "end": "42460"
  },
  {
    "start": "44000",
    "end": "44000"
  },
  {
    "text": "But first, for administrative stuff, there’s a website.\nI don’t know if you heard,",
    "start": "44300",
    "end": "50840"
  },
  {
    "text": "cars.mit.edu, where you should create an account,\nif you’re a registered student, that’s one of the requirements.",
    "start": "50840",
    "end": "57160"
  },
  {
    "text": "You need to have an account if you want to get credit for this, you need to submit code",
    "start": "57160",
    "end": "62540"
  },
  {
    "text": "for DeepTrafficJS,\nand DeepTeslaJS, and for DeepTraffic,",
    "start": "62540",
    "end": "68020"
  },
  {
    "text": "you have to have a neural network\nthat drives faster than 65mph. If you need help to achieve that speed",
    "start": "68020",
    "end": "74980"
  },
  {
    "text": "please e-mail us. We can give you some hints.",
    "start": "74980",
    "end": "79659"
  },
  {
    "text": "For those of you who are\nold school SNL fans, there’s the Deep Thoughts\nsection now,",
    "start": "80640",
    "end": "85840"
  },
  {
    "text": "in the profile page, where we encourage you to talk about the kinds of things that\nyou tried in DeepTraffic",
    "start": "87380",
    "end": "94159"
  },
  {
    "text": "or any of the other DeepTesla or\nany of the work you've done",
    "start": "94160",
    "end": "99140"
  },
  {
    "text": "as part of this class\nfor DeepLearning. Okay,",
    "start": "99540",
    "end": "103780"
  },
  {
    "text": "we have talked about the Vanilla Neural Networks\non the left. The Vanilla Neural Network",
    "start": "105100",
    "end": "111320"
  },
  {
    "start": "110000",
    "end": "110000"
  },
  {
    "text": "is the one where it's computing is approximating a function that maps\nfrom one input",
    "start": "111320",
    "end": "116820"
  },
  {
    "text": "to one output. An example is mapping images to the number that is shown\nin the image.",
    "start": "117180",
    "end": "123830"
  },
  {
    "text": "For ImageNet is mapping an image to what's the object in the image. It can be anything.",
    "start": "123940",
    "end": "130019"
  },
  {
    "text": "In fact, Convolutional Neural Networks\ncan operate on audio, you can give it a chunk of audio,\na five second audio clip,",
    "start": "130020",
    "end": "137209"
  },
  {
    "text": "that still counts as one input because it’s fixed-size. As long as the size of the input is fixed,",
    "start": "137660",
    "end": "143400"
  },
  {
    "text": "that's one chunk of input and as long as you have ground truth",
    "start": "143400",
    "end": "149600"
  },
  {
    "text": "that maps that chunk of input\nto some output ground truth, that’s the Vanilla Neural Network.",
    "start": "149600",
    "end": "156030"
  },
  {
    "text": "Whether there's a fully connected\nneural network or convolutional neural network.",
    "start": "156100",
    "end": "160900"
  },
  {
    "text": "Today we’ll talk about the amazing, the mysterious Recurrent\nNeural Networks.",
    "start": "161200",
    "end": "167720"
  },
  {
    "text": "They compute functions from one to many, from many to one,",
    "start": "167720",
    "end": "173840"
  },
  {
    "text": "from many to many.",
    "start": "173840",
    "end": "175200"
  },
  {
    "text": "Also bidirectional. What does that mean? They take its input sequences,",
    "start": "179320",
    "end": "185280"
  },
  {
    "text": "time series, audio, video, whenever there's a sequence of data,",
    "start": "185700",
    "end": "192480"
  },
  {
    "text": "and that temporal dynamics that connects the data is more important than the spatial",
    "start": "192480",
    "end": "198280"
  },
  {
    "text": "content of each individual frame. So, whenever there's\na lot of information being conveyed in a sequence,",
    "start": "200060",
    "end": "206640"
  },
  {
    "text": "in a temporal change of whatever that type of data is, that's when you want to use\nRecurrent Neural Networks",
    "start": "207300",
    "end": "213600"
  },
  {
    "text": "like speech, natural language, audio",
    "start": "213600",
    "end": "218240"
  },
  {
    "text": "and the power of this is that for many of them, for a Recurrent Neural Network, where they really shine,",
    "start": "218680",
    "end": "224980"
  },
  {
    "text": "is when the size of the input\nis variable, so you don’t have a fixed chunk of data",
    "start": "224980",
    "end": "231200"
  },
  {
    "text": "that you're putting in\nis variable input. And the same goes\nfor the output,",
    "start": "231200",
    "end": "235980"
  },
  {
    "text": "so you can give it\na sequence of speech, several seconds of speech",
    "start": "236620",
    "end": "242940"
  },
  {
    "text": "and then the output is a single label of whether\nthe speaker is male or female.",
    "start": "243700",
    "end": "250700"
  },
  {
    "text": "That’s many to one. You can also do",
    "start": "251340",
    "end": "256780"
  },
  {
    "text": "many to many. Translation. You can have natural language",
    "start": "257380",
    "end": "262460"
  },
  {
    "text": "put into the network in Spanish and the output is in English.",
    "start": "262460",
    "end": "269180"
  },
  {
    "text": "Machine translation. That's many to many. And that many to many\ndoesn't have to be",
    "start": "270320",
    "end": "275860"
  },
  {
    "text": "mapped directly into same sized sequences. For video, the sequence size\nmight be the same",
    "start": "275860",
    "end": "282660"
  },
  {
    "text": "you're labeling every single frame,\nyou put in a five second clip",
    "start": "282680",
    "end": "287340"
  },
  {
    "text": "of somebody playing basketball and you can label\nevery single frame counting the number of people\nin every single frame.",
    "start": "289030",
    "end": "295219"
  },
  {
    "text": "That's many to many when the size of the input and\nthe size of the output is the same Yes, question?",
    "start": "295220",
    "end": "300460"
  },
  {
    "text": "The question was, are there are any models where there's\nfeedback from output and input? That's exactly what\nRecurrent Neural Networks are.",
    "start": "300840",
    "end": "309020"
  },
  {
    "text": "It produces output, and it copies that output and loops it back in.",
    "start": "309880",
    "end": "315139"
  },
  {
    "text": "That's almost the definition of\na Recurrent Neural Network. There's a loop in there\nthat produces the output",
    "start": "318160",
    "end": "324360"
  },
  {
    "text": "and also takes that output\nas input once again.",
    "start": "324400",
    "end": "327320"
  },
  {
    "text": "There's also many to many\nwhere the sequences don't align. Like machine translation,",
    "start": "329560",
    "end": "334680"
  },
  {
    "text": "the size of the output sequence might be  totally different\nthan the input sequence. We will look on a lot\nof cool applications;",
    "start": "334700",
    "end": "341910"
  },
  {
    "text": "you can start a song, learn the audio of\na particular song have the Recurrent Neural Network",
    "start": "344240",
    "end": "351240"
  },
  {
    "text": "to continue that song after\na certain period of time. So it can learn to generate sequences",
    "start": "351240",
    "end": "357840"
  },
  {
    "text": "of audio, of natural language, of video. Okay.",
    "start": "357900",
    "end": "361940"
  },
  {
    "start": "361000",
    "end": "361000"
  },
  {
    "text": "I know I promised not many equations, but this is so beautifully simple",
    "start": "364120",
    "end": "371080"
  },
  {
    "text": "that we have to cover\nbackpropagation. It's also the thing that, if you're a little bit lazy",
    "start": "371080",
    "end": "378199"
  },
  {
    "text": "and you go to the internet and start using\nthe basic tutorials of TensorFlow, you ignore how backpropagation work.",
    "start": "378200",
    "end": "385699"
  },
  {
    "text": "At you peril. You kind of assume it just works. I give it some inputs, some outputs,",
    "start": "385700",
    "end": "391030"
  },
  {
    "text": "and it's like Lego pieces\nI can assemble them like you might have done\nwith DeepTraffic A bunch of layers put in together",
    "start": "391050",
    "end": "396710"
  },
  {
    "text": "and then just press Train. backpropagation is the mechanism that neural networks currently--",
    "start": "396740",
    "end": "403140"
  },
  {
    "text": "The best mechanism we know of\nthat is used for training. So you need to understand",
    "start": "403140",
    "end": "407480"
  },
  {
    "text": "the simple power of backpropagation, but also the dangers.",
    "start": "409460",
    "end": "414480"
  },
  {
    "text": "Summary, I put on the top of the slide,\nthere's an input for the network that's an image,",
    "start": "417500",
    "end": "424380"
  },
  {
    "text": "there's a bunch of neurons, all with differentiable smooth activation functions\non each neuron,",
    "start": "424380",
    "end": "432620"
  },
  {
    "text": "and then, as you pass through those\nactivation functions,",
    "start": "432620",
    "end": "438280"
  },
  {
    "text": "take in an input, pass it through this net of differentiable\ncompute nodes,",
    "start": "438280",
    "end": "445520"
  },
  {
    "text": "you produce an output. In that output you also have a ground truth,",
    "start": "445520",
    "end": "450759"
  },
  {
    "text": "the correct, the truth that you hope or you expect the network to produce.",
    "start": "450760",
    "end": "455870"
  },
  {
    "text": "And you can look at\nthe differences between what the network actually produced and what you hoped it would produce,",
    "start": "455920",
    "end": "462300"
  },
  {
    "text": "and that's an error. And then you backward\npropagate that error, punishing or rewarding",
    "start": "462300",
    "end": "468000"
  },
  {
    "text": "the parameters of the network that resulted in that output",
    "start": "470680",
    "end": "474400"
  },
  {
    "text": "Let's start with a really\nsimple example.",
    "start": "477160",
    "end": "479920"
  },
  {
    "text": "There's a function that takes its input up on top,",
    "start": "483060",
    "end": "488860"
  },
  {
    "text": "three variables, X, Y and Z. The function does two things: it adds X and Y",
    "start": "489200",
    "end": "495940"
  },
  {
    "text": "and then it multiplies that sum by Z. And then we can formulate\nthat as a circuit,",
    "start": "495940",
    "end": "503020"
  },
  {
    "text": "circuit of gates, where there's a Plus gate, and a Multiplication gate.",
    "start": "503440",
    "end": "509120"
  },
  {
    "start": "514000",
    "end": "514000"
  },
  {
    "text": "Let's take some inputs, shown in blue. Let's say it's X is negative two,",
    "start": "516140",
    "end": "521860"
  },
  {
    "text": "Y is five and Z\nis negative four. And let's do a forward pass",
    "start": "521860",
    "end": "527420"
  },
  {
    "text": "through the circuit to produce the output. Negative two plus five\nequals three",
    "start": "527420",
    "end": "535020"
  },
  {
    "text": "q is that intermediate value, three.",
    "start": "535860",
    "end": "539279"
  },
  {
    "text": "This is so simple, and so important to understand that I just want to take my time for this",
    "start": "541300",
    "end": "547350"
  },
  {
    "text": "because everything else about neural\nnetworks just builds on these concepts",
    "start": "547370",
    "end": "551810"
  },
  {
    "text": "The add gate produces q, in this case, is three, and three times negative\nfour is twelve.",
    "start": "553880",
    "end": "559660"
  },
  {
    "text": "That's the output. The output of the circuit\nof this network,",
    "start": "559660",
    "end": "564480"
  },
  {
    "text": "if you think of it as such, is negative twelve. The forward pass is shown in blue",
    "start": "564880",
    "end": "571570"
  },
  {
    "text": "the backward pass\nwill be shown in red in a second here What we want to do, what would make us happy,",
    "start": "571580",
    "end": "577720"
  },
  {
    "text": "what would make f happy is for the output to be\nas high possible. Negative twelve,\nso-so, it could be better.",
    "start": "577720",
    "end": "584400"
  },
  {
    "text": "How do we teach it How do we adjust X, Y and Z,",
    "start": "584400",
    "end": "588640"
  },
  {
    "text": "to ensure it produces a higher f",
    "start": "589540",
    "end": "594040"
  },
  {
    "text": "makes f happier. Let's start backward,",
    "start": "595420",
    "end": "599340"
  },
  {
    "text": "The backward pass. We'll make the gradient\non the output one,",
    "start": "600660",
    "end": "605839"
  },
  {
    "text": "meaning we want this to increase. We want f to increase. That's how we encode our happiness.",
    "start": "605840",
    "end": "611140"
  },
  {
    "text": "We want it to go up by one. In order to then propagate",
    "start": "612100",
    "end": "619260"
  },
  {
    "start": "614000",
    "end": "614000"
  },
  {
    "text": "that fact that we want the f to go up by one, we have to look at",
    "start": "621140",
    "end": "627760"
  },
  {
    "text": "the gradient on each one of the gates. And what's a gradient?",
    "start": "627760",
    "end": "632480"
  },
  {
    "text": "It's a partial derivative",
    "start": "634000",
    "end": "638740"
  },
  {
    "text": "with respect to its inputs. The partial derivative of\nthe output of the gate with respect to its inputs,",
    "start": "640560",
    "end": "647260"
  },
  {
    "text": "if you don't know what that means, is just",
    "start": "647260",
    "end": "650180"
  },
  {
    "text": "how much does the output change when I change the inputs a little bit. What is the slope of that change\nif I increase X",
    "start": "654040",
    "end": "662280"
  },
  {
    "text": "for the first function of addition, f of X, Y equals X plus Y. If I increase X by a little bit,",
    "start": "662340",
    "end": "669040"
  },
  {
    "text": "what happens to f? If I increase Y by a little bit,\nwhat happens to f? Taking a partial derivative of those",
    "start": "669040",
    "end": "675160"
  },
  {
    "text": "with respect to X and Y you just get a slope of one When you increase X,",
    "start": "675160",
    "end": "680640"
  },
  {
    "text": "f increases linearly. Same with Y. Multiplication is a little trickier.",
    "start": "680640",
    "end": "686760"
  },
  {
    "text": "When you increase X, f increases by Y.",
    "start": "688740",
    "end": "693639"
  },
  {
    "text": "Do the partial derivative of f\nwith respect to X is Y, the partial derivative of f\nwith respect to Y is X.",
    "start": "694500",
    "end": "701700"
  },
  {
    "text": "If you think about it,\nwhat happens is the gradients, when you change X,",
    "start": "704500",
    "end": "710399"
  },
  {
    "text": "the gradient of change doesn't care about X. It cares about Y.",
    "start": "711020",
    "end": "717080"
  },
  {
    "start": "718000",
    "end": "718000"
  },
  {
    "text": "It's flipped. So we can backpropagate that one, the indication of what\nmakes X happy backward.",
    "start": "718140",
    "end": "725580"
  },
  {
    "text": "And that's done by computing the local gradient.",
    "start": "727440",
    "end": "731339"
  },
  {
    "text": "For q, the partial derivative\nof f with respect to q,",
    "start": "735020",
    "end": "741300"
  },
  {
    "text": "that intermediate value, that gradient would be negative four.",
    "start": "741300",
    "end": "746820"
  },
  {
    "text": "It will take the value of Z as I said it's the Multiplication gate, It'll take the value of Z",
    "start": "746820",
    "end": "752460"
  },
  {
    "text": "and assign it to the gradient. And the same for the partial derivative of f\nwith respect to Z,",
    "start": "755260",
    "end": "761940"
  },
  {
    "text": "it will assign that to q. The value of the forward pass on the q. There's a three",
    "start": "761940",
    "end": "767500"
  },
  {
    "text": "and a negative four\non the forward pass in blue and that's flipped. Negative four and three",
    "start": "767500",
    "end": "772959"
  },
  {
    "text": "on the backward pass. That's the gradient. And then we continue in\nthe same exact process.",
    "start": "773640",
    "end": "779819"
  },
  {
    "text": "But wait. What makes all of this work,",
    "start": "780600",
    "end": "786160"
  },
  {
    "text": "is the Chain Rule. It's magical.",
    "start": "787220",
    "end": "790519"
  },
  {
    "text": "What it allows us to do is to compute the gradient,",
    "start": "792720",
    "end": "796680"
  },
  {
    "text": "the gradien of f with respect to\nthe inputs X, Y, Z. We don't need to construct",
    "start": "800100",
    "end": "805480"
  },
  {
    "text": "the giant function that is the partial derivative of f\nwith respect to X, Y and Z",
    "start": "807160",
    "end": "815860"
  },
  {
    "text": "analytically. We can do it step by step backpropagating the gradients. We can multiply\nthe gradients together",
    "start": "815860",
    "end": "822490"
  },
  {
    "start": "821000",
    "end": "821000"
  },
  {
    "text": "as opposed to doing\nthe partial derivative of f with respect to X. We have just the intermediate,",
    "start": "822510",
    "end": "828700"
  },
  {
    "text": "the local gradient of f with respect to q,\nand of q with respect to X,",
    "start": "828700",
    "end": "833980"
  },
  {
    "text": "and multiply them together. So, Instead of computing",
    "start": "833980",
    "end": "839940"
  },
  {
    "text": "gradient of that giant function X plus Y times Z,",
    "start": "840560",
    "end": "846700"
  },
  {
    "text": "in this case is not that giant, but it gets pretty giant\nwith neural networks, we just go step by step.",
    "start": "846700",
    "end": "852200"
  },
  {
    "text": "Look at the first function, simple addition, q equals X plus Y, and the second function,\nmultiplication,",
    "start": "852200",
    "end": "860160"
  },
  {
    "text": "f equals q times Z.",
    "start": "860160",
    "end": "862160"
  },
  {
    "text": "The gradient on X and Y, the partial derivative",
    "start": "865420",
    "end": "872240"
  },
  {
    "text": "of f with respect to X and Y is computed by multiplying",
    "start": "872780",
    "end": "877839"
  },
  {
    "text": "the gradient on the output,\nnegative four, times the gradient on the inputs,",
    "start": "878260",
    "end": "884740"
  },
  {
    "text": "which as we talked about, when the operation is addition, that's just one.",
    "start": "884740",
    "end": "890080"
  },
  {
    "text": "It's negative four times one.",
    "start": "890080",
    "end": "892080"
  },
  {
    "start": "895000",
    "end": "895000"
  },
  {
    "text": "What does that mean? Let's interpret those numbers. You now have gradients on X, Y and Z",
    "start": "896480",
    "end": "904200"
  },
  {
    "text": "the partial derivatives of F\nwith respect to X, Y, Z. That means,",
    "start": "905840",
    "end": "910079"
  },
  {
    "text": "for X and Y is negative four,\nfor Z is three. That means, in order to\nmake f happy,",
    "start": "911360",
    "end": "917680"
  },
  {
    "text": "we have to decrease the inputs that have\na negative gradient",
    "start": "917680",
    "end": "925360"
  },
  {
    "text": "and increase the inputs that\nhave a positive gradient. The negatives ones are X and Y,",
    "start": "925360",
    "end": "930399"
  },
  {
    "text": "the positive is Z.",
    "start": "930400",
    "end": "931850"
  },
  {
    "text": "Hopefully, I don't say the word “Beautiful” too many\ntimes in this presentation this is very simple.\nBeautifully simple.",
    "start": "935680",
    "end": "942240"
  },
  {
    "text": "Because this gradient\nis a local worker, it propagates for you;",
    "start": "944780",
    "end": "950160"
  },
  {
    "text": "it has no knowledge of the broader happiness of f.",
    "start": "950160",
    "end": "955800"
  },
  {
    "text": "It computes the greater between\nthe output and the input. And it can propagate this gradient",
    "start": "958120",
    "end": "963700"
  },
  {
    "text": "based on, in this case f, a gradient of one but also the error.",
    "start": "963700",
    "end": "969839"
  },
  {
    "text": "Instead of one we can have on\nthe output the error as the measure of happiness. And then we can propagate\nthat error backwards.",
    "start": "970420",
    "end": "977320"
  },
  {
    "text": "These gates are important\nbecause we can break down almost every operation\nwe can think of",
    "start": "977340",
    "end": "982450"
  },
  {
    "text": "that we work within neural networks into one or several gates like these.",
    "start": "982460",
    "end": "987180"
  },
  {
    "text": "The most popular are three, which is addition, multiplication and the Max operation.",
    "start": "988260",
    "end": "994080"
  },
  {
    "text": "For addition,",
    "start": "994080",
    "end": "995300"
  },
  {
    "text": "the process is you take a forward pass\nthrough the network, so we have a value on every single gate,",
    "start": "999340",
    "end": "1005780"
  },
  {
    "text": "and then you take the backward pass. And through the backward pass\nyou compute those gradients.",
    "start": "1007040",
    "end": "1012920"
  },
  {
    "text": "For an add gate, you equally distribute the gradients on the output to the input,",
    "start": "1013520",
    "end": "1018960"
  },
  {
    "text": "when the gradient on the output\nis negative four, you equally distribute it tonegative four.",
    "start": "1018960",
    "end": "1023910"
  },
  {
    "text": "And you ignore the forward pass value. That three is ignored\nwhen you backpropagate it.",
    "start": "1026700",
    "end": "1032480"
  },
  {
    "text": "On the Multiply gate, it's trickier. You switch the forward pass values,",
    "start": "1035940",
    "end": "1042740"
  },
  {
    "text": "if you look at f, that's a\nMultiply gate,",
    "start": "1043560",
    "end": "1047680"
  },
  {
    "text": "the forward pass values are switched and multiplied by the value of\nthe gradient in the output.",
    "start": "1049720",
    "end": "1056620"
  },
  {
    "text": "If it's confusing, go through\nthe slides slowly. It'll make a lot more sense.",
    "start": "1057960",
    "end": "1063760"
  },
  {
    "text": "Hopefully. One more gate. There's the Max gate, which takes the inputs",
    "start": "1064160",
    "end": "1070820"
  },
  {
    "text": "and produces as output the value that is larger.",
    "start": "1070820",
    "end": "1075720"
  },
  {
    "text": "When computing the gradient\nof the Max gate, it distributes the gradient",
    "start": "1076360",
    "end": "1083740"
  },
  {
    "text": "similarly to the Add gate, but to only one,\nto only one  of the inputs;",
    "start": "1084260",
    "end": "1093640"
  },
  {
    "text": "the largest one. unlike the Add gate,\npays attention to the input",
    "start": "1093980",
    "end": "1099160"
  },
  {
    "text": "the input values on\nthe forward pass. All right.",
    "start": "1099180",
    "end": "1103540"
  },
  {
    "text": "Lots of numbers but\nthe whole point here is, it's really simple;",
    "start": "1104780",
    "end": "1110500"
  },
  {
    "start": "1107000",
    "end": "1107000"
  },
  {
    "text": "a neural network is just\na simple collection of these gates. You take a forward pass,",
    "start": "1113160",
    "end": "1119560"
  },
  {
    "text": "you calculate some kind of function in the end, the gradient in the very end, and you propagate that back.",
    "start": "1120080",
    "end": "1125960"
  },
  {
    "text": "Usually, for neural networks,\nthat's an Error function. A Loss function, Objective function,",
    "start": "1125960",
    "end": "1131820"
  },
  {
    "text": "a Cost function.\nAll the same word. That's the Sigmoid function there",
    "start": "1132480",
    "end": "1140140"
  },
  {
    "text": "When you have three weights W zero, W one, W two and X, two inputs, X0, X1,",
    "start": "1140140",
    "end": "1149429"
  },
  {
    "text": "that's going to be\nthe Sigmoid function. That's how you compute the output",
    "start": "1149440",
    "end": "1153320"
  },
  {
    "text": "of the neuron. But then you can decompose\nthat neuron you can separate it all into",
    "start": "1158500",
    "end": "1164679"
  },
  {
    "text": "just a set of gates like this Addition, multiplication, there's an exponential in there\nand division",
    "start": "1164680",
    "end": "1171620"
  },
  {
    "text": "but all very similar. And you repeat the exact same process.",
    "start": "1171620",
    "end": "1175120"
  },
  {
    "text": "there's five inputs, there's three weights and two inputs.\nX zero, X one.",
    "start": "1178820",
    "end": "1184140"
  },
  {
    "text": "You take a forward pass\nthrough this circuit,",
    "start": "1186280",
    "end": "1190860"
  },
  {
    "text": "in this case again, you want it to increase so that\nthe gradient of the output is one",
    "start": "1192340",
    "end": "1198940"
  },
  {
    "text": "and you backpropagate that gradient of one, to the inputs.",
    "start": "1198940",
    "end": "1203420"
  },
  {
    "text": "Now in neural networks, there's a bunch of parameters that you're trying through\nthis process, modify.",
    "start": "1204120",
    "end": "1209900"
  },
  {
    "text": "And you don't get to modify the inputs You get to modify the weights\nalong the way,",
    "start": "1209900",
    "end": "1215059"
  },
  {
    "text": "and the biases. The inputs are fixed, the outputs are fixed, the outputs that you hope",
    "start": "1215060",
    "end": "1220860"
  },
  {
    "text": "the network will produce. What you're modifying is the weights. So I get to try to adjust those weights",
    "start": "1221600",
    "end": "1228040"
  },
  {
    "text": "in the direction of the gradient.",
    "start": "1229400",
    "end": "1231780"
  },
  {
    "start": "1233000",
    "end": "1233000"
  },
  {
    "text": "That's the task of backpropagation.  The main way that\nneural networks learn.",
    "start": "1234840",
    "end": "1241040"
  },
  {
    "text": "As we update the weights\nand the biases to decrease the loss function.",
    "start": "1241560",
    "end": "1246400"
  },
  {
    "text": "The lower the loss function the better. In this case, you have",
    "start": "1247020",
    "end": "1252340"
  },
  {
    "text": "three inputs on the top left. A simple network, three inputs.",
    "start": "1252660",
    "end": "1257220"
  },
  {
    "text": "Three weights on each of the inputs. There's a bias on the node, b and produces an output",
    "start": "1258760",
    "end": "1266279"
  },
  {
    "text": "a, and that little symbol is indicating\na Sigmoid function.",
    "start": "1266960",
    "end": "1273279"
  },
  {
    "text": "And the loss is computed as Y minus\nA squared,",
    "start": "1275480",
    "end": "1281220"
  },
  {
    "text": "divided by two, where Y is the ground truth,",
    "start": "1282660",
    "end": "1287700"
  },
  {
    "text": "the output that you want\nthe network to produce. And that loss function\nis backpropagating",
    "start": "1287960",
    "end": "1294980"
  },
  {
    "text": "in exactly the same way that\nwe described before. The subtasks involved in this update of\nweights and biases",
    "start": "1294980",
    "end": "1302559"
  },
  {
    "text": "is that the forward pass computes the network output at every neuron, and finally, the output layer,",
    "start": "1302560",
    "end": "1309760"
  },
  {
    "text": "computes the error,\nthe difference between a and b, and then",
    "start": "1310180",
    "end": "1315120"
  },
  {
    "text": "backward propagates\nthe gradients. Instead of one on the output, it will be the error on the output\nand you backpropagated.",
    "start": "1315660",
    "end": "1323240"
  },
  {
    "text": "And then, once you know the gradient, you adjust the weights\nand the biases in the direction of the gradient.",
    "start": "1323410",
    "end": "1329560"
  },
  {
    "text": "Actually, the opposite of the\ndirection of the gradient, because you want the loss to decrease.",
    "start": "1329580",
    "end": "1334980"
  },
  {
    "text": "And the amount by which\nyou make that adjustment is called the Learning Rate.",
    "start": "1334980",
    "end": "1340100"
  },
  {
    "text": "The learning rate can be\nthe same across the entire network or can be individual\nthrough every weight.",
    "start": "1340130",
    "end": "1345550"
  },
  {
    "text": "And the process of adjusting the weights and biases is just optimization.",
    "start": "1352500",
    "end": "1358880"
  },
  {
    "text": "Learning is an Optimization problem. You have an objective function,\nand you're trying to minimize it.",
    "start": "1359120",
    "end": "1364850"
  },
  {
    "text": "And your variables are the parameters,\nthe weights and biases. Neural networks just happen to have",
    "start": "1364880",
    "end": "1371360"
  },
  {
    "text": "tens, hundreds of thousands, millions of those parameters.",
    "start": "1371360",
    "end": "1376060"
  },
  {
    "text": "So the function that you're trying\nto minimize is highly non-linear. But it boils down to\nsomething like this, you have",
    "start": "1377080",
    "end": "1383200"
  },
  {
    "text": "two weights, two plots--\nor actually one weight",
    "start": "1383200",
    "end": "1389240"
  },
  {
    "text": "and as you adjust it, the cost",
    "start": "1389240",
    "end": "1391960"
  },
  {
    "text": "you adjust in such a way that\nminimizes the output cost.",
    "start": "1394720",
    "end": "1398320"
  },
  {
    "text": "And there's a bunch of\noptimization methods for doing this. this is a convex function,",
    "start": "1400860",
    "end": "1408040"
  },
  {
    "text": "You can find the local minimum. If you know about these\nkinds of terminologies,",
    "start": "1408040",
    "end": "1413780"
  },
  {
    "text": "the local minimum is the same\nas the global minimum, it's not a weirdly hilly terrain",
    "start": "1413780",
    "end": "1419340"
  },
  {
    "text": "where you can get stuck in. Your goal is to get to\nthe bottom of this thing and if it's really complex terrain,",
    "start": "1419340",
    "end": "1426440"
  },
  {
    "text": "it will be hard to get\nto the bottom of it.",
    "start": "1426520",
    "end": "1429000"
  },
  {
    "text": "This general approach\nis gradient descent, and there's a lot of different ways to\ndo a gradient descent.",
    "start": "1432660",
    "end": "1439220"
  },
  {
    "text": "Various ways of adding\nrandomness into the process, so you don't get stuck into the weird",
    "start": "1440320",
    "end": "1446420"
  },
  {
    "text": "crevices of the terrain. But it's messy.",
    "start": "1446420",
    "end": "1451480"
  },
  {
    "text": "You have to be really careful. This is the part you have\nto be aware of, when you design a network\nfor DeepTraffic",
    "start": "1451780",
    "end": "1458700"
  },
  {
    "text": "and nothing is happening this might be what's happening:",
    "start": "1458700",
    "end": "1462860"
  },
  {
    "text": "vanishing gradients or exploding gradients.",
    "start": "1464180",
    "end": "1467679"
  },
  {
    "text": "When the partial derivatives are small, so you take\nthe Sigmoid function,",
    "start": "1469740",
    "end": "1474920"
  },
  {
    "text": "the most popular for a while, activation function, the derivative is zero at the tails.",
    "start": "1475360",
    "end": "1483040"
  },
  {
    "text": "When the input to the Sigmoid functions is\nreally high or really low,",
    "start": "1483040",
    "end": "1488340"
  },
  {
    "text": "that derivative is going to be zero.",
    "start": "1488860",
    "end": "1491360"
  },
  {
    "text": "Gradient tells on how much\nI want to adjust the weights. The gradient might be zero,",
    "start": "1495060",
    "end": "1500120"
  },
  {
    "text": "and so you backpropagate that zero, a very low number, and it gets less and less",
    "start": "1500120",
    "end": "1506140"
  },
  {
    "text": "as you backpropagate and so the result is that",
    "start": "1506140",
    "end": "1510920"
  },
  {
    "text": "you think you don't need to\nadjust the weights at all. And when a large fraction\nof the network weights don't need to be adjusted,",
    "start": "1512680",
    "end": "1519630"
  },
  {
    "text": "they don't adjust the weights. And you are not doing any learning So the learning is slow.",
    "start": "1519830",
    "end": "1525040"
  },
  {
    "start": "1527000",
    "end": "1527000"
  },
  {
    "text": "There are some fixes to this, there are different types\nof functions.",
    "start": "1528280",
    "end": "1533860"
  },
  {
    "text": "There's a piece, the ReLUs function which is the most\npopular activation function.",
    "start": "1533860",
    "end": "1539840"
  },
  {
    "text": "But again, if the neurons are initialized poorly,",
    "start": "1540500",
    "end": "1546940"
  },
  {
    "text": "this function might not fire. it might be zero gradient",
    "start": "1549100",
    "end": "1554559"
  },
  {
    "text": "for the entire data set. Nothing that you produce as input,",
    "start": "1554560",
    "end": "1560940"
  },
  {
    "text": "you run all your thousands\nof images of cats, and none of them fire at all.",
    "start": "1561260",
    "end": "1566720"
  },
  {
    "text": "That's the danger here. So you have to pick",
    "start": "1566720",
    "end": "1572380"
  },
  {
    "start": "1573000",
    "end": "1573000"
  },
  {
    "text": "both the optimization engine, the solver that you use",
    "start": "1573680",
    "end": "1579640"
  },
  {
    "text": "and the activation functions\ncarefully. You can't just plug and play\nlike they're Lego's",
    "start": "1579640",
    "end": "1585780"
  },
  {
    "text": "You have to be aware of the function. SGD, Stochastic Gradient Descent,",
    "start": "1585780",
    "end": "1592920"
  },
  {
    "text": "that's the Vanilla\noptimization algorithm for gradient descent.",
    "start": "1598760",
    "end": "1603980"
  },
  {
    "text": "For optimizing the loss function\nover the gradients And what's visualized here is,",
    "start": "1604620",
    "end": "1610660"
  },
  {
    "text": "again, if you have done\nany numerical optimization, and non-linear optimization,",
    "start": "1610660",
    "end": "1616340"
  },
  {
    "text": "there's the famous saddle point, that's tricky for these\nalgorithms to deal with.",
    "start": "1616340",
    "end": "1622240"
  },
  {
    "text": "What happens is, it's easy\nfor them to oscillate, get stuck in that saddle and\noscillating back and forth",
    "start": "1622700",
    "end": "1629220"
  },
  {
    "text": "as opposed to what they\nwant to do which is go down into-- You get so happy that you found this",
    "start": "1629220",
    "end": "1637179"
  },
  {
    "text": "low point that you forget there's\na much lower point. So you get stuck with the gradient.",
    "start": "1638900",
    "end": "1645100"
  },
  {
    "text": "The momentum of the gradient keeps rocking it back and forth\nwithout you going to a much greater global minimum.",
    "start": "1645100",
    "end": "1652059"
  },
  {
    "text": "And there's a lot of clever\nways to solving that, the Atom optimizer is one of those.",
    "start": "1652060",
    "end": "1658080"
  },
  {
    "start": "1659000",
    "end": "1659000"
  },
  {
    "text": "But in this case, as long as\nthe gradients don't vanish",
    "start": "1660460",
    "end": "1665340"
  },
  {
    "text": "SGD, the Stochastic Gradient Descent, one of these algorithms\nwill get you there It might take a little while,\nbut it will get you there",
    "start": "1666180",
    "end": "1673690"
  },
  {
    "text": "Yes, question. The question was,",
    "start": "1674300",
    "end": "1679160"
  },
  {
    "text": "you're dealing with a function\nthat is not convex, how do we ensure anything about",
    "start": "1680260",
    "end": "1685760"
  },
  {
    "text": "converging to anything that's reasonably good, the local optimum converges to--",
    "start": "1686160",
    "end": "1692100"
  },
  {
    "text": "The answer is, you can't. This isn't only a non-linear function",
    "start": "1692100",
    "end": "1699400"
  },
  {
    "text": "it's a highly non-function The power and the beauty\nof neural networks",
    "start": "1699700",
    "end": "1704920"
  },
  {
    "text": "is that it can represent these arbitrarily complex functions.",
    "start": "1705460",
    "end": "1712419"
  },
  {
    "text": "It's incredible. And it can learn these\nfunctions from data",
    "start": "1712420",
    "end": "1717040"
  },
  {
    "text": "But the reason people are referring to\nneural networks training as art",
    "start": "1718000",
    "end": "1723000"
  },
  {
    "text": "is you're trying to play\nwith parameters that don't get stuck in\nthese local optimal.",
    "start": "1723040",
    "end": "1728480"
  },
  {
    "text": "For stupid reasons\nand for clever reasons. Yes, question.",
    "start": "1728480",
    "end": "1731660"
  },
  {
    "text": "The Question continues\non the same thread.",
    "start": "1733780",
    "end": "1737700"
  },
  {
    "text": "The thing is, we're dealing\nwith functions where we don't know what\nthe global optimal is.",
    "start": "1740340",
    "end": "1746260"
  },
  {
    "text": "That's the crocs of it. Everything we talked about,",
    "start": "1746260",
    "end": "1752440"
  },
  {
    "text": "interpreting text, interpreting video, even driving.",
    "start": "1753100",
    "end": "1758560"
  },
  {
    "text": "What's the optimal for driving? Never crashing?",
    "start": "1758560",
    "end": "1762880"
  },
  {
    "text": "It sounds easy to say that, you actually have to\nformulate the world under which it defines all of those\nthings and it becomes a really",
    "start": "1765260",
    "end": "1773020"
  },
  {
    "text": "non-linear objective function for which you don't know what the optimal is.",
    "start": "1773040",
    "end": "1777240"
  },
  {
    "text": "That's why you keep trying and get impressed\nevery time it gets better. It is essentially the process.",
    "start": "1781160",
    "end": "1787100"
  },
  {
    "text": "And you can also compare, you can compare with\nhuman-level performance. For ImageNet,",
    "start": "1787460",
    "end": "1793360"
  },
  {
    "text": "who can tell the difference\nbetween cats and dogs, and top five categories,",
    "start": "1793370",
    "end": "1798660"
  },
  {
    "text": "96% of the time accuracy, and then you get impressed when a machine can do better than that.",
    "start": "1800500",
    "end": "1806600"
  },
  {
    "text": "But you don't know\nwhat the best is.",
    "start": "1806660",
    "end": "1808460"
  },
  {
    "text": "These videos can be watched for hours, I won't play it until I\nexplain this slide.",
    "start": "1815210",
    "end": "1819600"
  },
  {
    "text": "Let's pause to reflect\non backpropagation before I go on to Recurrent\nNeural Networks. Yes, question.",
    "start": "1821070",
    "end": "1826880"
  },
  {
    "text": "In this practical manner,\nhow can you tell when you're actually creating a net\nwhether you're facing the management\ngradient problem",
    "start": "1826880",
    "end": "1834140"
  },
  {
    "text": "or you need to change your optimizer",
    "start": "1834140",
    "end": "1837780"
  },
  {
    "text": "or you've reached a local minimum? The question was,",
    "start": "1839780",
    "end": "1845059"
  },
  {
    "text": "how do you practically know when you hit the vanishing\ngradient problem?",
    "start": "1845060",
    "end": "1851240"
  },
  {
    "text": "The vanishing gradient could be--",
    "start": "1851600",
    "end": "1853299"
  },
  {
    "start": "1859000",
    "end": "1859000"
  },
  {
    "text": "The derivative being zero\non the gradient, happens when the activation\nis exploding,",
    "start": "1860760",
    "end": "1867240"
  },
  {
    "text": "like really high values and really low values. To really high values is easy. Your network has just gone crazy.",
    "start": "1867240",
    "end": "1874660"
  },
  {
    "text": "It produces very large values. And you can fix a lot of those things\nby just capping the activations.",
    "start": "1874660",
    "end": "1883279"
  },
  {
    "text": "The values being really low, resulting in a vanishing gradient,\nare really hard to detect",
    "start": "1885920",
    "end": "1892240"
  },
  {
    "text": "There's a lot of research in\ntrying to figure out how to detect these things.",
    "start": "1894660",
    "end": "1899880"
  },
  {
    "text": "If you're not careful, often times you can find that,",
    "start": "1900380",
    "end": "1906080"
  },
  {
    "text": "and this isn't hard to do, we're like 40 or 50 percent\nof the network, of the neurons,",
    "start": "1908660",
    "end": "1914560"
  },
  {
    "text": "are dead. We will call it, for ReLU, they're dead ReLU",
    "start": "1915580",
    "end": "1920800"
  },
  {
    "text": "They're not firing at all. How do you detect that? That's part of learning",
    "start": "1920800",
    "end": "1926809"
  },
  {
    "text": "If they never fire you can detect that by running it through\nthe entire training set. There are a lot of tricks. But that's the problem.",
    "start": "1926840",
    "end": "1934150"
  },
  {
    "text": "You try to learn and then you look at the loss function and it's not",
    "start": "1934460",
    "end": "1940260"
  },
  {
    "text": "converging to anything reasonable. They are going all over the place,\nor just converging very slowly.",
    "start": "1940960",
    "end": "1946470"
  },
  {
    "text": "And that's an indication that\nsomething is wrong That something could be\nthe loss function is bad, that something could be\nyou already found the optimal,",
    "start": "1946490",
    "end": "1953410"
  },
  {
    "text": "or that something could be\nthe vanishing gradient. And again, that's why it's an art.",
    "start": "1953420",
    "end": "1957919"
  },
  {
    "text": "Certainly, at least some fraction of the neurons\nneeds to be firing.",
    "start": "1961100",
    "end": "1967960"
  },
  {
    "text": "Otherwise, initialization is\nreally poorly done. Okay, to reflect on the",
    "start": "1969120",
    "end": "1974840"
  },
  {
    "start": "1972000",
    "end": "1972000"
  },
  {
    "text": "simplicity of backpropagation and the power of it,",
    "start": "1975180",
    "end": "1980280"
  },
  {
    "text": "this kind of step of backpropagating the loss function\nto the gradients locally,",
    "start": "1982080",
    "end": "1986909"
  },
  {
    "text": "is the way neural networks learn. It's really the only way",
    "start": "1988120",
    "end": "1993700"
  },
  {
    "text": "that we have effectively been able to to train a neural network",
    "start": "1993700",
    "end": "1999640"
  },
  {
    "text": "network to learn a function. To adjusting the weights and biases, the huge number of weights and biases,\nthe parameters",
    "start": "1999640",
    "end": "2006440"
  },
  {
    "text": "It's just through this optimization. It's backpropagating the error, where you have\nthe supervised ground truth.",
    "start": "2006440",
    "end": "2014500"
  },
  {
    "text": "the question is whether this process, of fitting,",
    "start": "2014580",
    "end": "2018980"
  },
  {
    "text": "adjusting the parameters of a highly non-linear function\nto minimize a single objective,",
    "start": "2021800",
    "end": "2029700"
  },
  {
    "text": "is the way you achieve intelligence.",
    "start": "2029700",
    "end": "2035039"
  },
  {
    "text": "Human-level intelligence. That's something to think about. You have to think about,\nfor driving purposes,",
    "start": "2035040",
    "end": "2040950"
  },
  {
    "text": "what is the limitation\nof this approach? What's not happening? The neural network designed,\nthe architecture",
    "start": "2040980",
    "end": "2048660"
  },
  {
    "text": "is not being adjusted. any of the edges, the layers,\nnothing is being evolved",
    "start": "2048690",
    "end": "2056370"
  },
  {
    "text": "There are other\noptimization approaches that I think are more",
    "start": "2058340",
    "end": "2064080"
  },
  {
    "text": "interesting and inspiring\nthan effective. For example, this is",
    "start": "2067200",
    "end": "2072560"
  },
  {
    "text": "using soft cubes to-- This is falling out of the field",
    "start": "2073700",
    "end": "2080000"
  },
  {
    "text": "of evolutionary robotics. Where you evolve",
    "start": "2080000",
    "end": "2085280"
  },
  {
    "text": "the dynamics of a robot using genetic algorithms and that's",
    "start": "2085580",
    "end": "2090960"
  },
  {
    "text": "These robots have been taught to, in simulation, obviously,",
    "start": "2100020",
    "end": "2104640"
  },
  {
    "text": "to walk and to swim. That one is swimming.",
    "start": "2105520",
    "end": "2109460"
  },
  {
    "text": "The nice thing here is that dynamics that highly non- linear space as well,",
    "start": "2114640",
    "end": "2119740"
  },
  {
    "text": "that controls the dynamics of\nthis weird shaped robot with a lot of degrees of freedom,",
    "start": "2119740",
    "end": "2125980"
  },
  {
    "text": "it's the same kind of thing\nas the neural network. In fact, people have applied\ngeneric algorithms,",
    "start": "2125980",
    "end": "2131400"
  },
  {
    "text": "ant colony optimization, all kinds of\nsort of nature inspire algorithms for automatizing the weights\nand the biases",
    "start": "2132090",
    "end": "2138450"
  },
  {
    "text": "but they don't seem to\ncurrently work that well. It's a cool idea to be using",
    "start": "2138500",
    "end": "2143660"
  },
  {
    "text": "nature-type evolutionary\nalgorithms to evolve something that's already nature\ninspired which is neural networks.",
    "start": "2143660",
    "end": "2150000"
  },
  {
    "text": "But, something to think about the backpropagation,\nwhile really simple",
    "start": "2150000",
    "end": "2157920"
  },
  {
    "text": "it's kind of dumb and\nthe question is whether general intelligence reasoning\ncan be achieved with this process.",
    "start": "2157920",
    "end": "2164049"
  },
  {
    "text": "All right, Recurrent Neural Networks, on the left there's an input X",
    "start": "2164360",
    "end": "2170600"
  },
  {
    "start": "2167000",
    "end": "2167000"
  },
  {
    "text": "with weights on the input, U, there's a hidden state, hidden layer S,",
    "start": "2170980",
    "end": "2177900"
  },
  {
    "text": "with weights on",
    "start": "2178640",
    "end": "2181059"
  },
  {
    "text": "the edge connecting the hidden states to each other and then more weights,\nV, the on the output O.",
    "start": "2185260",
    "end": "2193000"
  },
  {
    "text": "It's a really simple network,\nthere's inputs, there's hidden states,",
    "start": "2193680",
    "end": "2198520"
  },
  {
    "text": "the memory of this network and there's outputs.",
    "start": "2199200",
    "end": "2203760"
  },
  {
    "text": "But the fact that there's this loop where the hidden states are\nconnected to each other",
    "start": "2206260",
    "end": "2213600"
  },
  {
    "text": "means that as opposed to\nproducing a single input, the network takes arbitrary\nnumbers of inputs,",
    "start": "2213600",
    "end": "2220820"
  },
  {
    "text": "it just keeps taking X, one at a time and produces a sequence of Xs",
    "start": "2220820",
    "end": "2226120"
  },
  {
    "text": "through time. Depending on",
    "start": "2226480",
    "end": "2231500"
  },
  {
    "text": "the duration of the sequence\nyou're interested in, you can think of this network in its unrolled state.",
    "start": "2231500",
    "end": "2237900"
  },
  {
    "text": "You can unroll this neural network where the inputs are in the bottom,\nXt-1, Xt, Xt+1,",
    "start": "2238860",
    "end": "2245520"
  },
  {
    "text": "and same with the outputs, Ot-1, Ot, Ot+1,",
    "start": "2245780",
    "end": "2251460"
  },
  {
    "text": "and it becomes like\na regular neural network, unrolled some\narbitrary number of times.",
    "start": "2252360",
    "end": "2258240"
  },
  {
    "start": "2259000",
    "end": "2259000"
  },
  {
    "text": "The parameters, again, there's weights, there's biases, similar to CNNs,",
    "start": "2260300",
    "end": "2266580"
  },
  {
    "text": "convolutional neural networks and just like convolutional\nneural networks make certain spatial\nconsistency assumptions,",
    "start": "2266580",
    "end": "2275339"
  },
  {
    "text": "the recurrent neural network assume temporal consistency\namongst the parameters,",
    "start": "2275620",
    "end": "2280880"
  },
  {
    "text": "shares the parameters. That W, that U, that V, is the same for every single time step.",
    "start": "2280880",
    "end": "2288100"
  },
  {
    "text": "You're learning the same parameter, no matter the duration\nof the sequence",
    "start": "2288100",
    "end": "2293740"
  },
  {
    "text": "and that allows you to look at arbitrary\nlong sequences",
    "start": "2294360",
    "end": "2299080"
  },
  {
    "text": "without having an\nexplosion of parameters. ",
    "start": "2299760",
    "end": "2302860"
  },
  {
    "text": "This process is the same exact\nprocess that's repeated base on the different variants\nthat we talk about before,",
    "start": "2309480",
    "end": "2315260"
  },
  {
    "text": "in terms of inputs and outputs, one to many, many to one,\nmany to many.",
    "start": "2315260",
    "end": "2319180"
  },
  {
    "start": "2319000",
    "end": "2319000"
  },
  {
    "text": "The backpropagation process is exactly the same as\nfor regular neural networks.",
    "start": "2320380",
    "end": "2325640"
  },
  {
    "text": "It's a fancy name of\nbackpropagation through time, BPTT,",
    "start": "2325640",
    "end": "2330440"
  },
  {
    "text": "but it's just backpropagation\nthrough an unrolled",
    "start": "2331020",
    "end": "2334980"
  },
  {
    "text": "recurrent neural network, where the errors are on\nthe computed on the outputs,",
    "start": "2338700",
    "end": "2343820"
  },
  {
    "text": "the gradients are computed, backpropagated",
    "start": "2344600",
    "end": "2349280"
  },
  {
    "text": "and computed on the inputs, again, suffering for\nthe same exact problem",
    "start": "2350200",
    "end": "2355580"
  },
  {
    "text": "of vanishing gradients. The problem is that the depth of these networks\ncan be arbitrary long",
    "start": "2356460",
    "end": "2363840"
  },
  {
    "text": "if at any point the gradients hits a lower number, zero,",
    "start": "2363840",
    "end": "2368700"
  },
  {
    "text": "becomes, that neural becomes saturated. That gradient, let's call it saturated,",
    "start": "2369460",
    "end": "2374480"
  },
  {
    "text": "that gradient gets-- drives all the earlier layer to zero,",
    "start": "2374540",
    "end": "2380240"
  },
  {
    "text": "so is easy to run to a problem where you're really ignoring\nthe majority of the sequence.",
    "start": "2381240",
    "end": "2386680"
  },
  {
    "text": "This is just another Python weight, sudo-called weight to look at it.",
    "start": "2387880",
    "end": "2393660"
  },
  {
    "text": "Is you have the same w, remember you're sharing the weights and all the parameters\nfrom time to time,",
    "start": "2394040",
    "end": "2400880"
  },
  {
    "text": "so if the weights are such WHH,",
    "start": "2401340",
    "end": "2407339"
  },
  {
    "text": "if the weights are such\nthat they produce [unintelligible]",
    "start": "2407400",
    "end": "2411700"
  },
  {
    "text": "they have a negative value\nthat results in the gradient that goes to zero,",
    "start": "2414120",
    "end": "2421359"
  },
  {
    "text": "that propagates through the rest. That's the sudo-call for backpropagation,",
    "start": "2422080",
    "end": "2427140"
  },
  {
    "text": "pass to the RNN, that WHH propagates back.",
    "start": "2427140",
    "end": "2433500"
  },
  {
    "start": "2434000",
    "end": "2434000"
  },
  {
    "text": "You get this things with exploding and\nvanishing gradients",
    "start": "2435580",
    "end": "2439140"
  },
  {
    "text": "for example, error surfaces for\na single hidden unit RNN, this is visualizing the gradient,",
    "start": "2440660",
    "end": "2447680"
  },
  {
    "text": "the value of the weight,\nthe value of the bias and the error, the error could be really flat\nor could explode,",
    "start": "2447680",
    "end": "2455500"
  },
  {
    "text": "both are going to lead to you not making--",
    "start": "2456740",
    "end": "2462080"
  },
  {
    "text": "either making steps that\nare too gradual or too big. It's the geometric interpretation.",
    "start": "2462220",
    "end": "2468680"
  },
  {
    "start": "2468000",
    "end": "2468000"
  },
  {
    "text": "Okay. What other variants that\nwe look at, a little bit? are they [unintelligible 00:41:13]? It doesn't have to be only one way,",
    "start": "2468680",
    "end": "2475360"
  },
  {
    "text": "it can be bi-directional, that could be edges going forward\nand edges going back",
    "start": "2475380",
    "end": "2480420"
  },
  {
    "text": "What that's needed for is things like filling in missing,\nwhatever the data is,",
    "start": "2481080",
    "end": "2487769"
  },
  {
    "text": "filling in missing elements of that data, whether that's images, or words,\nor audio.",
    "start": "2487780",
    "end": "2493730"
  },
  {
    "text": "Generally, as always is the case\nin neural network, the deeper it goes, the better.",
    "start": "2494820",
    "end": "2499400"
  },
  {
    "text": "That deep referring to\nthe number of layers in a single temporal instance.",
    "start": "2500740",
    "end": "2508079"
  },
  {
    "text": "On the right of the slide we're stacking",
    "start": "2508080",
    "end": "2512780"
  },
  {
    "text": "node in the temporal domain. Each of those layers\nhas its own set of weights,",
    "start": "2514700",
    "end": "2522579"
  },
  {
    "text": "its own set of biases. These things are awesome but they need a lot of data",
    "start": "2522640",
    "end": "2528540"
  },
  {
    "start": "2528000",
    "end": "2528000"
  },
  {
    "text": "when you add extra layers in this way. The problem is, while\nrecurrent neural network,",
    "start": "2532840",
    "end": "2540099"
  },
  {
    "text": "in theory, is supposed to be able to learn\nany kind of sequence, the reality is they're not really\ngood at remembering",
    "start": "2540380",
    "end": "2548310"
  },
  {
    "text": "what happened a while ago, the long-term dependency. Here's a silly example,",
    "start": "2548360",
    "end": "2554740"
  },
  {
    "text": "let's think of a story about Bob, Bob is eating an apple.",
    "start": "2555040",
    "end": "2561540"
  },
  {
    "text": "The apple part is generated by\nthe recurrent neural network.",
    "start": "2562540",
    "end": "2567140"
  },
  {
    "text": "Your recurrent neural networks\ncan learn to generate \"apple\" because it's seen in a lot of sentences,\nwith \"Bob\" and \"eating\"",
    "start": "2570340",
    "end": "2576400"
  },
  {
    "text": "and it can generate the word apple. For a longer sentence, like",
    "start": "2576420",
    "end": "2581720"
  },
  {
    "text": "\"Bob likes apples, he's hungry and decided to have a snack, so now he's eating an apple\",",
    "start": "2581720",
    "end": "2586860"
  },
  {
    "text": "you have to maintain the state that we're talking about Bob and we're talking about apples,",
    "start": "2587440",
    "end": "2593240"
  },
  {
    "text": "through several discreet semantic",
    "start": "2593240",
    "end": "2597480"
  },
  {
    "text": "sentences. That kind of long-term memory is not--",
    "start": "2598780",
    "end": "2604460"
  },
  {
    "text": "because of different effects, but vanishing gradients,",
    "start": "2605520",
    "end": "2609920"
  },
  {
    "text": "it's difficult to propagate the important stuff\nthat happened a while ago in order to maintain that context",
    "start": "2610920",
    "end": "2617920"
  },
  {
    "text": "in generating \"apple\", or classifying some concept\nthat happened way down the line. ",
    "start": "2617920",
    "end": "2622880"
  },
  {
    "text": "When people talk about recurrent neural networks",
    "start": "2625260",
    "end": "2631020"
  },
  {
    "start": "2629000",
    "end": "2629000"
  },
  {
    "text": "these days, they're talking about LSTMs, long-short-term memory networks",
    "start": "2631240",
    "end": "2639240"
  },
  {
    "text": "so all the impressive results results on time series\nand audio and video and all that, that requires LSTMs.",
    "start": "2640400",
    "end": "2647380"
  },
  {
    "text": "Again, vanilla RNNs are on top of the slide,",
    "start": "2647860",
    "end": "2651200"
  },
  {
    "text": "each cell is simple, there are some hidden units, there's an input, and there's an output.",
    "start": "2653680",
    "end": "2660520"
  },
  {
    "text": "Here, we used TANH\nas activation function,",
    "start": "2661760",
    "end": "2665280"
  },
  {
    "text": "it's just another popular\nSigmoid type activation function.",
    "start": "2668360",
    "end": "2672880"
  },
  {
    "text": "LSTMs are more complicated, or they look more complicated but",
    "start": "2675100",
    "end": "2680920"
  },
  {
    "text": "in some ways, they're more intuitive\nfor us to understand. There's a bunch of gates in each cell,",
    "start": "2682580",
    "end": "2688579"
  },
  {
    "text": "we'll go through those. In yellow are different\nneural network layers,",
    "start": "2689620",
    "end": "2694260"
  },
  {
    "text": "Sigmoid and TANH, are just different types\nof activation functions.",
    "start": "2694840",
    "end": "2700420"
  },
  {
    "text": "TANH is an activation function that squishes the input into\nthe range of negative one to one.",
    "start": "2700480",
    "end": "2707700"
  },
  {
    "text": "Sigmoid function squishes it between zero and one and that serve different purposes.",
    "start": "2708840",
    "end": "2715540"
  },
  {
    "text": "There's some pointwise operations, addition, multiplication,",
    "start": "2715920",
    "end": "2721060"
  },
  {
    "text": "and there's connections, data being passed from layer to layer,",
    "start": "2721940",
    "end": "2728240"
  },
  {
    "text": "shown by the arrows. There's concatenation and there's\na copy operation on the output",
    "start": "2728680",
    "end": "2735420"
  },
  {
    "text": "We copy, the output of each cell\nit's copied to the next cell and to the output.",
    "start": "2735420",
    "end": "2741480"
  },
  {
    "start": "2743000",
    "end": "2743000"
  },
  {
    "text": "Let me try to make it, clarified,",
    "start": "2743060",
    "end": "2745760"
  },
  {
    "start": "2748000",
    "end": "2748000"
  },
  {
    "text": "clarify a little bit. There's this conveyer belt",
    "start": "2750480",
    "end": "2756840"
  },
  {
    "text": "going through inside of\neach individual cell and they all have, there's really\nthree steps in the conveyer belt.",
    "start": "2757180",
    "end": "2765240"
  },
  {
    "text": "The first is, there is a Sigmoid function",
    "start": "2765280",
    "end": "2770480"
  },
  {
    "text": "that's responsible for deciding",
    "start": "2770480",
    "end": "2773260"
  },
  {
    "text": "what to forget and what to ignore, it's responsible for",
    "start": "2775820",
    "end": "2781280"
  },
  {
    "text": "taking in the input, the new input, x(t), taking in the state of the previous,",
    "start": "2782340",
    "end": "2790700"
  },
  {
    "text": "the output of the previous cell,\nprevious time step and deciding \"do I want to keep\nthat in my memory or not?\"",
    "start": "2791660",
    "end": "2800000"
  },
  {
    "text": "and \"do I want to integrate the new input into my memory or not?\" This allows you to",
    "start": "2800000",
    "end": "2805900"
  },
  {
    "text": "selective about the information\nwhich you learn. For example, there's that sentence\n\"Bob and Alice are having lunch,",
    "start": "2805980",
    "end": "2813740"
  },
  {
    "text": "Bob likes apples, Alice like oranges, she is eating an orange\".",
    "start": "2813820",
    "end": "2818480"
  },
  {
    "text": "Bob and Alice are having lunch, Bob likes apples, right now, if you had said\nyou have a hidden state,",
    "start": "2822880",
    "end": "2830119"
  },
  {
    "text": "keeping track of the gender\nof the person we're talking about",
    "start": "2830240",
    "end": "2833560"
  },
  {
    "text": "you might say that there's both genders\non the first sentence, there's male in the second sentence,",
    "start": "2836000",
    "end": "2841040"
  },
  {
    "text": "female in the third sentence, and that way when you have to generate a sentence\nabout who's eating what,",
    "start": "2841060",
    "end": "2847940"
  },
  {
    "text": "you'll know- you keep the gender information in order to make an\naccurate generation of text",
    "start": "2847940",
    "end": "2856080"
  },
  {
    "text": "corresponding to the proper person. You have to forget certain things,",
    "start": "2856120",
    "end": "2861880"
  },
  {
    "text": "like forget that Bob existed\nat that moment, you have to forget Bob likes apples",
    "start": "2861920",
    "end": "2869160"
  },
  {
    "text": "but you have to remember that Alice likes oranges so you have to selectively remember\nand forget certain things",
    "start": "2869560",
    "end": "2877500"
  },
  {
    "text": "that's LSTM in a nutshell. You decided what to forget,\ndecided what to remember",
    "start": "2877500",
    "end": "2883880"
  },
  {
    "text": "and decided what to output\nin that cell.",
    "start": "2883940",
    "end": "2886780"
  },
  {
    "start": "2888000",
    "end": "2888000"
  },
  {
    "text": "Zoom in a little bit,\nbecause this is pretty cool There's a state running\nthrough the cell,",
    "start": "2891620",
    "end": "2899539"
  },
  {
    "text": "this conveyer belt, previous state like the gender",
    "start": "2900240",
    "end": "2905140"
  },
  {
    "text": "that we're currently talking about, that's the state that you're\nkeeping track of",
    "start": "2905920",
    "end": "2911100"
  },
  {
    "text": "and that's running through the cell. Then there's three Sigmoid layers",
    "start": "2911180",
    "end": "2916579"
  },
  {
    "text": "outputting one, a number between the zero and one,",
    "start": "2917180",
    "end": "2922480"
  },
  {
    "text": "one when you want that\ninformation to go through and zero when you\ndon't want it to go through,",
    "start": "2922480",
    "end": "2929660"
  },
  {
    "text": "the conveyer belt\nthat maintains the state.",
    "start": "2931060",
    "end": "2934260"
  },
  {
    "text": "First, Sigmoid function is, we decided what to forget\nand what to ignore,",
    "start": "2936100",
    "end": "2942059"
  },
  {
    "text": "that's the first one, we take the input from\nthe previous time step, the input to the network",
    "start": "2942060",
    "end": "2949160"
  },
  {
    "text": "on the current time step and decided, do I want to forget\nor do I want to ignore those?",
    "start": "2949180",
    "end": "2954420"
  },
  {
    "text": "Then we decided which part of the state to update,",
    "start": "2955560",
    "end": "2960820"
  },
  {
    "text": "what part of our memory do we have\nto update with this information and what values to insert in that update.",
    "start": "2961100",
    "end": "2968120"
  },
  {
    "text": "Third step is, we perform\nthe actual update and perform the actual forgetting,",
    "start": "2970360",
    "end": "2976540"
  },
  {
    "text": "that's why you have\nthe Sigmoid function, you just multiply it,",
    "start": "2976980",
    "end": "2981460"
  },
  {
    "text": "when is zero is forgetting, when is one that information passes through.",
    "start": "2982480",
    "end": "2987059"
  },
  {
    "text": "Finally, we produce an output from the cell,",
    "start": "2989300",
    "end": "2993860"
  },
  {
    "text": "if its translation is producing an output\nin the English language",
    "start": "2995180",
    "end": "3001300"
  },
  {
    "text": "where the input was\nin Spanish language and then that same output",
    "start": "3001340",
    "end": "3006020"
  },
  {
    "text": "it's copied to the next cell.",
    "start": "3006460",
    "end": "3008940"
  },
  {
    "start": "3012000",
    "end": "3012000"
  },
  {
    "text": "What can we get done with this\nkind of approach? We can look at machine translation.",
    "start": "3014060",
    "end": "3020420"
  },
  {
    "text": "I guess what I'm trying to-- question. what is your representation\nof this state?",
    "start": "3020460",
    "end": "3027600"
  },
  {
    "text": "Is it like a floating point or is it like a vector or what is it, exactly?",
    "start": "3027600",
    "end": "3032440"
  },
  {
    "text": "The state is the activation",
    "start": "3033680",
    "end": "3038600"
  },
  {
    "text": "multiplied by the weight, it's the output of the Sigmoid or\nthe TANH activations.",
    "start": "3040240",
    "end": "3045900"
  },
  {
    "text": "There's a bunch of neurons\nand they're firing a number between negative one or one,\nor between zero and one,",
    "start": "3047320",
    "end": "3052960"
  },
  {
    "text": "that whole's a state. It just that calling it a state\nit's sort of simplifying, but the point is that there's",
    "start": "3053320",
    "end": "3059520"
  },
  {
    "text": "a bunch of numbers been constantly\nmodified by the weights and the biases,",
    "start": "3059520",
    "end": "3064480"
  },
  {
    "text": "those numbers hold the state and the modification\nof those numbers",
    "start": "3065420",
    "end": "3071240"
  },
  {
    "text": "is controlled by the weights and then once all of that is done,",
    "start": "3071340",
    "end": "3076260"
  },
  {
    "text": "the resulting output of the recurrent neural network it's compared to the desired output",
    "start": "3076600",
    "end": "3082580"
  },
  {
    "text": "and the errors are backpropagated\nto the weights.",
    "start": "3082760",
    "end": "3085540"
  },
  {
    "text": "Hopefully, that makes sense.  So, machine translation is one\npopular application",
    "start": "3087840",
    "end": "3094300"
  },
  {
    "text": "all of it is the same, all of these networks\nthat I've talked about,",
    "start": "3097200",
    "end": "3102520"
  },
  {
    "text": "they're really similar constructs. You have some inputs,",
    "start": "3102560",
    "end": "3108460"
  },
  {
    "text": "whatever language that is again, German maybe,\nI think everything is German,",
    "start": "3108980",
    "end": "3115660"
  },
  {
    "text": "and the output. The inputs are in one language, a set of characters",
    "start": "3118680",
    "end": "3125700"
  },
  {
    "text": "composed a word in one language, there's a state being propagated and once that sentence is over,",
    "start": "3125760",
    "end": "3132300"
  },
  {
    "text": "you start, as opposed\nto collecting inputs, start producing outputs and you can output in the English language.",
    "start": "3132340",
    "end": "3138600"
  },
  {
    "start": "3139000",
    "end": "3139000"
  },
  {
    "text": "There's a ton of great work on\nmachine translations. It's what Google is supposedly using\nfor their translator,",
    "start": "3139820",
    "end": "3146680"
  },
  {
    "text": "same thing. I've show this previously but now you all know how it works,",
    "start": "3146680",
    "end": "3152680"
  },
  {
    "text": "same exact thing, LSTMs generating handwritten characters,",
    "start": "3152700",
    "end": "3157800"
  },
  {
    "text": "handwriting in arbitrary styles, controlling the drawing,",
    "start": "3157800",
    "end": "3162060"
  },
  {
    "text": "where the input is text\nand the output is handwriting. Is again, the same kind of",
    "start": "3163780",
    "end": "3169059"
  },
  {
    "text": "network with some depths here, the input is the text,",
    "start": "3170220",
    "end": "3175280"
  },
  {
    "text": "the output is the control\nof the writing. Character-level text generation,",
    "start": "3175400",
    "end": "3182000"
  },
  {
    "start": "3179000",
    "end": "3179000"
  },
  {
    "text": "this is the thing that taught us about life, the meaning of life,",
    "start": "3182960",
    "end": "3189059"
  },
  {
    "text": "literary recognition and the tradition\nof ancient human reproduction. That's again, the same process,",
    "start": "3189120",
    "end": "3196340"
  },
  {
    "text": "input one character at the time, what we see there is the encoding\nof the characters on the input layer,",
    "start": "3196680",
    "end": "3203579"
  },
  {
    "text": "there's a hidden state, hidden layer that is keeping track\nof those activations,",
    "start": "3203860",
    "end": "3208950"
  },
  {
    "text": "the outputs of the activation functions\nand every single",
    "start": "3209000",
    "end": "3215140"
  },
  {
    "text": "time it's outputting its best prediction",
    "start": "3217620",
    "end": "3222700"
  },
  {
    "text": "of the next character that follows. Now, on a lot of these applications you want to ignore the output",
    "start": "3222780",
    "end": "3229220"
  },
  {
    "text": "until the input sentence is over and then you start listening\nto the output,",
    "start": "3229420",
    "end": "3235860"
  },
  {
    "text": "but the point is that it just\nkeeps generating text, whether is given an input or not, so you producing input",
    "start": "3235860",
    "end": "3242280"
  },
  {
    "text": "is just adding, steering the recurrent neural network. You can answer questions",
    "start": "3242360",
    "end": "3248800"
  },
  {
    "start": "3246000",
    "end": "3246000"
  },
  {
    "text": "about an image, the input you get there, you could almost arbitrary\nstack things together,",
    "start": "3251480",
    "end": "3258060"
  },
  {
    "text": "you take an image as your input,\nbottom left there, put it in your convolutional neural network,",
    "start": "3258100",
    "end": "3264460"
  },
  {
    "text": "and take the question. There's something call\nword embeddings,",
    "start": "3266640",
    "end": "3273400"
  },
  {
    "text": "it's to broaden the representative\nmeaning of the words. \"How many books?\" is the question.",
    "start": "3273460",
    "end": "3280160"
  },
  {
    "text": "You want to take the word embeddings and the image and produce your best estimate of the answer.",
    "start": "3280220",
    "end": "3286920"
  },
  {
    "text": "For question of \"what color is the cat?\" it could be gray or black, it's the different LSTM flavors",
    "start": "3286980",
    "end": "3294300"
  },
  {
    "text": "producing that answer. Same with counting chairs you can give an image of a chair",
    "start": "3294500",
    "end": "3300100"
  },
  {
    "text": "and as the question\n\"how many chairs are there?\" And it can produce an answer of \"three\".",
    "start": "3300780",
    "end": "3306540"
  },
  {
    "text": "I should say this is really hard, arbitrary question asks an arbitrary image,",
    "start": "3308420",
    "end": "3314200"
  },
  {
    "text": "you are both interpreting-- you are doing natural languages processing and you're doing computer vision,\nall in one network.",
    "start": "3314200",
    "end": "3320579"
  },
  {
    "start": "3321000",
    "end": "3321000"
  },
  {
    "text": "Same thing with\nthe image capture generation, you can detect",
    "start": "3322340",
    "end": "3328640"
  },
  {
    "text": "the different objects in the scene, generate those words, stitch them together\nin syntactically correct sentences",
    "start": "3328760",
    "end": "3337760"
  },
  {
    "text": "and rearrange the sentences. All of those are LSTMs, the second and the third step,",
    "start": "3337840",
    "end": "3343470"
  },
  {
    "text": "the first is computer vision\ndetecting the objects, segmenting the image and\ndetecting the objects,",
    "start": "3343490",
    "end": "3348690"
  },
  {
    "text": "that way you can generate\na caption that says \"a man is sitting in a chair\nwith a dog in his lap\".",
    "start": "3348700",
    "end": "3354300"
  },
  {
    "start": "3356000",
    "end": "3356000"
  },
  {
    "text": "Again, LSTMs for video. Caption generation for video,",
    "start": "3356900",
    "end": "3362440"
  },
  {
    "text": "the input, and every frame it's an image that goes into the LSTM,",
    "start": "3363200",
    "end": "3368500"
  },
  {
    "text": "the input is an image and the output is a set of characters.",
    "start": "3368560",
    "end": "3373660"
  },
  {
    "text": "First, you load in the video, in this case the output is on top, you encode",
    "start": "3373760",
    "end": "3379260"
  },
  {
    "text": "the video into a representation\ninside the network and then you start generating words",
    "start": "3379880",
    "end": "3386660"
  },
  {
    "text": "about that video. First comes the input, the encoding\nstage,  then the decoding stage.",
    "start": "3386720",
    "end": "3391600"
  },
  {
    "text": "Take in the video, say a man is taking, talking, whatever",
    "start": "3392700",
    "end": "3397140"
  },
  {
    "text": "and because the input and\nthe output are arbitrary, there also has to be indicators\nof the beginnings and",
    "start": "3398020",
    "end": "3404600"
  },
  {
    "text": "the ends of a sentence, in this case, end of sentences. You want to know when you stop",
    "start": "3404680",
    "end": "3410400"
  },
  {
    "start": "3410000",
    "end": "3410000"
  },
  {
    "text": "in order to generate syntactically\ncorrect sentences. that indicates the end of a sentence.\nYou want also to be able\nto generate a period",
    "start": "3411520",
    "end": "3417400"
  },
  {
    "text": "You can also, again,\nrecurrent neural networks, LSTMs here, controlling",
    "start": "3421500",
    "end": "3427400"
  },
  {
    "text": "the steering of a sliding window on an image",
    "start": "3427800",
    "end": "3434460"
  },
  {
    "text": "that is used to classify\nwhat is contained in that image. Here, a CNN being steered by\na recurrent neural network",
    "start": "3435360",
    "end": "3443560"
  },
  {
    "text": "in order to convert this imagen into the number that's associated\nwith a house number,",
    "start": "3445080",
    "end": "3452000"
  },
  {
    "start": "3453000",
    "end": "3453000"
  },
  {
    "text": "it's called visual attention. That visual attention\ncan be used to steer for the perception side",
    "start": "3453300",
    "end": "3459800"
  },
  {
    "text": "and it can be used to steer\na network for the generation. On the right, we can generate an image as--",
    "start": "3459900",
    "end": "3467120"
  },
  {
    "text": "So the output of the network-- it's a LSTM where the output on every time step",
    "start": "3470700",
    "end": "3476940"
  },
  {
    "text": "is visual, and this way you can draw numbers.",
    "start": "3477640",
    "end": "3482260"
  },
  {
    "start": "3483000",
    "end": "3483000"
  },
  {
    "text": "Here, I mention this before,",
    "start": "3486080",
    "end": "3491280"
  },
  {
    "text": "is taking in as input silent video, sequence of images",
    "start": "3492780",
    "end": "3497240"
  },
  {
    "text": "and producing audio. This is",
    "start": "3499580",
    "end": "3503920"
  },
  {
    "text": "an LSTM that has convolutional layers\nfor every single frame,",
    "start": "3505280",
    "end": "3511839"
  },
  {
    "text": "takes images as input and produces",
    "start": "3512760",
    "end": "3516940"
  },
  {
    "text": "a spectrogram, audio as output.",
    "start": "3517820",
    "end": "3520460"
  },
  {
    "text": "The training set is a person hitting\nan object with a drumstick and your task is to generate,\ngiven a silent video,",
    "start": "3525780",
    "end": "3533579"
  },
  {
    "text": "generate the sound that\nthe drumstick will make when in contact with that object.",
    "start": "3533740",
    "end": "3540260"
  },
  {
    "start": "3540000",
    "end": "3540000"
  },
  {
    "text": "Medical diagnosis, that's actually-- I've listed some places\nwhere it has been really successful",
    "start": "3543040",
    "end": "3550470"
  },
  {
    "text": "and pretty cool, but it's also beginning to be applied in places where",
    "start": "3550500",
    "end": "3555580"
  },
  {
    "text": "can actually really help",
    "start": "3556660",
    "end": "3561000"
  },
  {
    "text": "civilization, in medical applications. For medical diagnosis",
    "start": "3562320",
    "end": "3567839"
  },
  {
    "text": "there's the highly spars and",
    "start": "3568940",
    "end": "3573900"
  },
  {
    "text": "variable lengths sequence of information in the form of,",
    "start": "3575780",
    "end": "3581020"
  },
  {
    "text": "for example, patient\nelectronic health records. So, Every time you visit a doctor, there's a test being done,\nthat information is there",
    "start": "3581080",
    "end": "3588340"
  },
  {
    "text": "and you can look it as a sequence\nover a period of time and then given that data,\nthat's the input,",
    "start": "3588400",
    "end": "3594680"
  },
  {
    "text": "the output is the diagnosis, a medical diagnosis,",
    "start": "3595240",
    "end": "3600240"
  },
  {
    "text": "in this case, we can look at\npredicting diabetes, scoliosis, asthma and so on,",
    "start": "3600300",
    "end": "3607279"
  },
  {
    "text": "with pretty good accuracy. There's something that",
    "start": "3609140",
    "end": "3614960"
  },
  {
    "text": "all of us wish we could do, is stock market prediction.",
    "start": "3615740",
    "end": "3622780"
  },
  {
    "text": "You can input, for example, well first of all,\nyou can input the raw stock data,",
    "start": "3625580",
    "end": "3630680"
  },
  {
    "text": "[unintelligible 01:00:30] books\nand so on, financial data, but you can also look at news articles\nfrom all over the web",
    "start": "3630680",
    "end": "3637300"
  },
  {
    "text": "and take those as input as shown here, on the X axis is time, articles from different days,",
    "start": "3638240",
    "end": "3644120"
  },
  {
    "text": "LSTM, once again, and produce an output\nof your prediction,",
    "start": "3645620",
    "end": "3651740"
  },
  {
    "text": "binary prediction, whether\nthe stock would go up or down. Nobody has been able to\nreally successfully do this",
    "start": "3652040",
    "end": "3659500"
  },
  {
    "text": "but there is a bunch of results and trying to perform above random",
    "start": "3659540",
    "end": "3664759"
  },
  {
    "text": "which is how you make money, significantly above random",
    "start": "3666840",
    "end": "3672220"
  },
  {
    "text": "on the prediction of\nit's going up or down? So you could buy or sell and especially",
    "start": "3672340",
    "end": "3678020"
  },
  {
    "text": "when there is-- in the cases when there was crashes\nit's easier to predict,",
    "start": "3678280",
    "end": "3682560"
  },
  {
    "text": "so you can predict\nan encroaching crash. These are shown in the table, the error rates from different stocks,",
    "start": "3683380",
    "end": "3690400"
  },
  {
    "text": "automotive stocks. You can also generate audio,",
    "start": "3691740",
    "end": "3698339"
  },
  {
    "text": "is the exact same process\nas it generates language, you generate audio. Here's trained on",
    "start": "3698340",
    "end": "3704280"
  },
  {
    "text": "a single speaker, a few hours epics",
    "start": "3705880",
    "end": "3710920"
  },
  {
    "text": "of them speaking and you just learn,\nthat's raw audio of the speaker",
    "start": "3710980",
    "end": "3717060"
  },
  {
    "text": "and it's learning slowly to generate [audio]",
    "start": "3718920",
    "end": "3727940"
  },
  {
    "text": "Obviously, they were reading numbers.",
    "start": "3739860",
    "end": "3742660"
  },
  {
    "text": "this is incredible, this is trained on a compress spectrogram\nof the audio, raw audio",
    "start": "3746140",
    "end": "3754119"
  },
  {
    "text": "and is producing something that over just a few epics is producing\nsomething that sounds like words,",
    "start": "3755760",
    "end": "3763340"
  },
  {
    "text": "it could do this lecture for me, I wish.",
    "start": "3763340",
    "end": "3765860"
  },
  {
    "text": "This is amazing, this is raw input, raw output,",
    "start": "3779840",
    "end": "3785840"
  },
  {
    "text": "all again, LSTMs, and there's a lot of work\nin voice recognition,",
    "start": "3786100",
    "end": "3793080"
  },
  {
    "text": "audio recognition. You're mapping--",
    "start": "3793120",
    "end": "3795760"
  },
  {
    "text": "let me turn it up. You are mapping any kind of audio\nto a classification,",
    "start": "3800660",
    "end": "3805540"
  },
  {
    "text": "you can take the audio of the road",
    "start": "3809640",
    "end": "3812200"
  },
  {
    "text": "and that's the spectrogram\non the bottom there, being shown you could detect whether\nthe road is wet",
    "start": "3815680",
    "end": "3822240"
  },
  {
    "text": "is wet or the road is dry.",
    "start": "3822300",
    "end": "3824020"
  },
  {
    "text": "you could do the same thing for recognizing the gender\nof the speaker",
    "start": "3827640",
    "end": "3834619"
  },
  {
    "text": "or recognizing many to many map of the actual words\nbeing spoken,",
    "start": "3834700",
    "end": "3839940"
  },
  {
    "text": "speech recognition. This is about driving, so let's see where recurrent neural|\nnetworks apply in driving.",
    "start": "3840200",
    "end": "3847800"
  },
  {
    "text": "We talked about the NVIDIA approach, the thing that actually powers\nDeepTeslaJS,",
    "start": "3848580",
    "end": "3856260"
  },
  {
    "text": "it is a simple convolutional neural network, there's five convolutional layers",
    "start": "3856280",
    "end": "3861460"
  },
  {
    "text": "in their approach, three fully\nconnected layers, you can add as many layers\nas you want in DeepTesla,",
    "start": "3861500",
    "end": "3868320"
  },
  {
    "text": "that's a quarter of million parameters to optimize",
    "start": "3869380",
    "end": "3875000"
  },
  {
    "text": "all you are taking is a single image, no temporal information,\nsingle image and producing the steering angle,\nthat's the approach,",
    "start": "3875020",
    "end": "3882160"
  },
  {
    "text": "that's the DeepTesla way,",
    "start": "3882190",
    "end": "3884359"
  },
  {
    "text": "taking a single imagen image and learning a regression\nof the steering angle.",
    "start": "3887500",
    "end": "3893800"
  },
  {
    "text": "One of the prizes for the competition is\nthe Udacity, self-driving",
    "start": "3895280",
    "end": "3901319"
  },
  {
    "text": "car engineer nanodegree for free, this thing is awesome,",
    "start": "3901440",
    "end": "3907619"
  },
  {
    "text": "I encourage everyone to check it out, but they did a competition",
    "start": "3907720",
    "end": "3911480"
  },
  {
    "text": "that's very similar to ours, but a very large group\nof obsessed people,",
    "start": "3915080",
    "end": "3922820"
  },
  {
    "text": "they were very clever,\nthey went beyond convolutional neural networks\nof predicting steering,",
    "start": "3923690",
    "end": "3928800"
  },
  {
    "text": "taking a sequence of images\nand predicting steering, what they did is, the winners,",
    "start": "3928820",
    "end": "3934660"
  },
  {
    "text": "at least the first and I'll talk about\nthe second place winner tomorrow,",
    "start": "3934660",
    "end": "3939180"
  },
  {
    "text": "on 3D convolutional neural networks, the first and the third place winners\nused RNNs,",
    "start": "3939820",
    "end": "3946380"
  },
  {
    "text": "used LSTMs, recurrent neural networks and map a sequence of images",
    "start": "3946420",
    "end": "3953660"
  },
  {
    "text": "to a sequence of steering angles. For anyone, statistically speaking,",
    "start": "3953680",
    "end": "3960160"
  },
  {
    "text": "anybody here who is\nnot a computer vision person, most likely what'd you want to use,\nfor whatever application",
    "start": "3960440",
    "end": "3966580"
  },
  {
    "text": "you're interested in, is RNNs, the world is full of time series data,",
    "start": "3966580",
    "end": "3972700"
  },
  {
    "text": "very few of us are working on data that is no time series data,",
    "start": "3973200",
    "end": "3978680"
  },
  {
    "text": "in fact, whenever it's just snapshots, you're really just reducing\nthe problem to",
    "start": "3978740",
    "end": "3984460"
  },
  {
    "text": "the size that you can handle but most data in the world is time series data.",
    "start": "3984460",
    "end": "3989800"
  },
  {
    "text": "This is the approach you end up using if you want to apply it\nin your own research,",
    "start": "3989860",
    "end": "3996000"
  },
  {
    "text": "RNNs is the way to go.",
    "start": "4001320",
    "end": "4003300"
  },
  {
    "text": "Again, what are they doing? How do you put images",
    "start": "4006700",
    "end": "4013700"
  },
  {
    "text": "into a recurrent neural network? it's the same thing, you take,",
    "start": "4013780",
    "end": "4019779"
  },
  {
    "text": "you have to convert\nan image into numbers in some kind of way, a powerful way of doing that\nis convolutional neural networks,",
    "start": "4020300",
    "end": "4027980"
  },
  {
    "text": "so you can take either 3D convolutional\nneural networks",
    "start": "4028040",
    "end": "4033060"
  },
  {
    "text": "or 2D convolutional neural networks once it takes time into\nconsideration and whatnot,",
    "start": "4033100",
    "end": "4038740"
  },
  {
    "text": "process that image to extract a representation\nof that image and that becomes the input\nto the LSTM",
    "start": "4038800",
    "end": "4046880"
  },
  {
    "text": "and the output at every single cell,\nat every single timestep, is a predicted steering angle,",
    "start": "4047220",
    "end": "4053019"
  },
  {
    "text": "the speed of the vehicle\nand the torque that's what the first place winner did, they didn't just do the steering angle,",
    "start": "4053050",
    "end": "4058990"
  },
  {
    "text": "also did the speed and torque and the sequence length\nthat they were using",
    "start": "4059030",
    "end": "4064720"
  },
  {
    "text": "for training and for testing, for the input and the output, is a sequence length of 10 ",
    "start": "4065300",
    "end": "4071320"
  },
  {
    "text": "did they used supervised learning or did they used reinforcement\nlearning? The question was, did they used\nsupervised learning?",
    "start": "4072860",
    "end": "4080740"
  },
  {
    "text": "Yes, they were given the same thing\nas in DeepTesla, a sequence of frames\nwhere the have a sequence of",
    "start": "4080800",
    "end": "4086540"
  },
  {
    "text": "steering angles, speed and torque, I think there's other information\ntoo available,",
    "start": "4086930",
    "end": "4091400"
  },
  {
    "text": "there's no reinforcement\nlearning here. Question.  Do you have a sense of\nhow much information",
    "start": "4092720",
    "end": "4098940"
  },
  {
    "text": "is being passed, how many\nLSTM gates are there in this problem?",
    "start": "4099000",
    "end": "4103639"
  },
  {
    "text": "The question was, how many LSTM gates\nare in this problem?",
    "start": "4107740",
    "end": "4111440"
  },
  {
    "text": "This network,",
    "start": "4113820",
    "end": "4116279"
  },
  {
    "text": "it's true that this diagrams kind of hide the number of parameters here,\nbut it's arbitrary",
    "start": "4121180",
    "end": "4128210"
  },
  {
    "text": "just like convolutional\nneural networks are arbitrary, the size of the input is arbitrary,",
    "start": "4128220",
    "end": "4135060"
  },
  {
    "text": "the size of Sigmoid function, TANH is arbitrary, so you can make it as large as you want,\nas deep as you want",
    "start": "4135100",
    "end": "4142440"
  },
  {
    "text": "and the deeper and larger, the better.  What these folks actually used--",
    "start": "4142440",
    "end": "4147900"
  },
  {
    "text": "the way these competitions work and I encourage you, if you're\ninterested in machine learning",
    "start": "4148120",
    "end": "4154200"
  },
  {
    "text": "to participate in Kaggle, I don't know how to pronounce it,\ncompetitions",
    "start": "4154200",
    "end": "4159119"
  },
  {
    "text": "where basically everyone\nis doing the same thing, you're using LSTMs or if it's one- on-one mapping,",
    "start": "4159360",
    "end": "4165660"
  },
  {
    "text": "using convolutional neural network\nfully connecting networks with some clever pre-processing",
    "start": "4165680",
    "end": "4170740"
  },
  {
    "text": "and the whole job is\nthat takes months and you probably,\nif you're a researcher, that's what you'd be doing\nyour own research,",
    "start": "4170760",
    "end": "4176570"
  },
  {
    "text": "playing with parameters, playing with pre-processing\nof the data, playing with the different parameter\nthat controls the size of the network",
    "start": "4176580",
    "end": "4183400"
  },
  {
    "text": "the learning rate, I've mentioned, this type of optimizer, all these kinds of things,\nthat's what you're playing with,",
    "start": "4183430",
    "end": "4189890"
  },
  {
    "text": "using your own human intuition and you're using your--",
    "start": "4189920",
    "end": "4193780"
  },
  {
    "text": "whatever probing you can do in monitoring the performansce of the network through time.",
    "start": "4196920",
    "end": "4202600"
  },
  {
    "text": "Yes?",
    "start": "4203940",
    "end": "4204640"
  },
  {
    "text": "The question was, you said that there's a",
    "start": "4217020",
    "end": "4223040"
  },
  {
    "text": "memory of tenth in this LCM, and I thought RNNs are\nsupposed to be arbitrary.",
    "start": "4223080",
    "end": "4230760"
  },
  {
    "text": "It has to do with the training, how the network is trained.",
    "start": "4232740",
    "end": "4239540"
  },
  {
    "text": "It's trained with sequences of 10. The structure is still the same,\nyou only have one cell that's\nlooping onto each other.",
    "start": "4239600",
    "end": "4246390"
  },
  {
    "text": "But the question is, in what chunks, what is the size of the sequence",
    "start": "4246480",
    "end": "4253700"
  },
  {
    "text": "that we should do in the training\nand then the testing. It can be arbitrary length.",
    "start": "4253720",
    "end": "4259580"
  },
  {
    "text": "It's just usually better\nto be consistent and have a fixed length.",
    "start": "4259640",
    "end": "4263740"
  },
  {
    "text": "You're not stacking 10 cells together. It's just a single cell still.",
    "start": "4267060",
    "end": "4272300"
  },
  {
    "text": "The third-place winner, Team Chauffeur,",
    "start": "4276460",
    "end": "4280520"
  },
  {
    "text": "used something called\ntransfer learning and it's something I don't think\nI mentioned",
    "start": "4281860",
    "end": "4286600"
  },
  {
    "text": "but it's kind of implied, the amazing power of neural networks.",
    "start": "4287280",
    "end": "4294140"
  },
  {
    "text": "First, you need a lot of data\nto do anything. That's the cost, that's\nthe limitation in neural networks.",
    "start": "4295000",
    "end": "4300960"
  },
  {
    "text": "But what you could do is, there's",
    "start": "4300960",
    "end": "4305200"
  },
  {
    "text": "neural networks that have been\ntrained on very large data sets. ImageNet,",
    "start": "4306840",
    "end": "4310760"
  },
  {
    "text": "Vdg Net, AlexNet, ResNet, all these networks are trained\non a huge amount of data.",
    "start": "4311920",
    "end": "4319480"
  },
  {
    "text": "Those networks are trained to tell the differences between a cat and dog\nSpecific optical recognition",
    "start": "4321200",
    "end": "4327440"
  },
  {
    "text": "of single images. How do I then take that network and apply it to my problem,",
    "start": "4327440",
    "end": "4332679"
  },
  {
    "text": "say of driving or length detection, or medical diagnosis, or cancer or not?",
    "start": "4332720",
    "end": "4338560"
  },
  {
    "text": "The beauty of neural networks,",
    "start": "4339460",
    "end": "4341100"
  },
  {
    "text": "the promise of transfer learning, is that you can just take that network, chop off the final layer,",
    "start": "4346160",
    "end": "4352480"
  },
  {
    "text": "the fully connected layer that maps from all those cool high-dimensional features that you\nhave learned about visual space,",
    "start": "4353060",
    "end": "4360800"
  },
  {
    "text": "and as opposed to predicting cat vs. dog, you teach it to predict\ncancer or no cancer.",
    "start": "4361360",
    "end": "4367480"
  },
  {
    "text": "You teach it to predict lane or no lane,\ntruck or no truck.",
    "start": "4367940",
    "end": "4372239"
  },
  {
    "text": "As long as the visual space under which that network operates is similar or the data like\nif it's audio or whatever",
    "start": "4373240",
    "end": "4380340"
  },
  {
    "text": "if it's similar, if the features are\nuseful then you learn, in studying the problem of\ncat vs dog deeply,",
    "start": "4380440",
    "end": "4387400"
  },
  {
    "text": "you have learned actually\nhow to see the world. As you're going to apply\nthat visual knowledge,",
    "start": "4387860",
    "end": "4393660"
  },
  {
    "text": "you can transfer that learning to another domain. That's the beautiful power\nof neural networks",
    "start": "4393660",
    "end": "4400240"
  },
  {
    "text": "it's that they're transferable. What they did here is--",
    "start": "4400240",
    "end": "4406000"
  },
  {
    "text": "I didn't spend enough time\nlooking through the code I'm not sure which of the\ngiant nework they took",
    "start": "4407960",
    "end": "4414719"
  },
  {
    "text": "but they took a giant\nconvolutional neural network, they chopped off",
    "start": "4414720",
    "end": "4421020"
  },
  {
    "text": "the end layer, which produced 3000 features, and they took those 3000 features",
    "start": "4421700",
    "end": "4427179"
  },
  {
    "text": "to every single image frame, and that's the Xt. They gave that as the input to LSTM.",
    "start": "4427240",
    "end": "4434100"
  },
  {
    "text": "And the sequence length,\nin that case, was 50. This process is pretty",
    "start": "4434420",
    "end": "4440200"
  },
  {
    "text": "similar across domains.\nThat's the beauty of it. The art of neural networks is in the--",
    "start": "4444200",
    "end": "4451640"
  },
  {
    "text": "Well that's a good sign [chuckles], I guess I should warp it up--",
    "start": "4453300",
    "end": "4457119"
  },
  {
    "text": "The art of the neural networks is\nin the proper parameter tuning.  That's the tricky part, and that's the part you can't be taught.",
    "start": "4467640",
    "end": "4474840"
  },
  {
    "text": "That's experience, sadly enough. That's why they talk about",
    "start": "4474940",
    "end": "4480880"
  },
  {
    "text": "Stochastic Gradient Descent SGD, That's what Geoffrey Hinton",
    "start": "4481900",
    "end": "4487240"
  },
  {
    "text": "refers to as Stochastic Graduate Student Descent,",
    "start": "4487240",
    "end": "4491760"
  },
  {
    "text": "meaning you just keep\nhiring graduate students to play with the hyperparameters until the problem is solved",
    "start": "4493040",
    "end": "4499340"
  },
  {
    "text": "[laughter].",
    "start": "4499980",
    "end": "4502180"
  },
  {
    "text": "I have about 100+ slides on driver state, which is the thing that\nI'm most passionate about,",
    "start": "4506700",
    "end": "4515900"
  },
  {
    "text": "and I think will save the best for last. I'll talk about that tomorrow.\nWe have a guest speaker",
    "start": "4515900",
    "end": "4523220"
  },
  {
    "text": "from the White House, will talk about the future\nof Artificial Intelligence from the perspective of policy,",
    "start": "4524020",
    "end": "4530100"
  },
  {
    "text": "and what I would like you to do first\noff you registered students is submit the two tutorial assignments,",
    "start": "4532310",
    "end": "4537639"
  },
  {
    "text": "and pick up can we just set the boxes right here\nor something?",
    "start": "4537660",
    "end": "4542680"
  },
  {
    "text": "Just stop by and pick up a shirt. And give us a card on the way.",
    "start": "4542760",
    "end": "4548079"
  },
  {
    "text": "Thanks guys. [Applause]",
    "start": "4548800",
    "end": "4556420"
  }
]