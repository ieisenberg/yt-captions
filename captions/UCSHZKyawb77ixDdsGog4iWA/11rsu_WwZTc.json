[
  {
    "text": "Thank You Sammy so I'll tell you about",
    "start": "30",
    "end": "6359"
  },
  {
    "text": "some very high-level stuff today no new",
    "start": "6359",
    "end": "11580"
  },
  {
    "text": "algorithm some of you already know about the book that Ian Goodfellow erinkoval",
    "start": "11580",
    "end": "18210"
  },
  {
    "text": "and I have written and it's now in presale by MIT press I think you can",
    "start": "18210",
    "end": "24300"
  },
  {
    "text": "find it on Amazon or something and paper",
    "start": "24300",
    "end": "29900"
  },
  {
    "text": "the actual shipping is going to be in December hopefully for nibs",
    "start": "29900",
    "end": "36050"
  },
  {
    "text": "so we've already heard that story at",
    "start": "36050",
    "end": "41430"
  },
  {
    "text": "least well from several people here at least from Andrew I think but it's good",
    "start": "41430",
    "end": "48840"
  },
  {
    "text": "to ponder a little bit some of these ingredients that seem to be important",
    "start": "48840",
    "end": "54149"
  },
  {
    "text": "for deep learning to succeed but in general for machine learning to succeed",
    "start": "54149",
    "end": "59609"
  },
  {
    "text": "to learn really complicated tasks of the kind we want to reach human level performance so if a machine is going to",
    "start": "59609",
    "end": "69750"
  },
  {
    "text": "be intelligent it's going to need to acquire a lot of information about the",
    "start": "69750",
    "end": "76229"
  },
  {
    "text": "world and the big success of machine learning for AI has been to show that we",
    "start": "76229",
    "end": "82770"
  },
  {
    "text": "can provide that information through data through examples but but really",
    "start": "82770",
    "end": "88290"
  },
  {
    "text": "think about it you know that that machine will need to know a huge amount of information about the world around us",
    "start": "88290",
    "end": "93360"
  },
  {
    "text": "this is not how we're doing it now because we're not able to train such big models but it will come one day and so",
    "start": "93360",
    "end": "99600"
  },
  {
    "text": "we'll need models that are much bigger than the ones we currently have of course that means machine learning",
    "start": "99600",
    "end": "105899"
  },
  {
    "text": "algorithms that can represent complicated functions that's you know one good thing about neural nets but",
    "start": "105899",
    "end": "111450"
  },
  {
    "text": "there are many other machine learning approaches that allow you in principle to represent very flexible forms like",
    "start": "111450",
    "end": "119100"
  },
  {
    "text": "nonparametric methods classical nonparametric methods or svms but",
    "start": "119100",
    "end": "124490"
  },
  {
    "text": "they're going to be missing 0.4 and potentially 0.5 depending on the methods",
    "start": "124490",
    "end": "131810"
  },
  {
    "text": "point 3 of course you you need enough computing power to train and use these big models and",
    "start": "131810",
    "end": "138740"
  },
  {
    "text": "point-five just says that it's not enough to be able to train the model you",
    "start": "138740",
    "end": "144390"
  },
  {
    "text": "have to be able to use it in a reasonably efficient way from a computational perspective this is not",
    "start": "144390",
    "end": "150240"
  },
  {
    "text": "always the case with some probabilistic models where inference in other words answering questions having the computer",
    "start": "150240",
    "end": "156930"
  },
  {
    "text": "do something can be intractable and then you need to do some approximations which could be efficient or or not now the",
    "start": "156930",
    "end": "164190"
  },
  {
    "text": "point I really want to talk about is the fourth one how do we defeat the curse of",
    "start": "164190",
    "end": "169770"
  },
  {
    "text": "dimensionality in other words if you don't assume much about the world it's",
    "start": "169770",
    "end": "176880"
  },
  {
    "text": "actually impossible to learn about it and and so I'm going to tell you a bit",
    "start": "176880",
    "end": "183480"
  },
  {
    "text": "about the assumptions that are behind a lot of deep learning algorithms which",
    "start": "183480",
    "end": "188910"
  },
  {
    "text": "make it possible to work as well as we are seeing in practice in the last few years something wrong Microsoft bug okay",
    "start": "188910",
    "end": "211310"
  },
  {
    "text": "so how do we bypass the curse of dimensionality the curse of dimensionality is about the",
    "start": "211310",
    "end": "217730"
  },
  {
    "text": "exponentially large number of configurations of the space variables that we want to model the number of",
    "start": "217730",
    "end": "225350"
  },
  {
    "text": "values that all of the variables that we observe can take is going to be",
    "start": "225350",
    "end": "231150"
  },
  {
    "text": "exponentially large in general because there's a compositional nature if if each pixel can take two values and you",
    "start": "231150",
    "end": "237510"
  },
  {
    "text": "got a million pixels then you got two to one million number of possible images so",
    "start": "237510",
    "end": "242910"
  },
  {
    "text": "the only way to beat an exponential is to use another exponential so we need to",
    "start": "242910",
    "end": "249989"
  },
  {
    "text": "make our models compositional we need to build our models in such a way that they",
    "start": "249989",
    "end": "255900"
  },
  {
    "text": "can represent functions that look very complicated but yet these models need to",
    "start": "255900",
    "end": "264210"
  },
  {
    "text": "have a reasonably small number of parameters reasonably small in the sense compared to the number of configurations",
    "start": "264210",
    "end": "270550"
  },
  {
    "text": "of the variables the number of parameters should be small and we can",
    "start": "270550",
    "end": "277060"
  },
  {
    "text": "achieve that by by composing little pieces together composing layers together it can put composing units on",
    "start": "277060",
    "end": "284080"
  },
  {
    "text": "the same layer together and that's essentially what's happening with deep learning so you actually have two kinds",
    "start": "284080",
    "end": "289330"
  },
  {
    "text": "of compositions there's the the compositions happening on the same layer this is the idea of distributed",
    "start": "289330",
    "end": "296800"
  },
  {
    "text": "representations which I'm going to try to explain a bit more this is what you get when you learn embeddings for",
    "start": "296800",
    "end": "302830"
  },
  {
    "text": "forwards or for images representations in general and then there's the idea of having multiple levels of representation",
    "start": "302830",
    "end": "309240"
  },
  {
    "text": "that's the notion of depth and there there is another kind of composition",
    "start": "309240",
    "end": "315670"
  },
  {
    "text": "takes place whereas the the first one is a kind of parallel composition I'm you know I can choose",
    "start": "315670",
    "end": "320680"
  },
  {
    "text": "the values of my different units separately and then they together represent an exponentially large number",
    "start": "320680",
    "end": "326080"
  },
  {
    "text": "of possible configurations in the second case there's a sequential composition where I take the output of one level and",
    "start": "326080",
    "end": "333010"
  },
  {
    "text": "and I combine them in new ways to build features for the next level and so on and so on right so so the reason deep",
    "start": "333010",
    "end": "344170"
  },
  {
    "text": "learning is working is because the world around us is better modeled by making",
    "start": "344170",
    "end": "351400"
  },
  {
    "text": "these assumptions it's not necessarily true that deep learning is going to work for any machine learning problem in fact",
    "start": "351400",
    "end": "357760"
  },
  {
    "text": "if if we consider the set of all possible distributions that we would like to work from deep learning is no",
    "start": "357760",
    "end": "364480"
  },
  {
    "text": "better than any other and that's this is basically what the no free lunch theorem is saying it's because we are incredibly",
    "start": "364480",
    "end": "372250"
  },
  {
    "text": "lucky that we live in this world which can be described by using composition that these algorithms are working so",
    "start": "372250",
    "end": "378460"
  },
  {
    "text": "well this is important to really understand this",
    "start": "378460",
    "end": "384990"
  },
  {
    "text": "so before I go a bit more into distributed representations let me say a",
    "start": "386719",
    "end": "392159"
  },
  {
    "text": "few words about non distributor presentations so if you're thinking about things like clustering engrams for",
    "start": "392159",
    "end": "398370"
  },
  {
    "text": "language modeling classical nearest neighbors SVM's with Gaussian kernels",
    "start": "398370",
    "end": "404599"
  },
  {
    "text": "classical nonparametric models with local kernels and decision trees all",
    "start": "404599",
    "end": "412439"
  },
  {
    "text": "these things the way these algorithms really work is actually pretty",
    "start": "412439",
    "end": "417930"
  },
  {
    "text": "straightforward if you you know cut the crap and hide the math and try to",
    "start": "417930",
    "end": "423360"
  },
  {
    "text": "understand what is going on they they look at the data in in data space and",
    "start": "423360",
    "end": "429870"
  },
  {
    "text": "they break that space into regions and they're going to use different free",
    "start": "429870",
    "end": "436439"
  },
  {
    "text": "parameters for each of those regions to figure out what the right answer should be the right answer it doesn't have to be supervised learning even an S",
    "start": "436439",
    "end": "442379"
  },
  {
    "text": "provides I think there's a right answer it might be the density or something like that okay and you might think that that's the",
    "start": "442379",
    "end": "449819"
  },
  {
    "text": "only way of solving a problem you know we consider all of the cases and we have an answer for each of the cases and we",
    "start": "449819",
    "end": "455550"
  },
  {
    "text": "can maybe interpolate between those cases that we've seen the problem with",
    "start": "455550",
    "end": "460830"
  },
  {
    "text": "this is somebody comes up with a new example which isn't in between two of",
    "start": "460830",
    "end": "467370"
  },
  {
    "text": "the examples we've seen something that a la requires us to extrapolate something that's you know non-trivial",
    "start": "467370",
    "end": "473520"
  },
  {
    "text": "generalization and and these algorithms just fail they don't they don't really have a recipe for saying something",
    "start": "473520",
    "end": "479009"
  },
  {
    "text": "meaningful away from the training examples there's another interesting",
    "start": "479009",
    "end": "485069"
  },
  {
    "text": "thing to note here which I would like to you to keep in mind before I show the next slide which is in red here which is",
    "start": "485069",
    "end": "493680"
  },
  {
    "text": "we can do a kind of simple counting to relate the number of parameters a number",
    "start": "493680",
    "end": "500159"
  },
  {
    "text": "of free parameters that can be learning and the number of regions in the data",
    "start": "500159",
    "end": "506069"
  },
  {
    "text": "space that we can distinguish so here we basically have linear relationship",
    "start": "506069",
    "end": "512250"
  },
  {
    "text": "between these two things right so for each region I'm going to need at least something like some kind of Center for",
    "start": "512250",
    "end": "519390"
  },
  {
    "text": "the region and maybe if I need to output something I'll lean an extra set of parameters to tell",
    "start": "519390",
    "end": "525510"
  },
  {
    "text": "me what the answer should be in that area so the number of parameters grows linearly with the number of regions that",
    "start": "525510",
    "end": "531750"
  },
  {
    "text": "I I'm going to be able to distinguish the good news is I can have any kind of",
    "start": "531750",
    "end": "537839"
  },
  {
    "text": "function right so I can break up the space in any way I want and then for each of those regions I can have any kind of output that I need so for",
    "start": "537839",
    "end": "546570"
  },
  {
    "text": "decision trees the regions would be you know splitting across axes and so on and for this is more like four nearest",
    "start": "546570",
    "end": "553320"
  },
  {
    "text": "neighbor or something like that now another bug I don't think I will",
    "start": "553320",
    "end": "564270"
  },
  {
    "text": "send this hope works this time oh I have",
    "start": "564270",
    "end": "575100"
  },
  {
    "text": "a another option sorry about this okay",
    "start": "575100",
    "end": "590690"
  },
  {
    "text": "so so here's the the point of view of",
    "start": "590690",
    "end": "596610"
  },
  {
    "text": "the suit representations for solving the same general machine learning problem we have a data space and we want to break",
    "start": "596610",
    "end": "602550"
  },
  {
    "text": "it down but we're going to break it down in a way that's not general we're going",
    "start": "602550",
    "end": "607830"
  },
  {
    "text": "to break it down in a way that makes assumptions about the data but it's",
    "start": "607830",
    "end": "612990"
  },
  {
    "text": "going to be compositional and it's going to allow us to you know be exponentially more efficient so how are we going to do",
    "start": "612990",
    "end": "618510"
  },
  {
    "text": "this so in the picture on the right what you see is a way to break the input",
    "start": "618510",
    "end": "624510"
  },
  {
    "text": "space by the intersection of half-planes and this is the kind of thing you would have with a what happens at the first",
    "start": "624510",
    "end": "631589"
  },
  {
    "text": "layer of a neural net so here imagine the input is 2-dimensional so I can plot it here and I have three binary hidden",
    "start": "631589",
    "end": "639270"
  },
  {
    "text": "units c1 c2 c3 so because they're binary you can think of them as little binary",
    "start": "639270",
    "end": "646980"
  },
  {
    "text": "classifiers and because it's only a one layer net you can think of what they're",
    "start": "646980",
    "end": "653040"
  },
  {
    "text": "doing is a linear classification and so those colored hyperplanes here are the decision surfaces for each of",
    "start": "653040",
    "end": "660780"
  },
  {
    "text": "them now these three bits there can take they can take eight values right",
    "start": "660780",
    "end": "666060"
  },
  {
    "text": "corresponding to you know whether each of them is on or off and and those",
    "start": "666060",
    "end": "671160"
  },
  {
    "text": "different configurations of those bits correspond to actually seven regions",
    "start": "671160",
    "end": "677370"
  },
  {
    "text": "here because there's one of the eight regions which does is not feasible so so",
    "start": "677370",
    "end": "683250"
  },
  {
    "text": "now you see that we're defining a number of regions which is corresponding to all of the possible intersections of the",
    "start": "683250",
    "end": "689700"
  },
  {
    "text": "corresponding half-planes and and now we can play the game of how many regions do",
    "start": "689700",
    "end": "697380"
  },
  {
    "text": "we get for how many parameters and what we see is that as if we played the game",
    "start": "697380",
    "end": "702510"
  },
  {
    "text": "of growing the number of dimensions features and also of inputs we can get",
    "start": "702510",
    "end": "708990"
  },
  {
    "text": "an exponentially large number of regions which are all of these intersections right there's an exponential number of these intersections corresponding to",
    "start": "708990",
    "end": "717450"
  },
  {
    "text": "different binary configurations yet the number of parameters grows linearly with the number of units so it looks like",
    "start": "717450",
    "end": "724740"
  },
  {
    "text": "we're able to express a function then on top of that I could imagine you have a linear classifier right that's that's",
    "start": "724740",
    "end": "732030"
  },
  {
    "text": "the one hidden layer new on that so so the number of parameters grows just linearly with the number of features but",
    "start": "732030",
    "end": "740970"
  },
  {
    "text": "the number of regions that the network can really provide a different answer to grows exponentially so this is very cool",
    "start": "740970",
    "end": "750060"
  },
  {
    "text": "and the reason it's very cool is that it allows those neural nets to generalize",
    "start": "750060",
    "end": "755210"
  },
  {
    "text": "because while we're learning about each of those features we can generalize to",
    "start": "755210",
    "end": "764490"
  },
  {
    "text": "regions we've never seen because we've learned enough about each of those features separately I'm going to give",
    "start": "764490",
    "end": "770730"
  },
  {
    "text": "you an example of this in a couple of slide actually it's let's do it first so",
    "start": "770730",
    "end": "778070"
  },
  {
    "text": "so think about those features so the input is an image of a person and think",
    "start": "778070",
    "end": "784470"
  },
  {
    "text": "of those features as things like I have a detector that says that the person wears glasses",
    "start": "784470",
    "end": "790620"
  },
  {
    "text": "and I have another unit that's detecting that the person is a female or male and",
    "start": "790620",
    "end": "796350"
  },
  {
    "text": "I have another unit that texts that the person is a child or not and you can imagine you know hundreds or thousands",
    "start": "796350",
    "end": "802200"
  },
  {
    "text": "of these things of course so so the good news is you could imagine learning about",
    "start": "802200",
    "end": "811080"
  },
  {
    "text": "each of these feature detectors these little classifiers separately in fact",
    "start": "811080",
    "end": "819450"
  },
  {
    "text": "you could do better than that you could share you know intermediate layers between the input and those features but",
    "start": "819450",
    "end": "824640"
  },
  {
    "text": "but let's you know take even the worst case and imagine we were to train those separately which is the case in the",
    "start": "824640",
    "end": "829740"
  },
  {
    "text": "linear model that I show before we have a separate set of parameters for each of",
    "start": "829740",
    "end": "834990"
  },
  {
    "text": "these detectors so if I have n features each of them say needs order of K",
    "start": "834990",
    "end": "840990"
  },
  {
    "text": "parameters then I need order of NK parameters and I need order of NK",
    "start": "840990",
    "end": "846660"
  },
  {
    "text": "examples and one thing you should know from you know which machine learning theory is that if you have order of P",
    "start": "846660",
    "end": "857660"
  },
  {
    "text": "parameters you need order of P examples to do a reasonable job of jaw's age of journalizing you can you can get around",
    "start": "857810",
    "end": "865620"
  },
  {
    "text": "that by regularizing and effectively having less degrees of freedom but but you know to keep things simple you need",
    "start": "865620",
    "end": "871500"
  },
  {
    "text": "about the same number of examples or maybe a hundred times more or ten times more as the number of really free",
    "start": "871500",
    "end": "877710"
  },
  {
    "text": "parameters so so now the relationship between the number of regions that I can",
    "start": "877710",
    "end": "886230"
  },
  {
    "text": "represent and the number of examples I need is quite nice because the number of",
    "start": "886230",
    "end": "892080"
  },
  {
    "text": "regions is going to be to to the number of features of these binary features so",
    "start": "892080",
    "end": "897090"
  },
  {
    "text": "you know a person could wear glasses or not be a female or a male or child or not and I could have a hundred of these things and I could probably recognize",
    "start": "897090",
    "end": "905640"
  },
  {
    "text": "reasonably well all of these two to the 100 configurations of people even though",
    "start": "905640",
    "end": "911970"
  },
  {
    "text": "I've obviously not seen all of those to do 100 configurations why is it that I'm able to do that I'm able to do that",
    "start": "911970",
    "end": "918780"
  },
  {
    "text": "because the the models can learn about each these binary features kind of independently in the sense that I don't",
    "start": "918780",
    "end": "925170"
  },
  {
    "text": "need to see every possible configuration of the other features to know about",
    "start": "925170",
    "end": "930900"
  },
  {
    "text": "wearing glasses like I can learn about wearing glasses even though I've never",
    "start": "930900",
    "end": "936300"
  },
  {
    "text": "seen somebody who was a female and a child and chubby and had you know yellow",
    "start": "936300",
    "end": "944610"
  },
  {
    "text": "shoes and and and I have seen enough examples of people wearing glasses I can",
    "start": "944610",
    "end": "950190"
  },
  {
    "text": "learn about wearing glasses in general I don't need to see all of the configurations of the other features to",
    "start": "950190",
    "end": "955230"
  },
  {
    "text": "learn about one feature okay and so so this is really what what you know why",
    "start": "955230",
    "end": "962520"
  },
  {
    "text": "this thing works is because we're making assumptions about the data that those",
    "start": "962520",
    "end": "968760"
  },
  {
    "text": "features are meaningful by themselves and you don't need to actually have data",
    "start": "968760",
    "end": "974280"
  },
  {
    "text": "for each of the regions the exponential number of regions in order to learn the",
    "start": "974280",
    "end": "980580"
  },
  {
    "text": "proper way of detecting or lore of discovering these these these intermediate features let me add",
    "start": "980580",
    "end": "988950"
  },
  {
    "text": "something here there were some experiments recently actually showing that this kind of thing is really",
    "start": "988950",
    "end": "997140"
  },
  {
    "text": "happening because the features I was talking about",
    "start": "997140",
    "end": "1003010"
  },
  {
    "text": "not only I'm assuming that they exist but the the optimization methods or",
    "start": "1003010",
    "end": "1009170"
  },
  {
    "text": "training procedures discover them they can learn them and this is an experiment",
    "start": "1009170",
    "end": "1015410"
  },
  {
    "text": "that's been done in 2012 all Tour Alba's lab at MIT where they trained a usual",
    "start": "1015410",
    "end": "1023090"
  },
  {
    "text": "confidence to recognize places so the outputs of the net are just the types of",
    "start": "1023090",
    "end": "1030560"
  },
  {
    "text": "places like is this a beach scene or an office scene or street scene and so on but but then the the thing they've done",
    "start": "1030560",
    "end": "1038089"
  },
  {
    "text": "is they ask people to analyze the the hidden units to try to figure out what each hidden unit was doing and they found that there's a large proportion of",
    "start": "1038089",
    "end": "1044270"
  },
  {
    "text": "units that humans can find a pretty obvious interpretation for what those units like so so they see a bunch of",
    "start": "1044270",
    "end": "1052940"
  },
  {
    "text": "units which you know like people are different kinds of people or animals or buildings or",
    "start": "1052940",
    "end": "1059840"
  },
  {
    "text": "seedings or tables lighting and so on so it's like if indeed the those neural",
    "start": "1059840",
    "end": "1066710"
  },
  {
    "text": "nets are discovering semantic features they are semantic because actually people give them names as the",
    "start": "1066710",
    "end": "1072860"
  },
  {
    "text": "intermediate features you know in order to reach the final goal of here transpiring scenes and the reason",
    "start": "1072860",
    "end": "1080659"
  },
  {
    "text": "they're generalizing is because now you can combine those features in an exponentially large number of ways right you could have a scene that has a table",
    "start": "1080659",
    "end": "1089600"
  },
  {
    "text": "different kind of lighting some people you know maybe a pet and and you can say",
    "start": "1089600",
    "end": "1095150"
  },
  {
    "text": "something meaningful about the combinations of these things because the network is able to learn all of these",
    "start": "1095150",
    "end": "1101450"
  },
  {
    "text": "features without having to see all of the possible configurations of them so I",
    "start": "1101450",
    "end": "1108140"
  },
  {
    "text": "don't know if my explanation makes sense to you but now is the chance to ask me a question all clear usually it's not yeah",
    "start": "1108140",
    "end": "1123190"
  },
  {
    "text": "with decision trees right to some extent",
    "start": "1125700",
    "end": "1132789"
  },
  {
    "text": "so if the question is can't we do the same thing with a set of decision trees yeah in fact this is one of the reasons",
    "start": "1132789",
    "end": "1139779"
  },
  {
    "text": "why forests work better or bagged trees work better than single trees forests or",
    "start": "1139779",
    "end": "1145570"
  },
  {
    "text": "actually or Bank trees are like one layer one level deeper than a single trees but but they still don't have as",
    "start": "1145570",
    "end": "1153520"
  },
  {
    "text": "much of a sort of distributed aspect as neural nets so they be and and usually",
    "start": "1153520",
    "end": "1161320"
  },
  {
    "text": "they're not trained jointly I mean boosted trees are you know to some",
    "start": "1161320",
    "end": "1166539"
  },
  {
    "text": "extent in a greedy way but yeah any other question yeah cases where what non-conditional",
    "start": "1166539",
    "end": "1181890"
  },
  {
    "text": "non computer vision non compositional I",
    "start": "1181890",
    "end": "1187000"
  },
  {
    "text": "don't understand the question I mean I don't sound what you mean what do you mean non compositional yeah it's",
    "start": "1187000",
    "end": "1193809"
  },
  {
    "text": "everywhere around us I don't think I don't think that there are examples of neural nets that really work well where",
    "start": "1193809",
    "end": "1199270"
  },
  {
    "text": "the data doesn't have some kind of compositional structure in it but if you come up with an example I'd like to hear",
    "start": "1199270",
    "end": "1205120"
  },
  {
    "text": "about it okie s yes",
    "start": "1205120",
    "end": "1211110"
  },
  {
    "text": "to think about this issue in graphical model terms is is if it can be done but",
    "start": "1228240",
    "end": "1235660"
  },
  {
    "text": "you have to think about not feature detection like I've been doing here but",
    "start": "1235660",
    "end": "1241150"
  },
  {
    "text": "about generating an image or something like that right then it's easier to",
    "start": "1241150",
    "end": "1246160"
  },
  {
    "text": "think about it so so the same kinds of things happen if you think about how I could generate an image if you think",
    "start": "1246160",
    "end": "1253150"
  },
  {
    "text": "about underlying factors like which objects where they are what's their identity what's their size these are all",
    "start": "1253150",
    "end": "1260200"
  },
  {
    "text": "independent factors which you compose together in in funny ways if you were to",
    "start": "1260200",
    "end": "1265360"
  },
  {
    "text": "do a graphics engine you can see exactly what those ways are and it's much much",
    "start": "1265360",
    "end": "1271420"
  },
  {
    "text": "easier to represent that joint of distribution using this compositional structure then if you're trying to work",
    "start": "1271420",
    "end": "1279190"
  },
  {
    "text": "directly in the pixel space which is normally what you would do with a classical nonparametric method and it",
    "start": "1279190",
    "end": "1286030"
  },
  {
    "text": "wouldn't work but if you look at our best D generative models now for images for example like ganz or V AES they're",
    "start": "1286030",
    "end": "1293590"
  },
  {
    "text": "really you know we're not there yet but they're amazingly better than anything",
    "start": "1293590",
    "end": "1298750"
  },
  {
    "text": "that people could dream up just a few years ago in in machine learning okay",
    "start": "1298750",
    "end": "1304930"
  },
  {
    "text": "let me move on because of other things to talk about so this is all kind of",
    "start": "1304930",
    "end": "1311590"
  },
  {
    "text": "hand wavy but some people have done some math around these ideas and and so for",
    "start": "1311590",
    "end": "1321850"
  },
  {
    "text": "example there's one result from two years ago I clear where we study the",
    "start": "1321850",
    "end": "1327990"
  },
  {
    "text": "single layer case and we consider a network with rectifiers rellis and we",
    "start": "1327990",
    "end": "1338470"
  },
  {
    "text": "find that the the network of course computes a piecewise linear function and",
    "start": "1338470",
    "end": "1345570"
  },
  {
    "text": "so one way to quantify the richness of",
    "start": "1345570",
    "end": "1351370"
  },
  {
    "text": "the function that it can compute I was talking about regions here but well you can do the same thing here you can count how many pieces does does this",
    "start": "1351370",
    "end": "1359049"
  },
  {
    "text": "network have in its input to output function and and it turns out that is it",
    "start": "1359049",
    "end": "1365619"
  },
  {
    "text": "six potential in in the number of inputs",
    "start": "1365619",
    "end": "1371649"
  },
  {
    "text": "well it's a number of units to the power number of inputs so that's for a sort of",
    "start": "1371649",
    "end": "1378460"
  },
  {
    "text": "district representation there's this an exponential kicking in we also studied the the depth aspect so what you need to",
    "start": "1378460",
    "end": "1386080"
  },
  {
    "text": "know about depth is that there's a lot",
    "start": "1386080",
    "end": "1391419"
  },
  {
    "text": "of earlier theory that says that a single layer is sufficient to represent any function however that theory doesn't",
    "start": "1391419",
    "end": "1397929"
  },
  {
    "text": "specify how many units you get you might need and in fact you might need an especially large number of units so what",
    "start": "1397929",
    "end": "1406840"
  },
  {
    "text": "several results show is that there are functions that can be represented very",
    "start": "1406840",
    "end": "1414879"
  },
  {
    "text": "efficiently with few units so few parameters if you allow the network to",
    "start": "1414879",
    "end": "1420970"
  },
  {
    "text": "be deep enough so out of all the functions again it's a luckiness thing",
    "start": "1420970",
    "end": "1426309"
  },
  {
    "text": "right out of all the functions that exists there's a very very small fraction which happen to be very easy to",
    "start": "1426309",
    "end": "1434529"
  },
  {
    "text": "represent with a deep network and if you try to represent these these functions",
    "start": "1434529",
    "end": "1440590"
  },
  {
    "text": "with a shallow network you're screwed you're going to need an exponential number of parameters and so you're gonna",
    "start": "1440590",
    "end": "1448690"
  },
  {
    "text": "need an exponential number of examples to learn these things but again we're",
    "start": "1448690",
    "end": "1453759"
  },
  {
    "text": "incredibly lucky that the function we want to learn have this property but in",
    "start": "1453759",
    "end": "1458950"
  },
  {
    "text": "the sense it's not surprising I mean we use this kind of compositionality and depth everywhere we when we write a computer program we just don't have like",
    "start": "1458950",
    "end": "1465519"
  },
  {
    "text": "a single main we have you know functions and call functions and and we were able",
    "start": "1465519",
    "end": "1472059"
  },
  {
    "text": "to show similar things as what I was telling you about for the single layer case that as you increase depth for",
    "start": "1472059",
    "end": "1480070"
  },
  {
    "text": "these deep relu networks the number of pieces in the piecewise linear function",
    "start": "1480070",
    "end": "1486639"
  },
  {
    "text": "grows exponentially with the depth so so it's it's already exponentially large",
    "start": "1486639",
    "end": "1492340"
  },
  {
    "text": "with a single-layer but it gets exponentially even more with a deeper",
    "start": "1492340",
    "end": "1497350"
  },
  {
    "text": "net okay so so this this was a topic of representation of functions why why deep",
    "start": "1497350",
    "end": "1505240"
  },
  {
    "text": "learn deep architectures can can be very powerful if we're lucky and we seem to",
    "start": "1505240",
    "end": "1510460"
  },
  {
    "text": "be looking the other another topic I",
    "start": "1510460",
    "end": "1516039"
  },
  {
    "text": "want to mention that's kind of very much in the foundations is how is it that",
    "start": "1516039",
    "end": "1521770"
  },
  {
    "text": "we're able to train these neural nets in the first place in the 90s many people",
    "start": "1521770",
    "end": "1527789"
  },
  {
    "text": "decided to not do any more research on your nuts because there were 30 Korra's",
    "start": "1527789",
    "end": "1533049"
  },
  {
    "text": "ult's showing that there are really an exponentially large number of local minima in the training objective in of a",
    "start": "1533049",
    "end": "1542500"
  },
  {
    "text": "neural net so in other words the function we want to learn has many of",
    "start": "1542500",
    "end": "1548080"
  },
  {
    "text": "these holes and if we start at a random place well what's the chance we're going",
    "start": "1548080",
    "end": "1553899"
  },
  {
    "text": "to find the best one the the one that corresponds to a good cost and that was",
    "start": "1553899",
    "end": "1559330"
  },
  {
    "text": "one of the motivations for people who flocked into a very large area of",
    "start": "1559330",
    "end": "1565210"
  },
  {
    "text": "research in machine learning in the 90s and 2000's based on algorithms that",
    "start": "1565210",
    "end": "1570340"
  },
  {
    "text": "require on the convex optimization to Train because of course if we can do context optimization we eliminate this",
    "start": "1570340",
    "end": "1577090"
  },
  {
    "text": "problem if if the objective function is convex in the parameters then we know there's a single global minimum right so",
    "start": "1577090",
    "end": "1587500"
  },
  {
    "text": "let me show you a picture here you get a sense of if you look on the right hand top this is if you draw a random",
    "start": "1587500",
    "end": "1594549"
  },
  {
    "text": "function in 1d or 2d or 3d like here this is a kind of a random smooth",
    "start": "1594549",
    "end": "1600669"
  },
  {
    "text": "function in 2d you see that is going to have many ups and downs this is a local",
    "start": "1600669",
    "end": "1605950"
  },
  {
    "text": "minimum and but but the good news is",
    "start": "1605950",
    "end": "1611770"
  },
  {
    "text": "that in high dimension it's a totally different story so what are the dimensions here we're talking about the",
    "start": "1611770",
    "end": "1617890"
  },
  {
    "text": "parameters of the model and the vertical axis is the cost we're trying to",
    "start": "1617890",
    "end": "1623440"
  },
  {
    "text": "minimize and what happens in high dimension is that instead of having a huge number of",
    "start": "1623440",
    "end": "1631890"
  },
  {
    "text": "local minima on our way when we're trying to optimize what we encounter instead is a huge number of saddle",
    "start": "1631890",
    "end": "1638130"
  },
  {
    "text": "points so saddle point is like the thing on the bottom right in in 2d so you have",
    "start": "1638130",
    "end": "1643890"
  },
  {
    "text": "two parameters and y-axis is the cost you want to minimize and so what you see in a saddle point is yeah you have",
    "start": "1643890",
    "end": "1649700"
  },
  {
    "text": "dimensions or directions where the the objective function draws a a minimum so",
    "start": "1649700",
    "end": "1657809"
  },
  {
    "text": "there's like a curve that it curves up and in other directions it curves down",
    "start": "1657809",
    "end": "1664320"
  },
  {
    "text": "so we are you know saddle point has both a minimum in some direction and a maximum in other directions so this is",
    "start": "1664320",
    "end": "1673159"
  },
  {
    "text": "this is interesting because even though it's a these these points like saddle",
    "start": "1673159",
    "end": "1683010"
  },
  {
    "text": "points and many more are places where you could get stuck in principle if you're exactly at the subtle point you don't move but if you move a little bit",
    "start": "1683010",
    "end": "1689070"
  },
  {
    "text": "away from it you will go down the saddle right so what what our work in the other",
    "start": "1689070",
    "end": "1699090"
  },
  {
    "text": "paper other work from NYU tremonica and collaborators of Yann",
    "start": "1699090",
    "end": "1705299"
  },
  {
    "text": "Locker showed is that actually in very",
    "start": "1705299",
    "end": "1712620"
  },
  {
    "text": "high dimension not only you know it's it's the issue is more saddle points",
    "start": "1712620",
    "end": "1718710"
  },
  {
    "text": "than local minima but but the local minima are good so let me try to explain",
    "start": "1718710",
    "end": "1724169"
  },
  {
    "text": "what I mean by this so let me show you",
    "start": "1724169",
    "end": "1730470"
  },
  {
    "text": "actually first an experiment from from the NYU guys so they did an experiment",
    "start": "1730470",
    "end": "1736169"
  },
  {
    "text": "where they gradually change the size of the neural net and they they look at",
    "start": "1736169",
    "end": "1742769"
  },
  {
    "text": "what looks like local minima but they could be you know saddle points that are the lowest that they could obtain by",
    "start": "1742769",
    "end": "1749250"
  },
  {
    "text": "training and what you're looking at is a distribution of errors they get from",
    "start": "1749250",
    "end": "1754799"
  },
  {
    "text": "different initialization of their training and so what happens is that when the network is small like the",
    "start": "1754799",
    "end": "1762280"
  },
  {
    "text": "pink here on the right there's a widespread distribution of cost that you can get depending on where you you you",
    "start": "1762280",
    "end": "1769480"
  },
  {
    "text": "start and they're pretty high and if you increase the size of the network it's like all of the local minima that you",
    "start": "1769480",
    "end": "1777310"
  },
  {
    "text": "find concentrate around a particular costs so you don't get any of these bad",
    "start": "1777310",
    "end": "1784470"
  },
  {
    "text": "local minima that you would get with a small Network they're all kind of pretty good and if you increase even more the",
    "start": "1784470",
    "end": "1791170"
  },
  {
    "text": "size of network this is like a single hidden layer network you know not very complicated this phenomenon increases even more in",
    "start": "1791170",
    "end": "1798160"
  },
  {
    "text": "other words they all kind of converge to the same kind of costs so let me try to explain what's going on so if we go back",
    "start": "1798160",
    "end": "1805930"
  },
  {
    "text": "to the picture of the saddle point but instead of being in 2d imagine you are in a million D and in fact you know",
    "start": "1805930",
    "end": "1813160"
  },
  {
    "text": "people have billion D networks these days I'm sure andrew has even bigger",
    "start": "1813160",
    "end": "1818440"
  },
  {
    "text": "ones I'm not sure but so what happens in",
    "start": "1818440",
    "end": "1824920"
  },
  {
    "text": "this very high dimensional space of parameters is that if if things are not",
    "start": "1824920",
    "end": "1832000"
  },
  {
    "text": "really you know really bad for you so if you imagine a little bit of randomness",
    "start": "1832000",
    "end": "1837310"
  },
  {
    "text": "in the way the problem is set up and there it seems to be the case in order",
    "start": "1837310",
    "end": "1842650"
  },
  {
    "text": "to have a true local minimum you need to have the curvature going up like this in",
    "start": "1842650",
    "end": "1848260"
  },
  {
    "text": "all the you know billion directions so",
    "start": "1848260",
    "end": "1853270"
  },
  {
    "text": "if there is a certain probability of this event happening that all know that this particular directions is curving up",
    "start": "1853270",
    "end": "1859060"
  },
  {
    "text": "and this one is grabbing up the probability that all of them curve up becomes exponentially small so we we",
    "start": "1859060",
    "end": "1867790"
  },
  {
    "text": "tested that experimentally what you see in the bottom left is a curve that shows",
    "start": "1867790",
    "end": "1874500"
  },
  {
    "text": "the training error as a function of what's called the index of the critical",
    "start": "1874500",
    "end": "1881380"
  },
  {
    "text": "point which is just the fraction of the directions which are",
    "start": "1881380",
    "end": "1889240"
  },
  {
    "text": "curving down right so so 0% would mean",
    "start": "1889240",
    "end": "1895580"
  },
  {
    "text": "it's a local minimum a hundred percent would be it's a local maximum and",
    "start": "1895580",
    "end": "1901340"
  },
  {
    "text": "anything in between is a saddle point so what we find is that as training",
    "start": "1901340",
    "end": "1907460"
  },
  {
    "text": "progresses we're going close to a bunch of saddle points and these and none of",
    "start": "1907460",
    "end": "1915049"
  },
  {
    "text": "them are local minima otherwise we would be stuck and and in fact we never",
    "start": "1915049",
    "end": "1923000"
  },
  {
    "text": "encounter local minima until we reach the lowest possible cost that we were",
    "start": "1923000",
    "end": "1928070"
  },
  {
    "text": "able to get in addition there is a theory suggesting that so the the local",
    "start": "1928070",
    "end": "1936470"
  },
  {
    "text": "the low the the local minima will actually be close in cost to the global",
    "start": "1936470",
    "end": "1943190"
  },
  {
    "text": "minimum they will be above and they will concentrate in a little band above the global minimum but that band of local",
    "start": "1943190",
    "end": "1952580"
  },
  {
    "text": "minima will be close to the global minimum and and the larger 2-dimension the more this is going to be true so as",
    "start": "1952580",
    "end": "1960110"
  },
  {
    "text": "you go to go back to my analogy right at some point of course you will get local minima even though it's unlikely when",
    "start": "1960110",
    "end": "1967070"
  },
  {
    "text": "you're in the middle when you get close to the bottom well you can't go lower so you know it has to rise up in all the",
    "start": "1967070",
    "end": "1973250"
  },
  {
    "text": "directions but it's yeah so that's kind of good news I think in spite of this I",
    "start": "1973250",
    "end": "1980390"
  },
  {
    "text": "don't think that the optimization problem of neural nets is solved there are still many cases where we find",
    "start": "1980390",
    "end": "1985730"
  },
  {
    "text": "ourselves to be stuck and we still don't understand what the landscape looks like this set of beautiful experiments by in",
    "start": "1985730",
    "end": "1993080"
  },
  {
    "text": "Goodfellow that help us visualize a bit what's going on but I think one of the open problems of optimization for neural",
    "start": "1993080",
    "end": "1999230"
  },
  {
    "text": "nets is we know what does the landscape actually look like it's hard to visualize of course because it's very",
    "start": "1999230",
    "end": "2004929"
  },
  {
    "text": "high dimensional but for example we don't know what those saddle points",
    "start": "2004929",
    "end": "2011740"
  },
  {
    "text": "really look like when we actually measure the gradient near those when",
    "start": "2011740",
    "end": "2017740"
  },
  {
    "text": "we're approaching those saddle points is it's not close to zero so we never go to actually flat places this may be too due to the fact that",
    "start": "2017740",
    "end": "2024530"
  },
  {
    "text": "we're using SGD and it's kind of hovering above things there might be conditioning issues or even if you are",
    "start": "2024530",
    "end": "2030710"
  },
  {
    "text": "at a cell nearer saddle point you might be stuck even though it's not a local women because in many directions",
    "start": "2030710",
    "end": "2036620"
  },
  {
    "text": "it's still going up maybe you know 95% of the directions and and the other",
    "start": "2036620",
    "end": "2043400"
  },
  {
    "text": "directions are hard to reach because simply there's a lot more curvature in some directions and other directions and",
    "start": "2043400",
    "end": "2049280"
  },
  {
    "text": "that's you know the traditional ill conditioning problem we don't know exactly you know what what's making it",
    "start": "2049280",
    "end": "2056510"
  },
  {
    "text": "hard to try in some some networks usually continents are pretty easy to train but when you go into things like",
    "start": "2056510",
    "end": "2062300"
  },
  {
    "text": "machine translation or even worse reasoning tasks like with things like you know Turing machines and things like",
    "start": "2062300",
    "end": "2068149"
  },
  {
    "text": "that it gets really really hard to train these things and people have to use all kinds of tricks like curriculum learning which are essentially optimization",
    "start": "2068150",
    "end": "2075080"
  },
  {
    "text": "tricks to make the optimization easier so I don't want to tell you that all the",
    "start": "2075080",
    "end": "2080810"
  },
  {
    "text": "optimization problem of neural nets is easy it's done we don't need to worry about it but it's much easier and less",
    "start": "2080810",
    "end": "2087169"
  },
  {
    "text": "of a concern than what people thought in",
    "start": "2087170",
    "end": "2092330"
  },
  {
    "text": "the 90s ok so so was she learning I mean",
    "start": "2092330",
    "end": "2102740"
  },
  {
    "text": "deep learning is moving out of pattern recognition and into more complicated tasks for example including reasoning",
    "start": "2102740",
    "end": "2108710"
  },
  {
    "text": "and and and combining deep learning with reinforcement learning planning and things like that",
    "start": "2108710",
    "end": "2115100"
  },
  {
    "text": "you've heard about attention that's one of the tools that is really really",
    "start": "2115100",
    "end": "2120650"
  },
  {
    "text": "useful for many of these tasks we've sort of come up with attention",
    "start": "2120650",
    "end": "2128140"
  },
  {
    "text": "mechanisms as not a way to focus on what's going on in the outside will like",
    "start": "2128140",
    "end": "2133940"
  },
  {
    "text": "we usually think of attention like attention in the visual space but internal attention right in the space of",
    "start": "2133940",
    "end": "2139340"
  },
  {
    "text": "representations that have been built so that's what we do here in machine translation and it's been extremely",
    "start": "2139340",
    "end": "2146030"
  },
  {
    "text": "successful as quark said so I'm not going to show you any of these pictures",
    "start": "2146030",
    "end": "2151330"
  },
  {
    "text": "blah blah another so I'm getting more now into the domain of challenges a",
    "start": "2151330",
    "end": "2160100"
  },
  {
    "text": "challenge that I've been working on since I was a baby researcher as a PhD student is long-term dependencies and",
    "start": "2160100",
    "end": "2169180"
  },
  {
    "text": "recurrent Nets and although we've made a lot of progress this is still something",
    "start": "2169180",
    "end": "2175670"
  },
  {
    "text": "that we haven't completely cracked and it's connected to the optimization problem that I told you before but it's",
    "start": "2175670",
    "end": "2182090"
  },
  {
    "text": "a very particular kind of optimization problem so some of the ideas that we've",
    "start": "2182090",
    "end": "2188480"
  },
  {
    "text": "used to try to make the propagation of information and gradients easier include",
    "start": "2188480",
    "end": "2196240"
  },
  {
    "text": "using skip connections over time include using multiple time scales there's some",
    "start": "2196240",
    "end": "2203030"
  },
  {
    "text": "recent work in this direction from from my lab and other groups and even the",
    "start": "2203030",
    "end": "2208100"
  },
  {
    "text": "attention mechanism itself you can think of a way to help dealing with with long",
    "start": "2208100",
    "end": "2215510"
  },
  {
    "text": "term dependencies so the way to see this is to think of the place on which we're",
    "start": "2215510",
    "end": "2224420"
  },
  {
    "text": "putting attention as part of the state right so so imagine really you have a",
    "start": "2224420",
    "end": "2229580"
  },
  {
    "text": "recurrent net and it has two kinds of state it has the usual recurrent net",
    "start": "2229580",
    "end": "2234680"
  },
  {
    "text": "state but it has the content of the memory you know Kwok told you about memory nets and neural Cheng machines",
    "start": "2234680",
    "end": "2240370"
  },
  {
    "text": "and the full state really includes all of these things and and now we are able",
    "start": "2240370",
    "end": "2247790"
  },
  {
    "text": "to read or write from that memory I mean the little recurrent net is able to do that so what happens is that there are",
    "start": "2247790",
    "end": "2256600"
  },
  {
    "text": "memory elements which don't change or time maybe they're being written once",
    "start": "2256600",
    "end": "2262730"
  },
  {
    "text": "and and so the information that has been stored there it can stay for as much",
    "start": "2262730",
    "end": "2268460"
  },
  {
    "text": "time as you know they're not going to be overwritten so so that means that if you",
    "start": "2268460",
    "end": "2275750"
  },
  {
    "text": "consider the gradients back propagated through those cells they can go pretty much unhampered and there's no vanishing",
    "start": "2275750",
    "end": "2282770"
  },
  {
    "text": "gradient problem so this is something that to be that that view of the problem",
    "start": "2282770",
    "end": "2288770"
  },
  {
    "text": "of long-term dependence sieze with memory i think is could be very useful all right",
    "start": "2288770",
    "end": "2294089"
  },
  {
    "text": "in the last part of my presentation I want to tell you about what I think is the biggest challenge ahead of us which",
    "start": "2294089",
    "end": "2300599"
  },
  {
    "text": "is unsupervised learning any question about attention and memory before I move",
    "start": "2300599",
    "end": "2306119"
  },
  {
    "text": "on to and provides learning ok so why do",
    "start": "2306119",
    "end": "2315239"
  },
  {
    "text": "we care about unsupervised learning it's not working well actually it's working a",
    "start": "2315239",
    "end": "2324749"
  },
  {
    "text": "lot better than it was but it's still not something you find in industrial products at least not in an obvious way",
    "start": "2324749",
    "end": "2331789"
  },
  {
    "text": "there are less obvious ways where unsupervised learning is actually already extremely successful so for",
    "start": "2331789",
    "end": "2337170"
  },
  {
    "text": "example when you train word embeddings with word to Veck or any other model and you use that to pre train like we did",
    "start": "2337170",
    "end": "2342900"
  },
  {
    "text": "our machine translation systems or other kinds of NLP tasks you're you're exploiting as provides learning even",
    "start": "2342900",
    "end": "2350459"
  },
  {
    "text": "when you train a language model that you're going to stick in some other thing or pre train something with that",
    "start": "2350459",
    "end": "2357809"
  },
  {
    "text": "you're also doing unsupervised learning but I think the potential of and the",
    "start": "2357809",
    "end": "2366989"
  },
  {
    "text": "importance of ents provides learning is is usually underrated so why do we care",
    "start": "2366989",
    "end": "2375989"
  },
  {
    "text": "first of all the idea of ins provides learning is that we can train we can we can learn something from large",
    "start": "2375989",
    "end": "2381569"
  },
  {
    "text": "quantities of unlabeled data that humans have not curated and we have lots of",
    "start": "2381569",
    "end": "2386670"
  },
  {
    "text": "that humans are very good at learning",
    "start": "2386670",
    "end": "2392249"
  },
  {
    "text": "from unlabeled data I have an example",
    "start": "2392249",
    "end": "2397769"
  },
  {
    "text": "that I used often that is makes it very very clear that for example children can",
    "start": "2397769",
    "end": "2404039"
  },
  {
    "text": "learn all kinds of things about the world even though no one no no no adult",
    "start": "2404039",
    "end": "2410009"
  },
  {
    "text": "ever tells them anything about it until much later when is too late",
    "start": "2410009",
    "end": "2416130"
  },
  {
    "text": "physics so you know a two or three year old understands physics you know if she",
    "start": "2416130",
    "end": "2424900"
  },
  {
    "text": "has a ball she knows what's gonna happen when she drops the ball she knows you know how liquids behave she knows all",
    "start": "2424900",
    "end": "2431349"
  },
  {
    "text": "kinds of things about objects and an ordinary Newtonian physics even though",
    "start": "2431349",
    "end": "2437349"
  },
  {
    "text": "she doesn't have explicit equations and a way to destroy them with words but she can predict what's going to happen next",
    "start": "2437349",
    "end": "2443769"
  },
  {
    "text": "right and the parents don't tell the children you know force equals mass",
    "start": "2443769",
    "end": "2450130"
  },
  {
    "text": "times acceleration right so this is",
    "start": "2450130",
    "end": "2455950"
  },
  {
    "text": "purely unsupervised and it's very powerful we don't even have that right now we don't have computers that can",
    "start": "2455950",
    "end": "2461140"
  },
  {
    "text": "understand the kinds of physics that children can understand so it looks like",
    "start": "2461140",
    "end": "2467200"
  },
  {
    "text": "it's a skill that humans have and that's very important for humans to make sense",
    "start": "2467200",
    "end": "2473650"
  },
  {
    "text": "of the world around us but we haven't really yet succeeded to put in machines",
    "start": "2473650",
    "end": "2479670"
  },
  {
    "text": "let me tell you other reasons that are connected to this why unsupervised",
    "start": "2479670",
    "end": "2485079"
  },
  {
    "text": "learning to be useful when you do supervised learning essentially the way you train your system as you you you you",
    "start": "2485079",
    "end": "2491799"
  },
  {
    "text": "focus on a particular task those here's the inputs and here's the the input variables and here's an output variable",
    "start": "2491799",
    "end": "2497049"
  },
  {
    "text": "that I would like you to predict given the input your learning P of Y given X but if you're doing as provides learning",
    "start": "2497049",
    "end": "2503529"
  },
  {
    "text": "essentially you're learning about all the possible questions that could be asked about the data of your observe so",
    "start": "2503529",
    "end": "2510549"
  },
  {
    "text": "it's not that you know there's X 1 X 2 X 3 and Y everything is an X and you can",
    "start": "2510549",
    "end": "2515589"
  },
  {
    "text": "predict any of the X given any of the other X right if I give you a picture and I had a part of it you can guess",
    "start": "2515589",
    "end": "2521410"
  },
  {
    "text": "what's missing if I hide if I hide the you know the caption you can generate",
    "start": "2521410",
    "end": "2527589"
  },
  {
    "text": "the caption given the image if I hide hide the image and I give you the caption you can you can you know guess",
    "start": "2527589",
    "end": "2533799"
  },
  {
    "text": "what the image would be or draw it or figure out you know from examples which one is the most appropriate so you can",
    "start": "2533799",
    "end": "2539710"
  },
  {
    "text": "answer any questions about the data when you have captured the Joint Distribution between them essentially so that's that",
    "start": "2539710",
    "end": "2547749"
  },
  {
    "text": "could be useful another practical thing that ins",
    "start": "2547749",
    "end": "2554650"
  },
  {
    "text": "provides learning has been used in fact this is how the whole deep learning thing started is that it could be used",
    "start": "2554650",
    "end": "2559720"
  },
  {
    "text": "as a regular Iser because in addition to telling our model",
    "start": "2559720",
    "end": "2568240"
  },
  {
    "text": "that we want to predict Y given X we're saying find representations of X that",
    "start": "2568240",
    "end": "2575560"
  },
  {
    "text": "both predict Y and somehow capture something about the distribution of X",
    "start": "2575560",
    "end": "2581350"
  },
  {
    "text": "know the leading factors the explanatory factors of X and this again is making an",
    "start": "2581350",
    "end": "2587830"
  },
  {
    "text": "assumption about the data so we can use that as a regular Iser if the assumption is valid that the essentially the",
    "start": "2587830",
    "end": "2594220"
  },
  {
    "text": "assumption is that the factor Y that we're trying to predict is one of the",
    "start": "2594220",
    "end": "2599800"
  },
  {
    "text": "factors that explain X and that by doing this provides learning to discover factors that explain X we're going to",
    "start": "2599800",
    "end": "2606310"
  },
  {
    "text": "pick Y among the other factors and so it's going to be much easier now to do",
    "start": "2606310",
    "end": "2611350"
  },
  {
    "text": "supervised learning of course this is also the reason why transfer learning",
    "start": "2611350",
    "end": "2616570"
  },
  {
    "text": "works because there are underlying factors that explain the inputs for a",
    "start": "2616570",
    "end": "2623320"
  },
  {
    "text": "bunch of tasks and maybe a different subset of factors explained are relevant for one task and another subset of",
    "start": "2623320",
    "end": "2629710"
  },
  {
    "text": "factors is relevant for another task but if these factors overlap then there's a potential for synergy you know by doing",
    "start": "2629710",
    "end": "2637540"
  },
  {
    "text": "multi task learning so the reason multi task learning is working is because unsupervised learning is working is",
    "start": "2637540",
    "end": "2643480"
  },
  {
    "text": "because there are representations and factors that explain the data that can",
    "start": "2643480",
    "end": "2650470"
  },
  {
    "text": "be useful for our supervised learning tasks of interest that also could be",
    "start": "2650470",
    "end": "2656440"
  },
  {
    "text": "used for domain adaptation for the same reason um the other thing that people",
    "start": "2656440",
    "end": "2662110"
  },
  {
    "text": "don't talk about as much about unsupervised learning and I think it was",
    "start": "2662110",
    "end": "2667540"
  },
  {
    "text": "part of the initial success that we had with stacking auto-encoders and rbms is that you can actually make the",
    "start": "2667540",
    "end": "2674680"
  },
  {
    "text": "optimization problem of training deep nets easier because if you're gonna",
    "start": "2674680",
    "end": "2681010"
  },
  {
    "text": "you know for the most part if you're gonna train a bunch of RBMS or a bunch",
    "start": "2681010",
    "end": "2686599"
  },
  {
    "text": "of voto encoders and I'm not saying this is the right way of doing it but you know it captures some of the spirit of",
    "start": "2686599",
    "end": "2692059"
  },
  {
    "text": "what ins provides learning does a lot of the learning can be done locally you're trying to extract some information you're trying to discover some",
    "start": "2692059",
    "end": "2698630"
  },
  {
    "text": "dependencies that's that's a local thing once you have a slightly better representation we can again tweak it to extract better more independence or",
    "start": "2698630",
    "end": "2705380"
  },
  {
    "text": "something of that so so there's a sense in which the optimization problem might be easier if you have a very deep net",
    "start": "2705380",
    "end": "2713140"
  },
  {
    "text": "another reason why we should care about unsupervised learning even if our ultimate goal is to do supervised",
    "start": "2713140",
    "end": "2718490"
  },
  {
    "text": "learning is because sometimes the output variables are complicated",
    "start": "2718490",
    "end": "2725359"
  },
  {
    "text": "they are compositional they have a Joint Distribution so in machine translation which we talked about the output is a",
    "start": "2725359",
    "end": "2731930"
  },
  {
    "text": "sentence the sentence is a set of as a couple of words that have a complicated Joint Distribution given the input in",
    "start": "2731930",
    "end": "2738500"
  },
  {
    "text": "the other language and so it turns out that many of the things we discover by exploring unsupervised learning which is",
    "start": "2738500",
    "end": "2746000"
  },
  {
    "text": "essentially about capturing joint distributions can be often used to deal",
    "start": "2746000",
    "end": "2751910"
  },
  {
    "text": "with these structured output problems where you you have many outputs that form a you know compositional",
    "start": "2751910",
    "end": "2757640"
  },
  {
    "text": "complicated distribution there's another reason why unsupervised learning I think",
    "start": "2757640",
    "end": "2764000"
  },
  {
    "text": "is going to be really necessary for AI model-based reinforcement learning so I",
    "start": "2764000",
    "end": "2774099"
  },
  {
    "text": "think I have another slide just for this",
    "start": "2774099",
    "end": "2778270"
  },
  {
    "text": "let's think about self-driving cars is very popular topic these days how did I",
    "start": "2781539",
    "end": "2789440"
  },
  {
    "text": "learn that I shouldn't do some things with the wheel that will kill myself",
    "start": "2789440",
    "end": "2794720"
  },
  {
    "text": "right when I'm driving because I haven't experienced these states where I get",
    "start": "2794720",
    "end": "2800930"
  },
  {
    "text": "killed and I simply haven't done it like a thousand times to get learn how to avoid it",
    "start": "2800930",
    "end": "2806440"
  },
  {
    "text": "so supervised learning where we're our rather you know traditional",
    "start": "2806440",
    "end": "2812420"
  },
  {
    "text": "reinforcement learning like and policy learning kind of thing or",
    "start": "2812420",
    "end": "2819400"
  },
  {
    "text": "actor critic or things like that won't work because I need to generalize about",
    "start": "2819400",
    "end": "2828760"
  },
  {
    "text": "situations that I'm never going to encounter because otherwise if I did I would die so these are like dangerous",
    "start": "2828760",
    "end": "2836810"
  },
  {
    "text": "states that I need to generalize about these states but I you know can't have",
    "start": "2836810",
    "end": "2843890"
  },
  {
    "text": "enough data for them and and I'm sure there are lots of machine learning applications where we would be in that",
    "start": "2843890",
    "end": "2849440"
  },
  {
    "text": "situation I remember a couple of decades ago I you know I've got some data from",
    "start": "2849440",
    "end": "2854930"
  },
  {
    "text": "nuclear plant and so you know they wanted to predict that you know when it's gonna blow up to avoid it so I said",
    "start": "2854930",
    "end": "2865580"
  },
  {
    "text": "how many how many yeah it's at zero",
    "start": "2865580",
    "end": "2873609"
  },
  {
    "text": "right so you see sometimes it's hard to do supervised learning because the data",
    "start": "2873609",
    "end": "2879710"
  },
  {
    "text": "you would like to have you can't have it's it's it's data that you know situations that are very rare or you",
    "start": "2879710",
    "end": "2884750"
  },
  {
    "text": "know so how can we possibly solve this problem well the only solution I can see",
    "start": "2884750",
    "end": "2890210"
  },
  {
    "text": "is that we learn enough about the world that we can predict how things would",
    "start": "2890210",
    "end": "2895609"
  },
  {
    "text": "unfold right when I'm driving you know I have a kind of mental model of physics",
    "start": "2895609",
    "end": "2900830"
  },
  {
    "text": "and how cars behave that I can figure out you know if I turned right at this point I'm going to end up on the wall",
    "start": "2900830",
    "end": "2906830"
  },
  {
    "text": "and it's going to be very bad for me and I don't need to actually experience that to know that it's bad I can make a",
    "start": "2906830",
    "end": "2913580"
  },
  {
    "text": "mental simulation of what would happen so I need a kind of generative model of",
    "start": "2913580",
    "end": "2919180"
  },
  {
    "text": "how the world would unfold if I do such and such actions and unsupervised",
    "start": "2919180",
    "end": "2925460"
  },
  {
    "text": "learning is sort of the ideal thing to do that but of course it's going to be hard because we're going to have to",
    "start": "2925460",
    "end": "2930980"
  },
  {
    "text": "train models that capture a lot of aspects of the world in order to be able",
    "start": "2930980",
    "end": "2936800"
  },
  {
    "text": "to learn to generalize properly in those situations even though they don't see",
    "start": "2936800",
    "end": "2941869"
  },
  {
    "text": "any data of it so that's that's one reason why I think",
    "start": "2941869",
    "end": "2950049"
  },
  {
    "text": "reinforcement learning needs to be worked on more so I have a little thing",
    "start": "2950049",
    "end": "2958819"
  },
  {
    "text": "here I think people who have been doing deep learning can collaborate with",
    "start": "2958819",
    "end": "2965869"
  },
  {
    "text": "people who are doing reinforcement learning and not just by providing a black box that they can use in their",
    "start": "2965869",
    "end": "2971329"
  },
  {
    "text": "usual algorithms I think there are things that we do in supervised deep",
    "start": "2971329",
    "end": "2976400"
  },
  {
    "text": "learning that orange provides deep learning that can be useful in sort of",
    "start": "2976400",
    "end": "2982099"
  },
  {
    "text": "rethinking our enforcement learning so so one example also so well one thing I",
    "start": "2982099",
    "end": "2988999"
  },
  {
    "text": "really like to think about is credit assignment in other words how do",
    "start": "2988999",
    "end": "2994579"
  },
  {
    "text": "different machine learning algorithms figure out what the hidden units are supposed to do what the intermediate computations or the intermediate actions",
    "start": "2994579",
    "end": "3000999"
  },
  {
    "text": "should be this is what credit assignment is about and that prop is the best",
    "start": "3000999",
    "end": "3006999"
  },
  {
    "text": "recipe we currently have for doing credit assignment it tells the you know parameters of some intermediary should",
    "start": "3006999",
    "end": "3013689"
  },
  {
    "text": "change so that the costs much much later you know hundred steps later if it's a recurrent net should be reduced so we",
    "start": "3013689",
    "end": "3023859"
  },
  {
    "text": "could probably use some inspiration from backrub and how it's used to improve",
    "start": "3023859",
    "end": "3030269"
  },
  {
    "text": "reinforcement learning and one such cue is how when we do supervised backprop",
    "start": "3030269",
    "end": "3040089"
  },
  {
    "text": "say we don't predict the expected loss",
    "start": "3040089",
    "end": "3046599"
  },
  {
    "text": "that we're going to have and then try to minimize it where the expectation would",
    "start": "3046599",
    "end": "3052630"
  },
  {
    "text": "be over the different realizations of the correct class that's not what we do but this is what people do in RL they",
    "start": "3052630",
    "end": "3060009"
  },
  {
    "text": "they will learn a critic or a cue function which is the expected learning",
    "start": "3060009",
    "end": "3066789"
  },
  {
    "text": "the expected value of the future reward or the future loss in our case that might be you know minus log probability",
    "start": "3066789",
    "end": "3072219"
  },
  {
    "text": "of the correct answer given the input and then they will backdrop",
    "start": "3072219",
    "end": "3078010"
  },
  {
    "text": "through this or use it to estimate the gradient on the actions instead when we",
    "start": "3078010",
    "end": "3086500"
  },
  {
    "text": "when we do supervised learning we're going to do credit assignment where we",
    "start": "3086500",
    "end": "3093190"
  },
  {
    "text": "use the particular observations of the correct class that actually happened for this X right we have X we have Y and we",
    "start": "3093190",
    "end": "3100900"
  },
  {
    "text": "use the Y to figure out what how to change our prediction or action so it",
    "start": "3100900",
    "end": "3108160"
  },
  {
    "text": "looks like this is something that should be done for our L and in fact we we have",
    "start": "3108160",
    "end": "3115750"
  },
  {
    "text": "a paper on something like this for a sequence prediction this is this is the",
    "start": "3115750",
    "end": "3121270"
  },
  {
    "text": "kind of work which is at the intersection of dealing with structured outputs reinforcement learning and",
    "start": "3121270",
    "end": "3126670"
  },
  {
    "text": "service learning so I think there's a lot of potential benefit of changing the",
    "start": "3126670",
    "end": "3134280"
  },
  {
    "text": "frame of thinking that people in the RL have had for many decades people in RL I mean not thinking about the world in",
    "start": "3134280",
    "end": "3141910"
  },
  {
    "text": "with the same eyes as people doing your net they've been thinking about the world in terms of discrete states that",
    "start": "3141910",
    "end": "3148690"
  },
  {
    "text": "could be enumerated and proving theorems about these algorithms that depend on",
    "start": "3148690",
    "end": "3154710"
  },
  {
    "text": "essentially you know collecting enough data to fill all the possible configurations of the state and their",
    "start": "3154710",
    "end": "3160230"
  },
  {
    "text": "you know the corresponding effects on the reward when you start thinking in",
    "start": "3160230",
    "end": "3166000"
  },
  {
    "text": "terms of neural nets and deep learning the way to approach problems is very very different okay let me continue",
    "start": "3166000",
    "end": "3174369"
  },
  {
    "text": "about as provides learning and why this is so important if you look at the kinds",
    "start": "3174369",
    "end": "3180940"
  },
  {
    "text": "of mistakes that our current machine learning algorithms make you find that",
    "start": "3180940",
    "end": "3186640"
  },
  {
    "text": "our our neural nets are just cheating they're using the wrong cues to try to",
    "start": "3186640",
    "end": "3193420"
  },
  {
    "text": "produce the answers and sometimes it works sometimes it doesn't work so how can we make our our models be you know",
    "start": "3193420",
    "end": "3203290"
  },
  {
    "text": "smarter make less mistakes well",
    "start": "3203290",
    "end": "3207630"
  },
  {
    "text": "the only solution is to make sure that those models really understand how the",
    "start": "3210290",
    "end": "3215490"
  },
  {
    "text": "world works at least at the level of humans to get human level accuracy human level performance it may be not",
    "start": "3215490",
    "end": "3223740"
  },
  {
    "text": "necessary to do this for a particular problem you're trying to solve so maybe we can you know get away with doing",
    "start": "3223740",
    "end": "3228930"
  },
  {
    "text": "speech recognition without really understanding of the meaning of the words probably that's going to be okay",
    "start": "3228930",
    "end": "3235830"
  },
  {
    "text": "but for other tasks especially those involving language I think having models",
    "start": "3235830",
    "end": "3241680"
  },
  {
    "text": "that actually understand how the world tix is going to be very very important",
    "start": "3241680",
    "end": "3246619"
  },
  {
    "text": "to so how could we have machines that understand how the world works well one",
    "start": "3247940",
    "end": "3253650"
  },
  {
    "text": "of the ideas that I've been talking a lot about in the last decade is that of",
    "start": "3253650",
    "end": "3259230"
  },
  {
    "text": "disentangling factors of variation this is related to a very old idea in pattern",
    "start": "3259230",
    "end": "3264660"
  },
  {
    "text": "recognition computer vision called invariance the idea of invariance was that we would like to compute or design",
    "start": "3264660",
    "end": "3272160"
  },
  {
    "text": "initially design and now learn features say of the image that are invariant to",
    "start": "3272160",
    "end": "3278010"
  },
  {
    "text": "the things we don't care about maybe we want to do object recognition so we don't care about position or orientation",
    "start": "3278010",
    "end": "3283110"
  },
  {
    "text": "so we would like to have features that are translation invariant rotation invariant scaling invariant whatever so",
    "start": "3283110",
    "end": "3289590"
  },
  {
    "text": "this is what invariance is about but when you're in the business of doing ends provides learning of trying to figure out how the world works it's not",
    "start": "3289590",
    "end": "3296190"
  },
  {
    "text": "good enough to do two extracting variant features what we actually want to do is to extract all of the factors that",
    "start": "3296190",
    "end": "3303090"
  },
  {
    "text": "explain the data so if we're doing speech recognition we want not only to extract the phonemes but we also want to",
    "start": "3303090",
    "end": "3310050"
  },
  {
    "text": "figure out you know what kind of voice is that maybe who is it what kind of",
    "start": "3310050",
    "end": "3315530"
  },
  {
    "text": "recording conditions or what kind of microphone is it in a car is it outside all that information which you're trying",
    "start": "3315530",
    "end": "3321870"
  },
  {
    "text": "to get rid of normally you actually want to learn about so that you'll be able to",
    "start": "3321870",
    "end": "3329340"
  },
  {
    "text": "generalize even to new tasks for example maybe the next day I'm not going to ask you to recognize phonemes but recognize who's speaking more generally if we're",
    "start": "3329340",
    "end": "3338160"
  },
  {
    "text": "able to disentangle these that explained how the data varies everything becomes easy especially if",
    "start": "3338160",
    "end": "3345940"
  },
  {
    "text": "those factors now can be generated in an independent way and to generate the data",
    "start": "3345940",
    "end": "3352440"
  },
  {
    "text": "we we can for example we can learn to",
    "start": "3352440",
    "end": "3357850"
  },
  {
    "text": "answer a question that only depends on one or two factors and basically we eliminate all the other ones because we've separated them so a lot of things",
    "start": "3357850",
    "end": "3365500"
  },
  {
    "text": "become much easier so that's one notion right we can design tangle factors",
    "start": "3365500",
    "end": "3370690"
  },
  {
    "text": "there's another notion which is the notion of multiple levels of abstraction which is of course at the heart of what",
    "start": "3370690",
    "end": "3376810"
  },
  {
    "text": "we're trying to do with deep learning and the idea is that we can have",
    "start": "3376810",
    "end": "3384510"
  },
  {
    "text": "representations of the world representation of the data as you know",
    "start": "3384510",
    "end": "3390610"
  },
  {
    "text": "description that involves factors are features and we can do that at multiple",
    "start": "3390610",
    "end": "3399310"
  },
  {
    "text": "levels and there are more abstract levels so if I'm looking at a document",
    "start": "3399310",
    "end": "3406530"
  },
  {
    "text": "you know there's the level of the pixels the level of the strokes the level of the characters the level of the words",
    "start": "3406530",
    "end": "3412270"
  },
  {
    "text": "and maybe the level of the meaning of individual words and we actually have you know systems that will recognize",
    "start": "3412270",
    "end": "3417880"
  },
  {
    "text": "from a scanned document all of these levels when we go higher up we're not",
    "start": "3417880",
    "end": "3422890"
  },
  {
    "text": "sure what the right levels are but clearly there must be representations of the meaning not just of single words but",
    "start": "3422890",
    "end": "3427900"
  },
  {
    "text": "of you know sequences of words and the whole paragraph what's the story and why is it important to represent things in",
    "start": "3427900",
    "end": "3433900"
  },
  {
    "text": "that way because higher levels of abstraction are representations from",
    "start": "3433900",
    "end": "3441760"
  },
  {
    "text": "which it is much easier to do things to answer questions so the the more",
    "start": "3441760",
    "end": "3448300"
  },
  {
    "text": "semantic levels mean basically we can very easily act on the information when it's represented that way if you think",
    "start": "3448300",
    "end": "3455800"
  },
  {
    "text": "about the level of words it's much easier to check whether a particular word is in the document if I have the words extracted then if I have to do it",
    "start": "3455800",
    "end": "3462130"
  },
  {
    "text": "from the pixels and if I have to answer a complicated question about you know the intention of the person working at",
    "start": "3462130",
    "end": "3469450"
  },
  {
    "text": "level of words is not high enough it's not abstract enough I need to work at a more abstract level which in which maybe the",
    "start": "3469450",
    "end": "3476380"
  },
  {
    "text": "same notion could be represented with many different types of words where many",
    "start": "3476380",
    "end": "3481480"
  },
  {
    "text": "different sentences could express the same meaning and I want to be able to capture that meaning so the last slide I",
    "start": "3481480",
    "end": "3490510"
  },
  {
    "text": "have is something that I've been working on in the last couple of years which is",
    "start": "3490510",
    "end": "3498400"
  },
  {
    "text": "trying to which is connected to ants provides learning but more generally to",
    "start": "3498400",
    "end": "3503760"
  },
  {
    "text": "the relationship between how we can build intelligent machines and and the",
    "start": "3503760",
    "end": "3510640"
  },
  {
    "text": "intelligence of humans or animals and as you may know this was one of the key",
    "start": "3510640",
    "end": "3517440"
  },
  {
    "text": "motivations for doing neural nets in the first place the intuition is this that",
    "start": "3517440",
    "end": "3523680"
  },
  {
    "text": "we are hoping that there are a few simple key principles that explain you",
    "start": "3523680",
    "end": "3533980"
  },
  {
    "text": "know what allows us to be intelligent and that if we can discover these principles of course we can also build",
    "start": "3533980",
    "end": "3540910"
  },
  {
    "text": "machines that are intelligent that's why the neural nets were you know inspired",
    "start": "3540910",
    "end": "3547630"
  },
  {
    "text": "by things we know from the brain in the first place we don't know this is true",
    "start": "3547630",
    "end": "3552910"
  },
  {
    "text": "but if it is then you know it's it's it's great and I mean this would make it",
    "start": "3552910",
    "end": "3560530"
  },
  {
    "text": "much easier to understand how brains work as well as building AI so in in",
    "start": "3560530",
    "end": "3568330"
  },
  {
    "text": "trying to bridge this gap because right now our best neural nets are very very different from what's going on in brains",
    "start": "3568330",
    "end": "3574359"
  },
  {
    "text": "as far as you know we can tell by talking to neuro scientists in",
    "start": "3574359",
    "end": "3579550"
  },
  {
    "text": "particular backprop although it's it's kicking",
    "start": "3579550",
    "end": "3586270"
  },
  {
    "text": "Assam from a machine learning point of view it's not clear at all how something like this would be implemented in brains",
    "start": "3586270",
    "end": "3592480"
  },
  {
    "text": "so I've been trying to explore that and and also trying to see how we could",
    "start": "3592480",
    "end": "3599470"
  },
  {
    "text": "generalize those credit assignment principles that would come out in order",
    "start": "3599470",
    "end": "3605800"
  },
  {
    "text": "to also do once provide learning so we've we've made a little bit of",
    "start": "3605800",
    "end": "3612520"
  },
  {
    "text": "progress a couple of years ago I came up with an idea called target prop which is",
    "start": "3612520",
    "end": "3619480"
  },
  {
    "text": "a way of generalizing back prop 2 propagating targets for each layer of",
    "start": "3619480",
    "end": "3628000"
  },
  {
    "text": "course this idea has a long history more",
    "start": "3628000",
    "end": "3634150"
  },
  {
    "text": "recently we've been looking at ways to",
    "start": "3634150",
    "end": "3639460"
  },
  {
    "text": "implement gradient estimation in deep recurrent networks that perform some",
    "start": "3639460",
    "end": "3646359"
  },
  {
    "text": "computation that turn out to end up with",
    "start": "3646359",
    "end": "3651750"
  },
  {
    "text": "parameter updates corresponding to gradient descent in the prediction error that looked like something that",
    "start": "3651750",
    "end": "3659369"
  },
  {
    "text": "neuroscientists have been observing and and don't completely understand called SCDP spike timing-dependent plasticity",
    "start": "3659369",
    "end": "3665230"
  },
  {
    "text": "so I don't really have time to go into this but I think this whole area of",
    "start": "3665230",
    "end": "3672630"
  },
  {
    "text": "reconnecting neuroscience with machine learning and neural nets is something",
    "start": "3672630",
    "end": "3678880"
  },
  {
    "text": "that has been kind of forgotten by the the machining community because we're all so busy you know building self-driving cars but I think over the",
    "start": "3678880",
    "end": "3688960"
  },
  {
    "text": "long term it's a it's a very exciting prospect thank you very much",
    "start": "3688960",
    "end": "3697349"
  },
  {
    "text": "yes questions yeah",
    "start": "3705260",
    "end": "3712520"
  },
  {
    "text": "to begin with great talk my question is regarding you know the lack of interlab",
    "start": "3719230",
    "end": "3724690"
  },
  {
    "text": "between the results in the study of complex networks like when they study",
    "start": "3724690",
    "end": "3730210"
  },
  {
    "text": "the brain networks right there lot of publications which that talk about the emergence of hubs and especially a lot",
    "start": "3730210",
    "end": "3737020"
  },
  {
    "text": "of publications on the degree distribution of the inter neuron Network right but then when you look at the",
    "start": "3737020",
    "end": "3743260"
  },
  {
    "text": "degree distribution of the so-called neurons in deep Nets you don't get to",
    "start": "3743260",
    "end": "3749020"
  },
  {
    "text": "see the emergence of the hub behavior so right why do you think that there's such lack of overlap between like because I",
    "start": "3749020",
    "end": "3756790"
  },
  {
    "text": "think the hop story is maybe not that important first of all I really think",
    "start": "3756790",
    "end": "3765760"
  },
  {
    "text": "that in order to understand the brain we have to understand learning in the brain and if we look at our experience in",
    "start": "3765760",
    "end": "3773290"
  },
  {
    "text": "machine learning and deep learning although the architecture does matter you know what matters even more is the",
    "start": "3773290",
    "end": "3781420"
  },
  {
    "text": "general principles that allow us to train these these things so I think the",
    "start": "3781420",
    "end": "3787300"
  },
  {
    "text": "the study of the connectivity makes sense you can't have a you know fully",
    "start": "3787300",
    "end": "3792340"
  },
  {
    "text": "connected thing and having a way to have a short number of hops to go from anywhere to anywhere is a reasonable",
    "start": "3792340",
    "end": "3798940"
  },
  {
    "text": "idea but it's it I don't think it really",
    "start": "3798940",
    "end": "3804280"
  },
  {
    "text": "explains that much that the the central question is how does the brain learn",
    "start": "3804280",
    "end": "3809320"
  },
  {
    "text": "complicated things and it does it better than then our current machines yet we we",
    "start": "3809320",
    "end": "3818410"
  },
  {
    "text": "don't know even a simple way of training brains that that at least fits the",
    "start": "3818410",
    "end": "3824770"
  },
  {
    "text": "biology reasonably yeah there are any cases with real war examples where the",
    "start": "3824770",
    "end": "3831609"
  },
  {
    "text": "cursive of dimensionality is still a problem for neural nets yeah any time it",
    "start": "3831609",
    "end": "3838450"
  },
  {
    "text": "doesn't work I mean from a generalisation point of view so Andrew told us yesterday that we",
    "start": "3838450",
    "end": "3849609"
  },
  {
    "text": "can just add more data and computing power and for some problems this may work but sometimes the amount of data",
    "start": "3849609",
    "end": "3856030"
  },
  {
    "text": "you would need is just you know too large with our current techniques and",
    "start": "3856030",
    "end": "3862260"
  },
  {
    "text": "you know we'll need also to develop you know the how did you call it the Hail",
    "start": "3862260",
    "end": "3868270"
  },
  {
    "text": "Mary all right we also need to do some research on the algorithms and the",
    "start": "3868270",
    "end": "3873549"
  },
  {
    "text": "architectures to be able to learn about",
    "start": "3873549",
    "end": "3878680"
  },
  {
    "text": "how the world is organized so that we can generalize in much more powerful ways and that is needed because the the",
    "start": "3878680",
    "end": "3888549"
  },
  {
    "text": "kind of tasks we want to solve involved in many many variables that have an explanation umber of possible values and",
    "start": "3888549",
    "end": "3894369"
  },
  {
    "text": "that's the curse of dimensionality essentially so it's facing you know pretty much all of the AI problems",
    "start": "3894369",
    "end": "3901299"
  },
  {
    "text": "around us all right the question on multi-agent reinforcement learning yeah",
    "start": "3901299",
    "end": "3908529"
  },
  {
    "text": "if you assume all cars can never predict all possible potential accidents what",
    "start": "3908529",
    "end": "3915430"
  },
  {
    "text": "about the potential for transfer learning and things like that yeah so so",
    "start": "3915430",
    "end": "3920829"
  },
  {
    "text": "I was giving an example of a single human learning how to drive we might be",
    "start": "3920829",
    "end": "3927880"
  },
  {
    "text": "able to use you know the millions of people's you know using self-driving cars correcting and some of them making",
    "start": "3927880",
    "end": "3933430"
  },
  {
    "text": "accidents to actually make some progress without actually solving the hard problems and this is probably going to",
    "start": "3933430",
    "end": "3939160"
  },
  {
    "text": "be doing for a while but and we should do it we should definitely use all the data we have currently if you look at",
    "start": "3939160",
    "end": "3946059"
  },
  {
    "text": "the amount of data were using for speech recognition or language modeling it's hugely more than what any human you know",
    "start": "3946059",
    "end": "3953109"
  },
  {
    "text": "actually sees in their lifetime so we're doing something wrong and we could do",
    "start": "3953109",
    "end": "3959140"
  },
  {
    "text": "better with less data and babies and kids you know can do it yes well there's",
    "start": "3959140",
    "end": "3979390"
  },
  {
    "text": "quite a bit of work on video these days it's mostly a computational bottleneck",
    "start": "3979390",
    "end": "3985769"
  },
  {
    "text": "yeah well keep in mind we're doing em this just a couple of years ago yeah",
    "start": "3988770",
    "end": "3999130"
  },
  {
    "text": "absolutely yeah I don't think it's a fundamental issue if if we're able to do it well on",
    "start": "3999130",
    "end": "4006839"
  },
  {
    "text": "static images the same principles will you know allow us to do sequences we're",
    "start": "4006839",
    "end": "4012270"
  },
  {
    "text": "already doing you know sequential things for example an interesting project is",
    "start": "4012270",
    "end": "4018720"
  },
  {
    "text": "speech synthesis with recurrent nets and stuff like that or convolutional nets",
    "start": "4018720",
    "end": "4023819"
  },
  {
    "text": "whatever so it's more like we're not sure how to train them well and how to",
    "start": "4023819",
    "end": "4030480"
  },
  {
    "text": "discover these explanatory factors and so on that's my cue",
    "start": "4030480",
    "end": "4036440"
  },
  {
    "text": "yeah I have a question maybe non-technical so we have seen the human error rates and then the versus the our",
    "start": "4036440",
    "end": "4043470"
  },
  {
    "text": "algorithms error rates for things that are we are used to like image recognition speech recognition right",
    "start": "4043470",
    "end": "4049440"
  },
  {
    "text": "right those are any beena experiments where we try to train humans for things that we are not used to right I'm not",
    "start": "4049440",
    "end": "4056190"
  },
  {
    "text": "trained the machine at the same time see right so this how capable are algorithms",
    "start": "4056190",
    "end": "4061349"
  },
  {
    "text": "you're asking if these experiments have been done yeah I don't know but I'm I'm",
    "start": "4061349",
    "end": "4067170"
  },
  {
    "text": "sure the humans would beat the hell out of the machines for now for this kind of thing humans are able to learn a new",
    "start": "4067170",
    "end": "4074309"
  },
  {
    "text": "task or new concepts from very few examples and we know that in order for",
    "start": "4074309",
    "end": "4079680"
  },
  {
    "text": "machines to do to do as well they they just need more sort of common sense right more general knowledge of the",
    "start": "4079680",
    "end": "4085680"
  },
  {
    "text": "world this is what allows humans to learn so quickly on on a few examples yeah you presented experimental data",
    "start": "4085680",
    "end": "4093690"
  },
  {
    "text": "very showed that lots of local minimum for these parameters or maybe saddle saddle points",
    "start": "4093690",
    "end": "4099270"
  },
  {
    "text": "have similar performance yeah are these local minima that's there locally right are these local minima separated widely",
    "start": "4099270",
    "end": "4107609"
  },
  {
    "text": "in parameter space are they close by that's a good question I could I guess a related question is once you claim the",
    "start": "4107609",
    "end": "4113640"
  },
  {
    "text": "network if there are lots of local minima does that suggest that you could compress the network and represent it",
    "start": "4113640",
    "end": "4119970"
  },
  {
    "text": "with far fewer parameters maybe so for your first question we have",
    "start": "4119970",
    "end": "4128520"
  },
  {
    "text": "some experiments dating from 2009 where we try to visualize in 2d the",
    "start": "4128520",
    "end": "4136759"
  },
  {
    "text": "trajectories of training so this is a paper first author is dumitru Aaron former PhD students with me where we we",
    "start": "4136760",
    "end": "4146370"
  },
  {
    "text": "wanted to see how different depending on where you start you know where do you end up dude different trajectories end",
    "start": "4146370",
    "end": "4153870"
  },
  {
    "text": "up in the same place or do they all go in a different place it turns out they all go in a different place and so the",
    "start": "4153870",
    "end": "4159838"
  },
  {
    "text": "number of local minima is much larger than the number of trajectories that we tried like 500 or a thousand it's so",
    "start": "4159839",
    "end": "4166710"
  },
  {
    "text": "much larger that you know no two random see initial seeds end up near each other",
    "start": "4166710",
    "end": "4171990"
  },
  {
    "text": "so it looks like there's a huge number of local minimum which is in agreement with the theory that there's an",
    "start": "4171990",
    "end": "4177480"
  },
  {
    "text": "exponential number of them but the good news is they're all kind of equivalent in terms of cost if you have a large",
    "start": "4177480",
    "end": "4183088"
  },
  {
    "text": "network I'm not sure I'm sure there are",
    "start": "4183089",
    "end": "4194310"
  },
  {
    "text": "many ways to compress these networks there's a lot of redundancy in many ways",
    "start": "4194310",
    "end": "4199370"
  },
  {
    "text": "there are there are redundancies due to",
    "start": "4199370",
    "end": "4204450"
  },
  {
    "text": "the numbering like you could flip all you know take that unit put it here to",
    "start": "4204450",
    "end": "4210390"
  },
  {
    "text": "get you and put it here and so on but I don't think you're going to gain a lot of bits from that so we've talked",
    "start": "4210390",
    "end": "4218880"
  },
  {
    "text": "about that one of the main advantages of deep learning is that they it can work with lots of data and but you were",
    "start": "4218880",
    "end": "4226140"
  },
  {
    "text": "mentioning before that we need also to capture the ability of humans of working",
    "start": "4226140",
    "end": "4231360"
  },
  {
    "text": "with a few data but the reason we're able to work with fewer data is because we have first",
    "start": "4231360",
    "end": "4237370"
  },
  {
    "text": "learned from a lot of data about you know the general knowledge of the world",
    "start": "4237370",
    "end": "4242920"
  },
  {
    "text": "right so how can we adapt neural networks to bring us to this new fuel",
    "start": "4242920",
    "end": "4250989"
  },
  {
    "text": "data paradigm we have to do a lot better add-ons provides learning and of the",
    "start": "4250989",
    "end": "4257380"
  },
  {
    "text": "kind that really discovers sort of explanations about the world that's what I think let's say thank you again so",
    "start": "4257380",
    "end": "4276210"
  },
  {
    "text": "before we stop this workshop first an announcement",
    "start": "4276210",
    "end": "4281890"
  },
  {
    "text": "you might remember yesterday Carl invited all the women here for an",
    "start": "4281890",
    "end": "4287020"
  },
  {
    "text": "informal dinner it's going to be right outside right now after we close so",
    "start": "4287020",
    "end": "4293440"
  },
  {
    "text": "before we close actually I'd like to thank all the the speaker today and yesterday I think everybody appreciated",
    "start": "4293440",
    "end": "4300580"
  },
  {
    "text": "their talk so thanks again all of you",
    "start": "4300580",
    "end": "4305190"
  },
  {
    "text": "and thanks to all the attendants I think it was a very nice weekend hope you enjoyed",
    "start": "4309449",
    "end": "4316670"
  }
]