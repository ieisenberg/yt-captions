[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "sound is good okay great so I wanted to talk to you about unsupervised learning and that's the area where there's been a",
    "start": "60",
    "end": "6569"
  },
  {
    "text": "lot of research but compared to supervised learning that you've heard about today like convolutional networks",
    "start": "6569",
    "end": "13110"
  },
  {
    "text": "you know unsupervised learning is not very yet alright so I'm going to show you lots of lots of areas parts of the",
    "start": "13110",
    "end": "20820"
  },
  {
    "text": "talk are going to be a little bit more mathematical I apologize for that but I'll try to give you gist off of the",
    "start": "20820",
    "end": "27300"
  },
  {
    "text": "foundations the math behind these models as well as try to highlight some some of the application areas okay what's the",
    "start": "27300",
    "end": "34800"
  },
  {
    "text": "motivation well the motivation is that you know the space of data that we have today is is just growing right you know",
    "start": "34800",
    "end": "42149"
  },
  {
    "text": "if you look at the space of images you know speech if you look at social network data if you look at scientific",
    "start": "42149",
    "end": "48660"
  },
  {
    "text": "data I would argue that most of the data that we see today is unlabeled right so",
    "start": "48660",
    "end": "56489"
  },
  {
    "text": "how can we develop statistical models models that can discover interesting kind of structure in unsupervised way or",
    "start": "56489",
    "end": "63390"
  },
  {
    "text": "semi-supervised way and that's what I'm interested in as well as how can we sort",
    "start": "63390",
    "end": "68400"
  },
  {
    "text": "of apply these models across multiple different multiple different domains and one particular framework of doing that",
    "start": "68400",
    "end": "75090"
  },
  {
    "text": "is the framework of deep learning where you're trying to learn hierarchical representations of data and and again as",
    "start": "75090",
    "end": "81930"
  },
  {
    "text": "we go as I go through the talk I'm going to show you some some examples I've tried so here's here's one example you",
    "start": "81930",
    "end": "89579"
  },
  {
    "start": "87000",
    "end": "87000"
  },
  {
    "text": "know you can take a simple bag of words representation of an article or a newspaper you can use something that's",
    "start": "89579",
    "end": "97680"
  },
  {
    "text": "called an autoencoder just multiple levels you extract some latent code and",
    "start": "97680",
    "end": "104040"
  },
  {
    "text": "then you get some representation out of it right and this is done completely in unsupervised way you don't provide any",
    "start": "104040",
    "end": "109380"
  },
  {
    "text": "labels and if you look at the kind of structure that the model discovering you know it could be useful for visualization for example to see what's",
    "start": "109380",
    "end": "116670"
  },
  {
    "text": "what kind of structure you you see in your data this was done on the on the Reuters data set I've tried to kind of",
    "start": "116670",
    "end": "125750"
  },
  {
    "text": "cluster together lots of different a supervised learning techniques and I'll touch on some of them it's a little bit",
    "start": "125750",
    "end": "132150"
  },
  {
    "text": "you know it's it's not full set but the way that I typically think about these models is that there",
    "start": "132150",
    "end": "137970"
  },
  {
    "text": "is a class of what I would call non probabilistic models you know models like sparse coding auto-encoders",
    "start": "137970",
    "end": "145220"
  },
  {
    "text": "clustering based methods and these are all very very powerful powerful",
    "start": "145220",
    "end": "150299"
  },
  {
    "text": "techniques and I'll cover some of them in that talk as well and then there is sort of a space of probabilistic models",
    "start": "150299",
    "end": "158099"
  },
  {
    "text": "and within probably stick models you have tractable models you know things like fully observed belief networks",
    "start": "158099",
    "end": "164510"
  },
  {
    "text": "there's a beautiful class of models called neural auto regressive density estimators more recently we've seen some",
    "start": "164510",
    "end": "171359"
  },
  {
    "text": "successes of so-called pixel recurrent neural network models or and I'll show",
    "start": "171359",
    "end": "179129"
  },
  {
    "text": "you some examples of that there is a class of so called intractable models where you know you are looking at models",
    "start": "179129",
    "end": "185549"
  },
  {
    "text": "like Boltzmann machines and models like variation water encoders something that's been quite there's been a lot of",
    "start": "185549",
    "end": "191879"
  },
  {
    "text": "development in our community and deep learning community in that space Helmholtz machines I'll tell you a",
    "start": "191879",
    "end": "197370"
  },
  {
    "text": "little bit about what these models are and a whole bunch of others as well right one particular structure within",
    "start": "197370",
    "end": "204630"
  },
  {
    "text": "these models is that when you're building these generative models or of data you typically have to specify what",
    "start": "204630",
    "end": "211109"
  },
  {
    "text": "the distributions you're looking at so you have to specify what the probability of the data and generally doing some",
    "start": "211109",
    "end": "216959"
  },
  {
    "text": "kind of approximate maximum likelihood estimation and then more recently you know we've seen some very exciting",
    "start": "216959",
    "end": "222359"
  },
  {
    "text": "models coming out these are generative adversarial networks moment matching networks and",
    "start": "222359",
    "end": "228449"
  },
  {
    "text": "this is sort of a slightly different class of models where you don't really have to specify what the density is you",
    "start": "228449",
    "end": "235169"
  },
  {
    "text": "just need to be able to sample from those models and I'm going to show you some some examples of that okay so my",
    "start": "235169",
    "end": "242129"
  },
  {
    "start": "242000",
    "end": "242000"
  },
  {
    "text": "talk is going to be sort of structured though I'd like to introduce you to the basic building blocks models like sparse",
    "start": "242129",
    "end": "248970"
  },
  {
    "text": "coding models because I think that is a very important class of models particularly for folks who are working",
    "start": "248970",
    "end": "254160"
  },
  {
    "text": "in in industry and looking for simpler models autoencoder is a beautiful class",
    "start": "254160",
    "end": "259919"
  },
  {
    "text": "of models and then the second part of the talk I'll focus more on on generative models",
    "start": "259919",
    "end": "265169"
  },
  {
    "text": "I'll give you an introduction on into restricted Boltzmann machines and deep both machines these are sort of model statistical models that can model",
    "start": "265169",
    "end": "274370"
  },
  {
    "text": "so complicated complicated data and I'll spend some time showing you some",
    "start": "274370",
    "end": "280590"
  },
  {
    "text": "examples some recent developments in our community specifically in the case of variation autoencoders which is I view",
    "start": "280590",
    "end": "286590"
  },
  {
    "text": "them as a subclass of Helmholtz machines and I'll finish off by giving you an intuition about you know a slightly",
    "start": "286590",
    "end": "293340"
  },
  {
    "text": "different class of models which would be these generative adversarial networks okay so let's let's jump into the first",
    "start": "293340",
    "end": "300570"
  },
  {
    "text": "part but before I do that let me just sort of give you a little bit of motivation and know Angie's done a great",
    "start": "300570",
    "end": "306660"
  },
  {
    "start": "301000",
    "end": "301000"
  },
  {
    "text": "job and Richard sort of alluded to that as well but the idea is you know if I'm",
    "start": "306660",
    "end": "313050"
  },
  {
    "text": "trying to classify a particularly image right and if I say well you know if I'm looking specific pixel representation",
    "start": "313050",
    "end": "318270"
  },
  {
    "text": "might be difficult for me to classify what I'm seeing right on the other hand if I can find the right representations",
    "start": "318270",
    "end": "325740"
  },
  {
    "text": "right the right representations for these images and then I sort of get the right features so get the right",
    "start": "325740",
    "end": "331710"
  },
  {
    "text": "structure from the data that it might be easier for me to you know see what's what's going on with my data right so",
    "start": "331710",
    "end": "339000"
  },
  {
    "text": "how do I find these representations and this is this is sort of one of",
    "start": "339000",
    "end": "344630"
  },
  {
    "start": "340000",
    "end": "340000"
  },
  {
    "text": "traditional approaches that we've seen for a long time is that you know you have a data you're creating some",
    "start": "344630",
    "end": "350520"
  },
  {
    "text": "features and then you run in your learning algorithm and for the longest time an object recognition or an audio",
    "start": "350520",
    "end": "356460"
  },
  {
    "text": "classification you typically use some kind of hand design features and then you start classifying what you have and",
    "start": "356460",
    "end": "363440"
  },
  {
    "start": "362000",
    "end": "362000"
  },
  {
    "text": "you know like Andre was saying in the space of vision it's been a lot of different features designs of what's the",
    "start": "363440",
    "end": "372539"
  },
  {
    "text": "right structure we should see in the data in the space of audio same thing is",
    "start": "372539",
    "end": "379349"
  },
  {
    "start": "377000",
    "end": "377000"
  },
  {
    "text": "happening how can you find these right representations for your for your data",
    "start": "379349",
    "end": "384479"
  },
  {
    "text": "and the idea behind their presentation learning in particular in deep learning",
    "start": "384479",
    "end": "391560"
  },
  {
    "text": "is can we actually learn these representations automatically right and more importantly can we actually learn",
    "start": "391560",
    "end": "396659"
  },
  {
    "text": "these representations in unsupervised way right by just seeing lots and lots of unlabeled can we achieve that and you know there's",
    "start": "396659",
    "end": "404760"
  },
  {
    "text": "been a lot of work done in that space but we're not there yet so I wanted to sort of lower your expectations as as I",
    "start": "404760",
    "end": "410850"
  },
  {
    "text": "show you some some of the results ok sparse coding this is one of the models",
    "start": "410850",
    "end": "417360"
  },
  {
    "text": "that I think that everybody should know what it is it was actually you know",
    "start": "417360",
    "end": "422370"
  },
  {
    "text": "first has its roots in 96 and it was originally developed to explain early",
    "start": "422370",
    "end": "428580"
  },
  {
    "text": "visual processing in the brain sort of I think of it as an edge detector and the objective here is the following well if",
    "start": "428580",
    "end": "435210"
  },
  {
    "text": "I give you set of data points x1 up to xn you'd want to learn a dictionary of basis Phi 1 up to Phi K right so that",
    "start": "435210",
    "end": "443160"
  },
  {
    "text": "every single data point can be written as a linear combination of the basis that's fairly simple right there is one",
    "start": "443160",
    "end": "450300"
  },
  {
    "text": "constraint in that you'd want your coefficients to be sparse you'd want",
    "start": "450300",
    "end": "455730"
  },
  {
    "text": "them to be mostly 0 right so every data point is represented as a sparse linear",
    "start": "455730",
    "end": "460860"
  },
  {
    "text": "combination of bases right so this is if",
    "start": "460860",
    "end": "466020"
  },
  {
    "text": "you apply sparse coding to natural images right and this says this was",
    "start": "466020",
    "end": "471870"
  },
  {
    "text": "originally it's been a lot of work developed at Stanford with n doings group so if you apply sparse coding to",
    "start": "471870",
    "end": "478320"
  },
  {
    "text": "you know take little patches of images and it learn these bases these dictionaries this is how they look like",
    "start": "478320",
    "end": "483870"
  },
  {
    "text": "and it's they look really nice in terms of you know finding the edge edge like structure so if you've given a new",
    "start": "483870",
    "end": "490950"
  },
  {
    "text": "example I can say well this new example can be written as a linear combination of a few of these bases right and taking",
    "start": "490950",
    "end": "499110"
  },
  {
    "text": "that representation it turns out that particular representation is sparse representation is quite useful as a",
    "start": "499110",
    "end": "505380"
  },
  {
    "text": "feature representation of your data right so it's quite useful to have it and in general helps how do we how do we",
    "start": "505380",
    "end": "514680"
  },
  {
    "text": "fit these models well if I give you a whole bunch of",
    "start": "514680",
    "end": "519960"
  },
  {
    "start": "517000",
    "end": "517000"
  },
  {
    "text": "image patches but these don't necessarily have to be image patches this could be you know little speech signals or any kind of data you're",
    "start": "519960",
    "end": "526800"
  },
  {
    "text": "working with you'd want to learn in a dictionary basis you have to form you have to solve this optimization problem right so the first term here",
    "start": "526800",
    "end": "534540"
  },
  {
    "text": "you can think of it as a reconstruction error which is to say well I take a linear combination of my basis I want",
    "start": "534540",
    "end": "541199"
  },
  {
    "text": "them to match my data and that is the second term which is you can think of it",
    "start": "541199",
    "end": "546240"
  },
  {
    "text": "as a sparse penalty term which essentially says you know try to penalize my coefficients so that most of",
    "start": "546240",
    "end": "554069"
  },
  {
    "text": "them are zero right that way every single data point can be written as just the linear combination sparse linear",
    "start": "554069",
    "end": "559350"
  },
  {
    "text": "combination of the basis and it turns out there is an easy optimization for",
    "start": "559350",
    "end": "565050"
  },
  {
    "text": "doing that if you fix your dictionary of bases right by one up to five SK and you solve",
    "start": "565050",
    "end": "572250"
  },
  {
    "text": "for the activations that becomes a standard lasso problem right there's a lot of solvers for for solving that",
    "start": "572250",
    "end": "579360"
  },
  {
    "text": "particular problem that's a general very you know it's it's it's a su problem",
    "start": "579360",
    "end": "584610"
  },
  {
    "text": "which is fairly easy to to optimize and then if you fix the activations and you optimized for dictionary basis then it's",
    "start": "584610",
    "end": "591750"
  },
  {
    "text": "a well-known quadratic programming problem right each problem is convex so you can sort",
    "start": "591750",
    "end": "597449"
  },
  {
    "text": "of alternate between finding coefficients finding bases and so forth so you can optimize this function and there's been a lot of recent work in the",
    "start": "597449",
    "end": "604680"
  },
  {
    "text": "last ten years of doing these things online and doing it more efficiently and so forth right at test time given a new",
    "start": "604680",
    "end": "614730"
  },
  {
    "text": "input or a new image patch and given a set of learned basis once you have your dictionary you can then just solve a la",
    "start": "614730",
    "end": "622709"
  },
  {
    "start": "619000",
    "end": "619000"
  },
  {
    "text": "soupe problem to find the right coefficients right so in this case given a test sample or test patch you can find",
    "start": "622709",
    "end": "629160"
  },
  {
    "text": "well it's written by as a linear combination of subsets of the bases",
    "start": "629160",
    "end": "634639"
  },
  {
    "text": "right and it turns out again that that particular representation is very useful particularly if you're interested in",
    "start": "634639",
    "end": "640589"
  },
  {
    "text": "classifying what you see in images and this is done in completely unsupervised way right there is no class labels there",
    "start": "640589",
    "end": "646860"
  },
  {
    "text": "is no specific supervisory signal that's that's here so back in 2006 there was",
    "start": "646860",
    "end": "653779"
  },
  {
    "start": "651000",
    "end": "651000"
  },
  {
    "text": "work done again at Stanford that",
    "start": "653779",
    "end": "659040"
  },
  {
    "text": "basically showed a very interesting result so if I give you an input like this and these are my learned bases remember",
    "start": "659040",
    "end": "664620"
  },
  {
    "text": "these little edges what happens is that you just control these bases you can get these different",
    "start": "664620",
    "end": "670560"
  },
  {
    "text": "feature Maps much like you know the future maps that we've seen in convolutional neural networks and then",
    "start": "670560",
    "end": "675600"
  },
  {
    "text": "you take these feature maps and you can just do a classification right and this was done on a one of the older data sets",
    "start": "675600",
    "end": "682290"
  },
  {
    "text": "the Caltech 101 which is sort of a data set that predates imagenet and if you",
    "start": "682290",
    "end": "689430"
  },
  {
    "text": "look at you know some of the competing algorithms if you do a simple logistic regression versus if you do PCA and then",
    "start": "689430",
    "end": "696420"
  },
  {
    "text": "do logistic regression versus finding these features using sparse coding you",
    "start": "696420",
    "end": "701820"
  },
  {
    "text": "can get substantial improvements right so that's again that's that's and you",
    "start": "701820",
    "end": "708480"
  },
  {
    "text": "see sparks coding popping up in a lot of different areas not just in deep learning but folks who are using looking",
    "start": "708480",
    "end": "714029"
  },
  {
    "text": "at medical imaging domain in neuroscience these are very popular models because they're easily they're",
    "start": "714029",
    "end": "721110"
  },
  {
    "text": "easy to fit they're easy to to deal with so what's the interpretation of the",
    "start": "721110",
    "end": "728250"
  },
  {
    "start": "725000",
    "end": "725000"
  },
  {
    "text": "sparse coding well look let's look at this equation again and we can think of sparse coding as finding and over",
    "start": "728250",
    "end": "734310"
  },
  {
    "text": "complete representation of your data right now the encoding function we can",
    "start": "734310",
    "end": "741029"
  },
  {
    "text": "think of this encoding function which is well I give you an input find me the features or sparse coefficients or bases",
    "start": "741029",
    "end": "747570"
  },
  {
    "text": "that make up my image we can think of encoding as an implicit and very nonlinear function of X right but it's",
    "start": "747570",
    "end": "754230"
  },
  {
    "text": "an implicit function we don't really specify it and the decoder or the reconstruction is just a seem simple",
    "start": "754230",
    "end": "760920"
  },
  {
    "text": "linear function and it's and it's very explicit just take your coefficients and",
    "start": "760920",
    "end": "766860"
  },
  {
    "text": "then multiply it by the you know find the right basis and get back get back",
    "start": "766860",
    "end": "772140"
  },
  {
    "text": "the image or the data right and that sort of flows naturally into the ideas",
    "start": "772140",
    "end": "779550"
  },
  {
    "text": "of autoencoders right the auto encoder is the general framework where if I give you an input data let's say it's an",
    "start": "779550",
    "end": "785850"
  },
  {
    "text": "input image you encode it you get some representation some feature representation and then you have a",
    "start": "785850",
    "end": "792120"
  },
  {
    "text": "decoder given that representation you're decoding it back into the image so you can think of encoding as a",
    "start": "792120",
    "end": "798940"
  },
  {
    "text": "as a feed-forward bottom-up pass right much like in convolutional neural",
    "start": "798940",
    "end": "803950"
  },
  {
    "text": "network given the image you're doing a forward pass and then there is also feedback and generative or top-down pass",
    "start": "803950",
    "end": "811150"
  },
  {
    "text": "right given features you're reconstructing back back the input image in the details is what's going inside",
    "start": "811150",
    "end": "817540"
  },
  {
    "text": "the encoder decoder they matter a lot and obviously you need some form of constraints you need some of constraints",
    "start": "817540",
    "end": "823600"
  },
  {
    "text": "to avoid learning an identity right because if you don't put these constraints what you could do is just take your input copy it to your features",
    "start": "823600",
    "end": "831670"
  },
  {
    "text": "and then reconstruct back right and that would be a trivial solution so we need to introduce some some additional",
    "start": "831670",
    "end": "837550"
  },
  {
    "text": "constraints if you're dealing with binary features if you want extract",
    "start": "837550",
    "end": "844240"
  },
  {
    "text": "binary features for example I'm going to show you later why you'd want to do that you can pass your your encoder through a",
    "start": "844240",
    "end": "850510"
  },
  {
    "text": "sigmoid non-linearity much like in the neural network and then you have a have a linear decoder that we can strike back",
    "start": "850510",
    "end": "856300"
  },
  {
    "text": "the input and the way we optimize these little building blocks or these little blocks is we can just have an encoder",
    "start": "856300",
    "end": "865290"
  },
  {
    "text": "right which takes your input takes a linear combination passes it through some non-linearity the sigmoid",
    "start": "865290",
    "end": "871840"
  },
  {
    "text": "non-linearity or could be rectified linear units could be 10h non-linearity and then there is a decoder where you",
    "start": "871840",
    "end": "877360"
  },
  {
    "text": "reconstruct back your original input right so this is nothing more than a neural network with one hidden layer and",
    "start": "877360",
    "end": "883810"
  },
  {
    "text": "typically that hidden layer would have a small dimensionality than the input so we can think of it as a bottleneck layer",
    "start": "883810",
    "end": "889390"
  },
  {
    "text": "right and we can determine the network parameters you know the parameters of the encoding the parameters of the",
    "start": "889390",
    "end": "894760"
  },
  {
    "text": "decoder by writing down the reconstruction error and that's what the reconstruction would look like you know",
    "start": "894760",
    "end": "900370"
  },
  {
    "text": "given the input in code decode and make sure whatever you decoding is as close as possible to to the original to the",
    "start": "900370",
    "end": "907390"
  },
  {
    "text": "original input all right then we can use back propagation algorithm to to to",
    "start": "907390",
    "end": "912640"
  },
  {
    "text": "Train there is an interesting sort of relationship between all encoders and property and principle component",
    "start": "912640",
    "end": "919000"
  },
  {
    "text": "analysis many of you have probably heard about pca as a practitioner you know if you're",
    "start": "919000",
    "end": "924190"
  },
  {
    "text": "dealing with large data and you want to see what's going on PCA is the first thing to use right much like what you seek regression",
    "start": "924190",
    "end": "931080"
  },
  {
    "text": "so and the idea here is that if the parameters of encoder and decoder are shared and you actually have the hidden",
    "start": "931080",
    "end": "938550"
  },
  {
    "text": "layer which is a linear layer so you don't use any nonlinearities then it turns out that the space the latent",
    "start": "938550",
    "end": "944910"
  },
  {
    "text": "space that the model will discover is going to be the same space as the space discovered by PCA it effectively will",
    "start": "944910",
    "end": "951030"
  },
  {
    "text": "collapse the principal component analysis right you're doing PCA which is sort of a nice connection because it",
    "start": "951030",
    "end": "957810"
  },
  {
    "text": "basically says that all encoders you can think of them as nonlinear extensions of PCA all right so you can learn a little",
    "start": "957810",
    "end": "964890"
  },
  {
    "text": "richer features if if you are using",
    "start": "964890",
    "end": "970920"
  },
  {
    "text": "autoencoders okay so here's another model if you're dealing with binary input sometimes we're dealing with like",
    "start": "970920",
    "end": "977640"
  },
  {
    "start": "972000",
    "end": "972000"
  },
  {
    "text": "Amnesty for example again your encoder and decoder could use sigmoid nonlinearities so given an input you",
    "start": "977640",
    "end": "983820"
  },
  {
    "text": "extract some binary features binary features you reconstruct back the binary input and that's actually you know",
    "start": "983820",
    "end": "990390"
  },
  {
    "text": "relates to a model called restricted both machines something that I'm going to tell you about later in the talk okay",
    "start": "990390",
    "end": "997170"
  },
  {
    "start": "997000",
    "end": "997000"
  },
  {
    "text": "there is also other classes of models where you can say well I can also introduce some sparsity much like in",
    "start": "997170",
    "end": "1003740"
  },
  {
    "text": "sparse coding to say that you know I need to constrain my latent features on my latent space to be sparse and that",
    "start": "1003740",
    "end": "1010580"
  },
  {
    "text": "actually allows you to learn quite reasonable features nice features here's",
    "start": "1010580",
    "end": "1016790"
  },
  {
    "text": "one particular model called predict expires decomposition where you effectively you know if you look at the",
    "start": "1016790",
    "end": "1022850"
  },
  {
    "text": "first part of the equation here the decoder part that pretty much looks like a sparse coding model right but in",
    "start": "1022850",
    "end": "1029300"
  },
  {
    "text": "addition you have an encoding part that essentially says train an encoder such",
    "start": "1029300",
    "end": "1034400"
  },
  {
    "text": "that it actually approximates what my latent code should be right so",
    "start": "1034400",
    "end": "1040060"
  },
  {
    "text": "effectively you can think of this model is that is in coder there is a decoder but then you put the sparsity constraint",
    "start": "1040060",
    "end": "1045199"
  },
  {
    "text": "on your latent representation and you can optimize for for that model and",
    "start": "1045200",
    "end": "1051910"
  },
  {
    "text": "obviously the other thing that we've been doing in the last you know seven eight and ten years is well what you can",
    "start": "1051910",
    "end": "1059270"
  },
  {
    "start": "1052000",
    "end": "1052000"
  },
  {
    "text": "do is you can actually stack these things together right so you can learn low-level features",
    "start": "1059270",
    "end": "1064820"
  },
  {
    "text": "try to learn high-level features and so forth so just building these blocks and",
    "start": "1064820",
    "end": "1070400"
  },
  {
    "text": "perhaps at the top level if you try to solve a classification problem you can do that or and this is sometimes known",
    "start": "1070400",
    "end": "1078020"
  },
  {
    "text": "as a greedy greedy lair wise learning and this is sometimes useful whenever you have lots and lots of unlabeled data",
    "start": "1078020",
    "end": "1084170"
  },
  {
    "text": "and when you have a little label data right a small sample of label data",
    "start": "1084170",
    "end": "1089480"
  },
  {
    "text": "typically these models help you find meaningful representations such that you don't need a lot of label data to solve",
    "start": "1089480",
    "end": "1096530"
  },
  {
    "text": "the particular task that you're trying to solve right and this is again you can remove the decoding part and then you",
    "start": "1096530",
    "end": "1102530"
  },
  {
    "text": "end up with a standard or convolutional architecture again your encoder and decoder could use could be convolutional",
    "start": "1102530",
    "end": "1108860"
  },
  {
    "text": "and it's it depends on what problem you tackling and typically you know you can",
    "start": "1108860",
    "end": "1114950"
  },
  {
    "text": "stack these things together and optimize for particular tasks that you're trying to solve okay here's an example of just",
    "start": "1114950",
    "end": "1124280"
  },
  {
    "start": "1123000",
    "end": "1123000"
  },
  {
    "text": "wanted to show you some examples some early examples back in 2006 this was a way of trying to build these nonlinear",
    "start": "1124280",
    "end": "1131660"
  },
  {
    "text": "autoencoders and you can sort of pre train these models using restricted Boltzmann's or auto-encoders generally",
    "start": "1131660",
    "end": "1139460"
  },
  {
    "text": "and then you know you can stitch them together into this deep autoencoder and",
    "start": "1139460",
    "end": "1144590"
  },
  {
    "text": "back propagate through reconstruction laws right one thing I want to point out is that here's one particular example",
    "start": "1144590",
    "end": "1151940"
  },
  {
    "text": "you know the top row I show you real faces the second row you're seeing faces",
    "start": "1151940",
    "end": "1157430"
  },
  {
    "text": "reconstructed from a bottleneck of of 30 dimensional real valid bottlenecks you",
    "start": "1157430",
    "end": "1163070"
  },
  {
    "text": "can think of it as just a compression mechanism given the data high dimensional data you're compressing it down to 30 dimensional code and then",
    "start": "1163070",
    "end": "1170120"
  },
  {
    "text": "from that 30 dimensional code you're reconstructing back the original data right so if you look at the first row",
    "start": "1170120",
    "end": "1175490"
  },
  {
    "text": "this is the data the second row shows you reconstructed data and the last row shows you PCA solution right one thing I",
    "start": "1175490",
    "end": "1183350"
  },
  {
    "text": "want to point out is that you know the solution here you have a much Sharpe representation which means that it's",
    "start": "1183350",
    "end": "1188510"
  },
  {
    "text": "capturing a little bit more structure in the data it's so kind of interesting to see that sometimes these models tend to",
    "start": "1188510",
    "end": "1194670"
  },
  {
    "text": "how should I say they tend to regularize your data right like for example if you see this person with glasses removes the",
    "start": "1194670",
    "end": "1201550"
  },
  {
    "text": "glasses and that genuinely has to do with the fact that there is only one person with glasses so the model just basically said that's noise get rid of",
    "start": "1201550",
    "end": "1208030"
  },
  {
    "text": "it oh it sort of gets rid of moustaches right like if you see this there's no mustache right and then again that has",
    "start": "1208030",
    "end": "1214150"
  },
  {
    "text": "to do with the fact that there's enough capacity so the model might think that that's just a noise and you know if",
    "start": "1214150",
    "end": "1221410"
  },
  {
    "start": "1220000",
    "end": "1220000"
  },
  {
    "text": "you're dealing with text type of data this was done using a Reuters data set",
    "start": "1221410",
    "end": "1227830"
  },
  {
    "text": "you have about eight hundred thousand stories you take bag of words representation something very simple you",
    "start": "1227830",
    "end": "1233080"
  },
  {
    "text": "can press it down to two dimensional space and then you see what that space looks like right and I always like to",
    "start": "1233080",
    "end": "1238510"
  },
  {
    "text": "joke that you know the model basically discovers that European community economic policies that just next",
    "start": "1238510",
    "end": "1244210"
  },
  {
    "text": "disasters and accidents right this is done this was back in I think they was",
    "start": "1244210",
    "end": "1249250"
  },
  {
    "text": "collecting ninety six right I think today is probably going to become closer those difference but again this is just",
    "start": "1249250",
    "end": "1256780"
  },
  {
    "text": "a way typically autoencoders a way of compression or trying to do dimensionality reduction but we'll see",
    "start": "1256780",
    "end": "1262900"
  },
  {
    "text": "later that they don't have to be okay there is another class of algorithm called semantic hashing which is to say",
    "start": "1262900",
    "end": "1268750"
  },
  {
    "start": "1265000",
    "end": "1265000"
  },
  {
    "text": "well what if you take your data and compress it down to binary representation wouldn't that be nice",
    "start": "1268750",
    "end": "1275130"
  },
  {
    "text": "because if you have binary representation you can search in the binary space very efficiently right in",
    "start": "1275130",
    "end": "1282220"
  },
  {
    "text": "fact if you can can compress your data down to twenty dimension 20 dimensional",
    "start": "1282220",
    "end": "1287679"
  },
  {
    "text": "binary code to to the twenty is about four gigabytes so you can just store everything in memory and you can look at",
    "start": "1287679",
    "end": "1294910"
  },
  {
    "text": "the you know just do memory lookups without actually doing any search at all",
    "start": "1294910",
    "end": "1300480"
  },
  {
    "text": "right so this sort of representation sometimes have been used successful in computer vision where you take your",
    "start": "1300480",
    "end": "1306970"
  },
  {
    "text": "images and then you learn these binary representations you know thirty",
    "start": "1306970",
    "end": "1312850"
  },
  {
    "text": "dimensional codes two hundred dimensional codes and it turns out it's very efficient to search through large",
    "start": "1312850",
    "end": "1318640"
  },
  {
    "text": "volumes of data using binary representation so you can you know takes a fraction of a millisecond to retrieve",
    "start": "1318640",
    "end": "1324450"
  },
  {
    "text": "images from you know a set of millions and millions of just and again this is also an active",
    "start": "1324450",
    "end": "1331059"
  },
  {
    "text": "area of research right now because people are trying to figure out we have these large databases how can you search through them",
    "start": "1331059",
    "end": "1336460"
  },
  {
    "text": "efficiently and sort of learning a semantic hashing function that maps your data to the binary presentation turns",
    "start": "1336460",
    "end": "1342820"
  },
  {
    "text": "out to be quite useful okay now let's step back a little bit and say let's now",
    "start": "1342820",
    "end": "1349330"
  },
  {
    "text": "look at generative models let's look at probabilistic models and how different they are and I'm going to show you some",
    "start": "1349330",
    "end": "1355300"
  },
  {
    "text": "examples of where they applicable here's one example of a simple model trying to",
    "start": "1355300",
    "end": "1363580"
  },
  {
    "start": "1361000",
    "end": "1361000"
  },
  {
    "text": "learn a distribution of these handwritten characters so we have you know we have Sanskrit we have Arabic we",
    "start": "1363580",
    "end": "1370480"
  },
  {
    "text": "have Syria like and now we can build a model that says well can you actually",
    "start": "1370480",
    "end": "1376210"
  },
  {
    "text": "generate me what a Sanskrit should look like the flickering you see at the top",
    "start": "1376210",
    "end": "1381460"
  },
  {
    "text": "these are you know a neurons you can think of them as neurons firing and what you're seeing at the bottom is you're",
    "start": "1381460",
    "end": "1386679"
  },
  {
    "text": "seeing what the model generates what it beliefs ask it should look like right so",
    "start": "1386679",
    "end": "1391840"
  },
  {
    "text": "in some sense when you think about generative models you think about models that can generate or they can sample the",
    "start": "1391840",
    "end": "1398380"
  },
  {
    "text": "distribution or they can sample the data this is a fairly simple model we have about 25,000 characters you know coming",
    "start": "1398380",
    "end": "1406270"
  },
  {
    "text": "from 50 different alphabets about around the world you have about two million parameters is one of the older models but this is you know what the model is a",
    "start": "1406270",
    "end": "1413290"
  },
  {
    "text": "scree should look like and I think that I've asked couple of people to say that is that does that really look like",
    "start": "1413290",
    "end": "1419110"
  },
  {
    "text": "Sanskrit okay great which can mean two things it can mean",
    "start": "1419110",
    "end": "1425470"
  },
  {
    "text": "that the model is red actually generalizing or the model is overfitting right meaning that is just memorizing",
    "start": "1425470",
    "end": "1431410"
  },
  {
    "text": "what the training data looks like and I'm just showing you examples from the training data we'll come back to that point as we go through the talk here's",
    "start": "1431410",
    "end": "1439300"
  },
  {
    "text": "you know you can also do conditional simulation you know given half of the image can you complete the remaining",
    "start": "1439300",
    "end": "1444460"
  },
  {
    "text": "half right and more recently there's been a lot of advances it's actually the",
    "start": "1444460",
    "end": "1451059"
  },
  {
    "text": "last couple of years for the conditional generations and it's pretty amazing what you can do in terms of in painting given",
    "start": "1451059",
    "end": "1458200"
  },
  {
    "text": "half of the image but the other half of Lima should look like this is sort of a simple example but it does show you that it's",
    "start": "1458200",
    "end": "1463840"
  },
  {
    "text": "trying to you know be consistent with what different strokes look like right",
    "start": "1463840",
    "end": "1469330"
  },
  {
    "text": "so why is it so difficult in the space of so-called undirected graphical models",
    "start": "1469330",
    "end": "1475450"
  },
  {
    "text": "of Boltzmann machines the difficulty really comes from the following fact if I show you this image which is a 28 by",
    "start": "1475450",
    "end": "1480820"
  },
  {
    "text": "28 image it's a binary image right so some pixels are on some pixels are off there are 2 to the 28 by 28 possible",
    "start": "1480820",
    "end": "1489280"
  },
  {
    "text": "images so in fact there are 2 to the 784 possible configurations right and that space is exponential so how can you",
    "start": "1489280",
    "end": "1496570"
  },
  {
    "text": "build models that figure out in the space of characters there's only a little tiny subspace in that space right",
    "start": "1496570",
    "end": "1502960"
  },
  {
    "text": "if you start genuinely generating you know 200 by 200 images you know that",
    "start": "1502960",
    "end": "1509830"
  },
  {
    "text": "space is huge in the space of real images is really really tiny right so",
    "start": "1509830",
    "end": "1515170"
  },
  {
    "text": "how do you find that space how do we generalize to new images that's that's a very difficult question in general to to",
    "start": "1515170",
    "end": "1522010"
  },
  {
    "text": "answer one class of models is so-called fully observed models right there's sort",
    "start": "1522010",
    "end": "1528880"
  },
  {
    "text": "of been a stream of learning generative models that are tractable and they have",
    "start": "1528880",
    "end": "1534010"
  },
  {
    "text": "very nice properties like you can compute the probabilities you can do can do maximum likelihood estimation here's one example where I can if I try to",
    "start": "1534010",
    "end": "1540610"
  },
  {
    "text": "model the image I can write it down as you know taking the first pixel more than the first pixel then modeling the",
    "start": "1540610",
    "end": "1546400"
  },
  {
    "text": "second pixel given the first pixel and just just writing it down in terms of conditional project of the conditional",
    "start": "1546400",
    "end": "1552880"
  },
  {
    "text": "probabilities an inch conditional probability can take a very complicated form right it could be a complicated",
    "start": "1552880",
    "end": "1558070"
  },
  {
    "text": "neural network and oh sorry so there's",
    "start": "1558070",
    "end": "1564790"
  },
  {
    "text": "been a number of successful models one of the early models called neural auto",
    "start": "1564790",
    "end": "1570610"
  },
  {
    "text": "regressive density estimator actually developed by Hugo real-valued extension",
    "start": "1570610",
    "end": "1575770"
  },
  {
    "text": "of these models and more recently we start seeing these flavors of models there were a couple of papers popped up",
    "start": "1575770",
    "end": "1582880"
  },
  {
    "text": "actually this year from deep mind where they sort of make these conditionals to",
    "start": "1582880",
    "end": "1588520"
  },
  {
    "text": "be you know sophisticated RNN so STM's or convolutional models and they can actually generate remarkable images",
    "start": "1588520",
    "end": "1595280"
  },
  {
    "text": "and so this is just a pixel CNN generating I guess elephants yeah and",
    "start": "1595280",
    "end": "1601880"
  },
  {
    "text": "actually looks pretty pretty interesting right the drawback of these models is that we yet have to see how good of",
    "start": "1601880",
    "end": "1608420"
  },
  {
    "text": "representations these models are learning so that we could use these representations for other tasks like",
    "start": "1608420",
    "end": "1613670"
  },
  {
    "text": "classifying images or find similar images and such right now let me jump",
    "start": "1613670",
    "end": "1619940"
  },
  {
    "text": "into a class of models called restricted Boltzmann so this is the class of models where we're actually trying to learn",
    "start": "1619940",
    "end": "1625880"
  },
  {
    "text": "some latent structures some latent representation these models belong to the class of so-called graphical models",
    "start": "1625880",
    "end": "1632510"
  },
  {
    "text": "and graphical models very powerful framework for representing dependency structure between random variables this",
    "start": "1632510",
    "end": "1639320"
  },
  {
    "text": "is an example where we have you can think of this particular model you have",
    "start": "1639320",
    "end": "1644810"
  },
  {
    "text": "some pixels this is stochastic binary so called visible variables you can think of pixels in your image and you have",
    "start": "1644810",
    "end": "1650780"
  },
  {
    "text": "stochastic binary hidden variables you can think of them as feature detectors so detecting certain patterns that you see in the data",
    "start": "1650780",
    "end": "1656120"
  },
  {
    "text": "much like sparse coding models she has a bipartite structure you can write down the probability the Joint Distribution",
    "start": "1656120",
    "end": "1662450"
  },
  {
    "text": "over all of these variables you sort of have pairwise term you have Union return",
    "start": "1662450",
    "end": "1667850"
  },
  {
    "text": "but it's not really important what they look like the important thing here is that if I look at this conditional probability of the data given given the",
    "start": "1667850",
    "end": "1674960"
  },
  {
    "text": "features I can actually write down explicitly what it looks like and what does that mean that basically means that",
    "start": "1674960",
    "end": "1680300"
  },
  {
    "text": "if you tell me what features you see in the image I can generate the data for you right I can generate the",
    "start": "1680300",
    "end": "1686540"
  },
  {
    "text": "corresponding input in terms of learning features so what do these models learn they stop learn something similar that",
    "start": "1686540",
    "end": "1693650"
  },
  {
    "start": "1688000",
    "end": "1688000"
  },
  {
    "text": "we've seen in sparse coding right it's and so these classes of models are very similar to each other so given a new",
    "start": "1693650",
    "end": "1700670"
  },
  {
    "text": "image I can say well this new image is made up by some combination of these learned weights of these learned bases",
    "start": "1700670",
    "end": "1707390"
  },
  {
    "text": "and the numbers here are given by the probabilities that each particular edge is present in the data in terms of how",
    "start": "1707390",
    "end": "1715640"
  },
  {
    "text": "we learn these models one thing I want to make another point I should make here",
    "start": "1715640",
    "end": "1721640"
  },
  {
    "text": "is that given an input I can actually quickly infer what teachers I'm seeing in the image so that",
    "start": "1721640",
    "end": "1728309"
  },
  {
    "text": "operation is very easy to do unlike in sparse coding models it's it's a bit more closer to Northern Quarter given",
    "start": "1728309",
    "end": "1734369"
  },
  {
    "text": "the data I can actually tell you what features are present in my in my input which is very important for things like",
    "start": "1734369",
    "end": "1739529"
  },
  {
    "text": "information retrieval or classifying images because you need to do it you need to do it fast how do we learn these",
    "start": "1739529",
    "end": "1745919"
  },
  {
    "start": "1744000",
    "end": "1744000"
  },
  {
    "text": "models let me just give you an intuition maybe a little bit of math behind how we learn these models if I give you set of",
    "start": "1745919",
    "end": "1753059"
  },
  {
    "text": "training examples and I want to learn model parameters I can maximize the log-likelihood objective right and",
    "start": "1753059",
    "end": "1758879"
  },
  {
    "text": "you've probably seen that in these tutorials max and while objective is essentially nothing more than saying I",
    "start": "1758879",
    "end": "1765719"
  },
  {
    "text": "want to make sure that the probability of observing these images is as high as possible right so finally the parameters",
    "start": "1765719",
    "end": "1772289"
  },
  {
    "text": "of the probability of observing what I'm seeing is high and that's why you're maximizing the the likelihood objective",
    "start": "1772289",
    "end": "1780509"
  },
  {
    "text": "or the log of the likelihood objection would just you know take a product into the sum you take the derivative there's",
    "start": "1780509",
    "end": "1786029"
  },
  {
    "text": "a little bit of algebra I promise you it's not it's not very difficult like you know second-year college algebra you",
    "start": "1786029",
    "end": "1794129"
  },
  {
    "text": "differentiate and you basically have this learning rule which is the difference between two terms the first",
    "start": "1794129",
    "end": "1801869"
  },
  {
    "text": "term you can think of it as looking at sufficient statistic so cost official",
    "start": "1801869",
    "end": "1807419"
  },
  {
    "text": "statistics driven by the data and the second term is the sufficient statistics driven by the model right maybe I can",
    "start": "1807419",
    "end": "1814379"
  },
  {
    "text": "parse it out what does that mean intuitively what that means is that you look at the correlations you see in the",
    "start": "1814379",
    "end": "1820079"
  },
  {
    "text": "data right and then you look at the correlations that the model is telling you it should be and you're trying to",
    "start": "1820079",
    "end": "1826289"
  },
  {
    "text": "match the two that's what the learning is trying to do right it's trying to match the correlations that you see in",
    "start": "1826289",
    "end": "1832949"
  },
  {
    "text": "the data right so the model is actually respecting the statistics that you see in the data but it turns out that the",
    "start": "1832949",
    "end": "1839159"
  },
  {
    "text": "second term is very difficult to compute and it's precisely because the space of all possible images is so high",
    "start": "1839159",
    "end": "1845309"
  },
  {
    "text": "dimensional that you need to figure out or use some kind of approximate learning algorithms to do that",
    "start": "1845309",
    "end": "1850379"
  },
  {
    "text": "all right so you have these difference between these two terms the first term is easy to compute it turns out because",
    "start": "1850379",
    "end": "1856079"
  },
  {
    "text": "of a particular structure of the model right and we can actually do it do it",
    "start": "1856079",
    "end": "1861510"
  },
  {
    "text": "explicitly the second term is the difficult difficult one to compute right so it sort of requires you know summing",
    "start": "1861510",
    "end": "1867780"
  },
  {
    "text": "over all possible configurations all possible images that that that you could possibly see so it's this term is",
    "start": "1867780",
    "end": "1874380"
  },
  {
    "text": "intractable and what a lot of different algorithms are doing and we'll see that over and over again is using so-called",
    "start": "1874380",
    "end": "1880950"
  },
  {
    "text": "Monte Carlo sampling or Markov chain Monte Carlo sampling a Monte Carlo estimation right so let me give you an",
    "start": "1880950",
    "end": "1887280"
  },
  {
    "text": "intuition what what this term is doing that's a general trick for you know approximating exponential sums right the",
    "start": "1887280",
    "end": "1894570"
  },
  {
    "text": "whole subfield in in statistics that's basically dedicated to how do we",
    "start": "1894570",
    "end": "1901110"
  },
  {
    "text": "approximate exponential sums in fact if you could do that if you could solve that problem you could solve a lot of",
    "start": "1901110",
    "end": "1906720"
  },
  {
    "text": "problems in machine learning and the idea is very simple actually the idea is",
    "start": "1906720",
    "end": "1912540"
  },
  {
    "text": "to say well you're going to be replacing the average by sampling and there's",
    "start": "1912540",
    "end": "1918180"
  },
  {
    "text": "something that's called a Gibbs sampling Markov chain Monte Carlo which is essentially does something very simple",
    "start": "1918180",
    "end": "1924120"
  },
  {
    "text": "it basically says well start with a data sample the states of the latent",
    "start": "1924120",
    "end": "1929220"
  },
  {
    "text": "variables you know sample the data sample the states of the way through then sample the data from these conditional distributions something that",
    "start": "1929220",
    "end": "1934890"
  },
  {
    "text": "you can compute explicitly right and that's a general trick you know much like in sparse coding we you know we're",
    "start": "1934890",
    "end": "1941430"
  },
  {
    "text": "optimizing for the basis when we're optimizing for the coefficients here you're inferring the coefficients then",
    "start": "1941430",
    "end": "1947370"
  },
  {
    "text": "you you know in vary what the data should look like and so forth and then you can just run a Markov chain and sort",
    "start": "1947370",
    "end": "1952500"
  },
  {
    "text": "of approximate approximate you know this exponential sum so you start with the",
    "start": "1952500",
    "end": "1959010"
  },
  {
    "text": "data you sample the states of the hidden variables you resample the data and so forth and the only problem with a lot of",
    "start": "1959010",
    "end": "1965580"
  },
  {
    "text": "these methods is that you know you need to run them up to infinity to guarantee",
    "start": "1965580",
    "end": "1972750"
  },
  {
    "text": "that you're sort of getting the right thing and so obviously you know you will never run them you know infinite you",
    "start": "1972750",
    "end": "1980640"
  },
  {
    "text": "don't have time to do that so there's a very clever algorithm that contrastive divergence algorithm that was developed",
    "start": "1980640",
    "end": "1986820"
  },
  {
    "start": "1982000",
    "end": "1982000"
  },
  {
    "text": "by Hinton back in 2002 and it was very clever it basically said well instead of",
    "start": "1986820",
    "end": "1992370"
  },
  {
    "text": "this thing up to infinity run it for one step right and so you're just running it",
    "start": "1992370",
    "end": "1999690"
  },
  {
    "text": "for one step you start with a training vector you you update the hidden units you update all the visible units again",
    "start": "1999690",
    "end": "2006380"
  },
  {
    "text": "so that's your reconstruction much like an autoencoder you reconstruct your data you update the hidden units again and",
    "start": "2006380",
    "end": "2012680"
  },
  {
    "text": "then you just update the model parameter which is just looking at you know empirically the statistics between the data and the model right very similar to",
    "start": "2012680",
    "end": "2020780"
  },
  {
    "text": "what the auto encoder is doing but slight slight differences and implementation is basically takes about",
    "start": "2020780",
    "end": "2026420"
  },
  {
    "text": "like penalize of MATLAB code I suspect is going to be you know two lines intensive flow although I don't think",
    "start": "2026420",
    "end": "2032750"
  },
  {
    "text": "terms of flow folks implemented Boltzmann machines yet that would be my request but you can extend these models",
    "start": "2032750",
    "end": "2042890"
  },
  {
    "text": "to dealing with real valued data right so whenever you're dealing with images for example and it's just a little",
    "start": "2042890",
    "end": "2048169"
  },
  {
    "text": "change to the definition of the model and your conditional probabilities hedge is going to be bunch of gaussians so",
    "start": "2048169",
    "end": "2054800"
  },
  {
    "text": "that basically means that given the features sample me the space of images and i can sample you give you you know",
    "start": "2054800",
    "end": "2060980"
  },
  {
    "text": "real real valued images the structure of the model remains the same if you train",
    "start": "2060980",
    "end": "2066290"
  },
  {
    "text": "this model on you know the these images you sort of tend to find edges something",
    "start": "2066290",
    "end": "2072440"
  },
  {
    "text": "similar again to what you'd see in sparse coding and I see in depending upon analysis model auto-encoders and",
    "start": "2072440",
    "end": "2078260"
  },
  {
    "text": "such and again you can sort of say well every single image is made up by some some linear combination of these basis",
    "start": "2078260",
    "end": "2084648"
  },
  {
    "text": "functions you can also extend these models to dealing with count data right if you're dealing with documents in this",
    "start": "2084649",
    "end": "2091669"
  },
  {
    "start": "2085000",
    "end": "2085000"
  },
  {
    "text": "case again it's slight change to the model K here denotes your vocabulary",
    "start": "2091669",
    "end": "2097100"
  },
  {
    "text": "size and DK the knots number of words that you're seeing in your document right so if you you know it's it's a bag",
    "start": "2097100",
    "end": "2103400"
  },
  {
    "text": "of words representation and the conditional here is given by so called softmax distribution much like what",
    "start": "2103400",
    "end": "2108950"
  },
  {
    "text": "you've seen in in the previous classes when we you know the distribution of possible words right and the parameters",
    "start": "2108950",
    "end": "2116450"
  },
  {
    "text": "here double use you can think of them as you know something similar to as what the back embedding would do and so if",
    "start": "2116450",
    "end": "2124450"
  },
  {
    "text": "you apply to in again some some of datasets you know you tend to find",
    "start": "2124450",
    "end": "2130020"
  },
  {
    "text": "reasonable features right so can to find you know features about Russia about us",
    "start": "2130020",
    "end": "2135280"
  },
  {
    "text": "about computers and so forth right so much like you found these representations little edges every image",
    "start": "2135280",
    "end": "2141460"
  },
  {
    "text": "is made up by some combination of these edges in case of documents or webpages",
    "start": "2141460",
    "end": "2147730"
  },
  {
    "text": "you're saying the same thing it's just made up cyma linear combination of of these learned topics every single",
    "start": "2147730",
    "end": "2153490"
  },
  {
    "text": "document is made up by some combination of these topics right you can also look at one-step reconstruction so you can",
    "start": "2153490",
    "end": "2159520"
  },
  {
    "text": "basically say well how can I find similarity between the words so if I show you chocolate cake and further",
    "start": "2159520",
    "end": "2165310"
  },
  {
    "text": "states of hidden units and then I reconstruct back the distribution of possible words you know it tells me you",
    "start": "2165310",
    "end": "2171730"
  },
  {
    "text": "know chocolate cake cake chocolate sweet dessert cupcake food sugar and so forth right I particularly like the one about",
    "start": "2171730",
    "end": "2178180"
  },
  {
    "text": "the flour hi and then there is a Japanese sign the model sort of generates flour Japan Sakura blossom",
    "start": "2178180",
    "end": "2187540"
  },
  {
    "text": "Tokyo all right so it sort of picks up again on low-level correlations that you see in your data you can also apply",
    "start": "2187540",
    "end": "2194200"
  },
  {
    "start": "2193000",
    "end": "2193000"
  },
  {
    "text": "these kinds of models to collaborative filtering where every single observed variable you can model you know can",
    "start": "2194200",
    "end": "2201400"
  },
  {
    "text": "represent a user rating for a particular movie right so every single user would",
    "start": "2201400",
    "end": "2209230"
  },
  {
    "text": "rate a certain subset of movies and so you can represent it as the state of visible via your hidden states can",
    "start": "2209230",
    "end": "2215260"
  },
  {
    "text": "represent user preferences what they are and on the netflix data set if you look",
    "start": "2215260",
    "end": "2220330"
  },
  {
    "text": "at the latent space that the model is learning you know some of these hidden variables are capturing specific movie",
    "start": "2220330",
    "end": "2227170"
  },
  {
    "text": "genre right so for example there is this is actually one hidden union dedicated",
    "start": "2227170",
    "end": "2232480"
  },
  {
    "text": "to michael michael moore's movies rights instead of like very strong i think it's sort of you know either people like it",
    "start": "2232480",
    "end": "2238600"
  },
  {
    "text": "or hate it so there are a few hidden specifically dedicated to that but it also finds interesting things like you",
    "start": "2238600",
    "end": "2244330"
  },
  {
    "text": "know action movies and so forth right so it finds that particular structure in the data so you can model different",
    "start": "2244330",
    "end": "2249940"
  },
  {
    "text": "kinds of modality real-valued data can model count data a multinomial x' and it's very easy to",
    "start": "2249940",
    "end": "2257040"
  },
  {
    "text": "infer the states of the hidden variables so that's given just the product off of logistic functions and that's very important in a lot of different",
    "start": "2257040",
    "end": "2263160"
  },
  {
    "text": "applications given the input that can quickly tell you what topics I see in the data right one thing that I want to",
    "start": "2263160",
    "end": "2269910"
  },
  {
    "start": "2269000",
    "end": "2269000"
  },
  {
    "text": "point out that's an important point is a lot of these models can be viewed as product models sometimes people call",
    "start": "2269910",
    "end": "2276599"
  },
  {
    "text": "them product of experts and this is because of the following the following",
    "start": "2276599",
    "end": "2282660"
  },
  {
    "text": "intuition if I write down the Joint Distribution of my hidden observed variables I can write it down in this",
    "start": "2282660",
    "end": "2288240"
  },
  {
    "text": "sort of log-linear form right but if I sum out or integrate out the states of",
    "start": "2288240",
    "end": "2294089"
  },
  {
    "text": "the hidden variables I have a bunch of product of a whole bunch of functions right so what is what does it mean what",
    "start": "2294089",
    "end": "2302369"
  },
  {
    "text": "what's the intuition here so let me show you an example suppose the model finds these specific topics right and suppose",
    "start": "2302369",
    "end": "2310140"
  },
  {
    "text": "I'm going to be telling you that the document contains topic government corruption in mafia then the word silvio",
    "start": "2310140",
    "end": "2316380"
  },
  {
    "text": "berlusconi will have very high probability right I guess does anybody know you know everybody knows who Celia",
    "start": "2316380",
    "end": "2322680"
  },
  {
    "text": "Celia Berlusconi right he's he had like you know he's in head of the government he's connected to mafia he is his very",
    "start": "2322680",
    "end": "2329339"
  },
  {
    "text": "corrupt was corrupt and I guess I should add like a bunker bunker parties here right then it will become completely",
    "start": "2329339",
    "end": "2335220"
  },
  {
    "text": "clear what I'm talking about but then you know one point I want to make here is that it's it's you know you",
    "start": "2335220",
    "end": "2342660"
  },
  {
    "text": "can think of these models as a product each hidden variable defines a distribution over possible words over",
    "start": "2342660",
    "end": "2349260"
  },
  {
    "text": "possible topics and once you take the intersection of these distributions you can be very precise about what is it",
    "start": "2349260",
    "end": "2355770"
  },
  {
    "text": "that you modeling right so that's unlike generally topic models or let Indian",
    "start": "2355770",
    "end": "2361319"
  },
  {
    "text": "education models models where you're actually using mixture like a approach",
    "start": "2361319",
    "end": "2367770"
  },
  {
    "text": "and then typically these models do perform far better than traditional mixture based models and this comes to",
    "start": "2367770",
    "end": "2373980"
  },
  {
    "start": "2372000",
    "end": "2372000"
  },
  {
    "text": "the point of local versus global versus distributed representations right in in",
    "start": "2373980",
    "end": "2379290"
  },
  {
    "text": "a lot of different algorithms you know even the supervised learning algorithm such as clustering you typically have some your",
    "start": "2379290",
    "end": "2386230"
  },
  {
    "text": "partitioning the space and you're finding local prototypes right and the",
    "start": "2386230",
    "end": "2392230"
  },
  {
    "text": "number of parameters for which you have basically you know parameters for each region the number of regions typically",
    "start": "2392230",
    "end": "2397330"
  },
  {
    "text": "grow with linearly with a number of parameters but in models like factor",
    "start": "2397330",
    "end": "2403180"
  },
  {
    "text": "models PCA restricted Boltzmann machines deep models you typically have distributed representations right what's",
    "start": "2403180",
    "end": "2409330"
  },
  {
    "text": "the idea here the idea here is that if I show you the two inputs right each particular neuron can you know",
    "start": "2409330",
    "end": "2416530"
  },
  {
    "text": "differentiate between two parts of the plane given the second one you know I can partition it again given the third",
    "start": "2416530",
    "end": "2423400"
  },
  {
    "text": "hidden variable you can partition it again so you can see that every single neuron will be affecting lots of",
    "start": "2423400",
    "end": "2429820"
  },
  {
    "text": "different regions and that's the idea behind distribute representations because every single parameter is",
    "start": "2429820",
    "end": "2435220"
  },
  {
    "text": "affecting many many widgets not just the local region and so the number of regions grow roughly exponentially with",
    "start": "2435220",
    "end": "2440560"
  },
  {
    "text": "the number of parameters right so that's the differences between these these two",
    "start": "2440560",
    "end": "2445840"
  },
  {
    "text": "classes of models important to know about them now let me jump and quickly tell you a little bit of inspiration",
    "start": "2445840",
    "end": "2451900"
  },
  {
    "text": "behind what what can we build with these models right as we've seen with convolutional networks the first layer",
    "start": "2451900",
    "end": "2459040"
  },
  {
    "start": "2455000",
    "end": "2455000"
  },
  {
    "text": "we typically learn some low-level features like edges or you know if you're working with the word word table",
    "start": "2459040",
    "end": "2467020"
  },
  {
    "text": "typically learn some low-level structure and the hope is that the higher-level features will start picking up some",
    "start": "2467020",
    "end": "2472300"
  },
  {
    "text": "higher-level structure as as you are building and these kinds of models can",
    "start": "2472300",
    "end": "2477700"
  },
  {
    "text": "be built in completely unsupervised way because what you're trying to do is you're trying to model the data you try to model the distribution of of the data",
    "start": "2477700",
    "end": "2484330"
  },
  {
    "start": "2484000",
    "end": "2484000"
  },
  {
    "text": "you can write down the probability distribution for this models known as a",
    "start": "2484330",
    "end": "2490020"
  },
  {
    "text": "Boltzmann machine model you have dependencies between hidden variables so now introducing some extra you know some",
    "start": "2490020",
    "end": "2498880"
  },
  {
    "text": "extra layers and dependencies between those layers and if we look at the equation the first part of the equation",
    "start": "2498880",
    "end": "2505480"
  },
  {
    "text": "is basically the same as what we had with restricted Boltzmann and then the second and third part of the equation",
    "start": "2505480",
    "end": "2511270"
  },
  {
    "text": "essentially modeling dependencies between you know the first and the second hidden layer and the second",
    "start": "2511270",
    "end": "2516460"
  },
  {
    "text": "hidden layer third in there right there is also a very natural notion of bottom-up and top-down so if I want to see what's the",
    "start": "2516460",
    "end": "2522670"
  },
  {
    "text": "probability of a particular Union being taking value 1 it's really depend on",
    "start": "2522670",
    "end": "2528070"
  },
  {
    "text": "what's coming from below what's coming from above so there has to be some consensus in the model to say ah yes",
    "start": "2528070",
    "end": "2533860"
  },
  {
    "text": "what I'm seeing in the image and what my model believes the overall structure should be should be an agreement right",
    "start": "2533860",
    "end": "2541690"
  },
  {
    "text": "and so in this case of course in this case hidden variables become dependent even when you condition on on the data",
    "start": "2541690",
    "end": "2547060"
  },
  {
    "text": "so these kinds of models we'll see a lot is you introducing more flexibility you're introducing more structure but",
    "start": "2547060",
    "end": "2554170"
  },
  {
    "text": "then learning becomes much more difficult right you have to deal you know how do you influence in these",
    "start": "2554170",
    "end": "2560140"
  },
  {
    "text": "models right now let me give you an intuition of what how can we learn this",
    "start": "2560140",
    "end": "2566830"
  },
  {
    "text": "model what's the maximum likelihood estimator doing here well if I differentiate this model with respect to",
    "start": "2566830",
    "end": "2572920"
  },
  {
    "text": "parameter basically run into the same learning rule and it's the same one you know you see whatever you're working with undirected graphical models factor",
    "start": "2572920",
    "end": "2579730"
  },
  {
    "text": "graphs conditional random fields you might have heard about those those ones it really is just trying to look at the",
    "start": "2579730",
    "end": "2585400"
  },
  {
    "text": "statistics driven by the data correlations that you see in the data and the correlations that the model is telling you it's seeing in the data and",
    "start": "2585400",
    "end": "2591940"
  },
  {
    "text": "you just try to match the two right that's exactly what's happening in that particular equation right but the first",
    "start": "2591940",
    "end": "2598810"
  },
  {
    "text": "term is no longer factorial so you know you have to do some approximation with these models let me give you an",
    "start": "2598810",
    "end": "2603940"
  },
  {
    "text": "intuition watch what each term is doing so far as I have some data right and I get to observe these characters well",
    "start": "2603940",
    "end": "2610990"
  },
  {
    "text": "what I can do is I really want to tell the model this is real right these are",
    "start": "2610990",
    "end": "2616210"
  },
  {
    "text": "real characters so I want to put some probability mass around them and say these are real and then there is some",
    "start": "2616210",
    "end": "2622240"
  },
  {
    "text": "sort of a data point that looks like this just bunch of pixels on and off and I really want to tell my model that you",
    "start": "2622240",
    "end": "2629050"
  },
  {
    "text": "know put all the zero probability on this this is not real right and so the",
    "start": "2629050",
    "end": "2635830"
  },
  {
    "text": "first term is exactly trying to do that the first term is just trying to say put the probability max where you see the",
    "start": "2635830",
    "end": "2641080"
  },
  {
    "text": "data and the second term is effectively trying to say well look at this entire exponential space and just say no",
    "start": "2641080",
    "end": "2646320"
  },
  {
    "text": "everything else is not real it's just the real thing is what I'm seeing in my data and so you can use sort of",
    "start": "2646320",
    "end": "2652910"
  },
  {
    "text": "advanced techniques for doing that it's a class of algorithms called variational inference it's something that's called",
    "start": "2652910",
    "end": "2658640"
  },
  {
    "text": "stochastic approximation which is Monte Carlo based inference I'm not going to go into these techniques but in general",
    "start": "2658640",
    "end": "2663680"
  },
  {
    "text": "you can you can train these models so one question is how good are they all",
    "start": "2663680",
    "end": "2668750"
  },
  {
    "start": "2665000",
    "end": "2665000"
  },
  {
    "text": "right because a lot of proximation that go into these models so what I'm going to do is if you have if you haven't seen",
    "start": "2668750",
    "end": "2675079"
  },
  {
    "text": "it I'm going to show you two panels on one panel you will see the real data or",
    "start": "2675079",
    "end": "2680269"
  },
  {
    "text": "another panel you'll see data simulated by the model or the fake data and you have to tell me which one is which okay",
    "start": "2680269",
    "end": "2686750"
  },
  {
    "text": "so again these are handwritten characters coming from you know alphabets around the world how many of",
    "start": "2686750",
    "end": "2692150"
  },
  {
    "text": "you think this is simulated and the other part was real honestly okay some",
    "start": "2692150",
    "end": "2697519"
  },
  {
    "text": "what about the other way around I get half and half which is great if you look",
    "start": "2697519",
    "end": "2705680"
  },
  {
    "text": "at these images a little bit more carefully you will see the difference right so you will see that this is",
    "start": "2705680",
    "end": "2712339"
  },
  {
    "text": "simulated and this is real right because if you look at the real data it's much",
    "start": "2712339",
    "end": "2717950"
  },
  {
    "text": "crisper there is more diversity when you're simulating the data it's a lot of structure in the simulated characters",
    "start": "2717950",
    "end": "2723859"
  },
  {
    "text": "but some you know sometimes a little bit files it isn't as much diversity right and I've learned that trick from from my",
    "start": "2723859",
    "end": "2732130"
  },
  {
    "text": "neuroscience friends if I show you quickly enough you won't see the difference and and you know if you're",
    "start": "2732130",
    "end": "2741019"
  },
  {
    "text": "using these models for for for classifying you know you can do proper analysis which is to say given a new",
    "start": "2741019",
    "end": "2746269"
  },
  {
    "text": "character you find further states of the latent variables hidden variables if I classify based on that how good are they",
    "start": "2746269",
    "end": "2752150"
  },
  {
    "text": "and and they are they're you know they're much better than some of the existing techniques this is another",
    "start": "2752150",
    "end": "2757609"
  },
  {
    "start": "2754000",
    "end": "2754000"
  },
  {
    "text": "example you know trying to generate 3d objects this is sort of a toys data sets and later on I'll show you some you know",
    "start": "2757609",
    "end": "2763700"
  },
  {
    "text": "bigger advances that's been happening in the last few years this was done a few years ago you know if you look at the",
    "start": "2763700",
    "end": "2769190"
  },
  {
    "text": "space of generated samples they you know they sort of you know you obviously you",
    "start": "2769190",
    "end": "2775400"
  },
  {
    "text": "can see the difference here's here's paid look at this particular image right this image looks like car with wings",
    "start": "2775400",
    "end": "2782059"
  },
  {
    "text": "don't you think right so there's sometimes it can sort of simulate things that are not",
    "start": "2782059",
    "end": "2787770"
  },
  {
    "text": "necessarily realistic and for some reason it just doesn't generate donkeys and elephants too often right but it",
    "start": "2787770",
    "end": "2793860"
  },
  {
    "text": "generates people with guns more often like if you look at here and here and here and that again has to do with the",
    "start": "2793860",
    "end": "2799740"
  },
  {
    "text": "fact that you know you exploring this exponential space of possible images and",
    "start": "2799740",
    "end": "2805020"
  },
  {
    "text": "it's sometimes it's very hard to assign the right probabilities the different parts of the space right and then",
    "start": "2805020",
    "end": "2812400"
  },
  {
    "start": "2812000",
    "end": "2812000"
  },
  {
    "text": "obviously you can do things like pattern completion so given half of the image can you complete the remaining half so",
    "start": "2812400",
    "end": "2817410"
  },
  {
    "text": "the second one shows what the completions look like and the last one is what the truth is so you can do you can do these things so where else can we",
    "start": "2817410",
    "end": "2824790"
  },
  {
    "text": "use these models these are sort of toy examples but where else let me show you one example where these models can",
    "start": "2824790",
    "end": "2831030"
  },
  {
    "start": "2829000",
    "end": "2829000"
  },
  {
    "text": "potentially succeed which is trying to model the space of the multi model space",
    "start": "2831030",
    "end": "2836280"
  },
  {
    "text": "which is the space of you know images and texts or you know generally if you",
    "start": "2836280",
    "end": "2841530"
  },
  {
    "text": "look at the data it's not just single source it's a collection of different modalities alright so how can we take",
    "start": "2841530",
    "end": "2847260"
  },
  {
    "text": "all of these modalities into account and this is really just the idea of you know given images and texts can you actually",
    "start": "2847260",
    "end": "2853590"
  },
  {
    "text": "find a concept that relates these two different sources of sources of data and",
    "start": "2853590",
    "end": "2860040"
  },
  {
    "start": "2860000",
    "end": "2860000"
  },
  {
    "text": "there are a few challenges and that's why you know models like generative models sometimes probablistic models",
    "start": "2860040",
    "end": "2865560"
  },
  {
    "text": "could be useful general one of the biggest challenge we've seen is that typically when you're working with",
    "start": "2865560",
    "end": "2870810"
  },
  {
    "text": "images and text these are very different modalities right if you think about images in pixel representation they're",
    "start": "2870810",
    "end": "2877110"
  },
  {
    "text": "very dense if you're looking at text it's typically very sparse right so it's very difficult to learn these cross",
    "start": "2877110",
    "end": "2883590"
  },
  {
    "text": "model features from low-level representation perhaps a big challenge is that a lot of",
    "start": "2883590",
    "end": "2890070"
  },
  {
    "text": "times we see data that's very noisy right sometimes it's just non-existent giving an image there is no text or if",
    "start": "2890070",
    "end": "2896010"
  },
  {
    "text": "you look at the first image you know a lot of the tags about is what kind of camera was used to to describe that",
    "start": "2896010",
    "end": "2902490"
  },
  {
    "text": "particular image which doesn't really tell us anything about the image itself right and these are this would be the",
    "start": "2902490",
    "end": "2909000"
  },
  {
    "text": "text generated by a version of a Boltzmann machine model sort of does you know samples what what",
    "start": "2909000",
    "end": "2915430"
  },
  {
    "text": "should look like and this is the the idea again is very simple if you just build a simple representation given",
    "start": "2915430",
    "end": "2922390"
  },
  {
    "start": "2917000",
    "end": "2917000"
  },
  {
    "text": "images and given text you just try to find what the common representation is it's very difficult to learn these cross",
    "start": "2922390",
    "end": "2928150"
  },
  {
    "text": "model features but if you actually build a hierarchical model so you start with",
    "start": "2928150",
    "end": "2933160"
  },
  {
    "text": "representation you know you can build a Gaussian model replicate itself max molecule so build up that representation",
    "start": "2933160",
    "end": "2938200"
  },
  {
    "text": "then it turns out it's much more it gives you much richer representation is",
    "start": "2938200",
    "end": "2944349"
  },
  {
    "text": "also when if a notion of bottom-up and top-down which means that you know low level or images or tags can effectively",
    "start": "2944349",
    "end": "2953109"
  },
  {
    "text": "effect low level representation of images and the other way around so information flows between images and",
    "start": "2953109",
    "end": "2958450"
  },
  {
    "text": "texts sort of gets into some stable state and this is what you know the text",
    "start": "2958450",
    "end": "2964150"
  },
  {
    "start": "2963000",
    "end": "2963000"
  },
  {
    "text": "generated from images looks like some of the examples you know a lot of them a lot of them look reasonable but more",
    "start": "2964150",
    "end": "2971290"
  },
  {
    "text": "recently with the advances of cornets this is probably not that surprising",
    "start": "2971290",
    "end": "2976720"
  },
  {
    "text": "here's some examples of the model that's not quite doing the right thing right I",
    "start": "2976720",
    "end": "2983049"
  },
  {
    "text": "particularly like the second one for some reason it sort of correlates with Barack Obama and such and we've sort of",
    "start": "2983049",
    "end": "2989470"
  },
  {
    "text": "the features when we were using this model we didn't have at that time sort of image that features right now I don't",
    "start": "2989470",
    "end": "2995829"
  },
  {
    "text": "think we would be making these mistakes but generally speaking you know what we found in a lot of the data is that there aren't a lot of images of animals right",
    "start": "2995829",
    "end": "3002369"
  },
  {
    "text": "which brings us to the next problem is that if you don't see images of animals then the mall is confused because it sees a lot of Obama signs and these are",
    "start": "3002369",
    "end": "3008940"
  },
  {
    "text": "black and also white and and blue sort of signs that appearing a lot you can",
    "start": "3008940",
    "end": "3014339"
  },
  {
    "text": "also do images from text given text or tags can retrieve relevant images so you",
    "start": "3014339",
    "end": "3021030"
  },
  {
    "text": "know this is the data set itself at about million images it's a nice nice data set and you have you know very noisy tags and the question is can you",
    "start": "3021030",
    "end": "3028710"
  },
  {
    "text": "actually learn some representation from from those images one thing that I want to highlight here is you know we've",
    "start": "3028710",
    "end": "3035339"
  },
  {
    "text": "tried you know this is 25,000 labeled image somebody went and label what's going on in those images what classes we",
    "start": "3035339",
    "end": "3041099"
  },
  {
    "text": "see in those images and you get some numbers which is main average precision but what's important here is that we",
    "start": "3041099",
    "end": "3046530"
  },
  {
    "text": "found that if we actually use label data and we pre train these channels separately using a million",
    "start": "3046530",
    "end": "3053910"
  },
  {
    "text": "label data points then we can actually get some performance improvement so at least that was a little bit of happy",
    "start": "3053910",
    "end": "3060700"
  },
  {
    "text": "sign for us to say that you know unlabeled data can help in the situations where you don't have a lot of",
    "start": "3060700",
    "end": "3066760"
  },
  {
    "text": "labeled examples so here was helping us it was helping us a lot and then once",
    "start": "3066760",
    "end": "3072130"
  },
  {
    "start": "3071000",
    "end": "3071000"
  },
  {
    "text": "you get into this sort of representations dealing with text and images this is one particular thing you",
    "start": "3072130",
    "end": "3077590"
  },
  {
    "text": "can do and I think Richard pointed out you know what happens in the space of linguistic regularities you can do the",
    "start": "3077590",
    "end": "3086230"
  },
  {
    "text": "same thing with images just kind of fun to do they they sometimes work they don't work all the time but here's one",
    "start": "3086230",
    "end": "3091990"
  },
  {
    "text": "example if I take that particularly image at the top and I say get the representation of this image subtract",
    "start": "3091990",
    "end": "3098020"
  },
  {
    "text": "the representation of day add the night and then find closes images you get these images right and then you can do",
    "start": "3098020",
    "end": "3104350"
  },
  {
    "text": "some interesting things like take these kittens and say - ball + box to get kittens in the box right if you take",
    "start": "3104350",
    "end": "3110950"
  },
  {
    "text": "this particular image and say - box + ball get kittens in the ball right except for this thing that's a duck so",
    "start": "3110950",
    "end": "3118840"
  },
  {
    "text": "you know that's you can sort of get these interesting interesting representations of course these are all",
    "start": "3118840",
    "end": "3124540"
  },
  {
    "text": "sort of fun things to look at but they don't really mean much because we're not specifically optimizing for those things",
    "start": "3124540",
    "end": "3129850"
  },
  {
    "text": "right now let me spend some time also",
    "start": "3129850",
    "end": "3136240"
  },
  {
    "text": "talking about another class of models these are known as Helmholtz machines and variational tinker's these are the",
    "start": "3136240",
    "end": "3141820"
  },
  {
    "text": "models that have been sort of popping up in our community in the last two years right so what is a Helmholtz machine a",
    "start": "3141820",
    "end": "3148800"
  },
  {
    "text": "hell-house machine was developed back in 95 and it was developed by Hinton and",
    "start": "3148800",
    "end": "3154630"
  },
  {
    "text": "Peter Dayan and Brandon Frey and Radford Neal and it has this particular",
    "start": "3154630",
    "end": "3160480"
  },
  {
    "text": "architecture you have a generative process given some latent state you just",
    "start": "3160480",
    "end": "3167080"
  },
  {
    "text": "it's a neural net with a stochastic neural network that generates the input data right and then you have so-called",
    "start": "3167080",
    "end": "3173500"
  },
  {
    "text": "approximate inference step which is to say given the data infer approximately",
    "start": "3173500",
    "end": "3178750"
  },
  {
    "text": "what the latent States should look like right and again it was",
    "start": "3178750",
    "end": "3184069"
  },
  {
    "text": "developed in 95 there's something is called waste sleep algorithm and it never worked basically people just said",
    "start": "3184069",
    "end": "3190819"
  },
  {
    "text": "it just doesn't work and you know then we started looking at restricted boltzmann shades on both machines",
    "start": "3190819",
    "end": "3196369"
  },
  {
    "text": "because they were working a little bit better and then two years ago people figure out how to make them work and so",
    "start": "3196369",
    "end": "3202309"
  },
  {
    "text": "now 10 years later I'm going to show you the trick now these models actually working pretty well the difference",
    "start": "3202309",
    "end": "3208039"
  },
  {
    "text": "between Helmholtz machines and deep Boltzmann chains is very subtle they almost look identical the big",
    "start": "3208039",
    "end": "3213589"
  },
  {
    "text": "difference between the two is that in hell most machines you have a generative process that generates the data and you",
    "start": "3213589",
    "end": "3220039"
  },
  {
    "start": "3214000",
    "end": "3214000"
  },
  {
    "text": "have a separate recognition model that tries to recognize what you see in the data so you can think of this cue",
    "start": "3220039",
    "end": "3225740"
  },
  {
    "text": "function as a convolutional neural network given the data tries to figure out what the feature should look like and then there is a generative model",
    "start": "3225740",
    "end": "3232490"
  },
  {
    "text": "given the features it generates the data Boltzmann machine is sort of similar class of models but it has undirected",
    "start": "3232490",
    "end": "3238700"
  },
  {
    "text": "connection so you can think of it as generative and recognition connections are the same so it's sort of a system",
    "start": "3238700",
    "end": "3243950"
  },
  {
    "text": "that tries to know which some equilibrium state when you're running it",
    "start": "3243950",
    "end": "3249109"
  },
  {
    "text": "so it's a little bit the semantics is a little bit different between these two models so what is the variation water",
    "start": "3249109",
    "end": "3255020"
  },
  {
    "text": "encoder variational core is is a Helmholtz machine it defines a generative process in terms of sampling",
    "start": "3255020",
    "end": "3260569"
  },
  {
    "start": "3260000",
    "end": "3260000"
  },
  {
    "text": "through cascades of stochastic layers and it's if you look at it there's just bunch of conditional probability",
    "start": "3260569",
    "end": "3267380"
  },
  {
    "text": "distributions that you're defining so you can generate the data so theta here will denote the parameters of the",
    "start": "3267380",
    "end": "3272480"
  },
  {
    "text": "variation auto-encoders you have a number of stochastic layers and sampling",
    "start": "3272480",
    "end": "3277549"
  },
  {
    "text": "from you know this conditional probability distributions hallam you know we're assuming that we can do it it's a tractable it has to be tractable",
    "start": "3277549",
    "end": "3284589"
  },
  {
    "text": "but the innovation here is that every single conditional probability can",
    "start": "3284589",
    "end": "3291260"
  },
  {
    "text": "actually be you know it can be very complicated function in keynote you know a nonlinear you can model nonlinear",
    "start": "3291260",
    "end": "3297799"
  },
  {
    "text": "relationships it can be a multi-lane only a neural network deterministic neural network right so it becomes",
    "start": "3297799",
    "end": "3303230"
  },
  {
    "text": "fairly fairly powerful he's here's an example of I have a stochastic layer you have a deterministic way have a",
    "start": "3303230",
    "end": "3308960"
  },
  {
    "text": "stochastic way and then you generate generate the data right so you can introduce these nonlinearities",
    "start": "3308960",
    "end": "3314180"
  },
  {
    "text": "into these models and this conditional probability we didn't know the one layer",
    "start": "3314180",
    "end": "3319820"
  },
  {
    "text": "neural network right now I'll show you some examples but maybe I can just give",
    "start": "3319820",
    "end": "3325940"
  },
  {
    "text": "you a little intuition behind what these equations do and a lot of these kinds of",
    "start": "3325940",
    "end": "3333080"
  },
  {
    "text": "models learning is very hard to do and there is a class of models called variational learning and what the",
    "start": "3333080",
    "end": "3339620"
  },
  {
    "text": "variational learning is trying to do is basically trying to do the phone but I want to maximize the probability of the data that I observe but I cannot do it",
    "start": "3339620",
    "end": "3346640"
  },
  {
    "text": "directly so instead what I'm going to do is I'm going to maximize the so called variational Lobot which is this term",
    "start": "3346640",
    "end": "3351950"
  },
  {
    "text": "here right and it's effectively saying well if I take the log of expectation I",
    "start": "3351950",
    "end": "3358100"
  },
  {
    "text": "can take the log and push it inside right and it turns out just logistically",
    "start": "3358100",
    "end": "3363580"
  },
  {
    "text": "working in this representation is much easier than working in this representation right if you go a little",
    "start": "3363580",
    "end": "3369290"
  },
  {
    "text": "bit through the math turns out that you can actually you know optimize this variation above but you can't really",
    "start": "3369290",
    "end": "3375020"
  },
  {
    "text": "optimize this particular likelihood objective it's a little bit surprising",
    "start": "3375020",
    "end": "3380240"
  },
  {
    "text": "for those of you who haven't seen variation of learning how it's done you know but this one little check this one",
    "start": "3380240",
    "end": "3386480"
  },
  {
    "text": "little so-called Jensen's inequality actually allows you to solve a lot of problems right and the other way to",
    "start": "3386480",
    "end": "3393830"
  },
  {
    "text": "write the lower bound is to say well there is a log likelihood function and something is called KL divergence which",
    "start": "3393830",
    "end": "3399470"
  },
  {
    "text": "is the distance between your approximating distribution Q which is your recognition model and the truth the",
    "start": "3399470",
    "end": "3405950"
  },
  {
    "text": "truth in these models would be the true posterior cording to your to your model and it's hard to optimize these kinds of",
    "start": "3405950",
    "end": "3412850"
  },
  {
    "text": "models in general you know you're trying to optimize your generative model you try to optimize the recognition model",
    "start": "3412850",
    "end": "3418040"
  },
  {
    "text": "and back in 88 back in 95 Hinton and his students basically they developed this",
    "start": "3418040",
    "end": "3424070"
  },
  {
    "text": "wake-sleep algorithm that was a bunch of different things put together but it was never quite the right algorithm because",
    "start": "3424070",
    "end": "3429530"
  },
  {
    "text": "it wasn't really optimizing anything it was just bunch of things alternating but",
    "start": "3429530",
    "end": "3434600"
  },
  {
    "text": "in 2014 there was a beautiful trick introduced by kingman Welling and there was a few other groups that came up with",
    "start": "3434600",
    "end": "3440990"
  },
  {
    "text": "the same trick called recanalization trick right so let me show you what repolarization trick does intuitively",
    "start": "3440990",
    "end": "3447840"
  },
  {
    "text": "so let's say your recognition distribution is a Gaussian right so a Gaussian I can write it as you know I",
    "start": "3447840",
    "end": "3454410"
  },
  {
    "text": "mean in the variance so this is the mean this is the variance notice that my mean depends on the layer below could be very",
    "start": "3454410",
    "end": "3462330"
  },
  {
    "text": "nonlinear function the variance also depends on the layer below so it could also be a nonlinear function but what I",
    "start": "3462330",
    "end": "3470220"
  },
  {
    "text": "can do is I can actually do the following I can express this particular Gaussian in terms of Xillia variables so",
    "start": "3470220",
    "end": "3476880"
  },
  {
    "text": "I can say well if I sample this epsilon from normal 0 1 a Gaussian distribution then I can write this particular H right",
    "start": "3476880",
    "end": "3486050"
  },
  {
    "text": "my-my-my state in a deterministic way it's just mean plus essentially standard",
    "start": "3486050",
    "end": "3493410"
  },
  {
    "text": "deviation or variance square root of the variance times this epsilon right so",
    "start": "3493410",
    "end": "3498600"
  },
  {
    "text": "this is just a simple parameterization of the gaussians right and just pulling out the mean and the variance there's no",
    "start": "3498600",
    "end": "3504240"
  },
  {
    "text": "sort of surprises here so I can write my recognition model as this Gaussian or I",
    "start": "3504240",
    "end": "3510720"
  },
  {
    "text": "can write it in terms of noise plus the deterministic part right so the recognition distribution can be",
    "start": "3510720",
    "end": "3516360"
  },
  {
    "text": "represented as a deterministic mapping and that's that's the beauty because it turns out that you can collapse these",
    "start": "3516360",
    "end": "3522630"
  },
  {
    "text": "complicated models effectively into auto-encoders right and we know how to deal with auto-encoders we can back",
    "start": "3522630",
    "end": "3528270"
  },
  {
    "text": "propagate through the entire through the entire model so we have a deterministic encoder and then the distribution of",
    "start": "3528270",
    "end": "3534330"
  },
  {
    "text": "these auxiliary variables really don't depend on parameters right so we sort of it's almost like taking a stochastic",
    "start": "3534330",
    "end": "3540930"
  },
  {
    "text": "system and separating the stochastic part and deterministic part in the terminus tic part you can do back",
    "start": "3540930",
    "end": "3546720"
  },
  {
    "text": "propagation so you can do learning and the stochastic Park you can do sampling right so just think of it as a separation between the two the two",
    "start": "3546720",
    "end": "3555930"
  },
  {
    "text": "pieces so now if I take the gradient of the variational bound or variational objective with respect to parameters",
    "start": "3555930",
    "end": "3562410"
  },
  {
    "text": "this is something that we couldn't do back in 95 and we couldn't do it in the last 10 years people tried using",
    "start": "3562410",
    "end": "3568350"
  },
  {
    "text": "reinforced algorithm or some approximations that never worked but here what we can do is we can do the",
    "start": "3568350",
    "end": "3573750"
  },
  {
    "text": "following we can say well I can write this expression because it's a Gaussian a sampling bunch of these axillary",
    "start": "3573750",
    "end": "3580230"
  },
  {
    "text": "variables and then this log I can just inject the noise in here the whole thing here",
    "start": "3580230",
    "end": "3585390"
  },
  {
    "text": "becomes deterministic and that's that's where the beauty comes in you take these gradient here and you push it inside the",
    "start": "3585390",
    "end": "3593010"
  },
  {
    "text": "expectation right so before if you take the gradient of expectation it's like",
    "start": "3593010",
    "end": "3598589"
  },
  {
    "text": "taking the gradient of averages like you compute bunch of averages in you're taking the gradient what you're doing",
    "start": "3598589",
    "end": "3603930"
  },
  {
    "text": "now with reprimands a ssin trick is you're taking the gradients and then taking the average right it turns out",
    "start": "3603930",
    "end": "3610109"
  },
  {
    "text": "that huge it reduces the variance in your training and it actually allows you to learn these models quite quite",
    "start": "3610109",
    "end": "3616950"
  },
  {
    "text": "efficiently so the mapping edge here is is completely deterministic and gradients here can be computed by back",
    "start": "3616950",
    "end": "3623190"
  },
  {
    "text": "propagation is a deterministic system and you can think of this thing inside it's just an autoencoder that that you",
    "start": "3623190",
    "end": "3630390"
  },
  {
    "text": "are that you are optimizing and obviously there are other extensions of these models that we've looked at and a",
    "start": "3630390",
    "end": "3636900"
  },
  {
    "text": "bunch of other teams looked at where you can say well maybe we can improve these models by drawing multiple samples these",
    "start": "3636900",
    "end": "3642180"
  },
  {
    "text": "are so-called K samples importance weighting bounce and so if you can make them a little bit better a little bit",
    "start": "3642180",
    "end": "3648329"
  },
  {
    "text": "more precise you can model a little bit more complicated distributions over the over the posterior but now let me sort",
    "start": "3648329",
    "end": "3655859"
  },
  {
    "text": "of step back a little bit and say why am I telling you about this what's the point there's a bunch of equations you injecting noise why do we need noise why",
    "start": "3655859",
    "end": "3661920"
  },
  {
    "text": "do we need stochastic scene systems in general right here's a motivating",
    "start": "3661920",
    "end": "3667020"
  },
  {
    "text": "example we wanted to build a model that given captions we want to generate the",
    "start": "3667020",
    "end": "3673289"
  },
  {
    "text": "image right and my student was very ambitious and basically said I want to be able to just tell give you any",
    "start": "3673289",
    "end": "3679799"
  },
  {
    "text": "sentence and and I want to be able to generate image like kind of like an artificial pain I want to paint what",
    "start": "3679799",
    "end": "3685440"
  },
  {
    "text": "what's in my what's in my caption in the most general way right so this is one",
    "start": "3685440",
    "end": "3691740"
  },
  {
    "text": "example of a Helmholtz machine where you have a generative model which is a stochastic recurrent network is just a change sequence of a variational",
    "start": "3691740",
    "end": "3697289"
  },
  {
    "text": "auto-encoders and there's a recognition model which is you can think of a deterministic system like a",
    "start": "3697289",
    "end": "3702299"
  },
  {
    "text": "convolutional system that tries to approximate what the latency states are but why do I need why do I need to the",
    "start": "3702299",
    "end": "3708720"
  },
  {
    "text": "caste city here why do I need very short encoders here and the reasons very simple suppose I",
    "start": "3708720",
    "end": "3714920"
  },
  {
    "text": "you know I give you the following tasks right I say a stop sign is flying in blue skies okay",
    "start": "3714920",
    "end": "3721190"
  },
  {
    "text": "now if you were using a deterministic system like an auto encoder you would generate one image right because it's a",
    "start": "3721190",
    "end": "3727880"
  },
  {
    "text": "deterministic system give an input I give you you know I'll give you output once you have stochastic system you",
    "start": "3727880",
    "end": "3733670"
  },
  {
    "text": "inject this noise this latent noise that allows it to actually generate a whole space of possible images right so for",
    "start": "3733670",
    "end": "3740089"
  },
  {
    "text": "example it tends to generate like this stop sign and this stop sign they look very different right and there's a car",
    "start": "3740089",
    "end": "3745339"
  },
  {
    "text": "here so maybe it's not really flying is just can't draw the pole here this one",
    "start": "3745339",
    "end": "3750619"
  },
  {
    "text": "looks like they're clouds here's this yellow school buses flying in those guys right so here we wanted to test the",
    "start": "3750619",
    "end": "3756680"
  },
  {
    "text": "system to see does it understand something about what's what's in the sentence here's a herd of elephants is",
    "start": "3756680",
    "end": "3763819"
  },
  {
    "text": "flying in blue skies now we cannot generate elephants although there are no techniques that are getting better but",
    "start": "3763819",
    "end": "3769640"
  },
  {
    "text": "you know sometimes it generates two of them right and the commercial plane flying in blue skies but this is where",
    "start": "3769640",
    "end": "3775400"
  },
  {
    "text": "we need stochastic because we want to be able to generate the whole distribution of possible outcomes not necessarily",
    "start": "3775400",
    "end": "3781160"
  },
  {
    "text": "just one particular point right here's you know we can basically do things like",
    "start": "3781160",
    "end": "3786200"
  },
  {
    "text": "you know a yellow school bus parked in the parking lot versus a red school bus parked in the parking lot versus a green school bus park in the",
    "start": "3786200",
    "end": "3793400"
  },
  {
    "text": "parking lots it's sort of the blue school bus rights it's sort of we can't quite generate blue school buses but",
    "start": "3793400",
    "end": "3799040"
  },
  {
    "text": "we've seen blue cars and we've seen blue buses so it can sort of make an association to to draw these different",
    "start": "3799040",
    "end": "3805160"
  },
  {
    "text": "things they look a little bit fuzzy but you know in terms of comparing two",
    "start": "3805160",
    "end": "3811130"
  },
  {
    "text": "different models if I give you a group of people on the beach with surfboards this is what we can generate there is",
    "start": "3811130",
    "end": "3817400"
  },
  {
    "text": "another model called webcam model which is a model based on that the cellular networks something out I'll talk last",
    "start": "3817400",
    "end": "3823640"
  },
  {
    "text": "part of this talk and there is these models convolutional deconvolution all the ocean water encoders which is again",
    "start": "3823640",
    "end": "3830720"
  },
  {
    "text": "convolutional deconvolution 'l auto-encoders just with with some noise and you can certainly see that you know",
    "start": "3830720",
    "end": "3836030"
  },
  {
    "text": "it's generally we found it's very hard to be able to generate scenes with arbitrary inputs as a text right here's",
    "start": "3836030",
    "end": "3844160"
  },
  {
    "text": "here's my favorite one a toilet it's it's open in the bathroom all right I don't know if you can see a",
    "start": "3844160",
    "end": "3849859"
  },
  {
    "text": "toilet sits here maybe but you can say toilets it's it's open in the grass field that was a little bit better at",
    "start": "3849859",
    "end": "3855859"
  },
  {
    "text": "least the colors were quite right and when we put this paper on archive one of",
    "start": "3855859",
    "end": "3862160"
  },
  {
    "text": "one of the students basically came to me and said well this is really bad because",
    "start": "3862160",
    "end": "3868039"
  },
  {
    "text": "you can always ask Google right and if you if you type that particular query into Google search it gives you that all",
    "start": "3868039",
    "end": "3875779"
  },
  {
    "text": "right which was a little bit disappointing but now if you actually",
    "start": "3875779",
    "end": "3881779"
  },
  {
    "text": "Google or if you actually put this query into Google this image comes before this",
    "start": "3881779",
    "end": "3886910"
  },
  {
    "text": "image and generally because what's happening is that people are just",
    "start": "3886910",
    "end": "3891920"
  },
  {
    "text": "clicking on that image all the time to figure out what's going on in that image so we got bumped up before before that",
    "start": "3891920",
    "end": "3898220"
  },
  {
    "text": "other means so so now I can say that according to Google this is a much better representation for that sentence",
    "start": "3898220",
    "end": "3904640"
  },
  {
    "text": "now than listen here's another here's another sort of interesting model which",
    "start": "3904640",
    "end": "3910220"
  },
  {
    "text": "is a model where you're trying to build a recurrent neural network again it's a",
    "start": "3910220",
    "end": "3915950"
  },
  {
    "text": "generative model but it's a generative model of text this was this model was trained on about 7,000 romance novels",
    "start": "3915950",
    "end": "3924109"
  },
  {
    "text": "and you take a caption model and you hook it up to the to the caption",
    "start": "3924109",
    "end": "3929779"
  },
  {
    "text": "generation system so you're basically saying the model here's an image generate me you know in the style of",
    "start": "3929779",
    "end": "3936410"
  },
  {
    "text": "romantic books what what you'd see here and you know it generates generates",
    "start": "3936410",
    "end": "3941569"
  },
  {
    "text": "something interesting we barely were able to catch the breeze on the beach and so forth she's beautiful but the",
    "start": "3941569",
    "end": "3947000"
  },
  {
    "text": "truth is I don't know what to do the Sun was just starting to fade away leaving",
    "start": "3947000",
    "end": "3952309"
  },
  {
    "text": "people scattered around the Atlantic Ocean so and there are a bunch of different things that you can do",
    "start": "3952309",
    "end": "3958369"
  },
  {
    "text": "obviously you know we're not there yet in terms of generating romantic stories but here's a one example where it's a",
    "start": "3958369",
    "end": "3964430"
  },
  {
    "text": "generative model it seems like syntactically we can actually generate you know reasonable things semantically",
    "start": "3964430",
    "end": "3972140"
  },
  {
    "text": "we're not there yet right and actually that particular work was inspired a little bit by by actually by by deuce",
    "start": "3972140",
    "end": "3979390"
  },
  {
    "text": "system that would give image it would I think of generate poems right but the points were predefined so",
    "start": "3979390",
    "end": "3987030"
  },
  {
    "text": "it was mostly the selecting the right point for the image here we actually were trying to you know generate",
    "start": "3987030",
    "end": "3993119"
  },
  {
    "text": "something something so there's still a lot of work to do in that space because you know syntactically we can get there",
    "start": "3993119",
    "end": "3999359"
  },
  {
    "text": "semantically we are nowhere near you know getting getting the right structure here is another last example that I want",
    "start": "3999359",
    "end": "4006050"
  },
  {
    "text": "to show you this was done in the case of one shot warning which is can you build",
    "start": "4006050",
    "end": "4011210"
  },
  {
    "text": "generative model of characters right that's a very defined domain very well",
    "start": "4011210",
    "end": "4016579"
  },
  {
    "text": "defined domain it's a very simple domain but it's also very hard right here's one example we've shown this example to",
    "start": "4016579",
    "end": "4023390"
  },
  {
    "text": "people and to the algorithm and we can say well can you draw me this this example and you know on one panel humans",
    "start": "4023390",
    "end": "4030740"
  },
  {
    "text": "would draw you know how they believe these this example should look like and then on the other panel we have machines",
    "start": "4030740",
    "end": "4036349"
  },
  {
    "text": "drawing it right this is it really just a generative model of based on a single",
    "start": "4036349",
    "end": "4041720"
  },
  {
    "text": "example showing you example and try to generate what it is and so question for you how many of you think this was a",
    "start": "4041720",
    "end": "4048049"
  },
  {
    "text": "machine generated and this was human generated ah what about the other way",
    "start": "4048049",
    "end": "4053510"
  },
  {
    "text": "around more MORE aha so there is a vote what this is what about this one this is",
    "start": "4053510",
    "end": "4059030"
  },
  {
    "text": "the how many do you think this machine generated is a human generated a few what about that aware it around ah great",
    "start": "4059030",
    "end": "4066829"
  },
  {
    "text": "great well the truth is I don't really know which one was genuine about this machine because that was done I should",
    "start": "4066829",
    "end": "4073730"
  },
  {
    "text": "actually ask Brandon Lake who designed the experiments for this particular model but I can tell you that I can tell",
    "start": "4073730",
    "end": "4079790"
  },
  {
    "text": "you that you know there's been a lot of studies he's done a lot of studies and it's about you know it's almost 50/50 so",
    "start": "4079790",
    "end": "4085520"
  },
  {
    "text": "in sort of this kind of small carve domain we can actually compete with people you know trying to generate these",
    "start": "4085520",
    "end": "4093819"
  },
  {
    "text": "these characters now let me step back a little bit and tell you about a",
    "start": "4093819",
    "end": "4099798"
  },
  {
    "text": "different class of models these are models known as generative adversarial",
    "start": "4099799",
    "end": "4104810"
  },
  {
    "text": "networks and they've been gaining a lot of attraction in in our community",
    "start": "4104810",
    "end": "4110330"
  },
  {
    "text": "because they seem to produce remarkable results so here's here's the idea we're not",
    "start": "4110330",
    "end": "4117190"
  },
  {
    "text": "going to be really defining explicitly the density but we need to be able to sample from the model right and the",
    "start": "4117190",
    "end": "4123880"
  },
  {
    "text": "interesting thing is that there's no variation learning there is no maximum likelihood estimation there is no Markov chain Monte Carlo there's no sampling",
    "start": "4123880",
    "end": "4129880"
  },
  {
    "text": "how do you do that how do you learn these models and it turns out that you can learn these models by playing a game",
    "start": "4129880",
    "end": "4135609"
  },
  {
    "text": "and that's a very clever strategy and the idea is the following you're going",
    "start": "4135609",
    "end": "4140829"
  },
  {
    "text": "to be setting up a game between two players you're going to have a discriminator do you think of it as a",
    "start": "4140829",
    "end": "4145838"
  },
  {
    "text": "convolutional network convolutional neural network and then you're going to have a generator gee maybe you can think",
    "start": "4145839",
    "end": "4151269"
  },
  {
    "text": "of it as as a variational tank or a Helmholtz machine or something that gives you samples from the data the",
    "start": "4151269",
    "end": "4157988"
  },
  {
    "text": "discriminated D is going to be discriminating between a sample from the data distribution and a sample from the",
    "start": "4157989",
    "end": "4165429"
  },
  {
    "text": "generator so the goal of the discriminate is to is to basically say is this a fake sample or is this a real",
    "start": "4165429",
    "end": "4171639"
  },
  {
    "text": "sample right fake sample the sample generated by the model real sample is",
    "start": "4171639",
    "end": "4177159"
  },
  {
    "text": "what you see in your data right can you tell the difference between the two right and the generator is going to be",
    "start": "4177159",
    "end": "4183338"
  },
  {
    "text": "trying to fool the discriminator by trying to generate samples that are hard",
    "start": "4183339",
    "end": "4188559"
  },
  {
    "text": "for discriminator to discriminate so my goal is a generator would be to generate really nice-looking digits so that the",
    "start": "4188559",
    "end": "4195730"
  },
  {
    "text": "discriminator won't be able to tell the difference between you know simulate it and the real right that's the key idea",
    "start": "4195730",
    "end": "4201510"
  },
  {
    "text": "and so here is intuitively what what",
    "start": "4201510",
    "end": "4206829"
  },
  {
    "text": "that looks like let's say you have you have some data so images of faces give",
    "start": "4206829",
    "end": "4213849"
  },
  {
    "text": "you an image your face and now I have a discriminated basically Tazewell if I get a real face I push it through some",
    "start": "4213849",
    "end": "4221289"
  },
  {
    "text": "function some differentiable function think of it as a convolutional neural network or another differentiable function and here I'm outputting one",
    "start": "4221289",
    "end": "4228519"
  },
  {
    "text": "right so I want to output one if it's a real sample right then you have a generator right and generator is you",
    "start": "4228519",
    "end": "4235989"
  },
  {
    "text": "have some noise so input noise think of it as a Gaussian distribution thinking about Helmholtz machines given some",
    "start": "4235989",
    "end": "4241539"
  },
  {
    "text": "noise I go through differentiable function which is your generator and January's sample this is my design my",
    "start": "4241539",
    "end": "4247570"
  },
  {
    "text": "sample might look like right and then on top of it I take this sample I put it",
    "start": "4247570",
    "end": "4253330"
  },
  {
    "text": "into my discriminator and I say for my discriminator I want to output 0 right",
    "start": "4253330",
    "end": "4258430"
  },
  {
    "text": "because my discriminator will have to say well this is fake and this is real",
    "start": "4258430",
    "end": "4264100"
  },
  {
    "text": "right that's the goal and the generator basically says well how can I get a sample such that my discriminate is",
    "start": "4264100",
    "end": "4272380"
  },
  {
    "text": "going to be confused such that the discriminator always outputs 1 here right because it believes it's a true",
    "start": "4272380",
    "end": "4277660"
  },
  {
    "text": "sample believe is it coming from the truth data right so now you have these",
    "start": "4277660",
    "end": "4283360"
  },
  {
    "text": "systems so what's what's the objective the objective is a min Max value",
    "start": "4283360",
    "end": "4289330"
  },
  {
    "text": "function it's a very intuitive objective function that has the following",
    "start": "4289330",
    "end": "4294550"
  },
  {
    "text": "structure you have a discriminator that says well this is an expectation with respect to distribution data",
    "start": "4294550",
    "end": "4300760"
  },
  {
    "text": "distribution so this is basically saying I want to classify any data points that I get from my data as being real right",
    "start": "4300760",
    "end": "4307180"
  },
  {
    "text": "so I want this output to be 1 because if it's 1 the whole thing is going to be 0",
    "start": "4307180",
    "end": "4312280"
  },
  {
    "text": "if it's less than 1 it's going to be negative and I really want to maximize it and then discriminator says well any",
    "start": "4312280",
    "end": "4319540"
  },
  {
    "text": "time I generate a sample whatever samples comes out from my generator you",
    "start": "4319540",
    "end": "4324790"
  },
  {
    "text": "know I want to classify it as being fake right that's the goal of the discriminator and then there is a",
    "start": "4324790",
    "end": "4330970"
  },
  {
    "text": "generator the generator is sort of the other you know you try to minimize this function which essentially says well",
    "start": "4330970",
    "end": "4336750"
  },
  {
    "text": "generate samples that discriminate it would classify as real so I really want to try to imagine you know change the",
    "start": "4336750",
    "end": "4342670"
  },
  {
    "text": "parameters of my generator such that this would produce 0 right so also so",
    "start": "4342670",
    "end": "4349120"
  },
  {
    "text": "the discriminant would produce 1 right so trying to full full the discriminator",
    "start": "4349120",
    "end": "4355030"
  },
  {
    "text": "right and it turns out the optimal strategy for discriminate is this this ratio which is probability of the data",
    "start": "4355030",
    "end": "4361120"
  },
  {
    "text": "divided probability of the data by plus probability of the model and in general if you succeed in building a good",
    "start": "4361120",
    "end": "4367420"
  },
  {
    "text": "generative model then probability of the data would be the same as probability of the models so discriminator will always be confused still 1/2",
    "start": "4367420",
    "end": "4375720"
  },
  {
    "text": "right and here's one particular it seems like a simple idea but it turns out that work remarkably well here's an",
    "start": "4375820",
    "end": "4384099"
  },
  {
    "text": "architecture called D convolutional generative adversarial network architecture that takes the code this is",
    "start": "4384099",
    "end": "4390010"
  },
  {
    "text": "a random code it's a Gaussian code it passes through a sequence of convolutions also a sequence of",
    "start": "4390010",
    "end": "4395530"
  },
  {
    "text": "deconvolution so given the code you sort of deconvolve it back to high dimensional image and you train it using",
    "start": "4395530",
    "end": "4403900"
  },
  {
    "text": "at the settle setting right this is your sampling you generate the image and then",
    "start": "4403900",
    "end": "4408909"
  },
  {
    "text": "there is a discriminator which is just a convolutional neural net what is trying to say is that the real is that a fake and if you train these models on",
    "start": "4408909",
    "end": "4416349"
  },
  {
    "text": "bedrooms these are called L some data sets a bunch of bedrooms this is how",
    "start": "4416349",
    "end": "4421840"
  },
  {
    "text": "samples from the model would look like which is pretty impressive in fact when I look at these samples I'm",
    "start": "4421840",
    "end": "4427690"
  },
  {
    "text": "also sort of maybe the models memorizing the data because these samples look",
    "start": "4427690",
    "end": "4432790"
  },
  {
    "text": "remarkably impressive then there was a follow-up work these are samples from",
    "start": "4432790",
    "end": "4439540"
  },
  {
    "text": "the CFR data set so here you seeing training samples and here you're seeing",
    "start": "4439540",
    "end": "4444760"
  },
  {
    "text": "samples generated from the model which is again very impressive if you look at",
    "start": "4444760",
    "end": "4450310"
  },
  {
    "text": "the structure in these samples it's quite remarkable that you can generate",
    "start": "4450310",
    "end": "4455460"
  },
  {
    "text": "you know samples that look very realistic actually this is what's done",
    "start": "4455460",
    "end": "4461829"
  },
  {
    "text": "again this was done by team team Shannon and his collaborators if you look at the",
    "start": "4461829",
    "end": "4467110"
  },
  {
    "text": "image net and you look at the training data on the image net and looking at the samples again you look live the horse",
    "start": "4467110",
    "end": "4473440"
  },
  {
    "text": "this is like there is some animal there is an airplane and so forth is like some",
    "start": "4473440",
    "end": "4478780"
  },
  {
    "text": "kind of a truck and such right so it looks you know when I look at these images and I was very impressed by the",
    "start": "4478780",
    "end": "4487210"
  },
  {
    "text": "quality of these image because Jo is very very hard to generate realistic looking images and the last thing I want",
    "start": "4487210",
    "end": "4493119"
  },
  {
    "text": "to point out this was picked up by Ian good fellow if we cherry pick some of",
    "start": "4493119",
    "end": "4499060"
  },
  {
    "text": "the examples this is what generated images look like alright so you can sort",
    "start": "4499060",
    "end": "4504190"
  },
  {
    "text": "of like see there is a little bit of interesting structure that you're seeing in these samples right and one question",
    "start": "4504190",
    "end": "4512860"
  },
  {
    "text": "still remains with these models is how can we evaluate these models properly",
    "start": "4512860",
    "end": "4517960"
  },
  {
    "text": "right is the model really learning a space of all possible images and how",
    "start": "4517960",
    "end": "4523930"
  },
  {
    "text": "images what's the coherency in those images or is the model mostly kind of like blurring things around and just",
    "start": "4523930",
    "end": "4530290"
  },
  {
    "text": "making some small changes to the data so the question that I would really want would like to answer to be you know to",
    "start": "4530290",
    "end": "4536500"
  },
  {
    "text": "get an answer to is that if I showing you a new example a new test image a new kind of a horse would the model say yes",
    "start": "4536500",
    "end": "4543400"
  },
  {
    "text": "this is this is a likely image this is this is very probable images I've seen you know similar images before or",
    "start": "4543400",
    "end": "4550390"
  },
  {
    "text": "something like that or not so that still remains an open question but again this is the class of models which steps away",
    "start": "4550390",
    "end": "4556960"
  },
  {
    "text": "from maximum likelihood estimations sort of sets it up in the game theoretic framework which is which is a really",
    "start": "4556960",
    "end": "4563410"
  },
  {
    "text": "nice set of work and in computer vision community a lot of people are shown a lot of progress and using these kinds of",
    "start": "4563410",
    "end": "4569920"
  },
  {
    "text": "models because they tend to generate much more realistic looking images so",
    "start": "4569920",
    "end": "4575830"
  },
  {
    "text": "let me just summarize to say that you know I've shown you hopefully a set of learning algorithms for deep",
    "start": "4575830",
    "end": "4581740"
  },
  {
    "text": "unsupervised models you know there's a lot of space in these models a lot of excitement in that space and just wanted",
    "start": "4581740",
    "end": "4588430"
  },
  {
    "text": "to point out that these models the deep models they improve upon current state of the art and a lot of different application domains and as I mentioned",
    "start": "4588430",
    "end": "4594970"
  },
  {
    "text": "before there's been a lot of progress in discriminative models convolutional models using recurrent neural networks",
    "start": "4594970",
    "end": "4601570"
  },
  {
    "text": "for solving you know action recognition models dealing with videos and unsupervised learning it still remains",
    "start": "4601570",
    "end": "4608790"
  },
  {
    "text": "sort of a field where we've made some progress but there's still a lot of",
    "start": "4608790",
    "end": "4614080"
  },
  {
    "text": "progress to be made and let me let me stop there so thank you",
    "start": "4614080",
    "end": "4620790"
  },
  {
    "text": "do the mics oh sorry so as a Bayesian",
    "start": "4633830",
    "end": "4644730"
  },
  {
    "text": "guy I'm pretty depressed by the fact that can can generate clearer image than the variational in total encoder so my",
    "start": "4644730",
    "end": "4652260"
  },
  {
    "text": "question is do you think there could be a energy based framework or probabilistic interpretation of why",
    "start": "4652260",
    "end": "4658740"
  },
  {
    "text": "again is so successful other than it's just a me max game I think that generally you know if you look at I sort",
    "start": "4658740",
    "end": "4666090"
  },
  {
    "text": "of go back and forth between variational - encoders because you know some of my friends at Open AI saying that they can",
    "start": "4666090",
    "end": "4672150"
  },
  {
    "text": "actually generate really nice-looking images using variational encoders I'm",
    "start": "4672150",
    "end": "4677400"
  },
  {
    "text": "looking at Peter here but you know what I think that one of the problems with",
    "start": "4677400",
    "end": "4683340"
  },
  {
    "text": "image generation today is that with variation autoencoders there is this notion of Gaussian loss function right",
    "start": "4683340",
    "end": "4690750"
  },
  {
    "text": "and what it does is it basically says well never produce you know crystal-clear images because if you're",
    "start": "4690750",
    "end": "4698040"
  },
  {
    "text": "wrong if you put at the edge in the wrong place you're going to be penalized a lot because of the l2 loss function",
    "start": "4698040",
    "end": "4704790"
  },
  {
    "text": "right what the gans are doing ganz are basically saying well I don't really",
    "start": "4704790",
    "end": "4710280"
  },
  {
    "text": "care where I put the edge as long as it looks realistic so that I can fool my classifier so what tends to happen in",
    "start": "4710280",
    "end": "4716130"
  },
  {
    "text": "practice and a lot of times if you actually look at the images generated by Ganz sometimes they have a lot of",
    "start": "4716130",
    "end": "4721560"
  },
  {
    "text": "artifacts like you know these specific things that that pop up right whereas in",
    "start": "4721560",
    "end": "4727410"
  },
  {
    "text": "variation think odors you don't see that but again the problem variational thinker is they tend to produce images that are much more diffused or you know",
    "start": "4727410",
    "end": "4735300"
  },
  {
    "text": "not as short or not as clear as what ganz is doing and there's been some work on you know trying to sharpen the images",
    "start": "4735300",
    "end": "4741030"
  },
  {
    "text": "which is using variational encoders to generate you the globally coherent scene and then you're using generative slls to",
    "start": "4741030",
    "end": "4748110"
  },
  {
    "text": "maybe sharpen it again it's it's it depends what loss function you're using",
    "start": "4748110",
    "end": "4753210"
  },
  {
    "text": "and again seem to be able to deal with that problem implicitly right because they don't really care whether",
    "start": "4753210",
    "end": "4759000"
  },
  {
    "text": "you get the edge quite right or not as long as its Falls your classifier thank",
    "start": "4759000",
    "end": "4765420"
  },
  {
    "text": "you hi thank you very much for the interesting talk I have a question about",
    "start": "4765420",
    "end": "4774450"
  },
  {
    "text": "the evaporation auto-encoder for the Marshall engine data set like the Street",
    "start": "4774450",
    "end": "4780150"
  },
  {
    "text": "View house number I noticed that many implementation they use a PCA to",
    "start": "4780150",
    "end": "4785220"
  },
  {
    "text": "pre-process the data before the trainer model why is your thought on that pea processing step why is necessary to do",
    "start": "4785220",
    "end": "4793260"
  },
  {
    "text": "that right why don't we just learn from the raw pixel I actually don't know my",
    "start": "4793260",
    "end": "4800010"
  },
  {
    "text": "experience has been that we don't really do a lot of pre-processing I mean what you can do is you can do CCA pre-processing is you can take the mean",
    "start": "4800010",
    "end": "4806400"
  },
  {
    "text": "you can take the sort of the second-order covariance structure from the data that sometimes helps sometimes",
    "start": "4806400",
    "end": "4811830"
  },
  {
    "text": "it doesn't but I don't see any particular reason why you'd want to do PCA Prasad pre-processing right I mean it's just one of just like you know",
    "start": "4811830",
    "end": "4820710"
  },
  {
    "text": "we've seen a lot in our field people just doing X Y and then later on they figure out that they don't really need x",
    "start": "4820710",
    "end": "4827010"
  },
  {
    "text": "and y right so it's maybe it was working better for their implementation for",
    "start": "4827010",
    "end": "4832260"
  },
  {
    "text": "their particular task but generally I haven't seen people doing a lot of pre-processing using PCA for training",
    "start": "4832260",
    "end": "4839010"
  },
  {
    "text": "you know variational - encoders",
    "start": "4839010",
    "end": "4842360"
  },
  {
    "text": "any more question yes there's one where",
    "start": "4845990",
    "end": "4851330"
  },
  {
    "text": "is regarding binary rbms so if you look at the literature for let us say",
    "start": "4851330",
    "end": "4858850"
  },
  {
    "text": "estimation of the partition function for easing models right you will see that the literature is lot more rich compared",
    "start": "4858850",
    "end": "4864860"
  },
  {
    "text": "to the variational inference literature for restricted Boltzmann machines especially in the binary context is",
    "start": "4864860",
    "end": "4870560"
  },
  {
    "text": "there like a cultural reason for this because specifically you know like you have for the strictly ferromagnetic case",
    "start": "4870560",
    "end": "4876170"
  },
  {
    "text": "you have a fully polynomial time approximation scheme for estimating the",
    "start": "4876170",
    "end": "4881330"
  },
  {
    "text": "log partition function right so but then I don't see usage of these F press kind",
    "start": "4881330",
    "end": "4886880"
  },
  {
    "text": "of algorithms in the RBM space so when you kind of juxtapose with the literature for the easing models",
    "start": "4886880",
    "end": "4892820"
  },
  {
    "text": "compared to binary rpms you'll find like a very stark asymmetry is there like a reason for this yeah so they think the",
    "start": "4892820",
    "end": "4899030"
  },
  {
    "text": "thing about easing models is that you know here like in ferromagnetic case or if you are you know you have certain",
    "start": "4899030",
    "end": "4905420"
  },
  {
    "text": "particular structure to the easing models you can use a lot of techniques like even if use techniques like coupling from the paths you can draw",
    "start": "4905420",
    "end": "4910460"
  },
  {
    "text": "exact samples from the models right you can compute the log partition function the polynomial convey a specific structure they think about rbms is that",
    "start": "4910460",
    "end": "4917510"
  },
  {
    "text": "generally those assumptions don't apply like you cannot learn them all which is this ferromagnetic model with RBA it's",
    "start": "4917510",
    "end": "4923150"
  },
  {
    "text": "just way all your weights are positive right that's a lot of constraints to put on on these class of models so that's",
    "start": "4923150",
    "end": "4929090"
  },
  {
    "text": "why you know and once you get outside of these assumptions then the problem",
    "start": "4929090",
    "end": "4934550"
  },
  {
    "text": "becomes np-hard right for estimating the partition function and obviously for learning these systems you need the",
    "start": "4934550",
    "end": "4939770"
  },
  {
    "text": "gradient of the log of the partition function right and that's where all the problems come in I don't think there is there is a solution for that and",
    "start": "4939770",
    "end": "4947300"
  },
  {
    "text": "unfortunately variational methods are also not working as well as you know",
    "start": "4947300",
    "end": "4952610"
  },
  {
    "text": "approximations like contrastive divergence or something based on based on sampling people have looked at better",
    "start": "4952610",
    "end": "4958400"
  },
  {
    "text": "approximations and using sort of more sophisticated techniques but it's it hasn't it hasn't really popped up yet",
    "start": "4958400",
    "end": "4963730"
  },
  {
    "text": "practically it just doesn't work as well but it's a good question no my question",
    "start": "4963730",
    "end": "4970490"
  },
  {
    "text": "is about using auto coder with a to get",
    "start": "4970490",
    "end": "4975920"
  },
  {
    "text": "semantic hash especially in text do we need to any",
    "start": "4975920",
    "end": "4981110"
  },
  {
    "text": "special representation text text representation like word to vector as",
    "start": "4981110",
    "end": "4987260"
  },
  {
    "text": "input for the for our text sequence so you know I've talked about them all",
    "start": "4987260",
    "end": "4993530"
  },
  {
    "text": "which is very simple knowledge is modeling bag of words yes right obviously you can take word to Veck and initialize the model right because it's",
    "start": "4993530",
    "end": "5000220"
  },
  {
    "text": "a way of just taking your words and projecting them into the semantic space yeah right there is been a lot of",
    "start": "5000220",
    "end": "5007090"
  },
  {
    "text": "technique recent techniques using like richard was mentioning gr use as a way you know if you want if you want to work",
    "start": "5007090",
    "end": "5012850"
  },
  {
    "text": "with sentences or if you want to embed the entire document into the semantic space if you want to make it binary you",
    "start": "5012850",
    "end": "5019630"
  },
  {
    "text": "know you can use gr use bi-directional gr use sort of get the representation of the document I think that would probably",
    "start": "5019630",
    "end": "5025150"
  },
  {
    "text": "work better than using word to back and then just adding things up and then based on that you can learn a hashing function that map's that particular",
    "start": "5025150",
    "end": "5031420"
  },
  {
    "text": "representation to the binary space right in which case you can you can do searching fairly efficiently so as an",
    "start": "5031420",
    "end": "5037900"
  },
  {
    "text": "input representation a lots of choices you can use bi-directional gr use which is the you know the the the the method",
    "start": "5037900",
    "end": "5045460"
  },
  {
    "text": "of choice right now you can use glove or you can use work the rack can be you",
    "start": "5045460",
    "end": "5050620"
  },
  {
    "text": "know some them sum up the representations of the words okay so using only back of word we use only",
    "start": "5050620",
    "end": "5056830"
  },
  {
    "text": "normal needs work neural network that is no records which were or yeah yeah that's right that's simple it's just right but again",
    "start": "5056830",
    "end": "5063070"
  },
  {
    "text": "you're doing your representation can be whatever that representation is as long as differentiable right okay so in this case you can you can sort of back",
    "start": "5063070",
    "end": "5068770"
  },
  {
    "text": "propagate through the bi-directional gr use and get rip you know learn what",
    "start": "5068770",
    "end": "5074950"
  },
  {
    "text": "there is what this game isn't a Shashank you so much okay let's thank Russ again",
    "start": "5074950",
    "end": "5083760"
  }
]