[
  {
    "start": "0",
    "end": "120000"
  },
  {
    "text": "The thing I would very much like to talk about today is the state of the art in deep learning.",
    "start": "0",
    "end": "6133"
  },
  {
    "text": "Here we stand in 2019 really at the height of some of the great accomplishments",
    "start": "6166",
    "end": "11299"
  },
  {
    "text": "that have happened. But also stand at the beginning. And it's up to us to define where this incredible",
    "start": "11367",
    "end": "18333"
  },
  {
    "text": "data-driven technology takes us. And so I'd like to talk a little bit about the breakthroughs that happened in 2017 and 2018",
    "start": "18400",
    "end": "26533"
  },
  {
    "text": "that take us to this point. So this lecture is not on the state of the art results on",
    "start": "26634",
    "end": "36200"
  },
  {
    "text": "main machine learning benchmarks. So the various image classification and object detection",
    "start": "36266",
    "end": "43266"
  },
  {
    "text": "or the NLP benchmarks or the GAN benchmarks. This isn't about the cutting edge algorithm",
    "start": "43333",
    "end": "51634"
  },
  {
    "text": "that's available on github that performs best on a particular benchmark. This is about ideas",
    "start": "51664",
    "end": "58166"
  },
  {
    "text": "ideas and developments that are at the cutting edge of what defines this exciting field of deep learning.",
    "start": "58333",
    "end": "65834"
  },
  {
    "text": "And so I'd like to go through a bunch of different areas that I think they're really exciting.",
    "start": "65867",
    "end": "71333"
  },
  {
    "text": "Of course this is also not a lecture that's complete There's other things that may be totally missing that happened",
    "start": "71367",
    "end": "78299"
  },
  {
    "text": "in 2017-18 that are particularly exciting to people here and people beyond.",
    "start": "78367",
    "end": "84000"
  },
  {
    "text": "For example medical applications of deep learning is something I totally don't touch on.",
    "start": "84133",
    "end": "89033"
  },
  {
    "text": "And protein folding and all kinds of applications that there has been some exciting developments",
    "start": "90734",
    "end": "96400"
  },
  {
    "text": "from deep mind and so on that don't touch on. So forgive me if your favorite developments are missing",
    "start": "96400",
    "end": "103033"
  },
  {
    "text": "but hopefully this encompasses some of the really fundamental things that have happened",
    "start": "103100",
    "end": "108500"
  },
  {
    "text": "both on the theory side and the application side and then the community side of all of us being able to work",
    "start": "108533",
    "end": "114232"
  },
  {
    "text": "together on this and these kinds of technologies. I think 2018 in terms of deep learning is the year of",
    "start": "114300",
    "end": "121567"
  },
  {
    "start": "120000",
    "end": "840000"
  },
  {
    "text": "natural language processing. Many have described this year as the ImageNet moment.",
    "start": "121634",
    "end": "128100"
  },
  {
    "text": "In 2012 for computer vision when AlexNet was the first neural network that really gave that big jump in performance.",
    "start": "128300",
    "end": "136867"
  },
  {
    "text": "And computer vision it started to inspire people what's possible with deep learning with purely learning based methods.",
    "start": "136867",
    "end": "142900"
  },
  {
    "text": "In the same way there's been a series of developments from 2016-17 led up to 18 with a development of BERT",
    "start": "142934",
    "end": "153000"
  },
  {
    "text": "that has made on benchmarks and in our ability to",
    "start": "153133",
    "end": "160500"
  },
  {
    "text": "apply NLP to solve various NLP tasks, natural language processing tasks a total leap.",
    "start": "160533",
    "end": "167333"
  },
  {
    "text": "So let's tell the story of what takes us there. There's a few developments. I've mentioned a little bit on Monday",
    "start": "167367",
    "end": "174066"
  },
  {
    "text": "about the encoder decoder or recurrent neural networks. So this idea of recurrent neural networks encode sequences of data",
    "start": "174133",
    "end": "185734"
  },
  {
    "text": "and output something, output either a single prediction or another sequence.",
    "start": "186000",
    "end": "192933"
  },
  {
    "text": "When the input sequence and the output sequence are not the same, necessarily the same size,",
    "start": "193000",
    "end": "199099"
  },
  {
    "text": "they're like in machine translation we have to translate from one language to another the encoder decoder architecture takes the following process.",
    "start": "199300",
    "end": "210500"
  },
  {
    "text": "It takes in the sequence of words or the sequence of samples as the input",
    "start": "210667",
    "end": "215834"
  },
  {
    "text": "and uses the recurrent units whether LSTM, GRU and beyond",
    "start": "215867",
    "end": "222233"
  },
  {
    "text": "and encodes that sentence into a single vector. So forms an embedding of that sentence of what it",
    "start": "222667",
    "end": "231000"
  },
  {
    "text": "represent, representation of that sentence. And then feeds that representation in the decoder",
    "start": "231033",
    "end": "239800"
  },
  {
    "text": "recurrent neural network that then generates the sequence of words that form",
    "start": "239867",
    "end": "248567"
  },
  {
    "text": "the sentence in the language that's being translated to. So first you encode by taking the sequence and",
    "start": "248600",
    "end": "256000"
  },
  {
    "text": "mapping it to a fixed size vector representation. And then you decode by taking that fixed size vector representation",
    "start": "256033",
    "end": "265934"
  },
  {
    "text": "and unrolling it into the sentence that can be of different length than the input sentence. Okay that's the encoder-decoder structure for recurrent neural networks",
    "start": "265964",
    "end": "272867"
  },
  {
    "text": "has been very effective for machine translation and dealing with arbitrary length input sequences,",
    "start": "273066",
    "end": "279967"
  },
  {
    "text": "arbitrary length output sequences. Next step attention.",
    "start": "280000",
    "end": "285100"
  },
  {
    "text": "What is attention? Well it's the next step beyond it's an improvement on the",
    "start": "285300",
    "end": "290733"
  },
  {
    "text": "the encoder-decoder architecture.",
    "start": "290795",
    "end": "296233"
  },
  {
    "text": "It allows the, it provides a mechanism that allows to look back at the input sequence.",
    "start": "296433",
    "end": "302432"
  },
  {
    "text": "So suppose to saying that you have a sequence that's the input sentence",
    "start": "302467",
    "end": "307767"
  },
  {
    "text": "and that all gets collapsed into a single vector representation. You're allowed to look back at the particular samples from the input sequence",
    "start": "309266",
    "end": "315767"
  },
  {
    "text": "as part of the decoding process. That's attention and you can also learn which aspects",
    "start": "317000",
    "end": "324332"
  },
  {
    "text": "are important for which aspects of the decoding process, which aspects the input sequence",
    "start": "324367",
    "end": "331000"
  },
  {
    "text": "are important to the output sequence. Visualize in another way",
    "start": "331233",
    "end": "336733"
  },
  {
    "text": "and there's a few visualizations here. They're quite incredible that are done by Jay Alammar.",
    "start": "336811",
    "end": "344934"
  },
  {
    "text": "I highly recommend you follow the links and look at the",
    "start": "344967",
    "end": "351133"
  },
  {
    "text": "further details of these visualizations of attention. So if we look at neural machine translation",
    "start": "351223",
    "end": "356900"
  },
  {
    "text": "the encoder RNN takes a sequence of words and throughout, after every sequence forms a set of",
    "start": "356988",
    "end": "366166"
  },
  {
    "text": "hidden representations, hidden state that captures the representation of the worlds that followed.",
    "start": "366196",
    "end": "371500"
  },
  {
    "text": "And those sets of hidden representations as opposed to being collapsed to a single fixed size vector, are then",
    "start": "372533",
    "end": "379367"
  },
  {
    "text": "all pushed forward to the decoder. That are then used by the decoder to translate",
    "start": "379400",
    "end": "385600"
  },
  {
    "text": "but in a selective way. Where the decoder here visualized on the y-axis",
    "start": "385667",
    "end": "392000"
  },
  {
    "text": "the input language and on the X the output language the decoder weighs the different parts of the input sequence differently",
    "start": "392166",
    "end": "403834"
  },
  {
    "text": "in order to determine how to best translate generate the word that forms a translation in the full output sentence.",
    "start": "403867",
    "end": "411734"
  },
  {
    "text": "Okay that's attention, allowing expanding the encoder-decoder architecture",
    "start": "411767",
    "end": "417667"
  },
  {
    "text": "to allow for selective attention to the input sequence",
    "start": "417748",
    "end": "425065"
  },
  {
    "text": "as opposed to collapsing everything down into fixed representation. Okay next step self-attention.",
    "start": "425100",
    "end": "432033"
  },
  {
    "text": "In the encoding process allowing the encoder to also",
    "start": "432166",
    "end": "439266"
  },
  {
    "text": "selectively look informing the hidden representations",
    "start": "439342",
    "end": "444300"
  },
  {
    "text": "at other parts of the input sequence in order to form those representations.",
    "start": "444367",
    "end": "449500"
  },
  {
    "text": "It allows you to determine for certain words.",
    "start": "449533",
    "end": "455500"
  },
  {
    "text": "What are the important relevant aspects of the input sequence that can help you encode that word the best?",
    "start": "455734",
    "end": "462233"
  },
  {
    "text": "So it improves the encoder process by allowing it to look at the entirety of the context. That's self-attention.",
    "start": "463333",
    "end": "470232"
  },
  {
    "text": "Building a transformer. It's using the self attention mechanism in the encoder",
    "start": "472734",
    "end": "480266"
  },
  {
    "text": "to form these sets of representations on the input sequence. And then as part of the decoding process follow the same",
    "start": "480333",
    "end": "488200"
  },
  {
    "text": "but in reverse with a bunch of self-attention that's able to look back again.",
    "start": "488269",
    "end": "493966"
  },
  {
    "text": "So it's self attention on the encoder attention on the decoder and that's where the magic, that's where the entirety magic is.",
    "start": "493997",
    "end": "501967"
  },
  {
    "text": "That's able to capture the rich context of the input sequence in order to generate",
    "start": "501997",
    "end": "507366"
  },
  {
    "text": "in the contextual way the output sequence. So let's take a step back then and look at what is critical to natural language",
    "start": "507437",
    "end": "516800"
  },
  {
    "text": "in order to be able to reason about words, construct a language model",
    "start": "516834",
    "end": "522199"
  },
  {
    "text": "and be able to reason about the words in order to classify a sentence or translate a sentence",
    "start": "522233",
    "end": "528500"
  },
  {
    "text": "or compare two sentences and so on. There the sentences are collections of words or characters",
    "start": "528700",
    "end": "537032"
  },
  {
    "text": "and those characters and words have to have an efficient representation that's meaningful for that kind of understanding.",
    "start": "537100",
    "end": "543066"
  },
  {
    "text": "And that's what the process of embedding is. We talked a little bit about it on Monday. And so the traditional Word2Vec  process of",
    "start": "543100",
    "end": "551967"
  },
  {
    "text": "embedding is you use some kind of trick in an unsupervised way to map words into",
    "start": "551997",
    "end": "557433"
  },
  {
    "text": "into a compressed representation. So language modeling is the process of determining",
    "start": "557506",
    "end": "566900"
  },
  {
    "text": "which words follow each other usually. So one way you can use it as in a skip gram model",
    "start": "566934",
    "end": "573500"
  },
  {
    "text": "taking a huge datasets of words you know, there's writing all over the place taking those datasets",
    "start": "573590",
    "end": "579867"
  },
  {
    "text": "and feeding a neural network that in a supervised way looks",
    "start": "579900",
    "end": "585433"
  },
  {
    "text": "at which words are usually follow the input. So the input is a word the output is which word are",
    "start": "585518",
    "end": "593233"
  },
  {
    "text": "statistically likely to follow that word. And the same with the preceding word. And doing this kind of unsupervised learning",
    "start": "593310",
    "end": "599433"
  },
  {
    "text": "if you throw away the output and the input and just taking the hidden representation form in the middle",
    "start": "602567",
    "end": "607967"
  },
  {
    "text": "that's how you form this compressed embedding a meaningful representation that when",
    "start": "608000",
    "end": "614300"
  },
  {
    "text": "two words are related in a language modeling sense, two words that are related they're going to be in that representation close to each other.",
    "start": "614500",
    "end": "621767"
  },
  {
    "text": "And when they're totally unrelated have nothing to do with each other they're far away ELMo is the approach of using bi-directional L STMs",
    "start": "621800",
    "end": "633033"
  },
  {
    "text": "to learn that representation. And what bi-directional, bi-directionally? So looking not just the sequence that let up to the word but in both directions the sequence that",
    "start": "633063",
    "end": "639066"
  },
  {
    "text": "following, the sequence that before. And that allows you to learn the rich full context of the word.",
    "start": "639139",
    "end": "648934"
  },
  {
    "text": "In learning the rich full context of the word you're forming representations that are much better able to represent the statistical language model",
    "start": "649333",
    "end": "664867"
  },
  {
    "text": "behind the kind of corpus of language that you're you're looking at. And this has taken a big leap in ability to then",
    "start": "664897",
    "end": "671300"
  },
  {
    "text": "that for further algorithms then with the language model a reasoning about doing things like",
    "start": "671362",
    "end": "677934"
  },
  {
    "text": "sentence classification, sentence comparison, so on. Translation that representation is much more effective",
    "start": "677967",
    "end": "684100"
  },
  {
    "text": "for working with language. The idea of the OpenAI transformer",
    "start": "684164",
    "end": "690033"
  },
  {
    "text": "is the next step forward is taking the the same transformer that I mentioned previously.",
    "start": "690097",
    "end": "697433"
  },
  {
    "text": "The encoder with self-attention decoder with attention looking back at the input sequence.",
    "start": "697467",
    "end": "702532"
  },
  {
    "text": "And using, taking the language learned by the decoder",
    "start": "702567",
    "end": "709867"
  },
  {
    "text": "and using that as a language model and then chopping off layers and training in a specific on a specific language tasks like sentence classification.",
    "start": "712734",
    "end": "721500"
  },
  {
    "text": "Now BERT is the thing that did the big leap in performance.",
    "start": "722000",
    "end": "727800"
  },
  {
    "text": "With the transformer formulation there is always there's no bi-directional element.",
    "start": "727834",
    "end": "733100"
  },
  {
    "text": "There is, it's always moving forward. So the encoding step and the decoding step with BERT is",
    "start": "733634",
    "end": "739834"
  },
  {
    "text": "it's richly bi-directional it takes in the full sequence of the sentence",
    "start": "740734",
    "end": "746600"
  },
  {
    "text": "and masks out some percentage of the words, 15% of the words.",
    "start": "749834",
    "end": "756165"
  },
  {
    "text": "15% of the samples of tokens from the sequence. And tasks the entire encoding",
    "start": "756200",
    "end": "766033"
  },
  {
    "text": "self-attention mechanism to predict the words that are missing.",
    "start": "766118",
    "end": "771734"
  },
  {
    "text": "That construct and then you stack a ton of them together. A ton of those encoders self-attention feed-forward network,",
    "start": "771767",
    "end": "782367"
  },
  {
    "text": "self attention feed forward network together. And that allows you to learn the rich context of the language to then at the end perform all kinds of tasks.",
    "start": "782397",
    "end": "789734"
  },
  {
    "text": "You can create first of all, like Elmo and like Word2Vec, create rich contextual embeddings.",
    "start": "790000",
    "end": "797467"
  },
  {
    "text": "Take a set of words and represent them in the space that's very efficient to reason with.",
    "start": "797533",
    "end": "804165"
  },
  {
    "text": "You can do language classification, you can do settings pair classification,",
    "start": "804200",
    "end": "809467"
  },
  {
    "text": "you can do the similarity of two sentences, multiple choice question answering, general question answering,",
    "start": "809500",
    "end": "814667"
  },
  {
    "text": "tagging of sentences. okay I'll link it on that one a little bit too long.",
    "start": "814700",
    "end": "822300"
  },
  {
    "text": "but it is also the one I'm really excited about and really if there's a breakthrough this year",
    "start": "822333",
    "end": "828066"
  },
  {
    "text": "is been it's thanks to BERT. The other thing I'm very excited about is totally",
    "start": "828100",
    "end": "834867"
  },
  {
    "text": "jumping away from the new rips,",
    "start": "834931",
    "end": "840967"
  },
  {
    "start": "840000",
    "end": "985000"
  },
  {
    "text": "the theory, those kind of academic developments and deep learning and into the world of applied deep learning.",
    "start": "840997",
    "end": "849367"
  },
  {
    "text": "So Tesla has a system called Autopilot",
    "start": "850000",
    "end": "855000"
  },
  {
    "text": "where the hardware version 2 of that system",
    "start": "855033",
    "end": "858600"
  },
  {
    "text": "is a newer  implementation of the NVIDIA Drive PX 2 system",
    "start": "860233",
    "end": "868933"
  },
  {
    "text": "which runs a ton of neural networks. There's 8 cameras on the car and",
    "start": "868967",
    "end": "876767"
  },
  {
    "text": "a variant of the inception network is now taking in all a cameras",
    "start": "876841",
    "end": "885700"
  },
  {
    "text": "at different resolutions as input and performing various tasks,",
    "start": "885777",
    "end": "891033"
  },
  {
    "text": "like drivable area segmentation, like object detection and some basic localization tasks.",
    "start": "891266",
    "end": "898033"
  },
  {
    "text": "So you have now a huge fleet of vehicles where it's not engineers",
    "start": "898100",
    "end": "906000"
  },
  {
    "text": "some I'm sure engineers but it's really regular consumers, people that have purchased the car have no understanding",
    "start": "906166",
    "end": "913433"
  },
  {
    "text": "in many cases of what neural networks limitations the capabilities are so on. Now it has a neural network is controlling the well being",
    "start": "913497",
    "end": "921533"
  },
  {
    "text": "has its decisions, its perceptions and the control decisions based on those perceptions",
    "start": "921603",
    "end": "927900"
  },
  {
    "text": "are controlling the life of a human being. And that to me is one of the great breakthroughs of 17 and 18.",
    "start": "927985",
    "end": "935333"
  },
  {
    "text": "In terms of the development of what AI can do in a practical sense in impacting the world.",
    "start": "935401",
    "end": "943866"
  },
  {
    "text": "And so one billion miles over 1 billion miles have been driven in Autopilot.",
    "start": "944033",
    "end": "949133"
  },
  {
    "text": "Now there's two types of systems in currently operating in Tesla's. .There's hardware version 1, hardware version 2.",
    "start": "949166",
    "end": "956667"
  },
  {
    "text": "Hardware version 1 was Intel Mobileye monocular camera perception system.",
    "start": "956667",
    "end": "961934"
  },
  {
    "text": "As far as we know that was not using a neural network. And it was a fix system. That wasn't learning, at least online learning in the Tesla's.",
    "start": "961967",
    "end": "968667"
  },
  {
    "text": "The other is hardware version 2 and it's about half and half now in terms of the miles driven.",
    "start": "968697",
    "end": "974233"
  },
  {
    "text": "The hardware version 2 has a neural network that's always learning. There's weekly updates. It's always improving the model shipping new weights and so on.",
    "start": "974266",
    "end": "982166"
  },
  {
    "text": "That's the exciting set of breakthroughs in terms of AutoML, the dream of automating some aspects or",
    "start": "982200",
    "end": "991667"
  },
  {
    "start": "985000",
    "end": "1112000"
  },
  {
    "text": "all aspects or many aspects as possible of the machine learning process where you can just drop in a dataset that you're working on",
    "start": "991731",
    "end": "1001933"
  },
  {
    "text": "and the system will automatically determine all the parameters",
    "start": "1002133",
    "end": "1007100"
  },
  {
    "text": "from the details of the architectures, the size are the architecture, the different modules and then architecture",
    "start": "1007300",
    "end": "1014100"
  },
  {
    "text": "the hyper parameters use for training the architecture running that they're doing the inference everything.",
    "start": "1014333",
    "end": "1020366"
  },
  {
    "text": "All is done for you. All you just feed it is data So that's been the success of the neural architecture search in 16 and 17.",
    "start": "1020400",
    "end": "1030433"
  },
  {
    "text": "And there's been a few ideas with Google AutoML that's really trying to almost create an API we just drop in data set.",
    "start": "1030500",
    "end": "1036799"
  },
  {
    "text": "And it's using reinforcement learning and recurrent neural networks to given a few modules,",
    "start": "1036834",
    "end": "1044433"
  },
  {
    "text": "stitch them together in such a way where the objective function is optimizing the performance of the overall system.",
    "start": "1044467",
    "end": "1050533"
  },
  {
    "text": "And they've showed a lot of exciting results. Google showed and others that outperform state of art systems",
    "start": "1050567",
    "end": "1056467"
  },
  {
    "text": "both in terms of efficiency and in terms of accuracy. Now in 18 there've been a few improvements on",
    "start": "1056500",
    "end": "1063967"
  },
  {
    "text": "this direction and one of them is a AdaNet where it's now using the same reinforcement",
    "start": "1064000",
    "end": "1071399"
  },
  {
    "text": "learning AutoML formulation to build ensembles on your network. So in many cases state-of-the-art performance can be achieved",
    "start": "1071433",
    "end": "1079100"
  },
  {
    "text": "by as opposed to taking a single architecture, is building up a multitude and ensemble a collection of architectures.",
    "start": "1079188",
    "end": "1086967"
  },
  {
    "text": "And that's what is doing here is given candidate architectures,",
    "start": "1087000",
    "end": "1092000"
  },
  {
    "text": "stitching them together to form an ensemble to get state-of-the-art performance. Now that state of the art performance is not a leap",
    "start": "1092084",
    "end": "1100633"
  },
  {
    "text": "a breakthrough leap forward but it's nevertheless a step forward. And it's a very exciting field that's going to be",
    "start": "1100699",
    "end": "1108667"
  },
  {
    "text": "receiving more and more attention. There's an area of machine learning that's heavily under studied",
    "start": "1108742",
    "end": "1115533"
  },
  {
    "start": "1112000",
    "end": "1373000"
  },
  {
    "text": "and I think it's extremely exciting area. And if you look at 2012 with AlexNet achieving",
    "start": "1115567",
    "end": "1126367"
  },
  {
    "text": "the breakthrough performance of showing what deep learning networks are capable of.",
    "start": "1126444",
    "end": "1132333"
  },
  {
    "text": "From that point, from 2012 to today there's been non-stop",
    "start": "1132367",
    "end": "1138065"
  },
  {
    "text": "extremely active developments of different architectures that even on just ImageNet alone on doing the image classification task",
    "start": "1138127",
    "end": "1144700"
  },
  {
    "text": "have improved performance over and over and over with totally new ideas.",
    "start": "1144784",
    "end": "1151467"
  },
  {
    "text": "Now on the other side on the data side there's been very few ideas about how to do data augmentation.",
    "start": "1151500",
    "end": "1159667"
  },
  {
    "text": "So data augmentation is the process of, you know, it's what",
    "start": "1159700",
    "end": "1167500"
  },
  {
    "text": "kids always do when you learn about an object right? You look at an object and you kind of like twist it around is",
    "start": "1167533",
    "end": "1176066"
  },
  {
    "text": "is taking the raw data and messing it in such a way",
    "start": "1176133",
    "end": "1182300"
  },
  {
    "text": "that it can give you much richer representation of what this can this data can look like in other forms",
    "start": "1182367",
    "end": "1188767"
  },
  {
    "text": "in other contexts in the real world. There's been very few developments I think still",
    "start": "1188838",
    "end": "1195867"
  },
  {
    "text": "and there's this AutoAugment is just a step a tiny step into that direction that I hope that",
    "start": "1195952",
    "end": "1203000"
  },
  {
    "text": "we as a community invest a lot of effort in. So what AutoAugment does? As it says, ok, so there's these data augmentation methods",
    "start": "1203033",
    "end": "1213567"
  },
  {
    "text": "like translating the image, sharing the image, doing color manipulation like color inversion.",
    "start": "1213655",
    "end": "1219433"
  },
  {
    "text": "Let's take those as basic actions you can take and then use reinforcement learning and an RNN again construct to stitch those actions",
    "start": "1219467",
    "end": "1230667"
  },
  {
    "text": "together in such a way that can augment data like an ImageNet, you train on the data, it gets state-of-the-art performance.",
    "start": "1230697",
    "end": "1240533"
  },
  {
    "text": "So mess with the data in a way that optimizes the way you mess with the data. So.",
    "start": "1240600",
    "end": "1247166"
  },
  {
    "text": "And then they've also showed that given that the",
    "start": "1247367",
    "end": "1252767"
  },
  {
    "text": "set of data augmentation policies that are learned to optimize for example for ImageNet",
    "start": "1252859",
    "end": "1262634"
  },
  {
    "text": "given the some kind of architecture you can take that learn the set of policies for data augmentation and apply it to a totally different dataset.",
    "start": "1262664",
    "end": "1270967"
  },
  {
    "text": "So there's the process of transfer learning. So what is transfer learning?",
    "start": "1271000",
    "end": "1277333"
  },
  {
    "text": "We talked about transfer learning, you have a neural network that learns to do cat versus dog",
    "start": "1277333",
    "end": "1283100"
  },
  {
    "text": "or no learns to do a thousand class classification problem on image. And then you transfer, you chop off few layers and you transfer on the task of",
    "start": "1283166",
    "end": "1290767"
  },
  {
    "text": "your own dataset of cat versus dog. What you're transferring is the weights",
    "start": "1290855",
    "end": "1297600"
  },
  {
    "text": "that are learned on the ImageNet classification task. And now you're then fine-tuning those weights on the",
    "start": "1297681",
    "end": "1305633"
  },
  {
    "text": "specific, personal cat vs. dog dataset you have.",
    "start": "1305720",
    "end": "1311567"
  },
  {
    "text": "Now you can do the same thing here. You can transfer as part of the transfer learning process,",
    "start": "1312634",
    "end": "1318967"
  },
  {
    "text": "take the data augmentation policies learned on ImageNet,",
    "start": "1319000",
    "end": "1324567"
  },
  {
    "text": "and transfer those. You can transfer both the weights and the policies. That's a really super exciting idea I think.",
    "start": "1324600",
    "end": "1333600"
  },
  {
    "text": "It wasn't quite demonstrated extremely well here in terms of performance,",
    "start": "1333662",
    "end": "1338700"
  },
  {
    "text": "so it got an improvement in performance and so on, but any kind of inspired an idea that's something",
    "start": "1338700",
    "end": "1345299"
  },
  {
    "text": "that we need to really think about. How to augment data in an interesting way such that given just a few samples of data?",
    "start": "1345388",
    "end": "1354934"
  },
  {
    "text": "We can generate huge data sets in a way that you can then form meaningful complex rich representations from.",
    "start": "1354967",
    "end": "1363700"
  },
  {
    "text": "I think that's really exciting in one of the ways that you break open the problem of how do we learn a lot from a little.",
    "start": "1363734",
    "end": "1370200"
  },
  {
    "text": "Training deep neural networks with synthetic data. This also really an exciting topic",
    "start": "1370233",
    "end": "1379133"
  },
  {
    "start": "1373000",
    "end": "1477000"
  },
  {
    "text": "that a few groups but especially NVIDIA invested a lot in. Here's a from a CVPR2018 probably my favorite work on this topic",
    "start": "1379166",
    "end": "1388567"
  },
  {
    "text": "is they really went crazy and said ok let's mess",
    "start": "1388834",
    "end": "1394066"
  },
  {
    "text": "with synthetic data in every way we could possibly can. So on the left there're shown a set of backgrounds",
    "start": "1394141",
    "end": "1401400"
  },
  {
    "text": "then there's also a set of artificial objects and you have a car or some kind of object that you're trying to classify.",
    "start": "1401462",
    "end": "1407433"
  },
  {
    "text": "So let's take that car and mess with it with every way possible. Apply lighting variation to whatever way possible,",
    "start": "1407467",
    "end": "1415700"
  },
  {
    "text": "rotate everything that is crazy so what NVIDIA is really good at is creating realistic scenes.",
    "start": "1415734",
    "end": "1423867"
  },
  {
    "text": "And they said okay let's create realistic scenes but let's also go away aboveboard and not do realistic at all.",
    "start": "1423900",
    "end": "1430934"
  },
  {
    "text": "Do things that can't possibly happen in reality. And so generally these huge datasets I want",
    "start": "1430967",
    "end": "1436533"
  },
  {
    "text": "to train and again achieve quite interesting quite a quite good performance",
    "start": "1436610",
    "end": "1442900"
  },
  {
    "text": "on image classification. Of course they're trying to apply  to ImageNet and so on these kinds of tasks,",
    "start": "1443000",
    "end": "1448433"
  },
  {
    "text": "you're not going to outperform networks that were trained on ImageNet. But they show that with just a small sample from from those real images",
    "start": "1448467",
    "end": "1458433"
  },
  {
    "text": "they can fine tune this network train on synthetic images, totally fake images to achieve state of the art performance.",
    "start": "1458634",
    "end": "1464500"
  },
  {
    "text": "Again another way to generate, to get, to learn a lot for very little",
    "start": "1464533",
    "end": "1469734"
  },
  {
    "text": "by generating fake worlds synthetically.",
    "start": "1469767",
    "end": "1474867"
  },
  {
    "start": "1477000",
    "end": "1599000"
  },
  {
    "text": "The process of annotation which for supervised learning is what you need to do in order to",
    "start": "1477033",
    "end": "1485066"
  },
  {
    "text": "train the network, you need to be able to provide ground truth, you need to be able to label whatever the entity that is being learned.",
    "start": "1485266",
    "end": "1492767"
  },
  {
    "text": "And so for image classification that's saying what is going on in the image. And part of that was done on ImageNet by",
    "start": "1492834",
    "end": "1499667"
  },
  {
    "text": "doing a Google search for creating candidates. Now saying what's going on in the image is a pretty easy tasks.",
    "start": "1499747",
    "end": "1506466"
  },
  {
    "text": "Then there is the object detection task of detecting the boundary box.",
    "start": "1506500",
    "end": "1511867"
  },
  {
    "text": "And so saying drawing the actual boundary box is a little bit more difficult but it's a couple of clicks and so on.",
    "start": "1512033",
    "end": "1519300"
  },
  {
    "text": "Then if we take the finals the probably one of the higher complexity tasks of perception",
    "start": "1519467",
    "end": "1527333"
  },
  {
    "text": "of image understanding is segmentation. It's actually drawing either pixel level or polygons",
    "start": "1527367",
    "end": "1534567"
  },
  {
    "text": "the outline of particular object. Now if you have to annotate that that's extremely costly.",
    "start": "1534656",
    "end": "1539767"
  },
  {
    "text": "So the work with Polygon-RNN is to use recurrent neural networks to make suggestions for polygons.",
    "start": "1539834",
    "end": "1546734"
  },
  {
    "text": "It's really interesting. There's a few tricks to form these high-resolution polygons.",
    "start": "1546767",
    "end": "1552433"
  },
  {
    "text": "So the idea is it drops in a single point you draw a boundary box around an object.",
    "start": "1552467",
    "end": "1559900"
  },
  {
    "text": "You use convolutional neural networks to drop the first point. And then use recurrent neural networks to draw around it.",
    "start": "1559934",
    "end": "1567132"
  },
  {
    "text": "And the performance is really good There's a few tricks and this tool is available online.",
    "start": "1567166",
    "end": "1573000"
  },
  {
    "text": "It's a really interesting idea again the dream with AutoML is to remove",
    "start": "1573033",
    "end": "1578466"
  },
  {
    "text": "the human from the picture as much as possible. With data augmentation remove the human from the",
    "start": "1578536",
    "end": "1583566"
  },
  {
    "text": "picture as much as possible for a menial data. Automate the boring stuff and in this case",
    "start": "1583600",
    "end": "1588800"
  },
  {
    "text": "the act of drawing a polygon tried to automated as much as possible. The interesting other dimension along which",
    "start": "1588849",
    "end": "1601567"
  },
  {
    "start": "1599000",
    "end": "1746000"
  },
  {
    "text": "deep learning is recently being trying to be optimized is how do we make deep learning accessible.",
    "start": "1601600",
    "end": "1610200"
  },
  {
    "text": "Fast, cheap, accessible. So the DAWNBench from Stanford the benchmark",
    "start": "1610233",
    "end": "1616332"
  },
  {
    "text": "the DAWNBench benchmark from Stanford asked formulated an interesting competition,",
    "start": "1616413",
    "end": "1622967"
  },
  {
    "text": "which got a lot of attention and a lot of progress. It's saying if we want to achieve 93% accuracy",
    "start": "1623000",
    "end": "1630900"
  },
  {
    "text": "on ImageNet and 94% on CIFAR10, let's now compete, that's like the requirement,",
    "start": "1630965",
    "end": "1637934"
  },
  {
    "text": "let's now compete how you can do it in the least amount of time and for the least amount of dollars.",
    "start": "1638006",
    "end": "1644166"
  },
  {
    "text": "Do the training in the least amount of time and the training in the least amount of dollars like literally dollars you are allowed to spend to do this.",
    "start": "1644934",
    "end": "1653066"
  },
  {
    "text": "And fast AI you know it's a renegade awesome renegade group of deep learning researchers",
    "start": "1653100",
    "end": "1660567"
  },
  {
    "text": "have been able to train on ImageNet in 3 hours. So this is for training process for 25 bucks.",
    "start": "1661667",
    "end": "1669000"
  },
  {
    "text": "So training a network that achieves 93% accuracy for 25 bucks,",
    "start": "1669166",
    "end": "1675300"
  },
  {
    "text": "and 94% accuracy for 26 cents on CIFAR10.",
    "start": "1675333",
    "end": "1680767"
  },
  {
    "text": "So the key idea that they were playing with is quite simple. But really boils down to messing with the learning rate",
    "start": "1680834",
    "end": "1689300"
  },
  {
    "text": "throughout the process of training. So the learning rate is how much you based on the loss function",
    "start": "1689333",
    "end": "1695132"
  },
  {
    "text": "based on the error the neural network observes, how much do you adjust the weights. So they found that if they crank up the learning rate",
    "start": "1695201",
    "end": "1707367"
  },
  {
    "text": "while decreasing the momentum, which is a parameter of the optimization process,",
    "start": "1707410",
    "end": "1713734"
  },
  {
    "text": "and they do it that jointly they're able to make the network learn really fast. That's really exciting and the benchmark itself is also really exciting",
    "start": "1713900",
    "end": "1723467"
  },
  {
    "text": "because that's exactly for people sitting in this room that opens up the door to doing all kinds of fundamental deep learning",
    "start": "1723500",
    "end": "1732800"
  },
  {
    "text": "problems without the resources of Google DeepMind or OpenAI or Facebook or so on, without computational resources.",
    "start": "1732834",
    "end": "1740667"
  },
  {
    "text": "That's important for academia that's important for independent researchers and so on. So GANs. There's been a lot of work on",
    "start": "1740700",
    "end": "1749299"
  },
  {
    "start": "1746000",
    "end": "1814000"
  },
  {
    "text": "generative adversarial neural networks. And in some ways there has not been breakthrough",
    "start": "1749364",
    "end": "1756367"
  },
  {
    "text": "ideas in GANs for quite a bit.",
    "start": "1756437",
    "end": "1762066"
  },
  {
    "text": "And I think began from Google DeepMind an ability to generate",
    "start": "1762166",
    "end": "1769066"
  },
  {
    "text": "incredibly high-resolution images. And it's the same GAN technique,",
    "start": "1769140",
    "end": "1776266"
  },
  {
    "text": "so in terms of breakthroughs and innovations but scaled. So the increase the model capacity and increase the the batch size",
    "start": "1776334",
    "end": "1784799"
  },
  {
    "text": "the number of images that are fed that are fed to the network. It produces incredible images",
    "start": "1784866",
    "end": "1792166"
  },
  {
    "text": "I encourage you to go online and and look at them It's hard to believe that they're generated.",
    "start": "1792467",
    "end": "1797767"
  },
  {
    "text": "So that was 2018 for GANs was a year of scaling and parameter tuning",
    "start": "1799000",
    "end": "1808867"
  },
  {
    "text": "as opposed to breakthrough new ideas. Video-to-Video Synthesis. This work is from NVIDIA",
    "start": "1809834",
    "end": "1818100"
  },
  {
    "start": "1814000",
    "end": "1932000"
  },
  {
    "text": "is looking at the problem so there's been a lot of work on general going from image to image.",
    "start": "1818300",
    "end": "1825166"
  },
  {
    "text": "So from a particular image generating another image. So whether it's colorizing an image or just to traditionally define GANs.",
    "start": "1825433",
    "end": "1834934"
  },
  {
    "text": "The idea with video to video synthesis that a few people have been working on but NVIDIA took a good step forward is to make the video",
    "start": "1836000",
    "end": "1848934"
  },
  {
    "text": "to make the temporal consistency the temporal dynamics part of the optimization process. So make it look not jumpy.",
    "start": "1849000",
    "end": "1856000"
  },
  {
    "text": "So if you look here at the comparison the for this particular.",
    "start": "1856033",
    "end": "1860367"
  },
  {
    "text": "So the input is the labels on the top left and the output of the of the NVIDIA approach is on the bottom right.",
    "start": "1861400",
    "end": "1872033"
  },
  {
    "text": "See it's temper it's very temporarily consistent. If you look at the image to image mapping that's",
    "start": "1872118",
    "end": "1878834"
  },
  {
    "text": "that state the pix2pixHD. It's very jumpy, it's not temporally consistent at all.",
    "start": "1878899",
    "end": "1885266"
  },
  {
    "text": "And there's some naive approaches for trying to maintain temporal consistency.",
    "start": "1886166",
    "end": "1891734"
  },
  {
    "text": "That's in the bottom left. So you can apply this to all kinds of tasks all kinds of video to video mapping.",
    "start": "1891867",
    "end": "1898166"
  },
  {
    "text": "Here is mapping it to face edges. Edge detection on faces mapping it to faces.",
    "start": "1898257",
    "end": "1905065"
  },
  {
    "text": "Generating faces from just edges. You can look at body pose to actual images.",
    "start": "1905132",
    "end": "1915000"
  },
  {
    "text": "As an input to the network you can take the pose of the person and generate the  video of the person.",
    "start": "1915066",
    "end": "1923700"
  },
  {
    "text": "Okay semantic segmentation. The problem of perception, so if began with AlexNet and ImageNet",
    "start": "1931367",
    "end": "1939200"
  },
  {
    "start": "1932000",
    "end": "2163000"
  },
  {
    "text": "has been further and further developments where the input, the problem is of basic image classification,",
    "start": "1939273",
    "end": "1944600"
  },
  {
    "text": "where the input is an image and the output is a classification was going on in that image and the fundamental architecture can be reused",
    "start": "1944691",
    "end": "1951000"
  },
  {
    "text": "for more complex tasks like detection like segmentation and so on, interpreting what's going on in the image.",
    "start": "1951060",
    "end": "1957200"
  },
  {
    "text": "So these large networks from VGGNet, GoogLeNet, ResNet, SENet, DenseNet",
    "start": "1957233",
    "end": "1965333"
  },
  {
    "text": "all these networks are forming rich representations that can then be used for all kinds of tasks whether that task is object detection.",
    "start": "1965367",
    "end": "1972433"
  },
  {
    "text": "This here shown is the region based methods where the neural network is tasked the",
    "start": "1972498",
    "end": "1978700"
  },
  {
    "text": "convolutional layers make region proposals. So much of candidates to be considered.",
    "start": "1978787",
    "end": "1985433"
  },
  {
    "text": "And then there's a step that's determining what's in those different regions and forming boundary boxes around them in a for-loop way.",
    "start": "1985467",
    "end": "1993100"
  },
  {
    "text": "And then there is the one-shot method single-shot method where in a single pass",
    "start": "1993133",
    "end": "1998500"
  },
  {
    "text": "all of the boundary boxes in their classes generated. And there has been a tremendous amount of work",
    "start": "1998533",
    "end": "2005799"
  },
  {
    "text": "in the space of object detection. Some are single shot method, some are region based methods.",
    "start": "2005876",
    "end": "2014500"
  },
  {
    "text": "And there's been a lot of exciting work but not more not I would say breakthrough ideas.",
    "start": "2014533",
    "end": "2022399"
  },
  {
    "text": "And then we take it to the highest level of perception which is semantic segmentation.",
    "start": "2022433",
    "end": "2027600"
  },
  {
    "text": "There's also been a lot of work there the state of the art performance",
    "start": "2028066",
    "end": "2033233"
  },
  {
    "text": "is at least for the open source systems is DeepLabv3+ on the PASCAL VOC challenge.",
    "start": "2033367",
    "end": "2042400"
  },
  {
    "text": "So semantic segmentation and catch it all up started 2014 with fully convolution neural networks.",
    "start": "2043000",
    "end": "2049266"
  },
  {
    "text": "Chopping off the fully connected layers and then outputting the heatmap very grainy very low resolution.",
    "start": "2049433",
    "end": "2061166"
  },
  {
    "text": "Then improving that was SegNet performing maxpooling with a breakthrough idea that's reused in a lot of cases is",
    "start": "2061200",
    "end": "2070200"
  },
  {
    "text": "Dilated Convolution, Atrous convolutions having some spacing which increases the",
    "start": "2070275",
    "end": "2076899"
  },
  {
    "text": "field of view of the convolutional filter. The key idea behind DeepLabv3 that",
    "start": "2076973",
    "end": "2085232"
  },
  {
    "text": "is the state of the art is the multi-scale processing.",
    "start": "2085325",
    "end": "2090000"
  },
  {
    "text": "Without increasing the parameters the multi scale is achieved by the \"atrous rate\"",
    "start": "2091133",
    "end": "2098000"
  },
  {
    "text": "So taking those atrous convolutions and increasing the spacing. And you can think of the increasing that spacing",
    "start": "2098133",
    "end": "2105133"
  },
  {
    "text": "by enlarging the model's field of view. And so you can consider all these different scales of processing and looking at the",
    "start": "2105201",
    "end": "2114333"
  },
  {
    "text": "at the layers of features. So allowing you to be able to grasp the greater context",
    "start": "2114567",
    "end": "2124233"
  },
  {
    "text": "as part of the upsampling deconvolutional step. And that's what's produced in the state of art performances",
    "start": "2124314",
    "end": "2129900"
  },
  {
    "text": "and that's where we have the tutorial on github showing this DeepLab",
    "start": "2129934",
    "end": "2140166"
  },
  {
    "text": "architecture trained on CityScapes. CityScapes is a driving segmentation data set",
    "start": "2141000",
    "end": "2149066"
  },
  {
    "text": "that is one of the most commonly used for the task of driving scene segmentation.",
    "start": "2150133",
    "end": "2160166"
  },
  {
    "text": "Okay on the deep reinforcement learning for.",
    "start": "2161000",
    "end": "2165000"
  },
  {
    "start": "2163000",
    "end": "2614000"
  },
  {
    "text": "So this is touching a bit a bit on the 2017. But i think the excitement really settled in 2018",
    "start": "2168734",
    "end": "2177734"
  },
  {
    "text": "as the work from Google and from OpenAI, DeepMind. So it started in DQN paper from Google DeepMind where they beat a bunch of",
    "start": "2177801",
    "end": "2187333"
  },
  {
    "text": "a bunch of Atari games achieving superhuman performance with deep reinforcement learning methods.",
    "start": "2189266",
    "end": "2196367"
  },
  {
    "text": "That are taking in just the raw pixels of the game, so the same kind of architecture is able to learn how to beat these,",
    "start": "2196400",
    "end": "2202533"
  },
  {
    "text": "how to beat these games. Super exciting idea that kind of has echoes",
    "start": "2202567",
    "end": "2207734"
  },
  {
    "text": "of what general intelligence is. Taking in the raw raw information and being able to understand",
    "start": "2207767",
    "end": "2214600"
  },
  {
    "text": "the game, the sort of physics of the game sufficient to be able to beat it. Then in 2016 AlphaGo with some supervision and some playing against itself,",
    "start": "2214634",
    "end": "2225900"
  },
  {
    "text": "self play, some supervised learning on expert world champ players",
    "start": "2226000",
    "end": "2231500"
  },
  {
    "text": "and some self play where it plays against itself was able to beat the top of the world champion at Go.",
    "start": "2231533",
    "end": "2239100"
  },
  {
    "text": "And then 2017 AlphaGo Zero a specialized version of Alpha Zero",
    "start": "2239166",
    "end": "2245367"
  },
  {
    "text": "was able to beat the AlphaGo with just a few days of training.",
    "start": "2245459",
    "end": "2252200"
  },
  {
    "text": "and zero supervision from expert games. So through the process of self play again this is kind of",
    "start": "2252233",
    "end": "2259233"
  },
  {
    "text": "getting the human out of the picture more and more and more",
    "start": "2259266",
    "end": "2264667"
  },
  {
    "text": "which is why Alpha Zero is probably or this AlphaGo Zero was the demonstration of",
    "start": "2264767",
    "end": "2272734"
  },
  {
    "text": "the cleanest demonstration of all the nice progress in deep reinforcement learning. I think if we look at the history of AI",
    "start": "2272934",
    "end": "2279834"
  },
  {
    "text": "when you're sitting on a porch hundred years from now sort of reminiscing back Alpha Zero will be a thing that people will",
    "start": "2279898",
    "end": "2290233"
  },
  {
    "text": "remember as an interesting moment in time, as a key moment in time.",
    "start": "2290266",
    "end": "2295667"
  },
  {
    "text": "And Alpha Zero was applied in 2017 to beat.",
    "start": "2295767",
    "end": "2302433"
  },
  {
    "text": "Alpha Zero paper was in 2017 and it was this year played StockFish in chess which is the best engine, chess playing engines",
    "start": "2302524",
    "end": "2312700"
  },
  {
    "text": "is able to beat it with just four hours of training of course the four hours this caveat.",
    "start": "2312734",
    "end": "2319166"
  },
  {
    "text": "Because four hours for Google DeepMind is highly distributed training. So it's not four hours for an undergraduate student sitting in their dorm room.",
    "start": "2319196",
    "end": "2328033"
  },
  {
    "text": "But meaning it was able to self play to very quickly learn to beat the state of the art chess engine.",
    "start": "2329734",
    "end": "2336867"
  },
  {
    "text": "And learned to beat the state of the art Shogi engine Elmo.",
    "start": "2336900",
    "end": "2341233"
  },
  {
    "text": "And the interesting thing here is you know with perfect information games like chess",
    "start": "2342300",
    "end": "2348500"
  },
  {
    "text": "you have a tree and you have all the decisions you could possibly make and so the farther along you look at along that tree presumably the better you do.",
    "start": "2348734",
    "end": "2357367"
  },
  {
    "text": "That's how DeepBlue beat Kasparov in the 90s is you just look as far as possible in a down the tree",
    "start": "2357400",
    "end": "2365867"
  },
  {
    "text": "to determine which is the action is the most optimal. If you look at the way human grandmasters think",
    "start": "2366033",
    "end": "2373166"
  },
  {
    "text": "it certainly doesn't feel like they're like looking down a tree. There's something like creative intuition there's something like",
    "start": "2373333",
    "end": "2379533"
  },
  {
    "text": "you can see the patterns in the board, you can do a few calculations but really it's an order of hundreds.",
    "start": "2379582",
    "end": "2385333"
  },
  {
    "text": "It's not on the order of millions or billions which is kind of the",
    "start": "2385367",
    "end": "2390333"
  },
  {
    "text": "the StockFish the state of the art chess engine approach.",
    "start": "2390409",
    "end": "2396033"
  },
  {
    "text": "And Alpha Zero is moving closer and closer closer towards the human grandmaster concerning very few future moves.",
    "start": "2396233",
    "end": "2402900"
  },
  {
    "text": "It's able through the neural network estimator that's estimating the quality of the move and the quality of the different, the current quality of the board and",
    "start": "2403000",
    "end": "2410467"
  },
  {
    "text": "and the quality of the moves that follow. It's able to do much much less look ahead.",
    "start": "2410539",
    "end": "2416834"
  },
  {
    "text": "So the neural network learns the fundamental information just like when a grandmaster looks",
    "start": "2416867",
    "end": "2421734"
  },
  {
    "text": "at a board they can tell how good that is. So that's again interesting, it's a step towards",
    "start": "2421934",
    "end": "2428500"
  },
  {
    "text": "at least echoes of what human intelligence is in this very structured formal constrained world of chess",
    "start": "2430033",
    "end": "2436567"
  },
  {
    "text": "and go and shogi. And then there's the other side of the world that's messy.",
    "start": "2436734",
    "end": "2442133"
  },
  {
    "text": "It's still games. It's still constrained in that way but OpenAI has taken on the challenge of playing games",
    "start": "2442333",
    "end": "2450033"
  },
  {
    "text": "that are much messier to have this resemblance",
    "start": "2450112",
    "end": "2456033"
  },
  {
    "text": "of the real world and the fact that you have to do teamwork, you have to look at long time horizons",
    "start": "2456066",
    "end": "2461767"
  },
  {
    "text": "with huge amounts of imperfect information, hidden information, uncertainty.",
    "start": "2461830",
    "end": "2467466"
  },
  {
    "text": "So within that world they've taken on the challenge of a popular game Dota 2.",
    "start": "2467500",
    "end": "2472734"
  },
  {
    "text": "On the human side of that",
    "start": "2473333",
    "end": "2479567"
  },
  {
    "text": "there's the competition the international hosted every year where you know in 2018 the winning team gets 11 million dollars. So it's a very popular very active competition has been",
    "start": "2479567",
    "end": "2485867"
  },
  {
    "text": "going on for a few years. They've been improving and it achieved a lot of interesting milestones in 2017.",
    "start": "2485957",
    "end": "2495667"
  },
  {
    "text": "Their 1v1 bot beat the top professional Dota 2 player. The way you achieve great things is as you try.",
    "start": "2495700",
    "end": "2504500"
  },
  {
    "text": "And in 2018 they tried to go 5v5. The OpenAI team lost two games",
    "start": "2504567",
    "end": "2511400"
  },
  {
    "text": "a go against the top Dota 2 players at the 2018 international.",
    "start": "2511476",
    "end": "2517133"
  },
  {
    "text": "And of course their ranking here the MMR ranking in Dota 2",
    "start": "2517166",
    "end": "2522500"
  },
  {
    "text": "has been increasing over and over but there's a lot of challenges here that make it extremely difficult.",
    "start": "2522667",
    "end": "2528000"
  },
  {
    "text": "To beat the human players and this is, you know, in every story rocky",
    "start": "2528030",
    "end": "2535500"
  },
  {
    "text": "or whatever you think about losing is essential element of a story that leads to then",
    "start": "2535700",
    "end": "2541099"
  },
  {
    "text": "a movie in a book and the greatness. So you better believe that they're coming back next year.",
    "start": "2541133",
    "end": "2546533"
  },
  {
    "text": "And there's going to be a lot of exciting developments there. It also, Dota 2 and this particular video game makes it currently",
    "start": "2546600",
    "end": "2554667"
  },
  {
    "text": "this really two games that have the public eye in terms of AI taking on his benchmarks.",
    "start": "2554700",
    "end": "2562166"
  },
  {
    "text": "So we saw go incredible accomplishment What's next? So last year the associate were the best paper in Europe's.",
    "start": "2562367",
    "end": "2572533"
  },
  {
    "text": "There was the heads up Texas No Limit Hold'em AI was able to beat the top level players was completely current",
    "start": "2572600",
    "end": "2582567"
  },
  {
    "text": "well not completely but currently out of reach is the general not heads up one versus one but the general team",
    "start": "2582639",
    "end": "2589233"
  },
  {
    "text": "Texas No Limit Hold'em here you go. And on the gaming side this dream of Dota 2 now",
    "start": "2589266",
    "end": "2597231"
  },
  {
    "text": "that's the benchmark that everybody's targeting. And it's actually incredibly difficult one and some people think would be a long time before we can win.",
    "start": "2597291",
    "end": "2604333"
  },
  {
    "text": "And on the more practical side of things the",
    "start": "2604400",
    "end": "2610882"
  },
  {
    "text": "2018, start in 2017 has been a year of",
    "start": "2610943",
    "end": "2616033"
  },
  {
    "start": "2614000",
    "end": "2680000"
  },
  {
    "text": "of the frameworks growing up of maturing",
    "start": "2616066",
    "end": "2622799"
  },
  {
    "text": "and creating ecosystems around them. With TensorFlow with the history there dating back a few years",
    "start": "2622865",
    "end": "2630633"
  },
  {
    "text": "has really with TensorFlow 1.0 as come",
    "start": "2630724",
    "end": "2635734"
  },
  {
    "text": "to be sort of a mature framework PyTorch 1.0 came out 2018 is matured as well.",
    "start": "2635867",
    "end": "2643166"
  },
  {
    "text": "And now the really exciting developments in the TensorFlow with the eager execution and beyond",
    "start": "2643433",
    "end": "2649099"
  },
  {
    "text": "that's coming out TensorFlow 2.0 in in 2019. So really those two players have made incredible leaps in standardizing deep learning.",
    "start": "2649300",
    "end": "2664333"
  },
  {
    "text": "In the fact that a lot of the ideas I talked about today and Monday and we'll keep talking about",
    "start": "2664500",
    "end": "2670033"
  },
  {
    "text": "are all have a github repository with implementations in TensorFlow and PyTorch.",
    "start": "2670066",
    "end": "2676333"
  },
  {
    "text": "Making extremely accessible and that's really exciting. it's probably best to quote Geoff Hinton the \"Godfather\" of deep learning,",
    "start": "2676433",
    "end": "2686300"
  },
  {
    "start": "2680000",
    "end": "2785000"
  },
  {
    "text": "one of the key people behind backpropagation said recently on backpropagation is \"My view is throw it all away and start again\"",
    "start": "2686333",
    "end": "2695467"
  },
  {
    "text": "His believes backpropagation is totally broken and an idea that has ancient",
    "start": "2695497",
    "end": "2700900"
  },
  {
    "text": "and it needs to be completely revolutionized and the practical protocol for doing that is he said the future",
    "start": "2700934",
    "end": "2708133"
  },
  {
    "text": "depends on some graduate student who's deeply suspicious of everything I've said that's probably a good way to end",
    "start": "2708166",
    "end": "2716600"
  },
  {
    "text": "the discussion about what the state of the art in deep learning holds because everything we're doing is fundamentally based on",
    "start": "2716667",
    "end": "2725533"
  },
  {
    "text": "ideas from the 60s and the 80s and really in terms of",
    "start": "2725567",
    "end": "2730834"
  },
  {
    "text": "new ideas, there has not been many new ideas especially the state of the art results that I've mentioned",
    "start": "2730867",
    "end": "2737767"
  },
  {
    "text": "are all based on fundamentally, on stochastic gradient descent and backpropagation.",
    "start": "2737858",
    "end": "2745533"
  },
  {
    "text": "It's ripe for totally new ideas. So it's up to us to define",
    "start": "2745567",
    "end": "2751166"
  },
  {
    "text": "the real breakthroughs and the real state of the art 2019 and beyond. So that I'd like to thank you and",
    "start": "2751251",
    "end": "2760567"
  },
  {
    "text": "the stuff is on the website deeplearning.mit.edu.",
    "start": "2760600",
    "end": "2764867"
  }
]