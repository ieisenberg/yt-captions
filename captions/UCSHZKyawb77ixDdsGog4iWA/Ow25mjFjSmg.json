[
  {
    "start": "0",
    "end": "46000"
  },
  {
    "text": "- Today, we're happy and honored to have Vladimir Vapnik with us, co-inventor of supported\nvector machines, support vector",
    "start": "130",
    "end": "7200"
  },
  {
    "text": "clustering, VC theory\nof statistical learning, and author of \"Statistical\nLearning Theory\".",
    "start": "7200",
    "end": "12540"
  },
  {
    "text": "He's one of the greatest\nand most impactful statisticians and computer\nscientists of our time.",
    "start": "12540",
    "end": "19130"
  },
  {
    "text": "Plus, he grew up in the Soviet Union, eventually heading the\nComputer Science Department,",
    "start": "19130",
    "end": "24860"
  },
  {
    "text": "Institute of Controlled\nSciences in Moscow. So he will give today's lecture in Russian",
    "start": "24860",
    "end": "30039"
  },
  {
    "text": "and I will translate. Just kidding, right. (laughing)\n(audience laughs)",
    "start": "30040",
    "end": "37150"
  },
  {
    "text": "It's an honor and a pleasure\nto have Vladimir with us today, so please give him a warm welcome.",
    "start": "37150",
    "end": "42972"
  },
  {
    "text": "(audience applauds) - Thank you.",
    "start": "42972",
    "end": "47803"
  },
  {
    "start": "46000",
    "end": "227000"
  },
  {
    "text": "About 50 years ago,\nProfessor Chervonenkis and me",
    "start": "50067",
    "end": "54153"
  },
  {
    "text": "started statistical learning theory. The problem was to answer the question",
    "start": "56354",
    "end": "62070"
  },
  {
    "text": "when if we will do well with training data if you will have small amount\nof our own training data,",
    "start": "62070",
    "end": "70430"
  },
  {
    "text": "you will do well also on the test data. You will minimize expectation of error.",
    "start": "70430",
    "end": "78833"
  },
  {
    "text": "So this solves this problem.",
    "start": "80050",
    "end": "82517"
  },
  {
    "text": "There are this theory in most in all books they might be written\nin different languages,",
    "start": "85084",
    "end": "91930"
  },
  {
    "text": "but mostly they follow this line. The line is that just",
    "start": "91930",
    "end": "96630"
  },
  {
    "text": "law of large numbers is not enough, then you need uniform law of large number, you need convergence, and so on,",
    "start": "98630",
    "end": "104909"
  },
  {
    "text": "but because we started this\ndiscussion about empirical error",
    "start": "105950",
    "end": "110950"
  },
  {
    "text": "how good you do on the training data, and bounds shows the better\nyou do in the training data,",
    "start": "112040",
    "end": "119520"
  },
  {
    "text": "you will be better on test data. People decided that this is\nonly way to have a training data",
    "start": "119520",
    "end": "126710"
  },
  {
    "text": "and do something with this training data, to minimize number of\nerror and all algorithm",
    "start": "128160",
    "end": "135640"
  },
  {
    "text": "was constructed based on this principle. About five years ago I\nfound that there exists",
    "start": "135640",
    "end": "143260"
  },
  {
    "text": "another principle, even more\ninteresting than this one, because in this principle\nit is brute force principle,",
    "start": "143260",
    "end": "152390"
  },
  {
    "text": "give me more data, you\ngive better answer with. So the second principle\nis intelligent principle",
    "start": "152390",
    "end": "159383"
  },
  {
    "text": "and I will talk today about this. So I will start with\nstatistical learning theory",
    "start": "159383",
    "end": "167099"
  },
  {
    "text": "and then I will introduce this new theory, but because there are only\ntwo ways for generalization,",
    "start": "167100",
    "end": "175040"
  },
  {
    "text": "one is data, another I will show what I call the complete\nstatistical learning theory",
    "start": "175040",
    "end": "181910"
  },
  {
    "text": "because there are more, another way to do something.",
    "start": "181910",
    "end": "188403"
  },
  {
    "text": "There is no short way for generalization. You should use both of them,\nso that is complete theory.",
    "start": "188404",
    "end": "196650"
  },
  {
    "text": "But it is not so bad because you will see that learning theory",
    "start": "197610",
    "end": "204400"
  },
  {
    "text": "move in different direction. In direction of intelligence\nto understanding",
    "start": "205660",
    "end": "211580"
  },
  {
    "text": "what is intellence. It is not the same but Turing explained.",
    "start": "211580",
    "end": "216770"
  },
  {
    "text": "Turing told that you should\nimitate intelligent person but now, question what is intelligence,",
    "start": "216770",
    "end": "224100"
  },
  {
    "text": "and we will discuss this. So let me start.",
    "start": "224100",
    "end": "227603"
  },
  {
    "start": "227000",
    "end": "664000"
  },
  {
    "text": "The first part is, this is\ntheory of generalization,",
    "start": "229460",
    "end": "233567"
  },
  {
    "text": "and that is the question, when in set of given set of function,",
    "start": "235410",
    "end": "242250"
  },
  {
    "text": "you can minimize functional. This is pretty general functional.",
    "start": "242250",
    "end": "247450"
  },
  {
    "text": "Instead of y, f of x,\nand L is loss function,",
    "start": "247450",
    "end": "252450"
  },
  {
    "text": "I can consider difference\nbetween y and function.",
    "start": "253690",
    "end": "258690"
  },
  {
    "text": "It is what they're doing in regression and pattern recognition\nand all this stuff,",
    "start": "259080",
    "end": "265660"
  },
  {
    "text": "but this is more general setting. But this is more important that when we consider\nminimization of functional,",
    "start": "265660",
    "end": "273953"
  },
  {
    "text": "we should say in which set of fucntions. In the given set of function,",
    "start": "274015",
    "end": "279270"
  },
  {
    "text": "you have to minimize functional. If probability measure is unknown,",
    "start": "279270",
    "end": "284410"
  },
  {
    "text": "but we are given iid data pairs.",
    "start": "284410",
    "end": "287753"
  },
  {
    "text": "That exact setting of\nboth pattern recognition and regression estimation problem",
    "start": "290255",
    "end": "297651"
  },
  {
    "text": "of pattern recognition set of function is indicated functions",
    "start": "297651",
    "end": "305660"
  },
  {
    "text": "for regression function\nbecause continuous function,",
    "start": "306310",
    "end": "311310"
  },
  {
    "text": "but the statement is the same, but the answer is very simple.",
    "start": "311715",
    "end": "317930"
  },
  {
    "text": "We can minimize this functional using data",
    "start": "317930",
    "end": "322026"
  },
  {
    "text": "if and only if this dimension h of set of function is finite.",
    "start": "323680",
    "end": "330300"
  },
  {
    "text": "Everything depend on which set of function you have to minimize this functional,",
    "start": "330300",
    "end": "335800"
  },
  {
    "text": "and you can not avoid that. So we call it capacity,\nbut maybe better called,",
    "start": "335800",
    "end": "342750"
  },
  {
    "text": "diversity of the set of function, but measure of this capacity,\ndiversity, is VC-dimension.",
    "start": "342750",
    "end": "350310"
  },
  {
    "text": "And what is VC dimension,\nI will give you definition. First for your set of indicator functions,",
    "start": "351460",
    "end": "358998"
  },
  {
    "text": "t is step function you\nconsider continuous function f,",
    "start": "358998",
    "end": "363998"
  },
  {
    "text": "and in front of it, you're\nlooking for indicator. If function is positive you say one,",
    "start": "364100",
    "end": "373550"
  },
  {
    "text": "if not positive you say\nzero, that is indicator. The VC-dimension of set\nindicator function equals h",
    "start": "374860",
    "end": "383520"
  },
  {
    "text": "if h is the maximal number of vectors",
    "start": "384919",
    "end": "389919"
  },
  {
    "text": "that can be shattered,\nseparated in all possible two h subsets",
    "start": "391246",
    "end": "397400"
  },
  {
    "text": "using indicator functions from this set. So you have set, you should find h vectors",
    "start": "398420",
    "end": "408240"
  },
  {
    "text": "which you can shatter in all possible way, in the l possible way. But you cannot shatter\nin h plus one vector.",
    "start": "410630",
    "end": "419130"
  },
  {
    "text": "Then VC-dimension of this\nset of function will be h.",
    "start": "419130",
    "end": "424130"
  },
  {
    "text": "This is purely a rhetorical definition. And if you can shatter for any l,",
    "start": "424450",
    "end": "431123"
  },
  {
    "text": "then this says the VC\ndimension is infinite. And then I will give you two\ntheorems which we will use,",
    "start": "432120",
    "end": "439830"
  },
  {
    "text": "and then the main theorem,\nprobably of this theory.",
    "start": "439830",
    "end": "443672"
  },
  {
    "text": "If set of function has VC-dimension h, then this probability one\nminus eta for all functions",
    "start": "445040",
    "end": "452830"
  },
  {
    "text": "in the set the bound holds true. And because this bound\nholds true for all function,",
    "start": "453975",
    "end": "461330"
  },
  {
    "text": "and you would like to\nhave minimal estimate,",
    "start": "461330",
    "end": "466330"
  },
  {
    "text": "minimal right hand side, you will choose function\nwhich minimize empirical loss.",
    "start": "467100",
    "end": "473923"
  },
  {
    "text": "And the second theorem, if you\nhave set of linear functions,",
    "start": "476420",
    "end": "481420"
  },
  {
    "text": "indicator from the set of linear function, it so happen that your\nvector x inside of circle",
    "start": "481490",
    "end": "488900"
  },
  {
    "text": "for radius one, and w inside\nof C, then we see the dimension",
    "start": "488900",
    "end": "493900"
  },
  {
    "text": "is bounded by maximum of two radius C",
    "start": "494520",
    "end": "498970"
  },
  {
    "text": "and n, n is the dimensionality\nof the object plus one. That means exactly that\nVC-dimension can be smaller",
    "start": "500290",
    "end": "510040"
  },
  {
    "text": "than dimensionality of the space. And you can control some\npart of VC-dimension,",
    "start": "510780",
    "end": "516380"
  },
  {
    "text": "and I will show you\nafterwards how to do it, it is very important theorem.",
    "start": "516380",
    "end": "520412"
  },
  {
    "text": "But, what is the general way\nsuggest the VC-dimension for",
    "start": "523210",
    "end": "527540"
  },
  {
    "text": "searching for functions. You have set of function,\nmaybe set of function have infinite VC-dimension.",
    "start": "528810",
    "end": "535970"
  },
  {
    "text": "Then you make a structure\nof this set of function. You choose subset of function,\nthe small subset of function,",
    "start": "535970",
    "end": "544390"
  },
  {
    "text": "with VC-dimension h, then\nanother subset of function which includes a small\none, VC-dimension h two,",
    "start": "544390",
    "end": "551930"
  },
  {
    "text": "so this last VC, we have\nlast VC type of function. It's loss to VC dimension.",
    "start": "551930",
    "end": "558140"
  },
  {
    "text": "And then, when you\nminimize exist functional, you're doing two things. You choose appropriate subset,",
    "start": "558140",
    "end": "566023"
  },
  {
    "text": "and then in this subset,\nyou pick up function which minimize empirical loss.",
    "start": "567130",
    "end": "575412"
  },
  {
    "text": "But, you can see, that epsilon in this depend",
    "start": "577270",
    "end": "583689"
  },
  {
    "text": "on VC-dimension of subset you choose, at VC-dimension over L.",
    "start": "583690",
    "end": "590240"
  },
  {
    "text": "And also, it depend on empirical\nloss which you achieve. So you can do as soon\nas you make a structure,",
    "start": "590240",
    "end": "599800"
  },
  {
    "text": "you can do whatever you want, even if you have infinite VC-dimension",
    "start": "599800",
    "end": "605890"
  },
  {
    "text": "in initial set of function.",
    "start": "605890",
    "end": "607523"
  },
  {
    "text": "But, now, this is more\nor less, all what contain the main result of VC theory.",
    "start": "611210",
    "end": "618357"
  },
  {
    "text": "But VC theory does not answer\nfour very important questions.",
    "start": "619372",
    "end": "624372"
  },
  {
    "text": "How to choose loss function L y f? I told that any function",
    "start": "626358",
    "end": "632160"
  },
  {
    "text": "how to select admissible\nset of function f of x? I told you that given set of function,",
    "start": "633906",
    "end": "641899"
  },
  {
    "text": "but when there's a stupid set of function, how to construct good set of function?",
    "start": "641900",
    "end": "648813"
  },
  {
    "text": "How to construct structure on\nadmissible set of function? And then, how to minimize function",
    "start": "650375",
    "end": "656843"
  },
  {
    "text": "and how to construct the structure. In this talk I will try to\nanswer all those questions.",
    "start": "656843",
    "end": "663633"
  },
  {
    "start": "664000",
    "end": "1633000"
  },
  {
    "text": "Target functional for minimization. And this is important\nslide, God plays dice.",
    "start": "664820",
    "end": "673423"
  },
  {
    "text": "What is setting of pattern\nrecognition problem? I will consider in this talk,\njust pattern recognition",
    "start": "675300",
    "end": "683899"
  },
  {
    "text": "problem for two class specification, but generalization is straightforward.",
    "start": "683900",
    "end": "689373"
  },
  {
    "text": "So, what is setting of\npattern-recognition problem?",
    "start": "691940",
    "end": "696933"
  },
  {
    "text": "Given generator, given\nnature we generate randomly, independently, citation x\nwhich come on the object,",
    "start": "697860",
    "end": "707475"
  },
  {
    "text": "and this object is\nconditional probability of y. Given x, say that y equals one, given x,",
    "start": "707917",
    "end": "717270"
  },
  {
    "text": "and y equals zero given x. This object knows this\nconditional probability,",
    "start": "718130",
    "end": "724683"
  },
  {
    "text": "and plays dice. So he has the function of\nconditional probability,",
    "start": "725530",
    "end": "731579"
  },
  {
    "text": "he has x on the input,\nhe plays dice, and say y.",
    "start": "731580",
    "end": "736063"
  },
  {
    "text": "That is the most general\nsetting of logic problem, deterministical particular\ncase of this set.",
    "start": "737593",
    "end": "743633"
  },
  {
    "text": "And what does learning machine, learning machine has a set of functions, and it can choose any\nfunction from this set.",
    "start": "744650",
    "end": "753600"
  },
  {
    "text": "The problem with observing l observations, x one y one, x l y l,",
    "start": "753600",
    "end": "762250"
  },
  {
    "text": "to pick up the function",
    "start": "762250",
    "end": "766640"
  },
  {
    "text": "for classification. That means, given observations",
    "start": "768230",
    "end": "772436"
  },
  {
    "text": "generated by a kind of\nconditional probability P x, y",
    "start": "774073",
    "end": "779073"
  },
  {
    "text": "equal P y given x on P\nof x, that's our theorem,",
    "start": "779760",
    "end": "784760"
  },
  {
    "text": "finds the rule that minimize function l. So in my case when I have y zero or one,",
    "start": "785850",
    "end": "792579"
  },
  {
    "text": "or in theta, also zero, one,\nit is indicator function, so last function l, just\ncollect how many errors I do,",
    "start": "792580",
    "end": "802350"
  },
  {
    "text": "but I have probability measure, it collect expectation of error.",
    "start": "802350",
    "end": "808140"
  },
  {
    "text": "I would like to find\nfunction which guarantee the smallest expectation of l.",
    "start": "808140",
    "end": "814133"
  },
  {
    "text": "But this is not very good. Why it not very good? Because my function l, this model",
    "start": "815950",
    "end": "823300"
  },
  {
    "text": "is just, it is everywhere zero except for some points where\nit is one or minus one,",
    "start": "823300",
    "end": "831993"
  },
  {
    "text": "and if I could model it is one. So, it is zero everywhere in defined,",
    "start": "834258",
    "end": "840663"
  },
  {
    "text": "and one in some point. So I cannot use gradient in this case.",
    "start": "841890",
    "end": "849360"
  },
  {
    "text": "So I should to do something smarter. And what people doing is they replace",
    "start": "849360",
    "end": "855790"
  },
  {
    "text": "model and indicator function with just y minus f of x.",
    "start": "855790",
    "end": "863570"
  },
  {
    "text": "This create error. Instead of whatever I formulated before.",
    "start": "863570",
    "end": "870610"
  },
  {
    "text": "It's not so bad choice\nbecause it so happen that minimum of this function l",
    "start": "870610",
    "end": "877380"
  },
  {
    "text": "gives conditional probability function, probability of y equal one given x,",
    "start": "877380",
    "end": "884763"
  },
  {
    "text": "and then when we can find this probability of y equal one given x,\nwe easily can construct",
    "start": "885600",
    "end": "894330"
  },
  {
    "text": "our decision or rule, we\njust consider function",
    "start": "894470",
    "end": "899410"
  },
  {
    "text": "if our conditional probability exceed .5, say first class, if it's\nless than .5, second class,",
    "start": "900890",
    "end": "909750"
  },
  {
    "text": "and this is optimal solution. But something wrong with this replacement.",
    "start": "909750",
    "end": "917603"
  },
  {
    "text": "Let us rewrite the first line. I will subtract from bracket\ninside on the first term.",
    "start": "918480",
    "end": "927550"
  },
  {
    "text": "Regression is an odd regression, so I have two brackets instead of one,",
    "start": "928600",
    "end": "935517"
  },
  {
    "text": "and then I make a square. So the last integral show\nme the first integral",
    "start": "936560",
    "end": "942889"
  },
  {
    "text": "does not depend on function,\nwhich I looking for,",
    "start": "942890",
    "end": "947890"
  },
  {
    "text": "and I have to minimize\nmy function l over f over set of function,",
    "start": "948100",
    "end": "954130"
  },
  {
    "text": "just sum of two last terms.",
    "start": "954130",
    "end": "958313"
  },
  {
    "text": "Have a good it is just\nnormal binome for two terms.",
    "start": "960575",
    "end": "964863"
  },
  {
    "text": "Square of one, square of second, and two terms is multiplications.",
    "start": "967480",
    "end": "974936"
  },
  {
    "text": "But our goal is to\nminimize first integral,",
    "start": "976010",
    "end": "981010"
  },
  {
    "text": "to find function which is close to conditional probability of function,",
    "start": "981080",
    "end": "986370"
  },
  {
    "text": "not sum of two integrals. We can show the second\nintegral eventually will go,",
    "start": "986370",
    "end": "991474"
  },
  {
    "text": "it goes to the, it'll go to zero, but it will slow down rate of convergence.",
    "start": "991474",
    "end": "996803"
  },
  {
    "text": "To have a rate of convergence big, we need to find way, how\nto minimize first integral,",
    "start": "998583",
    "end": "1007310"
  },
  {
    "text": "not sum of these two. And that means that not this square loss,",
    "start": "1007310",
    "end": "1013930"
  },
  {
    "text": "but something else. What they can do? There exists,",
    "start": "1013930",
    "end": "1019963"
  },
  {
    "text": "first of all, when y, it is zero or one,",
    "start": "1024430",
    "end": "1029430"
  },
  {
    "text": "probability of y equal one given x",
    "start": "1029730",
    "end": "1034730"
  },
  {
    "text": "is some real valued function\nbetween zero and one,",
    "start": "1035400",
    "end": "1039307"
  },
  {
    "text": "because from Bayesian formula,",
    "start": "1040890",
    "end": "1045890"
  },
  {
    "text": "we know that conditional probability of y equals one given x, and then p of x,",
    "start": "1046230",
    "end": "1054809"
  },
  {
    "text": "it is joint density with\ny given now comma x. That is just always true.",
    "start": "1054810",
    "end": "1064480"
  },
  {
    "text": "Now, if I will multiply on some function, G of x minus x, star,",
    "start": "1065760",
    "end": "1073382"
  },
  {
    "text": "which belong to L two space. And the integral, I\nwill have this equation.",
    "start": "1074780",
    "end": "1082463"
  },
  {
    "text": "And they can say that\nconditional probability solution of this equation.",
    "start": "1084820",
    "end": "1089043"
  },
  {
    "text": "It was constructed like that, because you should put",
    "start": "1091600",
    "end": "1098380"
  },
  {
    "text": "conditional probability,\nI did something like that. But, I would like to solve this equation",
    "start": "1098380",
    "end": "1106760"
  },
  {
    "text": "to find function when I don't\nknow probability measure,",
    "start": "1107110",
    "end": "1112110"
  },
  {
    "text": "but I'm given data, given observations",
    "start": "1112290",
    "end": "1117290"
  },
  {
    "text": "generated according to p x, p y,x,",
    "start": "1117870",
    "end": "1122870"
  },
  {
    "text": "I would like to solve this equation. But solving equation,\nit is ill-posed problem.",
    "start": "1123160",
    "end": "1129173"
  },
  {
    "text": "OK, let's do that. But before I will do that,\nI would like to mention",
    "start": "1130490",
    "end": "1136500"
  },
  {
    "text": "that in classical statistics, there is a way",
    "start": "1136500",
    "end": "1141940"
  },
  {
    "text": "how to replace unknown probability measure",
    "start": "1144806",
    "end": "1150370"
  },
  {
    "text": "with empirical measure. And that is the most important part, is main inductive step statistics.",
    "start": "1151246",
    "end": "1159210"
  },
  {
    "text": "In statistics, we're given data and would like to know function, and it doesn't matter how many data.",
    "start": "1159210",
    "end": "1165760"
  },
  {
    "text": "We will see, it is not\nequivalent to function. So in classical statistics,",
    "start": "1165760",
    "end": "1173980"
  },
  {
    "text": "people suggest to approximate cumulative distribution function",
    "start": "1173980",
    "end": "1180150"
  },
  {
    "text": "by empirical cumulative\ndistribution function, and that is empirical cumulative\ndistribution function.",
    "start": "1180150",
    "end": "1187410"
  },
  {
    "text": "And 30 years, mathematicians\ntried to prove,",
    "start": "1187410",
    "end": "1192410"
  },
  {
    "text": "that it is good idea that we can do that, and in '33, Komogorov found exact bound",
    "start": "1192510",
    "end": "1201780"
  },
  {
    "text": "which is on the last line, almost the same like in the last line,",
    "start": "1201780",
    "end": "1207300"
  },
  {
    "text": "and then people prove that\nyou can bound like that.",
    "start": "1207300",
    "end": "1212300"
  },
  {
    "text": "Now, if we can replace unknown measure with empirical measure,",
    "start": "1213530",
    "end": "1219350"
  },
  {
    "text": "we can construct our problem,",
    "start": "1219350",
    "end": "1223230"
  },
  {
    "text": "our constructive problem,\nwhat we should do. Let us replace in this function now",
    "start": "1226800",
    "end": "1232330"
  },
  {
    "text": "which we would like to minimize instead of a real mirror our\nmeasure, empirical measure.",
    "start": "1232330",
    "end": "1239419"
  },
  {
    "text": "And then you have, empirical, real square root functional",
    "start": "1239420",
    "end": "1247519"
  },
  {
    "text": "which you have to minimize to find our problem,",
    "start": "1247520",
    "end": "1254600"
  },
  {
    "text": "to find our conditional\nprobability of this is whatever you want.",
    "start": "1255230",
    "end": "1259970"
  },
  {
    "text": "But let me consider new\nconstructive setting, where we also will replace\nunknown probability measure",
    "start": "1261720",
    "end": "1268280"
  },
  {
    "text": "with empirical probability measure, obtained on the training data.",
    "start": "1269355",
    "end": "1274790"
  },
  {
    "text": "And you will see the last equation to find conditional probability,",
    "start": "1274790",
    "end": "1282830"
  },
  {
    "text": "we have to solve this equation in set of function f of x,",
    "start": "1282830",
    "end": "1288633"
  },
  {
    "text": "right hand side is known\nbecause our function G is known. In left hand side you don't\nknow f of x or it is our goal",
    "start": "1290630",
    "end": "1299420"
  },
  {
    "text": "to set a function to find this function. In classical statistics,\nit was one algorithm",
    "start": "1299420",
    "end": "1308720"
  },
  {
    "text": "called Watson-Nadaraya estimator which show how to estimate\nconditional probability",
    "start": "1308720",
    "end": "1315175"
  },
  {
    "text": "or integration of function. They just somehow defined this.",
    "start": "1315176",
    "end": "1320700"
  },
  {
    "text": "And this is, I show function which is",
    "start": "1320700",
    "end": "1324909"
  },
  {
    "text": "have a numerator, and denominator. So, G is special kernel, say, Gaussian.",
    "start": "1325820",
    "end": "1333623"
  },
  {
    "text": "So this is estimate of regression, a very general way of\nestimation regression,",
    "start": "1335550",
    "end": "1342030"
  },
  {
    "text": "conditional probability,\nand all this business, how to find, if Gaussian,\nhow to find the best value",
    "start": "1342030",
    "end": "1350060"
  },
  {
    "text": "of variance to approximate\nconditional probability well.",
    "start": "1350540",
    "end": "1355540"
  },
  {
    "text": "So they spent a lot of\ntime on this subject and they have this. But you can see the line in middle,",
    "start": "1355680",
    "end": "1365540"
  },
  {
    "text": "this is Watson, Nadaraya-Watson estimator",
    "start": "1365890",
    "end": "1370890"
  },
  {
    "text": "comes from corrupted equation, not from equation which we derive.",
    "start": "1371000",
    "end": "1376580"
  },
  {
    "text": "Here, it is in the middle is f of x, not of f of x, i",
    "start": "1376580",
    "end": "1384059"
  },
  {
    "text": "like in last, like in kernel,\nso then you can put out of sum f of x and you\nwill get this function.",
    "start": "1384060",
    "end": "1392245"
  },
  {
    "text": "So, actually classical\nNadaraya-Watson estimator, it is solution of corrupted\nequation which we obtained.",
    "start": "1392245",
    "end": "1401850"
  },
  {
    "text": "But what means to solve equation? To solve equation means I\njust take the difference",
    "start": "1404797",
    "end": "1412550"
  },
  {
    "text": "between left hand side\nand right hand side. Define area where I would like",
    "start": "1412670",
    "end": "1418620"
  },
  {
    "text": "that my function will operate, take the square,",
    "start": "1418620",
    "end": "1423993"
  },
  {
    "text": "and sum, and integrate over\nsum probability measure, and minimize this functional.",
    "start": "1426399",
    "end": "1431473"
  },
  {
    "text": "So let us do that, and if\nyou will do simple algebra, it is just very simple,\nand you can check it,",
    "start": "1433370",
    "end": "1442140"
  },
  {
    "text": "you will have this R f function\nwhich is y minus f of x,",
    "start": "1442140",
    "end": "1447140"
  },
  {
    "text": "y j minus f of x j",
    "start": "1450050",
    "end": "1455050"
  },
  {
    "text": "multiply on some\ncoefficients, where x y, x j.",
    "start": "1456010",
    "end": "1461010"
  },
  {
    "text": "So, we can estimate this value, and this value is j x y, x i,",
    "start": "1463765",
    "end": "1472370"
  },
  {
    "text": "j x i, x g over measure, and this is matrix, if you\nknow from Watson-Nadaraya",
    "start": "1472990",
    "end": "1480940"
  },
  {
    "text": "exact formula, we know this\nmatrix, this element of matrix.",
    "start": "1480940",
    "end": "1485940"
  },
  {
    "text": "So, what is V-matrix? If we will replace this integral\nwith empirical integral,",
    "start": "1489050",
    "end": "1498243"
  },
  {
    "text": "we will have this estimate of the matrix. If we will use just, say,",
    "start": "1499398",
    "end": "1506903"
  },
  {
    "text": "line between minus one and one, and new f x is uniformly\ndistributed over this line,",
    "start": "1508290",
    "end": "1515890"
  },
  {
    "text": "we will have, we will have",
    "start": "1515890",
    "end": "1521732"
  },
  {
    "text": "n G's Gaussian distribution, we will have this V-matrix.",
    "start": "1524192",
    "end": "1529630"
  },
  {
    "text": "So V-matrix is easy to find.",
    "start": "1529630",
    "end": "1533003"
  },
  {
    "text": "Now, I would like to use vector notation. What I will do, I will call Y vectors",
    "start": "1534740",
    "end": "1543019"
  },
  {
    "text": "of elements of training data, y one, y l, I am given l pairs,",
    "start": "1543020",
    "end": "1549590"
  },
  {
    "text": "so I create Y, vector Y, which is vector of all Ys.",
    "start": "1549590",
    "end": "1557470"
  },
  {
    "text": "I will create F, capital from f, which is also all dimensional vector",
    "start": "1557470",
    "end": "1564200"
  },
  {
    "text": "of I will pick up function f, and this is really of this function,",
    "start": "1564200",
    "end": "1571280"
  },
  {
    "text": "endpoint x one, and the last\nis value of this function, is a point x l.",
    "start": "1571280",
    "end": "1578960"
  },
  {
    "text": "So this is f, and I have V-matrix. So, and I can rewrite\nthis functional in the way",
    "start": "1578960",
    "end": "1586090"
  },
  {
    "text": "that in matrix form I have\ny minus F, capital from f,",
    "start": "1586090",
    "end": "1591090"
  },
  {
    "text": "V, y minus F, capital from f. But if I will write this\nnotation in least squares method,",
    "start": "1592810",
    "end": "1601586"
  },
  {
    "text": "I will have Y minus F capital from f, Y minus F capital from f,",
    "start": "1601586",
    "end": "1609140"
  },
  {
    "text": "and here, instead of Y, identity matrix. So, I got some improvement",
    "start": "1609140",
    "end": "1617550"
  },
  {
    "text": "over least squares method. And I hope that it will",
    "start": "1618580",
    "end": "1624860"
  },
  {
    "text": "give me a rate of convergence better than this least squares method, but, let's see.",
    "start": "1626720",
    "end": "1634550"
  },
  {
    "start": "1633000",
    "end": "2246000"
  },
  {
    "text": "But it is not major stuff. Because, OK, I am prove\nrate of convergence,",
    "start": "1634550",
    "end": "1640669"
  },
  {
    "text": "but is still this square method good? But now, the most important part,",
    "start": "1640670",
    "end": "1645870"
  },
  {
    "text": "selection of admissible set of functions. What it means?",
    "start": "1645870",
    "end": "1650462"
  },
  {
    "text": "When you construct a neural network,",
    "start": "1652200",
    "end": "1655313"
  },
  {
    "text": "you talking that you are\ndoing smart structure, what it means, you're talking\nthat you're constructing",
    "start": "1657250",
    "end": "1665190"
  },
  {
    "text": "smart, admissible set of functions. You know something, you are smart guys,",
    "start": "1665190",
    "end": "1672310"
  },
  {
    "text": "you're just constructing,\nand then you minimize over the set of functions.",
    "start": "1672310",
    "end": "1677093"
  },
  {
    "text": "But, let me consider from\na theoretical perspective,",
    "start": "1679160",
    "end": "1684160"
  },
  {
    "text": "what it is. If you consider Hilbert space,",
    "start": "1684557",
    "end": "1688853"
  },
  {
    "text": "and also Euclidean space, in this space, there are\ntwo ways of convergence.",
    "start": "1689960",
    "end": "1697033"
  },
  {
    "text": "Strong convergence, it is\nconvergence of functions, it is first line.",
    "start": "1698220",
    "end": "1703930"
  },
  {
    "text": "My set of, my sequence of\nfunction f l converge to f zero,",
    "start": "1703930",
    "end": "1708930"
  },
  {
    "text": "if f l is integral converged\nfrom f l goes to infinity.",
    "start": "1709750",
    "end": "1714750"
  },
  {
    "text": "But there exists weak convergence. We say that my set of, my\nsequence of function f l",
    "start": "1716550",
    "end": "1724769"
  },
  {
    "text": "converge for f zero, if this inner product",
    "start": "1726060",
    "end": "1731060"
  },
  {
    "text": "converged to this inner\nproduct for all function phi,",
    "start": "1731340",
    "end": "1735856"
  },
  {
    "text": "from Hilbert space. You can see that this is an inner product",
    "start": "1737070",
    "end": "1743299"
  },
  {
    "text": "described property of function, if for all functions,\nproperty is the same,",
    "start": "1743300",
    "end": "1751720"
  },
  {
    "text": "that will be convergence.",
    "start": "1751720",
    "end": "1753263"
  },
  {
    "text": "So it is easy to show that if\nyou have strong convergence, you also have weak convergence,",
    "start": "1757850",
    "end": "1764090"
  },
  {
    "text": "just from Watson, from Schwarz inequality,",
    "start": "1764090",
    "end": "1768653"
  },
  {
    "text": "Cauchy-Schwarz inequality. But also, one can prove that\nif you have weak convergence,",
    "start": "1769935",
    "end": "1777600"
  },
  {
    "text": "and your set of function\nbelong to compact, you also have strong convergence.",
    "start": "1778634",
    "end": "1784260"
  },
  {
    "text": "So, in some sense, the both\nconvergence are equivalent.",
    "start": "1784260",
    "end": "1789260"
  },
  {
    "text": "In our consideration of machine logic,",
    "start": "1790070",
    "end": "1795070"
  },
  {
    "text": "you can see the strong\nconvergence everywhere, 100%. But what about weak convergence?",
    "start": "1795490",
    "end": "1801660"
  },
  {
    "text": "Let's explore this opportunity.",
    "start": "1801660",
    "end": "1803463"
  },
  {
    "text": "Let us consider pattern recognition case. For pattern recognition\ncase, the first line,",
    "start": "1809510",
    "end": "1816700"
  },
  {
    "text": "I just write in the\ndefinition of weak convergence equals second, and that is I use",
    "start": "1816700",
    "end": "1825159"
  },
  {
    "text": "bias in the equation, it is phi from dP equal one over phi",
    "start": "1826580",
    "end": "1832280"
  },
  {
    "text": "for all functions from phi. So it converge,",
    "start": "1832280",
    "end": "1840460"
  },
  {
    "text": "it must converge for\nall functions from phi. If it converge for all functions from phi,",
    "start": "1840460",
    "end": "1847593"
  },
  {
    "text": "I will have one function\nwhich is desired one.",
    "start": "1848930",
    "end": "1853143"
  },
  {
    "text": "But it is not realistic. Let us do following. Let us select from set of Hilbert space,",
    "start": "1854150",
    "end": "1862930"
  },
  {
    "text": "m function phi. We will talk a lot how to\nselect these functions.",
    "start": "1862930",
    "end": "1868408"
  },
  {
    "text": "So, and then we will consider equality,",
    "start": "1869350",
    "end": "1872992"
  },
  {
    "text": "not for all function, but\njust for this m function. And we will call admissible\nset of functions,",
    "start": "1874877",
    "end": "1882293"
  },
  {
    "text": "the set of functions which\nsatisfies this equality. We know that our function must satisfy",
    "start": "1883201",
    "end": "1890673"
  },
  {
    "text": "this equality for any phi, any phi, because of the convergence,",
    "start": "1893620",
    "end": "1899187"
  },
  {
    "text": "but we select something which we want.",
    "start": "1900127",
    "end": "1903294"
  },
  {
    "text": "OK, if you will use, instead of our",
    "start": "1906562",
    "end": "1911562"
  },
  {
    "text": "cumulative distribution\nfunction, an empirical estimate, we will have instead of\nintegral property like here,",
    "start": "1915080",
    "end": "1923193"
  },
  {
    "text": "the property written by the sum. Again, let me use the same notation",
    "start": "1925257",
    "end": "1933590"
  },
  {
    "text": "for matrix m, I will use Y vector,",
    "start": "1933590",
    "end": "1937976"
  },
  {
    "text": "I will use function F capital\nfrom f, which is f from x one,",
    "start": "1939160",
    "end": "1944160"
  },
  {
    "text": "f from x l, which is vector, for any function f I have vector,",
    "start": "1944600",
    "end": "1949093"
  },
  {
    "text": "and also, I introduce new vector,",
    "start": "1950050",
    "end": "1955050"
  },
  {
    "text": "vector of predicates on the value x one, x l.",
    "start": "1956360",
    "end": "1962330"
  },
  {
    "text": "Because phi is function, I can consider value of\nfunction in this case.",
    "start": "1962330",
    "end": "1969740"
  },
  {
    "text": "Then, I can write",
    "start": "1969740",
    "end": "1974260"
  },
  {
    "text": "my equation in this form. I would like that my\nadmissible set of function",
    "start": "1975940",
    "end": "1985160"
  },
  {
    "text": "satisfy in vector form, these m equations.",
    "start": "1985160",
    "end": "1988853"
  },
  {
    "text": "Now,",
    "start": "1991160",
    "end": "1991993"
  },
  {
    "text": "let me explain what we talking about.",
    "start": "1996910",
    "end": "2000083"
  },
  {
    "text": "There is a duck test logic.",
    "start": "2003105",
    "end": "2005277"
  },
  {
    "text": "If it looks like a\nduck, swims like a duck, and quack like a duck,\nthen it probably is a duck.",
    "start": "2009720",
    "end": "2016793"
  },
  {
    "text": "What this means, we have\nthese statistical invariants in vector form like this, this line.",
    "start": "2018160",
    "end": "2024820"
  },
  {
    "text": "What it does, it collect\nadmissible function which identify animals as a duck",
    "start": "2024820",
    "end": "2033299"
  },
  {
    "text": "if it looks, swims and quack like a duck. So if you will choose\npredicate which explains",
    "start": "2033300",
    "end": "2042640"
  },
  {
    "text": "what means, looks, swims, and quack, then your admissible\nfunction will be a function,",
    "start": "2042640",
    "end": "2051919"
  },
  {
    "text": "such function for which classify animals",
    "start": "2051920",
    "end": "2056919"
  },
  {
    "text": "that swim, quack, and looks like a duck.",
    "start": "2058210",
    "end": "2063202"
  },
  {
    "text": "Concept of predicate, it is\nvery different from feature. Why so?",
    "start": "2065980",
    "end": "2072721"
  },
  {
    "text": "With increasing number of predicates, the VC-dimension of admissible\nset of function is decreased.",
    "start": "2072722",
    "end": "2079652"
  },
  {
    "text": "Why does it decrease? Because we have set of function, then we from this set of\nfunction, select function",
    "start": "2080700",
    "end": "2089040"
  },
  {
    "text": "which satisfy new predicate. Not all of function will satisfy,",
    "start": "2089040",
    "end": "2094429"
  },
  {
    "text": "and consider only set of\nfunction which satisfy all these predicates. But with increasing number of features,",
    "start": "2094430",
    "end": "2102120"
  },
  {
    "text": "VC-dimension increase\nbecause your decisions are all becoming more and more diverse.",
    "start": "2102120",
    "end": "2109973"
  },
  {
    "text": "So what is exact setting of\ncomplete learning problem?",
    "start": "2111690",
    "end": "2115453"
  },
  {
    "text": "Minimize functional\nwhich is this V matrix.",
    "start": "2117010",
    "end": "2122010"
  },
  {
    "text": "A little bit improved of\nthe square functional, subject to this constraint.",
    "start": "2123360",
    "end": "2128063"
  },
  {
    "text": "And this constraint is\nwhat you would like to see",
    "start": "2129738",
    "end": "2134738"
  },
  {
    "text": "in the set of admissible functions.",
    "start": "2134880",
    "end": "2137073"
  },
  {
    "text": "But, existing classical\nmethod of pattern recognition, they just minimize this functional,",
    "start": "2140160",
    "end": "2148950"
  },
  {
    "text": "the least square functional. So, minimizing this functional\nsubject to this constraint,",
    "start": "2148950",
    "end": "2156290"
  },
  {
    "text": "that is our set. That was exact setting,",
    "start": "2156290",
    "end": "2162660"
  },
  {
    "text": "which is in mathematical is called",
    "start": "2162660",
    "end": "2165859"
  },
  {
    "text": "conditional optimization, optimization of functional\nunder conditions.",
    "start": "2170160",
    "end": "2174503"
  },
  {
    "text": "But the approximation is\nunconditional optimization. I would like minimize this functional",
    "start": "2175640",
    "end": "2181440"
  },
  {
    "text": "subject to this constraint,\nbut I will do following. I will make sum of this functional,",
    "start": "2181440",
    "end": "2189500"
  },
  {
    "text": "and I will take square of difference between this constraint and make a sum,",
    "start": "2189500",
    "end": "2199290"
  },
  {
    "text": "the sum weight, weight, it should be one. So I would like to do both,\nto minimize over both,",
    "start": "2200545",
    "end": "2208650"
  },
  {
    "text": "and both weights have\nimportant for me to minimize, they can have important for\nme to minimize constraint.",
    "start": "2209200",
    "end": "2218483"
  },
  {
    "text": "And then if I will do that, I can rewrite this function in this way",
    "start": "2219971",
    "end": "2225840"
  },
  {
    "text": "where you have to\nminimize this functional,",
    "start": "2227090",
    "end": "2231583"
  },
  {
    "text": "and where P is just covariance\nmatrix of your predicate.",
    "start": "2233250",
    "end": "2240082"
  },
  {
    "text": "And everything is simple compute.",
    "start": "2241690",
    "end": "2243613"
  },
  {
    "start": "2246000",
    "end": "3196000"
  },
  {
    "text": "So that is concept, what we have to do.",
    "start": "2247470",
    "end": "2252182"
  },
  {
    "text": "We have to solve our problem using both big and strong,",
    "start": "2253475",
    "end": "2259840"
  },
  {
    "text": "strong convergence that\nmeans, using invariance",
    "start": "2259840",
    "end": "2264630"
  },
  {
    "text": "and minimizing functionals, and we can do it in exact way,",
    "start": "2265910",
    "end": "2274603"
  },
  {
    "text": "and in approximation. But, here it was written\nfor any set of functions.",
    "start": "2275720",
    "end": "2283120"
  },
  {
    "text": "I did not talk how I will minimize that. So it is true how as the\nleast squares method,",
    "start": "2283160",
    "end": "2289970"
  },
  {
    "text": "you can minimize the\nleast square functional for any set of functions,\nthat is the same here.",
    "start": "2289970",
    "end": "2295422"
  },
  {
    "text": "But now, let me see",
    "start": "2297540",
    "end": "2300080"
  },
  {
    "text": "how I can find the solution. And first of all, I will\ndo it for reproducing",
    "start": "2302760",
    "end": "2309540"
  },
  {
    "text": "kernel Hilbert space. This is the definition of\nreproducing kernel Hilbert space.",
    "start": "2309540",
    "end": "2316339"
  },
  {
    "text": "You have some kernel\nwhich is Mercer kernel, you multiply, you're taking the product",
    "start": "2316340",
    "end": "2321780"
  },
  {
    "text": "this function f of x, and\nyou have the same function, it is important to use the same function",
    "start": "2321780",
    "end": "2328040"
  },
  {
    "text": "it is called reproducing\nkernel of Hilbert space.",
    "start": "2328040",
    "end": "2331933"
  },
  {
    "text": "And it is known that\nkernel, Mercer kernel, have expansion of lambda where lambda",
    "start": "2333198",
    "end": "2339329"
  },
  {
    "text": "is there's no negative values, and psi is orthonormal functions.",
    "start": "2339330",
    "end": "2344815"
  },
  {
    "text": "So, set of function",
    "start": "2346547",
    "end": "2348130"
  },
  {
    "text": "is inner product and norm. This is inner product, and that is norm.",
    "start": "2351559",
    "end": "2358205"
  },
  {
    "text": "Forms reproducing kernel Hilbert space, it is very easy to check.",
    "start": "2358205",
    "end": "2363290"
  },
  {
    "text": "It means that if you will use",
    "start": "2363290",
    "end": "2365880"
  },
  {
    "text": "some function psi,",
    "start": "2369060",
    "end": "2374060"
  },
  {
    "text": "orthonormal function psi, and its expansion of c,",
    "start": "2374580",
    "end": "2379800"
  },
  {
    "text": "and if you will have this set of function, and if you will introduce\nspecial inner product,",
    "start": "2379800",
    "end": "2387863"
  },
  {
    "text": "inner product of this type, and then you will have the definition.",
    "start": "2389140",
    "end": "2395490"
  },
  {
    "text": "So you will have reproduction\nkernel Hilbert space. It is pretty general space.",
    "start": "2395490",
    "end": "2400303"
  },
  {
    "text": "But, in reproducing kernel Hilbert space, you have a great theorem\ncalled representer theorem.",
    "start": "2403590",
    "end": "2410330"
  },
  {
    "text": "And representer theorem says if you would like to\nminimize this functional,",
    "start": "2411640",
    "end": "2417134"
  },
  {
    "text": "you subset the function,\nand subset of function, it's norm of your function",
    "start": "2417134",
    "end": "2423450"
  },
  {
    "text": "in reproducing kernel\nHilbert space is bounded,",
    "start": "2423450",
    "end": "2428450"
  },
  {
    "text": "so then your solution has\na linear representation, over kernel with finite\nnumber of parameters.",
    "start": "2428940",
    "end": "2436363"
  },
  {
    "text": "So, let's introduce\nmatrix, vector of functions",
    "start": "2438970",
    "end": "2443970"
  },
  {
    "text": "f K x one effects, it is vector expansion,",
    "start": "2445130",
    "end": "2449882"
  },
  {
    "text": "and then square of our norm",
    "start": "2451160",
    "end": "2455900"
  },
  {
    "text": "will be A, and this is A\nover K, this is what is,",
    "start": "2458920",
    "end": "2463793"
  },
  {
    "text": "how it looks, your function\nwhich you're looking for, A K A is norm of your function",
    "start": "2465284",
    "end": "2472579"
  },
  {
    "text": "from reproducing kernel Hilbert space, K is matrix K xi xj,",
    "start": "2474590",
    "end": "2483170"
  },
  {
    "text": "and this is F of f from reproducing kernel Hilbert space",
    "start": "2483580",
    "end": "2489790"
  },
  {
    "text": "is just linear function. Subset of function is bounded\nnorm inside of VC-dimension,",
    "start": "2489790",
    "end": "2498880"
  },
  {
    "text": "the smaller C, the smaller VC-dimension. And that is according to\nsecond theorem with j,",
    "start": "2499420",
    "end": "2506050"
  },
  {
    "text": "show you before. To control VC-dimension,\nyou should control just C.",
    "start": "2506050",
    "end": "2515540"
  },
  {
    "text": "You should be looking for\nnorm of this function.",
    "start": "2515540",
    "end": "2520540"
  },
  {
    "text": "So the conditional\nminimization in producing kernel Hilbert space,\nhas closed form solution",
    "start": "2521940",
    "end": "2529089"
  },
  {
    "text": "to minimize this functional\nsubject to this constraint. And constraint on bound of the norm",
    "start": "2529090",
    "end": "2536970"
  },
  {
    "text": "will give you, and this is your solution, linear expansion of this\nvector of functions,",
    "start": "2537821",
    "end": "2547589"
  },
  {
    "text": "and value of coefficients",
    "start": "2547620",
    "end": "2549940"
  },
  {
    "text": "is like that where it is just in closed form",
    "start": "2553399",
    "end": "2559310"
  },
  {
    "text": "with this function of matrix,\nthis multiplication of matrix, this is gamma c, C is depends on this ,",
    "start": "2559310",
    "end": "2568460"
  },
  {
    "text": "where you see gamma of\nc depends on this C, this is identical matrix,\nor you have the solution.",
    "start": "2569660",
    "end": "2577560"
  },
  {
    "text": "And to find u over here, you\nhave to solve linear equation.",
    "start": "2577610",
    "end": "2582610"
  },
  {
    "text": "So, you're solving linear equation, and then you have closed form solution.",
    "start": "2582890",
    "end": "2587250"
  },
  {
    "text": "So the complete problem in\nreproducing kernel Hilbert space",
    "start": "2588310",
    "end": "2593310"
  },
  {
    "text": "have closed form solution.",
    "start": "2594040",
    "end": "2597533"
  },
  {
    "text": "But what about unconditional minimization,",
    "start": "2599400",
    "end": "2604400"
  },
  {
    "text": "approximate minimization like that in this constraint?",
    "start": "2604460",
    "end": "2609740"
  },
  {
    "text": "It also has closed form solution, and this is how it looks,\nclosed form solution.",
    "start": "2609740",
    "end": "2615280"
  },
  {
    "text": "Your solution is coefficients\nover this expansion, and you have this equation",
    "start": "2615280",
    "end": "2622660"
  },
  {
    "text": "you have to find A, so everything is computable.",
    "start": "2622660",
    "end": "2628360"
  },
  {
    "text": "But very special rule play in explanation support vector machine.",
    "start": "2633020",
    "end": "2638422"
  },
  {
    "text": "What is support vector machine? Given data, I would",
    "start": "2640170",
    "end": "2644510"
  },
  {
    "text": "I would like when reproducing\nkernel Hilbert space,",
    "start": "2646890",
    "end": "2649973"
  },
  {
    "text": "this bounded norm\nminimize this functional.",
    "start": "2652315",
    "end": "2656880"
  },
  {
    "text": "Then, when they're\nminimizing this functional, that is exactly you will\nget support vector machine.",
    "start": "2658800",
    "end": "2666750"
  },
  {
    "text": "If you look there I've\nsupported the machine, it is just minimization this function now,",
    "start": "2668630",
    "end": "2674910"
  },
  {
    "text": "this set of function. But here, you'll do something else. I will do data like in\nsupport vector machine,",
    "start": "2674910",
    "end": "2684280"
  },
  {
    "text": "y i minus A theta A, that is I would like approximate data.",
    "start": "2685560",
    "end": "2692703"
  },
  {
    "text": "It's unknown like here,\nbut also I would like to that my invariant will be good enough,",
    "start": "2692703",
    "end": "2701810"
  },
  {
    "text": "will be close. Left hand side and right hand side, often invariant will be close. So I would like minimize this functional",
    "start": "2702970",
    "end": "2710270"
  },
  {
    "text": "under the same constraint. And this is the solution. The solution is like A,",
    "start": "2710270",
    "end": "2716063"
  },
  {
    "text": "A is function which is phi from t,",
    "start": "2717006",
    "end": "2721810"
  },
  {
    "text": "and phi from t, as you remember, it is vector of predicate,",
    "start": "2723990",
    "end": "2730173"
  },
  {
    "text": "and this is indicator vector. If I would like, here",
    "start": "2731330",
    "end": "2736900"
  },
  {
    "text": "to make strong for",
    "start": "2737940",
    "end": "2742440"
  },
  {
    "text": "invariants and not too strong\nfor approximation function,",
    "start": "2744720",
    "end": "2749720"
  },
  {
    "text": "I just want to use just weak convergence.",
    "start": "2751090",
    "end": "2754490"
  },
  {
    "text": "I will have that my A is defined by invariants,",
    "start": "2759384",
    "end": "2765203"
  },
  {
    "text": "by function of predicate. But, my function of predicate,\nhow I choose predicate?",
    "start": "2766228",
    "end": "2773963"
  },
  {
    "text": "I can choose any function I want. That means that if I\ncan choose one function",
    "start": "2773963",
    "end": "2782090"
  },
  {
    "text": "which will give me optimal solution, then there exists a smart predicate",
    "start": "2782660",
    "end": "2787940"
  },
  {
    "text": "that I will not need a lot of data. I need what, why I need data,",
    "start": "2789144",
    "end": "2794740"
  },
  {
    "text": "I need data for expansion of our kernel. But for estimating\ncoefficients, I don't need data.",
    "start": "2794740",
    "end": "2802530"
  },
  {
    "text": "I use my predicate and that means that",
    "start": "2802530",
    "end": "2807240"
  },
  {
    "text": "what is, what means predicate? It is property, it is explanation about what I want.",
    "start": "2809070",
    "end": "2816200"
  },
  {
    "text": "I will talk about this later. So, this example of\nsupport vector machine show",
    "start": "2816200",
    "end": "2823360"
  },
  {
    "text": "that philosophy of this\nlearning is very different.",
    "start": "2824820",
    "end": "2829137"
  },
  {
    "text": "According to representer theorem, solution of learning\nproblem in reproducing",
    "start": "2830590",
    "end": "2835609"
  },
  {
    "text": "kernel Hilbert space have a property. It defined linear parametric function",
    "start": "2835610",
    "end": "2841049"
  },
  {
    "text": "in form of expansion of kernel function.",
    "start": "2841050",
    "end": "2844553"
  },
  {
    "text": "That means that optimal expansion belong to one layer network,\nnot multi-layer network,",
    "start": "2847400",
    "end": "2854490"
  },
  {
    "text": "but because of reproducing\nkernel Hilbert space,",
    "start": "2855820",
    "end": "2860820"
  },
  {
    "text": "is richer than this. Maybe it is not the best idea to use",
    "start": "2861710",
    "end": "2866780"
  },
  {
    "text": "deep network, deep network, OK, we will discuss this deep net.",
    "start": "2868560",
    "end": "2873023"
  },
  {
    "text": "Observation vectors and kernel\ndefine basis for expansion for optimal l parametric solution.",
    "start": "2875130",
    "end": "2883270"
  },
  {
    "text": "But parameters is defined by invariants,",
    "start": "2883270",
    "end": "2888270"
  },
  {
    "text": "but you could use these invariants, and what it means if you\nwill write in the form,",
    "start": "2890717",
    "end": "2898280"
  },
  {
    "text": "that you have K which is matrix, depending on your training data,",
    "start": "2898280",
    "end": "2904765"
  },
  {
    "text": "phi is predicate matrix, so\nyou have vector over here,",
    "start": "2904765",
    "end": "2909765"
  },
  {
    "text": "and this is Y vector, F is vector, so you have element, you have different",
    "start": "2910540",
    "end": "2920510"
  },
  {
    "text": "formulation of learning problem in term of this vector and these values.",
    "start": "2924140",
    "end": "2931172"
  },
  {
    "text": "So since function in Hilbert space can be,",
    "start": "2932620",
    "end": "2936703"
  },
  {
    "text": "since any function from\nHilbert space can be used, because when we talked\nabout weak convergence,",
    "start": "2938950",
    "end": "2946309"
  },
  {
    "text": "it converged for all functions\nin the case of Hilbert space. So any function can be used.",
    "start": "2946310",
    "end": "2952343"
  },
  {
    "text": "So we have a chance to pick\nup several smart functions,",
    "start": "2953380",
    "end": "2958380"
  },
  {
    "text": "or maybe even one, and it will\nbe enough for our training, and then we will talk\nabout what means smart,",
    "start": "2958950",
    "end": "2966020"
  },
  {
    "text": "it is intelligent learning, not just brute force learning.",
    "start": "2966020",
    "end": "2970803"
  },
  {
    "text": "So, let me give you an\nillustration to have a feeling. This is least square method, what it does.",
    "start": "2973000",
    "end": "2980632"
  },
  {
    "text": "This is V-matrix method, it will do a little bit improvement. It's a little bit better.",
    "start": "2981980",
    "end": "2988770"
  },
  {
    "text": "Here, if I will introduce\ninvariants, I will do better. Here, if I will use both\ninvariant and V-matrix,",
    "start": "2988770",
    "end": "2998200"
  },
  {
    "text": "so you can see there's\ndifference for 48 points, and to be sure that it\nis difficult problem,",
    "start": "2999480",
    "end": "3009480"
  },
  {
    "text": "we took 16 from one class\nand 32 from another class, it is not difficult.",
    "start": "3010370",
    "end": "3017223"
  },
  {
    "text": "This is 98 points, the same story, this is 192 points, the same story,",
    "start": "3017223",
    "end": "3022653"
  },
  {
    "text": "and this is multidimensional case, so we check something and we did it.",
    "start": "3024200",
    "end": "3030680"
  },
  {
    "text": "And that very interesting case. We introduce some invariants",
    "start": "3030680",
    "end": "3038220"
  },
  {
    "text": "and what 22.73 error rate for diabetes.",
    "start": "3038220",
    "end": "3043220"
  },
  {
    "text": "And we decided can we find invariant to improve it, performance. And what we did, we're\njust looking for area",
    "start": "3044266",
    "end": "3052780"
  },
  {
    "text": "where our invariants does not take place, they violate it.",
    "start": "3052780",
    "end": "3058033"
  },
  {
    "text": "Then we just took predicate which",
    "start": "3059184",
    "end": "3063180"
  },
  {
    "text": "just doing with this area which can't have one when your,",
    "start": "3064410",
    "end": "3071793"
  },
  {
    "text": "when point belong to this area, and zero when it doesn't belong. And we improve using this invariant",
    "start": "3073220",
    "end": "3080359"
  },
  {
    "text": "from 73, .73 to .07.",
    "start": "3083169",
    "end": "3087597"
  },
  {
    "text": "So, that means that if you have smart way",
    "start": "3090336",
    "end": "3093753"
  },
  {
    "text": "to looking for invariants,\nthen you can have a chance",
    "start": "3096351",
    "end": "3101351"
  },
  {
    "text": "to improve your performance, but this is exactly the same\nphilosophy which use physicist.",
    "start": "3101477",
    "end": "3107903"
  },
  {
    "text": "Find the solution, the box in figure, where the existing solution,\nfind situation, the box,",
    "start": "3108930",
    "end": "3117060"
  },
  {
    "text": "where existing solution, the\napproximation which we obtained contradicts evidence,\ndoes not keep invariants,",
    "start": "3118400",
    "end": "3127963"
  },
  {
    "text": "contradicts invariance inside the box. And then modify the solution,\nobtain new approximation",
    "start": "3129700",
    "end": "3136300"
  },
  {
    "text": "which resolves this contradiction, which doesn't have this contradiction.",
    "start": "3137330",
    "end": "3142780"
  },
  {
    "text": "So you're just looking,\nyou inventing invariants, looking where you have contradiction,",
    "start": "3142780",
    "end": "3148990"
  },
  {
    "text": "and that is exactly the same principle that use physicists to\ndiscover law of nature.",
    "start": "3148990",
    "end": "3155020"
  },
  {
    "text": "To discover law of nature, physicists first trying to find situation where existing theory\ncontradict observations.",
    "start": "3155020",
    "end": "3162283"
  },
  {
    "text": "So invariant fail. Theoretical predictions do\nnot supported by experiments,",
    "start": "3164090",
    "end": "3169750"
  },
  {
    "text": "that means invariant, but. Then they trying to reconstruct theory to remove contradiction, they\nconstruct new approximation",
    "start": "3169750",
    "end": "3177590"
  },
  {
    "text": "of theory which does not\ncontain contradiction observed. But the most important part in physics",
    "start": "3177590",
    "end": "3185120"
  },
  {
    "text": "like in here is the more difficult part in scientific discovery,",
    "start": "3185120",
    "end": "3191200"
  },
  {
    "text": "how to find contradictive situation.",
    "start": "3191200",
    "end": "3194766"
  },
  {
    "start": "3196000",
    "end": "3568000"
  },
  {
    "text": "Let me show something about neural net. I am not fond of neural nets,",
    "start": "3197103",
    "end": "3202640"
  },
  {
    "text": "but we can use",
    "start": "3202640",
    "end": "3206799"
  },
  {
    "text": "our theory from neural net as well. What is neural net? That is neural net, you're\nminimizing least square error.",
    "start": "3208090",
    "end": "3215563"
  },
  {
    "text": "I would minimize this\napproximation of invariants",
    "start": "3217540",
    "end": "3222437"
  },
  {
    "text": "which is contained VP\nmatrix, V plus P matrix,",
    "start": "3224460",
    "end": "3229460"
  },
  {
    "text": "matrix P with invariant, and then I will do the\nsame back propagation.",
    "start": "3230011",
    "end": "3235470"
  },
  {
    "text": "It so happen that I easily\ncan do back propagation, not just for this matrix,\nbut also for this matrix.",
    "start": "3238700",
    "end": "3247040"
  },
  {
    "text": "And if I will do back propagation, I need the back propagation\nto do only one correction.",
    "start": "3247040",
    "end": "3254323"
  },
  {
    "text": "Instead of back propagation error where is y minus what you\nhave on the last layer,",
    "start": "3255600",
    "end": "3263890"
  },
  {
    "text": "and you have all l observations. You just have this matrix and you multiply",
    "start": "3264500",
    "end": "3271610"
  },
  {
    "text": "your vector of propagation on this matrix, you're correcting your propagation.",
    "start": "3273840",
    "end": "3278463"
  },
  {
    "text": "So that is what is back propagation about. You do for one step, it is OK, the same.",
    "start": "3281170",
    "end": "3289430"
  },
  {
    "text": "You have back propagation,\nit is border condition, in back propagation step you\nshould do some correction,",
    "start": "3289430",
    "end": "3297720"
  },
  {
    "text": "only one on the very last level,\nand then update your steps.",
    "start": "3297720",
    "end": "3302720"
  },
  {
    "text": "So I came to NSC and ask guys who have a neural network",
    "start": "3303030",
    "end": "3309690"
  },
  {
    "text": "to incorporate this,\njust this improvement,",
    "start": "3309690",
    "end": "3314690"
  },
  {
    "text": "small improvement this. I took just one invariant,\na very trivial invariant.",
    "start": "3315690",
    "end": "3324210"
  },
  {
    "text": "C of x equals one, c of x equals one, I will show you that it's\nnot so simple invariant,",
    "start": "3324266",
    "end": "3329970"
  },
  {
    "text": "we will discuss what\nit, what invariant does. And then, we set V equal\nI, we did not use V matrix.",
    "start": "3329970",
    "end": "3338620"
  },
  {
    "text": "Here it is just identity matrix. We used 1,000 examples,\n100 per class, batch six.",
    "start": "3339180",
    "end": "3346960"
  },
  {
    "text": "And this is this line is what does neural network,",
    "start": "3349990",
    "end": "3358950"
  },
  {
    "text": "deep neural network, they have\na good deep neural network.",
    "start": "3358950",
    "end": "3362442"
  },
  {
    "text": "And that's what this does\nimprove neural network. Instead of 3.1 they got 2.9.",
    "start": "3364360",
    "end": "3369493"
  },
  {
    "text": "OK, let's do just one, not very important cosine coefficient.",
    "start": "3373048",
    "end": "3380410"
  },
  {
    "text": "I have a picture of my, my digit.",
    "start": "3380410",
    "end": "3385410"
  },
  {
    "text": "I make cosine expansion, so I have Fourier coefficients",
    "start": "3386450",
    "end": "3393930"
  },
  {
    "text": "for one coefficient for here,\nand I will use this predicate",
    "start": "3394760",
    "end": "3399760"
  },
  {
    "text": "so I will use predicate\nwith this one coefficient. And again, you will see just one invariant",
    "start": "3402140",
    "end": "3408859"
  },
  {
    "text": "with stupid cosine makes improvement, OK? Then we decide, let's do more.",
    "start": "3408860",
    "end": "3415210"
  },
  {
    "text": "Let's do 16 coefficients of Fourier. Four from x one and four for x two.",
    "start": "3415210",
    "end": "3423753"
  },
  {
    "text": "And we got .6 bigger, but I can do whatever I want,\nI can do 1600 invariants.",
    "start": "3424980",
    "end": "3432433"
  },
  {
    "text": "And I can make, it's a\nlot of game can be played. But, let's also, but in neural net,",
    "start": "3436080",
    "end": "3445810"
  },
  {
    "text": "we use approximation of exact solution but it works.",
    "start": "3445810",
    "end": "3453743"
  },
  {
    "text": "The statistical part of\nlearning theory is complete. Why is it complete?",
    "start": "3457040",
    "end": "3463470"
  },
  {
    "text": "Because there exist only\ntwo ways for convergence in Hilbert space.",
    "start": "3463470",
    "end": "3469133"
  },
  {
    "text": "Convergence in functions,\nconvergence in functionals. There are no third way of convergence.",
    "start": "3470680",
    "end": "3478122"
  },
  {
    "text": "So, from a conceptual point of view, you can play one of two\ngame or two games together.",
    "start": "3479280",
    "end": "3484283"
  },
  {
    "text": "So why I call this complete, you cannot imagine something else.",
    "start": "3485630",
    "end": "3492580"
  },
  {
    "text": "If you would like to do something",
    "start": "3492580",
    "end": "3498353"
  },
  {
    "text": "what is improved learning,\nyou should ask yourself how to choose invariants,",
    "start": "3499550",
    "end": "3506418"
  },
  {
    "text": "and that's what I will talk about.",
    "start": "3506418",
    "end": "3508422"
  },
  {
    "text": "Invariant, it is something\nabout intelligence, when they talk about duck test,",
    "start": "3516099",
    "end": "3526050"
  },
  {
    "text": "I use looks like a\nduck, swims like a duck, quack like a duck, but I can\nsay play chess like a duck.",
    "start": "3527260",
    "end": "3534493"
  },
  {
    "text": "I can say whatever I want and it should be invariant equality,",
    "start": "3535880",
    "end": "3542483"
  },
  {
    "text": "or say singing can not be like a duck, OK? But, among all these stupid predicate,",
    "start": "3543840",
    "end": "3552053"
  },
  {
    "text": "there exists smart predicate, and subject of learning",
    "start": "3553150",
    "end": "3559070"
  },
  {
    "text": "and subject of intelligent learning, some have to pick up the smart invariants.",
    "start": "3559070",
    "end": "3564430"
  },
  {
    "text": "Let us discuss what is predicate.",
    "start": "3566250",
    "end": "3571500"
  },
  {
    "start": "3568000",
    "end": "4239000"
  },
  {
    "text": "I don't know, exactly, I think this is for many\nhundred years theory, I will show you that it is continuation",
    "start": "3571500",
    "end": "3578060"
  },
  {
    "text": "of major philosophy from Plato to Hegel to Wigner and so on, I will show you.",
    "start": "3578060",
    "end": "3585360"
  },
  {
    "text": "But it is in predicate, so my claim that when you're talking\nnot about imitation,",
    "start": "3585360",
    "end": "3593420"
  },
  {
    "text": "but what is essence of\nintelligence, essence in predicate.",
    "start": "3593420",
    "end": "3598420"
  },
  {
    "text": "Predicate is something extra. OK, I will show you later. Let's say predicate, that is",
    "start": "3599990",
    "end": "3608059"
  },
  {
    "text": "that invariant holds true, mathematically. So let's take f of x equals one.",
    "start": "3611130",
    "end": "3616883"
  },
  {
    "text": "What does this predicate? Expected number of element\nof class y equals one",
    "start": "3618340",
    "end": "3626750"
  },
  {
    "text": "computed using this predicate, equal to number of training\nexample of the first class.",
    "start": "3626750",
    "end": "3634857"
  },
  {
    "text": "When you will use this predicate, you will pick up function\nwhich will give you",
    "start": "3636262",
    "end": "3643940"
  },
  {
    "text": "on this training data, the number of represents",
    "start": "3643940",
    "end": "3649797"
  },
  {
    "text": "of the first class, exactly the same like in your training data.",
    "start": "3650910",
    "end": "3654933"
  },
  {
    "text": "And that is this predicate. And you saw how strong this\npredicate for neural net in terms of class of recognition",
    "start": "3660228",
    "end": "3666540"
  },
  {
    "text": "because they have something. Let's take another\nstupid predicate, just x.",
    "start": "3666540",
    "end": "3673033"
  },
  {
    "text": "It looks like a duck,\nit is center of mass.",
    "start": "3673033",
    "end": "3677063"
  },
  {
    "text": "I want the expected center of mass, expected to be the respect\nthe conditional probability",
    "start": "3678600",
    "end": "3684490"
  },
  {
    "text": "will be equal to average to\ncenter of mass which I see",
    "start": "3685619",
    "end": "3690619"
  },
  {
    "text": "on my training data.",
    "start": "3691340",
    "end": "3694143"
  },
  {
    "text": "That looks like a duck. But you can do smart looks like a duck.",
    "start": "3696630",
    "end": "3701693"
  },
  {
    "text": "So, I can consider x x transport",
    "start": "3703000",
    "end": "3707700"
  },
  {
    "text": "which makes matrix, it is correlation, covariation matrix, and I can see n squared",
    "start": "3709210",
    "end": "3717730"
  },
  {
    "text": "over two predicate of this type that's covariation which I will get",
    "start": "3717730",
    "end": "3724710"
  },
  {
    "text": "using this predicate using function which, sorry.",
    "start": "3725700",
    "end": "3734440"
  },
  {
    "text": "Predication which I will\nget with obtained solution will be the same like\ncovariation which I observed",
    "start": "3734760",
    "end": "3743220"
  },
  {
    "text": "on my training data. That means this predicate.",
    "start": "3743220",
    "end": "3746920"
  },
  {
    "text": "So, but, I sense that we should not\ngo for general predicate,",
    "start": "3750240",
    "end": "3757018"
  },
  {
    "text": "and we can imagine when your\npredicate, I am not so smart, that to construct very general predicate,",
    "start": "3757018",
    "end": "3763090"
  },
  {
    "text": "but let's do for 2D image predicate. Like u x one, x two, the\nfunction of two variables,",
    "start": "3763090",
    "end": "3772790"
  },
  {
    "text": "it is image of digits, say, in our case. And we have this function,\ny, this function, y.",
    "start": "3773280",
    "end": "3780349"
  },
  {
    "text": "And let's consider predicate\nlike I will take image, I will consider coefficients for Fourier,",
    "start": "3780350",
    "end": "3789100"
  },
  {
    "text": "it is my predicate. I want expected value with respect to this",
    "start": "3789100",
    "end": "3794500"
  },
  {
    "text": "over my predicate, will be the same like I show my training data.",
    "start": "3794500",
    "end": "3800243"
  },
  {
    "text": "I can consider convolution, because convolution neural\nnetwork is one predicate.",
    "start": "3803820",
    "end": "3810869"
  },
  {
    "text": "I will show you which is predicate.",
    "start": "3810870",
    "end": "3813447"
  },
  {
    "text": "And this is this convolution of point x y, x of different points.",
    "start": "3816056",
    "end": "3821453"
  },
  {
    "text": "You can use value, you\ncan use whatever you want, because whatever is\ncoming from inner product",
    "start": "3823720",
    "end": "3829849"
  },
  {
    "text": "it is you who decided what you should use.",
    "start": "3829850",
    "end": "3833743"
  },
  {
    "text": "And the understanding\nof image recognition,",
    "start": "3835150",
    "end": "3840150"
  },
  {
    "text": "it means understand which\npredicate involved in that. But also, the difference\nbetween predicate and invariant,",
    "start": "3840180",
    "end": "3849160"
  },
  {
    "text": "predicate is abstract concept, but invariant is from your training data,",
    "start": "3850190",
    "end": "3856503"
  },
  {
    "text": "it's what makes specific\nyour abstract concept. It's also general, but it is specific.",
    "start": "3857400",
    "end": "3863853"
  },
  {
    "text": "And that is, I want you to show",
    "start": "3868150",
    "end": "3873230"
  },
  {
    "text": "instruments for special predicates.",
    "start": "3875660",
    "end": "3877662"
  },
  {
    "text": "Let us consider vector x y, x j, just the digit recognition,",
    "start": "3881130",
    "end": "3886579"
  },
  {
    "text": "keep in mind, your digit recognition. And suppose x is your pixel space,",
    "start": "3886580",
    "end": "3892993"
  },
  {
    "text": "and you may clean your transformation, small linear transformation\nof your pixel space.",
    "start": "3894728",
    "end": "3900523"
  },
  {
    "text": "And if you have small linear\ntransformation of pixel space, you transform your picture.",
    "start": "3902950",
    "end": "3910360"
  },
  {
    "text": "But you can transform picture, you can also transform\nusing Lie derivative, I will show you what is that.",
    "start": "3910360",
    "end": "3916500"
  },
  {
    "text": "But, to see what is\nthat, I took this picture from paper by Simard et al.",
    "start": "3916500",
    "end": "3923172"
  },
  {
    "text": "Show you, this is\ntransformation, the first line.",
    "start": "3924570",
    "end": "3929563"
  },
  {
    "text": "In pixel space, you may\nclean your transformation. And here, you make the same transformation",
    "start": "3931100",
    "end": "3936743"
  },
  {
    "text": "as Lie derivative, this is Lie derivative, this black one.",
    "start": "3936743",
    "end": "3941813"
  },
  {
    "text": "It just computed Lie derivative, I will show you how to compute. And alpha is coefficient, and\nusing different coefficients,",
    "start": "3942660",
    "end": "3950310"
  },
  {
    "text": "a equals minus two, you\njust have this pair, a equals minus one, you can have this,",
    "start": "3950310",
    "end": "3958460"
  },
  {
    "text": "this, this, and all this stuff. So, you can create clone,",
    "start": "3958460",
    "end": "3966470"
  },
  {
    "text": "clones of digit two, which is transformed with\nrespect to Lie derivative.",
    "start": "3966470",
    "end": "3974160"
  },
  {
    "text": "You don't need to have a lot of data.",
    "start": "3974160",
    "end": "3978273"
  },
  {
    "text": "You need to have for digital recognition, you need to have predicate,\nand from any data",
    "start": "3979330",
    "end": "3987430"
  },
  {
    "text": "you can get this predicate. And OK, I will show you\ninvariant with this predicate.",
    "start": "3987430",
    "end": "3994635"
  },
  {
    "text": "And this is what is Lie derivative. It is horizontal translation, x, first coordinate, plus a,",
    "start": "3994635",
    "end": "4002950"
  },
  {
    "text": "you just move it in direction a x one. Then vertical transformation.",
    "start": "4002950",
    "end": "4009100"
  },
  {
    "text": "This is rotation, this\nstandard from geometry, for full rotation, for small\nrotation, you have that.",
    "start": "4009100",
    "end": "4017670"
  },
  {
    "text": "And this is d dx, this is d dx two and so on.",
    "start": "4017670",
    "end": "4024280"
  },
  {
    "text": "You have all this stuff here. And this is a big illustration\nfrom Patrick Simard.",
    "start": "4024280",
    "end": "4031363"
  },
  {
    "text": "Clones, you have this three and\nyou create all these clones.",
    "start": "4032390",
    "end": "4037390"
  },
  {
    "text": "Just you choose, one, two, three, four, five Lie derivatives, two\ndifferent coefficients,",
    "start": "4039340",
    "end": "4047500"
  },
  {
    "text": "and then you have all this stuff.",
    "start": "4047500",
    "end": "4049450"
  },
  {
    "text": "But this is smart guy, why you not taking, like predicate, just Lie derivative of it?",
    "start": "4055500",
    "end": "4062290"
  },
  {
    "text": "And we'll take invariant with\nrespect to Lie derivative, it is, I would like to learn\nif using statistical invariant",
    "start": "4063330",
    "end": "4072660"
  },
  {
    "text": "try to estimate such\nconditional probability,",
    "start": "4073150",
    "end": "4078150"
  },
  {
    "text": "which keep invariant with\nrespect to all derivatives.",
    "start": "4078650",
    "end": "4082547"
  },
  {
    "text": "But even more, it's\nagain from what was done. So suppose I have",
    "start": "4088620",
    "end": "4095140"
  },
  {
    "text": "this set of clones of my digit.",
    "start": "4095140",
    "end": "4099373"
  },
  {
    "text": "This is set of clones of another digit.",
    "start": "4100720",
    "end": "4105720"
  },
  {
    "text": "I call tangent distance\nthe closest element from these two clones, what that means.",
    "start": "4107090",
    "end": "4113620"
  },
  {
    "text": "I have two digits, they are\ndifferent, say, two, three. Then I massage them with\nlinear transformation",
    "start": "4113620",
    "end": "4120020"
  },
  {
    "text": "to make it as close as is possible, and measure closeness of them.",
    "start": "4120020",
    "end": "4125509"
  },
  {
    "text": "That's called Lie invariant. That's called tangent distance.",
    "start": "4125510",
    "end": "4131883"
  },
  {
    "text": "And now, I believe that\nthis is general concept of predicate symmetry.",
    "start": "4132870",
    "end": "4141563"
  },
  {
    "text": "When you have any picture, you can say what is measure of symmetry\nof these two pictures.",
    "start": "4143540",
    "end": "4150799"
  },
  {
    "text": "So for example, you have\nthree, what I can do, I have, this is my digit three,",
    "start": "4150800",
    "end": "4158063"
  },
  {
    "text": "I will have for horizontal symmetry, I will take first line\nhere, second line here,",
    "start": "4159030",
    "end": "4165690"
  },
  {
    "text": "last line here, then I\nwill do the following. I will make another digit. I will make last line the first line.",
    "start": "4165690",
    "end": "4174493"
  },
  {
    "text": "This line, the second line. So what I am doing.",
    "start": "4174493",
    "end": "4178012"
  },
  {
    "text": "I take three, so I leave it like vector like that.",
    "start": "4183220",
    "end": "4189779"
  },
  {
    "text": "Now I will do this. It is two different images.",
    "start": "4189780",
    "end": "4196413"
  },
  {
    "text": "And now, I will say, let me massage them, this tangent distance, these\ntwo different pictures,",
    "start": "4197380",
    "end": "4204360"
  },
  {
    "text": "how close they are. If they are very close, I can say this is coefficient of\nsymmetry of this three.",
    "start": "4204360",
    "end": "4213719"
  },
  {
    "text": "But I can say horizontal\nsymmetry, vertical symmetry, antisymmetry, what means antisymmetry?",
    "start": "4213720",
    "end": "4220980"
  },
  {
    "text": "Digit s, it has vertical antisymmetry.",
    "start": "4220980",
    "end": "4225980"
  },
  {
    "text": "I can have vertical symmetry, everything. So you can play many games with symmetry,",
    "start": "4227710",
    "end": "4234230"
  },
  {
    "text": "and this is just one predicate.",
    "start": "4234230",
    "end": "4236413"
  },
  {
    "start": "4239000",
    "end": "4570000"
  },
  {
    "text": "I have conclusion remarks. What we did, is that we can\nminimize this functional",
    "start": "4239583",
    "end": "4247260"
  },
  {
    "text": "which is slightly better\nthan, say, least square subject to this constraint\nwhich is serious,",
    "start": "4247260",
    "end": "4254810"
  },
  {
    "text": "because this constraint means\nadmissible set of functions",
    "start": "4254810",
    "end": "4259810"
  },
  {
    "text": "which are trying to construct being smart.",
    "start": "4260490",
    "end": "4264583"
  },
  {
    "text": "So I can consider, say,\nall continuous functions.",
    "start": "4265790",
    "end": "4270790"
  },
  {
    "text": "And then from these continuous functions, select by smart predicate,\nadmissible set of functions.",
    "start": "4271610",
    "end": "4278440"
  },
  {
    "text": "And I can do it, because\naccording to weak convergence,",
    "start": "4278897",
    "end": "4283897"
  },
  {
    "text": "any invariant take\nplace with any function, invariant take place with any predicate.",
    "start": "4284430",
    "end": "4291532"
  },
  {
    "text": "So, and also, this provide unique solution for reproducing kernel Hilbert space,",
    "start": "4295940",
    "end": "4302923"
  },
  {
    "text": "and approximation for neural\nnetwork, approximate solution.",
    "start": "4304076",
    "end": "4308037"
  },
  {
    "text": "But further progress goes\nbeyond statistical reasoning.",
    "start": "4309620",
    "end": "4314620"
  },
  {
    "text": "It goes in direction of\nsearching of predicate which forms basis for\nunderstanding of problems",
    "start": "4316130",
    "end": "4321740"
  },
  {
    "text": "existing in the world. And what means understanding? It means that, in say,",
    "start": "4321740",
    "end": "4330670"
  },
  {
    "text": "2D image recognition, there exists concept of symmetry, there exists concept of structure,",
    "start": "4330670",
    "end": "4338103"
  },
  {
    "text": "and if you will know these concepts, I believe that it's not a lot, I will show you why I say it is not a lot,",
    "start": "4338103",
    "end": "4345540"
  },
  {
    "text": "you will understand this problem.",
    "start": "4345540",
    "end": "4348280"
  },
  {
    "text": "And I think that this\nline, it's very old line. It start from Plato, what says Plato?",
    "start": "4351101",
    "end": "4358060"
  },
  {
    "text": "Plato says that there is a vault of ideas, and vault of things, and vault of ideas",
    "start": "4358060",
    "end": "4365130"
  },
  {
    "text": "make vault of things. But you see that I have ideas",
    "start": "4366623",
    "end": "4374433"
  },
  {
    "text": "which is predicate, which abstract can be applied\nto different situations,",
    "start": "4375650",
    "end": "4380193"
  },
  {
    "text": "but I have vault of things. But then, in 300 years ago,",
    "start": "4381370",
    "end": "4389380"
  },
  {
    "text": "it was classical German philosophy about that, what it means,\nideas, what means things.",
    "start": "4389380",
    "end": "4396667"
  },
  {
    "text": "And Hegel told, whatever\nis reasonable, it exists. It is exactly what we\nsaid about predicate.",
    "start": "4396667",
    "end": "4403850"
  },
  {
    "text": "And whatever exists, it is reasonable. So there is two connections.",
    "start": "4403850",
    "end": "4408933"
  },
  {
    "text": "But recently, 60 years ago, Wigner wrote an article",
    "start": "4410630",
    "end": "4415540"
  },
  {
    "text": "about unreasonable\neffectiveness of mathematics.",
    "start": "4416970",
    "end": "4421970"
  },
  {
    "text": "It just says that mathematics\nknows something about reality. If you would like to understand reality,",
    "start": "4422279",
    "end": "4428470"
  },
  {
    "text": "you should look in equation\nand you will see how it works. So predicate, it's abstract idea,",
    "start": "4428470",
    "end": "4435820"
  },
  {
    "text": "while invariants that are built using them form elements of solution.",
    "start": "4436680",
    "end": "4442873"
  },
  {
    "text": "These two concepts reflect\nessence of intelligence, not just its imitation which\nis in artificial intelligence.",
    "start": "4444570",
    "end": "4453532"
  },
  {
    "text": "But, that is subject\nwhich we should attack. And also, I have two more\nslides, one slide is challenge.",
    "start": "4454590",
    "end": "4464453"
  },
  {
    "text": "I know that people from deep network get .5% of error rate",
    "start": "4466320",
    "end": "4473040"
  },
  {
    "text": "using 60,000 observations. The challenge is, use 1% of this data",
    "start": "4473040",
    "end": "4479389"
  },
  {
    "text": "and get the same result. But even smart predicate and\nall these clones which exist,",
    "start": "4479390",
    "end": "4486350"
  },
  {
    "text": "I think that it is doable. And the very last slide.",
    "start": "4489340",
    "end": "4493963"
  },
  {
    "text": "In 1928, guy Valdimir Propp published book",
    "start": "4496501",
    "end": "4501501"
  },
  {
    "text": "\"Morphology of Folk Tale\" where\nhe describes 31 predicates",
    "start": "4501847",
    "end": "4506847"
  },
  {
    "text": "that allow to synthesize\nall Russian folk tales.",
    "start": "4508690",
    "end": "4512893"
  },
  {
    "text": "Later, his morphology, the 31 predicates,",
    "start": "4514320",
    "end": "4519320"
  },
  {
    "text": "was applied to literature, to theater, to film, to television,\nto television series,",
    "start": "4520040",
    "end": "4528143"
  },
  {
    "text": "to games, et cetera, and this 31 was enough.",
    "start": "4529040",
    "end": "4533323"
  },
  {
    "text": "And this I read from\nWikipedia, you can check it, with Wikipedia of the book.",
    "start": "4534510",
    "end": "4540530"
  },
  {
    "text": "Propp found 31 predicates which describe different actions of people in real world.",
    "start": "4540530",
    "end": "4546302"
  },
  {
    "text": "Probably there exist a\nsmall amount of predicates that describe 2D images.",
    "start": "4547240",
    "end": "4552097"
  },
  {
    "text": "And that is intelligence,\nthat is how to find them.",
    "start": "4553880",
    "end": "4558230"
  },
  {
    "text": "That what I believe\nshould be learning about. Thank you.",
    "start": "4559290",
    "end": "4565243"
  },
  {
    "text": "(audience applauds)",
    "start": "4565243",
    "end": "4568243"
  },
  {
    "start": "4570000",
    "end": "4638000"
  },
  {
    "text": "- [Host] I think we have\ntime for a few questions. - [Man in Audience] Hello,\nthank you, I have two questions.",
    "start": "4571136",
    "end": "4577929"
  },
  {
    "text": "First one is, do you know of any predicates that you recommend for",
    "start": "4577930",
    "end": "4583000"
  },
  {
    "text": "language classification\ntasks, specifically? And the second question is,\ndo you have any strategies",
    "start": "4583000",
    "end": "4588480"
  },
  {
    "text": "for hedging against over fitting? Like if you specify too many predicates, then you might be sort of--",
    "start": "4588480",
    "end": "4594797"
  },
  {
    "text": "- Sorry, I not hear you well, but second question is about over fitting? - [Man in Audience] Over fitting, yes.",
    "start": "4594797",
    "end": "4600350"
  },
  {
    "text": "- Yes, let me answer this. - [Man in Audience] Sure. - The more predicate, you have\nwhy over fitting can happen.",
    "start": "4600350",
    "end": "4607090"
  },
  {
    "text": "Because your set of function is big, and you have small amount of\ndata in selecting function.",
    "start": "4607090",
    "end": "4614100"
  },
  {
    "text": "So you can select whatever you want. But if you increase number of predicate,",
    "start": "4614100",
    "end": "4619343"
  },
  {
    "text": "you decrease set of function. So the more predicate, the\nless over fitting happened.",
    "start": "4620460",
    "end": "4628580"
  },
  {
    "text": "And if you will, a theory\nof mathematics says, that if you have infinite\nnumber of predicate,",
    "start": "4628618",
    "end": "4634150"
  },
  {
    "text": "you are left with one\nfunction, if you want. - [Host] He also asked\nabout natural language.",
    "start": "4634150",
    "end": "4640970"
  },
  {
    "start": "4638000",
    "end": "4761000"
  },
  {
    "text": "Recommendations for\npredicates for language, natural language\nprocessing, the Turing test,",
    "start": "4640970",
    "end": "4648010"
  },
  {
    "text": "any good predicates. - You know it is very complicated story,",
    "start": "4648010",
    "end": "4654920"
  },
  {
    "text": "natural language, I don't know.",
    "start": "4654920",
    "end": "4657130"
  },
  {
    "text": "- Questions?\n- You know, it is, whatever I am talking it\nis very trivial, simple.",
    "start": "4660496",
    "end": "4666111"
  },
  {
    "text": "Everyone familiar with 2D images.",
    "start": "4669330",
    "end": "4672913"
  },
  {
    "text": "And we can think, like\nthis guy Vladimir Propp, what is predicate in these images.",
    "start": "4674784",
    "end": "4680853"
  },
  {
    "text": "Can we formulate, if you're smart guy, say, couple of dozens, or\nmaybe one dozen predicate,",
    "start": "4682149",
    "end": "4688237"
  },
  {
    "text": "it should be enough.",
    "start": "4688237",
    "end": "4689713"
  },
  {
    "text": "- [Host] But language\nis harder than images. - Oh yeah, absolutely. (audience laughs)",
    "start": "4693380",
    "end": "4699800"
  },
  {
    "text": "Yeah, but don't do the\nimmediately hard problem. Try to--\n- Try?",
    "start": "4699800",
    "end": "4705449"
  },
  {
    "text": "- Yeah, I tried a very\nsimple, just step-by-step.",
    "start": "4705450",
    "end": "4709493"
  },
  {
    "text": "It so happens that is\nmain line of philosophy",
    "start": "4710530",
    "end": "4715530"
  },
  {
    "text": "from Plato to this guy",
    "start": "4718390",
    "end": "4723390"
  },
  {
    "text": "who says that ideas is not too much. There's not too many\nideas that are existing",
    "start": "4723470",
    "end": "4730060"
  },
  {
    "text": "in world of ideas. It could be like that.",
    "start": "4730060",
    "end": "4734003"
  },
  {
    "text": "- Vladimir, thank you so\nmuch for coming today, and please give him a big hand. (audience applauding)",
    "start": "4735400",
    "end": "4743587"
  }
]