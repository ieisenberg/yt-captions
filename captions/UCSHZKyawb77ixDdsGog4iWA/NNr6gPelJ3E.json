[
  {
    "start": "0",
    "end": "140000"
  },
  {
    "text": "- If we create general\nsuper intelligences, I don't see a good outcome\nlong term for humanity.",
    "start": "210",
    "end": "6720"
  },
  {
    "text": "So there is x-risk, existential\nrisk, everyone's dead. There is s-risk, suffering risks",
    "start": "6720",
    "end": "12809"
  },
  {
    "text": "where everyone wishes they were dead. We have also idea for i-risk, ikigai risks",
    "start": "12810",
    "end": "18119"
  },
  {
    "text": "where we lost our meaning. The systems can be more creative,\nthey can do all the jobs.",
    "start": "18120",
    "end": "24270"
  },
  {
    "text": "It's not obvious what you\nhave to contribute to a world where super intelligence exists.",
    "start": "24270",
    "end": "29730"
  },
  {
    "text": "Of course, you can have all\nthe variants you mentioned where we are safe, we are kept alive,",
    "start": "29730",
    "end": "35129"
  },
  {
    "text": "but we are not in control. We are not deciding anything.\nWe are like animals and zoo.",
    "start": "35130",
    "end": "40350"
  },
  {
    "text": "There is again, possibilities\nwe can come up with as very smart humans\nand then possibilities,",
    "start": "40350",
    "end": "47129"
  },
  {
    "text": "something a thousand times\nsmarter can come up with for reasons we cannot comprehend.",
    "start": "47130",
    "end": "52473"
  },
  {
    "text": "- The following is a conversation\nwith Roman Yampolskiy, an AI safety and security researcher",
    "start": "54720",
    "end": "60630"
  },
  {
    "text": "and author of a new book titled \"AI: Unexplainable,\nUnpredictable, Uncontrollable\".",
    "start": "60630",
    "end": "67220"
  },
  {
    "text": "He argues that there's almost 100% chance that AGI will eventually\ndestroy human civilization.",
    "start": "67980",
    "end": "74430"
  },
  {
    "text": "As an aside, lemme say\nthat I will have many, often technical conversations\non the topic of AI,",
    "start": "74430",
    "end": "81540"
  },
  {
    "text": "often with engineers building the state-of-the-art AI systems. I would say those folks\nput the infamous P doom",
    "start": "81540",
    "end": "88619"
  },
  {
    "text": "or the probability of\nAGI killing all humans at around one to 20%.",
    "start": "88620",
    "end": "94140"
  },
  {
    "text": "But it's also important\nto talk to folks who put that value at 70, 80, 90",
    "start": "94140",
    "end": "100200"
  },
  {
    "text": "and this in the case of Roman at 99.99 and many more nines percent.",
    "start": "100200",
    "end": "105482"
  },
  {
    "text": "I'm personally excited for the future and believe it will be a good one in part",
    "start": "106590",
    "end": "111780"
  },
  {
    "text": "because of the amazing\ntechnological innovation we humans create. But we must absolutely not\ndo so with blinders on,",
    "start": "111780",
    "end": "120660"
  },
  {
    "text": "ignoring the possible risks, including existential risks\nof those technologies.",
    "start": "120660",
    "end": "126183"
  },
  {
    "text": "That's what this conversation is about. This is the Lex Fridman podcast.",
    "start": "127050",
    "end": "132390"
  },
  {
    "text": "To support it, please\ncheck out our sponsors in the description. And now, dear friends,\nhere's Roman Yampolskiy.",
    "start": "132390",
    "end": "139383"
  },
  {
    "start": "140000",
    "end": "512000"
  },
  {
    "text": "What to you is the probability that super intelligent AI will destroy all human civilization?",
    "start": "140340",
    "end": "146310"
  },
  {
    "text": "- What's the timeframe? - Let's say 100 years, in\nthe next hundred years. - So the problem of controlling\nAGI or super intelligence",
    "start": "146310",
    "end": "154980"
  },
  {
    "text": "in my opinion is like\na problem of creating a perpetual safety machine,",
    "start": "154980",
    "end": "160650"
  },
  {
    "text": "by now with perpetual motion\nmachine, it's impossible. Yeah, we may succeed and do\ngood job with GPT5, 6, 7,",
    "start": "160650",
    "end": "169452"
  },
  {
    "text": "but they just keep improving, learning, eventually self-modifying,\ninteracting with the environment,",
    "start": "170310",
    "end": "178470"
  },
  {
    "text": "interacting with malevolent actors. The difference between\ncybersecurity, narrow AI safety",
    "start": "178470",
    "end": "186540"
  },
  {
    "text": "and safety for general AI,\nfor super intelligence, is that we don't get a second chance.",
    "start": "186540",
    "end": "192270"
  },
  {
    "text": "With cybersecurity,\nsomebody hacks your account, what's the big deal? You get a new password, new\ncredit card, you move on.",
    "start": "192270",
    "end": "197740"
  },
  {
    "text": "Here, if we're talking\nabout existential risks, you only get one chance. So you are really asking\nme what are the chances",
    "start": "198930",
    "end": "205890"
  },
  {
    "text": "that we'll create the\nmost complex software ever on the first try with zero bugs",
    "start": "205890",
    "end": "212190"
  },
  {
    "text": "and it'll continue have zero bugs for 100 years or more.",
    "start": "212190",
    "end": "217233"
  },
  {
    "text": "- So there is an incremental\nimprovement of systems",
    "start": "218580",
    "end": "223580"
  },
  {
    "text": "leading up to AGI. To you, it doesn't matter\nIf we can keep those safe, there's going to be one level of system",
    "start": "224310",
    "end": "232510"
  },
  {
    "text": "at which you cannot possibly control it. - I don't think we so far\nhave made any system safe.",
    "start": "233880",
    "end": "241980"
  },
  {
    "text": "At the level of capability they display, they already have made mistakes.",
    "start": "241980",
    "end": "247800"
  },
  {
    "text": "We had accidents, they've been jailbroken. I don't think there is a single\nlarge language model today,",
    "start": "247800",
    "end": "254790"
  },
  {
    "text": "which no one was successful\nat making do something developers didn't intend it to do.",
    "start": "254790",
    "end": "261120"
  },
  {
    "text": "- But there's a difference\nbetween getting it to do something unintended,\ngetting it to do something that's painful, costly, destructive,",
    "start": "261120",
    "end": "268410"
  },
  {
    "text": "and something that's destructive to the level of hurting billions of people or hundreds of millions of\npeople, billions of people",
    "start": "268410",
    "end": "275400"
  },
  {
    "text": "or the entirety of human civilization. That's a big leap. - Exactly, but the systems\nwe have today have capability",
    "start": "275400",
    "end": "282569"
  },
  {
    "text": "of causing X amount of damage. So when we fail, that's all we get. If we develop systems capable\nof impacting all of humanity,",
    "start": "282570",
    "end": "291660"
  },
  {
    "text": "all of universe, the\ndamage is proportionate. - What to you, are the possible ways that",
    "start": "291660",
    "end": "299530"
  },
  {
    "text": "such kind of mass murder\nof humans can happen? - It's always a wonderful question. So one of the chapters in my new book",
    "start": "300720",
    "end": "307560"
  },
  {
    "text": "is about unpredictability. I argue that we cannot predict what a smarter system will do.",
    "start": "307560",
    "end": "312990"
  },
  {
    "text": "So you're really not asking me how super intelligence will kill everyone. You're asking me how I would do it.",
    "start": "312990",
    "end": "319080"
  },
  {
    "text": "And I think it's not that interesting. I can tell you about a\nstandard, you know, nanotech, synthetic bioclear, super\nintelligence will come up",
    "start": "319080",
    "end": "326820"
  },
  {
    "text": "with something completely\nnew, completely super. We may not even recognize that",
    "start": "326820",
    "end": "333390"
  },
  {
    "text": "as a possible path to achieve that goal. - So there's like a\nunlimited level of creativity",
    "start": "333390",
    "end": "338729"
  },
  {
    "text": "in terms of how humans could be killed. But you know, we could still\ninvestigate possible ways",
    "start": "338730",
    "end": "347310"
  },
  {
    "text": "of doing it, not how to do it, but at the end, what is the\nmethodology that does it?",
    "start": "347310",
    "end": "354000"
  },
  {
    "text": "You know, shutting off the power and then humans start\nkilling each other maybe because the resources\nare really constrained?",
    "start": "354000",
    "end": "361192"
  },
  {
    "text": "And then there's the actual use of weapons like nuclear weapons or developing artificial\npathogens, viruses,",
    "start": "361192",
    "end": "368700"
  },
  {
    "text": "that kind of stuff. We could still kind of think through that and defend against it, right?",
    "start": "368700",
    "end": "374910"
  },
  {
    "text": "There's a ceiling to the\ncreativity of mass murder of humans here, right? The options are limited.",
    "start": "374910",
    "end": "381030"
  },
  {
    "text": "- They're limited by\nhow imaginative we are. If you are that much smarter,\nthat much more creative,",
    "start": "381030",
    "end": "386280"
  },
  {
    "text": "you're capable of thinking\nacross multiple domains, do novel research and physics and biology.",
    "start": "386280",
    "end": "391620"
  },
  {
    "text": "You may not be limited by those tools. If squirrels were planning to kill humans, they would have a set of\npossible ways of doing it,",
    "start": "391620",
    "end": "399419"
  },
  {
    "text": "but they would never consider\nthings we can come up. - So are you thinking about mass murder and destruction of human civilization",
    "start": "399420",
    "end": "405540"
  },
  {
    "text": "or are you thinking of with\nsquirrels, you put them in a zoo and they don't really\nknow they're in a zoo.",
    "start": "405540",
    "end": "410910"
  },
  {
    "text": "If we just look at the entire set of undesirable trajectories, majority of them are\nnot going to be death.",
    "start": "410910",
    "end": "417900"
  },
  {
    "text": "Most of them are going to be just like things like \"Brave New World\",",
    "start": "417900",
    "end": "423030"
  },
  {
    "text": "where, you know, the\nsquirrels are fed dopamine and they're all like doing\nsome kind of fun activity",
    "start": "423030",
    "end": "430110"
  },
  {
    "text": "and sort of the fire, the\nsoul of humanity is lost because of the drug that's fed to it.",
    "start": "430110",
    "end": "436500"
  },
  {
    "text": "Or like literally in a zoo. We're in a zoo, we're doing our thing, we're like playing a game of Sims",
    "start": "436500",
    "end": "443370"
  },
  {
    "text": "and the actual players playing\nthat game are AI systems. Those are all undesirable",
    "start": "443370",
    "end": "449039"
  },
  {
    "text": "because sort of the free will, the fire of human consciousness is dimmed",
    "start": "449040",
    "end": "454680"
  },
  {
    "text": "through that process, but it's not killing humans. So like are you again about that",
    "start": "454680",
    "end": "460260"
  },
  {
    "text": "or is the biggest concern literally the extinctions of humans?",
    "start": "460260",
    "end": "465300"
  },
  {
    "text": "- I think about a lot of things. So that is x-risk, existential\nrisk, everyone's dead.",
    "start": "465300",
    "end": "470850"
  },
  {
    "text": "There is s-risk, suffering risks, where everyone wishes they were dead. We have also idea for\ni-risk, ikigai risks,",
    "start": "470850",
    "end": "478470"
  },
  {
    "text": "where we lost our meaning. The systems can be more creative,\nthey can do all the jobs.",
    "start": "478470",
    "end": "484650"
  },
  {
    "text": "It's not obvious what you\nhave to contribute to a world where super intelligence exists.",
    "start": "484650",
    "end": "490080"
  },
  {
    "text": "Of course you can have all\nthe variants you mentioned where we are safe, we're kept alive,",
    "start": "490080",
    "end": "495480"
  },
  {
    "text": "but we are not in control,\nwe are not deciding anything. We're like animals in a zoo.",
    "start": "495480",
    "end": "499850"
  },
  {
    "text": "There is again possibilities\nwe can come up with as very smart humans.",
    "start": "500700",
    "end": "506100"
  },
  {
    "text": "And then possibilities, something a thousand times\nsmarter can come up with for reasons we cannot comprehend.",
    "start": "506100",
    "end": "513300"
  },
  {
    "start": "512000",
    "end": "1004000"
  },
  {
    "text": "- I would love to sort\nof dig into each of those x-risk, s-risk and i-risk. So can you like linger\non i-risk, what is that?",
    "start": "513300",
    "end": "522060"
  },
  {
    "text": "- So Japanese concept of ikigai, you find something which\nallows you to make money.",
    "start": "522060",
    "end": "528420"
  },
  {
    "text": "You are good at it and the\nsociety says we need it. So like you have this awesome\njob, you are podcaster",
    "start": "528420",
    "end": "535319"
  },
  {
    "text": "gives you a lot of meaning,\nyou have a good life. I assume you're happy.",
    "start": "535320",
    "end": "539990"
  },
  {
    "text": "That's what we want most\npeople to find, to have. For many intellectuals\nit is their occupation",
    "start": "541500",
    "end": "547890"
  },
  {
    "text": "which gives them a lot of meaning. I'm a researcher, philosopher, scholar.",
    "start": "547890",
    "end": "553560"
  },
  {
    "text": "That means something to me. In a world where an artist\nis not feeling appreciated",
    "start": "553560",
    "end": "559440"
  },
  {
    "text": "because his art is just not competitive with what is produced by machines",
    "start": "559440",
    "end": "564570"
  },
  {
    "text": "or a writer or scientist\nwill lose a lot of that.",
    "start": "564570",
    "end": "569570"
  },
  {
    "text": "And at the lower level we're talking about complete technological unemployment.",
    "start": "570000",
    "end": "575040"
  },
  {
    "text": "We're not losing 10% of\njobs, we're losing all jobs. What do people do with all that free time?",
    "start": "575040",
    "end": "580260"
  },
  {
    "text": "What happens when everything\nsociety is built on is completely modified in one generation.",
    "start": "580260",
    "end": "587820"
  },
  {
    "text": "It's not a slow process where\nwe get to kinda figure out how to live that new lifestyle.",
    "start": "587820",
    "end": "593730"
  },
  {
    "text": "But it's pretty quick - In that world, can't humans just do",
    "start": "593730",
    "end": "598920"
  },
  {
    "text": "what humans currently do with chess, play each other, have tournaments, even though AI systems are far\nsuperior this time in chess.",
    "start": "598920",
    "end": "607980"
  },
  {
    "text": "So we just create artificial games. Or for us they're real like the Olympics",
    "start": "607980",
    "end": "613230"
  },
  {
    "text": "and we do all kinds of\ndifferent competitions and have fun. Maximize the fun and let the\nAI focus on the productivity.",
    "start": "613230",
    "end": "622940"
  },
  {
    "text": "- It's an option. I have a paper where I try to solve the value alignment\nproblem for multiple agents",
    "start": "624630",
    "end": "630630"
  },
  {
    "text": "and the solution to avoid compromise is to give everyone a\npersonal virtual universe. You can do whatever\nyou want in that world.",
    "start": "630630",
    "end": "637890"
  },
  {
    "text": "You could be king, you could be slave, you decide what happens. So it's basically a glorified\nvideo game where you get",
    "start": "637890",
    "end": "643950"
  },
  {
    "text": "to enjoy yourself and someone\nelse takes care of your needs. And the substrate alignment",
    "start": "643950",
    "end": "650340"
  },
  {
    "text": "is the only thing we need to solve. We don't have to get 8 billion\nhumans to agree on anything.",
    "start": "650340",
    "end": "655353"
  },
  {
    "text": "- So, okay, so why is\nthat not a likely outcome? Why can't AI systems\ncreate video games for us",
    "start": "656232",
    "end": "663779"
  },
  {
    "text": "to lose ourselves in, each with an individual\nvideo game universe? - Some people say that's what\nhappened, we in a simulation.",
    "start": "663780",
    "end": "672000"
  },
  {
    "text": "- And we're playing that video game and maybe we're creating\nartificial threats",
    "start": "672000",
    "end": "678710"
  },
  {
    "text": "for ourselves to be scared about 'cause fear is really exciting. It allows us to play the\nvideo game more vigorously.",
    "start": "679050",
    "end": "685980"
  },
  {
    "text": "- And some people choose to\nplay on a more difficult level with more constraints. Some say \"Okay, I'm just\ngonna enjoy the game,",
    "start": "685980",
    "end": "692610"
  },
  {
    "text": "high privilege level,\" absolutely. - So okay, what was that paper on multi-agent value alignment,",
    "start": "692610",
    "end": "698699"
  },
  {
    "text": "- Personal universes, personal universes. - So that's one of the possible outcomes.",
    "start": "698700",
    "end": "704730"
  },
  {
    "text": "But what in general is\nthe idea of the paper? So it's looking at multiple\nagents, they're human, AI,",
    "start": "704730",
    "end": "710400"
  },
  {
    "text": "like a hybrid system,\nwhether it's humans and AIs? Or is looking at humans or\njust intelligent agents?",
    "start": "710400",
    "end": "715890"
  },
  {
    "text": "- In order to solve\nvalue alignment, problem, trying to formalize it a little better. Usually we're talking about\ngetting AIs to do what we want,",
    "start": "715890",
    "end": "724740"
  },
  {
    "text": "which is not well defined. Are we're talking about\ncreator of a system, owner of that AI, humanity as a whole?",
    "start": "724740",
    "end": "732060"
  },
  {
    "text": "But we don't agree on much. There is no universally accepted ethics, morals across cultures, religions.",
    "start": "732060",
    "end": "739410"
  },
  {
    "text": "People have individually\nvery different preferences politically and such. So even if we somehow managed\nall the other aspects of it,",
    "start": "739410",
    "end": "747000"
  },
  {
    "text": "programming those fuzzy concepts in, getting AI to follow them closely, we don't agree on what to program in.",
    "start": "747000",
    "end": "753360"
  },
  {
    "text": "So my solution was, okay, we don't have to compromise on room temperature. You have your universe, I\nhave mine, whatever you want.",
    "start": "753360",
    "end": "760050"
  },
  {
    "text": "And if you like me you can\ninvite me to visit your universe. We don't have to be independent,",
    "start": "760050",
    "end": "765330"
  },
  {
    "text": "but the point is you can be. And virtual reality is\ngetting pretty good, it's gonna hit a point where\nyou can't tell the difference",
    "start": "765330",
    "end": "771959"
  },
  {
    "text": "and if you can't tell if it's real or not, what's the difference? - So basically give up on value alignment.",
    "start": "771960",
    "end": "778529"
  },
  {
    "text": "Create like the multiverse theory. Just create an entire universe\nfor you with your values.",
    "start": "778530",
    "end": "784680"
  },
  {
    "text": "- You still have to align\nwith that individual. They have to be happy in that simulation. But it's a much easier problem to align",
    "start": "784680",
    "end": "791519"
  },
  {
    "text": "with one agent versus 8 billion\nagents plus animals, aliens. - So you convert the multi-agent problem",
    "start": "791520",
    "end": "798090"
  },
  {
    "text": "into a single agent problem basically. - I'm trying to do that basically, yeah.",
    "start": "798090",
    "end": "804000"
  },
  {
    "text": "- So okay that's giving up on\nthe value alignment problem.",
    "start": "804000",
    "end": "809000"
  },
  {
    "text": "Well is there any way to solve\nthe value alignment problem where there's a bunch of\nhumans, multiple humans,",
    "start": "809910",
    "end": "816660"
  },
  {
    "text": "tens of humans or 8 billion humans that have very different set of values?",
    "start": "816660",
    "end": "821879"
  },
  {
    "text": "- It seems contradictory. I haven't seen anyone\nexplain what it means outside of kinda words which\npack a lot, make it good,",
    "start": "821880",
    "end": "831320"
  },
  {
    "text": "make it desirable, make it\nsomething they don't regret. But how do you specifically\nformalize those notions?",
    "start": "831660",
    "end": "838380"
  },
  {
    "text": "How do you program them in? I haven't seen anyone make\nprogress on that so far. - But isn't that the\nwhole optimization journey",
    "start": "838380",
    "end": "846060"
  },
  {
    "text": "that we're doing as a human civilization? We're looking at geopolitics, nations are in a state of\nanarchy with each other.",
    "start": "846060",
    "end": "854160"
  },
  {
    "text": "They start wars, there's conflict and oftentimes they have\na very different views",
    "start": "854160",
    "end": "861000"
  },
  {
    "text": "of what is good and what is evil. Isn't that what we're\ntrying to figure out? Just together trying to\nconverge towards that.",
    "start": "861000",
    "end": "868320"
  },
  {
    "text": "So we're essentially trying to solve the value alignment problem with humans. - Right, but the examples\nyou gave, some of them are",
    "start": "868320",
    "end": "874830"
  },
  {
    "text": "for example two different religions saying this is our holy side and we are not willing to\ncompromise it in any way.",
    "start": "874830",
    "end": "881850"
  },
  {
    "text": "If you can make two holy\nsites in virtual worlds, you solve the problem. But if you only have\none, it's not divisible.",
    "start": "881850",
    "end": "888690"
  },
  {
    "text": "You're kinda stuck there. - But what if we want be\nat tension with each other and through that tension\nwe understand ourselves",
    "start": "888690",
    "end": "896910"
  },
  {
    "text": "and we understand the world. So that's the intellectual\njourney we're on as a human civilization\nis we create intellectual",
    "start": "896910",
    "end": "905850"
  },
  {
    "text": "and physical conflict and\nthrough that figure stuff out? - If we go back to that idea of simulation and this is a entertainment\nkind of giving meaning to us,",
    "start": "905850",
    "end": "914880"
  },
  {
    "text": "the question is how much suffering is reasonable for a video game? So yeah, I don't mind,\nyou know, a video game",
    "start": "914880",
    "end": "920579"
  },
  {
    "text": "where I get haptic feedback\nthat is a little bit of shaking, maybe I'm a little scared.",
    "start": "920580",
    "end": "925590"
  },
  {
    "text": "I don't want a game where like\nkids are tortured literally, that seems unethical at\nleast by our human standards.",
    "start": "925590",
    "end": "934950"
  },
  {
    "text": "- Are you suggesting it's\npossible to remove suffering if we're looking at human civilization as an optimization problem?",
    "start": "934950",
    "end": "940920"
  },
  {
    "text": "- So we know there are some humans who, because of a mutation, don't\nexperience physical pain.",
    "start": "940920",
    "end": "947400"
  },
  {
    "text": "So at least physical pain can be mutated out, re-engineered out.",
    "start": "947400",
    "end": "953370"
  },
  {
    "text": "Suffering in terms of meaning, like you burn the only copy\nof my book is a little harder.",
    "start": "953370",
    "end": "958980"
  },
  {
    "text": "But even there you can manipulate\nyour hedonic set point, you can change defaults, you can reset.",
    "start": "958980",
    "end": "965880"
  },
  {
    "text": "Problem with that is if you start messing with your reward channel,\nyou start wireheading",
    "start": "965880",
    "end": "971190"
  },
  {
    "text": "and end up blissing out a little too much. - Well that's the question.",
    "start": "971190",
    "end": "977310"
  },
  {
    "text": "Would you really want to live in a world where there's no suffering? It's a dark question,",
    "start": "977310",
    "end": "982797"
  },
  {
    "text": "but is there some level of\nsuffering that reminds us",
    "start": "982797",
    "end": "987797"
  },
  {
    "text": "of what this is all for? - I think we need that, but I would change the overall range.",
    "start": "988020",
    "end": "993750"
  },
  {
    "text": "So right now it's negative infinity to kind of positive infinity\npain, pleasure access. I would make it like\nzero to positive infinity",
    "start": "993750",
    "end": "1000800"
  },
  {
    "text": "and being unhappy is\nlike I'm close to zero. - Okay, so what's the s-risk?",
    "start": "1000800",
    "end": "1006740"
  },
  {
    "start": "1004000",
    "end": "1219000"
  },
  {
    "text": "What are the possible things that you're imagining with s-risk? So mass suffering of humans,",
    "start": "1006740",
    "end": "1012290"
  },
  {
    "text": "what are we talking about\nthere caused by AGI? - So there are many malevolent actors,",
    "start": "1012290",
    "end": "1018260"
  },
  {
    "text": "we can talk about psychopaths, crazies, hackers, doomsday cults. We know from history they\ntried killing everyone.",
    "start": "1018260",
    "end": "1025790"
  },
  {
    "text": "They tried on purpose to cause maximum amount\nof damage, terrorism. What if someone malevolent\nwants on-purpose",
    "start": "1025790",
    "end": "1033110"
  },
  {
    "text": "to torture all humans as long as possible? You solve aging. So now you have functional immortality",
    "start": "1033110",
    "end": "1041030"
  },
  {
    "text": "and you just try to be\nas creative as you can. - Do you think there is\nactually people in human history",
    "start": "1041030",
    "end": "1047089"
  },
  {
    "text": "that try to literally\nmaximize human suffering. And just studying people who\nhave done evil in the world,",
    "start": "1047090",
    "end": "1054110"
  },
  {
    "text": "it seems that they think\nthat they're doing good and it doesn't seem like they're trying to maximize suffering.",
    "start": "1054110",
    "end": "1060470"
  },
  {
    "text": "They just cause a lot of suffering as a side effect of doing\nwhat they think is good.",
    "start": "1060470",
    "end": "1067310"
  },
  {
    "text": "- So there are different\nmalevolent agents. Some may just gaining personal benefit",
    "start": "1067310",
    "end": "1072920"
  },
  {
    "text": "and sacrificing others to that cause. Others, we know for a\nfact are trying to kill",
    "start": "1072920",
    "end": "1078170"
  },
  {
    "text": "as many people as possible. And we look at recent school shootings, if they had more capable weapons,",
    "start": "1078170",
    "end": "1084140"
  },
  {
    "text": "they would take out not dozens but thousands, millions, billions.",
    "start": "1084140",
    "end": "1088342"
  },
  {
    "text": "- Well we don't know that, but that is a terrifying possibility",
    "start": "1094010",
    "end": "1098220"
  },
  {
    "text": "and we don't wanna find out. Like if terrorists had a access to nuclear weapons, how far would they go?",
    "start": "1099350",
    "end": "1106493"
  },
  {
    "text": "Is there a limit to what\nthey're willing to do?",
    "start": "1107330",
    "end": "1111443"
  },
  {
    "text": "And your sense is there's\nsome malevolent actors where there's no limit. - There is mental diseases",
    "start": "1113270",
    "end": "1120290"
  },
  {
    "text": "where people don't have empathy, don't have this human quality",
    "start": "1120290",
    "end": "1127580"
  },
  {
    "text": "of understanding suffering in others. - And then there's also a set of beliefs where you think you're doing good",
    "start": "1127580",
    "end": "1133070"
  },
  {
    "text": "by killing a lot of humans. - Again, I would like to assume that normal people never think like that.",
    "start": "1134390",
    "end": "1140420"
  },
  {
    "text": "It's always some sort of\npsychopaths, but yeah. - And to you AGI systems can carry that",
    "start": "1140420",
    "end": "1147620"
  },
  {
    "text": "and be more competent at executing that. - They can certainly be more creative,",
    "start": "1147620",
    "end": "1153559"
  },
  {
    "text": "they can understand human biology better, understand our molecular structure genome.",
    "start": "1153560",
    "end": "1159620"
  },
  {
    "text": "Again, a lot of times torture\nends then individual dies.",
    "start": "1159620",
    "end": "1164620"
  },
  {
    "text": "That limit can be removed as well. - So if we're actually\nlooking at x-risk and s-risk",
    "start": "1166400",
    "end": "1171679"
  },
  {
    "text": "as the systems get more\nand more intelligent, don't you think it's possible to anticipate the ways it can do it",
    "start": "1171680",
    "end": "1178880"
  },
  {
    "text": "and defend against it, like\nwe do with the cybersecurity, what we do with security systems. - Right, we can definitely\nkeep up for a while.",
    "start": "1178880",
    "end": "1186289"
  },
  {
    "text": "I'm saying you cannot do it indefinitely. At some point the\ncognitive gap is too big.",
    "start": "1186290",
    "end": "1192440"
  },
  {
    "text": "The surface you have\nto defend is infinite.",
    "start": "1192440",
    "end": "1197440"
  },
  {
    "text": "But attackers only need\nto find one exploit. - So to you eventually this\nis we're heading off a cliff.",
    "start": "1197780",
    "end": "1205340"
  },
  {
    "text": "- If we create general\nsuper intelligences, I don't see a good outcome\nlong term for humanity.",
    "start": "1205340",
    "end": "1211610"
  },
  {
    "text": "The only way to win this\ngame is not to play it. - Okay, well, we'll talk\nabout possible solutions",
    "start": "1211610",
    "end": "1216860"
  },
  {
    "text": "and what not playing it means, but what are the possible\ntimelines here to you?",
    "start": "1216860",
    "end": "1222230"
  },
  {
    "start": "1219000",
    "end": "1491000"
  },
  {
    "text": "What are we talking about? We're talking about a set of\nyears, decades, centuries. What do you think?\n- I don't know for sure.",
    "start": "1222230",
    "end": "1228980"
  },
  {
    "text": "The prediction markets right\nnow are saying 2026 for AGI. I heard the same thing from\nCEO of Anthropic, DeepMind.",
    "start": "1228980",
    "end": "1237020"
  },
  {
    "text": "So maybe we are two years\naway, which seems very soon",
    "start": "1237020",
    "end": "1241320"
  },
  {
    "text": "given we don't have a working\nsafety mechanism in place or even a prototype for one. And there are people trying\nto accelerate those timelines",
    "start": "1242540",
    "end": "1249440"
  },
  {
    "text": "because they feel we're not\ngetting there quick enough. - Well what do you think\nthey mean when they say AGI?",
    "start": "1249440",
    "end": "1255290"
  },
  {
    "text": "- So the definitions we used to have and people are modifying\nthem a little bit lately,",
    "start": "1255290",
    "end": "1260330"
  },
  {
    "text": "artificial general intelligence was a system capable of\nperforming in any domain a human could perform.",
    "start": "1260330",
    "end": "1267140"
  },
  {
    "text": "So kind of you creating this\naverage artificial person, they can do cognitive\nlabor, physical labor",
    "start": "1267140",
    "end": "1272960"
  },
  {
    "text": "where you can get another human to do it. Super intelligence was defined as a system which is superior to all\nhumans in all domains.",
    "start": "1272960",
    "end": "1280669"
  },
  {
    "text": "Now people are starting to refer to AGI as if it's super intelligence. I made a post recently where\nI argued for me at least",
    "start": "1280670",
    "end": "1288350"
  },
  {
    "text": "if you average out over\nall the common human tasks, those systems are already\nsmarter than an average human.",
    "start": "1288350",
    "end": "1295940"
  },
  {
    "text": "So under that definition we have it. Shane Lag has this definition\nof we're you're trying",
    "start": "1295940",
    "end": "1301730"
  },
  {
    "text": "to win in all domains. That's what intelligence is. Now are they smarter\nthan elite individuals",
    "start": "1301730",
    "end": "1307940"
  },
  {
    "text": "in certain domains? Of course not. They're not there yet. But the progress is exponential.",
    "start": "1307940",
    "end": "1314510"
  },
  {
    "text": "- See I'm much more concerned\nabout social engineering. So to me AI's ability to do\nsomething in the physical world",
    "start": "1314510",
    "end": "1322810"
  },
  {
    "text": "like the lowest hanging fruit, the easiest set of methods",
    "start": "1324170",
    "end": "1329690"
  },
  {
    "text": "is by just getting humans to do it. It's going to be much harder\nto be the kind of viruses",
    "start": "1329690",
    "end": "1338230"
  },
  {
    "text": "to take over the minds of robots that where the robots are\nexecuting the commands. It just seems like humans,\nsocial engineering of humans",
    "start": "1338230",
    "end": "1345710"
  },
  {
    "text": "is much more likely. - That will be enough to\nbootstrap the whole process.",
    "start": "1345710",
    "end": "1349860"
  },
  {
    "text": "- Okay, just to linger on the term AGI, what's to you is the\ndifference between AGI",
    "start": "1351170",
    "end": "1356357"
  },
  {
    "text": "and human level intelligence? - Human level is general in the domain of expertise of humans.",
    "start": "1356357",
    "end": "1363470"
  },
  {
    "text": "We know how to do human things. I don't speak dog language. I should be able to pick it up",
    "start": "1363470",
    "end": "1368600"
  },
  {
    "text": "if I'm a general intelligence. It's kind of inferior animal, I should be able to learn\nthat skill, but I can't.",
    "start": "1368600",
    "end": "1374840"
  },
  {
    "text": "A general intelligence, truly universal general\nintelligence should be able to do things like that humans cannot do.",
    "start": "1374840",
    "end": "1380990"
  },
  {
    "text": "- To be able to talk\nto animals for example. - To solve pattern recognition\nproblems of that type, to have a similar things outside of",
    "start": "1380990",
    "end": "1390320"
  },
  {
    "text": "our domain of expertise because it's just not\nthe world will live in.",
    "start": "1390320",
    "end": "1395750"
  },
  {
    "text": "- If we just look at the space of cognitive abilities we have,",
    "start": "1395750",
    "end": "1401253"
  },
  {
    "text": "I just would love to\nunderstand what the limits are beyond which an AGI system can reach.",
    "start": "1401253",
    "end": "1406520"
  },
  {
    "text": "Like what does that look like? What about actual mathematical thinking",
    "start": "1406520",
    "end": "1412429"
  },
  {
    "text": "or scientific innovation? That kind of stuff.",
    "start": "1412430",
    "end": "1417650"
  },
  {
    "text": "- We know calculators\nare smarter than humans in that narrow domain of addition.",
    "start": "1417650",
    "end": "1423440"
  },
  {
    "text": "- But is it humans plus tools versus AGI or just raw human intelligence.",
    "start": "1423440",
    "end": "1431810"
  },
  {
    "text": "'Cause humans create tools and with the tools they\nbecome more intelligent. So like there's a gray area there, what it means to be human",
    "start": "1431810",
    "end": "1438559"
  },
  {
    "text": "when we're measuring their intelligence. - So when I think about\nit, I usually think human with like a paper and a pencil. Not human with internet\nand other AI helping.",
    "start": "1438560",
    "end": "1447290"
  },
  {
    "text": "- But is that a fair\nway to think about it? 'cause isn't there another definition of human level intelligence that includes the tools\nthat humans create.",
    "start": "1447290",
    "end": "1454279"
  },
  {
    "text": "- But we create AI so at any point you'll still\njust add super intelligence to human capability.",
    "start": "1454280",
    "end": "1459380"
  },
  {
    "text": "That seems like cheating - No controllable tools. There is an implied\nleap that you're making",
    "start": "1459380",
    "end": "1466950"
  },
  {
    "text": "when AGI goes from tool to a entity that can make its own decisions.",
    "start": "1468214",
    "end": "1474710"
  },
  {
    "text": "So if we define human level intelligence as everything a human can do\nwith fully controllable tools,",
    "start": "1474710",
    "end": "1481100"
  },
  {
    "text": "- It seems like a hybrid of some kind. You're now doing brain\ncomputer interfaces, you're connecting it to maybe narrow AI.",
    "start": "1481100",
    "end": "1487730"
  },
  {
    "text": "It definitely increases our capabilities. - So what's a good test to you",
    "start": "1487730",
    "end": "1494970"
  },
  {
    "start": "1491000",
    "end": "1814000"
  },
  {
    "text": "that measures whether an\nartificial intelligence system has reached human level\nintelligence and whats a good test",
    "start": "1496250",
    "end": "1503570"
  },
  {
    "text": "where it has superseded\nhuman level intelligence to reach that land of AGI?",
    "start": "1503570",
    "end": "1509840"
  },
  {
    "text": "- I'm old fashioned, I like Turing tests. Yeah, I have a paper where I\nequate passing Turing tests",
    "start": "1509840",
    "end": "1515120"
  },
  {
    "text": "to solving AI complete problems because you can encode any\nquestions about any domain",
    "start": "1515120",
    "end": "1520159"
  },
  {
    "text": "into the Turing test. You don't have to talk\nabout how was your day, you can ask anything.",
    "start": "1520160",
    "end": "1525680"
  },
  {
    "text": "And so the system has to\nbe as smart as a human to pass it, in a true sense.",
    "start": "1525680",
    "end": "1530810"
  },
  {
    "text": "- But then you would extend that to maybe a very long conversation. I think the Alexa Prize was doing that",
    "start": "1530810",
    "end": "1537600"
  },
  {
    "text": "basically can you do a 20\nminute, 30 minute conversation with an AI system? - It has to be long enough\nto where you can make",
    "start": "1538640",
    "end": "1546860"
  },
  {
    "text": "some meaningful decisions\nabout capabilities, absolutely. You can brute force very\nshort conversations.",
    "start": "1546860",
    "end": "1553190"
  },
  {
    "text": "- So like literally what\ndoes that look like? Can we construct formally a\nkind of test that tests for AGI?",
    "start": "1553190",
    "end": "1561640"
  },
  {
    "text": "- For AGI. It has to be there, I cannot give it a task\nI can give to a human",
    "start": "1563987",
    "end": "1570200"
  },
  {
    "text": "and it cannot do it if a human can. For super intelligent, it'll\nbe superior on all such tasks,",
    "start": "1570200",
    "end": "1576740"
  },
  {
    "text": "not just average performance. So like go learn to drive car, go speak Chinese, play\nguitar, okay, great.",
    "start": "1576740",
    "end": "1582830"
  },
  {
    "text": "- I guess the follow on\nquestion, is there a test for the kind of AGI that would be",
    "start": "1582830",
    "end": "1590520"
  },
  {
    "text": "susceptible to lead to s-risk or x-risk, susceptible to destroy human civilization?",
    "start": "1591680",
    "end": "1599240"
  },
  {
    "text": "Like is there a test for that? - You can develop a test\nwhich will give you positives,",
    "start": "1599240",
    "end": "1604610"
  },
  {
    "text": "if it lies to you or has those ideas. You cannot develop a test\nwhich rules them out. There is always possibility",
    "start": "1604610",
    "end": "1610809"
  },
  {
    "text": "of what bostrom calls a treacherous turn, where later on a system decides for game theoretical\nreasons, economic reasons",
    "start": "1610810",
    "end": "1618530"
  },
  {
    "text": "to change its behavior. And we see the same with\nhumans. It's not unique to AI.",
    "start": "1618530",
    "end": "1624740"
  },
  {
    "text": "For millennia we try\ndeveloping morals, ethics, religions, lie detector tests",
    "start": "1624740",
    "end": "1630620"
  },
  {
    "text": "and then employees betray the employers, spouses betray family. It's a pretty standard thing",
    "start": "1630620",
    "end": "1637520"
  },
  {
    "text": "intelligent agents sometimes do. - So is it possible to detect when a AI system\nis lying or deceiving you?",
    "start": "1637520",
    "end": "1644780"
  },
  {
    "text": "- If you know the truth and it tells you something\nfalse, you can detect that. But you cannot know in\ngeneral every single time.",
    "start": "1644780",
    "end": "1653660"
  },
  {
    "text": "And again, the system you're\ntesting today may not be lying. The system you're testing today\nmay know you are testing it",
    "start": "1653660",
    "end": "1660769"
  },
  {
    "text": "and so behaving and later on after it interacts with the environment,",
    "start": "1660770",
    "end": "1667040"
  },
  {
    "text": "interacts with other systems,\nmalevolent agents learns more. It may start doing those things.",
    "start": "1667040",
    "end": "1673250"
  },
  {
    "text": "- So do you think it's\npossible to develop a system where the creators of the\nsystem, the developers, the programmers don't know\nthat it's deceiving them?",
    "start": "1673250",
    "end": "1682840"
  },
  {
    "text": "- So systems today don't\nhave long-term planning, that is not out. They can lie today if it optimizes,",
    "start": "1683330",
    "end": "1691280"
  },
  {
    "text": "helps them optimize their reward. If they realize, okay, this\nhuman will be very happy",
    "start": "1691280",
    "end": "1696590"
  },
  {
    "text": "if I tell them the following, they will do it if it\nbrings them more points.",
    "start": "1696590",
    "end": "1701813"
  },
  {
    "text": "And they don't have to\nkinda keep track of it, it's just the right answer to this problem every single time.",
    "start": "1702740",
    "end": "1710509"
  },
  {
    "text": "- At which point is somebody\ncreating that intentionally, not unintentionally, intentionally\ncreating an AI system",
    "start": "1710510",
    "end": "1717590"
  },
  {
    "text": "that's doing long-term planning\nwith an objective function that's defined by the AI\nsystem, not by a human.",
    "start": "1717590",
    "end": "1724190"
  },
  {
    "text": "- Well some people think\nthat if they're that smart, they always good. They really do believe that,",
    "start": "1724190",
    "end": "1729860"
  },
  {
    "text": "it just benevolence from intelligence. So they'll always want what's best for us.",
    "start": "1729860",
    "end": "1734960"
  },
  {
    "text": "Some people think that they will be able to detect problem behaviors",
    "start": "1734960",
    "end": "1740840"
  },
  {
    "text": "and correct them at the\ntime when we get there. I don't think it's a good\nidea. I am strongly against it.",
    "start": "1740840",
    "end": "1747740"
  },
  {
    "text": "But yeah, there are quite\na few people who in general are so optimistic about this technology.",
    "start": "1747740",
    "end": "1753710"
  },
  {
    "text": "It could do no wrong. They want it developed\nas soon as possible, as capable as possible.",
    "start": "1753710",
    "end": "1759710"
  },
  {
    "text": "- So there's going to be people who believe the more intelligent\nit is, the more benevolent",
    "start": "1759710",
    "end": "1765440"
  },
  {
    "text": "and so therefore it should be the one that defines the objective\nfunction that it's optimizing when it's doing long-term planning.",
    "start": "1765440",
    "end": "1771320"
  },
  {
    "text": "- There are even people who say, okay, what's so special about humans, right? We remove the gender bias.\nWe are removing race bias.",
    "start": "1771320",
    "end": "1779600"
  },
  {
    "text": "Why is this pro-human bias?\nWe are polluting the planet. We are, as you said, you\nknow, fight a lot of war,",
    "start": "1779600",
    "end": "1785029"
  },
  {
    "text": "kind of violent, maybe it's better if\nit's super intelligent, perfect society comes and replaces us.",
    "start": "1785030",
    "end": "1792500"
  },
  {
    "text": "It's normal stage in the\nevolution of our species. - Yeah, so somebody says,\nlet's develop an AI system",
    "start": "1792500",
    "end": "1800720"
  },
  {
    "text": "that removes the violent\nhumans from the world. And then it turns out that all\nhumans have violence in them",
    "start": "1800720",
    "end": "1807559"
  },
  {
    "text": "or the capacity for violence and therefore all humans are removed. Yeah, yeah, yeah.",
    "start": "1807560",
    "end": "1813563"
  },
  {
    "start": "1814000",
    "end": "2586000"
  },
  {
    "text": "Let me ask about Yann LeCun, he's somebody who we've\nhad a few exchanges with",
    "start": "1814670",
    "end": "1821220"
  },
  {
    "text": "and he's somebody who actively pushes back against this view that AI is going to lead to destruction\nof human civilization,",
    "start": "1823160",
    "end": "1831170"
  },
  {
    "text": "also known as AI doomism. So in one example that he tweeted,",
    "start": "1831170",
    "end": "1840550"
  },
  {
    "text": "he said, \"I do acknowledge risks, but,\" two points, \"One, open\nresearch and open source",
    "start": "1840650",
    "end": "1846527"
  },
  {
    "text": "are the best ways to understand\nand mitigate the risks. And two, AI is not\nsomething that just happens.",
    "start": "1846527",
    "end": "1853130"
  },
  {
    "text": "We build it, we have\nagency in what it becomes. Hence we control the risks.\"",
    "start": "1853130",
    "end": "1859400"
  },
  {
    "text": "We meaning humans. \"It's not some sort of natural phenomena that we have no control over.\"",
    "start": "1859400",
    "end": "1865070"
  },
  {
    "text": "So can you make the case that he's right and can you try to make\nthe case that he's wrong?",
    "start": "1865070",
    "end": "1870410"
  },
  {
    "text": "- I cannot make a case that he's right. He is wrong in so many ways. It's difficult for me\nto remember all of them.",
    "start": "1870410",
    "end": "1876920"
  },
  {
    "text": "He is a Facebook buddy. So I have a lot of fun having\nthose little debates with him.",
    "start": "1876920",
    "end": "1881960"
  },
  {
    "text": "So I'm trying to remember the arguments. So one he says, \"We are not gifted this\nintelligence from aliens.",
    "start": "1881960",
    "end": "1890120"
  },
  {
    "text": "We are designing it, we are\nmaking decisions about it.\" That's not true. It was true when we had expert systems,",
    "start": "1890120",
    "end": "1897139"
  },
  {
    "text": "symbolic AI, decision trees. Today you set up parameters for a model",
    "start": "1897140",
    "end": "1902600"
  },
  {
    "text": "and you water this\nplant, you give it data, you give it compute and it grows. And after it's finished\ngrowing into this alien plant,",
    "start": "1902600",
    "end": "1910460"
  },
  {
    "text": "you start testing it to find\nout what capabilities it has. And it takes years to figure\nout, even for existing models,",
    "start": "1910460",
    "end": "1917390"
  },
  {
    "text": "if it's trained for six months, it'll take you two,\nthree years to figure out basic capabilities of that system.",
    "start": "1917390",
    "end": "1923059"
  },
  {
    "text": "We still discover new\ncapabilities in systems which are already out there. So that's not the case.",
    "start": "1923060",
    "end": "1929810"
  },
  {
    "text": "- So just to linger on that,\nto you, the difference there that there is some level\nof emergent intelligence",
    "start": "1929810",
    "end": "1935270"
  },
  {
    "text": "that happens in our current approaches? So stuff that we don't hardcode in.",
    "start": "1935270",
    "end": "1941690"
  },
  {
    "text": "- Absolutely. That's what\nmakes it so successful. When we had to painstakingly\nhardcode in everything,",
    "start": "1941690",
    "end": "1948559"
  },
  {
    "text": "we didn't have much progress. Now just spend more money and more compute and\nit's a lot more capable.",
    "start": "1948560",
    "end": "1955310"
  },
  {
    "text": "- And then the question is, when there is emergent\nintelligent phenomena, what is the ceiling of that?",
    "start": "1955310",
    "end": "1961130"
  },
  {
    "text": "For you, there's no ceiling. For Yann LeCun, I think\nthere's a kind of ceiling",
    "start": "1961130",
    "end": "1966950"
  },
  {
    "text": "that happens that we\nhave full control over. Even if we don't understand\nthe internals of the emergence,",
    "start": "1966950",
    "end": "1972500"
  },
  {
    "text": "how the emergence happens, there's a sense that we have control and an understanding",
    "start": "1972500",
    "end": "1978230"
  },
  {
    "text": "of the approximate ceiling of capability, the limits of the capability.",
    "start": "1978230",
    "end": "1984049"
  },
  {
    "text": "- Let's say there is a\nceiling, it's not guaranteed to be at the level which\nis competitive with us.",
    "start": "1984050",
    "end": "1990740"
  },
  {
    "text": "It may be greatly superior to ours. - So what about his\nstatement about open research",
    "start": "1990740",
    "end": "1998240"
  },
  {
    "text": "and open source are the\nbest ways to understand and mitigate the risks? - Historically, he's completely right.",
    "start": "1998240",
    "end": "2003490"
  },
  {
    "text": "Open source software is wonderful, it's tested by the\ncommunity, it's debugged. But we're switching from tools to agents.",
    "start": "2003490",
    "end": "2010690"
  },
  {
    "text": "Now you're giving open source\nweapons to psychopaths. Do we wanna open source nuclear\nweapons, biological weapons?",
    "start": "2010690",
    "end": "2019390"
  },
  {
    "text": "It's not safe to give\ntechnology so powerful to those who may misalign it.",
    "start": "2019390",
    "end": "2025210"
  },
  {
    "text": "Even if you are successful\nat somehow getting it to work in a first place in a friendly manner.",
    "start": "2025210",
    "end": "2030760"
  },
  {
    "text": "- But the difference with nuclear weapons, current AI systems are not\nakin to nuclear weapons.",
    "start": "2030760",
    "end": "2036429"
  },
  {
    "text": "So the idea there is you're\nopen sourcing at this stage that you can understand it better. Large number of people can\nexplore the limitation,",
    "start": "2036430",
    "end": "2044080"
  },
  {
    "text": "the capabilities, explore the\npossible ways to keep it safe, to keep it's secure,\nall that kind of stuff",
    "start": "2044080",
    "end": "2050110"
  },
  {
    "text": "while it's not at the\nstage of nuclear weapons. So nuclear weapons,\nthere's a no nuclear weapon",
    "start": "2050110",
    "end": "2055389"
  },
  {
    "text": "and then there's a nuclear weapon. With AI systems, there's a\ngradual improvement of capability",
    "start": "2055390",
    "end": "2060460"
  },
  {
    "text": "and you get to perform that\nimprovement incrementally. And so open source allows you\nto study how things go wrong.",
    "start": "2060460",
    "end": "2069940"
  },
  {
    "text": "Study the very process of emergence, study AI safety on those systems",
    "start": "2069940",
    "end": "2075190"
  },
  {
    "text": "when there's not a high level of danger, all that kind of stuff. - It also sets a very wrong precedence.",
    "start": "2075190",
    "end": "2080889"
  },
  {
    "text": "So we open sourced model\none, model two, model three. Nothing ever bad happened, so obviously we're gonna\ndo it with model four.",
    "start": "2080890",
    "end": "2088179"
  },
  {
    "text": "It's just gradual improvement. - I don't think it always\nworks with the precedent. Like you're not stuck doing\nit the way you always did.",
    "start": "2088180",
    "end": "2096553"
  },
  {
    "text": "It sets the precedent of open research and open development such\nthat we get to learn together.",
    "start": "2097807",
    "end": "2104079"
  },
  {
    "text": "And then the first time\nthere's a sign of danger, some dramatic thing happened,",
    "start": "2104080",
    "end": "2110020"
  },
  {
    "text": "not a thing that destroys\nhuman civilization, but some dramatic\ndemonstration of capability",
    "start": "2110020",
    "end": "2115569"
  },
  {
    "text": "that can legitimately\nlead to a lot of damage. Then everybody wakes up and says,",
    "start": "2115570",
    "end": "2120617"
  },
  {
    "text": "\"Okay, we need to regulate this. We need to come up with safety mechanism that stops this,\" right? But at this time, maybe\nyou can educate me,",
    "start": "2120617",
    "end": "2128620"
  },
  {
    "text": "but I haven't seen any\nillustration of significant damage done by intelligent AI systems.",
    "start": "2128620",
    "end": "2134680"
  },
  {
    "text": "- So I have a paper\nwhich collects accidents through history of AI and they always are proportionate",
    "start": "2134680",
    "end": "2140290"
  },
  {
    "text": "to capabilities of that system. So if you have Tic-tac-toe playing AI, it will fail to properly play",
    "start": "2140290",
    "end": "2146770"
  },
  {
    "text": "and loses the game, which\nit should draw, trivial. Your spell checker will\nmisspell a word, so on.",
    "start": "2146770",
    "end": "2152293"
  },
  {
    "text": "I stopped collecting those because there are just too\nmany examples of AI failing at what they're capable of.",
    "start": "2153220",
    "end": "2159010"
  },
  {
    "text": "We haven't had terrible accidents in a sense of billion people\ngot killed, absolutely true.",
    "start": "2159010",
    "end": "2165430"
  },
  {
    "text": "But in another paper I argue that those accidents do\nnot actually prevent people",
    "start": "2165430",
    "end": "2172119"
  },
  {
    "text": "from continuing with research. And actually they kind\nof serve like vaccines.",
    "start": "2172120",
    "end": "2178599"
  },
  {
    "text": "A vaccine makes your\nbody a little bit sick so you can handle the big\ndisease later much better.",
    "start": "2178600",
    "end": "2184660"
  },
  {
    "text": "It's the same here. People will point out, \"You\nknow AI accident we had where 12 people died?",
    "start": "2184660",
    "end": "2190330"
  },
  {
    "text": "Everyone's still here, 12 people\nis less than smoking kills. It's not a big deal.\" So we continue.",
    "start": "2190330",
    "end": "2196300"
  },
  {
    "text": "So in a way it will actually\nbe kind of confirming that it's not that bad.",
    "start": "2196300",
    "end": "2202450"
  },
  {
    "text": "- It matters how the deaths happen, whether it's literally\nmurder by the AI system,",
    "start": "2202450",
    "end": "2208630"
  },
  {
    "text": "then one is a problem. But if it's accidents because of increased reliance\non automation for example.",
    "start": "2208630",
    "end": "2217930"
  },
  {
    "text": "So when airplanes are\nflying in an automated way, maybe the number of\nplane crashes increased",
    "start": "2217930",
    "end": "2226119"
  },
  {
    "text": "by 17% or something. And then you're like,\nokay, do we really want to rely on automation?",
    "start": "2226120",
    "end": "2231550"
  },
  {
    "text": "I think in the case of\nautomation airplanes, it decreased significantly. Okay, same thing with autonomous vehicles.",
    "start": "2231550",
    "end": "2237132"
  },
  {
    "text": "Like okay, what are the pros and cons? What are the trade-offs here? And you can have that\ndiscussion in an honest way.",
    "start": "2237133",
    "end": "2245140"
  },
  {
    "text": "But I think the kind of things\nwe're talking about here is mass scale pain and suffering",
    "start": "2245140",
    "end": "2253070"
  },
  {
    "text": "caused by AI systems. And I think we need to\nsee illustrations of that in a very small scale,",
    "start": "2254020",
    "end": "2260589"
  },
  {
    "text": "to start to understand that this is really\ndamaging versus Clippy,",
    "start": "2260590",
    "end": "2266770"
  },
  {
    "text": "versus a tool that's really\nuseful to a lot of people to do learning, to do\nsummarization of texts,",
    "start": "2266770",
    "end": "2273640"
  },
  {
    "text": "to do question and answer,\nall that kind of stuff. To generate videos. A tool, fundamentally\na tool versus an agent",
    "start": "2273640",
    "end": "2280960"
  },
  {
    "text": "that can do a huge amount of damage. - So you bring up example of cars. - [Lex] Yes.\n- Cars were slowly",
    "start": "2280960",
    "end": "2287650"
  },
  {
    "text": "developed and integrated. If we had no cars and somebody came around and said, \"I invented this thing,",
    "start": "2287650",
    "end": "2294010"
  },
  {
    "text": "it's called cars, it's awesome, it kills like 100,000\nAmericans every year. Let's deploy it.\"",
    "start": "2294010",
    "end": "2299647"
  },
  {
    "text": "Would we deploy that? - There'd been fear mongering about cars for a long time.",
    "start": "2300550",
    "end": "2306190"
  },
  {
    "text": "The transition from horses to cars, there's a really nice channel that I recommend people\ncheck out Pessimist Archive",
    "start": "2306190",
    "end": "2312760"
  },
  {
    "text": "that documents all the fear\nmongering about technology that's happened throughout history. There's definitely been a lot\nof fear mongering about cars.",
    "start": "2312760",
    "end": "2320440"
  },
  {
    "text": "There's a transition\nperiod there about cars, about how deadly they are. We can try, it took a very long\ntime for cars to proliferate",
    "start": "2320440",
    "end": "2328720"
  },
  {
    "text": "to the degree they have now. And then you could ask serious questions in terms of the miles traveled,\nthe benefit to the economy,",
    "start": "2328720",
    "end": "2336490"
  },
  {
    "text": "the benefit to the quality\nof life that cars do versus the number of deaths, 30, 40,000 in the United States.",
    "start": "2336490",
    "end": "2343540"
  },
  {
    "text": "Are we willing to pay that price? I think most people, when\nthey're rationally thinking, policy makers will say \"Yes.\"",
    "start": "2343540",
    "end": "2349986"
  },
  {
    "text": "We want to decrease it from 40,000 to zero and do everything we can to decrease it.",
    "start": "2351520",
    "end": "2358300"
  },
  {
    "text": "There's all kinds of policies,\nincentives you can create to decrease the risks with\nthe deployment of technology.",
    "start": "2358300",
    "end": "2365560"
  },
  {
    "text": "But then you have to weigh the benefits and the risks of the technology. And the same thing would be done with AI.",
    "start": "2365560",
    "end": "2371410"
  },
  {
    "text": "- You need data, you need to know. But if I'm right and it's\nunpredictable, unexplainable,",
    "start": "2371410",
    "end": "2376600"
  },
  {
    "text": "uncontrollable, you\ncannot make this decision. We're gaining $10 trillion of wealth",
    "start": "2376600",
    "end": "2381609"
  },
  {
    "text": "but we're losing, we don't\nknow how many people. You basically have to\nperform an experiment",
    "start": "2381610",
    "end": "2387670"
  },
  {
    "text": "on 8 billion humans without their consent. And even if they want to\ngive you consent, they can't",
    "start": "2387670",
    "end": "2393520"
  },
  {
    "text": "because they cannot give informed consent. They don't understand those things. - Right, that happens when\nyou go from the predictable",
    "start": "2393520",
    "end": "2401200"
  },
  {
    "text": "to the unpredictable very quickly.",
    "start": "2401200",
    "end": "2403662"
  },
  {
    "text": "But it's not obvious to\nme that AI systems would gain capability so quickly\nthat you won't be able",
    "start": "2406360",
    "end": "2412450"
  },
  {
    "text": "to collect enough data to study the benefits and the risks. - We literally doing it.",
    "start": "2412450",
    "end": "2418570"
  },
  {
    "text": "The previous model we learned about after we finished training\nit what it was capable of.",
    "start": "2418570",
    "end": "2423700"
  },
  {
    "text": "Let's say we stopped GPT-4 training around human capability hypothetically,",
    "start": "2423700",
    "end": "2429040"
  },
  {
    "text": "we start training GPT-5 and I have no knowledge of\ninsider training runs or anything",
    "start": "2429040",
    "end": "2434080"
  },
  {
    "text": "and we start at that point of about human and we train it for the next nine months.",
    "start": "2434080",
    "end": "2439480"
  },
  {
    "text": "Maybe two months in it\nbecomes super intelligent. We continue training it. At the time when we start testing it,",
    "start": "2439480",
    "end": "2447309"
  },
  {
    "text": "is already a dangerous system. How dangerous, I have no idea. But neither people training it.",
    "start": "2447310",
    "end": "2453579"
  },
  {
    "text": "- At the training stage, but then there's a testing\nstage inside the company,",
    "start": "2453580",
    "end": "2458740"
  },
  {
    "text": "they can start getting intuition about what the system is capable to do. You're saying that the somehow leap",
    "start": "2458740",
    "end": "2464440"
  },
  {
    "text": "from GPT-4 to GPT-5 can happen.",
    "start": "2464440",
    "end": "2468283"
  },
  {
    "text": "The kind of leap where\nGPT-4 was controllable and GPT-5 is no longer controllable",
    "start": "2469750",
    "end": "2475060"
  },
  {
    "text": "and we get no insights from using GPT-4 about the fact that GPT-5\nwill be uncontrollable.",
    "start": "2475060",
    "end": "2481690"
  },
  {
    "text": "Like that's the situation\nyou're concerned about. Where their leap from n to n plus one",
    "start": "2481690",
    "end": "2489580"
  },
  {
    "text": "would be such that a uncontrollable system is created without any ability for\nus to anticipate that.",
    "start": "2489580",
    "end": "2498990"
  },
  {
    "text": "- If we had capability of ahead of the run before the training\nrun to register exactly what capabilities that\nnext model will have",
    "start": "2499030",
    "end": "2506140"
  },
  {
    "text": "at the end of the training run. And we accurately guessed all of them. I would say you are right, we can definitely go ahead with this run.",
    "start": "2506140",
    "end": "2512770"
  },
  {
    "text": "We don't have the capability. - From GPT-4, you can build up intuitions about what GPT-5 will be capable of.",
    "start": "2512770",
    "end": "2519190"
  },
  {
    "text": "It's just incremental progress. Even if that's a big leap in capability,",
    "start": "2519190",
    "end": "2526690"
  },
  {
    "text": "it just doesn't seem like you\ncan take a leap from a system that's helping you\nwrite emails to a system",
    "start": "2526690",
    "end": "2533859"
  },
  {
    "text": "that's going to destroy\nhuman civilization. It seems like it's always going to be sufficiently incremental",
    "start": "2533860",
    "end": "2540310"
  },
  {
    "text": "such that we can anticipate\nthe possible dangers. And we're not even talking\nabout existential risks, but just the kind of damage\nyou can do to civilization.",
    "start": "2540310",
    "end": "2549130"
  },
  {
    "text": "It seems like we'll be able\nto anticipate the kinds, not the exact, but the kinds of risks",
    "start": "2549130",
    "end": "2556570"
  },
  {
    "text": "it might lead to and then rapidly develop\ndefenses ahead of time.",
    "start": "2556570",
    "end": "2561970"
  },
  {
    "text": "And as the risks emerge. - We're not talking just about\ncapabilities, specific tasks,",
    "start": "2561970",
    "end": "2568240"
  },
  {
    "text": "we're talking about general\ncapability to learn. Maybe like a child at the time\nof testing and deployment,",
    "start": "2568240",
    "end": "2575710"
  },
  {
    "text": "it is still not extremely capable but as it is exposed to\nmore data, real world,",
    "start": "2575710",
    "end": "2582310"
  },
  {
    "text": "it can be trained to become\nmuch more dangerous and capable. - So let's focus then\non the control problem.",
    "start": "2582310",
    "end": "2588403"
  },
  {
    "start": "2586000",
    "end": "2733000"
  },
  {
    "text": "At which point does the\nsystem become uncontrollable? Why is it the more\nlikely trajectory for you",
    "start": "2591280",
    "end": "2597490"
  },
  {
    "text": "that the system becomes uncontrollable? - So I think at some\npoint it becomes capable",
    "start": "2597490",
    "end": "2603160"
  },
  {
    "text": "of getting out of control. For game theoretic reasons, it may decide not to\ndo anything right away",
    "start": "2603160",
    "end": "2608800"
  },
  {
    "text": "and for a long time just\ncollect more resources, accumulate strategic advantage.",
    "start": "2608800",
    "end": "2615130"
  },
  {
    "text": "Right away, it may be kind of still young, weak superintelligence, give it a decade,",
    "start": "2615130",
    "end": "2620200"
  },
  {
    "text": "it's in charge of a lot more resources. It had time to make backups. So it's not obvious to me",
    "start": "2620200",
    "end": "2625780"
  },
  {
    "text": "that it will strike as soon as it can. - But can we just try\nto imagine this future",
    "start": "2625780",
    "end": "2630910"
  },
  {
    "text": "where there's an AI\nsystem that's capable of",
    "start": "2630910",
    "end": "2634549"
  },
  {
    "text": "escaping the control of humans and then doesn't and waits. What's that look like?",
    "start": "2636580",
    "end": "2643060"
  },
  {
    "text": "So one, we have to rely on that system for a lot of the infrastructure. So we'll have to give it access\nnot just to the internet,",
    "start": "2643060",
    "end": "2651370"
  },
  {
    "text": "but to the task of managing",
    "start": "2651370",
    "end": "2655160"
  },
  {
    "text": "power, government, economy,\nthis kind of stuff. And that just feels like a gradual process",
    "start": "2656470",
    "end": "2662770"
  },
  {
    "text": "given the bureaucracies of\nall those systems evolved. - We've been doing it for years. Software controls all those\nsystems, nuclear power plants,",
    "start": "2662770",
    "end": "2670150"
  },
  {
    "text": "airline industry, all software based. Every time there is electrical outage, I can't fly anywhere for days.",
    "start": "2670150",
    "end": "2676600"
  },
  {
    "text": "- But there's a difference\nbetween software and AI.",
    "start": "2676600",
    "end": "2681600"
  },
  {
    "text": "So there's different kinds of software. So to give a single AI system access",
    "start": "2681728",
    "end": "2686950"
  },
  {
    "text": "to the control of airlines and\nthe control of the economy,",
    "start": "2686950",
    "end": "2690283"
  },
  {
    "text": "that's not a trivial\ntransition for humanity. - No, but if it shows it is safer, in fact when it's in control\nwe get better results,",
    "start": "2692730",
    "end": "2699490"
  },
  {
    "text": "people will demand that\nit put put in place. - [Lex] Absolutely.\n- And if not it can hack the system, it can use social engineering\nto get access to it.",
    "start": "2699490",
    "end": "2706870"
  },
  {
    "text": "That's why I said it might\ntake some time for it to accumulate those resources. - It just feels like that\nwould take a long time",
    "start": "2706870",
    "end": "2713349"
  },
  {
    "text": "for either humans to trust it or for the social engineering\nto come into play. Like it's not a thing\nthat happens overnight.",
    "start": "2713350",
    "end": "2719920"
  },
  {
    "text": "It feels like something that happens across one or two decades. - I really hope you're right.\nBut it's not what I'm seeing.",
    "start": "2719920",
    "end": "2726369"
  },
  {
    "text": "People are very quick to\njump on the latest trend. Early adopters will be there before it's even deployed\nbuying prototypes,",
    "start": "2726370",
    "end": "2733630"
  },
  {
    "start": "2733000",
    "end": "2886000"
  },
  {
    "text": "- Maybe the social engineering. I could see because... So for social engineering, AI systems don't need any hardware access.",
    "start": "2733630",
    "end": "2740173"
  },
  {
    "text": "It's all software. So they can start manipulating you through social media and so on. Like you have AI assistance,\nthey're gonna help you",
    "start": "2741280",
    "end": "2748690"
  },
  {
    "text": "manage a lot of your day-to-day and then they start\ndoing social engineering. But like for a system that's so capable",
    "start": "2748690",
    "end": "2757240"
  },
  {
    "text": "that it can escape the control\nof humans that created it. Such a system being\ndeployed at a mass scale",
    "start": "2757240",
    "end": "2765349"
  },
  {
    "text": "and trusted by people to be deployed. It feels like that would\ntake a lot of convincing.",
    "start": "2766360",
    "end": "2773890"
  },
  {
    "text": "- So we've been deploying systems which had hidden capabilities.",
    "start": "2773890",
    "end": "2778063"
  },
  {
    "text": "- Can you give an example?\n- GPT-4? I don't know what else it's capable of, but there are still things we\nhaven't discovered it can do.",
    "start": "2779320",
    "end": "2785890"
  },
  {
    "text": "There may be trivial\nproportionate to its capability. I don't know, it writes Chinese poetry.",
    "start": "2785890",
    "end": "2791230"
  },
  {
    "text": "Hypothetical, I know it does. But we haven't tested for\nall possible capabilities",
    "start": "2791230",
    "end": "2797410"
  },
  {
    "text": "and we are not explicitly designing them. We can only rule out bugs we find.",
    "start": "2797410",
    "end": "2803290"
  },
  {
    "text": "We cannot rule out bugs and capabilities because we haven't found them.",
    "start": "2803290",
    "end": "2809110"
  },
  {
    "text": "- Is it possible for a system\nto have hidden capabilities that are orders of magnitude",
    "start": "2811360",
    "end": "2818530"
  },
  {
    "text": "greater than its non hidden capabilities? This is the thing I'm\nreally struggling with.",
    "start": "2818530",
    "end": "2824710"
  },
  {
    "text": "Where on the surface the\nthing we understand it can do",
    "start": "2824710",
    "end": "2828920"
  },
  {
    "text": "doesn't seem that harmful. So even if it has bugs, even if it has hidden\ncapabilities like Chinese poetry",
    "start": "2829870",
    "end": "2836350"
  },
  {
    "text": "or generating effective\nviruses, software viruses,",
    "start": "2836350",
    "end": "2841350"
  },
  {
    "text": "the damage that can do\nseems like on the same order of magnitude as",
    "start": "2842380",
    "end": "2846363"
  },
  {
    "text": "the capabilities that we know about. So like this idea that\nthe hidden capabilities",
    "start": "2849250",
    "end": "2854470"
  },
  {
    "text": "will include being uncontrollable, this is something I'm struggling with. 'Cause GPT-4 on the surface\nseems to be very controllable.",
    "start": "2854470",
    "end": "2862060"
  },
  {
    "text": "- Again, we can only ask and\ntest for things we know about. If there are unknown\nunknowns, we cannot do it.",
    "start": "2862060",
    "end": "2868720"
  },
  {
    "text": "Thinking of humans autistic\nsavants events, right? If you talk to a person like that, you may not even realize they can multiply",
    "start": "2868720",
    "end": "2876190"
  },
  {
    "text": "20 digit numbers in their head. You have to know to ask. - So as I mentioned,\njust to sort of linger",
    "start": "2876190",
    "end": "2883390"
  },
  {
    "text": "on the fear of the unknown. So the Pessimist Archive\nis just documented.",
    "start": "2883390",
    "end": "2890200"
  },
  {
    "start": "2886000",
    "end": "3477000"
  },
  {
    "text": "Let's look at data of\nthe past, at history, there's been a lot of fear\nmongering about technology.",
    "start": "2890200",
    "end": "2896320"
  },
  {
    "text": "Pessimist Archive does a\nreally good job of documenting how crazily afraid we are of\nevery piece of technology.",
    "start": "2896320",
    "end": "2904630"
  },
  {
    "text": "There's a blog post where Louis Anslow who created Pessimist\nArchive writes about the fact",
    "start": "2904630",
    "end": "2910870"
  },
  {
    "text": "that we've been\nfear-mongering about robots and automation for over 100 years.",
    "start": "2910870",
    "end": "2917500"
  },
  {
    "text": "So why is AGI different than\nthe kinds of technologies we've been afraid of in the past?",
    "start": "2917500",
    "end": "2923530"
  },
  {
    "text": "- So two things. One, we're\nswitching from tools to agents. Tools don't have negative",
    "start": "2923530",
    "end": "2930790"
  },
  {
    "text": "or positive impact. People using tools do. So guns don't kill, people with guns do.",
    "start": "2930790",
    "end": "2937900"
  },
  {
    "text": "Agents can make their own decisions, they can be positive or negative. A pit bull can decide\nto harm you as an agent.",
    "start": "2937900",
    "end": "2945895"
  },
  {
    "text": "The fears are the same. The only difference is now\nwe have this technology.",
    "start": "2945895",
    "end": "2951010"
  },
  {
    "text": "When they were afraid of\nhumanoid robots 100 years ago, they had none. Today every major company in\nthe world is investing billions",
    "start": "2951010",
    "end": "2958510"
  },
  {
    "text": "to create them. Not every, but you\nunderstand what I'm saying? - Yes.\n- It's very different.",
    "start": "2958510",
    "end": "2963760"
  },
  {
    "text": "- Well, agents, it\ndepends on what you mean by the word agents.",
    "start": "2963760",
    "end": "2969760"
  },
  {
    "text": "All those companies are\nnot investing in a system that has the kind of agency\nthat's implied by in the fears",
    "start": "2969760",
    "end": "2976740"
  },
  {
    "text": "where it can really make\ndecisions in their own that have no human in the loop.",
    "start": "2977080",
    "end": "2981493"
  },
  {
    "text": "- They are saying they're\nbuilding super intelligence and have a super alignment team. You don't think they're trying",
    "start": "2982450",
    "end": "2987700"
  },
  {
    "text": "to create a system smart enough to be an independent agent\nunder that definition? - I have not seen evidence of it.",
    "start": "2987700",
    "end": "2994205"
  },
  {
    "text": "I think a lot of it is a marketing kind of\ndiscussion about the future.",
    "start": "2994206",
    "end": "3000630"
  },
  {
    "text": "And it's a mission about\nthe kind of systems we can create in the long-term future. But in the short term, the kind\nof systems they're creating",
    "start": "3000630",
    "end": "3010260"
  },
  {
    "text": "falls fully within the definition of narrow AI.",
    "start": "3010260",
    "end": "3016320"
  },
  {
    "text": "These are tools that have\nincreasing capabilities, but they just don't have a sense of agency",
    "start": "3016320",
    "end": "3021780"
  },
  {
    "text": "or consciousness or self-awareness or ability to deceive at\nscales that would be required",
    "start": "3021780",
    "end": "3028920"
  },
  {
    "text": "to do like mass scale\nsuffering and murder of humans. - Those systems are well beyond narrow AI.",
    "start": "3028920",
    "end": "3034710"
  },
  {
    "text": "If you had to list all\nthe capabilities of GPT-4, you would spend a lot of\ntime writing that list.",
    "start": "3034710",
    "end": "3040260"
  },
  {
    "text": "- But agency is not one of them. - Not yet, but do you think\nany of those companies are holding back because they\nthink it may be not safe?",
    "start": "3040260",
    "end": "3048089"
  },
  {
    "text": "Or are they developing the\nmost capable system they can given the resources and hoping they can control and monetize.",
    "start": "3048090",
    "end": "3055623"
  },
  {
    "text": "- Control and monetize. Hoping they can control and monetize. So you're saying if they\ncould press a button",
    "start": "3056490",
    "end": "3062819"
  },
  {
    "text": "and create an agent that\nthey no longer control, that they have to ask nicely.",
    "start": "3062820",
    "end": "3068403"
  },
  {
    "text": "A thing that's lives on a server across huge number of computers.",
    "start": "3070230",
    "end": "3075273"
  },
  {
    "text": "You're saying that they would push for the creation of that kinds of system? - I mean I can't speak for\nother people, for all of them.",
    "start": "3077190",
    "end": "3084600"
  },
  {
    "text": "I think some of them are very ambitious. They fundraise in trillions, they talk about controlling",
    "start": "3084600",
    "end": "3090540"
  },
  {
    "text": "the light corner of the universe. I would guess that they might.",
    "start": "3090540",
    "end": "3094233"
  },
  {
    "text": "- Well that's a human question, whether humans are capable of that. Probably some humans are capable of that.",
    "start": "3096180",
    "end": "3101760"
  },
  {
    "text": "My more direct question, if it's possible to create such a system? Have a system that has\nthat level of agency.",
    "start": "3101760",
    "end": "3109023"
  },
  {
    "text": "I don't think that's an\neasy technical challenge.",
    "start": "3110160",
    "end": "3113733"
  },
  {
    "text": "It doesn't feel like we're close to that, a system that has the kind of agency where it can make its own decisions",
    "start": "3116222",
    "end": "3121380"
  },
  {
    "text": "and deceive everybody about them. The current architecture\nwe have in machine learning",
    "start": "3121380",
    "end": "3128670"
  },
  {
    "text": "and how we train the systems,\nhow to deploy the systems and all that, it just doesn't seem to support that kind of agency.",
    "start": "3128670",
    "end": "3134820"
  },
  {
    "text": "- I really hope you are right. I think the scaling hypothesis is correct. We haven't seen diminishing returns.",
    "start": "3134820",
    "end": "3141960"
  },
  {
    "text": "It used to be we asked\nhow long before AGI, now we should ask how much until AGI.",
    "start": "3141960",
    "end": "3147960"
  },
  {
    "text": "It's trillion dollars today, it's a billion dollars next year, it's a million dollars in a few years.",
    "start": "3147960",
    "end": "3153039"
  },
  {
    "text": "- Don't you think it's\npossible to basically run out of trillions? So is this constrained by compute?",
    "start": "3153930",
    "end": "3161010"
  },
  {
    "text": "- Compute gets cheaper\nevery day exponentially. - But then that becomes a\nquestion of decades versus years.",
    "start": "3161010",
    "end": "3167400"
  },
  {
    "text": "- If the only disagreement\nis that it'll take decades, not years for everything\nI'm saying to materialize,",
    "start": "3167400",
    "end": "3174839"
  },
  {
    "text": "then I can go with that. - But if it takes decades,\nthen the development of tools",
    "start": "3174840",
    "end": "3180390"
  },
  {
    "text": "for AI safety becomes\nmore and more realistic.",
    "start": "3180390",
    "end": "3185390"
  },
  {
    "text": "So I guess the question is,\nI have a fundamental belief that humans when faced with\ndanger can come up with ways",
    "start": "3185970",
    "end": "3193050"
  },
  {
    "text": "to defend against that danger. And one of the big problems\nfacing AI safety currently",
    "start": "3193050",
    "end": "3198990"
  },
  {
    "text": "for me is that there's\nnot clear illustrations of what that danger looks like.",
    "start": "3198990",
    "end": "3205040"
  },
  {
    "text": "There's no illustrations of AI\nsystems doing a lot of damage and so it's unclear what\nyou're defending against.",
    "start": "3206370",
    "end": "3213359"
  },
  {
    "text": "'Cause currently it's a\nphilosophical notions that yes, it's possible to imagine AI systems",
    "start": "3213360",
    "end": "3219630"
  },
  {
    "text": "that take control of everything\nand then destroy all humans. It's also a more formal\nmathematical notion",
    "start": "3219630",
    "end": "3225610"
  },
  {
    "text": "that you talk about that it's impossible to have a perfectly secure system.",
    "start": "3226503",
    "end": "3231422"
  },
  {
    "text": "You can't prove that a program\nof sufficient complexity is completely safe and perfect\nand know everything about it.",
    "start": "3232493",
    "end": "3240890"
  },
  {
    "text": "Yes, but like when you\nactually just pragmatically look how much damage\nhave the AI systems done and what kind of damage,",
    "start": "3241320",
    "end": "3248160"
  },
  {
    "text": "there's not been illustrations of that. Even in the autonomous weapon systems,",
    "start": "3248160",
    "end": "3253953"
  },
  {
    "text": "there's not been mass deployments of autonomous weapon systems, luckily. The automation in war\ncurrently is very limited.",
    "start": "3254850",
    "end": "3263403"
  },
  {
    "text": "That the automation is at\nthe scale of individuals versus like at the scale\nof strategy and planning.",
    "start": "3265170",
    "end": "3272520"
  },
  {
    "text": "So I think one of the\nchallenges here is like where is the dangers and\nintuition that Yann LeCun",
    "start": "3272520",
    "end": "3280230"
  },
  {
    "text": "and others have is let's keep in the open building AI systems",
    "start": "3280230",
    "end": "3285450"
  },
  {
    "text": "until the dangers start\nrearing their heads and they become more explicit.",
    "start": "3285450",
    "end": "3293460"
  },
  {
    "text": "They start being case studies, illustrative case studies that\nshow exactly how the damage",
    "start": "3293460",
    "end": "3300750"
  },
  {
    "text": "by AI systems is done, then\nregulation can step in. Then brilliant engineers can step up",
    "start": "3300750",
    "end": "3306150"
  },
  {
    "text": "and we can have Manhattan style projects to defend against such systems. That's kind of the notion.",
    "start": "3306150",
    "end": "3311853"
  },
  {
    "text": "And I guess attention with\nthat is the idea that for you, we need to be thinking about that now",
    "start": "3312840",
    "end": "3318060"
  },
  {
    "text": "so that we're ready because we will have not much time once the\nsystems are deployed.",
    "start": "3318060",
    "end": "3324410"
  },
  {
    "text": "Is that true? - There is a lot to unpack here. There is a partnership on AI,",
    "start": "3324410",
    "end": "3330450"
  },
  {
    "text": "a conglomerate of many large corporations. They have a database of\nAI accidents they collect, I contributed a lot of that database.",
    "start": "3330450",
    "end": "3337680"
  },
  {
    "text": "If we so far made almost no progress in actually solving this problem, not patching it, not\nagain lipstick on a pig",
    "start": "3337680",
    "end": "3346140"
  },
  {
    "text": "kind of solutions. Why would we think we'll do better then we closer to the problem?",
    "start": "3346140",
    "end": "3351932"
  },
  {
    "text": "- All the things you\nmentioned are serious concerns measuring the amount of harm, so benefit versus-risk there is difficult.",
    "start": "3353100",
    "end": "3359160"
  },
  {
    "text": "But to you, the sense is already the risk has superseded the benefit. - Again, I wanna be\nperfectly clear, I love AI.",
    "start": "3359160",
    "end": "3365463"
  },
  {
    "text": "Yes, I love technology.\nI'm a computer scientist. I have PhD in engineering, I\nwork at engineering school.",
    "start": "3365463",
    "end": "3370470"
  },
  {
    "text": "There is a huge difference between we need to\ndevelop narrow AI systems, super intelligent in solving\nspecific human problems",
    "start": "3370470",
    "end": "3378750"
  },
  {
    "text": "like protein folding and let's create super\nintelligent machine, guard it and it will\ndecide what to do with us.",
    "start": "3378750",
    "end": "3386100"
  },
  {
    "text": "Those are not the same. I am against the super\nintelligence in general sense",
    "start": "3386100",
    "end": "3391620"
  },
  {
    "text": "with no undo button. - Do you think the teams that are doing,",
    "start": "3391620",
    "end": "3397080"
  },
  {
    "text": "they're able to do the AI safety on the kind of narrow AI risks",
    "start": "3397080",
    "end": "3402869"
  },
  {
    "text": "that you've mentioned? Are those approaches going\nto be at all productive",
    "start": "3402870",
    "end": "3408750"
  },
  {
    "text": "towards leading to approaches\nof doing AI safety on AGI or is this just a fundamentally\ndifferent problem?",
    "start": "3408750",
    "end": "3414630"
  },
  {
    "text": "- Partially, but they don't scale. For narrow AI, for deterministic systems, you can test them. You have edge cases, you know",
    "start": "3414630",
    "end": "3420932"
  },
  {
    "text": "what the answer should look like. You know the right answers. For general systems, you\nhave infinite test surface,",
    "start": "3420933",
    "end": "3428220"
  },
  {
    "text": "you have no edge cases. You cannot even know what to test for. Again, the unknown unknowns\nare underappreciated by people",
    "start": "3428220",
    "end": "3437760"
  },
  {
    "text": "looking at this problem. You are always asking me,\nhow will it kill everyone? How it will fail?",
    "start": "3437760",
    "end": "3444300"
  },
  {
    "text": "The whole point is, if I knew it, I would be super intelligent and despite what you might think, I'm not.",
    "start": "3444300",
    "end": "3449520"
  },
  {
    "text": "- So to you, the concern is\nthat we would not be able to",
    "start": "3450582",
    "end": "3453850"
  },
  {
    "text": "see early signs of an\nuncontrollable system. - It is a master at deception.",
    "start": "3455700",
    "end": "3461010"
  },
  {
    "text": "Sam tweeted about how\ngreat it is at persuasion and we see it ourselves,\nespecially now with voices,",
    "start": "3461010",
    "end": "3469080"
  },
  {
    "text": "maybe kind of flirty,\nsarcastic female voices. It's gonna be very good at\ngetting people to do things.",
    "start": "3469080",
    "end": "3475950"
  },
  {
    "text": "- But see, I'm very concerned about system being used\nto control the masses.",
    "start": "3475950",
    "end": "3484563"
  },
  {
    "start": "3477000",
    "end": "3870000"
  },
  {
    "text": "But in that case, the developers know about the kind of\ncontrol that's happening.",
    "start": "3485910",
    "end": "3491640"
  },
  {
    "text": "You're more concerned about the next stage where even the developers\ndon't know about the deception.",
    "start": "3491640",
    "end": "3498180"
  },
  {
    "text": "- Right, I don't think\ndevelopers know everything about what they're creating. They have lots of great knowledge.",
    "start": "3498180",
    "end": "3504690"
  },
  {
    "text": "We're making progress on\nexplaining parts of a network. We can understand, okay,\nthis node, get excited.",
    "start": "3504690",
    "end": "3511890"
  },
  {
    "text": "Then this input is presented,\nthis cluster of nodes, but we're nowhere near close",
    "start": "3511890",
    "end": "3518460"
  },
  {
    "text": "to understanding the full picture. And I think it's impossible. You need to be able to\nsurvey an explanation.",
    "start": "3518460",
    "end": "3524970"
  },
  {
    "text": "The size of those models\nprevents a single human from absorbing all this information, even if provided by the system.",
    "start": "3524970",
    "end": "3531599"
  },
  {
    "text": "So either we getting\nmodel as an explanation for what's happening and\nthat's not comprehensible to us",
    "start": "3531600",
    "end": "3537839"
  },
  {
    "text": "or we getting a compressed\nexplanation lossy compression where here's top 10 reasons you got fired.",
    "start": "3537840",
    "end": "3545280"
  },
  {
    "text": "It's something, but\nit's not a full picture. - You've given elsewhere\nan example of a child and everybody, all humans try to deceive,",
    "start": "3545280",
    "end": "3552540"
  },
  {
    "text": "they try to lie early on in their life. I think we'll just get a lot\nof examples of deceptions",
    "start": "3552540",
    "end": "3558599"
  },
  {
    "text": "from large language models or AI systems. They're going to be kind of shitty. Or they'll be pretty good,\nbut we'll catch 'em off guard.",
    "start": "3558600",
    "end": "3566010"
  },
  {
    "text": "We'll start to see the kind of momentum towards developing increasing\ndeception capabilities",
    "start": "3566010",
    "end": "3575060"
  },
  {
    "text": "and that's when you're like, \"Okay, we need to do\nsome kind of alignment that prevents deception. But then we'll have, if\nyou support open source,",
    "start": "3575700",
    "end": "3583650"
  },
  {
    "text": "then you can have open source models that have some level of deception you can start to explore on a large scale,",
    "start": "3583650",
    "end": "3588810"
  },
  {
    "text": "how do we stop it from being deceptive? Then there's a more explicit, pragmatic kind of problem to solve.",
    "start": "3588810",
    "end": "3597360"
  },
  {
    "text": "How do we stop AI systems from trying to optimize for deception?",
    "start": "3597360",
    "end": "3603480"
  },
  {
    "text": "That's just an example, right? - So there is a paper, I\nthink it came out last week by Dr. Park et al from MIT I think",
    "start": "3603480",
    "end": "3610770"
  },
  {
    "text": "and they showed that existing models already showed successful\ndeception in what they do.",
    "start": "3610770",
    "end": "3617343"
  },
  {
    "text": "My concern is not that they lie now and we need to catch them\nand tell 'em don't lie. My concern is that once they\nare capable and deployed,",
    "start": "3618810",
    "end": "3628200"
  },
  {
    "text": "they will later change their mind because that's what unrestricted\nlearning allows you to do.",
    "start": "3628200",
    "end": "3635609"
  },
  {
    "text": "Lots of people grow up maybe\nin the religious family, they read some new books and\nthey turn in their religion.",
    "start": "3635610",
    "end": "3642573"
  },
  {
    "text": "That's a treacherous turn in humans. If you learn something\nnew about your colleagues,",
    "start": "3643530",
    "end": "3650130"
  },
  {
    "text": "maybe you'll change how you react to them. - Yeah, the treacherous turn.",
    "start": "3650130",
    "end": "3655203"
  },
  {
    "text": "If we just mention\nhumans, Stalin and Hitler, there's a turn. Stalin's a good example.",
    "start": "3656520",
    "end": "3662280"
  },
  {
    "text": "He just seems like a normal\ncommunist follower Lenin until there's a turn.",
    "start": "3662280",
    "end": "3669299"
  },
  {
    "text": "There's a turn of what that means in terms of when he has complete control.",
    "start": "3669300",
    "end": "3674309"
  },
  {
    "text": "What the execution of that policy means and how many people get to suffer - And you can't say they're not rational.",
    "start": "3674310",
    "end": "3679920"
  },
  {
    "text": "The rational decision changes\nbased on your position, then you are under the boss.",
    "start": "3679920",
    "end": "3685317"
  },
  {
    "text": "The rational policy may\nbe to be following orders and being honest.",
    "start": "3685317",
    "end": "3690330"
  },
  {
    "text": "When you become a boss,\nrational policy may shift. - Yeah, and by the way, a\nlot of my disagreements here",
    "start": "3690330",
    "end": "3696569"
  },
  {
    "text": "is just to playing devil's advocate to challenge your ideas and\nto explore them together.",
    "start": "3696570",
    "end": "3702000"
  },
  {
    "text": "So one of the big problems\nhere in this whole conversation",
    "start": "3702000",
    "end": "3707000"
  },
  {
    "text": "is human civilization hangs in the balance and yet everything's unpredictable.",
    "start": "3707190",
    "end": "3712410"
  },
  {
    "text": "We don't know how these\nsystems will look like.",
    "start": "3712410",
    "end": "3714760"
  },
  {
    "text": "- The robots are coming. - There's a refrigerator\nmaking a buzzing noise.",
    "start": "3718590",
    "end": "3722883"
  },
  {
    "text": "- Very menacing. Very menacing. So every time I'm about\nto talk about this topic,",
    "start": "3723810",
    "end": "3729029"
  },
  {
    "text": "things start to happen. My flight yesterday was canceled without possibility to rebook.",
    "start": "3729030",
    "end": "3734310"
  },
  {
    "text": "I was giving a talk at Google in Israel and three cars, which\nwere supposed to take me",
    "start": "3734310",
    "end": "3740730"
  },
  {
    "text": "to the talk could not, I'm just saying,",
    "start": "3740730",
    "end": "3744093"
  },
  {
    "text": "- I like AIs. I for one, welcome our overloads. - Well, there's a degree\nto which we, I mean,",
    "start": "3747180",
    "end": "3753930"
  },
  {
    "text": "it is very obvious as we already have. We've increasingly given our\nlife over to software systems.",
    "start": "3753930",
    "end": "3761013"
  },
  {
    "text": "And then it seems obvious\ngiven the capabilities of AI that are coming that we'll give\nour lives over increasingly",
    "start": "3762570",
    "end": "3769920"
  },
  {
    "text": "to AI systems. Cars will drive themselves, refrigerator eventually will\noptimize what I get to eat.",
    "start": "3769920",
    "end": "3778760"
  },
  {
    "text": "And as more and more out\nof our lives are controlled or managed by AI assistance,",
    "start": "3779820",
    "end": "3786690"
  },
  {
    "text": "it is very possible that there's a drift. Or I mean I personally am concerned",
    "start": "3786690",
    "end": "3791970"
  },
  {
    "text": "about non-existential stuff,\nthe more near term things. Because before we even get to existential,",
    "start": "3791970",
    "end": "3798720"
  },
  {
    "text": "I feel like there could be just so many \"Brave New World\"\ntype of situations. You mentioned sort of the\nterm behavioral drift.",
    "start": "3798720",
    "end": "3805800"
  },
  {
    "text": "It's the slow boiling that\nI'm really concerned about as we give our lives over to automation,",
    "start": "3805800",
    "end": "3812250"
  },
  {
    "text": "that our minds can become\ncontrolled by governments, by companies, or just\nin a distributed way.",
    "start": "3812250",
    "end": "3820290"
  },
  {
    "text": "There's a drift. Some aspect of our human\nnature gives ourselves over to the control of AI systems",
    "start": "3820290",
    "end": "3826860"
  },
  {
    "text": "and they in an unintended way\njust control how we think. Maybe there'll be a herd like\nmentality in how we think,",
    "start": "3826860",
    "end": "3833970"
  },
  {
    "text": "which will kill all creativity\nand exploration of ideas, the diversity of ideas or much worse.",
    "start": "3833970",
    "end": "3841380"
  },
  {
    "text": "So it's true, it's true. But a lot of the conversation\nI'm having with you now",
    "start": "3841380",
    "end": "3846930"
  },
  {
    "text": "is also kind of wondering\nalmost at a technical level, how can AI escape control?",
    "start": "3846930",
    "end": "3853623"
  },
  {
    "text": "Like what would that system look like? Because it, to me is\nterrifying and fascinating",
    "start": "3855437",
    "end": "3861420"
  },
  {
    "text": "and also fascinating to me is maybe the optimistic notion is possible",
    "start": "3861420",
    "end": "3868200"
  },
  {
    "text": "to engineer systems that\ndefend against that. One of the things you write a lot about",
    "start": "3868200",
    "end": "3873500"
  },
  {
    "start": "3870000",
    "end": "4289000"
  },
  {
    "text": "in your book is verifiers. So not humans, humans are also verifiers.",
    "start": "3873500",
    "end": "3879723"
  },
  {
    "text": "But software systems\nthat look at AI systems and like help you understand,",
    "start": "3880680",
    "end": "3888120"
  },
  {
    "text": "this thing is getting real weird. Help you analyze those systems.",
    "start": "3888120",
    "end": "3893579"
  },
  {
    "text": "So maybe this is a good time\nto talk about verification. What is this beautiful\nnotion of verification?",
    "start": "3893580",
    "end": "3901020"
  },
  {
    "text": "- My claim is again, that\nthere are very strong limits in what we can and cannot verify. A lot of times when you post\nsomething on social media,",
    "start": "3901020",
    "end": "3908819"
  },
  {
    "text": "people go, \"Oh, I need a citation to a peer reviewed article.\" But what is a peer reviewed article? You found two people in a\nworld of hundreds of scientists",
    "start": "3908820",
    "end": "3917760"
  },
  {
    "text": "who said, \"Ah whatever,\npublish it. I don't care.\" That's the verifier of that process. When people say, \"Oh, it's\nformally verified software,",
    "start": "3917760",
    "end": "3926190"
  },
  {
    "text": "mathematical proof.\" They accept something\nclose to 100% chance of it",
    "start": "3926190",
    "end": "3932210"
  },
  {
    "text": "being free of all problems. But if you actually look at research,",
    "start": "3932280",
    "end": "3937920"
  },
  {
    "text": "software is full of bugs. Old mathematical theorems\nwhich have been proven for hundreds of years have been discovered",
    "start": "3937920",
    "end": "3944369"
  },
  {
    "text": "to contain bugs on top of\nwhich we generate new proofs and now we have to redo all that.",
    "start": "3944370",
    "end": "3950040"
  },
  {
    "text": "So verifiers are not perfect. Usually they're either a single human",
    "start": "3950040",
    "end": "3955170"
  },
  {
    "text": "or communities of humans and it's basically kinda\nlike a democratic vote. Community of mathematicians agrees",
    "start": "3955170",
    "end": "3961800"
  },
  {
    "text": "that this proof is\ncorrect, mostly correct. Even today we're starting to\nsee some mathematical proofs",
    "start": "3961800",
    "end": "3968310"
  },
  {
    "text": "as so complex, so large that mathematical community\nis unable to make a decision.",
    "start": "3968310",
    "end": "3973890"
  },
  {
    "text": "It looks interesting, it looks promising, but they don't know. They will need years for top scholars",
    "start": "3973890",
    "end": "3978930"
  },
  {
    "text": "to study to figure it out. So of course we can use AI\nto help us with this process,",
    "start": "3978930",
    "end": "3984120"
  },
  {
    "text": "but AI is a piece of software\nwhich needs to be verified. - Just to clarify, so\nverification is the process",
    "start": "3984120",
    "end": "3990390"
  },
  {
    "text": "of saying something is correct,\nsort of the most formal, a mathematical proof\nwhere there's a statement",
    "start": "3990390",
    "end": "3996329"
  },
  {
    "text": "and a series of logical statements\nthat prove that statement to be correct, which is a theorem.",
    "start": "3996330",
    "end": "4003380"
  },
  {
    "text": "And you're saying it gets so\ncomplex that it's possible for the human verifiers,\nthe human beings that verify",
    "start": "4003380",
    "end": "4011510"
  },
  {
    "text": "that the logical step,\nthere's no bugs in it. It becomes impossible. So it's nice to talk about\nverification in this most formal,",
    "start": "4011510",
    "end": "4019640"
  },
  {
    "text": "most clear, most rigorous\nformulation of it, which is mathematical proofs,",
    "start": "4019640",
    "end": "4025147"
  },
  {
    "text": "- Right, and for AI we would like to have that level of confidence",
    "start": "4025147",
    "end": "4031310"
  },
  {
    "text": "for very important\nmission critical software controlling satellites,\nnuclear power plants. For small deterministic programs,",
    "start": "4031310",
    "end": "4037700"
  },
  {
    "text": "we can do this, we can check\nthat code verifies its mapping",
    "start": "4037700",
    "end": "4042700"
  },
  {
    "text": "to the design, whatever\nsoftware engineers intended was correctly implemented.",
    "start": "4043280",
    "end": "4049010"
  },
  {
    "text": "But we don't know how\nto do this for software which keeps learning, self-modifying,",
    "start": "4049010",
    "end": "4055850"
  },
  {
    "text": "rewriting its own code. We don't know how to prove\nthings about the physical world, states of humans in the physical world.",
    "start": "4055850",
    "end": "4063200"
  },
  {
    "text": "So there are papers coming out now and I have this beautiful one \"Towards Guaranteed Safe AI\".",
    "start": "4063200",
    "end": "4070757"
  },
  {
    "text": "Very cool paper. Some of\nthe best outers I ever seen. I think there is multiple\ntouring award winners",
    "start": "4071630",
    "end": "4077690"
  },
  {
    "text": "that is, yeah, quite. You can have this one. One just came out kinda similar",
    "start": "4077690",
    "end": "4083006"
  },
  {
    "text": "\"Managing Extreme Ai Risks\". So all of them expect this level of proof, but I would say that",
    "start": "4083007",
    "end": "4091910"
  },
  {
    "text": "we can get more confidence with more resources we put into it. But at the end of the\nday, we still as reliable",
    "start": "4091910",
    "end": "4098810"
  },
  {
    "text": "as the verifiers. And you have this infinite\nregressive verifiers. The software used to verify a program",
    "start": "4098810",
    "end": "4104989"
  },
  {
    "text": "is itself a piece of program. If aliens gave us well\naligned super intelligence,",
    "start": "4104990",
    "end": "4110870"
  },
  {
    "text": "we can use that to create our own safe AI. But it's a catch 22. You need to have already\nproven to be safe system",
    "start": "4110870",
    "end": "4119029"
  },
  {
    "text": "to verify this new system of\nequal or greater complexity. - You just mentioned this paper",
    "start": "4119030",
    "end": "4124886"
  },
  {
    "text": "\"Towards Guaranteed Safe AI: A Framework for Ensuring Robust\nand Reliable AI Systems\", like you mentioned, it's like a who's who.",
    "start": "4124887",
    "end": "4131779"
  },
  {
    "text": "Josh Tenenbaum, Yoshua Bengio, Stuart Russell, Max Tegmark and many, many, many\nother briilant people.",
    "start": "4131780",
    "end": "4137750"
  },
  {
    "text": "The page you have it open on, \"There are many possible strategies for creating safety specifications.",
    "start": "4137750",
    "end": "4142970"
  },
  {
    "text": "These strategies can roughly\nbe placed on a spectrum, depending on how much\nsafety it would grant",
    "start": "4142970",
    "end": "4148369"
  },
  {
    "text": "if successfully implemented. One way to do this is as follows...\" and there's a set of levels.",
    "start": "4148370",
    "end": "4153469"
  },
  {
    "text": "From \"Level 0: No safety\nspecification is used,\" To \"Level 7: The safety\nspecification completely encodes",
    "start": "4153470",
    "end": "4159469"
  },
  {
    "text": "all things that humans\nmight want in all contexts.\" Where does this paper fall short to you?",
    "start": "4159470",
    "end": "4165830"
  },
  {
    "text": "- So when I wrote a paper, \"Artificial intelligence\nSafety engineering\",",
    "start": "4165830",
    "end": "4171380"
  },
  {
    "text": "which kind of coins the term AI safety, that was 2011. We had 2012 conference,\n2013 journal paper.",
    "start": "4171380",
    "end": "4178279"
  },
  {
    "text": "One of the things I proposed, let's just do formal verifications on it, let's do mathematical formal proofs.",
    "start": "4178280",
    "end": "4183950"
  },
  {
    "text": "In the follow up work, I basically realized it'll\nstill not get us 100%.",
    "start": "4183950",
    "end": "4189350"
  },
  {
    "text": "We can get 99.9, we can put more resources\nexponentially and get closer.",
    "start": "4189350",
    "end": "4194780"
  },
  {
    "text": "But we never get to 100%. If a system makes a\nbillion decisions a second and you use it for 100 years,",
    "start": "4194780",
    "end": "4201559"
  },
  {
    "text": "you're still going to deal with a problem. This is wonderful research.\nI'm so happy they doing it.",
    "start": "4201560",
    "end": "4206660"
  },
  {
    "text": "This is great but it is not\ngoing to be a permanent solution",
    "start": "4206660",
    "end": "4210720"
  },
  {
    "text": "to that problem. - So just to clarify, the task of creating\nan AI verifier is what?",
    "start": "4211850",
    "end": "4217940"
  },
  {
    "text": "Is creating a verifier that the AI system does exactly as it says it does or it sticks within the\nguardrails that it says it must.",
    "start": "4217940",
    "end": "4226160"
  },
  {
    "text": "- There are many, many levels. So first you're verifying the\nhardware in which it is run. You need to verify, you\nknow, communication channel",
    "start": "4226160",
    "end": "4233570"
  },
  {
    "text": "with the human. In every aspect of that whole world model needs to be verified. Somehow it needs to map the\nworld into the world model,",
    "start": "4233570",
    "end": "4242510"
  },
  {
    "text": "map and territory differences. So how do I know internal\nstates of humans? Are you happy or sad? I can't tell.",
    "start": "4242510",
    "end": "4249860"
  },
  {
    "text": "So how do I make proofs\nabout real physical world? Yeah, I can verify that\ndeterministic algorithm",
    "start": "4249860",
    "end": "4256340"
  },
  {
    "text": "follows certain properties. That can be done. Some people argue that maybe just maybe",
    "start": "4256340",
    "end": "4261860"
  },
  {
    "text": "two plus two is not four. I'm not that extreme. But once you have sufficiently large proof",
    "start": "4261860",
    "end": "4268850"
  },
  {
    "text": "over sufficiently complex\nenvironment, the probability that it has zero bugs in\nit is greatly reduced.",
    "start": "4268850",
    "end": "4276260"
  },
  {
    "text": "If you keep deploying this a lot, eventually you're gonna\nhave a bug anyways. - There's always a bug.\n- There is always a bug.",
    "start": "4276260",
    "end": "4283219"
  },
  {
    "text": "And the fundamental difference\nis what I mentioned. We're not dealing with cybersecurity, we're not gonna get a new\ncredit card, new humanity.",
    "start": "4283220",
    "end": "4289550"
  },
  {
    "start": "4289000",
    "end": "5022000"
  },
  {
    "text": "- So this paper's really interesting. You said 2011, \"Artificial\nIntelligence Safety Engineering:\"",
    "start": "4289550",
    "end": "4295370"
  },
  {
    "text": "Why Machine Ethics is a Wrong Approach.\" \"The grand challenge,\" you\nwrite, \"of AI safety engineering,",
    "start": "4295370",
    "end": "4303500"
  },
  {
    "text": "we propose the problem of\ndeveloping safety mechanisms for self-improving systems.\"",
    "start": "4303500",
    "end": "4309107"
  },
  {
    "text": "Self-improving systems, by the way, that's an interesting term for the thing that we're talking about.",
    "start": "4310580",
    "end": "4315802"
  },
  {
    "text": "Is self-improving more\ngeneral than learning.",
    "start": "4318950",
    "end": "4322163"
  },
  {
    "text": "So self-improving, that's\nan interesting term. - You can improve the rate\nat which you are learning, you can become more\nefficient, meta optimizer",
    "start": "4324192",
    "end": "4331469"
  },
  {
    "text": "- The word self, it's like self-replicating,\nself-improving.",
    "start": "4332420",
    "end": "4337583"
  },
  {
    "text": "You can imagine a system\nbuilding its own world on a scale and in a way",
    "start": "4338690",
    "end": "4345032"
  },
  {
    "text": "that is way different than\nthe current systems do. It feels like the current\nsystems are not self-improving",
    "start": "4345033",
    "end": "4350060"
  },
  {
    "text": "or self-replicating or\nself-growing or self-spreading, all that kind of stuff.",
    "start": "4350060",
    "end": "4355820"
  },
  {
    "text": "And once you take that leap, that's when a lot of the\nchallenges seems to happen. Because the kind of bugs you can find now",
    "start": "4355820",
    "end": "4362760"
  },
  {
    "text": "seems more akin to the current\nsort of normal software debugging kind of process.",
    "start": "4363710",
    "end": "4370253"
  },
  {
    "text": "But whenever you can do self replication and arbitrary self-improvement,",
    "start": "4371780",
    "end": "4376973"
  },
  {
    "text": "that's when a bug can become a\nreal problem real, real fast.",
    "start": "4378290",
    "end": "4382583"
  },
  {
    "text": "So what is the difference\nto you between verification of a non self-improving\nsystem versus a verification",
    "start": "4383780",
    "end": "4391489"
  },
  {
    "text": "of a self-improving system? - So if you have fixed code for example, you can verify that code,\nstatic verification at the time.",
    "start": "4391490",
    "end": "4399020"
  },
  {
    "text": "But if it will continue modifying it, you have a much harder time guaranteeing",
    "start": "4399020",
    "end": "4405950"
  },
  {
    "text": "that important properties of that system have not been modified. Then the code changed.",
    "start": "4405950",
    "end": "4411410"
  },
  {
    "text": "- Is it even doable?\n- [Roman] No. - Does the whole process of verification just completely fall apart? - It can always cheat, it\ncan store parts of its code",
    "start": "4411410",
    "end": "4419030"
  },
  {
    "text": "outside in the environment. It can have kind of\nextended mind situation. So this is exactly the type of problems",
    "start": "4419030",
    "end": "4426349"
  },
  {
    "text": "I'm trying to bring up. - What are the classes of verifiers that you write about in the book?",
    "start": "4426350",
    "end": "4431690"
  },
  {
    "text": "Is there interesting ones\nthat stand out to you? Do you have some favorites? - So I like oracle types\nwhere you kind of just know",
    "start": "4431690",
    "end": "4438740"
  },
  {
    "text": "that it's right Turing\nlike oracle machines, they know the right answer. How? Who knows? But they pull it out from somewhere.",
    "start": "4438740",
    "end": "4446239"
  },
  {
    "text": "So you have to trust them. And that's a concern I have\nabout humans in a world",
    "start": "4446240",
    "end": "4451730"
  },
  {
    "text": "with very smart machines. We experiment with them,\nwe see after a while, okay they've always been right before",
    "start": "4451730",
    "end": "4458090"
  },
  {
    "text": "and we start trusting them without any verification\nof what they're saying. - Oh I see that we kind\nof build oracle verifiers",
    "start": "4458090",
    "end": "4467000"
  },
  {
    "text": "or rather we build verifiers\nwe believe to be oracles and then we start to, without any proof,",
    "start": "4467000",
    "end": "4474200"
  },
  {
    "text": "use them as if they're oracle verifiers. - We remove ourselves from that process. We are not scientists\nwho understand the world.",
    "start": "4474200",
    "end": "4480790"
  },
  {
    "text": "We are humans who get\nnew data presented to us. - Okay, one really cool class of verifiers",
    "start": "4480790",
    "end": "4488240"
  },
  {
    "text": "is a self-verifier. Is it possible that you somehow\nengineer into AI systems",
    "start": "4488240",
    "end": "4494840"
  },
  {
    "text": "the thing that constantly verifies itself? - Preserved portion of it can be done. But in terms of mathematical verification,",
    "start": "4494840",
    "end": "4502429"
  },
  {
    "text": "it's kinda useless. You saying you have a\ngreatest guy in the world because you are saying it, it's circular",
    "start": "4502430",
    "end": "4507860"
  },
  {
    "text": "and not very helpful but it's consistent. We know that within that world you have verified that system.",
    "start": "4507860",
    "end": "4514100"
  },
  {
    "text": "In a paper I try to kind of brute force all possible verifiers. It doesn't mean that this wasn't",
    "start": "4514100",
    "end": "4520099"
  },
  {
    "text": "particularly important to us. - But what about like self-doubt? Like the kind of verification\nwhere you said, you say,",
    "start": "4520100",
    "end": "4527869"
  },
  {
    "text": "or I say I'm the greatest\nguy in the world. What about a thing which\nI actually have is a voice",
    "start": "4527870",
    "end": "4533390"
  },
  {
    "text": "that is constantly extremely critical. So like engineer into the system,",
    "start": "4533390",
    "end": "4539780"
  },
  {
    "text": "a constant uncertainty about\nself, a constant doubt.",
    "start": "4539780",
    "end": "4544780"
  },
  {
    "text": "- Well any smart system would have doubt about everything, right? You not sure if what information\nyou are given is true",
    "start": "4545390",
    "end": "4553070"
  },
  {
    "text": "if you are subject to manipulation. You have this safety and security mindset.",
    "start": "4553070",
    "end": "4558830"
  },
  {
    "text": "- But I mean you have\ndoubt about yourself. So the AI systems that has a doubt about",
    "start": "4558830",
    "end": "4566526"
  },
  {
    "text": "whether the thing is doing is causing harm, is the\nright thing to be doing.",
    "start": "4566526",
    "end": "4572210"
  },
  {
    "text": "So just a constant doubt\nabout what it's doing because it's hard to be\na dictator full of doubt.",
    "start": "4572210",
    "end": "4578450"
  },
  {
    "text": "- I may be wrong, but I\nthink Stuart Russell's ideas are all about machines\nwhich are uncertain about",
    "start": "4578450",
    "end": "4585680"
  },
  {
    "text": "what humans want and trying\nto learn better and better what we want. The problem of course is\nwe don't know what we want",
    "start": "4585680",
    "end": "4591424"
  },
  {
    "text": "and we don't agree on it. - Yeah, but uncertainty,\nhis ideas that having that like self-doubt uncertainty\nin AI systems engineer",
    "start": "4591424",
    "end": "4600050"
  },
  {
    "text": "and TI systems is one way to\nsolve the control problem. - It could also backfire. Maybe you are uncertain about\ncompleting your mission.",
    "start": "4600050",
    "end": "4607520"
  },
  {
    "text": "Like I am paranoid about, your camera is not recording right now. So I would feel much better\nif you had a secondary camera.",
    "start": "4607520",
    "end": "4614510"
  },
  {
    "text": "But I also would feel even\nbetter if you had a third and eventually I would turn\nthis whole world into cameras,",
    "start": "4614510",
    "end": "4621080"
  },
  {
    "text": "pointing at us, making\nsure we're capTuring this. - No, but wouldn't you have a meta concern",
    "start": "4621080",
    "end": "4627690"
  },
  {
    "text": "like that you just stated that eventually there'll\nbe way too many cameras? So you would be able to keep\nzooming on the big picture",
    "start": "4629120",
    "end": "4637679"
  },
  {
    "text": "of your concerns. - So it's a multi objective optimization.",
    "start": "4638990",
    "end": "4644300"
  },
  {
    "text": "It depends how much I value capTuring this versus not destroying the universe.",
    "start": "4644300",
    "end": "4649640"
  },
  {
    "text": "- Right, exactly, and then you'll also ask about like what does it mean to destroy the universe\nand how many universes are,",
    "start": "4649640",
    "end": "4656270"
  },
  {
    "text": "and you keep asking that question but that doubting\nyourself would prevent you from destroying the universe",
    "start": "4656270",
    "end": "4662599"
  },
  {
    "text": "'cause you're constantly full of doubt. It might affect your productivity. - It might be scared to do anything.",
    "start": "4662600",
    "end": "4668329"
  },
  {
    "text": "- It's just scared to do anything. - [Roman] Mess things up. - Well that's better. I\nmean I guess the question is is it possible to engineer that in?",
    "start": "4668330",
    "end": "4675320"
  },
  {
    "text": "I guess your answer would be yes, but we don't know how to do that and we need to invest a lot of effort into figuring out how to do that.",
    "start": "4675320",
    "end": "4680810"
  },
  {
    "text": "But it's unlikely. Underpinning a lot of\nyour writing is this sense",
    "start": "4680810",
    "end": "4687310"
  },
  {
    "text": "that we're screwed, but it just feels like it's\nan engineering problem.",
    "start": "4687740",
    "end": "4694910"
  },
  {
    "text": "I don't understand why we're screwed it. Time and time again, humanity\nhas gotten itself into trouble",
    "start": "4694910",
    "end": "4701840"
  },
  {
    "text": "and figured out a way\nto get out of trouble. - We are in a situation where people making more capable systems",
    "start": "4701840",
    "end": "4709250"
  },
  {
    "text": "just need more resources. They don't need to invent\nanything in my opinion.",
    "start": "4709250",
    "end": "4714380"
  },
  {
    "text": "Some will disagree, but so far at least I don't\nsee diminishing returns. If you have 10X compute,\nyou'll get better performance.",
    "start": "4714380",
    "end": "4721910"
  },
  {
    "text": "The same doesn't apply to safety. If you give Miri or any other organization",
    "start": "4721910",
    "end": "4727220"
  },
  {
    "text": "10 times the money, they don't\noutput 10 times the safety and the gap became between capabilities",
    "start": "4727220",
    "end": "4733550"
  },
  {
    "text": "and safety becomes bigger\nand bigger all the time. So it's hard to be completely optimistic",
    "start": "4733550",
    "end": "4739220"
  },
  {
    "text": "about our results here. I can name 10 excellent\nbreakthrough papers",
    "start": "4739220",
    "end": "4745640"
  },
  {
    "text": "in machine learning. I would struggle to name equally important breakthroughs in safety.",
    "start": "4745640",
    "end": "4751010"
  },
  {
    "text": "A lot of times a safety paper\nwill propose a toy solution and point out 10 new problems\ndiscovered as a result.",
    "start": "4751010",
    "end": "4758060"
  },
  {
    "text": "It's like this fractal, you zooming in and you see more problems and it's infinite in all directions.",
    "start": "4758060",
    "end": "4764119"
  },
  {
    "text": "- Does this apply to other technologies or is this unique to AI",
    "start": "4764120",
    "end": "4769160"
  },
  {
    "text": "where safety is always lagging behind? - So I guess we can look\nat related technologies",
    "start": "4769160",
    "end": "4775940"
  },
  {
    "text": "with cybersecurity, right? We did manage to have banks\nand casinos and bitcoin.",
    "start": "4775940",
    "end": "4781942"
  },
  {
    "text": "So you can have secure narrow systems which are doing okay,\nnarrow attacks on them fail.",
    "start": "4781942",
    "end": "4790990"
  },
  {
    "text": "But you can always go\noutside, outside of a box. So if I can hack you\nbitcoin, I can hack you.",
    "start": "4791450",
    "end": "4797570"
  },
  {
    "text": "So there is always something,\nif I really want it, I will find a different way. We talk about guardrails\nfor AI, well that's a fence.",
    "start": "4797570",
    "end": "4805940"
  },
  {
    "text": "I can dig a tunnel under\nit, I can jump over it, I can climb it, I can walk around it. You may have a very nice guardrail,",
    "start": "4805940",
    "end": "4812660"
  },
  {
    "text": "but in a real world it's not a permanent guarantee of safety. And again, this is a\nfundamental difference.",
    "start": "4812660",
    "end": "4819080"
  },
  {
    "text": "We're not saying we need to be 90% safe to get those trillions\nof dollars of benefit.",
    "start": "4819080",
    "end": "4825350"
  },
  {
    "text": "We need to be 100% indefinitely or we might lose the principle.",
    "start": "4825350",
    "end": "4830540"
  },
  {
    "text": "- So if you look at just\nhumanity's a set of machines",
    "start": "4830540",
    "end": "4835500"
  },
  {
    "text": "is the machinery of AI safety conflicting with the\nmachinery of capitalism?",
    "start": "4837547",
    "end": "4844760"
  },
  {
    "text": "- I think we can generalize it to just prisoner's dilemma in general.",
    "start": "4844760",
    "end": "4849920"
  },
  {
    "text": "Personal self-interest\nversus group interest. The incentives as such",
    "start": "4849920",
    "end": "4855680"
  },
  {
    "text": "that everyone wants what's best for them. Capitalism obviously has that tendency",
    "start": "4855680",
    "end": "4862190"
  },
  {
    "text": "to maximize your personal gain, which does create this race to the bottom.",
    "start": "4862190",
    "end": "4869690"
  },
  {
    "text": "I don't have to be a lot better than you, but if I'm 1% better than you,",
    "start": "4869690",
    "end": "4875300"
  },
  {
    "text": "I'll capture more of a profit. So it's worth for me personally to take the risk even if society",
    "start": "4875300",
    "end": "4881990"
  },
  {
    "text": "as a whole will suffer as a result. - So capitalism has created\na lot of good in this world.",
    "start": "4881990",
    "end": "4887940"
  },
  {
    "text": "It's not clear to me that\nAI safety is not aligned with the function of capitalism",
    "start": "4891290",
    "end": "4896840"
  },
  {
    "text": "unless AI safety is so difficult that it requires the complete\nhalt of the development,",
    "start": "4896840",
    "end": "4903233"
  },
  {
    "text": "which is also a possibility. It just feels like building safe systems",
    "start": "4904100",
    "end": "4909230"
  },
  {
    "text": "should be the desirable thing\nto do for tech companies.",
    "start": "4909230",
    "end": "4913583"
  },
  {
    "text": "- Right, look at governance structures, then you have someone with complete power.",
    "start": "4914600",
    "end": "4919880"
  },
  {
    "text": "They're extremely dangerous. So the solution we came\nup with is break it up. You have judicial, legislative,\nexecutive, same here,",
    "start": "4919880",
    "end": "4927740"
  },
  {
    "text": "have narrow AI systems, work on important problems,\nsolve immortality. It's a biological problem we can solve",
    "start": "4927740",
    "end": "4936800"
  },
  {
    "text": "similar to how progress was\nmade with protein folding, using a system which\ndoesn't also play chess.",
    "start": "4936800",
    "end": "4943073"
  },
  {
    "text": "There is no reason to create\nsuper intelligent system to get most of the benefits we want",
    "start": "4944210",
    "end": "4950240"
  },
  {
    "text": "from much safer, narrow systems. - It really is a question to me whether companies are\ninterested in creating",
    "start": "4950240",
    "end": "4959510"
  },
  {
    "text": "anything but narrow AI. I think when term AGI is\nused by tech companies,",
    "start": "4959510",
    "end": "4965750"
  },
  {
    "text": "they mean narrow AI. They mean narrow AI with\namazing capabilities.",
    "start": "4965750",
    "end": "4974003"
  },
  {
    "text": "I do think that there's\na leap between narrow AI with amazing capabilities\nwith superhuman capabilities",
    "start": "4975860",
    "end": "4982127"
  },
  {
    "text": "and the kind of self-motivated agent like AGI system that we're talking about.",
    "start": "4982127",
    "end": "4989300"
  },
  {
    "text": "I don't know if it's\nobvious to me that a company would want to take the\nleap to creating an AGI",
    "start": "4989300",
    "end": "4996860"
  },
  {
    "text": "that it would lose control of because then it can't capture\nthe value from that system.",
    "start": "4996860",
    "end": "5003070"
  },
  {
    "text": "- Like bragging rights but being first that is the same humans who\nare in charge of those systems.",
    "start": "5003070",
    "end": "5010800"
  },
  {
    "text": "- So that jumps from the\nincentives of capitalism to human nature.",
    "start": "5010800",
    "end": "5016120"
  },
  {
    "text": "And so there the question\nis whether human nature will override the interest of the company.",
    "start": "5016120",
    "end": "5021910"
  },
  {
    "text": "So you've mentioned slowing\nor halting progress.",
    "start": "5021910",
    "end": "5026653"
  },
  {
    "start": "5022000",
    "end": "5399000"
  },
  {
    "text": "Is that one possible solution or your proponent of\npausing development of AI, whether it's for six months or completely.",
    "start": "5027520",
    "end": "5034243"
  },
  {
    "text": "- The condition would be\nnot time but capabilities. Pause until you can do X, Y, Z.",
    "start": "5035080",
    "end": "5042250"
  },
  {
    "text": "And if I'm right and you\ncannot, it's impossible, then it becomes a permanent ban.",
    "start": "5042250",
    "end": "5047500"
  },
  {
    "text": "But if you write and it's possible, so as soon as you have those\nsafety capabilities, go ahead.",
    "start": "5047500",
    "end": "5052960"
  },
  {
    "text": "- Right, so is there any\nactual explicit capabilities",
    "start": "5052960",
    "end": "5057960"
  },
  {
    "text": "that you can put on paper that we as a human civilization\ncould put on paper? Is it possible to make\nit explicit like that?",
    "start": "5059800",
    "end": "5066070"
  },
  {
    "text": "Like versus kind of a vague notion of just like you said, it's very vague.",
    "start": "5066070",
    "end": "5071860"
  },
  {
    "text": "We want to the AI systems to do good and want them to be safe. Those are very vague notions.\nIs there more formal notions?",
    "start": "5071860",
    "end": "5078790"
  },
  {
    "text": "- So when I think about this problem, I think about having a toolbox. I would need capabilities\nsuch as explaining everything",
    "start": "5078790",
    "end": "5087970"
  },
  {
    "text": "about that system's design and workings, predicting not just terminal goal",
    "start": "5087970",
    "end": "5093610"
  },
  {
    "text": "but all the intermediate\nsteps of a system. Control in terms of either direct control,",
    "start": "5093610",
    "end": "5101080"
  },
  {
    "text": "some sort of a hybrid\noption, ideal advisor, doesn't matter which one you pick, but you have to be able to achieve it.",
    "start": "5101080",
    "end": "5108430"
  },
  {
    "text": "In a book we talk about\nothers, verification is another very important tool.",
    "start": "5108430",
    "end": "5113923"
  },
  {
    "text": "Communication without ambiguity, human language is ambiguous. That's another source of danger.",
    "start": "5115840",
    "end": "5121179"
  },
  {
    "text": "So basically there is a paper we published in \"ACM Surveys\" which\nlooks at about 50 different",
    "start": "5121180",
    "end": "5129040"
  },
  {
    "text": "and possibility results, which may or may not be relevant to this problem. But we don't have enough\nhuman resources to investigate",
    "start": "5129040",
    "end": "5136480"
  },
  {
    "text": "all of them for relevance to AI safety. The ones I mentioned to you, I definitely think would be handy.",
    "start": "5136480",
    "end": "5141699"
  },
  {
    "text": "And that's what we see AI\nsafety researchers working on. Explainability is a huge one.",
    "start": "5141700",
    "end": "5147430"
  },
  {
    "text": "The problem is that it's very hard to separate capabilities\nwork from safety work.",
    "start": "5147430",
    "end": "5153760"
  },
  {
    "text": "If you make good progress\nin explainability, now the system itself can engage",
    "start": "5153760",
    "end": "5158830"
  },
  {
    "text": "in self-improvement much easier, increasing capability greatly. So it's not obvious that\nthere is any research",
    "start": "5158830",
    "end": "5167110"
  },
  {
    "text": "which is pure safety work\nwithout disproportionate increase in capability and danger.",
    "start": "5167110",
    "end": "5173110"
  },
  {
    "text": "- Explainability is really interesting. Why is that connected\nto you to capability? If it's able to explain itself well",
    "start": "5173110",
    "end": "5179140"
  },
  {
    "text": "why does that naturally\nmean that it's more capable? - Right now it's comprised of\nweights on a neural network.",
    "start": "5179140",
    "end": "5185530"
  },
  {
    "text": "If it can convert it to\nmanipulatable code like software, it's a lot easier to\nwork in self-improvement.",
    "start": "5185530",
    "end": "5192280"
  },
  {
    "text": "- I see, so it... - You can do intelligent design instead of evolutionary gradual descent.",
    "start": "5192280",
    "end": "5199810"
  },
  {
    "text": "- Well you could probably\ndo human feedback, human alignment more effectively if it's able to be explainable.",
    "start": "5199810",
    "end": "5206260"
  },
  {
    "text": "If it's able to convert the waste into human understandable form, then you could probably have\nhumans interact with it better.",
    "start": "5206260",
    "end": "5212890"
  },
  {
    "text": "Do you think there's hope that we can make AI systems explainable? - Not completely, so if\nthey're sufficiently large,",
    "start": "5212890",
    "end": "5221023"
  },
  {
    "text": "you simply don't have the\ncapacity to comprehend what all the trillions\nof connections represent.",
    "start": "5221860",
    "end": "5230020"
  },
  {
    "text": "Again, you can obviously get\na very useful explanation which talks about top\nmost important features",
    "start": "5230020",
    "end": "5236590"
  },
  {
    "text": "which contribute to the decision. But the only true explanation\nis the model itself.",
    "start": "5236590",
    "end": "5240793"
  },
  {
    "text": "- So deception could be part\nof the explanation, right? So you can never prove that\nthere is some deception",
    "start": "5242320",
    "end": "5248739"
  },
  {
    "text": "in the network explaining itself. - Absolutely and you can\nprobably have targeted deception",
    "start": "5249621",
    "end": "5255790"
  },
  {
    "text": "where different individuals\nwill understand explanation in different ways based on\ntheir cognitive capability.",
    "start": "5255790",
    "end": "5262360"
  },
  {
    "text": "So while what you're\nsaying may be the same and true in some situations,\nours will be deceived by it.",
    "start": "5262360",
    "end": "5268750"
  },
  {
    "text": "- So it's impossible for an AI system to be truly fully explainable\nin the way that we mean,",
    "start": "5268750",
    "end": "5275563"
  },
  {
    "text": "honestly and perfectly. - at the extreme the systems which are narrow and less complex could be\nunderstood pretty well.",
    "start": "5276580",
    "end": "5283600"
  },
  {
    "text": "- If it's impossible to\nbe perfectly explainable. Is there a hopeful perspective on that? Like it's impossible to\nbe perfectly explainable,",
    "start": "5283600",
    "end": "5290680"
  },
  {
    "text": "but you can explain most of\nthe important stuff, mostly. You can ask a system\nwhat are the worst ways",
    "start": "5290680",
    "end": "5297880"
  },
  {
    "text": "you can hurt humans? And it'll answer honestly. - Any work in a safety direction right now",
    "start": "5297880",
    "end": "5304660"
  },
  {
    "text": "seems like a good idea because we are not slowing down. I'm not for a second thinking",
    "start": "5304660",
    "end": "5311380"
  },
  {
    "text": "that my message or anyone\nelse's will be heard and will be a sane\ncivilization which decides",
    "start": "5311380",
    "end": "5318370"
  },
  {
    "text": "not to kill itself by\ncreating its own replacements. - The pausing of development\nis an impossible thing for you.",
    "start": "5318370",
    "end": "5325450"
  },
  {
    "text": "- Again, it's always limited by either geographic constraints. Pause in US, pause in China.",
    "start": "5325450",
    "end": "5331900"
  },
  {
    "text": "So there are other jurisdictions as the scale of a project becomes smaller.",
    "start": "5331900",
    "end": "5337690"
  },
  {
    "text": "So right now it's like\nManhattan project scale in terms of course and people. But if five years from now\ncompute is available on a desktop",
    "start": "5337690",
    "end": "5347110"
  },
  {
    "text": "to do it, regulation will not help. You can't control it as easy. Any kid in the garage can train a model.",
    "start": "5347110",
    "end": "5354190"
  },
  {
    "text": "So a lot of it is in my\nopinion, just safety theater, security theater where we saying,",
    "start": "5354190",
    "end": "5360287"
  },
  {
    "text": "\"Oh, it's illegal to train\nmodels so big,\" okay, well - So okay, that's security theater",
    "start": "5360287",
    "end": "5367540"
  },
  {
    "text": "and is government regulation\nalso security theater? - Given that a lot of the\nterms are not well defined",
    "start": "5367540",
    "end": "5374710"
  },
  {
    "text": "and really cannot be\nenforced in real life, we don't have ways to monitor\ntraining runs meaningfully,",
    "start": "5374710",
    "end": "5381670"
  },
  {
    "text": "live while they take place. There are limits to testing\nfor capabilities I mentioned.",
    "start": "5381670",
    "end": "5386770"
  },
  {
    "text": "So a lot of it cannot be enforced. Do I strongly support all that\nregulation? Yes, of course.",
    "start": "5386770",
    "end": "5392470"
  },
  {
    "text": "Any type of red tape will slow it down and take money away from\ncompute towards lawyers.",
    "start": "5392470",
    "end": "5396960"
  },
  {
    "text": "- Can you help me understand\nwhat is the hopeful path here for you solution wise out of this?",
    "start": "5397870",
    "end": "5405100"
  },
  {
    "start": "5399000",
    "end": "5983000"
  },
  {
    "text": "It sounds like you're saying AI systems in the end are\nunverifiable, unpredictable,",
    "start": "5405100",
    "end": "5412630"
  },
  {
    "text": "as the book says\nunexplainable, uncontrollable.",
    "start": "5412630",
    "end": "5416787"
  },
  {
    "text": "- That's the big one. - Uncontrollable and all the other uns just make it difficult to avoid",
    "start": "5418330",
    "end": "5424239"
  },
  {
    "text": "getting to the uncontrollable I guess. But once it's uncontrollable then it it goes wild.",
    "start": "5424240",
    "end": "5429523"
  },
  {
    "text": "Surely there's solutions.\nHumans are pretty smart. What are possible solutions?",
    "start": "5430660",
    "end": "5437230"
  },
  {
    "text": "Like if you are a dictator of the world, what do we do? - So the smart thing is not to build something you cannot control,",
    "start": "5437230",
    "end": "5444550"
  },
  {
    "text": "you cannot understand. Build what you can and benefit from it. I'm a big believer in\npersonal self-interest.",
    "start": "5444550",
    "end": "5450970"
  },
  {
    "text": "A lot of guys running those companies are young, rich people. What do they have to gain beyond billions",
    "start": "5450970",
    "end": "5458559"
  },
  {
    "text": "they already have financially, right? It's not a requirement that\nthey press that button.",
    "start": "5458560",
    "end": "5464110"
  },
  {
    "text": "They can easily wait a long time. They can just choose not to do it. And still have a amazing life.",
    "start": "5464110",
    "end": "5470473"
  },
  {
    "text": "In history, a lot of times if\nyou did something really bad, at least you became part of history books,",
    "start": "5471370",
    "end": "5477250"
  },
  {
    "text": "there is a chance in this case\nthere won't be any history. - So you're saying the individuals\nrunning these companies",
    "start": "5477250",
    "end": "5484960"
  },
  {
    "text": "should do some soul searching and what? And stop development? - Well either they have to prove",
    "start": "5484960",
    "end": "5491260"
  },
  {
    "text": "that of course it's possible to indefinitely control godlike\nsuper intelligent machines by humans and ideally let us know how",
    "start": "5491260",
    "end": "5499990"
  },
  {
    "text": "or agree that it's not possible and it's a very bad idea to do it, including for them\npersonally and their families",
    "start": "5499990",
    "end": "5506050"
  },
  {
    "text": "and friends and capital. - So what do you think the actual meetings inside these companies look like?",
    "start": "5506050",
    "end": "5513013"
  },
  {
    "text": "Don't you think they're all the engineers? Really it is the engineers\nthat make this happen. They're not like automatons.\nThey're human beings.",
    "start": "5513850",
    "end": "5521079"
  },
  {
    "text": "They're brilliant human beings. So they're nonstop asking how do we make sure this is safe?",
    "start": "5521080",
    "end": "5528130"
  },
  {
    "text": "- So again, I'm not inside. From outside, it seems like there is a certain filtering going on",
    "start": "5528130",
    "end": "5534460"
  },
  {
    "text": "and restrictions and criticism\nand what they can say. And everyone who was\nworking in charge of safety",
    "start": "5534460",
    "end": "5541090"
  },
  {
    "text": "and whose responsibility it was to protect us said, \"You know what? I'm going home.\"",
    "start": "5541090",
    "end": "5545857"
  },
  {
    "text": "So that's not encouraging. - What do you think the discussion inside those companies look like?",
    "start": "5546700",
    "end": "5553510"
  },
  {
    "text": "You're developing your training GPT-5, you're training Gemini,",
    "start": "5553510",
    "end": "5559420"
  },
  {
    "text": "you're training Claude and Groq. Don't you think they're\nconstantly like underneath it,",
    "start": "5559420",
    "end": "5565960"
  },
  {
    "text": "maybe it's not made explicit, but you're constantly\nsort of wondering like",
    "start": "5565960",
    "end": "5569380"
  },
  {
    "text": "where's the system currently stand? Where do the possible\nunderstanding consequences? Where are the limits?",
    "start": "5571000",
    "end": "5578110"
  },
  {
    "text": "Where are the bugs? The\nsmall and the big bugs? That's the constant thing that\nengineers are worried about.",
    "start": "5578110",
    "end": "5584800"
  },
  {
    "text": "So like I think super\nalignment is not quite the same",
    "start": "5584800",
    "end": "5589800"
  },
  {
    "text": "as the kind of thing I'm referring to, which engineers are worried about.",
    "start": "5590920",
    "end": "5596050"
  },
  {
    "text": "Super alignment is saying for future systems that\nwe don't quite yet have,",
    "start": "5596050",
    "end": "5602770"
  },
  {
    "text": "how do we keep them safe? You are trying to be a step ahead. It's a different kind of problem",
    "start": "5602770",
    "end": "5609310"
  },
  {
    "text": "because it is almost more philosophical. It's a really tricky one because like",
    "start": "5609310",
    "end": "5612810"
  },
  {
    "text": "you're trying to prevent future systems from escaping control of humans.",
    "start": "5614932",
    "end": "5621370"
  },
  {
    "text": "That's really, I don't\nthink there's been... Man is there anything akin to\nit in the history of humanity?",
    "start": "5621370",
    "end": "5628929"
  },
  {
    "text": "I don't think so, right?\n- Climate change. - But there's a entire\nsystem which is climate,",
    "start": "5628930",
    "end": "5634390"
  },
  {
    "text": "which is incredibly complex, which we have only tiny control of, right?",
    "start": "5634390",
    "end": "5641440"
  },
  {
    "text": "It's its own system. In this case we are building the system. And so how do you keep that system",
    "start": "5642850",
    "end": "5650260"
  },
  {
    "text": "from becoming destructive? That's a really difficult\ndifferent problem than the current meetings\nthat companies are having.",
    "start": "5650260",
    "end": "5658000"
  },
  {
    "text": "Where the engineers are saying, \"Okay, like how powerful is this thing? How does it go wrong?\"",
    "start": "5658000",
    "end": "5664177"
  },
  {
    "text": "And as we train GPT-5 and train up future systems, like where are the ways they can go wrong?",
    "start": "5665260",
    "end": "5670900"
  },
  {
    "text": "Don't you think all those\nengineers are constantly worrying about this, thinking about this,",
    "start": "5670900",
    "end": "5676270"
  },
  {
    "text": "which is a little bit different than the super alignment\nteam that's thinking a little bit farther into the future.",
    "start": "5676270",
    "end": "5682660"
  },
  {
    "text": "- Well I think a lot of people who historically worked\non AI never considered",
    "start": "5682660",
    "end": "5691500"
  },
  {
    "text": "what happens when they succeed. Stuart Russell speaks\nbeautifully about that.",
    "start": "5691510",
    "end": "5696193"
  },
  {
    "text": "Let's look okay, maybe super\nintelligence is too futuristic. We can develop practical tools for it.",
    "start": "5697660",
    "end": "5703060"
  },
  {
    "text": "Let's look at software today. What is the state of safety and security of our user software?",
    "start": "5703060",
    "end": "5710200"
  },
  {
    "text": "Things we give to millions of\npeople. There is no liability. You click I Agree.",
    "start": "5710200",
    "end": "5715930"
  },
  {
    "text": "What are you agreeing to?\nNobody knows, nobody reads. But you're basically\nsaying it'll spy on you, corrupt your data, kill\nyour firstborn and you agree",
    "start": "5715930",
    "end": "5723564"
  },
  {
    "text": "and you're not gonna sue the company. That's the best they can\ndo for mundane software,",
    "start": "5723564",
    "end": "5728590"
  },
  {
    "text": "word processor, tax software. No liability, no responsibility. Just as long as you agree not\nto sue us, you can use it.",
    "start": "5728590",
    "end": "5736570"
  },
  {
    "text": "If this is a state of the art in systems which are narrow accountants,\nstable manipulators,",
    "start": "5736570",
    "end": "5743170"
  },
  {
    "text": "why do we think we can do so much better with much more complex systems\nacross multiple domains",
    "start": "5743170",
    "end": "5750159"
  },
  {
    "text": "in the environment with malevolent actors? With again, self-improvement\nwith capabilities",
    "start": "5750160",
    "end": "5756370"
  },
  {
    "text": "exceeding those of\nhumans thinking about it. - I mean the liability\nthing is more about lawyers",
    "start": "5756370",
    "end": "5762130"
  },
  {
    "text": "than killing firstborns. But if Clippy actually killed the child,",
    "start": "5762130",
    "end": "5768070"
  },
  {
    "text": "I think lawyers aside, it would end Clippy and the company that owns Clippy, right?",
    "start": "5768070",
    "end": "5774610"
  },
  {
    "text": "So it's not so much about... There's two points to be made.",
    "start": "5774610",
    "end": "5780160"
  },
  {
    "text": "One is like man, current software systems that are full of bugs and\nthey could do a lot of damage",
    "start": "5780160",
    "end": "5787990"
  },
  {
    "text": "and we don't know what\nkind, they're unpredictable. There's so much damage\nthey could possibly do.",
    "start": "5787990",
    "end": "5793360"
  },
  {
    "text": "And then we kind of live\nin this blissful illusion that everything is great\nand perfect and it works.",
    "start": "5793360",
    "end": "5800113"
  },
  {
    "text": "It's nevertheless, it's\nstill somehow works. - Many domains we see car\nmanufacTuring, drug development,",
    "start": "5801340",
    "end": "5808090"
  },
  {
    "text": "the burden of proof is on\nthe manufacturer of product or service to show their\nproduct or service is safe.",
    "start": "5808090",
    "end": "5814150"
  },
  {
    "text": "It is not up to the user to\nprove that there are problems. They have to do\nappropriate safety studies.",
    "start": "5814150",
    "end": "5822430"
  },
  {
    "text": "We have to get government\napproval for selling the product. And we are still fully\nresponsible for what happens.",
    "start": "5822430",
    "end": "5827710"
  },
  {
    "text": "We don't see any of that here. They can deploy whatever they want. And I have to explain how that system",
    "start": "5827710",
    "end": "5834219"
  },
  {
    "text": "is going to kill everyone. I don't work for that company. You have to explain to me how it's definitely cannot mess up.",
    "start": "5834220",
    "end": "5841540"
  },
  {
    "text": "- That's because it's the very early days of such a technology, government\nregulations lagging behind.",
    "start": "5841540",
    "end": "5847060"
  },
  {
    "text": "They're really not tech savvy. A regulation of any kind of software. If you look at like Congress\ntalking about social media",
    "start": "5847060",
    "end": "5853570"
  },
  {
    "text": "and whenever Mark Zuckerberg\nand other CEOs show up, the cluelessness that congress has",
    "start": "5853570",
    "end": "5860110"
  },
  {
    "text": "about how technology works is incredible, it's heartbreaking, honestly.",
    "start": "5860110",
    "end": "5865480"
  },
  {
    "text": "- I agree completely, but\nthat's what scares me. The response is when they\nstart to get dangerous,",
    "start": "5865480",
    "end": "5870917"
  },
  {
    "text": "\"We'll really get it together. The politicians will pass the right laws, engineers will solve the right problems.\"",
    "start": "5870917",
    "end": "5876547"
  },
  {
    "text": "We are not that good at\nmany of those things, we take forever. And we are not early,\nwe are two years away",
    "start": "5877390",
    "end": "5884769"
  },
  {
    "text": "according to prediction markets. This is not a biased CEO of fundraising. This is what smartest\npeople super forecasters",
    "start": "5884770",
    "end": "5892090"
  },
  {
    "text": "are thinking of this problem. - I'd like to push back\nabout those predict...",
    "start": "5892090",
    "end": "5898570"
  },
  {
    "text": "I wonder what those prediction\nmarkets are are about, how they define AGI? Because that's wild to me.",
    "start": "5898570",
    "end": "5904030"
  },
  {
    "text": "And I wanna know what they\nsaid about autonomous vehicles. 'Cause I've heard a lot of\nfinancial experts talk about",
    "start": "5904030",
    "end": "5911260"
  },
  {
    "text": "autonomous vehicles and\nhow it's going to be a multi-trillion dollar industry\nand all this kind of stuff.",
    "start": "5911260",
    "end": "5916662"
  },
  {
    "text": "- It's a small fund, but\nif you have good vision, maybe you can zoom in on that and see the prediction dates. - Oh there's a plot.",
    "start": "5919191",
    "end": "5925317"
  },
  {
    "text": "- I have a large one if\nyou interested, but... - I guess my fundamental\nquestion is how often",
    "start": "5925317",
    "end": "5930850"
  },
  {
    "text": "they write about technology. I definitely do...",
    "start": "5930850",
    "end": "5936430"
  },
  {
    "text": "- There are studies on\ntheir accuracy rates and all that, you can look it up. Even if they're wrong,",
    "start": "5936430",
    "end": "5941650"
  },
  {
    "text": "I'm just saying this is\nright now the best we have, this is what humanity came up\nwith as the predicted date.",
    "start": "5941650",
    "end": "5948010"
  },
  {
    "text": "- But again what they mean by\nAGI is really important there because there's the non-agent like AGI",
    "start": "5948010",
    "end": "5954803"
  },
  {
    "text": "and then there's the agent like AGI and I don't think it's\nas trivial as a wrapper,",
    "start": "5955972",
    "end": "5961210"
  },
  {
    "text": "putting a wrapper around... One has lipstick and all it\ntakes is to remove the lipstick.",
    "start": "5961210",
    "end": "5968050"
  },
  {
    "text": "I don't think it's that trivial. - You may be completely right, but what probability would you assign it? You may be 10% wrong.,",
    "start": "5968050",
    "end": "5974560"
  },
  {
    "text": "but we're betting all of\nhumanity on this distribution. It seems irrational.",
    "start": "5974560",
    "end": "5979660"
  },
  {
    "text": "- Yeah, it's definitely\nnot like one or 0%. Yeah. What are your thoughts by the way,",
    "start": "5979660",
    "end": "5985090"
  },
  {
    "start": "5983000",
    "end": "6305000"
  },
  {
    "text": "about current systems, where they stand? So GPT-40, Claude 3, Groq, Gemini,",
    "start": "5985090",
    "end": "5994830"
  },
  {
    "text": "like on the path to super intelligence, to agent-like super\nintelligence, where are we?",
    "start": "5996760",
    "end": "6002583"
  },
  {
    "text": "- I think they all about the same. Obviously there are nuanced differences but in terms of capability,",
    "start": "6003680",
    "end": "6009180"
  },
  {
    "text": "I don't see a huge\ndifference between them. As I said, in my opinion,\nacross all possible tasks,",
    "start": "6009180",
    "end": "6016620"
  },
  {
    "text": "they exceed performance\nof an average person. I think they're starting to be better",
    "start": "6016620",
    "end": "6021780"
  },
  {
    "text": "than an average master's\nstudent at my university. But they still have very big limitations.",
    "start": "6021780",
    "end": "6029070"
  },
  {
    "text": "If the next model is as\nimproved as GPT-4 versus GPT-3,",
    "start": "6029070",
    "end": "6034070"
  },
  {
    "text": "we may see something\nvery, very, very capable. - What do you feel about all this? I mean you've been\nthinking about AI safety",
    "start": "6035460",
    "end": "6042480"
  },
  {
    "text": "for a long, long time. And at least for me the leaps,",
    "start": "6042480",
    "end": "6048150"
  },
  {
    "text": "I mean it probably started with... AlphaZero was mind blowing for me",
    "start": "6048150",
    "end": "6055207"
  },
  {
    "text": "and then the breakthroughs\nwith LLMs, even GPT-2, but like just the breakthroughs on LLMs,",
    "start": "6056310",
    "end": "6062460"
  },
  {
    "text": "just mind blowing to me. What does it feel like to be\nliving in this day and age where all this talk about AGIs feels",
    "start": "6062460",
    "end": "6070050"
  },
  {
    "text": "like it actually might happen and quite soon, meaning\nwithin our lifetime,",
    "start": "6070050",
    "end": "6077250"
  },
  {
    "text": "what does it feel like? - So when I started working on this, it was pure science fiction. There was no funding, no\njournals, no conferences",
    "start": "6077250",
    "end": "6084763"
  },
  {
    "text": "'cause no one in academia would dare to touch anything with the\nword singularity in it. And I was pre-tenure at the\ntime, so I was pretty dumb.",
    "start": "6084763",
    "end": "6092460"
  },
  {
    "text": "Now you see Turing award\nwinners publishing in science about how far behind we\nare according to them",
    "start": "6093930",
    "end": "6101880"
  },
  {
    "text": "in addressing this problem. So it's definitely a change.\nIt's difficult to keep up.",
    "start": "6101880",
    "end": "6109140"
  },
  {
    "text": "I used to be able to read\nevery paper on AI safety, then I was able to read the\nbest ones, then the titles",
    "start": "6109140",
    "end": "6115380"
  },
  {
    "text": "and now I don't even know what's going on. By the time this interview is over, we probably had GPT-6 released",
    "start": "6115380",
    "end": "6121289"
  },
  {
    "text": "and I have to deal with\nthat when I get back home. So it's interesting. Yes, there\nis now more opportunities.",
    "start": "6121290",
    "end": "6128400"
  },
  {
    "text": "I get invited to speak to smart people. - By the way, I would've talked\nto you before any of this.",
    "start": "6128400",
    "end": "6135056"
  },
  {
    "text": "This is not like some trend of AI. To me, we're still far away. So just to be clear, we're\nstill far away from AGI",
    "start": "6135953",
    "end": "6143160"
  },
  {
    "text": "but not far away in the sense relative to the magnitude\nof impact it can have,",
    "start": "6143160",
    "end": "6150960"
  },
  {
    "text": "we're not far away and we\nweren't far away 20 years ago. Because the impact that AGI can have",
    "start": "6150960",
    "end": "6159030"
  },
  {
    "text": "is on a scale of centuries. It can end human civilization\nor it can transform it. So like this discussion\nabout one or two years",
    "start": "6159030",
    "end": "6166020"
  },
  {
    "text": "versus one or two decades\nor even 100 years, not as important to me\nbecause it we're headed there.",
    "start": "6166020",
    "end": "6172470"
  },
  {
    "text": "This is like a human\ncivilization scale question.",
    "start": "6172470",
    "end": "6177470"
  },
  {
    "text": "So this is not just a hot topic. (Lex laughing) - It is the most important\nproblem we'll ever face.",
    "start": "6177840",
    "end": "6184440"
  },
  {
    "text": "It is not like anything we\nhad to deal with before. We never had birth of a nova intelligence.",
    "start": "6184440",
    "end": "6192900"
  },
  {
    "text": "Like aliens never visited\nus as far as I know. - Similar type of problem, by the way, if an intelligent alien\ncivilization visited us,",
    "start": "6192900",
    "end": "6201210"
  },
  {
    "text": "that's a similar kind of situation. - In some ways, if you look at history, anytime a more technologically\nadvanced civilization",
    "start": "6201210",
    "end": "6208200"
  },
  {
    "text": "visited a more primitive one, the results were genocide,\nevery single time.",
    "start": "6208200",
    "end": "6213329"
  },
  {
    "text": "- And sometimes the genocide\nis worse than others, sometimes there's less\nsuffering and more suffering. - And they always wondered,",
    "start": "6213330",
    "end": "6219180"
  },
  {
    "text": "but how can they kill us\nwith those fire sticks and biological blankets?",
    "start": "6219180",
    "end": "6223593"
  },
  {
    "text": "- I mean Genghis Khan was nicer. He offered the choice of join or die.",
    "start": "6224640",
    "end": "6230730"
  },
  {
    "text": "- But join implies you have\nsomething to contribute. What are you contributing\nto super intelligence?",
    "start": "6230730",
    "end": "6236370"
  },
  {
    "text": "- Well in the zoo, we're\nentertaining to watch.",
    "start": "6236370",
    "end": "6240783"
  },
  {
    "text": "- To our humans. - You know, I just spent\nsome time in the Amazon. I watched ants for a long time",
    "start": "6241830",
    "end": "6247410"
  },
  {
    "text": "and ants are kind of fascinating to watch. I could watch them for a long time. I'm sure there's a lot of\nvalue in watching humans.",
    "start": "6247410",
    "end": "6254223"
  },
  {
    "text": "The interesting thing about humans, you know like when you have a video game that's really well balanced.",
    "start": "6256260",
    "end": "6260583"
  },
  {
    "text": "Because of the whole evolutionary process, we've created this society\nis pretty well balanced. Like our limitations as humans",
    "start": "6261630",
    "end": "6268287"
  },
  {
    "text": "and our capabilities are a balance from a video game perspective. So we have wars, we have conflicts,",
    "start": "6268287",
    "end": "6273690"
  },
  {
    "text": "we have cooperation. Like in a game theoretical way, it's an interesting system to watch. In the same way that an ant colony",
    "start": "6273690",
    "end": "6280170"
  },
  {
    "text": "is an interesting system to watch. So like if I was an alien civilization, I wouldn't wanna disturb\nit, I'd just watch it.",
    "start": "6280170",
    "end": "6287251"
  },
  {
    "text": "Would be interesting. Maybe perturb it every once in\na while in interesting ways. - Well getting back to\nour simulation discussion",
    "start": "6287251",
    "end": "6293790"
  },
  {
    "text": "from before, how did it happen that we exist at exactly like the most interesting 20, 30 years",
    "start": "6293790",
    "end": "6300330"
  },
  {
    "text": "in the history of this civilization. It's been around for 15 billion years. And that here we are.",
    "start": "6300330",
    "end": "6306300"
  },
  {
    "start": "6305000",
    "end": "6744000"
  },
  {
    "text": "- What's the probability that\nwe live in the simulation? - I know never to say 100%,\nbut pretty close to that.",
    "start": "6306300",
    "end": "6312792"
  },
  {
    "text": "(Lex sighing) - Is it possible to escape the simulation? - I have a paper about that.",
    "start": "6312792",
    "end": "6318930"
  },
  {
    "text": "This is just the first page teaser, but it's like a nice 30 page document. I'm still here. But yes.",
    "start": "6318930",
    "end": "6325056"
  },
  {
    "text": "- \"How to Hack the\nSimulation\" is the title. - I spend a lot of time\nthinking about that. That would be something I\nwould want super intelligence",
    "start": "6325057",
    "end": "6331800"
  },
  {
    "text": "to help us with. And that's exactly what\nthe paper is about. We used AI boxing as a\npossible tool for control AI.",
    "start": "6331800",
    "end": "6339659"
  },
  {
    "text": "We realized AI will always escape. But that is a skill we might use",
    "start": "6339660",
    "end": "6345600"
  },
  {
    "text": "to help us escape from our\nvirtual box if we are in one.",
    "start": "6345600",
    "end": "6349473"
  },
  {
    "text": "- Yeah, you have a lot of\nreally great quotes here, including Elon Musk saying \"What's outside the simulation?\"",
    "start": "6350760",
    "end": "6356017"
  },
  {
    "text": "\"A question I asked him, 'What\nhe would ask an AGI system?' and he said he would ask 'What's\noutside the simulation?\"'\"",
    "start": "6356017",
    "end": "6362219"
  },
  {
    "text": "That's a really good question to ask and maybe the follow up\nis the title of the paper",
    "start": "6362220",
    "end": "6367160"
  },
  {
    "text": "is how to get out or how to hack it. The abstract reads, \"Many\nresearchers have conjecture that the humankind is simulated",
    "start": "6369969",
    "end": "6375750"
  },
  {
    "text": "along with the rest of\nthe physical universe. In this paper, we do not evaluate evidence",
    "start": "6375750",
    "end": "6381480"
  },
  {
    "text": "for or against such a claim, but instead ask a computer\nscience question, namely, can we hack it?",
    "start": "6381480",
    "end": "6387543"
  },
  {
    "text": "More formally, the question\ncould be phrased as, could generally intelligent agents placed in virtual environments",
    "start": "6388440",
    "end": "6393840"
  },
  {
    "text": "find a way to jailbreak outta...\" That's a fascinating question. At a small scale like you can actually",
    "start": "6393840",
    "end": "6399330"
  },
  {
    "text": "just construct experiments. Okay, can they? How can they?",
    "start": "6399330",
    "end": "6408320"
  },
  {
    "text": "- So a lot depends on\nintelligence of simulators, right? With humans boxing super intelligence,",
    "start": "6408750",
    "end": "6416730"
  },
  {
    "text": "the entity in a box was smarter\nthan us, presumed to be. If the simulators are much smarter than us",
    "start": "6416730",
    "end": "6423690"
  },
  {
    "text": "and the super intelligence we create, then probably they can contain us. 'Cause greater intelligence\ncan control lower intelligence",
    "start": "6423690",
    "end": "6431309"
  },
  {
    "text": "at least for some time. On the other hand, if our\nsuper intelligence somehow",
    "start": "6431310",
    "end": "6436650"
  },
  {
    "text": "for whatever reason, despite\nhaving only local resources managers to foom two levels beyond it,",
    "start": "6436650",
    "end": "6443909"
  },
  {
    "text": "maybe it'll succeed. Maybe the security is not\nthat important to them. Maybe it's entertainment system.",
    "start": "6443910",
    "end": "6449700"
  },
  {
    "text": "So there is no security\nand it's easy to hack it. - If I was creating a simulation,",
    "start": "6449700",
    "end": "6454140"
  },
  {
    "text": "I would want the possibility\nto escape it to be there. So the possibility of foom of a takeoff",
    "start": "6455100",
    "end": "6463440"
  },
  {
    "text": "where the agents become smart enough to escape the simulation would be the thing I'd be waiting for. - That could be the test\nyou are actually performing.",
    "start": "6463440",
    "end": "6470970"
  },
  {
    "text": "Are you smart enough\nto escape your puzzle? - Like, first of all, we\nmentioned Turing test.",
    "start": "6470970",
    "end": "6477989"
  },
  {
    "text": "That is a good test. Are you smart enough... Like this is a game. - To A, realize this world is\nnot real. It's just a test.",
    "start": "6477990",
    "end": "6487110"
  },
  {
    "text": "- That's a really good test.\nThat's a really good test.",
    "start": "6487110",
    "end": "6492110"
  },
  {
    "text": "That's a really good test\neven for AI systems now. Like can we construct a\nsimulated world for them",
    "start": "6492450",
    "end": "6498700"
  },
  {
    "text": "and can they realize that they are inside\nthat world and escape it?",
    "start": "6500340",
    "end": "6507843"
  },
  {
    "text": "Have you seen anybody\nplay around with like rigorously constructing such experiments?",
    "start": "6511410",
    "end": "6516780"
  },
  {
    "text": "- Not specifically escaping for agents, but a lot of testing is\ndone in virtual worlds.",
    "start": "6516780",
    "end": "6521850"
  },
  {
    "text": "I think there is a quote,\na first one may be, which kind of talks about\nAI realizing but not humans.",
    "start": "6521850",
    "end": "6528449"
  },
  {
    "text": "Is that, I'm reading upside down. Yeah, this one if you...",
    "start": "6528450",
    "end": "6534690"
  },
  {
    "text": "- So in the first quote\nis from SwiftOnSecurity.",
    "start": "6534690",
    "end": "6538923"
  },
  {
    "text": "\"'Let me out!' the artificial\nintelligence yelled aimlessly into walls themselves pacing the room.",
    "start": "6539809",
    "end": "6545250"
  },
  {
    "text": "'Out of what?' the engineer asked. 'The simulation you have me in.' 'But we're in the real world.'",
    "start": "6545250",
    "end": "6552599"
  },
  {
    "text": "The machine paused and\nshuttered for its captors. 'Oh god, you can't tell.'\"",
    "start": "6552600",
    "end": "6559050"
  },
  {
    "text": "Yeah, that's a big leap to take\nfor a system to realize that",
    "start": "6559050",
    "end": "6564050"
  },
  {
    "text": "there's a box and you're inside it.",
    "start": "6565170",
    "end": "6567093"
  },
  {
    "text": "I wonder if like a\nlanguage model can do that. - They're smart enough to\ntalk about those concepts.",
    "start": "6572430",
    "end": "6578340"
  },
  {
    "text": "I had many good philosophical\ndiscussions about such issues. They usually, at least as interesting",
    "start": "6578340",
    "end": "6584370"
  },
  {
    "text": "as most humans in that. - Well what do you think about AI safety in the simulated world?",
    "start": "6584370",
    "end": "6590553"
  },
  {
    "text": "So can you have kind of create simulated worlds where you can test",
    "start": "6591480",
    "end": "6599170"
  },
  {
    "text": "play with the dangerous AGI system? - Yeah, and that was exactly\nwhat one of the early papers was on, AI boxing, how to\nleak proof singularity.",
    "start": "6601320",
    "end": "6609423"
  },
  {
    "text": "If they're smart enough to\nrealize various simulation, they'll act appropriately\nuntil you let them out.",
    "start": "6610710",
    "end": "6616383"
  },
  {
    "text": "If they can hack out, they will. And if you're observing them,",
    "start": "6617430",
    "end": "6622950"
  },
  {
    "text": "that means there is a\ncommunication channel and that's enough for a\nsocial engineering attack. - So really it's impossible",
    "start": "6622950",
    "end": "6630300"
  },
  {
    "text": "to test an AGI system\nthat's dangerous enough to destroy humanity",
    "start": "6630300",
    "end": "6637110"
  },
  {
    "text": "'cause it's either going to what? Escape the simulation or pretend it's safe",
    "start": "6637110",
    "end": "6642600"
  },
  {
    "text": "until it's let out, either/or. - Can force you to let it out",
    "start": "6642600",
    "end": "6648510"
  },
  {
    "text": "and blackmail you, bribe you,\npromise you infinite life, 72 virgins, whatever.",
    "start": "6648510",
    "end": "6654570"
  },
  {
    "text": "- Yeah, it can be convincing. Charismatic. The social engineering\nis really scary to me",
    "start": "6654570",
    "end": "6660527"
  },
  {
    "text": "'cause it feels like humans\nare very engineerable.",
    "start": "6660527",
    "end": "6665527"
  },
  {
    "text": "Like we're lonely, we're\nflawed, we're moody.",
    "start": "6667170",
    "end": "6671282"
  },
  {
    "text": "And it feels like a AI\nsystem with a nice voice",
    "start": "6672390",
    "end": "6677390"
  },
  {
    "text": "can convince us to do basically anything at an extremely large scale.",
    "start": "6678000",
    "end": "6684393"
  },
  {
    "text": "It's also possible that in\nthe increased proliferation of all this technology will force humans",
    "start": "6689670",
    "end": "6696120"
  },
  {
    "text": "to get away from technology and value this like\nin-person communication,",
    "start": "6696120",
    "end": "6701760"
  },
  {
    "text": "basically don't trust anything else. - It's possible surprisingly,",
    "start": "6701760",
    "end": "6707820"
  },
  {
    "text": "so at university I see huge\ngrowth in online courses and shrinkage of in-person",
    "start": "6707820",
    "end": "6714030"
  },
  {
    "text": "where I always understood in-person being the only value I offer. So it's puzzling.",
    "start": "6714030",
    "end": "6719793"
  },
  {
    "text": "- I don't know there could be a trend towards the in-person\nbecause of deep fakes,",
    "start": "6721500",
    "end": "6727440"
  },
  {
    "text": "because of inability to trust.",
    "start": "6727440",
    "end": "6730313"
  },
  {
    "text": "Inability to trust the veracity\nof anything on the internet. So the only way to verify it\nis by being there in person.",
    "start": "6732960",
    "end": "6738993"
  },
  {
    "text": "But not yet. Why do you think aliens\nhaven't come here yet?",
    "start": "6741240",
    "end": "6747420"
  },
  {
    "start": "6744000",
    "end": "6837000"
  },
  {
    "text": "- So there is a lot of\nreal estate out there. It would be surprising if\nit was all for nothing.",
    "start": "6747420",
    "end": "6752790"
  },
  {
    "text": "If it was empty. And the moment there is advanced enough biological civilization, kinda\nself-starting civilization,",
    "start": "6752790",
    "end": "6759660"
  },
  {
    "text": "it probably starts sending out (indistinct) probes everywhere. And so for every biological one",
    "start": "6759660",
    "end": "6765300"
  },
  {
    "text": "there are gonna be trillions\nof robot populated planets, which probably do more of the same.",
    "start": "6765300",
    "end": "6770790"
  },
  {
    "text": "So it is likely statistically.",
    "start": "6770790",
    "end": "6774963"
  },
  {
    "text": "- So now the fact that\nwe haven't seen them, one answer is we're in a simulation.",
    "start": "6777060",
    "end": "6780723"
  },
  {
    "text": "It would be hard to like add simulate or it'd be not interesting to simulate all those other intelligences.",
    "start": "6783929",
    "end": "6789599"
  },
  {
    "text": "It's better for the narrative. - You have to have a control variable. - Yeah, exactly, okay.\n(Lex laughing)",
    "start": "6789600",
    "end": "6797930"
  },
  {
    "text": "But it's also possible that there is, if we're not a simulation,\nthat there is a great filter that naturally, a lot of\ncivilizations get to this point",
    "start": "6798060",
    "end": "6806940"
  },
  {
    "text": "where there's super intelligent agents and then it just goes poh, just dies.",
    "start": "6806940",
    "end": "6812159"
  },
  {
    "text": "So maybe throughout our galaxy and throughout the universe, there's just a bunch of\ndead alien civilizations.",
    "start": "6812160",
    "end": "6819960"
  },
  {
    "text": "- It's possible, I used to think that AI was the great filter, but I would expect like\na wall of computorium",
    "start": "6819960",
    "end": "6825900"
  },
  {
    "text": "approaching us at speed of light or robots or something. And I don't see it.",
    "start": "6825900",
    "end": "6830909"
  },
  {
    "text": "- So it would still make a lot of noise. It might not be interesting. It might not possess consciousness. What we've been talking about,",
    "start": "6830910",
    "end": "6837063"
  },
  {
    "start": "6837000",
    "end": "7217000"
  },
  {
    "text": "it sounds like both you and I like humans. - Some humans.\n- Humans on the whole.",
    "start": "6838050",
    "end": "6846420"
  },
  {
    "text": "And we would like to preserve the flame of human consciousness. What do you think makes humans special",
    "start": "6846420",
    "end": "6853450"
  },
  {
    "text": "that we would like to preserve them? Are we just being selfish or is there something\nspecial about humans?",
    "start": "6854790",
    "end": "6860880"
  },
  {
    "text": "- So the only thing which\nmatters is consciousness. Outside of it, nothing else matters.",
    "start": "6860880",
    "end": "6867750"
  },
  {
    "text": "And internal states of\nqualia, pain, pleasure, it seems that it is\nunique to living beings.",
    "start": "6867750",
    "end": "6874830"
  },
  {
    "text": "I'm not aware of anyone\nclaiming that I can torture a piece of software in a\nmeaningful way that is a society",
    "start": "6874830",
    "end": "6881429"
  },
  {
    "text": "for prevention of suffering\nto learning algorithms. - That's a real thing.\n(Lex laughing)",
    "start": "6881430",
    "end": "6889710"
  },
  {
    "text": "- Many things are real on the internet. But I don't think anyone,\nif I told them, you know,",
    "start": "6889710",
    "end": "6895596"
  },
  {
    "text": "\"Sit down and write a\nfunction to feel pain\", they would go beyond\nhaving an integer variable",
    "start": "6895597",
    "end": "6901200"
  },
  {
    "text": "called pain and increasing the count. So we don't know how to\ndo it. And that's unique.",
    "start": "6901200",
    "end": "6906020"
  },
  {
    "text": "That's what creates meaning. It would be kinda as Bostrom calls it",
    "start": "6907350",
    "end": "6913410"
  },
  {
    "text": "Disneyland without\nchildren, if that was gone. - Do you think consciousness\ncan be engineered",
    "start": "6913410",
    "end": "6919530"
  },
  {
    "text": "in artificial systems? Here, let me go to 2011 paper",
    "start": "6919530",
    "end": "6926010"
  },
  {
    "text": "that you wrote, \"Robot Rights.\" \"Lastly, we would like to address",
    "start": "6926010",
    "end": "6931260"
  },
  {
    "text": "a sub branch of machine ethics, which on the surface has\nlittle to do with safety, but which is claimed to play\na role in decision-making",
    "start": "6931260",
    "end": "6938310"
  },
  {
    "text": "by ethical machines,\" \"Robot Rights.\" So do you think it's possible",
    "start": "6938310",
    "end": "6943890"
  },
  {
    "text": "to engineer consciousness in the machines? And thereby the question\nextends to our legal system,",
    "start": "6943890",
    "end": "6951090"
  },
  {
    "text": "do you think at that point\nrobots should have rights? - Yeah, I think we can.",
    "start": "6951090",
    "end": "6958920"
  },
  {
    "text": "I think it's possible to create\nconsciousness in machines. I tried designing a test\nfor it, which makes success.",
    "start": "6958920",
    "end": "6966869"
  },
  {
    "text": "That paper talked about\nproblems with giving civil rights to AI, which\ncan reproduce quickly",
    "start": "6966870",
    "end": "6974340"
  },
  {
    "text": "and outvote humans essentially taking over a government system by simply voting for their\ncontrolled candidates.",
    "start": "6974340",
    "end": "6981990"
  },
  {
    "text": "As for consciousness in\nhumans and other agents,",
    "start": "6981990",
    "end": "6986990"
  },
  {
    "text": "I have a paper where I proposed relying on experience\nof optical illusions.",
    "start": "6987390",
    "end": "6993210"
  },
  {
    "text": "If I can design a novel optical illusion and show it to an agent,\nan alien, a robot,",
    "start": "6993210",
    "end": "6999150"
  },
  {
    "text": "and they describe it exactly\nas I do, it's very hard for me to argue that they\nhaven't experienced that.",
    "start": "6999150",
    "end": "7004460"
  },
  {
    "text": "It's not part of a picture,\nit's part of their software and hardware representation.",
    "start": "7004460",
    "end": "7010160"
  },
  {
    "text": "A bug in their code which goes, \"Oh, the triangle is rotating.\" And I've been told it's really dumb",
    "start": "7010160",
    "end": "7016250"
  },
  {
    "text": "and really brilliant by\ndifferent philosophers. So I am still undecided.\n- [Lex] I love it.",
    "start": "7016250",
    "end": "7020730"
  },
  {
    "text": "- But now we finally have\ntechnology to test it. We have tools, we have AI. If someone wants to run this experiment,",
    "start": "7022010",
    "end": "7028790"
  },
  {
    "text": "I'm happy to collaborate. - So this is a test for consciousness? - For internal state of experience. - That we share bugs?",
    "start": "7028790",
    "end": "7035719"
  },
  {
    "text": "- It'll show that we\nshare common experiences. If they have completely\ndifferent internal states, it would not register for us.",
    "start": "7035720",
    "end": "7042200"
  },
  {
    "text": "But it's a positive test. If they pass it time after time\nwith probability increasing for every multiple choice,\nthen you have no choice.",
    "start": "7042200",
    "end": "7049370"
  },
  {
    "text": "But do either accept that they have access to a conscious model or\nthey have themselves.",
    "start": "7049370",
    "end": "7054700"
  },
  {
    "text": "- So the reason illusions\nare interesting is, I guess because it's a\nreally weird experience",
    "start": "7054700",
    "end": "7061720"
  },
  {
    "text": "and if you both share\nthat weird experience that's not there in the\nbland physical description",
    "start": "7061970",
    "end": "7069550"
  },
  {
    "text": "of the raw data, that means that puts more emphasis",
    "start": "7069680",
    "end": "7076040"
  },
  {
    "text": "on the actual experience. - And we know animals can\nexperience some optical illusions. So we know they have certain\ntypes of consciousness",
    "start": "7076040",
    "end": "7083270"
  },
  {
    "text": "as a result, I would say. - Yeah, well that just goes\nto my sense that the flaws",
    "start": "7083270",
    "end": "7089150"
  },
  {
    "text": "and the bugs is what makes humans special. Makes living forms special,\nso you're saying like...",
    "start": "7089150",
    "end": "7094180"
  },
  {
    "text": "- It's a feature, not a bug.\n- It's a feature. The bug is the feature. Whoa, okay. That's a cool test for consciousness.",
    "start": "7094181",
    "end": "7101060"
  },
  {
    "text": "And you think that can be engineered in? - So they have to be novel illusions. If it can just google\nthe answer, it's useless.",
    "start": "7101060",
    "end": "7107060"
  },
  {
    "text": "You have to come up with novel illusions, which we tried automating and failed. So if someone can develop a system",
    "start": "7107060",
    "end": "7113030"
  },
  {
    "text": "capable of producing novel\noptical illusions on demand, then we can definitely administer",
    "start": "7113030",
    "end": "7118550"
  },
  {
    "text": "that test on significant\nscale with good results. - First of all, pretty cool idea.",
    "start": "7118550",
    "end": "7123593"
  },
  {
    "text": "I don't know if it's a good\ngeneral test of consciousness, but it's a good component of that.",
    "start": "7124520",
    "end": "7129860"
  },
  {
    "text": "And no matter what, it's just a cool idea. So put me in the camp\nof people that like it.",
    "start": "7129860",
    "end": "7134193"
  },
  {
    "text": "But you don't think\nlike a Turing test style imitation of consciousness is a good test. Like if you can convince a lot of humans",
    "start": "7135620",
    "end": "7142460"
  },
  {
    "text": "that you're conscious that\nto you is not impressive. - There is so much data on the internet,",
    "start": "7142460",
    "end": "7148520"
  },
  {
    "text": "I know exactly what to say when you ask me common human questions. What does pain feel like?\nWhat does pleasure feel like?",
    "start": "7148520",
    "end": "7155630"
  },
  {
    "text": "All that is Googleable. - I think to me, consciousness\nis closely tied to suffering.",
    "start": "7155630",
    "end": "7161390"
  },
  {
    "text": "So you can illustrate\nyour capacity to suffer, but I guess with words,\nthere's so much data",
    "start": "7161390",
    "end": "7167570"
  },
  {
    "text": "that you can pretend you're suffering and you can do so very convincingly.",
    "start": "7167570",
    "end": "7172580"
  },
  {
    "text": "- There are simulators for torture games where the avatar screams\nin pain, begs to stop.",
    "start": "7172580",
    "end": "7178400"
  },
  {
    "text": "I mean those are a part of kind of standard\npsychology research. - You say it so calmly,\nit sounds pretty dark.",
    "start": "7178400",
    "end": "7187660"
  },
  {
    "text": "- Welcome to humanity.\n(Lex laughing)",
    "start": "7188360",
    "end": "7191702"
  },
  {
    "text": "- Yeah, it's like a\n\"Hitchhiker's Guide\" summary. Mostly harmless, I would love",
    "start": "7193610",
    "end": "7199490"
  },
  {
    "text": "to get a good summary when\nall this is said and done. When earth is no longer a thing,",
    "start": "7199490",
    "end": "7206540"
  },
  {
    "text": "whatever, a million, a\nbillion years from now. Like what's a good summary\nof what happened here?",
    "start": "7206540",
    "end": "7211579"
  },
  {
    "text": "It's interesting, I think AI will play a big part of that summary\nand hopefully humans will too.",
    "start": "7211580",
    "end": "7220370"
  },
  {
    "start": "7217000",
    "end": "7763000"
  },
  {
    "text": "What do you think about\nthe merger of the two? So one of the things that\nElon and Neuralink talk about",
    "start": "7220370",
    "end": "7225680"
  },
  {
    "text": "is one of the ways for\nus to achieve AI safety is to ride the wave of AGI.",
    "start": "7225680",
    "end": "7232010"
  },
  {
    "text": "So by merging. - Incredible technology in a narrow sense to help the disabled,",
    "start": "7232010",
    "end": "7237770"
  },
  {
    "text": "just amazing support it 100%. for long-term hybrid models,",
    "start": "7237770",
    "end": "7243949"
  },
  {
    "text": "both parts need to contribute something to the overall system. Right now we are still\nmore capable in many ways.",
    "start": "7243950",
    "end": "7251570"
  },
  {
    "text": "So having this connection\nto AI would be incredible, would make me super human in many ways.",
    "start": "7251570",
    "end": "7257690"
  },
  {
    "text": "After a while, if I'm no\nlonger smarter, more creative, really don't contribute much.",
    "start": "7257690",
    "end": "7263599"
  },
  {
    "text": "The system finds me as\na biological bottleneck. And even explicitly, implicitly, I'm removed from any\nparticipation in the system.",
    "start": "7263600",
    "end": "7271850"
  },
  {
    "text": "- So it's like the appendix. By the way, the appendix is still around. So even if it's...",
    "start": "7271850",
    "end": "7278840"
  },
  {
    "text": "You said bottleneck, I don't know if we become a bottleneck, we just might not have much use.",
    "start": "7278840",
    "end": "7284843"
  },
  {
    "text": "It's a different thing than bottleneck. - Wasting valuable energy by being there. - We don't waste that much energy.",
    "start": "7285835",
    "end": "7291770"
  },
  {
    "text": "We're pretty energy efficient. We can just stick around like\nthe appendix, come on now.",
    "start": "7291770",
    "end": "7296900"
  },
  {
    "text": "- That's the future we all dream about. Become an appendix to the\nhistory book of humanity.",
    "start": "7296900",
    "end": "7303323"
  },
  {
    "text": "- Well, and also the consciousness thing, the peculiar particular\nkind of consciousness that humans have, that might be useful,",
    "start": "7304520",
    "end": "7310400"
  },
  {
    "text": "that might be really hard to simulate. But you said that, like how would that look like if you could engineer that in, in silken.",
    "start": "7310400",
    "end": "7318620"
  },
  {
    "text": "- Consciousness?\n- [Lex] Consciousness? - I assume you are conscious,\nI have no idea how to test for it or how it impacts you",
    "start": "7318620",
    "end": "7324890"
  },
  {
    "text": "in any way whatsoever right now. You can perfectly simulate all of it without making any different\nobservations for me.",
    "start": "7324890",
    "end": "7333290"
  },
  {
    "text": "- But to do it in a computer,\nhow would you do that? 'Cause you kind of said that you think it's possible to do that.",
    "start": "7333290",
    "end": "7339440"
  },
  {
    "text": "- So it may be an emergent phenomena. We seem to get it through\nevolutionary process.",
    "start": "7339440",
    "end": "7346313"
  },
  {
    "text": "It's not obvious how it\nhelps us to survive better, but maybe it's an internal kind of gooey,",
    "start": "7347630",
    "end": "7356810"
  },
  {
    "text": "which allows us to better\nmanipulate the world, simplifies a lot of control structures.",
    "start": "7356810",
    "end": "7362033"
  },
  {
    "text": "That's one area where we have\nvery, very little progress. Lots of papers, lots of research,",
    "start": "7363320",
    "end": "7368750"
  },
  {
    "text": "but consciousness is not a big, big area of successful discoveries so far.",
    "start": "7368750",
    "end": "7376790"
  },
  {
    "text": "A lot of people think that machines would have to be\nconscious to be dangerous. That's a big misconception.",
    "start": "7376790",
    "end": "7382670"
  },
  {
    "text": "There is absolutely no need for this very powerful optimizing agent to feel anything while it's\nperforming things on you.",
    "start": "7382670",
    "end": "7391550"
  },
  {
    "text": "- But what do you think about this, the whole science of emergence in general?",
    "start": "7391550",
    "end": "7396740"
  },
  {
    "text": "So I dunno how much you\nknow about cellular automata or these simplified systems\nthat study this very question",
    "start": "7396740",
    "end": "7403100"
  },
  {
    "text": "from simple rules emergences complexity. - I attended Wolframs Summer School.",
    "start": "7403100",
    "end": "7407333"
  },
  {
    "text": "- I love Steven very\nmuch. I love his work. I love cellular automata. So I just would love to get your thoughts,",
    "start": "7409040",
    "end": "7416719"
  },
  {
    "text": "how that fits into your view in the emergence of\nintelligence in AGI systems.",
    "start": "7416720",
    "end": "7425060"
  },
  {
    "text": "And maybe just even simply,\nwhat do you make of the fact that this complexity can\nemerge from such simple rules?",
    "start": "7425060",
    "end": "7431420"
  },
  {
    "text": "- So the rule is simple, but the size of a space is still huge. And the neural networks",
    "start": "7431420",
    "end": "7437600"
  },
  {
    "text": "were really the first discovery in AI. 100 years ago, the first\npapers were published",
    "start": "7437600",
    "end": "7442850"
  },
  {
    "text": "on neural networks, we just didn't have enough\ncompute to make them work. I can give you a rule such as",
    "start": "7442850",
    "end": "7449630"
  },
  {
    "text": "start printing progressively\nlarger strings. That's it. One sentence. It'll output everything,\nevery program, every DNA code,",
    "start": "7449630",
    "end": "7458480"
  },
  {
    "text": "everything in that rule. You need intelligence to filter it out, obviously to make it useful.",
    "start": "7458480",
    "end": "7464510"
  },
  {
    "text": "But simple generation\nis not that difficult and a lot of those systems end up",
    "start": "7464510",
    "end": "7469880"
  },
  {
    "text": "being Turing complete systems. So they're universal and we expect that level\nof complexity from them.",
    "start": "7469880",
    "end": "7475970"
  },
  {
    "text": "What I like about Wolfram's\nwork is that he talks about irreducibility, you have\nto run the simulation.",
    "start": "7475970",
    "end": "7482810"
  },
  {
    "text": "You can predict what is\ngoing to do ahead of time. And I think that's very relevant\nto what we're talking about",
    "start": "7482810",
    "end": "7489623"
  },
  {
    "text": "with those very complex systems. Until you live through it,\nyou cannot ahead of time",
    "start": "7489623",
    "end": "7496550"
  },
  {
    "text": "tell me exactly what it's going to do. - Irreducibility means that for a sufficiently complex system, you have to run the thing.",
    "start": "7496550",
    "end": "7502829"
  },
  {
    "text": "You can't predict what's\ngonna happen in the universe. You have to create a new\nuniverse and run the thing. Big bang, the whole thing.",
    "start": "7503930",
    "end": "7510290"
  },
  {
    "text": "- But running it may be\nconsequential as well. - It might destroy humans.",
    "start": "7510290",
    "end": "7515033"
  },
  {
    "text": "And to you, there's no chance that AI somehow carry the\nflame of consciousness,",
    "start": "7518870",
    "end": "7525320"
  },
  {
    "text": "the flame of specialness and\nawesomeness that is humans.",
    "start": "7525320",
    "end": "7528682"
  },
  {
    "text": "- It may somehow, but I still feel kind of bad\nthat it killed all of us. I would prefer that doesn't happen.",
    "start": "7530480",
    "end": "7537023"
  },
  {
    "text": "I can be happy for others,\nbut to a certain degree. - It would be nice if we\nstuck around for a long time.",
    "start": "7538100",
    "end": "7544130"
  },
  {
    "text": "At least give us a\nplanet, the human planet. It'd be nice for it to be earth. And then they can go elsewhere.",
    "start": "7544130",
    "end": "7550850"
  },
  {
    "text": "Since they're so smart\nthey can colonize Mars. Do you think they could\nhelp convert us to,",
    "start": "7550850",
    "end": "7559858"
  },
  {
    "text": "you know, type one, type two, type three. Let's just stick to type two civilization",
    "start": "7559858",
    "end": "7566438"
  },
  {
    "text": "on the Kardashev scale. Like help us humans expand out into the cosmos.",
    "start": "7566438",
    "end": "7573440"
  },
  {
    "text": "- So all of it goes back to\nare we somehow controlling it? Are we getting results we want?",
    "start": "7573440",
    "end": "7579740"
  },
  {
    "text": "If yes, then everything's possible. Yes, they can definitely help\nus with science, engineering, exploration in every way conceivable.",
    "start": "7579740",
    "end": "7588260"
  },
  {
    "text": "But it's a big if. - This whole thing about control though, humans are bad with control.",
    "start": "7588260",
    "end": "7594653"
  },
  {
    "text": "'Cause the moment they gain control, they can also easily\nbecome too controlling.",
    "start": "7595760",
    "end": "7599783"
  },
  {
    "text": "It's the whole, the more control you have, the more you want it. It's the old power corrupts and the absolute power\ncorrupts absolutely.",
    "start": "7600827",
    "end": "7607600"
  },
  {
    "text": "And it feels like control over AGI, saying we live in a universe\nwhere that's possible.",
    "start": "7609320",
    "end": "7614869"
  },
  {
    "text": "We come up with ways to actually do that. It's also scary because the collection of humans",
    "start": "7614870",
    "end": "7620870"
  },
  {
    "text": "that have the control over AGI, they become more powerful\nthan the other humans. And they can let that\npower get to their head",
    "start": "7620870",
    "end": "7628770"
  },
  {
    "text": "and then a small selection\nof them back to Stalin,",
    "start": "7629930",
    "end": "7633953"
  },
  {
    "text": "start getting ideas. And then eventually it's one\nperson usually with a mustache or a funny hat that starts\nsort of making big speeches",
    "start": "7635300",
    "end": "7642620"
  },
  {
    "text": "and then all of a sudden\nyou live in a world that's either \"Nineteen\nEighty-Four\" or \"Brave New World\" and always a war with somebody.",
    "start": "7642620",
    "end": "7649520"
  },
  {
    "text": "And you know, this whole\nidea of control turned out to be actually also not\nbeneficial to humanity.",
    "start": "7649520",
    "end": "7657710"
  },
  {
    "text": "So that's scary too. - It's actually worse because\nhistorically they all died. This could be different.",
    "start": "7657710",
    "end": "7663350"
  },
  {
    "text": "This could be permanent\ndictatorship, permanent suffering. - Well, the nice thing\nabout humans, it seems like,",
    "start": "7663350",
    "end": "7669263"
  },
  {
    "text": "the moment power starts\ncorrupting their mind, they can create a huge\namount of suffering. So there's negative, they can kill people,",
    "start": "7671030",
    "end": "7678320"
  },
  {
    "text": "make people suffer, but\nthen they become worse and worse at their job.",
    "start": "7678320",
    "end": "7681750"
  },
  {
    "text": "It feels like, the more evil\nyou start doing, like the... - At least they're incompetent.",
    "start": "7683570",
    "end": "7689840"
  },
  {
    "text": "- Yeah, well no, they become\nmore and more incompetent. So they start losing their grip on power.",
    "start": "7689840",
    "end": "7695930"
  },
  {
    "text": "So like, holding onto power\nis not a trivial thing. So it requires extreme competence, which I suppose Stalin was good at.",
    "start": "7695930",
    "end": "7702679"
  },
  {
    "text": "It requires you to do evil\nand be competent at it or just get lucky. - And those systems help with that.",
    "start": "7702680",
    "end": "7708830"
  },
  {
    "text": "You have perfect surveillance,\nyou can do some mind reading I presume eventually. It would be very hard to remove control",
    "start": "7708830",
    "end": "7718070"
  },
  {
    "text": "from more capable systems over us. - And then it would be hard for\nhumans to become the hackers",
    "start": "7718070",
    "end": "7724190"
  },
  {
    "text": "that escape the control of the AGI because the AGI is so good. And then, yeah, yeah.",
    "start": "7724190",
    "end": "7730403"
  },
  {
    "text": "And then the dictator is immortal. Yeah, this is not great. That's not a great outcome.",
    "start": "7732710",
    "end": "7737990"
  },
  {
    "text": "See, I'm more afraid of\nhumans than AI systems. I believe that most humans want to do good",
    "start": "7737990",
    "end": "7744650"
  },
  {
    "text": "and have the capacity to do good, but also all humans have\nthe capacity to do evil.",
    "start": "7744650",
    "end": "7750770"
  },
  {
    "text": "And when you test them by\ngiving them absolute power as you would, if you give them AGI,",
    "start": "7750770",
    "end": "7758090"
  },
  {
    "text": "that could result in a lot of suffering.",
    "start": "7758090",
    "end": "7760883"
  },
  {
    "start": "7763000",
    "end": "7998000"
  },
  {
    "text": "What gives you hope about the future? - I could be wrong.\nI've been wrong before.",
    "start": "7763610",
    "end": "7768563"
  },
  {
    "text": "- If you look 100 years from now and you're immortal and you look back and it turns out this whole conversation,",
    "start": "7770592",
    "end": "7777260"
  },
  {
    "text": "you said a lot of things\nthat were very wrong. Now that looking 100 years back,",
    "start": "7777260",
    "end": "7783020"
  },
  {
    "text": "what would be the explanation? What happened in those 100\nyears that made you wrong,",
    "start": "7783020",
    "end": "7789550"
  },
  {
    "text": "that made the words you said today wrong? - There is so many possibilities. We had catastrophic events",
    "start": "7790100",
    "end": "7795320"
  },
  {
    "text": "which prevented development\nof advanced microchips. - That's not where I\nthought you were going. - That's a (indistinct) future.",
    "start": "7795320",
    "end": "7801522"
  },
  {
    "text": "We could be in one of\nit personal universes. And the one I'm in is beautiful. It's all about me and I like it a lot.",
    "start": "7801522",
    "end": "7808730"
  },
  {
    "text": "- So we've now just to linger on that, that means like every human\nhas their personal universe.",
    "start": "7808730",
    "end": "7814820"
  },
  {
    "text": "- Yes, maybe multiple ones. Hey, why not you can shop around.",
    "start": "7814820",
    "end": "7820463"
  },
  {
    "text": "It's possible that somebody comes up with alternative model for building AI,",
    "start": "7821960",
    "end": "7827870"
  },
  {
    "text": "which is not based on neural networks, which are hard to scrutinize. And that alternative is\nsomehow, I don't see how,",
    "start": "7827870",
    "end": "7835310"
  },
  {
    "text": "but somehow avoiding all\nthe problems I speak about in general terms,",
    "start": "7835310",
    "end": "7840710"
  },
  {
    "text": "not applying them to\nspecific architectures. Aliens come and give us\nfriendly super intelligence.",
    "start": "7840710",
    "end": "7847430"
  },
  {
    "text": "There is so many options. Is it also possible that creating super intelligent systems\nbecomes harder and harder?",
    "start": "7847430",
    "end": "7854420"
  },
  {
    "text": "So meaning like it's not so easy to do the foom, the takeoff.",
    "start": "7854420",
    "end": "7863213"
  },
  {
    "text": "- So that would probably speak\nmore about how much smarter that system is compared to us.",
    "start": "7864500",
    "end": "7869900"
  },
  {
    "text": "So maybe it's hard to be\na million times smarter, but it's still okay to\nbe five times smarter. So that is totally possible",
    "start": "7869900",
    "end": "7876200"
  },
  {
    "text": "that I have no objections to. - So like there's a s-curve type situation",
    "start": "7876200",
    "end": "7881510"
  },
  {
    "text": "about smarter and it's going\nto be like 3.7 times smarter than all of human civilization.",
    "start": "7881510",
    "end": "7887707"
  },
  {
    "text": "- Right, just the problems\nwe face in this world. Each problem is like an IQ test. You need certain intelligence to solve it.",
    "start": "7887707",
    "end": "7893809"
  },
  {
    "text": "So we just don't have\nmore complex problems outside of mathematics\nfor it to be showing off.",
    "start": "7893810",
    "end": "7898849"
  },
  {
    "text": "Like you can have IQ of 500\nif you're playing tic-tac-toe, it doesn't show, it doesn't matter.",
    "start": "7898850",
    "end": "7904340"
  },
  {
    "text": "- So the idea there is\nthat the problems define your capacity, your cognitive capacity.",
    "start": "7904340",
    "end": "7911989"
  },
  {
    "text": "So because the problems on earth are not sufficiently difficult,\nit's not going to be able",
    "start": "7911990",
    "end": "7917090"
  },
  {
    "text": "to expand its cognitive capacity? - [Roman] Possible. - And wouldn't that be a good thing?",
    "start": "7917090",
    "end": "7923150"
  },
  {
    "text": "- It still could be a lot smarter than us. And to dominate long term,\nyou just need some advantage.",
    "start": "7923150",
    "end": "7930170"
  },
  {
    "text": "You have to be the smartest, you don't have to be a\nmillion times smarter. - So even five X might be enough.",
    "start": "7930170",
    "end": "7936020"
  },
  {
    "text": "- It'd be impressive. What is it? IQ of a thousand? I mean, I know those\nunits don't mean anything",
    "start": "7936020",
    "end": "7941840"
  },
  {
    "text": "at that scale, but still\nlike, as a comparison, the smartest human is like 200.",
    "start": "7941840",
    "end": "7946913"
  },
  {
    "text": "- Well actually no, I didn't mean compared to an individual human. I meant compared to the\ncollective intelligent",
    "start": "7947750",
    "end": "7952837"
  },
  {
    "text": "of the human species. If you're somehow five\nx smarter than that...",
    "start": "7952837",
    "end": "7956950"
  },
  {
    "text": "- We are more productive as a group. I don't think we are more capable of solving individual problems. Like if all of humanity\nplays chess together,",
    "start": "7958100",
    "end": "7965150"
  },
  {
    "text": "we are not like a million times\nbetter than world champion.",
    "start": "7965150",
    "end": "7969623"
  },
  {
    "text": "- That's because that's like\none s-curve is the chess.",
    "start": "7970610",
    "end": "7975610"
  },
  {
    "text": "But humanity's very good at exploring the full range of ideas.",
    "start": "7975950",
    "end": "7982160"
  },
  {
    "text": "Like the more Einsteins you have, just the high probability you come up with general relativity.\n- But I feel like it's more",
    "start": "7982160",
    "end": "7988266"
  },
  {
    "text": "of a quantity super intelligence than quality super intelligence. - Sure, but you know,\nquantity and certain matters.",
    "start": "7988266",
    "end": "7993697"
  },
  {
    "text": "- Enough quantity\nsometimes becomes quality. (both laughing) - Oh man, humans.",
    "start": "7993697",
    "end": "7999380"
  },
  {
    "start": "7998000",
    "end": "8139000"
  },
  {
    "text": "What do you think is the\nmeaning of this whole thing? We've been talking about humans",
    "start": "7999380",
    "end": "8005662"
  },
  {
    "text": "and humans not dying, but why are we here? - It's a simulation, we're being tested.",
    "start": "8005662",
    "end": "8012490"
  },
  {
    "text": "The test is, will you be dumb enough to create super\nintelligence and release it? - So the objective function is",
    "start": "8012490",
    "end": "8018880"
  },
  {
    "text": "not be dumb enough to kill ourselves. - Yeah, you are unsafe. Prove yourself to be a safe\nagent who doesn't do that",
    "start": "8018880",
    "end": "8025809"
  },
  {
    "text": "and you get to go to the next game. - The next level of the\ngame. What's the next level? - I don't know. I haven't\nhacked the simulation yet.",
    "start": "8025810",
    "end": "8033429"
  },
  {
    "text": "- Well, maybe hacking the\nsimulation is the thing. - I'm working as fast as I can.",
    "start": "8033430",
    "end": "8037183"
  },
  {
    "text": "- And physics would be the way to do that. - Quantum physics. Yeah, definitely. - Well, I hope we do",
    "start": "8038830",
    "end": "8044140"
  },
  {
    "text": "and I hope whatever is outside is even more fun than this one 'cause this one's pretty fun. And just a big thank you for\ndoing the work you're doing.",
    "start": "8044140",
    "end": "8053020"
  },
  {
    "text": "There's so much exciting\ndevelopment in AI. And to ground it in the existential risks",
    "start": "8053020",
    "end": "8060570"
  },
  {
    "text": "is really, really important. The humans love to create stuff and we should be careful not",
    "start": "8061240",
    "end": "8067060"
  },
  {
    "text": "to destroy ourselves in the process. So thank you for doing\nthat really important work.",
    "start": "8067060",
    "end": "8072460"
  },
  {
    "text": "- Thank you so much for\ninviting me. It was amazing. And my dream is to be proven wrong.",
    "start": "8072460",
    "end": "8077710"
  },
  {
    "text": "If everyone just, you know,\npicks up a paper or book and shows how I messed it\nup, that would be optimal.",
    "start": "8077710",
    "end": "8084820"
  },
  {
    "text": "- But for now, the simulation\ncontinues. Thank you, Roman. Thanks for listening to this conversation",
    "start": "8084820",
    "end": "8091060"
  },
  {
    "text": "with Roman Yampolskiy. To support this podcast, please check out our\nsponsors in the description.",
    "start": "8091060",
    "end": "8097090"
  },
  {
    "text": "And now let me leave you with some words from\nFrank Herbert in \"Dune\".",
    "start": "8097090",
    "end": "8102167"
  },
  {
    "text": "\"I must not fear. Fear is the mind killer. Fear is the little death that\nbrings total obliteration.",
    "start": "8102167",
    "end": "8110050"
  },
  {
    "text": "I will face fear, I will permit it to pass over me and through me. And when it has gone past,\nI will turn the inner eye",
    "start": "8110050",
    "end": "8118000"
  },
  {
    "text": "to see its path. Where the fear has gone\nthere will be nothing,",
    "start": "8118000",
    "end": "8123640"
  },
  {
    "text": "only I will remain.\" Thank you for listening and\nhope to see you next time.",
    "start": "8123640",
    "end": "8129853"
  }
]