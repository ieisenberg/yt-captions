[
  {
    "start": "0",
    "end": "134000"
  },
  {
    "text": "today I'd like to overview the exciting field of deep reinforcement learning introduced overview and provide you some",
    "start": "0",
    "end": "7350"
  },
  {
    "text": "of the basics I think it's one of the most exciting fields in artificial",
    "start": "7350",
    "end": "13920"
  },
  {
    "text": "intelligence it's marrying the power and the ability of deep neural networks to",
    "start": "13920",
    "end": "20400"
  },
  {
    "text": "represent and comprehend the world with the ability to act on that understanding",
    "start": "20400",
    "end": "28230"
  },
  {
    "text": "on that representation taking as a whole that's really what the creation of",
    "start": "28230",
    "end": "34530"
  },
  {
    "text": "intelligent beings is understand the world and act and the exciting",
    "start": "34530",
    "end": "40290"
  },
  {
    "text": "breakthroughs that recently have happened captivate our imagination about what's possible and that's why this is",
    "start": "40290",
    "end": "46530"
  },
  {
    "text": "my favorite area of deep learning and artificial intelligence in general and I hope you feel the same so what is deep",
    "start": "46530",
    "end": "53670"
  },
  {
    "text": "reinforcement learning we've talked about deep learning which is taking",
    "start": "53670",
    "end": "58739"
  },
  {
    "text": "samples of data being able to in a supervised way compress encode the",
    "start": "58739",
    "end": "64920"
  },
  {
    "text": "representation that data in the way that you can reason about it I would take that power and apply it to the world",
    "start": "64920",
    "end": "72210"
  },
  {
    "text": "where sequential decisions are to be made so it's looking at problems and",
    "start": "72210",
    "end": "79939"
  },
  {
    "text": "formulations of tasks where an agent an intelligent system has to make a",
    "start": "79939",
    "end": "86939"
  },
  {
    "text": "sequence of decisions and the decisions that are made have an effect on the",
    "start": "86939",
    "end": "93119"
  },
  {
    "text": "world around the agent how how do all of us any intelligent being that it's",
    "start": "93119",
    "end": "100530"
  },
  {
    "text": "tasked with operating in the world how did he learn anything especially when you know very little in the beginning",
    "start": "100530",
    "end": "106490"
  },
  {
    "text": "it's trial and error is the fundamental process by which reinforcement learning",
    "start": "106490",
    "end": "111930"
  },
  {
    "text": "agents learn and the deep part of deep",
    "start": "111930",
    "end": "116939"
  },
  {
    "text": "reinforcement learning is neural networks as using the frameworks and reinforcement learning where the neural",
    "start": "116939",
    "end": "125159"
  },
  {
    "text": "network is doing the representation of the world based on which the actions are",
    "start": "125159",
    "end": "131280"
  },
  {
    "text": "made and we have to take a step back when we look at the types of learning sometimes",
    "start": "131280",
    "end": "138310"
  },
  {
    "start": "134000",
    "end": "395000"
  },
  {
    "text": "the terminology itself can confuse us to the fundamentals there are supervised",
    "start": "138310",
    "end": "144640"
  },
  {
    "text": "learning there semi-supervised learning there's unsupervised learning there's reinforcement learning and there's this",
    "start": "144640",
    "end": "150310"
  },
  {
    "text": "feeling that supervised learning is really the only one where you have to perform the manual annotation where you",
    "start": "150310",
    "end": "156070"
  },
  {
    "text": "have to do the large-scale supervision that's not the case every type of",
    "start": "156070",
    "end": "163540"
  },
  {
    "text": "machine learning is supervised learning it's supervised by a loss function or a",
    "start": "163540",
    "end": "171160"
  },
  {
    "text": "function that tells you what's good and what's bad you know even looking at our",
    "start": "171160",
    "end": "176920"
  },
  {
    "text": "own existence is how we humans figure out what's good and bad there's all",
    "start": "176920",
    "end": "182050"
  },
  {
    "text": "kinds of sources direct and indirect by which our morals and ethics we figure out what's good and bad the difference",
    "start": "182050",
    "end": "189010"
  },
  {
    "text": "we supervised and unsupervised and reinforcement learning is the source of that supervision what's implied when you",
    "start": "189010",
    "end": "195670"
  },
  {
    "text": "say unsupervised is that the cost of human labor required to attain the",
    "start": "195670",
    "end": "201489"
  },
  {
    "text": "supervision is low but it's never Turtles all the way down it's Turtles",
    "start": "201489",
    "end": "208600"
  },
  {
    "text": "and then there's a human at the bottom there at some point there needs to be",
    "start": "208600",
    "end": "214720"
  },
  {
    "text": "human intervention human input to provide what's good and what's bad and",
    "start": "214720",
    "end": "221290"
  },
  {
    "text": "this will arise in reinforcement learning as well I have to remember that because the challenges and the exciting",
    "start": "221290",
    "end": "227440"
  },
  {
    "text": "opportunities of reinforcement learning lie in the fact of how do we get that",
    "start": "227440",
    "end": "232600"
  },
  {
    "text": "supervision in the most efficient way possible but supervision nevertheless is required for any system that has an",
    "start": "232600",
    "end": "239470"
  },
  {
    "text": "input and an output that's trying to learn like a neural network does to",
    "start": "239470",
    "end": "245019"
  },
  {
    "text": "provide an output that's good he needs somebody to say what's good and what's bad for you curious about that there's",
    "start": "245019",
    "end": "251799"
  },
  {
    "text": "been a few books a couple written throughout the last few centuries from Socrates to Nietzsche I recommend the",
    "start": "251799",
    "end": "258070"
  },
  {
    "text": "latter especially so let's look at supervised learning and reinforcement learning let like to propose a way to",
    "start": "258070",
    "end": "266080"
  },
  {
    "text": "think about the difference that is illustrative and useful when we",
    "start": "266080",
    "end": "271990"
  },
  {
    "text": "start talking about the techniques so supervised learning is taking a bunch of examples of data and learning from those",
    "start": "271990",
    "end": "282250"
  },
  {
    "text": "examples where a ground truth provides you the compressed semantic meaning of",
    "start": "282250",
    "end": "288759"
  },
  {
    "text": "what's in that data and from those examples one by one whether it's sequences or single samples we learn",
    "start": "288759",
    "end": "297310"
  },
  {
    "text": "what how to then few take future such samples and interpret them reinforcement",
    "start": "297310",
    "end": "304720"
  },
  {
    "text": "learning is teaching what we teach an agent through experience not by showing",
    "start": "304720",
    "end": "311259"
  },
  {
    "text": "a singular sample of a data set but by putting them out into the world the",
    "start": "311259",
    "end": "316960"
  },
  {
    "text": "distinction there the essential element of reinforcement learning then for us now we'll talk about a bunch of",
    "start": "316960",
    "end": "323110"
  },
  {
    "text": "algorithms but the essential design step",
    "start": "323110",
    "end": "328120"
  },
  {
    "text": "is to provide the world in which to experience the agent learns from the",
    "start": "328120",
    "end": "333969"
  },
  {
    "text": "world the from the world it gets the dynamics of that world the physics of",
    "start": "333969",
    "end": "339849"
  },
  {
    "text": "the world from that world that gets the rewards what's good and bad and us as designers of that agent do not just have",
    "start": "339849",
    "end": "348279"
  },
  {
    "text": "to do the algorithm we have to do design the the world in which that agent is",
    "start": "348279",
    "end": "355090"
  },
  {
    "text": "trying to solve a task the design of the world is the process of reinforcement",
    "start": "355090",
    "end": "361270"
  },
  {
    "text": "learning the design of examples the annotation of examples is the world of supervised learning and the essential",
    "start": "361270",
    "end": "369400"
  },
  {
    "text": "perhaps the most difficult element of reinforcement learning is the reward the good versus bad here a baby starts",
    "start": "369400",
    "end": "378190"
  },
  {
    "text": "walking across the room we want to define success as a baby walking across",
    "start": "378190",
    "end": "385120"
  },
  {
    "text": "the room and reaching the destination that's success and failure is the",
    "start": "385120",
    "end": "390130"
  },
  {
    "text": "inability to reach that destination simple and reinforcement learning in humans",
    "start": "390130",
    "end": "397469"
  },
  {
    "start": "395000",
    "end": "502000"
  },
  {
    "text": "the way we learn from these very few examples appear to learn from very few",
    "start": "397900",
    "end": "404000"
  },
  {
    "text": "examples of trial and error is a mystery a beautiful mystery full of open questions it could be from the huge",
    "start": "404000",
    "end": "410960"
  },
  {
    "text": "amount of data 230 million years worth of bipedal data there who've been walking what mammals walking ability to",
    "start": "410960",
    "end": "418700"
  },
  {
    "text": "walk or 500 million years the ability to see having eyes so that's the the",
    "start": "418700",
    "end": "424310"
  },
  {
    "text": "hardware side somehow genetically encoded in us is the ability to comprehend this world extremely",
    "start": "424310",
    "end": "429830"
  },
  {
    "text": "efficiently it could be through not the hardware not the five hundred million",
    "start": "429830",
    "end": "434960"
  },
  {
    "text": "years but the the few minutes hours days months maybe even years in the very",
    "start": "434960",
    "end": "443300"
  },
  {
    "text": "beginning were born the ability to learn really quickly through observation to",
    "start": "443300",
    "end": "448580"
  },
  {
    "text": "aggregate that information filter all the junk that you don't need and be able to learn really quickly through",
    "start": "448580",
    "end": "455450"
  },
  {
    "text": "imitation learning through observation the way for walking that might mean observing others talk the idea there is",
    "start": "455450",
    "end": "462950"
  },
  {
    "text": "if there was no other around we would never be able to learn this the",
    "start": "462950",
    "end": "468290"
  },
  {
    "text": "fundamentals of this walking or as efficiently it's through observation and",
    "start": "468290",
    "end": "473770"
  },
  {
    "text": "then it could be the algorithm totally not understood is the algorithm that our",
    "start": "473770",
    "end": "479690"
  },
  {
    "text": "brain uses to learn the backpropagation that's an artificial neural networks the",
    "start": "479690",
    "end": "485390"
  },
  {
    "text": "same kind of processes not understood in the brain that could be the key so I",
    "start": "485390",
    "end": "491840"
  },
  {
    "text": "want you to think about that as we talk about the very trivial by comparison",
    "start": "491840",
    "end": "496910"
  },
  {
    "text": "accomplishments and reinforcement learning and how do we take the next steps but it nevertheless is exciting to",
    "start": "496910",
    "end": "505910"
  },
  {
    "start": "502000",
    "end": "735000"
  },
  {
    "text": "have machines that learn how to act in the world the process of learning for",
    "start": "505910",
    "end": "513710"
  },
  {
    "text": "those who have fallen in love with artificial intelligence the process of",
    "start": "513710",
    "end": "518870"
  },
  {
    "text": "learning is thought of as intelligence it's the ability to know very little and through experience examples interaction",
    "start": "518870",
    "end": "526520"
  },
  {
    "text": "with the world in whatever medium whether it's data or simulation so on be able to form much richer and",
    "start": "526520",
    "end": "533459"
  },
  {
    "text": "interesting representations of that world be able to act in that world that's that's the dream so let's look at this stack of what an",
    "start": "533459",
    "end": "540720"
  },
  {
    "text": "age what it means to be an agent in this world from top the input to the bottom",
    "start": "540720",
    "end": "546390"
  },
  {
    "text": "the output is the there's an environment we have to sense that environment we",
    "start": "546390",
    "end": "551610"
  },
  {
    "text": "have just a few tools as humans have several sensory systems on cars you can",
    "start": "551610",
    "end": "558570"
  },
  {
    "text": "have lidar camera stereo vision audio microphone networking GPS IMU sensor so on whatever",
    "start": "558570",
    "end": "566310"
  },
  {
    "text": "robot you can think about there's a way to sense that world and you have this raw sensory data and then once you have",
    "start": "566310",
    "end": "572850"
  },
  {
    "text": "the raw sensory data you're tasked with representing that data in such a way that you can make sense of it as opposed",
    "start": "572850",
    "end": "579089"
  },
  {
    "text": "to all the the raw sensors and the I the cones and so on that taking just giant",
    "start": "579089",
    "end": "585959"
  },
  {
    "text": "stream of high bandwidth information we have to be able to form higher",
    "start": "585959",
    "end": "592640"
  },
  {
    "text": "abstractions of features based on which we can reason from edges to corners to faces and so on that's exactly what deep",
    "start": "592640",
    "end": "599520"
  },
  {
    "text": "learning neural networks have stepped in to be able to in an automated fashion with as little human input as possible",
    "start": "599520",
    "end": "606510"
  },
  {
    "text": "be able to form higher-order representations of that information then",
    "start": "606510",
    "end": "611940"
  },
  {
    "text": "there is the the learning aspect building on top of the greater abstractions form through the",
    "start": "611940",
    "end": "617160"
  },
  {
    "text": "representations be able to accomplish something useful well--there's discriminative tasks a generative task",
    "start": "617160",
    "end": "622950"
  },
  {
    "text": "and so on based on the representation be able to make sense of the data be able to generate new data and so on from",
    "start": "622950",
    "end": "630990"
  },
  {
    "text": "sequence the sequence to sequence the sample from Sam of the sequence and so on and so forth to actions as we'll talk",
    "start": "630990",
    "end": "636540"
  },
  {
    "text": "about and then there is the ability to",
    "start": "636540",
    "end": "642470"
  },
  {
    "text": "aggregate all the information has been received in the past to the useful",
    "start": "642470",
    "end": "649130"
  },
  {
    "text": "information that's pertinent to the task at hand it's the thing the old it looks",
    "start": "649130",
    "end": "656190"
  },
  {
    "text": "like a duck quacks like a duck swims like a duck three different data sets I'm sure there's state-of-the-art",
    "start": "656190",
    "end": "661529"
  },
  {
    "text": "algorithms for the three image class education audio recognition video",
    "start": "661529",
    "end": "667179"
  },
  {
    "text": "classification - activity recognition so on aggregating those three together is",
    "start": "667179",
    "end": "673029"
  },
  {
    "text": "still an open problem and that could be the last piece again I want you to think about as we think about reinforcement",
    "start": "673029",
    "end": "678610"
  },
  {
    "text": "learning agents how do we play how do we transfer from the game of Atari to the game of go to the game of dota to the",
    "start": "678610",
    "end": "687429"
  },
  {
    "text": "game of a robot navigating an uncertain environment in the real world and once",
    "start": "687429",
    "end": "695350"
  },
  {
    "text": "you have that once you sense the raw world once you have a representation of that world then we need to act which is",
    "start": "695350",
    "end": "703799"
  },
  {
    "text": "provide actions within the constraints of the world in such a way that we believe can get us towards success the",
    "start": "703799",
    "end": "711879"
  },
  {
    "text": "promise excitement of deep learning is is the part of the stack that converts",
    "start": "711879",
    "end": "716949"
  },
  {
    "text": "raw data into meaningful representations the promise the dream of deeper enforcement learning is going beyond and",
    "start": "716949",
    "end": "725040"
  },
  {
    "text": "building an agent that uses that representation and acts achieve success",
    "start": "725040",
    "end": "730660"
  },
  {
    "text": "in the world that's super exciting the framework and the formulation",
    "start": "730660",
    "end": "736329"
  },
  {
    "start": "735000",
    "end": "846000"
  },
  {
    "text": "reinforcement learning at its simplest is that there's an environment and",
    "start": "736329",
    "end": "743860"
  },
  {
    "text": "there's an agent that acts in that environment the agent senses the environment by a by some observation",
    "start": "743860",
    "end": "752230"
  },
  {
    "text": "well there's partial or complete observation of the environment and it",
    "start": "752230",
    "end": "758759"
  },
  {
    "text": "gives the environment and action it acts in that environment and through the action the environment changes in some",
    "start": "758759",
    "end": "765939"
  },
  {
    "text": "way and then a new observation occurs and then also as you provide they",
    "start": "765939",
    "end": "771730"
  },
  {
    "text": "actually make the observations you receive a reward in most formulations of this of this framework this entire",
    "start": "771730",
    "end": "779589"
  },
  {
    "text": "system has no memory that the the only",
    "start": "779589",
    "end": "784929"
  },
  {
    "text": "thing you two could be concerned about as a state you came from the state you arrived in and the reward received the",
    "start": "784929",
    "end": "791110"
  },
  {
    "text": "open question here is what can't be modeled in this kind of way can we model all of it",
    "start": "791110",
    "end": "796850"
  },
  {
    "text": "from from human life to the game of go can all this be model in this way and",
    "start": "796850",
    "end": "803019"
  },
  {
    "text": "what are is this a good way to formulate the learning problem of robotic systems",
    "start": "803019",
    "end": "809569"
  },
  {
    "text": "in the real world in simulated world those are the open questions the",
    "start": "809569",
    "end": "815300"
  },
  {
    "text": "environment could be fully observable or partially observable like in poker",
    "start": "815300",
    "end": "821209"
  },
  {
    "text": "it could be single agent or multi agent Atari versus driving like deep traffic",
    "start": "821209",
    "end": "826899"
  },
  {
    "text": "deterministic or stochastic static versus dynamic static is in chess",
    "start": "826899",
    "end": "833329"
  },
  {
    "text": "dynamic again and driving in most real-world applications the screen versus continuous like games chess or",
    "start": "833329",
    "end": "840670"
  },
  {
    "text": "continuous and carpal balancing a polo on a cart the challenge for RL in real world",
    "start": "840670",
    "end": "847370"
  },
  {
    "start": "846000",
    "end": "940000"
  },
  {
    "text": "applications is that as a reminder",
    "start": "847370",
    "end": "855639"
  },
  {
    "text": "supervised learning is teaching by example learning by example teaching",
    "start": "855790",
    "end": "862339"
  },
  {
    "text": "from our perspective reinforcement learning is teaching by experience and the way we provide experience the",
    "start": "862339",
    "end": "869209"
  },
  {
    "text": "reinforcement learning agents currently for the most part is through simulation or through highly constrained real-world",
    "start": "869209",
    "end": "876680"
  },
  {
    "text": "scenarios so the challenge is in the fact that most of the successes is with",
    "start": "876680",
    "end": "886509"
  },
  {
    "text": "systems environments that are simulated so there's two ways to then close this",
    "start": "886509",
    "end": "893810"
  },
  {
    "text": "gap to directions of research and work one is to improve the algorithms improve",
    "start": "893810",
    "end": "902180"
  },
  {
    "text": "the ability of the algorithm student to form policies that are transferable across all kinds of domains including",
    "start": "902180",
    "end": "908509"
  },
  {
    "text": "the real world including especially in the real world so train and simulation transfer to the real world",
    "start": "908509",
    "end": "916509"
  },
  {
    "text": "or is we improve the simulation in such a way that the fidelity of the",
    "start": "916570",
    "end": "921610"
  },
  {
    "text": "simulation increased increases to the point where the gap between reality and simulation is is minimal to a degree",
    "start": "921610",
    "end": "930700"
  },
  {
    "text": "that things learn the simulation are directly trivially transferable to the",
    "start": "930700",
    "end": "935920"
  },
  {
    "text": "to the real world okay the major components of an RL agent",
    "start": "935920",
    "end": "942850"
  },
  {
    "start": "940000",
    "end": "1062000"
  },
  {
    "text": "an agent operates based on a strategy",
    "start": "942850",
    "end": "947980"
  },
  {
    "text": "called the policy it sees the world it makes a decision that's a policy makes a",
    "start": "947980",
    "end": "954070"
  },
  {
    "text": "decision how to act sees the reward sees a new state acts sees a reward",
    "start": "954070",
    "end": "960700"
  },
  {
    "text": "she's new States and acts and this repeats forever until a terminal state",
    "start": "960700",
    "end": "965800"
  },
  {
    "text": "the value function is the estimate of",
    "start": "965800",
    "end": "972010"
  },
  {
    "text": "how good a state is or how good a state action pair is meaning taking an action",
    "start": "972010",
    "end": "979600"
  },
  {
    "text": "in a particular state how good is that ability to evaluate that and then the",
    "start": "979600",
    "end": "986380"
  },
  {
    "text": "model different from the environment from the perspective the agent so the environment has a model based on which",
    "start": "986380",
    "end": "993040"
  },
  {
    "text": "it operates and then the agent has a representation best understanding of that model so the purpose for an RL",
    "start": "993040",
    "end": "1001290"
  },
  {
    "text": "agent in this simply formulated framework is to maximize reward the way",
    "start": "1001290",
    "end": "1008790"
  },
  {
    "text": "that the reward mathematically and practically is talked about is with a",
    "start": "1008790",
    "end": "1015290"
  },
  {
    "text": "discounted framework so we discount further and further future award so the",
    "start": "1015290",
    "end": "1021330"
  },
  {
    "text": "reward that's farther into the future is means less to us in terms of maximization than reward that's in the",
    "start": "1021330",
    "end": "1028439"
  },
  {
    "text": "near term and so why do we discount it so first a lot of it is a math trick to",
    "start": "1028440",
    "end": "1034230"
  },
  {
    "text": "be able to prove certain aspects analyze certain aspects of convergence and in general on a more philosophical sense",
    "start": "1034230",
    "end": "1041270"
  },
  {
    "text": "because environments either are or can be thought of a stochastic random it's",
    "start": "1041270",
    "end": "1047069"
  },
  {
    "text": "very difficult to there's a degree of uncertainty which makes it difficult to really",
    "start": "1047070",
    "end": "1052380"
  },
  {
    "text": "estimate the the the reward they'll be in the future because of the ripple",
    "start": "1052380",
    "end": "1059310"
  },
  {
    "text": "effect of the uncertainty let's look at an example a simple one helps us",
    "start": "1059310",
    "end": "1065430"
  },
  {
    "start": "1062000",
    "end": "1385000"
  },
  {
    "text": "understand policy's rewards actions there's a robot in the room there's 12",
    "start": "1065430",
    "end": "1072690"
  },
  {
    "text": "cells in which you can step it starts in the bottom left it tries to get rewards",
    "start": "1072690",
    "end": "1077700"
  },
  {
    "text": "on the on the top right there's a plus one it's a really good thing at the top right wants to get there by walking",
    "start": "1077700",
    "end": "1084360"
  },
  {
    "text": "around there's a negative 1 which is really bad you wants to avoid that Square and the choice of action is this",
    "start": "1084360",
    "end": "1090990"
  },
  {
    "text": "up-down left-right for actions so you could think of there being a negative",
    "start": "1090990",
    "end": "1097830"
  },
  {
    "text": "reward of point 0 4 for each step so there's a cost to each step and there's a stochastic nature to this world",
    "start": "1097830",
    "end": "1104190"
  },
  {
    "text": "potentially we'll talk about both deterministic stochastic so in the in the stochastic case when you choose the",
    "start": "1104190",
    "end": "1110070"
  },
  {
    "text": "action up with an 80% probability with an 80% chance you move up but with 10%",
    "start": "1110070",
    "end": "1118230"
  },
  {
    "text": "chance to move left another 10 move right so that's the Catholic nature even though you try to go up you might end up",
    "start": "1118230",
    "end": "1124860"
  },
  {
    "text": "in a blocks to the left into the right so for a deterministic world the optimal",
    "start": "1124860",
    "end": "1131790"
  },
  {
    "text": "policy here given that we always start in the bottom left is really shortest path is you know you can't ever because",
    "start": "1131790",
    "end": "1139590"
  },
  {
    "text": "there's no stochasticity you're never gonna screw up and just fall into the hole negative 1 hole that you just",
    "start": "1139590",
    "end": "1144900"
  },
  {
    "text": "compute the shortest path and walk along that shortest path why shortest path",
    "start": "1144900",
    "end": "1149940"
  },
  {
    "text": "because every single step hurts there's a negative a reward to it point 0 4",
    "start": "1149940",
    "end": "1155490"
  },
  {
    "text": "so shortest path is the thing that minimizes the reward shortest path to the to the plus 1 block ok let's look at",
    "start": "1155490",
    "end": "1163800"
  },
  {
    "text": "it stochastic world like I mentioned the 80% up and then split to 20 10 % to left",
    "start": "1163800",
    "end": "1168840"
  },
  {
    "text": "and right how does the policy change well first of all we need to have we",
    "start": "1168840",
    "end": "1175200"
  },
  {
    "text": "need to have a plan for every single block in the area because you might end up there due to this the castus 'ti of",
    "start": "1175200",
    "end": "1180660"
  },
  {
    "text": "the world ok the the basic addition there is that we're trying to",
    "start": "1180660",
    "end": "1185789"
  },
  {
    "text": "go avoid up the closer you get to the negative one hole so just try to avoid",
    "start": "1185789",
    "end": "1192629"
  },
  {
    "text": "up because up the stochastic nature of up means that you might fall into the",
    "start": "1192629",
    "end": "1198809"
  },
  {
    "text": "hole with a 10% chance and given the point zero for step reward you're",
    "start": "1198809",
    "end": "1203820"
  },
  {
    "text": "willing to take the long way home in some cases in order to avoid that possibility the negative one possibility",
    "start": "1203820",
    "end": "1210809"
  },
  {
    "text": "now let's look at a reward for each step if it decreases to negative two it really hurts to take every step then",
    "start": "1210809",
    "end": "1218220"
  },
  {
    "text": "again we go to the shortest path despite the fact that there's a stochastic",
    "start": "1218220",
    "end": "1223710"
  },
  {
    "text": "nature in fact you don't really care that you step into the negative one hole because every step really hurts you just",
    "start": "1223710",
    "end": "1229739"
  },
  {
    "text": "want to get home and then you can play with this reward structure right yes",
    "start": "1229739",
    "end": "1235460"
  },
  {
    "text": "instead of negative 2 or negative point 0 4 you can look at negative 0.1 and you",
    "start": "1235460",
    "end": "1243749"
  },
  {
    "text": "can see immediately that the structure of the policy it changes so with a",
    "start": "1243749",
    "end": "1250919"
  },
  {
    "text": "higher value the higher negative reward free step immediately the urgency of the",
    "start": "1250919",
    "end": "1258239"
  },
  {
    "text": "agent increases versus the less urgency the lower the negative reward and when",
    "start": "1258239",
    "end": "1266039"
  },
  {
    "text": "the reward flips so it's positive the",
    "start": "1266039",
    "end": "1272940"
  },
  {
    "text": "every step is a positive so the entire system which is actually quite common in",
    "start": "1272940",
    "end": "1278359"
  },
  {
    "text": "reinforcement learning the entire system is full of positive rewards and so that then the optimal policy becomes the",
    "start": "1278359",
    "end": "1284489"
  },
  {
    "text": "longest path is grad school taking as",
    "start": "1284489",
    "end": "1289919"
  },
  {
    "text": "long as possible never reaching the destination so what lessons do we draw",
    "start": "1289919",
    "end": "1298080"
  },
  {
    "text": "from robot in the room two things the environment model the dynamics is just",
    "start": "1298080",
    "end": "1303539"
  },
  {
    "text": "there in the trivial example the stochastic nature the difference between 80 percent 100 percent and 50 percent",
    "start": "1303539",
    "end": "1310279"
  },
  {
    "text": "the model of the world the environment has a big impact on what the optimal policy is",
    "start": "1310279",
    "end": "1316500"
  },
  {
    "text": "and the reward structure most importantly the thing we can often",
    "start": "1316500",
    "end": "1321750"
  },
  {
    "text": "control more in our constructs of the",
    "start": "1321750",
    "end": "1328650"
  },
  {
    "text": "task we try to solve them enforcement is the what is good and what is bad and how",
    "start": "1328650",
    "end": "1334620"
  },
  {
    "text": "bad is it and how good is it the reward structure is a big impact and that has a complete change like like",
    "start": "1334620",
    "end": "1343140"
  },
  {
    "text": "Robert Frost say the complete change on the policy the choices the agent makes",
    "start": "1343140",
    "end": "1348570"
  },
  {
    "text": "so at when you formulate a reinforcement learning framework as researchers as",
    "start": "1348570",
    "end": "1355020"
  },
  {
    "text": "students what you often do is you design the environment you design the world in",
    "start": "1355020",
    "end": "1360059"
  },
  {
    "text": "which the system learns even when your ultimate goal is the physical robot it",
    "start": "1360059",
    "end": "1365370"
  },
  {
    "text": "does still there's a lot of work still done simulation so you design the world the parameters of that world and you",
    "start": "1365370",
    "end": "1370919"
  },
  {
    "text": "also design the reward structure and it can have a transformative results slight",
    "start": "1370919",
    "end": "1378450"
  },
  {
    "text": "variations in those parameters going to huge results on huge differences on the policy that's arrived and of course the",
    "start": "1378450",
    "end": "1386960"
  },
  {
    "start": "1385000",
    "end": "1581000"
  },
  {
    "text": "example I've shown before I really love is the impact of the the changing reward",
    "start": "1386960",
    "end": "1395130"
  },
  {
    "text": "structure might have unintended consequences and those consequences for",
    "start": "1395130",
    "end": "1402000"
  },
  {
    "text": "real-world system can have obviously highly detrimental costs that are more",
    "start": "1402000",
    "end": "1409320"
  },
  {
    "text": "than just a failed game of Atari so here is a human performing the task gate",
    "start": "1409320",
    "end": "1414360"
  },
  {
    "text": "playing the game of coast runners racing around the track and so it's when you",
    "start": "1414360",
    "end": "1420600"
  },
  {
    "text": "finish first and you finish fast you get a lot of points and so it's natural to",
    "start": "1420600",
    "end": "1427140"
  },
  {
    "text": "then okay let's do an RL agent and then optimize this for those points and will you find out in the game is that you",
    "start": "1427140",
    "end": "1434490"
  },
  {
    "text": "also get points by picking up the little green turbo things and with agent",
    "start": "1434490",
    "end": "1440970"
  },
  {
    "text": "figures out is that you can actually get a lot more points even by simply focusing on the green turbos",
    "start": "1440970",
    "end": "1449570"
  },
  {
    "text": "focusing on the green turbos just rotating over and over slamming into the wall fire and everything just picking it",
    "start": "1449570",
    "end": "1455909"
  },
  {
    "text": "up especially because ability to pick up those turbos can avoid the terminal",
    "start": "1455909",
    "end": "1463140"
  },
  {
    "text": "state at the end of finishing the race in fact finishing the race means you stop collecting positive reward so you",
    "start": "1463140",
    "end": "1469230"
  },
  {
    "text": "never want to finish collected turbos and though that's a trivial example it's",
    "start": "1469230",
    "end": "1476250"
  },
  {
    "text": "not actually easy to find such examples but they're out there of unintended consequences that can have highly",
    "start": "1476250",
    "end": "1483269"
  },
  {
    "text": "negative detrimental effects when put in the real world we'll talk about a little bit of robotics when you put robots for",
    "start": "1483269",
    "end": "1491039"
  },
  {
    "text": "wheeled ones like autonomous vehicles into the real world and you have objective functions that have to",
    "start": "1491039",
    "end": "1496260"
  },
  {
    "text": "navigate difficult intersections full of pedestrians you have to form intent models those pedestrians here you see",
    "start": "1496260",
    "end": "1502710"
  },
  {
    "text": "cars asserting themselves through dense intersections taking risks and within",
    "start": "1502710",
    "end": "1510360"
  },
  {
    "text": "those risks that are taking by us humans will drive vehicles we have to then encode that ability to take subtle risk",
    "start": "1510360",
    "end": "1518190"
  },
  {
    "text": "into into AI based control algorithms",
    "start": "1518190",
    "end": "1523500"
  },
  {
    "text": "perception then you have to think about at the end of the day there's an",
    "start": "1523500",
    "end": "1528720"
  },
  {
    "text": "objective function and if that objective function does not anticipate the green",
    "start": "1528720",
    "end": "1533760"
  },
  {
    "text": "turbos that are to be collected and then result in some understand the consequences could have very negative",
    "start": "1533760",
    "end": "1543600"
  },
  {
    "text": "effects especially in situations that involve human life that's the field of",
    "start": "1543600",
    "end": "1549059"
  },
  {
    "text": "AI safety and some of the folks will talk about deep mind and open AI that are doing incredible work in RL also",
    "start": "1549059",
    "end": "1556139"
  },
  {
    "text": "have groups that are working on a AI safety for a very good reason this is a",
    "start": "1556139",
    "end": "1561600"
  },
  {
    "text": "problem that I believe that artificial intelligent will define some of the most",
    "start": "1561600",
    "end": "1566940"
  },
  {
    "text": "impactful positive things in the 21st century but I also believe we are nowhere close",
    "start": "1566940",
    "end": "1573269"
  },
  {
    "text": "to solving some of the fundamental problems of AI safety that we also need",
    "start": "1573269",
    "end": "1578399"
  },
  {
    "text": "to address as we those algorithms okay examples and reinforcement learning systems all of it",
    "start": "1578399",
    "end": "1585540"
  },
  {
    "start": "1581000",
    "end": "1792000"
  },
  {
    "text": "has to do with formulation or rewards formulation of states and actions you have the traditional the often used",
    "start": "1585540",
    "end": "1595140"
  },
  {
    "text": "benchmark of a cart balancing a poll continuous so the action is the",
    "start": "1595140",
    "end": "1600780"
  },
  {
    "text": "horizontal force to the cart the goal is to balance the poll so stays top and the moving cart and the reward is one in",
    "start": "1600780",
    "end": "1608040"
  },
  {
    "text": "each time step if the poll is upright in the state measured by the cart by the",
    "start": "1608040",
    "end": "1613350"
  },
  {
    "text": "agent is the pole angle angular speed and of course self sensing of the cart",
    "start": "1613350",
    "end": "1619560"
  },
  {
    "text": "position and the horizontal velocity another example here didn't want to",
    "start": "1619560",
    "end": "1624660"
  },
  {
    "text": "include the video because it's really disturbing but I do want to include the slide because it's really important to",
    "start": "1624660",
    "end": "1630300"
  },
  {
    "text": "think about is by sensing the the raw pixels learning and teaching an agent to",
    "start": "1630300",
    "end": "1636950"
  },
  {
    "text": "play a game of doom so the goal there is to eliminate all opponents the state is",
    "start": "1636950",
    "end": "1644010"
  },
  {
    "text": "the raw game pixels the action is up/down shoot reload and so on and the",
    "start": "1644010",
    "end": "1651960"
  },
  {
    "text": "positive reward is when an opponent is eliminated and negative one the agent is",
    "start": "1651960",
    "end": "1657660"
  },
  {
    "text": "eliminated simple I added it here because again on the topic of AI safety",
    "start": "1657660",
    "end": "1665420"
  },
  {
    "text": "we have to think about objective functions and how that translate into",
    "start": "1665420",
    "end": "1670440"
  },
  {
    "text": "the world of not just autonomous vehicles but things that even more",
    "start": "1670440",
    "end": "1678090"
  },
  {
    "text": "directly have harm like autonomous weapon systems and we have a lecture on this in the AGI series and on the",
    "start": "1678090",
    "end": "1685980"
  },
  {
    "text": "robotics platform the manipulate object manipulation and grasping objects there's a few benchmarks there's a few",
    "start": "1685980",
    "end": "1692490"
  },
  {
    "text": "interesting applications learning the problem of grabbing objects moving",
    "start": "1692490",
    "end": "1697650"
  },
  {
    "text": "objects manipulating objects rotating and so on especially when those objects",
    "start": "1697650",
    "end": "1703230"
  },
  {
    "text": "don't have have complicated shapes and so the goal is to pick up an object in",
    "start": "1703230",
    "end": "1708780"
  },
  {
    "text": "the purely in the grasping objects allenge the state is the visual racial slurs visual visual base the raw",
    "start": "1708780",
    "end": "1714960"
  },
  {
    "text": "pixels of the objects the actions is to move the arm grasp the object pick it up",
    "start": "1714960",
    "end": "1720240"
  },
  {
    "text": "and obviously it's positive when the pickup is successful the reason I'm",
    "start": "1720240",
    "end": "1725760"
  },
  {
    "text": "personally excited by this is because it'll finally allow us to solve the",
    "start": "1725760",
    "end": "1732420"
  },
  {
    "text": "problem of the claw which has been torturing me for many years",
    "start": "1732420",
    "end": "1737670"
  },
  {
    "text": "I don't know that's not at all why I'm excited by it okay and then we have to",
    "start": "1737670",
    "end": "1743760"
  },
  {
    "text": "think about as we get greater and greater degree of application in the real world with robotics",
    "start": "1743760",
    "end": "1749360"
  },
  {
    "text": "like cars the the main focus of my passion in terms of robotics is how do",
    "start": "1749360",
    "end": "1755400"
  },
  {
    "text": "we encode some of the things that us humans encode how do we you know we have to think about our own objective",
    "start": "1755400",
    "end": "1761460"
  },
  {
    "text": "function our own reward structure our own model of the environment about which we perceive and reasonable in order to",
    "start": "1761460",
    "end": "1768480"
  },
  {
    "text": "then encode machines that are doing the same and I believe autonomous driving is in that category but to ask questions of",
    "start": "1768480",
    "end": "1774450"
  },
  {
    "text": "ethics we have to ask questions of of risk value of human life value of",
    "start": "1774450",
    "end": "1781470"
  },
  {
    "text": "efficiency money and so on all these in front of ethical questions that an autonomous vehicle unfortunately has to",
    "start": "1781470",
    "end": "1787230"
  },
  {
    "text": "solve before it becomes fully autonomous so here are the key takeaways of the",
    "start": "1787230",
    "end": "1795210"
  },
  {
    "start": "1792000",
    "end": "1885000"
  },
  {
    "text": "real-world impact of reinforcement learning agents on the deep learning",
    "start": "1795210",
    "end": "1801030"
  },
  {
    "text": "side okay these neural networks that form high representation the fun part is the algorithms all the different",
    "start": "1801030",
    "end": "1806640"
  },
  {
    "text": "architectures the different encoder/decoder structures all the attentions self attention recurrent",
    "start": "1806640",
    "end": "1814650"
  },
  {
    "text": "Sallust Engr use all the fun architectures and the data so that and",
    "start": "1814650",
    "end": "1819950"
  },
  {
    "text": "the ability to leverage different data sets in order to discriminate better",
    "start": "1819950",
    "end": "1826080"
  },
  {
    "text": "than perform this Crematory tasks better than you know MIT does better than stand",
    "start": "1826080",
    "end": "1833100"
  },
  {
    "text": "for that kind of thing that's the fun part the hard part is asking good questions and collecting huge amounts of",
    "start": "1833100",
    "end": "1840150"
  },
  {
    "text": "data that's representative over the task that's for real world impact not cvpr publication real-world impact",
    "start": "1840150",
    "end": "1846630"
  },
  {
    "text": "a huge amount of data on a deeper enforcement learning side the key challenge the fun part again is the",
    "start": "1846630",
    "end": "1854190"
  },
  {
    "text": "algorithms how do we learn from data some of the stuff I'll talk about today the hard part is defining the",
    "start": "1854190",
    "end": "1860909"
  },
  {
    "text": "environment defining the acts of space and the reward structure as I mentioned this is the big challenge and the",
    "start": "1860909",
    "end": "1866820"
  },
  {
    "text": "hardest part is how to crack the gap between simulation in the real world the leaping lizard that's the hardest part",
    "start": "1866820",
    "end": "1873899"
  },
  {
    "text": "we don't even know how to solve that transfer learning problem yet for the real world in fact the three types of",
    "start": "1873899",
    "end": "1881250"
  },
  {
    "text": "reinforcement learning there's countless",
    "start": "1881250",
    "end": "1886710"
  },
  {
    "start": "1885000",
    "end": "2128000"
  },
  {
    "text": "algorithms and there's a lot of ways to economize them but at the highest level there's model-based and there's model",
    "start": "1886710",
    "end": "1893490"
  },
  {
    "text": "free model based algorithms learn the model of the world so as you interact",
    "start": "1893490",
    "end": "1900059"
  },
  {
    "text": "with the world you construct your estimate of how you believe the dynamics",
    "start": "1900059",
    "end": "1906990"
  },
  {
    "text": "of that world operates the nice thing about doing that is once you have a",
    "start": "1906990",
    "end": "1913649"
  },
  {
    "text": "model or an estimate of a model you're able to anticipate you're able to plan",
    "start": "1913649",
    "end": "1919200"
  },
  {
    "text": "into the future you're able to use the model to in a branching way predict how",
    "start": "1919200",
    "end": "1926759"
  },
  {
    "text": "your actions will change the world so you can plan far into the future this is the mechanism by which you can you",
    "start": "1926759",
    "end": "1933120"
  },
  {
    "text": "can do chess in the simplest form because in chess you don't even need to",
    "start": "1933120",
    "end": "1938370"
  },
  {
    "text": "learn the model the models learnt is given to you chess go and so on the most important way in which they're",
    "start": "1938370",
    "end": "1943980"
  },
  {
    "text": "different I think is the sample efficiency is how many examples of data",
    "start": "1943980",
    "end": "1949470"
  },
  {
    "text": "are needed to be able to successfully operate in the world and so model based methods because they're constructing a",
    "start": "1949470",
    "end": "1954899"
  },
  {
    "text": "model if they can are extremely simple efficient because once you have a model",
    "start": "1954899",
    "end": "1961139"
  },
  {
    "text": "you can do all kinds of reasoning that doesn't require experiencing every possibility of that model you can unroll",
    "start": "1961139",
    "end": "1969149"
  },
  {
    "text": "the model to see how the world changes based on your actions value based",
    "start": "1969149",
    "end": "1975990"
  },
  {
    "text": "methods are ones that look to estimate the quality of states the quality of",
    "start": "1975990",
    "end": "1981929"
  },
  {
    "text": "taking a certain action in the certain state so they're called off policy",
    "start": "1981929",
    "end": "1988320"
  },
  {
    "text": "versus the last category that's on policy what does it mean to be off policy it means that they constantly",
    "start": "1988320",
    "end": "1997530"
  },
  {
    "text": "value based agents constantly update how good is taken action in a state and they",
    "start": "1997530",
    "end": "2004380"
  },
  {
    "text": "have this model of that goodness of taking action in a state and they use",
    "start": "2004380",
    "end": "2010320"
  },
  {
    "text": "that to pick them optimal action they don't directly learn a policy a strategy",
    "start": "2010320",
    "end": "2017010"
  },
  {
    "text": "of how to act they learn how good it is to be in a state and use that goodness",
    "start": "2017010",
    "end": "2022799"
  },
  {
    "text": "information to then pick the best one and then every once in a while flip a",
    "start": "2022799",
    "end": "2028650"
  },
  {
    "text": "coin in order to explore and then policy based methods our ones that directly",
    "start": "2028650",
    "end": "2034080"
  },
  {
    "text": "learn a policy function so they take as input the the world representation of",
    "start": "2034080",
    "end": "2042750"
  },
  {
    "text": "that world neural networks and this output a action where the action is",
    "start": "2042750",
    "end": "2048419"
  },
  {
    "text": "stochastic so okay that's the range of model-based value based and policy based",
    "start": "2048419",
    "end": "2054740"
  },
  {
    "text": "here's an image from open AI that I really like I encourage you to as we",
    "start": "2054740",
    "end": "2061530"
  },
  {
    "text": "further explore here to look up spinning up in deeper enforcement learning from open AI here's an image that texana",
    "start": "2061530",
    "end": "2068128"
  },
  {
    "text": "mises in the way that I described some of the recent developments in RL so at",
    "start": "2068129",
    "end": "2073408"
  },
  {
    "text": "the very top the distinction between model free RL and model-based RL in",
    "start": "2073409",
    "end": "2079940"
  },
  {
    "text": "model free RL which is what we'll focus on today there is a distinction between",
    "start": "2079940",
    "end": "2085349"
  },
  {
    "text": "policy optimization so on policy methods and q-learning",
    "start": "2085349",
    "end": "2091080"
  },
  {
    "text": "which is all policy methods pause optimizations methods that directly optimize the policy they'll directly",
    "start": "2091080",
    "end": "2097080"
  },
  {
    "text": "learn the policy in some way and then q-learning off policy methods learn like",
    "start": "2097080",
    "end": "2103410"
  },
  {
    "text": "I mentioned the value of taking a certain action in the state and from that learned that learned Q value be",
    "start": "2103410",
    "end": "2112530"
  },
  {
    "text": "able to choose how to act in the world so let's look at a few sample representative",
    "start": "2112530",
    "end": "2120660"
  },
  {
    "text": "approaches in this space let's start with the with the one that really was",
    "start": "2120660",
    "end": "2128500"
  },
  {
    "start": "2128000",
    "end": "2320000"
  },
  {
    "text": "one of the first great breakthroughs from google deepmind on the deep IRL side and solving atari games dqn deep",
    "start": "2128500",
    "end": "2136480"
  },
  {
    "text": "queue learning networks deep queue networks and let's take a step back and",
    "start": "2136480",
    "end": "2142060"
  },
  {
    "text": "think about what cue learning is q-learning looks at the state action value function queue that estimates",
    "start": "2142060",
    "end": "2151120"
  },
  {
    "text": "based on a particular policy or based on an optimal policy how good is it to take an action in this state the estimated",
    "start": "2151120",
    "end": "2162240"
  },
  {
    "text": "reward if I take an action in this state and continue operating under an optimal",
    "start": "2162240",
    "end": "2167560"
  },
  {
    "text": "optimal policy it gives you directly a way to say amongst all the actions I",
    "start": "2167560",
    "end": "2174310"
  },
  {
    "text": "have which action should that take to maximize the reward now in the beginning you know nothing you know you don't have",
    "start": "2174310",
    "end": "2180700"
  },
  {
    "text": "this value estimation you don't have this cue function so you have to learn it and you learn it with a bellman",
    "start": "2180700",
    "end": "2187480"
  },
  {
    "text": "equation of updating it you take your current estimate and update it with the reward you seed received after you take",
    "start": "2187480",
    "end": "2194050"
  },
  {
    "text": "an action here it's off policy and model",
    "start": "2194050",
    "end": "2199690"
  },
  {
    "text": "free you don't have to have any estimate or knowledge of the world you don't have to have any policy whatsoever all you're",
    "start": "2199690",
    "end": "2206800"
  },
  {
    "text": "doing is roaming about the world collecting data when you took a certain action here award you received and",
    "start": "2206800",
    "end": "2212860"
  },
  {
    "text": "you're updating gradually this table where the table has state states on the",
    "start": "2212860",
    "end": "2222550"
  },
  {
    "text": "y-axis and actions on the x-axis and the",
    "start": "2222550",
    "end": "2229900"
  },
  {
    "text": "key part there is because you always have an estimate of what of to take an",
    "start": "2229900",
    "end": "2236320"
  },
  {
    "text": "action of the value of taking that action so you can always take the optimal one but because you know very",
    "start": "2236320",
    "end": "2242440"
  },
  {
    "text": "little in the beginning that optimal is going to you have no way of knowing",
    "start": "2242440",
    "end": "2247450"
  },
  {
    "text": "that's good or not so there's some degree of expiration the fundamental aspect of value based methods or ami are",
    "start": "2247450",
    "end": "2253720"
  },
  {
    "text": "all methods like I said it's trial and error is exploration so for value based",
    "start": "2253720",
    "end": "2259270"
  },
  {
    "text": "methods that q-learning the way that's done is with the flip of a coin epsilon greedy with a flip of a",
    "start": "2259270",
    "end": "2265900"
  },
  {
    "text": "coin you can choose to just take a random action and you slowly decrease epsilon",
    "start": "2265900",
    "end": "2274930"
  },
  {
    "text": "to zero as your agent learns more and more and more so in the beginning you explore a lot with epsilon 1 and epsilon",
    "start": "2274930",
    "end": "2281380"
  },
  {
    "text": "of zero in the end when you're just acting greedy based on the your",
    "start": "2281380",
    "end": "2286450"
  },
  {
    "text": "understanding of the world as represented by the q-value function for non neural network approaches this is",
    "start": "2286450",
    "end": "2292930"
  },
  {
    "text": "simply a table the Q this Q function is a table like I said on the Y State X",
    "start": "2292930",
    "end": "2300300"
  },
  {
    "text": "actions and in each cell you have a reward that's at this counter reward",
    "start": "2300300",
    "end": "2307570"
  },
  {
    "text": "that you estimated to be received there and as you walk around with this bellami equation you can update that table but",
    "start": "2307570",
    "end": "2314080"
  },
  {
    "text": "it's a table nevertheless number of states times number of actions now if",
    "start": "2314080",
    "end": "2319480"
  },
  {
    "text": "you look at any practical real-world problem and an arcade game with raw",
    "start": "2319480",
    "end": "2325690"
  },
  {
    "start": "2320000",
    "end": "2880000"
  },
  {
    "text": "sensory input is a very crude first step towards the real world so raw sensor",
    "start": "2325690",
    "end": "2330730"
  },
  {
    "text": "information this kind of value iteration and updating a table is impractical",
    "start": "2330730",
    "end": "2337300"
  },
  {
    "text": "because here's for a game of break out if we look at four consecutive frames of a game of breakout size of the of the",
    "start": "2337300",
    "end": "2346180"
  },
  {
    "text": "raw sensory input is 84 by 84 pixels grayscale every pixel has 256 values",
    "start": "2346180",
    "end": "2355020"
  },
  {
    "text": "that's 256 to the power of whatever 84",
    "start": "2355020",
    "end": "2362680"
  },
  {
    "text": "times 84 times 4 is whatever it is it's significantly larger the number of atoms",
    "start": "2362680",
    "end": "2369100"
  },
  {
    "text": "in the universe so the size of this cue table if we use the traditional approach",
    "start": "2369100",
    "end": "2374320"
  },
  {
    "text": "is intractable",
    "start": "2374320",
    "end": "2377460"
  },
  {
    "text": "you'll know it's to the rescue deep RL is rl+ neural networks where the neural",
    "start": "2379470",
    "end": "2385020"
  },
  {
    "text": "networks is tasked with taking this in Valley based methods taking this cue",
    "start": "2385020",
    "end": "2391500"
  },
  {
    "text": "table and learning a compress representation of it learning an approximator for the function from state",
    "start": "2391500",
    "end": "2398790"
  },
  {
    "text": "action to the value that's what previously talked about the ability the",
    "start": "2398790",
    "end": "2404760"
  },
  {
    "text": "powerful ability of neural networks to form representations from extremely high",
    "start": "2404760",
    "end": "2410850"
  },
  {
    "text": "dimensional complex raw sensory information so it's simple the framework",
    "start": "2410850",
    "end": "2416130"
  },
  {
    "text": "remains for the most part the same in reinforcement learning it's just that this cue function for",
    "start": "2416130",
    "end": "2423600"
  },
  {
    "text": "value based methods becomes a neural network and becomes an approximator where the hope is as you navigate the",
    "start": "2423600",
    "end": "2430710"
  },
  {
    "text": "world and you pick up new knowledge through the back propagating the gradient and the loss function that",
    "start": "2430710",
    "end": "2437400"
  },
  {
    "text": "you're able to form a good representation of the optimal q function so using your networks with you'll know",
    "start": "2437400",
    "end": "2444150"
  },
  {
    "text": "it's a good at which is function approximator x' and that's DQ 1 deep Q Network was used to have the initial",
    "start": "2444150",
    "end": "2452660"
  },
  {
    "text": "incredible nice results on our K games where the input is the raw sensory",
    "start": "2452660",
    "end": "2457740"
  },
  {
    "text": "pixels with a few convolutional layers for the connected layers and the output is a set of actions you know probability",
    "start": "2457740",
    "end": "2467250"
  },
  {
    "text": "of taking that action and then you sample that and you choose the best action and so this simple agent whether",
    "start": "2467250",
    "end": "2472350"
  },
  {
    "text": "the neural network that estimates that Q function very simple network is able to",
    "start": "2472350",
    "end": "2477360"
  },
  {
    "text": "achieve superhuman performance on many of these arcade games that excited the",
    "start": "2477360",
    "end": "2482910"
  },
  {
    "text": "world because it's taking raw sensory information with a pretty simple network",
    "start": "2482910",
    "end": "2488010"
  },
  {
    "text": "that doesn't in the beginning understand any of the physics of the world any of the dynamics of the environment and",
    "start": "2488010",
    "end": "2493560"
  },
  {
    "text": "through that intractable space the intractable state space is able to learn",
    "start": "2493560",
    "end": "2501060"
  },
  {
    "text": "how to actually do pretty well the loss function for DQ n has to Q functions one",
    "start": "2501060",
    "end": "2512160"
  },
  {
    "text": "is the expected the predicted Q value of",
    "start": "2512160",
    "end": "2518670"
  },
  {
    "text": "a taking an action in a particular state and the other is the target against",
    "start": "2518670",
    "end": "2525630"
  },
  {
    "text": "which the loss function is calculated which is what is the value that you got",
    "start": "2525630",
    "end": "2531240"
  },
  {
    "text": "once you actually take in that action and once you've taken that action the",
    "start": "2531240",
    "end": "2537180"
  },
  {
    "text": "way you calculate the value is by looking at the next step and choosing the max to Singh if you take the best",
    "start": "2537180",
    "end": "2544410"
  },
  {
    "text": "action in the next state what is going to be the Q function so there's two estimators going on with in terms of",
    "start": "2544410",
    "end": "2551250"
  },
  {
    "text": "neural networks those two forward passes here there's two Q's in this equation so in traditional DQ n that's just that's",
    "start": "2551250",
    "end": "2560010"
  },
  {
    "text": "done by a single neural network with a few tricks and double DQ n that's done",
    "start": "2560010",
    "end": "2565110"
  },
  {
    "text": "by two neural networks and I mentioned tricks because with this and with most",
    "start": "2565110",
    "end": "2571110"
  },
  {
    "text": "of RL tricks tell a lot of the story a lot of what makes",
    "start": "2571110",
    "end": "2577170"
  },
  {
    "text": "systems work is the details in in games and robotic systems in these cases the",
    "start": "2577170",
    "end": "2586080"
  },
  {
    "text": "two biggest tricks for DQ n that will reappear and a lot of value based methods is experience replay so think of",
    "start": "2586080",
    "end": "2594000"
  },
  {
    "text": "an agent that plays through these games as also collecting memories you collect",
    "start": "2594000",
    "end": "2600300"
  },
  {
    "text": "this bank of memories that can then be replayed the power of that one of the",
    "start": "2600300",
    "end": "2607290"
  },
  {
    "text": "central elements of what makes value based methods attractive is that because",
    "start": "2607290",
    "end": "2613830"
  },
  {
    "text": "you're not directly estimating the policy but are learning the quality of taking an action in a particular state",
    "start": "2613830",
    "end": "2619230"
  },
  {
    "text": "the you're able to then jump around through your memory and and play",
    "start": "2619230",
    "end": "2625610"
  },
  {
    "text": "different aspects of that memory so learn train the network through the",
    "start": "2625610",
    "end": "2631470"
  },
  {
    "text": "historical data and then the other trick simple is like I said that there is so",
    "start": "2631470",
    "end": "2639000"
  },
  {
    "text": "the loss function has two queues so you're it's it's a dragon chasing its",
    "start": "2639000",
    "end": "2644720"
  },
  {
    "text": "own tail it's easy for the loss function to become unstable so the training does",
    "start": "2644720",
    "end": "2649970"
  },
  {
    "text": "not converge so the trick of fixing a target Network is taking one of the queues and only updating in every X",
    "start": "2649970",
    "end": "2657170"
  },
  {
    "text": "steps every thousand steps and so on and taking the same kind of network it's just fixing it so for the target",
    "start": "2657170",
    "end": "2663200"
  },
  {
    "text": "network that defines the loss function just keeping it fixed and only updating any regulator so you're chasing a fixed",
    "start": "2663200",
    "end": "2669800"
  },
  {
    "text": "target with a loss function as opposed to a dynamic one so you can solve a lot",
    "start": "2669800",
    "end": "2675680"
  },
  {
    "text": "of the Atari games with minimal effort come up with some creative solutions here break out here after 10 minutes of",
    "start": "2675680",
    "end": "2683510"
  },
  {
    "text": "training on the left after a to have 2 hours of training on the right is coming up with some creative solutions again",
    "start": "2683510",
    "end": "2689210"
  },
  {
    "text": "it's pretty cool because this is raw pixels right we're now like there's been",
    "start": "2689210",
    "end": "2695359"
  },
  {
    "text": "a few years since this breakthrough so kind of take it for granted but I still",
    "start": "2695359",
    "end": "2702260"
  },
  {
    "text": "for the most part captivated by just how beautiful it is that from the raw",
    "start": "2702260",
    "end": "2707390"
  },
  {
    "text": "sensory information neural networks are able to learn to act",
    "start": "2707390",
    "end": "2712819"
  },
  {
    "text": "in a way that actually supersedes humans in terms of creativity in terms of in terms of actual raw performance it's",
    "start": "2712819",
    "end": "2719809"
  },
  {
    "text": "really exciting and games of simple form is the cleanest way to demonstrate that and you the the same kind of DQ and",
    "start": "2719809",
    "end": "2728000"
  },
  {
    "text": "network is able to achieve superhuman performance and a bunch of different games there's improvements to this like dual",
    "start": "2728000",
    "end": "2734690"
  },
  {
    "text": "DQ one again the q function can be decomposed which is useful in to the",
    "start": "2734690",
    "end": "2740210"
  },
  {
    "text": "value estimate of being in that state and what's called and in future slides",
    "start": "2740210",
    "end": "2745790"
  },
  {
    "text": "that we called advantage so the advantage of taking action in that state the nice thing of the",
    "start": "2745790",
    "end": "2751309"
  },
  {
    "text": "advantage as a measure is that it's a measure of the action quality relative",
    "start": "2751309",
    "end": "2758510"
  },
  {
    "text": "to the average action that could be taken there so if it's very useful",
    "start": "2758510",
    "end": "2764660"
  },
  {
    "text": "advantage versus sort of raw reward is that if all the actions you have to take are pretty good you want to know well",
    "start": "2764660",
    "end": "2772010"
  },
  {
    "text": "how much better it is in terms of optimism that's a better measure for choosing",
    "start": "2772010",
    "end": "2778010"
  },
  {
    "text": "actions in a value-based sense so when you have these two estimates you have",
    "start": "2778010",
    "end": "2784430"
  },
  {
    "text": "these two streams for neural networking the dueling DQ n DG QM where one",
    "start": "2784430",
    "end": "2790250"
  },
  {
    "text": "estimates the value the other the advantage and that's again that dueling",
    "start": "2790250",
    "end": "2797900"
  },
  {
    "text": "nature is useful for also on the there are many states in which the action is",
    "start": "2797900",
    "end": "2804340"
  },
  {
    "text": "decoupled the quality of the actions is decouple from the state so many states",
    "start": "2804340",
    "end": "2809840"
  },
  {
    "text": "it doesn't matter which action you take so you don't need to learn all the",
    "start": "2809840",
    "end": "2816170"
  },
  {
    "text": "different complexities all the topology of different actions when you in a particular state and another one is",
    "start": "2816170",
    "end": "2825700"
  },
  {
    "text": "prioritize experience for play like I said experience replay is really key to these algorithms and the thing that",
    "start": "2825700",
    "end": "2832310"
  },
  {
    "text": "sinks some of the policy optimization methods and experiments replay is collecting different memories but if you",
    "start": "2832310",
    "end": "2840500"
  },
  {
    "text": "just sample randomly in those memories you're now affected the sampled",
    "start": "2840500",
    "end": "2846380"
  },
  {
    "text": "experiences are really affected by the frequency of those experience occurred not their importance so prioritize",
    "start": "2846380",
    "end": "2854030"
  },
  {
    "text": "experience replay assigns a priority a value based on the magnitude of the",
    "start": "2854030",
    "end": "2862090"
  },
  {
    "text": "temporal difference learned error so the the stuff you have learned the most from",
    "start": "2862090",
    "end": "2867230"
  },
  {
    "text": "is given a higher priority and therefore you get to see through the experience",
    "start": "2867230",
    "end": "2873350"
  },
  {
    "text": "replay process that that particular experience more often okay moving on to",
    "start": "2873350",
    "end": "2883070"
  },
  {
    "start": "2880000",
    "end": "3036000"
  },
  {
    "text": "policy gradients this is on policy versus q-learning off policy policy",
    "start": "2883070",
    "end": "2889880"
  },
  {
    "text": "gradient is directly optimizing the policy where",
    "start": "2889880",
    "end": "2895440"
  },
  {
    "text": "the input is the raw pixels and the policy network represents the forms of",
    "start": "2895440",
    "end": "2904050"
  },
  {
    "text": "representations of that environment space and as output produces a stochastic estimate a probability of the",
    "start": "2904050",
    "end": "2910890"
  },
  {
    "text": "different actions here in the pong the pixels a single output that produces the",
    "start": "2910890",
    "end": "2917070"
  },
  {
    "text": "probability of moving the paddle up so how do pause gradients vanilla policy grading the very basic works is you",
    "start": "2917070",
    "end": "2924600"
  },
  {
    "text": "unroll the environment you play through the environment here pong moving the",
    "start": "2924600",
    "end": "2930930"
  },
  {
    "text": "paddle up and down and so on collecting no rewards and only collecting reward at",
    "start": "2930930",
    "end": "2937290"
  },
  {
    "text": "the very end based on whether you win or lose every single action you're taking",
    "start": "2937290",
    "end": "2943200"
  },
  {
    "text": "along the way gets either punished or rewarded based on whether it led to victory or defeat this also is",
    "start": "2943200",
    "end": "2951660"
  },
  {
    "text": "remarkable that this works at all because the credit assignment there's a is I mean every single thing you did",
    "start": "2951660",
    "end": "2959430"
  },
  {
    "text": "along the way is averaged out it's like muddied it's the reason that policy",
    "start": "2959430",
    "end": "2966450"
  },
  {
    "text": "gradient methods are more inefficient but it's still very surprising that it works at all so the pros versus DQ one",
    "start": "2966450",
    "end": "2974730"
  },
  {
    "text": "the value based methods is that if the world is so messy that you can't learn a q function the nice thing about policy",
    "start": "2974730",
    "end": "2980910"
  },
  {
    "text": "gradient because it's learning the policy directly that it will at least learn a pretty good policy usually in",
    "start": "2980910",
    "end": "2987270"
  },
  {
    "text": "many cases faster convergence it's able to deal with stochastic policies so value based methods can out learners the",
    "start": "2987270",
    "end": "2993870"
  },
  {
    "text": "gassing policies and it's much more naturally able to deal with continuous actions the cons is it's inefficient",
    "start": "2993870",
    "end": "3001450"
  },
  {
    "text": "versus dqn it's it can become highly",
    "start": "3001450",
    "end": "3006470"
  },
  {
    "text": "unstable as we'll talk about some solutions to this during the training process and the credit assignment so if",
    "start": "3006470",
    "end": "3013550"
  },
  {
    "text": "we look at the chain of actions that lead to a positive reward some might be",
    "start": "3013550",
    "end": "3019250"
  },
  {
    "text": "awesome action some may be good action some might be terrible actions but that doesn't matter as long as the death",
    "start": "3019250",
    "end": "3025640"
  },
  {
    "text": "the nation was good and that's then every single action along the way gets a positive reinforcement that's the",
    "start": "3025640",
    "end": "3033650"
  },
  {
    "text": "downside and there's now improvements to that advantage actor critic methods a to see combining the best of value based",
    "start": "3033650",
    "end": "3043460"
  },
  {
    "start": "3036000",
    "end": "3172000"
  },
  {
    "text": "methods and policy base methods so",
    "start": "3043460",
    "end": "3048829"
  },
  {
    "text": "having an actor two networks an actor which is policy based and that's the one",
    "start": "3048829",
    "end": "3055760"
  },
  {
    "text": "that's takes the actions samples the actions from the policy Network and the critic that measures how good those",
    "start": "3055760",
    "end": "3062839"
  },
  {
    "text": "actions are and the critic is value based all right so as opposed to in the",
    "start": "3062839",
    "end": "3068809"
  },
  {
    "text": "policy update the first equation there the reward coming from the destination the that our war being from whether you",
    "start": "3068809",
    "end": "3076369"
  },
  {
    "text": "won the game or not every single step along the way you now learn a Q value",
    "start": "3076369",
    "end": "3082400"
  },
  {
    "text": "function Q s a state and action using the critic Network so you're able to now",
    "start": "3082400",
    "end": "3091160"
  },
  {
    "text": "learn about the environment about evaluating your own actions at every step so you're much more sample",
    "start": "3091160",
    "end": "3096380"
  },
  {
    "text": "efficient there's a synchronous from deep mind and synchronous from open AI",
    "start": "3096380",
    "end": "3102170"
  },
  {
    "text": "variants of this but of the actor advantage actor critic framework but",
    "start": "3102170",
    "end": "3107960"
  },
  {
    "text": "both are highly parallelizable the difference with a three C the",
    "start": "3107960",
    "end": "3115000"
  },
  {
    "text": "asynchronous one is that every single agency just throw these agents operating",
    "start": "3115000",
    "end": "3121309"
  },
  {
    "text": "in the environment and they're learning they're rolling out the games and getting the reward they're updating the",
    "start": "3121309",
    "end": "3127160"
  },
  {
    "text": "original Network asynchronously the global network parameters asynchronously",
    "start": "3127160",
    "end": "3132470"
  },
  {
    "text": "and as a result they're also operating constantly an outdated versions of that",
    "start": "3132470",
    "end": "3138589"
  },
  {
    "text": "network the open AI approach that fixes this is that there's a coordinator that",
    "start": "3138589",
    "end": "3144589"
  },
  {
    "text": "there's these rounds where everybody all the agents in parallel are rolling out",
    "start": "3144589",
    "end": "3149599"
  },
  {
    "text": "the episode but then the coordinator waits for everybody to finish in order to make the update to the global network",
    "start": "3149599",
    "end": "3155269"
  },
  {
    "text": "and then distributes all the same parameter to all the agents and so that means that",
    "start": "3155269",
    "end": "3161470"
  },
  {
    "text": "every iteration starts with the same global parameters and that has really nice properties in terms of conversions",
    "start": "3161470",
    "end": "3168750"
  },
  {
    "text": "and stability of the training process okay from google deepmind the deep",
    "start": "3168750",
    "end": "3174730"
  },
  {
    "start": "3172000",
    "end": "3252000"
  },
  {
    "text": "deterministic policy gradient is combining the ideas of dqn but dealing",
    "start": "3174730",
    "end": "3181690"
  },
  {
    "text": "with continuous action spaces so taking a policy network but instead of the",
    "start": "3181690",
    "end": "3188530"
  },
  {
    "text": "actor actor critic framework but instead of picking a stochastic policy having",
    "start": "3188530",
    "end": "3196030"
  },
  {
    "text": "the actor operator on the since the casting nature is picking the best picking a deterministic policy so it's",
    "start": "3196030",
    "end": "3202270"
  },
  {
    "text": "always choosing the best action but ok with that the problem quite naturally is",
    "start": "3202270",
    "end": "3209350"
  },
  {
    "text": "that when the policy is now deterministic it's able to do continuous action space but because it's termina",
    "start": "3209350",
    "end": "3215320"
  },
  {
    "text": "stick it's never exploring so the way we inject exploration into the system is by adding noise either adding noise into",
    "start": "3215320",
    "end": "3222610"
  },
  {
    "text": "the action space on the output or adding noise into the parameters of the network that have then that create perturbations",
    "start": "3222610",
    "end": "3232120"
  },
  {
    "text": "and the actions such that the final result is that you try different kinds of things and the the scale of the noise",
    "start": "3232120",
    "end": "3238480"
  },
  {
    "text": "just like well the epsilon greedy in the exploration for DQ on the scale of the noise decreases as you learn more and",
    "start": "3238480",
    "end": "3244150"
  },
  {
    "text": "more so on the policy optimization side from open ai and others",
    "start": "3244150",
    "end": "3252269"
  },
  {
    "start": "3252000",
    "end": "3363000"
  },
  {
    "text": "we'll do a lecture just on this there's been a lot of exciting work here the",
    "start": "3252700",
    "end": "3257890"
  },
  {
    "text": "basic idea of optimization on policy optimization with PPO and TRP au is",
    "start": "3257890",
    "end": "3265109"
  },
  {
    "text": "first of all we want to formulate reinforcement learning as purely an",
    "start": "3265109",
    "end": "3272560"
  },
  {
    "text": "optimization problem and second of all if policy optimization the actions you",
    "start": "3272560",
    "end": "3281710"
  },
  {
    "text": "take influences the rest of your the optimization process you have to be very",
    "start": "3281710",
    "end": "3287470"
  },
  {
    "text": "careful about the actions you take in particular you have to avoid taking",
    "start": "3287470",
    "end": "3292720"
  },
  {
    "text": "really bad actions when you're convergence the the training performance in general collapses so how do we do",
    "start": "3292720",
    "end": "3301390"
  },
  {
    "text": "that there's the line search methods which is where gradient descent or gradient descent falls under which which is the",
    "start": "3301390",
    "end": "3309130"
  },
  {
    "text": "how we train deep neural networks is you first pick a direction of the gradient",
    "start": "3309130",
    "end": "3315820"
  },
  {
    "text": "and then pick the step size the problem with that is that can get you into",
    "start": "3315820",
    "end": "3322000"
  },
  {
    "text": "trouble here there's a nice visualization walking along a ridge is",
    "start": "3322000",
    "end": "3327640"
  },
  {
    "text": "it can it can result in you stepping off that Ridge again the collapsing of the",
    "start": "3327640",
    "end": "3332829"
  },
  {
    "text": "training process the performance the trust region is is the underlying idea",
    "start": "3332829",
    "end": "3338319"
  },
  {
    "text": "here for the for the policy optimization methods that first pick the step size so",
    "start": "3338319",
    "end": "3344470"
  },
  {
    "text": "that constrain in various kinds of ways the the magnitude of the difference to",
    "start": "3344470",
    "end": "3349630"
  },
  {
    "text": "the weights that's applied and then the direction so it placing a much higher",
    "start": "3349630",
    "end": "3355990"
  },
  {
    "text": "priority not choosing bad actions that can throw you off the optimization path should actually we should take to that",
    "start": "3355990",
    "end": "3362020"
  },
  {
    "text": "path and finally the on the model-based methods and we'll also talk about them",
    "start": "3362020",
    "end": "3367420"
  },
  {
    "start": "3363000",
    "end": "3650000"
  },
  {
    "text": "in the robotics side there's a lot of interesting approaches now where deep",
    "start": "3367420",
    "end": "3372460"
  },
  {
    "text": "learning is starting to be used for a model-based methods when the model has to be learned but of course when the",
    "start": "3372460",
    "end": "3379599"
  },
  {
    "text": "model doesn't have to be learned it's given inherent to the game you know the model like Ingo and chess and so on out",
    "start": "3379599",
    "end": "3386260"
  },
  {
    "text": "zero has really done incredible stuff so what's wise what is the model here so",
    "start": "3386260",
    "end": "3393610"
  },
  {
    "text": "the way that a lot of these games are approached you know game of Go it's turn-based one person goes and then",
    "start": "3393610",
    "end": "3400030"
  },
  {
    "text": "another person goes and there's this game tree at every point as a set of actions that could be taken and quickly",
    "start": "3400030",
    "end": "3405550"
  },
  {
    "text": "if you look at that game tree it's it becomes you know a girl's exponentially so it becomes huge a game of go is the",
    "start": "3405550",
    "end": "3412600"
  },
  {
    "text": "hugest of all in terms of because the number of choices you have is the largest and there's chess and then you",
    "start": "3412600",
    "end": "3420070"
  },
  {
    "text": "know it gets the checkers and then tic-tac-toe and it's just the the degree at every step increases decreased based",
    "start": "3420070",
    "end": "3426370"
  },
  {
    "text": "on the game structure and so the task for a neural network there is to learn the quality of the board it's that it's",
    "start": "3426370",
    "end": "3433210"
  },
  {
    "text": "to learn which boards which game positions are most likely to result in a",
    "start": "3433210",
    "end": "3443220"
  },
  {
    "text": "are most useful to explore and a result in a highly successful state so that",
    "start": "3443220",
    "end": "3449500"
  },
  {
    "text": "choice of what's good to explore what's what branch is good to go down is where",
    "start": "3449500",
    "end": "3454900"
  },
  {
    "text": "we can have neural network step in and without phago it was pre trained the",
    "start": "3454900",
    "end": "3459970"
  },
  {
    "text": "first success that beat the world champion was pre trained on expert games then with alphago zero",
    "start": "3459970",
    "end": "3467260"
  },
  {
    "text": "it was no pre training on expert systems",
    "start": "3467260",
    "end": "3472420"
  },
  {
    "text": "so no imitation learning is just purely through self play through suggesting",
    "start": "3472420",
    "end": "3477820"
  },
  {
    "text": "through playing itself new board positions many of these systems use Monte Carlo tree search and during the",
    "start": "3477820",
    "end": "3484150"
  },
  {
    "text": "search balancing exploitation exploration so going deep on promising positions based on the estimation then",
    "start": "3484150",
    "end": "3489850"
  },
  {
    "text": "you'll network or with a flip of a coin playing under play positions and so this",
    "start": "3489850",
    "end": "3498040"
  },
  {
    "text": "kind of here you can think of as an intuition of looking at a board and estimating how good that board is and",
    "start": "3498040",
    "end": "3505320"
  },
  {
    "text": "also estimating how good that board is likely to lead to victory down the end",
    "start": "3505320",
    "end": "3511270"
  },
  {
    "text": "so as to mean just general quality and probability of leading to victory then",
    "start": "3511270",
    "end": "3516430"
  },
  {
    "text": "the next step forward is alpha zero using the same similar architecture with",
    "start": "3516430",
    "end": "3522609"
  },
  {
    "text": "MCTS what do you call it research but applying it to different games and",
    "start": "3522609",
    "end": "3527970"
  },
  {
    "text": "applying it and competing against other engines state-of-the-art engines and go",
    "start": "3527970",
    "end": "3533680"
  },
  {
    "text": "and shogi in chess and outperforming them with very few very few steps so",
    "start": "3533680",
    "end": "3540730"
  },
  {
    "text": "here's this model-based approaches which are really extremely simple efficient if",
    "start": "3540730",
    "end": "3547119"
  },
  {
    "text": "you can construct us such a model and in in the robotics if you can learn such a model I can be exceptionally powerful",
    "start": "3547119",
    "end": "3554859"
  },
  {
    "text": "here beating the the engines which are",
    "start": "3554859",
    "end": "3560470"
  },
  {
    "text": "far superior to humans already stockfish can destroy most humans on earth at the",
    "start": "3560470",
    "end": "3565990"
  },
  {
    "text": "game of chess the ability through learning through through estimating the quality of a board to be able to defeat",
    "start": "3565990",
    "end": "3572740"
  },
  {
    "text": "these engines is incredible and the the exciting aspect here versus engines that",
    "start": "3572740",
    "end": "3578890"
  },
  {
    "text": "don't use neural networks is that the number its it really has to do with",
    "start": "3578890",
    "end": "3586079"
  },
  {
    "text": "based on the neural network you explore certain positions you explore certain",
    "start": "3586079",
    "end": "3592839"
  },
  {
    "text": "parts of the tree and if you look at grandmasters human players in chess they",
    "start": "3592839",
    "end": "3600849"
  },
  {
    "text": "seem to explore very few moves they have a really good neural network at estimating which are the likely branches",
    "start": "3600849",
    "end": "3608799"
  },
  {
    "text": "which would provide value to explore and on the other side stock fish and so on",
    "start": "3608799",
    "end": "3615369"
  },
  {
    "text": "are much more brute force in their estimation for the MCTS and then alpha",
    "start": "3615369",
    "end": "3620890"
  },
  {
    "text": "zero is a step towards the Grandmaster is the number of branches need to be explored as much much fewer a lot of the",
    "start": "3620890",
    "end": "3627970"
  },
  {
    "text": "work is done in the representation form by the neural network it's just super exciting and then it's able to uh",
    "start": "3627970",
    "end": "3634390"
  },
  {
    "text": "perform stockfish in chess it's able to outperform Elmo and shogi and it's",
    "start": "3634390",
    "end": "3641640"
  },
  {
    "text": "itself in go or the previous iterations of alphago zero and so on now the",
    "start": "3641640",
    "end": "3650440"
  },
  {
    "start": "3650000",
    "end": "3789000"
  },
  {
    "text": "challenge here the sobering truth is that majority of real world application",
    "start": "3650440",
    "end": "3657430"
  },
  {
    "text": "of agents that have to act in this world perceive the world and act in this world are for the most part not based have no",
    "start": "3657430",
    "end": "3665529"
  },
  {
    "text": "RL involved so the action is not learned use neural networks to perceive certain",
    "start": "3665529",
    "end": "3672279"
  },
  {
    "text": "aspects of the world but ultimately the action is not is not learned from data",
    "start": "3672279",
    "end": "3678720"
  },
  {
    "text": "that's true for all most of the autonomous vehicle companies are all of the autonomous vehicle companies",
    "start": "3678720",
    "end": "3684970"
  },
  {
    "text": "operating today and it's true for robotic manipulation in the industrial",
    "start": "3684970",
    "end": "3690760"
  },
  {
    "text": "robotics and any of the humanoid robots have to navigate in this world under uncertain conditions all the work from",
    "start": "3690760",
    "end": "3696819"
  },
  {
    "text": "Boston Dynamics doesn't involve any machine learning as far as we know now",
    "start": "3696819",
    "end": "3702730"
  },
  {
    "text": "that's beginning to change here with animal the the recent development where",
    "start": "3702730",
    "end": "3709660"
  },
  {
    "text": "the certain aspects of the control a robotic could be learned",
    "start": "3709660",
    "end": "3714970"
  },
  {
    "text": "you're trying to learn more efficient movement you're trying to learn more robust movement on top of the other",
    "start": "3714970",
    "end": "3721359"
  },
  {
    "text": "controllers so it's quite exciting through RL to be able to learn some of",
    "start": "3721359",
    "end": "3726670"
  },
  {
    "text": "the control dynamics here that's able to teach this particular robot to be able",
    "start": "3726670",
    "end": "3732160"
  },
  {
    "text": "to get up from arbitrary positions so it's less hard coding in order to be able to deal with unexpected nishal",
    "start": "3732160",
    "end": "3740770"
  },
  {
    "text": "conditions and unexpected perturbations so that's exciting there in terms of",
    "start": "3740770",
    "end": "3745809"
  },
  {
    "text": "learning the control dynamics and some of the driving policy so maybe behavioral driving behavior",
    "start": "3745809",
    "end": "3752260"
  },
  {
    "text": "decisions changing lanes turning and so on that if you if you were here last week heard",
    "start": "3752260",
    "end": "3758289"
  },
  {
    "text": "from way moe they they're starting to use some RL in terms of the driving policy in order to especially predict the future they're",
    "start": "3758289",
    "end": "3764799"
  },
  {
    "text": "trying to anticipate intent modeling predict what the pedestrians the cars are going to be based on environment that are trying to",
    "start": "3764799",
    "end": "3770680"
  },
  {
    "text": "unroll what's happened recently into the future and beginning to move beyond sort",
    "start": "3770680",
    "end": "3777279"
  },
  {
    "text": "of pure end to end on NVIDIA and to end learning approach of the control decisions are actually moving to",
    "start": "3777279",
    "end": "3784200"
  },
  {
    "text": "RL and making long-term planning decisions but again the challenge is the",
    "start": "3784200",
    "end": "3792020"
  },
  {
    "start": "3789000",
    "end": "3884000"
  },
  {
    "text": "the gap the leap needed to go from simulation to real-world all most the",
    "start": "3792020",
    "end": "3798660"
  },
  {
    "text": "work is done from the design of the environment and the design and the reward structure and because most of",
    "start": "3798660",
    "end": "3803820"
  },
  {
    "text": "that work now is in simulation we need to either develop better algorithms for transfer learning or close the distance",
    "start": "3803820",
    "end": "3811020"
  },
  {
    "text": "between simulation in the real world and also we could think outside the box a",
    "start": "3811020",
    "end": "3817290"
  },
  {
    "text": "little bit at the conversation with Peter bill recently one of the leading researchers in deep RL it kind of on the",
    "start": "3817290",
    "end": "3825000"
  },
  {
    "text": "side quickly mentioned the the idea is that we don't need to make simulation",
    "start": "3825000",
    "end": "3831720"
  },
  {
    "text": "more realistic what we could do is just create an infinite number of simulations",
    "start": "3831720",
    "end": "3838320"
  },
  {
    "text": "or very large number of simulations and the naturally the regularization aspect",
    "start": "3838320",
    "end": "3846690"
  },
  {
    "text": "of having all those simulations will make it so that our our reality is just another sample from those simulations",
    "start": "3846690",
    "end": "3853010"
  },
  {
    "text": "and so maybe the solution isn't to create higher fidelity simulation or to create transfer learning algorithms",
    "start": "3853010",
    "end": "3859190"
  },
  {
    "text": "maybe it's to build a arbitrary number",
    "start": "3859190",
    "end": "3865619"
  },
  {
    "text": "of simulations so then that step towards creating a agent that work that works in",
    "start": "3865619",
    "end": "3871680"
  },
  {
    "text": "the real world is a trivial one and maybe that's exactly whoever created the",
    "start": "3871680",
    "end": "3876690"
  },
  {
    "text": "simulation we're living in and the multiverse that we're living in did next",
    "start": "3876690",
    "end": "3884220"
  },
  {
    "start": "3884000",
    "end": "4050000"
  },
  {
    "text": "steps the lecture videos will have several in RL will be made all available",
    "start": "3884220",
    "end": "3889589"
  },
  {
    "text": "on deep learning that MIT ID you will have several tutorials in RL on github",
    "start": "3889589",
    "end": "3894890"
  },
  {
    "text": "the link is there and I really like the essay from open AI on spinning up as a",
    "start": "3894890",
    "end": "3901349"
  },
  {
    "text": "deep our researcher you know if you're interested in getting into research in RL what are the steps need to take from",
    "start": "3901349",
    "end": "3907440"
  },
  {
    "text": "the background of developing the mathematical background prop stat and multivariate calculus to some of the",
    "start": "3907440",
    "end": "3914640"
  },
  {
    "text": "basics like it's covered last week on deep learning some the basics ideas in RL just terminology",
    "start": "3914640",
    "end": "3920550"
  },
  {
    "text": "and so on some basic concepts then picking a framework tends to flow our PI torch and learn by doing i implemented",
    "start": "3920550",
    "end": "3929810"
  },
  {
    "text": "guram as i mentioned today those are the core RL algorithms so implement all isms",
    "start": "3929810",
    "end": "3935190"
  },
  {
    "text": "from scratch it should only take about two hundred three hundred lines of code there actually when you put it down on",
    "start": "3935190",
    "end": "3941700"
  },
  {
    "text": "paper quite simple intuitive algorithms and then read papers about those",
    "start": "3941700",
    "end": "3947940"
  },
  {
    "text": "algorithms that follow after looking not for the big waving performance the hand",
    "start": "3947940",
    "end": "3954480"
  },
  {
    "text": "waving performance but for the tricks that were used to change these algorithms the tricks tell a lot of the",
    "start": "3954480",
    "end": "3959610"
  },
  {
    "text": "story and that's the useful parts that they need to learn and iterate fast on",
    "start": "3959610",
    "end": "3965730"
  },
  {
    "text": "simple benchmark environments so open the I Jim has provided a lot of easy to use environments that you can play with",
    "start": "3965730",
    "end": "3972090"
  },
  {
    "text": "that you can train an agent in minutes hours as opposed to days and weeks and",
    "start": "3972090",
    "end": "3977370"
  },
  {
    "text": "so iterating fast is the best way to learn these algorithms and then on the research side there's three ways to get",
    "start": "3977370",
    "end": "3984120"
  },
  {
    "text": "a best paper award right two to publish and to contribute and have an impact in",
    "start": "3984120",
    "end": "3990330"
  },
  {
    "text": "the research community in in RL one is improving existing approach given us a",
    "start": "3990330",
    "end": "3995730"
  },
  {
    "text": "particular benchmarks there's a few benchmark datasets environments that are emerging so you want to improve on the",
    "start": "3995730",
    "end": "4002150"
  },
  {
    "text": "existing approach some aspect of the convergence in the performance you can focus on an unsolved task there's",
    "start": "4002150",
    "end": "4008960"
  },
  {
    "text": "certain games that just haven't been solved through their RL formulation or",
    "start": "4008960",
    "end": "4014030"
  },
  {
    "text": "you can come up with a totally new problem that hasn't been addressed by RL before so with that I'd like to thank",
    "start": "4014030",
    "end": "4022010"
  },
  {
    "text": "you very much tomorrow I'll hope to see you here for deep traffic Thanks",
    "start": "4022010",
    "end": "4027880"
  },
  {
    "text": "you",
    "start": "4028349",
    "end": "4030410"
  },
  {
    "text": "you",
    "start": "4037430",
    "end": "4039490"
  }
]