[
  {
    "text": "so I want to tell you guys about speech recognition and deep learning I think deep learning has been playing",
    "start": "900",
    "end": "7140"
  },
  {
    "text": "an increasingly large role in speech recognition and one of the things I think is most exciting about this field",
    "start": "7140",
    "end": "13230"
  },
  {
    "text": "is that speech recognitions at a place right now where it's becoming good enough to enable really exciting",
    "start": "13230",
    "end": "19830"
  },
  {
    "text": "applications that end up the hands of users so for example if we want to",
    "start": "19830",
    "end": "25109"
  },
  {
    "text": "caption video content and make it accessible to to everyone it used to be that we would sort of try to do this but",
    "start": "25109",
    "end": "31650"
  },
  {
    "text": "you still need a human to get really good captioning for something like a lecture but it's possible that we can do",
    "start": "31650",
    "end": "37920"
  },
  {
    "text": "a lot of this with higher quality in the future with deep learning we can do things like hands-free interfaces in",
    "start": "37920",
    "end": "42960"
  },
  {
    "text": "cars make it safer to use technology while we're on the go keep people's eyes on the road of course and make mobile",
    "start": "42960",
    "end": "49379"
  },
  {
    "text": "devices home devices much easier much more efficient and enjoyable to use but",
    "start": "49379",
    "end": "56100"
  },
  {
    "text": "another actually sort of fun recent study that that some folks if I do",
    "start": "56100",
    "end": "61350"
  },
  {
    "text": "participated in along with Stanford and UW is to show that for even something straight forward that we sort of take",
    "start": "61350",
    "end": "67020"
  },
  {
    "text": "for granted as an application of speech which is just texting someone with voice",
    "start": "67020",
    "end": "72270"
  },
  {
    "text": "or writing a piece of text the study show you can actually go three times faster with voice recognition systems",
    "start": "72270",
    "end": "79619"
  },
  {
    "text": "that are available today so it's not just like a little bit faster now even with the errors that a speech",
    "start": "79619",
    "end": "84750"
  },
  {
    "text": "recognition system can make it's actually a lot faster and the reason I wanted to highlight this result which is",
    "start": "84750",
    "end": "92790"
  },
  {
    "text": "pretty recent is that the speech engine that was used for this study is actually powered by a lot of the deep learning",
    "start": "92790",
    "end": "99030"
  },
  {
    "text": "methods and I'm going to tell you about so hopefully when you walk away today you have an appreciation or an understanding of the sort of high-level",
    "start": "99030",
    "end": "105780"
  },
  {
    "text": "ideas that make a result like this possible so there are a whole bunch of",
    "start": "105780",
    "end": "112560"
  },
  {
    "text": "different components that make up a complete speech application so for",
    "start": "112560",
    "end": "118619"
  },
  {
    "text": "example there's speech transcription so if I just talk I want to come up with words that represent you know whatever I",
    "start": "118619",
    "end": "126210"
  },
  {
    "text": "just said there's also other tasks though like word spotting or triggering so for example if my phone is sitting",
    "start": "126210",
    "end": "132480"
  },
  {
    "text": "over there and I want to say hey phone go do something for me actually has to be listening continuously for me to say that word and",
    "start": "132480",
    "end": "140250"
  },
  {
    "text": "likewise there are things like speaker identification or verification so that if I want to authenticate myself or I",
    "start": "140250",
    "end": "146879"
  },
  {
    "text": "want to be able to tell apart different users in a room I've got to be able to recognize your voice even though I don't",
    "start": "146879",
    "end": "151980"
  },
  {
    "text": "know what you're saying so these are different tasks I'm not going to cover all of them today instead",
    "start": "151980",
    "end": "157319"
  },
  {
    "text": "I'm going to just focus on the bread and butter of speech recognition we're going to focus on building a speech engine",
    "start": "157319",
    "end": "162900"
  },
  {
    "text": "that can accurately transcribe audio into words so that's our main goal this",
    "start": "162900",
    "end": "168480"
  },
  {
    "text": "is a very basic goal of artificial intelligence right historically people",
    "start": "168480",
    "end": "175590"
  },
  {
    "text": "are very very good at listening to someone talk just like you guys are listening to me right now and you can",
    "start": "175590",
    "end": "181829"
  },
  {
    "text": "very quickly turn words turn audio into words and into meaning on your own",
    "start": "181829",
    "end": "187980"
  },
  {
    "text": "almost effortlessly and for machines this has historically been incredibly",
    "start": "187980",
    "end": "193110"
  },
  {
    "text": "hard so you think of this is like one of those sort of consummate AI tasks so the",
    "start": "193110",
    "end": "198359"
  },
  {
    "text": "goal of building a speech pipeline is if you just give me a raw audio wave like you recorded on your laptop or your cell",
    "start": "198359",
    "end": "204840"
  },
  {
    "text": "phone I want to somehow build a speech recognizer that can do this very simple task of printing out hello world when I",
    "start": "204840",
    "end": "212010"
  },
  {
    "text": "actually say hello world so before I dig into the deep learning part I want to",
    "start": "212010",
    "end": "218220"
  },
  {
    "text": "step back a little bit and spend maybe ten minutes talking about how a",
    "start": "218220",
    "end": "224280"
  },
  {
    "text": "traditional speech recognition pipeline is working for two reasons if you're out",
    "start": "224280",
    "end": "229919"
  },
  {
    "text": "in the wild you're doing an internship you're trying to build a speech recognition system with a lot of the",
    "start": "229919",
    "end": "236250"
  },
  {
    "text": "tools that are out there you're going to bump into a lot of systems that are built on technologies that look like",
    "start": "236250",
    "end": "242579"
  },
  {
    "text": "this so I want you to understand a little bit of the vocabulary and how those things are put together and also",
    "start": "242579",
    "end": "248459"
  },
  {
    "text": "this will sort of give you a story for what deep learning is doing in speech",
    "start": "248459",
    "end": "253500"
  },
  {
    "text": "recognition today that is kind of special and that I think paves the way",
    "start": "253500",
    "end": "258570"
  },
  {
    "text": "for for much bigger results in the future so traditional systems break the",
    "start": "258570",
    "end": "266310"
  },
  {
    "text": "problem of converting an audio wave of taking audio and break and turning it into a",
    "start": "266310",
    "end": "272710"
  },
  {
    "text": "transcription into a bunch of different pieces so I'm going to start out with my",
    "start": "272710",
    "end": "278560"
  },
  {
    "text": "raw audio and I'm just going to represent that by X and then usually we",
    "start": "278560",
    "end": "284680"
  },
  {
    "text": "have to decide on some kind of feature representation we have to convert this into some other form that's easier to",
    "start": "284680",
    "end": "290590"
  },
  {
    "text": "deal with than a raw audio wave and in a traditional speech system I often have",
    "start": "290590",
    "end": "296500"
  },
  {
    "text": "something called an acoustic model and the job of the acoustic model is to learn the relationship between these",
    "start": "296500",
    "end": "303550"
  },
  {
    "text": "features that represent my audio and the words that someone is trying to say and",
    "start": "303550",
    "end": "309210"
  },
  {
    "text": "then I'll often have a language model which encapsulate Sall of my knowledge",
    "start": "309210",
    "end": "314289"
  },
  {
    "text": "about what kinds of words what spellings and what combinations of words are most likely in the language that I'm trying",
    "start": "314289",
    "end": "320919"
  },
  {
    "text": "to transcribe and once you have all of these pieces so these might be these",
    "start": "320919",
    "end": "326470"
  },
  {
    "text": "different models might be driven by machine learning themselves what you would need to build in a traditional system is something called a decoder and",
    "start": "326470",
    "end": "333789"
  },
  {
    "text": "the job of a decoder which itself might involve some modeling efforts and",
    "start": "333789",
    "end": "338949"
  },
  {
    "text": "machine learning algorithms is to find the sequence of words W that maximizes",
    "start": "338949",
    "end": "346360"
  },
  {
    "text": "this probability the probability of the particular sequence W given your audio that's straightforward but that's",
    "start": "346360",
    "end": "353710"
  },
  {
    "text": "equivalent to maximizing the product of the contributions from your acoustic model and from your language model so a",
    "start": "353710",
    "end": "360940"
  },
  {
    "text": "traditional speech system is broken down into these pieces and a lot of the effort and getting that system to work",
    "start": "360940",
    "end": "366729"
  },
  {
    "text": "is is in developing this sort of portion that combines them all so it turns out",
    "start": "366729",
    "end": "373810"
  },
  {
    "text": "that if you want to just directly transcribe audio you can't just go",
    "start": "373810",
    "end": "378819"
  },
  {
    "text": "straight to characters and the reason is and it's especially apparent in English that the way something is spelled in",
    "start": "378819",
    "end": "386139"
  },
  {
    "text": "characters doesn't always correspond well to the way that it sounds so if if",
    "start": "386139",
    "end": "391449"
  },
  {
    "text": "I give you the word night for example without context you don't really know whether I'm talking about like a knight",
    "start": "391449",
    "end": "397330"
  },
  {
    "text": "in armor or whether I'm talking like knight like in like an evening and so a way to get around this to",
    "start": "397330",
    "end": "403090"
  },
  {
    "text": "abstract this problem away from a traditional system is to replace this with a sort of intermediate",
    "start": "403090",
    "end": "409360"
  },
  {
    "text": "representation instead of trying to predict characters I'll just try to predict something called phonemes so as",
    "start": "409360",
    "end": "416080"
  },
  {
    "text": "an example if I want to represent the word hello what I might try to do is",
    "start": "416080",
    "end": "421330"
  },
  {
    "text": "break it down into these units of sound so the first one is like the that H",
    "start": "421330",
    "end": "426819"
  },
  {
    "text": "sound in hello and then an a sound which is actually only one possible",
    "start": "426819",
    "end": "431949"
  },
  {
    "text": "pronunciation of an e and then an L and an O sound and that would be my string",
    "start": "431949",
    "end": "437620"
  },
  {
    "text": "that I try to come up with using all of my different speech components so this",
    "start": "437620",
    "end": "445089"
  },
  {
    "text": "in one sense makes the modeling problem easier my acoustic model and so on can be simpler because I don't have to worry",
    "start": "445089",
    "end": "451870"
  },
  {
    "text": "about spelling but it does have this problem that I have to think about where these things come from",
    "start": "451870",
    "end": "457779"
  },
  {
    "text": "so these phonemes are intuitively they're the perceptual e distinct units",
    "start": "457779",
    "end": "464229"
  },
  {
    "text": "of sound that we can use to distinguish words and they're very approximate this",
    "start": "464229",
    "end": "471699"
  },
  {
    "text": "might be our imagination that these things actually exist it's not clear how fundamental this is but they're sort of",
    "start": "471699",
    "end": "478449"
  },
  {
    "text": "standardized there are a bunch of different conventions for how to define these and if you're and if you end up",
    "start": "478449",
    "end": "485860"
  },
  {
    "text": "working on a system that uses phonemes one popular data set is called timet and",
    "start": "485860",
    "end": "492189"
  },
  {
    "text": "so this actually has a corpus of audio frames with examples of each of these phonemes so once you have this phoneme",
    "start": "492189",
    "end": "503949"
  },
  {
    "text": "representation unfortunately it adds even more complexity to this traditional",
    "start": "503949",
    "end": "509469"
  },
  {
    "text": "pipeline because now my acoustic model doesn't associate this audio feature",
    "start": "509469",
    "end": "514630"
  },
  {
    "text": "with words it actually associates them with another kind of transcription with the transcription into phonemes and so I",
    "start": "514630",
    "end": "521198"
  },
  {
    "text": "have to introduce yet another component into my pipeline that tries to understand how do I convert the",
    "start": "521199",
    "end": "528370"
  },
  {
    "text": "transcriptions in phonemes into actual Spelling's and so I need some kind of dick or a lexicon to tell me all of that so",
    "start": "528370",
    "end": "536900"
  },
  {
    "text": "this is a way of taking our knowledge about a language and baking it into this engineered pipeline and then once you've",
    "start": "536900",
    "end": "544700"
  },
  {
    "text": "got all that again all of your work now goes into this decoder that has a slightly more complicated task in order",
    "start": "544700",
    "end": "551840"
  },
  {
    "text": "to infer the most likely word transcription given the audio so this is",
    "start": "551840",
    "end": "558230"
  },
  {
    "text": "a tried and true pipeline it's been around for a long time you'll see a whole bunch of these systems out there",
    "start": "558230",
    "end": "564830"
  },
  {
    "text": "and we're still using a lot of the vocabulary from these systems but",
    "start": "564830",
    "end": "570980"
  },
  {
    "text": "traditionally the big advantage is that it's very tweakable if you want to go add a new pronunciation for a word",
    "start": "570980",
    "end": "577490"
  },
  {
    "text": "you've never heard before you can just drop it right in that's great but it's also really hard to get working",
    "start": "577490",
    "end": "584060"
  },
  {
    "text": "well if you start from scratch with this system and you have no experience in speech recognition it's actually quite",
    "start": "584060",
    "end": "591050"
  },
  {
    "text": "confusing and hard to debug it's very difficult to know which of these various models is the one that's behind your",
    "start": "591050",
    "end": "598100"
  },
  {
    "text": "error and especially once we start dealing with things like accents heavy noise different kinds of ambiguity that",
    "start": "598100",
    "end": "605060"
  },
  {
    "text": "makes the problem even harder to engineer around because trying to think ourselves about how do i tweaked my",
    "start": "605060",
    "end": "611560"
  },
  {
    "text": "pronunciation model for example to account for someone's accent that I haven't heard that's a very hard",
    "start": "611560",
    "end": "617150"
  },
  {
    "text": "engineering judgment for us to make so there are all kinds of design decisions",
    "start": "617150",
    "end": "622460"
  },
  {
    "text": "that go into this pipeline like choosing the future representation for example so",
    "start": "622460",
    "end": "628160"
  },
  {
    "text": "the first place that deep learning has started to make an impact in speech",
    "start": "628160",
    "end": "634310"
  },
  {
    "text": "recognition starting a few years ago is to just take one of the core machine",
    "start": "634310",
    "end": "640070"
  },
  {
    "text": "learning components of the system and replace it with a deep learning algorithm so I mentioned back in this",
    "start": "640070",
    "end": "646340"
  },
  {
    "text": "previous pipeline that we had this little model here whose job is to learn the relationship between a sequence of",
    "start": "646340",
    "end": "653360"
  },
  {
    "text": "phonemes and the audio that we're hearing so this is called the acoustic model and there are lots of different",
    "start": "653360",
    "end": "661610"
  },
  {
    "text": "methods for training this thing so take your favorite machine learning algorithm you can probably find someone who is",
    "start": "661610",
    "end": "667680"
  },
  {
    "text": "trained in acoustic model with that algorithm whether it's a Gaussian mixture model or a bunch of decision",
    "start": "667680",
    "end": "673590"
  },
  {
    "text": "trees and random forests anything for estimating these kinds of densities there's a lot of work and trying to make",
    "start": "673590",
    "end": "679500"
  },
  {
    "text": "better acoustic models so some work by George Dahl and co-authors took what was",
    "start": "679500",
    "end": "687330"
  },
  {
    "text": "a state of the art deep learning system back in 2011 which is a deep belief",
    "start": "687330",
    "end": "692340"
  },
  {
    "text": "network with some pre training strategies and dropped it into a state of the art pipeline in place of this",
    "start": "692340",
    "end": "699480"
  },
  {
    "text": "acoustic model and the results are actually pretty striking because even though we had neural networks and these",
    "start": "699480",
    "end": "706590"
  },
  {
    "text": "pipelines for a while what ended up happening is that when you replace the",
    "start": "706590",
    "end": "711650"
  },
  {
    "text": "Gaussian mixture model in hmm system that already existed with this deep",
    "start": "711650",
    "end": "717600"
  },
  {
    "text": "belief network as an acoustic model you actually got something between like a ten and twenty percent relative",
    "start": "717600",
    "end": "722610"
  },
  {
    "text": "improvement in accuracy which is a huge jump this is highly noticeable to a",
    "start": "722610",
    "end": "727830"
  },
  {
    "text": "person and if you compare this to the amount of progress that had been made in preceding years this is a giant leap for",
    "start": "727830",
    "end": "736590"
  },
  {
    "text": "a single paper to make compared to a progress we've been able to make previously so this is in some sense the",
    "start": "736590",
    "end": "744330"
  },
  {
    "text": "first generation of deep learning for speech recognition which is I take one of these components and I swap it out",
    "start": "744330",
    "end": "752100"
  },
  {
    "text": "for for my favorite deep learning algorithm so the picture looks sort of",
    "start": "752100",
    "end": "758700"
  },
  {
    "text": "like this so with these traditional speech recognition pipelines the problem that",
    "start": "758700",
    "end": "765750"
  },
  {
    "text": "we would always run into is that if you gave me a lot more data he gave me a much bigger computer so that I could",
    "start": "765750",
    "end": "771930"
  },
  {
    "text": "train a huge model that actually didn't help me because all the problems I had were in the construction of this",
    "start": "771930",
    "end": "778860"
  },
  {
    "text": "pipeline and so eventually if you gave me more data in a bigger computer the",
    "start": "778860",
    "end": "784170"
  },
  {
    "text": "performance of our speech recognition system would just kind of peter out it would just reach a ceiling that was very",
    "start": "784170",
    "end": "790200"
  },
  {
    "text": "hard to get over and so we just start coming up with lots of different strategies we start specializing for",
    "start": "790200",
    "end": "795450"
  },
  {
    "text": "each application we try to specialize for each user and try to make things a little bit",
    "start": "795450",
    "end": "800670"
  },
  {
    "text": "better around the edges and what these deep learning acoustic models did was in some sense moved that barrier a little",
    "start": "800670",
    "end": "808860"
  },
  {
    "text": "ways it made it possible for us to take a bit more data much faster computers",
    "start": "808860",
    "end": "814290"
  },
  {
    "text": "that let us try a whole lot of models and move that ceiling up quite a ways so",
    "start": "814290",
    "end": "820320"
  },
  {
    "text": "the question that many in the research community including folks if I do have been trying to answer is can we go to a",
    "start": "820320",
    "end": "827850"
  },
  {
    "text": "next-generation version of this insight can we for instance build a speech",
    "start": "827850",
    "end": "833340"
  },
  {
    "text": "engine that is powered by deep learning all the way from the audio input to the",
    "start": "833340",
    "end": "839430"
  },
  {
    "text": "transcription itself can we replace as much of that traditional system with deep learning as possible so that over",
    "start": "839430",
    "end": "845700"
  },
  {
    "text": "time is you give researchers more data and bigger computers and the ability to",
    "start": "845700",
    "end": "851370"
  },
  {
    "text": "try more models their speech recognition performance just keeps going up and we can potentially solve speech for",
    "start": "851370",
    "end": "857190"
  },
  {
    "text": "everybody so the goal of this tutorial is not to to get you up here which",
    "start": "857190",
    "end": "864570"
  },
  {
    "text": "requires a whole bunch of things that I'll tell you about near the end but what we want to try to do is give you",
    "start": "864570",
    "end": "870810"
  },
  {
    "text": "enough to get a point on this curve and then once you're on the curve the the",
    "start": "870810",
    "end": "876750"
  },
  {
    "text": "idea is that what remains is now a problem of scale it's about data and about getting bigger computers and",
    "start": "876750",
    "end": "884120"
  },
  {
    "text": "coming up with ways to build bigger models so that's my objective so that when you walk away from here you have a",
    "start": "884120",
    "end": "890970"
  },
  {
    "text": "picture of what you would need to build to get this point and then after that",
    "start": "890970",
    "end": "895980"
  },
  {
    "text": "it's hopefully all about scale so thanks to Vinay Rao who's been helping put this",
    "start": "895980",
    "end": "902550"
  },
  {
    "text": "tutorial together there is going to be some starter code live for the basic",
    "start": "902550",
    "end": "908580"
  },
  {
    "text": "pipeline the deep learning part of the pipeline that we're talking about so there are some open source implementations of things like CTC but",
    "start": "908580",
    "end": "917640"
  },
  {
    "text": "we wanted to make sure that there's a system out there that's pretty representative of the acoustic models that I'm going to be talking about in",
    "start": "917640",
    "end": "924000"
  },
  {
    "text": "the first half of the presentation here so this will be enough that you can get",
    "start": "924000",
    "end": "929190"
  },
  {
    "text": "a simple pipeline going with something called max Dakota which I'll tell you about later and the",
    "start": "929190",
    "end": "934920"
  },
  {
    "text": "idea is that this is sort of a scale model of the acoustic models that I do and other places are powering real",
    "start": "934920",
    "end": "941820"
  },
  {
    "text": "production speech engines so this will get you that point on the curve okay",
    "start": "941820",
    "end": "949260"
  },
  {
    "text": "so here's what we're going to talk about the first part I'm just going to introduce a few preliminaries talk about",
    "start": "949260",
    "end": "956040"
  },
  {
    "text": "pre-processing so we still have a little bit of pre-processing around but it's not really fundamental I think it's",
    "start": "956040",
    "end": "961890"
  },
  {
    "text": "probably going to go away in the long run we'll talk about what is probably the most mature piece of sequence",
    "start": "961890",
    "end": "969950"
  },
  {
    "text": "learning technologies for deep learning right now so it turns out that one of the fundamental problems of doing speech",
    "start": "969950",
    "end": "976980"
  },
  {
    "text": "recognition is how do I build a neural network that can map this audio signal to a transcription that can have a quite",
    "start": "976980",
    "end": "983910"
  },
  {
    "text": "variable length and so CTC is one highly mature method for doing this and I think",
    "start": "983910",
    "end": "990149"
  },
  {
    "text": "you're actually going to hear about maybe some some other solutions later today then I'll say a little bit about",
    "start": "990149",
    "end": "995730"
  },
  {
    "text": "training and just what that looks like oops and then finally say a bit about",
    "start": "995730",
    "end": "1001430"
  },
  {
    "text": "decoding and language models which is sort of an addendum to the current acoustic models that we can build that",
    "start": "1001430",
    "end": "1008510"
  },
  {
    "text": "make them perform a lot better and then once you have this that's a picture of what you need to to get this point on",
    "start": "1008510",
    "end": "1015500"
  },
  {
    "text": "the curve and then I'll talk a little bit about what's remaining how do you scale up from this little scale model up",
    "start": "1015500",
    "end": "1021920"
  },
  {
    "text": "to the full thing what does what does that actually entail and then time permitting we'll talk a little bit about",
    "start": "1021920",
    "end": "1027890"
  },
  {
    "text": "production how could you put something like this into a cloud server and actually serve real users with it great",
    "start": "1027890",
    "end": "1036770"
  },
  {
    "text": "so how is audio represented this should be pretty straightforward I think unlike",
    "start": "1036770",
    "end": "1044300"
  },
  {
    "text": "a two dimensional image where we normally have a 2d grid of pixels audio is just a 1d signal and there are a",
    "start": "1044300",
    "end": "1051169"
  },
  {
    "text": "bunch of different formats for audio but typically this one-dimensional wave that that is actually me saying something",
    "start": "1051169",
    "end": "1057800"
  },
  {
    "text": "like hello world is something like 8,000 samples per second or 16,000 samples per",
    "start": "1057800",
    "end": "1065270"
  },
  {
    "text": "second and each wave is quantized into eight or 16 bits so when we represent this audio",
    "start": "1065270",
    "end": "1072330"
  },
  {
    "text": "signal that's going to go into our pipeline you could just think of that as a one dimensional vector so when I have",
    "start": "1072330",
    "end": "1077760"
  },
  {
    "text": "that box called X that represented my audio signal you can figure this was being broke down broken down into",
    "start": "1077760",
    "end": "1084240"
  },
  {
    "text": "samples X 1 X 2 and so forth and if I had a one-second audio clip this vector",
    "start": "1084240",
    "end": "1090270"
  },
  {
    "text": "would have a length of either say 8,000 or 16,000 samples and each element would",
    "start": "1090270",
    "end": "1096390"
  },
  {
    "text": "be say a floating-point number that I had extracted from this eight or 16-bit sample this is really simple now once I",
    "start": "1096390",
    "end": "1104549"
  },
  {
    "text": "have an audio clip we'll do a little bit of pre-processing so there are a couple of ways to start the first is to just do",
    "start": "1104549",
    "end": "1112140"
  },
  {
    "text": "some vanilla pre-processing like convert to a simple spectrogram so if you look",
    "start": "1112140",
    "end": "1118230"
  },
  {
    "text": "at a traditional speech pipeline you're going to see things like M FCC's which are mell frequency capital coefficients",
    "start": "1118230",
    "end": "1124860"
  },
  {
    "text": "you'll see a whole bunch of plays on spectrograms where you take differences",
    "start": "1124860",
    "end": "1130260"
  },
  {
    "text": "in different kinds of features and try to engineer complex representations but",
    "start": "1130260",
    "end": "1135540"
  },
  {
    "text": "for the stuff that we're going to do today a simple spectrogram is just fine and it turns out as you'll see in a",
    "start": "1135540",
    "end": "1141030"
  },
  {
    "text": "second we lose a little bit of information when we do this but it turns out not to not to be a huge difference",
    "start": "1141030",
    "end": "1147799"
  },
  {
    "text": "now I said a moment ago that I think probably this is going to go away in the long run and that's because today you",
    "start": "1147799",
    "end": "1155790"
  },
  {
    "text": "can actually find recent research and trying to do away with even this pre-processing part and having your",
    "start": "1155790",
    "end": "1161820"
  },
  {
    "text": "neural network process the audio wave directly and just train its own feature transformation so there's some",
    "start": "1161820",
    "end": "1168179"
  },
  {
    "text": "references at the end that you can look at for this so it's a quick straw poll",
    "start": "1168179",
    "end": "1175080"
  },
  {
    "text": "how many people have seen a spectrogram or computed a spectrogram before pretty",
    "start": "1175080",
    "end": "1180750"
  },
  {
    "text": "good maybe 50% ok so the idea behind a spectrogram is that it's sort of like a",
    "start": "1180750",
    "end": "1186929"
  },
  {
    "text": "frequency domain representation but instead of representing this entire signal in terms of frequencies I'm just",
    "start": "1186929",
    "end": "1194880"
  },
  {
    "text": "going to represent a small small window in terms of frequencies so to to process",
    "start": "1194880",
    "end": "1201390"
  },
  {
    "text": "this audio clip the first thing I'm going to do is cut out a little window that's typically about 20 milliseconds",
    "start": "1201390",
    "end": "1208470"
  },
  {
    "text": "long and when you get down to that scale it's usually very clear that these audio signals are made up of sort of a",
    "start": "1208470",
    "end": "1214860"
  },
  {
    "text": "combination of different frequencies of sine waves and then what we do is we",
    "start": "1214860",
    "end": "1220230"
  },
  {
    "text": "compute an FFT it basically converts this little signal into the frequency domain and then we just take the log of",
    "start": "1220230",
    "end": "1228030"
  },
  {
    "text": "the power at each frequency and so if you look at your what the result of this",
    "start": "1228030",
    "end": "1234720"
  },
  {
    "text": "is it basically tells us for every frequency of sine wave what is the",
    "start": "1234720",
    "end": "1241830"
  },
  {
    "text": "magnitude what's the amount of power represented by that sine wave that makes up this original signal so over here in",
    "start": "1241830",
    "end": "1249780"
  },
  {
    "text": "this example we have a very strong low frequency component in the signal and",
    "start": "1249780",
    "end": "1255480"
  },
  {
    "text": "then we have differing magnitudes at different differing frequencies so we",
    "start": "1255480",
    "end": "1263760"
  },
  {
    "text": "can just think of this as a vector so now instead of representing this little 20 millisecond slice as sort of a",
    "start": "1263760",
    "end": "1270060"
  },
  {
    "text": "sequence of audio samples instead I'm going to represent it as a vector here where each element represents sort of",
    "start": "1270060",
    "end": "1278460"
  },
  {
    "text": "the strengths of each frequency in this little window and the next step beyond",
    "start": "1278460",
    "end": "1284940"
  },
  {
    "text": "this is that if I just told you how to process one little window you can of course apply this to a whole bunch of",
    "start": "1284940",
    "end": "1290400"
  },
  {
    "text": "windows across the entire piece of audio and and that gives you what we call a",
    "start": "1290400",
    "end": "1296100"
  },
  {
    "text": "spectrogram and you can use either disjoint windows that are just sort of adjacent or you can apply them to",
    "start": "1296100",
    "end": "1302130"
  },
  {
    "text": "overlapping windows if you like so there's a little bit of parameter tuning there but this is an alternative",
    "start": "1302130",
    "end": "1308460"
  },
  {
    "text": "representation of this audio signal that happens to be easier to use for a lot of",
    "start": "1308460",
    "end": "1313800"
  },
  {
    "text": "purposes okay so our goal starting from",
    "start": "1313800",
    "end": "1320670"
  },
  {
    "text": "this representation is to build what I'm going to call an acoustic model but which is really to the extent we can",
    "start": "1320670",
    "end": "1326940"
  },
  {
    "text": "make it happen is really going to be an entire speech engine that is represented by a neural network",
    "start": "1326940",
    "end": "1332120"
  },
  {
    "text": "so what we would like to do is build a neural net that if we could train it",
    "start": "1332120",
    "end": "1338210"
  },
  {
    "text": "from a whole bunch of pairs X which is my original audio that I turn into a spectrogram and Y star that's the ground",
    "start": "1338210",
    "end": "1345530"
  },
  {
    "text": "truth transcription that some human is given me if I were to train this big neural network off of these pairs what",
    "start": "1345530",
    "end": "1353690"
  },
  {
    "text": "I'd like it to produce is some kind of output that I'm representing by the character C here so that I could later",
    "start": "1353690",
    "end": "1361550"
  },
  {
    "text": "extract the correct transcription which I'm going to denote by Y so if I said",
    "start": "1361550",
    "end": "1367460"
  },
  {
    "text": "hello the first thing I'm going to do is run pre-processing to get all these spectrogram frames and then I'm going to",
    "start": "1367460",
    "end": "1373820"
  },
  {
    "text": "have a recurrent neural network that consumes each frame and processes them into some new representation called C",
    "start": "1373820",
    "end": "1381260"
  },
  {
    "text": "and hopefully I can engineer my network in such a way but I can just read the",
    "start": "1381260",
    "end": "1386570"
  },
  {
    "text": "transcription off of these output neurons so that's kind of the the intuitive picture of what we want to",
    "start": "1386570",
    "end": "1393380"
  },
  {
    "text": "accomplish so as I mentioned back in the outline there's one obvious fundamental",
    "start": "1393380",
    "end": "1400310"
  },
  {
    "text": "problem here which is that the length of the input is not the same as the length",
    "start": "1400310",
    "end": "1407420"
  },
  {
    "text": "of the transcription so if I say hello very slowly then I can have a very long",
    "start": "1407420",
    "end": "1413930"
  },
  {
    "text": "audio signal even though I didn't change the length of the transcription or if I say hello very quickly then I kind of",
    "start": "1413930",
    "end": "1420140"
  },
  {
    "text": "very short transcript or a very short piece of audio and so that means that this output of my neural network is",
    "start": "1420140",
    "end": "1426020"
  },
  {
    "text": "changing length and I need to come up with some way to reprimand neural",
    "start": "1426020",
    "end": "1431570"
  },
  {
    "text": "network output to this fixed length transcription and also do it in a way that we can actually train this pipeline",
    "start": "1431570",
    "end": "1437980"
  },
  {
    "text": "so the traditional way to deal with this problem if you were building a speech",
    "start": "1437980",
    "end": "1446300"
  },
  {
    "text": "engine several years ago is to just try to bootstrap the whole system so I had actually train a neural network to",
    "start": "1446300",
    "end": "1452900"
  },
  {
    "text": "correctly predict the sounds at every frame using some kind of data set like",
    "start": "1452900",
    "end": "1458120"
  },
  {
    "text": "timet where someone has lovingly annotated all of the phonemes for me and then I try to figure out the",
    "start": "1458120",
    "end": "1464289"
  },
  {
    "text": "alignment between my saying hello in a phonetic transcription with the input audio and then once I've lined up all of",
    "start": "1464289",
    "end": "1472029"
  },
  {
    "text": "the sounds with the input audio now I don't care about length anymore because I can just make a one-to-one mapping",
    "start": "1472029",
    "end": "1478210"
  },
  {
    "text": "between the audio input and the phoneme outputs that I'm trying to target but",
    "start": "1478210",
    "end": "1483489"
  },
  {
    "text": "this alignment process is horribly error-prone you have to do a lot of extra work to make it work well and so",
    "start": "1483489",
    "end": "1490509"
  },
  {
    "text": "we really don't want to do this we really want to have some kind of solution that lets us solve this",
    "start": "1490509",
    "end": "1496179"
  },
  {
    "text": "straightaway so there are multiple ways to do it and as I mentioned there's some current",
    "start": "1496179",
    "end": "1501639"
  },
  {
    "text": "research on how to use things like attentional model sequence to sequence models that you'll hear about later in",
    "start": "1501639",
    "end": "1508090"
  },
  {
    "text": "order to solve this kind of problem but as I said we'll focus on something",
    "start": "1508090",
    "end": "1513580"
  },
  {
    "text": "called connexion connectionist temporal classification or ctc that is sort of",
    "start": "1513580",
    "end": "1519220"
  },
  {
    "text": "current state of the art for how to do this so here's the basic idea so our recurrent neural network has",
    "start": "1519220",
    "end": "1527919"
  },
  {
    "text": "these output neurons that I'm calling C and the job of these output neurons is",
    "start": "1527919",
    "end": "1533499"
  },
  {
    "text": "to encode a distribution over over the output symbols so as because of the",
    "start": "1533499",
    "end": "1542019"
  },
  {
    "text": "structure of the recurrent Network the length of this symbol sequence C is the same as the length of my audio input so",
    "start": "1542019",
    "end": "1548350"
  },
  {
    "text": "if my audio inputs a was two seconds long that might have a hundred audio",
    "start": "1548350",
    "end": "1554499"
  },
  {
    "text": "frames and that would mean that the length of C is also a hundred a hundred different values so if we were working",
    "start": "1554499",
    "end": "1561940"
  },
  {
    "text": "on a phoneme based model then C would be some kind of phoning representation I",
    "start": "1561940",
    "end": "1567340"
  },
  {
    "text": "mean we would also include a blank symbol which is special for CTC but if",
    "start": "1567340",
    "end": "1572830"
  },
  {
    "text": "as we'll do in the rest of this talk we're trying to just predict the",
    "start": "1572830",
    "end": "1578259"
  },
  {
    "text": "graphemes trying to predict the characters in this language directly from the audio then I would just let C",
    "start": "1578259",
    "end": "1584980"
  },
  {
    "text": "take on a value that's in my alphabet or take on a blank or a space if my",
    "start": "1584980",
    "end": "1590679"
  },
  {
    "text": "language has spaces in it and then the second thing I'm going to do sigh my RNN gives me a distribution over",
    "start": "1590679",
    "end": "1598950"
  },
  {
    "text": "these symbols see is what I'm going to try to define some kind of mapping that can convert this long transcription C",
    "start": "1598950",
    "end": "1607520"
  },
  {
    "text": "into the final transcription Y that's like hello that's the actual string that",
    "start": "1607520",
    "end": "1613320"
  },
  {
    "text": "I want and now recognizing that C is itself a probabilistic creature there's",
    "start": "1613320",
    "end": "1619770"
  },
  {
    "text": "a distribution over choices of C that correspond to the audio once I apply",
    "start": "1619770",
    "end": "1625200"
  },
  {
    "text": "this function that also means that there's a distribution over Y there's a distribution over the possible",
    "start": "1625200",
    "end": "1630600"
  },
  {
    "text": "transcriptions that I could get and what I'll want to do to train my network is to maximize the probability of the",
    "start": "1630600",
    "end": "1637350"
  },
  {
    "text": "correct transcription given the audio so those are the three steps that we have",
    "start": "1637350",
    "end": "1642659"
  },
  {
    "text": "to accomplish in order to make CTC work so let's start with the first one so we",
    "start": "1642659",
    "end": "1651000"
  },
  {
    "text": "have these output neurons C and they represent a distribution over the different symbols that I could be",
    "start": "1651000",
    "end": "1657630"
  },
  {
    "text": "hearing in the audio so I've got some audio signal down here you can see the spectrogram frames poking up and this is",
    "start": "1657630",
    "end": "1665010"
  },
  {
    "text": "being processed by this recurrent neural network and the output is a big bank of",
    "start": "1665010",
    "end": "1671340"
  },
  {
    "text": "softmax in herranz so for the first frame of audio I have a neuron that",
    "start": "1671340",
    "end": "1677340"
  },
  {
    "text": "corresponds to each of the symbols that C could represent and they and this set",
    "start": "1677340",
    "end": "1684809"
  },
  {
    "text": "of softmax neurons here the with the output summing to 1 represents the probability of say C 1 having the value",
    "start": "1684809",
    "end": "1692730"
  },
  {
    "text": "ABC and so on or this special blank character so for example if I pick one",
    "start": "1692730",
    "end": "1699000"
  },
  {
    "text": "of the neurons over here then the first row which it represents the character B",
    "start": "1699000",
    "end": "1705059"
  },
  {
    "text": "and the 17th column which is the 17th frame in time this represents the",
    "start": "1705059",
    "end": "1711990"
  },
  {
    "text": "probability that C 1 7 represents the character be given the audio so once I",
    "start": "1711990",
    "end": "1721289"
  },
  {
    "text": "have this that also means that I can just define a distribution not just over",
    "start": "1721289",
    "end": "1727590"
  },
  {
    "text": "the visual characters but if I just assume that all of the characters are independent which is kind of a naive",
    "start": "1727590",
    "end": "1733770"
  },
  {
    "text": "assumption but if I bake this into the system I can define a distribution over all possible sequences of characters in",
    "start": "1733770",
    "end": "1742320"
  },
  {
    "text": "this alphabet so if I gave you a specific instance a specific character",
    "start": "1742320",
    "end": "1749070"
  },
  {
    "text": "string using this alphabet for instance I represent the string hello as HHH e",
    "start": "1749070",
    "end": "1757160"
  },
  {
    "text": "blank e blank blank LL blank ello and then a bunch of blanks this is a string",
    "start": "1757160",
    "end": "1763380"
  },
  {
    "text": "in this alphabet for for C and I can just use this formula to compute the",
    "start": "1763380",
    "end": "1769080"
  },
  {
    "text": "probability of this specific sequence of characters so that's how we we compute",
    "start": "1769080",
    "end": "1774900"
  },
  {
    "text": "the probability for a sequence of characters when they have the same length as the audio input so the second",
    "start": "1774900",
    "end": "1784500"
  },
  {
    "text": "step and this is in some sense the kind of neat trick in CTC is to define a",
    "start": "1784500",
    "end": "1792690"
  },
  {
    "text": "mapping from this long encoding of the",
    "start": "1792690",
    "end": "1798810"
  },
  {
    "text": "audio into symbols that crunches it down to the actual transcription that we're",
    "start": "1798810",
    "end": "1804690"
  },
  {
    "text": "trying to predict and the rule is this operator takes this character sequence",
    "start": "1804690",
    "end": "1810630"
  },
  {
    "text": "and it picks up all the duplicates all of the adjacent characters that are repeated and discards the duplicates and",
    "start": "1810630",
    "end": "1818310"
  },
  {
    "text": "just keep some of them and then it drops all of the blanks so in this example you",
    "start": "1818310",
    "end": "1824640"
  },
  {
    "text": "see you have three H's together so I just keep one H and then I have a blank",
    "start": "1824640",
    "end": "1830220"
  },
  {
    "text": "I throw that away and I keep an e when I have two L's so I keep one of the LS over here and then another blank and an",
    "start": "1830220",
    "end": "1836880"
  },
  {
    "text": "elbow and the one key thing to note is that when I have two characters that are",
    "start": "1836880",
    "end": "1842580"
  },
  {
    "text": "different right next to each other I just end up keeping those two characters in my output but if I ever have a double",
    "start": "1842580",
    "end": "1849780"
  },
  {
    "text": "character like ll in hello then I'll need to have a blank character that that",
    "start": "1849780",
    "end": "1856410"
  },
  {
    "text": "gets put in between but if our neural network gave me this",
    "start": "1856410",
    "end": "1861750"
  },
  {
    "text": "transcription told me that this was the right answer we just have to apply this operator and we get back Vic string",
    "start": "1861750",
    "end": "1868350"
  },
  {
    "text": "hello so now that we have a way to",
    "start": "1868350",
    "end": "1874040"
  },
  {
    "text": "define a distribution over these sequences of symbols that are the same length as the audio and we now have a",
    "start": "1874040",
    "end": "1880710"
  },
  {
    "text": "mapping from those strings into transcriptions as I said this gives us a",
    "start": "1880710",
    "end": "1886530"
  },
  {
    "text": "probability distribution over the possible final transcriptions so if I",
    "start": "1886530",
    "end": "1892350"
  },
  {
    "text": "look at the probability distribution over all the different sequences of symbols right",
    "start": "1892350",
    "end": "1897720"
  },
  {
    "text": "I might have hello written out like on the last slide and maybe that has probability 0.1 and then I might have",
    "start": "1897720",
    "end": "1905130"
  },
  {
    "text": "hello but written a different way with a different by say replacing this H with a blank that has a smaller probability and",
    "start": "1905130",
    "end": "1912840"
  },
  {
    "text": "I have a whole bunch of different possible symbol sequences below that and what you'll notice is that if I go",
    "start": "1912840",
    "end": "1921000"
  },
  {
    "text": "through every possible combination of symbols here there are several combinations that all",
    "start": "1921000",
    "end": "1928290"
  },
  {
    "text": "map to the same transcription so here's one version of hello there's a second",
    "start": "1928290",
    "end": "1933690"
  },
  {
    "text": "version of hello there's a third version of hello and so if I now ask what's the probability of the transcription hello",
    "start": "1933690",
    "end": "1941040"
  },
  {
    "text": "the way that I compute that is I go through all of the possible character",
    "start": "1941040",
    "end": "1946320"
  },
  {
    "text": "sequences that correspond to the transcription hello and I add up all of",
    "start": "1946320",
    "end": "1951750"
  },
  {
    "text": "their probabilities so I have to sum over all possible choices of C that",
    "start": "1951750",
    "end": "1957210"
  },
  {
    "text": "could give me that transcription in the end so you can kind of think of this as",
    "start": "1957210",
    "end": "1963500"
  },
  {
    "text": "searching through all the possible alignments right I could shift these characters around a",
    "start": "1963500",
    "end": "1970170"
  },
  {
    "text": "little bit I can move them forward backward I could expand them by adding duplicates or squish them up depending",
    "start": "1970170",
    "end": "1975780"
  },
  {
    "text": "on how fast someone is talking and that corresponds to every possible alignment between the audio and the characters",
    "start": "1975780",
    "end": "1984060"
  },
  {
    "text": "that I want to transcribe it sort of solves the problem of the variable length and the way that I get the",
    "start": "1984060",
    "end": "1990150"
  },
  {
    "text": "probability of a specific transcription is to sum up to marginalize over all the different",
    "start": "1990150",
    "end": "1996659"
  },
  {
    "text": "alignments that could be feasible and then if we have a whole bunch of other",
    "start": "1996659",
    "end": "2002690"
  },
  {
    "text": "possibilities in here like the word yellow-eyed compute them in the same way and so this equation just says to sum",
    "start": "2002690",
    "end": "2009259"
  },
  {
    "text": "over all the character sequences see so that when I apply this little mapping operator I end up with the transcription",
    "start": "2009259",
    "end": "2015889"
  },
  {
    "text": "why is oh I'm missing a EE you're",
    "start": "2015889",
    "end": "2029210"
  },
  {
    "text": "talking about this one so when we apply this sort of squeezing operator here we",
    "start": "2029210",
    "end": "2036080"
  },
  {
    "text": "drop this double e to get a single Ian hello so we remove all the duplicates so",
    "start": "2036080",
    "end": "2043369"
  },
  {
    "text": "the same way we did for an H right so",
    "start": "2043369",
    "end": "2050260"
  },
  {
    "text": "whenever you see two characters together like this where they're adjacent duplicates you sort of squeeze all those",
    "start": "2050260",
    "end": "2057138"
  },
  {
    "text": "duplicates out and you just keep one of them but here we have a blank in between so if we drop all the duplicates first",
    "start": "2057139",
    "end": "2063770"
  },
  {
    "text": "then we still have two L's left and then we remove all the blanks so this gives",
    "start": "2063770",
    "end": "2069618"
  },
  {
    "text": "the algorithm a way to represent repeated characters in the transcription there's another one in the back",
    "start": "2069619",
    "end": "2078040"
  },
  {
    "text": "oh I see yeah this is maybe I put a",
    "start": "2080039",
    "end": "2085210"
  },
  {
    "text": "space in here really I'd have put a space character in here instead of a blank really this could be h-e-l-l-o H",
    "start": "2085210",
    "end": "2096960"
  },
  {
    "text": "yeah so the this space here is erroneous",
    "start": "2096960",
    "end": "2101759"
  },
  {
    "text": "okay very good okay so once I've defined this right I",
    "start": "2102029",
    "end": "2109930"
  },
  {
    "text": "just gave you a formula to compute the probability of a string given the audio",
    "start": "2109930",
    "end": "2115440"
  },
  {
    "text": "so as as with every good starting to a machine learning algorithm we go and we",
    "start": "2115440",
    "end": "2121240"
  },
  {
    "text": "try to apply maximum likelihood I now give you the correct transcription and your job is to tune the neural network",
    "start": "2121240",
    "end": "2128200"
  },
  {
    "text": "to maximize the probability of that transcription using this model that I just defined so in equations what I'm",
    "start": "2128200",
    "end": "2137589"
  },
  {
    "text": "going to do is I want to maximize the log probability of Y star for a given",
    "start": "2137589",
    "end": "2143559"
  },
  {
    "text": "example I want to maximize the probability of the correct transcription",
    "start": "2143559",
    "end": "2148869"
  },
  {
    "text": "given the audio X and then I'm just going to sum over all the examples and",
    "start": "2148869",
    "end": "2155759"
  },
  {
    "text": "then what I want to do is just replace this with the equation that I had on the",
    "start": "2155759",
    "end": "2161859"
  },
  {
    "text": "last page that says in order to compute the probability of a given transcription I have to sum over all of the possible",
    "start": "2161859",
    "end": "2168549"
  },
  {
    "text": "symbol sequences that could have given me that transcription sum over all the possible alignments that would map that",
    "start": "2168549",
    "end": "2175720"
  },
  {
    "text": "transcription to my audio so Alex grades and co-authors in 2006 actually show",
    "start": "2175720",
    "end": "2182230"
  },
  {
    "text": "that because of this independence assumption there is a clever way there",
    "start": "2182230",
    "end": "2187299"
  },
  {
    "text": "is a dynamic programming algorithm that can efficiently compute this summation for you and not only commute compute",
    "start": "2187299",
    "end": "2194500"
  },
  {
    "text": "this summation so that you can compute the objective function but actually compute its gradient with respect to to",
    "start": "2194500",
    "end": "2200410"
  },
  {
    "text": "the output neurons of your neural network so if you look at the paper the algorithm details are in there",
    "start": "2200410",
    "end": "2206570"
  },
  {
    "text": "what school right now in the history of speech and deep learning is that this is",
    "start": "2206570",
    "end": "2211760"
  },
  {
    "text": "at the level of a technology this is something that's now implemented in a bunch of places so that you can download",
    "start": "2211760",
    "end": "2217280"
  },
  {
    "text": "a software package that efficiently will calculate this ctc loss function for you",
    "start": "2217280",
    "end": "2224720"
  },
  {
    "text": "that can calculate this likelihood and can also just give you back the gradient so I won't go into the equations here",
    "start": "2224720",
    "end": "2230960"
  },
  {
    "text": "instead I'll tell you that there are a whole bunch of implementations on the web that you can now use as part of deep",
    "start": "2230960",
    "end": "2237680"
  },
  {
    "text": "learning packages so one of them from Baidu implements CTC on the GPU is",
    "start": "2237680",
    "end": "2243350"
  },
  {
    "text": "called warp CTC Stanford and group they're actually one of Andrews students",
    "start": "2243350",
    "end": "2250160"
  },
  {
    "text": "has a CTC implementation and there's also now CTC losses implemented in",
    "start": "2250160",
    "end": "2255800"
  },
  {
    "text": "packages like tensor flow so this is something that's sufficiently widely distributed that you can use use these",
    "start": "2255800",
    "end": "2264080"
  },
  {
    "text": "algorithms off the shelf so the way that these work the way that we go about",
    "start": "2264080",
    "end": "2269120"
  },
  {
    "text": "training is we start from our audio spectrogram we have our neural network structure where you get to choose how",
    "start": "2269120",
    "end": "2276380"
  },
  {
    "text": "it's put together and then it outputs this Bank of softmax neurons and then",
    "start": "2276380",
    "end": "2281840"
  },
  {
    "text": "there are pieces of off-the-shelf software that will compute for you the CTC cost function they'll compute this",
    "start": "2281840",
    "end": "2288770"
  },
  {
    "text": "log likelihood given a transcription and the output neurons from your recurrent",
    "start": "2288770",
    "end": "2294530"
  },
  {
    "text": "Network and then the software will also be able to tell you the gradient with",
    "start": "2294530",
    "end": "2300080"
  },
  {
    "text": "respect to the output neurons and once you've got that you're set you can feed them back into the rest of your code and",
    "start": "2300080",
    "end": "2306170"
  },
  {
    "text": "get the gradient with respect to all of these parameters so as I said this is all available now in sort of efficient",
    "start": "2306170",
    "end": "2313010"
  },
  {
    "text": "off-the-shelf software so you don't have to do this work yourself so that's pretty much all there is to the high",
    "start": "2313010",
    "end": "2320900"
  },
  {
    "text": "level algorithm with this it's actually enough to get a sort of a working",
    "start": "2320900",
    "end": "2326450"
  },
  {
    "text": "Drosophila of speech recognition going there are a few a few little tricks",
    "start": "2326450",
    "end": "2331640"
  },
  {
    "text": "though that you might need along the way on easy problems you might not need these but as you get to more",
    "start": "2331640",
    "end": "2338900"
  },
  {
    "text": "difficult datasets with a lot of noise they can become more and more important so the first one that we've been calling",
    "start": "2338900",
    "end": "2345559"
  },
  {
    "text": "sort of grad in the vein of all of the grad algorithms out there is basically a",
    "start": "2345559",
    "end": "2352609"
  },
  {
    "text": "trick to help with recurrent neural networks so it turns out that when you",
    "start": "2352609",
    "end": "2358010"
  },
  {
    "text": "try to train one of these big RNN models on some off-the-shelf speech data one of",
    "start": "2358010",
    "end": "2364460"
  },
  {
    "text": "the things that can really get you is seeing very long utterances early in the process because if you have a really",
    "start": "2364460",
    "end": "2372049"
  },
  {
    "text": "long audience then if your neural network is badly initialized you'll often end up with things like underflow",
    "start": "2372049",
    "end": "2378589"
  },
  {
    "text": "and overflow as you try to go and compute the probabilities and you end up with gradients exploding as you try to",
    "start": "2378589",
    "end": "2384710"
  },
  {
    "text": "do back propagation and it can make your optimization a real mess and it's coming",
    "start": "2384710",
    "end": "2389720"
  },
  {
    "text": "from the fact that these utterances are really long and really hard and the neural network just isn't ready to deal",
    "start": "2389720",
    "end": "2395690"
  },
  {
    "text": "with those transcriptions and so one of the fixes that you can use is during the early parts of training usually in the",
    "start": "2395690",
    "end": "2402740"
  },
  {
    "text": "first epic is you just sort all of your audio by length and now when you process",
    "start": "2402740",
    "end": "2408260"
  },
  {
    "text": "a mini batch you just take the short utterances first so that you're working with really short rnns that are quite",
    "start": "2408260",
    "end": "2414980"
  },
  {
    "text": "easy to train and don't blow up and don't have a lot of catastrophic numerical problems and then as time goes",
    "start": "2414980",
    "end": "2421339"
  },
  {
    "text": "by you start operating on longer and longer addresses that get more and more difficult so we call this sort of grad",
    "start": "2421339",
    "end": "2428539"
  },
  {
    "text": "it's basically a curriculum learning method and so you can see some work from yoshua bengio and his team on a whole",
    "start": "2428539",
    "end": "2435859"
  },
  {
    "text": "bunch of strategies for this but you can think of the short utterances as being the easy ones and if you start out with",
    "start": "2435859",
    "end": "2441230"
  },
  {
    "text": "the easy utterances and move to the longer ones your optimization algorithm can do better so here's what an example",
    "start": "2441230",
    "end": "2448789"
  },
  {
    "text": "from one of the models that we've trained where your CTC cost starts up here and you know after a while you",
    "start": "2448789",
    "end": "2455569"
  },
  {
    "text": "optimize and you sort of bottom out around you know what a log likelihood of maybe 30 and then if you add this sort",
    "start": "2455569",
    "end": "2463460"
  },
  {
    "text": "of grad strategy after the first epic you're actually doing better and you can reach a better optimum than you",
    "start": "2463460",
    "end": "2470150"
  },
  {
    "text": "without it and in addition another strategy that's extremely helpful for",
    "start": "2470150",
    "end": "2475609"
  },
  {
    "text": "recurrent networks and very deep neural networks is batch normalization so so",
    "start": "2475609",
    "end": "2480859"
  },
  {
    "text": "this becoming very popular and it's also available as sort of an off-the-shelf package inside of a lot of the different",
    "start": "2480859",
    "end": "2487339"
  },
  {
    "text": "frameworks that are available today so if you start having trouble you can consider putting batch normalization",
    "start": "2487339",
    "end": "2492559"
  },
  {
    "text": "into your network okay so our neural network now spits out this big bank of",
    "start": "2492559",
    "end": "2499670"
  },
  {
    "text": "softmax neurons we've got a training algorithm we're just doing gradient descent how do we actually get a",
    "start": "2499670",
    "end": "2506480"
  },
  {
    "text": "transcription this process as I said is meant to be as close to characters as",
    "start": "2506480",
    "end": "2511730"
  },
  {
    "text": "possible but we still sort of need to decode these outputs and you might think",
    "start": "2511730",
    "end": "2517760"
  },
  {
    "text": "that one simple solution which turns out to be approximate to get the correct transcription is just go through here",
    "start": "2517760",
    "end": "2524299"
  },
  {
    "text": "and pick the most likely sequence of symbols for C and then apply our little",
    "start": "2524299",
    "end": "2530510"
  },
  {
    "text": "squeeze operator to get back the transcription the way that we defined it so this turns out not to be the optimal",
    "start": "2530510",
    "end": "2537260"
  },
  {
    "text": "thing this actually doesn't give you the most likely transcription because it's not accounting for the fact that every",
    "start": "2537260",
    "end": "2543740"
  },
  {
    "text": "transcription might have multiple sequences of C's multiple alignments in",
    "start": "2543740",
    "end": "2549020"
  },
  {
    "text": "this representation but you can actually do this and this is called the max",
    "start": "2549020",
    "end": "2554329"
  },
  {
    "text": "decoding and so for this sort of contrived example here",
    "start": "2554329",
    "end": "2559730"
  },
  {
    "text": "I put little red dots on the most likely C and if you see there's a couple of",
    "start": "2559730",
    "end": "2565940"
  },
  {
    "text": "blanks a couple of C's is another blank a more blanks bees more blanks and if",
    "start": "2565940",
    "end": "2572839"
  },
  {
    "text": "you apply our little squeeze operator you just get the word cab if you do this",
    "start": "2572839",
    "end": "2579410"
  },
  {
    "text": "it is often terrible it'll often give you a very strange transcription that",
    "start": "2579410",
    "end": "2585559"
  },
  {
    "text": "doesn't look like English necessarily but the reason I mention it is that this",
    "start": "2585559",
    "end": "2590690"
  },
  {
    "text": "is a really handy diagnostic that if you're kind of wondering what's going on in the network glancing at a few of",
    "start": "2590690",
    "end": "2596990"
  },
  {
    "text": "these will often tell you if the network's starting to pick up any signal or if it's just outputting gobbled",
    "start": "2596990",
    "end": "2602450"
  },
  {
    "text": "cook so I'll give you a more detailed example in a second of how that happens",
    "start": "2602450",
    "end": "2607480"
  },
  {
    "text": "all right so these are all the concepts of our of our very simple pipeline and",
    "start": "2607480",
    "end": "2613010"
  },
  {
    "text": "the demo code that we're going to put up on the web will basically let you work on all of these pieces so once we try to",
    "start": "2613010",
    "end": "2621770"
  },
  {
    "text": "train these I want to give you an example of the sort of data that we're training on a tanker is a ship designed",
    "start": "2621770",
    "end": "2627200"
  },
  {
    "text": "to carry large volumes of oil okay so this is just a person sitting there",
    "start": "2627200",
    "end": "2633800"
  },
  {
    "text": "reading The Wall Street Journal to us so this is a sort of simple data set it's really popular in the speech research",
    "start": "2633800",
    "end": "2640970"
  },
  {
    "text": "community it's published by the linguistic data consortium there's also a free alternative called libera speech",
    "start": "2640970",
    "end": "2647450"
  },
  {
    "text": "that's very similar but instead of people reading The Wall Street Journal is people reading Creative Commons audiobooks so in the demo code that we",
    "start": "2647450",
    "end": "2658520"
  },
  {
    "text": "have a really simple network that works reasonably well it looks like this so there's a sort of family of models that",
    "start": "2658520",
    "end": "2666200"
  },
  {
    "text": "we've been working with where you start from your spectrogram you have maybe one layer or several of convolutional",
    "start": "2666200",
    "end": "2673430"
  },
  {
    "text": "filters at the bottom and then on top of that you have some kind of recurrent neural network it might just be a",
    "start": "2673430",
    "end": "2678770"
  },
  {
    "text": "vanilla RNN but but you can also use like LS TM or GRU cells any of your",
    "start": "2678770",
    "end": "2686960"
  },
  {
    "text": "favorite RNN creatures from the literature and then on top of that we have some fully connected layers that",
    "start": "2686960",
    "end": "2693290"
  },
  {
    "text": "produce these softmax outputs and those are the things that go into CTC for training so this is pretty",
    "start": "2693290",
    "end": "2700069"
  },
  {
    "text": "straightforward the implementation on the web uses the the work CTC code and then we would just train this big neural",
    "start": "2700069",
    "end": "2706490"
  },
  {
    "text": "network with stochastic gradient descent Nesterov momentum all the stuff that you've probably seen in a whole bunch of",
    "start": "2706490",
    "end": "2712700"
  },
  {
    "text": "other talks so far all right so if you actually run this what is going on",
    "start": "2712700",
    "end": "2718940"
  },
  {
    "text": "inside so I mentioned that looking at the max decoding is kind of a handy way",
    "start": "2718940",
    "end": "2725329"
  },
  {
    "text": "to see what's what's going on inside this creature so I wanted to show you an",
    "start": "2725329",
    "end": "2730640"
  },
  {
    "text": "example so this is a picture this is a visualization",
    "start": "2730640",
    "end": "2736190"
  },
  {
    "text": "those softmax neurons at the top of one of these big neural networks so this is",
    "start": "2736190",
    "end": "2741200"
  },
  {
    "text": "the representation of see from all the previous slides so on the horizontal",
    "start": "2741200",
    "end": "2746270"
  },
  {
    "text": "axis this is basically time this is the frame number or which chunk of the spectrogram we're seeing and then on the",
    "start": "2746270",
    "end": "2753079"
  },
  {
    "text": "vertical axis here you see these are all the characters in the English alphabet or a space or a blank so after three",
    "start": "2753079",
    "end": "2760670"
  },
  {
    "text": "hundred iterations of training which is not very much the system has learned something amazing which is that it",
    "start": "2760670",
    "end": "2766040"
  },
  {
    "text": "should just output blanks and spaces all the time because these are by far because of all the silence and things in",
    "start": "2766040",
    "end": "2773240"
  },
  {
    "text": "your data set these are the most common characters right I just want to fill up the whole space with blanks but you can",
    "start": "2773240",
    "end": "2779569"
  },
  {
    "text": "see it's kind of randomly poking out a few characters here and if you run your little Mac's decoding strategy to see",
    "start": "2779569",
    "end": "2786050"
  },
  {
    "text": "what is the system think the transcription is it thinks it transcription is at and so but after",
    "start": "2786050",
    "end": "2793339"
  },
  {
    "text": "three hundred iterations that's okay but this is a sign that the neural networks not going crazy your gradient isn't",
    "start": "2793339",
    "end": "2799130"
  },
  {
    "text": "busted it's at least learned what is the most likely characters then after maybe",
    "start": "2799130",
    "end": "2805250"
  },
  {
    "text": "1500 or so you start to get a little bit of structure and if you try to like",
    "start": "2805250",
    "end": "2810500"
  },
  {
    "text": "mouthed these words you might be able to sort of see that there's some English",
    "start": "2810500",
    "end": "2815540"
  },
  {
    "text": "like sounds in here like they are just in frightened something kind of odd but",
    "start": "2815540",
    "end": "2821690"
  },
  {
    "text": "it's actually looking much better than just h it's actually starting to output something go a little bit farther it's a",
    "start": "2821690",
    "end": "2828680"
  },
  {
    "text": "little bit more organized you could start to see that we have sort of",
    "start": "2828680",
    "end": "2834200"
  },
  {
    "text": "fragments of possibly words starting to form and then after you're getting close",
    "start": "2834200",
    "end": "2840170"
  },
  {
    "text": "to convergence it's still not a real sentence but does this make sense to people he guess like what the correct",
    "start": "2840170",
    "end": "2847700"
  },
  {
    "text": "transcription might be yeah so you might have a couple of candidates the the",
    "start": "2847700",
    "end": "2854300"
  },
  {
    "text": "correct one is actually there just in front and so you can see that sort of",
    "start": "2854300",
    "end": "2860150"
  },
  {
    "text": "it's sort of sounding it out with English characters like I have a young son and I kind of figure I'm eventually",
    "start": "2860150",
    "end": "2866569"
  },
  {
    "text": "going to see him producing max Dakota puts of English and you're just going to",
    "start": "2866569",
    "end": "2872090"
  },
  {
    "text": "like sound these things that we like if they're just in front there but but this is why this max decoding strategy is",
    "start": "2872090",
    "end": "2879620"
  },
  {
    "text": "really handy because you can kind of look at this output and say yeah it's starting to get some actual signal out of the data it's not just gobbledygook",
    "start": "2879620",
    "end": "2886300"
  },
  {
    "text": "so because this is like my favorite speech recognition party game I wanted",
    "start": "2886300",
    "end": "2891440"
  },
  {
    "text": "to show you a few more of these so here's the max decoded output the poor",
    "start": "2891440",
    "end": "2896840"
  },
  {
    "text": "little things cried Cynthia think of them having been turned to the wall all these years so you can hear like the",
    "start": "2896840",
    "end": "2903860"
  },
  {
    "text": "sound of the breath at the end turns into a little bit of a word Cynthia is sort of in this transcription",
    "start": "2903860",
    "end": "2913000"
  },
  {
    "text": "and you'll find that things like proper names and so on tend to get sounded out but if those names are not in your audio",
    "start": "2913000",
    "end": "2920120"
  },
  {
    "text": "data there's no way the network could have learned how to say the name Cynthia and we'll come back to how to solve that",
    "start": "2920120",
    "end": "2926180"
  },
  {
    "text": "later did you see the true label the poor little things cried Cynthia and",
    "start": "2926180",
    "end": "2932090"
  },
  {
    "text": "that the last word is actually all these years and there isn't a word hanging off at the end so here's another one that is",
    "start": "2932090",
    "end": "2940400"
  },
  {
    "text": "true bad grade how many people figured",
    "start": "2940400",
    "end": "2945410"
  },
  {
    "text": "out what this is this is the max decoded transcription sounds sounds good to you",
    "start": "2945410",
    "end": "2952010"
  },
  {
    "text": "it sounds good to me if you told me that this was the ground truth like oh that's weird I have to go",
    "start": "2952010",
    "end": "2957170"
  },
  {
    "text": "what lookup what this is here's the actual true label turns out this is a",
    "start": "2957170",
    "end": "2962570"
  },
  {
    "text": "French word that means something like rubbernecking I had no idea what this",
    "start": "2962570",
    "end": "2968030"
  },
  {
    "text": "word was so this is again the cool examples of what these neural networks are able to figure out with no knowledge",
    "start": "2968030",
    "end": "2975080"
  },
  {
    "text": "of the language itself okay so let's go",
    "start": "2975080",
    "end": "2981320"
  },
  {
    "text": "back to decoding we just talked about max decoding which is sort of an approximate way of going from these",
    "start": "2981320",
    "end": "2988180"
  },
  {
    "text": "probability vectors to a transcription Y and if you want to find the actual most",
    "start": "2988180",
    "end": "2994790"
  },
  {
    "text": "likely transcription Y there's actually no algorithm in general that can give",
    "start": "2994790",
    "end": "3000190"
  },
  {
    "text": "you the perfect solution efficiently so the reason for that remember is that for a",
    "start": "3000190",
    "end": "3006040"
  },
  {
    "text": "single transcription why I have an efficient algorithm to compute its probability but if I want to search over",
    "start": "3006040",
    "end": "3012190"
  },
  {
    "text": "every possible transcription I don't know how to do that because there combinatorially or exponentially many",
    "start": "3012190",
    "end": "3019260"
  },
  {
    "text": "possible transcriptions and I'd have to run this algorithm to compute the probability of all of them so we have to",
    "start": "3019260",
    "end": "3026410"
  },
  {
    "text": "resort to some kind of generic search strategy and so one proposed in the original paper briefly is a sort of",
    "start": "3026410",
    "end": "3033610"
  },
  {
    "text": "prefix decoding strategy so I don't want to spend a ton of time on this instead I",
    "start": "3033610",
    "end": "3039580"
  },
  {
    "text": "want to step to sort of the next piece of the picture so there were a bunch of",
    "start": "3039580",
    "end": "3045160"
  },
  {
    "text": "examples in there right like proper names like Cynthia and things like but Dow Derby where unless you had heard",
    "start": "3045160",
    "end": "3054340"
  },
  {
    "text": "this word before you have no hope of getting it right with your neural network and so there are lots of",
    "start": "3054340",
    "end": "3061270"
  },
  {
    "text": "examples like this in the literature of things that are sort of spelled out phonetically but aren't legitimate",
    "start": "3061270",
    "end": "3067630"
  },
  {
    "text": "English transcriptions and so what we'd like to do is come up with a way to fold",
    "start": "3067630",
    "end": "3073840"
  },
  {
    "text": "in just a little bit of that knowledge about the language that take a small",
    "start": "3073840",
    "end": "3078850"
  },
  {
    "text": "step backward from a perfect end-to-end system and make make these transcriptions better so as I said the",
    "start": "3078850",
    "end": "3086890"
  },
  {
    "text": "real problem here is that you don't have enough audio available to learn all these things if we had millions and",
    "start": "3086890",
    "end": "3092770"
  },
  {
    "text": "millions of hours of audio sitting around you could probably learn all these transcriptions because you just hear enough words that you know how to",
    "start": "3092770",
    "end": "3099490"
  },
  {
    "text": "spell them all maybe the way a human does but unfortunately we just don't",
    "start": "3099490",
    "end": "3104860"
  },
  {
    "text": "have enough audio for that so we have to find a way to get around that data problem there's also an example of",
    "start": "3104860",
    "end": "3111160"
  },
  {
    "text": "something that in the AI lab we've dubbed the Tchaikovsky problem which is that there are certain names in the",
    "start": "3111160",
    "end": "3117040"
  },
  {
    "text": "world right like proper names that if you've never heard of it before you have no idea how it's spelled and the only",
    "start": "3117040",
    "end": "3123880"
  },
  {
    "text": "way to know it is to have seen this word in text before and to see it in context so part of the purpose of these language",
    "start": "3123880",
    "end": "3131800"
  },
  {
    "text": "models is to get examples like this correct so there are a couple of solutions one",
    "start": "3131800",
    "end": "3136830"
  },
  {
    "text": "would be to just step back to a more traditional pipeline right use phonemes because then we can bake new words in",
    "start": "3136830",
    "end": "3143340"
  },
  {
    "text": "along with their phonetic pronunciation and the system will just get it right",
    "start": "3143340",
    "end": "3148370"
  },
  {
    "text": "but in in this case I want to focus on just fusing in a traditional language",
    "start": "3148370",
    "end": "3154110"
  },
  {
    "text": "model that gives us the probability a priori of any sequence of words so the",
    "start": "3154110",
    "end": "3160440"
  },
  {
    "text": "reason that this is helpful is that using a language model we can train these things from massive text corpora",
    "start": "3160440",
    "end": "3167430"
  },
  {
    "text": "we have way way more text in the world than we have transcribed audio and so",
    "start": "3167430",
    "end": "3172980"
  },
  {
    "text": "that makes it possible to train these giant language models with huge vocabulary and they can also pick up the",
    "start": "3172980",
    "end": "3179220"
  },
  {
    "text": "sort of contextual things that will tip you off to the fact that Tchaikovsky concerto is a reasonable thing for a",
    "start": "3179220",
    "end": "3185670"
  },
  {
    "text": "person to ask and that this particular transcription which we have seen in the",
    "start": "3185670",
    "end": "3190920"
  },
  {
    "text": "past trike offski concerto even though composed of legitimate English words is",
    "start": "3190920",
    "end": "3196770"
  },
  {
    "text": "is nonsense so there's actually not much to see on",
    "start": "3196770",
    "end": "3202800"
  },
  {
    "text": "the language modeling front for this except that the reasons for sticking with traditional and grand models are",
    "start": "3202800",
    "end": "3209250"
  },
  {
    "text": "kind of interesting if you're excited about speech applications so if you go use a package like Ken LM on the web to",
    "start": "3209250",
    "end": "3217050"
  },
  {
    "text": "go build yourself a giant and Grahm language model these are really simple and well supported and so that makes",
    "start": "3217050",
    "end": "3224820"
  },
  {
    "text": "them easy to get working and they'll let you train from lots of corpora but for",
    "start": "3224820",
    "end": "3230370"
  },
  {
    "text": "speech recognition in practice one of the nice things about Engram models as opposed to trying to say use like an RNN",
    "start": "3230370",
    "end": "3237870"
  },
  {
    "text": "model is that we can update these things very quickly if you have a big distributed cluster you can update that",
    "start": "3237870",
    "end": "3243690"
  },
  {
    "text": "Engram model very rapidly in parallel from new data to keep track of whatever the trending words are today that your",
    "start": "3243690",
    "end": "3249840"
  },
  {
    "text": "speech engine might need to deal with and we also have the need to query this",
    "start": "3249840",
    "end": "3254910"
  },
  {
    "text": "thing very rapidly inside our decoding loop that you'll see in just a second",
    "start": "3254910",
    "end": "3259980"
  },
  {
    "text": "and so being able to just look up the probabilities in a table the way an Engram model is structured is very",
    "start": "3259980",
    "end": "3265220"
  },
  {
    "text": "valuable so I hope someday all of this will go away and be replaced with an",
    "start": "3265220",
    "end": "3271310"
  },
  {
    "text": "amazing neural network but this is the really best practice today so in order",
    "start": "3271310",
    "end": "3279050"
  },
  {
    "text": "to fuse this into the system since to get the most likely transcription right",
    "start": "3279050",
    "end": "3285109"
  },
  {
    "text": "probably of Y given X to maximize that thing we need to use a generic search algorithm anyway this opens up a door",
    "start": "3285109",
    "end": "3293119"
  },
  {
    "text": "once we're using a generic search scheme to do our decoding and find the most likely transcription we can add some",
    "start": "3293119",
    "end": "3299180"
  },
  {
    "text": "extra cost terms so in a previous piece of work from Audi haneun and several",
    "start": "3299180",
    "end": "3305840"
  },
  {
    "text": "co-authors what you do is you take the probability of a given word sequence",
    "start": "3305840",
    "end": "3311470"
  },
  {
    "text": "from your audio so this is what you would get from your giant RNN and you",
    "start": "3311470",
    "end": "3318080"
  },
  {
    "text": "can just multiply it by some extra terms the probability of the word sequence according to your language model raised",
    "start": "3318080",
    "end": "3324050"
  },
  {
    "text": "to some power and then multiplied by the length we raised to another power you see that if you just take the log of",
    "start": "3324050",
    "end": "3330980"
  },
  {
    "text": "this objective function right then you get the log probability that was your original objective you get alpha times",
    "start": "3330980",
    "end": "3338390"
  },
  {
    "text": "the log probability of the language model and beta times the log of the",
    "start": "3338390",
    "end": "3343430"
  },
  {
    "text": "length and these alpha and beta parameters let you sort of trade-off the importance of getting a transcription",
    "start": "3343430",
    "end": "3350690"
  },
  {
    "text": "that makes sense to your language model versus getting a transcription that makes sense to your acoustic model and",
    "start": "3350690",
    "end": "3355730"
  },
  {
    "text": "actually sounds like the thing that you heard and the reason for this extra term over here is that as you're multiplying",
    "start": "3355730",
    "end": "3363500"
  },
  {
    "text": "in all of these terms you tend to penalize long transcriptions a bit too much and so having a little bonus or",
    "start": "3363500",
    "end": "3370490"
  },
  {
    "text": "penalty at the end to tweak to get the transcription length right is very helpful so the basic idea behind this is",
    "start": "3370490",
    "end": "3378260"
  },
  {
    "text": "just to use beam search so beam search really popular search algorithm a whole bunch of instances of it and the rough",
    "start": "3378260",
    "end": "3386200"
  },
  {
    "text": "strategy is this so starting from time zero starting from T equals one at the",
    "start": "3386200",
    "end": "3393170"
  },
  {
    "text": "very beginning of your audio input I start out with an empty list that I'm",
    "start": "3393170",
    "end": "3398240"
  },
  {
    "text": "going to pop you late with prefixes and these prefixes are just partial transcriptions that represent what I think I've heard so far",
    "start": "3398240",
    "end": "3405470"
  },
  {
    "text": "in the audio up to the current time and the way that this proceeds is I'm going",
    "start": "3405470",
    "end": "3413000"
  },
  {
    "text": "to take at the current time step each candidate prefix out of this list and then I'm going to try all of the",
    "start": "3413000",
    "end": "3421010"
  },
  {
    "text": "possible characters in my soft max neurons that can possibly follow it so",
    "start": "3421010",
    "end": "3426140"
  },
  {
    "text": "for example I can try adding a blank I say if the next element of C is actually",
    "start": "3426140",
    "end": "3433130"
  },
  {
    "text": "supposed to be a blank then what that would mean is that I don't change my prefix right because the blanks are just",
    "start": "3433130",
    "end": "3439040"
  },
  {
    "text": "going to get dropped later but I need to incorporate the probability of that blank character into the probability of",
    "start": "3439040",
    "end": "3446870"
  },
  {
    "text": "this prefix right it represents one of the ways that I could reach that prefix",
    "start": "3446870",
    "end": "3452150"
  },
  {
    "text": "and so I need to sum that probability into that candidate and likewise",
    "start": "3452150",
    "end": "3457490"
  },
  {
    "text": "whenever I add a space to the end of a prefix that signals that this prefix",
    "start": "3457490",
    "end": "3463610"
  },
  {
    "text": "represents the end of a word and so in addition to adding the probability of the space into my current estimate this",
    "start": "3463610",
    "end": "3470450"
  },
  {
    "text": "gives me the chance to go look up that word in my language model and fold that into my current score and then if I try",
    "start": "3470450",
    "end": "3477950"
  },
  {
    "text": "adding a new character onto this prefix it's just straightforward I just go and update the probabilities based on the",
    "start": "3477950",
    "end": "3484160"
  },
  {
    "text": "probability of that character and then at the end of this I'm going to have a huge list of possible prefixes that",
    "start": "3484160",
    "end": "3491060"
  },
  {
    "text": "could be generated and this is where you would normally get the exponential blow-up of trying all possible prefixes",
    "start": "3491060",
    "end": "3498710"
  },
  {
    "text": "to find the best one and what beam search does is it just says take the que",
    "start": "3498710",
    "end": "3503720"
  },
  {
    "text": "most probable prefixes after I remove all the duplicates in here and then go",
    "start": "3503720",
    "end": "3509630"
  },
  {
    "text": "and do this again and so if you have a really large que then your algorithm will be a bit more accurate in finding",
    "start": "3509630",
    "end": "3515960"
  },
  {
    "text": "the best possible solution to this maximization problem but it'll be slower",
    "start": "3515960",
    "end": "3521920"
  },
  {
    "text": "so here's what ends up happening if you run this decoding algorithm if you just",
    "start": "3521920",
    "end": "3527750"
  },
  {
    "text": "run it on the are n n outputs you'll see that you it's actually better than straight max",
    "start": "3527750",
    "end": "3533090"
  },
  {
    "text": "decoding you find slightly better solutions but you still make things like spelling errors like Boston with an AI",
    "start": "3533090",
    "end": "3539650"
  },
  {
    "text": "but once you add in a language model that can actually tell you that the word Boston with an O is much more probable",
    "start": "3539650",
    "end": "3547220"
  },
  {
    "text": "than Boston with an AI see this so one",
    "start": "3547220",
    "end": "3554570"
  },
  {
    "text": "place they can also drop in deep learning that I wanted to mention very rapidly is just if you're not happy with",
    "start": "3554570",
    "end": "3560120"
  },
  {
    "text": "your Engram model because it doesn't have enough context where you've seen a really amazing neural language modeling",
    "start": "3560120",
    "end": "3566840"
  },
  {
    "text": "paper that you'd like to fold in one really easy way to do this and Link it to your current pipeline is to do",
    "start": "3566840",
    "end": "3573470"
  },
  {
    "text": "rescore eeen so when this decoding strategy finishes it can give you the",
    "start": "3573470",
    "end": "3578990"
  },
  {
    "text": "most probable transcription but it also gives you this big list of the top K transcriptions in terms of probability",
    "start": "3578990",
    "end": "3587660"
  },
  {
    "text": "and what you can do is to take what you",
    "start": "3587660",
    "end": "3593570"
  },
  {
    "text": "can do is take your recurrent Network and just rescore all of these and",
    "start": "3593570",
    "end": "3599750"
  },
  {
    "text": "basically reorder them according to this new model so in the instance of a neural",
    "start": "3599750",
    "end": "3605930"
  },
  {
    "text": "language model let's say that this is my N best list right I have five candidates",
    "start": "3605930",
    "end": "3611480"
  },
  {
    "text": "that were output by my decoding strategy and the first one is I'm a connoisseur",
    "start": "3611480",
    "end": "3617510"
  },
  {
    "text": "looking for wine and pork chops sounds good to me I'm a connoisseur looking for",
    "start": "3617510",
    "end": "3622640"
  },
  {
    "text": "wine and pork shots so this is actually quite subtle and depending on what kind",
    "start": "3622640",
    "end": "3630560"
  },
  {
    "text": "of connoisseur you are sort of up to interpretation what you're looking for but perhaps a neural language model is",
    "start": "3630560",
    "end": "3636890"
  },
  {
    "text": "going to be a little bit better if figuring out that wine and port are closely related and if you're a connoisseur you might be looking for",
    "start": "3636890",
    "end": "3642770"
  },
  {
    "text": "wine import shots and so what you would hope to happen is that a neural language model trained on a bunch of text is",
    "start": "3642770",
    "end": "3650630"
  },
  {
    "text": "going to correctly reorder these things and figure out that the second beam",
    "start": "3650630",
    "end": "3656090"
  },
  {
    "text": "candid is actually the correct one even though your Engram model didn't help you",
    "start": "3656090",
    "end": "3662000"
  },
  {
    "text": "okay so that is really the scale model that is the set of concepts that you",
    "start": "3662000",
    "end": "3670460"
  },
  {
    "text": "need to get a working speech recognition engine based on deep learning and so the",
    "start": "3670460",
    "end": "3676820"
  },
  {
    "text": "thing that's left to go to state-of-the-art performance and start serving users is scale so I'm going to",
    "start": "3676820",
    "end": "3683869"
  },
  {
    "text": "kind of run through quickly a bunch of the different tactics that you can use to try to get there so the two pieces of",
    "start": "3683869",
    "end": "3692240"
  },
  {
    "text": "scale that I want to cover of course our data and computing power where do you get them so the first thing to know this",
    "start": "3692240",
    "end": "3699320"
  },
  {
    "text": "is just a number you can keep in the back of your head for all purposes which is that transcribing speech data is not",
    "start": "3699320",
    "end": "3704390"
  },
  {
    "text": "cheap but it's also not prohibitive it's about 50 cents to a dollar a minute depending on the quality you want and",
    "start": "3704390",
    "end": "3710420"
  },
  {
    "text": "who's transcribing it and the difficulty of the data so typical speech benchmarks",
    "start": "3710420",
    "end": "3716150"
  },
  {
    "text": "you'll see out there maybe hundreds to thousands of hours it's like the Liberty",
    "start": "3716150",
    "end": "3721640"
  },
  {
    "text": "speech data set is maybe hundreds of hours there's another data set called Vox Forge and you can kind of cobble",
    "start": "3721640",
    "end": "3727790"
  },
  {
    "text": "these together and get maybe hundreds to thousands of hours but the real challenge is that the application",
    "start": "3727790",
    "end": "3734030"
  },
  {
    "text": "matters a lot so all the utterances I was playing for you are examples of read",
    "start": "3734030",
    "end": "3740780"
  },
  {
    "text": "speech people are sitting in a nice quiet room they're reading something wonderful to me and so I'm going to end",
    "start": "3740780",
    "end": "3746420"
  },
  {
    "text": "up with a speech engine that's really awesome at listening to The Wall Street Journal but maybe not so good at",
    "start": "3746420",
    "end": "3752270"
  },
  {
    "text": "listening to someone in a crowded cafe so the application that you want to target really needs to match your data",
    "start": "3752270",
    "end": "3759320"
  },
  {
    "text": "set and so it's worth at the outset if you're thinking about going and buying a bunch of speech data to think of what is",
    "start": "3759320",
    "end": "3767000"
  },
  {
    "text": "the style of speech you're actually targeting are you worried about red speech like the ones we're hearing or do",
    "start": "3767000",
    "end": "3772280"
  },
  {
    "text": "you care about conversational speech it turns out that when people talk in a conversation it when they're spontaneous",
    "start": "3772280",
    "end": "3779030"
  },
  {
    "text": "they're just coming up with what to say on the fly versus if they have something that they're just dictating and they",
    "start": "3779030",
    "end": "3784940"
  },
  {
    "text": "already know what to say they behave differently and they can exhibit all of these effects like disfluency and",
    "start": "3784940",
    "end": "3791270"
  },
  {
    "text": "stuttering and then in addition to that we have all kinds of environmental factors that",
    "start": "3791270",
    "end": "3796750"
  },
  {
    "text": "might matter for an application like reverb and echo we start to care about the quality of microphones and whether",
    "start": "3796750",
    "end": "3802450"
  },
  {
    "text": "they have noise canceling there's something called Lombard effect that I'll mention again in a second and of",
    "start": "3802450",
    "end": "3808540"
  },
  {
    "text": "course things like speaker accents where you really have to think carefully about how you collect your data to make sure",
    "start": "3808540",
    "end": "3814119"
  },
  {
    "text": "that you you actually represent the kinds of cases you want to test on so",
    "start": "3814119",
    "end": "3819990"
  },
  {
    "text": "the reason that red speech is really popular is because we can get a lot of it and even if it doesn't perfectly",
    "start": "3819990",
    "end": "3826060"
  },
  {
    "text": "match your application it's cheap and getting a lot of it can still help you so I wanted to say a few things about",
    "start": "3826060",
    "end": "3832570"
  },
  {
    "text": "red speech because for less than ten bucks an hour's often a lot less you can get a whole bunch of data and it has the",
    "start": "3832570",
    "end": "3838900"
  },
  {
    "text": "disadvantage that you lose a lot of things like inflection and conversation allottee but but it can still be helpful",
    "start": "3838900",
    "end": "3847510"
  },
  {
    "text": "so one of the things that we've tried doing and I'm always interested to hear",
    "start": "3847510",
    "end": "3853420"
  },
  {
    "text": "more clever schemes for this is you can kind of engineer the way that people read to try to get the effects that you",
    "start": "3853420",
    "end": "3859630"
  },
  {
    "text": "want so so here's one which is that if you want a little bit more conversation",
    "start": "3859630",
    "end": "3865420"
  },
  {
    "text": "ality you want to get people out of that kind of humdrum dictation you can start giving them reading material that's a",
    "start": "3865420",
    "end": "3871000"
  },
  {
    "text": "little more exciting you can give them like movie scripts and books and people will actually start voice acting for you",
    "start": "3871000",
    "end": "3877080"
  },
  {
    "text": "creep in set the witch and see if it is properly heated so that we can put the",
    "start": "3877080",
    "end": "3883060"
  },
  {
    "text": "bread in so these are really wonderful workers right there like kind of really",
    "start": "3883060",
    "end": "3889869"
  },
  {
    "text": "getting into it to give you better data",
    "start": "3889869",
    "end": "3894420"
  },
  {
    "text": "the wolf is dead the wolf is dead and danced for joy around about the well with their mother",
    "start": "3898380",
    "end": "3907710"
  },
  {
    "text": "so yeah people reading poetry they get this sort of lyrical quality into it that you don't get from from just",
    "start": "3907710",
    "end": "3913869"
  },
  {
    "text": "reading The Wall Street Journal and finally there's something called the Lombard effect that happens when people",
    "start": "3913869",
    "end": "3920230"
  },
  {
    "text": "are in noisy environments so if you're in like a noisy party and you're trying to talk to you friend who's a couple of chairs away",
    "start": "3920230",
    "end": "3926849"
  },
  {
    "text": "you'll catch yourself involuntarily going hey over there what are you doing you raise your inflection and you kind",
    "start": "3926849",
    "end": "3933420"
  },
  {
    "text": "of you try to use different tactics to get your signal-to-noise ratio up you'll",
    "start": "3933420",
    "end": "3939599"
  },
  {
    "text": "sort of work around the the channel problem and so this this is very problematic when you're trying to do",
    "start": "3939599",
    "end": "3945990"
  },
  {
    "text": "transcription a noisy environment because people will talk to their phones using all these effects even though the",
    "start": "3945990",
    "end": "3951330"
  },
  {
    "text": "noise canceling and everything could actually help them so one strategy we've tried with varying levels of success",
    "start": "3951330",
    "end": "3957330"
  },
  {
    "text": "then they fell asleep and evening pass but no one came to the poor children is",
    "start": "3957330",
    "end": "3962849"
  },
  {
    "text": "to actually play loud noise in people's headphones to try to get them to elicit",
    "start": "3962849",
    "end": "3968220"
  },
  {
    "text": "this behavior again here this person is kind of raising their voice a little bit in a way that they wouldn't if they were",
    "start": "3968220",
    "end": "3974430"
  },
  {
    "text": "just reading and similarly as I mentioned there are a whole bunch of",
    "start": "3974430",
    "end": "3980550"
  },
  {
    "text": "different augmentation strategies so there are all these effects of environment like reverberation echo",
    "start": "3980550",
    "end": "3986609"
  },
  {
    "text": "background noise that we would like our speech engine to be robust to and one",
    "start": "3986609",
    "end": "3992160"
  },
  {
    "text": "way you could go about trying to solve this is to go collect a bunch of audio from those cases and then transcribe it",
    "start": "3992160",
    "end": "3997619"
  },
  {
    "text": "but but getting that raw audio is really expensive so instead an alternative is",
    "start": "3997619",
    "end": "4002869"
  },
  {
    "text": "to take the really cheap read speech that's very clean and use some like off",
    "start": "4002869",
    "end": "4007910"
  },
  {
    "text": "the shores off the source off the shelf open source audio toolkit to synthesize",
    "start": "4007910",
    "end": "4014839"
  },
  {
    "text": "all the things you want to be robust to so for example if we want to simulate",
    "start": "4014839",
    "end": "4021170"
  },
  {
    "text": "noise in a cafe here here's just me talking to my laptop in a quiet room",
    "start": "4021170",
    "end": "4028270"
  },
  {
    "text": "hello how are you so if I'm just asking how are you and then here's the sound of",
    "start": "4029619",
    "end": "4034940"
  },
  {
    "text": "a cafe so I can obviously collect these",
    "start": "4034940",
    "end": "4040230"
  },
  {
    "text": "independently very cheaply then I can synthesize this by just adding these signals together hello how are you which",
    "start": "4040230",
    "end": "4048030"
  },
  {
    "text": "actually sounds I don't know sounds to me like my talking to my laptop at a Starbucks or something",
    "start": "4048030",
    "end": "4053910"
  },
  {
    "text": "and so for our work on deep speech we actually take something like 10,000 hours of raw audio that sounds kind of",
    "start": "4053910",
    "end": "4060300"
  },
  {
    "text": "like this and then we pile on lots and lots of audio tracks from Creative",
    "start": "4060300",
    "end": "4065670"
  },
  {
    "text": "Commons videos it turns out there's a strange thing people upload like noise tracks to the web that last four hours",
    "start": "4065670",
    "end": "4073310"
  },
  {
    "text": "is like really soothing to listen to the highway or something and so you can",
    "start": "4073310",
    "end": "4078390"
  },
  {
    "text": "download all all these this free found data and you can just overlay it on this voice and you can synthesize perhaps",
    "start": "4078390",
    "end": "4084900"
  },
  {
    "text": "hundreds of thousands of hours of unique audio and so the idea here is that it's",
    "start": "4084900",
    "end": "4091350"
  },
  {
    "text": "just much easier to engineer your data pipeline to be robust than it is to",
    "start": "4091350",
    "end": "4097020"
  },
  {
    "text": "engineer the speech engine itself to be robust so whenever you encounter an environment that you've never seen",
    "start": "4097020",
    "end": "4102390"
  },
  {
    "text": "before and your speech engine is breaking down you should shift your instinct away from trying to engineer",
    "start": "4102390",
    "end": "4108480"
  },
  {
    "text": "the engine to fix it and toward this idea of how do I reproduce it really cheaply in my data so here's that Wall",
    "start": "4108480",
    "end": "4116160"
  },
  {
    "text": "Street Journal example again is it designed to carry large volumes of oil or other liquid cargo and so if I wanted",
    "start": "4116160",
    "end": "4124230"
  },
  {
    "text": "to for instance deal with a person reading Wall Street Journal on a tanker maybe taking a ship designed to carry",
    "start": "4124230",
    "end": "4131640"
  },
  {
    "text": "large volumes of oil or other liquid cargo there's lots of reverb in this room so you can't hear the reverb on the",
    "start": "4131640",
    "end": "4136950"
  },
  {
    "text": "audio but basically you know you can synthesize these things with one line of socks on the command line so from some",
    "start": "4136950",
    "end": "4146758"
  },
  {
    "text": "of our own work with building a large scale speech engine with these technologies this helps a ton and you",
    "start": "4146759",
    "end": "4153359"
  },
  {
    "text": "can actually see that when we run on clean and noisy test utterances as we",
    "start": "4153359",
    "end": "4161008"
  },
  {
    "text": "add more and more data all the way up to about 10,000 hours and using a lot of",
    "start": "4161009",
    "end": "4166798"
  },
  {
    "text": "these synthesis strategies we can just steadily improve the performance of the engine and in fact on",
    "start": "4166799",
    "end": "4173140"
  },
  {
    "text": "things like clean speech you can get down well below 10% word error rate which is a pretty pretty strong engine",
    "start": "4173140",
    "end": "4181980"
  },
  {
    "text": "okay let's talk about computation because the caveat on that last slide is",
    "start": "4181980",
    "end": "4188318"
  },
  {
    "text": "yes more data will help if you have a big enough model and big models usually",
    "start": "4188319",
    "end": "4193600"
  },
  {
    "text": "mean lots of computation so what I haven't talked about is how big are",
    "start": "4193600",
    "end": "4199150"
  },
  {
    "text": "these neural networks and how big is one experiment so if you actually want to train one of these things at scale what",
    "start": "4199150",
    "end": "4204250"
  },
  {
    "text": "are you in for so here's the the back of the envelope it's going to take at least",
    "start": "4204250",
    "end": "4209520"
  },
  {
    "text": "the number of connections in your neural network so take one slice of that are n",
    "start": "4209520",
    "end": "4215050"
  },
  {
    "text": "n the number of unique connections multiplied by the number of frames once you unroll the recurrent network once",
    "start": "4215050",
    "end": "4221980"
  },
  {
    "text": "you unfold it multiplied by the number of utterances you've got a process in your data set times the number of training epochs the",
    "start": "4221980",
    "end": "4229240"
  },
  {
    "text": "number of times you loop through the data set times three because you have to do forward propagation to flops for",
    "start": "4229240",
    "end": "4239590"
  },
  {
    "text": "every connection because there's a multiplying and add so if you multiply this out for some parameters from the",
    "start": "4239590",
    "end": "4245610"
  },
  {
    "text": "the deep speech engine if I do you get something like 1.2 times 10 to the 19",
    "start": "4245610",
    "end": "4251560"
  },
  {
    "text": "flops so about 10 XO flops and if you run this on a Titan X card this will",
    "start": "4251560",
    "end": "4258040"
  },
  {
    "text": "take about a month now if you already know what the model is that might be",
    "start": "4258040",
    "end": "4263230"
  },
  {
    "text": "tolerable if you're you're on your epic run to get your best performance so far then this is okay but if you don't know",
    "start": "4263230",
    "end": "4270040"
  },
  {
    "text": "what model is going to work you're targeting some new scenario then you want it done now so you can try lots and",
    "start": "4270040",
    "end": "4275350"
  },
  {
    "text": "lots of models quickly so the easy fix is just to try using a bunch more GPUs",
    "start": "4275350",
    "end": "4281320"
  },
  {
    "text": "with data parallelism and the good news is is that so far it looks like speech",
    "start": "4281320",
    "end": "4287410"
  },
  {
    "text": "recognition allows us to use mini batch sizes we can process enough utterances in parallel that this is actually",
    "start": "4287410",
    "end": "4294100"
  },
  {
    "text": "efficient so you'd like to keep you know maybe a bit more than 64 utterances on each GPU",
    "start": "4294100",
    "end": "4300360"
  },
  {
    "text": "and up to a total mini batch size of like a thousand or maybe two thousand it's still useful and so if you've got",
    "start": "4300360",
    "end": "4307860"
  },
  {
    "text": "if you're putting together your your infrastructure you can go out and you can buy a server that'll fit eight of",
    "start": "4307860",
    "end": "4314340"
  },
  {
    "text": "these Titan GP using them and that'll actually get you to less than a week training time which is pretty",
    "start": "4314340",
    "end": "4319380"
  },
  {
    "text": "respectable so there are a whole bunch of ways to use GPUs if I do we've been",
    "start": "4319380",
    "end": "4324750"
  },
  {
    "text": "using synchronous SGD it turns out that you've got to optimize things like all",
    "start": "4324750",
    "end": "4330000"
  },
  {
    "text": "reduce code once you leave one node you have to start worrying about your network and if you want to keep scaling",
    "start": "4330000",
    "end": "4336650"
  },
  {
    "text": "than thinking about things like network traffic and the right strategy for moving all of your data becomes",
    "start": "4336650",
    "end": "4343469"
  },
  {
    "text": "important but we've had success scaling really well all the way out to things",
    "start": "4343469",
    "end": "4348750"
  },
  {
    "text": "like 64 GPUs and just getting linear speed ups all over the way so if you've",
    "start": "4348750",
    "end": "4354119"
  },
  {
    "text": "got a big cluster available these things scale really well and there are a bunch of other solutions for instance",
    "start": "4354119",
    "end": "4359960"
  },
  {
    "text": "asynchronous SGD is now kind of a mainstay of distributed deep learning there's also been some work recently of",
    "start": "4359960",
    "end": "4366360"
  },
  {
    "text": "trying to go back to synchronous SGD that has a lot of nice properties but using things like backup workers so",
    "start": "4366360",
    "end": "4373760"
  },
  {
    "text": "that's sort of the easy thing just throw more GPUs at it and go faster one word",
    "start": "4373760",
    "end": "4379590"
  },
  {
    "text": "of warning as you're trying to build these systems is to watch for code that",
    "start": "4379590",
    "end": "4386429"
  },
  {
    "text": "isn't as optimized as you expected it to be and so this back of the envelope",
    "start": "4386429",
    "end": "4391639"
  },
  {
    "text": "calculation that we did of figuring out how many flops are involved in our network and then calculating how long it",
    "start": "4391639",
    "end": "4398880"
  },
  {
    "text": "would take to run if our GPU are running at full efficiency you should actually",
    "start": "4398880",
    "end": "4404309"
  },
  {
    "text": "do this for your network this we call this the speed of light this is the fastest your code could ever run on one",
    "start": "4404309",
    "end": "4410280"
  },
  {
    "text": "GPU and if you find that you're just drastically underperforming that number",
    "start": "4410280",
    "end": "4415800"
  },
  {
    "text": "what could be happening to you is that you've hit a little edge case in one of",
    "start": "4415800",
    "end": "4420960"
  },
  {
    "text": "the libraries that you're using and you're actually suffering a huge setback that you don't need to be feeling right",
    "start": "4420960",
    "end": "4426030"
  },
  {
    "text": "now so one of the things we found back in November is that in libraries like Kublai's you can actually use mini batch",
    "start": "4426030",
    "end": "4432659"
  },
  {
    "text": "sizes hit these weird catastrophic cases in the library where you could be suffering",
    "start": "4432659",
    "end": "4438380"
  },
  {
    "text": "like a factor of two or three performance reduction so that might take your wonderful one-week training time",
    "start": "4438380",
    "end": "4445280"
  },
  {
    "text": "and blow it up to say a three week training time so that's why I wanted to",
    "start": "4445280",
    "end": "4450950"
  },
  {
    "text": "go through this and ask you to keep in mind while you're training these things try to figure out how long it ought to",
    "start": "4450950",
    "end": "4457430"
  },
  {
    "text": "be taking and if it's going a lot slower be suspicious that there's some code you could be optimizing another good trick",
    "start": "4457430",
    "end": "4464660"
  },
  {
    "text": "that's particularly speech you can also use this for other recurrent networks is",
    "start": "4464660",
    "end": "4470210"
  },
  {
    "text": "to try to keep similar length utterances together so if you look at your data set",
    "start": "4470210",
    "end": "4476960"
  },
  {
    "text": "like a lot of things you have this sort of distribution over possible utterance lengths and so you see there's a whole",
    "start": "4476960",
    "end": "4484490"
  },
  {
    "text": "bunch that are you know maybe within about 50% of each other but there's also a large number of utterances that are",
    "start": "4484490",
    "end": "4491000"
  },
  {
    "text": "very short and so what happens is when we want to process a whole bunch of",
    "start": "4491000",
    "end": "4496280"
  },
  {
    "text": "these uh pterence --is in parallel if we just randomly select say a thousand",
    "start": "4496280",
    "end": "4501740"
  },
  {
    "text": "utterances to go into a mini batch there's a high probability that we're going to get a whole bunch of these",
    "start": "4501740",
    "end": "4507350"
  },
  {
    "text": "little short utterances along with some really long uh pterence --is and in order to make all the ctc libraries work",
    "start": "4507350",
    "end": "4514070"
  },
  {
    "text": "and all of our recurrent Network computations easy what we have to do is pad these audio signals with zero and",
    "start": "4514070",
    "end": "4520040"
  },
  {
    "text": "that lines up meaning that we're wasting huge amounts of computation maybe a factor of two or more and so one way to",
    "start": "4520040",
    "end": "4527330"
  },
  {
    "text": "get around it is just sort all of your utterances by length and then try to",
    "start": "4527330",
    "end": "4532400"
  },
  {
    "text": "keep the mini-batches to be similar lengths so that you just don't end up with quite as much waste in each MIDI",
    "start": "4532400",
    "end": "4538610"
  },
  {
    "text": "batch and and this kind of modifies your your algorithm a little bit but in the",
    "start": "4538610",
    "end": "4543770"
  },
  {
    "text": "end is worthwhile all right this is kind of all I want to say about computation",
    "start": "4543770",
    "end": "4549860"
  },
  {
    "text": "if you're if you've got a few GPUs keep an eye on your running time so that you",
    "start": "4549860",
    "end": "4555410"
  },
  {
    "text": "know what to optimize and pay attention to the easy wins like keeping your utterances together you can actually",
    "start": "4555410",
    "end": "4560990"
  },
  {
    "text": "scale really well and I think for a lot of the jobs we see you can have your",
    "start": "4560990",
    "end": "4566929"
  },
  {
    "text": "your GPU running at something like 50% of the peak and that's all in with",
    "start": "4566929",
    "end": "4572209"
  },
  {
    "text": "network time with all the bandwidth bound stuff you can actually run a two to three teraflops on a GPU that can",
    "start": "4572209",
    "end": "4578630"
  },
  {
    "text": "only do five teraflops in the perfect case so what can you actually do with",
    "start": "4578630",
    "end": "4584570"
  },
  {
    "text": "this I one of my favorite results from one of our largest models is actually in",
    "start": "4584570",
    "end": "4589670"
  },
  {
    "text": "Mandarin so we have a whole bunch of labeled Mandarin data if I do and so one of the things that we did was we scaled",
    "start": "4589670",
    "end": "4596059"
  },
  {
    "text": "up this model trained it on a huge amount of Mandarin data and then as we always do we sit down and we do error",
    "start": "4596059",
    "end": "4602389"
  },
  {
    "text": "analysis and what we would do is have a whole bunch of humans sitting around try",
    "start": "4602389",
    "end": "4609979"
  },
  {
    "text": "to debate the transcriptions and figure out the ground truth that tend to be very high quality and then we go and",
    "start": "4609979",
    "end": "4615920"
  },
  {
    "text": "we'd run now a sort of holdout test on some new people and on the speech engine itself and so if you benchmark a single",
    "start": "4615920",
    "end": "4624139"
  },
  {
    "text": "human being against this deep speech engine in Mandarin that's powered by all",
    "start": "4624139",
    "end": "4629570"
  },
  {
    "text": "the technologies we were just talking about it turns out that the speech engine can get an error rate that's down",
    "start": "4629570",
    "end": "4636110"
  },
  {
    "text": "below six percent character error rate so only about six percent of the characters are wrong and a single human",
    "start": "4636110",
    "end": "4642289"
  },
  {
    "text": "sitting there listening to these transcriptions actually does quite a bit worse it's almost ten percent if you",
    "start": "4642289",
    "end": "4649579"
  },
  {
    "text": "give people a bit of an advantage which is you going to you now assemble a committee of people and you get them a",
    "start": "4649579",
    "end": "4657050"
  },
  {
    "text": "fresh test set so that no one has seen it before and we run this test again it turns out that the two engines are that",
    "start": "4657050",
    "end": "4664969"
  },
  {
    "text": "the two cases are actually really similar and you can end up with a committee of native Mandarin speakers sitting around debating no no I think",
    "start": "4664969",
    "end": "4671689"
  },
  {
    "text": "this person said this or no they have an accent it's from the north I think they're actually saying that and then",
    "start": "4671689",
    "end": "4677869"
  },
  {
    "text": "when you show them the deep speech transcription they actually go ah that that's what it was and so you can",
    "start": "4677869",
    "end": "4685039"
  },
  {
    "text": "actually get this technology up to a point where it's highly competitive with human beings even human beings working",
    "start": "4685039",
    "end": "4691369"
  },
  {
    "text": "together and this is sort of where I think all the speech recognition systems are heading thanks to deep learning and",
    "start": "4691369",
    "end": "4698119"
  },
  {
    "text": "the technologies that we're talking about here any questions so far",
    "start": "4698119",
    "end": "4704610"
  },
  {
    "text": "yeah go ahead yep sorry yeah so the",
    "start": "4704610",
    "end": "4714429"
  },
  {
    "text": "question is if humans have such a hard time coming up with the correct transcription how do you know what the truth is and the real answer is you",
    "start": "4714429",
    "end": "4721420"
  },
  {
    "text": "don't really sometimes you might have a little bit of user feedback but in this instance we have very high quality",
    "start": "4721420",
    "end": "4728580"
  },
  {
    "text": "transcriptions that are coming from many labelers teamed up with a speech engine and so that could be wrong we do",
    "start": "4728580",
    "end": "4736750"
  },
  {
    "text": "occasionally find errors where we just think that's a label error but when you have a committee of humans around the",
    "start": "4736750",
    "end": "4742929"
  },
  {
    "text": "the really astonishing thing is that you can look at the output of the speech engines and the humans will suddenly",
    "start": "4742929",
    "end": "4748179"
  },
  {
    "text": "jump ship and say oh no no no no this each engine is actually correct because",
    "start": "4748179",
    "end": "4753639"
  },
  {
    "text": "it'll often come up with an obscure word or place that they weren't aware of yeah",
    "start": "4753639",
    "end": "4762730"
  },
  {
    "text": "so so this is a you know an inherently ambiguous result but let's say that a",
    "start": "4762730",
    "end": "4768100"
  },
  {
    "text": "community of human beings tend to disagree with another committee of human beings about the same amount as a as a",
    "start": "4768100",
    "end": "4774340"
  },
  {
    "text": "speech engine does yeah yeah so this is",
    "start": "4774340",
    "end": "4790750"
  },
  {
    "text": "a so this is using the CTC cost right that's really the core component of this",
    "start": "4790750",
    "end": "4795790"
  },
  {
    "text": "system it's how you deal with mapping one variable length sequence to another and the CTC cost is not perfect it has",
    "start": "4795790",
    "end": "4803620"
  },
  {
    "text": "this assumption of Independence baked into the probabilistic model and because",
    "start": "4803620",
    "end": "4809110"
  },
  {
    "text": "of that assumption we're introducing some bias into the system and for languages like English where the",
    "start": "4809110",
    "end": "4815080"
  },
  {
    "text": "characters are obviously not independent of each other this might be a limitation in practice the thing that we see is",
    "start": "4815080",
    "end": "4822310"
  },
  {
    "text": "that as you add a lot of data and your model gets much more powerful you can still find your way around it but it",
    "start": "4822310",
    "end": "4828969"
  },
  {
    "text": "might take more data and a bigger model than necessary and of course we hope that all the new",
    "start": "4828969",
    "end": "4834040"
  },
  {
    "text": "state-of-the-art methods coming out of the deep learning community are going to give us an even better solution okay",
    "start": "4834040",
    "end": "4840929"
  },
  {
    "text": "right",
    "start": "4840929",
    "end": "4843929"
  },
  {
    "text": "empirically determined yeah so the question is for a spectrogram with we",
    "start": "4850630",
    "end": "4857239"
  },
  {
    "text": "talked about these little spectrogram frames being computed from 20 milliseconds of audio and is that number special is there a reason for it so this",
    "start": "4857239",
    "end": "4865760"
  },
  {
    "text": "is really determined from years and years of experience this is captured from the traditional speech community we",
    "start": "4865760",
    "end": "4872239"
  },
  {
    "text": "know this works pretty well there's actually some fun things you can do you can take a spectrogram go back and find",
    "start": "4872239",
    "end": "4879110"
  },
  {
    "text": "the best audio that corresponds to that spectrogram to listen to it and see if",
    "start": "4879110",
    "end": "4884150"
  },
  {
    "text": "you lost anything and spectrograms of about this level of quantization you can kind of tell what",
    "start": "4884150",
    "end": "4889400"
  },
  {
    "text": "people are saying it's a little bit garbled but it's still actually pretty good so amongst all the hyper parameters",
    "start": "4889400",
    "end": "4895969"
  },
  {
    "text": "you could choose this one's kind of a good trade-off in keeping the information but also saving a little bit",
    "start": "4895969",
    "end": "4901760"
  },
  {
    "text": "of the phase by doing it frequently yeah",
    "start": "4901760",
    "end": "4908079"
  },
  {
    "text": "I think in a lot of the models the in the demo for example we don't use",
    "start": "4908889",
    "end": "4914060"
  },
  {
    "text": "overlapping windows they're just adjacent yeah",
    "start": "4914060",
    "end": "4921400"
  },
  {
    "text": "yeah so those results are from from in-house software it Baidu if you use",
    "start": "4926570",
    "end": "4932010"
  },
  {
    "text": "something like open MPI for example on a cluster of GPUs actually works pretty",
    "start": "4932010",
    "end": "4937289"
  },
  {
    "text": "well on a bunch of machines but I think",
    "start": "4937289",
    "end": "4942719"
  },
  {
    "text": "some of the algorithms like all reduce once you start moving huge amounts of data they're not optimal you'll suffer a",
    "start": "4942719",
    "end": "4948959"
  },
  {
    "text": "hit once you start going to that many GPUs within a single box if you use the",
    "start": "4948959",
    "end": "4957360"
  },
  {
    "text": "CUDA libraries to move data back and forth just on a local box that stuff is pretty well optimized and you can often",
    "start": "4957360",
    "end": "4963780"
  },
  {
    "text": "do it yourself okay so I want to take a few more questions at the end and maybe we can run into the",
    "start": "4963780",
    "end": "4970949"
  },
  {
    "text": "break a little bit I wanted to just dive right through a few comments about production here so of course the",
    "start": "4970949",
    "end": "4981570"
  },
  {
    "text": "ultimate goal of solving speech recognition is to improve people's lives and enable exciting products and so that",
    "start": "4981570",
    "end": "4988349"
  },
  {
    "text": "means even though so far we've trained a bunch of acoustic and language models we",
    "start": "4988349",
    "end": "4993690"
  },
  {
    "text": "also want to get these things in production and users tend to care about more than just accuracy accuracy of",
    "start": "4993690",
    "end": "4999840"
  },
  {
    "text": "course matters a lot but we also care about things like latency users want to see the engine send them some feedback",
    "start": "4999840",
    "end": "5006260"
  },
  {
    "text": "very quickly so that they know that it's responding and that it's understanding what they're saying and we also need",
    "start": "5006260",
    "end": "5011959"
  },
  {
    "text": "this to be economical so that we can serve lots of users without breaking the bank so in practice a lot of the neural",
    "start": "5011959",
    "end": "5019010"
  },
  {
    "text": "networks that we use in research papers because they're awesome for beating benchmark results turn out not to work",
    "start": "5019010",
    "end": "5024409"
  },
  {
    "text": "that well on a production engine so one in particular that I think is worth",
    "start": "5024409",
    "end": "5030139"
  },
  {
    "text": "keeping an eye on is that it's really common to use bi-directional recurrent neural networks and so throughout the",
    "start": "5030139",
    "end": "5036920"
  },
  {
    "text": "talk I've been drawing my RNN with connections that just go forward in time but you'll see a lot of research results",
    "start": "5036920",
    "end": "5043070"
  },
  {
    "text": "that also have a pass that goes backward in time and this works fine if you just",
    "start": "5043070",
    "end": "5048349"
  },
  {
    "text": "want to process data offline but the problem is that if I want to compute",
    "start": "5048349",
    "end": "5053780"
  },
  {
    "text": "this neurons output up at the top of my network I have to wait until I see the entire audio segment so that I can compute this",
    "start": "5053780",
    "end": "5061100"
  },
  {
    "text": "backward recurrence and get this response so this sort of anti causal",
    "start": "5061100",
    "end": "5066350"
  },
  {
    "text": "part of my neural network that gets to see the future means that I can't respond to a user on the fly because I",
    "start": "5066350",
    "end": "5072860"
  },
  {
    "text": "need to wait for the end of their signal so if you start out with these",
    "start": "5072860",
    "end": "5079130"
  },
  {
    "text": "bi-directional rnns that are actually much easier to get working and then you jump to using a recurrent network that",
    "start": "5079130",
    "end": "5085460"
  },
  {
    "text": "is forward only it'll turn out that you're going to lose some accuracy and you might kind of hope that CTC because",
    "start": "5085460",
    "end": "5093170"
  },
  {
    "text": "it doesn't care about the alignment would somehow magically learn to shift the output over to get better accuracy",
    "start": "5093170",
    "end": "5099950"
  },
  {
    "text": "and just artificially delay the response so that it could get more context on its own but it kind of turns out to only do",
    "start": "5099950",
    "end": "5107300"
  },
  {
    "text": "that a little bit in practice it's really tough to control it and so if you find that you're doing much worse",
    "start": "5107300",
    "end": "5113480"
  },
  {
    "text": "sometimes you have to sort of engage in model engineering so even though I've been talking about these recurrent",
    "start": "5113480",
    "end": "5119300"
  },
  {
    "text": "networks I want you to bear in mind that there's this dual optimization going on",
    "start": "5119300",
    "end": "5124730"
  },
  {
    "text": "you want to find a model structure that gives you really good accuracy but you also have to think carefully about how",
    "start": "5124730",
    "end": "5130520"
  },
  {
    "text": "you set up the structure so that this little neuron at the top can actually see enough context to get an accurate",
    "start": "5130520",
    "end": "5137030"
  },
  {
    "text": "answer and and not depend too much on the future so for example what we could",
    "start": "5137030",
    "end": "5143480"
  },
  {
    "text": "do is tweak this model so that this neuron at the top that's trying to output the character L and hello can see",
    "start": "5143480",
    "end": "5151310"
  },
  {
    "text": "some future frames but it doesn't have this backward recurrence so it only gets to see a little bit of context that lets",
    "start": "5151310",
    "end": "5157880"
  },
  {
    "text": "us kind of contain the amount of latency in the model you skip over this so in",
    "start": "5157880",
    "end": "5166700"
  },
  {
    "text": "terms of other online aspects of course we want this to be efficient right we",
    "start": "5166700",
    "end": "5172430"
  },
  {
    "text": "want to serve lots of users on a small number of machines if possible and one",
    "start": "5172430",
    "end": "5178460"
  },
  {
    "text": "of the things you think you might find if you have a really big deep neural network or recurrent neural network is that it's really hard to deploy them on",
    "start": "5178460",
    "end": "5185260"
  },
  {
    "text": "conventional CPUs CPUs are awesome for or serial jobs you just want to go as",
    "start": "5185260",
    "end": "5191660"
  },
  {
    "text": "fast as you can for this one string of instructions but as we've discovered with so much of deep learning GPUs are",
    "start": "5191660",
    "end": "5199160"
  },
  {
    "text": "really fantastic because when we work with neural networks we love processing lots and lots of arithmetic in parallel",
    "start": "5199160",
    "end": "5205150"
  },
  {
    "text": "but it's really only efficient if the batch that we're working on the hunks of audio that we're working on are are in a",
    "start": "5205150",
    "end": "5215030"
  },
  {
    "text": "big enough batch so if we just process one stream of audio so that my GPU is multiplying matrices times vectors then",
    "start": "5215030",
    "end": "5222050"
  },
  {
    "text": "my GPU is going to be really inefficient so for example unlike a K 1200 GPU this",
    "start": "5222050",
    "end": "5229370"
  },
  {
    "text": "is something you could put in a server in the cloud what you'll find is that you get really poor throughput",
    "start": "5229370",
    "end": "5235210"
  },
  {
    "text": "considering the the dollar value of this Hardware if you're only processing one",
    "start": "5235210",
    "end": "5240620"
  },
  {
    "text": "piece of audio at a time whereas if you could somehow batch up audio to have say 10 or 32 streams going at once then you",
    "start": "5240620",
    "end": "5248660"
  },
  {
    "text": "can actually squeeze out a lot more more performance from that piece of hardware so one of the things that we've been",
    "start": "5248660",
    "end": "5255620"
  },
  {
    "text": "working on that works really well is not too too bad to implement is to just",
    "start": "5255620",
    "end": "5260840"
  },
  {
    "text": "batch all of the packets as data comes in so if I have a whole bunch of users talking to my server and they're sending",
    "start": "5260840",
    "end": "5267320"
  },
  {
    "text": "me little hundred millisecond packets of audio what I can do is I can sit and I",
    "start": "5267320",
    "end": "5272930"
  },
  {
    "text": "can listen to all these users and when I catch a whole batch of utterances coming in or a whole bunch of audio packets",
    "start": "5272930",
    "end": "5279440"
  },
  {
    "text": "coming in from different people that start around the same time I plug those all into my GPU and I process those",
    "start": "5279440",
    "end": "5286610"
  },
  {
    "text": "matrix multiplications together so instead of multiplying a matrix times only one little audio piece I get to",
    "start": "5286610",
    "end": "5292460"
  },
  {
    "text": "multiply it by a batch of say four audio pieces and it's much more efficient and",
    "start": "5292460",
    "end": "5297550"
  },
  {
    "text": "if you actually do this on a live server and you plow a whole bunch of audio",
    "start": "5297550",
    "end": "5302660"
  },
  {
    "text": "streams through it you could support maybe 10 20 30 users in parallel and as",
    "start": "5302660",
    "end": "5308060"
  },
  {
    "text": "the load on that server goes up I have more and more users piling on what happens is that the GPU will naturally",
    "start": "5308060",
    "end": "5314750"
  },
  {
    "text": "start batching up more and more packets into single matrix multiplications so as",
    "start": "5314750",
    "end": "5320330"
  },
  {
    "text": "you get more users you actually get much more efficient as well and so in",
    "start": "5320330",
    "end": "5326000"
  },
  {
    "text": "practice when you have a whole bunch of users on one machine you usually don't see matrix multiplications happening",
    "start": "5326000",
    "end": "5332270"
  },
  {
    "text": "with fewer than maybe a batch sizes of four so the summary of all of this is",
    "start": "5332270",
    "end": "5339200"
  },
  {
    "text": "that deep learning is really making the the first steps to building a",
    "start": "5339200",
    "end": "5344690"
  },
  {
    "text": "state-of-the-art speech engine easier than they've ever been so if you want to build a new state-of-the-art speech engine for some new language all the",
    "start": "5344690",
    "end": "5351530"
  },
  {
    "text": "components that you need are things that we've covered so far and the performance",
    "start": "5351530",
    "end": "5356570"
  },
  {
    "text": "now is really significantly driven by data and models and I think as we were discussing earlier I think future models",
    "start": "5356570",
    "end": "5362930"
  },
  {
    "text": "from deep learning are going to make that influence of data and computing power even stronger and of course data",
    "start": "5362930",
    "end": "5371090"
  },
  {
    "text": "and compute is important so that we can try lots and lots of models and keep making progress and I think this",
    "start": "5371090",
    "end": "5377330"
  },
  {
    "text": "technology is now at a stage where it's not just a research system anymore we're",
    "start": "5377330",
    "end": "5382490"
  },
  {
    "text": "seeing that the end end deep learning technologies are now mature enough that we can get them into productions I think",
    "start": "5382490",
    "end": "5388580"
  },
  {
    "text": "you guys are going to be seeing deep learning play a bigger bigger role in the speech engines that are powering all",
    "start": "5388580",
    "end": "5393770"
  },
  {
    "text": "the devices that we use so thank you very much",
    "start": "5393770",
    "end": "5398140"
  },
  {
    "text": "I think we're right at the end of time sounds good",
    "start": "5402210",
    "end": "5407790"
  },
  {
    "text": "alright we had one in the back who's waiting patiently go ahead more than one",
    "start": "5407790",
    "end": "5418230"
  },
  {
    "text": "voice simultaneously so the question is how does the engine handle more than one voice simultaneously so right now",
    "start": "5418230",
    "end": "5425010"
  },
  {
    "text": "there's nothing in this formalism that allows you to account for multiple speakers and so usually when you listen",
    "start": "5425010",
    "end": "5434460"
  },
  {
    "text": "to an audio clip in practice it's clear that there's one dominant speaker and so",
    "start": "5434460",
    "end": "5439980"
  },
  {
    "text": "this beach engine of course learns whatever it was taught from the labels and it will try to filter out background",
    "start": "5439980",
    "end": "5446070"
  },
  {
    "text": "speakers and just transcribe the dominant one but if it's really ambiguous then then undefined results",
    "start": "5446070",
    "end": "5454460"
  },
  {
    "text": "you customize the transcription to the specific characteristics of a particular",
    "start": "5454550",
    "end": "5461550"
  },
  {
    "text": "speaker so we're not doing that in these",
    "start": "5461550",
    "end": "5467010"
  },
  {
    "text": "pipelines right now but of course a lot of different strategies have been",
    "start": "5467010",
    "end": "5472710"
  },
  {
    "text": "developed in the traditional speech literature there are things like I've Ector 'z that try to quantify someone's",
    "start": "5472710",
    "end": "5477930"
  },
  {
    "text": "voice and those make useful features for improving speech engines you could also imagine taking a lot of the concepts",
    "start": "5477930",
    "end": "5485130"
  },
  {
    "text": "like embeddings for example and tossing them in here so I think a lot of that is left open to future work I do a question",
    "start": "5485130",
    "end": "5494340"
  },
  {
    "text": "button I think we have to break for time but I'll step off stage here and you",
    "start": "5494340",
    "end": "5500190"
  },
  {
    "text": "guys can come to me with your questions thank you so much",
    "start": "5500190",
    "end": "5503989"
  },
  {
    "text": "so we'll reconvene at 2:45 for presentation by Alex",
    "start": "5506910",
    "end": "5513989"
  }
]