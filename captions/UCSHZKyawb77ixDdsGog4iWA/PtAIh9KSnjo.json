[
  {
    "text": "so good morning everyone so I'm going to talk about uh",
    "start": "560",
    "end": "7759"
  },
  {
    "text": "some of the core methods in deep reinforcement learning um so the aim of this talk is",
    "start": "7759",
    "end": "14679"
  },
  {
    "text": "as follows um first I'll do a brief introduction to what deepl is and um",
    "start": "14679",
    "end": "20160"
  },
  {
    "text": "whether it might make sense to apply it in your problem um I'll talk about uh",
    "start": "20160",
    "end": "25400"
  },
  {
    "text": "some of the core uh techniques uh so they're on the one hand",
    "start": "25400",
    "end": "30439"
  },
  {
    "text": "we have the policy gradient methods uh then on the other hand we have uh methods that uh learn a q function",
    "start": "30439",
    "end": "37320"
  },
  {
    "text": "including Q learning and sarsa and um I'll talk a little at the",
    "start": "37320",
    "end": "42840"
  },
  {
    "text": "end about what are the pros and cons of these different methods so first what is reinforcement",
    "start": "42840",
    "end": "49760"
  },
  {
    "text": "learning um it's a branch of machine learning cons uh concerned with taking sequences of actions um",
    "start": "49760",
    "end": "56199"
  },
  {
    "text": "so um often uh it's described in in terms of an agent inter interacting with",
    "start": "56199",
    "end": "61800"
  },
  {
    "text": "the previously unknown environment um and it's trying to maximize some kind of cumulative reward some kind of reward",
    "start": "61800",
    "end": "68799"
  },
  {
    "text": "function that we've defined um accumulated over time and uh pretty much any kind of task where you have some",
    "start": "68799",
    "end": "75200"
  },
  {
    "text": "kind of goal that you want to achieve can be stated in these terms uh so this is an extremely General uh",
    "start": "75200",
    "end": "82840"
  },
  {
    "text": "formulation uh what's deep reinforcement learning it's pretty simple it's just uh reinforcement learning where you're",
    "start": "83720",
    "end": "89400"
  },
  {
    "text": "using uh neural networks uh as function approximators um so uh the interesting",
    "start": "89400",
    "end": "96320"
  },
  {
    "text": "thing about reinforcement learning and contrast to supervised learning is um it's actually not totally obvious what",
    "start": "96320",
    "end": "102320"
  },
  {
    "text": "you should use your neural network to approximate in reinforcement learning and there are different kinds of algorithms that approximate different",
    "start": "102320",
    "end": "108399"
  },
  {
    "text": "things so uh one choice is to use the neural network to approximate your policy which is uh how the agent chooses",
    "start": "108399",
    "end": "115159"
  },
  {
    "text": "its actions um another choice is to approximate the value functions which measure how good or bad uh different",
    "start": "115159",
    "end": "122079"
  },
  {
    "text": "states are or how or actions and um last you can use the um",
    "start": "122079",
    "end": "128239"
  },
  {
    "text": "you can try to learn a model of the system a Dynamics model uh which will make predictions about next States and",
    "start": "128239",
    "end": "135959"
  },
  {
    "text": "rewards okay so I'll now give a few examples of different um different",
    "start": "136640",
    "end": "141800"
  },
  {
    "text": "places where you might apply reinforcement learning and what the observations and uh actions would be uh",
    "start": "141800",
    "end": "148560"
  },
  {
    "text": "so one example is robotics um so here you could imagine a",
    "start": "148560",
    "end": "154239"
  },
  {
    "text": "robot where the observations are the camera images and the joint angles of the robot um the actions are the joint",
    "start": "154239",
    "end": "160440"
  },
  {
    "text": "torqus you're applying and um the reward is going to depend on what",
    "start": "160440",
    "end": "166319"
  },
  {
    "text": "you want the robot to do so so this is something we uh as the algorithm designer get to Define so uh the rewards",
    "start": "166319",
    "end": "174519"
  },
  {
    "text": "could be uh to stay balanced uh to navigate to some Target location or something more abstract like Serve and",
    "start": "174519",
    "end": "181280"
  },
  {
    "text": "Protect humans uh so reinforcement learning has also been used in a lot of um more",
    "start": "181280",
    "end": "189080"
  },
  {
    "text": "practical applications um well applications that have been practical in the past uh I think robotics will be",
    "start": "189080",
    "end": "195840"
  },
  {
    "text": "very practical in the future um but uh for example um one uh one area is um",
    "start": "195840",
    "end": "203879"
  },
  {
    "text": "Inventory management uh so this is just one example of how you could use reinforcement learning for a",
    "start": "203879",
    "end": "208920"
  },
  {
    "text": "decision-making problem uh so you you have to decide how much to stock up on uh of every item and uh your",
    "start": "208920",
    "end": "217439"
  },
  {
    "text": "observations would be your current inventory levels um actions would be how much of each item you're going to",
    "start": "217439",
    "end": "222920"
  },
  {
    "text": "purchase and uh reward is your profit uh so people in operations",
    "start": "222920",
    "end": "229200"
  },
  {
    "text": "research this is uh this is a subfield um study this kind of problem a lot",
    "start": "229200",
    "end": "237040"
  },
  {
    "text": "um okay there are also a lot of uh machine learning problems where people have started to apply reinforcement",
    "start": "237040",
    "end": "243040"
  },
  {
    "text": "learning techniques so uh one example is um",
    "start": "243040",
    "end": "248720"
  },
  {
    "text": "attention um so the idea in attention is you don't want to look at the whole input at once uh you want to just focus",
    "start": "248720",
    "end": "254239"
  },
  {
    "text": "on part of it uh so uh one example of this is um with a large image you might",
    "start": "254239",
    "end": "259680"
  },
  {
    "text": "want to just crop out part of it and uh use that and just do detection on that part of the image um so uh here your",
    "start": "259680",
    "end": "267800"
  },
  {
    "text": "observation would be your current image window action is where to look or where to crop your image um and uh reward is",
    "start": "267800",
    "end": "275520"
  },
  {
    "text": "um your whether you make a classification error or not so here the um the actions are",
    "start": "275520",
    "end": "282080"
  },
  {
    "text": "trying to um here you have to um try to choose the right area of the image to look at so you'll do the correct",
    "start": "282080",
    "end": "289240"
  },
  {
    "text": "classification um reinforcement learning has also been used in um structured",
    "start": "289240",
    "end": "294360"
  },
  {
    "text": "prediction problems um which haven't uh which in the past",
    "start": "294360",
    "end": "300199"
  },
  {
    "text": "weren't considered to be reinforcement learning problems uh but it turns out that um like to actually properly solve",
    "start": "300199",
    "end": "307479"
  },
  {
    "text": "them it it actually is a reinforcement learning problem uh so machine translation for example",
    "start": "307479",
    "end": "314639"
  },
  {
    "text": "um uh you so you get a sour a sentence in the source language and you have to emit a sentence in the target language",
    "start": "314639",
    "end": "321720"
  },
  {
    "text": "um and uh you can uh here your observations are the sentence in the source language you're emitting one word",
    "start": "321720",
    "end": "328360"
  },
  {
    "text": "at a time in the Target l language and uh you have some reward function that looks at the whole sentence and tells",
    "start": "328360",
    "end": "334680"
  },
  {
    "text": "you how good your translation was um so since this is non-differentiable and it's um you yeah you can't just uh like",
    "start": "334680",
    "end": "342000"
  },
  {
    "text": "differentiate through the whole thing and do gradiant to sent so um it turns out you have to do um you can use a",
    "start": "342000",
    "end": "347720"
  },
  {
    "text": "policy gradient method to optimize your translation system um so people have started to do",
    "start": "347720",
    "end": "354800"
  },
  {
    "text": "that okay so that's just those are just a few examples um not exhaustive at all",
    "start": "354800",
    "end": "361160"
  },
  {
    "text": "um but uh I just want to uh since I just want to say a little bit about how",
    "start": "361160",
    "end": "366479"
  },
  {
    "text": "reinforcement learning fits into um the um fits into the picture of all the",
    "start": "366479",
    "end": "372440"
  },
  {
    "text": "other um types of machine learning problems so previous um I mean previous",
    "start": "372440",
    "end": "379120"
  },
  {
    "text": "uh courses in this uh series have talked about uh supervised learning and un supervised learning so how does uh",
    "start": "379120",
    "end": "385280"
  },
  {
    "text": "reinforcement learning relate to them how is it different um so let's just uh",
    "start": "385280",
    "end": "390599"
  },
  {
    "text": "first compare it to let's look at supervised learning so in supervised learning first um the environment",
    "start": "390599",
    "end": "396560"
  },
  {
    "text": "samples an input output pair from some distribution row um the agent makes a prediction um",
    "start": "396560",
    "end": "404720"
  },
  {
    "text": "why hat using its function f and uh it",
    "start": "404720",
    "end": "409880"
  },
  {
    "text": "receives some loss which tells it if it made the right prediction or the wrong prediction um so the interpretation is",
    "start": "409880",
    "end": "416800"
  },
  {
    "text": "um environment asks the agent a question and then tells the right answer um so contextual Bandits are um",
    "start": "416800",
    "end": "425800"
  },
  {
    "text": "make this problem a little harder in that they give um The Learning agent a little bit less information um so now",
    "start": "425800",
    "end": "432479"
  },
  {
    "text": "the environment samples an input um but notice that there's not a correct output associated with it um then the agent",
    "start": "432479",
    "end": "439599"
  },
  {
    "text": "takes an action and uh the agent receives some cost which is from um some probability",
    "start": "439599",
    "end": "447919"
  },
  {
    "text": "distribution so here um C is the cost we're sampling it from some probability distribution um and the agent doesn't",
    "start": "447919",
    "end": "454840"
  },
  {
    "text": "know what this probability distribution is so that's what makes the problem hard um so environment asks the agent a",
    "start": "454840",
    "end": "462160"
  },
  {
    "text": "question and uh the agent answers and the environment gives her a noisy score on the",
    "start": "462160",
    "end": "468800"
  },
  {
    "text": "answer um so this is applied um this actually has a lot of applications so",
    "start": "469479",
    "end": "474720"
  },
  {
    "text": "personalized recommendations is one big one along with advertising so um you have to besides um like uh customers who",
    "start": "474720",
    "end": "482159"
  },
  {
    "text": "liked this I mean you for you have a customer and you know what they liked in the past so you have to make a",
    "start": "482159",
    "end": "487639"
  },
  {
    "text": "prediction about what they're going to like in the future uh so you show them appropriate ads or links like what",
    "start": "487639",
    "end": "494039"
  },
  {
    "text": "either like what ad what book you want to try to advertise to them or what video you want to show them and so",
    "start": "494039",
    "end": "500919"
  },
  {
    "text": "on um so here you can the big difference between this and the supervised learning setting is you don't have access to the",
    "start": "500919",
    "end": "507639"
  },
  {
    "text": "function uh the Lost function trying to optimize so in particular you can't differentiate through it um we don't",
    "start": "507639",
    "end": "513279"
  },
  {
    "text": "know the process that generates C so we can't um compute the grading of the loss function and use that to tune the",
    "start": "513279",
    "end": "519919"
  },
  {
    "text": "agent's parameters so that makes it uh so that makes the problem a bit harder",
    "start": "519919",
    "end": "525680"
  },
  {
    "text": "or you you have to use a different kind of algorithm um lastly uh reinforcement",
    "start": "525680",
    "end": "531160"
  },
  {
    "text": "learning is um almost the same as the contextual Bandit setting except now the environment is",
    "start": "531160",
    "end": "536880"
  },
  {
    "text": "stateful so now instead of sampling um the initial state from scratch every",
    "start": "536880",
    "end": "542399"
  },
  {
    "text": "time step uh from the same distribution um the um State evolves over time uh so",
    "start": "542399",
    "end": "549519"
  },
  {
    "text": "you have some transition probability distribution called P here where um the the state X subt is uh conditioned on",
    "start": "549519",
    "end": "557160"
  },
  {
    "text": "the previous state and the previous action and uh that makes the problem",
    "start": "557160",
    "end": "563720"
  },
  {
    "text": "quite a bit harder because now well for a number of reasons uh for one thing the inputs you're getting depend on what",
    "start": "563720",
    "end": "570279"
  },
  {
    "text": "actions you're taking so now that makes it harder to develop a stable reliable",
    "start": "570279",
    "end": "575360"
  },
  {
    "text": "algorithm because now as the agent starts to learn it gets different inputs so that can lead to all sorts of um",
    "start": "575360",
    "end": "582760"
  },
  {
    "text": "outof control um behavior and it also means you have delayed effects because uh since the",
    "start": "582760",
    "end": "589519"
  },
  {
    "text": "system is stateful um uh you might need to take a lot of actions to get into the right state so um you might need to um",
    "start": "589519",
    "end": "598640"
  },
  {
    "text": "you can't just greedily every time step you have to uh you have to think ahead",
    "start": "598640",
    "end": "605240"
  },
  {
    "text": "effectively okay so just to summarize these differences there are two differences the first one is you don't",
    "start": "606360",
    "end": "612720"
  },
  {
    "text": "have full analytic access to the function you're trying to optimize you have to query it through",
    "start": "612720",
    "end": "618440"
  },
  {
    "text": "interaction uh second uh you're interacting with a stateful world which means that the input you get is going to",
    "start": "618440",
    "end": "624279"
  },
  {
    "text": "depend on your previous actions and if you just take the first of those differences uh between supervised",
    "start": "624279",
    "end": "630720"
  },
  {
    "text": "learning and reinforcement learning you get the contextual Bandit setting so that's sort of halfway in",
    "start": "630720",
    "end": "636959"
  },
  {
    "text": "between okay so uh I realized that there uh multiple this audience probably has",
    "start": "637000",
    "end": "642920"
  },
  {
    "text": "people with different interests uh some people are um doing research and want to know about what's the latest in the",
    "start": "642920",
    "end": "649240"
  },
  {
    "text": "research world and some people are um want to apply these machine learning techniques to practical applications um",
    "start": "649240",
    "end": "656399"
  },
  {
    "text": "so this slide is um for the latter group of people um so if you're wondering um",
    "start": "656399",
    "end": "662839"
  },
  {
    "text": "if you have some problem where you think reinforcement learning might be relevant and you're wondering if you should apply",
    "start": "662839",
    "end": "668000"
  },
  {
    "text": "reinforcement learning um so first uh I should say that the answer might be no",
    "start": "668000",
    "end": "674800"
  },
  {
    "text": "it might be Overkill especially uh deep reinforcement learning so this is a set of fairly new techniques where it's not",
    "start": "674800",
    "end": "681079"
  },
  {
    "text": "going to work out of the box very well um and uh it's these techniques aren't",
    "start": "681079",
    "end": "687040"
  },
  {
    "text": "that well established so they require a lot of they have a lot of knobs to be tuned so uh it might be Overkill and",
    "start": "687040",
    "end": "693760"
  },
  {
    "text": "yeah these techniques aren't that well established at the moment so it might be worth investigating some other methods",
    "start": "693760",
    "end": "700399"
  },
  {
    "text": "first um so one one so if your problem has a small number of parameters you're",
    "start": "700399",
    "end": "707000"
  },
  {
    "text": "trying to optimize over and um you have a simulator that you can uh like just do",
    "start": "707000",
    "end": "712880"
  },
  {
    "text": "lots of experiments on um then derivative free optimization methods are",
    "start": "712880",
    "end": "718519"
  },
  {
    "text": "likely to be better than reinforcement learning or they're likely to be easier to get working um so these methods just",
    "start": "718519",
    "end": "725639"
  },
  {
    "text": "uh look at um they just you give them a blackbox function where you put in a parameter vector and it'll give you a",
    "start": "725639",
    "end": "731639"
  },
  {
    "text": "noisy estimate of the score and these algorithms will just optimize uh over",
    "start": "731639",
    "end": "737199"
  },
  {
    "text": "the parameters of that blackbox I mean that are being put into that black box um so uh yeah there's a variety of",
    "start": "737199",
    "end": "744639"
  },
  {
    "text": "different methods um for derivative free optimization but these are easier to understand than reinforcement learning",
    "start": "744639",
    "end": "750320"
  },
  {
    "text": "and they do kind of work out of the box um okay a lot of problems are",
    "start": "750320",
    "end": "757160"
  },
  {
    "text": "actually um can be seen as cont contextual banded problems and the statefulness Of The World Isn't that",
    "start": "757160",
    "end": "763199"
  },
  {
    "text": "relevant um so for example in advertising um this is where people",
    "start": "763199",
    "end": "768760"
  },
  {
    "text": "people look at advertising as a contextual Bandit problem most of the time because you decide what ad to",
    "start": "768760",
    "end": "773920"
  },
  {
    "text": "present the user with and then they either um click on it or they don't um",
    "start": "773920",
    "end": "780079"
  },
  {
    "text": "but it's really um the user is kind of stateful because if you show them a terrible ad uh they might just go and",
    "start": "780079",
    "end": "785800"
  },
  {
    "text": "download ad block uh so uh there is like your actions do have some repercussions",
    "start": "785800",
    "end": "792160"
  },
  {
    "text": "um but um often you can just approximate it as being a contextual Bandit problem where there is no state so uh there's a",
    "start": "792160",
    "end": "800079"
  },
  {
    "text": "better theoretical understanding of contextual Bandit problems uh and methods that are that have some",
    "start": "800079",
    "end": "805680"
  },
  {
    "text": "guarantees so in that case um so if it is a contextual banit problem you might",
    "start": "805680",
    "end": "810959"
  },
  {
    "text": "want to use those kind of algorithms instead um and lastly um the um",
    "start": "810959",
    "end": "816760"
  },
  {
    "text": "operations research field has been uh using um these methods for a while on",
    "start": "816760",
    "end": "822959"
  },
  {
    "text": "real problems and um they have a set of methods um which are um just pretty much",
    "start": "822959",
    "end": "829639"
  },
  {
    "text": "the basic algorithms uh policy iteration and value iteration but they're um sort",
    "start": "829639",
    "end": "835160"
  },
  {
    "text": "of well um they're welldeveloped ways of doing feature engineering for these problems that end up working pretty",
    "start": "835160",
    "end": "840959"
  },
  {
    "text": "decently so these uh techniques are also worth considering instead of trying to throw a big neural network at",
    "start": "840959",
    "end": "848480"
  },
  {
    "text": "it okay so now well now that I've talked about what why not to use deep reinforcement learning or what it's not",
    "start": "849920",
    "end": "856440"
  },
  {
    "text": "good for um I'll just talk about um some recent uh success stories in deep reinforcement learning which are",
    "start": "856440",
    "end": "863360"
  },
  {
    "text": "achievements that probably wouldn't have been possible using these other techniques um so um a a few years ago",
    "start": "863360",
    "end": "870759"
  },
  {
    "text": "there is a pretty um influential result um by uh Min all from Deep Mind uh where",
    "start": "870759",
    "end": "877560"
  },
  {
    "text": "they used um a deep Q learning algorithm um to play Atari games using the screen",
    "start": "877560",
    "end": "883399"
  },
  {
    "text": "images as input um and uh that's hard because you have",
    "start": "883399",
    "end": "889160"
  },
  {
    "text": "these G these games are you're trying to do different things in all these games and there're some of them are kind of complicated so it's pretty remarkable",
    "start": "889160",
    "end": "895880"
  },
  {
    "text": "that you can just use a simple uh that a simple algorithm can solve them all um",
    "start": "895880",
    "end": "901560"
  },
  {
    "text": "this the same algorithm can solve them all uh so since then people have also um",
    "start": "901560",
    "end": "906839"
  },
  {
    "text": "solved or or solv this domain using uh policy gradients and another algorithm",
    "start": "906839",
    "end": "912680"
  },
  {
    "text": "called dagger um so another big uh groundbreaking result was um beating a",
    "start": "912680",
    "end": "920759"
  },
  {
    "text": "um a champion level player at go um also by Deep Mind um using a combination of",
    "start": "920759",
    "end": "927839"
  },
  {
    "text": "um super learning from uh like from expert games plus policy gradients to",
    "start": "927839",
    "end": "934319"
  },
  {
    "text": "fine-tune the supervised learning policy um plus Monte Carlo tree search um plus",
    "start": "934319",
    "end": "941279"
  },
  {
    "text": "value functions to make the search work better so a combination of techniques and reinforcement",
    "start": "941279",
    "end": "948480"
  },
  {
    "text": "learning um robotic so some of my colleagues at uh Berkeley had some um",
    "start": "948560",
    "end": "953920"
  },
  {
    "text": "very nice results uh learning in real time how to do manipulation tasks um",
    "start": "953920",
    "end": "959000"
  },
  {
    "text": "using an algorithm called guided policy search um using the PR2",
    "start": "959000",
    "end": "965480"
  },
  {
    "text": "robot um and uh some of my colleagues and I have um been working on robotic",
    "start": "965480",
    "end": "970560"
  },
  {
    "text": "Locomotion um using um policy gradient",
    "start": "970560",
    "end": "975600"
  },
  {
    "text": "methods and uh people have been working on Locomotion for a while and have been",
    "start": "975920",
    "end": "980959"
  },
  {
    "text": "able to achieve pretty good results uh using uh very like highly engineered domain specific methods but um",
    "start": "980959",
    "end": "988319"
  },
  {
    "text": "previously there hadn't been much success using general methods to solve",
    "start": "988319",
    "end": "993480"
  },
  {
    "text": "it and last uh there have been some recent results um playing 3D games using",
    "start": "993480",
    "end": "998880"
  },
  {
    "text": "policy gradients um in fact there was even a contest I heard about a couple days ago with this new visz Doom uh task",
    "start": "998880",
    "end": "1006639"
  },
  {
    "text": "which um is pretty nice so you might want to check out viz",
    "start": "1006639",
    "end": "1012720"
  },
  {
    "text": "Doom okay so that's that's it for the highle overview uh part of this um now",
    "start": "1012720",
    "end": "1018920"
  },
  {
    "text": "I'm going to start getting into the actual formalism and the technical",
    "start": "1018920",
    "end": "1024480"
  },
  {
    "text": "details okay so the basic object uh in uh the field of reinforcement learning",
    "start": "1025600",
    "end": "1032640"
  },
  {
    "text": "is the markof decision process um so the markof decision process is defined by",
    "start": "1032640",
    "end": "1038640"
  },
  {
    "text": "the following components you have a state space this is all the different states of the system uh the action space",
    "start": "1038640",
    "end": "1046000"
  },
  {
    "text": "these are all the actions the agent can take and you have um this probability",
    "start": "1046000",
    "end": "1051440"
  },
  {
    "text": "distribution um which uh which determines the probability of next date and reward so R is the reward S Prime is",
    "start": "1051440",
    "end": "1058840"
  },
  {
    "text": "the next state s and a are the actions so it's a conditional probability distribution sometime sometimes people",
    "start": "1058840",
    "end": "1065039"
  },
  {
    "text": "split this out into a separate reward function but that's basically an equivalent",
    "start": "1065039",
    "end": "1070840"
  },
  {
    "text": "formulation okay and sometimes there's some extra objects to find um will",
    "start": "1070840",
    "end": "1077760"
  },
  {
    "text": "will'll be interested in the we'll we'll consider an an initial State distribution so this is um the world",
    "start": "1077760",
    "end": "1083360"
  },
  {
    "text": "starts out in a certain State and uh the typical optimization",
    "start": "1083360",
    "end": "1088760"
  },
  {
    "text": "problem you want to solve given this mdp is to maximize expected cumulative reward though there are various um ways",
    "start": "1088760",
    "end": "1096400"
  },
  {
    "text": "of defining that more precisely which I'll go into uh",
    "start": "1096400",
    "end": "1101960"
  },
  {
    "text": "later okay so there are various different settings of reinforcement learning um where you define a slightly",
    "start": "1102840",
    "end": "1109799"
  },
  {
    "text": "different optimization problem the one we'll be most concerned with is called the episodic setting so here the agents",
    "start": "1109799",
    "end": "1117039"
  },
  {
    "text": "experience is split up into a um a series of episodes which have um finite",
    "start": "1117039",
    "end": "1123080"
  },
  {
    "text": "length so in each episode uh we first sample the initial",
    "start": "1123080",
    "end": "1128120"
  },
  {
    "text": "state of the world from some probability distribution me and then um the agent uh",
    "start": "1128120",
    "end": "1133799"
  },
  {
    "text": "keeps on acting until um the world ends up in some terminal state",
    "start": "1133799",
    "end": "1140240"
  },
  {
    "text": "um so just to give a example of what terminal States might be like and how an",
    "start": "1140240",
    "end": "1145400"
  },
  {
    "text": "epis episodic um reinforcement learning problem might look um so one example is",
    "start": "1145400",
    "end": "1151480"
  },
  {
    "text": "um when termination is good and you want to terminate the episode as fast as possible uh so if we imagine setting up",
    "start": "1151480",
    "end": "1158440"
  },
  {
    "text": "a task with some kind of Taxi robot that should get to the destination as fast as possible then the episode would be like",
    "start": "1158440",
    "end": "1164919"
  },
  {
    "text": "one trip and uh it's terminate it's trying to terminate the episode as fast as",
    "start": "1164919",
    "end": "1171559"
  },
  {
    "text": "possible um another example is um a waiter robot um where you have a fixed",
    "start": "1171559",
    "end": "1178200"
  },
  {
    "text": "length shift but the waiter has to accumulate it has to do as well as possible during that shift so there the",
    "start": "1178200",
    "end": "1183600"
  },
  {
    "text": "episode has a fixed length um the waiter has to say maximize tips or uh customer",
    "start": "1183600",
    "end": "1191280"
  },
  {
    "text": "happiness um and then you could imagine another kind of task where uh termination is bad and you want the",
    "start": "1191280",
    "end": "1197280"
  },
  {
    "text": "episode to last as long as possible um so you can view life as an example of",
    "start": "1197280",
    "end": "1202440"
  },
  {
    "text": "that um but also you could imagine having a a walking robot um where uh you",
    "start": "1202440",
    "end": "1209320"
  },
  {
    "text": "want it to walk as far as possible before it falls",
    "start": "1209320",
    "end": "1214120"
  },
  {
    "text": "over and in this setting it's pretty easy to find to Define what the goal is",
    "start": "1214799",
    "end": "1220080"
  },
  {
    "text": "um to we just want to maximize the expectation of the total reward per episode",
    "start": "1220080",
    "end": "1227919"
  },
  {
    "text": "okay and the last object we're going to introduce here is um a policy so the",
    "start": "1229280",
    "end": "1234480"
  },
  {
    "text": "policy is just the function that the agent uses to choose its actions so we have deterministic",
    "start": "1234480",
    "end": "1241520"
  },
  {
    "text": "policies which are just uh the policy is denoted by pi so we have the action is",
    "start": "1241520",
    "end": "1246799"
  },
  {
    "text": "just some function of the state and uh we also have U stochastic policies where",
    "start": "1246799",
    "end": "1252080"
  },
  {
    "text": "the policy is a conditional probability distribution um so here is just we're",
    "start": "1252080",
    "end": "1258840"
  },
  {
    "text": "just going to make a little bit more precise um the setting of The episodic mdp um so first we sample the initial",
    "start": "1258840",
    "end": "1266559"
  },
  {
    "text": "state from this distribution me um then we um then we get uh we sample the first",
    "start": "1266559",
    "end": "1273799"
  },
  {
    "text": "action from the policy a Zer from the policy then we sample next state and reward uh from the transition",
    "start": "1273799",
    "end": "1279679"
  },
  {
    "text": "probability distribution and so on until we reach a terminal State s subt and",
    "start": "1279679",
    "end": "1285120"
  },
  {
    "text": "then um the quantity we care about is the sum of all these rewards r0 plus R1",
    "start": "1285120",
    "end": "1291080"
  },
  {
    "text": "dot dot dot plus r subt minus one and um we want to maximize yeah so Ada is Ada",
    "start": "1291080",
    "end": "1297520"
  },
  {
    "text": "of Pi is just defined as the um expected total reward of the policy",
    "start": "1297520",
    "end": "1303960"
  },
  {
    "text": "Pi here's a picture that um illustrates exactly the same thing so you can look at it as a graphical",
    "start": "1304880",
    "end": "1312720"
  },
  {
    "text": "model okay and lastly um in the policy gradient section in particular we're",
    "start": "1314960",
    "end": "1320480"
  },
  {
    "text": "going to be interested in parameterized policies so here we have a parameter Vector um Theta which specifies U which",
    "start": "1320480",
    "end": "1328840"
  },
  {
    "text": "specifies exactly what the policy is so um for example the family of policies could be just a neural n you have a",
    "start": "1328840",
    "end": "1335960"
  },
  {
    "text": "certain neural network architecture and Theta specifies all the weights of this neural",
    "start": "1335960",
    "end": "1342639"
  },
  {
    "text": "network so we could have a a deterministic policy of course or stochastic policy",
    "start": "1343039",
    "end": "1350000"
  },
  {
    "text": "um and if you're wondering like concretely what would a policy look like I mean how do you use a neural network",
    "start": "1350000",
    "end": "1356320"
  },
  {
    "text": "to represent your policy it's actually exactly you do exactly the same thing you would do if this were a",
    "start": "1356320",
    "end": "1361960"
  },
  {
    "text": "classification or a regression problem uh so uh in so s here the state here is",
    "start": "1361960",
    "end": "1368320"
  },
  {
    "text": "your input and the action is your output um so um if you have a discrete action",
    "start": "1368320",
    "end": "1374520"
  },
  {
    "text": "space a discret set of actions um then um you would use a Network that outputs a vector of probabilities the",
    "start": "1374520",
    "end": "1381400"
  },
  {
    "text": "probabilities of the different actions this is exactly like a classifier and if you have a continuous",
    "start": "1381400",
    "end": "1388159"
  },
  {
    "text": "action space um you you would have your neural network output the mean and uh the diagonal of a covariance matrix of a",
    "start": "1388159",
    "end": "1395279"
  },
  {
    "text": "gaussian distribution um so this is just like you're doing regression so you can use the same kind of architectures You'",
    "start": "1395279",
    "end": "1401760"
  },
  {
    "text": "use in supervis learning okay so that's uh that's just",
    "start": "1401760",
    "end": "1409400"
  },
  {
    "text": "the that's it for the formalism of mdps so now I'm going to go into policy",
    "start": "1409400",
    "end": "1414840"
  },
  {
    "text": "gradient methods which are one uh Broad and general class of reinforcement",
    "start": "1414840",
    "end": "1419919"
  },
  {
    "text": "learning methods which are um quite effective so to give a brief overview of",
    "start": "1419919",
    "end": "1427679"
  },
  {
    "text": "this um here's here's the intuition of what policy grading methods are going to do",
    "start": "1427679",
    "end": "1434559"
  },
  {
    "text": "um so here capital R means the sum of rewards of the whole episode episode um",
    "start": "1434559",
    "end": "1439760"
  },
  {
    "text": "so our optimization problem is we want to maximize the expectation of the total reward um given our parameterized policy",
    "start": "1439760",
    "end": "1447760"
  },
  {
    "text": "Pi sub Theta and um the intuition of how our algorithm is going to work is um we",
    "start": "1447760",
    "end": "1455120"
  },
  {
    "text": "we're going to collect a bunch of trajectories I mean this is just run a bunch of episodes using our policy and",
    "start": "1455120",
    "end": "1461200"
  },
  {
    "text": "then we want to make the good trajectories more probable so I mean some of the trajectories were lucky and",
    "start": "1461200",
    "end": "1466919"
  },
  {
    "text": "they were really good some of them the agent was unlucky and they were bad and um The Good the ones that were good",
    "start": "1466919",
    "end": "1473320"
  },
  {
    "text": "meaning there was high reward um that means the agent probably took good actions there so we want to uh increase",
    "start": "1473320",
    "end": "1479880"
  },
  {
    "text": "the probability of the actions from those trajectories so um so the most basic version of uh policy gradient",
    "start": "1479880",
    "end": "1486720"
  },
  {
    "text": "methods just try to make the good trajectories more probable without trying to figure out which were the good actions and which were the bad actions",
    "start": "1486720",
    "end": "1493840"
  },
  {
    "text": "um slightly better methods or more um elaborate methods uh TR to figure out",
    "start": "1493840",
    "end": "1499039"
  },
  {
    "text": "which actions were good and which ones were bad and then they try to make the good actions more",
    "start": "1499039",
    "end": "1504720"
  },
  {
    "text": "probable and um lastly there's another class of methods which um which actually",
    "start": "1504720",
    "end": "1509960"
  },
  {
    "text": "try to push the actions towards better actions so they differentiate the loss",
    "start": "1509960",
    "end": "1515039"
  },
  {
    "text": "function with respect to the actions and they try to push the actions to better actions um so we're mostly going to talk",
    "start": "1515039",
    "end": "1520960"
  },
  {
    "text": "about one and two",
    "start": "1520960",
    "end": "1523840"
  },
  {
    "text": "here oh there's a question",
    "start": "1526559",
    "end": "1530320"
  },
  {
    "text": "oh uh yeah good question so um well we're maximizing over the policy we're",
    "start": "1532760",
    "end": "1538960"
  },
  {
    "text": "trying to find uh the best policy but here um the policy is assumed to be parameterized so there's some parameter",
    "start": "1538960",
    "end": "1545840"
  },
  {
    "text": "Vector Theta that specifies the policy and now we just want to maximize with respect to",
    "start": "1545840",
    "end": "1551880"
  },
  {
    "text": "Theta any other questions okay",
    "start": "1551880",
    "end": "1558760"
  },
  {
    "text": "um so there's a very um a very fundamental fundamental concept which is",
    "start": "1558760",
    "end": "1564120"
  },
  {
    "text": "called the score function grading estimator uh which um underlies policy gradient methods so actually to",
    "start": "1564120",
    "end": "1571360"
  },
  {
    "text": "introduce this we're not going to talk about policies and RL at all we're just going to assume uh we have some",
    "start": "1571360",
    "end": "1577279"
  },
  {
    "text": "expectation we have expectation of f ofx where X is sampled from some uh",
    "start": "1577279",
    "end": "1583480"
  },
  {
    "text": "parameterized probability distribution so we want to compute uh",
    "start": "1583480",
    "end": "1588559"
  },
  {
    "text": "the gring of this expectation with respect to Theta um so there's a general formula um that'll do this and the way",
    "start": "1588559",
    "end": "1595679"
  },
  {
    "text": "you derive it is you just write the expectation as an integral um and then you just um move some things around uh",
    "start": "1595679",
    "end": "1603520"
  },
  {
    "text": "you you swap the integral with the derivative and you um you turn it back into an expectation and uh what you get",
    "start": "1603520",
    "end": "1610720"
  },
  {
    "text": "at the end is this bottom line which says that you take the expectation of function value times grad log",
    "start": "1610720",
    "end": "1617440"
  },
  {
    "text": "probability uh so the in this is an unbiased estimator of the",
    "start": "1617440",
    "end": "1623520"
  },
  {
    "text": "gradient meaning if we get enough samples it'll Converge on the right thing um so uh the way you can compute",
    "start": "1623520",
    "end": "1630720"
  },
  {
    "text": "this estimator meaning the way you can get a noisy estimate of the grading of the expectation is you um just collect",
    "start": "1630720",
    "end": "1638520"
  },
  {
    "text": "one you just get one sample um from this distribution and then you compute then",
    "start": "1638520",
    "end": "1644000"
  },
  {
    "text": "you multiply F ofx times grad log probability um so uh the only requirement for being",
    "start": "1644000",
    "end": "1652559"
  },
  {
    "text": "able to use this estimator is uh we need to be able to compute the probability density I mean we need to be able to an",
    "start": "1652559",
    "end": "1659279"
  },
  {
    "text": "analytically compute it and we need to be able to differentiate it with respect to Theta and um often it needs to be",
    "start": "1659279",
    "end": "1668360"
  },
  {
    "text": "differentiable um there's another uh way of deriving it using important sampling",
    "start": "1668799",
    "end": "1674799"
  },
  {
    "text": "so you write down the important sampling estimator for the expectation and then you just uh swap the derivative with the",
    "start": "1674799",
    "end": "1680720"
  },
  {
    "text": "expectation and you get the same thing okay so so now let me just give a",
    "start": "1680720",
    "end": "1686720"
  },
  {
    "text": "little bit of intuition about this estimator oops okay so f ofx is",
    "start": "1686720",
    "end": "1694279"
  },
  {
    "text": "measuring how good the sample X is um so that means that so G hat here is our",
    "start": "1694279",
    "end": "1700880"
  },
  {
    "text": "gradient estimator meaning this is what we get if we take one sample X subi and",
    "start": "1700880",
    "end": "1706200"
  },
  {
    "text": "we compute our estimator this is our estimate of the gradient um so if we",
    "start": "1706200",
    "end": "1711240"
  },
  {
    "text": "move in Direction Gat um that pushes up the log probability of our sample xabi",
    "start": "1711240",
    "end": "1717799"
  },
  {
    "text": "in proportion to how good it is so if we have really good um if we got a really good function value then we're going to",
    "start": "1717799",
    "end": "1724440"
  },
  {
    "text": "try to push up its log probability a lot and if it was a bad function value then we're not going to try to push it up",
    "start": "1724440",
    "end": "1730200"
  },
  {
    "text": "very much so it's pretty simple intuition um the really nice thing is um",
    "start": "1730200",
    "end": "1739000"
  },
  {
    "text": "this is valid even if f ofx is discontinuous or if f ofx is um unknown",
    "start": "1739000",
    "end": "1744080"
  },
  {
    "text": "meaning you only uh you don't get to differentiate it you just get to see the function values um or um the sample",
    "start": "1744080",
    "end": "1751399"
  },
  {
    "text": "space um is a discrete set so X doesn't even have to be continuous um and this is um quite uh this is quite remarkable",
    "start": "1751399",
    "end": "1758919"
  },
  {
    "text": "actually that you don't even need to have access to the full um you don't need to know exactly um what the",
    "start": "1758919",
    "end": "1765480"
  },
  {
    "text": "function is that you're optimizing you just have to be able to query um for the function value um and this",
    "start": "1765480",
    "end": "1773240"
  },
  {
    "text": "means this is a way of um being able to differentiate um",
    "start": "1773240",
    "end": "1778960"
  },
  {
    "text": "functions through a system that has non-differentiable pieces um so for example in um in robotic Locomotion one",
    "start": "1778960",
    "end": "1787240"
  },
  {
    "text": "issue is that um you have contacts between the robot's foot and the ground and um contact you make and break",
    "start": "1787240",
    "end": "1794559"
  },
  {
    "text": "contact and that causes a discontinuous change in the Dynamics um so that makes it really hard to do smooth optimization",
    "start": "1794559",
    "end": "1801360"
  },
  {
    "text": "techniques to come up with the right Behavior so when you use this kind of um grading estimator along with policy",
    "start": "1801360",
    "end": "1807640"
  },
  {
    "text": "gradients which I'm going to uh talk about very soon um you can actually just",
    "start": "1807640",
    "end": "1813120"
  },
  {
    "text": "uh differentiate you can optimize this system um even though it has differentiable pieces in",
    "start": "1813120",
    "end": "1819279"
  },
  {
    "text": "it okay so",
    "start": "1819279",
    "end": "1824480"
  },
  {
    "text": "uh okay so here's another little picture of what's going on so we have our function f ofx um which we're trying to",
    "start": "1824480",
    "end": "1832080"
  },
  {
    "text": "maximize the expectation of and then we have our probability density P ofx um so",
    "start": "1832080",
    "end": "1837640"
  },
  {
    "text": "we just sample a bunch of values from our probability density those are the blue dots on the x axis and um then uh",
    "start": "1837640",
    "end": "1845799"
  },
  {
    "text": "we um so then we we look at the function values and um we're trying to push the",
    "start": "1845799",
    "end": "1853120"
  },
  {
    "text": "uh probability distribution so that the probability goes up at um these samples",
    "start": "1853120",
    "end": "1858399"
  },
  {
    "text": "in proportion to the function value um so over on the right side of the curve",
    "start": "1858399",
    "end": "1864120"
  },
  {
    "text": "uh that means we're trying to push that F uh probability value up really hard and on the left side we're pushing it up",
    "start": "1864120",
    "end": "1870000"
  },
  {
    "text": "softly uh so what's going to happen is the probability density is going to slide to the right if you can imagine a",
    "start": "1870000",
    "end": "1877320"
  },
  {
    "text": "sort of physical analogy there okay so that's that's the score",
    "start": "1877320",
    "end": "1882919"
  },
  {
    "text": "function gradient estimator this is a general technique um it can be used in",
    "start": "1882919",
    "end": "1888200"
  },
  {
    "text": "various machine learning problems um now we're going to apply it to the",
    "start": "1888200",
    "end": "1894320"
  },
  {
    "text": "reinforcement learning setting and um we're going to take our random variable",
    "start": "1894320",
    "end": "1899600"
  },
  {
    "text": "X to be a whole trajectory um so the trajectory consists of State action reward State action",
    "start": "1899600",
    "end": "1906000"
  },
  {
    "text": "reward and so on until the end of the episode and uh now um to get our",
    "start": "1906000",
    "end": "1912399"
  },
  {
    "text": "gradient estimator uh to get the um uh to get the gradient of the expected",
    "start": "1912399",
    "end": "1918240"
  },
  {
    "text": "reward all we've got to do is um compute the grad log probability uh times the total",
    "start": "1918240",
    "end": "1924760"
  },
  {
    "text": "reward so um so this uh probability of the trajectory that sounds like a really",
    "start": "1924760",
    "end": "1930720"
  },
  {
    "text": "unfriendly quantity because uh there's uh a long complicated process that's",
    "start": "1930720",
    "end": "1936279"
  },
  {
    "text": "generates this trajectory with lots of uh lots of time steps but um log um okay",
    "start": "1936279",
    "end": "1942279"
  },
  {
    "text": "so we can write out what this process is what this probability density is um so we have uh it's just a product of",
    "start": "1942279",
    "end": "1949200"
  },
  {
    "text": "probabilities we've got our initial uh we've got our mu of s0 which is just our",
    "start": "1949200",
    "end": "1954279"
  },
  {
    "text": "initial State distribution and then every time step we have um we sample the action according to Pi and we sample the",
    "start": "1954279",
    "end": "1961320"
  },
  {
    "text": "next state and reward according to our Dynamics model so uh log turns that product into",
    "start": "1961320",
    "end": "1967960"
  },
  {
    "text": "a sum and here's the cool part um everything that doesn't um contain Theta",
    "start": "1967960",
    "end": "1975360"
  },
  {
    "text": "drops out um so the thing is we didn't know uh there are parts of this um",
    "start": "1975360",
    "end": "1981519"
  },
  {
    "text": "probability uh distribution P of to given Theta that we don't have access to so if this is reinforcement learning uh",
    "start": "1981519",
    "end": "1988120"
  },
  {
    "text": "we don't assume that we know the Dynamics model of the system we just find out about it by sampling uh by",
    "start": "1988120",
    "end": "1995200"
  },
  {
    "text": "doing sample doing episodes um so um so since this uh product turns into a sum",
    "start": "1995200",
    "end": "2002639"
  },
  {
    "text": "all the the pieces uh like the log uh log P there and the log me uh which we",
    "start": "2002639",
    "end": "2009360"
  },
  {
    "text": "don't know just drop out so it doesn't matter um and uh what we get in the end",
    "start": "2009360",
    "end": "2016200"
  },
  {
    "text": "is um we get a sum of log probab sum of uh log probabilities of actions so grad",
    "start": "2016200",
    "end": "2023159"
  },
  {
    "text": "log Pi of action given State um so our formula looks like um",
    "start": "2023159",
    "end": "2030200"
  },
  {
    "text": "our formula for the grading of the expectation is just the expectation over trajectories of um total reward of the",
    "start": "2030200",
    "end": "2037240"
  },
  {
    "text": "trajectory times grad um grad of the sum of all the log",
    "start": "2037240",
    "end": "2044158"
  },
  {
    "text": "probs so the interpretation of this is um we're uh taking our good trajectories",
    "start": "2045159",
    "end": "2051720"
  },
  {
    "text": "and we're trying to increase their probability in proportion to how good they are um and you can think of this as",
    "start": "2051720",
    "end": "2057720"
  },
  {
    "text": "uh being similar to supervised learning where we treat the good trajectories with high rewards as um positive",
    "start": "2057720",
    "end": "2063480"
  },
  {
    "text": "examples in our supervised learning problem so we're using those to train the policy and which actions are good",
    "start": "2063480",
    "end": "2070800"
  },
  {
    "text": "we're basically treating those actions as positive",
    "start": "2070800",
    "end": "2075118"
  },
  {
    "text": "examples okay now we can improve this formula a little bit um so that was just",
    "start": "2077399",
    "end": "2082638"
  },
  {
    "text": "uh the most basic uh I mean this is an unbiased estimator for the policy gradient so uh if we just take that",
    "start": "2082639",
    "end": "2089638"
  },
  {
    "text": "expression inside the expectation on the right hand side and we take one sample of that it has the right mean so if we",
    "start": "2089639",
    "end": "2097079"
  },
  {
    "text": "just get enough of them we're going to get the policy gradient um okay so that's um but we can",
    "start": "2097079",
    "end": "2102960"
  },
  {
    "text": "also write down some other formulas uh that have the same mean but have lower variance so we can come up with better",
    "start": "2102960",
    "end": "2108960"
  },
  {
    "text": "estimators for the policy gradient um and that's actually quite important because the one from the previous slide",
    "start": "2108960",
    "end": "2114680"
  },
  {
    "text": "is really bad when you have uh a long a large number of time steps meaning it",
    "start": "2114680",
    "end": "2119839"
  },
  {
    "text": "has really high variance so uh the first thing we can do is you uh we can use the temporal",
    "start": "2119839",
    "end": "2126640"
  },
  {
    "text": "structure of the problem um by the way to derive these next bunch of formulas it just takes a bunch of",
    "start": "2126640",
    "end": "2133119"
  },
  {
    "text": "really straightforward manipulation where you move around expectations um and I'm not going to go through all the",
    "start": "2133119",
    "end": "2138560"
  },
  {
    "text": "math um but uh I'll just say what the formulas",
    "start": "2138560",
    "end": "2144359"
  },
  {
    "text": "are so um okay so we can repeat the same argument from the previous slide um to",
    "start": "2144400",
    "end": "2150680"
  },
  {
    "text": "just derive the gradient estimator for a single reward term so we end up with that reward term times the grad some of",
    "start": "2150680",
    "end": "2158160"
  },
  {
    "text": "log probs and just summing over that we get a new formula um where we're not",
    "start": "2158160",
    "end": "2165280"
  },
  {
    "text": "multiplying the sum of the the grad log prob of the whole thing times the sum of all rewards now um so let's look at that",
    "start": "2165280",
    "end": "2173680"
  },
  {
    "text": "bottom formula um now we have a sum over time of grad log probability of the",
    "start": "2173680",
    "end": "2179079"
  },
  {
    "text": "action at time that time times the sum of future rewards um so so now I mean in",
    "start": "2179079",
    "end": "2185200"
  },
  {
    "text": "the formula from the previous slide we would have had all the rewards in that sum um but now we just have the future",
    "start": "2185200",
    "end": "2191800"
  },
  {
    "text": "rewards and um that kind of makes sense because um an action can't affect the probability of the um previous rewards",
    "start": "2191800",
    "end": "2199560"
  },
  {
    "text": "uh so to figure out if the action is good we should have only we should only be looking at the future",
    "start": "2199560",
    "end": "2206200"
  },
  {
    "text": "rewards so this is a slightly better formula than the one on the previous slide meaning it has this exact same",
    "start": "2206200",
    "end": "2212160"
  },
  {
    "text": "mean um except uh different uh the expr",
    "start": "2212160",
    "end": "2217880"
  },
  {
    "text": "inside the expectation there has lower variance um and we can further reduce the variance by introducing a",
    "start": "2217880",
    "end": "2225240"
  },
  {
    "text": "baseline um so now uh we can take any old function uh B which takes in a state",
    "start": "2225240",
    "end": "2231520"
  },
  {
    "text": "and it outputs a real number and um we can subtract it from our sum of future",
    "start": "2231520",
    "end": "2238319"
  },
  {
    "text": "rewards and um we didn't affect the mean of the um estimator at all so we yeah we",
    "start": "2238319",
    "end": "2245920"
  },
  {
    "text": "didn't change uh the expect At All by introducing this",
    "start": "2245920",
    "end": "2251039"
  },
  {
    "text": "Baseline um so yeah for any choice of B this gives us an unbiased estimator by",
    "start": "2252000",
    "end": "2257040"
  },
  {
    "text": "the way if you're not that familiar with the terminology of estimators what I'm saying is uh we have a um expectation um",
    "start": "2257040",
    "end": "2265520"
  },
  {
    "text": "on the right hand side of that for uh formula uh and uh the quantity inside that expectation is What's called the",
    "start": "2265520",
    "end": "2271760"
  },
  {
    "text": "estimator and um if we get a bunch of samples uh then we can get an estimate of um",
    "start": "2271760",
    "end": "2278079"
  },
  {
    "text": "of the thing on the left hand side which is what we care about so um so when I say it's an",
    "start": "2278079",
    "end": "2284680"
  },
  {
    "text": "unbiased estimator that just means that well that just means that this equation is correct meaning that the thing on the",
    "start": "2284680",
    "end": "2290560"
  },
  {
    "text": "right hand side equals the thing on the left hand side um so yeah this works for any",
    "start": "2290560",
    "end": "2296160"
  },
  {
    "text": "choice of Baseline and um a near optimal choice is to use the expected return so",
    "start": "2296160",
    "end": "2302640"
  },
  {
    "text": "the expected sum of future rewards and uh the interpretation of",
    "start": "2302640",
    "end": "2308760"
  },
  {
    "text": "that is um if we took an action we only want to increase the probability of the",
    "start": "2308760",
    "end": "2314520"
  },
  {
    "text": "action if it was a good action um so how do we tell if it was a good action well the sum of rewards after that action",
    "start": "2314520",
    "end": "2321359"
  },
  {
    "text": "should have been better than expected um so the B ofs is the expected sum of",
    "start": "2321359",
    "end": "2326839"
  },
  {
    "text": "rewards and we're just taking the difference between the measured thing and the expected",
    "start": "2326839",
    "end": "2332760"
  },
  {
    "text": "thing yeah okay so uh that's okay that's the that that was a pretty",
    "start": "2333319",
    "end": "2340760"
  },
  {
    "text": "key thing for variance reduction um and I'm going to introduce one last um variance reduction technique and",
    "start": "2340760",
    "end": "2347200"
  },
  {
    "text": "actually all three of these are really important so um basically nothing's going to work um except for maybe really",
    "start": "2347200",
    "end": "2353160"
  },
  {
    "text": "small scale problems unless you do these things um so the last variance reduction technique is to to use discounts um",
    "start": "2353160",
    "end": "2361520"
  },
  {
    "text": "so um the discount Factor um ignores delayed effects between actions and Rewards so what we we're going to do",
    "start": "2361520",
    "end": "2369200"
  },
  {
    "text": "here looks kind of like a hack but there's an explanation for it um which is instead of taking the sum of rewards",
    "start": "2369200",
    "end": "2376280"
  },
  {
    "text": "uh we're going to take a discounted sum of rewards meaning that um we uh we add",
    "start": "2376280",
    "end": "2381560"
  },
  {
    "text": "this exponential Factor uh gamma so that um when so when we're multiplying the",
    "start": "2381560",
    "end": "2388040"
  },
  {
    "text": "grad log probability by some future award uh we multiply it by some uh",
    "start": "2388040",
    "end": "2393680"
  },
  {
    "text": "quantity that decays with time so people typically use like gamma equals 99 or",
    "start": "2393680",
    "end": "2399240"
  },
  {
    "text": "gamma equals 095 uh so that means like if you Ed 099 that means after 100 time",
    "start": "2399240",
    "end": "2405160"
  },
  {
    "text": "steps um you're going to be um uh you're going to be reducing the reward by a",
    "start": "2405160",
    "end": "2411560"
  },
  {
    "text": "factor of one over e so um so you're exponentially um you're decaying the um",
    "start": "2411560",
    "end": "2418400"
  },
  {
    "text": "effect of the future rewards and the intuition is that um an action uh the",
    "start": "2418400",
    "end": "2424079"
  },
  {
    "text": "action shouldn't affect rewards really far in the future like the system should um the s u the like the assumption is",
    "start": "2424079",
    "end": "2432040"
  },
  {
    "text": "that the system doesn't have really long-term memory and it's sort of resets it's or or the there aren't effect the",
    "start": "2432040",
    "end": "2438400"
  },
  {
    "text": "effects aren't that far delayed uh so you can just ignore um the interaction between action and a a reward way way in",
    "start": "2438400",
    "end": "2447040"
  },
  {
    "text": "the future that's the uh intuition um so now instead of taking the Baseline to be the",
    "start": "2447040",
    "end": "2453160"
  },
  {
    "text": "expected sum of future rewards we want to do a discounted sum uh so now were measuring if the action was better than",
    "start": "2453160",
    "end": "2458920"
  },
  {
    "text": "expected according to this um like the according to the discounted",
    "start": "2458920",
    "end": "2464880"
  },
  {
    "text": "sum um and now there's a more General class of formulas that looks like the one that I just wrote so this this one",
    "start": "2464880",
    "end": "2471400"
  },
  {
    "text": "that's on the top of the slide is pretty good and um this is like almost as good as anything you're going to do to within",
    "start": "2471400",
    "end": "2477839"
  },
  {
    "text": "a small constant Factor uh but um there's there's a more General class of",
    "start": "2477839",
    "end": "2483480"
  },
  {
    "text": "formulas that um look like um grad log probability times uh some quantity a hat",
    "start": "2483480",
    "end": "2490640"
  },
  {
    "text": "which we call the advantage estimate and this is in general just going to be um an estimate of um this is an it has a",
    "start": "2490640",
    "end": "2498000"
  },
  {
    "text": "more a precise definition which is how much uh how like how much was this",
    "start": "2498000",
    "end": "2503599"
  },
  {
    "text": "action um better than the um average action taken by the policy but in but",
    "start": "2503599",
    "end": "2509119"
  },
  {
    "text": "informally this just means how much better was the action then expected so and and this formula makes a",
    "start": "2509119",
    "end": "2516359"
  },
  {
    "text": "lot of sense because we we want to increase the probability of the good actions and de decrease the probability of the bad ones so we should um we",
    "start": "2516359",
    "end": "2523599"
  },
  {
    "text": "should increase it in proportion to the goodness of the",
    "start": "2523599",
    "end": "2528039"
  },
  {
    "text": "action okay so just to summarize so I just told you there's this gradient estimator meaning there's this",
    "start": "2529040",
    "end": "2534839"
  },
  {
    "text": "expression you can compute which gives you a noisy estimate of the policy gradient so how do you actually turn",
    "start": "2534839",
    "end": "2539920"
  },
  {
    "text": "this into an algorithm uh so this is silly algorithm s uh so um so here's",
    "start": "2539920",
    "end": "2547839"
  },
  {
    "text": "what the algorithm looks like it's pretty much what you'd expect uh you um you take your policy um you initialize",
    "start": "2547839",
    "end": "2555079"
  },
  {
    "text": "your policy parameter and your Baseline function um you uh for each it each",
    "start": "2555079",
    "end": "2560960"
  },
  {
    "text": "iteration you um execute the um the uh current policy to get a bunch of whole",
    "start": "2560960",
    "end": "2567559"
  },
  {
    "text": "episodes meaning whole trajectories and um each time step in the each trajectory",
    "start": "2567559",
    "end": "2573760"
  },
  {
    "text": "you should compute the return meaning the sum of rewards following that time step the sum of discounted rewards and",
    "start": "2573760",
    "end": "2579680"
  },
  {
    "text": "the advantage estimate which is um the sum of discounted rewards minus the Baseline uh then you refit the Baseline",
    "start": "2579680",
    "end": "2587160"
  },
  {
    "text": "by trying to um make the Baseline function equal the returns uh and then",
    "start": "2587160",
    "end": "2593280"
  },
  {
    "text": "um you update the policy using a policy gradient estimator so you're just doing SGD while updating the Baseline as you",
    "start": "2593280",
    "end": "2599960"
  },
  {
    "text": "go along yeah so that's that's the vanilla",
    "start": "2599960",
    "end": "2607640"
  },
  {
    "text": "policy gradient algorithm um and this is um I'll briefly talk this has been used",
    "start": "2607640",
    "end": "2615280"
  },
  {
    "text": "to obtain some pretty good results so it's not um that bad of an algorithm but um there there several different",
    "start": "2615280",
    "end": "2622880"
  },
  {
    "text": "directions that it can be improved so one one uh issue that you",
    "start": "2622880",
    "end": "2629680"
  },
  {
    "text": "run into um is with step sizes um so in supervised learning step sizes aren't",
    "start": "2629680",
    "end": "2634720"
  },
  {
    "text": "that big of a deal um because uh maybe you take too big of a",
    "start": "2634720",
    "end": "2640280"
  },
  {
    "text": "step um but that's okay um you'll fix it the next update and um your uh current",
    "start": "2640280",
    "end": "2646319"
  },
  {
    "text": "function your current classifier for example doesn't affect what inputs you're getting so even if you just um",
    "start": "2646319",
    "end": "2652440"
  },
  {
    "text": "are doing really uh even if your network is just kind of thrashing around for a while because you're taking too big",
    "start": "2652440",
    "end": "2658240"
  },
  {
    "text": "steps uh that's not going to cause any problems um but",
    "start": "2658240",
    "end": "2664000"
  },
  {
    "text": "um uh yeah and reinfor so yeah so step sizes aren't that big of a deal you can just anal them uh you can start off with",
    "start": "2664000",
    "end": "2671200"
  },
  {
    "text": "a large step size and anal them down to zero and that um works pretty well um in",
    "start": "2671200",
    "end": "2676680"
  },
  {
    "text": "reinforcement learning if you take too big of a step you might wreck your policy um and even if you don't actually",
    "start": "2676680",
    "end": "2682680"
  },
  {
    "text": "change the network that much so you don't lose all your nice features um you",
    "start": "2682680",
    "end": "2687920"
  },
  {
    "text": "you might just change its Behavior a little too much and now it's going to do something totally different and visit a",
    "start": "2687920",
    "end": "2693160"
  },
  {
    "text": "totally different part of State space um so since in reinforcement learning the system is stateful and your state",
    "start": "2693160",
    "end": "2699520"
  },
  {
    "text": "distribution depends on your policy that makes that like brings uh a really a",
    "start": "2699520",
    "end": "2705559"
  },
  {
    "text": "different problem and uh now like after you took that step the next batch of data you're going to get was collected",
    "start": "2705559",
    "end": "2711760"
  },
  {
    "text": "by the bad policy and now you're never going to recover because you just forgot",
    "start": "2711760",
    "end": "2718119"
  },
  {
    "text": "everything yeah so um One Way um that uh my",
    "start": "2718319",
    "end": "2724160"
  },
  {
    "text": "colleagues and I well one way to fix this is to try to um to try to stop the",
    "start": "2724160",
    "end": "2729559"
  },
  {
    "text": "basically try to stop the policy from taking too big of a step so um you can look at the K Divergence between the um",
    "start": "2729559",
    "end": "2736760"
  },
  {
    "text": "old policy and the new policy um like before the update and after the update and make sure that um the uh",
    "start": "2736760",
    "end": "2743160"
  },
  {
    "text": "distributions aren't that different so you're not taking too big of a step it's kind of an obvious thing to do uh so my",
    "start": "2743160",
    "end": "2748440"
  },
  {
    "text": "colleagues and I developed an algorithm called trust region policy optimization um which looks at the yeah looks at the",
    "start": "2748440",
    "end": "2754839"
  },
  {
    "text": "action distributions and tries to make sure the K Divergence isn't too large",
    "start": "2754839",
    "end": "2759880"
  },
  {
    "text": "and uh there's this is very closely related to previous meth natural policy gradient methods which uh which are",
    "start": "2759880",
    "end": "2768520"
  },
  {
    "text": "based on um which are doing something similar but usually it's not um set up",
    "start": "2768520",
    "end": "2773640"
  },
  {
    "text": "as a hard constraint on the K",
    "start": "2773640",
    "end": "2777558"
  },
  {
    "text": "Divergence so another um type of extension of policy gradient methods is",
    "start": "2779800",
    "end": "2785359"
  },
  {
    "text": "um to do more uh to use value uh value functions to do um more variance",
    "start": "2785359",
    "end": "2791200"
  },
  {
    "text": "reduction um instead of just using them as a baseline you can also um you can",
    "start": "2791200",
    "end": "2797040"
  },
  {
    "text": "use them more aggressively and introduce some bias um so I won't go into the",
    "start": "2797040",
    "end": "2802079"
  },
  {
    "text": "details in this talk um but um sometimes these are called actor critic methods",
    "start": "2802079",
    "end": "2809960"
  },
  {
    "text": "um there's also another type of approach which I briefly touched on in the um",
    "start": "2811640",
    "end": "2817240"
  },
  {
    "text": "earlier slide um where instead of just trying to increase the probability of the good actions you actually",
    "start": "2817240",
    "end": "2823160"
  },
  {
    "text": "differentiate your loss with respect to the actions um this is like the reparameterization trick which is used",
    "start": "2823160",
    "end": "2828839"
  },
  {
    "text": "um for um like for density modeling and unsupervised learning",
    "start": "2828839",
    "end": "2834000"
  },
  {
    "text": "um so uh here you're trying to instead of just increasing the probability of the good actions you're trying to push",
    "start": "2834000",
    "end": "2840119"
  },
  {
    "text": "the actions towards better actions and I'd say both of these bullet",
    "start": "2840119",
    "end": "2845839"
  },
  {
    "text": "points um you're um potentially decreasing your variance a lot but at",
    "start": "2845839",
    "end": "2851040"
  },
  {
    "text": "the cost of increasing bias so it's actually U makes the algorithms a little harder to um like to understand and to",
    "start": "2851040",
    "end": "2857960"
  },
  {
    "text": "get them working because um with high variance if you just uh crank up the amount of data you can always drive your",
    "start": "2857960",
    "end": "2864400"
  },
  {
    "text": "variants down as much as you want but with bias even if no matter how much data you get you're not going to get rid",
    "start": "2864400",
    "end": "2870800"
  },
  {
    "text": "of the bias so if your grading is pointing in the wrong direction then you're not going to learn anything",
    "start": "2870800",
    "end": "2878519"
  },
  {
    "text": "okay so now uh that that's it for the policy gradient section of this um this talk um so I wanted to show a quick",
    "start": "2879240",
    "end": "2886880"
  },
  {
    "text": "video of uh some work that my colleagues and I did on learning Locomotion controllers with policy gradient methods",
    "start": "2886880",
    "end": "2894480"
  },
  {
    "text": "which I think um well I found pretty exciting when I saw it uh",
    "start": "2894480",
    "end": "2899880"
  },
  {
    "text": "so hopefully it's you find it",
    "start": "2899880",
    "end": "2905440"
  },
  {
    "text": "interesting so here what we've got is a um humanoid a",
    "start": "2905440",
    "end": "2910520"
  },
  {
    "text": "simulated let's see it okay yeah it's a simulated humanoid robot um in a physics simulator a realistic physics simulator",
    "start": "2910520",
    "end": "2917680"
  },
  {
    "text": "called Moko and uh it has a neural network policy uh which takes in um The Joint",
    "start": "2917680",
    "end": "2925280"
  },
  {
    "text": "angles of the robot and uh maybe some and a little bit of other kinematic information like joint it's got joint",
    "start": "2925280",
    "end": "2931520"
  },
  {
    "text": "velocities and also um positions of the different um Links of the robot so",
    "start": "2931520",
    "end": "2936839"
  },
  {
    "text": "that's what the input is it's pretty much the raw um state of the robot like no clever feature engineering there and",
    "start": "2936839",
    "end": "2944400"
  },
  {
    "text": "um the output is going to be the joint torqus which are set 100 times a second",
    "start": "2944400",
    "end": "2949480"
  },
  {
    "text": "so we're just mapping from joint angles to joint torqus and uh we Define a reward",
    "start": "2949480",
    "end": "2956760"
  },
  {
    "text": "function which is to move forward as fast as possible so it gets a reward for moving forward and um it gets a uh so um",
    "start": "2956760",
    "end": "2966839"
  },
  {
    "text": "the episode ends when it its head goes below a certain height meaning it fell over so that's basically the setup there",
    "start": "2966839",
    "end": "2973000"
  },
  {
    "text": "was a little bit of tweaking for the reward function but um not too extensive",
    "start": "2973000",
    "end": "2978400"
  },
  {
    "text": "um so",
    "start": "2978400",
    "end": "2983680"
  },
  {
    "text": "whoops yeah so you can see first it just Falls forward a lot of times and then slowly it starts to develop a uh half",
    "start": "2990880",
    "end": "2998640"
  },
  {
    "text": "decent looking walk and uh eventually it gets it down pretty",
    "start": "2998640",
    "end": "3005920"
  },
  {
    "text": "well and at the very end of this um it could just keep running uh indefinitely so I think it was actually stable in a",
    "start": "3005920",
    "end": "3013040"
  },
  {
    "text": "strong sense meaning I could just leave it for 15 minutes and it wouldn't fall over it would just keep going so uh here's another um robot",
    "start": "3013040",
    "end": "3021559"
  },
  {
    "text": "model that um we just created without too much thought I mean we just decided",
    "start": "3021559",
    "end": "3027000"
  },
  {
    "text": "to put a bunch of legs on this thing um and uh so we don't even know how this thing is supposed to walk um and uh just",
    "start": "3027000",
    "end": "3035760"
  },
  {
    "text": "give it to the same algorithm and it just figures out some kind of crazy way to walk",
    "start": "3035760",
    "end": "3041799"
  },
  {
    "text": "um so that's the nice thing about reinforcement learning uh you don't even need to know what you want it to do um I",
    "start": "3041799",
    "end": "3048760"
  },
  {
    "text": "think this is also the physics are a little unrealistic here",
    "start": "3048760",
    "end": "3053400"
  },
  {
    "text": "but here we set up up we used this um a similar model to the one in the first uh",
    "start": "3055000",
    "end": "3060440"
  },
  {
    "text": "demo but uh here we just give it a reward for having its head at a certain height so there's a re word telling it",
    "start": "3060440",
    "end": "3066839"
  },
  {
    "text": "to get its head up as high as possible and then it figures out how to get up off the",
    "start": "3066839",
    "end": "3072799"
  },
  {
    "text": "ground oh let's see um I have I have low battery uh does anyone have a charger",
    "start": "3074319",
    "end": "3081000"
  },
  {
    "text": "that I could",
    "start": "3081000",
    "end": "3084280"
  },
  {
    "text": "oh thanks a lot you're a",
    "start": "3088680",
    "end": "3092040"
  },
  {
    "text": "lifesaver okay any questions about policy gradients before I move on to the next",
    "start": "3102480",
    "end": "3108760"
  },
  {
    "text": "part m",
    "start": "3110640",
    "end": "3114640"
  },
  {
    "text": "oh yeah so the question was is the system time invariant uh yes the that's that's",
    "start": "3116839",
    "end": "3122040"
  },
  {
    "text": "assumed that it's stationary oh right and also that it",
    "start": "3122040",
    "end": "3127839"
  },
  {
    "text": "doesn't change from one episode to the next of course in some real world problems that might not be the case so",
    "start": "3127839",
    "end": "3133160"
  },
  {
    "text": "that's I think that's also an interesting problem setting where you have a non-stationary",
    "start": "3133160",
    "end": "3139000"
  },
  {
    "text": "environment um so the question question was uh for the Baseline to learn a good Baseline uh do you need to know the",
    "start": "3144520",
    "end": "3150520"
  },
  {
    "text": "Dynamics of the system um so no you can just learn it by doing regression you just uh estimate the",
    "start": "3150520",
    "end": "3157440"
  },
  {
    "text": "empirical returns and then you do regression to try to uh fit a function to that",
    "start": "3157440",
    "end": "3164160"
  },
  {
    "text": "yeah so the question is um there's a discount factor which um causes the um which should cause the",
    "start": "3189599",
    "end": "3197000"
  },
  {
    "text": "policy to disregard any effects that are delayed by more than a 100 time steps so um how does it still work that this guy",
    "start": "3197000",
    "end": "3204079"
  },
  {
    "text": "learns how to stand up um even though that might take more than 100 time steps is that correct yeah um so yeah you're",
    "start": "3204079",
    "end": "3211319"
  },
  {
    "text": "right um and in fact I would say that these methods um aren't guaranteed to work well if you have more than a 100",
    "start": "3211319",
    "end": "3217960"
  },
  {
    "text": "time steps uh so sometimes they work anyway often they work anyway but",
    "start": "3217960",
    "end": "3223160"
  },
  {
    "text": "there's no guarantee um so I think there's actually something pretty fundamental missing in how uh like how",
    "start": "3223160",
    "end": "3228559"
  },
  {
    "text": "to deal with really long time scales and people have recently been thinking about hierarchical reinforcement learning",
    "start": "3228559",
    "end": "3234640"
  },
  {
    "text": "where you have um different levels of uh detail of the system where you might have a like one level of description",
    "start": "3234640",
    "end": "3242240"
  },
  {
    "text": "where you have a um like a short time a small time step and then you have successively larger time steps and uh",
    "start": "3242240",
    "end": "3249359"
  },
  {
    "text": "you can that allows you to plan over much longer Horizons um so that's something that's currently in active",
    "start": "3249359",
    "end": "3254720"
  },
  {
    "text": "area of research but yeah I would say that none of these methods are going to um do are guaranteed to do anything",
    "start": "3254720",
    "end": "3260880"
  },
  {
    "text": "reasonable if you have uh more than one over one minus gamma time steps uh",
    "start": "3260880",
    "end": "3266400"
  },
  {
    "text": "between action and",
    "start": "3266400",
    "end": "3269520"
  },
  {
    "text": "reward oh yeah so in this kind of task if you introduced terrain or something could it uh I think if it didn't if you",
    "start": "3274480",
    "end": "3282359"
  },
  {
    "text": "didn't train it to deal with terrain then it um then it might fail it probably would fail actually I don't",
    "start": "3282359",
    "end": "3288319"
  },
  {
    "text": "think it would fail because uh the funny thing about these policies are actually really robust because um you train them",
    "start": "3288319",
    "end": "3295280"
  },
  {
    "text": "with the stochastic policy policy um so there's a lot of noise being generated by the policy itself um so in practice",
    "start": "3295280",
    "end": "3303559"
  },
  {
    "text": "uh it's um it's actually so it's able to deal with huge noise introduced by the policy and as a result um I found that",
    "start": "3303559",
    "end": "3310839"
  },
  {
    "text": "if you um change the Dynamics parameters a little it can usually still work but yeah there's no guarantee that it'll do",
    "start": "3310839",
    "end": "3317079"
  },
  {
    "text": "anything if you give it something you didn't train it for um I I think that you probably could train it um this do",
    "start": "3317079",
    "end": "3323359"
  },
  {
    "text": "the same kind of training with uh terrain I didn't have any terrain so I didn't try it but that would be nice to",
    "start": "3323359",
    "end": "3331280"
  },
  {
    "text": "try okay I'm going to move on to the next part of the talk uh feel free if you have more questions to find me",
    "start": "3331640",
    "end": "3340000"
  },
  {
    "text": "afterwards okay so now I'm going to talk about a different uh type of reinforcement learning",
    "start": "3346160",
    "end": "3353079"
  },
  {
    "text": "algorithm so okay so these uh so the previous kind of methods are",
    "start": "3353079",
    "end": "3359559"
  },
  {
    "text": "distinguished by the fact that they learn they explicitly represent a policy which is the function that chooses your",
    "start": "3359559",
    "end": "3365720"
  },
  {
    "text": "actions and they try to optimize it with respect to the parameters of the policy um so the nice thing about the policy",
    "start": "3365720",
    "end": "3372000"
  },
  {
    "text": "gradient methods we just talked about is that you're optimizing the thing you care about um so and you're optimizing",
    "start": "3372000",
    "end": "3378720"
  },
  {
    "text": "it with gradient descent so that makes it kind of easy to understand what's going on um because if you take if",
    "start": "3378720",
    "end": "3385079"
  },
  {
    "text": "you're getting the proper grading estimate and you take small enough steps then you should be improving I mean of",
    "start": "3385079",
    "end": "3390359"
  },
  {
    "text": "course you still could get stuck in a local minimum but at least uh or you get stuck in a bad local minimum but at",
    "start": "3390359",
    "end": "3396720"
  },
  {
    "text": "least it's a local minimum and you can use the our understanding of optimization to figure out what's going",
    "start": "3396720",
    "end": "3403039"
  },
  {
    "text": "on so these next class of methods are a little different because um they're not",
    "start": "3403039",
    "end": "3408079"
  },
  {
    "text": "optimizing the policy directly uh they're learning something else called a q function uh which measures how good um",
    "start": "3408079",
    "end": "3415200"
  },
  {
    "text": "State action pairs are are so it measures um I'll I'll say that more formally L later but it's just measuring",
    "start": "3415200",
    "end": "3421400"
  },
  {
    "text": "how good the actions are um and uh these methods are actually um the",
    "start": "3421400",
    "end": "3429319"
  },
  {
    "text": "these are um able to ex exactly solve um mdps efficiently in uh the setting where",
    "start": "3429319",
    "end": "3436440"
  },
  {
    "text": "you have a finite number of states and actions um so these are these are the preferred methods for exactly solving",
    "start": "3436440",
    "end": "3442599"
  },
  {
    "text": "them in in those settings um but um you can apply them uh with um continuous",
    "start": "3442599",
    "end": "3449240"
  },
  {
    "text": "States and actions and um using um using expressive function approximators like",
    "start": "3449240",
    "end": "3455200"
  },
  {
    "text": "neural networks but it's a little harder to understand um what's going on in these",
    "start": "3455200",
    "end": "3460359"
  },
  {
    "text": "methods like when they're going to work and when they're not going to work so um I'll Define um the relevant",
    "start": "3460359",
    "end": "3468799"
  },
  {
    "text": "quantities here uh so the Q function is defined as uh the expected sum of",
    "start": "3468799",
    "end": "3474200"
  },
  {
    "text": "rewards um when we condition on the first state and the first action um",
    "start": "3474200",
    "end": "3479400"
  },
  {
    "text": "so we're conditioning on s0 equals s a0 equals a and we're um we're and the Q",
    "start": "3479400",
    "end": "3487119"
  },
  {
    "text": "function is the expected discounted sum of rewards uh when we're acting under the policy",
    "start": "3487119",
    "end": "3493559"
  },
  {
    "text": "Pi so um by convention I'm starting out",
    "start": "3493559",
    "end": "3499000"
  },
  {
    "text": "with uh time Step Zero I could have also said that um we're taking RT plus RT + 1",
    "start": "3499000",
    "end": "3505400"
  },
  {
    "text": "plus RT plus two and so on uh but since we're assuming the system is stationary it should be exactly the same so just by",
    "start": "3505400",
    "end": "3513119"
  },
  {
    "text": "convention I'm going to say that the first I'm going to always use time Z 1 two three and so on Just for ease of",
    "start": "3513119",
    "end": "3520799"
  },
  {
    "text": "notation so the Q function is just telling you how good and this state action pair is under your current policy",
    "start": "3520799",
    "end": "3527680"
  },
  {
    "text": "um the value function well the state value function usually called V is uh",
    "start": "3527680",
    "end": "3533039"
  },
  {
    "text": "just um conditioning on the state it's uh telling you how good that state",
    "start": "3533039",
    "end": "3539960"
  },
  {
    "text": "is what's the expected reward at that State and lastly there's an the advantage function is the difference",
    "start": "3539960",
    "end": "3545440"
  },
  {
    "text": "between the Q function and the state value function meaning how much better is that action than uh what the policy",
    "start": "3545440",
    "end": "3551520"
  },
  {
    "text": "would have done we're not going to talk about Advantage functions in this section but",
    "start": "3551520",
    "end": "3557119"
  },
  {
    "text": "it was actually this corresponds to the notion of Advantage estimator uh we briefly mentioned in the previous",
    "start": "3557119",
    "end": "3565039"
  },
  {
    "text": "section so here we're going to consider um methods that explicitly store and update the Q function instead of the",
    "start": "3565039",
    "end": "3571400"
  },
  {
    "text": "policy and um updates them using uh what are called Bellman",
    "start": "3571400",
    "end": "3578000"
  },
  {
    "text": "equations so um so the Bellman equation um so a Bellman equation in general is a",
    "start": "3578920",
    "end": "3585520"
  },
  {
    "text": "consistency equation that should be satisfied by a value function um so here um I'm writing down",
    "start": "3585520",
    "end": "3592880"
  },
  {
    "text": "the Bellman equation for QP and um what it's saying is that um the um",
    "start": "3592880",
    "end": "3600440"
  },
  {
    "text": "expected sum of rewards should be um the first reward plus this expected sum of",
    "start": "3600440",
    "end": "3606240"
  },
  {
    "text": "rewards at after the first time step so it's saying something pretty simple that's um so r0 is the first reward uh V",
    "start": "3606240",
    "end": "3613480"
  },
  {
    "text": "Pi of S1 is just um adding up all the rewards at after at After Time Step",
    "start": "3613480",
    "end": "3621039"
  },
  {
    "text": "Zero um so uh in the second equation we write",
    "start": "3621039",
    "end": "3626119"
  },
  {
    "text": "out this relationship just involving the Q function so we have a consistency equation that the Q function should",
    "start": "3626119",
    "end": "3633920"
  },
  {
    "text": "satisfy um we can slightly generalize this to use um ktime steps instead of",
    "start": "3636240",
    "end": "3641559"
  },
  {
    "text": "just one time step so uh we can expand out the um expectation the expected sum",
    "start": "3641559",
    "end": "3647200"
  },
  {
    "text": "of rewards to write write out K rewards explicitly and then uh cap it off with",
    "start": "3647200",
    "end": "3652359"
  },
  {
    "text": "the value function at the very end which accounts for all the rewards after",
    "start": "3652359",
    "end": "3658400"
  },
  {
    "text": "that okay so here's the Bellman equation from the previous slide so now I'm going to introduce a very important concept",
    "start": "3659400",
    "end": "3665960"
  },
  {
    "text": "called a Bellman backup so uh so we have this equation",
    "start": "3665960",
    "end": "3672359"
  },
  {
    "text": "that the uh value the um value function Q Pi should satisfy um but we don't know",
    "start": "3672359",
    "end": "3678079"
  },
  {
    "text": "Q let's assume we don't know Q Pi so let's say we have some uh some other Q",
    "start": "3678079",
    "end": "3683400"
  },
  {
    "text": "function um so we Define this Bellman backup operator that that operates on an",
    "start": "3683400",
    "end": "3689920"
  },
  {
    "text": "arbitrary Q function so it maps The Q function to a new Q function and uh it's",
    "start": "3689920",
    "end": "3695440"
  },
  {
    "text": "defined by just taking the right hand side of the Bellman equation and uh plugging in um our Q function our new Q",
    "start": "3695440",
    "end": "3703240"
  },
  {
    "text": "function Q instead of the um Q",
    "start": "3703240",
    "end": "3708039"
  },
  {
    "text": "Pi so uh Q Pi is a fix point of this operator u meaning if we apply the",
    "start": "3709200",
    "end": "3716079"
  },
  {
    "text": "backup operator we get it the same thing back and um and very nicely if we keep",
    "start": "3716079",
    "end": "3724400"
  },
  {
    "text": "applying this backup operator repeatedly to any old arbitrary initial um Q",
    "start": "3724400",
    "end": "3729920"
  },
  {
    "text": "function Q the series will converge to Q Pi which is the fix point of the operator so that's",
    "start": "3729920",
    "end": "3738440"
  },
  {
    "text": "uh yeah so that's um so that way you can uh you can um",
    "start": "3738720",
    "end": "3745960"
  },
  {
    "text": "one way you can use an iterative algorithm to estimate Q Pi by taking any old initial Q function and repeatedly",
    "start": "3745960",
    "end": "3752359"
  },
  {
    "text": "applying this backup operator um so now there's another kind",
    "start": "3752359",
    "end": "3757480"
  },
  {
    "text": "of Q function that we're going to introduce uh called qar so the previous",
    "start": "3757480",
    "end": "3762960"
  },
  {
    "text": "Q function Q Pi was this is the uh telling you uh the value function under the current under some policy Pi so it",
    "start": "3762960",
    "end": "3770400"
  },
  {
    "text": "only makes sense with regard to some particular fixed policy Pi qar um",
    "start": "3770400",
    "end": "3776400"
  },
  {
    "text": "is going to be um is going to involve the optimal policy instead so um so qar",
    "start": "3776400",
    "end": "3783640"
  },
  {
    "text": "is just defined as the Q function of the optimal policy so here we have Pi star the optimal policy and qar is just the Q",
    "start": "3783640",
    "end": "3790920"
  },
  {
    "text": "function of the optimal policy and um it also happens to be uh the pointwise maximum over all policies of uh the Q",
    "start": "3790920",
    "end": "3799400"
  },
  {
    "text": "function um at each state action pair",
    "start": "3799400",
    "end": "3804640"
  },
  {
    "text": "so uh so the optimal policy is deterministic and um it should satisfy",
    "start": "3805880",
    "end": "3812079"
  },
  {
    "text": "this equation that um it takes the argmax of the optimal Q function so recall that the Q function tells you",
    "start": "3812079",
    "end": "3818559"
  },
  {
    "text": "your expected return if you take the um the given action um so obviously the",
    "start": "3818559",
    "end": "3824079"
  },
  {
    "text": "optimal policy should take the action that has the best expected return so that's why um that's why this last",
    "start": "3824079",
    "end": "3831440"
  },
  {
    "text": "equation is um evident um",
    "start": "3831440",
    "end": "3836599"
  },
  {
    "text": "so um so now now that we know this property of the optimal policy uh we can",
    "start": "3836920",
    "end": "3842359"
  },
  {
    "text": "rewrite the Bellman equation so uh so on the that that first",
    "start": "3842359",
    "end": "3847880"
  },
  {
    "text": "equation is that's just the Bellman equation from the previous slides for given policy Pi um now um we can take",
    "start": "3847880",
    "end": "3854960"
  },
  {
    "text": "that expectation over actions and replace it by what the optimal policy is going to do which is just going to take",
    "start": "3854960",
    "end": "3860960"
  },
  {
    "text": "it's going to take the argmax of the optimal Q function there's a typo on my slide that should say qar um inside of",
    "start": "3860960",
    "end": "3868920"
  },
  {
    "text": "um on the right hand side so um so now we have a Bellman equation that the",
    "start": "3868920",
    "end": "3874760"
  },
  {
    "text": "optimal policy should satisfy now we can do the same thing",
    "start": "3874760",
    "end": "3880400"
  },
  {
    "text": "with the backup operator um so um we we take that Bellman equation and we uh",
    "start": "3880400",
    "end": "3886440"
  },
  {
    "text": "plug in an arbitrary Q function on the right hand side instead of the optimal Q",
    "start": "3886440",
    "end": "3891480"
  },
  {
    "text": "function qar um so qar um is a fixed point of",
    "start": "3891480",
    "end": "3898319"
  },
  {
    "text": "this Bellman operator that's just a restatement of the Bellman equation and uh again if we reply um",
    "start": "3898319",
    "end": "3907039"
  },
  {
    "text": "this Bellman operator repeatedly to an arbitrary initial Q function it converges to qar which is the optimal Q",
    "start": "3907039",
    "end": "3915839"
  },
  {
    "text": "function this is um the BAC F fixo theorem in both cases can be used to",
    "start": "3915839",
    "end": "3920880"
  },
  {
    "text": "prove it",
    "start": "3920880",
    "end": "3924119"
  },
  {
    "text": "okay so based on these ideas um there are two classic algorithms for exactly",
    "start": "3926160",
    "end": "3931559"
  },
  {
    "text": "solving mdps these are sometimes called dynamic programming algorithms because they're actually quite related to the",
    "start": "3931559",
    "end": "3937920"
  },
  {
    "text": "kind of dynamic programming algorithms that are used to solve uh search problems",
    "start": "3937920",
    "end": "3943279"
  },
  {
    "text": "um so one is called value iteration and you just initialize your Q function",
    "start": "3943279",
    "end": "3948760"
  },
  {
    "text": "arbitrarily and you repeatedly do Bellman backups until it converges",
    "start": "3948760",
    "end": "3955119"
  },
  {
    "text": "uh the second one is called policy iteration um you initialize your policy",
    "start": "3955920",
    "end": "3961279"
  },
  {
    "text": "arbitrarily uh then uh each step you uh first can compute um either exactly or",
    "start": "3961279",
    "end": "3967920"
  },
  {
    "text": "approximately uh the Q function of that policy and then uh you update your",
    "start": "3967920",
    "end": "3974760"
  },
  {
    "text": "policy to be the greedy policy for the Q function you just computed uh so that means that uh you your new policy just",
    "start": "3974760",
    "end": "3982000"
  },
  {
    "text": "takes the argmax of the Q function so it takes the action that's best according to that Q",
    "start": "3982000",
    "end": "3988720"
  },
  {
    "text": "function um so I didn't say anything about how you compute QP uh so one way",
    "start": "3990400",
    "end": "3995480"
  },
  {
    "text": "to do it is to compute it um you can compute it exactly because it happens that the Bellman equation for QP is a",
    "start": "3995480",
    "end": "4001400"
  },
  {
    "text": "linear system equations so often you can just solve them exactly um more commonly",
    "start": "4001400",
    "end": "4007720"
  },
  {
    "text": "well if you have a large scale problem you might not be able to solve this system uh so what people often do is",
    "start": "4007720",
    "end": "4013359"
  },
  {
    "text": "they do um a finite number of bellman backups uh which gives you which doesn't",
    "start": "4013359",
    "end": "4018480"
  },
  {
    "text": "exactly Converge on Q Pi but it gives you something that's approximately Q",
    "start": "4018480",
    "end": "4024839"
  },
  {
    "text": "Pi okay so that's um I just told you algorithms that you can Implement if you",
    "start": "4026000",
    "end": "4031200"
  },
  {
    "text": "have full access to the mdp like you know the whole table of probabilities um but in reinforcement learning usually",
    "start": "4031200",
    "end": "4038319"
  },
  {
    "text": "the assumption is that you don't know any of these probability distributions you don't know the reward function you don't know the transition probabilities",
    "start": "4038319",
    "end": "4044839"
  },
  {
    "text": "so all these things things have to be um estimated from data or they have to or you're only able to access the system",
    "start": "4044839",
    "end": "4050880"
  },
  {
    "text": "through interaction so now it turns out that these uh algorithms can be um also",
    "start": "4050880",
    "end": "4057599"
  },
  {
    "text": "implemented if you only access the system through interactions which is kind of remarkable I",
    "start": "4057599",
    "end": "4063319"
  },
  {
    "text": "think um so so the way it works is um so let's recall our backup formulas for Q",
    "start": "4063319",
    "end": "4069760"
  },
  {
    "text": "pi and qar um so we can um so we can in both",
    "start": "4069760",
    "end": "4078599"
  },
  {
    "text": "cases we have this a certain quantity inside an expectation in both both cases",
    "start": "4078599",
    "end": "4083680"
  },
  {
    "text": "we can compute an unbiased estimator um of the right of that quantity inside the",
    "start": "4083680",
    "end": "4089079"
  },
  {
    "text": "expectation just using a single sample meaning uh if we have uh if we sampled",
    "start": "4089079",
    "end": "4094839"
  },
  {
    "text": "some data from our system um using any old policy uh then uh we can get an unbiased estimator",
    "start": "4094839",
    "end": "4102400"
  },
  {
    "text": "of the quantity on the right hand side of those expectations I mean the quantity on inside of the",
    "start": "4102400",
    "end": "4108400"
  },
  {
    "text": "right hand expectations so basically we can do an approximate version of this uh Bellman",
    "start": "4108400",
    "end": "4115560"
  },
  {
    "text": "backup uh which is unbiased um and uh even with this noise",
    "start": "4115560",
    "end": "4123318"
  },
  {
    "text": "so we're doing a noisy version of the Bellman backup even with this noise it can be proven that if you do if you",
    "start": "4123319",
    "end": "4128560"
  },
  {
    "text": "choose your step size appropriately with the right schedule you're still going to converge to um QP um or qar um depending",
    "start": "4128560",
    "end": "4138238"
  },
  {
    "text": "uh on which algorithm you're implementing okay so now well well I'll",
    "start": "4138239",
    "end": "4144838"
  },
  {
    "text": "say at this point that this is uh pretty much the fundamental idea and now you can uh you can come up with algorithms",
    "start": "4144839",
    "end": "4152920"
  },
  {
    "text": "uh that can be applied in the uh reinforcement learning setting where you're just accessing the system through sampling and you can also uh start",
    "start": "4152920",
    "end": "4160560"
  },
  {
    "text": "introducing function approximation here so in I haven't said anything about what the Q function is I've just told you",
    "start": "4160560",
    "end": "4167199"
  },
  {
    "text": "it's a function of state and action um but now we can start having neural network Q functions for",
    "start": "4167199",
    "end": "4173560"
  },
  {
    "text": "example um so uh so we can parameterize the Q function with the neural network",
    "start": "4173560",
    "end": "4179359"
  },
  {
    "text": "um let's call it Q Theta um and now um instead of doing the Bellman backup I",
    "start": "4179359",
    "end": "4185359"
  },
  {
    "text": "mean it doesn't make sense to do the Bellman backup exactly because we're not just setting the values of the neural",
    "start": "4185359",
    "end": "4190920"
  },
  {
    "text": "network output um the best we can do is try to um like encourage the neural network to have some output values so",
    "start": "4190920",
    "end": "4198520"
  },
  {
    "text": "what we do is instead of doing the um the way we do this backup is we set up a lease squares problem uh so we write",
    "start": "4198520",
    "end": "4205159"
  },
  {
    "text": "down this quadratic objective that says that the Q function should be approximately equal to the back upb",
    "start": "4205159",
    "end": "4211159"
  },
  {
    "text": "value and then we just minimize it with uh SGD um so one version of this algorithm",
    "start": "4211159",
    "end": "4218520"
  },
  {
    "text": "uh which was introduced about 10 years ago called neural fitted Q iteration um well it works exactly the way you'd",
    "start": "4218520",
    "end": "4224600"
  },
  {
    "text": "expect you you sample trajectories um using your current policy uh which might",
    "start": "4224600",
    "end": "4230320"
  },
  {
    "text": "be um determined by the Q function or it could be any old policy as it turns out",
    "start": "4230320",
    "end": "4236719"
  },
  {
    "text": "um and uh then you um you solve the lease squares problem where you're",
    "start": "4236719",
    "end": "4241840"
  },
  {
    "text": "trying to minimize um this quadratic um you you try to minimize this quadratic",
    "start": "4241840",
    "end": "4247960"
  },
  {
    "text": "error which is um based on the um Bellman backup the backup for qar",
    "start": "4247960",
    "end": "4255440"
  },
  {
    "text": "so um one thing I haven't mentioned so far is what do you actually use as your",
    "start": "4255440",
    "end": "4261360"
  },
  {
    "text": "policy so I said sample trajectory using your policy um so if you have a Q function you can turn it into a policy",
    "start": "4261360",
    "end": "4268600"
  },
  {
    "text": "um by just uh taking the action that has the highest Q value that's what you typically do so the Q function measures",
    "start": "4268600",
    "end": "4275159"
  },
  {
    "text": "the goodness of all your actions so you can easily turn that into a policy by taking your best action or by taking",
    "start": "4275159",
    "end": "4281080"
  },
  {
    "text": "actions um where uh the log probability is um proportional the goodness or",
    "start": "4281080",
    "end": "4286159"
  },
  {
    "text": "something like that so you you might take typically probability is uh is exponential of Q",
    "start": "4286159",
    "end": "4294320"
  },
  {
    "text": "value um over some kind of temperature parameter um that's called boltzman",
    "start": "4294320",
    "end": "4299520"
  },
  {
    "text": "exploration um whereas um if you use just the um greedy if you just take the",
    "start": "4299520",
    "end": "4304920"
  },
  {
    "text": "argmax that's called the greedy policy so um it turns out that with these kind of Q learning algorithms you don't have",
    "start": "4304920",
    "end": "4311320"
  },
  {
    "text": "to execute the greedy policy um to for learning to work um there's you actually",
    "start": "4311320",
    "end": "4317719"
  },
  {
    "text": "have some freedom in what policy you can execute um which is actually one very nice property of these algorithms that",
    "start": "4317719",
    "end": "4324920"
  },
  {
    "text": "you can use an exploration technique um which uh where your policy is actively",
    "start": "4324920",
    "end": "4330560"
  },
  {
    "text": "trying to reach um new states or do something new and uh still learn the correct uh still uh converge still move",
    "start": "4330560",
    "end": "4338239"
  },
  {
    "text": "towards qar or QP as the case may be okay so that's uh",
    "start": "4338239",
    "end": "4345880"
  },
  {
    "text": "so that's a um a very basic neural fitted Q iteration is sort of a basic way of doing this a more recent",
    "start": "4345880",
    "end": "4352400"
  },
  {
    "text": "algorithm that's gotten a lot of attention is the one that was um from M at all from Deep Mind uh which is",
    "start": "4352400",
    "end": "4359320"
  },
  {
    "text": "basically an online version of this algorithm uh with a with a couple of um useful tweaks in it so um and but",
    "start": "4359320",
    "end": "4367920"
  },
  {
    "text": "actually when you look at the two tricks they're actually kind of um very um they make a lot of sense if you just think",
    "start": "4367920",
    "end": "4374239"
  },
  {
    "text": "about what valuator is doing so uh one uh one technique is uh you use this uh",
    "start": "4374239",
    "end": "4380840"
  },
  {
    "text": "replay pool where it's a rolling history of your past data and um that's just the",
    "start": "4380840",
    "end": "4386239"
  },
  {
    "text": "data you're going to use to fit your Q function um so that makes sure you have like a representative sample of data um",
    "start": "4386239",
    "end": "4393360"
  },
  {
    "text": "to uh fit your Q function to and um the second the second uh idea is to use a",
    "start": "4393360",
    "end": "4399760"
  },
  {
    "text": "Target Network um so instead of using your Q current Q function and just doing Bellman backups on that um you have some",
    "start": "4399760",
    "end": "4407520"
  },
  {
    "text": "lagged version of your Q function so you have this target Network which is a copy of your Q function at some earlier time",
    "start": "4407520",
    "end": "4414280"
  },
  {
    "text": "and you use that in the backups so that also um if you think about value iteration uh you're trying to you have",
    "start": "4414280",
    "end": "4420000"
  },
  {
    "text": "your old Q function and you're trying to make the new one equal to the backup version of the old one so using the",
    "start": "4420000",
    "end": "4425239"
  },
  {
    "text": "target Network just is sort of the natural thing to do if you're trying to implement value iteration in an online",
    "start": "4425239",
    "end": "4432000"
  },
  {
    "text": "way um so and there have been many extensions for post since then I've got a bunch of citations at the bottom of",
    "start": "4432000",
    "end": "4438159"
  },
  {
    "text": "the slide um so this algorithm the d uh dqn algorithm uh is um is using the um",
    "start": "4438159",
    "end": "4447520"
  },
  {
    "text": "backup B which is the backup for qar um remember that I also introduced this",
    "start": "4447520",
    "end": "4453440"
  },
  {
    "text": "other backup BP which is the backup for qpy um so so there's another algorithm",
    "start": "4453440",
    "end": "4459719"
  },
  {
    "text": "like a very classic algorithm called sarsa uh which um is an online way of um",
    "start": "4459719",
    "end": "4465880"
  },
  {
    "text": "doing the bpy backup essentially um well it's sort of an online version of policy",
    "start": "4465880",
    "end": "4471280"
  },
  {
    "text": "iteration um uh but uh so it's it's actually um found to work as well um or",
    "start": "4471280",
    "end": "4478960"
  },
  {
    "text": "better than DQ well better than using the B backup in some settings um not all",
    "start": "4478960",
    "end": "4484480"
  },
  {
    "text": "settings so I think the jury's still out um on exactly um how these things",
    "start": "4484480",
    "end": "4490239"
  },
  {
    "text": "compare um but uh it's I think um it's worth considering both policy iteration",
    "start": "4490239",
    "end": "4496360"
  },
  {
    "text": "and value iteration and and all the different online versions of these algorithms and taking them seriously",
    "start": "4496360",
    "end": "4501480"
  },
  {
    "text": "because it's not clear right now exactly which are how how they all compare each",
    "start": "4501480",
    "end": "4507360"
  },
  {
    "text": "other in the function approximation setting okay so that's uh that's the",
    "start": "4507360",
    "end": "4515000"
  },
  {
    "text": "overview of all the technical parts and now I just have a couple conclusion",
    "start": "4515000",
    "end": "4520320"
  },
  {
    "text": "slides um so so let me just summarize the current state of affairs I",
    "start": "4520320",
    "end": "4525679"
  },
  {
    "text": "introduced uh two kinds of algorithms uh policy grading algorithms which explicitly represent a policy and",
    "start": "4525679",
    "end": "4531639"
  },
  {
    "text": "optimize it and um Q function learning algorithms which explicitly represent a",
    "start": "4531639",
    "end": "4537080"
  },
  {
    "text": "q function which is the goodness of different actions and use that to implicitly represent a policy um so so",
    "start": "4537080",
    "end": "4544639"
  },
  {
    "text": "policy gradient methods there's a lot of um uh so there have been some successes with different kinds different variants",
    "start": "4544639",
    "end": "4551239"
  },
  {
    "text": "of it so you have vanilla policy gradient methods um there is a recent paper um on this uh a3c method um which",
    "start": "4551239",
    "end": "4559600"
  },
  {
    "text": "is an async uh implementation of it uh which gets very good results um there's",
    "start": "4559600",
    "end": "4566960"
  },
  {
    "text": "also um another kind of methods are the natural policy gradient methods trust region methods oh so the video I showed",
    "start": "4566960",
    "end": "4573760"
  },
  {
    "text": "you was obtained using uh trust region policy optimization which is one of these in the second category so that",
    "start": "4573760",
    "end": "4580840"
  },
  {
    "text": "makes it um I think these trust region methods and natural policy gradient methods are are uh More Sample efficient",
    "start": "4580840",
    "end": "4586800"
  },
  {
    "text": "than the um vanilla methods because uh you end up um you're doing more than one",
    "start": "4586800",
    "end": "4592120"
  },
  {
    "text": "uh grading update with each little bit of data you collect so with the vanilla policy gradient you just compute one",
    "start": "4592120",
    "end": "4598760"
  },
  {
    "text": "little gradient estimate and then you throw it away with natural policy gradient you're solving a little optimization problem with it so you get",
    "start": "4598760",
    "end": "4605040"
  },
  {
    "text": "more juice out of it um so that's um that's what we have",
    "start": "4605040",
    "end": "4611840"
  },
  {
    "text": "in the policy gradient World um and uh in the Q function world we have uh the",
    "start": "4611840",
    "end": "4617679"
  },
  {
    "text": "dqn algorithm and some of its relatives um and these are sort of uh descendants",
    "start": "4617679",
    "end": "4625040"
  },
  {
    "text": "of value iteration um where you're approximating the Bellman backup using value iteration",
    "start": "4625040",
    "end": "4632280"
  },
  {
    "text": "um and then sarsa is um also it's related to policy",
    "start": "4632280",
    "end": "4638239"
  },
  {
    "text": "iteration um these are both different I mean these are uh estimating different they're",
    "start": "4638239",
    "end": "4645120"
  },
  {
    "text": "dealing with different Bellman equations so it's kind of interesting that both kinds of methods work and they all",
    "start": "4645120",
    "end": "4650159"
  },
  {
    "text": "they're both they have fairly similar behaviors as a turns out um so here's what I would say the um",
    "start": "4650159",
    "end": "4658960"
  },
  {
    "text": "here's how I would compare them and this is um like anecdotal evidence but uh I",
    "start": "4658960",
    "end": "4664159"
  },
  {
    "text": "think this is the consensus right now um the Q function methods are more sample efficient when they work um but uh they",
    "start": "4664159",
    "end": "4671639"
  },
  {
    "text": "don't work as generally as policy gradient methods and it's it's a little harder to figure out what's going on uh",
    "start": "4671639",
    "end": "4677120"
  },
  {
    "text": "when they don't work um and that kind of makes sense because in the policy gradient methods you're optimizing exactly the thing you care about with",
    "start": "4677120",
    "end": "4684000"
  },
  {
    "text": "gradient descent whereas with Q function methods you're doing something indirect where you're optim you're trying to",
    "start": "4684000",
    "end": "4689639"
  },
  {
    "text": "learn a q function and then you're hoping that it gives you a good policy um and yeah so I would also point",
    "start": "4689639",
    "end": "4696840"
  },
  {
    "text": "out that there um there are also some confounds so it's hard to make a good conclusion at this point because people",
    "start": "4696840",
    "end": "4703239"
  },
  {
    "text": "use um uh different um like time Horizons in the policy gradient methods versus the Q",
    "start": "4703239",
    "end": "4709760"
  },
  {
    "text": "function methods so they do one step look ahads on the Q functions and multi-step look ahads on the policy",
    "start": "4709760",
    "end": "4715679"
  },
  {
    "text": "gradients so it's not clear if the extra if the differences come from like using different time Horizons or um some",
    "start": "4715679",
    "end": "4723639"
  },
  {
    "text": "differences in how the algorithms are working because you're either doing regression for a q function versus uh",
    "start": "4723639",
    "end": "4729920"
  },
  {
    "text": "learning a policy using policy gradients um so just to summarize it I",
    "start": "4729920",
    "end": "4735600"
  },
  {
    "text": "would say here here are some of our core model free reinforcement learning algorithms and uh they oh whoops I'm",
    "start": "4735600",
    "end": "4744280"
  },
  {
    "text": "missing a word in the First Column which I think should say uh like reliability",
    "start": "4744280",
    "end": "4750080"
  },
  {
    "text": "and robustness uh so this just means like is it going to work on um new problems without um like without",
    "start": "4750080",
    "end": "4757520"
  },
  {
    "text": "parameter tuning um or is it going to um mysteriously either work or not work um",
    "start": "4757520",
    "end": "4764159"
  },
  {
    "text": "so this this would be my um slightly sloppy summary of um all these different",
    "start": "4764159",
    "end": "4771400"
  },
  {
    "text": "algorithms I would say there's still some room for improvement um there might be some improvements in the basic",
    "start": "4771400",
    "end": "4777199"
  },
  {
    "text": "methods because uh there's some nice properties of the Q function methods um that we don't have in the policy",
    "start": "4777199",
    "end": "4783159"
  },
  {
    "text": "gradient methods like you can easily do off you can easily um explore with a",
    "start": "4783159",
    "end": "4788440"
  },
  {
    "text": "different policy than the one that you're um learning the Q function for and that's really important um you can",
    "start": "4788440",
    "end": "4794840"
  },
  {
    "text": "do that very easily with policy grading methods um whereas the policy grading methods just seem like they're more um",
    "start": "4794840",
    "end": "4801960"
  },
  {
    "text": "you can just apply them and they're like more likely to work and uh it's well",
    "start": "4801960",
    "end": "4807080"
  },
  {
    "text": "understood what's going on so I think yeah there's still I don't",
    "start": "4807080",
    "end": "4812239"
  },
  {
    "text": "know if it's possible to get the best of both worlds but that's uh that's the Hope um and that's it for my talk thank",
    "start": "4812239",
    "end": "4819600"
  },
  {
    "text": "you",
    "start": "4819600",
    "end": "4822600"
  },
  {
    "text": "any",
    "start": "4828639",
    "end": "4830920"
  },
  {
    "text": "questions oh yeah so in model-based reinforcement learning uh what lines of research do I find most",
    "start": "4839320",
    "end": "4846400"
  },
  {
    "text": "interesting I think the work uh from my colleagues on guided policy search is very nice so I would say that's a kind",
    "start": "4846400",
    "end": "4852120"
  },
  {
    "text": "of modelbased reinforcement learning um I also like um there's some methods that",
    "start": "4852120",
    "end": "4859239"
  },
  {
    "text": "are using the model for faster learning like for variance reduction so there's a paper called stochastic value gradients",
    "start": "4859239",
    "end": "4865760"
  },
  {
    "text": "that I like a lot um I think it's a pretty wide openen",
    "start": "4865760",
    "end": "4872199"
  },
  {
    "text": "area so I don't think there have been a lot of really compelling results uh where you're able to learn extremely",
    "start": "4872199",
    "end": "4879199"
  },
  {
    "text": "fast uh like you're able to learn with much better sample efficiency using a model so it's seems like that should be",
    "start": "4879199",
    "end": "4885480"
  },
  {
    "text": "possible but I don't think it's been demonstrated um yet so maybe in the next",
    "start": "4885480",
    "end": "4891280"
  },
  {
    "text": "couple years we'll see that happen hello uh",
    "start": "4891280",
    "end": "4898040"
  },
  {
    "text": "hi uh thanks for the talk so I have a question is that is it true or not true that um most of this problem require",
    "start": "4898040",
    "end": "4905239"
  },
  {
    "text": "some kind of simulated uh world to to uh run experiments in the episodes right oh",
    "start": "4905239",
    "end": "4912360"
  },
  {
    "text": "yeah so um are you asking um does this work in the real world is that the",
    "start": "4912360",
    "end": "4917920"
  },
  {
    "text": "question or does um yeah I would say um it it does work if you have a lot of",
    "start": "4917920",
    "end": "4923679"
  },
  {
    "text": "patience and you're willing to execute this thing for a while so the um Locomotion results I showed um add up to",
    "start": "4923679",
    "end": "4930520"
  },
  {
    "text": "about two weeks of real time uh so it's actually not that bad especially when you consider uh that babies toddlers",
    "start": "4930520",
    "end": "4937560"
  },
  {
    "text": "take a while to learn how to walk properly even though Evolution already puts in a lot of uh built-in information",
    "start": "4937560",
    "end": "4944000"
  },
  {
    "text": "um so uh i' I'd say um maybe yeah I'd",
    "start": "4944000",
    "end": "4949400"
  },
  {
    "text": "say it it's it can be run in the real world some of my colleagues in Berkeley are doing uh some experiments where they",
    "start": "4949400",
    "end": "4955199"
  },
  {
    "text": "are running just regular reinforcement learning algorithms in the real world um very brave um but uh I hope to see some",
    "start": "4955199",
    "end": "4963320"
  },
  {
    "text": "nice results from that soon thank you hi uh thanks for your talk here on",
    "start": "4963320",
    "end": "4970639"
  },
  {
    "text": "the other side here um I was wondering what was your intuition on the Lost",
    "start": "4970639",
    "end": "4976880"
  },
  {
    "text": "surface of those uh deep reinforcement learning optimization problems and um",
    "start": "4976880",
    "end": "4983760"
  },
  {
    "text": "maybe especially how it evolves in the from the as the policy learns and I",
    "start": "4983760",
    "end": "4989239"
  },
  {
    "text": "should specify in the policy G in case so I think the situation is a",
    "start": "4989239",
    "end": "4994400"
  },
  {
    "text": "little bit different in reinforcement learning from in supervised learning uh so in reinforcement learning the uh loss",
    "start": "4994400",
    "end": "5001239"
  },
  {
    "text": "you have um you have one kind of local Minima um in policy space um so for",
    "start": "5001239",
    "end": "5009679"
  },
  {
    "text": "example um let's say you want your so I'm going keep going back to The Locomotion example because I spent a lot",
    "start": "5009679",
    "end": "5015199"
  },
  {
    "text": "of time on it but uh let's say you want your robot to walk um there's one local minimum where it just stands and it",
    "start": "5015199",
    "end": "5021719"
  },
  {
    "text": "doesn't bother to walk because there's too much penalty for falling over and there's another local minimum where it just Dives forward because uh it gets a",
    "start": "5021719",
    "end": "5029000"
  },
  {
    "text": "little bit of reward for that before it falls to its Doom um so uh so even so I",
    "start": "5029000",
    "end": "5036440"
  },
  {
    "text": "think that that's actually the the hard part about the uh like the optimization",
    "start": "5036440",
    "end": "5041520"
  },
  {
    "text": "problem is actually Define is because of the different be space of behaviors and",
    "start": "5041520",
    "end": "5046800"
  },
  {
    "text": "actually has nothing to do with the neural network um so I've also found that um yeah it matters surprisingly",
    "start": "5046800",
    "end": "5053520"
  },
  {
    "text": "little what kind of architecture you use um like what kind of neural network architecture you use because I think the",
    "start": "5053520",
    "end": "5060000"
  },
  {
    "text": "most of the hardness and the weirdness of the problem comes from uh like what the Behavior space looks like rather",
    "start": "5060000",
    "end": "5066120"
  },
  {
    "text": "than what the actual numerical optimization landscape looks like cool thank",
    "start": "5066120",
    "end": "5073280"
  },
  {
    "text": "you so uh there are many problems where uh the reward is only observed uh at the",
    "start": "5073280",
    "end": "5080360"
  },
  {
    "text": "end of the task so in the final in the terminal state in each episode uh and you don't see rewards uh in intermediate",
    "start": "5080360",
    "end": "5087400"
  },
  {
    "text": "States so how much harder do these problems become for deep reinforcement learning in your experience thanks yeah",
    "start": "5087400",
    "end": "5093800"
  },
  {
    "text": "so you have if you don't get the reward until the end then um then you can't",
    "start": "5093800",
    "end": "5101280"
  },
  {
    "text": "um well then it's probably it might be harder to learn yeah I I don't have anything uh anything precise to say",
    "start": "5101280",
    "end": "5108400"
  },
  {
    "text": "about that I think it's going to be harder if you have less if your rewards are further away yeah so so for example",
    "start": "5108400",
    "end": "5115159"
  },
  {
    "text": "for your uh in your video for the last example of getting up and getting the head above a certain height for example",
    "start": "5115159",
    "end": "5122239"
  },
  {
    "text": "that could be one where you only get a plus one if you're above and you don't get anything below are you doing",
    "start": "5122239",
    "end": "5128159"
  },
  {
    "text": "something that was kind of if you get your head higher then you still get something partial yeah so I think we uh",
    "start": "5128159",
    "end": "5134800"
  },
  {
    "text": "came up with a reward like distance from height squared um which made the problem easier um yeah the problem would have",
    "start": "5134800",
    "end": "5141000"
  },
  {
    "text": "been a lot harder if you get zero reward until you get your head above the height um and it's actually um that would be a",
    "start": "5141000",
    "end": "5147600"
  },
  {
    "text": "problem of exploration which is that you have to um explore all the different states be to figure out where you're",
    "start": "5147600",
    "end": "5154400"
  },
  {
    "text": "going to get good reward thanks okay uh one last question okay uh",
    "start": "5154400",
    "end": "5162639"
  },
  {
    "text": "so I have a question about how do you choose to quantize your space time because in your Locomotion example you",
    "start": "5162639",
    "end": "5167880"
  },
  {
    "text": "clearly has the continuous system right uh yeah so it's actually",
    "start": "5167880",
    "end": "5173600"
  },
  {
    "text": "really important how you Discord eyesee time like what time step you use because um if because the algorithm has um I",
    "start": "5173600",
    "end": "5182239"
  },
  {
    "text": "mean the algorithm does care about what the time step is so it's not like um",
    "start": "5182239",
    "end": "5187679"
  },
  {
    "text": "yeah because you um you have discount factors and you're also sampling a different action at every time step so",
    "start": "5187679",
    "end": "5195560"
  },
  {
    "text": "um yeah so if you choose too small of a Time step then uh you then the rewards",
    "start": "5195560",
    "end": "5202400"
  },
  {
    "text": "will be delayed by more time steps so that makes the like the credit assignment harder and also um your",
    "start": "5202400",
    "end": "5209520"
  },
  {
    "text": "exploration will be more like a random walk because you're changing your mind really frequently",
    "start": "5209520",
    "end": "5214840"
  },
  {
    "text": "so yeah the time step is pretty important and I'd say that's um that's a flaw in current methods okay thank",
    "start": "5214840",
    "end": "5223800"
  },
  {
    "text": "[Applause] you thank you so take a short break we",
    "start": "5225070",
    "end": "5232360"
  },
  {
    "text": "convene in 15 minutes",
    "start": "5232360",
    "end": "5236400"
  }
]