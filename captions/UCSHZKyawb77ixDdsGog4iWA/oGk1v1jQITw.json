[
  {
    "text": "thank you everybody thanks for coming back very soon after lunch I'll try to make it entertaining to avoid some post",
    "start": "30",
    "end": "7589"
  },
  {
    "text": "food coma so I actually have a lot - OH - being here - Andrew and Chris and my",
    "start": "7589",
    "end": "14040"
  },
  {
    "text": "PhD at Stanford here it's it's really it's always fun to be back I figured",
    "start": "14040",
    "end": "19470"
  },
  {
    "text": "there's a going to be a broad range of capabilities in the room so I'm sorry I",
    "start": "19470",
    "end": "25859"
  },
  {
    "text": "will probably bore some of you for the first two-thirds of the talk because I'll go over the basics of what's NLP",
    "start": "25859",
    "end": "33840"
  },
  {
    "text": "when natural language processing what's deep learning and what's really at the intersection of the two and then the",
    "start": "33840",
    "end": "39540"
  },
  {
    "text": "last third I will talk a little bit about some exciting new research that's happening right now so let's get started",
    "start": "39540",
    "end": "46829"
  },
  {
    "text": "with what is natural language processing it's really a feel at the intersection of computer science AI and linguistics",
    "start": "46829",
    "end": "53760"
  },
  {
    "text": "and you could define a lot of goals and a lot of these statements here we could really talk and philosophize a lot about",
    "start": "53760",
    "end": "60030"
  },
  {
    "text": "but I'll move through them pretty quickly for me the goal of natural",
    "start": "60030",
    "end": "65369"
  },
  {
    "text": "language processing is for computers to process or scare quotes understand natural language in order to perform",
    "start": "65369",
    "end": "71040"
  },
  {
    "text": "tasks that are actually useful for people such as question answering the",
    "start": "71040",
    "end": "76439"
  },
  {
    "text": "caveat here is that really fully understanding and representing the meaning of language or even defining it",
    "start": "76439",
    "end": "83430"
  },
  {
    "text": "is quite an elusive goal so whenever I say the model understands I'm sorry I",
    "start": "83430",
    "end": "89460"
  },
  {
    "text": "shouldn't say that really these models don't understand in the sense that we understand language anything so whenever somebody says they",
    "start": "89460",
    "end": "96780"
  },
  {
    "text": "can read or represent the full meaning and its entire glory it's it's usually",
    "start": "96780",
    "end": "101880"
  },
  {
    "text": "not quite true really perfect language understanding is in some sense AI",
    "start": "101880",
    "end": "106979"
  },
  {
    "text": "complete in the sense that you need to understand all of visual inputs and thought and and a lot of other complex",
    "start": "106979",
    "end": "113790"
  },
  {
    "text": "things so a little more concretely as we try to tackle this overall problem of",
    "start": "113790",
    "end": "119070"
  },
  {
    "text": "understanding language what are sort of the different levels that we often look at it often and for many people starts",
    "start": "119070",
    "end": "126090"
  },
  {
    "text": "at speech and then once you have speech you might say alright now I know what phonemes are smaller parts of words I",
    "start": "126090",
    "end": "132569"
  },
  {
    "text": "understand words form Nets morphology or morphological analysis once I know what",
    "start": "132569",
    "end": "137970"
  },
  {
    "text": "the meaning of words are I might try to understand how they're put together in grammatical ways such that the sentences",
    "start": "137970",
    "end": "145170"
  },
  {
    "text": "are understandable or at least grammatically correct too a lot of speakers of the language once we go and",
    "start": "145170",
    "end": "152430"
  },
  {
    "text": "we understand the structure we actually want to get to the meaning and that's really where I think most of the interesting most of my interests lies",
    "start": "152430",
    "end": "160530"
  },
  {
    "text": "and semantic interpretation actually trying to get to the meaning in some useful capacity and then after that we",
    "start": "160530",
    "end": "166770"
  },
  {
    "text": "might say well if we understand now the meaning of the whole sentence what's how do we actually interact what's the discourse how do we have you",
    "start": "166770",
    "end": "173670"
  },
  {
    "text": "know spoken dialogue system and things like that where deep learning has really improved the state of the art",
    "start": "173670",
    "end": "180210"
  },
  {
    "text": "significantly is really in speech recognition and syntax and semantics and",
    "start": "180210",
    "end": "185310"
  },
  {
    "text": "the interesting thing is that we're kind of actually skipping some of these levels deep learning doesn't require",
    "start": "185310",
    "end": "191430"
  },
  {
    "text": "often morphological analysis to create very useful systems and in some cases",
    "start": "191430",
    "end": "196440"
  },
  {
    "text": "actually skips syntactic analysis entirely as well it doesn't have to know about the grammar it doesn't have to be",
    "start": "196440",
    "end": "201870"
  },
  {
    "text": "taught about what mound phrases are prepositional phrases it can actually get straight to some semantically useful",
    "start": "201870",
    "end": "208050"
  },
  {
    "text": "tasks right away and that's going to be one of the sort of advantages that we",
    "start": "208050",
    "end": "213570"
  },
  {
    "text": "don't have to actually be as inspired by linguistics as traditional natural language processing had to be so why is",
    "start": "213570",
    "end": "220440"
  },
  {
    "text": "NLP hard well there's a lot of complexity in representing and learning",
    "start": "220440",
    "end": "225810"
  },
  {
    "text": "and especially using linguistics situational world and visual knowledge really all of these are connected when",
    "start": "225810",
    "end": "231690"
  },
  {
    "text": "it gets to the meaning of language to really understand what red means can you do that without visual understanding for",
    "start": "231690",
    "end": "238050"
  },
  {
    "text": "instance if you have for instance this sentence here Jane hit June and then she",
    "start": "238050",
    "end": "243750"
  },
  {
    "text": "fell or and then she ran depending on which verb comes after she the",
    "start": "243750",
    "end": "251240"
  },
  {
    "text": "definition the meaning of she actually changes and this is one subtask you might look at so called an F or a",
    "start": "251240",
    "end": "258180"
  },
  {
    "text": "resolution or cor efference resolution in general where you try to understand who does she actually refer to and it",
    "start": "258180",
    "end": "263370"
  },
  {
    "text": "really depends on the meaning again somewhat scare quotes here of the verb that follows this pronoun",
    "start": "263370",
    "end": "273060"
  },
  {
    "text": "similarly there's a lot of ambiguity so here we have a very simple sentence for words",
    "start": "273060",
    "end": "278320"
  },
  {
    "text": "I made her duck now that simple sentence can actually have at least four",
    "start": "278320",
    "end": "284170"
  },
  {
    "text": "different meanings if you can think about it for a little bit right you made her a duck that she loves for Christmas",
    "start": "284170",
    "end": "290950"
  },
  {
    "text": "as for dinner you made her dock like me just now and so on there are actually four different",
    "start": "290950",
    "end": "297040"
  },
  {
    "text": "meanings and to know which one requires in some sense situational awareness or",
    "start": "297040",
    "end": "302350"
  },
  {
    "text": "knowledge to really disambiguate what what is meant here so that's sort of the",
    "start": "302350",
    "end": "308740"
  },
  {
    "text": "high level of NLP now where does it actually become useful in terms of applications well they actually range",
    "start": "308740",
    "end": "315010"
  },
  {
    "text": "from very simple things that we kind of assume or you're given now we use them all the time every day to more and more",
    "start": "315010",
    "end": "320470"
  },
  {
    "text": "complex and then also more in the realm of research the simple ones are things like spell checking or key word search",
    "start": "320470",
    "end": "327400"
  },
  {
    "text": "and finding synonyms and ophisaurus then the meaty medium sort of difficulty ones",
    "start": "327400",
    "end": "333850"
  },
  {
    "text": "are the extract information from websites trying to extract sort of product prices or dates and locations",
    "start": "333850",
    "end": "339669"
  },
  {
    "text": "people or company names are called named entity recognition you can go a little bit above that and try to classify sort",
    "start": "339669",
    "end": "347620"
  },
  {
    "text": "of reading levels for school text for instance or do sentiment analysis that can be helpful if you have a lot of",
    "start": "347620",
    "end": "353919"
  },
  {
    "text": "customer emails that come in and you want to prioritize highly the ones of customers for really really review right",
    "start": "353919",
    "end": "360010"
  },
  {
    "text": "now and then the really hard ones and I think in some sense the most interesting ones are machine translation trying to",
    "start": "360010",
    "end": "367780"
  },
  {
    "text": "actually be able to translate between all the different languages in the world question answering clearly something",
    "start": "367780",
    "end": "373900"
  },
  {
    "text": "that is a very exciting and useful piece of technology especially over very large",
    "start": "373900",
    "end": "380080"
  },
  {
    "text": "complex domains can be used to automated for automated email replies I know pretty much everybody here would love to",
    "start": "380080",
    "end": "386740"
  },
  {
    "text": "have some simple automated email reply system and then spoken dialogue systems",
    "start": "386740",
    "end": "392320"
  },
  {
    "text": "bots are very hip right now these are all sort of complex things that are still in the realm of research to do",
    "start": "392320",
    "end": "397870"
  },
  {
    "text": "them really well we're making huge progress with deep learning on these three but there's still nowhere near human",
    "start": "397870",
    "end": "404890"
  },
  {
    "text": "accuracy so let's look at the",
    "start": "404890",
    "end": "411040"
  },
  {
    "text": "representations I mention you know we have morphology and words and syntax and",
    "start": "411040",
    "end": "416680"
  },
  {
    "text": "semantics and so on we can look at one example a namely machine translation and",
    "start": "416680",
    "end": "422460"
  },
  {
    "text": "look at how did people try to solve this problem of machine translation well it",
    "start": "422460",
    "end": "428710"
  },
  {
    "text": "turns out they actually tried all these different levels with varying degrees of success you can try to have a direct",
    "start": "428710",
    "end": "434950"
  },
  {
    "text": "translation of words to other words the problem is that is often a very tricky mapping one the meaning of one word in",
    "start": "434950",
    "end": "440440"
  },
  {
    "text": "English might have three different words in German and vice versa you can have three of the same words in",
    "start": "440440",
    "end": "446860"
  },
  {
    "text": "English meaning all this single same word in German for instance so then people said well let's try to maybe do",
    "start": "446860",
    "end": "452920"
  },
  {
    "text": "some tactic transfer where we have whole phrases like to kick the bucket just means stab them in German okay not a fun",
    "start": "452920",
    "end": "458710"
  },
  {
    "text": "example and then semantic transfer might be well let's try to find a logical representation of the whole sentence the",
    "start": "458710",
    "end": "464560"
  },
  {
    "text": "actual meaning in some human understandable form and and try to just find another surface representation of",
    "start": "464560",
    "end": "470770"
  },
  {
    "text": "that now of course that will also get rid of a lot of the subtleties of language and so they're tricky problems",
    "start": "470770",
    "end": "478990"
  },
  {
    "text": "in all these kinds of representations now the question is what does deep learning do you've already saw at least",
    "start": "478990",
    "end": "485290"
  },
  {
    "text": "two methods standard neural networks before and convolutional neural networks",
    "start": "485290",
    "end": "490480"
  },
  {
    "text": "for vision and in some sense there's going to be a huge similarity here to",
    "start": "490480",
    "end": "496419"
  },
  {
    "text": "these methods because just like images that are essentially a long list of",
    "start": "496419",
    "end": "502360"
  },
  {
    "text": "numbers the vector and standard neural networks where the hidden state is also",
    "start": "502360",
    "end": "508000"
  },
  {
    "text": "just a vector or a list of numbers that is also going to be the main representation that we will use",
    "start": "508000",
    "end": "514000"
  },
  {
    "text": "throughout for characters for words for short phrases for sentences and in some",
    "start": "514000",
    "end": "519849"
  },
  {
    "text": "cases for entire documents they will all be vectors and with that we are sort of",
    "start": "519849",
    "end": "526630"
  },
  {
    "text": "finishing up the whirlwind of what's NLP of course you could give an entire lecture on all like almost every single",
    "start": "526630",
    "end": "533470"
  },
  {
    "text": "slide I just gave we're very a very high level but we'll continue at that speed to try to squeeze",
    "start": "533470",
    "end": "540540"
  },
  {
    "text": "this complex deep learning for NLP subject area into an hour and a half I think there are two most two of the most",
    "start": "540540",
    "end": "547840"
  },
  {
    "text": "important basic Lego blocks that you nowadays want to know in order to be able to sort of creatively play around",
    "start": "547840",
    "end": "554200"
  },
  {
    "text": "with more complex models and those are going to be word vectors and sequence",
    "start": "554200",
    "end": "560320"
  },
  {
    "text": "models namely recurrent neural networks and I kind of split this into words",
    "start": "560320",
    "end": "566470"
  },
  {
    "text": "sentences and multiple sentences but really you could use recurrent neural networks for shorter phrases as well as",
    "start": "566470",
    "end": "573160"
  },
  {
    "text": "multiple sentences but in many cases we'll see that they have some limitations as you move to longer and",
    "start": "573160",
    "end": "578830"
  },
  {
    "text": "longer sequences and just use the default neural network sequence models",
    "start": "578830",
    "end": "583890"
  },
  {
    "text": "alright so let's start with words and maybe one last blast from the past here",
    "start": "583890",
    "end": "590280"
  },
  {
    "text": "to represent the meaning of words we actually used to use a taxonomy like",
    "start": "590280",
    "end": "595720"
  },
  {
    "text": "word net that kind of defines each word in relationship to lots of other ones so",
    "start": "595720",
    "end": "601540"
  },
  {
    "text": "you can for instance define hyper names and is a relationships you might say the word Panda for instance in its first",
    "start": "601540",
    "end": "608290"
  },
  {
    "text": "meaning as a noun basically goes through this complex tags directed acyclic graph",
    "start": "608290",
    "end": "615610"
  },
  {
    "text": "most of it is roughly just a tree and in the end like everything it is an entity but it's actually a physical entity a",
    "start": "615610",
    "end": "621640"
  },
  {
    "text": "type of object it's a whole object it's a living thing it's an organism animal and so on so you basically can define a",
    "start": "621640",
    "end": "627880"
  },
  {
    "text": "word like this and another way at each node of this tree you actually have so called sunset so synonym sets here's an",
    "start": "627880",
    "end": "634810"
  },
  {
    "text": "example for the synonym set of the word good good can have a lot of different",
    "start": "634810",
    "end": "640630"
  },
  {
    "text": "meanings can actually be both an adjective and as well as an adverb as",
    "start": "640630",
    "end": "645820"
  },
  {
    "text": "well as a noun now what are the problems with this kind of discrete representation well they can be great as",
    "start": "645820",
    "end": "654220"
  },
  {
    "text": "a resource of your human you want to find synonyms but they're ever they're never going to be quite sufficient to",
    "start": "654220",
    "end": "662290"
  },
  {
    "text": "capture all the nuances that we have in language so for instance the synonyms",
    "start": "662290",
    "end": "667450"
  },
  {
    "text": "here for good were adapt Axford practice proficient and skillful but of course",
    "start": "667450",
    "end": "673770"
  },
  {
    "text": "you would use these words in slightly different contexts you would not use the",
    "start": "673770",
    "end": "680140"
  },
  {
    "text": "word expert in exactly this all the same context as you would use the meaning of good or the word good likewise it will",
    "start": "680140",
    "end": "688150"
  },
  {
    "text": "be missing a lot of new words language is this interesting living organism we change it all the time you might have",
    "start": "688150",
    "end": "693700"
  },
  {
    "text": "some kids they say Yolo and all of a sudden you know you need to update your dictionary likewise maybe in Silicon",
    "start": "693700",
    "end": "700960"
  },
  {
    "text": "Valley you might see ninja a lot and now you need to update your dictionary again and that is basically going to be a",
    "start": "700960",
    "end": "706390"
  },
  {
    "text": "Sisyphus job right nobody will ever be able to really capture all the meanings and and this living breathing organism",
    "start": "706390",
    "end": "713830"
  },
  {
    "text": "that languages so it's also very subjective some people might think ninja",
    "start": "713830",
    "end": "719740"
  },
  {
    "text": "should just be deleted from the dictionary and I don't want to include it I'll just think nifty or badass is",
    "start": "719740",
    "end": "725470"
  },
  {
    "text": "kind of a silly word and should not be included in a proper dictionary but it's being used in real language and so on it",
    "start": "725470",
    "end": "730810"
  },
  {
    "text": "requires human labor as soon as you change your domain you have to ask people to update it and it's also hard",
    "start": "730810",
    "end": "737020"
  },
  {
    "text": "to compute accurate word similarities some of these words are subtly different and it's really a continuum in which we",
    "start": "737020",
    "end": "742930"
  },
  {
    "text": "can measure their similarities so instead what we're going to use and what",
    "start": "742930",
    "end": "748450"
  },
  {
    "text": "is also the first step for deep learning will actually realize it's not quite deep learning in many cases but it is",
    "start": "748450",
    "end": "755590"
  },
  {
    "text": "sort of the first step to use deep learning and NLP is we will use distributional similarities so what does",
    "start": "755590",
    "end": "761530"
  },
  {
    "text": "that mean basically the idea is that we'll use the neighbors of a word to represent that word itself it's a pretty",
    "start": "761530",
    "end": "769510"
  },
  {
    "text": "old concept and here's an example for instance for the word banking we might actually represent banking in terms of",
    "start": "769510",
    "end": "776380"
  },
  {
    "text": "all these other words that are around it so let's do a very simple example where",
    "start": "776380",
    "end": "783880"
  },
  {
    "text": "we look at a window around each word and so here the window length that's just",
    "start": "783880",
    "end": "789040"
  },
  {
    "text": "for simplicity say it's one we represent each word only with the words one to left and one to the right of it we'll",
    "start": "789040",
    "end": "795250"
  },
  {
    "text": "just use the symmetric context around each word and here's a simple example",
    "start": "795250",
    "end": "800400"
  },
  {
    "text": "corpus so if the three sentences in my corpus of course we would always want to use",
    "start": "800400",
    "end": "805510"
  },
  {
    "text": "corpora with billions of words instead of just a couple but just to give you an idea of what's being captured in these",
    "start": "805510",
    "end": "811840"
  },
  {
    "text": "word vectors is I like people earning I like NLP and I enjoy flying and now this",
    "start": "811840",
    "end": "818560"
  },
  {
    "text": "is it's very simple so-called corcoran statistic you'll just simply see here I",
    "start": "818560",
    "end": "823810"
  },
  {
    "text": "for instance appears twice in its window size of one here the word like isn't its",
    "start": "823810",
    "end": "829630"
  },
  {
    "text": "window and its context and the word enjoy is once in its context and for",
    "start": "829630",
    "end": "835000"
  },
  {
    "text": "like you have twice to its left I and once deep and once NLP it turns out if",
    "start": "835000",
    "end": "843820"
  },
  {
    "text": "you just take those vectors now this could be a vector of presentation just each row could be a vector",
    "start": "843820",
    "end": "849100"
  },
  {
    "text": "representation for words unfortunately as soon as your vocabulary increases that vector dimensionality would change",
    "start": "849100",
    "end": "855430"
  },
  {
    "text": "and hence you have to retrain your whole model it's also very sparse and really",
    "start": "855430",
    "end": "861040"
  },
  {
    "text": "it's going to be somewhat noisy if you use that vector now another better thing",
    "start": "861040",
    "end": "866560"
  },
  {
    "text": "to do might be to run SVD or something simple like say dimensionality reduction on such a co-occurrence matrix and that",
    "start": "866560",
    "end": "874000"
  },
  {
    "text": "actually gives you a reasonable first approximation to word vectors very old method works reasonably well now what",
    "start": "874000",
    "end": "881680"
  },
  {
    "text": "works even better than simple PCA is actually a model introduced by Thomas McAuliffe in 2013 called word Tyvek so",
    "start": "881680",
    "end": "889960"
  },
  {
    "text": "instead of capturing Corcoran's counts directly out of a matrix like that you'll actually go through each window",
    "start": "889960",
    "end": "896230"
  },
  {
    "text": "in a large corpus and try to predict a word that's in the center of each window",
    "start": "896230",
    "end": "901300"
  },
  {
    "text": "and use that to predict the words around it that way you can very quickly train",
    "start": "901300",
    "end": "906910"
  },
  {
    "text": "you can train almost on line though few people do this and and add words to",
    "start": "906910",
    "end": "912760"
  },
  {
    "text": "vocabulary very quickly in this zooming fashion so now let's look a little bit",
    "start": "912760",
    "end": "918250"
  },
  {
    "text": "at this model where Tyvek because it's first very simple NLP model and to sort",
    "start": "918250",
    "end": "925840"
  },
  {
    "text": "of is very instructive we won't go into too many details but at least look at a couple of equations so again",
    "start": "925840",
    "end": "932320"
  },
  {
    "text": "main goal is to breeding words in a window of some length that we define em type or",
    "start": "932320",
    "end": "938199"
  },
  {
    "text": "parameter of every word now the objective function will essentially try to maximize here the log probability of",
    "start": "938199",
    "end": "944709"
  },
  {
    "text": "any of these contacts words given the Center word so we go through our entire corpus T very long sequence and at each",
    "start": "944709",
    "end": "952899"
  },
  {
    "text": "time step J we will basically look at all the words in the context of the",
    "start": "952899",
    "end": "959620"
  },
  {
    "text": "current word T and basically try to maximize here this probability of trying",
    "start": "959620",
    "end": "967000"
  },
  {
    "text": "to be able to predict that word that is around the current word T and theta are",
    "start": "967000",
    "end": "973480"
  },
  {
    "text": "all the parameters namely all the word vectors that we'd want to optimize so now how do we actually define this",
    "start": "973480",
    "end": "979269"
  },
  {
    "text": "probability P here the simplest way to do this and this is not the actual way",
    "start": "979269",
    "end": "985389"
  },
  {
    "text": "but it's the simplest and first to understand and derive this model is with",
    "start": "985389",
    "end": "990430"
  },
  {
    "text": "this very simple inner product here and that's why we can't quite call a deep there's not going to be many layers of",
    "start": "990430",
    "end": "997209"
  },
  {
    "text": "nonlinearities like we see in deep neural networks to be just a simple inner product and the higher debt in a",
    "start": "997209",
    "end": "1002370"
  },
  {
    "text": "product is the more likely these two will be predicting one another so here",
    "start": "1002370",
    "end": "1009839"
  },
  {
    "text": "see the context is the dissenter word sorry oh is the outside word and",
    "start": "1009839",
    "end": "1014930"
  },
  {
    "text": "basically this inner product the larger it is the more likely we were going to predict this and these are both just",
    "start": "1014930",
    "end": "1021180"
  },
  {
    "text": "standard and dimensional vectors and now in order to get a real probability we'll",
    "start": "1021180",
    "end": "1026850"
  },
  {
    "text": "essentially apply softmax to all the potential inner products that you might have in your vocabulary and one thing",
    "start": "1026850",
    "end": "1033329"
  },
  {
    "text": "you will notice here is well this denominator is actually going to be a",
    "start": "1033329",
    "end": "1038400"
  },
  {
    "text": "very large sum I will want to sum here overall potential inner products for every single window that would be true",
    "start": "1038400",
    "end": "1044459"
  },
  {
    "text": "slow so now the real methods that we would use we're going to are going to",
    "start": "1044459",
    "end": "1049860"
  },
  {
    "text": "approximate the sum in a variety of clever ways now I could literally talk",
    "start": "1049860",
    "end": "1055230"
  },
  {
    "text": "to next hour and a half just about how to optimize the details of this equation but then we'll all deplete our mental",
    "start": "1055230",
    "end": "1061260"
  },
  {
    "text": "energy for the rest of the day and so I'm just going to point you to the class I taught earlier this year so yes 24d we",
    "start": "1061260",
    "end": "1069210"
  },
  {
    "text": "we have lots of different slides that go into all the details of this equation how to approximate it and then how to",
    "start": "1069210",
    "end": "1075330"
  },
  {
    "text": "optimize it it's going to be very similar to the way we optimize any other neural network we're going to use",
    "start": "1075330",
    "end": "1081149"
  },
  {
    "text": "stochastic gradient descent we're going to look at mini batches of a couple of hundred windows at a time and an update",
    "start": "1081149",
    "end": "1088110"
  },
  {
    "text": "those word vectors and we're just going to take simple gradients of each of these vectors as we go through windows",
    "start": "1088110",
    "end": "1095190"
  },
  {
    "text": "in a large corpus all right now we briefly mentioned PCA like methods and",
    "start": "1095190",
    "end": "1102510"
  },
  {
    "text": "based on senior Lu decomposition often or standard a simple PCA now we also had",
    "start": "1102510",
    "end": "1108539"
  },
  {
    "text": "this word Tyvek model there's actually one model that combines the best of both",
    "start": "1108539",
    "end": "1113789"
  },
  {
    "text": "worlds namely glove or global vectors introduced by Geoffrey Pennington in 2014 and it has a very similar idea and",
    "start": "1113789",
    "end": "1122490"
  },
  {
    "text": "you'll notice here there's some similarity you have this inner product again for different pairs but this model",
    "start": "1122490",
    "end": "1128190"
  },
  {
    "text": "will actually go over the Corcoran's matrix once you have this Corcoran's matrix it's much more efficient to try",
    "start": "1128190",
    "end": "1133799"
  },
  {
    "text": "to predict once how often two words appear next to each other rather than do it 50 times each time that that pair",
    "start": "1133799",
    "end": "1142679"
  },
  {
    "text": "appears in an actual corpus so in some sense you can be more efficiently going through all the current statistics and",
    "start": "1142679",
    "end": "1149429"
  },
  {
    "text": "you're going to basically try to minimize the this this subtraction here",
    "start": "1149429",
    "end": "1156330"
  },
  {
    "text": "and what that basically means is that each inner product will try to approximate the log probability of these",
    "start": "1156330",
    "end": "1162630"
  },
  {
    "text": "two words actually co-occurring now you have this function here which",
    "start": "1162630",
    "end": "1169740"
  },
  {
    "text": "essentially will allow us to not overly weight certain pairs that occur very",
    "start": "1169740",
    "end": "1175860"
  },
  {
    "text": "very frequently the for instance co-occurs with lots of different words and you want to basically lower the",
    "start": "1175860",
    "end": "1181740"
  },
  {
    "text": "importance of all the words that Corker with that so you can train this very",
    "start": "1181740",
    "end": "1187169"
  },
  {
    "text": "fast it scales to gigantic corpora in fact we train this on common crawl which",
    "start": "1187169",
    "end": "1194640"
  },
  {
    "text": "is a really great data set of most of the internet it's many billions of tokens and it gets also very good",
    "start": "1194640",
    "end": "1202080"
  },
  {
    "text": "performance on small corpora because it makes use very efficiently of these Corcoran",
    "start": "1202080",
    "end": "1207240"
  },
  {
    "text": "statistics and that's essentially what words well word vectors are always capturing so if in one sentence you just",
    "start": "1207240",
    "end": "1213420"
  },
  {
    "text": "want to remember every time you hear word vectors in deep learning one they're not quite deep even though we",
    "start": "1213420",
    "end": "1220290"
  },
  {
    "text": "call them sort of step one of deep learning and to it they're really just capturing Corcoran's counts how often",
    "start": "1220290",
    "end": "1225570"
  },
  {
    "text": "does a word appear in the context of other words so let's look at the some",
    "start": "1225570",
    "end": "1232050"
  },
  {
    "text": "interesting results of these glove vectors here the first thing we do is look at nearest neighbors so now that we",
    "start": "1232050",
    "end": "1238830"
  },
  {
    "text": "have these n dimensional vectors usually you say n between 50 to at most 500 good",
    "start": "1238830",
    "end": "1245460"
  },
  {
    "text": "general numbers 100 or 200 dimensional each of these each word is now represented as a single vector and so we",
    "start": "1245460",
    "end": "1253710"
  },
  {
    "text": "can look in this vector space for words that appear close by we started and",
    "start": "1253710",
    "end": "1258720"
  },
  {
    "text": "looked for the nearest neighbors of frog and well turned out",
    "start": "1258720",
    "end": "1263970"
  },
  {
    "text": "these are the nearest neighbors which was a little confusing since we're not biologists but fortunately when you",
    "start": "1263970",
    "end": "1269760"
  },
  {
    "text": "actually look up in Google what what those mean you'll see that they are actually all indeed different kinds of",
    "start": "1269760",
    "end": "1276690"
  },
  {
    "text": "frogs some appear very rarely in the corpus and others like toad or much more",
    "start": "1276690",
    "end": "1282420"
  },
  {
    "text": "frequent now one of the most exciting results that came out of word vectors",
    "start": "1282420",
    "end": "1287430"
  },
  {
    "text": "actually these word analogies so the idea here is can linearly can there be",
    "start": "1287430",
    "end": "1295170"
  },
  {
    "text": "relationships between different word vectors that simply fall out of very linear and simple addition and",
    "start": "1295170",
    "end": "1301590"
  },
  {
    "text": "subtraction so the idea here is what is meant a woman equal to king to something",
    "start": "1301590",
    "end": "1308190"
  },
  {
    "text": "else as in what is the right analogy when I try to basically fill in here the",
    "start": "1308190",
    "end": "1318150"
  },
  {
    "text": "last missing word now the way we're going to do this is very very simple cosine similarity or basically just take",
    "start": "1318150",
    "end": "1326400"
  },
  {
    "text": "let's take an example here the vector of woman we subtract the word vector we",
    "start": "1326400",
    "end": "1334140"
  },
  {
    "text": "learned of man and we add the word vector of king and the resulting vector I the art max for this",
    "start": "1334140",
    "end": "1342030"
  },
  {
    "text": "turns out to going to be Queen for a lot of these different models and that was",
    "start": "1342030",
    "end": "1347340"
  },
  {
    "text": "very surprising again we're capturing core current statistics so man might in",
    "start": "1347340",
    "end": "1352470"
  },
  {
    "text": "its context often have things like running and fighting other silly things",
    "start": "1352470",
    "end": "1357570"
  },
  {
    "text": "that men do and then you subtract those kinds of words from the context and you",
    "start": "1357570",
    "end": "1363480"
  },
  {
    "text": "add them again and in some sense it's intuitive though surprising that it works out that well for so many",
    "start": "1363480",
    "end": "1369540"
  },
  {
    "text": "different examples so here are some some other examples similar to the king and",
    "start": "1369540",
    "end": "1376470"
  },
  {
    "text": "queen example where we basically took these two hundred dimensional vectors and we projected them down to two",
    "start": "1376470",
    "end": "1381600"
  },
  {
    "text": "dimensions again with a very simple method like PCA and what we find is",
    "start": "1381600",
    "end": "1387420"
  },
  {
    "text": "actually quite interestingly even in just the two first principal components of this space we have some very",
    "start": "1387420",
    "end": "1393540"
  },
  {
    "text": "interesting sort of female male relationships so men to women is similar",
    "start": "1393540",
    "end": "1398640"
  },
  {
    "text": "to uncle and aunt brother and sister sir and madam and so on so this is an",
    "start": "1398640",
    "end": "1405060"
  },
  {
    "text": "interesting semantic relationship that falls out of essentially Corcoran's",
    "start": "1405060",
    "end": "1410970"
  },
  {
    "text": "counts in specific windows around each word and a large corpus here's another",
    "start": "1410970",
    "end": "1416970"
  },
  {
    "text": "one that's more of a syntactic relationship we actually have here superlatives like slow slower slowest is",
    "start": "1416970",
    "end": "1423780"
  },
  {
    "text": "in a similar vector relationship to short shorter and shortest or strong",
    "start": "1423780",
    "end": "1429540"
  },
  {
    "text": "stronger and strongest so this was very exciting and of course when you see an",
    "start": "1429540",
    "end": "1434670"
  },
  {
    "text": "interesting qualitative result you want to try to quantify who can do better in",
    "start": "1434670",
    "end": "1440280"
  },
  {
    "text": "trying to understand these analogies and what are the different modes and hyper parameters that modify the performance",
    "start": "1440280",
    "end": "1447390"
  },
  {
    "text": "now this is something that you will notice in pretty much every deep learning project ever which is more data",
    "start": "1447390",
    "end": "1453180"
  },
  {
    "text": "will give you better performance it's probably the single most useful thing you can do to machine learning or deep",
    "start": "1453180",
    "end": "1458910"
  },
  {
    "text": "learning system is to train it with more data and we found that too now they're different vector sizes too which is a",
    "start": "1458910",
    "end": "1465690"
  },
  {
    "text": "common hyper parameter like I said usually between 52 and so I wondered here we have 300",
    "start": "1465690",
    "end": "1471520"
  },
  {
    "text": "dimensional that essentially gave us the best performance for these different",
    "start": "1471520",
    "end": "1476890"
  },
  {
    "text": "kinds of semantics and tactic relationships now in many ways having a",
    "start": "1476890",
    "end": "1482560"
  },
  {
    "text": "single vector for words can be oversimplifying right some words have multiple meanings maybe they should have",
    "start": "1482560",
    "end": "1488260"
  },
  {
    "text": "multiple vectors sometimes the word meaning changes overtime and so on so",
    "start": "1488260",
    "end": "1496930"
  },
  {
    "text": "there's a lot of simplifying assumptions here but again our final goal for deep NLP is going to be to create useful",
    "start": "1496930",
    "end": "1503830"
  },
  {
    "text": "systems and it turns out this is a useful first step to create such systems",
    "start": "1503830",
    "end": "1508840"
  },
  {
    "text": "that mimic some human language behavior in order to create useful applications",
    "start": "1508840",
    "end": "1516010"
  },
  {
    "text": "for us all right but words word vectors are very useful but words of course never appear in isolation and what we",
    "start": "1516010",
    "end": "1522520"
  },
  {
    "text": "really want to do is understand words in their context and so this leads us to the second section here on recurrent",
    "start": "1522520",
    "end": "1529840"
  },
  {
    "text": "neural networks so we already went over the basic definition of standard neural",
    "start": "1529840",
    "end": "1536980"
  },
  {
    "text": "networks really the main difference between a standard neural network and a recurrent neural network which I'll",
    "start": "1536980",
    "end": "1543250"
  },
  {
    "text": "abbreviate as RN and now is that we will tie the weights at each time step and that will allow us to essentially",
    "start": "1543250",
    "end": "1550090"
  },
  {
    "text": "condition the neural network on all the previous words in theory and practice how we can optimize it it won't be",
    "start": "1550090",
    "end": "1556420"
  },
  {
    "text": "really all the previous words we've more like at most the last 30 words but in",
    "start": "1556420",
    "end": "1561490"
  },
  {
    "text": "theory this is what a powerful model can do so let's look at the definition of a",
    "start": "1561490",
    "end": "1567130"
  },
  {
    "text": "recurrent neural network and this is going to be a very important definition so we'll go into a little bit of details here so let's assume for now we have our",
    "start": "1567130",
    "end": "1575080"
  },
  {
    "text": "word vectors as given and we'll represent each sequence in the beginning it's just a list of these word vectors",
    "start": "1575080",
    "end": "1582300"
  },
  {
    "text": "now what we're going to do is we're computing a hidden state HT at each time",
    "start": "1582300",
    "end": "1587650"
  },
  {
    "text": "step and the way we're going to do this is with a simple neural network architecture in fact you can think of",
    "start": "1587650",
    "end": "1595800"
  },
  {
    "text": "this summation here is really just a single layer neural network if you were",
    "start": "1595800",
    "end": "1601450"
  },
  {
    "text": "to concatenate the two matrices in these two that but intuitively we basically will map",
    "start": "1601450",
    "end": "1607260"
  },
  {
    "text": "our current word vector at that time step T sometimes I use these square",
    "start": "1607260",
    "end": "1612390"
  },
  {
    "text": "brackets to denote that we're taking the word vector from that time step in there",
    "start": "1612390",
    "end": "1618030"
  },
  {
    "text": "we map that with a linear layer a simple matrix vector product and we sum up some",
    "start": "1618030",
    "end": "1624120"
  },
  {
    "text": "that matrix vector product to another matrix vector product of the previous hidden state at the previous time step",
    "start": "1624120",
    "end": "1630860"
  },
  {
    "text": "we sum those two and reapply in one case a simple sigmoid function to define this",
    "start": "1630860",
    "end": "1637620"
  },
  {
    "text": "standard neural network layer that will be HT and now at each time step we want",
    "start": "1637620",
    "end": "1642750"
  },
  {
    "text": "to predict some kind of class probability over a set of potential",
    "start": "1642750",
    "end": "1648690"
  },
  {
    "text": "events classes words and so on and we use the standard softmax classifier some other communities called logistic",
    "start": "1648690",
    "end": "1654780"
  },
  {
    "text": "regression classifier so here we have a",
    "start": "1654780",
    "end": "1661140"
  },
  {
    "text": "simple matrix WS for the softmax weights",
    "start": "1661140",
    "end": "1666860"
  },
  {
    "text": "we have basically a number of rows are going to be a number of classes that we have and the number of columns is the",
    "start": "1666860",
    "end": "1673440"
  },
  {
    "text": "same as the hidden dimension sometimes",
    "start": "1673440",
    "end": "1678750"
  },
  {
    "text": "we want to predict the next word in a sequence in order to be able to identify",
    "start": "1678750",
    "end": "1683840"
  },
  {
    "text": "the most likely sequence so for instance if I asked for a speech recognition system what is the price of wood now in",
    "start": "1683840",
    "end": "1691920"
  },
  {
    "text": "isolation if you hear wood you would probably assume it's the wo uld",
    "start": "1691920",
    "end": "1697190"
  },
  {
    "text": "auxiliary verb wood but in this particular context the price of it wouldn't make sense to have a verb",
    "start": "1697190",
    "end": "1702840"
  },
  {
    "text": "following that and so it's more like the wo D to find the price of wood so",
    "start": "1702840",
    "end": "1708960"
  },
  {
    "text": "language modeling is very useful task and it's also very instructive to use as an example for where recurrent neural",
    "start": "1708960",
    "end": "1716340"
  },
  {
    "text": "networks refine so in our case here this softmax is going to be quite a large",
    "start": "1716340",
    "end": "1722580"
  },
  {
    "text": "matrix that goes over the entire vocabulary of all the possible words that we have so each word is going to be",
    "start": "1722580",
    "end": "1729150"
  },
  {
    "text": "our class the classes for language models are the words in our vocabulary and so we can define here",
    "start": "1729150",
    "end": "1737040"
  },
  {
    "text": "this y hat T the jf1 is basically denoting here the probability that the J",
    "start": "1737040",
    "end": "1744570"
  },
  {
    "text": "word at the J index will come next after all the previous words very useful model",
    "start": "1744570",
    "end": "1750660"
  },
  {
    "text": "again for speech recognition for machine translation for just finding a prior for",
    "start": "1750660",
    "end": "1755940"
  },
  {
    "text": "language in general alright again main difference the standard",
    "start": "1755940",
    "end": "1762180"
  },
  {
    "text": "neural networks we just have the same set of W weights at all the different time steps everything else is pretty",
    "start": "1762180",
    "end": "1768180"
  },
  {
    "text": "much a standard neural network we often initialize the first h0 here just either",
    "start": "1768180",
    "end": "1776340"
  },
  {
    "text": "randomly or all zeroes and again in language modeling in particular the next",
    "start": "1776340",
    "end": "1783240"
  },
  {
    "text": "word is our class of the softmax now we can measure basically the performance of",
    "start": "1783240",
    "end": "1789030"
  },
  {
    "text": "language models with terms are called perplexity which really is here the",
    "start": "1789030",
    "end": "1795540"
  },
  {
    "text": "average log likelihood of the basically the probabilities of being able to",
    "start": "1795540",
    "end": "1800760"
  },
  {
    "text": "predict the next word so you want to really give the highest probability to the word that actually will appear next",
    "start": "1800760",
    "end": "1806910"
  },
  {
    "text": "in a long sequence and then the higher that probability is the lower your",
    "start": "1806910",
    "end": "1813330"
  },
  {
    "text": "perplexity in hence the models less perplexed to see the next word in some sense you can think of language modeling",
    "start": "1813330",
    "end": "1820860"
  },
  {
    "text": "as almost NLP complete and some silly sense that you just if you can actually",
    "start": "1820860",
    "end": "1827220"
  },
  {
    "text": "predict every single word that follows after any arbitrary sequence of words in a perfect way you would have",
    "start": "1827220",
    "end": "1834090"
  },
  {
    "text": "disambiguated a lot of things you can you can say for instance what is the answer to the following question ask the",
    "start": "1834090",
    "end": "1840390"
  },
  {
    "text": "question and then the next couple of words would be the predicted answer so there's no way we can actually ever do",
    "start": "1840390",
    "end": "1845790"
  },
  {
    "text": "perfect job in language modeling but there's certain contexts where we can give a very high probability to the",
    "start": "1845790",
    "end": "1852090"
  },
  {
    "text": "right next couple of words now this is the standard recurrent neural network",
    "start": "1852090",
    "end": "1858000"
  },
  {
    "text": "and one problem with this is that we will modify the hidden state here at every time set so even if I have words",
    "start": "1858000",
    "end": "1865260"
  },
  {
    "text": "like the and a and sentence period and things like that it will stick",
    "start": "1865260",
    "end": "1870960"
  },
  {
    "text": "frequently modify in my hidden state now that can be problematic let's say for",
    "start": "1870960",
    "end": "1876059"
  },
  {
    "text": "instance I want to train a sentiment analysis algorithm and I talk about",
    "start": "1876059",
    "end": "1881880"
  },
  {
    "text": "movies and I talk about the plot for a very long time then I say oh man this movie was really wonderful it's great to",
    "start": "1881880",
    "end": "1888510"
  },
  {
    "text": "watch and then especially the ending and you talk again for like fifty timesteps or 50 words or hundred words about the",
    "start": "1888510",
    "end": "1894899"
  },
  {
    "text": "plot now all these plot words will essentially modify my hidden states if at the end of that whole sequence I want",
    "start": "1894899",
    "end": "1901140"
  },
  {
    "text": "to classify the sentiment the word wonderful and great that I mentioned somewhere in the middle might be completely gone because I keep updating",
    "start": "1901140",
    "end": "1908880"
  },
  {
    "text": "my hidden state with all these content words to talk about the plot now the way",
    "start": "1908880",
    "end": "1915510"
  },
  {
    "text": "to improve this is by use better kinds of recurrent units and I'll introduce",
    "start": "1915510",
    "end": "1921390"
  },
  {
    "text": "here a particular kind so called gated recurrent units introduced by Cho in",
    "start": "1921390",
    "end": "1928620"
  },
  {
    "text": "some sense and we'll learn more about the LS TM tomorrow when Kwok gives his",
    "start": "1928620",
    "end": "1934350"
  },
  {
    "text": "lecture but G R user in some sense a special case of LS DMS and the main idea",
    "start": "1934350",
    "end": "1939960"
  },
  {
    "text": "is that we want to have the ability to keep certain memories around without having the current input modify modify",
    "start": "1939960",
    "end": "1947789"
  },
  {
    "text": "them at all so again this example of sentiment analysis I say something's great that should somehow be captured in",
    "start": "1947789",
    "end": "1953520"
  },
  {
    "text": "my hidden state and I don't want all the content words to talk about the plot in a movie review to modify that is",
    "start": "1953520",
    "end": "1959279"
  },
  {
    "text": "actually overall I was a great movie and then we also want to allow error messages to flow at different strengths",
    "start": "1959279",
    "end": "1966330"
  },
  {
    "text": "depending on the input so if I say great I want that to modify a lot of things in",
    "start": "1966330",
    "end": "1971340"
  },
  {
    "text": "the past so let's define a giryu fortunately since you already know the",
    "start": "1971340",
    "end": "1977970"
  },
  {
    "text": "basic Lego block of a standard neural network there's only really one or two subtleties here that are different there",
    "start": "1977970",
    "end": "1985890"
  },
  {
    "text": "are a couple of different steps that we'll need to compute at every time step so in the standard RNN",
    "start": "1985890",
    "end": "1992490"
  },
  {
    "text": "what we did was just have this one single neural network that we hope would capture all this complexity of the",
    "start": "1992490",
    "end": "1997740"
  },
  {
    "text": "sequence instead now we'll first compute a couple of gates at that time step so",
    "start": "1997740",
    "end": "2003440"
  },
  {
    "text": "the first thing will compute is the so called update gate it's just yet another neural network",
    "start": "2003440",
    "end": "2008960"
  },
  {
    "text": "layer based on the current input word vector and again the past hidden state so these look quite familiar but this",
    "start": "2008960",
    "end": "2015470"
  },
  {
    "text": "will just be an intermediate value and we'll call it the update gate then we'll also compute a reset gate is yet another",
    "start": "2015470",
    "end": "2022730"
  },
  {
    "text": "standard neural network layer again just matrix vector product summation matrix vector product some kind of",
    "start": "2022730",
    "end": "2029420"
  },
  {
    "text": "non-linearity here namely Sigma it's actually important in this case that it is a sigmoid just just basically both of",
    "start": "2029420",
    "end": "2036050"
  },
  {
    "text": "these will be vectors with numbers that are between 0 and 1 now we'll compute a",
    "start": "2036050",
    "end": "2042590"
  },
  {
    "text": "new memory content an intermediate age tilt here with yet another neural",
    "start": "2042590",
    "end": "2048679"
  },
  {
    "text": "network but then we have this little funky symbol in here basically this will",
    "start": "2048680",
    "end": "2053690"
  },
  {
    "text": "be an element-wise multiplication so basically what this will allow us to do",
    "start": "2053690",
    "end": "2059149"
  },
  {
    "text": "is if that reset gate is 0 we can essentially ignore all the previous",
    "start": "2059150",
    "end": "2064850"
  },
  {
    "text": "memory elements and only store the new word information so for instance if I",
    "start": "2064850",
    "end": "2071990"
  },
  {
    "text": "talked for a long time about the plot now I say this was an awesome movie now",
    "start": "2071990",
    "end": "2077929"
  },
  {
    "text": "you want to basically be able to ignore if your whole goal of this sequence classification model is to capture",
    "start": "2077930",
    "end": "2083629"
  },
  {
    "text": "sentiment I'm going to be able to ignore past content this is of course if this",
    "start": "2083630",
    "end": "2088730"
  },
  {
    "text": "was a 0 entirely a 0 vector now this will be more subtle this is a long vector if you know maybe a hundred or",
    "start": "2088730",
    "end": "2094820"
  },
  {
    "text": "200 dimensions so maybe some dimensions should be reset but others maybe not and then here we'll have our finally",
    "start": "2094820",
    "end": "2103070"
  },
  {
    "text": "final memory and that essentially combines these two states the previous",
    "start": "2103070",
    "end": "2108850"
  },
  {
    "text": "hidden state and this intermediate one at our current time step and what this will allow us to do is essentially also",
    "start": "2108850",
    "end": "2115490"
  },
  {
    "text": "say well maybe we want to ignore everything that's currently happening and only update the last time step we",
    "start": "2115490",
    "end": "2122060"
  },
  {
    "text": "basically copy over the previous time step in the hidden state of that and ignore the current thing again simple",
    "start": "2122060",
    "end": "2129200"
  },
  {
    "text": "example in sentiment maybe there's a lot of talk about the plot when a movie was released if you want to basically have",
    "start": "2129200",
    "end": "2135140"
  },
  {
    "text": "the ability to ignore that and just copy that in the beginning may have said it was an awesome movie so",
    "start": "2135140",
    "end": "2140870"
  },
  {
    "text": "here's an attempt at a clean illustration I have to say personally I in the end find the equations a little",
    "start": "2140870",
    "end": "2145940"
  },
  {
    "text": "more intuitive than the visualizations that we try to do but some people are are more visual here so this is in some",
    "start": "2145940",
    "end": "2152510"
  },
  {
    "text": "ways basically here we have our word vector and it goes through different layers and then some of these layers",
    "start": "2152510",
    "end": "2158690"
  },
  {
    "text": "will essentially modify other outputs of previous time steps so this is a pretty",
    "start": "2158690",
    "end": "2167630"
  },
  {
    "text": "nifty model and it's read the second most important basic Lego block that",
    "start": "2167630",
    "end": "2174620"
  },
  {
    "text": "we're going to learn about today and so just want to make sure we take a little",
    "start": "2174620",
    "end": "2180830"
  },
  {
    "text": "bit of time I'll repeat this here again if the reset gate this R value is close",
    "start": "2180830",
    "end": "2186950"
  },
  {
    "text": "to zero those kinds of hidden dimensions are basically allowed to be dropped and",
    "start": "2186950",
    "end": "2193540"
  },
  {
    "text": "if the update gates Z basically is one then we can copy information in of that",
    "start": "2193540",
    "end": "2201380"
  },
  {
    "text": "unit through many many different time steps and if you think about optimization a lot what this will also",
    "start": "2201380",
    "end": "2208010"
  },
  {
    "text": "mean is that the gradient can flow through the recurrent wheel network through multiple time steps until it",
    "start": "2208010",
    "end": "2214130"
  },
  {
    "text": "actually matters and you want to update a specific word for instance and go all the way through many different time",
    "start": "2214130",
    "end": "2220580"
  },
  {
    "text": "steps so then what this also allows us is to",
    "start": "2220580",
    "end": "2225980"
  },
  {
    "text": "actually have some units that have different update frequencies some you",
    "start": "2225980",
    "end": "2232610"
  },
  {
    "text": "might want to reset every other word other ones you might really cap like they have some long-term context and",
    "start": "2232610",
    "end": "2238550"
  },
  {
    "text": "they stay around for much longer all right this is the geo you it's the",
    "start": "2238550",
    "end": "2245210"
  },
  {
    "text": "second most important building block for today there are like I said a lot of other variants of recurrent neural",
    "start": "2245210",
    "end": "2252230"
  },
  {
    "text": "networks lots of amazing work in that space right now and tomorrow quoc will",
    "start": "2252230",
    "end": "2257300"
  },
  {
    "text": "we'll talk a lot about some more advanced methods so now that you've",
    "start": "2257300",
    "end": "2263000"
  },
  {
    "text": "understand word vectors and neural network sequence models you really have",
    "start": "2263000",
    "end": "2269390"
  },
  {
    "text": "the two most important concepts for deep NLP and that's pretty awesome so congrats we",
    "start": "2269390",
    "end": "2276530"
  },
  {
    "text": "can now in some ways really play around with those two Lego blocks plus some slight modifications of them very",
    "start": "2276530",
    "end": "2283430"
  },
  {
    "text": "creatively and build a lot of really cool models a lot of the models that I'll show you and that you can read and",
    "start": "2283430",
    "end": "2289460"
  },
  {
    "text": "see and read the latest papers that are now coming out almost every week on archive will have some kind of component",
    "start": "2289460",
    "end": "2297020"
  },
  {
    "text": "of these will use really these two components in a major way now this is",
    "start": "2297020",
    "end": "2302540"
  },
  {
    "text": "one of the few slides now with something really new because I want to keep it",
    "start": "2302540",
    "end": "2309110"
  },
  {
    "text": "exciting for the people who already knew all this stuff and took the class and everything this is tackling a important problem",
    "start": "2309110",
    "end": "2316160"
  },
  {
    "text": "which is and all these models that you'll see in pretty much most of these",
    "start": "2316160",
    "end": "2321320"
  },
  {
    "text": "papers we have in the end one final softmax here right and that softmax is",
    "start": "2321320",
    "end": "2327500"
  },
  {
    "text": "basically our default way of classifying what we can see next what kinds of classes we can predict the problem with",
    "start": "2327500",
    "end": "2335270"
  },
  {
    "text": "that is of course that that will only ever predict accurately frequently seen classes that we had at training time but",
    "start": "2335270",
    "end": "2341990"
  },
  {
    "text": "in the case of language modeling for instance where our classes are the words we may see a test time some completely",
    "start": "2341990",
    "end": "2348290"
  },
  {
    "text": "new words maybe I'm just going to introduce to you a new name srini for",
    "start": "2348290",
    "end": "2353720"
  },
  {
    "text": "instance and nobody may have like seen that word at training time but now that",
    "start": "2353720",
    "end": "2359930"
  },
  {
    "text": "I mentioned him and I will introduce him to you you should be able to predict the word trini and that person in a new",
    "start": "2359930",
    "end": "2367010"
  },
  {
    "text": "context and so the solution that we're literally going to release only next week and in a new paper is to essentially",
    "start": "2367010",
    "end": "2374120"
  },
  {
    "text": "combine the standard softmax that we can train with a pointer component and that pointer component will allow us to point",
    "start": "2374120",
    "end": "2381140"
  },
  {
    "text": "to previous contexts and then predict based on that to see that word so let's",
    "start": "2381140",
    "end": "2387110"
  },
  {
    "text": "for instance take the example you have language modeling again we may read a long article about the Fed chair Janet",
    "start": "2387110",
    "end": "2394400"
  },
  {
    "text": "Yellen and maybe the word Yellen had not appeared in training time before so we",
    "start": "2394400",
    "end": "2400340"
  },
  {
    "text": "couldn't ever predict it even though we just learned about it and now a couple of sentences later interest rates were",
    "start": "2400340",
    "end": "2406190"
  },
  {
    "text": "based and then missus and now we want to predict that next word now if that",
    "start": "2406190",
    "end": "2411830"
  },
  {
    "text": "hadn't appeared in our softmax standard training procedure at training time we would never be able to predict it what",
    "start": "2411830",
    "end": "2419090"
  },
  {
    "text": "this model will do and we're kind of calling it a pointer sentinel mixture model is it will essentially first try",
    "start": "2419090",
    "end": "2424400"
  },
  {
    "text": "to see what any of these previous words maybe be the right candidate so we can",
    "start": "2424400",
    "end": "2430280"
  },
  {
    "text": "really take into consideration the previous context of say the last hundred words and if we see that word and that",
    "start": "2430280",
    "end": "2435830"
  },
  {
    "text": "word makes sense after you know we train it of course then we might give a lot of probability mass to just that word at",
    "start": "2435830",
    "end": "2442520"
  },
  {
    "text": "this current position in our previous immediate context at test time and then",
    "start": "2442520",
    "end": "2449360"
  },
  {
    "text": "we have also the sentinel which is basically going to be the rest of the probability if we cannot refer to the",
    "start": "2449360",
    "end": "2455120"
  },
  {
    "text": "some of the words that we just saw and that one will go directly to our",
    "start": "2455120",
    "end": "2460820"
  },
  {
    "text": "standard softmax and then what we'll essentially have is a mixture model that allows us to say either we have or we",
    "start": "2460820",
    "end": "2469340"
  },
  {
    "text": "have a combination of both of essentially words that just appeared in this context and words that we saw in",
    "start": "2469340",
    "end": "2474890"
  },
  {
    "text": "our standard softmax language modeling system so I think this is a pretty",
    "start": "2474890",
    "end": "2480530"
  },
  {
    "text": "important next step because it will allow us to predict things we've never seen a training time and that's",
    "start": "2480530",
    "end": "2486050"
  },
  {
    "text": "something that's clearly a human capability that most or pretty much none of these language models had before and",
    "start": "2486050",
    "end": "2492020"
  },
  {
    "text": "so to look at how much it actually helps it'll be interesting to look at some of",
    "start": "2492020",
    "end": "2497900"
  },
  {
    "text": "the performance before so again what we're measuring here is perplexity and the lower the better because it's",
    "start": "2497900",
    "end": "2504920"
  },
  {
    "text": "essentially inverse here of the actual probability that we assigned to the",
    "start": "2504920",
    "end": "2510080"
  },
  {
    "text": "correct next word and in just 2010 so six years ago there this was some great",
    "start": "2510080",
    "end": "2517340"
  },
  {
    "text": "work early work by Thomas McAuliffe where he compared to a lot of standard",
    "start": "2517340",
    "end": "2522740"
  },
  {
    "text": "natural language processing methods syntactic neural net syntactic models",
    "start": "2522740",
    "end": "2528430"
  },
  {
    "text": "that essentially tried to predict the next word and had a perplexity of 107 and he was able to use the standard",
    "start": "2528430",
    "end": "2535580"
  },
  {
    "text": "recurrent neural networks and actually an ensemble of eight of them to really significantly push down the",
    "start": "2535580",
    "end": "2542180"
  },
  {
    "text": "perplexity especially when you combine it with standard count based methods for",
    "start": "2542180",
    "end": "2547700"
  },
  {
    "text": "language modeling so in 2010 he made great progress by pushing it down to 87",
    "start": "2547700",
    "end": "2554090"
  },
  {
    "text": "and now this is one of the great examples of how much progress is being",
    "start": "2554090",
    "end": "2559550"
  },
  {
    "text": "made in the field thanks to deep learning we're two years ago white",
    "start": "2559550",
    "end": "2564830"
  },
  {
    "text": "chicks are memba and and his collaborators were able to push that down even further to 78 with a very",
    "start": "2564830",
    "end": "2572780"
  },
  {
    "text": "large lsdm similar to a GRU like model but even more advanced quark will will",
    "start": "2572780",
    "end": "2578210"
  },
  {
    "text": "teach you the basics of LS CMS tomorrow then last year we pushed the the",
    "start": "2578210",
    "end": "2584870"
  },
  {
    "text": "performance was pushed down even further by yarn gull and then this one actually",
    "start": "2584870",
    "end": "2590600"
  },
  {
    "text": "came out just a couple of weeks ago variational recurrent highway networks pushed it down even further but this",
    "start": "2590600",
    "end": "2597620"
  },
  {
    "text": "pointer sentiment model is able to get it down to 70 so in just a short amount of time we pushed it down by more than",
    "start": "2597620",
    "end": "2605300"
  },
  {
    "text": "10 perplexity points and in two years and that is really an increased speed in",
    "start": "2605300",
    "end": "2612020"
  },
  {
    "text": "performance that we're seeing now that deep learning so if changing a lot of areas of natural language processing",
    "start": "2612020",
    "end": "2619120"
  },
  {
    "text": "alright now we have sort of our basic Lego blocks the word vectors and the GRU",
    "start": "2619120",
    "end": "2625280"
  },
  {
    "text": "sequence models and now we can talk a little bit about some of the ongoing",
    "start": "2625280",
    "end": "2630470"
  },
  {
    "text": "research that we're working on and I'll start that with maybe a controversial",
    "start": "2630470",
    "end": "2636020"
  },
  {
    "text": "question which is could we possibly reduce all NLP tasks to essentially",
    "start": "2636020",
    "end": "2643430"
  },
  {
    "text": "question answering tasks over some kind of input and in some ways that's a trivial observation that you could do",
    "start": "2643430",
    "end": "2650510"
  },
  {
    "text": "that but it actually might help us to think of models that could take any kind",
    "start": "2650510",
    "end": "2655850"
  },
  {
    "text": "of input a question about that input and try to produce an output sequence so let",
    "start": "2655850",
    "end": "2662750"
  },
  {
    "text": "me give you a couple of examples of what I mean by this so here we have the first",
    "start": "2662750",
    "end": "2668060"
  },
  {
    "text": "one is a task that we would standardly associate with answering I'll give you a couple of",
    "start": "2668060",
    "end": "2673369"
  },
  {
    "text": "facts Mary walk to the bathroom send her went to the garden Daniel went back to the garden Sandra took the milk",
    "start": "2673369",
    "end": "2679400"
  },
  {
    "text": "there where's the milk and now you might have to logically reason so I try to",
    "start": "2679400",
    "end": "2684890"
  },
  {
    "text": "find the sentence about milk maybe Sandra took the milk there and I",
    "start": "2684890",
    "end": "2691279"
  },
  {
    "text": "would have to maybe do an F for a resolution find out what does there refer to and then you try to find you",
    "start": "2691279",
    "end": "2698690"
  },
  {
    "text": "know the previous sentence that mentioned Sandra see that it's garden and then give an answer garden so this",
    "start": "2698690",
    "end": "2704329"
  },
  {
    "text": "is a simple logical reasoning question answering task and that's what most people in the QA field sort of",
    "start": "2704329",
    "end": "2711289"
  },
  {
    "text": "associated with some kinds of question answers but we can also say everybody's",
    "start": "2711289",
    "end": "2716749"
  },
  {
    "text": "happy and the question is what's the sentiment and the answer is positive all right so this is a different subfield of",
    "start": "2716749",
    "end": "2723049"
  },
  {
    "text": "NLP that tackles sentiment analysis we can go further and ask what are the",
    "start": "2723049",
    "end": "2729200"
  },
  {
    "text": "named entities of a sentence like Jane has a baby in Dresden and you want to find out that Jane is a person in",
    "start": "2729200",
    "end": "2734809"
  },
  {
    "text": "Dresden as a location this is an example of sequence tagging you can even go as",
    "start": "2734809",
    "end": "2740630"
  },
  {
    "text": "far and say you know I think the smile is incredible and the question is what's the translation into French and you get",
    "start": "2740630",
    "end": "2748249"
  },
  {
    "text": "you know Japan's kusuma del a on clay habla and dad in some ways would be",
    "start": "2748249",
    "end": "2754599"
  },
  {
    "text": "phenomenal if we're able to actually tackle all these different kinds of",
    "start": "2754599",
    "end": "2760489"
  },
  {
    "text": "tasks with the same kind of model so maybe it would be an interesting new",
    "start": "2760489",
    "end": "2765920"
  },
  {
    "text": "goal for NLP to try to develop a single joint model for general question",
    "start": "2765920",
    "end": "2772849"
  },
  {
    "text": "answering I think it would push us to think about new kinds of sequence models",
    "start": "2772849",
    "end": "2779569"
  },
  {
    "text": "and new kinds of reasoning capabilities in an interesting way now there are two major obstacles to actually achieving",
    "start": "2779569",
    "end": "2786650"
  },
  {
    "text": "the single joint model for arbitrary QA tests the first one is that we don't",
    "start": "2786650",
    "end": "2791839"
  },
  {
    "text": "even have a single model architecture that gets consistent state-of-the-art results across a variety of different",
    "start": "2791839",
    "end": "2798109"
  },
  {
    "text": "tasks so for instance for question answering and this is a data set called Bobby did face book published",
    "start": "2798109",
    "end": "2805130"
  },
  {
    "text": "last year strongly supervised memory networks get the state of the art for sentiment analysis you had tree lsdm",
    "start": "2805130",
    "end": "2813140"
  },
  {
    "text": "models developed by cashing ty here at Stanford last year and for part of",
    "start": "2813140",
    "end": "2820190"
  },
  {
    "text": "speech tagging you might have bi-directional lsdm conditional random fields one thing you do notice is all",
    "start": "2820190",
    "end": "2826040"
  },
  {
    "text": "the current state-of-the-art methods are deep learning sometimes they still connect to other traditional methods",
    "start": "2826040",
    "end": "2833660"
  },
  {
    "text": "like conditional random fields and undirected graphical models but there's always some some kind of deep learning",
    "start": "2833660",
    "end": "2839090"
  },
  {
    "text": "component in them so that is the first obstacle the second one is that really",
    "start": "2839090",
    "end": "2847040"
  },
  {
    "text": "fully joint multitask learning is very very hard usually when we do do it we",
    "start": "2847040",
    "end": "2853370"
  },
  {
    "text": "restrict it to lower layers so for instance in natural language processing all we're currently able to share in",
    "start": "2853370",
    "end": "2859730"
  },
  {
    "text": "some principled way our word vectors we take the same word vectors we trained for instance with glove or work avec and",
    "start": "2859730",
    "end": "2865760"
  },
  {
    "text": "we initialize our deep neural network sequence models with those word vectors",
    "start": "2865760",
    "end": "2870940"
  },
  {
    "text": "in computer vision and we're actually a little further ahead and you're able to",
    "start": "2870940",
    "end": "2876710"
  },
  {
    "text": "use multiple of the different layers and you initialize a lot of your CNN models",
    "start": "2876710",
    "end": "2882470"
  },
  {
    "text": "with first pre trained CNN that was pre trained on imagenet for instance now usually people evaluate",
    "start": "2882470",
    "end": "2890540"
  },
  {
    "text": "multitask learning with only two tasks they trained on for a first task and then they evaluate the model that they",
    "start": "2890540",
    "end": "2897200"
  },
  {
    "text": "initialize from the first on the second task but they often ignore how much the performance degrades on the original",
    "start": "2897200",
    "end": "2903560"
  },
  {
    "text": "task so when somebody takes an image net CNN and applies it to a new problem they rarely ever go back and say how much did",
    "start": "2903560",
    "end": "2909860"
  },
  {
    "text": "my accuracy actually decrease on the original data set and furthermore we",
    "start": "2909860",
    "end": "2915530"
  },
  {
    "text": "usually only look at tasks that are actually related and then we find out look there's some amazing transfer learning capability going on what we",
    "start": "2915530",
    "end": "2923270"
  },
  {
    "text": "don't look a look at often in the literature and in most people's work is that when the tasks aren't related to",
    "start": "2923270",
    "end": "2929210"
  },
  {
    "text": "one another they actually hurt each other and this is a so called catastrophic forgetting it's not there's",
    "start": "2929210",
    "end": "2936440"
  },
  {
    "text": "not too much work that right now now I also would like to",
    "start": "2936440",
    "end": "2943440"
  },
  {
    "text": "say that right now almost nobody uses the exact same decoder or classifier for",
    "start": "2943440",
    "end": "2949470"
  },
  {
    "text": "a variety of different kinds of outputs right we at least replace the softmax to",
    "start": "2949470",
    "end": "2955080"
  },
  {
    "text": "try to predict different kinds of problems all right so this is the second",
    "start": "2955080",
    "end": "2960180"
  },
  {
    "text": "obstacle now for now we'll only tackle the first obstacle and this is basically",
    "start": "2960180",
    "end": "2965220"
  },
  {
    "text": "what motivated us to come up with dynamic memory networks they are essentially an architecture to try to",
    "start": "2965220",
    "end": "2971520"
  },
  {
    "text": "tackle arbitrary question-answering paths when I'll talk about dynamic",
    "start": "2971520",
    "end": "2976860"
  },
  {
    "text": "memory networks is important to note here that for each of the different tasks I'll talk about it'll be a different dynamic memory network it",
    "start": "2976860",
    "end": "2984600"
  },
  {
    "text": "won't have the exact same weights will just be the same general architecture so",
    "start": "2984600",
    "end": "2990870"
  },
  {
    "text": "the high-level idea for DM ends is as follows imagine you had to read a bunch",
    "start": "2990870",
    "end": "2996780"
  },
  {
    "text": "of facts like these here they're all very simple in and of themselves but if",
    "start": "2996780",
    "end": "3002450"
  },
  {
    "text": "I now ask you a question I showed you these and I asked where Sandra you know",
    "start": "3002450",
    "end": "3008150"
  },
  {
    "text": "it'd be very hard even if you read them all of them and be kind of hard to remember and so the idea here is that",
    "start": "3008150",
    "end": "3015050"
  },
  {
    "text": "for complex questions we might actually want to allow you to have multiple glances at just at the input and just",
    "start": "3015050",
    "end": "3025280"
  },
  {
    "text": "like I promised our one of our most important basic Lego blocks will be this GRU we just introduced in the previous",
    "start": "3025280",
    "end": "3031010"
  },
  {
    "text": "section now here's this whole model in all its gory details and we'll dive into",
    "start": "3031010",
    "end": "3038300"
  },
  {
    "text": "all of that in the next couple of slides so don't worry it's it's a big model a",
    "start": "3038300",
    "end": "3044060"
  },
  {
    "text": "couple of observations so the first one is I think we're moving in deep learning now to try to use more proper software",
    "start": "3044060",
    "end": "3051230"
  },
  {
    "text": "engineering principles basically to modularize encapsulate certain",
    "start": "3051230",
    "end": "3056570"
  },
  {
    "text": "capabilities and then take those as basic Lego blocks and build more complex models on top of them a lot of times",
    "start": "3056570",
    "end": "3064220"
  },
  {
    "text": "nowadays you just have a CNN that's like one little block in a complex paper and then other things happen on top here",
    "start": "3064220",
    "end": "3070670"
  },
  {
    "text": "we'll have the gru or word vectors basically has you know one module a sub module in these",
    "start": "3070670",
    "end": "3077480"
  },
  {
    "text": "different ones here and I'm not even mentioning word vectors anymore but word vectors still play a crucial role and",
    "start": "3077480",
    "end": "3083360"
  },
  {
    "text": "each of these words is essentially represented as this word vector but we just kind of assume that it's there",
    "start": "3083360",
    "end": "3088820"
  },
  {
    "text": "okay so let's walk on a very high level through this model they're essentially four different modules there's the input",
    "start": "3088820",
    "end": "3095540"
  },
  {
    "text": "module which will be a neural network sequence model and giryu and there's a",
    "start": "3095540",
    "end": "3100670"
  },
  {
    "text": "question module an episodic memory module and an answering module and sometimes we also have these semantic",
    "start": "3100670",
    "end": "3107630"
  },
  {
    "text": "memory modules here but for now these are Ray just our word vectors and we'll ignore that for now so let's go through",
    "start": "3107630",
    "end": "3114140"
  },
  {
    "text": "this here is our corpus and our question is where is the football and this is our",
    "start": "3114140",
    "end": "3120290"
  },
  {
    "text": "input that should allow us to answer this question now if I ask this question I will essentially use the final",
    "start": "3120290",
    "end": "3127940"
  },
  {
    "text": "representation of this question to learn to pay attention to the right kinds of inputs that seem relevant for given what",
    "start": "3127940",
    "end": "3135410"
  },
  {
    "text": "I know to answer this question so whereas the football well it would make sense to basically pay attention to all",
    "start": "3135410",
    "end": "3141980"
  },
  {
    "text": "the sentences that mention football and maybe especially the last ones if the football moves around a lot so what we'll observe here is that this",
    "start": "3141980",
    "end": "3149570"
  },
  {
    "text": "last sentence will get a lot of attention so John put down the football and now what we'll basically do is that",
    "start": "3149570",
    "end": "3156770"
  },
  {
    "text": "this hidden state of this recurrent neural network model will be given as",
    "start": "3156770",
    "end": "3162260"
  },
  {
    "text": "input to another recurrent neural network because it seemed relevant to answer this current question at hand now",
    "start": "3162260",
    "end": "3170150"
  },
  {
    "text": "we'll basically agglomerate all these different facts that seem relevant at the time and is now the gru in this",
    "start": "3170150",
    "end": "3177350"
  },
  {
    "text": "final vector m and now this vector M together with the question will be used to go over the inputs again if the model",
    "start": "3177350",
    "end": "3184160"
  },
  {
    "text": "deems that doesn't have enough information yet to answer the question so if I ask you where's the football and",
    "start": "3184160",
    "end": "3189200"
  },
  {
    "text": "it's so far only found that John put down the football you don't know enough you still don't know where it is but you",
    "start": "3189200",
    "end": "3194510"
  },
  {
    "text": "now have a new fact namely John seems relevant to answer the question and that fact is now represented in this vector M",
    "start": "3194510",
    "end": "3201620"
  },
  {
    "text": "which is also just the last in the state of another Network now we'll go over the inputs",
    "start": "3201620",
    "end": "3207810"
  },
  {
    "text": "again now that we know that John and the football irrelevant will be learned to pay attention to John move to the",
    "start": "3207810",
    "end": "3214320"
  },
  {
    "text": "bedroom and John went to the hallway again those are going to get",
    "start": "3214320",
    "end": "3221180"
  },
  {
    "text": "agglomerated here in this recurrent neural network and now the model seems",
    "start": "3221180",
    "end": "3226590"
  },
  {
    "text": "thinks that it actually knows enough because it basically intrinsically",
    "start": "3226590",
    "end": "3231860"
  },
  {
    "text": "captured things about the football John found a location and so on of course we",
    "start": "3231860",
    "end": "3237120"
  },
  {
    "text": "didn't have to tell it anybody anything about their people their locations if X moves to Y and y is in the set of",
    "start": "3237120",
    "end": "3243720"
  },
  {
    "text": "locations then this happens none of that you just give it a lot of stories like that and in its hidden states it will capture these kinds of",
    "start": "3243720",
    "end": "3250890"
  },
  {
    "text": "patterns so then we have the final vector M and we'll give that to an",
    "start": "3250890",
    "end": "3256080"
  },
  {
    "text": "answer module which produces in our standard softmax way the answer all",
    "start": "3256080",
    "end": "3261840"
  },
  {
    "text": "right now let's zoom into the different modules of this overall dynamic memory",
    "start": "3261840",
    "end": "3267180"
  },
  {
    "text": "network architecture the input fortunately is just a standard GRU the way we defined it before so simple word",
    "start": "3267180",
    "end": "3275940"
  },
  {
    "text": "vectors hidden states reset gates update gates and so on the question module is",
    "start": "3275940",
    "end": "3284130"
  },
  {
    "text": "also just the GRU a separate one with its own weights and the final vector q",
    "start": "3284130",
    "end": "3290490"
  },
  {
    "text": "here is just going to be the last hidden state of that recurrent neural networks you can't model now the interesting",
    "start": "3290490",
    "end": "3297060"
  },
  {
    "text": "stuff happens in the episodic memory module which is essentially a sort of",
    "start": "3297060",
    "end": "3302610"
  },
  {
    "text": "meta gated GRU where this gate will",
    "start": "3302610",
    "end": "3308040"
  },
  {
    "text": "basically define is defined computed by the attention mechanism and will",
    "start": "3308040",
    "end": "3314100"
  },
  {
    "text": "basically say this current state sentence si here seems to matter and the",
    "start": "3314100",
    "end": "3320370"
  },
  {
    "text": "superscript T is the episode that we have so each episode basically means we're going over the input entirely one",
    "start": "3320370",
    "end": "3328110"
  },
  {
    "text": "time so it starts at g1 here and what",
    "start": "3328110",
    "end": "3333750"
  },
  {
    "text": "this basically will allow us to do is to say well if G is",
    "start": "3333750",
    "end": "3339110"
  },
  {
    "text": "zero then what we'll do is basically just copy over the past states from the",
    "start": "3339180",
    "end": "3345700"
  },
  {
    "text": "input nothing will happen and unlike before in all these GRU equations this G is just a single scalar number it will",
    "start": "3345700",
    "end": "3353170"
  },
  {
    "text": "basically say if G is zero then this sentence is completely irrelevant to my",
    "start": "3353170",
    "end": "3359230"
  },
  {
    "text": "current question at hand I can completely skip it all right and there are lots of examples like mary mary traveled to the hallway",
    "start": "3359230",
    "end": "3366370"
  },
  {
    "text": "that are just completely irrelevant to answering the current question in those cases this g will be zero and we're just",
    "start": "3366370",
    "end": "3374050"
  },
  {
    "text": "copying the previous hidden state of this recurrent neural network over otherwise we'll have a standard giryu",
    "start": "3374050",
    "end": "3381430"
  },
  {
    "text": "model so now of course the big question is how do we compute this G and this",
    "start": "3381430",
    "end": "3387070"
  },
  {
    "text": "might look a little ugly but it's quite simple basically we're going to compute two vector similarities multiplicative",
    "start": "3387070",
    "end": "3394270"
  },
  {
    "text": "an edit of one with absolute values of all the single values of the sentence",
    "start": "3394270",
    "end": "3401140"
  },
  {
    "text": "vector that we currently have and the question vector and the first the memory state of the previous pass of the input",
    "start": "3401140",
    "end": "3407860"
  },
  {
    "text": "and the first pass over the input the memory state is initialized to be just a",
    "start": "3407860",
    "end": "3412960"
  },
  {
    "text": "question and then afterwards at agglomerated relevant facts so intuitively here if the sentence",
    "start": "3412960",
    "end": "3420220"
  },
  {
    "text": "mentions John for instance and the question is or mentions football and the question is where is the football",
    "start": "3420220",
    "end": "3425770"
  },
  {
    "text": "then you'd hope that the question vector Q mentions has some units that are more active because football was mentioned",
    "start": "3425770",
    "end": "3432430"
  },
  {
    "text": "and the sentence vector mentions football so there's some units that are more active because football is mentioned and hence some of these inner",
    "start": "3432430",
    "end": "3439390"
  },
  {
    "text": "products or absolute values of subtractions are going to be large and then what we're going to do is just plug",
    "start": "3439390",
    "end": "3446710"
  },
  {
    "text": "that into a standard through standard single layer neural network and in a standard linear layer here and then we",
    "start": "3446710",
    "end": "3453940"
  },
  {
    "text": "apply a soft max to essentially weight all of these different potential sentences that we might have to compute",
    "start": "3453940",
    "end": "3460780"
  },
  {
    "text": "the final gate so this will basically a soft attention mechanism that sums to one and we'll pay most attention to the",
    "start": "3460780",
    "end": "3468730"
  },
  {
    "text": "facts that seem most relevant given what I no so far and the question then when the",
    "start": "3468730",
    "end": "3476320"
  },
  {
    "text": "end of the input has reached all these relevant facts here are summarized in another GRU that basically moves up here",
    "start": "3476320",
    "end": "3483040"
  },
  {
    "text": "and you can train a classifier also if you have the right kind of supervision",
    "start": "3483040",
    "end": "3488790"
  },
  {
    "text": "to basically train that the model knows enough to actually answer the question and stop iterating over the inputs if",
    "start": "3488790",
    "end": "3497050"
  },
  {
    "text": "you don't have that kind of supervision you can also just say I will go over the inputs a fixed number of times and that",
    "start": "3497050",
    "end": "3503680"
  },
  {
    "text": "that works reasonably well to all right there's a lot to sink in so I'll give",
    "start": "3503680",
    "end": "3509950"
  },
  {
    "text": "you a couple seconds basically we pay attention to different facts given a",
    "start": "3509950",
    "end": "3515020"
  },
  {
    "text": "certain question we iterate over the input multiple times and we agglomerate",
    "start": "3515020",
    "end": "3520060"
  },
  {
    "text": "the facts that seem relevant given the current knowledge and the question now I",
    "start": "3520060",
    "end": "3525700"
  },
  {
    "text": "don't usually talk about neuroscience I'm not a neuroscientist but there is a very interesting relationship here that",
    "start": "3525700",
    "end": "3531700"
  },
  {
    "text": "a friend of mine Sam Gershman pointed out which is that the episodic memory in general for humans is actually the",
    "start": "3531700",
    "end": "3539050"
  },
  {
    "text": "memory of autobiographical events so it's the time when we remember the first time I went to school or something like",
    "start": "3539050",
    "end": "3545110"
  },
  {
    "text": "that and essentially a collection of our past personal experiences that occurred at a particular time in a particular",
    "start": "3545110",
    "end": "3550780"
  },
  {
    "text": "place and just like our episodic memory that can be triggered with a variety of",
    "start": "3550780",
    "end": "3556420"
  },
  {
    "text": "different inputs this is also this episodic memory is also triggered with a",
    "start": "3556420",
    "end": "3561520"
  },
  {
    "text": "specific question at hand and what's also interesting is the hippocampus which is a seat of the episodic memory",
    "start": "3561520",
    "end": "3567700"
  },
  {
    "text": "in humans is actually active during transitive inference so transitive inference is you know going from A to B",
    "start": "3567700",
    "end": "3573130"
  },
  {
    "text": "to C to have some connection from A to C or in this case here with this football",
    "start": "3573130",
    "end": "3578470"
  },
  {
    "text": "for instance you first had to find facts about John into football and then finding where John was and then find the",
    "start": "3578470",
    "end": "3584740"
  },
  {
    "text": "location of John so those are examples of transitive inference and it turns out that you also need in the dmn these",
    "start": "3584740",
    "end": "3593380"
  },
  {
    "text": "multiple passes to enable the capability to do transitive inference now the final",
    "start": "3593380",
    "end": "3601360"
  },
  {
    "text": "module again is very simple G or UN softmax to produce the final answers the main difference here is that",
    "start": "3601360",
    "end": "3608259"
  },
  {
    "text": "instead of just having the current the previous hidden state 18 minus 1 as",
    "start": "3608259",
    "end": "3613869"
  },
  {
    "text": "input will also include the question at every time and we will include the",
    "start": "3613869",
    "end": "3619390"
  },
  {
    "text": "answer that was generated at the previous time step but rather than that it's our standard softmax from your",
    "start": "3619390",
    "end": "3625029"
  },
  {
    "text": "standard cross-entropy errors to minimize it and now beautiful thing of this whole model is that it's end-to-end",
    "start": "3625029",
    "end": "3630400"
  },
  {
    "text": "trainable these four different modules will actually all train based on the",
    "start": "3630400",
    "end": "3636309"
  },
  {
    "text": "cross entropy of that final softmax all these different modules communicate with vectors and we'll just have Delta",
    "start": "3636309",
    "end": "3643420"
  },
  {
    "text": "messages and back propagation to train them now there's been a lot of work in",
    "start": "3643420",
    "end": "3649569"
  },
  {
    "text": "the last two years on models like this in fact quoc will cover a lot of these",
    "start": "3649569",
    "end": "3655089"
  },
  {
    "text": "really interesting models tomorrow different types of memory structures and so on and the dynamic memory network is",
    "start": "3655089",
    "end": "3661089"
  },
  {
    "text": "in some sense one of those models one one particular model is a proper",
    "start": "3661089",
    "end": "3668380"
  },
  {
    "text": "comparison because it's there a lot of similarities namely memory networks from",
    "start": "3668380",
    "end": "3673420"
  },
  {
    "text": "jason weston those basically also have inputs and scoring and attention",
    "start": "3673420",
    "end": "3679569"
  },
  {
    "text": "response mechanisms the main difference is that they use different kinds of",
    "start": "3679569",
    "end": "3685079"
  },
  {
    "text": "basic Lego blocks for these different kinds of mechanisms for input they use",
    "start": "3685079",
    "end": "3690730"
  },
  {
    "text": "bag of words representation z' or non-linear on linear embeddings for the",
    "start": "3690730",
    "end": "3696220"
  },
  {
    "text": "attention and responses they have different kinds of iteratively to run functions the main interesting sort of",
    "start": "3696220",
    "end": "3702369"
  },
  {
    "text": "difference to the dmn is that the dmn really use this recurrent neural network",
    "start": "3702369",
    "end": "3708309"
  },
  {
    "text": "type sequence models for all of these different modules and capabilities and in some sense that helps us to have a",
    "start": "3708309",
    "end": "3716109"
  },
  {
    "text": "broader range of applications that include things like sequence tagging and so let me go over a couple of results",
    "start": "3716109",
    "end": "3723250"
  },
  {
    "text": "and experiments of this model so the first one is on this Bobbie dataset did",
    "start": "3723250",
    "end": "3729730"
  },
  {
    "text": "Facebook publish it basically has a lot",
    "start": "3729730",
    "end": "3734950"
  },
  {
    "text": "of these kinds of simple logical reasoning type questions in fact all these like where's the",
    "start": "3734950",
    "end": "3740049"
  },
  {
    "text": "Paul those were examples from the Facebook Bobby data set and it also includes things like yes/no questions",
    "start": "3740049",
    "end": "3747359"
  },
  {
    "text": "simple counting negation some indefinite knowledge where the answer might be may",
    "start": "3747359",
    "end": "3752619"
  },
  {
    "text": "be basic coreference where you have to realize what does she",
    "start": "3752619",
    "end": "3757630"
  },
  {
    "text": "who does she refer to or he reasoning over time if this happened before that",
    "start": "3757630",
    "end": "3762749"
  },
  {
    "text": "and so on and basically this dynamic memory network I think is currently the",
    "start": "3762749",
    "end": "3768579"
  },
  {
    "text": "state of the art on this data set of the simple simple logical reasoning now the",
    "start": "3768579",
    "end": "3774819"
  },
  {
    "text": "problem with this data set is that it's a synthetic data set and so it had only",
    "start": "3774819",
    "end": "3779890"
  },
  {
    "text": "a certain set of generating like human general human defined generative",
    "start": "3779890",
    "end": "3786099"
  },
  {
    "text": "functions that created certain patterns and in that sense it's only necessary",
    "start": "3786099",
    "end": "3791949"
  },
  {
    "text": "and not a sufficient condition of solving it with sometimes a hundred percent accuracy to real question",
    "start": "3791949",
    "end": "3797319"
  },
  {
    "text": "answering so there's still a lot of complexity the main interesting bit to",
    "start": "3797319",
    "end": "3802959"
  },
  {
    "text": "point out here is that there are different numbers of training examples for each of these different subtasks and",
    "start": "3802959",
    "end": "3810130"
  },
  {
    "text": "so you have basically a thousand examples of simple negation for instance and it's always a similar kind of",
    "start": "3810130",
    "end": "3816670"
  },
  {
    "text": "pattern and hence you're able to classify it very well now real language you will never have that many examples",
    "start": "3816670",
    "end": "3822219"
  },
  {
    "text": "for each type of pattern you want to learn and so it's still general question answering is still an open problem and",
    "start": "3822219",
    "end": "3828309"
  },
  {
    "text": "non-trivial now what's cool is this same architecture of allowing the model to go",
    "start": "3828309",
    "end": "3834219"
  },
  {
    "text": "over inputs multiple times also got state of the art and sentiment analysis very different kind of task and we",
    "start": "3834219",
    "end": "3843759"
  },
  {
    "text": "actually analyzed whether it's really helpful to have multiple passes over the input and it turns out it is so there's",
    "start": "3843759",
    "end": "3850869"
  },
  {
    "text": "certain things like reasoning over three facts or Counting where you really have to have this dynamic this episodic",
    "start": "3850869",
    "end": "3858999"
  },
  {
    "text": "memory module and it goes over the input maybe five times for sentiment it",
    "start": "3858999",
    "end": "3864789"
  },
  {
    "text": "actually turns out it hurts after going over the input more than two times and",
    "start": "3864789",
    "end": "3869979"
  },
  {
    "text": "that's actually one of the things we're now working on is can we find model that does the same thing for every",
    "start": "3869979",
    "end": "3875860"
  },
  {
    "text": "single input with the same weights to try to learn this different tasks we can",
    "start": "3875860",
    "end": "3881890"
  },
  {
    "text": "actually look at a couple of fun examples of this model and what happens",
    "start": "3881890",
    "end": "3888040"
  },
  {
    "text": "with tough sentiment sentences generally to be honest sentiment you can probably",
    "start": "3888040",
    "end": "3893380"
  },
  {
    "text": "get to like seventy five percent accuracy with some very simple models that just basically find like great",
    "start": "3893380",
    "end": "3899800"
  },
  {
    "text": "words like great and wonderful and awesome and you'll get to something that's roughly right here some of the",
    "start": "3899800",
    "end": "3905590"
  },
  {
    "text": "examples that those are the kinds of examples that you now need to get right to retry to push the state-of-the-art",
    "start": "3905590",
    "end": "3911560"
  },
  {
    "text": "further in sentiment analysis so here the sentences in its ragged cheap and",
    "start": "3911560",
    "end": "3917440"
  },
  {
    "text": "unassuming way the movie works so this sentence is incorrect even if you allow",
    "start": "3917440",
    "end": "3922960"
  },
  {
    "text": "the dmn but I have this whole architecture but only allow one pass over the input once you have two passes",
    "start": "3922960",
    "end": "3929620"
  },
  {
    "text": "over the input it actually learns to pay attention not just to these very strong",
    "start": "3929620",
    "end": "3935850"
  },
  {
    "text": "adjectives but in the end actually to the movie working so here these fields",
    "start": "3935850",
    "end": "3944740"
  },
  {
    "text": "are essentially the gating function G that we defined that pays attention to specific words and the darker it is the",
    "start": "3944740",
    "end": "3952720"
  },
  {
    "text": "larger that gate is and the more open it is amor that word effects the hidden",
    "start": "3952720",
    "end": "3959290"
  },
  {
    "text": "state in the episodic memory module so it goes over the input the first time",
    "start": "3959290",
    "end": "3965190"
  },
  {
    "text": "pays attention to cheap and unassuming and way and a little bit of works too",
    "start": "3965190",
    "end": "3970870"
  },
  {
    "text": "but the second time it basically figured out it agglomerate it's sort of the facts of that sentence and then learn to",
    "start": "3970870",
    "end": "3977200"
  },
  {
    "text": "pay attention more to specific words that seem more important just one more",
    "start": "3977200",
    "end": "3984430"
  },
  {
    "text": "example here my response to the film is best described as lukewarm so in general",
    "start": "3984430",
    "end": "3991060"
  },
  {
    "text": "sentiment analysis when you look at unique an scores like the word best is",
    "start": "3991060",
    "end": "3997960"
  },
  {
    "text": "basically some of the most one of the most positive words you could possibly use in a sentence and the first time the",
    "start": "3997960",
    "end": "4004590"
  },
  {
    "text": "model passes over the sentence that also pays most attention took this incredibly positive word maybe",
    "start": "4004590",
    "end": "4009990"
  },
  {
    "text": "best but then this site once it agglomerate at the context actually realizes well best actually here is not",
    "start": "4009990",
    "end": "4018240"
  },
  {
    "text": "used in its adjective way but it's actually an adverb that best describes",
    "start": "4018240",
    "end": "4023310"
  },
  {
    "text": "something and what it describes is actually lukewarm and hence it's actually a negative sentence so those",
    "start": "4023310",
    "end": "4029520"
  },
  {
    "text": "are the kinds of examples that you need to get to now to appreciate improvements in sentiment analysis where we basically",
    "start": "4029520",
    "end": "4036260"
  },
  {
    "text": "also went from on this particular data set these are all neural network type",
    "start": "4036260",
    "end": "4042030"
  },
  {
    "text": "models that started 82 until then that same data set existed for around 8 years",
    "start": "4042030",
    "end": "4047579"
  },
  {
    "text": "and none of the standard NLP models had reached above 80% accuracy and now we're",
    "start": "4047579",
    "end": "4053760"
  },
  {
    "text": "basically in the high high 80s and and those are the kinds of improvements that that you see across a variety of",
    "start": "4053760",
    "end": "4060660"
  },
  {
    "text": "different NLP tasks now that deep learning has come and deep learning",
    "start": "4060660",
    "end": "4067079"
  },
  {
    "text": "techniques are being used in NLP and now the last task in NLP that this model",
    "start": "4067079",
    "end": "4072359"
  },
  {
    "text": "turn out are also working for Ivy Wallen as part of speech tagging now part of speech tagging is less exciting of a task it's more of an intermediate task",
    "start": "4072359",
    "end": "4079160"
  },
  {
    "text": "but it's still fascinating to see that after this data set has been around for",
    "start": "4079160",
    "end": "4084540"
  },
  {
    "text": "over 20 years you can still improve the state of the art was the same kind of architecture",
    "start": "4084540",
    "end": "4089849"
  },
  {
    "text": "that also did well and fuzzy reasoning of sentiment and discrete logical reasoning for for question answering now",
    "start": "4089849",
    "end": "4097350"
  },
  {
    "text": "we had a new person joined a group Zhiming and he he thought well that's",
    "start": "4097350",
    "end": "4104250"
  },
  {
    "text": "cool but he was more of a computer vision researcher and so he thought well could",
    "start": "4104250",
    "end": "4109920"
  },
  {
    "text": "I use this create question-answering module now to do visual question-answering so combine sort of",
    "start": "4109920",
    "end": "4116460"
  },
  {
    "text": "some stat was going on in the group and NLP and apply it to a computer vision and he did not have to know all of the",
    "start": "4116460",
    "end": "4124830"
  },
  {
    "text": "different aspects of the code all he had to do was change the input module from",
    "start": "4124830",
    "end": "4130230"
  },
  {
    "text": "one that gives you hidden states at each word over a long sequence of you know",
    "start": "4130230",
    "end": "4137338"
  },
  {
    "text": "words and sentences to an input module that would give him vector years four sequences of regions in an",
    "start": "4137339",
    "end": "4143970"
  },
  {
    "text": "image and he literally did not touch some of the other parts of the code I",
    "start": "4143970",
    "end": "4149270"
  },
  {
    "text": "did have to look carefully at this input module aware again here our basic Lego",
    "start": "4149270",
    "end": "4155940"
  },
  {
    "text": "block that Andre introduced really well of our convolutional neural network and",
    "start": "4155940",
    "end": "4161790"
  },
  {
    "text": "then each the convolutional networks will essentially give us 14 by 14 many",
    "start": "4161790",
    "end": "4166830"
  },
  {
    "text": "vectors one for each and it's one of its top states one representing each region",
    "start": "4166830",
    "end": "4172859"
  },
  {
    "text": "of an image and then what we'll do is basically take those vectors and now replace the word vectors we used to have",
    "start": "4172859",
    "end": "4179400"
  },
  {
    "text": "with CNN vectors and then plug them into GRU now again the GRU we know as our",
    "start": "4179400",
    "end": "4185759"
  },
  {
    "text": "basic Lego block we already defined it one addition here is that it'll actually be a bi-directional GRU will go once",
    "start": "4185760",
    "end": "4194100"
  },
  {
    "text": "from left to right in this snake-like fashion and another one goes from right",
    "start": "4194100",
    "end": "4199200"
  },
  {
    "text": "to left backwards now both of these will basically have hidden state and you can",
    "start": "4199200",
    "end": "4204240"
  },
  {
    "text": "just concatenate the hidden states of both of these to compute the final hidden state at each for each block of",
    "start": "4204240",
    "end": "4211560"
  },
  {
    "text": "the image and that model to actually achieve state-of-the-art results this",
    "start": "4211560",
    "end": "4219090"
  },
  {
    "text": "data set has been only released last year so everybody now works on deep learning techniques to try to solve it",
    "start": "4219090",
    "end": "4225290"
  },
  {
    "text": "and I was at first a little skeptical it was just too good to be true that this",
    "start": "4225290",
    "end": "4230520"
  },
  {
    "text": "model we developed for NLP would work so well so we really dug in to looking at",
    "start": "4230520",
    "end": "4235530"
  },
  {
    "text": "the attention so what I showed you here these G values again that we computed",
    "start": "4235530",
    "end": "4244550"
  },
  {
    "text": "with this equation now instead of paying attention to words it paid attention to",
    "start": "4244550",
    "end": "4251490"
  },
  {
    "text": "different regions in the image and we started basically analyzing going",
    "start": "4251490",
    "end": "4257310"
  },
  {
    "text": "through a bunch of those on the Deaf set and analyzing what is it actually paying attention to again it's being trained",
    "start": "4257310",
    "end": "4263850"
  },
  {
    "text": "only with the image the question and the final answer that's what you get a",
    "start": "4263850",
    "end": "4269010"
  },
  {
    "text": "training time you do not get this sort of latent representation of where you",
    "start": "4269010",
    "end": "4274140"
  },
  {
    "text": "should actually pay it attention to in the image in order to answer that question correctly so when",
    "start": "4274140",
    "end": "4279300"
  },
  {
    "text": "the question was what is the main color on the bus and learned to actually pay attention here to that bus mic well okay",
    "start": "4279300",
    "end": "4286350"
  },
  {
    "text": "maybe that's not that impressive it's just the main object in the center of the image and you know what it types the",
    "start": "4286350",
    "end": "4293010"
  },
  {
    "text": "type of trees are in the background well maybe it just you know connects tree with anything that's green and pays",
    "start": "4293010",
    "end": "4299760"
  },
  {
    "text": "attention to that so I was neat but you know not not super impressive yet so is",
    "start": "4299760",
    "end": "4306060"
  },
  {
    "text": "this in the wild kind of more interesting and actually pays attention to a man-made structure in the background and correctly answer's no",
    "start": "4306060",
    "end": "4314390"
  },
  {
    "text": "then this one is kind of interesting who is on both photos the answers girl now",
    "start": "4314390",
    "end": "4321510"
  },
  {
    "text": "to be honest I don't think the model actually knows that there are two people tries to match them and so on it just",
    "start": "4321510",
    "end": "4328949"
  },
  {
    "text": "finds the main person or main object in in this in the scene the main object is",
    "start": "4328949",
    "end": "4334890"
  },
  {
    "text": "a little baby girl so it says girl this one's also relatively trivial what time",
    "start": "4334890",
    "end": "4340650"
  },
  {
    "text": "of day was this picture taken the answers night because it's very dark picture at least in the sky now this one",
    "start": "4340650",
    "end": "4347010"
  },
  {
    "text": "is getting a little more interesting what is the boy holding the answer a surfboard and it actually does pay",
    "start": "4347010",
    "end": "4352739"
  },
  {
    "text": "attention to both of the arms and then what's just below that arm so that's a little more interesting kind of",
    "start": "4352739",
    "end": "4359580"
  },
  {
    "text": "attention visualization and then for a while we're also worried well what if in",
    "start": "4359580",
    "end": "4364860"
  },
  {
    "text": "the data set it just learns really well from language alone yes it pays attention to things but maybe it'll just",
    "start": "4364860",
    "end": "4370679"
  },
  {
    "text": "say things that it often sees in the text so if I asked you what or what color are the bananas you don't really",
    "start": "4370679",
    "end": "4376739"
  },
  {
    "text": "have to look at an image in 95% of the cases you're right just saying yellow without seeing an image so it was really",
    "start": "4376739",
    "end": "4383820"
  },
  {
    "text": "this one I was kind of excited about because it actually paid attention to the bananas in the middle and then did",
    "start": "4383820",
    "end": "4389850"
  },
  {
    "text": "say green and kind of overruled the prior that it would get from from",
    "start": "4389850",
    "end": "4395070"
  },
  {
    "text": "language alone what's the pattern on the cat's fur on its tail pays attention",
    "start": "4395070",
    "end": "4400969"
  },
  {
    "text": "mostly to the tail and says stripes now this one here was interesting and fit",
    "start": "4400969",
    "end": "4406770"
  },
  {
    "text": "the player hit the ball the answer yes though I have to say that we later had a journalist want to do his own",
    "start": "4406770",
    "end": "4415350"
  },
  {
    "text": "question he he asked John marker from New York Times and we just put together",
    "start": "4415350",
    "end": "4421680"
  },
  {
    "text": "this demo and the night before and he's like well I want to ask my own question and I am like okay and he asked is the",
    "start": "4421680",
    "end": "4430350"
  },
  {
    "text": "girl wearing a hat and you know it wasn't made for production so it's kind of slow and the system was cranking it",
    "start": "4430350",
    "end": "4436320"
  },
  {
    "text": "like well you know like trying to come up with excuses it's kind of black background and the plaque hat and it",
    "start": "4436320",
    "end": "4441990"
  },
  {
    "text": "might be kind of hard to see and unfortunately I got it right and said yes and then after the interview I said",
    "start": "4441990",
    "end": "4448170"
  },
  {
    "text": "well maybe let's look and see if like what I imma just asked it myself less",
    "start": "4448170",
    "end": "4453450"
  },
  {
    "text": "stressful situation a bunch of questions on my own and these are all the questions like the first eight questions",
    "start": "4453450",
    "end": "4459300"
  },
  {
    "text": "that I could come up with and somewhat to my surprise it actually got them all right so what is the girl holding a",
    "start": "4459300",
    "end": "4465210"
  },
  {
    "text": "tennis racket what's she playing playing tennis or what's she doing I was to go wearing shorts what is the color of the",
    "start": "4465210",
    "end": "4472080"
  },
  {
    "text": "ground brown then I was like well okay let's try to break it by asking just like what's the color of like the sound",
    "start": "4472080",
    "end": "4477120"
  },
  {
    "text": "of this the smallest object the ball actually got that right to because her skirt white also kind of interesting",
    "start": "4477120",
    "end": "4483840"
  },
  {
    "text": "like when you asked him all what she's wearing shorts but in you asked about the skirt and it still sort of is you",
    "start": "4483840",
    "end": "4489900"
  },
  {
    "text": "know sort of capturing that you might call this different things what and then this one was interesting",
    "start": "4489900",
    "end": "4495810"
  },
  {
    "text": "what did the girl just hit tennis ball and then as like well what if I asked is",
    "start": "4495810",
    "end": "4501240"
  },
  {
    "text": "the girl about to hit the tennis ball and said yes and then did the girl just hit the tennis ball and it said yes",
    "start": "4501240",
    "end": "4507270"
  },
  {
    "text": "again so then I finally found a way to break it so it doesn't have enough the Corcoran statistics to understand and",
    "start": "4507270",
    "end": "4513330"
  },
  {
    "text": "again spare quote understand sort of which angles does the arm have to be in order to assume that the ball was just",
    "start": "4513330",
    "end": "4519780"
  },
  {
    "text": "adores about it but what it basically does show us is that once it saw a lot",
    "start": "4519780",
    "end": "4526410"
  },
  {
    "text": "of examples on a specific domain it really can capture quite a lot of different things now see if we can get",
    "start": "4526410",
    "end": "4534210"
  },
  {
    "text": "the demo up I have to be a VPN to make it work but so here's here's one",
    "start": "4534210",
    "end": "4542100"
  },
  {
    "text": "one example the best way to hope for any chance of enjoying this film is by lowering your expectations again one of",
    "start": "4542100",
    "end": "4548610"
  },
  {
    "text": "those kinds of sentences that you have to now get correct in order to get",
    "start": "4548610",
    "end": "4556310"
  },
  {
    "text": "improved performance on sentiment and actually correctly says that this is",
    "start": "4556310",
    "end": "4561930"
  },
  {
    "text": "this is negative now we can also actually ask that question in Chinese",
    "start": "4561930",
    "end": "4569000"
  },
  {
    "text": "this is one of the beautiful things off of the dmn and in general really of most",
    "start": "4569000",
    "end": "4576030"
  },
  {
    "text": "deep learning techniques we don't have to be experts in a domain or even in a language to create a very very accurate",
    "start": "4576030",
    "end": "4581940"
  },
  {
    "text": "model for for that language or that domain there's no more future",
    "start": "4581940",
    "end": "4587670"
  },
  {
    "text": "engineering I'm not going to make a fool of myself trying to read that one out loud but that's an interesting example",
    "start": "4587670",
    "end": "4594320"
  },
  {
    "text": "you can also this is the what parts of speech are there you can have other things like you know named entities and",
    "start": "4594590",
    "end": "4601680"
  },
  {
    "text": "other sequence problems I can also ask what are the men wearing on the head",
    "start": "4601680",
    "end": "4606980"
  },
  {
    "text": "answers helmets and then maybe a slightly more interesting question why are the men wearing helmets and the",
    "start": "4606980",
    "end": "4615570"
  },
  {
    "text": "answer is safety so especially we're close to the circle of death here at Stanford where a lot of bikes crash and",
    "start": "4615570",
    "end": "4622860"
  },
  {
    "text": "it's a good answer all right with that I'll leave a couple of minutes for for",
    "start": "4622860",
    "end": "4629100"
  },
  {
    "text": "questions so basically the summary is word vectors and recurrent neural networks are super useful building",
    "start": "4629100",
    "end": "4636060"
  },
  {
    "text": "blocks once you really appreciate and understand those two building blocks you're kind of ready to have some fun",
    "start": "4636060",
    "end": "4642210"
  },
  {
    "text": "and build more complex models really in the end this dmn is a way to combine that in just a variety of new ways to a",
    "start": "4642210",
    "end": "4649980"
  },
  {
    "text": "larger more complex model and that's also where the state I think of deep learning is for natural language",
    "start": "4649980",
    "end": "4655770"
  },
  {
    "text": "processing we've tackled a lot of these smaller sub-problems intermediate tasks and now we can work on more interesting",
    "start": "4655770",
    "end": "4662960"
  },
  {
    "text": "complex problems like dialogue and question answering machine translation and things like that all right",
    "start": "4662960",
    "end": "4669450"
  },
  {
    "text": "thank you",
    "start": "4669450",
    "end": "4671810"
  },
  {
    "text": "I mean all right cool yeah a quick",
    "start": "4676150",
    "end": "4682240"
  },
  {
    "text": "question in the dynamic memory Network you have the the RN and you also",
    "start": "4682240",
    "end": "4688420"
  },
  {
    "text": "mentioned that if you have better assumption of the input right so you",
    "start": "4688420",
    "end": "4694960"
  },
  {
    "text": "used to work on the tray LST M right so if you change they are in into a tree",
    "start": "4694960",
    "end": "4701320"
  },
  {
    "text": "structure would that help it's a good question I I actually loved researchers",
    "start": "4701320",
    "end": "4706810"
  },
  {
    "text": "at in my whole PhD about tree structures and somewhat surprising in the last",
    "start": "4706810",
    "end": "4712210"
  },
  {
    "text": "couple of weeks to actually some new results on SNL I understand for natural",
    "start": "4712210",
    "end": "4717520"
  },
  {
    "text": "language inference data said where tree structures are again the state of the art and I have to say that I think the",
    "start": "4717520",
    "end": "4726120"
  },
  {
    "text": "the dynamic memory Network by having this ability in the episodic memory to",
    "start": "4726120",
    "end": "4731140"
  },
  {
    "text": "keep track of different sub phrases and pay attention to those and then combine them over multiple passes I think you",
    "start": "4731140",
    "end": "4738940"
  },
  {
    "text": "can kind of get away with not having a tree structures so yes you might have a slight improvement representing",
    "start": "4738940",
    "end": "4746760"
  },
  {
    "text": "sentences as trees in your input module but I think they're only going to be slight and I think the episodic memory",
    "start": "4746760",
    "end": "4753100"
  },
  {
    "text": "module that has this capability to go over the input multiple times pay attention to certain sub phrases will capture a lot of the kinds of",
    "start": "4753100",
    "end": "4759610"
  },
  {
    "text": "complexities that you might want to capture in tree structures so I don't my short answer is I don't think you necessarily need it have you tried it we",
    "start": "4759610",
    "end": "4767230"
  },
  {
    "text": "have not no thanks hi a question is about question",
    "start": "4767230",
    "end": "4774580"
  },
  {
    "text": "answering say if we want to apply questions into some specific domains",
    "start": "4774580",
    "end": "4779980"
  },
  {
    "text": "that health healthcare but we don't really have the data we don't have questions appears and what sure we'll do",
    "start": "4779980",
    "end": "4786430"
  },
  {
    "text": "are there any general principles here it's a great question what do you do if",
    "start": "4786430",
    "end": "4791530"
  },
  {
    "text": "you want to question answering on a complex domain you don't have the data I think and this feels maybe like a",
    "start": "4791530",
    "end": "4797830"
  },
  {
    "text": "cop-out but I think it's very true both in practice and in theory create the data like if you cannot possibly create",
    "start": "4797830",
    "end": "4804490"
  },
  {
    "text": "more than a thousand examples of anything then maybe automating that process is not that important so clearly you",
    "start": "4804490",
    "end": "4811210"
  },
  {
    "text": "should be able to create some data and in many cases that is the best use of your time is just to sit down or ask the",
    "start": "4811210",
    "end": "4816850"
  },
  {
    "text": "domain expert to create a lot of questions and then have people find the answers and then measure how they",
    "start": "4816850",
    "end": "4823570"
  },
  {
    "text": "actually get to those answers try to have them in a constrained environment and so on I think most companies for",
    "start": "4823570",
    "end": "4829300"
  },
  {
    "text": "instance when you try to do automated email replies which is in some ways a little bit similar to question answering",
    "start": "4829300",
    "end": "4834670"
  },
  {
    "text": "well there's a nice nice nice domain because everybody had already emailed",
    "start": "4834670",
    "end": "4840340"
  },
  {
    "text": "there were already answered before so you can use sort of past behavior now if you had a search engine where people",
    "start": "4840340",
    "end": "4846850"
  },
  {
    "text": "asked a lot of questions then you can also use that too in bootstrap and see where did they actually fail and then",
    "start": "4846850",
    "end": "4852369"
  },
  {
    "text": "take all those really tough queries where they failed have some humans sit there and collect the data so that's",
    "start": "4852369",
    "end": "4857710"
  },
  {
    "text": "that's the simplest answer now the other answer is let's work together for the Mexican like many years on research for",
    "start": "4857710",
    "end": "4864880"
  },
  {
    "text": "smaller training data set sizes and complex reasoning the the fact of the",
    "start": "4864880",
    "end": "4870670"
  },
  {
    "text": "matter for that line of research will still be if you if a system has never seen a certain type of reasoning I'll be",
    "start": "4870670",
    "end": "4877210"
  },
  {
    "text": "hard for the systems to pick up that type of reasoning I think we're going to",
    "start": "4877210",
    "end": "4882550"
  },
  {
    "text": "get with these kinds of architectures to the space where at least if it has seen this type of reasoning a specific type",
    "start": "4882550",
    "end": "4888610"
  },
  {
    "text": "of transitive reasoning or temporal reasoning or sort of cause and effect type reasoning at least like a couple",
    "start": "4888610",
    "end": "4895719"
  },
  {
    "text": "hundred times then you should be able to train a system with these kinds of models to do it are these QA systems",
    "start": "4895719",
    "end": "4905739"
  },
  {
    "text": "currently robust to false input our questions for the woman playing tennis if you asked what's the man holding",
    "start": "4905739",
    "end": "4913119"
  },
  {
    "text": "would it replied there is no man it would not and largely because at",
    "start": "4913119",
    "end": "4919090"
  },
  {
    "text": "training time you never try to mess with it like that I'm pretty sure if you added a lot of training examples where",
    "start": "4919090",
    "end": "4925960"
  },
  {
    "text": "you had those it would probably eventually pick it up those would be important for like real-world implementations and so real-world",
    "start": "4925960",
    "end": "4933130"
  },
  {
    "text": "implementations of this in security are actually kind of tricky I think whenever you train a system we know we",
    "start": "4933130",
    "end": "4939520"
  },
  {
    "text": "can for instance both steal certain classifiers by using them a lot we know we can fool them into",
    "start": "4939520",
    "end": "4945099"
  },
  {
    "text": "classifying certain images for instance as others we have folks in the audience who worked on that exact line of work so",
    "start": "4945099",
    "end": "4952789"
  },
  {
    "text": "I would be careful using it in security environments right now yeah I have a",
    "start": "4952789",
    "end": "4959389"
  },
  {
    "text": "question oh wow up there yeah I have a question actually",
    "start": "4959389",
    "end": "4965480"
  },
  {
    "text": "uh there was a slide where you had the input module and and there were a bunch of sentences so what those sentences",
    "start": "4965480",
    "end": "4972920"
  },
  {
    "text": "themselves are n ends because you know sequence is basically made up of those individual words in sake love you know",
    "start": "4972920",
    "end": "4980360"
  },
  {
    "text": "representation so what those you know also when are n ends that word you know stitch together or so the answer there",
    "start": "4980360",
    "end": "4986929"
  },
  {
    "text": "is a little complex because we have two two papers with the dmn and the answer is different for each the simplest in",
    "start": "4986929",
    "end": "4994610"
  },
  {
    "text": "the simplest form of that there it is actually a single gru that goes from the first word through all the sentences as if there",
    "start": "4994610",
    "end": "5001929"
  },
  {
    "text": "are one gigantic sequence and but it has access to each sentence period at the end to pay a special attention to the",
    "start": "5001929",
    "end": "5008679"
  },
  {
    "text": "end of sentences and so yes in the simplest form it is just a giryu that",
    "start": "5008679",
    "end": "5013690"
  },
  {
    "text": "goes over all the words this is a normal process to basically just concatenate all the sentences into one gigantic you",
    "start": "5013690",
    "end": "5021130"
  },
  {
    "text": "know so the answer there and this is kind of why I split the the talk into",
    "start": "5021130",
    "end": "5026650"
  },
  {
    "text": "three different ones from like words single sentences and in multiple sentences I think if you just had a",
    "start": "5026650",
    "end": "5032170"
  },
  {
    "text": "single gru that goes over everything and now you try to reason over that entire sequence it would not work very well",
    "start": "5032170",
    "end": "5037750"
  },
  {
    "text": "your read to have an additional structure such as an intention mechanism or a pointer mechanism that has the",
    "start": "5037750",
    "end": "5044199"
  },
  {
    "text": "ability to pay attention to specific parts of your input to do that very accurately but yeah in general that's",
    "start": "5044199",
    "end": "5050380"
  },
  {
    "text": "fine as long as you have this additional mechanism thank you thank you great question so in the recurrent neural Nets",
    "start": "5050380",
    "end": "5057610"
  },
  {
    "text": "you're using sigmoids in visual recognition I guess are",
    "start": "5057610",
    "end": "5063389"
  },
  {
    "text": "rectified linear units for the more popular non-linearity that's right so rail users are great now when you look",
    "start": "5063389",
    "end": "5070719"
  },
  {
    "text": "at the GRU equations here and you have these reset gates and so these reset gates here",
    "start": "5070719",
    "end": "5076039"
  },
  {
    "text": "you want them to essentially be be between zero and one so that it can either ignore this input entirely or you",
    "start": "5076039",
    "end": "5081769"
  },
  {
    "text": "have it normally be part of the computation of H tilt so in some cases you really do want to have Sigma lights",
    "start": "5081769",
    "end": "5090170"
  },
  {
    "text": "there but other ones for instance some like simpler things where you actually",
    "start": "5090170",
    "end": "5095510"
  },
  {
    "text": "don't have that much recurrence such as going from one member state to another in the second iteration of this model",
    "start": "5095510",
    "end": "5101690"
  },
  {
    "text": "actually rail used were we're good mom good like activation functions to did",
    "start": "5101690",
    "end": "5108530"
  },
  {
    "text": "you guys try to after training this network try to take these weights for",
    "start": "5108530",
    "end": "5113869"
  },
  {
    "text": "the images and do object detection again so these weights would be augmented with",
    "start": "5113869",
    "end": "5118940"
  },
  {
    "text": "the text victors did you try to use that is a very cool idea that we did not",
    "start": "5118940",
    "end": "5124849"
  },
  {
    "text": "explore no there you go you got to do it",
    "start": "5124849",
    "end": "5129920"
  },
  {
    "text": "fast yeah feel this feel is moving fast you just let the cat out of the box so so",
    "start": "5129920",
    "end": "5141199"
  },
  {
    "text": "those attention models are pretty powerful when you have an opportunity data and then you can learn you know to",
    "start": "5141199",
    "end": "5146690"
  },
  {
    "text": "make make yourself with data but even",
    "start": "5146690",
    "end": "5151820"
  },
  {
    "text": "though those are some of the tasks are pretty gets a trivial to human but it's hard for model tuner so what do you",
    "start": "5151820",
    "end": "5159289"
  },
  {
    "text": "think of a casinos right now even right now we have not a non G base on the web right no inequity pedia we not we know a",
    "start": "5159289",
    "end": "5167150"
  },
  {
    "text": "lot about you know common sense but how what do you think about you cover those knowledge base into those models I",
    "start": "5167150",
    "end": "5175989"
  },
  {
    "text": "actually love that line of research too and that was kind of what we start out with this semantic memory module in the",
    "start": "5175989",
    "end": "5182539"
  },
  {
    "text": "simplest form is just word vectors I think in one next iteration would activity to have knowledge bases also",
    "start": "5182539",
    "end": "5187670"
  },
  {
    "text": "influence the reasoning there's very little work on combining text and",
    "start": "5187670",
    "end": "5194119"
  },
  {
    "text": "knowledge bases to do overall complex question answering that requires reasoning thing is a phenomenally",
    "start": "5194119",
    "end": "5200539"
  },
  {
    "text": "interesting area of research so where any night hints or any starting point",
    "start": "5200539",
    "end": "5205579"
  },
  {
    "text": "about it so there are some photos there are some papers that reasoning over knowledge bases alone so",
    "start": "5205579",
    "end": "5212750"
  },
  {
    "text": "we had a paper on recursive no tensor networks that basically takes a triplet a word vector for an entity might be in",
    "start": "5212750",
    "end": "5220699"
  },
  {
    "text": "freebase might be in word net a relation a vector for a relationship and a vector",
    "start": "5220699",
    "end": "5227449"
  },
  {
    "text": "for another entity and then basically pipe them into a neural network and say yes no are these two entities actually",
    "start": "5227449",
    "end": "5232880"
  },
  {
    "text": "in that relationship and you can have a variety of different architectures I think semi work done on that as well",
    "start": "5232880",
    "end": "5239170"
  },
  {
    "text": "wait that's a different brother different Benjy oh I think over there all right and it's true that's true yeah",
    "start": "5239170",
    "end": "5248750"
  },
  {
    "text": "if antoine board right that's right that's right so so i think you can also",
    "start": "5248750",
    "end": "5254060"
  },
  {
    "text": "reason over knowledge graphs and you could then try to combine that with reasoning over fuzzy text it has been a",
    "start": "5254060",
    "end": "5261679"
  },
  {
    "text": "boat it all has been done i think nobody has yet really combined it in a principled way great question yeah one last question",
    "start": "5261679",
    "end": "5269600"
  },
  {
    "text": "a whole question so so what the model answer my questions correctly so how do",
    "start": "5269600",
    "end": "5275750"
  },
  {
    "text": "i check the model actually understand understood my question and the woods which are logic was a models logic",
    "start": "5275750",
    "end": "5282590"
  },
  {
    "text": "behind that it's a good question in some ways it's a common question for for",
    "start": "5282590",
    "end": "5288890"
  },
  {
    "text": "neural network interpretability so income division at the sometimes we can at least the visualizes the features",
    "start": "5288890",
    "end": "5295610"
  },
  {
    "text": "right so how about the right and so i think the best thing that we could do right now is to show these attention",
    "start": "5295610",
    "end": "5302390"
  },
  {
    "text": "scores where you know for sentiment we're like oh how did it come up the sentiment oh it paid attention to the",
    "start": "5302390",
    "end": "5308239"
  },
  {
    "text": "movie working and likewise for question answering we can see like which facts at",
    "start": "5308239",
    "end": "5313340"
  },
  {
    "text": "which sentences that actually pay attention to in order to answer that overall question so that is I think the",
    "start": "5313340",
    "end": "5319130"
  },
  {
    "text": "best answer that we could come up with right now but how yeah there's certain other complexities that there's still an",
    "start": "5319130",
    "end": "5325280"
  },
  {
    "text": "area of open resources thank you all right thank you everybody",
    "start": "5325280",
    "end": "5330489"
  },
  {
    "text": "so thank you Richard we'll take another coffee break for 30 minutes so please come back at 2:45 but for a presentation",
    "start": "5334850",
    "end": "5341940"
  },
  {
    "text": "by sherry more",
    "start": "5341940",
    "end": "5344780"
  }
]