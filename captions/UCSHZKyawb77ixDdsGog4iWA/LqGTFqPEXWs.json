[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": "there's some magic on learning rate that",
    "start": "1150",
    "end": "3730"
  },
  {
    "text": "you played around with yeah interesting",
    "start": "3730",
    "end": "5529"
  },
  {
    "text": "yeah so this is all work that came from",
    "start": "5529",
    "end": "7180"
  },
  {
    "text": "a guy called Leslie Smith Leslie's a",
    "start": "7180",
    "end": "10290"
  },
  {
    "text": "researcher who like us cares a lot about",
    "start": "10290",
    "end": "13980"
  },
  {
    "text": "just the practicalities of training",
    "start": "13980",
    "end": "17530"
  },
  {
    "text": "neural networks quickly and accurately",
    "start": "17530",
    "end": "19590"
  },
  {
    "text": "which i think is what everybody should",
    "start": "19590",
    "end": "21700"
  },
  {
    "start": "20000",
    "end": "45000"
  },
  {
    "text": "care about but almost nobody does and he",
    "start": "21700",
    "end": "26010"
  },
  {
    "text": "discovered something very interesting",
    "start": "26010",
    "end": "27610"
  },
  {
    "text": "which he calls super convergence which",
    "start": "27610",
    "end": "29980"
  },
  {
    "text": "is there are certain networks that with",
    "start": "29980",
    "end": "31570"
  },
  {
    "text": "certain settings of high parameters",
    "start": "31570",
    "end": "33190"
  },
  {
    "text": "could suddenly be trained 10 times",
    "start": "33190",
    "end": "35920"
  },
  {
    "text": "faster by using a 10 times higher",
    "start": "35920",
    "end": "38170"
  },
  {
    "text": "learning rate now no one published that",
    "start": "38170",
    "end": "43179"
  },
  {
    "text": "paper because it's not an area of kind",
    "start": "43179",
    "end": "48640"
  },
  {
    "start": "45000",
    "end": "96000"
  },
  {
    "text": "of active research in the academic world",
    "start": "48640",
    "end": "50320"
  },
  {
    "text": "no academics recognized this is",
    "start": "50320",
    "end": "52059"
  },
  {
    "text": "important and also deep learning in",
    "start": "52059",
    "end": "55079"
  },
  {
    "text": "academia is not considered a",
    "start": "55079",
    "end": "58409"
  },
  {
    "text": "experimental science so unlike in",
    "start": "58409",
    "end": "60579"
  },
  {
    "text": "physics where you could say like I just",
    "start": "60579",
    "end": "62710"
  },
  {
    "text": "saw as a subatomic particle do something",
    "start": "62710",
    "end": "65470"
  },
  {
    "text": "which the theory doesn't explain you",
    "start": "65470",
    "end": "67480"
  },
  {
    "text": "could publish that without an",
    "start": "67480",
    "end": "69490"
  },
  {
    "text": "explanation and then in the next 60",
    "start": "69490",
    "end": "71620"
  },
  {
    "text": "years people can try to work out how to",
    "start": "71620",
    "end": "73270"
  },
  {
    "text": "explain it",
    "start": "73270",
    "end": "73870"
  },
  {
    "text": "we don't allow this in the deep learning",
    "start": "73870",
    "end": "75880"
  },
  {
    "text": "world so it's it's literally impossible",
    "start": "75880",
    "end": "78520"
  },
  {
    "text": "for Leslie to publish a paper that says",
    "start": "78520",
    "end": "81100"
  },
  {
    "text": "I've just seen something amazing happen",
    "start": "81100",
    "end": "83500"
  },
  {
    "text": "this thing trained ten times faster than",
    "start": "83500",
    "end": "85330"
  },
  {
    "text": "it should have I don't know why",
    "start": "85330",
    "end": "86710"
  },
  {
    "text": "and so the reviewers were like we can't",
    "start": "86710",
    "end": "89020"
  },
  {
    "text": "publish that because you don't know why",
    "start": "89020",
    "end": "90010"
  },
  {
    "text": "so anyway that's important to pause on",
    "start": "90010",
    "end": "92230"
  },
  {
    "text": "because there's so many discoveries that",
    "start": "92230",
    "end": "94600"
  },
  {
    "text": "would need to start like that every",
    "start": "94600",
    "end": "96580"
  },
  {
    "start": "96000",
    "end": "193000"
  },
  {
    "text": "every other scientific field I know of",
    "start": "96580",
    "end": "98410"
  },
  {
    "text": "work so that way I don't know why ours",
    "start": "98410",
    "end": "101230"
  },
  {
    "text": "is uniquely disinterested",
    "start": "101230",
    "end": "103510"
  },
  {
    "text": "in publishing unexplained experimental",
    "start": "103510",
    "end": "107170"
  },
  {
    "text": "results but there it is so it wasn't",
    "start": "107170",
    "end": "109360"
  },
  {
    "text": "published having said that I read a lot",
    "start": "109360",
    "end": "115000"
  },
  {
    "text": "more unpublished papers and published",
    "start": "115000",
    "end": "116530"
  },
  {
    "text": "papers because that's where you find the",
    "start": "116530",
    "end": "118030"
  },
  {
    "text": "interesting insights so I absolutely",
    "start": "118030",
    "end": "120910"
  },
  {
    "text": "read this paper and I was just like this",
    "start": "120910",
    "end": "124780"
  },
  {
    "text": "is astonishingly mind-blowing and weird",
    "start": "124780",
    "end": "128860"
  },
  {
    "text": "and awesome and like why isn't everybody",
    "start": "128860",
    "end": "131230"
  },
  {
    "text": "only talking about this because like if",
    "start": "131230",
    "end": "133090"
  },
  {
    "text": "you can train these things ten times",
    "start": "133090",
    "end": "134500"
  },
  {
    "text": "faster they also generalize better",
    "start": "134500",
    "end": "136720"
  },
  {
    "text": "because your your doing less epochs",
    "start": "136720",
    "end": "138670"
  },
  {
    "text": "which means you look at the data less",
    "start": "138670",
    "end": "140080"
  },
  {
    "text": "you get better accuracy so I've been",
    "start": "140080",
    "end": "142870"
  },
  {
    "text": "kind of studying that ever since and",
    "start": "142870",
    "end": "145440"
  },
  {
    "text": "eventually Leslie kind of figured out a",
    "start": "145440",
    "end": "148360"
  },
  {
    "text": "lot of how to get it's done and we added",
    "start": "148360",
    "end": "151330"
  },
  {
    "text": "minor tweaks and a big part of the trick",
    "start": "151330",
    "end": "153670"
  },
  {
    "text": "is starting at a very low learning rate",
    "start": "153670",
    "end": "155860"
  },
  {
    "text": "very gradually increasing it so as",
    "start": "155860",
    "end": "158770"
  },
  {
    "text": "you're training your model you would",
    "start": "158770",
    "end": "160090"
  },
  {
    "text": "take very small steps at the start and",
    "start": "160090",
    "end": "162010"
  },
  {
    "text": "it gradually makes them bigger and",
    "start": "162010",
    "end": "163390"
  },
  {
    "text": "bigger and troll eventually you're",
    "start": "163390",
    "end": "165040"
  },
  {
    "text": "taking much bigger steps than anybody",
    "start": "165040",
    "end": "166989"
  },
  {
    "text": "thought as possible there's a few other",
    "start": "166989",
    "end": "170020"
  },
  {
    "text": "little tricks to make it work but ever",
    "start": "170020",
    "end": "172360"
  },
  {
    "text": "ever it basically we can reliable to get",
    "start": "172360",
    "end": "174310"
  },
  {
    "text": "super convergence and so for the drawing",
    "start": "174310",
    "end": "176170"
  },
  {
    "text": "bench thing we were using just much",
    "start": "176170",
    "end": "178450"
  },
  {
    "text": "higher learning rates than people",
    "start": "178450",
    "end": "180690"
  },
  {
    "text": "expected to work what do you think the",
    "start": "180690",
    "end": "182890"
  },
  {
    "text": "future of I mean makes so much sense for",
    "start": "182890",
    "end": "185020"
  },
  {
    "text": "that to be a critical hyper parameter",
    "start": "185020",
    "end": "186910"
  },
  {
    "text": "learning rate that you very what do you",
    "start": "186910",
    "end": "188980"
  },
  {
    "text": "think the future of learning rate magic",
    "start": "188980",
    "end": "191280"
  },
  {
    "text": "looks like well there's been a lot of",
    "start": "191280",
    "end": "194500"
  },
  {
    "start": "193000",
    "end": "286000"
  },
  {
    "text": "great work in the last 12 months in this",
    "start": "194500",
    "end": "196270"
  },
  {
    "text": "area it's and people are increasingly",
    "start": "196270",
    "end": "198459"
  },
  {
    "text": "realizing that up to might like we just",
    "start": "198459",
    "end": "200860"
  },
  {
    "text": "have no idea really how optimizers work",
    "start": "200860",
    "end": "202630"
  },
  {
    "text": "and the combination of weight decay",
    "start": "202630",
    "end": "205750"
  },
  {
    "text": "which is how we regularize optimizers",
    "start": "205750",
    "end": "207459"
  },
  {
    "text": "and the learning rate and then other",
    "start": "207459",
    "end": "209799"
  },
  {
    "text": "things like the epsilon we use in in the",
    "start": "209799",
    "end": "212110"
  },
  {
    "text": "atom optimizer they all work together in",
    "start": "212110",
    "end": "214989"
  },
  {
    "text": "weird ways and different parts of the",
    "start": "214989",
    "end": "217989"
  },
  {
    "text": "model this is another thing we've done a",
    "start": "217989",
    "end": "219730"
  },
  {
    "text": "lot of work on is research into how",
    "start": "219730",
    "end": "221980"
  },
  {
    "text": "different parts of the model should be",
    "start": "221980",
    "end": "223959"
  },
  {
    "text": "trained at different rates in different",
    "start": "223959",
    "end": "225310"
  },
  {
    "text": "ways so we do something we call",
    "start": "225310",
    "end": "227890"
  },
  {
    "text": "discriminative learning rates which is",
    "start": "227890",
    "end": "229480"
  },
  {
    "text": "really important particularly for",
    "start": "229480",
    "end": "230739"
  },
  {
    "text": "transfer learning so really I think in",
    "start": "230739",
    "end": "234340"
  },
  {
    "text": "the last 12 months a lot of people have",
    "start": "234340",
    "end": "235570"
  },
  {
    "text": "realized that all this stuff is",
    "start": "235570",
    "end": "236950"
  },
  {
    "text": "important there's been a lot of great",
    "start": "236950",
    "end": "238540"
  },
  {
    "text": "work coming out and we're starting to",
    "start": "238540",
    "end": "240910"
  },
  {
    "text": "see algorithms here which have very very",
    "start": "240910",
    "end": "245739"
  },
  {
    "text": "few dials if any that you have to touch",
    "start": "245739",
    "end": "247900"
  },
  {
    "text": "so like that I think what's gonna happen",
    "start": "247900",
    "end": "249400"
  },
  {
    "text": "is the idea of a learning rate well it",
    "start": "249400",
    "end": "251079"
  },
  {
    "text": "almost already has disappeared in the",
    "start": "251079",
    "end": "253180"
  },
  {
    "text": "latest research and instead it's just",
    "start": "253180",
    "end": "255340"
  },
  {
    "text": "like you know we we know enough about",
    "start": "255340",
    "end": "258489"
  },
  {
    "text": "how to interpret the gradients and the",
    "start": "258489",
    "end": "262840"
  },
  {
    "text": "change of gradients we see to know how",
    "start": "262840",
    "end": "264430"
  },
  {
    "text": "to set every parameter you can't wait it",
    "start": "264430",
    "end": "268289"
  },
  {
    "text": "you",
    "start": "273930",
    "end": "275990"
  }
]