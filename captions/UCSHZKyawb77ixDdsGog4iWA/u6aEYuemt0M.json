[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "yeah so thank you very much for the introduction so today I'll speak about deep learning especially in the context",
    "start": "30",
    "end": "6060"
  },
  {
    "text": "of computer vision so you saw in the previous talk is neural networks so you saw the neural networks are organized",
    "start": "6060",
    "end": "12480"
  },
  {
    "text": "into these layers fully connected layers where neurons in one layer are not connected but they're connected fully to",
    "start": "12480",
    "end": "18240"
  },
  {
    "text": "all the neurons in the previous layer and we saw that basically we have this layer wise structure from input until",
    "start": "18240",
    "end": "23460"
  },
  {
    "text": "output and there are neurons and nonlinearities etc now so far we have",
    "start": "23460",
    "end": "28710"
  },
  {
    "text": "not made too many assumptions about the inputs so in particular here we just assume that an input is some kind of a",
    "start": "28710",
    "end": "33719"
  },
  {
    "text": "vector of numbers that we plug into this neural network so that both a bug and a",
    "start": "33719",
    "end": "40230"
  },
  {
    "text": "feature to some extent because in most in most real-world applications we actually can make some assumptions about",
    "start": "40230",
    "end": "45989"
  },
  {
    "text": "the input that make learning much more efficient learning much more efficient",
    "start": "45989",
    "end": "52079"
  },
  {
    "text": "so in particular usually we don't just want to plug in into neural networks vectors of numbers but they actually",
    "start": "52079",
    "end": "58890"
  },
  {
    "text": "have some kind of a structure so we don't have vectors of numbers but these numbers are arranged in some kind of a layout like an N dimensional array of",
    "start": "58890",
    "end": "66299"
  },
  {
    "text": "numbers so for example spectrograms are two dimensional arrays of numbers images are three dimensional arrays of numbers videos would be four dimensional arrays",
    "start": "66299",
    "end": "73290"
  },
  {
    "text": "of numbers text you could treat as one dimensional array of numbers and so whenever you have this kind of local",
    "start": "73290",
    "end": "78750"
  },
  {
    "text": "connectivity structure in your data then you'd like to take advantage of it and convolutional neural networks allow you",
    "start": "78750",
    "end": "84360"
  },
  {
    "text": "to do that so before I dive into commercial neural networks and all the details of the architectures I'd like to briefly talk",
    "start": "84360",
    "end": "91229"
  },
  {
    "text": "about a bit of the history of how this field evolved over time so I like to start off usually with talking about",
    "start": "91229",
    "end": "97049"
  },
  {
    "text": "Hubble and Wiesel and the experiments that they performed in 1960s so what they were doing is trying to study the",
    "start": "97049",
    "end": "102869"
  },
  {
    "text": "computations that happen in the early visual cortex areas of a cat and so they",
    "start": "102869",
    "end": "108509"
  },
  {
    "text": "had cat and they plugged in electrodes that could record from the different neurons and then they showed the cat",
    "start": "108509",
    "end": "114450"
  },
  {
    "text": "different patterns of light and they were trying to debug a neurons effectively and try to show them different patterns and see what they",
    "start": "114450",
    "end": "120390"
  },
  {
    "text": "responded to and a lot of these experiments inspired some of the modeling that came in afterwards so in",
    "start": "120390",
    "end": "127229"
  },
  {
    "text": "particular one of the early models that try to take advantage of some of the results of these experiments where the",
    "start": "127229",
    "end": "133079"
  },
  {
    "text": "was the model called Newark cockney truant from Fukushima in 1980s and so what you saw here was these this",
    "start": "133079",
    "end": "139680"
  },
  {
    "text": "architecture that again is layer wise similar to what you see in the cortex where you have these simple and complex",
    "start": "139680",
    "end": "144750"
  },
  {
    "text": "cells where the simple cells detect small things in the visual field and then you have this local connectivity",
    "start": "144750",
    "end": "150750"
  },
  {
    "text": "pattern and the simple and complex cells alternate in this layered architecture throughout and so this was this looks a",
    "start": "150750",
    "end": "158250"
  },
  {
    "text": "bit like a comm net because you have some of its features like say the local connectivity but at the time this was not trained with backpropagation these",
    "start": "158250",
    "end": "164159"
  },
  {
    "text": "were specific heuristic Allah chose in' updates that and this was unsupervised",
    "start": "164159",
    "end": "170549"
  },
  {
    "text": "learning back then so the first time that we've actually used back propagation to train some of these networks was an experiment of a young",
    "start": "170549",
    "end": "176909"
  },
  {
    "text": "lagoon in the 1990s and so this is an example of one of the networks that was",
    "start": "176909",
    "end": "182879"
  },
  {
    "text": "developed back then in 1990s by anne lagoon as lina at five and this is what you would recognize today as a convolutional neural network so it has a",
    "start": "182879",
    "end": "189750"
  },
  {
    "text": "lot of the very simple computational layers and it's alternating and it's a similar kind of design to what you would",
    "start": "189750",
    "end": "195989"
  },
  {
    "text": "see in the Fukushima's new york cognate Ron but this was actually trained with backpropagation and to end using",
    "start": "195989",
    "end": "201030"
  },
  {
    "text": "supervised learning now so this happened in roughly 1990s and we're here in 2016",
    "start": "201030",
    "end": "207480"
  },
  {
    "text": "basically about 20 years later now computer vision has has for a long time",
    "start": "207480",
    "end": "215970"
  },
  {
    "text": "kind of worked here on larger images and a lot of these models back then were applied to very small kind of settings",
    "start": "215970",
    "end": "222239"
  },
  {
    "text": "like say recognizing digits in zip codes and things like that and they were very successful in those",
    "start": "222239",
    "end": "227729"
  },
  {
    "text": "domains but back at least when I entered computer vision roughly 2011 it was thought that a lot of people were aware",
    "start": "227729",
    "end": "233729"
  },
  {
    "text": "of these models but it was thought that they would not scale up naively into large complex images that they would be",
    "start": "233729",
    "end": "240510"
  },
  {
    "text": "constrained to these toy tasks for a long time or I shouldn't say toy because these were very important tasks but certainly like smaller visual",
    "start": "240510",
    "end": "246510"
  },
  {
    "text": "recognition problems and so in computer vision in roughly 2011 it was much more common to use a kind of these feature",
    "start": "246510",
    "end": "253290"
  },
  {
    "text": "based approaches at the time and they didn't work essentially that well so when I entered my PhD in 2011 working on",
    "start": "253290",
    "end": "258959"
  },
  {
    "text": "computer vision you would run a state of the art object detector on this image and you might get something like this",
    "start": "258959",
    "end": "266080"
  },
  {
    "start": "263000",
    "end": "263000"
  },
  {
    "text": "where cars were detected in trees and you would kind of just shrug your shoulders and say well that just happens sometimes you kind of just accept it as",
    "start": "266080",
    "end": "272199"
  },
  {
    "text": "a as a something that would just happen and of course this is a caricature things actually worked like relatively",
    "start": "272199",
    "end": "278289"
  },
  {
    "text": "decent I should say but definitely there were many mistakes that you would not see today about four years nian 2016",
    "start": "278289",
    "end": "285189"
  },
  {
    "text": "five years later and so a lot of computer vision kind of looked much more like this when you look into a paper of",
    "start": "285189",
    "end": "290620"
  },
  {
    "text": "trying that try to do image classification you would find this section in the paper on the features that they used so this is one page of",
    "start": "290620",
    "end": "298150"
  },
  {
    "text": "features and so they would use yeah a gist hog etc and then the second page of",
    "start": "298150",
    "end": "304870"
  },
  {
    "text": "features and all their hyper parameters so all kinds of different histograms and you would extract this kitchen sink of features and a third page here and so",
    "start": "304870",
    "end": "312639"
  },
  {
    "text": "you end up with this very large complex code base because some of these feature types are implemented in MATLAB some of",
    "start": "312639",
    "end": "318219"
  },
  {
    "text": "them in Python some of them in C++ and you end up with this large code base of extracting all these features caching them and then eventually plugging them",
    "start": "318219",
    "end": "324460"
  },
  {
    "text": "into linear classifiers to do some kind of visual recognition tasks so it was quite unwieldy but it worked to some",
    "start": "324460",
    "end": "332080"
  },
  {
    "text": "extent but they were definitely a room for improvement and so a lot of this change in computer vision in 2012 with",
    "start": "332080",
    "end": "338349"
  },
  {
    "text": "this paper from Astrid chef Sookie Ilya sutskever and Geoff Hinton so this is the first time that someone",
    "start": "338349",
    "end": "344020"
  },
  {
    "text": "took a convolutional neural network that is very similar to the one that you saw in from 1998 from Jana Kuhn and I'll go",
    "start": "344020",
    "end": "350259"
  },
  {
    "text": "into details of how they defer exactly but they took that kind of network they scaled it up they made it much bigger",
    "start": "350259",
    "end": "356169"
  },
  {
    "text": "and they trained it on a much bigger data set on GPUs and things basically ended up working extremely well and this",
    "start": "356169",
    "end": "361210"
  },
  {
    "text": "is the first time the computer vision community has really noticed these models and adopted them to work on",
    "start": "361210",
    "end": "366339"
  },
  {
    "text": "larger images so we saw that the performance of these models has improved",
    "start": "366339",
    "end": "372250"
  },
  {
    "text": "drastically here we are looking at the image net eyeless VRC visual recognition",
    "start": "372250",
    "end": "377469"
  },
  {
    "text": "challenge over the years and we're looking at the top 5 error so low is good and you can see that from 2010 in",
    "start": "377469",
    "end": "384129"
  },
  {
    "text": "the beginning these were feature based methods and then in 2012 we had this huge jump in performance and that was",
    "start": "384129",
    "end": "390009"
  },
  {
    "text": "due to the first kind of convolutional neural network in 2012 and then we've managed to push that over time and now",
    "start": "390009",
    "end": "395770"
  },
  {
    "text": "we're down to about 3.5 7% I think the results for image 2 thousand imagenet challenge 2016 are",
    "start": "395770",
    "end": "402160"
  },
  {
    "text": "actually due to come out today but I don't think that actually they've come out yet I have this second tab here",
    "start": "402160",
    "end": "408100"
  },
  {
    "text": "opened I was waiting for the result but I don't think this is a Pia tiah okay no",
    "start": "408100",
    "end": "415210"
  },
  {
    "text": "nothing alright well we'll get to find out very soon what happens right here so I'm very",
    "start": "415210",
    "end": "420700"
  },
  {
    "text": "excited to see that just to put this in context by the way because you're just looking at numbers like three point five",
    "start": "420700",
    "end": "425830"
  },
  {
    "text": "seven how good is that that's actually really really good so what something that I did about two years ago now now is that I try to",
    "start": "425830",
    "end": "433210"
  },
  {
    "text": "measure the human accuracy on this data set and so what I did for that is I developed this web interface where I",
    "start": "433210",
    "end": "439270"
  },
  {
    "text": "would show myself image net images from the test set and then I had this interface here where I would have all",
    "start": "439270",
    "end": "446080"
  },
  {
    "text": "the different classes of image net there's 1,000 of them and some example images and then basically you go down",
    "start": "446080",
    "end": "451570"
  },
  {
    "text": "this list and you scroll for a long time and you find what class you think that image might be and then I competed",
    "start": "451570",
    "end": "456730"
  },
  {
    "text": "against the ComNet at the time and this was Google net in 2000 in 2014 and so",
    "start": "456730",
    "end": "465310"
  },
  {
    "text": "hot dog is a very simple class you can do that quite easily but why is the accuracy not 0% it well some of the",
    "start": "465310",
    "end": "471100"
  },
  {
    "text": "things like hot dog seems very easy why isn't it trivial for humans to see well it turns out that some of the images in",
    "start": "471100",
    "end": "476200"
  },
  {
    "text": "a test set of image net are actually mislabeled but also some of the images are just very difficult to guess so in",
    "start": "476200",
    "end": "482440"
  },
  {
    "text": "particular if you have this Terrier there's 50 different types of terriers and it turns out to be very difficult task to find exactly which type of",
    "start": "482440",
    "end": "489040"
  },
  {
    "text": "Terrier that is you can spend minutes trying to find it turns out that good convolutional neural networks are",
    "start": "489040",
    "end": "494260"
  },
  {
    "text": "actually extremely good at this and so this is where I would lose points compared to ComNet so I estimate that",
    "start": "494260",
    "end": "501130"
  },
  {
    "text": "human accuracy based on this is roughly 2 to 5 percent range depending on how much time you have and how much",
    "start": "501130",
    "end": "506170"
  },
  {
    "text": "expertise you have and how many people you involve and how much they really want to do this which is not too much and so really we're doing extremely well",
    "start": "506170",
    "end": "514000"
  },
  {
    "text": "and so we're down to 3 percent and I think the error rate if I remember correctly was about 1.5 percent so if we",
    "start": "514000",
    "end": "521050"
  },
  {
    "text": "get below 1.5 percent I would be extremely suspicious on image net that seems wrong so to summarize basically",
    "start": "521050",
    "end": "528070"
  },
  {
    "text": "what we've done is before 2012 computer somewhat like this where we had these",
    "start": "528070",
    "end": "533930"
  },
  {
    "text": "feature extractors and then we trained a small portion at the end of the feature extractor extraction step and so we only",
    "start": "533930",
    "end": "540230"
  },
  {
    "text": "trained this last piece on top of these features that were fixed and we basically replaced the feature extractor in step with a single convolutional",
    "start": "540230",
    "end": "546830"
  },
  {
    "text": "neural network and now we trained everything completely end-to-end and this turns out to work quite nicely so I'm going to go into details of how this",
    "start": "546830",
    "end": "553550"
  },
  {
    "text": "works in a bit also in terms of code complexity we kind of went from a setup",
    "start": "553550",
    "end": "558860"
  },
  {
    "text": "that looks whoops way ahead okay we went from a setup that looks something like that and papers to",
    "start": "558860",
    "end": "565490"
  },
  {
    "text": "something like you know instead of extracting all these things we just say applied 20 layers with three by three",
    "start": "565490",
    "end": "570650"
  },
  {
    "text": "column or something like that and things work quite well this is of course an over-exaggeration but I think it's a correct first order statement to make is",
    "start": "570650",
    "end": "577040"
  },
  {
    "text": "that we've definitely seen that we've reduced code complexity quite a lot because these architectures are so",
    "start": "577040",
    "end": "582680"
  },
  {
    "text": "homogeneous compared to what we've done before so it's also remarkable that so",
    "start": "582680",
    "end": "587870"
  },
  {
    "text": "we had this reduction in complexity we had this amazing performance on imagenet one other thing that was quite amazing",
    "start": "587870",
    "end": "592970"
  },
  {
    "text": "about the results in 2012 that is also a separate thing that did not have to be the case is that the features that you",
    "start": "592970",
    "end": "599540"
  },
  {
    "start": "599000",
    "end": "599000"
  },
  {
    "text": "learn by training on image that turned out to be quite generic and you can apply them in different settings so in",
    "start": "599540",
    "end": "604640"
  },
  {
    "text": "other words this transfer learning works extremely well and of course I didn't go into details of convolutional networks",
    "start": "604640",
    "end": "610070"
  },
  {
    "text": "yet but we start with an image and we have a sequence of layers just like in a normal neural network and at the end we",
    "start": "610070",
    "end": "615110"
  },
  {
    "text": "have a classifier and when you pre train this network on image net then it turns out that the features that you learn in",
    "start": "615110",
    "end": "621020"
  },
  {
    "text": "the middle are actually transferable and you can use them on different data sets and that this works extremely well and",
    "start": "621020",
    "end": "626450"
  },
  {
    "text": "so that didn't have to be the case you might imagine that you could have a convolutional network that works extremely well on image net but when you",
    "start": "626450",
    "end": "632510"
  },
  {
    "text": "try to run it on some something else like birds data set or something that it might just not work well but that is not the case and that's a very interesting",
    "start": "632510",
    "end": "638810"
  },
  {
    "text": "finding in my opinion so people notice this back in roughly 2013 after the",
    "start": "638810",
    "end": "644540"
  },
  {
    "text": "first convolution networks they noticed that you can actually take many computer vision datasets and it used to be that",
    "start": "644540",
    "end": "649670"
  },
  {
    "text": "you would compete on all of these kind of separately and design features maybe for some of these separately and you can just shortcut all those steps that we",
    "start": "649670",
    "end": "656540"
  },
  {
    "text": "had designed and you can just take these pre trained features that you get from imagenet and you can just train a linear",
    "start": "656540",
    "end": "662720"
  },
  {
    "text": "classifier on every single data set on top of those features and you up many state-of-the-art results across many different data sets and so this was",
    "start": "662720",
    "end": "669490"
  },
  {
    "text": "quite a remarkable finding back then I believe so things worked very well an image net Thanks transferred very well",
    "start": "669490",
    "end": "675640"
  },
  {
    "text": "and the code complexity of course got much much more manageable so now all",
    "start": "675640",
    "end": "680680"
  },
  {
    "start": "676000",
    "end": "676000"
  },
  {
    "text": "this power is actually available to you with very few lines of code if you want to just use a convolutional network on",
    "start": "680680",
    "end": "686410"
  },
  {
    "text": "images it turns out to be only a few lines of code if you use for example Karis is one of the deep learning libraries that I'm going to go into and",
    "start": "686410",
    "end": "692740"
  },
  {
    "text": "I'll mention again later in the talk but basically just load a state-of-the-art complex all neural network you take an",
    "start": "692740",
    "end": "698770"
  },
  {
    "text": "image you load it and you compute your predictions and it tells you that this is an African elephant inside that image",
    "start": "698770",
    "end": "704800"
  },
  {
    "text": "and this took a couple couple hundred or a couple ten milliseconds if you have a GPU and so everything does much faster",
    "start": "704800",
    "end": "710830"
  },
  {
    "text": "much simpler works really well transfers really well so this was really a huge advance in computer vision and so as a",
    "start": "710830",
    "end": "716290"
  },
  {
    "text": "result of all these nice properties commnets today are everywhere so here's a collection of some of the some of the",
    "start": "716290",
    "end": "723100"
  },
  {
    "start": "718000",
    "end": "718000"
  },
  {
    "text": "things I try to find across across different applications so for example you can search google photos for",
    "start": "723100",
    "end": "728649"
  },
  {
    "text": "different types of categories like in this case rubik's cube you can find",
    "start": "728649",
    "end": "734320"
  },
  {
    "text": "house numbers very efficiently you can of course this is very relevant in self-driving cars and we're doing perception in the cars accomplishable",
    "start": "734320",
    "end": "741130"
  },
  {
    "text": "networks are very relevant they're medical image diagnosis recognizing Chinese characters doing all kinds of",
    "start": "741130",
    "end": "746440"
  },
  {
    "text": "medical segmentation tasks quite random tasks like wail recognition and more",
    "start": "746440",
    "end": "751810"
  },
  {
    "text": "generally many tackle challenges satellite image analysis recognizing different types of galaxies you may have",
    "start": "751810",
    "end": "757900"
  },
  {
    "text": "seen recently that a wave net from deepmind also very interesting paper that they generate music and they",
    "start": "757900",
    "end": "764709"
  },
  {
    "text": "generate speech and so this is a generative model and that's also just a comm that is doing most of the heavy lifting here so it's a convolutional",
    "start": "764709",
    "end": "771279"
  },
  {
    "text": "network on top of sound and other tasks like image captioning in the context of",
    "start": "771279",
    "end": "777040"
  },
  {
    "text": "reinforcement learning and agent in environment interactions we've also seen a lot of advances of using commnets as",
    "start": "777040",
    "end": "782620"
  },
  {
    "text": "the core computational building block so when you want to play Atari games or you want to play alphago or doom or",
    "start": "782620",
    "end": "788110"
  },
  {
    "text": "Starcraft or if you want to get robots to perform interesting manipulation tasks all of this users come that's as a",
    "start": "788110",
    "end": "794110"
  },
  {
    "text": "core computational block to do very impressive things not",
    "start": "794110",
    "end": "799760"
  },
  {
    "text": "only are we using it for a lot of different application we're also finding uses in art so so here are some examples",
    "start": "799760",
    "end": "807170"
  },
  {
    "text": "from deep dreams so you can basically simulate what it looks like what it feels like maybe to be on some drugs so",
    "start": "807170",
    "end": "813560"
  },
  {
    "text": "you can take images and you can just loosen it features these income that's or you might be familiar with neural style which allows you to take arbitrary",
    "start": "813560",
    "end": "819860"
  },
  {
    "text": "images and transfer arbitrary styles of different paintings like Van Gogh on top of them and this is all using",
    "start": "819860",
    "end": "825230"
  },
  {
    "text": "convolutional networks the last thing I'd like to note that I find also interesting is that in the process of",
    "start": "825230",
    "end": "830720"
  },
  {
    "text": "trying to develop better computer vision architectures and trying to basically optimize for performance on the image",
    "start": "830720",
    "end": "836630"
  },
  {
    "text": "net challenge we've actually ended up converging to something that potentially might function something like your visual cortex in some ways and so these",
    "start": "836630",
    "end": "843860"
  },
  {
    "text": "are some of the experiments that I find interesting where they've studied macaque monkeys and they record from a",
    "start": "843860",
    "end": "850100"
  },
  {
    "text": "subpopulation of the of the i.t cortex this is the part that does a lot of object recognition and so they record so",
    "start": "850100",
    "end": "857000"
  },
  {
    "text": "basically they take a monkey and they take a ComNet and they show them images and then you look at what those images are represented at the end of this",
    "start": "857000",
    "end": "863840"
  },
  {
    "text": "network so inside the monkey's brain or on top of your convolutional network as we look at representations of different images and then it turns out that",
    "start": "863840",
    "end": "870320"
  },
  {
    "text": "there's a mapping between those two spaces that actually seems to indicate to some extent that some of the things",
    "start": "870320",
    "end": "875960"
  },
  {
    "text": "we're doing somehow ended up converging to something that the brain could be doing as well in the visual cortex so",
    "start": "875960",
    "end": "882110"
  },
  {
    "text": "that's just some intro I'm now going to dive into convolutional networks and try to explain the briefly how these",
    "start": "882110",
    "end": "888740"
  },
  {
    "text": "networks work of course there's an entire class on this that I taught which is a convolutional networks class and so",
    "start": "888740",
    "end": "894230"
  },
  {
    "text": "I'm going to distill some of you know those 13 lectures into one lecture so we'll see how that goes I won't cover",
    "start": "894230",
    "end": "900020"
  },
  {
    "text": "everything of course okay so convolutional neural network is really just a single function it goes",
    "start": "900020",
    "end": "906710"
  },
  {
    "text": "from it's a function from the raw pixels of some kind of an image so we take 224 by 224 by 3 image so 3 here is for the",
    "start": "906710",
    "end": "913610"
  },
  {
    "text": "color channels RGB you take the raw pixels you put it through this function and you get 1000 numbers at the end in",
    "start": "913610",
    "end": "919070"
  },
  {
    "text": "the case of image classification if you're trying to categorize images into 1000 different classes and really",
    "start": "919070",
    "end": "925010"
  },
  {
    "text": "functionally all that's happening in a convolutional net work is just dot products and max operations that's everything but",
    "start": "925010",
    "end": "931120"
  },
  {
    "text": "they're wired up together in interesting ways so that you are basically doing visual recognition and in particular the",
    "start": "931120",
    "end": "937690"
  },
  {
    "text": "this function f has a lot of knobs in it so these w's here that participate in these dot products and in these",
    "start": "937690",
    "end": "943330"
  },
  {
    "text": "convolutions and fully connected layers and so on these WS are all parameters of this network so normally you might have",
    "start": "943330",
    "end": "949090"
  },
  {
    "text": "about on the order of 10 million parameters and those are basically knobs that change this function and so we'd",
    "start": "949090",
    "end": "956410"
  },
  {
    "text": "like to change those knobs of course so that when you put images through that function you get probabilities that are",
    "start": "956410",
    "end": "961990"
  },
  {
    "text": "consistent with your training data and so that gives us a lot to tune and turns out that we can do that tuning",
    "start": "961990",
    "end": "967540"
  },
  {
    "text": "automatically with back propagation through that search process now more concretely accomplish on your network is",
    "start": "967540",
    "end": "973720"
  },
  {
    "text": "made up of a sequence of layers just as in a case of normal neural networks but we have different types of layers that",
    "start": "973720",
    "end": "978970"
  },
  {
    "text": "we play with so we have convolutional layers here I'm using rectified linear unit relu for short as a non-linearity",
    "start": "978970",
    "end": "985530"
  },
  {
    "text": "so I'm making that and explicit its own layer pooling layers and fully connected",
    "start": "985530",
    "end": "991210"
  },
  {
    "text": "layers the core computational building block of a convolutional network though is this convolutional layer and we have",
    "start": "991210",
    "end": "997090"
  },
  {
    "text": "nonlinearities interspersed we are probably getting rid of things like pooling layers we might see them slightly going away over time and fully",
    "start": "997090",
    "end": "1003570"
  },
  {
    "text": "connected layers can actually be represented there are basically equivalent to convolutional layers as well and so really it's just a sequence",
    "start": "1003570",
    "end": "1009990"
  },
  {
    "text": "of complex in the simplest case so let me explain convolutional layer because that's the core computational building",
    "start": "1009990",
    "end": "1015270"
  },
  {
    "text": "block here that does all the heavy lifting so the entire comm that is this",
    "start": "1015270",
    "end": "1021750"
  },
  {
    "start": "1018000",
    "end": "1018000"
  },
  {
    "text": "collection of layers and these layers don't function over vectors so they don't transform vectors as a normal",
    "start": "1021750",
    "end": "1026970"
  },
  {
    "text": "neural network but they function over volumes so a layer will take a volume a three-dimensional volume of numbers an",
    "start": "1026970",
    "end": "1032730"
  },
  {
    "text": "array in this case for example we have a 32 by 32 by 3 image so those three dimensions are the width height and I'll",
    "start": "1032730",
    "end": "1039270"
  },
  {
    "text": "refer to the third dimension as depth we have three channels that's not to be confused with the depth of a network",
    "start": "1039270",
    "end": "1044640"
  },
  {
    "text": "which is the number of layers in that network so this is just depth of a volume so this complex layer accepts a",
    "start": "1044640",
    "end": "1050160"
  },
  {
    "text": "three dimensional volume and it produces a three dimensional volume using some weights so the way it actually produces",
    "start": "1050160",
    "end": "1055680"
  },
  {
    "text": "this output volume is as follows we're going to have these filters in a convolutional layer so these filters are always small",
    "start": "1055680",
    "end": "1062670"
  },
  {
    "text": "patiently like say for example five by five filter but their depth extends always through the input depth of the",
    "start": "1062670",
    "end": "1069270"
  },
  {
    "text": "input volume so since the input volume has three channels the depth is 3 then",
    "start": "1069270",
    "end": "1075120"
  },
  {
    "text": "our filters will always match that number so we have depth of 3 in our filters as well and then we can take",
    "start": "1075120",
    "end": "1081510"
  },
  {
    "text": "those filters and we can basically convolve them with the input volume so what that amounts to is we take this",
    "start": "1081510",
    "end": "1087330"
  },
  {
    "text": "filter oh yeah so that's just the point that the channels here must match we take that filter and we slide it through",
    "start": "1087330",
    "end": "1094410"
  },
  {
    "text": "all spatial positions of the input volume and along the way as we're sliding this filter we're computing dot products so W transpose X plus B where W",
    "start": "1094410",
    "end": "1102150"
  },
  {
    "text": "are the filters and X is a small piece of the input volume and B is offset and so this is basically the convolutional",
    "start": "1102150",
    "end": "1108180"
  },
  {
    "text": "operation you're taking this filter and you're sliding it through at all spatial positions and you're computing that products so when you do this you end up",
    "start": "1108180",
    "end": "1114930"
  },
  {
    "text": "with this activation map so in this case we get a 28 by 28 activate activation",
    "start": "1114930",
    "end": "1120690"
  },
  {
    "text": "map 28 comes from the fact that there are 28 unique positions to place this 5x5 filter into this 3 32 by 32 space so",
    "start": "1120690",
    "end": "1129480"
  },
  {
    "text": "there are 28 by 28 unique positions you can place that filter in and every one of those you're going to get a single number of how well that filter alikes",
    "start": "1129480",
    "end": "1137250"
  },
  {
    "text": "that part of the input so that carves out a single activation map and now in a",
    "start": "1137250",
    "end": "1143820"
  },
  {
    "text": "compositional layer we don't just have a single filter but we're going to have an entire set of filters so here's another filter a green filter we're going to",
    "start": "1143820",
    "end": "1150000"
  },
  {
    "text": "slide it through the input volume it has its own parameters so these there are 75 numbers here that basically make up a",
    "start": "1150000",
    "end": "1156870"
  },
  {
    "text": "filter there are different 75 numbers we convolve them through get a new activation map and we continue doing",
    "start": "1156870",
    "end": "1162120"
  },
  {
    "text": "this for all the filters in that convolutional layer so for example if we had six filters in this convolutional",
    "start": "1162120",
    "end": "1167760"
  },
  {
    "start": "1165000",
    "end": "1165000"
  },
  {
    "text": "layer then we might end up with 28 by 28 activation maps six times and we stock",
    "start": "1167760",
    "end": "1172860"
  },
  {
    "text": "them along the depth dimension to arrive at the output volume of 28 by 28 by 6 and so really what we've done is we've",
    "start": "1172860",
    "end": "1179100"
  },
  {
    "text": "we represented the original image which is 32 by 32 by 3 into a kind of a new image that is 28 by 28 by 6 where this",
    "start": "1179100",
    "end": "1187950"
  },
  {
    "text": "image basically has these 6 channels that tell you how well every filter matches or likes every part of the input",
    "start": "1187950",
    "end": "1196030"
  },
  {
    "text": "so let's compare this operation to say using fully connected layer as you would in a normal neural network",
    "start": "1196030",
    "end": "1202059"
  },
  {
    "text": "so in particular we saw that we process the 32 by 32 by 3 volume into 28 by 28",
    "start": "1202059",
    "end": "1207289"
  },
  {
    "text": "by 6 volume and one question might want to ask is how many parameters would this",
    "start": "1207289",
    "end": "1212659"
  },
  {
    "text": "require if we wanted a fully connected layer of the same number of output neurons here so we wanted 28 by 28 by 6",
    "start": "1212659",
    "end": "1218870"
  },
  {
    "text": "or times 20 times when it's 12 times 28 times 6 number of neurons fully",
    "start": "1218870",
    "end": "1224090"
  },
  {
    "text": "connected how many parameters would that be turns out that that would be quite a few parameters right because every",
    "start": "1224090",
    "end": "1229130"
  },
  {
    "text": "single neuron in the output volume would be fully connected to all of the 32 by 32 by 3 numbers here so basically every",
    "start": "1229130",
    "end": "1236450"
  },
  {
    "text": "one of those - 28 by 28 by 6 now Newell's is connected to 32 by 3 2 by 3 turns out to be about 15 million",
    "start": "1236450",
    "end": "1242750"
  },
  {
    "text": "parameters and also on that order of number of multiplies so you're doing a lot of compute and you're introducing a",
    "start": "1242750",
    "end": "1247850"
  },
  {
    "text": "huge amount of parameters into your network now since we're doing convolution instead you'll notice that",
    "start": "1247850",
    "end": "1254140"
  },
  {
    "text": "think about a number of parameters that we've introduced with this example convolutional layer so we've used we had",
    "start": "1254140",
    "end": "1261710"
  },
  {
    "text": "6 filters and every one of them was a 5 by 5 by 3 filter so basically we just",
    "start": "1261710",
    "end": "1267049"
  },
  {
    "text": "have 5 by 5 by 3 filters we have 6 of them if you just multiply that out we have 450 parameters and in this I'm not",
    "start": "1267049",
    "end": "1272960"
  },
  {
    "text": "counting the biases I'm just counting the raw weights so compared to 15 million we've only introduced very few",
    "start": "1272960",
    "end": "1278330"
  },
  {
    "text": "parameters also how many multiplies have we done so computationally how many flops are we doing well we have twenty by",
    "start": "1278330",
    "end": "1285770"
  },
  {
    "text": "twenty eight by six outputs to produce and every one of these numbers is a function of a five by five by three",
    "start": "1285770",
    "end": "1290960"
  },
  {
    "text": "region in the original image so basically we have 20 by 20 by 6 and then",
    "start": "1290960",
    "end": "1296210"
  },
  {
    "text": "there's every one of them is computed by doing 5 times 5 times 3 multiplies so you end up with only on the order of 350,000 multiplies so we've reduced from",
    "start": "1296210",
    "end": "1304880"
  },
  {
    "text": "15 million to quite a few so we're doing less flops and we're using fewer parameters and really what we've done",
    "start": "1304880",
    "end": "1310940"
  },
  {
    "text": "here is we've made assumptions right so we've made the assumption that because the fully connected layer if this wasn't",
    "start": "1310940",
    "end": "1317960"
  },
  {
    "text": "fully connected layer could compute the exact same thing but it would so specific setting of those 15 million",
    "start": "1317960",
    "end": "1323899"
  },
  {
    "text": "parameters would actually produce the exact output of this convolutional layer but we've done it much more efficiently we've done that by",
    "start": "1323899",
    "end": "1329120"
  },
  {
    "text": "reducing these biases so in particular we've made assumptions we've assumed for",
    "start": "1329120",
    "end": "1334250"
  },
  {
    "text": "example that since we have these fixed filters that we're sliding across space we've assumed that if there's some interesting feature that you'd like to",
    "start": "1334250",
    "end": "1340370"
  },
  {
    "text": "detect in one part of the image like say top left then that feature will also be useful somewhere else like on the bottom right because we fix these filters and",
    "start": "1340370",
    "end": "1347390"
  },
  {
    "text": "apply them at all the spatial positions equally you might notice that this is not always something that you might want",
    "start": "1347390",
    "end": "1352820"
  },
  {
    "text": "for example if you're getting inputs that are centered face images and you're doing some kind of a face recognition isn't like that then you might expect",
    "start": "1352820",
    "end": "1359450"
  },
  {
    "text": "that you might want different filters at different spatial positions like say for I region so you might want to have some",
    "start": "1359450",
    "end": "1364790"
  },
  {
    "text": "I like filters and for math region you might want to have mouth specific features and so on and so in that case you might not want to use convolutional",
    "start": "1364790",
    "end": "1371000"
  },
  {
    "text": "layer because those features have to be shared across all spatial positions and the second assumptions that we made is",
    "start": "1371000",
    "end": "1377270"
  },
  {
    "text": "that these filters are small locally and so we don't have global connectivity we have this local connectivity but that's",
    "start": "1377270",
    "end": "1383240"
  },
  {
    "text": "okay because we end up stacking up these convolutional layers in sequence and so this the neurons at the end of the",
    "start": "1383240",
    "end": "1389300"
  },
  {
    "text": "ComNet will grow their receptive field as you stack these convolutional layers on top of each other so at the end of",
    "start": "1389300",
    "end": "1394580"
  },
  {
    "text": "the combat those neurons end up being a function of the entire image eventually so just to give an idea about what these",
    "start": "1394580",
    "end": "1400220"
  },
  {
    "text": "activation maps look like concretely here's an example of an image on the top left this is a part of a car I believe",
    "start": "1400220",
    "end": "1406190"
  },
  {
    "text": "and we have these different filters at we have 32 different small filters here and so if we were to convolve these",
    "start": "1406190",
    "end": "1412040"
  },
  {
    "text": "filters with this image we end up with these activation labs so this filter if you convolve it you get this activation",
    "start": "1412040",
    "end": "1417740"
  },
  {
    "text": "lab and so on so this one for example has some orange stuff in it so when we convolve with this image you see that",
    "start": "1417740",
    "end": "1422929"
  },
  {
    "text": "this white here is denying the fact that that filter matches that part of the image quite well and so we get these",
    "start": "1422929",
    "end": "1428330"
  },
  {
    "text": "activation maps you stack them up and then that goes into the next convolutional layer so the way this",
    "start": "1428330",
    "end": "1434510"
  },
  {
    "text": "looks then looks like then is that we processed this with some kind of a convolutional layer we get some output",
    "start": "1434510",
    "end": "1440600"
  },
  {
    "text": "we apply a rectified linear units some kind of a non-linearity as normal and then we would just repeat that operation",
    "start": "1440600",
    "end": "1446450"
  },
  {
    "text": "so we keep plugging these values into the next convolutional layer and so they",
    "start": "1446450",
    "end": "1451790"
  },
  {
    "text": "plug into each other in sequence okay and so we end up processing the image over time so that's the convolutional",
    "start": "1451790",
    "end": "1458150"
  },
  {
    "text": "layer and you'll notice that there are a few more layers so in particular the pooling layer i'll explain very briefly pooling layer is quite simple if",
    "start": "1458150",
    "end": "1466760"
  },
  {
    "text": "you've used Photoshop or something like that you've taken a large image and you've resized it you downsampled the",
    "start": "1466760",
    "end": "1471950"
  },
  {
    "text": "image well pulling layers do basically something exactly like that but they're doing it on every single channel",
    "start": "1471950",
    "end": "1477350"
  },
  {
    "text": "independently so for every one of these channels independently in a input volume will pluck out that activation lab will",
    "start": "1477350",
    "end": "1484850"
  },
  {
    "text": "down sample it and that becomes a channel in the output volume so it's really down sampling operation on these",
    "start": "1484850",
    "end": "1491150"
  },
  {
    "text": "volumes so for example one of the common ways of doing this in the context of neural networks especially is to use",
    "start": "1491150",
    "end": "1496310"
  },
  {
    "start": "1496000",
    "end": "1496000"
  },
  {
    "text": "maximum operation so in this case it would be common to say for example use two by two filters stride to so and do",
    "start": "1496310",
    "end": "1504140"
  },
  {
    "text": "max operation so if this is an input channel in a volume then we're basically what that amounts to is we're truncating",
    "start": "1504140",
    "end": "1511430"
  },
  {
    "text": "it into these two by two regions and we're taking a max over four numbers to produce one piece of the output okay so",
    "start": "1511430",
    "end": "1518900"
  },
  {
    "text": "this is a very cheap operation that down samples your volumes it's really a way to control the capacity of the network so you don't want too many numbers you",
    "start": "1518900",
    "end": "1525140"
  },
  {
    "text": "don't want things to be too computationally expensive it turns out that a pooling layer allows you to down sample your volumes you're going to end",
    "start": "1525140",
    "end": "1531170"
  },
  {
    "text": "up doing less computation and it turns out to not hurt the performance too much so we use them basically as a way of",
    "start": "1531170",
    "end": "1537080"
  },
  {
    "text": "controlling the capacity of these networks and the last layer that I want to briefly mention of course is the",
    "start": "1537080",
    "end": "1542720"
  },
  {
    "text": "fully connected layer which is exactly as what you're familiar with so we have these volumes throughout as we process",
    "start": "1542720",
    "end": "1548090"
  },
  {
    "text": "the image at the end you're left with this volume and now you'd like to predict some classes so we do is we just take that volume we stretch it out into",
    "start": "1548090",
    "end": "1554480"
  },
  {
    "text": "a single column and then we apply for the connected layer which is really amounts to just a matrix multiplication and then that gives us probabilities",
    "start": "1554480",
    "end": "1562220"
  },
  {
    "text": "after applying like a soft Max or something like that so let me now show you briefly a demo of",
    "start": "1562220",
    "end": "1568190"
  },
  {
    "text": "what the convolutional Network looks like so this is common nsj this is a",
    "start": "1568190",
    "end": "1573590"
  },
  {
    "text": "deep learning library for training convolutional neural networks that I've that is implemented in JavaScript I wrote this maybe two years ago at this",
    "start": "1573590",
    "end": "1580520"
  },
  {
    "text": "point so here what we're doing is we're training a convolutional network on the c 410 dataset see 410 is a data set of",
    "start": "1580520",
    "end": "1586460"
  },
  {
    "text": "50,000 images each image is 32 by 32 by 3 and there are different ten different",
    "start": "1586460",
    "end": "1591560"
  },
  {
    "text": "classes so here we are training this network in the browser and you can the loss is decreasing which means that",
    "start": "1591560",
    "end": "1598070"
  },
  {
    "text": "we're better classifying these inputs and so here's the network specification",
    "start": "1598070",
    "end": "1603170"
  },
  {
    "text": "which you can play with because this is all done in the browser so you can just change this and play with this so this",
    "start": "1603170",
    "end": "1608240"
  },
  {
    "text": "is an input image and this convolutional network I'm showing here all the intermediate activations and all the",
    "start": "1608240",
    "end": "1613370"
  },
  {
    "text": "intermediate basically activation maps that we're producing so here we have a set of filters",
    "start": "1613370",
    "end": "1619280"
  },
  {
    "text": "we're convolving them with the image and getting all these activation maps I'm also showing the gradients but I don't",
    "start": "1619280",
    "end": "1624470"
  },
  {
    "text": "want to dwell on that too much then your threshold so rel will do anything below",
    "start": "1624470",
    "end": "1629540"
  },
  {
    "text": "zero gets clamped at zero and then you pull so this is just down sampling operation and then another convolution",
    "start": "1629540",
    "end": "1636220"
  },
  {
    "text": "relu pool compre loophole etc until at the end we have a fully connected layer and then we have our soft max so that we",
    "start": "1636220",
    "end": "1643190"
  },
  {
    "text": "get probabilities out and then we apply a loss to those probabilities and back propagate and so here we see that I've",
    "start": "1643190",
    "end": "1649160"
  },
  {
    "text": "been training in this tab for the last maybe 30 seconds or one minute and we're already getting about 30 percent",
    "start": "1649160",
    "end": "1654560"
  },
  {
    "text": "accuracy on C for ten so this these are test images from C for ten and these are the outputs of this compositional",
    "start": "1654560",
    "end": "1659870"
  },
  {
    "text": "network and you can see that it learned that this is already a car or something like that so this trains pretty quickly in JavaScript so you can play with this",
    "start": "1659870",
    "end": "1666950"
  },
  {
    "text": "and continue the architecture and so on another thing I'd like to show you is this video because it gives you again",
    "start": "1666950",
    "end": "1673220"
  },
  {
    "text": "this like very intuitive visceral feeling of exactly what this is computing is there's a very good video by Jason Kaczynski from recent advance",
    "start": "1673220",
    "end": "1680780"
  },
  {
    "text": "I'm going to play this in a bit this is from the deep visualization tool box so you can download this code and you can",
    "start": "1680780",
    "end": "1686630"
  },
  {
    "text": "play with this it's this interactive convolutional network demo this is neural networks have enabled computers",
    "start": "1686630",
    "end": "1692210"
  },
  {
    "text": "to better see and understand the world they can recognize good buses and zip top left corner we showed you in this",
    "start": "1692210",
    "end": "1698390"
  },
  {
    "text": "case kappa hi Daddy so what we're seeing here is these are activation laps in some particular shown in real time as",
    "start": "1698390",
    "end": "1705530"
  },
  {
    "text": "this demo is running so these are for the calm one layer of an Alex net which we're going to go into in much more",
    "start": "1705530",
    "end": "1711470"
  },
  {
    "text": "detail but these are the different activation maps that are being produced at this point neural network called Alex",
    "start": "1711470",
    "end": "1717590"
  },
  {
    "text": "net running in cafe by interacting with the network we can see what some of the neurons",
    "start": "1717590",
    "end": "1723690"
  },
  {
    "text": "for example on this first leg the unit in the center responds strongly to light to dark edges its neighbor one neuron",
    "start": "1723690",
    "end": "1732460"
  },
  {
    "text": "over responds to edges in the opposite direction dark to light using",
    "start": "1732460",
    "end": "1738580"
  },
  {
    "text": "optimization we can synthetically produce images that light up each neuron on this layer to see what each neuron is",
    "start": "1738580",
    "end": "1743680"
  },
  {
    "text": "looking for we can scroll through every layer in the network to see what it does including convolution pooling and",
    "start": "1743680",
    "end": "1750210"
  },
  {
    "text": "normalization layers we can switch back and forth between showing the actual activations and showing images",
    "start": "1750210",
    "end": "1756700"
  },
  {
    "text": "synthesized to produce high activation but the time you get to the fifth",
    "start": "1756700",
    "end": "1763390"
  },
  {
    "text": "convolutional layer the features being computed represent abstract concepts for",
    "start": "1763390",
    "end": "1769600"
  },
  {
    "text": "example this neuron seems to respond to phases we can further investigate this neuron by showing a few different types",
    "start": "1769600",
    "end": "1775060"
  },
  {
    "text": "of information first we can artificially create optimized images using new regularization techniques that are",
    "start": "1775060",
    "end": "1780970"
  },
  {
    "text": "described in our paper these synthetic images show that this neuron fire is in response to a face and shoulders we can",
    "start": "1780970",
    "end": "1787210"
  },
  {
    "text": "also plot the images from the training set that activate this neuron the most as well as pixels from those images most",
    "start": "1787210",
    "end": "1792490"
  },
  {
    "text": "responsible for the high activations computed via the D combination technique this feature responds to multiple faces",
    "start": "1792490",
    "end": "1798700"
  },
  {
    "text": "in different locations and by looking at the D cons we can see that it would",
    "start": "1798700",
    "end": "1804280"
  },
  {
    "text": "respond more strongly if we had even darker eyes and rosy lips we can also confirm that it cares about the head and",
    "start": "1804280",
    "end": "1810190"
  },
  {
    "text": "shoulders that ignores the arms and torso we can even see that it fires to some",
    "start": "1810190",
    "end": "1815470"
  },
  {
    "text": "extent for cat faces using back prop or decom we can see that this unit depends",
    "start": "1815470",
    "end": "1821650"
  },
  {
    "text": "most strongly on a couple units in the previous layer con four and on about a dozen or so in con 3 now let's look at",
    "start": "1821650",
    "end": "1829060"
  },
  {
    "text": "another neuron on this layer so what's this unit doing from the top 9 images we",
    "start": "1829060",
    "end": "1834400"
  },
  {
    "text": "might conclude that it fires 4 different types of clothing but examining the synthetic images shows that it may be",
    "start": "1834400",
    "end": "1839980"
  },
  {
    "text": "detecting not clothing say but wrinkles in the live plot we can see that it's activated by my shirt and",
    "start": "1839980",
    "end": "1846580"
  },
  {
    "text": "smoothing out half of my shirt causes that hack of the activations to decrease",
    "start": "1846580",
    "end": "1851919"
  },
  {
    "text": "finally here's another interesting memo this one has learned to look for printed",
    "start": "1851919",
    "end": "1857510"
  },
  {
    "text": "text in a variety of sizes colors and fonts this is pretty cool because we",
    "start": "1857510",
    "end": "1863270"
  },
  {
    "text": "never asked the network to look for wrinkles or text or faces the only papers were provided were at the very",
    "start": "1863270",
    "end": "1868340"
  },
  {
    "text": "last layer so the only reason the network learned features like text and faces in the middle was to support final",
    "start": "1868340",
    "end": "1874100"
  },
  {
    "text": "decisions at that last layer for example the text detector may provide good evidence that a rectangle is in fact a",
    "start": "1874100",
    "end": "1881210"
  },
  {
    "text": "book seen on edge and detecting many books next to each other might be a good way of detecting a bookcase which was",
    "start": "1881210",
    "end": "1887210"
  },
  {
    "text": "one of the categories we trained the net to recognize in this video we've shown",
    "start": "1887210",
    "end": "1892370"
  },
  {
    "text": "some of the features of the deep is toolbox okay so I encourage you to play with that it's really fun so I hope that",
    "start": "1892370",
    "end": "1898100"
  },
  {
    "text": "gives you an idea about exactly what's going on there at these convolutional layers we downsample them from what from time to time there's usually some fully",
    "start": "1898100",
    "end": "1903679"
  },
  {
    "text": "connected layers at the end but mostly it's just these convolutional operations stacked on top of each other so what I'd",
    "start": "1903679",
    "end": "1909679"
  },
  {
    "text": "like to do now is I'll dive into some details of how these architectures are actually put together the way I'll do",
    "start": "1909679",
    "end": "1914780"
  },
  {
    "text": "this is I'll go over all the winners of the imagenet challenges and I'll tell you about the architectures how they came about how they differ and so you'll",
    "start": "1914780",
    "end": "1921080"
  },
  {
    "text": "get a concrete idea about what these architectures look like in practice so we'll start off with the Alex net in 2012 so the Alex net just to give you an",
    "start": "1921080",
    "end": "1930020"
  },
  {
    "text": "idea about the sizes of these networks and the images that they process it took to 27 by 220 7 by 3 images and the first",
    "start": "1930020",
    "end": "1938179"
  },
  {
    "start": "1933000",
    "end": "1933000"
  },
  {
    "text": "layer of an Alex net for example was a completion layer that had 11 by 11 filters applied with a stride of four",
    "start": "1938179",
    "end": "1944929"
  },
  {
    "text": "and there are 96 of them stride of four I didn't fully explain because I wanted to save some time but intuitively it",
    "start": "1944929",
    "end": "1951799"
  },
  {
    "text": "just means that as you're sliding this filter across the input you don't have to slide in one pixel at a time but you can actually jump a few pixels at a time",
    "start": "1951799",
    "end": "1957890"
  },
  {
    "text": "so we have 11 by 11 filters with a stride a skip of four and we have 96 of",
    "start": "1957890",
    "end": "1963080"
  },
  {
    "text": "them you can try to compute for example what is the output volume if you apply this this sort of convolutional layer on",
    "start": "1963080",
    "end": "1970340"
  },
  {
    "text": "top of this volume and I didn't go into details of how you compute that but basically there are formulas for this and you can look into details in",
    "start": "1970340",
    "end": "1976820"
  },
  {
    "text": "the class but you arrive at 55 by 55 by 96 volume as output the total number of",
    "start": "1976820",
    "end": "1983840"
  },
  {
    "text": "parameters in this layer we have 96 filters every one of them is 11 by 11 by",
    "start": "1983840",
    "end": "1989480"
  },
  {
    "text": "3 because that's the input depth of these images so basically just amounts",
    "start": "1989480",
    "end": "1995360"
  },
  {
    "text": "to 11 but times 11 times 3 and then you have 96 filters so about 35,000 parameters in this very first layer then",
    "start": "1995360",
    "end": "2002799"
  },
  {
    "text": "the second layer of an Alex net is a pooling layer so we apply three by three filters at Stride of two and they do max",
    "start": "2002799",
    "end": "2008950"
  },
  {
    "text": "pooling so you can again compute the output volume size of that after applying this to that volume and you",
    "start": "2008950",
    "end": "2015340"
  },
  {
    "text": "arrive if you do some very simple arithmetic there you arrive at 27 by 27 by 96 so this is the down sampling",
    "start": "2015340",
    "end": "2021460"
  },
  {
    "text": "operation you can think about what is the number of parameters and this pooling layer and of course it's zero so",
    "start": "2021460",
    "end": "2028809"
  },
  {
    "text": "pooling layers compute a fixed function fixed down sampling operation there are no parameters involved in pulling a",
    "start": "2028809",
    "end": "2034000"
  },
  {
    "text": "layer all the parameters are in convolutional layers and the fully connected layers which are some extent equivalent to convolutional layers so",
    "start": "2034000",
    "end": "2041259"
  },
  {
    "text": "you can go ahead and just basically based on the description in the paper although is non-trivial I think based on",
    "start": "2041259",
    "end": "2046480"
  },
  {
    "text": "the description of this particular paper but you can go ahead and decipher what the volumes are throughout you can look",
    "start": "2046480",
    "end": "2052270"
  },
  {
    "text": "at the kind of patterns that emerge in terms of how you actually increase number of filters in higher",
    "start": "2052270",
    "end": "2057730"
  },
  {
    "text": "convolutional layers so we started off with 96 then we go to 256 filters then to 384 and eventually 4096 units click",
    "start": "2057730",
    "end": "2066310"
  },
  {
    "text": "on layers you'll see also normalization layers here which have since become slightly deprecated it's not very common",
    "start": "2066310",
    "end": "2072310"
  },
  {
    "text": "to use the normalization layers that were used at the time for the election architecture what's interesting to note",
    "start": "2072310",
    "end": "2078158"
  },
  {
    "text": "is how this differs from the 1998 Iyanla cool network so in particular I usually like to think about for things",
    "start": "2078159",
    "end": "2084878"
  },
  {
    "text": "that hold back progress so at least in a deep learning so the data is a",
    "start": "2084879",
    "end": "2090118"
  },
  {
    "text": "constraint compute and then I like to differentially differentiate between algorithms and infrastructure algorithms",
    "start": "2090119",
    "end": "2096608"
  },
  {
    "text": "being something that feels like research and infrastructure being something that feels like a lot of engineering has to happen and so in particular we've had",
    "start": "2096609",
    "end": "2102550"
  },
  {
    "text": "progress in all those four fronts so we see that in 1998 the data you could get ahold of maybe",
    "start": "2102550",
    "end": "2108369"
  },
  {
    "text": "would be on the order of a few thousand whereas now we have a few million so we had three orders of magnitude of increase in number of data compute GPUs",
    "start": "2108369",
    "end": "2116290"
  },
  {
    "text": "have become available and we use them to train these networks they are about say roughly 20 times faster than CPUs and",
    "start": "2116290",
    "end": "2122980"
  },
  {
    "text": "then of course CPUs we have today are much much faster than CPUs that they have back in 1998 so I don't know",
    "start": "2122980",
    "end": "2128320"
  },
  {
    "text": "exactly to what that works out to but I wouldn't be surprised if it's again on the order of three orders of magnitude of improvement again I'd like to",
    "start": "2128320",
    "end": "2134800"
  },
  {
    "text": "actually skip over algorithm and talk about infrastructure so in this case we're talking about Nvidia releasing the",
    "start": "2134800",
    "end": "2140560"
  },
  {
    "text": "cuda library that allows you to efficiently create all these matrix vector operations and apply them on arrays of numbers so that's a piece of",
    "start": "2140560",
    "end": "2148390"
  },
  {
    "text": "software that you rely on and that we take advantage of that wasn't available before and finally algorithms is kind of",
    "start": "2148390",
    "end": "2154329"
  },
  {
    "text": "an interesting one because there's been in those 20 years there's been much less improvement in an algorithms than all",
    "start": "2154329",
    "end": "2160540"
  },
  {
    "text": "these other three pieces so in particular what we've done with the 1998 network is we've made it bigger so you",
    "start": "2160540",
    "end": "2165940"
  },
  {
    "text": "have more channels you have more layers by bit and the two really new things algorithmically are dropout and",
    "start": "2165940",
    "end": "2173800"
  },
  {
    "text": "rectified linear units so dropout is a regularization technique developed by",
    "start": "2173800",
    "end": "2179770"
  },
  {
    "text": "geoff hinton and colleagues and rectified linear units are these nonlinearities that train much faster",
    "start": "2179770",
    "end": "2185440"
  },
  {
    "text": "than sigmoids and ten HS and this paper actually had a plot that showed that the",
    "start": "2185440",
    "end": "2190660"
  },
  {
    "text": "rectified linear units trained a bit faster than sigmoids and that's intuitively because of the vanishing gradient problems and when you have very",
    "start": "2190660",
    "end": "2196960"
  },
  {
    "text": "deep networks with sigmoids those gradients banish as Hugh was talking about in last lecture so what's",
    "start": "2196960",
    "end": "2203680"
  },
  {
    "text": "interesting also to note by the way is that both drop out and relu are basically like one line or two lines of",
    "start": "2203680",
    "end": "2209079"
  },
  {
    "text": "code to change so it's about two line diff total in those twenty years and both of them consist of setting things",
    "start": "2209079",
    "end": "2215230"
  },
  {
    "text": "to zero so with the relevance of things to zero when they're lower than zero and with dropout you set things to zero at",
    "start": "2215230",
    "end": "2221200"
  },
  {
    "text": "random so it's a good idea to set things to zero apparently that's what we learned so if you try to find a new cool",
    "start": "2221200",
    "end": "2228010"
  },
  {
    "text": "algorithm look for one line dips that set something to zero probably will work better and we could add you here to this",
    "start": "2228010",
    "end": "2233770"
  },
  {
    "text": "list now some of the newest things that happened some of the",
    "start": "2233770",
    "end": "2239310"
  },
  {
    "text": "comparing it again and giving you an idea about the hyper parameters that are in this architecture it was the first",
    "start": "2239310",
    "end": "2245520"
  },
  {
    "text": "use of rectified linear units we haven't seen that as much before this network using the normalization layers which are",
    "start": "2245520",
    "end": "2251070"
  },
  {
    "text": "not used anymore at least in a specific way that they use them in this paper they used heavy data",
    "start": "2251070",
    "end": "2256620"
  },
  {
    "text": "augmentation so you don't only put in you don't only pipe these images into the networks exactly as they come from",
    "start": "2256620",
    "end": "2262770"
  },
  {
    "text": "the data set but you jitter them spatially around a bit and you work them and you change the colors a bit and you just do this randomly because you're",
    "start": "2262770",
    "end": "2269220"
  },
  {
    "text": "trying to build in some invariances to these small perturbations and you're basically hallucinating additional data it was the first real use of drop out",
    "start": "2269220",
    "end": "2278660"
  },
  {
    "text": "and roughly you see standard hyper parameters like say batch sizes of roughly 128 using stochastic gradient",
    "start": "2278660",
    "end": "2285630"
  },
  {
    "text": "descent with momentum usually point nine in the momentum learning rates of 1e",
    "start": "2285630",
    "end": "2290940"
  },
  {
    "text": "negative two you reduce them in normal ways so you're reduced roughly by factor of ten whenever validation stops",
    "start": "2290940",
    "end": "2296610"
  },
  {
    "text": "improving and weight decay of just a bit five you negative four and ensemble",
    "start": "2296610",
    "end": "2302870"
  },
  {
    "text": "so you train seven independent commercial networks separately and then you just average their predictions",
    "start": "2302870",
    "end": "2308910"
  },
  {
    "text": "always gives you additional 2% improvement so this is Alex net the winner of 2012 in 2013 the winner was",
    "start": "2308910",
    "end": "2316260"
  },
  {
    "text": "the Z F net this was developed by Matthew Siler and Rob progress in 2013",
    "start": "2316260",
    "end": "2322320"
  },
  {
    "text": "and this was an improvement on top of Alex net architecture in particular one of the bigger differences here where",
    "start": "2322320",
    "end": "2328140"
  },
  {
    "text": "that the convolutional layer the first convolutional layer they went from 11 by 11 stride four to seven by seven strike",
    "start": "2328140",
    "end": "2333600"
  },
  {
    "text": "two so if slightly smaller filters and you apply them more densely and then also they notice that these",
    "start": "2333600",
    "end": "2338700"
  },
  {
    "text": "convolutional layers in the middle if you make them larger if you scale them up then you actually gain performance so",
    "start": "2338700",
    "end": "2344490"
  },
  {
    "text": "they managed to improve a tiny bit matthew Zeiler then went he became the founder of clarify and he worked on this",
    "start": "2344490",
    "end": "2352740"
  },
  {
    "text": "a bit more inside clarify and he managed to push the performance to 11% which was the winning entry at the time but we",
    "start": "2352740",
    "end": "2358110"
  },
  {
    "text": "don't actually know what gets you from 14% to 11% because Matthew never disclosed the full details of what",
    "start": "2358110",
    "end": "2363750"
  },
  {
    "text": "happened there but he did say that it was more tweaking of these hyper parameters and optimizing that a bit so",
    "start": "2363750",
    "end": "2369690"
  },
  {
    "text": "that was 2013 winner in 2014 we saw a slightly bigger to this so one of the networks that was",
    "start": "2369690",
    "end": "2375360"
  },
  {
    "text": "introduced then was a vgg net from Karen Simonian and andrew zisserman what's beautiful about vgg net and they",
    "start": "2375360",
    "end": "2380700"
  },
  {
    "text": "explored a few architectures here and the one that ended up working best was this D column which is what I'm highlighting it was beautiful about the",
    "start": "2380700",
    "end": "2386160"
  },
  {
    "text": "vgg net is that it's so simple so you might have noticed in these previous in these previous networks you have these",
    "start": "2386160",
    "end": "2392460"
  },
  {
    "text": "different filter sizes different layers and you do different amount of strides and everything kind of looks a bit hairy and you're not sure where these hyper",
    "start": "2392460",
    "end": "2398460"
  },
  {
    "text": "parameters are coming from VG's unit is extremely uniform all you do is 3x3 convolutions with stride one pad one and",
    "start": "2398460",
    "end": "2405000"
  },
  {
    "text": "you do two by two Macs Bowling's with stride two and you do this throughout completely homogeneous architecture and",
    "start": "2405000",
    "end": "2411360"
  },
  {
    "text": "you just alternate a few columns and a few pool layers and you get top top performance so they managed to reduce",
    "start": "2411360",
    "end": "2417450"
  },
  {
    "text": "the air down to 7.3% in the vdg net just with a very simple item Oh genius",
    "start": "2417450",
    "end": "2423210"
  },
  {
    "text": "architecture so it's I've also here written out this a D architecture it's just so you can",
    "start": "2423210",
    "end": "2428310"
  },
  {
    "text": "see I'm not I'm not sure how instructed this is because it's kind of dense but you can definitely see and you can look",
    "start": "2428310",
    "end": "2433830"
  },
  {
    "text": "at this outline perhaps but you can see how these volumes develop and you can see the kinds of sizes of these filters",
    "start": "2433830",
    "end": "2440180"
  },
  {
    "text": "so they're always three by three but the number of filters again grows so we started off with 64 and then we go to",
    "start": "2440180",
    "end": "2445350"
  },
  {
    "text": "128 256 512 so we're just doubling it over time I also have a few numbers here",
    "start": "2445350",
    "end": "2452250"
  },
  {
    "text": "just to give you an idea of the scale at which these networks normally operate so we have on the order 140 million",
    "start": "2452250",
    "end": "2457710"
  },
  {
    "text": "parameters this is actually quite a lot I'll show you in a bit that this can be about five or ten million parameters and",
    "start": "2457710",
    "end": "2462720"
  },
  {
    "text": "works just as well and it's about hundred megabytes for image in terms of",
    "start": "2462720",
    "end": "2467730"
  },
  {
    "text": "memory in the forward pass and then the backward pass also needs roughly on that order so that's roughly the numbers that",
    "start": "2467730",
    "end": "2473460"
  },
  {
    "text": "were we're working with here also you can note that most of the and this is true mostly in convolutional networks is",
    "start": "2473460",
    "end": "2480330"
  },
  {
    "text": "that most of the memory is in the early convolutional layers most of the parameters at least in the case where you use these giant fully connected",
    "start": "2480330",
    "end": "2486450"
  },
  {
    "text": "layers at the top would be here so the winner actually in 2014 was not the VG",
    "start": "2486450",
    "end": "2491640"
  },
  {
    "text": "net I only present it because it's such a simple architecture but the winner was actually Google net with a slightly",
    "start": "2491640",
    "end": "2496670"
  },
  {
    "text": "hairier architecture we should say so it's still a sequence of things but in this case they've put inception modules",
    "start": "2496670",
    "end": "2504090"
  },
  {
    "text": "in sequence and this is an example inception module I don't know then too much time to go into the details but you can see that it consists",
    "start": "2504090",
    "end": "2509800"
  },
  {
    "text": "basically of convolutions and different kinds of strides and so on so the Google",
    "start": "2509800",
    "end": "2514840"
  },
  {
    "text": "net is look slightly a hairier but it turns out to be more efficient in",
    "start": "2514840",
    "end": "2520900"
  },
  {
    "text": "several respects so for example it works a bit better than vgg net at least at the time it only has 5 million",
    "start": "2520900",
    "end": "2527770"
  },
  {
    "text": "parameters compared to VG nets 140 million parameters so a huge reduction and you do that by the way by just",
    "start": "2527770",
    "end": "2533380"
  },
  {
    "text": "throwing away fully connected layers so you'll notice in this breakdown I did these poly connected layers here have 100 million parameters and 16 million",
    "start": "2533380",
    "end": "2540220"
  },
  {
    "text": "parameters turns out you don't actually need that so if you take them away that actually doesn't hurt the performance too much so you can get a huge reduction",
    "start": "2540220",
    "end": "2547600"
  },
  {
    "text": "of parameters and it was it was slightly we can also compare to the original",
    "start": "2547600",
    "end": "2554350"
  },
  {
    "text": "Alex net so compared to the original Alex net we have fewer parameters a bit more compute and a much better performance so Google net was really",
    "start": "2554350",
    "end": "2561100"
  },
  {
    "text": "optimized to have a low footprint both memory wise both computation wise and both parameter wise but it looks a bit",
    "start": "2561100",
    "end": "2567370"
  },
  {
    "text": "uglier and VG net is a very beautiful homogeneous architecture but there are some inefficiencies in it okay so that's",
    "start": "2567370",
    "end": "2574300"
  },
  {
    "text": "a 2014 now in 2015 we had a slightly bigger Delta on top of the architectures",
    "start": "2574300",
    "end": "2580090"
  },
  {
    "text": "so right now these architectures if you on laocoon looked at them maybe in 1998 he would still recognize everything so everything looks very like simple you've",
    "start": "2580090",
    "end": "2586780"
  },
  {
    "text": "just played with had parameters so one of the first kind of bigger departures I would argue was in 2015 with the",
    "start": "2586780",
    "end": "2591820"
  },
  {
    "text": "introduction of residual networks and so this has worked from kamini Hey and colleagues in Microsoft Research Asia",
    "start": "2591820",
    "end": "2598120"
  },
  {
    "text": "and so they did not only win the image net challenge in 2015 but they want a whole bunch of challenges and this was",
    "start": "2598120",
    "end": "2604480"
  },
  {
    "text": "all just by applying these residual networks that were trained on image net and then fine-tuned on all these",
    "start": "2604480",
    "end": "2609580"
  },
  {
    "text": "different tasks and you basically can crush lots of different tasks whenever you get a new awesome Kombat so at this",
    "start": "2609580",
    "end": "2618250"
  },
  {
    "text": "time the performance was basically 3.5 7% from these residual networks so this is 2015 also this paper try to argue",
    "start": "2618250",
    "end": "2625420"
  },
  {
    "text": "that if you look at the number of layers it goes up and then it they made the point that with residual",
    "start": "2625420",
    "end": "2631180"
  },
  {
    "text": "networks as well see in a bit you can introduce many more layers and they and that that correlates strongly with",
    "start": "2631180",
    "end": "2636430"
  },
  {
    "text": "performance we've since found that in fact you can make these residual works quite shop quite a lot shallower",
    "start": "2636430",
    "end": "2641549"
  },
  {
    "text": "like say on the order of 20 or 30 layers and they work just as fine just as well so it's not necessarily the depth here",
    "start": "2641549",
    "end": "2647130"
  },
  {
    "text": "but I'll go into that in a bit but you get a much better performance what's interesting about this paper is this this plot here where they compare",
    "start": "2647130",
    "end": "2655109"
  },
  {
    "text": "these residual networks and I'll go into details of how they work in a bit and these what they call plane networks which is everything I've explained until",
    "start": "2655109",
    "end": "2661319"
  },
  {
    "text": "now and the problem with plane networks is that when you try to scale them up and introduce additional layers they",
    "start": "2661319",
    "end": "2667049"
  },
  {
    "text": "don't get monotonically better so if you take a 20 layer model and on this is on",
    "start": "2667049",
    "end": "2672450"
  },
  {
    "text": "C far ten experiments if you take a 20 layer model and you run it and then you take a 56 layer model you'll see that",
    "start": "2672450",
    "end": "2678930"
  },
  {
    "text": "the 56 layer model performs worse and this is not just on the test data so it's not just an overfitting issue this",
    "start": "2678930",
    "end": "2684960"
  },
  {
    "text": "is on the training data the 56 layer model performs worse on the training data than the 20 layer model even though",
    "start": "2684960",
    "end": "2690569"
  },
  {
    "text": "the 56 layer model can imitate 20 layer model by setting 36 layers to compute identities so basically it's an",
    "start": "2690569",
    "end": "2697049"
  },
  {
    "text": "optimization problem that you can't find the solution once your problem size grows that much bigger in this plane net",
    "start": "2697049",
    "end": "2704270"
  },
  {
    "text": "architecture so in the residual networks that they proposed they found that when you wire them up in a slightly different",
    "start": "2704270",
    "end": "2709740"
  },
  {
    "text": "way you monotonically get a better performance as you add more layers so more layers always strictly better and",
    "start": "2709740",
    "end": "2716880"
  },
  {
    "text": "you don't run into these optimization issues so comparing residual networks to plane networks in plane networks as I've",
    "start": "2716880",
    "end": "2723359"
  },
  {
    "text": "explained already you have this sequence of convolutional layers where every convolutional layer operates over volume",
    "start": "2723359",
    "end": "2728819"
  },
  {
    "text": "before and produces volume in residual networks we have this first convolutional layer on top of the raw",
    "start": "2728819",
    "end": "2733859"
  },
  {
    "text": "image then there's a pooling layer so at this point we've reduced to 56 by 56 by",
    "start": "2733859",
    "end": "2739559"
  },
  {
    "text": "64 the original image and then from here on they have these residual blocks with these funny skipped connections and this",
    "start": "2739559",
    "end": "2745890"
  },
  {
    "text": "turns out to be quite important so let me show you what these look like",
    "start": "2745890",
    "end": "2751920"
  },
  {
    "text": "so the original climbing paper had this architecture here shown under original so on the left you see original residual",
    "start": "2751920",
    "end": "2759059"
  },
  {
    "text": "networks design since then they had an additional paper that played with the architecture and found that there's a better arrangement of layers inside this",
    "start": "2759059",
    "end": "2766619"
  },
  {
    "text": "block that works better empirically and so the way this works so concentrate on the proposed one in the middle since",
    "start": "2766619",
    "end": "2772109"
  },
  {
    "text": "that works so well you have this pathway where you have this representation of the image X and",
    "start": "2772109",
    "end": "2778229"
  },
  {
    "text": "then instead of transforming that representation X to get a new X to plug in later we end up having this X we go",
    "start": "2778229",
    "end": "2785429"
  },
  {
    "text": "off and we do some compute on the side so that's that residual block doing some computation and then you add your result",
    "start": "2785429",
    "end": "2791759"
  },
  {
    "text": "on top of X so you have this addition operation here going to the next residual block so you have this X and",
    "start": "2791759",
    "end": "2798329"
  },
  {
    "text": "you always compute deltas to it and I think this it's not intuitive that this should work much better or why that",
    "start": "2798329",
    "end": "2804239"
  },
  {
    "text": "works much better I think it becomes a bit more intuitively clear if you actually understand the backpropagation dynamics and how backprop works and this",
    "start": "2804239",
    "end": "2810929"
  },
  {
    "text": "is why I always urge people also to implement back rub themselves to get an intuition for how it works what it's computing and so on because if you",
    "start": "2810929",
    "end": "2817890"
  },
  {
    "text": "understand back rub you'll see that addition operation is a gradient distributor so you get a gradient from",
    "start": "2817890",
    "end": "2823859"
  },
  {
    "text": "the top and this gradient will flow equally to all the children that participated in that addition so you have gradient flowing here from the",
    "start": "2823859",
    "end": "2829769"
  },
  {
    "text": "supervision so you have supervision at the very bottom here in this diagram and it kind of flows upwards and it flows",
    "start": "2829769",
    "end": "2835409"
  },
  {
    "text": "through these residual blocks and then gets added to this stream and so you end up with but this addition distributes",
    "start": "2835409",
    "end": "2840419"
  },
  {
    "text": "that gradient always identically through so what you end up with is this kind of a gradient superhighway as I like to",
    "start": "2840419",
    "end": "2846599"
  },
  {
    "text": "call it where these gradients from your supervision go directly to the original convolutional layer and then on top of",
    "start": "2846599",
    "end": "2851759"
  },
  {
    "text": "that you get these deltas from all the residual blocks so these block can come on online and can help out that original",
    "start": "2851759",
    "end": "2858509"
  },
  {
    "text": "stream of information this is also related to I think why LST MS along short-term memory networks work better",
    "start": "2858509",
    "end": "2865769"
  },
  {
    "text": "than recurrent neural networks because they also have these kind of additional addition operations in the lsdm and it",
    "start": "2865769",
    "end": "2871799"
  },
  {
    "text": "just makes the gradients flow significantly better then there were some results on top of residual networks",
    "start": "2871799",
    "end": "2877229"
  },
  {
    "text": "that I thought were quite amusing so recently for example we had this result on deep networks with stochastic depth the idea here was that the authors",
    "start": "2877229",
    "end": "2885539"
  },
  {
    "text": "of this paper noticed that you have these residual blocks that compute Delta's on top of your string and you",
    "start": "2885539",
    "end": "2891119"
  },
  {
    "text": "can basically randomly throw out layers so you have these say hundred blocks 100 residual box and you can randomly drop",
    "start": "2891119",
    "end": "2897329"
  },
  {
    "text": "them out and at test time similar to drop out you introduce all of them and they all work at the same time but you",
    "start": "2897329",
    "end": "2903359"
  },
  {
    "text": "have to scale things and it just like with dropout but basically it's kind of a unintuitive result because you can throw out layers",
    "start": "2903359",
    "end": "2909630"
  },
  {
    "text": "at random and I think it breaks the original notion of what we had of commnets of as like these these feature",
    "start": "2909630",
    "end": "2915810"
  },
  {
    "text": "transformers we compute more and more complex features over time or something like that and I think it seems much more",
    "start": "2915810",
    "end": "2922290"
  },
  {
    "text": "intuitive to think about these residual networks at least to me as some kinds of dynamical systems where you have this",
    "start": "2922290",
    "end": "2929130"
  },
  {
    "text": "original representation of the image X and then every single residual block is kind of like a vector field that because",
    "start": "2929130",
    "end": "2934560"
  },
  {
    "text": "it computes in a delta on top of your signal and so these vector fields nudge your original representation X towards a",
    "start": "2934560",
    "end": "2941160"
  },
  {
    "text": "space where you can decode the answer Y of like the class of that X and so if",
    "start": "2941160",
    "end": "2946440"
  },
  {
    "text": "you drop off some of these residual blocks at random then if you haven't applied one of these vector fields then the other vector fields that come later",
    "start": "2946440",
    "end": "2952470"
  },
  {
    "text": "can kind of make up for it and they nudge they basically nudge the they pick",
    "start": "2952470",
    "end": "2957480"
  },
  {
    "text": "up the slack and they nudge along anyways and so that's possibly why this the image I currently have in mind of",
    "start": "2957480",
    "end": "2963810"
  },
  {
    "text": "how these things work so much more like dynamical systems in fact another experiments that people are playing with",
    "start": "2963810",
    "end": "2969660"
  },
  {
    "text": "that I also find interesting is you don't have you can share these residual blocks so it starts to look more like a",
    "start": "2969660",
    "end": "2974880"
  },
  {
    "text": "recurrent neural network so these residual blocks would have shared connectivity and then you have this dynamical system really where you're",
    "start": "2974880",
    "end": "2981600"
  },
  {
    "text": "just running a single RNN a single vector field did you keep iterating over and over and then your fixed point gives you the answer so it's kind of",
    "start": "2981600",
    "end": "2987810"
  },
  {
    "text": "interesting what's happening it looks very funny ok we've had many",
    "start": "2987810",
    "end": "2993300"
  },
  {
    "text": "more interesting results that so people are playing a lot with these residual networks and improving on them in",
    "start": "2993300",
    "end": "2998580"
  },
  {
    "text": "various ways so as I mentioned already it turns out that you can make these residual networks much shallower and",
    "start": "2998580",
    "end": "3003860"
  },
  {
    "text": "make them wider so you introduce more channels and that can work just as well if not better so it's not necessarily",
    "start": "3003860",
    "end": "3009770"
  },
  {
    "text": "the depth that is giving you a lot of the performance it's you can scale down the depth and if you increase the width",
    "start": "3009770",
    "end": "3016010"
  },
  {
    "text": "that can actually work better and they're also more efficient if you do it that way there's more funny",
    "start": "3016010",
    "end": "3022130"
  },
  {
    "text": "regularization techniques here swap-out is a funny regularization technique that actually interpolates between plane nets",
    "start": "3022130",
    "end": "3028310"
  },
  {
    "text": "rez nets and dropout so that's also a funny paper with fractal nets we",
    "start": "3028310",
    "end": "3033680"
  },
  {
    "text": "actually have many more different types of nets and so people have really experimented with this a lot I'm really eager to see what the winning",
    "start": "3033680",
    "end": "3039210"
  },
  {
    "text": "we'll be in 2016 as a result of a lot of this one of the things that has really enabled this rapid experimentation in",
    "start": "3039210",
    "end": "3044970"
  },
  {
    "text": "the community is that somehow we've developed luckily this culture of sharing a lot of code among ourselves so",
    "start": "3044970",
    "end": "3051119"
  },
  {
    "text": "for example Facebook has released just as an example Facebook has released residual networks code and torch that is",
    "start": "3051119",
    "end": "3057060"
  },
  {
    "text": "really good that a lot of these papers I believe have adopted and worked on top of and that allowed them to actually really scale up their experiments and",
    "start": "3057060",
    "end": "3063930"
  },
  {
    "text": "and it explore different architectures so it's great that this has happened unfortunately a lot of these papers are",
    "start": "3063930",
    "end": "3070800"
  },
  {
    "text": "come kind of on archive and it's kind of a chaos as these are being uploaded so at this point I think this is a natural point to plug very briefly in my archive",
    "start": "3070800",
    "end": "3078119"
  },
  {
    "text": "sanity calm so this is the best website ever and what it does is it crawls",
    "start": "3078119",
    "end": "3083369"
  },
  {
    "text": "archive and it takes all the papers and it analyzes all the papers the full-text",
    "start": "3083369",
    "end": "3088619"
  },
  {
    "text": "of the papers and creates tf-idf bag-of-words features for all the papers and then you can do things like you can",
    "start": "3088619",
    "end": "3093630"
  },
  {
    "text": "search a particular paper like residual networks paper here and you can look for similar papers on archive and so this is",
    "start": "3093630",
    "end": "3099000"
  },
  {
    "text": "a sorted list of basically all the residual networks papers that are most related to that paper or you can also",
    "start": "3099000",
    "end": "3104160"
  },
  {
    "text": "create user accounts and you can create a library of papers that you like and then archive Sanofi will train a support vector machine for you and basically you",
    "start": "3104160",
    "end": "3111030"
  },
  {
    "text": "can look at what our archive papers over the last month that I would enjoy the most and that's just computed by archive",
    "start": "3111030",
    "end": "3116670"
  },
  {
    "text": "sanity and so it's like a curated feed specifically for you so I use this quite a bit and I find it in useful so I hope",
    "start": "3116670",
    "end": "3122730"
  },
  {
    "text": "that other people do as well okay so we saw convolutional neural networks I",
    "start": "3122730",
    "end": "3128430"
  },
  {
    "text": "explained how they work I explained some of the background context I've given you an idea of what they look like in practice and we went through case",
    "start": "3128430",
    "end": "3134280"
  },
  {
    "text": "studies of the winning architectures over time but so far we've only looked at image classification specifically so",
    "start": "3134280",
    "end": "3139619"
  },
  {
    "text": "we're categorizing images into some number of bins so I'd like to briefly talk about addressing other tasks in",
    "start": "3139619",
    "end": "3144839"
  },
  {
    "text": "computer vision and how you might go about doing that so the way to think about doing other",
    "start": "3144839",
    "end": "3150570"
  },
  {
    "text": "tasks in computer vision is that really what we have is you can think of this computational convolutional neural network as this block of compute that",
    "start": "3150570",
    "end": "3157560"
  },
  {
    "start": "3154000",
    "end": "3154000"
  },
  {
    "text": "has a few million parameters in it and it can do basically arbitrary functions that are very nice over images and so",
    "start": "3157560",
    "end": "3164640"
  },
  {
    "text": "takes an image gives you some kind of features and now different tasks will basically look as follows you want to",
    "start": "3164640",
    "end": "3171410"
  },
  {
    "text": "picked some kind of a thing and different tasks there will be different things and you always have a desired thing and then you want to make the",
    "start": "3171410",
    "end": "3177410"
  },
  {
    "text": "predicted thing much more closer to the desired thing and you back propagate so this is the only part usually that",
    "start": "3177410",
    "end": "3182780"
  },
  {
    "text": "changes from task to task you'll see that these comments don't change too much what changes is your last function at the very end and that's what actually",
    "start": "3182780",
    "end": "3188750"
  },
  {
    "text": "helps you really transfer a lot of these winning architectures they usually use these pre trained networks and you don't",
    "start": "3188750",
    "end": "3194180"
  },
  {
    "text": "worry too much about the details of that architecture because you're only worried about you know adding a small piece at the top or changing the last function or",
    "start": "3194180",
    "end": "3200390"
  },
  {
    "text": "substituting a new data set and so on so just to make this slightly more concrete in image classification we apply this",
    "start": "3200390",
    "end": "3206000"
  },
  {
    "start": "3204000",
    "end": "3204000"
  },
  {
    "text": "compute block we get these features and then if I want to do classification I would basically predict 1,000 numbers",
    "start": "3206000",
    "end": "3211849"
  },
  {
    "text": "that give me the LOC probabilities of different classes and then I have a predicted thing a desired thing",
    "start": "3211849",
    "end": "3216859"
  },
  {
    "text": "particular class and I can back prop if I'm doing image captioning the it also looks very similar instead of predicting",
    "start": "3216859",
    "end": "3223250"
  },
  {
    "text": "just a vector of 1,000 numbers I now have for example at ten thousand ten thousand words in some kind of",
    "start": "3223250",
    "end": "3229369"
  },
  {
    "text": "vocabulary and I'd be predicting ten thousand numbers and a sequence of them and so I can use a recurrent neural",
    "start": "3229369",
    "end": "3234440"
  },
  {
    "text": "network which you will hear much more about I think in Richards lecture just after this and so I produce a sequence",
    "start": "3234440",
    "end": "3241309"
  },
  {
    "text": "of ten thousand dimensional vectors and that's just a description and they indicate the probabilities of different words to be emitted at different time",
    "start": "3241309",
    "end": "3246859"
  },
  {
    "text": "steps or for example if you want to do localization again most of the block stays unchanged but now we also want",
    "start": "3246859",
    "end": "3253789"
  },
  {
    "start": "3249000",
    "end": "3249000"
  },
  {
    "text": "some kind of a extent in the image so suppose we want to classify we don't only just want to classify this as an",
    "start": "3253789",
    "end": "3259880"
  },
  {
    "text": "airplane but we want to localize it with X Y width height bounding box coordinates and if we make a specific",
    "start": "3259880",
    "end": "3265099"
  },
  {
    "text": "assumption as well that there's always a single one thing in the image like a single airplane in every image then you",
    "start": "3265099",
    "end": "3270680"
  },
  {
    "text": "can just afford to just predict that so we predict these softmax scores just like before and apply the cross-entropy",
    "start": "3270680",
    "end": "3276319"
  },
  {
    "text": "loss and then we can predict X Y width height on top of that and we use alloc and l2 loss or a Hooper loss or",
    "start": "3276319",
    "end": "3281779"
  },
  {
    "text": "something like that so you just have a predicted thing a desired thing and you just back drop if you want to do",
    "start": "3281779",
    "end": "3288349"
  },
  {
    "start": "3288000",
    "end": "3288000"
  },
  {
    "text": "reinforcement learning because you want to play different games then again the setup is you just predict some different thing and it has some different",
    "start": "3288349",
    "end": "3294289"
  },
  {
    "text": "semantics so in this case we will be for example predicting eight numbers that give us the probabilities of taking",
    "start": "3294289",
    "end": "3299569"
  },
  {
    "text": "different actions for example there are eight discrete actions in Atari then we just predict eight numbers and then",
    "start": "3299569",
    "end": "3305180"
  },
  {
    "text": "we trained us with a slightly different manner because in the case of reinforcement learning you don't actually have a you don't actually know",
    "start": "3305180",
    "end": "3311510"
  },
  {
    "text": "what the correct action is to take at any point in time but you can still get a desired thing eventually because you",
    "start": "3311510",
    "end": "3317180"
  },
  {
    "text": "just run these rollouts over time and you just see what what happens and then",
    "start": "3317180",
    "end": "3322220"
  },
  {
    "text": "that helps you that helps inform exactly what the correct answer should have been or what the desired thing should have",
    "start": "3322220",
    "end": "3327770"
  },
  {
    "text": "been in any one of those rollouts in any point in time I don't want to dwell on this too much in this lecture though it's outside of the scope you'll hear",
    "start": "3327770",
    "end": "3333950"
  },
  {
    "text": "much more about reinforcement learning in the in a later lecture if you wanted to do segmentation for example then you",
    "start": "3333950",
    "end": "3340880"
  },
  {
    "start": "3338000",
    "end": "3338000"
  },
  {
    "text": "don't want to predict a single vector of numbers for a single for single image but every single pixel has its own",
    "start": "3340880",
    "end": "3347329"
  },
  {
    "text": "category that you'd like to predict so data set will actually be colored like this and you have different classes different areas and then instead of",
    "start": "3347329",
    "end": "3354170"
  },
  {
    "text": "predicting a single vector of classes you predict an entire array of 224 by 224 since that's the extent of the",
    "start": "3354170",
    "end": "3360559"
  },
  {
    "text": "original image for example times 20 if you have 20 different classes and then you basically have 2 24 by 2 24",
    "start": "3360559",
    "end": "3366829"
  },
  {
    "text": "independent soft maxes here that's one way you could pose this and then you back propagate this would here would be",
    "start": "3366829",
    "end": "3372740"
  },
  {
    "text": "slightly more difficult because you see here I have a decom players mentioned here and I didn't explain the",
    "start": "3372740",
    "end": "3378079"
  },
  {
    "text": "convolutional layers they're related to convolutional layers they do a very similar operation but kind of backwards",
    "start": "3378079",
    "end": "3383809"
  },
  {
    "text": "in some way so a compilation layer kind of does these down sampling operations as it computes a decon layer does these",
    "start": "3383809",
    "end": "3389359"
  },
  {
    "text": "kind of up sampling operations as it computes these convolutions but in fact you can implement a decomp layer using",
    "start": "3389359",
    "end": "3394670"
  },
  {
    "text": "accomplish so what you do is you decom forward pass is the cobbler backward pass and the decom backward pass is the",
    "start": "3394670",
    "end": "3401720"
  },
  {
    "text": "complex basically so they're basically an identical operation but just are you up sampling we're down sampling kind of",
    "start": "3401720",
    "end": "3407890"
  },
  {
    "text": "so you can use decomp layers or you can use hyper columns and there are different things that people do in",
    "start": "3407890",
    "end": "3413329"
  },
  {
    "text": "segmentation literature but that's just the rough idea as you're just changing to loss function at the end if you",
    "start": "3413329",
    "end": "3418640"
  },
  {
    "text": "wanted to do auto-encoders so you want to do some surprise landing or something like that well you're just trying to predict the original image so you're",
    "start": "3418640",
    "end": "3424819"
  },
  {
    "text": "trying to get the convolutional network to implement the identity transformation and the trick of course it makes it",
    "start": "3424819",
    "end": "3430220"
  },
  {
    "text": "non-trivial is that you're forcing the representation to go through this representational bottleneck of 7 by 7 by",
    "start": "3430220",
    "end": "3435589"
  },
  {
    "text": "512 so the network must find an efficient represent of the original image so that it can decode it later so that would be a auto",
    "start": "3435589",
    "end": "3442140"
  },
  {
    "text": "encoder you again have an l2 loss at the end and your backdrop or if you want to do variational auto-encoders you have to",
    "start": "3442140",
    "end": "3447479"
  },
  {
    "start": "3446000",
    "end": "3446000"
  },
  {
    "text": "introduce a repair motorisation layer and you have to append an additional small loss that makes your posterior",
    "start": "3447479",
    "end": "3452549"
  },
  {
    "text": "beer prior but it's just like an additional layer and then you have an entire generative model and you can actually like sample images as well if",
    "start": "3452549",
    "end": "3458900"
  },
  {
    "text": "you wanted to do detection things get a little more hairy perhaps a compared to",
    "start": "3458900",
    "end": "3464009"
  },
  {
    "start": "3461000",
    "end": "3461000"
  },
  {
    "text": "localization or something like that so one of my favorite detectors perhaps to explain as the yellow detector because it's perhaps the simplest one it doesn't",
    "start": "3464009",
    "end": "3470759"
  },
  {
    "text": "work the best but it's the simplest one to explain and has the core idea of how people do detection in computer vision",
    "start": "3470759",
    "end": "3476459"
  },
  {
    "text": "and so the way this works is we reduced the original image to a seven by seven",
    "start": "3476459",
    "end": "3481890"
  },
  {
    "text": "by 512 feature so really there are these 49 discrete locations that we have and",
    "start": "3481890",
    "end": "3488209"
  },
  {
    "text": "at every single one of these 49 locations we're going to predict in yellow we're going to predict a class so",
    "start": "3488209",
    "end": "3494789"
  },
  {
    "text": "that's shown here on the top right so every single one of these forty-nine will be some kind of a soft Max and then",
    "start": "3494789",
    "end": "3500369"
  },
  {
    "text": "additionally at every single position we're going to predict some number of bounding boxes and so there's going to",
    "start": "3500369",
    "end": "3505890"
  },
  {
    "text": "be a B number of bounding boxes say B is 10 so we're going to be predicting 50",
    "start": "3505890",
    "end": "3510989"
  },
  {
    "text": "numbers and the the 5 comes from the fact that every bounding box will have five numbers associated with it so you have to describe the XY the width and",
    "start": "3510989",
    "end": "3517829"
  },
  {
    "text": "the height and you have to also indicate some kind of a confidence of that bounding box so that's the fifth number",
    "start": "3517829",
    "end": "3524279"
  },
  {
    "text": "is some kind of a confidence measure so you basically end up predicting these bounding boxes they have positions they",
    "start": "3524279",
    "end": "3529679"
  },
  {
    "text": "have class they have confidence and then you have some true bounding boxes in the image so you know that there are certain",
    "start": "3529679",
    "end": "3535410"
  },
  {
    "text": "true boxes and they have certain class and what you do then is you match up the",
    "start": "3535410",
    "end": "3540839"
  },
  {
    "text": "desired thing with the predicted thing and whatever so say for example you had one bounding box of a cat then you would",
    "start": "3540839",
    "end": "3548670"
  },
  {
    "text": "find the closest predicted bounding box and you would mark it as a positive and you would try to make that associated",
    "start": "3548670",
    "end": "3554130"
  },
  {
    "text": "grid cell predict cat and you would nudge the prediction to be slightly more towards the cat the box and so all this",
    "start": "3554130",
    "end": "3561329"
  },
  {
    "text": "can be done with simple losses and you just back propagate that and then you have a detector or if you want to get much more fancy you you could do dense",
    "start": "3561329",
    "end": "3567900"
  },
  {
    "start": "3566000",
    "end": "3566000"
  },
  {
    "text": "image captioning so in this case this is a combination of detection and image captioning this is a paper with my equal quality in Johnson and",
    "start": "3567900",
    "end": "3574780"
  },
  {
    "text": "Feifei Lee from last year and so what we did here is image comes in and it becomes much more complex I don't maybe want to go into it as much",
    "start": "3574780",
    "end": "3581170"
  },
  {
    "text": "but the first order approximation is that instead it's basically a detection but instead of predicting fixed classes",
    "start": "3581170",
    "end": "3587200"
  },
  {
    "text": "we instead predict a sequence of words so we use a recurrent neural network there but basically I can take an image",
    "start": "3587200",
    "end": "3592870"
  },
  {
    "text": "then and you can predict you can both detect and predict and describe everything in a complex visual scene so",
    "start": "3592870",
    "end": "3599050"
  },
  {
    "text": "that's just some overview of different tasks that people care about most of them consist of just changing this top",
    "start": "3599050",
    "end": "3604150"
  },
  {
    "text": "part you put different loss function a different data set but you'll see that this computational block stays",
    "start": "3604150",
    "end": "3609370"
  },
  {
    "text": "relatively unchanged from time to time and that's why as I mentioned when you do transfer learning you just want to",
    "start": "3609370",
    "end": "3614560"
  },
  {
    "text": "kind of take these blue train networks and you mostly want to use whatever works well on imagenet because a lot of that does not change too much okay so in",
    "start": "3614560",
    "end": "3622570"
  },
  {
    "text": "the last part of the talk I'd like to just make sure we're good on time okay we're good so in the last part of the",
    "start": "3622570",
    "end": "3628000"
  },
  {
    "text": "talk I just wanted to give some hints of some practical considerations when you want to apply convolutional net works in",
    "start": "3628000",
    "end": "3633610"
  },
  {
    "text": "practice so first consideration you might have if you want to run these networks is what hardware do I use so some of the options",
    "start": "3633610",
    "end": "3641200"
  },
  {
    "text": "that I think are available to you well first of all you can just buy a machine so for example and Vidya has these",
    "start": "3641200",
    "end": "3647890"
  },
  {
    "text": "digits dev boxes that you can buy they have Titan X GPUs which are strong GPUs you can also if you're much more",
    "start": "3647890",
    "end": "3654580"
  },
  {
    "text": "ambitious you can buy dgx one which has the newest Pascal P 100 GPS unfortunately the dgx one is about a",
    "start": "3654580",
    "end": "3660610"
  },
  {
    "text": "hundred and thirty thousand dollars so this is kind of an expensive supercomputer but the digits death box I",
    "start": "3660610",
    "end": "3666550"
  },
  {
    "text": "think is more accessible and so that's one option we can go with alternatively you can look at the specs of a dev box",
    "start": "3666550",
    "end": "3673330"
  },
  {
    "text": "and those specs are there good specs and then you can buy all the components yourself and assemble it like Lego",
    "start": "3673330",
    "end": "3678690"
  },
  {
    "text": "unfortunately you that's prone to mistakes of course but you can definitely reduce the price maybe by",
    "start": "3678690",
    "end": "3684340"
  },
  {
    "text": "fracture like to it compared to the Nvidia machine but of course Nvidia",
    "start": "3684340",
    "end": "3689650"
  },
  {
    "text": "machine would just come with all the software installed all the hardware is ready and you can just do work there are a few GPU offerings in the cloud but",
    "start": "3689650",
    "end": "3695620"
  },
  {
    "text": "unfortunately it's actually not at a good place right now it's actually quite difficult to get GPUs in the cloud good",
    "start": "3695620",
    "end": "3700990"
  },
  {
    "text": "GPUs at least so Amazon AWS has these great k5 five 20s they're not very good GPUs",
    "start": "3700990",
    "end": "3707020"
  },
  {
    "text": "they're not fast they don't have too much memory it's actually kind of a problem Microsoft Azure is coming up",
    "start": "3707020",
    "end": "3712720"
  },
  {
    "text": "Azura is coming up with its own offering soon so I think they've announced it and it's in some kind of a beta stage if I",
    "start": "3712720",
    "end": "3718720"
  },
  {
    "text": "remember correctly and so those are powerful GPUs K 80s that would be available to at open the eye for example",
    "start": "3718720",
    "end": "3724210"
  },
  {
    "text": "you use Sarah scale so seer scale is much more a slightly different model you can't spin up GPUs on demand but they",
    "start": "3724210",
    "end": "3729820"
  },
  {
    "text": "allow you to rent a box in the cloud so what that amounts to is that we have these boxes somewhere in the cloud I have just the DNA I just have the URL is",
    "start": "3729820",
    "end": "3738520"
  },
  {
    "text": "sh2 it it's a it's a Titan X boxes in the machine and so you can just do work that way so these options are available",
    "start": "3738520",
    "end": "3745990"
  },
  {
    "text": "to hardware wise in terms of software there are many different frameworks of course that you could use for deep",
    "start": "3745990",
    "end": "3751090"
  },
  {
    "text": "learning so these are some of the more common ones that you might see in practice so different people have",
    "start": "3751090",
    "end": "3757480"
  },
  {
    "text": "different recommendations on this I would my personal recommendation right now to most people if you just want to apply this in practical settings 90% of",
    "start": "3757480",
    "end": "3765910"
  },
  {
    "text": "the use cases are probably addressable with things like Harris so Karis would I go to number one thing to look at Karis",
    "start": "3765910",
    "end": "3773320"
  },
  {
    "text": "is a layer over tensorflow or Theano and basically just a higher-level",
    "start": "3773320",
    "end": "3779530"
  },
  {
    "text": "API over either of those so for example I usually use Karis on top of tensorflow and it's a much more higher level",
    "start": "3779530",
    "end": "3785680"
  },
  {
    "text": "language than raw tensorflow so you can also work in raw tensorflow but you'll have to do a lot of low level",
    "start": "3785680",
    "end": "3790960"
  },
  {
    "text": "stuff if you need all that freedom then that's great because that allows you to have much more freedom in terms of how you design everything but it can be",
    "start": "3790960",
    "end": "3798880"
  },
  {
    "text": "slightly more worthy for example you have to assign every single weight you have to assign a name stuff like that and so it's just much more wordy but you",
    "start": "3798880",
    "end": "3805840"
  },
  {
    "text": "can work at that level or for most applications I think Karis would be sufficient and I've used torch for a long time I still really like torch it's",
    "start": "3805840",
    "end": "3812470"
  },
  {
    "text": "very lightweight interpretable it works just just fine so those are the options that I would currently consider at least another",
    "start": "3812470",
    "end": "3821590"
  },
  {
    "text": "practical consideration you might be wondering what architecture what architecture do I use in my problem so my answer here and I've already hinted",
    "start": "3821590",
    "end": "3828040"
  },
  {
    "text": "at this is don't be a hero don't go crazy don't design your own neural",
    "start": "3828040",
    "end": "3833530"
  },
  {
    "text": "networks and convolutional layers and don't probably don't you don't to do that probably so the algorithm is",
    "start": "3833530",
    "end": "3839540"
  },
  {
    "text": "actually very simple look at whatever is currently the latest released thing that works really well in iOS VRC you",
    "start": "3839540",
    "end": "3845990"
  },
  {
    "text": "download that pre-trained model and then you potentially add or delete some layers on top because you want to do",
    "start": "3845990",
    "end": "3851150"
  },
  {
    "text": "some other tasks so that usually requires some tinkering at the top or something like that and then you fine-tune it on your application so",
    "start": "3851150",
    "end": "3857299"
  },
  {
    "text": "actually a very straightforward process the first degree I think to most applications would be don't tinker with",
    "start": "3857299",
    "end": "3862730"
  },
  {
    "text": "it too much you're going to break it but of course you can also take two 2:31 end and then you might become much better at",
    "start": "3862730",
    "end": "3868250"
  },
  {
    "text": "at tinkering with these architectures second is how do I choose the parameters",
    "start": "3868250",
    "end": "3874940"
  },
  {
    "text": "and my answer here again would be don't be a hero look into papers look what",
    "start": "3874940",
    "end": "3880549"
  },
  {
    "text": "happens they use for the most part you'll see that all papers use the same hyper parameters they look very similar so Adam when you add them for",
    "start": "3880549",
    "end": "3887030"
  },
  {
    "text": "optimization it's always learning rate one in negative three or one integrate four so four you can also use sed",
    "start": "3887030",
    "end": "3894380"
  },
  {
    "text": "momentum it's always the similar kinds of learning rates so don't go too crazy designing this one of the things you probably want to play with the most is",
    "start": "3894380",
    "end": "3900559"
  },
  {
    "text": "the regularization so and in particular not the l2 regularization but the dropout rates is something I would",
    "start": "3900559",
    "end": "3906079"
  },
  {
    "text": "advise instead and so because you might have it smaller or a much larger data",
    "start": "3906079",
    "end": "3911660"
  },
  {
    "text": "set if you have a much smaller data set and overfitting is a concern so you want to make sure that you regular eyes properly with dropout and then you might",
    "start": "3911660",
    "end": "3918109"
  },
  {
    "text": "want to as a second degree consideration may be the learning rate you want to tune that a tiny bit but that yeah",
    "start": "3918109",
    "end": "3923299"
  },
  {
    "text": "that's usually doesn't as much of an effect so really there's like two hyper parameters and you take a pre train",
    "start": "3923299",
    "end": "3928579"
  },
  {
    "text": "network and this is 90% of the use cases I would say yeah so compared to when",
    "start": "3928579",
    "end": "3934819"
  },
  {
    "text": "computer version 2011 where you might have hundreds of high parameters so yeah",
    "start": "3934819",
    "end": "3940240"
  },
  {
    "text": "okay and in terms of distributed training so if you want to work at scale",
    "start": "3940240",
    "end": "3945650"
  },
  {
    "text": "because if you want to Train imagenet or some large scale data sets you might want to train across multiple GPUs so",
    "start": "3945650",
    "end": "3951140"
  },
  {
    "text": "just to give an idea most of these state-of-the-art networks are trained on the order of a few weeks across multiple GPUs usually four or",
    "start": "3951140",
    "end": "3957260"
  },
  {
    "text": "eight GPUs and these GPS are roughly on the order of one thousand dollars each but then you also have to house them so",
    "start": "3957260",
    "end": "3962900"
  },
  {
    "text": "of course that has a different price but you almost always want to train on multiple GPUs if possible usually you",
    "start": "3962900",
    "end": "3968720"
  },
  {
    "text": "don't end up training across machines that's much more rare I think to train across machines what's much more common is you have a single machine and it has eight",
    "start": "3968720",
    "end": "3974869"
  },
  {
    "text": "Titan exes or something like that and you do distributor training on those eight titan axis there are different",
    "start": "3974869",
    "end": "3980089"
  },
  {
    "text": "ways to distribute a training so if you're very if you're feeling fancy you can try to do some model parallelism",
    "start": "3980089",
    "end": "3985490"
  },
  {
    "text": "where you split your network across multiple GPUs I would instead advise some kind of a data parallelism",
    "start": "3985490",
    "end": "3991220"
  },
  {
    "text": "architecture so usually what you see in practice is you have a GPUs so I take my batch of 256 images or something like",
    "start": "3991220",
    "end": "3998210"
  },
  {
    "text": "that I split it and I split it equally across the GPUs I do forward pass on those GPUs and then I basically just add",
    "start": "3998210",
    "end": "4005140"
  },
  {
    "text": "up all the gradients and I propagate that through so you're just distributing this batch and you're doing mathematical you're",
    "start": "4005140",
    "end": "4011140"
  },
  {
    "text": "doing the exact same thing as if you had a giant GPU but you're just splitting up that batch across different GPUs but",
    "start": "4011140",
    "end": "4016960"
  },
  {
    "text": "you're still doing synchronous training with SGD as normal so that's what you'll see most in practice which i think is the best thing to do right now for most",
    "start": "4016960",
    "end": "4024910"
  },
  {
    "text": "normal applications and other kind of considerations that sometimes enter that you could may be worried about is that",
    "start": "4024910",
    "end": "4031839"
  },
  {
    "text": "there are these bottlenecks to be aware of some particular CPU to disk bottleneck this means that you have a giant data set it's somewhere on some",
    "start": "4031839",
    "end": "4038170"
  },
  {
    "text": "disk you want that disk to probably be an SSD because you want this loading to be quick because these GPUs process data",
    "start": "4038170",
    "end": "4044200"
  },
  {
    "text": "very quickly and that might actually be a Balan like like loading the data could be a bomb like so many applications you might want to pre process your data make",
    "start": "4044200",
    "end": "4050769"
  },
  {
    "text": "sure that it's read out contiguously and very raw form from something like an HD f5 file or some kind of other binary",
    "start": "4050769",
    "end": "4056890"
  },
  {
    "text": "format and another bottleneck to be aware of is the CPU GPU bottleneck so",
    "start": "4056890",
    "end": "4061930"
  },
  {
    "text": "the GPU is doing a lot of heavy lifting of the neural network and the CPU is loading the data and you might want to use things like prefetching threads",
    "start": "4061930",
    "end": "4067869"
  },
  {
    "text": "where the CPU while the networks are doing forward backward on the GPU your CPU is busy loading the data from the",
    "start": "4067869",
    "end": "4073869"
  },
  {
    "text": "disk and maybe doing some pre-processing and making sure that it can ship it off to the GPU at the next time step so",
    "start": "4073869",
    "end": "4080950"
  },
  {
    "text": "those are some of the practical considerations I could come up with for this lecture if you wanted to learn much more about convolutional neural networks",
    "start": "4080950",
    "end": "4086769"
  },
  {
    "text": "and a lot of what I've been talking about then I encourage you to check out CS 231 n we have lecture videos",
    "start": "4086769",
    "end": "4091900"
  },
  {
    "text": "available we have notes slides and assignments everything is up and available so you're welcome to check it",
    "start": "4091900",
    "end": "4098440"
  },
  {
    "text": "out and that's it thank you",
    "start": "4098440",
    "end": "4102778"
  },
  {
    "text": "so I guess I can take some questions yeah",
    "start": "4109850",
    "end": "4114140"
  },
  {
    "text": "hello hello hi I'm Kyle afar from Luna",
    "start": "4132709",
    "end": "4137929"
  },
  {
    "text": "I'm using a lot of convolutional nets for genomics when the problems that we see is that our genomic sequence tends",
    "start": "4137929",
    "end": "4144479"
  },
  {
    "text": "to be arbitrary length so right now we're pattern for a lot of zeros but we're curious as to what your thoughts",
    "start": "4144479",
    "end": "4150149"
  },
  {
    "text": "are on using CN NS for things of arbitrary size where we can't just down sample to 277 by 277 yep",
    "start": "4150149",
    "end": "4158039"
  },
  {
    "text": "so is this lecture genomic sequence of like a TCG like that kind of sequence exactly yeah so some of the options",
    "start": "4158039",
    "end": "4163380"
  },
  {
    "text": "would be so recurrent neural networks might be a good fit because they allow arbitrarily sized contexts another",
    "start": "4163380",
    "end": "4168959"
  },
  {
    "text": "option I would say is if you look at the wave net paper from deep mind they have audio and they're using convolutional",
    "start": "4168959",
    "end": "4174449"
  },
  {
    "text": "networks for processing it and I would basically adopt that kind of an architecture they have this clever way of doing what's called a truce or",
    "start": "4174449",
    "end": "4180209"
  },
  {
    "text": "dilated convolutions and so that allows you to capture a lot of context with few layers and so let's call dilated",
    "start": "4180209",
    "end": "4186749"
  },
  {
    "text": "convolutions and the wavelet paper has some details and there's an efficient implementation of it that you should be aware of on github and so you might be",
    "start": "4186749",
    "end": "4192179"
  },
  {
    "text": "able to just drag and drop the fast wave net code into that application and so you have much larger context but it's of",
    "start": "4192179",
    "end": "4197669"
  },
  {
    "text": "course not infinite context as you might have with a recurrent Network yeah we're definitely checking those out we also tried our n ends they're quite slow for",
    "start": "4197669",
    "end": "4204449"
  },
  {
    "text": "these things our main problems that the genes can be very short or very long but the whole sequence matters so I think",
    "start": "4204449",
    "end": "4212039"
  },
  {
    "text": "that's one of the challenges that we're looking at with this type of problem interesting yeah so those would be the",
    "start": "4212039",
    "end": "4218340"
  },
  {
    "text": "two options that I would play with basically I think those are the two demo where thank you",
    "start": "4218340",
    "end": "4224059"
  },
  {
    "text": "thanks for a great lecture so my question is that is there a clear mathematical or conceptual understanding",
    "start": "4227159",
    "end": "4233349"
  },
  {
    "text": "when people decide how many hidden layers have to be part of their architecture yeah so the answer with a",
    "start": "4233349",
    "end": "4240489"
  },
  {
    "text": "lot of this is there a mathematical understanding will likely be no because we are in very early phases of just",
    "start": "4240489",
    "end": "4245709"
  },
  {
    "text": "doing a lot of empirical and I'll guess and check kind of work and so theory is",
    "start": "4245709",
    "end": "4251110"
  },
  {
    "text": "in some some ways like lagging behind a bit I would say that was residual networks you want to have more layers",
    "start": "4251110",
    "end": "4256809"
  },
  {
    "text": "usually works better and so you can take these layers outdoor you can put them in and it's just mostly computational",
    "start": "4256809",
    "end": "4262479"
  },
  {
    "text": "consideration of how much can you fit in so our consideration is usually is you have a GPU it has maybe 16 gigs of ram",
    "start": "4262479",
    "end": "4268900"
  },
  {
    "text": "or 12 gigs of ram or something I want certain batch size and I have these considerations and that upper",
    "start": "4268900",
    "end": "4274030"
  },
  {
    "text": "bounds the amount of like layers or how big they could be and so I use the biggest thing that fits in my GPU and",
    "start": "4274030",
    "end": "4279639"
  },
  {
    "text": "that's mostly what the way you choose this and then you regularize it very strongly so if you have a very small",
    "start": "4279639",
    "end": "4285159"
  },
  {
    "text": "data set then you might end up with a pretty big Network for your data set so you might want to make sure that you are tuning those dropout rates properly and",
    "start": "4285159",
    "end": "4291459"
  },
  {
    "text": "so you're not overfitting I have",
    "start": "4291459",
    "end": "4298510"
  },
  {
    "text": "question my understanding is that the recent convolution that doesn't use polling layers right so the question is",
    "start": "4298510",
    "end": "4305229"
  },
  {
    "text": "why you know why don't they use fungal areas so you know is there still a place",
    "start": "4305229",
    "end": "4311920"
  },
  {
    "text": "for puli yeah yeah so certainly so if you saw for example the residual Network at the end there was a single pooling",
    "start": "4311920",
    "end": "4318219"
  },
  {
    "text": "layer at the very beginning but mostly they went away you're right so took her I wonder if I can find the slide I",
    "start": "4318219",
    "end": "4323739"
  },
  {
    "text": "wonder if this is a good idea to try to find the slide that's bro okay let me",
    "start": "4323739",
    "end": "4330280"
  },
  {
    "text": "just find this okay so this was the residual network architecture so you see",
    "start": "4330280",
    "end": "4336010"
  },
  {
    "text": "that they do a first comm and then there's a single pool right there but certainly the trend has been to throw",
    "start": "4336010",
    "end": "4341260"
  },
  {
    "text": "them away over time and there's a paper also it's called striving for simplicity the all convolutional neural network and the",
    "start": "4341260",
    "end": "4348489"
  },
  {
    "text": "point in that paper is look you can actually do stranded convolutions you can throw away pulling layers all together or it's just as well so pulling",
    "start": "4348489",
    "end": "4354610"
  },
  {
    "text": "layers are kind of I would say this kind of a bit of a historical vestige of they needed things to be efficient then they",
    "start": "4354610",
    "end": "4359650"
  },
  {
    "text": "need to control Bastian downsample things quite a lot and so we're kind of throwing them away over time and yeah they're not doing",
    "start": "4359650",
    "end": "4366820"
  },
  {
    "text": "anything like super useful they're doing this fixed operation and you want to learn as much as possible so maybe you",
    "start": "4366820",
    "end": "4372760"
  },
  {
    "text": "don't actually want to get rid of that information so it's always more appealing to it's probably more",
    "start": "4372760",
    "end": "4378369"
  },
  {
    "text": "appealing I would say to throw them away well you mentioned there is a sort of cognitive or brain analogy that the",
    "start": "4378369",
    "end": "4385090"
  },
  {
    "text": "brain is doing polling so yeah so I think that analogy is stretched by a lot so the brain I'm not sure every brain is",
    "start": "4385090",
    "end": "4390489"
  },
  {
    "text": "doing pulling yeah about image",
    "start": "4390489",
    "end": "4397239"
  },
  {
    "text": "compression not for justification about the usage of neural networks for image",
    "start": "4397239",
    "end": "4402310"
  },
  {
    "text": "compression do we have any examples sorry I couldn't hear the question it's of classification for images can we use",
    "start": "4402310",
    "end": "4408429"
  },
  {
    "text": "the neural networks for image compression image compression yeah I think there's actually a really exciting",
    "start": "4408429",
    "end": "4414340"
  },
  {
    "text": "work in this area so one that I'm aware of for example as a recent work from Google where they're using commercial",
    "start": "4414340",
    "end": "4420280"
  },
  {
    "text": "networks and recurrent networks to come up with variably sized codes for images so certainly a lot of these generative",
    "start": "4420280",
    "end": "4426190"
  },
  {
    "text": "models I mean they are very related to compression so definitely a lot of work in the area of them that I'm excited",
    "start": "4426190",
    "end": "4432280"
  },
  {
    "text": "about also for example super resolution networks so you saw the recent acquisition of magic pony by Twitter so",
    "start": "4432280",
    "end": "4438969"
  },
  {
    "text": "they were also doing something that basically allows you to compress you can send low resolution strings because you can up sample it on the client and so a",
    "start": "4438969",
    "end": "4446860"
  },
  {
    "text": "lot of work in that area yeah I had we should patent it after you I can't",
    "start": "4446860",
    "end": "4454150"
  },
  {
    "text": "please comment on scalability regarding number of classes so what does it take",
    "start": "4454150",
    "end": "4459219"
  },
  {
    "text": "if we go up to 10,000 or 100,000 classes hmm yes so if you have a lot of classes",
    "start": "4459219",
    "end": "4464770"
  },
  {
    "text": "then of course you can grow your softmax but that becomes inefficient at some point because you're doing a giant matrix multiply so some of the ways that",
    "start": "4464770",
    "end": "4471699"
  },
  {
    "text": "people are addressing this in practice I believe is you use of like hierarchical softmax and things like that so you you",
    "start": "4471699",
    "end": "4479320"
  },
  {
    "text": "decompose your classes into groups and then you kind of predict one group at a time and you kind of converge that way",
    "start": "4479320",
    "end": "4486150"
  },
  {
    "text": "so I'm not I see these papers but I don't I'm not an expert on exactly how this works but I do know that they are",
    "start": "4486150",
    "end": "4492369"
  },
  {
    "text": "called softmax is something that people thing especially for example in language models this is often used because you have a huge amount of words and you",
    "start": "4492369",
    "end": "4499300"
  },
  {
    "text": "still need to predict them somehow so I believe Thomas Mikhailov for example he has some papers on using hierarchical softmax in this context would you could",
    "start": "4499300",
    "end": "4507640"
  },
  {
    "text": "you talk a little bit about the convolutional functions like what what considerations you should make in",
    "start": "4507640",
    "end": "4513690"
  },
  {
    "text": "selecting the functions that are used in the convolutional filters selecting the functions that are used in the",
    "start": "4513690",
    "end": "4519460"
  },
  {
    "text": "convolutional filters so these filters are just parameters right so we train those filters they're just numbers that",
    "start": "4519460",
    "end": "4525990"
  },
  {
    "text": "we trained with backpropagation okay are you talking about the nonlinearities perhaps or yeah I'm just wondering about",
    "start": "4525990",
    "end": "4532830"
  },
  {
    "text": "when you're selecting those the features or when you're getting the when you're trying to train to just understand",
    "start": "4532830",
    "end": "4539260"
  },
  {
    "text": "different features with an image what what are those filters actually doing well I see you're talking about",
    "start": "4539260",
    "end": "4544870"
  },
  {
    "text": "understanding exactly what those filters are looking for in the engine somewhat so a lot of interesting work especially for example so Jason your Sinskey he has",
    "start": "4544870",
    "end": "4551950"
  },
  {
    "text": "this deepest toolbox and I've shown you that you can kind of debug it that way a bit there's an entire lecture to encourage you to watch in CS 231 and on",
    "start": "4551950",
    "end": "4558850"
  },
  {
    "text": "visualizing understanding accomplish all networks so people use things like a decom or guided or guided back",
    "start": "4558850",
    "end": "4565390"
  },
  {
    "text": "propagation or you back propagate to image and you try to find the stimulus that maximally activates any arbitrary",
    "start": "4565390",
    "end": "4570910"
  },
  {
    "text": "neuron so different ways of probing it and different ways have been developed and there's a lecture about it so I",
    "start": "4570910",
    "end": "4577510"
  },
  {
    "text": "would I would check that out great thanks I had a question regarding the size of fine-tuning data set for",
    "start": "4577510",
    "end": "4585040"
  },
  {
    "text": "example is there a ballpark game number if you are trying to do classification how many do you put you need for",
    "start": "4585040",
    "end": "4592470"
  },
  {
    "text": "fine-tuning it to your sample set so how many how many data points do you need to",
    "start": "4592470",
    "end": "4598180"
  },
  {
    "text": "get good performance since the question okay so so okay so this is like the most",
    "start": "4598180",
    "end": "4604420"
  },
  {
    "text": "boring answer I think because the more the better always and it's really hard to say actually the company you need so",
    "start": "4604420",
    "end": "4612610"
  },
  {
    "text": "usually one way one way to look at it as one heuristic that people sometimes follow is you look at a number of",
    "start": "4612610",
    "end": "4617680"
  },
  {
    "text": "parameters and you want the number of examples to be on the order of number of parameters that's one way people sometimes break it down right for",
    "start": "4617680",
    "end": "4624310"
  },
  {
    "text": "fine-tuning because we will have an image net model so I was hoping that most of the things",
    "start": "4624310",
    "end": "4629590"
  },
  {
    "text": "would be taken care or there and then you're just fine-tuning so you you might need a lower order I say so when you're",
    "start": "4629590",
    "end": "4634840"
  },
  {
    "text": "saying fine-tuning are you finding the whole network or you're using some of it or just the top classifier just the top classifier yeah so one another way to",
    "start": "4634840",
    "end": "4641260"
  },
  {
    "text": "look at it is you have some number of parameters and you can estimate the number of bits that you think every",
    "start": "4641260",
    "end": "4646300"
  },
  {
    "text": "parameter has and then you count the number of bits in your data so that's kind of like comparisons you would do but really uh yeah I have no good answer",
    "start": "4646300",
    "end": "4654310"
  },
  {
    "text": "so the more the better and you have to try and you have to regularize and you have to cross validate that and you have to see what performance you get over time because it's to task it and then",
    "start": "4654310",
    "end": "4661870"
  },
  {
    "text": "for me to say something stronger hi I would like to know how do you think the",
    "start": "4661870",
    "end": "4667030"
  },
  {
    "text": "Covenant will work in this rady case like is they just a simple extension of the 2d case oh do we need some extra",
    "start": "4667030",
    "end": "4674410"
  },
  {
    "text": "tweak about in 3d case so you're talking specifically about say videos or some 3d",
    "start": "4674410",
    "end": "4679750"
  },
  {
    "text": "accurate talking about the image that has the depth information oh I see so",
    "start": "4679750",
    "end": "4685240"
  },
  {
    "text": "say you have like RGB D input and things like that yeah so I'm not too familiar with people do but I do know for example that people",
    "start": "4685240",
    "end": "4693100"
  },
  {
    "text": "try to have for example one thing he can do is just treat it as a fourth Channel or maybe you want the separate ComNet on",
    "start": "4693100",
    "end": "4698890"
  },
  {
    "text": "top of the depth channel and do some fusion later so I don't know exactly what the state-of-the-art in treating that depth channel is right now so I",
    "start": "4698890",
    "end": "4707560"
  },
  {
    "text": "don't exactly how they do what I do right now oh so maybe just one more question just how do you think the 3d",
    "start": "4707560",
    "end": "4713290"
  },
  {
    "text": "object great condition a 3d object yeah recognition so what is the output that",
    "start": "4713290",
    "end": "4719110"
  },
  {
    "text": "you'd like the oppo is still the class probability but we are not treating the",
    "start": "4719110",
    "end": "4724600"
  },
  {
    "text": "2d image but the 3d representation of the art I say so do you have a mesh or point cloud yeah I see yeah so also not",
    "start": "4724600",
    "end": "4732100"
  },
  {
    "text": "not exactly my area for currently but so the problem with these meshes and so on is that there's just like a rotational",
    "start": "4732100",
    "end": "4738040"
  },
  {
    "text": "degree of freedom that I'm not sure what people do about honestly so the yeah so",
    "start": "4738040",
    "end": "4744250"
  },
  {
    "text": "I'm actually not an expert on this so I don't want to comment there are some obvious things you might want to try like you might want to plug in all the possible ways you could orient this and",
    "start": "4744250",
    "end": "4750850"
  },
  {
    "text": "then a test time averaged over them so there would be some of the obvious things to play with but I don't I'm not actually sure what the state of the art",
    "start": "4750850",
    "end": "4756610"
  },
  {
    "text": "is okay thank you so coming back to the distributed",
    "start": "4756610",
    "end": "4763440"
  },
  {
    "text": "training is it possible to do even the classification a distributed way or my questions in future can I imagine my our",
    "start": "4763440",
    "end": "4770670"
  },
  {
    "text": "cellphones do these things together for one inquiry our cellphones oh I see",
    "start": "4770670",
    "end": "4777810"
  },
  {
    "text": "you're trying to get cell phones distributed training yes it's a train it's arrived quite a fun very radical",
    "start": "4777810",
    "end": "4786390"
  },
  {
    "text": "idea so related thoughts I had recently was so I had come the jas in the browser and I was thinking of basically this",
    "start": "4786390",
    "end": "4793020"
  },
  {
    "text": "trains narrow networks and I was thinking about similar questions because you could imagine shipping this off as an ad equivalent like the people just",
    "start": "4793020",
    "end": "4799200"
  },
  {
    "text": "include this in the JavaScript and then everyone's browsers are kind of like training a small network so I think",
    "start": "4799200",
    "end": "4805350"
  },
  {
    "text": "that's a related question do you think there's like too much communication overhead or it could be actually really disturbed in an efficient way",
    "start": "4805350",
    "end": "4811950"
  },
  {
    "text": "yes so the problem with distributing it a lot is actually a stale gradients problem so when you look at some of the",
    "start": "4811950",
    "end": "4818070"
  },
  {
    "text": "papers that Google has put out about distributed training as you look at the number of workers when you do",
    "start": "4818070",
    "end": "4823440"
  },
  {
    "text": "asynchronous SGD number of workers and the the performance improvement you get it kind of like plateaus quite quickly",
    "start": "4823440",
    "end": "4828780"
  },
  {
    "text": "after like eight workers or something quite small so I'm not sure if there are ways of dealing with thousands of",
    "start": "4828780",
    "end": "4834300"
  },
  {
    "text": "workers the issue is that you have a distributed every worker has this like specific snapshot of the weights that",
    "start": "4834300",
    "end": "4841020"
  },
  {
    "text": "are currently I have to pull you pull from the master and now you have a set",
    "start": "4841020",
    "end": "4846810"
  },
  {
    "text": "of ways that you're using and do forward backward and you send an update but by the time you send an update and you down your forward backward the parameters",
    "start": "4846810",
    "end": "4853440"
  },
  {
    "text": "server has now done like lots of updates from like thousands of other things and so you're grading the scale you've",
    "start": "4853440",
    "end": "4859470"
  },
  {
    "text": "evaluated it every wrong an old location and so it's an incorrect direction now",
    "start": "4859470",
    "end": "4864660"
  },
  {
    "text": "and everything breaks so that's the challenge and I'm not sure what people are doing about this yeah I was",
    "start": "4864660",
    "end": "4871620"
  },
  {
    "text": "wondering about applications of convolutional nets to two inputs at a",
    "start": "4871620",
    "end": "4876900"
  },
  {
    "text": "time so let's say you have two pictures of jigsaw puzzles puzzles these are pieces they're trying to",
    "start": "4876900",
    "end": "4882270"
  },
  {
    "text": "figure out if they fit together or whether one object compares to the other in a specific way have you heard of any",
    "start": "4882270",
    "end": "4888090"
  },
  {
    "text": "implementation of this kind yes so you have two inputs instead of one yeah so the common way",
    "start": "4888090",
    "end": "4893310"
  },
  {
    "text": "dealing with that as you put a comment on each and then you do some kind of a fusion eventually to to merge the information right I see and same for",
    "start": "4893310",
    "end": "4901130"
  },
  {
    "text": "recurrent neural networks if you have like variable input so for example in the context the videos where you have",
    "start": "4901130",
    "end": "4906870"
  },
  {
    "text": "frames coming in yeah then yeah so some of the approaches are you have accomplished all Network on the frame and then at the top you tie it in with",
    "start": "4906870",
    "end": "4913110"
  },
  {
    "text": "the recurrent neural network so you have these you reduce the image to some kind of a lower dimensional representation",
    "start": "4913110",
    "end": "4918570"
  },
  {
    "text": "and then that get that's an input to a recurrent neural network at the top there are other ways to play with this",
    "start": "4918570",
    "end": "4923760"
  },
  {
    "text": "for example you can actually make the recurrent you can make every single neuron in the calm that recurrent that's also one funny way of doing this so",
    "start": "4923760",
    "end": "4930960"
  },
  {
    "text": "right now when a neuron computes its output it's only a function of a local neighborhood and below it but you can",
    "start": "4930960",
    "end": "4938130"
  },
  {
    "text": "also make it in addition a function of that same local neighborhood or like its own activation perhaps in the previous",
    "start": "4938130",
    "end": "4944040"
  },
  {
    "text": "time step if that makes sense so so this so this neuron is not just computing a",
    "start": "4944040",
    "end": "4949470"
  },
  {
    "text": "dot product with the with the current patch but it's also incorporating a dot product of its own and maybe it's",
    "start": "4949470",
    "end": "4955230"
  },
  {
    "text": "neighborhoods activations at the previous time step of the frame so that's kind of like a small or an update hidden inside every single neuron so",
    "start": "4955230",
    "end": "4961680"
  },
  {
    "text": "those are the things that I think people play with when I'm not familiar with what currently is working best in this area pretty awesome thank you yeah yeah",
    "start": "4961680",
    "end": "4968180"
  },
  {
    "text": "thanks for the great talk final question regarding the latency for",
    "start": "4968180",
    "end": "4973440"
  },
  {
    "text": "the models that are trained using multiple layers so especially at the prediction time you know as we add more",
    "start": "4973440",
    "end": "4978540"
  },
  {
    "text": "more layers for the forward pass it will take some time you know it'll increase in the latency right for the prediction",
    "start": "4978540",
    "end": "4984630"
  },
  {
    "text": "so what are the numbers that we have seen you know you know presently that",
    "start": "4984630",
    "end": "4989640"
  },
  {
    "text": "you know that you know if you can share that you know the prediction time or the you know the latency at the the forward",
    "start": "4989640",
    "end": "4996930"
  },
  {
    "text": "pass so you're worried for example you're some you want to run a friction very quickly would it be on an embedded",
    "start": "4996930",
    "end": "5002120"
  },
  {
    "text": "device or is this in the cloud uh you're suppose you know it's a cell phone you know you have your you know identifying",
    "start": "5002120",
    "end": "5008210"
  },
  {
    "text": "the objects or you know you're you're doing some you know image analysis or something yeah so there's definitely a",
    "start": "5008210",
    "end": "5014780"
  },
  {
    "text": "lot of work on this so one way you would approach this actually is you have this network that you've trained using floating point arithmetic 32 bits say",
    "start": "5014780",
    "end": "5021770"
  },
  {
    "text": "and so there's a lot of work on taking that Network and discretizing all",
    "start": "5021770",
    "end": "5027170"
  },
  {
    "text": "into like intz and making it much smaller and pruning connections so one of the works I'm related to this for",
    "start": "5027170",
    "end": "5033409"
  },
  {
    "text": "example is someone here at Stanford has few papers on getting rid of spurious connections and reducing the network as",
    "start": "5033409",
    "end": "5039080"
  },
  {
    "text": "much as possible and then making everything very efficient with integer arithmetic so basically you achieve this",
    "start": "5039080",
    "end": "5044989"
  },
  {
    "text": "by discretizing all the weights and all the activations and throwing away and",
    "start": "5044989",
    "end": "5051260"
  },
  {
    "text": "pruning the network so there are some tricks like that that people play that's mostly what you would do in an embedded",
    "start": "5051260",
    "end": "5057020"
  },
  {
    "text": "device and then the challenge of course is you've changed the network and now you just kind of are crossing your",
    "start": "5057020",
    "end": "5062060"
  },
  {
    "text": "fingers that it works well and so I think what's interesting for Reese from research standpoint is he'd like to do",
    "start": "5062060",
    "end": "5067520"
  },
  {
    "text": "you'd like your test time to exactly match your training time right so then you get the best performance and so the",
    "start": "5067520",
    "end": "5073250"
  },
  {
    "text": "question is how do we train with low precision arithmetic and there's a lot of work on this as well so say from your show up in Joe's lab as well and so",
    "start": "5073250",
    "end": "5081110"
  },
  {
    "text": "that's exciting directions of how you train in low precision regime do you have any numbers I mean that you can",
    "start": "5081110",
    "end": "5086719"
  },
  {
    "text": "share for the new state of the art how much time does it yes I see the papers",
    "start": "5086719",
    "end": "5092030"
  },
  {
    "text": "but I'm not sure if I remember the exact reductions it's on the order of okay I don't want to say because it's go no",
    "start": "5092030",
    "end": "5097480"
  },
  {
    "text": "thanks I don't want to try to guess this thank you all right we're out of time",
    "start": "5097480",
    "end": "5102679"
  },
  {
    "text": "let's thank Andrew",
    "start": "5102679",
    "end": "5105820"
  },
  {
    "text": "lunch is outside and will restart at 12:45",
    "start": "5111240",
    "end": "5117330"
  }
]