[
  {
    "start": "0",
    "end": "138000"
  },
  {
    "text": "- I see the danger of this\nconcentration of power through proprietary AI systems",
    "start": "120",
    "end": "6030"
  },
  {
    "text": "as a much bigger danger\nthan everything else. What works against this",
    "start": "6030",
    "end": "11429"
  },
  {
    "text": "is people who think that\nfor reasons of security, we should keep AI systems\nunder lock and key",
    "start": "11430",
    "end": "18600"
  },
  {
    "text": "because it's too dangerous to put it in the hands of everybody. That would lead to a very bad future",
    "start": "18600",
    "end": "25380"
  },
  {
    "text": "in which all of our information diet is controlled by a small\nnumber of companies",
    "start": "25380",
    "end": "30900"
  },
  {
    "text": "through proprietary systems. - I believe that people\nare fundamentally good and so if AI, especially open source AI",
    "start": "30900",
    "end": "38550"
  },
  {
    "text": "can make them smarter, it just empowers the goodness in humans.",
    "start": "38550",
    "end": "44250"
  },
  {
    "text": "- So I share that feeling. Okay? I think people are\nfundamentally good. (laughing)",
    "start": "44250",
    "end": "50280"
  },
  {
    "text": "And in fact a lot of doomers are doomers because they don't think that\npeople are fundamentally good.",
    "start": "50280",
    "end": "55530"
  },
  {
    "text": "- The following is a\nconversation with Yann LeCun, his third time on this podcast.",
    "start": "57720",
    "end": "62910"
  },
  {
    "text": "He is the chief AI scientist at Meta, professor at NYU, Turing Award winner",
    "start": "62910",
    "end": "68820"
  },
  {
    "text": "and one of the seminal figures in the history of artificial intelligence. He and Meta AI",
    "start": "68820",
    "end": "75720"
  },
  {
    "text": "have been big proponents of\nopen sourcing AI development, and have been walking the walk",
    "start": "75720",
    "end": "81390"
  },
  {
    "text": "by open sourcing many\nof their biggest models, including LLaMA 2 and eventually LLaMA 3.",
    "start": "81390",
    "end": "88230"
  },
  {
    "text": "Also, Yann has been an outspoken critic of those people in the AI community",
    "start": "88230",
    "end": "94500"
  },
  {
    "text": "who warn about the looming danger and existential threat of AGI.",
    "start": "94500",
    "end": "99690"
  },
  {
    "text": "He believes the AGI\nwill be created one day, but it will be good.",
    "start": "99690",
    "end": "105510"
  },
  {
    "text": "It will not escape human control nor will it dominate and kill all humans.",
    "start": "105510",
    "end": "112200"
  },
  {
    "text": "At this moment of rapid AI development, this happens to be somewhat\na controversial position.",
    "start": "112200",
    "end": "118860"
  },
  {
    "text": "And so it's been fun seeing Yann get into a lot of intense and fascinating discussions online",
    "start": "118860",
    "end": "124930"
  },
  {
    "text": "as we do in this very conversation. This is the Lex Fridman podcast. To support it,",
    "start": "125820",
    "end": "131340"
  },
  {
    "text": "please check out our\nsponsors in the description. And now, dear friends, here's Yann LeCun.",
    "start": "131340",
    "end": "137043"
  },
  {
    "start": "138000",
    "end": "834000"
  },
  {
    "text": "You've had some strong statements, technical statements about the future of artificial\nintelligence recently,",
    "start": "138030",
    "end": "145620"
  },
  {
    "text": "throughout your career\nactually but recently as well. You've said that autoregressive LLMs",
    "start": "145620",
    "end": "151980"
  },
  {
    "text": "are not the way we're\ngoing to make progress towards superhuman intelligence.",
    "start": "151980",
    "end": "158790"
  },
  {
    "text": "These are the large language models like GPT-4, like LLaMA\n2 and 3 soon and so on.",
    "start": "158790",
    "end": "164280"
  },
  {
    "text": "How do they work and why are they not going\nto take us all the way? - For a number of reasons. The first is that there is\na number of characteristics",
    "start": "164280",
    "end": "171840"
  },
  {
    "text": "of intelligent behavior. For example, the capacity\nto understand the world,",
    "start": "171840",
    "end": "178860"
  },
  {
    "text": "understand the physical world, the ability to remember\nand retrieve things,",
    "start": "178860",
    "end": "185523"
  },
  {
    "text": "persistent memory, the ability to reason\nand the ability to plan.",
    "start": "186406",
    "end": "192390"
  },
  {
    "text": "Those are four essential characteristic of intelligent systems or entities,",
    "start": "192390",
    "end": "198150"
  },
  {
    "text": "humans, animals. LLMs can do none of those, or they can only do them\nin a very primitive way.",
    "start": "198150",
    "end": "206610"
  },
  {
    "text": "And they don't really\nunderstand the physical world, they don't really have persistent memory, they can't really reason",
    "start": "206610",
    "end": "212310"
  },
  {
    "text": "and they certainly can't plan. And so if you expect the\nsystem to become intelligent",
    "start": "212310",
    "end": "218910"
  },
  {
    "text": "just without having the\npossibility of doing those things, you're making a mistake.",
    "start": "218910",
    "end": "224793"
  },
  {
    "text": "That is not to say that\nautoregressive LLMs are not useful, they're certainly useful.",
    "start": "225870",
    "end": "232070"
  },
  {
    "text": "That they're not interesting, that we can't build a whole ecosystem of\napplications around them,",
    "start": "233910",
    "end": "240120"
  },
  {
    "text": "of course we can. But as a path towards\nhuman level intelligence,",
    "start": "240120",
    "end": "245609"
  },
  {
    "text": "they're missing essential components. And then there is another tidbit or fact",
    "start": "245609",
    "end": "251190"
  },
  {
    "text": "that I think is very interesting; those LLMs are trained on\nenormous amounts of text.",
    "start": "251190",
    "end": "256560"
  },
  {
    "text": "Basically the entirety of all publicly available\ntext on the internet, right? That's typically on the\norder of 10 to the 13 tokens.",
    "start": "256560",
    "end": "266540"
  },
  {
    "text": "Each token is typically two bytes. So that's two 10 to the\n13 bytes as training data.",
    "start": "266580",
    "end": "271820"
  },
  {
    "text": "It would take you or me 170,000 years to just read through this at\neight hours a day. (laughs)",
    "start": "271820",
    "end": "277964"
  },
  {
    "text": "So it seems like an enormous\namount of knowledge, right? That those systems can accumulate.",
    "start": "277964",
    "end": "283010"
  },
  {
    "text": "But then you realize it's\nreally not that much data. If you talk to\ndevelopmental psychologists,",
    "start": "286170",
    "end": "291711"
  },
  {
    "text": "and they tell you a 4-year-old has been awake for 16,000\nhours in his or her life,",
    "start": "291712",
    "end": "297573"
  },
  {
    "text": "and the amount of information that has reached the\nvisual cortex of that child",
    "start": "300210",
    "end": "306274"
  },
  {
    "text": "in four years is about 10 to 15 bytes.",
    "start": "306274",
    "end": "312090"
  },
  {
    "text": "And you can compute this by estimating that the optical nerve carry about 20 megabytes\nper second, roughly.",
    "start": "312090",
    "end": "319650"
  },
  {
    "text": "And so 10 to the 15 bytes for a 4-year-old versus two times 10 to the 13 bytes",
    "start": "319650",
    "end": "325410"
  },
  {
    "text": "for 170,000 years worth of reading. What that tells you is\nthat through sensory input,",
    "start": "325410",
    "end": "333780"
  },
  {
    "text": "we see a lot more information than we do through language. And that despite our intuition,",
    "start": "333780",
    "end": "340949"
  },
  {
    "text": "most of what we learn\nand most of our knowledge is through our observation and interaction",
    "start": "340950",
    "end": "346568"
  },
  {
    "text": "with the real world, not through language. Everything that we learn in\nthe first few years of life,",
    "start": "346568",
    "end": "351690"
  },
  {
    "text": "and certainly everything\nthat animals learn has nothing to do with language.",
    "start": "351690",
    "end": "357060"
  },
  {
    "text": "- So it would be good to maybe push against\nsome of the intuition behind what you're saying. So it is true there's\nseveral orders of magnitude",
    "start": "357060",
    "end": "365525"
  },
  {
    "text": "more data coming into the\nhuman mind, much faster,",
    "start": "365525",
    "end": "370525"
  },
  {
    "text": "and the human mind is able to\nlearn very quickly from that, filter the data very quickly. Somebody might argue",
    "start": "370620",
    "end": "376620"
  },
  {
    "text": "your comparison between\nsensory data versus language. That language is already very compressed.",
    "start": "376620",
    "end": "383250"
  },
  {
    "text": "It already contains a lot more information than the bytes it takes to store them, if you compare it to visual data.",
    "start": "383250",
    "end": "389370"
  },
  {
    "text": "So there's a lot of wisdom in language. There's words and the way\nwe stitch them together, it already contains a lot of information.",
    "start": "389370",
    "end": "396270"
  },
  {
    "text": "So is it possible that language alone already has enough wisdom\nand knowledge in there",
    "start": "396270",
    "end": "405650"
  },
  {
    "text": "to be able to, from that\nlanguage construct a world model and understanding of the world,",
    "start": "407010",
    "end": "412640"
  },
  {
    "text": "an understanding of the physical world that you're saying LLMs lack? - So it's a big debate among philosophers",
    "start": "412640",
    "end": "420090"
  },
  {
    "text": "and also cognitive scientists, like whether intelligence needs\nto be grounded in reality.",
    "start": "420090",
    "end": "425819"
  },
  {
    "text": "I'm clearly in the camp that yes, intelligence cannot appear without some grounding in some reality.",
    "start": "425820",
    "end": "434310"
  },
  {
    "text": "It doesn't need to be physical reality, it could be simulated but the environment is just much richer",
    "start": "434310",
    "end": "440910"
  },
  {
    "text": "than what you can express in language. Language is a very approximate\nrepresentation or percepts",
    "start": "440910",
    "end": "447380"
  },
  {
    "text": "and or mental models, right? I mean, there's a lot of\ntasks that we accomplish where we manipulate a mental\nmodel of the situation at hand,",
    "start": "447600",
    "end": "457190"
  },
  {
    "text": "and that has nothing to do with language. Everything that's physical,\nmechanical, whatever,",
    "start": "458310",
    "end": "463530"
  },
  {
    "text": "when we build something, when we accomplish a task, a moderate task of grabbing\nsomething, et cetera,",
    "start": "463530",
    "end": "470219"
  },
  {
    "text": "we plan our action sequences, and we do this by essentially imagining the result",
    "start": "470220",
    "end": "475710"
  },
  {
    "text": "of the outcome of sequence of\nactions that we might imagine.",
    "start": "475710",
    "end": "480710"
  },
  {
    "text": "And that requires mental models that don't have much to do with language. And that's, I would argue,",
    "start": "481260",
    "end": "487830"
  },
  {
    "text": "most of our knowledge is derived from that interaction\nwith the physical world.",
    "start": "487830",
    "end": "493740"
  },
  {
    "text": "So a lot of my colleagues who are more interested in\nthings like computer vision",
    "start": "493740",
    "end": "499290"
  },
  {
    "text": "are really on that camp that AI needs to be embodied, essentially.",
    "start": "499290",
    "end": "505080"
  },
  {
    "text": "And then other people\ncoming from the NLP side or maybe some other motivation",
    "start": "505080",
    "end": "512508"
  },
  {
    "text": "don't necessarily agree with that. And philosophers are split as well.",
    "start": "512508",
    "end": "517083"
  },
  {
    "text": "And the complexity of the\nworld is hard to imagine.",
    "start": "518400",
    "end": "522770"
  },
  {
    "text": "It's hard to represent\nall the complexities",
    "start": "524700",
    "end": "529700"
  },
  {
    "text": "that we take completely for\ngranted in the real world that we don't even imagine\nrequire intelligence, right? This is the old Moravec's paradox",
    "start": "531000",
    "end": "537960"
  },
  {
    "text": "from the pioneer of\nrobotics, Hans Moravec, who said, how is it that with computers,",
    "start": "537960",
    "end": "543270"
  },
  {
    "text": "it seems to be easy to do\nhigh level complex tasks like playing chess and solving integrals",
    "start": "543270",
    "end": "548400"
  },
  {
    "text": "and doing things like that, whereas the thing we take for\ngranted that we do every day,",
    "start": "548400",
    "end": "553485"
  },
  {
    "text": "like, I don't know,\nlearning to drive a car or grabbing an object,",
    "start": "553486",
    "end": "558630"
  },
  {
    "text": "we can't do with computers. (laughs) And we have LLMs that\ncan pass the bar exam,",
    "start": "558630",
    "end": "566780"
  },
  {
    "text": "so they must be smart. But then they can't\nlaunch a drive in 20 hours like any 17-year-old.",
    "start": "568320",
    "end": "575430"
  },
  {
    "text": "They can't learn to clear\nout the dinner table and fill out the dishwasher like any 10-year-old\ncan learn in one shot.",
    "start": "575430",
    "end": "582735"
  },
  {
    "text": "Why is that? Like what are we missing? What type of learning or reasoning architecture\nor whatever are we missing",
    "start": "582736",
    "end": "592230"
  },
  {
    "text": "that basically prevent us from having level five self-driving cars",
    "start": "592230",
    "end": "598889"
  },
  {
    "text": "and domestic robots? - Can a large language model\nconstruct a world model",
    "start": "598890",
    "end": "605520"
  },
  {
    "text": "that does know how to drive and does know how to fill a dishwasher, but just doesn't know how to deal with visual data at this time?",
    "start": "605520",
    "end": "612540"
  },
  {
    "text": "So it can operate in a space of concepts. - So yeah, that's what a lot\nof people are working on.",
    "start": "612540",
    "end": "619950"
  },
  {
    "text": "So the answer, the short answer is no. And the more complex answer is you can use all kind of tricks",
    "start": "619950",
    "end": "626303"
  },
  {
    "text": "to get an LLM to basically\ndigest visual representations",
    "start": "626303",
    "end": "631303"
  },
  {
    "text": "of images or video or\naudio for that matter.",
    "start": "635987",
    "end": "640593"
  },
  {
    "text": "And a classical way of doing this is you train a vision system in some way,",
    "start": "642360",
    "end": "648513"
  },
  {
    "text": "and we have a number of ways\nto train vision systems, either supervised,\nunsupervised, self-supervised, all kinds of different ways.",
    "start": "649380",
    "end": "655160"
  },
  {
    "text": "That will turn any image into\na high level representation.",
    "start": "656280",
    "end": "661053"
  },
  {
    "text": "Basically, a list of tokens that are really similar\nto the kind of tokens that a typical LLM takes as an input.",
    "start": "662160",
    "end": "670650"
  },
  {
    "text": "And then you just feed that to the LLM in addition to the text,",
    "start": "670650",
    "end": "677070"
  },
  {
    "text": "and you just expect\nthe LLM during training to kind of be able to\nuse those representations",
    "start": "677070",
    "end": "685076"
  },
  {
    "text": "to help make decisions. I mean, there's been\nwork along those lines for quite a long time.",
    "start": "685076",
    "end": "690363"
  },
  {
    "text": "And now you see those systems, right? I mean, there are LLMs that\nhave some vision extension.",
    "start": "691320",
    "end": "696630"
  },
  {
    "text": "But they're basically hacks in the sense that those things are not like trained to handle,",
    "start": "696630",
    "end": "701675"
  },
  {
    "text": "to really understand the world. They're not trained\nwith video, for example. They don't really understand\nintuitive physics,",
    "start": "701675",
    "end": "708930"
  },
  {
    "text": "at least not at the moment. - So you don't think there's something special to\nyou about intuitive physics,",
    "start": "708930",
    "end": "714300"
  },
  {
    "text": "about sort of common sense reasoning about the physical space,\nabout physical reality? That to you is a giant leap",
    "start": "714300",
    "end": "720690"
  },
  {
    "text": "that LLMs are just not able to do? - We're not gonna be able to do this with the type of LLMs that\nwe are working with today.",
    "start": "720690",
    "end": "727800"
  },
  {
    "text": "And there's a number of reasons for this, but the main reason is the way LLMs are trained is\nthat you take a piece of text,",
    "start": "727800",
    "end": "736530"
  },
  {
    "text": "you remove some of the words\nin that text, you mask them, you replace them by black markers,",
    "start": "736530",
    "end": "742620"
  },
  {
    "text": "and you train a gigantic neural net to predict the words that are missing. And if you build this neural\nnet in a particular way",
    "start": "742620",
    "end": "750270"
  },
  {
    "text": "so that it can only look at words that are to the left of the\none it's trying to predict,",
    "start": "750270",
    "end": "756090"
  },
  {
    "text": "then what you have is a system that basically is trying to predict the next word in a text, right? So then you can feed it a text, a prompt,",
    "start": "756090",
    "end": "763410"
  },
  {
    "text": "and you can ask it to\npredict the next word. It can never predict\nthe next word exactly. And so what it's gonna do",
    "start": "763410",
    "end": "769080"
  },
  {
    "text": "is produce a probability distribution of all the possible\nwords in the dictionary.",
    "start": "769080",
    "end": "774990"
  },
  {
    "text": "In fact, it doesn't predict words, it predicts tokens that\nare kind of subword units. And so it's easy to handle the uncertainty",
    "start": "774990",
    "end": "781438"
  },
  {
    "text": "in the prediction there because there's only a finite number of possible words in the dictionary,",
    "start": "781439",
    "end": "787320"
  },
  {
    "text": "and you can just compute\na distribution over them. Then what the system does is that it picks a word\nfrom that distribution.",
    "start": "787320",
    "end": "796830"
  },
  {
    "text": "Of course, there's a higher\nchance of picking words that have a higher probability\nwithin that distribution. So you sample from that distribution",
    "start": "796830",
    "end": "802800"
  },
  {
    "text": "to actually produce a word, and then you shift that\nword into the input.",
    "start": "802800",
    "end": "807460"
  },
  {
    "text": "And so that allows the system now to predict the second word, right? And once you do this, you shift it into the input, et cetera.",
    "start": "808350",
    "end": "815250"
  },
  {
    "text": "That's called autoregressive prediction, which is why those LLMs should be called autoregressive LLMs,",
    "start": "815250",
    "end": "822140"
  },
  {
    "text": "but we just call them at LLMs. And there is a difference\nbetween this kind of process",
    "start": "823320",
    "end": "830579"
  },
  {
    "text": "and a process by which\nbefore producing a word, when you talk. When you and I talk,",
    "start": "830580",
    "end": "836610"
  },
  {
    "start": "834000",
    "end": "1066000"
  },
  {
    "text": "you and I are bilinguals. We think about what we're gonna say, and it's relatively independent",
    "start": "836610",
    "end": "841980"
  },
  {
    "text": "of the language in which we're gonna say. When we talk about like, I don't know, let's say a mathematical\nconcept or something.",
    "start": "841980",
    "end": "849029"
  },
  {
    "text": "The kind of thinking that we're doing and the answer that\nwe're planning to produce",
    "start": "849030",
    "end": "853269"
  },
  {
    "text": "is not linked to whether\nwe're gonna say it in French or Russian or English.",
    "start": "854160",
    "end": "859380"
  },
  {
    "text": "- Chomsky just rolled his\neyes, but I understand. So you're saying that\nthere's a bigger abstraction",
    "start": "859380",
    "end": "864834"
  },
  {
    "text": "that goes before language- - [Yann] Yeah.\n- And maps onto language.",
    "start": "864834",
    "end": "870240"
  },
  {
    "text": "- Right. It's certainly true for a\nlot of thinking that we do. - Is that obvious that we don't?",
    "start": "870240",
    "end": "875700"
  },
  {
    "text": "Like you're saying your\nthinking is same in French as it is in English? - Yeah, pretty much.",
    "start": "875700",
    "end": "882000"
  },
  {
    "text": "- Pretty much or is this... Like how flexible are you, like if there's a\nprobability distribution?",
    "start": "882000",
    "end": "888344"
  },
  {
    "text": "(both laugh) - Well, it depends what\nkind of thinking, right? If it's like producing puns,",
    "start": "888344",
    "end": "893811"
  },
  {
    "text": "I get much better in French\nthan English about that (laughs) or much worse- - Is there an abstract\nrepresentation of puns?",
    "start": "893811",
    "end": "900082"
  },
  {
    "text": "Like is your humor an abstract... Like when you tweet and your tweets are\nsometimes a little bit spicy,",
    "start": "900082",
    "end": "906001"
  },
  {
    "text": "is there an abstract representation\nin your brain of a tweet before it maps onto English?",
    "start": "906001",
    "end": "911790"
  },
  {
    "text": "- There is an abstract representation of imagining the reaction\nof a reader to that text.",
    "start": "911790",
    "end": "918380"
  },
  {
    "text": "- Oh, you start with laughter and then figure out how\nto make that happen? - Figure out like a\nreaction you wanna cause",
    "start": "918720",
    "end": "925726"
  },
  {
    "text": "and then figure out how to say it so that it causes that reaction. But that's like really close to language.",
    "start": "925726",
    "end": "930779"
  },
  {
    "text": "But think about like\na mathematical concept or imagining something you\nwant to build out of wood",
    "start": "930780",
    "end": "938006"
  },
  {
    "text": "or something like this, right? The kind of thinking you're doing has absolutely nothing to\ndo with language, really.",
    "start": "938006",
    "end": "943470"
  },
  {
    "text": "Like it's not like you have necessarily like an internal monologue\nin any particular language. You're imagining mental\nmodels of the thing, right?",
    "start": "943470",
    "end": "951893"
  },
  {
    "text": "I mean, if I ask you to like imagine what this water bottle will look like if I rotate it 90 degrees,",
    "start": "951893",
    "end": "959048"
  },
  {
    "text": "that has nothing to do with language. And so clearly",
    "start": "959048",
    "end": "964650"
  },
  {
    "text": "there is a more abstract\nlevel of representation in which we do most of our thinking",
    "start": "964650",
    "end": "971172"
  },
  {
    "text": "and we plan what we're gonna say if the output is uttered words",
    "start": "971172",
    "end": "978870"
  },
  {
    "text": "as opposed to an output\nbeing muscle actions, right?",
    "start": "979830",
    "end": "984830"
  },
  {
    "text": "We plan our answer before we produce it. And LLMs don't do that, they just produce one\nword after the other,",
    "start": "986610",
    "end": "992973"
  },
  {
    "text": "instinctively if you want. It's a bit like the subconscious\nactions where you don't...",
    "start": "993840",
    "end": "1000184"
  },
  {
    "text": "Like you're distracted. You're doing something, you're completely concentrated and someone comes to you\nand asks you a question.",
    "start": "1001940",
    "end": "1007850"
  },
  {
    "text": "And you kind of answer the question. You don't have time to\nthink about the answer, but the answer is easy so you don't need to pay attention",
    "start": "1007850",
    "end": "1014030"
  },
  {
    "text": "and you sort of respond automatically. That's kind of what an LLM does, right? It doesn't think about its answer, really.",
    "start": "1014030",
    "end": "1021170"
  },
  {
    "text": "It retrieves it because it's\naccumulated a lot of knowledge, so it can retrieve some things, but it's going to just spit\nout one token after the other",
    "start": "1021170",
    "end": "1030949"
  },
  {
    "text": "without planning the answer. - But you're making it sound\njust one token after the other,",
    "start": "1030950",
    "end": "1037250"
  },
  {
    "text": "one token at a time generation\nis bound to be simplistic.",
    "start": "1037250",
    "end": "1042250"
  },
  {
    "text": "But if the world model is\nsufficiently sophisticated, that one token at a time,",
    "start": "1045110",
    "end": "1050212"
  },
  {
    "text": "the most likely thing it\ngenerates as a sequence of tokens is going to be a deeply profound thing.",
    "start": "1051251",
    "end": "1059150"
  },
  {
    "text": "- Okay. But then that assumes that those systems actually possess an internal world model.",
    "start": "1059150",
    "end": "1064880"
  },
  {
    "text": "- So it really goes to the... I think the fundamental question is can you build a really\ncomplete world model?",
    "start": "1064880",
    "end": "1073780"
  },
  {
    "start": "1066000",
    "end": "1507000"
  },
  {
    "text": "Not complete, but one that has a deep\nunderstanding of the world. - Yeah.",
    "start": "1074150",
    "end": "1079423"
  },
  {
    "text": "So can you build this\nfirst of all by prediction? - [Lex] Right.",
    "start": "1079423",
    "end": "1084433"
  },
  {
    "text": "- And the answer is probably yes. Can you build it by predicting words?",
    "start": "1084433",
    "end": "1090710"
  },
  {
    "text": "And the answer is most probably no, because language is\nvery poor in terms of...",
    "start": "1090710",
    "end": "1097490"
  },
  {
    "text": "Or weak or low bandwidth if you want, there's just not enough information there. So building world models\nmeans observing the world",
    "start": "1097490",
    "end": "1106200"
  },
  {
    "text": "and understanding why the world\nis evolving the way it is.",
    "start": "1107150",
    "end": "1112150"
  },
  {
    "text": "And then the extra\ncomponent of a world model",
    "start": "1113600",
    "end": "1118600"
  },
  {
    "text": "is something that can predict how the world is going to evolve as a consequence of an\naction you might take, right?",
    "start": "1118670",
    "end": "1125570"
  },
  {
    "text": "So one model really is, here is my idea of the state\nof the world at time T, here is an action I might take.",
    "start": "1125570",
    "end": "1131059"
  },
  {
    "text": "What is the predicted state of the world at time T plus one? Now, that state of the world",
    "start": "1131060",
    "end": "1137397"
  },
  {
    "text": "does not need to represent\neverything about the world, it just needs to represent enough that's relevant for\nthis planning of the action,",
    "start": "1137397",
    "end": "1146120"
  },
  {
    "text": "but not necessarily all the details. Now, here is the problem. You're not going to be able to do this",
    "start": "1146120",
    "end": "1151782"
  },
  {
    "text": "with generative models. So a generative model\nthat's trained on video,",
    "start": "1151782",
    "end": "1156860"
  },
  {
    "text": "and we've tried to do this for 10 years. You take a video, show a system a piece of video",
    "start": "1156860",
    "end": "1162440"
  },
  {
    "text": "and then ask you to predict\nthe reminder of the video. Basically predict what's gonna happen.",
    "start": "1162440",
    "end": "1167870"
  },
  {
    "text": "- One frame at a time. Do the same thing as sort of\nthe autoregressive LLMs do,",
    "start": "1167870",
    "end": "1173330"
  },
  {
    "text": "but for video. - Right. Either one frame at a time or\na group of frames at a time. But yeah, a large video\nmodel, if you want. (laughing)",
    "start": "1173330",
    "end": "1182489"
  },
  {
    "text": "The idea of doing this has been floating around for a long time. And at FAIR,",
    "start": "1183650",
    "end": "1188603"
  },
  {
    "text": "some colleagues and I have been trying to do\nthis for about 10 years.",
    "start": "1189658",
    "end": "1193393"
  },
  {
    "text": "And you can't really do the\nsame trick as with LLMs, because LLMs, as I said,",
    "start": "1194870",
    "end": "1202070"
  },
  {
    "text": "you can't predict exactly\nwhich word is gonna follow a sequence of words, but you can predict the\ndistribution of the words.",
    "start": "1202070",
    "end": "1209540"
  },
  {
    "text": "Now, if you go to video, what you would have to do is predict the distribution of all possible frames in a video.",
    "start": "1209540",
    "end": "1216500"
  },
  {
    "text": "And we don't really know\nhow to do that properly. We do not know how to\nrepresent distributions",
    "start": "1216500",
    "end": "1221955"
  },
  {
    "text": "over high dimensional continuous spaces in ways that are useful.",
    "start": "1221956",
    "end": "1225893"
  },
  {
    "text": "And there lies the main issue. And the reason we can do this",
    "start": "1227300",
    "end": "1233059"
  },
  {
    "text": "is because the world is incredibly more complicated and richer",
    "start": "1233060",
    "end": "1238130"
  },
  {
    "text": "in terms of information than text. Text is discreet. Video is high dimensional and continuous.",
    "start": "1238130",
    "end": "1245000"
  },
  {
    "text": "A lot of details in this. So if I take a video of this room,",
    "start": "1245000",
    "end": "1249833"
  },
  {
    "text": "and the video is a camera panning around,",
    "start": "1250785",
    "end": "1254543"
  },
  {
    "text": "there is no way I can predict everything that's gonna be\nin the room as I pan around, the system cannot predict\nwhat's gonna be in the room",
    "start": "1256850",
    "end": "1262190"
  },
  {
    "text": "as the camera is panning. Maybe it's gonna predict, this is a room where there's\na light and there is a wall",
    "start": "1262190",
    "end": "1268640"
  },
  {
    "text": "and things like that. It can't predict what the\npainting of the wall looks like or what the texture of\nthe couch looks like.",
    "start": "1268640",
    "end": "1274160"
  },
  {
    "text": "Certainly not the texture of the carpet. So there's no way it can\npredict all those details.",
    "start": "1274160",
    "end": "1279200"
  },
  {
    "text": "So the way to handle this is one way to possibly to handle this,",
    "start": "1279200",
    "end": "1284900"
  },
  {
    "text": "which we've been working for a long time, is to have a model that has\nwhat's called a latent variable. And the latent variable\nis fed to a neural net,",
    "start": "1284900",
    "end": "1293060"
  },
  {
    "text": "and it's supposed to represent all the information about the world that you don't perceive yet. And that you need to augment the system",
    "start": "1293060",
    "end": "1302326"
  },
  {
    "text": "for the prediction to do a\ngood job at predicting pixels, including the fine texture\nof the carpet and the couch",
    "start": "1303396",
    "end": "1312220"
  },
  {
    "text": "and the painting on the wall. That has been a complete\nfailure, essentially.",
    "start": "1313520",
    "end": "1320210"
  },
  {
    "text": "And we've tried lots of things. We tried just straight neural nets, we tried GANs, we tried VAEs,",
    "start": "1320210",
    "end": "1328345"
  },
  {
    "text": "all kinds of regularized auto encoders, we tried many things.",
    "start": "1328345",
    "end": "1333980"
  },
  {
    "text": "We also tried those kind of methods to learn good representations\nof images or video",
    "start": "1333980",
    "end": "1340750"
  },
  {
    "text": "that could then be used as input for example, an image\nclassification system.",
    "start": "1340767",
    "end": "1346613"
  },
  {
    "text": "And that also has basically failed. Like all the systems that\nattempt to predict missing parts",
    "start": "1347513",
    "end": "1353600"
  },
  {
    "text": "of an image or a video from a corrupted version of it, basically.",
    "start": "1353600",
    "end": "1360260"
  },
  {
    "text": "So, right, take an image or a video, corrupt it or transform it in some way, and then try to reconstruct\nthe complete video or image",
    "start": "1360260",
    "end": "1367520"
  },
  {
    "text": "from the corrupted version. And then hope that internally, the system will develop good\nrepresentations of images",
    "start": "1367520",
    "end": "1374900"
  },
  {
    "text": "that you can use for object recognition, segmentation, whatever it is. That has been essentially\na complete failure.",
    "start": "1374900",
    "end": "1381860"
  },
  {
    "text": "And it works really well for text. That's the principle that\nis used for LLMs, right? - So where's the failure exactly?",
    "start": "1382700",
    "end": "1388808"
  },
  {
    "text": "Is it that it is very difficult to form a good representation of an image,",
    "start": "1388808",
    "end": "1394243"
  },
  {
    "text": "like a good embedding of all the important\ninformation in the image?",
    "start": "1394243",
    "end": "1399350"
  },
  {
    "text": "Is it in terms of the consistency of image to image to image to\nimage that forms the video?",
    "start": "1399350",
    "end": "1404030"
  },
  {
    "text": "If we do a highlight reel\nof all the ways you failed. What's that look like? - Okay.",
    "start": "1406049",
    "end": "1411463"
  },
  {
    "text": "So the reason this doesn't work is... First of all, I have to tell\nyou exactly what doesn't work",
    "start": "1411463",
    "end": "1417230"
  },
  {
    "text": "because there is something\nelse that does work. So the thing that does not work is training the system to\nlearn representations of images",
    "start": "1417230",
    "end": "1426670"
  },
  {
    "text": "by training it to reconstruct a good image from a corrupted version of it.",
    "start": "1427760",
    "end": "1433610"
  },
  {
    "text": "Okay. That's what doesn't work. And we have a whole slew\nof techniques for this that are variant of then\nusing auto encoders.",
    "start": "1433610",
    "end": "1442490"
  },
  {
    "text": "Something called MAE, developed by some of\nmy colleagues at FAIR, masked autoencoder. So it's basically like the\nLLMs or things like this",
    "start": "1442490",
    "end": "1451818"
  },
  {
    "text": "where you train the\nsystem by corrupting text, except you corrupt images. You remove patches from it and you train a gigantic\nneural network to reconstruct.",
    "start": "1451818",
    "end": "1459470"
  },
  {
    "text": "The features you get are not good. And you know they're not good because if you now train\nthe same architecture,",
    "start": "1459470",
    "end": "1465500"
  },
  {
    "text": "but you train it to\nsupervise with label data, with textual descriptions\nof images, et cetera,",
    "start": "1465500",
    "end": "1474020"
  },
  {
    "text": "you do get good representations. And the performance on\nrecognition tasks is much better",
    "start": "1474020",
    "end": "1479690"
  },
  {
    "text": "than if you do this self\nsupervised free training. - So the architecture is good. - The architecture is good.",
    "start": "1479690",
    "end": "1485383"
  },
  {
    "text": "The architecture of the encoder is good. Okay? But the fact that you train the\nsystem to reconstruct images",
    "start": "1485383",
    "end": "1491390"
  },
  {
    "text": "does not lead it to produce long good generic features of images. - [Lex] When you train it\nin a self supervised way.",
    "start": "1491390",
    "end": "1498380"
  },
  {
    "text": "- Self supervised by reconstruction. - [Lex] Yeah, by reconstruction. - Okay, so what's the alternative? (both laugh)",
    "start": "1498380",
    "end": "1504380"
  },
  {
    "text": "The alternative is joint embedding. - What is joint embedding? What are these architectures\nthat you're so excited about?",
    "start": "1504380",
    "end": "1511250"
  },
  {
    "start": "1507000",
    "end": "1695000"
  },
  {
    "text": "- Okay, so now instead\nof training a system to encode the image and then training it to\nreconstruct the full image",
    "start": "1511250",
    "end": "1517762"
  },
  {
    "text": "from a corrupted version, you take the full image, you take the corrupted\nor transformed version,",
    "start": "1517762",
    "end": "1525410"
  },
  {
    "text": "you run them both through encoders, which in general are\nidentical but not necessarily.",
    "start": "1525410",
    "end": "1530813"
  },
  {
    "text": "And then you train a predictor\non top of those encoders",
    "start": "1531800",
    "end": "1536800"
  },
  {
    "text": "to predict the representation\nof the full input",
    "start": "1537474",
    "end": "1542474"
  },
  {
    "text": "from the representation\nof the corrupted one. Okay?",
    "start": "1542480",
    "end": "1547820"
  },
  {
    "text": "So joint embedding, because you're taking the full input and the corrupted version\nor transformed version,",
    "start": "1547820",
    "end": "1554180"
  },
  {
    "text": "run them both through encoders so you get a joint embedding. And then you're saying can I predict the\nrepresentation of the full one",
    "start": "1554180",
    "end": "1562040"
  },
  {
    "text": "from the representation\nof the corrupted one? Okay? And I call this a JEPA,",
    "start": "1562040",
    "end": "1567639"
  },
  {
    "text": "so that means joint embedding\npredictive architecture because there's joint embedding and there is this predictor that predicts the representation",
    "start": "1567640",
    "end": "1573410"
  },
  {
    "text": "of the good guy from the bad guy. And the big question is how do you train something like this?",
    "start": "1573410",
    "end": "1580730"
  },
  {
    "text": "And until five years ago or six years ago, we didn't have particularly good answers",
    "start": "1580730",
    "end": "1586340"
  },
  {
    "text": "for how you train those things, except for one called\ncontrastive learning.",
    "start": "1586340",
    "end": "1591923"
  },
  {
    "text": "And the idea of contrastive learning is you take a pair of images that are, again, an image\nand a corrupted version",
    "start": "1594890",
    "end": "1602480"
  },
  {
    "text": "or degraded version somehow or transformed version\nof the original one. And you train the predicted representation",
    "start": "1602480",
    "end": "1609950"
  },
  {
    "text": "to be the same as that. If you only do this, this system collapses. It basically completely ignores the input",
    "start": "1609950",
    "end": "1615710"
  },
  {
    "text": "and produces representations\nthat are constant. So the contrastive methods avoid this.",
    "start": "1615710",
    "end": "1622790"
  },
  {
    "text": "And those things have been\naround since the early '90s, I had a paper on this in 1993,",
    "start": "1622790",
    "end": "1627173"
  },
  {
    "text": "is you also show pairs of images\nthat you know are different",
    "start": "1628760",
    "end": "1633360"
  },
  {
    "text": "and then you push away the\nrepresentations from each other. So you say not only do\nrepresentations of things",
    "start": "1634460",
    "end": "1640430"
  },
  {
    "text": "that we know are the same, should be the same or should be similar, but representation of things\nthat we know are different",
    "start": "1640430",
    "end": "1645769"
  },
  {
    "text": "should be different. And that prevents the collapse, but it has some limitation. And there's a whole bunch of techniques",
    "start": "1645770",
    "end": "1651890"
  },
  {
    "text": "that have appeared over\nthe last six, seven years that can revive this type of method.",
    "start": "1651890",
    "end": "1658637"
  },
  {
    "text": "Some of them from FAIR, some of them from Google and other places.",
    "start": "1658637",
    "end": "1664002"
  },
  {
    "text": "But there are limitations to\nthose contrastive methods. What has changed in the\nlast three, four years",
    "start": "1664002",
    "end": "1671930"
  },
  {
    "text": "is now we have methods\nthat are non-contrastive. So they don't require those\nnegative contrastive samples",
    "start": "1671930",
    "end": "1679010"
  },
  {
    "text": "of images that we know are different. You train them only with images",
    "start": "1679010",
    "end": "1684350"
  },
  {
    "text": "that are different versions or different views of the same thing. And you rely on some other tweaks",
    "start": "1684350",
    "end": "1690770"
  },
  {
    "text": "to prevent the system from collapsing. And we have half a dozen\ndifferent methods for this now.",
    "start": "1690770",
    "end": "1696019"
  },
  {
    "start": "1695000",
    "end": "2251000"
  },
  {
    "text": "- So what is the fundamental difference between joint embedding\narchitectures and LLMs?",
    "start": "1696020",
    "end": "1702379"
  },
  {
    "text": "So can JEPA take us to AGI? Whether we should say that\nyou don't like the term AGI",
    "start": "1702380",
    "end": "1711860"
  },
  {
    "text": "and we'll probably argue, I think every single\ntime I've talked to you we've argued about the G in AGI.",
    "start": "1711860",
    "end": "1716870"
  },
  {
    "text": "- [Yann] Yes. - I get it, I get it, I get it. (laughing) Well we'll probably\ncontinue to argue about it.",
    "start": "1716870",
    "end": "1722360"
  },
  {
    "text": "It's great. Because you're like French,",
    "start": "1722360",
    "end": "1728111"
  },
  {
    "text": "and ami is I guess friend in French- - [Yann] Yes. - And AMI stands for advanced\nmachine intelligence-",
    "start": "1728111",
    "end": "1735890"
  },
  {
    "text": "- [Yann] Right. - But either way, can\nJEPA take us to that, towards that advanced\nmachine intelligence?",
    "start": "1735890",
    "end": "1742640"
  },
  {
    "text": "- Well, so it's a first step. Okay? So first of all, what's the difference with generative architectures like LLMs?",
    "start": "1742640",
    "end": "1750835"
  },
  {
    "text": "So LLMs or vision systems that\nare trained by reconstruction",
    "start": "1750835",
    "end": "1755835"
  },
  {
    "text": "generate the inputs, right? They generate the original input",
    "start": "1757670",
    "end": "1762950"
  },
  {
    "text": "that is non-corrupted,\nnon-transformed, right? So you have to predict all the pixels.",
    "start": "1762950",
    "end": "1768922"
  },
  {
    "text": "And there is a huge amount of\nresources spent in the system to actually predict all those\npixels, all the details.",
    "start": "1769970",
    "end": "1776809"
  },
  {
    "text": "In a JEPA, you're not trying\nto predict all the pixels, you're only trying to predict",
    "start": "1776809",
    "end": "1782330"
  },
  {
    "text": "an abstract representation\nof the inputs, right? And that's much easier in many ways.",
    "start": "1782330",
    "end": "1789500"
  },
  {
    "text": "So what the JEPA system when it's being trained is trying to do, is extract as much information\nas possible from the input,",
    "start": "1789500",
    "end": "1796159"
  },
  {
    "text": "but yet only extract information that is relatively easily predictable.",
    "start": "1796160",
    "end": "1800543"
  },
  {
    "text": "Okay. So there's a lot of things in the world that we cannot predict. Like for example, if you\nhave a self driving car",
    "start": "1801740",
    "end": "1807274"
  },
  {
    "text": "driving down the street or road. There may be trees around the road.",
    "start": "1807274",
    "end": "1813410"
  },
  {
    "text": "And it could be a windy day, so the leaves on the\ntree are kind of moving in kind of semi chaotic random ways",
    "start": "1813410",
    "end": "1819650"
  },
  {
    "text": "that you can't predict and you don't care, you don't want to predict. So what you want is your encoder",
    "start": "1819650",
    "end": "1825350"
  },
  {
    "text": "to basically eliminate all those details. It'll tell you there's moving leaves, but it's not gonna keep the details of exactly what's going on.",
    "start": "1825350",
    "end": "1832048"
  },
  {
    "text": "And so when you do the prediction\nin representation space, you're not going to have to predict",
    "start": "1832048",
    "end": "1837289"
  },
  {
    "text": "every single pixel of every leaf. And that not only is a lot simpler,",
    "start": "1837290",
    "end": "1843590"
  },
  {
    "text": "but also it allows the system to essentially learn an abstract\nrepresentation of the world",
    "start": "1843590",
    "end": "1849338"
  },
  {
    "text": "where what can be modeled\nand predicted is preserved",
    "start": "1849338",
    "end": "1854338"
  },
  {
    "text": "and the rest is viewed as noise and eliminated by the encoder. So it kind of lifts the\nlevel of abstraction",
    "start": "1854720",
    "end": "1860890"
  },
  {
    "text": "of the representation. If you think about this, this is something we do\nabsolutely all the time. Whenever we describe a phenomenon,",
    "start": "1860890",
    "end": "1867020"
  },
  {
    "text": "we describe it at a particular\nlevel of abstraction. And we don't always describe\nevery natural phenomenon",
    "start": "1867020",
    "end": "1873470"
  },
  {
    "text": "in terms of quantum field theory, right? That would be impossible, right? So we have multiple levels of abstraction",
    "start": "1873470",
    "end": "1879723"
  },
  {
    "text": "to describe what happens in the world. Starting from quantum field theory to like atomic theory and\nmolecules in chemistry,",
    "start": "1879723",
    "end": "1887840"
  },
  {
    "text": "materials, all the way up to kind of\nconcrete objects in the real world",
    "start": "1887840",
    "end": "1893929"
  },
  {
    "text": "and things like that. So we can't just only model\neverything at the lowest level.",
    "start": "1893930",
    "end": "1899620"
  },
  {
    "text": "And that's what the idea\nof JEPA is really about. Learn abstract representation\nin a self supervised manner.",
    "start": "1900470",
    "end": "1909383"
  },
  {
    "text": "And you can do it hierarchically as well. So that I think is an essential component",
    "start": "1909384",
    "end": "1914510"
  },
  {
    "text": "of an intelligent system. And in language, we can\nget away without doing this because language is already\nto some level abstract",
    "start": "1914510",
    "end": "1922610"
  },
  {
    "text": "and already has eliminated\na lot of information that is not predictable. And so we can get away without\ndoing the joint embedding,",
    "start": "1922610",
    "end": "1931040"
  },
  {
    "text": "without lifting the abstraction level and by directly predicting words.",
    "start": "1931040",
    "end": "1935453"
  },
  {
    "text": "- So joint embedding. It's still generative, but it's generative in this\nabstract representation space.",
    "start": "1936380",
    "end": "1943340"
  },
  {
    "text": "- [Yann] Yeah. - And you're saying language, we were lazy with language 'cause we already got the\nabstract representation for free",
    "start": "1943340",
    "end": "1950420"
  },
  {
    "text": "and now we have to zoom out, actually think about\ngenerally intelligent systems, we have to deal with the full mess",
    "start": "1950420",
    "end": "1957828"
  },
  {
    "text": "of physical of reality, of reality. And you do have to do this step",
    "start": "1957828",
    "end": "1962930"
  },
  {
    "text": "of jumping from the full,\nrich, detailed reality",
    "start": "1962930",
    "end": "1967730"
  },
  {
    "text": "to an abstract representation\nof that reality based on what you can then reason",
    "start": "1970821",
    "end": "1976400"
  },
  {
    "text": "and all that kind of stuff. - Right. And the thing is those\nself supervised algorithms that learn by prediction,",
    "start": "1976400",
    "end": "1982343"
  },
  {
    "text": "even in representation space, they learn more concept",
    "start": "1983210",
    "end": "1989270"
  },
  {
    "text": "if the input data you feed\nthem is more redundant. The more redundancy there is in the data, the more they're able to capture",
    "start": "1989270",
    "end": "1995539"
  },
  {
    "text": "some internal structure of it. And so there, there is way more\nredundancy in the structure",
    "start": "1995540",
    "end": "2001159"
  },
  {
    "text": "in perceptual inputs,\nsensory input like vision, than there is in text,",
    "start": "2002291",
    "end": "2008500"
  },
  {
    "text": "which is not nearly as redundant. This is back to the\nquestion you were asking a few minutes ago. Language might represent\nmore information really",
    "start": "2008500",
    "end": "2015520"
  },
  {
    "text": "because it's already compressed, you're right about that. But that means it's also less redundant. And so self supervised\nonly will not work as well.",
    "start": "2015520",
    "end": "2023679"
  },
  {
    "text": "- Is it possible to join the self supervised\ntraining on visual data",
    "start": "2023680",
    "end": "2029981"
  },
  {
    "text": "and self supervised\ntraining on language data? There is a huge amount of knowledge",
    "start": "2029981",
    "end": "2036520"
  },
  {
    "text": "even though you talk down about\nthose 10 to the 13 tokens. Those 10 to the 13 tokens",
    "start": "2036520",
    "end": "2041890"
  },
  {
    "text": "represent the entirety, a large fraction of what\nus humans have figured out.",
    "start": "2041890",
    "end": "2048252"
  },
  {
    "text": "Both the shit talk on Reddit and the contents of all\nthe books and the articles and the full spectrum of\nhuman intellectual creation.",
    "start": "2049210",
    "end": "2058533"
  },
  {
    "text": "So is it possible to\njoin those two together? - Well, eventually, yes,",
    "start": "2058534",
    "end": "2063730"
  },
  {
    "text": "but I think if we do this too early, we run the risk of being tempted to cheat.",
    "start": "2063730",
    "end": "2070330"
  },
  {
    "text": "And in fact, that's what\npeople are doing at the moment with vision language model. We're basically cheating. We are using language as a crutch",
    "start": "2070330",
    "end": "2078220"
  },
  {
    "text": "to help the deficiencies\nof our vision systems to kind of learn good representations\nfrom images and video.",
    "start": "2078220",
    "end": "2086470"
  },
  {
    "text": "And the problem with this is that we might improve our\nvision language system a bit,",
    "start": "2086470",
    "end": "2092840"
  },
  {
    "text": "I mean our language models\nby feeding them images. But we're not gonna get to the level",
    "start": "2093790",
    "end": "2099520"
  },
  {
    "text": "of even the intelligence or level of understanding of the world of a cat or a dog which\ndoesn't have language.",
    "start": "2099520",
    "end": "2106663"
  },
  {
    "text": "They don't have language and they understand the world\nmuch better than any LLM. They can plan really complex actions",
    "start": "2107590",
    "end": "2114140"
  },
  {
    "text": "and sort of imagine the\nresult of a bunch of actions. How do we get machines to learn that",
    "start": "2115000",
    "end": "2120430"
  },
  {
    "text": "before we combine that with language? Obviously, if we combine\nthis with language, this is gonna be a winner,",
    "start": "2120430",
    "end": "2126193"
  },
  {
    "text": "but before that we have to focus on like how do we get systems\nto learn how the world works? - So this kind of joint embedding\npredictive architecture,",
    "start": "2128410",
    "end": "2137209"
  },
  {
    "text": "for you, that's gonna be able to learn something like common sense, something like what a cat uses",
    "start": "2137209",
    "end": "2143440"
  },
  {
    "text": "to predict how to mess with\nits owner most optimally by knocking over a thing.",
    "start": "2143440",
    "end": "2149499"
  },
  {
    "text": "- That's the hope. In fact, the techniques we're\nusing are non-contrastive.",
    "start": "2149499",
    "end": "2154615"
  },
  {
    "text": "So not only is the\narchitecture non-generative, the learning procedures we're\nusing are non-contrastive.",
    "start": "2154615",
    "end": "2160750"
  },
  {
    "text": "We have two sets of techniques. One set is based on distillation and there's a number of methods\nthat use this principle.",
    "start": "2160750",
    "end": "2170104"
  },
  {
    "text": "One by DeepMind called BYOL. A couple by FAIR, one called VICReg and\nanother one called I-JEPA.",
    "start": "2170260",
    "end": "2179730"
  },
  {
    "text": "And VICReg, I should say, is not a distillation method actually, but I-JEPA and BYOL certainly are.",
    "start": "2180070",
    "end": "2185620"
  },
  {
    "text": "And there's another one\nalso called DINO or Dino, also produced at FAIR.",
    "start": "2185620",
    "end": "2191740"
  },
  {
    "text": "And the idea of those things is that you take the full\ninput, let's say an image. You run it through an encoder,",
    "start": "2191740",
    "end": "2197773"
  },
  {
    "text": "produces a representation. And then you corrupt that\ninput or transform it, run it through essentially what\namounts to the same encoder",
    "start": "2198814",
    "end": "2206500"
  },
  {
    "text": "with some minor differences. And then train a predictor. Sometimes a predictor is very simple,",
    "start": "2206500",
    "end": "2211869"
  },
  {
    "text": "sometimes it doesn't exist. But train a predictor to\npredict a representation of the first uncorrupted input\nfrom the corrupted input.",
    "start": "2211870",
    "end": "2220230"
  },
  {
    "text": "But you only train the second branch. You only train the part of the network",
    "start": "2222070",
    "end": "2227500"
  },
  {
    "text": "that is fed with the corrupted input. The other network, you don't train.",
    "start": "2227500",
    "end": "2232780"
  },
  {
    "text": "But since they share the same weight, when you modify the first one, it also modifies the second one.",
    "start": "2232780",
    "end": "2238349"
  },
  {
    "text": "And with various tricks, you can prevent the system from collapsing with the collapse of the\ntype I was explaining before",
    "start": "2238350",
    "end": "2244147"
  },
  {
    "text": "where the system basically\nignores the input. So that works very well.",
    "start": "2244147",
    "end": "2251049"
  },
  {
    "start": "2251000",
    "end": "2331000"
  },
  {
    "text": "The two techniques\nwe've developed at FAIR, DINO and I-JEPA work really well for that.",
    "start": "2251050",
    "end": "2258724"
  },
  {
    "text": "- So what kind of data\nare we talking about here? - So there's several scenarios. One scenario is you take an image,",
    "start": "2259270",
    "end": "2267279"
  },
  {
    "text": "you corrupt it by changing\nthe cropping, for example,",
    "start": "2267280",
    "end": "2272280"
  },
  {
    "text": "changing the size a little bit, maybe changing the\norientation, blurring it, changing the colors,",
    "start": "2272735",
    "end": "2278230"
  },
  {
    "text": "doing all kinds of horrible things to it- - But basic horrible things. - Basic horrible things that sort of degrade\nthe quality a little bit",
    "start": "2278230",
    "end": "2284200"
  },
  {
    "text": "and change the framing, crop the image.",
    "start": "2284200",
    "end": "2288313"
  },
  {
    "text": "And in some cases, in the case of I-JEPA, you don't need to do any of this, you just mask some parts of it, right?",
    "start": "2289703",
    "end": "2296137"
  },
  {
    "text": "You just basically remove some regions like a big block, essentially.",
    "start": "2296137",
    "end": "2301840"
  },
  {
    "text": "And then run through the encoders and train the entire system, encoder and predictor,",
    "start": "2301840",
    "end": "2307630"
  },
  {
    "text": "to predict the representation\nof the good one from the representation\nof the corrupted one.",
    "start": "2307630",
    "end": "2311740"
  },
  {
    "text": "So that's the I-JEPA. It doesn't need to know that\nit's an image, for example, because the only thing it needs to know",
    "start": "2313630",
    "end": "2319540"
  },
  {
    "text": "is how to do this masking. Whereas with DINO, you need to know it's an image because you need to do things",
    "start": "2319540",
    "end": "2325420"
  },
  {
    "text": "like geometry transformation and blurring and things like that that\nare really image specific.",
    "start": "2325420",
    "end": "2331599"
  },
  {
    "start": "2331000",
    "end": "2662000"
  },
  {
    "text": "A more recent version of this\nthat we have is called V-JEPA. So it's basically the same idea as I-JEPA",
    "start": "2331600",
    "end": "2336820"
  },
  {
    "text": "except it's applied to video. So now you take a whole video and you mask a whole chunk of it.",
    "start": "2336820",
    "end": "2342670"
  },
  {
    "text": "And what we mask is actually\nkind of a temporal tube. So like a whole segment\nof each frame in the video",
    "start": "2342670",
    "end": "2347941"
  },
  {
    "text": "over the entire video. - And that tube is like\nstatically positioned throughout the frames?",
    "start": "2347941",
    "end": "2354130"
  },
  {
    "text": "It's literally just a straight tube? - Throughout the tube, yeah. Typically it's 16 frames or something, and we mask the same region\nover the entire 16 frames.",
    "start": "2354130",
    "end": "2362350"
  },
  {
    "text": "It's a different one for\nevery video, obviously. And then again, train that system",
    "start": "2362350",
    "end": "2368530"
  },
  {
    "text": "so as to predict the\nrepresentation of the full video from the partially masked video.",
    "start": "2368530",
    "end": "2374034"
  },
  {
    "text": "And that works really well. It's the first system that we have that learns good representations of video",
    "start": "2374034",
    "end": "2379930"
  },
  {
    "text": "so that when you feed\nthose representations to a supervised classifier head,",
    "start": "2379930",
    "end": "2384940"
  },
  {
    "text": "it can tell you what action\nis taking place in the video with pretty good accuracy.",
    "start": "2384940",
    "end": "2389773"
  },
  {
    "text": "So it's the first time we get\nsomething of that quality. - So that's a good test",
    "start": "2391120",
    "end": "2397030"
  },
  {
    "text": "that a good representation is formed. That means there's something to this. - Yeah. We also preliminary result",
    "start": "2397030",
    "end": "2403450"
  },
  {
    "text": "that seem to indicate that the representation\nallows our system to tell",
    "start": "2403450",
    "end": "2409510"
  },
  {
    "text": "whether the video is physically possible or completely impossible because some object disappeared",
    "start": "2409510",
    "end": "2415329"
  },
  {
    "text": "or an object suddenly jumped\nfrom one location to another or changed shape or something.",
    "start": "2415330",
    "end": "2421840"
  },
  {
    "text": "- So it's able to capture\nsome physics based constraints",
    "start": "2421840",
    "end": "2426840"
  },
  {
    "text": "about the reality\nrepresented in the video? - [Yann] Yeah. - About the appearance and\nthe disappearance of objects?",
    "start": "2427420",
    "end": "2432599"
  },
  {
    "text": "- Yeah. That's really new. - Okay, but can this actually",
    "start": "2432600",
    "end": "2438260"
  },
  {
    "text": "get us to this kind of world model that understands enough about the world",
    "start": "2440140",
    "end": "2446200"
  },
  {
    "text": "to be able to drive a car? - Possibly. And this is gonna take a while",
    "start": "2446200",
    "end": "2451540"
  },
  {
    "text": "before we get to that point. And there are systems\nalready, robotic systems,",
    "start": "2451540",
    "end": "2456880"
  },
  {
    "text": "that are based on this idea. What you need for this",
    "start": "2456880",
    "end": "2462700"
  },
  {
    "text": "is a slightly modified version of this where imagine that you have a video,",
    "start": "2462700",
    "end": "2469623"
  },
  {
    "text": "a complete video, and what you're doing to this video is that you are either\ntranslating it in time",
    "start": "2471010",
    "end": "2477609"
  },
  {
    "text": "towards the future. So you'll only see the\nbeginning of the video, but you don't see the latter part of it that is in the original one.",
    "start": "2477610",
    "end": "2483373"
  },
  {
    "text": "Or you just mask the second\nhalf of the video, for example. And then you train this I-JEPA system",
    "start": "2484300",
    "end": "2490830"
  },
  {
    "text": "or the type I described, to predict representation\nof the full video from the shifted one.",
    "start": "2490830",
    "end": "2496180"
  },
  {
    "text": "But you also feed the\npredictor with an action. For example, the wheel is turned",
    "start": "2496180",
    "end": "2502450"
  },
  {
    "text": "10 degrees to the right\nor something, right? So if it's a dash cam in a car",
    "start": "2502450",
    "end": "2509920"
  },
  {
    "text": "and you know the angle of the wheel, you should be able to\npredict to some extent what's going to happen to what you see.",
    "start": "2509920",
    "end": "2516853"
  },
  {
    "text": "You're not gonna be able\nto predict all the details of objects that appear\nin the view, obviously,",
    "start": "2517779",
    "end": "2522819"
  },
  {
    "text": "but at an abstract representation level, you can probably predict\nwhat's gonna happen.",
    "start": "2522820",
    "end": "2528700"
  },
  {
    "text": "So now what you have is an internal model that says, here is my idea of the state of the world at time T,",
    "start": "2528700",
    "end": "2535270"
  },
  {
    "text": "here is an action I'm taking, here is a prediction of the state of the\nworld at time T plus one,",
    "start": "2535270",
    "end": "2540550"
  },
  {
    "text": "T plus delta T, T plus two seconds, whatever it is. If you have a model of this type,",
    "start": "2540550",
    "end": "2546220"
  },
  {
    "text": "you can use it for planning. So now you can do what LLMs cannot do,",
    "start": "2546220",
    "end": "2551560"
  },
  {
    "text": "which is planning what you're gonna do so as you arrive at a particular outcome",
    "start": "2551560",
    "end": "2557620"
  },
  {
    "text": "or satisfy a particular objective, right? So you can have a number\nof objectives, right?",
    "start": "2557620",
    "end": "2564865"
  },
  {
    "text": "I can predict that if I have\nan object like this, right?",
    "start": "2564865",
    "end": "2569865"
  },
  {
    "text": "And I open my hand, it's gonna fall, right? And if I push it with a\nparticular force on the table,",
    "start": "2570860",
    "end": "2577716"
  },
  {
    "text": "it's gonna move. If I push the table itself, it's probably not gonna\nmove with the same force.",
    "start": "2577716",
    "end": "2583331"
  },
  {
    "text": "So we have this internal model\nof the world in our mind,",
    "start": "2583331",
    "end": "2587860"
  },
  {
    "text": "which allows us to plan\nsequences of actions to arrive at a particular goal. And so now if you have this world model,",
    "start": "2589211",
    "end": "2598569"
  },
  {
    "text": "we can imagine a sequence of actions, predict what the outcome of the sequence of action is going to be,",
    "start": "2598570",
    "end": "2605260"
  },
  {
    "text": "measure to what extent the final state satisfies a particular objective",
    "start": "2605260",
    "end": "2610960"
  },
  {
    "text": "like moving the bottle\nto the left of the table. And then plan a sequence of actions",
    "start": "2610960",
    "end": "2618430"
  },
  {
    "text": "that will minimize this\nobjective at runtime. We're not talking about learning, we're talking about inference time, right?",
    "start": "2618430",
    "end": "2624977"
  },
  {
    "text": "So this is planning, really. And in optimal control, this is a very classical thing. It's called model predictive control.",
    "start": "2624977",
    "end": "2630550"
  },
  {
    "text": "You have a model of the\nsystem you want to control that can predict the sequence of states",
    "start": "2630550",
    "end": "2635882"
  },
  {
    "text": "corresponding to a sequence of commands. And you are planning\na sequence of commands",
    "start": "2635882",
    "end": "2642250"
  },
  {
    "text": "so that according to your world model, the end state of the system will satisfy any objectives that you fix.",
    "start": "2642250",
    "end": "2650622"
  },
  {
    "text": "This is the way rocket\ntrajectories have been planned",
    "start": "2650622",
    "end": "2655622"
  },
  {
    "text": "since computers have been around. So since the early '60s, essentially. - So yes, for a model predictive control,",
    "start": "2656440",
    "end": "2661840"
  },
  {
    "text": "but you also often talk\nabout hierarchical planning. - [Yann] Yeah.",
    "start": "2661840",
    "end": "2666843"
  },
  {
    "start": "2662000",
    "end": "3040000"
  },
  {
    "text": "- Can hierarchical planning\nemerge from this somehow? - Well, so no. You will have to build\na specific architecture",
    "start": "2666843",
    "end": "2672229"
  },
  {
    "text": "to allow for hierarchical planning. So hierarchical planning\nis absolutely necessary if you want to plan complex actions.",
    "start": "2672229",
    "end": "2679513"
  },
  {
    "text": "If I wanna go from, let's\nsay, from New York to Paris, this the example I use all the time. And I'm sitting in my office at NYU.",
    "start": "2680650",
    "end": "2688150"
  },
  {
    "text": "My objective that I need to minimize is my distance to Paris. At a high level, a very abstract\nrepresentation of my location,",
    "start": "2688150",
    "end": "2697390"
  },
  {
    "text": "I would have to decompose\nthis into two sub-goals. First one is go to the airport, second one is catch a plane to Paris.",
    "start": "2697390",
    "end": "2704680"
  },
  {
    "text": "Okay. So my sub-goal is now\ngoing to the airport. My objective function is\nmy distance to the airport.",
    "start": "2704680",
    "end": "2711750"
  },
  {
    "text": "How do I go to the airport? Well, I have to go in the\nstreet and hail a taxi,",
    "start": "2712660",
    "end": "2718299"
  },
  {
    "text": "which you can do in New York. Okay, now I have another sub-goal. Go down on the street.",
    "start": "2718300",
    "end": "2724724"
  },
  {
    "text": "Well, that means going to the elevator, going down the elevator, walk out to the street.",
    "start": "2724724",
    "end": "2730010"
  },
  {
    "text": "How do I go to the elevator? I have to stand up from my chair,",
    "start": "2730930",
    "end": "2736359"
  },
  {
    "text": "open the door of my office, go to the elevator, push the button. How do I get up for my chair?",
    "start": "2736360",
    "end": "2742329"
  },
  {
    "text": "Like you can imagine going\ndown all the way down to basically what amounts",
    "start": "2742330",
    "end": "2747400"
  },
  {
    "text": "to millisecond by\nmillisecond muscle control. Okay? And obviously you're not\ngoing to plan your entire trip",
    "start": "2747400",
    "end": "2755020"
  },
  {
    "text": "from New York to Paris in terms of millisecond by\nmillisecond muscle control.",
    "start": "2755020",
    "end": "2760300"
  },
  {
    "text": "First, that would be incredibly expensive, but it will also be completely impossible because you don't know all the conditions",
    "start": "2760300",
    "end": "2766480"
  },
  {
    "text": "of what's gonna happen. How long it's gonna take to catch a taxi",
    "start": "2766480",
    "end": "2770450"
  },
  {
    "text": "or to go to the airport with traffic. I mean, you would have to know exactly",
    "start": "2771761",
    "end": "2776920"
  },
  {
    "text": "the condition of everything to be able to do this planning, and you don't have the information. So you have to do this\nhierarchical planning",
    "start": "2776920",
    "end": "2783970"
  },
  {
    "text": "so that you can start acting and then sort of re-planning as you go. And nobody really knows\nhow to do this in AI.",
    "start": "2783970",
    "end": "2792013"
  },
  {
    "text": "Nobody knows how to train a system to learn the appropriate\nmultiple levels of representation so that hierarchical planning works.",
    "start": "2793390",
    "end": "2801309"
  },
  {
    "text": "- Does something like that already emerge? So like can you use an LLM,",
    "start": "2801310",
    "end": "2805453"
  },
  {
    "text": "state-of-the-art LLM, to get you from New York to Paris by doing exactly the kind of detailed",
    "start": "2806740",
    "end": "2814240"
  },
  {
    "text": "set of questions that you just did? Which is can you give me a\nlist of 10 steps I need to do",
    "start": "2814240",
    "end": "2821230"
  },
  {
    "text": "to get from New York to Paris? And then for each of those steps, can you give me a list of 10 steps",
    "start": "2821230",
    "end": "2827200"
  },
  {
    "text": "how I make that step happen? And for each of those steps, can you give me a list of 10 steps",
    "start": "2827200",
    "end": "2832240"
  },
  {
    "text": "to make each one of those, until you're moving\nyour individual muscles? Maybe not.",
    "start": "2832240",
    "end": "2837970"
  },
  {
    "text": "Whatever you can actually act upon using your own mind. - Right. So there's a lot of questions",
    "start": "2837970",
    "end": "2843250"
  },
  {
    "text": "that are also implied by this, right? So the first thing is LLMs\nwill be able to answer some of those questions",
    "start": "2843250",
    "end": "2848770"
  },
  {
    "text": "down to some level of abstraction. Under the condition that\nthey've been trained",
    "start": "2848770",
    "end": "2854530"
  },
  {
    "text": "with similar scenarios\nin their training set. - They would be able to\nanswer all of those questions.",
    "start": "2854530",
    "end": "2860140"
  },
  {
    "text": "But some of them may be hallucinated, meaning non-factual. - Yeah, true.",
    "start": "2860140",
    "end": "2865143"
  },
  {
    "text": "I mean they'll probably\nproduce some answer. Except they're not gonna be able to really kind of produce millisecond by millisecond muscle control",
    "start": "2865143",
    "end": "2870280"
  },
  {
    "text": "of how you stand up\nfrom your chair, right? But down to some level of abstraction where you can describe things by words,",
    "start": "2870280",
    "end": "2877809"
  },
  {
    "text": "they might be able to give you a plan, but only under the condition\nthat they've been trained to produce those kind of plans, right?",
    "start": "2877810",
    "end": "2884109"
  },
  {
    "text": "They're not gonna be able\nto plan for situations they never encountered before.",
    "start": "2884110",
    "end": "2889360"
  },
  {
    "text": "They basically are going to\nhave to regurgitate the template that they've been trained on. - But where, just for the\nexample of New York to Paris,",
    "start": "2889360",
    "end": "2895630"
  },
  {
    "text": "is it gonna start getting into trouble? Like at which layer of abstraction do you think you'll start?",
    "start": "2895631",
    "end": "2902560"
  },
  {
    "text": "Because like I can imagine almost every single part of that, an LLM will be able to\nanswer somewhat accurately,",
    "start": "2902560",
    "end": "2907750"
  },
  {
    "text": "especially when you're talking\nabout New York and Paris, major cities. - So I mean certainly an LLM",
    "start": "2907750",
    "end": "2913420"
  },
  {
    "text": "would be able to solve that problem if you fine tune it for it. - [Lex] Sure. - And so I can't say that\nan LLM cannot do this,",
    "start": "2913420",
    "end": "2922130"
  },
  {
    "text": "it can't do this if you train it for it, there's no question, down to a certain level",
    "start": "2922420",
    "end": "2927789"
  },
  {
    "text": "where things can be\nformulated in terms of words. But like if you wanna go down to like how do you climb down the stairs",
    "start": "2927790",
    "end": "2934443"
  },
  {
    "text": "or just stand up from your\nchair in terms of words, like you can't do it.",
    "start": "2934444",
    "end": "2939343"
  },
  {
    "text": "That's one of the reasons you need experience of the physical world,",
    "start": "2940747",
    "end": "2946210"
  },
  {
    "text": "which is much higher bandwidth than what you can express in words, in human language. - So everything we've been talking about",
    "start": "2946210",
    "end": "2952480"
  },
  {
    "text": "on the joint embedding space, is it possible that that's what we need for like the interaction\nwith physical reality",
    "start": "2952480",
    "end": "2958579"
  },
  {
    "text": "on the robotics front? And then just the LLMs are the\nthing that sits on top of it",
    "start": "2958579",
    "end": "2964290"
  },
  {
    "text": "for the bigger reasoning about like the fact that I\nneed to book a plane ticket",
    "start": "2964290",
    "end": "2970720"
  },
  {
    "text": "and I need to know know how to\ngo to the websites and so on. - Sure. And a lot of plans that people know about",
    "start": "2970720",
    "end": "2977210"
  },
  {
    "text": "that are relatively high\nlevel are actually learned. Most people don't invent\nthe plans by themselves.",
    "start": "2977210",
    "end": "2986409"
  },
  {
    "text": "We have some ability to do\nthis, of course, obviously, but most plans that people use",
    "start": "2990956",
    "end": "2997420"
  },
  {
    "text": "are plans that have been trained on. Like they've seen other\npeople use those plans or they've been told\nhow to do things, right?",
    "start": "2997420",
    "end": "3004170"
  },
  {
    "text": "That you can't invent how\nyou like take a person who's never heard of airplanes",
    "start": "3004170",
    "end": "3009270"
  },
  {
    "text": "and tell them like, how do\nyou go from New York to Paris? They're probably not going to be able to kind of deconstruct the whole plan",
    "start": "3009270",
    "end": "3016170"
  },
  {
    "text": "unless they've seen\nexamples of that before. So certainly LLMs are\ngonna be able to do this.",
    "start": "3016170",
    "end": "3021210"
  },
  {
    "text": "But then how you link this\nfrom the low level of actions,",
    "start": "3021210",
    "end": "3026210"
  },
  {
    "text": "that needs to be done\nwith things like JEPA, that basically lift the abstraction level",
    "start": "3028242",
    "end": "3033810"
  },
  {
    "text": "of the representation without attempting to reconstruct every detail of the situation. That's why we need JEPAs for.",
    "start": "3033810",
    "end": "3039693"
  },
  {
    "start": "3040000",
    "end": "3966000"
  },
  {
    "text": "- I would love to sort of\nlinger on your skepticism around autoregressive LLMs.",
    "start": "3040800",
    "end": "3048450"
  },
  {
    "text": "So one way I would like\nto test that skepticism is everything you say makes a lot of sense,",
    "start": "3048450",
    "end": "3055100"
  },
  {
    "text": "but if I apply everything\nyou said today and in general to like, I don't know,",
    "start": "3057510",
    "end": "3063450"
  },
  {
    "text": "10 years ago, maybe a little bit less. No, let's say three years ago. I wouldn't be able to\npredict the success of LLMs.",
    "start": "3063450",
    "end": "3072660"
  },
  {
    "text": "So does it make sense to you that autoregressive LLMs\nare able to be so damn good?",
    "start": "3072660",
    "end": "3079682"
  },
  {
    "text": "- [Yann] Yes. - Can you explain your intuition? Because if I were to take\nyour wisdom and intuition",
    "start": "3080610",
    "end": "3089160"
  },
  {
    "text": "at face value, I would say there's no\nway autoregressive LLMs one token at a time,",
    "start": "3089160",
    "end": "3094349"
  },
  {
    "text": "would be able to do the kind\nof things they're doing. - No, there's one thing\nthat autoregressive LLMs or that LLMs in general, not\njust the autoregressive ones,",
    "start": "3094350",
    "end": "3102450"
  },
  {
    "text": "but including the BERT\nstyle bidirectional ones, are exploiting and its\nself supervised running.",
    "start": "3102450",
    "end": "3109259"
  },
  {
    "text": "And I've been a very, very strong advocate of self supervised running for many years. So those things are an incredibly\nimpressive demonstration",
    "start": "3109260",
    "end": "3118340"
  },
  {
    "text": "that self supervised\nlearning actually works. The idea that started...",
    "start": "3118620",
    "end": "3124920"
  },
  {
    "text": "It didn't start with BERT, but it was really kind of a\ngood demonstration with this. So the idea that you take a\npiece of text, you corrupt it,",
    "start": "3124920",
    "end": "3134609"
  },
  {
    "text": "and then you train some\ngigantic neural net to reconstruct the parts that are missing. That has been an enormous...",
    "start": "3134610",
    "end": "3141050"
  },
  {
    "text": "Produced an enormous amount of benefits. It allowed us to create systems\nthat understand language,",
    "start": "3143550",
    "end": "3150620"
  },
  {
    "text": "systems that can translate hundreds of languages in any direction,",
    "start": "3151350",
    "end": "3156510"
  },
  {
    "text": "systems that are multilingual. It's a single system that can be trained to\nunderstand hundreds of languages",
    "start": "3156510",
    "end": "3163200"
  },
  {
    "text": "and translate in any direction and produce summaries",
    "start": "3163200",
    "end": "3168390"
  },
  {
    "text": "and then answer questions\nand produce text. And then there's a special case of it,",
    "start": "3168390",
    "end": "3173663"
  },
  {
    "text": "which is the autoregressive trick where you constrain the system to not elaborate a\nrepresentation of the text",
    "start": "3174632",
    "end": "3182010"
  },
  {
    "text": "from looking at the entire text, but only predicting a word from the words that have come before.",
    "start": "3182010",
    "end": "3188280"
  },
  {
    "text": "Right? And you do this by constraining the\narchitecture of the network. And that's what you can build\nan autoregressive LLM from.",
    "start": "3188280",
    "end": "3195120"
  },
  {
    "text": "So there was a surprise many years ago with what's called decoder only LLM.",
    "start": "3195120",
    "end": "3200910"
  },
  {
    "text": "So systems of this type that are just trying to produce\nwords from the previous one.",
    "start": "3200910",
    "end": "3207502"
  },
  {
    "text": "And the fact that when you scale them up, they tend to really kind of\nunderstand more about language.",
    "start": "3208080",
    "end": "3216230"
  },
  {
    "text": "When you train them on lots of data, you make them really big. That was kind of a surprise. And that surprise occurred\nquite a while back.",
    "start": "3216870",
    "end": "3222869"
  },
  {
    "text": "Like with work from Google,\nMeta, OpenAI, et cetera,",
    "start": "3222870",
    "end": "3227870"
  },
  {
    "text": "going back to the GPT kind of general pre-trained transformers.",
    "start": "3230219",
    "end": "3236790"
  },
  {
    "text": "- You mean like GPT-2? Like there's a certain place where you start to realize scaling might actually keep\ngiving us an emergent benefit.",
    "start": "3236790",
    "end": "3246380"
  },
  {
    "text": "- Yeah, I mean there were\nwork from various places, but if you want to kind of\nplace it in the GPT timeline,",
    "start": "3246690",
    "end": "3254210"
  },
  {
    "text": "that would be around GPT-2, yeah. - Well, 'cause you said it, you're so charismatic and\nyou said so many words,",
    "start": "3256380",
    "end": "3263580"
  },
  {
    "text": "but self supervised learning, yes. But again, the same\nintuition you're applying",
    "start": "3263580",
    "end": "3268980"
  },
  {
    "text": "to saying that autoregressive LLMs cannot have a deep\nunderstanding of the world,",
    "start": "3268980",
    "end": "3275190"
  },
  {
    "text": "if we just apply that same intuition, does it make sense to you",
    "start": "3275190",
    "end": "3279490"
  },
  {
    "text": "that they're able to form enough of a representation in the world to be damn convincing,",
    "start": "3280500",
    "end": "3285780"
  },
  {
    "text": "essentially passing the\noriginal Turing test with flying colors.",
    "start": "3285780",
    "end": "3290790"
  },
  {
    "text": "- Well, we're fooled by\ntheir fluency, right? We just assume that if a system is fluent",
    "start": "3290790",
    "end": "3296087"
  },
  {
    "text": "in manipulating language, then it has all the characteristics\nof human intelligence. But that impression is false.",
    "start": "3296087",
    "end": "3304051"
  },
  {
    "text": "We're really fooled by it. - Well, what do you think\nAlan Turing would say? Without understanding anything,",
    "start": "3304051",
    "end": "3310049"
  },
  {
    "text": "just hanging out with it- - Alan Turing would decide that a Turing test is a really bad test. (Lex chuckles)",
    "start": "3310050",
    "end": "3315579"
  },
  {
    "text": "Okay. This is what the AI community\nhas decided many years ago that the Turing test was a\nreally bad test of intelligence.",
    "start": "3315579",
    "end": "3322020"
  },
  {
    "text": "- What would Hans Moravec say about the large language models? - Hans Moravec would say the Moravec's paradox still applies.",
    "start": "3322020",
    "end": "3330180"
  },
  {
    "text": "- [Lex] Okay. - Okay? Okay, we can pass- - You don't think he\nwould be really impressed. - No, of course everybody\nwould be impressed.",
    "start": "3330180",
    "end": "3335677"
  },
  {
    "text": "(laughs) But it is not a question\nof being impressed or not, it is a question of knowing",
    "start": "3335677",
    "end": "3341460"
  },
  {
    "text": "what the limit of those systems can do. Again, they are impressive. They can do a lot of useful things.",
    "start": "3341460",
    "end": "3347490"
  },
  {
    "text": "There's a whole industry that\nis being built around them. They're gonna make progress, but there is a lot of\nthings they cannot do.",
    "start": "3347490",
    "end": "3353670"
  },
  {
    "text": "And we have to realize what they cannot do and then figure out how we get there.",
    "start": "3353670",
    "end": "3359940"
  },
  {
    "text": "And I'm not saying this... I'm saying this from\nbasically 10 years of research",
    "start": "3359940",
    "end": "3367370"
  },
  {
    "text": "on the idea of self supervised running, actually that's going\nback more than 10 years,",
    "start": "3367370",
    "end": "3373770"
  },
  {
    "text": "but the idea of self supervised learning. So basically capturing\nthe internal structure of a piece of a set of inputs",
    "start": "3373770",
    "end": "3381180"
  },
  {
    "text": "without training the system\nfor any particular task, right? Learning representations. The conference I co-founded 14 years ago",
    "start": "3381180",
    "end": "3388830"
  },
  {
    "text": "is called International Conference on Learning Representations, that's the entire issue that\ndeep learning is dealing with.",
    "start": "3388830",
    "end": "3394548"
  },
  {
    "text": "Right? And it's been my obsession\nfor almost 40 years now. So learning representation\nis really the thing.",
    "start": "3394548",
    "end": "3402160"
  },
  {
    "text": "For the longest time we could only do this\nwith supervised learning. And then we started working on",
    "start": "3402160",
    "end": "3407425"
  },
  {
    "text": "what we used to call unsupervised learning and sort of revived the idea\nof unsupervised learning",
    "start": "3407425",
    "end": "3415222"
  },
  {
    "text": "in the early 2000s with\nYoshua Bengio and Jeff Hinton. Then discovered that supervised learning",
    "start": "3415222",
    "end": "3420690"
  },
  {
    "text": "actually works pretty well if you can collect enough data. And so the whole idea of\nunsupervised self supervision",
    "start": "3420690",
    "end": "3427236"
  },
  {
    "text": "took a backseat for a bit and then I kind of tried\nto revive it in a big way,",
    "start": "3427236",
    "end": "3434977"
  },
  {
    "text": "starting in 2014 basically\nwhen we started FAIR, and really pushing for\nlike finding new methods",
    "start": "3436890",
    "end": "3444036"
  },
  {
    "text": "to do self supervised running, both for text and for images\nand for video and audio.",
    "start": "3444036",
    "end": "3449910"
  },
  {
    "text": "And some of that work has\nbeen incredibly successful. I mean, the reason why we have multilingual translation system,",
    "start": "3449910",
    "end": "3457320"
  },
  {
    "text": "things to do, content moderation on Meta,\nfor example, on Facebook that are multilingual,",
    "start": "3457320",
    "end": "3462653"
  },
  {
    "text": "that understand whether piece of text is hate speech or not, or something is due to their progress using self supervised running for NLP,",
    "start": "3462653",
    "end": "3470100"
  },
  {
    "text": "combining this with\ntransformer architectures and blah blah blah. But that's the big success\nof self supervised running.",
    "start": "3470100",
    "end": "3475800"
  },
  {
    "text": "We had similar success\nin speech recognition, a system called Wav2Vec, which is also a joint embedding\narchitecture by the way,",
    "start": "3475800",
    "end": "3482520"
  },
  {
    "text": "trained with contrastive learning. And that system also can produce speech recognition systems\nthat are multilingual",
    "start": "3482520",
    "end": "3490590"
  },
  {
    "text": "with mostly unlabeled data and only need a few\nminutes of labeled data to actually do speech recognition.",
    "start": "3490590",
    "end": "3496950"
  },
  {
    "text": "That's amazing. We have systems now based on\nthose combination of ideas",
    "start": "3496950",
    "end": "3502230"
  },
  {
    "text": "that can do real time translation of hundreds of languages into each other, speech to speech.",
    "start": "3502230",
    "end": "3508050"
  },
  {
    "text": "- Speech to speech, even including, which is fascinating, languages that don't have written forms-",
    "start": "3508050",
    "end": "3513925"
  },
  {
    "text": "- That's right.\n- They're spoken only. - That's right. We don't go through text, it goes directly from speech to speech using an internal representation",
    "start": "3513925",
    "end": "3520290"
  },
  {
    "text": "of kinda speech units that are discrete. But it's called Textless NLP. We used to call it this way.",
    "start": "3520290",
    "end": "3525600"
  },
  {
    "text": "But yeah. I mean incredible success there. And then for 10 years we\ntried to apply this idea",
    "start": "3525600",
    "end": "3533130"
  },
  {
    "text": "to learning representations of images by training a system to predict videos, learning intuitive physics",
    "start": "3533130",
    "end": "3538619"
  },
  {
    "text": "by training a system to predict what's gonna happen in the video. And tried and tried and failed and failed",
    "start": "3538620",
    "end": "3545100"
  },
  {
    "text": "with generative models, with models that predict pixels. We could not get them to learn",
    "start": "3545100",
    "end": "3550836"
  },
  {
    "text": "good representations of images, we could not get them to learn\ngood presentations of videos.",
    "start": "3550836",
    "end": "3556410"
  },
  {
    "text": "And we tried many times, we published lots of papers on it. They kind of sort of worked,\nbut not really great.",
    "start": "3556410",
    "end": "3562293"
  },
  {
    "text": "It started working, we abandoned this idea\nof predicting every pixel and basically just doing the\njoint embedding and predicting",
    "start": "3563220",
    "end": "3570960"
  },
  {
    "text": "in representation space. That works. So there's ample evidence",
    "start": "3570960",
    "end": "3576180"
  },
  {
    "text": "that we're not gonna be able\nto learn good representations of the real world",
    "start": "3576180",
    "end": "3582000"
  },
  {
    "text": "using generative model. So I'm telling people, everybody's talking about generative AI. If you're really interested\nin human level AI,",
    "start": "3582000",
    "end": "3588810"
  },
  {
    "text": "abandon the idea of generative AI. (Lex laughs) - Okay. But you really think it's possible",
    "start": "3588810",
    "end": "3594840"
  },
  {
    "text": "to get far with joint\nembedding representation? So like there's common sense reasoning",
    "start": "3594840",
    "end": "3601350"
  },
  {
    "text": "and then there's high level reasoning. Like I feel like those are two...",
    "start": "3601350",
    "end": "3608579"
  },
  {
    "text": "The kind of reasoning\nthat LLMs are able to do. Okay, let me not use the word reasoning,",
    "start": "3608580",
    "end": "3613650"
  },
  {
    "text": "but the kind of stuff\nthat LLMs are able to do seems fundamentally different than the common sense reasoning we use",
    "start": "3613650",
    "end": "3619589"
  },
  {
    "text": "to navigate the world. - [Yann] Yeah. - It seems like we're gonna need both- - Sure.\n- Would you be able to get,",
    "start": "3619590",
    "end": "3625140"
  },
  {
    "text": "with the joint embedding which\nis a JEPA type of approach, looking at video, would\nyou be able to learn,",
    "start": "3625140",
    "end": "3630633"
  },
  {
    "text": "let's see, well, how to get from New York to Paris, or how to understand the state\nof politics in the world?",
    "start": "3632370",
    "end": "3640520"
  },
  {
    "text": "(both laugh) Right? These are things where various humans generate a lot of\nlanguage and opinions on,",
    "start": "3642565",
    "end": "3649050"
  },
  {
    "text": "in the space of language, but don't visually represent that in any clearly compressible way.",
    "start": "3649050",
    "end": "3656069"
  },
  {
    "text": "- Right. Well, there's a lot of situations that might be difficult for a purely language\nbased system to know.",
    "start": "3656070",
    "end": "3664859"
  },
  {
    "text": "Like, okay, you can probably\nlearn from reading texts, the entirety of the publicly\navailable text in the world",
    "start": "3664860",
    "end": "3671460"
  },
  {
    "text": "that I cannot get from New York to Paris by snapping my fingers. That's not gonna work, right? - [Lex] Yes.",
    "start": "3671460",
    "end": "3677153"
  },
  {
    "text": "- But there's probably\nsort of more complex scenarios of this type which an LLM may never have encountered",
    "start": "3678420",
    "end": "3685770"
  },
  {
    "text": "and may not be able to determine whether it's possible or not. So that link from the low\nlevel to the high level...",
    "start": "3685770",
    "end": "3694627"
  },
  {
    "text": "The thing is that the high\nlevel that language expresses is based on the common\nexperience of the low level,",
    "start": "3695490",
    "end": "3703230"
  },
  {
    "text": "which LLMs currently do not have. When we talk to each other, we know we have a common\nexperience of the world.",
    "start": "3703230",
    "end": "3710106"
  },
  {
    "text": "Like a lot of it is similar. And LLMs don't have that.",
    "start": "3710106",
    "end": "3719039"
  },
  {
    "text": "- But see, there it's present. You and I have a common\nexperience of the world in terms of the physics\nof how gravity works",
    "start": "3719040",
    "end": "3725880"
  },
  {
    "text": "and stuff like this. And that common knowledge of the world,",
    "start": "3725880",
    "end": "3731713"
  },
  {
    "text": "I feel like is there in the language. We don't explicitly express it,",
    "start": "3731730",
    "end": "3737820"
  },
  {
    "text": "but if you have a huge amount of text, you're going to get this stuff\nthat's between the lines.",
    "start": "3737820",
    "end": "3744807"
  },
  {
    "text": "In order to form a consistent world model, you're going to have to\nunderstand how gravity works,",
    "start": "3744808",
    "end": "3751680"
  },
  {
    "text": "even if you don't have an\nexplicit explanation of gravity. So even though, in the case of gravity,",
    "start": "3751680",
    "end": "3757410"
  },
  {
    "text": "there is explicit explanation. There's gravity in Wikipedia. But like the stuff that we think of",
    "start": "3757410",
    "end": "3764370"
  },
  {
    "text": "as common sense reasoning, I feel like to generate\nlanguage correctly, you're going to have to figure that out.",
    "start": "3764370",
    "end": "3771603"
  },
  {
    "text": "Now, you could say as you have, there's not enough text-\n- Well, I agree. - Sorry. Okay, yeah.",
    "start": "3771603",
    "end": "3776737"
  },
  {
    "text": "(laughs) You don't think so? - No, I agree with what you just said, which is that to be able to\ndo high level common sense...",
    "start": "3776737",
    "end": "3783349"
  },
  {
    "text": "To have high level common sense, you need to have the\nlow level common sense to build on top of. - [Lex] Yeah.",
    "start": "3783349",
    "end": "3789030"
  },
  {
    "text": "But that's not there. - That's not there in LLMs. LLMs are purely trained from text. So then the other statement you made,",
    "start": "3789030",
    "end": "3795009"
  },
  {
    "text": "I would not agree with the fact that implicit\nin all languages in the world",
    "start": "3795010",
    "end": "3800369"
  },
  {
    "text": "is the underlying reality. There's a lot about underlying reality which is not expressed in language.",
    "start": "3800370",
    "end": "3806880"
  },
  {
    "text": "- Is that obvious to you? - Yeah, totally. - So like all the conversations we have...",
    "start": "3806880",
    "end": "3814044"
  },
  {
    "text": "Okay, there's the dark web, meaning whatever, the private conversations\nlike DMs and stuff like this,",
    "start": "3814044",
    "end": "3821190"
  },
  {
    "text": "which is much, much larger\nprobably than what's available, what LLMs are trained on.",
    "start": "3821190",
    "end": "3826920"
  },
  {
    "text": "- You don't need to communicate the stuff that is common. - But the humor, all of it. No, you do.",
    "start": "3826920",
    "end": "3832198"
  },
  {
    "text": "You don't need to, but it comes through. Like if I accidentally knock this over,",
    "start": "3832198",
    "end": "3838320"
  },
  {
    "text": "you'll probably make fun of me. And in the content of\nthe you making fun of me will be explanation of\nthe fact that cups fall",
    "start": "3838320",
    "end": "3847410"
  },
  {
    "text": "and then gravity works in this way. And then you'll have some\nvery vague information",
    "start": "3847410",
    "end": "3852750"
  },
  {
    "text": "about what kind of things\nexplode when they hit the ground. And then maybe you'll\nmake a joke about entropy",
    "start": "3852750",
    "end": "3859050"
  },
  {
    "text": "or something like this and we will never be able\nto reconstruct this again. Like, okay, you'll make\na little joke like this",
    "start": "3859050",
    "end": "3864769"
  },
  {
    "text": "and there'll be trillion of other jokes. And from the jokes, you can piece together the\nfact that gravity works",
    "start": "3864769",
    "end": "3870960"
  },
  {
    "text": "and mugs can break and\nall this kind of stuff, you don't need to see... It'll be very inefficient.",
    "start": "3870960",
    "end": "3876870"
  },
  {
    "text": "It's easier for like to not knock the thing over. (laughing) - [Yann] Yeah.",
    "start": "3876870",
    "end": "3882533"
  },
  {
    "text": "- But I feel like it would be there if you have enough of that data. - I just think that most of\nthe information of this type",
    "start": "3882533",
    "end": "3890730"
  },
  {
    "text": "that we have accumulated\nwhen we were babies is just not present in text,",
    "start": "3890730",
    "end": "3898050"
  },
  {
    "text": "in any description, essentially. And the sensory data\nis a much richer source for getting that kind of understanding.",
    "start": "3898050",
    "end": "3904560"
  },
  {
    "text": "I mean, that's the 16,000 hours of wake time of a 4-year-old. And tend to do 15 bytes,\ngoing through vision.",
    "start": "3904560",
    "end": "3912540"
  },
  {
    "text": "Just vision, right? There is a similar bandwidth of touch",
    "start": "3912540",
    "end": "3917640"
  },
  {
    "text": "and a little less through audio. And then text doesn't... Language doesn't come in\nuntil like a year in life.",
    "start": "3917640",
    "end": "3926510"
  },
  {
    "text": "And by the time you are nine years old, you've learned about gravity, you know about inertia,",
    "start": "3926790",
    "end": "3931920"
  },
  {
    "text": "you know about gravity, you know there's stability, you know about the distinction between animate and inanimate objects.",
    "start": "3931920",
    "end": "3938850"
  },
  {
    "text": "By 18 months, you know about like why\npeople want to do things and you help them if they can't.",
    "start": "3938850",
    "end": "3945510"
  },
  {
    "text": "I mean there's a lot of\nthings that you learn mostly by observation, really not even through interaction.",
    "start": "3945510",
    "end": "3952010"
  },
  {
    "text": "In the first few months of life, babies don't really have\nany influence on the world. They can only observe, right?",
    "start": "3952010",
    "end": "3958109"
  },
  {
    "text": "And you accumulate like a\ngigantic amount of knowledge just from that. So that's what we're missing\nfrom current AI systems.",
    "start": "3958110",
    "end": "3966423"
  },
  {
    "start": "3966000",
    "end": "4290000"
  },
  {
    "text": "- I think in one of your\nslides you have this nice plot that is one of the ways you\nshow that LLMs are limited.",
    "start": "3967500",
    "end": "3973980"
  },
  {
    "text": "I wonder if you could\ntalk about hallucinations from your perspectives. Why hallucinations happen\nfrom large language models,",
    "start": "3973980",
    "end": "3983312"
  },
  {
    "text": "and to what degree is\nthat a fundamental flaw of large language models.",
    "start": "3983520",
    "end": "3989400"
  },
  {
    "text": "- Right. So because of the\nautoregressive prediction, every time an LLM produces\na token or a word,",
    "start": "3989400",
    "end": "3997898"
  },
  {
    "text": "there is some level of\nprobability for that word to take you out of the\nset of reasonable answers.",
    "start": "3997898",
    "end": "4004133"
  },
  {
    "text": "And if you assume, which is a very strong assumption, that the probability of such error",
    "start": "4005229",
    "end": "4010650"
  },
  {
    "text": "is those errors are independent across a sequence of\ntokens being produced.",
    "start": "4013190",
    "end": "4019520"
  },
  {
    "text": "What that means is that every\ntime you produce a token, the probability that you stay within the set\nof correct answer decreases",
    "start": "4019520",
    "end": "4026990"
  },
  {
    "text": "and it decreases exponentially. - So there's a strong, like\nyou said, assumption there that if there's a non-zero\nprobability of making a mistake,",
    "start": "4026990",
    "end": "4034820"
  },
  {
    "text": "which there appears to be, then there's going to be a kind of drift. - Yeah.",
    "start": "4034820",
    "end": "4039920"
  },
  {
    "text": "And that drift is exponential. It's like errors accumulate, right? So the probability that an\nanswer would be nonsensical",
    "start": "4039920",
    "end": "4047377"
  },
  {
    "text": "increases exponentially\nwith the number of tokens. - Is that obvious to you by the way?",
    "start": "4047377",
    "end": "4053839"
  },
  {
    "text": "Well, so mathematically speaking maybe, but like isn't there a\nkind of gravitational pull",
    "start": "4053840",
    "end": "4059988"
  },
  {
    "text": "towards the truth? Because on average, hopefully, the truth is well represented\nin the training set.",
    "start": "4059988",
    "end": "4068990"
  },
  {
    "text": "- No, it's basically a struggle against the curse of dimensionality.",
    "start": "4068990",
    "end": "4075560"
  },
  {
    "text": "So the way you can correct for this is that you fine tune the system by having it produce answers",
    "start": "4075560",
    "end": "4081590"
  },
  {
    "text": "for all kinds of questions\nthat people might come up with. And people are people,",
    "start": "4081590",
    "end": "4086779"
  },
  {
    "text": "so a lot of the questions that they have are very similar to each other. So you can probably cover, you know, 80% or whatever of\nquestions that people will ask",
    "start": "4086780",
    "end": "4096549"
  },
  {
    "text": "by collecting data.",
    "start": "4096822",
    "end": "4100162"
  },
  {
    "text": "And then you fine tune the system to produce good answers\nfor all of those things. And it's probably gonna\nbe able to learn that",
    "start": "4102050",
    "end": "4107827"
  },
  {
    "text": "because it's got a lot\nof capacity to learn. But then there is the\nenormous set of prompts",
    "start": "4107827",
    "end": "4116623"
  },
  {
    "text": "that you have not covered during training. And that set is enormous. Like within the set of\nall possible prompts,",
    "start": "4116990",
    "end": "4123319"
  },
  {
    "text": "the proportion of prompts that\nhave been used for training is absolutely tiny.",
    "start": "4123320",
    "end": "4128692"
  },
  {
    "text": "It's a tiny, tiny, tiny subset\nof all possible prompts. And so the system will behave properly",
    "start": "4129668",
    "end": "4136130"
  },
  {
    "text": "on the prompts that it's\nbeen either trained, pre-trained or fine tuned.",
    "start": "4136130",
    "end": "4139402"
  },
  {
    "text": "But then there is an\nentire space of things that it cannot possibly\nhave been trained on",
    "start": "4141530",
    "end": "4146900"
  },
  {
    "text": "because it's just the number is gigantic. So whatever training the system",
    "start": "4146900",
    "end": "4153120"
  },
  {
    "text": "has been subject to produce\nappropriate answers, you can break it by finding out a prompt",
    "start": "4154032",
    "end": "4160580"
  },
  {
    "text": "that will be outside of the set of prompts it's been trained on",
    "start": "4160580",
    "end": "4165589"
  },
  {
    "text": "or things that are similar, and then it will just\nspew complete nonsense. - When you say prompt,",
    "start": "4165590",
    "end": "4171067"
  },
  {
    "text": "do you mean that exact prompt or do you mean a prompt that's like, in many parts very different than...",
    "start": "4171067",
    "end": "4178730"
  },
  {
    "text": "Is it that easy to ask a question or to say a thing that\nhasn't been said before",
    "start": "4178730",
    "end": "4185540"
  },
  {
    "text": "on the internet? - I mean, people have come up with things where like you put essentially",
    "start": "4185540",
    "end": "4191210"
  },
  {
    "text": "a random sequence of\ncharacters in a prompt and that's enough to kind of\nthrow the system into a mode",
    "start": "4191210",
    "end": "4197690"
  },
  {
    "text": "where it's gonna answer\nsomething completely different than it would have answered without this.",
    "start": "4197690",
    "end": "4203420"
  },
  {
    "text": "So that's a way to jailbreak\nthe system, basically. Go outside of its conditioning, right?",
    "start": "4203420",
    "end": "4209380"
  },
  {
    "text": "- So that's a very clear\ndemonstration of it. But of course, that goes outside",
    "start": "4209380",
    "end": "4216547"
  },
  {
    "text": "of what it's designed to do, right? If you actually stitch together reasonably grammatical sentences,",
    "start": "4216860",
    "end": "4222800"
  },
  {
    "text": "is it that easy to break it? - Yeah. Some people have done things like",
    "start": "4222800",
    "end": "4229010"
  },
  {
    "text": "you write a sentence in English or you ask a question in English and it produces a perfectly fine answer.",
    "start": "4229010",
    "end": "4236690"
  },
  {
    "text": "And then you just substitute a few words by the same word in another language,",
    "start": "4236690",
    "end": "4242480"
  },
  {
    "text": "and all of a sudden the\nanswer is complete nonsense. - Yeah. So I guess what I'm saying is like, which fraction of prompts that\nhumans are likely to generate",
    "start": "4242480",
    "end": "4251860"
  },
  {
    "text": "are going to break the system? - So the problem is that\nthere is a long tail.",
    "start": "4252470",
    "end": "4257600"
  },
  {
    "text": "- [Lex] Yes. - This is an issue that a\nlot of people have realized in social networks and stuff like that,",
    "start": "4257600",
    "end": "4264079"
  },
  {
    "text": "which is there's a very, very long tail of things that people will ask. And you can fine tune the system",
    "start": "4264080",
    "end": "4269839"
  },
  {
    "text": "for the 80% or whatever of the things that most people will ask.",
    "start": "4269839",
    "end": "4276170"
  },
  {
    "text": "And then this long tail is so large that you're not gonna be\nable to fine tune the system for all the conditions.",
    "start": "4276170",
    "end": "4281900"
  },
  {
    "text": "And in the end, the system ends up being kind of a giant lookup\ntable, right? (laughing) Essentially. Which is not really what you want.",
    "start": "4281900",
    "end": "4287809"
  },
  {
    "text": "You want systems that can reason, certainly that can plan. So the type of reasoning\nthat takes place in LLM",
    "start": "4287810",
    "end": "4293809"
  },
  {
    "start": "4290000",
    "end": "5342000"
  },
  {
    "text": "is very, very primitive. And the reason you can tell it's primitive is because the amount of computation",
    "start": "4293810",
    "end": "4299113"
  },
  {
    "text": "that is spent per token\nproduced is constant. So if you ask a question",
    "start": "4299113",
    "end": "4305840"
  },
  {
    "text": "and that question has an answer\nin a given number of token, the amount of computation\ndevoted to computing that answer",
    "start": "4305840",
    "end": "4312770"
  },
  {
    "text": "can be exactly estimated. It's the size of the prediction network",
    "start": "4312770",
    "end": "4319728"
  },
  {
    "text": "with its 36 layers or 92\nlayers or whatever it is, multiplied by number of tokens.",
    "start": "4319728",
    "end": "4325460"
  },
  {
    "text": "That's it. And so essentially, it doesn't matter if\nthe question being asked",
    "start": "4325460",
    "end": "4330560"
  },
  {
    "text": "is simple to answer,\ncomplicated to answer, impossible to answer",
    "start": "4332060",
    "end": "4337760"
  },
  {
    "text": "because it's decided,\nwell, there's something. The amount of computation the system will be able to\ndevote to the answer is constant",
    "start": "4337760",
    "end": "4345590"
  },
  {
    "text": "or is proportional to the\nnumber of token produced in the answer, right? This is not the way we work,",
    "start": "4345590",
    "end": "4350840"
  },
  {
    "text": "the way we reason is that when we are faced with a complex problem",
    "start": "4350840",
    "end": "4357050"
  },
  {
    "text": "or a complex question, we spend more time trying to\nsolve it and answer it, right?",
    "start": "4357050",
    "end": "4362719"
  },
  {
    "text": "Because it's more difficult. - There's a prediction element, there's an iterative element where you're like adjusting\nyour understanding of a thing",
    "start": "4362720",
    "end": "4372250"
  },
  {
    "text": "by going over and over and over. There's a hierarchical elements on. Does this mean it's a\nfundamental flaw of LLMs-",
    "start": "4372410",
    "end": "4379457"
  },
  {
    "text": "- [Yann] Yeah. - Or does it mean that... (laughs) There's more part to that question? (laughs)",
    "start": "4379457",
    "end": "4384710"
  },
  {
    "text": "Now you're just behaving like an LLM. (laughs) Immediately answering. No, that it's just the\nlow level world model",
    "start": "4384710",
    "end": "4393760"
  },
  {
    "text": "on top of which we can then build some of these kinds of mechanisms, like you said, persistent\nlong-term memory or reasoning,",
    "start": "4394100",
    "end": "4403543"
  },
  {
    "text": "so on. But we need that world model\nthat comes from language.",
    "start": "4403543",
    "end": "4409190"
  },
  {
    "text": "Maybe it is not so difficult to build this kind of reasoning system on top of a well constructed world model.",
    "start": "4409190",
    "end": "4416780"
  },
  {
    "text": "- Okay. Whether it's difficult or not, the near future will say, because a lot of people\nare working on reasoning",
    "start": "4416780",
    "end": "4423620"
  },
  {
    "text": "and planning abilities\nfor dialogue systems. I mean, even if we restrict\nourselves to language,",
    "start": "4423620",
    "end": "4430703"
  },
  {
    "text": "just having the ability to plan your answer before you answer, in terms that are not necessarily linked",
    "start": "4432590",
    "end": "4439460"
  },
  {
    "text": "with the language you're gonna\nuse to produce the answer. Right? So this idea of this mental model that allows you to plan\nwhat you're gonna say",
    "start": "4439460",
    "end": "4446000"
  },
  {
    "text": "before you say it. That is very important.",
    "start": "4446000",
    "end": "4451730"
  },
  {
    "text": "I think there's going\nto be a lot of systems over the next few years that are going to have this capability,",
    "start": "4451730",
    "end": "4457400"
  },
  {
    "text": "but the blueprint of those systems will be extremely different\nfrom autoregressive LLMs.",
    "start": "4457400",
    "end": "4463190"
  },
  {
    "text": "So it's the same difference as the difference between",
    "start": "4463190",
    "end": "4469550"
  },
  {
    "text": "what psychology has called\nsystem one and system two in humans, right? So system one is the type of\ntask that you can accomplish",
    "start": "4469550",
    "end": "4475640"
  },
  {
    "text": "without like deliberately\nconsciously think about how you do them. You just do them.",
    "start": "4475640",
    "end": "4482119"
  },
  {
    "text": "You've done them enough that you can just do it\nsubconsciously, right? Without thinking about them. If you're an experienced driver,",
    "start": "4482120",
    "end": "4488600"
  },
  {
    "text": "you can drive without\nreally thinking about it and you can talk to\nsomeone at the same time or listen to the radio, right?",
    "start": "4488600",
    "end": "4494240"
  },
  {
    "text": "If you are a very\nexperienced chess player, you can play against a\nnon-experienced chess player",
    "start": "4495650",
    "end": "4501050"
  },
  {
    "text": "without really thinking either, you just recognize the\npattern and you play, right? That's system one.",
    "start": "4501050",
    "end": "4506633"
  },
  {
    "text": "So all the things that\nyou do instinctively without really having to deliberately plan and think about it.",
    "start": "4508190",
    "end": "4513493"
  },
  {
    "text": "And then there is other\ntasks where you need to plan. So if you are a not too\nexperienced chess player",
    "start": "4513493",
    "end": "4519139"
  },
  {
    "text": "or you are experienced but you play against another\nexperienced chess player, you think about all\nkinds of options, right?",
    "start": "4519139",
    "end": "4524780"
  },
  {
    "text": "You think about it for a while, right? And you're much better if you\nhave time to think about it",
    "start": "4524780",
    "end": "4530005"
  },
  {
    "text": "than you are if you play\nblitz with limited time.",
    "start": "4530005",
    "end": "4535005"
  },
  {
    "text": "And so this type of deliberate planning,",
    "start": "4535347",
    "end": "4539303"
  },
  {
    "text": "which uses your internal world\nmodel, that's system two, this is what LLMs currently cannot do.",
    "start": "4540590",
    "end": "4546510"
  },
  {
    "text": "How do we get them to do this, right? How do we build a system that can do this kind\nof planning or reasoning",
    "start": "4546510",
    "end": "4555050"
  },
  {
    "text": "that devotes more resources to complex problems\nthan to simple problems.",
    "start": "4555050",
    "end": "4560330"
  },
  {
    "text": "And it's not going to be autoregressive prediction of tokens, it's going to be more\nsomething akin to inference",
    "start": "4560330",
    "end": "4568040"
  },
  {
    "text": "of latent variables in what used to be called\nprobabilistic models",
    "start": "4568040",
    "end": "4574480"
  },
  {
    "text": "or graphical models and\nthings of that type. So basically the principle is like this.",
    "start": "4574880",
    "end": "4579700"
  },
  {
    "text": "The prompt is like observed variables.",
    "start": "4581000",
    "end": "4584633"
  },
  {
    "text": "And what the model does is that it's basically a measure of...",
    "start": "4586010",
    "end": "4593360"
  },
  {
    "text": "It can measure to what extent an answer is a good answer for a prompt. Okay?",
    "start": "4593360",
    "end": "4598969"
  },
  {
    "text": "So think of it as some\ngigantic neural net, but it's got only one output. And that output is a scaler number,",
    "start": "4598970",
    "end": "4605179"
  },
  {
    "text": "which is let's say zero if the answer is a good\nanswer for the question, and a large number",
    "start": "4605180",
    "end": "4611120"
  },
  {
    "text": "if the answer is not a good\nanswer for the question. Imagine you had this model. If you had such a model,",
    "start": "4611120",
    "end": "4616610"
  },
  {
    "text": "you could use it to produce good answers. The way you would do is produce the prompt",
    "start": "4616610",
    "end": "4622490"
  },
  {
    "text": "and then search through the\nspace of possible answers for one that minimizes that number.",
    "start": "4622490",
    "end": "4627472"
  },
  {
    "text": "That's called an energy based model. - But that energy based model would need the model\nconstructed by the LLM.",
    "start": "4630110",
    "end": "4638540"
  },
  {
    "text": "- Well, so really what you need to do would be to not search over\npossible strings of text",
    "start": "4638540",
    "end": "4644466"
  },
  {
    "text": "that minimize that energy. But what you would do is do this in abstract\nrepresentation space.",
    "start": "4644466",
    "end": "4651050"
  },
  {
    "text": "So in sort of the space\nof abstract thoughts, you would elaborate a thought, right?",
    "start": "4651050",
    "end": "4657020"
  },
  {
    "text": "Using this process of minimizing\nthe output of your model.",
    "start": "4657020",
    "end": "4662020"
  },
  {
    "text": "Okay? Which is just a scaler. It's an optimization process, right? So now the way the system\nproduces its answer",
    "start": "4662090",
    "end": "4668969"
  },
  {
    "text": "is through optimization by minimizing an objective\nfunction basically, right?",
    "start": "4668969",
    "end": "4676040"
  },
  {
    "text": "And this is, we're\ntalking about inference, we're not talking about training, right? The system has been trained already.",
    "start": "4676040",
    "end": "4681050"
  },
  {
    "text": "So now we have an abstract representation of the thought of the answer, representation of the answer.",
    "start": "4681050",
    "end": "4686690"
  },
  {
    "text": "We feed that to basically\nan autoregressive decoder, which can be very simple, that turns this into a text\nthat expresses this thought.",
    "start": "4686690",
    "end": "4695540"
  },
  {
    "text": "Okay? So that in my opinion is the blueprint of future data systems.",
    "start": "4695540",
    "end": "4701014"
  },
  {
    "text": "They will think about their answer, plan their answer by optimization before turning it into text.",
    "start": "4701015",
    "end": "4707971"
  },
  {
    "text": "And that is turning complete. - Can you explain exactly what the optimization problem there is?",
    "start": "4707971",
    "end": "4714470"
  },
  {
    "text": "Like what's the objective function? Just linger on it. You kind of briefly described it,",
    "start": "4714470",
    "end": "4720470"
  },
  {
    "text": "but over what space are you optimizing? - The space of representations-",
    "start": "4720470",
    "end": "4725810"
  },
  {
    "text": "- Goes abstract representation. - That's right. So you have an abstract\nrepresentation inside the system.",
    "start": "4725810",
    "end": "4731570"
  },
  {
    "text": "You have a prompt. The prompt goes through an encoder, produces a representation, perhaps goes through a predictor that predicts a\nrepresentation of the answer,",
    "start": "4731570",
    "end": "4738250"
  },
  {
    "text": "of the proper answer. But that representation\nmay not be a good answer",
    "start": "4738250",
    "end": "4743840"
  },
  {
    "text": "because there might be\nsome complicated reasoning you need to do, right? So then you have another process",
    "start": "4743840",
    "end": "4751220"
  },
  {
    "text": "that takes the representation\nof the answers and modifies it so as to minimize a cost function",
    "start": "4751220",
    "end": "4760040"
  },
  {
    "text": "that measures to what extent the answer is a good\nanswer for the question. Now we sort of ignore the fact for...",
    "start": "4760040",
    "end": "4767563"
  },
  {
    "text": "I mean, the issue for a moment of how you train that system to measure whether an answer\nis a good answer for sure.",
    "start": "4767563",
    "end": "4775358"
  },
  {
    "text": "- But suppose such a\nsystem could be created, what's the process? This kind of search like process.",
    "start": "4775358",
    "end": "4782412"
  },
  {
    "text": "- It's an optimization process. You can do this if the entire\nsystem is differentiable,",
    "start": "4782412",
    "end": "4787639"
  },
  {
    "text": "that scaler output is the result of running\nthrough some neural net, running the answer,",
    "start": "4787640",
    "end": "4794420"
  },
  {
    "text": "the representation of the\nanswer through some neural net. Then by gradient descent, by back propagating gradients,",
    "start": "4794420",
    "end": "4800599"
  },
  {
    "text": "you can figure out like how to modify the\nrepresentation of the answers so as to minimize that. - So that's still a gradient based.",
    "start": "4800600",
    "end": "4806660"
  },
  {
    "text": "- It's gradient based inference. So now you have a\nrepresentation of the answer in abstract space.",
    "start": "4806660",
    "end": "4812060"
  },
  {
    "text": "Now you can turn it into text, right? And the cool thing about this",
    "start": "4812060",
    "end": "4817540"
  },
  {
    "text": "is that the representation now can be optimized through gradient descent, but also is independent of the language",
    "start": "4817540",
    "end": "4824630"
  },
  {
    "text": "in which you're going\nto express the answer. - Right. So you're operating in the\nsubstruct of representation.",
    "start": "4824630",
    "end": "4830090"
  },
  {
    "text": "I mean this goes back\nto the joint embedding. - [Yann] Right. - That it's better to\nwork in the space of...",
    "start": "4830090",
    "end": "4836108"
  },
  {
    "text": "I don't know. Or to romanticize the notion like space of concepts versus the space of concrete\nsensory information.",
    "start": "4836108",
    "end": "4845620"
  },
  {
    "text": "- Right. - Okay. But can this do something like reasoning, which is what we're talking about?",
    "start": "4845720",
    "end": "4851900"
  },
  {
    "text": "- Well, not really, only in a very simple way. I mean basically you can\nthink of those things as doing",
    "start": "4851900",
    "end": "4857150"
  },
  {
    "text": "the kind of optimization\nI was talking about, except they're optimizing\nthe discrete space which is the space of\npossible sequences of tokens.",
    "start": "4857150",
    "end": "4865370"
  },
  {
    "text": "And they do this optimization\nin a horribly inefficient way, which is generate a lot of hypothesis",
    "start": "4865370",
    "end": "4871250"
  },
  {
    "text": "and then select the best ones. And that's incredibly wasteful",
    "start": "4871250",
    "end": "4876560"
  },
  {
    "text": "in terms of competition, 'cause you basically have to run your LLM for like every possible\ngenerative sequence.",
    "start": "4876560",
    "end": "4884813"
  },
  {
    "text": "And it's incredibly wasteful. So it's much better to do an optimization",
    "start": "4885680",
    "end": "4891922"
  },
  {
    "text": "in continuous space where you can do gradient descent as opposed to like generate tons of things and then select the best,",
    "start": "4891922",
    "end": "4898100"
  },
  {
    "text": "you just iteratively refine your answer to go towards the best, right? That's much more efficient.",
    "start": "4898100",
    "end": "4904190"
  },
  {
    "text": "But you can only do this\nin continuous spaces with differentiable functions. - You're talking about the reasoning,",
    "start": "4904190",
    "end": "4910280"
  },
  {
    "text": "like ability to think\ndeeply or to reason deeply. How do you know what is an answer",
    "start": "4910280",
    "end": "4918093"
  },
  {
    "text": "that's better or worse\nbased on deep reasoning?",
    "start": "4918093",
    "end": "4923093"
  },
  {
    "text": "- Right. So then we're asking the question, of conceptually, how do you\ntrain an energy based model? Right?",
    "start": "4924650",
    "end": "4930201"
  },
  {
    "text": "So energy based model is a function with a scaler\noutput, just a number. You give it two inputs, X and Y,",
    "start": "4930202",
    "end": "4937282"
  },
  {
    "text": "and it tells you whether Y\nis compatible with X or not. X you observe, let's say it's a prompt, an\nimage, a video, whatever.",
    "start": "4938210",
    "end": "4944630"
  },
  {
    "text": "And Y is a proposal for an answer, a continuation of video, whatever.",
    "start": "4944630",
    "end": "4950650"
  },
  {
    "text": "And it tells you whether\nY is compatible with X. And the way it tells you\nthat Y is compatible with X",
    "start": "4950650",
    "end": "4957389"
  },
  {
    "text": "is that the output of that\nfunction would be zero if Y is compatible with X, it would be a positive number, non-zero",
    "start": "4957410",
    "end": "4964760"
  },
  {
    "text": "if Y is not compatible with X. Okay. How do you train a system like this? At a completely general level,",
    "start": "4964760",
    "end": "4971840"
  },
  {
    "text": "is you show it pairs of X\nand Ys that are compatible, a question and the corresponding answer.",
    "start": "4971840",
    "end": "4978770"
  },
  {
    "text": "And you train the parameters\nof the big neural net inside to produce zero.",
    "start": "4978770",
    "end": "4983633"
  },
  {
    "text": "Okay. Now that doesn't completely work because the system might decide, well, I'm just gonna\nsay zero for everything.",
    "start": "4984620",
    "end": "4991610"
  },
  {
    "text": "So now you have to have a process to make sure that for a wrong Y, the energy will be larger than zero.",
    "start": "4991610",
    "end": "4998740"
  },
  {
    "text": "And there you have two options, one is contrastive methods. So contrastive method is\nyou show an X and a bad Y,",
    "start": "4998740",
    "end": "5005053"
  },
  {
    "text": "and you tell the system, well, give a high energy to this. Like push up the energy, right? Change the weights in the neural\nnet that compute the energy",
    "start": "5006280",
    "end": "5013060"
  },
  {
    "text": "so that it goes up. So that's contrasting methods. The problem with this is\nif the space of Y is large,",
    "start": "5013060",
    "end": "5021250"
  },
  {
    "text": "the number of such contrasted samples you're gonna have to show is gigantic.",
    "start": "5021250",
    "end": "5027432"
  },
  {
    "text": "But people do this. They do this when you\ntrain a system with RLHF,",
    "start": "5028570",
    "end": "5033610"
  },
  {
    "text": "basically what you're training is what's called a reward model, which is basically an objective function",
    "start": "5033610",
    "end": "5040120"
  },
  {
    "text": "that tells you whether\nan answer is good or bad. And that's basically exactly what this is.",
    "start": "5040120",
    "end": "5046960"
  },
  {
    "text": "So we already do this to some extent. We're just not using it for inference, we're just using it for training.",
    "start": "5046960",
    "end": "5051580"
  },
  {
    "text": "There is another set of methods which are non-contrastive,\nand I prefer those. And those non-contrastive\nmethod basically say,",
    "start": "5054070",
    "end": "5062817"
  },
  {
    "text": "okay, the energy function needs to have low energy on\npairs of XYs that are compatible",
    "start": "5062817",
    "end": "5069618"
  },
  {
    "text": "that come from your training set. How do you make sure that the energy is gonna be higher everywhere else?",
    "start": "5069618",
    "end": "5076123"
  },
  {
    "text": "And the way you do this is by having a regularizer, a criterion,",
    "start": "5077230",
    "end": "5083190"
  },
  {
    "text": "a term in your cost function that basically minimizes\nthe volume of space",
    "start": "5083470",
    "end": "5089200"
  },
  {
    "text": "that can take low energy. And the precise way to do this, there's all kinds of different\nspecific ways to do this",
    "start": "5089200",
    "end": "5095290"
  },
  {
    "text": "depending on the architecture, but that's the basic principle. So that if you push\ndown the energy function",
    "start": "5095290",
    "end": "5100990"
  },
  {
    "text": "for particular regions in the XY space, it will automatically\ngo up in other places",
    "start": "5100990",
    "end": "5106150"
  },
  {
    "text": "because there's only a\nlimited volume of space that can take low energy. Okay?",
    "start": "5106150",
    "end": "5111963"
  },
  {
    "text": "By the construction of the system or by the regularizing function. - We've been talking very generally,",
    "start": "5111963",
    "end": "5118810"
  },
  {
    "text": "but what is a good X and a good Y? What is a good representation of X and Y?",
    "start": "5118810",
    "end": "5125770"
  },
  {
    "text": "Because we've been talking about language. And if you just take language directly, that presumably is not good,",
    "start": "5125770",
    "end": "5132220"
  },
  {
    "text": "so there has to be some kind of abstract\nrepresentation of ideas. - Yeah. I mean you can do this\nwith language directly",
    "start": "5132220",
    "end": "5139167"
  },
  {
    "text": "by just, you know, X is a text and Y is the continuation of that text. - [Lex] Yes.",
    "start": "5139167",
    "end": "5145210"
  },
  {
    "text": "- Or X is a question, Y is the answer. - But you're saying\nthat's not gonna take it. I mean, that's going to\ndo what LLMs are doing.",
    "start": "5145210",
    "end": "5152650"
  },
  {
    "text": "- Well, no. It depends on how the internal\nstructure of the system is built. If the internal structure of the system",
    "start": "5152650",
    "end": "5159040"
  },
  {
    "text": "is built in such a way\nthat inside of the system there is a latent variable, let's called it Z,",
    "start": "5159040",
    "end": "5164710"
  },
  {
    "text": "that you can manipulate so as to minimize the output energy,",
    "start": "5164710",
    "end": "5171433"
  },
  {
    "text": "then that Z can be viewed as\nrepresentation of a good answer that you can translate into\na Y that is a good answer.",
    "start": "5172900",
    "end": "5179410"
  },
  {
    "text": "- So this kind of system could be trained in a very similar way? - Very similar way. But you have to have this\nway of preventing collapse,",
    "start": "5181060",
    "end": "5186506"
  },
  {
    "text": "of ensuring that there is high energy for things you don't train it on.",
    "start": "5186506",
    "end": "5193586"
  },
  {
    "text": "And currently it's very implicit in LLMs. It is done in a way",
    "start": "5193586",
    "end": "5199360"
  },
  {
    "text": "that people don't realize it's being done, but it is being done. It's due to the fact that when you give a high\nprobability to a word,",
    "start": "5199360",
    "end": "5208003"
  },
  {
    "text": "automatically you give low\nprobability to other words because you only have a finite amount of probability\nto go around. (laughing)",
    "start": "5209320",
    "end": "5215725"
  },
  {
    "text": "Right? They have to sub to one. So when you minimize the\ncross entropy or whatever, when you train your LLM\nto predict the next word,",
    "start": "5215725",
    "end": "5225197"
  },
  {
    "text": "you are increasing the probability your system will give to the correct word, but you're also decreasing the probability",
    "start": "5225197",
    "end": "5230620"
  },
  {
    "text": "it will give to the incorrect words. Now, indirectly, that gives\na low probability to...",
    "start": "5230620",
    "end": "5237070"
  },
  {
    "text": "A high probability to sequences\nof words that are good and low probability two\nsequences of words that are bad, but it's very indirect.",
    "start": "5237070",
    "end": "5243334"
  },
  {
    "text": "It's not obvious why this\nactually works at all, because you're not doing\nit on a joint probability",
    "start": "5243334",
    "end": "5250331"
  },
  {
    "text": "of all the symbols in a sequence, you're just doing it kind of,",
    "start": "5250331",
    "end": "5254323"
  },
  {
    "text": "sort of factorized that probability in terms of conditional probabilities over successive tokens.",
    "start": "5255940",
    "end": "5261730"
  },
  {
    "text": "- So how do you do this for visual data? - So we've been doing this with all JEPA architectures,\nbasically the-",
    "start": "5261730",
    "end": "5267113"
  },
  {
    "text": "- [Lex] The joint embedding? - I-JEPA. So there, the compatibility\nbetween two things",
    "start": "5267113",
    "end": "5272290"
  },
  {
    "text": "is here's an image or a video, here is a corrupted, shifted\nor transformed version",
    "start": "5272290",
    "end": "5278350"
  },
  {
    "text": "of that image or video or masked. Okay? And then the energy of the system",
    "start": "5278350",
    "end": "5284199"
  },
  {
    "text": "is the prediction error\nof the representation.",
    "start": "5284200",
    "end": "5289200"
  },
  {
    "text": "The predicted representation\nof the good thing versus the actual representation\nof the good thing, right?",
    "start": "5291899",
    "end": "5297550"
  },
  {
    "text": "So you run the corrupted\nimage to the system, predict the representation of\nthe good input uncorrupted,",
    "start": "5297550",
    "end": "5304840"
  },
  {
    "text": "and then compute the prediction error. That's the energy of the system. So this system will tell you,",
    "start": "5304840",
    "end": "5310090"
  },
  {
    "text": "this is a good image and\nthis is a corrupted version.",
    "start": "5310090",
    "end": "5315090"
  },
  {
    "text": "It will give you zero energy if those two things are effectively, one of them is a corrupted\nversion of the other,",
    "start": "5316740",
    "end": "5323350"
  },
  {
    "text": "give you a high energy if the two images are\ncompletely different. - And hopefully that whole process",
    "start": "5323350",
    "end": "5328570"
  },
  {
    "text": "gives you a really nice\ncompressed representation of reality, of visual reality.",
    "start": "5328570",
    "end": "5334780"
  },
  {
    "text": "- And we know it does because then we use those presentations as input to a classification\nsystem or something, and it works-\n- And then",
    "start": "5334780",
    "end": "5340828"
  },
  {
    "text": "that classification system\nworks really nicely. Okay. Well, so to summarize, you recommend in a spicy way\nthat only Yann LeCun can,",
    "start": "5340828",
    "end": "5349500"
  },
  {
    "start": "5342000",
    "end": "5650000"
  },
  {
    "text": "you recommend that we\nabandon generative models in favor of joint embedding architectures? - [Yann] Yes.",
    "start": "5350440",
    "end": "5356109"
  },
  {
    "text": "- Abandon autoregressive generation. - [Yann] Yes. - Abandon... (laughs) This feels like court testimony.",
    "start": "5356109",
    "end": "5361780"
  },
  {
    "text": "Abandon probabilistic models in favor of energy based\nmodels, as we talked about. Abandon contrastive methods",
    "start": "5361780",
    "end": "5367870"
  },
  {
    "text": "in favor of regularized methods. And let me ask you about this;",
    "start": "5367870",
    "end": "5372103"
  },
  {
    "text": "you've been for a while, a\ncritic of reinforcement learning. - [Yann] Yes. - So the last recommendation\nis that we abandon RL",
    "start": "5373210",
    "end": "5381309"
  },
  {
    "text": "in favor of model predictive control, as you were talking about. And only use RL",
    "start": "5381310",
    "end": "5386560"
  },
  {
    "text": "when planning doesn't yield\nthe predicted outcome. And we use RL in that case",
    "start": "5386560",
    "end": "5392800"
  },
  {
    "text": "to adjust the world model or the critic. - [Yann] Yes. - So you've mentioned RLHF,",
    "start": "5392800",
    "end": "5400510"
  },
  {
    "text": "reinforcement learning\nwith human feedback. Why do you still hate\nreinforcement learning?",
    "start": "5400510",
    "end": "5405850"
  },
  {
    "text": "- [Yann] I don't hate\nreinforcement learning, and I think it's-\n- So it's all love? - I think it should not\nbe abandoned completely,",
    "start": "5405850",
    "end": "5412150"
  },
  {
    "text": "but I think it's use should be minimized because it's incredibly\ninefficient in terms of samples.",
    "start": "5412150",
    "end": "5418390"
  },
  {
    "text": "And so the proper way to train a system is to first have it learn",
    "start": "5418390",
    "end": "5424103"
  },
  {
    "text": "good representations of\nthe world and world models from mostly observation,",
    "start": "5424103",
    "end": "5429580"
  },
  {
    "text": "maybe a little bit of interactions. - And then steer it based on that. If the representation is good, then the adjustments should be minimal.",
    "start": "5429580",
    "end": "5436809"
  },
  {
    "text": "- Yeah. Now there's two things. If you've learned the world model, you can use the world model\nto plan a sequence of actions",
    "start": "5436810",
    "end": "5442690"
  },
  {
    "text": "to arrive at a particular objective. You don't need RL, unless the way you measure\nwhether you succeed",
    "start": "5442690",
    "end": "5450369"
  },
  {
    "text": "might be inexact. Your idea of whether you were\ngonna fall from your bike",
    "start": "5450370",
    "end": "5456260"
  },
  {
    "text": "might be wrong, or whether the person\nyou're fighting with MMA was gonna do something and they do something else. (laughing)",
    "start": "5458470",
    "end": "5465414"
  },
  {
    "text": "So there's two ways you can be wrong. Either your objective function",
    "start": "5465414",
    "end": "5472840"
  },
  {
    "text": "does not reflect the actual objective function\nyou want to optimize, or your world model is inaccurate, right?",
    "start": "5472840",
    "end": "5479800"
  },
  {
    "text": "So the prediction you were making about what was gonna happen\nin the world is inaccurate.",
    "start": "5479800",
    "end": "5485320"
  },
  {
    "text": "So if you want to adjust your world model while you are operating the world",
    "start": "5485320",
    "end": "5490930"
  },
  {
    "text": "or your objective function, that is basically in the realm of RL. This is what RL deals with\nto some extent, right?",
    "start": "5490930",
    "end": "5498763"
  },
  {
    "text": "So adjust your world model. And the way to adjust your\nworld model, even in advance,",
    "start": "5498763",
    "end": "5504177"
  },
  {
    "text": "is to explore parts of the\nspace with your world model, where you know that your\nworld model is inaccurate.",
    "start": "5504177",
    "end": "5510760"
  },
  {
    "text": "That's called curiosity\nbasically, or play, right? When you play, you kind of explore\npart of the state space",
    "start": "5510760",
    "end": "5518740"
  },
  {
    "text": "that you don't want to do for real",
    "start": "5518740",
    "end": "5523740"
  },
  {
    "text": "because it might be dangerous, but you can adjust your world model",
    "start": "5524179",
    "end": "5527930"
  },
  {
    "text": "without killing yourself\nbasically. (laughs) So that's what you want to use RL for.",
    "start": "5529870",
    "end": "5534957"
  },
  {
    "text": "When it comes time to\nlearning a particular task, you already have all the\ngood representations,",
    "start": "5534957",
    "end": "5540580"
  },
  {
    "text": "you already have your world model, but you need to adjust it\nfor the situation at hand. That's when you use RL.",
    "start": "5540580",
    "end": "5546670"
  },
  {
    "text": "- Why do you think RLHF works so well? This enforcement learning\nwith human feedback,",
    "start": "5546670",
    "end": "5552640"
  },
  {
    "text": "why did it have such a\ntransformational effect on large language models that came before?",
    "start": "5552640",
    "end": "5558308"
  },
  {
    "text": "- So what's had the\ntransformational effect is human feedback. There is many ways to use it",
    "start": "5558308",
    "end": "5563590"
  },
  {
    "text": "and some of it is just\npurely supervised, actually, it's not really reinforcement learning. - So it's the HF. (laughing)",
    "start": "5563590",
    "end": "5569053"
  },
  {
    "text": "- It's the HF. And then there is various ways\nto use human feedback, right? So you can ask humans to rate answers,",
    "start": "5569053",
    "end": "5576505"
  },
  {
    "text": "multiple answers that are\nproduced by a world model. And then what you do is you\ntrain an objective function",
    "start": "5576505",
    "end": "5585580"
  },
  {
    "text": "to predict that rating. And then you can use\nthat objective function",
    "start": "5585580",
    "end": "5591489"
  },
  {
    "text": "to predict whether an answer is good, and you can back propagate\nreally through this to fine tune your system",
    "start": "5591490",
    "end": "5596690"
  },
  {
    "text": "so that it only produces\nhighly rated answers. Okay?",
    "start": "5596690",
    "end": "5602224"
  },
  {
    "text": "So that's one way. So that's like in RL, that means training what's\ncalled a reward model, right?",
    "start": "5602224",
    "end": "5609369"
  },
  {
    "text": "So something that, basically your small neural net that estimates to what extent\nan answer is good, right?",
    "start": "5609370",
    "end": "5615099"
  },
  {
    "text": "It's very similar to the objective I was talking about earlier for planning, except now it's not used for planning,",
    "start": "5615100",
    "end": "5621310"
  },
  {
    "text": "it's used for fine tuning your system. I think it would be much more efficient to use it for planning,",
    "start": "5621310",
    "end": "5626469"
  },
  {
    "text": "but currently it's used to fine tune the parameters of the system.",
    "start": "5626470",
    "end": "5632620"
  },
  {
    "text": "Now, there's several ways to do this. Some of them are supervised. You just ask a human person,",
    "start": "5632620",
    "end": "5639700"
  },
  {
    "text": "like what is a good\nanswer for this, right? Then you just type the answer.",
    "start": "5639700",
    "end": "5644233"
  },
  {
    "text": "I mean, there's lots of ways that those systems are being adjusted.",
    "start": "5645614",
    "end": "5649153"
  },
  {
    "start": "5650000",
    "end": "6228000"
  },
  {
    "text": "- Now, a lot of people\nhave been very critical of the recently released\nGoogle's Gemini 1.5",
    "start": "5650740",
    "end": "5657830"
  },
  {
    "text": "for essentially, in my words,\nI could say super woke. Woke in the negative\nconnotation of that word.",
    "start": "5659080",
    "end": "5666579"
  },
  {
    "text": "There is some almost hilariously\nabsurd things that it does, like it modifies history,",
    "start": "5666580",
    "end": "5672760"
  },
  {
    "text": "like generating images of\na black George Washington or perhaps more seriously",
    "start": "5672760",
    "end": "5680575"
  },
  {
    "text": "something that you commented on Twitter, which is refusing to comment\non or generate images of,",
    "start": "5680575",
    "end": "5688260"
  },
  {
    "text": "or even descriptions of\nTiananmen Square or the tank men,",
    "start": "5689770",
    "end": "5694770"
  },
  {
    "text": "one of the most sort of legendary\nprotest images in history.",
    "start": "5695590",
    "end": "5700590"
  },
  {
    "text": "And of course, these\nimages are highly censored by the Chinese government.",
    "start": "5701200",
    "end": "5706780"
  },
  {
    "text": "And therefore everybody\nstarted asking questions of what is the process\nof designing these LLMs?",
    "start": "5706780",
    "end": "5714447"
  },
  {
    "text": "What is the role of censorship in these, and all that kind of stuff. So you commented on Twitter",
    "start": "5714741",
    "end": "5722290"
  },
  {
    "text": "saying that open source is the answer. (laughs)\n- Yeah. - Essentially. So can you explain?",
    "start": "5722290",
    "end": "5728233"
  },
  {
    "text": "- I actually made that comment on just about every social network I can. (Lex laughs) And I've made that point\nmultiple times in various forums.",
    "start": "5729520",
    "end": "5738933"
  },
  {
    "text": "Here's my point of view on this. People can complain that\nAI systems are biased,",
    "start": "5740770",
    "end": "5747310"
  },
  {
    "text": "and they generally are biased by the distribution of the training data that they've been trained on",
    "start": "5747310",
    "end": "5753800"
  },
  {
    "text": "that reflects biases in society. And that is potentially\noffensive to some people",
    "start": "5755110",
    "end": "5763909"
  },
  {
    "text": "or potentially not. And some techniques to de-bias",
    "start": "5765040",
    "end": "5770710"
  },
  {
    "text": "then become offensive to some people because of historical\nincorrectness and things like that.",
    "start": "5770710",
    "end": "5780420"
  },
  {
    "text": "And so you can ask the question. You can ask two questions. The first question is,",
    "start": "5783250",
    "end": "5788339"
  },
  {
    "text": "is it possible to produce an\nAI system that is not biased? And the answer is absolutely not.",
    "start": "5788339",
    "end": "5793390"
  },
  {
    "text": "And it's not because of\ntechnological challenges, although there are technological\nchallenges to that.",
    "start": "5793390",
    "end": "5801370"
  },
  {
    "text": "It's because bias is in\nthe eye of the beholder.",
    "start": "5801370",
    "end": "5805423"
  },
  {
    "text": "Different people may have different ideas about what constitutes\nbias for a lot of things.",
    "start": "5806800",
    "end": "5812737"
  },
  {
    "text": "I mean there are facts\nthat are indisputable, but there are a lot of opinions or things",
    "start": "5812737",
    "end": "5819730"
  },
  {
    "text": "that can be expressed in different ways. And so you cannot have an unbiased system, that's just an impossibility.",
    "start": "5819730",
    "end": "5826333"
  },
  {
    "text": "And so what's the answer to this? And the answer is the\nsame answer that we found",
    "start": "5828760",
    "end": "5836530"
  },
  {
    "text": "in liberal democracy about the press. The press needs to be free and diverse.",
    "start": "5836530",
    "end": "5844213"
  },
  {
    "text": "We have free speech for a good reason. It's because we don't want\nall of our information",
    "start": "5845440",
    "end": "5851937"
  },
  {
    "text": "to come from a unique source, 'cause that's opposite to\nthe whole idea of democracy",
    "start": "5851937",
    "end": "5860120"
  },
  {
    "text": "and progressive ideas\nand even science, right? In science, people have to\nargue for different opinions.",
    "start": "5861173",
    "end": "5868430"
  },
  {
    "text": "And science makes progress\nwhen people disagree and they come up with an answer and a consensus forms, right?",
    "start": "5868431",
    "end": "5874600"
  },
  {
    "text": "And it's true in all\ndemocracies around the world. So there is a future\nwhich is already happening",
    "start": "5874600",
    "end": "5882720"
  },
  {
    "text": "where every single one of our interaction with the digital world will be mediated by AI systems,",
    "start": "5883300",
    "end": "5890410"
  },
  {
    "text": "AI assistance, right? We're gonna have smart glasses. You can already buy them\nfrom Meta, (laughing)",
    "start": "5890410",
    "end": "5896890"
  },
  {
    "text": "the Ray-Ban Meta. Where you can talk to them and they are connected with an LLM and you can get answers\non any question you have.",
    "start": "5896890",
    "end": "5905920"
  },
  {
    "text": "Or you can be looking at a monument and there is a camera in\nthe system, in the glasses,",
    "start": "5905920",
    "end": "5911964"
  },
  {
    "text": "you can ask it like what can you tell me about this building or this monument? You can be looking at a\nmenu in a foreign language",
    "start": "5911964",
    "end": "5919120"
  },
  {
    "text": "and the thing we will\ntranslate it for you. We can do real time translation if we speak different languages.",
    "start": "5919120",
    "end": "5924760"
  },
  {
    "text": "So a lot of our interactions\nwith the digital world are going to be mediated by those systems",
    "start": "5924760",
    "end": "5929980"
  },
  {
    "text": "in the near future. Increasingly, the search\nengines that we're gonna use",
    "start": "5929980",
    "end": "5936943"
  },
  {
    "text": "are not gonna be search engines, they're gonna be dialogue systems that we just ask a question,",
    "start": "5936943",
    "end": "5944080"
  },
  {
    "text": "and it will answer and then point you to the perhaps appropriate\nreference for it.",
    "start": "5944080",
    "end": "5949690"
  },
  {
    "text": "But here is the thing, we cannot afford those systems to come from a handful of companies on the west coast of the US",
    "start": "5949690",
    "end": "5955300"
  },
  {
    "text": "because those systems will constitute the repository of all human knowledge. And we cannot have that be controlled",
    "start": "5957190",
    "end": "5965238"
  },
  {
    "text": "by a small number of people, right? It has to be diverse for the same reason the\npress has to be diverse.",
    "start": "5965238",
    "end": "5972190"
  },
  {
    "text": "So how do we get a diverse\nset of AI assistance? It's very expensive and difficult",
    "start": "5972190",
    "end": "5978730"
  },
  {
    "text": "to train a base model, right? A base LLM at the moment. In the future might be\nsomething different,",
    "start": "5978730",
    "end": "5983889"
  },
  {
    "text": "but at the moment that's an LLM. So only a few companies\ncan do this properly.",
    "start": "5983890",
    "end": "5989443"
  },
  {
    "text": "And if some of those\nsubsystems are open source,",
    "start": "5990520",
    "end": "5995520"
  },
  {
    "text": "anybody can use them, anybody can fine tune them. If we put in place some systems",
    "start": "5996100",
    "end": "6001590"
  },
  {
    "text": "that allows any group of people, whether they are individual citizens,",
    "start": "6001590",
    "end": "6010080"
  },
  {
    "text": "groups of citizens, government organizations, NGOs, companies, whatever,",
    "start": "6010080",
    "end": "6018000"
  },
  {
    "text": "to take those open source\nsystems, AI systems,",
    "start": "6018000",
    "end": "6023000"
  },
  {
    "text": "and fine tune them for their\nown purpose on their own data, there we're gonna have\na very large diversity",
    "start": "6023850",
    "end": "6029580"
  },
  {
    "text": "of different AI systems that are specialized for\nall of those things, right? So I'll tell you,",
    "start": "6029580",
    "end": "6035980"
  },
  {
    "text": "I talked to the French\ngovernment quite a bit and the French government will not accept",
    "start": "6035980",
    "end": "6041280"
  },
  {
    "text": "that the digital diet\nof all their citizens be controlled by three companies",
    "start": "6041280",
    "end": "6046350"
  },
  {
    "text": "on the west coast of the US. That's just not acceptable. It's a danger to democracy. Regardless of how well intentioned",
    "start": "6046350",
    "end": "6052537"
  },
  {
    "text": "those companies are, right? And it's also a danger to local culture,",
    "start": "6052537",
    "end": "6060960"
  },
  {
    "text": "to values, to language, right? I was talking with the\nfounder of Infosys in India.",
    "start": "6060960",
    "end": "6070310"
  },
  {
    "text": "He's funding a project\nto fine tune LLaMA 2, the open source model produced by Meta.",
    "start": "6073110",
    "end": "6079860"
  },
  {
    "text": "So that LLaMA 2 speaks all 22\nofficial languages in India. It's very important for people in India.",
    "start": "6079860",
    "end": "6086400"
  },
  {
    "text": "I was talking to a\nformer colleague of mine, Moustapha Cisse, who used to be a scientist at FAIR, and then moved back to Africa",
    "start": "6086400",
    "end": "6092430"
  },
  {
    "text": "and created a research\nlab for Google in Africa and now has a new startup Kera.",
    "start": "6092430",
    "end": "6097950"
  },
  {
    "text": "And what he's trying to\ndo is basically have LLM that speaks the local languages in Senegal so that people can have\naccess to medical information,",
    "start": "6097950",
    "end": "6106170"
  },
  {
    "text": "'cause they don't have access to doctors, it's a very small number of\ndoctors per capita in Senegal.",
    "start": "6106170",
    "end": "6112126"
  },
  {
    "text": "I mean, you can't have any of this unless you have open source platforms.",
    "start": "6112126",
    "end": "6118020"
  },
  {
    "text": "So with open source platforms, you can have AI systems that are not only diverse in\nterms of political opinions or things of that type,",
    "start": "6118020",
    "end": "6125070"
  },
  {
    "text": "but in terms of language,\nculture, value systems,",
    "start": "6125070",
    "end": "6130070"
  },
  {
    "text": "political opinions, technical\nabilities in various domains.",
    "start": "6131910",
    "end": "6136910"
  },
  {
    "text": "And you can have an industry, an ecosystem of companies that fine tune those open source systems",
    "start": "6138900",
    "end": "6144570"
  },
  {
    "text": "for vertical applications\nin industry, right? You have, I don't know, a\npublisher has thousands of books",
    "start": "6144570",
    "end": "6150270"
  },
  {
    "text": "and they want to build a system that allows a customer\nto just ask a question about the content of any of their books.",
    "start": "6150270",
    "end": "6157650"
  },
  {
    "text": "You need to train on their\nproprietary data, right? You have a company, we have one within Meta\nit's called Meta Mate.",
    "start": "6157650",
    "end": "6164460"
  },
  {
    "text": "And it's basically an LLM that can answer any question about internal stuff\nabout about the company.",
    "start": "6164460",
    "end": "6172080"
  },
  {
    "text": "Very useful. A lot of companies want this, right? A lot of companies want this\nnot just for their employees,",
    "start": "6172080",
    "end": "6177870"
  },
  {
    "text": "but also for their customers, to take care of their customers. So the only way you're\ngonna have an AI industry,",
    "start": "6177870",
    "end": "6184350"
  },
  {
    "text": "the only way you're gonna have AI systems that are not uniquely biased, is if you have open source platforms",
    "start": "6184350",
    "end": "6190290"
  },
  {
    "text": "on top of which any group can\nbuild specialized systems.",
    "start": "6190290",
    "end": "6195290"
  },
  {
    "text": "So the inevitable direction of history",
    "start": "6196710",
    "end": "6201710"
  },
  {
    "text": "is that the vast majority of AI systems will be built on top of\nopen source platforms.",
    "start": "6202394",
    "end": "6208440"
  },
  {
    "text": "- So that's a beautiful vision. So meaning like a company\nlike Meta or Google or so on,",
    "start": "6208440",
    "end": "6215180"
  },
  {
    "text": "should take only minimal fine tuning steps after the building, the\nfoundation, pre-trained model.",
    "start": "6217920",
    "end": "6224909"
  },
  {
    "text": "As few steps as possible. - Basically. (Lex sighs) - Can Meta afford to do that?",
    "start": "6224910",
    "end": "6231540"
  },
  {
    "start": "6228000",
    "end": "6446000"
  },
  {
    "text": "- No. - So I don't know if you know this, but companies are supposed\nto make money somehow. And open source is like giving away...",
    "start": "6231540",
    "end": "6240750"
  },
  {
    "text": "I don't know, Mark made a video, Mark Zuckerberg. A very sexy video talking\nabout 350,000 Nvidia H100s.",
    "start": "6240750",
    "end": "6248923"
  },
  {
    "text": "The math of that is, just for the GPUs,\nthat's a hundred billion,",
    "start": "6252570",
    "end": "6257733"
  },
  {
    "text": "plus the infrastructure\nfor training everything. So I'm no business guy,",
    "start": "6259800",
    "end": "6266070"
  },
  {
    "text": "but how do you make money on that? So the vision you paint\nis a really powerful one, but how is it possible to make money?",
    "start": "6266070",
    "end": "6272610"
  },
  {
    "text": "- Okay. So you have several\nbusiness models, right? The business model that\nMeta is built around",
    "start": "6272610",
    "end": "6279639"
  },
  {
    "text": "is you offer a service, and the financing of that service",
    "start": "6280620",
    "end": "6288719"
  },
  {
    "text": "is either through ads or\nthrough business customers. So for example, if you have an LLM",
    "start": "6288720",
    "end": "6294989"
  },
  {
    "text": "that can help a mom-and-pop pizza place",
    "start": "6294990",
    "end": "6298960"
  },
  {
    "text": "by talking to their\ncustomers through WhatsApp, and so the customers\ncan just order a pizza",
    "start": "6300540",
    "end": "6305971"
  },
  {
    "text": "and the system will just ask them, like what topping do you want\nor what size, blah blah, blah.",
    "start": "6305971",
    "end": "6311047"
  },
  {
    "text": "The business will pay for that. Okay? That's a model.",
    "start": "6312570",
    "end": "6315736"
  },
  {
    "text": "And otherwise, if it's a system that is on the more kind\nof classical services, it can be ad supported or\nthere's several models.",
    "start": "6319440",
    "end": "6328139"
  },
  {
    "text": "But the point is, if you have a big enough\npotential customer base",
    "start": "6328140",
    "end": "6334679"
  },
  {
    "text": "and you need to build that\nsystem anyway for them,",
    "start": "6334680",
    "end": "6339003"
  },
  {
    "text": "it doesn't hurt you to actually distribute it to open source. - Again, I'm no business guy,",
    "start": "6340140",
    "end": "6345420"
  },
  {
    "text": "but if you release the open source model, then other people can\ndo the same kind of task",
    "start": "6345420",
    "end": "6351780"
  },
  {
    "text": "and compete on it. Basically provide fine\ntuned models for businesses,",
    "start": "6351780",
    "end": "6357026"
  },
  {
    "text": "is the bet that Meta is making... By the way, I'm a huge fan of all this. But is the bet that Meta is making",
    "start": "6357026",
    "end": "6363510"
  },
  {
    "text": "is like, \"we'll do a better job of it?\" - Well, no. The bet is more, we already have a huge user\nbase and customer base.",
    "start": "6363510",
    "end": "6373110"
  },
  {
    "text": "- [Lex] Ah, right.\n- Right? So it's gonna be useful to them. Whatever we offer them is gonna be useful and there is a way to\nderive revenue from this.",
    "start": "6373110",
    "end": "6381000"
  },
  {
    "text": "- [Lex] Sure. - And it doesn't hurt that we provide that system\nor the base model, right?",
    "start": "6381000",
    "end": "6388280"
  },
  {
    "text": "The foundation model in open source for others to build\napplications on top of it too.",
    "start": "6389400",
    "end": "6395820"
  },
  {
    "text": "If those applications turn out to be useful for our customers, we can just buy it for them.",
    "start": "6395820",
    "end": "6399843"
  },
  {
    "text": "It could be that they\nwill improve the platform. In fact, we see this already. I mean there is literally\nmillions of downloads of LLaMA 2",
    "start": "6402270",
    "end": "6410820"
  },
  {
    "text": "and thousands of people\nwho have provided ideas about how to make it better. So this clearly accelerates progress",
    "start": "6410820",
    "end": "6418815"
  },
  {
    "text": "to make the system available to sort of a wide community of people.",
    "start": "6418815",
    "end": "6425423"
  },
  {
    "text": "And there is literally\nthousands of businesses who are building applications with it.",
    "start": "6425430",
    "end": "6429699"
  },
  {
    "text": "Meta's ability to derive\nrevenue from this technology is not impaired by the distribution",
    "start": "6435630",
    "end": "6444032"
  },
  {
    "text": "of base models in open source. - The fundamental criticism\nthat Gemini is getting is that, as you pointed\nout on the west coast...",
    "start": "6444210",
    "end": "6451060"
  },
  {
    "start": "6446000",
    "end": "6598000"
  },
  {
    "text": "Just to clarify, we're currently in the east coast, where I would suppose Meta\nAI headquarters would be.",
    "start": "6451060",
    "end": "6458511"
  },
  {
    "text": "(laughs) So strong words about the west coast. But I guess the issue that happens is,",
    "start": "6458511",
    "end": "6466950"
  },
  {
    "text": "I think it's fair to say\nthat most tech people have a political affiliation\nwith the left wing.",
    "start": "6466950",
    "end": "6473550"
  },
  {
    "text": "They lean left. And so the problem that people\nare criticizing Gemini with is that in that de-biasing\nprocess that you mentioned,",
    "start": "6473550",
    "end": "6482580"
  },
  {
    "text": "that their ideological\nlean becomes obvious.",
    "start": "6482580",
    "end": "6487457"
  },
  {
    "text": "Is this something that could be escaped? You're saying open source is the only way?",
    "start": "6489990",
    "end": "6496770"
  },
  {
    "text": "- [Yann] Yeah. - Have you witnessed this\nkind of ideological lean that makes engineering difficult?",
    "start": "6496770",
    "end": "6502380"
  },
  {
    "text": "- No, I don't think it has to do... I don't think the issue has to do with the political leaning of the people designing those systems.",
    "start": "6502380",
    "end": "6509340"
  },
  {
    "text": "It has to do with the\nacceptability or political leanings of their customer base or audience, right?",
    "start": "6509340",
    "end": "6518340"
  },
  {
    "text": "So a big company cannot afford\nto offend too many people.",
    "start": "6518340",
    "end": "6523340"
  },
  {
    "text": "So they're going to make sure that whatever product\nthey put out is \"safe,\"",
    "start": "6523680",
    "end": "6529440"
  },
  {
    "text": "whatever that means. And it's very possible to overdo it.",
    "start": "6529440",
    "end": "6535807"
  },
  {
    "text": "And it's also very possible to... It's impossible to do it\nproperly for everyone. You're not going to satisfy everyone.",
    "start": "6535807",
    "end": "6542520"
  },
  {
    "text": "So that's what I said before, you cannot have a system that is unbiased and is perceived as unbiased by everyone.",
    "start": "6542520",
    "end": "6547743"
  },
  {
    "text": "It's gonna be, you push it in one way, one set of people are\ngonna see it as biased.",
    "start": "6548700",
    "end": "6554287"
  },
  {
    "text": "And then you push it the other way and another set of people\nis gonna see it as biased. And then in addition to this,",
    "start": "6554288",
    "end": "6559493"
  },
  {
    "text": "there's the issue of\nif you push the system perhaps a little too far in one direction, it's gonna be non-factual, right?",
    "start": "6559493",
    "end": "6565889"
  },
  {
    "text": "You're gonna have black Nazi soldiers in-",
    "start": "6565890",
    "end": "6570890"
  },
  {
    "text": "- Yeah. So we should mention image generation of black Nazi soldiers,",
    "start": "6570988",
    "end": "6576281"
  },
  {
    "text": "which is not factually accurate. - Right. And can be offensive for\nsome people as well, right?",
    "start": "6576281",
    "end": "6582363"
  },
  {
    "text": "So it's gonna be impossible to kind of produce systems\nthat are unbiased for everyone. So the only solution\nthat I see is diversity.",
    "start": "6584266",
    "end": "6593040"
  },
  {
    "text": "- And diversity in full\nmeaning of that word, diversity in every possible way. - [Yann] Yeah.",
    "start": "6593040",
    "end": "6598927"
  },
  {
    "start": "6598000",
    "end": "7076000"
  },
  {
    "text": "- Marc Andreessen just tweeted today,",
    "start": "6598928",
    "end": "6602703"
  },
  {
    "text": "let me do a TL;DR. The conclusion is only\nstartups and open source can avoid the issue that he's\nhighlighting with big tech.",
    "start": "6604200",
    "end": "6612273"
  },
  {
    "text": "He's asking, can big tech actually field\ngenerative AI products? One, ever escalating demands\nfrom internal activists,",
    "start": "6613170",
    "end": "6620790"
  },
  {
    "text": "employee mobs, crazed executives, broken boards, pressure groups, extremist regulators,\ngovernment agencies, the press,",
    "start": "6620790",
    "end": "6628230"
  },
  {
    "text": "in quotes \"experts,\" and everything corrupting the output.",
    "start": "6628230",
    "end": "6634290"
  },
  {
    "text": "Two, constant risk of\ngenerating a bad answer or drawing a bad picture\nor rendering a bad video.",
    "start": "6634290",
    "end": "6640623"
  },
  {
    "text": "Who knows what it's going\nto say or do at any moment? Three, legal exposure,\nproduct liability, slander,",
    "start": "6641490",
    "end": "6648180"
  },
  {
    "text": "election law, many other things and so on. Anything that makes Congress mad.",
    "start": "6648180",
    "end": "6653943"
  },
  {
    "text": "Four, continuous attempts to tighten grip on acceptable output, degrade the model, like how good it actually is",
    "start": "6654990",
    "end": "6661895"
  },
  {
    "text": "in terms of usable and\npleasant to use and effective and all that kind of stuff.",
    "start": "6661895",
    "end": "6666960"
  },
  {
    "text": "And five, publicity of\nbad text, images, video, actual puts those examples\ninto the training data",
    "start": "6666960",
    "end": "6673110"
  },
  {
    "text": "for the next version. And so on. So he just highlights\nhow difficult this is.",
    "start": "6673110",
    "end": "6678420"
  },
  {
    "text": "From all kinds of people being unhappy. He just said you can't create a system that makes everybody happy.",
    "start": "6678420",
    "end": "6684750"
  },
  {
    "text": "- [Yann] Yes. - So if you're going to do\nthe fine tuning yourself and keep a close source,",
    "start": "6684750",
    "end": "6690873"
  },
  {
    "text": "essentially the problem there is then trying to minimize\nthe number of people who are going to be unhappy. - [Yann] Yeah.",
    "start": "6691980",
    "end": "6698159"
  },
  {
    "text": "- And you're saying like the only... That that's almost\nimpossible to do, right? And the better way is to do open source.",
    "start": "6698160",
    "end": "6704763"
  },
  {
    "text": "- Basically, yeah. I mean Marc is right about a\nnumber of things that he lists",
    "start": "6705690",
    "end": "6711710"
  },
  {
    "text": "that indeed scare large companies.",
    "start": "6711990",
    "end": "6715353"
  },
  {
    "text": "Certainly, congressional\ninvestigations is one of them. Legal liability.",
    "start": "6717078",
    "end": "6721593"
  },
  {
    "text": "Making things that get people to hurt\nthemselves or hurt others. Like big companies are really careful",
    "start": "6724290",
    "end": "6732630"
  },
  {
    "text": "about not producing things of this type,",
    "start": "6732630",
    "end": "6735242"
  },
  {
    "text": "because they have... They don't want to hurt\nanyone, first of all. And then second, they wanna\npreserve their business.",
    "start": "6738060",
    "end": "6743220"
  },
  {
    "text": "So it's essentially impossible\nfor systems like this that can inevitably\nformulate political opinions",
    "start": "6743220",
    "end": "6750989"
  },
  {
    "text": "and opinions about various things that may be political or not, but that people may disagree about.",
    "start": "6750990",
    "end": "6756239"
  },
  {
    "text": "About, you know, moral issues and things about like\nquestions about religion",
    "start": "6756240",
    "end": "6762929"
  },
  {
    "text": "and things like that, right? Or cultural issues that people from different communities would disagree with in the first place.",
    "start": "6763470",
    "end": "6770160"
  },
  {
    "text": "So there's only kind of a\nrelatively small number of things that people will sort of agree on,",
    "start": "6770160",
    "end": "6775963"
  },
  {
    "text": "basic principles. But beyond that, if you want those systems to be useful,",
    "start": "6775963",
    "end": "6781890"
  },
  {
    "text": "they will necessarily have\nto offend a number of people,",
    "start": "6781890",
    "end": "6786890"
  },
  {
    "text": "inevitably. - And so open source is just better- - [Yann] Diversity is better, right?",
    "start": "6787440",
    "end": "6792856"
  },
  {
    "text": "- And open source enables diversity. - That's right. Open source enables diversity. - This can be a fascinating world",
    "start": "6792856",
    "end": "6799679"
  },
  {
    "text": "where if it's true that\nthe open source world, if Meta leads the way and creates this kind of open\nsource foundation model world,",
    "start": "6799680",
    "end": "6807630"
  },
  {
    "text": "there's going to be, like governments will have a\nfine tuned model. (laughing) - [Yann] Yeah.",
    "start": "6807630",
    "end": "6813199"
  },
  {
    "text": "- And then potentially, people that vote left and right",
    "start": "6813199",
    "end": "6819210"
  },
  {
    "text": "will have their own model and preference to be able to choose. And it will potentially\ndivide us even more but that's on us humans.",
    "start": "6819210",
    "end": "6826980"
  },
  {
    "text": "We get to figure out... Basically the technology enables humans to human more effectively.",
    "start": "6826980",
    "end": "6833430"
  },
  {
    "text": "And all the difficult ethical\nquestions that humans raise we'll just leave it up\nto us to figure that out.",
    "start": "6833430",
    "end": "6842300"
  },
  {
    "text": "- Yeah, I mean there are\nsome limits to what... The same way there are\nlimits to free speech, there has to be some\nlimit to the kind of stuff",
    "start": "6842640",
    "end": "6848940"
  },
  {
    "text": "that those systems might\nbe authorized to produce,",
    "start": "6848940",
    "end": "6853940"
  },
  {
    "text": "some guardrails. So I mean, that's one thing\nI've been interested in, which is in the type of architecture",
    "start": "6855480",
    "end": "6860969"
  },
  {
    "text": "that we were discussing before, where the output of the system",
    "start": "6860970",
    "end": "6866160"
  },
  {
    "text": "is a result of an inference\nto satisfy an objective. That objective can include guardrails.",
    "start": "6866160",
    "end": "6872013"
  },
  {
    "text": "And we can put guardrails\nin open source systems. I mean, if we eventually have systems",
    "start": "6872940",
    "end": "6879090"
  },
  {
    "text": "that are built with this blueprint, we can put guardrails in those systems that guarantee",
    "start": "6879090",
    "end": "6884760"
  },
  {
    "text": "that there is sort of a\nminimum set of guardrails that make the system non-dangerous\nand non-toxic, et cetera.",
    "start": "6884760",
    "end": "6890839"
  },
  {
    "text": "Basic things that\neverybody would agree on. And then the fine tuning\nthat people will add",
    "start": "6890839",
    "end": "6897944"
  },
  {
    "text": "or the additional guardrails\nthat people will add will kind of cater to their\ncommunity, whatever it is.",
    "start": "6897944",
    "end": "6904961"
  },
  {
    "text": "- And yeah, the fine tuning would be more about the gray\nareas of what is hate speech, what is dangerous and\nall that kind of stuff.",
    "start": "6904980",
    "end": "6911490"
  },
  {
    "text": "I mean, you've- - [Yann] Or different value systems. - Different value systems. But still even with the objectives",
    "start": "6911490",
    "end": "6916800"
  },
  {
    "text": "of how to build a bio weapon, for example, I think something you've commented on, or at least there's a paper",
    "start": "6916800",
    "end": "6923280"
  },
  {
    "text": "where a collection of researchers is trying to understand the\nsocial impacts of these LLMs.",
    "start": "6923280",
    "end": "6928113"
  },
  {
    "text": "And I guess one threshold that's nice is like does the LLM make it\nany easier than a search would,",
    "start": "6929370",
    "end": "6936920"
  },
  {
    "text": "like a Google search would? - Right. So the increasing number\nof studies on this",
    "start": "6938010",
    "end": "6944550"
  },
  {
    "text": "seems to point to the\nfact that it doesn't help. So having an LLM doesn't help you",
    "start": "6944550",
    "end": "6952043"
  },
  {
    "text": "design or build a bio\nweapon or a chemical weapon",
    "start": "6952043",
    "end": "6957043"
  },
  {
    "text": "if you already have access to\na search engine and a library. And so the sort of increased\ninformation you get",
    "start": "6957240",
    "end": "6964586"
  },
  {
    "text": "or the ease with which you get\nit doesn't really help you. That's the first thing. The second thing is,",
    "start": "6964586",
    "end": "6970290"
  },
  {
    "text": "it's one thing to have\na list of instructions of how to make a chemical weapon,\nfor example, a bio weapon.",
    "start": "6970290",
    "end": "6977070"
  },
  {
    "text": "It's another thing to actually build it. And it's much harder than you might think, and then LLM will not help you with that.",
    "start": "6977070",
    "end": "6983825"
  },
  {
    "text": "In fact, nobody in the world, not even like countries use bio weapons because most of the time they have no idea",
    "start": "6985710",
    "end": "6991887"
  },
  {
    "text": "how to protect their own\npopulations against it. So it's too dangerous\nactually to kind of ever use.",
    "start": "6991887",
    "end": "6999156"
  },
  {
    "text": "And it's in fact banned\nby international treaties. Chemical weapons is different.",
    "start": "6999270",
    "end": "7005230"
  },
  {
    "text": "It's also banned by treaties, but it's the same problem.",
    "start": "7005230",
    "end": "7010730"
  },
  {
    "text": "It's difficult to use in situations that doesn't\nturn against the perpetrators.",
    "start": "7010730",
    "end": "7016490"
  },
  {
    "text": "But we could ask Elon Musk. Like I can give you a very\nprecise list of instructions of how you build a rocket engine.",
    "start": "7016490",
    "end": "7023393"
  },
  {
    "text": "And even if you have\na team of 50 engineers that are really experienced building it, you're still gonna have\nto blow up a dozen of them",
    "start": "7024350",
    "end": "7030140"
  },
  {
    "text": "before you get one that works. And it's the same with\nchemical weapons or bio weapons",
    "start": "7030140",
    "end": "7038050"
  },
  {
    "text": "or things like this. It requires expertise in the real world that the LLM is not gonna help you with.",
    "start": "7038710",
    "end": "7045260"
  },
  {
    "text": "- And it requires even\nthe common sense expertise that we've been talking about, which is how to take\nlanguage based instructions",
    "start": "7045260",
    "end": "7054050"
  },
  {
    "text": "and materialize them in the physical world requires a lot of knowledge\nthat's not in the instructions.",
    "start": "7054050",
    "end": "7061610"
  },
  {
    "text": "- Yeah, exactly. A lot of biologists have\nposted on this actually in response to those things saying like do you realize how hard it is",
    "start": "7061610",
    "end": "7067520"
  },
  {
    "text": "to actually do the lab work? Like this is not trivial. - Yeah.",
    "start": "7067520",
    "end": "7072733"
  },
  {
    "text": "And that's Hans Moravec\ncomes to light once again. Just to linger on LLaMA.",
    "start": "7072733",
    "end": "7079040"
  },
  {
    "start": "7076000",
    "end": "7460000"
  },
  {
    "text": "Mark announced that LLaMA\n3 is coming out eventually, I don't think there's a release date, but what are you most excited about?",
    "start": "7079040",
    "end": "7086929"
  },
  {
    "text": "First of all, LLaMA 2\nthat's already out there, and maybe the future LLaMA 3, 4, 5, 6, 10,",
    "start": "7086930",
    "end": "7092810"
  },
  {
    "text": "just the future of the\nopen source under Meta? - Well, a number of things.",
    "start": "7092810",
    "end": "7098143"
  },
  {
    "text": "So there's gonna be like\nvarious versions of LLaMA that are improvements of previous LLaMAs.",
    "start": "7098143",
    "end": "7106940"
  },
  {
    "text": "Bigger, better, multimodal,\nthings like that. And then in future generations,",
    "start": "7106940",
    "end": "7112070"
  },
  {
    "text": "systems that are capable of planning, that really understand\nhow the world works, maybe are trained from video\nso they have some world model.",
    "start": "7112070",
    "end": "7119600"
  },
  {
    "text": "Maybe capable of the type\nof reasoning and planning I was talking about earlier. Like how long is that gonna take?",
    "start": "7119600",
    "end": "7125360"
  },
  {
    "text": "Like when is the research that\nis going in that direction going to sort of feed into\nthe product line, if you want,",
    "start": "7125360",
    "end": "7132590"
  },
  {
    "text": "of LLaMA? I don't know, I can't tell you. And there's a few breakthroughs that we have to basically go through",
    "start": "7132590",
    "end": "7139700"
  },
  {
    "text": "before we can get there. But you'll be able to monitor our progress because we publish our research, right?",
    "start": "7139700",
    "end": "7147050"
  },
  {
    "text": "So last week we published the V-JEPA work, which is sort of a first step",
    "start": "7147050",
    "end": "7153260"
  },
  {
    "text": "towards training systems from video. And then the next step\nis gonna be world models",
    "start": "7153260",
    "end": "7158990"
  },
  {
    "text": "based on kind of this type of idea, training from video. There's similar work at\nDeepMind also taking place,",
    "start": "7158990",
    "end": "7168149"
  },
  {
    "text": "and also at UC Berkeley\non world models and video.",
    "start": "7168149",
    "end": "7173149"
  },
  {
    "text": "A lot of people are working on this. I think a lot of good ideas are appearing. My bet is that those systems\nare gonna be JEPA-like,",
    "start": "7173840",
    "end": "7181790"
  },
  {
    "text": "they're not gonna be generative models. And we'll see what the future will tell.",
    "start": "7181790",
    "end": "7189635"
  },
  {
    "text": "There's really good work at... A gentleman called Danijar\nHafner who is now DeepMind,",
    "start": "7189636",
    "end": "7196520"
  },
  {
    "text": "who's worked on kind\nof models of this type that learn representations and then use them for\nplanning or learning tasks",
    "start": "7196520",
    "end": "7202772"
  },
  {
    "text": "by reinforcement training. And a lot of work at Berkeley by Pieter Abbeel, Sergey Levine,",
    "start": "7202772",
    "end": "7211145"
  },
  {
    "text": "a bunch of other people of that type. I'm collaborating with actually in the context of some\ngrants with my NYU hat.",
    "start": "7211145",
    "end": "7218190"
  },
  {
    "text": "And then collaborations also through Meta, 'cause the lab at Berkeley is associated with Meta\nin some way, with FAIR.",
    "start": "7220010",
    "end": "7228320"
  },
  {
    "text": "So I think it's very exciting. I think I'm super excited about...",
    "start": "7228320",
    "end": "7234230"
  },
  {
    "text": "I haven't been that excited about like the direction\nof machine learning and AI since 10 years ago when FAIR was started,",
    "start": "7234230",
    "end": "7241998"
  },
  {
    "text": "and before that, 30 years ago, when we were working on, sorry 35, on combination nets and the\nearly days of neural net.",
    "start": "7241998",
    "end": "7251341"
  },
  {
    "text": "So I'm super excited because I see a path towards",
    "start": "7251341",
    "end": "7257600"
  },
  {
    "text": "potentially human level intelligence with systems that can\nunderstand the world,",
    "start": "7257600",
    "end": "7264140"
  },
  {
    "text": "remember, plan, reason. There is some set of ideas\nto make progress there",
    "start": "7264140",
    "end": "7269716"
  },
  {
    "text": "that might have a chance of working. And I'm really excited about this. What I like is that",
    "start": "7269716",
    "end": "7275610"
  },
  {
    "text": "somewhat we get onto like a good direction and perhaps succeed before my\nbrain turns to a white sauce",
    "start": "7278399",
    "end": "7284900"
  },
  {
    "text": "or before I need to retire. (laughs) - Yeah. Yeah.",
    "start": "7284900",
    "end": "7290233"
  },
  {
    "text": "Are you also excited by... Is it beautiful to you just\nthe amount of GPUs involved,",
    "start": "7290233",
    "end": "7298040"
  },
  {
    "text": "sort of the whole training\nprocess on this much compute? Just zooming out,",
    "start": "7298040",
    "end": "7303931"
  },
  {
    "text": "just looking at earth and humans together have built these computing devices",
    "start": "7303931",
    "end": "7309739"
  },
  {
    "text": "and are able to train this one brain, we then open source.",
    "start": "7309740",
    "end": "7316431"
  },
  {
    "text": "(laughs) Like giving birth to\nthis open source brain trained on this gigantic compute system.",
    "start": "7316431",
    "end": "7324350"
  },
  {
    "text": "There's just the details\nof how to train on that, how to build the infrastructure\nand the hardware,",
    "start": "7324350",
    "end": "7330080"
  },
  {
    "text": "the cooling, all of this kind of stuff. Are you just still the\nmost of your excitement is in the theory aspect of it?",
    "start": "7330080",
    "end": "7336900"
  },
  {
    "text": "Meaning like the software. - Well, I used to be a\nhardware guy many years ago. (laughs)\n- Yes, yes, that's right.",
    "start": "7336900",
    "end": "7342440"
  },
  {
    "text": "- Decades ago. - Hardware has improved a little bit. Changed a little bit, yeah.",
    "start": "7342440",
    "end": "7347810"
  },
  {
    "text": "- I mean, certainly scale is\nnecessary but not sufficient. - [Lex] Absolutely.",
    "start": "7347810",
    "end": "7353233"
  },
  {
    "text": "- So we certainly need computation. I mean, we're still far\nin terms of compute power from what we would need",
    "start": "7353233",
    "end": "7359630"
  },
  {
    "text": "to match the compute\npower of the human brain. This may occur in the next couple decades,",
    "start": "7359630",
    "end": "7365448"
  },
  {
    "text": "but we're still some ways away. And certainly in terms\nof power efficiency, we're really far.",
    "start": "7365448",
    "end": "7370793"
  },
  {
    "text": "So a lot of progress to make in hardware. And right now a lot of\nthe progress is not...",
    "start": "7371960",
    "end": "7380269"
  },
  {
    "text": "I mean, there's a bit coming\nfrom Silicon technology, but a lot of it coming from\narchitectural innovation",
    "start": "7380270",
    "end": "7386480"
  },
  {
    "text": "and quite a bit coming from\nlike more efficient ways of implementing the architectures\nthat have become popular.",
    "start": "7386480",
    "end": "7393620"
  },
  {
    "text": "Basically combination of\ntransformers and com net, right? And so there's still some ways to go",
    "start": "7393620",
    "end": "7402260"
  },
  {
    "text": "until we are going to saturate. We're gonna have to come up",
    "start": "7402260",
    "end": "7408380"
  },
  {
    "text": "with like new principles,\nnew fabrication technology, new basic components,",
    "start": "7408380",
    "end": "7414503"
  },
  {
    "text": "perhaps based on sort\nof different principles than those classical digital CMOS.",
    "start": "7415713",
    "end": "7421969"
  },
  {
    "text": "- Interesting. So you think in order to build AmI, ami,",
    "start": "7421970",
    "end": "7426353"
  },
  {
    "text": "we potentially might need\nsome hardware innovation too? - Well, if we wanna make it ubiquitous,",
    "start": "7428270",
    "end": "7435679"
  },
  {
    "text": "yeah, certainly. Because we're gonna have to\nreduce the power consumption.",
    "start": "7435680",
    "end": "7441580"
  },
  {
    "text": "A GPU today, right? Is half a kilowatt to a kilowatt. Human brain is about 25 watts.",
    "start": "7442190",
    "end": "7448703"
  },
  {
    "text": "And the GPU is way below\nthe power of human brain. You need something like a hundred thousand or a million to match it.",
    "start": "7449904",
    "end": "7456368"
  },
  {
    "text": "So we are off by a huge factor.",
    "start": "7456369",
    "end": "7459773"
  },
  {
    "start": "7460000",
    "end": "7728000"
  },
  {
    "text": "- You often say that\nAGI is not coming soon. Meaning like not this year,\nnot the next few years,",
    "start": "7461450",
    "end": "7470270"
  },
  {
    "text": "potentially farther away. What's your basic intuition behind that?",
    "start": "7470270",
    "end": "7475760"
  },
  {
    "text": "- So first of all, it's\nnot to be an event, right? The idea somehow which is popularized by\nscience fiction in Hollywood",
    "start": "7475760",
    "end": "7482819"
  },
  {
    "text": "that somehow somebody is\ngonna discover the secret, the secret to AGI or\nhuman level AI or AmI,",
    "start": "7482819",
    "end": "7490504"
  },
  {
    "text": "whatever you wanna call it, and then turn on a machine\nand then we have AGI. That's just not going to happen.",
    "start": "7490504",
    "end": "7497120"
  },
  {
    "text": "It's not going to be an event. It's gonna be gradual progress.",
    "start": "7497120",
    "end": "7502643"
  },
  {
    "text": "Are we gonna have systems that can learn from\nvideo how the world works and learn good representations?",
    "start": "7503630",
    "end": "7509450"
  },
  {
    "text": "Yeah. Before we get them to\nthe scale and performance that we observe in humans, it's gonna take quite a while.",
    "start": "7509450",
    "end": "7515630"
  },
  {
    "text": "It's not gonna happen in one day. Are we gonna get systems that can have large amount\nof associated memories",
    "start": "7515630",
    "end": "7524215"
  },
  {
    "text": "so they can remember stuff? Yeah. But same, it's not gonna happen tomorrow. I mean, there is some basic techniques",
    "start": "7524215",
    "end": "7530510"
  },
  {
    "text": "that need to be developed. We have a lot of them, but like to get this to work\ntogether with a full system",
    "start": "7530510",
    "end": "7536120"
  },
  {
    "text": "is another story. Are we gonna have systems\nthat can reason and plan, perhaps along the lines of\nobjective driven AI architectures",
    "start": "7536120",
    "end": "7543110"
  },
  {
    "text": "that I described before? Yeah, but like before we\nget this to work properly, it's gonna take a while.",
    "start": "7543110",
    "end": "7548483"
  },
  {
    "text": "And before we get all those\nthings to work together. And then on top of this, have systems that can learn\nlike hierarchical planning,",
    "start": "7549320",
    "end": "7555139"
  },
  {
    "text": "hierarchical representations, systems that can be configured for a lot of different situation at hands",
    "start": "7555140",
    "end": "7560620"
  },
  {
    "text": "the way the human brain can. All of this is gonna\ntake at least a decade,",
    "start": "7560620",
    "end": "7567523"
  },
  {
    "text": "probably much more, because there are a lot of problems that we're not seeing right now",
    "start": "7567523",
    "end": "7572662"
  },
  {
    "text": "that we have not encountered. And so we don't know if\nthere is an easy solution within this framework.",
    "start": "7572663",
    "end": "7578573"
  },
  {
    "text": "It's not just around the corner. I mean, I've been hearing\npeople for the last 12, 15 years",
    "start": "7581600",
    "end": "7587540"
  },
  {
    "text": "claiming that AGI is\njust around the corner and being systematically wrong.",
    "start": "7587540",
    "end": "7592580"
  },
  {
    "text": "And I knew they were wrong\nwhen they were saying it. I called it bullshit. (laughs) - Why do you think people\nhave been calling...",
    "start": "7592580",
    "end": "7598160"
  },
  {
    "text": "First of all, I mean,\nfrom the beginning of, from the birth of the term\nartificial intelligence, there has been an eternal optimism",
    "start": "7598160",
    "end": "7605280"
  },
  {
    "text": "that's perhaps unlike other technologies. Is it Moravec's paradox?",
    "start": "7606200",
    "end": "7611780"
  },
  {
    "text": "Is it the explanation for why people are so\noptimistic about AGI?",
    "start": "7611780",
    "end": "7616969"
  },
  {
    "text": "- I don't think it's\njust Moravec's paradox. Moravec's paradox is a consequence of realizing that the world\nis not as easy as we think.",
    "start": "7616970",
    "end": "7623720"
  },
  {
    "text": "So first of all, intelligence\nis not a linear thing that you can measure with a scaler,",
    "start": "7623720",
    "end": "7630530"
  },
  {
    "text": "with a single number. Can you say that humans are\nsmarter than orangutans?",
    "start": "7630530",
    "end": "7637736"
  },
  {
    "text": "In some ways, yes, but in some ways orangutans\nare smarter than humans in a lot of domains",
    "start": "7638330",
    "end": "7643790"
  },
  {
    "text": "that allows them to survive\nin the forest, (laughing) for example. - So IQ is a very limited\nmeasure of intelligence.",
    "start": "7643790",
    "end": "7650360"
  },
  {
    "text": "True intelligence is bigger than what IQ,\nfor example, measures. - Well, IQ can measure\napproximately something for humans,",
    "start": "7650360",
    "end": "7658762"
  },
  {
    "text": "but because humans kind of come in relatively kind of uniform form, right?",
    "start": "7659660",
    "end": "7668410"
  },
  {
    "text": "- [Lex] Yeah. - But it only measures one type of ability that may be relevant for\nsome tasks, but not others.",
    "start": "7668410",
    "end": "7676613"
  },
  {
    "text": "But then if you are talking\nabout other intelligent entities for which the basic things\nthat are easy to them",
    "start": "7678920",
    "end": "7687140"
  },
  {
    "text": "is very different, then it doesn't mean anything. So intelligence is a collection of skills",
    "start": "7687140",
    "end": "7695623"
  },
  {
    "text": "and an ability to acquire\nnew skills efficiently. Right?",
    "start": "7698090",
    "end": "7703910"
  },
  {
    "text": "And the collection of skills that a particular\nintelligent entity possess",
    "start": "7703910",
    "end": "7709500"
  },
  {
    "text": "or is capable of learning quickly is different from the collection\nof skills of another one.",
    "start": "7709500",
    "end": "7715310"
  },
  {
    "text": "And because it's a multidimensional thing, the set of skills is a\nhigh dimensional space, you can't measure.",
    "start": "7715310",
    "end": "7720738"
  },
  {
    "text": "You cannot compare two things as to whether one is more\nintelligent than the other. It's multidimensional.",
    "start": "7720738",
    "end": "7726863"
  },
  {
    "start": "7728000",
    "end": "8678000"
  },
  {
    "text": "- So you push back against what\nare called AI doomers a lot.",
    "start": "7728720",
    "end": "7733720"
  },
  {
    "text": "Can you explain their perspective and why you think they're wrong? - Okay.",
    "start": "7735419",
    "end": "7740593"
  },
  {
    "text": "So AI doomers imagine all\nkinds of catastrophe scenarios of how AI could escape our control",
    "start": "7740593",
    "end": "7747440"
  },
  {
    "text": "and basically kill us all. (laughs) And that relies on a\nwhole bunch of assumptions",
    "start": "7747440",
    "end": "7754430"
  },
  {
    "text": "that are mostly false. So the first assumption is that the emergence\nof super intelligence",
    "start": "7754430",
    "end": "7760190"
  },
  {
    "text": "could be an event. That at some point we're\ngoing to figure out the secret and we'll turn on a machine\nthat is super intelligent.",
    "start": "7760190",
    "end": "7768290"
  },
  {
    "text": "And because we'd never done it before, it's gonna take over the\nworld and kill us all. That is false.",
    "start": "7768290",
    "end": "7773869"
  },
  {
    "text": "It's not gonna be an event. We're gonna have systems that\nare like as smart as a cat,",
    "start": "7773870",
    "end": "7779581"
  },
  {
    "text": "have all the characteristics\nof human level intelligence,",
    "start": "7779581",
    "end": "7784581"
  },
  {
    "text": "but their level of intelligence would be like a cat or a\nparrot maybe or something.",
    "start": "7784940",
    "end": "7789833"
  },
  {
    "text": "And then we're gonna walk our way up to kind of make those\nthings more intelligent. And as we make them more intelligent,",
    "start": "7790740",
    "end": "7796760"
  },
  {
    "text": "we're also gonna put\nsome guardrails in them and learn how to kind\nof put some guardrails so they behave properly. And we're not gonna do\nthis with just one...",
    "start": "7796760",
    "end": "7803810"
  },
  {
    "text": "It's not gonna be one effort, but it's gonna be lots of\ndifferent people doing this. And some of them are gonna succeed",
    "start": "7803810",
    "end": "7809270"
  },
  {
    "text": "at making intelligent systems\nthat are controllable and safe and have the right guardrails.",
    "start": "7809270",
    "end": "7814370"
  },
  {
    "text": "And if some other goes rogue, then we can use the good ones\nto go against the rogue ones. (laughs)",
    "start": "7814370",
    "end": "7820370"
  },
  {
    "text": "So it's gonna be smart AI\npolice against your rogue AI. So it's not gonna be like\nwe're gonna be exposed",
    "start": "7820370",
    "end": "7827660"
  },
  {
    "text": "to like a single rogue AI\nthat's gonna kill us all. That's just not happening. Now, there is another fallacy,",
    "start": "7827660",
    "end": "7833270"
  },
  {
    "text": "which is the fact that because\nthe system is intelligent, it necessarily wants to take over.",
    "start": "7833270",
    "end": "7838012"
  },
  {
    "text": "And there is several arguments that make people scared of this, which I think are\ncompletely false as well.",
    "start": "7840650",
    "end": "7848128"
  },
  {
    "text": "So one of them is in nature,",
    "start": "7848128",
    "end": "7853128"
  },
  {
    "text": "it seems to be that the\nmore intelligent species are the ones that end\nup dominating the other. And even extinguishing the others",
    "start": "7853400",
    "end": "7863107"
  },
  {
    "text": "sometimes by design,\nsometimes just by mistake.",
    "start": "7863107",
    "end": "7866782"
  },
  {
    "text": "And so there is sort of a thinking by which you say, well, if AI systems",
    "start": "7869649",
    "end": "7875989"
  },
  {
    "text": "are more intelligent than us, surely they're going to eliminate us, if not by design,",
    "start": "7875990",
    "end": "7881480"
  },
  {
    "text": "simply because they don't care about us. And that's just preposterous\nfor a number of reasons.",
    "start": "7881480",
    "end": "7887398"
  },
  {
    "text": "First reason is they're\nnot going to be a species. They're not gonna be a\nspecies that competes with us.",
    "start": "7887398",
    "end": "7893240"
  },
  {
    "text": "They're not gonna have\nthe desire to dominate because the desire to dominate is something that has to be hardwired",
    "start": "7893240",
    "end": "7898580"
  },
  {
    "text": "into an intelligent system. It is hardwired in humans,",
    "start": "7898580",
    "end": "7903623"
  },
  {
    "text": "it is hardwired in baboons, in chimpanzees, in wolves, not in orangutans.",
    "start": "7904619",
    "end": "7909983"
  },
  {
    "text": "The species in which this\ndesire to dominate or submit",
    "start": "7911300",
    "end": "7916300"
  },
  {
    "text": "or attain status in other ways is specific to social species.",
    "start": "7916460",
    "end": "7923239"
  },
  {
    "text": "Non-social species like\norangutans don't have it. Right? And they are as smart as we are, almost.",
    "start": "7923240",
    "end": "7929060"
  },
  {
    "text": "Right? - And to you, there's\nnot significant incentive for humans to encode\nthat into the AI systems.",
    "start": "7929060",
    "end": "7935119"
  },
  {
    "text": "And to the degree they do, there'll be other AIs that\nsort of punish them for it.",
    "start": "7935120",
    "end": "7942079"
  },
  {
    "text": "Out-compete them over- - Well, there's all kinds of incentive to make AI systems submissive to humans. Right?\n- [Lex] Right.",
    "start": "7942080",
    "end": "7947700"
  },
  {
    "text": "- I mean, this is the way\nwe're gonna build them, right? And so then people say,\noh, but look at LLMs.",
    "start": "7947700",
    "end": "7952733"
  },
  {
    "text": "LLMs are not controllable. And they're right, LLMs are not controllable. But objective driven AI,",
    "start": "7952733",
    "end": "7957920"
  },
  {
    "text": "so systems that derive their answers by optimization of an objective",
    "start": "7957920",
    "end": "7963710"
  },
  {
    "text": "means they have to\noptimize this objective, and that objective can include guardrails. One guardrail is obey humans.",
    "start": "7963710",
    "end": "7972770"
  },
  {
    "text": "Another guardrail is don't obey humans if it's hurting other humans- - I've heard that before\nsomewhere, I don't remember-",
    "start": "7972770",
    "end": "7979520"
  },
  {
    "text": "- [Yann] Yes.\n(Lex laughs) Maybe in a book. (laughs) - Yeah. But speaking of that book, could there be unintended\nconsequences also",
    "start": "7979520",
    "end": "7988010"
  },
  {
    "text": "from all of this? - No, of course. So this is not a simple problem, right? I mean designing those guardrails",
    "start": "7988010",
    "end": "7994176"
  },
  {
    "text": "so that the system behaves properly is not gonna be a simple issue",
    "start": "7994176",
    "end": "8000790"
  },
  {
    "text": "for which there is a silver bullet, for which you have a mathematical proof that the system can be safe. It's gonna be very progressive,",
    "start": "8000790",
    "end": "8007360"
  },
  {
    "text": "iterative design system where we put those guardrails in such a way that the\nsystem behave properly.",
    "start": "8007360",
    "end": "8012940"
  },
  {
    "text": "And sometimes they're\ngoing to do something that was unexpected because\nthe guardrail wasn't right,",
    "start": "8012940",
    "end": "8018096"
  },
  {
    "text": "and we're gonna correct them\nso that they do it right. The idea somehow that we\ncan't get it slightly wrong,",
    "start": "8018097",
    "end": "8024070"
  },
  {
    "text": "because if we get it\nslightly wrong we all die, is ridiculous.",
    "start": "8024070",
    "end": "8027913"
  },
  {
    "text": "We're just gonna go progressively. The analogy I've used many\ntimes is turbojet design.",
    "start": "8029260",
    "end": "8036847"
  },
  {
    "text": "How did we figure out how to make turbojets so\nunbelievably reliable, right?",
    "start": "8040490",
    "end": "8046693"
  },
  {
    "text": "I mean, those are like incredibly\ncomplex pieces of hardware that run at really high temperatures",
    "start": "8046693",
    "end": "8052810"
  },
  {
    "text": "for 20 hours at a time sometimes. And we can fly halfway around the world",
    "start": "8052810",
    "end": "8060790"
  },
  {
    "text": "on a two engine jet liner\nat near the speed of sound.",
    "start": "8060790",
    "end": "8065790"
  },
  {
    "text": "Like how incredible is this? It is just unbelievable. And did we do this",
    "start": "8067240",
    "end": "8073510"
  },
  {
    "text": "because we invented\nlike a general principle of how to make turbojets safe? No, it took decades",
    "start": "8073510",
    "end": "8079030"
  },
  {
    "text": "to kind of fine tune the\ndesign of those systems so that they were safe. Is there a separate group",
    "start": "8079030",
    "end": "8086231"
  },
  {
    "text": "within General Electric\nor Snecma or whatever that is specialized in turbojet safety?",
    "start": "8086231",
    "end": "8094600"
  },
  {
    "text": "No. The design is all about safety. Because a better turbojet\nis also a safer turbojet,",
    "start": "8094600",
    "end": "8101200"
  },
  {
    "text": "a more reliable one. It's the same for AI. Like do you need specific\nprovisions to make AI safe?",
    "start": "8101200",
    "end": "8108550"
  },
  {
    "text": "No, you need to make better AI systems and they will be safe because they are designed\nto be more useful",
    "start": "8108550",
    "end": "8114880"
  },
  {
    "text": "and more controllable. - So let's imagine a system, AI system that's able to\nbe incredibly convincing",
    "start": "8114880",
    "end": "8122819"
  },
  {
    "text": "and can convince you of anything. I can at least imagine such a system.",
    "start": "8123310",
    "end": "8128113"
  },
  {
    "text": "And I can see such a\nsystem be weapon-like, because it can control people's minds,",
    "start": "8129160",
    "end": "8135490"
  },
  {
    "text": "we're pretty gullible. We want to believe a thing. And you can have an AI\nsystem that controls it",
    "start": "8135490",
    "end": "8140860"
  },
  {
    "text": "and you could see governments\nusing that as a weapon. So do you think if you\nimagine such a system,",
    "start": "8140860",
    "end": "8147613"
  },
  {
    "text": "there's any parallel to\nsomething like nuclear weapons? - [Yann] No.",
    "start": "8148600",
    "end": "8154449"
  },
  {
    "text": "- So why is that technology different? So you're saying there's going\nto be gradual development?",
    "start": "8154450",
    "end": "8161440"
  },
  {
    "text": "- [Yann] Yeah. - I mean it might be rapid, but they'll be iterative. And then we'll be able to\nkind of respond and so on.",
    "start": "8161440",
    "end": "8169060"
  },
  {
    "text": "- So that AI system designed\nby Vladimir Putin or whatever, or his minions (laughing)",
    "start": "8169060",
    "end": "8176765"
  },
  {
    "text": "is gonna be like trying\nto talk to every American",
    "start": "8176765",
    "end": "8181765"
  },
  {
    "text": "to convince them to vote for- - [Lex] Whoever. - Whoever pleases Putin or whatever.",
    "start": "8181780",
    "end": "8190920"
  },
  {
    "text": "Or rile people up against each other",
    "start": "8191200",
    "end": "8196200"
  },
  {
    "text": "as they've been trying to do. They're not gonna be talking to you, they're gonna be talking\nto your AI assistant",
    "start": "8196465",
    "end": "8203200"
  },
  {
    "text": "which is going to be as\nsmart as theirs, right? Because as I said, in the future,",
    "start": "8204401",
    "end": "8211120"
  },
  {
    "text": "every single one of your\ninteraction with the digital world will be mediated by your AI assistant. So the first thing you're\ngonna ask is, is this a scam?",
    "start": "8211120",
    "end": "8218388"
  },
  {
    "text": "Like is this thing like\ntelling me the truth? Like it's not even going\nto be able to get to you because it's only going to\ntalk to your AI assistant,",
    "start": "8218389",
    "end": "8225340"
  },
  {
    "text": "and your AI is not even going to... It's gonna be like a spam filter, right?",
    "start": "8225340",
    "end": "8230740"
  },
  {
    "text": "You're not even seeing the\nemail, the spam email, right? It's automatically put in a\nfolder that you never see.",
    "start": "8230740",
    "end": "8236743"
  },
  {
    "text": "It's gonna be the same thing. That AI system that tries to\nconvince you of something, it's gonna be talking to an AI system",
    "start": "8236743",
    "end": "8242920"
  },
  {
    "text": "which is gonna be at least as smart as it. And is gonna say, this is spam. (laughs)",
    "start": "8242920",
    "end": "8249508"
  },
  {
    "text": "It's not even going to\nbring it to your attention. - So to you it's very\ndifficult for any one AI system",
    "start": "8249508",
    "end": "8254720"
  },
  {
    "text": "to take such a big leap ahead to where it can convince\neven the other AI systems?",
    "start": "8254721",
    "end": "8260050"
  },
  {
    "text": "So like there's always going\nto be this kind of race where nobody's way ahead?",
    "start": "8260050",
    "end": "8266590"
  },
  {
    "text": "- That's the history of the world. History of the world is whenever there is a progress someplace,",
    "start": "8266590",
    "end": "8271636"
  },
  {
    "text": "there is a countermeasure. And it's a cat and mouse game.",
    "start": "8271637",
    "end": "8277290"
  },
  {
    "text": "- Mostly yes, but this is why nuclear\nweapons are so interesting because that was such a powerful weapon",
    "start": "8277290",
    "end": "8285370"
  },
  {
    "text": "that it mattered who got it first. That you could imagine Hitler, Stalin, Mao",
    "start": "8285370",
    "end": "8293417"
  },
  {
    "text": "getting the weapon first and that having a different\nkind of impact on the world",
    "start": "8296441",
    "end": "8301475"
  },
  {
    "text": "than the United States\ngetting the weapon first. To you, nuclear weapons is like...",
    "start": "8301476",
    "end": "8307510"
  },
  {
    "text": "You don't imagine a breakthrough discovery and then Manhattan project\nlike effort for AI?",
    "start": "8307510",
    "end": "8315820"
  },
  {
    "text": "- No. As I said, it's not going to be an event. It's gonna be continuous progress.",
    "start": "8315821",
    "end": "8321956"
  },
  {
    "text": "And whenever one breakthrough occurs, it's gonna be widely\ndisseminated really quickly.",
    "start": "8321956",
    "end": "8328990"
  },
  {
    "text": "Probably first within industry. I mean, this is not a domain where government or military organizations",
    "start": "8328990",
    "end": "8335156"
  },
  {
    "text": "are particularly innovative, and they're in fact way behind. And so this is gonna come from industry.",
    "start": "8335156",
    "end": "8342340"
  },
  {
    "text": "And this kind of information\ndisseminates extremely quickly. We've seen this over the\nlast few years, right?",
    "start": "8342340",
    "end": "8348130"
  },
  {
    "text": "Where you have a new... Like even take AlphaGo. This was reproduced within three months",
    "start": "8348130",
    "end": "8353980"
  },
  {
    "text": "even without like particularly\ndetailed information, right? - Yeah. This is an industry that's\nnot good at secrecy.",
    "start": "8354910",
    "end": "8360916"
  },
  {
    "text": "(laughs) - But even if there is, just the fact that you know\nthat something is possible",
    "start": "8360916",
    "end": "8366262"
  },
  {
    "text": "makes you like realize that it's worth investing\nthe time to actually do it. You may be the second person\nto do it but you'll do it.",
    "start": "8366263",
    "end": "8375222"
  },
  {
    "text": "Say for all the innovations of self supervised running transformers,",
    "start": "8376822",
    "end": "8383359"
  },
  {
    "text": "decoder only architectures, LLMs. I mean those things, you don't need to know exactly\nthe details of how they work",
    "start": "8383361",
    "end": "8389859"
  },
  {
    "text": "to know that it's possible because it's deployed and\nthen it's getting reproduced. And then people who work\nfor those companies move.",
    "start": "8389860",
    "end": "8399690"
  },
  {
    "text": "They go from one company to another. And the information disseminates. What makes the success\nof the US tech industry",
    "start": "8400420",
    "end": "8409750"
  },
  {
    "text": "and Silicon Valley in\nparticular, is exactly that, is because information\ncirculates really, really quickly and disseminates very quickly.",
    "start": "8409750",
    "end": "8417460"
  },
  {
    "text": "And so the whole region sort of is ahead because of that\ncirculation of information.",
    "start": "8417460",
    "end": "8424570"
  },
  {
    "text": "- Maybe just to linger on\nthe psychology of AI doomers. You give in the classic Yann LeCun way,",
    "start": "8424570",
    "end": "8431902"
  },
  {
    "text": "a pretty good example of just when a new technology comes to be, you say engineer says,",
    "start": "8431903",
    "end": "8438724"
  },
  {
    "text": "\"I invented this new thing,\nI call it a ballpen.\"",
    "start": "8438724",
    "end": "8443724"
  },
  {
    "text": "And then the TwitterSphere responds, \"OMG people could write\nhorrible things with it like misinformation,\npropaganda, hate speech.",
    "start": "8444220",
    "end": "8451030"
  },
  {
    "text": "Ban it now!\" Then writing doomers come in, akin to the AI doomers,",
    "start": "8451030",
    "end": "8457547"
  },
  {
    "text": "\"imagine if everyone can get a ballpen. This could destroy society. There should be a law",
    "start": "8457547",
    "end": "8463120"
  },
  {
    "text": "against using ballpen\nto write hate speech, regulate ballpens now.\" And then the pencil industry mogul says,",
    "start": "8463120",
    "end": "8469787"
  },
  {
    "text": "\"yeah, ballpens are very dangerous, unlike pencil writing which is erasable,",
    "start": "8469787",
    "end": "8475810"
  },
  {
    "text": "ballpen writing stays forever. Government should require a\nlicense for a pen manufacturer.\"",
    "start": "8475810",
    "end": "8481807"
  },
  {
    "text": "I mean, this does seem to\nbe part of human psychology",
    "start": "8482680",
    "end": "8487680"
  },
  {
    "text": "when it comes up against new technology.",
    "start": "8488114",
    "end": "8491593"
  },
  {
    "text": "What deep insights can\nyou speak to about this? - Well, there is a natural\nfear of new technology",
    "start": "8493146",
    "end": "8502300"
  },
  {
    "text": "and the impact it can have on society. And people have kind\nof instinctive reaction",
    "start": "8503500",
    "end": "8508960"
  },
  {
    "text": "to the world they know being threatened by major transformations",
    "start": "8508960",
    "end": "8515987"
  },
  {
    "text": "that are either cultural phenomena or technological revolutions.",
    "start": "8515987",
    "end": "8521053"
  },
  {
    "text": "And they fear for their culture, they fear for their job, they fear for the future of their children",
    "start": "8522190",
    "end": "8530207"
  },
  {
    "text": "and their way of life, right? So any change is feared.",
    "start": "8530207",
    "end": "8537043"
  },
  {
    "text": "And you see this along history, like any technological\nrevolution or cultural phenomenon",
    "start": "8537043",
    "end": "8544060"
  },
  {
    "text": "was always accompanied by\ngroups or reaction in the media",
    "start": "8544060",
    "end": "8549060"
  },
  {
    "text": "that basically attributed\nall the problems,",
    "start": "8551095",
    "end": "8556095"
  },
  {
    "text": "the current problems of society to that particular change, right? Electricity was going to\nkill everyone at some point.",
    "start": "8556360",
    "end": "8564493"
  },
  {
    "text": "The train was going to be a horrible thing because you can't breathe\npast 50 kilometers an hour.",
    "start": "8565397",
    "end": "8570883"
  },
  {
    "text": "And so there's a wonderful website called a Pessimists Archive, right? Which has all those\nnewspaper clips (laughing)",
    "start": "8572230",
    "end": "8579430"
  },
  {
    "text": "of all the horrible things\npeople imagined would arrive because of either technological innovation",
    "start": "8579430",
    "end": "8586878"
  },
  {
    "text": "or a cultural phenomenon.",
    "start": "8586878",
    "end": "8589933"
  },
  {
    "text": "Wonderful examples of jazz or comic books",
    "start": "8593505",
    "end": "8598505"
  },
  {
    "text": "being blamed for unemployment or young people not\nwanting to work anymore",
    "start": "8598670",
    "end": "8605847"
  },
  {
    "text": "and things like that, right? And that has existed for centuries.",
    "start": "8605847",
    "end": "8610693"
  },
  {
    "text": "And it's knee jerk reactions. The question is do we embrace\nchange or do we resist it?",
    "start": "8614323",
    "end": "8623520"
  },
  {
    "text": "And what are the real dangers as opposed to the imagined imagined ones?",
    "start": "8625137",
    "end": "8630523"
  },
  {
    "text": "- So people worry about... I think one thing they\nworry about with big tech, something we've been\ntalking about over and over",
    "start": "8631810",
    "end": "8638650"
  },
  {
    "text": "but I think worth mentioning again, they worry about how powerful AI will be",
    "start": "8638650",
    "end": "8645050"
  },
  {
    "text": "and they worry about it being in the hands of\none centralized power of just a handful of central control.",
    "start": "8645970",
    "end": "8653740"
  },
  {
    "text": "And so that's the\nskepticism with big tech. These companies can make\na huge amount of money",
    "start": "8653740",
    "end": "8658780"
  },
  {
    "text": "and control this technology. And by so doing,",
    "start": "8658780",
    "end": "8664003"
  },
  {
    "text": "take advantage, abuse the\nlittle guy in society. - Well, that's exactly why we\nneed open source platforms.",
    "start": "8664900",
    "end": "8671920"
  },
  {
    "text": "- Yeah. I just wanted to... (laughs) Nail the point home more and more. - [Yann] Yes.",
    "start": "8671920",
    "end": "8677545"
  },
  {
    "start": "8678000",
    "end": "8931000"
  },
  {
    "text": "- So let me ask you on your... Like I said, you do get a little bit",
    "start": "8678520",
    "end": "8682420"
  },
  {
    "text": "flavorful on the internet. Joscha Bach tweeted\nsomething that you LOL'd at",
    "start": "8684490",
    "end": "8690850"
  },
  {
    "text": "in reference to HAL 9,000. Quote, \"I appreciate your argument and I fully understand your frustration,",
    "start": "8690850",
    "end": "8697540"
  },
  {
    "text": "but whether the pod bay doors\nshould be opened or closed is a complex and nuanced issue.\"",
    "start": "8697540",
    "end": "8703900"
  },
  {
    "text": "So you're at the head of Meta AI. This is something that really worries me,",
    "start": "8703900",
    "end": "8712030"
  },
  {
    "text": "that our AI overlords will speak down to us with\ncorporate speak of this nature",
    "start": "8712030",
    "end": "8720460"
  },
  {
    "text": "and you sort of resist that\nwith your way of being. Is this something you can just comment on",
    "start": "8720460",
    "end": "8727150"
  },
  {
    "text": "sort of working at a big company, how you can avoid the\nover fearing, I suppose,",
    "start": "8727150",
    "end": "8734550"
  },
  {
    "text": "the through caution create harm? - Yeah. Again, I think the answer to\nthis is open source platforms",
    "start": "8737830",
    "end": "8745822"
  },
  {
    "text": "and then enabling a widely\ndiverse set of people to build AI assistants",
    "start": "8745822",
    "end": "8753699"
  },
  {
    "text": "that represent the diversity of cultures, opinions, languages, and value systems across the world.",
    "start": "8753700",
    "end": "8759605"
  },
  {
    "text": "So that you're not bound\nto just be brainwashed",
    "start": "8759606",
    "end": "8764606"
  },
  {
    "text": "by a particular way of thinking because of a single AI entity.",
    "start": "8764950",
    "end": "8770439"
  },
  {
    "text": "So I mean, I think it's a\nreally, really important question for society. And the problem I'm seeing,",
    "start": "8770440",
    "end": "8776503"
  },
  {
    "text": "which is why I've been so vocal and sometimes a little sardonic about it- - Never stop.",
    "start": "8780430",
    "end": "8786370"
  },
  {
    "text": "Never stop, Yann. (both laugh) We love it.\n- Is because I see the danger",
    "start": "8786370",
    "end": "8791500"
  },
  {
    "text": "of this concentration of power through proprietary AI systems as a much bigger danger\nthan everything else.",
    "start": "8791500",
    "end": "8799870"
  },
  {
    "text": "That if we really want\ndiversity of opinion AI systems",
    "start": "8799870",
    "end": "8804870"
  },
  {
    "text": "that in the future that we'll all be interacting\nthrough AI systems,",
    "start": "8806290",
    "end": "8812980"
  },
  {
    "text": "we need those to be diverse for the preservation\nof a diversity of ideas",
    "start": "8812980",
    "end": "8818022"
  },
  {
    "text": "and creeds and political\nopinions and whatever,",
    "start": "8818023",
    "end": "8823023"
  },
  {
    "text": "and the preservation of democracy. And what works against this",
    "start": "8826150",
    "end": "8832149"
  },
  {
    "text": "is people who think that\nfor reasons of security, we should keep AI systems\nunder lock and key",
    "start": "8832150",
    "end": "8839320"
  },
  {
    "text": "because it's too dangerous to put it in the hands of everybody because it could be used\nby terrorists or something.",
    "start": "8839320",
    "end": "8846240"
  },
  {
    "text": "That would lead to\npotentially a very bad future",
    "start": "8848800",
    "end": "8853800"
  },
  {
    "text": "in which all of our information diet is controlled by a small\nnumber of companies through proprietary systems.",
    "start": "8856240",
    "end": "8863113"
  },
  {
    "text": "- So you trust humans with this technology to build systems that are on\nthe whole good for humanity?",
    "start": "8864294",
    "end": "8872042"
  },
  {
    "text": "- Isn't that what democracy\nand free speech is all about? - I think so. - Do you trust institutions\nto do the right thing?",
    "start": "8873250",
    "end": "8880360"
  },
  {
    "text": "Do you trust people to do the right thing? And yeah, there's bad people\nwho are gonna do bad things,",
    "start": "8880360",
    "end": "8885370"
  },
  {
    "text": "but they're not going to\nhave superior technology to the good people. So then it's gonna be my good\nAI against your bad AI, right?",
    "start": "8885370",
    "end": "8892540"
  },
  {
    "text": "I mean it's the examples that\nwe were just talking about of maybe some rogue country\nwill build some AI system",
    "start": "8892540",
    "end": "8900809"
  },
  {
    "text": "that's gonna try to convince everybody to go into a civil war or something",
    "start": "8901827",
    "end": "8907055"
  },
  {
    "text": "or elect a favorable ruler.",
    "start": "8907055",
    "end": "8911803"
  },
  {
    "text": "But then they will have to go\npast our AI systems, right? (laughs) - An AI system with a\nstrong Russian accent",
    "start": "8912892",
    "end": "8918699"
  },
  {
    "text": "will be trying to convince our- - And doesn't put any\narticles in their sentences. (both laugh)",
    "start": "8918700",
    "end": "8925750"
  },
  {
    "text": "- Well, it'll be at the very\nleast, absurdly comedic. Okay. So since we talked about\nsort of the physical reality,",
    "start": "8925750",
    "end": "8935083"
  },
  {
    "start": "8931000",
    "end": "9480000"
  },
  {
    "text": "I'd love to ask your vision\nof the future with robots in this physical reality.",
    "start": "8935380",
    "end": "8940570"
  },
  {
    "text": "So many of the kinds of intelligence you've been speaking about would empower robots",
    "start": "8940570",
    "end": "8946720"
  },
  {
    "text": "to be more effective\ncollaborators with us humans. So since Tesla's Optimus team",
    "start": "8946720",
    "end": "8954489"
  },
  {
    "text": "has been showing us some\nprogress in humanoid robots, I think it really reinvigorated\nthe whole industry",
    "start": "8954490",
    "end": "8960610"
  },
  {
    "text": "that I think Boston\nDynamics has been leading for a very, very long time. So now there's all kinds of companies,",
    "start": "8960610",
    "end": "8965680"
  },
  {
    "text": "Figure AI, obviously Boston Dynamics- - [Yann] Unitree. - Unitree.",
    "start": "8965680",
    "end": "8971533"
  },
  {
    "text": "But there's like a lot of them. It's great. It's great. I mean I love it.",
    "start": "8971533",
    "end": "8976333"
  },
  {
    "text": "So do you think there'll be\nmillions of humanoid robots walking around soon?",
    "start": "8977740",
    "end": "8984040"
  },
  {
    "text": "- Not soon, but it's gonna happen. Like the next decade I think is gonna be really\ninteresting in robots.",
    "start": "8984040",
    "end": "8989500"
  },
  {
    "text": "Like the emergence of\nthe robotics industry has been in the waiting for 10, 20 years,",
    "start": "8989500",
    "end": "8997720"
  },
  {
    "text": "without really emerging other than for like kind\nof pre-program behavior and stuff like that.",
    "start": "8997720",
    "end": "9003096"
  },
  {
    "text": "And the main issue is again,\nthe Moravec's paradox.",
    "start": "9003096",
    "end": "9008096"
  },
  {
    "text": "Like how do we get the systems to understand how the world works and kind of plan actions? And so we can do it for\nreally specialized tasks.",
    "start": "9008700",
    "end": "9015724"
  },
  {
    "text": "And the way Boston Dynamics goes about it is basically with a lot of\nhandcrafted dynamical models",
    "start": "9017820",
    "end": "9025265"
  },
  {
    "text": "and careful planning in advance, which is very classical robotics\nwith a lot of innovation,",
    "start": "9025265",
    "end": "9032010"
  },
  {
    "text": "a little bit of perception, but it's still not... Like they can't build a\ndomestic robot, right?",
    "start": "9032010",
    "end": "9038732"
  },
  {
    "text": "And we're still some distance away from completely autonomous\nlevel five driving.",
    "start": "9040260",
    "end": "9046173"
  },
  {
    "text": "And we're certainly very far away from having level five autonomous driving",
    "start": "9047610",
    "end": "9053402"
  },
  {
    "text": "by a system that can train itself by driving 20 hours, like any 17-year-old.",
    "start": "9053403",
    "end": "9059463"
  },
  {
    "text": "So until we have, again, world models,",
    "start": "9060636",
    "end": "9065636"
  },
  {
    "text": "systems that can train themselves to understand how the world works, we're not gonna have significant\nprogress in robotics.",
    "start": "9068070",
    "end": "9076920"
  },
  {
    "text": "So a lot of the people working on robotic hardware at the moment are betting or banking",
    "start": "9076920",
    "end": "9083520"
  },
  {
    "text": "on the fact that AI is gonna make sufficient\nprogress towards that. - And they're hoping to\ndiscover a product in it too-",
    "start": "9083520",
    "end": "9091381"
  },
  {
    "text": "- [Yann] Yeah. - Before you have a\nreally strong world model, there'll be an almost strong world model.",
    "start": "9091381",
    "end": "9098010"
  },
  {
    "text": "And people are trying to find a product in a clumsy robot, I suppose.",
    "start": "9098010",
    "end": "9103740"
  },
  {
    "text": "Like not a perfectly efficient robot. So there's the factory setting where humanoid robots can help automate some\naspects of the factory.",
    "start": "9103740",
    "end": "9111270"
  },
  {
    "text": "I think that's a crazy difficult task 'cause of all the safety required and all this kind of stuff, I think in the home is more interesting.",
    "start": "9111270",
    "end": "9117700"
  },
  {
    "text": "But then you start to think... I think you mentioned loading\nthe dishwasher, right?",
    "start": "9117700",
    "end": "9123240"
  },
  {
    "text": "- [Yann] Yeah. - Like I suppose that's\none of the main problems you're working on. - I mean there's cleaning up. (laughs)",
    "start": "9123240",
    "end": "9130696"
  },
  {
    "text": "- [Lex] Yeah. - Cleaning the house, clearing up the table after a meal,",
    "start": "9130696",
    "end": "9137190"
  },
  {
    "text": "washing the dishes, all\nthose tasks, cooking. I mean all the tasks that in\nprinciple could be automated",
    "start": "9137190",
    "end": "9144029"
  },
  {
    "text": "but are actually incredibly sophisticated, really complicated. - But even just basic navigation",
    "start": "9144030",
    "end": "9149730"
  },
  {
    "text": "around a space full of uncertainty. - That sort of works. Like you can sort of do this now.",
    "start": "9149730",
    "end": "9155580"
  },
  {
    "text": "Navigation is fine. - Well, navigation in a way\nthat's compelling to us humans",
    "start": "9155580",
    "end": "9160708"
  },
  {
    "text": "is a different thing. - Yeah. It's not gonna be necessarily... I mean we have demos actually",
    "start": "9160708",
    "end": "9166560"
  },
  {
    "text": "'cause there is a so-called\nembodied AI group at FAIR",
    "start": "9166560",
    "end": "9171560"
  },
  {
    "text": "and they've been not\nbuilding their own robots but using commercial robots.",
    "start": "9172080",
    "end": "9177152"
  },
  {
    "text": "And you can tell the robot\ndog like go to the fridge and they can actually open the fridge",
    "start": "9177990",
    "end": "9183630"
  },
  {
    "text": "and they can probably pick\nup a can in the fridge and stuff like that and bring it to you.",
    "start": "9183630",
    "end": "9188963"
  },
  {
    "text": "So it can navigate, it can grab objects as long as it's been\ntrained to recognize them,",
    "start": "9188963",
    "end": "9194337"
  },
  {
    "text": "which vision systems work\npretty well nowadays. But it's not like a\ncompletely general robot",
    "start": "9194337",
    "end": "9203188"
  },
  {
    "text": "that would be sophisticated enough to do things like clearing\nup the dinner table.",
    "start": "9203188",
    "end": "9208785"
  },
  {
    "text": "(laughs) - Yeah, to me that's an exciting future of getting humanoid robots.",
    "start": "9208785",
    "end": "9215040"
  },
  {
    "text": "Robots in general in\nthe home more and more because it gets humans to really directly\ninteract with AI systems",
    "start": "9215040",
    "end": "9220817"
  },
  {
    "text": "in the physical space. And in so doing it allows us to philosophically,\npsychologically explore",
    "start": "9220818",
    "end": "9226620"
  },
  {
    "text": "our relationships with robots. It can be really, really interesting. So I hope you make progress\non the whole JEPA thing soon.",
    "start": "9226620",
    "end": "9234228"
  },
  {
    "text": "(laughs) - Well, I mean, I hope\nthings can work as planned.",
    "start": "9234228",
    "end": "9238594"
  },
  {
    "text": "I mean, again, we've been like\nkinda working on this idea of self supervised learning\nfrom video for 10 years.",
    "start": "9240540",
    "end": "9247113"
  },
  {
    "text": "And only made significant\nprogress in the last two or three. - And actually you've mentioned",
    "start": "9247950",
    "end": "9253590"
  },
  {
    "text": "that there's a lot of\ninteresting breakthroughs that can happen without having\naccess to a lot of compute. So if you're interested in doing a PhD",
    "start": "9253590",
    "end": "9260310"
  },
  {
    "text": "in this kind of stuff, there's a lot of possibilities still to do innovative work.",
    "start": "9260310",
    "end": "9265590"
  },
  {
    "text": "So like what advice would you give to a undergrad that's\nlooking to go to grad school and do a PhD?",
    "start": "9265590",
    "end": "9272340"
  },
  {
    "text": "- So basically, I've listed them already. This idea of how do you train\na world model by observation?",
    "start": "9272340",
    "end": "9278733"
  },
  {
    "text": "And you don't have to train necessarily on gigantic data sets.",
    "start": "9279810",
    "end": "9283833"
  },
  {
    "text": "I mean, it could turn out to be necessary to actually train on large data sets to have emergent properties\nlike we have with LLMs.",
    "start": "9285690",
    "end": "9291780"
  },
  {
    "text": "But I think there is a lot of\ngood ideas that can be done without necessarily scaling up. Then there is how do you do planning",
    "start": "9291780",
    "end": "9298470"
  },
  {
    "text": "with a learn world model? If the world the system evolves in is not the physical world,",
    "start": "9298470",
    "end": "9303720"
  },
  {
    "text": "but is the world of let's say the internet or some sort of world",
    "start": "9303720",
    "end": "9309558"
  },
  {
    "text": "of where an action consists in doing a search in a search engine or interrogating a database,",
    "start": "9309558",
    "end": "9314942"
  },
  {
    "text": "or running a simulation or calling a calculator or solving a differential equation,",
    "start": "9314943",
    "end": "9321303"
  },
  {
    "text": "how do you get a system to actually plan a sequence of actions to give the solution to a problem?",
    "start": "9322590",
    "end": "9328921"
  },
  {
    "text": "And so the question of planning is not just a question of\nplanning physical actions,",
    "start": "9328921",
    "end": "9335161"
  },
  {
    "text": "it could be planning actions to use tools for a dialogue system or for any kind of intelligence system.",
    "start": "9335161",
    "end": "9342333"
  },
  {
    "text": "And there's some work on\nthis but not a huge amount. Some work at FAIR,",
    "start": "9343230",
    "end": "9348523"
  },
  {
    "text": "one called Toolformer,\nwhich was a couple years ago and some more recent work on planning,",
    "start": "9348524",
    "end": "9355190"
  },
  {
    "text": "but I don't think we\nhave like a good solution for any of that.",
    "start": "9355191",
    "end": "9360810"
  },
  {
    "text": "Then there is the question\nof hierarchical planning. So the example I mentioned",
    "start": "9360810",
    "end": "9365994"
  },
  {
    "text": "of planning a trip from New York to Paris, that's hierarchical,",
    "start": "9365995",
    "end": "9371453"
  },
  {
    "text": "but almost every action that we take involves hierarchical\nplanning in some sense.",
    "start": "9371453",
    "end": "9377520"
  },
  {
    "text": "And we really have absolutely\nno idea how to do this. Like there's zero demonstration",
    "start": "9377520",
    "end": "9382680"
  },
  {
    "text": "of hierarchical planning in AI,",
    "start": "9382680",
    "end": "9386463"
  },
  {
    "text": "where the various levels\nof representations that are necessary have been learned.",
    "start": "9388050",
    "end": "9396479"
  },
  {
    "text": "We can do like two level\nhierarchical planning when we design the two levels. So for example, you have like\na dog legged robot, right?",
    "start": "9396480",
    "end": "9404880"
  },
  {
    "text": "You want it to go from the\nliving room to the kitchen. You can plan a path that\navoids the obstacle.",
    "start": "9404880",
    "end": "9411300"
  },
  {
    "text": "And then you can send this\nto a lower level planner that figures out how to move the legs",
    "start": "9411300",
    "end": "9416622"
  },
  {
    "text": "to kind of follow that\ntrajectories, right? So that works, but that two level planning\nis designed by hand, right?",
    "start": "9416622",
    "end": "9423933"
  },
  {
    "text": "We specify what the proper\nlevels of abstraction, the representation at each\nlevel of abstraction have to be.",
    "start": "9425310",
    "end": "9433170"
  },
  {
    "text": "How do you learn this? How do you learn that\nhierarchical representation of action plans, right?",
    "start": "9433170",
    "end": "9439923"
  },
  {
    "text": "With com nets and deep learning, we can train the system to learn hierarchical\nrepresentations of percepts.",
    "start": "9440880",
    "end": "9446313"
  },
  {
    "text": "What is the equivalent when what you're trying to\nrepresent are action plans? - For action plans. Yeah.",
    "start": "9447420",
    "end": "9452693"
  },
  {
    "text": "So you want basically a\nrobot dog or humanoid robot that turns on and travels\nfrom New York to Paris",
    "start": "9452693",
    "end": "9458943"
  },
  {
    "text": "all by itself. - [Yann] For example. - All right. It might have some trouble at the TSA but-",
    "start": "9458944",
    "end": "9467062"
  },
  {
    "text": "- No, but even doing\nsomething fairly simple like a household task. - [Lex] Sure. - Like cooking or something.",
    "start": "9467063",
    "end": "9473850"
  },
  {
    "text": "- Yeah. There's a lot involved. It's a super complex task. Once again, we take it for granted.",
    "start": "9473850",
    "end": "9479343"
  },
  {
    "start": "9480000",
    "end": "10037000"
  },
  {
    "text": "What hope do you have for\nthe future of humanity?",
    "start": "9480630",
    "end": "9485630"
  },
  {
    "text": "We're talking about so\nmany exciting technologies, so many exciting possibilities. What gives you hope when you look out",
    "start": "9485745",
    "end": "9492150"
  },
  {
    "text": "over the next 10, 20, 50, 100 years? If you look at social media, there's wars going on, there's\ndivision, there's hatred,",
    "start": "9492150",
    "end": "9501320"
  },
  {
    "text": "all this kind of stuff\nthat's also part of humanity. But amidst all that, what gives you hope?",
    "start": "9501660",
    "end": "9507003"
  },
  {
    "text": "- I love that question.",
    "start": "9509130",
    "end": "9510280"
  },
  {
    "text": "We can make humanity smarter with AI. Okay?",
    "start": "9514500",
    "end": "9520380"
  },
  {
    "text": "I mean AI basically will\namplify human intelligence. It's as if every one of us",
    "start": "9520380",
    "end": "9527819"
  },
  {
    "text": "will have a staff of smart AI assistants. They might be smarter than us.",
    "start": "9527820",
    "end": "9533733"
  },
  {
    "text": "They'll do our bidding, perhaps execute a task",
    "start": "9534660",
    "end": "9541800"
  },
  {
    "text": "in ways that are much better\nthan we could do ourselves because they'd be smarter than us.",
    "start": "9541800",
    "end": "9547830"
  },
  {
    "text": "And so it's like everyone\nwould be the boss of a staff of super smart virtual people.",
    "start": "9547830",
    "end": "9554674"
  },
  {
    "text": "So we shouldn't feel threatened by this any more than we should feel threatened by being the manager of a group of people,",
    "start": "9555960",
    "end": "9562632"
  },
  {
    "text": "some of whom are more intelligent than us. I certainly have a lot\nof experience with this.",
    "start": "9562632",
    "end": "9569213"
  },
  {
    "text": "(laughs) Of having people working with\nme who are smarter than me.",
    "start": "9569213",
    "end": "9574964"
  },
  {
    "text": "That's actually a wonderful thing. So having machines that\nare smarter than us,",
    "start": "9574964",
    "end": "9580290"
  },
  {
    "text": "that assist us in all of\nour tasks, our daily lives, whether it's professional or personal,",
    "start": "9580290",
    "end": "9585810"
  },
  {
    "text": "I think would be an\nabsolutely wonderful thing. Because intelligence is the commodity",
    "start": "9585810",
    "end": "9590402"
  },
  {
    "text": "that is most in demand. I mean, all the mistakes\nthat humanity makes is because of lack of\nintelligence, really,",
    "start": "9592585",
    "end": "9599310"
  },
  {
    "text": "or lack of knowledge, which is related. So making people smarter\nwhich can only be better.",
    "start": "9599310",
    "end": "9607410"
  },
  {
    "text": "I mean, for the same reason that public education is a good thing",
    "start": "9607410",
    "end": "9610640"
  },
  {
    "text": "and books are a good thing, and the internet is also a\ngood thing, intrinsically. And even social networks are a good thing",
    "start": "9612600",
    "end": "9619800"
  },
  {
    "text": "if you run them properly. (laughs) It's difficult, but you can.",
    "start": "9619800",
    "end": "9623239"
  },
  {
    "text": "Because it helps the communication",
    "start": "9625620",
    "end": "9630620"
  },
  {
    "text": "of information and knowledge and the transmission of knowledge. So AI is gonna make humanity smarter.",
    "start": "9631410",
    "end": "9636753"
  },
  {
    "text": "And the analogy I've been using is the fact that perhaps\nan equivalent event",
    "start": "9637920",
    "end": "9644887"
  },
  {
    "text": "in the history of humanity to what might be provided by\ngeneralization of AI assistant",
    "start": "9644887",
    "end": "9652640"
  },
  {
    "text": "is the invention of the printing press. It made everybody smarter. The fact that people could\nhave access to books.",
    "start": "9653130",
    "end": "9662122"
  },
  {
    "text": "Books were a lot cheaper\nthan they were before. And so a lot more people had\nan incentive to learn to read,",
    "start": "9663720",
    "end": "9670830"
  },
  {
    "text": "which wasn't the case before. And people became smarter.",
    "start": "9670830",
    "end": "9677730"
  },
  {
    "text": "It enabled the enlightenment, right? There wouldn't be an enlightenment without the printing press.",
    "start": "9677730",
    "end": "9684689"
  },
  {
    "text": "It enabled philosophy, rationalism,",
    "start": "9684690",
    "end": "9689690"
  },
  {
    "text": "escape from religious doctrine, democracy, science.",
    "start": "9690817",
    "end": "9698522"
  },
  {
    "text": "And certainly without this there wouldn't have been\nthe American Revolution or the French Revolution.",
    "start": "9701550",
    "end": "9707520"
  },
  {
    "text": "And so we'll still be under\nfeudal regimes perhaps.",
    "start": "9707520",
    "end": "9712173"
  },
  {
    "text": "And so it completely transformed the world because people became smarter",
    "start": "9713910",
    "end": "9719430"
  },
  {
    "text": "and kinda learned about things. Now, it also created 200 years",
    "start": "9719430",
    "end": "9725040"
  },
  {
    "text": "of essentially religious\nconflicts in Europe, right? Because the first thing that\npeople read was the Bible",
    "start": "9725040",
    "end": "9732450"
  },
  {
    "text": "and realized that perhaps there was a different\ninterpretation of the Bible than what the priests were telling them.",
    "start": "9732450",
    "end": "9739920"
  },
  {
    "text": "And so that created\nthe Protestant movement and created a rift. And in fact, the Catholic church",
    "start": "9739920",
    "end": "9745512"
  },
  {
    "text": "didn't like the idea of the printing press but they had no choice. And so it had some bad\neffects and some good effects.",
    "start": "9745512",
    "end": "9752273"
  },
  {
    "text": "I don't think anyone today would say that the invention\nof the printing press had an overall negative effect",
    "start": "9752274",
    "end": "9758250"
  },
  {
    "text": "despite the fact that it created 200 years of religious conflicts in Europe.",
    "start": "9758250",
    "end": "9764370"
  },
  {
    "text": "Now compare this, and I was very proud of myself to come up with this analogy,",
    "start": "9764370",
    "end": "9771630"
  },
  {
    "text": "but realized someone else came\nwith the same idea before me. Compare this with what\nhappened in the Ottoman Empire.",
    "start": "9771630",
    "end": "9778890"
  },
  {
    "text": "The Ottoman Empire banned the\nprinting press for 200 years.",
    "start": "9778890",
    "end": "9783890"
  },
  {
    "text": "And it didn't ban it for all languages, only for Arabic. You could actually print books",
    "start": "9787410",
    "end": "9793230"
  },
  {
    "text": "in Latin or Hebrew or whatever\nin the Ottoman Empire, just not in Arabic.",
    "start": "9793230",
    "end": "9799293"
  },
  {
    "text": "And I thought it was because the rulers just wanted to preserve",
    "start": "9800670",
    "end": "9807720"
  },
  {
    "text": "the control over the\npopulation and the dogma, religious dogma and everything.",
    "start": "9807720",
    "end": "9812223"
  },
  {
    "text": "But after talking with\nthe UAE Minister of AI,",
    "start": "9813060",
    "end": "9817353"
  },
  {
    "text": "Omar Al Olama, he told me no, there was another reason.",
    "start": "9818580",
    "end": "9824163"
  },
  {
    "text": "And the other reason was that it was to preserve the corporation\nof calligraphers, right?",
    "start": "9825114",
    "end": "9832877"
  },
  {
    "text": "There's like an art form which is writing those\nbeautiful Arabic poems",
    "start": "9833820",
    "end": "9841040"
  },
  {
    "text": "or whatever religious text in this thing. And it was very powerful\ncorporation of scribes basically",
    "start": "9842220",
    "end": "9847979"
  },
  {
    "text": "that kinda run a big chunk of the empire. And we couldn't put them out of business.",
    "start": "9847980",
    "end": "9854189"
  },
  {
    "text": "So they banned the bridging press in part to protect that business.",
    "start": "9854190",
    "end": "9858604"
  },
  {
    "text": "Now, what's the analogy for AI today? Like who are we protecting by banning AI? Like who are the people who\nare asking that AI be regulated",
    "start": "9861330",
    "end": "9868920"
  },
  {
    "text": "to protect their jobs? And of course, it's a real question",
    "start": "9868920",
    "end": "9875250"
  },
  {
    "text": "of what is gonna be the effect of technological transformation like AI",
    "start": "9875250",
    "end": "9881489"
  },
  {
    "text": "on the job market and the labor market? And there are economists",
    "start": "9881490",
    "end": "9886920"
  },
  {
    "text": "who are much more expert\nat this than I am, but when I talk to them, they tell us we're not\ngonna run out of job.",
    "start": "9886920",
    "end": "9894236"
  },
  {
    "text": "This is not gonna cause mass unemployment. This is just gonna be gradual shift",
    "start": "9894237",
    "end": "9901020"
  },
  {
    "text": "of different professions. The professions that are gonna be hot 10 or 15 years from now,",
    "start": "9901020",
    "end": "9905883"
  },
  {
    "text": "we have no idea today\nwhat they're gonna be. The same way if we go\nback 20 years in the past,",
    "start": "9906870",
    "end": "9912149"
  },
  {
    "text": "like who could have thought 20 years ago that like the hottest job,",
    "start": "9912150",
    "end": "9917580"
  },
  {
    "text": "even like 5, 10 years ago\nwas mobile app developer? Like smartphones weren't invented.",
    "start": "9917580",
    "end": "9923399"
  },
  {
    "text": "- Most of the jobs of the future might be in the Metaverse. (laughs) - Well, it could be. Yeah.",
    "start": "9923400",
    "end": "9929069"
  },
  {
    "text": "- But the point is you\ncan't possibly predict. But you're right. I mean, you've made a\nlot of strong points.",
    "start": "9929070",
    "end": "9935301"
  },
  {
    "text": "And I believe that people\nare fundamentally good, and so if AI, especially open source AI",
    "start": "9936213",
    "end": "9942237"
  },
  {
    "text": "can make them smarter, it just empowers the goodness in humans.",
    "start": "9942237",
    "end": "9948359"
  },
  {
    "text": "- So I share that feeling. Okay? I think people are\nfundamentally good. (laughing)",
    "start": "9948360",
    "end": "9954390"
  },
  {
    "text": "And in fact a lot of doomers are doomers because they don't think that\npeople are fundamentally good.",
    "start": "9954390",
    "end": "9960360"
  },
  {
    "text": "And they either don't trust people or they don't trust the\ninstitution to do the right thing",
    "start": "9960361",
    "end": "9967860"
  },
  {
    "text": "so that people behave properly. - Well, I think both you\nand I believe in humanity,",
    "start": "9967860",
    "end": "9973500"
  },
  {
    "text": "and I think I speak for a lot of people in saying thank you for pushing\nthe open source movement,",
    "start": "9973500",
    "end": "9980040"
  },
  {
    "text": "pushing to making both\nresearch and AI open source, making it available to people,",
    "start": "9980040",
    "end": "9985619"
  },
  {
    "text": "and also the models themselves, making that open source also. So thank you for that. And thank you for speaking your mind",
    "start": "9985620",
    "end": "9992250"
  },
  {
    "text": "in such colorful and beautiful\nways on the internet. I hope you never stop. You're one of the most fun people I know",
    "start": "9992250",
    "end": "9997890"
  },
  {
    "text": "and get to be a fan of. So Yann, thank you for\nspeaking to me once again, and thank you for being you.",
    "start": "9997890",
    "end": "10003979"
  },
  {
    "text": "- Thank you Lex. - Thanks for listening to this\nconversation with Yann LeCun. To support this podcast,",
    "start": "10003980",
    "end": "10009620"
  },
  {
    "text": "please check out our\nsponsors in the description. And now let me leave you with some words from Arthur C. Clarke,",
    "start": "10009620",
    "end": "10015652"
  },
  {
    "text": "\"the only way to discover\nthe limits of the possible is to go beyond them into the impossible.\"",
    "start": "10016917",
    "end": "10023536"
  },
  {
    "text": "Thank you for listening and\nhope to see you next time.",
    "start": "10024620",
    "end": "10027773"
  }
]