[
  {
    "start": "0",
    "end": "505000"
  },
  {
    "text": "today we will talk about deep reinforcement learning the question we",
    "start": "0",
    "end": "8099"
  },
  {
    "text": "would like to explore it's to which degree we can teach systems to act to",
    "start": "8099",
    "end": "15599"
  },
  {
    "text": "perceive and act in this world from data so let's take a step back and think of",
    "start": "15599",
    "end": "22650"
  },
  {
    "text": "what is the full range of tasks then artificial intelligence system needs to accomplish here's the stack from top to",
    "start": "22650",
    "end": "30720"
  },
  {
    "text": "bottom top the input bottom output the environment at the top the world that",
    "start": "30720",
    "end": "36690"
  },
  {
    "text": "the agent is operating in sensed by sensors taking in the world outside and",
    "start": "36690",
    "end": "43379"
  },
  {
    "text": "converting it to raw data interpretable by machines sensor data and from that",
    "start": "43379",
    "end": "50160"
  },
  {
    "text": "raw sensor data you extract features you extract structure from that data such",
    "start": "50160",
    "end": "58350"
  },
  {
    "text": "that you can input it make sense of it discriminate separate understand the",
    "start": "58350",
    "end": "64830"
  },
  {
    "text": "data and as we discussed you form higher",
    "start": "64830",
    "end": "70500"
  },
  {
    "text": "and higher order representations a hierarchy of representations based on which the machine learning techniques",
    "start": "70500",
    "end": "77280"
  },
  {
    "text": "can then be applied once the machine",
    "start": "77280",
    "end": "83759"
  },
  {
    "text": "learning techniques the understanding as I mentioned converts the data into",
    "start": "83759",
    "end": "88950"
  },
  {
    "text": "features into higher order representations and into simple actionable useful information we",
    "start": "88950",
    "end": "95280"
  },
  {
    "text": "aggregate that information into knowledge we take the pieces of knowledge extracted from the data",
    "start": "95280",
    "end": "100640"
  },
  {
    "text": "through the machine learning techniques and to build a taxonomy a library of",
    "start": "100640",
    "end": "108659"
  },
  {
    "text": "knowledge and with that knowledge we reason an aging estas to reason to",
    "start": "108659",
    "end": "117570"
  },
  {
    "text": "aggregate to connect pieces of data it's seen in the recent past or the distant",
    "start": "117570",
    "end": "124409"
  },
  {
    "text": "past to make sense of the world that's operating in and finally to make a plan",
    "start": "124409",
    "end": "129750"
  },
  {
    "text": "of how to act in that world based on its objectives based on what it wants to accomplished as I mentioned a simple but",
    "start": "129750",
    "end": "137900"
  },
  {
    "text": "commonly accepted definition of intelligence is a system that's able to accomplish complex goals so system",
    "start": "137900",
    "end": "145580"
  },
  {
    "text": "that's operating in the environment in this world must have a goal must have an objective function a reward function and",
    "start": "145580",
    "end": "152540"
  },
  {
    "text": "based on that it forms a plan and takes action and because there operates in",
    "start": "152540",
    "end": "157550"
  },
  {
    "text": "many cases in the physical world it must have tools effectors with which",
    "start": "157550",
    "end": "163130"
  },
  {
    "text": "it applies the actions to change something about the world that's the full stack of an artificial intelligence",
    "start": "163130",
    "end": "170780"
  },
  {
    "text": "system that acts in the world and the question is what kind of task can such a",
    "start": "170780",
    "end": "178790"
  },
  {
    "text": "system take on what kind of task can an artificial intelligence system learn as",
    "start": "178790",
    "end": "184330"
  },
  {
    "text": "we understand AI today we will talk about the advancement of deeper",
    "start": "184330",
    "end": "191180"
  },
  {
    "text": "enforcement learning approaches and some of the fascinating ways it's able to take much of the stack and treat it as",
    "start": "191180",
    "end": "198739"
  },
  {
    "text": "an end-to-end learning problem but we look at games we look at simple",
    "start": "198739",
    "end": "204380"
  },
  {
    "text": "formalized worlds while it's still impressive beautiful and unprecedented accomplishments it's nevertheless formal",
    "start": "204380",
    "end": "212120"
  },
  {
    "text": "tasks can we then move beyond games and into expert tasks of medical diagnosis",
    "start": "212120",
    "end": "220459"
  },
  {
    "text": "of design and into natural language and",
    "start": "220459",
    "end": "225769"
  },
  {
    "text": "finally the human level tasks of emotion imagination consciousness let's once",
    "start": "225769",
    "end": "237170"
  },
  {
    "text": "again review the stack in practicality in the tools we have the input for",
    "start": "237170",
    "end": "243980"
  },
  {
    "text": "robots operating in the world from cars to humanoid to drones as light our",
    "start": "243980",
    "end": "250880"
  },
  {
    "text": "camera radar GPS stereo cameras audio microphone networking for communication",
    "start": "250880",
    "end": "257600"
  },
  {
    "text": "and the various ways to measure kinematics with IMU",
    "start": "257600",
    "end": "263419"
  },
  {
    "text": "the raw sensory data is then processed features of form to representations are",
    "start": "263419",
    "end": "269930"
  },
  {
    "text": "formed and multiple higher and higher order representations that's what deep learning gets us before",
    "start": "269930",
    "end": "275449"
  },
  {
    "text": "neural networks before the advent of before the recent successes of neural",
    "start": "275449",
    "end": "281029"
  },
  {
    "text": "networks to go deeper and therefore be able to form high order representations of the data that was done by experts by",
    "start": "281029",
    "end": "288560"
  },
  {
    "text": "human experts today networks are able to do that that's the representation piece",
    "start": "288560",
    "end": "293569"
  },
  {
    "text": "and on top of the representation piece the final layers these networks are able",
    "start": "293569",
    "end": "298789"
  },
  {
    "text": "to accomplish the supervised learning tasks the generative tasks and the",
    "start": "298789",
    "end": "304270"
  },
  {
    "text": "unsupervised clustering tasks through machine learning that's what we talked",
    "start": "304270",
    "end": "310550"
  },
  {
    "text": "about a little in lecture one and we'll continue tomorrow and Wednesday",
    "start": "310550",
    "end": "316689"
  },
  {
    "text": "that's supervised learning and you can think about the output of those networks",
    "start": "316689",
    "end": "321919"
  },
  {
    "text": "as simple clean useful valuable information that's the knowledge and",
    "start": "321919",
    "end": "327879"
  },
  {
    "text": "that knowledge can be in the form of single numbers it could be regression",
    "start": "327879",
    "end": "334550"
  },
  {
    "text": "continuous variables it could be a sequence of numbers it can be images audio sentences text speech once that",
    "start": "334550",
    "end": "345409"
  },
  {
    "text": "knowledge is extracted and aggregated how do we connect it in multi resolution",
    "start": "345409",
    "end": "351439"
  },
  {
    "text": "always form hierarchies of ideas connect ideas the trivial silly example is",
    "start": "351439",
    "end": "358479"
  },
  {
    "text": "connecting images activity recognition and audio for example if it looks like a",
    "start": "358479",
    "end": "364849"
  },
  {
    "text": "duck quacks like a duck and swims like a duck we do not currently have approaches",
    "start": "364849",
    "end": "370699"
  },
  {
    "text": "that effectively integrate this information to produce a higher confidence estimate that is in fact the",
    "start": "370699",
    "end": "377689"
  },
  {
    "text": "duck and the planning piece the task of taking the sensory information fusing",
    "start": "377689",
    "end": "385430"
  },
  {
    "text": "the sensory information and making action control and longer-term plans based on that information as we'll",
    "start": "385430",
    "end": "393800"
  },
  {
    "text": "discuss today are more and more amenable to the learning approach to the deep learning",
    "start": "393800",
    "end": "399840"
  },
  {
    "text": "approach but to date have been the most successful and non learning optimization based approaches like with the several",
    "start": "399840",
    "end": "406530"
  },
  {
    "text": "of the guest speakers we have including the creator of this robot Atlas in Boston Dynamics so the question how much",
    "start": "406530",
    "end": "416460"
  },
  {
    "text": "of the stack can be learned and to end from the input to the output we know we can learn the representation and the",
    "start": "416460",
    "end": "423630"
  },
  {
    "text": "knowledge from the representation and to knowledge even with the kernel methods of SVM and certainly with with neural",
    "start": "423630",
    "end": "433590"
  },
  {
    "text": "networks mapping from representation to information has been where the primary",
    "start": "433590",
    "end": "441060"
  },
  {
    "text": "success of machine learning over the past three decades has been mapping from",
    "start": "441060",
    "end": "446340"
  },
  {
    "text": "raw sensory data to knowledge that's where the success the automated",
    "start": "446340",
    "end": "451620"
  },
  {
    "text": "representation learning of deep learning has been a success going straight from",
    "start": "451620",
    "end": "457350"
  },
  {
    "text": "raw data to knowledge the open question for us today and beyond is if we can",
    "start": "457350",
    "end": "463950"
  },
  {
    "text": "expand the red box there of what can be learned and to end from sensory data to",
    "start": "463950",
    "end": "468990"
  },
  {
    "text": "reasoning so aggregating forming higher representations of the extracted knowledge and forming plans and acting",
    "start": "468990",
    "end": "477510"
  },
  {
    "text": "in this world from the raw sensory data we will show the incredible fact that we're able to do CERN exactly what's",
    "start": "477510",
    "end": "485190"
  },
  {
    "text": "shown here and to end with deeper enforcement learning on trivial tasks in a generalizable way the question is",
    "start": "485190",
    "end": "492750"
  },
  {
    "text": "whether that can then move on to real-world tasks of autonomous vehicles",
    "start": "492750",
    "end": "497790"
  },
  {
    "text": "of humanoid robotics and so on that's",
    "start": "497790",
    "end": "503880"
  },
  {
    "text": "the open question so today let's talk about reinforcement learning there's three types of machine learning",
    "start": "503880",
    "end": "511400"
  },
  {
    "start": "505000",
    "end": "1430000"
  },
  {
    "text": "supervised unsupervised are the categories at the",
    "start": "511880",
    "end": "517440"
  },
  {
    "text": "extremes in relative to the amount of human and human input that's required",
    "start": "517440",
    "end": "522630"
  },
  {
    "text": "for supervised learning every piece of data that's used for teaching these systems is first labeled by human beings",
    "start": "522630",
    "end": "530339"
  },
  {
    "text": "and unsupervised learning on the right is no data is labeled by human beings in",
    "start": "530339",
    "end": "536540"
  },
  {
    "text": "between is some sparse input from humans semi-supervised learning is when only",
    "start": "536540",
    "end": "544740"
  },
  {
    "text": "part of the data is provided by humans ground truth and the rest must be inferred generalized by the system and",
    "start": "544740",
    "end": "551700"
  },
  {
    "text": "that's what reinforcement learning Falls reinforcement learning has shown there",
    "start": "551700",
    "end": "557220"
  },
  {
    "text": "with the cats as I said every successful presentation must include cats they're",
    "start": "557220",
    "end": "564180"
  },
  {
    "text": "supposed to be Pavlov's cats and ringing a bell and every time they ring a bell",
    "start": "564180",
    "end": "570450"
  },
  {
    "text": "they're given food and they learn this process the goal of reinforcement",
    "start": "570450",
    "end": "576420"
  },
  {
    "text": "learning is to learn from sparse reward",
    "start": "576420",
    "end": "581490"
  },
  {
    "text": "data from learn from sparse supervised data and take advantage of the fact that",
    "start": "581490",
    "end": "587130"
  },
  {
    "text": "in simulation or in the real world there is a temporal consistency to the world there is a temporal dynamics that",
    "start": "587130",
    "end": "595020"
  },
  {
    "text": "follows from state to state the state through time and so you can propagate information even if the information that",
    "start": "595020",
    "end": "602040"
  },
  {
    "text": "you're received about the the supervision the ground truth is sparse you can follow that information back",
    "start": "602040",
    "end": "608610"
  },
  {
    "text": "through time to infer something about the reality of what happened before then even if your reward signals were weak so",
    "start": "608610",
    "end": "616770"
  },
  {
    "text": "it's using the fact that the physical world evolves through time and some some",
    "start": "616770",
    "end": "622290"
  },
  {
    "text": "sort of predictable way to take sparse information and generalize it over the",
    "start": "622290",
    "end": "629160"
  },
  {
    "text": "entirety of the experience as being learned so we apply this the two problems today we'll talk about deep",
    "start": "629160",
    "end": "636120"
  },
  {
    "text": "traffic as a methodology deep reinforcement learning so deep traffic",
    "start": "636120",
    "end": "642990"
  },
  {
    "text": "is a competition that we ran last year and expanded significantly this year and",
    "start": "642990",
    "end": "649620"
  },
  {
    "text": "I'll talk about some of the details and how the folks in this room can on your smart phone today or if you have a",
    "start": "649620",
    "end": "656310"
  },
  {
    "text": "laptop training agent while I'm talking training a neural network in the browser",
    "start": "656310",
    "end": "661940"
  },
  {
    "text": "some of the things we've added our we've added the capability we've now turned it",
    "start": "661940",
    "end": "667710"
  },
  {
    "text": "into a multi agent deeper enforcement learning problem where you can control up to ten cars within your network",
    "start": "667710",
    "end": "674570"
  },
  {
    "text": "perhaps less significant but pretty cool is the ability to customize the way the",
    "start": "674570",
    "end": "681330"
  },
  {
    "text": "agent looks so you can upload and people have to an absurd degree have already",
    "start": "681330",
    "end": "687240"
  },
  {
    "text": "begun doing so uploading different images instead of the car that's shown there as long as it maintains the",
    "start": "687240",
    "end": "694290"
  },
  {
    "text": "dimensions shown here is a SpaceX rocket the competition is hosted on the website",
    "start": "694290",
    "end": "701750"
  },
  {
    "text": "self-driving cars that MIT ID you slash deep traffic will return to this later",
    "start": "701750",
    "end": "707690"
  },
  {
    "text": "the code is on github with some more information a starter code and a paper",
    "start": "707690",
    "end": "714450"
  },
  {
    "text": "describing some of the fundamental insights that will help you win at this",
    "start": "714450",
    "end": "720060"
  },
  {
    "text": "competition is an archive so from supervised learning in lecture",
    "start": "720060",
    "end": "726330"
  },
  {
    "text": "one to today supervised learning we can think of as memorization of ground truth",
    "start": "726330",
    "end": "735060"
  },
  {
    "text": "data in order to form representations that generalizes from that ground truth",
    "start": "735060",
    "end": "740360"
  },
  {
    "text": "reinforcement learning is we can think of as a way to brute force propagate",
    "start": "740360",
    "end": "746130"
  },
  {
    "text": "that information the sparse information through time to to assign quality reward",
    "start": "746130",
    "end": "757890"
  },
  {
    "text": "to state that does not directly have a reward to make sense of this world when",
    "start": "757890",
    "end": "765090"
  },
  {
    "text": "the rewards are sparse but are connected through time you can think of that as",
    "start": "765090",
    "end": "770340"
  },
  {
    "text": "reasoning so the through time is modeled in most",
    "start": "770340",
    "end": "779460"
  },
  {
    "text": "reinforcement learning approaches very simply that there's an agent taking an",
    "start": "779460",
    "end": "785430"
  },
  {
    "text": "action in a state and receiving a little reward and the agent operating in an environment execute an action receives",
    "start": "785430",
    "end": "793320"
  },
  {
    "text": "an observed state and new state and receives their reward this process continues over and over and some",
    "start": "793320",
    "end": "802980"
  },
  {
    "text": "examples we can think of any of the video games some of which we'll talk about today like Atari breakout as the",
    "start": "802980",
    "end": "811530"
  },
  {
    "text": "environment the agent is the paddle each",
    "start": "811530",
    "end": "816810"
  },
  {
    "text": "action that the agent takes has an influence on the evolution of the",
    "start": "816810",
    "end": "823050"
  },
  {
    "text": "environment and the success is measured by some reward mechanism in this case",
    "start": "823050",
    "end": "828930"
  },
  {
    "text": "points are given by the game and every game has a different point scheme that",
    "start": "828930",
    "end": "835740"
  },
  {
    "text": "must be converted normalized into a way that's interpreted by the system and the",
    "start": "835740",
    "end": "841230"
  },
  {
    "text": "goal is to maximize those points maximize the reward the continuous",
    "start": "841230",
    "end": "849030"
  },
  {
    "text": "problem of card pole by balancing the goal is to balance the pole on top of a moving cart the state is the angle the",
    "start": "849030",
    "end": "855990"
  },
  {
    "text": "angular speed the position of horizontal velocity the actions are the horizontal",
    "start": "855990",
    "end": "861210"
  },
  {
    "text": "force applied to the cart and the reward is one at each time step if the pole is still upright",
    "start": "861210",
    "end": "868700"
  },
  {
    "text": "all the first-person shooters the video games is now Starcraft the strategy games in case",
    "start": "868700",
    "end": "880700"
  },
  {
    "text": "of first-person shooter and doom what is the goal the environment is the game the",
    "start": "880700",
    "end": "885800"
  },
  {
    "text": "goal is to eliminate all opponents the state is the raw game pixels coming in the actions is moving up down left right",
    "start": "885800",
    "end": "893240"
  },
  {
    "text": "and so on and the reward is positive when eliminating an opponent and",
    "start": "893240",
    "end": "900190"
  },
  {
    "text": "negative when the agent is eliminated",
    "start": "900190",
    "end": "904900"
  },
  {
    "text": "industrial robotics been packin with a robotic arm the goal is to pick up a",
    "start": "905380",
    "end": "910910"
  },
  {
    "text": "device from a box and put it into a container the state is the raw pixels of the real world that the robot observes",
    "start": "910910",
    "end": "917660"
  },
  {
    "text": "the actions are the possible actions of the robot the different degrees of freedom are moving through those degrees",
    "start": "917660",
    "end": "923900"
  },
  {
    "text": "moving the different actuators to realize of the position of the arm and the reward is positive when placing a",
    "start": "923900",
    "end": "930980"
  },
  {
    "text": "device successfully and negative otherwise everything could be modeled in",
    "start": "930980",
    "end": "936380"
  },
  {
    "text": "this way Markov decision process there's a state as zero action a zero and reward",
    "start": "936380",
    "end": "943610"
  },
  {
    "text": "received a new state is achieved again action rewards state action rewards",
    "start": "943610",
    "end": "948890"
  },
  {
    "text": "state until a terminal state is reached and the major components of",
    "start": "948890",
    "end": "955510"
  },
  {
    "text": "reinforcement learning is a policy some kind of plan of what to do in every",
    "start": "955510",
    "end": "961010"
  },
  {
    "text": "single state what kind of action to perform a value function a some kind of",
    "start": "961010",
    "end": "968540"
  },
  {
    "text": "sense of what is a good state to be in of what is a good action to take in a state and sometimes a model that the",
    "start": "968540",
    "end": "979640"
  },
  {
    "text": "agent represents the environment with some kind of sense of the environment its operating in the dynamics of that",
    "start": "979640",
    "end": "986060"
  },
  {
    "text": "environment that's useful for making decisions about actions let's take a",
    "start": "986060",
    "end": "991310"
  },
  {
    "text": "trivial example a grid world of three by four twelve",
    "start": "991310",
    "end": "998350"
  },
  {
    "text": "squares we start at the bottom left and their task with walking about this world",
    "start": "998350",
    "end": "1004889"
  },
  {
    "text": "to maximize reward they're awarded at the top right is a plus 1 and a 1 square",
    "start": "1004889",
    "end": "1011129"
  },
  {
    "text": "below that is a negative 1 and every step you take is a punishment or is a",
    "start": "1011129",
    "end": "1016350"
  },
  {
    "text": "negative reward of 0.04 so what is the optimal policy in this world now when",
    "start": "1016350",
    "end": "1024720"
  },
  {
    "text": "everything is deterministic perhaps this is the policy when you start the bottom",
    "start": "1024720",
    "end": "1031079"
  },
  {
    "text": "left well because every step hurts every step has a negative reward then you want to take the shortest path",
    "start": "1031079",
    "end": "1037918"
  },
  {
    "text": "to the maximum square with a maximum reward when the state space is",
    "start": "1037919",
    "end": "1042959"
  },
  {
    "text": "non-deterministic as presented before with a probability of 0.8 when you",
    "start": "1042959",
    "end": "1051059"
  },
  {
    "text": "choose to go up you go up but with probability 0.1 you go left and point 1",
    "start": "1051059",
    "end": "1057210"
  },
  {
    "text": "you go right unfair again much like life that would be the optimal policy what is",
    "start": "1057210",
    "end": "1066270"
  },
  {
    "text": "the Keith observation here that every single state in the space must have a plan because you can't because then a",
    "start": "1066270",
    "end": "1074760"
  },
  {
    "text": "non-deterministic aspect of the control you can't control where you're going to",
    "start": "1074760",
    "end": "1080039"
  },
  {
    "text": "end up so you must have a plan for every place that's the policy having an action an optimal action to take in every",
    "start": "1080039",
    "end": "1086610"
  },
  {
    "text": "single state now suppose we change the reward structure and for every step we",
    "start": "1086610",
    "end": "1091710"
  },
  {
    "text": "take there's a negative reward is a negative 2 so it really hurts there's a",
    "start": "1091710",
    "end": "1097230"
  },
  {
    "text": "high punishment for every single step we take so no matter what we always take",
    "start": "1097230",
    "end": "1102630"
  },
  {
    "text": "the shortest path the optimal policy is to take the shortest path to the to the only spot on the board that doesn't",
    "start": "1102630",
    "end": "1110370"
  },
  {
    "text": "result in punishment if we decrease the",
    "start": "1110370",
    "end": "1115710"
  },
  {
    "text": "reward of each step to negative 0.1 the policy changes whether",
    "start": "1115710",
    "end": "1123430"
  },
  {
    "text": "some extra degree of wandering encouraged and as we go further and",
    "start": "1123430",
    "end": "1130000"
  },
  {
    "text": "further in lowering the punishment as before to negative 0.04 more wandering",
    "start": "1130000",
    "end": "1136210"
  },
  {
    "text": "and more wandering is allowed and when we finally turn the reward into positive",
    "start": "1136210",
    "end": "1146230"
  },
  {
    "text": "so every step it every step is increases",
    "start": "1146230",
    "end": "1152650"
  },
  {
    "text": "the reward then there's a significant incentive to to stay on the board",
    "start": "1152650",
    "end": "1158170"
  },
  {
    "text": "without ever reaching the destination kind of like college for a lot of people",
    "start": "1158170",
    "end": "1166050"
  },
  {
    "text": "so the value function the way we think about the value of a state or the value",
    "start": "1167760",
    "end": "1174340"
  },
  {
    "text": "of anything in the environment is the reward were likely to receive in the",
    "start": "1174340",
    "end": "1181360"
  },
  {
    "text": "future and the way we see the reward were likely to receive as we discount",
    "start": "1181360",
    "end": "1187720"
  },
  {
    "text": "the future award because we can't always count on it",
    "start": "1187720",
    "end": "1192930"
  },
  {
    "text": "here Gama further and further out into the future more and more discounts",
    "start": "1192930",
    "end": "1199180"
  },
  {
    "text": "decreases the reward the importance of the reward received and the good",
    "start": "1199180",
    "end": "1205060"
  },
  {
    "text": "strategy is taking the sum of these rewards and maximizing it maximizing the scoundrel ward",
    "start": "1205060",
    "end": "1211240"
  },
  {
    "text": "that's what reinforcement learning hopes to achieve and with cue learning we use",
    "start": "1211240",
    "end": "1219510"
  },
  {
    "text": "any policy to estimate the value of taking an action in a state so off",
    "start": "1219510",
    "end": "1228520"
  },
  {
    "text": "policy forget policy we move about the world and use the bellman equation here",
    "start": "1228520",
    "end": "1234550"
  },
  {
    "text": "on the bottom to continuously update our estimate of how good a certain action is",
    "start": "1234550",
    "end": "1239650"
  },
  {
    "text": "in a certain state so we don't need this",
    "start": "1239650",
    "end": "1245020"
  },
  {
    "text": "this allows us to operate in a much larger state space in a much larger action space we move about this world",
    "start": "1245020",
    "end": "1250600"
  },
  {
    "text": "through simulation or in the real world taking actions and updating our estimate of how good certain actions are over",
    "start": "1250600",
    "end": "1257080"
  },
  {
    "text": "I'm the new state at the left is the is the updated value the old state is the",
    "start": "1257080",
    "end": "1264460"
  },
  {
    "text": "starting value for the equation and we update that old state estimation with the sum of the reward received by taking",
    "start": "1264460",
    "end": "1273130"
  },
  {
    "text": "action s tax action a and state us and",
    "start": "1273130",
    "end": "1278340"
  },
  {
    "text": "the maximum reward that's possible to be received in the following states",
    "start": "1278340",
    "end": "1285390"
  },
  {
    "text": "discounted that update is decreased with",
    "start": "1285390",
    "end": "1290590"
  },
  {
    "text": "a learning rate the higher the learning rate the more value we the the faster",
    "start": "1290590",
    "end": "1295930"
  },
  {
    "text": "will learn the more value we assigned to new information that's simple that's it",
    "start": "1295930",
    "end": "1301000"
  },
  {
    "text": "that's Q learning the simple update rule allows us to to explore the world and as",
    "start": "1301000",
    "end": "1307570"
  },
  {
    "text": "we explore get more and more information about what's good to do in this world",
    "start": "1307570",
    "end": "1313600"
  },
  {
    "text": "and there's always a balance in the various problem spaces we'll discuss there's always a balance between",
    "start": "1313600",
    "end": "1319830"
  },
  {
    "text": "exploration and exploitation as you form",
    "start": "1319830",
    "end": "1325420"
  },
  {
    "text": "a better and better estimate of the Q function of what actions are good to take you start to get a sense of what is",
    "start": "1325420",
    "end": "1331390"
  },
  {
    "text": "the best action to take but it's not a perfect sense it's still an approximation and so there's value of",
    "start": "1331390",
    "end": "1337540"
  },
  {
    "text": "exploration but the better and better your estimate becomes the less and less exploration has a benefit so usually we",
    "start": "1337540",
    "end": "1345280"
  },
  {
    "text": "want to explore a lot in the beginning and less and less so towards the end and when we finally release the system out",
    "start": "1345280",
    "end": "1351910"
  },
  {
    "text": "into the world and wish it to operate its best then we have it operate as a",
    "start": "1351910",
    "end": "1358150"
  },
  {
    "text": "greedy system always taking the optimal action according to the q2 key value function and everything I'm talking",
    "start": "1358150",
    "end": "1366730"
  },
  {
    "text": "about now is permit rised and our parameters that are very important for",
    "start": "1366730",
    "end": "1373570"
  },
  {
    "text": "winning the deep traffic competition which is using this very algorithm with",
    "start": "1373570",
    "end": "1379420"
  },
  {
    "text": "a neural network at its core so for sin",
    "start": "1379420",
    "end": "1384600"
  },
  {
    "text": "table representation of a cue function where the y-axis is state four states s",
    "start": "1384600",
    "end": "1390840"
  },
  {
    "text": "one two three four and the x-axis is actions a one two three four we can",
    "start": "1390840",
    "end": "1398460"
  },
  {
    "text": "think of this table as randomly initiated or initiated initialized in any kind of way that's not",
    "start": "1398460",
    "end": "1405750"
  },
  {
    "text": "representative of actual reality and as we move about this world and we take actions we update this table with the",
    "start": "1405750",
    "end": "1411960"
  },
  {
    "text": "bellman equation shown up top and here slides now are online you can see a",
    "start": "1411960",
    "end": "1417270"
  },
  {
    "text": "simple pseudocode algorithm of how to update it how to run this bellman",
    "start": "1417270",
    "end": "1422580"
  },
  {
    "text": "equation and over time the approximation becomes the optimal cue table",
    "start": "1422580",
    "end": "1429000"
  },
  {
    "text": "the problem is when that cue table it becomes exponential in size when we take",
    "start": "1429000",
    "end": "1435930"
  },
  {
    "start": "1430000",
    "end": "2160000"
  },
  {
    "text": "in raw sensory information as we do with cameras with deep crash or with deep",
    "start": "1435930",
    "end": "1441780"
  },
  {
    "text": "traffic it's taking the full grid space and taking that information the raw the",
    "start": "1441780",
    "end": "1447660"
  },
  {
    "text": "raw grid pixels of deep traffic and when you take the arcade games here they're",
    "start": "1447660",
    "end": "1453270"
  },
  {
    "text": "taking the raw pixels of the game or when we take go the game of go when it's",
    "start": "1453270",
    "end": "1459900"
  },
  {
    "text": "taking the units the the board the raw state of the board as the input the",
    "start": "1459900",
    "end": "1467990"
  },
  {
    "text": "potential state space the number of possible combinations of what states it",
    "start": "1467990",
    "end": "1475050"
  },
  {
    "text": "possible is extremely large larger than we can certainly hold the memory and",
    "start": "1475050",
    "end": "1480990"
  },
  {
    "text": "larger that we can ever be able to accurately approximate through the bellman equation over time through",
    "start": "1480990",
    "end": "1488220"
  },
  {
    "text": "simulation through the simple update of the bellman equation so this is where",
    "start": "1488220",
    "end": "1494610"
  },
  {
    "text": "deep reinforcement learning comes in neural networks are really good approximate errs they're really good at",
    "start": "1494610",
    "end": "1501600"
  },
  {
    "text": "exactly this task of learning this kind of cue table",
    "start": "1501600",
    "end": "1507410"
  },
  {
    "text": "so as we started with supervised learning or neural networks helped us memorize patterns using supervised",
    "start": "1509040",
    "end": "1515460"
  },
  {
    "text": "ground true data and we'll move to reinforcement learning that hopes to propagate outcomes to knowledge deep",
    "start": "1515460",
    "end": "1525370"
  },
  {
    "text": "learning allows us to do so on much larger state spaces are much larger",
    "start": "1525370",
    "end": "1530700"
  },
  {
    "text": "action spaces which means it's generalizable it's much more capable to",
    "start": "1530700",
    "end": "1538390"
  },
  {
    "text": "deal with the raw stuff of sensory data which means it's much more capable to",
    "start": "1538390",
    "end": "1546190"
  },
  {
    "text": "deal with the broad variation of real world applications and it does so",
    "start": "1546190",
    "end": "1555130"
  },
  {
    "text": "because it's able to learn the representations as we discussed on",
    "start": "1555130",
    "end": "1561100"
  },
  {
    "text": "Monday the understanding comes from",
    "start": "1561100",
    "end": "1567030"
  },
  {
    "text": "converting the raw sensory information into into simple useful information",
    "start": "1567030",
    "end": "1572830"
  },
  {
    "text": "based on which the action in this particular state can be taken in the same exact way so instead of the cue",
    "start": "1572830",
    "end": "1579970"
  },
  {
    "text": "table instead of this cue function we plug in a neural network where the input is the state space no matter how complex",
    "start": "1579970",
    "end": "1586990"
  },
  {
    "text": "and the output is a value for each of the actions that you could take input is",
    "start": "1586990",
    "end": "1595510"
  },
  {
    "text": "the state output is the value of the function it's simple this is deep Q",
    "start": "1595510",
    "end": "1603220"
  },
  {
    "text": "Network DQ one at the core of the success of deep mind a lot of the cool",
    "start": "1603220",
    "end": "1609640"
  },
  {
    "text": "stuff you see about video games D queuing or variants of DQ and our play",
    "start": "1609640",
    "end": "1614670"
  },
  {
    "text": "this is water first with a nature paper a deep mind the success came of playing",
    "start": "1614670",
    "end": "1623050"
  },
  {
    "text": "the different games including Atari games",
    "start": "1623050",
    "end": "1628350"
  },
  {
    "text": "how are these things trained very similar to supervised learning the",
    "start": "1630130",
    "end": "1638860"
  },
  {
    "text": "bellman equation up top it takes the reward and the discounted",
    "start": "1638860",
    "end": "1645880"
  },
  {
    "text": "expected reward from future states the",
    "start": "1645880",
    "end": "1652480"
  },
  {
    "text": "loss function here for neural network and you'll now work learners with a loss function it takes the reward received at",
    "start": "1652480",
    "end": "1660520"
  },
  {
    "text": "the current state does a forward pass through a neural network to estimate the value of the future state of the best",
    "start": "1660520",
    "end": "1669420"
  },
  {
    "text": "action to take in the future state and then subtract that from the forward pass",
    "start": "1669420",
    "end": "1677790"
  },
  {
    "text": "through the network for the current state in action so you take the difference between what your a Q",
    "start": "1677790",
    "end": "1685620"
  },
  {
    "text": "estimator then you'll network believes the value of the current state is and what it more",
    "start": "1685620",
    "end": "1694060"
  },
  {
    "text": "likely is to be based on the value of the future states that are reachable",
    "start": "1694060",
    "end": "1699880"
  },
  {
    "text": "based on the actions you can take here's",
    "start": "1699880",
    "end": "1706870"
  },
  {
    "text": "the algorithm input is the state output is the Q value for each action or in",
    "start": "1706870",
    "end": "1713710"
  },
  {
    "text": "this diagram input is the state in action and the output is the Q value it's very similar architectures so given",
    "start": "1713710",
    "end": "1721540"
  },
  {
    "text": "a transition of s a are s prime s",
    "start": "1721540",
    "end": "1727590"
  },
  {
    "text": "current state taking an action receiving reward and achieving US prime state the",
    "start": "1727590",
    "end": "1736370"
  },
  {
    "text": "the update is to a feed-forward pass through the network for the current",
    "start": "1736370",
    "end": "1741650"
  },
  {
    "text": "state do a feed-forward pass for each of the possible actions taken in the next",
    "start": "1741650",
    "end": "1747740"
  },
  {
    "text": "state and that's how we compute the two parts of the loss function and update",
    "start": "1747740",
    "end": "1753890"
  },
  {
    "text": "the weights using back propagation again loss function back propagation is how",
    "start": "1753890",
    "end": "1759140"
  },
  {
    "text": "the network is trained this has actually been around for much longer than the",
    "start": "1759140",
    "end": "1765230"
  },
  {
    "text": "deep mind a few tricks made it made it",
    "start": "1765230",
    "end": "1770270"
  },
  {
    "text": "really work experience replays the",
    "start": "1770270",
    "end": "1775580"
  },
  {
    "text": "biggest one so as the games are played through simulation or if it's a physical",
    "start": "1775580",
    "end": "1781970"
  },
  {
    "text": "system as it acts in the world it's actually collecting the observations",
    "start": "1781970",
    "end": "1789200"
  },
  {
    "text": "into a library of experiences and that training is performed by randomly",
    "start": "1789200",
    "end": "1794930"
  },
  {
    "text": "sampling the library in the past by randomly sampling the previous",
    "start": "1794930",
    "end": "1800420"
  },
  {
    "text": "experiences and batches so you're not always training on the natural",
    "start": "1800420",
    "end": "1806000"
  },
  {
    "text": "continuous evolution of the system you're training on randomly picked batches of those experiences that's like",
    "start": "1806000",
    "end": "1812330"
  },
  {
    "text": "huge it's a it's a seems like a subtle trick but it's a really important one so",
    "start": "1812330",
    "end": "1818470"
  },
  {
    "text": "the system doesn't over fit a particular evolution of this of the game of the",
    "start": "1818470",
    "end": "1827240"
  },
  {
    "text": "simulation another important again",
    "start": "1827240",
    "end": "1833390"
  },
  {
    "text": "subtle trick as in a lot of deep learning approaches the subtle tricks make all the difference is fixing the",
    "start": "1833390",
    "end": "1841010"
  },
  {
    "text": "target network for the loss function if you notice you have to use the neural",
    "start": "1841010",
    "end": "1847610"
  },
  {
    "text": "network thick the singly neural network the gqi network to estimate the value of the current state and action pair and",
    "start": "1847610",
    "end": "1857870"
  },
  {
    "text": "next so using it multiple times and as",
    "start": "1857870",
    "end": "1863360"
  },
  {
    "text": "you perform that operation you're updating the network which means the",
    "start": "1863360",
    "end": "1868820"
  },
  {
    "text": "target function inside that loss function is always changing so you're the very nature your loss function is",
    "start": "1868820",
    "end": "1875900"
  },
  {
    "text": "changing all the time as you're learning and that's a big problem for stability that can create big problems for the",
    "start": "1875900",
    "end": "1882680"
  },
  {
    "text": "learning process so this little trick is to fix the network and only update it",
    "start": "1882680",
    "end": "1888410"
  },
  {
    "text": "every safe thousand steps so as you",
    "start": "1888410",
    "end": "1893929"
  },
  {
    "text": "train the network the the network that's used to compute the target function",
    "start": "1893929",
    "end": "1899690"
  },
  {
    "text": "inside the loss function is fixed it produces a more stable computation on a",
    "start": "1899690",
    "end": "1905840"
  },
  {
    "text": "loss function so the ground doesn't shift under you as you're trying to find",
    "start": "1905840",
    "end": "1911630"
  },
  {
    "text": "a minimal for the loss function the loss function doesn't change in unpredictable",
    "start": "1911630",
    "end": "1917450"
  },
  {
    "text": "difficult to understand ways and reward clipping which is always true with",
    "start": "1917450",
    "end": "1925220"
  },
  {
    "text": "general systems that are operating it's seeking to operate in the generalized",
    "start": "1925220",
    "end": "1930770"
  },
  {
    "text": "way is for very for these various games the points are different some some",
    "start": "1930770",
    "end": "1937610"
  },
  {
    "text": "points are low some points are high some go positive and negative and they're all normalized to a point where the good",
    "start": "1937610",
    "end": "1943730"
  },
  {
    "text": "points or the positive points are a 1 and negative points are a negative 1",
    "start": "1943730",
    "end": "1949730"
  },
  {
    "text": "that's reward clipping simplify the reward structure and because a lot of",
    "start": "1949730",
    "end": "1955550"
  },
  {
    "text": "the games are 30 FPS or 60 FPS and the actions are not it's not valuable to",
    "start": "1955550",
    "end": "1963140"
  },
  {
    "text": "take actions at such a high rate inside of these as particularly Atari games then you only take an action every four",
    "start": "1963140",
    "end": "1969740"
  },
  {
    "text": "steps while still taking in the frames as part of the temporal window to make decisions tricks but hopefully gives you",
    "start": "1969740",
    "end": "1977179"
  },
  {
    "text": "a sense of the kind of things necessary for both seminal papers like this one",
    "start": "1977179",
    "end": "1985580"
  },
  {
    "text": "and for the more important accomplishment of winning deep traffic is that",
    "start": "1985580",
    "end": "1991280"
  },
  {
    "text": "the tricks make all the difference here on the bottom is the circle is when the",
    "start": "1991280",
    "end": "1999410"
  },
  {
    "text": "technique is used in the x1 it's not looking at replay and target takes target network and experience replay",
    "start": "1999410",
    "end": "2006190"
  },
  {
    "text": "when both are used for the game of breakout River raid sea quests and Space",
    "start": "2006190",
    "end": "2011830"
  },
  {
    "text": "Invaders the higher the number the better it is the more points achieved so when it",
    "start": "2011830",
    "end": "2018760"
  },
  {
    "text": "gives you a sense that when replay and target both gives significant improvements in the performance of the",
    "start": "2018760",
    "end": "2024760"
  },
  {
    "text": "system order of magnitude improvements two orders of magnitude for breakup and",
    "start": "2024760",
    "end": "2034110"
  },
  {
    "text": "here is pseudocode of implementing dq1",
    "start": "2034110",
    "end": "2039280"
  },
  {
    "text": "the learning the key thing to notice and you can look to the slides is the the",
    "start": "2039280",
    "end": "2048760"
  },
  {
    "text": "loop the while loop of playing through the games and selecting the actions to",
    "start": "2048760",
    "end": "2054129"
  },
  {
    "text": "play is not part of the training it's it's part of the saving the observations",
    "start": "2054130",
    "end": "2062050"
  },
  {
    "text": "the state action reward next state observation is saving them into replay",
    "start": "2062050",
    "end": "2067720"
  },
  {
    "text": "memory into that library and then you sample randomly from that replay memory",
    "start": "2067720",
    "end": "2072908"
  },
  {
    "text": "to then train the network based on the loss function and with probability up up",
    "start": "2072909",
    "end": "2080560"
  },
  {
    "text": "top with the probability epsilon select a random action that epsilon is the",
    "start": "2080560",
    "end": "2086398"
  },
  {
    "text": "probability of exploration that decreases that's something you'll see in",
    "start": "2086399",
    "end": "2091810"
  },
  {
    "text": "deep traffic as well is the rate at which that exploration decreases over",
    "start": "2091810",
    "end": "2097630"
  },
  {
    "text": "time through the training process you want to explore a lot first and less and less over time so this algorithm is",
    "start": "2097630",
    "end": "2105190"
  },
  {
    "text": "being able to accomplish in 2015 and since a lot of incredible things things",
    "start": "2105190",
    "end": "2112450"
  },
  {
    "text": "that made the AI world think that we",
    "start": "2112450",
    "end": "2118770"
  },
  {
    "text": "were onto something that general AI is within reach for the first",
    "start": "2118770",
    "end": "2126640"
  },
  {
    "text": "time that raw sensor information was used to create a system that acts and makes sense of the world make sense of",
    "start": "2126640",
    "end": "2133599"
  },
  {
    "text": "the physics of the world enough to be able to succeed in it from very little information but these games are trivial",
    "start": "2133599",
    "end": "2142349"
  },
  {
    "text": "even though there is a lot of them this",
    "start": "2142829",
    "end": "2148539"
  },
  {
    "text": "dqn approach has been able to outperform a lot of the Atari games that's what's been reported on",
    "start": "2148539",
    "end": "2154319"
  },
  {
    "text": "outperform the human level performance but again these games are trivial what I",
    "start": "2154319",
    "end": "2160989"
  },
  {
    "start": "2160000",
    "end": "2510000"
  },
  {
    "text": "think and perhaps biased I'm biased but one of the greatest accomplishments of",
    "start": "2160989",
    "end": "2166869"
  },
  {
    "text": "artificial intelligence in the last decade at least from the philosophical",
    "start": "2166869",
    "end": "2172509"
  },
  {
    "text": "or the research perspective is alphago 0",
    "start": "2172509",
    "end": "2179069"
  },
  {
    "text": "first alphago and then alphago 0 its",
    "start": "2179069",
    "end": "2184390"
  },
  {
    "text": "deepmind system that beat the best in the world in a game of go so what's the",
    "start": "2184390",
    "end": "2190390"
  },
  {
    "text": "game of go it's simple I won't get into",
    "start": "2190390",
    "end": "2195400"
  },
  {
    "text": "the rules but basically it's a 19 by 19 board shown on the bottom of the slide",
    "start": "2195400",
    "end": "2202049"
  },
  {
    "text": "for the bottom row of the table for a board of 19 by 19 the number of legal",
    "start": "2202049",
    "end": "2210369"
  },
  {
    "text": "game positions is 2 times 10 to the power of 170 it's a very large number of",
    "start": "2210369",
    "end": "2217719"
  },
  {
    "text": "possible positions to consider any one time especially the game evolves the",
    "start": "2217719",
    "end": "2223150"
  },
  {
    "text": "number of possible moves is huge much larger than in chess so that's why AI",
    "start": "2223150",
    "end": "2232199"
  },
  {
    "text": "the community thought that this game is not solvable until 2016 when alphago",
    "start": "2232199",
    "end": "2241799"
  },
  {
    "text": "used this use human expert position play to seed in a supervised way",
    "start": "2241799",
    "end": "2250019"
  },
  {
    "text": "reinforcement learning approach and I'll describe in a little bit of detail and a",
    "start": "2250019",
    "end": "2255489"
  },
  {
    "text": "couple of slides here to beat the best in the world and then",
    "start": "2255489",
    "end": "2262660"
  },
  {
    "text": "alphago 0 that is the accomplishment of",
    "start": "2262660",
    "end": "2267940"
  },
  {
    "text": "the decade for me in AI is being able to play with no training data on human",
    "start": "2267940",
    "end": "2279370"
  },
  {
    "text": "expert games and beat the best in the",
    "start": "2279370",
    "end": "2284710"
  },
  {
    "text": "world in an extremely complex game this is not Atari this is and this is a much",
    "start": "2284710",
    "end": "2292140"
  },
  {
    "text": "higher order difficulty game and that and the quality of players that is",
    "start": "2292140",
    "end": "2298690"
  },
  {
    "text": "competing in is much higher and it's able to extremely quickly here to",
    "start": "2298690",
    "end": "2304270"
  },
  {
    "text": "achieve a rating that's better than alphago and better than the different",
    "start": "2304270",
    "end": "2309700"
  },
  {
    "text": "variants of alphago and certainly better than the best of the human players in 21",
    "start": "2309700",
    "end": "2315250"
  },
  {
    "text": "days of self play so how does it work all of these approaches much much like",
    "start": "2315250",
    "end": "2323650"
  },
  {
    "text": "the previous ones the traditional ones that are not based on deep learning are",
    "start": "2323650",
    "end": "2329490"
  },
  {
    "text": "using Monte Carlo tree search MCTS which is when you have such a large",
    "start": "2329490",
    "end": "2337270"
  },
  {
    "text": "state space you start at a board and you play and you choose moves with some",
    "start": "2337270",
    "end": "2346410"
  },
  {
    "text": "exploitation exploration balancing choosing to explore totally new",
    "start": "2346410",
    "end": "2352720"
  },
  {
    "text": "positions or to go deep in the positions you know are good until the bottom of the game is reached until the final",
    "start": "2352720",
    "end": "2359140"
  },
  {
    "text": "state is reached and then you back propagate the quality of the choices you",
    "start": "2359140",
    "end": "2364720"
  },
  {
    "text": "made leading to that position and in that way you learn the value of of board positions and play that's been",
    "start": "2364720",
    "end": "2374320"
  },
  {
    "text": "used by the most successful go playing engines before and alphago since but you",
    "start": "2374320",
    "end": "2382000"
  },
  {
    "text": "might be able to guess what's the difference with alphago verse to the previous approaches they use the neural",
    "start": "2382000",
    "end": "2388870"
  },
  {
    "text": "network as the intuition quote-unquote - what",
    "start": "2388870",
    "end": "2395230"
  },
  {
    "text": "are the good states what are the good next board positions to explore and the",
    "start": "2395230",
    "end": "2406240"
  },
  {
    "text": "key things again the tricks make all the difference that made alphago zero work",
    "start": "2406240",
    "end": "2413770"
  },
  {
    "text": "and work much better than alphago is first because there was no expert play",
    "start": "2413770",
    "end": "2419370"
  },
  {
    "text": "instead of human games alphago used that very same Monte Carlo",
    "start": "2419370",
    "end": "2428980"
  },
  {
    "text": "tree search algorithm MCTS to do an intelligent look ahead based on the",
    "start": "2428980",
    "end": "2434800"
  },
  {
    "text": "neural network prediction of where dove the good States to take it checked that",
    "start": "2434800",
    "end": "2440260"
  },
  {
    "text": "instead of human expert play it checked how good indeed are those states it's a",
    "start": "2440260",
    "end": "2447310"
  },
  {
    "text": "simple look ahead action that does the ground truth that does the target",
    "start": "2447310",
    "end": "2452790"
  },
  {
    "text": "correction that produces the loss function the second part is the multitask learning what's now called",
    "start": "2452790",
    "end": "2459100"
  },
  {
    "text": "multitask learning is the networkers is quote-unquote two-headed in the sense",
    "start": "2459100",
    "end": "2464830"
  },
  {
    "text": "that first it outputs the probability of which move to take the obvious thing and it's also producing a probability of",
    "start": "2464830",
    "end": "2471070"
  },
  {
    "text": "winning and there's a few ways to combine that information and continuously train both parts of the",
    "start": "2471070",
    "end": "2478450"
  },
  {
    "text": "network depending on the choice taken so you want to take the best choice in the short term and achieve the positions",
    "start": "2478450",
    "end": "2486040"
  },
  {
    "text": "that are highly a slightly hood of winning for the player that's whose turn it is and another big step is that they",
    "start": "2486040",
    "end": "2497230"
  },
  {
    "text": "updated from 2015 the updated of the state-of-the-art architecture which are",
    "start": "2497230",
    "end": "2502240"
  },
  {
    "text": "now the architecture that one imagenet as the residual networks ResNet for",
    "start": "2502240",
    "end": "2508180"
  },
  {
    "text": "imagenet those that's it and those little changes made all the",
    "start": "2508180",
    "end": "2513670"
  },
  {
    "start": "2510000",
    "end": "3275000"
  },
  {
    "text": "difference so that takes us to deep traffic and the eight billion hours",
    "start": "2513670",
    "end": "2518860"
  },
  {
    "text": "stuck in traffic America's pastime so we tried to",
    "start": "2518860",
    "end": "2524200"
  },
  {
    "text": "simulate driving that behavior layer of driving so not the immediate control not",
    "start": "2524200",
    "end": "2532359"
  },
  {
    "text": "the motion planning but beyond that on top on top of those control decisions",
    "start": "2532359",
    "end": "2537970"
  },
  {
    "text": "the human interpretable decisions of changing lane of speeding up slowing down modeling that in a micro traffic",
    "start": "2537970",
    "end": "2546550"
  },
  {
    "text": "simulation framework that's popular in traffic engineering the kind of shown here we apply deep reinforcement",
    "start": "2546550",
    "end": "2554349"
  },
  {
    "text": "learning to that I'll call it deep traffic the goal is to achieve the highest average speed over a long period",
    "start": "2554349",
    "end": "2561460"
  },
  {
    "text": "of time weaving in and out of traffic for students here the requirement is to",
    "start": "2561460",
    "end": "2567670"
  },
  {
    "text": "follow the tutorial and achieve a speed of 65 miles an hour and if you really",
    "start": "2567670",
    "end": "2575680"
  },
  {
    "text": "want to achieve a speed over 70 miles an hour which is what's acquired to win and",
    "start": "2575680",
    "end": "2582390"
  },
  {
    "text": "perhaps upload your own image to make sure you look good doing it what you",
    "start": "2582390",
    "end": "2590230"
  },
  {
    "text": "should do clear instructions to compete read the tutorial you can change",
    "start": "2590230",
    "end": "2597550"
  },
  {
    "text": "parameters in the code box on that website cars done on mighty dad you size deep traffic click the white button that",
    "start": "2597550",
    "end": "2604450"
  },
  {
    "text": "says apply code which applies the code that you write these are the parameters that you specify then you'll network it",
    "start": "2604450",
    "end": "2611470"
  },
  {
    "text": "applies those parameters creates the architecture do you specify and now you have a network written in JavaScript",
    "start": "2611470",
    "end": "2619000"
  },
  {
    "text": "living in the browser ready to be trained then you click the blue button that says run training and that trains",
    "start": "2619000",
    "end": "2626770"
  },
  {
    "text": "the network much faster than one's actually being visualized in the browser",
    "start": "2626770",
    "end": "2632290"
  },
  {
    "text": "a thousand times faster by evolving the game making decisions taking in the grid",
    "start": "2632290",
    "end": "2638470"
  },
  {
    "text": "space as I'll talk about here in a second the speed limit is 80 miles an hour based on the various adjustments",
    "start": "2638470",
    "end": "2645640"
  },
  {
    "text": "were made to the game reaching 80 miles an hour is certainly impossible an average and reaching some of the speeds",
    "start": "2645640",
    "end": "2653109"
  },
  {
    "text": "that we've achieved last year it's much much much more difficult finally when you're happy and the",
    "start": "2653109",
    "end": "2660460"
  },
  {
    "text": "training is done submit the model to competition for those super eager",
    "start": "2660460",
    "end": "2668109"
  },
  {
    "text": "dedicated students you can do so every five minutes and to visualize your",
    "start": "2668109",
    "end": "2674410"
  },
  {
    "text": "submission you can click the request visualization specifying the custom",
    "start": "2674410",
    "end": "2681310"
  },
  {
    "text": "image and the color okay so here's the simulation speed limit 80",
    "start": "2681310",
    "end": "2687880"
  },
  {
    "text": "miles an hour cars 20 on the screen one of them is a red one in this case that's that one is",
    "start": "2687880",
    "end": "2694900"
  },
  {
    "text": "controlled by a neural network its speed it's allowed the actions of speed up slow down change lanes left-right or",
    "start": "2694900",
    "end": "2703240"
  },
  {
    "text": "stay exactly the same the other cars are",
    "start": "2703240",
    "end": "2710190"
  },
  {
    "text": "pretty dumb they speed up slow down turn left right but they don't have a purpose",
    "start": "2710190",
    "end": "2716200"
  },
  {
    "text": "in their existence they do so randomly or at least purpose has not been",
    "start": "2716200",
    "end": "2721869"
  },
  {
    "text": "discovered the road the car the speed the road is a grid space an occupancy",
    "start": "2721869",
    "end": "2728950"
  },
  {
    "text": "grid that specifies when it's empty it's set to a B meaning that the the",
    "start": "2728950",
    "end": "2740410"
  },
  {
    "text": "grid value is whatever speed is achievable if you were inside that grid",
    "start": "2740410",
    "end": "2745930"
  },
  {
    "text": "and when there's other cars that are going slow the value in that grid is the",
    "start": "2745930",
    "end": "2751030"
  },
  {
    "text": "speed of that car that's the state space that's the state representation and you can choose how much what slice that",
    "start": "2751030",
    "end": "2757990"
  },
  {
    "text": "state space you take in that's the input to the neural network for a visual",
    "start": "2757990",
    "end": "2766480"
  },
  {
    "text": "Asian purposes you can choose normal speed or fast speed for watching the",
    "start": "2766480",
    "end": "2771700"
  },
  {
    "text": "network operate and there's display options to help you build intuition",
    "start": "2771700",
    "end": "2778060"
  },
  {
    "text": "about the network takes in and what space that car is operating in the default is no extra information is added",
    "start": "2778060",
    "end": "2785050"
  },
  {
    "text": "then there's the learning input which visualizes exactly which part of the",
    "start": "2785050",
    "end": "2790330"
  },
  {
    "text": "road the is serves as the input to the network then there is the safety system",
    "start": "2790330",
    "end": "2796090"
  },
  {
    "text": "which I'll describe in a little bit which is all the parts of the road the car is not allowed to go into because it",
    "start": "2796090",
    "end": "2802150"
  },
  {
    "text": "would result in a collision and that with JavaScript would be very difficult to animate and the full map here's a",
    "start": "2802150",
    "end": "2810430"
  },
  {
    "text": "safety system you could think of this system as a CC basic radar ultrasonic",
    "start": "2810430",
    "end": "2817030"
  },
  {
    "text": "sensors helping you avoid the obvious collisions to obviously detectable",
    "start": "2817030",
    "end": "2822220"
  },
  {
    "text": "objects around you and the task for this red car for the steel Network is to move about this space is to move about the",
    "start": "2822220",
    "end": "2831369"
  },
  {
    "text": "space under the constraints of the safety system the red shows all the",
    "start": "2831369",
    "end": "2837760"
  },
  {
    "text": "parts of the grid it's not able to move into so the goal for the car is to not",
    "start": "2837760",
    "end": "2843369"
  },
  {
    "text": "get stuck in traffic it's make big sweeping motions to avoid crowds of cars",
    "start": "2843369",
    "end": "2852089"
  },
  {
    "text": "the input like DQ n is the state space the output is the value of the different",
    "start": "2852119",
    "end": "2858220"
  },
  {
    "text": "actions and based on the epsilon parameter through training and through",
    "start": "2858220",
    "end": "2864420"
  },
  {
    "text": "inference evaluation process you choose how much exploration you want to do",
    "start": "2864420",
    "end": "2870130"
  },
  {
    "text": "these are all parameters the learning is done in the browser on your own computer",
    "start": "2870130",
    "end": "2877890"
  },
  {
    "text": "utilizing only the CPU the action space there's five giving you some of the",
    "start": "2878670",
    "end": "2886180"
  },
  {
    "text": "variables here perhaps you go back to the slides to look at it the brain quote unquote is the thing that takes in the",
    "start": "2886180",
    "end": "2894400"
  },
  {
    "text": "state and the reward takes a four passed through the state and produce to",
    "start": "2894400",
    "end": "2899730"
  },
  {
    "text": "the next action the brain is where the neural network is contained both of the",
    "start": "2899730",
    "end": "2905070"
  },
  {
    "text": "training and the evaluation the learning input can be controlled in width forward",
    "start": "2905070",
    "end": "2912600"
  },
  {
    "text": "length and backward length lane side number of lanes to the side that you see patches ahead as the patches ahead that",
    "start": "2912600",
    "end": "2919980"
  },
  {
    "text": "you see patches behind as patches behind the you see mu this year can control the",
    "start": "2919980",
    "end": "2927180"
  },
  {
    "text": "number of agents that are controlled by the neural network anywhere from one to",
    "start": "2927180",
    "end": "2933890"
  },
  {
    "text": "ten and the evaluation is performed",
    "start": "2933890",
    "end": "2940080"
  },
  {
    "text": "exactly the same way you have to achieve the highest average speed for the agents",
    "start": "2940080",
    "end": "2945110"
  },
  {
    "text": "the very critical thing here is the agents are not aware of each other so",
    "start": "2945110",
    "end": "2952470"
  },
  {
    "text": "they're not jointly jointly planning the network is trained under the joint",
    "start": "2952470",
    "end": "2960530"
  },
  {
    "text": "objective of achieving the average speed for all of them but the actions are taking in a greedy",
    "start": "2960530",
    "end": "2967350"
  },
  {
    "text": "way for each it's very interesting what can be learned in this way because this",
    "start": "2967350",
    "end": "2973320"
  },
  {
    "text": "kinds of approaches are scalable to an arbitrary number of cars and you could imagine us plopping down the best cars",
    "start": "2973320",
    "end": "2981240"
  },
  {
    "text": "from this class together and having them compete in this way the best neural",
    "start": "2981240",
    "end": "2987960"
  },
  {
    "text": "networks because they're full in their greedy operation the number of networks",
    "start": "2987960",
    "end": "2995550"
  },
  {
    "text": "that can concurrently operate is fully scaleable there's a lot of parameters",
    "start": "2995550",
    "end": "3001070"
  },
  {
    "text": "the temporal window the layers the many",
    "start": "3001070",
    "end": "3007910"
  },
  {
    "text": "layers types that can be added here's a fully connected layer with tenure ons the activation functions all of these",
    "start": "3007910",
    "end": "3014240"
  },
  {
    "text": "things can be customized as specified in the tutorial the final layer a fully",
    "start": "3014240",
    "end": "3020360"
  },
  {
    "text": "connected layer with output a five regression giving the value of each of",
    "start": "3020360",
    "end": "3026570"
  },
  {
    "text": "the five actions and there's a lot of more specific parameters some of which",
    "start": "3026570",
    "end": "3031760"
  },
  {
    "text": "have this just from gamma to epsilon to experience",
    "start": "3031760",
    "end": "3039400"
  },
  {
    "text": "replay size to learning rate in temporal window the optimizer the learning rate",
    "start": "3039400",
    "end": "3047950"
  },
  {
    "text": "momentum batch size l2 l1 to K for regularization and so on there's a big",
    "start": "3047950",
    "end": "3054130"
  },
  {
    "text": "white button that says apply code that you press that kills all the work you've done up to this point so be careful",
    "start": "3054130",
    "end": "3060370"
  },
  {
    "text": "doing it it should be doing it only at the very beginning if you happen to",
    "start": "3060370",
    "end": "3066220"
  },
  {
    "text": "leave your computer running in training for several days as as folks have done the blue training button you press and",
    "start": "3066220",
    "end": "3073810"
  },
  {
    "text": "it trains based on the parameters you specify and the network state gets shipped to the main simulation from time",
    "start": "3073810",
    "end": "3080290"
  },
  {
    "text": "to time so the thing you see in the browser as you open up the web site is running then the same network that's",
    "start": "3080290",
    "end": "3086410"
  },
  {
    "text": "being trained and regularly it updates that network so it's getting better and better even if the training takes weeks",
    "start": "3086410",
    "end": "3092590"
  },
  {
    "text": "for you it's constantly updating the network you see on the left so if the car for the network that you're training",
    "start": "3092590",
    "end": "3099550"
  },
  {
    "text": "is just standing in place and not moving it's probably time to restart and change",
    "start": "3099550",
    "end": "3106450"
  },
  {
    "text": "the parameters maybe add a few layers to your network number of iterations is",
    "start": "3106450",
    "end": "3112210"
  },
  {
    "text": "certainly an important parameter to control and the evaluation is something",
    "start": "3112210",
    "end": "3118090"
  },
  {
    "text": "we've done a lot of worked on since last year to remove the degree of randomness to remove the the incentive to submit",
    "start": "3118090",
    "end": "3127270"
  },
  {
    "text": "the same code over and over again to hope to produce a higher reward a higher evaluation score the method for",
    "start": "3127270",
    "end": "3135790"
  },
  {
    "text": "evaluation is we collect the average speed over ten runs about 45 seconds of",
    "start": "3135790",
    "end": "3143410"
  },
  {
    "text": "game each not minutes 45 simulated seconds and there is five hundreds of",
    "start": "3143410",
    "end": "3150640"
  },
  {
    "text": "those and we take the median speed of the 500 runs it's done server-side so",
    "start": "3150640",
    "end": "3155680"
  },
  {
    "text": "extremely difficult to cheat I urge you to try you can try it locally there's a",
    "start": "3155680",
    "end": "3162100"
  },
  {
    "text": "start evaluation run but that one doesn't count that's just for you to feel better by you network that's that should",
    "start": "3162100",
    "end": "3169310"
  },
  {
    "text": "produce a result that's very similar to the one we were produced on the server it's to build your own intuition and as",
    "start": "3169310",
    "end": "3176810"
  },
  {
    "text": "I said we significantly reduce the influence of randomness so the the score the speed you get for the network you",
    "start": "3176810",
    "end": "3183770"
  },
  {
    "text": "design should be very similar with every valuation loading is saving if the",
    "start": "3183770",
    "end": "3190880"
  },
  {
    "text": "network is huge and you want to switch computers you can save the network it saves both the architecture of the",
    "start": "3190880",
    "end": "3196040"
  },
  {
    "text": "network and the weights and the on the network and you can load it back in",
    "start": "3196040",
    "end": "3202540"
  },
  {
    "text": "obviously when you load it in it's not saving any of the data you've already",
    "start": "3202540",
    "end": "3208760"
  },
  {
    "text": "done you can't do transfer learning with javascript in the browser yet submitting",
    "start": "3208760",
    "end": "3215060"
  },
  {
    "text": "your network submit model to competition and make sure you run training first otherwise it'll be initiated the way to",
    "start": "3215060",
    "end": "3222740"
  },
  {
    "text": "initiate it randomly and will not do so well you can resubmit us off and you like and the highest score is what counts the",
    "start": "3222740",
    "end": "3230210"
  },
  {
    "text": "coolest part is you can load your custom image specify colors and request the",
    "start": "3230210",
    "end": "3235310"
  },
  {
    "text": "visualization we have not yet shown the visualization but I promise you it's",
    "start": "3235310",
    "end": "3241460"
  },
  {
    "text": "going to be awesome again read the tutorial change the parameters in the code box click apply code run training",
    "start": "3241460",
    "end": "3248620"
  },
  {
    "text": "everybody in this room on the way home on the train hopefully not in your car",
    "start": "3248620",
    "end": "3253910"
  },
  {
    "text": "should be able to do this in the browser and then you can visualize request visualization because it's an expensive",
    "start": "3253910",
    "end": "3259490"
  },
  {
    "text": "process you have to want it for us to do it because we have to run in server-side",
    "start": "3259490",
    "end": "3267310"
  },
  {
    "text": "competition link is there github starter code is there and the details for those",
    "start": "3267760",
    "end": "3274190"
  },
  {
    "text": "that truly want to win is in the archive paper so the question that will come up",
    "start": "3274190",
    "end": "3279950"
  },
  {
    "start": "3275000",
    "end": "3475000"
  },
  {
    "text": "throughout is whether these reinforcement learning approaches are at all or rather if action planning control",
    "start": "3279950",
    "end": "3286940"
  },
  {
    "text": "is amenable to learning certainly in the case of driving we can't do it alpha go",
    "start": "3286940",
    "end": "3294500"
  },
  {
    "text": "zero did we can learn from scratch from self play",
    "start": "3294500",
    "end": "3299560"
  },
  {
    "text": "because that will result in millions of crashes in order to learn to avoid the",
    "start": "3299560",
    "end": "3307210"
  },
  {
    "text": "crashes unless we're working like we are deep crash on the RC car or we're",
    "start": "3307210",
    "end": "3312250"
  },
  {
    "text": "working in a simulation so we can look at export data we can look at driver data which we have a lot of and learn",
    "start": "3312250",
    "end": "3318490"
  },
  {
    "text": "from it's an open question whether this is applicable to date and I'll bring up",
    "start": "3318490",
    "end": "3324730"
  },
  {
    "text": "two companies because they're both guest speakers deep IRL is not involved in the",
    "start": "3324730",
    "end": "3330250"
  },
  {
    "text": "most successful robots operating in the real world in the case of Boston",
    "start": "3330250",
    "end": "3336700"
  },
  {
    "text": "Dynamics most of the perception control",
    "start": "3336700",
    "end": "3342280"
  },
  {
    "text": "and planning like in this robot does not involve learning approaches except with",
    "start": "3342280",
    "end": "3349000"
  },
  {
    "text": "minimal addition on the perception side best of our knowledge and certainly the",
    "start": "3349000",
    "end": "3356320"
  },
  {
    "text": "same is true with Wei MO as the speaker on Friday will talk about deep learning",
    "start": "3356320",
    "end": "3361360"
  },
  {
    "text": "is used a little bit in perception on top but most of the work is done from the sensors and the optimization base",
    "start": "3361360",
    "end": "3370000"
  },
  {
    "text": "the model-based approaches trajectory generation and optimizing which trajectory trajectory is best to avoid",
    "start": "3370000",
    "end": "3377440"
  },
  {
    "text": "collisions deep IRL is not involved and",
    "start": "3377440",
    "end": "3383100"
  },
  {
    "text": "coming back and back again the unexpected local POC is a high reward which arises in all of these",
    "start": "3383100",
    "end": "3390010"
  },
  {
    "text": "situations and apply in the real world so for the cat video that's pretty short",
    "start": "3390010",
    "end": "3395680"
  },
  {
    "text": "where the cats are ringing the bell and they're learning that the ring in the bell is is mapping to food I urge you to",
    "start": "3395680",
    "end": "3406060"
  },
  {
    "text": "think about how that can evolve over time in unexpected ways they may not",
    "start": "3406060",
    "end": "3411220"
  },
  {
    "text": "have a desirable effect where the final reward is in the form of food and the",
    "start": "3411220",
    "end": "3418060"
  },
  {
    "text": "intended effect is to ring the bell that's",
    "start": "3418060",
    "end": "3423810"
  },
  {
    "text": "ASAT comes in for the artificial general intelligence course in two weeks that something will explore extensively its",
    "start": "3423810",
    "end": "3431670"
  },
  {
    "text": "how these reinforcement learning planning algorithms will evolve in ways",
    "start": "3431670",
    "end": "3438930"
  },
  {
    "text": "they're not expected and how we can constrain them how we can design reward",
    "start": "3438930",
    "end": "3444840"
  },
  {
    "text": "functions that result in safe operation so I encourage you to come to the talk",
    "start": "3444840",
    "end": "3451590"
  },
  {
    "text": "on Friday at 1:00 p.m. as a reminder so 1:00 p.m. not 7:00 p.m. in Stata 32 one",
    "start": "3451590",
    "end": "3458400"
  },
  {
    "text": "two three and two the awesome talks in two weeks from Boston Dynamics to Ray",
    "start": "3458400",
    "end": "3463860"
  },
  {
    "text": "Kurzweil and so on for AGI now tomorrow we'll talk about computer vision and",
    "start": "3463860",
    "end": "3469830"
  },
  {
    "text": "psyche fuse thank you everybody [Applause]",
    "start": "3469830",
    "end": "3476589"
  }
]