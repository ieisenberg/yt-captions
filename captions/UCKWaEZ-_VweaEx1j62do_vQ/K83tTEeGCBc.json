[
  {
    "start": "0",
    "end": "89000"
  },
  {
    "text": "Hello and welcome to Mixture of Experts. I'm your host, Tim Huang. Each week we bring together a panel\nof researchers, product leaders,",
    "start": "6530",
    "end": "13030"
  },
  {
    "text": "engineers, policy experts, and more to\ndiscuss, debate, and distill down the week's biggest news and trends in AI.",
    "start": "13070",
    "end": "19449"
  },
  {
    "text": "So today on the show, three stories. First one, the state of the open source. Uh, what are the biggest trends\nin open source models and how will",
    "start": "19959",
    "end": "25880"
  },
  {
    "text": "they shape the business of AI? Second, the future of retrieval augmented\ngeneration, or RAG, Uh, they've come so",
    "start": "25880",
    "end": "32205"
  },
  {
    "text": "far, where are they going to go next? And then finally, Kolmogorov\nArnold Networks, or KAN.",
    "start": "32205",
    "end": "37935"
  },
  {
    "text": "What the hell are they? Why are all the nerds\nsuddenly talking about it? And should we buy the hype? So today on the show, I'm ably supported\nby an incredible panel of experts.",
    "start": "38165",
    "end": "46825"
  },
  {
    "text": "So, uh, first off, Marina Danilevsky,\na senior research scientist at IBM. Marina, thanks for joining.",
    "start": "47215",
    "end": "52574"
  },
  {
    "text": "And particularly, thanks to you for\njoining us so early Pacific time. David Cox, VP AI Models and\nDirector of the MIT IBM Watson Lab.",
    "start": "55300",
    "end": "63669"
  },
  {
    "text": "David, thanks for joining the show. Pleasure to be here. And returning for the second episode,\nwe were joking that we just make this",
    "start": "63699",
    "end": "71289"
  },
  {
    "text": "a kush varsity show going forward. Uh, he's unfortunately declined that,\nbut Krish Varshney, IBM fellow working",
    "start": "71290",
    "end": "77939"
  },
  {
    "text": "on issues surrounding AI governance. Krish, welcome back to the show. Yeah, it's great to be here. And, uh, yeah, I'm the Varshney\nwith the hair, um, so, yeah.",
    "start": "77940",
    "end": "84500"
  },
  {
    "text": "We'll use that as a little mnemonic, so.",
    "start": "87030",
    "end": "88680"
  },
  {
    "start": "89000",
    "end": "886000"
  },
  {
    "text": "Well, great. So let's start with the first\nstory that we want to cover today on Mixture of Experts. Um, so I think from where I'm\nsitting, uh, you know, there has",
    "start": "93615",
    "end": "102104"
  },
  {
    "text": "been just so much happening in\nthe world of open source, right? So Meta, of course, released\nLlama 3 a few weeks back.",
    "start": "102104",
    "end": "108604"
  },
  {
    "text": "Um, Apple, in a very, you know, big\nmove for them, I think, released the open ELM on device models.",
    "start": "108904",
    "end": "115704"
  },
  {
    "text": "And then IBM just recently released\nits Granite family of models. David, I kinda wanna give you\na chance to kind of first plug",
    "start": "116035",
    "end": "123220"
  },
  {
    "text": "Granite, tell us what it is and\nwhat you guys have been working on. Um, and then I kind of want you to kind\nof go into, you know, why it is that IBM",
    "start": "123220",
    "end": "130479"
  },
  {
    "text": "decided to release granite open source\nand why it thinks that doing this matters. And I think from there I think we\ncould talk more broadly about what's",
    "start": "130479",
    "end": "136569"
  },
  {
    "text": "happening in open source, but, uh,\nwanted to give you a shot to talk a little bit about the work that\nyou and the team have been doing. Sure. Yeah. Happy to.",
    "start": "136570",
    "end": "142860"
  },
  {
    "text": "Um, we actually had two major\nopen source announcements. This is a big week for us\nacross IBM and Red Hat.",
    "start": "143239",
    "end": "149153"
  },
  {
    "text": "Uh, the first, uh, uh, was that we open\nsourced the Granite code family of models. So these are models in a variety of sizes,\n3, 8, 20, and 34 billion parameters,",
    "start": "149545",
    "end": "160793"
  },
  {
    "text": "trained on 116 programming languages. These are, um, you know, state of the art\nmodels, competitive with, uh, You know,",
    "start": "161204",
    "end": "169015"
  },
  {
    "text": "the best in the field, and one of the\nareas that we really optimized for, for enterprise users, because ultimately IBM\nis interested in supporting enterprise,",
    "start": "169125",
    "end": "177484"
  },
  {
    "text": "is that all around capability, you know,\nnot just Python and code generation,",
    "start": "177855",
    "end": "183284"
  },
  {
    "text": "which is often the focus for the academic\ncommunity, but also Java and Rust and all kinds of other languages, and also\nthings like code fixing and explaining.",
    "start": "183285",
    "end": "193025"
  },
  {
    "text": "Um, So there's a lot of things you can\ndo with code models and it's really being integrated into the software development,\nyou know, fabric of how we do software",
    "start": "193445",
    "end": "201800"
  },
  {
    "text": "and software is integrated into the\nfabric of everything we do in society. And we wanted to release these because we,\nyou know, ultimately our position is that",
    "start": "201800",
    "end": "209490"
  },
  {
    "text": "that open wins, you know, like communities\nwill build around these models. People will build things, um, that,\nyou know, that we wouldn't expect.",
    "start": "209550",
    "end": "217114"
  },
  {
    "text": "They'll be able to extend the models,\nand that's, that's super powerful. And that leads a little bit to the\nsecond, um, announcement, which,",
    "start": "217115",
    "end": "224864"
  },
  {
    "text": "which happened through Red Hat. So we have a technology that we developed\nfor doing alignment of models, uh, we",
    "start": "224895",
    "end": "230213"
  },
  {
    "text": "call large scale alignment for chatbots. And that gave rise to a\nproject called InstructLab.",
    "start": "230214",
    "end": "235474"
  },
  {
    "text": "And what InstructLab is, is a\nway to actually add aggregate community contributions to the\ninstruction tuning of a model.",
    "start": "235885",
    "end": "244235"
  },
  {
    "text": "So now, uh, any developer anywhere\nin the world can submit new skills",
    "start": "244515",
    "end": "250064"
  },
  {
    "text": "and new knowledge to a model. And then that actually gets integrated. And then we do a weekly\nbuild of that model.",
    "start": "250135",
    "end": "257540"
  },
  {
    "text": "So it's a different cycle of development\nand different kind of community forming where we're not just forming\naround the model and, you know,",
    "start": "257550",
    "end": "265069"
  },
  {
    "text": "building inference tools and things\nlike that, but we're actually able to merge all those contributions and\nthen update the model every week.",
    "start": "265110",
    "end": "272820"
  },
  {
    "text": "So we're really excited about this. It's been a fantastic partnership. with Red Hat building this out. They, you know, open source\nbetter than, than anyone.",
    "start": "273440",
    "end": "280835"
  },
  {
    "text": "And, uh, we're really excited this got\nannounced, uh, at Red Hat Summit on, on Tuesday by Matt Hicks, the CEO of Red Hat.",
    "start": "280885",
    "end": "287615"
  },
  {
    "text": "Yeah, that's awesome. And I think, I don't know if you'd agree\nwith this, it's like, I see what IBM has done with Granite and with InstructLab,\nand it's kind of like, You know, I was",
    "start": "287934",
    "end": "296090"
  },
  {
    "text": "joking with a friend the other day. I was like, it's open source,\nputting its big boy pants on, right? , like, they were kind of like moving\ninto like open source being something",
    "start": "296090",
    "end": "303140"
  },
  {
    "text": "that like enterprises will actually use. Um, and I think that's really changing\nwhat we mean by open source, right?",
    "start": "303140",
    "end": "310370"
  },
  {
    "text": "Like, I think like the big trends\neven a few months ago was like, oh my God, these open source models\nare just getting so big, right?",
    "start": "310400",
    "end": "316100"
  },
  {
    "text": "Like they're huge parameter models\nand like, isn't that the exciting thing is that open source models\nwill be, you know, on par as",
    "start": "316100",
    "end": "322100"
  },
  {
    "text": "sophisticated as like state of the art. But what's kind of interesting\nhere, I think, is really,",
    "start": "322100",
    "end": "327010"
  },
  {
    "text": "sort of like, twofold, right? Like, I think, like, what's interesting\nwith Granite is you guys are releasing a class of models of different sizes.",
    "start": "327250",
    "end": "333349"
  },
  {
    "text": "Sort of on the idea that, like, not\neverybody's going to need, like, the chunkiest model in the whole world.",
    "start": "333729",
    "end": "338599"
  },
  {
    "text": "Uh, which I think is\nreally, really interesting. Um, and then I think, again, it's\nalso kind of like on par with what",
    "start": "338989",
    "end": "344400"
  },
  {
    "text": "we see out of the Apple announcement,\nthe OpenELM announcement, right? Which is like, these are not\nthe biggest models, but they're on device models, right?",
    "start": "344400",
    "end": "350959"
  },
  {
    "text": "And it kind of feels like, I don't know if\nyou'd agree with this, David, it's like, it feels like open source is finally now\nkind of like responding to market need.",
    "start": "350989",
    "end": "357339"
  },
  {
    "text": "Like, in some ways, like enterprises are\nlike, how do we actually apply this stuff? And now, like, essentially the open\nsource community is like trying to now,",
    "start": "357660",
    "end": "363009"
  },
  {
    "text": "you know, adapt to actually provide\nsolutions to that, but I don't know if that's like a characterization\nyou guys would agree with. Yeah, yeah, no, and I think, I\nthink you're spot on, you know,",
    "start": "363170",
    "end": "371240"
  },
  {
    "text": "there isn't just one thing that\npeople want to do with LLMs. So there's not going to be\njust one LLM that wins today.",
    "start": "371249",
    "end": "377090"
  },
  {
    "text": "Um, you know, we have our models\nrunning on laptops and there are, that's, they're really interesting,\nuh, you know, advantages to doing that.",
    "start": "377640",
    "end": "387069"
  },
  {
    "text": "Like if you, if you want to be\non prem, you don't want to be sending data over the network. It's, it's, uh, you know, there's\nYou know, it's proprietary,",
    "start": "387100",
    "end": "394180"
  },
  {
    "text": "you're worried about IP. You can run these models, in\nmany cases, on your laptop. For other applications, you\nwant the very best performance.",
    "start": "394190",
    "end": "401460"
  },
  {
    "text": "You know, say you're doing an application\nmodernization that's just going to happen once, um, and it needs to\nbe, you know, the highest quality.",
    "start": "401500",
    "end": "407510"
  },
  {
    "text": "Then you can move to one\nof the larger models. Um, so we, we really are trying\nto be responsive across the",
    "start": "407510",
    "end": "412615"
  },
  {
    "text": "spectrum of, of different needs. And yeah, at IBM, we're, we are trying\nto be sort of, uh, I think you said the,",
    "start": "412625",
    "end": "419955"
  },
  {
    "text": "the big boy pants, you know, like we're\nvery transparent about what data we put in the models, which is, which is not\nalways true, but which is very important",
    "start": "419964",
    "end": "427375"
  },
  {
    "text": "if you're an enterprise one to use these. The other point of differentiation\nfor our models is we released",
    "start": "427435",
    "end": "432695"
  },
  {
    "text": "them under Apache 2 license. Just a clean Apache 2, no\nadditional restrictions.",
    "start": "432695",
    "end": "438715"
  },
  {
    "text": "And this can be really\nimportant for adoption. We knew this was something that our\ncustomers would ultimately need and want.",
    "start": "438994",
    "end": "444715"
  },
  {
    "text": "So that's how we're evolving\nour approach to open source.",
    "start": "445195",
    "end": "451635"
  },
  {
    "text": "And again, like you said,\nmeeting the customer's needs.",
    "start": "452155",
    "end": "454835"
  },
  {
    "text": "You know, I think we can use this\nas springboard to kind of talk about like how this is gonna sort\nof shape the, the market as a whole. Right. Because I think, you know, if I'm now, you\nknow, like an enterprise sort of thinking",
    "start": "459975",
    "end": "469065"
  },
  {
    "text": "about how to integrate LLMs, right? It feels like there's\nincreasingly options, right? Well, we can, you know, go work with.",
    "start": "469065",
    "end": "475245"
  },
  {
    "text": "We can try to do it all\nourselves in house, right? Like, we can try to go with, like,\nthe big proprietary models, right?",
    "start": "475520",
    "end": "481690"
  },
  {
    "text": "Um, and it kind of also feels like\nthere's going to be a range of new businesses that emerge here as well. Like, just like the whole business of,\nlike, you come to us with a problem",
    "start": "482110",
    "end": "490109"
  },
  {
    "text": "and we fine tune open source models\nfor you, seems like it'll increasingly become a big part of the ecosystem.",
    "start": "490110",
    "end": "495310"
  },
  {
    "text": "But um, yeah, I'm kind of curious as\nfrom, from your point of view, kind of like in, you know, the kind of research\nspace and even thinking about like,",
    "start": "495320",
    "end": "502710"
  },
  {
    "text": "you know, where this all goes, just if\nyou've got views on, on how this will kind of impact the ecosystem as a whole.",
    "start": "502799",
    "end": "507919"
  },
  {
    "text": "Yeah, I mean, uh, one great thing\nthat, uh, I mean, InstructLab enables is really, I mean, shifting\npower to, uh, value creators.",
    "start": "509205",
    "end": "516784"
  },
  {
    "text": "So, um, it, uh, really allows,\nuh, I mean, as David said, I mean, this whole community to, uh, to\nreally, uh, congeal around this.",
    "start": "516794",
    "end": "524224"
  },
  {
    "text": "thing and make these models\nauthentic for themselves. It's some sort of commitment\nto locality as well.",
    "start": "524365",
    "end": "530814"
  },
  {
    "text": "I mean, for whatever you need for your\nenterprise, for your organization, you can really make things, make things yours.",
    "start": "530865",
    "end": "538235"
  },
  {
    "text": "So I think it's, uh, it's\nan awesome, awesome thing. Yeah, I really appreciate being\nable to add the skills as you find",
    "start": "538245",
    "end": "545884"
  },
  {
    "text": "them needed for your own use case. So the thing with all of these\nmodels is that it's very hard to predict when you put them out.",
    "start": "545885",
    "end": "551145"
  },
  {
    "text": "What are they actually\ngoing to be used for? So being able to have the flexibility to\nsay, Oh, I've realized I have a use case.",
    "start": "551425",
    "end": "557764"
  },
  {
    "text": "I need to adapt quickly. I need to make the model adapt quickly. Sometimes it's something that's\nproprietary or somewhere else.",
    "start": "557795",
    "end": "562975"
  },
  {
    "text": "You just don't have the ability to move\nthat quickly or even to stress test or check and was this going somewhere or\nis this not going to be helpful at all?",
    "start": "563004",
    "end": "569625"
  },
  {
    "text": "So from that perspective, actually,\nthe way that, uh, we've released a stress lab is very, is very good.",
    "start": "569904",
    "end": "575165"
  },
  {
    "text": "It's very effective. It's very rare that, um, any\ngiven company or enterprise's",
    "start": "575165",
    "end": "582675"
  },
  {
    "text": "needs would be represented. It would be a top of mind\nfor the developer of any",
    "start": "582704",
    "end": "587854"
  },
  {
    "text": "given base foundation model. Like, does Meta, you know, care\nabout an insurance company? Well, they do.",
    "start": "587854",
    "end": "593249"
  },
  {
    "text": "You know, they probably do, but\nit's not their primary concern. It's not the main thing\nthey're thinking about.",
    "start": "593250",
    "end": "603308"
  },
  {
    "text": "So having a base that's built for\nenterprise, but then giving the ability to customize and really focus and bring\nin, you know, knowledge and particular",
    "start": "603309",
    "end": "613150"
  },
  {
    "text": "things you want to do that are specific\nto that industry can be really powerful. Can I, so we have a few more\nminutes on this topic, can I",
    "start": "613180",
    "end": "619444"
  },
  {
    "text": "play jerk for a second, right? Because I do think that like, you know,\none of the most interesting things about open source is that early on,\nyou know, if you were, if you were",
    "start": "619444",
    "end": "627635"
  },
  {
    "text": "a government writer, someone worried\nabout AI ethics or AI safety, right? You'd basically say.",
    "start": "627635",
    "end": "632725"
  },
  {
    "text": "Well, the rise of these few leading\ncompanies with proprietary models is like really good for us, right?",
    "start": "633090",
    "end": "638190"
  },
  {
    "text": "Because we only have to go to a\nfew companies and change their policies in order to sort of\nsecure the ecosystem, right?",
    "start": "638470",
    "end": "643899"
  },
  {
    "text": "And I think you might say,\nwell, one of the issues of these increasing proliferation\nof open source models, right?",
    "start": "644430",
    "end": "650169"
  },
  {
    "text": "And the fact that everybody's\nkind of going to be running their models on premises, right? Is that there's a lot more\nroom for people to misuse?",
    "start": "650169",
    "end": "657339"
  },
  {
    "text": "These models, um, and also like you might\nthink that also they create all of these supply chain security issues as well.",
    "start": "657574",
    "end": "663385"
  },
  {
    "text": "Like I'm kind of thinking\nabout how like, uh, NPM, right? Like other instances in which\nopen source is really taken off.",
    "start": "663395",
    "end": "668665"
  },
  {
    "text": "Um, you know, security ends up being\nthis really big problem because like the provenance of any particular\ncomponent is really difficult and",
    "start": "669025",
    "end": "674954"
  },
  {
    "text": "your stack might rely on, you know,\nhundreds of open source components. And I guess I'm kind of curious. I mean, I don't think anyone's got a\ngood solution to this and, and look, I",
    "start": "674954",
    "end": "682500"
  },
  {
    "text": "came up as like a free software advocate. So like, I, I'm on, I'm on the\nside of what's going on here.",
    "start": "682500",
    "end": "687410"
  },
  {
    "text": "But I'd love to kind of panelists\nto sort of like, you know, offer an opinion about that. Like, do you buy that those are risks?",
    "start": "687750",
    "end": "692890"
  },
  {
    "text": "I don't know if there's kind of smart\nsolutions you guys are thinking about. Just to kind of wrestle with that a\nlittle bit, I think it's one of the most interesting parts of this development.",
    "start": "693339",
    "end": "699350"
  },
  {
    "text": "Yeah, one thing just to start off,\num, on the security issue, um, history has proven in open source\nsoftware that open source ultimately",
    "start": "699520",
    "end": "708509"
  },
  {
    "text": "ends up being safer, not less safe. There were efforts, for instance, to\ncreate, you know, private versions of",
    "start": "708509",
    "end": "714279"
  },
  {
    "text": "the Linux kernel, and it turns out it's\njust hard to keep those safe because more eyes mean, uh, you know, more,\nuh, more sort of, you know, uh, people",
    "start": "714309",
    "end": "722329"
  },
  {
    "text": "who can find, uh, you know, problems,\nunderstand problems, and, and fix them. Um, so I think having that transparency,\nenabling the academic community to",
    "start": "722329",
    "end": "730668"
  },
  {
    "text": "get involved, to build solutions,\nuh, for many problems that we may face, I think is super important.",
    "start": "730670",
    "end": "735880"
  },
  {
    "text": "I, I will also say we're very careful\nabout what we, um, what we release. I mean, we're, we're, we're very careful\nabout what data goes into these models,",
    "start": "736160",
    "end": "743889"
  },
  {
    "text": "uh, before we release them, ensuring\nthat they're, you know, minimizing the risks, uh, any potential risks around,\nyou know, you know, potentially dangerous,",
    "start": "744300",
    "end": "753524"
  },
  {
    "text": "uh, you know, activities where we're\nnot releasing models that we think are, could be used for, for, for ill intent.",
    "start": "753555",
    "end": "758824"
  },
  {
    "text": "Of course not. Yeah. And I think, uh, I mean, I do think that\nthere's going to be need almost for like a consumer reports or a wire cutter.",
    "start": "759194",
    "end": "764784"
  },
  {
    "text": "For these models at some point. Mm-Hmm. where it's basically like, there's gonna\nbe so many models out there that it's gonna literally be like, well we had a\ncouple experts spend like a few hours",
    "start": "765150",
    "end": "771839"
  },
  {
    "text": "really testing this thing, you know? And this is like an important part\nof the, the ecosystem Kush, it looks like you might wanna get in.",
    "start": "771840",
    "end": "777210"
  },
  {
    "text": "Yeah, I mean, uh, we actually\ndo work on exactly that, the consumer reports sort of idea.",
    "start": "777570",
    "end": "783270"
  },
  {
    "text": "So we call it, uh, AI fact sheets,\num, and, uh, model risk assessment. And, uh, it is, uh, exactly\na way to, uh, to analyze.",
    "start": "783275",
    "end": "791685"
  },
  {
    "text": "Uh, these different models that are\nout there, um, give them different scores along different dimensions. Um, and as a consumer, um, you can,\nI mean, really look at different",
    "start": "792170",
    "end": "801180"
  },
  {
    "text": "vendors, different, uh, sort of\noptions and, uh, get a good sense of, uh, of what's available.",
    "start": "801180",
    "end": "806399"
  },
  {
    "text": "So this is actually, um, something already\navailable through, uh, through Watson X. gov and it's one of our,\nuh, flagship products.",
    "start": "806400",
    "end": "812750"
  },
  {
    "text": "Yeah, I imagine it's come about some kind\nof future when I quit my job as a podcast host to be, uh, like a model sommelier.",
    "start": "813040",
    "end": "818449"
  },
  {
    "text": "Yeah, it's just like, have you considered\nlike this, this model for your use case? Yeah, fine vintage. That's awesome.",
    "start": "818520",
    "end": "824290"
  },
  {
    "text": "Yeah, fine vintage. Yeah, exactly right. Yeah, really good oaky overtones on this. 2024 was a good year for LLMs, yeah.",
    "start": "824380",
    "end": "830849"
  },
  {
    "text": "Yeah, exactly. Um, Marina, any final thoughts before\nwe move to the next topic here? Yeah, I would say that it's, uh,\nstill very early days also with this",
    "start": "831129",
    "end": "839079"
  },
  {
    "text": "technology and everything that we're\ngoing into, especially as scientists, we would like to try not to have the\nhubris of thinking, yeah, we've got",
    "start": "839080",
    "end": "845570"
  },
  {
    "text": "this, you know, leave it with us. We've, we've sorted out the rest of this. There's been so many interesting\ndevelopments and surprises in this",
    "start": "845570",
    "end": "852199"
  },
  {
    "text": "technology in the last few years. And we'll, we think that it\nwill continue to be for sure. In that sense, open source is actually\ngoing to be more efficient, even",
    "start": "852200",
    "end": "859330"
  },
  {
    "text": "From a market standpoint, more eyes\nmeans more ideas means more places that this is going to develop in\nunexpected and interesting ways.",
    "start": "859640",
    "end": "866540"
  },
  {
    "text": "So it's actually even, I think, more\nefficient besides whatever thoughts we may have about the morality of it as well.",
    "start": "866550",
    "end": "872000"
  },
  {
    "text": "Yeah, no, for sure. And again, I'm kind of arguing\nagainst myself because like, I'm Very pro open source. Um, I think it's just like a very\ninteresting kind of set of considerations",
    "start": "872400",
    "end": "879519"
  },
  {
    "text": "as like the whole architecture of the\nindustry sort of shapes, uh, and changes.",
    "start": "879520",
    "end": "883770"
  },
  {
    "start": "886000",
    "end": "1844000"
  },
  {
    "text": "Well, this is great. So let's move to the second topic today. I really want to talk about retrieval\naugmented generation or RAG.",
    "start": "888850",
    "end": "894799"
  },
  {
    "text": "Um, so if you're not familiar with\nthis, RAG is a Uh, basically one of",
    "start": "894849",
    "end": "899884"
  },
  {
    "text": "the hotnesses, uh, in, in, in AI. Um, if you look at the papers at\niClear this year or ACL, um, there",
    "start": "899894",
    "end": "906664"
  },
  {
    "text": "are a lot of papers using RAG\nmethods, um, and, you know, I guess Marina, I should keep me honest here.",
    "start": "906664",
    "end": "912974"
  },
  {
    "text": "I mean, I think one of the reasons\nthat it has been So prolific and of so much interest is that RAG seems to\nkind of open a window for solving a",
    "start": "912974",
    "end": "921620"
  },
  {
    "text": "lot of the models, the problems that\nwe have with language models, right? Like, well, we can't train these\nmodels, pre train these models all",
    "start": "921620",
    "end": "927709"
  },
  {
    "text": "the time, but if they're really\ngood at pulling data from elsewhere, um, you know, this is a good way of\nkeeping their responses up to date.",
    "start": "927710",
    "end": "933940"
  },
  {
    "text": "Um, it's a good way of ensuring\nthat they're, you know, more factual potentially. Um, and, um, and so I, I'm\ncurious because I know your",
    "start": "934240",
    "end": "940470"
  },
  {
    "text": "group recently released a paper. Um, thinking about and using RAG and\nso maybe as a springboard for the conversation on if you want to quickly\ntalk about that and then we can kind of",
    "start": "940470",
    "end": "947785"
  },
  {
    "text": "more generally talk about, you know, I\nguess from your point of view, what you see as sort of the existing limitations\nof RAG and what are the big technical",
    "start": "947795",
    "end": "954205"
  },
  {
    "text": "problems that need to be solved? Sure. That sounds great. So, um, the paper that you're\nreferring to, it's a description",
    "start": "954205",
    "end": "961094"
  },
  {
    "text": "of a methodology in a system for\ntrying to evaluate it more deeply. Again, the point of RAG is it's\none thing to be able to have a.",
    "start": "961115",
    "end": "968285"
  },
  {
    "text": "conversation with an LLM in which you\nask it to write a haiku about frogs. They're great at that, no problem.",
    "start": "968585",
    "end": "973725"
  },
  {
    "text": "We live by business use cases and\nso it's very important that when you have business use cases that rely on\nfactual information and it's really",
    "start": "974125",
    "end": "982445"
  },
  {
    "text": "a problem if you get things wrong,\nthis is where you get into rag. Like you said, Being able to point to a\nreference of, all right, the reason I'm",
    "start": "982455",
    "end": "988105"
  },
  {
    "text": "giving you this answer is because this\nis the content that I am relying on, whether it's informational or it comes\nfrom a knowledge base, whatever, then",
    "start": "988125",
    "end": "995134"
  },
  {
    "text": "you want to actually go and double check. Is this going to act the\nway that I expect it to act? And it's one thing again to test these\nLLM models against large benchmarks.",
    "start": "995135",
    "end": "1004694"
  },
  {
    "text": "There were some good comments last week\nabout benchmarks and the use, you know, usefulness of them as time goes on.",
    "start": "1004830",
    "end": "1009700"
  },
  {
    "text": "It's another thing to actually see\nwhat happens in a customer's use case. This is an old data analysis necessity.",
    "start": "1010040",
    "end": "1016600"
  },
  {
    "text": "You have to go into, okay, what\nwent into the test cases that you've created your testing?",
    "start": "1016600",
    "end": "1021699"
  },
  {
    "text": "Where did your data come from? What are the documents? How have you managed to without\nknowing it introduced biases into the",
    "start": "1021699",
    "end": "1027530"
  },
  {
    "text": "evaluation that you're doing because\nof the way your annotations are done? Because of the way you've\ndefined your metrics.",
    "start": "1027530",
    "end": "1032764"
  },
  {
    "text": "Because people have different\nunderstandings of what is acceptable, what is not. You have over, uh, corrected\nfor a particular query type.",
    "start": "1033005",
    "end": "1040394"
  },
  {
    "text": "You have overcorrected for a\nparticular way of responding. This is all, uh, analysis that you need\nto do to have confidence in the solution",
    "start": "1040395",
    "end": "1048945"
  },
  {
    "text": "that you put out that includes an LLM,\nbut is not just the LLM by itself. It's the LLM as a part of a solution.",
    "start": "1048945",
    "end": "1054654"
  },
  {
    "text": "And so that's something\nthat my group does a lot. I know Krishna's group does that a lot\nas well, is diving into the details",
    "start": "1054985",
    "end": "1061285"
  },
  {
    "text": "of that, especially how we take our\ncustomers through getting confidence and what does it mean to deploy their LLM.",
    "start": "1061285",
    "end": "1066904"
  },
  {
    "text": "And our system has a fun way For those of\nus from the 90s, we called InspectoRAGet,",
    "start": "1066915",
    "end": "1072360"
  },
  {
    "text": "Inspector Gadget, um, and it, it really\nis a way to, to make sure that you can",
    "start": "1072379",
    "end": "1077980"
  },
  {
    "text": "take yourself through that analysis\nand, and feel confidence in what you're getting, not just the aggregate number. It's funny about the 90s.",
    "start": "1077980",
    "end": "1083730"
  },
  {
    "text": "I was in a class that a friend\nwas teaching today, or earlier this week, and one of the kids\nwas like, I hear back in the day,",
    "start": "1083780",
    "end": "1089180"
  },
  {
    "text": "there's this thing called GeoCities. I hear it was really cool\nor something like that. I was like, Oh my God, I\ngotta, I gotta get out of here.",
    "start": "1089180",
    "end": "1094900"
  },
  {
    "text": "Um, Yeah, so there's, there's,\nthere's so much to go into there. And I think there's kind of like\nmaybe two topics we could dive into.",
    "start": "1095890",
    "end": "1102409"
  },
  {
    "text": "You know, I think the first summary\nand I'd love to get your thoughts on is, I think one really great theme,\nI think that came up from last week's",
    "start": "1102520",
    "end": "1108700"
  },
  {
    "text": "episode was kind of the idea that almost\nAI is in this kind of weird period of like benchmark bankruptcy, or like\nessentially there's like all of these",
    "start": "1108700",
    "end": "1116520"
  },
  {
    "text": "benchmarks that no one cares about. And then the benchmarks that do\npeople do care about are like so thin. Thoroughly gamed that they basically\nprovide no valid information anymore.",
    "start": "1116800",
    "end": "1125500"
  },
  {
    "text": "And like one outcome that I think\nShobi was saying on, on the, uh, on the episode was like, well, that's one\nof the reasons why like the solution",
    "start": "1125850",
    "end": "1132479"
  },
  {
    "text": "now is like, just talk to the model\nfor 15 minutes and then you figure out whether or not it's good or not. And it strikes me that like.",
    "start": "1132479",
    "end": "1137820"
  },
  {
    "text": "I don't know if you'd put Inspector\nRaggett in kind of this context, it's like, it seems like there's\nalso a switch from like, from",
    "start": "1138870",
    "end": "1144409"
  },
  {
    "text": "benchmarks to like monitoring as the\nway that we really assess whether or not models are high quality.",
    "start": "1144419",
    "end": "1149989"
  },
  {
    "text": "I don't know if you'd buy that because\nI, as I take kind of your group's work is an attempt to say, okay,\nwell, we're not going to really.",
    "start": "1150049",
    "end": "1155860"
  },
  {
    "text": "You know, benchmarks are a useful guide,\nbut really in practice, what most people want is to see, like, lots and lots of\ntelemetry about their models, and like,",
    "start": "1156150",
    "end": "1164120"
  },
  {
    "text": "that's how we approach this problem. Um, but kind of curious to\nget your response on that. Like, do you buy the idea that\nAI is in a benchmark bankruptcy?",
    "start": "1164130",
    "end": "1170230"
  },
  {
    "text": "And do you see kind of RAG IT as\nsort of a solution to that or, uh, an answer to that in some ways? Yeah, I think you should think of\nbenchmarks as something that you",
    "start": "1170510",
    "end": "1177340"
  },
  {
    "text": "should iterate on rapidly and evolve. Now, the problem with talk to the model\nfor 15 minutes and just get it vibes.",
    "start": "1177340",
    "end": "1183980"
  },
  {
    "text": "Uh, the feel of it is, uh, people\nare not very good at coming up",
    "start": "1184590",
    "end": "1189860"
  },
  {
    "text": "with what is the right thing to\ntalk about it to for 15 minutes consistently, they are not very good. They themselves will only think of\nwhatever came into their head, whatever",
    "start": "1189860",
    "end": "1198409"
  },
  {
    "text": "they were talking about to their\ncustomer last week, and they will introduce, like I said, a really a lot\nof biases and what they thought of.",
    "start": "1198409",
    "end": "1203810"
  },
  {
    "text": "Then you end up being very nastily\nsurprised when you actually go ahead and deploy your model. And they're like, well, that.",
    "start": "1204139",
    "end": "1209170"
  },
  {
    "text": "Didn't work, but I talked\nto it for 15 minutes. It seemed fine. You wouldn't also, um, you know,\ndeploy, uh, a representative to a",
    "start": "1209395",
    "end": "1216564"
  },
  {
    "text": "customer after talking to them for 15\nminutes and thinking that seems fine. So, realistically, what you actually\nwant and what I hope the point is",
    "start": "1216564",
    "end": "1223664"
  },
  {
    "text": "of approaches like Inspector Agat\nis constant evolving benchmarks. Yeah, talk to it for 15 minutes\nand then go check yourself.",
    "start": "1223664",
    "end": "1230184"
  },
  {
    "text": "Hey, what data did you end\nup actually putting in? What kind of questions\ndid you end up putting in? Do you realize that you didn't\ndo the right vibe check?",
    "start": "1230394",
    "end": "1236623"
  },
  {
    "text": "Do that a few times, your vibe\ncheck becomes then into something systematic, but it's something\nthat is, that is iterative, that",
    "start": "1236795",
    "end": "1242774"
  },
  {
    "text": "is interactive, rather than some\nacademic somewhere put out a benchmark. I don't know what this\nhas to do with my data.",
    "start": "1242774",
    "end": "1247925"
  },
  {
    "text": "Make your own, make it iterative,\nand constantly, you know, check yourself for what you're doing\nis actually proper quality.",
    "start": "1248295",
    "end": "1255124"
  },
  {
    "text": "That ends up being really\nthe right thing to do. So move yourself from that, um, You\nknow, shout out to Daniel Kahneman from that system one thinking\nto the system two thinking, then",
    "start": "1255435",
    "end": "1263774"
  },
  {
    "text": "you're going to have confidence\nin what you're actually deploying. Yeah, this is uh, I think it's so\ninteresting because I think this is",
    "start": "1263774",
    "end": "1268849"
  },
  {
    "text": "What you're describing is going to be an\nenormous need across like every company that attempts to adopt this stuff.",
    "start": "1268850",
    "end": "1274660"
  },
  {
    "text": "And you know, I was joking earlier about\nbeing a model sommelier, like I think my other business proposal is like, oh,\nyou're an eval atelier, where you're",
    "start": "1274660",
    "end": "1282398"
  },
  {
    "text": "basically like, we help to craft Finally\ncrafted evals for like what you need and",
    "start": "1282399",
    "end": "1287870"
  },
  {
    "text": "it kind of what you're talking about. 'cause like the art of creating a good\nbenchmark and evolving that benchmark, I think they're describing a lot of\nour jobs actually here at IBM is what,",
    "start": "1287870",
    "end": "1293835"
  },
  {
    "text": "that's literally what we're doing. for our customer eval. Yeah. So, um, David Kush, I dunno if you've\ngot responses you wanna jump in.",
    "start": "1294095",
    "end": "1300919"
  },
  {
    "text": "Um, maybe I can be a little bit\ncontroversial, um, so, uh, yeah, um,",
    "start": "1301645",
    "end": "1306635"
  },
  {
    "text": "I mean, people talk about RAG being a\nsolution for hallucination, for lack of factuality, lack of faithfulness, lack\nof groundedness, these sort of things,",
    "start": "1306825",
    "end": "1314615"
  },
  {
    "text": "but, um, to me, I mean, it's part of the\nsolution, but, uh, I don't think it's",
    "start": "1314655",
    "end": "1319674"
  },
  {
    "text": "the full solution, because even when you\nget the retrieved documents, There's a model in between and it can ignore those\ndocuments, it can get confused by them,",
    "start": "1319674",
    "end": "1328475"
  },
  {
    "text": "it can, uh, I mean, just hallucinate\nanyways, I mean, all sorts of things. So, um, uh, to me, I mean, what Marina is\ntalking about is very important, not just,",
    "start": "1328485",
    "end": "1340414"
  },
  {
    "text": "I mean, over time, but, uh, as part of\nthe, the, the process initially as well,",
    "start": "1340485",
    "end": "1346585"
  },
  {
    "text": "or in runtime and, uh, in inference time. So. I mean, checking for hallucinations\nseparately, um, uh, thinking about,",
    "start": "1346585",
    "end": "1354005"
  },
  {
    "text": "can we trace back the information? Where did it come from in those documents? Uh, can we even come up with new\narchitectures that, uh, uh, don't",
    "start": "1354005",
    "end": "1362945"
  },
  {
    "text": "hallucinate, uh, by design in some ways? So I think there's, RAG gets\na lot of play right now, but",
    "start": "1362945",
    "end": "1369455"
  },
  {
    "text": "I think it's a stepping stone. Um, I don't think it's the,\nthe end of the journey. I actually completely agree\nwith you, fully agree with you.",
    "start": "1369855",
    "end": "1376924"
  },
  {
    "text": "Rag has not fixed it. It's just an additional\nstep in the direction. I completely agree with you.",
    "start": "1376995",
    "end": "1382185"
  },
  {
    "text": "Very interesting. So do you think in like, I don't\nknow, it's always tough to predict on these things, like in two\nyears, we'll be talking about RAG?",
    "start": "1382555",
    "end": "1387975"
  },
  {
    "text": "Uh, I think we will because, um, RAG\nis like, I mean, it's search, right? And, uh, a lot of the companies who\nare, uh, in the cell LM game are, uh,",
    "start": "1388030",
    "end": "1396610"
  },
  {
    "text": "search companies at the end of the day. So, um, I think it'll stick around. Uh, it'll, uh, have,\nuh, a lot to, to, to do.",
    "start": "1396610",
    "end": "1403389"
  },
  {
    "text": "But, uh, yeah, I mean, I think for\nenterprise use cases, um, maybe it'll, uh, get a little bit less, uh, emphasis.",
    "start": "1403389",
    "end": "1410550"
  },
  {
    "text": "Maybe not. I, I don't know. Well, I mean, for freshness of data,\nsome kind of retrieval can be helpful.",
    "start": "1410550",
    "end": "1416033"
  },
  {
    "text": "Like, you just added it to the database,\nyou can retrieve it immediately. So, there are more than one problem\nthat RAG solves, and I agree with,",
    "start": "1416034",
    "end": "1423334"
  },
  {
    "text": "uh, with Christian and Marina that\nhallucination, that's a, that's a Except it, you know, it helps a little bit,\nlike it's a separate problem that we",
    "start": "1423544",
    "end": "1430875"
  },
  {
    "text": "need to address lots of different ways. Um, but, but the ability to access new\ninformation, the ability to customize",
    "start": "1431065",
    "end": "1438914"
  },
  {
    "text": "quickly, I mean, we're starting to\nget a, I think, layers of technology that allow us to, to address that. And StructLab is one of them.",
    "start": "1438915",
    "end": "1444745"
  },
  {
    "text": "If you wanted to ingest knowledge into\nthe LLM and build it into your sort of, your, into the LLM itself, you can, do\nthat, but you probably still want to",
    "start": "1444745",
    "end": "1453245"
  },
  {
    "text": "be retrieving things and there's going\nto be a balance and we're going to figure that balance out, I think, over\nthe next, uh, next couple of years.",
    "start": "1453414",
    "end": "1459475"
  },
  {
    "text": "Yeah. Yeah. I think it's definitely like\nthe much more, realistic pathway that I've heard, right? Like, I think like the other alternatives\nI've heard are like, well, at some point",
    "start": "1460014",
    "end": "1468465"
  },
  {
    "text": "the model will become so big and know\neverything, and then we'll be able to pre train it frequently enough, right?",
    "start": "1468465",
    "end": "1473475"
  },
  {
    "text": "I'm just like, really? How many H100s are you going\nto buy to pull this off? You know, it's just like, it's not\nwithin the realm of possibility.",
    "start": "1473775",
    "end": "1480674"
  },
  {
    "text": "I was just going to say, and not\nevery company is going to give their data over to OpenAI to let them,\nyou know, their proprietary data.",
    "start": "1483065",
    "end": "1490545"
  },
  {
    "text": "It's a real problem. And all I was going to say is, I mean,\nlike these sort of ideas, like having multiple levels and layers, I mean,\nit's part of computer engineering.",
    "start": "1490645",
    "end": "1497965"
  },
  {
    "text": "I mean, caching, different\ntypes of locality. I mean, this is all like very much\nthe sort of thinking that, uh,",
    "start": "1497965",
    "end": "1505315"
  },
  {
    "text": "Computer people have had, so it\njust needs to come into this too. Yeah, orchestration, right?",
    "start": "1505875",
    "end": "1511585"
  },
  {
    "text": "And making sure that there's\nrouting involved, there's decisions that involves, there's different\nchecks, there's different guards.",
    "start": "1511625",
    "end": "1516955"
  },
  {
    "text": "That's not going to go away. I don't care how long you've trained the\num, um, that's not going to be fixed.",
    "start": "1517420",
    "end": "1522843"
  },
  {
    "text": "Yeah, for sure. So we have a few minutes\nleft on this topic. I think the last area that I want to\nkind of push us into is I think Marina,",
    "start": "1522843",
    "end": "1529920"
  },
  {
    "text": "you had kind of a really sort of\ninteresting comment when you're explaining Inspector Ragget, which is basically,\nThis kind of feature of trust, right?",
    "start": "1529930",
    "end": "1537730"
  },
  {
    "text": "Essentially, like, what does a user\nneed to be shown to trust the model? Um, and I think what I love about that\ntopic is that in some ways it's, it",
    "start": "1537760",
    "end": "1545980"
  },
  {
    "text": "pushes you into the realm away from\nlike, like, it turns out people trust models regardless whether or not they're\na huge parameter model or a tiny model.",
    "start": "1546309",
    "end": "1552999"
  },
  {
    "text": "And like, You know, I heard this great\nanecdote where, um, this MLE uh, was telling me this story where they're like,\nwe were doing an eval where there's these",
    "start": "1553010",
    "end": "1560034"
  },
  {
    "text": "side-by-sides, and what we discovered\nis that the users that we were testing against just felt that longer outputs\nwere more credible and trustworthy",
    "start": "1560035",
    "end": "1568135"
  },
  {
    "text": "regardless of any content they included. Right. And I was like, oh, that makes sense,\nbecause like you're saying, like do 500 of these tasks in the next hour.",
    "start": "1568285",
    "end": "1575184"
  },
  {
    "text": "And so they're just using these\nvisual heuristics to evaluate text. And one visual heuristic that we\nused to tell whether or not something",
    "start": "1575485",
    "end": "1581215"
  },
  {
    "text": "is more substantive is like. Is it long and look dense? Um, and I think this is like such an\ninteresting thing because if you go",
    "start": "1581215",
    "end": "1587164"
  },
  {
    "text": "down that route, I mean, I guess it's\na prescription for madness because you're basically like, well, does font\nchoice influence how people think?",
    "start": "1587164",
    "end": "1594124"
  },
  {
    "text": "Like how trustworthy their models are? Yeah, exactly. And so I guess Marina, I'd love for you\nto kind of like riff on that a little bit.",
    "start": "1594205",
    "end": "1598885"
  },
  {
    "text": "You know, how far does\nthis rabbit hole go? Like, once you move away from\nbenchmarks and you say, we're gonna give you a dashboard of different\nthings, you know, you're now kind of",
    "start": "1599725",
    "end": "1607004"
  },
  {
    "text": "in almost like the theater of trust. Like, what do we need to show you,\nand what is the metric that drives the most trust with the user, and\nis that trust justified or not?",
    "start": "1607004",
    "end": "1614575"
  },
  {
    "text": "And just would love to get your\nthoughts on that as someone who's working in the space. There's some real\ninteresting psychology here.",
    "start": "1614575",
    "end": "1619595"
  },
  {
    "text": "So we know that people get extremely\nmad at, uh, computers when they make one mistake, but they're much more okay\nif you were told that it was a human.",
    "start": "1619595",
    "end": "1626684"
  },
  {
    "text": "So that's an interesting psychology. There's another fact that these models\nare fabulous snake oil salesmen.",
    "start": "1626735",
    "end": "1632763"
  },
  {
    "text": "They will tell you something that you will\nread it and you'll believe it even if in the back of your mind you're like, wait,\nisn't that not what I thought that was?",
    "start": "1632965",
    "end": "1639644"
  },
  {
    "text": "But it'll sound so convincing. And so accurate that you're like, Oh yeah,\nthat's, that's, that's the right answer.",
    "start": "1640004",
    "end": "1645435"
  },
  {
    "text": "I have no further questions. They're very good at that. So in that sense, actually, even\nhuman evaluation is very challenging.",
    "start": "1645435",
    "end": "1651314"
  },
  {
    "text": "People are bad at catching\nthese kinds of things. On the other hand, you find yourself in\nan enterprise situation and that's risky.",
    "start": "1651314",
    "end": "1657715"
  },
  {
    "text": "If you really did give incorrect\ninformation, that is very risky. It's again, it's a good reason that\nyou can't really deploy these models",
    "start": "1657734",
    "end": "1663783"
  },
  {
    "text": "by themselves with, with no support. But I think there's a lot of psychology\nactually in setting expectation just",
    "start": "1664114",
    "end": "1669604"
  },
  {
    "text": "in the same way that when we first had. Wikipedia in the world when we first had\nGoogle search and people thought, Oh,",
    "start": "1669774",
    "end": "1675365"
  },
  {
    "text": "well, if it's on the internet, it's true. And then people learned and I think\nthat over time it's going to be the same thing where you're going to learn\nwhat kind of things really need to be.",
    "start": "1675365",
    "end": "1683145"
  },
  {
    "text": "What is the right way to\ninteract with these models? What is the safe way? What is the consumer report\nsort of appropriate way?",
    "start": "1683145",
    "end": "1689534"
  },
  {
    "text": "So some of this is technology. A lot of this is people. A lot of this is people psychology. I cannot give you enough data\npoints and tell you I will",
    "start": "1689535",
    "end": "1697454"
  },
  {
    "text": "never ever ever make a mistake. It's not possible. So we're going to have to figure out\nhow it is to set people's expectations.",
    "start": "1697454",
    "end": "1704155"
  },
  {
    "text": "If people are allowed to sometimes make\nmistakes and you ask for a clarification, how do we get to that state of the world\nalso with the use of the OLM technology?",
    "start": "1704195",
    "end": "1712054"
  },
  {
    "text": "Yeah, for sure. Yeah, I think, uh, and, and this\nsort of pushes into, I think, a sort of interesting direction is like,\nunder certain conditions, I'll just",
    "start": "1713035",
    "end": "1721375"
  },
  {
    "text": "throw out the hot take, is like,\nunder certain conditions, basically optimally safe performance of the\nmodel is not necessarily optimally.",
    "start": "1721375",
    "end": "1728533"
  },
  {
    "text": "Uh, easy to use, I guess, right? So like that is to say like, a perfectly\narticulate model may actually signal",
    "start": "1729195",
    "end": "1735955"
  },
  {
    "text": "more trustworthiness than is warranted. And so there actually may be weird\nsituations where there's kind of this trade off, which is like, we actually\nwant it to perform worse because",
    "start": "1735955",
    "end": "1743325"
  },
  {
    "text": "it inspires, uh, uh, uh, an optimal\nlevel of doubt in the user, right? Would be this sort of, the theory\nthat I'm, I guess I'm arguing.",
    "start": "1743325",
    "end": "1749865"
  },
  {
    "text": "Well, short of, you don't have to have\na performer, if it could just express its own uncertainty, that would always\nbe a big, a big improvement because",
    "start": "1751050",
    "end": "1758769"
  },
  {
    "text": "people really aren't great at, um,\ndissociating things like fluency and,",
    "start": "1758770",
    "end": "1764979"
  },
  {
    "text": "you know, convincingness of, of, of\nsort of discourse with actual truth. And particularly in contexts like this.",
    "start": "1764979",
    "end": "1770979"
  },
  {
    "text": "That enterprises are working in with\nrag where you're taking enterprise documents, HR documents and policies\nand you have to be correct about them.",
    "start": "1771520",
    "end": "1779809"
  },
  {
    "text": "Unless the person who's vibe checking\nthat model really understands those policies in perfect detail.",
    "start": "1780199",
    "end": "1785148"
  },
  {
    "text": "It's very tricky to evaluate and\npeople tend to be fooled very easily. Kush you want to jump in? I see you kind of.",
    "start": "1785359",
    "end": "1790320"
  },
  {
    "text": "Um, no, I mean, I think the word Marina\nused earlier, humility, um, is the key. I mean, the AI needs to be humble,\num, and we need to be humble as well.",
    "start": "1790670",
    "end": "1800000"
  },
  {
    "text": "So, I think the combination,\nuh, is the right way to go. Yeah, for sure.",
    "start": "1800040",
    "end": "1805630"
  },
  {
    "text": "Um, yeah, and I think it's part of the\nproblem too, you know, I was talking with my friend who kind of relayed this\nstory as well, is basically that, um,",
    "start": "1805870",
    "end": "1812080"
  },
  {
    "text": "people have all these expectations that\nare built up around computers, which makes this particularly difficult,\nand like the language model behaves",
    "start": "1812540",
    "end": "1818939"
  },
  {
    "text": "in such a fundamentally different\nway that it's like violating our expectations, where you're like,\nIt's good at poetry, but bad at math.",
    "start": "1818940",
    "end": "1825005"
  },
  {
    "text": "Like, that literally flips everything we\nhave built up in terms of intuitions about computers for like the last 20 years.",
    "start": "1825395",
    "end": "1830625"
  },
  {
    "text": "And so, like, the adage I've been using\nis like, everything you think computers are good at, like, LLMs are bad at. Everything that LLMs are bad at,\nlike, you know, computers are good at.",
    "start": "1830905",
    "end": "1838905"
  },
  {
    "text": "And it's kind of this weird mismatch that\nwe're sort of navigating at the moment.",
    "start": "1838905",
    "end": "1842195"
  },
  {
    "start": "1844000",
    "end": "2755000"
  },
  {
    "text": "Alright, so Kush, uh, you and I\nare going to kick this topic off. Um, this is going to be the\nbig challenge of the episode.",
    "start": "1847450",
    "end": "1853340"
  },
  {
    "text": "Um, which is, if you've been watching the\nmore nerdy channels of the AI discourse",
    "start": "1853450",
    "end": "1858679"
  },
  {
    "text": "online, people have been very recently\nexcited by something called uh, the Kolmogorov Arnold representation theorem,\nwhich is giving rise to a paper that",
    "start": "1858680",
    "end": "1868410"
  },
  {
    "text": "proposes a kind of Kolmogorov Arnold\nnetwork, or CAN for short, and um, it's",
    "start": "1868410",
    "end": "1875880"
  },
  {
    "text": "very difficult to tell from the outside, I\nthink if you're not a technical person, as to like what it is and why it's exciting.",
    "start": "1875880",
    "end": "1881969"
  },
  {
    "text": "Which worries me because it has all\nof the indications of being like, oh, do you, do you use blockchain? 'cause like blockchain\nwill solve this, right?",
    "start": "1882160",
    "end": "1888340"
  },
  {
    "text": "Like, I think we're like gonna\nrapidly go down that direction. . So what I kind of wanna do over the\nnext, um, few minutes as we close",
    "start": "1888340",
    "end": "1895090"
  },
  {
    "text": "out this episode is to basically give\nthe clearest, easiest to understand explanation of cans that anyone has\nyet articulated on the internet.",
    "start": "1895095",
    "end": "1904500"
  },
  {
    "text": "And we're gonna do this together. Right? Okay.\nYeah. Um, so Kush, no pressure, uh, on this one. No pressure . Um.",
    "start": "1904500",
    "end": "1909505"
  },
  {
    "text": "So I think, Kush, maybe the best place\nto start reading some of the papers is, can you give kind of like a quick\nexplanation of why models, large",
    "start": "1910175",
    "end": "1920694"
  },
  {
    "text": "language models, approximate functions? What does that mean, exactly?",
    "start": "1920694",
    "end": "1925735"
  },
  {
    "text": "Yeah, that's precisely\nthe right place to start. So, um, yeah, uh, I mean, so a\nmathematical function, uh, it's",
    "start": "1926385",
    "end": "1935024"
  },
  {
    "text": "looking in some space, right? Um, so in, uh, middle school\nor high school, we saw these",
    "start": "1935065",
    "end": "1940794"
  },
  {
    "text": "one dimensional functions. Um, uh, if our data was just one\ndimensional, what that, uh, function",
    "start": "1940795",
    "end": "1947945"
  },
  {
    "text": "is trying to do is fit that data, and\nby that, use the function rather than",
    "start": "1947945",
    "end": "1953355"
  },
  {
    "text": "the data to predict the next thing. Um, so by doing so we're actually\nable to, uh, so called generalize.",
    "start": "1953355",
    "end": "1960945"
  },
  {
    "text": "So data tells us the pattern, the function\ndescribes the pattern so that the next time we want to make a prediction, we use\nthe function instead of the past data.",
    "start": "1960975",
    "end": "1970755"
  },
  {
    "text": "So I think that's the very key point. And then we can go into What\nthose functions are, how to",
    "start": "1970755",
    "end": "1976845"
  },
  {
    "text": "represent those functions, and how\nto compute those functions, but yeah, that's the starting point. Right, and a prediction here,\njust to make it very simple, is",
    "start": "1976845",
    "end": "1984575"
  },
  {
    "text": "like, um, Uh, tall people, that's\none variable, tend to be heavier.",
    "start": "1984575",
    "end": "1990444"
  },
  {
    "text": "Is that right? Like, for example, that's a prediction\nthat you could build a function around. Yeah, exactly. And you can be very\nquantitative about that.",
    "start": "1990740",
    "end": "1996820"
  },
  {
    "text": "So if I'm six feet tall, maybe\nthat predicts that I'm 180",
    "start": "1996820",
    "end": "2002128"
  },
  {
    "text": "pounds or something like that. So, um, yeah. So then I think the next step, we're going\nto take this step by step, or I think",
    "start": "2002130",
    "end": "2007259"
  },
  {
    "text": "through this problem step by step, right,\nis basically, um, so As I take it, right?",
    "start": "2007260",
    "end": "2014945"
  },
  {
    "text": "Like one of the things that machine\nlearning has really done better than kind of traditional models of AI\nor traditional models of, you know,",
    "start": "2014945",
    "end": "2022205"
  },
  {
    "text": "computer programming, is that we've,\nwe've frequently tried to kind of like hand draft all of these rules, right?",
    "start": "2022205",
    "end": "2029225"
  },
  {
    "text": "So like, you want to write an\nalgorithm to divide, you know, pictures of cats from pictures of dogs. You would do feature engineering, right?",
    "start": "2029465",
    "end": "2034774"
  },
  {
    "text": "You get a bunch of people together to\nkind of like write these equations, these functions out, right? Mm-Hmm, . And I guess, is it right\nto say that machine learning, what",
    "start": "2034775",
    "end": "2041825"
  },
  {
    "text": "it's been really good at doing is. Yeah, I think that's a good way to put it.",
    "start": "2041825",
    "end": "2051940"
  },
  {
    "text": "I'll be a little bit more specific though. So we as humans, um, the\nalgorithm designers, et cetera,",
    "start": "2052219",
    "end": "2058630"
  },
  {
    "text": "um, have been the ones who come\nup with what the functions are. Um, the functions have parameters and it's\nthe learning algorithm that's figuring",
    "start": "2058710",
    "end": "2066919"
  },
  {
    "text": "out the parameters to best, uh, Best fit\nthe data, but at some level, um, we, the,",
    "start": "2066930",
    "end": "2073050"
  },
  {
    "text": "the, um, computer scientists, the ai, um,\nfolks are the ones who decided what were",
    "start": "2073110",
    "end": "2078240"
  },
  {
    "text": "the functions, uh, in this library or\nthis universe of, of possible functions. And then let the, uh, the algorithm\nfigure out the, uh, uh, the nuance,",
    "start": "2078240",
    "end": "2086990"
  },
  {
    "text": "the, the, the parameters and so forth. Yeah, for sure. And so what we, what we do here, I\nguess right now in the world of AI is.",
    "start": "2086990",
    "end": "2095390"
  },
  {
    "text": "multi layer perceptrons, right? Which is like this very particular\nway of implementing AI that is",
    "start": "2095895",
    "end": "2102115"
  },
  {
    "text": "doing the approximation of these\ncomplex functions on its own. And it can solve all of\nthese magical things, right?",
    "start": "2102115",
    "end": "2108615"
  },
  {
    "text": "Like it can, you know, have conversations\nwith you, it can sort pictures of cats from dogs, whatever, whatever you want.",
    "start": "2108615",
    "end": "2113535"
  },
  {
    "text": "You talk a little bit about maybe\nlike the trade offs of that, like what, what do we need to do in\norder to achieve that magic, right?",
    "start": "2114205",
    "end": "2119594"
  },
  {
    "text": "Like, um, yeah. Yeah. So, um, there used to be this,\nuh, car dealership that had a",
    "start": "2119664",
    "end": "2125470"
  },
  {
    "text": "commercial one in the nineties again. Um, so they used to work. It's\nreally dating all of ourselves here. Exactly.",
    "start": "2125470",
    "end": "2130830"
  },
  {
    "text": "So, um, I'm really enjoying this. Yeah. So they, uh, used to say, um,\nstacking them deep, selling them cheap, uh, in terms of their cars.",
    "start": "2131345",
    "end": "2137800"
  },
  {
    "text": "Um, so, uh, so with these, uh,\nthe multilayer perceptions or the feedforward neural networks, um, uh,\nthe trend has been just, uh, there's",
    "start": "2137830",
    "end": "2147770"
  },
  {
    "text": "this one, kind of, these layers. What they do is. They multiply some inputs by some\nweights, um, add them up, and then",
    "start": "2147775",
    "end": "2154795"
  },
  {
    "text": "apply a non linearity, um, something\noften which is called a ReLU function, a rectified linear unit, which kind\nof, um, changes, uh, the output, right?",
    "start": "2155155",
    "end": "2164974"
  },
  {
    "text": "So you have those, you layer them on top\nof each other, you stack them really deep. Um, you keep doing this, keep doing\nthis, keep doing this, um, and that",
    "start": "2164995",
    "end": "2173595"
  },
  {
    "text": "way, uh, you actually end up with\nthe ability to, um, uh, end up with almost any, uh, sort of nonlinear\nfunction, uh, to describe the data.",
    "start": "2173595",
    "end": "2181864"
  },
  {
    "text": "So, uh, you talked about, uh,\nthis universal representation or approximation theorems and stuff.",
    "start": "2181864",
    "end": "2188024"
  },
  {
    "text": "So, um, uh, you can actually prove\nthat through even not very deep neural networks, um, uh, you're able\nto represent any function, uh, that,",
    "start": "2188025",
    "end": "2197744"
  },
  {
    "text": "uh, that you have in front of you. So that has been, seems like\nwe're at the magic, right?",
    "start": "2197744",
    "end": "2203160"
  },
  {
    "text": "Like, this is, this is how it works. Um, I guess one result of that, right,\nis that these models have been, like,",
    "start": "2203190",
    "end": "2209620"
  },
  {
    "text": "they're really expensive to make. Like, you need, like, a lot of energy,\nand a lot of chips, and a lot of",
    "start": "2209820",
    "end": "2214990"
  },
  {
    "text": "data, and a lot of computing time. Um, and so, along comes\nKolmogorov and Arnold.",
    "start": "2214990",
    "end": "2221560"
  },
  {
    "text": "I actually don't know who those\npeople are, I just know them from their representation theorem. You should know Kolmogorov\nfrom a lot of other things.",
    "start": "2221570",
    "end": "2228150"
  },
  {
    "text": "Oh, he's that one. Okay. Yeah, same guy. The same Kolmogorov. Okay.\nAll right.",
    "start": "2228730",
    "end": "2233260"
  },
  {
    "text": "I know that guy. Uh, is it the big Lebowski? Yeah. Um, so, uh, tell me about the\nrepresentation theorem, right?",
    "start": "2234810",
    "end": "2241909"
  },
  {
    "text": "What does it, what does it tell us? What's, what's the big deal? Yeah. So, um, like we just were talking\nabout, so when you have some function",
    "start": "2241909",
    "end": "2249940"
  },
  {
    "text": "that you're trying to represent, um,\nMathematicians have come up with all sorts of ways to, um, be able to decompose\nthat goal of representing this function",
    "start": "2249940",
    "end": "2260079"
  },
  {
    "text": "I have in front of me in terms of more\nprimitive, uh, other functions, right? So, uh, this is, I mean, we see it,\nuh, as an electrical engineer, I see",
    "start": "2260079",
    "end": "2269039"
  },
  {
    "text": "it, um, like Fourier transforms or\nFourier series are ways of decomposing a function into, uh, sines and cosine\nfunctions, um, the same way, um, uh,",
    "start": "2269040",
    "end": "2279310"
  },
  {
    "text": "with the, uh, the multilayer perceptrons. It's into that particular structure. You're decomposing into these\nweights and these nonlinearities.",
    "start": "2279310",
    "end": "2287590"
  },
  {
    "text": "So what the Kolmogorov Arnold\nrepresentation theorem is about is",
    "start": "2287600",
    "end": "2292830"
  },
  {
    "text": "decomposing, again, any function\ninto some other functions. In this case, they happen\nto be one dimensional.",
    "start": "2292870",
    "end": "2299029"
  },
  {
    "text": "So they could be splines or some\nother smooth one dimensional function, and by combining them Uh,\nin a particular, uh, summation, uh,",
    "start": "2299920",
    "end": "2309520"
  },
  {
    "text": "you can, again, uh, represent any\nmultidimensional nonlinear function. And that's the proof, um, is what,\nuh, what Nkolmogorov Arnold tells us,",
    "start": "2309520",
    "end": "2318179"
  },
  {
    "text": "that, uh, in this way of taking 1D\nfunctions, you can come up, uh, and represent any, uh, nonlinear function.",
    "start": "2318230",
    "end": "2323729"
  },
  {
    "text": "Multidimensional function. Yeah.\nWhich is, which is pretty wild, right? Because what you're sort of telling\nme is basically that like the",
    "start": "2323730",
    "end": "2329790"
  },
  {
    "text": "machine learning models that we\nhave now can work this magic, right? Mm-Hmm. , they basically come up with a magic\nmathematical formula that can help",
    "start": "2329795",
    "end": "2336359"
  },
  {
    "text": "you tell pictures of cats from dogs. Let's just take that example. Mm-Hmm. . And then kind of what you're saying\nis that we can take that magic",
    "start": "2336360",
    "end": "2341970"
  },
  {
    "text": "formula and like represent it, like\nwe can break it down to these tiny, tiny, tiny Lego blocks, right?",
    "start": "2341970",
    "end": "2347940"
  },
  {
    "text": "Mm-Hmm. , like all the way down to what\nwe were just talking about. Tall people are heavier, like\nsingle variable stuff, right?",
    "start": "2347940",
    "end": "2354530"
  },
  {
    "text": "And, and kind of the theory, and Chris\nyou should keep me honest, you're the one who actually understands this stuff,\nis like, Like, any, I don't know, or",
    "start": "2354590",
    "end": "2361490"
  },
  {
    "text": "like most very complex, uh, formulas\ncan, can be reducible in this way, I",
    "start": "2361490",
    "end": "2366700"
  },
  {
    "text": "think is what the theorem is saying. Is that right? Okay, so we're there. Um, the KAN network then is a\nnetwork that attempts to do that?",
    "start": "2366710",
    "end": "2373740"
  },
  {
    "text": "Yeah, exactly. So, um, uh, so when in the regular\nneural network we had, um, these",
    "start": "2373790",
    "end": "2379880"
  },
  {
    "text": "weights, um, on these edges that\nare multiplying the inputs here. We're applying that function, the\nsplines or the other sort of 1D,",
    "start": "2379880",
    "end": "2387690"
  },
  {
    "text": "um, uh, sort of non linear function. And then instead of, uh, I\nmean, you add in the same way.",
    "start": "2387780",
    "end": "2394059"
  },
  {
    "text": "So the Cayenne is also adding\nup the inputs, but then there's no additional, um, non linearity\nafterwards, um, like the ReLU that we",
    "start": "2394079",
    "end": "2401730"
  },
  {
    "text": "see, um, in the normal neural network. So all the non linearity is done,\num, before you add things up.",
    "start": "2401730",
    "end": "2408020"
  },
  {
    "text": "Uh, so it's. It's just more complication in one\nplace rather than in a different place.",
    "start": "2408480",
    "end": "2413510"
  },
  {
    "text": "And so, uh, just by doing that\nchange, you can, um, uh, reduce the",
    "start": "2413510",
    "end": "2419080"
  },
  {
    "text": "number of parameters, um, because,\nuh, this more complicated thing on the edges, uh, is actually, uh,\nmore, uh, able to represent these",
    "start": "2419080",
    "end": "2428290"
  },
  {
    "text": "weird, uh, various sort of behaviors. So, so that's kind of what it's,\nuh, what it's trying to do.",
    "start": "2428539",
    "end": "2433330"
  },
  {
    "text": "So we're there. Deep breath. Yes. Two last questions. Yes. Yes.",
    "start": "2434820",
    "end": "2439420"
  },
  {
    "text": "Does it matter? What, what would, what would KAN\nmodels, what's the, what's the promise of KAN models if they can do this?",
    "start": "2439930",
    "end": "2445730"
  },
  {
    "text": "It seems like you've just taken\nthis complex thing and turned it into a bunch of Lego blocks. Yeah. So from my one brain cell standpoint, I'm\nlike, isn't that kind of the same thing?",
    "start": "2445730",
    "end": "2452789"
  },
  {
    "text": "Yeah. Um, so it is true. I mean, you're just shifting\nthe nonlinearities from one",
    "start": "2453080",
    "end": "2459170"
  },
  {
    "text": "place to a different place. One thing that the KANs are\nable to do better a little bit",
    "start": "2459170",
    "end": "2465040"
  },
  {
    "text": "is some more interpretability. So, um, when you look at those splines,\nuh, they actually make sense to us.",
    "start": "2465040",
    "end": "2471195"
  },
  {
    "text": "Um, so that, uh, height and weight sort\nof relationship or any of those other things, um, uh, we can understand better.",
    "start": "2471225",
    "end": "2477635"
  },
  {
    "text": "Uh, so there's this interpretability\nmethod called SHAP, which, um, people have been using for a while.",
    "start": "2477644",
    "end": "2483905"
  },
  {
    "text": "Um, this is like automatic SHAP without\nhaving to do SHAP in, in a sense. Um, you know, Interoperability there\nfor some folks who are not maybe",
    "start": "2483925",
    "end": "2491305"
  },
  {
    "text": "as into the papers is just simply\nunderstanding what the models, why the model is doing what it's doing.",
    "start": "2491305",
    "end": "2496174"
  },
  {
    "text": "Yeah, exactly. Exactly. Yep. Um, and so, I mean, that's one advantage. Um, the disadvantage though is that,\nuh, uh, our hardware infrastructure",
    "start": "2496425",
    "end": "2505965"
  },
  {
    "text": "has not been optimized, um, for,\nuh, uh, for these sort of things, for these splines and so forth.",
    "start": "2505974",
    "end": "2512105"
  },
  {
    "text": "Whereas the matrix vector computations\nfor neural networks, um, are, um, Uh,",
    "start": "2512715",
    "end": "2517815"
  },
  {
    "text": "kind of, uh, like, very highly optimized\nthrough those, uh, H100s and so forth.",
    "start": "2517865",
    "end": "2523565"
  },
  {
    "text": "So, uh, so that's, I\nthink, the difference. We might, if this catches on,\nI mean, develop some hardware",
    "start": "2523565",
    "end": "2528875"
  },
  {
    "text": "for this type of thing as well. Um, and, uh, I mean, the last point that\nI'll make is these are not new ideas.",
    "start": "2528875",
    "end": "2534855"
  },
  {
    "text": "I mean, this is something that's\nbeen around, uh, Uh, even our team,",
    "start": "2534855",
    "end": "2539920"
  },
  {
    "text": "uh, like a couple years ago, um, we\ndeveloped something called Koffer Nets. It uses continued fractions, which are,\num, a third way of representing functions.",
    "start": "2539950",
    "end": "2548639"
  },
  {
    "text": "Also, it has a, uh, approximation\nor, um, universal approximation theorem associated with it.",
    "start": "2548639",
    "end": "2553809"
  },
  {
    "text": "Um, it uses continued fractions. that have been known since antiquity,\nlike the ancient Indians and ancient",
    "start": "2553880",
    "end": "2559655"
  },
  {
    "text": "Greeks knew about all this stuff. So, I mean, like all\nthis fancy math is great. And, um, uh, it's just different ways\nof putting together, I mean, different,",
    "start": "2559665",
    "end": "2568635"
  },
  {
    "text": "uh, of these functions together. And then at the end of the day, um,\nuh, they all let you, I mean, kind",
    "start": "2568664",
    "end": "2574784"
  },
  {
    "text": "represent, uh, these different nonlinear\nfunctions, uh, how you train them, uh, might be more or less costly, where\nthe interpretability is, might be",
    "start": "2575125",
    "end": "2582735"
  },
  {
    "text": "more or less, uh, easy to, or hard. So, uh, so, I mean, it could turn\ninto something, uh, it might just be",
    "start": "2582735",
    "end": "2589225"
  },
  {
    "text": "another option, um, so we'll, we'll see. Yeah, for sure. Yeah, it's fascinating. And that, I think, opens up definitely\na direction that I hadn't really thought",
    "start": "2589225",
    "end": "2595775"
  },
  {
    "text": "of, because I think the main thing I\nhad heard is, well, you can make much more energy efficient models, right? But, um, it seems like two\nthings you're pointing out.",
    "start": "2595775",
    "end": "2602443"
  },
  {
    "text": "One of them is, like, We might be able\nto understand why these models are making the decisions they do at a much, like,\ncloser level of depth than we have in",
    "start": "2602444",
    "end": "2609859"
  },
  {
    "text": "the past, which seems, which seems huge. Um, and then I think the second\npoint is actually that this is, like, not new stuff, right?",
    "start": "2609860",
    "end": "2616259"
  },
  {
    "text": "Like, I mean, much like\nneural nets themselves, right? This is like, we're just like\npulling all this old stuff back",
    "start": "2616259",
    "end": "2621310"
  },
  {
    "text": "again and being like, oh, I guess\nit works now, you know, ultimately. Well, and I think that resonates\nreally well with Kush's point about the hardware match.",
    "start": "2621310",
    "end": "2627160"
  },
  {
    "text": "I mean, oftentimes, uh, you know, we,\nget success in the field moves, not",
    "start": "2627160",
    "end": "2632755"
  },
  {
    "text": "because something is the mathematically\noptimal thing, but it's something that can be done at sort of a\nrational scale with irrational speed.",
    "start": "2632755",
    "end": "2641225"
  },
  {
    "text": "And as you say, you know, deep\nlearning was, you know, kind of a rebrand of artificial neural networks.",
    "start": "2641624",
    "end": "2646645"
  },
  {
    "text": "It was around for decades\nbefore it caught on. Why did it catch on? It's not because there were some\nmathematical breakthroughs because",
    "start": "2646645",
    "end": "2651695"
  },
  {
    "text": "the hardware, like GPUs by accident,\nwere really good at doing this. And then that just sort\nof set us on a path.",
    "start": "2651725",
    "end": "2657294"
  },
  {
    "text": "So Um, obviously all these new\ndevelopments are really exciting, and we could build different hardware\npotentially, um, but, you know, any",
    "start": "2657295",
    "end": "2665580"
  },
  {
    "text": "new idea like this is going to compete\nagainst how wonderfully good GPUs",
    "start": "2665580",
    "end": "2670790"
  },
  {
    "text": "are at doing the basic computations\nneeded for, for deep learning. Um, so, you know, there's, it's an\ninteresting battle, you know, an",
    "start": "2670810",
    "end": "2677919"
  },
  {
    "text": "interesting set of trade offs there. Yeah, that I think relationship between\nsort of like hardware and what's happening",
    "start": "2677919",
    "end": "2683290"
  },
  {
    "text": "on the model side, I think is like one\nof the most interesting aspects of this. And like, how long does it take\nfor a model to influence hardware?",
    "start": "2683290",
    "end": "2689230"
  },
  {
    "text": "You know, are we just locked into\nCUDA for the rest of our lives? It's like all these things are like\nvery, very interesting questions.",
    "start": "2689540",
    "end": "2694590"
  },
  {
    "text": "Um, so, uh, Marina, any last thoughts,\nuh, before we, we close up today?",
    "start": "2694640",
    "end": "2699279"
  },
  {
    "text": "Yeah, um, even more general\ncomment continuing what Christian David said is representations\nof data are not created equal.",
    "start": "2699760",
    "end": "2706329"
  },
  {
    "text": "So yes, it's the same information,\nbut when you change the way that you represent it, you're able to do things\nwith it that you weren't able to before.",
    "start": "2706915",
    "end": "2712904"
  },
  {
    "text": "So even with something like a large\nlanguage model, uh, you're representing data that exists, let's say on the\ninternet, but you're representing it",
    "start": "2713245",
    "end": "2719264"
  },
  {
    "text": "in such a way that you can access it\nin a way that you couldn't before. Same thing with, for\nexample, KAN versus MLP.",
    "start": "2719265",
    "end": "2724945"
  },
  {
    "text": "The representation changes, there's\ngoing to be trade offs, but it's always very interesting to try this. Uh, the fact that we now have\nmore of these options open to us",
    "start": "2724965",
    "end": "2732560"
  },
  {
    "text": "because the hardware has caught up\nto the math that has been around for years or decades or centuries.",
    "start": "2732560",
    "end": "2737770"
  },
  {
    "text": "Yeah, that means try again,\ntry again, try again, see what, what new things will come up. Data representation is really\none of the things underlying",
    "start": "2738060",
    "end": "2745830"
  },
  {
    "text": "driving this current era of AI. So more work in this direction\nis just gonna continue to drive",
    "start": "2745839",
    "end": "2752375"
  },
  {
    "text": "things an interesting place. That's great. Yeah. Well, I can't think of\na better note to end on.",
    "start": "2752375",
    "end": "2757575"
  },
  {
    "start": "2755000",
    "end": "2783000"
  },
  {
    "text": "Um, Kush, MVP, thank you for\ncoming on the show again. Um, and, uh, Marina, David, hope\nto have you on the show again.",
    "start": "2757635",
    "end": "2764985"
  },
  {
    "text": "And, uh, I hope, uh, all of you listeners\nout there, join us next week for another episode of Mixture of Experts.",
    "start": "2764985",
    "end": "2771585"
  },
  {
    "text": "Thanks everyone. Thanks. Thank you. Appreciate it.",
    "start": "2771855",
    "end": "2775820"
  }
]