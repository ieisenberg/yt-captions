[
  {
    "start": "0",
    "end": "133000"
  },
  {
    "text": "Ethics is based on the Greek word Ethos.",
    "start": "720",
    "end": "4200"
  },
  {
    "text": "Culture is an expression of the Ethos",
    "start": "9670",
    "end": "13449"
  },
  {
    "text": "or the atmosphere that is established through ethics,",
    "start": "13475",
    "end": "17014"
  },
  {
    "text": "the unwritten rules of a group of people.",
    "start": "17037",
    "end": "19441"
  },
  {
    "text": "Our culture is able to establish a consent-based understanding of Artificial Intelligence.",
    "start": "20120",
    "end": "28926"
  },
  {
    "text": "The outcomes of AI are so much better",
    "start": "30341",
    "end": "33415"
  },
  {
    "text": "if we feed it with data given with consent as opposed to just taking the data without consent.",
    "start": "33417",
    "end": "39482"
  },
  {
    "text": "We at IBM have written these rules as part of our ethical principles.",
    "start": "39940",
    "end": "44259"
  },
  {
    "text": "Now, apart from consent,",
    "start": "44483",
    "end": "47368"
  },
  {
    "text": "a culture that nurtures responsible AI truly values diversity and inclusivity.",
    "start": "47413",
    "end": "53157"
  },
  {
    "text": "AI ethics, remember, is a team sport.",
    "start": "53603",
    "end": "58215"
  },
  {
    "text": "Exclusion breeds elitism",
    "start": "60775",
    "end": "63683"
  },
  {
    "text": "that, in turn, breeds the very toxic notion that one human being is better than another human being.",
    "start": "63705",
    "end": "70857"
  },
  {
    "text": "We actually know via this mathematical model",
    "start": "71360",
    "end": "75273"
  },
  {
    "text": "that the wider the variance, the more standard the mean.",
    "start": "75296",
    "end": "79516"
  },
  {
    "text": "Or put another way, the more diverse a group of people actually trying to tackle a really complicated problem,",
    "start": "79605",
    "end": "86362"
  },
  {
    "text": "the less chance for error.",
    "start": "86362",
    "end": "88115"
  },
  {
    "text": "So it's really important to consider things like gender, race, ethnicity,",
    "start": "88452",
    "end": "93622"
  },
  {
    "text": "age, neurodiversity, worldview, skill set, as you're starting your team.",
    "start": "93623",
    "end": "99218"
  },
  {
    "text": "So now that you've got this amazingly diverse team, what next?",
    "start": "99418",
    "end": "104945"
  },
  {
    "text": "Remember, earning trust in artificial intelligence is not a technological challenge,",
    "start": "105542",
    "end": "110001"
  },
  {
    "text": "it's a sociotechnological challenge.",
    "start": "110046",
    "end": "112060"
  },
  {
    "text": "So thus it's really important to adopt frameworks for systemic empathy.",
    "start": "112600",
    "end": "117489"
  },
  {
    "text": "We have a map, we have exercises, we have design thinking as a means to do this,",
    "start": "117942",
    "end": "123342"
  },
  {
    "text": "and we use this approach to generate artificial intelligence",
    "start": "123365",
    "end": "127684"
  },
  {
    "text": "that is intentional in its efforts to augment a human being.",
    "start": "127684",
    "end": "132592"
  },
  {
    "text": "We have an amazing design thinking practice.",
    "start": "132812",
    "end": "135433"
  },
  {
    "start": "133000",
    "end": "224000"
  },
  {
    "text": "Our design practice is based on what is somebody thinking, seeing, hearing and doing.",
    "start": "136110",
    "end": "143249"
  },
  {
    "text": "The very expression of culture.",
    "start": "147370",
    "end": "149469"
  },
  {
    "text": "Now, about 80% of efforts in artificial intelligence actually get stuck in proof of concept,",
    "start": "155912",
    "end": "162269"
  },
  {
    "text": "and this is for a wide variety of different kinds of reasons.",
    "start": "162292",
    "end": "164947"
  },
  {
    "text": "Some of the top ones are that oftentimes",
    "start": "165840",
    "end": "170164"
  },
  {
    "text": "the investments in the AI aren't tied directly to business strategy,",
    "start": "170188",
    "end": "174252"
  },
  {
    "text": "or people simply don't trust the results of the model.",
    "start": "174254",
    "end": "177621"
  },
  {
    "text": "So we actually use design thinking to walk both C-suite as well as technologists through four different stages.",
    "start": "177710",
    "end": "185728"
  },
  {
    "text": "We start with intent.",
    "start": "186510",
    "end": "188610"
  },
  {
    "text": "What is the intent behind the investment in this AI model and how is it tied directly to strategy?",
    "start": "191410",
    "end": "197499"
  },
  {
    "text": "We identify the sources of data that they have access to and how it is being collected.",
    "start": "198201",
    "end": "207285"
  },
  {
    "text": "We evaluate the data sources and the effects of the proposed AI model,",
    "start": "207781",
    "end": "217445"
  },
  {
    "text": "and we plan how to roll out the effort.",
    "start": "217468",
    "end": "221997"
  },
  {
    "text": "In the evaluate phase, we embed frameworks for systemic empathy",
    "start": "223788",
    "end": "228489"
  },
  {
    "start": "224000",
    "end": "429000"
  },
  {
    "text": "called \"tech ethics by design\" on an as-needed basis.",
    "start": "228490",
    "end": "232396"
  },
  {
    "text": "This is based on three different steps.",
    "start": "232760",
    "end": "234860"
  },
  {
    "text": "The first step is called \"layers of effect\".",
    "start": "235220",
    "end": "238279"
  },
  {
    "text": "Looks kind of like this, right?",
    "start": "240780",
    "end": "243000"
  },
  {
    "text": "These would be your primary and your secondary effects, both intended and known.",
    "start": "243450",
    "end": "250680"
  },
  {
    "text": "And this third one is actually tertiary effects, unintended and possibly known effects of your AI model.",
    "start": "251130",
    "end": "259769"
  },
  {
    "text": "And this is actually where your team of people who will be doing this workshop together",
    "start": "260130",
    "end": "265454"
  },
  {
    "text": "will actually might be coming up with ideas on what could potential harm look like, right?",
    "start": "265455",
    "end": "271220"
  },
  {
    "text": "So, it's really important you've got the right people in the room.",
    "start": "271441",
    "end": "273745"
  },
  {
    "text": "The second step for tech ethics by design is called \"dichotomy mapping\".",
    "start": "274950",
    "end": "281790"
  },
  {
    "text": "And in this step, you take these ideas around potential tertiary effects, again unintended and possibly known,",
    "start": "283870",
    "end": "291492"
  },
  {
    "text": "and you split them up into what could be potentially beneficial, and what could be potentially harmful?",
    "start": "291517",
    "end": "300414"
  },
  {
    "text": "And then the third step is called \"ethical hacking\".",
    "start": "302560",
    "end": "306789"
  },
  {
    "text": "In this step, the first thing to do as an organization is think about",
    "start": "310106",
    "end": "315244"
  },
  {
    "text": "what are your principles for artificial intelligence?",
    "start": "315267",
    "end": "318556"
  },
  {
    "text": "Again, what is your Ethos?",
    "start": "318722",
    "end": "320770"
  },
  {
    "text": "What are you going to stand by?",
    "start": "320975",
    "end": "322844"
  },
  {
    "text": "The second step, so again, these are principles or values,",
    "start": "323227",
    "end": "328496"
  },
  {
    "text": "then this part is given these principles, given these values, what are the rights of the individual?",
    "start": "328541",
    "end": "336209"
  },
  {
    "text": "What are the rights of the end user?",
    "start": "336300",
    "end": "338729"
  },
  {
    "text": "And then given that given those rights and this particular harm,",
    "start": "339848",
    "end": "346320"
  },
  {
    "text": "which might have come up in the prior example, in the prior step,",
    "start": "346364",
    "end": "350453"
  },
  {
    "text": "how would a team, how would your team mitigate against any potential harm",
    "start": "350455",
    "end": "357386"
  },
  {
    "text": "so that it's actually you're designing intentionally in order to protect against this particular harm?",
    "start": "357431",
    "end": "365042"
  },
  {
    "text": "I cannot underscore enough how valuable this framework is,",
    "start": "366747",
    "end": "372257"
  },
  {
    "text": "it has really unlocked true epiphanies on teams.",
    "start": "372258",
    "end": "376014"
  },
  {
    "text": "In closing, we have the ability to set up the correct Ethos,",
    "start": "376911",
    "end": "382164"
  },
  {
    "text": "right, the ethical standards to augment humans and truly democratize artificial intelligence.",
    "start": "382187",
    "end": "387608"
  },
  {
    "text": "In order to achieve systemic equity in AI, we need to work with truly diverse teams, right?",
    "start": "388791",
    "end": "397215"
  },
  {
    "text": "And the way that these folks work together is to use design thinking",
    "start": "397386",
    "end": "402064"
  },
  {
    "text": "to tie AI models to business intent and crack the empathy code",
    "start": "402088",
    "end": "408341"
  },
  {
    "text": "well before any programing code is written.",
    "start": "408342",
    "end": "410775"
  },
  {
    "text": "Thank you.",
    "start": "411229",
    "end": "411995"
  },
  {
    "text": "If you like this video and series, please comment below.",
    "start": "413350",
    "end": "417144"
  },
  {
    "text": "Stay tuned for more videos that are part of the series and to get updates, please like and subscribe.",
    "start": "417510",
    "end": "423912"
  }
]