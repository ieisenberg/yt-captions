[
  {
    "start": "0",
    "end": "10000"
  },
  {
    "text": "We are here to talk to you about PyTorch\nand how it's a great tool",
    "start": "33",
    "end": "3870"
  },
  {
    "text": "for you to scale your deep learning models\nin a cheap and a fast and efficient way.",
    "start": "3870",
    "end": "8608"
  },
  {
    "text": "So when we're scaling neural networks up,\nwhat that means is",
    "start": "9843",
    "end": "12979"
  },
  {
    "start": "10000",
    "end": "231000"
  },
  {
    "text": "we are adding more and more layers",
    "start": "12979",
    "end": "18151"
  },
  {
    "text": "to your neural network.",
    "start": "18685",
    "end": "20286"
  },
  {
    "text": "And what this does is it allows your model\nto capture more nuance in your data",
    "start": "20286",
    "end": "25625"
  },
  {
    "text": "and allows your model to perform\nmore complex tasks.",
    "start": "26092",
    "end": "29161"
  },
  {
    "text": "Well, all of this doesn't come for free.",
    "start": "29195",
    "end": "31131"
  },
  {
    "text": "The model will start to require\nmore memory and more compute.",
    "start": "31131",
    "end": "36436"
  },
  {
    "text": "Let's see the llama model, for example.",
    "start": "36669",
    "end": "38972"
  },
  {
    "text": "It has four variants",
    "start": "40540",
    "end": "41975"
  },
  {
    "text": "ranging from 7 billion parameters",
    "start": "41975",
    "end": "44177"
  },
  {
    "text": "to 70 billion parameters.",
    "start": "44844",
    "end": "47680"
  },
  {
    "text": "The smallest variant, 7 billion,",
    "start": "47680",
    "end": "49749"
  },
  {
    "text": "was trained using 2 trillion tokens",
    "start": "50717",
    "end": "53218"
  },
  {
    "text": "and it required around",
    "start": "55221",
    "end": "56790"
  },
  {
    "text": "roughly 200,000 GPU hours.",
    "start": "56790",
    "end": "59993"
  },
  {
    "text": "That's a long time.",
    "start": "62095",
    "end": "63496"
  },
  {
    "text": "So PyTorch, along with offering",
    "start": "63496",
    "end": "65932"
  },
  {
    "text": "the modular building blocks\nto build your neural networks,",
    "start": "65932",
    "end": "69235"
  },
  {
    "text": "it also offers some utilities\nto distribute to this training workload.",
    "start": "69269",
    "end": "72739"
  },
  {
    "text": "And let's take a look at one of them",
    "start": "73606",
    "end": "75742"
  },
  {
    "text": "called Distributed Data Parallel (DDP).",
    "start": "76876",
    "end": "78511"
  },
  {
    "text": "So when you're training your model",
    "start": "78511",
    "end": "79579"
  },
  {
    "text": "with DDP, there are three steps that need to happen.",
    "start": "79579",
    "end": "82115"
  },
  {
    "text": "First, the forward pass\nwhere you take the data",
    "start": "83716",
    "end": "87620"
  },
  {
    "text": "and you pass it through the model,",
    "start": "87654",
    "end": "90457"
  },
  {
    "text": "computes the loss",
    "start": "90657",
    "end": "92625"
  },
  {
    "text": "which is then back propagated",
    "start": "92759",
    "end": "95161"
  },
  {
    "text": "through the model,\nwhich then gives you the gradients.",
    "start": "96563",
    "end": "99165"
  },
  {
    "text": "The third step where we update the model's\nweights is preceded",
    "start": "99833",
    "end": "104104"
  },
  {
    "text": "by a synchronization,\nwhere all the computed gradients",
    "start": "104104",
    "end": "112344"
  },
  {
    "text": "from each of these replicas\nare communicated with each other.",
    "start": "112345",
    "end": "117050"
  },
  {
    "text": "Now, the hallmark of DDP\nand really all distributed training",
    "start": "117684",
    "end": "121588"
  },
  {
    "text": "is an overlap of",
    "start": "123022",
    "end": "124657"
  },
  {
    "text": "the computation and communication.",
    "start": "124657",
    "end": "128627"
  },
  {
    "text": "Essentially what",
    "start": "128928",
    "end": "129795"
  },
  {
    "text": "that means is simultaneously\nwe are doing the back propagation",
    "start": "129796",
    "end": "133933"
  },
  {
    "text": "while communicating\nall of the calculated gradients.",
    "start": "134300",
    "end": "137203"
  },
  {
    "text": "This saves us time\nand keeps the GPU running",
    "start": "137604",
    "end": "140206"
  },
  {
    "text": "at a near 100% utilization.",
    "start": "140507",
    "end": "143076"
  },
  {
    "text": "At a very high level,",
    "start": "144110",
    "end": "145011"
  },
  {
    "text": "what that looks like:",
    "start": "145011",
    "end": "147213"
  },
  {
    "text": "You first divide the model into buckets.",
    "start": "147213",
    "end": "151951"
  },
  {
    "text": "Each replica calculates",
    "start": "153887",
    "end": "155388"
  },
  {
    "text": "the gradients of the first bucket",
    "start": "155388",
    "end": "158258"
  },
  {
    "text": "and while it is calculating",
    "start": "158691",
    "end": "160659"
  },
  {
    "text": "the gradients of the second bucket,",
    "start": "160660",
    "end": "164330"
  },
  {
    "text": "these first bucket\ngradients are synchronized.",
    "start": "164330",
    "end": "168835"
  },
  {
    "text": "And this is happening simultaneously",
    "start": "170703",
    "end": "172772"
  },
  {
    "text": "with the computation of the gradients\nin the second bucket.",
    "start": "172772",
    "end": "176142"
  },
  {
    "text": "And similarly,\nas each bucket is being calculated,",
    "start": "176576",
    "end": "180580"
  },
  {
    "text": "the preceding buckets\ngradients are being communicated.",
    "start": "180580",
    "end": "183316"
  },
  {
    "text": "This ensures that all the GPUs are running",
    "start": "184851",
    "end": "187353"
  },
  {
    "text": "at full utilization\nand you're not having any idle workers.",
    "start": "187887",
    "end": "191291"
  },
  {
    "text": "They're all working\nvery hard to train your model.",
    "start": "191691",
    "end": "193493"
  },
  {
    "text": "And this is the",
    "start": "194961",
    "end": "195728"
  },
  {
    "text": "case where the model fits in one GPU.",
    "start": "195728",
    "end": "198530"
  },
  {
    "text": "For example,\nthe 7 billion model can fit on",
    "start": "198898",
    "end": "202000"
  },
  {
    "text": "on any cloud GPUs today.",
    "start": "203403",
    "end": "206039"
  },
  {
    "text": "But when you start scaling\nthat up, like, for example, 70 billion",
    "start": "206039",
    "end": "209576"
  },
  {
    "text": "or even the 30 billion models, it's\nvery difficult to fit them in one GPU.",
    "start": "209742",
    "end": "214514"
  },
  {
    "text": "So in that paradigm, in those regimes,",
    "start": "214981",
    "end": "217617"
  },
  {
    "text": "the DDP model does not work.",
    "start": "218518",
    "end": "221087"
  },
  {
    "text": "Now I'll call upon Raghu to talk to us\nabout what in PyTorch allows us",
    "start": "221087",
    "end": "226092"
  },
  {
    "text": "to train models which are larger than what\nyour single GPU can accommodate.",
    "start": "226092",
    "end": "230430"
  },
  {
    "start": "231000",
    "end": "527000"
  },
  {
    "text": "All right, so you've heard",
    "start": "231898",
    "end": "233465"
  },
  {
    "text": "all about distributed data parallel,\nand how do you scale a small model",
    "start": "233466",
    "end": "237971"
  },
  {
    "text": "on very large number of GPUs\nand reduce your training times.",
    "start": "238271",
    "end": "241808"
  },
  {
    "text": "So now let's look at what happens",
    "start": "242175",
    "end": "244510"
  },
  {
    "text": "when we have a model\nthat does not fit in a single GPU.",
    "start": "244510",
    "end": "247879"
  },
  {
    "text": "And that is\nwhere FSDP comes to your rescue.",
    "start": "248281",
    "end": "251284"
  },
  {
    "text": "FSDP stands for \"Fully Sharded Data Parallel\"",
    "start": "252151",
    "end": "255588"
  },
  {
    "text": "and what fully sharded\ndata parallel does is it",
    "start": "256022",
    "end": "258625"
  },
  {
    "text": "takes the model, breaks it down into\nwhat are called",
    "start": "258625",
    "end": "261661"
  },
  {
    "text": "units, and all of these units are",
    "start": "261661",
    "end": "267467"
  },
  {
    "text": "then sharded across GPUs.",
    "start": "267467",
    "end": "271170"
  },
  {
    "text": "So you can take that--",
    "start": "271304",
    "end": "272171"
  },
  {
    "text": "Think of it as shredding the model\nand then each GPU owns",
    "start": "272171",
    "end": "277242"
  },
  {
    "text": "small portions of this model.",
    "start": "277243",
    "end": "280647"
  },
  {
    "text": "So that's where\nshards are coming in.",
    "start": "281247",
    "end": "283049"
  },
  {
    "text": "Now, what",
    "start": "284050",
    "end": "284917"
  },
  {
    "text": "happens after that is pretty much\nvery much like what DDP is doing.",
    "start": "284917",
    "end": "288921"
  },
  {
    "text": "Think of instead of a model,\nyou are thinking of it as a unit.",
    "start": "288955",
    "end": "293026"
  },
  {
    "text": "And during the initial unit",
    "start": "293459",
    "end": "296996"
  },
  {
    "text": "construction, from the shards,",
    "start": "296996",
    "end": "299098"
  },
  {
    "text": "you are going to do an AllGather.",
    "start": "300099",
    "end": "303336"
  },
  {
    "text": "And this happens in the forward pass.",
    "start": "304170",
    "end": "306739"
  },
  {
    "text": "So you're gathering\nthe unit, you're computing on top of it.",
    "start": "306873",
    "end": "310076"
  },
  {
    "text": "And this happens across all the units.",
    "start": "311177",
    "end": "313680"
  },
  {
    "text": "So as soon as your unit is computed,\nyou lose that unit's memory",
    "start": "313680",
    "end": "318350"
  },
  {
    "text": "and then you go to the next unit\nand so on.",
    "start": "318351",
    "end": "320353"
  },
  {
    "text": "And that's the forward pass.",
    "start": "321054",
    "end": "322322"
  },
  {
    "text": "Once the entire forward pass is",
    "start": "322322",
    "end": "323856"
  },
  {
    "text": "computed, your loss is ",
    "start": "323856",
    "end": "325525"
  },
  {
    "text": "computed, and you go--",
    "start": "325525",
    "end": "327593"
  },
  {
    "text": "So this is your forward pass.",
    "start": "327627",
    "end": "329829"
  },
  {
    "text": "And now you're going to do an AllGather",
    "start": "329829",
    "end": "332765"
  },
  {
    "text": "in the backward pass.",
    "start": "333966",
    "end": "336135"
  },
  {
    "text": "And during the backward pass,\nwhat is happening is you are computing",
    "start": "336135",
    "end": "340540"
  },
  {
    "text": "again, very much like here,\nyou are gathering the unit, you're computing",
    "start": "340540",
    "end": "344110"
  },
  {
    "text": "the back propagation,\nbut just in the reverse fashion.",
    "start": "344377",
    "end": "347847"
  },
  {
    "text": "Once you have computed the gradients,",
    "start": "348781",
    "end": "351117"
  },
  {
    "text": "then those are again,",
    "start": "351617",
    "end": "354687"
  },
  {
    "text": "very much\nlike what you have done for your DDP.",
    "start": "354987",
    "end": "357990"
  },
  {
    "text": "Those are synchronized across all the GPUs\nthat are responsible for holding",
    "start": "358024",
    "end": "363629"
  },
  {
    "text": "that particular portion of the model.",
    "start": "364130",
    "end": "366899"
  },
  {
    "text": "So once you are synchronized,\nthat completes your entire step",
    "start": "367400",
    "end": "370268"
  },
  {
    "text": "and then you continue doing all of this\nand then FSDP, very much like DDP,",
    "start": "370470",
    "end": "375742"
  },
  {
    "text": "you are going to utilize overlap\nin a significant way",
    "start": "376008",
    "end": "379612"
  },
  {
    "text": "because imagine in DDP there was only one\nsingle synchronization step, in FSDP,",
    "start": "379812",
    "end": "385118"
  },
  {
    "text": "I have more opportunities\nfor doing overlap and keeping those GPUs",
    "start": "385118",
    "end": "390089"
  },
  {
    "text": "continuously busy\nwhile you are doing your computation.",
    "start": "390089",
    "end": "393960"
  },
  {
    "text": "And that is how you achieve scale of these models.",
    "start": "395763",
    "end": "398998"
  },
  {
    "text": "So what are the typical ways\nin which people train these large models?",
    "start": "399198",
    "end": "403636"
  },
  {
    "text": "Most of the world knows about them\nas HPC systems,",
    "start": "403803",
    "end": "406939"
  },
  {
    "text": "very large scale systems\nwith very high speed interconnects,",
    "start": "407206",
    "end": "410943"
  },
  {
    "text": "state of the art GPUs and servers\nand training these models.",
    "start": "411377",
    "end": "414747"
  },
  {
    "text": "So what happens if I have \"not a HPC\nsystem\" that, you know, I have a good node.",
    "start": "415081",
    "end": "421287"
  },
  {
    "text": "So let's say that instead of this GPU,\nwe call this a node.",
    "start": "421521",
    "end": "425091"
  },
  {
    "text": "Typically nodes have eight,",
    "start": "425625",
    "end": "428327"
  },
  {
    "text": "maybe 16 GPUs in them.",
    "start": "428327",
    "end": "431197"
  },
  {
    "text": "And many of these, the typical HPC system",
    "start": "431831",
    "end": "435535"
  },
  {
    "text": "has an HPC interconnect things like",
    "start": "435535",
    "end": "438905"
  },
  {
    "text": "the interconnect",
    "start": "440273",
    "end": "443443"
  },
  {
    "text": "where people",
    "start": "444243",
    "end": "445378"
  },
  {
    "text": "may have heard the term InfiniBand.",
    "start": "445378",
    "end": "447979"
  },
  {
    "text": "So very high speed\ninterconnect between nodes;",
    "start": "448614",
    "end": "451150"
  },
  {
    "text": "and people may\nhave heard the term Ethernet.",
    "start": "452351",
    "end": "455254"
  },
  {
    "text": "Ethernet is far more",
    "start": "456422",
    "end": "457924"
  },
  {
    "text": "common in many environments,\nwhether it is cloud",
    "start": "457924",
    "end": "461828"
  },
  {
    "text": "or on-prem environments,\nwhereas InfiniBand is less common.",
    "start": "461828",
    "end": "465331"
  },
  {
    "text": "So there is a common misconception\nthat these larger models",
    "start": "465698",
    "end": "470136"
  },
  {
    "text": "will always need InfiniBand,\neven though that is true",
    "start": "470136",
    "end": "473405"
  },
  {
    "text": "for some of the larger models\nand it is faster.",
    "start": "473406",
    "end": "476042"
  },
  {
    "text": "But with Ethernet also, you can train\nthese models in an efficient manner.",
    "start": "476309",
    "end": "481013"
  },
  {
    "text": "And that is where IBM helped, worked\nwith the PyTorch community,",
    "start": "481781",
    "end": "486419"
  },
  {
    "text": "and introduced this concept of rate\nlimiter.",
    "start": "486619",
    "end": "490857"
  },
  {
    "text": "What rate limiter is doing,\nis really balancing off the trade off",
    "start": "491924",
    "end": "496662"
  },
  {
    "text": "between the overlap\nin terms of communication and computation",
    "start": "496662",
    "end": "501600"
  },
  {
    "text": "by managing the memory in the GPU better.",
    "start": "501901",
    "end": "505738"
  },
  {
    "text": "And that is how you reduce the amount",
    "start": "506105",
    "end": "508574"
  },
  {
    "text": "of communication per",
    "start": "509275",
    "end": "511310"
  },
  {
    "text": "GPU computation time step.\nAnd you are increasing the amount",
    "start": "511644",
    "end": "515146"
  },
  {
    "text": "of compute while you're keeping\nthat communication constant.",
    "start": "515147",
    "end": "519050"
  },
  {
    "text": "And that is how you can get away\nwith training on Ethernet",
    "start": "519051",
    "end": "523523"
  },
  {
    "text": "while achieving\nsimilar kinds of benefits as InfiniBand.",
    "start": "523923",
    "end": "527560"
  },
  {
    "start": "527000",
    "end": "905000"
  },
  {
    "text": "So Raghu, we saw quite a few things\nabout how PyTorch helps",
    "start": "528027",
    "end": "532097"
  },
  {
    "text": "in scaling up your deep\nlearning workloads and you AI workloads.",
    "start": "532298",
    "end": "535935"
  },
  {
    "text": "We saw FSDP,\nwe saw the rate limiter API.",
    "start": "535935",
    "end": "539605"
  },
  {
    "text": "And they do a pretty good job.",
    "start": "539872",
    "end": "542441"
  },
  {
    "text": "Absolutely.",
    "start": "542875",
    "end": "543609"
  },
  {
    "text": "And we also saw how",
    "start": "543609",
    "end": "545411"
  },
  {
    "text": "interconnects play a role on\nhow do you scale up and so on, right?",
    "start": "545411",
    "end": "549582"
  },
  {
    "text": "But what happens inside a single node\nbetween that CPU and GPU?",
    "start": "549916",
    "end": "554587"
  },
  {
    "text": "Tell me a little bit more about it. ",
    "start": "554620",
    "end": "556722"
  },
  {
    "text": "Yeah, you know,",
    "start": "556722",
    "end": "558290"
  },
  {
    "text": "it's an interesting question\nyou raise because there is",
    "start": "558291",
    "end": "561727"
  },
  {
    "text": "some more efficiency gains that we can\nwe can opt in.",
    "start": "561727",
    "end": "565665"
  },
  {
    "text": "And let's take a look at, you know,\nwhat's what's happening on the board.",
    "start": "566165",
    "end": "569334"
  },
  {
    "text": "So the reason-- one of the reasons --why PyTorch is",
    "start": "570002",
    "end": "572972"
  },
  {
    "text": "so popular\nis because of its programing paradigm",
    "start": "572972",
    "end": "575740"
  },
  {
    "text": "called \"eager mode\".",
    "start": "575975",
    "end": "577977"
  },
  {
    "text": "And eager mode allows developers",
    "start": "577977",
    "end": "581279"
  },
  {
    "text": "to have a very Pythonic approach\nto their programs.",
    "start": "581447",
    "end": "584750"
  },
  {
    "text": "It allows dynamism, like,\nyou know, you can do your if-else blocks.",
    "start": "585284",
    "end": "588554"
  },
  {
    "text": "You're going to have for-loops\nwithin that.",
    "start": "588554",
    "end": "591390"
  },
  {
    "text": "It's very flexible.",
    "start": "591390",
    "end": "592491"
  },
  {
    "text": "Yeah.",
    "start": "592491",
    "end": "592825"
  },
  {
    "text": "And we already spoke about CPU and GPU.",
    "start": "592825",
    "end": "595728"
  },
  {
    "text": "Yeah.",
    "start": "596228",
    "end": "596728"
  },
  {
    "text": "So I'm going to draw a\nsmall little schematic over here,",
    "start": "597063",
    "end": "600498"
  },
  {
    "text": "which sort of illustrates\nwhat's happening in",
    "start": "600733",
    "end": "603535"
  },
  {
    "text": "eager mode.",
    "start": "603869",
    "end": "608941"
  },
  {
    "text": "Let's say this is your CPU queue",
    "start": "608941",
    "end": "611009"
  },
  {
    "text": "and this is your GPU",
    "start": "613980",
    "end": "615214"
  },
  {
    "text": "queue. And queue is essentially",
    "start": "615214",
    "end": "617583"
  },
  {
    "text": "the order of instructions\nthat each chip is launching.",
    "start": "619185",
    "end": "623122"
  },
  {
    "text": "Mm hmm.",
    "start": "623456",
    "end": "624123"
  },
  {
    "text": "Now, taking a step back, your AI programs,",
    "start": "624123",
    "end": "627193"
  },
  {
    "text": "your AI models are essentially\na sequence of operations.",
    "start": "627526",
    "end": "630997"
  },
  {
    "text": "Small AI models, small deep networks\nhave lesser instructions comparatively.",
    "start": "631631",
    "end": "637536"
  },
  {
    "text": "And larger models have many more.",
    "start": "637536",
    "end": "640072"
  },
  {
    "text": "Mm hmm. Mm hmm.",
    "start": "640106",
    "end": "641641"
  },
  {
    "text": "So every line of code that I write in",
    "start": "641641",
    "end": "644043"
  },
  {
    "text": "in my Python program\nbasically translates to going to the GPU.",
    "start": "644076",
    "end": "648446"
  },
  {
    "text": "Is that essentially what's happening? Yes.",
    "start": "648514",
    "end": "651183"
  },
  {
    "text": "Almost every line is an independent instruction",
    "start": "651484",
    "end": "654820"
  },
  {
    "text": "that your CPU\nis deriving from the program.",
    "start": "654954",
    "end": "658490"
  },
  {
    "text": "And so we can probably think\nof your model,",
    "start": "659525",
    "end": "663295"
  },
  {
    "text": "the execution of your model,\nas a sequence of instructions over here.",
    "start": "663629",
    "end": "667533"
  },
  {
    "text": "Mm hmm.",
    "start": "667833",
    "end": "670503"
  },
  {
    "text": "And because you're using\na hardware accelerator like a GPU,",
    "start": "670503",
    "end": "673272"
  },
  {
    "text": "you're going to queue up\nthose instructions",
    "start": "673839",
    "end": "676141"
  },
  {
    "text": "onto the relevant operations\nspecific to the back end that are using.",
    "start": "676275",
    "end": "680146"
  },
  {
    "text": "Okay.",
    "start": "680179",
    "end": "680813"
  },
  {
    "text": "So let's say you have CUDA operations",
    "start": "680813",
    "end": "683816"
  },
  {
    "text": "on an Nvidia GPU.",
    "start": "683816",
    "end": "693125"
  },
  {
    "text": "So this is kind of how eager mode works.",
    "start": "695294",
    "end": "697229"
  },
  {
    "text": "This is the appropriate paradigm\nthat allows you to iterate",
    "start": "697229",
    "end": "700966"
  },
  {
    "text": "or interact with your program\nin a very, you know, 1-to-1 way.",
    "start": "700966",
    "end": "704670"
  },
  {
    "text": "Okay.",
    "start": "704737",
    "end": "705638"
  },
  {
    "text": "So what is happening between,",
    "start": "705771",
    "end": "708240"
  },
  {
    "text": "you know, these times\nwhen-- what happens there?",
    "start": "708240",
    "end": "712244"
  },
  {
    "text": "And that's, you know, if you've hit the",
    "start": "712445",
    "end": "714780"
  },
  {
    "text": "nail on the head,",
    "start": "715915",
    "end": "718183"
  },
  {
    "text": "these empty spaces over here",
    "start": "718984",
    "end": "721654"
  },
  {
    "text": "are actually times\nwhen the GPU is not working.",
    "start": "721654",
    "end": "724256"
  },
  {
    "text": "These are",
    "start": "724924",
    "end": "726192"
  },
  {
    "text": "idle.",
    "start": "726592",
    "end": "727425"
  },
  {
    "text": "And, you know, that's what we do",
    "start": "728861",
    "end": "730029"
  },
  {
    "text": "not like about our GPU is\nbecause they're a very expensive resource.",
    "start": "730029",
    "end": "733833"
  },
  {
    "text": "We don't like them to be idle.",
    "start": "734166",
    "end": "735568"
  },
  {
    "text": "We want them to be working all through,",
    "start": "735568",
    "end": "738270"
  },
  {
    "text": "like you want to maximize the utilization over there.",
    "start": "738270",
    "end": "740906"
  },
  {
    "text": "And these get more and more as you scale.",
    "start": "741607",
    "end": "744243"
  },
  {
    "text": "These are the model size, right? Exactly.",
    "start": "744243",
    "end": "746445"
  },
  {
    "text": "Like as you as your models get larger,\nthe skew starts getting much longer.",
    "start": "746445",
    "end": "750750"
  },
  {
    "text": "And as the queue starts getting longer,\nthe number of these idle spaces",
    "start": "751150",
    "end": "754820"
  },
  {
    "text": "start increasing",
    "start": "755387",
    "end": "756555"
  },
  {
    "text": "quite a lot.",
    "start": "757790",
    "end": "758490"
  },
  {
    "text": "Yeah.",
    "start": "758491",
    "end": "758791"
  },
  {
    "text": "And these essentially translate\nto a lot of costs, unnecessary costs.",
    "start": "758791",
    "end": "762928"
  },
  {
    "text": "You're burning GPU hours without\nactually getting any bang for your buck.",
    "start": "763262",
    "end": "766599"
  },
  {
    "text": "Okay, so how do we address this?",
    "start": "766832",
    "end": "769801"
  },
  {
    "text": "So here we have an interesting tradeoff.",
    "start": "769802",
    "end": "772538"
  },
  {
    "text": "It poses a pretty interesting\nengineering challenge.",
    "start": "772538",
    "end": "774774"
  },
  {
    "text": "We want that you got more, like, flexibility.",
    "start": "775207",
    "end": "777642"
  },
  {
    "text": "It makes programing fun. Mm hmm.",
    "start": "778177",
    "end": "780479"
  },
  {
    "text": "But we also want that efficiency. Mm hmm.",
    "start": "780479",
    "end": "782481"
  },
  {
    "text": "So earlier last last year",
    "start": "782715",
    "end": "785151"
  },
  {
    "text": "at the PyTorch conference,\nwe announced 2.0.",
    "start": "785951",
    "end": "790256"
  },
  {
    "text": "So PyTorch 2.0 packs\nin a very interesting new paradigm.",
    "start": "790256",
    "end": "795326"
  },
  {
    "text": "So it's essentially a new compiler",
    "start": "795728",
    "end": "798330"
  },
  {
    "text": "called TorchDynamo.",
    "start": "798330",
    "end": "800699"
  },
  {
    "text": "And this still allows you",
    "start": "801033",
    "end": "802668"
  },
  {
    "text": "to program your PyTorch code,\nas you always have.",
    "start": "802668",
    "end": "805971"
  },
  {
    "text": "It's still got that flexibility,\nthat interactivity.",
    "start": "806405",
    "end": "809141"
  },
  {
    "text": "But what it does\ndifferently is instead of having",
    "start": "810276",
    "end": "815447"
  },
  {
    "text": "separate instructions",
    "start": "818050",
    "end": "819819"
  },
  {
    "text": "queued up like in eager mode,",
    "start": "819819",
    "end": "822720"
  },
  {
    "text": "your program is essentially converted\nto a graph of operations.",
    "start": "822721",
    "end": "826892"
  },
  {
    "text": "It's almost a static graph.",
    "start": "827293",
    "end": "828794"
  },
  {
    "text": "And so all of these instructions\nare sort of merged together.",
    "start": "828794",
    "end": "831630"
  },
  {
    "text": "Mm hmm.",
    "start": "831664",
    "end": "832231"
  },
  {
    "text": "And they form like it's a 1-to-m.",
    "start": "832898",
    "end": "836902"
  },
  {
    "text": "Mm hmm.",
    "start": "837336",
    "end": "837937"
  },
  {
    "text": "And sometimes you might have graph breaks,\nYou know,",
    "start": "837937",
    "end": "839939"
  },
  {
    "text": "you might want to come back to eager\nmode paradigm.",
    "start": "839939",
    "end": "843409"
  },
  {
    "text": "So your entire program\nhas been translated",
    "start": "843909",
    "end": "854820"
  },
  {
    "text": "to two instructions over here instead of many different instructions.",
    "start": "854820",
    "end": "859925"
  },
  {
    "text": "And on the GPU,",
    "start": "862561",
    "end": "864662"
  },
  {
    "text": "again,\nyou're queuing up only two instructions",
    "start": "865264",
    "end": "868233"
  },
  {
    "text": "instead of all of n instructions\nover here.",
    "start": "868234",
    "end": "870870"
  },
  {
    "text": "Yeah.",
    "start": "871070",
    "end": "873138"
  },
  {
    "text": "And quite often\nyou won't even have this break.",
    "start": "875641",
    "end": "878177"
  },
  {
    "text": "So your entire program, your entire model,\nis this one block of instructions in it",
    "start": "878444",
    "end": "882780"
  },
  {
    "text": "which executes seamlessly on the GPU.",
    "start": "883249",
    "end": "886151"
  },
  {
    "text": "That is pretty cool.",
    "start": "886285",
    "end": "887118"
  },
  {
    "text": "And how do you actually achieve this\nfrom a programing standpoint?",
    "start": "887119",
    "end": "891757"
  },
  {
    "text": "Like what does, say me, as a developer,",
    "start": "891757",
    "end": "894960"
  },
  {
    "text": "what do I need to do to get this?",
    "start": "894960",
    "end": "897363"
  },
  {
    "text": "One line of code.",
    "start": "898931",
    "end": "900266"
  },
  {
    "text": "That's wonderful.",
    "start": "900266",
    "end": "904036"
  },
  {
    "start": "905000",
    "end": "1108000"
  },
  {
    "text": "All of this",
    "start": "906438",
    "end": "906972"
  },
  {
    "text": "goodness\nin one API called torch.compile.",
    "start": "906972",
    "end": "910109"
  },
  {
    "text": "So that's like super easy for me\nas a developer to get up to.",
    "start": "910175",
    "end": "913679"
  },
  {
    "text": "What kind of speed ups\nhave you seen with torch.compile?",
    "start": "913679",
    "end": "916982"
  },
  {
    "text": "Let's say just about training it.",
    "start": "916982",
    "end": "919184"
  },
  {
    "text": "Oh, torch.compile",
    "start": "920052",
    "end": "921854"
  },
  {
    "text": "primarily targets training workloads",
    "start": "921854",
    "end": "923889"
  },
  {
    "text": "and depending on the architecture\nand the model, it varies.",
    "start": "924356",
    "end": "928360"
  },
  {
    "text": "But we have seen speed ups of at least\n40 times. Wow.",
    "start": "928394",
    "end": "931829"
  },
  {
    "text": "So that is an order of magnitude.",
    "start": "932197",
    "end": "934233"
  },
  {
    "text": "Many orders of magnitude of on display\nand eager mode.",
    "start": "934300",
    "end": "938771"
  },
  {
    "text": "Okay, wonderful.",
    "start": "939004",
    "end": "940272"
  },
  {
    "text": "And at IBM, what we have been doing\nis to really work with the community",
    "start": "940272",
    "end": "945177"
  },
  {
    "text": "and to look at torch.compile\nfrom an inference lens.",
    "start": "945177",
    "end": "949214"
  },
  {
    "text": "So what are the beauty of compile?",
    "start": "949481",
    "end": "950950"
  },
  {
    "text": "I feel, is that it\nnot only works for training and giving",
    "start": "950950",
    "end": "954119"
  },
  {
    "text": "that kind of speed up, but\nit actually works for inferencing as well.",
    "start": "954119",
    "end": "958157"
  },
  {
    "text": "The same kind of gaps that you're seeing,\nwe see that being provided to.",
    "start": "958424",
    "end": "961794"
  },
  {
    "text": "Are you saying the same fundamental\nprinciple applies?",
    "start": "961827",
    "end": "964530"
  },
  {
    "text": "I understand,\nbut it's not exclusive to this training.",
    "start": "964897",
    "end": "967498"
  },
  {
    "text": "It's not exclusive to training.",
    "start": "967499",
    "end": "968767"
  },
  {
    "text": "So I think torch.compile\nis the new paradigm that is going to",
    "start": "968767",
    "end": "973005"
  },
  {
    "text": "change efficiency, completely across\nboth training and inferencing.",
    "start": "973005",
    "end": "977910"
  },
  {
    "text": "So fantastic times for PyTorch.",
    "start": "978610",
    "end": "980946"
  },
  {
    "text": "And for the users. Yes, I know.",
    "start": "981580",
    "end": "983449"
  },
  {
    "text": "And what else are we doing\nto get PyTorch to be more",
    "start": "983449",
    "end": "986251"
  },
  {
    "text": "community",
    "start": "987353",
    "end": "987853"
  },
  {
    "text": "oriented, to grow that option of larger\nmodels?",
    "start": "987853",
    "end": "991390"
  },
  {
    "text": "Yeah, I mean, it's no secret\nlike we're living right now",
    "start": "991590",
    "end": "995361"
  },
  {
    "text": "in the age of language\nmodels, large language models which are",
    "start": "995361",
    "end": "998664"
  },
  {
    "text": "just awe inspiring\nand what simple algorithms are able to do.",
    "start": "999832",
    "end": "1003969"
  },
  {
    "text": "Mm hmm.",
    "start": "1004169",
    "end": "1004737"
  },
  {
    "text": "Well, not so simple,\nbut you're probably familiar with Llama.",
    "start": "1004737",
    "end": "1008807"
  },
  {
    "text": "It's a language model from Meta.\nYeah. Yeah.",
    "start": "1008941",
    "end": "1011209"
  },
  {
    "text": "And a lot of that code is open sourced.",
    "start": "1011477",
    "end": "1014446"
  },
  {
    "text": "You know, the model itself, the way\nit's there, available to the community",
    "start": "1014546",
    "end": "1017916"
  },
  {
    "text": "in a very permissive license.",
    "start": "1018217",
    "end": "1019717"
  },
  {
    "text": "Moreover,",
    "start": "1020152",
    "end": "1020753"
  },
  {
    "text": "we also have a lot of scripts available\non how you can be using these downstream.",
    "start": "1020753",
    "end": "1024890"
  },
  {
    "text": "Okay.",
    "start": "1024923",
    "end": "1025824"
  },
  {
    "text": "There is one GitHub repository\nthat you should check out.",
    "start": "1025824",
    "end": "1028360"
  },
  {
    "text": "Okay. It's called llama recipes.",
    "start": "1028427",
    "end": "1030829"
  },
  {
    "text": "So it's under the facebookresearch/llama-recipes on GitHub.",
    "start": "1031296",
    "end": "1035433"
  },
  {
    "text": "Oh, we'll add a link to to that in the video.A",
    "start": "1035868",
    "end": "1037970"
  },
  {
    "text": "And you'll notice",
    "start": "1038570",
    "end": "1040839"
  },
  {
    "text": "quite interestingly\nthat they also use FSDP there too.",
    "start": "1041273",
    "end": "1043776"
  },
  {
    "text": "Okay, especially for fine tuning llama\nyou know.",
    "start": "1043942",
    "end": "1046645"
  },
  {
    "text": "Yeah.",
    "start": "1046678",
    "end": "1047046"
  },
  {
    "text": "You want to adapt all of that knowledge\nand that model",
    "start": "1047046",
    "end": "1049915"
  },
  {
    "text": "to a particular domain\nor to a particular topic of interest.",
    "start": "1049915",
    "end": "1053519"
  },
  {
    "text": "Yeah.\nSo you want to fine tune it.",
    "start": "1053552",
    "end": "1055154"
  },
  {
    "text": "Mm hmm.\nAnd of course, like we already saw how",
    "start": "1055154",
    "end": "1057856"
  },
  {
    "text": "[crosstalk] really works and everything.",
    "start": "1058924",
    "end": "1060459"
  },
  {
    "text": "Yeah. Yeah.",
    "start": "1060459",
    "end": "1060993"
  },
  {
    "text": "And how FSDP helps resolve\na lot of that with an easy to use API.",
    "start": "1060993",
    "end": "1064630"
  },
  {
    "text": "And you can see that in practice",
    "start": "1064963",
    "end": "1066598"
  },
  {
    "text": "at the llama recipes\nrepository. ",
    "start": "1066598",
    "end": "1068901"
  },
  {
    "text": "Wonderful! So I think everything is coming together",
    "start": "1068901",
    "end": "1070636"
  },
  {
    "text": "with Python in the models,\nthe way we train them, tune them,",
    "start": "1070636",
    "end": "1074239"
  },
  {
    "text": "and I'm sure you know, very soon\nit'll be around inferencing tools.",
    "start": "1074573",
    "end": "1077876"
  },
  {
    "text": "So this is fantastic\nnews for the PyTorch community.",
    "start": "1077876",
    "end": "1080212"
  },
  {
    "text": "Yeah, I'm really looking forward\nto what the community comes up with",
    "start": "1080245",
    "end": "1083515"
  },
  {
    "text": "and what they're,\nhow they start using this.",
    "start": "1083782",
    "end": "1085551"
  },
  {
    "text": "Absolutely!",
    "start": "1085551",
    "end": "1086452"
  },
  {
    "text": "Probably had a like\na, probably a new step in this age.",
    "start": "1086452",
    "end": "1090622"
  },
  {
    "text": "Absolutely.",
    "start": "1090656",
    "end": "1091290"
  },
  {
    "text": "Looking forward to that. Thank you, Suraj.",
    "start": "1091290",
    "end": "1093425"
  },
  {
    "text": "Thanks Raghu.",
    "start": "1093425",
    "end": "1093926"
  },
  {
    "text": "Thanks for watching.",
    "start": "1094760",
    "end": "1095661"
  },
  {
    "text": "And if you like the video, please click on\nlike and subscribe to our channel.",
    "start": "1095661",
    "end": "1099465"
  },
  {
    "text": "And if you want to learn more\nabout PyTorch, about the concepts",
    "start": "1100232",
    "end": "1103235"
  },
  {
    "text": "we spoke about in this video,\nand even Llama,",
    "start": "1103235",
    "end": "1106238"
  },
  {
    "text": "do check out the links\nbelow in the video description.",
    "start": "1106238",
    "end": "1108340"
  }
]