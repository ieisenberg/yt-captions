[
  {
    "text": "If you're not getting the responses you want from LLMs,",
    "start": "90",
    "end": "5984"
  },
  {
    "text": "or \"Large Language Models\", like the model that powers chatGPT,",
    "start": "5984",
    "end": "12172"
  },
  {
    "text": "I may know what's wrong.",
    "start": "12172",
    "end": "14136"
  },
  {
    "text": "It might be you.",
    "start": "14830",
    "end": "15830"
  },
  {
    "text": "No, no, no.",
    "start": "16000",
    "end": "16630"
  },
  {
    "text": "Hear me out.",
    "start": "16630",
    "end": "17500"
  },
  {
    "text": "Look, you see, the way that we prompt these large language models is very important.",
    "start": "17500",
    "end": "23520"
  },
  {
    "text": "So prompting plays a significant impact in the quality of the response that the LLM will generate.",
    "start": "25080",
    "end": "30959"
  },
  {
    "text": "Let's take a look at an example.",
    "start": "31590",
    "end": "33269"
  },
  {
    "text": "I'm working on a homework assignment for my Econ 101 class, and I need some help.",
    "start": "33280",
    "end": "38510"
  },
  {
    "text": "So, I issue the following prompt to a large language model.",
    "start": "38696",
    "end": "42434"
  },
  {
    "text": "Question: \"explain the different types of banks.\"",
    "start": "42552",
    "end": "45560"
  },
  {
    "text": "The LLM responds with this,",
    "start": "45561",
    "end": "48371"
  },
  {
    "text": "\"Banks along a river can take various forms depending on whether they are natural or artificial\".",
    "start": "48371",
    "end": "52859"
  },
  {
    "text": "Whoa ho, hang on there!",
    "start": "52890",
    "end": "55439"
  },
  {
    "text": "Here I am trying to understand the difference between a credit union and an investment institution",
    "start": "55830",
    "end": "60744"
  },
  {
    "text": "and it's talking to me about river banks!",
    "start": "60744",
    "end": "62915"
  },
  {
    "text": "This is an example of a particular type of prompt,",
    "start": "63210",
    "end": "67644"
  },
  {
    "text": "and that is called \"zero-shot\" prompting.",
    "start": "67644",
    "end": "73368"
  },
  {
    "text": "You're providing the model with a single question or instruction",
    "start": "73770",
    "end": "77604"
  },
  {
    "text": "without any additional context, examples, or guidance.",
    "start": "77605",
    "end": "80849"
  },
  {
    "text": "The model is expected to understand and answer the prompt without that context,",
    "start": "81180",
    "end": "86136"
  },
  {
    "text": "and to do so, it relies solely on its preexisting knowledge and its ability to generalize from that knowledge",
    "start": "86136",
    "end": "92423"
  },
  {
    "text": "to generate a relevant and accurate response.",
    "start": "92423",
    "end": "94843"
  },
  {
    "text": "And as you can see, it can lead to some suboptimal responses.",
    "start": "94928",
    "end": "99910"
  },
  {
    "text": "Now, \"bank\" is a homograph.",
    "start": "99930",
    "end": "102150"
  },
  {
    "text": "It has multiple meanings.",
    "start": "102150",
    "end": "103919"
  },
  {
    "text": "One method to clear that ambiguity up is to employ a different type of prompting",
    "start": "104370",
    "end": "110775"
  },
  {
    "text": "called \"few-shot\" prompting.",
    "start": "110775",
    "end": "115060"
  },
  {
    "text": "Now, here is an example of few-shot prompting.",
    "start": "115780",
    "end": "118659"
  },
  {
    "text": "So, we've got a question: \"what is the primary function of a bank?\"",
    "start": "119080",
    "end": "122255"
  },
  {
    "text": "Answer: \"A bank's primary function is",
    "start": "122255",
    "end": "124577"
  },
  {
    "text": "to accept deposits, provide loans and offer other financial services to individuals and businesses.\"",
    "start": "124578",
    "end": "128830"
  },
  {
    "text": "Question: \"explain the different types of bank.\"",
    "start": "129070",
    "end": "131627"
  },
  {
    "text": "With few-shot prompting, the model is provided with one or more examples",
    "start": "131627",
    "end": "135785"
  },
  {
    "text": "to help guide its understanding of the task at hand.",
    "start": "135785",
    "end": "138639"
  },
  {
    "text": "By providing an example related to financial institutions,",
    "start": "139060",
    "end": "142509"
  },
  {
    "text": "the LLM is more likely to understand that you are asking about types of bank in the context of finance...",
    "start": "142509",
    "end": "149145"
  },
  {
    "text": "...rather than the stream at the bottom of your garden.",
    "start": "149626",
    "end": "152710"
  },
  {
    "text": "Now, in this example, we could probably just have used a better zero-shot prompt.",
    "start": "152830",
    "end": "157840"
  },
  {
    "text": "Like, \"explain the different types of banking financial institutions.\"",
    "start": "157840",
    "end": "162790"
  },
  {
    "text": "That probably would have worked.",
    "start": "162790",
    "end": "163959"
  },
  {
    "text": "But few-shot prompting has other advantages too.",
    "start": "164050",
    "end": "167889"
  },
  {
    "text": "It can help an LLM understand the expected format a response should take.",
    "start": "168130",
    "end": "173600"
  },
  {
    "text": "Like this, so we've got question: \"create a title from my web page, then a title tag with all of our banks\".",
    "start": "174130",
    "end": "180520"
  },
  {
    "text": "Then, question: \"create a heading for my article.\"",
    "start": "180850",
    "end": "183295"
  },
  {
    "text": "Then we have an H1 title pair with types of banks in those tags,",
    "start": "183295",
    "end": "188134"
  },
  {
    "text": "and then we say, \"question: list the types of banks.\"",
    "start": "188134",
    "end": "190830"
  },
  {
    "text": "And here the LLM may derive \"we are looking for answers in HTML notation\"",
    "start": "190830",
    "end": "196593"
  },
  {
    "text": "and respond accordingly like this.",
    "start": "196593",
    "end": "200198"
  },
  {
    "text": "Now there's another way that few-shot prompting can help, and that is to aid reasoning.",
    "start": "200198",
    "end": "205590"
  },
  {
    "text": "So let's take an example from the paper, \"Large Language Models Are Zero-Shot Reasoners\"",
    "start": "205590",
    "end": "210972"
  },
  {
    "text": "which was written by the University of Tokyo and Google Research.",
    "start": "210973",
    "end": "214975"
  },
  {
    "text": "And they issued this zero-shot prompt to a large language model.",
    "start": "215097",
    "end": "219986"
  },
  {
    "text": "Question: \"a juggler can juggle 16 balls.",
    "start": "220550",
    "end": "223069"
  },
  {
    "text": "Half of the balls are golf balls, and half of the golf balls are blue.",
    "start": "223460",
    "end": "226939"
  },
  {
    "text": "How many blue golf balls are there?\"",
    "start": "227240",
    "end": "229936"
  },
  {
    "text": "The answer is...",
    "start": "230270",
    "end": "232041"
  },
  {
    "text": "now, can you figure this out?",
    "start": "232520",
    "end": "233520"
  },
  {
    "text": "It's not too tricky, but it was for the LLM.",
    "start": "234370",
    "end": "238205"
  },
  {
    "text": "\"8\"",
    "start": "238557",
    "end": "239218"
  },
  {
    "text": "Wrong!",
    "start": "240000",
    "end": "241000"
  },
  {
    "text": "So next in the paper they tried a few-shot prompt.",
    "start": "241120",
    "end": "244728"
  },
  {
    "text": "So we start off with a sample question and a sample completion.",
    "start": "244870",
    "end": "248379"
  },
  {
    "text": "So, question: \"Roger has five tennis balls.",
    "start": "248380",
    "end": "251050"
  },
  {
    "text": "He buys two more cans of tennis balls.",
    "start": "251050",
    "end": "253060"
  },
  {
    "text": "Each can has three tennis balls.",
    "start": "253360",
    "end": "255159"
  },
  {
    "text": "How many tennis balls does he have now?\".",
    "start": "255160",
    "end": "256750"
  },
  {
    "text": "Answer: the answer is 11.",
    "start": "257260",
    "end": "259269"
  },
  {
    "text": "Question: \"a juggler can juggle 16 balls...\" and so forth.",
    "start": "259600",
    "end": "262630"
  },
  {
    "text": "Now we've shown the model of what a right answer looks like by applying addition and multiplication to a sentence.",
    "start": "263380",
    "end": "269460"
  },
  {
    "text": "So, did the a few-shot prompt get us the right answer?",
    "start": "269500",
    "end": "274809"
  },
  {
    "text": "Eight, again!",
    "start": "276180",
    "end": "276990"
  },
  {
    "text": "No!",
    "start": "276990",
    "end": "277990"
  },
  {
    "text": "But by making a slight change, we can improve the reasoning of the LLM and get the right answer.",
    "start": "278040",
    "end": "285899"
  },
  {
    "text": "And we can apply that change to either few-shot prompts, or to zero-shot prompts.",
    "start": "286470",
    "end": "292410"
  },
  {
    "text": "So, what is this mysterious addition?",
    "start": "292500",
    "end": "296490"
  },
  {
    "text": "Well, it's called \"chain of thought\", or CoT,",
    "start": "297000",
    "end": "307086"
  },
  {
    "text": "and to invoke it, just add wording such as,",
    "start": "307086",
    "end": "310319"
  },
  {
    "text": "\"let's think step by step.\"",
    "start": "310319",
    "end": "312354"
  },
  {
    "text": "Effectively, we've asked the LLM to document its thinking.",
    "start": "313620",
    "end": "317790"
  },
  {
    "text": "We're asking to see its chain of thought.",
    "start": "318390",
    "end": "320939"
  },
  {
    "text": "And here's what we get.",
    "start": "321240",
    "end": "322668"
  },
  {
    "text": "So the LLM responds to us with, \"there are 16 balls in total.",
    "start": "322680",
    "end": "327000"
  },
  {
    "text": "Half of the balls are golf balls.",
    "start": "327030",
    "end": "328769"
  },
  {
    "text": "That means there are eight golf balls.",
    "start": "328800",
    "end": "330430"
  },
  {
    "text": "Half of the golf balls are blue.",
    "start": "330450",
    "end": "331959"
  },
  {
    "text": "That means there are four golf balls.\"",
    "start": "331980",
    "end": "334666"
  },
  {
    "text": "Four golf balls!",
    "start": "334666",
    "end": "336659"
  },
  {
    "text": "That is the right answer!",
    "start": "336700",
    "end": "338909"
  },
  {
    "text": "Now, this particular test was applied to the InstructGPT model, which is a couple of years old.",
    "start": "339420",
    "end": "345779"
  },
  {
    "text": "Newer models like GPT-4",
    "start": "346080",
    "end": "348371"
  },
  {
    "text": "can invoke mathematical reasoning without the \"let's think\" step-by-step chain-of-thought prompting.",
    "start": "348372",
    "end": "353789"
  },
  {
    "text": "But chain-of-thought prompting remains a valuable tool in prompt engineering for a number of reasons.",
    "start": "353940",
    "end": "361470"
  },
  {
    "text": "Reason number one is it encourages the model to provide a more detailed,",
    "start": "362190",
    "end": "367260"
  },
  {
    "text": "and specifically, a more transparent response.",
    "start": "367260",
    "end": "372000"
  },
  {
    "text": "And an explanation of that response and its reasoning process.",
    "start": "373230",
    "end": "377490"
  },
  {
    "text": "And this helps users better understand how the model arrived at a particular answer,",
    "start": "377490",
    "end": "382871"
  },
  {
    "text": "making it easier for them to evaluate the correctness and relevance of the response.",
    "start": "382872",
    "end": "386819"
  },
  {
    "text": "And that's all an important part of XAI, or \"Explainable AI\".",
    "start": "387150",
    "end": "393647"
  },
  {
    "text": "And then reason number two for chain-of-thought prompting",
    "start": "394430",
    "end": "397840"
  },
  {
    "text": "is that it can be used to improve the quality of a model's response",
    "start": "397957",
    "end": "402049"
  },
  {
    "text": "by encouraging it to consider alternative perspectives.",
    "start": "402049",
    "end": "408167"
  },
  {
    "text": "Or different approaches.",
    "start": "409550",
    "end": "411349"
  },
  {
    "text": "And by asking the model to think through various possibilities, it can generate more well-rounded and comprehensive answers,",
    "start": "411350",
    "end": "417933"
  },
  {
    "text": "which can be particularly valuable when dealing with open-ended or subjective questions.",
    "start": "418134",
    "end": "422869"
  },
  {
    "text": "Look ultimately few-shot prompting and chain-of-thought prompting",
    "start": "423230",
    "end": "427967"
  },
  {
    "text": "are powerful techniques that can be employed to improve the quality of responses generated by large language models.",
    "start": "427967",
    "end": "433989"
  },
  {
    "text": "By providing the model with additional context, examples or guidance,",
    "start": "434000",
    "end": "439398"
  },
  {
    "text": "users can help the model better understand the task at hand",
    "start": "439398",
    "end": "441940"
  },
  {
    "text": "and generate more accurate, relevant and well-reasoned responses.",
    "start": "441940",
    "end": "447404"
  },
  {
    "text": "And not to mention, keep better track of how many golf balls we're juggling.",
    "start": "447404",
    "end": "453173"
  },
  {
    "text": "If you have any questions, please drop us a line below.",
    "start": "453690",
    "end": "456299"
  },
  {
    "text": "And if you want to see more videos like this in the future, please like and subscribe.",
    "start": "456300",
    "end": "461305"
  },
  {
    "text": "Thanks for watching.",
    "start": "461520",
    "end": "462905"
  }
]