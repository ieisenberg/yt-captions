[
  {
    "text": "Left to their own devices, large language models have a bit of a knowledge problem.",
    "start": "280",
    "end": "4480"
  },
  {
    "text": "If a piece of information wasn't in their training set, they won't be able to recall it.",
    "start": "4800",
    "end": "10480"
  },
  {
    "text": "Maybe something newsworthy that happened after the model completed training,",
    "start": "10820",
    "end": "14335"
  },
  {
    "text": "such as who won the 2025 Best Picture at the Oscars, or it could be something proprietary like a client's purchase history.",
    "start": "14335",
    "end": "22600"
  },
  {
    "text": "So to overcome that knowledge problem, we can use augmented generation techniques.",
    "start": "22860",
    "end": "30253"
  },
  {
    "text": "For example, retrieval.",
    "start": "31320",
    "end": "34560"
  },
  {
    "text": "So retrieval, augmented generation, otherwise known as",
    "start": "35040",
    "end": "41401"
  },
  {
    "text": "RAG",
    "start": "41660",
    "end": "42600"
  },
  {
    "text": "Now how does that work?",
    "start": "42600",
    "end": "44479"
  },
  {
    "text": "Well, essentially we have here a model and the model is going to query an external searchable knowledge base.",
    "start": "44660",
    "end": "55720"
  },
  {
    "text": "Here's where we've got our knowledge and that's going to return portions",
    "start": "56120",
    "end": "59968"
  },
  {
    "text": "of relevant documents to provide additional context.",
    "start": "59969",
    "end": "64400"
  },
  {
    "text": "So we get the documents, we get some context, and we pass that to the LLM model",
    "start": "64879",
    "end": "70979"
  },
  {
    "text": "to update its knowledge, if you like, and that updated context, that's used to generate an answer.",
    "start": "71333",
    "end": "77930"
  },
  {
    "text": "Anora",
    "start": "79035",
    "end": "79689"
  },
  {
    "text": "that won best picture this year, probably got it out of that data set.",
    "start": "81191",
    "end": "84099"
  },
  {
    "text": "But retrieval isn't the only augmented generation game in town.",
    "start": "84860",
    "end": "89760"
  },
  {
    "text": "Another one is cash augmented generation or CAG.",
    "start": "90200",
    "end": "97500"
  },
  {
    "text": "Now CAG is an alternative method.",
    "start": "98200",
    "end": "100758"
  },
  {
    "text": "So rather than querying a knowledge database for answers, the core idea of CAG is to preload the entire knowledge base.",
    "start": "100880",
    "end": "109899"
  },
  {
    "text": "So we take everything we know and we put it all into the context window.",
    "start": "109900",
    "end": "116790"
  },
  {
    "text": "All of it.",
    "start": "117470",
    "end": "118310"
  },
  {
    "text": "The Oscar winners, last week's lunch special at the office cafeteria, whatever you want.",
    "start": "118310",
    "end": "122890"
  },
  {
    "text": "So rather than feeding the model just curated knowledge,",
    "start": "123510",
    "end": "127279"
  },
  {
    "text": "we are feeding the model everything, not just the stuff we deemed relevant to the query.",
    "start": "127279",
    "end": "133509"
  },
  {
    "text": "So RAG versus CAG.",
    "start": "134610",
    "end": "136909"
  },
  {
    "text": "Let's get into how these two things work.",
    "start": "137350",
    "end": "139550"
  },
  {
    "text": "the capabilities of each approach, and an enticing game to test your own knowledge,",
    "start": "140140",
    "end": "145419"
  },
  {
    "text": "and let's start with RAG.",
    "start": "145840",
    "end": "147759"
  },
  {
    "text": "So RAG is essentially a two-phase system.",
    "start": "148240",
    "end": "150699"
  },
  {
    "text": "You've got an offline phase where you ingest and index your knowledge, and then you've got",
    "start": "150760",
    "end": "155081"
  },
  {
    "text": "an online phase where you retrieve and generate on demand.",
    "start": "155081",
    "end": "158720"
  },
  {
    "text": "And the offline part, pretty straightforward.",
    "start": "158740",
    "end": "160800"
  },
  {
    "text": "So you can start with some documents.",
    "start": "161200",
    "end": "162239"
  },
  {
    "text": "So this is your knowledge.",
    "start": "162960",
    "end": "163960"
  },
  {
    "text": "This could be Word files, PDFs, whatever.",
    "start": "164740",
    "end": "168139"
  },
  {
    "text": "and you're going to break them into chunks and create vector embeddings for each",
    "start": "168700",
    "end": "175381"
  },
  {
    "text": "chunk using the help of something called an embedding model.",
    "start": "175381",
    "end": "182399"
  },
  {
    "text": "Now that embedding model is going to create embeddings and it's going to store them",
    "start": "184540",
    "end": "189840"
  },
  {
    "text": "in a database, and specifically this is a vector database where the embeddings are stored.",
    "start": "190421",
    "end": "200009"
  },
  {
    "text": "So you've essentially now created a searchable index of your knowledge.",
    "start": "200050",
    "end": "204889"
  },
  {
    "text": "So when a user prompt comes in from the user, this is where the online phase of this is going to kick in.",
    "start": "205090",
    "end": "213489"
  },
  {
    "text": "So, first thing that's going to happen is we're going to go to a RAG retriever.",
    "start": "213490",
    "end": "219729"
  },
  {
    "text": "and that RAG retriever is gonna take the user's question and it's gonna turn it into",
    "start": "220740",
    "end": "225793"
  },
  {
    "text": "a vector using the same embedding model that we used earlier,",
    "start": "225793",
    "end": "230559"
  },
  {
    "text": "and that's gonna perform a similarity search of your vector database.",
    "start": "230980",
    "end": "235900"
  },
  {
    "text": "Now that's gonna return the top K most relevant document chunks from here that are related to this query.",
    "start": "236480",
    "end": "241939"
  },
  {
    "text": "There might be something like three to five passages that are most likely to contain the answer to the user's query,",
    "start": "242040",
    "end": "249347"
  },
  {
    "text": "and we're gonna take those chunks and we're gonna put them",
    "start": "249980",
    "end": "253780"
  },
  {
    "text": "into the context window of the LLM alongside",
    "start": "253781",
    "end": "260066"
  },
  {
    "text": "the user's initial query and all of this is then gonna get sent to the large language model.",
    "start": "260066",
    "end": "268339"
  },
  {
    "text": "So the model is gonna see the question the user submitted plus",
    "start": "268640",
    "end": "273353"
  },
  {
    "text": "these relevant bits of context and use that to generate an answer.",
    "start": "273353",
    "end": "278879"
  },
  {
    "text": "We're basically saying to the model, here's the question, here's some potentially useful",
    "start": "278930",
    "end": "282932"
  },
  {
    "text": "information to help you answer it that we got out of this vector database, off you go.",
    "start": "282932",
    "end": "287069"
  },
  {
    "text": "And the beauty of RAG is that it's very modular, so you",
    "start": "287710",
    "end": "291785"
  },
  {
    "text": "can swap out the vector database, you could swap out a different embedding model, or you",
    "start": "291785",
    "end": "296287"
  },
  {
    "text": "could change the LLM without rebuilding this entire system.",
    "start": "296287",
    "end": "300930"
  },
  {
    "text": "That's RAG.",
    "start": "301870",
    "end": "302870"
  },
  {
    "text": "What about CAG?",
    "start": "303410",
    "end": "304410"
  },
  {
    "text": "Well, CAG takes a completely different approach.",
    "start": "304890",
    "end": "308040"
  },
  {
    "text": "So instead of retrieving knowledge on demand, you front load it all into the model's context all at once.",
    "start": "308200",
    "end": "315519"
  },
  {
    "text": "So we'll start again with our documents.",
    "start": "315880",
    "end": "318919"
  },
  {
    "text": "This is all of our gathered knowledge.",
    "start": "318960",
    "end": "321499"
  },
  {
    "text": "And we're gonna format them into one massive prompt that fits inside of the model's context window.",
    "start": "321660",
    "end": "330579"
  },
  {
    "text": "So here it's gonna fit into this.",
    "start": "331000",
    "end": "332879"
  },
  {
    "text": "Now, this could be tens or even hundreds of thousands of tokens",
    "start": "332880",
    "end": "338990"
  },
  {
    "text": "and then the large language model is going to take this massive amount of input and it's going to process it.",
    "start": "338990",
    "end": "347569"
  },
  {
    "text": "So effectively this kind of knowledge blob is going to be processed in a single forward pass",
    "start": "347990",
    "end": "352780"
  },
  {
    "text": "and it's going to capture and store the model's internal state after it's digested all of this information.",
    "start": "352780",
    "end": "359509"
  },
  {
    "text": "Now this internal state blob, it's actually got a name, it's called the KV cache, or the key value cache,",
    "start": "359510",
    "end": "369323"
  },
  {
    "text": "and it's created from each self-attention layer and it represents the model's",
    "start": "369323",
    "end": "373603"
  },
  {
    "text": "encoded form of all of your documents, all of your knowledge,",
    "start": "373603",
    "end": "377994"
  },
  {
    "text": "so it's kind of like the models already read your documents and now it's memorized it.",
    "start": "377994",
    "end": "383069"
  },
  {
    "text": "So when a user submits a query in this situation then we take all of this KV cache",
    "start": "383069",
    "end": "391207"
  },
  {
    "text": "and we add the query to it and all of that gets sent into the large language model.",
    "start": "391207",
    "end": "398669"
  },
  {
    "text": "And because the Transformers cache has already got all of the knowledge tokens in it,",
    "start": "399290",
    "end": "403416"
  },
  {
    "text": "the model can use any relevant information as it generates an answer without having to reprocess all of this text again.",
    "start": "403416",
    "end": "411729"
  },
  {
    "text": "So the fundamental distinction between RAG and CAG comes down to when and how knowledge is processed.",
    "start": "412590",
    "end": "419750"
  },
  {
    "text": "With RAG, We say, let's fetch only the stuff that we think we're going to actually need.",
    "start": "419830",
    "end": "425620"
  },
  {
    "text": "CAG, that says let's load everything, all of our documents up front and then remember it for later.",
    "start": "426360",
    "end": "432198"
  },
  {
    "text": "So with RAG, your knowledge base can be really, really large.",
    "start": "432680",
    "end": "436339"
  },
  {
    "text": "This could be millions of documents stored in here because you're only retrieving small pieces at a time.",
    "start": "436440",
    "end": "444160"
  },
  {
    "text": "The model only sees what's relevant for a particular query.",
    "start": "444460",
    "end": "448100"
  },
  {
    "text": "Whereas with CAG you are constrained by the size of the model's context window.",
    "start": "448960",
    "end": "454839"
  },
  {
    "text": "Now a typical model today that can have a context window of something like 32 ,000 to 100 ,000 tokens.",
    "start": "455220",
    "end": "464320"
  },
  {
    "text": "Some are a bit larger than that but that's pretty standard.",
    "start": "464540",
    "end": "467260"
  },
  {
    "text": "It's substantial but it's still finite and everything all of these docs need to fit in that window.",
    "start": "467940",
    "end": "474480"
  },
  {
    "text": "So let's talk about capabilities of each approach and we're going to start with accuracy.",
    "start": "474940",
    "end": "479859"
  },
  {
    "text": "Now, RAG's accuracy is really intrinsically tied to a particular component.",
    "start": "480580",
    "end": "485679"
  },
  {
    "text": "When we talk about accuracy with RAG, we are talking about the retriever.",
    "start": "485840",
    "end": "492518"
  },
  {
    "text": "That's what's important here, because if the retriever fails to fetch a relevant document,",
    "start": "492580",
    "end": "497569"
  },
  {
    "text": "well then the LLM might not have the facts to answer correctly,",
    "start": "497569",
    "end": "501360"
  },
  {
    "text": "but if the retriever works well, then it actually shields the LLM from receiving irrelevant information.",
    "start": "501440",
    "end": "507380"
  },
  {
    "text": "Now, CAG, on the other hand, that preloads all potential relevant information.",
    "start": "508390",
    "end": "512570"
  },
  {
    "text": "So it guarantees that the information is in there somewhere, I mean,",
    "start": "513070",
    "end": "515987"
  },
  {
    "text": "assuming that the knowledge cache actually does contain the question being asked,",
    "start": "515987",
    "end": "519948"
  },
  {
    "text": "but with CAG, all of the work is handed over to the model",
    "start": "519950",
    "end": "527290"
  },
  {
    "text": "to extract the right piece of information from that large context.",
    "start": "527290",
    "end": "531690"
  },
  {
    "text": "So there's the potential here that the LLM might get confused or it might mix in some unrelated information into its answer.",
    "start": "532390",
    "end": "538789"
  },
  {
    "text": "So that's accuracy.",
    "start": "539510",
    "end": "540529"
  },
  {
    "text": "What about latency?",
    "start": "540990",
    "end": "541990"
  },
  {
    "text": "Well, RAG, that introduces an extra step, namely the retrieval into the query workflow and that adds to response time.",
    "start": "542290",
    "end": "551089"
  },
  {
    "text": "So when we look at latency with RAG, it's a bit higher,",
    "start": "551150",
    "end": "555387"
  },
  {
    "text": "because each query incurs the overhead of embedding the query and then",
    "start": "555387",
    "end": "559738"
  },
  {
    "text": "searching the index and then having the LLM process the retrieved text.",
    "start": "559738",
    "end": "564009"
  },
  {
    "text": "But with CAG, once the knowledge is cached, answering a query",
    "start": "564430",
    "end": "568528"
  },
  {
    "text": "is just one forward pass of the LLM on the user prompt plus the generation.",
    "start": "568528",
    "end": "573090"
  },
  {
    "text": "There's no retrieval lookup time.",
    "start": "573390",
    "end": "575470"
  },
  {
    "text": "So when it comes to latency, CAG is going to be lower.",
    "start": "575810",
    "end": "580473"
  },
  {
    "text": "Alright, what about scalability?",
    "start": "580621",
    "end": "583810"
  },
  {
    "text": "Well, RAG can scale to as much as you can fit into your vector database.",
    "start": "583910",
    "end": "589488"
  },
  {
    "text": "So we can have some very large data sets when we are using RAG",
    "start": "589690",
    "end": "596142"
  },
  {
    "text": "And that's because it only pulled a tiny slice of the data per query.",
    "start": "596870",
    "end": "600870"
  },
  {
    "text": "So if you have 10 million documents,",
    "start": "600930",
    "end": "603993"
  },
  {
    "text": "you can index them all and you can still retrieve just a few relevant ones for any single question.",
    "start": "603993",
    "end": "609049"
  },
  {
    "text": "The LLM is never going to see all 10 million documents at once, but CAG, however, that does have a hard limit.",
    "start": "609430",
    "end": "616110"
  },
  {
    "text": "So with CAG, the scalability restriction is basically related to the model context size.",
    "start": "616410",
    "end": "624449"
  },
  {
    "text": "We can only put in there what the model will allow us to fit.",
    "start": "624450",
    "end": "627690"
  },
  {
    "text": "And as I mentioned earlier, that's typically like 32 to 100K tokens.",
    "start": "627790",
    "end": "632009"
  },
  {
    "text": "So that might be a few hundred documents at most.",
    "start": "632570",
    "end": "635889"
  },
  {
    "text": "And even as context windows grow, as they are expected to,",
    "start": "636350",
    "end": "640071"
  },
  {
    "text": "RAG will likely always maintain a bit of an edge when it comes to scalability.",
    "start": "640071",
    "end": "644230"
  },
  {
    "text": "One more, data freshness.",
    "start": "645170",
    "end": "646428"
  },
  {
    "text": "Now, when knowledge changes, RAG, that can just, well, it can just update the index very easily.",
    "start": "646970",
    "end": "654128"
  },
  {
    "text": "So it doesn't take a lot of work to do that.",
    "start": "654130",
    "end": "657349"
  },
  {
    "text": "It can update incrementally as you add new document embeddings or as you remove outdated ones on the fly.",
    "start": "657410",
    "end": "664129"
  },
  {
    "text": "It can always use new information with minimal downtime.",
    "start": "664510",
    "end": "668269"
  },
  {
    "text": "But CAG, on the other hand, that is going to require some re-computation when anything actually changes.",
    "start": "669210",
    "end": "677110"
  },
  {
    "text": "If the data changes frequently, then CAG kind of loses some of its appeal",
    "start": "677530",
    "end": "681836"
  },
  {
    "text": "because you're essentially reloading often, which is going to negate the caching benefit.",
    "start": "681836",
    "end": "686169"
  },
  {
    "text": "All right, so let's play a game.",
    "start": "686730",
    "end": "688070"
  },
  {
    "text": "It's called RAG or CAG.",
    "start": "688190",
    "end": "690389"
  },
  {
    "text": "Now I'm gonna give you a use case and you're gonna shout out",
    "start": "690530",
    "end": "694147"
  },
  {
    "text": "RAG if you think retrieval augmented generation is the best option,",
    "start": "694147",
    "end": "699027"
  },
  {
    "text": "or you'll yell out CAG if you think cache augmented generation is the way to go.",
    "start": "699027",
    "end": "703550"
  },
  {
    "text": "Ready? Alright.",
    "start": "704248",
    "end": "704946"
  },
  {
    "text": "Scenario one, I am building an IT help desk bot.",
    "start": "705710",
    "end": "710429"
  },
  {
    "text": "So users can submit questions and the system's gonna use a product manual to help augment its answers.",
    "start": "711370",
    "end": "717630"
  },
  {
    "text": "Now, the product manual is about 200 pages.",
    "start": "717730",
    "end": "719829"
  },
  {
    "text": "It's only updated a few times a year.",
    "start": "720050",
    "end": "722248"
  },
  {
    "text": "So, RAG or CAG?",
    "start": "722398",
    "end": "725250"
  },
  {
    "text": "Don't be shy.",
    "start": "726070",
    "end": "727070"
  },
  {
    "text": "Getting acronyms at the screen is an entirely normal process.",
    "start": "727410",
    "end": "730610"
  },
  {
    "text": "All right, I'm gonna imagine that most people here are probably saying...",
    "start": "731970",
    "end": "735469"
  },
  {
    "text": "CAG for this one.",
    "start": "737140",
    "end": "738839"
  },
  {
    "text": "The knowledge base, in this case the product manual, it's small enough to fit in most LLM context windows,",
    "start": "739560",
    "end": "745473"
  },
  {
    "text": "the information is pretty static so the caches need to be updated very frequently,",
    "start": "745473",
    "end": "749382"
  },
  {
    "text": "and by caching the information we'll be able to answer queries faster than if we had to query a vector database.",
    "start": "749382",
    "end": "756320"
  },
  {
    "text": "So I think CAG is probably the answer for this one.",
    "start": "756600",
    "end": "759178"
  },
  {
    "text": "What about scenario two?",
    "start": "759980",
    "end": "761079"
  },
  {
    "text": "So with this one you're going to be building a research assistant for a law firm.",
    "start": "761160",
    "end": "766339"
  },
  {
    "text": "Now the system needs to search through thousands of legal cases",
    "start": "766610",
    "end": "770454"
  },
  {
    "text": "that are constantly being updated with new rulings and new amendments.",
    "start": "770454",
    "end": "774128"
  },
  {
    "text": "And when lawyers submit queries, they need answers with accurate citations to relevant legal documents.",
    "start": "774830",
    "end": "780309"
  },
  {
    "text": "So for this one, RAG or CAG.",
    "start": "780730",
    "end": "783050"
  },
  {
    "text": "I think RAG is the way to go here.",
    "start": "784693",
    "end": "789260"
  },
  {
    "text": "The knowledge base in this case, it's massive and it's dynamic with this new content been added all the time.",
    "start": "790260",
    "end": "795940"
  },
  {
    "text": "So attempting to cache all this information would quickly exceed most models context windows",
    "start": "796080",
    "end": "800742"
  },
  {
    "text": "and also that requirement for precise citations to source materials",
    "start": "800742",
    "end": "805328"
  },
  {
    "text": "is actually something that RAG naturally supports through its retrieval mechanism.",
    "start": "805328",
    "end": "809599"
  },
  {
    "text": "It will tell us where it got its information from.",
    "start": "809720",
    "end": "812519"
  },
  {
    "text": "And also the ability to incrementally update the vector database as new legal documents emerge",
    "start": "813290",
    "end": "818518"
  },
  {
    "text": "means that the system always has access to the most current information without requiring full cache recomputation.",
    "start": "818518",
    "end": "824429"
  },
  {
    "text": "So, rag all the way here.",
    "start": "824429",
    "end": "826250"
  },
  {
    "text": "One last one, one last game of RAG or CAG.",
    "start": "827730",
    "end": "830329"
  },
  {
    "text": "So, scenario three, you're building a clinical decision support system for hospitals.",
    "start": "830630",
    "end": "836490"
  },
  {
    "text": "And the idea here is that doctors need to query patient records and treatment guides and drug interactions.",
    "start": "836610",
    "end": "843258"
  },
  {
    "text": "And the responses need to be really comprehensive and of course, very accurate",
    "start": "843630",
    "end": "847602"
  },
  {
    "text": "because they're going to be used by doctors during patient consultations.",
    "start": "847602",
    "end": "852529"
  },
  {
    "text": "And the doctors are often gonna ask complex follow-up questions.",
    "start": "852870",
    "end": "856089"
  },
  {
    "text": "So RAG or CAG for that?",
    "start": "856850",
    "end": "858229"
  },
  {
    "text": "Well, how about...",
    "start": "858950",
    "end": "860590"
  },
  {
    "text": "Both.",
    "start": "861729",
    "end": "862229"
  },
  {
    "text": "Because in this case, the system could first use RAG to retrieve the most relevant subset from the massive knowledge base.",
    "start": "862680",
    "end": "872000"
  },
  {
    "text": "So pulling in specific sections of a particular patient's history and some research papers that are based on the doctor's query.",
    "start": "872214",
    "end": "879498"
  },
  {
    "text": "And then instead of simply passing those retrieved chunks to the LLM,",
    "start": "879800",
    "end": "883556"
  },
  {
    "text": "it could load all that retrieved content into a long context model that uses CAG,",
    "start": "883556",
    "end": "891082"
  },
  {
    "text": "creating a temporary working memory, if you like, for the specific patient case.",
    "start": "891082",
    "end": "896228"
  },
  {
    "text": "So it's really a hybrid approach.",
    "start": "896390",
    "end": "898690"
  },
  {
    "text": "RAG's ability to efficiently search enormous knowledge bases,",
    "start": "899064",
    "end": "902953"
  },
  {
    "text": "and then CAG's capability for providing the full breadth of medical knowledge when needed for those follow-up questions",
    "start": "902953",
    "end": "909153"
  },
  {
    "text": "without the system repeatedly querying the database.",
    "start": "909153",
    "end": "912090"
  },
  {
    "text": "So essentially, RAG and CAG are two strategies for enhancing LLMs with external knowledge,",
    "start": "912664",
    "end": "919066"
  },
  {
    "text": "and you'd consider RAG when your knowledge source is very large, or it's frequently updated, or you need citations,",
    "start": "919066",
    "end": "926056"
  },
  {
    "text": "or where resources for running long context window models are a bit limited,",
    "start": "926056",
    "end": "929946"
  },
  {
    "text": "but you would consider CAG when you have a fixed set of knowledge that",
    "start": "929946",
    "end": "934076"
  },
  {
    "text": "can fit within the context window of the model you're using,",
    "start": "934076",
    "end": "937354"
  },
  {
    "text": "where latency is important, it needs to be fast, and where you want to simplify deployment.",
    "start": "937354",
    "end": "943470"
  },
  {
    "text": "RAG or CAG, the choice is up to you.",
    "start": "944290",
    "end": "947649"
  }
]