[
  {
    "start": "0",
    "end": "66000"
  },
  {
    "text": "A word on word embeddings.",
    "start": "200",
    "end": "3203"
  },
  {
    "text": "Maybe a few words.",
    "start": "3837",
    "end": "5138"
  },
  {
    "text": "Word embeddings\nrepresent words as numbers,",
    "start": "5138",
    "end": "8249"
  },
  {
    "text": "specifically as numeric vectors,",
    "start": "8249",
    "end": "12205"
  },
  {
    "text": "in a way that captures the semantic relationships\nand contextual information.",
    "start": "12639",
    "end": "17851"
  },
  {
    "text": "So that means words with similar\nmeanings are positioned",
    "start": "17851",
    "end": "20887"
  },
  {
    "text": "close to each other, and the distance\nand direction between vectors",
    "start": "20887",
    "end": "24623"
  },
  {
    "text": "encode the degree of similarity\nbetween words.",
    "start": "24824",
    "end": "28786"
  },
  {
    "text": "But why do we need to transform words\ninto numbers?",
    "start": "29015",
    "end": "33553"
  },
  {
    "text": "The reason vectors are used to represent words is that",
    "start": "33800",
    "end": "36958"
  },
  {
    "text": "most machine learning algorithms are just incapable of processing",
    "start": "36958",
    "end": "40269"
  },
  {
    "text": "plain text in its raw form.",
    "start": "40269",
    "end": "42300"
  },
  {
    "text": "They require numbers as input to perform any task,",
    "start": "42475",
    "end": "46319"
  },
  {
    "text": "and that's where word embeddings come in.",
    "start": "46319",
    "end": "48415"
  },
  {
    "text": "So let's take a look at how word embeddings are used\nand the model is used to create them.",
    "start": "48448",
    "end": "53837"
  },
  {
    "text": "And let's start with a\nlook at some applications.",
    "start": "54054",
    "end": "55922"
  },
  {
    "text": "Now what embeddings have become\na fundamental tool",
    "start": "55922",
    "end": "58892"
  },
  {
    "text": "in the world of NLP.",
    "start": "59097",
    "end": "62100"
  },
  {
    "text": "That's natural language processing.",
    "start": "62390",
    "end": "65693"
  },
  {
    "start": "66000",
    "end": "172000"
  },
  {
    "text": "Natural language processing helps machines understand\nhuman language.",
    "start": "66674",
    "end": "70545"
  },
  {
    "text": "Word embeddings are used in\nvarious NLP tasks, so for example,",
    "start": "70870",
    "end": "74774"
  },
  {
    "text": "you'll find them used in text classification very frequently.",
    "start": "75041",
    "end": "79687"
  },
  {
    "text": "Now in text classification, word embeddings are often used in tasks such as",
    "start": "79687",
    "end": "84215"
  },
  {
    "text": "spam detection and topic categorization.",
    "start": "84215",
    "end": "87452"
  },
  {
    "text": "Another common task is NER,",
    "start": "87587",
    "end": "91590"
  },
  {
    "text": "that's an acronym for named entity recognition,",
    "start": "91591",
    "end": "94594"
  },
  {
    "text": "and there is used to identify\nand classify entities in text.",
    "start": "95261",
    "end": "99065"
  },
  {
    "text": "And an entity is like a name of a person\nor a place or an organization.",
    "start": "99065",
    "end": "103203"
  },
  {
    "text": "Now, word embeddings can also help with tasks",
    "start": "103870",
    "end": "107240"
  },
  {
    "text": "related to word similarity and word analogy tasks.",
    "start": "107240",
    "end": "113279"
  },
  {
    "text": "So, for example, the king is the queen\nas man is to a woman,",
    "start": "113279",
    "end": "117750"
  },
  {
    "text": "and then another example is in Q&A.",
    "start": "118384",
    "end": "121454"
  },
  {
    "text": "So question and answering systems",
    "start": "121488",
    "end": "124699"
  },
  {
    "text": "they can benefit from word embeddings for measuring semantic",
    "start": "124699",
    "end": "128651"
  },
  {
    "text": "similarities between words or documents for tasks like",
    "start": "128652",
    "end": "131777"
  },
  {
    "text": "clustering related articles, or finding similar documents, or",
    "start": "131777",
    "end": "135465"
  },
  {
    "text": "recommending similar items.",
    "start": "135466",
    "end": "137569"
  },
  {
    "text": "Now, word embeddings are created by\ntrained models on a large corpus of text.",
    "start": "137804",
    "end": "143009"
  },
  {
    "text": "So maybe, like all of Wikipedia,",
    "start": "143009",
    "end": "145621"
  },
  {
    "text": "the process begins with preprocessing the text,",
    "start": "145621",
    "end": "147945"
  },
  {
    "text": "including tokenization and removing stopwords and punctuation.",
    "start": "147945",
    "end": "150717"
  },
  {
    "text": "A sliding context window identifies target and context words,",
    "start": "151251",
    "end": "154754"
  },
  {
    "text": "allowing the model to learn word relationships.",
    "start": "154954",
    "end": "157790"
  },
  {
    "text": "Then the model is trained to predict\nbased on their context.",
    "start": "157790",
    "end": "160926"
  },
  {
    "text": "Positioning semantically similar words close together",
    "start": "160927",
    "end": "163459"
  },
  {
    "text": "in the vector space and throughout the training,",
    "start": "163459",
    "end": "166266"
  },
  {
    "text": "the model parameters are adjusted\nto minimize prediction errors.",
    "start": "166566",
    "end": "170202"
  },
  {
    "text": "So what does this look like?",
    "start": "170503",
    "end": "173306"
  },
  {
    "text": "Well, let's start with a super small\ncorpus of just six words.",
    "start": "173306",
    "end": "178111"
  },
  {
    "text": "Here they are.",
    "start": "178678",
    "end": "180079"
  },
  {
    "text": "Now we'll represent each word\nas a three dimensional vector.",
    "start": "180079",
    "end": "184317"
  },
  {
    "text": "So each dimension",
    "start": "185485",
    "end": "187153"
  },
  {
    "text": "has a numeric value\ncreating a unique vector for each word.",
    "start": "187153",
    "end": "191191"
  },
  {
    "text": "And these values\nrepresent the word's position",
    "start": "191457",
    "end": "193593"
  },
  {
    "text": "in a continuous three dimensional vector space.",
    "start": "193593",
    "end": "195962"
  },
  {
    "text": "And if you look closely, you can see that words with similar",
    "start": "195962",
    "end": "200233"
  },
  {
    "text": "meanings or contexts\nhave similar vector representations.",
    "start": "200233",
    "end": "204270"
  },
  {
    "text": "So, for instance, the vectors for apple and for orange",
    "start": "204437",
    "end": "208174"
  },
  {
    "text": "are close together,\nreflecting this semantic relationship.",
    "start": "208441",
    "end": "211711"
  },
  {
    "text": "Likewise, the vectors for happy\nand sad have opposite directions,",
    "start": "212145",
    "end": "216116"
  },
  {
    "text": "indicating their contrasting meanings.",
    "start": "216116",
    "end": "218318"
  },
  {
    "text": "Now, of course, in real life, it's\nnot this simple.",
    "start": "218685",
    "end": "221987"
  },
  {
    "text": "A corpus of six words isn't going to be too helpful in practice,",
    "start": "222088",
    "end": "225458"
  },
  {
    "text": "and actual word embeddings typically have\nhundreds of dimensions, not just three.",
    "start": "225758",
    "end": "229896"
  },
  {
    "text": "To capture more intricate relationships\nand nuances in meaning.",
    "start": "230163",
    "end": "234032"
  },
  {
    "start": "234000",
    "end": "360000"
  },
  {
    "text": "Now, there are two fundamental approaches\nto how word",
    "start": "234701",
    "end": "238071"
  },
  {
    "text": "embedding methods generate\neffective representations for words.",
    "start": "238071",
    "end": "242075"
  },
  {
    "text": "So let's take a look\nat some of these embedding methods.",
    "start": "242308",
    "end": "246045"
  },
  {
    "text": "And we'll start with the first one\nwhich is frequency.",
    "start": "246512",
    "end": "252351"
  },
  {
    "text": "So frequency based embeddings.",
    "start": "252552",
    "end": "255922"
  },
  {
    "text": "Now frequency based embeddings are word",
    "start": "256923",
    "end": "259792"
  },
  {
    "text": "representations that are derived\nfrom the frequency of words in a corpus.",
    "start": "259792",
    "end": "263695"
  },
  {
    "text": "They're based on the idea that the\nimportance or the significance of a word",
    "start": "264197",
    "end": "267466"
  },
  {
    "text": "can be inferred from how frequently it occurs in the text.",
    "start": "267467",
    "end": "271314"
  },
  {
    "text": "Now, one such embedding of frequency\nbased is called TF-IDF",
    "start": "271314",
    "end": "278617"
  },
  {
    "text": "that stands for Term Frequency Inverse Document Frequency.",
    "start": "279051",
    "end": "283716"
  },
  {
    "text": "TF-IDF highlights words that are frequent",
    "start": "283716",
    "end": "287220"
  },
  {
    "text": "within a specific document,\nbut are rare across the entire corpus.",
    "start": "287220",
    "end": "291758"
  },
  {
    "text": "So, for example, in a document\nabout coffee, TF-IDF would emphasize words",
    "start": "291791",
    "end": "296896"
  },
  {
    "text": "like espresso or cappuccino,\nwhich might appear often",
    "start": "296896",
    "end": "300733"
  },
  {
    "text": "in that document, but rarely in others\nabout different topics.",
    "start": "300733",
    "end": "304270"
  },
  {
    "text": "Common words like the or and, which appear\nfrequently across all documents,",
    "start": "304604",
    "end": "309642"
  },
  {
    "text": "would receive low TF-IDF based scores.",
    "start": "309809",
    "end": "313646"
  },
  {
    "text": "Now another embedding type",
    "start": "314614",
    "end": "317183"
  },
  {
    "text": "is called prediction based embeddings",
    "start": "317183",
    "end": "321054"
  },
  {
    "text": "and prediction based embeddings.",
    "start": "321721",
    "end": "324390"
  },
  {
    "text": "They capture semantic relationships\nand contextual information between words.",
    "start": "324390",
    "end": "329629"
  },
  {
    "text": "So, for example, in the sentences,\n\"the dog is barking loudly.\" and \"the dog is wagging its tail.\"",
    "start": "329762",
    "end": "336602"
  },
  {
    "text": "A prediction based model would learn to associate dog",
    "start": "336936",
    "end": "340540"
  },
  {
    "text": "with words like bark, wag, and tail.",
    "start": "340740",
    "end": "343742"
  },
  {
    "text": "This allows these models to create a single fixed representation",
    "start": "343843",
    "end": "347146"
  },
  {
    "text": "for dog that encompasses various, well, dog related concepts.",
    "start": "347146",
    "end": "351217"
  },
  {
    "text": "Prediction based embeddings.",
    "start": "351851",
    "end": "353353"
  },
  {
    "text": "They excel at separating words\nwith close meanings,",
    "start": "353353",
    "end": "356422"
  },
  {
    "text": "and can manage the various senses\nin which a word may be used.",
    "start": "356422",
    "end": "359826"
  },
  {
    "start": "360000",
    "end": "517000"
  },
  {
    "text": "Now there are various models for generating word embeddings.",
    "start": "360660",
    "end": "364630"
  },
  {
    "text": "One of the most popular\nis called word2vec",
    "start": "365131",
    "end": "370603"
  },
  {
    "text": "that was developed by Google in 2013.",
    "start": "370870",
    "end": "373873"
  },
  {
    "text": "Now word2vec\nhas two main architectures.",
    "start": "373906",
    "end": "377210"
  },
  {
    "text": "There's something called c b, o, w",
    "start": "377543",
    "end": "380813"
  },
  {
    "text": "and there's something called skip-gram,",
    "start": "381180",
    "end": "384805"
  },
  {
    "text": "and CBOW, that's an acronym\nfor Continuous Bag of Words.",
    "start": "384805",
    "end": "389055"
  },
  {
    "text": "Now CBOW predicts a target word\nbased on its surrounding context words.",
    "start": "389355",
    "end": "394627"
  },
  {
    "text": "Well, skip-gram does the opposite,",
    "start": "394627",
    "end": "396763"
  },
  {
    "text": "predicting the context words\ngiven a target word.",
    "start": "396763",
    "end": "399766"
  },
  {
    "text": "Now another popular method is called\nGLOVE.",
    "start": "399899",
    "end": "403336"
  },
  {
    "text": "Also an acronym, that one stands for Global Vectors for Word Representation.",
    "start": "403636",
    "end": "409100"
  },
  {
    "text": "That was created at Stanford University in 2014",
    "start": "409101",
    "end": "412044"
  },
  {
    "text": "and that uses co-occurrence statistics\nto create word vectors.",
    "start": "412378",
    "end": "416149"
  },
  {
    "text": "Now, these models, they differ in their approach.",
    "start": "416582",
    "end": "419085"
  },
  {
    "text": "What's a vec that focuses on learning from\nthe immediate context around each word?",
    "start": "419085",
    "end": "423589"
  },
  {
    "text": "While glove takes a broader view\nby analyzing",
    "start": "423856",
    "end": "426392"
  },
  {
    "text": "how often words appear together\nacross the entire corpus,",
    "start": "426392",
    "end": "430129"
  },
  {
    "text": "then uses this information\nto create word vectors.",
    "start": "430329",
    "end": "433332"
  },
  {
    "text": "Now, while these two word embedding models\ncontinue to be valuable tools in NLP,",
    "start": "433633",
    "end": "438404"
  },
  {
    "text": "the field has seen some\nsignificant advances with the emergence",
    "start": "439005",
    "end": "441941"
  },
  {
    "text": "of new tech, particularly transformers.",
    "start": "441941",
    "end": "444944"
  },
  {
    "text": "While traditional word embeddings\nassign a fixed vector to each word,",
    "start": "445011",
    "end": "448981"
  },
  {
    "text": "transformer models use a different type of embedding,",
    "start": "449315",
    "end": "453119"
  },
  {
    "text": "and it's called a contextual based embedding.",
    "start": "453352",
    "end": "456956"
  },
  {
    "text": "Now, contextual based embeddings are where",
    "start": "457590",
    "end": "460827"
  },
  {
    "text": "the representation of a word changes based on its surrounding context.",
    "start": "460827",
    "end": "465164"
  },
  {
    "text": "So, for example, in a transformer model,",
    "start": "465198",
    "end": "467166"
  },
  {
    "text": "the word bank would have different\nrepresentations in the sentence",
    "start": "467166",
    "end": "470702"
  },
  {
    "text": "I'm going to the bank to deposit money\nand I'm sitting on the bank of a river.",
    "start": "470970",
    "end": "475842"
  },
  {
    "text": "This context sensitivity\nallows these models to capture",
    "start": "476576",
    "end": "479311"
  },
  {
    "text": "more nuanced meanings and relationships\nbetween words, which has led to all sorts",
    "start": "479312",
    "end": "483816"
  },
  {
    "text": "of improvements in the various fields of NLP tasks.",
    "start": "483816",
    "end": "487453"
  },
  {
    "text": "So that's word embeddings,",
    "start": "488120",
    "end": "490204"
  },
  {
    "text": "from simple numeric vectors to complex representations.",
    "start": "490204",
    "end": "494794"
  },
  {
    "text": "Word embeddings have revolutionized how machines understand and process human language.",
    "start": "495127",
    "end": "500466"
  },
  {
    "text": "Proving that transforming words\ninto numbers is indeed a powerful tool",
    "start": "500466",
    "end": "505004"
  },
  {
    "text": "for making sense of our linguistic world.",
    "start": "505171",
    "end": "508039"
  }
]