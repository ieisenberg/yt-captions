[
  {
    "text": "This is how you can extract content \nfrom client contracts using Gen AI.",
    "start": "880",
    "end": "6160"
  },
  {
    "text": "Contracts can be long.",
    "start": "6160",
    "end": "7880"
  },
  {
    "text": "Many are 50 pages or more.",
    "start": "7880",
    "end": "10360"
  },
  {
    "text": "Instead of scrolling through and reading these long documents,",
    "start": "10360",
    "end": "13959"
  },
  {
    "text": "what if we could interact with  them in a conversational manner",
    "start": "13960",
    "end": "16840"
  },
  {
    "text": "and only extract the information that we need?",
    "start": "16840",
    "end": "20280"
  },
  {
    "text": "We're going to use LLM's to extract the data we want",
    "start": "20280",
    "end": "24120"
  },
  {
    "text": "and create a new summary document in less than 20 minutes.",
    "start": "24120",
    "end": "27733"
  },
  {
    "text": "The two LLM's are going to use our Granite.13b.chat",
    "start": "29040",
    "end": "33000"
  },
  {
    "text": "to extract the text and Mistral Large to transform the output",
    "start": "33000",
    "end": "37239"
  },
  {
    "text": "into an easy to read table.",
    "start": "37240",
    "end": "39440"
  },
  {
    "text": "Let's get started.",
    "start": "39440",
    "end": "41800"
  },
  {
    "text": "Here's the contract in the public directory in our application.",
    "start": "41800",
    "end": "46000"
  },
  {
    "text": "The first thing we need to do is import our dependencies.",
    "start": "46000",
    "end": "49207"
  },
  {
    "text": "We'll be using link chain and link chain community.",
    "start": "49880",
    "end": "54309"
  },
  {
    "text": "For this project, they'll help us load and process our contract",
    "start": "55440",
    "end": "59199"
  },
  {
    "text": "with the help of PDF Parse.",
    "start": "59200",
    "end": "61193"
  },
  {
    "text": "To interact with our LLM's will be using watsonx,",
    "start": "64520",
    "end": "67399"
  },
  {
    "text": "which doesn't need to be imported here.",
    "start": "67400",
    "end": "70160"
  },
  {
    "text": "We'll use dotenv for importing end variables.",
    "start": "70160",
    "end": "75243"
  },
  {
    "text": "An axios HTTP requests.",
    "start": "77600",
    "end": "80762"
  },
  {
    "text": "And lastly, we can use the built in Node.js file system module",
    "start": "83200",
    "end": "87079"
  },
  {
    "text": "for adding our data to a new file.",
    "start": "87080",
    "end": "89920"
  },
  {
    "text": "Before we start coding, we need to have our credentials defined in our M file.",
    "start": "93600",
    "end": "99479"
  },
  {
    "text": "The credentials we'll need include an IBM Cloud API key,",
    "start": "99480",
    "end": "103800"
  },
  {
    "text": "our wastonx project ID and the watsonx.ai API URL.",
    "start": "103800",
    "end": "110152"
  },
  {
    "text": "For ease, we'll be implementing the solution all in one large function.",
    "start": "113560",
    "end": "118040"
  },
  {
    "text": "So let's define and export that function here.",
    "start": "118040",
    "end": "121219"
  },
  {
    "text": "Next, we'll need to reach out to watsonx to grab a bear token",
    "start": "124600",
    "end": "128791"
  },
  {
    "text": "which will pass to our two API calls.",
    "start": "128791",
    "end": "131108"
  },
  {
    "text": "We can save the response as a variable to use later.",
    "start": "140680",
    "end": "144146"
  },
  {
    "text": "Now let's load our contract so we can interact with it.",
    "start": "149440",
    "end": "153247"
  },
  {
    "text": "We'll instantiate the PDF loader class",
    "start": "155200",
    "end": "157440"
  },
  {
    "text": "to load and process our PDF document.",
    "start": "157440",
    "end": "160822"
  },
  {
    "text": "Now we get to the LLM's.",
    "start": "166600",
    "end": "169880"
  },
  {
    "text": "Our first prompt will extract and transform",
    "start": "169880",
    "end": "172120"
  },
  {
    "text": "key pieces of information from our contract.",
    "start": "172120",
    "end": "175720"
  },
  {
    "text": "For this, we're going to use Granite.13b.chat,",
    "start": "175720",
    "end": "180320"
  },
  {
    "text": "and we're going to tell it to extract the title, the name,",
    "start": "180320",
    "end": "184240"
  },
  {
    "text": "the services, effective date,",
    "start": "184240",
    "end": "186480"
  },
  {
    "text": "and the compensation from the contract.",
    "start": "186480",
    "end": "188790"
  },
  {
    "text": "Then we want the LLM to transform this data into JSON format.",
    "start": "191080",
    "end": "196560"
  },
  {
    "text": "And we'll set the output to a variable that we can use later.",
    "start": "196560",
    "end": "200872"
  },
  {
    "text": "Sometimes models respond with extra characters",
    "start": "204320",
    "end": "206520"
  },
  {
    "text": "in the output that we may not need.",
    "start": "206520",
    "end": "209760"
  },
  {
    "text": "This output contains back ticks, so let's go ahead",
    "start": "210320",
    "end": "212560"
  },
  {
    "text": "and remove any back ticks from the response output.",
    "start": "212560",
    "end": "216089"
  },
  {
    "text": "Okay, that output looks good.",
    "start": "220224",
    "end": "222984"
  },
  {
    "text": "So now we're going to set up our second LLM.",
    "start": "223240",
    "end": "226640"
  },
  {
    "text": "And this time we're going to be working",
    "start": "226640",
    "end": "227960"
  },
  {
    "text": "with the Mistral large model.",
    "start": "227960",
    "end": "232600"
  },
  {
    "text": "I'm going to copy the first LLM since we're going to be using",
    "start": "232600",
    "end": "235320"
  },
  {
    "text": "the same credentials and parameters",
    "start": "235320",
    "end": "236920"
  },
  {
    "text": "that we used in our first model call.",
    "start": "236920",
    "end": "240200"
  },
  {
    "text": "For our next prompt.",
    "start": "240200",
    "end": "243160"
  },
  {
    "text": "Let's tell it to refer to the previous output",
    "start": "243160",
    "end": "246080"
  },
  {
    "text": "and then create a simple table.",
    "start": "246080",
    "end": "247854"
  },
  {
    "text": "And we're going to save that to a variable as well.",
    "start": "251930",
    "end": "253752"
  },
  {
    "text": "Okay, so once we have the responses back from our LLM,",
    "start": "255755",
    "end": "259475"
  },
  {
    "text": "we're ready to write the file to a local file",
    "start": "259680",
    "end": "265080"
  },
  {
    "text": "using the fs.writeFile method.",
    "start": "265080",
    "end": "269720"
  },
  {
    "text": "So let's tell it to create a .md file.",
    "start": "270795",
    "end": "274868"
  },
  {
    "text": "We'll put in our callback.",
    "start": "276980",
    "end": "279000"
  },
  {
    "text": "Okay. Now let's check the new summarize file.",
    "start": "281498",
    "end": "284332"
  },
  {
    "text": "Okay, that looks great.",
    "start": "294280",
    "end": "295433"
  },
  {
    "text": "Congratulations.",
    "start": "296920",
    "end": "298600"
  },
  {
    "text": "You've successfully built a \ncontent extraction solution.",
    "start": "298600",
    "end": "302920"
  }
]