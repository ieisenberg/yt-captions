[
  {
    "text": "By now, you've probably tried out some really helpful AI models",
    "start": "370",
    "end": "3995"
  },
  {
    "text": "to summarize your data, to act as a pair programmer, and much more.",
    "start": "3995",
    "end": "8149"
  },
  {
    "text": "But traditionally, this meant using cloud services.",
    "start": "8590",
    "end": "11589"
  },
  {
    "text": "Perhaps you were using an LLM through some type of chatbot, or for me, an API that was hosted on a cloud service.",
    "start": "11710",
    "end": "19029"
  },
  {
    "text": "But at the end of the day, I was using someone else's cloud computing resources.",
    "start": "19130",
    "end": "23209"
  },
  {
    "text": "Now, what if I told you, though, that there's open source way.",
    "start": "23910",
    "end": "26890"
  },
  {
    "text": "to run AI models and LLMs locally from your own machine.",
    "start": "27320",
    "end": "31939"
  },
  {
    "text": "And that allows you to save on cost for your AI bills,",
    "start": "32520",
    "end": "36063"
  },
  {
    "text": "to keep your data private and as a developer to build out applications and features that use AI from your on machine.",
    "start": "36063",
    "end": "43880"
  },
  {
    "text": "Now that's right, today we're gonna be taking a look at Ollama to run large language models or LLMs from your old machine.",
    "start": "44480",
    "end": "52359"
  },
  {
    "text": "Now, this is really, really cool because you're able to take a model",
    "start": "53020",
    "end": "56746"
  },
  {
    "text": "which might be quantized or compressed and run it from your own system resources",
    "start": "56746",
    "end": "62337"
  },
  {
    "text": "and integrate with a huge ecosystem of models from Llama to Mistral and beyond.",
    "start": "62338",
    "end": "67299"
  },
  {
    "text": "And for organizations that are looking to use AI in their applications, it can be very helpful",
    "start": "67760",
    "end": "72470"
  },
  {
    "text": "because we're able take and deploy a small or large language model",
    "start": "72470",
    "end": "77349"
  },
  {
    "text": "locally and ensure that customer data doesn't leave the secure environment at all.",
    "start": "77349",
    "end": "81819"
  },
  {
    "text": "And this is all using a simple command that we're going to take a look at here in a second.",
    "start": "82140",
    "end": "86899"
  },
  {
    "text": "But how does it work and what should you know about using Olamma?",
    "start": "87300",
    "end": "90680"
  },
  {
    "text": "Well let's begin with the Olammas CLI.",
    "start": "91020",
    "end": "93578"
  },
  {
    "text": "Now whether you're on Windows, Mac or Linux,",
    "start": "93920",
    "end": "97858"
  },
  {
    "text": "you can head over to ollamma.com in order to download the CLI or command line interface for your machine.",
    "start": "97858",
    "end": "105519"
  },
  {
    "text": "Now this allows you to download models, to run them, and to interact with them all from your own terminal.",
    "start": "106040",
    "end": "111540"
  },
  {
    "text": "While in the past you had to go over to repositories such as Hugging Face",
    "start": "111980",
    "end": "116113"
  },
  {
    "text": "in order to download model weights,",
    "start": "116113",
    "end": "117896"
  },
  {
    "text": "and you had work with complicated setup in order to get the model ready to be inferenced and chatted with,",
    "start": "117897",
    "end": "123693"
  },
  {
    "text": "this is all simplified with Olamma through a single command, Olammarrun.",
    "start": "123693",
    "end": "130039"
  },
  {
    "text": "After that, we'll pass in the argument or parameter of the model name.",
    "start": "130580",
    "end": "134479"
  },
  {
    "text": "It could be granite, could be llama, could be deep seek, and that'll kick off the process.",
    "start": "134700",
    "end": "139020"
  },
  {
    "text": "to download one of the Olamas own models,",
    "start": "139300",
    "end": "142081"
  },
  {
    "text": "their compressed and optimized models to your machine, and start up an inference server,",
    "start": "142081",
    "end": "146916"
  },
  {
    "text": "similar to how you would start up a web server and serve your web pages.",
    "start": "146916",
    "end": "150560"
  },
  {
    "text": "This will drop you into a GPT looking chat window,",
    "start": "151140",
    "end": "154407"
  },
  {
    "text": "and you can almost think of the olama run command as a package manager for AI,",
    "start": "154407",
    "end": "159201"
  },
  {
    "text": "allowing you to run and manage models with a single command.",
    "start": "159201",
    "end": "162759"
  },
  {
    "text": "Now, that's awesome, but speaking of models, Thanks for watching!",
    "start": "163300",
    "end": "165860"
  },
  {
    "text": "Olama has a catalog of standardized and customizable language, multi-model embedding and tool calling models.",
    "start": "166280",
    "end": "173259"
  },
  {
    "text": "Now, I just said a lot of things, but let's break it down here.",
    "start": "173360",
    "end": "176760"
  },
  {
    "text": "The first one that I wanted to mention is a type of model for language.",
    "start": "177120",
    "end": "181058"
  },
  {
    "text": "So for example, that means working with your text and data,",
    "start": "181420",
    "end": "184446"
  },
  {
    "text": "either in a conversational or base type of format or an instructional format for answering questions and answers.",
    "start": "184446",
    "end": "191459"
  },
  {
    "text": "Now, the second type is multi-modal models.",
    "start": "191960",
    "end": "194699"
  },
  {
    "text": "So for example.",
    "start": "194700",
    "end": "195700"
  },
  {
    "text": "working with images and being able to analyze, hey, what's going on in this specific frame?",
    "start": "196000",
    "end": "200319"
  },
  {
    "text": "Now, the next one is a type of model called embedding,",
    "start": "201160",
    "end": "204924"
  },
  {
    "text": "which is essentially taking our data from PDFs and other types of data formats",
    "start": "204924",
    "end": "209226"
  },
  {
    "text": "and preparing it to be used in a vector database to do question and answering on our own unique data.",
    "start": "209227",
    "end": "215120"
  },
  {
    "text": "And then finally, the last type of model that you can use is tool calling.",
    "start": "215640",
    "end": "219639"
  },
  {
    "text": "So, it's a fine-tuned version of a language model.",
    "start": "219760",
    "end": "222778"
  },
  {
    "text": "that is familiar with calling different functions, APIs, and services in an agentic way.",
    "start": "223100",
    "end": "228520"
  },
  {
    "text": "Now, these are some of the types of models, but how would you pick the right model for your use case?",
    "start": "228940",
    "end": "233839"
  },
  {
    "text": "Well, it's a good question, and it depends on your project's requirements.",
    "start": "234200",
    "end": "238198"
  },
  {
    "text": "But some of popular models that we see being used a lot in the community are, for example, llamas series of models.",
    "start": "238380",
    "end": "245599"
  },
  {
    "text": "Different open and fine-tuned models for various use cases that provide support for different languages.",
    "start": "246060",
    "end": "251739"
  },
  {
    "text": "as well as IBM's Granite model.",
    "start": "252230",
    "end": "255349"
  },
  {
    "text": "So, the Granite Model is a enterprise ready LLM that can be used with RAG or agentic functionality.",
    "start": "255490",
    "end": "261769"
  },
  {
    "text": "And we also see these types of reasoning models that are being",
    "start": "262210",
    "end": "266595"
  },
  {
    "text": "more and more popular that essentially provide chain of thought or thinking capabilities to answer your questions.",
    "start": "266596",
    "end": "273249"
  },
  {
    "text": "Now, beyond just using the Model Catalog, you can actually take advantage of what's known",
    "start": "274050",
    "end": "277862"
  },
  {
    "text": "as the Ollama model file to essentially just how Docker has abstracted the complexities of containers.",
    "start": "277863",
    "end": "286190"
  },
  {
    "text": "Well, we're using a model file to abstract the complexities models to be able to import",
    "start": "286770",
    "end": "291544"
  },
  {
    "text": "from Hugging Face, for example,",
    "start": "291545",
    "end": "293482"
  },
  {
    "text": "or to start from a model that you already have",
    "start": "293482",
    "end": "296165"
  },
  {
    "text": "and customize it with system prompts and different parameters to be the best model for your use cases.",
    "start": "296165",
    "end": "301950"
  },
  {
    "text": "But no matter what type of model that you want to use,",
    "start": "302790",
    "end": "305299"
  },
  {
    "text": "your request at the end of the day will be passed through the Ollama server, which is running on localhost on port 11434.",
    "start": "305299",
    "end": "313389"
  },
  {
    "text": "So for example, let's say you're making a request or prompt to the model from your CLI, from your terminal with Ollama.",
    "start": "313990",
    "end": "321029"
  },
  {
    "text": "Well, this is actually being passed to the O Lama model server,",
    "start": "321170",
    "end": "324429"
  },
  {
    "text": "and in a similar way for applications that might want to use models,",
    "start": "324429",
    "end": "328892"
  },
  {
    "text": "for example with Lang chain or another framework, you're make a post request to the model that's running on local host.",
    "start": "328893",
    "end": "335170"
  },
  {
    "text": "on this specific port, which is a REST server.",
    "start": "335610",
    "end": "339089"
  },
  {
    "text": "So it has endpoints, and we can make that request",
    "start": "339470",
    "end": "342049"
  },
  {
    "text": "similar to how we would make a request to any other service that's running on our machine.",
    "start": "342049",
    "end": "346050"
  },
  {
    "text": "And right here is the simplicity of Ollamo for developers.",
    "start": "346550",
    "end": "349750"
  },
  {
    "text": "It lifts the weight of having to run the model in your application, and it abstracts the model as an API.",
    "start": "350150",
    "end": "356229"
  },
  {
    "text": "So you make that requests, and you get that response back all locally on your machine.",
    "start": "356710",
    "end": "361169"
  },
  {
    "text": "Or let's say you want to run Ollama on another machine and you just SSH there, or you make that request remotely.",
    "start": "361650",
    "end": "368189"
  },
  {
    "text": "No matter what you do though, Ollama is doing the heavy lifting of running the model.",
    "start": "368670",
    "end": "372490"
  },
  {
    "text": "And you can even connect other interfaces, such as Open Web UI,",
    "start": "372850",
    "end": "376871"
  },
  {
    "text": "in order to set up a simple RAG pipeline and use your PDFs or other documents",
    "start": "376871",
    "end": "381851"
  },
  {
    "text": "to be passed with the context of that information to Ollama and get that response back.",
    "start": "381851",
    "end": "387729"
  },
  {
    "text": "But that is the Ollama server.",
    "start": "388070",
    "end": "389930"
  },
  {
    "text": "Now.",
    "start": "389930",
    "end": "390930"
  },
  {
    "text": "Whether you want to save on cloud costs when you're using AI models, or have",
    "start": "391110",
    "end": "395636"
  },
  {
    "text": "private sensitive data that can't leave your premise, or even",
    "start": "395636",
    "end": "398382"
  },
  {
    "text": "limited internet access in an IoT device,",
    "start": "398382",
    "end": "401418"
  },
  {
    "text": "you can use Ollamma and the power of open source AI to use and manage LLMs from your local machine.",
    "start": "401419",
    "end": "407829"
  },
  {
    "text": "Now, it isn't the only tool for doing this, but as a developer, it's made my life much easier, and I encourage you to check it out.",
    "start": "408310",
    "end": "415290"
  },
  {
    "text": "As always, thank you so much for joining us today.",
    "start": "415290",
    "end": "418070"
  },
  {
    "text": "Make sure to like the video if you learned something.",
    "start": "418210",
    "end": "420350"
  },
  {
    "text": "and have a good one!",
    "start": "420560",
    "end": "421560"
  }
]