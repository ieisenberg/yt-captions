[
  {
    "start": "0",
    "end": "50000"
  },
  {
    "text": "IBM has a rich history of both  \ncontributing to open source",
    "start": "420",
    "end": "4470"
  },
  {
    "text": "and leveraging open source in its offerings.",
    "start": "4470",
    "end": "7059"
  },
  {
    "text": "And IBM continues that tradition with watsonx.",
    "start": "7240",
    "end": "10818"
  },
  {
    "text": "What is watsonx?",
    "start": "10818",
    "end": "12570"
  },
  {
    "text": "Well, that's our new enterprise platform for AI and data.",
    "start": "12570",
    "end": "15790"
  },
  {
    "text": "And why do we leverage \nopen source and watsonx?",
    "start": "15791",
    "end": "19247"
  },
  {
    "text": "Well, open source gives us the best AI.",
    "start": "19247",
    "end": "22734"
  },
  {
    "text": "It gives us the best innovation",
    "start": "22734",
    "end": "25800"
  },
  {
    "text": "and it gives us the best models.",
    "start": "25801",
    "end": "28335"
  },
  {
    "text": "And so today we're going to look at the  \nopen source that's in watsonx,",
    "start": "28615",
    "end": "31714"
  },
  {
    "text": "and we're going to look at it from three different aspects.",
    "start": "31714",
    "end": "34793"
  },
  {
    "text": "We're going to look at it from \nmodel training and validation.",
    "start": "34892",
    "end": "37359"
  },
  {
    "text": "We're going to look at it from model tuning  \nand inferencing.",
    "start": "37800",
    "end": "41382"
  },
  {
    "text": "And we're going to look at it from data gathering and analytics.",
    "start": "41382",
    "end": "45189"
  },
  {
    "text": "OK, Let's get started with model training and validation.",
    "start": "45996",
    "end": "49747"
  },
  {
    "start": "50000",
    "end": "153000"
  },
  {
    "text": "Training and validating models can take a \nlarge amount of cluster resources,",
    "start": "50220",
    "end": "54828"
  },
  {
    "text": "especially when the models we're looking at are those huge,",
    "start": "54828",
    "end": "58795"
  },
  {
    "text": "multiple-billion parameter foundation models \nthat everyone's talking about.",
    "start": "58795",
    "end": "63682"
  },
  {
    "text": "So, to efficiently use a cluster and to  \nmake it easier for data scientists,",
    "start": "63880",
    "end": "67950"
  },
  {
    "text": "we have an open source project called Codeflare.",
    "start": "67950",
    "end": "70942"
  },
  {
    "text": "Codeflare provides user friendly abstractions",
    "start": "71634",
    "end": "75072"
  },
  {
    "text": "for scaling, queuing and deploying  \nmachine learning workloads.",
    "start": "75072",
    "end": "79184"
  },
  {
    "text": "It integrates Ray, KubeRay, and \nPyTorch to provide these features.",
    "start": "79184",
    "end": "83903"
  },
  {
    "text": "With Ray, it provides a job abstraction.",
    "start": "83903",
    "end": "86323"
  },
  {
    "text": "KubeRay allows Ray to run on  \nKubernetes platforms like OpenShift.",
    "start": "86323",
    "end": "90119"
  },
  {
    "text": "And we'll talk a little bit more   \nabout PyTorch in a minute.",
    "start": "90351",
    "end": "93463"
  },
  {
    "text": "Let's look at a typical Codeflare use case.",
    "start": "93793",
    "end": "96954"
  },
  {
    "text": "Again, the first thing it's going to \nallow us to do is spin up a Ray cluster.",
    "start": "96954",
    "end": "102101"
  },
  {
    "text": "It's then going to allow the data scientist  \nto submit training jobs to the cluster.",
    "start": "102101",
    "end": "107693"
  },
  {
    "text": "If the OpenShift cluster is heavily used \nand there aren't resources available,",
    "start": "107693",
    "end": "112245"
  },
  {
    "text": "Codeflare is able to actually queue the jobs",
    "start": "112245",
    "end": "114760"
  },
  {
    "text": "and wait until there's resources available to run the jobs.",
    "start": "114760",
    "end": "118106"
  },
  {
    "text": "And in some cases, if the cluster is full,",
    "start": "118221",
    "end": "122588"
  },
  {
    "text": "it can actually be scaled up.",
    "start": "122588",
    "end": "124562"
  },
  {
    "text": "And so it's possible to actually scale up \nthe cluster in certain cases from Codeflare.",
    "start": "124662",
    "end": "129465"
  },
  {
    "text": "And then when all the  \ntraining and validation is done,",
    "start": "129547",
    "end": "131917"
  },
  {
    "text": "it can actually delete the Ray jobs  \nand take them off the cluster.",
    "start": "131917",
    "end": "137512"
  },
  {
    "text": "So again, what's nice about Codeflare",
    "start": "137726",
    "end": "140972"
  },
  {
    "text": "is it enables the data scientist \nto efficiently use a cluster,",
    "start": "140973",
    "end": "145533"
  },
  {
    "text": "or in some cases multiple OpenShift clusters,",
    "start": "145533",
    "end": "148899"
  },
  {
    "text": "and not have them worry about \n the infrastructure underneath.",
    "start": "148899",
    "end": "152640"
  },
  {
    "start": "153000",
    "end": "276000"
  },
  {
    "text": "We just looked at how we run model  \ntraining and validation on a cluster.",
    "start": "153540",
    "end": "160286"
  },
  {
    "text": "But now let's look at how \nwe actually represent those models.",
    "start": "160517",
    "end": "164207"
  },
  {
    "text": "And the open source project that we use \nto represent the models is PyTorch.",
    "start": "164339",
    "end": "169593"
  },
  {
    "text": "PyTorch provides some key \nfeatures for representing models.",
    "start": "169741",
    "end": "174199"
  },
  {
    "text": "One of which is tensor support.",
    "start": "174199",
    "end": "177282"
  },
  {
    "text": "What's a tensor?",
    "start": "177282",
    "end": "178699"
  },
  {
    "text": "Well, it's a huge multi-dimensional array",
    "start": "178699",
    "end": "181449"
  },
  {
    "text": "that supports all those weighted values  \nor probabilities that are in the model",
    "start": "181449",
    "end": "187151"
  },
  {
    "text": "that we tweak over time to get the model right",
    "start": "187151",
    "end": "189655"
  },
  {
    "text": "to be able to predict things correctly.",
    "start": "189655",
    "end": "192228"
  },
  {
    "text": "The other key features that PyTorch provides",
    "start": "192409",
    "end": "194844"
  },
  {
    "text": "are GPU support and distributed training.",
    "start": "194844",
    "end": "198282"
  },
  {
    "text": "When we train the models,",
    "start": "198562",
    "end": "201515"
  },
  {
    "text": "we're actually doing large amounts of computation,",
    "start": "201516",
    "end": "205039"
  },
  {
    "text": "and the GPUs that PyTorch is able to effectively \nuse allow us to do that very efficiently.",
    "start": "205039",
    "end": "211283"
  },
  {
    "text": "And PyTorch also provides distributed training.",
    "start": "211401",
    "end": "213660"
  },
  {
    "text": "So with those large foundation models \nthat wouldn't fit on a single machine,  ",
    "start": "213808",
    "end": "217071"
  },
  {
    "text": "PyTorch enables us to do distributed training",
    "start": "217072",
    "end": "219879"
  },
  {
    "text": "across a large number of machines.",
    "start": "219879",
    "end": "222000"
  },
  {
    "text": "Let's look  at the key features that PyTorch provides.",
    "start": "222072",
    "end": "225453"
  },
  {
    "text": "One of which is neural network creation.",
    "start": "225453",
    "end": "228705"
  },
  {
    "text": "There's different types of neural networks,",
    "start": "228705",
    "end": "230403"
  },
  {
    "text": "and PyTorch makes it easy to create \n all the different popular types of neural networks.",
    "start": "230403",
    "end": "234696"
  },
  {
    "text": "PyTorch also provides easy loading of data.",
    "start": "234844",
    "end": "237980"
  },
  {
    "text": "Another key feature of PyTorch is training loops.",
    "start": "238080",
    "end": "241323"
  },
  {
    "text": "So built-in, easy-to-use training loops \nthat are tweaking the model data",
    "start": "241438",
    "end": "245945"
  },
  {
    "text": "to improve its ability to more accurately \nprovide inferencing.",
    "start": "245945",
    "end": "250891"
  },
  {
    "text": "And finally, PyTorch also provides built-in model adjustments.",
    "start": "251385",
    "end": "255193"
  },
  {
    "text": "The key one here is the auto gradient calculation.",
    "start": "255292",
    "end": "258329"
  },
  {
    "text": "So think from your calculus days when \nyou're calculating gradients.",
    "start": "258444",
    "end": "262328"
  },
  {
    "text": "Having that feature built in, making \nthe minor tweaks to the model",
    "start": "262328",
    "end": "267150"
  },
  {
    "text": "so that improves it and gets it over time \nto be a better predictor and better usage.",
    "start": "267150",
    "end": "273845"
  },
  {
    "text": "This is what PyTorch provides.",
    "start": "273944",
    "end": "276482"
  },
  {
    "start": "276000",
    "end": "403000"
  },
  {
    "text": "We just looked at how to represent models.",
    "start": "276630",
    "end": "279213"
  },
  {
    "text": "But now let's look at model tuning and inferencing.",
    "start": "279213",
    "end": "282962"
  },
  {
    "text": "And what do we mean by this?",
    "start": "283110",
    "end": "284610"
  },
  {
    "text": "Well, we want to be able to serve  \na large number of AI models",
    "start": "284610",
    "end": "289745"
  },
  {
    "text": "and be able to do it at scale on OpenShift.",
    "start": "289746",
    "end": "293160"
  },
  {
    "text": "So the open source projects that we look at,",
    "start": "294660",
    "end": "297491"
  },
  {
    "text": "the first key one is Kserve/ModelMesh.",
    "start": "297491",
    "end": "300060"
  },
  {
    "text": "So this is what we use to actually  \nserve up the models.",
    "start": "300274",
    "end": "303320"
  },
  {
    "text": "And originally there was just Kserve,",
    "start": "303403",
    "end": "305831"
  },
  {
    "text": "which would allow us to put one \nmodel in a single pod.",
    "start": "305831",
    "end": "309625"
  },
  {
    "text": "So one pod per model.",
    "start": "309625",
    "end": "312548"
  },
  {
    "text": "That's not very efficient at all.",
    "start": "312729",
    "end": "315191"
  },
  {
    "text": "And Kserve was merged with another \nopen source project called ModelMesh.",
    "start": "315290",
    "end": "319309"
  },
  {
    "text": "And ModelMesh is much better at",
    "start": "319375",
    "end": "322929"
  },
  {
    "text": "being able to efficiently get large, \nthousands of models in a single pod.",
    "start": "322929",
    "end": "328814"
  },
  {
    "text": "So between these two technologies,",
    "start": "329029",
    "end": "331313"
  },
  {
    "text": "we're able to serve up thousands of  \nmodels efficiently on a OpenShift cluster.",
    "start": "331313",
    "end": "335871"
  },
  {
    "text": "Now, where are we going to find all these models?",
    "start": "336000",
    "end": "339267"
  },
  {
    "text": "Well, Huggingface has over 200,000 models,  \nopen source models.",
    "start": "339267",
    "end": "344805"
  },
  {
    "text": "It's typically referred to as the GitHub of models.",
    "start": "344805",
    "end": "348314"
  },
  {
    "text": "And IBM has a partnership with Huggingface.",
    "start": "348314",
    "end": "350948"
  },
  {
    "text": "And again, it's a great place to find great models  \nto use on our IBM watsonx offerings.",
    "start": "350948",
    "end": "357613"
  },
  {
    "text": "The other key open source \ntechnologies we have are Caikit.",
    "start": "357614",
    "end": "362213"
  },
  {
    "text": "Caikit is an open source project that \nprovides APIs for prompt tuning.",
    "start": "362213",
    "end": "367067"
  },
  {
    "text": "So again, typically on the inferencing side,",
    "start": "367166",
    "end": "370974"
  },
  {
    "text": "you're serving up the models,",
    "start": "370974",
    "end": "372891"
  },
  {
    "text": "but you also, in some cases, need to do   \na little bit of tuning to improve the results.",
    "start": "372891",
    "end": "378128"
  },
  {
    "text": "And Caikit provides tuning APIs to do that.",
    "start": "378128",
    "end": "380632"
  },
  {
    "text": "The next technology is Kubeflow.",
    "start": "380731",
    "end": "384120"
  },
  {
    "text": "Kubeflow provides orchestration of  \nmachine learning workloads",
    "start": "384268",
    "end": "388671"
  },
  {
    "text": "and again allow you to build those those  \nmachine learning pipelines",
    "start": "388671",
    "end": "391673"
  },
  {
    "text": "that you need to make life easy.",
    "start": "391674",
    "end": "394016"
  },
  {
    "text": "So again, we have a wonderful large  \nnumber of open source projects",
    "start": "394115",
    "end": "398490"
  },
  {
    "text": "that provide our prompt tuning and  \ninferencing, all running on OpenShift.",
    "start": "398490",
    "end": "402880"
  },
  {
    "start": "403000",
    "end": "451000"
  },
  {
    "text": "Now let's switch gears and look at data \ngathering and analytics.",
    "start": "403045",
    "end": "406861"
  },
  {
    "text": "And the open source project  \nthat we use for that is Presto.",
    "start": "406861",
    "end": "410580"
  },
  {
    "text": "What is Presto?",
    "start": "411120",
    "end": "412620"
  },
  {
    "text": "Presto is an SQL query engine,",
    "start": "412620",
    "end": "415751"
  },
  {
    "text": "and it's used for open data analytics",
    "start": "415751",
    "end": "419028"
  },
  {
    "text": "and for the open data lakehouse.",
    "start": "419028",
    "end": "421422"
  },
  {
    "text": "And let's look at the key  \nfeatures that it provides:",
    "start": "421652",
    "end": "424699"
  },
  {
    "text": "high performance,",
    "start": "424699",
    "end": "426911"
  },
  {
    "text": "Presto is highly scalable.",
    "start": "427109",
    "end": "429796"
  },
  {
    "text": "It provides federated queries,",
    "start": "430076",
    "end": "432414"
  },
  {
    "text": "and it's able to query the data where it lives.",
    "start": "432613",
    "end": "435587"
  },
  {
    "text": "I hope I've convinced you that watsonx \nhas continued IBM's long tradition",
    "start": "435785",
    "end": "441657"
  },
  {
    "text": "of contributing to open source and \nleveraging open source and its offerings.",
    "start": "441657",
    "end": "446539"
  },
  {
    "text": "If you'd like to learn more, \nplease check out the links below.",
    "start": "446737",
    "end": "450226"
  }
]