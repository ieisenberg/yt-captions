[
  {
    "text": "Remember how back in the day people would",
    "start": "940",
    "end": "3187"
  },
  {
    "text": "Google themselves, you type your name into a search engine and you see what it knows about you?",
    "start": "3187",
    "end": "8179"
  },
  {
    "text": "Well, the modern equivalent of that is to do the same thing with a chatbot.",
    "start": "8380",
    "end": "13198"
  },
  {
    "text": "So when I ask a large language model, who is Martin Keen?",
    "start": "13440",
    "end": "18339"
  },
  {
    "text": "Well, the response varies greatly depending upon which model I'm asking,",
    "start": "18540",
    "end": "22997"
  },
  {
    "text": "because different models, they have different training data sets, they have a different knowledge cutoff dates.",
    "start": "22997",
    "end": "27520"
  },
  {
    "text": "So what a given model knows about me, well, it differs greatly.",
    "start": "28000",
    "end": "32299"
  },
  {
    "text": "But how could we improve the model's answer?",
    "start": "32520",
    "end": "35740"
  },
  {
    "text": "Well, there's three ways.",
    "start": "36080",
    "end": "37779"
  },
  {
    "text": "So let's start with a model here, and we're gonna see how we can improve its responses.",
    "start": "38000",
    "end": "44280"
  },
  {
    "text": "Well, the first thing it could do is it could go out and it could perform a search,",
    "start": "44840",
    "end": "51059"
  },
  {
    "text": "a search for new data that either wasn't in its training data set,",
    "start": "51059",
    "end": "54615"
  },
  {
    "text": "or it was just data that became available after the model finished training,",
    "start": "54616",
    "end": "58219"
  },
  {
    "text": "and then it could incorporate those results from the search back into its answer.",
    "start": "58840",
    "end": "63320"
  },
  {
    "text": "That is called RAG or Retrieval Augmented Generation.",
    "start": "63620",
    "end": "70359"
  },
  {
    "text": "That's one method.",
    "start": "71200",
    "end": "72200"
  },
  {
    "text": "Or we could pick a specialized model, a model that's been trained on, let's say, transcripts of these videos.",
    "start": "72760",
    "end": "81260"
  },
  {
    "text": "That would be an example of something called fine tuning,",
    "start": "81800",
    "end": "88037"
  },
  {
    "text": "or we could ask the model a query that better specifies what we're looking for.",
    "start": "89061",
    "end": "96329"
  },
  {
    "text": "So maybe the LLM already knows plenty about the Martin Keens of the world,",
    "start": "96450",
    "end": "101620"
  },
  {
    "text": "but let's tell the model that we're referring to the Martin keen who works at IBM,",
    "start": "101621",
    "end": "105927"
  },
  {
    "text": "rather than the Martin Keen that founded Keen Shoes.",
    "start": "105927",
    "end": "110010"
  },
  {
    "text": "That is an example of prompt engineering.",
    "start": "110170",
    "end": "114930"
  },
  {
    "text": "Three ways to get better outputs out of large language models, each with their pluses and minuses.",
    "start": "115950",
    "end": "123009"
  },
  {
    "text": "Let's start with RAG.",
    "start": "123250",
    "end": "124590"
  },
  {
    "text": "So let's break it down.",
    "start": "125450",
    "end": "126450"
  },
  {
    "text": "First there's retrieval.",
    "start": "126470",
    "end": "128409"
  },
  {
    "text": "So retrieval of external up-to-date information.",
    "start": "128570",
    "end": "131830"
  },
  {
    "text": "Then there's augmentation.",
    "start": "132350",
    "end": "134310"
  },
  {
    "text": "That's augmentation of the original prompt with the retrieved information added in.",
    "start": "134810",
    "end": "139310"
  },
  {
    "text": "And then finally there's generation.",
    "start": "139310",
    "end": "141829"
  },
  {
    "text": "That's generation of a response based on all of this enriched context.",
    "start": "142210",
    "end": "147129"
  },
  {
    "text": "So we can think of it like this.",
    "start": "147918",
    "end": "150250"
  },
  {
    "text": "So we start with a query and the query comes in to a large language model.",
    "start": "150390",
    "end": "159550"
  },
  {
    "text": "Now, what RAG is gonna do is it's first going to go searching through a corpus of information.",
    "start": "160110",
    "end": "167810"
  },
  {
    "text": "So we have this corpus here full of some sort of data.",
    "start": "168370",
    "end": "173610"
  },
  {
    "text": "Now, perhaps, that's your organization's documents.",
    "start": "173810",
    "end": "176839"
  },
  {
    "text": "So it might be spreadsheets, PDFs, internal wikis, you know, stuff like that,",
    "start": "176880",
    "end": "181020"
  },
  {
    "text": "But unlike a typical search engine that just matches keywords,",
    "start": "181800",
    "end": "187167"
  },
  {
    "text": "RAG converts both your question, the query, and all of the documents into something called vector embeddings.",
    "start": "187167",
    "end": "197439"
  },
  {
    "text": "So these are all converted into vectors.",
    "start": "198280",
    "end": "200319"
  },
  {
    "text": "essentially turning words and phrases into long lists of numbers that capture their meaning.",
    "start": "200760",
    "end": "206680"
  },
  {
    "text": "So when you ask a query like, what was our company's revenue growth last quarter?",
    "start": "207380",
    "end": "213439"
  },
  {
    "text": "Well, RAG will find documents that are mathematically similar in meaning to your question,",
    "start": "214180",
    "end": "218717"
  },
  {
    "text": "even if they don't use the exact same words.",
    "start": "218717",
    "end": "221698"
  },
  {
    "text": "So it might find documents mentioning fourth quarter performance or quarterly sales.",
    "start": "221800",
    "end": "227620"
  },
  {
    "text": "Those don't contain the keyword revenue growth, but they are semantically similar.",
    "start": "228230",
    "end": "234209"
  },
  {
    "text": "Now, once RAG finds the relevant information, it adds this information",
    "start": "234950",
    "end": "239718"
  },
  {
    "text": "back into your original query before passing it to the language model.",
    "start": "239718",
    "end": "245649"
  },
  {
    "text": "So instead of the model just kind of guessing based on its training data,",
    "start": "246310",
    "end": "249709"
  },
  {
    "text": "it can now generate a response that incorporates your actual facts and figures.",
    "start": "249710",
    "end": "254869"
  },
  {
    "text": "So this makes RAG particularly valuable when you are looking for information that is up to date,",
    "start": "255740",
    "end": "264629"
  },
  {
    "text": "and it's also very valuable when you need in to add in information that is domain specific as well,",
    "start": "264630",
    "end": "273689"
  },
  {
    "text": "but there are some costs to this.",
    "start": "274910",
    "end": "277549"
  },
  {
    "text": "Let's go with the red pen.",
    "start": "278130",
    "end": "279550"
  },
  {
    "text": "So one cost, that would be the cost of performance.",
    "start": "280150",
    "end": "285589"
  },
  {
    "text": "for performing all of this, because you have this retrieval step here, and that",
    "start": "285840",
    "end": "290013"
  },
  {
    "text": "adds latency to each query compared to a simple prompt to a model.",
    "start": "290013",
    "end": "294660"
  },
  {
    "text": "There are also costs related to just kind of the processing of this as well.",
    "start": "295320",
    "end": "301440"
  },
  {
    "text": "So if we think about what we're having to do here, we've got documents that need to be vector embeddings,",
    "start": "301480",
    "end": "307766"
  },
  {
    "text": "and we need to store these vector embedding in a database.",
    "start": "307766",
    "end": "310800"
  },
  {
    "text": "All of this adds to processing costs, it adds to infrastructure costs",
    "start": "311460",
    "end": "314899"
  },
  {
    "text": "to make this solution work.",
    "start": "315370",
    "end": "316710"
  },
  {
    "text": "All right, next up, fine tuning.",
    "start": "317450",
    "end": "319910"
  },
  {
    "text": "So remember how we discussed getting better answers about me by",
    "start": "320130",
    "end": "324093"
  },
  {
    "text": "training a model specifically on, let's say, my video transcripts.",
    "start": "324093",
    "end": "326869"
  },
  {
    "text": "Well, that is fine tuning in action.",
    "start": "326870",
    "end": "330429"
  },
  {
    "text": "So what we do with fine tuning is we take a model, but specifically an existing model.",
    "start": "330810",
    "end": "339613"
  },
  {
    "text": "and that existing model has broad knowledge.",
    "start": "340470",
    "end": "343930"
  },
  {
    "text": "And then we're gonna give it additional specialized training on a focused data set.",
    "start": "344230",
    "end": "351430"
  },
  {
    "text": "So this is now specialized to what we want to develop particular expertise on.",
    "start": "351810",
    "end": "357910"
  },
  {
    "text": "Now, during fine tuning, we're updating the model's internal parameters through additional training.",
    "start": "358650",
    "end": "365209"
  },
  {
    "text": "So the model starts out with some weights here.",
    "start": "365410",
    "end": "369790"
  },
  {
    "text": "like this, and those weights were optimized during its initial pre-training.",
    "start": "370450",
    "end": "376010"
  },
  {
    "text": "And as we fine tune, we're making small adjustments here to the model's weights using this specialized data set.",
    "start": "376230",
    "end": "386310"
  },
  {
    "text": "So this is being incorporated.",
    "start": "386430",
    "end": "388169"
  },
  {
    "text": "Now this process typically uses supervised learning where we provide input-output",
    "start": "389450",
    "end": "394759"
  },
  {
    "text": "pairs that demonstrate the kind of responses we want.",
    "start": "394759",
    "end": "397649"
  },
  {
    "text": "So for example, if we're fine-tuning for technical support, we might provide thousands of examples of customer queries,",
    "start": "397650",
    "end": "406365"
  },
  {
    "text": "and those would be paired with correct technical responses.",
    "start": "406365",
    "end": "409819"
  },
  {
    "text": "The model adjusts its weights through back propagation",
    "start": "410420",
    "end": "413974"
  },
  {
    "text": "to minimize the difference between its predicted outputs and the targeted responses.",
    "start": "413974",
    "end": "418120"
  },
  {
    "text": "So we're not just teaching the model new facts here, we're actually modifying how it processes information.",
    "start": "418580",
    "end": "426179"
  },
  {
    "text": "The model is learning to recognize domain-specific patterns.",
    "start": "426930",
    "end": "430749"
  },
  {
    "text": "So, fine-tuning shows its strength when you particularly need a model that has very deep domain expertise.",
    "start": "431850",
    "end": "441389"
  },
  {
    "text": "That's what we can really add in with fine tuning,",
    "start": "442130",
    "end": "444890"
  },
  {
    "text": "and also, it's much faster, specifically at inference time.",
    "start": "445490",
    "end": "451048"
  },
  {
    "text": "So when we are putting the queries in, it's faster than RAG because it doesn't need to search through external data,",
    "start": "451150",
    "end": "457630"
  },
  {
    "text": "and because the knowledge is kind of baked into the model's weights, you don't need to maintain a separate vector database,",
    "start": "458210",
    "end": "463569"
  },
  {
    "text": "but there's some downsides as well.",
    "start": "463990",
    "end": "466729"
  },
  {
    "text": "Well, there's certainly issues here with the training complexity of all of this.",
    "start": "466730",
    "end": "473769"
  },
  {
    "text": "You're going to need thousands of high quality training examples.",
    "start": "474510",
    "end": "479450"
  },
  {
    "text": "There are also issues with computational cost.",
    "start": "479990",
    "end": "485209"
  },
  {
    "text": "The computational cost for training this model can be substantial and is going to require a whole bunch of GPUs.",
    "start": "485910",
    "end": "491430"
  },
  {
    "text": "And there's also challenges related to maintenance as well",
    "start": "492041",
    "end": "497439"
  },
  {
    "text": "because unlike RAG where you can easily add new documents to your knowledge base at any point.",
    "start": "497439",
    "end": "502149"
  },
  {
    "text": "Updating a fine-tune model requires another round of training",
    "start": "502630",
    "end": "506649"
  },
  {
    "text": "and then perhaps most importantly of all there is a risk of something called catastrophic forgetting.",
    "start": "507050",
    "end": "516869"
  },
  {
    "text": "Now that's when the model loses some of its general capabilities while it's busy learning these specialized ones.",
    "start": "517390",
    "end": "524269"
  },
  {
    "text": "So finally let's explore prompt engineering.",
    "start": "524270",
    "end": "528129"
  },
  {
    "text": "Now specifying Martin Keen who works at IBM versus",
    "start": "528780",
    "end": "532560"
  },
  {
    "text": "Martin Keene who founded Keene Shoes, that's prompt engineering, but at its most basic.",
    "start": "532560",
    "end": "537599"
  },
  {
    "text": "Prompt engineering goes far beyond simple clarification.",
    "start": "537960",
    "end": "541519"
  },
  {
    "text": "So let's think about when we input a prompt, the model receives this prompt and it processes it through a series of layers,",
    "start": "541880",
    "end": "556231"
  },
  {
    "text": "and these layers are essentially tension mechanisms and each one",
    "start": "556231",
    "end": "561318"
  },
  {
    "text": "focuses on different aspects of your prompt text that came in.",
    "start": "561318",
    "end": "565179"
  },
  {
    "text": "And by including specific elements in your prompt, so examples or context or how you want the format to look,",
    "start": "565680",
    "end": "572831"
  },
  {
    "text": "you're directing the model's attention to relevant patterns it learned during training.",
    "start": "572831",
    "end": "578299"
  },
  {
    "text": "So for example, telling a model to think about this step-by-step,",
    "start": "578760",
    "end": "582906"
  },
  {
    "text": "that activates patterns it learnt from training data where methodical reasoning led to accurate results.",
    "start": "582906",
    "end": "589230"
  },
  {
    "text": "So a well-engineered prompt can transform a model's output without any additional training or without data retrieval.",
    "start": "589750",
    "end": "599429"
  },
  {
    "text": "So take an example of a prompt.",
    "start": "599930",
    "end": "602649"
  },
  {
    "text": "Let's say we say, is this code secure?",
    "start": "602810",
    "end": "605490"
  },
  {
    "text": "Not a very good prompt.",
    "start": "606510",
    "end": "607570"
  },
  {
    "text": "An engineered prompt, it might read a bit more like this.",
    "start": "608210",
    "end": "611970"
  },
  {
    "text": "It's much more detailed.",
    "start": "612190",
    "end": "613850"
  },
  {
    "text": "Now.",
    "start": "613850",
    "end": "614790"
  },
  {
    "text": "We haven't changed the model, we haven't added new data, we've just better activated its existing capabilities.",
    "start": "614790",
    "end": "622509"
  },
  {
    "text": "Now I think the benefits to this are pretty obvious.",
    "start": "623170",
    "end": "626029"
  },
  {
    "text": "One is that we don't need to change any of our back-end infrastructure here",
    "start": "626790",
    "end": "632355"
  },
  {
    "text": "because there are no infrastructure changes at all in order to prompt better, it's all on the user.",
    "start": "632356",
    "end": "639070"
  },
  {
    "text": "There's also the benefit that by doing this, You get to see immediate responses and immediate results to what you do.",
    "start": "639790",
    "end": "649440"
  },
  {
    "text": "We don't have to add in new training data or any kind of data processing,",
    "start": "650040",
    "end": "653818"
  },
  {
    "text": "but of course there are some limitations to this as well.",
    "start": "653818",
    "end": "657240"
  },
  {
    "text": "Prompt engineering is as much an art as it is a science.",
    "start": "658060",
    "end": "661378"
  },
  {
    "text": "So there is certainly a good amount of trial and error in this sort of process to find effective prompts,",
    "start": "661820",
    "end": "670534"
  },
  {
    "text": "and you're also limited in what you can do here, you're limited",
    "start": "670888",
    "end": "675985"
  },
  {
    "text": "to existing knowledge because you're not able to actually add anything else in here.",
    "start": "675985",
    "end": "683610"
  },
  {
    "text": "No additional amount of prompt engineering is going to teach it truly new information.",
    "start": "683810",
    "end": "688690"
  },
  {
    "text": "You're not going to the model anything that's outdated in the model.",
    "start": "688950",
    "end": "693157"
  },
  {
    "text": "So we've talked about now RAG as being one option and we talked about fine tuning as being another one.",
    "start": "693916",
    "end": "704610"
  },
  {
    "text": "And now, just now, we've talked about prompt engineering as well",
    "start": "704990",
    "end": "710909"
  },
  {
    "text": "and I've really talked about those as three different distinct things here,",
    "start": "711190",
    "end": "717449"
  },
  {
    "text": "but they're commonly used actually in combination.",
    "start": "717610",
    "end": "721928"
  },
  {
    "text": "We might use all three together.",
    "start": "722270",
    "end": "724589"
  },
  {
    "text": "So consider a legal AI system.",
    "start": "725310",
    "end": "727350"
  },
  {
    "text": "RAG, that could retrieve specific cases and recent court decisions.",
    "start": "727830",
    "end": "732129"
  },
  {
    "text": "The prompt engineering part, that could make sure that we follow proper legal document formats by asking for it.",
    "start": "732980",
    "end": "739178"
  },
  {
    "text": "And then fine-tuning, that can help the model master firm-specific policies.",
    "start": "739500",
    "end": "743840"
  },
  {
    "text": "I mean, basically, we can think of it like this.",
    "start": "744480",
    "end": "746919"
  },
  {
    "text": "We can think that prompt engineering offers flexibility and immediate results, but it can't extend knowledge.",
    "start": "747160",
    "end": "753599"
  },
  {
    "text": "RAG, that can extend knowledge, it provides up-to-date information, but with computational overhead.",
    "start": "754160",
    "end": "759618"
  },
  {
    "text": "and then fine-tuning,",
    "start": "759620",
    "end": "761179"
  },
  {
    "text": "that enables deep domain expertise, but it requires significant resources and maintenance.",
    "start": "761530",
    "end": "766909"
  },
  {
    "text": "Basically, it comes down to picking the methods that work for you.",
    "start": "767630",
    "end": "771990"
  },
  {
    "text": "You know, we've, we sure come a long way from vanity searching on Google.",
    "start": "772790",
    "end": "777589"
  }
]