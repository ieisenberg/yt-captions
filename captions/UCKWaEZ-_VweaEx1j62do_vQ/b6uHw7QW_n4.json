[
  {
    "start": "0",
    "end": "33000"
  },
  {
    "text": "Whether you're just getting started on your journey to becoming a data scientist",
    "start": "720",
    "end": "4634"
  },
  {
    "text": "or you've been here for years,",
    "start": "4634",
    "end": "6000"
  },
  {
    "text": "you'll probably recognize the KNN algorithm.",
    "start": "6000",
    "end": "9524"
  },
  {
    "text": "It stands for K-Nearest Neighbors,",
    "start": "9524",
    "end": "12785"
  },
  {
    "text": "and it's one of the most popular and simplest",
    "start": "12785",
    "end": "15877"
  },
  {
    "text": "classification and regression classifiers used in machine learning today.",
    "start": "15877",
    "end": "20000"
  },
  {
    "text": "As a classification algorithm, KNN operates on the assumption",
    "start": "20460",
    "end": "23686"
  },
  {
    "text": "that similar data points are located near each other",
    "start": "23687",
    "end": "27194"
  },
  {
    "text": "and can be grouped in the same category",
    "start": "27324",
    "end": "29863"
  },
  {
    "text": "based on their proximity.",
    "start": "29864",
    "end": "31170"
  },
  {
    "text": "So let's consider an example.",
    "start": "31434",
    "end": "34013"
  },
  {
    "start": "33000",
    "end": "64000"
  },
  {
    "text": "Imagine we have a data set containing information",
    "start": "34301",
    "end": "37696"
  },
  {
    "text": "about different types of fruit.",
    "start": "37696",
    "end": "40960"
  },
  {
    "text": "So let's visualize a fruit dataset here.",
    "start": "41424",
    "end": "46583"
  },
  {
    "text": "Now we have each fruit categorized by two things.",
    "start": "47019",
    "end": "50829"
  },
  {
    "text": "We have it categorized by its sweetness,",
    "start": "51210",
    "end": "55309"
  },
  {
    "text": "that's our X-axis here.",
    "start": "55410",
    "end": "57551"
  },
  {
    "text": "And then on the Y-axis we are classifying it by its crunchiness.",
    "start": "57866",
    "end": "63805"
  },
  {
    "text": "Now we've already labeled some data points.",
    "start": "64890",
    "end": "68069"
  },
  {
    "text": "So we've got a few apples here.",
    "start": "68070",
    "end": "72540"
  },
  {
    "text": "Apples are very crunchy and somewhat sweet.",
    "start": "73091",
    "end": "76840"
  },
  {
    "text": "And then we have a few oranges down here.",
    "start": "77245",
    "end": "81414"
  },
  {
    "text": "Oranges are very sweet.",
    "start": "81762",
    "end": "83472"
  },
  {
    "text": "Not so crunchy.",
    "start": "83695",
    "end": "84695"
  },
  {
    "text": "Now suppose you have a new fruit that you want to classify.",
    "start": "85110",
    "end": "88439"
  },
  {
    "text": "Well, we measure its crunchiness ",
    "start": "88440",
    "end": "90940"
  },
  {
    "text": "and we measure its sweetness,",
    "start": "90940",
    "end": "93862"
  },
  {
    "text": "and then we can plot it on the graph.",
    "start": "93862",
    "end": "96023"
  },
  {
    "text": "Let's say it comes out maybe here.",
    "start": "96023",
    "end": "99218"
  },
  {
    "text": "The KNN an algorithm will then look at the",
    "start": "100097",
    "end": "103117"
  },
  {
    "text": "K nearest points on the graph to this new fruit.",
    "start": "103117",
    "end": "106319"
  },
  {
    "text": "And if most of these nearest points are classified as apples,",
    "start": "106680",
    "end": "111115"
  },
  {
    "text": "the algorithm will classify the new fruit as an apple as well.",
    "start": "111115",
    "end": "115000"
  },
  {
    "text": "How's that for an apples to apples comparison?",
    "start": "115420",
    "end": "119618"
  },
  {
    "text": "Now, before a classification can be made,",
    "start": "120030",
    "end": "123017"
  },
  {
    "text": "the distance must be defined.",
    "start": "123017",
    "end": "124914"
  },
  {
    "text": "And there are only two requirements for a KNN algorithm",
    "start": "124914",
    "end": "128550"
  },
  {
    "text": "to achieve its goal.",
    "start": "128550",
    "end": "130116"
  },
  {
    "text": "And the first one is what's called the distance metric.",
    "start": "130435",
    "end": "135985"
  },
  {
    "start": "137000",
    "end": "201000"
  },
  {
    "text": "The distance between the query points",
    "start": "137640",
    "end": "140946"
  },
  {
    "text": "and the other data points needs to be calculated",
    "start": "140946",
    "end": "144263"
  },
  {
    "text": "following decision boundaries and partitioning query points into different regions,",
    "start": "144263",
    "end": "148792"
  },
  {
    "text": "which are commonly visualized using Voronezh diagrams,",
    "start": "148792",
    "end": "151292"
  },
  {
    "text": "which kind of look like a kaleidoscope.",
    "start": "151292",
    "end": "153294"
  },
  {
    "text": "And this distance serves as our distance metric",
    "start": "153294",
    "end": "156128"
  },
  {
    "text": "and can be calculated using various measures",
    "start": "156128",
    "end": "159070"
  },
  {
    "text": "such as Euclidean distance or Manhattan distance.",
    "start": "159070",
    "end": "161898"
  },
  {
    "text": "So that's number one.",
    "start": "162370",
    "end": "163509"
  },
  {
    "text": "Number two is we now need to define the value of K",
    "start": "163800",
    "end": "170561"
  },
  {
    "text": "and the K value in the KNN algorithm defines how many neighbors will be checked",
    "start": "170561",
    "end": "175581"
  },
  {
    "text": "to determine the classification of a specific query point.",
    "start": "175581",
    "end": "179129"
  },
  {
    "text": "So for example, if K equals one,",
    "start": "179370",
    "end": "183859"
  },
  {
    "text": "the instance will be assigned to the same class as its single nearest neighbor.",
    "start": "183995",
    "end": "189269"
  },
  {
    "text": "Choosing the right K value largely depends on the input data.",
    "start": "190020",
    "end": "193740"
  },
  {
    "text": "Data with more outliers, or noise,",
    "start": "193740",
    "end": "196198"
  },
  {
    "text": "will likely perform much better with higher values of K.",
    "start": "196342",
    "end": "201105"
  },
  {
    "start": "201000",
    "end": "275000"
  },
  {
    "text": "Also, it's recommended to choose an odd number for K",
    "start": "201600",
    "end": "204354"
  },
  {
    "text": "to minimize the chances of ties in classification.",
    "start": "204354",
    "end": "207300"
  },
  {
    "text": "Now, just like any machine learning algorithm,",
    "start": "207870",
    "end": "210093"
  },
  {
    "text": "KNN has its strengths and it has its weaknesses.",
    "start": "210093",
    "end": "213209"
  },
  {
    "text": "So let's take a look at some of those.",
    "start": "213210",
    "end": "215219"
  },
  {
    "text": "And on the plus side, we have to say that KNN is quite easy to implement.",
    "start": "215520",
    "end": "222480"
  },
  {
    "text": "Its simplicity and its accuracy",
    "start": "224430",
    "end": "227120"
  },
  {
    "text": "make it one of the first classifiers that a new data scientist will learn.",
    "start": "227120",
    "end": "231750"
  },
  {
    "text": "It also has only a few hyper parameters,",
    "start": "232260",
    "end": "237944"
  },
  {
    "text": "which is a big advantage as well.",
    "start": "238132",
    "end": "241177"
  },
  {
    "text": "KNN only requires a K value and a distance metric,",
    "start": "241490",
    "end": "246136"
  },
  {
    "text": "which is a lot less than other machine learning algorithms.",
    "start": "246136",
    "end": "249240"
  },
  {
    "text": "Also, in the plus category, we can say that it's very adaptable as well.",
    "start": "249930",
    "end": "256049"
  },
  {
    "text": "Meaning as new training samples are added,",
    "start": "256950",
    "end": "260096"
  },
  {
    "text": "the algorithm adjusts to account for any new data.",
    "start": "260096",
    "end": "263379"
  },
  {
    "text": "Since all training data is stored into memory.",
    "start": "263797",
    "end": "266646"
  },
  {
    "text": "That sounds good, but there's also a drawback here, and that is...",
    "start": "267000",
    "end": "271140"
  },
  {
    "text": "but because of that, it doesn't scale very well.",
    "start": "271140",
    "end": "274878"
  },
  {
    "start": "275000",
    "end": "312000"
  },
  {
    "text": "As a data set grows, the algorithm becomes less efficient",
    "start": "275790",
    "end": "280000"
  },
  {
    "text": "due to increased computational complexity",
    "start": "280000",
    "end": "282521"
  },
  {
    "text": "compromising the overall model performance.",
    "start": "282521",
    "end": "285249"
  },
  {
    "text": "And this this inability to scale,",
    "start": "285443",
    "end": "287578"
  },
  {
    "text": "it comes from KNN being what's called a lazy algorithm,",
    "start": "287579",
    "end": "291617"
  },
  {
    "text": "meaning it stores all training data and defers the computation",
    "start": "291617",
    "end": "295128"
  },
  {
    "text": "to the time of classification.",
    "start": "295128",
    "end": "297318"
  },
  {
    "text": "That results in higher memory usage and slower processing",
    "start": "297684",
    "end": "300852"
  },
  {
    "text": "compared to other classifiers.",
    "start": "300852",
    "end": "302712"
  },
  {
    "text": "Now, KNN also tends to fall victim to something called",
    "start": "303260",
    "end": "308469"
  },
  {
    "text": "the curse of dimensionality,",
    "start": "308469",
    "end": "312323"
  },
  {
    "start": "312000",
    "end": "356000"
  },
  {
    "text": "which means it doesn't perform well with high dimensional data inputs.",
    "start": "312871",
    "end": "317910"
  },
  {
    "text": "So in our sweetness-to-crunchiness example, this is a 2D space.",
    "start": "317930",
    "end": "322399"
  },
  {
    "text": "It's relatively easy to find the nearest neighbors and classify new fruits accurately.",
    "start": "322700",
    "end": "327199"
  },
  {
    "text": "However, if we keep adding more features like color and size and weight and so on,",
    "start": "327740",
    "end": "333005"
  },
  {
    "text": "the data points become sparse in the high dimensional space.",
    "start": "333006",
    "end": "336860"
  },
  {
    "text": "The distances between the points starts to become similar,",
    "start": "337130",
    "end": "340299"
  },
  {
    "text": "making it difficult for KNN to find meaningful neighbors.",
    "start": "340299",
    "end": "343975"
  },
  {
    "text": "And it can also lead to something called the peaking phenomenon,",
    "start": "343975",
    "end": "346863"
  },
  {
    "text": "where after reaching an optimal number of features,",
    "start": "346863",
    "end": "349760"
  },
  {
    "text": "adding more features just increases noise and increases classification errors,",
    "start": "349760",
    "end": "354030"
  },
  {
    "text": "especially when the sample size is small.",
    "start": "354030",
    "end": "356284"
  },
  {
    "start": "356000",
    "end": "402000"
  },
  {
    "text": "Feature selection and dimensionality reduction techniques",
    "start": "356875",
    "end": "359805"
  },
  {
    "text": "can help minimize the curse of dimensionality.",
    "start": "359805",
    "end": "362930"
  },
  {
    "text": "But if not done carefully, they can make KNN prone to another downside.",
    "start": "363110",
    "end": "368047"
  },
  {
    "text": "And that is the downside of over fitting.",
    "start": "368280",
    "end": "373288"
  },
  {
    "text": "Lower values of K can overfit the data,",
    "start": "374000",
    "end": "377630"
  },
  {
    "text": "whereas higher values of k tend to smooth out the prediction values",
    "start": "377630",
    "end": "381237"
  },
  {
    "text": "since it's averaging the values over a greater area or neighborhood.",
    "start": "381237",
    "end": "385100"
  },
  {
    "text": "So because of all this, the KNN algorithm is commonly used",
    "start": "385520",
    "end": "388940"
  },
  {
    "text": "for simple recommendation systems.",
    "start": "388941",
    "end": "391790"
  },
  {
    "text": "So for example, the algorithm can be applied",
    "start": "391940",
    "end": "396346"
  },
  {
    "text": "in the areas of data free processing.",
    "start": "396346",
    "end": "400819"
  },
  {
    "text": "That's a pretty common use case for KNN.",
    "start": "400820",
    "end": "404796"
  },
  {
    "start": "402000",
    "end": "465000"
  },
  {
    "text": "And that's because the KNN algorithm is helpful for data sets with missing values,",
    "start": "404900",
    "end": "409949"
  },
  {
    "text": "since it can estimate for those values",
    "start": "409950",
    "end": "412812"
  },
  {
    "text": "using a process known as missing data imputation.",
    "start": "412812",
    "end": "415601"
  },
  {
    "text": "Now, another use case is in finance",
    "start": "416150",
    "end": "420367"
  },
  {
    "text": "and the KNN algorithm, it's often used in stock market forecasting,",
    "start": "421232",
    "end": "426281"
  },
  {
    "text": "currency exchange rate, tradin, futures and money laundering",
    "start": "426282",
    "end": "431139"
  },
  {
    "text": "... analysis! Money laundering analysis.",
    "start": "431384",
    "end": "433795"
  },
  {
    "text": "And also we have to consider the use case for health care.",
    "start": "434294",
    "end": "440614"
  },
  {
    "text": "It's been used to make predictions on the risk of heart attacks and prostate cancer",
    "start": "441260",
    "end": "445556"
  },
  {
    "text": "by calculating the most likely gene expressions.",
    "start": "445556",
    "end": "448256"
  },
  {
    "text": "So that's KNN,",
    "start": "448574",
    "end": "451329"
  },
  {
    "text": "a simple but imperfect classification and regression classifier.",
    "start": "451329",
    "end": "455653"
  },
  {
    "text": "In the right context, it's straightforward approach is as delightful",
    "start": "455900",
    "end": "460442"
  },
  {
    "text": "as biting into a perfectly classified apple.",
    "start": "460442",
    "end": "464766"
  },
  {
    "start": "465000",
    "end": "480000"
  },
  {
    "text": "If you like this video and want to see more like it, please like and subscribe.",
    "start": "466250",
    "end": "470210"
  },
  {
    "text": "If you have any questions or want to share your thoughts about this topic,",
    "start": "471170",
    "end": "474396"
  },
  {
    "text": "please leave a comment below.",
    "start": "474396",
    "end": "475940"
  }
]