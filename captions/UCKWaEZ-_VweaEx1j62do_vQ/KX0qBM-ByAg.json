[
  {
    "text": "AI has captured the world's imagination.",
    "start": "760",
    "end": "2740"
  },
  {
    "text": "The landscape is evolving rapidly",
    "start": "3420",
    "end": "5927"
  },
  {
    "text": "as enterprises begin to leverage the technology's potential for real-world business applications.",
    "start": "5927",
    "end": "10960"
  },
  {
    "text": "Ideas in the space for creating and improving applications are expanding in every direction,",
    "start": "11884",
    "end": "20699"
  },
  {
    "text": "and at the heart of these ideas are AI accelerators, hardware accelerators,",
    "start": "22100",
    "end": "28960"
  },
  {
    "text": "which is specific hardware designed for the inferencing and AI workloads.",
    "start": "29290",
    "end": "34570"
  },
  {
    "text": "These enable the transformation because they enable faster and more efficient processing.",
    "start": "34910",
    "end": "39969"
  },
  {
    "text": "I like to think of where we are right now in the AI landscape as where the world was when the automobile was invented.",
    "start": "40990",
    "end": "48129"
  },
  {
    "text": "So at first, all automobiles basically looked the same,",
    "start": "48770",
    "end": "53557"
  },
  {
    "text": "more or less shaped like the wagons and carriages they were meant to replace,",
    "start": "53557",
    "end": "57249"
  },
  {
    "text": "but very quickly...",
    "start": "57250",
    "end": "58909"
  },
  {
    "text": "the world and industry realized, wow,",
    "start": "59300",
    "end": "62198"
  },
  {
    "text": "we could make ambulances and passenger cars and race cars and refrigerated milk trucks,",
    "start": "62198",
    "end": "68180"
  },
  {
    "text": "and all of these things have vastly different customizations in order to achieve their purpose.",
    "start": "68600",
    "end": "74680"
  },
  {
    "text": "That's really where we are with AI in 2025.",
    "start": "75560",
    "end": "78759"
  },
  {
    "text": "One size no longer fits all when it comes to AI.",
    "start": "79700",
    "end": "84780"
  },
  {
    "text": "Hardware accelerators are really great at",
    "start": "84780",
    "end": "87797"
  },
  {
    "text": "helping to right size solutions for AI",
    "start": "87797",
    "end": "92696"
  },
  {
    "text": "and to really explain the intersection between hardware acceleration",
    "start": "92696",
    "end": "97421"
  },
  {
    "text": "and AI, let's take a admittedly simplified look at the AI stack.",
    "start": "97421",
    "end": "103060"
  },
  {
    "text": "So at its fundamental level, at the bottom, we have infrastructure that's hard to spell.",
    "start": "104440",
    "end": "112579"
  },
  {
    "text": "It's hardware.",
    "start": "112840",
    "end": "113840"
  },
  {
    "text": "On top of that, of course, we have our models.",
    "start": "114820",
    "end": "117519"
  },
  {
    "text": "You've all heard of these.",
    "start": "121450",
    "end": "122450"
  },
  {
    "text": "And then at the top left, but most definitely not least, is software, management of the tasks, but also governance,",
    "start": "122890",
    "end": "130929"
  },
  {
    "text": "and governance is making sure that the AI continues to behave correctly,",
    "start": "131750",
    "end": "136722"
  },
  {
    "text": "without bias, ethically, as it evolves and develops and goes forward.",
    "start": "136722",
    "end": "142650"
  },
  {
    "text": "In addition, the software manages security, making sure that",
    "start": "142990",
    "end": "147009"
  },
  {
    "text": "the AI model data and the data that's fed in and out of those models remains private and secure.",
    "start": "147009",
    "end": "152050"
  },
  {
    "text": "Where do the accelerators fit in this stack?",
    "start": "154010",
    "end": "156250"
  },
  {
    "text": "Well, you might have guessed already, they are part of the hardware.",
    "start": "157390",
    "end": "161249"
  },
  {
    "text": "So the accelerators are here.",
    "start": "161250",
    "end": "163229"
  },
  {
    "text": "What specifically is an AI accelerator?",
    "start": "167080",
    "end": "169360"
  },
  {
    "text": "Again, this is purpose-built hardware, silicon, that's built and optimized",
    "start": "170080",
    "end": "175779"
  },
  {
    "text": "for the high-compute matrix mathematics that is necessary to do AI transactions.",
    "start": "175779",
    "end": "182699"
  },
  {
    "text": "So the linear algebra, the tensor calculations, this hardware is laid out specifically,",
    "start": "183180",
    "end": "188079"
  },
  {
    "text": "the silicon is designed to optimize and do that faster so that the inferencing answers can happen faster,",
    "start": "188540",
    "end": "195639"
  },
  {
    "text": "with reduced power consumption and a smaller physical footprint",
    "start": "196140",
    "end": "199179"
  },
  {
    "text": "compared to a general purpose piece of hardware that's designed to do everything and therefore has to cover all bases.",
    "start": "199179",
    "end": "204639"
  },
  {
    "text": "Accelerators do use all of the computer architecture, modern techniques",
    "start": "206156",
    "end": "211580"
  },
  {
    "text": "like parallel processing and optimize memory bandwidths and they interact with memory,",
    "start": "211580",
    "end": "218120"
  },
  {
    "text": "they have a large memory component.",
    "start": "218200",
    "end": "219659"
  },
  {
    "text": "It's a specific hardware engine that does the compute,",
    "start": "219660",
    "end": "222419"
  },
  {
    "text": "and that either has its own memory or it shares memory with other processors in the system.",
    "start": "222850",
    "end": "229069"
  },
  {
    "text": "Let's draw a picture of a system to really get a sense of where these accelerators might exist in an actual piece of hardware.",
    "start": "230290",
    "end": "238590"
  },
  {
    "text": "So this is a computer system.",
    "start": "239410",
    "end": "241350"
  },
  {
    "text": "It could be a server or a mainframe,",
    "start": "241570",
    "end": "243950"
  },
  {
    "text": "and like most today, let's make it an SMP so it has a variety of processor chips on it.",
    "start": "244583",
    "end": "252446"
  },
  {
    "text": "These AI accelerators, they could be located anywhere within the system.",
    "start": "253700",
    "end": "258419"
  },
  {
    "text": "For example, you might wanna have an on-die accelerator close to your processing compute.",
    "start": "259120",
    "end": "264079"
  },
  {
    "text": "You could have that on one or more of your processor compute engines",
    "start": "264820",
    "end": "269696"
  },
  {
    "text": "You could have a AI engine somewhere else in the system, not necessarily on a processor chip, but still within the box.",
    "start": "270450",
    "end": "277149"
  },
  {
    "text": "Additionally, since servers interact with the external world through industry standard IO protocols,",
    "start": "278303",
    "end": "285222"
  },
  {
    "text": "you could actually attach an AI accelerator",
    "start": "285222",
    "end": "287530"
  },
  {
    "text": "externally to the box,",
    "start": "290390",
    "end": "291550"
  },
  {
    "text": "and those accelerators could come with their own compute as well as their own memory.",
    "start": "291950",
    "end": "296850"
  },
  {
    "text": "The other thing to note about these hardware accelerators,",
    "start": "297370",
    "end": "299281"
  },
  {
    "text": "and this is what really provides that flexibility and scale that we really need in computing today",
    "start": "299281",
    "end": "304673"
  },
  {
    "text": "to enable all of these use cases and these ideas that people are coming up with in the AI industry, is the...",
    "start": "304673",
    "end": "312726"
  },
  {
    "text": "concept that each one of these AI engines doesn't have to be the exact same design.",
    "start": "313860",
    "end": "318899"
  },
  {
    "text": "Your on-chip accelerator, for example, could be optimized in a different way for a different type of workloads",
    "start": "319460",
    "end": "324483"
  },
  {
    "text": "than the one that's inside the system, which may or may not be different from the ones that you attach to the box.",
    "start": "324484",
    "end": "330060"
  },
  {
    "text": "So there's definitely a heterogeneous system of hardware and within hardware,",
    "start": "330640",
    "end": "336839"
  },
  {
    "text": "and heterogenous system of accelerators that are becoming available so that we can meet these growing AI needs.",
    "start": "336839",
    "end": "343740"
  },
  {
    "text": "To really see how these accelerators now interact with the rest of the stack,",
    "start": "345140",
    "end": "350139"
  },
  {
    "text": "let's take a couple of minutes to just think about models for a second,",
    "start": "350139",
    "end": "354160"
  },
  {
    "text": "and I think, you know, we all I think are familiar now with our traditional AI models, right?",
    "start": "355094",
    "end": "360889"
  },
  {
    "text": "Machine learning, you know, deep learning.",
    "start": "361570",
    "end": "364769"
  },
  {
    "text": "These are relatively fast.",
    "start": "365090",
    "end": "367690"
  },
  {
    "text": "They're relatively small in terms of model size,",
    "start": "369150",
    "end": "372429"
  },
  {
    "text": "and they're pretty good at coming up with, I'll say, suggestions.",
    "start": "373390",
    "end": "379530"
  },
  {
    "text": "They can't give me life advice necessarily, but they can offer suggestions.",
    "start": "383690",
    "end": "387109"
  },
  {
    "text": "Gen AI coming out in recent years, more complex, right?",
    "start": "390000",
    "end": "394420"
  },
  {
    "text": "Another level, right, they're generative, right, versus predictive, right.",
    "start": "394660",
    "end": "398959"
  },
  {
    "text": "These are things like our LLMs and simulators and things, right these are much larger models in general,",
    "start": "399000",
    "end": "405199"
  },
  {
    "text": "but they have, you know, they take more energy to run, they are more costly,",
    "start": "406300",
    "end": "410610"
  },
  {
    "text": "but they're also more powerful in terms of delivering accuracy and doing more complex things.",
    "start": "410610",
    "end": "416300"
  },
  {
    "text": "They can...",
    "start": "416620",
    "end": "417620"
  },
  {
    "text": "dare I say it, maybe not give life advice, but in some cases give, you know, advice.",
    "start": "418650",
    "end": "423650"
  },
  {
    "text": "Do some advisement.",
    "start": "423850",
    "end": "425229"
  },
  {
    "text": "Because we have this new, more complicated one,",
    "start": "427440",
    "end": "431307"
  },
  {
    "text": "it doesn't mean that we should just abandon the traditional AI because it does have a lot of value.",
    "start": "431307",
    "end": "436660"
  },
  {
    "text": "Most notably, it's best and it's small and it is less expensive, right?",
    "start": "437080",
    "end": "441659"
  },
  {
    "text": "So if we can get a good answer out of that, it is worthwhile to use it.",
    "start": "441820",
    "end": "447939"
  },
  {
    "text": "Really, right-sizing your model is where you want to be when you are doing AI.",
    "start": "448740",
    "end": "455339"
  },
  {
    "text": "If you have a AI task that's this size and your model is little,",
    "start": "457190",
    "end": "465342"
  },
  {
    "text": "you run the risk of, you might get an answer fast, but that answer might be wrong,",
    "start": "465342",
    "end": "470437"
  },
  {
    "text": "or it might come back with more risk than your application can tolerate.",
    "start": "470437",
    "end": "473749"
  },
  {
    "text": "So you might say, okay, I'll err on the side of caution and I'll just always use a huge model,",
    "start": "474790",
    "end": "479502"
  },
  {
    "text": "but you're not being cost effective and you're being sustainable when you do this.",
    "start": "479502",
    "end": "484509"
  },
  {
    "text": "In addition, you run the risk,",
    "start": "484510",
    "end": "486790"
  },
  {
    "text": "if you happen to be attempting to do something in the same compute space as your running system,",
    "start": "487130",
    "end": "494276"
  },
  {
    "text": "you run the risk of kneecapping other simultaneously running workloads on your box if you use too large of a model.",
    "start": "494276",
    "end": "501010"
  },
  {
    "text": "So ideally, what we wanna go for and try to do is to right size the model to the task that we're doing.",
    "start": "501950",
    "end": "508089"
  },
  {
    "text": "Ok, how do hardware accelerators help with that?",
    "start": "511785",
    "end": "516749"
  },
  {
    "text": "To understand that, it helps to think about the model the way hardware thinks about the model,",
    "start": "518450",
    "end": "523344"
  },
  {
    "text": "which is that the model really, at the end of the day, is data,",
    "start": "523344",
    "end": "527909"
  },
  {
    "text": "and when you think about hardware and how you optimize hardware, what we think about generally is performance,",
    "start": "528350",
    "end": "534600"
  },
  {
    "text": "a common metric of performance in the hardware world,",
    "start": "534601",
    "end": "538302"
  },
  {
    "text": "familiar to me, I'm a hardware person,",
    "start": "538490",
    "end": "540409"
  },
  {
    "text": "If you couldn't tell already, is latency or we call it response time, right?",
    "start": "540410",
    "end": "546539"
  },
  {
    "text": "How fast do you respond?",
    "start": "546560",
    "end": "548100"
  },
  {
    "text": "And when you're measuring response time as a function of size,",
    "start": "548780",
    "end": "553027"
  },
  {
    "text": "right, the size of your working set, the size of your data, the of your model,",
    "start": "553027",
    "end": "557178"
  },
  {
    "text": "basically, generally, the bigger that model gets, the worse your response time gets, right.",
    "start": "557178",
    "end": "563558"
  },
  {
    "text": "It takes longer and longer to get the answer out there, right?",
    "start": "563660",
    "end": "567222"
  },
  {
    "text": "If we try to use a similar graph to measure performance of AI, it's not quite as linear,",
    "start": "568310",
    "end": "577470"
  },
  {
    "text": "because what we're trying to do really when we're measuring how well AI performs is we really say, well, how effective is it?",
    "start": "578490",
    "end": "585308"
  },
  {
    "text": "And if you try to draw a graph like this, it would end up looking a little bit more confusing,",
    "start": "586250",
    "end": "593590"
  },
  {
    "text": "and you kind of have to think, well what's going on there?",
    "start": "594410",
    "end": "596089"
  },
  {
    "text": "What's going on there is response time,",
    "start": "596530",
    "end": "599090"
  },
  {
    "text": "hardware performance is not the only factor",
    "start": "599610",
    "end": "603099"
  },
  {
    "text": "when you're talking about measuring the success or the effectiveness of the AI.",
    "start": "603099",
    "end": "608470"
  },
  {
    "text": "It's really accuracy as well as performance.",
    "start": "608890",
    "end": "611710"
  },
  {
    "text": "And the variable that's hidden here is a third dimension.",
    "start": "612270",
    "end": "616089"
  },
  {
    "text": "Bear with me as I try to draw a third-dimension.",
    "start": "616770",
    "end": "619149"
  },
  {
    "text": "Which we touched on before I hinted at it.",
    "start": "620250",
    "end": "622049"
  },
  {
    "text": "It's real your use case,",
    "start": "622150",
    "end": "623409"
  },
  {
    "text": "and that green line is really coming out in the third dimension here.",
    "start": "626250",
    "end": "628669"
  },
  {
    "text": "So the questions you want to be asking when you're trying to decide what model to use are,",
    "start": "629185",
    "end": "634529"
  },
  {
    "text": "how much risk do I need?",
    "start": "637250",
    "end": "638409"
  },
  {
    "text": "If I'm a credit card fraud detection system, you know,",
    "start": "639050",
    "end": "642930"
  },
  {
    "text": "I have a different risk profile, I have different inputs, I have levels of",
    "start": "642930",
    "end": "648753"
  },
  {
    "text": "tolerance than I do if I'm batch document summarization process, right?",
    "start": "648753",
    "end": "653230"
  },
  {
    "text": "So those factors drive the need for models that align with specific use cases.",
    "start": "653630",
    "end": "658070"
  },
  {
    "text": "So at the end of the day, what's happening is your use case is dictating your model size,",
    "start": "658710",
    "end": "664529"
  },
  {
    "text": "and if you have knowledge of your model size, and you have access to these hardware accelerators,",
    "start": "664530",
    "end": "671891"
  },
  {
    "text": "then you can leverage these hardware accelerator and run",
    "start": "671891",
    "end": "675886"
  },
  {
    "text": "the model that you need on the optimized hardware accelerator for the task at hand.",
    "start": "675886",
    "end": "681259"
  },
  {
    "text": "Let me illustrate all this with an example.",
    "start": "682180",
    "end": "684940"
  },
  {
    "text": "It's a common example, but it's one of my favorites because it's very personal and it resonates,",
    "start": "685560",
    "end": "690294"
  },
  {
    "text": "I think with everybody, it's credit card fraud detect, right?",
    "start": "690294",
    "end": "693120"
  },
  {
    "text": "I wanna use my credit card online all the time,",
    "start": "693240",
    "end": "695620"
  },
  {
    "text": "and I want to hit pay now and I wanted to go through as long as it's me.",
    "start": "696070",
    "end": "699290"
  },
  {
    "text": "If it's not me, I want it to stop.",
    "start": "699390",
    "end": "700849"
  },
  {
    "text": "So that task is fairly complex.",
    "start": "701870",
    "end": "706589"
  },
  {
    "text": "It also has a high dependence on response time because",
    "start": "706978",
    "end": "712307"
  },
  {
    "text": "I'm gonna navigate away if it takes too long to use this site, I'll go someplace else, right.",
    "start": "712307",
    "end": "718329"
  },
  {
    "text": "So let's illustrate how we might use AI models and hardware accelerators to optimize for that use case.",
    "start": "718735",
    "end": "727308"
  },
  {
    "text": "So we have a task here,",
    "start": "728070",
    "end": "731409"
  },
  {
    "text": "nd really, that task is, is it fraud?",
    "start": "732590",
    "end": "734870"
  },
  {
    "text": "All right.I want to get the answer as fast as possible.",
    "start": "736670",
    "end": "739010"
  },
  {
    "text": "So maybe I'll start by using a.",
    "start": "739330",
    "end": "741549"
  },
  {
    "text": "ML DL model.",
    "start": "744189",
    "end": "745189"
  },
  {
    "text": "This will be fast, it'll have a low response time, and it'll have a good forecast of whether or not this is fraud or not.",
    "start": "745890",
    "end": "753610"
  },
  {
    "text": "Maybe it'll be 80% I'm making these numbers up,",
    "start": "754070",
    "end": "756730"
  },
  {
    "text": "successful.",
    "start": "758470",
    "end": "759470"
  },
  {
    "text": "That model, if I'm doing something like this,",
    "start": "761010",
    "end": "763225"
  },
  {
    "text": "I will probably, if have it, want to use a hardware accelerator",
    "start": "763225",
    "end": "767112"
  },
  {
    "text": "that's on the same processor chip as the transaction workload that's running.",
    "start": "767112",
    "end": "772009"
  },
  {
    "text": "The reason is that transaction workload already has the data",
    "start": "772630",
    "end": "775990"
  },
  {
    "text": "that needs to go into that model, and the model's a relatively small size,",
    "start": "775990",
    "end": "779386"
  },
  {
    "text": "so you can pull that data into the cache right there in the processor chip",
    "start": "779386",
    "end": "782303"
  },
  {
    "text": "and enable in real time actually getting, you know, your suggestion of whether or not this is fraud,",
    "start": "782303",
    "end": "787910"
  },
  {
    "text": "yery quickly.",
    "start": "789070",
    "end": "790070"
  },
  {
    "text": "Then I'd say 80% of the time, that answer comes back.",
    "start": "791741",
    "end": "796059"
  },
  {
    "text": "They're like, yeah, this is you.",
    "start": "796180",
    "end": "797820"
  },
  {
    "text": "This is a valid purchase.",
    "start": "798080",
    "end": "799200"
  },
  {
    "text": "I'm gonna approve it.",
    "start": "799520",
    "end": "800700"
  },
  {
    "text": "And then you get your toy that you bought and nobody loses any money and everything is good,",
    "start": "800980",
    "end": "806079"
  },
  {
    "text": "but maybe 20% of time, the model says, hmm, I'm not sure.",
    "start": "807079",
    "end": "813459"
  },
  {
    "text": "Some things look legit, but some things look suspicious.",
    "start": "813780",
    "end": "815779"
  },
  {
    "text": "We need to take a deeper look at this.",
    "start": "816460",
    "end": "817899"
  },
  {
    "text": "At that point, then, you could decide that now you know it's worth the investment",
    "start": "818430",
    "end": "823761"
  },
  {
    "text": "of going into a more expensive gen AI model,",
    "start": "823762",
    "end": "826950"
  },
  {
    "text": "and that model requires a lot more memory because it's a much bigger model.",
    "start": "827990",
    "end": "832128"
  },
  {
    "text": "So you wouldn't be using the AI engine in this case that you had on your die.",
    "start": "832510",
    "end": "837429"
  },
  {
    "text": "You might want to use an AI engine that's on a different die that's not running a",
    "start": "837850",
    "end": "841336"
  },
  {
    "text": "performance critical workload, or maybe one that's in a different place on the system,",
    "start": "841336",
    "end": "845269"
  },
  {
    "text": "or maybe one attached and sharded through the IO peripherals that has its own memory.",
    "start": "845650",
    "end": "850549"
  },
  {
    "text": "Based off of that, if you have that flexibility, then you can decide which to use and the hardware can use that,",
    "start": "851770",
    "end": "857789"
  },
  {
    "text": "and then at the end of that transaction, it can come out and say, no, it was good.",
    "start": "858050",
    "end": "862808"
  },
  {
    "text": "It's okay, it turns out that that was the right one.",
    "start": "863270",
    "end": "865149"
  },
  {
    "text": "So the idea here is using more than one model, this concept of multi-model AI, or ensemble AI,",
    "start": "865490",
    "end": "873570"
  },
  {
    "text": "can really be supported and enabled and optimized",
    "start": "874480",
    "end": "878837"
  },
  {
    "text": "by having these hardware accelerators that are optimized for different model types,",
    "start": "878838",
    "end": "883759"
  },
  {
    "text": "and then, when you think about when you're doing AI, the last thing to think about",
    "start": "884380",
    "end": "887653"
  },
  {
    "text": "here is, you know, we're not doing one of these a day, right?",
    "start": "887653",
    "end": "890480"
  },
  {
    "text": "We're doing millions of these day, many of them at the same time, right.",
    "start": "890500",
    "end": "894960"
  },
  {
    "text": "So you want to have all of these processing AI engines running all at the time, doing different things,",
    "start": "895140",
    "end": "900499"
  },
  {
    "text": "and then the next second or millisecond, It's doing a completely different model with a completely difference task.",
    "start": "900499",
    "end": "905169"
  },
  {
    "text": "So hardware accelerators give you the flexibility to do that at scale,",
    "start": "905650",
    "end": "909909"
  },
  {
    "text": "and because of that, they intersect well with the AI stack as we know it today",
    "start": "910350",
    "end": "915121"
  },
  {
    "text": "and are really starting to really be used to",
    "start": "915121",
    "end": "918220"
  },
  {
    "text": "ensure that we're continuing to expand and meet business need by deploying scalable,",
    "start": "918221",
    "end": "923373"
  },
  {
    "text": "efficient and secure AI to meet today's business problems and tomorrow's.",
    "start": "923373",
    "end": "927429"
  }
]