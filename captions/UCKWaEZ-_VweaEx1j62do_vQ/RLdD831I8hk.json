[
  {
    "text": "Running these generative AI algorithms at scale \ncan be very challenging,",
    "start": "120",
    "end": "3787"
  },
  {
    "text": "overwhelming, and costly.",
    "start": "3788",
    "end": "5133"
  },
  {
    "text": "In fact, there's three areas that \nI want to highlight",
    "start": "5133",
    "end": "7571"
  },
  {
    "text": "where there's exponential growth occurring over time.",
    "start": "7700",
    "end": "11718"
  },
  {
    "text": "And if I were to log scale this here,",
    "start": "11718",
    "end": "13906"
  },
  {
    "text": "then it would look much like this here.",
    "start": "13906",
    "end": "16336"
  },
  {
    "text": "But the first one would be the model size.",
    "start": "16594",
    "end": "20628"
  },
  {
    "text": "So at the beginning, these models, \nthey were thousands of parameters. ",
    "start": "20923",
    "end": "25720"
  },
  {
    "text": "Then they move to millions and \nnow billions and even trillions. ",
    "start": "25720",
    "end": "28886"
  },
  {
    "text": "But this requires specialized hardware in order \nto even run and train",
    "start": "28960",
    "end": "32805"
  },
  {
    "text": "these very large algorithms.",
    "start": "32805",
    "end": "34527"
  },
  {
    "text": "Now the second one would be data size.",
    "start": "34711",
    "end": "37706"
  },
  {
    "text": "Now the data size is growing.",
    "start": "38149",
    "end": "39940"
  },
  {
    "text": "You know, if you think about Granite and you think \nabout Llama,",
    "start": "39940",
    "end": "43573"
  },
  {
    "text": "but a human in turn can read about a million different words every single year.",
    "start": "43574",
    "end": "49072"
  },
  {
    "text": "Now, a system such as an algorithm like this",
    "start": "49238",
    "end": "51495"
  },
  {
    "text": "can read about a billion times ten to the 6th,",
    "start": "51495",
    "end": "53576"
  },
  {
    "text": "or 6 orders of magnitude more in just a single month.",
    "start": "53576",
    "end": "56961"
  },
  {
    "text": "We're actually beginning to run out of data, by 2030",
    "start": "57108",
    "end": "60255"
  },
  {
    "text": "I think that we're going to see synthetic data actually overtake real world data.",
    "start": "60384",
    "end": "65093"
  },
  {
    "text": "Now the third one is demand.",
    "start": "65388",
    "end": "67345"
  },
  {
    "text": "So over time, you have seen that these models have \nbecome integral to our daily lives.",
    "start": "67456",
    "end": "74788"
  },
  {
    "text": "So when you look at ChatGPT and just five days  after it was released,",
    "start": "74880",
    "end": "78979"
  },
  {
    "text": "it had 1 million users.",
    "start": "78980",
    "end": "80477"
  },
  {
    "text": "And if you look about a year later,",
    "start": "80477",
    "end": "82867"
  },
  {
    "text": "there were about 100 million users.",
    "start": "82867",
    "end": "84828"
  },
  {
    "text": "So every time we think about having to write, you know, a piece,",
    "start": "84829",
    "end": "88163"
  },
  {
    "text": "but we can solve this what's \ncalled a whitespace problem,",
    "start": "88163",
    "end": "90510"
  },
  {
    "text": "by using these models to help prompt us or to tell us",
    "start": "90510",
    "end": "93208"
  },
  {
    "text": "what we should think about and how we should write.",
    "start": "93208",
    "end": "95868"
  },
  {
    "text": "But now if I take these and I \nmultiply them together, well,  ",
    "start": "95960",
    "end": "100240"
  },
  {
    "text": "this gives us this unfathomable compute scale \nthat we need in order to run them.",
    "start": "100240",
    "end": "106014"
  },
  {
    "text": "And again this is a log scale here.",
    "start": "106290",
    "end": "108757"
  },
  {
    "text": "Now what this means is that we need this agentic architecture",
    "start": "108757",
    "end": "112480"
  },
  {
    "text": "to run these specialized models,",
    "start": "112480",
    "end": "114465"
  },
  {
    "text": "and they help solve the problem of \ntrying to make these more usable.",
    "start": "114465",
    "end": "118173"
  },
  {
    "text": "So what can we really do to help,",
    "start": "118320",
    "end": "120362"
  },
  {
    "text": "to make this more manageable and usable systems for inference and so on.",
    "start": "120362",
    "end": "125220"
  },
  {
    "text": "Well, let's go ahead and find out next.",
    "start": "125312",
    "end": "128157"
  },
  {
    "text": "Generative AI algorithms can be \nscaled across hundreds of GPUs.",
    "start": "128508",
    "end": "131879"
  },
  {
    "text": "In fact, you can put them on V100's, A100's,",
    "start": "131880",
    "end": "134880"
  },
  {
    "text": "or even look at the different \nH series that Nvidia provides",
    "start": "134880",
    "end": "138520"
  },
  {
    "text": "or even other vendors.",
    "start": "138520",
    "end": "139960"
  },
  {
    "text": "But even so, with hundreds of thousands",
    "start": "139960",
    "end": "142080"
  },
  {
    "text": "of different types of requests per second,",
    "start": "142080",
    "end": "144080"
  },
  {
    "text": "this can strain the system,",
    "start": "144080",
    "end": "145600"
  },
  {
    "text": "but it could also strain the underlying hardware.",
    "start": "145600",
    "end": "147840"
  },
  {
    "text": "So to help out,",
    "start": "147840",
    "end": "148840"
  },
  {
    "text": "let's look at a couple of \nstrategies that we can take.",
    "start": "148840",
    "end": "151369"
  },
  {
    "text": "Now, the first one is called a \nbatch-based generative AI system.",
    "start": "151498",
    "end": "157142"
  },
  {
    "text": "Now here what happens is we want to create",
    "start": "157308",
    "end": "160560"
  },
  {
    "text": "these very dynamic fill-in-the-blank sentences",
    "start": "160560",
    "end": "163120"
  },
  {
    "text": "that come from the output of \nthese large language models.",
    "start": "163120",
    "end": "166040"
  },
  {
    "text": "We then store them on a content delivery network.",
    "start": "166040",
    "end": "168680"
  },
  {
    "text": "And this is really cached all around the world.",
    "start": "168680",
    "end": "170827"
  },
  {
    "text": "And then on the edge",
    "start": "170920",
    "end": "172120"
  },
  {
    "text": "we will then pull in all of \nthose fill-in-the-blank sentences",
    "start": "172120",
    "end": "175400"
  },
  {
    "text": "and we'll insert the personalized information",
    "start": "175400",
    "end": "178079"
  },
  {
    "text": "and then serve it over to the user.",
    "start": "178080",
    "end": "179560"
  },
  {
    "text": "So this becomes a very personalized experience.",
    "start": "179560",
    "end": "182588"
  },
  {
    "text": "Now the second one is cache-based generative AI.",
    "start": "182680",
    "end": "188862"
  },
  {
    "text": "Now here, the whole strategy \nis that we want to cache",
    "start": "189102",
    "end": "191960"
  },
  {
    "text": "as much content as we possibly can",
    "start": "191960",
    "end": "194320"
  },
  {
    "text": "on servers that are around the world on a CDN.",
    "start": "194320",
    "end": "197640"
  },
  {
    "text": "And what we do is we want to \nfind the most common cases",
    "start": "197640",
    "end": "200120"
  },
  {
    "text": "that we can generate content \nfor, and then push that up.",
    "start": "200120",
    "end": "203680"
  },
  {
    "text": "And that means that the least \ntype of content that we generate,",
    "start": "203680",
    "end": "207239"
  },
  {
    "text": "we want to do that on-demand",
    "start": "207240",
    "end": "209040"
  },
  {
    "text": "and then, in turn, serve it.",
    "start": "209040",
    "end": "210400"
  },
  {
    "text": "But this gives us kind of the \nbest of both worlds where we can",
    "start": "210474",
    "end": "213080"
  },
  {
    "text": "cut in half maybe 90% of \nthe requests per second, ",
    "start": "213080",
    "end": "215920"
  },
  {
    "text": "and then the other 10% is created on-demand.",
    "start": "215920",
    "end": "219520"
  },
  {
    "text": "Now the other approach",
    "start": "219520",
    "end": "220840"
  },
  {
    "text": "would be what's called an \"agentic architecture\".",
    "start": "220840",
    "end": "225000"
  },
  {
    "text": "And this type of architecture, it's emerging,",
    "start": "225000",
    "end": "228600"
  },
  {
    "text": "but it's where you take these \nvery large, complex models",
    "start": "228600",
    "end": "231400"
  },
  {
    "text": "and you can break them down into smaller models",
    "start": "231400",
    "end": "233959"
  },
  {
    "text": "so that they're specialized.",
    "start": "233960",
    "end": "235080"
  },
  {
    "text": "So it's almost like a mixture of experts.",
    "start": "235080",
    "end": "237200"
  },
  {
    "text": "And these agents, in turn, \ncan communicate to each other.",
    "start": "237200",
    "end": "239720"
  },
  {
    "text": "One example would be",
    "start": "240404",
    "end": "241954"
  },
  {
    "text": "having a large language model \njudge the output of another large language model.",
    "start": "241954",
    "end": "246603"
  },
  {
    "text": "You could even have another large language model",
    "start": "246733",
    "end": "248880"
  },
  {
    "text": "be self-introspective and then pass the output off",
    "start": "248880",
    "end": "252440"
  },
  {
    "text": "onto another specialized \nmodel that could transform",
    "start": "252440",
    "end": "255440"
  },
  {
    "text": "that information that's then in turn served up.",
    "start": "255440",
    "end": "257834"
  },
  {
    "text": "But these smaller models then \nrequire smaller footprints",
    "start": "258000",
    "end": "261760"
  },
  {
    "text": "that then can run and be scaled across these",
    "start": "261760",
    "end": "264440"
  },
  {
    "text": "hundreds of different types of GPUs.",
    "start": "264440",
    "end": "267373"
  },
  {
    "text": "Now, these types of models,",
    "start": "267520",
    "end": "268560"
  },
  {
    "text": "they may not run on these commodity machines",
    "start": "268560",
    "end": "271400"
  },
  {
    "text": "or any of the available GPUs that \nyou might have for your teams,",
    "start": "271400",
    "end": "274000"
  },
  {
    "text": "such as a 32GB GPU chip.",
    "start": "274000",
    "end": "277280"
  },
  {
    "text": "Now some of them can,",
    "start": "277280",
    "end": "278360"
  },
  {
    "text": "there are some Granite models that are smaller,",
    "start": "278360",
    "end": "280639"
  },
  {
    "text": "even some Llama models that could fit,",
    "start": "280640",
    "end": "282680"
  },
  {
    "text": "and other vendors also have pieces.",
    "start": "282680",
    "end": "285240"
  },
  {
    "text": "But the vast majority of some \nof the most powerful models",
    "start": "285240",
    "end": "287880"
  },
  {
    "text": "you need to get to run and \nother types of hardware.",
    "start": "287880",
    "end": "292560"
  },
  {
    "text": "Now to combat some of this,",
    "start": "292560",
    "end": "293880"
  },
  {
    "text": "one of the options is a technique \nthat's called model distillation.",
    "start": "293880",
    "end": "298500"
  },
  {
    "text": "But the whole idea around this",
    "start": "298684",
    "end": "301000"
  },
  {
    "text": "is that we want to be able \nto extract the information",
    "start": "301000",
    "end": "304160"
  },
  {
    "text": "that really matters to the \ndomain of which we're working on.",
    "start": "304160",
    "end": "306680"
  },
  {
    "text": "We can take that information and \nwe can do in-context learning.",
    "start": "306680",
    "end": "310560"
  },
  {
    "text": "But traditionally you would want to teach",
    "start": "310560",
    "end": "312720"
  },
  {
    "text": "another smaller model through gradient update.",
    "start": "312720",
    "end": "315720"
  },
  {
    "text": "So it becomes much more powerful and fine tuned.",
    "start": "315720",
    "end": "318600"
  },
  {
    "text": "And can be applied to your \nproblem in a very accurate way.",
    "start": "318600",
    "end": "322920"
  },
  {
    "text": "Now, the second method that we could do",
    "start": "322920",
    "end": "324880"
  },
  {
    "text": "is called a student teacher approach.",
    "start": "324880",
    "end": "329640"
  },
  {
    "text": "And here, instead of looking at the data per se,",
    "start": "330600",
    "end": "333240"
  },
  {
    "text": "we want to create what's called a new behavior.",
    "start": "333240",
    "end": "336360"
  },
  {
    "text": "And we can create a new skill or a composite skill",
    "start": "336360",
    "end": "339680"
  },
  {
    "text": "based on the task that we want to have.",
    "start": "339680",
    "end": "341360"
  },
  {
    "text": "It could be text extraction.",
    "start": "341360",
    "end": "342759"
  },
  {
    "text": "It could be summarization.",
    "start": "342760",
    "end": "344600"
  },
  {
    "text": "It could be just fluent writing.",
    "start": "344600",
    "end": "346840"
  },
  {
    "text": "But having a model that's the teacher",
    "start": "346840",
    "end": "349480"
  },
  {
    "text": "that might would know some of the task.",
    "start": "349480",
    "end": "351720"
  },
  {
    "text": "And you could even have a bank of models",
    "start": "351720",
    "end": "353400"
  },
  {
    "text": "where your teacher models",
    "start": "354400",
    "end": "356479"
  },
  {
    "text": "would then be asked question \nby your student models.",
    "start": "356480",
    "end": "358800"
  },
  {
    "text": "So the data would flow this way,",
    "start": "358800",
    "end": "361159"
  },
  {
    "text": "and then the output from your teacher model",
    "start": "361160",
    "end": "363200"
  },
  {
    "text": "will go back to the student so \nthat it could learn over time",
    "start": "363200",
    "end": "366160"
  },
  {
    "text": "and develop those types of \nskills that it would need.",
    "start": "366160",
    "end": "370480"
  },
  {
    "text": "And now if I want to look at another approach,",
    "start": "370480",
    "end": "372791"
  },
  {
    "text": "where I want to shrink a model,",
    "start": "372791",
    "end": "375000"
  },
  {
    "text": "quantization is a nice approach.",
    "start": "375179",
    "end": "378759"
  },
  {
    "text": "So here is where I want to compress a model",
    "start": "378760",
    "end": "381360"
  },
  {
    "text": "into a much smaller footprint.",
    "start": "381360",
    "end": "384159"
  },
  {
    "text": "So I might take these 32-bit",
    "start": "384160",
    "end": "386960"
  },
  {
    "text": "floating point numbers and I \nwant to make them much smaller.",
    "start": "386960",
    "end": "389840"
  },
  {
    "text": "So I might want to make it \ninto an 8-bit representation",
    "start": "389840",
    "end": "392488"
  },
  {
    "text": "of that floating point number.",
    "start": "392489",
    "end": "393800"
  },
  {
    "text": "Or I could do a 4-bit representation.",
    "start": "393800",
    "end": "396919"
  },
  {
    "text": "Now there are different pros and cons of the order",
    "start": "396994",
    "end": "398874"
  },
  {
    "text": "in which you do it.",
    "start": "398874",
    "end": "399639"
  },
  {
    "text": "So one of them could be you \nshrink it before training.",
    "start": "399769",
    "end": "403089"
  },
  {
    "text": "Now this requires more compute \nresources when you do train,",
    "start": "403200",
    "end": "406840"
  },
  {
    "text": "but it creates a smaller model",
    "start": "406840",
    "end": "409216"
  },
  {
    "text": "and it still can maintain a \nlot of the accuracy levels,",
    "start": "409308",
    "end": "412720"
  },
  {
    "text": "whichever way you measure accuracy,",
    "start": "412720",
    "end": "415360"
  },
  {
    "text": "whenever you apply that at inference time.",
    "start": "415360",
    "end": "418159"
  },
  {
    "text": "Now, you could also do this post training.",
    "start": "418160",
    "end": "421120"
  },
  {
    "text": "Now, this is, you know, \nlesser requirements on compute",
    "start": "421120",
    "end": "425040"
  },
  {
    "text": "when you train, but when you \ndo and apply this at inference,",
    "start": "425040",
    "end": "428600"
  },
  {
    "text": "your accuracy might go down, right?",
    "start": "428600",
    "end": "430160"
  },
  {
    "text": "So those trade offs,",
    "start": "430160",
    "end": "431520"
  },
  {
    "text": "is something that \nyou do need to keep in mind",
    "start": "431520",
    "end": "433806"
  },
  {
    "text": "as you apply this compression technique.",
    "start": "433807",
    "end": "437087"
  },
  {
    "text": "If you liked this video and \nwant to see more like it,",
    "start": "437360",
    "end": "439599"
  },
  {
    "text": "please like and subscribe.",
    "start": "439600",
    "end": "441120"
  },
  {
    "text": "If you have any questions or want to \nshare your thoughts about this topic,",
    "start": "441120",
    "end": "444160"
  },
  {
    "text": "please leave a comment below.",
    "start": "444160",
    "end": "445946"
  }
]