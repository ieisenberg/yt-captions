[
  {
    "text": "This is how to build your first",
    "start": "60",
    "end": "1229"
  },
  {
    "text": "LLM powered agent using the IBM Bee framework.",
    "start": "1230",
    "end": "3928"
  },
  {
    "text": "So our research labs have been cooking up a React agent framework",
    "start": "3930",
    "end": "6871"
  },
  {
    "text": "that allows you to use a bunch of tools,",
    "start": "6872",
    "end": "8438"
  },
  {
    "text": "work with different LLMs, memory and logging.",
    "start": "8438",
    "end": "11065"
  },
  {
    "text": "Pretty much everything needed to have a great agent, but it gets better.",
    "start": "11070",
    "end": "14467"
  },
  {
    "text": "There's a bunch of features that are going to help you",
    "start": "14467",
    "end": "16087"
  },
  {
    "text": "if you're trying to do this for real.",
    "start": "16087",
    "end": "17395"
  },
  {
    "text": "I'll come back to these in a sec.",
    "start": "17400",
    "end": "18539"
  },
  {
    "text": "I'm going to show you everything you need to know about it in a few minutes.",
    "start": "18540",
    "end": "21492"
  },
  {
    "text": "You're probably thinking, why wouldn't I just use long chain or can I use open source LLMs?",
    "start": "21549",
    "end": "25291"
  },
  {
    "text": "Or is it just limited to IBM stuff?",
    "start": "25291",
    "end": "27504"
  },
  {
    "text": "It's written in TypeScript and I haven't touched the language since my startup crashed and burned.",
    "start": "27510",
    "end": "31426"
  },
  {
    "text": "So this is going to be fun.",
    "start": "31470",
    "end": "32748"
  },
  {
    "text": "I'm going to break it down in just three simple steps,",
    "start": "32790",
    "end": "34690"
  },
  {
    "text": "and it begins with generating an answer using string.",
    "start": "34690",
    "end": "37355"
  },
  {
    "text": "Now my code just wants to belong.",
    "start": "37355",
    "end": "39165"
  },
  {
    "text": "So I'm going to create a new file called flow.ts to hold It.",
    "start": "39165",
    "end": "42147"
  },
  {
    "text": "There are a number of LLM adapters in the Bee framework,",
    "start": "42199",
    "end": "44638"
  },
  {
    "text": "including Grok, Langchain, Ollama, OpenAI and Watsonx.ai",
    "start": "44638",
    "end": "48000"
  },
  {
    "text": "I'm going to use the latter.",
    "start": "48073",
    "end": "48825"
  },
  {
    "text": "I've already got my API key and project ID.",
    "start": "48825",
    "end": "50847"
  },
  {
    "text": "I'll make them available to the process by using",
    "start": "50847",
    "end": "52829"
  },
  {
    "text": "the .env.config method.",
    "start": "52830",
    "end": "54509"
  },
  {
    "text": "And while I'm at it I'll bring in the Watson Chat LLM class.",
    "start": "54510",
    "end": "57357"
  },
  {
    "text": "I can connect in LLM on watsonx.ai.",
    "start": "57360",
    "end": "60175"
  },
  {
    "text": "Now rather than using any old model, I can specify the Llama 3.1 70 b instruct preset via the from preset class here.",
    "start": "60175",
    "end": "66221"
  },
  {
    "text": "I can also set parameters like the decoding method and the max tokens.",
    "start": "66221",
    "end": "69234"
  },
  {
    "text": "My goal right now is to just generate output using a prompt.",
    "start": "69234",
    "end": "71993"
  },
  {
    "text": "To do this, I'm going to use the LLM.stream method.",
    "start": "72000",
    "end": "74703"
  },
  {
    "text": "Given I'm coding in TypeScript, I need to import the base message and the role primitive to form a prompt.",
    "start": "74703",
    "end": "79345"
  },
  {
    "text": "I can then throw together an asynchronous function and call the LLM stream method.",
    "start": "79350",
    "end": "83058"
  },
  {
    "text": "To that I can pass through my prompt.",
    "start": "83058",
    "end": "84776"
  },
  {
    "text": "Who is Nicolas Renotte?",
    "start": "84780",
    "end": "85794"
  },
  {
    "text": "Just like that time I had a Carolina Reaper before a 16 hour flight,",
    "start": "85794",
    "end": "89338"
  },
  {
    "text": "I've made a catastrophic error.",
    "start": "89340",
    "end": "90928"
  },
  {
    "text": "I haven't initialized my project yet or installed any dependencies.",
    "start": "90930",
    "end": "94048"
  },
  {
    "text": "Let's fix that.",
    "start": "94050",
    "end": "94757"
  },
  {
    "text": "I'll initialize the node project by running yarn in it",
    "start": "94757",
    "end": "97168"
  },
  {
    "text": "and install TSX.env,",
    "start": "97170",
    "end": "99118"
  },
  {
    "text": "the Bee agent framework and the IBM generative A.I. Node SDK.",
    "start": "99120",
    "end": "103257"
  },
  {
    "text": "I'm going to make one quick tweak to the packages.json file and add a script called Flow, which runs the flow.ts file using TSX.",
    "start": "103260",
    "end": "110260"
  },
  {
    "text": "If I run \"yarn run flow\", I get an okay result from the LLM,",
    "start": "110260",
    "end": "114596"
  },
  {
    "text": "but it doesn't really know me and it doesn't have access to the net to find out.",
    "start": "114596",
    "end": "118161"
  },
  {
    "text": "So I've got the streaming working.",
    "start": "118284",
    "end": "119398"
  },
  {
    "text": "But let's be real.",
    "start": "119400",
    "end": "120077"
  },
  {
    "text": "You're watching this for agents and so far I haven't quite delivered.",
    "start": "120077",
    "end": "123538"
  },
  {
    "text": "This is all about to change in Part 2: building an agent with function call.",
    "start": "123540",
    "end": "126778"
  },
  {
    "text": "Let's change it up.",
    "start": "126778",
    "end": "127587"
  },
  {
    "text": "I can bring the Bee agent from the framework and begin creating a new instance of the LLM agent.",
    "start": "127590",
    "end": "132207"
  },
  {
    "text": "The first parameter I'll pass is my existing LLM.",
    "start": "132210",
    "end": "134488"
  },
  {
    "text": "Sometimes you'd rather not know who and what you texted after a big night out.",
    "start": "134490",
    "end": "137807"
  },
  {
    "text": "But for our LLM adding memory is going to provide a lot of context,",
    "start": "137807",
    "end": "140908"
  },
  {
    "text": "so I'll import the token memory class and add it to the agent.",
    "start": "140908",
    "end": "143428"
  },
  {
    "text": "Now tools. I can bring in the DuckDuckGoSearchTool to access the net,",
    "start": "143430",
    "end": "146913"
  },
  {
    "text": "and the OpenMeteoTool for all things weather.",
    "start": "146913",
    "end": "149358"
  },
  {
    "text": "Again, I'll add these as a new parameter to the LLM.",
    "start": "149358",
    "end": "151618"
  },
  {
    "text": "Now to bring it all together.",
    "start": "151618",
    "end": "152724"
  },
  {
    "text": "We'll get rid of the function we wrote for the baseline generation and use the agent.",
    "start": "152724",
    "end": "156411"
  },
  {
    "text": "There's two methods I need to run the agent: .run and .observe.",
    "start": "156420",
    "end": "159298"
  },
  {
    "text": "To the run method I'll pass the prompt and execution parameters like number of retries.",
    "start": "159300",
    "end": "163317"
  },
  {
    "text": "Then I'll use the emitter.",
    "start": "163320",
    "end": "164459"
  },
  {
    "text": "This allows me to see what's happening at each stage of the agent workflow.",
    "start": "164460",
    "end": "167593"
  },
  {
    "text": "Each time there's a completed action, I'll be able to see the status by observing the update action.",
    "start": "167593",
    "end": "171744"
  },
  {
    "text": "In this case, I'll console.log the update key and the update value.",
    "start": "171750",
    "end": "174748"
  },
  {
    "text": "This will show things like the output from the function and the final response.",
    "start": "174750",
    "end": "177871"
  },
  {
    "text": "And last but not least, I'll console.log the final result text for good measure.",
    "start": "177871",
    "end": "181255"
  },
  {
    "text": "I can ask when IBM was founded",
    "start": "181260",
    "end": "183024"
  },
  {
    "text": "and after searching the net using DuckDuckGo,",
    "start": "183024",
    "end": "185567"
  },
  {
    "text": "we get the correct response.",
    "start": "185567",
    "end": "186803"
  },
  {
    "text": "Likewise, if we want the agent to use the weather tool, I can ask what the weather is like in New York",
    "start": "186803",
    "end": "191075"
  },
  {
    "text": "and at the time get a valid response by leveraging open media.",
    "start": "191075",
    "end": "194092"
  },
  {
    "text": "These tools are slick.",
    "start": "194100",
    "end": "195209"
  },
  {
    "text": "We can search the net, call an API, but what if I wanted to write some code",
    "start": "195210",
    "end": "198741"
  },
  {
    "text": "or execute some logic using a code interpreter?",
    "start": "198741",
    "end": "201433"
  },
  {
    "text": "This brings me to Part 3: adding a Python code interpreter.",
    "start": "201510",
    "end": "204913"
  },
  {
    "text": "First up, I need to bring the python tool and the local python storage class",
    "start": "204913",
    "end": "208000"
  },
  {
    "text": "that the Python tool will be used to execute code",
    "start": "208000",
    "end": "210375"
  },
  {
    "text": "and the storage component allows for code to be read and output locally.",
    "start": "210375",
    "end": "213832"
  },
  {
    "text": "Now to configure the Python tool.",
    "start": "213897",
    "end": "215509"
  },
  {
    "text": "I'm setting up the code interpreter URL and the storage locations.",
    "start": "215509",
    "end": "218417"
  },
  {
    "text": "This tells my agent where to run Python code and where to store any files it might create or read.",
    "start": "218480",
    "end": "222806"
  },
  {
    "text": "Given we're running our agent in TypeScript, we need somewhere to execute Python code.",
    "start": "222810",
    "end": "226556"
  },
  {
    "text": "The Bee agent framework comes with a standalone code interpreter which can be run via doc.",
    "start": "226560",
    "end": "230416"
  },
  {
    "text": "I haven't done this yet, so that's going to be that.",
    "start": "230416",
    "end": "231866"
  },
  {
    "text": "The Docker file is available via the Bee agent GitHub repo.",
    "start": "231866",
    "end": "234479"
  },
  {
    "text": "First clone and cd into it and install any remaining dependencies.",
    "start": "234479",
    "end": "237355"
  },
  {
    "text": "Then I can spin up the container using \"yarn run infra:start-code-interpreter\" ",
    "start": "237360",
    "end": "240969"
  },
  {
    "text": "Then if I change the prompt to something like \"is 3 a prime number?\", I can run it using yarn run flow.",
    "start": "240969",
    "end": "245933"
  },
  {
    "text": "And finally we get their correct result.",
    "start": "245940",
    "end": "248319"
  },
  {
    "text": "Hey, guys. Editing Nic here, I hope you enjoyed the video.",
    "start": "251940",
    "end": "254164"
  },
  {
    "text": "Let me know what you thought in the comments and code will be down to look.",
    "start": "254164",
    "end": "257047"
  }
]