[
  {
    "text": "Hey, quick question.",
    "start": "590",
    "end": "1829"
  },
  {
    "text": "Did you know that you can run the latest large language models locally on your laptop?",
    "start": "1950",
    "end": "6088"
  },
  {
    "text": "This means you don't have any dependencies on cloud services",
    "start": "6530",
    "end": "9663"
  },
  {
    "text": "and you get full data privacy while using optimized models to chat,",
    "start": "9663",
    "end": "14036"
  },
  {
    "text": "uses code assistance and integrate AI into your applications with RAG or even agentic behavior.",
    "start": "14036",
    "end": "20010"
  },
  {
    "text": "So today we're taking a look at Ollama.",
    "start": "20470",
    "end": "22430"
  },
  {
    "text": "It's a developer tool that has been quickly growing in popularity",
    "start": "22650",
    "end": "25886"
  },
  {
    "text": "and we're gonna show you how you can start using it on your machine right now,",
    "start": "25886",
    "end": "29329"
  },
  {
    "text": "but real quick, before we start installing things, what value does this open source project provide to you?",
    "start": "29760",
    "end": "35539"
  },
  {
    "text": "Well, as a developer, traditionally I'd need to request computing resources",
    "start": "35980",
    "end": "39797"
  },
  {
    "text": "or hardware to run something as intensive as a large language model.",
    "start": "39797",
    "end": "43420"
  },
  {
    "text": "And to use cloud services involves sending my data to somebody else, which might not always be feasible.",
    "start": "43920",
    "end": "49480"
  },
  {
    "text": "So by running models from my local machine, I can maintain full control over my AI and use a model through an API,",
    "start": "49900",
    "end": "57020"
  },
  {
    "text": "just like I would with another service, like a database on my own system.",
    "start": "57420",
    "end": "60879"
  },
  {
    "text": "Let's see this in action by switching over to my laptop and heading to ollama.com,",
    "start": "61340",
    "end": "65760"
  },
  {
    "text": "and this is where you can install the command line tool for Mac,",
    "start": "65920",
    "end": "69071"
  },
  {
    "text": "Windows, and of course, Linux, but also browse the repository of models.",
    "start": "69071",
    "end": "73420"
  },
  {
    "text": "For example, foundation models from the leading AI labs, but also",
    "start": "73500",
    "end": "77303"
  },
  {
    "text": "more fine tuned or task specific models such as code assistants.",
    "start": "77303",
    "end": "81119"
  },
  {
    "text": "Which one should you use?",
    "start": "81660",
    "end": "82859"
  },
  {
    "text": "Well, we'll take a look at that soon,",
    "start": "83080",
    "end": "84420"
  },
  {
    "text": "but for now, I'll open up my terminal where Ollama has been installed,",
    "start": "84730",
    "end": "88029"
  },
  {
    "text": "and the first step is downloading and chatting with a model locally.",
    "start": "88370",
    "end": "91349"
  },
  {
    "text": "So now I have Ollama set up on my local machine.",
    "start": "91930",
    "end": "94849"
  },
  {
    "text": "And what we're going to do first is use the Ollama run command, which is almost two commands in one.",
    "start": "95130",
    "end": "100009"
  },
  {
    "text": "What's going to happen is it's going to pull the model from Olamma's model store,",
    "start": "100510",
    "end": "104559"
  },
  {
    "text": "if we don't already have it, and also start up an inference server for us",
    "start": "104559",
    "end": "108446"
  },
  {
    "text": "to make requests to the LLM that's running on our own machine.",
    "start": "108446",
    "end": "111150"
  },
  {
    "text": "So let's go ahead and do that now.",
    "start": "111630",
    "end": "112849"
  },
  {
    "text": "We're going to run Ollama run granite 3.1 dense,",
    "start": "113030",
    "end": "116150"
  },
  {
    "text": "and so while we have a chat interface here where we could ask questions, behind the scenes,",
    "start": "116810",
    "end": "120682"
  },
  {
    "text": "what we've done is downloaded a quantized",
    "start": "120682",
    "end": "123644"
  },
  {
    "text": "or compressed version of a model that's capable of running on limited hardware,",
    "start": "123644",
    "end": "127489"
  },
  {
    "text": "and we're also using a back end like llama C++ to run the model.",
    "start": "127810",
    "end": "131369"
  },
  {
    "text": "So every time that we ask and chat with the model, for example, asking vim or emacs,",
    "start": "131890",
    "end": "137808"
  },
  {
    "text": "What's happening is we're getting our response, but we're also making a post request to the API that's running on localhost.",
    "start": "137808",
    "end": "144830"
  },
  {
    "text": "Pretty cool, right?",
    "start": "145210",
    "end": "146210"
  },
  {
    "text": "So for our example, I ran the granite 3.1 model and as a",
    "start": "146790",
    "end": "150349"
  },
  {
    "text": "developer, it has a lot of features that are quite interesting to me.",
    "start": "150350",
    "end": "153769"
  },
  {
    "text": "So it supports 11 different languages so it could translate between Spanish and English and back and forth,",
    "start": "153910",
    "end": "159190"
  },
  {
    "text": "and it's also optimized for enterprise specific tasks.",
    "start": "159370",
    "end": "162509"
  },
  {
    "text": "This includes high benchmarks on RAG capabilities,",
    "start": "162930",
    "end": "165349"
  },
  {
    "text": "which RAG allows us to use our unique data with the LLM by providing it in the context window of our queries,",
    "start": "165730",
    "end": "171675"
  },
  {
    "text": "but also capabilities for agentic behavior and much more,",
    "start": "171675",
    "end": "174749"
  },
  {
    "text": "but as always, it's good to keep your options open.",
    "start": "175090",
    "end": "178090"
  },
  {
    "text": "The Ollama model catalog is quite impressive with models for embedding, vision, tools, and many more,",
    "start": "178670",
    "end": "184842"
  },
  {
    "text": "but you could also import your own fine-tuned models, for example,",
    "start": "184842",
    "end": "188489"
  },
  {
    "text": "or use them from Hugging Face by using what's known as the Ollama model file.",
    "start": "188489",
    "end": "193069"
  },
  {
    "text": "So we've installed Olamma, we've chatted with the model running locally, and we've explored the model ecosystem,",
    "start": "193250",
    "end": "198650"
  },
  {
    "text": "but there's a big question left,",
    "start": "198910",
    "end": "200389"
  },
  {
    "text": "what about integrating an LLM like this into our existing application?",
    "start": "200730",
    "end": "204149"
  },
  {
    "text": "So let me hop out of the chat window and let's make sure that the model is running locally on our system.",
    "start": "204850",
    "end": "210090"
  },
  {
    "text": "So Ollama PS can show us the running models,",
    "start": "210950",
    "end": "213530"
  },
  {
    "text": "and now that we have a model running on localhost,",
    "start": "213710",
    "end": "216211"
  },
  {
    "text": "our application needs a way to communicate with this model in a standardized format.",
    "start": "216211",
    "end": "220330"
  },
  {
    "text": "That's where we're going to be using what's known as Langchain",
    "start": "220990",
    "end": "224199"
  },
  {
    "text": "and specifically Langchain for Java in our application,",
    "start": "224199",
    "end": "227182"
  },
  {
    "text": "which is a framework that's grown in popularity and allows us to use",
    "start": "227182",
    "end": "231092"
  },
  {
    "text": "a standardized API to make calls to the model from our application that's written in Java.",
    "start": "231092",
    "end": "237389"
  },
  {
    "text": "Now, we're going to be using Quarkus, which is a Kubernetes optimized Java flavor",
    "start": "237770",
    "end": "242274"
  },
  {
    "text": "that supports this Langchain for J extension in order to call our model from the application.",
    "start": "242274",
    "end": "247649"
  },
  {
    "text": "Let's get started.",
    "start": "248050",
    "end": "249050"
  },
  {
    "text": "So let's take a look at the application that we're currently working on.",
    "start": "249360",
    "end": "252280"
  },
  {
    "text": "So I'll open it up here in the browser.",
    "start": "252480",
    "end": "254418"
  },
  {
    "text": "Now, what's happening is that this fictitious organization Parasol",
    "start": "254820",
    "end": "258734"
  },
  {
    "text": "is being overwhelmed by new insurance claims",
    "start": "258735",
    "end": "262514"
  },
  {
    "text": "and could use the help of an AI, like a large language model,",
    "start": "262515",
    "end": "266122"
  },
  {
    "text": "to help process this overwhelming amount of information and make better and quicker decisions,",
    "start": "266122",
    "end": "271460"
  },
  {
    "text": "but how do we do that behind the scenes?",
    "start": "271780",
    "end": "273439"
  },
  {
    "text": "So here in our project, we've added Lang chain for J as a dependency, and we're going to specify",
    "start": "273740",
    "end": "278479"
  },
  {
    "text": "the URL as localhost on",
    "start": "278910",
    "end": "281209"
  },
  {
    "text": "port 11434 in our application.properties, pointing to where our model is running on our machine.",
    "start": "281209",
    "end": "287149"
  },
  {
    "text": "Now we're also gonna be using a web socket in order to make a post request to the model,",
    "start": "287630",
    "end": "291890"
  },
  {
    "text": "and now our agents have AI capabilities, specifically a helpful assistant that can work with them to complete their job tasks.",
    "start": "292270",
    "end": "299768"
  },
  {
    "text": "So let's ask the model to summarize the claim details.",
    "start": "300150",
    "end": "303209"
  },
  {
    "text": "And there we go.",
    "start": "303490",
    "end": "304709"
  },
  {
    "text": "In the form of tokens, we've made that request to the model.",
    "start": "304810",
    "end": "307750"
  },
  {
    "text": "running with Ollama on our local machine and we're able to quickly prototype from our laptop.",
    "start": "308000",
    "end": "313059"
  },
  {
    "text": "It's just as simple as that.",
    "start": "313220",
    "end": "314819"
  },
  {
    "text": "So running AI locally can be really handy when it comes to prototyping, proof of concepts and much more,",
    "start": "315100",
    "end": "321119"
  },
  {
    "text": "and another common use case is code assistance,",
    "start": "321660",
    "end": "324191"
  },
  {
    "text": "connecting a locally running model to your IDE instead of using paid services.",
    "start": "324191",
    "end": "328660"
  },
  {
    "text": "When it comes to production, however, you might need more advanced capabilities, but for getting started today,",
    "start": "329160",
    "end": "334412"
  },
  {
    "text": "Ollama is a great pick for developers.",
    "start": "334412",
    "end": "336138"
  },
  {
    "text": "So what are you working on or interested in?",
    "start": "336500",
    "end": "339160"
  },
  {
    "text": "Let us know in the comments below,",
    "start": "339420",
    "end": "340978"
  },
  {
    "text": "but thanks as always for watching and don't forget to like the video if you learned something today.",
    "start": "340978",
    "end": "345899"
  },
  {
    "text": "Have a good one.",
    "start": "346220",
    "end": "347220"
  }
]