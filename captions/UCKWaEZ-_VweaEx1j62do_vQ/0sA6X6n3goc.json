[
  {
    "start": "0",
    "end": "108000"
  },
  {
    "text": "Shobhit Varshney: I've never seen,\nuh, like AGI being more plausible than we are standing right now. So my hot take, where we are right\nnow, 2024, um, five years out, I",
    "start": "50",
    "end": "9300"
  },
  {
    "text": "would see us, uh, be able to get\nto very, very intelligent machines.",
    "start": "9300",
    "end": "13600"
  },
  {
    "text": "Tim Hwang: Hello, and happy Friday. You're listening to Mixture of Experts. I'm your host, Tim Hwang, back again.",
    "start": "23575",
    "end": "29043"
  },
  {
    "text": "Each week, Mixture of Experts\ndistills down the week's most important headlines and chatter in\nthe world of artificial intelligence.",
    "start": "29685",
    "end": "35584"
  },
  {
    "text": "From research papers and product\nannouncements to ethics governance and just plain gossip, we've got you covered.",
    "start": "35644",
    "end": "41364"
  },
  {
    "text": "This week on the show, First, the\nannual ACM Conference on Fairness, Accountability, and Transparency, or\nFAccT, is happening this week in Rio.",
    "start": "41705",
    "end": "49350"
  },
  {
    "text": "We'll talk about the latest\ndevelopments in ML fairness and the state of responsible AI. Next up, Leopold Aschenbrenner's AI\nsafety screen situational awareness hit",
    "start": "49680",
    "end": "57920"
  },
  {
    "text": "the airwaves with a widely talked about\ninterview with Dwarkesh Patel, what's the best way to forecast AI capabilities, and\nwhat's going on with safety at OpenAI.",
    "start": "57920",
    "end": "65429"
  },
  {
    "text": "And finally, benchmarking,\nbenchmarking, benchmarking. This week we talk about the latest\nin RAG benchmarking and what it tells",
    "start": "65929",
    "end": "72019"
  },
  {
    "text": "us about the industry as a whole. As always, I'm joined by an\nincredible group of experts who will help us cut through the noise\nand drop some hot takes as we go.",
    "start": "72019",
    "end": "79070"
  },
  {
    "text": "Vagner Santana, Staff Research Scientist,\nMaster Inventor, and importantly, debuting for the first time on MOE.",
    "start": "79399",
    "end": "84630"
  },
  {
    "text": "Vagner, welcome to the show. Vagner Figueredo de Santana:\nThanks for having me. Tim Hwang: Uh, next up, Marina\nDanilevsky, Senior Research Scientist.",
    "start": "84630",
    "end": "90809"
  },
  {
    "text": "Welcome back to the show. Marina Danilevsky:\nThanks, happy to be here. Tim Hwang: And Shobhit Varshney, who\nhas been with us since episode number",
    "start": "90809",
    "end": "97108"
  },
  {
    "text": "one, uh, senior partner consulting on\nAI for US, Canada, and Latin America. Shobhit, welcome back to the show.",
    "start": "97109",
    "end": "102548"
  },
  {
    "text": "Shobhit Varshney: Absolutely love these. Thanks for having me again.",
    "start": "102630",
    "end": "104379"
  },
  {
    "start": "108000",
    "end": "945000"
  },
  {
    "text": "Tim Hwang: All right, well,\nlet's just jump right into it. So the first story I want to cover\nis the annual FAccT conference",
    "start": "109469",
    "end": "115368"
  },
  {
    "text": "is happening this year in Rio. So for those who don't know, it is. Uh, arguably the leading conference\non topics of machine learning",
    "start": "115369",
    "end": "122315"
  },
  {
    "text": "fairness and responsible AI. And I thought this would be a good\njumping off point just because if you've been watching this space for some time,\nresponsible AI and ML fairness has become",
    "start": "122315",
    "end": "131695"
  },
  {
    "text": "kind of a buzzword that lots and lots\nof people have used in recent years. And I think these conferences are a good\ntime to check in on what this kind of,",
    "start": "131695",
    "end": "137908"
  },
  {
    "text": "you know, state of play is in fairness\nand accountability questions in AI. And Vagner, one of the reasons I wanted\nto have you on the show was, um, you've",
    "start": "137908",
    "end": "145484"
  },
  {
    "text": "been watching kind of the papers and\nthe chatter around the conference. Maybe I can just kind of toss it to you\nfirst for our listeners, uh, any sort of",
    "start": "145484",
    "end": "151984"
  },
  {
    "text": "patterns or trends that you've noticed, I\nthink this year, um, at FAccT, if there's particular papers that you think people\nshould check out, just curious about",
    "start": "151984",
    "end": "158964"
  },
  {
    "text": "your review or your kind of thoughts\non, um, what you're seeing out there, um, uh, at this year's conference.",
    "start": "158964",
    "end": "164185"
  },
  {
    "text": "Vagner Figueredo de Santana: Well, there\nare interesting discussions around, um,",
    "start": "164250",
    "end": "169290"
  },
  {
    "text": "uh, synthetic data around how, uh, how\npeople are using LLMs to create data",
    "start": "169560",
    "end": "175639"
  },
  {
    "text": "and then also to assess LLMs using LLMs. So there, there's this discussion going\non also about, uh, responsible AI.",
    "start": "175640",
    "end": "183500"
  },
  {
    "text": "Uh, one of the papers I selected\nto discuss with y'all, I think has to do with how to, how people\nare learning about responsible AI.",
    "start": "183640",
    "end": "191340"
  },
  {
    "text": "on the job. I think that that's important\nbecause people are getting interested and people are following\nand trying to find resources.",
    "start": "191635",
    "end": "198334"
  },
  {
    "text": "But then that comes with\nall the complexities of working in an organization.",
    "start": "198535",
    "end": "204444"
  },
  {
    "text": "So that is one aspect as well. And, well, the other aspect connects\nwith copyright and also how to deal",
    "start": "204444",
    "end": "211624"
  },
  {
    "text": "with all the labor that is being packed\nby the, the, uh, the use of LLMs.",
    "start": "211624",
    "end": "218215"
  },
  {
    "text": "In a wide range of of\njobs around the world. Tim Hwang: Yeah, for sure. And I did want to pick up on that\nsecond theme, specifically, you know,",
    "start": "218420",
    "end": "225660"
  },
  {
    "text": "there's much more as true with all these\nconferences, there's many more papers that you'd ever have time to discuss.",
    "start": "225660",
    "end": "230760"
  },
  {
    "text": "But I think what's so interesting\nabout the responsible AI topic is. You know, this is really an evolution, I\nthink, in fairness in ML, where I would",
    "start": "231479",
    "end": "238640"
  },
  {
    "text": "say even a few years ago, basically a\nlot of the attention was like, can we define in computational terms, what\na fair machine learning algorithm is?",
    "start": "238640",
    "end": "247480"
  },
  {
    "text": "And it kind of feels like there's a\nlot more work now that's happening in this much broader question, which is\nokay, well, we have all these techniques",
    "start": "247880",
    "end": "253340"
  },
  {
    "text": "and approaches around fairness in ML. How do we actually get like an\norganization to implement it?",
    "start": "253340",
    "end": "258340"
  },
  {
    "text": "How do we get people to learn about it? What are the techniques that people use? And You know, Vagner, I think in\naddition to your research, it kind",
    "start": "258520",
    "end": "264795"
  },
  {
    "text": "of sounds like you've been doing some\nwork on this internally within IBM. And so I'm kind of just curious if you\nwant to talk a little bit about that",
    "start": "264795",
    "end": "270585"
  },
  {
    "text": "paper that you mentioned and then just\nkind of map it to your own experience. I think I'm curious about, like, what\nyou're sort of learning as someone who's,",
    "start": "270585",
    "end": "276514"
  },
  {
    "text": "you know, very much in the trenches, you\nknow, trying to get this work to work. Vagner Figueredo de Santana: Yeah,\nand one, one of the aspects that,",
    "start": "276725",
    "end": "282215"
  },
  {
    "text": "um, that the paper covers and,\nand has to do with incentives. And we need to be aware of the symptoms\nthat our organizations have before",
    "start": "282284",
    "end": "290145"
  },
  {
    "text": "thinking about responsible AI, because\notherwise, uh, well, we'll be facing a lot of blockers all along the way.",
    "start": "290585",
    "end": "296904"
  },
  {
    "text": "Um, and also, uh, there is an interesting\naspect that the paper highlights about, um, The, the discipline identities\nthat we have when we are like in,",
    "start": "297285",
    "end": "306895"
  },
  {
    "text": "in, in hard technical teams, they\nhave their own discipline identities. And uh, when they are looking for, uh,\nlet's say resources about responsible",
    "start": "306905",
    "end": "315405"
  },
  {
    "text": "AI, they'll probably go into resources. They are used to look for.",
    "start": "315405",
    "end": "320565"
  },
  {
    "text": "So they're going to be\nlooking for technical, uh, technical libraries or metrics. And sometimes we need to go beyond this.",
    "start": "320614",
    "end": "328905"
  },
  {
    "text": "Um, our own discipline identities\nand look for other skills and other disciplines and to learn more about, let's\nsay, social technical impacts, right?",
    "start": "329259",
    "end": "337749"
  },
  {
    "text": "Beyond go beyond focusing on, let's\nsay, some fair metrics for folks, uh,",
    "start": "338249",
    "end": "343960"
  },
  {
    "text": "focusing on more technical aspects. And the other way is to. Be as well, right? For people, uh, uh, thinking about, uh,\nlet's say indirect impacts on society.",
    "start": "343989",
    "end": "352660"
  },
  {
    "text": "They also need to be aware of the daily\njob of data scientists and coders and",
    "start": "352660",
    "end": "359530"
  },
  {
    "text": "researchers, and how can we like, do\nthis, uh, uh, a connection, right? Tim Hwang: Yeah. I think you're just rundown, I think\nruns into, or I think highlights.",
    "start": "359530",
    "end": "368020"
  },
  {
    "text": "I think a bunch of the issues, uh,\nyou know, I think it was a, it was a joke that I had for, uh, uh, with\na friend for a while that like, oh.",
    "start": "368020",
    "end": "374455"
  },
  {
    "text": "The main thing a lot of big companies\nwould do when they wanted to do like ethical AI or responsible AI would be\nlike, well, we're going to create like",
    "start": "374974",
    "end": "381585"
  },
  {
    "text": "this, like secret group of lawyers\nthat will just determine everything. Um, and it was like, this is like not a\ngood way of, of doing, you know, things.",
    "start": "381585",
    "end": "390194"
  },
  {
    "text": "Um, and I guess I'm kind of curious, I\nmean, Marine, if I can bring you into this discussion, you know, um, I guess maybe\none thing I'd be curious about is like how",
    "start": "390594",
    "end": "398020"
  },
  {
    "text": "you all think about things like fairness\nand responsible AI in, in rag, right? Which are, you're literally trying to\npull information from another source.",
    "start": "398020",
    "end": "404630"
  },
  {
    "text": "Um, and, and I guess I'm kind of\ncurious about like if you've kind of, you know, have thoughts on\nthis particular discussion, right?",
    "start": "405039",
    "end": "411330"
  },
  {
    "text": "Like how should organizations kind of best\norganize themselves to, to do this right? Because I think part of it is this kind\nof interdisciplinary crosstalk, which",
    "start": "411340",
    "end": "418880"
  },
  {
    "text": "I think organizations of different\nsize, you know, do better or worse at, you know, in different capacities.",
    "start": "419080",
    "end": "423860"
  },
  {
    "text": "Marina Danilevsky: I think something\nthat Vagner said about incentives really pops up here as well,\nwhich is why should you care?",
    "start": "424780",
    "end": "431120"
  },
  {
    "text": "Well, it's because you would\nlike your customers finally to be using your rag system. And if it is giving answers that are not,\num, not even so much fair, but there's",
    "start": "431120",
    "end": "439939"
  },
  {
    "text": "a risk that, uh, it's going to give\nsomething that is irresponsible that is going to lead to your users being misled\nor being upset or taking legal action,",
    "start": "439939",
    "end": "449210"
  },
  {
    "text": "then your solution is not going to be,\nuh, But it's not going to be taken. So actually, because we usually are\nlooking at enterprise use cases, we",
    "start": "449500",
    "end": "457385"
  },
  {
    "text": "are very, very incentivized to make\nsure that we are communicating things that are, you know, fair, ethical,\nregardless of what our own ideas are.",
    "start": "457385",
    "end": "465164"
  },
  {
    "text": "It's because if we don't succeed\nin that, it will not be purchased. The risk is too high.",
    "start": "465164",
    "end": "469995"
  },
  {
    "text": "Um, there's too many, you know,\nfun stories in the news about, uh, what happens when you don't\npay enough attention to that.",
    "start": "470344",
    "end": "475564"
  },
  {
    "text": "Tim Hwang: So you're actually seeing that. Cause I think, I dunno, I had a Fear,\nyou know, um, which I still kind of have, which is like, maybe this\ndiscussion is gonna become a little",
    "start": "475825",
    "end": "483095"
  },
  {
    "text": "bit like, um, like data privacy. Where like, I think early on there was\nkind of this idea that like, oh, well",
    "start": "483095",
    "end": "488495"
  },
  {
    "text": "the minute there's a really big data\nbreach, then everybody's suddenly gonna care about data privacy and security. And like consumers will all prefer\nthe, the better privacy option, right?",
    "start": "488495",
    "end": "497194"
  },
  {
    "text": "But then I think you can make\nthe argument that one of the things that's happened is that. There's just like so many huge\ndata breaches now, so many big",
    "start": "497200",
    "end": "503115"
  },
  {
    "text": "failures that like almost like\nthe Overton window has shifted. We're just kind of like,\nOh, you know, someone leaked",
    "start": "503115",
    "end": "508205"
  },
  {
    "text": "billions of customer records. I guess that just happens. But that is something that you're seeing\nis kind of sounds like that, like at",
    "start": "508215",
    "end": "513804"
  },
  {
    "text": "least in fairness because we've had\nall these high profile failures, it's not necessarily people have just been\ncome resigned to it actually still",
    "start": "513825",
    "end": "520724"
  },
  {
    "text": "remains kind of like a thing that\npeople are really concerned about. Shobhit Varshney: We've seen\nthis quite a bit, right? If you look at, uh, the AI culture.",
    "start": "520725",
    "end": "526725"
  },
  {
    "text": "The AI framework, responsible AI\nframework, and what we should prioritize and which ones are high risk, how do you\ncategorize use cases, so on and so forth.",
    "start": "527069",
    "end": "533750"
  },
  {
    "text": "And there's actual tooling and\nplatforms that are needed to go drive these at a price, right? And those three layers have\nto be addressed one by one.",
    "start": "534140",
    "end": "541260"
  },
  {
    "text": "Um, the AI culture around, hey, look\nat your day to day workflows and see where you can apply AI, and you have\nto do this in a responsible way,",
    "start": "541280",
    "end": "547919"
  },
  {
    "text": "and here's a framework around it. Unfortunately, the reality on the ground\nfor, Most of the Fortune 100 companies",
    "start": "547920",
    "end": "553170"
  },
  {
    "text": "that I work with, the responsible AI\nteam, you have to go, it's easy to go create a governance board, and you go\nto the governance board for guidance",
    "start": "553180",
    "end": "560850"
  },
  {
    "text": "and coaching and making sure that\nyou're not doing the wrong things. Unfortunately, it becomes, I'm going to\nquote Lord of the Rings, Gandalf, standing",
    "start": "560850",
    "end": "568229"
  },
  {
    "text": "on a bridge and saying, you shall not\npass, right, go back to the shadows. So we've, we've, Somehow created a,\nuh, forcing function that anything",
    "start": "568229",
    "end": "577505"
  },
  {
    "text": "that goes to the governance board adds\nabout two months of delay to a project. So the value of the unlock for the\nbusiness gets diminished and I might",
    "start": "577505",
    "end": "585724"
  },
  {
    "text": "as well not even deal with this and\nI should just go stick to my RPAs or automation scripts or regular AI\nstuff, and that'll be just fine, right?",
    "start": "585725",
    "end": "593730"
  },
  {
    "text": "So they've become a rate limiting\nstep at this point, and that has to fundamentally change. And for that, the next layer that I was\ntalking about in terms of platforms,",
    "start": "594130",
    "end": "601819"
  },
  {
    "text": "that becomes more and more critical. So instead of saying that, hey, you need\nto go figure out all these 20 different checklists in your RAG pattern so I can\nknow exactly where the data is coming",
    "start": "601830",
    "end": "610079"
  },
  {
    "text": "from, there's, uh, there's metrics that\nI need to report against, and so on and so forth, now you start to move towards\na platform approach where you say, Hey,",
    "start": "610079",
    "end": "617510"
  },
  {
    "text": "use the platform pre approved accelerators\nall the rag when we looked at rag patterns within IBM consulting within a week's\ntime we had like 121 different different",
    "start": "617605",
    "end": "627444"
  },
  {
    "text": "ways in which people were doing rags\nand we said guys time out we've got to go consolidate we'll create scribe flow\nwe'll create a mechanism that has the",
    "start": "627454",
    "end": "634404"
  },
  {
    "text": "best of all techniques in one single\nspot right so when you start to get to a platform then you come to a point where\nthe governance boards are pointing you",
    "start": "634414",
    "end": "642025"
  },
  {
    "text": "towards accelerators Versus becoming\na you shall not pass moment, right? I think that the whole culture leading\nto governance, then leading to the stack.",
    "start": "642025",
    "end": "651139"
  },
  {
    "text": "And we're doing this with a lot\nof our Fortune 100 companies. Recently, last couple of weeks back, I\nhad Pepsi on stage with us where we're",
    "start": "651140",
    "end": "658519"
  },
  {
    "text": "talking about how we're helping them\nbuild a culture of responsible AI and the frameworks and so on and so forth. This is one of the many examples where\nwe've had to go do this end to end",
    "start": "658519",
    "end": "666589"
  },
  {
    "text": "from culture to frameworks to actual\ntooling that goes and deploys that.",
    "start": "666959",
    "end": "670928"
  },
  {
    "text": "Tim Hwang: Yeah, that's really\ninteresting, and actually, I should add that I'm surprised that it has taken\nthis long for us to get to a Lord of the Rings reference, so I think we're\nat episode six right now is the first",
    "start": "672110",
    "end": "680640"
  },
  {
    "text": "one that we've actually, uh, heard. Yeah, exactly. Well, and I think, I don't know, I mean,\nmaybe one last nuance to kind of touch",
    "start": "680640",
    "end": "686980"
  },
  {
    "text": "on, I'd be curious to get the panel's\nthoughts, and Vagner, maybe to throw it back to you, is like, you know, so\nfor the last few episodes, we've all",
    "start": "686980",
    "end": "692339"
  },
  {
    "text": "been very excited about open source. And it feels like part of the problem of\nopen source is that, you know, suddenly.",
    "start": "692339",
    "end": "698530"
  },
  {
    "text": "Like your fairness methodologies\nare almost competing with like just being able to like pull something off\nthe shelf and like deploy it in any",
    "start": "698880",
    "end": "705170"
  },
  {
    "text": "reckless way that you really want to. Um, and I guess I'm kind of curious\nabout like how we think about sort",
    "start": "705170",
    "end": "710990"
  },
  {
    "text": "of responsible AI going forwards in\na world where like anyone can just pull AI off the shelf and use it.",
    "start": "710990",
    "end": "715899"
  },
  {
    "text": "Um, because it feels like in a world\nwhere like maybe there's only two or three platforms, you really can say, okay,\nwell, if you want access to this advanced",
    "start": "716259",
    "end": "722930"
  },
  {
    "text": "technology, you're going to have to go\nthrough this additional compliance cost, even if it takes you, um, a little bit\nmore time, but that kind of lever is, I",
    "start": "722930",
    "end": "730005"
  },
  {
    "text": "don't know, from my point of view, seems\nto be like breaking down a little bit as it becomes more and more accessible. I don't know if you'd buy that.",
    "start": "730005",
    "end": "735055"
  },
  {
    "text": "You might also just say, Tim,\nyou're totally wrong, but I don't know, Vagir, if you've got any\nthoughts on that or anyone really. Vagner Figueredo de Santana: In terms of\nopen source, I think that the interesting",
    "start": "735065",
    "end": "742704"
  },
  {
    "text": "aspect is that, uh, people, um, have more\ntransparency as, as we all know, and,",
    "start": "742914",
    "end": "750490"
  },
  {
    "text": "and also thinking about, uh, well, fully\nopen source models, because people are",
    "start": "750610",
    "end": "756139"
  },
  {
    "text": "also discussing that when you also only\nhave the model and you don't know the data, you used to train the model, you\njust have, have, have open source model,",
    "start": "756139",
    "end": "764429"
  },
  {
    "text": "and when you know more about the model,\nthen you have a fully open source, right? So I think that that's important,\nand people are getting more and",
    "start": "764900",
    "end": "771040"
  },
  {
    "text": "more interested interested on that. And when we talk to clients, they\nare also interested on, um, finding",
    "start": "771040",
    "end": "776940"
  },
  {
    "text": "the right model for the right task. I think that that is also interesting. Uh, that, uh, pattern that is\nemerging, like people discussing,",
    "start": "776940",
    "end": "784179"
  },
  {
    "text": "okay, is this the right size of\nmodel for solving my problem? Is, is this, uh, like, is this language\nmodel or, uh, or generative AI?",
    "start": "784180",
    "end": "793229"
  },
  {
    "text": "Fully open. Uh, can I host that in\nmy own private cloud? So these are questions that are\nappearing when we talk about",
    "start": "793750",
    "end": "800438"
  },
  {
    "text": "responsibility AI right now. Shobhit Varshney: Yeah, we, uh,\nagain, I'm coming in from a very enterprise approach to this.",
    "start": "800569",
    "end": "806720"
  },
  {
    "text": "My, my square focus is how\ndo we scale these, right? And when you look at a, uh, step by step\nprocess, any workflow that's happening",
    "start": "807190",
    "end": "813480"
  },
  {
    "text": "in an organization today, right? Seven different steps. Uh, step number one, you're\ngoing to pull some data. Step number three, you're going\nto do some fraud detection.",
    "start": "813490",
    "end": "819990"
  },
  {
    "text": "Step number four, now you're going\nto go extract something from a document, an invoice, a contract,\nsomething that came to you, right?",
    "start": "820290",
    "end": "825610"
  },
  {
    "text": "Now, say where I was able to\ndo OCR and pull that out with about 80 percent accuracy. So far, that's best where we were now.",
    "start": "825979",
    "end": "833040"
  },
  {
    "text": "All of a sudden, we have LLMs and\nwe say, Hey, I've recently believed that I could potentially get about\n90, 92 percent accuracy and squeeze",
    "start": "833040",
    "end": "839600"
  },
  {
    "text": "more out of this document, right? So now you're saying about 10, 12\npoints of additional benefit that you can derive from it, right?",
    "start": "839600",
    "end": "845419"
  },
  {
    "text": "At that point, we stop and say, if you\nhave reason to believe in LLM could do this, let's Talk about constraints.",
    "start": "845800",
    "end": "850700"
  },
  {
    "text": "The constraints around\ncost and envelope, right? How much can I afford if I'm doing this\na thousand times or a million times? There's a different ROI\nattached to it, right?",
    "start": "851000",
    "end": "857790"
  },
  {
    "text": "Then there is security\nwhere the data resides. Models follow the data gravity. They go, we deploy them closer to where\nthe restricted data sets are and so forth.",
    "start": "858079",
    "end": "866350"
  },
  {
    "text": "Then you start to look at how\nquickly you need an answer. The latency of a model matters. You're trying to start figuring out, Okay.",
    "start": "866620",
    "end": "872140"
  },
  {
    "text": "Uh, from a compliance perspective,\nwhen I have to go explain to somebody how I came up with this answer, which\nmeans I need auditable responses, I",
    "start": "872315",
    "end": "880084"
  },
  {
    "text": "need more deterministic responses in\ncertain use cases and so on and so forth. So you come up with a set of constraints,\nand given those five, ten different",
    "start": "880085",
    "end": "886284"
  },
  {
    "text": "constraints, now you have two or three\ngood athletes that you start to test with. And then from there on, we start\nto move towards metrics and",
    "start": "886285",
    "end": "892435"
  },
  {
    "text": "see which one is giving me more\nversus the others and so forth. But it's very critical at a\nstep level, at a sub level.",
    "start": "892435",
    "end": "898245"
  },
  {
    "text": "task level, you're trying to figure\nout which LLM is going to do the job. And we're getting away from, hey, earlier\nwe said, hey, can I, can a GPT 4 model",
    "start": "898725",
    "end": "905204"
  },
  {
    "text": "do the entire workflow end to end? So we'll talk about that in a\nlittle bit, but I think we're still at the sub task level.",
    "start": "905224",
    "end": "910665"
  },
  {
    "text": "We're surgically infusing AI and\nGen AI and seeing if it can do this one thing incredibly well. I'll take care of the\nrest before and after.",
    "start": "910854",
    "end": "917244"
  },
  {
    "text": "Tim Hwang: Yeah, no, I think it makes\na lot of sense, and I think goes to this really interesting question,\nwhich we won't have time to address today, but we should do on a future\nepisode is, you know, what's that",
    "start": "918070",
    "end": "926058"
  },
  {
    "text": "mean for responsible AI, right? Because it's like, you know, you\nhave lots and lots of sub modules that may have, you know, various,\nvarious different types of problems.",
    "start": "926059",
    "end": "933919"
  },
  {
    "text": "There's almost kind of a question about\nwhether or not any one deployment is responsible, but then whether or not\nthe whole system hangs together is a",
    "start": "934220",
    "end": "940539"
  },
  {
    "text": "whole nother set of analysis, right? That actually is another question.",
    "start": "940539",
    "end": "943460"
  },
  {
    "start": "945000",
    "end": "1766000"
  },
  {
    "text": "Great. So I want to move us to\nthe second topic of today. This is a really big week if\nyou track the discourse around",
    "start": "949084",
    "end": "955935"
  },
  {
    "text": "artificial general intelligence. Leopold Aschenbrenner, who is a\nformer OpenAI Superalignment team",
    "start": "955944",
    "end": "962704"
  },
  {
    "text": "member, published this massive online\nscreed called Situational Awareness. Um, This would have kind of existed,\nI think, as sort of a weird, obscure",
    "start": "962704",
    "end": "971485"
  },
  {
    "text": "screed, but Leopold ended up doing\nan interview with Dwarkesh Patel, the sort of influential tech podcaster,\nand this story and this document",
    "start": "971485",
    "end": "980584"
  },
  {
    "text": "has now just gone everywhere. So I've caught up with friends\nwho, you know, work in policy in D.C. saying, we're getting calls from\ncongressional offices saying what",
    "start": "980605",
    "end": "988063"
  },
  {
    "text": "our take is on situational awareness. So I just want to take a\nquick breather here, um. Uh, because the claims of situational\nawareness are quite breathless.",
    "start": "988064",
    "end": "996969"
  },
  {
    "text": "Um, the argument is if you take all the\nexisting trends in AI and you project linearly, we will reach a point where\nAI becomes, you know, sort of, um,",
    "start": "997319",
    "end": "1006870"
  },
  {
    "text": "uh, transformational in its impact. And so I think this is kind of\na great opportunity to bring in.",
    "start": "1006979",
    "end": "1012670"
  },
  {
    "text": "You show a bit to this conversation,\nbecause the way I sort of see the discourse is that there's a circle\nof people who are in like AGI",
    "start": "1012810",
    "end": "1019550"
  },
  {
    "text": "super intelligence land, right? Who are like the AI is going\nto take over the world, right? But then I think like there's this\nvast group of other people who are just",
    "start": "1019560",
    "end": "1027059"
  },
  {
    "text": "like doing work with the technology,\nwho are like talking to companies that are implementing the technology. And I'm kind of curious as\nsomeone who's like really right.",
    "start": "1027060",
    "end": "1034319"
  },
  {
    "text": "You know, at the front lines of that. Like, are companies like,\nsituational awareness, do we need to be worried about AGI?",
    "start": "1034565",
    "end": "1040845"
  },
  {
    "text": "Like, does that even enter\ninto the commercial discussion? Or is this like a completely, like,\nalmost in the parallel dimension?",
    "start": "1040855",
    "end": "1046954"
  },
  {
    "text": "Shobhit Varshney: I've never seen,\num, like, AGI being more plausible than we are studying right now. So, My hot take, where we are right\nnow, 2024, um, five years out, I",
    "start": "1046964",
    "end": "1056054"
  },
  {
    "text": "would see us, uh, be able to get\nto very, very intelligent machines. Now, the definition of AGI\nhas been very weak, right?",
    "start": "1056054",
    "end": "1063044"
  },
  {
    "text": "Everybody has their own\ninterpretation of what Artificial General Intelligence would mean. And even if you compare two different\npeople, it's very difficult for",
    "start": "1063054",
    "end": "1069304"
  },
  {
    "text": "us to really have a good metric on\nis this person in the show that's really intelligent or not, right?",
    "start": "1069304",
    "end": "1074625"
  },
  {
    "text": "Like if you ask my wife or my kids,\nyou'd have a very different answer. So it's a very different point\nand even being able to define",
    "start": "1074655",
    "end": "1080514"
  },
  {
    "text": "what AGI looks like, right? But if you just talk about intelligence,\nwe've been doing an incredibly good",
    "start": "1080514",
    "end": "1086594"
  },
  {
    "text": "job at making progress every two years. If you like, stepping away from\na half a year increments is",
    "start": "1086594",
    "end": "1093264"
  },
  {
    "text": "looking at a two year horizon. Right? When, uh, GTP4, uh, stopped training\nand you know, they've, they've discussed",
    "start": "1093264",
    "end": "1098745"
  },
  {
    "text": "this in 2022, you're looking at about\na half a billion dollar spend, uh, about a 10 megawatt, uh, about, uh,\n25,000 a hundred, a 100, uh, GPUs",
    "start": "1098745",
    "end": "1108625"
  },
  {
    "text": "from Nvidia at that point, right? That's kind of, kind of what\nthey, uh, must have spent doing this 2024 today you have.",
    "start": "1108655",
    "end": "1114295"
  },
  {
    "text": "You can have a hundred\nthousand Edge 100 equivalents. You're seeing what, how much investment,\nmeta and others are making into this.",
    "start": "1114350",
    "end": "1119750"
  },
  {
    "text": "Right? And then you, uh, you had this huge,\nbig announcement with OpenAI and Microsoft that they go to establish a\nhundred billion dollars super computer.",
    "start": "1119750",
    "end": "1126860"
  },
  {
    "text": "Right now we're talking about\nsomething that starts to get into 2026 timeframe when you can\npotentially have a gigawatt cluster.",
    "start": "1126865",
    "end": "1133520"
  },
  {
    "text": "You can have this big, big giant machine,\nand the power needed for it would be equal to say the Hoover Dam, right?",
    "start": "1133730",
    "end": "1140149"
  },
  {
    "text": "So, or nuclear react. Right. So now you're trying to start to\nsay that I can solve a lot of trough problems by throwing more compute at it.",
    "start": "1140150",
    "end": "1147044"
  },
  {
    "text": "That's just part of the equation, right? There's better algorithms, there\nare better data that's needed. It's a combination of those.",
    "start": "1147415",
    "end": "1152814"
  },
  {
    "text": "You can't overcorrect for bad quality\ndata with having more compute.",
    "start": "1152814",
    "end": "1158375"
  },
  {
    "text": "So we're getting to a point where\nnow you would get to more and more compute power being available. If you keep extrapolating that out, I see\na situation where we would have More than",
    "start": "1158844",
    "end": "1169695"
  },
  {
    "text": "a nuclear reactor attached to this one of\nthese big machines, and you can then have a huge cluster that just intelligently\nlook, crumpling through numbers.",
    "start": "1169695",
    "end": "1177095"
  },
  {
    "text": "I think what he's extrapolating was\nby 2030, we'll be at 100 gigawatts.",
    "start": "1178185",
    "end": "1183675"
  },
  {
    "text": "I think that's a stretch. That's about 20 percent of U. S. electric production, but it\ndoes bring in a few different",
    "start": "1183835",
    "end": "1191015"
  },
  {
    "text": "aspects of the safety of the A.I., who should have access to it,\nnations versus private sector.",
    "start": "1191195",
    "end": "1196164"
  },
  {
    "text": "We solve for that with the nuclear\nenergy saying that, hey, only the big national government should have\naccess to nuclear power, right?",
    "start": "1196770",
    "end": "1203299"
  },
  {
    "text": "We, uh, to nuclear arsenal. And then we trust that there's a mechanism\nin place that has checks and balances",
    "start": "1203300",
    "end": "1209950"
  },
  {
    "text": "in the government that has access to\nsomething that's super, uh, foundational, such a massive impact on humanity.",
    "start": "1209969",
    "end": "1215529"
  },
  {
    "text": "It should be in the hands of governments. But if you start to look at some of the\nworld leaders around right now, a lot",
    "start": "1215700",
    "end": "1221990"
  },
  {
    "text": "of them and potential elections coming\nup and stuff too, they don't quite understand what we're dealing with. I'm just looking at the axis of AI.",
    "start": "1221990",
    "end": "1229410"
  },
  {
    "text": "I would, I could easily see a\ngeopolitical issue here where the country that has has those clusters.",
    "start": "1229410",
    "end": "1235919"
  },
  {
    "text": "If you think about the Oppenheim, if you\ngo back in time and look at what we did in the During that stage, you would not\nwant to have that entire establishment",
    "start": "1236389",
    "end": "1243919"
  },
  {
    "text": "in a different country, right? US went out of our way to\nensure that that's being built inside the United States, right?",
    "start": "1243919",
    "end": "1249639"
  },
  {
    "text": "So you'll see a lot more of\nconcentration of AI superpowers and how much they're investing in\nbuilding the energy, the requirements,",
    "start": "1249879",
    "end": "1256669"
  },
  {
    "text": "building these massive clusters. And if you follow the trajectory of\nelectric production, I was giving",
    "start": "1256709",
    "end": "1262490"
  },
  {
    "text": "a talk recently on how much was the\nimpact AGI and supercomputers and stuff.",
    "start": "1262490",
    "end": "1268539"
  },
  {
    "text": "And I had looked at this detail around\nthe per capita electric generation, right?",
    "start": "1269015",
    "end": "1274225"
  },
  {
    "text": "How much electricity does each\nof the countries generate? And if you just look back at\nthe last 30 years, US, United",
    "start": "1274225",
    "end": "1280324"
  },
  {
    "text": "States has declined 5 percent in\nelectricity production per capita.",
    "start": "1280324",
    "end": "1285143"
  },
  {
    "text": "United Kingdoms, the UK has gone\ndown 23 percent in the last 30 years.",
    "start": "1285845",
    "end": "1290265"
  },
  {
    "text": "China has gone up nine times in\nenergy production per capita, right? So you're starting to see axes of power\nthat who has access to what kind of",
    "start": "1291014",
    "end": "1299794"
  },
  {
    "text": "energy, who has access to what kind of\ncompute power, and then to your earlier point, Vagner, once you start to open\nup these models and open weights being",
    "start": "1299825",
    "end": "1307025"
  },
  {
    "text": "available, you're essentially giving\npeople the recipes of how you can go replicate these things on your own, right?",
    "start": "1307025",
    "end": "1312754"
  },
  {
    "text": "So I think we're at this weird\nintersection of private versus government.",
    "start": "1312955",
    "end": "1318225"
  },
  {
    "text": "And then, does the AI intelligence\nthen dictate geopolitical power? And when does that tip over?",
    "start": "1318885",
    "end": "1324794"
  },
  {
    "text": "At what point does the\ngovernment start getting really, really serious about safety? Who has access to these technologies\ninside of OpenAI, or the big tech",
    "start": "1324865",
    "end": "1332715"
  },
  {
    "text": "giants, and things of that nature? How open are you about that? But what data is going in, and so forth. I'm just very fascinated by\nthe impact it's going to have.",
    "start": "1332725",
    "end": "1340200"
  },
  {
    "text": "Yeah, Tim Hwang: it's actually, I don't\nknow, I feel like you, you, you surprised me there, actually, right? I thought you were going to go in\na completely different direction.",
    "start": "1340250",
    "end": "1346299"
  },
  {
    "text": "I feel like when I talk to many kind\nof folks who are like in enterprise on the business side of this, they'll\nbasically say, This is not happening.",
    "start": "1346300",
    "end": "1354570"
  },
  {
    "text": "This is not realistic. Like you see what's happening with\nAI right now It's never gonna be like what this guy Leopold says And and\nit feels like you're actually going",
    "start": "1354649",
    "end": "1363320"
  },
  {
    "text": "the opposite direction You kind of\nsay look you take all the existing trends you extrapolate them out and\nwe're gonna be in a really weird place",
    "start": "1363320",
    "end": "1369229"
  },
  {
    "text": "in 24 months, I guess Marina Vagner. I'm curious if you two sort of like agree\nwith this kind of assessment or, you",
    "start": "1369249",
    "end": "1376219"
  },
  {
    "text": "know, from the researcher side, is it\nright to say, hey, these linear trends are basically what we should use to\nthink about capabilities in, say, 2026?",
    "start": "1376219",
    "end": "1384279"
  },
  {
    "text": "Marina Danilevsky: Tim, we all\nknow that nothing wrong has ever happened from linear extrapolation. In the history of",
    "start": "1384909",
    "end": "1389860"
  },
  {
    "text": "Tim Hwang: humans, it's very dependable, Marina Danilevsky: very dependable. This is always how things go.",
    "start": "1390610",
    "end": "1395450"
  },
  {
    "text": "Um, I, I think I do have a bit of a\ndifferent perspective than Shobhit and maybe a little bit more like\nthe one that you had said where,",
    "start": "1395790",
    "end": "1403170"
  },
  {
    "text": "yeah, I don't, I don't agree with\nall of the linear extrapolations. And of course we all can have\nthe perspectives that we have",
    "start": "1403510",
    "end": "1408870"
  },
  {
    "text": "on how things are going to go. But I think that even if you continue\nto throw more compute, more data, the",
    "start": "1408870",
    "end": "1414420"
  },
  {
    "text": "way that AI is currently implemented,\nand we're just in another wave. We've gone through waves before. We're in the current wave.",
    "start": "1414480",
    "end": "1420240"
  },
  {
    "text": "There are, to my mind, limitations to\nwhat you're going to be able to achieve. And it is not completely clear\nhow you will actually get out of",
    "start": "1420550",
    "end": "1429050"
  },
  {
    "text": "an AI never recommending you to\nput glue on pizza just because you gave it more compute and more data.",
    "start": "1429299",
    "end": "1435049"
  },
  {
    "text": "Um, and so I think that\nwhile we are Closer. We're still not there.",
    "start": "1435359",
    "end": "1440549"
  },
  {
    "text": "And in my mind, there's at least another\none or several technological waves that",
    "start": "1440550",
    "end": "1445950"
  },
  {
    "text": "need to come before we really get there. So is there going to be a lot\nof interesting things coming?",
    "start": "1445950",
    "end": "1451210"
  },
  {
    "text": "Sure. The points that show but raises about\naccessibility and who gets to actually",
    "start": "1451210",
    "end": "1456289"
  },
  {
    "text": "have these models that has a lot of\nreally interesting implications for being able to disseminate misinformation.",
    "start": "1456289",
    "end": "1463549"
  },
  {
    "text": "have an impact on how people perceive\ninformation, and so on and so forth. Do I think that that's\ngoing to get to AGI?",
    "start": "1463764",
    "end": "1469544"
  },
  {
    "text": "Personally, no, but it doesn't\nmean that it's not going to get to places that are very impactful.",
    "start": "1469594",
    "end": "1475165"
  },
  {
    "text": "Tim Hwang: Yeah, and I think\nthere's actually one thread, and maybe I'll throw it to Vagner. I'm curious about your thoughts\nthat I hadn't really thought about,",
    "start": "1475845",
    "end": "1481354"
  },
  {
    "text": "which is very, very interesting. It's kind of, I guess there's\nthis kind of bet about like, what",
    "start": "1481354",
    "end": "1486535"
  },
  {
    "text": "does Compute actually get you? Right? Um, there's kind of one view which is if,\nso long as I feed in more data and more",
    "start": "1486535",
    "end": "1492570"
  },
  {
    "text": "compute, the representation in the model\nwill eventually just become accurate. Like, we'll solve the just eat\nrocks problem by basically, like,",
    "start": "1492570",
    "end": "1500148"
  },
  {
    "text": "You know, kind of like computing\nour way out of the problem. I guess, Marina, you're kind of saying,\nI don't want to mischaracterize you,",
    "start": "1500480",
    "end": "1505840"
  },
  {
    "text": "that sort of like, there's actually\nsome genuine questions as to whether or not that, that will even happen, right?",
    "start": "1505840",
    "end": "1511599"
  },
  {
    "text": "Like the models will become more\npowerful, but they might not necessarily become more accurate, right? We normally think about things\ngetting better as, you know, kind",
    "start": "1511609",
    "end": "1518619"
  },
  {
    "text": "of trending in a certain direction. I guess you're saying we can see\nimprovement, but it might be very multidimensional in a way that kind of is\na little bit counterintuitive, I think.",
    "start": "1518620",
    "end": "1525669"
  },
  {
    "text": "Yeah. Vagner Figueredo de Santana:\nYeah, I think that the, the, the. issue with, uh, increasing more, uh,\nor requiring more compute to improve",
    "start": "1525860",
    "end": "1534740"
  },
  {
    "text": "or increase, uh, the already really\nlarge models, uh, we'll, we'll be",
    "start": "1534740",
    "end": "1540000"
  },
  {
    "text": "seeing like, uh, less and less, uh,\norganizations controlling everything. So that's in terms of responsibility,\nI think it's, it's something",
    "start": "1540000",
    "end": "1547269"
  },
  {
    "text": "that may be concerning. And in terms of ever meant\nenvironmental impacts, also people",
    "start": "1547269",
    "end": "1553159"
  },
  {
    "text": "are thinking a lot about the energy\nthat, uh, these models are, uh, Not only, uh, requiring for training,\nbut also inference at scale, right?",
    "start": "1553159",
    "end": "1561745"
  },
  {
    "text": "So then these, I think that\nbalancing all of these, I think that that's a big challenge.",
    "start": "1561745",
    "end": "1567724"
  },
  {
    "text": "And in terms of responsibility, right? What we are always trying to\nthink about is, uh, do we?",
    "start": "1567724",
    "end": "1572844"
  },
  {
    "text": "really need to create\nthis technology right now? What are the problems\nthat we're going to solve? Uh, can we solve the problems that we have\nwith the technologies we already have?",
    "start": "1572965",
    "end": "1581754"
  },
  {
    "text": "Right, there's a lot of interesting\nquestions and, uh, uh, well, in terms of responsible AI, we need\nto think about these all the time,",
    "start": "1581805",
    "end": "1587914"
  },
  {
    "text": "not only as afterthought, right? Tim Hwang: Like part of the\nresponsibility might just be like, no AI. Shobhit Varshney: I think the\ncost and impact of this is going",
    "start": "1588365",
    "end": "1597745"
  },
  {
    "text": "to start plummeting, right? If you just look at the Compute\npower that's in your phone today that causes lamenting.",
    "start": "1597745",
    "end": "1603105"
  },
  {
    "text": "So over time, we'll solve for this. I think from enterprise perspective,\nTim, your original question. I think we are over complicating\nhow work gets done in organization.",
    "start": "1603105",
    "end": "1610945"
  },
  {
    "text": "If you have access to hundreds or\nmillions of MIT and Harvard and",
    "start": "1611445",
    "end": "1617005"
  },
  {
    "text": "Stanford grads, And you put them into\nsomething very mundane and say, you're going to do procurement analysis.",
    "start": "1617005",
    "end": "1622275"
  },
  {
    "text": "You're going to get an invoice,\nyou're going to compare it against something, right? That's the kind of work that\nhappens in an enterprise, right? So I have reason to believe that if you\nput a really intelligent person or an",
    "start": "1622465",
    "end": "1631245"
  },
  {
    "text": "equivalent of a person, digital labor\ninside of a particular workflow, that sub task will get done very, very well.",
    "start": "1631254",
    "end": "1637540"
  },
  {
    "text": "There are all kinds of guardrails and\nstuff that you can create around that. that particular task. So if you look at it in levels, the first\nlevel is, can I do a subtask really well?",
    "start": "1637541",
    "end": "1645379"
  },
  {
    "text": "In the previous discussion,\nI said step number four, I'm going to extract something out. Can I do that task really well?",
    "start": "1645379",
    "end": "1650790"
  },
  {
    "text": "And that starts to become\na specific unit of work. Then you go one level up and say,\ntoday a human asks each machine,",
    "start": "1650790",
    "end": "1657439"
  },
  {
    "text": "each LLM to go do different steps. Can I replace that with an orchestration\nwhere an LLM agent can figure out a",
    "start": "1657439",
    "end": "1663360"
  },
  {
    "text": "plan, manage the memory and stuff, and\nautomate the entire flow end to end? There's a very plausible\npath for us to get to.",
    "start": "1663360",
    "end": "1669429"
  },
  {
    "text": "Figuring out how step is done, auto, uh,\norchestrating all of those workflows, and",
    "start": "1670074",
    "end": "1675205"
  },
  {
    "text": "now you start to move up the hierarchy of\nwhat a human supervisor would have done versus a summer intern would have told.",
    "start": "1675205",
    "end": "1680875"
  },
  {
    "text": "And you always double check\nwhat a summer intern does. That's where we are today. And over time, you see a progression\ntowards work itself getting",
    "start": "1680875",
    "end": "1687894"
  },
  {
    "text": "automated to with a very, very high\naccuracy, especially with the cost of AI just plummeting over time.",
    "start": "1687905",
    "end": "1692804"
  },
  {
    "text": "Tim Hwang: Yeah, that's, that's\nalmost a great way of thinking about it is to show me your earlier\npoint about sort of AGI having like",
    "start": "1693275",
    "end": "1699245"
  },
  {
    "text": "this very amorphous definition. It's almost interesting thinking\nabout the idea like William Gibson has this quote, right?",
    "start": "1699245",
    "end": "1704585"
  },
  {
    "text": "Like the future is here. It's just not widely distributed yet. But kind of what you're\nsaying is like AGI is here. It's just not widely distributed yet.",
    "start": "1704585",
    "end": "1710794"
  },
  {
    "text": "Like for certain types of tasks,\nlike the AI that we have right now can do all the possible things.",
    "start": "1710805",
    "end": "1716283"
  },
  {
    "text": "Yeah.\nJob tasks, right? Um, and you're basically just kind\nof talking about like how far up the organizational chain this thing will go.",
    "start": "1716455",
    "end": "1723495"
  },
  {
    "text": "I don't think Shobhit Varshney: that, I don't think\nwe'll get into a point where we'll have a crisp definition of what AGI is and\nwe'll say, hey, today, rah, rah, open",
    "start": "1723514",
    "end": "1729263"
  },
  {
    "text": "a champagne, we'll reach that, right? It's incremental progress. It's a different definition in each field. Feel in each domain. In each task, right?",
    "start": "1729264",
    "end": "1735010"
  },
  {
    "text": "So I think the right, the right frames\nto say that machines will get super intelligent over time and they will exceed\nhuman intelligence in certain tasks.",
    "start": "1735189",
    "end": "1742479"
  },
  {
    "text": "And one thing that humans don't do\nreally well is share our knowledge amongst our ourselves, right? If you put two experts in a room, it's\nvery difficult for them to actually",
    "start": "1742659",
    "end": "1749500"
  },
  {
    "text": "go at a problem together, right? We don't do a really good job at\nexpanding out using the network effect.",
    "start": "1749504",
    "end": "1754510"
  },
  {
    "text": "I think that's gonna change when\nyou have super intelligent machines that can talk to each other. And, and drive better safety, better\nalgorithms, better research, and start",
    "start": "1754870",
    "end": "1762570"
  },
  {
    "text": "to build better algorithms all together. Right. So I think I'm very excited about\nthe direction that we're going.",
    "start": "1762570",
    "end": "1766299"
  },
  {
    "start": "1766000",
    "end": "2436000"
  },
  {
    "text": "Tim Hwang: So I want to move us on to\nthe final topic, um, uh, and the way I want to tee this up is that there's this\nfamous, uh, clip of Steve Ballmer when",
    "start": "1771630",
    "end": "1779470"
  },
  {
    "text": "he was CEO of Microsoft, where he's like,\nif you've seen Steve Ballmer before, he's like this big muscular guy, he's like very\nsweaty on stage, and he's just shouting,",
    "start": "1779470",
    "end": "1788130"
  },
  {
    "text": "Developers, developers, developers. And I kind of feel like if you had\nplayed that scene again today, people",
    "start": "1788365",
    "end": "1793695"
  },
  {
    "text": "would be being like benchmarking,\nbenchmarking, benchmarking. Um, because I think it is becoming\nsuch an important aspect of sort",
    "start": "1793695",
    "end": "1800513"
  },
  {
    "text": "of like the supply chain of AI. Um, and you know, there's lots\nand lots of things we could talk about with benchmarking.",
    "start": "1800514",
    "end": "1805625"
  },
  {
    "text": "We have, and we will continue to. Um, but I think Marina, particularly\nwith you on the line, I figured",
    "start": "1805635",
    "end": "1810745"
  },
  {
    "text": "it would be great to kind of\nzoom in specifically to rag. Um, and do a little bit where we kind of\ntalk to the listeners about essentially",
    "start": "1810745",
    "end": "1819415"
  },
  {
    "text": "what's happening in RAG benchmarking. Um, and then I think from there kind\nof talk a little bit about what that",
    "start": "1819415",
    "end": "1825355"
  },
  {
    "text": "tells us about how benchmarking in\nthe industry is evolving, uh, not just in the industry, but I would say\nin research as well, uh, as a whole.",
    "start": "1825355",
    "end": "1831935"
  },
  {
    "text": "But um, if you, if you will, I wanted\nto kind of throw it to you and to say, if it's possible, we'd love kind of a\nshort crash course into how people think",
    "start": "1832315",
    "end": "1840105"
  },
  {
    "text": "about measuring the quality of rag. And I think that'll almost give us\nsomething very concrete to talk about in terms of benchmarking generally.",
    "start": "1840105",
    "end": "1845924"
  },
  {
    "text": "Marina Danilevsky: Sure. Sounds good. So I will say benchmarking that's\nbeen around for a very long time. It's always been something that\nwas extremely important for",
    "start": "1846555",
    "end": "1853805"
  },
  {
    "text": "systems, databases, ML, everything. So I understand folks are maybe\nlooking at that right now.",
    "start": "1854365",
    "end": "1859495"
  },
  {
    "text": "Um, but.\nYeah, you're into it before Tim Hwang: it was cool. Marina Danilevsky: That's right. That's right. We were into it before it was cool.",
    "start": "1859495",
    "end": "1864929"
  },
  {
    "text": "We discovered the band first. Um, so the thing right now with a rag,\nlet's talk about what rag is again, real",
    "start": "1864970",
    "end": "1871780"
  },
  {
    "text": "quickly, and then we'll see what it is\nthat you need to evaluate the retrieval, the augmented, the generation part. All right. So what are you trying to do?",
    "start": "1871780",
    "end": "1877509"
  },
  {
    "text": "You're trying to finally give\ninformation that is supported by a knowledge that you can say, okay,\nthis is knowledge that I can assume",
    "start": "1877520",
    "end": "1884750"
  },
  {
    "text": "here's information I'm giving you. So what happens with RAG? Remember, a user has\nsome sort of an inquiry.",
    "start": "1884770",
    "end": "1890560"
  },
  {
    "text": "You fetch something that is related and\nyou say, I'm going to give, use this information to give you the answer.",
    "start": "1890750",
    "end": "1896320"
  },
  {
    "text": "Okay. Where's all of this going to break? It's going to break when the\nquery is not well formed.",
    "start": "1896800",
    "end": "1901839"
  },
  {
    "text": "So you're not fetching the right thing. If you're not fetching the right thing. Then you don't know that there's\ninformation you didn't get it.",
    "start": "1901869",
    "end": "1907920"
  },
  {
    "text": "So that's something to evaluate. If you are fetching the right\nthing or even not the right thing, you don't have to generate an\nanswer based on that multiple ways",
    "start": "1907960",
    "end": "1915020"
  },
  {
    "text": "that that's going to break down. So you're going to have a\nmodel that gives you an answer. That's not based on the\ninformation you fetched.",
    "start": "1915020",
    "end": "1920809"
  },
  {
    "text": "It's going to give you\nan incomplete answer. It's going to give you an answer. That's a mix of. Some of it is drawing from it.",
    "start": "1920819",
    "end": "1926155"
  },
  {
    "text": "Some of it is drawing from its parameter. Some of it is just making up because\nit decided to go off, uh, especially a little later in the response.",
    "start": "1926155",
    "end": "1932215"
  },
  {
    "text": "And you have to check all of that. Uh, can you force the model to\ngive you a different answer because you told it, no, no, no, you told\nme it was this way, but I'm going",
    "start": "1932475",
    "end": "1940565"
  },
  {
    "text": "to say, now assume it's that way. Okay.\nCan you, can you mess it up that way? Can you give the answer quickly enough?",
    "start": "1940565",
    "end": "1946615"
  },
  {
    "text": "Um, so these are all of the things\nthat you have to manage to evaluate. So when people talk about context\nrelevance or answerability or the",
    "start": "1947015",
    "end": "1956764"
  },
  {
    "text": "faithfulness or completeness, all of\nthese different metrics that people have, uh, this is really what we're\ntalking about with evaluating RAG.",
    "start": "1956765",
    "end": "1963424"
  },
  {
    "text": "A couple of points here. You can try to benchmark, uh,\nwith, uh, against a gold answer.",
    "start": "1963974",
    "end": "1970275"
  },
  {
    "text": "Which is usually something that\nworks in cases like classification or anything where there is a very,\nvery clear thing as an answer.",
    "start": "1970485",
    "end": "1976535"
  },
  {
    "text": "The problem with generative AI\nis, remember that word generative? Everything is created fresh, which\nmeans that there might have been a lot",
    "start": "1976815",
    "end": "1982694"
  },
  {
    "text": "of different ways to create an answer. So when you're saying that I'm going to\nhave some sort of an overlap metric, like rouge or blue, anything of that kind,\nthat's not always going to be great.",
    "start": "1982695",
    "end": "1991035"
  },
  {
    "text": "It'll tell you if you've gone off\ncompletely, but it won't tell you subtleties that, oh, maybe you rephrase\nthe answer a little differently, but",
    "start": "1991045",
    "end": "1996895"
  },
  {
    "text": "it still would have been, acceptable. So problems there. So then you say, okay,\nlet's not have references.",
    "start": "1996895",
    "end": "2002330"
  },
  {
    "text": "Let's just judge the answer as\nit is the problem with all of the metrics that I just mentioned. Nothing has a completely\nclear definition and it can't.",
    "start": "2002330",
    "end": "2009369"
  },
  {
    "text": "Because you cannot get everybody\nto agree on what does complete mean, what does faithful mean. Believe me, I've tried.",
    "start": "2009955",
    "end": "2014825"
  },
  {
    "text": "We have had so many\narguments with research. Tim Hwang: Well, Marina Danilevsky: I mean, it's\nkind of existential, because like, Tim Hwang: yeah, sorry, go ahead.",
    "start": "2015005",
    "end": "2020225"
  },
  {
    "text": "Marina Danilevsky: No, it is. You're completely right. It is like, what does it mean for\nyou that an answer is complete? Not only can the researchers not\nagree, then the customers can't agree.",
    "start": "2020845",
    "end": "2028144"
  },
  {
    "text": "So when you are talking about\nbenchmarking, uh, there are bits that you try to benchmark\nfirst, parts of this system.",
    "start": "2028715",
    "end": "2034475"
  },
  {
    "text": "Um, As Shobhit was saying, well, how\ndo you do on just the retriever part? How do you do on just the generative part? How do you do on, you\nknow, just faithfulness?",
    "start": "2034475",
    "end": "2039890"
  },
  {
    "text": "And the problem is that here, the\nwhole is not the sum of its parts. You put all of that together in an\nend to end experience, and it is not",
    "start": "2040640",
    "end": "2047540"
  },
  {
    "text": "equivalent to, I checked every part\nindividually, therefore I know how it's going to go together, doesn't go that way.",
    "start": "2047550",
    "end": "2052828"
  },
  {
    "text": "And it's a very difficult thing to\nactually benchmark because the more parts there are to a system, the more complex\nit is to know what happens when you put",
    "start": "2053280",
    "end": "2061950"
  },
  {
    "text": "all of them together in different ways. So that's actually why people are\nso interested in the benchmarks right now is because the state\nof it is a little confusing.",
    "start": "2061950",
    "end": "2069050"
  },
  {
    "text": "It's a little bit incomplete. We're just like, well, it isn't\nthat we can actually trust. And then, of course, what we talked\nabout in previous episodes that",
    "start": "2069300",
    "end": "2074859"
  },
  {
    "text": "the benchmarks do get saturated. Very quickly. As soon as you have one out, a\nfew months later, okay, everybody",
    "start": "2074860",
    "end": "2080315"
  },
  {
    "text": "already can deal with that one. You know, you got to thank it\nfor its service and move on. Tim Hwang: Yeah. What I love about this is that it's like,\nit starts very tactical and then becomes",
    "start": "2080315",
    "end": "2088565"
  },
  {
    "text": "like existential very quickly, where\nyou're basically like, what is truth? What is clarity anyways?",
    "start": "2088585",
    "end": "2093844"
  },
  {
    "text": "You know, of which like\nthere kind of is no answer. I guess. So I don't know if Marina, this\nis a good way to sum it up.",
    "start": "2093925",
    "end": "2099285"
  },
  {
    "text": "I mean, are you sort of saying\nthat there is no RAG benchmark in a certain sense, right? Like there's no commonly understood\nnorm for judging RAG quality.",
    "start": "2099285",
    "end": "2109060"
  },
  {
    "text": "Marina Danilevsky: We do, we do our\nbest and I think there are incremental implementations that are better and\nbetter and better as we have one benchmark",
    "start": "2109390",
    "end": "2117989"
  },
  {
    "text": "realized something it didn't cover. Do another one. Do another one. Do another one. So there are, you know, in\nincremental approximations of",
    "start": "2117990",
    "end": "2125450"
  },
  {
    "text": "what isn't is not going to work. And at some point in time again, it's\nprobably going to reach a level where we say, All right, this is good enough.",
    "start": "2125460",
    "end": "2131529"
  },
  {
    "text": "We've we've kind of, you know,\nsaturated this as much as we can. But what ends up happening is then you\nend up moving to other use cases, right?",
    "start": "2131560",
    "end": "2138135"
  },
  {
    "text": "Shobhit mentioned agents. It's a very interesting\ndirection that we're going in. Well, now you don't just have texts. You don't just have that\ngoing out of the rack.",
    "start": "2138135",
    "end": "2144635"
  },
  {
    "text": "Now you have, I am calling functions. I am using tools. I'm having something else\nhappen in the middle. My execution plan as an LLM agent\nis absolutely all over the place.",
    "start": "2144664",
    "end": "2153385"
  },
  {
    "text": "Now you don't just have an R and a G. Now you have, I don't know how many\nthings every single time you add.",
    "start": "2153704",
    "end": "2158595"
  },
  {
    "text": "Now, how do you benchmark? Now, how do you benchmark? So we all are having a lot\nof fun constantly making. new problems for ourselves that we then\nhave to test that then reveal additional",
    "start": "2158945",
    "end": "2168099"
  },
  {
    "text": "problems and and things we can implement Tim Hwang: yeah i love the idea that\nkind of like um eval design itself is",
    "start": "2168140",
    "end": "2174749"
  },
  {
    "text": "trying to hill climb like basically\nlike yeah it has like a very similar pattern to the evals themselves",
    "start": "2174749",
    "end": "2180120"
  },
  {
    "text": "Shobhit Varshney: so Yeah, so Tim,\nuh, just working with real clients, uh, one of my big, big clients,\nwe're looking at contracts and RAG",
    "start": "2180220",
    "end": "2187080"
  },
  {
    "text": "is a great example of that, right? Given a whole bunch of thousands,\na few thousand contracts, I want to ask questions against it, expect\nto get a good answers, right?",
    "start": "2187080",
    "end": "2193430"
  },
  {
    "text": "So when you start to look at the\nkind of questions and queries that people are going, what's\ngoing to be insightful for them? There's a level one question\nis, can I find something in a",
    "start": "2193850",
    "end": "2201799"
  },
  {
    "text": "contract that tells you what's\nthe expiry date of the contract? Or is there an exit clause\nin this contract or not? That's a simple RAG pattern, right?",
    "start": "2201800",
    "end": "2208180"
  },
  {
    "text": "Very naive, it can work. Then you start to look at. This is a contract, but then it\nhas amendments stapled to it.",
    "start": "2208190",
    "end": "2213280"
  },
  {
    "text": "And now the answer of the end date\nactually is in the third amendment that overrides the previous amendment, right? So now you're looking at the\nwhole chain of thought of how to",
    "start": "2213580",
    "end": "2219639"
  },
  {
    "text": "read this particular document. Then a level three of a question\ncould be when I'm trying to cross compare and say, Hey, I want to\norder another thousand units.",
    "start": "2219640",
    "end": "2227129"
  },
  {
    "text": "Which one of these contracts is\nclosest to the threshold where I'm going to get some cash back? It's a more complex question.",
    "start": "2227129",
    "end": "2232860"
  },
  {
    "text": "And you very quickly start\nto move away from a rag. So the perception is that, oh, I can ask,\nI can dump contracts, ask questions, but",
    "start": "2233230",
    "end": "2240855"
  },
  {
    "text": "in reality, a human would have gone and\nlooked at another system in an SAP and said, here are all the orders to date,\nand then done some math on it, and then",
    "start": "2240855",
    "end": "2248395"
  },
  {
    "text": "given you an answer that's going across. So it's not quite right. There's no document that gives you the\nanswer that you can go retrieve on demand.",
    "start": "2248395",
    "end": "2255315"
  },
  {
    "text": "So you need to have some type of a\nrouter in the middle that understands what kind of question is asked. And then you may have to go chat with\nsome structured data at the back end to",
    "start": "2255750",
    "end": "2263220"
  },
  {
    "text": "bring that in and then call unstructured. Starts to get really complex. We talk about RAG, but we should really\nbe talking more about the use case end",
    "start": "2263220",
    "end": "2271509"
  },
  {
    "text": "to end that has much more than just\nthe RAG patterns need to be metrics. So that's more complex than what\nMarina you were talking about.",
    "start": "2271509",
    "end": "2277200"
  },
  {
    "text": "And now we're talking about a\nwhole end to end chain and how do you measure accuracy in this case? Two different people have\ntwo different answers.",
    "start": "2277420",
    "end": "2284179"
  },
  {
    "text": "Marina Danilevsky: Yeah. See, we even disagree because I\ncall all of that rag in my mind. And so like, we don't even\nagree on what rag patterns are",
    "start": "2285330",
    "end": "2290660"
  },
  {
    "text": "because to me, I'm like, great. You're, you're retrieving\na function answer. You're retrieving something\nfrom a knowledge base. You're, you're still kind of, you\nknow, retrieving this case just",
    "start": "2290720",
    "end": "2297599"
  },
  {
    "text": "means, you know, function call. But so even with that, right, you can\nthink of rag pattern as just a single call and only informational query.",
    "start": "2297600",
    "end": "2303568"
  },
  {
    "text": "You can think of it as the entire thing\nthat you're talking about, Shobhit. And I think that you\nend up having to extend.",
    "start": "2303569",
    "end": "2308970"
  },
  {
    "text": "How you do the evaluation. Great.\nWe've done it for one small pattern. Now, how about when you\nextend, extend, extend, extend? So yeah, you're right, Tim.",
    "start": "2309200",
    "end": "2314950"
  },
  {
    "text": "Hill climbing. Why, why sit on our laurels when\nthere are more complicated problems to Tim Hwang: solve?\nLike, we could be building.",
    "start": "2315280",
    "end": "2321049"
  },
  {
    "text": "Yeah. And I mean, you know, my bias is just\nlike, I think one of the things I'm most interested to see in the AI space is just\nlike, The continued growth of evals as an",
    "start": "2321050",
    "end": "2330135"
  },
  {
    "text": "industry, because this is like where the\nendless value will emerge, right, which is like companies being like, is it good?",
    "start": "2330135",
    "end": "2335805"
  },
  {
    "text": "And it like actually ends up\nbeing like this very, very deep question that really requires some\nreal sort of craft and expertise.",
    "start": "2336165",
    "end": "2342275"
  },
  {
    "text": "So Vagner, you get the privilege of\nhaving the last word on the episode as our, uh, inaugural, or sorry, our\ndebut guest, uh, this, uh, this episode.",
    "start": "2343820",
    "end": "2352400"
  },
  {
    "text": "Um, any final thoughts, uh, on kind of the\nbenchmarking question or RAG in general? I mean, I'm always excited\nabout RAG hot takes.",
    "start": "2352760",
    "end": "2359259"
  },
  {
    "text": "Vagner Figueredo de Santana: Um, well,\nno, not a word that relates to RAG, just a final word that I like to, to Uh,\nuh, more of a provocation, like, so for",
    "start": "2360220",
    "end": "2370000"
  },
  {
    "text": "folks interested in, on, on Responsible\nAI, I think it's worth to try to go beyond your discipline identity, your\nbubble of content, and try to reach out",
    "start": "2370000",
    "end": "2379550"
  },
  {
    "text": "to other contents because, and we are\ntalking about FAccT, in fact, uh, it's interesting because they, Go to a more\ntechnical and also more to the humanity.",
    "start": "2379730",
    "end": "2387870"
  },
  {
    "text": "So try to find a subject that are\ninterested like brag or other Subjects and",
    "start": "2387870",
    "end": "2393290"
  },
  {
    "text": "try to go outsider discipline identity. I think it's good for for the\nThe whole community as a whole.",
    "start": "2393290",
    "end": "2400499"
  },
  {
    "text": "Tim Hwang: Yeah, for sure. That's a great note to end on Well, uh,\nthat's all the time we have for today. Uh marina showbit.",
    "start": "2401250",
    "end": "2406760"
  },
  {
    "text": "Thanks for joining us again Shobhit Varshney: Thank you\nso much for having us, Tim. This is awesome. Most fun thing we do every week.",
    "start": "2406760",
    "end": "2412394"
  },
  {
    "text": "Tim Hwang: Yeah, definitely. Thanks for joining and Vagner. Thanks for joining and hopefully\nwe'll have you back again sometime.",
    "start": "2413175",
    "end": "2418045"
  },
  {
    "text": "Vagner Figueredo de Santana: Thank you. Tim Hwang: Great Well, if you enjoyed\nwhat you heard you can get MOE on Apple podcast Spotify and good podcast platforms\neverywhere And we'll see you next week",
    "start": "2418214",
    "end": "2428785"
  }
]