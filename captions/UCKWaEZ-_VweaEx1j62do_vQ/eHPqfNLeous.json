[
  {
    "start": "0",
    "end": "13000"
  },
  {
    "text": "My name's Kate Soule.",
    "start": "86",
    "end": "1086"
  },
  {
    "text": "I'm a senior manager of business strategy at IBM Research.",
    "start": "1136",
    "end": "3775"
  },
  {
    "text": "And today I'm going to give a short overview of what are the different strategies",
    "start": "4046",
    "end": "7366"
  },
  {
    "text": "we can employ in order to improve the foundation model's trustworthiness",
    "start": "7366",
    "end": "11314"
  },
  {
    "text": "and efficiency for enterprise deployment.",
    "start": "11314",
    "end": "12956"
  },
  {
    "text": "At its core, you can break down a foundation model into a couple of key components;",
    "start": "14036",
    "end": "18243"
  },
  {
    "text": "the data, the architecture, and the training.",
    "start": "18243",
    "end": "35566"
  },
  {
    "start": "41000",
    "end": "77000"
  },
  {
    "text": "And for each one of these areas, there are different strategies and techniques that we can employ",
    "start": "41416",
    "end": "46016"
  },
  {
    "text": "in order to improve efficiency and model trustworthiness.",
    "start": "46016",
    "end": "49426"
  },
  {
    "text": "Let's start with the data.",
    "start": "50326",
    "end": "51376"
  },
  {
    "text": "When we talk about data, we're normally going to talk about it in regards to its quantity, quality, and degree of specialization.",
    "start": "52776",
    "end": "67275"
  },
  {
    "start": "77000",
    "end": "259000"
  },
  {
    "text": "On the quantity side, these models are trained on enormous amounts of unlabeled data in a unsupervised fashion.",
    "start": "77846",
    "end": "85525"
  },
  {
    "text": "It's this volume of data that they're trained on that gives them the super power",
    "start": "86006",
    "end": "89281"
  },
  {
    "text": "to be able to perform an immense number of different downstream tasks easily and efficiently.",
    "start": "89281",
    "end": "94376"
  },
  {
    "text": "But the more data that you're training your model on, you're going to drive up your compute costs when it comes time for training and inference.",
    "start": "95066",
    "end": "101425"
  },
  {
    "text": "There's been a lot of research actually going into it.",
    "start": "101876",
    "end": "103946"
  },
  {
    "text": "How much data do you need in order to train a foundation model as efficiently a foundation model, as efficiently as possible?",
    "start": "103976",
    "end": "110034"
  },
  {
    "text": "If you're trying to make it efficient for training, what's the least amount of data you need per parameter in your model?",
    "start": "110576",
    "end": "117534"
  },
  {
    "text": "So that's a way to measure model sizes, the number of parameters in it.",
    "start": "117536",
    "end": "120415"
  },
  {
    "text": "So it’s the least amount of data that you need per model parameter in order to get a degree of accuracy.",
    "start": "120806",
    "end": "126263"
  },
  {
    "text": "If you’re just focused on the training costs.",
    "start": "126263",
    "end": "129265"
  },
  {
    "text": "You need about ten words per parameter in your training dataset to make these models efficient.",
    "start": "129536",
    "end": "134515"
  },
  {
    "text": "If you're talking about the inference cost though, so think of this as like the fixed cost to train the model once",
    "start": "134966",
    "end": "140119"
  },
  {
    "text": "versus the the marginal cost to use the model over and over and over again.",
    "start": "140119",
    "end": "144176"
  },
  {
    "text": "You can improve the efficiency of your model by making your models far more data dense,",
    "start": "144746",
    "end": "149270"
  },
  {
    "text": "using as many as 100 plus words of data in your training dataset per parameter of your model.",
    "start": "149270",
    "end": "155876"
  },
  {
    "text": "The quality of your data is also going to be incredibly important when we talk about model trustworthiness.",
    "start": "157076",
    "end": "162326"
  },
  {
    "text": "It's just like machine learning, if you have a poor input quality of your data, you're going to have a poor quality model output,",
    "start": "162926",
    "end": "169435"
  },
  {
    "text": "if you have biased training data, you're going to have a biased model but unlike traditional machine learning, there's so much data to sort through.",
    "start": "169435",
    "end": "178045"
  },
  {
    "text": "It is really hard to verify the quality of your data.",
    "start": "178046",
    "end": "181045"
  },
  {
    "text": "And that's why it's important to be very careful of where the data is coming from.",
    "start": "181616",
    "end": "185205"
  },
  {
    "text": "Most of this data is scraped from the Internet",
    "start": "185205",
    "end": "186702"
  },
  {
    "text": "and there are dark corners of the Internet that should not be used when creating training data for these foundation models,",
    "start": "186702",
    "end": "193329"
  },
  {
    "text": "but also then employing hate and profanity filtering,",
    "start": "193329",
    "end": "196907"
  },
  {
    "text": "HAP filtering on this training dataset in order to extract and take out remove any hateful or harmful materials.",
    "start": "197142",
    "end": "205465"
  },
  {
    "text": "Finally, another strategy that you could take on the data side to improve the efficiency of your model is the degree of specialization.",
    "start": "206726",
    "end": "213235"
  },
  {
    "text": "Here the insight is coming from, if you have a question, let's say, about a medical issue,",
    "start": "214046",
    "end": "220043"
  },
  {
    "text": "would you rather ask the smartest person that you know or would you rather ask your doctor?",
    "start": "220043",
    "end": "224306"
  },
  {
    "text": "You would probably rather ask your doctor.",
    "start": "224756",
    "end": "226375"
  },
  {
    "text": "You want an expert's opinion.",
    "start": "226376",
    "end": "227695"
  },
  {
    "text": "And the same goes for foundation models.",
    "start": "228026",
    "end": "230035"
  },
  {
    "text": "We're seeing that smaller models that are specialized in a domain, that are experts,",
    "start": "230486",
    "end": "235472"
  },
  {
    "text": "that are trained on maybe a mixture of 50/50 specialized medical data or finance data.",
    "start": "235473",
    "end": "241195"
  },
  {
    "text": "And 50/50 general data can perform as well, if not better than a much larger general purpose foundation model with no degree of specialization",
    "start": "241196",
    "end": "250712"
  },
  {
    "text": "allowing us if we have tasks that are specialized to a domain to get away with much more lighter weight, smaller, efficient expert models.",
    "start": "250712",
    "end": "258445"
  },
  {
    "start": "259000",
    "end": "473000"
  },
  {
    "text": "So that's the data of the picture.",
    "start": "259676",
    "end": "261155"
  },
  {
    "text": "Now, let's talk about architecture.",
    "start": "261186",
    "end": "262806"
  },
  {
    "text": "On the architecture side, we think of architecture as a way to have a blueprint for how this data gets encoded into the model.",
    "start": "264036",
    "end": "271536"
  },
  {
    "text": "And there are different styles that have emerged that have different advantages.",
    "start": "271536",
    "end": "276005"
  },
  {
    "text": "One of the styles that have emerged is a decoder only model.",
    "start": "276906",
    "end": "280866"
  },
  {
    "text": "This is, for example, GPT3.",
    "start": "281346",
    "end": "283116"
  },
  {
    "text": "These models are really performative, they're really powerful, but they're also very dense,",
    "start": "283716",
    "end": "288380"
  },
  {
    "text": "whereas we have other models that have emerged, such as encoder decoder models that are much more lightweight and efficient.",
    "start": "288380",
    "end": "294726"
  },
  {
    "text": "In addition to the style of the architecture, there's the size.",
    "start": "295986",
    "end": "299106"
  },
  {
    "text": "And here I'm talking again about the parameters.",
    "start": "299766",
    "end": "302016"
  },
  {
    "text": "So you both want to make sure that your size of your model is related to the size of the training data that you have.",
    "start": "302526",
    "end": "308525"
  },
  {
    "text": "If you have too big of a model, too many parameters for the amount of training data you're going to over fit leading to trustworthiness issues.",
    "start": "308736",
    "end": "315606"
  },
  {
    "text": "And you're also going to have efficiency issues.",
    "start": "316326",
    "end": "318514"
  },
  {
    "text": "The bigger your model, the more compute costs it will take, both to train it and to run inference.",
    "start": "318906",
    "end": "324275"
  },
  {
    "text": "Finally, is the training, which is how I stitch all of this together, the data and the architecture with compute.",
    "start": "325716",
    "end": "331445"
  },
  {
    "text": "And you can break training down into a couple of different steps.",
    "start": "332106",
    "end": "334895"
  },
  {
    "text": "There's the pre training.",
    "start": "335826",
    "end": "336826"
  },
  {
    "text": "So I say pre-training to be very specific here.",
    "start": "343046",
    "end": "346194"
  },
  {
    "text": "These models, if you remember, are meant to be a starting point.",
    "start": "346646",
    "end": "349315"
  },
  {
    "text": "They're meant to be taken and then retrained in a process that's called tuning and used for different downstream tasks.",
    "start": "349826",
    "end": "355405"
  },
  {
    "text": "So pre-training refers to creating the first foundational model, the starting point,",
    "start": "355856",
    "end": "360130"
  },
  {
    "text": "and the pre-training is going to drive a huge percentage of the compute costs and the carbon footprint of foundation models.",
    "start": "360131",
    "end": "368125"
  },
  {
    "text": "These costs are going to be dictated by your architecture choices, your hardware choices, the inference stack choices,",
    "start": "368486",
    "end": "374789"
  },
  {
    "text": "all of which can be dictated and result in different carbon costs and compute costs.",
    "start": "374789",
    "end": "380395"
  },
  {
    "text": "After the initial pre training, there's a really interesting phase called the alignment phase,",
    "start": "382146",
    "end": "386858"
  },
  {
    "text": "which is when I take my pre-trained model, but it's not ready for prime time yet.",
    "start": "386858",
    "end": "392555"
  },
  {
    "text": "I need to polish it.",
    "start": "392566",
    "end": "393586"
  },
  {
    "text": "I need to align it closer to my values of how I want my model to behave, my values such as safety and trustworthiness.",
    "start": "393586",
    "end": "401416"
  },
  {
    "text": "There is an active area of research along alignments and how to do this effectively and as efficiently as possible.",
    "start": "402016",
    "end": "409366"
  },
  {
    "text": "Some techniques around alignment include things like reinforcement learning with human feedback, R.L.H.F., which is when I have a human actually sit in the loop.",
    "start": "409696",
    "end": "418066"
  },
  {
    "text": "They're evaluating model performances, scoring it, and then creating a reward function.",
    "start": "418066",
    "end": "422505"
  },
  {
    "text": "It's basically a game and you try and get the model to give responses that are going to have the highest scores from the human annotators",
    "start": "422505",
    "end": "429694"
  },
  {
    "text": "there are other methods that are more data driven, so everything in this picture so far has been under unsupervised learning.",
    "start": "429695",
    "end": "436155"
  },
  {
    "text": "There's been no labeled data.",
    "start": "436156",
    "end": "437595"
  },
  {
    "text": "We scraped a bunch of data from the Internet, we've trained it,",
    "start": "437596",
    "end": "440038"
  },
  {
    "text": "but now if we start to bring some supervised learning back in, we bring some labeled data that's specific for our task,",
    "start": "440038",
    "end": "446516"
  },
  {
    "text": "or that is labeled data that shows their domains.",
    "start": "446516",
    "end": "450884"
  },
  {
    "text": "You can do a process called tuning, which is where you actually go through and update some of the parameters",
    "start": "450886",
    "end": "456521"
  },
  {
    "text": "of this pre-trained model and ground it in that labeled data to be more effective.",
    "start": "456522",
    "end": "460996"
  },
  {
    "text": "There's a number of other techniques, things that will happen in the alignment phase, this is an active area of research,",
    "start": "461866",
    "end": "466938"
  },
  {
    "text": "of things like editing post-processing that you can also do and to improve your model's fairness once it's trained.",
    "start": "466938",
    "end": "472215"
  },
  {
    "start": "473000",
    "end": "646000"
  },
  {
    "text": "So now that we've talked about the different components and strategies,",
    "start": "474206",
    "end": "477232"
  },
  {
    "text": "let's talk about what IBM is doing in order to build efficient and trustworthy foundation models",
    "start": "477232",
    "end": "482876"
  },
  {
    "text": "now being made available through watsonx and infused into our product pipelines.",
    "start": "482877",
    "end": "487075"
  },
  {
    "text": "On the data side, IBM is building what we believe to be one of the largest enterprise datasets for foundation model training.",
    "start": "488056",
    "end": "495976"
  },
  {
    "text": "In addition to building a huge amount of data for training, we're taking a heavy emphasis on the quality of that data,",
    "start": "497686",
    "end": "504204"
  },
  {
    "text": "making sure that it's taken from reputable sources, and that every single data point that we collect and curate goes through legal reviews,",
    "start": "504204",
    "end": "513262"
  },
  {
    "text": "ensuring there's no ownership issues, copyright issues,",
    "start": "513262",
    "end": "516336"
  },
  {
    "text": "and then is red teamed in order to identify what potentially toxic information needs to be taken out to make the quality of the data as safe as possible.",
    "start": "516337",
    "end": "525555"
  },
  {
    "text": "And then finally, we're actively targeting different degrees of specialization, things like finance and cybersecurity,",
    "start": "526336",
    "end": "531792"
  },
  {
    "text": "so that we can train these expert models that are going to be more efficient for our enterprise needs.",
    "start": "531792",
    "end": "537436"
  },
  {
    "text": "On the architecture side, we're building a variety of different style models the encoder decoder models, decoder only models and more.",
    "start": "539036",
    "end": "547076"
  },
  {
    "text": "We're coming up also with net new architectures that have never been seen before.",
    "start": "547466",
    "end": "551936"
  },
  {
    "text": "Models that promise to have ultra efficient operations and have modular components that allow expert knowledge",
    "start": "552266",
    "end": "559311"
  },
  {
    "text": "to be infused in them that we think are going to drive a lot of value in the space.",
    "start": "559311",
    "end": "562766"
  },
  {
    "text": "And we're building models of a variety of sizes from smaller 3 billion size models to 20 billion and larger.",
    "start": "563186",
    "end": "569688"
  },
  {
    "text": "And then finally on the training side, what's really exciting here are some of the advancements coming from IBM Research, including a new technique called LiGO,",
    "start": "571556",
    "end": "582165"
  },
  {
    "text": "Learning to Grow, which is a modular training approach, allowing you to recycle models, reuse them, and thereby save significant carbon and compute costs.",
    "start": "582165",
    "end": "593985"
  },
  {
    "text": "When training these models, leading to some of our models being the some of the most sustainably trained foundation models available.",
    "start": "593986",
    "end": "601545"
  },
  {
    "text": "We've also done all of our training on Vela,",
    "start": "602496",
    "end": "605253"
  },
  {
    "text": "the supercomputer built by IBM Research that we've optimized across the stack to ensure that our training and inference is as efficient as possible.",
    "start": "605254",
    "end": "615065"
  },
  {
    "text": "And then IBM Research is working on a number of advanced alignment techniques,",
    "start": "616056",
    "end": "620781"
  },
  {
    "text": "including things like reinforcement learning with human feedback, but also going into advanced tuning approaches,",
    "start": "620781",
    "end": "627711"
  },
  {
    "text": "allowing models to follow strict rules in terms of what type of behaviors they can exhibit.",
    "start": "627711",
    "end": "634565"
  },
  {
    "text": "If you're interested in learning more, please check out the links below",
    "start": "635696",
    "end": "639099"
  },
  {
    "text": "both about the different innovations that are coming out from IBM Research on model safety and about our watsonx product portfolio.",
    "start": "639099",
    "end": "645385"
  }
]