[
  {
    "text": "If you have a use case for generative AI,",
    "start": "80",
    "end": "2840"
  },
  {
    "text": "how do you decide on which \nfoundation model to pick to run it?",
    "start": "2840",
    "end": "7519"
  },
  {
    "text": "With the huge number of \nfoundation models out there,",
    "start": "7520",
    "end": "11600"
  },
  {
    "text": "It's not an easy question.",
    "start": "11600",
    "end": "12880"
  },
  {
    "text": "Different models are trained on different \ndata and have different parameter counts,",
    "start": "12880",
    "end": "16000"
  },
  {
    "text": "and picking the wrong model can \nhave severe unwanted impact,",
    "start": "16000",
    "end": "20440"
  },
  {
    "text": "like biases originating from the training data \nor hallucinations that are just plain wrong.",
    "start": "20440",
    "end": "25440"
  },
  {
    "text": "Now, one approach is to just pick the largest,  ",
    "start": "25440",
    "end": "29320"
  },
  {
    "text": "most massive model out \nthere to execute every task.",
    "start": "29320",
    "end": "33920"
  },
  {
    "text": "The largest models have huge parameter counts",
    "start": "33920",
    "end": "36719"
  },
  {
    "text": "and are usually pretty good generalists, \nbut with large models come costs,",
    "start": "36720",
    "end": "42480"
  },
  {
    "text": "costs of compute, cost of \ncomplexity and costs of variability.",
    "start": "42480",
    "end": "46160"
  },
  {
    "text": "So often the better approach is to pick the right \nsize model for the specific use case you have.",
    "start": "46160",
    "end": "52400"
  },
  {
    "text": "So let me propose to you an \nAI model selection framework.",
    "start": "52400",
    "end": "58280"
  },
  {
    "text": "It has six pretty simple stages.",
    "start": "58280",
    "end": "60879"
  },
  {
    "text": "Let's take a look at what they areand then \ngive some examples of how this might work.",
    "start": "60880",
    "end": "66200"
  },
  {
    "text": "Now, stage one, that is to \nclearly articulate your use case.",
    "start": "66200",
    "end": "71799"
  },
  {
    "text": "What exactly are you planning \nto use generative A.I. for?",
    "start": "71800",
    "end": "76280"
  },
  {
    "text": "From there you'll list some of the \nmodel options available to you.",
    "start": "76280",
    "end": "79680"
  },
  {
    "text": "Perhaps there are already a subset of foundation \nmodels running that you have access to.",
    "start": "79680",
    "end": "85440"
  },
  {
    "text": "With a short list of models you'll next \nwant to identify each model's size,",
    "start": "85440",
    "end": "89840"
  },
  {
    "text": "performance costs, risks, and deployment methods.",
    "start": "90560",
    "end": "94680"
  },
  {
    "text": "Next, evaluate those model characteristics \nfor your specific use case.",
    "start": "94680",
    "end": "99560"
  },
  {
    "text": "Run some tests.",
    "start": "99560",
    "end": "100799"
  },
  {
    "text": "That's the next stage,",
    "start": "100800",
    "end": "102520"
  },
  {
    "text": "testing options based on your previously \nidentified use case and deployment needs.",
    "start": "102520",
    "end": "106640"
  },
  {
    "text": "And then finally, choose the option \nthat provides the most value.",
    "start": "106640",
    "end": "111360"
  },
  {
    "text": "So let's put this framework to the test.",
    "start": "111360",
    "end": "114560"
  },
  {
    "text": "Now, my use case, we're going to say \nthat is a use case for text generation.",
    "start": "114560",
    "end": "122079"
  },
  {
    "text": "I need the AI to write personalized \nemails for my awesome marketing campaign.",
    "start": "122080",
    "end": "127640"
  },
  {
    "text": "That's stage one.",
    "start": "127640",
    "end": "129440"
  },
  {
    "text": "Now, my organization is already using \ntwo foundation models for other things,  ",
    "start": "129440",
    "end": "133600"
  },
  {
    "text": "so I'll evaluate those.",
    "start": "133600",
    "end": "135400"
  },
  {
    "text": "First of all, we've got Llama 2",
    "start": "135400",
    "end": "139680"
  },
  {
    "text": "and specifically the Llama 2 70 model. a \nfairly large model, 70 billion parameters.",
    "start": "139680",
    "end": "146760"
  },
  {
    "text": "It's from meta and I know it's quite \ngood at some text generation use cases.",
    "start": "146760",
    "end": "151640"
  },
  {
    "text": "Then there's also Granite that we have deployed.",
    "start": "151640",
    "end": "156160"
  },
  {
    "text": "Granite is a smaller general \npurpose model and that's from IBM.",
    "start": "156160",
    "end": "160920"
  },
  {
    "text": "And I know there is a 13 billion parameter model",
    "start": "160920",
    "end": "164680"
  },
  {
    "text": "that I've heard does quite well \nwith text generation as well.",
    "start": "164680",
    "end": "168200"
  },
  {
    "text": "So those are the models I'm going \nto evaluate, Llama 2 and Granite.",
    "start": "168200",
    "end": "174080"
  },
  {
    "text": "Next, we need to evaluate model \nsize, performance, and risks.",
    "start": "174080",
    "end": "178600"
  },
  {
    "text": "And a good place to start \nhere is with the model card.",
    "start": "178600",
    "end": "184960"
  },
  {
    "text": "The model cards might tell \nus if the model has been  ",
    "start": "184960",
    "end": "188320"
  },
  {
    "text": "trained on data specifically for our purposes.",
    "start": "188320",
    "end": "191400"
  },
  {
    "text": "Pre-trained Foundation models are \nfine tuned for specific use cases",
    "start": "191400",
    "end": "195360"
  },
  {
    "text": "such as sentiment analysis or document \nsummarization or maybe text generation.",
    "start": "195360",
    "end": "201200"
  },
  {
    "text": "And that's important to know \nbecause if a model is pre trained",
    "start": "201200",
    "end": "204840"
  },
  {
    "text": "on a use case close to ours, it may \nperform better when processing our prompts",
    "start": "204840",
    "end": "209480"
  },
  {
    "text": "and enable us to use zero shot \nprompting to obtain our desired results.",
    "start": "209480",
    "end": "214159"
  },
  {
    "text": "And that means we can simply \nask the model to perform tasks",
    "start": "214160",
    "end": "217760"
  },
  {
    "text": "without having to provide \nmultiple completed examples first.",
    "start": "217760",
    "end": "222200"
  },
  {
    "text": "Now, when it comes to evaluating model performance \nfor our use case, we can consider three factors.",
    "start": "222200",
    "end": "228879"
  },
  {
    "text": "The first factor that we \nwould consider is accuracy.",
    "start": "228880",
    "end": "233960"
  },
  {
    "text": "Now, accuracy denotes how close \nthe generated output is to the",
    "start": "233960",
    "end": "238120"
  },
  {
    "text": "desired output, and it can be measured\nobjectively and repeatedly",
    "start": "238120",
    "end": "243040"
  },
  {
    "text": "by choosing evaluation metrics that \nare relevant to your use cases.",
    "start": "243040",
    "end": "246719"
  },
  {
    "text": "So for example, if your use case \nrelated to text translation,",
    "start": "246720",
    "end": "251720"
  },
  {
    "text": "the B.L.E.U. - that's the BiLingual \nEvaluation Understudy benchmark,",
    "start": "251720",
    "end": "258560"
  },
  {
    "text": "can be used to indicate the quality \nof the generated translations.",
    "start": "258560",
    "end": "263080"
  },
  {
    "text": "Now the second factor relates \nto reliably of the model.",
    "start": "263080",
    "end": "269680"
  },
  {
    "text": "Now that's a function of several \nfactors actually, such as consistency,  ",
    "start": "269680",
    "end": "273800"
  },
  {
    "text": "explainability and trustworthiness,",
    "start": "273800",
    "end": "275960"
  },
  {
    "text": "as well as how well a model \navoids toxicity like hate speech.",
    "start": "275960",
    "end": "280160"
  },
  {
    "text": "Reliability comes down to trust,",
    "start": "280160",
    "end": "282600"
  },
  {
    "text": "and trust is built through transparency \nand traceability of the training data",
    "start": "282600",
    "end": "286880"
  },
  {
    "text": "and accuracy and reliability of the output.",
    "start": "286880",
    "end": "290400"
  },
  {
    "text": "And then the third factor that is speed.",
    "start": "290400",
    "end": "295080"
  },
  {
    "text": "And specifically we're saying",
    "start": "295080",
    "end": "296840"
  },
  {
    "text": "how quickly does a user get a \nresponse to a submitted prompt?",
    "start": "296840",
    "end": "300440"
  },
  {
    "text": "Now, speed and accuracy \nare often a trade off here.",
    "start": "300440",
    "end": "305000"
  },
  {
    "text": "Larger models may be slower, but \nperhaps deliver a more accurate answer.",
    "start": "305000",
    "end": "309080"
  },
  {
    "text": "Or then again, maybe the smaller model is faster  ",
    "start": "309080",
    "end": "312439"
  },
  {
    "text": "and has minimal differences in \naccuracy to the larger model.",
    "start": "312440",
    "end": "315520"
  },
  {
    "text": "It really comes down to finding the sweet \nspot between performance, speed and cost.",
    "start": "315520",
    "end": "320960"
  },
  {
    "text": "A smaller, less expensive model may not offer  ",
    "start": "320960",
    "end": "323240"
  },
  {
    "text": "performance or accuracy metrics \non par with an expensive one, but",
    "start": "323240",
    "end": "327440"
  },
  {
    "text": "it would still be preferable over the latter.",
    "start": "327440",
    "end": "330000"
  },
  {
    "text": "If you consider any additional benefits, \nthe model might deliver like lower latency",
    "start": "330000",
    "end": "333920"
  },
  {
    "text": "and greater transparency into \nthe model inputs and outputs.",
    "start": "333920",
    "end": "337800"
  },
  {
    "text": "The way to find out is to simply \nselect the model that's likely  ",
    "start": "337800",
    "end": "341000"
  },
  {
    "text": "to deliver the desired output and well, test it.",
    "start": "341000",
    "end": "346120"
  },
  {
    "text": "Test that model with your \nprompts to see if it works,",
    "start": "346120",
    "end": "349919"
  },
  {
    "text": "and then assess the model, performance \nand quality of the output using metrics.",
    "start": "349920",
    "end": "354960"
  },
  {
    "text": "Now, I've mentioned deployment in \npassing, so a quick word on that.",
    "start": "354960",
    "end": "358680"
  },
  {
    "text": "As a decision factor, we need to evaluate where \nand how we want the model and data to be deployed.",
    "start": "358680",
    "end": "365479"
  },
  {
    "text": "So let's say that we're leaning towards\nLlama 2",
    "start": "365480",
    "end": "369920"
  },
  {
    "text": "as our chosen model based on our testing.",
    "start": "369920",
    "end": "374640"
  },
  {
    "text": "Right, cool. Llama 2.",
    "start": "374640",
    "end": "376080"
  },
  {
    "text": "That's an open source model and we could \ninference with it on a public cloud.",
    "start": "376080",
    "end": "380840"
  },
  {
    "text": "So we've got a public cloud already out here.",
    "start": "380840",
    "end": "384800"
  },
  {
    "text": "It's got an element of choice in it, which \nis limited to we can just inference to that.",
    "start": "384800",
    "end": "391599"
  },
  {
    "text": "But if we decide we want to fine tune \nthe model with our own enterprise data,",
    "start": "391600",
    "end": "396560"
  },
  {
    "text": "we might need to deploy it on prem.",
    "start": "396560",
    "end": "400560"
  },
  {
    "text": "So this is where we have \nour own version of Llama two",
    "start": "400560",
    "end": "407120"
  },
  {
    "text": "and we are going to provide fine tuning to it.",
    "start": "407120",
    "end": "410600"
  },
  {
    "text": "Now, deploying on premise \ngives you greater control,",
    "start": "410600",
    "end": "413840"
  },
  {
    "text": "and more security benefits compared \nto a public cloud environment.",
    "start": "413840",
    "end": "417280"
  },
  {
    "text": "But it's an expensive proposition,",
    "start": "417280",
    "end": "419840"
  },
  {
    "text": "especially when factoring \nmodel size and compute power,",
    "start": "419840",
    "end": "423320"
  },
  {
    "text": "including the number of GPUs it takes\nto run a single large language model.",
    "start": "423320",
    "end": "427800"
  },
  {
    "text": "Now, everything we've discussed \nhere is tied to a specific use case,",
    "start": "427800",
    "end": "432440"
  },
  {
    "text": "but of course it's quite likely that any given \norganization will have multiple use cases.",
    "start": "432440",
    "end": "437760"
  },
  {
    "text": "And as we run through this \nmodel selection framework,",
    "start": "437760",
    "end": "441360"
  },
  {
    "text": "we might find that each use case is better \nsuited to a different foundation model.",
    "start": "441360",
    "end": "446719"
  },
  {
    "text": "That's called a multi model approach.",
    "start": "446720",
    "end": "449440"
  },
  {
    "text": "Essentially, not all A.I. models are the \nsame, and neither are your use cases.",
    "start": "449440",
    "end": "455320"
  },
  {
    "text": "And this framework might be just \nwhat you need to pair the models",
    "start": "455320",
    "end": "459960"
  },
  {
    "text": "and the use cases together to find \na winning combination of both.",
    "start": "459960",
    "end": "474080"
  }
]