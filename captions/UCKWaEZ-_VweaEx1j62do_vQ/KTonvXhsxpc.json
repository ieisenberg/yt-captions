[
  {
    "text": "Chances are you've heard about the newest entrants to the very crowded and very competitive realm of AI models,",
    "start": "390",
    "end": "7943"
  },
  {
    "text": "DeepSeek.",
    "start": "7943",
    "end": "8896"
  },
  {
    "text": "It's a startup based in China,",
    "start": "8896",
    "end": "11733"
  },
  {
    "text": "and it caught everyone's attention by taking over OpenAI's",
    "start": "11733",
    "end": "15080"
  },
  {
    "text": "coveted spot for most downloaded free app in the US on Apple's App Store.",
    "start": "15080",
    "end": "20339"
  },
  {
    "text": "So how?",
    "start": "20550",
    "end": "21330"
  },
  {
    "text": "Well, by releasing an open source model that it claims",
    "start": "21330",
    "end": "24892"
  },
  {
    "text": "can match all surpass the performance of other industry leading models",
    "start": "24892",
    "end": "29233"
  },
  {
    "text": "and at a fraction of the cost.",
    "start": "29808",
    "end": "32460"
  },
  {
    "text": "Now, the specific model that's really making a splash from DeepSeek is called DeepSeek R1,",
    "start": "32490",
    "end": "42110"
  },
  {
    "text": "and the R here that implies reasoning,",
    "start": "42110",
    "end": "47323"
  },
  {
    "text": "because this is a reasoning model.",
    "start": "47323",
    "end": "53990"
  },
  {
    "text": "DeepSeek R1 is their reasoning model.",
    "start": "54420",
    "end": "57379"
  },
  {
    "text": "Now DeepSeek our one performs as well as some of the other models, including OpenAI's own reasoning model.",
    "start": "57380",
    "end": "65400"
  },
  {
    "text": "That's called o1,",
    "start": "65430",
    "end": "68123"
  },
  {
    "text": "and it can match or even outperform it across a number of AI benchmarks for math and coding tasks,",
    "start": "68123",
    "end": "74485"
  },
  {
    "text": "which is all the more remarkable because according to DeepSeek,",
    "start": "74485",
    "end": "78646"
  },
  {
    "text": "DeepSeek R1 is trained with far fewer chips and is approximately 96% cheaper to run than o1.",
    "start": "78646",
    "end": "88859"
  },
  {
    "text": "Now, unlike previous AI models which produce an answer without explaining the why,",
    "start": "89430",
    "end": "93920"
  },
  {
    "text": "a reasoning model solves complex problems by breaking them down into steps.",
    "start": "93920",
    "end": "100078"
  },
  {
    "text": "So before answering a user query, the model spends time thinking,",
    "start": "100440",
    "end": "104840"
  },
  {
    "text": "thinking in air quotes here,",
    "start": "105328",
    "end": "106792"
  },
  {
    "text": "and that thinking time could be a few seconds or even minutes.",
    "start": "107193",
    "end": "110849"
  },
  {
    "text": "Now, during this time, the model is performing step by step analysis through a process that is known as chain of thought.",
    "start": "110970",
    "end": "122570"
  },
  {
    "text": "And unlike other reasoning models, R1 shows the use of that chain of thought process as it breaks the problem down,",
    "start": "123490",
    "end": "131910"
  },
  {
    "text": "as it generates insights, as it backtrack and says it needs to,",
    "start": "131910",
    "end": "135520"
  },
  {
    "text": "and as it ultimately arrives at an answer.",
    "start": "135730",
    "end": "138532"
  },
  {
    "text": "Now I'm going to get into how this model works.",
    "start": "139195",
    "end": "141740"
  },
  {
    "text": "But before that, let's talk about how it came to be a DeepSeek R1 seems to have come out of nowhere.",
    "start": "141760",
    "end": "149189"
  },
  {
    "text": "But there are in fact many DeepSeek models that brought us to this point a model avalanche, if you like.",
    "start": "149200",
    "end": "156668"
  },
  {
    "text": "And my colleague Aaron can help dig us out.",
    "start": "156910",
    "end": "160270"
  },
  {
    "text": "Well, thanks, Martin.",
    "start": "160900",
    "end": "161620"
  },
  {
    "text": "There is certainly a lot to dig out here.",
    "start": "161620",
    "end": "163360"
  },
  {
    "text": "There's a lot of these models.",
    "start": "163720",
    "end": "164889"
  },
  {
    "text": "But let's start from the very top in the beginning of all this.",
    "start": "164890",
    "end": "167379"
  },
  {
    "text": "So we began and we go to, let's say, DeepSeek version one,",
    "start": "167770",
    "end": "171867"
  },
  {
    "text": "which is a 67 billion model that was released in January of 2024.",
    "start": "171867",
    "end": "175870"
  },
  {
    "text": "Now, this is a traditional transformer with a focus on the feedforward neural networks.",
    "start": "176110",
    "end": "180580"
  },
  {
    "text": "This gets us down into DeepSeek version two, which really put this on the map.",
    "start": "181060",
    "end": "185020"
  },
  {
    "text": "This is a very large 236 billion model that was released not that far away from the original, which is June 2024.",
    "start": "185320",
    "end": "193060"
  },
  {
    "text": "But to put this into perspective, there are really two novel aspects around this model.",
    "start": "193450",
    "end": "197440"
  },
  {
    "text": "The first one,",
    "start": "197470",
    "end": "198094"
  },
  {
    "text": "was the multi-headed laden attention.",
    "start": "198524",
    "end": "200919"
  },
  {
    "text": "And the second aspect was the DeepSeek mixture of experts.",
    "start": "201220",
    "end": "204349"
  },
  {
    "text": "It just made the model really fast and performant.",
    "start": "204370",
    "end": "206889"
  },
  {
    "text": "And it set us up for success for the DeepSeek version three, which was released December of 2024.",
    "start": "207160",
    "end": "212769"
  },
  {
    "text": "Now, this one is even bigger.",
    "start": "213130",
    "end": "214580"
  },
  {
    "text": "It's 671 billion parameters.",
    "start": "214600",
    "end": "217299"
  },
  {
    "text": "But this is where we began to see the introduction of using reinforcement learning with that model,",
    "start": "217600",
    "end": "223447"
  },
  {
    "text": "and some other contributions that this model had is it was able to balance load across many GPUs,",
    "start": "223447",
    "end": "228976"
  },
  {
    "text": "because they used a lot of H800s within their infrastructure and that was also built around on top of DeepSeek v2.",
    "start": "228976",
    "end": "236770"
  },
  {
    "text": "So all these models accumulate and build on top of each other, which gets us down into DeepSeek R1-Zero,",
    "start": "236800",
    "end": "242811"
  },
  {
    "text": "which was released in January of 2025.",
    "start": "242811",
    "end": "245229"
  },
  {
    "text": "So this is the first of the reasoning model is now, right?",
    "start": "245320",
    "end": "248889"
  },
  {
    "text": "It is, yeah.",
    "start": "248920",
    "end": "249760"
  },
  {
    "text": "And it's really neat how they began to train, you know, these types of models, right?",
    "start": "249760",
    "end": "253799"
  },
  {
    "text": "So it's a type of fine tuning.",
    "start": "253810",
    "end": "255069"
  },
  {
    "text": "But on this one, the exclusively use reinforcement learning,",
    "start": "255340",
    "end": "259001"
  },
  {
    "text": "which is a way where you have policies and you want to reward or you want to penalize",
    "start": "259001",
    "end": "263639"
  },
  {
    "text": "the model for some action that it has taken or output that it has taken in itself learns over time,",
    "start": "263639",
    "end": "269079"
  },
  {
    "text": "and it was very performant.",
    "start": "269650",
    "end": "270940"
  },
  {
    "text": "It did well, but it got even better with DeepSeek R1, right, which was again built on top of R1-Zero,",
    "start": "270940",
    "end": "278109"
  },
  {
    "text": "and this one used a combination of reinforcement learning,",
    "start": "278500",
    "end": "281646"
  },
  {
    "text": "and supervised fine tuning the best of both worlds so that it could even be better,",
    "start": "281646",
    "end": "286149"
  },
  {
    "text": "and it's very close to performance on many standards and benchmarks as some of these OpenAI models we have now.",
    "start": "286150",
    "end": "292029"
  },
  {
    "text": "And this gets us down into now distilled models, which is like a whole other paradigm.",
    "start": "292510",
    "end": "296679"
  },
  {
    "text": "Distilled models.",
    "start": "296680",
    "end": "297310"
  },
  {
    "text": "Okay, so tell me what that is all about.",
    "start": "297310",
    "end": "300399"
  },
  {
    "text": "Yeah, great question and comment.",
    "start": "300850",
    "end": "302759"
  },
  {
    "text": "So first of all, for a distilled model is where you have a student model, which is a very small model,",
    "start": "302770",
    "end": "309014"
  },
  {
    "text": "and you have the teacher model, which is very big, and you want to distill",
    "start": "309343",
    "end": "313452"
  },
  {
    "text": "or extract knowledge from the teacher model down into the student model,",
    "start": "313453",
    "end": "317619"
  },
  {
    "text": "and some aspects you could think of it as model compression.",
    "start": "317620",
    "end": "320798"
  },
  {
    "text": "But one interesting aspect around this is this is not just compression or transferring knowledge,",
    "start": "321100",
    "end": "325664"
  },
  {
    "text": "but it's model translation because we're going from the R1-Zero, right? Which is",
    "start": "325664",
    "end": "330329"
  },
  {
    "text": "one of those mixture of expert models down into, for example, a Llama series model,",
    "start": "330329",
    "end": "336412"
  },
  {
    "text": "which is not a mixture of experts, but it's a traditional transformer, right?",
    "start": "336412",
    "end": "340839"
  },
  {
    "text": "So, so you're going from one architecture type to another.",
    "start": "340840",
    "end": "343149"
  },
  {
    "text": "And we do the same with Qwen, right?",
    "start": "343300",
    "end": "345220"
  },
  {
    "text": "So there's different series of models that are the foundation that we then distill into from the R1-Zero.",
    "start": "345220",
    "end": "351879"
  },
  {
    "text": "Well, thanks.",
    "start": "352910",
    "end": "353450"
  },
  {
    "text": "It's really interesting to get the history behind all this.",
    "start": "353450",
    "end": "355649"
  },
  {
    "text": "It didn't come from nowhere,",
    "start": "355670",
    "end": "357800"
  },
  {
    "text": "but with all of these distilled models coming, I think you might need your shovel back to dig your way out of those.",
    "start": "358160",
    "end": "364170"
  },
  {
    "text": "Thank you very much.",
    "start": "364190",
    "end": "364970"
  },
  {
    "text": "There's going to be a lot of distilled models.",
    "start": "364970",
    "end": "366709"
  },
  {
    "text": "So you're exactly right.",
    "start": "366710",
    "end": "367789"
  },
  {
    "text": "I think you're going to go dig.",
    "start": "367790",
    "end": "369199"
  },
  {
    "text": "Thanks.",
    "start": "369230",
    "end": "369920"
  },
  {
    "text": "So our one didn't come from nowhere is an evolution of other models, but",
    "start": "369920",
    "end": "375313"
  },
  {
    "text": "how does DeepSeek operate at such comparatively low cost?",
    "start": "375313",
    "end": "379879"
  },
  {
    "text": "Well, by using a fraction of the highly specialized Invidia chips used by their American competitors to train their systems.",
    "start": "379910",
    "end": "389209"
  },
  {
    "text": "If I can illustrate this in a graph.",
    "start": "389480",
    "end": "391309"
  },
  {
    "text": "So if we consider different types of model and then the number of GPUs that they use.",
    "start": "391310",
    "end": "398509"
  },
  {
    "text": "Well, DeepSeek engineers, for example, they said that they only need 2000",
    "start": "399050",
    "end": "405630"
  },
  {
    "text": "GPUs  that's graphical processing units to train the DeepSeek V3 Model, DeepSeek V3.",
    "start": "405631",
    "end": "416751"
  },
  {
    "text": "Now in isolation,",
    "start": "417194",
    "end": "418962"
  },
  {
    "text": "what does that mean?",
    "start": "419120",
    "end": "419900"
  },
  {
    "text": "Is that good? Is that bad?",
    "start": "419900",
    "end": "421100"
  },
  {
    "text": "Well, by contrast, meta said that the company was training that latest opensource model.",
    "start": "421100",
    "end": "427549"
  },
  {
    "text": "That's Llama 4 and they are using a computer cluster with over 100,000 Nvidia GPUs.",
    "start": "427550",
    "end": "438439"
  },
  {
    "text": "So that brings up the question of how is it so efficient?",
    "start": "439160",
    "end": "442759"
  },
  {
    "text": "Well, DeepSeek R1 combines chain of thought reasoning with a process called reinforcement learning.",
    "start": "443090",
    "end": "452810"
  },
  {
    "text": "This is a capability that Aaron mentioned just now which arrived at the V3 model of DeepSeek.",
    "start": "453170",
    "end": "459529"
  },
  {
    "text": "And here an autonomous agent learns to perform a task through",
    "start": "459860",
    "end": "464064"
  },
  {
    "text": "trial and error without any instructions from a human user.",
    "start": "464065",
    "end": "468560"
  },
  {
    "text": "Now, traditionally, models will improve their ability to reason",
    "start": "468680",
    "end": "473022"
  },
  {
    "text": "by being trained on labeled examples of correct or incorrect behavior.",
    "start": "473023",
    "end": "477410"
  },
  {
    "text": "That's known as supervised learning, or by",
    "start": "477440",
    "end": "480760"
  },
  {
    "text": "extracting information from hidden patterns that send us unsupervised learning.",
    "start": "480760",
    "end": "484640"
  },
  {
    "text": "But the key hypothesis here with reinforcement learning is to reward the model for correctness,",
    "start": "484880",
    "end": "492879"
  },
  {
    "text": "no matter how it arrived at the right answer and let the model discover the best way to think all on its own.",
    "start": "492879",
    "end": "501050"
  },
  {
    "text": "Now DeepSeek R1 also uses a mixture of experts, architectural or MoE,",
    "start": "501920",
    "end": "508558"
  },
  {
    "text": "and a mixture of experts architecture is considerably less resource intensive to train.",
    "start": "508558",
    "end": "515629"
  },
  {
    "text": "Now the MoE architecture divides an AI model up into separate entities or",
    "start": "515960",
    "end": "521197"
  },
  {
    "text": "sub networks, which we can think of as being individual experts.",
    "start": "521197",
    "end": "526520"
  },
  {
    "text": "So in my little neural network here, I'm going to create three",
    "start": "526880",
    "end": "531440"
  },
  {
    "text": "experts and a real MoE architecture probably have quite a bit more than that.",
    "start": "532670",
    "end": "536570"
  },
  {
    "text": "But each one of these is specialized in a subset of the input data,",
    "start": "537020",
    "end": "542483"
  },
  {
    "text": "and the model only activates the specific experts needed for a given task. So a request comes in.",
    "start": "542483",
    "end": "548659"
  },
  {
    "text": "We activate the experts that we need and we only use those rather than activating the entire neural network.",
    "start": "549020",
    "end": "557410"
  },
  {
    "text": "So consequently, the MoE architecture reduces computational costs",
    "start": "557420",
    "end": "561652"
  },
  {
    "text": "during pre-training and achieves faster performance during inference time",
    "start": "561652",
    "end": "565819"
  },
  {
    "text": "and look MoE, that architecture isn't unique to models from DeepSeek.",
    "start": "565819",
    "end": "570498"
  },
  {
    "text": "There are models from the French AI company Mistral that also use this,",
    "start": "570710",
    "end": "576099"
  },
  {
    "text": "and in fact the IBM Granite model that is also built on a mixture of experts architecture.",
    "start": "576560",
    "end": "584960"
  },
  {
    "text": "So it's a commonly used architecture.",
    "start": "585020",
    "end": "587119"
  },
  {
    "text": "So that is DeepSeek R1.",
    "start": "588090",
    "end": "591169"
  },
  {
    "text": "It's an AI reasoning model that is matching other industry leading models on reasoning benchmarks,",
    "start": "591210",
    "end": "597593"
  },
  {
    "text": "while being delivered at a fraction of the cost in both training and inference.",
    "start": "597593",
    "end": "602640"
  },
  {
    "text": "All of which makes me think this is an exciting time for AI reasoning models.",
    "start": "603060",
    "end": "609390"
  }
]