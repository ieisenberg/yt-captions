[
  {
    "start": "0",
    "end": "41000"
  },
  {
    "text": "On a scale from 0 to 10, how\nbig of a deal is DeepSeek R1? Kate Soule is Director of Technical\nProduct Management for Granite.",
    "start": "80",
    "end": "7029"
  },
  {
    "text": "Kate, welcome to the show. What do you think? I'm going to take maybe a little\nbit of a controversial position. I'm going to say 5. Chris Hay, Distinguished Engineer\nand CTO of Customer Transformation.",
    "start": "7220",
    "end": "15299"
  },
  {
    "text": "Chris, welcome back as always. 0 to 10. What do you think? 9. 11 or 9.",
    "start": "15339",
    "end": "20340"
  },
  {
    "text": "9, but I'm not sure which\nis the bigger number. Wow, that is a niche reference. And finally, Aaron Baughman is IBM\nFellow and Master Inventor, zero to 10.",
    "start": "20390",
    "end": "28810"
  },
  {
    "text": "Aaron, what do you think? Yeah, that's a great question,\nand I think we're gonna be right in between the other two at a 7.5.",
    "start": "28820",
    "end": "35039"
  },
  {
    "text": "All that and more on\ntoday's Mixture of Experts.",
    "start": "35039",
    "end": "37720"
  },
  {
    "start": "41000",
    "end": "1260000"
  },
  {
    "text": "I'm Tim Hwang, and welcome\nto Mixture of Experts. This is episode 40 of the\nMixture of Experts show.",
    "start": "42750",
    "end": "48785"
  },
  {
    "text": "I'm really excited to meet this\nmilestone with an all-star cast. Each week, MoE is the place to tune\ninto to hear the news and analysis",
    "start": "49065",
    "end": "56275"
  },
  {
    "text": "on the biggest headlines and\ntrends in artificial intelligence. And today we're all going\nto talk about DeepSeek-R1.",
    "start": "56285",
    "end": "62314"
  },
  {
    "text": "It's basically anything that anyone is\ntalking about right now is DeepSeek-R1. It's the talk of the AI chatter\nclass, it's rocking markets, and",
    "start": "62335",
    "end": "69663"
  },
  {
    "text": "even my dad is texting me about it. Um, so what I want to do is start\nthe first segment with a little",
    "start": "69675",
    "end": "75100"
  },
  {
    "text": "bit of DeepSeek-R1 myth busting. Um, if you've been anywhere around\nour, uh, AI, uh, in the last",
    "start": "75100",
    "end": "81439"
  },
  {
    "text": "week, you know the basic story. Um, there is a, a Chinese lab, uh,\nDeepSeek that has released a new",
    "start": "81440",
    "end": "87569"
  },
  {
    "text": "model called R1, uh, that is both\nopen source and competitive with the state of the art models coming out of\nAnthropic, OpenAI and all the names",
    "start": "87569",
    "end": "95750"
  },
  {
    "text": "that we're really familiar with. And there has been so much hype\nabout this story that as I said,",
    "start": "95760",
    "end": "101330"
  },
  {
    "text": "even my dad's texting me about it. And a lot of the mainstream\ncoverage has actually been getting a lot of the facts wrong.",
    "start": "101330",
    "end": "107078"
  },
  {
    "text": "So kind of what I want to start with is\njust to knock down a bunch of myths, so we can kind of calibrate as we really kind\nof peel back and talk about this story.",
    "start": "107509",
    "end": "115400"
  },
  {
    "text": "And, Kate, I want to start with you,\nbecause I know you were, uh, angry about this in the show Slack, and\nso I wanted to give you a chance to",
    "start": "115890",
    "end": "121909"
  },
  {
    "text": "kind of like, you know, let loose. Um, I think the first meme that we've\nheard in a lot of this mainstream",
    "start": "121909",
    "end": "126898"
  },
  {
    "text": "news coverage is we can now train\nstate of the art models for 5.5 million dollars.",
    "start": "126910",
    "end": "132645"
  },
  {
    "text": "And that's so crazy expensive\nrelative to the kind of numbers that we've heard before, right? I think the Stargate price\nwas a hundred billion dollars",
    "start": "132895",
    "end": "140504"
  },
  {
    "text": "or something crazy like that. Um, so Kate, how true is that number? Can we, can we really train models for 5.5 million dollars now?",
    "start": "140505",
    "end": "146875"
  },
  {
    "text": "So, first, the number is true, it's\npublished in the, uh, presumably, it's published in the paper, DeepSeek isn't\nnecessarily hiding anything about this",
    "start": "147660",
    "end": "155660"
  },
  {
    "text": "number, it's heavily caveated if you,\nif you look at it, but the takeaway that people are driving from this number is a\nlittle bit crazy, so, yes, training one",
    "start": "155660",
    "end": "167155"
  },
  {
    "text": "iteration of a base model, DeepSeek-V3,\nby the way, this all came out in December.",
    "start": "167155",
    "end": "172494"
  },
  {
    "text": "This isn't late breaking\nnews as of last week. Back in December, they trained this model. They said one iteration of\ntraining it would cost about 5.6 million dollars.",
    "start": "172494",
    "end": "181813"
  },
  {
    "text": "But that's like saying if a startup could\ngo and train a model for the same cost.",
    "start": "182425",
    "end": "187585"
  },
  {
    "text": "That's like saying if I'm going\nto go run a marathon that the only distance I'll ever run is 26 miles.",
    "start": "187595",
    "end": "192155"
  },
  {
    "text": "The reality is you're\ngoing to train for months. Practicing, training, running hundreds\nand thousands of miles potentially leading",
    "start": "192684",
    "end": "200370"
  },
  {
    "text": "up to that one race that then takes 26.2 miles. And if you look at what that number\nis and take that metaphor even a step",
    "start": "200370",
    "end": "208849"
  },
  {
    "text": "further, it's like saying, okay, what\nif I'm running a race, but I take a break every mile, I stop, I, you know,\ntake a drink of water, I take a nap, I",
    "start": "208880",
    "end": "217935"
  },
  {
    "text": "come back the next day, I keep running. And you only add up your time from\nthe actual miles that you're running",
    "start": "217935",
    "end": "222995"
  },
  {
    "text": "in the race, not all your breaks. That's like the equivalent of\nwhat this number represents. It's a really valuable number\nto understand and impressive.",
    "start": "222995",
    "end": "230254"
  },
  {
    "text": "The parts that they're measuring, they\ndid bring down a lot in efficiency. But that number does not represent\nthe cost to go and now train a model.",
    "start": "230294",
    "end": "237765"
  },
  {
    "text": "It's not like we're going to have startups\nnow flooding, you know, the ecosystem with their own version of 600 billion\nparameter mixture of expert models.",
    "start": "237765",
    "end": "244704"
  },
  {
    "text": "That's super helpful. Yeah, and I think it's\na great calibration. I want to kind of pick up on that\nlast thing that you said, which is there is a lot that's new here\nfrom an efficiency standpoint.",
    "start": "244755",
    "end": "253625"
  },
  {
    "text": "And maybe Chris, I'll toss it to you for\nsort of the next meme that we're hearing kind of flow around in this space, which\nis DeepSeek-R1 is a huge breakthrough.",
    "start": "253644",
    "end": "262360"
  },
  {
    "text": "Models are running way more\nefficiently than they used to. You know, dot, dot, dot,\nDeepSeek is so far ahead.",
    "start": "262400",
    "end": "268280"
  },
  {
    "text": "I know you said that you, uh, felt\nlike this model was a big deal, like 9.11, um, a bunch of a big deal.",
    "start": "268640",
    "end": "275460"
  },
  {
    "text": "Uh, but can you tell us a little bit\nabout, like, has DeepSeek really unlocked some novel things, and if so, how big of\na deal are these novel things that they're",
    "start": "275520",
    "end": "283984"
  },
  {
    "text": "really uncovering with this new model? I said 9.11 or 9.9, so clearly,\nTim, you think 9.11 is the",
    "start": "284145",
    "end": "293054"
  },
  {
    "text": "bigger number out of those two. Sorry, there's some\nuncertainty bars there. I actually think it is a big deal, so\nI think there is a few things, which",
    "start": "293085",
    "end": "302635"
  },
  {
    "text": "is, we're sort of joining everything\ntogether there, so we're actually saying, okay, here's the base model,\nand then there's the RL training",
    "start": "302635",
    "end": "311294"
  },
  {
    "text": "for the R1 part of that, right? And actually, if we separate out the\nkind of the DeepSeek-V3 version from",
    "start": "311304",
    "end": "318884"
  },
  {
    "text": "the RL training there for a second,\nI think there is a big deal there. Because the reality is... Nevermind\nthe 5.5 million bucks, right?",
    "start": "318885",
    "end": "326670"
  },
  {
    "text": "You are going to be able to take\nan existing base model that has been pre-trained and then you\nare going to be able to do RL",
    "start": "326900",
    "end": "334780"
  },
  {
    "text": "training over the top of that. You're going to be able to take\nyour cold start fine tune data. So you can take a relatively small\namount of data set and then put that on",
    "start": "334790",
    "end": "342779"
  },
  {
    "text": "top and train it to do amazing tasks. And I know that myself, right? Because I took like a tiny model\nmyself, one and a half billion",
    "start": "342840",
    "end": "350840"
  },
  {
    "text": "parameters, absolutely tiny Qwen model. And then I put maybe a thousand\nlines of SFT data, right?",
    "start": "350840",
    "end": "358210"
  },
  {
    "text": "And I got that thing to be\nable to tune math, right? Basic arithmetic at the kind\nof same level as GPT 4o, right?",
    "start": "358440",
    "end": "366150"
  },
  {
    "text": "Just myself and I, I'm telling\nyou right now, I love IBM. They do not pay me five\nand a half million dollars.",
    "start": "366159",
    "end": "372379"
  },
  {
    "text": "That was on my laptop. So, so this is a big deal and it's a\nbig deal because the thing that they're",
    "start": "372570",
    "end": "378484"
  },
  {
    "text": "showing there is long chain of thought has\na huge impact and accurate data because",
    "start": "378484",
    "end": "385405"
  },
  {
    "text": "actually even to the point of RL, they\nthey started with pure RL training, right?",
    "start": "385475",
    "end": "391475"
  },
  {
    "text": "So they actually just said, here's your\nrewards for not doing anything else. And, and we're going to\ntrain the model that way.",
    "start": "391475",
    "end": "397115"
  },
  {
    "text": "And then they went to say, actually, if\nwe, if we do one round of fine tuning with a really good chain of thought, a set\nof data, maybe a few thousand rows worth",
    "start": "397335",
    "end": "406835"
  },
  {
    "text": "of data, and then do RL training after\nthat, then we get much better results. So actually what they're showing is\nthat we can maybe stop obsessing about",
    "start": "406835",
    "end": "416824"
  },
  {
    "text": "pre-training so much and we can get\ninto this kind of post training world and Inference time compute world and\nfor that you don't even need five and",
    "start": "416854",
    "end": "425804"
  },
  {
    "text": "a half million dollars. Just your laptop\nand a little bit of tenacity and a little bit of GPU is gonna do that job.",
    "start": "425804",
    "end": "432094"
  },
  {
    "text": "That's awesome yeah, we're gonna talk\na little bit more about RL and chain of thought in just a second, but\nAaron I think before we move to that one other question I want to ask you\nthe kind of other third big meme is",
    "start": "432104",
    "end": "441265"
  },
  {
    "text": "that everybody suddenly discovered\nJevons paradox, uh, this week.",
    "start": "441265",
    "end": "446125"
  },
  {
    "text": "Um, and I think one of the narratives\nthat popped up is NVIDIA is doomed. You need a lot less compute, uh,\ncompute for these models now.",
    "start": "446385",
    "end": "453415"
  },
  {
    "text": "You know, NVIDIA's stock\nprice took a tumble. I bought the dip for what it's worth. Um, and I want to say to, uh, I guess\nfor Aaron, if you wanted to kind of",
    "start": "453654",
    "end": "461215"
  },
  {
    "text": "respond to this, this question, or\nwhether or not you think it's a myth at all, is, um, are we going to need\na lot less compute in the future?",
    "start": "461215",
    "end": "467764"
  },
  {
    "text": "Is NVIDIA doomed? Like how should we read this? And if you want to explain Jevons\nparadox, you can go ahead too.",
    "start": "468020",
    "end": "472630"
  },
  {
    "text": "Yeah, I mean, I mean, so, so I think,\nyou know, fundamentally, you know, that that's an interesting notion. Um, you know, but I tend to follow,\num, the dynamics of AI, you know, which",
    "start": "473400",
    "end": "481460"
  },
  {
    "text": "comes in to me three different areas. One is the scaling law, right? Is that, you know, it tends to say\nthat, um, as you scale up the train",
    "start": "481490",
    "end": "489010"
  },
  {
    "text": "of AI systems, you get better results,\nwhich means bigger models are better. Generally, right? And then the shifting curve\nwhere new ideas are making",
    "start": "489010",
    "end": "497090"
  },
  {
    "text": "training more efficient, right? And so this, you know,\naffects that scaling law. So the more new ideas you get,\nyou know, the smaller models",
    "start": "497090",
    "end": "505719"
  },
  {
    "text": "become more powerful, right? But then there's a third, which is\na shifting paradigm of these big revolutionary ideas and can an order\nof magnitude change the scale of which",
    "start": "505719",
    "end": "515744"
  },
  {
    "text": "you actually need to train these models\nin order to get performance, right? And so I think by having those points\none, two, and three laid out, uh, which",
    "start": "515745",
    "end": "523575"
  },
  {
    "text": "is, you know, backed by a lot of research\nover time, you know, you know, I think that, yes, there, there's always going\nto be a demand for GPUs, but I do think",
    "start": "523575",
    "end": "531854"
  },
  {
    "text": "that, um, there's going to be different\nchip architectures that are coming out, but also, um, if you look at some of the\nefficiency gains that- that V2 had, um,",
    "start": "531865",
    "end": "540110"
  },
  {
    "text": "such as a multi-head attention, were they\nable to cache a lot- a lot of the weights? The, the token throughput is incredible,\nuh, that they were able to achieve,",
    "start": "540110",
    "end": "547400"
  },
  {
    "text": "but I think that was one of the\nbigger innovations that they had. Uh, and then the second one was there,\nwhat they call the DeepSeek, um, MoE, uh,",
    "start": "547400",
    "end": "555040"
  },
  {
    "text": "where they're able to sort of partition\nout and share uh, knowledge amongst these different agents that they can have.",
    "start": "555040",
    "end": "561079"
  },
  {
    "text": "And that also helps, but those two\nthings where some of those, um, uh, pieces that gave us the shifting\ncurve on that scale law, which said,",
    "start": "561080",
    "end": "568440"
  },
  {
    "text": "okay, I don't need as many GPUs now. But if you look at the foundational\nmodel, um, so if you go to,",
    "start": "568440",
    "end": "574529"
  },
  {
    "text": "let's say, DeepSeek-V2, right? It's big. It's a very big model and v3 is\neven bigger with a- what is it?",
    "start": "574539",
    "end": "580650"
  },
  {
    "text": "671 billion parameters, right? That's a very big model. Yeah, it's, it's chunky.",
    "start": "580650",
    "end": "585300"
  },
  {
    "text": "Yeah. So, I mean, I mean, that\nthat's, it's very fun, right? To watch that curve. And I, and I think that we'll see\nagglomeration of models together, we",
    "start": "585810",
    "end": "594439"
  },
  {
    "text": "can do reverse distillation, right? To create and combine\nsmaller models together.",
    "start": "594439",
    "end": "600120"
  },
  {
    "text": "Uh, you can do model distillation to\ncreate smaller models, you know, um, but, but it's, it's going to be fun.",
    "start": "600505",
    "end": "606715"
  },
  {
    "text": "I want to maybe pick out a point\nthat Chris actually mentioned that I think is really important, which\nis the like, can we just stop",
    "start": "606835",
    "end": "614135"
  },
  {
    "text": "worrying about pre-training now? Because I think everyone is talking about\nthis 5.5, 5.6 million dollar number.",
    "start": "614135",
    "end": "621055"
  },
  {
    "text": "And they're tagging it to all of\nthese amazing performance improvements that we're seeing in the R1 model and\nthat, and the distilled models, and",
    "start": "621455",
    "end": "629934"
  },
  {
    "text": "they're kind of equating the two and\nsaying, all right, now we can just go and we are getting like this crazy\nperformance at a pretty minimal cost.",
    "start": "629935",
    "end": "635824"
  },
  {
    "text": "And I think it's really important to\ndisambiguate these two things, right?",
    "start": "636424",
    "end": "646840"
  },
  {
    "text": "A step in this process costs about 5.6 million, the\ntrue cost of building this pre-training model is likely orders of magnitude\nhigher, but Regardless, it almost",
    "start": "646850",
    "end": "656915"
  },
  {
    "text": "doesn't matter, like this 5.6 million\nnumber doesn't even matter because you can take this big model that's now open\nsource and distill it basically for free",
    "start": "656915",
    "end": "665574"
  },
  {
    "text": "on top of other open source, smaller\nmodels that are out there to get crazy performance improvements and build.",
    "start": "665645",
    "end": "672255"
  },
  {
    "text": "So it's not that startups are going to\ngo and build and pre-train their own 600 billion parameter model because\nthe cost is only 5.6 million dollars.",
    "start": "672255",
    "end": "680444"
  },
  {
    "text": "That's the wrong takeaway. It's that we now have the ability to\ndistill and thanks to more and more",
    "start": "680455",
    "end": "685505"
  },
  {
    "text": "competitive models being put into\nthe open source that distillation is becoming even more powerful and use\nreinforcement learning as a technique",
    "start": "685505",
    "end": "692105"
  },
  {
    "text": "that DeepSeek used really effectively to\ngo and build our own smaller versions of these models that are really powerful.",
    "start": "692114",
    "end": "698945"
  },
  {
    "text": "And that is where there's actually very\nlow barrier to entry now, as Chris is saying, you know, doing it on your laptop.",
    "start": "698985",
    "end": "704195"
  },
  {
    "text": "Yeah, yeah, yeah. I find that, you know, very, very nice\nbecause it's like a house of cards, right? I mean, they're only quoting the top card.",
    "start": "704265",
    "end": "710464"
  },
  {
    "text": "They're not quoting any of\nthe cards at the bottom. And if you move one of those cards at the\nbottom, the whole house collapse, right? And so that 5.5 million is only the\ncost associated with maybe you know,",
    "start": "710474",
    "end": "720529"
  },
  {
    "text": "you know, one, one epoch right of\nthis type of training and that's it. But if you look at the hardware, even\nthat they use, what are the H800's?",
    "start": "720540",
    "end": "728260"
  },
  {
    "text": "Just procuring those alone or using\nthose as a service is expensive, right? Um, and so, so they're excluding\nlots of costs associated with",
    "start": "728670",
    "end": "735809"
  },
  {
    "text": "prior research ablation studies and\nlots of different things, right? Which um that that number is\nvery, very much misleading.",
    "start": "735809",
    "end": "742850"
  },
  {
    "text": "Uh, Chris, I see you nodding. Do you want to jump in? I don't know if you have a comment Uh, no, I was just nodding in\nagreement because I'm a very",
    "start": "742900",
    "end": "750145"
  },
  {
    "text": "kind and collaborative person. Uh, no, I, I, I, no, I,\nI, I absolutely agree.",
    "start": "750145",
    "end": "757925"
  },
  {
    "text": "I, I think that, um, you're gonna\ngo for the big hit numbers, right? You're gonna say, we did this super cheap.",
    "start": "758025",
    "end": "764574"
  },
  {
    "text": "And you are really going to miss\nout all the steps that took you to get there in the first place, right?",
    "start": "764954",
    "end": "770444"
  },
  {
    "text": "And, um, and as Kate probably knows\nbetter than anyone, right, that the amount of experimentation that it\ntakes for these models, right, to",
    "start": "770495",
    "end": "778594"
  },
  {
    "text": "get to the final version is a lot. So the, the actual final epoch, as\nAaron was saying, that final training",
    "start": "778594",
    "end": "785185"
  },
  {
    "text": "run, that's just, that's just the\nkind of the end of the road there. So, um, but.",
    "start": "785185",
    "end": "789745"
  },
  {
    "text": "You know what? No one wants to hear about the\nbig journey going up there. They want to hear the big number. We're in a hype industry, baby.",
    "start": "790290",
    "end": "796290"
  },
  {
    "text": "So we'll, yeah, five and a half million. Here we go, right? Kate, I guess maybe one last myth I've\nseen kind of popping up that might",
    "start": "796290",
    "end": "802419"
  },
  {
    "text": "be good to address before we, we do a\nsegment on distillation, because it's already come up a couple times, and\nI think it is worthwhile to explain",
    "start": "802420",
    "end": "808470"
  },
  {
    "text": "why, what it is and why it matters. But maybe one last thing to cover before\nwe get to that, Kate, is, um, on the",
    "start": "808470",
    "end": "814000"
  },
  {
    "text": "point about RL, um, it feels like, the\nDeepSeek narrative has also been a little",
    "start": "814000",
    "end": "819130"
  },
  {
    "text": "bit about like the revenge of RL, like\nreinforcement learning's back, baby. Um, and I know some people have gone so\nfar to be like, everything is RL now.",
    "start": "819130",
    "end": "826899"
  },
  {
    "text": "You know, fine tuning is dead. Um, do you want to talk\na little bit about that? Like even, you know, with everything that\nwe've said, like how much does R1 indicate",
    "start": "826919",
    "end": "834149"
  },
  {
    "text": "to us that really RL will be kind of like\nthe more dominant method for these types of fine tuning efforts going forward?",
    "start": "834149",
    "end": "839940"
  },
  {
    "text": "Yeah, and I'm really curious to get\nChris's take on this because I know he's just run these experiments right locally\non his own laptop, but so DeepSeek in",
    "start": "840070",
    "end": "848650"
  },
  {
    "text": "their paper, they trained two models, uh,\nin addition to all the smaller distilled models that they, that they worked on.",
    "start": "848650",
    "end": "854600"
  },
  {
    "text": "One model was trained with just\nreinforcement learning only. So there's no additional\ndata that's added.",
    "start": "855150",
    "end": "861198"
  },
  {
    "text": "You've got your pre-trained\nmodel, which costs, you know, 5.6 million plus, you know, all\nthe arguable buffer on top.",
    "start": "861198",
    "end": "867990"
  },
  {
    "text": "And they just use reinforcement learning\nusing some rules based systems more or less to be able to verify the results,\num, and, and score the responses.",
    "start": "868620",
    "end": "877140"
  },
  {
    "text": "And then, and so they called\nthat R1-Zero, I believe. Then there's R1, which they also\ncreated because in their paper they",
    "start": "877599",
    "end": "884550"
  },
  {
    "text": "mentioned that there were some, you\nknow, rough edges, so to speak, on the, the reinforcement learning-only model.",
    "start": "884550",
    "end": "890504"
  },
  {
    "text": "And in that model, they start the\nmodel first with some fine tuning, basically using some structured\ndata in order to better prime it for",
    "start": "890864",
    "end": "900003"
  },
  {
    "text": "this reinforcement learning task. And that is the model that everyone's\nnow playing with on the DeepSeek app and",
    "start": "900004",
    "end": "905384"
  },
  {
    "text": "that everyone's really excited about. So I think it's a really interesting\nlook at, you know, the takeaway",
    "start": "905435",
    "end": "911724"
  },
  {
    "text": "shouldn't be that, oh, we, you know,\ncan't do RL only we had to, you know,",
    "start": "911724",
    "end": "916785"
  },
  {
    "text": "resort to this cold start and fine\ntuning before the model was released. The takeaway that I think people\nshould have is it's amazing how",
    "start": "916785",
    "end": "922985"
  },
  {
    "text": "far they were able to push just RL. And yeah, there's still always\ngoing to be a need for some structured data potentially.",
    "start": "922985",
    "end": "929434"
  },
  {
    "text": "And there's, you know, maybe a hybrid\napproach is best, but it is kind of crazy how far they were able to push it.",
    "start": "929435",
    "end": "935365"
  },
  {
    "text": "Now, what they also published in their\npaper, getting to the distilled models, and you asked about distillation,\ndistillation has been around forever.",
    "start": "935910",
    "end": "943889"
  },
  {
    "text": "It's where, you know, back to, you know,\nearly days of the first Llama model, you know, a group of students distilled\nthat into Vicuna, um, and it's basically",
    "start": "943950",
    "end": "952329"
  },
  {
    "text": "where you generate a bunch of synthetic\nstructure data from a big model and use that to fine tune a small model.",
    "start": "952329",
    "end": "957379"
  },
  {
    "text": "So DeepSeek used that same kind of thought\nprocess doing just RL only on a small",
    "start": "958210",
    "end": "963790"
  },
  {
    "text": "model, so no big models involved, just RL\nand try to see how far could they get, you know, they published numbers on Qwen32B.",
    "start": "963790",
    "end": "971569"
  },
  {
    "text": "So how far can we push\nQwen32B's reasoning just on RL? And they weren't able to, in the paper,\nthey claimed they weren't able to",
    "start": "971589",
    "end": "978929"
  },
  {
    "text": "push it nearly as far, get any real\nreasoning capabilities out of the model, they had to resort to distillation,\ntake their big R1 model, generate a",
    "start": "978929",
    "end": "987115"
  },
  {
    "text": "bunch of synthetic data, and tune it. So, you know, I'm curious from your\nperspective, like, what your take",
    "start": "987115",
    "end": "992805"
  },
  {
    "text": "is on that, Chris, based off of\nsome of the RL experiments you've been doing with small models. You said you also, I think, did\nsome fine tuning first to start it",
    "start": "992805",
    "end": "1000104"
  },
  {
    "text": "off and then with chain of thought\nreasoning and then RL on top. Now, for me, the critical thing is\nthe long chain of thought reasoning.",
    "start": "1000104",
    "end": "1007065"
  },
  {
    "text": "That is actually an accurate\nlong chain of thought reasoning. That is the thing that\nreally enabled everything.",
    "start": "1007095",
    "end": "1013464"
  },
  {
    "text": "So, again, if you look at the paper, when\nthey did RL, um, they said they got there.",
    "start": "1013475",
    "end": "1018694"
  },
  {
    "text": "But, you know, if you think about,\nespecially math problems, LLMs are not really good at that.",
    "start": "1018734",
    "end": "1024365"
  },
  {
    "text": "So you're going to say, what's 25 plus 8? What's this? Whatever. And you're going to ask an LLM, you\nknow, to go and generate me this sum.",
    "start": "1024375",
    "end": "1031605"
  },
  {
    "text": "And it may or may not get it right. It may or may not get the sums and the\nlength of chain of thoughts that you want.",
    "start": "1031995",
    "end": "1037925"
  },
  {
    "text": "It may not get its explanations right. So it's, it's really a, a bit\nof a crap shoot and getting",
    "start": "1037965",
    "end": "1043255"
  },
  {
    "text": "an accurate chain of thought. And then at the end of it, they're\nusing this thing called a verifier. And what the verifier does is, is, you\nknow, take the answer that you've got",
    "start": "1043295",
    "end": "1052654"
  },
  {
    "text": "and go you know, run a bit of rules\nto run the equation and say, yeah, that was correct or that was wrong.",
    "start": "1052705",
    "end": "1058850"
  },
  {
    "text": "And then you get a, you get\na bit of a reward, you know, it's like, here's a cookie. Well done model. Good job.",
    "start": "1058860",
    "end": "1064110"
  },
  {
    "text": "But, but if you think about how\nlong that's going to take it, it, you really are monkeys and\ntypewriters at that point, right?",
    "start": "1064540",
    "end": "1071658"
  },
  {
    "text": "It's going to take time for the models\nto, to come back with the right answers. Now, if, if you run a fine\ntuning step before that.",
    "start": "1071660",
    "end": "1078940"
  },
  {
    "text": "So if you can produce long, accurate chain\nof thoughts for those math equations,",
    "start": "1079230",
    "end": "1085260"
  },
  {
    "text": "for example, and I'm picking math because\nthat was in the paper, then the model is gonna look at that and say, okay,\nI'm doing this equation here, and you",
    "start": "1085260",
    "end": "1093270"
  },
  {
    "text": "explain every single step, step one,\nstep two, step three, and then I'm reflecting back, this was right, this\nwas wrong, and then finally I'm gonna",
    "start": "1093270",
    "end": "1100169"
  },
  {
    "text": "check the answers back, then you're just\ngoing to need less steps for the model to be able to learn what it has to do.",
    "start": "1100169",
    "end": "1106870"
  },
  {
    "text": "And then you can use RL afterwards\nto go, oh, this particular song- sum you went wrong.",
    "start": "1106870",
    "end": "1113128"
  },
  {
    "text": "So I'm going to give you a cookie here. Um, because you've now, you know, we've\nnow given you a different way, you",
    "start": "1113340",
    "end": "1119369"
  },
  {
    "text": "know, here's the right way of doing it. And you don't get a cookie\nif you got it wrong. So I think, I think that combination\nbetween the two is the key thing.",
    "start": "1119369",
    "end": "1126870"
  },
  {
    "text": "But I actually think the real,\nuh, take away from that paper is the long chain of thought.",
    "start": "1126870",
    "end": "1132305"
  },
  {
    "text": "So when I did my experiment, uh, on\nmy YouTube channel, the thing that I did is I took a slightly different\napproach from what DeepSeek did.",
    "start": "1132305",
    "end": "1140554"
  },
  {
    "text": "And, and I have a thing\ncalled a math compiler. So what I did is I, I automatically\ngenerate the, uh, the math equations,",
    "start": "1140925",
    "end": "1147873"
  },
  {
    "text": "and then I put it into my compiler, and\nI generate an abstract syntax tree, and then I walk the tree, and then it, I\ndon't need the LLM to do the math, I'm",
    "start": "1148134",
    "end": "1157074"
  },
  {
    "text": "just going step zero, step one, step two,\nand I'm just walking the tree, and I'm outputting the explanations, and then what\nI do is I use the LLM to transform that",
    "start": "1157074",
    "end": "1165634"
  },
  {
    "text": "into something that the model actually\nunders- you know, is actually human language, and the explanations behind\nit, and then that's how I got these",
    "start": "1165634",
    "end": "1172654"
  },
  {
    "text": "really accurate, uh, chains of thought. And then when I put that in just as an,\nuh, you know, fine tuning step, I think",
    "start": "1172654",
    "end": "1179398"
  },
  {
    "text": "I used maybe a hundred different examples\nand and honestly the math and I did it",
    "start": "1179399",
    "end": "1184900"
  },
  {
    "text": "on a one and a half billion parameter\nmodel the math was incredible, right? It was like a couple of decimal precisions\nof uh, you know accuracy out which which",
    "start": "1184910",
    "end": "1196290"
  },
  {
    "text": "the larger models of six months ago would\nbe nowhere near so so I think the this",
    "start": "1197370",
    "end": "1202450"
  },
  {
    "text": "The real innovation is the long chain of\nthought and the accurate chain of thought. It's not to say RL won't get you\nthere, it will get you there, but",
    "start": "1202970",
    "end": "1209889"
  },
  {
    "text": "it's just going to take a long time. So if you can, if you can short set\nthat a little bit, and then have RL",
    "start": "1209950",
    "end": "1215270"
  },
  {
    "text": "sort of, you know, do the kind of,\nuh, the smooth and out of the edges, then you're really going to win.",
    "start": "1215279",
    "end": "1220479"
  },
  {
    "text": "That's kind of my view on this. RL is really valuable for tasks\nlike math and things where it's",
    "start": "1220790",
    "end": "1225860"
  },
  {
    "text": "easy to check the accuracy, right? Um, as well as, relatively easy to\ngenerate that chain of thought, but when",
    "start": "1225889",
    "end": "1234405"
  },
  {
    "text": "we look like in the paper, for example,\nthey talked about still needing some instruction tuning for tasks like tool\ncalling, you know, instruction following,",
    "start": "1234405",
    "end": "1243214"
  },
  {
    "text": "like there's still going to be a need\nfor having like these reasoning models aren't designed to do every single task.",
    "start": "1243214",
    "end": "1248884"
  },
  {
    "text": "They're specific for reasoning, and\nyou're still going to potentially need instruction tuning in order to\nhandle some of those more specific",
    "start": "1248904",
    "end": "1255595"
  },
  {
    "text": "instruction following tasks.",
    "start": "1255595",
    "end": "1256934"
  },
  {
    "start": "1260000",
    "end": "1881000"
  },
  {
    "text": "So I'm going to move us\non to our next segment. Uh, so this is super helpful, I think, in\nterms of setting the scene, knocking down some of the myths that have popped up.",
    "start": "1262045",
    "end": "1267515"
  },
  {
    "text": "We've already talked a bunch about\ndistillation, um, and I think on the last episode, Skyler actually\ngave like a short, brief explanation",
    "start": "1267925",
    "end": "1274885"
  },
  {
    "text": "of it, but for those who weren't\nlistening on the previous episode, maybe Aaron, I'll toss it to you. I think it's worth it for our listeners\nto, just get a sense of like, what",
    "start": "1274885",
    "end": "1283095"
  },
  {
    "text": "is distillation in the first place? Um, and then I think if you want to give\nthat explanation, there's some interesting things I think that are worth getting\ninto about like, well, what does this",
    "start": "1283095",
    "end": "1290225"
  },
  {
    "text": "mean for where the industry is going? But maybe I'll toss it to you to give\nthe quick capsule explainer first.",
    "start": "1290245",
    "end": "1294785"
  },
  {
    "text": "Yeah. So, I mean, I mean, model distillation\nis very powerful technique. You know, it's about having a teacher\nmodel, you know, that could be a bigger",
    "start": "1295404",
    "end": "1302114"
  },
  {
    "text": "model where it's encoded, you know,\nmuch more information, uh, through weights and through embeddings.",
    "start": "1302114",
    "end": "1307595"
  },
  {
    "text": "And what you want to do is transfer\nthat knowledge to a student model. And then usually that student model could\nbe smaller and it requires, then in turn,",
    "start": "1307605",
    "end": "1314630"
  },
  {
    "text": "less resources to train, right, and to\nalso use for inference, right, and some people and groups think of this as model\ncompression, right, where you're making",
    "start": "1314630",
    "end": "1322819"
  },
  {
    "text": "a model smaller, and so on, um, and then,\nand then there's, there's different things that you can distill, right, you can\ndistill like response based knowledge,",
    "start": "1322820",
    "end": "1331028"
  },
  {
    "text": "You can dispel feature-based knowledge\nor even like the relations between all the different connections within all\nof the neurons that you have, right?",
    "start": "1331344",
    "end": "1340674"
  },
  {
    "text": "And one interesting thing that I\nwanted to bring up that I saw within the R1 paper is that the distillation\nprocess, it wasn't just about, to me,",
    "start": "1341124",
    "end": "1350574"
  },
  {
    "text": "about just doing this, you know, model\ncompression or getting knowledge out.",
    "start": "1350584",
    "end": "1355695"
  },
  {
    "text": "But it was almost like\nthis model translation. Uh, because what I saw is that\nyou were actually distilling",
    "start": "1355725",
    "end": "1361655"
  },
  {
    "text": "information, uh, from an MOE, right? And then you were going directly\nto the student model, which was a",
    "start": "1361665",
    "end": "1368165"
  },
  {
    "text": "densely connected, you know, feed\nforward, uh, neural network and many, uh, different cases, right?",
    "start": "1368184",
    "end": "1373605"
  },
  {
    "text": "And so, and so just changing that\nmodel architecture, um, looked to be, um, a different way of doing this type\nof model distillation that, that I",
    "start": "1373605",
    "end": "1382485"
  },
  {
    "text": "thought also looked You know, gave R1,\nI think, some advantages, especially whenever you were looking at using\nlike Qwen2.5 and like the Llama 3",
    "start": "1382495",
    "end": "1391745"
  },
  {
    "text": "series, right, as the base foundational\nmodel to pull information out.",
    "start": "1391745",
    "end": "1397105"
  },
  {
    "text": "Yeah, and I think one of the most\ninteresting elements of distillation is sort of the idea that um, you know, you,\nyou can, you can take any large model",
    "start": "1397165",
    "end": "1404605"
  },
  {
    "text": "and bring that sort of knowledge into\nwhatever it is that you're building. Um, you know, I think really just\nliterally, I think in the last 24, 48",
    "start": "1404985",
    "end": "1411615"
  },
  {
    "text": "hours, there was a little bit of kind\nof a controversy over did DeepSeek use effectively kind of OpenAI's chains\nof thoughts or other inputs, outputs",
    "start": "1411615",
    "end": "1418835"
  },
  {
    "text": "to kind of do the distillation here. Um, I guess, Kate, kind of question\nis like, this makes it very hard for",
    "start": "1418835",
    "end": "1425105"
  },
  {
    "text": "any model company to kind of protect\nits models in some ways, right? Because everything is distillable.",
    "start": "1425105",
    "end": "1430115"
  },
  {
    "text": "Is that the right way\nof thinking about it? Yeah, I mean, I think by releasing a\nvery capable, the most capable model",
    "start": "1430115",
    "end": "1437734"
  },
  {
    "text": "to date in the open source with a\npermissible MIT license, DeepSeek is",
    "start": "1437735",
    "end": "1443139"
  },
  {
    "text": "essentially allowing and eroding kind of\nthat competitive moat that all the big",
    "start": "1443139",
    "end": "1448479"
  },
  {
    "text": "model providers have had to date, keeping\ntheir biggest models behind closed doors.",
    "start": "1448489",
    "end": "1453509"
  },
  {
    "text": "And regardless of whether or not DeepSeek\nalso benefited from distillation from those bigger models, we're now able to\ngo and take that really big model in the",
    "start": "1453949",
    "end": "1462100"
  },
  {
    "text": "open and use it indiscriminately, where\nbefore people, I mean, this distillation",
    "start": "1462100",
    "end": "1467429"
  },
  {
    "text": "from GPT has been going on for, ages. Anyone can go to Hugging Face and find\ntons of data sets that were generated",
    "start": "1467450",
    "end": "1472895"
  },
  {
    "text": "from GPT models, uh, that are, you know,\nformatted and designed for training and",
    "start": "1472895",
    "end": "1478295"
  },
  {
    "text": "likely taken without the rights to do so. Um, so this is like a secret that's not\nsecret that's been going on forever.",
    "start": "1478315",
    "end": "1486164"
  },
  {
    "text": "So yes, it most likely worked its\nway in some degree or fashion to the DeepSeek model, but it almost doesn't\nmatter anymore because DeepSeek now is",
    "start": "1486445",
    "end": "1493215"
  },
  {
    "text": "out there and that model can be used\nto run very similar style distillation. With great effect on as many small\nmodels as you like, and anyone now has",
    "start": "1493215",
    "end": "1501885"
  },
  {
    "text": "the rights if they, uh, use DeepSeek's\nmodel to do so, according to the license that it's published under.",
    "start": "1501885",
    "end": "1507125"
  },
  {
    "text": "That's right. Yeah, I think one of the funniest parts\nabout the kind of news cycle has been like, they used a secret, you know,\nsinister technique called distillation.",
    "start": "1507175",
    "end": "1514394"
  },
  {
    "text": "Yeah, it's like, actually\neverybody's been, everybody's been distilling all the time. It's just like happening It's been around forever. And it costs 5.5 million dollars.",
    "start": "1514395",
    "end": "1521195"
  },
  {
    "text": "That's right. Yeah, exactly. What strikes me, I mean, Chris, even\nto the example that you gave earlier,",
    "start": "1521215",
    "end": "1526824"
  },
  {
    "text": "right, like you don't, it turns out\nyou don't need a whole lot of data to make these models much, much better.",
    "start": "1526834",
    "end": "1532225"
  },
  {
    "text": "And it kind of seems like there's this\nsort of like fundamental thing in the market where it's like, unless you\nwant to control and really down to the",
    "start": "1532554",
    "end": "1538904"
  },
  {
    "text": "nth level, prevent people from getting\noutputs from a model, there's basically no way to stop distillation, right?",
    "start": "1538914",
    "end": "1545445"
  },
  {
    "text": "I don't know if you think there's a\nrealistic way to prevent that at all. No, I don't think so. I mean, the reality is, as Kate said,\nthere's open weight models out there,",
    "start": "1545524",
    "end": "1553934"
  },
  {
    "text": "um, and people are gonna do that. And I, I think that, and, and I love\nthis by the way, and the reason I love",
    "start": "1554264",
    "end": "1561804"
  },
  {
    "text": "it is that I'm, I'm all for chaos. I'm all for open source. I'm all for sharing and collaborations.",
    "start": "1561805",
    "end": "1568825"
  },
  {
    "text": "So, you know what, people are\ngoing to go off now, they're going to create their own data sets. They're going to distill\nfrom different models.",
    "start": "1568825",
    "end": "1574760"
  },
  {
    "text": "They're going to share\nthat out in the community. And you know what, we're going\nto all end up with better stuff. Right? So I'm, I'm not a big fan of the\nclosed models, personally, my opinion.",
    "start": "1574760",
    "end": "1584039"
  },
  {
    "text": "Um, I'm a big fan of sharing\nand learning from each other. So I, that's what gets me excited\nabout kind of the DeepSeek stuff.",
    "start": "1584460",
    "end": "1591520"
  },
  {
    "text": "And, and it, and again, it's not just\nthe fact that, um, they put the model out there that you can distill from is they\ntalked about the techniques that they use.",
    "start": "1591520",
    "end": "1599490"
  },
  {
    "text": "So, so yeah, it's, it's cool. We can all start doing interesting things. And, and you know what?",
    "start": "1599499",
    "end": "1605110"
  },
  {
    "text": "I don't think everybody's going to,\nI don't think we're suddenly all going to be going out competing with\nOpenAI, Anthropic, blah, blah, blah.",
    "start": "1605305",
    "end": "1610875"
  },
  {
    "text": "I don't think the, you know, all\nof these people sitting in their bedroom are going to do that. But you know what they might be able to\ndo is take one of these out of the box",
    "start": "1611085",
    "end": "1618724"
  },
  {
    "text": "pre-trained model and then solve one\nof their own particular tasks that the general model can't do that's specific\nto their use case and make it easier.",
    "start": "1618724",
    "end": "1626865"
  },
  {
    "text": "Um, but again, don't,\ndon't undersell this. I mean, Kate, you know this\nbetter than anyone, right?",
    "start": "1627115",
    "end": "1632175"
  },
  {
    "text": "Fine tuning models is really hard, right? Because of all of the biases, you might,\nyou might think, hey, my model is now",
    "start": "1632465",
    "end": "1639455"
  },
  {
    "text": "great at doing this one particular task. But then you've just ruined that\nmodel from doing any other tasks there",
    "start": "1639455",
    "end": "1645475"
  },
  {
    "text": "because you didn't have the right\nbiases and mixes within that data set. Yeah. I mean, just take a look at the\nHugging Face Open LLM Leaderboard.",
    "start": "1645475",
    "end": "1653210"
  },
  {
    "text": "All those distilled versions of Llama\nand Qwen are on there, and they all rank significantly lower than the original\nmodel that they were distilled from.",
    "start": "1653235",
    "end": "1660585"
  },
  {
    "text": "Uh, on those Open LLM Leaderboard\ntasks, which are not predominantly reasoning-based tasks.",
    "start": "1661104",
    "end": "1666485"
  },
  {
    "text": "So the model was boosted in\nreasoning, but other general performance characteristics drop.",
    "start": "1666504",
    "end": "1671513"
  },
  {
    "text": "But I, I think it's still\nincredibly powerful. And as we talk about, you know,\nDeepSeek's introducing this new",
    "start": "1672044",
    "end": "1679174"
  },
  {
    "text": "era of efficient open source AI. It's true. It's just not true because they\ntrained this really cost-effective",
    "start": "1679225",
    "end": "1686715"
  },
  {
    "text": "model during the pre-training. It's true because we now have the methods\nto create these armies of distilled fit",
    "start": "1686735",
    "end": "1693283"
  },
  {
    "text": "for purpose models that are specific for\nthe tasks that you care about because we have better tooling, like powerful teacher\nmodels, out in the open source ecosystem.",
    "start": "1693284",
    "end": "1701234"
  },
  {
    "text": "Yeah, yeah, yeah. I think that there's a lot of\nsecret agents, you know, that are hidden amongst our labs.",
    "start": "1701375",
    "end": "1706509"
  },
  {
    "text": "And, you know, in the next couple\ndays, weeks, you'll see them to become super agents that are going\nto be a release that we can all use.",
    "start": "1706509",
    "end": "1712459"
  },
  {
    "text": "So, um, I, I really think this might\nhave been one of the impetuses, you know, to sort of grandstand out, you know,\nwhat's happening within the field of AI.",
    "start": "1712459",
    "end": "1720769"
  },
  {
    "text": "Um, DeepSeek just happened to be right\ntime, right place, you know, to do it, to put all, to connect all the dots together.",
    "start": "1721239",
    "end": "1726830"
  },
  {
    "text": "Um, but, yeah. Um, I do think that lots of these\ntechnologies and new innovations that",
    "start": "1727389",
    "end": "1733450"
  },
  {
    "text": "are coming out, inventions, um, you\nknow, you ask the question, can you prevent someone from distilling a model?",
    "start": "1733450",
    "end": "1739090"
  },
  {
    "text": "Um, that sort of brings\nme back to biometrics. You know, it used to be,\ncan you prevent someone from stealing a picture of your face?",
    "start": "1739120",
    "end": "1744720"
  },
  {
    "text": "You know, and we, and we came up\nwith this cancelable biometric invention so that if someone took\nyour picture, you could revoke your",
    "start": "1745104",
    "end": "1751783"
  },
  {
    "text": "biometric and create a new one, right? So I mean, I mean, there, I think\nthere might be some cancelable",
    "start": "1751784",
    "end": "1757004"
  },
  {
    "text": "technologies, um, and patents, right? That we could work on together, right? To achieve some of this.",
    "start": "1757024",
    "end": "1761764"
  },
  {
    "text": "Final question here, I think for Kate,\num, particularly given your work on Granite, you know, I think there's\nmaybe one point of view, which is, well,",
    "start": "1762145",
    "end": "1768175"
  },
  {
    "text": "you know, the only reason you know,\ninvestors have put money in towards building these giant, giant models\nis kind of the idea that if you build",
    "start": "1768895",
    "end": "1777065"
  },
  {
    "text": "these giant models, you'll be able to\ncapture all the value from that model. And it sort of seems to me that like,\nif distillation gets good and you know,",
    "start": "1777065",
    "end": "1783714"
  },
  {
    "text": "granted, distillation is hard in some\nrespects, but you think you've got enough eyeballs, someone will eventually\nfigure out ways of cracking it.",
    "start": "1783715",
    "end": "1789261"
  },
  {
    "text": "You know, is there an argument here\nthat it kind of erodes the incentive for people to invest in building\nthe big model in the first place?",
    "start": "1789960",
    "end": "1796320"
  },
  {
    "text": "Uh, like, and there's kind of a really\ninteresting question, which is, you know, it's almost kind of an accident\nthat like we've ended up with these giant models and it's partially based\non the idea that like, well, you could",
    "start": "1796360",
    "end": "1804950"
  },
  {
    "text": "have some exclusive control over this,\nbut it feels like this is rapidly kind of escaping the ability for anyone to\nbe able to kind of exclusively control.",
    "start": "1804959",
    "end": "1811590"
  },
  {
    "text": "Yeah, look, I don't think there's\nany incentive to really build big models to run at inference time.",
    "start": "1811740",
    "end": "1817899"
  },
  {
    "text": "The incentive is to big, really big models\nto help you build really small models. And all it takes, like we've, it\nstarted with Llama releasing, you",
    "start": "1817960",
    "end": "1826559"
  },
  {
    "text": "know, 400 billion plus parameter\nmodel, NVIDIA released a 400 billion plus parameter model as a teacher.",
    "start": "1826560",
    "end": "1831470"
  },
  {
    "text": "And now DeepSeek releasing their\n600 billion plus parameters, you know, size isn't everything. They also have to have high\nquality post-training, which is",
    "start": "1831860",
    "end": "1838860"
  },
  {
    "text": "why the reinforcement learning\npart of DeepSeek is so important. But we're seeing more and more\nlarge models that can be used openly",
    "start": "1838860",
    "end": "1845729"
  },
  {
    "text": "to train these smaller models. And I think it's just going to continue\nto make this more of a teacher model",
    "start": "1845769",
    "end": "1850969"
  },
  {
    "text": "based commodity like why pay for those big\nmodels if we've got similar capabilities out in the open that you can customize\nfurther and I think we are going to",
    "start": "1850990",
    "end": "1859025"
  },
  {
    "text": "converge on to a point where we've got\npowerful enough tools to craft the smaller",
    "start": "1859045",
    "end": "1866244"
  },
  {
    "text": "models that we need that are going to run,\nyou know 80 percent to 90 percent of our workflows for generative AI in the future.",
    "start": "1866245",
    "end": "1872115"
  },
  {
    "text": "Yeah, it's kind of a funny world\nwhere it's kind of like you ever, you never talk to the giant model that's\njust inside company headquarters and",
    "start": "1872135",
    "end": "1877524"
  },
  {
    "text": "then just like lots of tiny models\nthat are coming out around it.",
    "start": "1877525",
    "end": "1879775"
  },
  {
    "start": "1881000",
    "end": "2357000"
  },
  {
    "text": "Well, in the last few minutes,\nI want to zoom out a little bit. We've been talking a lot about DeepSeek\nand what's going on underneath the hood.",
    "start": "1884715",
    "end": "1890405"
  },
  {
    "text": "Um, and I want to just take a moment\nto talk a little bit about what all the other companies are doing. Relative to this development in the AI\nspace, um, Sam Altman, of course, the",
    "start": "1890475",
    "end": "1900530"
  },
  {
    "text": "head of OpenAI put out a little tweet\nthread kind of responding to this news. Um, and I'll just quote\na little bit of it.",
    "start": "1900570",
    "end": "1906729"
  },
  {
    "text": "He said: \"we are excited to continue\nto execute on our research roadmap and believe more compute is more important\nnow than ever to succeed at our mission.\"",
    "start": "1906729",
    "end": "1913959"
  },
  {
    "text": "Um, which is really like a statement by a\nguy that says, steady as she goes, we're continuing on the research path as you\nknow, we had planned, um, and nothing",
    "start": "1914370",
    "end": "1923299"
  },
  {
    "text": "has changed by the DeepSeek, uh, release. Um, I guess, Chris, maybe\nI'll kick it to you.",
    "start": "1923300",
    "end": "1928760"
  },
  {
    "text": "Do you buy that? Like, is OpenAI pretty much gonna\njust keep doing its strategy? Or does this really kind of fundamentally\nchange what they're gonna need to do?",
    "start": "1929130",
    "end": "1935349"
  },
  {
    "text": "Nah, he's gonna release his model sooner. He's been holding on to these models for\ntoo long, and he needs to get on with it.",
    "start": "1935670",
    "end": "1940518"
  },
  {
    "text": "And good on you, DeepSeek, right? Where's my o3? You showed me it at the end of Christmas. Do I have it in my hands?",
    "start": "1940720",
    "end": "1946110"
  },
  {
    "text": "No, so thank you DeepSeek, maybe we'll\nget his model out a bit quicker and then we'll get o4 and o5 and then\nmaybe we'll get some of these models",
    "start": "1946245",
    "end": "1953894"
  },
  {
    "text": "in Europe because guess what, they're\nreleasing vision models and video models and I don't have any of them so\nI'm gonna get them as well, so woohoo!",
    "start": "1953945",
    "end": "1962134"
  },
  {
    "text": "Uh, so I guess ultimately\nwhat you're saying is it just accelerates his roadmap, right? To just get him off the fence.",
    "start": "1962705",
    "end": "1968184"
  },
  {
    "text": "There is no way he's just gonna\nsit there and go, uh uh uh, I'm not giving you my model while DeepSeek\nis getting all of this press.",
    "start": "1969005",
    "end": "1976755"
  },
  {
    "text": "He's gonna respond and\nwe're gonna get new models. But I think, I mean, Aaron, maybe to\nturn it to you, you don't think this changes, like, their approach to,\nI guess, ironically, being kind of",
    "start": "1976934",
    "end": "1984765"
  },
  {
    "text": "a closed source model here, right? Like, this is not the kind of situation\nwhere you believe that OpenAI or Anthropic, any of the big kind of\nproviders, would say, hey, now we",
    "start": "1984765",
    "end": "1992955"
  },
  {
    "text": "need to start switching to open\nsource is the way we play this game. Um, I don't think so. I mean, I mean, I think so.",
    "start": "1992955",
    "end": "1999365"
  },
  {
    "text": "I mean, this, this could go in\nseveral directions, you know, but I think, you know, open versus closed\nsource, you know, I think that there's",
    "start": "1999365",
    "end": "2005214"
  },
  {
    "text": "advantages and disadvantages to both,\nbut I think ultimately it helps the academic community, which then in\nturn fuels, you know, economies of",
    "start": "2005214",
    "end": "2012954"
  },
  {
    "text": "scale for the average consumer, right? Because, uh, because if you think\nabout it, you have two groups, right?",
    "start": "2012955",
    "end": "2017985"
  },
  {
    "text": "You have the open source group, closed\ngroup, they compete, you know, um, to make sure that one is better than the\nother, which then spurns innovation.",
    "start": "2017985",
    "end": "2025344"
  },
  {
    "text": "Okay.\nGreat. And then within each one of those\ngroups, you have companies and organizations that then in turn competes.",
    "start": "2025345",
    "end": "2031110"
  },
  {
    "text": "You have like this 20 and of\ncompetition that further accelerates, you know, this, uh, innovation.",
    "start": "2031130",
    "end": "2037779"
  },
  {
    "text": "And so I, and so, so Sam Altman, you know,\nyou know, I think he's going to release a secret agents, right, uh, sooner, right.",
    "start": "2037779",
    "end": "2044570"
  },
  {
    "text": "Make them available, right. And, and lots of the techniques that,\nyou know, DeepSeek has shown, you know,",
    "start": "2044589",
    "end": "2049600"
  },
  {
    "text": "like that caching layer of the key value\nand queries that they've, you know, come up with some of their MoE innovations\nand then some of the parallelization",
    "start": "2049950",
    "end": "2058005"
  },
  {
    "text": "whenever they can share context and\ninformation amongst their grid, lots of",
    "start": "2058495",
    "end": "2064695"
  },
  {
    "text": "that is going to be included, I think,\nin Sam Altman's but pushed even further, you know, with their own innovations and\nit's going to splinter out a bit, but",
    "start": "2064695",
    "end": "2072275"
  },
  {
    "text": "the fundamental like model distillation\nand so on and so forth, you know, I think that's going to be, uh, very key.",
    "start": "2072285",
    "end": "2078304"
  },
  {
    "text": "And then it brings a value\nproposition down to frameworks, you know, um, how can I better train\nthe models for my own fit purpose?",
    "start": "2078304",
    "end": "2085384"
  },
  {
    "text": "You know, whether I'm an\nenterprise or a customer and then also how can I trust it, right? Because there's going to be a zoo\nof models now that are out there.",
    "start": "2085685",
    "end": "2092454"
  },
  {
    "text": "It's just very confusing\nto pick which one do I use? Yeah, Kate, uh, so we've\nbeen talking about OpenAI. Obviously they take up a\nbunch of sort of airtime.",
    "start": "2092455",
    "end": "2099569"
  },
  {
    "text": "Um, but I guess one thing to kind of, as\nwe think about zooming out to tell this DeepSeek story is whether or not we think\nOpenAI is kind of similarly situated.",
    "start": "2099660",
    "end": "2107960"
  },
  {
    "text": "Like, you know, everything we've\nbeen hearing is, okay, OpenAI is going to continue its strategy,\nit's just going to move faster.",
    "start": "2108320",
    "end": "2113869"
  },
  {
    "text": "Do you think it changes the economics\nat all or the kind of decision making at all for say a Google or a Meta or,\num, you know, even like an Anthropic?",
    "start": "2114315",
    "end": "2122385"
  },
  {
    "text": "I don't think it changes the decision\nmaking or strategy, uh, overall.",
    "start": "2122425",
    "end": "2127945"
  },
  {
    "text": "I think a lot of DeepSeek\nstrategy, you know, necessity is the mother of invention. They only had access to H800 chips.",
    "start": "2128385",
    "end": "2134445"
  },
  {
    "text": "So they optimized the hell of it. They invested in efficient architecture\nlike MoEs and DeepSeek was born, right?",
    "start": "2134455",
    "end": "2141045"
  },
  {
    "text": "So I think the U.S. based labs are\noperating with very different constraints",
    "start": "2142095",
    "end": "2147655"
  },
  {
    "text": "and DeepSeek's innovation doesn't\nnecessarily, constraints and incentives. And I don't think DeepSeek\nnecessarily changed that calculus.",
    "start": "2147705",
    "end": "2156364"
  },
  {
    "text": "I also think a lot, again,\nwhat we've talked about today with DeepSeek is distillation. And for the labs pursuing\nAGI, distillation is not",
    "start": "2156715",
    "end": "2165210"
  },
  {
    "text": "necessarily as relevant, right? They need to keep training as big a\nmodel as possible and have incentives to",
    "start": "2165210",
    "end": "2171410"
  },
  {
    "text": "try and keep that behind closed doors. Whereas the business value, again,\nmy take is the business value is all",
    "start": "2171410",
    "end": "2176829"
  },
  {
    "text": "around these distilled smaller models\nthat are actually what people are going to deploy in a commercial setting.",
    "start": "2176829",
    "end": "2181450"
  },
  {
    "text": "And I don't think they're at least\nat the highest strategy level and what they're working on with\nright in terms of their investment",
    "start": "2181920",
    "end": "2187589"
  },
  {
    "text": "profiles, that longer term AGI game. And for that you still need a\ncrap ton of big model of big GPUs.",
    "start": "2187590",
    "end": "2193600"
  },
  {
    "text": "And they're not going to want to\nrelease any of that out in the open. Right? Yeah. It's not like they're going\nto use Stargate to do like small distilled models.",
    "start": "2194049",
    "end": "2199340"
  },
  {
    "text": "That would be the funniest thing. It's actually an inference cluster. Surprise.",
    "start": "2199340",
    "end": "2203910"
  },
  {
    "text": "Um, that's yeah, that's I\nthink really fascinating. I guess, Chris, maybe I'll turn\nto you for the sort of last",
    "start": "2205629",
    "end": "2210760"
  },
  {
    "text": "word and last question here. You know, Kate just talked a\nlittle bit about the idea that Chinese researchers are operating\nunder very different constraints.",
    "start": "2210760",
    "end": "2218460"
  },
  {
    "text": "So they kind of develop different types\nof methodology, different types of models, different types of proficiencies.",
    "start": "2218469",
    "end": "2223499"
  },
  {
    "text": "Um, and do you think there's something\nto the idea that like, almost we're like, we have an embarrassment of compute",
    "start": "2223960",
    "end": "2229690"
  },
  {
    "text": "among the U.S. labs. And So it actually kind of like, limits,\nlike the degree to which we would ever invest in the kind of thing\nthat DeepSeek would be working on.",
    "start": "2229850",
    "end": "2237570"
  },
  {
    "text": "Um, I'm really sort of interested in the\nidea that like, these constraints really mean that AI will start to look pretty\ndifferent in different parts of the world.",
    "start": "2237920",
    "end": "2243989"
  },
  {
    "text": "As researchers operate under very\ndifferent constraints of what they need to do to deploy systems. I, I think that's exactly the case, right?",
    "start": "2244260",
    "end": "2249930"
  },
  {
    "text": "And you can see a little bit of\nreinforcement learning happening there and reward modeling, right? Which you, you were saying here.",
    "start": "2249930",
    "end": "2255800"
  },
  {
    "text": "You're going to have less\ncompute available to you. And guess what? They have different incentives\nat that point, and they've been",
    "start": "2256160",
    "end": "2262310"
  },
  {
    "text": "rewarded by being more efficient. So if you've got an abundance of\ncompute, you're not really going to be optimizing for efficiency.",
    "start": "2262310",
    "end": "2268549"
  },
  {
    "text": "You're going to be trying to be\ngetting your models out first. And I think that's also, you\nknow, speaking from my own",
    "start": "2268549",
    "end": "2273820"
  },
  {
    "text": "experience, I, you know, I don't have any H100's kicking around. What have I got? I've got my MacBook Pro, right? Where I've got \n-- You're more like the deep sea researcher, basically.",
    "start": "2273820",
    "end": "2282355"
  },
  {
    "text": "Exactly. So you're trying to come up with\ninnovative techniques to work within the hardware constraints\nthat you run within today.",
    "start": "2282955",
    "end": "2290395"
  },
  {
    "text": "So I, I, and I think honestly, if they\ndidn't have the chip constraints in China,",
    "start": "2290395",
    "end": "2298105"
  },
  {
    "text": "I'm not sure that DeepSeek would have\nprobably came up with those techniques, because they, they would have been\njust trying to focus and catch up with",
    "start": "2298680",
    "end": "2305609"
  },
  {
    "text": "everybody else, as opposed to trying\nto take things from a different angle. And, and therefore, again, one\nof the reasons I believe in open",
    "start": "2305610",
    "end": "2311859"
  },
  {
    "text": "source very much, and everybody's\nsharing their papers, everybody's running under different constraints. And they're going to find new\ninnovations, and if we share that,",
    "start": "2311860",
    "end": "2318359"
  },
  {
    "text": "we're all going to learn from each\nother and be able to contribute. And that's not just the big labs,\nbut the, the people in the community,",
    "start": "2318360",
    "end": "2324849"
  },
  {
    "text": "just with their laptops, trying to\ndiscover and experiment with new things. Ah, so I love this panel. Kate, Aaron, Chris, thank you for\njoining us on the show as always",
    "start": "2324889",
    "end": "2333180"
  },
  {
    "text": "and walking us through DeepSeek. Uh, a lot more to talk about and\nwe will be tracking the story.",
    "start": "2333180",
    "end": "2338420"
  },
  {
    "text": "Um, and thanks for you\nlisteners for joining us. Uh, if you enjoyed what you heard, you\ncan get us on Apple Podcasts, Spotify,",
    "start": "2338510",
    "end": "2344950"
  },
  {
    "text": "and podcast platforms everywhere. And we will see you next\nweek on a jam packed episode again of Mixture of Experts.",
    "start": "2344950",
    "end": "2350170"
  }
]