[
  {
    "text": "How good is an AI model at forecasting?",
    "start": "540",
    "end": "3600"
  },
  {
    "text": "We can put an actual number on it.",
    "start": "4300",
    "end": "6795"
  },
  {
    "text": "In machine learning a loss function tracks the degree of error in the output from an  AI model,",
    "start": "6795",
    "end": "16063"
  },
  {
    "text": "and it does this by quantifying the difference or the loss between a predicted value.",
    "start": "16064",
    "end": "22230"
  },
  {
    "text": "So let's say that that is five, the model gave us five, as the output and then comparing that to the actual value.",
    "start": "22500",
    "end": "33030"
  },
  {
    "text": "So maybe the model gave us ten and we call that the ground truth.",
    "start": "33390",
    "end": "40049"
  },
  {
    "text": "Now, if the model's predictions are accurate, then the difference between these two numbers,",
    "start": "40320",
    "end": "47637"
  },
  {
    "text": "the loss, in effect, is comparatively small.",
    "start": "47637",
    "end": "52979"
  },
  {
    "text": "If it's predictions are inaccurate, let's say it came back with an output of one instead of five, then the loss is larger.",
    "start": "53490",
    "end": "62610"
  },
  {
    "text": "So let me give you an example of how we can use this.",
    "start": "63300",
    "end": "65849"
  },
  {
    "text": "Now, I have for a colleague who built an AI model to forecast how many views his videos would receive on YouTube.",
    "start": "66300",
    "end": "74219"
  },
  {
    "text": "He fed the model YouTube titles and then the model forecast how many views that video would receive in its first week.",
    "start": "74640",
    "end": "82710"
  },
  {
    "text": "Here they are.",
    "start": "83070",
    "end": "84070"
  },
  {
    "text": "Little bit vain, if you ask me.",
    "start": "84810",
    "end": "86279"
  },
  {
    "text": "But it wasn't me.",
    "start": "86280",
    "end": "87599"
  },
  {
    "text": "It was my colleague.",
    "start": "88140",
    "end": "89140"
  },
  {
    "text": "Now, how well did the model do?",
    "start": "89160",
    "end": "90930"
  },
  {
    "text": "Well, when comparing the model forecasts to the actual number of real YouTube views,",
    "start": "90940",
    "end": "96732"
  },
  {
    "text": "the model wasn't getting too close.",
    "start": "96732",
    "end": "98939"
  },
  {
    "text": "The model predicted that the cold brew video would bomb, and that pour over guide video would be a big hit.",
    "start": "98970",
    "end": "104519"
  },
  {
    "text": "Just wasn't the case, though.",
    "start": "104950",
    "end": "106229"
  },
  {
    "text": "Now, this is a hard problem to solve and clearly this model needs some adjustments",
    "start": "106740",
    "end": "110868"
  },
  {
    "text": "and that's where loss functions can help.",
    "start": "110868",
    "end": "114118"
  },
  {
    "text": "Loss functions",
    "start": "114540",
    "end": "115320"
  },
  {
    "text": "let us define how well a model is doing mathematically.",
    "start": "115320",
    "end": "118319"
  },
  {
    "text": "And if we can calculate loss, we can then adjust model parameters and see if that increases loss,",
    "start": "118530",
    "end": "124868"
  },
  {
    "text": "meaning it's made it worse, or if it decreases loss, meaning it's made it better.",
    "start": "124869",
    "end": "129270"
  },
  {
    "text": "And at some point we can say that a machine learning model has been sufficiently trained.",
    "start": "129600",
    "end": "133469"
  },
  {
    "text": "When loss has been minimized below some predefined threshold.",
    "start": "133710",
    "end": "137610"
  },
  {
    "text": "Now at a high level, we can divide loss functions into two types, regression loss functions and then classification loss functions.",
    "start": "138560",
    "end": "146680"
  },
  {
    "text": "And let's start.",
    "start": "146690",
    "end": "148430"
  },
  {
    "text": "With regression, which measures errors in predictions involving continuous values.",
    "start": "149520",
    "end": "155819"
  },
  {
    "text": "Predictions like the price of a house or the temperature for a given day or well, the views for a YouTube video.",
    "start": "156120",
    "end": "162569"
  },
  {
    "text": "Now, in these cases",
    "start": "162930",
    "end": "164450"
  },
  {
    "text": "the loss function measures how far off the model's predictions are from the actual continuous target values.",
    "start": "164450",
    "end": "169979"
  },
  {
    "text": "Now, regression loss must be sensitive to two things, basically whether the forecast is correct or not.",
    "start": "170610",
    "end": "176340"
  },
  {
    "text": "But also the degree to which it diverges from the ground truth.",
    "start": "176820",
    "end": "180880"
  },
  {
    "text": "And there are multiple ways to calculate regression loss functions.",
    "start": "180900",
    "end": "185189"
  },
  {
    "text": "Now, the most common of those is called MSE or mean squared error.",
    "start": "185550",
    "end": "193289"
  },
  {
    "text": "Now, as its name suggests,",
    "start": "193740",
    "end": "195068"
  },
  {
    "text": "MSE is calculated as the average of the squared difference",
    "start": "195068",
    "end": "198931"
  },
  {
    "text": "between the predicted value and the true value across all training examples.",
    "start": "198932",
    "end": "203520"
  },
  {
    "text": "And squaring the error means the MSE gives large mistakes a disproportionately heavy impact on overall loss,",
    "start": "203700",
    "end": "211528"
  },
  {
    "text": "which strongly punishes outliers.",
    "start": "211528",
    "end": "214290"
  },
  {
    "text": "So that's MSE.",
    "start": "214620",
    "end": "216030"
  },
  {
    "text": "MAE or mean absolute error measures the average absolute difference between the predicted value",
    "start": "216720",
    "end": "225690"
  },
  {
    "text": "and MAE and is less sensitive to outliers compared to MSE as it doesn't square the errors.",
    "start": "225690",
    "end": "230889"
  },
  {
    "text": "So how do you decide which regression loss function to pick?",
    "start": "231980",
    "end": "235849"
  },
  {
    "text": "Well, if your ground truth data has relatively few extreme outliers with minimal deviation.",
    "start": "235880",
    "end": "242299"
  },
  {
    "text": "Like, I don't know, the temperature ranges in the month of July in the southern US, which, trust me, is basically always hot.",
    "start": "242690",
    "end": "248900"
  },
  {
    "text": "Well then MSE is a particularly useful option for you",
    "start": "249350",
    "end": "253946"
  },
  {
    "text": "as you want to heavily penalize predictions that are far off from the actual values.",
    "start": "253946",
    "end": "257930"
  },
  {
    "text": "MAE is a better option when data does contain more outliers.",
    "start": "258500",
    "end": "262579"
  },
  {
    "text": "And we don't want those outliers to overly influence the model.",
    "start": "262850",
    "end": "266018"
  },
  {
    "text": "Forecasting demand for a product.",
    "start": "266018",
    "end": "268339"
  },
  {
    "text": "That's a good example where occasional surges in sales shouldn't overly skew the model.",
    "start": "268370",
    "end": "273320"
  },
  {
    "text": "But there is a third choice.",
    "start": "274210",
    "end": "275529"
  },
  {
    "text": "The third choice is called huber loss.",
    "start": "275830",
    "end": "279550"
  },
  {
    "text": "Now, hubar loss is a compromise.",
    "start": "280480",
    "end": "282569"
  },
  {
    "text": "It's a compromise between MSE and MAE.",
    "start": "282580",
    "end": "286185"
  },
  {
    "text": "It behaves like MSE for small errors and MAE for large errors,",
    "start": "286186",
    "end": "290600"
  },
  {
    "text": "which makes it useful when you want the benefits of penalizing large errors but not too harshly.",
    "start": "290600",
    "end": "296559"
  },
  {
    "text": "Now I've calculated the lost functions for the YouTube example.",
    "start": "297130",
    "end": "300190"
  },
  {
    "text": "This is the MAE value summing up the absolute differences,",
    "start": "300760",
    "end": "304997"
  },
  {
    "text": "meaning on average the predictions were off by about 16,000 views per video.",
    "start": "304997",
    "end": "309690"
  },
  {
    "text": "The MSE lost function, that's over 400 million.",
    "start": "310540",
    "end": "316360"
  },
  {
    "text": "It skyrockets and that's due to the squaring of large errors, and the huber loss.",
    "start": "316360",
    "end": "321919"
  },
  {
    "text": "That also indicates poor predictions, but provides a more balanced perspective,",
    "start": "321940",
    "end": "325483"
  },
  {
    "text": "penalizing large errors less severely than MSI.",
    "start": "325483",
    "end": "328929"
  },
  {
    "text": "But look, these numbers don't mean a whole lot on their own.",
    "start": "329410",
    "end": "332769"
  },
  {
    "text": "We want to adjust the model's parameters, generate new forecasts and see where we move the needle on loss.",
    "start": "333040",
    "end": "339249"
  },
  {
    "text": "But before we get to how to do that, let's talk about the other type of loss function classification.",
    "start": "339790",
    "end": "344860"
  },
  {
    "text": "Unlike regression loss functions which deal with predicting continuous numerical values,",
    "start": "345760",
    "end": "350258"
  },
  {
    "text": "classification loss functions, well, they're focused on determining the accuracy of categorical predictions.",
    "start": "350258",
    "end": "358029"
  },
  {
    "text": "Is an email spam or not spam?",
    "start": "358300",
    "end": "361029"
  },
  {
    "text": "Are these plants classified into their correct species based on their features?",
    "start": "361280",
    "end": "365079"
  },
  {
    "text": "So the loss function in classification tasks measures how well the predicted",
    "start": "365680",
    "end": "370977"
  },
  {
    "text": "probabilities or labels match the actual categories.",
    "start": "370977",
    "end": "375490"
  },
  {
    "text": "Now cross entropy loss is one way of doing this, and it's the most widely used loss function for classification tasks.",
    "start": "376130",
    "end": "388620"
  },
  {
    "text": "Now, what is entropy?",
    "start": "388920",
    "end": "390209"
  },
  {
    "text": "It's a measure of uncertainty within a system.",
    "start": "390540",
    "end": "393149"
  },
  {
    "text": "So if you're flipping a coin, there are only two possible outcomes heads or tails.",
    "start": "393180",
    "end": "397919"
  },
  {
    "text": "The uncertainty is pretty low.",
    "start": "397980",
    "end": "400110"
  },
  {
    "text": "So low entropy.",
    "start": "400590",
    "end": "401820"
  },
  {
    "text": "Running a six sided die means there's more uncertainty about which of these six possible numbers will come up.",
    "start": "402270",
    "end": "407879"
  },
  {
    "text": "The entropy is higher.",
    "start": "407880",
    "end": "409290"
  },
  {
    "text": "Now cross entropy loss measures how uncertain the model's predictions are compared to the actual outcomes.",
    "start": "409860",
    "end": "415319"
  },
  {
    "text": "In supervised learning, model predictions are compared to the ground truth classifications provided by data tables.",
    "start": "415830",
    "end": "421709"
  },
  {
    "text": "Those ground truth labels are certain, and so they have low or in fact no entropy.",
    "start": "422220",
    "end": "427349"
  },
  {
    "text": "As such, we can measure the loss in terms of the difference in certainty we'd have using the ground truth labels",
    "start": "427800",
    "end": "433146"
  },
  {
    "text": "to the certainty of the labels predicted by the model.",
    "start": "433146",
    "end": "436620"
  },
  {
    "text": "Now, an alternative to this is called hinge loss instead.",
    "start": "437190",
    "end": "443010"
  },
  {
    "text": "Now, this is commonly used in support of vector machines",
    "start": "443960",
    "end": "446775"
  },
  {
    "text": "and hence loss encourages the model to make both correct predictions and to do so with a certain level of confidence.",
    "start": "446775",
    "end": "455069"
  },
  {
    "text": "It's all about measuring that level of confidence,",
    "start": "455090",
    "end": "458288"
  },
  {
    "text": "and it focuses on maximizing the margin between classes with the goal that the model is not just correct,",
    "start": "458288",
    "end": "464802"
  },
  {
    "text": "but it's confidently correct by a specified margin.",
    "start": "464802",
    "end": "468949"
  },
  {
    "text": "And this makes the hinge loss particularly useful in binary classification tasks",
    "start": "468950",
    "end": "473408"
  },
  {
    "text": "where the distinction between classes needs to be as clear and as far apart as possible.",
    "start": "473408",
    "end": "478849"
  },
  {
    "text": "So we've calculated our loss function.",
    "start": "479720",
    "end": "481840"
  },
  {
    "text": "Great, but what can we do with that information?",
    "start": "481850",
    "end": "485029"
  },
  {
    "text": "Now remember that the primary reason for calculating the loss function is to guide the model's learning process.",
    "start": "486190",
    "end": "491320"
  },
  {
    "text": "The last function provides a numeric value that indicates how far off the model's predictions are from the actual results.",
    "start": "491680",
    "end": "498979"
  },
  {
    "text": "And by analyzing this loss, we can adjust the model's parameters typically through a process called optimization.",
    "start": "499000",
    "end": "506140"
  },
  {
    "text": "In essence, the loss function acts as a feedback mechanism,",
    "start": "506260",
    "end": "509720"
  },
  {
    "text": "telling the model how well it's performing and where it needs to improve.",
    "start": "509720",
    "end": "513159"
  },
  {
    "text": "The lower the loss, the better the model's predictions align with the true outcomes.",
    "start": "513640",
    "end": "518199"
  },
  {
    "text": "Now, after adjusting the YouTube prediction model,",
    "start": "518830",
    "end": "521737"
  },
  {
    "text": "we get a new set of forecasts and we can now compare the loss functions between the two models,",
    "start": "521738",
    "end": "528646"
  },
  {
    "text": "and in all three cases, the loss function is now lower,",
    "start": "528646",
    "end": "533008"
  },
  {
    "text": "indicating less loss with the greatest effect on MSE, mean squared error.",
    "start": "533008",
    "end": "539619"
  },
  {
    "text": "As the model reduced the large prediction error for the poorer the video.",
    "start": "539830",
    "end": "543190"
  },
  {
    "text": "Now that's lost function as an evaluation metric,",
    "start": "544030",
    "end": "547570"
  },
  {
    "text": "but it can also be used as inputs into an algorithm that actually influences the model parameters.",
    "start": "547570",
    "end": "553420"
  },
  {
    "text": "To minimize loss, for example, by using gradient descent.",
    "start": "553570",
    "end": "557559"
  },
  {
    "text": "And that works by calculating the gradient or the slope of a loss function",
    "start": "557860",
    "end": "564776"
  },
  {
    "text": "with respect to each parameter.",
    "start": "564776",
    "end": "567493"
  },
  {
    "text": "Using the gradient of the loss function",
    "start": "567493",
    "end": "570123"
  },
  {
    "text": "Optimization algorithms determine which direction to step the model in order to move down the gradient",
    "start": "570123",
    "end": "577394"
  },
  {
    "text": "and therefore reduce loss.",
    "start": "577394",
    "end": "580118"
  },
  {
    "text": "The model learns by updating the weight and bias terms until the loss function has been sufficiently minimized.",
    "start": "580510",
    "end": "587679"
  },
  {
    "text": "So that's loss function.",
    "start": "588310",
    "end": "590690"
  },
  {
    "text": "It's both a scorekeeper that measures how well your model is performing,",
    "start": "590710",
    "end": "594815"
  },
  {
    "text": "and a guide that directs the model's learning process,",
    "start": "594815",
    "end": "598269"
  },
  {
    "text": "and a thanks to lost function.",
    "start": "598660",
    "end": "600418"
  },
  {
    "text": "My a, my colleague, can keep tweaking his YouTube AI model",
    "start": "600430",
    "end": "605579"
  },
  {
    "text": "to minimize the loss and teach that model to make better predictions.",
    "start": "605579",
    "end": "609790"
  }
]