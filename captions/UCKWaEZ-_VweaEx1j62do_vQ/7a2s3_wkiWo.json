[
  {
    "text": "There's a lot of attention on large language \nmodels, or LLMs, and rightfully so.",
    "start": "720",
    "end": "5356"
  },
  {
    "text": "These AI models have proven to be remarkable at \nperforming a multitude of AI tasks.",
    "start": "5356",
    "end": "10262"
  },
  {
    "text": "The question is how large is large?",
    "start": "10262",
    "end": "13687"
  },
  {
    "text": "Or better yet, is larger always better?",
    "start": "13687",
    "end": "22193"
  },
  {
    "text": "To answer that question, we will explore attributes of LLMs",
    "start": "24397",
    "end": "29293"
  },
  {
    "text": "and in the process I might even convince you\nthat there's an alternative that is better with less.",
    "start": "29293",
    "end": "36123"
  },
  {
    "text": "But we'll take a detour and we'll look at a very unlikely area for an example, dinosaurs.",
    "start": "36469",
    "end": "42456"
  },
  {
    "text": "Dinosaurs were large and had huge scale.",
    "start": "42566",
    "end": "45847"
  },
  {
    "text": "And one would expect that that was sufficient to ensure they did not become extinct.",
    "start": "45973",
    "end": "50739"
  },
  {
    "text": "However, the characteristic of large and huge scale was not sufficient to prevent extinction.",
    "start": "50834",
    "end": "56906"
  },
  {
    "text": "Contrast that with ants.",
    "start": "57237",
    "end": "59559"
  },
  {
    "text": "Ants are smaller. Yet they continue to thrive. \nAnd I would point to two things. Specialization  ",
    "start": "59686",
    "end": "72520"
  },
  {
    "text": "and efficiency. Now I realize, and I can see you \nat home saying, \"Well, Kip, that is a very poor  ",
    "start": "72520",
    "end": "83079"
  },
  {
    "text": "analogy.\" But stick with me and you'll see where \nI'm headed. Let's answer the question: What is the  ",
    "start": "83080",
    "end": "88440"
  },
  {
    "text": "relationship between this poor analogy and LLMs? \nI'll answer that by looking at three attributes  ",
    "start": "88440",
    "end": "95360"
  },
  {
    "text": "of LLMs. Let's start with cost. When you talk \nof cost, the different components of LLMs,, is  ",
    "start": "95360",
    "end": "102480"
  },
  {
    "text": "the cost of the consumption of the energy used to \ntrain the models, the cost of compute, the cost of  ",
    "start": "102480",
    "end": "107840"
  },
  {
    "text": "inferencing. There's also the cost of the carbon \nthat is emitted when LLMs are in use. But for  ",
    "start": "107840",
    "end": "114200"
  },
  {
    "text": "simplicity, I will examine two models and compare \nthem in terms of energy consumption to train these  ",
    "start": "114200",
    "end": "120680"
  },
  {
    "text": "models. So we'll start with cost. As I said, we \nlook at a large model at 175 billion parameters  ",
    "start": "120680",
    "end": "129960"
  },
  {
    "text": "and a smaller model at 13 billion parameters. \nAnd now the energy consumed to train the larger  ",
    "start": "129960",
    "end": "137480"
  },
  {
    "text": "model was 284,000 kilowatt hours. And for the \nsmaller model, it was 153,000 kilowatt hours. Now,  ",
    "start": "137480",
    "end": "151480"
  },
  {
    "text": "you're probably saying \"Kip, this is logical. \nWhy do we even need to talk about it?\" Well,  ",
    "start": "151480",
    "end": "157400"
  },
  {
    "text": "the reason I'm bringing it up is to make sure \nwe're clear [that] cost is always a consideration.  ",
    "start": "157400",
    "end": "162959"
  },
  {
    "text": "In fact, I'll go further and point out that it \ntakes about a 10th of CPU hours to train the  ",
    "start": "162960",
    "end": "172360"
  },
  {
    "text": "smaller model relative to the larger model. The \nnext attribute that I want us to look at is that  ",
    "start": "172360",
    "end": "179280"
  },
  {
    "text": "of latency. And for that, once again, we'll look \nat two models and we'll compare the performance of  ",
    "start": "179280",
    "end": "186120"
  },
  {
    "text": "the two. We'll start with a 70 billion parameter \nfor the larger model, and we'll look compared to  ",
    "start": "186120",
    "end": "192920"
  },
  {
    "text": "a 13 billion parameter model for the smaller one. \nI should add, this model is trained on enterprise  ",
    "start": "192920",
    "end": "204520"
  },
  {
    "text": "domain-specific data. Now, when our test was \nperformed comparing these two models, that  ",
    "start": "204520",
    "end": "213720"
  },
  {
    "text": "smaller model performed three times faster than \nthe larger model. And I think we can appreciate  ",
    "start": "213720",
    "end": "221040"
  },
  {
    "text": "that because of the variable or the scale of the \ndata, obviously the response time for the larger  ",
    "start": "221040",
    "end": "226599"
  },
  {
    "text": "model would be slower than that of the smaller \nmodel. And you may come back and say, \"Well, Kip,  ",
    "start": "226600",
    "end": "231640"
  },
  {
    "text": "I don't care about cost necessarily\" or \"I don't \nnecessarily care as much about the latency. What  ",
    "start": "231640",
    "end": "238560"
  },
  {
    "text": "is important to me is the performance.\" Well, let \nus look at accuracy. And again, we'll compare the  ",
    "start": "238560",
    "end": "244920"
  },
  {
    "text": "two models. The 13 billion parameter model and the \n70 billion parameter model. So these two models  ",
    "start": "244920",
    "end": "250920"
  },
  {
    "text": "were tested on financial services tasks and they \nwere tested on 11 tasks, on sentiment analysis,  ",
    "start": "250920",
    "end": "260600"
  },
  {
    "text": "classification, question and answering, \nsummarization. A number of generative AI  ",
    "start": "260600",
    "end": "265440"
  },
  {
    "text": "tasks. And when the results came out, this is how \nthey faired: The 70 billion parameter model had  ",
    "start": "265440",
    "end": "275120"
  },
  {
    "text": "0.59 result in terms of accuracy, the 30 billion \nparameter model had 0.57. Now, one would expect  ",
    "start": "275120",
    "end": "286680"
  },
  {
    "text": "that the larger model would perform significantly \nbetter than the smaller model. But because this  ",
    "start": "286680",
    "end": "292240"
  },
  {
    "text": "model was trained on domain data specific to this \nindustry, its performance is relatively similar  ",
    "start": "292240",
    "end": "298599"
  },
  {
    "text": "to that of the larger model. I think you begin to \nget the picture that I'm trying to paint for you.  ",
    "start": "298600",
    "end": "305520"
  },
  {
    "text": "Domain-specific models are a consideration when \nthinking through what LLM should I use. There  ",
    "start": "305520",
    "end": "311960"
  },
  {
    "text": "is no question about the performance of LLMs, \ngenerally speaking, in terms of the different task  ",
    "start": "311960",
    "end": "318080"
  },
  {
    "text": "that they do. As I mentioned at the beginning, \nthey are superb. However, I would like to put out  ",
    "start": "318080",
    "end": "325159"
  },
  {
    "text": "for your consideration that domain-specific \nmodels because of two things I mentioned  ",
    "start": "325160",
    "end": "329600"
  },
  {
    "text": "earlier--specialization and efficiency, should be \na consideration. So let's go back to the question  ",
    "start": "329600",
    "end": "337040"
  },
  {
    "text": "we started off earlier. Is larger always better? \nNot necessarily. The question then becomes,  ",
    "start": "337040",
    "end": "346040"
  },
  {
    "text": "how do I choose which model or should I choose in \nlarger model? My answer will be \"It depends.\" It  ",
    "start": "346040",
    "end": "353200"
  },
  {
    "text": "depends on the use case that you need. You need \nit for what? I want you to take away from this,  ",
    "start": "353200",
    "end": "359040"
  },
  {
    "text": "though, is in certain scenarios, in certain \nuse cases, domain-specific models will be  ",
    "start": "359040",
    "end": "365280"
  },
  {
    "text": "a better alternative. And here's why. As we \nhave seen from the examination you performed,  ",
    "start": "365280",
    "end": "371680"
  },
  {
    "text": "it was equal or comparable to the larger model \nin terms of the accuracy. It performed better  ",
    "start": "371680",
    "end": "378320"
  },
  {
    "text": "in terms of that latency and it cost much less \nfrom a cost perspective. So when you take these  ",
    "start": "378320",
    "end": "386680"
  },
  {
    "text": "three attributes into consideration and this is \njust an example, there are more attributes you  ",
    "start": "386680",
    "end": "390400"
  },
  {
    "text": "can look at domain-specific models should be \na consideration in terms of the LLMs that you  ",
    "start": "390400",
    "end": "396000"
  },
  {
    "text": "use at your organization. And with that, I thank \nyou. If you liked this video and want to see more  ",
    "start": "396000",
    "end": "402856"
  },
  {
    "text": "like it, please like and subscribe. If you have \nquestions, please drop them in the comments below.",
    "start": "402856",
    "end": "411919"
  }
]