[
  {
    "start": "0",
    "end": "19000"
  },
  {
    "text": "What is inferencing.",
    "start": "800",
    "end": "2729"
  },
  {
    "text": "It's an AI model's time to shine its moment of truth, a test of",
    "start": "2750",
    "end": "7147"
  },
  {
    "text": "how well the model can apply information learned during training to make a prediction or solve a task.",
    "start": "7147",
    "end": "13710"
  },
  {
    "text": "And with it comes a focus on cost and speed.",
    "start": "13730",
    "end": "16668"
  },
  {
    "text": "Let's get into it.",
    "start": "17480",
    "end": "18649"
  },
  {
    "start": "19000",
    "end": "312000"
  },
  {
    "text": "So an AI model,",
    "start": "19830",
    "end": "21570"
  },
  {
    "text": "it   goes through two primary stages.",
    "start": "22410",
    "end": "27180"
  },
  {
    "text": "What are those?",
    "start": "27690",
    "end": "28690"
  },
  {
    "text": "The first of those is the training stage where the model learns how to do stuff.",
    "start": "29130",
    "end": "37198"
  },
  {
    "text": "And then we have the inferencing.",
    "start": "38010",
    "end": "41399"
  },
  {
    "text": "Stage that comes after training.",
    "start": "42700",
    "end": "46359"
  },
  {
    "text": "Now, we can think of this as the difference between learning something and then putting what we've learned into practice.",
    "start": "47170",
    "end": "56439"
  },
  {
    "text": "So during training, a deep learning model computes how the examples in its training set are related.",
    "start": "56680",
    "end": "63189"
  },
  {
    "text": "What it's doing effectively here is it's figuring out relationships between all of the data in its training set.",
    "start": "63550",
    "end": "72699"
  },
  {
    "text": "And it encodes these relationships into what are called a series of model weights.",
    "start": "73270",
    "end": "80679"
  },
  {
    "text": "These are the weights that connect artificial neurons.",
    "start": "80920",
    "end": "84639"
  },
  {
    "text": "So that's training.",
    "start": "85240",
    "end": "86240"
  },
  {
    "text": "Now, during inference, a model goes to work on what we provide it, which is real time data.",
    "start": "86620",
    "end": "95590"
  },
  {
    "text": "So this is the actual data that we are inputting into the model.",
    "start": "95950",
    "end": "100148"
  },
  {
    "text": "What happens in inferencing is the model.",
    "start": "101460",
    "end": "103199"
  },
  {
    "text": "Compares the user's query with the information processed during training and all of those stored weights.",
    "start": "103470",
    "end": "108959"
  },
  {
    "text": "And what the model effectively does is it generalizes based on everything that it has learned during training.",
    "start": "109330",
    "end": "116430"
  },
  {
    "text": "So it generalizes from this stored representation to be able to interpret this new unseen data",
    "start": "116940",
    "end": "123379"
  },
  {
    "text": "in much the same way that you and I can draw on prior knowledge to infer",
    "start": "123379",
    "end": "127827"
  },
  {
    "text": "the meaning of a new word or make sense of a new situation.",
    "start": "127828",
    "end": "130949"
  },
  {
    "text": "And what's the goal of this?",
    "start": "131940",
    "end": "133050"
  },
  {
    "text": "Well, the goal of AI inference is to calculate an output, basically a result, an actionable result.",
    "start": "133350",
    "end": "141929"
  },
  {
    "text": "So what sort of result are we talking about?",
    "start": "143070",
    "end": "147569"
  },
  {
    "text": "Well, let's consider a model that attempts to accurately flag incoming email,",
    "start": "148200",
    "end": "154043"
  },
  {
    "text": "and it's going to flag it based on whether or not it thinks it is spam.",
    "start": "154043",
    "end": "158999"
  },
  {
    "text": "We are going to build a spam detector model.",
    "start": "159090",
    "end": "163050"
  },
  {
    "text": "Right.",
    "start": "164050",
    "end": "165040"
  },
  {
    "text": "So during the training stage, this model would be fed a large labeled data set.",
    "start": "165040",
    "end": "172478"
  },
  {
    "text": "So we get into a whole load of data here, and this contains a bunch of emails that have been labeled.",
    "start": "172840",
    "end": "180338"
  },
  {
    "text": "The civically the labels are spam or not spam for each email.",
    "start": "180670",
    "end": "189369"
  },
  {
    "text": "And what happens here is the model learns to recognize patterns and features commonly associated with spam emails.",
    "start": "190210",
    "end": "196749"
  },
  {
    "text": "So these might include the presence of certain keywords.",
    "start": "197020",
    "end": "199809"
  },
  {
    "text": "Yeah.",
    "start": "200440",
    "end": "201320"
  },
  {
    "text": "These ones so unusual.",
    "start": "201320",
    "end": "203380"
  },
  {
    "text": "Send the email addresses, excessive use of exclamation marks, all that sort of thing.",
    "start": "203380",
    "end": "207550"
  },
  {
    "text": "Now the model encodes these learned patterns into its weight here, creating a complex set of rules to identify spam.",
    "start": "208390",
    "end": "217749"
  },
  {
    "text": "Now, during inference, this model is put to the test.",
    "start": "218630",
    "end": "222379"
  },
  {
    "text": "It's put to the test with new unseen data in real time, like when a new email arrives in a user's inbox.",
    "start": "222560",
    "end": "233090"
  },
  {
    "text": "The model analyzes the incoming email, comparing its characteristics",
    "start": "233840",
    "end": "238019"
  },
  {
    "text": "to the patterns it's learned during training and then makes a prediction.",
    "start": "238020",
    "end": "242809"
  },
  {
    "text": "Is this new unseen email spam or not spam?",
    "start": "243050",
    "end": "247819"
  },
  {
    "text": "Now, the actionable result here might be a probability score",
    "start": "248510",
    "end": "253002"
  },
  {
    "text": "indicating how likely the email is to be spam, which is then tied into a business rule.",
    "start": "253002",
    "end": "258560"
  },
  {
    "text": "So, for example, if the model assigns a 90% probability",
    "start": "258589",
    "end": "264397"
  },
  {
    "text": "that what we're looking at here is spam, well we should move that email directly to the spam folder.",
    "start": "264397",
    "end": "272029"
  },
  {
    "text": "That's what the business rule would say.",
    "start": "272090",
    "end": "273589"
  },
  {
    "text": "But if the probability the model comes back with is just 50%,",
    "start": "274130",
    "end": "278407"
  },
  {
    "text": "the business rule might say to leave the email in the box, but flag it for the users to decide what to do.",
    "start": "278407",
    "end": "285170"
  },
  {
    "text": "So what's happening here is the model is generalizing.",
    "start": "285200",
    "end": "288680"
  },
  {
    "text": "It can identify spam emails even if they don't exactly match any specific example from its training data.",
    "start": "289100",
    "end": "296239"
  },
  {
    "text": "As long as they share similar characteristics with the spam patterns, it's learned.",
    "start": "296510",
    "end": "300930"
  },
  {
    "text": "Okay.",
    "start": "301610",
    "end": "302389"
  },
  {
    "text": "Now, when the topic of inferencing comes up, it is often accompanied with four preceding words.",
    "start": "302390",
    "end": "309140"
  },
  {
    "text": "Let's cover those next.",
    "start": "309830",
    "end": "311000"
  },
  {
    "start": "312000",
    "end": "459000"
  },
  {
    "text": "The high cost of those are the words often added before inferencing.",
    "start": "312890",
    "end": "319636"
  },
  {
    "text": "Training AI models, particularly large language models, can cost millions of dollars in computing processing time.",
    "start": "319636",
    "end": "325879"
  },
  {
    "text": "But as expensive as training an AI model can be, it is dwarfed by the expense of inferencing.",
    "start": "326150",
    "end": "333139"
  },
  {
    "text": "Each time someone runs an AI model, there's a cost, a cost in kilowatt hours, a cost in dollars, a cost in carbon emissions.",
    "start": "333620",
    "end": "340009"
  },
  {
    "text": "On average, something like about 90% of an AI model's life is spent in inferencing mode.",
    "start": "340550",
    "end": "350539"
  },
  {
    "text": "And therefore, most of the AI's carbon footprint comes from serving models to the world, not in training them.",
    "start": "350720",
    "end": "356540"
  },
  {
    "text": "In fact, by some estimates, running a large model puts more carbon into the atmosphere over its lifetime",
    "start": "356810",
    "end": "362432"
  },
  {
    "text": "than the average American car.",
    "start": "362432",
    "end": "364130"
  },
  {
    "text": "Now, the high costs of inferencing, they stem from a number of different factors.",
    "start": "364640",
    "end": "370459"
  },
  {
    "text": "So let's take a look at some of those.",
    "start": "370550",
    "end": "372349"
  },
  {
    "text": "First of all, there's just the the sheer scale, the scale of operations.",
    "start": "372860",
    "end": "378589"
  },
  {
    "text": "While training happens just once, inferencing happens millions or even billions of times over a model's lifetime,",
    "start": "378800",
    "end": "385697"
  },
  {
    "text": "a chat bot might field millions of queries every day, each requiring a separate inference.",
    "start": "385697",
    "end": "390860"
  },
  {
    "text": "Second, there's the need.",
    "start": "391670",
    "end": "393050"
  },
  {
    "text": "The need,",
    "start": "393530",
    "end": "394530"
  },
  {
    "text": "for speed.",
    "start": "395230",
    "end": "396100"
  },
  {
    "text": "We want fast AI models.",
    "start": "396100",
    "end": "399590"
  },
  {
    "text": "We're working with real time data here requiring near instantaneous responses",
    "start": "399610",
    "end": "404789"
  },
  {
    "text": "which often necessitate powerful energy hungry hardware like GPUs.",
    "start": "404790",
    "end": "410410"
  },
  {
    "text": "Third, we have to consider also just the general complexity of these AI models.",
    "start": "411250",
    "end": "418179"
  },
  {
    "text": "As models grew larger and more sophisticated to handle the more complex tasks,",
    "start": "418570",
    "end": "423009"
  },
  {
    "text": "they require more computational resources for each inference.",
    "start": "423009",
    "end": "425709"
  },
  {
    "text": "This is particularly true for LLMs with billions of parameters.",
    "start": "426160",
    "end": "429790"
  },
  {
    "text": "And then finally there is the cost in terms of infrastructure costs,",
    "start": "430390",
    "end": "436353"
  },
  {
    "text": "data centers to maintain and cool low latency network connections to power,",
    "start": "436353",
    "end": "441699"
  },
  {
    "text": "all these factors contribute to significant ongoing costs in terms of energy consumption,",
    "start": "442150",
    "end": "446919"
  },
  {
    "text": "hardware wear, and tear and operational expenses.",
    "start": "447220",
    "end": "449649"
  },
  {
    "text": "Which brings up the question of if there's a better way to do this faster and more efficiently.",
    "start": "450310",
    "end": "458110"
  },
  {
    "start": "459000",
    "end": "640000"
  },
  {
    "text": "How fast an AI model runs depends on the stack.",
    "start": "459640",
    "end": "463899"
  },
  {
    "text": "What's the stack?",
    "start": "464770",
    "end": "465610"
  },
  {
    "text": "Well, improvements made to each layer can speed up inferencing and top of the stack.",
    "start": "465610",
    "end": "470649"
  },
  {
    "text": "Is hardware at the hardware level.",
    "start": "471540",
    "end": "475589"
  },
  {
    "text": "Engineers are developing specialized chips.",
    "start": "475770",
    "end": "479039"
  },
  {
    "text": "These are chips made for AI,",
    "start": "479340",
    "end": "483328"
  },
  {
    "text": "and they're optimized for the types of mathematical operations that dominate deep learning, particularly matrix multiplication.",
    "start": "483328",
    "end": "490170"
  },
  {
    "text": "These AI accelerators can significantly speed up inferencing tasks compared to traditional CPUs and even to GPUs,",
    "start": "490650",
    "end": "497059"
  },
  {
    "text": "and to do so in a more energy efficient way.",
    "start": "497610",
    "end": "500608"
  },
  {
    "text": "Now, bottom of the stack.",
    "start": "501150",
    "end": "503399"
  },
  {
    "text": "I put software.",
    "start": "504480",
    "end": "505769"
  },
  {
    "text": "And on the software side, there are several approaches to accelerate inferencing.",
    "start": "506070",
    "end": "510689"
  },
  {
    "text": "One is model compression.",
    "start": "511020",
    "end": "513149"
  },
  {
    "text": "Now that involves techniques like pruning and quantization.",
    "start": "513210",
    "end": "516749"
  },
  {
    "text": "So what do we mean by those?",
    "start": "517080",
    "end": "518880"
  },
  {
    "text": "Well, first of all, pruning that removes unnecessary weights from the model.",
    "start": "519150",
    "end": "525929"
  },
  {
    "text": "So it's reducing its size without significantly impacting accuracy.",
    "start": "526230",
    "end": "530730"
  },
  {
    "text": "And then for quantization, what that is talking about is reducing the precision of the model's weights,",
    "start": "531030",
    "end": "538313"
  },
  {
    "text": "such as from 32 bit floating point numbers to eight bit integers,",
    "start": "538313",
    "end": "542544"
  },
  {
    "text": "and that can really speed up computations and reduce memory requirements.",
    "start": "542544",
    "end": "546720"
  },
  {
    "text": "Okay, so we've got hardware and software.",
    "start": "547280",
    "end": "549450"
  },
  {
    "text": "What's in the middle?",
    "start": "550290",
    "end": "551290"
  },
  {
    "text": "Middleware of course, middleware bridges the gap between the hardware and the software,",
    "start": "552090",
    "end": "558067"
  },
  {
    "text": "and middleware frameworks can perform a bunch of things to help here.",
    "start": "558067",
    "end": "561958"
  },
  {
    "text": "One of those things is called graph fusion.",
    "start": "562260",
    "end": "566669"
  },
  {
    "text": "And graph fusion reduces the number of nodes in the communication graph,",
    "start": "567660",
    "end": "572527"
  },
  {
    "text": "and that minimizes the roundtrips between CPU's and GPUs.",
    "start": "572527",
    "end": "577469"
  },
  {
    "text": "And they can also implement parallel tenses as well.",
    "start": "577980",
    "end": "582899"
  },
  {
    "text": "Strategically splitting the models computational graph into chunks",
    "start": "583530",
    "end": "588397"
  },
  {
    "text": "and those chunks can be spread across multiple GPUs and run at the same time.",
    "start": "588397",
    "end": "592798"
  },
  {
    "text": "So running a 17 billion parameter model",
    "start": "593370",
    "end": "596410"
  },
  {
    "text": "that requires something like 150GB of memory, which is nearly twice as much as an NVIDIA a100 GPU holds,",
    "start": "596410",
    "end": "605250"
  },
  {
    "text": "but if the compiler can split the AI model's computational graph into strategic chunks,",
    "start": "605490",
    "end": "612466"
  },
  {
    "text": "those operations can be spread across GPUs and run at the same time.",
    "start": "612466",
    "end": "617370"
  },
  {
    "text": "So that's inferencing.",
    "start": "618000",
    "end": "619970"
  },
  {
    "text": "It's a game, a game of pattern matching that tends complex training into rapid fire problem solving.",
    "start": "619980",
    "end": "626820"
  },
  {
    "text": "One spammy email at a time.",
    "start": "627330",
    "end": "629999"
  },
  {
    "text": "If you have any questions, please drop us a line below.",
    "start": "631310",
    "end": "633919"
  },
  {
    "text": "And if you want to see more videos like this in the future, please like and subscribe.",
    "start": "633920",
    "end": "638539"
  },
  {
    "text": "Thanks for watching.",
    "start": "639110",
    "end": "640110"
  }
]