[
  {
    "text": "The more we use AI algorithms to discover patterns, to generate insights and just to help us make decisions,",
    "start": "490",
    "end": "6788"
  },
  {
    "text": "the more we should be concerned with the impact of algorithmic bias.",
    "start": "6789",
    "end": "11889"
  },
  {
    "text": "What is it and how can we minimize it?",
    "start": "11920",
    "end": "13960"
  },
  {
    "text": "Well, let's find out.",
    "start": "14290",
    "end": "16149"
  },
  {
    "text": "Algorithmic bias can lead to harmful decisions and actions, causing machine learning algorithms to produce unfair or discriminatory outcomes.",
    "start": "18000",
    "end": "26439"
  },
  {
    "text": "It's something we want to avoid.",
    "start": "26460",
    "end": "27988"
  },
  {
    "text": "So let's take a look at the causes of algorithmic bias, some real world examples and mitigation strategy,",
    "start": "27990",
    "end": "37139"
  },
  {
    "text": "and let's get started with causes.",
    "start": "37140",
    "end": "40530"
  },
  {
    "text": "Now algorithmic bias is not necessarily caused by the AI algorithms themselves, but by how data is collected and coded.",
    "start": "40680",
    "end": "50909"
  },
  {
    "text": "We can think of this in terms of four different causes.",
    "start": "51390",
    "end": "54989"
  },
  {
    "text": "Now, the most obvious is biases that occur in the actual training dataset itself.",
    "start": "55260",
    "end": "63599"
  },
  {
    "text": "Essentially, we're talking about bad data.",
    "start": "63900",
    "end": "67650"
  },
  {
    "text": "That's data that's non-representative or lacks information or is in some other way a misrepresentation of the ground truth.",
    "start": "67950",
    "end": "76740"
  },
  {
    "text": "It can also be data that is incorrectly classified, causing the algorithm to misunderstand",
    "start": "77430",
    "end": "82933"
  },
  {
    "text": "what the data represents, and a little bad data can go a long way.",
    "start": "82933",
    "end": "89280"
  },
  {
    "text": "AI systems that generate biased results may use those results as input data for further decision making,",
    "start": "89310",
    "end": "96329"
  },
  {
    "text": "and while that creates a feedback loop that can reinforce this bias over and over again.",
    "start": "96540",
    "end": "104099"
  },
  {
    "text": "Now, another cause, of algorithm, of algorithmic bias is really related to algorithmic design.",
    "start": "104580",
    "end": "112080"
  },
  {
    "text": "So this is talking about programing errors such as an eye designer",
    "start": "113000",
    "end": "117839"
  },
  {
    "text": "unfairly weighting factors in the decision making process.",
    "start": "117839",
    "end": "120798"
  },
  {
    "text": "They can unknowingly transfer into the system some of those biases,",
    "start": "121040",
    "end": "126297"
  },
  {
    "text": "where it might be developers that might embed the algorithm with subjective rules",
    "start": "126297",
    "end": "130388"
  },
  {
    "text": "based on their own conscious or unconscious biases.",
    "start": "130389",
    "end": "133039"
  },
  {
    "text": "Poor algorithmic design that can also lead to correlation bias,",
    "start": "133550",
    "end": "137697"
  },
  {
    "text": "such as an algorithm that determines a causal relationship between increased shark attacks and higher ice cream sales.",
    "start": "137697",
    "end": "145999"
  },
  {
    "text": "Hey, they're both higher in the summer,",
    "start": "146300",
    "end": "148009"
  },
  {
    "text": "but that's correlation, not causation,",
    "start": "148940",
    "end": "151970"
  },
  {
    "text": "and an example of where the model failed to consider other factors in the data that may be of more importance.",
    "start": "152180",
    "end": "158329"
  },
  {
    "text": "We can also have biases in proxy data as well.",
    "start": "159290",
    "end": "164989"
  },
  {
    "text": "What's proxy data?",
    "start": "166230",
    "end": "167610"
  },
  {
    "text": "Well, that's data used as a stand in for attributes not available in the ground truth data.",
    "start": "167970",
    "end": "173830"
  },
  {
    "text": "So things like race or gender.",
    "start": "173850",
    "end": "175800"
  },
  {
    "text": "And that could be because they're in some way protected or they just plain unavailable.",
    "start": "176100",
    "end": "180629"
  },
  {
    "text": "For example, zip codes often serve as proxies for social economic status,",
    "start": "181230",
    "end": "186017"
  },
  {
    "text": "that might unfairly disadvantage particular demographic groups when evaluating applications or opportunities.",
    "start": "186018",
    "end": "192000"
  },
  {
    "text": "And there's also biases in evaluation as well.",
    "start": "192810",
    "end": "198270"
  },
  {
    "text": "How we interpret the results from an algorithm.",
    "start": "199140",
    "end": "202500"
  },
  {
    "text": "So even if the algorithm is completely neutral and it's completely data driven,",
    "start": "202500",
    "end": "207225"
  },
  {
    "text": "how an individual or how a business applies the algorithms output",
    "start": "207225",
    "end": "211802"
  },
  {
    "text": "can lead to unfair outcomes depending on how they understand those outputs.",
    "start": "211802",
    "end": "216629"
  },
  {
    "text": "Now there are a bunch of real world",
    "start": "218000",
    "end": "219919"
  },
  {
    "text": "examples of algorithmic bias.",
    "start": "220960",
    "end": "223320"
  },
  {
    "text": "Look, I'm not here to name and shame, but",
    "start": "223330",
    "end": "226657"
  },
  {
    "text": "let's briefly discuss a few high profile ones, like biases that have occurred in recruitment.",
    "start": "226657",
    "end": "235330"
  },
  {
    "text": "Now an IT company built an algorithm, and that algorithm could review resumes.",
    "start": "236080",
    "end": "241519"
  },
  {
    "text": "Unfortunately, they discovered this algorithm systematically discriminated against female job applicants.",
    "start": "241750",
    "end": "248349"
  },
  {
    "text": "Why?",
    "start": "248800",
    "end": "249790"
  },
  {
    "text": "Well, developers are training the hiring algorithm, use resumes from past hires,",
    "start": "249790",
    "end": "255491"
  },
  {
    "text": "and it turns out that those past hires were predominantly male.",
    "start": "255491",
    "end": "259629"
  },
  {
    "text": "As a result, the algorithm favored keywords and characteristics found in men's resumes.",
    "start": "260110",
    "end": "265209"
  },
  {
    "text": "For example, the algorithm downgraded resumes that included the word women,",
    "start": "265240",
    "end": "270224"
  },
  {
    "text": "as in women's rugby team, and it favored the kind of words that men tend to use more often, such as captured and executed.",
    "start": "270224",
    "end": "278710"
  },
  {
    "text": "Now, algorithms, they also play a big role in guiding decisions in the area of finance.",
    "start": "279580",
    "end": "287305"
  },
  {
    "text": "In the financial services sector and algorithmic bias here can have severe consequences for people's livelihoods,",
    "start": "287305",
    "end": "295271"
  },
  {
    "text": "as historical data can contain all sorts of demographic biases affecting things like creditworthiness and loan approvals.",
    "start": "295271",
    "end": "302768"
  },
  {
    "text": "For example, a study from the University of California, Berkeley showed that an AI",
    "start": "303280",
    "end": "307749"
  },
  {
    "text": "system for mortgages routinely charges minority borrowers higher rates",
    "start": "307750",
    "end": "311898"
  },
  {
    "text": "for the same loans when compared to white borrowers.",
    "start": "311898",
    "end": "315070"
  },
  {
    "text": "And look, I could go on AI image generator, for example, where",
    "start": "315490",
    "end": "320004"
  },
  {
    "text": "generated images of people in specialized professions found biases related to gender and age,",
    "start": "320004",
    "end": "325249"
  },
  {
    "text": "or how bias in pricing led to ride",
    "start": "325249",
    "end": "328785"
  },
  {
    "text": "sharing algorithms charging more for drop offs in neighborhoods with high nonwhite populations,",
    "start": "328785",
    "end": "333763"
  },
  {
    "text": "or how policing algorithms in Columbia reflected social biases that misrepresented forecasted criminal activity,",
    "start": "333763",
    "end": "341559"
  },
  {
    "text": "but how about we instead use the remaining time to figure out the steps that we can take to reduce algorithmic bias?",
    "start": "342370",
    "end": "351789"
  },
  {
    "text": "Now mitigating bias from AI systems,",
    "start": "353990",
    "end": "357499"
  },
  {
    "text": "that starts with AI governance.",
    "start": "357500",
    "end": "360049"
  },
  {
    "text": "Many of the guardrails that make sure air tools and systems are safe and ethical.",
    "start": "360350",
    "end": "364730"
  },
  {
    "text": "So let's take a look at four ways to do that across the system lifecycle.",
    "start": "364760",
    "end": "370569"
  },
  {
    "text": "And first up is diverse and representative data.",
    "start": "370580",
    "end": "376518"
  },
  {
    "text": "Machine learning is only as good as the data that trains it.",
    "start": "377150",
    "end": "381949"
  },
  {
    "text": "Data fed into machine learning models must be representative of all groups of people",
    "start": "382370",
    "end": "388023"
  },
  {
    "text": "and reflected of the actual demographics of society.",
    "start": "388023",
    "end": "391430"
  },
  {
    "text": "Unlike, say, a training data set filled with only male resumes,",
    "start": "391700",
    "end": "396110"
  },
  {
    "text": "but good representative data is just the start.",
    "start": "396770",
    "end": "400789"
  },
  {
    "text": "There should also be a system for ongoing bias detection",
    "start": "401300",
    "end": "406573"
  },
  {
    "text": "and that can detect and correct potential biases before they create problems.",
    "start": "406574",
    "end": "411470"
  },
  {
    "text": "Now, that could be through initiatives like impact assessments, algorithmic auditing and causation tests.",
    "start": "411980",
    "end": "418369"
  },
  {
    "text": "Remember sharks and ice cream?",
    "start": "418700",
    "end": "421190"
  },
  {
    "text": "Now, this is where human in the loop processes can help,",
    "start": "421790",
    "end": "425783"
  },
  {
    "text": "where recommendations be reviewed by humans before a decision is made final.",
    "start": "425783",
    "end": "431509"
  },
  {
    "text": "Now the outputs of AI algorithms can often be something of a black box, making it difficult to understand their outcomes.",
    "start": "432440",
    "end": "440060"
  },
  {
    "text": "So transparent AI those systems document and do their best to explain the underlying algorithms methodology.",
    "start": "440480",
    "end": "449870"
  },
  {
    "text": "Now, to be clear, this is still an emerging field,",
    "start": "450230",
    "end": "453783"
  },
  {
    "text": "but advances are being made in AI interpretability,",
    "start": "453783",
    "end": "457944"
  },
  {
    "text": "which goes some way to explaining how algorithms arrive at their outcomes.",
    "start": "457945",
    "end": "463189"
  },
  {
    "text": "And then finally, we have inclusive AI, which means developing AI systems",
    "start": "463490",
    "end": "469695"
  },
  {
    "text": "where the developers, where the data scientists, where the machine learning engineers are varied racially,",
    "start": "469696",
    "end": "476628"
  },
  {
    "text": "economically, by education level, by gender, by job description and all sorts of other demographic metrics",
    "start": "476628",
    "end": "483138"
  },
  {
    "text": "This will bring different perspectives to help identify and mitigate biases that might otherwise go unnoticed.",
    "start": "483680",
    "end": "490669"
  },
  {
    "text": "The fact is that algorithmic bias has many causes, and as AI becomes more prevalent in decision making,",
    "start": "491180",
    "end": "500285"
  },
  {
    "text": "the importance of detecting and mitigating these biases only grows.",
    "start": "500285",
    "end": "505669"
  },
  {
    "text": "If you have any questions, please drop us a line below.",
    "start": "507780",
    "end": "510389"
  },
  {
    "text": "And if you want to see more videos like this in the future, please like and subscribe.",
    "start": "510420",
    "end": "515009"
  },
  {
    "text": "Thanks for watching.",
    "start": "515640",
    "end": "516640"
  }
]