[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "text": "Jeff, I have a question for you, and that is, can I really think?",
    "start": "60",
    "end": "4740"
  },
  {
    "text": "Well, Martin, let me answer your question with a math problem.",
    "start": "5370",
    "end": "8909"
  },
  {
    "text": "Exactly the response I would expect from an IBM Distinguished Engineer.",
    "start": "9420",
    "end": "13139"
  },
  {
    "text": "Go on.",
    "start": "13440",
    "end": "13950"
  },
  {
    "text": "I wouldn't want to disappoint.",
    "start": "13950",
    "end": "15240"
  },
  {
    "start": "15000",
    "end": "127000"
  },
  {
    "text": "So here's your math problem Martin: Oliver picks 44 kiwis on Friday,",
    "start": "15540",
    "end": "19770"
  },
  {
    "text": "Take notes.",
    "start": "19780",
    "end": "20670"
  },
  {
    "text": "Okay, 44.",
    "start": "20670",
    "end": "21720"
  },
  {
    "text": "Then he picks 58 kiwis on Saturday.",
    "start": "21930",
    "end": "24480"
  },
  {
    "text": "All right, 58.",
    "start": "24960",
    "end": "26244"
  },
  {
    "text": "On Sunday, he picks double the number that he picked on Friday,",
    "start": "26244",
    "end": "29820"
  },
  {
    "text": "But five of those are smaller.",
    "start": "30240",
    "end": "32279"
  },
  {
    "text": "Doesn't matter, 88.",
    "start": "33390",
    "end": "35390"
  },
  {
    "text": "So what's the total?",
    "start": "36080",
    "end": "37080"
  },
  {
    "text": "Well, that adds up to 190.",
    "start": "37520",
    "end": "41629"
  },
  {
    "text": "Well, Martin, according to my AI chat bot, you forgot to subtract",
    "start": "41960",
    "end": "47298"
  },
  {
    "text": "the fact that I told you five of them were smaller, so it should have been 185.",
    "start": "47298",
    "end": "52310"
  },
  {
    "text": "Smaller or not, they still count.",
    "start": "53030",
    "end": "55550"
  },
  {
    "text": "I think you need a new chat bot.",
    "start": "55970",
    "end": "57529"
  },
  {
    "text": "Okay, I agree.",
    "start": "58010",
    "end": "59090"
  },
  {
    "text": "You know, the fact that five of these were smaller doesn't really change the total.",
    "start": "59390",
    "end": "63468"
  },
  {
    "text": "But a research paper recently turned up that some LLMs got tripped up by these kinds of extraneous details.",
    "start": "64010",
    "end": "71420"
  },
  {
    "text": "How could an AI that seems so smart make such an obvious mistake?",
    "start": "71870",
    "end": "76370"
  },
  {
    "text": "Yeah, I've seen that paper,",
    "start": "76640",
    "end": "78109"
  },
  {
    "text": "and it all comes down to training data.",
    "start": "78110",
    "end": "80690"
  },
  {
    "text": "The paper proposes the LLMs perform something called probabilistic pattern matching.",
    "start": "80810",
    "end": "86629"
  },
  {
    "text": "I'll take notes on this.",
    "start": "86840",
    "end": "87889"
  },
  {
    "text": "Thank you.",
    "start": "88160",
    "end": "88790"
  },
  {
    "text": "And that means they search to find the closest data in the training dataset that matches the data.",
    "start": "88790",
    "end": "94519"
  },
  {
    "text": "They're looking for similar examples of, well, in this case, math problems.",
    "start": "94520",
    "end": "98060"
  },
  {
    "text": "And most of the time, when little details like five were smaller than average,",
    "start": "98600",
    "end": "103241"
  },
  {
    "text": "when that appears in a math problem, there's almost always a reason for that, right?",
    "start": "103241",
    "end": "107029"
  },
  {
    "text": "Almost every time a caveat like that was added in those math problems, in the training data,",
    "start": "107570",
    "end": "111910"
  },
  {
    "text": "the answer required taking that caveat into consideration.",
    "start": "111910",
    "end": "115489"
  },
  {
    "text": "Hence the LLM incorrectly electing to subtract five from the total",
    "start": "115850",
    "end": "121255"
  },
  {
    "text": "because that was the probabilistic pattern seen in most of the relevant training examples.",
    "start": "121255",
    "end": "126618"
  },
  {
    "start": "127000",
    "end": "290000"
  },
  {
    "text": "So, Martin, That brings us back to the broader question of Can I Really Think.",
    "start": "127130",
    "end": "131629"
  },
  {
    "text": "That is a good question.",
    "start": "131720",
    "end": "133430"
  },
  {
    "text": "Or is it really just simulating or imitating thought and reasoning",
    "start": "133730",
    "end": "138034"
  },
  {
    "text": "or, in fact, the broader question of are we all living in a simulation?",
    "start": "138035",
    "end": "142810"
  },
  {
    "text": "Is everything imitation?",
    "start": "142820",
    "end": "144020"
  },
  {
    "text": "Is anything actually real?",
    "start": "144020",
    "end": "145699"
  },
  {
    "text": "A slow down there, Jeff.",
    "start": "146250",
    "end": "147440"
  },
  {
    "text": "I think you're beginning to hallucinate like a chatbot,",
    "start": "147440",
    "end": "150879"
  },
  {
    "text": "but yes, this does imply that LLM pattern matching is coming at the expense of actual reasoning.",
    "start": "150910",
    "end": "158299"
  },
  {
    "text": "LLMs often come to the right answer without really having a proper understanding of the underlying concepts",
    "start": "158310",
    "end": "163610"
  },
  {
    "text": "which can lead to all sorts of issues like these.",
    "start": "163610",
    "end": "166069"
  },
  {
    "text": "So we saw how the extraneous details in the math problem I gave you were able to throw off in LLM,",
    "start": "166430",
    "end": "172571"
  },
  {
    "text": "but what else could cause models to struggle with reasoning like this?",
    "start": "172571",
    "end": "176240"
  },
  {
    "text": "Yeah LLMs struggle with logical reasoning because there's something called token bias.",
    "start": "176570",
    "end": "181700"
  },
  {
    "text": "I'll take a note on that.",
    "start": "181730",
    "end": "182730"
  },
  {
    "text": "Thanks.",
    "start": "183050",
    "end": "183650"
  },
  {
    "text": "Now, remember, these systems are effectively predicting the next word, or more accurately, the next token in a sequence,",
    "start": "183650",
    "end": "190711"
  },
  {
    "text": "and the reasoning output of the model changes when a single token of input changes,",
    "start": "190712",
    "end": "196298"
  },
  {
    "text": "which means that tiny little tweaks in how you prompt an LLM with a question",
    "start": "196298",
    "end": "200971"
  },
  {
    "text": "can have an outsized effect on the reasoning presented in the output.",
    "start": "200971",
    "end": "206269"
  },
  {
    "text": "So I suppose this is a bit like autocomplete on steroids.",
    "start": "206450",
    "end": "209300"
  },
  {
    "text": "If I say Mary had a little lamb, its fleece was white, as.",
    "start": "209690",
    "end": "213830"
  },
  {
    "text": "I'm going to say the autocomplete on that is snow, Jeff.",
    "start": "214550",
    "end": "217587"
  },
  {
    "text": "Well, no.",
    "start": "217665",
    "end": "218719"
  },
  {
    "text": "According to the autocomplete on my phone, it's Mary had a little lamb,",
    "start": "218720",
    "end": "223219"
  },
  {
    "text": "Its fleece was white as a little lamb, Yeah.",
    "start": "223220",
    "end": "228439"
  },
  {
    "text": "Kind of unimpressive.",
    "start": "228440",
    "end": "229459"
  },
  {
    "text": "Well, what would cause such a thing?",
    "start": "230570",
    "end": "232580"
  },
  {
    "text": "Well, the autocomplete uses a prediction scheme based on probabilities as to what the next word would be,",
    "start": "233090",
    "end": "238698"
  },
  {
    "text": "and LLMs do something similar as well, albeit with some additional smarts like attention.",
    "start": "238700",
    "end": "243080"
  },
  {
    "text": "And most of the time it's right,",
    "start": "243320",
    "end": "244639"
  },
  {
    "text": "but when it isn't, we get hallucinations and then, yeah",
    "start": "244820",
    "end": "248106"
  },
  {
    "text": "get weird extraneous details and stuff like this that you and I, we would quickly filter it out.",
    "start": "248106",
    "end": "253282"
  },
  {
    "text": "A chatbot that appears to be reasoning may actually just be doing a super sophisticated autocomplete where it's",
    "start": "253282",
    "end": "259850"
  },
  {
    "text": "guessing not only the next word, but also the next sentence, the next paragraph or even the entire document.",
    "start": "259850",
    "end": "265669"
  },
  {
    "text": "What a buzzkill, Martin.",
    "start": "266060",
    "end": "267139"
  },
  {
    "text": "I mean, we you just ruin the magic for all of us.",
    "start": "267500",
    "end": "271369"
  },
  {
    "text": "Thanks for that.",
    "start": "271430",
    "end": "272430"
  },
  {
    "text": "It's sort of like if I tell you when you see a magician saw a lady in a box in half and then I tell you, in fact,",
    "start": "273450",
    "end": "280193"
  },
  {
    "text": "there are two ladies, and you're just seeing the arms of one and the legs of the other.",
    "start": "280193",
    "end": "284029"
  },
  {
    "text": "Poof.",
    "start": "284390",
    "end": "285080"
  },
  {
    "text": "No more magic.",
    "start": "285080",
    "end": "286080"
  },
  {
    "text": "As with all things AI reasoning is evolving.",
    "start": "286430",
    "end": "289639"
  },
  {
    "text": "Look, we can smugly proclaim that I just doesn't understand concepts the way we superior humans do,",
    "start": "289640",
    "end": "296149"
  },
  {
    "start": "290000",
    "end": "430000"
  },
  {
    "text": "but some recent advancements are seeing big improvements in reasoning.",
    "start": "296450",
    "end": "300589"
  },
  {
    "text": "Most of Pre-training models today rely on something called training time compute.",
    "start": "300950",
    "end": "305990"
  },
  {
    "text": "Here, I'll take some notes.",
    "start": "306290",
    "end": "307370"
  },
  {
    "text": "No, thank you very much.",
    "start": "307370",
    "end": "308779"
  },
  {
    "text": "Now the models learned to reason,",
    "start": "309350",
    "end": "311300"
  },
  {
    "text": "or as we've seen, actually, they learned to perform probabilistic pattern matching during model training.",
    "start": "311570",
    "end": "318050"
  },
  {
    "text": "Then the model is released and now it's a fixed entity.",
    "start": "318410",
    "end": "322129"
  },
  {
    "text": "So the underlying model here, it doesn't change.",
    "start": "322160",
    "end": "324679"
  },
  {
    "text": "Now, remember, we talked about token bias, how small changes",
    "start": "325160",
    "end": "328990"
  },
  {
    "text": "in the input tokens, meaning your prompt can affect the reasoning in the output.",
    "start": "328990",
    "end": "332578"
  },
  {
    "text": "Well, that can actually be a good thing as we improve LLM reasoning through some prompt engineering techniques.",
    "start": "333260",
    "end": "339290"
  },
  {
    "text": "For example, a number of papers have shown significant LLM reasoning",
    "start": "339710",
    "end": "343185"
  },
  {
    "text": "improvements through something called chain of thought prompting.",
    "start": "343185",
    "end": "346610"
  },
  {
    "text": "Right. I've heard about that.",
    "start": "346850",
    "end": "348210"
  },
  {
    "text": "That's where you append things like things step by step to the prompt,",
    "start": "348410",
    "end": "352517"
  },
  {
    "text": "and that encourages the all of them to include reasoning steps before coming up with an answer.",
    "start": "352517",
    "end": "357139"
  },
  {
    "text": "Exactly right,",
    "start": "357260",
    "end": "358040"
  },
  {
    "text": "but the emphasis is on the person writing the prompts to use the right magic words,",
    "start": "358040",
    "end": "363288"
  },
  {
    "text": "the right incantations to get the LLM to adopt a chain of thought process.",
    "start": "363288",
    "end": "368959"
  },
  {
    "text": "What new models are doing is inference time.",
    "start": "369170",
    "end": "373050"
  },
  {
    "text": "Compute. It effectively tells the model to spend some time thinking before giving you an answer.",
    "start": "373070",
    "end": "379369"
  },
  {
    "text": "The amount of time it spends thinking is variable based on how much reasoning it needs to do.",
    "start": "379430",
    "end": "384380"
  },
  {
    "text": "A simple request might take a second or two.",
    "start": "384620",
    "end": "386810"
  },
  {
    "text": "Something longer might take several minutes only when it's completed its chain of thought",
    "start": "386810",
    "end": "392060"
  },
  {
    "text": "thinking period does it then start outputting an answer.",
    "start": "392060",
    "end": "395300"
  },
  {
    "text": "Basically, think before you speak.",
    "start": "395510",
    "end": "397009"
  },
  {
    "text": "Indeed,",
    "start": "397170",
    "end": "397850"
  },
  {
    "text": "and what makes inference time compute models interesting",
    "start": "397850",
    "end": "401645"
  },
  {
    "text": "is the inference reasoning is something that can be tuned",
    "start": "401645",
    "end": "404874"
  },
  {
    "text": "and improved without having to train and tweak the underlying model.",
    "start": "404874",
    "end": "409337"
  },
  {
    "text": "So there are now two places in the development of an LLM where reason can be improved,",
    "start": "409670",
    "end": "414952"
  },
  {
    "text": "at training time with better quality training data and that inference time with better chain of thought training.",
    "start": "414952",
    "end": "422809"
  },
  {
    "text": "Researchers at some of the AI labs are confident we'll see big",
    "start": "423230",
    "end": "426122"
  },
  {
    "text": "improvements in the reasoning of future LLM models because of this.",
    "start": "426122",
    "end": "430010"
  },
  {
    "text": "So, Martin, maybe we can finally get an AI that can actually count kiwis.",
    "start": "430460",
    "end": "435410"
  },
  {
    "text": "And that would be a glorious day,",
    "start": "435560",
    "end": "437060"
  },
  {
    "text": "but will it actually be thinking or just simulating thought a bunch of algorithms all running together?",
    "start": "437690",
    "end": "444619"
  },
  {
    "text": "I mean, after all, it's just a bunch of electrical circuits",
    "start": "444650",
    "end": "447764"
  },
  {
    "text": "and impulses running through those circuits at the end of the day, right?",
    "start": "447764",
    "end": "451130"
  },
  {
    "text": "Well, that's true.",
    "start": "451220",
    "end": "452269"
  },
  {
    "text": "But then so your thoughts are just a bunch of neurons firing electrical impulses in your brain,",
    "start": "452270",
    "end": "458089"
  },
  {
    "text": "and because we don't fully understand it, it seems almost magical.",
    "start": "458090",
    "end": "462949"
  },
  {
    "text": "Well, until I tell you how the magic trick works, which ruins the magic.",
    "start": "463340",
    "end": "466850"
  },
  {
    "text": "Sort of the way we think about AI as just tending to think and simulating thought",
    "start": "466850",
    "end": "473034"
  },
  {
    "text": "through using a bunch of algorithms that that's how the trick works.",
    "start": "473034",
    "end": "477828"
  },
  {
    "text": "But once you know how the trick works, then we have the question, is it really thought after all.",
    "start": "478010",
    "end": "483080"
  },
  {
    "text": "Is it really thought after all?",
    "start": "483440",
    "end": "484670"
  },
  {
    "text": "Jeff, you are asking a question for a philosopher, which I am not.",
    "start": "484710",
    "end": "488660"
  },
  {
    "text": "So I ask the next best thing, a popular chatbot, and we actually really like the response it came back with.",
    "start": "489200",
    "end": "494360"
  },
  {
    "text": "So I asked it what is the difference between thinking and a simulation?",
    "start": "494420",
    "end": "498649"
  },
  {
    "text": "And it said that thinking involves conscious, goal driven, subjective understanding and adaptability,",
    "start": "498860",
    "end": "507836"
  },
  {
    "text": "a simulation of thinking, like a language model,that creates the appearance of thinking,",
    "start": "507836",
    "end": "513682"
  },
  {
    "text": "by generating responses that fit patterns of real thought and language use, but",
    "start": "513682",
    "end": "517387"
  },
  {
    "text": "without actual awareness, without actual comprehension, or without actual purpose.",
    "start": "517387",
    "end": "523730"
  },
  {
    "text": "Actually sounds like a pretty good answer from a system that says it can't actually think.",
    "start": "524510",
    "end": "528470"
  }
]