[
  {
    "start": "0",
    "end": "27000"
  },
  {
    "text": "Will you be paying 200 a month for o1 Pro? Marina Danilevsky is a\nSenior Research Scientist. Marina, welcome to the show.",
    "start": "40",
    "end": "6080"
  },
  {
    "text": "Uh, will you? No, I will not. Vyoma Gajjar is an AI\nTechnical Solutions Architect.",
    "start": "6120",
    "end": "11059"
  },
  {
    "text": "Uh, Vyoma, welcome back. Uh, are you subscribing? Yes, shockingly. And last but not least is Kate\nSoule, Director of Technical Product",
    "start": "11270",
    "end": "18199"
  },
  {
    "text": "Management on the Granite team. Kate, welcome back. Will you be subscribing? Absolutely not. Okay, all that and more on\ntoday's Mixture of Experts.",
    "start": "18199",
    "end": "25845"
  },
  {
    "start": "27000",
    "end": "875000"
  },
  {
    "text": "I'm Tim Huang, and welcome\nto Mixture of Experts. Each week, MoE is dedicated to bringing\nthe top quality banter you need",
    "start": "31095",
    "end": "37395"
  },
  {
    "text": "to make sense of the ever evolving\nlandscape of artificial intelligence. Today, in addition to having the\nbest panel, don't tell the other",
    "start": "37395",
    "end": "43724"
  },
  {
    "text": "panelists, Kate, Vyoma, Marina, very\nexcited to have you on the show. We're going to talk about the latest\nhot trends coming out of NeurIPS,",
    "start": "43724",
    "end": "49945"
  },
  {
    "text": "designing evaluations for AGI,\nand the release of LLAMA 3.3 70B.",
    "start": "50105",
    "end": "54843"
  },
  {
    "text": "But first, we have to talk about what\nSam Altman's been cooking at OpenAI. If you've been catching the\nnews, OpenAI released, uh, uh, an",
    "start": "55265",
    "end": "61790"
  },
  {
    "text": "announcement that for the next 12 days,\nthey'll be making a product release announcement every day, um, to kind\nof celebrate the end of the year.",
    "start": "61790",
    "end": "69210"
  },
  {
    "text": "And there's already been a number of\ninteresting announcements, not least of which is the release of a new 200\na month tier for o1 Pro, which is",
    "start": "69550",
    "end": "79200"
  },
  {
    "text": "their kind of creme de la creme of\nmodels that they are making available. Um, suffice to say 200 a month\nis a lot of money, much more than",
    "start": "79230",
    "end": "86979"
  },
  {
    "text": "companies who have been providing\nthese services have charged before. And so I really wanted to kind of\njust start there because I think",
    "start": "87184",
    "end": "92445"
  },
  {
    "text": "it's such an intriguing thing. Um, Vyoma, I think you were the standout,\nyou said that you would subscribe.",
    "start": "92445",
    "end": "97505"
  },
  {
    "text": "So I want to hear that argument\nand then we'll get to Kate and Marina's unwarranted skepticism. Sure.",
    "start": "97505",
    "end": "102920"
  },
  {
    "text": "I feel OpenAI's strategy here is\nto increase more adoption, and that is something that they have\nbeen speaking continuously about.",
    "start": "103190",
    "end": "111210"
  },
  {
    "text": "Sam has been speaking continuously\nabout in a multiple, uh, conferences and talks that he's been giving.",
    "start": "111260",
    "end": "116520"
  },
  {
    "text": "He said that he wants to reach\nalmost like 1 billion users by 2025. And the whole aim behind using and coming\nup with the 01 Pro with like $200 is to...",
    "start": "116720",
    "end": "126960"
  },
  {
    "text": "try to get like AI developers, who is the\nmajority of the market trying to build these applications, to start using it.",
    "start": "127580",
    "end": "133609"
  },
  {
    "text": "Some of the key features\nthat he says are like reduced latency during like peak hours. It gives you like higher speed\nto implement some of these",
    "start": "133829",
    "end": "142609"
  },
  {
    "text": "models and use cases as well. And it'll be surprising that I was\nreading about it on X and ChatGPT,",
    "start": "142630",
    "end": "149176"
  },
  {
    "text": "et cetera, takes like almost 30 times\nmore money, it's more expensive to run.",
    "start": "149200",
    "end": "153940"
  },
  {
    "text": "Um, so if you look at it from a\nperspective as a daily software engineer, developer, engineer,\nweb developer, it, it, it seems",
    "start": "154350",
    "end": "163399"
  },
  {
    "text": "to be a steal for those people. And yeah, that's, that's why\nI feel that I would pay it.",
    "start": "163399",
    "end": "169549"
  },
  {
    "text": "That's great. Yeah. All right. Maybe Kate, I'll turn to you\nbecause I think your response was no hesitation, absolutely not.",
    "start": "169639",
    "end": "175239"
  },
  {
    "text": "Um, what's the argument, I guess,\nfor, for not wanting to pay? Because I mean, it sounds like\nthey're, they're like, here, get",
    "start": "175659",
    "end": "181510"
  },
  {
    "text": "access to one of the most powerful\nartificial intelligences in the world. And, you know, it's money, but, you\nknow, I guess what they're trying to",
    "start": "181510",
    "end": "188520"
  },
  {
    "text": "encourage is for us to think about\nthis as if it were a luxury product. I think my biggest, uh, umbrage at\nthe price tag is, you know, I can",
    "start": "188520",
    "end": "200490"
  },
  {
    "text": "see use cases for a one and having\na powerful model in your arsenal and",
    "start": "200490",
    "end": "205600"
  },
  {
    "text": "your disposal, but I don't want to\nrun that model for every single task and there's still a lot out there.",
    "start": "205600",
    "end": "211189"
  },
  {
    "text": "So trying to then have unlimited access\nfor a really high cost on a monthly basis",
    "start": "211240",
    "end": "217439"
  },
  {
    "text": "just doesn't quite make sense for the\nusage patterns that I use these models for and that I, that I see out in the\nworld, like, I want to be able to hit",
    "start": "217479",
    "end": "224200"
  },
  {
    "text": "that model when I need to on the outlying\ncases where I really need that power. The rest of the time, I don't\nwant to pay for that cost.",
    "start": "224200",
    "end": "231029"
  },
  {
    "text": "Why would I carry that with this\nreally high price tag month to month? Yeah, I was gonna say, I mean, I think\none of the funny things about this is the",
    "start": "231369",
    "end": "237609"
  },
  {
    "text": "prospect of paying $200 a month and then\nbeing like, I need help writing my email. Yeah. Like, it's kind of like a very\nsilly sort of thing to think about.",
    "start": "237609",
    "end": "246000"
  },
  {
    "text": "Um, I guess I have to ask you\nthis cause you work on Granite. Open source, right? I assume one of the arguments is\njust that open source is better.",
    "start": "246000",
    "end": "252868"
  },
  {
    "text": "Getting better and is free. I don't know if you would say that\nthat actually like is one reason why you're more skeptical here.",
    "start": "253149",
    "end": "258469"
  },
  {
    "text": "I mean, I think that's certainly a reason\nhow long you know do I want to pay to have that early access or am I willing to\nwait a couple of months and see what new",
    "start": "258519",
    "end": "266669"
  },
  {
    "text": "open source models come out that start\nto, you know, tear away at the performance",
    "start": "266669",
    "end": "271810"
  },
  {
    "text": "margins that o1's been able to gain. I don't have a need to have that today,\nand I'm willing to wait and to continue",
    "start": "271810",
    "end": "278179"
  },
  {
    "text": "working on the open source side of things\nas the field continues to play catch up. You know, I think with every release we're\nseeing of proprietary models, it takes",
    "start": "278180",
    "end": "285709"
  },
  {
    "text": "less and less time for an open source\nmodel to be released that can start to match and be equitable in that capability.",
    "start": "285709",
    "end": "292960"
  },
  {
    "text": "Yeah, it feels like they've\nreally kind of gotten out on a little bit of a limb here, right? I didn't even think about it until you\nmentioned it, is like, once you've gotten",
    "start": "293040",
    "end": "299065"
  },
  {
    "text": "all these people paying $200 a month,\nit will feel really bad to now say, hey, these capabilities are now available\nfor 50 bucks a month all of a sudden.",
    "start": "299065",
    "end": "307335"
  },
  {
    "text": "I think there's some, you\nknow, market testing, right? They need to see how\nfar they can push this. That's, that's a reasonable thing\nfor businesses to do, but I, it's",
    "start": "307555",
    "end": "315285"
  },
  {
    "text": "past my, uh, my, my, my my taste. It's a little too fine. Yeah, for sure.",
    "start": "315315",
    "end": "320005"
  },
  {
    "text": "Um, I guess Marina, maybe I'll toss\nit to you as kind of the last person. I know you were also a skeptic being like,\nno, I don't, I don't really think so.",
    "start": "320985",
    "end": "327735"
  },
  {
    "text": "Um, maybe one twist to the question I'll\nask you is, uh, you know, when I was working on chatbots back in the day, we\noften thought like, oh, what we're doing",
    "start": "327765",
    "end": "335494"
  },
  {
    "text": "is we're competing with like Netflix. So we can't really charge on more\non a monthly basis than someone would pay for a Netflix because\nit's like entertainment, basically.",
    "start": "335494",
    "end": "343190"
  },
  {
    "text": "Um, and I guess, I don't know,\nmaybe the question is someone who's kind of skeptical of $200,\nhow much would you pay, right?",
    "start": "343670",
    "end": "349970"
  },
  {
    "text": "Like, is an AI model worth\n$100 a month or $50 a month? I guess, how do you think\na little bit about that?",
    "start": "349970",
    "end": "355129"
  },
  {
    "text": "I think that's about what a lot\nof them are charging, right? OpenAI's got a lovely $20 a month\nuh, tier, so does Anthropic, uh,",
    "start": "356030",
    "end": "364995"
  },
  {
    "text": "Midjourney has something like that. So honestly, I think the market\nhas said that if you're going to be doing something consistent, that's\na kind of a reasonable amount of",
    "start": "364995",
    "end": "372204"
  },
  {
    "text": "money, somewhere in that 20 to 50. The 200 seems like a bit of a play\nfrom Steve Jobs of do you really,",
    "start": "372204",
    "end": "378285"
  },
  {
    "text": "really want to be an early adopter? Okay, you get to say, ha ha,\nI'm playing with the real model.",
    "start": "378285",
    "end": "382975"
  },
  {
    "text": "Realistically though, I agree with Kate. I think most people don't know how to\ncreate sophisticated enough use cases",
    "start": "383305",
    "end": "389555"
  },
  {
    "text": "to warrant the use of that much of a\nnuclear bomb, and you don't even know why you're spending the money that you are.",
    "start": "389845",
    "end": "396895"
  },
  {
    "text": "So you can actually get pretty far in\nfiguring out how to make use of all of these models that are coming out and\ncoming out quickly in the lower tier.",
    "start": "396905",
    "end": "404885"
  },
  {
    "text": "I mean, if again, if I was in charge of\nthe finances, I'd say give me a reason why this is a 10x quality increase.",
    "start": "404894",
    "end": "411484"
  },
  {
    "text": "And I don't see why it's a 10x\nquality increase when you don't have a 10x better understanding\nof how to actually make use of it.",
    "start": "412059",
    "end": "418419"
  },
  {
    "text": "Um, so I'm, I'm on Kate's side. I think part of this, and I think\nthe comparison to Apple is quite apt",
    "start": "418850",
    "end": "423860"
  },
  {
    "text": "in some ways is, um, you know, like\nApple has turned out not necessarily be the most popular phone, but the\nmost profitable the phone, right?",
    "start": "423860",
    "end": "431330"
  },
  {
    "text": "And it actually just turns out that a lot\nof people do really want to pay premium. I guess maybe what we're learning\nis like, does that actually also",
    "start": "431360",
    "end": "436940"
  },
  {
    "text": "apply for AI, because I think, you\nknow, it's hard to imagine other things that you pay $200 a month for.",
    "start": "436940",
    "end": "443220"
  },
  {
    "text": "It's like getting like to like commuting\nexpenses, utilities, like you pay that much for your internet bill,\nI guess, you know, in some cases.",
    "start": "443770",
    "end": "449620"
  },
  {
    "text": "So yeah, I think we're about to find\nout whether or not like people are going to bid up in that particular way.",
    "start": "449630",
    "end": "454669"
  },
  {
    "text": "I guess Vyoma maybe I'll turn it back\nto you, I mean, with all this criticism, still sticking with it, though. Yeah, I'm telling you, I feel the market\nniche that the OpenAI wants to stick to",
    "start": "454929",
    "end": "464094"
  },
  {
    "text": "is getting people to utilize these APIs,\num, for purposes in the case that they",
    "start": "464144",
    "end": "470735"
  },
  {
    "text": "want to build a small application, like,\nuh, they have a black box environment as well, where they can build, uh, something\non their own, get it out quick and dirty.",
    "start": "470735",
    "end": "478945"
  },
  {
    "text": "Experimentation is much more easier. And let's be honest, OpenAI\nhas the first mover advantage.",
    "start": "478945",
    "end": "484284"
  },
  {
    "text": "So everyone, like majority of\nthe people, know ChatGPT as the go-to thing for generative AI.",
    "start": "484284",
    "end": "490250"
  },
  {
    "text": "So they are leeching it and I\ncompletely see them, um, doing some of the, these, uh, marketing\nstrategies around the money, et cetera.",
    "start": "490610",
    "end": "499120"
  },
  {
    "text": "I feel they are monetizing on it now and,\nThat's one of the key reasons they might be getting some push from investors.",
    "start": "499339",
    "end": "505570"
  },
  {
    "text": "I don't know, but that's somehow I feel\nthe strategy that startups do follow.",
    "start": "505659",
    "end": "510860"
  },
  {
    "text": "And that's what everything\nis doing to as well. Yeah, for sure. The other announcement I kind of quickly\nwanted to touch on was, uh, OpenAI had",
    "start": "511000",
    "end": "518758"
  },
  {
    "text": "been hyping, uh, Sora, which is their\nkind of video generation, um, model.",
    "start": "518760",
    "end": "524070"
  },
  {
    "text": "Um, and it's now finally\nkind of widely available. Um, and I think this is a little bit\ninteresting just because you know, this",
    "start": "524350",
    "end": "530649"
  },
  {
    "text": "is almost like a very different kind of\nservice that they're supporting, right? Like they came up with language models. Now they kind of want to go multimodal.",
    "start": "530650",
    "end": "536830"
  },
  {
    "text": "They want to get into video, you know,\nin part to kind of compete with all the people that are doing generative\nAI on the image and video side.",
    "start": "536840",
    "end": "544178"
  },
  {
    "text": "And I guess I'm curious if the\npanel has any, any thoughts on this. Um, Kate, maybe I'll throw it to\nyou is like, it kind of feels like",
    "start": "544189",
    "end": "550120"
  },
  {
    "text": "this is like a pretty new front for\nOpenAI to try to go compete in from a technological standpoint, right?",
    "start": "550430",
    "end": "555645"
  },
  {
    "text": "Like I think like, this is like\na pretty different set of tools and teams and infrastructure.",
    "start": "555645",
    "end": "560845"
  },
  {
    "text": "I guess kind of like, do you think\nultimately this is sort of like a smart move on the part of OpenAI? Because it does feel like they're kind\nof like stretching themselves kind of in",
    "start": "561085",
    "end": "568145"
  },
  {
    "text": "every direction to try to compete on every\nsingle front in the generative AI market. I mean, I think it does make sense\nunder the broader vision, or OpenAI's",
    "start": "568145",
    "end": "578024"
  },
  {
    "text": "broader vision of pursuing AGI. I mean, I think you're going to\nneed to be able to have better,",
    "start": "578045",
    "end": "583475"
  },
  {
    "text": "uh, video understanding and\ngeneration capabilities to kind of handle this more multimodal task.",
    "start": "583905",
    "end": "590815"
  },
  {
    "text": "And we're starting to see models\nbeing able, one single model being able to handle multiple different\nmodalities and capabilities.",
    "start": "590815",
    "end": "597425"
  },
  {
    "text": "So you need to develop models that\ncan handle that right before you can start to merge it all together.",
    "start": "597435",
    "end": "601945"
  },
  {
    "text": "So I think under that broader pursuit and\numbrella, it does make sense to try and develop those technologies and techniques.",
    "start": "602445",
    "end": "609705"
  },
  {
    "text": "Yeah, I think it's kind of\nlike, well, we'll have to see. I mean, I think again, like part of\nthe goal is just like whether or not AGI itself is is the right bid, um, to\nkind of take on this market, um, and,",
    "start": "609855",
    "end": "620270"
  },
  {
    "text": "and whether or not this market really\nwill be kind of like one company to rule them all, if it will be like, you\nknow, he's the winner on video, and",
    "start": "620280",
    "end": "625900"
  },
  {
    "text": "you have the winner on text, and it'll\nkind of break down in a multimodal way. I mean, I'm really skeptical that\nthere's like the right economic",
    "start": "625900",
    "end": "631800"
  },
  {
    "text": "incentive to develop AGI in the way\nthat a lot of people are pursuing it. So we'll, we'll see, you know,\nbut if that's your broader vision,",
    "start": "631800",
    "end": "638760"
  },
  {
    "text": "I don't think you can have a\nlanguage-only model for AGI. Right? It needs to have better,\ndifferent domain understanding.",
    "start": "638760",
    "end": "645635"
  },
  {
    "text": "Um, how about this announcement? I mean, Vyoma, Marina, are you more\nexcited about this than having the prospect of having to pay, you know,\nyour, your internet's bills worth",
    "start": "645755",
    "end": "653735"
  },
  {
    "text": "each month for a language model? Yeah. Uh, I feel like the Sora announcement that\nwe saw, and I was going through the videos",
    "start": "653735",
    "end": "660204"
  },
  {
    "text": "and I was actually playing through it. The way that they've created, if you\nlook at the UI, it looks very, very",
    "start": "660204",
    "end": "665375"
  },
  {
    "text": "similar to your iCloud photos UI. Again, they're trying to drive more and\nmore people to, um, use it seamlessly",
    "start": "665375",
    "end": "672225"
  },
  {
    "text": "and also, uh, it, it creates an, um,\nera of creativity, like people are going",
    "start": "672605",
    "end": "679014"
  },
  {
    "text": "over there playing a little bit with\ntheir prompts, increases the nuances around prompt engineering as well.",
    "start": "679015",
    "end": "684944"
  },
  {
    "text": "I saw a lot of that, uh, happening\nwith different, uh, AI developers that I work with day in and day out.",
    "start": "685005",
    "end": "690503"
  },
  {
    "text": "They're like, if I tweak this in\na different manner, uh, will that particular frame in which it is\nbeing developed change, et cetera.",
    "start": "690515",
    "end": "696844"
  },
  {
    "text": "So it's, I feel it's also coming up\nwith a whole different, um, arena of doing much more better prompt\nengineering, prompt tuning as well.",
    "start": "696844",
    "end": "705500"
  },
  {
    "text": "I'll second that in saying that it's\na really good way again to get a better understanding of what this\nspace really is and a lot of data.",
    "start": "705650",
    "end": "713569"
  },
  {
    "text": "This is something that we don't have\nan entire text's worth of internet or internet's worth of text stuff for,\nwhereas here trying to see whether",
    "start": "713949",
    "end": "720650"
  },
  {
    "text": "anecdotally or if people are willing\nto share what they've done, people will get a much better sense of what\ncan these models do and then maybe",
    "start": "720650",
    "end": "727404"
  },
  {
    "text": "economic things will come where you\nhave a true multimodal model that can understand, you know, graphs and charts\nand pictures and videos at the same time.",
    "start": "727454",
    "end": "734694"
  },
  {
    "text": "Um, but this is a good way to get\na lot of data of what comes to people's minds and what they think\nthe technology ought to be useful for.",
    "start": "734755",
    "end": "740654"
  },
  {
    "text": "And that is interesting and it'll\nbe really interesting to see what comes out from, from this capability.",
    "start": "741035",
    "end": "745990"
  },
  {
    "text": "Yeah, I think kind of the model\ndiscovery, like you kind of build the model, but it's sort of interesting\nthat the people who design it are not",
    "start": "746120",
    "end": "752550"
  },
  {
    "text": "necessarily well positioned to know\nwhat it will be used for effectively. That's absolutely true. yeah, and the market's just\nlike, all right, well, let's",
    "start": "752550",
    "end": "758998"
  },
  {
    "text": "just like throw it out there. And then they're kind of just\nsort of like waiting, hoping that something will pop out. That's a great point that Marina\nbrought about that, and I know Kate",
    "start": "758999",
    "end": "766810"
  },
  {
    "text": "also spoke on the same point about AGI. Imagine, like, I just thought about it. All the users are writing their\nprompt creativity onto that",
    "start": "766810",
    "end": "774065"
  },
  {
    "text": "particular, uh, Sora interface. That is data itself. Imagine that data being utilized\nto gauge human creativity and",
    "start": "774075",
    "end": "782615"
  },
  {
    "text": "getting much more closer to AGI, so. And building on that, then also that\nmodel that you've trained can now generate",
    "start": "782615",
    "end": "788294"
  },
  {
    "text": "more synthetic data that you can then,\neven if you don't want an AGI model to be able to generate videos, you still need\nan AGI model that can understand videos.",
    "start": "788294",
    "end": "796370"
  },
  {
    "text": "And for that, you need more training data\nthrough either collecting data that's been generated by, you know, prompts,\ncreating synthetic data from the model",
    "start": "796760",
    "end": "804769"
  },
  {
    "text": "itself, Sora, to create some larger model. So it all, all I think\nis certainly related.",
    "start": "804769",
    "end": "810079"
  },
  {
    "text": "Yeah, for sure. And yeah, there's kind of a cool\npoint there about, I think, like, If we think synthetic data is going\nto be one big component to the",
    "start": "810300",
    "end": "817665"
  },
  {
    "text": "future, um, there's almost like\na first mover advantage, right? Well, yeah, okay, maybe\nit's uncontroversial, right?",
    "start": "817665",
    "end": "823214"
  },
  {
    "text": "But it's kind of just like, well,\nyou actually, if you're the first mover, you can acquire the data that\nhelps you make the synthetic data.",
    "start": "823495",
    "end": "829865"
  },
  {
    "text": "And so there's kind of this\ninteresting dynamic of like who gets there first actually ends up\nhaving a big impact on your ability.",
    "start": "830130",
    "end": "834920"
  },
  {
    "text": "And this is OpenAI's playbook, like\none of the reasons they were able to scale so quickly is they had first\nmover advantage and their terms and",
    "start": "835220",
    "end": "841080"
  },
  {
    "text": "conditions allow them to use every\nsingle prompt that was originally put into the model when it first released.",
    "start": "841080",
    "end": "846390"
  },
  {
    "text": "It wasn't a little bit later until they\nstarted to have more terms to protect the user's privacy with those prompts.",
    "start": "846499",
    "end": "852240"
  },
  {
    "text": "So yeah, definitely a model they can\nrinse and repeat here, so to speak. And now everyone else is caught on\nand is like, Oh, any model you put",
    "start": "852250",
    "end": "859160"
  },
  {
    "text": "out where we can't store the data\nor don't you dare store my data. So OpenAI got in there before people\ncaught up with critical thinking",
    "start": "859160",
    "end": "866100"
  },
  {
    "text": "of, oh, that's what you're doing. Yes. Yeah.",
    "start": "866100",
    "end": "869700"
  },
  {
    "start": "875000",
    "end": "1310000"
  },
  {
    "text": "I'm going to move us on. So this week is the Lollapalooza,\nmaybe that's too old of a reference. The Coachella of machine\nlearning is happening.",
    "start": "875349",
    "end": "882454"
  },
  {
    "text": "This week, uh, NeurIPS, the annual machine\nlearning conference, uh, one of the big ones next to, you know, ICML and ICLR,\num, and, uh, there's a ton of papers",
    "start": "882705",
    "end": "894055"
  },
  {
    "text": "being presented, a ton of awards going\non, a ton of industry action happening at this conference, certainly more than\nwe're going to have time to cover today.",
    "start": "894064",
    "end": "901634"
  },
  {
    "text": "But I do think I did want to take\na little bit of time just because I think it is a big research event and\nwe have a number of folks who are in",
    "start": "902035",
    "end": "907595"
  },
  {
    "text": "the research space, uh, here with us. On the episode. Um, I guess maybe Kate, I know we were\ntalking about before the episode, maybe",
    "start": "907595",
    "end": "913915"
  },
  {
    "text": "I'll kick it to you is, you know, given\nthe many thousands, thousands of papers",
    "start": "913915",
    "end": "919384"
  },
  {
    "text": "circulating around coming out of NeurIPS. Um, I'm curious if there's things that\nhave caught your eye, things you're",
    "start": "919395",
    "end": "924554"
  },
  {
    "text": "like, oh, that's what I'm reading. That's what I'm excited by. Um, what are pointers? Because I think for me personally,\nit's just like overwhelming.",
    "start": "924574",
    "end": "930095"
  },
  {
    "text": "Like you look on Twitter, it's\nlike, this is the big paper that's going to change everything. And then pretty soon you have like more\npapers than you're ever going to read.",
    "start": "930105",
    "end": "935615"
  },
  {
    "text": "So maybe I'll tee you up as I'm curious\nif there's like particular things you point people to take a look at I mean, I think there's some really\nexciting work that our colleagues",
    "start": "935945",
    "end": "944384"
  },
  {
    "text": "at IBM are presenting right now that\nI'm just really, really fascinated by and think has a lot of potential.",
    "start": "944384",
    "end": "949444"
  },
  {
    "text": "So I definitely encourage people to check\nout the paper called Waggle, which is a",
    "start": "949444",
    "end": "954935"
  },
  {
    "text": "paper on on learning that are our own panel expert Nathalie, uh, is representing\ntalking about unlearning in large language",
    "start": "954935",
    "end": "963380"
  },
  {
    "text": "models and they've got a new method there. Uh, there's also a paper called Trans-LoRA\nthat was produced by some of my colleagues",
    "start": "963390",
    "end": "971609"
  },
  {
    "text": "who sit right in, uh, Cambridge, Mass. And I'm really excited by this one\nbecause it's all about how do you",
    "start": "971609",
    "end": "977279"
  },
  {
    "text": "take a LoRA adapter that's been\nfine tuned for a very specific model and represents a bunch of different\ncapabilities and skills that you've added",
    "start": "977280",
    "end": "985139"
  },
  {
    "text": "to this model and you've trained it. And transfer it to a new model that\nit wasn't originally trained for,",
    "start": "985140",
    "end": "990940"
  },
  {
    "text": "because normally LoRA adapters are\npairwise kind of designed for an exact model during their training process.",
    "start": "990980",
    "end": "997509"
  },
  {
    "text": "And so I think that's going to be super\ncritical as we start to look at how do we make generative AI and building\non top of generative AI more modular?",
    "start": "997929",
    "end": "1005629"
  },
  {
    "text": "How do we keep pace with like these,\nbreakneck releases, you know, every month. It seems like we're getting new\nLlama models like Granite we're",
    "start": "1006030",
    "end": "1014525"
  },
  {
    "text": "continuing to release a bunch of\nupdates similarly, And I think that's just where the field is headed. And if we have to fine tune something\nfrom scratch or retrain LoRA from scratch",
    "start": "1014535",
    "end": "1023605"
  },
  {
    "text": "every single time a new model is released\nIt's just going to be unsustainable um, if we want to be able to keep pace.",
    "start": "1023665",
    "end": "1029125"
  },
  {
    "text": "So having more universal type of LoRA's\nthat can better adapt to these new models Um all I think is going to be\na really important Uh, part moving",
    "start": "1029545",
    "end": "1039178"
  },
  {
    "text": "forward to the broader ecosystem. That's great.   So, yeah, we definitely, uh,\nlisteners, you should check those out. Um, Vyoma, Marina, I'm curious if\nthere's other kind of things that",
    "start": "1039179",
    "end": "1047139"
  },
  {
    "text": "caught your eye, papers that you're\nof interest or, or otherwise. So, one of the papers that I was\nlooking into was the understanding the",
    "start": "1047139",
    "end": "1053930"
  },
  {
    "text": "bias in large scale visual data sets. So, we've been working a lot with\nlarge language models, uh, and, uh,",
    "start": "1053930",
    "end": "1060460"
  },
  {
    "text": "uh, data, which is, uh, language data. But here, this was based on of some data\nset or an experiment, which was done in",
    "start": "1060570",
    "end": "1067380"
  },
  {
    "text": "2011, which was called name that data set\nand what I, what they showcased in this",
    "start": "1067380",
    "end": "1072380"
  },
  {
    "text": "entire paper is how you can break down the\nimage by doing certain transformations,",
    "start": "1072390",
    "end": "1078320"
  },
  {
    "text": "such as like semantic segmentation, object\ndetection, finding that boundary and edge detection, and then kind of doing some\nsort of color and frequency transformation",
    "start": "1078330",
    "end": "1087889"
  },
  {
    "text": "on a piece of particular image to break\nit down such that you are able to, uh,",
    "start": "1087890",
    "end": "1093305"
  },
  {
    "text": "ingest that data in such a better manner\nthat a model that is being created on that data is much more accurate and precise.",
    "start": "1093335",
    "end": "1100374"
  },
  {
    "text": "So very, very, um, old techniques\nI might say, but like the order in which they performed it was.",
    "start": "1100695",
    "end": "1107554"
  },
  {
    "text": "Great in a visual use case. I think that was one of the\npapers that really got my eye.",
    "start": "1107920",
    "end": "1112800"
  },
  {
    "text": "I think that's interesting to me lately\nis the increase in structured now not",
    "start": "1113130",
    "end": "1119540"
  },
  {
    "text": "just data but the structured execution\nof language models for various tasks",
    "start": "1119540",
    "end": "1125100"
  },
  {
    "text": "as we continue to get more and more\nmultimodal not even just text, you know, text, image, video, but just\ntext, uh, with functions, with tool",
    "start": "1125120",
    "end": "1134240"
  },
  {
    "text": "calling, with things of that nature. I think we talked about this\non a previous episode as well. There's now some interesting\nwork going forward.",
    "start": "1134240",
    "end": "1140760"
  },
  {
    "text": "Uh, one particular paper I think\nI read recently, uh, SGLang. on how to actually execute the\nlanguage model in what your state",
    "start": "1140809",
    "end": "1149880"
  },
  {
    "text": "is, and how to have it be forking\nand going in different directions. I think that there's a lot to be said here\nabout how to make these models work for",
    "start": "1149880",
    "end": "1157880"
  },
  {
    "text": "you in a way that's not just sequential,\nand not just, oh, chain of thought, first do this, then do this, then do this.",
    "start": "1157880",
    "end": "1163260"
  },
  {
    "text": "No, let's turn it into a proper\nprogramming language and a proper structure with a definition with some\nintrinsic capabilities that the model",
    "start": "1163260",
    "end": "1171189"
  },
  {
    "text": "has besides just text generation. So that happens to be a particular\ntopic that I'm looking at with interest.",
    "start": "1171189",
    "end": "1176485"
  },
  {
    "text": "Yeah, and IBM actually has a\ndemo, I think, on that topic. Yes, it does. So how do we use SG Lang and some\nLoRA adapters, uh, coming back into",
    "start": "1176525",
    "end": "1185455"
  },
  {
    "text": "play, uh, different LoRA's in order to\nset up models that can run different things like uncertainty quantification,\nsafety checks, all within one workflow.",
    "start": "1185455",
    "end": "1193565"
  },
  {
    "text": "Using some clever masking to make\nsure you're not running inferences multiple times and to kind of set up\nthis really nice programmatic flow",
    "start": "1194540",
    "end": "1201950"
  },
  {
    "text": "for more advanced executions, uh,\nwith the, with the model in question. So if anyone's at NeurIPS, definitely\nrecommend checking out the booth.",
    "start": "1201950",
    "end": "1208519"
  },
  {
    "text": "That's great. Yeah. I feel like, uh, I don't know,\nmy main hope right now is like to have more time to read papers.",
    "start": "1208579",
    "end": "1213359"
  },
  {
    "text": "I do miss that period of my\nlife when I was able to do that. Um, I guess maybe the final question,\nI mean, Marina, maybe I'll kick",
    "start": "1213789",
    "end": "1219739"
  },
  {
    "text": "it to you is, uh, how do you keep\nup with all the papers in the space, just as a meta question?",
    "start": "1219739",
    "end": "1223415"
  },
  {
    "text": "Uh, I think I can't possibly,\nbut, uh, in general, giant shout out to my IBM colleagues.",
    "start": "1225645",
    "end": "1230934"
  },
  {
    "text": "We have some real good active Slack\nchannels where people post the things that they like, and there's particular folks\nwith particular areas of expertise that",
    "start": "1230934",
    "end": "1238655"
  },
  {
    "text": "I can look to and see, oh, what has, uh,\nsome particular researcher been posting. Lately. And that is the way because, um, yeah,\nit's, it's a lot of things, especially",
    "start": "1238655",
    "end": "1246965"
  },
  {
    "text": "now that there's, uh, a very welcome\nshift to people posting research even early, just, you know, preprints on\narchive and things of that nature.",
    "start": "1246965",
    "end": "1254644"
  },
  {
    "text": "And you really need the human curation\nto let you know what's noise and what's worth paying attention to. And yeah, I can't beat human\ncuration for that right now.",
    "start": "1255005",
    "end": "1262835"
  },
  {
    "text": ". \nYeah. It feels like, I feel like the, the,\nthe key infrastructure is group chats. Like that's all I have now. Yes.",
    "start": "1263435",
    "end": "1268780"
  },
  {
    "text": "just gonna add that- this is gonna\nmake Kate very happy on this. I use, uh, the, as a true AI developer,\nI go to watsonx, the AI platform.",
    "start": "1269825",
    "end": "1277995"
  },
  {
    "text": "I use the Granite model. I feed in my papers one by one. First I ask, okay, summarize this for me.",
    "start": "1278385",
    "end": "1284245"
  },
  {
    "text": "Then I'm like, tell me the key points. And then I go deeper, deeper, deeper. I mean, I go the other way around to\nreverse engineer the paper to kind",
    "start": "1284465",
    "end": "1292111"
  },
  {
    "text": "of figure out what to do with it. There's a script that I've written\nfor it, which I'm very proud of. So I usually-",
    "start": "1292111",
    "end": "1297785"
  },
  {
    "text": "You should open source that. Yeah, you gotta open source that. I need that in my life. Maybe I could do that. Yes, you should.",
    "start": "1297785",
    "end": "1302783"
  },
  {
    "text": "Absolutely. Okay. Thank you. You heard it here first\non Mixture of Experts.",
    "start": "1302845",
    "end": "1310694"
  },
  {
    "start": "1310000",
    "end": "1820000"
  },
  {
    "text": "I'm going to move us to\nour third topic of the day. Um, so, uh, ARC Prize, uh, which is\nan effort that was set up by Mike",
    "start": "1312565",
    "end": "1319184"
  },
  {
    "text": "Knoop of Zapier and Francois Chalet\nof, uh, Keras, um, is a benchmark",
    "start": "1319184",
    "end": "1324595"
  },
  {
    "text": "that attempts to evaluate whether\nor not models can learn new skills. And ostensibly what it's trying to\ndo is to be a benchmark for AGI.",
    "start": "1324605",
    "end": "1332975"
  },
  {
    "text": "In practice, what it means is that\nyou're asking the machine to solve a puzzle with these colored squares.",
    "start": "1333630",
    "end": "1338380"
  },
  {
    "text": "Um, and this is very interesting. I bring it up today just because\nI think they did the latest round of kind of competition against the\nbenchmark and showed the results",
    "start": "1338850",
    "end": "1346179"
  },
  {
    "text": "and a technical report came out. But I think this effort is just so\nintriguing because you know, we've done",
    "start": "1346199",
    "end": "1351275"
  },
  {
    "text": "it to this on the show where people\nsay, AGI, what does it really even mean? And I think in most cases, people have\nno idea what it really means or can't",
    "start": "1351275",
    "end": "1358945"
  },
  {
    "text": "really point to how they would measure it. And this seems to me to be like\nat least one of the efforts that say, well, here's maybe one way\nwe could go about measuring this.",
    "start": "1358945",
    "end": "1366655"
  },
  {
    "text": "Um, and so I did want to kind of just\nlike bring this up to kind of maybe square the circle, particularly with this\ngroup, um, about sort of evals for AGI.",
    "start": "1367095",
    "end": "1376330"
  },
  {
    "text": "Like, does that even\nmake sense as a category? Are people even looking\nfor those types of evals? There's just a bunch of really\ninteresting questions there.",
    "start": "1376400",
    "end": "1383759"
  },
  {
    "text": "And I guess Vyoma, maybe\nI'll turn it to you first. I'm kind of curious about like,\nwhen you see this kind of eval,",
    "start": "1383760",
    "end": "1388389"
  },
  {
    "text": "you know, is it helpful eval? Is it mostly a research curiosity? Like how do you think about\nsomething like ARC Prize?",
    "start": "1388959",
    "end": "1393780"
  },
  {
    "text": "Yeah, so when I look at ARC Prize, it\nwas, I think, um, it was founded in 2019,",
    "start": "1394095",
    "end": "1400235"
  },
  {
    "text": "created back then, when generative AI,\nlarge language models weren't a thing",
    "start": "1400325",
    "end": "1405674"
  },
  {
    "text": "back then, and I think, um, it helps\nbecause it's like one of the first in the game again as well, so people kind of\nrelate immediately back to it that this",
    "start": "1405685",
    "end": "1415230"
  },
  {
    "text": "is the benchmark to, um, evaluate AGI,\nbut AGI is way more bigger and better,",
    "start": "1415230",
    "end": "1421570"
  },
  {
    "text": "um, in doing things such as there are\nso many things that, uh, clients like OpenAI and then other companies such\nas Mistral, etc., they're coming up",
    "start": "1421830",
    "end": "1429759"
  },
  {
    "text": "with these models, which can annotate\nhuman data to help you act like human.",
    "start": "1429759",
    "end": "1434575"
  },
  {
    "text": "And there are different\nmethods to do that. So are AGI even, I won't say is\nthe pristine benchmark or standard,",
    "start": "1434925",
    "end": "1441645"
  },
  {
    "text": "but I do get the point as to why\npeople refer back to it a lot. Yeah, it sounds right.",
    "start": "1441985",
    "end": "1447095"
  },
  {
    "text": "I mean, I think that's kind of, I mean, we\ntalked about it earlier in this episode. I think Kate, you were like, it makes\nsense as if you were an AGI company,",
    "start": "1447145",
    "end": "1453804"
  },
  {
    "text": "this is the strategy you would pursue. As yeah, it's kind of interesting,\neven though we can think about it and talk about it in those concrete\nterms, when it comes down to like",
    "start": "1453815",
    "end": "1461260"
  },
  {
    "text": "the nitty gritty of machine learning,\nit's like, what do we even use to kind of measure progress against this? And no one really knows, I guess,\nyou know, kind of you're indicating",
    "start": "1461260",
    "end": "1468050"
  },
  {
    "text": "that it's like, well, we kind of\nfall back to this metric because we don't have anything else. Um, yeah, I kind of curious,\nKate, how you think about that?",
    "start": "1468050",
    "end": "1473740"
  },
  {
    "text": "I think there are, there's a number of\ndifferent ways you can think about it. One, we're always going to need\nnew benchmarks to continue to have",
    "start": "1473830",
    "end": "1481629"
  },
  {
    "text": "some targets we can solve for. So having a benchmark that\nhasn't been like, cracked, so to speak, is interesting.",
    "start": "1481629",
    "end": "1487314"
  },
  {
    "text": "I don't know that that means it's more\nthan a research curiosity, honestly, but there is something of value there.",
    "start": "1487345",
    "end": "1492915"
  },
  {
    "text": "There's something that we're\nmeasuring that models can't do today. Is that thing valuable? I don't know. The, we're really talking about\nsolving puzzles with colored dots.",
    "start": "1492915",
    "end": "1501844"
  },
  {
    "text": "How well can people who solve\nthose puzzles with colored dots correlate to the different tasks\noutside of solving those puzzles?",
    "start": "1502094",
    "end": "1507945"
  },
  {
    "text": "I'm not too sure. I also think there's something\nwrong calling that a test for AGI",
    "start": "1507965",
    "end": "1514095"
  },
  {
    "text": "because like, general is in the name. That task in that\nbenchmark is very specific.",
    "start": "1514105",
    "end": "1519450"
  },
  {
    "text": "It's like oddly specific. And it's one that humans can't\ndo very well today either.",
    "start": "1519520",
    "end": "1524950"
  },
  {
    "text": "So, you know, it doesn't quite resonate\nto me as a quote general intelligence",
    "start": "1524950",
    "end": "1530059"
  },
  {
    "text": "where I think breadth is super important,\num, if that's what we're really after.",
    "start": "1530080",
    "end": "1534949"
  },
  {
    "text": "Yeah, it almost feels like you need\nlike the the pentathlon or something. It's got to be like a bunch of\ndifferent tasks, I guess, in theory.",
    "start": "1535139",
    "end": "1541300"
  },
  {
    "text": "Um, I guess Marina, do you, do you\nthink, I was talking with a friend recently, I was like, I was like, do you\nthink evals are just broken right now?",
    "start": "1541330",
    "end": "1548365"
  },
  {
    "text": "Like, um, there's kind of a\nreceived wisdom that most of the market benchmarks or the understood\nbenchmarks are all super saturated.",
    "start": "1548685",
    "end": "1555915"
  },
  {
    "text": "Um, and then like, it's very clear\nthat the vibes evals, like you play around with it or like not.",
    "start": "1556404",
    "end": "1561395"
  },
  {
    "text": "comprehensive in the way we want. And then so there's kind of this big\nblurry thing about like, well, what's the next generation evals that we\nthink are going to be useful here?",
    "start": "1561850",
    "end": "1569169"
  },
  {
    "text": "And how broken is it? Maybe I'm being too much of a pessimist. It's a very hard thing to do. I mean, even this particular\nbenchmark that we're talking",
    "start": "1569630",
    "end": "1575519"
  },
  {
    "text": "about, it is one specific way\nto instantiate a few assumptions",
    "start": "1575519",
    "end": "1580530"
  },
  {
    "text": "about intelligence that they said. I was refreshing my memory\non what they had said. And I was like, okay, there are\nobjects and objects have goals.",
    "start": "1580530",
    "end": "1586370"
  },
  {
    "text": "And there's something about\nthe topology of space. Okay. Yes, this is all. True, and this is one way to go there.",
    "start": "1586760",
    "end": "1592790"
  },
  {
    "text": "It's certainly not a comprehensive way,\nbut with research It's all about well, we got to have some instantiation of it\nor we're never gonna make any progress",
    "start": "1592790",
    "end": "1599910"
  },
  {
    "text": "So I think you always have to take\nevery benchmark with a grain of salt. A benchmark is not an actual measure\nof quality It's a proxy if you want",
    "start": "1599920",
    "end": "1608299"
  },
  {
    "text": "to really get into ML speak quality\nis hidden, benchmark is observed.",
    "start": "1608299",
    "end": "1613019"
  },
  {
    "text": "And it is a limited proxy in a smaller\nspace than what the quality is. Think about all the\nhidden layers of quality.",
    "start": "1613360",
    "end": "1618960"
  },
  {
    "text": "We get a specific proxy. Um, the more variety you can do the\nbetter and the more you can also,",
    "start": "1619030",
    "end": "1625410"
  },
  {
    "text": "uh, understand that if something's\nbeen around for several months, as you said, it's been learned. Um, you, that's it, you, you've\nlearned it, you need to move",
    "start": "1625460",
    "end": "1633145"
  },
  {
    "text": "on and, and do something else. But the problem is, if we don't have\nsomething that's quantitative, then people are just going to argue over vibes.",
    "start": "1633145",
    "end": "1639335"
  },
  {
    "text": "Like, \"well, I had these five\nexamples in my head,\" \"well, I had these five examples in my head.\" And then you really do just say, I don't,\nI don't trust it, or I don't believe",
    "start": "1639385",
    "end": "1646003"
  },
  {
    "text": "it, or, but can't these things be faked? That way lies madness, as far as\nthe actual use of these things go.",
    "start": "1646005",
    "end": "1653575"
  },
  {
    "text": "We have to agree on something and put\nout the limitations and put out the constraints and still be able to agree\nthat there is something to compare on.",
    "start": "1653575",
    "end": "1662490"
  },
  {
    "text": "Um, so I, there's, there's no\nway around it with evaluation. It's never been easy. It's never going to be easy.",
    "start": "1662550",
    "end": "1668820"
  },
  {
    "text": "I don't think it's more broken\nthan it ever really used to be. Yeah, exactly. It's exactly as broken\nas it always has been.",
    "start": "1668820",
    "end": "1673790"
  },
  {
    "text": "It's as broken as it's been. Yeah. I think, cause I think you're\nseeing two meta trends. I feel like one of them is, We talked\nabout the hard math benchmark that",
    "start": "1673949",
    "end": "1683309"
  },
  {
    "text": "this group called Epoch put out. Um, and you know, it feels like one,\none bit of meta is like, we're going to",
    "start": "1683310",
    "end": "1688420"
  },
  {
    "text": "just make the difficulty so difficult\nthat like, it's almost like a way of us recreating that consensus where we're\nkind of like, Oh, well, if a machine can",
    "start": "1688420",
    "end": "1696869"
  },
  {
    "text": "do that, something is really happening. But it feels to me, that's like a very,\nalmost a very crude way of going at eval is like, all we do to try to get\nsome agreement to move beyond the vibes",
    "start": "1696870",
    "end": "1706620"
  },
  {
    "text": "is to try to create something that's\nso difficult that it's indisputable that if you hit it, it would, it would\nbe like a breakthrough in progress.",
    "start": "1706870",
    "end": "1714155"
  },
  {
    "text": "But, you know, on a day-to-day basis, it's\nlike, how useful is a metric like that? You know? Well, so ultimately, uh, ARC Prize, I\nguess, are we pretty sympathetic to it?",
    "start": "1714155",
    "end": "1722275"
  },
  {
    "text": "It kind of sounds like ultimately,\nit's like, it's measuring something. We're just not quite\nsure what it is just yet. I don't know if I'm a million\ndollars sympathetic to it.",
    "start": "1722285",
    "end": "1728365"
  },
  {
    "text": "I'm sympathetic to it as a benchmark,\nbut I guess it's up to them. Yeah. I like how large dollar amounts have\njust been this theme for the episode.",
    "start": "1728365",
    "end": "1737355"
  },
  {
    "text": "I feel, I feel once the AI agents, um,\nare utilized to kind of, uh, make these",
    "start": "1737725",
    "end": "1744845"
  },
  {
    "text": "AGI concepts much more simplified, I feel\nthat I wouldn't go to that extent saying",
    "start": "1745055",
    "end": "1751114"
  },
  {
    "text": "that that particular benchmark can be\nachieved and someone will win that prize. But I feel that with multiple permutation\ntransformations with AI agents, let's",
    "start": "1751125",
    "end": "1760075"
  },
  {
    "text": "say someone used generalization and\nsome sort of transfer learning and then created an agent to understand the human's\nway to learn, maybe, maybe not, but I",
    "start": "1760075",
    "end": "1770595"
  },
  {
    "text": "feel that that's a gray area right now\nand we don't know what can be achieved.",
    "start": "1770605",
    "end": "1776214"
  },
  {
    "text": "So let's say I'm not here to say\nthat it's here to stay or not, but there's something new comes along.",
    "start": "1776555",
    "end": "1781934"
  },
  {
    "text": "I feel that's something that\nwe're measuring against. I think I mean, and to Marina's point,\nI think one of the theories I've been",
    "start": "1781955",
    "end": "1788385"
  },
  {
    "text": "sort of chasing after is AI is just\nbeing used in so many different ways by so many different people now that like,\nwe will just end up seeing this like",
    "start": "1788385",
    "end": "1795704"
  },
  {
    "text": "vast fragmentation and evals, right? Like it won't be old days where\nit's like, it was good on MMLU, so",
    "start": "1795814",
    "end": "1801784"
  },
  {
    "text": "I guess it's just good in general. Like everything is going to\njust be measured by like very local needs and constraints.",
    "start": "1801785",
    "end": "1807399"
  },
  {
    "text": "And, you know, talk,\ntalk about group chats. I've been like encouraging all of my group\nchats, like we need our own bench, you",
    "start": "1807400",
    "end": "1812920"
  },
  {
    "text": "know, because I think it's just like every\ncommunity is so specific that like we just should have our own bespoke eval that we\njust run against models as they come out.",
    "start": "1812920",
    "end": "1819230"
  },
  {
    "start": "1820000",
    "end": "2451000"
  },
  {
    "text": "So for our next topic, I really want to\nfocus on the release of Llama 3.3 70B. Uh, background here is that Meta announced\nthat it was launching, uh, another",
    "start": "1824289",
    "end": "1833049"
  },
  {
    "text": "generation of its own Llama models, um,\nand most notably a sort of 70B version of",
    "start": "1833050",
    "end": "1838210"
  },
  {
    "text": "the model that promised 405B performance,\nbut in a much more compact format.",
    "start": "1838210",
    "end": "1843429"
  },
  {
    "text": "This is a trend that we've\nbeen seeing for a while. And I guess maybe ultimately, um,\nyou know, Kate, maybe I'll kick it",
    "start": "1843440",
    "end": "1849020"
  },
  {
    "text": "to you is, I guess the question I\nwant to ask is like, do we think that we're going to eventually just be\nable to have our cake and eat it too?",
    "start": "1849030",
    "end": "1854319"
  },
  {
    "text": "Like that, like we've been\noperating under this trade off of big model, hard to run, but good.",
    "start": "1854699",
    "end": "1860330"
  },
  {
    "text": "Little model, not so\ngood, but fast to run. And, you know, where everything seems\nto be going in my mind is like, maybe",
    "start": "1860550",
    "end": "1866919"
  },
  {
    "text": "that's just a total historical artifacts? Like, I don't know, do\nyou think that's the case? I think that we often conflate size as\nthe only driver of performance in a model.",
    "start": "1866920",
    "end": "1877490"
  },
  {
    "text": "And I think with this release of Llama\n3.3 70B, comparing it to the older 3.1",
    "start": "1877880",
    "end": "1883639"
  },
  {
    "text": "405B, we're seeing firsthand that size\nisn't the only way to drive performance, and that increasingly the quality of\nthe data used in the alignment step",
    "start": "1883639",
    "end": "1892575"
  },
  {
    "text": "of the model training is going to\nbe incredibly important for driving performance, particularly in key areas.",
    "start": "1892575",
    "end": "1898255"
  },
  {
    "text": "So if you look at the eval results, right,\nthe 3.3 70B, uh, is matching or you're",
    "start": "1898265",
    "end": "1904375"
  },
  {
    "text": "actually exceeding on some benchmarks the\nolder 405B in places like math reasoning.",
    "start": "1904385",
    "end": "1910515"
  },
  {
    "text": "And so I think that really speaks\nto the fact that you don't need a big model to do every task.",
    "start": "1911020",
    "end": "1916590"
  },
  {
    "text": "Smaller models can be just\nas good for different areas. And if we increasingly invest in\nthe data versus just letting it sit",
    "start": "1916830",
    "end": "1923760"
  },
  {
    "text": "on a compute for longer, training\nit at a bigger size, we can find new ways to unlock performance.",
    "start": "1923800",
    "end": "1929509"
  },
  {
    "text": "Yeah, that's a really interesting outcome. My friend was commenting that\nit's like, uh, it's almost kind of like a very heartening message.",
    "start": "1929620",
    "end": "1936029"
  },
  {
    "text": "You know, the kind of ideas like you\ndon't need to be born with a big brain so long as you've got good, good training.",
    "start": "1936350",
    "end": "1941260"
  },
  {
    "text": "Like you've got like a good pedagogy is\nlike actually what makes the difference. And, you know, I think we are kind\nof seeing that in some ways, right?",
    "start": "1941530",
    "end": "1947250"
  },
  {
    "text": "That like, I guess like the dream of\nmassive, massive architectures may not be like the ultimate lever that kind of\ngets us to really increase performance.",
    "start": "1947260",
    "end": "1955250"
  },
  {
    "text": "Um, uh, I guess, I think one idea I\nthink I want to kind of run by you",
    "start": "1955490",
    "end": "1960580"
  },
  {
    "text": "is just whether or not you think that\nthis, this will be the trend, right? Like I guess to Kate's point, like you\ncan imagine in the future that companies",
    "start": "1960580",
    "end": "1967100"
  },
  {
    "text": "end up spending just a lot more time\non their data more than anything else. Um, which is a little bit of a flip.",
    "start": "1967100",
    "end": "1972555"
  },
  {
    "text": "I mean, I think most of my experience with\nmachine learning people was like, I don't really know where the data comes from. So long as there's a\nlot of it, it will work.",
    "start": "1972645",
    "end": "1978965"
  },
  {
    "text": "Um, and this is like almost points towards\na pretty other different discriminating",
    "start": "1979225",
    "end": "1984174"
  },
  {
    "text": "kind of like approach to doing this work. Yeah. So I, I work with clients day\nin and day out, and I feel",
    "start": "1984245",
    "end": "1990054"
  },
  {
    "text": "that the trend is catching on. The clients no longer want to be\npaying so much money amount of",
    "start": "1990084",
    "end": "1996645"
  },
  {
    "text": "dollars for every API call to like\na large model and on, on something which is lying, not in their control.",
    "start": "1996645",
    "end": "2003565"
  },
  {
    "text": "Even though we, they say that, oh,\nwe say we indemnify the data, which you are not, we are not storing\nyour data for them in their head.",
    "start": "2003995",
    "end": "2011145"
  },
  {
    "text": "It's still not there yet. So people want it on their own\nprem, a smaller model trained on their own specific data.",
    "start": "2011234",
    "end": "2017444"
  },
  {
    "text": "There have been so many times\nthat I've sat with them and then curated the data flow. That listen, this is what we'll get in.",
    "start": "2017464",
    "end": "2023025"
  },
  {
    "text": "This is how we'll get it. So the trend is definitely,\ndefinitely catching on. And sometimes often, like historically,\nI've seen that efficiency gains that we",
    "start": "2023025",
    "end": "2032930"
  },
  {
    "text": "see, they are promising, but sometimes\nsome of these models, they, uh, there, there are some trade offs in like context\nhandling and then adaptability, et cetera.",
    "start": "2032930",
    "end": "2041520"
  },
  {
    "text": "So now I feel if we have a smaller\nmodel with good amount of data, that the domain-specific data, they\nare getting better value out of it.",
    "start": "2041809",
    "end": "2050045"
  },
  {
    "text": "And I see that happening. So yeah, I feel it's good and refreshing\nto see that no longer everyone, every",
    "start": "2050335",
    "end": "2056445"
  },
  {
    "text": "time I used to walk into a board\nmeeting and everyone would like, Ooh, 70 billion, Oh, 13 billion, I will\nbe comparing it under 405 billion.",
    "start": "2056445",
    "end": "2064513"
  },
  {
    "text": "I no longer have to have\nthat conversation anymore. So good for us. Yeah, I think it's kind of like, it's\nalmost like people want the metric.",
    "start": "2064574",
    "end": "2071784"
  },
  {
    "text": "They're like, oh, that's a lot of B. Like, where is this 405 B? That's a lot. Because now they have the legal\nteam, the finance team, as Marina was",
    "start": "2071805",
    "end": "2079981"
  },
  {
    "text": "mentioning, breathing down their necks. They're like, why do you\nhave such a big model? Why is it inflating our resources\nand the money that we have to",
    "start": "2079982",
    "end": "2087464"
  },
  {
    "text": "write a check on every month. So everything's coming back to that. Yeah. There's a little bit of a\nrace against time here though.",
    "start": "2087755",
    "end": "2093899"
  },
  {
    "text": "I don't know if Marina's got views on how\nthis will evolve as like, part of this is driven by just the cost of API calls.",
    "start": "2093900",
    "end": "2099440"
  },
  {
    "text": "And so there's kind of almost this game\nwhere it's like, how cheap will the API become versus how much work are people\nwilling to do upfront around their data?",
    "start": "2099759",
    "end": "2106550"
  },
  {
    "text": "I guess kind of you almost saying is\nlike, it seems like companies are really tending towards the data direction. Uh, so as a committed data centric\nresearcher, I'm very pleased to see",
    "start": "2106990",
    "end": "2114954"
  },
  {
    "text": "this direction of, uh, of things. Is it good? Excellent. Um, again, I'll just, uh, re, uh,\nsay what Kate had said, which is",
    "start": "2115445",
    "end": "2124454"
  },
  {
    "text": "that the 3.3 model, uh, versus\nthe 3.1, it's only post-training.",
    "start": "2124465",
    "end": "2129514"
  },
  {
    "text": "It's not, you know, reach,\nyou know, making a new model. It is differences in\npost-training techniques. So fine tuning alignment,\nthings of that nature.",
    "start": "2129585",
    "end": "2136500"
  },
  {
    "text": "And this also shows the value of going\nin the directions of the small different ways of adapting LoRA's because yeah,\nclients want things that are not",
    "start": "2136940",
    "end": "2144429"
  },
  {
    "text": "just good on the general benchmark. They want things that are good for them. And look, the big was good because\nwhenever you have new technology,",
    "start": "2144430",
    "end": "2151250"
  },
  {
    "text": "first you want to get it to work. Then you want it to get to work better. Then you want it to work cheaper, faster.",
    "start": "2151250",
    "end": "2156430"
  },
  {
    "text": "So we have like, all\nright, there's a new thing. Okay. Now we're getting those things a\nlittle bit smaller, cheaper, faster. There's a new thing again.",
    "start": "2156720",
    "end": "2161799"
  },
  {
    "text": "Now we're getting it\nsmaller, cheaper, faster. This is normal. This is a normal cyclic way\nof having the innovation. Clients are for sure catching up to\nthis fact and saying, yes, okay, I see",
    "start": "2161810",
    "end": "2170595"
  },
  {
    "text": "your 405, but I'm not gonna pay money on\nthat because I already know you're going to figure out ways to bring that down.",
    "start": "2170755",
    "end": "2175744"
  },
  {
    "text": "And I don't need all the\nthings that that model can do. I need really specific things for me. So this is again, even goes back\nto our conversation on benchmarks.",
    "start": "2175785",
    "end": "2183475"
  },
  {
    "text": "You look at the benchmark\nthat matters for you. You look at the size of the model that\nmatters for you and how much it costs. And this, this really matters a ton as\nwe try to make use of this technology,",
    "start": "2183515",
    "end": "2192915"
  },
  {
    "text": "not get the technology to work, but\nto get the technology to work for us. This trend is going to continue and\nI see it as a as a very good thing a",
    "start": "2193095",
    "end": "2200684"
  },
  {
    "text": "very heartening thing It means people\nare getting a better intuition of what the point of this tech is going to be\nwhich is not size for the sake of size.",
    "start": "2200685",
    "end": "2208125"
  },
  {
    "text": "I also think there's some really\ninteresting like scaling laws that are starting to emerge like you look\nat the performance of Llama 1 65B",
    "start": "2208305",
    "end": "2216715"
  },
  {
    "text": "versus you know, okay, maybe Lama 2\n13B was able to accomplish all of that. You look at what Llama 3 8B\ncould do compared to Llama 2 70B.",
    "start": "2216735",
    "end": "2225830"
  },
  {
    "text": "You know, again, we were able\nto take that, shrink it down. Now we're taking Llama 405B\nand shrinking it down into 70B.",
    "start": "2225830",
    "end": "2232430"
  },
  {
    "text": "And I think these updates are happening\nmore rapidly and we're increasingly. Uh, decreasing the amount of time,\nuh, that it takes to take those,",
    "start": "2232879",
    "end": "2241460"
  },
  {
    "text": "that large model performance and\nshrink it down into fewer parameters. And so it'd be interesting to plot that\nout at some point and see, cause I think",
    "start": "2241470",
    "end": "2247839"
  },
  {
    "text": "we're seeing a ramp up and as we continue\nto look at things that are scalable. So like amount of training data and\nsize of the model isn't very scalable.",
    "start": "2248170",
    "end": "2256640"
  },
  {
    "text": "It just, it's cost exponentially more to\nincrease the size of your model, right? But if we are looking at things\nlike investing in data quality and",
    "start": "2256640",
    "end": "2264840"
  },
  {
    "text": "other resources that maybe are. Uh, we can invest in more easily. I think we're going to continue to\nsee that increase in model performance",
    "start": "2264850",
    "end": "2272900"
  },
  {
    "text": "and shrinking of the, the model size. And to Vyoma's earlier point about,\nuh, agents, right, the complexity of",
    "start": "2272900",
    "end": "2279000"
  },
  {
    "text": "that is exponential already itself. So you do not want to be having an each\nagent have a 405 billion parameters.",
    "start": "2279010",
    "end": "2286260"
  },
  {
    "text": "That is, that is not something you can do. So it's a yet another driver\nin this direction of motivator.",
    "start": "2286540",
    "end": "2292109"
  },
  {
    "text": "One more driver that I've seen. And I don't know if anyone else has,\nbut I was in a call with one of the banks and they've also, uh, there's\na shift towards using some energy",
    "start": "2292310",
    "end": "2301090"
  },
  {
    "text": "efficient training pipelines as well. Everyone's looking into how do we\noptimize the hardware utilization?",
    "start": "2301090",
    "end": "2306760"
  },
  {
    "text": "Is there any sort of long\nterm environmental effect? And that's also a nuanced\ntopic, which is building up.",
    "start": "2306990",
    "end": "2312449"
  },
  {
    "text": "I saw some papers in NuerIPS also\non that, but I've hadn't had the chance to look deeper into it,\nbut I also see these conversations",
    "start": "2312469",
    "end": "2319770"
  },
  {
    "text": "coming up day in and day out. Although I guess one thing, I mean,\nmaybe Kate to push back a little",
    "start": "2319770",
    "end": "2325230"
  },
  {
    "text": "bit, like it is actually an important\nthing probably for our listeners to know that you, you kind of need the\n405 to get to this new Llama model.",
    "start": "2325230",
    "end": "2332420"
  },
  {
    "text": "Um, and I guess that is one of the\ninteresting dynamics is for all the benefit that these small models\nprovide, is it right to say like",
    "start": "2332840",
    "end": "2338460"
  },
  {
    "text": "we, we still need the, the mega\nsize model to get to, get to this? Again, I think we're conflating size\nas the only driver of performance.",
    "start": "2338470",
    "end": "2346359"
  },
  {
    "text": "So I think you need more performant\nmodels to get to smaller performant models, regardless of what size they are.",
    "start": "2346360",
    "end": "2352540"
  },
  {
    "text": "Um, and if you have something\nbigger that's performant, it's easier to shrink it down in size. But if I talk about and think of the\nnormal way we'd think about going",
    "start": "2352589",
    "end": "2361880"
  },
  {
    "text": "doing this right taking a big model and\nshrinking it down is generating synthetic data from it and what's called using\nit as a teacher model and training a",
    "start": "2361950",
    "end": "2368660"
  },
  {
    "text": "student model on that data you can use\na smaller model if it's better at math. You know Llama 3.3 70B is uh you\nknow outperforming 405B according",
    "start": "2368660",
    "end": "2377980"
  },
  {
    "text": "to a few other benchmarks on on math\nand instruction following and code so I could, and would prefer to use\nthat smaller model to train a new 70",
    "start": "2377980",
    "end": "2385715"
  },
  {
    "text": "billion parameter model than 405B. I don't have to go with the bigger one. I want to go wherever\nperformance is highest.",
    "start": "2385715",
    "end": "2391315"
  },
  {
    "text": "Yeah, this all calls to mind, I mean, I\nwant to benchmark or some kind of machine learning competition, which is like you\ntake the smallest amount of data to try to",
    "start": "2391365",
    "end": "2397734"
  },
  {
    "text": "create the highest level of performance. And like, it's almost like a form\nof like machine learning golf. It's like, what's the smallest number\nof strokes that get you to the goal?",
    "start": "2397735",
    "end": "2405265"
  },
  {
    "text": "You know, what's the smallest amount\nof data that gets you to the model that can actually achieve the task? And it feels like, you know, it sounds\nlike we may just be forced there because.",
    "start": "2405545",
    "end": "2412045"
  },
  {
    "text": "you know, legal and\nfinance are complaining. Now it feels like it's going to become\nmore of an incentive within the space. You're going to promote overfitting, Tim.",
    "start": "2412250",
    "end": "2419120"
  },
  {
    "text": "If you really do that kind of thing,\npeople will just game the benchmark.",
    "start": "2419230",
    "end": "2422050"
  },
  {
    "text": "Well, that's another\ntopic for another day. As per usual, we're at the end of the\nepisode and there's a lot more to talk",
    "start": "2424689",
    "end": "2429879"
  },
  {
    "text": "about, so we will have to bring this to\na future episode panel with you all on. Uh, thanks for joining us.",
    "start": "2429889",
    "end": "2435349"
  },
  {
    "text": "Uh, and thanks to all\nyou listeners out there. If you enjoyed what you heard, you\ncan get us on Apple Podcasts, Spotify,",
    "start": "2435440",
    "end": "2440770"
  },
  {
    "text": "and podcast platforms everywhere. And we will see you next\nweek on Mixture of Experts.",
    "start": "2440770",
    "end": "2444550"
  }
]