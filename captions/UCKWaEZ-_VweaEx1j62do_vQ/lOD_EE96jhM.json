[
  {
    "text": "Large language models have a problem.",
    "start": "760",
    "end": "3399"
  },
  {
    "text": "We know that they can process a text document, like a PDF maybe, and then they can respond to queries about it.",
    "start": "3620",
    "end": "12300"
  },
  {
    "text": "And they do this by encoding both the document and any prompt we provide",
    "start": "12300",
    "end": "16553"
  },
  {
    "text": "as tokens, and then putting that into the LLM, where it's processed",
    "start": "16553",
    "end": "22625"
  },
  {
    "text": "through attention mechanisms, and then generates a text-based response at the other side.",
    "start": "22625",
    "end": "28960"
  },
  {
    "text": "What happens if that document, that maybe that PDF, what if it contains some images?",
    "start": "30810",
    "end": "37570"
  },
  {
    "text": "Well, standard LLMs can't read images the way that they process text.",
    "start": "37970",
    "end": "43270"
  },
  {
    "text": "So a picture or a graph or a handwritten note that might contain some valuable information,",
    "start": "43390",
    "end": "48708"
  },
  {
    "text": "but without a way to convert that visual data into a form the LLM understands, it's gonna be inaccessible,",
    "start": "48708",
    "end": "55670"
  },
  {
    "text": "but this is where vision language models come in, or VLMs.",
    "start": "55970",
    "end": "64421"
  },
  {
    "text": "So vision language models are multimodal.",
    "start": "64630",
    "end": "69209"
  },
  {
    "text": "That means that they can take in text but they can also take in",
    "start": "69290",
    "end": "74763"
  },
  {
    "text": "image files as well and interpret their meaning and then generate as a response a text based output.",
    "start": "74763",
    "end": "84150"
  },
  {
    "text": "So what sort of tasks can we perform with a VLM.",
    "start": "84890",
    "end": "90209"
  },
  {
    "text": "Well one of those tasks is called the VQA,",
    "start": "90874",
    "end": "96408"
  },
  {
    "text": "or visual question answering.",
    "start": "96408",
    "end": "99659"
  },
  {
    "text": "That's just kind of a fancy way to say that you can show a VLM a picture and have it analyze it.",
    "start": "100560",
    "end": "105000"
  },
  {
    "text": "Maybe we'll show it a photo of a busy city street and we can ask, hey, what's happening here?",
    "start": "105300",
    "end": "111040"
  },
  {
    "text": "Now the model doesn't just see pixels, it recognizes objects and people and context",
    "start": "111360",
    "end": "116588"
  },
  {
    "text": "and it could tell you, for example, that there's a car waiting at a red light.",
    "start": "116588",
    "end": "120298"
  },
  {
    "text": "Then there's also the ability to provide captioning for images as well.",
    "start": "121400",
    "end": "126379"
  },
  {
    "text": "So here we have the model generate a natural language description of an image.",
    "start": "127566",
    "end": "132810"
  },
  {
    "text": "So if we showed a picture of a dog chasing a ball, it might say that's a golden retriever playing fetch in a park,",
    "start": "132850",
    "end": "139949"
  },
  {
    "text": "but VLMs aren't just about photographs.",
    "start": "140810",
    "end": "143190"
  },
  {
    "text": "They're also super useful for document understanding as well.",
    "start": "143370",
    "end": "149750"
  },
  {
    "text": "So let's say you've uploaded a scanned receipt.",
    "start": "150130",
    "end": "154129"
  },
  {
    "text": "The model can extract the text in that receipt.",
    "start": "154130",
    "end": "157110"
  },
  {
    "text": "It can organize it and then it can even summarize what it says,",
    "start": "157580",
    "end": "160440"
  },
  {
    "text": "and then what about data heavy visuals that we might have in a PDF?",
    "start": "161160",
    "end": "165540"
  },
  {
    "text": "Well, there we can use something called graph analysis to understand that as well.",
    "start": "166220",
    "end": "172219"
  },
  {
    "text": "So we could hand it a sales report and then we could ask, hey, what's the trend going on here?",
    "start": "172600",
    "end": "177840"
  },
  {
    "text": "And the model can extract the data in the graphs and interpret them.",
    "start": "178120",
    "end": "181680"
  },
  {
    "text": "So, vision language models, they don't just process images and text.",
    "start": "182780",
    "end": "186639"
  },
  {
    "text": "Separately, they merge them.",
    "start": "186900",
    "end": "189280"
  },
  {
    "text": "But how do they actually do that?",
    "start": "189900",
    "end": "191778"
  },
  {
    "text": "Well, let's break it down.",
    "start": "192400",
    "end": "193918"
  },
  {
    "text": "We'll start with the part that's already familiar.",
    "start": "194480",
    "end": "196978"
  },
  {
    "text": "That is the large language model, the LLM.",
    "start": "197640",
    "end": "204328"
  },
  {
    "text": "Now the LLM, it takes in as a prompt, a text prompt.",
    "start": "204540",
    "end": "211418"
  },
  {
    "text": "So this is us sending in a message to the large language model.",
    "start": "211480",
    "end": "216198"
  },
  {
    "text": "And it converts the words in that text prompt into text tokens.",
    "start": "216200",
    "end": "224838"
  },
  {
    "text": "So we go from there to there.",
    "start": "225980",
    "end": "227619"
  },
  {
    "text": "Now these are discrete numerical representations of language and these text tokens are then",
    "start": "227760",
    "end": "235264"
  },
  {
    "text": "processed through the model's internal mechanisms, so primarily its attention layers,",
    "start": "235264",
    "end": "239841"
  },
  {
    "text": "to determine the relationships, the contextual meaning, and the patterns between words.",
    "start": "239841",
    "end": "245699"
  },
  {
    "text": "And then the result of all of this is then output, which is a text output,",
    "start": "245960",
    "end": "252837"
  },
  {
    "text": "whether that's answering a question or summarizing a document or completing a sentence,",
    "start": "252837",
    "end": "257939"
  },
  {
    "text": "but vision language models introduce something new and that new thing that they introduce is an image input.",
    "start": "258660",
    "end": "268329"
  },
  {
    "text": "So, now we have to process this.",
    "start": "269289",
    "end": "272149"
  },
  {
    "text": "A photo, a graph, or anything else we want the model to understand, but there is a challenge here.",
    "start": "272630",
    "end": "278209"
  },
  {
    "text": "Large language models don't work with raw images, they only work with text tokens.",
    "start": "279170",
    "end": "285310"
  },
  {
    "text": "So, before the LLM can process an image, it first needs to be",
    "start": "285610",
    "end": "289806"
  },
  {
    "text": "converted into a format it can understand and that's where a vision encoder comes into play.",
    "start": "289807",
    "end": "299529"
  },
  {
    "text": "So unlike an LLM which tokenizes words, the vision encoder processes images as high-dimensional numerical data.",
    "start": "300954",
    "end": "310649"
  },
  {
    "text": "Now, it doesn't see the images the way that we do,",
    "start": "310830",
    "end": "314444"
  },
  {
    "text": "but instead it extracts things from So it extract patterns and edges and textures and spatial relationships,",
    "start": "314444",
    "end": "321178"
  },
  {
    "text": "and it's converting them into something called a feature vector, or in fact a bunch of feature vectors. ",
    "start": "321178",
    "end": "330350"
  },
  {
    "text": "Now that's a structured representation of the image's contents,",
    "start": "331250",
    "end": "336320"
  },
  {
    "text": "and this feature vector is essentially a dense embedding capturing the most relevant information from the image",
    "start": "336320",
    "end": "342678"
  },
  {
    "text": "while discarding all of the unnecessary details, kind of similar to how an LLM converts text into word embeddings.",
    "start": "342678",
    "end": "349330"
  },
  {
    "text": "So our images are now vectors, but these vectors can't be fed into a large language model directly either.",
    "start": "349850",
    "end": "357329"
  },
  {
    "text": "That's why we need an additional stage which is a projector.",
    "start": "357330",
    "end": "363169"
  },
  {
    "text": "Now this component maps the continuous image embeddings into a token based format.",
    "start": "364990",
    "end": "371089"
  },
  {
    "text": "So this gives us image tokens and that aligns with the text representation used by the LLM.",
    "start": "371470",
    "end": "380829"
  },
  {
    "text": "So at this stage, we now have image tokens and we have text tokens.",
    "start": "380890",
    "end": "386730"
  },
  {
    "text": "Both existing in the same latent space and these are then fed into the large",
    "start": "387230",
    "end": "391836"
  },
  {
    "text": "language model which processes them together using its attention mechanisms,",
    "start": "391836",
    "end": "396176"
  },
  {
    "text": "analyzing how different tokens relate to one another regardless of whether they originate from",
    "start": "396176",
    "end": "401667"
  },
  {
    "text": "text or if they originates from an image,",
    "start": "401667",
    "end": "405156"
  },
  {
    "text": "which gives us a text-based response generated by the model",
    "start": "405541",
    "end": "410007"
  },
  {
    "text": "whether that's a caption or an explanation of what's in an image",
    "start": "410007",
    "end": "414050"
  },
  {
    "text": "or an answer to a question that requires interpreting both the visual and the textual content.",
    "start": "414050",
    "end": "420358"
  },
  {
    "text": "In essence, a vision language model has extended an LLM",
    "start": "421020",
    "end": "425233"
  },
  {
    "text": "by introducing a multi-modal tokenization pipeline,",
    "start": "425233",
    "end": "429894"
  },
  {
    "text": "one that allows images to be represented in a way that text-based transformers can process natively.",
    "start": "429894",
    "end": "436999"
  },
  {
    "text": "So all this sounds great, but VLMs are not without their challenges.",
    "start": "437560",
    "end": "441419"
  },
  {
    "text": "So, for example, consider...",
    "start": "442100",
    "end": "444859"
  },
  {
    "text": "Tokenization bottlenecks.",
    "start": "445080",
    "end": "447720"
  },
  {
    "text": "Text tokenization is efficient because models break down words into sub-tokens but images,",
    "start": "448120",
    "end": "453604"
  },
  {
    "text": "well, they lack a natural token structure.",
    "start": "453605",
    "end": "456699"
  },
  {
    "text": "They must first be encoded and an encoded image",
    "start": "456780",
    "end": "460210"
  },
  {
    "text": "often requires quite a lot of tokens which increases the memory usage and it can slow down inference.",
    "start": "460210",
    "end": "468059"
  },
  {
    "text": "Now some models do incorporate optimization strategies like a perceiver resampler,",
    "start": "468480",
    "end": "472880"
  },
  {
    "text": "but the fact remains that processing images remains more computationally intensive than processing text alone.",
    "start": "473480",
    "end": "481600"
  },
  {
    "text": "Now, similar to traditional LLMs, vision language models,",
    "start": "482240",
    "end": "486236"
  },
  {
    "text": "they can produce hallucinations, generating responses that sound plausible but are factually incorrect,",
    "start": "486236",
    "end": "495100"
  },
  {
    "text": "and this happens because VLMs don't really see the images as humans do, they are",
    "start": "495140",
    "end": "499833"
  },
  {
    "text": "learning statistical associations about them.",
    "start": "499833",
    "end": "502660"
  },
  {
    "text": "That can lead to incorrect assumptions about objects in an image.",
    "start": "503020",
    "end": "506559"
  },
  {
    "text": "For instance, a VLM trained predominantly on internet-scale datasets may misinterpret medical images",
    "start": "507120",
    "end": "513151"
  },
  {
    "text": "if it hasn't been exposed to sufficient labeled medical data.",
    "start": "513151",
    "end": "517200"
  },
  {
    "text": "And this issue can also extend to the graphs and charts as well because",
    "start": "517799",
    "end": "521442"
  },
  {
    "text": "even models find tuned on specific datasets may still struggle with accuracy when they're interpreting complex visual data.",
    "start": "521442",
    "end": "529556"
  },
  {
    "text": "And then, another issue common to lots of AI is bias in training data.",
    "start": "529556",
    "end": "536539"
  },
  {
    "text": "So VLMs are often trained on massive data sets scraped from the web,",
    "start": "537120",
    "end": "541166"
  },
  {
    "text": "meaning they inherit the biases present in those data sets.",
    "start": "541166",
    "end": "545280"
  },
  {
    "text": "So for example, models trained on Western-centric data sets may misinterpret cultural artifacts from non-Western contexts.",
    "start": "545620",
    "end": "553299"
  },
  {
    "text": "So addressing these biases means basically being careful about data set curation.",
    "start": "553820",
    "end": "559499"
  },
  {
    "text": "So that's vision language models.",
    "start": "559500",
    "end": "563829"
  },
  {
    "text": "With them, LLMs do more than read.",
    "start": "564290",
    "end": "567128"
  },
  {
    "text": "They can see, they can interpret, and they can reason about the world in ways a bit more like we do visually.",
    "start": "567670",
    "end": "575928"
  }
]