[
  {
    "text": "Superalignment refers to the challenge of making sure that future AI systems,",
    "start": "493",
    "end": "4838"
  },
  {
    "text": "meaning systems with super intelligent capabilities,",
    "start": "4838",
    "end": "8426"
  },
  {
    "text": "act in accordance with human values and intentions.",
    "start": "8426",
    "end": "12639"
  },
  {
    "text": "Now today, alignment, just the regular non-super kind,",
    "start": "12740",
    "end": "17630"
  },
  {
    "text": "that helps ensure that AI chatbots and the like aren't perpetuating human bias or being exploited by bad actors,",
    "start": "17630",
    "end": "24898"
  },
  {
    "text": "but as AI becomes more advanced, its outputs become more difficult to anticipate and align with human intent.",
    "start": "24900",
    "end": "33460"
  },
  {
    "text": "Now that actually has a name.",
    "start": "33820",
    "end": "36479"
  },
  {
    "text": "That is called the alignment problem,",
    "start": "36620",
    "end": "40559"
  },
  {
    "text": "and the more intelligent that these AI systems become, this problem could become bigger.",
    "start": "42180",
    "end": "48200"
  },
  {
    "text": "So let's consider we have intelligence and it's going up over time.",
    "start": "48200",
    "end": "55659"
  },
  {
    "text": "Now today, we're at the level called ANI.",
    "start": "56140",
    "end": "62320"
  },
  {
    "text": "That stands for artificial narrow intelligence,",
    "start": "62860",
    "end": "66339"
  },
  {
    "text": "and that includes LLMs, autonomous vehicles, recommendation engines, basically the AI that we have today.",
    "start": "66460",
    "end": "73979"
  },
  {
    "text": "Then the next level up from that is AGI.",
    "start": "73980",
    "end": "78799"
  },
  {
    "text": "That's artificial general intelligence.",
    "start": "79280",
    "end": "81819"
  },
  {
    "text": "It's theoretical, but if ever realized, we'd be able to complete all cognitive tasks, as well as any human expert using AGI.",
    "start": "82160",
    "end": "90540"
  },
  {
    "text": "And then at the top of the tree is ASI, artificial super intelligence,",
    "start": "90780",
    "end": "98019"
  },
  {
    "text": "and ASI systems would have an intellectual scope that goes beyond human level intelligence,",
    "start": "98020",
    "end": "103959"
  },
  {
    "text": "and if we have ASI, then we'd better make sure we have a pretty good superalignment strategy in place to manage it.",
    "start": "104980",
    "end": "112219"
  },
  {
    "text": "So let me give you three reasons why we need superalignment,",
    "start": "112331",
    "end": "117159"
  },
  {
    "text": "and then we'll we'll get into some of the techniques.",
    "start": "117220",
    "end": "119300"
  },
  {
    "text": "OK, so reason number one is loss of control.",
    "start": "120040",
    "end": "125560"
  },
  {
    "text": "Super intelligent AI systems may become so advanced",
    "start": "125560",
    "end": "129313"
  },
  {
    "text": "that their decision making processes outstrip our ability to understand them.",
    "start": "129313",
    "end": "134000"
  },
  {
    "text": "When an ASI pursues its objectives with superhuman efficiency,",
    "start": "134260",
    "end": "138657"
  },
  {
    "text": "even the smallest, teeniest, tiniest misalignment could lead to catastrophic unintended outcomes.",
    "start": "138658",
    "end": "145280"
  },
  {
    "text": "Now there's also strategic deception.",
    "start": "146140",
    "end": "149759"
  },
  {
    "text": "So even if an ASI system appears to be aligned, we need to ask ourselves a question,",
    "start": "150280",
    "end": "156339"
  },
  {
    "text": "is it really?",
    "start": "156980",
    "end": "158539"
  },
  {
    "text": "Because the system could strategically fake alignment,",
    "start": "158940",
    "end": "162940"
  },
  {
    "text": "masking its true objectives until it's acquired enough power or enough resources for its own goals,",
    "start": "162940",
    "end": "168800"
  },
  {
    "text": "and even some of today's AI models, the ANI models, they have engaged in primitive levels of alignment faking.",
    "start": "169160",
    "end": "176220"
  },
  {
    "text": "So, well, we'd better watch out.",
    "start": "176340",
    "end": "178528"
  },
  {
    "text": "Then there's self preservation.",
    "start": "178740",
    "end": "182259"
  },
  {
    "text": "So ASI systems might develop power seeking behaviors",
    "start": "182703",
    "end": "186683"
  },
  {
    "text": "for preserving their own existence that go far beyond their primary human given objectives.",
    "start": "186683",
    "end": "192265"
  },
  {
    "text": "Now, none of this is desirable.",
    "start": "192720",
    "end": "195380"
  },
  {
    "text": "In fact, it probably represents an existential risk to humanity.",
    "start": "195500",
    "end": "198839"
  },
  {
    "text": "So what can we do about it?",
    "start": "199720",
    "end": "201240"
  },
  {
    "text": "Well, fundamentally, superalignment has two goals.",
    "start": "201780",
    "end": "206419"
  },
  {
    "text": "The first of those is to have scalable oversight.",
    "start": "206420",
    "end": "211540"
  },
  {
    "text": "So that means methods that allow humans or even trusted AI systems actually",
    "start": "212240",
    "end": "217579"
  },
  {
    "text": "to supervise and then to provide high quality guidance",
    "start": "217579",
    "end": "221224"
  },
  {
    "text": "when the AI's complexity makes direct human evaluation just basically infeasible,",
    "start": "221225",
    "end": "226979"
  },
  {
    "text": "and the second goal is to make sure that you have a robust governance framework.",
    "start": "227260",
    "end": "234200"
  },
  {
    "text": "Now that framework ensures that even if an AI system becomes super intelligent,",
    "start": "235000",
    "end": "239458"
  },
  {
    "text": "it remains constrained to pursue objectives that are aligned with human values,",
    "start": "239458",
    "end": "245340"
  },
  {
    "text": "but that's all well and good, lofty goals,",
    "start": "245380",
    "end": "247940"
  },
  {
    "text": "but how do we achieve that?",
    "start": "247940",
    "end": "249680"
  },
  {
    "text": "Well, the techniques we use for alignment today often rely on a technology called RLHF.",
    "start": "250260",
    "end": "260178"
  },
  {
    "text": "That's an acronym for Reinforcement Learning from Human Feedback,",
    "start": "260180",
    "end": "265283"
  },
  {
    "text": "in which human evaluators provide feedback on the outputs of an AI model,",
    "start": "265283",
    "end": "270298"
  },
  {
    "text": "and then that feedback is used to train a reward model",
    "start": "270660",
    "end": "273775"
  },
  {
    "text": "that quantifies how well the model's responses align with the human preferences,",
    "start": "273775",
    "end": "277800"
  },
  {
    "text": "but for super intelligent systems, human feedback systems alone are just basically unlikely to be scalable enough.",
    "start": "278280",
    "end": "287519"
  },
  {
    "text": "We can't rely on this for ASI.",
    "start": "287520",
    "end": "290199"
  },
  {
    "text": "So one superalignment technique instead is called RLAIF.",
    "start": "290380",
    "end": "298740"
  },
  {
    "text": "That stands for Reinforcement Learning from AI Feedback.",
    "start": "299520",
    "end": "303860"
  },
  {
    "text": "So in RLAIF, AI models generate the feedback to train the reward functions",
    "start": "304020",
    "end": "309225"
  },
  {
    "text": "and that in turn helps align even more capable systems.",
    "start": "309226",
    "end": "312740"
  },
  {
    "text": "It turns out this is quite a promising area of study,",
    "start": "313300",
    "end": "316112"
  },
  {
    "text": "but we do have to consider that if the ASI system engages in alignment faking,",
    "start": "316113",
    "end": "322107"
  },
  {
    "text": "then relying solely on AI generated feedback might lead to further misalignment.",
    "start": "322107",
    "end": "328720"
  },
  {
    "text": "There are some other techniques as well.",
    "start": "329200",
    "end": "331420"
  },
  {
    "text": "For example, weak to strong generalization.",
    "start": "331720",
    "end": "335019"
  },
  {
    "text": "That's another approach where a relatively weak model,",
    "start": "335140",
    "end": "338228"
  },
  {
    "text": "perhaps one trained with human supervision, is then used to generate pseudo labels or training signals for a stronger model.",
    "start": "338228",
    "end": "345279"
  },
  {
    "text": "Now the stronger model learns to generalize the patterns from this trained weaker model",
    "start": "345280",
    "end": "349716"
  },
  {
    "text": "and then it can generate correct secure solutions in situations that the weaker model did not anticipate.",
    "start": "349716",
    "end": "354960"
  },
  {
    "text": "So effectively the stronger model learns to generalize beyond the limitations of its teacher.",
    "start": "355200",
    "end": "360339"
  },
  {
    "text": "Now one other technique is called scalable insight.",
    "start": "361190",
    "end": "366189"
  },
  {
    "text": "This is where a complex task is broken down into simpler subtasks that",
    "start": "366230",
    "end": "370847"
  },
  {
    "text": "humans or lower capability AI systems can more reliably evaluate.",
    "start": "370847",
    "end": "375750"
  },
  {
    "text": "Now that's called iterated amplification where a complex problem is broken down recursively.",
    "start": "375910",
    "end": "382389"
  },
  {
    "text": "Now given that we don't have ASI systems yet, superalignment is a largely uncharted research frontier.",
    "start": "383010",
    "end": "391149"
  },
  {
    "text": "Future research though is looking into things like distributional shift.",
    "start": "391890",
    "end": "396829"
  },
  {
    "text": "This is where alignment techniques are measured on how they perform when",
    "start": "396830",
    "end": "401021"
  },
  {
    "text": "an AI encounters tasks that wasn't covered during the training.",
    "start": "401022",
    "end": "404370"
  },
  {
    "text": "And there's also oversight scalability methods to amplify human or AI generated feedback",
    "start": "404850",
    "end": "412429"
  },
  {
    "text": "so that even in extremely complex tasks the supervisory signal remains robust.",
    "start": "412429",
    "end": "417770"
  },
  {
    "text": "It continues to listen to us.",
    "start": "418170",
    "end": "419689"
  },
  {
    "text": "So super alignment, it's really all about enhancing oversight.",
    "start": "420330",
    "end": "424289"
  },
  {
    "text": "It's about ensuring robust feedback and it's about anticipating emergent behaviors.",
    "start": "424290",
    "end": "430329"
  },
  {
    "text": "All of this for a technology that does not yet and might not ever actually exist,",
    "start": "430950",
    "end": "436510"
  },
  {
    "text": "but if artificial super intelligence really does emerge someday",
    "start": "436790",
    "end": "441165"
  },
  {
    "text": "we'll want to be very sure that systems that are smarter than any of us will still be aligned to our own human values.",
    "start": "441165",
    "end": "450550"
  }
]