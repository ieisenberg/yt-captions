[
  {
    "text": "Generative AI makes it faster than ever before to create chatbots.",
    "start": "350",
    "end": "3310"
  },
  {
    "text": "It used to take a lot of manual effort to craft conversational responses and flows,",
    "start": "3950",
    "end": "8085"
  },
  {
    "text": "but now, LLMs promise to cut some of the time and effort out of the build process by doing more of the work for us.",
    "start": "8085",
    "end": "13210"
  },
  {
    "text": "If we say goodbye to handcrafted answers, is that a good trade-off?",
    "start": "14000",
    "end": "16940"
  },
  {
    "text": "Today we'll discuss how to balance human and LLM control while building effective chatbots.",
    "start": "17730",
    "end": "22309"
  },
  {
    "text": "Let's look at how we used to build chatbots before generative AI.",
    "start": "23150",
    "end": "25809"
  },
  {
    "text": "We used to train chatbots to understand natural language through classifiers.",
    "start": "26540",
    "end": "29899"
  },
  {
    "text": "So we might have something like.",
    "start": "31110",
    "end": "32649"
  },
  {
    "text": "What time are you open?",
    "start": "33140",
    "end": "36320"
  },
  {
    "text": "And we would craft an hour's intent for this.",
    "start": "39080",
    "end": "41520"
  },
  {
    "text": "Classifiers are trained on examples, so we'd give multiple examples for this intent.",
    "start": "43930",
    "end": "47770"
  },
  {
    "text": "So maybe I would do when do you close?",
    "start": "47850",
    "end": "52699"
  },
  {
    "text": "And again, same intent.",
    "start": "53960",
    "end": "55140"
  },
  {
    "text": "and for this intent, I'd want to very carefully control the answer, and I would say something like We're open 8 a.m. to 8 p.m. every day.",
    "start": "56036",
    "end": "65650"
  },
  {
    "text": "I want people to really know what the answer is to that hour's question.",
    "start": "65880",
    "end": "69379"
  },
  {
    "text": "I would also train additional intent.",
    "start": "70740",
    "end": "72459"
  },
  {
    "text": "So maybe I would have one like,",
    "start": "72900",
    "end": "74099"
  },
  {
    "text": "How do I open an account?",
    "start": "74700",
    "end": "79750"
  },
  {
    "text": "This will go to an account intent.",
    "start": "82530",
    "end": "84629"
  },
  {
    "text": "And again, I would very carefully control what happens from that.",
    "start": "87040",
    "end": "90540"
  },
  {
    "text": "Maybe I would open the account for them, maybe I would give them some steps to follow.",
    "start": "90660",
    "end": "94719"
  },
  {
    "text": "But again, the point is I had a very strict control over what happened.",
    "start": "95390",
    "end": "99089"
  },
  {
    "text": "When I got this intent, no matter how it was asked.",
    "start": "99340",
    "end": "101600"
  },
  {
    "text": "Let's imagine how how this training kind of scaled out.",
    "start": "101913",
    "end": "106680"
  },
  {
    "text": "So I could plot a curve",
    "start": "106720",
    "end": "109250"
  },
  {
    "text": "For the kinds of questions my chat bot received.",
    "start": "109840",
    "end": "112200"
  },
  {
    "text": "I would use.",
    "start": "112280",
    "end": "113280"
  },
  {
    "text": "The number of times I get each question on one axis.",
    "start": "113580",
    "end": "116560"
  },
  {
    "text": "I would do the frequency of the questions on the other.",
    "start": "117110",
    "end": "121670"
  },
  {
    "text": "And when I do that...",
    "start": "122430",
    "end": "123430"
  },
  {
    "text": "It tends to look something like this.",
    "start": "123570",
    "end": "125769"
  },
  {
    "text": "So I've got a nice long curve here.",
    "start": "127120",
    "end": "128839"
  },
  {
    "text": "And at the top of the curve is the questions I get all the time, so I probably get that hours kind of question the most.",
    "start": "129470",
    "end": "137069"
  },
  {
    "text": "And then accounts,",
    "start": "137820",
    "end": "140210"
  },
  {
    "text": "I probably get that a lot, maybe not quite as much.",
    "start": "140250",
    "end": "142330"
  },
  {
    "text": "So let's say it's it's here, right?",
    "start": "142350",
    "end": "144360"
  },
  {
    "text": "It's still a very frequently asked question.",
    "start": "144500",
    "end": "146219"
  },
  {
    "text": "It's just that it's not the most frequently asked.",
    "start": "146760",
    "end": "149039"
  },
  {
    "text": "And then I'm gonna have a nice long curve of questions here and there'll be questions that I hardly ever get.",
    "start": "149980",
    "end": "155446"
  },
  {
    "text": "Like how do I use my gold card while traveling overseas?",
    "start": "155590",
    "end": "166230"
  },
  {
    "text": "Maybe I only get this question one time.",
    "start": "167370",
    "end": "169409"
  },
  {
    "text": "And as I progress through this curve, it gets harder and harder to train the classifier to understand these things.",
    "start": "170289",
    "end": "177040"
  },
  {
    "text": "There's actually a point.",
    "start": "177610",
    "end": "178930"
  },
  {
    "text": "Somewhere along the curve.",
    "start": "179399",
    "end": "180479"
  },
  {
    "text": "We'll call it here.",
    "start": "180620",
    "end": "181620"
  },
  {
    "text": "Where you've gone past the point of diminishing returns.",
    "start": "182282",
    "end": "186110"
  },
  {
    "text": "It gets so hard to train and tense that your chat bot starts not understanding.",
    "start": "186290",
    "end": "189989"
  },
  {
    "text": "And you end up seeing a lot of responses like...",
    "start": "190870",
    "end": "192790"
  },
  {
    "text": "Hey, I didn't understand that, or it starts answering the wrong question and just starts looking confused.",
    "start": "193110",
    "end": "197590"
  },
  {
    "text": "And this is a really poor experience for your users.",
    "start": "198260",
    "end": "200599"
  },
  {
    "text": "And so this is where a generative AI comes in.",
    "start": "201450",
    "end": "203250"
  },
  {
    "text": "Using retrievable augmented generation, we don't need to train any classifiers.",
    "start": "204400",
    "end": "208159"
  },
  {
    "text": "As long as the answer to our user's questions in the document repository used by the system,",
    "start": "209020",
    "end": "214300"
  },
  {
    "text": "the RAG system can answer any question.",
    "start": "214850",
    "end": "216989"
  },
  {
    "text": "The training is very generalized.",
    "start": "217410",
    "end": "218729"
  },
  {
    "text": "So the process would look something like this.",
    "start": "219510",
    "end": "221329"
  },
  {
    "text": "We've got a user here, there asking questions to the chat bot.",
    "start": "222460",
    "end": "226300"
  },
  {
    "text": "The chat bot is sending that question to a document repository,",
    "start": "227640",
    "end": "232800"
  },
  {
    "text": "that repository retrieves some documents that help,",
    "start": "238120",
    "end": "241698"
  },
  {
    "text": "and the bot sends those documents and the question",
    "start": "243160",
    "end": "248130"
  },
  {
    "text": "to the LLM.",
    "start": "250620",
    "end": "251620"
  },
  {
    "text": "The LLM summarizes the answer.",
    "start": "251760",
    "end": "253379"
  },
  {
    "text": "So it's a very generalized process.",
    "start": "254810",
    "end": "256449"
  },
  {
    "text": "And we have a user question converted into a search query.",
    "start": "257140",
    "end": "260419"
  },
  {
    "text": "The query returns some documents,",
    "start": "260970",
    "end": "262730"
  },
  {
    "text": "and then those retrieved or returned documents are augmented by the LLM in a generated answer.",
    "start": "263480",
    "end": "269160"
  },
  {
    "text": "And so with this pattern, the LLM can answer both the very frequent",
    "start": "270130",
    "end": "274149"
  },
  {
    "text": "and the infrequent questions.",
    "start": "274710",
    "end": "276089"
  },
  {
    "text": "And there's a real beautiful simplicity in this pattern.",
    "start": "277170",
    "end": "279569"
  },
  {
    "text": "There's only two configuration points.",
    "start": "280090",
    "end": "282809"
  },
  {
    "text": "So number one.",
    "start": "283480",
    "end": "284480"
  },
  {
    "text": "You have the tuning of the search, the query, the retrieval process.",
    "start": "285130",
    "end": "288910"
  },
  {
    "text": "And number two.",
    "start": "289500",
    "end": "290500"
  },
  {
    "text": "You have the tuning of the answer generation process.",
    "start": "291110",
    "end": "294129"
  },
  {
    "text": "Two points, very simple, very generalized, no intents,",
    "start": "294310",
    "end": "296889"
  },
  {
    "text": "but you lose some degree of control.",
    "start": "297550",
    "end": "299629"
  },
  {
    "text": "Remember that when people ask me when my store is open, I had a really particular answer in mind.",
    "start": "299850",
    "end": "306524"
  },
  {
    "text": "I wanted to give I want to make sure they got exactly this text and in this pattern I don't have that exact control anymore.",
    "start": "306524",
    "end": "312258"
  },
  {
    "text": "The LLM can't give me that guarantee.",
    "start": "312460",
    "end": "314279"
  },
  {
    "text": "So what's the answer?",
    "start": "315360",
    "end": "316360"
  },
  {
    "text": "It's a hybrid approach.",
    "start": "316790",
    "end": "318190"
  },
  {
    "text": "So we're going to use a traditional classifier part of the time.",
    "start": "318730",
    "end": "321810"
  },
  {
    "text": "And we're gonna use rag the other half of the time.",
    "start": "322230",
    "end": "324170"
  },
  {
    "text": "So if I draw this out.",
    "start": "325160",
    "end": "326279"
  },
  {
    "text": "It starts out the same.",
    "start": "327240",
    "end": "328560"
  },
  {
    "text": "My user's asking a question to the bot.",
    "start": "328990",
    "end": "330768"
  },
  {
    "text": "But the bot's making a decision.",
    "start": "331290",
    "end": "332829"
  },
  {
    "text": "Is this a question I see all the time?",
    "start": "333360",
    "end": "335000"
  },
  {
    "text": "In which case I'm going to go with my intents.",
    "start": "335490",
    "end": "337509"
  },
  {
    "text": "Slash the curated responses,",
    "start": "338730",
    "end": "340529"
  },
  {
    "text": "and if this is something I don't get that much,",
    "start": "342240",
    "end": "343899"
  },
  {
    "text": "I'm going to go with the rag pattern that I've shown up here.",
    "start": "344310",
    "end": "347110"
  },
  {
    "text": "And if we look at our original long tail curve, we can think of this.",
    "start": "347550",
    "end": "350970"
  },
  {
    "text": "Left hand side",
    "start": "351350",
    "end": "352350"
  },
  {
    "text": "as a kind of a cache.",
    "start": "353180",
    "end": "354339"
  },
  {
    "text": "Those questions we get all the time, you can pull right out of it,",
    "start": "356770",
    "end": "359548"
  },
  {
    "text": "right out of its internal memory.",
    "start": "360720",
    "end": "361919"
  },
  {
    "text": "We're not going to the LLM, we're not doing any of these searches, we're now worrying about tokens and inference time and all those things.",
    "start": "362270",
    "end": "368649"
  },
  {
    "text": "So using this side of the cache is very quick.",
    "start": "369220",
    "end": "372059"
  },
  {
    "text": "So find the balance between fully generated conversational responses and generative AI powered rags.",
    "start": "372650",
    "end": "377669"
  },
  {
    "text": "This will help you build effective conversational AI that delights your users as quickly as possible.",
    "start": "378200",
    "end": "383419"
  }
]