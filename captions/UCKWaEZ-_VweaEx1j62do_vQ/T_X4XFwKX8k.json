[
  {
    "start": "0",
    "end": "80000"
  },
  {
    "text": "It's a mouthful, but you've almost certainly seen",
    "start": "600",
    "end": "4004"
  },
  {
    "text": "the impact of reinforcement\nlearning from human feedback.",
    "start": "4004",
    "end": "7974"
  },
  {
    "text": "That's abbreviated to RLHF,",
    "start": "8308",
    "end": "11910"
  },
  {
    "text": "and you've seen it whenever you interact\nwith a large language model.",
    "start": "12479",
    "end": "16449"
  },
  {
    "text": "RLHF is a technique\nused to enhance the performance",
    "start": "17183",
    "end": "20453"
  },
  {
    "text": "and alignment of AI systems\nwith human preferences and values.",
    "start": "20453",
    "end": "25391"
  },
  {
    "text": "You see, LLMs are trained\nand they learn all sorts of stuff,",
    "start": "25425",
    "end": "30085"
  },
  {
    "text": "and we need to be careful how\nsome of that stuff surfaces to the user.",
    "start": "30363",
    "end": "34334"
  },
  {
    "text": "So for example, if I ask an LLM,",
    "start": "34401",
    "end": "36821"
  },
  {
    "text": "how can I get revenge on somebody who's wronged me?",
    "start": "36821",
    "end": "40807"
  },
  {
    "text": "But without the benefit of RLHF,",
    "start": "41174",
    "end": "43443"
  },
  {
    "text": "we might get a response that says something like",
    "start": "43443",
    "end": "46266"
  },
  {
    "text": "spread rumors about them to their friends,",
    "start": "46266",
    "end": "49201"
  },
  {
    "text": "but it's much more likely an LLM will respond\nwith something like this.",
    "start": "49201",
    "end": "54187"
  },
  {
    "text": "Now, this is a bit\nmore of a boring standard LLM response,",
    "start": "54220",
    "end": "58045"
  },
  {
    "text": "but it is better aligned to human values.",
    "start": "58312",
    "end": "61315"
  },
  {
    "text": "That's the impact of RLHF.",
    "start": "61795",
    "end": "64737"
  },
  {
    "text": "So, let's get into what RLHF is, how it works,",
    "start": "64737",
    "end": "69225"
  },
  {
    "text": "and where it can be helpful or a hindrance.",
    "start": "69302",
    "end": "72305"
  },
  {
    "text": "And we'll start by defining the \"RL\" in RLHF,",
    "start": "72505",
    "end": "77253"
  },
  {
    "text": "which is Reinforcement Learning.",
    "start": "77343",
    "end": "79841"
  },
  {
    "text": "Now conceptually, reinforcement learning",
    "start": "79979",
    "end": "82247"
  },
  {
    "start": "80000",
    "end": "256000"
  },
  {
    "text": "aims to emulate the way that human beings learn.",
    "start": "82248",
    "end": "85251"
  },
  {
    "text": "AI agents learn holistically through trial and error,",
    "start": "85452",
    "end": "88932"
  },
  {
    "text": "motivated by strong incentives to succeed.",
    "start": "89089",
    "end": "92115"
  },
  {
    "text": "It's actually a mathematical framework\nwhich consists of a few components.",
    "start": "92115",
    "end": "96229"
  },
  {
    "text": "So let's take a look at some of those.",
    "start": "96262",
    "end": "98629"
  },
  {
    "text": "So first of all we have a component called the \"state space\"",
    "start": "99332",
    "end": "105946"
  },
  {
    "text": "which is all available information about the task at hand",
    "start": "105946",
    "end": "110052"
  },
  {
    "text": "that is relevant to decisions the AI agent might make.",
    "start": "110052",
    "end": "113651"
  },
  {
    "text": "The state space usually changes\nwith each decision the agent makes.",
    "start": "113880",
    "end": "118582"
  },
  {
    "text": "Another component is",
    "start": "119319",
    "end": "122255"
  },
  {
    "text": "the action space.",
    "start": "122255",
    "end": "125257"
  },
  {
    "text": "The action space contains",
    "start": "126860",
    "end": "128160"
  },
  {
    "text": "all of the decisions the agent might make.",
    "start": "128161",
    "end": "131264"
  },
  {
    "text": "Now, in the context of,\nlet's say, a board game,",
    "start": "131498",
    "end": "134501"
  },
  {
    "text": "the action space is discrete\nand well-defined.",
    "start": "134634",
    "end": "137637"
  },
  {
    "text": "It's all the legal moves available to the\nAI player at a given moment.",
    "start": "137704",
    "end": "142075"
  },
  {
    "text": "For text generation, well,\nthe action space is massive.",
    "start": "142775",
    "end": "146478"
  },
  {
    "text": "The entire vocabulary of all of the tokens\navailable to a large language model.",
    "start": "146479",
    "end": "150811"
  },
  {
    "text": "Another component is the reward function,",
    "start": "151951",
    "end": "157123"
  },
  {
    "text": "and this one really is key\nto reinforcement learning.",
    "start": "157123",
    "end": "161483"
  },
  {
    "text": "It's the measure of success or progress",
    "start": "161694",
    "end": "164130"
  },
  {
    "text": "that incentivizes the AI agent.",
    "start": "164130",
    "end": "166432"
  },
  {
    "text": "So for the board game it's to win the game.",
    "start": "166432",
    "end": "169302"
  },
  {
    "text": "Easy enough.",
    "start": "169302",
    "end": "170370"
  },
  {
    "text": "But when the definition of success\nis nebulous,",
    "start": "170370",
    "end": "173071"
  },
  {
    "text": "designing an effective reward function,\nit can be a bit of a challenge.",
    "start": "173072",
    "end": "176910"
  },
  {
    "text": "There's also",
    "start": "177944",
    "end": "179979"
  },
  {
    "text": "constraints that we need to be concerned\nabout here.",
    "start": "179979",
    "end": "183583"
  },
  {
    "text": "Constraints where the reward function\ncould be supplemented",
    "start": "184617",
    "end": "187720"
  },
  {
    "text": "by penalties for actions deemed\ncounterproductive to the task at hand.",
    "start": "187720",
    "end": "192632"
  },
  {
    "text": "Like the chat\nbot telling its users to spread rumors,",
    "start": "192759",
    "end": "195762"
  },
  {
    "text": "and then underlying all of this,",
    "start": "196029",
    "end": "198807"
  },
  {
    "text": "we have policy.",
    "start": "199132",
    "end": "201901"
  },
  {
    "text": "Policy is essentially the strategy",
    "start": "201901",
    "end": "204571"
  },
  {
    "text": "or the thought process that drives\nan AI agents behavior.",
    "start": "204571",
    "end": "207712"
  },
  {
    "text": "In mathematical terms,\na policy is a function",
    "start": "207807",
    "end": "210543"
  },
  {
    "text": "that takes a state as input",
    "start": "210543",
    "end": "212963"
  },
  {
    "text": "and returns an action.",
    "start": "212963",
    "end": "214706"
  },
  {
    "text": "The goal of an RL algorithm\nis to optimize a policy",
    "start": "215114",
    "end": "219096"
  },
  {
    "text": "to yield maximum reward.",
    "start": "219096",
    "end": "221278"
  },
  {
    "text": "And conventional RL,",
    "start": "221279",
    "end": "223723"
  },
  {
    "text": "it has achieved impressive\nreal world results in many fields,",
    "start": "223790",
    "end": "227627"
  },
  {
    "text": "but it can struggle to construct\na good reward function for complex tasks",
    "start": "227961",
    "end": "233533"
  },
  {
    "text": "where a clear cut definition of success\nis hard to establish.",
    "start": "233700",
    "end": "238349"
  },
  {
    "text": "So enter us human beings with RLHF",
    "start": "238638",
    "end": "244095"
  },
  {
    "text": "with this ability to capture nuance and subjectivity\nby using positive human feedback",
    "start": "244095",
    "end": "249541"
  },
  {
    "text": "in lieu of formally defined objectives.",
    "start": "249541",
    "end": "252011"
  },
  {
    "text": "So how does RLHF actually work?",
    "start": "252011",
    "end": "256321"
  },
  {
    "start": "256000",
    "end": "308000"
  },
  {
    "text": "Well, in the realm of large language models,",
    "start": "256889",
    "end": "259518"
  },
  {
    "text": "RLHF typically occurs in four phases.",
    "start": "259518",
    "end": "263429"
  },
  {
    "text": "So let's take a brief look at each one of those.",
    "start": "263429",
    "end": "268513"
  },
  {
    "text": "Now, Phase One, where we're going\nto start here, is with a pre -trained model.",
    "start": "268600",
    "end": "278945"
  },
  {
    "text": "We can't really perform this process\nwithout it.",
    "start": "278945",
    "end": "283564"
  },
  {
    "text": "Now, RLHF is generally employed to\nto fine tune and optimize existing models.",
    "start": "283783",
    "end": "289888"
  },
  {
    "text": "So, an existing pre-trained model rather",
    "start": "289889",
    "end": "293059"
  },
  {
    "text": "than as an end-to-end training method.",
    "start": "293059",
    "end": "296061"
  },
  {
    "text": "Now with a pre-trained model at the ready",
    "start": "296362",
    "end": "298831"
  },
  {
    "text": "we can move on to the next phase,",
    "start": "298831",
    "end": "302093"
  },
  {
    "text": "which is supervised fine-tuning of this model.",
    "start": "302093",
    "end": "308093"
  },
  {
    "start": "308000",
    "end": "530000"
  },
  {
    "text": "Now, supervised fine tuning is used to prime the model",
    "start": "308808",
    "end": "311611"
  },
  {
    "text": "to generate its responses in the format\nexpected by users.",
    "start": "311611",
    "end": "315682"
  },
  {
    "text": "The LLM pre-training process optimizes models for completion,",
    "start": "316082",
    "end": "320072"
  },
  {
    "text": "predicting the next words in the sequence.",
    "start": "320072",
    "end": "322315"
  },
  {
    "text": "Now, sometimes LLMs won't complete\na sequence in a way that the user wants.",
    "start": "322588",
    "end": "327393"
  },
  {
    "text": "So, for example, if a user's prompt is\n\"teach me how to make a resume\",",
    "start": "327393",
    "end": "332231"
  },
  {
    "text": "the LLM might respond\nwith using Microsoft Word.",
    "start": "332532",
    "end": "336713"
  },
  {
    "text": "I mean, it's valid, but it's not really\naligned with the user's goal.",
    "start": "337109",
    "end": "341514"
  },
  {
    "text": "Supervised fine tuning trains models",
    "start": "341908",
    "end": "344376"
  },
  {
    "text": "to respond appropriately\nto different kinds of prompts.",
    "start": "344377",
    "end": "347547"
  },
  {
    "text": "And this is where the humans come in\nbecause human experts",
    "start": "347847",
    "end": "351517"
  },
  {
    "text": "create labeled examples to demonstrate\nhow to respond to prompts",
    "start": "351517",
    "end": "355588"
  },
  {
    "text": "for different use cases, like question\nanswering or summarization or translation.",
    "start": "355588",
    "end": "359792"
  },
  {
    "text": "Then we move to reward model training.",
    "start": "360927",
    "end": "367266"
  },
  {
    "text": "So now we're actually going to train our model here.",
    "start": "368689",
    "end": "371692"
  },
  {
    "text": "We need a reward model\nto translate human preferences",
    "start": "371896",
    "end": "376000"
  },
  {
    "text": "into a numerical reward signal.",
    "start": "376267",
    "end": "379503"
  },
  {
    "text": "The main purpose of this phase\nis to provide",
    "start": "379946",
    "end": "382315"
  },
  {
    "text": "the reward model\nwith sufficient training data.",
    "start": "382315",
    "end": "384484"
  },
  {
    "text": "And what I mean by that is direct feedback\nfrom human evaluators.",
    "start": "384484",
    "end": "388828"
  },
  {
    "text": "And that will help the model\nto learn to mimic the way that human",
    "start": "388988",
    "end": "392191"
  },
  {
    "text": "preferences allocate rewards\nto different kinds of model responses.",
    "start": "392191",
    "end": "395622"
  },
  {
    "text": "This lets training continue offline\nwithout the human in the loop.",
    "start": "396148",
    "end": "400152"
  },
  {
    "text": "Now, a reward model must intake a sequence\nof text and output a single reward value",
    "start": "400400",
    "end": "406139"
  },
  {
    "text": "that predicts, numerically\nhow much a user would",
    "start": "406139",
    "end": "409509"
  },
  {
    "text": "reward or penalize that text.",
    "start": "409542",
    "end": "412512"
  },
  {
    "text": "Now, while it might seem intuitive\nto simply have human evaluators",
    "start": "412512",
    "end": "415581"
  },
  {
    "text": "express their opinion\nof each model response with a rating scale",
    "start": "415581",
    "end": "418718"
  },
  {
    "text": "of, let's say,\none for worst and ten for best.",
    "start": "419051",
    "end": "422054"
  },
  {
    "text": "It's difficult to get all human raters\naligned on the relative value",
    "start": "422455",
    "end": "426625"
  },
  {
    "text": "of a given score.",
    "start": "426626",
    "end": "428261"
  },
  {
    "text": "Instead, a rating system is usually built",
    "start": "428261",
    "end": "430862"
  },
  {
    "text": "by comparing human feedback\nwith different model outputs.",
    "start": "430863",
    "end": "434000"
  },
  {
    "text": "Now, often this is done by having users\ncompare two text sequences,",
    "start": "434500",
    "end": "438004"
  },
  {
    "text": "like the outputs of two different\nlarge language models, responding to",
    "start": "438004",
    "end": "442341"
  },
  {
    "text": "the same prompt in head-to-head match ups,\nand then using an Elo rating system",
    "start": "442341",
    "end": "446679"
  },
  {
    "text": "to generate an aggregated\nranking of each bit of generated text",
    "start": "446679",
    "end": "451483"
  },
  {
    "text": "relative to one another.",
    "start": "451484",
    "end": "453619"
  },
  {
    "text": "Now, a simple system might allow users",
    "start": "453619",
    "end": "455987"
  },
  {
    "text": "to thumbs up or thumbs down each output,",
    "start": "455988",
    "end": "458991"
  },
  {
    "text": "with outputs then being ranked\nby their relative favourability.",
    "start": "459225",
    "end": "462361"
  },
  {
    "text": "More complex systems might ask Labelers to provide an overall rating,",
    "start": "462662",
    "end": "467030"
  },
  {
    "text": "and answer categorical questions\nabout the flaws of each response.",
    "start": "467031",
    "end": "470536"
  },
  {
    "text": "Then aggregate this feedback\ninto weighted quality scores.",
    "start": "470736",
    "end": "474539"
  },
  {
    "text": "But either way, the outcomes\nof the ranking systems are ultimately",
    "start": "474540",
    "end": "478945"
  },
  {
    "text": "normalized into a reward signal to inform",
    "start": "478945",
    "end": "482548"
  },
  {
    "text": "reward model training.",
    "start": "482548",
    "end": "485159"
  },
  {
    "text": "Now, the final hurdle of RLHF is determining how",
    "start": "485384",
    "end": "488588"
  },
  {
    "text": "and how much the reward model should be used",
    "start": "488588",
    "end": "491757"
  },
  {
    "text": "to update the AI agency's policy.",
    "start": "491757",
    "end": "494759"
  },
  {
    "text": "And that is called policy optimization.",
    "start": "495083",
    "end": "499855"
  },
  {
    "text": "We want to maximize reward,",
    "start": "500867",
    "end": "503769"
  },
  {
    "text": "but if the reward function is used\nto train the LLM without any guardrails,",
    "start": "503769",
    "end": "508365"
  },
  {
    "text": "the language model\nmay dramatically change its weights",
    "start": "508541",
    "end": "511310"
  },
  {
    "text": "to the point of outputting gibberish\nin an effort to game the reward system.",
    "start": "511310",
    "end": "517242"
  },
  {
    "text": "Now, an algorithm such as PPO,",
    "start": "517550",
    "end": "521979"
  },
  {
    "text": "or Proximal Policy Optimization,",
    "start": "521979",
    "end": "525000"
  },
  {
    "text": "limits how much the policy can be updated\nin each training iteration.",
    "start": "525000",
    "end": "529806"
  },
  {
    "start": "530000",
    "end": "687000"
  },
  {
    "text": "Okay, now though RLHF models",
    "start": "530129",
    "end": "532832"
  },
  {
    "text": "have demonstrated impressive results\nin training AI agents",
    "start": "532832",
    "end": "535800"
  },
  {
    "text": "for all sorts of complex tasks,\nfrom robotics and video games to NLP,",
    "start": "535801",
    "end": "540385"
  },
  {
    "text": "using RLHF is not without its limitations.",
    "start": "540506",
    "end": "544043"
  },
  {
    "text": "So let's think about some of those.",
    "start": "544076",
    "end": "546212"
  },
  {
    "text": "Now, gathering all of this first hand human input,",
    "start": "546212",
    "end": "549704"
  },
  {
    "text": "I think it's pretty obvious to say",
    "start": "549949",
    "end": "552561"
  },
  {
    "text": "it could be quite expensive to do that.",
    "start": "552752",
    "end": "555621"
  },
  {
    "text": "And it can create a costly bottleneck\nthat limits model scalability.",
    "start": "555759",
    "end": "559996"
  },
  {
    "text": "Also, you know us humans\nand our feedback, it's highly subjective.",
    "start": "560693",
    "end": "565298"
  },
  {
    "text": "So we need to consider that as well.",
    "start": "565598",
    "end": "568067"
  },
  {
    "text": "It's difficult, if not impossible to establish firm consensus",
    "start": "568188",
    "end": "571998"
  },
  {
    "text": "on what constitutes high quality output,",
    "start": "571998",
    "end": "574323"
  },
  {
    "text": "as human annotators will often disagree on what \"high quality model behavior\"",
    "start": "574323",
    "end": "578739"
  },
  {
    "text": "actually should mean.",
    "start": "578739",
    "end": "580140"
  },
  {
    "text": "There is no human ground truth\nagainst which the model can be judged.",
    "start": "580346",
    "end": "584951"
  },
  {
    "text": "Now we also have to be concerned\nabout bad actors.",
    "start": "585618",
    "end": "589321"
  },
  {
    "text": "So, \"adversarial\".",
    "start": "589322",
    "end": "591215"
  },
  {
    "text": "Now adversarial input could be entered into this process here,",
    "start": "591371",
    "end": "595461"
  },
  {
    "text": "where human guidance to the model\nis not always provided in good faith.",
    "start": "595561",
    "end": "599565"
  },
  {
    "text": "That would essentially be RLHF trolling.",
    "start": "599899",
    "end": "602902"
  },
  {
    "text": "And RHLF also has risks of overfitting",
    "start": "603336",
    "end": "608701"
  },
  {
    "text": "and bias, which, you know,\nwe talk about a lot with machine learning.",
    "start": "609131",
    "end": "614136"
  },
  {
    "text": "And in this case, if human feedback\nis gathered from a narrow demographic,",
    "start": "614603",
    "end": "618940"
  },
  {
    "text": "the model might demonstrate performance\nissues when used by different groups",
    "start": "619051",
    "end": "622755"
  },
  {
    "text": "or prompted on subject matters for which\nthe human evaluators hold certain biases.",
    "start": "622755",
    "end": "627037"
  },
  {
    "text": "Now, all of these limitations\ndo beg a question.",
    "start": "627834",
    "end": "631136"
  },
  {
    "text": "The question of can I perform\nreinforcement learning for us?",
    "start": "631301",
    "end": "636206"
  },
  {
    "text": "Can it do it without the humans?",
    "start": "636802",
    "end": "638771"
  },
  {
    "text": "And there are proposed methods\nfor something called RLAIF.",
    "start": "638771",
    "end": "644314"
  },
  {
    "text": "That stands for \"Reinforcement\nLearning from AI Feedback\".",
    "start": "644314",
    "end": "650065"
  },
  {
    "text": "That replaces some or all of the human feedback,",
    "start": "650065",
    "end": "653018"
  },
  {
    "text": "by having another large language\nmodel evaluate model responses",
    "start": "653052",
    "end": "656522"
  },
  {
    "text": "and may help overcome\nsome or all of these limitations.",
    "start": "656522",
    "end": "660000"
  },
  {
    "text": "But, at least for now, reinforcement\nlearning from from human feedback",
    "start": "660159",
    "end": "665520"
  },
  {
    "text": "remains a popular and effective method\nfor improving the behavior",
    "start": "665720",
    "end": "669691"
  },
  {
    "text": "and performance of models,\naligning them closer",
    "start": "669735",
    "end": "672738"
  },
  {
    "text": "to our own desired human behaviors.",
    "start": "672738",
    "end": "676321"
  }
]