[
  {
    "text": "With the huge amount of large language models out there today,",
    "start": "250",
    "end": "3178"
  },
  {
    "text": "it can be a bit overwhelming to choose the perfect one for your use case.",
    "start": "3178",
    "end": "6969"
  },
  {
    "text": "Plus, the decision you make might have an impact on the accuracy of your results, as well as cost and performance.",
    "start": "7270",
    "end": "13130"
  },
  {
    "text": "But don't worry.",
    "start": "13650",
    "end": "14389"
  },
  {
    "text": "In the next few minutes, I'll show you as a developer how",
    "start": "14390",
    "end": "16932"
  },
  {
    "text": "I independently evaluate different models, both proprietary and open source,",
    "start": "16932",
    "end": "21097"
  },
  {
    "text": "and walk you through different demos of model use cases,",
    "start": "21097",
    "end": "23777"
  },
  {
    "text": "like summarization, questioning and answering on your data, and more.",
    "start": "23777",
    "end": "27370"
  },
  {
    "text": "Now, some people start off by looking at benchmarks or leaderboards,",
    "start": "27870",
    "end": "31526"
  },
  {
    "text": "but for me, the biggest consideration for model selection is the problem that you're trying to solve.",
    "start": "31526",
    "end": "36350"
  },
  {
    "text": "Because while GPT and other SaaS-based models are an easy and fast way to begin prototyping,",
    "start": "36870",
    "end": "41566"
  },
  {
    "text": "many organizations need the full control,",
    "start": "41567",
    "end": "43936"
  },
  {
    "text": "customization, and flexibility that an open-source model like Llama or Mistral provides.",
    "start": "43936",
    "end": "48610"
  },
  {
    "text": "But no matter what you choose, you'll need to consider the performance, speed, and price of the model.",
    "start": "49050",
    "end": "54570"
  },
  {
    "text": "And there's a lot of tools to help out with this.",
    "start": "54790",
    "end": "56890"
  },
  {
    "text": "So, let's get started.",
    "start": "57440",
    "end": "58440"
  },
  {
    "text": "Here I'm starting off at artificial analysis, comparing the entire landscape of models, both proprietary and open source.",
    "start": "58540",
    "end": "64859"
  },
  {
    "text": "And you're probably gonna see some familiar names here, but something I do wanna note is that there are some trends.",
    "start": "65120",
    "end": "70220"
  },
  {
    "text": "For example, with higher intelligence, typically results in a higher price or higher cost.",
    "start": "70580",
    "end": "75680"
  },
  {
    "text": "While at the same time, smaller models might result in faster speeds and lower costs at the same time.",
    "start": "76140",
    "end": "81059"
  },
  {
    "text": "Let's take intelligence as an example.",
    "start": "81440",
    "end": "83620"
  },
  {
    "text": "So, the numbers that they calculated actually result from a variety of benchmarks on MMLU-Pro and similar evaluations.",
    "start": "84170",
    "end": "91269"
  },
  {
    "text": "But let's say that you're scaling things up to millions of queries to your model.",
    "start": "91470",
    "end": "95609"
  },
  {
    "text": "You probably don't need a PhD-level AI for a simple task like that.",
    "start": "95950",
    "end": "99349"
  },
  {
    "text": "But one of my favorite community-based platforms to evaluate models is",
    "start": "99630",
    "end": "103107"
  },
  {
    "text": "the Chatbot Arena Leaderboard by UC Berkeley and ALM Arena,",
    "start": "103108",
    "end": "107476"
  },
  {
    "text": "which combines over a million blind user votes on models to rank them and essentially provide a vibe score.",
    "start": "107476",
    "end": "114328"
  },
  {
    "text": "Because benchmarks sometimes can be reverse engineered by models,",
    "start": "115050",
    "end": "117779"
  },
  {
    "text": "the Chatbot Arena is a great way to understand what the general AI community thinks is the best model.",
    "start": "117779",
    "end": "123370"
  },
  {
    "text": "And this directly correlates to its abilities on reasoning, math, writing, and more.",
    "start": "123570",
    "end": "128688"
  },
  {
    "text": "Plus, let's say, for example, that you want to compare two different models.",
    "start": "129130",
    "end": "132169"
  },
  {
    "text": "You can actually do so in the interface.",
    "start": "132470",
    "end": "134110"
  },
  {
    "text": "For example, I tried out this prompt to write an example customer response for a bank in JSON,",
    "start": "134490",
    "end": "139357"
  },
  {
    "text": "and we're able to compare between Granite 8 billion and Llama 8 billion.",
    "start": "139357",
    "end": "143219"
  },
  {
    "text": "So it's pretty cool, right?",
    "start": "143460",
    "end": "144460"
  },
  {
    "text": "And finally, for simply open-source foundation and fine tune models,",
    "start": "144620",
    "end": "148364"
  },
  {
    "text": "the Open LLM Leaderboard has a wide variety of model metrics",
    "start": "148365",
    "end": "152841"
  },
  {
    "text": "and filters in order to understand which model might be best for your specific use case.",
    "start": "152841",
    "end": "157520"
  },
  {
    "text": "For example, if you have a GPU or you wanna run it locally on your machine",
    "start": "157820",
    "end": "161650"
  },
  {
    "text": "or even do real-time inferencing on a mobile or edge device.",
    "start": "161650",
    "end": "165258"
  },
  {
    "text": "So what's great is that you can easily select these filters and see the model directly on Hugging Face.",
    "start": "165700",
    "end": "170459"
  },
  {
    "text": "For example, the number three result here is the Granite model.",
    "start": "170910",
    "end": "173830"
  },
  {
    "text": "And on Hugging Face, you can understand the millions of models and",
    "start": "174190",
    "end": "177341"
  },
  {
    "text": "datasets that are on there and understand how you can use it on your machine.",
    "start": "177341",
    "end": "181190"
  },
  {
    "text": "Now, we've taken a look at the general model landscape here, but let's start testing these out locally with our data.",
    "start": "181350",
    "end": "187729"
  },
  {
    "text": "For example, this Granite model that we have here on Hugging Face.",
    "start": "187810",
    "end": "190929"
  },
  {
    "text": "In order to test out different models and their use cases, we're going to use Ollama,",
    "start": "191220",
    "end": "195552"
  },
  {
    "text": "which is a popular developer tool that enables everybody to run their own large language models on their own system.",
    "start": "195552",
    "end": "201280"
  },
  {
    "text": "It's open source, and it has a model repository, meaning that we can run chat,",
    "start": "201660",
    "end": "205855"
  },
  {
    "text": "vision, tool calling, and even a rag-embedding models locally.",
    "start": "205855",
    "end": "209558"
  },
  {
    "text": "So to start, we're going to run Granite, specifically that Granite 3.1 model that we took a look at earlier on Hugging Face.",
    "start": "210060",
    "end": "216979"
  },
  {
    "text": "And here, it's already quantized or optimized and compressed for our machine.",
    "start": "217550",
    "end": "222150"
  },
  {
    "text": "And we're going to give it a quick question to make sure it's running.",
    "start": "222530",
    "end": "224910"
  },
  {
    "text": "Talk like a pirate.",
    "start": "225450",
    "end": "227310"
  },
  {
    "text": "Let's make sure.",
    "start": "227910",
    "end": "228890"
  },
  {
    "text": "And there we go.",
    "start": "228890",
    "end": "229890"
  },
  {
    "text": "We've got a funny response from our model.",
    "start": "229930",
    "end": "231709"
  },
  {
    "text": "But now, with the model running locally on our machine, I want to use it with my",
    "start": "232070",
    "end": "235655"
  },
  {
    "text": "own data to understand what it can do and its possibilities.",
    "start": "235655",
    "end": "239010"
  },
  {
    "text": "We're going use RAG or retrieval augmented generation in order to do this.",
    "start": "239430",
    "end": "243430"
  },
  {
    "text": "Here's an open-source AI interface called Open WebUI.",
    "start": "243660",
    "end": "247199"
  },
  {
    "text": "And it's going to allow us to use a local model that we have running, for example, Granite,",
    "start": "247380",
    "end": "251823"
  },
  {
    "text": "with Ollama, or maybe any open AI-compatible API model remotely as well.",
    "start": "251823",
    "end": "257519"
  },
  {
    "text": "But let's think about it as an AI application, right?",
    "start": "257959",
    "end": "260319"
  },
  {
    "text": "The back end could be our model and model server.",
    "start": "260480",
    "end": "263139"
  },
  {
    "text": "And the front end could a user interface like this that allows us to take in",
    "start": "263400",
    "end": "267634"
  },
  {
    "text": "our own custom data, to search the web, or to build agentic applications all with AI.",
    "start": "267635",
    "end": "273279"
  },
  {
    "text": "So let's start off with RAG by attaching a file of something the model traditionally wouldn't know.",
    "start": "273880",
    "end": "278879"
  },
  {
    "text": "This is specific enterprise data, right? Stuff that the model wasn't trained on originally, and specifically about Marty McFly.",
    "start": "279300",
    "end": "286159"
  },
  {
    "text": "And we're going to provide this to the model and ask a specific question.",
    "start": "286600",
    "end": "290511"
  },
  {
    "text": "What happened to Marty McFly in the 1955 accident from the claim?",
    "start": "290511",
    "end": "294339"
  },
  {
    "text": "Now, traditionally, a model wouldn't know about this information.",
    "start": "295000",
    "end": "297720"
  },
  {
    "text": "But by using an embedding model in the background, as well as a vector database,",
    "start": "297740",
    "end": "302078"
  },
  {
    "text": "we're able to pull certain information from that source document",
    "start": "302480",
    "end": "306088"
  },
  {
    "text": "and even provide that in ah the citations here to have a clear source of truth for our model's answers.",
    "start": "306088",
    "end": "312740"
  },
  {
    "text": "So we're about to try out RAG here, but also different agentic functions as well.",
    "start": "313100",
    "end": "317100"
  },
  {
    "text": "And it's a great place to get started with your own unique data.",
    "start": "317340",
    "end": "319859"
  },
  {
    "text": "Now, let's say that you're building applications and you want to use a free coding assistant within your IDE.",
    "start": "320320",
    "end": "325019"
  },
  {
    "text": "Well, traditionally, you need to use a SAS offering or a specifically fine-tuned coding model.",
    "start": "325400",
    "end": "330439"
  },
  {
    "text": "But now, more recently, one model can now work with a variety of languages, including your code.",
    "start": "330870",
    "end": "335990"
  },
  {
    "text": "So, what I've set up here is Continue.",
    "start": "336510",
    "end": "338250"
  },
  {
    "text": "And it's an open-source and free extension from the VS Code marketplace or IntelliJ",
    "start": "338550",
    "end": "343112"
  },
  {
    "text": "and specified it to use a local model that I have running with Ollama, that Granite model from earlier.",
    "start": "343113",
    "end": "348429"
  },
  {
    "text": "So what we're able to do is to chat with our code base, explain entire files, and make edits for us.",
    "start": "348870",
    "end": "354410"
  },
  {
    "text": "So here, I think we should add comments and some quick documentation",
    "start": "354770",
    "end": "358190"
  },
  {
    "text": "on what this class is doing so that other developers can understand it as well.",
    "start": "358640",
    "end": "362060"
  },
  {
    "text": "So I'm going to ask to add java.comments describing the service.",
    "start": "362440",
    "end": "366100"
  },
  {
    "text": "And it's going to go in and add this necessary and important documentation",
    "start": "366400",
    "end": "370792"
  },
  {
    "text": "to my project inline and be able to ask me to approve it or to deny it.",
    "start": "370792",
    "end": "376360"
  },
  {
    "text": "So, I think it's pretty cool.",
    "start": "376380",
    "end": "377820"
  },
  {
    "text": "And it is a great way to use an AI model with your code base as well.",
    "start": "377840",
    "end": "382275"
  },
  {
    "text": "Okay great, so now you know the various ways to evaluate and test models,",
    "start": "382275",
    "end": "386804"
  },
  {
    "text": "both from online leaderboards and benchmarks, as well as from your own machine.",
    "start": "386804",
    "end": "391009"
  },
  {
    "text": "But remember, it all comes down to your use case,",
    "start": "391370",
    "end": "393711"
  },
  {
    "text": "and there's even hybrid approaches of using a more powerful model in conjunction with a small model on device.",
    "start": "393711",
    "end": "400029"
  },
  {
    "text": "But we're just getting started, because after experimenting with models comes the stage of building something great with AI.",
    "start": "400470",
    "end": "406589"
  },
  {
    "text": "Now, what are you working on these days?",
    "start": "406890",
    "end": "408689"
  },
  {
    "text": "Please let us know in the comments below. But as always, thank you so much for watching.",
    "start": "408690",
    "end": "412889"
  },
  {
    "text": "Be sure to leave a like if you learned something today and have a good one.",
    "start": "413170",
    "end": "416069"
  }
]