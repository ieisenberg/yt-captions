[
  {
    "text": "In this video, I'm going to show you how to build\na large language model app to chat with your own data.",
    "start": "80",
    "end": "3800"
  },
  {
    "text": "This is arguably the cheapest and the most efficient way\nto get started with LLMs for your own business.",
    "start": "3840",
    "end": "8240"
  },
  {
    "text": "But before we do that, I want to back up a little.",
    "start": "8240",
    "end": "10320"
  },
  {
    "text": "The technique that makes this work is called\nretrieval augmented generation, a fancy way of saying",
    "start": "10320",
    "end": "14280"
  },
  {
    "text": "we chuck in chunks of your data into a prompt\nand get the LLM to answer based on that context.",
    "start": "14280",
    "end": "18520"
  },
  {
    "text": "The first thing that we need to do is build an app to chat to.",
    "start": "18560",
    "end": "20680"
  },
  {
    "text": "There's a bunch of dependencies that I need to import.",
    "start": "20840",
    "end": "22760"
  },
  {
    "text": "They’re mainly from langchain, but there's a little streamlit\nand watsonx thrown in for good measure.",
    "start": "22760",
    "end": "26400"
  },
  {
    "text": "I'll explain these as I use them, so don't stress for now.",
    "start": "26440",
    "end": "28560"
  },
  {
    "text": "Streamlit has a great set of chat components,\nso I'm going to make the most of them.",
    "start": "28560",
    "end": "31439"
  },
  {
    "text": "Add in the chat_input component to hold the prompt\nand then display the user message using the chat message",
    "start": "31440",
    "end": "35760"
  },
  {
    "text": "component via markdown.",
    "start": "35760",
    "end": "37039"
  },
  {
    "text": "This means I can now see the messages showing up in the app. But it's only displaying the last message posted.",
    "start": "37040",
    "end": "42720"
  },
  {
    "text": "Not all of them. Easy fix. Create a streamlit session state variable.",
    "start": "42760",
    "end": "45839"
  },
  {
    "text": "I'll call the message and append\nall of the user prompts into it.",
    "start": "45920",
    "end": "49039"
  },
  {
    "text": "While at it, I'll save the role type, in this case\nuser, into the dictionary.",
    "start": "49040",
    "end": "52080"
  },
  {
    "text": "And then I can test it out. Hmm.",
    "start": "52080",
    "end": "54080"
  },
  {
    "text": "But the history doesn't show up.",
    "start": "54120",
    "end": "56160"
  },
  {
    "text": "Well, turns out\nI haven't printed out the historical messages... yet.",
    "start": "56160",
    "end": "59600"
  },
  {
    "text": "Loop through all the messages in the session state message\nvariable and use the chat message component to display them.",
    "start": "59720",
    "end": "64520"
  },
  {
    "text": "And wait, did I save the app?",
    "start": "64560",
    "end": "66640"
  },
  {
    "text": "Of course not! I'd never make a mistake like that.",
    "start": "66680",
    "end": "69400"
  },
  {
    "text": "Let's just try that again...",
    "start": "69480",
    "end": "70880"
  },
  {
    "text": "and look at this!",
    "start": "70880",
    "end": "72360"
  },
  {
    "text": "Historical prompts that have been passed through.",
    "start": "72360",
    "end": "74400"
  },
  {
    "text": "Whoop de do, Nick. Where's the LLM at?",
    "start": "74400",
    "end": "76440"
  },
  {
    "text": "Well, let's do it.",
    "start": "76440",
    "end": "77440"
  },
  {
    "text": "I'm going to use the langchain interface to watsonx AI. Why?",
    "start": "77440",
    "end": "80400"
  },
  {
    "text": "Well, it uses state of art large language models,\ndoesn't use your data to train, and it's built for business.",
    "start": "80400",
    "end": "85560"
  },
  {
    "text": "But that's just scraping the surface.",
    "start": "85560",
    "end": "87159"
  },
  {
    "text": "To do that, I'll create a credentials dictionary\nwith an API key and use the ML service URL.",
    "start": "87160",
    "end": "91600"
  },
  {
    "text": "You can create an API key from the IBM Cloud IAM menu, URLs\nfor different regions as shown on the screen right now.",
    "start": "91600",
    "end": "96960"
  },
  {
    "text": "Then the LLM! I'm using llam-2-70b-chat\nbecause I'm pretty fond of those furry buggers.",
    "start": "96960",
    "end": "101840"
  },
  {
    "text": "Pass through some decoding parameters and specify\nthe project ID from watsonx.",
    "start": "101840",
    "end": "105520"
  },
  {
    "text": "Now then the prompt through to the LLM and boom.",
    "start": "105520",
    "end": "108759"
  },
  {
    "text": "Wait, it looks like it's running,\nbut I need to show the LLM responses as well.",
    "start": "108800",
    "end": "113120"
  },
  {
    "text": "Easy enough with the streamlit chat message component.",
    "start": "113160",
    "end": "115400"
  },
  {
    "text": "Note the chat role for the LLM response is “assistant”\nrather than “user”.",
    "start": "115440",
    "end": "119360"
  },
  {
    "text": "This helps to differentiate the responses.",
    "start": "119640",
    "end": "121320"
  },
  {
    "text": "I'll render the prompt as marked down\nand save the message to the session state as well.",
    "start": "121320",
    "end": "124720"
  },
  {
    "text": "That way, the history is displayed in the app. And...",
    "start": "124840",
    "end": "127320"
  },
  {
    "text": "now it works.",
    "start": "127440",
    "end": "128960"
  },
  {
    "text": "Yeah, yeah, that's great.",
    "start": "128960",
    "end": "129720"
  },
  {
    "text": "But where's the custom data come into play?",
    "start": "129720",
    "end": "131400"
  },
  {
    "text": "Enter phase three.",
    "start": "131400",
    "end": "132760"
  },
  {
    "text": "I’ll add in a load_pdf function\nand specify the name of the PDF file,",
    "start": "132760",
    "end": "136080"
  },
  {
    "text": "then pass that to the langchain VectorstoreIndexCreator, \nand choose the embeddings function to use.",
    "start": "136160",
    "end": "140440"
  },
  {
    "text": "This basically\nchunks up the PDF and loads it into a vector database.",
    "start": "140440",
    "end": "143640"
  },
  {
    "text": "Chromadb in this case.",
    "start": "143640",
    "end": "144840"
  },
  {
    "text": "Wrapping it in the st.cache_resource\nfunction means that streamlit",
    "start": "144840",
    "end": "147959"
  },
  {
    "text": "doesn't need to reload it each time,\nwhich makes it a whole heap faster.",
    "start": "147960",
    "end": "150880"
  },
  {
    "text": "Anyway, I can then pass that index\nto the langchain RetrievalQA chain and swap out the base LLM",
    "start": "150960",
    "end": "155520"
  },
  {
    "text": "the Q&A chain using chain.run.",
    "start": "155520",
    "end": "157440"
  },
  {
    "text": "And we can now chat with our PDF!  In this case,\na PDF to do with generative API.",
    "start": "157440",
    "end": "161880"
  },
  {
    "text": "Meta, I know.",
    "start": "162000",
    "end": "164000"
  }
]