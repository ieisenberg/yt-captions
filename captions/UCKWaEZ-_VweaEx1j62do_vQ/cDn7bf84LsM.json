[
  {
    "text": "Hi, my name is Erica and I'm going to show you how to use LangChain for a simple RAG example In Python.",
    "start": "300",
    "end": "7071"
  },
  {
    "text": "Large language models, LLMs, can be great for answering lots of questions,",
    "start": "7071",
    "end": "11823"
  },
  {
    "text": "but sometimes the models don't have the most up to date information",
    "start": "11824",
    "end": "15834"
  },
  {
    "text": "and can answer some questions about recent events.",
    "start": "15834",
    "end": "18539"
  },
  {
    "text": "For example, I was reading this recent announcement about the UFC and IBM partnership",
    "start": "19140",
    "end": "24328"
  },
  {
    "text": "on IBM.com and wanted to ask a LLM about it.",
    "start": "24329",
    "end": "27749"
  },
  {
    "text": "But when I asked the IBM granite model to tell me about the UFC announcement from November 14th, 2024,",
    "start": "29760",
    "end": "36548"
  },
  {
    "text": "it didn't know what I was talking about and mentioned it was trained on a limited data set up to only 2021.",
    "start": "36548",
    "end": "42000"
  },
  {
    "text": "How do I give this LLM the most up to date information so it can answer my question.",
    "start": "42750",
    "end": "47489"
  },
  {
    "text": "The answer is RAG, retrieval augmented generation.",
    "start": "48210",
    "end": "51930"
  },
  {
    "text": "Let me show you how it works.",
    "start": "52290",
    "end": "53640"
  },
  {
    "text": "Typically we have our user asking the question to the LLM, which generates a response.",
    "start": "54780",
    "end": "59790"
  },
  {
    "text": "But as you just saw, the LLM didn't have the right information, the context, to answer my question.",
    "start": "60210",
    "end": "65849"
  },
  {
    "text": "So we need to add something in the middle between the question and the LLM.",
    "start": "67570",
    "end": "71260"
  },
  {
    "text": "First, we'll add a knowledge base to include the content we want the LLM to read.",
    "start": "73470",
    "end": "77400"
  },
  {
    "text": "In this case, it'll be the most up to date content from IBM.com pages about some IBM products and announcements.",
    "start": "77760",
    "end": "84269"
  },
  {
    "text": "Second, we'll set up a retriever to fetch the content from the knowledge base.",
    "start": "86100",
    "end": "89939"
  },
  {
    "text": "Third, we'll set up the LLM to be fed the content.",
    "start": "92060",
    "end": "95209"
  },
  {
    "text": "Fourth will establish a prompt with instructions to be able to ask the LLM questions.",
    "start": "97660",
    "end": "102969"
  },
  {
    "text": "The top search results from search and retrieval will also be gathered here.",
    "start": "103330",
    "end": "106840"
  },
  {
    "text": "Once we've completed these four steps, we can start asking our questions about the content in our knowledge base.",
    "start": "109330",
    "end": "115359"
  },
  {
    "text": "Our query is search for in our knowledge Base Vector store.",
    "start": "115690",
    "end": "118630"
  },
  {
    "text": "The top results are returned as contacts for the LLM.",
    "start": "118960",
    "end": "122260"
  },
  {
    "text": "And finally, the LLM generates a response.",
    "start": "122260",
    "end": "124780"
  },
  {
    "text": "I'll walk through all these steps again in the Jupyter Notebook, linked in the description to this video.",
    "start": "125290",
    "end": "130029"
  },
  {
    "text": "Before we can begin, we need to fetch an API key and project ID for our notebook.",
    "start": "131690",
    "end": "136189"
  },
  {
    "text": "You can get these credentials by following the steps in the video linked in the description below.",
    "start": "136640",
    "end": "141110"
  },
  {
    "text": "We also have a few libraries to use for this tutorial.",
    "start": "142380",
    "end": "144960"
  },
  {
    "text": "If you don't have these packages installed yet, you can solve this with a quick pip install,",
    "start": "146160",
    "end": "150929"
  },
  {
    "text": "and here we can import the packages.",
    "start": "154880",
    "end": "156889"
  },
  {
    "text": "Next,",
    "start": "158290",
    "end": "158920"
  },
  {
    "text": "save your watsonx ID an watsonx API key in a separate .EMV file.",
    "start": "158920",
    "end": "164918"
  },
  {
    "text": "Make sure it's in the same directory as this notebook.",
    "start": "165340",
    "end": "167709"
  },
  {
    "text": "I have my credentials saved already, so I'll import those over",
    "start": "168100",
    "end": "171471"
  },
  {
    "text": "from my .EMV file and save them in a dictionary called credentials.",
    "start": "171471",
    "end": "175450"
  },
  {
    "text": "Okay, now we can get started with the workflow.",
    "start": "176850",
    "end": "179610"
  },
  {
    "text": "First will gather the information from some IBM.com URLs to create a knowledge base as a vector store.",
    "start": "180240",
    "end": "186930"
  },
  {
    "text": "Let's establish URL's dictionary.",
    "start": "191630",
    "end": "193669"
  },
  {
    "text": "It's a Python dictionary that helps us map the 25 URLs from which we will be getting the content.",
    "start": "194030",
    "end": "200089"
  },
  {
    "text": "You can see at the top here, I have the article about the UFC and IBM partnership I asked about before.",
    "start": "200600",
    "end": "207110"
  },
  {
    "text": "Let's also set up a name for our collection,",
    "start": "207740",
    "end": "210509"
  },
  {
    "text": "Ask IBM 2024.",
    "start": "210530",
    "end": "212479"
  },
  {
    "text": "Next, let's load our documents using the LangChain web based loader for the list of URLs we have.",
    "start": "213700",
    "end": "219668"
  },
  {
    "text": "Loaders load in data from a source and return a list of documents.",
    "start": "220330",
    "end": "224469"
  },
  {
    "text": "We'll print the page content of a sample document at the end to see how it's been loaded.",
    "start": "225100",
    "end": "229509"
  },
  {
    "text": "It can take a little while for it to finish loading,",
    "start": "231710",
    "end": "234020"
  },
  {
    "text": "and here's a sample document based on the sample document,",
    "start": "234560",
    "end": "237709"
  },
  {
    "text": "it looks like there's a lot of whitespace and newline characters that we can get rid of.",
    "start": "237980",
    "end": "241728"
  },
  {
    "text": "Let's clean that up with this code.",
    "start": "243330",
    "end": "244860"
  },
  {
    "text": "Let's see how our sample document looks now after we've cleaned it up.",
    "start": "248620",
    "end": "252009"
  },
  {
    "text": "Great. We've removed the whitespace successfully.",
    "start": "254280",
    "end": "256648"
  },
  {
    "text": "Before we vectorize our content.",
    "start": "258589",
    "end": "260208"
  },
  {
    "text": "We need to split it up into smaller, more manageable pieces known as chunks. LangChain's recursive character text splitter,",
    "start": "260209",
    "end": "268388"
  },
  {
    "text": "takes a large text and splits it based on a specified chunk size, meaning the number of characters.",
    "start": "268388",
    "end": "274189"
  },
  {
    "text": "In our case, we're going to go with a chunk size of 512.",
    "start": "274550",
    "end": "277789"
  },
  {
    "text": "Next, we need to instantiate an embedding model to vectorize our content.",
    "start": "278090",
    "end": "281869"
  },
  {
    "text": "In our case, we'll use IBM's Slate model,",
    "start": "282200",
    "end": "284839"
  },
  {
    "text": "and to finish off this step.",
    "start": "285410",
    "end": "286850"
  },
  {
    "text": "Let's load our content into a local instance of the vector database using Chroma.",
    "start": "287120",
    "end": "291680"
  },
  {
    "text": "We'll call it vector store.",
    "start": "292040",
    "end": "293389"
  },
  {
    "text": "The documents in the vector store will be made up of the docs we just chunked and they'll be embedded using the IBM Slate model.",
    "start": "293810",
    "end": "301100"
  },
  {
    "text": "For step two, we'll set up our vector store as a retriever.",
    "start": "301880",
    "end": "304999"
  },
  {
    "text": "The retrieved information from the vector Store, the content from the URLs",
    "start": "305390",
    "end": "310278"
  },
  {
    "text": "serves as additional context that the LLM will use to generate a response later in step four.",
    "start": "310278",
    "end": "317299"
  },
  {
    "text": "Code wise, all we need to do is set up our vector store as retriever.",
    "start": "318590",
    "end": "322610"
  },
  {
    "text": "For step three, we'll set up our generative LLM.",
    "start": "323450",
    "end": "326534"
  },
  {
    "text": "The generative model will use the retrieved information from step two to produce a relevant response to our questions.",
    "start": "326870",
    "end": "333350"
  },
  {
    "text": "First will establish which LLM we're going to use to generate the response.",
    "start": "333980",
    "end": "338119"
  },
  {
    "text": "For this tutorial, we'll use an IBM Granite model.",
    "start": "338840",
    "end": "341720"
  },
  {
    "text": "Next we'll set up the model parameters.",
    "start": "344220",
    "end": "346709"
  },
  {
    "text": "The model parameters available, and what they mean can be found in the description of this video.",
    "start": "346709",
    "end": "351299"
  },
  {
    "text": "And finally, in this step, we instantiate the LLM using watsonx.",
    "start": "352390",
    "end": "356859"
  },
  {
    "text": "In step four we'll set up our prompt which will combine our instructions,",
    "start": "356859",
    "end": "361598"
  },
  {
    "text": "the search results from step two, and our question to provide context to the LLM we just instantiated in step three.",
    "start": "361900",
    "end": "368800"
  },
  {
    "text": "First, let's set up instructions for the LLM.",
    "start": "369460",
    "end": "372970"
  },
  {
    "text": "We'll call it template because we'll also set up our prompt using a prompt template, and our instructions.",
    "start": "373450",
    "end": "380019"
  },
  {
    "text": "Let's also set up a helper function to format our docs to differentiate between individual page content.",
    "start": "381130",
    "end": "387279"
  },
  {
    "text": "Finally, as part of this step, we can set up a RAG chain with our search results for my retriever.",
    "start": "387460",
    "end": "392919"
  },
  {
    "text": "Our prompt, our helper function and our LLM.",
    "start": "393340",
    "end": "396360"
  },
  {
    "text": "Finally and step five and six, we can ask the other questions about our knowledge base.",
    "start": "397800",
    "end": "402720"
  },
  {
    "text": "The generative model will process the augmented context along with the user's question to produce a response.",
    "start": "403080",
    "end": "409078"
  },
  {
    "text": "First, let's ask our initial question.",
    "start": "410230",
    "end": "412389"
  },
  {
    "text": "Tell me about the UFC announcement from November 14th, 2024.",
    "start": "412810",
    "end": "417220"
  },
  {
    "text": "On November 14th, 2024, IBM and UFC announced a groundbreaking partnership,",
    "start": "418180",
    "end": "423651"
  },
  {
    "text": "and it looks like the model was able to answer a question this time.",
    "start": "423651",
    "end": "426819"
  },
  {
    "text": "Since it received the contacts from the UFC article, we fed it.",
    "start": "427210",
    "end": "430509"
  },
  {
    "text": "Next, let's ask about watsonx.data",
    "start": "432070",
    "end": "435190"
  },
  {
    "text": "What is watsonx.data?",
    "start": "436770",
    "end": "438689"
  },
  {
    "text": "watsonx.data is a service offered by IBM that enables users",
    "start": "439260",
    "end": "443484"
  },
  {
    "text": "to connect to various data sources and manage metadata for creating data products.",
    "start": "443484",
    "end": "448259"
  },
  {
    "text": "Looks good.",
    "start": "448740",
    "end": "449740"
  },
  {
    "text": "And finally, let's ask about watsonx.ai",
    "start": "450240",
    "end": "453703"
  },
  {
    "text": "What does watsonx.ai do?",
    "start": "454610",
    "end": "456750"
  },
  {
    "text": "watsonx.ai is a comprehensive AI platform that enables users to build, deploy and manage AI applications.",
    "start": "458370",
    "end": "465990"
  },
  {
    "text": "It was also able to respond to our watsonx.ai question.",
    "start": "466350",
    "end": "469740"
  },
  {
    "text": "Feel free to experiment with even more questions about the IBM offerings",
    "start": "470670",
    "end": "474832"
  },
  {
    "text": "and technologies discussed in the 25 articles you loaded into the knowledge base.",
    "start": "474832",
    "end": "479399"
  }
]