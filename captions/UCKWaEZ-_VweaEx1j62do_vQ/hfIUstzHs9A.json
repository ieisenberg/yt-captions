[
  {
    "start": "0",
    "end": "32000"
  },
  {
    "text": "Over the past couple of months, large language models, or LLMs, such as chatGPT, have taken the world by storm.",
    "start": "180",
    "end": "7980"
  },
  {
    "text": "Whether it's writing poetry or helping plan your upcoming vacation, we are seeing a step change in the performance of AI and its potential to drive enterprise value.",
    "start": "8790",
    "end": "19170"
  },
  {
    "text": "My name is Kate Soule.",
    "start": "19980",
    "end": "20980"
  },
  {
    "text": "I'm a senior manager of business strategy at IBM Research,",
    "start": "21100",
    "end": "24358"
  },
  {
    "text": "and today I'm going to give a brief overview of this new field of AI that's emerging and how it can be used in a business setting to drive value.",
    "start": "24358",
    "end": "32039"
  },
  {
    "start": "32000",
    "end": "142000"
  },
  {
    "text": "Now, large language models are actually a part of a different class of models called foundation models.",
    "start": "32990",
    "end": "38420"
  },
  {
    "text": "Now, the term \"foundation models\" was actually first coined by a team from Stanford when they saw that the field of AI was converging to a new paradigm.",
    "start": "42640",
    "end": "51369"
  },
  {
    "text": "Where before AI applications were being built by training,",
    "start": "52150",
    "end": "56564"
  },
  {
    "text": "maybe a library of different AI models, where each AI model was trained on very task-specific data to perform very specific task.",
    "start": "56564",
    "end": "65469"
  },
  {
    "text": "They predicted that we were going to start moving to a new paradigm,",
    "start": "68240",
    "end": "74728"
  },
  {
    "text": "where we would have a foundational capability, or a foundation model, that would drive all of these same use cases and applications.",
    "start": "74728",
    "end": "83840"
  },
  {
    "text": "So the same exact applications that we were envisioning before with conventional AI, and the same model could drive any number of additional applications.",
    "start": "84160",
    "end": "93319"
  },
  {
    "text": "The point is that this model could be transferred to any number of tasks.",
    "start": "93340",
    "end": "97450"
  },
  {
    "text": "What gives this model the super power to be able to transfer to multiple different tasks and perform multiple different functions",
    "start": "98140",
    "end": "104372"
  },
  {
    "text": "is that it's been trained on a huge amount, in an unsupervised manner, on unstructured data.",
    "start": "104372",
    "end": "110470"
  },
  {
    "text": "And what that means, in the language domain, is basically I'll feed a bunch of sentences-- and I'm talking terabytes of data here --to train this model.",
    "start": "117030",
    "end": "127140"
  },
  {
    "text": "And the start of my sentence might be \"no use crying over spilled\" and the end of my sentence might be \"milk\".",
    "start": "127560",
    "end": "135629"
  },
  {
    "text": "And I'm trying to get my model to predict the last word of the sentence based off of the words that it saw before.",
    "start": "135990",
    "end": "141479"
  },
  {
    "start": "142000",
    "end": "264000"
  },
  {
    "text": "And it's this generative capability of the model-- predicting and generating the next word --based off of previous words that it's seen beforehand,",
    "start": "142110",
    "end": "149234"
  },
  {
    "text": "that is why that foundation models are actually a part of the field of AI called generative AI",
    "start": "149234",
    "end": "160000"
  },
  {
    "text": "because we're generating something new in this case, the next word in a sentence.",
    "start": "160000",
    "end": "165029"
  },
  {
    "text": "And even though these models are trained to perform, at its core, a generation past, predicting the next word in the sentence, we actually can take these models,",
    "start": "166710",
    "end": "175430"
  },
  {
    "text": "and if you introduce a small amount of labeled data to the equation, you can tune them to perform traditional NLP tasks-- things like classification, or",
    "start": "175430",
    "end": "186353"
  },
  {
    "text": "named-entity recognition --things that you don't normally associate as being a generative-based model or capability.",
    "start": "186353",
    "end": "192479"
  },
  {
    "text": "And this process is called tuning.",
    "start": "193490",
    "end": "195409"
  },
  {
    "text": "Where you can tune your foundation model by introducing a small amount of data,",
    "start": "196590",
    "end": "199875"
  },
  {
    "text": "you update the parameters of your model and now perform a very specific natural language task.",
    "start": "199876",
    "end": "204449"
  },
  {
    "text": "If you don't have data, or have only very few data points, you can still take these foundation models and they actually work very well in low-labeled data domains.",
    "start": "205320",
    "end": "218522"
  },
  {
    "text": "And in a process called prompting or prompt engineering, you can apply these models for some of those same exact tasks.",
    "start": "219260",
    "end": "230069"
  },
  {
    "text": "So an example of prompting a model to perform a classification task",
    "start": "230490",
    "end": "234915"
  },
  {
    "text": "might be you could give a model a sentence and then ask it a question: Does this sentence have a positive sentiment or negative sentiment?",
    "start": "234915",
    "end": "242849"
  },
  {
    "text": "The model's going to try and finish generating words in that sentence, and the next natural word in that sentence would be the answer to your classification problem,",
    "start": "243360",
    "end": "250402"
  },
  {
    "text": "which would respond either positive or negative, depending on where it estimated the sentiment of the sentence would be.",
    "start": "250402",
    "end": "257278"
  },
  {
    "text": "And these models work surprisingly well when applied to these new settings and domains.",
    "start": "257880",
    "end": "262079"
  },
  {
    "text": "Now, this is a lot of where the advantages of foundation models come into play.",
    "start": "263690",
    "end": "268550"
  },
  {
    "start": "264000",
    "end": "294000"
  },
  {
    "text": "So if we talk about the advantages, the chief advantage is the performance.",
    "start": "269000",
    "end": "276670"
  },
  {
    "text": "These models have seen so much data.",
    "start": "280650",
    "end": "282999"
  },
  {
    "text": "Again, data with a capital D-- terabytes of data --that by the time that they're applied to small tasks,",
    "start": "283020",
    "end": "289154"
  },
  {
    "text": "they can drastically outperform a model that was only trained on just a few data points.",
    "start": "289154",
    "end": "293759"
  },
  {
    "start": "294000",
    "end": "323000"
  },
  {
    "text": "The second advantage of these models are the productivity gains.",
    "start": "294420",
    "end": "298920"
  },
  {
    "text": "So just like I said earlier, through prompting or tuning, you need far less label data to get to task-specific model",
    "start": "304700",
    "end": "313128"
  },
  {
    "text": "than if you had to start from scratch because your model is taking advantage of all the unlabeled data that it saw in its pre-training when we created this generative task.",
    "start": "313128",
    "end": "322638"
  },
  {
    "start": "323000",
    "end": "353000"
  },
  {
    "text": "With these advantages, there are also some disadvantages that are important to keep in mind.",
    "start": "323800",
    "end": "327608"
  },
  {
    "text": "And the first of those is the compute cost.",
    "start": "334830",
    "end": "338335"
  },
  {
    "text": "So that penalty for having this model see so much data is that they're very expensive to train,",
    "start": "340810",
    "end": "347017"
  },
  {
    "text": "making it difficult for smaller enterprises to train a foundation model on their own.",
    "start": "347017",
    "end": "351910"
  },
  {
    "start": "353000",
    "end": "432000"
  },
  {
    "text": "They're also expensive-- by the time they get to a huge size, a couple billion parameters --they're also very expensive to run inference.",
    "start": "353224",
    "end": "361499"
  },
  {
    "text": "You might require multiple GPUs at a time just to host these models and run inference, making them a more costly method than traditional approaches.",
    "start": "361770",
    "end": "368819"
  },
  {
    "text": "The second disadvantage of these models is on the trustworthiness side.",
    "start": "370630",
    "end": "373869"
  },
  {
    "text": "So just like data is a huge advantage for these models, they've seen so much unstructured data, it also comes at a cost, especially in the domain like language.",
    "start": "374320",
    "end": "382560"
  },
  {
    "text": "A lot of these models are trained basically off of language data that's been scraped from the Internet.",
    "start": "382570",
    "end": "387579"
  },
  {
    "text": "And there's so much data that these models have been trained on.",
    "start": "388090",
    "end": "391179"
  },
  {
    "text": "Even if you had a whole team of human annotators, you wouldn't be able to go through",
    "start": "391930",
    "end": "395495"
  },
  {
    "text": "and actually vet every single data point to make sure that it wasn't biased and didn't contain hate speech or other toxic information.",
    "start": "395495",
    "end": "401768"
  },
  {
    "text": "And that's just assuming you actually know what the data is.",
    "start": "402040",
    "end": "405010"
  },
  {
    "text": "Often we don't even know-- for a lot of these open source models that have been posted",
    "start": "405010",
    "end": "408997"
  },
  {
    "text": "--what the exact datasets are that these models have been trained on leading to trustworthiness issues.",
    "start": "408997",
    "end": "414820"
  },
  {
    "text": "So IBM recognizes the huge potential of these technologies.",
    "start": "415660",
    "end": "419319"
  },
  {
    "text": "But my partners in IBM Research are working on multiple different innovations to try",
    "start": "419620",
    "end": "423781"
  },
  {
    "text": "and improve also the efficiency of these models and the trustworthiness and reliability of these models to make them more relevant in a business setting.",
    "start": "423782",
    "end": "431230"
  },
  {
    "start": "432000",
    "end": "473000"
  },
  {
    "text": "All of these examples that I've talked through so far have just been on the language side.",
    "start": "432760",
    "end": "436570"
  },
  {
    "text": "But the reality is, there are a lot of other domains that foundation models can be applied towards.",
    "start": "436570",
    "end": "441399"
  },
  {
    "text": "Famously, we've seen foundation models for vision --looking at models such as DALL-E 2, which takes text data, and that's then used to generate a custom image.",
    "start": "441910",
    "end": "452250"
  },
  {
    "text": "We've seen models for code with products like Copilot that can help complete code as it's being authored.",
    "start": "453240",
    "end": "459449"
  },
  {
    "text": "And IBM's innovating across all of these domains.",
    "start": "460350",
    "end": "463110"
  },
  {
    "text": "So whether it's language models that we're building into products like Watson Assistant and Watson Discovery,",
    "start": "463470",
    "end": "468876"
  },
  {
    "text": "vision models that we're building into products like Maximo Visual Inspection,",
    "start": "468876",
    "end": "473183"
  },
  {
    "start": "473000",
    "end": "526000"
  },
  {
    "text": "or Ansible code models that we're building with our partners at Red Hat under Project Wisdom.",
    "start": "473183",
    "end": "478440"
  },
  {
    "text": "We're innovating across all of these domains and more.",
    "start": "478800",
    "end": "481470"
  },
  {
    "text": "We're working on chemistry.",
    "start": "482010",
    "end": "483870"
  },
  {
    "text": "So, for example, we just published and released molformer, which is a foundation model to promote molecule discovery or different targeted therapeutics.",
    "start": "486530",
    "end": "495769"
  },
  {
    "text": "And we're working on models for climate change, building Earth Science Foundation models using geospatial data to improve climate research.",
    "start": "496490",
    "end": "506307"
  },
  {
    "text": "I hope you found this video both informative and helpful.",
    "start": "506308",
    "end": "509119"
  },
  {
    "text": "If you're interested in learning more, particularly how IBM is working to improve some of these disadvantages,",
    "start": "509510",
    "end": "515057"
  },
  {
    "text": "making foundation models more trustworthy and more efficient, please take a look at the links below.",
    "start": "515058",
    "end": "519529"
  },
  {
    "text": "Thank you.",
    "start": "519950",
    "end": "520950"
  }
]