[
  {
    "text": "When it comes to picking a large language model,  \nwe're spoiled for choice.",
    "start": "90",
    "end": "4080"
  },
  {
    "text": "Last time I checked,  \nthere was something like",
    "start": "4110",
    "end": "6871"
  },
  {
    "text": "700,000 different LLMs, \n or large language models, ",
    "start": "6871",
    "end": "12195"
  },
  {
    "text": "on Hugging Face.",
    "start": "12195",
    "end": "13865"
  },
  {
    "text": "Now, I'd like to cover just a couple of those.",
    "start": "14220",
    "end": "17375"
  },
  {
    "text": "Specifically the IBM Granite Foundation models.",
    "start": "17460",
    "end": "21171"
  },
  {
    "text": "But first, let's consider how to pick an \n enterprise grade foundation model,",
    "start": "21330",
    "end": "25943"
  },
  {
    "text": "meaning an LLM suitable for  \ndeployment in an enterprise setting.",
    "start": "25943",
    "end": "30059"
  },
  {
    "text": "Something you'd be happy to run \n your business with.",
    "start": "30330",
    "end": "32909"
  },
  {
    "text": "So let's consider that through three \n different metrics.",
    "start": "33180",
    "end": "37370"
  },
  {
    "text": "So the foundation model, it needs \n to be performant.",
    "start": "37410",
    "end": "42380"
  },
  {
    "text": "That's an important metric,",
    "start": "42510",
    "end": "44519"
  },
  {
    "text": "but it also needs to be cost effective",
    "start": "44520",
    "end": "49189"
  },
  {
    "text": "and it needs to be trusted.",
    "start": "49189",
    "end": "53030"
  },
  {
    "text": "Those are the three metrics for you to consider,",
    "start": "53120",
    "end": "55468"
  },
  {
    "text": "and trusted, of course, because you  \ncan't scale generative AI with models",
    "start": "55469",
    "end": "59773"
  },
  {
    "text": "that you cannot trust.",
    "start": "59773",
    "end": "61240"
  },
  {
    "text": "So take these one by one.",
    "start": "61418",
    "end": "63627"
  },
  {
    "text": "Now, by performant,",
    "start": "63830",
    "end": "66647"
  },
  {
    "text": "we're talking about measurements \n like latency and throughput.",
    "start": "66647",
    "end": "69949"
  },
  {
    "text": "Is the foundation model able to keep up",
    "start": "70100",
    "end": "72937"
  },
  {
    "text": "with the speed an enterprise requires \n it to operate there?",
    "start": "72937",
    "end": "76726"
  },
  {
    "text": "And then related to that is cost effectiveness.",
    "start": "77262",
    "end": "82046"
  },
  {
    "text": "Now, according to the scientific journal Nature,",
    "start": "82310",
    "end": "84980"
  },
  {
    "text": "a search that's driven by generative AI",
    "start": "84981",
    "end": "88424"
  },
  {
    "text": "will use something like 4 to 5 times the amount of energy",
    "start": "88424",
    "end": "92362"
  },
  {
    "text": "that's needed to run a conventional web search.",
    "start": "92362",
    "end": "94729"
  },
  {
    "text": "So we need a foundation model that \n can deliver the necessary performance",
    "start": "94760",
    "end": "99290"
  },
  {
    "text": "with low inferencing costs.",
    "start": "99290",
    "end": "101548"
  },
  {
    "text": "And we need the foundation model to be trusted.",
    "start": "102000",
    "end": "106530"
  },
  {
    "text": "And we can gauge that through metrics \n like hallucination scores,",
    "start": "106880",
    "end": "109873"
  },
  {
    "text": "but also a model that offers transparency.",
    "start": "109873",
    "end": "112279"
  },
  {
    "text": "So we know what data the model was trained on.",
    "start": "112280",
    "end": "115544"
  },
  {
    "text": "And I think in many instances,  \nmodels are kind of skewed a bit like this.",
    "start": "115670",
    "end": "123000"
  },
  {
    "text": "They're highly performant, but  \nthey're expensive to run at inference time.",
    "start": "123110",
    "end": "129000"
  },
  {
    "text": "And there's a lack of transparency on  \nthe training data the model was built with.",
    "start": "129110",
    "end": "134265"
  },
  {
    "text": "Now, with the Granite models,",
    "start": "134540",
    "end": "136260"
  },
  {
    "text": "IBM set out to create enterprise  \ngrade foundation models",
    "start": "136261",
    "end": "139561"
  },
  {
    "text": "that apply an equal weight to all  \nthree of these metrics.",
    "start": "139561",
    "end": "144000"
  },
  {
    "text": "So it looks more like this.",
    "start": "144190",
    "end": "147737"
  },
  {
    "text": "So what should you know about the \n IBM Granite Foundation models?",
    "start": "148328",
    "end": "151597"
  },
  {
    "text": "Well, many of the models are open source.",
    "start": "151722",
    "end": "155364"
  },
  {
    "text": "You can find them on Hugging Face",
    "start": "155364",
    "end": "157095"
  },
  {
    "text": "under the Apache 2.0 license",
    "start": "157095",
    "end": "159216"
  },
  {
    "text": "that enables broad commercial usage.",
    "start": "159216",
    "end": "162163"
  },
  {
    "text": "Now, these models also have \n transparency in training data,",
    "start": "162351",
    "end": "167284"
  },
  {
    "text": "meaning we actually know the data sources \n that were used to train the models,",
    "start": "167284",
    "end": "171060"
  },
  {
    "text": "and that's quite atypical.",
    "start": "171061",
    "end": "173145"
  },
  {
    "text": "Most LLMs are notoriously vague",
    "start": "173271",
    "end": "177129"
  },
  {
    "text": "on how their models were trained,  \nso that's a nice change.",
    "start": "177129",
    "end": "180663"
  },
  {
    "text": "Now, Granite language models are  \ntrained on trusted enterprise data",
    "start": "180817",
    "end": "184832"
  },
  {
    "text": "spanning academic code, legal \n and finance data sources.",
    "start": "184832",
    "end": "189271"
  },
  {
    "text": "Such as?",
    "start": "189446",
    "end": "190625"
  },
  {
    "text": "Well, the first 13 billion parameter Granite LLM",
    "start": "190688",
    "end": "195469"
  },
  {
    "text": "was trained on about 6.5TB of data,",
    "start": "195469",
    "end": "201853"
  },
  {
    "text": "and that includes 1.8 million scientific papers",
    "start": "201854",
    "end": "206229"
  },
  {
    "text": "that were posted on archive.",
    "start": "206229",
    "end": "209145"
  },
  {
    "text": "It also includes all U.S. utility patents \n granted by the USPTO,",
    "start": "209348",
    "end": "216293"
  },
  {
    "text": "and that's from 1975 all the way  \nthrough to 2023.",
    "start": "216293",
    "end": "220502"
  },
  {
    "text": "And it includes the public domain free law,",
    "start": "220614",
    "end": "225648"
  },
  {
    "text": "which are legal opinions from  \nUS federal and state courts.",
    "start": "226287",
    "end": "230582"
  },
  {
    "text": "Essentially, the models have been \n governed and filtered",
    "start": "230736",
    "end": "233605"
  },
  {
    "text": "to only use enterprise safe data sources.",
    "start": "233606",
    "end": "237589"
  },
  {
    "text": "The Granite models have also been \n designed to be performant as well,",
    "start": "237817",
    "end": "242870"
  },
  {
    "text": "especially in areas of coding and \n language tasks,",
    "start": "242870",
    "end": "247170"
  },
  {
    "text": "outperforming some models that \n are actually twice their size.",
    "start": "247171",
    "end": "250440"
  },
  {
    "text": "And smaller models means also  \nthey're more efficient",
    "start": "250568",
    "end": "255273"
  },
  {
    "text": "with less compute requirements and  \na lower cost of inferencing.",
    "start": "255273",
    "end": "260069"
  },
  {
    "text": "Now I keep mentioning the Granite models, plural,",
    "start": "260360",
    "end": "263655"
  },
  {
    "text": "so which models are we talking about?",
    "start": "263655",
    "end": "266564"
  },
  {
    "text": "So Granite is actually a family of  \nLLM Foundation models",
    "start": "266767",
    "end": "270578"
  },
  {
    "text": "spanning multiple modalities.",
    "start": "270578",
    "end": "273000"
  },
  {
    "text": "And you can find many of these on Hugging Face.",
    "start": "273085",
    "end": "276590"
  },
  {
    "text": "So let's take a look at some of them.",
    "start": "276673",
    "end": "278310"
  },
  {
    "text": "And we'll start with Granite for Language.",
    "start": "278424",
    "end": "282854"
  },
  {
    "text": "Now, these are decoder models  \nof different parameter sizes.",
    "start": "283022",
    "end": "288000"
  },
  {
    "text": "So that includes a 7B open source model,",
    "start": "288140",
    "end": "292578"
  },
  {
    "text": "and the \"B\" here refers to billions of parameters.",
    "start": "292578",
    "end": "296197"
  },
  {
    "text": "So 7 billion parameters.",
    "start": "296197",
    "end": "298100"
  },
  {
    "text": "There's also an 8B model",
    "start": "298326",
    "end": "302220"
  },
  {
    "text": "that's designed specifically for  \nJapanese text.",
    "start": "302220",
    "end": "304920"
  },
  {
    "text": "There's a couple of 13B models",
    "start": "305181",
    "end": "308828"
  },
  {
    "text": "and there is a 20 billion parameter  \nmultilingual model",
    "start": "308828",
    "end": "314535"
  },
  {
    "text": "that supports English, German, Spanish, \n French and Portuguese.",
    "start": "314535",
    "end": "318500"
  },
  {
    "text": "Now there's also Granite for Code.",
    "start": "319073",
    "end": "323677"
  },
  {
    "text": "And that again comes in different parameter sizes",
    "start": "324407",
    "end": "327701"
  },
  {
    "text": "from 3 billion all the way through to  \n34 billion parameters.",
    "start": "327701",
    "end": "334210"
  },
  {
    "text": "And Granite for Code is trained on  \n116 programing languages.",
    "start": "334397",
    "end": "340589"
  },
  {
    "text": "Now there's also Granite for Time Series.",
    "start": "340826",
    "end": "347151"
  },
  {
    "text": "That's a family of pre-trained models \n for time series forecasting.",
    "start": "347313",
    "end": "351440"
  },
  {
    "text": "These models are trained on a  \ncollection of data sets",
    "start": "351693",
    "end": "353892"
  },
  {
    "text": "spanning a range of business and \n industrial application domains.",
    "start": "353892",
    "end": "357458"
  },
  {
    "text": "And these models are optimized to run  \non pretty much anything, even a laptop.",
    "start": "357543",
    "end": "361989"
  },
  {
    "text": "And then finally there is Granite for Geo Spatial,",
    "start": "362459",
    "end": "368154"
  },
  {
    "text": "which is a partnership between NASA and IBM",
    "start": "368154",
    "end": "371910"
  },
  {
    "text": "to create a foundation model for Earth observations",
    "start": "371910",
    "end": "374507"
  },
  {
    "text": "using large scale satellites and remote sensing data.",
    "start": "374507",
    "end": "378149"
  },
  {
    "text": "So that's the IBM Granite models,",
    "start": "378349",
    "end": "381000"
  },
  {
    "text": "models that are trusted, performant and efficient,",
    "start": "381000",
    "end": "383972"
  },
  {
    "text": "and that can be applied to a wide variety",
    "start": "383972",
    "end": "386642"
  },
  {
    "text": "of enterprise use cases.",
    "start": "386642",
    "end": "388829"
  }
]