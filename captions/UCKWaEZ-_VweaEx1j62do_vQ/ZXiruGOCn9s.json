[
  {
    "text": "No, it's not those transformers.",
    "start": "1440",
    "end": "4259"
  },
  {
    "text": "But they can do some pretty cool things, let me show you.",
    "start": "4470",
    "end": "7229"
  },
  {
    "text": "So why did the banana across the road?",
    "start": "7410",
    "end": "11639"
  },
  {
    "text": "Because it was sick of being mashed!",
    "start": "12360",
    "end": "14242"
  },
  {
    "text": "Yeah, I'm not sure I quite get that one.",
    "start": "15340",
    "end": "18239"
  },
  {
    "text": "And that's because it was created by a computer.",
    "start": "18480",
    "end": "22160"
  },
  {
    "text": "I literally asked it to tell me a joke.",
    "start": "22170",
    "end": "24690"
  },
  {
    "text": "And this is what it came up with.",
    "start": "25260",
    "end": "26550"
  },
  {
    "text": "Specifically, I used a GPT-3, or a generative pre-trained transformer model.",
    "start": "27510",
    "end": "35399"
  },
  {
    "text": "The 3 here means that this is the third generation.",
    "start": "35490",
    "end": "38340"
  },
  {
    "text": "GPT-3 is an autoregressive language model",
    "start": "39120",
    "end": "42369"
  },
  {
    "text": "that produces text that looks like it was written by a human.",
    "start": "42369",
    "end": "46409"
  },
  {
    "text": "GPT-3 can write poetry, craft emails and evidently come up with its own jokes.",
    "start": "47430",
    "end": "53391"
  },
  {
    "text": "Off you go.",
    "start": "54420",
    "end": "54920"
  },
  {
    "text": "Now, while our banana joke isn't exactly funny,",
    "start": "55950",
    "end": "59582"
  },
  {
    "text": "it does fit the typical pattern of a joke with a set-up and a punch line and sort of, kind of makes sense.",
    "start": "59582",
    "end": "64489"
  },
  {
    "text": "I mean, who wouldn't cross the road to avoid getting mashed?",
    "start": "64500",
    "end": "67409"
  },
  {
    "text": "But look, GPT-3 is just one example of a transformer.",
    "start": "67860",
    "end": "72375"
  },
  {
    "text": "Something that transforms from one sequence into another.",
    "start": "77840",
    "end": "83298"
  },
  {
    "text": "And language translation is just a great example.",
    "start": "83600",
    "end": "86450"
  },
  {
    "text": "Perhaps we want to take our sentence of,",
    "start": "86840",
    "end": "90602"
  },
  {
    "text": "\"Why did the banana cross the road?\",",
    "start": "90602",
    "end": "100480"
  },
  {
    "text": "and we want to take that English phrase and translate it into French.",
    "start": "100481",
    "end": "107770"
  },
  {
    "text": "Well, transformers consist of two parts.",
    "start": "108490",
    "end": "111559"
  },
  {
    "text": "There is an encoder, and there is a decoder.",
    "start": "111580",
    "end": "118580"
  },
  {
    "text": "The encoder works on the input sequence,",
    "start": "123360",
    "end": "128636"
  },
  {
    "text": "and the decoder operates on the target output sequence.",
    "start": "128637",
    "end": "134969"
  },
  {
    "text": "Now, on the face of it, translation seems like little more than just like a basic lookup task,",
    "start": "136560",
    "end": "141756"
  },
  {
    "text": "so convert the \"why\" here in our English sentence to the French equivalent, \"pourquoi\".",
    "start": "141756",
    "end": "149000"
  },
  {
    "text": "But of course, language translation doesn't really work that way.",
    "start": "150570",
    "end": "155650"
  },
  {
    "text": "Things like word order and turns of phrase often mix things up.",
    "start": "155670",
    "end": "159840"
  },
  {
    "text": "And the way Transformers work is through sequence-to-sequence learning",
    "start": "160320",
    "end": "164949"
  },
  {
    "text": "where the Transformer takes a sequence of tokens,",
    "start": "164949",
    "end": "168592"
  },
  {
    "text": "in this case words in a sentence,",
    "start": "168592",
    "end": "170870"
  },
  {
    "text": "and predicts the next word in the output sequence.",
    "start": "170870",
    "end": "174478"
  },
  {
    "text": "It does this through iterating through encoder layers,",
    "start": "175020",
    "end": "178325"
  },
  {
    "text": "so the encoder generates encodings",
    "start": "178325",
    "end": "181474"
  },
  {
    "text": "that define which part of the input sequence are relevant to each other",
    "start": "181474",
    "end": "186463"
  },
  {
    "text": "and then passes these encodings to the next encoder layer.",
    "start": "186463",
    "end": "189479"
  },
  {
    "text": "The decoder takes all of these encodings and uses their derived context",
    "start": "189870",
    "end": "194866"
  },
  {
    "text": "to generate the output sequence.",
    "start": "194867",
    "end": "196439"
  },
  {
    "text": "Now, transformers are a form of semi-supervised learning.",
    "start": "197220",
    "end": "203249"
  },
  {
    "text": "By \"semi-supervised\", we mean that they are pre-trained in an unsupervised manner",
    "start": "210640",
    "end": "216725"
  },
  {
    "text": "with a large, unlabeled data set,",
    "start": "216725",
    "end": "219711"
  },
  {
    "text": "and then they're fine tuned through supervised training to get them to perform better.",
    "start": "219711",
    "end": "225400"
  },
  {
    "text": "Now, in previous videos, I've talked about other machine learning algorithms",
    "start": "226180",
    "end": "229743"
  },
  {
    "text": "that handle sequential input like natural language.",
    "start": "229743",
    "end": "232448"
  },
  {
    "text": "For example, there are recurrent neural networks, or RRNs.",
    "start": "232780",
    "end": "235324"
  },
  {
    "text": "What makes Transformers a little bit different",
    "start": "237550",
    "end": "240658"
  },
  {
    "text": "is that they do not necessarily process data in order.",
    "start": "240658",
    "end": "243610"
  },
  {
    "text": "Transformers use something called an attention mechanism.",
    "start": "244270",
    "end": "248740"
  },
  {
    "text": "And this provides context around items in the input sequence,",
    "start": "252170",
    "end": "256147"
  },
  {
    "text": "so rather than starting our translation with the word \"why\" because it's at the start of the sentence,",
    "start": "256147",
    "end": "261285"
  },
  {
    "text": "the Transformer attempts to identify the context that bring meaning in each word in the sequence.",
    "start": "261285",
    "end": "267320"
  },
  {
    "text": "And it's this attention mechanism that gives Transformers a huge leg up",
    "start": "267890",
    "end": "272119"
  },
  {
    "text": "over algorithms like RNN that must run in sequence.",
    "start": "272119",
    "end": "275089"
  },
  {
    "text": "Transformers run multiple sequences in parallel.",
    "start": "275690",
    "end": "281290"
  },
  {
    "text": "And this vastly speeds up training times.",
    "start": "282820",
    "end": "285579"
  },
  {
    "text": "So beyond translations, what can Transformers be applied to?",
    "start": "286240",
    "end": "290180"
  },
  {
    "text": "Well, document summaries, they're another great example.",
    "start": "290200",
    "end": "293079"
  },
  {
    "text": "You can like feed in a whole article as the input sequence",
    "start": "293290",
    "end": "297551"
  },
  {
    "text": "and then generate an output sequence",
    "start": "297551",
    "end": "300869"
  },
  {
    "text": "that's going to really just be a couple of sentences that summarize the main points.",
    "start": "300869",
    "end": "305500"
  },
  {
    "text": "Transformers can create whole new documents of their own, for example, like write a whole blog post.",
    "start": "306220",
    "end": "311320"
  },
  {
    "text": "And beyond just language, Transformers have done things like learn to play chess",
    "start": "311710",
    "end": "317000"
  },
  {
    "text": "and perform image processing that even rivals the capabilities of convolutional neural networks.",
    "start": "317001",
    "end": "322128"
  },
  {
    "text": "Look, Transformers are a powerful, deep learning model, and",
    "start": "323200",
    "end": "326630"
  },
  {
    "text": "thanks to how the attention that mechanism can be parallelized,",
    "start": "326630",
    "end": "329919"
  },
  {
    "text": "are getting better all the time.",
    "start": "329919",
    "end": "331269"
  },
  {
    "text": "And who knows?",
    "start": "331270",
    "end": "332110"
  },
  {
    "text": "Pretty soon, maybe they'll even be able to pull off banana jokes that are actually funny.",
    "start": "332410",
    "end": "337839"
  },
  {
    "text": "If you have any questions, please drop us a line below,",
    "start": "340060",
    "end": "342682"
  },
  {
    "text": "and if you want to see more videos like this in the future,",
    "start": "342683",
    "end": "345664"
  },
  {
    "text": "please like and subscribe.",
    "start": "345664",
    "end": "347290"
  },
  {
    "text": "Thanks for watching.",
    "start": "347860",
    "end": "349017"
  }
]