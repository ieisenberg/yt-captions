[
  {
    "text": "Data pipelines are the backbone of every data-driven company, but too many fail to scale properly.",
    "start": "200",
    "end": "6379"
  },
  {
    "text": "They crash under pressure or waste precious resources.",
    "start": "6820",
    "end": "9880"
  },
  {
    "text": "AI models and big data isn't going to wait for slow data pipelines.",
    "start": "10660",
    "end": "15180"
  },
  {
    "text": "They demand continuous real-time processing,",
    "start": "15900",
    "end": "18872"
  },
  {
    "text": "which requires pipelines that can scale to handle millions and even billions of records",
    "start": "18872",
    "end": "23899"
  },
  {
    "text": "without crashing or causing bottlenecks.",
    "start": "23899",
    "end": "26230"
  },
  {
    "text": "A robust data pipeline guarantees that high-quality data",
    "start": "26230",
    "end": "30463"
  },
  {
    "text": "is received on time, whether it's for AI model training or real-time predictions or analytics.",
    "start": "30463",
    "end": "36720"
  },
  {
    "text": "So let's talk about the top techniques that we can use to build highly efficient and resilient data pipelines",
    "start": "37400",
    "end": "44049"
  },
  {
    "text": "so that you can use them right away to solve your biggest data challenges.",
    "start": "44049",
    "end": "48880"
  },
  {
    "text": "So to put some context around what we'll be talking about today,",
    "start": "49320",
    "end": "52283"
  },
  {
    "text": "we're mostly going to be talking about data pipelines that are written in Python.",
    "start": "52283",
    "end": "57039"
  },
  {
    "text": "Specifically that use the pandas library.",
    "start": "58170",
    "end": "60449"
  },
  {
    "text": "Now there's tons of data technologies out there for different kinds of data flows and data pipelines.",
    "start": "60590",
    "end": "65769"
  },
  {
    "text": "You probably work with one today or you plan on building one.",
    "start": "66270",
    "end": "69409"
  },
  {
    "text": "But essentially all a data pipeline is, is an ETL process",
    "start": "69810",
    "end": "73675"
  },
  {
    "text": "or basically extract, transform and load which moves data from point A to point B.",
    "start": "73676",
    "end": "81150"
  },
  {
    "text": "First, let's explore a few different methodologies we can use to achieve memory optimization within a data pipeline.",
    "start": "81870",
    "end": "87909"
  },
  {
    "text": "This is one of the biggest issues that people have as they start developing their data pipelines that is probably working fine,",
    "start": "88510",
    "end": "95517"
  },
  {
    "text": "and then all of a sudden you get three times the amount of traffic, and now you're hitting",
    "start": "95517",
    "end": "99889"
  },
  {
    "text": "memory resource limitations, which essentially causes your pipeline to fail.",
    "start": "99889",
    "end": "104010"
  },
  {
    "text": "So let's go through a few ways where we can actually look at our data pipelines,",
    "start": "104430",
    "end": "109750"
  },
  {
    "text": "and put in some memory optimization.",
    "start": "110170",
    "end": "112129"
  },
  {
    "text": "So one big thing that ends up blowing out your memory is basically how much data you're pulling into your pipeline.",
    "start": "113490",
    "end": "119129"
  },
  {
    "text": "So for that reason, we want to practice basically breaking up your data,",
    "start": "120110",
    "end": "128076"
  },
  {
    "text": "and you wanna basically do this at your extract or your read phase.",
    "start": "128550",
    "end": "131949"
  },
  {
    "text": "So as you're bringing in your data from your source database,",
    "start": "132570",
    "end": "137776"
  },
  {
    "text": "in all of our cases, that's gonna be A, you're bring it in, you have all your data.",
    "start": "137776",
    "end": "144650"
  },
  {
    "text": "This is where you might start to see memory allocation issues",
    "start": "145750",
    "end": "149671"
  },
  {
    "text": "because you just loaded in all that data and that's what can blow out your memory.",
    "start": "149671",
    "end": "154910"
  },
  {
    "text": "So the method we wanna use is called chunking.",
    "start": "155350",
    "end": "158270"
  },
  {
    "text": "So basically what you're gonna do is you're going to chunk your data into smaller pieces so that you're only looking at a subset.",
    "start": "158450",
    "end": "166030"
  },
  {
    "text": "And you can define this chunk of data on whatever methodology you want.",
    "start": "166110",
    "end": "170990"
  },
  {
    "text": "You can basically define it by the amount of physical memory that's taken up,",
    "start": "171150",
    "end": "176670"
  },
  {
    "text": "like so many gigs, or you can actually look at the number of rows or transactions that are involved.",
    "start": "176671",
    "end": "185190"
  },
  {
    "text": "So by breaking up your reads here, you then are gonna be able to optimize the stream going forward.",
    "start": "185930",
    "end": "191849"
  },
  {
    "text": "And this might take a little bit more time, but usually it's worth it for the trade-off of it being more resilient.",
    "start": "192330",
    "end": "198050"
  },
  {
    "text": "Now, this is gonna really improve again your extract phase, your read phase, and it'll lead into optimizing your transform.",
    "start": "198670",
    "end": "206330"
  },
  {
    "text": "However, don't forget about your load phase.",
    "start": "206930",
    "end": "209530"
  },
  {
    "text": "You wanna do the same thing when you're loading in all your data.",
    "start": "209850",
    "end": "213509"
  },
  {
    "text": "To then be brought in...",
    "start": "214120",
    "end": "215319"
  },
  {
    "text": "to your source.",
    "start": "220350",
    "end": "221350"
  },
  {
    "text": "So then basically what we have is you wanna mirror this kind of data breakage,",
    "start": "221570",
    "end": "227024"
  },
  {
    "text": "because if you just put it on your reads,",
    "start": "227024",
    "end": "229195"
  },
  {
    "text": "what's gonna happen is you're still just gonna hit your memory limit when you try and load all of that data all at once.",
    "start": "229195",
    "end": "235530"
  },
  {
    "text": "So make sure you're applying this logic here as well, so you don't wanna forget about your write phase in addition.",
    "start": "235930",
    "end": "242310"
  },
  {
    "text": "So from here, basically, you can have an end-to-end pipeline",
    "start": "243150",
    "end": "246344"
  },
  {
    "text": "and have it be much more resilient, so in that situation that I mentioned before we're now,",
    "start": "246344",
    "end": "250430"
  },
  {
    "text": "your volumes are three times bigger than you would expect, basically is gonna be able to handle them in smaller pieces",
    "start": "250910",
    "end": "256407"
  },
  {
    "text": "and you're never gonna have to redeploy your code in any of those kind of situations.",
    "start": "256408",
    "end": "261609"
  },
  {
    "text": "Now let's think about the data that we're actually moving through this pipeline",
    "start": "262370",
    "end": "266166"
  },
  {
    "text": "because the data itself also takes up memory.",
    "start": "266166",
    "end": "269189"
  },
  {
    "text": "So an area of opportunity that we can easily optimize is actually if you're working with a lot of string data.",
    "start": "269810",
    "end": "276009"
  },
  {
    "text": "We can actually transform them into different types of data types that can be processed much",
    "start": "276510",
    "end": "281792"
  },
  {
    "text": "faster in a more optimized way by Python and Pandas.",
    "start": "281792",
    "end": "286649"
  },
  {
    "text": "And this is essentially done by building out categories.",
    "start": "286990",
    "end": "291009"
  },
  {
    "text": "And basically what categories are is if you have limited data types,",
    "start": "295160",
    "end": "300381"
  },
  {
    "text": "so you know, instead of just saying it's a string, if we know that it's going to be",
    "start": "300381",
    "end": "306294"
  },
  {
    "text": "three different categories of data, it's always going to the same thing.",
    "start": "306294",
    "end": "310014"
  },
  {
    "text": "So, for example, if we know that it's always going to be just, you know, A, B, and C...",
    "start": "310014",
    "end": "316503"
  },
  {
    "text": "You can either do them just as strings, we can make them categories",
    "start": "320000",
    "end": "325755"
  },
  {
    "text": "and this data here is all the same but by putting it in this different data type",
    "start": "326628",
    "end": "333097"
  },
  {
    "text": "we actually the program itself can handle that kind of",
    "start": "333097",
    "end": "337124"
  },
  {
    "text": "data in a much more optimized way than kind of a mystery string that could be anything",
    "start": "337124",
    "end": "342840"
  },
  {
    "text": "so this is much more predictable it's easier to sort and work with data in this format",
    "start": "342840",
    "end": "348165"
  },
  {
    "text": "so then we can actually see how it's changing over time.",
    "start": "348165",
    "end": "351669"
  },
  {
    "text": "And so by, again, this is a great easy change that we can apply, that's going to help save a lot of memory overall.",
    "start": "352940",
    "end": "360019"
  },
  {
    "text": "So the other thing we want to, actually that's less of something that we should practice,",
    "start": "361160",
    "end": "366287"
  },
  {
    "text": "but more of something that we might want to avoid is actually avoiding some kind of recursive logic.",
    "start": "366287",
    "end": "372779"
  },
  {
    "text": "So we wanna avoid loops if we can.",
    "start": "373420",
    "end": "374960"
  },
  {
    "text": "And this isn't just to maybe you do need, there's real reason to add loops.",
    "start": "380210",
    "end": "383699"
  },
  {
    "text": "But a lot of times when these have been added in the past, it's to aggregate data of some kind.",
    "start": "384260",
    "end": "390399"
  },
  {
    "text": "Maybe you need to count something or group by something so that you're going to show the total sales for a given product.",
    "start": "390820",
    "end": "399139"
  },
  {
    "text": "And that's gonna happen somewhere in the middle of this pipeline.",
    "start": "399480",
    "end": "402199"
  },
  {
    "text": "Now you could do that by iterating through every line.",
    "start": "402800",
    "end": "406419"
  },
  {
    "text": "Pulling out every single product and basically incrementing a count each time.",
    "start": "407170",
    "end": "411549"
  },
  {
    "text": "So that's basically going to look like some kind of loop, you know, plus one each time,",
    "start": "412170",
    "end": "418043"
  },
  {
    "text": "that's really not going to be as optimal to do a count.",
    "start": "418043",
    "end": "421490"
  },
  {
    "text": "But what's good is that Pandas does offer great pre-built aggregation functions.",
    "start": "421710",
    "end": "428889"
  },
  {
    "text": "So, instead of this, we can basically just do a...",
    "start": "428890",
    "end": "432269"
  },
  {
    "text": "Like count function.",
    "start": "434630",
    "end": "435910"
  },
  {
    "text": "And this is gonna let all that optimization basically inherit from the program itself as opposed to you trying to write one.",
    "start": "436710",
    "end": "443789"
  },
  {
    "text": "And this also gonna take your code probably from this function probably when you had it in a loop.",
    "start": "444570",
    "end": "450089"
  },
  {
    "text": "It probably took about 10 lines.",
    "start": "451250",
    "end": "453449"
  },
  {
    "text": "This is basically gonna drive it all the way down to one.",
    "start": "453790",
    "end": "456129"
  },
  {
    "text": "So it's gonna make your code easier to read and easier to interpret.",
    "start": "456610",
    "end": "460990"
  },
  {
    "text": "But also it's going to basically be optimized.",
    "start": "461610",
    "end": "464250"
  },
  {
    "text": "You're going to inherit a lot of that good stuff that you just get from Python and Pandas overall.",
    "start": "464370",
    "end": "469669"
  },
  {
    "text": "So by following these methods, you can actually find a lot of great optimizations just from these basic implementations",
    "start": "470150",
    "end": "479754"
  },
  {
    "text": "so that we can see more resiliency and that our memory optimization shouldn't get blown out.",
    "start": "479754",
    "end": "486470"
  },
  {
    "text": "Of course, you should be monitoring your memory so you know if you're getting close to those memory limits",
    "start": "487070",
    "end": "491743"
  },
  {
    "text": "and you should keep an eye on it as you continue to maybe add more",
    "start": "491743",
    "end": "495847"
  },
  {
    "text": "complexity or maybe more transformation to your pipeline.",
    "start": "495847",
    "end": "498328"
  },
  {
    "text": "Now that we've covered memory efficiency, let's dive into failure control, which is vital for creating resilient data pipelines.",
    "start": "498910",
    "end": "506170"
  },
  {
    "text": "I want to encourage you to be in the mindset that your data pipeline is going to fail and that's okay.",
    "start": "506770",
    "end": "512089"
  },
  {
    "text": "We just need to make sure that it can restart automatically without manual intervention by you or a team member.",
    "start": "512539",
    "end": "519220"
  },
  {
    "text": "So all data pipelines should be ephemeral.",
    "start": "519800",
    "end": "522159"
  },
  {
    "text": "They're probably deployed on a containerized environment.",
    "start": "522380",
    "end": "525039"
  },
  {
    "text": "So they should be able to be spun up and spun down when they're needed.",
    "start": "525280",
    "end": "528299"
  },
  {
    "text": "So let's go through a few different optimization techniques",
    "start": "528920",
    "end": "532748"
  },
  {
    "text": "that we can apply here to make sure that our pipelines are ready to fail, but also ready to restart.",
    "start": "532749",
    "end": "539029"
  },
  {
    "text": "So one major area where we can Um, add in this optimization and resiliency",
    "start": "539029",
    "end": "545877"
  },
  {
    "text": "for failure is around the actual kind of the entry point of your job in general.",
    "start": "545877",
    "end": "551999"
  },
  {
    "text": "So this actually comes in with your schema.",
    "start": "552580",
    "end": "554859"
  },
  {
    "text": "A large reason why you might face failures is because your data is maybe incomplete or poor data quality.",
    "start": "557400",
    "end": "564480"
  },
  {
    "text": "You don't want that data in your source data anyway, but you basically wanna make sure that we're building in these controls.",
    "start": "564840",
    "end": "572020"
  },
  {
    "text": "So that as we're bringing in data from your source system.",
    "start": "572680",
    "end": "577059"
  },
  {
    "text": "And we're going to be loading it in.",
    "start": "579990",
    "end": "581690"
  },
  {
    "text": "We wanna make sure that we're basically building a gate",
    "start": "582230",
    "end": "585152"
  },
  {
    "text": "so that all this data we can make sure is actually matching and lining up to the schema you expect.",
    "start": "585152",
    "end": "591769"
  },
  {
    "text": "So at the beginning of your data pipeline, always make sure that you're clearly defining",
    "start": "592170",
    "end": "596287"
  },
  {
    "text": "the schema so you know that each row is correct.",
    "start": "596287",
    "end": "599990"
  },
  {
    "text": "And if you find one that isn't, maybe that is incomplete or it doesn't meet your data standards,",
    "start": "600370",
    "end": "606278"
  },
  {
    "text": "that you can kick this back early So that basically you can go forward only with the data",
    "start": "606278",
    "end": "612751"
  },
  {
    "text": "that is actually optimized and you want moving to your next endpoint.",
    "start": "612751",
    "end": "616980"
  },
  {
    "text": "This is actually also gonna help with your memory allocation",
    "start": "617420",
    "end": "620027"
  },
  {
    "text": "because you're not gonna waste time going through the transformation and load phases",
    "start": "620028",
    "end": "624675"
  },
  {
    "text": "for data that doesn't meet your quality standards.",
    "start": "624675",
    "end": "627299"
  },
  {
    "text": "So another area to look at is going to be mostly around how we design our pipelines.",
    "start": "629280",
    "end": "636200"
  },
  {
    "text": "Now, there's many different schools of thoughts about",
    "start": "636760",
    "end": "639269"
  },
  {
    "text": "how our pipeline should be built out, but we all know they have three parts.",
    "start": "639269",
    "end": "643058"
  },
  {
    "text": "They have ETL, but we want to make sure that in each of these, we're building out our retry logic.",
    "start": "643100",
    "end": "649639"
  },
  {
    "text": "We have all of our parts.",
    "start": "652050",
    "end": "653230"
  },
  {
    "text": "E, T, and L nothing special.",
    "start": "658980",
    "end": "660980"
  },
  {
    "text": "So within each of these three parts you want to make sure that you have your resiliency",
    "start": "661370",
    "end": "665865"
  },
  {
    "text": "built in so essentially it can fail at each piece and that you can retry.",
    "start": "665866",
    "end": "670349"
  },
  {
    "text": "Now you may be tempted to break these into three separate pipelines all together,",
    "start": "670750",
    "end": "676020"
  },
  {
    "text": "so that you maybe can mix and match them maybe find some reuse",
    "start": "676020",
    "end": "679047"
  },
  {
    "text": "however I would encourage you to stay away from that kind of design",
    "start": "679047",
    "end": "682734"
  },
  {
    "text": "because generally you want more resilient pipelines that are altogether",
    "start": "682734",
    "end": "686829"
  },
  {
    "text": "again that are going to carry all the way from point A to point B.",
    "start": "686829",
    "end": "691259"
  },
  {
    "text": "By putting in breaks in the process, you're just creating interdependencies that you're gonna have to manage.",
    "start": "691860",
    "end": "697519"
  },
  {
    "text": "And you're going to have to make sure there's the proper failover and control.",
    "start": "697900",
    "end": "701759"
  },
  {
    "text": "You're basically tripling the amount of complexity in your jobs.",
    "start": "702140",
    "end": "705559"
  },
  {
    "text": "So by looking at this modularity,",
    "start": "706140",
    "end": "708382"
  },
  {
    "text": "you should still be able to build in retry logic,",
    "start": "708382",
    "end": "711220"
  },
  {
    "text": "separate these pieces on your code so it's highly readable, and you still can see these three clear parts.",
    "start": "711220",
    "end": "716918"
  },
  {
    "text": "But you wanna make sure that you're building retry logic basically in each piece.",
    "start": "717400",
    "end": "721739"
  },
  {
    "text": "Retry logic, there's plenty of patterns for this.",
    "start": "722680",
    "end": "725559"
  },
  {
    "text": "Generally, we wanna make that if there is a small failure, you know that it retries the defaults usually three times.",
    "start": "725660",
    "end": "732899"
  },
  {
    "text": "So if there's a small outage or you're just rotating a key, you can basically,",
    "start": "733500",
    "end": "737776"
  },
  {
    "text": "this job is gonna restart on its own so that you don't need to have any manual intervention",
    "start": "737776",
    "end": "742499"
  },
  {
    "text": "but after it fails three times, then it's really going to fail.",
    "start": "742499",
    "end": "745970"
  },
  {
    "text": "You're gonna see everything crash and tear down and you'll get an error message in your logs.",
    "start": "746090",
    "end": "749950"
  },
  {
    "text": "So make sure that you've built that in really at any critical points, but at least within each phase.",
    "start": "750810",
    "end": "756210"
  },
  {
    "text": "So there's a clear retry catch.",
    "start": "756530",
    "end": "758370"
  },
  {
    "text": "You don't wanna wait all the way to load, you know, for at each phase of your pipeline.",
    "start": "758530",
    "end": "762789"
  },
  {
    "text": "Another thing that we can look at is now that we know that we're failing,",
    "start": "764940",
    "end": "769241"
  },
  {
    "text": "how do we basically know where to pick up where we left off?",
    "start": "769241",
    "end": "774559"
  },
  {
    "text": "And this is done through checkpointing.",
    "start": "775960",
    "end": "778279"
  },
  {
    "text": "Now, checkpointing is essentially drawing a line in the sand saying",
    "start": "779040",
    "end": "782785"
  },
  {
    "text": "when, you know, the last successful record, when was it pulled out?",
    "start": "782786",
    "end": "786840"
  },
  {
    "text": "So this is essentially the next follow-up when you're working on your you try.",
    "start": "787680",
    "end": "792979"
  },
  {
    "text": "So that...",
    "start": "793520",
    "end": "794820"
  },
  {
    "text": "You can basically automatically restart after a complete failure.",
    "start": "794890",
    "end": "799990"
  },
  {
    "text": "So let's say something went down for a few hours, you come back into the office,",
    "start": "800170",
    "end": "803695"
  },
  {
    "text": "you're ready to kickstart your data pipeline again.",
    "start": "803695",
    "end": "807330"
  },
  {
    "text": "How do you know where it left off?",
    "start": "807510",
    "end": "808889"
  },
  {
    "text": "Because maybe you were loading two terabytes of data.",
    "start": "808910",
    "end": "811050"
  },
  {
    "text": "Did it fail in the middle?",
    "start": "811710",
    "end": "812809"
  },
  {
    "text": "Did it failed at the end with only a few more gigs to go?",
    "start": "813190",
    "end": "816830"
  },
  {
    "text": "You probably don't want to start over from the beginning.",
    "start": "817250",
    "end": "819330"
  },
  {
    "text": "So that's where checkpoints can come in handy.",
    "start": "819970",
    "end": "822149"
  },
  {
    "text": "So that basically...",
    "start": "822150",
    "end": "823389"
  },
  {
    "text": "As you're building in all of your different source.",
    "start": "823950",
    "end": "828850"
  },
  {
    "text": "Systems, and they're coming in, as you actually load them.",
    "start": "828960",
    "end": "836480"
  },
  {
    "text": "Into the destination, you basically are then going to create a successful message.",
    "start": "838510",
    "end": "844809"
  },
  {
    "text": "So that we know the last successful data that was moved over,",
    "start": "847350",
    "end": "851764"
  },
  {
    "text": "when you say a target four, five, six, it's usually represented by a hash or a number or something.",
    "start": "851765",
    "end": "856489"
  },
  {
    "text": "So that let's say then it fails at the next transaction.",
    "start": "857370",
    "end": "860930"
  },
  {
    "text": "When the restart happens, it basically is gonna have this as a reference to know this is the line that we're gonna start at again.",
    "start": "861610",
    "end": "868308"
  },
  {
    "text": "So by building and checkpointing after each successful load, you're gonna have a lot more resiliency.",
    "start": "869290",
    "end": "875149"
  },
  {
    "text": "So, you know, you're making sure you're building it at the beginning of your pipeline, so you're only bringing good data in.",
    "start": "875730",
    "end": "881269"
  },
  {
    "text": "You're gonna have retry logic built in throughout,",
    "start": "881750",
    "end": "884140"
  },
  {
    "text": "and then you're also basically gonna plan for failure,",
    "start": "884140",
    "end": "887523"
  },
  {
    "text": "so that when it does happen, you're gonna store this checkpoint somewhere outside of the pipeline itself.",
    "start": "887523",
    "end": "892690"
  },
  {
    "text": "That's gonna be resilient to any, you now, pod failure or anything like that.",
    "start": "893190",
    "end": "897049"
  },
  {
    "text": "So you can basically restart this, probably automatically, but without very little manual intervention.",
    "start": "897410",
    "end": "904548"
  },
  {
    "text": "Incorporating memory efficiency and resiliency into your data pipelines is key to scaling with big data.",
    "start": "905254",
    "end": "911160"
  },
  {
    "text": "It ensures your AI and data workflows run smoothly without any interruptions.",
    "start": "911780",
    "end": "915860"
  },
  {
    "text": "These best practices help you build pipelines that stand the test of time,",
    "start": "916220",
    "end": "920462"
  },
  {
    "text": "so you'll have the tools you need to handle the growing demands of data today and tomorrow.",
    "start": "920462",
    "end": "925519"
  },
  {
    "text": "With optimized memory usage and built-in error recovery, your pipelines will be ready for whatever comes next.",
    "start": "926360",
    "end": "931940"
  }
]