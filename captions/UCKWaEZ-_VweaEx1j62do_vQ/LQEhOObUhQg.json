[
  {
    "start": "0",
    "end": "25000"
  },
  {
    "text": "Is pre-training dead?",
    "start": "0",
    "end": "1170"
  },
  {
    "text": "No, because 4.5 does the best cheese jokes ever, and why would we stall pre-training for lack of cheese jokes?",
    "start": "1220",
    "end": "10470"
  },
  {
    "text": "I need my cheese jokes, so no, pre-training is here to stay.",
    "start": "10510",
    "end": "13889"
  },
  {
    "text": "I knew I could count on you.",
    "start": "14530",
    "end": "15500"
  },
  {
    "text": "Kate, over to you.",
    "start": "15500",
    "end": "16739"
  },
  {
    "text": "It's already been dead.",
    "start": "16800",
    "end": "18078"
  },
  {
    "text": "Come on, we're beating the dead horse now.",
    "start": "18130",
    "end": "19820"
  },
  {
    "text": "All right, so with that, we will jump into this week's episode.",
    "start": "19860",
    "end": "23860"
  },
  {
    "start": "25000",
    "end": "1446000"
  },
  {
    "text": "Hello, everyone. I am Bryan Casey.",
    "start": "29110",
    "end": "30950"
  },
  {
    "text": "I am guest hosting this episode and welcome to Mixture of Experts.",
    "start": "31000",
    "end": "34650"
  },
  {
    "text": "Every week, Mixture of Experts goes through the hottest stories in artificial intelligence.",
    "start": "35120",
    "end": "39069"
  },
  {
    "text": "And a fun thing about the show is that we record on Thursday mornings.",
    "start": "39440",
    "end": "42809"
  },
  {
    "text": "And on Thursday afternoon this week, GPT-4.5 arrived, and we decided that this was a good time to do our first ever emergency pod.",
    "start": "43029",
    "end": "51649"
  },
  {
    "text": "Um, and so I'm excited and thrilled to have both Chris Hay and Kate Soule on the episode today.",
    "start": "51690",
    "end": "57925"
  },
  {
    "text": "If it wasn't obvious from the opening question,",
    "start": "58085",
    "end": "60545"
  },
  {
    "text": "the one and only topic that we'll be discussing today is, um, Thursday afternoon, OpenAI released the much anticipated, uh, GPT-4.5",
    "start": "60555",
    "end": "69654"
  },
  {
    "text": "Having seen many releases, we get new model releases every day of the week, um, around here.",
    "start": "69665",
    "end": "75455"
  },
  {
    "text": "There are some things to me that were pretty remarkable about this release, even in the way that OpenAI, uh, communicated it to the rest of the market.",
    "start": "75665",
    "end": "82285"
  },
  {
    "text": "Um, first, in all of their announcement materials, um, They didn't describe 4.5 as a frontier model.",
    "start": "82614",
    "end": "89220"
  },
  {
    "text": "Um, they were very clear in all of their communications that this was against kind of the normal conventional benchmarks,",
    "start": "89300",
    "end": "95820"
  },
  {
    "text": "was not going to be a world beating model.",
    "start": "95820",
    "end": "97659"
  },
  {
    "text": "Um, they talked heavily actually about the expense and cost of serving the model and the size of the model,",
    "start": "97890",
    "end": "103420"
  },
  {
    "text": "even saying that they had run out of GPUs, um, in terms of their ability to serve it.",
    "start": "103420",
    "end": "107439"
  },
  {
    "text": "Um, and that even in some of their like documentation, they were kind of non committal long term",
    "start": "107760",
    "end": "112361"
  },
  {
    "text": "about whether they were even going to keep this model available in the API.",
    "start": "112361",
    "end": "116640"
  },
  {
    "text": "Um, and so to just maybe riff a little bit on our opening question, just maybe start with you, Kate, a little bit, because you said that pre-training's already been dead.",
    "start": "116970",
    "end": "125924"
  },
  {
    "text": "Um, the immediate discussion in the market,",
    "start": "126215",
    "end": "129215"
  },
  {
    "text": "because I think the assumption is that GPT 4.5 was trained on something like 10x as much compute as GPT-4,",
    "start": "129355",
    "end": "136157"
  },
  {
    "text": "that's at least the hypothesis, that I've seen, uh, thrown out there.",
    "start": "136158",
    "end": "140053"
  },
  {
    "text": "People immediately went to the implications around, did we hit the wall, scaling laws, is pre-training dead?",
    "start": "140614",
    "end": "147355"
  },
  {
    "text": "What's your take?",
    "start": "147885",
    "end": "148515"
  },
  {
    "text": "I mean, I think even before GPT-4.5 came out, we saw really compelling evidence with DeepSeek",
    "start": "149145",
    "end": "156714"
  },
  {
    "text": "and other models that inference time compute is king, not pre-training computes.",
    "start": "156714",
    "end": "162423"
  },
  {
    "text": "We're seeing tons of things unlocked by spending more, by reasoning longer at inference time that costs more,",
    "start": "162615",
    "end": "169192"
  },
  {
    "text": "but if you spend more money there, it's unlocking all sorts of new performance gains.",
    "start": "169192",
    "end": "173640"
  },
  {
    "text": "And the old mode of just pay your way during pre-training by training for  longer and longer and more and more data.",
    "start": "174090",
    "end": "180308"
  },
  {
    "text": "It's, you know, We're not seeing the same gains all of the we're worked our way so far up the cost curve, so to speak,",
    "start": "180680",
    "end": "188221"
  },
  {
    "text": "we're really seeing a plateau from that perspective.",
    "start": "188222",
    "end": "191021"
  },
  {
    "text": "So, you know, I don't think this is actually unexpected.",
    "start": "191070",
    "end": "194760"
  },
  {
    "text": "Um, if you actually look at where we've been headed for a while now.",
    "start": "194820",
    "end": "198560"
  },
  {
    "text": "Maybe Chris to throw to bring you into this and to riff a little bit on your, um, your opening remarks, um, let's just say, um,",
    "start": "198789",
    "end": "205499"
  },
  {
    "text": "I think one of the reactions that I've seen in the community is that while it didn't beat, you know,",
    "start": "205499",
    "end": "211310"
  },
  {
    "text": "set like new kind of, um, uh, I think standards in terms",
    "start": "211310",
    "end": "215119"
  },
  {
    "text": "of like some of the math and science benchmarks,",
    "start": "215119",
    "end": "216719"
  },
  {
    "text": "even just like the conventional benchmarks,",
    "start": "216719",
    "end": "218342"
  },
  {
    "text": "the reaction and even discussion in the market was that the model was like really good at writing.",
    "start": "218343",
    "end": "222453"
  },
  {
    "text": "Um, it was funny and that, you know, people weren't used to seeing a model that was like actually funny in a way that like wasn't cringy.",
    "start": "222614",
    "end": "230515"
  },
  {
    "text": "Um, essentially that was maybe more creative,",
    "start": "230605",
    "end": "232903"
  },
  {
    "text": "um, than, uh, you know, past models.",
    "start": "233204",
    "end": "236073"
  },
  {
    "text": "Do you agree that like with kind of Kate's take, or do you also feel like we're kind of like underselling like how important is to have he reached those milestones?",
    "start": "236184",
    "end": "244375"
  },
  {
    "text": "Are we underestimating",
    "start": "244385",
    "end": "245493"
  },
  {
    "text": "the value and, you know, where kind of creativity and writing and humor and things like that sit on the intelligence curve.",
    "start": "245674",
    "end": "251265"
  },
  {
    "text": "But, you know, I'm curious your thoughts on that.",
    "start": "251265",
    "end": "253555"
  },
  {
    "text": "It's late at night for me. So if I agree with Kate, do I get to go home?",
    "start": "253614",
    "end": "256685"
  },
  {
    "text": "No, of course I'm going to disagree with Kate.",
    "start": "256774",
    "end": "260883"
  },
  {
    "text": "Where would be the fun if I didn't, right?",
    "start": "260885",
    "end": "262704"
  },
  {
    "text": "I expect nothing less.",
    "start": "262704",
    "end": "263575"
  },
  {
    "text": "I think.",
    "start": "263575",
    "end": "264064"
  },
  {
    "text": "So  I think the first thing it is the creativity. It is a genuinely funny model.",
    "start": "264064",
    "end": "269613"
  },
  {
    "text": "It is actually super, super funny and sassy.",
    "start": "269645",
    "end": "273104"
  },
  {
    "text": "And it is the first time I've really seen good creative writing coming from the model.",
    "start": "273174",
    "end": "278064"
  },
  {
    "text": "So actually, I, I think they've done some, something pretty good there.",
    "start": "278074",
    "end": "281324"
  },
  {
    "text": "Of course, it's not going to be good at the, uh, stuff that inference time computers is,",
    "start": "281614",
    "end": "285984"
  },
  {
    "text": "you know, math and stuff like that because you, it does need more time to think on that.",
    "start": "286120",
    "end": "289810"
  },
  {
    "text": "And I'm, and I'm okay with that.",
    "start": "290110",
    "end": "291699"
  },
  {
    "text": "Does that make pre-training dead?",
    "start": "291940",
    "end": "293740"
  },
  {
    "text": "No.",
    "start": "294010",
    "end": "294510"
  },
  {
    "text": "You know why?",
    "start": "294640",
    "end": "295390"
  },
  {
    "text": "Because if there's no pre-trained models,",
    "start": "295420",
    "end": "297940"
  },
  {
    "text": "what are you inferring at inference time?",
    "start": "298060",
    "end": "300520"
  },
  {
    "text": "Nothing.",
    "start": "300730",
    "end": "301600"
  },
  {
    "text": "You need the pre-trained model in the first place.",
    "start": "301750",
    "end": "304540"
  },
  {
    "text": "So I pre-training is not going anywhere. I. If I was going to make a prediction and",
    "start": "304870",
    "end": "312585"
  },
  {
    "text": "that prediction is going to be that there's a lot of techniques that we're doing at",
    "start": "312895",
    "end": "317495"
  },
  {
    "text": "fine tuning layers to be able to support inference time compute,",
    "start": "318205",
    "end": "321594"
  },
  {
    "text": "which I think can go back to pre-training because the reality is, right,",
    "start": "321594",
    "end": "325430"
  },
  {
    "text": "you know, here is the entire internet is probably not the most efficient way to do pre-training in the first place,",
    "start": "325430",
    "end": "332143"
  },
  {
    "text": "and actually, the biggest thing that we've learned is that the quality of data during reinforcement learning is",
    "start": "332534",
    "end": "338134"
  },
  {
    "text": "the quality of data for chain of thought during inference time is actually making a bigger impact than anything else.",
    "start": "338300",
    "end": "344470"
  },
  {
    "text": "So actually, if we go back to the pre-training cycle, rather than saying, hey, go look at the internet and tell me when you've suddenly learned something,",
    "start": "344560",
    "end": "351560"
  },
  {
    "text": "it's like that episode of The Simpsons where Bart went to Paris for three months and then suddenly at the end, he was like, I can speak French.",
    "start": "351560",
    "end": "360307"
  },
  {
    "text": "That's how we train large language models.",
    "start": "360775",
    "end": "362995"
  },
  {
    "text": "And I think that's going to change.",
    "start": "363185",
    "end": "364915"
  },
  {
    "text": "And it's going to be, look, how do we build quality, synthetic data sets to be able to do pre-train?",
    "start": "364915",
    "end": "369625"
  },
  {
    "text": "So I think we're going to go back and forward all of the time, and we're going to pronounce pre train as dead.",
    "start": "369625",
    "end": "374894"
  },
  {
    "text": "And then suddenly, you know, we're going to do something good again. And then it'll be like, oh",
    "start": "375094",
    "end": "379133"
  },
  {
    "text": "no, everybody pre-train.",
    "start": "379145",
    "end": "380735"
  },
  {
    "text": "And we're going to go back and forward, back and forward.",
    "start": "380775",
    "end": "382775"
  },
  {
    "text": "So no, pre training isn't going anywhere.",
    "start": "382775",
    "end": "384655"
  },
  {
    "text": "So just a couple of reactions, right?",
    "start": "384785",
    "end": "387315"
  },
  {
    "text": "So, especially on the emotional side,",
    "start": "387345",
    "end": "390275"
  },
  {
    "text": "the, uh, the humor, the characteristics,",
    "start": "390284",
    "end": "392754"
  },
  {
    "text": "that's not pre-training, right?",
    "start": "392754",
    "end": "394714"
  },
  {
    "text": "That's all imbued into the model during the alignment after pre training.",
    "start": "394715",
    "end": "398124"
  },
  {
    "text": "It doesn't matter, like, really, if we want to talk about the models doing a great job at that,",
    "start": "398125",
    "end": "401761"
  },
  {
    "text": "that's not because they pre trained it for 10 times longer. I highly doubt it.",
    "start": "401761",
    "end": "406444"
  },
  {
    "text": "It's really due to the alignment of the model.",
    "start": "406465",
    "end": "409105"
  },
  {
    "text": "So, I don't know that.",
    "start": "409115",
    "end": "410775"
  },
  {
    "text": "That was worth, you know, if they truly spent 10x on the pre-training, uh, really worth it,",
    "start": "411325",
    "end": "415735"
  },
  {
    "text": "but I do think you're right, Chris, that pre-training will change.",
    "start": "416405",
    "end": "419245"
  },
  {
    "text": "So when I say pre training's dead, the mode of throw more data and spend more until performance goes up, I think is dead.",
    "start": "419245",
    "end": "425714"
  },
  {
    "text": "Being smarter about how we pre-train, I completely agree.",
    "start": "426555",
    "end": "429465"
  },
  {
    "text": "I do think for the near term, we're going to see much more like base models as a commodity where I don't care,",
    "start": "429865",
    "end": "435925"
  },
  {
    "text": "uh, from a performance perspective.",
    "start": "436314",
    "end": "438414"
  },
  {
    "text": "I think there are other things that can differentiate base models, particularly on",
    "start": "438414",
    "end": "442504"
  },
  {
    "text": "a trust and transparency angle, especially if they're not driving up performance anymore, that becomes more interesting.",
    "start": "442775",
    "end": "448034"
  },
  {
    "text": "The licensing of the base model is another example, but you know, I think.",
    "start": "448034",
    "end": "452673"
  },
  {
    "text": "In a lot of ways, it's kind of like pick your right model size and then all the innovation right now is really happening on the alignment side.",
    "start": "453275",
    "end": "460595"
  },
  {
    "text": "So pick your favorite base model that meets your cost and other criteria, uh, and then apply your alignment techniques on top of that to really drive and meet your needs.",
    "start": "460615",
    "end": "469966"
  },
  {
    "text": "I agree and disagree.",
    "start": "470205",
    "end": "471404"
  },
  {
    "text": "And I think the reason I'm going to",
    "start": "471455",
    "end": "473070"
  },
  {
    "text": "say that is I don't think you care.",
    "start": "473071",
    "end": "475385"
  },
  {
    "text": "until you care.",
    "start": "475775",
    "end": "476675"
  },
  {
    "text": "And what I mean by that is, is base models",
    "start": "477065",
    "end": "480145"
  },
  {
    "text": "have been commoditized at the moment,",
    "start": "480145",
    "end": "481625"
  },
  {
    "text": "and at this point, time inference, time compute is the most important thing, and there's a ton of mileage to get on that.",
    "start": "482035",
    "end": "488253"
  },
  {
    "text": "And then there will be a point where that",
    "start": "488665",
    "end": "490934"
  },
  {
    "text": "mileage will slow down a little bit, and then it'll be like, oh, I need a better base model to be able to get there.",
    "start": "490944",
    "end": "497165"
  },
  {
    "text": "And then suddenly we're all gonna be like kids on the soccer field.",
    "start": "497175",
    "end": "500604"
  },
  {
    "text": "We're all gonna run towards the other end of the field.",
    "start": "500845",
    "end": "503665"
  },
  {
    "text": "And then we'll be like, oh yeah, I can get an extra percentage point,",
    "start": "503884",
    "end": "507165"
  },
  {
    "text": "or I can do a little bit better if I have a pre-trained model.",
    "start": "507174",
    "end": "509655"
  },
  {
    "text": "And we're gonna run over there,",
    "start": "509705",
    "end": "511095"
  },
  {
    "text": "and then we're gonna go, oh my goodness.",
    "start": "511375",
    "end": "513515"
  },
  {
    "text": "Tools, tools is the thing we, I'm going to mention agents, of course I'm going to mention agents,",
    "start": "513950",
    "end": "518518"
  },
  {
    "text": "it's like, and we'll be like, the best tools is what's going to make the models, and we'll be like, inference times compute, that is dead,",
    "start": "518519",
    "end": "526143"
  },
  {
    "text": "because tools is going to be the most important thing, and we're going to run over there.",
    "start": "526143",
    "end": "530230"
  },
  {
    "text": "And then actually, we're just going to run around in circles from thing to thing, optimizing and because we've done this dance before.",
    "start": "530500",
    "end": "537460"
  },
  {
    "text": "Uh, and, and that's what we're going to continue to do. And it's going to be fun, but all of these things are important.",
    "start": "537720",
    "end": "542519"
  },
  {
    "text": "If we take the, you know, base model as a commodity right now, another step further,",
    "start": "542649",
    "end": "547229"
  },
  {
    "text": "I think it will show that we're going to see a lot more innovation in the architectures.",
    "start": "547230",
    "end": "551829"
  },
  {
    "text": "So you have to pre train new architectures,",
    "start": "552040",
    "end": "554000"
  },
  {
    "text": "but as we talk about how do we get more efficient models, obviously a mixture of experts as an architecture is becoming important,",
    "start": "554415",
    "end": "560264"
  },
  {
    "text": "uh, in terms of broader efficiency and,  being able to maximize performance per cost,",
    "start": "560625",
    "end": "565725"
  },
  {
    "text": "and I think continuing to find ways people are going to try and differentiate and kind of break out of this commoditization,",
    "start": "566074",
    "end": "571503"
  },
  {
    "text": "right, by finding ways to drive architectural improvements. But I don't think the",
    "start": "571504",
    "end": "576435"
  },
  {
    "text": "story is going to be, we're going to have this new architecture that we trained for 10 times longer than anybody else,",
    "start": "576859",
    "end": "582030"
  },
  {
    "text": "and that's why the model is special.",
    "start": "582050",
    "end": "583229"
  },
  {
    "text": "It's going to be, we came up with this new architecture that's even more efficient and powerful",
    "start": "583270",
    "end": "587013"
  },
  {
    "text": "that you can now move to when you do all of the fancy alignment that gives you the true performance for the model.",
    "start": "587013",
    "end": "591958"
  },
  {
    "text": "One of the first places everybody's head goes when they see this happening is like,",
    "start": "592000",
    "end": "595168"
  },
  {
    "text": "Oh my God, what's happening to all the compute build out that's happening in the world right now is like, is that under threat?",
    "start": "595168",
    "end": "599819"
  },
  {
    "text": "But what didn't happen is like.",
    "start": "599819",
    "end": "600959"
  },
  {
    "text": "All of those, like, stocks just going nuclear or something like that.",
    "start": "601550",
    "end": "604899"
  },
  {
    "text": "And I think a lot of that is because",
    "start": "604900",
    "end": "606530"
  },
  {
    "text": "of the opportunity that is around test time and inference time compute.",
    "start": "607000",
    "end": "610000"
  },
  {
    "text": "And I saw even one of the former research leaders at OpenAI was just talking about,",
    "start": "610030",
    "end": "614498"
  },
  {
    "text": "like, it's pretty clear that in 2025, the",
    "start": "614500",
    "end": "617330"
  },
  {
    "text": "optimal way to use compute is not going to be to be just, you know, scaling, um, pre training, basically, as far as you can go.",
    "start": "617330",
    "end": "626039"
  },
  {
    "text": "And it's going to be in reasoning, and the gains are going to happen in reasoning",
    "start": "626040",
    "end": "628970"
  },
  {
    "text": "and",
    "start": "629300",
    "end": "629800"
  },
  {
    "text": "I know that we are kind of early in that journey, um, over the last, like, you know, few months or so.",
    "start": "630390",
    "end": "635420"
  },
  {
    "text": "Obviously IBM just had a release, where we started on that journey a few days ago.",
    "start": "635420",
    "end": "640079"
  },
  {
    "text": "You know, maybe could you talk a little bit about, like, what even that might look like?",
    "start": "641130",
    "end": "644170"
  },
  {
    "text": "Like, if we're gonna be, like, attacking this vector over the course of the next you know, until we get as far as we can go there.",
    "start": "644200",
    "end": "650520"
  },
  {
    "text": "Like, what are the types of things that people are going to explore?",
    "start": "650520",
    "end": "652780"
  },
  {
    "text": "What are the opportunities?",
    "start": "652780",
    "end": "653810"
  },
  {
    "text": "Is it just like make this thing think for a week and come back?",
    "start": "654030",
    "end": "656850"
  },
  {
    "text": "Or is, are we going to be a little bit more sophisticated than that?",
    "start": "656880",
    "end": "659059"
  },
  {
    "text": "Yeah. I mean, I think at the top level, the thing to think through is we now have a pass through model for cost, right?",
    "start": "659099",
    "end": "665668"
  },
  {
    "text": "So instead of a model provider spending a bunch of money in fixed costs to get high performance, model provider can just kind of pass that through and say,",
    "start": "665680",
    "end": "673113"
  },
  {
    "text": "look, you can host the model and pay for as or you can pay through an endpoint, but just keep paying until you get the performance you want.",
    "start": "673113",
    "end": "681293"
  },
  {
    "text": "And if you don't need all that performance pay less.",
    "start": "681725",
    "end": "684095"
  },
  {
    "text": "And so I think it's gonna like approach a much more, you know efficient market so to speak right where you're paying for what the task calls for",
    "start": "684145",
    "end": "691145"
  },
  {
    "text": "versus, you know, you've got some subscription of X dollars, you know, uh, a month that you, you know, are kind of locked into.",
    "start": "691145",
    "end": "702181"
  },
  {
    "text": "So I think we're going to see a lot more flexibility in pricing.",
    "start": "702230",
    "end": "704719"
  },
  {
    "text": "We already saw that with Anthropic, uh, 3.7 right, where you can set different cost,",
    "start": "704760",
    "end": "709679"
  },
  {
    "text": "uh, parameters for how much you want to pay basically to how long you want to think.",
    "start": "710149",
    "end": "714479"
  },
  {
    "text": "Uh, for a given task.",
    "start": "714995",
    "end": "716925"
  },
  {
    "text": "And so I think that's only going to continue until, you know, everything is going to be like, well, how much is it worth to you?",
    "start": "716944",
    "end": "722224"
  },
  {
    "text": "Like, I don't know if we'll get to like an auction setting almost like you could like even put it up for bid.",
    "start": "722234",
    "end": "727374"
  },
  {
    "text": "Right, but, um, I think it's going to be much more efficient.",
    "start": "727375",
    "end": "730394"
  },
  {
    "text": "Uh, In terms of actually getting economic value out of generative AI, because you'll pay for what something is worth.",
    "start": "730454",
    "end": "736730"
  },
  {
    "text": "Chris, maybe as like a question to that is like, as an end user of these tools,",
    "start": "736810",
    "end": "741060"
  },
  {
    "text": "I think being able to decide how much, like when I want to use, when I just want a quick answer, when I want to use reasoning,",
    "start": "741060",
    "end": "748146"
  },
  {
    "text": "when I like search is becoming like an increasingly significant thing that people are rolling out.",
    "start": "748146",
    "end": "753089"
  },
  {
    "text": "And like, I can just kind of decide when I'm using each one of those things.",
    "start": "753279",
    "end": "756060"
  },
  {
    "text": "As an application developer, um, and when now you're like trying to, you just want the model to get to the right answer at the lowest cost as fast as possible,",
    "start": "756630",
    "end": "763630"
  },
  {
    "text": "like, how are you, like, how would you be thinking about these trade offs going, going forward?",
    "start": "763630",
    "end": "769150"
  },
  {
    "text": "Are we happy that, like, so many of the, like, next incremental grains are going to be happening via",
    "start": "769159",
    "end": "774252"
  },
  {
    "text": "the, uh, you know, more of like reasoning models, um, versus the way that we used to get them through kind of the base models.",
    "start": "774439",
    "end": "779760"
  },
  {
    "text": "Is it more complex now when you're thinking about like the user experience?",
    "start": "779770",
    "end": "783170"
  },
  {
    "text": "It's like, oh, I used to be able to just count on getting that answer really quickly now it's like,",
    "start": "783189",
    "end": "786360"
  },
  {
    "text": "sometimes my answers are coming instantly and sometimes like a model goes off and like thinks for like, you know, five minutes before it comes back on something.",
    "start": "786820",
    "end": "793170"
  },
  {
    "text": "It's just like, as when you're thinking about like the developer community and they start to adopt some of these tools, like,",
    "start": "793450",
    "end": "799025"
  },
  {
    "text": "are they happy about the fact that like more and more of this is going to get put into reasoning over time?",
    "start": "799025",
    "end": "803824"
  },
  {
    "text": "Does that make things harder, um, to build this stuff into applications?",
    "start": "803824",
    "end": "807407"
  },
  {
    "text": "I think everything is a trade off,",
    "start": "807570",
    "end": "810240"
  },
  {
    "text": "and actually.",
    "start": "810320",
    "end": "811500"
  },
  {
    "text": "I really like Kate's analogy on this one, and, and I like it 'cause I did a video on this a while back.",
    "start": "811845",
    "end": "816975"
  },
  {
    "text": "Whereas I, I think we are gonna move into this agent marketplace and I think that is probably the most important thing.",
    "start": "817275",
    "end": "823965"
  },
  {
    "text": "And I think in the same way as we go into something like Fiverr and we say,",
    "start": "823965",
    "end": "828737"
  },
  {
    "text": "I need some, I'm gonna spend five bucks 'cause I need, uh, a video edited, or I need",
    "start": "828737",
    "end": "833825"
  },
  {
    "text": "somebody to go and code something up for me.",
    "start": "834095",
    "end": "836014"
  },
  {
    "text": "I think we're gonna be in the same world with agents and, and the reality is",
    "start": "836015",
    "end": "840965"
  },
  {
    "text": "that if I need a document translated and I need it done in five minutes, then you can have the best model in the world,",
    "start": "841115",
    "end": "849325"
  },
  {
    "text": "but if it ain't doing it in five minutes, which is when I need this thing to be done, then I don't care.",
    "start": "849595",
    "end": "854485"
  },
  {
    "text": "If I'm doing real time translation, so I once spent some time, I think I was in Moscow at the time,",
    "start": "854694",
    "end": "860766"
  },
  {
    "text": "and there was this guy who was translating what I said real time into Russian.",
    "start": "860767",
    "end": "865726"
  },
  {
    "text": "Now, It can sit and think all it likes, but the audience is going to be waiting for that translation, right?",
    "start": "866055",
    "end": "872459"
  },
  {
    "text": "So, so I think there are times where real time is going to be important, and that's going to be the same with coding.",
    "start": "872469",
    "end": "878969"
  },
  {
    "text": "But at the same time, accuracy is going to be important as well, because like that translation scenario, if the guy just started making up what I said",
    "start": "879299",
    "end": "886299"
  },
  {
    "text": "because he didn't understand it, then it's great that he's real time, but he's just spouting gibberish at that point, which isn't any use to anybody.",
    "start": "886299",
    "end": "896913"
  },
  {
    "text": "So I think if you're fast, and you're accurate, and you're cheap, and you can do the same job as something that is big and takes a long time,",
    "start": "897155",
    "end": "904155"
  },
  {
    "text": "that is going to, and expensive, that's going to win,",
    "start": "904155",
    "end": "909455"
  },
  {
    "text": "and that's just market dynamics.",
    "start": "909485",
    "end": "911214"
  },
  {
    "text": "Um, but when it comes to something really important, so, If I am, for example, needing to do some deep research and finding some chemical compound,",
    "start": "911585",
    "end": "924249"
  },
  {
    "text": "you know, having a, today's one billion parameter models, take a plucky guess in the air without thinking about it.",
    "start": "924249",
    "end": "933029"
  },
  {
    "text": "I honestly, I don't think you're going to be that satisfied with the result.",
    "start": "933415",
    "end": "936735"
  },
  {
    "text": "So it's going to be a balance on the task, on how much effort and thought and tools, etc.",
    "start": "936755",
    "end": "942021"
  },
  {
    "text": "You're going to, but it is going to be a marketplace dynamics, and it's going to be",
    "start": "942021",
    "end": "945306"
  },
  {
    "text": "latency, it's going to be cost, and it's going to be the level of intelligence that you need.",
    "start": "945414",
    "end": "949595"
  },
  {
    "text": "So.",
    "start": "949844",
    "end": "950354"
  },
  {
    "text": "To the point, this is again, coming back to this, why I still don't think pre training is going to go away,",
    "start": "950665",
    "end": "957683"
  },
  {
    "text": "which is if you can gain an edge on the base model so that it can actually reason a bit better with the combination of inference time compute,",
    "start": "957684",
    "end": "966994"
  },
  {
    "text": "with the combination of tools, then that might be the thing that gives you the edge.",
    "start": "967064",
    "end": "972405"
  },
  {
    "text": "In that scenario, and therefore every single company in this is in a race to have an edge.",
    "start": "972624",
    "end": "977774"
  },
  {
    "text": "And if they didn't have the race",
    "start": "977834",
    "end": "979464"
  },
  {
    "text": "for the edge, why are we all publishing benchmarks all the time?",
    "start": "979464",
    "end": "982864"
  },
  {
    "text": "Because we wouldn't care if we weren't, this is better than this one.",
    "start": "983134",
    "end": "986233"
  },
  {
    "text": "So I I just think these dynamics are going to play out and back to what I said beginning, I don't think this is going to go away in the case of development.",
    "start": "986524",
    "end": "995170"
  },
  {
    "text": "And I know this is a long run.",
    "start": "995219",
    "end": "996479"
  },
  {
    "text": "Bryan, I do apologize.",
    "start": "996479",
    "end": "997839"
  },
  {
    "text": "Sometimes I need a fast, so coming back to your original question,",
    "start": "998120",
    "end": "1002130"
  },
  {
    "text": "sorry, everyone,",
    "start": "1002399",
    "end": "1003130"
  },
  {
    "text": "it's taken me that long to do that,",
    "start": "1003130",
    "end": "1004339"
  },
  {
    "text": "If I'm in my VS code environment, if I'm just sort of doing auto complete stuff, that needs to be fast.",
    "start": "1004579",
    "end": "1010419"
  },
  {
    "text": "But if I'm writing an entire program, an entire game, or doing a migration,",
    "start": "1010680",
    "end": "1015680"
  },
  {
    "text": "and you know what, the model's going to take five, ten minutes, but actually it would have taken me two weeks to do it.",
    "start": "1015950",
    "end": "1021019"
  },
  {
    "text": "I'm going to wait that time, right?",
    "start": "1021360",
    "end": "1022800"
  },
  {
    "text": "If, especially if it's accurate, if I have to wait ten minutes and it's completely wrong, I'm not going to wait.",
    "start": "1022830",
    "end": "1027550"
  },
  {
    "text": "So it's, these are the marketplace dynamics that I see.",
    "start": "1027724",
    "end": "1030625"
  },
  {
    "text": "I think there's two really interesting points to bring up, Chris.",
    "start": "1030694",
    "end": "1033484"
  },
  {
    "text": "One is that you can think of costs from like, how much do I have to spend?",
    "start": "1033514",
    "end": "1037184"
  },
  {
    "text": "But obviously costs from latency is critical to think about as well.",
    "start": "1037194",
    "end": "1040304"
  },
  {
    "text": "So that's definitely like a third dimension to all of this as people starts into the market and figure out like, what is it worth to me?",
    "start": "1040364",
    "end": "1046135"
  },
  {
    "text": "How long can I wait and how, what performance do I need?",
    "start": "1046165",
    "end": "1048865"
  },
  {
    "text": "And like those three combined is going to kind of drive you to your, your model selection.",
    "start": "1048905",
    "end": "1053524"
  },
  {
    "text": "But I also think that like as we talk about the experiene and what we've built with generative AI so far,",
    "start": "1053964",
    "end": "1059307"
  },
  {
    "text": "everything we've done for the past two and a half years has been based off of chat, instant response.",
    "start": "1059307",
    "end": "1065139"
  },
  {
    "text": "So now that we have the reasons to wait, because we'll get better results, like waiting for a conversation, turning on conversation doesn't make sense.",
    "start": "1065579",
    "end": "1074648"
  },
  {
    "text": "No one would do that.",
    "start": "1074789",
    "end": "1075518"
  },
  {
    "text": "But now that we have reasons to wait, you know, I think we're going to see entirely new things get built with generative AI",
    "start": "1075830",
    "end": "1081998"
  },
  {
    "text": "or ideas of how you build with generative AI, because we now have the incentive to find those other patterns",
    "start": "1081999",
    "end": "1088610"
  },
  {
    "text": "and things that didn't require instantaneous responses now suddenly become in scope.",
    "start": "1088649",
    "end": "1093081"
  },
  {
    "text": "I'm curious how you think, um, that",
    "start": "1093169",
    "end": "1096309"
  },
  {
    "text": "this will actually come together and like people will consume it.",
    "start": "1096420",
    "end": "1099229"
  },
  {
    "text": "And so like open AI, um, it's kind of like a little bit of a joke online when people you open up the interface and you look at the model selection",
    "start": "1099229",
    "end": "1106229"
  },
  {
    "text": "it's like if you're not like listening to the equivalent of this show every day like how would you guess which one of these things you're supposed",
    "start": "1106229",
    "end": "1114022"
  },
  {
    "text": "to use and so they've been very clear that their part of their roadmap is to bring them together",
    "start": "1114022",
    "end": "1118812"
  },
  {
    "text": "and you ask a question and the model just kind of knows which ones of these things it's gonna",
    "start": "1118812",
    "end": "1122921"
  },
  {
    "text": "bring together and also part of me was even wondering in the wake of this where, you know,",
    "start": "1123280",
    "end": "1128470"
  },
  {
    "text": "if you're not going to be able to break through on the benchmarks in terms of like the criteria that I think the market understands, you know,",
    "start": "1128670",
    "end": "1135099"
  },
  {
    "text": "what's the purpose of actually shipping a base model that doesn't have reasoning, in it, if it's just going to end up underperforming whatever your last reasoning model is.",
    "start": "1135109",
    "end": "1142459"
  },
  {
    "text": "And one of the questions I walked away from like that kind of series of things is like,",
    "start": "1142920",
    "end": "1146195"
  },
  {
    "text": "are we getting to the are we coming to the end of the line in terms of like even having base models that don't have reasoning attached to them?",
    "start": "1146195",
    "end": "1153340"
  },
  {
    "text": "Will that be kind of like a weird artifact of history that we had those models at all and in the future all of this stuff will just be integrated together in a single model,",
    "start": "1153420",
    "end": "1160420"
  },
  {
    "text": "and the model itself will just decide whether it needs to use reasoning or gives you a straight answer right away?",
    "start": "1160420",
    "end": "1166397"
  },
  {
    "text": "Or do we think there's like a real chance that like, no, there can continue to be, you know, like each ones of these different classes of models",
    "start": "1166700",
    "end": "1173266"
  },
  {
    "text": "and they can each do kind of their discrete things.",
    "start": "1173266",
    "end": "1175220"
  },
  {
    "text": "But, you know, I'm curious, just like how much convergence that you see actually happening in that space.",
    "start": "1175220",
    "end": "1179309"
  },
  {
    "text": "And, you know, maybe Kate, I'll just turn it over to you to get kind of your initial take on it.",
    "start": "1179310",
    "end": "1182589"
  },
  {
    "text": "Yeah, so a couple of things.",
    "start": "1182629",
    "end": "1184039"
  },
  {
    "text": "I don't think",
    "start": "1184259",
    "end": "1185100"
  },
  {
    "text": "OpenAI made a mistake by releasing a non reasoning model.",
    "start": "1185399",
    "end": "1188860"
  },
  {
    "text": "I just think the fact that they released such a big one that costs so much,",
    "start": "1188899",
    "end": "1193260"
  },
  {
    "text": "you know, it was probably a waste of time, a bit, uh, a waste of money.",
    "start": "1194010",
    "end": "1198420"
  },
  {
    "text": "Like I think there are plenty of use cases that we're seeing right now where reasoning actually doesn't help.",
    "start": "1198820",
    "end": "1203757"
  },
  {
    "text": "Things like tool calling and things where you have very clear structured patterns and you kind of just want to like fine tune for that very specific thing, uh, you know,",
    "start": "1203899",
    "end": "1212045"
  },
  {
    "text": "doesn't necessarily require reasoning,",
    "start": "1212055",
    "end": "1213475"
  },
  {
    "text": "but I almost don't know that it matters like, are we going to have reasoning models and non reasoning models?",
    "start": "1213795",
    "end": "1218305"
  },
  {
    "text": "Because like, what is a model?",
    "start": "1218305",
    "end": "1219624"
  },
  {
    "text": "Like is OpenAI, are those models really just an individual model or are there either multiple models being routed to already?",
    "start": "1219695",
    "end": "1227954"
  },
  {
    "text": "Are there experts that have been reserved for different tasks?",
    "start": "1228014",
    "end": "1230425"
  },
  {
    "text": "Like our whole definition of what a model is or is not, I think is just going to continue to be fluid and,",
    "start": "1230435",
    "end": "1235292"
  },
  {
    "text": "continue to evolve as we find new and clever ways to bring this together.",
    "start": "1235292",
    "end": "1240073"
  },
  {
    "text": "I do think though that we're always going to need more instruction based focused.",
    "start": "1240354",
    "end": "1245914"
  },
  {
    "text": "Capabilities and more reason based capabilities and the ability to kind of switch back and forth, depending on what the task calls for.",
    "start": "1246245",
    "end": "1252725"
  },
  {
    "text": "And I'd agree with that, Kate.",
    "start": "1252804",
    "end": "1254753"
  },
  {
    "text": "I really would.",
    "start": "1254755",
    "end": "1256509"
  },
  {
    "text": "And I used this analogy before and I, I can't help thinking about the mainframes ironic for the company that we work for.",
    "start": "1256509",
    "end": "1264654"
  },
  {
    "text": "Right. But, you know, But you had these big massive mainframes and you know, and what is the world that we live in just now?",
    "start": "1264654",
    "end": "1272174"
  },
  {
    "text": "We have computer in our pocket, on our mobile phone, on our laptop, and then architecturally everything is distributed.",
    "start": "1272184",
    "end": "1279023"
  },
  {
    "text": "We have microservices and they all communicate and they all have specialized tasks and then we have good buses between them.",
    "start": "1279034",
    "end": "1285353"
  },
  {
    "text": "And if I really think forward into the future.",
    "start": "1285664",
    "end": "1288023"
  },
  {
    "text": "I do think that the models are going to get smaller and smaller.",
    "start": "1288324",
    "end": "1291395"
  },
  {
    "text": "They're going to get distilled down.",
    "start": "1291405",
    "end": "1293223"
  },
  {
    "text": "Um, I think that was probably the point of, uh, having such a large model is that they're going to use that for distillation.",
    "start": "1293223",
    "end": "1300764"
  },
  {
    "text": "And we're going to see some really good reasoning models, uh, with a very, very good base model, uh, based off of the GPT-4.5 architecture",
    "start": "1300764",
    "end": "1307764"
  },
  {
    "text": "and then in the future, GPT 5.",
    "start": "1307764",
    "end": "1310574"
  },
  {
    "text": "So I think that's really the",
    "start": "1310594",
    "end": "1312195"
  },
  {
    "text": "kind of, kind of point behind that,",
    "start": "1312195",
    "end": "1314134"
  },
  {
    "text": "and also to keep the hype cycle going, which I love.",
    "start": "1314135",
    "end": "1316415"
  },
  {
    "text": "Um, but the.",
    "start": "1316665",
    "end": "1318064"
  },
  {
    "text": "But I think we're gonna end up in this microservice based world of architecture, and in the same way as we move from mainframes to distributed computing.",
    "start": "1318370",
    "end": "1325780"
  },
  {
    "text": "And I, I see this exact same thing happening with generative AI because the reality is if I need something fast on my phone",
    "start": "1325780",
    "end": "1332048"
  },
  {
    "text": "and the models are getting capable, and I can do something that that, that maybe a GPT-3 model used to be able to do, but I can do that on, on my phone",
    "start": "1332048",
    "end": "1341650"
  },
  {
    "text": "on a couple hundred million parameters, then let's do that real time and then, uh, you know, if I need a little bit more reasoning, if I need a bigger model,",
    "start": "1342120",
    "end": "1349210"
  },
  {
    "text": "then I may go off and use some bigger compute.",
    "start": "1349210",
    "end": "1351159"
  },
  {
    "text": "So, and then we use mixture of experts at the moment to be able to do that routing, but then as network speeds get faster and faster,",
    "start": "1351160",
    "end": "1359793"
  },
  {
    "text": "latency gets faster and faster because the chips are getting faster and faster.",
    "start": "1359793",
    "end": "1363590"
  },
  {
    "text": "Again, why wouldn't you start to do that across a mesh of some sort?",
    "start": "1363850",
    "end": "1367760"
  },
  {
    "text": "So at that point, rather than yeah. necessarily using a mixture of experts where you've still got a large model and partitioning,",
    "start": "1367760",
    "end": "1373430"
  },
  {
    "text": "then you have truly separated AIs which are communicating with each other with true experts in the same way as we have humans.",
    "start": "1373760",
    "end": "1379709"
  },
  {
    "text": "So I, I see that expansion coming.",
    "start": "1379720",
    "end": "1382039"
  },
  {
    "text": "And, and again, this is Chris opinion.",
    "start": "1382040",
    "end": "1384070"
  },
  {
    "text": "I, I think as we move into probably, it's probably not a '25 thing, but I think as we look into '26, '27, I, I think we're",
    "start": "1384420",
    "end": "1392125"
  },
  {
    "text": "all going to be going, \"Oh my god, is the big, the days of the large model gone?",
    "start": "1392155",
    "end": "1396385"
  },
  {
    "text": "Is the day of inference time compute gone?\"",
    "start": "1396385",
    "end": "1398135"
  },
  {
    "text": "It's mesh.",
    "start": "1398385",
    "end": "1399175"
  },
  {
    "text": "We need to be in an AI mesh, and then, and single models are dead.",
    "start": "1399305",
    "end": "1402935"
  },
  {
    "text": "I'm, I'm sure that's coming.",
    "start": "1402955",
    "end": "1404534"
  },
  {
    "text": "All right.",
    "start": "1405445",
    "end": "1405794"
  },
  {
    "text": "Well, 12 months from today, Chris, we will have an emergency pod",
    "start": "1405795",
    "end": "1410064"
  },
  {
    "text": "on mesh networks, so I think that's a really good place to to end.",
    "start": "1410495",
    "end": "1414347"
  },
  {
    "text": "Chris, Kate.",
    "start": "1414347",
    "end": "1415495"
  },
  {
    "text": "Thank you for joining.",
    "start": "1415565",
    "end": "1416695"
  },
  {
    "text": "Um today I think this was obviously a topic that the industry has been waiting to see the outcome of for um a long time",
    "start": "1416725",
    "end": "1424504"
  },
  {
    "text": "and in some ways I feel like it asked as many questions as it ended up",
    "start": "1424504",
    "end": "1427609"
  },
  {
    "text": "answering, but that's good because it means we got to do the podcast for another 12 months.",
    "start": "1427620",
    "end": "1431370"
  },
  {
    "text": "So, uh, thank you both for joining.",
    "start": "1431379",
    "end": "1433460"
  },
  {
    "text": "And as always, uh, you can find Mixture of Experts on podcast platforms everywhere, and we will see you next time.",
    "start": "1433460",
    "end": "1438971"
  }
]