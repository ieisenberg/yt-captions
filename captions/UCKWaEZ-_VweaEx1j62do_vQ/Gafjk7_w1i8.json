[
  {
    "text": "Let's take a detailed look at RNNs.",
    "start": "420",
    "end": "3094"
  },
  {
    "text": "RNN stands for Recurrent Neural Network.",
    "start": "3094",
    "end": "5790"
  },
  {
    "text": "It is a type of neural network",
    "start": "5820",
    "end": "7523"
  },
  {
    "text": "designed to handle a sequence of data.",
    "start": "7523",
    "end": "9749"
  },
  {
    "text": "Unlike regular neural networks,",
    "start": "10020",
    "end": "12169"
  },
  {
    "text": "RNN has loops and this is allowing them",
    "start": "12169",
    "end": "15606"
  },
  {
    "text": "to use information from the past.",
    "start": "15607",
    "end": "17748"
  },
  {
    "text": "So when you look at it, this is a very powerful idea.",
    "start": "17760",
    "end": "20760"
  },
  {
    "text": "The main feature of RNNs is their memory.",
    "start": "21210",
    "end": "23789"
  },
  {
    "text": "They remember previous inputs from previous steps,",
    "start": "24030",
    "end": "27532"
  },
  {
    "text": "which is helping them to analyze context in data,",
    "start": "27532",
    "end": "30907"
  },
  {
    "text": "like words in sentence.",
    "start": "30907",
    "end": "33000"
  },
  {
    "text": "Let's start with a basic recurrent neuron.",
    "start": "33900",
    "end": "36378"
  },
  {
    "text": "Imagine we have an input xt and an output yt.",
    "start": "39790",
    "end": "45088"
  },
  {
    "text": "Unlike regular neurons, a \nrecurrent neuron has its own loop,",
    "start": "46600",
    "end": "50483"
  },
  {
    "text": "allowing it to use information from previous steps.",
    "start": "50484",
    "end": "54397"
  },
  {
    "text": "We call this loop the hidden state",
    "start": "55550",
    "end": "58461"
  },
  {
    "text": "and we represent it with ht.",
    "start": "58462",
    "end": "60950"
  },
  {
    "text": "To better understand how RNN's work,",
    "start": "62600",
    "end": "64836"
  },
  {
    "text": "let's unroll this tiny recurrent network over time.",
    "start": "64837",
    "end": "68359"
  },
  {
    "text": "We have inputs xt-2, xt-1 and xt.",
    "start": "79980",
    "end": "85670"
  },
  {
    "text": "Corresponding outputs yt-2, yt-1, and yt.",
    "start": "86670",
    "end": "93062"
  },
  {
    "text": "With hidden states ht-1 and ht.",
    "start": "94870",
    "end": "99089"
  },
  {
    "text": "This unrolling shows how\n the RNNs remember information",
    "start": "100930",
    "end": "105618"
  },
  {
    "text": "from previous steps through the hidden states.",
    "start": "105618",
    "end": "109142"
  },
  {
    "text": "Each hidden state carries what the network \nhas learned from previous inputs.",
    "start": "110026",
    "end": "116074"
  },
  {
    "text": "This is like making it possible\n to capture dependencies in data.",
    "start": "116510",
    "end": "122005"
  },
  {
    "text": "Again, let's think about the single neuron.",
    "start": "123140",
    "end": "125240"
  },
  {
    "text": "We have xt is our input, yt is our output.",
    "start": "125420",
    "end": "129138"
  },
  {
    "text": "And we have ht, our hidden state.",
    "start": "129139",
    "end": "131779"
  },
  {
    "text": "The output yt is computed based \non the hidden state ht.",
    "start": "132590",
    "end": "136610"
  },
  {
    "text": "And we can represent this relationship \nwith the following equation.",
    "start": "136610",
    "end": "140059"
  },
  {
    "text": "The hidden state ht is calculated",
    "start": "149630",
    "end": "152753"
  },
  {
    "text": "based on the current input xt \nand previous hidden state ht-1",
    "start": "152753",
    "end": "158000"
  },
  {
    "text": "In this equation, Wx and Wh are the weight matrices.",
    "start": "167500",
    "end": "171830"
  },
  {
    "text": "XT is the current input,",
    "start": "171850",
    "end": "173776"
  },
  {
    "text": "ht1 is the previous hidden state,",
    "start": "173776",
    "end": "176201"
  },
  {
    "text": "and b is the bias term.",
    "start": "176201",
    "end": "178491"
  },
  {
    "text": "And finally, the activation function",
    "start": "179110",
    "end": "181284"
  },
  {
    "text": "processes this combination\n to produce the final result.",
    "start": "181284",
    "end": "184774"
  },
  {
    "text": "In summary, RNNs use the hidden state to",
    "start": "185800",
    "end": "188710"
  },
  {
    "text": "carry information from \npast inputs to the hidden state.",
    "start": "188710",
    "end": "195219"
  },
  {
    "text": "The hidden state is updated at each time step,",
    "start": "195700",
    "end": "199282"
  },
  {
    "text": "allowing the network to learn and \nremember previous inputs.",
    "start": "199282",
    "end": "203635"
  },
  {
    "text": "The ability to process sequences \nmakes RNNs unique",
    "start": "204010",
    "end": "207377"
  },
  {
    "text": "and we can set them up in different ways.",
    "start": "207377",
    "end": "209758"
  },
  {
    "text": "One way is sequence-to-sequence network.",
    "start": "209950",
    "end": "212979"
  },
  {
    "text": "In here, you feed the network \nwith a sequence of inputs",
    "start": "213430",
    "end": "218200"
  },
  {
    "text": "and it produces a sequence of outputs.",
    "start": "218352",
    "end": "221705"
  },
  {
    "text": "This is really good for tasks like predicting time series data such as stock prices.",
    "start": "226630",
    "end": "232361"
  },
  {
    "text": "Another way is the sequence to vector network.",
    "start": "232660",
    "end": "235813"
  },
  {
    "text": "In here we feed the network with a sequence of inputs.",
    "start": "236050",
    "end": "241067"
  },
  {
    "text": "But we only care about the final output.",
    "start": "247330",
    "end": "249999"
  },
  {
    "text": "Imagine you have a sequence of words from a movie review.",
    "start": "250480",
    "end": "254019"
  },
  {
    "text": "We can feed the network with that\n sequence of words,",
    "start": "254170",
    "end": "257381"
  },
  {
    "text": "and at the end it can give us \na sentiment score",
    "start": "257382",
    "end": "260228"
  },
  {
    "text": "like zero for love, one for hate.",
    "start": "260228",
    "end": "262509"
  },
  {
    "text": "Another way is wait for the sequence network.",
    "start": "263650",
    "end": "266860"
  },
  {
    "text": "In here, we feed the network \nwith one single input",
    "start": "267250",
    "end": "273163"
  },
  {
    "text": "and it produces a sequence of outputs.",
    "start": "273163",
    "end": "277509"
  },
  {
    "text": "Imagine you have an image.",
    "start": "281600",
    "end": "283009"
  },
  {
    "text": "We can feed the network with that image",
    "start": "283370",
    "end": "285792"
  },
  {
    "text": "and the network can generate a caption",
    "start": "285792",
    "end": "288423"
  },
  {
    "text": "describing the image word-by-word.",
    "start": "288423",
    "end": "291067"
  },
  {
    "text": "Lastly, there's an encoder-decoder architecture.",
    "start": "291860",
    "end": "295173"
  },
  {
    "text": "In here, ",
    "start": "295660",
    "end": "297115"
  },
  {
    "text": "we feed the encoder part",
    "start": "298730",
    "end": "301167"
  },
  {
    "text": "with a sequence of inputs\nand it converts it into vector.",
    "start": "301703",
    "end": "307157"
  },
  {
    "text": "And after that, the decoder takes that vector",
    "start": "307580",
    "end": "311341"
  },
  {
    "text": "and produces as a sequence of outputs.",
    "start": "311694",
    "end": "316481"
  },
  {
    "text": "We can imagine this like this:",
    "start": "319797",
    "end": "322590"
  },
  {
    "text": "So we have a sentence in one language",
    "start": "322590",
    "end": "325880"
  },
  {
    "text": "and we give that sentence into the encoder part,",
    "start": "325881",
    "end": "329713"
  },
  {
    "text": "the encoder part will convert it into vector,",
    "start": "329713",
    "end": "332916"
  },
  {
    "text": "and after that, the decoder will \ntake that part, take that vector,",
    "start": "332916",
    "end": "338222"
  },
  {
    "text": "and convert it into a sentence \nin another language.",
    "start": "338222",
    "end": "342693"
  },
  {
    "text": "Now that we have covered how \nour RNNs handle sequences,",
    "start": "343380",
    "end": "346456"
  },
  {
    "text": "let's talk about some key challenges:",
    "start": "346456",
    "end": "348212"
  },
  {
    "text": "vanishing/exploding gradients and\n complexity in training.",
    "start": "348213",
    "end": "351724"
  },
  {
    "text": "One major issue is \nvanishing/exploding gradients.",
    "start": "351990",
    "end": "355169"
  },
  {
    "text": "When training RNNs,",
    "start": "355410",
    "end": "357370"
  },
  {
    "text": "the gradient update the weights\n can become very large",
    "start": "357370",
    "end": "360810"
  },
  {
    "text": "or very small during the back propagation.",
    "start": "360810",
    "end": "363128"
  },
  {
    "text": "So vanishing gradients make\nit hard for the network to",
    "start": "363510",
    "end": "367259"
  },
  {
    "text": "learn from previous inputs\n because updates are too tiny.",
    "start": "367320",
    "end": "371462"
  },
  {
    "text": "Exploding gradients, on the other hand,",
    "start": "376950",
    "end": "379619"
  },
  {
    "text": "keep the gradient unstable because",
    "start": "379950",
    "end": "382259"
  },
  {
    "text": "updates are too large.",
    "start": "382290",
    "end": "384000"
  },
  {
    "text": "To address these issues,",
    "start": "390730",
    "end": "393070"
  },
  {
    "text": "researchers develop specialized\nRNN architectures",
    "start": "394243",
    "end": "397486"
  },
  {
    "text": "like Long Short-Term Memory (LSTM)\n and Gated Recurrent Units (GRU).",
    "start": "397486",
    "end": "400930"
  },
  {
    "text": "These architectures use gates \nto control the flow of information",
    "start": "401200",
    "end": "405586"
  },
  {
    "text": "and keep the gradients stable \nduring the predicted frame.",
    "start": "405586",
    "end": "409269"
  },
  {
    "text": "So another challenge in RNNs is complexity in training.",
    "start": "409750",
    "end": "414940"
  },
  {
    "text": "RNNs require a lot of computational\npower and time to train.",
    "start": "415120",
    "end": "420120"
  },
  {
    "text": "So this is because RNNs needs to",
    "start": "420430",
    "end": "423608"
  },
  {
    "text": "process each sequence a step-by-step.",
    "start": "424550",
    "end": "427309"
  },
  {
    "text": "This can be very time consuming,\nespecially for long sequences.",
    "start": "428030",
    "end": "431990"
  },
  {
    "text": "In conclusion, while RNNs are \nincredibly powerful,",
    "start": "433070",
    "end": "436585"
  },
  {
    "text": "they also come with challenges like",
    "start": "436585",
    "end": "439257"
  },
  {
    "text": "vanishing and exploding gradients and",
    "start": "439391",
    "end": "442224"
  },
  {
    "text": "complex training requirements.",
    "start": "442308",
    "end": "444524"
  },
  {
    "text": "However, the advancements like \nLSTM and",
    "start": "444650",
    "end": "447761"
  },
  {
    "text": "GRU architectures help us to \naddress these issues,",
    "start": "447761",
    "end": "453078"
  },
  {
    "text": "it is also allowing us to train RNNs for \na wide range of applications.",
    "start": "453110",
    "end": "458594"
  }
]