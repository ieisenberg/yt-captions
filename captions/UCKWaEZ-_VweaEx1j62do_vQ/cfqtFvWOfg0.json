[
  {
    "text": "I'm going to state three facts.",
    "start": "300",
    "end": "2149"
  },
  {
    "text": "Your challenge is to tell me how they're related; they're all space in aviation theme, but that's not it.",
    "start": "2160",
    "end": "7440"
  },
  {
    "text": "So here we go! Number one-- the distance from the Earth to the Moon is 54 million kilometers.",
    "start": "7440",
    "end": "13139"
  },
  {
    "text": "Number two-- before I worked at IBM, I worked at a major Australian airline.",
    "start": "13200",
    "end": "18309"
  },
  {
    "text": "And number three-- the James Webb Telescope took the very first pictures of an exoplanet outside of our solar system.",
    "start": "18340",
    "end": "24390"
  },
  {
    "text": "What's the common thread?",
    "start": "25370",
    "end": "27229"
  },
  {
    "text": "Well, the answer is that all three \"facts\" are an example of an hallucination of a large language model, otherwise known as an LLM.",
    "start": "27800",
    "end": "44884"
  },
  {
    "text": "Things like chatGPT and Bing chat.",
    "start": "45520",
    "end": "48340"
  },
  {
    "text": "54 million K, that's the distance to Mars, not the moon.",
    "start": "49030",
    "end": "52659"
  },
  {
    "text": "It's my brother that works at the airline, not me.",
    "start": "53050",
    "end": "55359"
  },
  {
    "text": "And infamously, at the announcement of Google's LLM, Bard, it hallucinated about the Webb telescope.",
    "start": "55360",
    "end": "62610"
  },
  {
    "text": "The first picture of an exoplanet it was actually taken in 2004.",
    "start": "62610",
    "end": "65590"
  },
  {
    "text": "Now, while large language models can generate fluent and coherent text on various topics and domains,",
    "start": "66130",
    "end": "73054"
  },
  {
    "text": "they are also prone to just \"make stuff up\". Plausible sounding nonsense! So let's discuss, first of all, what a hallucination is.",
    "start": "73054",
    "end": "89330"
  },
  {
    "text": "We'll discuss why they happen.",
    "start": "89870",
    "end": "93590"
  },
  {
    "text": "And we'll take some steps to describe how you can minimize hallucinations with LLMs.",
    "start": "93980",
    "end": "102635"
  },
  {
    "text": "Now hallucinations are outputs of LLMs that deviate from facts or contextual logic,",
    "start": "103800",
    "end": "109477"
  },
  {
    "text": "and they can range from minor inconsistencies to completely fabricated or contradictory statements.",
    "start": "109477",
    "end": "115590"
  },
  {
    "text": "And we can categorize hallucinations across different levels of granularity.",
    "start": "115860",
    "end": "120839"
  },
  {
    "text": "Now, at the lowest level of granularity we could consider sentence contradiction.",
    "start": "120900",
    "end": "130687"
  },
  {
    "text": "This is really the simplest type, and this is where an LLM generates a sentence that contradicts one of the previous sentences.",
    "start": "131090",
    "end": "138509"
  },
  {
    "text": "So \"the sky is blue today.\"",
    "start": "138530",
    "end": "141379"
  },
  {
    "text": "\"The sky is green today.\" Another example would be prompt contradiction.",
    "start": "141950",
    "end": "149650"
  },
  {
    "text": "And this is where the generated sentence contradicts with the prompt that was used to generate it.",
    "start": "151250",
    "end": "158000"
  },
  {
    "text": "So if I ask an LLM to write a positive review of a restaurant and its returns, \"the food was terrible and the service was rude,\"",
    "start": "158030",
    "end": "166258"
  },
  {
    "text": "ah, that would be in direct contradiction to what I asked.",
    "start": "166258",
    "end": "170299"
  },
  {
    "text": "Now, we already gave some examples of another type here, which is a factual contradictions.",
    "start": "171050",
    "end": "178177"
  },
  {
    "text": "And these factual contradictions, or factual error hallucinations, are really just that-- absolutely nailed on facts that they got wrong.",
    "start": "178360",
    "end": "186520"
  },
  {
    "text": "Barack Obama was the first president of the United States-- something like that.",
    "start": "186580",
    "end": "190150"
  },
  {
    "text": "And then there are also nonsensical or otherwise irrelevant kind of information based hallucinations",
    "start": "191330",
    "end": "201736"
  },
  {
    "text": "where it just puts in something that really has no place being there. Like \"The capital of France is Paris.\"",
    "start": "201736",
    "end": "206465"
  },
  {
    "text": "\"Paris is also the name of a famous singer.\" Okay, umm, thanks?",
    "start": "207200",
    "end": "212448"
  },
  {
    "text": "Now with the question of what LLMs hallucinations are answered, we really need to answer the question of why.",
    "start": "212448",
    "end": "221750"
  },
  {
    "text": "And it's not an easy one to answer,",
    "start": "221760",
    "end": "223797"
  },
  {
    "text": "because the way that they derive their output is something of a black box, even to the engineers of the LLM itself.",
    "start": "223798",
    "end": "231330"
  },
  {
    "text": "But there are a number of common causes.",
    "start": "231510",
    "end": "234260"
  },
  {
    "text": "So let's take a look at a few of those.",
    "start": "234270",
    "end": "236220"
  },
  {
    "text": "One of those is a data quality.",
    "start": "237370",
    "end": "241013"
  },
  {
    "text": "Now LLMs are trained on a large corpora of text that may contain noise, errors, biases or inconsistencies.",
    "start": "242440",
    "end": "249850"
  },
  {
    "text": "For example, some LLMs were trained by scraping all of Wikipedia and all of Reddit.",
    "start": "249880",
    "end": "255002"
  },
  {
    "text": "It is everything on Reddit 100% accurate?",
    "start": "255220",
    "end": "258515"
  },
  {
    "text": "Well, look, even if it was even if the training data was entirely reliable,",
    "start": "258516",
    "end": "263568"
  },
  {
    "text": "that data may not cover all of the possible topics or domains the LLMs are expected to generate content about.",
    "start": "263568",
    "end": "270279"
  },
  {
    "text": "So LLMs may generalize from data without being able to verify its accuracy or relevance.",
    "start": "270280",
    "end": "277199"
  },
  {
    "text": "And sometimes it just gets it wrong.",
    "start": "277210",
    "end": "279819"
  },
  {
    "text": "As LLM reasoning capabilities improve, hallucinations tend to decline.",
    "start": "280150",
    "end": "286059"
  },
  {
    "text": "Now, another reason why hallucinations can happen is based upon the generation method.",
    "start": "287000",
    "end": "295490"
  },
  {
    "text": "Now, LLMs use various methods and objectives to generate text such as beam search,",
    "start": "296820",
    "end": "301565"
  },
  {
    "text": "sampling, maximum likelihood estimation, or reinforcement learning. And these methods and these objectives may introduce biases",
    "start": "301565",
    "end": "310293"
  },
  {
    "text": "and tradeoffs between things like fluency and diversity, between coherence and creativity, or between accuracy and novelty.",
    "start": "310293",
    "end": "318689"
  },
  {
    "text": "So, for example, beam search may favor high probability, but generic words over low probability, but specific words.",
    "start": "318690",
    "end": "328019"
  },
  {
    "text": "And another common cause for hallucinations is input context.",
    "start": "329310",
    "end": "333839"
  },
  {
    "text": "And this is one we can do something directly about as users.",
    "start": "333840",
    "end": "339000"
  },
  {
    "text": "Now, here, context refers to the information that is given to the model as an input prompt.",
    "start": "339360",
    "end": "344427"
  },
  {
    "text": "Context can help guide the model to produce the relevant and accurate outputs,",
    "start": "344790",
    "end": "349128"
  },
  {
    "text": "but it can also confuse or mislead the model if it's unclear or if it's inconsistent or if it's contradictory.",
    "start": "349128",
    "end": "355229"
  },
  {
    "text": "So, for example, if I ask an LLM chat bot, \"Can cats speak English?\"",
    "start": "355240",
    "end": "361914"
  },
  {
    "text": "I would expect the answer \"No, and do you need to sit down for a moment?\".",
    "start": "361914",
    "end": "367306"
  },
  {
    "text": "But perhaps I just forgotten to include a crucial little bit of information, a bit of context that this conversation thread",
    "start": "367920",
    "end": "375277"
  },
  {
    "text": "is talking about the Garfield cartoon strip, in which case the LLM should have answered,",
    "start": "375277",
    "end": "381131"
  },
  {
    "text": "\"Yes, cats can speak English and that cat is probably going to ask for second helpings of lasagna.\"",
    "start": "381131",
    "end": "387619"
  },
  {
    "text": "Context is important, and if we don't tell it we're looking for generated text suitable for an academic essay or a creative writing exercise,",
    "start": "388470",
    "end": "397612"
  },
  {
    "text": "we can't expect it to respond within that context.",
    "start": "397612",
    "end": "401009"
  },
  {
    "text": "Which brings us nicely to the third and final part-- what can we do to reduce hallucinations in our own conversations with LLMs?",
    "start": "401730",
    "end": "410510"
  },
  {
    "text": "So, yep, one thing we can certainly do is provide clear and specific prompts to the system.",
    "start": "410730",
    "end": "420248"
  },
  {
    "text": "Now, the more precise and the more detailed the input prompt,",
    "start": "421080",
    "end": "424543"
  },
  {
    "text": "the more likely the LLM will generate relevant and, most importantly, accurate outputs.",
    "start": "424544",
    "end": "430649"
  },
  {
    "text": "So, for example, instead of asking \"What happened in World War Two?\" That's not very clear.",
    "start": "431010",
    "end": "436290"
  },
  {
    "text": "It's not very specific.",
    "start": "436290",
    "end": "437429"
  },
  {
    "text": "We could say, \"Can you summarize the major events of World War Two,",
    "start": "437820",
    "end": "441400"
  },
  {
    "text": "including the key countries involved in the primary causes of the conflict?\"",
    "start": "441400",
    "end": "444662"
  },
  {
    "text": "Something like that that really gets at what we are trying to pull from this.",
    "start": "444662",
    "end": "448198"
  },
  {
    "text": "That gives the model a better understanding of what information is expected in the response.",
    "start": "449010",
    "end": "453860"
  },
  {
    "text": "We can employ something called active mitigation strategies.",
    "start": "455030",
    "end": "461910"
  },
  {
    "text": "And what these are are using some of the settings of the LLMs,",
    "start": "463030",
    "end": "466785"
  },
  {
    "text": "such as settings that control the parameters of how the LLM works during generation.",
    "start": "466785",
    "end": "472299"
  },
  {
    "text": "A good example of that is the temperature parameter, which can control the randomness of the output.",
    "start": "472540",
    "end": "477720"
  },
  {
    "text": "So a lower temperature will produce more conservative and focused responses,",
    "start": "477730",
    "end": "482012"
  },
  {
    "text": "while a higher temperature will generate more diverse and creative ones.",
    "start": "482012",
    "end": "486309"
  },
  {
    "text": "But the higher the temperature, the more opportunity for hallucination.",
    "start": "486310",
    "end": "491559"
  },
  {
    "text": "And then one more is multi-shot prompting.",
    "start": "492970",
    "end": "499510"
  },
  {
    "text": "And in contrast to single shot prompting where we only gave one prompt,",
    "start": "500590",
    "end": "505593"
  },
  {
    "text": "multi-shot prompting provides the LLM with multiple examples of the desired output format or context,",
    "start": "505593",
    "end": "511672"
  },
  {
    "text": "and that essentially primes the model, giving a clearer understanding of the user's expectations.",
    "start": "511672",
    "end": "517719"
  },
  {
    "text": "By presenting the LLM with several examples, we help it recognize the pattern or the context more effectively,",
    "start": "518169",
    "end": "525475"
  },
  {
    "text": "and this can be particularly useful in tasks that require a specific output format.",
    "start": "525475",
    "end": "530168"
  },
  {
    "text": "So, generating code, writing poetry or answering questions in a specific style.",
    "start": "530170",
    "end": "536620"
  },
  {
    "text": "So while large language models may sometimes hallucinate and take us on an unexpected journey, 54 million kilometers off target,",
    "start": "536740",
    "end": "546415"
  },
  {
    "text": "understanding the causes and employing the strategies to minimize those causes",
    "start": "546415",
    "end": "553214"
  },
  {
    "text": "really allows us to harness the true potential of these models and reduce hallucinations.",
    "start": "553215",
    "end": "559179"
  },
  {
    "text": "Although I did kind of enjoy reading about my fictional career down under.",
    "start": "560110",
    "end": "565362"
  },
  {
    "text": "If you have any questions, please drop us a line below.",
    "start": "566420",
    "end": "569019"
  },
  {
    "text": "And if you want to see more videos like this in the future, please like and subscribe.",
    "start": "569030",
    "end": "573650"
  },
  {
    "text": "Thanks for watching.",
    "start": "574220",
    "end": "575220"
  }
]