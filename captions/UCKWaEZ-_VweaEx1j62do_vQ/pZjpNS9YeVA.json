[
  {
    "text": "Imagine you just open electronics store, you're hiring some employees.",
    "start": "1020",
    "end": "4588"
  },
  {
    "text": "You need to make sure your clients have a good experience as they walk into the store, hopefully purchase more products.",
    "start": "4800",
    "end": "9330"
  },
  {
    "text": "And you need to standardize all of it.",
    "start": "9600",
    "end": "11339"
  },
  {
    "text": "How do you go about doing that?",
    "start": "11730",
    "end": "12959"
  },
  {
    "text": "As part of this video?",
    "start": "13740",
    "end": "14969"
  },
  {
    "text": "We're going to go over the fundamentals that will empower you to make the right decisions",
    "start": "15180",
    "end": "18863"
  },
  {
    "text": "when it comes to updating and tweaking your LLMs for your requirement.",
    "start": "18863",
    "end": "22949"
  },
  {
    "text": "We will do this from a perspective of context optimization and optimization.",
    "start": "23400",
    "end": "28739"
  },
  {
    "text": "Context optimization is essentially the window or the text that the model is going to take into account when it generates the text,",
    "start": "29310",
    "end": "36028"
  },
  {
    "text": " and the model optimization is actually updating the model based on specific requirements.",
    "start": "36028",
    "end": "41219"
  },
  {
    "text": "Now let's go back to our store.",
    "start": "41760",
    "end": "43858"
  },
  {
    "text": "We have hired our first employee.",
    "start": "45740",
    "end": "47899"
  },
  {
    "text": "A generalist,",
    "start": "48530",
    "end": "49460"
  },
  {
    "text": "polite enough,",
    "start": "49460",
    "end": "50460"
  },
  {
    "text": "But you won't just let him loose in the store.",
    "start": "50690",
    "end": "52430"
  },
  {
    "text": "You want to give some guidelines to this person.",
    "start": "52460",
    "end": "54649"
  },
  {
    "text": "So always greet the prospective clients.",
    "start": "55010",
    "end": "57678"
  },
  {
    "text": "Make sure you are polite and based on the question they ask and give them the top three options.",
    "start": "57920",
    "end": "62210"
  },
  {
    "text": "Maybe there's some sales going on that may be relevant to the client, etc., etc..",
    "start": "62420",
    "end": "66188"
  },
  {
    "text": "Similarly, in the context of an LLM ",
    "start": "66860",
    "end": "69769"
  },
  {
    "text": "They have this thing called prompt engineering, prompt engineers giving very clear guidelines on what they expect from the model.",
    "start": "71450",
    "end": "77719"
  },
  {
    "text": "You can do so by giving some text.",
    "start": "78110",
    "end": "79759"
  },
  {
    "text": "You can also give some examples like input and output so that the model can understand what are you really looking for?",
    "start": "79790",
    "end": "85489"
  },
  {
    "text": "You can also help the model break down a complex problem into sub points and make sure it's kind of understanding what you're going after in the long run.",
    "start": "85820",
    "end": "93798"
  },
  {
    "text": "This is called chain after prompting.",
    "start": "93840",
    "end": "95298"
  },
  {
    "text": "Our employee is doing well, but getting inundated by new information coming from all the new devices.",
    "start": "98420",
    "end": "103790"
  },
  {
    "text": "That smile can turn into a frown really quickly because it's hard to be up to speed with all the technology changes coming in.",
    "start": "104360",
    "end": "110599"
  },
  {
    "text": "So you have come up with a strategy where you have created this manual and this panel has all the updates for all the different gadgets coming in.",
    "start": "111200",
    "end": "119629"
  },
  {
    "text": "So you're good.",
    "start": "119750",
    "end": "120750"
  },
  {
    "text": "But you kind of expect the employee to read that document every time a user ask a question.",
    "start": "120920",
    "end": "125540"
  },
  {
    "text": "So you have devised a strategy based on the question.",
    "start": "125900",
    "end": "129580"
  },
  {
    "text": "You report some of the pages from the manual, give it to the employee to its answer comes back to it.",
    "start": "129590",
    "end": "134629"
  },
  {
    "text": "That, in a way is like rag, retrieval augmented generation,",
    "start": "135140",
    "end": "138886"
  },
  {
    "text": " which allows you to connect this LLM to your data sources to make sure that you're getting the right answers.",
    "start": "138886",
    "end": "145189"
  },
  {
    "text": "This can address things like hallucination as well,",
    "start": "145460",
    "end": "148021"
  },
  {
    "text": "because it can really in the prompt in doing say you need to give the answer only from these specified documents.",
    "start": "148021",
    "end": "153139"
  },
  {
    "text": "So it's a really powerful tool as well.",
    "start": "153260",
    "end": "154879"
  },
  {
    "text": "Now, going back to our store, business is doing really well.",
    "start": "155720",
    "end": "160280"
  },
  {
    "text": "And we need to hire more employees.",
    "start": "161170",
    "end": "163659"
  },
  {
    "text": "That's great.",
    "start": "164810",
    "end": "165410"
  },
  {
    "text": "But it was already hard with one employee.",
    "start": "165410",
    "end": "167190"
  },
  {
    "text": "How do you make sure you standardize the behavior for all three of them?",
    "start": "167210",
    "end": "170300"
  },
  {
    "text": "Being polite can mean different things to different people.",
    "start": "170720",
    "end": "173029"
  },
  {
    "text": "Secondly, your customers are getting more savvy.",
    "start": "173570",
    "end": "176089"
  },
  {
    "text": "They're asking more specialized questions, asking how to fix things.",
    "start": "176090",
    "end": "179330"
  },
  {
    "text": "So just reading off a guide is not going to do it.",
    "start": "179480",
    "end": "181668"
  },
  {
    "text": "What usually is, is you need them to go through.",
    "start": "182210",
    "end": "185300"
  },
  {
    "text": "A training school.",
    "start": "186230",
    "end": "187230"
  },
  {
    "text": "Be it from a sales perspective or technical perspective to really make sure that questions are answered.",
    "start": "188100",
    "end": "192330"
  },
  {
    "text": "That is like fine tuning.",
    "start": "192900",
    "end": "194430"
  },
  {
    "text": "Fine tuning allows you to actually update the model parameters based on your data",
    "start": "196100",
    "end": "200419"
  },
  {
    "text": "to ensure that you influence the behavior of the model and also make it specialize in a specific domain as well.",
    "start": "200419",
    "end": "206750"
  },
  {
    "text": "Now, remember in the beginning I mentioned we are doing this in the lens of context optimization.",
    "start": "207530",
    "end": "213080"
  },
  {
    "text": "And LLM optimization.",
    "start": "217360",
    "end": "219520"
  },
  {
    "text": "So all that means is that RAG and PE are essentially taking all the information you need the model to know before hand,",
    "start": "221790",
    "end": "229514"
  },
  {
    "text": "passing it over for the model to make its deduction, generate the text and come back to it.",
    "start": "229514",
    "end": "234720"
  },
  {
    "text": "Fine tuning is actually optimizing the model to ensure that you're getting the right responses with the right kind of behavior that you would need.",
    "start": "235080",
    "end": "242250"
  },
  {
    "text": "This addresses the two key problems we keep hearing from practitioners on why they're reluctant to move into production model behavior,",
    "start": "242760",
    "end": "250439"
  },
  {
    "text": "How would you really modulate the model output both from a text perspective as well, from kind of the vernacular and the qualitative aspects, if you will,",
    "start": "250740",
    "end": "258749"
  },
  {
    "text": "and then the real time data access,",
    "start": "258749",
    "end": "260909"
  },
  {
    "text": "how quickly can you get the model to answer a question from a real time data as well as ensure that it's accurate and relevant to the user?",
    "start": "261029",
    "end": "267630"
  },
  {
    "text": "So let's summarize that discussion so far at five points.",
    "start": "268990",
    "end": "271779"
  },
  {
    "text": "So this one is this whole technique or set of techniques is additive.",
    "start": "272170",
    "end": "276639"
  },
  {
    "text": "So they're all working and complementing each other.",
    "start": "276820",
    "end": "279459"
  },
  {
    "text": "The first to RAG and PE are done in the context of the context window optimization.",
    "start": "279730",
    "end": "284619"
  },
  {
    "text": "Fine tuning actually updates the model parameters.",
    "start": "284950",
    "end": "287470"
  },
  {
    "text": "This is important because the token window is limited.",
    "start": "287740",
    "end": "291099"
  },
  {
    "text": "So the more text you add to it, there can be more noise.",
    "start": "291340",
    "end": "294518"
  },
  {
    "text": "So you need to be careful about what you're passing to the model.",
    "start": "294730",
    "end": "297220"
  },
  {
    "text": "Secondly, on the model, while it may be expensive, the more you spend on the data and actually update the model with good quality data,",
    "start": "297670",
    "end": "304498"
  },
  {
    "text": "you can then use a smaller LLM, instead a bigger LLM and save costs in the long run as well.",
    "start": "304498",
    "end": "309669"
  },
  {
    "text": "The second one is always start with prompt engineering.",
    "start": "310420",
    "end": "314019"
  },
  {
    "text": "This is one of the most powerful and agile tools that you have in your repository to ensure that,",
    "start": "314050",
    "end": "318744"
  },
  {
    "text": "a) you understand whether even having an LLM based solution is right to the kind of data that you have the end users,",
    "start": "318744",
    "end": "325899"
  },
  {
    "text": "Is the baseline model accurate,",
    "start": "326050",
    "end": "327729"
  },
  {
    "text": "And all the work that you're doing, even the trial and error can actually be used for fine tuning.",
    "start": "327910",
    "end": "332380"
  },
  {
    "text": "So it's really, really worth it.",
    "start": "332530",
    "end": "334240"
  },
  {
    "text": "The third one is also important.",
    "start": "335020",
    "end": "336759"
  },
  {
    "text": "People start worrying about the context window optimization too soon.",
    "start": "336880",
    "end": "340540"
  },
  {
    "text": "So focus more on the accuracy",
    "start": "340720",
    "end": "342820"
  },
  {
    "text": "versus is the optimization.",
    "start": "343980",
    "end": "345450"
  },
  {
    "text": "So what I mean by that is as you get closer to the right answer, especially in the context of window optimization,",
    "start": "347680",
    "end": "354479"
  },
  {
    "text": "keep looking into the right answers and then start seeing different strategies on how you can reduce the window.",
    "start": "354480",
    "end": "359829"
  },
  {
    "text": "The fourth one is people say that data quantity is really key for fine tuning.",
    "start": "360670",
    "end": "365620"
  },
  {
    "text": "Yes, that's important.",
    "start": "365650",
    "end": "366759"
  },
  {
    "text": "But I would take the data quality, DQ, better than honestly that data quantity.",
    "start": "366970",
    "end": "373509"
  },
  {
    "text": "This is really valuable because you can really start a good example of fine tuning by just 100 examples.",
    "start": "376060",
    "end": "381690"
  },
  {
    "text": "Of course, that differs from every use case, but really focus on the quality.",
    "start": "381760",
    "end": "385720"
  },
  {
    "text": "Some of the output that we get from prompt engineering is also going to be very important.",
    "start": "385900",
    "end": "389619"
  },
  {
    "text": "This brings me to my last point.",
    "start": "390130",
    "end": "391779"
  },
  {
    "text": "You need to be able to quantify",
    "start": "392410",
    "end": "394210"
  },
  {
    "text": "and baseline",
    "start": "396550",
    "end": "397569"
  },
  {
    "text": "your success.",
    "start": "399540",
    "end": "400540"
  },
  {
    "text": "Just saying that the answer is good enough is not going to cut it, especially when you try these techniques and try the nuances.",
    "start": "401290",
    "end": "406809"
  },
  {
    "text": "The permutations between these three can be huge.",
    "start": "407080",
    "end": "409689"
  },
  {
    "text": "So you need to make sure from an accuracy perspective, precision perspective.",
    "start": "409870",
    "end": "414190"
  },
  {
    "text": "Again, going back to the context optimization, if you're using RAG, not only is the answer important,",
    "start": "414400",
    "end": "419823"
  },
  {
    "text": "it's also important what kind of documents you got from the vector database.",
    "start": "419823",
    "end": "423250"
  },
  {
    "text": "This will help you reduce latency.",
    "start": "423670",
    "end": "425409"
  },
  {
    "text": "So a lot of really good solutions that you can get if you can start really quantifying everything and what what success looks like for you.",
    "start": "425560",
    "end": "432639"
  },
  {
    "text": "So going to our diagram here, the two key commonality between all three is going to have increase accuracy.",
    "start": "433270",
    "end": "440198"
  },
  {
    "text": "Reducing hallucinations.",
    "start": "440890",
    "end": "442300"
  },
  {
    "text": "So not making up answers.",
    "start": "442360",
    "end": "443948"
  },
  {
    "text": "Start with your PE",
    "start": "444550",
    "end": "446798"
  },
  {
    "text": "Prompt Engineering will help you really ensure that you have the right solution.",
    "start": "446800",
    "end": "450039"
  },
  {
    "text": "So really quick iteration, super valuable RAG is going to help you connect your contact window to external data sources.",
    "start": "450050",
    "end": "456970"
  },
  {
    "text": "You can give it some guidance as well, and fine tuning actually changes the model behavior where you can control it more",
    "start": "456970",
    "end": "462751"
  },
  {
    "text": "and can become a specialized model in a specific domain that you have.",
    "start": "462751",
    "end": "466660"
  },
  {
    "text": "In terms of the commonality between drag and prompt engineering context window optimization is key.",
    "start": "467410",
    "end": "472949"
  },
  {
    "text": "Of course it is constrained by that.",
    "start": "472960",
    "end": "474970"
  },
  {
    "text": "So as you look for accuracy, you need to focus on how can you optimize that as well.",
    "start": "475150",
    "end": "479682"
  },
  {
    "text": "Between prompt engine and fine tuning wise, they both are kind of inferring some model. They do it in different ways.",
    "start": "479683",
    "end": "485920"
  },
  {
    "text": "So prompting and give some guidance on responding three points fine tuning almost guarantees it",
    "start": "486100",
    "end": "488966"
  },
  {
    "text": "in the vernacular that you want it.",
    "start": "488966",
    "end": "492850"
  },
  {
    "text": "Finally between RAG in fine tuning mode can incorporate the data sources, but really think of RAG as the short term memory",
    "start": "493390",
    "end": "499418"
  },
  {
    "text": "and fine tune as a long term memory for what you're trying to do.",
    "start": "499418",
    "end": "502509"
  },
  {
    "text": "So if I were to summarize context optimization is super valuable.",
    "start": "503080",
    "end": "507490"
  },
  {
    "text": "It is one of the easiest and the first route that you should take to how to optimize an LLM model.",
    "start": "507490",
    "end": "512365"
  },
  {
    "text": "The second one is once you have decided, okay, you have optimize it,",
    "start": "512770",
    "end": "516065"
  },
  {
    "text": "but you are saying you're getting more and more end users, latency is becoming a problem,",
    "start": "516066",
    "end": "520599"
  },
  {
    "text": "and now you know, okay, you know what?",
    "start": "521230",
    "end": "522548"
  },
  {
    "text": "I can fine tune my use case a bit more.",
    "start": "522549",
    "end": "524590"
  },
  {
    "text": "That's where you use fine tuning.",
    "start": "524830",
    "end": "526330"
  },
  {
    "text": "This will help you really specialize the model.",
    "start": "526960",
    "end": "529119"
  },
  {
    "text": "It will not be a generalist, so there's a risk there.",
    "start": "529420",
    "end": "531909"
  },
  {
    "text": "But as you focus your use case more fine tuning is the right way to go.",
    "start": "532120",
    "end": "535510"
  },
  {
    "text": "So if I were to summarize the discussion, as you know, all three techniques are really powerful.",
    "start": "537030",
    "end": "541679"
  },
  {
    "text": "But if you see from the lens of context optimization",
    "start": "542010",
    "end": "545084"
  },
  {
    "text": "focusing on all the words and things you want to send to the model before it generates text,",
    "start": "545084",
    "end": "549884"
  },
  {
    "text": "it is limited by the number of tokens.",
    "start": "549884",
    "end": "552089"
  },
  {
    "text": "So the more you increase there, there's going to be more latency, there's going to be maybe more downtime,",
    "start": "552360",
    "end": "558516"
  },
  {
    "text": "and the more documents you bring in, it could actually create more noise for the model",
    "start": "558517",
    "end": "561803"
  },
  {
    "text": "because it really doesn't understand that specific data.",
    "start": "561803",
    "end": "564480"
  },
  {
    "text": "However, on the other hand, if you have a model and you know, you have specific vernacular, very specialized domain,",
    "start": "564780",
    "end": "571442"
  },
  {
    "text": "medical, financial, legal, etc., etc.,",
    "start": "571442",
    "end": "574951"
  },
  {
    "text": "fine tuning is an option, but you can actually update the parameters of the model using your data.",
    "start": "574951",
    "end": "580410"
  },
  {
    "text": "As I mentioned before, you can look at the input and output you got from prompt engineering",
    "start": "580680",
    "end": "584938"
  },
  {
    "text": "and make the model a more specialized expert.",
    "start": "584939",
    "end": "587580"
  },
  {
    "text": "It can also help you control the model behavior, which is super important when you talk about corporations",
    "start": "587910",
    "end": "593558"
  },
  {
    "text": "and them using a LLMs for their end use those solutions as well.",
    "start": "593558",
    "end": "596940"
  },
  {
    "text": "Prompt engineering and RAG.",
    "start": "598180",
    "end": "599600"
  },
  {
    "text": "Again, the context window is the best way to kind of start and really ensure that you understand of the right way to go.",
    "start": "599620",
    "end": "605980"
  },
  {
    "text": "And you can augmented with fine tuning at the appropriate stage as well.",
    "start": "606340",
    "end": "609909"
  }
]