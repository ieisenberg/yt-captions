[
  {
    "start": "0",
    "end": "18000"
  },
  {
    "text": "Large language models. They are everywhere.",
    "start": "60",
    "end": "2632"
  },
  {
    "text": "They get some things amazingly right",
    "start": "2632",
    "end": "5267"
  },
  {
    "text": "and other things very interestingly wrong.",
    "start": "5267",
    "end": "7741"
  },
  {
    "text": "My name is Marina Danilevsky.",
    "start": "7819",
    "end": "9578"
  },
  {
    "text": "I am a Senior Research Scientist here at IBM Research.",
    "start": "9578",
    "end": "12236"
  },
  {
    "text": "And I want to tell you about a framework to help large language models",
    "start": "12314",
    "end": "16549"
  },
  {
    "text": "be more accurate and more up to date:",
    "start": "16549",
    "end": "18647"
  },
  {
    "start": "18000",
    "end": "42000"
  },
  {
    "text": "Retrieval-Augmented Generation, or RAG.",
    "start": "18648",
    "end": "21900"
  },
  {
    "text": "Let's just talk about the \"Generation\" part for a minute.",
    "start": "22680",
    "end": "24784"
  },
  {
    "text": "So forget the \"Retrieval-Augmented\".",
    "start": "24784",
    "end": "26709"
  },
  {
    "text": "So the generation, this refers to large language models, or LLMs,",
    "start": "26800",
    "end": "31077"
  },
  {
    "text": "that generate text in response to a user query, referred to as a prompt.",
    "start": "31077",
    "end": "35809"
  },
  {
    "text": "These models can have some undesirable behavior.",
    "start": "36000",
    "end": "38269"
  },
  {
    "text": "I want to tell you an anecdote to illustrate this.",
    "start": "38269",
    "end": "41284"
  },
  {
    "text": "So my kids, they recently asked me this question:",
    "start": "41284",
    "end": "44440"
  },
  {
    "start": "42000",
    "end": "82000"
  },
  {
    "text": "\"In our solar system, what planet has the most moons?\"",
    "start": "44440",
    "end": "48530"
  },
  {
    "text": "And my response was, “Oh, that's really great that you're asking this question. I loved space when I was your age.”",
    "start": "48713",
    "end": "55748"
  },
  {
    "text": "Of course, that was like 30 years ago.",
    "start": "55748",
    "end": "58074"
  },
  {
    "text": "But I know this! I read an article",
    "start": "58074",
    "end": "60924"
  },
  {
    "text": "and the article said that it was Jupiter and 88 moons. So that's the answer.",
    "start": "60924",
    "end": "66234"
  },
  {
    "text": "Now, actually, there's a couple of things wrong with my answer.",
    "start": "66234",
    "end": "69994"
  },
  {
    "text": "First of all, I have no source to support what I'm saying.",
    "start": "70380",
    "end": "74026"
  },
  {
    "text": "So even though I confidently said “I read an article, I know the answer!”, I'm not sourcing it.",
    "start": "74026",
    "end": "78232"
  },
  {
    "text": "I'm giving the answer off the top of my head.",
    "start": "78232",
    "end": "80592"
  },
  {
    "text": "And also, I actually haven't kept up with this for awhile, and my answer is out of date.",
    "start": "80592",
    "end": "85260"
  },
  {
    "start": "82000",
    "end": "138000"
  },
  {
    "text": "So we have two problems here. One is no source. And the second problem is that I am out of date.  ",
    "start": "86100",
    "end": "93000"
  },
  {
    "text": "And these, in fact, are two behaviors that are often observed as problematic",
    "start": "95400",
    "end": "101916"
  },
  {
    "text": "when interacting with large language models. They’re LLM challenges.",
    "start": "101916",
    "end": "106288"
  },
  {
    "text": "Now, what would have happened if I'd taken a beat and first gone",
    "start": "106288",
    "end": "110462"
  },
  {
    "text": "and looked up the answer on a reputable source like NASA?",
    "start": "110462",
    "end": "114180"
  },
  {
    "text": "Well, then I would have been able to say, “Ah, okay! So the answer is Saturn with 146 moons.”",
    "start": "115380",
    "end": "123252"
  },
  {
    "text": "And in fact, this keeps changing because scientists keep on discovering more and more moons.",
    "start": "123252",
    "end": "128084"
  },
  {
    "text": "So I have now grounded my answer in something more \nbelievable.",
    "start": "128085",
    "end": "131013"
  },
  {
    "text": "I have not hallucinated or made up an answer.",
    "start": "131091",
    "end": "133082"
  },
  {
    "text": "Oh, by the way, I didn't leak personal information about how long ago it's been since I was obsessed with space.",
    "start": "133082",
    "end": "138453"
  },
  {
    "start": "138000",
    "end": "263000"
  },
  {
    "text": "All right, so what does this have to do with large language models?",
    "start": "138753",
    "end": "142065"
  },
  {
    "text": "Well, how would a large language model have answered this question?",
    "start": "142065",
    "end": "146496"
  },
  {
    "text": "So let's say that I have a user asking this question about moons.",
    "start": "146496",
    "end": "151620"
  },
  {
    "text": "A large language model would confidently say,",
    "start": "151790",
    "end": "157920"
  },
  {
    "text": "OK, I have been trained and from what I know in my parameters during my training, the answer is Jupiter.",
    "start": "157921",
    "end": "165000"
  },
  {
    "text": "The answer is wrong. But, you know, we don't know.",
    "start": "166680",
    "end": "170230"
  },
  {
    "text": "The large language model is very confident in what it answered.",
    "start": "170231",
    "end": "172829"
  },
  {
    "text": "Now, what happens when you add this retrieval augmented part here?",
    "start": "172946",
    "end": "177688"
  },
  {
    "text": "What does that mean?",
    "start": "177688",
    "end": "179125"
  },
  {
    "text": "That means that now, instead of just relying on what the LLM knows,",
    "start": "179216",
    "end": "182892"
  },
  {
    "text": "we are adding a content store.",
    "start": "182892",
    "end": "185373"
  },
  {
    "text": "This could be open like the internet.",
    "start": "185556",
    "end": "187756"
  },
  {
    "text": "This can be closed like some collection of documents, collection of policies, whatever.",
    "start": "187756",
    "end": "194124"
  },
  {
    "text": "The point, though, now is that the LLM first goes and talks",
    "start": "194124",
    "end": "197765"
  },
  {
    "text": "to the content store and says, “Hey, can you retrieve for me",
    "start": "197766",
    "end": "202459"
  },
  {
    "text": "information that is relevant to what the user's query was?”",
    "start": "202800",
    "end": "205741"
  },
  {
    "text": "And now, with this retrieval-augmented answer, it's not Jupiter anymore.",
    "start": "205845",
    "end": "211948"
  },
  {
    "text": "We know that it is Saturn. What does this look like?",
    "start": "211948",
    "end": "215612"
  },
  {
    "text": "Well, first user prompts the LLM with their question.",
    "start": "215613",
    "end": "226339"
  },
  {
    "text": "They say, this is what my question was.",
    "start": "226482",
    "end": "228394"
  },
  {
    "text": "And originally, if we're just talking to a generative model,",
    "start": "228395",
    "end": "231978"
  },
  {
    "text": "the generative model says, “Oh, okay, I know the response. Here it is. Here's my response.”  ",
    "start": "232260",
    "end": "237167"
  },
  {
    "text": "But now in the RAG framework,",
    "start": "237600",
    "end": "240303"
  },
  {
    "text": "the generative model actually has an instruction that says, \"No, no, no.\"",
    "start": "240303",
    "end": "244318"
  },
  {
    "text": "\"First, go and retrieve relevant content.\"",
    "start": "244318",
    "end": "248515"
  },
  {
    "text": "\"Combine that with the user's question and only then generate the answer.\"",
    "start": "248660",
    "end": "253366"
  },
  {
    "text": "So the prompt now has three parts:",
    "start": "253549",
    "end": "257659"
  },
  {
    "text": "the instruction to pay attention to, the retrieved content, together with the user's question.",
    "start": "257659",
    "end": "263133"
  },
  {
    "start": "263000",
    "end": "395000"
  },
  {
    "text": "Now give a response. And in fact, now you can give evidence for why your response was what it was.  ",
    "start": "263318",
    "end": "269455"
  },
  {
    "text": "So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?  ",
    "start": "270000",
    "end": "275340"
  },
  {
    "text": "So first of all, I'll start with the out of date part.",
    "start": "275940",
    "end": "278604"
  },
  {
    "text": "Now, instead of having to retrain your model, if new information comes up, like,",
    "start": "278604",
    "end": "283224"
  },
  {
    "text": "hey, we found some more moons-- now to Jupiter again, maybe it'll be Saturn again in the future.",
    "start": "283224",
    "end": "288123"
  },
  {
    "text": "All you have to do is you augment your data store with new information, update information.",
    "start": "288227",
    "end": "293189"
  },
  {
    "text": "So now the next time that a user comes and asks the question, we're ready.",
    "start": "293189",
    "end": "297686"
  },
  {
    "text": "We just go ahead and retrieve the most up to date information.",
    "start": "297686",
    "end": "300291"
  },
  {
    "text": "The second problem, source.",
    "start": "300291",
    "end": "302258"
  },
  {
    "text": "Well, the large language model is now being instructed to pay attention",
    "start": "302259",
    "end": "307435"
  },
  {
    "text": "to primary source data before giving its response.",
    "start": "307435",
    "end": "310984"
  },
  {
    "text": "And in fact, now being able to give evidence.",
    "start": "310984",
    "end": "313711"
  },
  {
    "text": "This makes it less likely to hallucinate or to leak data",
    "start": "313711",
    "end": "317361"
  },
  {
    "text": "because it is less likely to rely only on information that it learned during training.",
    "start": "317361",
    "end": "321487"
  },
  {
    "text": "It also allows us to get the model to have a behavior that can be very positive,",
    "start": "321761",
    "end": "326580"
  },
  {
    "text": "which is knowing when to say, “I don't know.”",
    "start": "326580",
    "end": "329419"
  },
  {
    "text": "If the user's question cannot be reliably answered based on your data store,",
    "start": "329420",
    "end": "335362"
  },
  {
    "text": "the model should say, \"I don't know,\" instead of making up something that is believable and may mislead the user.",
    "start": "335362",
    "end": "341900"
  },
  {
    "text": "This can have a negative effect as well though, because if the retriever is not sufficiently good",
    "start": "341900",
    "end": "347578"
  },
  {
    "text": "to give the large language model the best, most high-quality grounding information,",
    "start": "347578",
    "end": "353062"
  },
  {
    "text": "then maybe the user's query that is answerable doesn't get an answer.",
    "start": "353062",
    "end": "357287"
  },
  {
    "text": "So this is actually why lots of folks, including many of us here at IBM,",
    "start": "357287",
    "end": "361469"
  },
  {
    "text": "are working the problem on both sides.",
    "start": "361469",
    "end": "363576"
  },
  {
    "text": "We are both working to improve the retriever",
    "start": "363576",
    "end": "366759"
  },
  {
    "text": "to give the large language model the best quality data on which to ground its response,",
    "start": "366760",
    "end": "372892"
  },
  {
    "text": "and also the generative part so that the LLM can give the richest, best response finally to the user",
    "start": "372892",
    "end": "379238"
  },
  {
    "text": "when it generates the answer.",
    "start": "379238",
    "end": "380947"
  },
  {
    "text": "Thank you for learning more about RAG and like and subscribe to the channel.",
    "start": "381103",
    "end": "385679"
  },
  {
    "text": "Thank you.",
    "start": "385679",
    "end": "386219"
  }
]