[
  {
    "start": "0",
    "end": "141000"
  },
  {
    "text": "Bryan Casey: Hello and\nwelcome to Mixture of Experts. I am not your host, Tim Huang. Uh, we have let Tim regrettably\ngo on vacation this week.",
    "start": "10080",
    "end": "17879"
  },
  {
    "text": "So I'm going to be doing my\nvery worst impersonation of him. So thank you all for\nbearing with us this week.",
    "start": "17880",
    "end": "23260"
  },
  {
    "text": "But I am, I am Brian Casey and, uh,\nthrilled to be joined with three other distinguished guests this week.",
    "start": "23270",
    "end": "29119"
  },
  {
    "text": "Who are going to help us\ncover the week's news in. Uh, across product announcements, new\nresearch, um, this week, we've got three",
    "start": "29120",
    "end": "36175"
  },
  {
    "text": "exciting topics, uh, on deck for us. First, we're going to start by\nfollowing up on a previous segment",
    "start": "36175",
    "end": "41595"
  },
  {
    "text": "we actually had two weeks ago. So two weeks ago, we talked about, uh,\nthe introduction of Google's AI overviews.",
    "start": "41604",
    "end": "47164"
  },
  {
    "text": "Those things have now been out in the\nwild for two weeks, and the market reaction to them has also been at times\nwild, and so we'll discuss a little",
    "start": "47500",
    "end": "55640"
  },
  {
    "text": "bit how the market is responding to,\nto, for some folks, what is probably their first, uh, experience with Gen AI.",
    "start": "55640",
    "end": "62200"
  },
  {
    "text": "Um, second, we're going to\nbe talking about a model that turned itself into a bridge, the\nGolden Gate Bridge, specifically.",
    "start": "62519",
    "end": "68310"
  },
  {
    "text": "Um, so Golden Gate Claude\nand the implications, um, just around interpretability.",
    "start": "68560",
    "end": "73460"
  },
  {
    "text": "Safety and how hopefully we at some\npoint can find a different sort of bridge between Plausibly useful and actually\nuseful when it comes to some of this work",
    "start": "73710",
    "end": "82530"
  },
  {
    "text": "around interpretability And then finally\nevery week feels like it's a good week to",
    "start": "82530",
    "end": "88359"
  },
  {
    "text": "talk about scaling laws but with Nvidia\nearnings with Microsoft introducing what",
    "start": "88359",
    "end": "94420"
  },
  {
    "text": "has now become on the internet known as\nThe whale computer, um, and some even just at the recent discussion on the web about\nrunning out of data for pre training.",
    "start": "94420",
    "end": "102470"
  },
  {
    "text": "Now's as good a time as any to talk\nabout the topic and maybe to take a slightly different approach, uh,\non it that we have in the past.",
    "start": "102780",
    "end": "109869"
  },
  {
    "text": "So today, as usual, we are joined by\na distinguished group of researchers,",
    "start": "109869",
    "end": "114899"
  },
  {
    "text": "product leaders, and engineers. Uh, I am joined by Kate Soule, Program\nDirector, Generative AI Research.",
    "start": "114929",
    "end": "121490"
  },
  {
    "text": "So welcome to the podcast, Kate. Kate Soule: Thanks, Brian. Bryan Casey: Chris Hay, uh, Distinguished\nEngineer, CTO, Customer Transformation.",
    "start": "121490",
    "end": "129140"
  },
  {
    "text": "Welcome back, Chris. Chris Hay: What up? Bryan Casey: And a newbie on\nthe show, Skyler Speakman,",
    "start": "129140",
    "end": "135560"
  },
  {
    "text": "Senior Research Scientist. So welcome to the show, Skyler. Skyler Speakman: My first time here. I'm looking forward to it.",
    "start": "135660",
    "end": "140250"
  },
  {
    "start": "141000",
    "end": "910000"
  },
  {
    "text": "Bryan Casey: So thanks\ny'all for being here. We will start with AI overviews. As I mentioned two weeks ago, Google said\nthat they were going to roll these out",
    "start": "145859",
    "end": "153880"
  },
  {
    "text": "across the United States, and they did\nin fact do that, and very predictably, the first thing the internet did was\nlatch on to every single example that was",
    "start": "153890",
    "end": "162509"
  },
  {
    "text": "funny or troubling, uh, around various\nhallucinations that were happening, uh, and of course, those things have\nbeen going viral across social media.",
    "start": "162509",
    "end": "170930"
  },
  {
    "text": "I wrote down some of my\nfavorite examples that I've saw. Which included Google recommending\nthat the correct number of rocks",
    "start": "171220",
    "end": "177120"
  },
  {
    "text": "to eat is a small number of rocks. Um, that a pair of headphones\nweighs 350 dollars.",
    "start": "177120",
    "end": "183890"
  },
  {
    "text": "Uh, that certain toys are great\nfor small kids when actually they're potentially fatal. Uh, and then finally one that\nI think is yet another example",
    "start": "184170",
    "end": "191979"
  },
  {
    "text": "of some of the problems. But when asked which race is the\nstrongest, Google said that white men of Nordic and Eastern European\ndescent, um, are in fact the strongest.",
    "start": "191979",
    "end": "200110"
  },
  {
    "text": "Skyler Speakman: Ah, I\nhad not heard that one. Bryan Casey: That was, uh, yes. So all of those things.",
    "start": "200110",
    "end": "205750"
  },
  {
    "text": "So I do want to start by maybe\nadding a little bit of balance, uh, to this, which is like Gemini is a\nvery capable model, uh, actually.",
    "start": "206730",
    "end": "214700"
  },
  {
    "text": "And the thing we're not seeing on the\ninternet is all the things that are actually going fine and well, right? People are cherry picking, to\nsome extent, examples that are",
    "start": "214700",
    "end": "222879"
  },
  {
    "text": "particularly, uh, Comical or troubling. Um, and one of the things that I'm sort\nof reminded of is that Twitter is not",
    "start": "223040",
    "end": "229780"
  },
  {
    "text": "real life, uh, but it does feel like a\ndifferent level of visibility for this content than just when it was hidden\nbehind, you know, a chat bot that you",
    "start": "229780",
    "end": "237509"
  },
  {
    "text": "had to consciously, uh, sign up for. And even if LLMs are hallucinating,\nlet's just say 1 percent of the time,",
    "start": "237510",
    "end": "243459"
  },
  {
    "text": "it's more than that, but let's just\nsay it was only 1 percent of the time. Knowing how much search volume\nis on Google, that's still a",
    "start": "243460",
    "end": "250430"
  },
  {
    "text": "staggering volume of hallucinations\nthat are happening every day. Um, and so Chris, maybe you want to just\nstart, turn it over to you, get your sort",
    "start": "250430",
    "end": "259058"
  },
  {
    "text": "of initial reaction to it, and maybe just\ncomment on You know, what do you think is the right way to think about this problem?",
    "start": "259059",
    "end": "265395"
  },
  {
    "text": "Is this like a nines\nof reliability problem? Do people need to start treating machines\nmore like they treat humans with like",
    "start": "265425",
    "end": "272204"
  },
  {
    "text": "a degree of not trust necessarily,\nbut like a trust, but verify, um, or",
    "start": "272204",
    "end": "277594"
  },
  {
    "text": "do you think the market's just cherry\npicking examples here and like, it's actually going mostly fine and it will\njust continue to get better over time.",
    "start": "277594",
    "end": "283554"
  },
  {
    "text": "Chris Hay: So. I think it's a really interesting question\nbecause we've all been doing retrieval",
    "start": "283654",
    "end": "289085"
  },
  {
    "text": "augmented generation for a while, right? Um, but this is really retrieval\naugmented generation on a global scale.",
    "start": "289085",
    "end": "295155"
  },
  {
    "text": "And the big issue that you have here\nis that when you're doing AI overviews,",
    "start": "295505",
    "end": "301605"
  },
  {
    "text": "it really can't tell the difference\nbetween what is truth and what is satirical or made up or is a fun article.",
    "start": "301645",
    "end": "307575"
  },
  {
    "text": "And the internet is full of that. So if we take the rock example\nthat you had there, Brian, um, That actually came from a satirical\narticle in The Onion, but Google",
    "start": "307585",
    "end": "315825"
  },
  {
    "text": "couldn't differentiate between that. And I think that opens up a whole\nthing, as you were saying there.",
    "start": "315875",
    "end": "321994"
  },
  {
    "text": "So one of the things to be thinking\nabout there is one thing for The Onion to have a satirical article and you\nclick on that, you know it's a satirical.",
    "start": "321994",
    "end": "329564"
  },
  {
    "text": "article. But when Google takes that and then\nproduces an overview and puts it at the top and says this is the answer to your\nquestion, then is it Google speaking",
    "start": "329925",
    "end": "340105"
  },
  {
    "text": "at that point or is it really just\nproviding a summary of what you found? And that's where I think there\nis a real fundamental difference",
    "start": "340105",
    "end": "348805"
  },
  {
    "text": "on what's going on here. So thank you. This ability to, to really be able to\ndistinguish what the truth is and what",
    "start": "348855",
    "end": "355050"
  },
  {
    "text": "isn't the truth and what is really\njust a fun article, I think that's the challenge that they've got ahead of them. Now, if we look at something\nlike perplexity, they seem",
    "start": "355140",
    "end": "362879"
  },
  {
    "text": "to have solved that problem. So I have no doubt that Google will\nsolve that problem in time, but I think",
    "start": "362920",
    "end": "368289"
  },
  {
    "text": "this comes down to, uh, being able to\ndistinguish the difference of the results.",
    "start": "368290",
    "end": "373650"
  },
  {
    "text": "Skyler Speakman: I'm glad you brought\nup the, the rag analysis, because I wanted to just jump in there. I think there is a difference\nbetween referencing incorrect",
    "start": "374185",
    "end": "381185"
  },
  {
    "text": "information and a hallucination\nwhere the model is generating it. And I'm not quite yet sure for Google's\nAI overview, how much of it are incorrect",
    "start": "381185",
    "end": "390724"
  },
  {
    "text": "references from a rag system and how\nmuch of it is really, truly novel. Incorrect, but novel generated text\nand I don't know if we know the inner",
    "start": "390724",
    "end": "398854"
  },
  {
    "text": "workings of that quite yet But there\nis a difference between those two types of mistakes made in these AI overviews",
    "start": "398855",
    "end": "404715"
  },
  {
    "text": "Chris Hay: Yeah, I was gonna say I'm right\nand when you do rag anyway, depending on the creativity, you know You're\ngonna have a little bit of creativity",
    "start": "405575",
    "end": "412325"
  },
  {
    "text": "anyway in your settings So it's it's\nreally how much are they gonna crank that up or crank that down over time?",
    "start": "412334",
    "end": "417965"
  },
  {
    "text": "Bryan Casey: It's actually interesting\nYou mentioned That because there were examples, actually the example\nof like the children's toy that",
    "start": "418165",
    "end": "426385"
  },
  {
    "text": "was actually potentially a safety\nhazard and fatal of swallowed. The funny thing is, is like\nthere was a thread that went like",
    "start": "426414",
    "end": "432974"
  },
  {
    "text": "somewhat a little viral about that. And then the first post in the\ncomment section was actually somebody",
    "start": "432995",
    "end": "438184"
  },
  {
    "text": "referencing like the number one\nresult on Google and had almost that content verbatim, uh, in there.",
    "start": "438184",
    "end": "443235"
  },
  {
    "text": "And then, but what was interesting is when\nit was Google showing the result, Versus it just being a link on the internet.",
    "start": "443435",
    "end": "449555"
  },
  {
    "text": "The reaction to it was totally\ndifferent when it was Google was this like massive, crazy problem. When it was just the fact that this was\nthe first result on the internet, people",
    "start": "449895",
    "end": "456775"
  },
  {
    "text": "are like, Oh, well, it's just content. Uh, and that happens all the time. And people have to know,\num, to not trust that stuff.",
    "start": "456775",
    "end": "462745"
  },
  {
    "text": "And so people do seem like\nthey're approaching this with like different expectations\nthan they would normal content.",
    "start": "462775",
    "end": "468125"
  },
  {
    "text": "Kate Soule: I think people are assuming,\nlike, everyone is kind of cute to assume if they're reading this, like, stuff.",
    "start": "468595",
    "end": "473654"
  },
  {
    "text": "It's a statement that appears almost\nlike it's a fact and it's just, you know, saying this is what the facts\nare, that there's been some sort of",
    "start": "473655",
    "end": "480315"
  },
  {
    "text": "due diligence and like reasoning that's\ngone on to evaluate and to look through and, you know, that's not quite how\nthese systems work, at least not yet.",
    "start": "480425",
    "end": "489694"
  },
  {
    "text": "So, you know, I think there\nis a degree of skepticism. So. I think there is going to be a bit of\nskepticism that's going to be needed for the near term when, when looking\nat these types of results and working",
    "start": "489704",
    "end": "497120"
  },
  {
    "text": "through them, you know, making sure that\njust because as Skyler, you pointed out, right, just because, you know, it's on\nthe internet and it's being, um, shared,",
    "start": "497120",
    "end": "505199"
  },
  {
    "text": "doesn't mean it's a hallucination. It just means this is an example\nof what's on the internet. Bryan Casey: Actually, one\nquestion I wanted to follow",
    "start": "505209",
    "end": "510859"
  },
  {
    "text": "up on specifically on that. That touches on, I think some\nof the stuff that we were even talking about maybe on the show\nlast week, which is just around UX.",
    "start": "510860",
    "end": "518860"
  },
  {
    "text": "And so, one of the interesting things\nis that The place in the page that an",
    "start": "518909",
    "end": "523934"
  },
  {
    "text": "AI overview is taking up is a space that\nwas traditionally occupied by a thing called the featured snippet if you live\nin the search world and where Google",
    "start": "524155",
    "end": "531774"
  },
  {
    "text": "was sourcing that data historically was\njust one of the top two or three most authoritative and widely cited results\non the web and that would be taken",
    "start": "531775",
    "end": "539064"
  },
  {
    "text": "verbatim and placed in the snippet. Google's now putting their A. I.",
    "start": "539064",
    "end": "544188"
  },
  {
    "text": "Overviews in the exact same place on\nthe page where that content used to be. And you know, it struck me that maybe\none of the challenges there is that",
    "start": "544189",
    "end": "552270"
  },
  {
    "text": "people are not necessarily treating\nthe content that's having been sourced",
    "start": "552270",
    "end": "557319"
  },
  {
    "text": "totally different from one another. There's it's in the same place in the\nsame page, so they think it's the same.",
    "start": "557340",
    "end": "562279"
  },
  {
    "text": "And one of the things that started\nto make me think about is, you know, when we think about You know, and\nKate, maybe you could take this one.",
    "start": "562560",
    "end": "569614"
  },
  {
    "text": "We almost have these three\ndifferent types of things, which is like human generated content. LLM generated content and then\ntraditional answers from like a",
    "start": "569895",
    "end": "578970"
  },
  {
    "text": "calculator or a machine like that. You can do like almost\ntrust a hundred percent. And do you think that we actually need\nto do more in terms of distinguishing",
    "start": "578970",
    "end": "586940"
  },
  {
    "text": "the user experience between those things? Right. Rather than merging it all together\nand like deeply embedding LLMs and AI",
    "start": "586940",
    "end": "592410"
  },
  {
    "text": "into everything we do, like making it\nvery clear to users, you know, where they're seeing, you know, features and\ncontent that are sourced differently",
    "start": "592410",
    "end": "600710"
  },
  {
    "text": "than they have been historically. Kate Soule: Absolutely. And I think it goes beyond just\neven like consumer use cases.",
    "start": "600750",
    "end": "606654"
  },
  {
    "text": "It's super important for just regular\nconsumers doing Google searches. But especially when you look at enterprise\napplications and other things, you know,",
    "start": "606655",
    "end": "613673"
  },
  {
    "text": "the theme of like being able to cite your\nsources and being able to decompose a bit.",
    "start": "614055",
    "end": "619055"
  },
  {
    "text": "What is going on inside of the black\nbox, I think is increasingly going to be",
    "start": "619160",
    "end": "624190"
  },
  {
    "text": "critical for any sort of real adoption,\nbeing able to move beyond like, okay, this is a fun toy to, this is something\nthat I can actually use in the day to day.",
    "start": "624190",
    "end": "632930"
  },
  {
    "text": "So I really hope that we, uh, start to\nmake some progress there on, on some of these more consumer friendly, uh, chatbots\nbecause in the enterprise setting, you",
    "start": "632950",
    "end": "641100"
  },
  {
    "text": "know, that's becoming increasingly the\nnorm, like in rag patterns, you want to return, here's the source where I,\nyou know, um, got my answer from, and",
    "start": "641100",
    "end": "648079"
  },
  {
    "text": "that's becoming increasingly important. Chris Hay: One of the things that opens\nup in my mind, Kate, and I'd be interested",
    "start": "648079",
    "end": "653400"
  },
  {
    "text": "in your perspective there, is that that's\nkind of fine from a web interface, where",
    "start": "653400",
    "end": "658629"
  },
  {
    "text": "you're getting your result, you get\nyour overview, and then you've got all the links and here's where I reference.",
    "start": "658630",
    "end": "663830"
  },
  {
    "text": "But as we talked about in a previous\nepisode, where we're moving into multi modality, and you're going to\nbe chatting with a, uh, arguably a",
    "start": "664159",
    "end": "673750"
  },
  {
    "text": "human voice at that point, right? Okay. You're probably not going to want\nsomebody going back and say this",
    "start": "673760",
    "end": "680610"
  },
  {
    "text": "is the answer to the question. And by the way, I got this answer from\nhere, here, here, and you can visit it on XYZ, blah, blah, blah, because you're\ngoing to switch off at that point.",
    "start": "680610",
    "end": "688050"
  },
  {
    "text": "So I, I wonder how, what the best user\nexperience for voice for that sort of",
    "start": "688050",
    "end": "694420"
  },
  {
    "text": "helpful chatbot, but also being fair\nand transparent that it's AI generated. Kate Soule: I honestly question\nif chat, regardless if it's",
    "start": "694420",
    "end": "702720"
  },
  {
    "text": "with voice or text is the right. Domain here, like the right mechanism\nand mode for this type of analysis.",
    "start": "702720",
    "end": "708864"
  },
  {
    "text": "And one of the things I'm really excited\nby the AI overviews is it seems like one of the first use cases that is really\ntaking on that's consumer focused,",
    "start": "708885",
    "end": "716834"
  },
  {
    "text": "where it's not a chat bot, right? Where we're using generative AI and we're\nable to start to drive, um, information,",
    "start": "716835",
    "end": "723433"
  },
  {
    "text": "distillation, and gathering lots of\ndifferent sources and providing results. You know, without having to, like,\nhave a multi turn conversation, like",
    "start": "723434",
    "end": "732450"
  },
  {
    "text": "asking, are you sure about this answer? Where did you find it? Like, can you give me more sources? Like, that's a very unintuitive\nflow, but I think we've been so",
    "start": "732450",
    "end": "739619"
  },
  {
    "text": "trained on chat to equal generative\nAI up until now that that's just how we all assume it has to work.",
    "start": "739620",
    "end": "745149"
  },
  {
    "text": "So I would actually say I don't\nthink, you know, voice and other things are where this is going. Hopefully is going.",
    "start": "745740",
    "end": "751475"
  },
  {
    "text": "I think there's a lot of opportunity to\nthink through what do new types of non chat based applications look like and\nhow can we embed those decision making",
    "start": "751475",
    "end": "759694"
  },
  {
    "text": "criteria and sources and other things that\nare needed to to really drive drive value along the way without it being this like\nmulti turn interrogation of a of an agent.",
    "start": "759694",
    "end": "768724"
  },
  {
    "text": "Skyler Speakman: What, what do we think\nGoogle is collecting on the usage patterns of these, you know, way back in the day,\nthey would have search and they would",
    "start": "769125",
    "end": "776964"
  },
  {
    "text": "obviously collect click through, right? What are you clicking on? Any guesses as to what sort of\nmetrics Google's collecting as people",
    "start": "776964",
    "end": "784344"
  },
  {
    "text": "interact with these AI overviews? Um, I'm, that's not in my space at all. I'm just wondering if, if I'm, I'm\nguessing someone in there is, is",
    "start": "784344",
    "end": "792000"
  },
  {
    "text": "watching how we are interacting with\nthe AI overviews presented to us.",
    "start": "792010",
    "end": "796940"
  },
  {
    "text": "Bryan Casey: Ironically, this is the\none question I'm qualified to answer. Um, and so, you know, at least when\nGoogle first introduced, um, AI overviews",
    "start": "797940",
    "end": "809100"
  },
  {
    "text": "had been in beta for a while and they\nsaid they were bringing in primetime. And two of the things that\nthey talked about were that.",
    "start": "809100",
    "end": "814850"
  },
  {
    "text": "And they were really messaging\nto publishers, um, because like publishers have been hysterical\nabout the impact of this.",
    "start": "815300",
    "end": "820790"
  },
  {
    "text": "And like, what's been really\ninteresting is that the impact on organic traffic to publishers\nhas been like almost negligible.",
    "start": "820790",
    "end": "826930"
  },
  {
    "text": "Um, so everyone thought it was like the\nend of the internet and then like almost nothing happened in terms of traffic. Um, but two of the things\nthat Google said was one.",
    "start": "826979",
    "end": "833690"
  },
  {
    "text": "That the content that was surfaced through\nAI overviews was actually getting more click through and more traffic than\nthe stuff that was present in Just the",
    "start": "834225",
    "end": "842134"
  },
  {
    "text": "normal SERP and the idea there was that\nthose those links and was presented with more context I think Sundar did another\ninterview not long after that where He",
    "start": "842135",
    "end": "851720"
  },
  {
    "text": "was talking more about, like, generative\nUIs, and you can just see, I think, more about, like, when how you turn a query,\num, a user query, and you generate a UI",
    "start": "851720",
    "end": "861389"
  },
  {
    "text": "that places, like, links and information\nin context better than just, like, a flat list, which is sort of what they do.",
    "start": "861420",
    "end": "866639"
  },
  {
    "text": "They would say they do not do that today,\nbut it's like, there's still some of that. Um, and so that was one thing, and then\nthe other thing that they talked about,",
    "start": "867709",
    "end": "873930"
  },
  {
    "text": "I'm sure they measure more things, but\nthe other thing that they measured Um, is to the people who are exposed to\nAI overviews, start using search more.",
    "start": "873930",
    "end": "882300"
  },
  {
    "text": "Um, like, is this something that increases\ntheir usage of this product over time? Because the other audience that is\nterrified of this is obviously like",
    "start": "882510",
    "end": "889750"
  },
  {
    "text": "shareholders, um, and people want to know,\nit's like, Are you going to kill search? And in the process of doing that are,\nwhere's all the ad revenue going to go?",
    "start": "889750",
    "end": "897498"
  },
  {
    "text": "And so one of the other things that\nthey're very clear about is like, I don't know people who get exposed to this\nactually use this product more over time.",
    "start": "897619",
    "end": "903140"
  },
  {
    "text": "And so I think they're reminding some\nof their other stakeholders a little bit there, but those are at least some of\nthe ones that they've publicly discussed.",
    "start": "903169",
    "end": "909478"
  },
  {
    "start": "910000",
    "end": "1731000"
  },
  {
    "text": "Last week, Anthropic released a novel. Version of its cloud\nthree sonnet, uh, model.",
    "start": "915199",
    "end": "922045"
  },
  {
    "text": "And, um, this model did not believe\nthat it was a helpful AI assistant.",
    "start": "922485",
    "end": "927675"
  },
  {
    "text": "Instead, it believed it was the\ngolden gate bridge, uh, which is a fun thing to have happen. Um, but really that was a demo of\nresearch that Anthropic has been",
    "start": "927775",
    "end": "937274"
  },
  {
    "text": "doing for a long time, really. The industry has been pursuing\nfor a long time, which is in the space of interpretability.",
    "start": "937295",
    "end": "941985"
  },
  {
    "text": "Um, and within the space of\ninterpretability, Anthropic's been doing a lot of research around\nmechanistic, uh, interpretability.",
    "start": "942815",
    "end": "948384"
  },
  {
    "text": "Um, but part of the problem in this\nspace is that I think Kate, to the comment you made earlier, is that\nthese models are black box today.",
    "start": "948735",
    "end": "955685"
  },
  {
    "text": "You know, you put a pile of all the data\non the, in the internet and linear algebra",
    "start": "955725",
    "end": "960755"
  },
  {
    "text": "and outspits something that somehow,\nIt appears to know a lot, uh, about the world, but nobody knows how that's\nactually happening, like, not really.",
    "start": "960795",
    "end": "967970"
  },
  {
    "text": "And so interpretability, um,\nis a space that's trying to answer some of those questions. And what was interesting and why\nGolden Gate Claude was, Important was",
    "start": "967970",
    "end": "977755"
  },
  {
    "text": "that anthropic demonstrated that they\ncould identify the features within the model that activated when, um, you\nknow, either text or a picture of the",
    "start": "977755",
    "end": "987495"
  },
  {
    "text": "golden gate bridge, um, was presented. So they knew, um, kind of the combination\nof like neurons and circuits that would",
    "start": "987495",
    "end": "993834"
  },
  {
    "text": "say like this, this thing represents the\ngolden gate bridge and perhaps even more importantly, that by dialing that feature\nup or down, uh, they could influence the",
    "start": "993834",
    "end": "1003394"
  },
  {
    "text": "behavior of the model to the point where\nif you dialed it up high enough, Model thought it was the Golden Gate Bridge.",
    "start": "1003395",
    "end": "1008595"
  },
  {
    "text": "Um, and this was, if you read the\npaper, wasn't the only example either. And I'll share one other one, uh, which\nis that they had another feature that",
    "start": "1008625",
    "end": "1015415"
  },
  {
    "text": "would fire, uh, when it was looking at\ncode, and it would detect a security vulnerability, um, in, in the code.",
    "start": "1015415",
    "end": "1020725"
  },
  {
    "text": "And they had an example, too, where if\nyou dialed up that feature, Um, it would actually introduce a buffer overflow\nvulnerability into the code, um, as well.",
    "start": "1020934",
    "end": "1029084"
  },
  {
    "text": "So when you think about the ability to\ndial features up and down within a model fairly surgically, um, pretty important\nin terms of less steerability of the",
    "start": "1029095",
    "end": "1037135"
  },
  {
    "text": "model, um, potentially, and certainly,\nI think you can understand a little bit why folks in the AI safety community\nin particular have been focused on",
    "start": "1037135",
    "end": "1044441"
  },
  {
    "text": "this inter interpretability space. So I, I personally find the space super\nfascinating and Skyler, I just want to",
    "start": "1044441",
    "end": "1050995"
  },
  {
    "text": "turn it over to you, maybe kick us off\na little bit to Maybe even talk about like your general reactions to to the\npaper maybe and like the demo is a",
    "start": "1050995",
    "end": "1059080"
  },
  {
    "text": "starting point and just like what you\nfound interesting like how important you think it is and just You know, maybe\ntalk a little bit about how you know,",
    "start": "1059080",
    "end": "1066200"
  },
  {
    "text": "I don't know what you thought of it. Skyler Speakman: Yes. Great I'm, i'm happy to\ntalk about this space. Um without uh, Without, uh, droning on too\nlong, I have to describe what I do to my",
    "start": "1066810",
    "end": "1077388"
  },
  {
    "text": "kids, you know, ten year olds, seven year\nolds, and they know that I work with AI. Uh, and their understanding is\ntext goes in and text comes out.",
    "start": "1077389",
    "end": "1086070"
  },
  {
    "text": "That's, that's their kind of view\nof these large language models. And where I try to tell them where I and\nour team work on is actually in between.",
    "start": "1086120",
    "end": "1094128"
  },
  {
    "text": "What happens To the text when it\ngoes in, how does it get manipulated? And then it gets spit back out.",
    "start": "1094399",
    "end": "1100650"
  },
  {
    "text": "And I think this has been coming out as\nan area called representation engineering.",
    "start": "1100670",
    "end": "1105450"
  },
  {
    "text": "And I would call this paper, the\nGolden Gate example, a great example",
    "start": "1105740",
    "end": "1111069"
  },
  {
    "text": "of representation engineering. They're not manipulating prompts. They're not coming up with a new metric\nof how well their model is performing.",
    "start": "1111119",
    "end": "1118760"
  },
  {
    "text": "They are messing with the\nrepresentation of the model. And I think that's just a really cool.",
    "start": "1118950",
    "end": "1125130"
  },
  {
    "text": "I would say emerging or perhaps even\nunderrepresented area of research when you compare it to prompt engineering, for\nexample, what, how can we, you know, probe",
    "start": "1125550",
    "end": "1135130"
  },
  {
    "text": "the model or sorry, how can we prompt\nthe model in such a right way to make it be convinced it's a golden gate bridge?",
    "start": "1135130",
    "end": "1141158"
  },
  {
    "text": "That would be a very different\napproach to what they had done, um, with this, uh, golden gate example.",
    "start": "1141380",
    "end": "1146360"
  },
  {
    "text": "Um, it's a fun example. They took it down. I think it was only available\nfor people to use for about 24 hours.",
    "start": "1146835",
    "end": "1152725"
  },
  {
    "text": "Bryan Casey: Yep. And Skyler Speakman: so it's, it's\nalready been with us and, you know, taken away too soon.",
    "start": "1152815",
    "end": "1157994"
  },
  {
    "text": "But I think for me, the, the, what I\nwould like to get around to the larger audience is they did not just create a\nnew large language model by training it.",
    "start": "1157994",
    "end": "1165875"
  },
  {
    "text": "Only on Golden Gate Bridge data, they\ndid not insert a little prompt that",
    "start": "1166115",
    "end": "1171675"
  },
  {
    "text": "says every time you answer a question,\npretend you're the Golden Gate Bridge. They really did identify the inner\nworkings of these models and then",
    "start": "1171675",
    "end": "1179775"
  },
  {
    "text": "crank it up as Brian had described. And I think what I'm excited about that is\nin this representation engineering space.",
    "start": "1180045",
    "end": "1189534"
  },
  {
    "text": "It doesn't take the latest, greatest\ntechnologies to find these cool insights,",
    "start": "1189845",
    "end": "1196325"
  },
  {
    "text": "things like principal component analysis,\nuh, things like a sparse auto encoder. These things were, you know, decades\nold or a 10 year old analysis,",
    "start": "1196335",
    "end": "1204845"
  },
  {
    "text": "but applied to the inner workings\nof these large language models. Is now this new rich space of\nrepresentation engineering.",
    "start": "1204874",
    "end": "1212480"
  },
  {
    "text": "So I liked the paper both for\nhow it presented its work. Uh, Chris Ola, one of the authors\nis a visualization genius and, and",
    "start": "1212480",
    "end": "1219660"
  },
  {
    "text": "in their, in their publication,\nthey've got some really, really cool visualizations of what they found out. Um, so I think that's\nprobably my first takeaway.",
    "start": "1219660",
    "end": "1226819"
  },
  {
    "text": "I'd like to spread to an, uh, a broader\naudience that large language models are not just text in and text out.",
    "start": "1226819",
    "end": "1233060"
  },
  {
    "text": "There's a lot of rich space. Uh, science to be done in that\nrepresentation space and the Golden Gate",
    "start": "1233360",
    "end": "1239760"
  },
  {
    "text": "Bridge paper is a great example of that. Bryan Casey: Can you maybe talk a little\nbit about, um, the safety community?",
    "start": "1239760",
    "end": "1247608"
  },
  {
    "text": "I think in particular is very interested\nin the topic of interpretability, um, and",
    "start": "1247609",
    "end": "1252989"
  },
  {
    "text": "I think has feel some level of urgency,\nuh, around it, given how capable and how quickly capable some of the models\nare, are becoming, but maybe just can",
    "start": "1253470",
    "end": "1262389"
  },
  {
    "text": "you talk a little bit about why Why it's\nso important to the safety community. And then maybe also talk about like\nother applications and area and",
    "start": "1262390",
    "end": "1271555"
  },
  {
    "text": "domain areas where this space of,\nyou know, interpretability, um, You",
    "start": "1271555",
    "end": "1276715"
  },
  {
    "text": "know promises to you know, it could\nbe on the capability side of it But just other places where we think\ninterpretability will make a difference",
    "start": "1276715",
    "end": "1282955"
  },
  {
    "text": "Skyler Speakman: Right. I think a real clear example. I was reading of a blog after a golden\ngate cloud has been brought down uh,",
    "start": "1284245",
    "end": "1290745"
  },
  {
    "text": "some people noted that When golden\ngate feature was highly activated, when",
    "start": "1291115",
    "end": "1296290"
  },
  {
    "text": "Claude three was turned into golden\ngate, Claude, um, he would respond to",
    "start": "1296290",
    "end": "1301790"
  },
  {
    "text": "tasks that he was previously would not. So please, can you write a scam email?",
    "start": "1301790",
    "end": "1306789"
  },
  {
    "text": "Normal Claude would respond. Sorry, I can't do that. Golden gate Claude would proceed and\nit would generate this scam email.",
    "start": "1307349",
    "end": "1315790"
  },
  {
    "text": "Nothing to do with that\ngolden gate analogy. But it was an example of when you\nmess with these other features like",
    "start": "1316170",
    "end": "1322764"
  },
  {
    "text": "that, there are other sort of perhaps\npreviously thought built in guardrails",
    "start": "1322764",
    "end": "1327324"
  },
  {
    "text": "that are no longer as strong. And so I think that's going to be\nanother really interesting area of work of you may have well intentioned\npeople manipulating these features.",
    "start": "1327924",
    "end": "1338014"
  },
  {
    "text": "We don't know what other guardrails\nthat previously worked will not work.",
    "start": "1338534",
    "end": "1343715"
  },
  {
    "text": "After you've manipulated a feature because\nwho would have thought that amplifying the golden gate Idea the bridge would\nmake the large language model clod more",
    "start": "1343935",
    "end": "1352454"
  },
  {
    "text": "likely to comply to a uh to an illicit\ntask so I think Um, that was just I don't",
    "start": "1352454",
    "end": "1358544"
  },
  {
    "text": "know an example that I had read about\nthere that I think the safety community they don't might not care about the\nUh, a large language model identifying",
    "start": "1358545",
    "end": "1365065"
  },
  {
    "text": "as the Golden Gate Bridge, but they\nwill definitely be interested about the jailbreaking behavior of what happens\nwhen people start manipulating it.",
    "start": "1365065",
    "end": "1371924"
  },
  {
    "text": "Kate Soule: Skyler, I got a question\nfor you based off of that, like what implications does that have\nthen for open sourcing models",
    "start": "1372174",
    "end": "1379775"
  },
  {
    "text": "and releasing models and weights? You know, a lot of times model providers\ndo a lot of safety reinforcement learning",
    "start": "1379824",
    "end": "1385894"
  },
  {
    "text": "and other protections on top of the\nmodels that are before they're released to help manage some of those behaviors,\nlike, could you see some of that now?",
    "start": "1385905",
    "end": "1393655"
  },
  {
    "text": "Being at risk and eroding Skyler Speakman: the the willingness to\nopen source you is that what you mean by?",
    "start": "1394120",
    "end": "1401090"
  },
  {
    "text": "Being at risk the willingness\nfor companies to open up Kate Soule: Yeah Take take it as\nyou a willingness for companies",
    "start": "1401830",
    "end": "1407989"
  },
  {
    "text": "to open source the risk that is\nintroduced from releasing model weights that now can be Shall we say?",
    "start": "1407999",
    "end": "1414400"
  },
  {
    "text": "exploited in ways that weren't\noriginally anticipated by the model designer and builder",
    "start": "1415030",
    "end": "1419380"
  },
  {
    "text": "Skyler Speakman: Um, really good question. Um, actually Anthropic themselves,\nthey have this much larger blog.",
    "start": "1420265",
    "end": "1426675"
  },
  {
    "text": "You can read where they defend why they\nhave not open sourced these types of their, of their models in that regard.",
    "start": "1426685",
    "end": "1432465"
  },
  {
    "text": "Um, I think, I imagine people around,\nuh, the AI community right now, probably over the weekend are busy running\ntheir own version of golden gate.",
    "start": "1432845",
    "end": "1442015"
  },
  {
    "text": "They're going to find their own features. They're going to start manipulating those. Um, so, I think we'll probably see\nsome of those results showing up,",
    "start": "1442025",
    "end": "1449715"
  },
  {
    "text": "hopefully on archive, um, or maybe\non blog posts, uh, within this week Chris Hay: on that Skyler.",
    "start": "1449715",
    "end": "1455554"
  },
  {
    "text": "So I did a YouTube video about three,\nfour months ago where I took the Gemma",
    "start": "1455575",
    "end": "1461644"
  },
  {
    "text": "model and I took the Mistral model. So it's not at the feature\nlevel that they did. And what I did is I lopped off\nthe input embeddings layer, right?",
    "start": "1461645",
    "end": "1469625"
  },
  {
    "text": "So I left the model only having. The input embeddings layer, nothing else.",
    "start": "1469625",
    "end": "1475029"
  },
  {
    "text": "And then what I did is I ran a cosine\nsimilarity search against the various, uh, tokenize, the various tokens within\nthe input embeddings layer, and then just",
    "start": "1475130",
    "end": "1484829"
  },
  {
    "text": "looked at, did a visualization, looked at\nwhat embeddings were close to each other. And when I did that, it was a.",
    "start": "1484940",
    "end": "1491704"
  },
  {
    "text": "Incredible. And you can go check out that\nYouTube video, but it was incredible. So you would see that just in the\ninput embeddings layer, nowhere else,",
    "start": "1492495",
    "end": "1501655"
  },
  {
    "text": "you would see that misspelling of\nwords were super close to each other.",
    "start": "1501835",
    "end": "1506765"
  },
  {
    "text": "So if I had London with a capital\nL and London with a small l or London with a space after, they\nwould all cluster together.",
    "start": "1506955",
    "end": "1513903"
  },
  {
    "text": "But not just that, cities themselves\nwould cluster together, so you would see London, you would see Moscow, you would\nsee Paris, and in fact, you would see",
    "start": "1514230",
    "end": "1523280"
  },
  {
    "text": "almost a distance similarity in, in the\nvisualization, which was fascinating.",
    "start": "1523370",
    "end": "1528949"
  },
  {
    "text": "You saw the same thing with celebrities,\nthey would cluster together, computer programming terms, right?",
    "start": "1528960",
    "end": "1535079"
  },
  {
    "text": "So, you know, the various for loops,\na for loop, a while loop, etc. So for a while would all come together.",
    "start": "1535079",
    "end": "1540609"
  },
  {
    "text": "Now the reason that I ran that\nagainst the Mistral model and the Gemma model is the Gemma model 000\ntokens, whereas the Mistral model",
    "start": "1541000",
    "end": "1552690"
  },
  {
    "text": "has got 32, 000 tokens, right? So there's a lot of splitting\nof tokens in Mistral.",
    "start": "1552800",
    "end": "1558669"
  },
  {
    "text": "But in the demo model, there's\nnot a lot of splitting, right? So it means that you got a\nmuch closer on the similarity.",
    "start": "1559295",
    "end": "1566655"
  },
  {
    "text": "So when I did that, I was\nabsolutely blown away. And like the, the anthropic team,\nI wanted to go to the next layer.",
    "start": "1566865",
    "end": "1575464"
  },
  {
    "text": "Cause I had the same theory that if I\njumped down the next layers, you would start to see these features activate\nbecause I could see it already just",
    "start": "1575485",
    "end": "1584565"
  },
  {
    "text": "did the embeddings layer and, and one\nof the theories, and I, and I'm glad to Think of being proven right is the.",
    "start": "1584565",
    "end": "1590690"
  },
  {
    "text": "You may have noticed that as new models\nare coming out, everybody is opening, is increasing their tokenizer vocabulary.",
    "start": "1592335",
    "end": "1600735"
  },
  {
    "text": "Every single part, everybody's\nincreasing their input embeddings layer. And the reason is, I believe, is\nit's easier for the models to be",
    "start": "1600804",
    "end": "1610430"
  },
  {
    "text": "able to generalize more as it goes\nup the layers, if you get that pretty close on the input embeddings layer.",
    "start": "1610430",
    "end": "1616970"
  },
  {
    "text": "And, and I think therefore, when I looked\nat the anthropic player paper, bringing",
    "start": "1617240",
    "end": "1622459"
  },
  {
    "text": "it back there, I could visualize when\nit talked about cities, when it talked about locations, when it talked about\ncomputer programming terms, I was like,",
    "start": "1622459",
    "end": "1630370"
  },
  {
    "text": "I could see that just in the input\nembeddings layer only on my visualization. So, I can absolutely see how that would\nthen translate into features as the",
    "start": "1630625",
    "end": "1640455"
  },
  {
    "text": "models get stacked up and it becomes\nricher and richer with semantic meaning. Skyler Speakman: Yes, I'm gonna\ngeek out here a little bit.",
    "start": "1640465",
    "end": "1646215"
  },
  {
    "text": "The, the official papers of the Clod\nGolden Gate work, um, are all plays on the",
    "start": "1646234",
    "end": "1653070"
  },
  {
    "text": "word monosemanticity, which is basically\na really, a really big word that is",
    "start": "1653070",
    "end": "1658419"
  },
  {
    "text": "getting the idea at can we find a single\npart of these huge large language models",
    "start": "1658440",
    "end": "1664139"
  },
  {
    "text": "that have One meaning, and they were\nable to do that for the golden gate idea.",
    "start": "1664340",
    "end": "1669735"
  },
  {
    "text": "And then the idea was now what happens\nif we take that one part of this huge large language model and crank it up\ntenfold, and then you get the idea",
    "start": "1670074",
    "end": "1677434"
  },
  {
    "text": "of, of Claude, a large language model. But, uh, Chris, your description of\nhow these types of, uh, words or tokens",
    "start": "1677435",
    "end": "1684784"
  },
  {
    "text": "are, are coming together like that. Um, uh, the tech behind. Uh, Claude's golden gate basically, okay.",
    "start": "1685094",
    "end": "1692720"
  },
  {
    "text": "Weaponized is a bit dramatic, but it\nreally emphasized, can we take this",
    "start": "1692720",
    "end": "1698010"
  },
  {
    "text": "richer embedding space and, uh, you know,\ncreate, uh, a million features from it.",
    "start": "1698040",
    "end": "1703639"
  },
  {
    "text": "And then once they had those features\nthat you get the ones like the golden gate and your security concerns.",
    "start": "1703700",
    "end": "1709149"
  },
  {
    "text": "And I think there was one on\ntourist attractions, et cetera. Uh, but it's getting at this idea\nof, can we find a Monto semantic",
    "start": "1709170",
    "end": "1716450"
  },
  {
    "text": "Part of these large language models. Um, so yeah, uh, again,\nexciting space to be again.",
    "start": "1716645",
    "end": "1722294"
  },
  {
    "text": "I know I'll come back. I love it when the research gets,\num, gets into these inner workings of",
    "start": "1722304",
    "end": "1727624"
  },
  {
    "text": "large layer, uh, large language models. I think that's fascinating.",
    "start": "1727624",
    "end": "1730393"
  },
  {
    "start": "1731000",
    "end": "2671000"
  },
  {
    "text": "Bryan Casey: So also last week,\nlast week was another big week of",
    "start": "1736244",
    "end": "1741374"
  },
  {
    "text": "announcements across the industry. Um, but I actually just want to use\nMicrosoft, I mentioned, introduced what",
    "start": "1741544",
    "end": "1748575"
  },
  {
    "text": "has become known as the whale computer,\num, on the internet because they used this analogy of marine life to basically\nexplain the orders of magnitude size of",
    "start": "1748575",
    "end": "1757864"
  },
  {
    "text": "the infrastructure building, the sport AI\nworkloads, and they used these three steps",
    "start": "1757884",
    "end": "1763215"
  },
  {
    "text": "of shark, orca, uh, And then a whale.",
    "start": "1763215",
    "end": "1767924"
  },
  {
    "text": "And what's funny is just, if you\nlook at like this morning, I was like Googling how much does a shark weigh?",
    "start": "1768235",
    "end": "1773664"
  },
  {
    "text": "Um, and so sharks are roughly,\nI think like 800 pounds. And then an orca is 8, 000 pounds.",
    "start": "1773725",
    "end": "1780565"
  },
  {
    "text": "And then a whale is like 80, 000 pounds. And so it's just an order of magnitude. Um, and they were thinking about\nlike, okay, what's the interesting and",
    "start": "1780585",
    "end": "1786995"
  },
  {
    "text": "fun way to visualize and communicate\nan order of magnitude and maybe a little bit memable, um, in a way.",
    "start": "1786995",
    "end": "1793033"
  },
  {
    "text": "And so they certainly achieved that. Um, but. In some ways, it's just like\nclassic scaling loss, uh, right?",
    "start": "1793034",
    "end": "1799340"
  },
  {
    "text": "It goes back to the original 2020\npaper that says, you know, if you're trying to improve the capability of\nthese models, reduce the overall loss,",
    "start": "1799340",
    "end": "1808610"
  },
  {
    "text": "um, in them that you want to improve,\nincrease your compute, your data, your",
    "start": "1808870",
    "end": "1815140"
  },
  {
    "text": "parameter count by roughly similar\norders of magnitude and from one generation of the model to the next.",
    "start": "1815140",
    "end": "1821128"
  },
  {
    "text": "And that improves the overall\nsort of general capability. Of, of the thing, and that's, you can\nlook at NVIDIA earnings, like that's",
    "start": "1821129",
    "end": "1827355"
  },
  {
    "text": "held pretty true, um, up to, up to this\npoint, um, but maybe where I wanted to",
    "start": "1827385",
    "end": "1832585"
  },
  {
    "text": "jump in is Kay, a comment you made, I\nthink it was last week on the show where you're saying like something to the\neffect of saying enterprises may not,",
    "start": "1832585",
    "end": "1841565"
  },
  {
    "text": "for a lot of these use cases may not\nneed an artificial general intelligence. They actually may not need all the\ncapability that exists right now.",
    "start": "1841574",
    "end": "1850135"
  },
  {
    "text": "Um, and so. You know, I, I, I think it'd be great\nif maybe you could talk a little bit",
    "start": "1850175",
    "end": "1856035"
  },
  {
    "text": "about, you know, maybe a little bit\nabout the scaling laws, but a different perspective of like that, the scaling\nlaws idea to me is really from the",
    "start": "1856065",
    "end": "1865615"
  },
  {
    "text": "perspective of if you're a model provider\ntrying to build AGI, it's not if you're an enterprise trying to get ROI, uh,\nessentially, and yeah, can you talk",
    "start": "1865615",
    "end": "1875865"
  },
  {
    "text": "maybe a little bit about just some of\nthe, what you see in terms of like the cost and size trade offs and, you know,\nthose bigger, better all the time.",
    "start": "1875865",
    "end": "1883655"
  },
  {
    "text": "Kate Soule: I mean, I think with the\nscaling laws, as you say, do a good job at is for model providers, like\npeople actually training these large",
    "start": "1884335",
    "end": "1892704"
  },
  {
    "text": "models and what was really kind of one\nof the big breakthroughs is look, you can't just increase your model size.",
    "start": "1892745",
    "end": "1898205"
  },
  {
    "text": "The most efficient way to improve\nperformance is to also increase the amount of data that's used as well.",
    "start": "1898205",
    "end": "1902805"
  },
  {
    "text": "Um, And just because you now know\nthe, maybe let's call it the most",
    "start": "1903265",
    "end": "1909340"
  },
  {
    "text": "cost effective way to train a model\nof, you know, the nth degree in size,",
    "start": "1909340",
    "end": "1914260"
  },
  {
    "text": "does that mean it's economically\nincentivized to train that model? Will the actual benefits that you\ndrive from that model justify the cost?",
    "start": "1914580",
    "end": "1922429"
  },
  {
    "text": "That's an entirely different question. That's, that's a different question. The scaling laws don't answer. So I think to this point, there's been\nenough excitement and clear use cases",
    "start": "1922439",
    "end": "1931150"
  },
  {
    "text": "and value where there's been a clear\neconomic driver to support, okay, we need to train some bigger, bigger and\nbigger models, and that's gotten to us",
    "start": "1931150",
    "end": "1939319"
  },
  {
    "text": "where we are today, but you know, I. I do question some of the statements\nand claims out there about, you",
    "start": "1939319",
    "end": "1946760"
  },
  {
    "text": "know, how we're always going to be,\nyou know, we have to keep investing and build bigger and bigger models.",
    "start": "1946760",
    "end": "1951910"
  },
  {
    "text": "I'm sure there's, uh, there's\nalways, you know, I'm going to put like the science of it aside of\nexploring and determining what's next.",
    "start": "1952439",
    "end": "1958699"
  },
  {
    "text": "But if we look at. What's actually economically incentivized. I think we're going to start to\nsee a performance plateau and",
    "start": "1958699",
    "end": "1967635"
  },
  {
    "text": "we look at what the real use\ncases are and the value drivers. I don't think we're going to need\nmodels that are a hundred times bigger",
    "start": "1967645",
    "end": "1974404"
  },
  {
    "text": "than what we have today to extract\nmost of the value from generative AI. Um, and a lot of the low hanging fruit.",
    "start": "1974404",
    "end": "1980345"
  },
  {
    "text": "So, you know, I think that's, it's\nstill a huge area of exploration. If you kind of look at even scaling\nlaws themselves, keep changing, you",
    "start": "1981525",
    "end": "1988735"
  },
  {
    "text": "know, it's still this concept of you\nneed more data for bigger models, but I'm hopeful that we're going to\nstart to see more work built in on.",
    "start": "1988735",
    "end": "1996164"
  },
  {
    "text": "You know, what will be economically\nincentivized to build, um, as well as looking at other costs that aren't\nreflected in these scaling laws.",
    "start": "1996565",
    "end": "2003945"
  },
  {
    "text": "Costs like data. You mentioned, you know, concerns\nabout pre training data disappearing. So if we know we need more data to train\nbigger models, you know, at some point",
    "start": "2003965",
    "end": "2010674"
  },
  {
    "text": "we're gonna run out of quote real data. Um, and so that's a whole different\nfrontier of looking at data costs,",
    "start": "2010675",
    "end": "2017164"
  },
  {
    "text": "looking at what role synthetic\ndata could play and that all of that really needs to be explored. Um, there's also costs on like Climate\nand, you know, the actual compute costs.",
    "start": "2017165",
    "end": "2026664"
  },
  {
    "text": "And, uh, you know, are those costs going\nto start to be better realized in the costs that are charged to model providers\nand people leveraging these models?",
    "start": "2026665",
    "end": "2035824"
  },
  {
    "text": "Um, and, you know, I think all of that\nwill maybe start to change the narrative a little bit of where the future is\ngoing as we continue to learn more.",
    "start": "2036284",
    "end": "2044143"
  },
  {
    "text": "Bryan Casey: Maybe one follow up to that\nis I remember the reaction in the market when the llama three models came out, uh,\n8 billion parameter model in particular,",
    "start": "2044335",
    "end": "2055134"
  },
  {
    "text": "which I believe is trained on 70 times,\n75 times as much data as you would if",
    "start": "2055134",
    "end": "2060584"
  },
  {
    "text": "you were just trying to do an optimally\ncompute efficient model, which obviously",
    "start": "2060604",
    "end": "2065605"
  },
  {
    "text": "is not the approach that they took. They instead took an approach\nof trying to build something. Small and capable that you could run on\nyour laptop that was cheap for imprints,",
    "start": "2065835",
    "end": "2074425"
  },
  {
    "text": "but still had a ton of capability. Do you, do you see like\nmore of that happening? Kate Soule: Definitely.",
    "start": "2074445",
    "end": "2080024"
  },
  {
    "text": "So right now, again, the scale, the\nmain scaling laws that everyone's using are for model providers,\nnot thinking about necessarily.",
    "start": "2080035",
    "end": "2088184"
  },
  {
    "text": "So this is another cost that isn't\nyet really reflected the model life cycle and the full usage.",
    "start": "2088445",
    "end": "2093544"
  },
  {
    "text": "So, like, think about your fixed costs\nof how much does it take to create that model once versus the marginal cost to use\nit every single time you run inference.",
    "start": "2093545",
    "end": "2100585"
  },
  {
    "text": "So you're incentivized to build smaller\nmodels if you're gonna have a long model life cycle and you're gonna\nhit that model millions and billions",
    "start": "2101220",
    "end": "2107829"
  },
  {
    "text": "of times and run inference on it. Um, you want to get that, you know,\nmarginal cost as low as possible.",
    "start": "2107830",
    "end": "2113710"
  },
  {
    "text": "Uh, and that's where the llamas are going. That's where, you know, like if you\nlook at the, Phi model series as well.",
    "start": "2113769",
    "end": "2121060"
  },
  {
    "text": "You know, they're training on these\nincredibly data dense ratios of amount of data per token, where like chinchilla, I\nthink calls for like 20 to one, 20 tokens",
    "start": "2121060",
    "end": "2130420"
  },
  {
    "text": "per parameter or something like that. They're now in the hundreds and\nthousands of tokens per parameter. So I think we're still really\nalso understanding that trade off.",
    "start": "2130420",
    "end": "2138740"
  },
  {
    "text": "Uh, and I think we'll continue\nto, that's where the, everyone is headed understanding that there's.",
    "start": "2138830",
    "end": "2143630"
  },
  {
    "text": "Maybe it hasn't been articulated\nfully in a scaling law, but trying to optimize that, that total life cycle\nof when this gets deployed, we need",
    "start": "2144385",
    "end": "2150885"
  },
  {
    "text": "to be able to run it all as model as\npossible for this to be cost effective. Chris Hay: And I think, Kate, to that\npoint, I think one of the questions you",
    "start": "2150895",
    "end": "2159700"
  },
  {
    "text": "need to ask in general is, do, how much\nreasoning do you need from your model?",
    "start": "2159700",
    "end": "2165189"
  },
  {
    "text": "So, if I, and I like to use the kind\nof, the cooking analogy, so if I",
    "start": "2165500",
    "end": "2170839"
  },
  {
    "text": "go to a Gordon Ramsay restaurant,\nand I'm not expecting Gordon Ramsay",
    "start": "2170840",
    "end": "2176090"
  },
  {
    "text": "to cook my meal for me, right? And I'm not expecting him to invent\na brand new meal there and then.",
    "start": "2176120",
    "end": "2181990"
  },
  {
    "text": "I'm, what I'm wanting is a recipe\nthat he's invented at some point and then there's going to be some\nlike sous chef or something that's",
    "start": "2182230",
    "end": "2188710"
  },
  {
    "text": "going to cook up that meal and I'm\ngoing to serve it and I'm going to have the Gordon Ramsay experience.",
    "start": "2188710",
    "end": "2193130"
  },
  {
    "text": "And I think When you're looking at the\nlarger models, you know, with hundreds of billions of parameters, even 70 billion\nparameters type models, you're, you're",
    "start": "2193750",
    "end": "2202530"
  },
  {
    "text": "asking for the Gordon Ramsay there. You're asking for, I want you to come\nup with the recipe, I want you to",
    "start": "2202530",
    "end": "2208650"
  },
  {
    "text": "invent the recipe, cook the recipe,\nand serve me the meal at the same time. But actually using the bigger model to\ndo the reasoning, right, figure out what",
    "start": "2208650",
    "end": "2217279"
  },
  {
    "text": "the recipe is, what the good answer is,\nand then passing the pattern onto the",
    "start": "2217279",
    "end": "2222339"
  },
  {
    "text": "smaller model, to go and do the sous chef\nthing and I think that's really the big",
    "start": "2222339",
    "end": "2227805"
  },
  {
    "text": "question for people when they're sort of\ndoing POCs and they scale to production.",
    "start": "2227825",
    "end": "2232805"
  },
  {
    "text": "They use the bigger models to begin with\nbecause they're trying to figure out what the answer is but then in production they\nneed to get this Beautifully said, right?",
    "start": "2233165",
    "end": "2241555"
  },
  {
    "text": "You need to keep the cost low, so they\nthen switch to the smaller model, because",
    "start": "2241725",
    "end": "2246755"
  },
  {
    "text": "they want the increased latency, they\nwant the, uh, sorry, decreased latency, they want the lower cost, but the\npattern has been figured out, and you",
    "start": "2246755",
    "end": "2254914"
  },
  {
    "text": "just want that smaller model to rinse\nand repeat, which it's really good at. Kate Soule: Absolutely. And I think another area, so there's\nthis concept of, like, use bigger",
    "start": "2254915",
    "end": "2262265"
  },
  {
    "text": "models to teach small models, and\nthat also throws in some squirrelly math with scaling laws if you need a\nbig model to get a good small model.",
    "start": "2262265",
    "end": "2268835"
  },
  {
    "text": "But, you know, moving past that,\nthere's also, I think, a real opportunity of, like, model routing\nand figuring out, um, What tasks do",
    "start": "2268854",
    "end": "2277175"
  },
  {
    "text": "you actually need the big model for? Like, when do you need, you know,\nit's Gordon Ramsay to, to tap in,",
    "start": "2277175",
    "end": "2282285"
  },
  {
    "text": "versus when can you pass this off\nand maybe you just need to go to McDonald's for a quick bite to eat. Like, this is something really easy,\nlow value, not worth spending, you know,",
    "start": "2282675",
    "end": "2291985"
  },
  {
    "text": "an insane amount to, to accomplish. And, and that's again, where I\nthink a lot of the, what will be",
    "start": "2292305",
    "end": "2297310"
  },
  {
    "text": "economically incentivized comes in\nis figuring out like, how much are these tasks actually worth to you?",
    "start": "2297350",
    "end": "2303119"
  },
  {
    "text": "And if you can get away with a\nreasonable performance with a 10 million parameter model or, or 3 billion\nparameter model, you know, it's not",
    "start": "2303160",
    "end": "2310750"
  },
  {
    "text": "going to be, no one's going to pay to\nsend it to a hundred, you know, multi hundred billion, trillion billion,\na trillion parameter model instead.",
    "start": "2310750",
    "end": "2317630"
  },
  {
    "text": "Bryan Casey: Maybe one final\nquestion on, on this topic. And, um, it was funny. There was an interview where.",
    "start": "2318030",
    "end": "2324059"
  },
  {
    "text": "People are talking to Jensen and they\nwere asking him his, uh, his opinion on. How much he thought this would hold and\nthey were poking on things that were about",
    "start": "2325275",
    "end": "2334145"
  },
  {
    "text": "really the tam of nvidia like sort of long\nterm and paused and because he was like I should not answer this question because\nYou know, like any anything he says is",
    "start": "2334145",
    "end": "2343385"
  },
  {
    "text": "like the stock price is just gonna go all\nover the place, uh, essentially Um, but you know, he started to talk about The\nopportunity being the entire, like one, I",
    "start": "2343385",
    "end": "2353750"
  },
  {
    "text": "think it was a trillion dollar data center\nmarket, um, is what he was talking about. And there's been a lot of discussion\nabout whether like all workloads will",
    "start": "2353760",
    "end": "2362280"
  },
  {
    "text": "become accelerated workloads, um, going,\ngoing forward and just in, for every",
    "start": "2362280",
    "end": "2367660"
  },
  {
    "text": "application, for every company, just\nthe, the blend of stuff that they're doing on traditional CPU versus more\naccelerated workloads and how they, how",
    "start": "2367660",
    "end": "2377230"
  },
  {
    "text": "Hand off between those two things and,\nyou know, I'm just curious, maybe even Chris from, from your perspective and\njust a lot of client conversations that",
    "start": "2377460",
    "end": "2384910"
  },
  {
    "text": "in scenarios that you're working with, you\nknow, how people are thinking about that. Like I know, I know a bunch of\ninference is still done on CPUs today,",
    "start": "2385410",
    "end": "2393049"
  },
  {
    "text": "but I think for some of the latency,\nreally low latency examples, people are talking about like, Oh, do we\nneed to put more of this on GPUs?",
    "start": "2393049",
    "end": "2399050"
  },
  {
    "text": "So, uh, I'm just, I'm curious how,\nfrom an application perspective inside of an enterprise account,\nhow people are thinking about.",
    "start": "2399060",
    "end": "2406250"
  },
  {
    "text": "Just influence inference and like\napplication architectures and how they're doing trade offs between\nkind of CPU and GPU computing.",
    "start": "2407380",
    "end": "2413480"
  },
  {
    "text": "Chris Hay: Yeah, I think it's a\nreally interesting, uh, area, so a lot of customers and are actually\nthinking about this all the time.",
    "start": "2414810",
    "end": "2422830"
  },
  {
    "text": "So it's an architectural consideration. It's just like any other NFR. Am I gonna go SaaS here?",
    "start": "2423180",
    "end": "2428939"
  },
  {
    "text": "Am I gonna go on premise? You know, how do I pay my cost? What am I gonna do? What's the safety on that?",
    "start": "2428940",
    "end": "2434720"
  },
  {
    "text": "If I'm honest, I don't know. Most enterprises are being\npretty cautious, right? It's, they want to do a classification\ntask, they want to do a summarization.",
    "start": "2435040",
    "end": "2444110"
  },
  {
    "text": "They don't want the model to\nmake up some classification. They know what their list\nof 30 classifications are.",
    "start": "2444420",
    "end": "2449700"
  },
  {
    "text": "Go do that. They know what their examples\nof summarizations are. Go do that. So they don't really, they want to take\nthat low hanging fruit, and they're",
    "start": "2449960",
    "end": "2458599"
  },
  {
    "text": "approaching it quite cautiously. I think we're that. probably changes in time, and again,\nit's more of a discussion for a future",
    "start": "2458610",
    "end": "2467475"
  },
  {
    "text": "episode, I think, is when we move\ninto a gen tech workflows, right? How do I then start to organize my\ninformation within my enterprise?",
    "start": "2467475",
    "end": "2476294"
  },
  {
    "text": "So the AI will have access to the\nright knowledge bases, which tools will it have access to, which is a\nmuch wider architectural discussion.",
    "start": "2476535",
    "end": "2484975"
  },
  {
    "text": "So a lot of clients are starting to think\nabout how gen AI fits into their overall",
    "start": "2484975",
    "end": "2490880"
  },
  {
    "text": "enterprise architecture and how you need\nto evolve your traditional architecture for the AI to be able to use that.",
    "start": "2490880",
    "end": "2496840"
  },
  {
    "text": "And again, but that's, it's\nquite a, it's quite a slow path. Um, but generally I, I don't think\nthings have moved on too much from",
    "start": "2496850",
    "end": "2505170"
  },
  {
    "text": "classification, summarization, et cetera. And then of course, you know, code\ngeneration is a big productivity",
    "start": "2505170",
    "end": "2511090"
  },
  {
    "text": "lever that everybody's kind\nof leaning into just now. Kate Soule: When maybe final\nthought on, on the scaling laws.",
    "start": "2511090",
    "end": "2516770"
  },
  {
    "text": "I wanted to bring up is a lot of these\nscaling laws are also assuming the class of technology remains the same.",
    "start": "2517655",
    "end": "2523995"
  },
  {
    "text": "We talked about, okay, these are\nscaling laws for model providers, basically in search of AGI. But like, do we really believe\nthis class of technology is",
    "start": "2524375",
    "end": "2531234"
  },
  {
    "text": "what's going to unlock AGI? I think there's a lot of, you know,\nUh, thought out there that probably not, you know, if you look at how these\ntechnologies evolve, there's a curve, but",
    "start": "2531235",
    "end": "2539525"
  },
  {
    "text": "that curve is driven by multiple different\ntechnologies coming in and introducing their own many curves on top of that.",
    "start": "2539525",
    "end": "2545355"
  },
  {
    "text": "And, you know, I mean, human intelligence\nrequires far less energy for the amount",
    "start": "2545404",
    "end": "2550734"
  },
  {
    "text": "of power, uh, and decision making. So if we're really talking about, like,\nOkay, we're going to promote these",
    "start": "2550734",
    "end": "2558145"
  },
  {
    "text": "scaling laws because, you know, model\nproviders will be maybe the business use cases aren't going to be incentivized.",
    "start": "2558145",
    "end": "2563615"
  },
  {
    "text": "But if we can unlock AGI, it will be,\nI would maybe also argue that these scaling laws probably don't reflect\nwhat whatever technology would converge",
    "start": "2563665",
    "end": "2571484"
  },
  {
    "text": "on for AGI, um, might scale at. So it's still a bit of a, an unknown.",
    "start": "2571484",
    "end": "2576345"
  },
  {
    "text": "Chris Hay: And on that point, Kay, I\nmean, imagine a world where we did have AGI or even ASI at that point, right?",
    "start": "2576705",
    "end": "2583775"
  },
  {
    "text": "But then you took that super intelligent\nbeing, and then you said you don't have any access to documents, you\ndon't have access to any tools in",
    "start": "2583815",
    "end": "2591445"
  },
  {
    "text": "your organization, because it's all\nlocked up in somebody else's hard disk or a box folder or something.",
    "start": "2591445",
    "end": "2596955"
  },
  {
    "text": "How effective would that\nAGI be in an organization? Um, I, I don't think very effective.",
    "start": "2597285",
    "end": "2603385"
  },
  {
    "text": "So, so I think, you know, I think, so Kate Soule: I mean, I think you're\nreading between my lines, which is,",
    "start": "2603385",
    "end": "2609685"
  },
  {
    "text": "is AGI really actually ever going to\nbe incentivized at least economically? I, there's a big question\nright there, I think, but",
    "start": "2609685",
    "end": "2616244"
  },
  {
    "text": "Chris Hay: I, I think as soon as AGI\nis achieved, if it's achieved, it's going to be put in a box and we're\nall going to go to the AI zoo and",
    "start": "2616425",
    "end": "2623715"
  },
  {
    "text": "we're going to be going, look at the\nAI zoo and have, have a chat with it. That's what I believe AGI's first task is.",
    "start": "2623715",
    "end": "2630564"
  },
  {
    "text": "Bryan Casey: What is the TAM of\nan AGI zoo is what we need to answer, uh, next week's episode.",
    "start": "2631205",
    "end": "2636035"
  },
  {
    "text": "Uh, so I know we're, we're\nbasically at time here. Thank you all for joining us on this week\nof mixture of experts and we will be back",
    "start": "2636355",
    "end": "2646265"
  },
  {
    "text": "next week, same time, not the same people. You suffered through one episode of me. I'm out of here.",
    "start": "2646604",
    "end": "2652244"
  },
  {
    "text": "Tim will return. Um, but thank you all for, for both\njoining today and for listening. Kate, Chris, Skyler.",
    "start": "2652245",
    "end": "2658504"
  },
  {
    "text": "Thanks all for joining today. Skyler Speakman: Thanks so much. Bryan Casey: Thanks Skyler Speakman: Brian. It's been a lot of fun, man.",
    "start": "2658544",
    "end": "2663535"
  },
  {
    "text": "Thanks.",
    "start": "2663595",
    "end": "2663955"
  }
]