[
  {
    "text": "We're going to take a look at back propagation.",
    "start": "720",
    "end": "3570"
  },
  {
    "text": "It's central to the functioning of neural networks, helping them to learn and adapt.",
    "start": "3990",
    "end": "8789"
  },
  {
    "text": "And we're going to cover it in simple but instructive terms.",
    "start": "9060",
    "end": "12720"
  },
  {
    "text": "So even if your only knowledge of neural networks is \"Isn't that something to do with chatGPT?\" Well, we've got you covered.",
    "start": "12780",
    "end": "21328"
  },
  {
    "text": "Now, a neural network fundamentally comprises multiple layers of neurons interconnected by weights.",
    "start": "21350",
    "end": "27682"
  },
  {
    "text": "So I'm going to draw some neurons here, and I'm organizing them in layers.",
    "start": "28140",
    "end": "36450"
  },
  {
    "text": "And these neurons are also known as nodes.",
    "start": "36450",
    "end": "39450"
  },
  {
    "text": "Now, the layers here are categorized.",
    "start": "40680",
    "end": "43710"
  },
  {
    "text": "So that's let's do that, the categorization.",
    "start": "43740",
    "end": "46369"
  },
  {
    "text": "We have a layer here called the input layer.",
    "start": "46380",
    "end": "50039"
  },
  {
    "text": "These two layers in the middle here are the hidden layer and the layer on the end here, that is the output layer.",
    "start": "50790",
    "end": "60735"
  },
  {
    "text": "And these neurons are all interconnected with each other across the layers.",
    "start": "61590",
    "end": "66689"
  },
  {
    "text": "So each neuron is connected to each other neuron in the next layer.",
    "start": "67020",
    "end": "75106"
  },
  {
    "text": "So you can see that here.",
    "start": "75870",
    "end": "79282"
  },
  {
    "text": "Okay, so now we have our basic neural network.",
    "start": "79560",
    "end": "82920"
  },
  {
    "text": "And during a process called forward propagation, the input data traverses through these layers where the weights,",
    "start": "83370",
    "end": "91252"
  },
  {
    "text": "biases and activation functions transform the data until an output is produced.",
    "start": "91252",
    "end": "96150"
  },
  {
    "text": "So, let's define those terms.",
    "start": "96180",
    "end": "99120"
  },
  {
    "text": "Weights, what is that when we're talking about a neural network?",
    "start": "99720",
    "end": "104730"
  },
  {
    "text": "Well, the weights define the strength of the connections between each of the neurons.",
    "start": "104760",
    "end": "111180"
  },
  {
    "text": "Then we have the activation function, and the activation function is applied to the weighted sum of the inputs",
    "start": "111810",
    "end": "120759"
  },
  {
    "text": "at each neuron to introduce non-linearity into the network, and that allows it to make complex relationships.",
    "start": "120760",
    "end": "127859"
  },
  {
    "text": "And that's really where we can use activation functions.",
    "start": "128100",
    "end": "132133"
  },
  {
    "text": "Commonly, you'll see activation functions used such as sigmoid, for example.",
    "start": "132330",
    "end": "136979"
  },
  {
    "text": "And then finally, biases.",
    "start": "137490",
    "end": "139853"
  },
  {
    "text": "So biases really are the additional parameter that shift the activation function to the left or the right, and that aids the network's flexibility.",
    "start": "140250",
    "end": "148739"
  },
  {
    "text": "So, consider a single training instance with its associated input data.",
    "start": "148770",
    "end": "153150"
  },
  {
    "text": "Now, this data propagates forward through the network,",
    "start": "153390",
    "end": "156610"
  },
  {
    "text": "causing every neutron to calculate a weighted sum of the inputs, which is then passed through its activation function.",
    "start": "156610",
    "end": "161999"
  },
  {
    "text": "And the final result is the network's output.",
    "start": "162300",
    "end": "164879"
  },
  {
    "text": "Great!",
    "start": "165540",
    "end": "166786"
  },
  {
    "text": "So where does back propagation come in?",
    "start": "166787",
    "end": "170400"
  },
  {
    "text": "Well, the initial output might not be accurate.",
    "start": "170430",
    "end": "174280"
  },
  {
    "text": "The network needs to learn from its mistakes and adjust its weights to improve.",
    "start": "174300",
    "end": "178989"
  },
  {
    "text": "And back propagation is essentially an algorithm used to train neural networks, applying the principle of error correction.",
    "start": "179040",
    "end": "186120"
  },
  {
    "text": "So, after forward propagation, the output error, which is the difference between the network's output and the actual output, is computed.",
    "start": "186330",
    "end": "195900"
  },
  {
    "text": "Now that's something called a loss function.",
    "start": "196260",
    "end": "202180"
  },
  {
    "text": "And the error is distributed back through the network, providing each neuron in the network a measure of its contribution to total error.",
    "start": "202470",
    "end": "211860"
  },
  {
    "text": "Using these measures, back propagation adjusts the weights and the biases of the network to minimize that error.",
    "start": "212730",
    "end": "218219"
  },
  {
    "text": "And the objective here is to improve the accuracy of the network's output during subsequent forward propagation.",
    "start": "218220",
    "end": "224189"
  },
  {
    "text": "It's a process of optimization, often employing a technique known as gradient descent.",
    "start": "224250",
    "end": "234350"
  },
  {
    "text": "Now, gradient descent, that's the topic of a whole video of its own,",
    "start": "234350",
    "end": "240889"
  },
  {
    "text": "but essentially, ",
    "start": "240890",
    "end": "241880"
  },
  {
    "text": "gradient descent is an algorithm used to find the optimal weights and biases that minimize the lost function.",
    "start": "241880",
    "end": "247488"
  },
  {
    "text": "It iteratively adjusts the weights and biases in the direction that reduces the error most rapidly.",
    "start": "247488",
    "end": "253669"
  },
  {
    "text": "And that means the steepest descent.",
    "start": "253880",
    "end": "256519"
  },
  {
    "text": "Now, back propagation is widely used in many neural networks.",
    "start": "257390",
    "end": "260629"
  },
  {
    "text": "So let's consider a speech recognition system.",
    "start": "260630",
    "end": "263299"
  },
  {
    "text": "We provide as input a spoken word, and it outputs a written transcript of that word.",
    "start": "263300",
    "end": "269479"
  },
  {
    "text": "Now, if during training our spoken inputs, it turns out that it doesn't match the written outputs, then back propagation may be able to help.",
    "start": "270020",
    "end": "278869"
  },
  {
    "text": "Look, I speak with a British accent, but I've lived in the US for years.",
    "start": "279230",
    "end": "284329"
  },
  {
    "text": "But when locals here ask for my name-- Martin --they often hear it as something different entirely, like Marvin or Morton or Mark.",
    "start": "284600",
    "end": "301220"
  },
  {
    "text": "If this neural network had made the same mistake, we'd calculate the error",
    "start": "301760",
    "end": "306340"
  },
  {
    "text": "by using the loss function to quantify the difference between the predicted output \"Marvin\" and the actual output \"Martin\".",
    "start": "306340",
    "end": "314090"
  },
  {
    "text": "We'd compute the gradient of the loss function with respect to the weight and biases in the network",
    "start": "314090",
    "end": "320161"
  },
  {
    "text": "and update the weighting biases in the network accordingly.",
    "start": "320161",
    "end": "323260"
  },
  {
    "text": "Then we'd undergo multiple iterations of forward propagation and back propagation,",
    "start": "323810",
    "end": "329480"
  },
  {
    "text": "tinkering with those weights and biases until we reach convergence-- a time where the network could reliably translate Martin into M-A-R-T-I-N.",
    "start": "329480",
    "end": "343053"
  },
  {
    "text": "This is can't be applied to people, can it?",
    "start": "343520",
    "end": "346249"
  },
  {
    "text": "Well, but anyway, let's just talk about one more thing with back propagation,",
    "start": "347060",
    "end": "350893"
  },
  {
    "text": "and that's the distinction between static and recurrent back propagation networks.",
    "start": "350893",
    "end": "357169"
  },
  {
    "text": "Let's start with static.",
    "start": "357740",
    "end": "361160"
  },
  {
    "text": "So static back propagation is employed in a feed-forward neural networks",
    "start": "361970",
    "end": "365722"
  },
  {
    "text": "where the data moves in a single direction from input layer to output layer.",
    "start": "365722",
    "end": "370008"
  },
  {
    "text": "Some example use cases of this, well, we can think of OCR,",
    "start": "370010",
    "end": "374611"
  },
  {
    "text": "or optical character recognition, where the goal is to identify and classify the letters and numbers in a given image.",
    "start": "374611",
    "end": "381048"
  },
  {
    "text": "Another common example is with spam detection, and here we are looking to use a neural network",
    "start": "381530",
    "end": "390198"
  },
  {
    "text": "to learn from features such as the emails, content and the sender's email address to classify an email as spam or not spam.",
    "start": "390198",
    "end": "399589"
  },
  {
    "text": "Now back propagation can also be applied to recurrent neural networks as well, or RNNs.",
    "start": "400130",
    "end": "410090"
  },
  {
    "text": "Now these networks have loops, and this type of back propagation is slightly more complex given the recursive nature of these networks.",
    "start": "410090",
    "end": "417199"
  },
  {
    "text": "Now, some use cases?",
    "start": "417200",
    "end": "417815"
  },
  {
    "text": "If we think about sentiment analysis, that's a common use case for this.",
    "start": "417815",
    "end": "424850"
  },
  {
    "text": "And that's a good example of where RNNs are used to analyze the sentiment of a piece of text, like a customer product review.",
    "start": "425480",
    "end": "432049"
  },
  {
    "text": "Another good example is time series prediction.",
    "start": "433550",
    "end": "439340"
  },
  {
    "text": "So predicting things like stock prices or weather patterns.",
    "start": "440030",
    "end": "444062"
  },
  {
    "text": "Ultimately, back propagation is the backbone of the learning in neural networks.",
    "start": "444860",
    "end": "449268"
  },
  {
    "text": "It tests for errors, working its way back from the output layer to the input layer,",
    "start": "449270",
    "end": "455108"
  },
  {
    "text": "adjusting the weights as it goes with the goal to minimize future errors.",
    "start": "455109",
    "end": "460699"
  },
  {
    "text": "Errors like how to pronounce Martin in a passible American accent.",
    "start": "462020",
    "end": "467509"
  },
  {
    "text": "MART-EN... MAR-EN... MART-ENNE.",
    "start": "468620",
    "end": "473658"
  }
]