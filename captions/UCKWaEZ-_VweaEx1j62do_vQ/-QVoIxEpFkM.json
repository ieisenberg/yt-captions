[
  {
    "start": "0",
    "end": "169000"
  },
  {
    "text": "In the context of large language models.",
    "start": "390",
    "end": "2879"
  },
  {
    "text": "What is a context window?",
    "start": "2880",
    "end": "5700"
  },
  {
    "text": "Well, it's the equivalent of its working memory.",
    "start": "5970",
    "end": "9099"
  },
  {
    "text": "It determines how long of a conversation the LLM can carry out without forgetting details from earlier in the exchange.",
    "start": "9120",
    "end": "17149"
  },
  {
    "text": "And allow me to illustrate this using the scientifically recognized IBU scale that's international blah units.",
    "start": "17160",
    "end": "25619"
  },
  {
    "text": "So blah here, that represents me sending a prompt to an LLM chatbot. Now the chatbot that returns with a response blah.",
    "start": "25770",
    "end": "41140"
  },
  {
    "text": "Right.",
    "start": "41490",
    "end": "42120"
  },
  {
    "text": "And then we continue the conversation.",
    "start": "42120",
    "end": "43920"
  },
  {
    "text": "So I say something else and then it responds back to me.",
    "start": "43920",
    "end": "51149"
  },
  {
    "text": "Blah, blah, blah, blah.",
    "start": "52440",
    "end": "54538"
  },
  {
    "text": "International blah units.",
    "start": "55490",
    "end": "57170"
  },
  {
    "text": "Now, this box here",
    "start": "58010",
    "end": "60408"
  },
  {
    "text": "represents the context window, and in this case, the entire conversation fits within it.",
    "start": "61280",
    "end": "69798"
  },
  {
    "text": "Now, that means that when the LLM generated this response here, this blah,",
    "start": "70550",
    "end": "75369"
  },
  {
    "text": "it had within its working memory my prompts to the model here and here.",
    "start": "75369",
    "end": "81769"
  },
  {
    "text": "And it also had the other response that the model had returned to me in order to build this response.",
    "start": "82280",
    "end": "88519"
  },
  {
    "text": "All good.",
    "start": "89450",
    "end": "90289"
  },
  {
    "text": "Now let's consider a longer conversation.",
    "start": "90290",
    "end": "93379"
  },
  {
    "text": "So more blahs.",
    "start": "93570",
    "end": "94880"
  },
  {
    "text": "I send my prompt blah.",
    "start": "95637",
    "end": "98420"
  },
  {
    "text": "It then sends me a response.",
    "start": "99410",
    "end": "102739"
  },
  {
    "text": "And now we go back and forth with more conversations.",
    "start": "103750",
    "end": "107379"
  },
  {
    "text": "I say something.",
    "start": "107500",
    "end": "108500"
  },
  {
    "text": "It responds to that.",
    "start": "109150",
    "end": "110380"
  },
  {
    "text": "I say one more thing and it responds to that.",
    "start": "110680",
    "end": "116140"
  },
  {
    "text": "So now we have a longer conversation here to deal with.",
    "start": "116250",
    "end": "120639"
  },
  {
    "text": "And it turns out that this conversation thread is actually longer than the context window of the model.",
    "start": "121150",
    "end": "128860"
  },
  {
    "text": "Now, that means that the blahs from earlier in the conversation are no longer available to the model.",
    "start": "129639",
    "end": "138630"
  },
  {
    "text": "It has no memory of them when generating new responses.",
    "start": "138640",
    "end": "142929"
  },
  {
    "text": "Now the LLM can do its best to infer what came earlier by looking at the conversation that is within its context window.",
    "start": "143650",
    "end": "151599"
  },
  {
    "text": "But now the LLM is making educated guesses and that can result in some wicked hallucinations.",
    "start": "152050",
    "end": "159443"
  },
  {
    "text": "So understanding how the context window works is essential to getting the most out about a LLMs.",
    "start": "159443",
    "end": "164978"
  },
  {
    "text": "Let's get into a bit more detail about that now.",
    "start": "165490",
    "end": "168490"
  },
  {
    "start": "169000",
    "end": "360000"
  },
  {
    "text": "Now my producer is telling me that context window size is in fact not measured in IBUs and that I made that up.",
    "start": "169050",
    "end": "177460"
  },
  {
    "text": "We actually measure context windows in something called tokens.",
    "start": "177880",
    "end": "182560"
  },
  {
    "text": "So let's describe tokenization.",
    "start": "182710",
    "end": "185439"
  },
  {
    "text": "Let's get into context, length, size, and we're going to talk about the challenges of long context windows.",
    "start": "185710",
    "end": "192550"
  },
  {
    "text": "So the start, what is a token?",
    "start": "192790",
    "end": "196808"
  },
  {
    "text": "Well, for us humans, the smallest unit of information that we use to represent language is a single.",
    "start": "196990",
    "end": "204039"
  },
  {
    "text": "Character.",
    "start": "204960",
    "end": "205960"
  },
  {
    "text": "So something like a letter or a number or a punctuation mark, something like that.",
    "start": "206280",
    "end": "214229"
  },
  {
    "text": "But the smallest unit of language that AI models use is called a token.",
    "start": "214890",
    "end": "221009"
  },
  {
    "text": "Now, a token can represent a character as well.",
    "start": "221400",
    "end": "225778"
  },
  {
    "text": "But it might also be a part of a word or a whole word or even a short multi-word phrase.",
    "start": "226260",
    "end": "233008"
  },
  {
    "text": "So, for example, let's consider the different roles played by the letter A.",
    "start": "233520",
    "end": "238919"
  },
  {
    "text": "So I'm going to write some sentences and we're going to tokenize them.",
    "start": "239160",
    "end": "241680"
  },
  {
    "text": "Let's start with Martin",
    "start": "242280",
    "end": "244770"
  },
  {
    "text": "drove a car.",
    "start": "246010",
    "end": "250993"
  },
  {
    "text": "Now A here is an entire word and it will be represented by a distinct token.",
    "start": "252180",
    "end": "262018"
  },
  {
    "text": "Now, what if we try a different sentence?",
    "start": "263030",
    "end": "264560"
  },
  {
    "text": "So, Martin",
    "start": "264590",
    "end": "265699"
  },
  {
    "text": "is",
    "start": "267420",
    "end": "268420"
  },
  {
    "text": "amoral.",
    "start": "270250",
    "end": "271250"
  },
  {
    "text": "Not sure why we would say that,",
    "start": "271840",
    "end": "273699"
  },
  {
    "text": "but look, in this case, A is not a word, but it's an addition to moral that significantly changes the meaning of that word.",
    "start": "273700",
    "end": "282790"
  },
  {
    "text": "So here a moral would be represented by two distinct tokens, a token for A and another token for moral.",
    "start": "282850",
    "end": "293290"
  },
  {
    "text": "All right.\none more.",
    "start": "294540",
    "end": "295780"
  },
  {
    "text": "Martin",
    "start": "296300",
    "end": "297300"
  },
  {
    "text": "loves",
    "start": "298910",
    "end": "299910"
  },
  {
    "text": "his cat.",
    "start": "300600",
    "end": "303176"
  },
  {
    "text": "Now the A in cat is simply a letter.",
    "start": "303850",
    "end": "307750"
  },
  {
    "text": "In a word, it carries no semantic meaning by itself and would therefore not be a distinct token.",
    "start": "307750",
    "end": "313809"
  },
  {
    "text": "The token here",
    "start": "314790",
    "end": "315790"
  },
  {
    "text": "It's just cat.",
    "start": "316380",
    "end": "317729"
  },
  {
    "text": "Now, the tool, the converts language, to tokens.",
    "start": "318300",
    "end": "321959"
  },
  {
    "text": "It's got a name.",
    "start": "321960",
    "end": "322960"
  },
  {
    "text": "It's called a tokenizer.",
    "start": "323220",
    "end": "326910"
  },
  {
    "text": "And different tokenizer, as might tokenize the same passage of writing differently.",
    "start": "328420",
    "end": "332800"
  },
  {
    "text": "But kind of a good rule of thumb is that a a regular word in English language is represented by something like 1.5",
    "start": "332800",
    "end": "344289"
  },
  {
    "text": "tokens by the tokenizer.",
    "start": "345350",
    "end": "348260"
  },
  {
    "text": "So hundred words that might result in 150 tokens.",
    "start": "348260",
    "end": "352670"
  },
  {
    "text": "So context windows consist of tokens, but how many tokens are we actually talking about?",
    "start": "353180",
    "end": "359810"
  },
  {
    "start": "360000",
    "end": "542000"
  },
  {
    "text": "To answer that, we need to understand how LLM process tokens",
    "start": "360790",
    "end": "364839"
  },
  {
    "text": "in a context window.",
    "start": "365790",
    "end": "368130"
  },
  {
    "text": "Now, transformer models use something called the self",
    "start": "368640",
    "end": "373979"
  },
  {
    "text": "attention",
    "start": "374910",
    "end": "375910"
  },
  {
    "text": "mechanism.",
    "start": "377280",
    "end": "378280"
  },
  {
    "text": "And the self attention mechanism is used to calculate the relationships",
    "start": "379410",
    "end": "383623"
  },
  {
    "text": "and the dependencies between different parts of an input like words at the beginning and at the end of a paragraph.",
    "start": "383623",
    "end": "390119"
  },
  {
    "text": "Now self attention mechanism computes vectors of weights in which each weight",
    "start": "390810",
    "end": "395461"
  },
  {
    "text": "represents how relevant that token is to the other tokens in the sequence.",
    "start": "395461",
    "end": "399239"
  },
  {
    "text": "So the size of the context window determines the maximum number of tokens",
    "start": "399720",
    "end": "404931"
  },
  {
    "text": "that the model can pay attention to at any one time.",
    "start": "404931",
    "end": "409200"
  },
  {
    "text": "Now, context window size has been rapidly increasing.",
    "start": "410100",
    "end": "414149"
  },
  {
    "text": "So the first LLMs that I used, they had context windows of around 2000 tokens. ",
    "start": "414160",
    "end": "421169"
  },
  {
    "text": "The IBM Granite three model today has a context window of",
    "start": "421710",
    "end": "424923"
  },
  {
    "text": "128,000 tokens, and other models have larger context when they still.",
    "start": "424923",
    "end": "431369"
  },
  {
    "text": "And but it almost seems like overkill, doesn't it?",
    "start": "432190",
    "end": "435430"
  },
  {
    "text": "I would have to be conversing with a chat bot all day to fill a 128K token window.",
    "start": "435460",
    "end": "442809"
  },
  {
    "text": "Well, actually, it's not necessarily true because there can be a lot of things taking up space within a model's context window.",
    "start": "443560",
    "end": "453500"
  },
  {
    "text": "So let's take a look at what some of those things could be.",
    "start": "453520",
    "end": "456669"
  },
  {
    "text": "Well, one of them is the the user input, the the blah that I sent into the model.",
    "start": "457120",
    "end": "464798"
  },
  {
    "text": "And of course, we also have the model responses as well,",
    "start": "465190",
    "end": "470529"
  },
  {
    "text": "the blahs that it was sending back,",
    "start": "470920",
    "end": "473560"
  },
  {
    "text": "but a context window may also contain all sorts of other things as well.",
    "start": "474070",
    "end": "479499"
  },
  {
    "text": "So most models provide what is called a system prompt.",
    "start": "479800",
    "end": "485560"
  },
  {
    "text": "Into the context window.",
    "start": "486540",
    "end": "488190"
  },
  {
    "text": "Now, this is often hidden from the user.",
    "start": "488430",
    "end": "491069"
  },
  {
    "text": "But it conditions the behavior of the model, telling it what it can and cannot do.",
    "start": "491520",
    "end": "496289"
  },
  {
    "text": "A user may also choose to attach some documents into",
    "start": "497130",
    "end": "502400"
  },
  {
    "text": "their contacts window, or they might put in some source code as well.",
    "start": "502400",
    "end": "507630"
  },
  {
    "text": "And that can be used by the LLM to refer to it and its responses.",
    "start": "508380",
    "end": "512549"
  },
  {
    "text": "And then supplementary information drawn from external data sources",
    "start": "512850",
    "end": "516901"
  },
  {
    "text": "for retrieval augmented generation or RAG,",
    "start": "516901",
    "end": "521694"
  },
  {
    "text": "that might be stored within the context window during inference.",
    "start": "521885",
    "end": "525239"
  },
  {
    "text": "So a few long documents, some snippets of source code, I can quickly fill up a context window.",
    "start": "525900",
    "end": "533580"
  },
  {
    "text": "So the bigger the context window, the better, right?",
    "start": "533820",
    "end": "537390"
  },
  {
    "text": "Well, larger context windows do present some challenges as well.",
    "start": "538140",
    "end": "542099"
  },
  {
    "start": "542000",
    "end": "690000"
  },
  {
    "text": "What sort of challenges?",
    "start": "542730",
    "end": "545190"
  },
  {
    "text": "Well, I think the most obvious one that would have to be",
    "start": "545700",
    "end": "548850"
  },
  {
    "text": "compute.",
    "start": "550070",
    "end": "551070"
  },
  {
    "text": "The compute requirements scale quadratically  with the length of a sequence.",
    "start": "551760",
    "end": "557640"
  },
  {
    "text": "What does that mean?",
    "start": "558000",
    "end": "558660"
  },
  {
    "text": "Well, essentially, as the number of input tokens doubles,",
    "start": "558660",
    "end": "563308"
  },
  {
    "text": "that results in the model needing four times as much processing power to handle it.",
    "start": "564370",
    "end": "571809"
  },
  {
    "text": "Now, remember, as the model predicts, the next token in a sequence.",
    "start": "572530",
    "end": "575860"
  },
  {
    "text": "It computes the relationships between the token and every single preceding token in that sequence.",
    "start": "576190",
    "end": "583090"
  },
  {
    "text": "So as context length increases, more and more computation is going to be required.",
    "start": "583390",
    "end": "589000"
  },
  {
    "text": "Now, long context windows also can negatively affect performance, specifically the performance of the model.",
    "start": "589690",
    "end": "599080"
  },
  {
    "text": "So like people and LLMs can be overwhelmed by an abundance of extra detail.",
    "start": "599830",
    "end": "606279"
  },
  {
    "text": "They can also get lazy and take all sorts of cognitive shortcuts.",
    "start": "606760",
    "end": "610379"
  },
  {
    "text": "A 2023 paper found that models perform best when relevant information is towards the",
    "start": "610390",
    "end": "616803"
  },
  {
    "text": "beginning or towards the end of the input context. And they found that performance",
    "start": "616803",
    "end": "622461"
  },
  {
    "text": "degrades when the model must carefully consider the information that is in the middle of long context.",
    "start": "622461",
    "end": "626380"
  },
  {
    "text": "And then finally, we also have to be concerned with a number of safety challenges as well.",
    "start": "627400",
    "end": "633669"
  },
  {
    "text": "Longer context window might have the unintended effect of presenting a longer attack surface for adversarial prompts,",
    "start": "634900",
    "end": "641780"
  },
  {
    "text": "a long context length can increase a model's vulnerability to jailbreaking,",
    "start": "641780",
    "end": "646244"
  },
  {
    "text": "where malicious content is embedded deep within the input, making it harder for the model safety mechanisms",
    "start": "646244",
    "end": "652171"
  },
  {
    "text": "to detect and filter out harmful instructions.",
    "start": "652171",
    "end": "655419"
  },
  {
    "text": "So no matter how you measure it with either with IBUs or more accurately, tokens,",
    "start": "655690",
    "end": "662909"
  },
  {
    "text": "selecting the appropriate number of tokens for a context window involves balancing the need",
    "start": "662909",
    "end": "668371"
  },
  {
    "text": "to supply ample information for the model's self attention mechanism.",
    "start": "668371",
    "end": "672819"
  },
  {
    "text": "With the increasing demands and performance issues those additional tokens may bring.",
    "start": "673360",
    "end": "678399"
  }
]