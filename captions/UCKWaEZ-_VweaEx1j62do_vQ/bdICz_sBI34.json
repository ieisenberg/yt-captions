[
  {
    "text": "GPT stands for Generative Pre-trained \nTransformer, the core technology behind ChatGPT,",
    "start": "80",
    "end": "5799"
  },
  {
    "text": "but what is this technology, really?",
    "start": "5800",
    "end": "8960"
  },
  {
    "text": "Let's get into it.",
    "start": "8960",
    "end": "11280"
  },
  {
    "text": "So let's break this down into what a GPT is,",
    "start": "11280",
    "end": "15600"
  },
  {
    "text": "a little bit of history of GPT models,",
    "start": "15600",
    "end": "18315"
  },
  {
    "text": "and then an example of how we've put GPTs\nto work right here in the studio,",
    "start": "18315",
    "end": "24480"
  },
  {
    "text": "and let's start with what. What is a GPT?",
    "start": "24480",
    "end": "28800"
  },
  {
    "text": "Well, a GPT is a type of large language model that  ",
    "start": "28800",
    "end": "31919"
  },
  {
    "text": "uses deep learning to produce natural \nlanguage text based on a given inputs.",
    "start": "31920",
    "end": "37320"
  },
  {
    "text": "And GPT models work by analyzing an input \nsequence and predicting the most likely outputs.",
    "start": "37320",
    "end": "42840"
  },
  {
    "text": "So let's break this down.",
    "start": "42840",
    "end": "45400"
  },
  {
    "text": "So we have generative is the G,",
    "start": "45400",
    "end": "50720"
  },
  {
    "text": "Pre-trained is the P,",
    "start": "50720",
    "end": "54368"
  },
  {
    "text": "and the T that is for Transformer.",
    "start": "54368",
    "end": "58930"
  },
  {
    "text": "So what does all of this actually mean?",
    "start": "58930",
    "end": "62454"
  },
  {
    "text": "Well, in generative pre-training, let's let's start with that.",
    "start": "62454",
    "end": "67437"
  },
  {
    "text": "So generative pre-training teaches the model \nto detect patterns in data and then apply those patterns to new inputs.",
    "start": "68480",
    "end": "77720"
  },
  {
    "text": "It's actually a form of learning \ncalled unsupervised learning,  ",
    "start": "77720",
    "end": "82880"
  },
  {
    "text": "where the model is given unlabeled data.",
    "start": "82880",
    "end": "85240"
  },
  {
    "text": "That means data that doesn't have \nany predefined labels or categories.",
    "start": "85240",
    "end": "88720"
  },
  {
    "text": "And then it must interpret it independently.",
    "start": "88720",
    "end": "91720"
  },
  {
    "text": "And by learning to detect \npatterns in those datasets.",
    "start": "91720",
    "end": "95120"
  },
  {
    "text": "The model can draw similar conclusions \nwhen exposed to new unseen inputs.",
    "start": "95120",
    "end": "100920"
  },
  {
    "text": "Now, GPT models are trained with billions or even   trillions of parameters which are  refined over the training process.",
    "start": "100920",
    "end": "108640"
  },
  {
    "text": "Now, the T in GPT that stands for Transformer.",
    "start": "108640",
    "end": "115040"
  },
  {
    "text": "Transformers are a type of neural network \nspecialized in natural language processing.",
    "start": "115040",
    "end": "119920"
  },
  {
    "text": "Transformers don't understand language \nin the same way that humans do.",
    "start": "119920",
    "end": "123560"
  },
  {
    "text": "Instead, they process words into discrete units.",
    "start": "123560",
    "end": "127200"
  },
  {
    "text": "Those units are called tokens, and \nfor those tokens, they're smaller  ",
    "start": "127200",
    "end": "133360"
  },
  {
    "text": "chunks of words or characters \nthat the model can understand  ",
    "start": "133360",
    "end": "137080"
  },
  {
    "text": "and transform all models of process data with \ntwo modules known as encoders and decoders.",
    "start": "137080",
    "end": "142480"
  },
  {
    "text": "And they use something called \nself attention mechanisms to  ",
    "start": "142480",
    "end": "145319"
  },
  {
    "text": "establish dependencies and relationships.",
    "start": "145320",
    "end": "148000"
  },
  {
    "text": "So let's define what those are and \nlet's start with self attention.",
    "start": "148000",
    "end": "154920"
  },
  {
    "text": "So what is a self attention mechanism?",
    "start": "154920",
    "end": "157800"
  },
  {
    "text": "Well, it's really the signature feature of \nTransform is the secret sauce, if you like,  ",
    "start": "157800",
    "end": "163400"
  },
  {
    "text": "older models like recurrent neural \nnetworks or convolutional neural networks.",
    "start": "163400",
    "end": "167959"
  },
  {
    "text": "They assess input data sequentially or \nhierarchically, but transformers can self direct  ",
    "start": "167960",
    "end": "173896"
  },
  {
    "text": "to their attention to the most important tokens \nin the input sequence, no matter where they are.",
    "start": "173896",
    "end": "180080"
  },
  {
    "text": "They allow the model to evaluate each word  ",
    "start": "180080",
    "end": "182240"
  },
  {
    "text": "significance within the context \nof the complete input sequence,  ",
    "start": "182240",
    "end": "185840"
  },
  {
    "text": "making it possible for the model to understand \nlinkages and dependencies between words.",
    "start": "185840",
    "end": "191319"
  },
  {
    "text": "Okay, so that self attention.",
    "start": "191320",
    "end": "193960"
  },
  {
    "text": "What about the encoder?",
    "start": "193960",
    "end": "196760"
  },
  {
    "text": "Well, the encoder module maps tokens onto a \nthree dimensional vector space in a process  ",
    "start": "196760",
    "end": "202080"
  },
  {
    "text": "called embedding tokens encoded nearby in the \n3D space or seem to be more similar in meaning.",
    "start": "202080",
    "end": "208680"
  },
  {
    "text": "The encoder blocks in the transformer \nnetwork assigns each embedding a weight  ",
    "start": "208680",
    "end": "213079"
  },
  {
    "text": "which determines its relative importance \nand positioned encode as capture semantics,  ",
    "start": "213080",
    "end": "217760"
  },
  {
    "text": "which lets GPT models differentiate between \ngroupings of the same words in different orders.",
    "start": "217760",
    "end": "223280"
  },
  {
    "text": "So for example, the egg came before the chicken \nas compared to the chicken came before the egg.",
    "start": "223280",
    "end": "229400"
  },
  {
    "text": "Same words in that sentence, \nbut different meanings.",
    "start": "229400",
    "end": "232680"
  },
  {
    "text": "There's also a decoder module as well.",
    "start": "232680",
    "end": "237280"
  },
  {
    "text": "And the decoder.",
    "start": "237280",
    "end": "238920"
  },
  {
    "text": "What that does is it predicts the most \nstatistically probable response to the  ",
    "start": "238920",
    "end": "242880"
  },
  {
    "text": "embeddings prepared by the encoders, by \nidentifying the most important portions  ",
    "start": "242880",
    "end": "247880"
  },
  {
    "text": "of the input sequence with self attention and then \ndetermining the output most likely to be correct.",
    "start": "247880",
    "end": "253880"
  },
  {
    "text": "Now a quick word on the history of \ngenerative Pre-trained Transformers.",
    "start": "253880",
    "end": "259359"
  },
  {
    "text": "The transformer architecture was first \nintroduced in 2017 in the Google brain paper,",
    "start": "259360",
    "end": "267199"
  },
  {
    "text": "\"Attention is all you need.\"",
    "start": "267200",
    "end": "269400"
  },
  {
    "text": "Today there are a whole bunch of generative A.I. models built on this architecture, including  open source models like Llama from Meta",
    "start": "269400",
    "end": "277953"
  },
  {
    "text": "and Granite from IBM, and closed source frontier models \nlike Google Gemini and Claude from Anthropic,",
    "start": "277954",
    "end": "284360"
  },
  {
    "text": "but I think the GPT model that most comes to mind \nfor most people is ChatGPT from OpenAI.",
    "start": "284360",
    "end": "293000"
  },
  {
    "text": "Now ChatGPT is not a specific GPT model.",
    "start": "293000",
    "end": "296520"
  },
  {
    "text": "It's a chat interface that allows users to interact   with various generative pre-trained transformers.",
    "start": "296520",
    "end": "302560"
  },
  {
    "text": "You pick the model you want from a list and today xthere's likely to be a GPT4 model like GPT4o.",
    "start": "302560",
    "end": "309600"
  },
  {
    "text": "But the first GPT model from OpenAI was \nGPT-1, and that came out back in 2018.",
    "start": "309600",
    "end": "318200"
  },
  {
    "text": "It was able to answer questions in a humanlike way to an extent, but it was  ",
    "start": "318200",
    "end": "323360"
  },
  {
    "text": "also highly prone to hallucinations and just general bouts of nonsense.",
    "start": "323360",
    "end": "328479"
  },
  {
    "text": "GPT2 That came out the following year as a much \nlarger model boasting 1.5 billion parameters.",
    "start": "328480",
    "end": "341280"
  },
  {
    "text": "Sounds like quite a lot.",
    "start": "341280",
    "end": "343360"
  },
  {
    "text": "Since then, linear scaling has resulted in each \nsubsequent model becoming larger and more capable.",
    "start": "343360",
    "end": "349479"
  },
  {
    "text": "So by the time we get to today's GPT4 models, well, those are estimated to contain something like 1.8 trillion parameters, which is a whole lot more.",
    "start": "349480",
    "end": "363720"
  },
  {
    "text": "So we talked about how a GPT is a \nfundamentally different type of model,  ",
    "start": "363720",
    "end": "368200"
  },
  {
    "text": "one that uses self attention mechanisms to see \nthe big picture and evaluate the relationships  ",
    "start": "368200",
    "end": "372920"
  },
  {
    "text": "between words in a sequence, allowing it to \ngenerate contextually relevant responses.",
    "start": "372920",
    "end": "377160"
  },
  {
    "text": "And I'd like to share a quick example of how  ",
    "start": "377160",
    "end": "381840"
  },
  {
    "text": "that's helped right here in \nmy role in video education.",
    "start": "381840",
    "end": "385919"
  },
  {
    "text": "We create close captions for every \nvideo using a speech to text service.",
    "start": "385920",
    "end": "391680"
  },
  {
    "text": "Now here's a snippet from the \ncourse I was working on this  ",
    "start": "391680",
    "end": "394400"
  },
  {
    "text": "week showing the transcript and the timestamps.",
    "start": "394400",
    "end": "398120"
  },
  {
    "text": "Now it's not bad, but there are some errors.",
    "start": "398120",
    "end": "401080"
  },
  {
    "text": "It's mis transcribed Cobal as CBL.",
    "start": "401080",
    "end": "404581"
  },
  {
    "text": "It's missed me saying a T in  HTTP and it had no idea that K.S. is actually a product called CICS,",
    "start": "404581",
    "end": "413164"
  },
  {
    "text": "which is pronounced kicks.",
    "start": "413164",
    "end": "414801"
  },
  {
    "text": "And that's all typical of air models built \non recurrent neural networks that process  ",
    "start": "414801",
    "end": "419600"
  },
  {
    "text": "data sequentially one word at a time.",
    "start": "419600",
    "end": "422600"
  },
  {
    "text": "So I gave this transcript to a GPT model,  ",
    "start": "422600",
    "end": "426400"
  },
  {
    "text": "along with the script that I based my talk on, which I called the ground truth.",
    "start": "426400",
    "end": "433479"
  },
  {
    "text": "So this was the actual script that I was reading from.",
    "start": "433480",
    "end": "438400"
  },
  {
    "text": "Then I told the GPT to fix the transcript,",
    "start": "438400",
    "end": "441639"
  },
  {
    "text": "and here's what it came up with.",
    "start": "441640",
    "end": "443800"
  },
  {
    "text": "It fixed all three errors.",
    "start": "443800",
    "end": "445319"
  },
  {
    "text": "CBL is Cobal, KS is CICS, and HTP is HTTP,",
    "start": "445320",
    "end": "451320"
  },
  {
    "text": "and in fact, I tried this again, but \nthis time removing the ground truth  ",
    "start": "451320",
    "end": "457160"
  },
  {
    "text": "entirely and instead just gave it a brief \nsynopsis that said This is a video about  ",
    "start": "457160",
    "end": "463280"
  },
  {
    "text": "a modern CISC application and it was \nstill able to fix those three errors.",
    "start": "463280",
    "end": "468600"
  },
  {
    "text": "And that's the self attention mechanism at work, processing the entire input   sequence and better understanding the context of what I was discussing.",
    "start": "468600",
    "end": "478600"
  },
  {
    "text": "Even without having the exact script in front of it.",
    "start": "478600",
    "end": "481995"
  },
  {
    "text": "The GPT model uses broader language and software knowledge \nto correct technical terms and acronyms.",
    "start": "481995",
    "end": "488280"
  },
  {
    "text": "So that's generative Pre-trained, Transformers or \nGPT as they form the foundation of generative A.I.",
    "start": "488280",
    "end": "494680"
  },
  {
    "text": "applications using transformer architecture and   undergoing supervised pre training on vast amounts of unlabeled data.",
    "start": "494680",
    "end": "504160"
  },
  {
    "text": "And if you happen to turn video captions on in this video and you spotted an error,  ",
    "start": "504160",
    "end": "509440"
  },
  {
    "text": "well now you know which model to blame.",
    "start": "510120",
    "end": "512400"
  }
]