[
  {
    "text": "You might be wondering, \nwhat's next in generative AI?",
    "start": "560",
    "end": "3280"
  },
  {
    "text": "Well, today's large language models,  ",
    "start": "3280",
    "end": "6080"
  },
  {
    "text": "what they do is they predict a token \ngiven a sequence of other tokens.",
    "start": "6080",
    "end": "10320"
  },
  {
    "text": "However, we're quickly moving up \nthe thought abstraction tree where  ",
    "start": "10320",
    "end": "13920"
  },
  {
    "text": "these language concept models are now emerging.",
    "start": "13920",
    "end": "16880"
  },
  {
    "text": "And what that is, is it's able to \nreason within the sentence space.",
    "start": "16880",
    "end": "20160"
  },
  {
    "text": "So if I were to talk about a topic, then in turn,  ",
    "start": "20160",
    "end": "23600"
  },
  {
    "text": "every single time the LCM were to predict what \nI was gonna say, it might be a bit different.",
    "start": "23600",
    "end": "28640"
  },
  {
    "text": "But now what we're doing is we're predicting the  ",
    "start": "28640",
    "end": "30640"
  },
  {
    "text": "probability of a concept giving \na series of sentences that comes.",
    "start": "30640",
    "end": "35120"
  },
  {
    "text": "But in general, both algorithms seek to predict  ",
    "start": "35120",
    "end": "37520"
  },
  {
    "text": "the next sequence or token \ngiven a series embeddings.",
    "start": "37520",
    "end": "42080"
  },
  {
    "text": "Now this gives rise to the notion that LLMs \nand now LCMs are about data representation.",
    "start": "42080",
    "end": "48640"
  },
  {
    "text": "Word embeddings are a way to represent words \nor sentences within these large vector spaces.",
    "start": "48640",
    "end": "54239"
  },
  {
    "text": "So for example, say if I'm drawing \na three dimensional grid here.",
    "start": "54240",
    "end": "58480"
  },
  {
    "text": "And I take the sentence, it's called artificial \nintelligence is a combination of art and science.",
    "start": "58480",
    "end": "64239"
  },
  {
    "text": "Now, perhaps it would land somewhere here.",
    "start": "64240",
    "end": "67040"
  },
  {
    "text": "So now I have my X, my Y, and my Z coordinates.",
    "start": "67040",
    "end": "71040"
  },
  {
    "text": "And this very effectively \nrepresents that sentence.",
    "start": "71040",
    "end": "75120"
  },
  {
    "text": "And what I can do now is I can figure \nout what the relationship is between  ",
    "start": "75120",
    "end": "78960"
  },
  {
    "text": "that sentence and maybe another \none that could land about here.",
    "start": "78960",
    "end": "83280"
  },
  {
    "text": "And I can take what's called a cosine similarity \nmeasure between this vector and that vector,",
    "start": "83280",
    "end": "88159"
  },
  {
    "text": "to tell me, in concept, where are they?",
    "start": "88160",
    "end": "91120"
  },
  {
    "text": "And there are many other ways of \nwhich I can take that measurement.",
    "start": "91120",
    "end": "94480"
  },
  {
    "text": "But now let's take a step back.",
    "start": "94480",
    "end": "96640"
  },
  {
    "text": "Arguably, one of the first \ntypes of embeddings that we  ",
    "start": "96640",
    "end": "99520"
  },
  {
    "text": "have is called a frequency-based embedding.",
    "start": "99520",
    "end": "103600"
  },
  {
    "text": "And what this does is this looks at the \nmost prominent terms within a sentence,  ",
    "start": "103600",
    "end": "108799"
  },
  {
    "text": "and it can represent how many times \ndid that word actually show up.",
    "start": "108800",
    "end": "113040"
  },
  {
    "text": "And that was one of the first ways to help us  ",
    "start": "113040",
    "end": "115200"
  },
  {
    "text": "identify the terms that are the most \nsignificant within a certain document.",
    "start": "115200",
    "end": "119360"
  },
  {
    "text": "However, this really lacked the depth of a  ",
    "start": "119360",
    "end": "121600"
  },
  {
    "text": "prediction-based embeddings \nthat we now have today.",
    "start": "121600",
    "end": "124560"
  },
  {
    "text": "And so the prediction- based \nembeddings are very much different.",
    "start": "124560",
    "end": "129280"
  },
  {
    "text": "And what they do is we use models \nto actually project words into a  ",
    "start": "129280",
    "end": "133360"
  },
  {
    "text": "higher dimensional space like we see here.",
    "start": "133360",
    "end": "135760"
  },
  {
    "text": "And this refers to our function embeddings,",
    "start": "135760",
    "end": "138239"
  },
  {
    "text": "but this is the type of embedding \nthat's really dominant today.",
    "start": "138240",
    "end": "141680"
  },
  {
    "text": "One of the most significant \nbreakthroughs that happened was in 2013,  ",
    "start": "141680",
    "end": "145439"
  },
  {
    "text": "with the introduction of the word-to-vector model.",
    "start": "145440",
    "end": "148160"
  },
  {
    "text": "And then shortly thereafter, we \nsaw the introduction of GloVe,  ",
    "start": "148160",
    "end": "152000"
  },
  {
    "text": "Elmo, BERT, Albert, and there's \neven a new one called SONAR.",
    "start": "152000",
    "end": "156160"
  },
  {
    "text": "But these techniques enable us \nto not only represent the words,",
    "start": "156160",
    "end": "159200"
  },
  {
    "text": "but also to capture what's called \nthe semantic and the contextual  ",
    "start": "159200",
    "end": "162959"
  },
  {
    "text": "value of where they land in this \nspace or what their meaning is.",
    "start": "162960",
    "end": "166880"
  },
  {
    "text": "Now you might be wondering, what \ndo embeddings have to do with LLMs?",
    "start": "166880",
    "end": "170640"
  },
  {
    "text": "Well word embeddings convert text \nor sentences into these tokens,  ",
    "start": "170640",
    "end": "174880"
  },
  {
    "text": "such that an LLM can really \nunderstand the vocabulary.",
    "start": "174880",
    "end": "178080"
  },
  {
    "text": "And what happens is these embeddings are here, \nso your sentences and words are inputs into that.",
    "start": "178080",
    "end": "184240"
  },
  {
    "text": "Now, as it feeds forward into a stack,  ",
    "start": "184240",
    "end": "186880"
  },
  {
    "text": "so I might have, let's say, five of \nthese different types of encoders,",
    "start": "186880",
    "end": "191040"
  },
  {
    "text": "and the data goes up, then it keeps going up \ninto even more of these encodars as it happens.",
    "start": "191040",
    "end": "197599"
  },
  {
    "text": "But you'll notice right in the middle, \nright, we have this multi-headed attention.",
    "start": "197600",
    "end": "201200"
  },
  {
    "text": "Which tells us which tokens are \nmost interesting rather than others.",
    "start": "201200",
    "end": "205680"
  },
  {
    "text": "Then we go to a fully connected neural network.",
    "start": "205680",
    "end": "208480"
  },
  {
    "text": "And when this happens,",
    "start": "208480",
    "end": "209920"
  },
  {
    "text": "this helps to find all the different \nrelationships between all of the tokens  ",
    "start": "209920",
    "end": "213760"
  },
  {
    "text": "that were input and already that were biased \ntowards what should we pay attention to.",
    "start": "213760",
    "end": "219439"
  },
  {
    "text": "Finally, we normalize that \ntext and then its output.",
    "start": "219440",
    "end": "222880"
  },
  {
    "text": "So now we have this encoded representation of the \ndata that's already been tokenized and embeddings.",
    "start": "222880",
    "end": "230240"
  },
  {
    "text": "And the next part of this is we then \ngo over into what's called a decoder.",
    "start": "230240",
    "end": "235120"
  },
  {
    "text": "And this is what this stack is.",
    "start": "235120",
    "end": "237200"
  },
  {
    "text": "And you might be wondering, how \nare these two linked together?",
    "start": "237200",
    "end": "240239"
  },
  {
    "text": "Well, the output of this, it actually \ngoes into one of these attention heads.",
    "start": "240240",
    "end": "246080"
  },
  {
    "text": "And whenever we do that, this influences \nwhat the decoder should pay attention to.",
    "start": "246080",
    "end": "252320"
  },
  {
    "text": "But first of all, let's just talk \na bit about how this works, right?",
    "start": "252320",
    "end": "255920"
  },
  {
    "text": "We also said that we have output from the encoder.",
    "start": "256560",
    "end": "260320"
  },
  {
    "text": "Well, that output can also \nbecome input from a previous  ",
    "start": "260320",
    "end": "263920"
  },
  {
    "text": "stack of these encoders into the decoder here.",
    "start": "263920",
    "end": "268480"
  },
  {
    "text": "And as this goes up, it follows much of \nthe same process that happens over here,",
    "start": "268480",
    "end": "273040"
  },
  {
    "text": "where we then go up and we \nmight tokenize it a bit more,  ",
    "start": "273040",
    "end": "276800"
  },
  {
    "text": "we might do some more embeddings \nto get into another space.",
    "start": "276800",
    "end": "279919"
  },
  {
    "text": "And then we go into a multi-head \nattention, which tells us again,  ",
    "start": "279920",
    "end": "283200"
  },
  {
    "text": "based on how we've trained the attention heads.",
    "start": "283200",
    "end": "286000"
  },
  {
    "text": "What should we pay attention to?",
    "start": "286000",
    "end": "287680"
  },
  {
    "text": "And then we go to a secondary tension, which as \nI mentioned, this is influenced by the encoder.",
    "start": "287680",
    "end": "293840"
  },
  {
    "text": "So the encoders has another representation \nthat says, based on the decoder,  ",
    "start": "293840",
    "end": "299120"
  },
  {
    "text": "how should we pay more attention to or less \nattention to which words in a sentence?",
    "start": "299120",
    "end": "303760"
  },
  {
    "text": "Then we go up again, right?",
    "start": "303760",
    "end": "305280"
  },
  {
    "text": "And then, we do this neural network \ntransformation to cross over and find  ",
    "start": "305280",
    "end": "309840"
  },
  {
    "text": "the different relationships between \nthe hidden neurons and the stack.",
    "start": "309840",
    "end": "313199"
  },
  {
    "text": "And then have an output.",
    "start": "313200",
    "end": "314960"
  },
  {
    "text": "And the output...",
    "start": "314960",
    "end": "316000"
  },
  {
    "text": "Will eventually be decoded back into some \nother text so that we then in turn,as humans,",
    "start": "316000",
    "end": "321920"
  },
  {
    "text": "can better understand the output representation so  ",
    "start": "321920",
    "end": "325920"
  },
  {
    "text": "that maybe even another agent can \nlink up to it and then use that.",
    "start": "325920",
    "end": "330080"
  },
  {
    "text": "But again, these stacks are stacked together.",
    "start": "330080",
    "end": "332960"
  },
  {
    "text": "We have five of these that I just drew up,  ",
    "start": "332960",
    "end": "335360"
  },
  {
    "text": "but then you might have 10 of \nthose that are stacked up together.",
    "start": "335360",
    "end": "338719"
  },
  {
    "text": "And if there's a two to one kind of relationship, \nthen you may interlace these inputs.",
    "start": "338720",
    "end": "344000"
  },
  {
    "text": "Right over into this 2X stack of encoding.",
    "start": "344000",
    "end": "347040"
  },
  {
    "text": "So that's a lot of where the art comes in,  ",
    "start": "347040",
    "end": "348880"
  },
  {
    "text": "is how do we put these two \nencoders and decoders together.",
    "start": "348880",
    "end": "352960"
  },
  {
    "text": "But in essence, this really \ngives us rise to the modern  ",
    "start": "352960",
    "end": "355947"
  },
  {
    "text": "day LLM that most of us use on a daily basis.",
    "start": "355947",
    "end": "359520"
  },
  {
    "text": "So now what if these LLMs could reason at the \nconcept level instead of at the token level?",
    "start": "359520",
    "end": "364639"
  },
  {
    "text": "Well, this is where LCMs enter the field.",
    "start": "364640",
    "end": "366960"
  },
  {
    "text": "So in LCM, it's trained to predict \nthe next concept or even sentence.",
    "start": "366960",
    "end": "371280"
  },
  {
    "text": "But now concept represents a high-level idea.",
    "start": "371280",
    "end": "374320"
  },
  {
    "text": "It's agnostic to language, and \nit's aggnostic to modality,",
    "start": "374320",
    "end": "377840"
  },
  {
    "text": "but this allows this hierarchical \nreasoning within the concept space.",
    "start": "377840",
    "end": "382080"
  },
  {
    "text": "So here is the fundamental idea of how it works.",
    "start": "382080",
    "end": "385919"
  },
  {
    "text": "So first, there are sentences at the very bottom.",
    "start": "385920",
    "end": "388480"
  },
  {
    "text": "So I have sentence one all \nthe way to sentence five.",
    "start": "388480",
    "end": "391280"
  },
  {
    "text": "And these represent different types of concepts.",
    "start": "391280",
    "end": "394320"
  },
  {
    "text": "But here, this input, it goes up into an encoder,  ",
    "start": "394320",
    "end": "398160"
  },
  {
    "text": "which in this case, we're going \nto use something called SONAR.",
    "start": "398160",
    "end": "401200"
  },
  {
    "text": "Right, and then as we encode \nit into the SONAR space,  ",
    "start": "401200",
    "end": "403600"
  },
  {
    "text": "the sequence of concepts are then passed to the \nLCM to predict the next concept or sentence.",
    "start": "403600",
    "end": "409760"
  },
  {
    "text": "And then the next piece of this is \nthat the sonor embedding then is  ",
    "start": "409760",
    "end": "413520"
  },
  {
    "text": "decoded into what the actual sentence structure is",
    "start": "413520",
    "end": "417039"
  },
  {
    "text": "so that even a human or even another \nagent can better understand it.",
    "start": "417040",
    "end": "421120"
  },
  {
    "text": "And so there's even what's \ncalled a diffusion-based LCM.",
    "start": "421120",
    "end": "424479"
  },
  {
    "text": "So just like image generation.",
    "start": "424480",
    "end": "426560"
  },
  {
    "text": "The model, it slowly removes noise from  ",
    "start": "426560",
    "end": "428800"
  },
  {
    "text": "the candidate concept until it's \nfinally revealed at the very end.",
    "start": "428800",
    "end": "432479"
  },
  {
    "text": "So what this means is that the last input \nsentence, which in this case would be S5,  ",
    "start": "432480",
    "end": "437680"
  },
  {
    "text": "it's input into an LCM and \nit's denoised over time.",
    "start": "437680",
    "end": "443280"
  },
  {
    "text": "And the embedding is slowly \nrevealed right at the end.",
    "start": "443280",
    "end": "448080"
  },
  {
    "text": "And this method, it also uses \nwhat's called a two tower method.",
    "start": "448080",
    "end": "451520"
  },
  {
    "text": "Where it splits the denoiser and decoder.",
    "start": "451520",
    "end": "454160"
  },
  {
    "text": "So we have two parts, much like what the encoder, \ndecoder, or transformer architecture looks like.",
    "start": "454160",
    "end": "459920"
  },
  {
    "text": "Now these LCMs, what it gets us, right,  ",
    "start": "459920",
    "end": "462160"
  },
  {
    "text": "is it first allows us to reason in \na much different way than before.",
    "start": "462160",
    "end": "467920"
  },
  {
    "text": "So we can reason at the abstract \nlevel rather than the token level.",
    "start": "467920",
    "end": "472640"
  },
  {
    "text": "It also allows us to go to this \nhierarchical kind of structure, right?",
    "start": "472640",
    "end": "477600"
  },
  {
    "text": "That's very important so that \nthese models can better understand.",
    "start": "477600",
    "end": "481120"
  },
  {
    "text": "What's happening much like how humans do.",
    "start": "481120",
    "end": "483919"
  },
  {
    "text": "Then we can also have much \nlonger content that goes in.",
    "start": "483920",
    "end": "490160"
  },
  {
    "text": "So we can call this even like a content, \nwe can even call it a context window.",
    "start": "490160",
    "end": "496000"
  },
  {
    "text": "But this is a higher level of reasoning \nthat these sentences now have.",
    "start": "496000",
    "end": "501120"
  },
  {
    "text": "Now, the other part that it also enables us to \ndo, is what's called a zero-shot generation.",
    "start": "501120",
    "end": "508960"
  },
  {
    "text": "Right, and this helps us to get specificity  ",
    "start": "508960",
    "end": "512320"
  },
  {
    "text": "without the need for all \nthese lower level type tokens.",
    "start": "512320",
    "end": "516560"
  },
  {
    "text": "And then also what I wanted to note \ntoo is that it's modality agnostic.",
    "start": "516560",
    "end": "522880"
  },
  {
    "text": "So what I can do is I can pass in \ndifferent types of sentences or  ",
    "start": "522880",
    "end": "526400"
  },
  {
    "text": "different types of sound, images into this encoder",
    "start": "526400",
    "end": "531040"
  },
  {
    "text": "and SONAR can handle it with relative ease and \nthen the LCM can then in turn reason over it.",
    "start": "531040",
    "end": "537199"
  },
  {
    "text": "So this story is really about \nwhat's called data representation,",
    "start": "537200",
    "end": "540480"
  },
  {
    "text": "and it shows that our systems and methods are  ",
    "start": "540480",
    "end": "542880"
  },
  {
    "text": "abstracting reasoning into this \nhigher order of thought levels.",
    "start": "542880",
    "end": "546160"
  },
  {
    "text": "This will make AI much more generalizable, \nbut also useful for everyday tasks.",
    "start": "546160",
    "end": "551138"
  }
]