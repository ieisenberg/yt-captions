[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6330"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6330",
    "end": "13320"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "13320",
    "end": "18370"
  },
  {
    "text": " RUSS TEDRAKE: OK, welcome back. ",
    "start": "18370",
    "end": "27810"
  },
  {
    "text": "So last time, I tried to ease\nus into a switch in thinking.",
    "start": "27810",
    "end": "34790"
  },
  {
    "text": "From instead of trying\nto explicitly solve the Hamilton-Jacobi\nequation, I wanted",
    "start": "34790",
    "end": "40310"
  },
  {
    "text": "to try thinking about a\ndifferent class of algorithms which we called policy search. Where you explicitly\nparameterize your control",
    "start": "40310",
    "end": "47449"
  },
  {
    "text": "system with some\nparameters and then just search in the space of\nparameters for a good solution.",
    "start": "47450",
    "end": "54559"
  },
  {
    "text": "So the idea was, let's\ngo ahead and define",
    "start": "54560",
    "end": "65430"
  },
  {
    "text": "some class of control systems\nby some parameter vector.",
    "start": "65430",
    "end": "70650"
  },
  {
    "start": "70650",
    "end": "80520"
  },
  {
    "text": "And then, our optimal\ncontrol problem, which we called sort of\nminimizing J pi of x0 t.",
    "start": "80520",
    "end": "94590"
  },
  {
    "text": "We could think of as\nexplicitly minimizing over",
    "start": "94590",
    "end": "101070"
  },
  {
    "text": "that vector alpha. Something that if I shorthand\npi of alpha with just alpha",
    "start": "101070",
    "end": "108060"
  },
  {
    "text": "of x0 t.  And if you're willing to\nrestrict your thinking",
    "start": "108060",
    "end": "118950"
  },
  {
    "text": "to a single initial\ncondition, then you can really just think of it\nas, I've got some old function",
    "start": "118950",
    "end": "126929"
  },
  {
    "text": "J of alpha, and I\nwant to minimize it with respect to alpha.",
    "start": "126930",
    "end": "132120"
  },
  {
    "text": "That puts you squarely in the\nland of you can call fmin. In MATLAB, you can do sort of\nany old optimization on it.",
    "start": "132120",
    "end": "141790"
  },
  {
    "text": " So today, I said the lecture is\ncalled Trajectory Optimization.",
    "start": "141790",
    "end": "152819"
  },
  {
    "text": "I tried to point out that\nthinking about this policy search, that could\nbe a general thing. That could encompass\nfeedback policies.",
    "start": "152820",
    "end": "160245"
  },
  {
    "text": " For instance, this\ncould be parameterized",
    "start": "160245",
    "end": "168180"
  },
  {
    "text": "by if K is filled out\nwith alphas, that's OK. And I also said it could be\nopen-loop trajectories, right?",
    "start": "168180",
    "end": "178200"
  },
  {
    "text": "In general, I could just ignore\nx, and it could just be-- let me see how I\nwrote it last time.",
    "start": "178200",
    "end": "186129"
  },
  {
    "text": "It could just be, let's\nsay, that u at time t",
    "start": "186130",
    "end": "191520"
  },
  {
    "text": "is just alpha of\nn where n equals some floor of to over dt.",
    "start": "191520",
    "end": "204045"
  },
  {
    "text": "I'm not sure that's the\nbest way to write it, but that's, I think, a\nclean way to write it. ",
    "start": "204045",
    "end": "215002"
  },
  {
    "text": "So what does that mean? That means that my\ncontrol policy over time",
    "start": "215002",
    "end": "221820"
  },
  {
    "text": "is just a set of-- it's a zero-order\nhold trajectory.",
    "start": "221820",
    "end": "229230"
  },
  {
    "text": " Where each of these are dt long.",
    "start": "229230",
    "end": "236470"
  },
  {
    "text": "And this one is alpha 1, this\none is alpha 2, and so on.",
    "start": "236470",
    "end": "241900"
  },
  {
    "text": " So if I'm willing\nto make some sort of simple tape of trajectories\nthat are parameterized",
    "start": "241900",
    "end": "253450"
  },
  {
    "text": "by these alphas-- and naturally, you can do\na cleaner job of doing this with splines or whatever.",
    "start": "253450",
    "end": "259370"
  },
  {
    "text": "But let's think about the\nsimple representation. Then solving this\nmin alpha J alpha",
    "start": "259370",
    "end": "267160"
  },
  {
    "text": "is equivalent to trying to\nfind the open-loop trajectory that I'd like to follow which\nminimizes J, for instance,",
    "start": "267160",
    "end": "276009"
  },
  {
    "text": "for some initial, for a\nparticular initial condition. So that class, this sort\nof the open-loop family",
    "start": "276010",
    "end": "291610"
  },
  {
    "text": "of control policies\nis special enough that there's a lot\nof methods that",
    "start": "291610",
    "end": "296620"
  },
  {
    "text": "are highly tuned for\nthat open-loop trajectory optimization.",
    "start": "296620",
    "end": "302287"
  },
  {
    "text": "So I want to talk about\na few of them today. They're very powerful. They tend to scale to fairly\nhigh-dimensional systems.",
    "start": "302287",
    "end": "309610"
  },
  {
    "text": "And I actually think they can\nbe used as a part of a process to design good\nfeedback controllers.",
    "start": "309610",
    "end": "315700"
  },
  {
    "text": "But that's a longer story. Let me just tell you\ntoday how to solve open-loop trajectories. ",
    "start": "315700",
    "end": "326710"
  },
  {
    "text": "In the trajectory optimization\nworld, there is roughly-- well, there's lots of ideas.",
    "start": "326710",
    "end": "333160"
  },
  {
    "text": "And so many ideas,\nactually, that it's going to slip into Tuesday I decided. But let me tell you about\nthe first two of them today.",
    "start": "333160",
    "end": "340860"
  },
  {
    "text": " I want to talk about\nfirst shooting methods",
    "start": "340860",
    "end": "357170"
  },
  {
    "text": "and then direct\ncollocation methods. ",
    "start": "357170",
    "end": "369920"
  },
  {
    "text": "And we have lots of\nhalf-baked examples that were coded in the\nmiddle of the night, so to hopefully bring\nthe message through.",
    "start": "369920",
    "end": "378120"
  },
  {
    "text": " OK, so what's a shooting method?",
    "start": "378120",
    "end": "386250"
  },
  {
    "text": "You might be able to guess. Shooting methods are--\nhow many people know? How many people know what\nshooting methods are?",
    "start": "386250",
    "end": "392340"
  },
  {
    "text": "Excellent, OK. ",
    "start": "392340",
    "end": "408110"
  },
  {
    "text": "How would you characterize\na shooting method? What would you-- ",
    "start": "408110",
    "end": "414078"
  },
  {
    "text": "AUDIENCE: Can we\nintegrate the states from the book from\nthe beginning time and the [? co ?] states\nfrom the end time",
    "start": "414078",
    "end": "420860"
  },
  {
    "text": "and hope they match up in\nthe middle or something? RUSS TEDRAKE:\nPerfect, well, yeah, I",
    "start": "420860",
    "end": "426689"
  },
  {
    "text": "mean, in general, actually so\nit's even simpler than that.",
    "start": "426690",
    "end": "432150"
  },
  {
    "text": "In general, so shooting\nmethods are often the title for solving\nboundary value problems, which is what you just said.",
    "start": "432150",
    "end": "439560"
  },
  {
    "text": "The name comes from\nboundary value problems. We can use them more\ngenerally, even if it's just",
    "start": "439560",
    "end": "445800"
  },
  {
    "text": "an initial value problem. But the basic idea is\nexactly what you said.",
    "start": "445800",
    "end": "451590"
  },
  {
    "text": "Let's just simulate the\nsystem with the parameters we have and then work to\nchange our parameters,",
    "start": "451590",
    "end": "458557"
  },
  {
    "text": "shoot at a little\nbit different place.  So let's say we have t over x\nfor some very simple system.",
    "start": "458557",
    "end": "469560"
  },
  {
    "text": " And I run my controller from\ninitial conditions here,",
    "start": "469560",
    "end": "476820"
  },
  {
    "text": "and I get some trajectory with\nalpha equals 1, let's say,",
    "start": "476820",
    "end": "483030"
  },
  {
    "text": "or even alpha equals\nsome long vector. Maybe it's 1, 2, 43, 6.",
    "start": "483030",
    "end": "490470"
  },
  {
    "text": "I get that. And let's say my goal is to get\nmy final conditions to be here.",
    "start": "490470",
    "end": "498712"
  },
  {
    "text": "Then, what I'm\ngoing to do is, I'm going to change alpha\nand run it again,",
    "start": "498713",
    "end": "504360"
  },
  {
    "text": "shooting successively until I\nget to my desired final value.",
    "start": "504360",
    "end": "509604"
  },
  {
    "text": "If I change alpha, then maybe\nmy controller gets me here. And if I change it\nagain, maybe it'll get me all the way\nup to the goal.",
    "start": "509605",
    "end": "516780"
  },
  {
    "start": "516780",
    "end": "522179"
  },
  {
    "text": "I see you. AUDIENCE: What's the update? RUSS TEDRAKE: Yeah, OK,\nI'll tell you the update.",
    "start": "522179",
    "end": "529320"
  },
  {
    "text": "But the big idea\nis, I'm going to I'm going to try to solve a problem,\nfor instance, a boundary value",
    "start": "529320",
    "end": "534829"
  },
  {
    "text": "problem, by starting with some\ninitial conditions simulating and just changing\nthe parameters.",
    "start": "534830",
    "end": "540379"
  },
  {
    "text": "So you can imagine, if the thing\nyou're trying to solve is not-- I mean, a boundary value\nproblem is obviously",
    "start": "540380",
    "end": "545630"
  },
  {
    "text": "one thing we can do. But maybe you also\nhave a cost that you're trying to optimize over that.",
    "start": "545630",
    "end": "550699"
  },
  {
    "text": "Then the basic idea still holds. ",
    "start": "550700",
    "end": "556190"
  },
  {
    "text": "So I told you the first\nway to start thinking",
    "start": "556190",
    "end": "561320"
  },
  {
    "text": "about how to do that last time. ",
    "start": "561320",
    "end": "567740"
  },
  {
    "text": "We can evaluate J of\nalpha pretty easily. Let me stick with my\nsuperscript alpha notation.",
    "start": "567740",
    "end": "575910"
  },
  {
    "text": "We can evaluate that with just\nforward simulation, right? ",
    "start": "575910",
    "end": "593190"
  },
  {
    "text": "And if you want to know\nhow to change alpha, then it helps to know the\ngradients-- partial J,",
    "start": "593190",
    "end": "602160"
  },
  {
    "text": "partial alpha, evaluated\nat the current alpha. ",
    "start": "602160",
    "end": "611420"
  },
  {
    "text": "I told you one way\nto do that last time, with an adjoint method. ",
    "start": "611420",
    "end": "621740"
  },
  {
    "text": "Which I still can't resist\ncalling back prop through time, because that's--",
    "start": "621740",
    "end": "627320"
  },
  {
    "text": "I learned it first as\na neural networks guy. ",
    "start": "627320",
    "end": "643200"
  },
  {
    "text": "And there's a second way to\ndo it, which in some cases, is not less efficient.",
    "start": "643200",
    "end": "649770"
  },
  {
    "text": "But it's certainly easier to\nderive and easier to code. So I want to do that\nto make sure we have",
    "start": "649770",
    "end": "655050"
  },
  {
    "text": "the intuition about it too. It's also useful. Again, this is the neural\nnetwork name for it.",
    "start": "655050",
    "end": "660910"
  },
  {
    "text": "There's probably a good name\nfrom more standard optimization theory. But in neural networks, people\ncall it real-time recurrent",
    "start": "660910",
    "end": "668400"
  },
  {
    "text": "learning, which is RTRL.",
    "start": "668400",
    "end": "678510"
  },
  {
    "text": "And this is BPTT. ",
    "start": "678510",
    "end": "686100"
  },
  {
    "text": "So how do we compute J of alpha? ",
    "start": "686100",
    "end": "692400"
  },
  {
    "text": "The adjoint method, I told\nyou that if you simulate the system forward\nin time, you get",
    "start": "692400",
    "end": "698640"
  },
  {
    "text": "the sort of forward equation. You figure out J. If you then\nsimulate the adjoint equation",
    "start": "698640",
    "end": "704400"
  },
  {
    "text": "backwards in time,\nI called it y. y dot is some function\ngoing backwards.",
    "start": "704400",
    "end": "709660"
  },
  {
    "text": "You can interpret y as being\nthe sensitivity of changing--",
    "start": "709660",
    "end": "715199"
  },
  {
    "text": "so let me just write down\nthe form of it quickly. So the adjoint,\nremember, was x dot",
    "start": "715200",
    "end": "724830"
  },
  {
    "text": "equals f of x going forward. ",
    "start": "724830",
    "end": "743037"
  },
  {
    "text": "Did I actually right\ndown the right equation for you going backwards? ",
    "start": "743038",
    "end": "750300"
  },
  {
    "text": "And then, negative y\ndot is f of x y minus G",
    "start": "750300",
    "end": "761279"
  },
  {
    "text": "of x T. Where those are the big\ngradient matrices I wrote down last time, going backwards.",
    "start": "761280",
    "end": "766920"
  },
  {
    "start": "766920",
    "end": "775250"
  },
  {
    "text": "And then, the update gives you,\nthat partial J partial alpha",
    "start": "775250",
    "end": "781460"
  },
  {
    "text": "is just a simple integral now. ",
    "start": "781460",
    "end": "796360"
  },
  {
    "text": "So this backwards\nequation, which happens to be the\nadjoint equation we saw in Pontryagin's\nminimum principle. y has",
    "start": "796360",
    "end": "802750"
  },
  {
    "text": "an interpretation\nas the sensitivity of the cost on changing x-- y\nat some time T, y at time 3.",
    "start": "802750",
    "end": "812140"
  },
  {
    "text": "It's the same size, the\nsame dimension as x-- this y variable. It's a column\nvector just like x.",
    "start": "812140",
    "end": "819279"
  },
  {
    "text": "It has the interpretation,\nit's the sensitivity of J on changing x at that time.",
    "start": "819280",
    "end": "825500"
  },
  {
    "text": "So you compute forward, then you\ncompute back the gradient of J with respect to x of t. And then knowing\nthat, you can simply",
    "start": "825500",
    "end": "831970"
  },
  {
    "text": "compute the full gradient\nwith respect to the alpha.",
    "start": "831970",
    "end": "838282"
  },
  {
    "text": "And that took a little\nbit of derivation through the Lagrange\nmultipliers. RTRL is actually even simpler.",
    "start": "838283",
    "end": "844899"
  },
  {
    "text": " Is it OK if that wall\ndisappears for a minute?",
    "start": "844900",
    "end": "851950"
  },
  {
    "text": "We wrote that last time, right? I'm just going to go\nto my third wall here. ",
    "start": "851950",
    "end": "863845"
  },
  {
    "text": "This time, I'm only\ngoing to simulate forward in time, which is\nwhy it's called real-time recurrent learning.",
    "start": "863845",
    "end": "869080"
  },
  {
    "text": "Because some people don't like\nhaving to simulate forward to capital T and then\nsimulate all the way",
    "start": "869080",
    "end": "875259"
  },
  {
    "text": "back to make an update. That's sort of, we have to go\nall the way to the end of time",
    "start": "875260",
    "end": "880750"
  },
  {
    "text": "in order to make your\nupdate at time 2. It's more appealing if you\ncan make your update at time 2 by just thinking\nabout time 0 to 2.",
    "start": "880750",
    "end": "887320"
  },
  {
    "text": "So you can do it in\na forward pass only. The name came from maybe that,\nI think, at one point in time,",
    "start": "887320",
    "end": "896009"
  },
  {
    "text": "someone maybe thought maybe\nthis is what the brain is doing. Because people thought\nthe brain can't be doing these backward\npasses efficiently.",
    "start": "896010",
    "end": "902878"
  },
  {
    "text": "So maybe real-time\nrecurring learning is what the brain is doing. But I don't think people\nreally think that anymore.",
    "start": "902878",
    "end": "908590"
  },
  {
    "text": "There was one paper\nthat thought that, which is a nice paper, but-- So let's do, let me\njust show you RTRL,",
    "start": "908590",
    "end": "914290"
  },
  {
    "text": "because it turns out to be maybe\neven the more simple thing. So I have J of alpha\nstarting from x0,",
    "start": "914290",
    "end": "926330"
  },
  {
    "text": "0 is the integral from T.\nOops, I did that again.",
    "start": "926330",
    "end": "939410"
  },
  {
    "text": " Let me come up with a\nnew working variable.",
    "start": "939410",
    "end": "947009"
  },
  {
    "text": "The previous one y was\nuseful coming backwards. If we're going to go forward,\nlet me define a matrix, P,",
    "start": "947010",
    "end": "955139"
  },
  {
    "text": "where the ij-th element is a\npartial xi, partial alpha i.",
    "start": "955140",
    "end": "961260"
  },
  {
    "start": "961260",
    "end": "968820"
  },
  {
    "text": "If I have that and I\nwant to take gradients with respect to this, then I can\ndo it pretty easily actually.",
    "start": "968820",
    "end": "973940"
  },
  {
    "text": "If I want to do partial\nJ partial alpha,",
    "start": "973940",
    "end": "979410"
  },
  {
    "text": "I can go inside the integral. It's just dt. I'm just using my\nchain rule here.",
    "start": "979410",
    "end": "984959"
  },
  {
    "text": "I get partial G, partial x,\npartial x, partial alpha.",
    "start": "984960",
    "end": "991940"
  },
  {
    "text": "These are x at some time\nT, plus partial G partial",
    "start": "991940",
    "end": "997590"
  },
  {
    "text": "U at time T, partial\npi, partial alpha.",
    "start": "997590",
    "end": "1005510"
  },
  {
    "text": " And in general, if I have a\nfeedback term in my policy,",
    "start": "1005510",
    "end": "1012900"
  },
  {
    "text": "I also got to worry\nabout partial G, partial U, partial pi, partial\nx, partial x, partial alpha.",
    "start": "1012900",
    "end": "1022770"
  },
  {
    "text": " It's just a chain rule\nderivative of this.",
    "start": "1022770",
    "end": "1028198"
  },
  {
    "text": "Do you agree?  It turns out, using\nthe matrices we",
    "start": "1028198",
    "end": "1036539"
  },
  {
    "text": "had used before, I can write\nthat very simply as integral from 0 to T of Gx, the big\nderivative of x times P,",
    "start": "1036540",
    "end": "1047459"
  },
  {
    "text": "this matrix here, plus the\nbig derivative of G on alpha.",
    "start": "1047460",
    "end": "1053309"
  },
  {
    "start": "1053310",
    "end": "1059660"
  },
  {
    "text": "If we know partial x of t\npartial alpha, then it's easy.",
    "start": "1059660",
    "end": "1067910"
  },
  {
    "text": "So how do we get partial\nx T partial alpha?",
    "start": "1067910",
    "end": "1073700"
  },
  {
    "text": "Well, that's easy too. ",
    "start": "1073700",
    "end": "1087919"
  },
  {
    "text": "Let's look at the\nforward equation here, xu which is, in this\ncase, pi alpha x of T.",
    "start": "1087920",
    "end": "1098790"
  },
  {
    "text": "So let's look at how\nthis changes with alpha. Let's take the derivative\nwith respect to alpha.",
    "start": "1098790",
    "end": "1104370"
  },
  {
    "text": "I'll write it even\nmore cleanly-- ",
    "start": "1104370",
    "end": "1117179"
  },
  {
    "text": "partial f, partial\nx plus partial f,",
    "start": "1117180",
    "end": "1123750"
  },
  {
    "text": "partial U, partial pi, partial\nx times P, which is partial",
    "start": "1123750",
    "end": "1131970"
  },
  {
    "text": "x, partial alpha. Let me write it as-- ",
    "start": "1131970",
    "end": "1153810"
  },
  {
    "text": "Everybody agree with\nthat chain rule? No? OK, ask me.",
    "start": "1153810",
    "end": "1159149"
  },
  {
    "text": "Yeah. AUDIENCE: So there you\nhave G with respect to x and then P\n[INAUDIBLE] And then",
    "start": "1159150",
    "end": "1165663"
  },
  {
    "text": "we have G of alpha which\nwould be the second variable. What happened in the third one?",
    "start": "1165663",
    "end": "1171820"
  },
  {
    "text": "RUSS TEDRAKE: G of x is\nactually partial G partial x plus partial U-- G partial U, partial\npi, partial x.",
    "start": "1171820",
    "end": "1179476"
  },
  {
    "text": "AUDIENCE: Oh, I see. RUSS TEDRAKE: That's the big\ngradient with respect to x. AUDIENCE: And that I on the\nbottom is J. Is that correct?",
    "start": "1179476",
    "end": "1187421"
  },
  {
    "text": "[INAUDIBLE] RUSS TEDRAKE: This\nis now the matrix P. AUDIENCE: [INAUDIBLE]",
    "start": "1187421",
    "end": "1193373"
  },
  {
    "text": "RUSS TEDRAKE: Oh, thank you. That should be a\nJ on the bottom. Good, thank you.",
    "start": "1193373",
    "end": "1198481"
  },
  {
    "text": "Please do catch me\non those things. ",
    "start": "1198482",
    "end": "1205149"
  },
  {
    "text": "Yeah, thank you. So then, are we happy with this? ",
    "start": "1205150",
    "end": "1214190"
  },
  {
    "text": "So this is pretty simple too. Just now, I get an\nequation forward in the gradients P\ndot is my big f of x.",
    "start": "1214190",
    "end": "1222720"
  },
  {
    "text": "Which I get is the direct\nand the indirect gradient with respect to x times P plus\nthe gradient of F with respect",
    "start": "1222720",
    "end": "1231840"
  },
  {
    "text": "to alpha. ",
    "start": "1231840",
    "end": "1245610"
  },
  {
    "text": "And that's it.  I'm almost waiting\nfor more to do, right?",
    "start": "1245610",
    "end": "1251919"
  },
  {
    "text": "So if you're willing to go-- if you want to go\nforward in time, then",
    "start": "1251920",
    "end": "1258670"
  },
  {
    "text": "if you're willing to keep around\nthis extra term, partial x",
    "start": "1258670",
    "end": "1266020"
  },
  {
    "text": "partial alpha, then as\nyou move forward in time, you can build up your gradient\nof partial J partial alpha.",
    "start": "1266020",
    "end": "1276640"
  },
  {
    "text": "And by the time you\nget to the end of time, you don't have to\ngo backwards again. You know what the\ntotal derivative is.",
    "start": "1276640",
    "end": "1281890"
  },
  {
    "start": "1281890",
    "end": "1287360"
  },
  {
    "text": "So why would you use this\nversus the other method? Does everybody agree with that?",
    "start": "1287360",
    "end": "1293330"
  },
  {
    "text": "I don't see big smiles, but\nthis is satisfying and simple--",
    "start": "1293330",
    "end": "1298440"
  },
  {
    "text": "smile.  The cost of this is\ncarrying around this matrix.",
    "start": "1298440",
    "end": "1305505"
  },
  {
    "text": " So potentially,\nthat could be big.",
    "start": "1305505",
    "end": "1311090"
  },
  {
    "text": "If you have a lot of\nparameters, let's say I have 100 parameters\nin my control system, or 10,000 parameters\nin my control system,",
    "start": "1311090",
    "end": "1317913"
  },
  {
    "text": "then you're actually integrating\nforward a matrix equation that could be pretty big. It's something that's x--",
    "start": "1317913",
    "end": "1323060"
  },
  {
    "text": "the dimension of x by\nthe number of parameters. So that's one-- that's really\nthe only problem with it.",
    "start": "1323060",
    "end": "1333240"
  },
  {
    "text": "The back prop\nthrough time is only",
    "start": "1333240",
    "end": "1339620"
  },
  {
    "text": "carrying around this y,\nwhich is the size of x. But to do this sort of nice\nforward-backward update,",
    "start": "1339620",
    "end": "1347120"
  },
  {
    "text": "you'd better be able to\nremember the trajectory that x has taken over time.",
    "start": "1347120",
    "end": "1354510"
  },
  {
    "text": "So for very long\ntrajectories, it might not be much more\nefficient to do this.",
    "start": "1354510",
    "end": "1360213"
  },
  {
    "text": "It's just a trade-off. Some problems are actually\nquite nicely done in RTRL. Some are nicely done with\nback prop through time.",
    "start": "1360213",
    "end": "1367180"
  },
  {
    "text": "The back prop through\ntime is, the reason it's so beautiful\nand clean is that-- ",
    "start": "1367180",
    "end": "1373700"
  },
  {
    "text": "so remembering your goal here is\nto compute a vector, partial J,",
    "start": "1373700",
    "end": "1380039"
  },
  {
    "text": "which J is a scalar with respect\nto a vector of parameters.",
    "start": "1380040",
    "end": "1385340"
  },
  {
    "text": "Here you have to carry around\na matrix to do that, partial x forward. The back prop through time\ntakes advantage of the fact",
    "start": "1385340",
    "end": "1392570"
  },
  {
    "text": "that at the end of\ntime, everything collapses to a scalar again. And that's why it only has\nto sort of carry backwards.",
    "start": "1392570",
    "end": "1399253"
  },
  {
    "text": "If you willing to go all\nthe way to the end of time, you can remember only the\neffect of that scalar value, j,",
    "start": "1399253",
    "end": "1406130"
  },
  {
    "text": "with respect to the x's. So that's why you're allowed\nto carry around less of this.",
    "start": "1406130",
    "end": "1413372"
  },
  {
    "text": "But it involves going\nforward and backwards. ",
    "start": "1413372",
    "end": "1419260"
  },
  {
    "text": "Is that intuitive? Yeah? ",
    "start": "1419260",
    "end": "1424639"
  },
  {
    "text": "What can I say more? ",
    "start": "1424640",
    "end": "1434708"
  },
  {
    "text": "There's various\nother reasons why you might prefer one or the other. So for instance, let's\nsay you have a final--",
    "start": "1434708",
    "end": "1441230"
  },
  {
    "text": "a boundary condition. Let's say you want to\nsolve this constraint,",
    "start": "1441230",
    "end": "1448620"
  },
  {
    "text": "minimize this function\nsubject to the constraint that x of capital T is my goal.",
    "start": "1448620",
    "end": "1453740"
  },
  {
    "text": " You can write that constraint\ninto either of them.",
    "start": "1453740",
    "end": "1459919"
  },
  {
    "text": "I find it more natural to\nwrite it into this RTRL form. Just because you know explicitly\npartial x, partial alpha.",
    "start": "1459920",
    "end": "1470120"
  },
  {
    "text": "So if you want to\ncompute at the end your constraint derivatives,\nsaying what should I have done? How should I have\nchanged alpha in order",
    "start": "1470120",
    "end": "1475340"
  },
  {
    "text": "to enforce that constraint? Then it's actually a\nsimple function of P.",
    "start": "1475340",
    "end": "1481220"
  },
  {
    "text": "So maybe I'm not giving\nyou a silver bullet by telling you one\nway or the other. But depending on exactly the\nway you want to use them,",
    "start": "1481220",
    "end": "1487700"
  },
  {
    "text": "sometimes one is more\nuseful than the other. So in the simulations\nI'm about to show you, I'm going to actually RTRL,\njust because it's also,",
    "start": "1487700",
    "end": "1494120"
  },
  {
    "text": "this is trivial to code. ",
    "start": "1494120",
    "end": "1501809"
  },
  {
    "text": "But the big idea here\nis really just that I'm",
    "start": "1501810",
    "end": "1506820"
  },
  {
    "text": "doing a shooting method. I'm simulating\nthis thing forward. And I'm trying to compute\nwhat's the change.",
    "start": "1506820",
    "end": "1511920"
  },
  {
    "text": "And that's my scalar\ncost with respect to a change in my parameters. So that I can then\nupdate my parameters.",
    "start": "1511920",
    "end": "1518970"
  },
  {
    "text": "And last time, we talked about\nmultiple ways to do that. You might do that with a simple\ngradient descent algorithm.",
    "start": "1518970",
    "end": "1524309"
  },
  {
    "start": "1524310",
    "end": "1530010"
  },
  {
    "text": "You could do gradient descent. ",
    "start": "1530010",
    "end": "1538740"
  },
  {
    "text": "You could say that\nalpha at the n plus 1 step is just alpha n minus\nsome learning rate, eta,",
    "start": "1538740",
    "end": "1548309"
  },
  {
    "text": "times partial J partial alpha. ",
    "start": "1548310",
    "end": "1559020"
  },
  {
    "text": "And I argued last time\nthat you can do better than that by sequential\nquadratic programming.",
    "start": "1559020",
    "end": "1568890"
  },
  {
    "text": "To the point where SQP methods\nare so robust that you just find something like SNOPT, you\ndownload it, and you use it.",
    "start": "1568890",
    "end": "1578790"
  },
  {
    "text": "And I'll try to convince you\nas we run some of these things that apart from installing\na package, which",
    "start": "1578790",
    "end": "1585420"
  },
  {
    "text": "takes a few minutes,\nonce you do it, you'll never go back to this.",
    "start": "1585420",
    "end": "1590460"
  },
  {
    "text": "I'll tell you one reason\nright off the bat. Choosing the learning rate\nis a pain in the butt.",
    "start": "1590460",
    "end": "1596279"
  },
  {
    "text": "You never have to do it again. So? ",
    "start": "1596280",
    "end": "1604590"
  },
  {
    "text": "Let's see some examples. Let me do the\npendulum with SNOPT.",
    "start": "1604590",
    "end": "1611070"
  },
  {
    "text": "So using SNOPT is just,\nall you do is you tell it--",
    "start": "1611070",
    "end": "1617730"
  },
  {
    "text": "you give it a function\nwhich can compute J and J, the derivatives of J. It's\nSNOPT stood for sparse nonlinear",
    "start": "1617730",
    "end": "1627030"
  },
  {
    "text": "optimal-- or optimization. And in a lot of\nproblems, many elements",
    "start": "1627030",
    "end": "1635820"
  },
  {
    "text": "of this vector, partial\nJ partial alpha, are 0. We're going to see that in our\ndirect collocation methods.",
    "start": "1635820",
    "end": "1643980"
  },
  {
    "text": "A lot of those gradients are 0. It happens in the\none I just told you. This is typically not--",
    "start": "1643980",
    "end": "1650070"
  },
  {
    "text": "typically all of those\nelements of that vector are non-zero, which\nmeans I'm not actually",
    "start": "1650070",
    "end": "1656190"
  },
  {
    "text": "getting an advantage\nby using the sparse. NPSOL is the non-sparse version. But I don't think it's\nmuch worse, if any worse.",
    "start": "1656190",
    "end": "1664038"
  },
  {
    "text": "Just for those of\nyou that are trying to figure out which\npackage you want to convince your PI to buy.",
    "start": "1664038",
    "end": "1669269"
  },
  {
    "text": "I think SNOPT is\nnormally pretty good. ",
    "start": "1669270",
    "end": "1674760"
  },
  {
    "text": "And there's a student\nversion that we can have you download\nthat'll do small problems. ",
    "start": "1674760",
    "end": "1682530"
  },
  {
    "text": "So let's do the pendulum. My cost function here,\nis just an LQR cost.",
    "start": "1682530",
    "end": "1706169"
  },
  {
    "text": "I have Q as a-- I have J as the integral\nfrom 0 to capital T",
    "start": "1706170",
    "end": "1714120"
  },
  {
    "text": "of x minus the x goal.",
    "start": "1714120",
    "end": "1719580"
  },
  {
    "text": "I had to be careful\nabout wrapping. But transpose Q x minus x\ngoal plus u transpose Ru.",
    "start": "1719580",
    "end": "1730169"
  },
  {
    "text": "The whole thing, dt, right? ",
    "start": "1730170",
    "end": "1735480"
  },
  {
    "text": "And I actually also have a final\nvalue cost where Q is 10, 1.",
    "start": "1735480",
    "end": "1760200"
  },
  {
    "text": "Qf is 100 times\nthat or something.",
    "start": "1760200",
    "end": "1765346"
  },
  {
    "text": "Q and R is 100.",
    "start": "1765346",
    "end": "1778465"
  },
  {
    "text": "So it's going to reward the\nthing for getting to the top.  Let's see what it does.",
    "start": "1778465",
    "end": "1784050"
  },
  {
    "text": "I'm going to plot the\ntrajectory for every step of the optimization.",
    "start": "1784050",
    "end": "1790290"
  },
  {
    "text": "So it's not quite doing\nsimple gradient descent. It's now going to do\na sequential quadratic programming update where it\nestimates the quadratic bowl",
    "start": "1790290",
    "end": "1797910"
  },
  {
    "text": "and tries to jump right to\nthe minimum of the bowl. So it's going to be a little\nbit more jumpy, as you see it,",
    "start": "1797910",
    "end": "1803020"
  },
  {
    "text": "but it's fast. ",
    "start": "1803020",
    "end": "1809400"
  },
  {
    "text": "It's finding the last\noptimization and then boop, right to the top. ",
    "start": "1809400",
    "end": "1817323"
  },
  {
    "text": "AUDIENCE: I have a question. So how is this\ntrajectory parameterized? Is it just openly. RUSS TEDRAKE: It's exactly\nthis, the floor of T over dt.",
    "start": "1817323",
    "end": "1827340"
  },
  {
    "text": "My sloppy notation was\nbecause it's MATLAB notation. AUDIENCE: So when\nthey say [INAUDIBLE]..",
    "start": "1827340",
    "end": "1836160"
  },
  {
    "text": "RUSS TEDRAKE: That's\nthe open-loop policy, which had U at time T,\nwas just, I do a floor,",
    "start": "1836160",
    "end": "1845820"
  },
  {
    "text": "and I go to the\nn-th index in alpha. So alpha 1 is my control\naction from 0 to dt.",
    "start": "1845820",
    "end": "1855783"
  },
  {
    "text": "AUDIENCE: So how many\nparameters is this case? [INAUDIBLE] RUSS TEDRAKE: Good. So I did two seconds\ncovered by 40 bins.",
    "start": "1855783",
    "end": "1870250"
  },
  {
    "text": " I bet if I do 20 bins, it's OK. It may be a little\nless faithful to the--",
    "start": "1870250",
    "end": "1878919"
  },
  {
    "text": "less smooth. But it works out.",
    "start": "1878920",
    "end": "1884100"
  },
  {
    "start": "1884100",
    "end": "1890340"
  },
  {
    "text": "So please realize that\nlike 60% of that time was just drawing those plots\nto the screen, easily, right?",
    "start": "1890340",
    "end": "1897870"
  },
  {
    "text": " Especially when it's reshaping\nthe screen and stuff like that.",
    "start": "1897870",
    "end": "1903100"
  },
  {
    "text": "That's wasted time. So hey, we could do the same\nthing on the [? karpal. ?]",
    "start": "1903100",
    "end": "1909507"
  },
  {
    "text": "AUDIENCE: Can we do this\nwithout [INAUDIBLE]?? RUSS TEDRAKE: Good question. Yes.",
    "start": "1909508",
    "end": "1915150"
  },
  {
    "text": "So yes, it's quite simple. In direct collocation,\nit's really simple.",
    "start": "1915150",
    "end": "1920970"
  },
  {
    "text": "So actually, my code does\nit for direct collocation and doesn't do it\nfor the gradient.",
    "start": "1920970",
    "end": "1926462"
  },
  {
    "text": "But it's actually quite fine. You just have to have T as\none of your parameters tucked into alpha and be\nable to compute",
    "start": "1926462",
    "end": "1932130"
  },
  {
    "text": "partial J, partial T, which\nis not very hard actually.",
    "start": "1932130",
    "end": "1939690"
  },
  {
    "text": "You just have to figure out\nhow this function changes when you take a derivative with\nrespect to T. And it's just,",
    "start": "1939690",
    "end": "1946529"
  },
  {
    "text": "it's like an x dot\ntimes the quantity J at T. It's not too bad.",
    "start": "1946530",
    "end": "1952419"
  },
  {
    "text": "If you can take that gradient,\nyou can optimize with respect to it. So what about for the\n[? card pull? ?] Well, oh I",
    "start": "1952420",
    "end": "1965728"
  },
  {
    "text": "forgot, I took off the zooming. So I gave it a fixed axis. ",
    "start": "1965728",
    "end": "1978850"
  },
  {
    "text": "It's going to make\na liar out of me. This is the slowest I've seen. ",
    "start": "1978850",
    "end": "1993770"
  },
  {
    "text": "There it is. ",
    "start": "1993770",
    "end": "2015060"
  },
  {
    "text": "Let me do that again. It's certainly more\nimpressive than that. ",
    "start": "2015060",
    "end": "2026320"
  },
  {
    "text": "It's starting from random\ninitial tapes, by the way. And every time I've run it\nfor the [? card pull, ?]",
    "start": "2026320",
    "end": "2031645"
  },
  {
    "text": "it comes up with\nthe same solution. ",
    "start": "2031645",
    "end": "2040020"
  },
  {
    "text": "Come on. ",
    "start": "2040020",
    "end": "2046290"
  },
  {
    "text": "There we go.  All right, not quite\nas impressive as it",
    "start": "2046290",
    "end": "2053319"
  },
  {
    "text": "was in the lab, but\nthat's still pretty good. If I turn off drawing,\nit bet it's a lot faster.",
    "start": "2053320",
    "end": "2059849"
  },
  {
    "start": "2059850",
    "end": "2077440"
  },
  {
    "text": "So I turned off\ndrawing, there you go.",
    "start": "2077440",
    "end": "2089155"
  },
  {
    "text": "I think my computer is\nslower on battery power too. That's probably--\nI'm disappointed.",
    "start": "2089155",
    "end": "2095329"
  },
  {
    "text": "Oh well, it's still pretty fast. ",
    "start": "2095329",
    "end": "2102890"
  },
  {
    "text": "AUDIENCE: So I was\njust wondering, if you simulated longer,\nwould it stay at the top, or would it fall in?",
    "start": "2102890",
    "end": "2109187"
  },
  {
    "text": " RUSS TEDRAKE: So I actually\nput a final value constraint in",
    "start": "2109187",
    "end": "2115940"
  },
  {
    "text": "on that. So it actually gets to 0, 0. ",
    "start": "2115940",
    "end": "2121010"
  },
  {
    "text": "So because I'm simulating, it\nprobably would stay up, right?",
    "start": "2121010",
    "end": "2126872"
  },
  {
    "text": "But I think the\nnatural thing to do would be to draw\nan LQR controller in at the top for instance.",
    "start": "2126872",
    "end": "2132958"
  },
  {
    "text": "And in fact, we're\ngoing to talk on Tuesday about how to LQR stabilize\nthat whole trajectory. Because for the most part,\nI think open-loop is just",
    "start": "2132958",
    "end": "2139040"
  },
  {
    "text": "the first piece of the-- just is one of the tools. ",
    "start": "2139040",
    "end": "2148260"
  },
  {
    "text": "Good. So what happens if we did-- ",
    "start": "2148260",
    "end": "2154395"
  },
  {
    "text": "you can do the acrobot too. I'll do the acrobot\nin a second here. But I also have the sort of\nsimple gradient [? descent ?]",
    "start": "2154395",
    "end": "2162470"
  },
  {
    "text": "version in here. That if I just did\nmy alpha equals",
    "start": "2162470",
    "end": "2169730"
  },
  {
    "text": "negative eta times dJ d alpha in\nthere, let's see how that does. ",
    "start": "2169730",
    "end": "2185740"
  },
  {
    "text": "Was that faster? Let's do it. Let me try that again. ",
    "start": "2185740",
    "end": "2194410"
  },
  {
    "text": "That's not running. I didn't save it.  I knew that was too\ngood to be true.",
    "start": "2194410",
    "end": "2200430"
  },
  {
    "text": " OK, my ears are too big.",
    "start": "2200430",
    "end": "2205538"
  },
  {
    "text": "I probably have to\nchange my learning rate. Thereby confirming\nmy complaint that--",
    "start": "2205538",
    "end": "2211080"
  },
  {
    "start": "2211080",
    "end": "2217390"
  },
  {
    "text": "OK, so it works.",
    "start": "2217390",
    "end": "2222700"
  },
  {
    "text": "But never do it again,\nyou don't need to. Just download SNOPT.",
    "start": "2222700",
    "end": "2228020"
  },
  {
    "text": "It'll get there eventually. You can see the\nerrors going down. And if I set my\nlearning rate properly,",
    "start": "2228020",
    "end": "2236350"
  },
  {
    "text": "it'll go down pretty fast. But not fast enough\nfor me to be patient. ",
    "start": "2236350",
    "end": "2243490"
  },
  {
    "text": "And then, here's the\nSNOPT version again.",
    "start": "2243490",
    "end": "2249930"
  },
  {
    "text": " Now the pendulum is very fast.",
    "start": "2249930",
    "end": "2256510"
  },
  {
    "text": "Good.  Let me do direct collocation\nbefore we get too mystified",
    "start": "2256510",
    "end": "2266760"
  },
  {
    "text": "by the simulations.  There's another idea. ",
    "start": "2266760",
    "end": "2275790"
  },
  {
    "text": "So shooting methods\nare certainly subject to local minima.",
    "start": "2275790",
    "end": "2280849"
  },
  {
    "text": "And I've got an example that\nI'll show you in a few minutes that I hope will make\nthat clear why they",
    "start": "2280850",
    "end": "2289580"
  },
  {
    "text": "can be subject to local minima.  Something that people\ndo, they do sometimes",
    "start": "2289580",
    "end": "2296480"
  },
  {
    "text": "multiple shooting methods. Sometimes it's sort of, there's\neven numerical sensitivity sometimes, sort of integrating\nthis thing for such",
    "start": "2296480",
    "end": "2303200"
  },
  {
    "text": "a long time. So a lot of times, people\nwill define some breakpoint in the middle of there,\nsome artificial breakpoint",
    "start": "2303200",
    "end": "2310010"
  },
  {
    "text": "in the middle of\ntheir trajectory. And say, I'm going\nto optimize, first",
    "start": "2310010",
    "end": "2315170"
  },
  {
    "text": "try to get me to this point. And then I'll say if I simulated\nfrom some other point to here",
    "start": "2315170",
    "end": "2322250"
  },
  {
    "text": "and then use as a constraint\nin their optimization to try to make that residual\ngo to 0, if that makes sense.",
    "start": "2322250",
    "end": "2329815"
  },
  {
    "text": "I'm just not going to talk\nabout multiple shooting. But just know that there's a\nversion that people use often,",
    "start": "2329815",
    "end": "2337442"
  },
  {
    "text": "which are the multiple\nshooting methods. ",
    "start": "2337443",
    "end": "2345133"
  },
  {
    "text": "To some extent,\ndirect collocation is maybe the extreme of that. ",
    "start": "2345133",
    "end": "2352563"
  },
  {
    "text": "I told you there's a\nlot of good reasons to use SNOPT, or some SQP.",
    "start": "2352563",
    "end": "2357619"
  },
  {
    "text": "So first of all, why use SNOPT?",
    "start": "2357620",
    "end": "2365610"
  },
  {
    "text": " No learning rate tweaking,\nthat's a big one for me.",
    "start": "2365610",
    "end": "2372800"
  },
  {
    "text": " It's often faster convergence.",
    "start": "2372800",
    "end": "2378950"
  },
  {
    "start": "2378950",
    "end": "2386510"
  },
  {
    "text": "Because you are doing big steps. You can sometimes jump\nover small local minima.",
    "start": "2386510",
    "end": "2395930"
  },
  {
    "start": "2395930",
    "end": "2402710"
  },
  {
    "text": "But there's a big one in there\nthat's not on the list yet. What's the big one\nI'm going to say?",
    "start": "2402710",
    "end": "2410240"
  },
  {
    "text": " What's perhaps the best\nreason, I think, to use SNOPT?",
    "start": "2410240",
    "end": "2417943"
  },
  {
    "text": "AUDIENCE: Fewer constraints? RUSS TEDRAKE: Good. It's easy to add constraints. ",
    "start": "2417943",
    "end": "2429022"
  },
  {
    "text": "Because the way these\nsequential programs are solved, there with these interior point\nmethods and things like this. And they're very efficient\nat handling constraints.",
    "start": "2429022",
    "end": "2437790"
  },
  {
    "text": "So in my pendulum swing up on\nthe simple gradient descent, I didn't actually have\na final constraint",
    "start": "2437790",
    "end": "2444510"
  },
  {
    "text": "on getting to the top\nin the SNOPT version. It's just trivial to add that.",
    "start": "2444510",
    "end": "2450390"
  },
  {
    "text": "I can put bounds on my\nactions very easily. I could say, do\ngradient descent, but never let U at time\nT be bigger than 5.",
    "start": "2450390",
    "end": "2460230"
  },
  {
    "text": "I can even put constraints on\nthe trajectory if I wanted to. ",
    "start": "2460230",
    "end": "2467369"
  },
  {
    "text": "So because of the power\nof nonlinear optimization to handle constraints, people\ncame up with a different way",
    "start": "2467370",
    "end": "2475500"
  },
  {
    "text": "to hand the optimal control\nproblem to an SQP method",
    "start": "2475500",
    "end": "2483930"
  },
  {
    "text": "that exploits those\nconstraint-solving abilities a little bit more explicitly.",
    "start": "2483930",
    "end": "2489240"
  },
  {
    "text": "And that's the direct\ncollocation methods. AUDIENCE: Can I ask a\ncouple of questions?",
    "start": "2489240",
    "end": "2494849"
  },
  {
    "text": "When we were talking\nabout this previous case that you showed, isn't\nit just providing the [INAUDIBLE] function,\nproviding the Q's and R's is",
    "start": "2494850",
    "end": "2503940"
  },
  {
    "text": "sufficient to actually\nget [INAUDIBLE].. So if we add an R\nconstraint on top of it,",
    "start": "2503940",
    "end": "2509250"
  },
  {
    "text": "it would be [INAUDIBLE]\non top and sort of like a 2 with\nrespect to our method?",
    "start": "2509250",
    "end": "2516119"
  },
  {
    "text": "Because the goal\nwe have essentially is to maximize or\nminimize the [INAUDIBLE]",
    "start": "2516120",
    "end": "2522690"
  },
  {
    "text": "over the trajectory. We can put some content, like\nit would be more information.",
    "start": "2522690",
    "end": "2527970"
  },
  {
    "text": "Like for example, I wanted to\nreach this state or that state for sure. We [INAUDIBLE].",
    "start": "2527970",
    "end": "2533290"
  },
  {
    "text": " RUSS TEDRAKE: So I think\nthat's a very RL way",
    "start": "2533290",
    "end": "2538350"
  },
  {
    "text": "to think about it-- not cheating. I mean, cheat, cheating is good. If you can hand more information\nto your algorithm, do it.",
    "start": "2538350",
    "end": "2545160"
  },
  {
    "text": "Don't worry about cheating. But no, I agree. So the question was, is it\nfair to give it a final value constraint, or am I comparing\napples and oranges, roughly,",
    "start": "2545160",
    "end": "2552450"
  },
  {
    "text": "right? Like if I say one is not using\nthe final value constraint and the other one is. So in my opinion, the goal is\nactually to get there at time",
    "start": "2552450",
    "end": "2561540"
  },
  {
    "text": "T. The optimal control\nprogram I'd like to solve is minimize some, even\nminimizing just u transpose Ru",
    "start": "2561540",
    "end": "2574440"
  },
  {
    "text": "dt subject to x of t is my goal.",
    "start": "2574440",
    "end": "2580260"
  },
  {
    "text": "That might be my favorite\nway to write it down. And then, just the\nquestion is, what methods",
    "start": "2580260",
    "end": "2585570"
  },
  {
    "text": "can I use to solve that? So the opposite\nview of the world",
    "start": "2585570",
    "end": "2591150"
  },
  {
    "text": "here maybe is that because\na lot of the methods don't handle these\nconstraints explicitly, I'm stuck writing down\na cost function, which",
    "start": "2591150",
    "end": "2598230"
  },
  {
    "text": "is x transpose Qx\nplus u transpose Ru, even if that's not\nexplicitly what I want.",
    "start": "2598230",
    "end": "2603810"
  },
  {
    "text": "Or maybe I should say,\nespecially the closest analogy is if I have a final\nvalue cost, straight",
    "start": "2603810",
    "end": "2610790"
  },
  {
    "text": "and maybe I make Qf really big. The only question is what\nyou really want to do. In most cases, I\nreally want to do that.",
    "start": "2610790",
    "end": "2618180"
  },
  {
    "text": "So I'm quite happy\nto use solvers which could do either case.",
    "start": "2618180",
    "end": "2624198"
  },
  {
    "text": "I think a more\npowerful solver is one that can handle either case. AUDIENCE: The other\nquestion is, if we want to solve something\nwhich takes a lot of time,",
    "start": "2624198",
    "end": "2632150"
  },
  {
    "text": "like T is relatively\nbig, it seems that if this open-loop policy\nthing that you're following",
    "start": "2632150",
    "end": "2638970"
  },
  {
    "text": "has one parameter per time step. RUSS TEDRAKE: It could\nhave a lot of parameters. Good.",
    "start": "2638970",
    "end": "2644700"
  },
  {
    "text": "AUDIENCE: But is it\npossible to just describe the whole policy with\na very limited space",
    "start": "2644700",
    "end": "2650730"
  },
  {
    "text": "with very few parameters\nand solve for that? RUSS TEDRAKE: So I tried\nto be careful to write down",
    "start": "2650730",
    "end": "2657060"
  },
  {
    "text": "the equations. So the question was-- can people hear the\nquestion or not? The question was\nroughly that if we're",
    "start": "2657060",
    "end": "2664800"
  },
  {
    "text": "worried about a problem with\na very long horizon time, it seems that I\nmight have to have a very large list of\nparameters to cover-- to make",
    "start": "2664800",
    "end": "2672150"
  },
  {
    "text": "a tape that's that long. And so, aren't the algorithms\nrather inefficient there?",
    "start": "2672150",
    "end": "2677190"
  },
  {
    "text": "Couldn't I do better by writing\ndown maybe a feedback policy? That's often the case is\nthat feedback policies can",
    "start": "2677190",
    "end": "2682440"
  },
  {
    "text": "be more compact. So I tried to write\ndown all these equations",
    "start": "2682440",
    "end": "2688799"
  },
  {
    "text": "as if there was some dependence\non x in your policy too. So the equations will be the\nsame if you do that version.",
    "start": "2688800",
    "end": "2696450"
  },
  {
    "text": "The only thing that\nI don't handle nicely in the things I'm\nthrowing up on the board is that I'm always\nsimulating from the same x0.",
    "start": "2696450",
    "end": "2703320"
  },
  {
    "text": "So I'm really\nexplicitly optimizing, even if I optimize a\nfeedback controller from a single initial\ncondition, its performance",
    "start": "2703320",
    "end": "2709280"
  },
  {
    "text": "from a single initial condition. So you can quite easily\nsay make a different cost",
    "start": "2709280",
    "end": "2716080"
  },
  {
    "text": "function, which is,\nlet's say I want J to be the sum\nof my performance from initial conditions\nK through 100.",
    "start": "2716080",
    "end": "2725290"
  },
  {
    "text": "And this would be,\nlet's say, J of xk, 0, or something like this.",
    "start": "2725290",
    "end": "2730900"
  },
  {
    "text": "Maybe I could start\nit from 100 of my favorite initial conditions. And that would try to optimize a\nfeedback policy perhaps better.",
    "start": "2730900",
    "end": "2739510"
  },
  {
    "text": "But the only thing that's not\nnicely addressed, I think, is choosing your initial\nconditions in a nice way.",
    "start": "2739510",
    "end": "2745870"
  },
  {
    "text": "DP-- [INAUDIBLE] program\nhandles that beautifully. And these things are\nmuch more local methods. So they have to be in there.",
    "start": "2745870",
    "end": "2752230"
  },
  {
    "text": "But I absolutely agree. Oftentimes, the open-loop tapes\nare not a particularly sparse",
    "start": "2752230",
    "end": "2759670"
  },
  {
    "text": "way to parameterize a policy. ",
    "start": "2759670",
    "end": "2766360"
  },
  {
    "text": "Good. So the direct collocation\nmethods, like I said,",
    "start": "2766360",
    "end": "2773050"
  },
  {
    "text": "more explicitly--\nthey're even more in the sense of open-loop\npolicies versus feedback policies, actually.",
    "start": "2773050",
    "end": "2779440"
  },
  {
    "text": "But they also more explicitly\nadd these constraints. So here's the idea.",
    "start": "2779440",
    "end": "2795900"
  },
  {
    "text": "Let's make my alpha\nmy vector that I'm trying to optimize over. A list of, can I\ncall it u0, u1, u2--",
    "start": "2795900",
    "end": "2807009"
  },
  {
    "text": "is that another reasonable way\nto describe what I've already done-- to u capital N. I've got\na list of control actions.",
    "start": "2807010",
    "end": "2816320"
  },
  {
    "text": "But now I'm going\nto actually also, I'm going to augment my\nparameter vector with the state",
    "start": "2816320",
    "end": "2825045"
  },
  {
    "text": "vector. ",
    "start": "2825045",
    "end": "2833190"
  },
  {
    "text": "So I'm just going to make an\neven bigger parameter vector. ",
    "start": "2833190",
    "end": "2839020"
  },
  {
    "text": "One of the reasons\nto do that is, I can now evaluate J of alpha,\nwhich is this 0 to T, G of x.",
    "start": "2839020",
    "end": "2850140"
  },
  {
    "text": "Maybe I even approximated\nthis discretization, right? So it could have a\ndt in there or not,",
    "start": "2850140",
    "end": "2867870"
  },
  {
    "text": "it doesn't matter to me. ",
    "start": "2867870",
    "end": "2874310"
  },
  {
    "text": "If I have u and x\nand all these things directly in my\nparameter vector, I",
    "start": "2874310",
    "end": "2880670"
  },
  {
    "text": "don't actually need to simulate\nin order to evaluate that. I can just evaluate\nit immediately.",
    "start": "2880670",
    "end": "2887940"
  },
  {
    "text": "I have x and I have u. The only problem is\nthat how do I pick alpha",
    "start": "2887940",
    "end": "2894780"
  },
  {
    "text": "so that x and u are consistent? It better be the\ncase that x1 looks",
    "start": "2894780",
    "end": "2899869"
  },
  {
    "text": "like the integration\nof x0 with u0 applied had better get me to x1.",
    "start": "2899870",
    "end": "2906320"
  },
  {
    "text": "So instead of having that sort\nof implicit in my equations, let's make it an\nexplicit constraint.",
    "start": "2906320",
    "end": "2913140"
  },
  {
    "text": "So let's do this subject to the\nconstraint that x of N plus 1--",
    "start": "2913140",
    "end": "2920029"
  },
  {
    "text": "actually, lots of constraints,\nso it's a list of constraints. x1 had better be f of x0\nu0 times dt, let's say.",
    "start": "2920030",
    "end": "2932348"
  },
  {
    "text": " It had better be equal to\n0, and so on, then x2--",
    "start": "2932348",
    "end": "2940960"
  },
  {
    "start": "2940960",
    "end": "2951760"
  },
  {
    "text": "right?  So if I'm willing to\nadd representation here,",
    "start": "2951760",
    "end": "2958500"
  },
  {
    "text": "I can actually evaluate\nmy cost function without explicitly simulating. That's cool.",
    "start": "2958500",
    "end": "2964290"
  },
  {
    "text": "I can take gradients\nvery quickly, because now it's just\nexplicitly the gradients, partial G, partial x.",
    "start": "2964290",
    "end": "2970170"
  },
  {
    "text": "Well, I know x, right? AUDIENCE: Wouldn't\nyou also want x1",
    "start": "2970170",
    "end": "2976609"
  },
  {
    "text": "minus f of x0 u0 dt minus x0?",
    "start": "2976610",
    "end": "2982000"
  },
  {
    "text": "[INAUDIBLE] RUSS TEDRAKE: Yes, thank you. Thank you.",
    "start": "2982000",
    "end": "2987960"
  },
  {
    "text": "So plus-- thank you. ",
    "start": "2987960",
    "end": "3004020"
  },
  {
    "text": "Good, thank you. I had some weird\nmix of discrete time and continuous time\nfloating around there. ",
    "start": "3004020",
    "end": "3015900"
  },
  {
    "text": "So if you parameterize\nit like this, you can very efficiently\ncalculate the gradients.",
    "start": "3015900",
    "end": "3021270"
  },
  {
    "text": "For instance, whereas\nyou can easily calculate the\ngradient with respect to time, in this\nparameterization,",
    "start": "3021270",
    "end": "3028080"
  },
  {
    "text": "and you just add a lot of\nconstraints to your solver. And you're asking your solver\nto solve for a lot more points.",
    "start": "3028080",
    "end": "3034080"
  },
  {
    "text": " It turns out that these\nsolvers handle constraints",
    "start": "3034080",
    "end": "3042070"
  },
  {
    "text": "very efficiently. In fact, it's often\ntimes more efficient to add constraints\nto the system, because it reduces\nthe search space.",
    "start": "3042070",
    "end": "3050710"
  },
  {
    "text": "So this is actually quite fast. The only criticism of it--\nthen there's another thing nice",
    "start": "3050710",
    "end": "3057670"
  },
  {
    "text": "thing about it is that you can-- I'll show you what\nI mean by this.",
    "start": "3057670",
    "end": "3063280"
  },
  {
    "text": "But you can sort\nof initialize this in ways that hop out\nof other local minima.",
    "start": "3063280",
    "end": "3070460"
  },
  {
    "text": "So let's say I just choose\nmy initial conditions in the previous simulations. I just always just\nchose u0 to [? uN ?]",
    "start": "3070460",
    "end": "3076839"
  },
  {
    "text": "to be some small random number. So the pendulum\nin the first thing would just shake\nhere a little bit.",
    "start": "3076840",
    "end": "3082150"
  },
  {
    "text": "And then it quickly changed\nuntil it swung up to the top. I can pick x perhaps\nmore intelligently.",
    "start": "3082150",
    "end": "3089800"
  },
  {
    "text": "It won't have satisfied\nthe constraints in the initial case. But I can choose an x. For the pendulum, let's\nsay my initial guess",
    "start": "3089800",
    "end": "3096370"
  },
  {
    "text": "at x would be a direct\ntrajectory that goes straight up to the top. ",
    "start": "3096370",
    "end": "3101500"
  },
  {
    "text": "If I start searching now for\nalphas that minimize this cost",
    "start": "3101500",
    "end": "3107380"
  },
  {
    "text": "and satisfy those\nconstraints, it just puts me in a different sort\nof area of the search space. And it might actually help me\nfind the swing of policies.",
    "start": "3107380",
    "end": "3114880"
  },
  {
    "text": "It's a very sort of\nheuristic thing to say. But it makes a big difference\nin practice I find. ",
    "start": "3114880",
    "end": "3122990"
  },
  {
    "text": "So I think most people\ntoday actually use these direct collocation methods\nfor trajectory optimization.",
    "start": "3122990",
    "end": "3129859"
  },
  {
    "text": "Yeah, Rick? AUDIENCE: [INAUDIBLE]\nchange those parameters",
    "start": "3129860",
    "end": "3138701"
  },
  {
    "text": "the vector would change. ",
    "start": "3138701",
    "end": "3145368"
  },
  {
    "text": "RUSS TEDRAKE: So you're\nworried that this will change-- the constraint\nmatrix would change? AUDIENCE: Well, aside\nof the alpha [INAUDIBLE]",
    "start": "3145368",
    "end": "3156360"
  },
  {
    "text": "RUSS TEDRAKE: So I don't\ndo it that way in my code. I do it, I say that this is\nvalid from 0 to T over N,",
    "start": "3156360",
    "end": "3168540"
  },
  {
    "text": "let's say. And this one is used from T\nover N to 2T over N and so on.",
    "start": "3168540",
    "end": "3178619"
  },
  {
    "text": "So it just stretches out. So the dt is not constant. I use that same action for\nlonger if my T stretches out.",
    "start": "3178620",
    "end": "3185945"
  },
  {
    "text": "And that keeps the\nparameter vector constant.  But I think if you were\nto purchase a DIRCOL--",
    "start": "3185945",
    "end": "3195510"
  },
  {
    "text": "this is often shorthand as\nDIRCOL, direct collocation, DIRCOL. And there was a DIRCOL\npackage that you",
    "start": "3195510",
    "end": "3202230"
  },
  {
    "text": "could get in\nFORTRAN 10 years ago or something that I think\na lot of people used. And I think those do things\nlike it stretches out time.",
    "start": "3202230",
    "end": "3209069"
  },
  {
    "text": "And then if dt gets ridiculous,\nit adds some more points. And then a more polished\nsoftware package",
    "start": "3209070",
    "end": "3214710"
  },
  {
    "text": "would do these things\nlike adding parameters and then reinitialize--\nreseeding the optimization.",
    "start": "3214710",
    "end": "3219960"
  },
  {
    "text": "Mine doesn't. ",
    "start": "3219960",
    "end": "3226588"
  },
  {
    "text": "Should I show you how it works? Is that the best\nthing to do here? It's a pretty simple idea.",
    "start": "3226588",
    "end": "3232049"
  },
  {
    "text": "The part that I can't\nreally express to you",
    "start": "3232050",
    "end": "3237480"
  },
  {
    "text": "efficiently here is why\nthat this is something",
    "start": "3237480",
    "end": "3242609"
  },
  {
    "text": "that the solvers\ncan do very well. I can tell you that it's about\nhow SQPs do interior point",
    "start": "3242610",
    "end": "3248340"
  },
  {
    "text": "methods, but I don't\nwant to get into that. So I think if you just sort of\ntake it on faith that they're good at handling\nconstraints, I think",
    "start": "3248340",
    "end": "3254838"
  },
  {
    "text": "it's reasonable to\nthink that maybe sending in an over-defined trajectory\nand allowing it to sort out",
    "start": "3254838",
    "end": "3261269"
  },
  {
    "text": "the constraints is a\nreasonable thing to try. And in practice, let me tell\nyou that it's pretty fast.",
    "start": "3261270",
    "end": "3269370"
  },
  {
    "text": "So can I do the\npendulum DIRCOL now? ",
    "start": "3269370",
    "end": "3279243"
  },
  {
    "text": "So that one was fast before. Now you notice the\ntime horizon here-- 3.06? Yeah, so that's what\nit wanted to be doing.",
    "start": "3279243",
    "end": "3286640"
  },
  {
    "text": "It liked 3.06 better than two. So maybe my other\nones would work better if I put 3 in there.",
    "start": "3286640",
    "end": "3294170"
  },
  {
    "text": "Can we do the-- someone called for the acrobot\nbefore, DIRCOL on the acrobot.",
    "start": "3294170",
    "end": "3299898"
  },
  {
    "text": "Oh, I should turn off plotting. Sorry. ",
    "start": "3299898",
    "end": "3305782"
  },
  {
    "text": "Let's see what happens. ",
    "start": "3305782",
    "end": "3319570"
  },
  {
    "text": "It's also got this\nfinal value constraint. That's why it quickly\ngot to the goal to satisfy that constraint. And now it's making sure\nthat all the dynamics",
    "start": "3319570",
    "end": "3326390"
  },
  {
    "text": "that these trajectories\nare satisfied. And then, it's just optimizing\nwithin that constraint",
    "start": "3326390",
    "end": "3332295"
  },
  {
    "text": "manifold. ",
    "start": "3332295",
    "end": "3338130"
  },
  {
    "text": "I didn't really mean to-- I'm afraid to hit\nControl-C. It might crash. That's the one thing about\n[? SNAP ?] being a [? MEX ?]",
    "start": "3338130",
    "end": "3343890"
  },
  {
    "text": "package calling FORTRAN-- stop. ",
    "start": "3343890",
    "end": "3350965"
  },
  {
    "text": "now my GUI is gone\nand my code is gone. Great. ",
    "start": "3350965",
    "end": "3360150"
  },
  {
    "text": "OK, good.  And it got to the top. No, that's the pendulum, sorry.",
    "start": "3360150",
    "end": "3366150"
  },
  {
    "text": "That's cheating. ",
    "start": "3366150",
    "end": "3387059"
  },
  {
    "text": "So turning off printing, just\nrun DIRCOL for a second here. ",
    "start": "3387060",
    "end": "3392648"
  },
  {
    "text": "AUDIENCE: How would\nusing these methods can be extended to stochastic case? ",
    "start": "3392648",
    "end": "3399758"
  },
  {
    "text": "RUSS TEDRAKE: I think\nthey're heavily tuned to the deterministic case. I won't make a blanket statement\nsaying they can't be extended.",
    "start": "3399758",
    "end": "3407310"
  },
  {
    "text": "But even the feedback-- I really think the direct\ncollocation in particular",
    "start": "3407310",
    "end": "3413430"
  },
  {
    "text": "feels very specialized\nfor opening the trajectory optimization.",
    "start": "3413430",
    "end": "3419408"
  },
  {
    "text": "This is slower than I remember. ",
    "start": "3419408",
    "end": "3429869"
  },
  {
    "text": "It gets there. Yeah, John. AUDIENCE: Is there\na reason why you need to use [INAUDIBLE] vector\nand not feedback [INAUDIBLE]..",
    "start": "3429870",
    "end": "3436050"
  },
  {
    "text": "You could evaluate the u given\nthe x and the [INAUDIBLE].. ",
    "start": "3436050",
    "end": "3447743"
  },
  {
    "text": "RUSS TEDRAKE: I think\nyou could do that. I think the key thing\nis parameterizing",
    "start": "3447743",
    "end": "3453220"
  },
  {
    "text": "the policy as well as x. But I think you're right. If you did some handful of\nw's or alphas there-- sorry--",
    "start": "3453220",
    "end": "3461140"
  },
  {
    "text": "then I think you could\nprobably do that too, yeah. I mean, implementing this\nconstraint is the only",
    "start": "3461140",
    "end": "3467908"
  },
  {
    "text": "real criticism that people\nhave of collocation methods. Almost everybody\nuses them, it seems. The only criticism\nthey have is that they",
    "start": "3467908",
    "end": "3474190"
  },
  {
    "text": "have sort of this fixed-step\nintegration in here. The constraint being satisfied--",
    "start": "3474190",
    "end": "3480020"
  },
  {
    "text": "it's hard to do sort of an\nODE solver in that step, because you need\nto be able to take the gradients of\nyour constraint.",
    "start": "3480020",
    "end": "3489530"
  },
  {
    "text": "So they tend to be fixed-step\nintegration routines, roughly. And so, the accuracy--",
    "start": "3489530",
    "end": "3495430"
  },
  {
    "text": "if people point to a problem\nwith collocation methods, they uniformly say that\nthey're not as accurate as the shooting methods, because\nyou don't actually numerically",
    "start": "3495430",
    "end": "3503110"
  },
  {
    "text": "simulate your system carefully. You've picked some time\ndiscretization of the system,",
    "start": "3503110",
    "end": "3508300"
  },
  {
    "text": "and you get it right for that. But I don't think that's\na big deal actually.",
    "start": "3508300",
    "end": "3515020"
  },
  {
    "text": "And if they're fast and they\nget out of local minima, then worst case, solve\nit this way and then",
    "start": "3515020",
    "end": "3521827"
  },
  {
    "text": "do a little shooting\nmethod at the end to finish the\noptimization, if you like. ",
    "start": "3521827",
    "end": "3533460"
  },
  {
    "text": "Run it one more time. ",
    "start": "3533460",
    "end": "3540440"
  },
  {
    "text": "John and I were\ntalking before class that there's really\nno reason why you couldn't compute\nthe gradients of an ODE,",
    "start": "3540440",
    "end": "3546710"
  },
  {
    "text": "update inside here. Well, how would you do that? You do it exactly like we did\nthe shooting method, right?",
    "start": "3546710",
    "end": "3552650"
  },
  {
    "text": "You could sort of run a little-- to compute the gradients\nof your constraint, you could actually\ndo the adjoint method",
    "start": "3552650",
    "end": "3559160"
  },
  {
    "text": "or the RTRL method to\ncompute those gradients. And then, that\nputs you somewhere in the land between\ndirect collocation",
    "start": "3559160",
    "end": "3565369"
  },
  {
    "text": "and multiple shooting. ",
    "start": "3565370",
    "end": "3571690"
  },
  {
    "text": "I swear that my laptop must\nbe operating on half a brain right now, because this\nwas much faster in lab. ",
    "start": "3571690",
    "end": "3586858"
  },
  {
    "text": "AUDIENCE: Do you\nhave a power cord? RUSS TEDRAKE: I'm not\nconnected into power. AUDIENCE: If you\ngo to the battery,",
    "start": "3586858",
    "end": "3592138"
  },
  {
    "text": "I guess it switches\nto energy saver. RUSS TEDRAKE:\nYeah, you're right.",
    "start": "3592138",
    "end": "3597450"
  },
  {
    "text": "I could probably turn\nit off right now.  So that was a slightly\ndifferent trajectory.",
    "start": "3597450",
    "end": "3604386"
  },
  {
    "start": "3604386",
    "end": "3617983"
  },
  {
    "text": "That's just graphics though. ",
    "start": "3617983",
    "end": "3625670"
  },
  {
    "text": "I'd be exceptionally\nembarrassed if I plugged it in or changed\nthe power settings and it was still slow. So let me just say that\nit was faster in live",
    "start": "3625670",
    "end": "3631460"
  },
  {
    "text": "and we'll leave it like that. So let me just\nmake the point of,",
    "start": "3631460",
    "end": "3639740"
  },
  {
    "text": "do people understand how\na problem like this-- we sort of saw a demonstration\nthat, if you could remember",
    "start": "3639740",
    "end": "3644780"
  },
  {
    "text": "in your head what\nthe acrobot did the first time and the\nacrobot did the second time. They both got to the\ngoal pretty well.",
    "start": "3644780",
    "end": "3649789"
  },
  {
    "text": "They took slightly different\ntrajectories, at least to my eye.  So one of the\ncomplaints about any",
    "start": "3649790",
    "end": "3655983"
  },
  {
    "text": "of these trajectory\noptimization methods is, they're only local methods. We're only going to find a\nlocal optima in my optimization.",
    "start": "3655983",
    "end": "3664970"
  },
  {
    "text": "For me, I think about\nthese problems a lot. It's still not\ncompletely intuitive what you think of a local minima\nin these settings might be.",
    "start": "3664970",
    "end": "3673494"
  },
  {
    "text": "There are some places where\nit could be pretty intuitive. So the pendulum, you can imagine\nif I found one policy that",
    "start": "3673495",
    "end": "3678860"
  },
  {
    "text": "pumped up with one\npump, you could imagine it might be hard to sort\nof get over a cost landscape,",
    "start": "3678860",
    "end": "3685160"
  },
  {
    "text": "so you did two pumps to get up. That could be a case where\na local minima makes sense.",
    "start": "3685160",
    "end": "3691310"
  },
  {
    "text": "I tried to come up with a little\nbit more obvious of an example",
    "start": "3691310",
    "end": "3696620"
  },
  {
    "text": "here relating to the original\ngrid world stuff we did.",
    "start": "3696620",
    "end": "3705750"
  },
  {
    "text": "So now, this isn't so\nmuch a dynamics problem. But I thought maybe a\npath-planning problem",
    "start": "3705750",
    "end": "3711080"
  },
  {
    "text": "would make the point. So here's a random geometric\nlandscape with Gaussian bumps",
    "start": "3711080",
    "end": "3718400"
  },
  {
    "text": "that you try to avoid\nor you incur cost. I'm actually plotting the cost\nlandscape as a function of x.",
    "start": "3718400",
    "end": "3723740"
  },
  {
    "text": "So you see there's a small hill,\nwhich is trying to take me down to the goal in red. Can I turn the lights all the\nway down for a minute here?",
    "start": "3723740",
    "end": "3731036"
  },
  {
    "start": "3731036",
    "end": "3737390"
  },
  {
    "text": "It'll be dramatic this way. So there's a goal here in red. And let's say the\ninitial conditions",
    "start": "3737390",
    "end": "3742520"
  },
  {
    "text": "are over there in green. And your task is to\ntake the system, which is x dot equals u, where x\nis the xy position of this,",
    "start": "3742520",
    "end": "3752420"
  },
  {
    "text": "and u is the velocity\nin x, the velocity in y. So just a trivial\ndynamical system,",
    "start": "3752420",
    "end": "3758843"
  },
  {
    "text": "but you're trying to\nfind a path that gets you to the goal with minimal cost.",
    "start": "3758843",
    "end": "3764330"
  },
  {
    "text": "So I did this example just\nbecause some people care about this kind of example. But also because I think it's\nsort of critically obvious",
    "start": "3764330",
    "end": "3773270"
  },
  {
    "text": "how you can have local minima. If I get a trajectory that's\non one side of the mountain and maybe the globally\noptimal trajectory",
    "start": "3773270",
    "end": "3779948"
  },
  {
    "text": "is on the other side\nof the mountain, it might be hard for me\nto get across the mountain to that trajectory. So let's just see that happen.",
    "start": "3779948",
    "end": "3786680"
  },
  {
    "text": "So do direct collocation\nhere, so what did I implement?",
    "start": "3786680",
    "end": "3791696"
  },
  {
    "start": "3791696",
    "end": "3802400"
  },
  {
    "text": "OK, so I forgot to type-- ",
    "start": "3802400",
    "end": "3816010"
  },
  {
    "text": "so I just did\ndirect collocation. And really quickly,\nit found this.",
    "start": "3816010",
    "end": "3822880"
  },
  {
    "text": "So this is not the\noptimization software's fault. What if I do this?",
    "start": "3822880",
    "end": "3829990"
  },
  {
    "start": "3829990",
    "end": "3839230"
  },
  {
    "text": "So it found some nice\npath through the foothills here to the goal.",
    "start": "3839230",
    "end": "3848220"
  },
  {
    "text": "Yeah, please. AUDIENCE: Can you try to\nspecify some of those--",
    "start": "3848220",
    "end": "3854079"
  },
  {
    "text": "since you have x\nparameterized, can you try to make it go through\na particular set of bumps",
    "start": "3854080",
    "end": "3863530"
  },
  {
    "text": "by specifying that? RUSS TEDRAKE: Exactly. So the reason I chose\nto do direct collocation for this is, my initial guess,\nu was just some random vector.",
    "start": "3863530",
    "end": "3871690"
  },
  {
    "text": "But my x was actually a direct\nline from start to the goal. AUDIENCE: So if you\nhad drawn it as a line",
    "start": "3871690",
    "end": "3877599"
  },
  {
    "text": "between those first two and\nthen going around to [INAUDIBLE] RUSS TEDRAKE: So the\none I thought to do was,",
    "start": "3877600",
    "end": "3883720"
  },
  {
    "text": "let's just do-- just because\nit was easy to type-- let's do an initial x which\njust goes directly this way and then see what happens.",
    "start": "3883720",
    "end": "3890208"
  },
  {
    "text": "I think that's what I had here. So if I change my x tape\nhere is now linspace.",
    "start": "3890208",
    "end": "3896380"
  },
  {
    "text": "So it interpolates in\nx0 straight to the goal. But then the other one is\njust x1 straight across,",
    "start": "3896380",
    "end": "3902470"
  },
  {
    "text": "reading that code is not what\nyou want to do in class here. But if I set the initial x tape\nto be that and I run it again,",
    "start": "3902470",
    "end": "3935830"
  },
  {
    "text": "It's doing its solving. It's properly doing\nit in some window. Oh, I turned off\nanimation, didn't I? ",
    "start": "3935830",
    "end": "3950260"
  },
  {
    "text": "Oops, that was a failure. But it found a different path.",
    "start": "3950260",
    "end": "3956450"
  },
  {
    "text": "But it actually told\nme, Warning, exited. So I have this check\npage 19 of [INAUDIBLE] 6",
    "start": "3956450",
    "end": "3962830"
  },
  {
    "text": "the paper to figure out\nwhat the heck exit 41 means. But it basically couldn't\nsatisfy the constraints.",
    "start": "3962830",
    "end": "3968908"
  },
  {
    "text": "So I bet if I just run it\nagain with the random initial conditions, it'll be OK. ",
    "start": "3968908",
    "end": "3989640"
  },
  {
    "text": "But the point is exactly this. I still found one\nthat just probably didn't satisfy some of the\nconstraints at some small part",
    "start": "3989640",
    "end": "3995970"
  },
  {
    "text": "of that trajectory here. It went the other way\naround the mountain.",
    "start": "3995970",
    "end": "4001130"
  },
  {
    "text": "So there are local\nminima in these problems. In problems like this,\nit's completely obvious",
    "start": "4001130",
    "end": "4006560"
  },
  {
    "text": "why there are local minima. In the acrobot and\nthings like that, you will find that\nthere are local minima.",
    "start": "4006560",
    "end": "4012980"
  },
  {
    "text": "If you start with different\nrandom parameterizations, you'll find slightly different\nswing-up trajectories.",
    "start": "4012980",
    "end": "4018110"
  },
  {
    "text": " So our local minima-- a killer-- a lot of\npeople say, well,",
    "start": "4018110",
    "end": "4025380"
  },
  {
    "text": "that means these methods stink. They're subject to local minima.",
    "start": "4025380",
    "end": "4030800"
  },
  {
    "text": "I don't care if I'm in a local\nminima for the most part. I mean, if I was really\ngoing to have to walk around",
    "start": "4030800",
    "end": "4036463"
  },
  {
    "text": "a mountain instead of walking\nthrough the mountains, then maybe I'd care. But if I'm doing an\nacrobot and it swings up",
    "start": "4036463",
    "end": "4043609"
  },
  {
    "text": "like this instead of swings up\nlike this, for the most part, I don't care. So although people\ntalk about it a lot,",
    "start": "4043610",
    "end": "4052800"
  },
  {
    "text": "I find in most of the\nproblems I care about, local minima aren't\nthat big of a deal. They exist.",
    "start": "4052800",
    "end": "4058460"
  },
  {
    "text": "Sometimes they can\nupset your numerics. But as long as you get\nto the goal, I'm happy. ",
    "start": "4058460",
    "end": "4066410"
  },
  {
    "text": "Now, there are a\ncouple other ideas",
    "start": "4066410",
    "end": "4072380"
  },
  {
    "text": "in these trajectory\noptimizations. And on Tuesday, I guess, I'm\ngoing to wait till Tuesday now,",
    "start": "4072380",
    "end": "4079640"
  },
  {
    "text": "first of all, I'm\ngoing to tell you how to stabilize\nthese trajectories. Because that's useless\nas it is right now.",
    "start": "4079640",
    "end": "4084770"
  },
  {
    "text": "If I just even simulated\nit with a different dt, it would probably fall or\nnot get to the mountain.",
    "start": "4084770",
    "end": "4091369"
  },
  {
    "text": " But it turns out, even\nwith a pretty coarse time",
    "start": "4091370",
    "end": "4096739"
  },
  {
    "text": "step, if you stabilize\nthe thing with feedback, then it works great. So I'll show you how to do\nthe trajectory stabilization",
    "start": "4096740",
    "end": "4103850"
  },
  {
    "text": "with an LQR method on Tuesday. And that's actually\ngoing to lead to another class of the\ntrajectory optimizers, which",
    "start": "4103850",
    "end": "4110089"
  },
  {
    "text": "would be an\niterative LQR method. And then, depending on how\nmuch I sleep this weekend,",
    "start": "4110090",
    "end": "4120257"
  },
  {
    "text": "I was thinking about doing\nthe discrete mechanics version of the trajectory\noptimizers on Tuesday too. We'll see.",
    "start": "4120258",
    "end": "4125750"
  },
  {
    "text": "So we'll push the walking\nback until Thursday just to complete the story about\nthese trajectories solvers.",
    "start": "4125750",
    "end": "4133740"
  },
  {
    "text": "Any Questions? They're pretty good. They're pretty fast,\nespecially if you",
    "start": "4133740",
    "end": "4139522"
  },
  {
    "text": "are plugged into the wall. OK, see you see you next week.",
    "start": "4139522",
    "end": "4145180"
  },
  {
    "start": "4145180",
    "end": "4146000"
  }
]