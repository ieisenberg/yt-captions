[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13330"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13330",
    "end": "21460"
  },
  {
    "text": "PROFESSOR: So let's begin. Today, I'm going to\nreview linear algebra.",
    "start": "21460",
    "end": "26600"
  },
  {
    "text": "So I'm assuming that you\nalready took some linear algebra course. And I'm going to just review\nthe relevant content that",
    "start": "26600",
    "end": "35160"
  },
  {
    "text": "will appear again and again\nthroughout the course. But do interrupt me if some\nconcepts are not clear,",
    "start": "35160",
    "end": "42070"
  },
  {
    "text": "if you don't remember some\nconcept from linear algebra.",
    "start": "42070",
    "end": "47150"
  },
  {
    "text": "I hope you do. But please let me know. I just don't know.",
    "start": "47150",
    "end": "53450"
  },
  {
    "text": "You have very different\nbackground knowledge. So it's hard to tune\nto one special group.",
    "start": "53450",
    "end": "60390"
  },
  {
    "text": "So I tailored this\nlecture notes so that it's a review for those who took\nthe most basic linear algebra",
    "start": "60390",
    "end": "66800"
  },
  {
    "text": "course. So if you already\nhave that experience, and don't understand it, please\nfeel free to interrupt me.",
    "start": "66800",
    "end": "73580"
  },
  {
    "text": " So I'm going to start by\ntalking about matrices. ",
    "start": "73580",
    "end": "81354"
  },
  {
    "text": "A matrix, in a very\nsimple form, is just a collection of numbers.",
    "start": "81354",
    "end": "86820"
  },
  {
    "text": "For example\n[1, 2, 3; 2, 3, 4;  4, 5, 10].",
    "start": "86820",
    "end": "93620"
  },
  {
    "text": "You can pick any number of\nrows, any number of columns. You just write down\nnumbers in a square format.",
    "start": "93620",
    "end": "99680"
  },
  {
    "text": "And that's the matrix. What's special about it?",
    "start": "99680",
    "end": "104810"
  },
  {
    "text": "So what kind of data can\nyou arrange in a matrix? So I'll take an example,\nwhich looks relevant to us.",
    "start": "104810",
    "end": "111840"
  },
  {
    "text": "So for example, we can index the\nrows by stocks, by companies,",
    "start": "111840",
    "end": "116850"
  },
  {
    "text": "like Apple.  Morgan Stanley should be\nthere, and then Google.",
    "start": "116850",
    "end": "125350"
  },
  {
    "text": " And then maybe we can\nindex the column by dates.",
    "start": "125350",
    "end": "131765"
  },
  {
    "text": " I'll say July 1st, October\n1st, September 1st.",
    "start": "131765",
    "end": "140920"
  },
  {
    "text": "And the numbers, you can\npick whatever data you want. But probably the\nsensible data will be the stock price on that day.",
    "start": "140920",
    "end": "148310"
  },
  {
    "text": "I don't know for example\n400, 500, and 5,000.",
    "start": "148310",
    "end": "153750"
  },
  {
    "text": "That would be great. So these kind of data,\nthat's just the matrix.",
    "start": "153750",
    "end": "160950"
  },
  {
    "text": "So defining a matrix\nis really simple. But why is it so powerful?",
    "start": "160950",
    "end": "167870"
  },
  {
    "text": "So that's an application\npoint of view, just as a collection of data. But from a theoretical\npoint of view,",
    "start": "167870",
    "end": "181610"
  },
  {
    "text": "a matrix, an m by n\nmatrix, is an operator.",
    "start": "181610",
    "end": "190860"
  },
  {
    "text": "It defines a linear\ntransformation. A defines a linear\ntransformation",
    "start": "190860",
    "end": "196250"
  },
  {
    "text": "from the vector space,\nn-dimensional vector space to the m-dimensional\nvector space.",
    "start": "196250",
    "end": "202779"
  },
  {
    "text": "That sounds a lot more\nabstract than this. ",
    "start": "202779",
    "end": "207840"
  },
  {
    "text": "So for example, let's just\ntake a very small example. If I use a 2 by 2\nmatrix, [2, 0;  0, 3].",
    "start": "207840",
    "end": "219540"
  },
  {
    "text": "Then [2, 0; 0, 3] times, let's\nsay [1, 1] is just [2, 3].",
    "start": "219540",
    "end": "227588"
  },
  {
    "start": "227589",
    "end": "233122"
  },
  {
    "text": "Does that makes sense? It's just matrix multiplication. So now try to combine\nthe point of view.",
    "start": "233122",
    "end": "240860"
  },
  {
    "text": "What does it mean to have a\nlinear transformation defined by a data set?",
    "start": "240860",
    "end": "246690"
  },
  {
    "text": "And things start\nto get confusing. What is it? Why does a data set define\na linear transformation?",
    "start": "246690",
    "end": "253790"
  },
  {
    "text": "And does it have any\nsensible meaning? So that's a good question\nto have in mind today.",
    "start": "253790",
    "end": "261209"
  },
  {
    "text": "And try to remember\nthis question. Because today I'll\ntry to really develop",
    "start": "261209",
    "end": "267040"
  },
  {
    "text": "a theory of eigenvalues and\neigenvectors in a purely theoretical language.",
    "start": "267040",
    "end": "273860"
  },
  {
    "text": "But it can still be\napplied to these data sets, and give very\nimportant properties",
    "start": "273860",
    "end": "284030"
  },
  {
    "text": "and very important quantities. You can get some useful\ninformation out of it.",
    "start": "284030",
    "end": "290030"
  },
  {
    "text": "Try to make sense out\nof why it happens. So that will be the goal today,\nto really treat linear algebra",
    "start": "290030",
    "end": "298830"
  },
  {
    "text": "as a theoretical thing. But remember that there's some\ndata set, like really data set",
    "start": "298830",
    "end": "304816"
  },
  {
    "text": "underlying.  This doesn't go up. That was a bad choice\nfor my first board.",
    "start": "304816",
    "end": "313230"
  },
  {
    "text": "Sorry. ",
    "start": "313230",
    "end": "322150"
  },
  {
    "text": "So the most important concepts\nfor us are the eigenvalues",
    "start": "322150",
    "end": "330880"
  },
  {
    "text": "and eigenvectors\nof a matrix, which is defined as a real number,\nlambda, and vector v,",
    "start": "330880",
    "end": "347740"
  },
  {
    "text": "is an eigenvalue, and\neigenvector of a matrix A,",
    "start": "347740",
    "end": "362250"
  },
  {
    "text": "if A times v is equal to\nlambda times V. We also",
    "start": "362250",
    "end": "369120"
  },
  {
    "text": "say that v is an eigenvector\ncorresponding to lambda.",
    "start": "369120",
    "end": "377925"
  },
  {
    "text": " So remember eigenvalues\nand eigenvectors always",
    "start": "377925",
    "end": "384570"
  },
  {
    "text": "come in pairs. And they are defined by the\nproperty that A*v = lambda*v.",
    "start": "384570",
    "end": "394710"
  },
  {
    "text": "First question, does all\nmatrix have eigenvalues and eigenvectors? ",
    "start": "394710",
    "end": "401508"
  },
  {
    "text": "Nope? So Av-- It looks like this\nis a very strange equation",
    "start": "401508",
    "end": "410620"
  },
  {
    "text": "to satisfy. But if you change it in this\nform, (A - lambda I)v = 0.",
    "start": "410620",
    "end": "417640"
  },
  {
    "text": "That still looks strange. But at least you\nunderstand that-- it's",
    "start": "417640",
    "end": "423480"
  },
  {
    "text": "an only if, this can happen\nonly if this can happen.",
    "start": "423480",
    "end": "428810"
  },
  {
    "text": "Happens only if A - lambda\nI does not have full rank.",
    "start": "428810",
    "end": "436610"
  },
  {
    "text": "So determinant of (A - lambda I)\nis equal to 0, if and only if,",
    "start": "436610",
    "end": "444064"
  },
  {
    "text": "in fact.  So now comes a very\ninteresting observation.",
    "start": "444065",
    "end": "452913"
  },
  {
    "text": " det(A - lambda I) is a\npolynomial of degree n.",
    "start": "452913",
    "end": "465400"
  },
  {
    "text": " I made a mistake. I should have said, this is\nonly for n by n matrices.",
    "start": "465400",
    "end": "472645"
  },
  {
    "start": "472645",
    "end": "480949"
  },
  {
    "text": "This is only for\nsquare matrices. Sorry. It's a polynomial of degree n.",
    "start": "480950",
    "end": "486430"
  },
  {
    "text": "That means it has a solution. It has to give n\nin terms of lambda.",
    "start": "486430",
    "end": "491435"
  },
  {
    "text": " So it has a solution.",
    "start": "491435",
    "end": "497110"
  },
  {
    "text": "It might be a complex number. ",
    "start": "497110",
    "end": "506250"
  },
  {
    "text": "I'm really sorry. I'm nervous in\nfront of the video. ",
    "start": "506250",
    "end": "512974"
  },
  {
    "text": "I understand why you were saying\nthat is doesn't necessarily exist. Let me repeat.",
    "start": "512974",
    "end": "518330"
  },
  {
    "text": "I made a few mistakes here. So let me repeat here. For n by n matrix A, a complex\nnumber lambda, and the vector",
    "start": "518330",
    "end": "529650"
  },
  {
    "text": "v, is an eigenvalue\nand eigenvector if it satisfies this condition. It doesn't have to be real.",
    "start": "529650",
    "end": "535779"
  },
  {
    "text": "Sorry about that. And now if we\nrephrase it this way, because this is a\npolynomial, it always",
    "start": "535780",
    "end": "543130"
  },
  {
    "text": "has at least one solution.  That was just a side point.",
    "start": "543130",
    "end": "549670"
  },
  {
    "text": "Very theoretical. So we see that there\nalways exists at least one eigenvalue and eigenvector. ",
    "start": "549670",
    "end": "557420"
  },
  {
    "text": "Now we saw its existence, what\nis the geometrical meaning of it? ",
    "start": "557420",
    "end": "574940"
  },
  {
    "text": "Now let's go back to the linear\ntransformation point of view. So suppose A is a 3 by 3 matrix.",
    "start": "574940",
    "end": "583990"
  },
  {
    "text": " Then A takes the vector in R^3\nand transforms it into another",
    "start": "583990",
    "end": "598510"
  },
  {
    "text": "vector in R^3. ",
    "start": "598510",
    "end": "604230"
  },
  {
    "text": "But if you have this\nrelation, what's going to happen is\nA, when applied to v,",
    "start": "604230",
    "end": "611350"
  },
  {
    "text": "it will just scale the vector\nv. If this was the original v, A of v will just be\nlambda times this vector.",
    "start": "611350",
    "end": "619589"
  },
  {
    "text": "That will be our Av, which\nis equal to lambda v. So eigenvectors are\nthose special vectors",
    "start": "619590",
    "end": "628160"
  },
  {
    "text": "which when applied this\nlinear transformation just get scaled by some amount, where\nthat amount is exactly lambda.",
    "start": "628160",
    "end": "639620"
  },
  {
    "text": "So what we established so\nfar, what we recall so far is every n by n matrix has\nat least one such direction.",
    "start": "639620",
    "end": "647649"
  },
  {
    "text": "There is some vector where the\nlinear transformation defined by A just scales that vector.",
    "start": "647650",
    "end": "655005"
  },
  {
    "text": "Which is quite\ninteresting, if you ever thought about it before. There's no reason such\nvector should exist.",
    "start": "655005",
    "end": "661110"
  },
  {
    "text": "Of course I'm\nlying a little bit. Because these might\nbe complex vectors. But at least in the\ncomplex world it's true.",
    "start": "661110",
    "end": "670410"
  },
  {
    "text": " So if you think about\nthis, this is very helpful.",
    "start": "670410",
    "end": "679070"
  },
  {
    "text": "It gives you the vectors-- from\nthese vectors' point of view, this linear transformation\nis really easy to understand.",
    "start": "679070",
    "end": "687590"
  },
  {
    "text": "That's why eigenvalues and\neigenvector are so good. It breaks down the\nlinear transformation into really simple operations.",
    "start": "687590",
    "end": "693420"
  },
  {
    "text": " Let me formalize that\na little bit more.",
    "start": "693420",
    "end": "700110"
  },
  {
    "text": "So in an extreme case a\nmatrix, an n by n matrix A,",
    "start": "700110",
    "end": "710110"
  },
  {
    "text": "we call it\ndiagonalizable, if there",
    "start": "710110",
    "end": "718370"
  },
  {
    "text": "exists an orthonormal\nmatrix, I'll",
    "start": "718370",
    "end": "726800"
  },
  {
    "text": "call what it is, U, such that\nA is equal to U times D times U",
    "start": "726800",
    "end": "740920"
  },
  {
    "text": "inverse for a diagonal matrix D.",
    "start": "740920",
    "end": "753540"
  },
  {
    "text": "Let me iterate through\nthis a little bit. ",
    "start": "753540",
    "end": "759930"
  },
  {
    "text": "What is an orthonormal matrix? It's a matrix defined by the\nrelation U times U transposed",
    "start": "759930",
    "end": "765530"
  },
  {
    "text": "is equal to the identity. What is a diagonal matrix? It's a matrix whose\nnonzero entries are all",
    "start": "765530",
    "end": "774090"
  },
  {
    "text": "on the diagonal. All the rest are zero. ",
    "start": "774090",
    "end": "781360"
  },
  {
    "text": "Why is it so good to\nhave this decomposition? What does it mean to have an\northonormal matrix like this?",
    "start": "781360",
    "end": "788269"
  },
  {
    "text": "It means basically I'll just\nexplain what's happening.",
    "start": "788270",
    "end": "796140"
  },
  {
    "text": "If that happens, if a\nmatrix is diagonalizable, if this A is\ndiagonalizable, there will be three directions,\nv_1, v_2, v_3,",
    "start": "796140",
    "end": "808560"
  },
  {
    "text": "such that when you apply this\nA, v_1 scales by some lambda_1.",
    "start": "808560",
    "end": "814570"
  },
  {
    "text": "v_2 scales by some lambda_2. And v_3 scales by some lambda_3.",
    "start": "814570",
    "end": "820244"
  },
  {
    "text": " So we can completely understand\nthe transformation A,",
    "start": "820244",
    "end": "828410"
  },
  {
    "text": "just in terms of\nthese three vectors. ",
    "start": "828410",
    "end": "838600"
  },
  {
    "text": "So this, the stuff here will\nbe the most important things",
    "start": "838600",
    "end": "845610"
  },
  {
    "text": "you'll use in linear algebra\nthroughout this course. So let me repeat\nit really slowly.",
    "start": "845610",
    "end": "853390"
  },
  {
    "text": "So an eigenvalue and eigenvector\nis defined by this relation.",
    "start": "853390",
    "end": "858730"
  },
  {
    "text": "We know that there are at least\none eigenvalue for each matrix, and there is an eigenvector\ncorresponding to it.",
    "start": "858730",
    "end": "865310"
  },
  {
    "text": "And eigenvectors have\nthis geometrical meaning where-- a vector\nis an eigenvector,",
    "start": "865310",
    "end": "872930"
  },
  {
    "text": "if the linear\ntransformation defined by A just scales that vector.",
    "start": "872930",
    "end": "878300"
  },
  {
    "text": "So for our setting,\nthe real good matrices are the matrices which\ncan be broken down",
    "start": "878300",
    "end": "885430"
  },
  {
    "text": "into these directions. And those directions\nare defined by this U.",
    "start": "885430",
    "end": "892180"
  },
  {
    "text": "And D defines how\nmuch it will scale. So in this case U will\nbe our v_1, v_2, v_3.",
    "start": "892180",
    "end": "902110"
  },
  {
    "text": "And D will be our lambda_1,\nlambda_2, lambda_3 all 0.",
    "start": "902110",
    "end": "907886"
  },
  {
    "start": "907887",
    "end": "917000"
  },
  {
    "text": "Any questions so far? ",
    "start": "917000",
    "end": "922930"
  },
  {
    "text": "So that is abstract. Now remember the question\nI posed in the beginning. So remember that matrix where we\nhad stocks and dates and stock",
    "start": "922930",
    "end": "933500"
  },
  {
    "text": "prices in the entries? What will an eigenvector\nof that matrix mean?",
    "start": "933500",
    "end": "940145"
  },
  {
    "text": "What will an eigenvalue mean?  So try to think\nabout that question.",
    "start": "940145",
    "end": "946620"
  },
  {
    "text": " It's not like it will have\nsome physical counterpart.",
    "start": "946620",
    "end": "952750"
  },
  {
    "text": "But there's some really\ninteresting things going on there. ",
    "start": "952750",
    "end": "969810"
  },
  {
    "text": "The bad news is that not all\nmatrices are diagonalizable. If a matrix is diagonalizable,\nit's really easy",
    "start": "969810",
    "end": "977460"
  },
  {
    "text": "to understand what it does. Because it really breaks down\ninto these three directions,",
    "start": "977460",
    "end": "982949"
  },
  {
    "text": "if it's a 3 by 3. If it's an n by n, it breaks\ndown into n directions. Unfortunately, not all\nmatrices are diagonalizable.",
    "start": "982950",
    "end": "992090"
  },
  {
    "text": "But there is a\nvery special class of matrices which are\nalways diagonalizable.",
    "start": "992090",
    "end": "998329"
  },
  {
    "text": "And fortunately we\nwill see those matrices throughout the course.",
    "start": "998330",
    "end": "1003340"
  },
  {
    "text": "Most of the matrices,\nn by n matrices, we will study, fall\ninto this category. ",
    "start": "1003340",
    "end": "1011900"
  },
  {
    "text": "So an n by n matrix\nA is symmetric",
    "start": "1011900",
    "end": "1021970"
  },
  {
    "text": "if A is equal to A transpose. Before proceeding,\nplease raise your hand",
    "start": "1021970",
    "end": "1030000"
  },
  {
    "text": "if you're familiar with\nall the concepts so far. OK. Good feeling.",
    "start": "1030000",
    "end": "1036180"
  },
  {
    "start": "1036180",
    "end": "1042500"
  },
  {
    "text": "So a matrix is symmetric if\nit's equal to its transpose. A transpose is obtained\nby taking the mirror",
    "start": "1042500",
    "end": "1047709"
  },
  {
    "text": "image across the diagonal.  And then it is known that\nall symmetric matrices",
    "start": "1047710",
    "end": "1064720"
  },
  {
    "text": "are diagonalizable. Ah, I've made another mistake.",
    "start": "1064720",
    "end": "1070816"
  },
  {
    "text": "Orthonormally.  So with this I missed matrices\northonormally diagonalizable.",
    "start": "1070817",
    "end": "1086970"
  },
  {
    "text": "So it's diagonalizable if\nwe drop this condition,",
    "start": "1086970",
    "end": "1093190"
  },
  {
    "text": "and replace it\nwith an invertible. ",
    "start": "1093190",
    "end": "1105150"
  },
  {
    "text": "So symmetric matrices\nare really good. And fortunately most of the n\nby n matrices that we will study",
    "start": "1105150",
    "end": "1113920"
  },
  {
    "text": "are symmetric. Just by the nature of\nit, it will be symmetric. The one I gave as an\nexample is not symmetric.",
    "start": "1113920",
    "end": "1122030"
  },
  {
    "text": "It's not symmetric. But I will address\nthat issue in a minute.",
    "start": "1122030",
    "end": "1129770"
  },
  {
    "text": "And another important\nthing is symmetric matrices",
    "start": "1129770",
    "end": "1140326"
  },
  {
    "text": "have real eigenvalues. ",
    "start": "1140326",
    "end": "1152340"
  },
  {
    "text": "So really this geometrical\npicture just the-- for symmetric\nmatrices, this picture",
    "start": "1152340",
    "end": "1158891"
  },
  {
    "text": "is really the picture\nyou should have in mind. ",
    "start": "1158891",
    "end": "1170870"
  },
  {
    "text": "So proof of Theorem 2.",
    "start": "1170870",
    "end": "1176870"
  },
  {
    "start": "1176870",
    "end": "1185030"
  },
  {
    "text": "Suppose lambda is an eigenvalue\nwith eigenvector v. Then",
    "start": "1185030",
    "end": "1198610"
  },
  {
    "text": "by definition we have this. ",
    "start": "1198610",
    "end": "1204720"
  },
  {
    "text": "Now multiply v\ntransposed on both sides. ",
    "start": "1204720",
    "end": "1212070"
  },
  {
    "text": "It is lambda times the norm v.",
    "start": "1212070",
    "end": "1220149"
  },
  {
    "text": "Now take the complex\nconjugate-- Real symmetric.",
    "start": "1220150",
    "end": "1234732"
  },
  {
    "start": "1234732",
    "end": "1240710"
  },
  {
    "text": "And then first A conjugate,\nwe have v^T A^T v,",
    "start": "1240710",
    "end": "1247539"
  },
  {
    "text": "and then take the\nconjugate of it. Then we get lambda...",
    "start": "1247540",
    "end": "1257020"
  },
  {
    "text": "v. And this side is\nequal to v^T A^T v.",
    "start": "1257020",
    "end": "1279490"
  },
  {
    "text": "But because A is real symmetric,\nwe see that A is equal",
    "start": "1279490",
    "end": "1287760"
  },
  {
    "text": "to the conjugate of\ncomplex conjugate of A. So this expression and this\nexpression is the same.",
    "start": "1287760",
    "end": "1295760"
  },
  {
    "text": "The right side should\nalso be the same. That means lambda is equal\nto the conjugate of lambda.",
    "start": "1295760",
    "end": "1303136"
  },
  {
    "text": "So lambda has to be a real. ",
    "start": "1303136",
    "end": "1319480"
  },
  {
    "text": "So Theorem 1 is a little\nbit more complicated, and it involves more\nadvanced concepts",
    "start": "1319480",
    "end": "1326490"
  },
  {
    "text": "like basis and linear\nsubspace, and so on.",
    "start": "1326490",
    "end": "1333500"
  },
  {
    "text": "And those concepts\nare not really important for this class. So I'll just skip the proof.",
    "start": "1333500",
    "end": "1338909"
  },
  {
    "text": "But it's really important to\nremember these two theorems. Wherever you see\na symmetric matrix",
    "start": "1338910",
    "end": "1345760"
  },
  {
    "text": "you should really feel like\nyou have control on it. Because you can diagonalize it. ",
    "start": "1345760",
    "end": "1354370"
  },
  {
    "text": "And moreover, all\neigenvalues are real, and you have really good\ncontrol on symmetric matrices.",
    "start": "1354370",
    "end": "1360562"
  },
  {
    "text": " That's good.",
    "start": "1360562",
    "end": "1368050"
  },
  {
    "text": "That was when\neverything went well. We can diagonalize it.",
    "start": "1368050",
    "end": "1373140"
  },
  {
    "text": "So, so far we saw that if\nfor a symmetric matrix,",
    "start": "1373140",
    "end": "1378760"
  },
  {
    "text": "we can diagonalize it. It's really easy to understand. But what about general matrices? ",
    "start": "1378760",
    "end": "1396690"
  },
  {
    "text": "In general, not all matrices are\ndiagonalizable, first of all. ",
    "start": "1396690",
    "end": "1417500"
  },
  {
    "text": "But sometimes we still want\nto decomposition like this. So diagonalization was A equals\nU times D times U inverse.",
    "start": "1417500",
    "end": "1433590"
  },
  {
    "start": "1433590",
    "end": "1439909"
  },
  {
    "text": "But we want something similar. We want to understand. So our goal, we want to\nstill understand the matrix,",
    "start": "1439910",
    "end": "1455020"
  },
  {
    "text": "give a matrix A through simple\noperations, such as scaling.",
    "start": "1455020",
    "end": "1462945"
  },
  {
    "text": " When the matrix was a\ndiagonalizable matrix this",
    "start": "1462945",
    "end": "1470800"
  },
  {
    "text": "was done, this was possible. Unfortunately, it's not\nalways diagonalizable. ",
    "start": "1470800",
    "end": "1478559"
  },
  {
    "text": "So we have to do something else. ",
    "start": "1478560",
    "end": "1485460"
  },
  {
    "text": "So that's what I\nwant to talk about. And luckily the\ngood news is there is a nice tool we can\nuse for all matrices,",
    "start": "1485460",
    "end": "1492600"
  },
  {
    "text": "even those slightly weaker,\nin fact, a little bit more weaker than this\ndiagonalization.",
    "start": "1492600",
    "end": "1498760"
  },
  {
    "text": "But still it distills some\nvery important information of the matrix. So it's called singular\nvalue decomposition.",
    "start": "1498760",
    "end": "1505240"
  },
  {
    "start": "1505240",
    "end": "1517220"
  },
  {
    "text": "So this will be our second\ntool of understanding matrices.",
    "start": "1517220",
    "end": "1522350"
  },
  {
    "text": "It's very similar to\nthis diagonalization, or in other words I call this\neigenvalue decomposition. ",
    "start": "1522350",
    "end": "1534399"
  },
  {
    "text": "But it has a slightly\ndifferent form. So what is its form? So theorem.",
    "start": "1534400",
    "end": "1541770"
  },
  {
    "text": "Let A be an m by n matrix. ",
    "start": "1541770",
    "end": "1551350"
  },
  {
    "text": "Then there always exists\northonormal matrices",
    "start": "1551350",
    "end": "1572400"
  },
  {
    "text": "U and V such that A is\nequal to U times sigma times",
    "start": "1572400",
    "end": "1585530"
  },
  {
    "text": "V transpose. For some diagonal matrix sigma.",
    "start": "1585530",
    "end": "1596880"
  },
  {
    "text": "Let me parse through the\ntheorem a little bit more. Whenever you're\ngiven a matrix, it",
    "start": "1596880",
    "end": "1602669"
  },
  {
    "text": "doesn't even have to be\na square matrix anymore. It can be non-symmetric. So whenever we're given an\nm by n matrix, in general,",
    "start": "1602670",
    "end": "1610400"
  },
  {
    "text": "there always exists\ntwo matrices, U and V, which are orthonormal,\nsuch that A",
    "start": "1610400",
    "end": "1618510"
  },
  {
    "text": "can be decomposed as U times\nsigma times V transposed, where sigma is a diagonal matrix.",
    "start": "1618510",
    "end": "1625290"
  },
  {
    "text": "But now the size of the\nmatrix are important so U is an m by n matrix,\nsigma is an m by n matrix,",
    "start": "1625290",
    "end": "1633740"
  },
  {
    "text": "and V is an n by n matrix. That just denotes the size,\nthe dimensions of the matrix.",
    "start": "1633740",
    "end": "1641010"
  },
  {
    "text": "So what does it mean for an\nm by n matrix to be diagonal? It just means the same thing.",
    "start": "1641010",
    "end": "1647410"
  },
  {
    "text": "So only the (i,i) entries\nare allowed to be nonzero. ",
    "start": "1647410",
    "end": "1659760"
  },
  {
    "text": "So that was just\na bunch of words. So let me rephrase this. ",
    "start": "1659760",
    "end": "1672370"
  },
  {
    "text": "So let me compare now eigenvalue\ndecomposition, with singular value decomposition.",
    "start": "1672370",
    "end": "1678060"
  },
  {
    "text": "So this is EVD, what\nwe just saw before.",
    "start": "1678060",
    "end": "1683370"
  },
  {
    "text": "It only-- SVD. This only works for\nn by n matrices,",
    "start": "1683370",
    "end": "1689259"
  },
  {
    "text": "which are diagonalizable. ",
    "start": "1689259",
    "end": "1695260"
  },
  {
    "text": "SVD works for all\ngeneral m by n matrices. ",
    "start": "1695260",
    "end": "1703470"
  },
  {
    "text": "However, this is powerful. Because it gives you one frame.",
    "start": "1703470",
    "end": "1708950"
  },
  {
    "text": "So v_1 with a v_2, v_3 for which\nA acts as a scaling operator.",
    "start": "1708950",
    "end": "1721507"
  },
  {
    "text": "Kind of like that. That's what A does,\nA does, A does. ",
    "start": "1721508",
    "end": "1729140"
  },
  {
    "text": "That's because these U on\nthe both sides are equal. However, for singular\nvalue decomposition,",
    "start": "1729140",
    "end": "1737766"
  },
  {
    "text": "this is called singular\nvalue decomposition. I just erased It. ",
    "start": "1737766",
    "end": "1748750"
  },
  {
    "text": "What you have instead\nis first of all, the spaces are different. If you take a vector in\nR^m, and bring it to R^n,",
    "start": "1748750",
    "end": "1762358"
  },
  {
    "text": "apply this operation A. What's\ngoing to happen here is there will be one frame in here,\nand one frame in here.",
    "start": "1762358",
    "end": "1768789"
  },
  {
    "text": "So there will be vectors\nv_1, v_2, v_3, v_4 like this.",
    "start": "1768790",
    "end": "1776430"
  },
  {
    "text": "And there will be vectors\nu_1, u_2, u_3 like this here.",
    "start": "1776430",
    "end": "1784300"
  },
  {
    "text": "And what's going to happen\nis when you take v_1, A will take v_1 to u_1\nand scale it a little bit",
    "start": "1784300",
    "end": "1792800"
  },
  {
    "text": "according to that diagonal. A will take v_2 to\nu_2, it will scale it.",
    "start": "1792800",
    "end": "1798420"
  },
  {
    "text": "It'll take v_3 to u_3, scale it. Wait a minute. But for v_4, we don't have u_4.",
    "start": "1798420",
    "end": "1805100"
  },
  {
    "text": "What's going to happen is this\nis just going to disappear. u_4, when applied\nA, will disappear.",
    "start": "1805100",
    "end": "1811509"
  },
  {
    "text": "So I know it's a very\nvague explanation, but this geometric picture,\ntry to compare them.",
    "start": "1811510",
    "end": "1818320"
  },
  {
    "text": "A diagonalization,\neigenvalue decomposition, works within its frame, so\nit's very, very powerful.",
    "start": "1818320",
    "end": "1825350"
  },
  {
    "text": "You just have some directions\nand you scale those directions. But the singular\nvalue composition",
    "start": "1825350",
    "end": "1831450"
  },
  {
    "text": "it's applicable to a more\ngeneral class of matrices, but it's rather more restricted.",
    "start": "1831450",
    "end": "1836750"
  },
  {
    "text": "You have two frames, one\nfor the original space, one for the target space. And what the linear\ntransformation does is,",
    "start": "1836750",
    "end": "1843320"
  },
  {
    "text": "it just sends one\nvector to another vector and scales it a little bit.",
    "start": "1843320",
    "end": "1849770"
  },
  {
    "text": " So now is another\ngood time to go back",
    "start": "1849770",
    "end": "1859148"
  },
  {
    "text": "to that matrix in\nthe very beginning. ",
    "start": "1859149",
    "end": "1872519"
  },
  {
    "text": "So remember that example where\nwe had a vector of companies,",
    "start": "1872520",
    "end": "1882714"
  },
  {
    "text": "and dates, and the\nentry was stock prices. ",
    "start": "1882714",
    "end": "1897400"
  },
  {
    "text": "So if it's an n by\nn matrix, you can try to apply both\neigenvalue decomposition,",
    "start": "1897400",
    "end": "1902940"
  },
  {
    "text": "and singular value\ndecomposition. But what will be more sensible\nis singular value decomposition",
    "start": "1902940",
    "end": "1908230"
  },
  {
    "text": "in this case. I won't explain why, and\nwhat's happening here. Peter will probably.",
    "start": "1908230",
    "end": "1916170"
  },
  {
    "text": "You will come to it later. But just try to do some\nimagining before listening",
    "start": "1916170",
    "end": "1921539"
  },
  {
    "text": "what's really happening\nin real world. So try to use your own\nimagination, your own language",
    "start": "1921540",
    "end": "1927379"
  },
  {
    "text": "to express. See what happens for\nthis matrix, what this decomposition is doing.",
    "start": "1927380",
    "end": "1932429"
  },
  {
    "start": "1932430",
    "end": "1940010"
  },
  {
    "text": "It just looks like\ntotally nonsense. Why does this have\neven a geometry?",
    "start": "1940010",
    "end": "1946630"
  },
  {
    "text": "Why does it define a linear\ntransformation and so on? ",
    "start": "1946630",
    "end": "1952440"
  },
  {
    "text": "It's just a beautiful\ntheory, which just gives many useful information. I can't really emphasize more.",
    "start": "1952440",
    "end": "1958750"
  },
  {
    "text": "Because-- emphasize\nenough, because really this is just universal, being\nused in all science, these.",
    "start": "1958750",
    "end": "1966010"
  },
  {
    "text": "I think the eigenvalue\ndecomposition, and the singular value decomposition. Not just for this\ncourse, but pretty much",
    "start": "1966010",
    "end": "1973559"
  },
  {
    "text": "it's safe to say in\nevery engineering, you'll encounter\none of the forms. ",
    "start": "1973560",
    "end": "1980150"
  },
  {
    "text": "So let me talk about the\nproof of the singular value",
    "start": "1980150",
    "end": "1985560"
  },
  {
    "text": "decomposition. And I will show you an\nexample of what singular value",
    "start": "1985560",
    "end": "1991120"
  },
  {
    "text": "decomposition does for some\nexample matrix, the matrix that I chose.",
    "start": "1991120",
    "end": "1997760"
  },
  {
    "text": "Proof of singular\nvalue decomposition,",
    "start": "1997760",
    "end": "2005665"
  },
  {
    "text": "which is interesting. It relies on eigenvalue\ndecomposition. ",
    "start": "2005665",
    "end": "2011030"
  },
  {
    "text": "So given a matrix A, consider\nthe eigenvalues of A times",
    "start": "2011030",
    "end": "2037125"
  },
  {
    "text": "A transpose. ",
    "start": "2037125",
    "end": "2044910"
  },
  {
    "text": "Oh, A transpose A. First\nobservation: that's",
    "start": "2044910",
    "end": "2057023"
  },
  {
    "text": "a symmetric matrix. ",
    "start": "2057024",
    "end": "2066169"
  },
  {
    "text": "So if you remember, it\nwill have real eigenvalues, and it's diagonalizable. ",
    "start": "2066170",
    "end": "2075109"
  },
  {
    "text": "So A^T of A has eigenvalues\nlambda_1, lambda_2,",
    "start": "2075110",
    "end": "2091356"
  },
  {
    "text": "up to, it's an n by n\nmatrix, so lambda_n.",
    "start": "2091356",
    "end": "2097326"
  },
  {
    "text": " And corresponding eigenvectors\nv_1, v_2, up to v_n.",
    "start": "2097326",
    "end": "2109387"
  },
  {
    "text": " And so for convenience, I\nwill cut it at lambda_r,",
    "start": "2109387",
    "end": "2118110"
  },
  {
    "text": "and assume all rest is 0. So there might be\nnone which are 0.",
    "start": "2118110",
    "end": "2123690"
  },
  {
    "text": "In that case we use\nall the eigenvalues. But I only am interested\nin nonzero eigenvalues.",
    "start": "2123690",
    "end": "2129850"
  },
  {
    "text": "So I'll say up to\nlambda_r, they're nonzero. Afterwards it's 0.",
    "start": "2129850",
    "end": "2135710"
  },
  {
    "text": "It's just a notational choice.  And now I'm just\ngoing to make a claim",
    "start": "2135710",
    "end": "2141950"
  },
  {
    "text": "that they're all positive. This part is kind\nof just believe me.",
    "start": "2141950",
    "end": "2149300"
  },
  {
    "text": " Then if that's the case, we\ncan rewrite the eigenvalues.",
    "start": "2149300",
    "end": "2156400"
  },
  {
    "text": "Rewrite eigenvalues as\nsigma_1^2, sigma_2^2,",
    "start": "2156400",
    "end": "2166609"
  },
  {
    "text": "sigma_r^2, and 0. ",
    "start": "2166610",
    "end": "2175610"
  },
  {
    "text": "That was my first step. My second step,\nthat was step one,",
    "start": "2175610",
    "end": "2181856"
  },
  {
    "text": "step two is to define\nu_1 as A*v_1 / sigma_1,",
    "start": "2181856",
    "end": "2190770"
  },
  {
    "text": "u_2 as A*v_2 / sigma_2.  And u_r as A*V_r / sigma_r.",
    "start": "2190770",
    "end": "2197950"
  },
  {
    "text": " And then u times r+1\nas-- up to u times m,",
    "start": "2197950",
    "end": "2209240"
  },
  {
    "text": "as complete the\nabove into a basis.",
    "start": "2209240",
    "end": "2216390"
  },
  {
    "start": "2216390",
    "end": "2222589"
  },
  {
    "text": "So for those who\ndon't understand, just think of we pick u_1\nup to u_r first, and then",
    "start": "2222590",
    "end": "2227700"
  },
  {
    "text": "arbitrarily pick the rest. And you'll see why I only care\nabout the nonzero eigenvalues.",
    "start": "2227700",
    "end": "2234890"
  },
  {
    "text": "Because I have to divide by\nsigmas, the sigma values. And if it's zero, I\ncan't do the division.",
    "start": "2234890",
    "end": "2242330"
  },
  {
    "text": "So that's why I identified\nthose which are not zero.  And then we're done.",
    "start": "2242330",
    "end": "2247650"
  },
  {
    "text": " So it doesn't look at\nall like we're done.",
    "start": "2247650",
    "end": "2252684"
  },
  {
    "text": "But I'm going to let my U be\nthis, u_1, u_2, up to u_n.",
    "start": "2252685",
    "end": "2261410"
  },
  {
    "text": " Sorry, it has to be n. ",
    "start": "2261410",
    "end": "2268650"
  },
  {
    "text": "My V I will pick as\nv_1, v_2, up to v_r.",
    "start": "2268650",
    "end": "2277109"
  },
  {
    "text": "And then v_(r+1) up to v_n. So this again just\ncomplete into a basis.",
    "start": "2277110",
    "end": "2283130"
  },
  {
    "start": "2283130",
    "end": "2295575"
  },
  {
    "text": "Now let's see what happens. ",
    "start": "2295575",
    "end": "2307960"
  },
  {
    "text": "So A times U transpose\ntimes V. Oh, ah.",
    "start": "2307960",
    "end": "2313772"
  },
  {
    "text": "That's why it's a problem. ",
    "start": "2313772",
    "end": "2319137"
  },
  {
    "text": "You have to do U times\nA times V transpose. So I would write V\nis n, and this is m.",
    "start": "2319137",
    "end": "2329290"
  },
  {
    "start": "2329290",
    "end": "2360500"
  },
  {
    "text": "Ah yes, so U times A\ntimes V transpose here. That will be u_1, u_2, u_m.",
    "start": "2360500",
    "end": "2371079"
  },
  {
    "text": "A. V transpose will be v_1\ntranspose, v_2 transpose,",
    "start": "2371080",
    "end": "2377865"
  },
  {
    "text": "to v_n transpose. ",
    "start": "2377865",
    "end": "2403605"
  },
  {
    "text": "I messed up something. Sorry. ",
    "start": "2403605",
    "end": "2416180"
  },
  {
    "text": "Oh, that's the\nform I want, right? Yeah. So I have to transpose\nU and V there.",
    "start": "2416180",
    "end": "2423760"
  },
  {
    "text": "OK, sorry.  Thank you. Thank you for the correction.",
    "start": "2423760",
    "end": "2429810"
  },
  {
    "text": "I know this looks\ndifferent from that. But I mean if you flip the\ndefinition it will be the same.",
    "start": "2429810",
    "end": "2436380"
  },
  {
    "text": "So I'll just not--\nstop making mistakes. Do you have a question? So, yeah.",
    "start": "2436380",
    "end": "2442170"
  },
  {
    "text": "Thank you. ",
    "start": "2442170",
    "end": "2448549"
  },
  {
    "text": "Yeah. That will make more sense. ",
    "start": "2448550",
    "end": "2455900"
  },
  {
    "text": "Thank you very much. And then you're going\nto have u_1 transpose up",
    "start": "2455900",
    "end": "2461603"
  },
  {
    "text": "to u_n transpose. A times V, because of\nthe definition of V,",
    "start": "2461603",
    "end": "2468300"
  },
  {
    "text": "will be lambda_1 of v_1. A times v_2 will\nbe lambda_2 of v_2.",
    "start": "2468300",
    "end": "2473990"
  },
  {
    "text": "Up to lambda_r of v_r,\nand the rest will be zero.",
    "start": "2473990",
    "end": "2479010"
  },
  {
    "text": "These all define the columns. ",
    "start": "2479010",
    "end": "2493874"
  },
  {
    "text": "Now let's do a few computations.",
    "start": "2493874",
    "end": "2507200"
  },
  {
    "text": "So u_1^T times lambda_1 v_1. u_1^T, and lambda_1 v_1.",
    "start": "2507200",
    "end": "2514050"
  },
  {
    "text": "When you take the dot product,\nwhat you're going to get is",
    "start": "2514050",
    "end": "2519140"
  },
  {
    "text": "v_1^T A transpose\nof v_1 lambda_1.",
    "start": "2519140",
    "end": "2525441"
  },
  {
    "start": "2525441",
    "end": "2533392"
  },
  {
    "text": "I'm missing something. ",
    "start": "2533393",
    "end": "2545850"
  },
  {
    "text": "Ah, sorry about that. This is not right. These are As.",
    "start": "2545850",
    "end": "2550855"
  },
  {
    "text": " I defined the eigenvalues\nfor A transpose A.",
    "start": "2550855",
    "end": "2567799"
  },
  {
    "text": "Then that's u_1 transpose\ntimes sigma_1 times u_1. That will be sigma_1.",
    "start": "2567800",
    "end": "2574760"
  },
  {
    "text": " And then if you look at the\nsecond entry, u_1 transpose",
    "start": "2574760",
    "end": "2581960"
  },
  {
    "text": "times A v_2, you get u_1\ntranspose times sigma_2 of u_2.",
    "start": "2581960",
    "end": "2591369"
  },
  {
    "text": " But I claim that\nthis is equal to 0.",
    "start": "2591370",
    "end": "2598430"
  },
  {
    "text": "So why is that the case? u_1 transpose is\nequal to V_1 transpose A transpose over sigma_1.",
    "start": "2598430",
    "end": "2606316"
  },
  {
    "text": "And we have sigma_2. u_2 is equal to A\ntimes v_2 over sigma_2.",
    "start": "2606316",
    "end": "2612876"
  },
  {
    "text": "So those two cancel. And we have v_1^T A^T\nA v_2 over sigma_1.",
    "start": "2612876",
    "end": "2622230"
  },
  {
    "text": "But v_1 and v_2 are two\ndifferent eigenvectors of this matrix.",
    "start": "2622230",
    "end": "2628160"
  },
  {
    "text": "At the beginning we can have an\northonormal decomposition of A transpose A. That means v_1^T\ntimes v_2 times that has to be",
    "start": "2628160",
    "end": "2639240"
  },
  {
    "text": "equal to zero. Because that's an eigenvalue. We have v_1^T times\nlambda_2 v_2 over sigma_1.",
    "start": "2639240",
    "end": "2649658"
  },
  {
    "text": "So we have lambda_2 over\nsigma_1 times v_1 transpose v_2. These two are\northogonal so give 0.",
    "start": "2649658",
    "end": "2658410"
  },
  {
    "text": "So if you do the\ncomputation, what you're going to have\nis sigma_1, sigma_2",
    "start": "2658410",
    "end": "2663560"
  },
  {
    "text": "on the diagonal, up to\nsigma_r, and then 0, 0 rest. And 0 the rest.",
    "start": "2663560",
    "end": "2672450"
  },
  {
    "text": "Sorry for the confusion. Actually the process\nis quite simple. I was just lost in the\ncomputation in the middle.",
    "start": "2672450",
    "end": "2679880"
  },
  {
    "text": "So process is first\nlook at A transpose A. Find the eigenvalues\nand eigenvectors.",
    "start": "2679880",
    "end": "2687450"
  },
  {
    "text": "And using those, they\ndefine the matrix V.",
    "start": "2687450",
    "end": "2693070"
  },
  {
    "text": "And you can define the\nmatrix U by applying A times V over sigma.",
    "start": "2693070",
    "end": "2698280"
  },
  {
    "text": "Each of those will\ndefine the entries of U. The reason I wanted to\ngo through this proof",
    "start": "2698280",
    "end": "2703900"
  },
  {
    "text": "is because this gives you a\nprocess of finding a singular value decomposition.",
    "start": "2703900",
    "end": "2709830"
  },
  {
    "text": "It was a little\nbit painful for me. But if you have a matrix\nthere's just these simple steps",
    "start": "2709830",
    "end": "2717069"
  },
  {
    "text": "you can follow to find the\nsingular value decomposition. So look at this matrix, find its\neigenvalues and eigenvectors.",
    "start": "2717070",
    "end": "2725460"
  },
  {
    "text": "Just arrange it\nin the right way. Of course, the right\nway needs some practice",
    "start": "2725460",
    "end": "2730490"
  },
  {
    "text": "to be done correctly. But once you do that, you\njust obtain a singular value composition.",
    "start": "2730490",
    "end": "2736750"
  },
  {
    "text": "And really I can't explain\nhow powerful it is. You will only later\nsee it in the course how powerful this\ndecomposition will be.",
    "start": "2736750",
    "end": "2743720"
  },
  {
    "text": "And only then you'll\nmore appreciate how good it is to have\nthis decomposition,",
    "start": "2743720",
    "end": "2748900"
  },
  {
    "text": "and be able to\ncompute it so simply. So let's try to do it by hand.",
    "start": "2748900",
    "end": "2756598"
  },
  {
    "text": "Yes? STUDENT: So when you\ncompute the [INAUDIBLE]. ",
    "start": "2756598",
    "end": "2765155"
  },
  {
    "text": "PROFESSOR: Yes. STUDENT: [INAUDIBLE]  PROFESSOR: It would have\nto be orthonormal, yeah.",
    "start": "2765155",
    "end": "2772490"
  },
  {
    "text": "It should be orthonormal. These should be orthonormal. ",
    "start": "2772490",
    "end": "2778238"
  },
  {
    "text": "These also.  And that's a good point,\nbecause that can be annoying",
    "start": "2778238",
    "end": "2785480"
  },
  {
    "text": "when you want to do it by hand. Actually this decomposition. You have to do some Gram-Schmidt\nprocess or something like that.",
    "start": "2785480",
    "end": "2791800"
  },
  {
    "text": " What I mean by\nhand, I don't really",
    "start": "2791800",
    "end": "2797085"
  },
  {
    "text": "mean by hand, other than\nwhen you're doing homework. Because you can use\nthe computer to do it.",
    "start": "2797085",
    "end": "2804030"
  },
  {
    "text": "And in fact, if you\nuse computer there are much better algorithms\nthan this that are known,",
    "start": "2804030",
    "end": "2809314"
  },
  {
    "text": "which can do this a lot more\nquickly and more efficiently. ",
    "start": "2809314",
    "end": "2815140"
  },
  {
    "text": "So let's try to do it by hand. ",
    "start": "2815140",
    "end": "2825849"
  },
  {
    "text": "So let A be this matrix:\n[3, 2  2; 2, 3, -2].",
    "start": "2825850",
    "end": "2836280"
  },
  {
    "text": "And we want to make the\neigenvalue decomposition of this.",
    "start": "2836280",
    "end": "2842079"
  },
  {
    "text": "A transpose A, we\nhave to compute that, is [3, 2, 2; 2, 3, -2].",
    "start": "2842080",
    "end": "2849180"
  },
  {
    "start": "2849180",
    "end": "2859044"
  },
  {
    "text": "And you will get [13, 12, 2; 12,\n 13, -2; 2, -2, 8].",
    "start": "2859044",
    "end": "2872824"
  },
  {
    "start": "2872824",
    "end": "2883920"
  },
  {
    "text": "And let me just say that the\neigenvalues are 0, 9, and 25.",
    "start": "2883920",
    "end": "2892672"
  },
  {
    "text": " So in this algorithm,\nsigma_1^2 will be 25.",
    "start": "2892672",
    "end": "2900900"
  },
  {
    "text": "Sigma_2^2 squared will be 9. And sigma_3^2 squared will be 0.",
    "start": "2900900",
    "end": "2907250"
  },
  {
    "text": "So we can take sigma_1\nto be 5, sigma_2 to be 3, sigma_3 to be 0. ",
    "start": "2907250",
    "end": "2916930"
  },
  {
    "text": "Now we have to find the\ncorresponding eigenvectors to find the singular\nvalue decomposition.",
    "start": "2916930",
    "end": "2924839"
  },
  {
    "text": "And I'll just do one\njust to remind you how to find an eigenvector.",
    "start": "2924840",
    "end": "2930120"
  },
  {
    "text": "So A transpose A,\nminus 25I is equal to,",
    "start": "2930120",
    "end": "2936345"
  },
  {
    "text": "if you subtract 25\nfrom these entries, you're going to get [-12, 12, 2;\n 12, -12, -2; 2, -2, -13].",
    "start": "2936345",
    "end": "2949784"
  },
  {
    "start": "2949784",
    "end": "2957000"
  },
  {
    "text": "And then you have to\nfind the vector which annihilates this matrix.",
    "start": "2957000",
    "end": "2962060"
  },
  {
    "text": "And that will be, I can take one\nof those vectors to be just 1",
    "start": "2962060",
    "end": "2967213"
  },
  {
    "text": "over square root of 2, 1\nover square root of two, 0, after normalizing.",
    "start": "2967213",
    "end": "2972474"
  },
  {
    "text": " And then just do it\nfor other vectors.",
    "start": "2972475",
    "end": "2978260"
  },
  {
    "start": "2978260",
    "end": "2983430"
  },
  {
    "text": "You find v_2 to be 1 over\nsquare root 18, negative 1",
    "start": "2983430",
    "end": "2992130"
  },
  {
    "text": "over square root 18,\n4 over square root 18.",
    "start": "2992130",
    "end": "2997744"
  },
  {
    "start": "2997744",
    "end": "3011220"
  },
  {
    "text": "Now then find v_3 to be the\none that annihilates this.",
    "start": "3011220",
    "end": "3019020"
  },
  {
    "text": "But I'll just say it's x, y, z. This will not be important. I'll explain why it's\nnot that important.",
    "start": "3019020",
    "end": "3025270"
  },
  {
    "start": "3025270",
    "end": "3035520"
  },
  {
    "text": "Then our v as written\nabove, actually",
    "start": "3035520",
    "end": "3044810"
  },
  {
    "text": "there it was transposed. So I will transpose it. That will be 1 over\nsquare root of 2, 1 over square root of 2, 0.",
    "start": "3044810",
    "end": "3053350"
  },
  {
    "text": "v_2 is that. So we can write 1 over\nsquare root 18, negative 1",
    "start": "3053350",
    "end": "3058609"
  },
  {
    "text": "over square root 18,\n4 over square root 18.",
    "start": "3058610",
    "end": "3063835"
  },
  {
    "text": "And here just write x, y, z. ",
    "start": "3063835",
    "end": "3070484"
  },
  {
    "text": "And U will be defined\nas u_1 and u_2,",
    "start": "3070485",
    "end": "3077210"
  },
  {
    "text": "where u_1 is A times\nv_1 over sigma_1. u_2 is A times v_2 over sigma_2.",
    "start": "3077210",
    "end": "3086610"
  },
  {
    "text": "So multiply A by this\nvector, divide by sigma_1 to get U. I already did\nthe computation for you.",
    "start": "3086610",
    "end": "3094414"
  },
  {
    "text": "It's going to be-- and\nthis is going to be-- yes?",
    "start": "3094415",
    "end": "3104810"
  },
  {
    "text": "STUDENT: How did you get v_1? PROFESSOR: v_1? So if you did the computation\nright in the beginning to get",
    "start": "3104810",
    "end": "3110980"
  },
  {
    "text": "the eigenvalues, then A^T\nA - 25I, this has to be--",
    "start": "3110980",
    "end": "3118560"
  },
  {
    "text": "has to not have full rank. So there has to be a vector v,\nwhich when multiplied by this",
    "start": "3118560",
    "end": "3123770"
  },
  {
    "text": "gives [0, 0, 0] vector. And then you say [a, b, c]\nand set it equal to [0, 0, 0].",
    "start": "3123770",
    "end": "3133670"
  },
  {
    "text": "And just solve the system\nof linear equations. There will be several of them. For example, we can\ntake [1, 1,  0] as well.",
    "start": "3133670",
    "end": "3140840"
  },
  {
    "text": "But I just normalized\nit to have [INAUDIBLE]. So there's a lot\nof work involved",
    "start": "3140840",
    "end": "3147349"
  },
  {
    "text": "if you want to do it by hand,\neven though you can do it. You have to find eigenvalues,\nfind eigenvectors.",
    "start": "3147350",
    "end": "3152630"
  },
  {
    "text": "In this case, you have\nto find three of them. And then you have to do\nmore work, and more work. But it can be done.",
    "start": "3152630",
    "end": "3159610"
  },
  {
    "text": "And we are done now. So now this decomposes A into\nU sigma V transformation.",
    "start": "3159610",
    "end": "3172020"
  },
  {
    "text": "So U is given as [1 over square\nroot 2, 1 over square root 2;",
    "start": "3172020",
    "end": "3177770"
  },
  {
    "text": "1 over square root 2, minus\n1 over square root 2]. Sigma was 5, 3, 0.",
    "start": "3177770",
    "end": "3187716"
  },
  {
    "text": " And V is this.",
    "start": "3187716",
    "end": "3195490"
  },
  {
    "text": "So V transpose is just\ntranspose of that. I'll just write it like\nthat, where V is that.",
    "start": "3195490",
    "end": "3202790"
  },
  {
    "text": "So we have this decomposition. And so let me actually write\nit, because I want to show you",
    "start": "3202790",
    "end": "3208369"
  },
  {
    "text": "why x, y, z is not important.  1 over square root 2,\n1 over square root 2,",
    "start": "3208370",
    "end": "3217010"
  },
  {
    "text": "0; 1 over square root 18,\nminus 1 over square root 18,",
    "start": "3217010",
    "end": "3223250"
  },
  {
    "text": "4 over square root 18; x, y, z. ",
    "start": "3223250",
    "end": "3230599"
  },
  {
    "text": "The reason I'm\nsaying this is not important is because I can just\ndrop-- oh what did I do here?",
    "start": "3230600",
    "end": "3236980"
  },
  {
    "text": "It has to be 2 by 3. I can just drop this column,\nand drop this column together.",
    "start": "3236980",
    "end": "3244369"
  },
  {
    "text": " It has to be that form. ",
    "start": "3244370",
    "end": "3265510"
  },
  {
    "text": "Drop this and drop\nthis altogether. So the message here is that\nthe eigenvectors corresponding",
    "start": "3265510",
    "end": "3273869"
  },
  {
    "text": "to eigenvalue zero\nare not important. The only relevant ones\nare nonzero eigenvalues.",
    "start": "3273870",
    "end": "3281640"
  },
  {
    "text": "So drop this, and drop this. That will save you\nsome computation. So let me state a different\nform of singular value",
    "start": "3281640",
    "end": "3290297"
  },
  {
    "text": "decomposition. ",
    "start": "3290297",
    "end": "3297940"
  },
  {
    "text": "So this works in general. There's a corollary. We get a simplified form of SVD.",
    "start": "3297940",
    "end": "3303075"
  },
  {
    "start": "3303075",
    "end": "3310730"
  },
  {
    "text": "Where A becomes equal to U\ntimes sigma times V transpose.",
    "start": "3310730",
    "end": "3316046"
  },
  {
    "text": " And A was an m by n matrix.",
    "start": "3316046",
    "end": "3321260"
  },
  {
    "text": "U is still an m by m matrix. But now sigma is\nalso m by m matrix.",
    "start": "3321260",
    "end": "3327320"
  },
  {
    "text": "This only works when m is\nless than or equal to n. ",
    "start": "3327320",
    "end": "3333190"
  },
  {
    "text": "And V is a m by n matrix. ",
    "start": "3333190",
    "end": "3338960"
  },
  {
    "text": "So the proof is\nexactly the same. And the last step is just\nto drop the irrelevant",
    "start": "3338960",
    "end": "3344460"
  },
  {
    "text": "information. So I will not write\ndown why it works. But you can see if\nyou go through it,",
    "start": "3344460",
    "end": "3351830"
  },
  {
    "text": "you'll see that\ndropping this part just corresponds to\nexactly that information. ",
    "start": "3351830",
    "end": "3359500"
  },
  {
    "text": "So that's the reduced form. So let's see. In the beginning\nwe had A. I erased",
    "start": "3359500",
    "end": "3366650"
  },
  {
    "text": "A. A was the 2 by 3\nmatrix in the beginning. And we obtained the\ndecomposition into 2 by 2, 2 by 2, and 2 by 3 matrix.",
    "start": "3366650",
    "end": "3375400"
  },
  {
    "text": "If we didn't delete the\nfifth column and fifth row, we would have obtained a 2\nby 2, times 2 by 3, times 3",
    "start": "3375400",
    "end": "3381319"
  },
  {
    "text": "by 3 matrix. But now we can simplify\nit by removing those. ",
    "start": "3381320",
    "end": "3388910"
  },
  {
    "text": "And it might not look that\nmuch different on this board. Because I just erased one row.",
    "start": "3388910",
    "end": "3395080"
  },
  {
    "text": "But many matrices that you'll\nsee in real application have a lot lower rank than the\nnumber of columns and rows.",
    "start": "3395080",
    "end": "3403349"
  },
  {
    "text": "So if r is a lot more smaller\nthan both m and n, then",
    "start": "3403350",
    "end": "3409510"
  },
  {
    "text": "this part really--\nit's not obvious here. But if m and n has\na big gap here,",
    "start": "3409510",
    "end": "3416400"
  },
  {
    "text": "really the number of\ncolumns that you're saving, it can be enormous.",
    "start": "3416400",
    "end": "3421560"
  },
  {
    "text": " So to illustrate an\nexample, look at this.",
    "start": "3421560",
    "end": "3429940"
  },
  {
    "text": "Now look at the\nstock prices, where you have companies and dates.",
    "start": "3429940",
    "end": "3438770"
  },
  {
    "text": "Previously I just gave an\nexample of a 3 by 3 matrix. But it's more sensible\nto have dates, a lot",
    "start": "3438770",
    "end": "3444990"
  },
  {
    "text": "more dates than companies. So let's say you recorded\n365 days of a year,",
    "start": "3444990",
    "end": "3451820"
  },
  {
    "text": "even though the market is\nnot open all days, and just like five companies.",
    "start": "3451820",
    "end": "3458130"
  },
  {
    "text": "If you did a decomposition this\nthis, you'll have a 5 by 5, 5 by 365, 365 by 365 here.",
    "start": "3458130",
    "end": "3465840"
  },
  {
    "text": "But now in the reduced form,\nyou're saving a lot of space. ",
    "start": "3465840",
    "end": "3471725"
  },
  {
    "text": "So if you just\nlook at the board, it doesn't look like\nit's so powerful. But in fact it is. So that's the reduced form.",
    "start": "3471726",
    "end": "3478289"
  },
  {
    "text": "And that will be the\nform that you'll see most of the time, this reduced form. ",
    "start": "3478290",
    "end": "3487350"
  },
  {
    "text": "So I made lot of mistakes today. I have one more topic, but\na totally irrelevant topic.",
    "start": "3487350",
    "end": "3493839"
  },
  {
    "text": "So any questions before I\nmove on to the next topic? ",
    "start": "3493840",
    "end": "3502594"
  },
  {
    "text": "Yes? STUDENT: [INAUDIBLE] ",
    "start": "3502594",
    "end": "3510295"
  },
  {
    "text": "PROFESSOR: Can you\npress the button? ",
    "start": "3510295",
    "end": "3527792"
  },
  {
    "text": "STUDENT: [INAUDIBLE] ",
    "start": "3527792",
    "end": "3537040"
  },
  {
    "text": "PROFESSOR: Oh, so in\nthis data, what it means. You're asking what the\neigenvectors will mean over",
    "start": "3537040",
    "end": "3542769"
  },
  {
    "text": "this data? It will give you some stocks.",
    "start": "3542770",
    "end": "3550960"
  },
  {
    "text": "It will give you\nlike the correlation. So each eigenvector\nwill give you",
    "start": "3550960",
    "end": "3557610"
  },
  {
    "text": "a group of companies that\nare correlated somehow. It measures their\ncorrelation with each other.",
    "start": "3557610",
    "end": "3563550"
  },
  {
    "text": "So I don't have a\nvery good explanation what its physical meaning is. Maybe you can give\njust a little bit more.",
    "start": "3563550",
    "end": "3572040"
  },
  {
    "text": "GUEST SPEAKER: Possibly. We will get into this\nin later lectures. But in the singular\nvalue decomposition,",
    "start": "3572040",
    "end": "3581280"
  },
  {
    "text": "what you want to think of is\nthese orthonormal matrices are really defining a new basis,\nsort of an orthogonal basis.",
    "start": "3581280",
    "end": "3590500"
  },
  {
    "text": "So you're taking the\noriginal coordinate system, then you're rotating it. And without changing\nor stretching",
    "start": "3590500",
    "end": "3597440"
  },
  {
    "text": "or squeezing the data. You're just rotating the axes. So an orthonormal\nmatrix gives you",
    "start": "3597440",
    "end": "3603160"
  },
  {
    "text": "the cosines of the\nnew coordinate system with respect to the old one. And so the singular\nvalue decomposition",
    "start": "3603160",
    "end": "3610002"
  },
  {
    "text": "then is simply sort\nof rotating the data into a different orientation.",
    "start": "3610002",
    "end": "3616020"
  },
  {
    "text": "And the orthonormal basis\nthat you're transforming to,",
    "start": "3616020",
    "end": "3623980"
  },
  {
    "text": "is essentially the coordinates\nof the original data in the transformed system.",
    "start": "3623980",
    "end": "3629930"
  },
  {
    "text": "So as Choongbum was\ncommenting, you're essentially looking at a representation\nof the original data",
    "start": "3629930",
    "end": "3638359"
  },
  {
    "text": "points in a linearly\ntransformed space,",
    "start": "3638360",
    "end": "3643480"
  },
  {
    "text": "and the correlations\nbetween different stocks, say, is represented by how those\npoints are oriented in the new,",
    "start": "3643480",
    "end": "3651990"
  },
  {
    "text": "in the transformed space. ",
    "start": "3651990",
    "end": "3657160"
  },
  {
    "text": "PROFESSOR: So you'll have to see\nreal data to really make sense out of it. ",
    "start": "3657160",
    "end": "3663812"
  },
  {
    "text": "But another way to think of\nit is where it comes from. So all this singular\nvalue decomposition,",
    "start": "3663812",
    "end": "3669202"
  },
  {
    "text": "if you remember\nthe proof, it comes from eigenvectors and\neigenvalues of A transpose A.",
    "start": "3669202",
    "end": "3675849"
  },
  {
    "text": "Now if you look at A\ntranspose A, or I'll just say it's A times A transposed.",
    "start": "3675850",
    "end": "3682460"
  },
  {
    "text": "It's pretty much the same. If you look at A\ntimes A transpose, you're going to get\nan m by n matrix.",
    "start": "3682460",
    "end": "3688425"
  },
  {
    "text": " And it'll be indexed\nboth by these companies.",
    "start": "3688425",
    "end": "3696432"
  },
  {
    "text": " And the numbers\nhere will represent",
    "start": "3696432",
    "end": "3702910"
  },
  {
    "text": "how much the\ncompanies are related to each other, how\nmuch correlation they have between each other.",
    "start": "3702910",
    "end": "3708690"
  },
  {
    "text": "So by looking at the\neigenvectors of this matrix, you're looking at the\ncorrelation between these stock",
    "start": "3708690",
    "end": "3714950"
  },
  {
    "text": "prices, let's say, these\ncompany stock prices. And that information is\nrepresented inside the singular",
    "start": "3714950",
    "end": "3721390"
  },
  {
    "text": "value decomposition. But again, it's a lot\nbetter to understand",
    "start": "3721390",
    "end": "3726609"
  },
  {
    "text": "if you have real\nnumbers and real data, which you will have later. So please be excited and wait.",
    "start": "3726610",
    "end": "3737030"
  },
  {
    "text": "You're going to see\nsome cool stuff. ",
    "start": "3737031",
    "end": "3746110"
  },
  {
    "text": "So that was all for eigenvalue\ndecomposition and singular value decomposition.",
    "start": "3746110",
    "end": "3752650"
  },
  {
    "text": "And the last thing I\nwant to mention today is something called\nPerron-Frobenius theorem.",
    "start": "3752650",
    "end": "3759970"
  },
  {
    "text": "This one even looks a lot\nmore theoretical than the ones I showed you.",
    "start": "3759970",
    "end": "3765319"
  },
  {
    "text": "But surprisingly a few\nyears ago, Steve Ross, he's a faculty in the\nbusiness school here,",
    "start": "3765320",
    "end": "3773550"
  },
  {
    "text": "found a very interesting result\ncalled Steve Ross recovery theorem that makes\nuse of this theorem,",
    "start": "3773550",
    "end": "3781250"
  },
  {
    "text": "makes use of\nPerron-Frobenius theorem that I will tell you today.",
    "start": "3781250",
    "end": "3786410"
  },
  {
    "text": "Unfortunately you will\nonly see a lecture on Steve Ross recovery\ntheorem towards the end",
    "start": "3786410",
    "end": "3791710"
  },
  {
    "text": "of the semester. So I will try to recall\nwhat it is later. But since we're talking\nabout linear algebra today,",
    "start": "3791710",
    "end": "3799110"
  },
  {
    "text": "let me introduce the theorem. This is called Perron-Frobenius. ",
    "start": "3799110",
    "end": "3808040"
  },
  {
    "text": "And you really won't believe\nthat it has any applications in finance because it\njust looks so theoretical.",
    "start": "3808040",
    "end": "3813955"
  },
  {
    "text": " I'm just stating a\nreally weak form.",
    "start": "3813955",
    "end": "3820830"
  },
  {
    "text": "Weak form. Let A be an n by n symmetric\nmatrix, whose entries are all",
    "start": "3820830",
    "end": "3834329"
  },
  {
    "text": "positive, with positive entries. ",
    "start": "3834330",
    "end": "3843790"
  },
  {
    "text": "Then there are a few\nproperties that they have.",
    "start": "3843790",
    "end": "3850910"
  },
  {
    "text": "First there exists\nan eigenvalue, there exists a largest\neigenvalue, lambda_0, such",
    "start": "3850910",
    "end": "3860610"
  },
  {
    "text": "that lambda is\nless than lambda_0. Well that's true for\nall other lambda.",
    "start": "3860610",
    "end": "3871182"
  },
  {
    "text": "So this statement is really\neasy for symmetric matrix. So forget about-- you\ncan drop symmetric,",
    "start": "3871182",
    "end": "3876709"
  },
  {
    "text": "but I'm just stated it,\nbecause I'm going to prove only for this weak case. Just think about the statement\nwhen it's not symmetric.",
    "start": "3876709",
    "end": "3884549"
  },
  {
    "text": "So if you have an n by n matrix\nwhose entries are all positive, then there exists an eigenvalue,\nlambda_0, a real eigenvalue",
    "start": "3884550",
    "end": "3893290"
  },
  {
    "text": "such that the absolute value\nof all of other eigenvalues",
    "start": "3893290",
    "end": "3899170"
  },
  {
    "text": "are strictly smaller\nthan this eigenvalue. So remember that if it's\nnot a symmetric matrix,",
    "start": "3899170",
    "end": "3905540"
  },
  {
    "text": "they can be complex values. This is saying that there's\na unique eigenvalue which has largest absolute value, and\nmoreover, it's a real number.",
    "start": "3905540",
    "end": "3913790"
  },
  {
    "text": " Second part, there\nexists an eigenvector,",
    "start": "3913790",
    "end": "3923260"
  },
  {
    "text": "a positive eigenvector\nwith positive entries,",
    "start": "3923260",
    "end": "3934810"
  },
  {
    "text": "corresponding to lambda 0.",
    "start": "3934810",
    "end": "3940120"
  },
  {
    "text": "So the eigenvector\ncorresponding to this lambda 0 has positive entries.",
    "start": "3940120",
    "end": "3946690"
  },
  {
    "text": "And the third part\nis lambda_0 is an eigenvalue of multiplicity 1,\nfor those who know what it is.",
    "start": "3946690",
    "end": "3965060"
  },
  {
    "text": "So this really is\na unique eigenvalue with a unique eigenvector,\nwhich has positive entries.",
    "start": "3965060",
    "end": "3971580"
  },
  {
    "text": "And it's larger, really\nlarger than other eigenvalues. ",
    "start": "3971580",
    "end": "3977660"
  },
  {
    "text": "So from the mathematician\npoint of view, this has many applications. It's probability theory.",
    "start": "3977660",
    "end": "3983060"
  },
  {
    "text": "My main research area\nis combinatorics, discrete mathematics. It's also used in there.",
    "start": "3983060",
    "end": "3990080"
  },
  {
    "text": "So from the theoretical\npoint of view, this has been used\nin many contexts.",
    "start": "3990080",
    "end": "3995470"
  },
  {
    "text": "It's not a standard theorem\ntaught in linear algebra. So I don't think probably most\nof you haven't seen it before.",
    "start": "3995470",
    "end": "4002990"
  },
  {
    "text": "But it's a well known\nresult, with many uses, theoretical uses.",
    "start": "4002990",
    "end": "4009069"
  },
  {
    "text": "But you also see one use\nin, later, as I mentioned,",
    "start": "4009070",
    "end": "4014700"
  },
  {
    "text": "in finance, which\nis quite surprising. ",
    "start": "4014700",
    "end": "4023740"
  },
  {
    "text": "So let me just give you some\nfeeling of why it happens. I won't give you the full\ndetail of the proof, but just",
    "start": "4023740",
    "end": "4030300"
  },
  {
    "text": "a very brief description. ",
    "start": "4030300",
    "end": "4036257"
  },
  {
    "text": "Sketch when A is symmetric, just\na simple case, A is symmetric.",
    "start": "4036257",
    "end": "4046214"
  },
  {
    "start": "4046214",
    "end": "4052690"
  },
  {
    "text": "In this case, this\nstatement, if you look at it.",
    "start": "4052690",
    "end": "4058540"
  },
  {
    "text": " First of all A has\nreal eigenvalues.",
    "start": "4058540",
    "end": "4064650"
  },
  {
    "start": "4064650",
    "end": "4073539"
  },
  {
    "text": "I'll say it's lambda_1,\nlambda_2, up to lambda_n.",
    "start": "4073540",
    "end": "4079530"
  },
  {
    "text": "And at some point, I'll\nsay up to lambda_i, it's greater than\nzero, pass to where this is smaller than zero.",
    "start": "4079530",
    "end": "4086670"
  },
  {
    "text": "There are some\npositive eigenvalues. There are some\nnegative eigenvalues. So that's observation one.",
    "start": "4086670",
    "end": "4093775"
  },
  {
    "text": " Things are more easy to control,\nbecause they are all real.",
    "start": "4093775",
    "end": "4102089"
  },
  {
    "text": "The first statement says that--\nmaybe I should have indexed it as lambda_0.",
    "start": "4102090",
    "end": "4107383"
  },
  {
    "text": "I'll just call this\nlambda 0 instead. This lambda_0 is in fact\nlarger in absolute value",
    "start": "4107384",
    "end": "4114180"
  },
  {
    "text": "than lambda_n. That's the content\nof the first bullet.",
    "start": "4114180",
    "end": "4122009"
  },
  {
    "text": "So if they all have all\npositive entries, then the positive, largest\npositive eigenvalue",
    "start": "4122010",
    "end": "4127790"
  },
  {
    "text": "dominates the smallest negative\neigenvalue, which yeah.",
    "start": "4127790",
    "end": "4138528"
  },
  {
    "text": "So why is that the case? First of all, to\nsee that you have to go through different steps.",
    "start": "4138529",
    "end": "4145609"
  },
  {
    "text": "So we go into observation two.  Lambda_1, so look at lambda_1.",
    "start": "4145610",
    "end": "4151949"
  },
  {
    "text": "lambda_1 has an eigenvector\nwith positive entries.",
    "start": "4151950",
    "end": "4161234"
  },
  {
    "start": "4161234",
    "end": "4167528"
  },
  {
    "text": "Why is that the case? That's because if\nyou look at A times v",
    "start": "4167529",
    "end": "4175939"
  },
  {
    "text": "equals lambda times v. If\nv-- let me state it this way.",
    "start": "4175939",
    "end": "4189185"
  },
  {
    "text": "Lambda_0 is the maximum\nof all lambda, lambda_0. ",
    "start": "4189185",
    "end": "4201559"
  },
  {
    "text": "That's not entirely correct. Lambda_1. ",
    "start": "4201560",
    "end": "4208985"
  },
  {
    "text": "Sorry about that. So If you look at this, if\nv has non-positive entries,",
    "start": "4208985",
    "end": "4214610"
  },
  {
    "text": "if it has a negative entry,\nif v has a negative entry,",
    "start": "4214610",
    "end": "4223750"
  },
  {
    "text": "then flip it. Flip the sign, and in this\nway obtain new vector v prime.",
    "start": "4223750",
    "end": "4234346"
  },
  {
    "text": " Since A has positive entries,\nA has positive entries.",
    "start": "4234346",
    "end": "4243070"
  },
  {
    "text": " What we conclude\nis that A times v",
    "start": "4243070",
    "end": "4249960"
  },
  {
    "text": "prime will be larger than A\ntimes v. You have to look.",
    "start": "4249960",
    "end": "4258590"
  },
  {
    "text": "Think about, because it\nhas positive entries, if it had some negative\npart somewhere, the magnitude will decrease.",
    "start": "4258590",
    "end": "4265119"
  },
  {
    "text": "So if you flip the sign it\nshould increase the magnitude. And this cannot happen.",
    "start": "4265120",
    "end": "4271720"
  },
  {
    "text": "This shouldn't happen. This should not happen. ",
    "start": "4271720",
    "end": "4280330"
  },
  {
    "text": "That's where the positive\nentries part is used. If you have positive\nentries, then it should have,",
    "start": "4280330",
    "end": "4289329"
  },
  {
    "text": "the eigenvector should have\npositive entries as well. So I will not work through\nthe details of the rest.",
    "start": "4289330",
    "end": "4298420"
  },
  {
    "text": "I will post it on\nthe lecture notes. But really this\ntheorem, in fact,",
    "start": "4298420",
    "end": "4304369"
  },
  {
    "text": "can be stated in a lot\nmore generality than this. I'm stating only\na very weak form. It doesn't have to have\nall positive entries.",
    "start": "4304369",
    "end": "4310630"
  },
  {
    "text": "It has to only be something\ncalled irreducible, which is a concept from\nprobability theory,",
    "start": "4310630",
    "end": "4316640"
  },
  {
    "text": "from Markov chains.  But here we will only\nuse it in this setting.",
    "start": "4316640",
    "end": "4324810"
  },
  {
    "text": "So I will review it later,\nbefore it's really being used. But just remember that how\nthese positive entries kick",
    "start": "4324810",
    "end": "4331219"
  },
  {
    "text": "into this kind of\nstatement, where there is an eigenvalue,\nlargest eigenvalue, why",
    "start": "4331220",
    "end": "4336290"
  },
  {
    "text": "there has to be a vector\nwhich is all positive entries.",
    "start": "4336290",
    "end": "4341380"
  },
  {
    "text": "Those will all come\ninto play later. So I think that's it for today.",
    "start": "4341380",
    "end": "4347272"
  },
  {
    "text": "If you have any last\nminute questions? ",
    "start": "4347272",
    "end": "4353450"
  },
  {
    "text": "If not, I will see\nyou on Thursday.",
    "start": "4353450",
    "end": "4356650"
  }
]