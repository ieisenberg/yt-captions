[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6910"
  },
  {
    "text": "offer high-quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6910",
    "end": "13460"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at",
    "start": "13460",
    "end": "19290"
  },
  {
    "text": "ocw.mit.edu.  PROFESSOR: OK, good morning.",
    "start": "19290",
    "end": "25310"
  },
  {
    "start": "22000",
    "end": "88000"
  },
  {
    "text": "So today, we're going to have\na fairly packed lecture.",
    "start": "25310",
    "end": "30930"
  },
  {
    "text": "We are going to conclude\nwith chapter two, discrete random variables. And we will be talking\nmostly about",
    "start": "30930",
    "end": "37140"
  },
  {
    "text": "multiple random variables. And this is also the\nlast lecture as far",
    "start": "37140",
    "end": "43059"
  },
  {
    "text": "as quiz one is concerned. So it's going to cover the\nmaterial until today, and of",
    "start": "43060",
    "end": "48350"
  },
  {
    "text": "course the next recitation\nand tutorial as well. OK, so we're going to review\nquickly what we introduced at",
    "start": "48350",
    "end": "57170"
  },
  {
    "text": "the end of last lecture, where\nwe talked about the joint PMF of two random variables.",
    "start": "57170",
    "end": "62300"
  },
  {
    "text": "We're going to talk about the\ncase of more than two random variables as well.",
    "start": "62300",
    "end": "67440"
  },
  {
    "text": "We're going to talk about\nthe familiar concepts of conditioning and independence,\nbut applied to random",
    "start": "67440",
    "end": "74300"
  },
  {
    "text": "variables instead of events. We're going to look at the\nexpectations once more, talk",
    "start": "74300",
    "end": "79320"
  },
  {
    "text": "about a few properties that they\nhave, and then solve a couple of problems and calculate\na few things in",
    "start": "79320",
    "end": "85900"
  },
  {
    "text": "somewhat clever ways. So the first point I want to\nmake is that, to a large",
    "start": "85900",
    "end": "91790"
  },
  {
    "start": "88000",
    "end": "150000"
  },
  {
    "text": "extent, whatever is happening\nin our chapter on discrete random variables is just an\nexercise in notation.",
    "start": "91790",
    "end": "99160"
  },
  {
    "text": "There is stuff and concepts that\nyou are already familiar with-- probabilities,\nprobabilities of two things",
    "start": "99160",
    "end": "105229"
  },
  {
    "text": "happening, conditional\nprobabilities. And all that we're doing, to\nsome extent, is rewriting",
    "start": "105230",
    "end": "111759"
  },
  {
    "text": "those familiar concepts\nin new notation. So for example, this\nis the joint PMF",
    "start": "111760",
    "end": "117810"
  },
  {
    "text": "of two random variable. It gives us, for any pair or\npossible values of those random variables, the\nprobability that that pair",
    "start": "117810",
    "end": "125510"
  },
  {
    "text": "occurs simultaneously. So it's the probability that\nsimultaneously x takes that value, and y takes\nthat other value.",
    "start": "125510",
    "end": "133580"
  },
  {
    "text": "And similarly, we have the\nnotion of the conditional PMF, which is just a list of the --\ncondition of -- the various",
    "start": "133580",
    "end": "141060"
  },
  {
    "text": "conditional probabilities\nof interest, conditional probability that one random\nvariable takes this value",
    "start": "141060",
    "end": "146450"
  },
  {
    "text": "given that the other random\nvariable takes that value. Now, a remark about conditional\nprobabilities.",
    "start": "146450",
    "end": "153640"
  },
  {
    "start": "150000",
    "end": "258000"
  },
  {
    "text": "Conditional probabilities\ngenerally are like ordinary probabilities. You condition on something\nparticular.",
    "start": "153640",
    "end": "160170"
  },
  {
    "text": "So here we condition\non a particular y. So think of little y as\na fixed quantity.",
    "start": "160170",
    "end": "166580"
  },
  {
    "text": "And then look at this\nas a function of x. So given that y, which we\ncondition on, given our new",
    "start": "166580",
    "end": "174430"
  },
  {
    "text": "universe, we're considering the\nvarious possibilities for x and the probabilities\nthat they have.",
    "start": "174430",
    "end": "181290"
  },
  {
    "text": "Now, the probabilities over\nall x's, of course, needs to add to 1. So we should have a relation\nof this kind.",
    "start": "181290",
    "end": "191530"
  },
  {
    "text": "So they're just like ordinary\nprobabilities over the different x's in a universe\nwhere we are told the value of",
    "start": "191530",
    "end": "198230"
  },
  {
    "text": "the random variable y. Now, how are these related? ",
    "start": "198230",
    "end": "205200"
  },
  {
    "text": "So we call these the marginal,\nthese the joint, these the conditional. And there are some relations\nbetween these.",
    "start": "205200",
    "end": "211510"
  },
  {
    "text": "For example, to find the\nmarginal from the joint, it's pretty straightforward.",
    "start": "211510",
    "end": "217730"
  },
  {
    "text": "The probability that x takes a\nparticular value is the sum of the probabilities of all of the\ndifferent ways that this",
    "start": "217730",
    "end": "225030"
  },
  {
    "text": "particular value may occur. What are the different ways? Well, it may occur together with\na certain y, or together",
    "start": "225030",
    "end": "231910"
  },
  {
    "text": "with some other y, or together\nwith some other y. So you look at all the possible\ny's that can go",
    "start": "231910",
    "end": "238030"
  },
  {
    "text": "together with this x, and add\nthe probabilities of all of those pairs for which we get\nthis particular value of x.",
    "start": "238030",
    "end": "247220"
  },
  {
    "text": "And then there's a relation\nbetween that connects these",
    "start": "247220",
    "end": "253120"
  },
  {
    "text": "two probabilities with the\nconditional probability. And it's this relation.",
    "start": "253120",
    "end": "258630"
  },
  {
    "start": "258000",
    "end": "331000"
  },
  {
    "text": "It's nothing new. It's just new notation for\nwriting what we already know,",
    "start": "258630",
    "end": "265160"
  },
  {
    "text": "that the probability of two\nthings happening is the probability that the first thing\nhappens, and then given",
    "start": "265160",
    "end": "271460"
  },
  {
    "text": "that the first thing happens,\nthe probability that the second one happened. So how do we go from\none to the other?",
    "start": "271460",
    "end": "279050"
  },
  {
    "text": "Think of A as being the event\nthat X takes the value, little x, and B being the event that\nY takes the value, little y.",
    "start": "279050",
    "end": "289120"
  },
  {
    "text": "So the joint probability is the\nprobability that these two things happen simultaneously.",
    "start": "289120",
    "end": "294220"
  },
  {
    "text": "It's the probability that X\ntakes this value times the conditional probability that Y\ntakes this value, given that X",
    "start": "294220",
    "end": "303280"
  },
  {
    "text": "took that first value. So it's the familiar\nmultiplication rule, but just",
    "start": "303280",
    "end": "308470"
  },
  {
    "text": "transcribed in our\nnew notation. So nothing new so far.",
    "start": "308470",
    "end": "313690"
  },
  {
    "text": "OK, why did we go through this\nexercise and this notation? It's because in the experiments\nwhere we're",
    "start": "313690",
    "end": "319980"
  },
  {
    "text": "interested in the real world,\ntypically there's going to be lots of uncertain quantities. There's going to be multiple\nrandom variables.",
    "start": "319980",
    "end": "327150"
  },
  {
    "text": "And we want to be able to talk\nabout them simultaneously. Okay. Why two and not more than two?",
    "start": "327150",
    "end": "335110"
  },
  {
    "start": "331000",
    "end": "442000"
  },
  {
    "text": "How about three random\nvariables? Well, if you understand what's\ngoing on in this slide, you",
    "start": "335110",
    "end": "341290"
  },
  {
    "text": "should be able to kind of\nautomatically generalize this to the case of multiple\nrandom variables.",
    "start": "341290",
    "end": "348260"
  },
  {
    "text": "So for example, if we have three\nrandom variables, X, Y, and Z, and you see an expression\nlike this, it",
    "start": "348260",
    "end": "356720"
  },
  {
    "text": "should be clear what it means. It's the probability that\nX takes this value and",
    "start": "356720",
    "end": "362070"
  },
  {
    "text": "simultaneously Y takes that\nvalue and simultaneously Z takes that value.",
    "start": "362070",
    "end": "367764"
  },
  {
    "text": "I guess that's an uppercase Z\nhere, that's a lowercase z.",
    "start": "367765",
    "end": "373280"
  },
  {
    "text": "And if I ask you to find the\nmarginal of X, if I tell you",
    "start": "373280",
    "end": "380500"
  },
  {
    "text": "the joint PMF of the three\nrandom variables and I ask you for this value, how\nwould you find it?",
    "start": "380500",
    "end": "387320"
  },
  {
    "text": "Well, you will try to generalize\nthis relation here. The probability that x occurs\nis the sum of the",
    "start": "387320",
    "end": "395250"
  },
  {
    "text": "probabilities of all events\nthat make X to take that",
    "start": "395250",
    "end": "404450"
  },
  {
    "text": "particular value. So what are all the events? Well, this particular x can\nhappen together with some y",
    "start": "404450",
    "end": "411530"
  },
  {
    "text": "and some z. We don't care which y and z. Any y and z will do.",
    "start": "411530",
    "end": "417889"
  },
  {
    "text": "So when we consider all\npossibilities, we need to add here over all possible values\nof y's and z's.",
    "start": "417890",
    "end": "424760"
  },
  {
    "text": "So consider all triples,\nx, y, z. Fix x and consider all the\npossibilities for the",
    "start": "424760",
    "end": "432379"
  },
  {
    "text": "remaining variables, y and z,\nadd these up, and that gives you the marginal PMF of X. And\nthen there's other things that",
    "start": "432380",
    "end": "444740"
  },
  {
    "start": "442000",
    "end": "468000"
  },
  {
    "text": "you can do. This is the multiplication\nrule for two events. We saw back in chapter one that\nthere's a multiplication",
    "start": "444740",
    "end": "452510"
  },
  {
    "text": "rule when you talk about\nmore than two events. And you can write a chain of\nconditional probabilities.",
    "start": "452510",
    "end": "458860"
  },
  {
    "text": "We can certainly do the same\nin our new notation. So let's look at this\nrule up here.",
    "start": "458860",
    "end": "465810"
  },
  {
    "text": " Multiplication rule for three\nrandom variables,",
    "start": "465810",
    "end": "471220"
  },
  {
    "start": "468000",
    "end": "540000"
  },
  {
    "text": "what does it say? The probability of three\nthings happening simultaneously, X, Y, Z taking\nspecific values, little x,",
    "start": "471220",
    "end": "479770"
  },
  {
    "text": "little y, little z, that\nprobability is the probability that the first thing happens,\nthat X takes that value.",
    "start": "479770",
    "end": "487210"
  },
  {
    "text": "Given that X takes that value,\nwe multiply it with the probability that Y takes\nalso a certain value.",
    "start": "487210",
    "end": "494650"
  },
  {
    "text": "And now, given that X and Y have\ntaken those particular values, we multiply with a\nconditional probability that",
    "start": "494650",
    "end": "501730"
  },
  {
    "text": "the third thing happens,\ngiven that the first two things happen.",
    "start": "501730",
    "end": "506960"
  },
  {
    "text": "So this is just the\nmultiplication rule for three events, which would be\nprobability of A intersection",
    "start": "506960",
    "end": "513529"
  },
  {
    "text": "B intersection C equals-- you know the rest\nof the formula. You just rewrite this formula\nin PMF notation.",
    "start": "513530",
    "end": "522330"
  },
  {
    "text": "Probability of A intersection\nB intersection C is the probability of A, which\ncorresponds to this term,",
    "start": "522330",
    "end": "529450"
  },
  {
    "text": "times the probability of B given\nA, times the probability of C given A and B.",
    "start": "529450",
    "end": "540700"
  },
  {
    "start": "540000",
    "end": "658000"
  },
  {
    "text": "So what else is there that's\nleft from chapter one that we can or should generalize\nto random variables?",
    "start": "540700",
    "end": "550190"
  },
  {
    "text": "Well, there's the notion\nof independence. So let's define what\nindependence means.",
    "start": "550190",
    "end": "556720"
  },
  {
    "text": "Instead of talking about just\ntwo random variables, let's go directly to the case of multiple\nrandom variables.",
    "start": "556720",
    "end": "562470"
  },
  {
    "text": "When we talked about events,\nthings were a little complicated. We had a simple definition for\nindependence of two events.",
    "start": "562470",
    "end": "568480"
  },
  {
    "text": "Two events are independent if\nthe probability of both is equal to the product of\nthe probabilities.",
    "start": "568480",
    "end": "573740"
  },
  {
    "text": "But for three events, it\nwas kind of messy. We needed to write down\nlots of conditions. For random variables,\nthings in some sense",
    "start": "573740",
    "end": "581140"
  },
  {
    "text": "are a little simpler. We only need to write down one\nformula and take this as the",
    "start": "581140",
    "end": "586360"
  },
  {
    "text": "definition of independence. Three random variables are\nindependent if and only if, by",
    "start": "586360",
    "end": "593630"
  },
  {
    "text": "definition, their joint\nprobability mass function factors out into individual\nprobability mass functions.",
    "start": "593630",
    "end": "602560"
  },
  {
    "text": "So the probability that all\nthree things happen is the",
    "start": "602560",
    "end": "608190"
  },
  {
    "text": "product of the individual\nprobabilities that each one of these three things\nis happening.",
    "start": "608190",
    "end": "614170"
  },
  {
    "text": "So independence means\nmathematically that you can just multiply probabilities to\nget to the probability of",
    "start": "614170",
    "end": "621030"
  },
  {
    "text": "several things happening\nsimultaneously.  So with three events, we have\nto write a huge number of",
    "start": "621030",
    "end": "631040"
  },
  {
    "text": "equations, of equalities\nthat have to hold. How can it be that with random\nvariables we can only manage",
    "start": "631040",
    "end": "637500"
  },
  {
    "text": "with one equality? Well, the catch is\nthat this is not really just one equality.",
    "start": "637500",
    "end": "643260"
  },
  {
    "text": "We require this to be true for\nevery little x, y, and z.",
    "start": "643260",
    "end": "648390"
  },
  {
    "text": "So in some sense, this is a\nbunch of conditions that are being put on the joint PMF, a\nbunch of conditions that we",
    "start": "648390",
    "end": "656300"
  },
  {
    "text": "need to check. So this is the mathematical\ndefinition. What is the intuitive content\nof this definition?",
    "start": "656300",
    "end": "665399"
  },
  {
    "start": "658000",
    "end": "907000"
  },
  {
    "text": "The intuitive content is\nthe same as for events.",
    "start": "665400",
    "end": "671130"
  },
  {
    "text": "Random variables are independent\nif knowing something about the realized\nvalues of some of these random",
    "start": "671130",
    "end": "679490"
  },
  {
    "text": "variables does not change our\nbeliefs about the likelihood",
    "start": "679490",
    "end": "685510"
  },
  {
    "text": "of various values for the\nremaining random variables. So independence would translate,\nfor example, to a",
    "start": "685510",
    "end": "694250"
  },
  {
    "text": "condition such as the\nconditional PMF of X , given",
    "start": "694250",
    "end": "699690"
  },
  {
    "text": "y, should be equal to the\nmarginal PMF of X. What is",
    "start": "699690",
    "end": "706420"
  },
  {
    "text": "this saying? That you have some original\nbeliefs about how likely it is",
    "start": "706420",
    "end": "713070"
  },
  {
    "text": "for X to take this value. Now, someone comes and\ntells you that Y took",
    "start": "713070",
    "end": "718350"
  },
  {
    "text": "on a certain value. This causes you, in principle,\nto revise your beliefs.",
    "start": "718350",
    "end": "723470"
  },
  {
    "text": "And your new beliefs will be\ncaptured by the conditional PMF, or the conditional\nprobabilities.",
    "start": "723470",
    "end": "728750"
  },
  {
    "text": "Independence means that your\nrevised beliefs actually will be the same as your\noriginal beliefs.",
    "start": "728750",
    "end": "735420"
  },
  {
    "text": "Telling you information about\nthe value of Y doesn't change what you expect for the\nrandom variable X.",
    "start": "735420",
    "end": "744400"
  },
  {
    "text": "Why didn't we use this\ndefinition for independence? Well, because this definition\nonly makes sense when this",
    "start": "744400",
    "end": "751900"
  },
  {
    "text": "conditional is well-defined. And this conditional is only\nwell-defined if the events",
    "start": "751900",
    "end": "763290"
  },
  {
    "text": "that Y takes on that particular\nvalue has positive probability. We cannot condition on events\nthat have zero probability, so",
    "start": "763290",
    "end": "771730"
  },
  {
    "text": "conditional probabilities are\nonly defined for y's that are likely to occur, that have\na positive probability.",
    "start": "771730",
    "end": "779500"
  },
  {
    "text": "Now, similarly, with multiple\nrandom variables, if they're independent, you would have\nrelations such as the",
    "start": "779500",
    "end": "787970"
  },
  {
    "text": "conditional of X, given y and\nz, should be the same as the",
    "start": "787970",
    "end": "794290"
  },
  {
    "text": "marginal of X. What\nis this saying? Again, that if I tell you the\nvalues, the realized values of",
    "start": "794290",
    "end": "801220"
  },
  {
    "text": "random variables Y and Z, this\nis not going to change your beliefs about how likely\nx is to occur.",
    "start": "801220",
    "end": "808900"
  },
  {
    "text": "Whatever you believed in the\nbeginning, you're going to believe the same thing\nafterwards. So it's important to keep that\nintuition in mind, because",
    "start": "808900",
    "end": "816130"
  },
  {
    "text": "sometimes this way you can tell\nwhether random variables are independent without having\nto do calculations and to",
    "start": "816130",
    "end": "822820"
  },
  {
    "text": "check this formula. OK, so let's check our concepts with a simple example.",
    "start": "822820",
    "end": "829250"
  },
  {
    "text": "Let's look at two random\nvariables that are discrete, take values between\none and for each.",
    "start": "829250",
    "end": "835100"
  },
  {
    "text": "And this is a table that\ngives us the joint PMF. So it tells us the probability\nthat X equals to 2 and Y",
    "start": "835100",
    "end": "845720"
  },
  {
    "text": "equals to 1 happening\nsimultaneously. It's an event that has\nprobability 1/20.",
    "start": "845720",
    "end": "850810"
  },
  {
    "text": "Are these two random variables\nindependent? You can try to check a\ncondition like this.",
    "start": "850810",
    "end": "857610"
  },
  {
    "text": "But can we tell directly\nfrom the table? If I tell you a value of Y,\ncould that give you useful",
    "start": "857610",
    "end": "868470"
  },
  {
    "text": "information about X?  Certainly. If I tell you that Y is equal\nto 1, this tells you that X",
    "start": "868470",
    "end": "878680"
  },
  {
    "text": "must be equal to 2. But if I tell you that Y was\nequal to 3, this tells you",
    "start": "878680",
    "end": "884870"
  },
  {
    "text": "that, still, X could\nbe anything. So telling you the value of\nY kind of changes what you",
    "start": "884870",
    "end": "892220"
  },
  {
    "text": "expect or what you consider\npossible for the values of the",
    "start": "892220",
    "end": "897240"
  },
  {
    "text": "other random variable. So by just inspecting here, we\ncan tell that the random",
    "start": "897240",
    "end": "903070"
  },
  {
    "text": "variables are not independent. ",
    "start": "903070",
    "end": "908290"
  },
  {
    "start": "907000",
    "end": "1168000"
  },
  {
    "text": "Okay. What's the other concept we\nintroduced in chapter one? We introduced the concept of\nconditional independence.",
    "start": "908290",
    "end": "914060"
  },
  {
    "text": "And conditional independence is\nlike ordinary independence but applied to a conditional\nuniverse where we're given",
    "start": "914060",
    "end": "920420"
  },
  {
    "text": "some information. So suppose someone tells you\nthat the outcome of the experiment is such that X is\nless than or equal to 2 and Y",
    "start": "920420",
    "end": "930420"
  },
  {
    "text": "is larger than or equal to 3. So we are given the information\nthat we now live",
    "start": "930420",
    "end": "937670"
  },
  {
    "text": "inside this universe. So what happens inside\nthis universe? Inside this universe, our random\nvariables are going to",
    "start": "937670",
    "end": "947200"
  },
  {
    "text": "have a new joint PMF which is\nconditioned on the event that",
    "start": "947200",
    "end": "955140"
  },
  {
    "text": "we were told that\nit has occurred. So let A correspond to this\nsort of event here.",
    "start": "955140",
    "end": "964780"
  },
  {
    "text": "And now we're dealing with\nconditional probabilities. What are those conditional\nprobabilities? We can put them in a table.",
    "start": "964780",
    "end": "971490"
  },
  {
    "text": "So it's a two by two table,\nsince we only have two possible values. What are they going to be?",
    "start": "971490",
    "end": "978079"
  },
  {
    "text": "Well, these probabilities\nshow up in the ratios 1, 2, 2, and 4. Those ratios have to\nstay the same.",
    "start": "978080",
    "end": "985480"
  },
  {
    "text": "The probabilities need\nto add up to one. So what should the denominators\nbe since these",
    "start": "985480",
    "end": "994030"
  },
  {
    "text": "numbers add up to nine? These are the conditional\nprobabilities. So this is the conditional\nPMF in this example.",
    "start": "994030",
    "end": "1000575"
  },
  {
    "text": " Now, in this conditional\nuniverse, is x",
    "start": "1000575",
    "end": "1006990"
  },
  {
    "text": "independent from y?  If I tell you that y takes this\nvalue, so we live in this",
    "start": "1006990",
    "end": "1021449"
  },
  {
    "text": "universe, what do you\nknow about x? What you know about x is at this\nvalue is twice as likely",
    "start": "1021450",
    "end": "1028109"
  },
  {
    "text": "as that value. If I condition on y taking this\nvalue, so we're living",
    "start": "1028109",
    "end": "1033859"
  },
  {
    "text": "here, what do you\nknow about x? What you know about x is that\nthis value is twice as likely",
    "start": "1033859",
    "end": "1041659"
  },
  {
    "text": "as that value. So it's the same. Whether we live here or we live\nthere, this x is twice as",
    "start": "1041660",
    "end": "1050250"
  },
  {
    "text": "likely as that x. So the conditional PMF in this\nnew universe, the conditional",
    "start": "1050250",
    "end": "1061560"
  },
  {
    "text": "PMF of X given y, in the new\nuniverse is the same as the",
    "start": "1061560",
    "end": "1075970"
  },
  {
    "text": "marginal PMF of X, but of course\nin the new universe.",
    "start": "1075970",
    "end": "1081250"
  },
  {
    "text": "So no matter what y is,\nthe conditional PMF of X is the same.",
    "start": "1081250",
    "end": "1086860"
  },
  {
    "text": "And that conditional\nPMF is 1/3 and 2/3.",
    "start": "1086860",
    "end": "1092150"
  },
  {
    "text": "This is the conditional PMF of\nX in the new universe no matter what y occurs. So Y does not give us any\ninformation about X, doesn't",
    "start": "1092150",
    "end": "1100330"
  },
  {
    "text": "cause us to change our beliefs\ninside this little universe.",
    "start": "1100330",
    "end": "1105620"
  },
  {
    "text": "And therefore the two random\nvariables are independent. Now, the other way that you\ncan verify that we have",
    "start": "1105620",
    "end": "1111179"
  },
  {
    "text": "independence is to find the\nmarginal PMFs of the two random variables.",
    "start": "1111180",
    "end": "1116250"
  },
  {
    "text": "The marginal PMF of\nX, you find it by adding those two terms. You get 1/3.",
    "start": "1116250",
    "end": "1122720"
  },
  {
    "text": "Adding those two terms,\nyou get 2/3. Marginal PMF of Y, you find it,\nyou add these two terms,",
    "start": "1122720",
    "end": "1128530"
  },
  {
    "text": "and you get 1/3. And the marginal PMF of Y\nhere is going to be 2/3.",
    "start": "1128530",
    "end": "1136470"
  },
  {
    "text": "And then you ask the question,\nis the joint the product of the marginals? And indeed it is.",
    "start": "1136470",
    "end": "1142630"
  },
  {
    "text": "This times this gives you 1/9. This times this gives you 2/9.",
    "start": "1142630",
    "end": "1148049"
  },
  {
    "text": "So the values in the table with\nthe joint PMFs is the product of the marginal PMFs of\nX and Y in this universe,",
    "start": "1148050",
    "end": "1157220"
  },
  {
    "text": "so the two random variables are independent inside this universe. So we say that they're\nconditionally independent.",
    "start": "1157220",
    "end": "1166704"
  },
  {
    "text": "All right. Now let's move to the new topic,\nto the new concept that",
    "start": "1166704",
    "end": "1172720"
  },
  {
    "start": "1168000",
    "end": "1635000"
  },
  {
    "text": "we introduce in this chapter,\nwhich is the concept of expectations. So what are the things\nto know here?",
    "start": "1172720",
    "end": "1178200"
  },
  {
    "text": "One is the general idea. The way to think about\nexpectations is that it's something like the average value\nfor random variable if",
    "start": "1178200",
    "end": "1186080"
  },
  {
    "text": "you do an experiment over and\nover, and if you interpret probabilities as frequencies.",
    "start": "1186080",
    "end": "1191549"
  },
  {
    "text": "So you get x's over and over\nwith a certain frequency --",
    "start": "1191550",
    "end": "1197030"
  },
  {
    "text": "P(x) -- a particular value, little\nx, gets realized. And each time that this happens,\nyou get x dollars.",
    "start": "1197030",
    "end": "1203960"
  },
  {
    "text": "How many dollars do you\nget on the average? Well, this formula gives you\nthat particular average.",
    "start": "1203960",
    "end": "1209330"
  },
  {
    "text": "So first thing we do is to write\ndown a definition for this sort of concept.",
    "start": "1209330",
    "end": "1215419"
  },
  {
    "text": "But then the other things you\nneed to know is how to calculate expectations using\nshortcuts sometimes, and what",
    "start": "1215420",
    "end": "1223990"
  },
  {
    "text": "properties they have. The most important shortcut\nthere is is that, if you want to calculate the expected value,\nthe average value for a",
    "start": "1223990",
    "end": "1231250"
  },
  {
    "text": "random variable, you do not need\nto find the PMF of that",
    "start": "1231250",
    "end": "1236380"
  },
  {
    "text": "random variable. But you can work directly with\nthe x's and the y's. So you do the experiment\nover and over.",
    "start": "1236380",
    "end": "1244210"
  },
  {
    "text": "The outcome of the experiment\nis a pair (x,y). And each time that a certain\n(x,y) happens,",
    "start": "1244210",
    "end": "1249400"
  },
  {
    "text": "you get so many dollars. So this fraction of the time,\na certain (x,y) happens.",
    "start": "1249400",
    "end": "1254990"
  },
  {
    "text": "And that fraction of the time,\nyou get so many dollars, so this is the average number\nof dollars that you get.",
    "start": "1254990",
    "end": "1260860"
  },
  {
    "text": "So what you end up, since it\nis the average, then that means that it corresponds\nto the expected value.",
    "start": "1260860",
    "end": "1267830"
  },
  {
    "text": "Now, this is something that, of\ncourse, needs a little bit of mathematical proof. But this is just a different\nway of accounting.",
    "start": "1267830",
    "end": "1273880"
  },
  {
    "text": "And it turns out we give\nyou the right answer. And it's a very useful\nshortcut.",
    "start": "1273880",
    "end": "1279420"
  },
  {
    "text": "Now, when we're talking about\nfunctions of random variables, in general, we cannot speak\njust about averages.",
    "start": "1279420",
    "end": "1286620"
  },
  {
    "text": "That is, the expected value\nof a function of a random variable is not the same\nas the function of",
    "start": "1286620",
    "end": "1291860"
  },
  {
    "text": "the expected values. A function of averages is\nnot the same as the average of a function.",
    "start": "1291860",
    "end": "1298380"
  },
  {
    "text": "So in general, this\nis not true. But what it's important to know\nis to know the exceptions",
    "start": "1298380",
    "end": "1303960"
  },
  {
    "text": "to this rule. And the important exceptions\nare mainly two. One is the case of linear",
    "start": "1303960",
    "end": "1311559"
  },
  {
    "text": "functions of a random variable. We discussed this last time. So the expected value of\ntemperature in Celsius is, you",
    "start": "1311560",
    "end": "1319810"
  },
  {
    "text": "first find the expected value of\ntemperature in Fahrenheit, and then you do the conversion\nto Celsius.",
    "start": "1319810",
    "end": "1325809"
  },
  {
    "text": "So whether you first average and\nthen do the conversion to the new units or not, it\nshouldn't matter when you get",
    "start": "1325810",
    "end": "1331730"
  },
  {
    "text": "the result. The other property that turns\nout to be true when you talk",
    "start": "1331730",
    "end": "1336740"
  },
  {
    "text": "about multiple random variables\nis that expectation still behaves linearly. So let X, Y, and Z be the score\nof a random student at",
    "start": "1336740",
    "end": "1346600"
  },
  {
    "text": "each one of the three\nsections of the SAT. So the overall SAT score is X\nplus Y plus Z. This is the",
    "start": "1346600",
    "end": "1356309"
  },
  {
    "text": "average score, the average\ntotal SAT score. Another way to calculate that\naverage is to look at the",
    "start": "1356310",
    "end": "1363789"
  },
  {
    "text": "first section of the SAT and\nsee what was the average. Look at the second section, look\nat what was the average,",
    "start": "1363790",
    "end": "1370710"
  },
  {
    "text": "and so the third, and\nadd the averages. So you can do the averages for\neach section separately, add",
    "start": "1370710",
    "end": "1376909"
  },
  {
    "text": "the averages, or you can find\ntotal scores for each student and average them. So I guess you probably believe\nthat this is correct",
    "start": "1376910",
    "end": "1385690"
  },
  {
    "text": "if you talk just about\naveraging scores. Since expectations are just the\nvariation of averages, it",
    "start": "1385690",
    "end": "1392580"
  },
  {
    "text": "turns out that this is\nalso true in general. And the derivation of this is\nvery simple, based on the",
    "start": "1392580",
    "end": "1399760"
  },
  {
    "text": "expected value rule. And you can look at\nit in the notes. So this is one exception,\nwhich is linearity.",
    "start": "1399760",
    "end": "1407740"
  },
  {
    "text": "The second important exception\nis the case of independent random variables, that the\nproduct of two random",
    "start": "1407740",
    "end": "1414520"
  },
  {
    "text": "variables has an expectation\nwhich is the product of the expectations. In general, this is not true.",
    "start": "1414520",
    "end": "1421400"
  },
  {
    "text": "But for the case where we have\nindependence, the expectation",
    "start": "1421400",
    "end": "1427010"
  },
  {
    "text": "works out as follows. Using the expected value rule,\nthis is how you calculate the",
    "start": "1427010",
    "end": "1435130"
  },
  {
    "text": "expected value of a function\nof a random variable. So think of this as being your\ng(X, Y) and this being your",
    "start": "1435130",
    "end": "1444810"
  },
  {
    "text": "g(little x, y). So this is something that's\ngenerally true. Now, if we have independence,\nthen the PMFs factor out, and",
    "start": "1444810",
    "end": "1460350"
  },
  {
    "text": "then you can separate this sum\nby bringing together the x",
    "start": "1460350",
    "end": "1465660"
  },
  {
    "text": "terms, bring them outside\nthe y summation. And you find that this is the\nsame as expected value of X",
    "start": "1465660",
    "end": "1474370"
  },
  {
    "text": "times the expected value of Y.\nSo independence is used in this step here.",
    "start": "1474370",
    "end": "1480140"
  },
  {
    "text": " OK, now what if X and Y are\nindependent, but instead of",
    "start": "1480140",
    "end": "1488640"
  },
  {
    "text": "taking the expectation of\nX times Y, we take the expectation of the product of\ntwo functions of X and Y?",
    "start": "1488640",
    "end": "1496600"
  },
  {
    "text": "I claim that the expected value\nof the product is still going to be the product of\nthe expected values.",
    "start": "1496600",
    "end": "1502630"
  },
  {
    "text": "How do we show that? We could show it by just redoing\nthis derivation here.",
    "start": "1502630",
    "end": "1509230"
  },
  {
    "text": "Instead of X and Y, we would\nhave g(X) and h(Y), so the algebra goes through.",
    "start": "1509230",
    "end": "1514850"
  },
  {
    "text": "But there's a better way to\nthink about it which is more conceptual. And here's the idea.",
    "start": "1514850",
    "end": "1520886"
  },
  {
    "text": "If X and Y are independent,\nwhat does it mean? X does not convey any\ninformation about Y. If X",
    "start": "1520886",
    "end": "1531180"
  },
  {
    "text": "conveys no information about Y,\ndoes X convey information",
    "start": "1531180",
    "end": "1536350"
  },
  {
    "text": "about h(Y)? No.",
    "start": "1536350",
    "end": "1541940"
  },
  {
    "text": "If X tells me nothing about Y,\nnothing new, it shouldn't tell me anything about h(Y).",
    "start": "1541940",
    "end": "1550580"
  },
  {
    "text": "Now, if X tells me nothing about\nh of h(Y), could g(X)",
    "start": "1550580",
    "end": "1559269"
  },
  {
    "text": "tell me something about h(Y)? No. So the idea is that, if X is\nunrelated to Y, doesn't have",
    "start": "1559270",
    "end": "1566780"
  },
  {
    "text": "any useful information, then\ng(X) could not have any useful information for h(Y).",
    "start": "1566780",
    "end": "1573250"
  },
  {
    "text": "So if X and Y are independent,\nthen g(X) and h(Y) are also",
    "start": "1573250",
    "end": "1581030"
  },
  {
    "text": "independent. ",
    "start": "1581030",
    "end": "1587150"
  },
  {
    "text": "So this is something that\none can try to prove mathematically, but it's more\nimportant to understand conceptually why this is so.",
    "start": "1587150",
    "end": "1594530"
  },
  {
    "text": "It's in terms of conveying\ninformation. So if X tells me nothing about\nY, X cannot tell me anything",
    "start": "1594530",
    "end": "1604950"
  },
  {
    "text": "about Y cubed, or X cannot\ntell me anything by Y squared, and so on.",
    "start": "1604950",
    "end": "1611030"
  },
  {
    "text": "That's the idea. And once we are convinced that\ng(X) and h(Y) are independent,",
    "start": "1611030",
    "end": "1617180"
  },
  {
    "text": "then we can apply our previous\nrule, that for independent random variables, expectations\nmultiply the right way.",
    "start": "1617180",
    "end": "1624390"
  },
  {
    "text": "Apply the previous rule, but\napply it now to these two independent random variables.",
    "start": "1624390",
    "end": "1630490"
  },
  {
    "text": "And we get the conclusion\nthat we wanted. ",
    "start": "1630490",
    "end": "1635500"
  },
  {
    "start": "1635000",
    "end": "1904000"
  },
  {
    "text": "Now, besides expectations, we\nalso introduced the concept of the variance. ",
    "start": "1635500",
    "end": "1643560"
  },
  {
    "text": "And if you remember the\ndefinition of the variance, let me write down the formula\nfor the variance of aX.",
    "start": "1643560",
    "end": "1651100"
  },
  {
    "text": "It's the expected value of the\nrandom variable that we're looking at minus the expected\nvalue of the random variable",
    "start": "1651100",
    "end": "1659630"
  },
  {
    "text": "that we're looking at. So this is the difference\nof the random",
    "start": "1659630",
    "end": "1664780"
  },
  {
    "text": "variable from its mean. And we take that difference\nand square it, so it's the",
    "start": "1664780",
    "end": "1670880"
  },
  {
    "text": "squared distance from the\nmean, and then take expectations of the\nwhole thing. So when you look at that\nexpression, you realize that a",
    "start": "1670880",
    "end": "1679570"
  },
  {
    "text": "can be pulled out of\nthose expressions.  And because there is a squared,\nwhen you pull out the",
    "start": "1679570",
    "end": "1690340"
  },
  {
    "text": "a, it's going to come\nout as an a-squared. So that gives us the rule for\nfinding the variance of a",
    "start": "1690340",
    "end": "1696049"
  },
  {
    "text": "scale or product of\na random variable. The variance captures the idea\nof how wide, how spread out a",
    "start": "1696050",
    "end": "1702370"
  },
  {
    "text": "certain distribution is. Bigger variance means it's\nmore spread out. Now, if you take a random\nvariable and the constants to",
    "start": "1702370",
    "end": "1709360"
  },
  {
    "text": "it, what does it do to\nits distribution? It just shifts it, but it\ndoesn't change its width.",
    "start": "1709360",
    "end": "1715480"
  },
  {
    "text": "So intuitively it\nmeans that the variance should not change. You can check that\nmathematically, but it should",
    "start": "1715480",
    "end": "1722360"
  },
  {
    "text": "also make sense intuitively. So the variance, when you add\nthe constant, does not change.",
    "start": "1722360",
    "end": "1727710"
  },
  {
    "text": "Now, can you add variances is\nthe way we added expectations? Does variance behave linearly?",
    "start": "1727710",
    "end": "1734760"
  },
  {
    "text": "It turns out that not always. Here, we need a condition. It's only in special cases--",
    "start": "1734760",
    "end": "1743880"
  },
  {
    "text": "for example, when the two\nrandom variables are independent-- that you can add variances.",
    "start": "1743880",
    "end": "1749299"
  },
  {
    "text": "The variance of the sum is the\nsum of the variances if X and Y are independent.",
    "start": "1749300",
    "end": "1755370"
  },
  {
    "text": "The derivation of this is,\nagain, very short and simple. We'll skip it, but it's an\nimportant fact to remember.",
    "start": "1755370",
    "end": "1762590"
  },
  {
    "text": "Now, to appreciate why this\nequality is not true always, we can think of some\nextreme examples.",
    "start": "1762590",
    "end": "1768980"
  },
  {
    "text": "Suppose that X is the same as\nY. What's going to be the variance of X plus Y?",
    "start": "1768980",
    "end": "1774520"
  },
  {
    "text": "Well, X plus Y, in this case,\nis the same as 2X, so we're",
    "start": "1774520",
    "end": "1779810"
  },
  {
    "text": "going to get 4 times the\nvariance of X, which is different than the variance of\nX plus the variance of X.",
    "start": "1779810",
    "end": "1789770"
  },
  {
    "text": "So that expression would give\nus twice the variance of X. But actually now it's 4 times\nthe variance of X. The other",
    "start": "1789770",
    "end": "1796460"
  },
  {
    "text": "extreme would be if X is equal\nto -Y. Then the variance is",
    "start": "1796460",
    "end": "1801990"
  },
  {
    "text": "the variance of the random\nvariable, which is always equal to 0.",
    "start": "1801990",
    "end": "1807020"
  },
  {
    "text": "Now, a random variable which\nis always equal to 0 has no uncertainty. It is always equal to its mean\nvalue, so the variance, in",
    "start": "1807020",
    "end": "1814570"
  },
  {
    "text": "this case, turns out to be 0. So in both of these cases,\nof course we have random",
    "start": "1814570",
    "end": "1819940"
  },
  {
    "text": "variables that are extremely\ndependent. Why are they dependent? Because if I tell you something\nabout Y, it tells",
    "start": "1819940",
    "end": "1827940"
  },
  {
    "text": "you an awful lot about the value\nof X. There's a lot of information about X if\nI tell you Y, in this",
    "start": "1827940",
    "end": "1834910"
  },
  {
    "text": "case or in that case. And finally, a short drill.",
    "start": "1834910",
    "end": "1839940"
  },
  {
    "text": "If I tell you that the random\nvariables are independent and you want to calculate the\nvariance of a linear combination of this kind,\nthen how do you argue?",
    "start": "1839940",
    "end": "1848330"
  },
  {
    "text": "You argue that, since X and Y\nare independent, this means that X and 3Y are also\nindependent.",
    "start": "1848330",
    "end": "1855659"
  },
  {
    "text": "X has no information about Y, so\nX has no information about -Y. X has no information about\n-Y, so X should not have any",
    "start": "1855660",
    "end": "1865000"
  },
  {
    "text": "information about -3Y.",
    "start": "1865000",
    "end": "1870270"
  },
  {
    "text": "So X and -3Y are independent. So the variance of Z should be\nthe variance of X plus the",
    "start": "1870270",
    "end": "1878480"
  },
  {
    "text": "variance of -3Y, which is the\nvariance of X plus 9 times the",
    "start": "1878480",
    "end": "1886910"
  },
  {
    "text": "variance of Y. The important\nthing to note here is that no matter what happens, you\nend up getting a",
    "start": "1886910",
    "end": "1894080"
  },
  {
    "text": "plus here, not a minus. So that's the sort of important\nthing to remember in",
    "start": "1894080",
    "end": "1901160"
  },
  {
    "text": "this type of calculation.  So this has been all concepts,\nreviews, new",
    "start": "1901160",
    "end": "1908889"
  },
  {
    "start": "1904000",
    "end": "2091000"
  },
  {
    "text": "concepts and all that. It's the usual fire hose. Now let's use them to do\nsomething useful finally.",
    "start": "1908890",
    "end": "1916679"
  },
  {
    "text": "So let's revisit our old\nexample, the binomial distribution, which counts the\nnumber of successes in",
    "start": "1916680",
    "end": "1923350"
  },
  {
    "text": "independent trials of a coin. It's a biased coin that has\na probability of heads, or",
    "start": "1923350",
    "end": "1929030"
  },
  {
    "text": "probability of success, equal\nto p at each trial. Finally, we can go through the\nexercise of calculating the",
    "start": "1929030",
    "end": "1936160"
  },
  {
    "text": "expected value of this\nrandom variable. And there's the way of\ncalculating that expectation",
    "start": "1936160",
    "end": "1941789"
  },
  {
    "text": "that would be the favorite\nof those people who enjoy algebra, which is to write down\nthe definition of the",
    "start": "1941790",
    "end": "1947500"
  },
  {
    "text": "expected value. We add over all possible values\nof the random variable, over all the possible k's, and\nweigh them according to the",
    "start": "1947500",
    "end": "1955580"
  },
  {
    "text": "probabilities that this\nparticular k occurs. The probability that X takes on\na particular value k is, of",
    "start": "1955580",
    "end": "1962250"
  },
  {
    "text": "course, the binomial\nPMF, which is this familiar formula.",
    "start": "1962250",
    "end": "1967559"
  },
  {
    "text": "Clearly, that would be a messy\nand challenging calculation. Can we find a shortcut? There's a very clever trick.",
    "start": "1967560",
    "end": "1974010"
  },
  {
    "text": "There's lots of problems in\nprobability that you can approach really nicely by\nbreaking up the random",
    "start": "1974010",
    "end": "1980000"
  },
  {
    "text": "variable of interest into a\nsum of simpler and more manageable random variables.",
    "start": "1980000",
    "end": "1986010"
  },
  {
    "text": "And if you can make it to be a\nsum of random variables that are just 0's or 1's,\nso much the better.",
    "start": "1986010",
    "end": "1992590"
  },
  {
    "text": "Life is easier. Random variables that take\nvalues 0 or 1, we call them indicator variables.",
    "start": "1992590",
    "end": "1998380"
  },
  {
    "text": "They indicate whether an event\nhas occurred or not. In this case, we look at each\ncoin flip one at a time.",
    "start": "1998380",
    "end": "2005600"
  },
  {
    "text": "For the i-th flip, if it\nresulted in heads or a success, we record it 1.",
    "start": "2005600",
    "end": "2012110"
  },
  {
    "text": "If not, we record it 0. And then we look at the\nrandom variable.",
    "start": "2012110",
    "end": "2017539"
  },
  {
    "text": "If we take the sum of the Xi's,\nwhat is it going to be?",
    "start": "2017540",
    "end": "2022580"
  },
  {
    "text": "We add one each time that we get\na success, so the sum is",
    "start": "2022580",
    "end": "2028029"
  },
  {
    "text": "going to be the total\nnumber of successes. So we break up the random\nvariable of interest as a sum",
    "start": "2028030",
    "end": "2033899"
  },
  {
    "text": "of really nice and simple\nrandom variables. And now we can use the linearity\nof expectations.",
    "start": "2033900",
    "end": "2040380"
  },
  {
    "text": "We're going to find the\nexpectation of X by finding the expectation of the Xi's\nand then adding the",
    "start": "2040380",
    "end": "2045700"
  },
  {
    "text": "expectations. What's the expected\nvalue of Xi? Well, Xi takes the value 1 with\nprobability p, and takes",
    "start": "2045700",
    "end": "2053050"
  },
  {
    "text": "the value 0 with probability\n1-p. So the expected value\nof Xi is just p.",
    "start": "2053050",
    "end": "2059070"
  },
  {
    "text": "So the expected value of X is\ngoing to be just n times p.",
    "start": "2059070",
    "end": "2064888"
  },
  {
    "text": "Because X is the sum of n terms,\neach one of which has expectation p, the expected\nvalue of the sum is the sum of",
    "start": "2064889",
    "end": "2073050"
  },
  {
    "text": "the expected values. So I guess that's a pretty good\nshortcut for doing this",
    "start": "2073050",
    "end": "2078440"
  },
  {
    "text": "horrendous calculation\nup there. So in case you didn't realize\nit, that's what we just",
    "start": "2078440",
    "end": "2087210"
  },
  {
    "text": "established without\ndoing any algebra. Good.",
    "start": "2087210",
    "end": "2092219"
  },
  {
    "text": "How about the variance\nof X, of Xi? Two ways to calculate it.",
    "start": "2092219",
    "end": "2097569"
  },
  {
    "text": "One is by using directly the\nformula for the variance, which would be -- let's see what it would be.",
    "start": "2097570",
    "end": "2103900"
  },
  {
    "text": "With probability\np, you get a 1. And in this case, you are\nso far from the mean.",
    "start": "2103900",
    "end": "2111270"
  },
  {
    "text": "That's your squared distance\nfrom the mean. With probability 1-p, you\nget a 0, which is so far",
    "start": "2111270",
    "end": "2118750"
  },
  {
    "text": "away from the mean. And then you can simplify that\nformula and get an answer.",
    "start": "2118750",
    "end": "2124380"
  },
  {
    "text": "How about a slightly easier\nway of doing it. Instead of doing the algebra\nhere, let me indicate the",
    "start": "2124380",
    "end": "2131359"
  },
  {
    "text": "slightly easier way. We have a formula for the\nvariance that tells us that we can find the variance by\nproceeding this way.",
    "start": "2131360",
    "end": "2142290"
  },
  {
    "text": "That's a formula that's\ngenerally true for variances. Why is this easier?",
    "start": "2142290",
    "end": "2147380"
  },
  {
    "text": "What's the expected value\nof Xi squared?  Backtrack.",
    "start": "2147380",
    "end": "2153290"
  },
  {
    "text": "What is Xi squared, after all? It's the same thing as Xi.",
    "start": "2153290",
    "end": "2159510"
  },
  {
    "text": "Since Xi takes value 0 and 1, Xi\nsquared also takes the same values, 0 and 1.",
    "start": "2159510",
    "end": "2165780"
  },
  {
    "text": "So the expected value of Xi\nsquared is the same as the expected value of Xi,\nwhich is equal to p.",
    "start": "2165780",
    "end": "2171990"
  },
  {
    "text": " And the expected value of Xi\nsquared is p squared, so we",
    "start": "2171990",
    "end": "2180530"
  },
  {
    "text": "get the final answer,\np times (1-p). If you were to work through and\ndo the cancellations in",
    "start": "2180530",
    "end": "2188630"
  },
  {
    "text": "this messy expression here,\nafter one line you would also get to the same formula.",
    "start": "2188630",
    "end": "2194049"
  },
  {
    "text": "But this sort of illustrates\nthat working with this formula for the variance, sometimes\nthings work",
    "start": "2194050",
    "end": "2200550"
  },
  {
    "text": "out a little faster. Finally, are we in business? Can we calculate the variance\nof the random",
    "start": "2200550",
    "end": "2207819"
  },
  {
    "text": "variable X as well? Well, we have the rule that\nfor independent random variables, the variance\nof the sum is",
    "start": "2207820",
    "end": "2215680"
  },
  {
    "text": "the sum of the variances. So to find the variance of X,\nwe just need to add the",
    "start": "2215680",
    "end": "2220930"
  },
  {
    "text": "variances of the Xi's. We have n Xi's, and each\none of them has",
    "start": "2220930",
    "end": "2227140"
  },
  {
    "text": "variance p_n times (1-p). And we are done.",
    "start": "2227140",
    "end": "2232290"
  },
  {
    "text": "So this way, we have calculated\nboth the mean and",
    "start": "2232290",
    "end": "2237780"
  },
  {
    "text": "the variance of the binomial\nrandom variable. It's interesting to look at this\nparticular formula and",
    "start": "2237780",
    "end": "2247280"
  },
  {
    "text": "see what it tells us. If you are to plot the variance\nof X as a function of",
    "start": "2247280",
    "end": "2253470"
  },
  {
    "text": "p, it has this shape. ",
    "start": "2253470",
    "end": "2265900"
  },
  {
    "text": "And the maximum is\nhere at 1/2.",
    "start": "2265900",
    "end": "2271309"
  },
  {
    "text": "p times (1-p) is 0 when\np is equal to 0. And when p equals to 1, it's a\nquadratic, so it must have",
    "start": "2271310",
    "end": "2278569"
  },
  {
    "text": "this particular shape. So what does it tell us? If you think about variance as\na measure of uncertainty, it",
    "start": "2278570",
    "end": "2285880"
  },
  {
    "text": "tells you that coin flips\nare most uncertain when your coin is fair.",
    "start": "2285880",
    "end": "2292620"
  },
  {
    "text": "When p is equal to 1/2, that's\nwhen you have the most randomness. And this is kind of intuitive.",
    "start": "2292620",
    "end": "2298790"
  },
  {
    "text": "if on the other hand I tell you\nthat the coin is extremely biased, p very close to 1, which\nmeans it almost always",
    "start": "2298790",
    "end": "2306490"
  },
  {
    "text": "gives you heads, then\nthat would be a case of low variance. There's low variability\nin the results.",
    "start": "2306490",
    "end": "2312869"
  },
  {
    "text": "There's little uncertainty about\nwhat's going to happen. It's going to be mostly heads\nwith some occasional tails.",
    "start": "2312870",
    "end": "2319570"
  },
  {
    "text": "So p equals 1/2. Fair coin, that's the coin which\nis the most uncertain of",
    "start": "2319570",
    "end": "2325350"
  },
  {
    "text": "all coins, in some sense. And it corresponds to the\nbiggest variance. It corresponds to an X that has\nthe widest distribution.",
    "start": "2325350",
    "end": "2333760"
  },
  {
    "text": "Now that we're on a roll and we\ncan calculate such hugely complicated sums in simple ways,\nlet us try to push our",
    "start": "2333760",
    "end": "2341400"
  },
  {
    "text": "luck and do a problem with\nthis flavor, but a little harder than that.",
    "start": "2341400",
    "end": "2346590"
  },
  {
    "text": "So you go to one of those old-fashioned cocktail parties. All males at least will have\nthose standard big hats which",
    "start": "2346590",
    "end": "2356010"
  },
  {
    "text": "look identical. They check them in when\nthey walk in. And when they walk out, since\nthey look pretty identical,",
    "start": "2356010",
    "end": "2363390"
  },
  {
    "text": "they just pick a random\nhat and go home. So n people, they pick their\nhats completely at random,",
    "start": "2363390",
    "end": "2371080"
  },
  {
    "text": "quote, unquote, and\nthen leave. And the question is, to say\nsomething about the number of",
    "start": "2371080",
    "end": "2376970"
  },
  {
    "text": "people who end up, by accident\nor by luck, to get back their",
    "start": "2376970",
    "end": "2382070"
  },
  {
    "text": "own hat, the exact same hat\nthat they checked in. OK, first what do we mean\ncompletely at random?",
    "start": "2382070",
    "end": "2388490"
  },
  {
    "text": "Completely at random, we\nbasically mean that any permutation of the hats\nis equally likely.",
    "start": "2388490",
    "end": "2394180"
  },
  {
    "text": "Any way of distributing those\nn hats to the n people, any particular way is as likely\nas any other way.",
    "start": "2394180",
    "end": "2401350"
  },
  {
    "text": "So there's complete symmetry\nbetween hats and people. So what we want to do is to\ncalculate the expected value",
    "start": "2401350",
    "end": "2408490"
  },
  {
    "start": "2405000",
    "end": "3041000"
  },
  {
    "text": "and the variance of this random\nvariable X. Let's start with the expected value. Let's reuse the trick from\nthe binomial case.",
    "start": "2408490",
    "end": "2417840"
  },
  {
    "text": "So total number of hats picked,\nwe're going to think of total number of hats\npicked as a sum of",
    "start": "2417840",
    "end": "2424140"
  },
  {
    "text": "(0, 1) random variables. X1 tells us whether person\n1 got their own hat back.",
    "start": "2424140",
    "end": "2430470"
  },
  {
    "text": "If they did, we record a 1. X2, the same thing. By adding all X's is how many\n1's did we get, which counts",
    "start": "2430470",
    "end": "2440910"
  },
  {
    "text": "how many people selected\ntheir own hats. So we broke down the random\nvariable of interest, the",
    "start": "2440910",
    "end": "2448100"
  },
  {
    "text": "number of people who get their\nown hats back, as a sum of random variables.",
    "start": "2448100",
    "end": "2453570"
  },
  {
    "text": "And these random variables,\nagain, are easy to handle, because they're binary. The only take two values.",
    "start": "2453570",
    "end": "2459250"
  },
  {
    "text": "What's the probability that Xi\nis equal to 1, the i-th person has a probability that they\nget their own hat?",
    "start": "2459250",
    "end": "2466730"
  },
  {
    "text": "There's n hats by symmetry. The chance is that they end up\ngetting their own hat, as",
    "start": "2466730",
    "end": "2471890"
  },
  {
    "text": "opposed to any one of the\nother n - 1 hats, is going to be 1/n.",
    "start": "2471890",
    "end": "2478020"
  },
  {
    "text": "So what's the expected\nvalue of Xi? It's one times 1/n.",
    "start": "2478020",
    "end": "2483130"
  },
  {
    "text": "With probability 1/n, you get\nyour own hat, or you get a value of 0 with probability\n1-1/n, which is 1/n.",
    "start": "2483130",
    "end": "2490960"
  },
  {
    "text": " All right, so we got the\nexpected value of the Xi's.",
    "start": "2490960",
    "end": "2498359"
  },
  {
    "text": "And remember, we want to do is\nto calculate the expected value of X by using this\ndecomposition?",
    "start": "2498360",
    "end": "2506900"
  },
  {
    "text": "Are the random variables Xi\nindependent of each other?",
    "start": "2506900",
    "end": "2512230"
  },
  {
    "text": "You can try to answer that\nquestion by writing down a joint PMF for the X's,\nbut I'm sure that",
    "start": "2512230",
    "end": "2518510"
  },
  {
    "text": "you will not succeed. But can you think intuitively? If I tell you information about\nsome of the Xi's, does",
    "start": "2518510",
    "end": "2525940"
  },
  {
    "text": "it give you information about\nthe remaining ones? Yeah. If I tell you that out of 10\npeople, 9 of them got their",
    "start": "2525940",
    "end": "2533950"
  },
  {
    "text": "own hat back, does that\ntell you something about the 10th person? Yes. If 9 got their own hat, then the\n10th must also have gotten",
    "start": "2533950",
    "end": "2542510"
  },
  {
    "text": "their own hat back. So the first 9 random variables\ntell you something about the 10th one.",
    "start": "2542510",
    "end": "2548790"
  },
  {
    "text": "And conveying information of\nthis sort, that's the case of dependence.",
    "start": "2548790",
    "end": "2554410"
  },
  {
    "text": "All right, so the random\nvariables are not independent. Are we stuck? Can we still calculate the\nexpected value of X?",
    "start": "2554410",
    "end": "2563240"
  },
  {
    "text": "Yes, we can. And the reason we can is that\nexpectations are linear.",
    "start": "2563240",
    "end": "2570710"
  },
  {
    "text": "Expectation of a sum of random\nvariables is the sum of the expectations. And that's always true.",
    "start": "2570710",
    "end": "2577490"
  },
  {
    "text": "There's no independence\nassumption that's being used to apply that rule.",
    "start": "2577490",
    "end": "2582540"
  },
  {
    "text": "So we have that the expected\nvalue of X is the sum of the expected value of the Xi's.",
    "start": "2582540",
    "end": "2589580"
  },
  {
    "text": "And this is a property\nthat's always true. You don't need independence. You don't care.",
    "start": "2589580",
    "end": "2595590"
  },
  {
    "text": "So we're adding n terms,\neach one of which has expected value 1/n. And the final answer is 1.",
    "start": "2595590",
    "end": "2602670"
  },
  {
    "text": "So out of the 100 people who\nselected hats at random, on the average, you expect only one\nof them to end up getting",
    "start": "2602670",
    "end": "2612589"
  },
  {
    "text": "their own hat back. Very good. So since we are succeeding so\nfar, let's try to see if we",
    "start": "2612590",
    "end": "2621619"
  },
  {
    "text": "can succeed in calculating\nthe variance as well. And of course, we will. But it's going to be a little\nmore complicated.",
    "start": "2621620",
    "end": "2630160"
  },
  {
    "text": "The reason it's going to be a\nlittle more complicated is because the Xi's are not\nindependent, so the variance",
    "start": "2630160",
    "end": "2636500"
  },
  {
    "text": "of the sum is not the same as\nthe sum of the variances. So it's not enough to find the\nvariances of the Xi's.",
    "start": "2636500",
    "end": "2644320"
  },
  {
    "text": "We'll have to do more work. And here's what's involved. Let's start with the general\nformula for the variance,",
    "start": "2644320",
    "end": "2652320"
  },
  {
    "text": "which, as I mentioned before,\nit's usually the simpler way to go about calculating\nvariances.",
    "start": "2652320",
    "end": "2658430"
  },
  {
    "text": "So we need to calculate the\nexpected value for X-squared, and subtract from it the\nexpectation squared.",
    "start": "2658430",
    "end": "2667110"
  },
  {
    "text": "Well, we already found the\nexpected value of X. It's equal to 1. So 1-squared gives us just 1.",
    "start": "2667110",
    "end": "2674580"
  },
  {
    "text": "So we're left with the task of\ncalculating the expected value of X-squared, the random\nvariable X-squared.",
    "start": "2674580",
    "end": "2683440"
  },
  {
    "text": "Let's try to follow\nthe same idea. Write this messy random\nvariable, X-squared, as a sum",
    "start": "2683440",
    "end": "2689770"
  },
  {
    "text": "of hopefully simpler\nrandom variables. So X is the sum of the\nXi's, so you square",
    "start": "2689770",
    "end": "2699349"
  },
  {
    "text": "both sides of this. And then you expand the\nright-hand side.",
    "start": "2699350",
    "end": "2705150"
  },
  {
    "text": "When you expand the right-hand\nside, you get the squares of the terms that appear here.",
    "start": "2705150",
    "end": "2711420"
  },
  {
    "text": "And then you get all\nthe cross-terms. For every pair of (i,j) that\nare different, i different",
    "start": "2711420",
    "end": "2719100"
  },
  {
    "text": "than j, you're going to have\na cross-term in the sum. So now, in order to calculate\nthe expected value of",
    "start": "2719100",
    "end": "2729230"
  },
  {
    "text": "X-squared, what does\nour task reduce to? It reduces to calculating the\nexpected value of this term",
    "start": "2729230",
    "end": "2736230"
  },
  {
    "text": "and calculating the expected\nvalue of that term. So let's do them\none at a time. Expected value of Xi squared,\nwhat is it going to be?",
    "start": "2736230",
    "end": "2747040"
  },
  {
    "text": "Same trick as before. Xi takes value 0 or 1, so Xi\nsquared takes just the same",
    "start": "2747040",
    "end": "2753349"
  },
  {
    "text": "values, 0 or 1. So that's the easy one. That's the same as expected\nvalue of Xi, which we already",
    "start": "2753350",
    "end": "2760680"
  },
  {
    "text": "know to be 1/n. So this gives us a first\ncontribution down here.",
    "start": "2760680",
    "end": "2767829"
  },
  {
    "text": " The expected value of this\nterm is going to be what?",
    "start": "2767830",
    "end": "2774220"
  },
  {
    "text": "We have n terms in\nthe summation. And each one of these terms\nhas an expectation of 1/n.",
    "start": "2774220",
    "end": "2781800"
  },
  {
    "text": "So we did a piece\nof the puzzle. So now let's deal with the\nsecond piece of the puzzle.",
    "start": "2781800",
    "end": "2788480"
  },
  {
    "text": "Let's find the expected\nvalue of Xi times Xj. Now by symmetry, the expected\nvalue of Xi times Xj is going",
    "start": "2788480",
    "end": "2795540"
  },
  {
    "text": "to be the same no matter\nwhat i and j you see. So let's just think about X1\nand X2 and try to find the",
    "start": "2795540",
    "end": "2804930"
  },
  {
    "text": "expected value of X1 and X2. X1 times X2 is a random\nvariable.",
    "start": "2804930",
    "end": "2811710"
  },
  {
    "text": "What values does it take? Only 0 or 1? Since X1 and X2 are 0 or 1,\ntheir product can only take",
    "start": "2811710",
    "end": "2820000"
  },
  {
    "text": "the values of 0 or 1. So to find the probability\ndistribution of this random variable, it's just sufficient\nto find the probability that",
    "start": "2820000",
    "end": "2827320"
  },
  {
    "text": "it takes the value of 1. Now, what does X1 times\nX2 equal to 1 mean?",
    "start": "2827320",
    "end": "2834500"
  },
  {
    "text": "It means that X1 was\n1 and X2 was 1. The only way that you can get\na product of 1 is if both of",
    "start": "2834500",
    "end": "2842390"
  },
  {
    "text": "them turned out to be 1's. So that's the same as saying,\npersons 1 and 2 both picked",
    "start": "2842390",
    "end": "2849570"
  },
  {
    "text": "their own hats. The probability that person 1\nand person 2 both pick their",
    "start": "2849570",
    "end": "2855510"
  },
  {
    "text": "own hats is the probability of\ntwo things happening, which is the product of the first thing\nhappening times the",
    "start": "2855510",
    "end": "2862320"
  },
  {
    "text": "conditional probability\nof the second, given that the first happened. And in words, this is the\nprobability that the first",
    "start": "2862320",
    "end": "2868690"
  },
  {
    "text": "person picked their own hat\ntimes the probability that the second person picks their own\nhat, given that the first",
    "start": "2868690",
    "end": "2874920"
  },
  {
    "text": "person already picked\ntheir own. So what's the probability\nthat the first person picks their own hat?",
    "start": "2874920",
    "end": "2880760"
  },
  {
    "text": "We know that it's 1/n. Now, how about the\nsecond person? If I tell you that one person\nhas their own hat, and that",
    "start": "2880760",
    "end": "2889540"
  },
  {
    "text": "person takes their hat and goes\naway, from the point of view of the second person,\nthere's n - 1 people left",
    "start": "2889540",
    "end": "2897250"
  },
  {
    "text": "looking at n - 1 hats. And they're getting just\nhats at random.",
    "start": "2897250",
    "end": "2902330"
  },
  {
    "text": "What's the chance that\nI will get my own? It's 1/n - 1. ",
    "start": "2902330",
    "end": "2909210"
  },
  {
    "text": "So think of them as person 1\ngoes, picks a hat at random, it happens to be their\nown, and it leaves.",
    "start": "2909210",
    "end": "2916849"
  },
  {
    "text": "You're left with n - 1 people,\nand there are n - 1 hats out there. Person 2 goes and picks a hat\nat random, with probability",
    "start": "2916850",
    "end": "2924490"
  },
  {
    "text": "1/n - 1, is going to\npick his own hat. So the expected value now of\nthis random variable is,",
    "start": "2924490",
    "end": "2932400"
  },
  {
    "text": "again, that same number,\nbecause this is a 0, 1 random variable.",
    "start": "2932400",
    "end": "2937500"
  },
  {
    "text": "So this is the same as expected\nvalue of Xi times Xj when i different than j.",
    "start": "2937500",
    "end": "2944810"
  },
  {
    "text": "So here, all that's left to do\nis to add the expectations of",
    "start": "2944810",
    "end": "2949830"
  },
  {
    "text": "these terms. Each one of these terms has an\nexpected value that's 1/n times (1/n - 1).",
    "start": "2949830",
    "end": "2956910"
  },
  {
    "text": "And how many terms do we have? How many of these are\nwe adding up? ",
    "start": "2956910",
    "end": "2964839"
  },
  {
    "text": "It's n-squared - n. When you expand the quadratic,\nthere's a total",
    "start": "2964840",
    "end": "2971829"
  },
  {
    "text": "of n-squared terms. Some are self-terms,\nn of them.",
    "start": "2971830",
    "end": "2977859"
  },
  {
    "text": "And the remaining number of\nterms is n-squared - n. So here we got n-squared\n- n terms.",
    "start": "2977860",
    "end": "2988309"
  },
  {
    "text": "And so we need to multiply\nhere with n-squared - n. ",
    "start": "2988310",
    "end": "2993810"
  },
  {
    "text": "And after you realize that this\nnumber here is 1, and you",
    "start": "2993810",
    "end": "2999980"
  },
  {
    "text": "realize that this is the same\nas the denominator, you get the answer that the expected\nvalue of X squared equals 2.",
    "start": "2999980",
    "end": "3006750"
  },
  {
    "text": "And then, finally going up to\nthe top formula, we get the expected value of X squared,\nwhich is 2 - 1, and the",
    "start": "3006750",
    "end": "3014720"
  },
  {
    "text": "variance is just equal to 1. So the variance of this random\nvariable, number of people who",
    "start": "3014720",
    "end": "3021680"
  },
  {
    "text": "get their own hats back,\nis also equal to 1, equal to the mean. Looks like magic.",
    "start": "3021680",
    "end": "3027690"
  },
  {
    "text": "Why is this the case? Well, there's a deeper\nexplanation why these two numbers should come out\nto be the same.",
    "start": "3027690",
    "end": "3033630"
  },
  {
    "text": "But this is something that would\nprobably have to wait a couple of chapters before we\ncould actually explain it.",
    "start": "3033630",
    "end": "3039420"
  },
  {
    "text": "And so I'll stop here. ",
    "start": "3039420",
    "end": "3041980"
  }
]