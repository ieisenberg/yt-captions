[
  {
    "text": "[SQUEAKING] [RUSTLING] [CLICKING]",
    "start": "0",
    "end": "5434"
  },
  {
    "start": "5434",
    "end": "11580"
  },
  {
    "text": "PROFESSOR: So today, for the\nfirst half of the lecture, we're going to have another\nguest lecturer, Gaurav Arya,",
    "start": "11580",
    "end": "18880"
  },
  {
    "text": "who's actually an\nundergraduate at MIT. But he's done research\nprojects with me, and also research projects\nwith Alan Edelman's group,",
    "start": "18880",
    "end": "25810"
  },
  {
    "text": "and he's done some really\ninteresting recent work",
    "start": "25810",
    "end": "30860"
  },
  {
    "text": "on differentiating random\nfunctions, like you program,",
    "start": "30860",
    "end": "36230"
  },
  {
    "text": "you flip a coin, and 50% it\ngives you sine, 50% chance it gives you cosine. So what does it mean to\ndifferentiate such a thing,",
    "start": "36230",
    "end": "43335"
  },
  {
    "text": "and how do you do\nit automatically? And so I thought\nthat would be a-- we thought that would be a\nnice topic for half a lecture.",
    "start": "43335",
    "end": "51190"
  },
  {
    "text": "So he's in Hong Kong\nright now, but he's going to give a guest lecture. So I think we can\nget started, Gaurav.",
    "start": "51190",
    "end": "58800"
  },
  {
    "text": "GAURAV ARYA: OK. Yeah, so I'm-- can\nyou hear me all right? PROFESSOR: Can anyone hear him?",
    "start": "58800",
    "end": "64720"
  },
  {
    "text": "Good. Good, yeah. GAURAV ARYA: OK. Yeah, yeah. I see myself. PROFESSOR: Yeah, yeah.",
    "start": "64720",
    "end": "70517"
  },
  {
    "text": "So I'll try and repeat any\nquestions and things like that. GAURAV ARYA: Yeah.",
    "start": "70517",
    "end": "76079"
  },
  {
    "text": "So I'm a junior at MIT. So yeah, as Professor\nJohnson said. I did a year with Professor\nJohnson previously.",
    "start": "76080",
    "end": "83580"
  },
  {
    "text": "And recently, I've been doing\na year in the Julia Lab, which is led by Professor Edelman.",
    "start": "83580",
    "end": "89580"
  },
  {
    "text": "And today, I'm going to be\ntalking about derivatives of random functions.",
    "start": "89580",
    "end": "94990"
  },
  {
    "text": "So I'm going to try and do this.",
    "start": "94990",
    "end": "101392"
  },
  {
    "text": "Push right-- oops. That's not what I wanted to do.",
    "start": "101392",
    "end": "106480"
  },
  {
    "text": "Yeah. So I wanted to start by just\ngiving a bit of motivation. So I'm going to just--",
    "start": "106480",
    "end": "112260"
  },
  {
    "text": "before I go into the\nrandom functions, I'm going to write\nthis function, which",
    "start": "112260",
    "end": "118110"
  },
  {
    "text": "I think you all probably know\nhow to differentiate better than I do at this point.",
    "start": "118110",
    "end": "123490"
  },
  {
    "text": "So here, A is a matrix, and this\nis a matrix-accepting function.",
    "start": "123490",
    "end": "130845"
  },
  {
    "text": "Can you read this all fine? PROFESSOR: Yep. We can see it. GAURAV ARYA: Yeah. So this is a function with\nmatrix input and matrix output.",
    "start": "130845",
    "end": "138760"
  },
  {
    "text": "And the reason I want\nto go through this again is just to set up\nan analogy with how I--",
    "start": "138760",
    "end": "147150"
  },
  {
    "text": "with differentiating\nrandom functions, what are the broad\nsteps that 1 takes in--",
    "start": "147150",
    "end": "152400"
  },
  {
    "text": "when you're faced with\na function like this. The essential complication\nis that your input and output",
    "start": "152400",
    "end": "158160"
  },
  {
    "text": "spaces are much\nmore complicated, and they're not\nreal-to-reals, like in single variable calculus.",
    "start": "158160",
    "end": "164110"
  },
  {
    "text": "So when you want to\ndifferentiate a function like this, it\nbecomes a lot harder",
    "start": "164110",
    "end": "169960"
  },
  {
    "text": "to just do it super\nmechanically like you might do for single variable calculus.",
    "start": "169960",
    "end": "175540"
  },
  {
    "text": "And it becomes a lot\nmore appealing to go back to the fundamental\nquestions that you",
    "start": "175540",
    "end": "181660"
  },
  {
    "text": "ask when taking a derivative. So the first might\nbe something like--",
    "start": "181660",
    "end": "186820"
  },
  {
    "text": "ultimately, derivatives\nare about when you perturb the input slightly,\nhow does the output perturb?",
    "start": "186820",
    "end": "193150"
  },
  {
    "text": "A map from input wiggles,\nthe output wiggles. So the first question\nyou might ask is just, if I perturb the input,\nhow does the output change?",
    "start": "193150",
    "end": "203080"
  },
  {
    "text": "So perturb the input. ",
    "start": "203080",
    "end": "210860"
  },
  {
    "text": "How does the output change? What's the sensitivity\nof the output with respect to the input?",
    "start": "210860",
    "end": "219190"
  },
  {
    "text": "So for the case of this\nfunction, what you've been-- what you've done in the class is\nyou may have written something",
    "start": "219190",
    "end": "226510"
  },
  {
    "text": "like this. You might have used\nthis notation df, which is the differential of f.",
    "start": "226510",
    "end": "231880"
  },
  {
    "text": "PROFESSOR: Yep. That's the notation. Yeah. GAURAV ARYA: Yeah. Yep. Sorry? PROFESSOR: That's\nthe notation, yes.",
    "start": "231880",
    "end": "237760"
  },
  {
    "text": "GAURAV ARYA: Yeah, OK. And you would have\nreplaced A with A plus dA.",
    "start": "237760",
    "end": "243985"
  },
  {
    "text": "So the A is your\nwiggle and the input, this is the differential. So we subtract off the\noriginal A squared.",
    "start": "243985",
    "end": "253000"
  },
  {
    "text": "You can expand this out. So the A squares cancel, so\nyou have-- you've done this,",
    "start": "253000",
    "end": "260049"
  },
  {
    "text": "I'm sure, quite a few times. Then we have this dA\nsquared term here.",
    "start": "260050",
    "end": "265990"
  },
  {
    "text": "And this-- now, I know that the next step\nyou're probably eager to do is to cancel out the dA squared.",
    "start": "265990",
    "end": "272440"
  },
  {
    "text": "But just this on its own is\nthe answer to question 1. We're not worried about\nneglecting the higher order",
    "start": "272440",
    "end": "282550"
  },
  {
    "text": "terms or forming the\nderivative, but just thinking about how do we write down\nthe change in the output.",
    "start": "282550",
    "end": "290169"
  },
  {
    "text": "And this is your function. So this is your\nanswer to question 1. And then the second thing is\nyou have this differential,",
    "start": "290170",
    "end": "299889"
  },
  {
    "text": "and you want to figure out what\nterms can you neglect here. So what terms do I keep,\nwhat terms do I neglect?",
    "start": "299890",
    "end": "306340"
  },
  {
    "text": "Because, really, our\ngoal at this point is to form this\nderivative, which",
    "start": "306340",
    "end": "311440"
  },
  {
    "text": "the idea of the\nderivative is to capture the sensitivity\nof f with respect",
    "start": "311440",
    "end": "317770"
  },
  {
    "text": "to A to the first order. The most important effects\nto the first order, and we have to drop the rest.",
    "start": "317770",
    "end": "324750"
  },
  {
    "text": "So the second question is what\nterms to neglect, like that.",
    "start": "324750",
    "end": "330800"
  },
  {
    "text": " Yeah. And here, you can just\ncross off the dA squared.",
    "start": "330800",
    "end": "339980"
  },
  {
    "text": "That's a higher order term. So you can justify it that way. And your derivative operator\nbecomes this map from dA",
    "start": "339980",
    "end": "350910"
  },
  {
    "text": "to what's left here. Right? The dA times A plus A times dA.",
    "start": "350910",
    "end": "360720"
  },
  {
    "text": "So I just wanted to separate\nout these two steps, because both of them\nare kind of non-trivial",
    "start": "360720",
    "end": "368460"
  },
  {
    "text": "in the case of random functions. So the first is just\nthinking about this map",
    "start": "368460",
    "end": "373710"
  },
  {
    "text": "from input perturbations\nto output perturbations. And the second is, OK, great. I understand that.",
    "start": "373710",
    "end": "379199"
  },
  {
    "text": "But now what terms\nshould I neglect to form this derivative?",
    "start": "379200",
    "end": "385590"
  },
  {
    "text": "Does that make sense? Are there any questions? PROFESSOR: Any questions? Nope. People are shaking their heads.",
    "start": "385590",
    "end": "391358"
  },
  {
    "text": "GAURAV ARYA: Great. PROFESSOR: They've\nseen this 100 times. GAURAV ARYA: So that is a\nmatrix-to-matrix function. But the idea there\nwas just thinking",
    "start": "391358",
    "end": "399990"
  },
  {
    "text": "about a function with a\nmore complicated input space and a more\ncomplicated output space. So now we're going to shift\nour attention to another case",
    "start": "399990",
    "end": "411099"
  },
  {
    "text": "where we were\ntrying to generalize the notion of the derivative\nto more complicated spaces.",
    "start": "411100",
    "end": "417520"
  },
  {
    "text": "And this time, we're considering\nfunctions which are random. So more concretely, I'm going\nto write a random function",
    "start": "417520",
    "end": "427940"
  },
  {
    "text": "with a capital letter\nX, which already might be a bit suggestive, because\ncapital letters are often used",
    "start": "427940",
    "end": "434360"
  },
  {
    "text": "for denoting random variables. And indeed, this is a random\nvariable value function.",
    "start": "434360",
    "end": "442740"
  },
  {
    "text": "So as a map, we might write\nthat it takes in some input P,",
    "start": "442740",
    "end": "452460"
  },
  {
    "text": "and it spits out X of P, where\nX of P is a random variable.",
    "start": "452460",
    "end": "463300"
  },
  {
    "text": "And I'm going to keep-- the output space is\npretty complicated, so I'm going to keep the\noutput space fairly simple.",
    "start": "463300",
    "end": "468449"
  },
  {
    "text": "The input, let's assume, is\njust a single real number. So P belongs to the reals.",
    "start": "468450",
    "end": "475280"
  },
  {
    "text": "So yeah. So now we want to ask\nthe same question. How-- So for our matrix function, we\nstarted with this map from to A",
    "start": "475280",
    "end": "484090"
  },
  {
    "text": "to A squared, and we got as\nour answer for the derivative another map, map from dA to\ndA times A plus A times dA.",
    "start": "484090",
    "end": "492520"
  },
  {
    "text": "And now we want to ask, what is\na useful notion of derivative?",
    "start": "492520",
    "end": "498099"
  },
  {
    "text": "First of all, what is our\nnotion of differential? How does the perturbation\nof the input affect--",
    "start": "498100",
    "end": "505300"
  },
  {
    "text": "if every time I\nrun this function I get a different value\nif it's a random function, what is a useful notion of\nsensitivity of the output,",
    "start": "505300",
    "end": "513740"
  },
  {
    "text": "and so forth? So we want to do\nthe same process on this random function.",
    "start": "513740",
    "end": "518926"
  },
  {
    "text": "PROFESSOR: So does\neveryone know what he means by a \"random variable.\" Like I said, suppose you flip\na coin, and if it's heads,",
    "start": "518926",
    "end": "527720"
  },
  {
    "text": "you return 1. And if it's tails, you return 0. All right? That would be a discrete\nrandom variable.",
    "start": "527720",
    "end": "532790"
  },
  {
    "text": "GAURAV ARYA: Yeah. Why don't I actually\nshow some examples? So let me first write them down. PROFESSOR: Yes. GAURAV ARYA: So we could have,\nlet's say, a random variable--",
    "start": "532790",
    "end": "543709"
  },
  {
    "text": "and one crucial thing is-- so\nthis is a random variable value function. So it's actually a whole family\nof random variables indexed",
    "start": "543710",
    "end": "551899"
  },
  {
    "text": "by parameter P. So\nlet's say one example you could have is X of P could\nbe following the Bernoulli",
    "start": "551900",
    "end": "562060"
  },
  {
    "text": "distribution with\nprobability P. This is your example of probability\nP of heads and otherwise tails.",
    "start": "562060",
    "end": "568480"
  },
  {
    "text": "Then maybe another example-- PROFESSOR: So that yeah. With probability P, you return\n1, and with probability--",
    "start": "568480",
    "end": "574585"
  },
  {
    "text": "GAURAV ARYA: Yeah. PROFESSOR: --1 minus P\nreturns 0, for example. GAURAV ARYA: Exactly. Yeah. And maybe another example.",
    "start": "574585",
    "end": "581060"
  },
  {
    "text": "So I'm going to write an\nexponential distribution here, and I'm not hoping to\nreally say what it is.",
    "start": "581060",
    "end": "587990"
  },
  {
    "text": "But the point is, this one's\na bit different in nature from the Bernoulli, because it's\nlike a continuous distribution.",
    "start": "587990",
    "end": "593016"
  },
  {
    "text": "And I'll show you how examples\nfrom that distribution look as well. So let me just try\nand share my screen.",
    "start": "593017",
    "end": "603380"
  },
  {
    "text": "Can you see-- PROFESSOR: Yep. GAURAV ARYA: You can see well? PROFESSOR: We can see\nyour screen, yeah. GAURAV ARYA: OK. So yeah.",
    "start": "603380",
    "end": "610610"
  },
  {
    "text": "So let's consider the\nBernoulli example. ",
    "start": "610610",
    "end": "615949"
  },
  {
    "text": "So X of P-- so really, what I'm\nwriting here is a program",
    "start": "615950",
    "end": "621230"
  },
  {
    "text": "that samples from X of P. So I'll call it sampleX(p). And in this case, this\nis basically a sampling",
    "start": "621230",
    "end": "630130"
  },
  {
    "text": "from a Bernoulli distribution. So this thing inside of the\nrand() is the distribution. But we're getting\nsamples from it,",
    "start": "630130",
    "end": "636149"
  },
  {
    "text": "so that's why there's\nthis rand in front. OK, so now I can run this\nprogram with X of 0.6",
    "start": "636150",
    "end": "642990"
  },
  {
    "text": "and sampling from X of 0.6. In this case, it's a 1 or 0. In this case, it's\nusing a Boolean",
    "start": "642990",
    "end": "648360"
  },
  {
    "text": "to represent the output. But the point is\nthat you're going to get true 60% of the time\nand false 40% of the time.",
    "start": "648360",
    "end": "657100"
  },
  {
    "text": "So you can verify this by\ntaking a bunch of samples. PROFESSOR: In Julia, true is\nequivalent to the number 1,",
    "start": "657100",
    "end": "664710"
  },
  {
    "text": "and false is equivalent\nto the number 0. GAURAV ARYA: Yeah. And then let's just stare\nat the other example",
    "start": "664710",
    "end": "671340"
  },
  {
    "text": "of an exponential distribution. So the Bernoulli\nis what's called a discrete random variable.",
    "start": "671340",
    "end": "678180"
  },
  {
    "text": "And now for an\nexponential distribution, maybe I'll make P 100. And now it's doing some\ncontinuous random thing.",
    "start": "678180",
    "end": "686128"
  },
  {
    "text": "But the point is you get a\ndifferent output every time. Now, the question is-- PROFESSOR: Do people know\nwhat the exponential--",
    "start": "686128",
    "end": "691214"
  },
  {
    "text": "GAURAV ARYA: --differentiation. So someone codes up a function-- PROFESSOR: One sec. One second. GAURAV ARYA: Yeah. PROFESSOR: Do people know what\nthe exponential distribution",
    "start": "691215",
    "end": "696390"
  },
  {
    "text": "means? It means the probability-- you're giving a\nnon-negative real number.",
    "start": "696390",
    "end": "703000"
  },
  {
    "text": "It's returning continuously out. But the probability\ndecays exponentially as it gets larger.",
    "start": "703000",
    "end": "708930"
  },
  {
    "text": "And so P here is just the\nscale of that exponential. So your-- the real number\nis in the order of--",
    "start": "708930",
    "end": "715800"
  },
  {
    "text": "sampleX(100) is real\nnumbers in the order of 100. It can be larger. It can be smaller. But as it gets\nbigger than 100, it",
    "start": "715800",
    "end": "722100"
  },
  {
    "text": "gets exponentially less\nlikely, and then follows an exponential curve. GAURAV ARYA: Right.",
    "start": "722100",
    "end": "727920"
  },
  {
    "text": "Yeah. PROFESSOR: But it's continuous\nnumbers that are coming out. GAURAV ARYA: Yeah, so I\nshould have said that. So here, I'm parameterizing\nexponential distribution",
    "start": "727920",
    "end": "734790"
  },
  {
    "text": "by its scale. Yeah. So another question is, what is\na useful notion of derivative",
    "start": "734790",
    "end": "742649"
  },
  {
    "text": "for programs like this? Someone codes up a\nprogram that samples from an exponential\ndistribution.",
    "start": "742650",
    "end": "749579"
  },
  {
    "text": "What's a useful\nnotion of derivatives? So there's two questions here. There's how do we find this\nuseful notion of derivative,",
    "start": "749580",
    "end": "757740"
  },
  {
    "text": "but there's also the why. Why would we-- for the\nfunctions you considered so far,",
    "start": "757740",
    "end": "765940"
  },
  {
    "text": "matrix value functions,\nthe why is pretty clear,",
    "start": "765940",
    "end": "770950"
  },
  {
    "text": "in that gradients are useful\nfor optimizing functions and so forth. And if you have a totally\ndeterministic function,",
    "start": "770950",
    "end": "778759"
  },
  {
    "text": "then it's very\nclear what it means to optimize that function. You have some\nobjective and you're trying to make it as\nsmall as possible,",
    "start": "778760",
    "end": "784695"
  },
  {
    "text": "and that's why you\nwant to take gradients. But for these random\nfunctions, maybe it's",
    "start": "784695",
    "end": "791050"
  },
  {
    "text": "not immediately obvious why we\nwant any notion of derivative for these programs.",
    "start": "791050",
    "end": "796550"
  },
  {
    "text": "So why do we want to\ndifferentiate these? And maybe this is a question\nI can ask to the class.",
    "start": "796550",
    "end": "804100"
  },
  {
    "text": "Why might we want\nto differentiate these sorts of programs? PROFESSOR: Does\nanyone have any notion",
    "start": "804100",
    "end": "811060"
  },
  {
    "text": "of why you would want to\ndifferentiate whatever deri-- we haven't\ndefined what it means, but why would we want the\nsensitivity, in some sense,",
    "start": "811060",
    "end": "818410"
  },
  {
    "text": "of that program with\nrespect to some parameter, even though the\noutputs are random?",
    "start": "818410",
    "end": "823930"
  },
  {
    "text": "Yeah. Someone raised-- yeah? AUDIENCE: Maybe\nmaximize the likelihood of something occurring? PROFESSOR: Maybe to maximize\nthe likelihood of something",
    "start": "823930",
    "end": "830403"
  },
  {
    "text": "occurring, for example. That's-- GAURAV ARYA: OK. Yeah, yeah, yeah. PROFESSOR: Another one? Yeah?",
    "start": "830403",
    "end": "835590"
  },
  {
    "text": "AUDIENCE: In physics, there are\nlots of systems with disorder that are random, in some sense.",
    "start": "835590",
    "end": "841399"
  },
  {
    "text": "And if you'd like to measure\nproperties of that system, then you need to take the\nrandomness into account. PROFESSOR: Yeah.",
    "start": "841400",
    "end": "846977"
  },
  {
    "text": "So if you have a physical\nsystem with randomness, and you want to measure the\nsensitivity of something,",
    "start": "846977",
    "end": "854649"
  },
  {
    "text": "some statistical quantity to-- GAURAV ARYA: Exactly, yeah. PROFESSOR: --some\nphysical parameter. Right. GAURAV ARYA: Yeah, yeah.",
    "start": "854650",
    "end": "859810"
  },
  {
    "text": "So both those answers are,\nyeah, totally correct. So basically, the idea\nis maybe you're normally",
    "start": "859810",
    "end": "866589"
  },
  {
    "text": "interested in some statistical\nquantities of your system. So X is your random variable.",
    "start": "866590",
    "end": "873790"
  },
  {
    "text": "But ultimately, it\nmight be computing some stochastic estimate\nwhich, on average, is",
    "start": "873790",
    "end": "882190"
  },
  {
    "text": "going to give you something you\nwant to minimize, for example. So I misspelled that.",
    "start": "882190",
    "end": "889210"
  },
  {
    "text": "So the physical\nquantity basically mean things that are\nconstructed using averages. So they're based\non expectations.",
    "start": "889210",
    "end": "895990"
  },
  {
    "text": " And you may want to get\nthese expectations somewhere",
    "start": "895990",
    "end": "904280"
  },
  {
    "text": "you want them, and you want to\nfind a P that maybe minimizes some expectation, and so forth.",
    "start": "904280",
    "end": "910300"
  },
  {
    "text": "So broadly, this is the why. And there's one more other\npoint today, which might--",
    "start": "910300",
    "end": "915790"
  },
  {
    "text": "which is that, sure, you\nhave this system which is, in principle, stochastic.",
    "start": "915790",
    "end": "921000"
  },
  {
    "text": "But you're interested in\nmaybe its average value. So why not just\nwrite a program which",
    "start": "921000",
    "end": "927690"
  },
  {
    "text": "directly computes\nthat average value, and is therefore deterministic? If you have X of\nP, then the program",
    "start": "927690",
    "end": "936390"
  },
  {
    "text": "you might be interested in\nis actually the expectation of X of P's average value. And this is a deterministic\nfunction of P.",
    "start": "936390",
    "end": "944040"
  },
  {
    "text": "So why not just code that\nup and differentiate that? And the answer why we\ndon't do that sometimes--",
    "start": "944040",
    "end": "952470"
  },
  {
    "text": "PROFESSOR: So quick question. GAURAV ARYA: Yeah? PROFESSOR: So quick question. So for the Bernoulli\nfunction that returned 1",
    "start": "952470",
    "end": "960900"
  },
  {
    "text": "with probability P and 0\nwith probability 1 minus P, what's the expectation\nvalue of that?",
    "start": "960900",
    "end": "968310"
  },
  {
    "text": "It's just P. Right? So yeah. Yeah. GAURAV ARYA: Yeah. PROFESSOR: And P\nis an easy function",
    "start": "968310",
    "end": "973680"
  },
  {
    "text": "to differentiate with\nrespect to P. Right? GAURAV ARYA: Yeah, yeah. ",
    "start": "973680",
    "end": "981773"
  },
  {
    "text": "So that's right. But you might have, for\nexample, a very simple-- a random walk. In maybe five lines of\ncode with a for loop,",
    "start": "981773",
    "end": "987750"
  },
  {
    "text": "you can write a random walk\nwhich walks left-to-right. Maybe it changes the\nprobability of left-to-right",
    "start": "987750",
    "end": "993105"
  },
  {
    "text": "depending on where you are. All that you can do\nin five lines of code. And now it becomes a lot easier\nto sample from this process",
    "start": "993105",
    "end": "1001070"
  },
  {
    "text": "than to analytically\ncompute its average. To compute its\naverage, you might have to look at the transition\nmatrix or something like that.",
    "start": "1001070",
    "end": "1007542"
  },
  {
    "text": "But it's super easy to\nsample from the process. So that's the point. It's often a lot\neasier to sample.",
    "start": "1007542",
    "end": "1017600"
  },
  {
    "text": "And therefore, these\nsamples are often called unbiased estimates,\nbecause on average, they're",
    "start": "1017600",
    "end": "1023300"
  },
  {
    "text": "giving you the right thing\nthan to compute analytically.",
    "start": "1023300",
    "end": "1029417"
  },
  {
    "start": "1029417",
    "end": "1034780"
  },
  {
    "text": "Exactly compute. ",
    "start": "1034780",
    "end": "1041890"
  },
  {
    "text": "Yeah. So then maybe I'll also give\nsome more concrete examples. So one example is in\ndeep learning, in ML,",
    "start": "1041890",
    "end": "1052420"
  },
  {
    "text": "there is this very\npopular architecture called the variational\nautoencoder, or VAE.",
    "start": "1052420",
    "end": "1060640"
  },
  {
    "text": "And this architecture\nhas randomness baked into its model. It's not just randomness from\nselecting a subset of examples",
    "start": "1060640",
    "end": "1069550"
  },
  {
    "text": "to train on, because it's\nintrinsically random. So the loss function of this VAE\nwould again be defined strictly",
    "start": "1069550",
    "end": "1080410"
  },
  {
    "text": "speaking as an expectation. But computationally, the model\ngives you the ability to sample",
    "start": "1080410",
    "end": "1089140"
  },
  {
    "text": "from this X of P. That's\nwhat super easy to do, whereas computing this\nanalytically is hard.",
    "start": "1089140",
    "end": "1094680"
  },
  {
    "text": "So the question is, if\nyou have a process-- if you have a program\nfrom which you can only get stochastic\nestimates of this,",
    "start": "1094680",
    "end": "1101990"
  },
  {
    "text": "we might be interested\nin dL of P by dP,",
    "start": "1101990",
    "end": "1108850"
  },
  {
    "text": "but how are we\ngoing to get that? Because we can't--\nwe're no longer-- we're not going directly\nfrom here to here.",
    "start": "1108850",
    "end": "1115879"
  },
  {
    "text": "Somehow, we have to start here\nat X of P and get dL by dP.",
    "start": "1115880",
    "end": "1122040"
  },
  {
    "text": "So this is one example. Let me give you another\nexample, which to me, is",
    "start": "1122040",
    "end": "1130352"
  },
  {
    "text": "the most compelling example. Now, it's sort of\nalready set, which is in the physical\nsciences, your model",
    "start": "1130353",
    "end": "1140790"
  },
  {
    "text": "may be inherently stochastic. Your model is stochastic. For example, you might have\ntwo molecules or two particles,",
    "start": "1140790",
    "end": "1152250"
  },
  {
    "text": "and they're interacting\nwith a rate r. They're binding\nor something that.",
    "start": "1152250",
    "end": "1158920"
  },
  {
    "text": "But this is the average\nrate of interaction. Whereas in reality, if you let\nthese molecules roam around,",
    "start": "1158920",
    "end": "1167130"
  },
  {
    "text": "they're going to interact--\nthe times at which they're going to interact is going to\nbe a stochastic process in time.",
    "start": "1167130",
    "end": "1174059"
  },
  {
    "text": "For example, in\nthis case, if you have a rate r of\ninteraction, then",
    "start": "1174060",
    "end": "1180120"
  },
  {
    "text": "the times at which\nthey interact, that's called a Poisson process.",
    "start": "1180120",
    "end": "1185355"
  },
  {
    "text": "And if you say, how long am I\ngoing to wait until they first interact, that's\nactually distributed",
    "start": "1185355",
    "end": "1192540"
  },
  {
    "text": "as an exponential distribution\nwith rate-- with scale r, or maybe 1 over r.",
    "start": "1192540",
    "end": "1198240"
  },
  {
    "text": "I'll have to think about that. It should be 1 over r rates,\nbecause the higher the rate is,",
    "start": "1198240",
    "end": "1205540"
  },
  {
    "text": "smaller-- the less time you'd\nexpect to wait. But the main point here is\nthat your model of the system--",
    "start": "1205540",
    "end": "1212538"
  },
  {
    "text": "you're interested in\nsome physical process, and your model of it is\ninherently stochastic. So here, there's no\ngetting around the fact",
    "start": "1212538",
    "end": "1220270"
  },
  {
    "text": "that you have to deal\nwith stochasticity. You can't just switch to\na different model that doesn't have stochasticity,\nbecause then you're",
    "start": "1220270",
    "end": "1227860"
  },
  {
    "text": "tied to this physical process. So this is another\ncase where you have",
    "start": "1227860",
    "end": "1233215"
  },
  {
    "text": "to deal with stochasticity. And now if you\nwant to-- let's say you have some model\nof what's going on,",
    "start": "1233215",
    "end": "1238970"
  },
  {
    "text": "but you don't know\nyour parameters. For example, you don't know r. Then you want to fit\nyour parameters to data.",
    "start": "1238970",
    "end": "1246900"
  },
  {
    "text": "And this is, again, some sort\nof optimization or inference problem. So you have some real\nworld data and you",
    "start": "1246900",
    "end": "1253040"
  },
  {
    "text": "want to optimize the\nparameters of your model to best fit the data. This is some\noptimization problem,",
    "start": "1253040",
    "end": "1258350"
  },
  {
    "text": "and again, we're dealing\nwith the same thing, which is have a system where it's\nvery easy to simulate it",
    "start": "1258350",
    "end": "1263480"
  },
  {
    "text": "in a stochastic\nway, but maybe it's really hard to exactly compute\nthe statistical quantities",
    "start": "1263480",
    "end": "1269389"
  },
  {
    "text": "for the system. So again, you want to solve\nthe same problem, which is that we're interested in the\nderivative of the expectation",
    "start": "1269390",
    "end": "1281720"
  },
  {
    "text": "of your program with\nrespect to P. OK, so our derivative is not--\nthis doesn't mean-- this isn't",
    "start": "1281720",
    "end": "1289600"
  },
  {
    "text": "our answer to the derivative,\nbut it's something we'd like our derivative\nto help us compute,",
    "start": "1289600",
    "end": "1296365"
  },
  {
    "text": "like if we're happy with\nmany different notions of the derivative. But at least let me compute\nthis using the notion",
    "start": "1296365",
    "end": "1302620"
  },
  {
    "text": "of using the derivative. And it's also a bit ambitious\nto hope to get this exactly",
    "start": "1302620",
    "end": "1308470"
  },
  {
    "text": "analytically, because\nwe couldn't even get the original\nexpectation analytically. So a more reasonable\ngoal might be--",
    "start": "1308470",
    "end": "1318530"
  },
  {
    "text": "so this is getting into\nautomatic differentiation. It's like you're given\na program X of P, and your goal is to produce\na new program, which",
    "start": "1318530",
    "end": "1325300"
  },
  {
    "text": "I'll call X tilde of P, which\naverages to the derivative of--",
    "start": "1325300",
    "end": "1335220"
  },
  {
    "text": "sorry. My screen turned off. Which averages to the\nderivative of the average",
    "start": "1335220",
    "end": "1340830"
  },
  {
    "text": "of your original program,\nwhich is a bit of a mouthful. So in machine\nlearning, this is often",
    "start": "1340830",
    "end": "1346230"
  },
  {
    "text": "called a gradient estimator\nthat you're trying to find. And this isn't necessarily--\neven X tilde of P,",
    "start": "1346230",
    "end": "1355680"
  },
  {
    "text": "this may not be our\nnotion of derivative that we're looking for. But it's certainly\nsomething we'd",
    "start": "1355680",
    "end": "1361770"
  },
  {
    "text": "like to be able to construct\nfrom whatever notion of derivative we come up with.",
    "start": "1361770",
    "end": "1367269"
  },
  {
    "text": "So this is the\nmotivation for why we might want to\ncharacterize the sensitivity of a random program, why\nwe might want a derivative.",
    "start": "1367270",
    "end": "1373794"
  },
  {
    "text": "PROFESSOR: Is everyone\nclear on this at this point? So this-- you're\ngoing to try and find a random program\nwhose average is",
    "start": "1373795",
    "end": "1380230"
  },
  {
    "text": "the derivative of the average\nof the original program. All right? So it's kind of a\nlittle mouthful.",
    "start": "1380230",
    "end": "1385870"
  },
  {
    "text": " OK, people are\nnodding their heads. So-- ",
    "start": "1385870",
    "end": "1393372"
  },
  {
    "text": "GAURAV ARYA: OK. So that is the why\nwe want to do this. And now the question is, how?",
    "start": "1393373",
    "end": "1399320"
  },
  {
    "text": "What notion of\nderivative should we come up with to let us do this? So yeah.",
    "start": "1399320",
    "end": "1406160"
  },
  {
    "text": "So once again, the functions\nwe're interested in look this. P maps to X of P. So now let's\nthink back to the beginning",
    "start": "1406160",
    "end": "1417860"
  },
  {
    "text": "where we had these\ntwo questions. One was, just think\nabout the differential, or just how the output\nis changing with respect",
    "start": "1417860",
    "end": "1425630"
  },
  {
    "text": "to the input, without\nworrying about turning it into a derivative that cuts\nout higher order things.",
    "start": "1425630",
    "end": "1431779"
  },
  {
    "text": "Just, how does the program-- how do perturbations of\nthe program look like? So let's try and\nanswer that first.",
    "start": "1431780",
    "end": "1438020"
  },
  {
    "text": "Let's answer question 1 first.  So the first thing I'm\ngoing to write is--",
    "start": "1438020",
    "end": "1444290"
  },
  {
    "text": " just try to naively characterize\nthe change in my program.",
    "start": "1444290",
    "end": "1455630"
  },
  {
    "text": "So here, I'm using slightly\ndifferent notation from before,",
    "start": "1455630",
    "end": "1460690"
  },
  {
    "text": "and maybe Professor Johnson\nor Professor Edelman might object to this. But this is what I'm\ncalling the differential",
    "start": "1460690",
    "end": "1469195"
  },
  {
    "text": "for a stochastic case. This is a stochastic\ndifferential,",
    "start": "1469195",
    "end": "1476140"
  },
  {
    "text": "and I'm writing it as-- So basically,\nepsilon is your dP. So this right-hand\nside should make sense.",
    "start": "1476140",
    "end": "1482720"
  },
  {
    "text": "It's basically X of P\nplus dP minus X of P. One thing I've done\nhere is I'm not super comfortable with\ndealing with differentials",
    "start": "1482720",
    "end": "1489990"
  },
  {
    "text": "as infinitesimal things, so\nI've written this left-hand side as a function. dX of epsilon equals X of P\nplus epsilon minus X of P.",
    "start": "1489990",
    "end": "1500230"
  },
  {
    "text": "So this is just\nexactly what it says. There is no dropping of\nhigher order terms here.",
    "start": "1500230",
    "end": "1508630"
  },
  {
    "text": "And epsilon, I'm thinking as-- so remember P was real. So epsilon, I'm thinking,\nis some small real number.",
    "start": "1508630",
    "end": "1514860"
  },
  {
    "text": "So this is basically just,\nfor a finite perturbation in the input, how does\nthe output perturb?",
    "start": "1514860",
    "end": "1521472"
  },
  {
    "text": "PROFESSOR: Yeah, so this is-- GAURAV ARYA:\nAlthough I might not have made it totally\nclear what this means yet. That'll come next. PROFESSOR: Yeah. So in lecture--",
    "start": "1521472",
    "end": "1526520"
  },
  {
    "text": "GAURAV ARYA: But\nI'm interested if-- PROFESSOR: In lecture 1,\nthis was what we called-- in lecture 1, this is what we\ncalled delta X. So delta X--",
    "start": "1526520",
    "end": "1532963"
  },
  {
    "text": "GAURAV ARYA: Yeah, OK. PROFESSOR: --was the\nnondifferential change. But that's fine. You can stick with\nyour favorite notation.",
    "start": "1532963",
    "end": "1538159"
  },
  {
    "text": "Don't change\nnotations midstream. GAURAV ARYA: Sure, yeah. So yeah. But this is-- so this idea--",
    "start": "1538160",
    "end": "1544850"
  },
  {
    "text": "but then, I guess, now you have\nto ask, what sort of object is this dX of epsilon?",
    "start": "1544850",
    "end": "1550920"
  },
  {
    "text": "So maybe this is also\nsomething I can ask the class. X of P was a random\nvariable and so forth,",
    "start": "1550920",
    "end": "1556830"
  },
  {
    "text": "so what is the dX of epsilon? ",
    "start": "1556830",
    "end": "1565574"
  },
  {
    "text": "PROFESSOR: So what is it? Yeah? AUDIENCE: A random variable. PROFESSOR: It's a random\nvariable is the answer.",
    "start": "1565574",
    "end": "1570759"
  },
  {
    "text": "GAURAV ARYA: Yeah, yeah. Generally, if you subtract\ntwo random variables, what you're going to get\nis another random variable.",
    "start": "1570760",
    "end": "1576870"
  },
  {
    "text": "However, I'd argue that I\nhaven't really fully specified to you what dX epsilon is.",
    "start": "1576870",
    "end": "1582106"
  },
  {
    "text": "You're right it should\nbe a random variable, but I think it\ncould be a whole lot of random variables dependent\non something I haven't really",
    "start": "1582107",
    "end": "1588830"
  },
  {
    "text": "told you. Does anyone see why-- is it totally unambiguous,\nor-- let's think of one--",
    "start": "1588830",
    "end": "1595880"
  },
  {
    "text": "let's think of a\nconcrete example. Let's think of the\ncase where X of P is distributed as the\nexponential distribution",
    "start": "1595880",
    "end": "1602510"
  },
  {
    "text": "with scale P. For\nthis case, is it",
    "start": "1602510",
    "end": "1609330"
  },
  {
    "text": "unambiguous what dX of\nepsilon is, or is it unclear? PROFESSOR: So what's-- is\nsomething missing from this",
    "start": "1609330",
    "end": "1615930"
  },
  {
    "text": "definition? Yeah? Someone's raising-- yeah? AUDIENCE: I guess by \"change,\"\ndo you mean a CDF change?",
    "start": "1615930",
    "end": "1623059"
  },
  {
    "text": "Or, like-- I'm not\nreally sure what you mean by \"subtracting\nrandom variables.\" PROFESSOR: Yeah. So the question is--",
    "start": "1623060",
    "end": "1628640"
  },
  {
    "text": "the comment was he\nwasn't sure what you mean by \"subtracting\nrandom variables.\" Do you mean the CDF change?",
    "start": "1628640",
    "end": "1633889"
  },
  {
    "text": "Or what is this minus? GAURAV ARYA: That's\na good question. So what I mean is you--",
    "start": "1633890",
    "end": "1640790"
  },
  {
    "text": "so a random variable is\nbasically telling you how to sample from something. A random variable says how you--",
    "start": "1640790",
    "end": "1647210"
  },
  {
    "text": "it gives you the procedure for\nsampling from a distribution. So I'm saying, take a sample-- I'm going to construct a new\nrandom variable which says,",
    "start": "1647210",
    "end": "1653880"
  },
  {
    "text": "how do you sample from dX? You sample from X\nof P plus epsilon, you sample from X of P, and\nyou subtract so that is--",
    "start": "1653880",
    "end": "1662120"
  },
  {
    "text": "yeah. PROFESSOR: It's a\nsampling procedure. OK. GAURAV ARYA: So it's very\ndifferent from doing operations with distributions. If you were to look\nat this-- if you're",
    "start": "1662120",
    "end": "1668568"
  },
  {
    "text": "trying to figure out\nthe distribution of dX in terms of the\ndistribution of these two, when you add two\nrandom variables,",
    "start": "1668568",
    "end": "1674230"
  },
  {
    "text": "the distribution convolves. You convolve the distribution. But if you think\nsample-wise, then it's",
    "start": "1674230",
    "end": "1679450"
  },
  {
    "text": "just a simple subtraction.  Yeah. But why might this still\nnot be fully specified?",
    "start": "1679450",
    "end": "1687590"
  },
  {
    "text": "PROFESSOR: Any other ideas\nwhat might be missing? Yeah? AUDIENCE: Is P Fixed, or--",
    "start": "1687590",
    "end": "1694840"
  },
  {
    "text": "PROFESSOR: Is P fixed? I guess, is that-- GAURAV ARYA: P is fixed. Actually, I should\nhave said that. So that's a good point. So in some sense,\nthere's a dependence on P",
    "start": "1694840",
    "end": "1701710"
  },
  {
    "text": "here that I'm sort\nof suppressing. Imagining you\nconsider a fixed P, and now you're thinking about\na neighborhood of P. But yeah.",
    "start": "1701710",
    "end": "1708280"
  },
  {
    "text": "So that is true. Yeah. PROFESSOR: Any other ideas?",
    "start": "1708280",
    "end": "1715350"
  },
  {
    "text": "No? We're drawing a blank, Gaurav. GAURAV ARYA: OK. So we know what's so-called the\nmarginal distributions of each",
    "start": "1715350",
    "end": "1724669"
  },
  {
    "text": "of these random variables. We know how to sample\nfrom each of these. But there's a question of what\ntheir joint distribution is,",
    "start": "1724670",
    "end": "1734680"
  },
  {
    "text": "which means-- for example, if you have-- Suppose you were considering X\nof P minus X of P for a second.",
    "start": "1734680",
    "end": "1744985"
  },
  {
    "text": " Well, maybe that's\na bit too confusing.",
    "start": "1744985",
    "end": "1751030"
  },
  {
    "text": "Let's say we're considering--\nso let me go to a new page. Let's go away from that\ncontext for a second",
    "start": "1751030",
    "end": "1757179"
  },
  {
    "text": "and let's consider subtracting\ntwo random variables, A and B, where let's say A and B\nfollow the same distribution.",
    "start": "1757180",
    "end": "1763640"
  },
  {
    "text": "So A and B follow the\nsame distribution. Let's call it some\ndistribution d.",
    "start": "1763640",
    "end": "1770049"
  },
  {
    "text": " Is this going to equal 0?",
    "start": "1770050",
    "end": "1775690"
  },
  {
    "text": "Like-- If the answers are yes--",
    "start": "1775690",
    "end": "1784695"
  },
  {
    "text": "PROFESSOR: So if A\nis a-- if A and B are both coin flips with\nprobability 1/2 half they're 1,",
    "start": "1784696",
    "end": "1792040"
  },
  {
    "text": "and probability 1 they're\n0, is the difference 0? People are shaking\ntheir heads no.",
    "start": "1792040",
    "end": "1798490"
  },
  {
    "text": "GAURAV ARYA: Yeah. So it actually depends. Because if A and\nB are independent,",
    "start": "1798490",
    "end": "1803960"
  },
  {
    "text": "like you would imagine\nfor two coin flips, then it would not always be 0. If on the other hand, A and\nB could be the same thing,",
    "start": "1803960",
    "end": "1811059"
  },
  {
    "text": "they could literally equal\nA, in which case, yes, this would be 0. So this is the idea of\na joint distribution.",
    "start": "1811060",
    "end": "1820180"
  },
  {
    "text": "Even if you know how to\nsample from a random variable marginally, you can figure out,\nin the whole probability space,",
    "start": "1820180",
    "end": "1827980"
  },
  {
    "text": "do they-- how do they\nbehave together as they-- so in some senses, what\nwe haven't specified",
    "start": "1827980",
    "end": "1833230"
  },
  {
    "text": "is a distribution of A comma\nB. Does that make sense? PROFESSOR: Yeah.",
    "start": "1833230",
    "end": "1838330"
  },
  {
    "text": "So are they the same\ncoin flip, or are they independent coin flips,\nor something in between? Yeah.",
    "start": "1838330",
    "end": "1844030"
  },
  {
    "text": "People are shaking their\nheads yes, so I think-- GAURAV ARYA: OK. So let's now go back\nto our particular case",
    "start": "1844030",
    "end": "1852520"
  },
  {
    "text": "of subtracting two\nrandom variables where we had X of P plus\nepsilon minus X of P.",
    "start": "1852520",
    "end": "1865430"
  },
  {
    "text": "And my claim is it's not enough\nto say that this thing is distributed as an exponential\ndistribution with rate P",
    "start": "1865430",
    "end": "1872840"
  },
  {
    "text": "plus epsilon, and this\nthing is distributed as an exponential\ndistribution with rate P. This",
    "start": "1872840",
    "end": "1879320"
  },
  {
    "text": "doesn't tell you enough. You have to specify how\nthey behave jointly.",
    "start": "1879320",
    "end": "1884330"
  },
  {
    "text": "And perhaps the most\nsimple thing to do is to say, why don't\nwe make these two",
    "start": "1884330",
    "end": "1890840"
  },
  {
    "text": "random variables independent? So basically, it means these\nsampling processes are not",
    "start": "1890840",
    "end": "1896270"
  },
  {
    "text": "tied to each other at all. We're going to take a\nsample from a sample from X of P, a sample of X of P plus\nepsilon, and subtract them.",
    "start": "1896270",
    "end": "1905340"
  },
  {
    "text": "So let's actually see\nhow that works in code. ",
    "start": "1905340",
    "end": "1912540"
  },
  {
    "text": "So yeah. So my sampleX is already set\nup as an exponential random variable, and now I'm\ngoing to sample_dx,",
    "start": "1912540",
    "end": "1921960"
  },
  {
    "text": "which takes as input epsilon. This is a little confusing. Let me put that.",
    "start": "1921960",
    "end": "1927780"
  },
  {
    "text": "And I'm going to fix p at 100. So then this looks like\nsampleX of 100 plus",
    "start": "1927780",
    "end": "1935730"
  },
  {
    "text": "epsilon minus sampleX at 100. And now let's try sampling\nfrom dX just to get a feel.",
    "start": "1935730",
    "end": "1945200"
  },
  {
    "text": "So you can sample dX. Maybe let's assume a small\nperturbation, like 0.1.",
    "start": "1945200",
    "end": "1951110"
  },
  {
    "text": "And we can see it looks like a\nrandom variable, as was said. And it takes fairly\nlarge values.",
    "start": "1951110",
    "end": "1957169"
  },
  {
    "text": "Here's a negative 145. Maybe let's make epsilon\nreally, really tiny.",
    "start": "1957170",
    "end": "1964930"
  },
  {
    "text": "It still takes\nfairly large values. It still looks pretty large. And at this point,\nmaybe alarm bells",
    "start": "1964930",
    "end": "1970710"
  },
  {
    "text": "should be ringing\nbecause in a derivative, you sort of have this\nidea that if you perturb the input by a smaller\nand smaller amount,",
    "start": "1970710",
    "end": "1977820"
  },
  {
    "text": "the output, in some sense\nof size, some sense of norm, should get smaller and\nsmaller and smaller.",
    "start": "1977820",
    "end": "1985590"
  },
  {
    "text": "Whereas here, this\nisn't happening at all. So if you were to try and\ndo something like divide",
    "start": "1985590",
    "end": "1992760"
  },
  {
    "text": "by your step size, as you often\ntry to do with derivatives, this looks concerning\nbecause, sure, this might",
    "start": "1992760",
    "end": "2000550"
  },
  {
    "text": "average to the right thing. But its variance is\ngoing to be really large because the right thing might\nbe something like, I don't know,",
    "start": "2000550",
    "end": "2006940"
  },
  {
    "text": "1. And we're getting these\nnumbers of order 10 to the 7, which, if\nyou trust the math,",
    "start": "2006940",
    "end": "2013597"
  },
  {
    "text": "are going to average to 1. But if you actually want\nto do this computationally, you're in trouble. So in making all of these,\nmaking these independent",
    "start": "2013597",
    "end": "2023790"
  },
  {
    "text": "is not a good idea. Are people convinced by this? Are there any questions?",
    "start": "2023790",
    "end": "2028910"
  },
  {
    "text": " PROFESSOR: So I think people\nare shaking their heads OK.",
    "start": "2028910",
    "end": "2036370"
  },
  {
    "text": "GAURAV ARYA: OK. All right. So what could be a\nbetter approach to this?",
    "start": "2036370",
    "end": "2044440"
  },
  {
    "text": "Well, here we have to-- so let's again stick\nto our example.",
    "start": "2044440",
    "end": "2049949"
  },
  {
    "text": "And all these papers, I'm going\nto fill up all their nooks and crannies later.",
    "start": "2049949",
    "end": "2055860"
  },
  {
    "text": "I feel bad about\nwasting so much.  Yeah, so let's go\nto this example.",
    "start": "2055860",
    "end": "2062320"
  },
  {
    "text": "And now we have this\nfamily of random variables. And we need to figure out how\nthey work with each other.",
    "start": "2062320",
    "end": "2069815"
  },
  {
    "text": "How are they\ndistributed jointly?  And here I have to introduce\nsome more concepts.",
    "start": "2069815",
    "end": "2078550"
  },
  {
    "text": "So how is a random\nvariable actually defined? We're trying to\ndifferentiate these objects.",
    "start": "2078550",
    "end": "2084690"
  },
  {
    "text": "Maybe we ought to understand\nthese objects a bit better. So this is getting into a\nbit of probability theory.",
    "start": "2084690",
    "end": "2090510"
  },
  {
    "text": "But basically, there's\nsomething called a sample space. ",
    "start": "2090510",
    "end": "2099190"
  },
  {
    "text": "Omega. This is a space of things. And it's equipped with a\nprobability distribution p.",
    "start": "2099190",
    "end": "2117995"
  },
  {
    "text": "Some other way you\ncan think about this is there's some random variable\nwith output space omega. But here I've actually\nwritten the distribution.",
    "start": "2117995",
    "end": "2125760"
  },
  {
    "text": "And then your random variable. Let's say any x of p is a map.",
    "start": "2125760",
    "end": "2134849"
  },
  {
    "text": "And also, for simplicity,\nI said p was real. Let's also make x of p a\nreal valued random variable,",
    "start": "2134850",
    "end": "2140670"
  },
  {
    "text": "for simplicity. In which case, x of p is defined\nas a map from omega to R.",
    "start": "2140670",
    "end": "2150140"
  },
  {
    "text": "So this is a bit\nconfusing because this is like a deterministic function.",
    "start": "2150140",
    "end": "2155309"
  },
  {
    "text": "So how are we defining\nrandom variables through a\ndeterministic function? The idea is that all of\nthe randomness is here.",
    "start": "2155310",
    "end": "2164130"
  },
  {
    "text": "This is a distribution\nwhich is independent of p. This distribution\ndoesn't depend on p.",
    "start": "2164130",
    "end": "2170710"
  },
  {
    "text": "And then we have a fixed\ndistribution, a fixed source of randomness, for\nall of our family, all",
    "start": "2170710",
    "end": "2176410"
  },
  {
    "text": "of the random variables\nin our family. And what differs within our\nfamily is what the map is.",
    "start": "2176410",
    "end": "2183380"
  },
  {
    "text": "This map is parametrized by p. So x, you can write\nx of p on the main--",
    "start": "2183380",
    "end": "2190550"
  },
  {
    "text": "PROFESSOR: It might help to\nbe concrete here, Gaurav. So on a computer,\nthere's a function",
    "start": "2190550",
    "end": "2195860"
  },
  {
    "text": "in Julia called rand that\nwill return a number between 0 and 1 uniformly distributed.",
    "start": "2195860",
    "end": "2202260"
  },
  {
    "text": "So now suppose you're\ngiven that as input, and then you want to spit out\nan exponentially distributed",
    "start": "2202260",
    "end": "2209300"
  },
  {
    "text": "thing as output. So you only need that\nsource of randomness, which who knows where\nthat comes from?",
    "start": "2209300",
    "end": "2215510"
  },
  {
    "text": "And then somehow, it\npasses through some map to produce some\nother distribution,",
    "start": "2215510",
    "end": "2222410"
  },
  {
    "text": "other type of random number. GAURAV ARYA: Right. Yeah, yeah. So actually, yes,\nI'll try and show",
    "start": "2222410",
    "end": "2228410"
  },
  {
    "text": "that for an exponential\ndistribution. Are there any other\nquestions first? ",
    "start": "2228410",
    "end": "2236560"
  },
  {
    "text": "PROFESSOR: OK, no other\nquestions right now. GAURAV ARYA: Yeah.",
    "start": "2236560",
    "end": "2241882"
  },
  {
    "text": "Yeah, so I'll try\nand show that for an exponential distribution. So again, x of p distributed\nas an exponential.",
    "start": "2241882",
    "end": "2248110"
  },
  {
    "text": "And then for simplicity,\njust for consistency, this isn't actually\nthe simplest choice",
    "start": "2248110",
    "end": "2254050"
  },
  {
    "text": "for doing this for an\nexponential random variable, but I'm always going to\nmake omega between 0 and 1.",
    "start": "2254050",
    "end": "2261519"
  },
  {
    "text": "And p is the uniform\ndistribution over 0 and 1.",
    "start": "2261520",
    "end": "2268970"
  },
  {
    "text": "PROFESSOR: You have a coin flip. GAURAV ARYA: Yeah. Exactly. And sort of going back\nto this map thing,",
    "start": "2268970",
    "end": "2275210"
  },
  {
    "text": "maybe I should have\nsaid also explicitly how do you sample from x of p? And the answer is sort of\nlike a two-step process.",
    "start": "2275210",
    "end": "2283080"
  },
  {
    "text": "First you pick a random\nomega, according to p, and then you plug\nit into this map.",
    "start": "2283080",
    "end": "2288799"
  },
  {
    "text": "That's how you\nsample from x of p. So here this is like\nyour call to rand. And this is going to give you--",
    "start": "2288800",
    "end": "2295010"
  },
  {
    "text": "if you think of\nsampling from p, that's going to give you an omega,\nwhich is between 0 and 1.",
    "start": "2295010",
    "end": "2301880"
  },
  {
    "text": "And then it turns\nout that the function for an exponential\ndistribution, think",
    "start": "2301880",
    "end": "2307520"
  },
  {
    "text": "it's log, maybe negative log. It should be negative log. The negative log of a uniform.",
    "start": "2307520",
    "end": "2316100"
  },
  {
    "text": "And I could also just\nwrite omega here. I'm writing 1 minus omega. But this is x of p of omega.",
    "start": "2316100",
    "end": "2327980"
  },
  {
    "text": "So maybe it'll become more clear\nif I try and show you in code.",
    "start": "2327980",
    "end": "2334290"
  },
  {
    "text": "PROFESSOR: Questions? AUDIENCE: Shouldn't that\ndepend on p in a way? PROFESSOR: The question was,\nshouldn't that depend on p?",
    "start": "2334290",
    "end": "2341849"
  },
  {
    "text": "Oh, yeah. Oh, your p scaling, yes. Yeah, you didn't\nput your scale in.",
    "start": "2341850",
    "end": "2346988"
  },
  {
    "text": "GAURAV ARYA: Yeah, sorry. That should have\nbeen multiplied by p. Thanks. Thanks for the question, yeah. PROFESSOR: Yeah, yeah.",
    "start": "2346988",
    "end": "2352930"
  },
  {
    "text": "GAURAV ARYA: Yeah, so we add-- PROFESSOR: So there's\na p outside the log. So that's [INAUDIBLE]. GAURAV ARYA: Yeah, so\nif you go back to here,",
    "start": "2352930",
    "end": "2358770"
  },
  {
    "text": "we had a way of sampling from\nan exponential distribution.",
    "start": "2358770",
    "end": "2364410"
  },
  {
    "text": "Now let's write our\nown sampler, where we use this trick, where\nour fundamental primitive is",
    "start": "2364410",
    "end": "2373530"
  },
  {
    "text": "going to be rand. So maybe I'll write\nthis in multiple lines. So first, we sample\nour omega, which",
    "start": "2373530",
    "end": "2380920"
  },
  {
    "text": "is a uniform random number. PROFESSOR: Yeah, so rand\nis uniform between 0 and 1, basically.",
    "start": "2380920",
    "end": "2386410"
  },
  {
    "text": "GAURAV ARYA: Yes, yes. And then we return\nlog of 1 minus omega.",
    "start": "2386410",
    "end": "2395340"
  },
  {
    "text": "PROFESSOR: Times p. GAURAV ARYA: Times p, yes. So yeah, so now if\nI try this with 100,",
    "start": "2395340",
    "end": "2404580"
  },
  {
    "text": "it does seem to look\nsomething centered around 100. We can check its mean. ",
    "start": "2404580",
    "end": "2410958"
  },
  {
    "text": "So yeah, so, in\nfact, under the hood, this is probably what\nyour computer is doing. There's a really optimized\nway of generating uniforms.",
    "start": "2410958",
    "end": "2416933"
  },
  {
    "text": "Actually I'm not 100% sure\nthat's how it's doing it, but this is one possible way. And why is this relevant to\nour differentiation problem?",
    "start": "2416933",
    "end": "2428820"
  },
  {
    "text": "It's relevant because if we\ngo back to this phrasing,",
    "start": "2428820",
    "end": "2435430"
  },
  {
    "text": "the key part is that this call\nto rand is independent of p.",
    "start": "2435430",
    "end": "2444390"
  },
  {
    "text": "So what we can do\nis we can not make all of these x of\np's independent,",
    "start": "2444390",
    "end": "2450900"
  },
  {
    "text": "but rather make them all rely on\nexactly the same call to rand,",
    "start": "2450900",
    "end": "2456000"
  },
  {
    "text": "which we just plug into\ntheir respective maps. ",
    "start": "2456000",
    "end": "2461710"
  },
  {
    "text": "Does that make sense?  Since I'm a bit short\non time, maybe I'll",
    "start": "2461710",
    "end": "2467770"
  },
  {
    "text": "switch to some slides.  PROFESSOR: There's a famous\nquote in computer science",
    "start": "2467770",
    "end": "2475090"
  },
  {
    "text": "that random numbers are far too\nimportant to be left to chance. ",
    "start": "2475090",
    "end": "2481120"
  },
  {
    "text": "So it's quite difficult to\nget a source of randomness on a computer. So there's a lot going\non in the rand function.",
    "start": "2481120",
    "end": "2488090"
  },
  {
    "text": "So once you have that, you\ndon't want to touch it. And then you kind of reuse\nthat for everything else.",
    "start": "2488090",
    "end": "2493435"
  },
  {
    "start": "2493435",
    "end": "2498580"
  },
  {
    "text": "GAURAV ARYA: Yeah, so this\nis how it looks pictorially. So we're sampling this\nrandom number from omega,",
    "start": "2498580",
    "end": "2505900"
  },
  {
    "text": "and you're plugging\nit in to x of p. And this is your map.",
    "start": "2505900",
    "end": "2514753"
  },
  {
    "text": "And then you're sort of\nlooking it up on the y-axis.  OK, so that's how\nx of p looks like.",
    "start": "2514753",
    "end": "2521200"
  },
  {
    "text": "We sample a uniform from\n0 to 1, we plug it in. And at a point is that\nx of p plus epsilon, for instance, is jointly\ndistributed with x and p.",
    "start": "2521200",
    "end": "2529270"
  },
  {
    "text": "So if we were to pick this\nomega 1 on the real line, then our sample from\nx of p would be this.",
    "start": "2529270",
    "end": "2536083"
  },
  {
    "text": "I hope you can see my mouse. And our sample from x of p\nplus epsilon would be this.",
    "start": "2536083",
    "end": "2543580"
  },
  {
    "text": "And then their difference\nwould be our sample from the x of epsilon,\nwhich, as you can see, is sort of getting smaller\nas epsilon gets smaller.",
    "start": "2543580",
    "end": "2551492"
  },
  {
    "text": "So this whole\nprocedure, which I'll describe in a bit\nmore detail, they're called the\nreparameterization trick, and it's a very old trick\nand used nearly everywhere.",
    "start": "2551492",
    "end": "2559460"
  },
  {
    "text": "For example, in those\nvariational autoencoders. So before the\nreparameterization trick,",
    "start": "2559460",
    "end": "2565670"
  },
  {
    "text": "you can also imagine how our\nidea of sampling from them independently would look like.",
    "start": "2565670",
    "end": "2571980"
  },
  {
    "text": "And it looks like this. Rather than using the same\nomega for both of them, you sample two\nindependent omegas,",
    "start": "2571980",
    "end": "2577940"
  },
  {
    "text": "and you take their difference. And you can see that this can\nbe pretty huge, no matter how small I make epsilon.",
    "start": "2577940",
    "end": "2583350"
  },
  {
    "text": "So it's sort of like\nyou're trying to figure out how this curve is shifting, how\nfast the area under this curve",
    "start": "2583350",
    "end": "2589460"
  },
  {
    "text": "is changing. And the way you decided to\ndo it is like a random sample from each of these curves.",
    "start": "2589460",
    "end": "2594650"
  },
  {
    "text": "But what's going to\nhappen is the noise of the intrinsic randomness\nis going to totally wash out",
    "start": "2594650",
    "end": "2599750"
  },
  {
    "text": "the signal of how much\nmy curve is going up. And the solution to that\nis use the same value",
    "start": "2599750",
    "end": "2607200"
  },
  {
    "text": "in the y-axis, same\nomega, for both curves and directly sample\ntheir difference. That's how you can estimate\nhow fast the curve is moving.",
    "start": "2607200",
    "end": "2616160"
  },
  {
    "text": "So that's what\nwe're going to do. So this is your dX of epsilon,\nwhich sort of subtracts",
    "start": "2616160",
    "end": "2622490"
  },
  {
    "text": "these at every fixed omega. And now the nice property is\nthat at every fixed omega,",
    "start": "2622490",
    "end": "2630079"
  },
  {
    "text": "dX of epsilon is kind\nof like order epsilon. It's of magnitude epsilon.",
    "start": "2630080",
    "end": "2635549"
  },
  {
    "text": "It's not O of 1. I think you've used this\nbig O notation in the class.",
    "start": "2635550",
    "end": "2640840"
  },
  {
    "text": "PROFESSOR: Yeah, we used\nlittle O notation, but yeah. GAURAV ARYA: OK, OK. PROFESSOR: This\nis similar, yeah.",
    "start": "2640840",
    "end": "2645990"
  },
  {
    "text": "GAURAV ARYA: Yeah. Yeah. PROFESSOR: So it's proportional\nto epsilon or smaller. ",
    "start": "2645990",
    "end": "2653090"
  },
  {
    "text": "GAURAV ARYA: Yeah, so\nthis is your differential. So we've solved\nproblem one, which was how do we describe\na change in our program?",
    "start": "2653090",
    "end": "2661099"
  },
  {
    "text": "We've decided that it's going\nto be itself a random variable. And it's basically\nthe subtraction",
    "start": "2661100",
    "end": "2668360"
  },
  {
    "text": "of these two random variables\nwith a particular joint distribution. Namely they're all sharing\nthe same source of randomness.",
    "start": "2668360",
    "end": "2675080"
  },
  {
    "text": "Yeah. And the final piece\nof the puzzle-- that's a bit scary--",
    "start": "2675080",
    "end": "2680090"
  },
  {
    "text": "is now that we\nhave differentials, we can just take the derivative\nkind of like point-wise",
    "start": "2680090",
    "end": "2686990"
  },
  {
    "text": "at every point on every omega.",
    "start": "2686990",
    "end": "2692090"
  },
  {
    "text": "So we have this differential. And this differential,\nif you fix omega 1, it's moving up in a\nnice well-behaved way.",
    "start": "2692090",
    "end": "2699740"
  },
  {
    "text": "This change is small. So we can actually\ncompute its derivative. And we can compute its\nderivative at every omega.",
    "start": "2699740",
    "end": "2707850"
  },
  {
    "text": "And then we'll have a\nnew random variable. So remember a random variable\non a probability space",
    "start": "2707850",
    "end": "2713279"
  },
  {
    "text": "is just a function from\nomega to some output. So now our function\nfrom omega to our output",
    "start": "2713280",
    "end": "2718680"
  },
  {
    "text": "will be this derivative\nat every particular omega.",
    "start": "2718680",
    "end": "2724920"
  },
  {
    "text": "So that's represented\nby this thing here, the limit as epsilon goes to 0\nof dX of epsilon over epsilon.",
    "start": "2724920",
    "end": "2731474"
  },
  {
    "text": " And I'm calling this delta. And it turns out you\ncan maybe-- it maybe",
    "start": "2731475",
    "end": "2738660"
  },
  {
    "text": "it's pretty clear\nintuitively staring at this, that if you're sampling the\nderivatives at every fixed",
    "start": "2738660",
    "end": "2744060"
  },
  {
    "text": "omega, what you're going\nto get, on average, is a derivative of the\naverage of the whole program. ",
    "start": "2744060",
    "end": "2751515"
  },
  {
    "text": "I said that our\nderivative might not be exactly the same thing\nas the gradient estimator, but it sure looks like,\nin this case, it is.",
    "start": "2751515",
    "end": "2757890"
  },
  {
    "text": "Like our derivative, which tells\nyou every point in the sample space how much is the\nrandom function changing,",
    "start": "2757890",
    "end": "2765630"
  },
  {
    "text": "is just, if you\naverage it out, it is giving us what we decided\nwe wanted at the beginning.",
    "start": "2765630",
    "end": "2771670"
  },
  {
    "text": "So that was quite a lot. Are there any questions? PROFESSOR: Any\nquestions at this point?",
    "start": "2771670",
    "end": "2777730"
  },
  {
    "text": " So everyone's silent so far.",
    "start": "2777730",
    "end": "2785640"
  },
  {
    "text": "Oh, one question. AUDIENCE: What does it mean\nfor the derivative to be 0? PROFESSOR: So what does\nit mean for the derivative",
    "start": "2785640",
    "end": "2792640"
  },
  {
    "text": "to be 0 for a program like this?",
    "start": "2792640",
    "end": "2797789"
  },
  {
    "text": "GAURAV ARYA: For the\nderivative to be 0. So there's two senses. So yeah, the derivative\nto be 0 everywhere.",
    "start": "2797790",
    "end": "2803650"
  },
  {
    "text": "So delta is a random variable. So maybe I should also\njust state for you",
    "start": "2803650",
    "end": "2809950"
  },
  {
    "text": "what delta is for an\nexponential random variable. So let me show you that.",
    "start": "2809950",
    "end": "2818109"
  },
  {
    "text": " So where did I have my--",
    "start": "2818110",
    "end": "2824200"
  },
  {
    "text": " I lost it. It's over here.",
    "start": "2824200",
    "end": "2829480"
  },
  {
    "text": "OK. So this was our function. There was a little\np added in there.",
    "start": "2829480",
    "end": "2835359"
  },
  {
    "text": "Negative p times log\nof 1 minus omega. So we had the dX--",
    "start": "2835360",
    "end": "2841225"
  },
  {
    "text": "I'll need a new page. So dX of epsilon. This is a random\nvariable, which,",
    "start": "2841225",
    "end": "2847480"
  },
  {
    "text": "given a probability space, is\njust a map, taking an omega. And it's d by dp of\nnegative p log of 1",
    "start": "2847480",
    "end": "2858280"
  },
  {
    "text": "minus omega, which is equal to\nnegative log of 1 minus omega.",
    "start": "2858280",
    "end": "2864980"
  },
  {
    "text": "Sorry. Sorry. This is delta of\nomega, I would say,",
    "start": "2864980",
    "end": "2870050"
  },
  {
    "text": "instead of taking\nthe derivative. So delta of omega is negative\nlog of 1 minus omega.",
    "start": "2870050",
    "end": "2877400"
  },
  {
    "text": "So this is your random variable. So in this case, it certainly\nlooks like it's not always 0. Although interestingly,\nit's always constant",
    "start": "2877400",
    "end": "2883670"
  },
  {
    "text": "because this p was\nkind of just the scale. Now your question is, what\nif we got a case where",
    "start": "2883670",
    "end": "2889430"
  },
  {
    "text": "delta everywhere in omega is 0? Well, that would just mean\nthat your random variable",
    "start": "2889430",
    "end": "2896390"
  },
  {
    "text": "isn't changing at all. So except in very\ndegenerate cases,",
    "start": "2896390",
    "end": "2901460"
  },
  {
    "text": "I wouldn't expect\nthat to happen, where the whole distribution\nof your random variable",
    "start": "2901460",
    "end": "2906680"
  },
  {
    "text": "isn't changing. Perhaps slightly\nmore common would be a case where the derivative\nof the loss, the derivative",
    "start": "2906680",
    "end": "2914480"
  },
  {
    "text": "of the expectation, is 0. Maybe it's slightly more common. And that would happen when\nthe expectation of delta is 0.",
    "start": "2914480",
    "end": "2921910"
  },
  {
    "text": "Yeah. Does that make sense? PROFESSOR: Another question. AUDIENCE: Yeah. I was just curious if this\nhad any connection to what it",
    "start": "2921910",
    "end": "2927910"
  },
  {
    "text": "means to be a\nstationary distribution for the exponential. PROFESSOR: So the\nquestion was, does this have any connection\nwith what it means to be",
    "start": "2927910",
    "end": "2934452"
  },
  {
    "text": "a stationary distribution? GAURAV ARYA: To be a\nstationary distribution. ",
    "start": "2934452",
    "end": "2943220"
  },
  {
    "text": "Not that I can think of\nat the top of my head. Yeah.",
    "start": "2943220",
    "end": "2949460"
  },
  {
    "text": "I'm not totally sure. I mean, I guess\nthere's some notion of derivative being 0 there.",
    "start": "2949460",
    "end": "2956120"
  },
  {
    "text": "I guess if you let your\np be sort of like timed,",
    "start": "2956120",
    "end": "2962730"
  },
  {
    "text": "like sort of like a\nstochastic process in time,",
    "start": "2962730",
    "end": "2969320"
  },
  {
    "text": "and you're differentiating\nwith respect to t, then I would expect even\nthen, even if you're--",
    "start": "2969320",
    "end": "2980080"
  },
  {
    "text": "I mean, yes, so it could\nbe the case that you get-- you would definitely get this\nif you're at a stationary--",
    "start": "2980080",
    "end": "2987490"
  },
  {
    "text": "if your t is\napproaching infinity, so instead you're\nshort of reaching a stationary distribution,\nand you're doing x of t",
    "start": "2987490",
    "end": "2992950"
  },
  {
    "text": "and x of t plus delta t, then\nany statistical quantities are going to go to 0.",
    "start": "2992950",
    "end": "2999369"
  },
  {
    "text": "It's possible that\nthis is still non-zero because the\nparameterization depends. I really have to\nthink about it more.",
    "start": "2999370",
    "end": "3004770"
  },
  {
    "text": "But yeah, maybe\nthere's some connection if you think of t as a\ntime or number of steps",
    "start": "3004770",
    "end": "3011250"
  },
  {
    "text": "in some continuous sense. Yeah. PROFESSOR: Another question? AUDIENCE: Shouldn't\nthat be minus 1 over p?",
    "start": "3011250",
    "end": "3016800"
  },
  {
    "text": "PROFESSOR: Shouldn't\nthat be minus 1 over p? Where? AUDIENCE: No, like--",
    "start": "3016800",
    "end": "3022200"
  },
  {
    "text": "GAURAV ARYA: Oh.  I don't think so. I mean, actually I think there\nare two different conventions.",
    "start": "3022200",
    "end": "3029347"
  },
  {
    "text": "That's why it's very\nimportant that we clarified at the beginning\nthat t was a scale. Often the exponential\ndistribution",
    "start": "3029347",
    "end": "3034500"
  },
  {
    "text": "is parametrized\nby the rate, which is the inverse of the scale. But here this makes\nsense to me because it's",
    "start": "3034500",
    "end": "3040710"
  },
  {
    "text": "getting larger with\nlarger p, which would make sense [INAUDIBLE]. PROFESSOR: Yes, p is\nnot a probability. AUDIENCE: [INAUDIBLE] 1\nminus p to the power minus px",
    "start": "3040710",
    "end": "3047849"
  },
  {
    "text": "or minus x by p? PROFESSOR: Yes. So the point is p\nis the scale of--",
    "start": "3047850",
    "end": "3054300"
  },
  {
    "text": "so the average is p of x. All right.",
    "start": "3054300",
    "end": "3060940"
  },
  {
    "text": "GAURAV ARYA: Yeah, so\nmaybe I can write down our gradient estimator program.",
    "start": "3060940",
    "end": "3067840"
  },
  {
    "text": "So this one looks like delta. And actually, it's\ntechnically a random variable,",
    "start": "3067840",
    "end": "3073670"
  },
  {
    "text": "but in this case-- oh, sorry, it is\na random variable. Sorry, I misspoke when I said it\nwas constant earlier because it",
    "start": "3073670",
    "end": "3079510"
  },
  {
    "text": "is a function of omega. So this is our random variable\nas a function of omega.",
    "start": "3079510",
    "end": "3087070"
  },
  {
    "text": "And then if you want\nto sample delta, we can do negative\nlog of 1 minus rand.",
    "start": "3087070",
    "end": "3095740"
  },
  {
    "text": "So theta and rand. And now if we sample\nthis a bunch of times,",
    "start": "3095740",
    "end": "3105670"
  },
  {
    "text": "I'm getting\nsomething close to 1. And it actually turns\nout-- so if we go to our--",
    "start": "3105670",
    "end": "3111940"
  },
  {
    "text": "so let's just remind\nourselves what x was. x was this.",
    "start": "3111940",
    "end": "3117940"
  },
  {
    "text": "If I do this, this\nis actually p.",
    "start": "3117940",
    "end": "3126630"
  },
  {
    "text": "So the derivative of p-- the derivative of expectation,\nwhich is p with respect to p, is just going to be 1.",
    "start": "3126630",
    "end": "3132780"
  },
  {
    "text": "This makes sense. But then the crucial\npart is if we're interested in automatic\ndifferentiation,",
    "start": "3132780",
    "end": "3138269"
  },
  {
    "text": "this notion of derivative-- we could have just said our\nnotion of derivative is 1.",
    "start": "3138270",
    "end": "3143520"
  },
  {
    "text": "That's the expectation\nof x by the p. It's 1. But the issue with that\nnotion of derivative is it doesn't compose.",
    "start": "3143520",
    "end": "3149819"
  },
  {
    "text": "So consider this new function. And remember it's\nsuper easy to just make the distribution really\ncomplicated by just doing a little bit of extra stuff.",
    "start": "3149820",
    "end": "3155920"
  },
  {
    "text": "So here I'm sampling from\nan exponential distribution. I'm squaring. And it has some mean. And I might be interested\nin its derivative.",
    "start": "3155920",
    "end": "3163710"
  },
  {
    "text": "And what you can\ndo, I guess you've learned about dual numbers. So this is going\nto be a bit fast.",
    "start": "3163710",
    "end": "3170980"
  },
  {
    "text": " Or maybe I'll just handwrite it.",
    "start": "3170980",
    "end": "3176250"
  },
  {
    "text": "So if you use the\nchain rule, it should be 2 times the sample from\nx times a sample from delta.",
    "start": "3176250",
    "end": "3185365"
  },
  {
    "start": "3185365",
    "end": "3191470"
  },
  {
    "text": "And you're just going to\nhave to trust me that that's the true derivative. I'm not 100% sure\nbecause I did a bit of--",
    "start": "3191470",
    "end": "3198520"
  },
  {
    "text": "I'm not 100% sure\nI got it right. But basically, the\nchain rule just works with this\nnotion of derivative,",
    "start": "3198520",
    "end": "3204620"
  },
  {
    "text": "which isn't something I've\ntotally justified to you, but that's sort of the upshot. So that's why this\nis really popular.",
    "start": "3204620",
    "end": "3211838"
  },
  {
    "text": "This is a really popular trick. You can also use it for\nreverse mode differential. PROFESSOR: Gaurav,\nshouldn't that",
    "start": "3211838",
    "end": "3218470"
  },
  {
    "text": "have been the same omega in the\nx, 100, and the sample delta?",
    "start": "3218470",
    "end": "3224290"
  },
  {
    "text": "Right now, you're using\nindependent random numbers in x, 100, and sample delta\nin that 2 times x times",
    "start": "3224290",
    "end": "3231040"
  },
  {
    "text": "sample times delta. GAURAV ARYA: Yeah, you're\nright, so this is probably not the right answer. So I would have to do--",
    "start": "3231040",
    "end": "3236230"
  },
  {
    "text": "PROFESSOR: So they\nshould have really been the same random number. GAURAV ARYA: --delta of rand. And then write my x as like\nnegative 100 times log of--",
    "start": "3236230",
    "end": "3244307"
  },
  {
    "text": "PROFESSOR: No, it\nhas to be the same-- GAURAV ARYA: --1 minus rand. PROFESSOR: No, no. No, you're still using\nindependent random numbers.",
    "start": "3244307",
    "end": "3250280"
  },
  {
    "text": "GAURAV ARYA: Oh. OK. Well, OK, maybe I don't have\ntime to get it right here. I guess it would be\nan omega and a rand.",
    "start": "3250280",
    "end": "3256819"
  },
  {
    "text": "But the point is that\nthis notion of derivative obeys the chain rule,\nwhich is really nice.",
    "start": "3256820",
    "end": "3263932"
  },
  {
    "text": "PROFESSOR: Yeah,\nbecause basically you're differentiating that it's\njust the ordinary derivative",
    "start": "3263932",
    "end": "3269210"
  },
  {
    "text": "of that completely\ndeterministic function that takes in rand, omega, and\noutputs something else.",
    "start": "3269210",
    "end": "3277192"
  },
  {
    "text": "Once you have the\ndeterministic function, it's the ordinary derivative,\nordinary chain rule, ordinary everything.",
    "start": "3277192",
    "end": "3282910"
  },
  {
    "text": "GAURAV ARYA: Yeah, yeah. And I think if you search\nup reparameterization trick, because it's gotten very popular\nin machine learning recently,",
    "start": "3282910",
    "end": "3288950"
  },
  {
    "text": "there's a lot of resources\nyou can find online. Some of them may explain it in\na more barebones, simpler way",
    "start": "3288950",
    "end": "3295100"
  },
  {
    "text": "than what I have done because\npart of the reason I've done what I've done is also\nto think about how we might",
    "start": "3295100",
    "end": "3304610"
  },
  {
    "text": "generalize to the discrete\ncase, which I'll try and just maybe just do a\nfive-minute overview of,",
    "start": "3304610",
    "end": "3309787"
  },
  {
    "text": "if that sounds fine? PROFESSOR: Sure. Yeah, that's fine. ",
    "start": "3309787",
    "end": "3315880"
  },
  {
    "text": "GAURAV ARYA: OK, so I'll\njust try and briefly explain. So basically, my project has\nbeen about taking that idea",
    "start": "3315880",
    "end": "3323140"
  },
  {
    "text": "and now handling discrete\nrandom variables. The other example I\ngave at the beginning was a Bernoulli random variable.",
    "start": "3323140",
    "end": "3330190"
  },
  {
    "text": "And now we want to\napply the same approach. Come up with a\nnotion of derivative",
    "start": "3330190",
    "end": "3335440"
  },
  {
    "text": "that sort of explains the\nsensitivity of our program with respect to p.",
    "start": "3335440",
    "end": "3340570"
  },
  {
    "text": "And why is this more difficult\nfor a discrete random variable? So does this step\nfunction here make sense?",
    "start": "3340570",
    "end": "3347780"
  },
  {
    "text": "Basically I'm using the\nsame probability space. I'm choosing a uniform\nomega from 0 to 1.",
    "start": "3347780",
    "end": "3352960"
  },
  {
    "text": "And now this point\nhere is 1 minus p. So if I land on the\nright, if I land",
    "start": "3352960",
    "end": "3358059"
  },
  {
    "text": "in this region of probability\np, I'm going to get a 1. Otherwise, I'm going to get a 0. Does that make sense?",
    "start": "3358060",
    "end": "3363355"
  },
  {
    "text": " PROFESSOR: Yep, people\nare nodding yes. GAURAV ARYA: All right.",
    "start": "3363355",
    "end": "3369030"
  },
  {
    "text": "So now if we perturb p,\nsomething qualitatively very different happens,\nwhich is that where",
    "start": "3369030",
    "end": "3376410"
  },
  {
    "text": "the step occurs changes. So if you think about it, nearly\neverywhere in our sample space,",
    "start": "3376410",
    "end": "3384660"
  },
  {
    "text": "nothing is changing. So as I make epsilon smaller\nand smaller and smaller,",
    "start": "3384660",
    "end": "3390210"
  },
  {
    "text": "that object delta now that\nI defined before is just going to be 0 everywhere because\nfor small enough epsilon,",
    "start": "3390210",
    "end": "3396780"
  },
  {
    "text": "nothing interesting\nis happening. But does this mean that the\nderivative of your expectation",
    "start": "3396780",
    "end": "3402510"
  },
  {
    "text": "is 0? No. In fact, for Bernoulli,\nthe derivative is 1.",
    "start": "3402510",
    "end": "3408390"
  },
  {
    "text": "So what are we missing? We're missing the fact\nthat the difference, if we look at the differential\nbetween these two,",
    "start": "3408390",
    "end": "3416970"
  },
  {
    "text": "it's nearly everywhere 0. But in the place where\nit's non-zero, it's huge.",
    "start": "3416970",
    "end": "3422490"
  },
  {
    "text": "It's a flip of the whole-- it's like a flip of the coin. This 1 isn't getting smaller\nand smaller in epsilon.",
    "start": "3422490",
    "end": "3429370"
  },
  {
    "text": "So for people who\nlike analysis, there was this interchange of\nlimit and expectation",
    "start": "3429370",
    "end": "3436349"
  },
  {
    "text": "that occurred here\nwhen we were trying to justify that this\nreparameterization trick works.",
    "start": "3436350",
    "end": "3442079"
  },
  {
    "text": "And that is no longer\nvalid, in this case, because if you think about\ndividing this by epsilon,",
    "start": "3442080",
    "end": "3447990"
  },
  {
    "text": "this magnitude of this thing\nis really, really large. It's 1 over epsilon.",
    "start": "3447990",
    "end": "3453340"
  },
  {
    "text": "So that's the\nessential challenge of discrete randomness. And if you think\nabout it, so remember",
    "start": "3453340",
    "end": "3460980"
  },
  {
    "text": "the derivative is the\nbest linear approximation of the sensitivity\nof your program.",
    "start": "3460980",
    "end": "3467480"
  },
  {
    "text": "It's the best\nlinear approximation of how your program\ngets perturbed. And here what's linear\nis how fast this step",
    "start": "3467480",
    "end": "3477500"
  },
  {
    "text": "is moving to the left. So it kind of feels like\nwe want to differentiate not the value, the\nchange in the value,",
    "start": "3477500",
    "end": "3486310"
  },
  {
    "text": "but rather we want to\ndifferentiate the probability that they're different. So that's sort of\nthe idea of how",
    "start": "3486310",
    "end": "3493300"
  },
  {
    "text": "to extend the usual\nreparameterization trick, which just has delta, into an\nobject which can also capture",
    "start": "3493300",
    "end": "3501580"
  },
  {
    "text": "these flips with tiny\nprobability, which you see in the discrete case.",
    "start": "3501580",
    "end": "3506600"
  },
  {
    "text": "So I know that's very fast. And it's just really\njust the essential idea.",
    "start": "3506600",
    "end": "3513039"
  },
  {
    "text": "Do people have questions? PROFESSOR: And so Gaurav\nactually just published a paper on exactly this.",
    "start": "3513040",
    "end": "3518980"
  },
  {
    "text": "Basically you write a\nprogram that has rand calls and maybe has an, if\nrand is greater than 0.5,",
    "start": "3518980",
    "end": "3524619"
  },
  {
    "text": "return cosine. Otherwise, return sine\nor something like that. And it can differentiate\nthat in this sense,",
    "start": "3524620",
    "end": "3531970"
  },
  {
    "text": "give you something whose\nexpectation value is the derivative of the\nexpectation value. But you have to track these\ndiscontinuous things that",
    "start": "3531970",
    "end": "3541220"
  },
  {
    "text": "don't have an 18.01 derivative. You have to track them\nseparately, in some sense.",
    "start": "3541220",
    "end": "3546468"
  },
  {
    "text": "GAURAV ARYA: Right. Yeah, so here's a quick demo. So here I have a Bernoulli,\nwhich its mean is around 0.5.",
    "start": "3546468",
    "end": "3553880"
  },
  {
    "text": "So we'd expect its\naverage value to be-- so we'd expect its\nderivative to be 1",
    "start": "3553880",
    "end": "3559220"
  },
  {
    "text": "because the expectation is p. So again, I'm always doing\nthis boring case where we just have a single distribution.",
    "start": "3559220",
    "end": "3564380"
  },
  {
    "text": "Maybe I'll try something more\ncomplicated right after this. But basically, the idea of our\napproach, using dual numbers--",
    "start": "3564380",
    "end": "3571295"
  },
  {
    "text": " this looks like a\ndual member, right?",
    "start": "3571295",
    "end": "3576620"
  },
  {
    "text": "There's this epsilon\nthing, which is like this infinitesimal change. So it's like you got a 1,\nand your derivative is 0.",
    "start": "3576620",
    "end": "3583708"
  },
  {
    "text": "This is sort of what\nthe reparameterization trick would always say, that the\nderivative contribution is 0. But sometimes,\nwhat these triples",
    "start": "3583708",
    "end": "3591290"
  },
  {
    "text": "tell you is that you could\nflip with probability that's like something times epsilon.",
    "start": "3591290",
    "end": "3596820"
  },
  {
    "text": "So it's a probability\nthat's differentiable.  And then what this\nwould be telling you,",
    "start": "3596820",
    "end": "3602790"
  },
  {
    "text": "if you collapse this into\na derivative estimator, so now the derivative object,\nwhich is all of these numbers,",
    "start": "3602790",
    "end": "3611789"
  },
  {
    "text": "is no longer the same thing\nas your gradient estimator. But it can be collapsed\ninto your gradient estimator",
    "start": "3611790",
    "end": "3617190"
  },
  {
    "text": "by doing this value plus this\nvalue multiplied by that value.",
    "start": "3617190",
    "end": "3622390"
  },
  {
    "text": "So if we sort of maybe--",
    "start": "3622390",
    "end": "3627829"
  },
  {
    "text": "if we let this be our triple,\nyou can sort of collapse it into a gradient estimator.",
    "start": "3627830",
    "end": "3632960"
  },
  {
    "text": "And it's going to be 2. But it's like 2 50% of the\ntime and 0 50% of the time.",
    "start": "3632960",
    "end": "3638644"
  },
  {
    "text": "So on average, that's what? Which is cool, but it's\nnot very impressive for a single Bernoulli. So maybe I'll try a random walk.",
    "start": "3638645",
    "end": "3646610"
  },
  {
    "text": "So it has to depend\non some parameter p. So I'm going to do a random\nwalk where I start maybe at 0.",
    "start": "3646610",
    "end": "3654320"
  },
  {
    "text": "I equals 0. Maybe n equals 0. And I take 100 steps.",
    "start": "3654320",
    "end": "3660406"
  },
  {
    "text": "Maybe I'll zoom on it more. And each step, I'm going\nto either stay put or go",
    "start": "3660406",
    "end": "3666050"
  },
  {
    "text": "to the right. So I'm going to do n plus\nequal to rand of Bernoulli.",
    "start": "3666050",
    "end": "3671510"
  },
  {
    "text": "And then I'm going to make\nmy transition function depend on p. For example, in this\ncase, maybe I'll",
    "start": "3671510",
    "end": "3677180"
  },
  {
    "text": "do something like p over 100.",
    "start": "3677180",
    "end": "3686869"
  },
  {
    "text": "Yeah, p times i over 100.",
    "start": "3686870",
    "end": "3691912"
  },
  {
    "text": "So this is something that\ndepends on where-- or maybe that should be [INAUDIBLE]. This is something that\ndepends on where you currently",
    "start": "3691912",
    "end": "3699190"
  },
  {
    "text": "are in your walk and\nalso depends on p. And if this Bernoulli\nturns out to be true, you're going to\nwalk to the right.",
    "start": "3699190",
    "end": "3705520"
  },
  {
    "text": "Otherwise, you'll stay put. And then we're\ngoing to return n. ",
    "start": "3705520",
    "end": "3711930"
  },
  {
    "text": "So now we can just check\nthat this function runs. It might not. ",
    "start": "3711930",
    "end": "3718680"
  },
  {
    "text": "Or it might do\nsomething strange.  Oh. Yeah.",
    "start": "3718680",
    "end": "3724940"
  },
  {
    "text": "Oh, this probability\nbecomes tidy. ",
    "start": "3724940",
    "end": "3731260"
  },
  {
    "text": "Let's make p a bit larger. So this is like p of 50. PROFESSOR: It's always 0\nbecause n starts at a 0.",
    "start": "3731260",
    "end": "3739033"
  },
  {
    "text": "GAURAV ARYA: Oh. OK, thank you. Maybe I'll do 100 minus-- so let's do 1 minus all of that.",
    "start": "3739033",
    "end": "3745580"
  },
  {
    "text": "Does that sound reasonable? PROFESSOR: Yeah. ",
    "start": "3745580",
    "end": "3750590"
  },
  {
    "text": "GAURAV ARYA: No. AUDIENCE: [INAUDIBLE] GAURAV ARYA: OK. ",
    "start": "3750590",
    "end": "3759339"
  },
  {
    "text": "So let's have a\nfunction which goes from 1 to 0, dependent\non where you are.",
    "start": "3759340",
    "end": "3769299"
  },
  {
    "text": "So it's going to\nstart at 1, go to 0. And let's multiply it by p.",
    "start": "3769300",
    "end": "3774520"
  },
  {
    "text": "And then let's make\np something like 0.5. OK, something\ninteresting is occurring. So we just sample from this.",
    "start": "3774520",
    "end": "3781160"
  },
  {
    "text": "Get something random. PROFESSOR: Yeah, so other people\nwere suggesting probability depending on i. But he's doing even\nmore complicated.",
    "start": "3781160",
    "end": "3786742"
  },
  {
    "text": "The probability depends on\nwhere you currently are. Right. GAURAV ARYA: OK. Great. All right. So I'll do that too.",
    "start": "3786742",
    "end": "3792720"
  },
  {
    "text": " So how should I do this? Maybe I'll do 1 minus\nn plus i over 200.",
    "start": "3792720",
    "end": "3801369"
  },
  {
    "text": "This should be a\nvalid probability. 0.5. OK.",
    "start": "3801370",
    "end": "3806440"
  },
  {
    "text": "So it has a mean. PROFESSOR: OK, yeah.",
    "start": "3806440",
    "end": "3811880"
  },
  {
    "text": "GAURAV ARYA: OK. If you just try and\ndifferentiate it the naive way by doing\nsomething like this",
    "start": "3811880",
    "end": "3818720"
  },
  {
    "text": "and then maybe\ndividing by this-- ",
    "start": "3818720",
    "end": "3827100"
  },
  {
    "text": "have I got my brackets right? There's one missing, right? That should be there.",
    "start": "3827100",
    "end": "3832130"
  },
  {
    "text": "OK. Then you're going to\nget something that's pretty large, which\nyou'll have to trust averages did the right thing.",
    "start": "3832130",
    "end": "3837660"
  },
  {
    "text": "But now if you try and use\nthese stochastic triples, so maybe let's just do\none stochastic triple.",
    "start": "3837660",
    "end": "3845300"
  },
  {
    "text": "PROFESSOR: So again,\nthe stochastic triple is the analog of what Professor\nEdelman did a couple lectures ago with automatic\ndifferentiation, where",
    "start": "3845300",
    "end": "3851580"
  },
  {
    "text": "you had the value of the\nfunction and the derivative. We call that a dual number. Now you can run three\nnumbers, the function,",
    "start": "3851580",
    "end": "3858850"
  },
  {
    "text": "the derivative, the change\nup, down of the distribution, and also the probability\nof a big jump.",
    "start": "3858850",
    "end": "3863980"
  },
  {
    "text": "So now you need a third\nbit of information. You carry through.",
    "start": "3863980",
    "end": "3869530"
  },
  {
    "text": "GAURAV ARYA: Yeah. So yeah, so for these\nstochastic triples, I'm just showing you the\nwhole triple object first.",
    "start": "3869530",
    "end": "3875230"
  },
  {
    "text": "There's never a\ndiscontinuous part. The deltas are telling\nyou how your output is changing continuously.",
    "start": "3875230",
    "end": "3880510"
  },
  {
    "text": "But your output is\nan integer, so it can't change\ncontinuously, but it can change to an adjacent\nvalue with some probability.",
    "start": "3880510",
    "end": "3887920"
  },
  {
    "text": "And the nice thing is\neven though the range of your function is pretty\nlarge-- it can be 36. It can be 29--",
    "start": "3887920",
    "end": "3892990"
  },
  {
    "text": "we sort of couple the original\n29 to the alternate, which",
    "start": "3892990",
    "end": "3898480"
  },
  {
    "text": "is like 30, which means that\nyour estimator is going to be lower variance than average.",
    "start": "3898480",
    "end": "3904440"
  },
  {
    "text": "It's just sort of\nthe stuff I was saying with this\njoint distribution and what the\nreparameterization trick does",
    "start": "3904440",
    "end": "3909819"
  },
  {
    "text": "so well is something\nwe're trying to capture with these stochastic triples. It's still a work in progress,\nand there's a lot of issues.",
    "start": "3909820",
    "end": "3918970"
  },
  {
    "text": "But now we can also-- so now we can take\nthis stochastic triple,",
    "start": "3918970",
    "end": "3924280"
  },
  {
    "text": "we can propagate it\nthrough your program, we can collapse it into\na gradient estimator.",
    "start": "3924280",
    "end": "3929440"
  },
  {
    "text": "And then we can do\nthis 1,000 times. And you're going to have\nto trust me that this is",
    "start": "3929440",
    "end": "3935500"
  },
  {
    "text": "the derivative of your program. You could verify it by\ncomputing the transition matrix or using an alternative\ngradient estimation method,",
    "start": "3935500",
    "end": "3943120"
  },
  {
    "text": "but that's the upshot. That with this new\nnotion of derivative, we can now answer\nthis problem we wanted",
    "start": "3943120",
    "end": "3949030"
  },
  {
    "text": "to solve from the\nstart, which is for any complicated\nstochastic simulation,",
    "start": "3949030",
    "end": "3955330"
  },
  {
    "text": "can we get a new program\nwhich averages the derivative of the average of the original?",
    "start": "3955330",
    "end": "3960670"
  },
  {
    "text": "Yeah. That's all I had. Are there any questions? PROFESSOR: Any questions?",
    "start": "3960670",
    "end": "3967740"
  },
  {
    "text": "OK, so let's take a\nfive-minute break. So we'll start again at 12:14.",
    "start": "3967740",
    "end": "3972890"
  },
  {
    "start": "3972890",
    "end": "3977000"
  }
]