[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5300"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high-quality\neducational resources for free.",
    "start": "5300",
    "end": "11600"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11600",
    "end": "18100"
  },
  {
    "text": "at ocw.mit.edu.  JAMES W. SWAN: Let's go\nahead and get started.",
    "start": "18100",
    "end": "24590"
  },
  {
    "text": " I hope everybody\nsaw the correction",
    "start": "24590",
    "end": "31550"
  },
  {
    "text": "to a typo in homework 1 that\nwas posted on Stellar last night and sent out to you. That's going to happen\nfrom time to time.",
    "start": "31550",
    "end": "39140"
  },
  {
    "text": "We have four course staff\nthat review all the problems. We try to look through it for\nany issues or ambiguities.",
    "start": "39140",
    "end": "45389"
  },
  {
    "text": "But from time to time,\nwe'll miss something and try to make a correction. The TAs gave a hint\nthat would have let you",
    "start": "45389",
    "end": "50660"
  },
  {
    "text": "solve the problem as written. But that's more\ndifficult than what",
    "start": "50660",
    "end": "56440"
  },
  {
    "text": "we had intended for you guys. We don't want to give you a\nhomework assignment that's punishing.",
    "start": "56440",
    "end": "61932"
  },
  {
    "text": "We want to give you\nan assignment that'll help you learn. Some people in this class that\nare very good at programming",
    "start": "61932",
    "end": "67490"
  },
  {
    "text": "have apparently\nalready completed that problem with the hint. But it's easier, as\noriginally intended.",
    "start": "67490",
    "end": "73299"
  },
  {
    "text": "And the correction resets that. So maybe you'll\nsee the distinction between those things\nand understand",
    "start": "73300",
    "end": "78910"
  },
  {
    "text": "why one version of the problem\nis much easier than another. But we try to respond\nas quickly as possible",
    "start": "78910",
    "end": "84939"
  },
  {
    "text": "when we notice a typo like\nthat so that we can set you guys on the right course. ",
    "start": "84940",
    "end": "92110"
  },
  {
    "text": "So we've got two lectures\nleft discussing linear algebra before we move on\nto other topics.",
    "start": "92110",
    "end": "97630"
  },
  {
    "text": "We're still going to talk about\ntransformations of matrices. We looked at one type\nof transformation",
    "start": "97630",
    "end": "103270"
  },
  {
    "text": "we could utilize for solving\nsystems of equations. Today, we'll look\nat another one, the eigenvalue decomposition.",
    "start": "103270",
    "end": "109170"
  },
  {
    "text": "And on Monday, we'll look at\nanother one called the singular value decomposition. Before jumping right in,\nI want to take a minute",
    "start": "109170",
    "end": "115680"
  },
  {
    "text": "and see if there are any\nquestions that I can answer, anything that's been\nunclear so far that I",
    "start": "115680",
    "end": "122109"
  },
  {
    "text": "can try to reemphasize\nor focus on for you. ",
    "start": "122110",
    "end": "128672"
  },
  {
    "text": "I was told the office hours\nare really well-attended. So hopefully, you're\ngetting an opportunity to ask any pressing questions\nduring the office hours",
    "start": "128672",
    "end": "136677"
  },
  {
    "text": "or you're meeting\nwith the instructors after class to ask\nanything that was unclear. We want to make sure that\nwe're answering those questions",
    "start": "136677",
    "end": "144920"
  },
  {
    "text": "in a timely fashion. This course moves at\na pretty quick pace. We don't want anyone\nto get left behind.",
    "start": "144920",
    "end": "151159"
  },
  {
    "text": "Speaking of getting left behind,\nwe ran out of time a little bit at the end of\nlecture on Wednesday.",
    "start": "151160",
    "end": "156770"
  },
  {
    "text": "That's OK. There were a lot\nof good questions that came up during class. And one topic that we\ndidn't get to discuss",
    "start": "156770",
    "end": "162110"
  },
  {
    "text": "is formal systems\nfor doing reordering in systems of equations.",
    "start": "162110",
    "end": "167660"
  },
  {
    "text": "We saw that reordering\nis important. In fact, it's essential for\nsolving certain problems",
    "start": "167660",
    "end": "173599"
  },
  {
    "text": "via Gaussian elimination. You won't be able to solve them. Either you'll incur a\nlarge numerical error",
    "start": "173600",
    "end": "180327"
  },
  {
    "text": "because you didn't\ndo pivoting-- you'd like to do pivoting in order to\nminimize the numerical error-- or you need to reorder in\norder to minimize fill-in.",
    "start": "180327",
    "end": "188390"
  },
  {
    "text": "As an example, I've\nsolved a research problem where there was something\nlike 40 million equations and unknowns, a system of\npartial differential equations.",
    "start": "188390",
    "end": "196610"
  },
  {
    "text": "And if you reorder\nthose equations, then you can solve via Gaussian\nelimination pretty readily.",
    "start": "196610",
    "end": "202470"
  },
  {
    "text": "But if you don't,\nwell-- my PC had-- I don't know-- like,\n192 gigabytes of RAM.",
    "start": "202470",
    "end": "208204"
  },
  {
    "text": " The elimination on that\nmatrix will fill the memory",
    "start": "208205",
    "end": "213950"
  },
  {
    "text": "of that PC up in 20 minutes. And you'll be stuck. It won't proceed after that.",
    "start": "213950",
    "end": "220100"
  },
  {
    "text": "So it's the difference\nbetween getting a solution and writing a publication\nabout the research problem",
    "start": "220100",
    "end": "225440"
  },
  {
    "text": "you're interested in and not. So how do you do reordering? Well, we use a process\ncalled permutation.",
    "start": "225440",
    "end": "232534"
  },
  {
    "text": "There's a certain\nclass of matrix called a permutation matrix that can-- its action, multiplying\nanother matrix,",
    "start": "232534",
    "end": "238520"
  },
  {
    "text": "can swap rows or columns. And here's an example\nof a permutation matrix",
    "start": "238520",
    "end": "243740"
  },
  {
    "text": "whose intention is to swap\nrow 1 and 2 of a matrix.",
    "start": "243740",
    "end": "249740"
  },
  {
    "text": "So here, it looks like\nidentity, except rather than having 1, 1 on the first\ntwo elements of the diagonal,",
    "start": "249740",
    "end": "257420"
  },
  {
    "text": "I have 0, 1 and 1, 0. Here's an example where I take\nthat sort of a matrix, which",
    "start": "257420",
    "end": "263330"
  },
  {
    "text": "should swap rows 1 and 2, and\nI multiply it by a vector. If you do this matrix\nvector multiplication, you'll see initially, the\nvector was x1, x2, x3.",
    "start": "263330",
    "end": "271490"
  },
  {
    "text": "But the product\nwill be x2, x1, x3. It swapped two rows\nin that vector.",
    "start": "271490",
    "end": "277250"
  },
  {
    "text": "Of course, a vector is\njust a matrix, right? It's an N by 1 matrix.",
    "start": "277250",
    "end": "282320"
  },
  {
    "text": "So P times A is the same\nas a matrix whose columns",
    "start": "282320",
    "end": "287720"
  },
  {
    "text": "are P times each of\nthe columns of A. That's what this\nnotation indicates here. And we know that\nP times a vector,",
    "start": "287720",
    "end": "295560"
  },
  {
    "text": "which is the column from A,\nwill swap two rows in A, right? So the product here\nwill be all the rows",
    "start": "295560",
    "end": "301740"
  },
  {
    "text": "of A, the different rows\nof AA superscript R, with row 1 and 2\nswapped with each other.",
    "start": "301740",
    "end": "308090"
  },
  {
    "text": "So permutation, multiplication\nby the special type of matrix, a permutation\nmatrix, does reordering of rows.",
    "start": "308090",
    "end": "317090"
  },
  {
    "text": "If I want to swap columns,\nI multiply my matrix from the right, IP transpose.",
    "start": "317090",
    "end": "323350"
  },
  {
    "text": "So if I want to\nswap column 1 and 2, I multiply A from the\nright by P transpose. How can I show that\nthat swaps columns?",
    "start": "323350",
    "end": "330569"
  },
  {
    "text": "Well, A times P\ntranspose is the same as P times A\ntranspose transpose.",
    "start": "330570",
    "end": "336760"
  },
  {
    "text": "P swaps rows. So it's swapping rows\nof A transpose, which is like swapping columns of A.",
    "start": "336760",
    "end": "343510"
  },
  {
    "text": "So we had some\nidentities associated with matrix-matrix\nmultiplication and their transposes.",
    "start": "343510",
    "end": "349312"
  },
  {
    "text": "And you can use that to work\nout how this permutation matrix will swap\ncolumns instead of rows if I multiply from the\nright instead of the left.",
    "start": "349312",
    "end": "356110"
  },
  {
    "text": " Here's an important\nconcept to know. Permutation matrices are-- would\nrefer to as unitary matrices.",
    "start": "356110",
    "end": "365750"
  },
  {
    "text": "They're transposed. It's also they're inverse. So P times P\ntranspose is identity.",
    "start": "365750",
    "end": "373060"
  },
  {
    "text": "If I swap the rows and\nthen I swap them back, I get back what I had before.",
    "start": "373060",
    "end": "378634"
  },
  {
    "text": "So there are lots\nof matrices that have this property\nthat they're unitary. We'll see some today.",
    "start": "378634",
    "end": "384819"
  },
  {
    "text": "But permutation matrices are\none class, maybe the simplest class, of unitary matrices. They're just doing row\nor column swaps, right?",
    "start": "384820",
    "end": "391630"
  },
  {
    "text": "That's their job. And so if I have some reordering\nof the equations or rows",
    "start": "391630",
    "end": "397480"
  },
  {
    "text": "of my system of\nequations that I want, that's going to be indicated by\na permutation matrix-- say, P1.",
    "start": "397480",
    "end": "403490"
  },
  {
    "text": "And I would multiply\nmy entire system of-- both sides of my\nsystem of equations by P1. That would reorder the rows.",
    "start": "403490",
    "end": "409940"
  },
  {
    "text": "If I have some\nreordering of the columns or the unknowns in my problem, I\nwould use a similar permutation",
    "start": "409940",
    "end": "416139"
  },
  {
    "text": "matrix, P2. Of course, P2 transpose\ntimes P2 is identity. So this product\nhere does nothing",
    "start": "416140",
    "end": "423010"
  },
  {
    "text": "to the system of equations. It just swaps the unknown. So there's a formal system for\ndoing this sort of swapping.",
    "start": "423010",
    "end": "428681"
  },
  {
    "text": "There are a couple other\nslides that are in your notes from last time that\nyou can look at and I'm happy to\nanswer questions on.",
    "start": "428681",
    "end": "433870"
  },
  {
    "text": "We don't have time\nto go into detail. It discusses the actual\nmethodology, the simplest",
    "start": "433871",
    "end": "439870"
  },
  {
    "text": "possible methodology, for\ndoing this kind of reordering or swapping. So this is a form\nof preconditioning.",
    "start": "439870",
    "end": "445190"
  },
  {
    "text": " If it's preconditioning\nfor pivoting, it's designed to\nminimize numerical error.",
    "start": "445190",
    "end": "452090"
  },
  {
    "text": "If it's preconditioning in order\nto minimize fill-in instead,",
    "start": "452090",
    "end": "457300"
  },
  {
    "text": "that's meant to make the problem\nsolvable on your computer. But it's a form\nof preconditioning",
    "start": "457300",
    "end": "462404"
  },
  {
    "text": "a system of equations. And we discussed\npreconditioning before. So now we know how to\nsolve systems of equations.",
    "start": "462404",
    "end": "469650"
  },
  {
    "text": "It's always done via\nGaussian elimination if we want an exact solution. There are lots of variants\non Gaussian elimination",
    "start": "469650",
    "end": "475770"
  },
  {
    "text": "that we can utilize. You're studying one of them\nin your homework assignment now, where you know the matrix\nis banded with some bandwidth.",
    "start": "475770",
    "end": "481800"
  },
  {
    "text": "So you don't do elimination\non an entire full matrix. You do it on a sparse matrix\nwhose structure you understand.",
    "start": "481800",
    "end": "488280"
  },
  {
    "text": "We discussed sparse\nmatrices and a little bit about reordering\nand now permutation.",
    "start": "488280",
    "end": "494500"
  },
  {
    "text": "I feel like my diffusion\nexample last time wasn't especially clear.",
    "start": "494500",
    "end": "499540"
  },
  {
    "text": "So let me give you a different\nexample of diffusion.",
    "start": "499540",
    "end": "504580"
  },
  {
    "text": "You guys know Plinko? Have you seen The\nPrice Is Right? This is a game where\nyou drop a chip",
    "start": "504580",
    "end": "510400"
  },
  {
    "text": "into a board with pegs in it. It's a model of diffusion.",
    "start": "510400",
    "end": "516000"
  },
  {
    "text": "The Plinko chip falls\nfrom level to level. It hits a peg. And it can go left or it can go\nright with equal probability.",
    "start": "516000",
    "end": "522549"
  },
  {
    "text": " So the Plinko chip\ndiffuses as it falls down.",
    "start": "522549",
    "end": "528800"
  },
  {
    "text": "This guy's excited. [LAUGHTER] He just won $10,000.",
    "start": "528800",
    "end": "533949"
  },
  {
    "text": "[LAUGHTER] ",
    "start": "533950",
    "end": "539779"
  },
  {
    "text": "There's a sparse\nmatrix that describes how the probability\nof finding the Plinko",
    "start": "539780",
    "end": "545810"
  },
  {
    "text": "chip in a certain cell\nevolves from level to level.",
    "start": "545810",
    "end": "554300"
  },
  {
    "text": "It works the same way the\ncellular automata model I showed you last time works. If the chip is in a particular\ncell, then at the next level,",
    "start": "554300",
    "end": "563340"
  },
  {
    "text": "there's a 50/50 chance\nthat I'll go to the left or I'll go to the right. It looks like this, right?",
    "start": "563340",
    "end": "568430"
  },
  {
    "text": "If the chip is here, there's\na 50/50 chance I'll go here or I'll go there. So if the probability was\n1 that I was in this cell,",
    "start": "568430",
    "end": "575810"
  },
  {
    "text": "then at the next level,\nit'll be half and a half. And at the next level, those\nhalves will split again.",
    "start": "575810",
    "end": "582350"
  },
  {
    "text": "So the probability that I'm in\na particular cell at level i is this Pi.",
    "start": "582350",
    "end": "588579"
  },
  {
    "text": "And the probability that I'm\nin a particular cell level i plus 1 is this Pi plus one. And there's some\nsparse matrix A which",
    "start": "588579",
    "end": "596700"
  },
  {
    "text": "spreads that probability out. It splits it into\nmy neighbors 50/50. ",
    "start": "596700",
    "end": "603690"
  },
  {
    "text": "Here's a simulation of Plinko. So I started with\nthe probability 1",
    "start": "603690",
    "end": "609060"
  },
  {
    "text": "in the center cell. And as I go through different\nlevels, I get split 50/50.",
    "start": "609060",
    "end": "615410"
  },
  {
    "text": "And you see a binomial or almost\nGaussian distribution spread as I go through\nmore and more levels",
    "start": "615410",
    "end": "622060"
  },
  {
    "text": "until it's equally probable\nthat I could wind up in any one of the cells. ",
    "start": "622060",
    "end": "628010"
  },
  {
    "text": "You can think about\nit this way, right? The probability\nat level i plus 1",
    "start": "628010",
    "end": "637180"
  },
  {
    "text": "that the chip is in cell\nN is inherited 50/50 from its two neighbors, right?",
    "start": "637180",
    "end": "643285"
  },
  {
    "text": "There's some probability that\nwas in these two neighbors. I would inherit half\nof that probability.",
    "start": "643286",
    "end": "649660"
  },
  {
    "text": "It would be split by these pegs. The sparse matrix that\nrepresents this operation",
    "start": "649660",
    "end": "657730"
  },
  {
    "text": "has two diagonals. And on each of those\ndiagonals is a half.",
    "start": "657730",
    "end": "662950"
  },
  {
    "text": "And you can build that matrix\nusing the spdiags command. It says that there's going to\nbe two diagonal components which",
    "start": "662950",
    "end": "672300"
  },
  {
    "text": "are equal to a half. And their position\nis going to be one on either side of\nthe central diagonal.",
    "start": "672300",
    "end": "680040"
  },
  {
    "text": "That's going to indicate that\nI pass this probability, 50/50, to each of my neighbors.",
    "start": "680040",
    "end": "685680"
  },
  {
    "text": "And then successive\nmultiplications by A will split this probability. And we'll see the\nsimulation that tells us",
    "start": "685680",
    "end": "691740"
  },
  {
    "text": "how probable it is\nto find the Plinko chip in a particular column. Yes? AUDIENCE: [INAUDIBLE]",
    "start": "691740",
    "end": "699077"
  },
  {
    "text": " JAMES W. SWAN: Yeah. So in diffusion in general?",
    "start": "699077",
    "end": "705110"
  },
  {
    "text": "AUDIENCE: Well, in this\ninstance in particular because [INAUDIBLE]",
    "start": "705110",
    "end": "710490"
  },
  {
    "text": "JAMES W. SWAN: Well, OK. That's fair enough. This is one particular model\nof the Plinko board, which",
    "start": "710490",
    "end": "717030"
  },
  {
    "text": "sort of imagines alternating\ncells that I'm falling through.",
    "start": "717030",
    "end": "722324"
  },
  {
    "text": "We could construct\nan alternative model, if we wanted to, that didn't\nhave that part of the picture, OK?",
    "start": "722324",
    "end": "727490"
  },
  {
    "text": " So that's a matrix that\nlooks like this, right? The central diagonal is 0.",
    "start": "727490",
    "end": "735440"
  },
  {
    "text": "Most of the\noff-diagonal components here are 0 and 1\nabove and 1 below. I get a half and a half.",
    "start": "735440",
    "end": "742095"
  },
  {
    "text": "And if I'm careful--\nsomebody mentioned I need boundary conditions. When the Plinko chip\ngets to the edge,",
    "start": "742095",
    "end": "747710"
  },
  {
    "text": "it doesn't fall out of the game. It gets reflected back in. So maybe we have to choose some\nspecial values for a couple",
    "start": "747710",
    "end": "754850"
  },
  {
    "text": "of elements of this matrix. But this is a sparse matrix. It has a sparse structure. It models a diffusion problem,\njust like we saw before.",
    "start": "754850",
    "end": "762140"
  },
  {
    "text": "Most of physics is\nlocal, like this, right? I just need to know what's\ngoing on with my neighbors. And I spread the\nprobability out.",
    "start": "762140",
    "end": "767780"
  },
  {
    "text": "I get this nice\ndiffusion problem. So it looks like this.",
    "start": "767780",
    "end": "773779"
  },
  {
    "text": "Here's something to notice. After many levels or cycles, I\nmultiply by A many, many times.",
    "start": "773780",
    "end": "780500"
  },
  {
    "text": "This probability distribution\nalways seems to flatten out. It becomes uniform. ",
    "start": "780500",
    "end": "787690"
  },
  {
    "text": "It turns out there are even\nspecial distributions for which A times A times\nthat distribution is",
    "start": "787690",
    "end": "794740"
  },
  {
    "text": "equal to that distribution. You can see it at the end here. This is one of those\nspecial distributions",
    "start": "794740",
    "end": "799839"
  },
  {
    "text": "where the probability is equal\nin every other cell, right?",
    "start": "799840",
    "end": "806140"
  },
  {
    "text": "And at the next level,\nit all gets passed down. That's one multiplication by--\nit all gets spread by 50%.",
    "start": "806140",
    "end": "811925"
  },
  {
    "text": "And the next\nmultiplication, everything gets spread by 50% again. And I recover the\nsame distribution that I had before, this\nuniform distribution.",
    "start": "811925",
    "end": "820430"
  },
  {
    "text": "That's a special distribution\nfor which A times A times P is equal to P. And\nthis distribution",
    "start": "820430",
    "end": "827170"
  },
  {
    "text": "is one of the eigenvectors\nof this matrix A times A.",
    "start": "827170",
    "end": "833500"
  },
  {
    "text": "It's a particular vector\nthat when I multiply it by this matrix AA, I\nget that vector back.",
    "start": "833500",
    "end": "843070"
  },
  {
    "text": "It happens to be unstretched. So this vector points\nin some direction. I transform it by the matrix.",
    "start": "843070",
    "end": "848649"
  },
  {
    "text": "And I get back something that\npoints in the same direction. That's the definition of this\nthing called an eigenvector.",
    "start": "848650",
    "end": "856029"
  },
  {
    "text": "And this will be the subject\nthat we focus on today. ",
    "start": "856030",
    "end": "862920"
  },
  {
    "text": "So eigenvectors of a matrix-- they're special vectors that\nare stretched on multiplication",
    "start": "862920",
    "end": "868339"
  },
  {
    "text": "by the matrix. So they're transformed. But they're only transformed\ninto a stretched form",
    "start": "868340",
    "end": "874152"
  },
  {
    "text": "of whatever they were before. They point in a direction. You transform them\nby the matrix. And you get something that\npoints in the same direction,",
    "start": "874152",
    "end": "880550"
  },
  {
    "text": "but is stretched. Before, we saw the\namount of stretch. The previous example, we saw\nthe amount of stretch was 1.",
    "start": "880550",
    "end": "886470"
  },
  {
    "text": "It wasn't stretched at all. You just get back the same\nvector you had before. But in principle, it could\ncome back with any length.",
    "start": "886470",
    "end": "892370"
  },
  {
    "text": " For a real N-by-N\nmatrix, there will",
    "start": "892370",
    "end": "899680"
  },
  {
    "text": "be eigenvectors and\neigenvalues, which are the amount of stretch,\nwhich are complex numbers.",
    "start": "899680",
    "end": "907565"
  },
  {
    "text": " And finding\neigenvector-eigenvalue pairs",
    "start": "907565",
    "end": "914120"
  },
  {
    "text": "involves solving N equations. We'd like to know what\nthese eigenvectors",
    "start": "914120",
    "end": "919760"
  },
  {
    "text": "and eigenvalues are. They're non-linear\nbecause they depend on both the value and the\nvector, the product of the two,",
    "start": "919760",
    "end": "928220"
  },
  {
    "text": "for N plus 1 unknowns. We don't know how to solve\nnon-linear equations yet. So we're kind of--",
    "start": "928220",
    "end": "933410"
  },
  {
    "text": "might seem like we're\nin a rough spot. But I'll show you\nthat we're not.",
    "start": "933410",
    "end": "938720"
  },
  {
    "text": "But because there's N equations\nfor N plus 1 unknowns, that means eigenvectors\nare not unique.",
    "start": "938720",
    "end": "945100"
  },
  {
    "text": "If W is an eigenvector,\nthan any other vector that points in\nthat same direction is also an eigenvector, right?",
    "start": "945100",
    "end": "951300"
  },
  {
    "text": "It also gets stretched\nby this factor lambda. So we can never say what\nan eigenvector is uniquely.",
    "start": "951300",
    "end": "959760"
  },
  {
    "text": "We can only prescribe\nits direction. Whatever its magnitude\nis, we don't care. We just care about\nits direction.",
    "start": "959760",
    "end": "965910"
  },
  {
    "text": "The amount of stretch,\nhowever, is unique. It's associated\nwith that direction.",
    "start": "965910",
    "end": "971079"
  },
  {
    "text": "So you have an\namount of stretch. And you have a direction. And that describes the\neigenvector-eigenvalue pair.",
    "start": "971080",
    "end": "977460"
  },
  {
    "text": " Is this clear? You've heard of eigenvalues\nand eigenvectors before?",
    "start": "977460",
    "end": "984250"
  },
  {
    "text": "Good.  So how do you find eigenvalues?",
    "start": "984250",
    "end": "990949"
  },
  {
    "text": "They seem like special\nsorts of solutions associated with a matrix. And if we understood them, then\nwe can do a transformation.",
    "start": "990950",
    "end": "998340"
  },
  {
    "text": "So I'll explain\nthat in a minute. But how do you actually\nfind these things, these eigenvalues?",
    "start": "998340",
    "end": "1003449"
  },
  {
    "text": "Well, I've got to solve an\nequation A times w equals lambda times w, which can be\ntransformed into A minus lambda",
    "start": "1003450",
    "end": "1012580"
  },
  {
    "text": "identity times w equals 0. And so the solution\nset to this equation is either w is equal to 0.",
    "start": "1012580",
    "end": "1020390"
  },
  {
    "text": "That's one possible\nsolution to this problem or the eigenvector w belongs to\nthe null space of this matrix.",
    "start": "1020390",
    "end": "1029140"
  },
  {
    "text": "It's one of those special\nvectors that when it multiplies this matrix gives back 0, right?",
    "start": "1029140",
    "end": "1034869"
  },
  {
    "text": "It gets projected out on\ntransformation by this matrix. Well, this solution doesn't\nseem very useful to us, right?",
    "start": "1034869",
    "end": "1042829"
  },
  {
    "text": "It's trivial. So let's go with this\nidea that w belongs to the null space\nof A minus lambda I.",
    "start": "1042829",
    "end": "1051640"
  },
  {
    "text": "That means A minus lambda I must\nbe a singular matrix, whatever it is, right?",
    "start": "1051640",
    "end": "1056649"
  },
  {
    "text": "And if it's singular, then the\ndeterminant of a minus lambda I must be equal to 0. ",
    "start": "1056650",
    "end": "1063450"
  },
  {
    "text": "So if this is true,\nand it should be true if we don't want a\ntrivial solution, then",
    "start": "1063450",
    "end": "1069000"
  },
  {
    "text": "the determinant of A minus\nlambda I is equal to 0. So if we can compute\nthat determinant",
    "start": "1069000",
    "end": "1074190"
  },
  {
    "text": "and solve for lambda, then\nwe'll know the eigenvalue. Well, it turns out that\nthe determinant of a matrix",
    "start": "1074190",
    "end": "1081750"
  },
  {
    "text": "like A minus lambda I is a\npolynomial in terms of lambda.",
    "start": "1081750",
    "end": "1089340"
  },
  {
    "text": "It's a polynomial\nof degree N called the characteristic polynomial. And the N roots of this\ncharacteristic polynomial",
    "start": "1089340",
    "end": "1097049"
  },
  {
    "text": "are called the\neigenvalues of the matrix. So there are N possible\nlambdas for which A minus",
    "start": "1097050",
    "end": "1104850"
  },
  {
    "text": "lambda I become singular. It has a null space. And associated with those\nvalues are eigenvectors, vectors",
    "start": "1104850",
    "end": "1112140"
  },
  {
    "text": "that live in that null space. So this polynomial-- we could\ncompute it for any matrix.",
    "start": "1112140",
    "end": "1119140"
  },
  {
    "text": "We could compute this\nthing in principle, right? And we might even be able\nto factor it into this form.",
    "start": "1119140",
    "end": "1127790"
  },
  {
    "text": "And then lambda 1,\nlambda 2, lambda N in this factorized form are\nall the possible eigenvalues",
    "start": "1127790",
    "end": "1134210"
  },
  {
    "text": "associated with our\nmatrix A, right? There are all the possible\namounts of stretch",
    "start": "1134210",
    "end": "1139400"
  },
  {
    "text": "that can be imparted to\nparticular eigenvectors. We don't know those\nvectors yet, right?",
    "start": "1139400",
    "end": "1144470"
  },
  {
    "text": "We'll find them in a second. But we know the\namounts of stretch that can be imparted\nby this matrix.",
    "start": "1144470",
    "end": "1149991"
  },
  {
    "text": "OK?  Any questions so far?",
    "start": "1149991",
    "end": "1156385"
  },
  {
    "text": "No. Let's do an example. Here's a matrix, minus 2, 1, 3.",
    "start": "1156385",
    "end": "1163218"
  },
  {
    "text": "And it's 0's everywhere else. And we'd like to find the\neigenvalues of this matrix.",
    "start": "1163218",
    "end": "1168830"
  },
  {
    "text": "So we need to know A minus\nlambda I and its determinant. So here's A minus lambda\nI. We just subtract lambda",
    "start": "1168830",
    "end": "1176080"
  },
  {
    "text": "from each of the diagonals. And the determinant--\nwell, here, it's just the product of\nthe diagonal elements.",
    "start": "1176080",
    "end": "1181480"
  },
  {
    "text": "So that's the determinant\nof a diagonal matrix like this, the product\nof the diagonal elements. So it's minus 2 minus lambda\ntimes 1 minus lambda times 3",
    "start": "1181480",
    "end": "1189880"
  },
  {
    "text": "minus lambda. And the determent of this\nhas to be equal to 0. So the amounts of\nstretch, the eigenvalues",
    "start": "1189880",
    "end": "1196600"
  },
  {
    "text": "imparted by this matrix,\nare minus 2, 1, and 3.",
    "start": "1196600",
    "end": "1201830"
  },
  {
    "text": "And we found the eigenvalues. Here's another matrix.",
    "start": "1201830",
    "end": "1206910"
  },
  {
    "text": "Can you work out the\neigenvalues of this matrix? Let's take 90 seconds. You can work with\nyour neighbors.",
    "start": "1206910",
    "end": "1212500"
  },
  {
    "text": "See if you can figure out the\neigenvalues of that matrix. ",
    "start": "1212500",
    "end": "1237880"
  },
  {
    "text": "Nobody's collaborating today. I'm going to do it myself. AUDIENCE: [INAUDIBLE]",
    "start": "1237880",
    "end": "1242950"
  },
  {
    "text": "JAMES W. SWAN: It's OK. ",
    "start": "1242950",
    "end": "1308820"
  },
  {
    "text": "OK. What are you finding? Anyone want to guess\nwhat are the eigenvalues?",
    "start": "1308820",
    "end": "1314113"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] JAMES W. SWAN: Good. OK. So we need to compute the\ndeterminant of A minus lambda",
    "start": "1314114",
    "end": "1320601"
  },
  {
    "text": "I. That'll be minus 2\nminus lambda times minus 2 minus lambda minus 1.",
    "start": "1320601",
    "end": "1327309"
  },
  {
    "text": "You can solve this to find that\nlambda equals minus 3 or minus 1.",
    "start": "1327310",
    "end": "1333325"
  },
  {
    "text": "These little checks are useful. If you couldn't do\nthis, that's OK. But you should try to\npractice this on your own",
    "start": "1333325",
    "end": "1338370"
  },
  {
    "text": "to make sure you can.  Here are some more examples.",
    "start": "1338370",
    "end": "1343720"
  },
  {
    "text": "So the elements of\na diagonal matrix are always the eigenvalues\nbecause the determinant",
    "start": "1343720",
    "end": "1348840"
  },
  {
    "text": "of a diagonal matrix\nis the product of the diagonal elements. So these diagonal values here\nare the roots of the secular",
    "start": "1348840",
    "end": "1357660"
  },
  {
    "text": "characteristic polynomial. They are the eigenvalues. It turns out the diagonal\nelements of a triangular matrix",
    "start": "1357660",
    "end": "1364429"
  },
  {
    "text": "are eigenvalues, too. This should seem\nfamiliar to you.",
    "start": "1364430",
    "end": "1369669"
  },
  {
    "text": "We talked about easy-to-solve\nsystems of equations, right? Diagonal systems of equations\nare easy to solve, right?",
    "start": "1369670",
    "end": "1375987"
  },
  {
    "text": "Triangular systems of\nequations are easy to solve. It's also easy to find\ntheir eigenvalues.",
    "start": "1375987",
    "end": "1382090"
  },
  {
    "text": "So the diagonal elements\nhere are the eigenvalues of the triangular matrix.",
    "start": "1382090",
    "end": "1388410"
  },
  {
    "text": "And eigenvalues have\ncertain properties that can be inferred from the\nproperties of polynomials, right?",
    "start": "1388410",
    "end": "1393490"
  },
  {
    "text": "Since they are the\nroots to a polynomial, if we know certain\nthings that should be true of those\npolynomial of roots,",
    "start": "1393490",
    "end": "1399240"
  },
  {
    "text": "that has to be true of the\neigenvalues themselves. So if we have a matrix\nwhich is real-valued,",
    "start": "1399240",
    "end": "1405559"
  },
  {
    "text": "then we know that\nwe're going to have this polynomial of degree N\nwhich is also real-valued, OK?",
    "start": "1405559",
    "end": "1413330"
  },
  {
    "text": "It can have no more\nthan N roots, right?",
    "start": "1413330",
    "end": "1418350"
  },
  {
    "text": "And so A can have no more\nthan N distinct eigenvalues.",
    "start": "1418350",
    "end": "1424549"
  },
  {
    "text": "The eigenvalues, like the\nfactors of the polynomial, don't have to be\ndistinct, though? You could have multiplicity in\nthe roots of the polynomial.",
    "start": "1424550",
    "end": "1432160"
  },
  {
    "text": "So it's possible that lambda\n1 here is an eigenvalue twice.",
    "start": "1432160",
    "end": "1438550"
  },
  {
    "text": "That's referred to as\nalgebraic multiplicity. We'll come back to\nthat idea in a second.",
    "start": "1438550",
    "end": "1444510"
  },
  {
    "text": "Because the polynomial\nis real-valued, it means that the\neigenvalues could be real or complex,\njust like the roots",
    "start": "1444510",
    "end": "1450890"
  },
  {
    "text": "of a real-valued polynomial. But complex eigenvalues always\nappear as conjugate pairs.",
    "start": "1450890",
    "end": "1457220"
  },
  {
    "text": "If there is a\ncomplex eigenvalue, then necessarily its\ncomplex conjugate",
    "start": "1457220",
    "end": "1462860"
  },
  {
    "text": "is also an eigenvalue. And here's a couple\nother properties. So the determinant\nof a matrix is",
    "start": "1462860",
    "end": "1470180"
  },
  {
    "text": "the product of the eigenvalues. We talked once about the\ntrace of a matrix, which is the sum of its\ndiagonal elements.",
    "start": "1470180",
    "end": "1477070"
  },
  {
    "text": "The trace of a matrix is also\nthe sum of the eigenvalues. ",
    "start": "1477070",
    "end": "1483490"
  },
  {
    "text": "These can sometimes\ncome in handy-- not often, but sometimes. ",
    "start": "1483490",
    "end": "1493160"
  },
  {
    "text": "Here's an example I\ntalked about before-- so a series of\nchemical reactions. So we have a batch,\na batch reactor.",
    "start": "1493160",
    "end": "1499970"
  },
  {
    "text": "We load some material in. And we want to know how the\nconcentrations of A, B, C, and D vary as a\nfunction of time.",
    "start": "1499970",
    "end": "1508700"
  },
  {
    "text": "And so A transforms into B.\nB and C are in equilibrium. C and D are in equilibrium. And our conservation equation\nfor material is here.",
    "start": "1508700",
    "end": "1516245"
  },
  {
    "text": " This is a rate matrix.",
    "start": "1516245",
    "end": "1522350"
  },
  {
    "text": "We'd like to understand what\nthe characteristic polynomial of that is.",
    "start": "1522350",
    "end": "1527600"
  },
  {
    "text": "The eigenvalues\nof that matrix are going to tell us something\nabout how different rate processes evolve in time.",
    "start": "1527600",
    "end": "1534770"
  },
  {
    "text": "You can imagine\njust using units. On this side, we have\nconcentration over time.",
    "start": "1534770",
    "end": "1540929"
  },
  {
    "text": "On this side, we\nhave concentration. And the rate matrix has units\nof rate, or 1 over time. So those eigenvalues\nalso have units of rate.",
    "start": "1540930",
    "end": "1548390"
  },
  {
    "text": "And they tell us the rate at\nwhich different transformations between these materials occur.",
    "start": "1548390",
    "end": "1555589"
  },
  {
    "text": "And so if we want to find\nthe characteristic polynomial of this matrix and we need to\ncompute the determinant of this",
    "start": "1555589",
    "end": "1560760"
  },
  {
    "text": "matrix minus lambda I-- so\nsubtract lambda from each of the diagonals-- even though this is a\nfour-by-four matrix,",
    "start": "1560760",
    "end": "1567570"
  },
  {
    "text": "its determinant\nis easy to compute because it's full of zeros. I'm not going to\ncompute it for you here.",
    "start": "1567570",
    "end": "1573690"
  },
  {
    "text": "It'll turn out that the\ncharacteristic polynomial looks like this. You should actually try\nto do this determinant",
    "start": "1573690",
    "end": "1578850"
  },
  {
    "text": "and show that the polynomial\nworks out to be this. But knowing that this is the\ncharacteristic polynomial,",
    "start": "1578850",
    "end": "1583890"
  },
  {
    "text": "what are the eigenvalues\nof the rate matrix? ",
    "start": "1583890",
    "end": "1589830"
  },
  {
    "text": "If that's the\ncharacteristic polynomial, what are the\neigenvalues, or tell me some of the eigenvalues\nof the rate matrix?",
    "start": "1589830",
    "end": "1595430"
  },
  {
    "text": "AUDIENCE: 0. JAMES W. SWAN: 0. 0's an eigenvalue. Lambda equals 0 is a solution.",
    "start": "1595430",
    "end": "1600470"
  },
  {
    "text": "Minus k1 is another solution. What is this eigenvalue\n0 correspond to?",
    "start": "1600470",
    "end": "1608236"
  },
  {
    "text": "What's that? AUDIENCE: [INAUDIBLE] ",
    "start": "1608236",
    "end": "1615410"
  },
  {
    "text": "JAMES W. SWAN: OK.  Physically, it's a rate process\nwith 0 rate, steady state.",
    "start": "1615410",
    "end": "1626890"
  },
  {
    "text": "So the 0 eigenvalue's going to\ncorrespond to the steady state. The eigenvector associated\nwith that eigenvalue",
    "start": "1626890",
    "end": "1632559"
  },
  {
    "text": "should correspond to the\nsteady state solution. How about this\neigenvalue minus k1?",
    "start": "1632560",
    "end": "1639430"
  },
  {
    "text": "This is a rate\nprocess with rate k1. What physical process\ndoes that represent? ",
    "start": "1639430",
    "end": "1647020"
  },
  {
    "text": "It's something evolving\nin time now, right? So that's the\ntransformation of A into B.",
    "start": "1647020",
    "end": "1653900"
  },
  {
    "text": "And the eigenvector should\nreflect that transformation. We'll see what those\neigenvectors are in a minute.",
    "start": "1653900",
    "end": "1661044"
  },
  {
    "text": "But these eigenvalues\ncan be interpreted in terms of physical processes. This quadratic solution\nhere has some eigenvalue.",
    "start": "1661044",
    "end": "1668230"
  },
  {
    "text": "I don't know what it is. You use the quadratic\nformula and you can find it. But it involves k2, k3, k4.",
    "start": "1668230",
    "end": "1674380"
  },
  {
    "text": "And this is a typo. It should be k5. And so that says something about\nthe interconversion between B,",
    "start": "1674380",
    "end": "1680590"
  },
  {
    "text": "C, and D, and the rate\nprocesses that occur as we convert from B to C to D.",
    "start": "1680590",
    "end": "1691542"
  },
  {
    "text": "Is that too fast? Do you want to write some more\non this slide before I go on, or are you OK?",
    "start": "1691542",
    "end": "1696980"
  },
  {
    "text": "Are there any\nquestions about this? No. ",
    "start": "1696980",
    "end": "1703740"
  },
  {
    "text": "Given an eigenvalue, a\nparticular eigenvalue, what's the corresponding eigenvector?",
    "start": "1703740",
    "end": "1709740"
  },
  {
    "text": "We know the eigenvector\nisn't uniquely specified. It belongs to the null\nspace of this matrix",
    "start": "1709740",
    "end": "1715500"
  },
  {
    "text": "A minus lambda I times identity.",
    "start": "1715500",
    "end": "1721710"
  },
  {
    "text": "Even though it's not\nunique, we might still try to find it using\nGaussian elimination, right?",
    "start": "1721710",
    "end": "1727740"
  },
  {
    "text": "So we may try to take-- we may try to solve the\nequation A minus lambda I times identity\nmultiplied by w equals",
    "start": "1727740",
    "end": "1736020"
  },
  {
    "text": "0 using Gaussian elimination. But because it's not\nunique, at some point,",
    "start": "1736020",
    "end": "1741870"
  },
  {
    "text": "we'll run out of rows\nto eliminate, right? There's a null space\nto this matrix, right?",
    "start": "1741870",
    "end": "1747530"
  },
  {
    "text": "We won't be able to\neliminate everything. We'd say it's rank\ndeficient, right?",
    "start": "1747530",
    "end": "1754160"
  },
  {
    "text": "So we'll be able to\neliminate up to some R, the rank of this matrix. And then all the\ncomponents below",
    "start": "1754160",
    "end": "1759740"
  },
  {
    "text": "are essentially free or\narbitrarily specified. There are no\nequations to say what",
    "start": "1759740",
    "end": "1764769"
  },
  {
    "text": "those components of\nthe eigenvector are. ",
    "start": "1764770",
    "end": "1771810"
  },
  {
    "text": "The number of all 0 rows-- it's called the geometric\nmultiplicity of the eigenvalue.",
    "start": "1771810",
    "end": "1778331"
  },
  {
    "text": "Sorry. Geometric is missing here. ",
    "start": "1778331",
    "end": "1785220"
  },
  {
    "text": "It's the number of components\nof the eigenvector that can be freely specified. ",
    "start": "1785220",
    "end": "1792460"
  },
  {
    "text": "The geometric\nmultiplicity might be 1. That's like saying that\nthe eigenvectors are all",
    "start": "1792460",
    "end": "1799566"
  },
  {
    "text": "pointing in the same\ndirection, but can have arbitrary magnitude, right? It might have geometric\nmultiplicity 2, which",
    "start": "1799567",
    "end": "1807400"
  },
  {
    "text": "means the eigenvectors\nassociated with this eigenvalue live in some plane. And any vector from that plane\nis a corresponding eigenvector.",
    "start": "1807400",
    "end": "1816180"
  },
  {
    "text": "It might have a higher geometric\nmultiplicity associated with it. ",
    "start": "1816180",
    "end": "1822480"
  },
  {
    "text": "So let's try something here. Let's try to find the\neigenvectors of this matrix.",
    "start": "1822480",
    "end": "1828660"
  },
  {
    "text": "I told you what the\neigenvalues were. They were the\ndiagonal values here. So they're minus 2, 1, and 3.",
    "start": "1828660",
    "end": "1835560"
  },
  {
    "text": "Let's look for the\neigenvector corresponding to this eigenvalue. So I want to solve this equation\nA minus this particular lambda,",
    "start": "1835560",
    "end": "1844450"
  },
  {
    "text": "which is minus 2, times\nidentity equals 0. So I got to do Gaussian\nelimination on this matrix.",
    "start": "1844450",
    "end": "1852070"
  },
  {
    "text": "It's already eliminated\nfor me, right? I have one row which\nis all 0's, which",
    "start": "1852070",
    "end": "1858130"
  },
  {
    "text": "says the first component\nof my eigenvector can be freely specified.",
    "start": "1858130",
    "end": "1865269"
  },
  {
    "text": "The other two\ncomponents have to be 0. 3 times the second component\nof my eigenvector is 0.",
    "start": "1865270",
    "end": "1872160"
  },
  {
    "text": "5 times the third\ncomponent is 0. So the other two\ncomponents have to be 0. But the first component\nis freely specified.",
    "start": "1872160",
    "end": "1878110"
  },
  {
    "text": "So the eigenvector associated\nwith this eigenvalue is 1, 0, 0.",
    "start": "1878110",
    "end": "1885520"
  },
  {
    "text": "If I take a vector which\npoints in the x-direction in R3",
    "start": "1885520",
    "end": "1890650"
  },
  {
    "text": "and I multiply it\nby this matrix, it gets stretched by minus 2. So I point in the\nother direction.",
    "start": "1890650",
    "end": "1896230"
  },
  {
    "text": "And I stretch out\nby a factor of 2. You can guess then what\nthe other eigenvectors are.",
    "start": "1896230",
    "end": "1903380"
  },
  {
    "text": "What's the eigenvector\nassociated with this eigenvalue here? 0, 1, 0, or anything\nproportional to that.",
    "start": "1903380",
    "end": "1910340"
  },
  {
    "text": "What's the\neigenvector associated with this eigenvalue? 0, 0, 1, or anything\nproportional to it.",
    "start": "1910340",
    "end": "1916230"
  },
  {
    "text": "All these eigenvectors have a\ngeometric multiplicity of 1, right?",
    "start": "1916230",
    "end": "1921690"
  },
  {
    "text": "I can just specify some\nscalar variant on them. And they'll transform\ninto themselves.",
    "start": "1921690",
    "end": "1927000"
  },
  {
    "start": "1927000",
    "end": "1932180"
  },
  {
    "text": "Here's a problem you can try. Here's our series of\nchemical reactions again. And we want to know the\neigenvector of the rate",
    "start": "1932180",
    "end": "1938390"
  },
  {
    "text": "matrix having eigenvalue 0. This should correspond\nto the steady state solution of our ordinary\ndifferential equation here.",
    "start": "1938390",
    "end": "1946227"
  },
  {
    "text": "So you've got to do\nelimination on this matrix.  Can you do that?",
    "start": "1946227",
    "end": "1951430"
  },
  {
    "text": "Can you find this eigenvector? Try it out with your neighbor. See if you can do it. And then we'll compare results.",
    "start": "1951430",
    "end": "1957179"
  },
  {
    "text": "This will just be a quick\ntest of understanding. ",
    "start": "1957180",
    "end": "2087454"
  },
  {
    "text": "Are you guys able to do this? Sort of, maybe? ",
    "start": "2087454",
    "end": "2096489"
  },
  {
    "text": "Here's the answer, or an\nanswer, for the eigenvector. It's not unique, right? It's got some constant\nout in front of it.",
    "start": "2096489",
    "end": "2104500"
  },
  {
    "text": "So you do Gaussian\nelimination here. So subtract or add the\nfirst row to the second row.",
    "start": "2104500",
    "end": "2110320"
  },
  {
    "text": "You'll eliminate this 0, right? And then add the second\nrow to the third row. You'll eliminate this k2.",
    "start": "2110320",
    "end": "2117970"
  },
  {
    "text": "You have to do a little bit more\nwork to do elimination of k4 here.",
    "start": "2117970",
    "end": "2123370"
  },
  {
    "text": "But that's not a big deal. Again, you'll add the\nthird row to the fourth row and eliminate that. And you'll also wind\nup eliminating this k5.",
    "start": "2123370",
    "end": "2130869"
  },
  {
    "text": "So the last row here\nwill be all 0's. And that means the last\ncomponent of our eigenvector's",
    "start": "2130870",
    "end": "2137140"
  },
  {
    "text": "freely specifiable. It can be anything we want. So I said it is 1. And then I did back\nsubstitution to determine all",
    "start": "2137140",
    "end": "2143860"
  },
  {
    "text": "the other components, right? That's the way to do this. And here's what the eigenvector\nlooks like when you're done.",
    "start": "2143860",
    "end": "2150780"
  },
  {
    "text": "The steady state\nsolution has no A in it. Of course, A is just eliminated\nby a forward reaction.",
    "start": "2150780",
    "end": "2156700"
  },
  {
    "text": "So if we let this run out to\ninfinity, there should be no A. And that's what happens. But there's equilibria\nbetween B, C, and D.",
    "start": "2156700",
    "end": "2165160"
  },
  {
    "text": "And the steady state solution\nreflects that equilibria. We have to pick what this\nconstant out in front is.",
    "start": "2165160",
    "end": "2170294"
  },
  {
    "text": "And we discussed this\nbefore, actually, right? You would pick that based on\nhow much material was initially in the reactor.",
    "start": "2170294",
    "end": "2175950"
  },
  {
    "text": "We've got to have an\noverall mass balance. And that's missing from this\nsystem of equations, right? Mass conservation is what gave\nthe null space for this rate",
    "start": "2175950",
    "end": "2184400"
  },
  {
    "text": "matrix in the first place. Make sense? Try this example out.",
    "start": "2184400",
    "end": "2190124"
  },
  {
    "text": "See if you can work\nthrough the details of it. I think it's useful to be able\nto do these sorts of things quickly.",
    "start": "2190124",
    "end": "2195249"
  },
  {
    "text": "Here are some simpler problems.  So here's a matrix.",
    "start": "2195249",
    "end": "2200920"
  },
  {
    "text": "It's not a very good matrix. Matrices can't be good or bad. It's not particularly\ninteresting. But it's all 0's.",
    "start": "2200920",
    "end": "2207900"
  },
  {
    "text": "So what are its eigenvalues? It's just 0, right?",
    "start": "2207900",
    "end": "2213750"
  },
  {
    "text": "The diagonal elements\nare the eigenvalues. And they're 0. That eigenvalue has\nalgebraic multiplicity 2.",
    "start": "2213750",
    "end": "2222320"
  },
  {
    "text": " It's a double root of the\nsecular characteristic",
    "start": "2222320",
    "end": "2228140"
  },
  {
    "text": "polynomial.  Can you give me\nthe eigenvectors?",
    "start": "2228140",
    "end": "2234510"
  },
  {
    "start": "2234510",
    "end": "2245260"
  },
  {
    "text": "Can you give me\neigenvectors of this matrix? Can you give me linearly\nindependent-- yeah?",
    "start": "2245260",
    "end": "2250640"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] JAMES W. SWAN: OK. AUDIENCE: [INAUDIBLE] JAMES W. SWAN: OK.",
    "start": "2250640",
    "end": "2255710"
  },
  {
    "text": "Good. So this is a very ambiguous sort\nof problem or question, right? Any vector I multiply by A here\nis going to be stretched by 0",
    "start": "2255710",
    "end": "2264050"
  },
  {
    "text": "because A by its very\nnature is all 0's. All those vectors\nlive in a plane.",
    "start": "2264050",
    "end": "2271910"
  },
  {
    "text": "So any vector from\nthat plane is going to be transformed in this way.",
    "start": "2271910",
    "end": "2277319"
  },
  {
    "text": "The eigenvector\ncorresponding to eigenvalue 0 has geometric multiplicity\n2 because I can freely",
    "start": "2277320",
    "end": "2284760"
  },
  {
    "text": "specify two of its components. Oh my goodness. I went so fast. We'll just do it this way. Algebraic multiplicity 2,\ngeometric multiplicity 2--",
    "start": "2284760",
    "end": "2293700"
  },
  {
    "text": "I can pick two vectors. They can be any two I\nwant in principle, right?",
    "start": "2293700",
    "end": "2300020"
  },
  {
    "text": "It has geometric multiplicity 2. Here's another matrix. It's a little more\ninteresting than the last one.",
    "start": "2300020",
    "end": "2306780"
  },
  {
    "text": "I stuck a 1 in there instead.  Again, the eigenvalues are 0.",
    "start": "2306780",
    "end": "2317240"
  },
  {
    "text": "It's a double root. So it has algebraic\nmultiplicity 2. ",
    "start": "2317240",
    "end": "2324019"
  },
  {
    "text": "But you can convince\nyourself that there's only one direction\nthat transforms",
    "start": "2324020",
    "end": "2330760"
  },
  {
    "text": "that squeeze down to 0, right? There's only one\nvector direction",
    "start": "2330760",
    "end": "2337840"
  },
  {
    "text": "that lives in the null\nspace of A minus lambda I-- lives in the null\nspace of A. And that's",
    "start": "2337840",
    "end": "2344559"
  },
  {
    "text": "vectors parallel to 1, 0. So the eigenvector associated\nwith that eigenvalue 0",
    "start": "2344560",
    "end": "2353049"
  },
  {
    "text": "has geometric\nmultiplicity 1 instead of geometric multiplicity 2.",
    "start": "2353050",
    "end": "2359200"
  },
  {
    "start": "2359200",
    "end": "2370209"
  },
  {
    "text": "Now, here's an\nexample for you to do. ",
    "start": "2370209",
    "end": "2376040"
  },
  {
    "text": "Can you find the eigenvalues\nand some linearly independent eigenvectors of this\nmatrix, which looks",
    "start": "2376040",
    "end": "2382099"
  },
  {
    "text": "like the one we just looked at. But now it's three-by-three\ninstead of two-by-two.",
    "start": "2382100",
    "end": "2387569"
  },
  {
    "text": "And if you find those\neigenvalues and eigenvectors, what are the algebraic and\ngeometric multiplicity?",
    "start": "2387570",
    "end": "2393329"
  },
  {
    "start": "2393330",
    "end": "2412410"
  },
  {
    "text": "Well, you guys must\nhad a rough week. You're usually\nmuch more talkative and energetic than this. [LAUGHTER]",
    "start": "2412410",
    "end": "2418944"
  },
  {
    "start": "2418944",
    "end": "2425110"
  },
  {
    "text": "Well, what are the\neigenvalues here? AUDIENCE: 0. JAMES W. SWAN: Yeah. They all turn out to be 0.",
    "start": "2425110",
    "end": "2431670"
  },
  {
    "text": "So that's an algebraic\nmultiplicity of 3. It'll turn out there are two\nvectors, two vector directions,",
    "start": "2431670",
    "end": "2439260"
  },
  {
    "text": "that I can specify that will\nboth be squeezed down to 0. In fact, any vector\nfrom the x-y plane",
    "start": "2439260",
    "end": "2447630"
  },
  {
    "text": "will also be squeezed down to 0. So this has algebraic\nmultiplicity 3 and geometric multiplicity 2.",
    "start": "2447630",
    "end": "2453725"
  },
  {
    "text": " I'm going to explain why this\nis important in a second.",
    "start": "2453726",
    "end": "2460200"
  },
  {
    "text": "But understanding\nthat this can happen is going to be useful for you.",
    "start": "2460200",
    "end": "2465250"
  },
  {
    "text": "So if an eigenvalue\nis distinct, then it has algebraic multiplicity 1.",
    "start": "2465250",
    "end": "2470510"
  },
  {
    "text": "It's the only eigenvalue\nwith that value. It's the only time that\namount of stretch is imparted.",
    "start": "2470510",
    "end": "2477420"
  },
  {
    "text": "And there will be only one\ncorresponding eigenvector. There will be a direction\nand an amount of stretch. ",
    "start": "2477420",
    "end": "2485330"
  },
  {
    "text": "If an eigenvalue has a\nalgebraic multiplicity M, well, you just saw that\nthe geometric multiplicity,",
    "start": "2485330",
    "end": "2495040"
  },
  {
    "text": "which is the dimension of\nthe null space of A minus lambda I-- it's the dimension\nof the space spanned",
    "start": "2495040",
    "end": "2502690"
  },
  {
    "text": "by no vectors of\nA minus lambda I-- it's going to be bigger\nthan 1 or equal to 1.",
    "start": "2502690",
    "end": "2509079"
  },
  {
    "text": "And it's going to be\nsmaller or equal to M. And we saw different variants on\nvalues that sit in this range.",
    "start": "2509080",
    "end": "2516650"
  },
  {
    "text": "So there could be as many\nas M linearly independent eigenvectors.",
    "start": "2516650",
    "end": "2522042"
  },
  {
    "text": "And there may be fewer.  So geometric multiplicity--\nit's the number",
    "start": "2522042",
    "end": "2527830"
  },
  {
    "text": "of linearly independent\neigenvectors associated with an eigenvalue. It's the dimension of the\nnull space of this matrix.",
    "start": "2527830",
    "end": "2533155"
  },
  {
    "text": " Problems for which the geometric\nand algebraic multiplicity",
    "start": "2533155",
    "end": "2540480"
  },
  {
    "text": "are the same for all the\neigenvalues and eigenvectors, all those pairs, are nice\nbecause the matrix then",
    "start": "2540480",
    "end": "2549570"
  },
  {
    "text": "is said to have a complete\nset of eigenvectors. There's enough\neigenvectors in the problem",
    "start": "2549570",
    "end": "2557190"
  },
  {
    "text": "that they describe the\nspan of our vector space",
    "start": "2557190",
    "end": "2563000"
  },
  {
    "text": "RN that our matrix is doing\ntransformations between. If we have geometric\nmultiplicity that's",
    "start": "2563000",
    "end": "2569250"
  },
  {
    "text": "smaller than the\nalgebraic multiplicity, then some of these\nstretched-- we",
    "start": "2569250",
    "end": "2575520"
  },
  {
    "text": "can't stretch in all\npossible directions in RN. There's going to be a direction\nthat might be left out.",
    "start": "2575520",
    "end": "2582609"
  },
  {
    "text": "We want to be able to do\na type of transformation called an eigendecomposition.",
    "start": "2582609",
    "end": "2588325"
  },
  {
    "text": "I'm going to show\nyou that in a second. It's useful for solving\nsystems of equations or for transforming systems\nof ordinary differential",
    "start": "2588325",
    "end": "2595500"
  },
  {
    "text": "equations, linear ordinary\ndifferential equations. But we're only going to\nbe able to do that when",
    "start": "2595500",
    "end": "2601620"
  },
  {
    "text": "we have this complete\nset of eigenvectors. When we don't have\nthat complete set, we're going to have to do\nother sorts of transformations.",
    "start": "2601620",
    "end": "2609750"
  },
  {
    "text": "You have a problem in your\nhomework now, I think, that has this sort of a\nhang-up associated with it.",
    "start": "2609750",
    "end": "2615796"
  },
  {
    "text": "It's the second problem\nin your homework set. That's something to think about. ",
    "start": "2615796",
    "end": "2623290"
  },
  {
    "text": "For a matrix with the\ncomplete set of eigenvectors, we can write the following.",
    "start": "2623290",
    "end": "2628800"
  },
  {
    "text": "A times a matrix W is equal\nto W times the matrix lambda.",
    "start": "2628800",
    "end": "2634100"
  },
  {
    "text": "Let me tell you what\nW and lambda are. So W's a matrix whose\ncolumns are made up of this--",
    "start": "2634100",
    "end": "2640670"
  },
  {
    "text": "all of these eigenvectors. And lambda's a matrix\nwhose diagonal values are",
    "start": "2640670",
    "end": "2648200"
  },
  {
    "text": "each of the corresponding\neigenvalues associated with those eigenvectors.",
    "start": "2648200",
    "end": "2653269"
  },
  {
    "text": "This is nothing more\nthan a restatement",
    "start": "2653270",
    "end": "2659000"
  },
  {
    "text": "of the original\neigenvalue problem. AW is lambda W. But\nnow each eigenvalue",
    "start": "2659000",
    "end": "2671000"
  },
  {
    "text": "has a corresponding\nparticular eigenvector. And we've stacked\nthose equations up",
    "start": "2671000",
    "end": "2677750"
  },
  {
    "text": "to make this statement about\nmatrix-matrix multiplication. So we've taken each of\nthese W's over here. And we've just made them the\ncolumns of a particular matrix.",
    "start": "2677750",
    "end": "2685169"
  },
  {
    "text": "But it's nothing more\nthan a restatement of the fundamental\neigenvalue problem we posed at the beginning here.",
    "start": "2685169",
    "end": "2690440"
  },
  {
    "text": " But what's nice is if I\nhave this complete set",
    "start": "2690440",
    "end": "2697130"
  },
  {
    "text": "of eigenvectors, then W has an\ninverse that I can write down.",
    "start": "2697130",
    "end": "2702619"
  },
  {
    "text": "So another way to state this\nsame equation is that lambda-- the eigenvalues can be found\nfrom this matrix product, W",
    "start": "2702620",
    "end": "2711619"
  },
  {
    "text": "inverse times A times W. And under these\ncircumstances, we say the matrix can\nbe diagonalized.",
    "start": "2711620",
    "end": "2718310"
  },
  {
    "text": "There's a transformation\nfrom A to a diagonal form.",
    "start": "2718310",
    "end": "2723890"
  },
  {
    "text": "That's good for us, right? We know diagonal systems of\nequations are easy to solve, right? So if I knew what the\neigenvectors were,",
    "start": "2723890",
    "end": "2731950"
  },
  {
    "text": "then I can transform my\nequation to this diagonal form. I could solve systems of\nequations really easily.",
    "start": "2731950",
    "end": "2737400"
  },
  {
    "text": "Of course, we just\nsaw that knowing what those eigenvectors\nare requires solving systems of equations, anyway.",
    "start": "2737400",
    "end": "2743380"
  },
  {
    "text": "So the problem of\nfinding the eigenvectors is as hard as the problem of\nsolving a system of equations.",
    "start": "2743380",
    "end": "2749680"
  },
  {
    "text": "But in principle, I can do\nthis sort of transformation. Equivalently, the matrix A can\nbe written as W times lambda",
    "start": "2749680",
    "end": "2755830"
  },
  {
    "text": "times W inverse. These are all equivalent\nways of writing this fundamental relationship\nup here when the inverse of W",
    "start": "2755830",
    "end": "2763835"
  },
  {
    "text": "exists.  So this means that if I know the\neigenvalues and eigenvectors,",
    "start": "2763835",
    "end": "2769520"
  },
  {
    "text": "I can easily reconstruct\nmy equation, right? If I know the\neigenvectors in A, then I can easily diagonalize my\nsystem of equations, right?",
    "start": "2769520",
    "end": "2777110"
  },
  {
    "text": "So this is a useful sort\nof transformation to do. We haven't talked about how\nit's done in the computer.",
    "start": "2777110",
    "end": "2783032"
  },
  {
    "text": "We've talked about how\nyou would do it by hand. These are ways you\ncould do it by hand. The computer won't do\nGaussian elimination",
    "start": "2783032",
    "end": "2788570"
  },
  {
    "text": "for each of those eigenvectors\nindependently, right? Each elimination procedure\nis order N cubed, right?",
    "start": "2788570",
    "end": "2795360"
  },
  {
    "text": "And you got to do that\nfor N eigenvectors. So that's N to the\nfourth operations. That's pretty slow.",
    "start": "2795360",
    "end": "2800757"
  },
  {
    "text": "There's an alternative\nway of doing it that's beyond the scope\nof this class called--",
    "start": "2800757",
    "end": "2806270"
  },
  {
    "text": "it's called the\nLanczos algorithm. And it's what's referred\nto as a Krylov subspace",
    "start": "2806270",
    "end": "2812870"
  },
  {
    "text": "method, that sort\nof iterative method where you take products of your\nmatrix with certain vectors",
    "start": "2812870",
    "end": "2817910"
  },
  {
    "text": "and from those products,\ninfer what the eigenvectors and eigenvalues are. So that's the way a\ncomputer's going to do it.",
    "start": "2817910",
    "end": "2823490"
  },
  {
    "text": "That's going to be an order\nN cubed sort of calculation to find all the eigenvalues\nand eigenvectors [INAUDIBLE] solving a system of equations.",
    "start": "2823490",
    "end": "2829924"
  },
  {
    "text": "But sometimes you\nwant these things. Here's an example of how\nthis eigendecomposition can",
    "start": "2829925",
    "end": "2835970"
  },
  {
    "text": "be useful to you if you did it. So we know the matrix A can\nbe represented as W lambda W",
    "start": "2835970",
    "end": "2842750"
  },
  {
    "text": "inverse times x equals b. This is our transformed\nsystem of equations here.",
    "start": "2842750",
    "end": "2848109"
  },
  {
    "text": "We've just substituted for A. If I multiply both sides of\nthis equation by W inverse,",
    "start": "2848110",
    "end": "2853520"
  },
  {
    "text": "then I've got lambda times\nthe quantity W inverse x is equal to W inverse b.",
    "start": "2853520",
    "end": "2859505"
  },
  {
    "text": "And if I call this\nquantity in parentheses y, then I have an easy-to-solve\nsystem of equations for y.",
    "start": "2859505",
    "end": "2865500"
  },
  {
    "text": " y is equal to lambda\ninverse times c.",
    "start": "2865500",
    "end": "2870530"
  },
  {
    "text": "But lambda inverse\nis just 1 over each of the diagonal\ncomponents of lambda. Lambda's a diagonal matrix.",
    "start": "2870530",
    "end": "2878269"
  },
  {
    "text": "Then all I need\nto do-- ooh, typo. There's an equal\nsign missing here. Sorry for that. Now all I need to do is\nsubstitute for what I called y",
    "start": "2878270",
    "end": "2885799"
  },
  {
    "text": "and what I called c. So y was W inverse times x. That's equal to lambda inverse\ntimes W inverse times b.",
    "start": "2885800",
    "end": "2893859"
  },
  {
    "text": "And so I multiply both sides of\nthis equation by W. And I get x is W lambda inverse W inverse b.",
    "start": "2893860",
    "end": "2899240"
  },
  {
    "text": "So if I knew the eigenvalues\nand eigenvectors, I can really easily solve\nthe system of equations. If I did this decomposition,\nI could solve many systems",
    "start": "2899240",
    "end": "2907160"
  },
  {
    "text": "of equations, right? They're simple to\nsolve with just matrix-matrix multiplication.",
    "start": "2907160",
    "end": "2913190"
  },
  {
    "text": "Now, how is W inverse computed?  Well, W inverse transpose\nare actually the eigenvectors",
    "start": "2913190",
    "end": "2922530"
  },
  {
    "text": "of A transpose.  You may have to compute\nthis matrix explicitly.",
    "start": "2922530",
    "end": "2929000"
  },
  {
    "text": "But there are times\nwhen we deal with so-called symmetric\nmatrices, ones for which they are equal to their transpose.",
    "start": "2929000",
    "end": "2937250"
  },
  {
    "text": "And if that's the\ncase, and if you take all of your eigenvectors\nand you normalize them so they're of length 1--",
    "start": "2937250",
    "end": "2943990"
  },
  {
    "text": "the Euclidean norm is 1-- then it'll turn out that\nW inverse is precisely",
    "start": "2943990",
    "end": "2950680"
  },
  {
    "text": "equal to W transpose, right? And so the eigenvalue\nmatrix will be unitary.",
    "start": "2950680",
    "end": "2956170"
  },
  {
    "text": "It'll have this property where\nits transposes is its inverse, right? So this becomes\ntrivial to do then,",
    "start": "2956170",
    "end": "2962090"
  },
  {
    "text": "this process of W inverse. It's not always true that\nthis is the case, right? It is true when we\ndeal with problems",
    "start": "2962090",
    "end": "2969470"
  },
  {
    "text": "that have symmetric matrices\nassociated with them. That pops up in a lot of cases.",
    "start": "2969470",
    "end": "2976910"
  },
  {
    "text": "You can prove-- I might ask you to\nshow this some time-- that the eigenvectors\nof a symmetric matrix",
    "start": "2976910",
    "end": "2981940"
  },
  {
    "text": "are orthogonal, that they\nsatisfy this property that--",
    "start": "2981940",
    "end": "2987050"
  },
  {
    "text": "I take the dot product between\ntwo different eigenvectors and it'll be equal to 0 unless\nthose are the same eigenvector.",
    "start": "2987050",
    "end": "2994982"
  },
  {
    "text": "That's a property associated\nwith symmetric matrices. ",
    "start": "2994982",
    "end": "3002203"
  },
  {
    "text": "They're also useful\nwhen analyzing systems of ordinary\ndifferential equations. So here, I've got a differential\nequation, a vector x dot.",
    "start": "3002204",
    "end": "3010160"
  },
  {
    "text": "So the time derivative of\nx is equal to A times x.",
    "start": "3010160",
    "end": "3015960"
  },
  {
    "text": "And if I substitute my\neigendecomposition-- so W lambda W inverse--",
    "start": "3015960",
    "end": "3022650"
  },
  {
    "text": "and I define a new\nunknown y instead of x, then I can diagonalize\nthat system of equations.",
    "start": "3022650",
    "end": "3028990"
  },
  {
    "text": "So you see y dot is\nequal to lambda times y where each component\nof y is decoupled",
    "start": "3028990",
    "end": "3035130"
  },
  {
    "text": "from all of the others. Each of them satisfies their own\nordinary differential equation",
    "start": "3035130",
    "end": "3040590"
  },
  {
    "text": "that's not coupled to\nany of the others, right? And it has a simple\nfirst-order rate constant,",
    "start": "3040590",
    "end": "3045610"
  },
  {
    "text": "which is the\neigenvalue associated with that particular\neigendirection.",
    "start": "3045610",
    "end": "3051940"
  },
  {
    "text": "So this system of\nODEs is decoupled. And it's easy to solve. You know the solution, right? It's an exponential.",
    "start": "3051940",
    "end": "3057390"
  },
  {
    "text": " And that can be quite\nhandy when we're looking at different sorts\nof chemical rate processes",
    "start": "3057390",
    "end": "3063850"
  },
  {
    "text": "that correspond to linear\ndifferential equations. We'll talk about nonlinear,\nsystems of nonlinear,",
    "start": "3063850",
    "end": "3069100"
  },
  {
    "text": "differential equations\nlater in this term. And you'll find out that\nthis same sort of analysis",
    "start": "3069100",
    "end": "3075250"
  },
  {
    "text": "can be quite useful there. So we'll linearize\nthose equations. And we'll ask is their linear--\nin their linearized form, what",
    "start": "3075250",
    "end": "3082282"
  },
  {
    "text": "are these different\nrate constants? How big are they? They might determine\nwhat we need to do in order to integrate\nthose equations numerically",
    "start": "3082282",
    "end": "3090935"
  },
  {
    "text": "because there are many\ntimes when there's not a complete set of eigenvectors. That happens.",
    "start": "3090935",
    "end": "3096380"
  },
  {
    "text": "And then the matrix can't\nbe diagonalized in this way. There are some\ncomponents that can't",
    "start": "3096380",
    "end": "3102830"
  },
  {
    "text": "be decoupled from each other. That's what this\ndiagonalization does, right? It splits up these different\nstretching directions",
    "start": "3102830",
    "end": "3110124"
  },
  {
    "text": "from each other. But there's some directions\nthat can't be decoupled from each other anymore. And then there are other\ntransformations one can do.",
    "start": "3110124",
    "end": "3116230"
  },
  {
    "text": "So there's an\nalmost diagonal form that you can transform into\ncalled the Jordan normal form.",
    "start": "3116230",
    "end": "3123610"
  },
  {
    "text": "There are other transformations\nthat one can do, like called, for example, Schur\ndecomposition, which is a transformation\ninto an upper triangular",
    "start": "3123610",
    "end": "3130960"
  },
  {
    "text": "form for this matrix. We'll talk next time about the\nsingular value decomposition,",
    "start": "3130960",
    "end": "3136129"
  },
  {
    "text": "which is another sort\nof transformation one can do when we don't have these\ncomplete sets of eigenvectors.",
    "start": "3136129",
    "end": "3142030"
  },
  {
    "start": "3142030",
    "end": "3149700"
  },
  {
    "text": "But this concludes our\ndiscussion of eigenvalues and eigenvectors. You'll get a chance to practice\nthese things on your next two",
    "start": "3149700",
    "end": "3155819"
  },
  {
    "text": "homework assignments, actually. So it'll come up in a couple\nof different circumstances. I would really\nencourage you to try",
    "start": "3155819",
    "end": "3163100"
  },
  {
    "text": "to solve some of these example\nproblems that were in here. Solving by hand can be useful. Make sure you can\nwork through the steps",
    "start": "3163100",
    "end": "3169080"
  },
  {
    "text": "and understand where these\ndifferent concepts come into play in terms\nof determining",
    "start": "3169080",
    "end": "3174960"
  },
  {
    "text": "what the eigenvalues\nand eigenvectors are. All right. Have a great weekend. See you on Monday.",
    "start": "3174960",
    "end": "3180630"
  },
  {
    "start": "3180630",
    "end": "3189584"
  }
]