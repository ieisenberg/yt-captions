[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "7410"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "7410",
    "end": "13960"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at",
    "start": "13960",
    "end": "19790"
  },
  {
    "text": "ocw.mit.edu. ",
    "start": "19790",
    "end": "24930"
  },
  {
    "text": "PROFESSOR: I guess\nwe should start. This is the last of\nthese lectures.",
    "start": "24930",
    "end": "32050"
  },
  {
    "text": "The final will be on next\nWednesday, as I hope you all know by this time, in the ice\nrink, whatever that means.",
    "start": "32050",
    "end": "41699"
  },
  {
    "text": "And there was some question\nabout how many sheets of paper you could bring in\nas crib sheets.",
    "start": "41700",
    "end": "49170"
  },
  {
    "text": "And it seems like the reasonable\nthing is four sheets, which means you can\nbring in the two sheets you",
    "start": "49170",
    "end": "55960"
  },
  {
    "text": "made up for the quiz\nplus two more. Or you can make up four new\nones if you want or do whatever you want.",
    "start": "55960",
    "end": "62949"
  },
  {
    "text": "I don't think it's very\nimportant how many sheets you bring in, because I've never\nseen anybody referring to",
    "start": "62950",
    "end": "69800"
  },
  {
    "text": "their sheets. I mean, it's a good way of\norganizing what you know to",
    "start": "69800",
    "end": "75770"
  },
  {
    "text": "try to put it on four\nsheets of paper. ",
    "start": "75770",
    "end": "80840"
  },
  {
    "text": "I want to mostly review what\nwe've done throughout the term, with a few more general\ncomments thrown in.",
    "start": "80840",
    "end": "89990"
  },
  {
    "text": "I thought I'd start with\nmartingales, because we then completely finish what\nwe wanted to",
    "start": "89990",
    "end": "95409"
  },
  {
    "text": "talk about last time. And the Strong Law of\nLarge Numbers was left slightly hanging.",
    "start": "95410",
    "end": "101800"
  },
  {
    "text": "And I want to show you\nhow to do that in a little better way.",
    "start": "101800",
    "end": "107870"
  },
  {
    "text": "And also show you that it's a\nmore general theorem than it appears to be at first sight.",
    "start": "107870",
    "end": "114550"
  },
  {
    "text": "So let's go with martingales.",
    "start": "114550",
    "end": "119686"
  },
  {
    "text": "The basic definition is a\nsequence of random variables is a martingale, if, for all\nelements of the sequence, the",
    "start": "119686",
    "end": "129429"
  },
  {
    "text": "expected value of Zn, given all\nof the previous values, is",
    "start": "129430",
    "end": "136939"
  },
  {
    "text": "equal to the random variable,\nZ n minus 1. Remember, and we've talked about\nthis a number of times,",
    "start": "136940",
    "end": "145230"
  },
  {
    "text": "when you're talking about the\nexpected value of one random variable, given a bunch of other\nrandom variables, you're",
    "start": "145230",
    "end": "152010"
  },
  {
    "text": "only taking the expectation\nover the first part. You're only taking the\nexpectation over Z sub n.",
    "start": "152010",
    "end": "158760"
  },
  {
    "text": "And the other quantities are\nstill random variables. Namely, you have an expected\nvalue Z sub n, for each sample",
    "start": "158760",
    "end": "165860"
  },
  {
    "text": "value of Z n minus 1, all\nthe way down to Z 1. And what the definition says is\nit's a martingale only if,",
    "start": "165860",
    "end": "174520"
  },
  {
    "text": "for all sample values of those\nearlier values, the expected",
    "start": "174520",
    "end": "180880"
  },
  {
    "text": "value is equal to the sample\nvalue of the most recent one.",
    "start": "180880",
    "end": "187190"
  },
  {
    "text": "Namely, the memory is all\ncontained right in this last term, effectively.",
    "start": "187190",
    "end": "192390"
  },
  {
    "text": "At least as far as expectation\nis concerned. Memory might be far broader than\nthat for everything else.",
    "start": "192390",
    "end": "200625"
  },
  {
    "text": "And the first thing we did with\nmartingales is we said the expected value was again,\nif you're only given part of",
    "start": "200626",
    "end": "209030"
  },
  {
    "text": "the history, if you're only\ngiven the history from i back to 1, where i is strictly less\nthan n minus 1, that expected",
    "start": "209030",
    "end": "218360"
  },
  {
    "text": "value is equal to Zi. So no matter where you start\ngoing back, the expected value",
    "start": "218360",
    "end": "223920"
  },
  {
    "text": "of Z sub n is the most recent\nvalue that is given.",
    "start": "223920",
    "end": "232160"
  },
  {
    "text": "So if the most recent value\ngiven is Z 1, then the expected value of Zn,\ngiven Z1, is Z1.",
    "start": "232160",
    "end": "240590"
  },
  {
    "text": "And also along with that, you\nhave the relationship, the expected value of Zn is equal\nto the expected value of Zi,",
    "start": "240590",
    "end": "247700"
  },
  {
    "text": "just by taking the expected\nvalue over Z sub i. So all of that's sort\nof straightforward.",
    "start": "247700",
    "end": "255950"
  },
  {
    "text": "We talked a good deal about\nthe increments of a martingale. The increments, X sub n equals\nZ sub n minus Zn minus 1, are",
    "start": "255950",
    "end": "267050"
  },
  {
    "text": "very much like the increments\nthat we have when a renewal process, when a Poisson\nprocess, all of these",
    "start": "267050",
    "end": "273900"
  },
  {
    "text": "processes we talked about we\ncan define in various ways. And here we can define a\nmartingale in two ways also.",
    "start": "273900",
    "end": "282410"
  },
  {
    "text": "One is by the actual martingale\nitself, which are, in a sense, the sums\nof the increments.",
    "start": "282410",
    "end": "289570"
  },
  {
    "text": "And the other ways in terms\nof the increments. And the increments satisfy the\nproperty that the expected",
    "start": "289570",
    "end": "295680"
  },
  {
    "text": "value of Xn, given all the\nearlier values, is equal to 0.",
    "start": "295680",
    "end": "301850"
  },
  {
    "text": "Namely, no matter what are all\nthe earlier values are, X sub n has mean 0 in order\nto be a martingale.",
    "start": "301850",
    "end": "311009"
  },
  {
    "text": "A good special case of this is\nwhere X sub n is equal to U",
    "start": "311010",
    "end": "316940"
  },
  {
    "text": "sub n times Y sub n, where the\nU sub n are IID, equiprobable",
    "start": "316940",
    "end": "325430"
  },
  {
    "text": "1 and minus 1. And the Y sub i's are anything\nyou want them to be. It's just that the U sub i's\nhave to be independent",
    "start": "325430",
    "end": "332490"
  },
  {
    "text": "of the Y sub i. So I think this shows that in\nfact martingales are really a",
    "start": "332490",
    "end": "340419"
  },
  {
    "text": "pretty broad class of things. And they were invented to talk\nabout fair gambling games,",
    "start": "340420",
    "end": "347050"
  },
  {
    "text": "where they wanted to give the\ngambler the opportunity to do whatever he wanted to do.",
    "start": "347050",
    "end": "352420"
  },
  {
    "text": "But the game itself was defined\nin such a way that, no matter what you do,\nthe game is fair.",
    "start": "352420",
    "end": "359340"
  },
  {
    "text": "You establish bets in one\nwhatever things you want to. And when you wind up with it,\nthe expected value of X sub n,",
    "start": "359340",
    "end": "367260"
  },
  {
    "text": "given the past, is always 0. And that's equivalent to saying\nthe expected value of Z",
    "start": "367260",
    "end": "373639"
  },
  {
    "text": "sub n, given the past,\nis equal to Z sub i.",
    "start": "373640",
    "end": "378900"
  },
  {
    "text": "Examples we talked about are 0\nmean random walks and products of unit-mean IID random\nvariables.",
    "start": "378900",
    "end": "387000"
  },
  {
    "text": "So they're both these product\nmartingales, and there are these sum martingales. And those are just two simple\nexamples, which",
    "start": "387000",
    "end": "395000"
  },
  {
    "text": "come up all the time. Then we talked about\nsubmartingales.",
    "start": "395000",
    "end": "400035"
  },
  {
    "text": "A submartingale is like\na martingale, except it grows with time.",
    "start": "400035",
    "end": "406290"
  },
  {
    "text": "And we're not going to talk\nabout supermartingales, because a supermartingale is\njust a negative submartingale.",
    "start": "406290",
    "end": "413750"
  },
  {
    "text": "So we don't have to\ntalk about that. A martingale is a\nsubmartingale.",
    "start": "413750",
    "end": "419569"
  },
  {
    "text": "So anything you know about\nsubmartingales applies to martingales also.",
    "start": "419570",
    "end": "424970"
  },
  {
    "text": "So you can state theorems for\nsubmartingales and they apply to martingales just as well.",
    "start": "424970",
    "end": "431159"
  },
  {
    "text": "You can say stronger things very\noften about martingales. ",
    "start": "431160",
    "end": "436949"
  },
  {
    "text": "And then we have the same\ntheorem for submartingales. ",
    "start": "436950",
    "end": "445750"
  },
  {
    "text": "Now that should say, and it did\nsay, until my evil twin got a hold of it, if Zn is a\nsubmartingales, then for n",
    "start": "445750",
    "end": "454830"
  },
  {
    "text": "greater than i, greater than\n0, this expected value is greater than or equal to Zi.",
    "start": "454830",
    "end": "461740"
  },
  {
    "text": "And the expected value Zn is\ngreater than equal to the expected value of Zi.",
    "start": "461740",
    "end": "467070"
  },
  {
    "text": "In other words, this theorem,\nfor submartingales, is the same as the corresponding\ntheorem for martingales,",
    "start": "467070",
    "end": "474470"
  },
  {
    "text": "except now you have inequalities\nthere, just like you have inequalities in\nthe definition of the",
    "start": "474470",
    "end": "481099"
  },
  {
    "text": "submartingales. So there's nothing\nstrange there. Then we found out that, if you\nhave a convex function, from",
    "start": "481100",
    "end": "490560"
  },
  {
    "text": "the reals into the reals, then\nJensen's inequality says that the expected value of h of X is\ngreater than or equal to h",
    "start": "490560",
    "end": "498750"
  },
  {
    "text": "of the expected value of x. We showed a picture for\nthat you remember. There's a convex curve.",
    "start": "498750",
    "end": "503800"
  },
  {
    "text": "There's some straight line. And what Jensen's inequality\nsays is you take an average",
    "start": "503800",
    "end": "510930"
  },
  {
    "text": "over the expected value\nof X, and you're somewhere above the line. And you take the average\nfirst, and you're",
    "start": "510930",
    "end": "517070"
  },
  {
    "text": "sitting on the line.  So if h of X is convex, that's\nwhat Jensen's inequality is.",
    "start": "517070",
    "end": "525620"
  },
  {
    "text": "And it follows from that that,\nif Zn is a submartingale-- and that includes\nmartingales--",
    "start": "525620",
    "end": "532480"
  },
  {
    "text": "and h is convex and the expected\nvalue of h of X is finite, then h of Zn is\na martingale also.",
    "start": "532480",
    "end": "542420"
  },
  {
    "text": "In other words, if you have\na martingale Z sub n, the expected value of Z sub\nn is a submartingale.",
    "start": "542420",
    "end": "551535"
  },
  {
    "text": "The expected value of E to\nR Zn is a martingale. Use whatever convex function you\nwant to, and you wind up,",
    "start": "551535",
    "end": "560790"
  },
  {
    "text": "martingales go into\nsubmartingales.  You can't get out of the range\nof submartingales that easily.",
    "start": "560790",
    "end": "570400"
  },
  {
    "text": "We then talked about stopped\nmartingales and stopped submartingales.",
    "start": "570400",
    "end": "576805"
  },
  {
    "text": "We said a stopped process,\nfor a possibly defective stopping time--",
    "start": "576805",
    "end": "583330"
  },
  {
    "text": "now you remember what\na stopping time is? A stopping time is a random\nvariable, which is a function",
    "start": "583330",
    "end": "589400"
  },
  {
    "text": "of everything that takes place\nup until the time of stopping. And you have to look at the\ndefinition carefully, because",
    "start": "589400",
    "end": "597530"
  },
  {
    "text": "stopping time comes in too many\nplaces to just say it and understand what it means.",
    "start": "597530",
    "end": "603770"
  },
  {
    "text": "But it's clear what it means,\nif you view yourself as an observer watching a sequence of\nrandom variables, of sample",
    "start": "603770",
    "end": "613000"
  },
  {
    "text": "values of random variables,\none after another. And after you see a certain\nnumber of random variables,",
    "start": "613000",
    "end": "620160"
  },
  {
    "text": "your rule says, stop. And then you don't\nobserve anymore. So you just observe this\nfinite number.",
    "start": "620160",
    "end": "626690"
  },
  {
    "text": "And then you stop\nat that point. And then you're all done. If it's a possibly defective\nstopping rule, then you might",
    "start": "626690",
    "end": "633170"
  },
  {
    "text": "keep on going forever,\nor you might stop. You don't know what you're\ngoing to do. ",
    "start": "633170",
    "end": "642610"
  },
  {
    "text": "The stopped process Z sub n\nstar is a little different from what we were\ndoing before.",
    "start": "642610",
    "end": "649399"
  },
  {
    "text": "Before what we were doing\nis we were sitting there observing this process. At a certain point, the stopping\nrule said stop.",
    "start": "649400",
    "end": "657389"
  },
  {
    "text": "And before, we were\nvery obedient. And when the stopping rule told\nus to stop, we stopped.",
    "start": "657390",
    "end": "664550"
  },
  {
    "text": "Now, since we know a little\nmore, we question authority a little more.",
    "start": "664550",
    "end": "669960"
  },
  {
    "text": "And when the stopping rule says\nstop, we break things into two processes.",
    "start": "669960",
    "end": "675180"
  },
  {
    "text": "There's the original process,\nwhich keeps on going. And there this stopped process,\nwhich just stops.",
    "start": "675180",
    "end": "682360"
  },
  {
    "text": "And it's convenient to have a\nstopped process instead of just a stopping rule. Because with a stopped process,\nyou can look at any",
    "start": "682360",
    "end": "691000"
  },
  {
    "text": "time into the future, and if\nit's already stopped, you know what the stopped value is.",
    "start": "691000",
    "end": "696050"
  },
  {
    "text": "You know what it was\nwhen it stopped. You don't necessarily know when\nit stopped, by looking at",
    "start": "696050",
    "end": "701100"
  },
  {
    "text": "in the future. But you know that it did stop. So the stopped process, well,\nit says here what it is.",
    "start": "701100",
    "end": "711448"
  },
  {
    "text": " It satisfies the stopped value\nat time n as equal to Z sub n,",
    "start": "711448",
    "end": "720740"
  },
  {
    "text": "if n is less than or equal to\nthe stopping time J, and Z sub n star is equal to Z sub J,\nif n is greater than J.",
    "start": "720740",
    "end": "729430"
  },
  {
    "text": "So you get up to the stopping\ntiming, and you stop. And then it just stays\nfixed forever after.",
    "start": "729430",
    "end": "734880"
  },
  {
    "text": "And the nice theorem there is\nthat the stopped process for a",
    "start": "734880",
    "end": "742170"
  },
  {
    "text": "submartingale, with a possibly\ndefective stopping rule, is a",
    "start": "742170",
    "end": "747570"
  },
  {
    "text": "submartingale again. What that means is it's just a\nconcise way of writing, the",
    "start": "747570",
    "end": "753970"
  },
  {
    "text": "stopped process for a martingale\nis a martingale in its own right.",
    "start": "753970",
    "end": "759140"
  },
  {
    "text": "And the stopped process for\na submartingale is a submartingale in\nits own right.",
    "start": "759140",
    "end": "766350"
  },
  {
    "text": "So the convenient thing is, you\ncan take a martingale, you can stop it, you still\nhave a martingale.",
    "start": "766350",
    "end": "773870"
  },
  {
    "text": "And everything you know about\nmartingales applies to this stopping process.",
    "start": "773870",
    "end": "779170"
  },
  {
    "text": "So we're getting to the point\nwhere, starting out with a martingale, we can do lots\nof things with it.",
    "start": "779170",
    "end": "784890"
  },
  {
    "text": "And that's the whole\nmathematical game. With a mathematical\ngame, you build up",
    "start": "784890",
    "end": "791040"
  },
  {
    "text": "theorems from nothing. As an experimentalist or an\nengineer, you sort of try to",
    "start": "791040",
    "end": "797690"
  },
  {
    "text": "figure out those things from\nthe reality around you.",
    "start": "797690",
    "end": "806050"
  },
  {
    "text": "Here, we're just\nbuilding it up. And the other part of that\ntheorem says that the expected",
    "start": "806050",
    "end": "816420"
  },
  {
    "text": "value of Z1 is less than or\nequal to the expected value of Zn star, is less than or equal\nto the expected value of Zn",
    "start": "816420",
    "end": "825520"
  },
  {
    "text": "for a submartingale. And they're all equal\nto a martingale. In other words, the marginal\nexpectations for a martingale,",
    "start": "825520",
    "end": "835130"
  },
  {
    "text": "they start out a Z1. They stay at Z1. And for the stopped process,\nthey stay at that same value.",
    "start": "835130",
    "end": "843370"
  },
  {
    "text": "And that's not too surprising. Because if you have a\nmartingale, if you go until",
    "start": "843370",
    "end": "850839"
  },
  {
    "text": "you reach the stopping point,\nfrom that stopping point on, the martingale has mean\n0, from that point on.",
    "start": "850840",
    "end": "859260"
  },
  {
    "text": " Not the martingale itself,\nbut the increments of the",
    "start": "859260",
    "end": "865610"
  },
  {
    "text": "martingale have mean 0,\nfrom that point on. And the stopped process\nhas mean 0.",
    "start": "865610",
    "end": "871920"
  },
  {
    "text": "In other words, the stopped\nprocess, the increments are actually 0. Whereas for the original\nprocess, the",
    "start": "871920",
    "end": "877680"
  },
  {
    "text": "increments wobble around. But they still have mean 0. So this is a very nice and\nuseful thing to know.",
    "start": "877680",
    "end": "886430"
  },
  {
    "text": "If you look at this product\nmartingale, Z sub n is E to the rsn minus n gamma or r .",
    "start": "886430",
    "end": "893760"
  },
  {
    "text": "Why is that a martingale?  How do you know it's\na martingale?",
    "start": "893760",
    "end": "899920"
  },
  {
    "text": " Well, you look at the expected\nvalue of this.",
    "start": "899920",
    "end": "907880"
  },
  {
    "text": "And it's expected\nvalue of this. The expected value of E to rsn\nis the moment generating",
    "start": "907880",
    "end": "915910"
  },
  {
    "text": "function of Z sub\nn, of s sub n.",
    "start": "915910",
    "end": "921610"
  },
  {
    "text": "It's moment generating\nfunction of E to rsn. And the moment generating\nfunction of E to rsn is just E",
    "start": "921610",
    "end": "931355"
  },
  {
    "text": "to the n gamma of r. So this is clearly something\nwhich should be a martingale,",
    "start": "931355",
    "end": "940560"
  },
  {
    "text": "because it just keeps at\nthat level all along. If you have a stopping rule,\nsuch as a threshold crossing,",
    "start": "940560",
    "end": "947900"
  },
  {
    "text": "then you've got a stopped\nmartingale. And subject to some little\nmathematical nitpicks, which",
    "start": "947900",
    "end": "954570"
  },
  {
    "text": "the text talks about, this leads\nyou to the much more general version of Wald's\nidentity, which says that the",
    "start": "954570",
    "end": "964370"
  },
  {
    "text": "expected value of Z, at the time\nof stopping, is equal to",
    "start": "964370",
    "end": "969790"
  },
  {
    "text": "the expected value of\nE to the rsJ minus J gamma of r equals 1. This you remember is what Wald's\nidentity was when we",
    "start": "969790",
    "end": "977310"
  },
  {
    "text": "were just talking about\nrandom walks. And this was a more general\nversion, because it's talking",
    "start": "977310",
    "end": "982980"
  },
  {
    "text": "about general stopping rules,\ninstead of just to thresholds. But it does have these little\nmathematical nitpicks in it,",
    "start": "982980",
    "end": "991730"
  },
  {
    "text": "which I'm not going to\ntalk about here. Then we have Kolmogorov's\nsubmartingale inequality.",
    "start": "991730",
    "end": "1000020"
  },
  {
    "text": "We talked about all of these\nthings last time. So we're going pretty quickly\nthrough them.",
    "start": "1000020",
    "end": "1006310"
  },
  {
    "text": "The submartingale inequality\nis really the Markov",
    "start": "1006310",
    "end": "1011390"
  },
  {
    "text": "inequality souped up. And what it says is, if you\nhave a non-negative",
    "start": "1011390",
    "end": "1018280"
  },
  {
    "text": "submartingale, that can\ninclude a non-negative martingale, for any positive\ninteger m, the probability",
    "start": "1018280",
    "end": "1027559"
  },
  {
    "text": "that the maximum is a Z sub i,\nfrom 1 to m, is greater than",
    "start": "1027560",
    "end": "1033939"
  },
  {
    "text": "or equal to a, is less than or\nequal to the expected value of Z sub m over a.",
    "start": "1033940",
    "end": "1039150"
  },
  {
    "text": "You see all that the Markov\ninequality says is the",
    "start": "1039150",
    "end": "1045150"
  },
  {
    "text": "probability that Z sub m is\ngreater than or equal to a, is less than or equal to this.",
    "start": "1045150",
    "end": "1050770"
  },
  {
    "text": "This puts a lot more teeth into\nit, because it lets you talk about all of these random\nvariables, up until time m.",
    "start": "1050770",
    "end": "1058310"
  },
  {
    "text": "And it says the maximum\nof them satisfies this inequality. I mean, we always knew that\nthe Markov inequality was",
    "start": "1058310",
    "end": "1064410"
  },
  {
    "text": "very, very weak. And this is also pretty weak. But it's not quite as\nweak, because it",
    "start": "1064410",
    "end": "1070580"
  },
  {
    "text": "covers a lot more things. If you have a non-negative\nmartingale-- this is submartingales,\nthis is martingales.",
    "start": "1070580",
    "end": "1078520"
  },
  {
    "text": "You see here, with\nsubmartingales, the expected value of Z sub m keeps\nincreasing with m.",
    "start": "1078520",
    "end": "1087640"
  },
  {
    "text": "So there's a trade-off between\nmaking m large and not making m large. ",
    "start": "1087640",
    "end": "1095039"
  },
  {
    "text": "If you're dealing with a\nmartingale, then expected value Z sub m is constant\nover all time.",
    "start": "1095040",
    "end": "1101010"
  },
  {
    "text": "It doesn't change. And therefore, you can take\nthis inequality here.",
    "start": "1101010",
    "end": "1107550"
  },
  {
    "text": "You can go to the limit,\nas m goes to infinity. And you wind up with a\nprobability, the sup of Zm,",
    "start": "1107550",
    "end": "1113919"
  },
  {
    "text": "greater than or equal to a, is\nless than or equal to the expected value of the first of\nthose random variables, the",
    "start": "1113920",
    "end": "1120510"
  },
  {
    "text": "expected value of\nZ1 divided by a. So this looks like a very\npowerful inequality.",
    "start": "1120510",
    "end": "1128440"
  },
  {
    "text": "It turns out that I don't know\nmany applications of that. And I don't know why.",
    "start": "1128440",
    "end": "1134070"
  },
  {
    "text": "It seems like it ought\nto be very useful. But I know one reason, which is\nwhat I'm going to show you",
    "start": "1134070",
    "end": "1139930"
  },
  {
    "text": "next, which is how you can\nreally use the submartingale inequality to make it do an\nawful lot of things that you",
    "start": "1139930",
    "end": "1147520"
  },
  {
    "text": "wouldn't imagine that it\ncould do otherwise. ",
    "start": "1147520",
    "end": "1153560"
  },
  {
    "text": "First, you go to the Kolmogorov\nversion of the Chebyshev inequality. This has the same relationship\nto the Kolmogorov",
    "start": "1153560",
    "end": "1162860"
  },
  {
    "text": "submartingale inequality as\nChebyshev has to Markov.",
    "start": "1162860",
    "end": "1168120"
  },
  {
    "text": "Namely, what you do is, instead\nof looking at the random variables Z sub n, you\nlook at the random variable Z",
    "start": "1168120",
    "end": "1176020"
  },
  {
    "text": "sub n squared. And what do we know now?",
    "start": "1176020",
    "end": "1181230"
  },
  {
    "text": "If Z sub n is a martingale or\na submartingale, Z sub n",
    "start": "1181230",
    "end": "1186419"
  },
  {
    "text": "squared is a martingale\nor submartingale also. Namely, well, the only thing\nwe can be sure of is that Z",
    "start": "1186420",
    "end": "1195520"
  },
  {
    "text": "sub n squared is a\nsubmartingale. But if it's a submartingale,\nthen we can apply this",
    "start": "1195520",
    "end": "1202260"
  },
  {
    "text": "inequality again. And what it tells us, in this\ncase, is the probability at",
    "start": "1202260",
    "end": "1207315"
  },
  {
    "text": "the maximum of the magnitudes\nof these random variables. Probably the maximum is greater\nthan or equal to b, is",
    "start": "1207315",
    "end": "1214370"
  },
  {
    "text": "less than or equal to the\nexpected value of Z sub m squared over b squared.",
    "start": "1214370",
    "end": "1220600"
  },
  {
    "text": "So before, just like the Markov\ninequality, the Markov inequality only works for\nnon-negative random variables.",
    "start": "1220600",
    "end": "1228529"
  },
  {
    "text": "You go to the Chebyshev\ninequality, because that works for negative or positive\nrandom variables.",
    "start": "1228530",
    "end": "1235500"
  },
  {
    "text": "So that makes it kind of neat. And then what you have is this\nthing, which goes down as 1",
    "start": "1235500",
    "end": "1243120"
  },
  {
    "text": "over b squared, which looks\na little stronger. But that's not the real reason\nthat you want to use it.",
    "start": "1243120",
    "end": "1250690"
  },
  {
    "text": "Now, this inequality here only\nworks for the first m values",
    "start": "1250690",
    "end": "1262480"
  },
  {
    "text": "of this random variable. What we're usually interested\nin here is what happens as m",
    "start": "1262480",
    "end": "1268970"
  },
  {
    "text": "gets very large. As m gets very large, this\nthing, very often, blows up.",
    "start": "1268970",
    "end": "1276620"
  },
  {
    "text": "So this [INAUDIBLE] does not really do what\nyou would like an inequality to do.",
    "start": "1276620",
    "end": "1282980"
  },
  {
    "text": "So what we're going to do is,\nfirst, we're going to say, if",
    "start": "1282980",
    "end": "1288960"
  },
  {
    "text": "you had this inequality here,\nthen you can lower bound this",
    "start": "1288960",
    "end": "1295950"
  },
  {
    "text": "by taking just a maximum, not\nover 1 up to m, but only over",
    "start": "1295950",
    "end": "1301940"
  },
  {
    "text": "m over 2 up to m. Now why do we want to do that? Well, hold on and you'll see.",
    "start": "1301940",
    "end": "1309250"
  },
  {
    "text": "But anyway this is bigger\nthan, greater than or equal to, this.",
    "start": "1309250",
    "end": "1315580"
  },
  {
    "text": "So what we're going to do now\nis we're going to take this inequality.",
    "start": "1315580",
    "end": "1320640"
  },
  {
    "text": "We're going to use it for m\nequals 2 to the m, for m",
    "start": "1320640",
    "end": "1325930"
  },
  {
    "text": "equals 2 to the k plus 1, m\nequals 2 to the k plus 2, all the way up to infinity.",
    "start": "1325930",
    "end": "1333180"
  },
  {
    "text": "And so we're going to find the\nprobability of the union over",
    "start": "1333180",
    "end": "1338280"
  },
  {
    "text": "j greater than or equal to k of\nthis quantity here, but now just maximized over 2 to the j\nminus 1, less than n, less",
    "start": "1338280",
    "end": "1348020"
  },
  {
    "text": "than or equal 2 to the j. And then the maximum of Z sub\nn, greater than or equal to.",
    "start": "1348020",
    "end": "1353730"
  },
  {
    "text": "And now, for each one of these\nj's here, we'll put in whatever b sub j we want.",
    "start": "1353730",
    "end": "1359460"
  },
  {
    "text": "So the general form of this\ninequality then becomes. We have this term on the left.",
    "start": "1359460",
    "end": "1365860"
  },
  {
    "text": "We use the union bound. And we get this term\non the right. So at this point, we have an\ninequality, which works for",
    "start": "1365860",
    "end": "1375470"
  },
  {
    "text": "all n, instead of just\nfor values smaller than some given amount.",
    "start": "1375470",
    "end": "1381340"
  },
  {
    "text": "So this is sort of a general\ntechnique for taking an inequality, which only works\nup to a certain value, and",
    "start": "1381340",
    "end": "1389130"
  },
  {
    "text": "extending it so it works\nover all values. You have to be pretty careful\nabout how you choose b sub j.",
    "start": "1389130",
    "end": "1397590"
  },
  {
    "text": "Now what we're going\nto do is say, OK. And remember, what is happening\nhere is we started",
    "start": "1397590",
    "end": "1405870"
  },
  {
    "text": "out with a submartingale\nor a martingale. When we take Z n squared, we\nstill have a submartingale.",
    "start": "1405870",
    "end": "1414920"
  },
  {
    "text": "So we can use a submartingale\ninequality, which is what we're doing here. We're using the submartingale\ninequality on Zm squared",
    "start": "1414920",
    "end": "1424210"
  },
  {
    "text": "rather than on Zm. And Zm squared is non-negative, so that works there.",
    "start": "1424210",
    "end": "1430059"
  },
  {
    "text": "Then we go down to this point. We take a union over\nall of these terms.",
    "start": "1430060",
    "end": "1435750"
  },
  {
    "text": "And note what happens. Every n is included in one\nof these terms, every n",
    "start": "1435750",
    "end": "1443430"
  },
  {
    "text": "beyond 2 the k. So if we want to prove something\nabout the limiting",
    "start": "1443430",
    "end": "1449140"
  },
  {
    "text": "values of Z sub n, we have\neverything included there,",
    "start": "1449140",
    "end": "1456860"
  },
  {
    "text": "everything beyond 2 to the k. But as far as the limit is\nconcerned, you don't care about any initial finite set.",
    "start": "1456860",
    "end": "1465340"
  },
  {
    "text": "You care what happens after\nthat initial finite set. So what we have then\n[INAUDIBLE]",
    "start": "1465340",
    "end": "1472940"
  },
  {
    "text": "of these terms, less than\nor equal to this term. When I apply this to a random\nwalk S sub n, S sub n is a",
    "start": "1472940",
    "end": "1483350"
  },
  {
    "text": "submartingale, at this point. The expected value\nof x squared will",
    "start": "1483350",
    "end": "1488790"
  },
  {
    "text": "assume a sigma squared. The expected value now S sub\nn, or Z sub n is we'll call",
    "start": "1488790",
    "end": "1497299"
  },
  {
    "text": "it, is the sum of these n\nIID random variables. So the expected value--",
    "start": "1497300",
    "end": "1503376"
  },
  {
    "text": "AUDIENCE: 10 o'clock. PROFESSOR: The expected value\nof Z to the 2J is just 2 to",
    "start": "1503376",
    "end": "1509430"
  },
  {
    "text": "the J times the expected value\nof x squared, in other words, sigma squared. [INAUDIBLE] just doing this\nfor a 0 mean [INAUDIBLE]",
    "start": "1509430",
    "end": "1517370"
  },
  {
    "text": "variable, because [INAUDIBLE] given an arbitrary\nnon-0 [INAUDIBLE]",
    "start": "1517370",
    "end": "1523590"
  },
  {
    "text": "random variable. You can look at it as to\nmean plus a random variable, which is 0 mean.",
    "start": "1523590",
    "end": "1529400"
  },
  {
    "text": "So that's the same ideas\nwe're using here. So we take this inequality now,\nand I'm going to use for",
    "start": "1529400",
    "end": "1538770"
  },
  {
    "text": "b sub J, 3/2 to the\nJ. Why 3/2 to J? Well you'll see in\njust a second.",
    "start": "1538770",
    "end": "1545430"
  },
  {
    "text": "But when I use 3/2 to the J here\nI get the maximum over S sub n, greater than or equal\nto 3/2 to the J.",
    "start": "1545430",
    "end": "1552830"
  },
  {
    "text": "And over here I get b sub J\nsquared is 9/4 to the J. And",
    "start": "1552830",
    "end": "1560010"
  },
  {
    "text": "here I have 2 the J also.",
    "start": "1560010",
    "end": "1566240"
  },
  {
    "text": "So when I sum this, it winds up\nwith 8/9 to the k times 9",
    "start": "1566240",
    "end": "1571809"
  },
  {
    "text": "sigma squared. So what I have now is something\nwhere, when k gets",
    "start": "1571810",
    "end": "1578040"
  },
  {
    "text": "larger, this term\nis going to 0. And I have something over here,\nwell that doesn't look",
    "start": "1578040",
    "end": "1585160"
  },
  {
    "text": "quite so attractive, but\njust wait a minute. What I'm really interested\nin is not S sub n.",
    "start": "1585160",
    "end": "1591010"
  },
  {
    "text": "But I'm interested in\nS sub n over n. For the strong law of large\nnumbers, I'd like to show that",
    "start": "1591010",
    "end": "1596710"
  },
  {
    "text": "S sub n over n approaches\na limit. And n in this case runs between\n2 to the J minus 1 and",
    "start": "1596710",
    "end": "1604690"
  },
  {
    "text": "2 to the J. So when I put that in here-- we'll see what that amounts\nto in the next slide.",
    "start": "1604690",
    "end": "1613640"
  },
  {
    "text": "For the strong law of large\nnumbers, what our theorem says",
    "start": "1613640",
    "end": "1620130"
  },
  {
    "text": "is that the probability of the\nset of sample points, for which S sub n over n equals 0,\nthat set of sample points has",
    "start": "1620130",
    "end": "1629890"
  },
  {
    "text": "probability y. So the proof of that, if I pick\nup this equation from the",
    "start": "1629890",
    "end": "1636000"
  },
  {
    "text": "previous slide, and when I lower\nbound the left side of",
    "start": "1636000",
    "end": "1644590"
  },
  {
    "text": "this, what I'm going to get, I'm\ngoing to divide by n here.",
    "start": "1644590",
    "end": "1650029"
  },
  {
    "text": "And I'm going to divide by\nsomething a little bit smaller, which is 2 to\nthe J minus 1 here.",
    "start": "1650030",
    "end": "1656060"
  },
  {
    "text": "So I get the maximum of S sub\nn over n, greater than or equal to 2 times 3/4 to the J.",
    "start": "1656060",
    "end": "1662929"
  },
  {
    "text": "Now you see why I picked-- ",
    "start": "1662930",
    "end": "1667940"
  },
  {
    "text": "I think you see at this\npoint why I picked the sub j the way I did. I wanted to pick it t to be\nsmaller than 2 to the J. And I",
    "start": "1667940",
    "end": "1675250"
  },
  {
    "text": "wanted to pick it to be big\nenough that it drove the right hand term to 0.",
    "start": "1675250",
    "end": "1683270"
  },
  {
    "text": "So now we're done really. Because, if I look at this\nexpression here, a sample",
    "start": "1683270",
    "end": "1690179"
  },
  {
    "text": "sequence S sub n of omega,\nthat's not contained in this",
    "start": "1690180",
    "end": "1695290"
  },
  {
    "text": "union, has to approach 0.",
    "start": "1695290",
    "end": "1701810"
  },
  {
    "text": "Because these terms from 2 to\nthe J minus 1 to 2 to the J,",
    "start": "1701810",
    "end": "1707650"
  },
  {
    "text": "in order to be in this set, they\nhave to be greater than or equal to 2 times\n3/4 to the J.",
    "start": "1707650",
    "end": "1714180"
  },
  {
    "text": "As j gets larger and larger,\nthis term goes to 0.",
    "start": "1714180",
    "end": "1719490"
  },
  {
    "text": "So the only terms that exceed\nthat are terms that are arbitrarily small.",
    "start": "1719490",
    "end": "1725180"
  },
  {
    "text": " So the complement of this set is\nthe set of terms for which",
    "start": "1725180",
    "end": "1734840"
  },
  {
    "text": "S sub n over n does\nnot approach 0. But the probability of that is\n8/9 to the k times time some",
    "start": "1734840",
    "end": "1743270"
  },
  {
    "text": "garbage over here. So now it's true for all k.",
    "start": "1743270",
    "end": "1749450"
  },
  {
    "text": "The terms which approach 0,\nnamely the sampled values for",
    "start": "1749450",
    "end": "1760130"
  },
  {
    "text": "which S sub n over n approaches\n0 are all complimentary to this set.",
    "start": "1760130",
    "end": "1767669"
  },
  {
    "text": "So the probability that S sub n\nover omega over n approaches",
    "start": "1767670",
    "end": "1776590"
  },
  {
    "text": "0 is greater than 1 minus\nthis quantity here.",
    "start": "1776590",
    "end": "1783460"
  },
  {
    "text": "That's true for all k. And since it's true for all\nk, this term goes to 0.",
    "start": "1783460",
    "end": "1790650"
  },
  {
    "text": "And the theorem is proven. Now why did I want to\ngo through this.",
    "start": "1790650",
    "end": "1797280"
  },
  {
    "text": "There are perhaps easier ways\nto prove the strong law of large numbers, just assuming\nthat the variance is finite.",
    "start": "1797280",
    "end": "1808990"
  },
  {
    "text": "Why this particular y? Well, if you look at this, it\napplies to much more than just",
    "start": "1808990",
    "end": "1815899"
  },
  {
    "text": "sums of IID random variables. It applies to arbitrary\nmartingales, so long as these",
    "start": "1815900",
    "end": "1823539"
  },
  {
    "text": "conditions are satisfied. It applies to these cases, like\nwhere you have a random",
    "start": "1823540",
    "end": "1829410"
  },
  {
    "text": "variable, which is plus or minus\n1 times some arbitrary random variable.",
    "start": "1829410",
    "end": "1835680"
  },
  {
    "text": "So this gives you sort of a\ngeneral way of proving strong laws of large numbers\nfor strange",
    "start": "1835680",
    "end": "1842390"
  },
  {
    "text": "sequences of random variables.  So that's the reason for\ngoing through this.",
    "start": "1842390",
    "end": "1848610"
  },
  {
    "text": "We now have a way of proving\nstrong laws of large numbers",
    "start": "1848610",
    "end": "1854049"
  },
  {
    "text": "for lots of different kinds of\nmartingales, rather than just for this set of things here.",
    "start": "1854050",
    "end": "1861690"
  },
  {
    "text": "So let's move on back to Markov\nchains, countable or",
    "start": "1861690",
    "end": "1870759"
  },
  {
    "text": "finite state. I'm moving back to chapter three\nand five in the text,",
    "start": "1870760",
    "end": "1876480"
  },
  {
    "text": "mostly chapter five, and trying\nto finish some sort of review of what we've done.",
    "start": "1876480",
    "end": "1882190"
  },
  {
    "text": "When I look back at what we've\ndone, it seems like we've proven an awful lot theorems. So all I can do is talk\nabout the theorems.",
    "start": "1882190",
    "end": "1889360"
  },
  {
    "text": " I should say something, again,\nthis last day, on this last",
    "start": "1889360",
    "end": "1898490"
  },
  {
    "text": "lecture, about why we spend so\nmuch time proving theorems.",
    "start": "1898490",
    "end": "1904309"
  },
  {
    "text": "In other words, we've just\nproven a theorem here. I promised you I would prove a\ntheorem every lecture, along",
    "start": "1904310",
    "end": "1913760"
  },
  {
    "text": "with talking about why they're\nimportant and so on. And most of you are engineers\nor you're scientists in",
    "start": "1913760",
    "end": "1924760"
  },
  {
    "text": "various fields. You're not mathematicians. Why should you be interested\nin all these theorems.",
    "start": "1924760",
    "end": "1931120"
  },
  {
    "text": "Why should you take abstract\ncourses, which look like math courses?",
    "start": "1931120",
    "end": "1937179"
  },
  {
    "text": "And the reason is this kind of\nstuff is more important for you than it is for\nmathematicians.",
    "start": "1937180",
    "end": "1943799"
  },
  {
    "text": "And it's more important for\nyou, because when you're dealing with a real engineering\nor real scientific",
    "start": "1943800",
    "end": "1950190"
  },
  {
    "text": "problem, how do you\ndeal with it? I mean, you have a real\nmess facing you.",
    "start": "1950190",
    "end": "1956650"
  },
  {
    "text": "You spend a lot of time trying\nto understand what that mess is all about.",
    "start": "1956650",
    "end": "1961850"
  },
  {
    "text": "And you don't form a model of\nit, and then apply theorems. What you do is to try\nto understand it.",
    "start": "1961850",
    "end": "1968690"
  },
  {
    "text": "You look at multiple models. When we were looking at\nhypothesis testing, we said",
    "start": "1968690",
    "end": "1975779"
  },
  {
    "text": "we're going to assume a\npriori probabilities. I lied about that\na little bit.",
    "start": "1975780",
    "end": "1981960"
  },
  {
    "text": "We we're not assuming a\npriori probabilities. We we're assuming a class of\nprobability models, each of",
    "start": "1981960",
    "end": "1990340"
  },
  {
    "text": "which had a priori probabilities\nin them. And then we said something\nabout that class of",
    "start": "1990340",
    "end": "1996560"
  },
  {
    "text": "probability models. And saying something about\nthat class of probability",
    "start": "1996560",
    "end": "2001950"
  },
  {
    "text": "models, we were able to say a\ngreat deal more than you can say if you refuse to even think\nabout a model, which",
    "start": "2001950",
    "end": "2008960"
  },
  {
    "text": "doesn't have a priori\nprobabilities in it. So by looking at lots of\ndifferent models, you can",
    "start": "2008960",
    "end": "2015960"
  },
  {
    "text": "understand an enormous number\nof things without really having any one model which\ndescribes the whole",
    "start": "2015960",
    "end": "2022940"
  },
  {
    "text": "situation for you. And that's why we try to prove\ntheorems for models, because",
    "start": "2022940",
    "end": "2030020"
  },
  {
    "text": "then, when you understand lots\nof simple models, you have these complicated physical\nsituations, and",
    "start": "2030020",
    "end": "2036279"
  },
  {
    "text": "you play with them. You play with them by applying\nvarious simple models that you understand to them.",
    "start": "2036280",
    "end": "2042169"
  },
  {
    "text": "And as you do this, you\ngradually understand the physical process better. And that's the way we\ndiscover things.",
    "start": "2042170",
    "end": "2049540"
  },
  {
    "text": "OK, end of lecture. Not end of lecture, but end of\npartial lecture about why you",
    "start": "2049540",
    "end": "2056830"
  },
  {
    "text": "want to learn some\nmathematics.  The first passage time from\nstate i to j, remember, is the",
    "start": "2056830",
    "end": "2066510"
  },
  {
    "text": "smallest n, when you start off\nin state i, at which you get",
    "start": "2066510",
    "end": "2072790"
  },
  {
    "text": "to state j. You start off in state i. You jump from one lily\npad to another.",
    "start": "2072790",
    "end": "2078089"
  },
  {
    "text": "You eventually wind up\nat lily pad number j. And we want to know how long\nit takes you to get to j.",
    "start": "2078090",
    "end": "2084989"
  },
  {
    "text": "That's a random variable,\nobviously. And this Tij, as possibly the\neffective random variable,",
    "start": "2084989",
    "end": "2094619"
  },
  {
    "text": "that has the probability\nmass function. It's the definition of\nwhat this probability",
    "start": "2094620",
    "end": "2100460"
  },
  {
    "text": "mass function is. And it has a distribution\nfunction.",
    "start": "2100460",
    "end": "2105480"
  },
  {
    "text": "And the probability\nmass function-- you probably remember\nhow we derived this.",
    "start": "2105480",
    "end": "2110900"
  },
  {
    "text": "We derived it by sort of\ncrawling up on it, by looking at it, first, for n equals 1,\nin which case it's just a",
    "start": "2110900",
    "end": "2119900"
  },
  {
    "text": "transition probability\nof n equals 2. In which case, it's the\nprobability that you first go",
    "start": "2119900",
    "end": "2126720"
  },
  {
    "text": "to k, and then in n minus\n1 steps, you go to j.",
    "start": "2126720",
    "end": "2131760"
  },
  {
    "text": "But you have to leave j out,\nbecause if you go to j in the first step, you've already\nhad your first passage.",
    "start": "2131760",
    "end": "2141849"
  },
  {
    "text": "so We define a state to be\nrecurrent, if T sub jj is",
    "start": "2141850",
    "end": "2149120"
  },
  {
    "text": "non-defective. And we define it to be\ntransient otherwise.",
    "start": "2149120",
    "end": "2154160"
  },
  {
    "text": "In other words, if it's not\ncertain that you ever get to state j, then you define it\nto be transient, if it's",
    "start": "2154160",
    "end": "2161540"
  },
  {
    "text": "recurrent and it's positive\nrecurrent, if the expected value of T sub jj is\nless than infinity.",
    "start": "2161540",
    "end": "2167740"
  },
  {
    "text": "And it's null recurrent\notherwise. How do we know how\nto analyze this?",
    "start": "2167740",
    "end": "2174190"
  },
  {
    "text": "Well we study renewal\nprocesses. And if you look at the renewal\nprocess where you've got a",
    "start": "2174190",
    "end": "2181300"
  },
  {
    "text": "renewal every time you hit\nstate j, you start out on stage j. The first time you hit state\nj, that's a renewal.",
    "start": "2181300",
    "end": "2188710"
  },
  {
    "text": "The next time you hit state\nj, that's another renewal. ",
    "start": "2188710",
    "end": "2193970"
  },
  {
    "text": "You have a renewal process where\nthe interrenewal time is",
    "start": "2193970",
    "end": "2200150"
  },
  {
    "text": "a random variable, which has\nthe PMF F sub ij event.",
    "start": "2200150",
    "end": "2207920"
  },
  {
    "text": " Excuse me, if you have a renewal\nprocess, if you start",
    "start": "2207920",
    "end": "2220059"
  },
  {
    "text": "in state j, where T sub jj is\nthe amount of time before",
    "start": "2220060",
    "end": "2225440"
  },
  {
    "text": "renewal occurs, from that time\non, you get another renewal",
    "start": "2225440",
    "end": "2230730"
  },
  {
    "text": "with another random variable\nwith the same distribution as Tjj. And F sub ij is the PMF\nof that renewal time.",
    "start": "2230730",
    "end": "2242610"
  },
  {
    "text": "And F sub ij is the distribution\nfunction of it.",
    "start": "2242610",
    "end": "2249620"
  },
  {
    "text": "So then when we define the state\nj as being recurrent, what we're really doing is going\nback to what we know",
    "start": "2249620",
    "end": "2257030"
  },
  {
    "text": "about renewal processes and\nsaying a Markov chain is",
    "start": "2257030",
    "end": "2263820"
  },
  {
    "text": "recurrent if the renewal process\nthat we define for",
    "start": "2263820",
    "end": "2269180"
  },
  {
    "text": "that countable state Markov\nchain has these various",
    "start": "2269180",
    "end": "2275700"
  },
  {
    "text": "properties for this renewal\nrandom variable. For each recurrent j, there's\nan integer renewal counting",
    "start": "2275700",
    "end": "2283640"
  },
  {
    "text": "process N sub jj of t. You start in state j at time t,\nwhich is after t steps of",
    "start": "2283640",
    "end": "2294180"
  },
  {
    "text": "the Markov process. What you're interested is how\nmany times have you hit state",
    "start": "2294180",
    "end": "2299850"
  },
  {
    "text": "j, up until time t. That's the counting process we\ntalk about in renewal theory.",
    "start": "2299850",
    "end": "2307620"
  },
  {
    "text": "So N sub jj of t is the number\nof visits to j starting in j.",
    "start": "2307620",
    "end": "2313550"
  },
  {
    "text": "And it has the interrenewal\ndistribution F sub jj, which is that quantity up there.",
    "start": "2313550",
    "end": "2319520"
  },
  {
    "text": "We have a delayed renewal\ncounting process N sub ij of",
    "start": "2319520",
    "end": "2325250"
  },
  {
    "text": "t, if we count visits\nto j, starting in i.",
    "start": "2325250",
    "end": "2331030"
  },
  {
    "text": "We didn't talk much about\ndelayed renewal processes, except for pointing out that\nwhen you have a delayed",
    "start": "2331030",
    "end": "2337220"
  },
  {
    "text": "renewal process, it\nreally is the same as a renewal processes. It just has some arbitrary\namount of time that's required",
    "start": "2337220",
    "end": "2345160"
  },
  {
    "text": "to get to state j for the\nfirst time and the keep recurrent on. Even if the expected time to\nget to j for first time is",
    "start": "2345160",
    "end": "2352810"
  },
  {
    "text": "infinite, and the expected time\nfor renewals from j to j is finite, you still have this\nsame renewal processes.",
    "start": "2352810",
    "end": "2361990"
  },
  {
    "text": "You can even lose an infinite\namount of time at the beginning, and you amortize\nit over time.",
    "start": "2361990",
    "end": "2368480"
  },
  {
    "text": "Don't ask me why you can\namortize an infinite amount of time over time. But you can.",
    "start": "2368480",
    "end": "2373760"
  },
  {
    "text": " And actually if you read about\ndelayed renewal processes, you",
    "start": "2373760",
    "end": "2380710"
  },
  {
    "text": "see why you actually get that. ",
    "start": "2380710",
    "end": "2386310"
  },
  {
    "text": "So all states in a class are\npositive recurrent, or all are",
    "start": "2386310",
    "end": "2391740"
  },
  {
    "text": "null recurrent, or all\nare transient. We've proved that theorem. It wasn't really a very\nhard theorem to prove.",
    "start": "2391740",
    "end": "2399030"
  },
  {
    "text": "And you can sort of see\nthat it ought to be.",
    "start": "2399030",
    "end": "2404040"
  },
  {
    "text": "Then we define the chain as\nbeing irreducible, if all state pairs communicate.",
    "start": "2404040",
    "end": "2409340"
  },
  {
    "text": "In other words, if for every\npair of states, there's a path that goes from one state\nto the other state.",
    "start": "2409340",
    "end": "2416800"
  },
  {
    "text": " This is intuitively a simple\nidea, if you have a finite",
    "start": "2416800",
    "end": "2422790"
  },
  {
    "text": "state and Markov chain. If you have a countably infinite\nstate Markov chain,",
    "start": "2422790",
    "end": "2429390"
  },
  {
    "text": "it seems to be a little\nmore peculiar. But it really isn't. For a countably infinite state\nan Markov chain every state",
    "start": "2429390",
    "end": "2438610"
  },
  {
    "text": "has a finite number. And you can take every\npair of states.",
    "start": "2438610",
    "end": "2444140"
  },
  {
    "text": "You can identify them. And you can see whether there's\na path going from one to the other. For all of these birth-death\nprocesses we've talked about,",
    "start": "2444140",
    "end": "2453390"
  },
  {
    "text": "I mean, it's obvious whether\nthe states all communicate or not. You just see if there's\nany break in the",
    "start": "2453390",
    "end": "2458940"
  },
  {
    "text": "chain at any point. And it really looks\nlike a chain. It's a node, two transitions,\nanother node, two transitions,",
    "start": "2458940",
    "end": "2467849"
  },
  {
    "text": "another node. And that's just the way chains\nare supposed to work. ",
    "start": "2467850",
    "end": "2475450"
  },
  {
    "text": "An irreducible class might\nbe positive recurrent. It might be null recurrent.",
    "start": "2475450",
    "end": "2481680"
  },
  {
    "text": "Or it might be transient. And we already have seen what\nmakes a state null recurrent",
    "start": "2481680",
    "end": "2488599"
  },
  {
    "text": "or transient. And it's the same thing\nfor the class.",
    "start": "2488600",
    "end": "2493720"
  },
  {
    "start": "2493720",
    "end": "2502910"
  },
  {
    "text": "We started out by saying\na state is either null recurrent, positive recurrent,\nor transient depending on this",
    "start": "2502910",
    "end": "2513390"
  },
  {
    "text": "renewal process associated\nwith it. And now there's this theorem,\nwhich says that if one node in",
    "start": "2513390",
    "end": "2520210"
  },
  {
    "text": "a class of states is positive\nrecurrent, they all are.",
    "start": "2520210",
    "end": "2526880"
  },
  {
    "text": "And you ought to be able\nto sort of see the reason for that. If I have one state which is\npositive recurrent, it means",
    "start": "2526880",
    "end": "2537040"
  },
  {
    "text": "that the expected time to\ngo from this state to this state is finite.",
    "start": "2537040",
    "end": "2542210"
  },
  {
    "text": " Now if I had some other\nstate, I have to go",
    "start": "2542210",
    "end": "2549060"
  },
  {
    "text": "from here to there. I can go through here and\nthen off to there.",
    "start": "2549060",
    "end": "2554230"
  },
  {
    "text": "So the amount of time it takes\nto get to there, and then from there to there, is also finite,\nexpected amount, and",
    "start": "2554230",
    "end": "2561740"
  },
  {
    "text": "the same backwards. So that was the way\nwe proved this. ",
    "start": "2561740",
    "end": "2569220"
  },
  {
    "text": "If we have an irreducible\nMarkov chain-- now this is the theorem you\nreally use all the time.",
    "start": "2569220",
    "end": "2578520"
  },
  {
    "text": "This is sort of says how you\noperate with these things.",
    "start": "2578520",
    "end": "2583580"
  },
  {
    "text": "It says the steady\nstate equations-- they're the equations you've\nused in half the problems",
    "start": "2583580",
    "end": "2590529"
  },
  {
    "text": "you've done with\nMarkov chains-- if these equations have a\nsolution for the pi sub j's,",
    "start": "2590530",
    "end": "2597350"
  },
  {
    "text": "remember the Markov chain is\ndefined in terms of the transition probabilities\nP sub ij.",
    "start": "2597350",
    "end": "2605050"
  },
  {
    "text": "We solve these equations to find\nout what the steady state probabilities pi sub j are.",
    "start": "2605050",
    "end": "2610780"
  },
  {
    "text": "And the theorem says, if you can\nfind the solution to those equations--",
    "start": "2610780",
    "end": "2616230"
  },
  {
    "text": "pi sub j's have to\nadd up to 1-- then the solution is unique.",
    "start": "2616230",
    "end": "2622720"
  },
  {
    "text": "The pi sub j's are equal to 1\nover the mean time to go from",
    "start": "2622720",
    "end": "2630640"
  },
  {
    "text": "that state back to\nthat state again. And what does that mean?",
    "start": "2630640",
    "end": "2638960"
  },
  {
    "text": "What that really gives you is\nnot a way to find pi sub j. It gives you a way to\nfind a T sub jj.",
    "start": "2638960",
    "end": "2648710"
  },
  {
    "text": "Because these equations are more\noften the way that you solve for the steady state\nprobabilities.",
    "start": "2648710",
    "end": "2655760"
  },
  {
    "text": "And then that gives you a way\nto find the mean recurrence time between visits to\nthis given state.",
    "start": "2655760",
    "end": "2662165"
  },
  {
    "text": " And what else does\nthis theorem say?",
    "start": "2662165",
    "end": "2668660"
  },
  {
    "text": "It says if the states are\npositive recurrent, then the steady state equations\nhave a solution.",
    "start": "2668660",
    "end": "2674450"
  },
  {
    "text": "So this is an if and only\nif kind of statement. It relates these equations,\nthese steady state equations,",
    "start": "2674450",
    "end": "2682569"
  },
  {
    "text": "to solutions and says, if\nthese equations have a",
    "start": "2682570",
    "end": "2688820"
  },
  {
    "text": "solution, then in fact\nyou have the steady state equations.",
    "start": "2688820",
    "end": "2694110"
  },
  {
    "text": "They satisfy all these\nrelationships about mean recurrence time.",
    "start": "2694110",
    "end": "2699310"
  },
  {
    "text": "And if the states are positive\nrecurrent, then those equations have a solution.",
    "start": "2699310",
    "end": "2704789"
  },
  {
    "text": "And in the solutions, the pi\nsub j's are all possible. So it's an infinite set of\nequations, so you can't",
    "start": "2704790",
    "end": "2711900"
  },
  {
    "text": "necessarily solve it. But you sort of know everything\nthere is to know about it, at this point.",
    "start": "2711900",
    "end": "2719470"
  },
  {
    "text": "Well, there's one other thing,\nwhen you have a birth-death chain, these equations simplify\na great deal.",
    "start": "2719470",
    "end": "2725670"
  },
  {
    "text": "The counting processes under\npositive recurrence have to satisfy this equation.",
    "start": "2725670",
    "end": "2733280"
  },
  {
    "text": "And my evil twin brother got a\nhold of this and left out the n in the copy that you have.",
    "start": "2733280",
    "end": "2740849"
  },
  {
    "text": "And I spotted it when I looked\nat it just a little bit.",
    "start": "2740850",
    "end": "2746060"
  },
  {
    "text": "He was still sleeping, so\nI've managed to find it. So it's corrected here.",
    "start": "2746060",
    "end": "2751650"
  },
  {
    "text": "And what does that say?  It says, when you have positive\nrecurrence, if you",
    "start": "2751650",
    "end": "2760380"
  },
  {
    "text": "look from 0 out to t, and you\ncount the number of times that you hit state j, that's\na random variable.",
    "start": "2760380",
    "end": "2770600"
  },
  {
    "text": "If you take that and divide by\nn, you look from time 0 out to",
    "start": "2770600",
    "end": "2776980"
  },
  {
    "text": "time N, N sub ij of N, it's\nthe number of times",
    "start": "2776980",
    "end": "2782960"
  },
  {
    "text": "you visit state j. You divide that by N, and\nyou go to the limit.",
    "start": "2782960",
    "end": "2788849"
  },
  {
    "text": "And there's a strong law of\nlarge numbers there, which was a strong law of large numbers\nfor renewal processes, which",
    "start": "2788850",
    "end": "2796650"
  },
  {
    "text": "says that it has a limit\nwith probability 1. And this says that limit\nis pi sub j.",
    "start": "2796650",
    "end": "2803355"
  },
  {
    "text": "And that's sort of\nobvious, again. I mean, visualize\nwhat happens.",
    "start": "2803355",
    "end": "2809340"
  },
  {
    "text": "You start out in state j. For one unit of time,\nyou're in state j.",
    "start": "2809340",
    "end": "2815029"
  },
  {
    "text": "Then you go away from state j,\nand for a long time you're out in the wilderness.",
    "start": "2815030",
    "end": "2820220"
  },
  {
    "text": "And then you finally get\nback to state j again. Think of a renewal reward\nprocess, where you get 1 unit",
    "start": "2820220",
    "end": "2827770"
  },
  {
    "text": "of reward every time you're in\nstate j and 0 reward every time you're not in state j.",
    "start": "2827770",
    "end": "2834359"
  },
  {
    "text": "That means every interrenewal\nperiod, you pick up one unit of reward.",
    "start": "2834360",
    "end": "2839830"
  },
  {
    "start": "2839830",
    "end": "2845000"
  },
  {
    "text": "Well, this is what that says. ",
    "start": "2845000",
    "end": "2852290"
  },
  {
    "text": "It says that the fraction of\nthose visits to state j-- ",
    "start": "2852290",
    "end": "2860800"
  },
  {
    "text": "that out of the total visits in\nthe Markov chain, the ones",
    "start": "2860800",
    "end": "2866230"
  },
  {
    "text": "that go to state j have\nprobability pi sub j. So again this is another\nrelationship with these steady",
    "start": "2866230",
    "end": "2874020"
  },
  {
    "text": "state probabilities. The steady state probabilities\ntell you what these mean recurrence times are.",
    "start": "2874020",
    "end": "2880890"
  },
  {
    "text": "And that tells you\nwhat this is. This, in a sense, is\nthe same as this.",
    "start": "2880890",
    "end": "2888060"
  },
  {
    "text": "Those are just sort of\nthe same results. So there's nothing\nspecial about it.",
    "start": "2888060",
    "end": "2894540"
  },
  {
    "text": "We talked a little bit about\nthe Markov model of age of renewal process for any\ninteger valued renewal",
    "start": "2894540",
    "end": "2902750"
  },
  {
    "text": "process, you can find a Markov\nchain which gives you the age",
    "start": "2902750",
    "end": "2913010"
  },
  {
    "text": "of that process. You visualize being\nin state j. And you visualize being in state\n0, of this Markov model,",
    "start": "2913010",
    "end": "2926789"
  },
  {
    "text": "at the point where you\nhave a renewal. One step later, if you have\nanother renewal, that happens",
    "start": "2926790",
    "end": "2936670"
  },
  {
    "text": "with probability P sub 00, you\ngo back to state 0 again.",
    "start": "2936670",
    "end": "2942069"
  },
  {
    "text": "If you don't have a renewal\nin the next time, you go to state 1. From state 1, you might\ngo to state 2.",
    "start": "2942070",
    "end": "2949830"
  },
  {
    "text": "When you're in state 2, it means\nyou're two time units away from state 0.",
    "start": "2949830",
    "end": "2955750"
  },
  {
    "text": "If you go back to state 0, it\nmeans you have a renewal in",
    "start": "2955750",
    "end": "2961830"
  },
  {
    "text": "three time units. Otherwise you go to state 3. Then you might have a renewal\nand so forth.",
    "start": "2961830",
    "end": "2970000"
  },
  {
    "text": "So for this very simple kind\nof Markov chain, this tells",
    "start": "2970000",
    "end": "2978360"
  },
  {
    "text": "you everything there is to\nknow, in the sense, about integer value renewal\nprocesses.",
    "start": "2978360",
    "end": "2984540"
  },
  {
    "text": "So there's this nice connection\nbetween the two. And it lets you see pretty\neasily about when you have no",
    "start": "2984540",
    "end": "2992510"
  },
  {
    "text": "recurrence. Now we spend a lot of time\ntalking about these birth-death Markov chains.",
    "start": "2992510",
    "end": "2998800"
  },
  {
    "text": "And the easy way to solve for\nbirth-death Markov of chains is to say intuitively that\nbetween any two adjacent",
    "start": "2998800",
    "end": "3010490"
  },
  {
    "text": "states, the number of times\nyou go up has to equal the number of times you go down,\nplus or minus 1.",
    "start": "3010490",
    "end": "3016980"
  },
  {
    "text": "If you start out here and you\nend up here, you're going this way one more time than you've\ngone that way and vice versa.",
    "start": "3016980",
    "end": "3025680"
  },
  {
    "text": "And combining that with the\nsteady state equations that we now have been talking about,\nit must be that the steady",
    "start": "3025680",
    "end": "3034599"
  },
  {
    "text": "state probability\nof pi sub i-- pi sub i times P sub i is the\nprobability of going from",
    "start": "3034600",
    "end": "3042000"
  },
  {
    "text": "state 2 to state 3. It's the probability of being\nin state 2 and making a transition to state 3.",
    "start": "3042000",
    "end": "3049760"
  },
  {
    "text": "This probability here is the\nprobability of being in state 3 and going to state 2.",
    "start": "3049760",
    "end": "3059580"
  },
  {
    "text": "And we're saying that\nasymptotically, as you look over an infinite number of\ntransitions, those two have to",
    "start": "3059580",
    "end": "3065330"
  },
  {
    "text": "be the same. The other way to do it, if you\nlike algebra, is to start out with a steady state equation.",
    "start": "3065330",
    "end": "3071750"
  },
  {
    "text": "And you can derive\nthis right away. I think it's nicer to\nsee intuitively why it has be true.",
    "start": "3071750",
    "end": "3079099"
  },
  {
    "text": "And what that says is if rho sub\ni is equal to P sub i over",
    "start": "3079100",
    "end": "3086850"
  },
  {
    "text": "Q sub i plus 1, P sub i is the\nup transition probability.",
    "start": "3086850",
    "end": "3094200"
  },
  {
    "text": "Q sub i is the down transition\nprobability. Rho sub i is the ratio of the\ntwo state probabilities.",
    "start": "3094200",
    "end": "3105329"
  },
  {
    "text": "And that's equal to this\nequation here. That's just how to calculate\nthese things.",
    "start": "3105330",
    "end": "3112830"
  },
  {
    "text": "And you've done that.  Let's go on to Markov\nprocesses.",
    "start": "3112830",
    "end": "3119185"
  },
  {
    "text": " I have no idea where I'm\ngoing to finish up.",
    "start": "3119185",
    "end": "3124990"
  },
  {
    "text": "I had a lot to do. I better not waste\ntoo much time.",
    "start": "3124990",
    "end": "3131100"
  },
  {
    "text": "Remember what a Markov\nprocess is now. ",
    "start": "3131100",
    "end": "3136650"
  },
  {
    "text": "At least the way we started\nout thinking about, it's a Markov chain along with\na holding time.",
    "start": "3136650",
    "end": "3144079"
  },
  {
    "text": "And each state is\na Markov chain. And the holding times are\nexponential, to be a countable",
    "start": "3144080",
    "end": "3150130"
  },
  {
    "text": "state Markov process. So we can visualize it\nas a sequence of",
    "start": "3150130",
    "end": "3156300"
  },
  {
    "text": "states, X0, X1, X2, X3. And a sequence of holding\ntimes, U1, U2, U3, U4.",
    "start": "3156300",
    "end": "3165240"
  },
  {
    "text": "these are all random\nvariables. And this kind of dependence\ndiagram says what random",
    "start": "3165240",
    "end": "3171110"
  },
  {
    "text": "variables depend on what\nrandom variables. U1, given X0, is independent\nof the rest of the world.",
    "start": "3171110",
    "end": "3179790"
  },
  {
    "text": "U2, given X1, is independent\nof the rest of the world, and so forth.",
    "start": "3179790",
    "end": "3185930"
  },
  {
    "text": "And if you look at this graph\nhere and you visualize the",
    "start": "3185930",
    "end": "3191059"
  },
  {
    "text": "fact that because of Bayes'\nrule, you could go both ways on this.",
    "start": "3191060",
    "end": "3196490"
  },
  {
    "text": "In other words, if this, given\nthis, is independent of",
    "start": "3196490",
    "end": "3202750"
  },
  {
    "text": "everything else, we\ncan go through the same kind of argument.",
    "start": "3202750",
    "end": "3208760"
  },
  {
    "text": "And we can make these arrows\ngo the opposite way.",
    "start": "3208760",
    "end": "3214850"
  },
  {
    "text": "And we can say, if we just\nconsider these states here, we can say that, given X3, U4 is\nindependent of X2 and also",
    "start": "3214850",
    "end": "3226480"
  },
  {
    "text": "independent of U3 and X1\nand U2 and so forth.",
    "start": "3226480",
    "end": "3232130"
  },
  {
    "text": "So if you look at the dependence\ngraph of a Markov chain, which is which states\ndepend on which other states,",
    "start": "3232130",
    "end": "3241230"
  },
  {
    "text": "those arrows there that we have,\nwhich make it easier to see what's going on, you\ncan take them off. You can redraw them in any way\nyou want to and look at the",
    "start": "3241230",
    "end": "3250210"
  },
  {
    "text": "dependencies in the\nopposite way. Now to understand what the\nstate is at any time t,",
    "start": "3250210",
    "end": "3265610"
  },
  {
    "text": "there's an equation\nto do that. It's an equation that\nisn't much help.",
    "start": "3265610",
    "end": "3271700"
  },
  {
    "text": "I think it's more help to look\nat this and to see from this",
    "start": "3271700",
    "end": "3277599"
  },
  {
    "text": "what's going on. You start in some\nstate that's 0. ",
    "start": "3277600",
    "end": "3284670"
  },
  {
    "text": "And starting in state 0, there's\na holding time in U0. The holding time is U1.",
    "start": "3284670",
    "end": "3291680"
  },
  {
    "text": "And you stay in. And the time in U1 is an\nexponential random variable",
    "start": "3291680",
    "end": "3298230"
  },
  {
    "text": "with rate U sub i. That's what this says. So at the end of that holding\ntime, you go from state i to",
    "start": "3298230",
    "end": "3306300"
  },
  {
    "text": "some other state. This is the state you go to. The state you go to is according\nto the mark Markov",
    "start": "3306300",
    "end": "3311950"
  },
  {
    "text": "chain probabilities. And it's state j in this case.",
    "start": "3311950",
    "end": "3317180"
  },
  {
    "text": "You stay in state j until the\nholding time U2, which is a",
    "start": "3317180",
    "end": "3322230"
  },
  {
    "text": "function of j, finishes you up\nat this time and so forth.",
    "start": "3322230",
    "end": "3328490"
  },
  {
    "text": "So if you want to look at what\nstate you're in at a given time, namely pick a time here\nand say what's the state at",
    "start": "3328490",
    "end": "3337060"
  },
  {
    "text": "this time, as a random\nvariable. So what you have to do then is\nyou have to climb your way up",
    "start": "3337060",
    "end": "3344460"
  },
  {
    "text": "from here to there. And you have to talk about the\nvalue of S1, S2, and S3.",
    "start": "3344460",
    "end": "3355150"
  },
  {
    "text": "And those are exponential\nrandom variables. But they're exponential random\nvariables that depend on the",
    "start": "3355150",
    "end": "3361230"
  },
  {
    "text": "state that you're in. So as you're climbing your way\nup and looking at this sample",
    "start": "3361230",
    "end": "3366480"
  },
  {
    "text": "function of the process, you\nhave to look at U1 an X0. X0 defines what U1 is,\nas a random variable.",
    "start": "3366480",
    "end": "3375740"
  },
  {
    "text": "It says that U1 is an\nexponential random variable, with rate U sub i.",
    "start": "3375740",
    "end": "3381190"
  },
  {
    "text": "So you get to here, then you\nhave some holding time here, which is a function of j and\nso forth, the whole way up.",
    "start": "3381190",
    "end": "3389940"
  },
  {
    "text": "Which is why I said that an\nequation for X of t, in terms of these S's is not going to\nhelp you a great deal.",
    "start": "3389940",
    "end": "3397910"
  },
  {
    "text": "Understanding how the process\nis working I think helps you a lot more.",
    "start": "3397910",
    "end": "3404770"
  },
  {
    "text": "We said that there were three\nways to represent a Markov process, which I'm giving\nhere in terms",
    "start": "3404770",
    "end": "3415349"
  },
  {
    "text": "just of Markov chains. The first one-- and the fact that these are all\nfor M/M/1 doesn't make any",
    "start": "3415350",
    "end": "3422430"
  },
  {
    "text": "difference. It's just these three\ngeneral [INAUDIBLE]. One of them is, you look at it\nin terms of the embedded",
    "start": "3422430",
    "end": "3431970"
  },
  {
    "text": "Markov chain. ",
    "start": "3431970",
    "end": "3444030"
  },
  {
    "text": "For this embedded Markov\nchain, the transition probabilities, when you're in\nstate 0 in an M/M/1 queue,",
    "start": "3444030",
    "end": "3451290"
  },
  {
    "text": "what's the next state\nyou go to? Well the only state you\ncan go to is state 1.",
    "start": "3451290",
    "end": "3457140"
  },
  {
    "text": "Because we don't have any\nself transitions. So you go up to state\n1 eventually. From state 1, you can go that\nway, with probability mu over",
    "start": "3457140",
    "end": "3465930"
  },
  {
    "text": "lambda plus mu. Or you can go this way, with\nprobability lambda over lambda",
    "start": "3465930",
    "end": "3471210"
  },
  {
    "text": "plus mu, and so forth\nthe whole way out. The next way of describing it,\nwhich is almost the same, is",
    "start": "3471210",
    "end": "3481140"
  },
  {
    "text": "instead of using the transition\nprobabilities and the embedded chain, you look\ndirectly at the transition",
    "start": "3481140",
    "end": "3488400"
  },
  {
    "text": "rates for the Poisson process. Meaning the transition rates are\nthe new sub i's associated",
    "start": "3488400",
    "end": "3495580"
  },
  {
    "text": "with the different states. When you get in state i, the\namount of time you spend is state i is an exponential\nrandom variable.",
    "start": "3495580",
    "end": "3504810"
  },
  {
    "text": "And when you make a transition,\nyou're either going to go to one state or\nanother state, in this case.",
    "start": "3504810",
    "end": "3512020"
  },
  {
    "text": "In general, you might go to any\none of a number of states. Now if I tell that we start out\nin state one and the next",
    "start": "3512020",
    "end": "3527650"
  },
  {
    "text": "state we go is state 2, now I\nask you what's the expected",
    "start": "3527650",
    "end": "3533430"
  },
  {
    "text": "amount of time that that\ntransition took? What's the answer? ",
    "start": "3533430",
    "end": "3540210"
  },
  {
    "text": "Is it queue 1, 2, or\nis it mu sub 1? ",
    "start": "3540210",
    "end": "3550550"
  },
  {
    "text": "Anybody awake out there? AUDIENCE: Sir, could you\nrepeat the question?",
    "start": "3550550",
    "end": "3556230"
  },
  {
    "text": "PROFESSOR: Yes. The question is, we started\nout in state 1. Given that we started out in\nstate 1 and given that the",
    "start": "3556230",
    "end": "3565730"
  },
  {
    "text": "next state is state 2, what's\nthe amount of time that it",
    "start": "3565730",
    "end": "3570770"
  },
  {
    "text": "takes to go from 1 to 2? It's an exponential\nrandom variable. What's the rate of that\nrandom variable?",
    "start": "3570770",
    "end": "3577722"
  },
  {
    "text": "AUDIENCE: Lambda plus U. PROFESSOR: What? AUDIENCE: Lambda plus U. PROFESSOR: Lambda plus mu?",
    "start": "3577722",
    "end": "3582945"
  },
  {
    "text": "Yes. Lambda plus mu in the\ncase of M/M/1 queue.",
    "start": "3582945",
    "end": "3588990"
  },
  {
    "text": "If you have an arbitrary change,\nwhy the amount of time that it takes is mu sub I. This\nis just back to this old",
    "start": "3588990",
    "end": "3600260"
  },
  {
    "text": "thing about splitting and combining of Poisson processes. ",
    "start": "3600260",
    "end": "3605860"
  },
  {
    "text": "When you have a combined Poisson\nprocess, which is what you have here, when you're in\nstate i, there's a combined",
    "start": "3605860",
    "end": "3613980"
  },
  {
    "text": "Poisson process, which is\nrunning, which says you go right with probability.",
    "start": "3613980",
    "end": "3620599"
  },
  {
    "text": "Lambda, you go left with\nprobability mu for an M/M/1 queue.",
    "start": "3620600",
    "end": "3626119"
  },
  {
    "text": "And you can look at it in terms\nof, first, you see what",
    "start": "3626120",
    "end": "3632640"
  },
  {
    "text": "the next state is. And then you ask how long did\nit take to get there? Or you look at in terms of how\nlong does it take to make a",
    "start": "3632640",
    "end": "3640590"
  },
  {
    "text": "transition and then which\nstate did you go to? And with these combined Poisson\nprocesses, those two",
    "start": "3640590",
    "end": "3646819"
  },
  {
    "text": "questions are independent\nof each other. And if there's one thing you\nremember from all of this,",
    "start": "3646820",
    "end": "3653990"
  },
  {
    "text": "please remember that. Because it's something that\nyou use in almost every",
    "start": "3653990",
    "end": "3660420"
  },
  {
    "text": "problem that you do with Markov chains and Markov processes. It just comes up all the time.",
    "start": "3660420",
    "end": "3667730"
  },
  {
    "text": "This final version here is\nlooking at the same Markov",
    "start": "3667730",
    "end": "3675710"
  },
  {
    "text": "process, but looking at it in\nsample time instead of looking",
    "start": "3675710",
    "end": "3683089"
  },
  {
    "text": "at the embedded queue. Now the important thing here\nis, when you look at it in",
    "start": "3683090",
    "end": "3688110"
  },
  {
    "text": "sample time, you might not\nbe able to do this. Because with this entire\ncannibal state Markov chain,",
    "start": "3688110",
    "end": "3700010"
  },
  {
    "text": "you might not be able to\ndefine these self-loop transition probabilities. Because these numbers\nmight get too large.",
    "start": "3700010",
    "end": "3707220"
  },
  {
    "text": "But for the M/M/1 queue,\nyou can do it. The important thing is that the\nsteady state probabilities",
    "start": "3707220",
    "end": "3713500"
  },
  {
    "text": "you find for these states are\nnot the same as the steady state probabilities you find for\nthe embedded Markov chain.",
    "start": "3713500",
    "end": "3721300"
  },
  {
    "text": "They are in fact the same as the\nsteady state probabilities for the Markov process itself.",
    "start": "3721300",
    "end": "3727329"
  },
  {
    "text": "That's these steady state\nprobabilities are the fraction of time that you spend\nin state j.",
    "start": "3727330",
    "end": "3734980"
  },
  {
    "text": "And this is a sample time\nMarkov process. It is the same fraction of time\nyou spend in state j.",
    "start": "3734980",
    "end": "3742570"
  },
  {
    "text": "Here you have this\nembedded chain. And for example, in the embedded\nchain, the only place",
    "start": "3742570",
    "end": "3748250"
  },
  {
    "text": "you go from state\n0 is state 1. Here from state 0, you\ncan stay in state",
    "start": "3748250",
    "end": "3755119"
  },
  {
    "text": "0 for a long time. Because here the increments\nof time are constant. ",
    "start": "3755120",
    "end": "3764059"
  },
  {
    "text": "We can look at delayed renewal\nreward theorems for the renewal process to see what's\ngoing on here, for the",
    "start": "3764060",
    "end": "3772610"
  },
  {
    "text": "fraction of time we\nspend in state j. We look at that picture\nup there.",
    "start": "3772610",
    "end": "3778570"
  },
  {
    "text": "We start out in state\nj, for example. Same as the renewal reward\nprocess that we had for a",
    "start": "3778570",
    "end": "3784930"
  },
  {
    "text": "Markov chain. We got a reward of 1 for the\namount of time that we",
    "start": "3784930",
    "end": "3790020"
  },
  {
    "text": "stay in state j. After that, we're wandering\naround in the wilderness. We finally come back\nto state j again.",
    "start": "3790020",
    "end": "3797580"
  },
  {
    "text": "We get 1 unit of reward\ntimes the amount of time we spend here. In other words, we're\naccumulating reward at a rate",
    "start": "3797580",
    "end": "3806580"
  },
  {
    "text": "of 1 unit per unit time,\nup to there. So the average reward we get per\nunit time is the expected",
    "start": "3806580",
    "end": "3816690"
  },
  {
    "text": "value of U of j divided by the\nexpected interrenewal time,",
    "start": "3816690",
    "end": "3824790"
  },
  {
    "text": "which is 1 over mu j times the\nexpected time, from one renewal to the next.",
    "start": "3824790",
    "end": "3832230"
  },
  {
    "text": "Which tells us that the fraction\nof time we spend in state j is equal to the fraction\nof transitions that",
    "start": "3832230",
    "end": "3842339"
  },
  {
    "text": "go to state j, divided by the\nrate at which we leave state j, times the expected\nnumber of overall",
    "start": "3842340",
    "end": "3849970"
  },
  {
    "text": "transitions per unit time. This is an important result.",
    "start": "3849970",
    "end": "3855330"
  },
  {
    "text": "Because depending on what M sub\ni is, depending on what the number of transitions per\nunit time is, it really tells",
    "start": "3855330",
    "end": "3863359"
  },
  {
    "text": "you what's going on. Because all of these bizarre\nMarkov processes that we've looked at are bizarre because of\nthe way that this behaves.",
    "start": "3863360",
    "end": "3873519"
  },
  {
    "text": "This can infinite or can be 0. ",
    "start": "3873520",
    "end": "3887080"
  },
  {
    "text": "At this point, we've been\ntalking about the expected",
    "start": "3887080",
    "end": "3894210"
  },
  {
    "text": "number of transitions per unit\ntime as a random variable, as",
    "start": "3894210",
    "end": "3901099"
  },
  {
    "text": "a limit in probability\n1, given that we start in state i. And suddenly, we see that it\ndoesn't depend on i at all.",
    "start": "3901100",
    "end": "3910270"
  },
  {
    "text": "So there is some number, M bar,\nwhich is the expected number of transitions per unit\ntime, which is independent of",
    "start": "3910270",
    "end": "3917900"
  },
  {
    "text": "what state we started in. We call that M M bar instead\nM sub I. And that's this",
    "start": "3917900",
    "end": "3926970"
  },
  {
    "text": "quantity here. And what we get from that it\nis the fraction of time we",
    "start": "3926970",
    "end": "3938600"
  },
  {
    "text": "spend in state j is\nproportional to pi",
    "start": "3938600",
    "end": "3944330"
  },
  {
    "text": "j over mu sub j. But since it has to add up to\n1, we have to divide it by",
    "start": "3944330",
    "end": "3950250"
  },
  {
    "text": "this quantity here. And this quantity here\nis one over--",
    "start": "3950250",
    "end": "3956330"
  },
  {
    "text": "this is the expected number of\ntransitions per unit time. ",
    "start": "3956330",
    "end": "3963190"
  },
  {
    "text": "And if we try to get the pi sub\nj's from P sub j's, the",
    "start": "3963190",
    "end": "3969710"
  },
  {
    "text": "corresponding thing, as we find\nout, the expected number transitions per unit time as\na sum over i, P sub i,",
    "start": "3969710",
    "end": "3978300"
  },
  {
    "text": "times mu sub i. You can play all sorts of games\nwith these equations.",
    "start": "3978300",
    "end": "3983640"
  },
  {
    "text": "And when you do so, all of those\nthings become evident. ",
    "start": "3983640",
    "end": "4004010"
  },
  {
    "text": "I would advise you to just\ncross this equation out.",
    "start": "4004010",
    "end": "4009460"
  },
  {
    "text": "I don't know what\nit came from. But it doesn't mean anything. ",
    "start": "4009460",
    "end": "4017780"
  },
  {
    "text": "We spent a lot of time talking\nabout what happens when the expected number of transitions\nper unit time",
    "start": "4017780",
    "end": "4026770"
  },
  {
    "text": "is either 0 or infinity. We had this case we looked at of\nan M/M/1 type queue, where",
    "start": "4026770",
    "end": "4035870"
  },
  {
    "text": "the server got rattled\nas time went on. And the server got rattled\nwith more and",
    "start": "4035870",
    "end": "4041010"
  },
  {
    "text": "more customers waiting. The customer's got discouraged\nand didn't come in. So we had a process where the\nlonger the queue got, the",
    "start": "4041010",
    "end": "4051090"
  },
  {
    "text": "longer time it took for\nanything to happen. ",
    "start": "4051090",
    "end": "4061600"
  },
  {
    "text": "So that as far as the embedded\nMarkov chain went, everything was fine.",
    "start": "4061600",
    "end": "4067549"
  },
  {
    "text": "But then we looked at the\nprocess itself, the time that it took in each of these higher\norder states was so",
    "start": "4067550",
    "end": "4075140"
  },
  {
    "text": "large, that, as a process,\nit didn't make any sense.",
    "start": "4075140",
    "end": "4080359"
  },
  {
    "text": "So the P sub i's were all 0. The pi sub i's all\nlooked fine. ",
    "start": "4080360",
    "end": "4086640"
  },
  {
    "text": "And the other kind of cases,\nwhere the expected number of transitions per unit time\nbecomes infinite.",
    "start": "4086640",
    "end": "4095070"
  },
  {
    "text": "And that's just the opposite\nkind of case, where, when you get to the higher ordered\nstates, things start happening",
    "start": "4095070",
    "end": "4101170"
  },
  {
    "text": "very, very fast. The higher ordered state you\ngo to, the faster the",
    "start": "4101170",
    "end": "4106309"
  },
  {
    "text": "transitions occur. It's like a small child. ",
    "start": "4106310",
    "end": "4112810"
  },
  {
    "text": "I mean, the more excited the\nsmall child gets, the faster things happen. And the faster things\nhappen, the more",
    "start": "4112810",
    "end": "4118670"
  },
  {
    "text": "excited the child gets. So pretty soon things are\nhappening so fast, the child just collapses.",
    "start": "4118670",
    "end": "4124790"
  },
  {
    "text": "And if you're lucky,\nthe child sleeps. So you can think\nof it that way. ",
    "start": "4124790",
    "end": "4132278"
  },
  {
    "text": "We talked about reversibility. ",
    "start": "4132279",
    "end": "4138989"
  },
  {
    "text": "And reversibility for Markov\nprocesses I think is somewhat easier to see then",
    "start": "4138990",
    "end": "4145170"
  },
  {
    "text": "reversibility for Markov chains. ",
    "start": "4145170",
    "end": "4152790"
  },
  {
    "text": "If you're dealing with a Markov\nprocess, we're sitting in state i for a while. At some time we make\na transition.",
    "start": "4152790",
    "end": "4160380"
  },
  {
    "text": "We go to state j. We sit there for a long time. Then we go to state\nk and so forth.",
    "start": "4160380",
    "end": "4166818"
  },
  {
    "text": "If we try to look at this\nprocess coming back the other way, we see that we're\nin state k.",
    "start": "4166819",
    "end": "4174739"
  },
  {
    "text": "At a certain point, we\nhad a transition. We had a transition\ninto state j.",
    "start": "4174740",
    "end": "4180778"
  },
  {
    "text": "And how long does it\ntake before that transition is over? ",
    "start": "4180779",
    "end": "4186318"
  },
  {
    "text": "We're in state j, so the amount\nof time that it takes is an exponentially distributed\nrandom variable.",
    "start": "4186319",
    "end": "4192509"
  },
  {
    "text": "And it's exponentially\ndistributed with the same amount of time, whether we're\ncoming in this way or whether",
    "start": "4192510",
    "end": "4198360"
  },
  {
    "text": "we're coming in this way. And that's the notion\nof reversibility. It doesn't make any difference\nwhether you look at it from",
    "start": "4198360",
    "end": "4205840"
  },
  {
    "text": "right to left or from\nleft to right. And in this kind of situation,\nif you find the steady state",
    "start": "4205840",
    "end": "4216620"
  },
  {
    "text": "probabilities for these\ntransitions or you find the",
    "start": "4216620",
    "end": "4223120"
  },
  {
    "text": "steady state fraction of time\nyou spend in each state.",
    "start": "4223120",
    "end": "4229180"
  },
  {
    "text": "I mean, we just showed that if\nyou look at this process going backwards, if you define all\nthe probabilities coming",
    "start": "4229180",
    "end": "4235630"
  },
  {
    "text": "backwards, the expected amount\nof time that you spend in",
    "start": "4235630",
    "end": "4240699"
  },
  {
    "text": "state i or the rate for leaving\nstate i is independent of right to left.",
    "start": "4240700",
    "end": "4246010"
  },
  {
    "text": "And a slightly more complicated\nargument says the P sub i's are the same\ngoing right to left.",
    "start": "4246010",
    "end": "4252139"
  },
  {
    "text": "And the fraction of time you\nspend in each state is obviously the same going\nfrom right to left as",
    "start": "4252140",
    "end": "4257830"
  },
  {
    "text": "these limits occur. So that gives you all these\nbizarre conditions for",
    "start": "4257830",
    "end": "4265980"
  },
  {
    "text": "queuing, which are\nvery useful. ",
    "start": "4265980",
    "end": "4275219"
  },
  {
    "text": "I'm not going to say any\nmore about that except the guessing theorem.",
    "start": "4275220",
    "end": "4283280"
  },
  {
    "text": "The guessing theorem says\nsuppose a Markov process is irreducible.",
    "start": "4283280",
    "end": "4288980"
  },
  {
    "text": "You can check pretty\neasily whether it's irreducible or not. You can't necessarily\ncheck very easily whether it's recurrent.",
    "start": "4288980",
    "end": "4296170"
  },
  {
    "text": " And suppose P sub i is a set\nof probabilities that",
    "start": "4296170",
    "end": "4302160"
  },
  {
    "text": "satisfies P sub i times\nQ sub ij equals P sub",
    "start": "4302160",
    "end": "4308530"
  },
  {
    "text": "j times Q sub ji. In other words, this is the\nprobability of being in state",
    "start": "4308530",
    "end": "4316520"
  },
  {
    "text": "i, and the next transition\nis to state j. This is the probability of being\nin state j, and the next",
    "start": "4316520",
    "end": "4323640"
  },
  {
    "text": "transition to state i. This says that if you can find\na set of probabilities which",
    "start": "4323640",
    "end": "4330500"
  },
  {
    "text": "satisfy these equations, and\nif they also satisfy this condition, P sub i, mu sub i,\nless than infinity, then P sub",
    "start": "4330500",
    "end": "4340640"
  },
  {
    "text": "i is greater than 0 for all i. P sub i is a steady state time\naveraged probability state i.",
    "start": "4340640",
    "end": "4346429"
  },
  {
    "text": "The processes is reversible. And the embedded chain is\npositive recurring.",
    "start": "4346430",
    "end": "4351929"
  },
  {
    "text": "So all you have to do is\nsolve those equations. And if you can solve those\nequations, you're done.",
    "start": "4351930",
    "end": "4357760"
  },
  {
    "text": " Everything is fine.",
    "start": "4357760",
    "end": "4363120"
  },
  {
    "text": "You don't have to know anything\nabout reversibility or renewal theory or\nanything else.",
    "start": "4363120",
    "end": "4368330"
  },
  {
    "text": "If you have that theorem,\nyou just solve for those equations. Solve these equations by\nguessing what the solution is,",
    "start": "4368330",
    "end": "4377710"
  },
  {
    "text": "and then you in fact have\na reversible process. ",
    "start": "4377710",
    "end": "4386690"
  },
  {
    "text": "So the useful application of\nthis is that all birth-death processes are reversible if this\nequation is satisfied.",
    "start": "4386690",
    "end": "4395889"
  },
  {
    "text": "And you can immediately\nfind the steady state probabilities of them. ",
    "start": "4395890",
    "end": "4403050"
  },
  {
    "text": "I'm not going to have much\ntime for random walks. ",
    "start": "4403050",
    "end": "4408679"
  },
  {
    "text": "But random walks are\nwhat we've been talking about all term. We just didn't call them random\nwalks until we got to",
    "start": "4408680",
    "end": "4414500"
  },
  {
    "text": "the seventh chapter. But a random walk is a sequence\nof random variables,",
    "start": "4414500",
    "end": "4421140"
  },
  {
    "text": "where each Sn in the sequence\nis a sum of some number of",
    "start": "4421140",
    "end": "4427490"
  },
  {
    "text": "underlying IID random variables,\nX1 up to X sub n. Well we're interested in\nexponential bounds on S sub n",
    "start": "4427490",
    "end": "4436780"
  },
  {
    "text": "for large n. These are known as\nChernoff bounds. We talked about them back\nin chapter one.",
    "start": "4436780",
    "end": "4443560"
  },
  {
    "text": "I'm not going to mention\nthem again now. We're interested in threshold\ncrossings. If you have two thresholds, one\npositive threshold, one",
    "start": "4443560",
    "end": "4451460"
  },
  {
    "text": "negative threshold, you would\nlike to know what's the stopping time when S sub\nn first crosses alpha?",
    "start": "4451460",
    "end": "4460810"
  },
  {
    "text": "Or what's the stopping time when\nit first crosses beta? What's the probability of\ncrossing alpha before you",
    "start": "4460810",
    "end": "4468210"
  },
  {
    "text": "cross beta or vice versa? And what's the distribution of\nthe overshoot, when you pass",
    "start": "4468210",
    "end": "4473760"
  },
  {
    "text": "one of them? So there all those questions. We pretty much talked\nabout the first two.",
    "start": "4473760",
    "end": "4480890"
  },
  {
    "text": "The question of overshoot,\nI think I mentioned this. The text doesn't say\nmuch about it.",
    "start": "4480890",
    "end": "4488460"
  },
  {
    "text": "Overshoot is just a nasty,\nnasty problem. If you ever have to find the\novershoot of something, go",
    "start": "4488460",
    "end": "4495250"
  },
  {
    "text": "look for a computer program to\nsimulate it or something. You're not going to solve\nthe problem very easily.",
    "start": "4495250",
    "end": "4502760"
  },
  {
    "text": "Fowler is the only book I know\nwhich does a reasonable job of",
    "start": "4502760",
    "end": "4508030"
  },
  {
    "text": "trying to solve this. And you have to be\nextraordinarily patient.",
    "start": "4508030",
    "end": "4513170"
  },
  {
    "text": "I mean Fowler does everything\nin the nicest possible way. Or at least he always seem to\ndo everything in the nicest",
    "start": "4513170",
    "end": "4519329"
  },
  {
    "text": "possible way. Most textbooks you look at,\nafter you understand the subject, you look at and you\nsay, oh, he should have done",
    "start": "4519330",
    "end": "4527110"
  },
  {
    "text": "it this way. I've never had that experience\nwith Fowler at all. Always, I look at it.",
    "start": "4527110",
    "end": "4533530"
  },
  {
    "text": "I say, oh, there's an\neasier way to do it. I try to do it the easier way. And then I find something's\nwrong with it.",
    "start": "4533530",
    "end": "4538800"
  },
  {
    "text": "And then I go back and say,\nah, I got to do it the way Fowler did it. So if you're serious about this\nfield and you don't have",
    "start": "4538800",
    "end": "4548590"
  },
  {
    "text": "a copy of this very\nold book, get it, because it's solid gold. ",
    "start": "4548590",
    "end": "4561460"
  },
  {
    "text": "Suppose a random variable has a\nmoment generating function, expected value of E to\nthe zr over some",
    "start": "4561460",
    "end": "4571470"
  },
  {
    "text": "positive region of r. And suppose it has a mean\nwhich is negative.",
    "start": "4571470",
    "end": "4577270"
  },
  {
    "text": "The Chernoff bound says that for\nany alpha greater than 0",
    "start": "4577270",
    "end": "4582570"
  },
  {
    "text": "and any r in 0 to r plus, the\nprobability that Z is greater",
    "start": "4582570",
    "end": "4587860"
  },
  {
    "text": "than or equal to alpha is\nless than or equal to this quantity here. You remember, we derived this.",
    "start": "4587860",
    "end": "4593369"
  },
  {
    "text": "The derivation is very simple. It's a an obvious result.",
    "start": "4593370",
    "end": "4599660"
  },
  {
    "text": "It's a little strange. Because this says that for\nthis random variable it's",
    "start": "4599660",
    "end": "4608130"
  },
  {
    "text": "complimentary distribution\nfunction has to go down as e to the minus r alpha.",
    "start": "4608130",
    "end": "4615270"
  },
  {
    "text": "Now all random variables can't\ngo down exponentially as e to the minus r alpha.",
    "start": "4615270",
    "end": "4621260"
  },
  {
    "text": "The reason for this is that\nthese moment generating",
    "start": "4621260",
    "end": "4626309"
  },
  {
    "text": "functions down exist\nfor all alpha. So what it's really saying is\nwhere it exists, it goes down",
    "start": "4626310",
    "end": "4634150"
  },
  {
    "text": "with alpha as e to the\nminus r alpha. We then define the\nsemi-invariant moment",
    "start": "4634150",
    "end": "4640160"
  },
  {
    "text": "generating function. And then a more convenient way\nof stating the Chernoff bound",
    "start": "4640160",
    "end": "4646000"
  },
  {
    "text": "was in this way. You look here. And you say, for a fixed value\nof n here, this probability of",
    "start": "4646000",
    "end": "4655800"
  },
  {
    "text": "S sub n is greater than or equal\nto n a, is something which is going down\nexponentially with n.",
    "start": "4655800",
    "end": "4662540"
  },
  {
    "text": "And if you optimize over\nr, this bound is exponentially tight. In other words, if you try to\nreplace this with anything",
    "start": "4662540",
    "end": "4674110"
  },
  {
    "text": "smaller, namely which goes down\nfaster, than for large enough n, the bound\nwill be false.",
    "start": "4674110",
    "end": "4681090"
  },
  {
    "text": "So this is the tightest bound\nyou can get when you optimize it over r.",
    "start": "4681090",
    "end": "4687170"
  },
  {
    "text": "So its exponential in n. Mostly we wanted to use it\nfor threshold crossings.",
    "start": "4687170",
    "end": "4693970"
  },
  {
    "text": "And for threshold crossings, we\nwould like to look at it in",
    "start": "4693970",
    "end": "4700630"
  },
  {
    "text": "another way. And we dealt with this\ngraphically.",
    "start": "4700630",
    "end": "4706679"
  },
  {
    "text": "Probability of Sn greater\nthan or equal to alpha. Now what we want to do is\nhold alpha constant.",
    "start": "4706680",
    "end": "4713240"
  },
  {
    "text": "Alpha is some threshold\nup there. We want to ask, what's the\nprobability that after n",
    "start": "4713240",
    "end": "4719060"
  },
  {
    "text": "trials, we're sitting\nabove alpha? And we'd like to try\nto solve that for different values of n.",
    "start": "4719060",
    "end": "4725580"
  },
  {
    "text": "The Chernoff bound, in this\ncase, this quantity here is this intercept here.",
    "start": "4725580",
    "end": "4732470"
  },
  {
    "text": "You take the semi-invariant\nmoment generating function as convex. You draw this curve.",
    "start": "4732470",
    "end": "4739640"
  },
  {
    "text": "You take a tangent of\nslope alpha over n. And you see where\nit hits here.",
    "start": "4739640",
    "end": "4746420"
  },
  {
    "text": "And this is the exponent\nthat you have. This is a negative exponent. As you very n, this tilts\naround on this curve.",
    "start": "4746420",
    "end": "4756400"
  },
  {
    "text": "And it comes in to this point. It goes back out again.",
    "start": "4756400",
    "end": "4762230"
  },
  {
    "text": "That's what happens to it. And that smallest exponent, as\nyou vary n, is the most likely",
    "start": "4762230",
    "end": "4771190"
  },
  {
    "text": "time at which you're going\nto cross that threshold. And what we found,\nfrom looking at",
    "start": "4771190",
    "end": "4777930"
  },
  {
    "text": "Wald's equality is that-- let me go on, because we're\nrunning out of time.",
    "start": "4777930",
    "end": "4786000"
  },
  {
    "text": " Wald's identity for two\nthresholds says this.",
    "start": "4786000",
    "end": "4794390"
  },
  {
    "text": "And the corollary says, if the\nunderlying random variable is less than 0, and if the\nr at which the--",
    "start": "4794390",
    "end": "4806580"
  },
  {
    "text": "the second solution of\ngamma of r equals 0. You have this convex curve.",
    "start": "4806580",
    "end": "4812000"
  },
  {
    "text": "Gamma is always equal to 0. There's some other value of r,\nfor which gamma is equal to 0.",
    "start": "4812000",
    "end": "4819380"
  },
  {
    "text": "And that's r star. And this says that the\nprobability that we have",
    "start": "4819380",
    "end": "4825080"
  },
  {
    "text": "crossed alpha at time j, where\nj is the time of first crossing, is less than\nor equal e to the",
    "start": "4825080",
    "end": "4832310"
  },
  {
    "text": "minus alpha r star. This bound is tight also. And that's a very nice result.",
    "start": "4832310",
    "end": "4838460"
  },
  {
    "text": "Because that just says that all\nyou got do is find r star. And that tells you what the\nprobability of crossing a",
    "start": "4838460",
    "end": "4845810"
  },
  {
    "text": "threshold is. And it's a very tight bound\nif alpha is very large. It doesn't make any difference\nwhat the negative threshold",
    "start": "4845810",
    "end": "4853780"
  },
  {
    "text": "is, or whether it's\nthere or not. This tells you the thing\nyou want to know.",
    "start": "4853780",
    "end": "4859660"
  },
  {
    "text": "I think I'm going to stop at\nthat point, because I have been sort of rushing to\nget to this point.",
    "start": "4859660",
    "end": "4868330"
  },
  {
    "text": "And it doesn't do any good\nto keep rushing. So thank you all for being\naround all term.",
    "start": "4868330",
    "end": "4876610"
  },
  {
    "text": "I appreciate it. Thank you. ",
    "start": "4876610",
    "end": "4886422"
  }
]