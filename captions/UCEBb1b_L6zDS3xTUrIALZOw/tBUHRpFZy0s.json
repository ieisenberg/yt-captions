[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6910"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6910",
    "end": "13460"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at",
    "start": "13460",
    "end": "19290"
  },
  {
    "text": "ocw.mit.edu. JOHN TSITSIKLIS: And we're going\nto continue today with our discussion of classical\nstatistics.",
    "start": "19290",
    "end": "26820"
  },
  {
    "text": "We'll start with a quick review\nof what we discussed last time, and then talk about\ntwo topics that cover a lot of",
    "start": "26820",
    "end": "34680"
  },
  {
    "text": "statistics that are happening\nin the real world. So two basic methods. One is the method of linear\nregression, and the other one",
    "start": "34680",
    "end": "43730"
  },
  {
    "text": "is the basic methods and\ntools for how to do hypothesis testing.",
    "start": "43730",
    "end": "49540"
  },
  {
    "text": "OK, so these two are topics\nthat any scientifically literate person should\nknow something about.",
    "start": "49540",
    "end": "57170"
  },
  {
    "text": "So we're going to introduce\nthe basic ideas and concepts involved. So in classical statistics we\nbasically have essentially a",
    "start": "57170",
    "end": "67580"
  },
  {
    "text": "family of possible models\nabout the world. So the world is the random\nvariable that we observe, and",
    "start": "67580",
    "end": "75190"
  },
  {
    "text": "we have a model for it, but\nactually not just one model, several candidate models.",
    "start": "75190",
    "end": "80960"
  },
  {
    "text": "And each candidate model\ncorresponds to a different value of a parameter theta\nthat we do not know.",
    "start": "80960",
    "end": "88070"
  },
  {
    "text": "So in contrast to Bayesian\nstatistics, this theta is assumed to be a constant\nthat we do not know.",
    "start": "88070",
    "end": "95540"
  },
  {
    "text": "It is not modeled as a random\nvariable, there's no probabilities associated\nwith theta. We only have probabilities\nabout the X's.",
    "start": "95540",
    "end": "103380"
  },
  {
    "text": "So in this context what is a\nreasonable way of choosing a value for the parameter?",
    "start": "103380",
    "end": "109350"
  },
  {
    "text": "One general approach is the\nmaximum likelihood approach, which chooses the\ntheta for which",
    "start": "109350",
    "end": "116090"
  },
  {
    "text": "this quantity is largest. So what does that mean\nintuitively? I'm trying to find the value of\ntheta under which the data",
    "start": "116090",
    "end": "124550"
  },
  {
    "text": "that I observe are most likely\nto have occurred. So is the thinking is\nessentially as follows.",
    "start": "124550",
    "end": "131470"
  },
  {
    "text": "Let's say I have to choose\nbetween two choices of theta. Under this theta the\nX that I observed",
    "start": "131470",
    "end": "136520"
  },
  {
    "text": "would be very unlikely. Under that theta the X that I\nobserved would have a decent probability of occurring.",
    "start": "136520",
    "end": "142830"
  },
  {
    "text": "So I chose the latter as\nmy estimate of theta.",
    "start": "142830",
    "end": "148340"
  },
  {
    "text": "It's interesting to do the\ncomparison with the Bayesian approach which we did discuss\nlast time, in the Bayesian",
    "start": "148340",
    "end": "154110"
  },
  {
    "text": "approach we also maximize over\ntheta, but we maximize a quantity in which the relation\nbetween X's and thetas run the",
    "start": "154110",
    "end": "163219"
  },
  {
    "text": "opposite way. Here in the Bayesian world,\nTheta is a random variable. So it has a distribution.",
    "start": "163220",
    "end": "168980"
  },
  {
    "text": "Once we observe the data, it has\na posterior distribution, and we find the value of Theta,\nwhich is most likely",
    "start": "168980",
    "end": "176480"
  },
  {
    "text": "under the posterior\ndistribution. As we discussed last time when\nyou do this maximization now",
    "start": "176480",
    "end": "183090"
  },
  {
    "text": "the posterior distribution is\ngiven by this expression. The denominator doesn't matter,\nand if you were to",
    "start": "183090",
    "end": "189760"
  },
  {
    "text": "take a prior, which is flat-- that is a constant independent\nof Theta, then that",
    "start": "189760",
    "end": "196209"
  },
  {
    "text": "term would go away. And syntactically,\nat least, the two approaches look the same.",
    "start": "196210",
    "end": "201970"
  },
  {
    "text": "So syntactically, or formally,\nmaximum likelihood estimation",
    "start": "201970",
    "end": "208170"
  },
  {
    "text": "is the same as Bayesian\nestimation in which you assume a prior which is flat, so that\nall possible values of Theta",
    "start": "208170",
    "end": "216090"
  },
  {
    "text": "are equally likely. Philosophically, however,\nthey're very different things. Here I'm picking the most\nlikely value of Theta.",
    "start": "216090",
    "end": "224150"
  },
  {
    "text": "Here I'm picking the value of\nTheta under which the observed data would have been more\nlikely to occur.",
    "start": "224150",
    "end": "231050"
  },
  {
    "text": "So maximum likelihood estimation\nis a general purpose method, so it's applied\nall over the place in",
    "start": "231050",
    "end": "237819"
  },
  {
    "text": "many, many different types\nof estimation problems. There is a special kind of\nestimation problem in which",
    "start": "237820",
    "end": "245100"
  },
  {
    "text": "you may forget about maximum\nlikelihood estimation, and come up with an estimate in\na straightforward way.",
    "start": "245100",
    "end": "252700"
  },
  {
    "text": "And this is the case where\nyou're trying to estimate the mean of the distribution of X,\nwhere X is a random variable.",
    "start": "252700",
    "end": "262389"
  },
  {
    "text": "You observe several independent\nidentically distributed random variables\nX1 up to Xn.",
    "start": "262390",
    "end": "270020"
  },
  {
    "text": "All of them have the same\ndistribution as this X. So they have a common mean. We do not know the mean we\nwant to estimate it.",
    "start": "270020",
    "end": "277020"
  },
  {
    "text": "What is more natural than just\ntaking the average of the values that we have observed?",
    "start": "277020",
    "end": "282470"
  },
  {
    "text": "So you generate lots of X's,\ntake the average of them, and you expect that this is going to\nbe a reasonable estimate of",
    "start": "282470",
    "end": "290560"
  },
  {
    "text": "the true mean of that\nrandom variable. And indeed we know from the weak\nlaw of large numbers that",
    "start": "290560",
    "end": "296290"
  },
  {
    "text": "this estimate converges in\nprobability to the true mean of the random variable.",
    "start": "296290",
    "end": "302680"
  },
  {
    "text": "The other thing that we talked\nabout last time is that besides giving a point estimate\nwe may want to also",
    "start": "302680",
    "end": "307770"
  },
  {
    "text": "give an interval that tells us\nsomething about where we might",
    "start": "307770",
    "end": "313530"
  },
  {
    "text": "believe theta to lie. And 1-alpha confidence interval\nis in interval",
    "start": "313530",
    "end": "321950"
  },
  {
    "text": "generated based on the data. So it's an interval from this\nvalue to that value. These values are written with\ncapital letters because",
    "start": "321950",
    "end": "330120"
  },
  {
    "text": "they're random, because they\ndepend on the data that we have seen. And this gives us an interval,\nand we would like this",
    "start": "330120",
    "end": "336740"
  },
  {
    "text": "interval to have the property\nthat theta is inside that interval with high\nprobability.",
    "start": "336740",
    "end": "342830"
  },
  {
    "text": "So typically we would take\n1-alpha to be a quantity such as 95% for example.",
    "start": "342830",
    "end": "349780"
  },
  {
    "text": "In which case we have a 95%\nconfidence interval. As we discussed last time it's\nimportant to have the right",
    "start": "349780",
    "end": "356980"
  },
  {
    "text": "interpretation of what's\n95% means. What it does not mean\nis the following--",
    "start": "356980",
    "end": "364640"
  },
  {
    "text": "the unknown value has 95%\npercent probability of being",
    "start": "364640",
    "end": "369800"
  },
  {
    "text": "in the interval that\nwe have generated. That's because the unknown\nvalue is not a random variable, it's a constant.",
    "start": "369800",
    "end": "375909"
  },
  {
    "text": "Once we generate the interval\neither it's inside or it's outside, but there's no\nprobabilities involved.",
    "start": "375910",
    "end": "382500"
  },
  {
    "text": "Rather the probabilities are\nto be interpreted over the random interval itself.",
    "start": "382500",
    "end": "388590"
  },
  {
    "text": "What a statement like this\nsays is that if I have a procedure for generating 95%\nconfidence intervals, then",
    "start": "388590",
    "end": "397060"
  },
  {
    "text": "whenever I use that procedure\nI'm going to get a random interval, and it's going to\nhave 95% probability of",
    "start": "397060",
    "end": "404259"
  },
  {
    "text": "capturing the true\nvalue of theta. So most of the time when I use\nthis particular procedure for",
    "start": "404260",
    "end": "413009"
  },
  {
    "text": "generating confidence intervals\nthe true theta will happen to lie inside that\nconfidence interval with",
    "start": "413010",
    "end": "419440"
  },
  {
    "text": "probability 95%. So the randomness in this\nstatement is with respect to my confidence interval, it's\nnot with respect to theta,",
    "start": "419440",
    "end": "429190"
  },
  {
    "text": "because theta is not random. How does one construct\nconfidence intervals?",
    "start": "429190",
    "end": "434710"
  },
  {
    "text": "There's various ways of going\nabout it, but in the case where we're dealing with the\nestimation of the mean of a",
    "start": "434710",
    "end": "440330"
  },
  {
    "text": "random variable doing this is\nstraightforward using the central limit theorem.",
    "start": "440330",
    "end": "445680"
  },
  {
    "text": "Basically we take our estimated\nmean, that's the",
    "start": "445680",
    "end": "451440"
  },
  {
    "text": "sample mean, and we take a\nsymmetric interval to the left and to the right of\nthe sample mean.",
    "start": "451440",
    "end": "458220"
  },
  {
    "text": "And we choose the width of that\ninterval by looking at the normal tables.",
    "start": "458220",
    "end": "463680"
  },
  {
    "text": "So if this quantity, 1-alpha is\n95% percent, we're going to",
    "start": "463680",
    "end": "470180"
  },
  {
    "text": "look at the 97.5 percentile of\nthe normal distribution.",
    "start": "470180",
    "end": "475789"
  },
  {
    "text": "Find the constant number that\ncorresponds to that value from the normal tables, and construct\nthe confidence",
    "start": "475790",
    "end": "482790"
  },
  {
    "text": "intervals according\nto this formula. So that gives you a pretty\nmechanical way of going about",
    "start": "482790",
    "end": "490810"
  },
  {
    "text": "constructing confidence\nintervals when you're estimating the sample mean. So constructing confidence\nintervals in this way involves",
    "start": "490810",
    "end": "498650"
  },
  {
    "text": "an approximation. The approximation is the\ncentral limit theorem. We are pretending that\nthe sample mean is a",
    "start": "498650",
    "end": "504490"
  },
  {
    "text": "normal random variable. Which is, more or less,\nright when n is large.",
    "start": "504490",
    "end": "510110"
  },
  {
    "text": "That's what the central limit\ntheorem tells us. And sometimes we may need to\ndo some extra approximation",
    "start": "510110",
    "end": "516429"
  },
  {
    "text": "work, because quite often\nwe do not know the true value of sigma. So we need to do some work\neither to estimate",
    "start": "516429",
    "end": "523559"
  },
  {
    "text": "sigma from the data. So sigma is, of course, the\nstandard deviation of the X's. We may want to estimate it from\nthe data, or we may have",
    "start": "523559",
    "end": "531410"
  },
  {
    "text": "an upper bound on sigma, and we\njust use that upper bound. ",
    "start": "531410",
    "end": "537430"
  },
  {
    "text": "So now let's move on\nto a new topic.",
    "start": "537430",
    "end": "542520"
  },
  {
    "text": "A lot of statistics in the\nreal world are of the",
    "start": "542520",
    "end": "549420"
  },
  {
    "text": "following flavor. So suppose that X is the SAT\nscore of a student in high",
    "start": "549420",
    "end": "556820"
  },
  {
    "text": "school, and Y is the MIT GPA\nof that same student.",
    "start": "556820",
    "end": "563620"
  },
  {
    "text": "So you expect that there is a\nrelation between these two. So you go and collect data for\ndifferent students, and you",
    "start": "563620",
    "end": "571240"
  },
  {
    "text": "record for a typical student\nthis would be their SAT score, that could be their MIT GPA.",
    "start": "571240",
    "end": "577699"
  },
  {
    "text": "And you plot all this data\non an (X,Y) diagram.",
    "start": "577700",
    "end": "583720"
  },
  {
    "text": "Now it's reasonable to believe\nthat there is some systematic relation between the two.",
    "start": "583720",
    "end": "589940"
  },
  {
    "text": "So people who had higher SAT\nscores in high school may have higher GPA in college.",
    "start": "589940",
    "end": "597110"
  },
  {
    "text": "Well that may or may\nnot be true. You want to construct a model of\nthis kind, and see to what",
    "start": "597110",
    "end": "605270"
  },
  {
    "text": "extent a relation of\nthis type is true. So you might hypothesize that\nthe real world is described by",
    "start": "605270",
    "end": "615560"
  },
  {
    "text": "a model of this kind. That there is a linear relation\nbetween the SAT",
    "start": "615560",
    "end": "622730"
  },
  {
    "text": "score, and the college GPA. So it's a linear relation with\nsome parameters, theta0 and",
    "start": "622730",
    "end": "630560"
  },
  {
    "text": "theta1 that we do not know. So we assume a linear relation\nfor the data, and depending on",
    "start": "630560",
    "end": "637460"
  },
  {
    "text": "the choices of theta0 and theta1\nit could be a different line through those data.",
    "start": "637460",
    "end": "643530"
  },
  {
    "text": "Now we would like to find the\nbest model of this kind to explain the data.",
    "start": "643530",
    "end": "649230"
  },
  {
    "text": "Of course there's going\nto be some randomness. So in general it's going to be\nimpossible to find a line that",
    "start": "649230",
    "end": "655370"
  },
  {
    "text": "goes through all of\nthe data points. So let's try to find the best\nline that comes closest to",
    "start": "655370",
    "end": "664020"
  },
  {
    "text": "explaining those data. And here's how we go about it. Suppose we try some particular\nvalues of theta0 and theta1.",
    "start": "664020",
    "end": "673100"
  },
  {
    "text": "These give us a certain line. Given that line, we can\nmake predictions.",
    "start": "673100",
    "end": "680760"
  },
  {
    "text": "For a student who had this x,\nthe model that we have would predict that y would\nbe this value.",
    "start": "680760",
    "end": "687579"
  },
  {
    "text": "The actual y is something else,\nand so this quantity is the error that our model would\nmake in predicting the y of",
    "start": "687580",
    "end": "697660"
  },
  {
    "text": "that particular student. We would like to choose a line\nfor which the predictions are",
    "start": "697660",
    "end": "703350"
  },
  {
    "text": "as good as possible. And what do we mean by\nas good as possible? As our criteria we're going\nto take the following.",
    "start": "703350",
    "end": "711149"
  },
  {
    "text": "We are going to look at the\nprediction error that our model makes for each\nparticular student.",
    "start": "711150",
    "end": "716310"
  },
  {
    "text": "Take the square of that, and\nthen add them up over all of our data points.",
    "start": "716310",
    "end": "722580"
  },
  {
    "text": "So what we're looking at is\nthe sum of this quantity squared, that quantity squared,\nthat quantity",
    "start": "722580",
    "end": "728270"
  },
  {
    "text": "squared, and so on. We add all of these squares, and\nwe would like to find the line for which the sum of\nthese squared prediction",
    "start": "728270",
    "end": "737500"
  },
  {
    "text": "errors are as small\nas possible. So that's the procedure.",
    "start": "737500",
    "end": "743949"
  },
  {
    "text": "We have our data, the\nX's and the Y's. And we're going to find theta's\nthe best model of this",
    "start": "743950",
    "end": "751339"
  },
  {
    "text": "type, the best possible model,\nby minimizing this sum of squared errors.",
    "start": "751340",
    "end": "758010"
  },
  {
    "text": "So that's a method that one\ncould pull out of the hat and say OK, that's how I'm going\nto build my model.",
    "start": "758010",
    "end": "764120"
  },
  {
    "text": "And it sounds pretty\nreasonable. And it sounds pretty reasonable\neven if you don't",
    "start": "764120",
    "end": "769529"
  },
  {
    "text": "know anything about\nprobability. But does it have some\nprobabilistic justification?",
    "start": "769530",
    "end": "775339"
  },
  {
    "text": "It turns out that yes, you can\nmotivate this method with probabilistic considerations\nunder certain assumptions.",
    "start": "775340",
    "end": "783100"
  },
  {
    "text": "So let's make a probabilistic\nmodel that's going to lead us to these particular way of\nestimating the parameters.",
    "start": "783100",
    "end": "790600"
  },
  {
    "text": "So here's a probabilistic\nmodel. I pick a student who had\na specific SAT score.",
    "start": "790600",
    "end": "798089"
  },
  {
    "text": "And that could be done at\nrandom, but also could be done in a systematic way. That is, I pick a student who\nhad an SAT of 600, a student",
    "start": "798090",
    "end": "805240"
  },
  {
    "text": "of 610 all the way to 1,400\nor 1,600, whatever the",
    "start": "805240",
    "end": "813170"
  },
  {
    "text": "right number is. I pick all those students. And I assume that for a student\nof this kind there's a",
    "start": "813170",
    "end": "820370"
  },
  {
    "text": "true model that tells me that\ntheir GPA is going to be a random variable, which is\nsomething predicted by their",
    "start": "820370",
    "end": "828580"
  },
  {
    "text": "SAT score plus some randomness,\nsome random noise. And I model that random noise\nby independent normal random",
    "start": "828580",
    "end": "836400"
  },
  {
    "text": "variables with 0 mean and\na certain variance. So this is a specific\nprobabilistic model, and now I",
    "start": "836400",
    "end": "844470"
  },
  {
    "text": "can think about doing maximum\nlikelihood estimation for this particular model.",
    "start": "844470",
    "end": "850980"
  },
  {
    "text": "So to do maximum likelihood\nestimation here I need to write down the likelihood of the\ny's that I have observed.",
    "start": "850980",
    "end": "859830"
  },
  {
    "text": "What's the likelihood of the\ny's that I have observed? Well, a particular w has a\nlikelihood of the form e to",
    "start": "859830",
    "end": "868425"
  },
  {
    "text": "the minus w squared over\n(2 sigma-squared). That's the likelihood\nof a particular w.",
    "start": "868425",
    "end": "877070"
  },
  {
    "text": "The probability, or the\nlikelihood of observing a particular value of y, that's\nthe same as the likelihood",
    "start": "877070",
    "end": "883990"
  },
  {
    "text": "that w takes a value of y\nminus this, minus that.",
    "start": "883990",
    "end": "889020"
  },
  {
    "text": "So the likelihood of the\ny's is of this form. Think of this as just being\nthe w_i-squared.",
    "start": "889020",
    "end": "897360"
  },
  {
    "text": "So this is the density -- and if we have multiple data you\nmultiply the likelihoods",
    "start": "897360",
    "end": "906060"
  },
  {
    "text": "of the different y's. So you have to write something\nlike this.",
    "start": "906060",
    "end": "912089"
  },
  {
    "text": "Since the w's are independent\nthat means that the y's are also independent.",
    "start": "912090",
    "end": "917910"
  },
  {
    "text": "The likelihood of a y vector\nis the product of the likelihoods of the\nindividual y's.",
    "start": "917910",
    "end": "924240"
  },
  {
    "text": "The likelihood of every\nindividual y is of this form. Where w is y_i minus these\ntwo quantities.",
    "start": "924240",
    "end": "933050"
  },
  {
    "text": "So this is the form that the\nlikelihood function is going to take under this\nparticular model.",
    "start": "933050",
    "end": "938880"
  },
  {
    "text": "And under the maximum likelihood\nmethodology we want to maximize this quantity with\nrespect to theta0 and theta1.",
    "start": "938880",
    "end": "949170"
  },
  {
    "text": "Now to do this maximization you\nmight as well consider the",
    "start": "949170",
    "end": "956930"
  },
  {
    "text": "logarithm and maximize the\nlogarithm, which is just the exponent up here.",
    "start": "956930",
    "end": "962730"
  },
  {
    "text": "Maximizing this exponent because\nwe have a minus sign is the same as minimizing\nthe exponent",
    "start": "962730",
    "end": "968900"
  },
  {
    "text": "without the minus sign. Sigma squared is a constant. So what you end up doing is\nminimizing this quantity here,",
    "start": "968900",
    "end": "977970"
  },
  {
    "text": "which is the same as\nwhat we had in our linear regression methods.",
    "start": "977970",
    "end": "983640"
  },
  {
    "text": "So in conclusion you might\nchoose to do linear regression",
    "start": "983640",
    "end": "989400"
  },
  {
    "text": "in this particular way,\njust because it looks",
    "start": "989400",
    "end": "994490"
  },
  {
    "text": "reasonable or plausible. Or you might interpret what\nyou're doing as maximum",
    "start": "994490",
    "end": "1001050"
  },
  {
    "text": "likelihood estimation, in which\nyou assume a model of this kind where the noise\nterms are normal random",
    "start": "1001050",
    "end": "1009519"
  },
  {
    "text": "variables with the same\ndistribution -- independent identically\ndistributed.",
    "start": "1009520",
    "end": "1014540"
  },
  {
    "text": "So linear regression implicitly\nmakes an assumption",
    "start": "1014540",
    "end": "1021320"
  },
  {
    "text": "of this kind. It's doing maximum likelihood\nestimation as if the world was",
    "start": "1021320",
    "end": "1027380"
  },
  {
    "text": "really described by a model of\nthis form, and with the W's being random variables.",
    "start": "1027380",
    "end": "1032560"
  },
  {
    "text": "So this gives us at least some\njustification that this",
    "start": "1032560",
    "end": "1037920"
  },
  {
    "text": "particular approach to fitting\nlines to data is not so arbitrary, but it has\na sound footing.",
    "start": "1037920",
    "end": "1045579"
  },
  {
    "text": "OK so then once you accept this\nformulation as being a reasonable one what's\nthe next step?",
    "start": "1045579",
    "end": "1052920"
  },
  {
    "text": "The next step is to see how to\ncarry out this minimization. This is not a very difficult\nminimization to do.",
    "start": "1052920",
    "end": "1062220"
  },
  {
    "text": "The way it's done is by setting\nthe derivatives of",
    "start": "1062220",
    "end": "1068260"
  },
  {
    "text": "this expression to 0. Now because this is a quadratic\nfunction of theta0",
    "start": "1068260",
    "end": "1074500"
  },
  {
    "text": "and theta1-- when you take the derivatives\nwith respect to theta0 and theta1-- you get linear functions\nof theta0 and theta1.",
    "start": "1074500",
    "end": "1083250"
  },
  {
    "text": "And you end up solving a system\nof linear equations in theta0 and theta1.",
    "start": "1083250",
    "end": "1089630"
  },
  {
    "text": "And it turns out that there's\nvery nice and simple formulas",
    "start": "1089630",
    "end": "1095660"
  },
  {
    "text": "for the optimal estimates\nof the parameters in terms of the data. And the formulas\nare these ones.",
    "start": "1095660",
    "end": "1103910"
  },
  {
    "text": "I said that these are nice\nand simple formulas. Let's see why.",
    "start": "1103910",
    "end": "1109800"
  },
  {
    "text": "How can we interpret them?  So suppose that the world is\ndescribed by a model of this",
    "start": "1109800",
    "end": "1122250"
  },
  {
    "text": "kind, where the X's and Y's\nare random variables.",
    "start": "1122250",
    "end": "1128990"
  },
  {
    "text": "And where W is a noise term\nthat's independent of X. So we're assuming that a linear\nmodel is indeed true, but not",
    "start": "1128990",
    "end": "1137750"
  },
  {
    "text": "exactly true. There's always some noise\nassociated with any particular data point that we obtain.",
    "start": "1137750",
    "end": "1144980"
  },
  {
    "text": "So if a model of this kind is\ntrue, and the W's have 0 mean",
    "start": "1144980",
    "end": "1150880"
  },
  {
    "text": "then we have that the expected\nvalue of Y would be theta0 plus theta1 expected value of\nX. And because W has 0 mean",
    "start": "1150880",
    "end": "1163570"
  },
  {
    "text": "there's no extra term. So in particular, theta0 would\nbe equal to expected value of",
    "start": "1163570",
    "end": "1171659"
  },
  {
    "text": "Y minus theta1 expected\nvalue of X.",
    "start": "1171660",
    "end": "1177380"
  },
  {
    "text": "So let's use this equation\nto try to come up with a reasonable estimate of theta0.",
    "start": "1177380",
    "end": "1184060"
  },
  {
    "text": "I do not know the expected\nvalue of Y, but I can estimate it. How do I estimate it?",
    "start": "1184060",
    "end": "1189820"
  },
  {
    "text": "I look at the average of all the\ny's that I have obtained. so I replace this, I estimate\nit with the average of the",
    "start": "1189820",
    "end": "1197320"
  },
  {
    "text": "data I have seen. Here, similarly with the X's.",
    "start": "1197320",
    "end": "1202429"
  },
  {
    "text": "I might not know the expected\nvalue of X's, but I have data points for the x's.",
    "start": "1202430",
    "end": "1208519"
  },
  {
    "text": "I look at the average of all my\ndata points, I come up with an estimate of this\nexpectation.",
    "start": "1208520",
    "end": "1216380"
  },
  {
    "text": "Now I don't know what theta1 is,\nbut my procedure is going",
    "start": "1216380",
    "end": "1221390"
  },
  {
    "text": "to generate an estimate of\ntheta1 called theta1 hat. And once I have this estimate,\nthen a reasonable person would",
    "start": "1221390",
    "end": "1229230"
  },
  {
    "text": "estimate theta0 in this\nparticular way. So that's how my estimate\nof theta0 is going to be",
    "start": "1229230",
    "end": "1237320"
  },
  {
    "text": "constructed. It's this formula here. We have not yet addressed the\nharder question, which is how",
    "start": "1237320",
    "end": "1244700"
  },
  {
    "text": "to estimate theta1 in\nthe first place. So to estimate theta0 I assumed\nthat I already had an",
    "start": "1244700",
    "end": "1250830"
  },
  {
    "text": "estimate for a theta1.  OK, the right formula for the\nestimate of theta1 happens to",
    "start": "1250830",
    "end": "1262059"
  },
  {
    "text": "be this one. It looks messy, but let's\ntry to interpret it.",
    "start": "1262060",
    "end": "1268632"
  },
  {
    "text": "What I'm going to do is I'm\ngoing to take this model for simplicity let's assume that\nthey're the random variables",
    "start": "1268632",
    "end": "1278340"
  },
  {
    "text": "have 0 means.  And see how we might estimate\nhow we might",
    "start": "1278340",
    "end": "1288799"
  },
  {
    "text": "try to estimate theta1. Let's multiply both sides of\nthis equation by X. So we get",
    "start": "1288800",
    "end": "1296270"
  },
  {
    "text": "Y times X equals theta0 plus\ntheta0 times X plus theta1",
    "start": "1296270",
    "end": "1308470"
  },
  {
    "text": "times X-squared, plus X times\nW. And now take expectations",
    "start": "1308470",
    "end": "1314530"
  },
  {
    "text": "of both sides. If I have 0 mean random\nvariables the expected value",
    "start": "1314530",
    "end": "1320160"
  },
  {
    "text": "of Y times X is just the\ncovariance of X with Y.",
    "start": "1320160",
    "end": "1327210"
  },
  {
    "text": "I have assumed that my random\nvariables have 0 means, so the expectation of this is 0.",
    "start": "1327210",
    "end": "1333680"
  },
  {
    "text": "This one is going to be the\nvariance of X, so I have theta1 times variance of X. And\nsince I'm assuming that my",
    "start": "1333680",
    "end": "1343260"
  },
  {
    "text": "random variables have 0 mean,\nand I'm also assuming that W is independent of X this last\nterm also has 0 mean.",
    "start": "1343260",
    "end": "1352250"
  },
  {
    "text": "So under such a probabilistic\nmodel this equation is true.",
    "start": "1352250",
    "end": "1359280"
  },
  {
    "text": "If we knew the variance and the\ncovariance then we would know the value of theta1.",
    "start": "1359280",
    "end": "1365930"
  },
  {
    "text": "But we only have data, we do\nnot necessarily know the variance and the covariance,\nbut we can estimate it.",
    "start": "1365930",
    "end": "1373070"
  },
  {
    "text": "What's a reasonable estimate\nof the variance? The reasonable estimate of the\nvariance is this quantity here",
    "start": "1373070",
    "end": "1379390"
  },
  {
    "text": "divided by n, and the reasonable\nestimate of the covariance is that numerator\ndivided by n.",
    "start": "1379390",
    "end": "1386730"
  },
  {
    "text": " So this is my estimate\nof the mean. I'm looking at the squared\ndistances from the mean, and I",
    "start": "1386730",
    "end": "1395390"
  },
  {
    "text": "average them over lots\nand lots of data. This is the most reasonable way\nof estimating the variance",
    "start": "1395390",
    "end": "1403990"
  },
  {
    "text": "of our distribution. And similarly the expected value\nof this quantity is the",
    "start": "1403990",
    "end": "1411399"
  },
  {
    "text": "covariance of X with Y, and then\nwe have lots and lots of data points. This quantity here is going to\nbe a very good estimate of the",
    "start": "1411400",
    "end": "1418894"
  },
  {
    "text": "covariance. So basically what this\nformula does is--",
    "start": "1418895",
    "end": "1424820"
  },
  {
    "text": "one way of thinking about it-- is that it starts from this\nrelation which is true",
    "start": "1424820",
    "end": "1430870"
  },
  {
    "text": "exactly, but estimates the\ncovariance and the variance on",
    "start": "1430870",
    "end": "1437230"
  },
  {
    "text": "the basis of the data, and then\nusing these estimates to come up with an estimate\nof theta1.",
    "start": "1437230",
    "end": "1445769"
  },
  {
    "text": "So this gives us a probabilistic\ninterpretation of the formulas that we have for\nthe way that the estimates",
    "start": "1445770",
    "end": "1453620"
  },
  {
    "text": "are constructed. If you're willing to assume that\nthis is the true model of",
    "start": "1453620",
    "end": "1459559"
  },
  {
    "text": "the world, the structure of the\ntrue model of the world, except that you do not\nknow means and covariances, and variances.",
    "start": "1459560",
    "end": "1467590"
  },
  {
    "text": "Then this is a natural way of\nestimating those unknown",
    "start": "1467590",
    "end": "1473010"
  },
  {
    "text": "parameters.  All right, so we have a\nclosed-form formula, we can",
    "start": "1473010",
    "end": "1479800"
  },
  {
    "text": "apply it whenever\nwe have data. Now linear regression is a\nsubject on which there are",
    "start": "1479800",
    "end": "1487809"
  },
  {
    "text": "whole courses, and whole\nbooks that are given. And the reason for that is that\nthere's a lot more that",
    "start": "1487810",
    "end": "1494560"
  },
  {
    "text": "you can bring into the topic,\nand many ways that you can elaborate on the simple solution\nthat we got for the",
    "start": "1494560",
    "end": "1502350"
  },
  {
    "text": "case of two parameters and only\ntwo random variables. So let me give you a little bit\nof flavor of what are the",
    "start": "1502350",
    "end": "1509550"
  },
  {
    "text": "topics that come up when you\nstart looking into linear regression in more depth. ",
    "start": "1509550",
    "end": "1516840"
  },
  {
    "text": "So in our discussions so far\nwe made the linear model in",
    "start": "1516840",
    "end": "1524390"
  },
  {
    "text": "which we're trying to explain\nthe values of one variable in terms of the values of\nanother variable.",
    "start": "1524390",
    "end": "1530860"
  },
  {
    "text": "We're trying to explain GPAs\nin terms of SAT scores, or we're trying to predict GPAs\nin terms of SAT scores.",
    "start": "1530860",
    "end": "1539640"
  },
  {
    "text": "But maybe your GPA is affected\nby several factors.",
    "start": "1539640",
    "end": "1547910"
  },
  {
    "text": "For example maybe your GPA is\naffected by your SAT score,",
    "start": "1547910",
    "end": "1556380"
  },
  {
    "text": "also the income of your family,\nthe years of education",
    "start": "1556380",
    "end": "1561820"
  },
  {
    "text": "of your grandmother, and many\nother factors like that. So you might write down a model\nin which I believe that",
    "start": "1561820",
    "end": "1571970"
  },
  {
    "text": "GPA has a relation, which is a\nlinear function of all these",
    "start": "1571970",
    "end": "1577820"
  },
  {
    "text": "other variables that\nI mentioned. So perhaps you have a theory of\nwhat determines performance",
    "start": "1577820",
    "end": "1584350"
  },
  {
    "text": "at college, and you want to\nbuild a model of that type.",
    "start": "1584350",
    "end": "1589539"
  },
  {
    "text": "How do we go about\nin this case? Well, again we collect\nthe data points. We look at the i-th student,\nwho has a college GPA.",
    "start": "1589540",
    "end": "1597980"
  },
  {
    "text": "We record their SAT score,\ntheir family income, and grandmother's years\nof education.",
    "start": "1597980",
    "end": "1605010"
  },
  {
    "text": "So this is one data point that\nis for one particular student.",
    "start": "1605010",
    "end": "1610390"
  },
  {
    "text": "We postulate the model\nof this form. For the i-th student this would\nbe the mistake that our",
    "start": "1610390",
    "end": "1616159"
  },
  {
    "text": "model makes if we have chosen\nspecific values for those parameters. And then we go and choose the\nparameters that are going to",
    "start": "1616160",
    "end": "1625450"
  },
  {
    "text": "give us, again, the\nsmallest possible sum of squared errors. So philosophically it's exactly\nthe same as what we",
    "start": "1625450",
    "end": "1632360"
  },
  {
    "text": "were discussing before, except\nthat now we're including multiple explanatory variables\nin our model instead of a",
    "start": "1632360",
    "end": "1639559"
  },
  {
    "text": "single explanatory variable. So that's the formulation. What do you do next?",
    "start": "1639560",
    "end": "1646070"
  },
  {
    "text": "Well, to do this minimization\nyou're going to take derivatives once you have your\ndata, you have a function of",
    "start": "1646070",
    "end": "1652750"
  },
  {
    "text": "these three parameters. You take the derivative with\nrespect to the parameter, set the derivative equal\nto 0, you get the",
    "start": "1652750",
    "end": "1659169"
  },
  {
    "text": "system of linear equations. You throw that system of\nlinear equations to the computer, and you get numerical\nvalues for the",
    "start": "1659170",
    "end": "1666260"
  },
  {
    "text": "optimal parameters. There are no nice closed-form\nformulas of the type that we",
    "start": "1666260",
    "end": "1672130"
  },
  {
    "text": "had in the previous slide\nwhen you're dealing with multiple variables. Unless you're willing to go\ninto matrix notation.",
    "start": "1672130",
    "end": "1682240"
  },
  {
    "text": "In that case you can again\nwrite down closed-form formulas, but they will be a\nlittle less intuitive than",
    "start": "1682240",
    "end": "1687290"
  },
  {
    "text": "what we had before. But the moral of the story is\nthat numerically this is a",
    "start": "1687290",
    "end": "1693549"
  },
  {
    "text": "procedure that's very easy. It's a problem, an optimization\nproblem that the",
    "start": "1693550",
    "end": "1698780"
  },
  {
    "text": "computer can solve for you. And it can solve it for\nyou very quickly. Because all that it involves\nis solving a",
    "start": "1698780",
    "end": "1705470"
  },
  {
    "text": "system of linear equations.  Now when you choose your\nexplanatory variables you may",
    "start": "1705470",
    "end": "1714270"
  },
  {
    "text": "have some choices. One person may think that your\nGPA a has something to do with",
    "start": "1714270",
    "end": "1723549"
  },
  {
    "text": "your SAT score. Some other person may think that\nyour GPA has something to do with the square of\nyour SAT score.",
    "start": "1723550",
    "end": "1731799"
  },
  {
    "text": "And that other person may\nwant to try to build a model of this kind.",
    "start": "1731800",
    "end": "1738840"
  },
  {
    "text": "Now when would you want\nto do this? ? Suppose that the data that\nyou have looks like this.",
    "start": "1738840",
    "end": "1747830"
  },
  {
    "text": " If the data looks like this then\nyou might be tempted to",
    "start": "1747830",
    "end": "1755740"
  },
  {
    "text": "say well a linear model does\nnot look right, but maybe a quadratic model will give me\na better fit for the data.",
    "start": "1755740",
    "end": "1765649"
  },
  {
    "text": "So if you want to fit a\nquadratic model to the data",
    "start": "1765650",
    "end": "1770690"
  },
  {
    "text": "then what you do is you take\nX-squared as your explanatory variable instead of X, and you\nbuild a model of this kind.",
    "start": "1770690",
    "end": "1782520"
  },
  {
    "text": "There's nothing really different\nin models of this kind compared to models\nof that kind.",
    "start": "1782520",
    "end": "1788830"
  },
  {
    "text": "They are still linear models\nbecause we have theta's",
    "start": "1788830",
    "end": "1794700"
  },
  {
    "text": "showing up in a linear\nfashion. What you take as your\nexplanatory variables, whether",
    "start": "1794700",
    "end": "1800460"
  },
  {
    "text": "it's X, whether it's X-squared,\nor whether it's some other function\nthat you chose. Some general function h of X,\ndoesn't make a difference.",
    "start": "1800460",
    "end": "1809590"
  },
  {
    "text": "So think of you h of X as being\nyour new X. So you can formulate the problem exactly\nthe same way, except that",
    "start": "1809590",
    "end": "1817620"
  },
  {
    "text": "instead of using X's you\nchoose h of X's. ",
    "start": "1817620",
    "end": "1823610"
  },
  {
    "text": "So it's basically a question\ndo I want to build a model that explains Y's based on the\nvalues of X, or do I want to",
    "start": "1823610",
    "end": "1831390"
  },
  {
    "text": "build a model that explains Y's\non the basis of the values of h of X. Which is the\nright value to use?",
    "start": "1831390",
    "end": "1838970"
  },
  {
    "text": "And with this picture here,\nwe see that it can make a difference. A linear model in X might be\na poor fit, but a quadratic",
    "start": "1838970",
    "end": "1847070"
  },
  {
    "text": "model might give us\na better fit. So this brings to the topic of\nhow to choose your functions h",
    "start": "1847070",
    "end": "1855450"
  },
  {
    "text": "of X if you're dealing with\na real world problem. So in a real world problem\nyou're just given X's and Y's.",
    "start": "1855450",
    "end": "1863080"
  },
  {
    "text": "And you have the freedom\nof building models of any kind you want. You have the freedom of choosing\na function h of X of",
    "start": "1863080",
    "end": "1871330"
  },
  {
    "text": "any type that you want. So this turns out to be a quite difficult and tricky topic.",
    "start": "1871330",
    "end": "1878799"
  },
  {
    "text": "Because you may be tempted\nto overdo it. For example, I got my 10 data\npoints, and I could say OK,",
    "start": "1878800",
    "end": "1888450"
  },
  {
    "text": "I'm going to choose an h of X.\nI'm going to choose h of X and",
    "start": "1888450",
    "end": "1895659"
  },
  {
    "text": "actually multiple h's of X\nto do a multiple linear regression in which I'm going to\nbuild a model that's uses a",
    "start": "1895660",
    "end": "1905030"
  },
  {
    "text": "10th degree polynomial. If I choose to fit my data with\na 10th degree polynomial",
    "start": "1905030",
    "end": "1911160"
  },
  {
    "text": "I'm going to fit my data\nperfectly, but I may obtain a model is does something like\nthis, and goes through all my",
    "start": "1911160",
    "end": "1918530"
  },
  {
    "text": "data points. So I can make my prediction\nerrors extremely small if I",
    "start": "1918530",
    "end": "1923830"
  },
  {
    "text": "use lots of parameters, and\nif I choose my h functions appropriately.",
    "start": "1923830",
    "end": "1929929"
  },
  {
    "text": "But clearly this would\nbe garbage. If you get those data points,\nand you say here's my model",
    "start": "1929930",
    "end": "1935270"
  },
  {
    "text": "that explains them. That has a polynomial going up\nand down, then you're probably",
    "start": "1935270",
    "end": "1941320"
  },
  {
    "text": "doing something wrong. So choosing how complicated\nthose functions, the h's, should be.",
    "start": "1941320",
    "end": "1947900"
  },
  {
    "text": "And how many explanatory\nvariables to use is a very delicate and deep topic on which\nthere's deep theory that",
    "start": "1947900",
    "end": "1956560"
  },
  {
    "text": "tells you what you should do,\nand what you shouldn't do. But the main thing that one\nshould avoid doing is having",
    "start": "1956560",
    "end": "1963830"
  },
  {
    "text": "too many parameters in\nyour model when you have too few data.",
    "start": "1963830",
    "end": "1968900"
  },
  {
    "text": "So if you only have 10 data\npoints, you shouldn't have 10 free parameters. With 10 free parameters you will\nbe able to fit your data",
    "start": "1968900",
    "end": "1976150"
  },
  {
    "text": "perfectly, but you wouldn't be\nable to really rely on the results that you are seeing.",
    "start": "1976150",
    "end": "1982010"
  },
  {
    "text": " OK, now in practice, when people\nrun linear regressions",
    "start": "1982010",
    "end": "1992630"
  },
  {
    "text": "they do not just give\npoint estimates for the parameters theta. But similar to what we did for\nthe case of estimating the",
    "start": "1992630",
    "end": "2000300"
  },
  {
    "text": "mean of a random variable you\nmight want to give confidence intervals that sort of tell you\nhow much randomness there",
    "start": "2000300",
    "end": "2007200"
  },
  {
    "text": "is when you estimate each one of\nthe particular parameters. There are formulas for building\nconfidence intervals",
    "start": "2007200",
    "end": "2013960"
  },
  {
    "text": "for the estimates\nof the theta's. We're not going to look\nat them, it would take too much time.",
    "start": "2013960",
    "end": "2019990"
  },
  {
    "text": "Also you might want to estimate\nthe variance in the noise that you have\nin your model.",
    "start": "2019990",
    "end": "2027400"
  },
  {
    "text": "That is if you are pretending\nthat your true model is of the",
    "start": "2027400",
    "end": "2032540"
  },
  {
    "text": "kind we were discussing before,\nnamely Y equals theta1 times X plus W, and W has a\nvariance sigma squared.",
    "start": "2032540",
    "end": "2042190"
  },
  {
    "text": "You might want to estimate this,\nbecause it tells you something about the model, and\nthis is called standard error.",
    "start": "2042190",
    "end": "2049199"
  },
  {
    "text": "It puts a limit on how\ngood predictions your model can make.",
    "start": "2049199",
    "end": "2054730"
  },
  {
    "text": "Even if you have the correct\ntheta0 and theta1, and somebody tells you X you can\nmake a prediction about Y, but",
    "start": "2054730",
    "end": "2062179"
  },
  {
    "text": "that prediction will\nnot be accurate. Because there's this additional\nrandomness. And if that additional\nrandomness is big, then your",
    "start": "2062179",
    "end": "2069699"
  },
  {
    "text": "predictions will also have a\nsubstantial error in them. There's another quantity that\ngets reported usually.",
    "start": "2069699",
    "end": "2078300"
  },
  {
    "text": "This is part of the computer\noutput that you get when you use a statistical package which\nis called R-square.",
    "start": "2078300",
    "end": "2085500"
  },
  {
    "text": "And its a measure of the\nexplanatory power of the model that you have built\nlinear regression.",
    "start": "2085500",
    "end": "2092469"
  },
  {
    "text": "Using linear regression. Instead of defining R-square\nexactly, let me give you a",
    "start": "2092469",
    "end": "2101029"
  },
  {
    "text": "sort of analogous quantity\nthat's involved. After you do your linear\nregression you can look at the",
    "start": "2101030",
    "end": "2108030"
  },
  {
    "text": "following quantity. You look at the variance of Y,\nwhich is something that you",
    "start": "2108030",
    "end": "2115720"
  },
  {
    "text": "can estimate from data. This is how much randomness\nthere is in Y. And compare it",
    "start": "2115720",
    "end": "2123369"
  },
  {
    "text": "with the randomness that you\nhave in Y, but conditioned on X. So this quantity tells\nme if I knew X how much",
    "start": "2123370",
    "end": "2135840"
  },
  {
    "text": "randomness would there\nstill be in my Y? So if I know X, I have more\ninformation, so Y is more",
    "start": "2135840",
    "end": "2143650"
  },
  {
    "text": "constrained. There's less randomness in Y.\nThis is the randomness in Y if I don't know anything about X.",
    "start": "2143650",
    "end": "2150789"
  },
  {
    "text": "So naturally this quantity would\nbe less than 1, and if this quantity is small it would\nmean that whenever I",
    "start": "2150790",
    "end": "2158830"
  },
  {
    "text": "know X then Y is very\nwell known. Which essentially tells me that\nknowing x allows me to",
    "start": "2158830",
    "end": "2167440"
  },
  {
    "text": "make very good predictions about\nY. Knowing X means that I'm explaining away most\nof the randomness in Y.",
    "start": "2167440",
    "end": "2177390"
  },
  {
    "text": "So if you read a statistical\nstudy that uses linear",
    "start": "2177390",
    "end": "2182589"
  },
  {
    "text": "regression you might encounter\nstatements of the form 60% of",
    "start": "2182590",
    "end": "2189730"
  },
  {
    "text": "a student's GPA is explained\nby the family income.",
    "start": "2189730",
    "end": "2196140"
  },
  {
    "text": "If you read the statements of\nthis kind it's really refers to quantities of this kind.",
    "start": "2196140",
    "end": "2203160"
  },
  {
    "text": "Out of the total variance in Y,\nhow much variance is left after we build our model?",
    "start": "2203160",
    "end": "2210060"
  },
  {
    "text": "So if only 40% of the variance\nof Y is left after we build",
    "start": "2210060",
    "end": "2216490"
  },
  {
    "text": "our model, that means that\nX explains 60% of the variations in Y's.",
    "start": "2216490",
    "end": "2222510"
  },
  {
    "text": "So the idea is that\nrandomness in Y is caused by multiple sources.",
    "start": "2222510",
    "end": "2229560"
  },
  {
    "text": "Our explanatory variable\nand random noise. And we ask the question what\npercentage of the total",
    "start": "2229560",
    "end": "2235609"
  },
  {
    "text": "randomness in Y is explained by variations in the X parameter?",
    "start": "2235610",
    "end": "2243030"
  },
  {
    "text": "And how much of the total\nrandomness in Y is attributed just to random effects?",
    "start": "2243030",
    "end": "2250390"
  },
  {
    "text": "So if you have a model that\nexplains most of the variation in Y then you can think that\nyou have a good model that",
    "start": "2250390",
    "end": "2257710"
  },
  {
    "text": "tells you something useful\nabout the real world. Now there's lots of things that\ncan go wrong when you use",
    "start": "2257710",
    "end": "2265990"
  },
  {
    "text": "linear regression, and there's\nmany pitfalls. One pitfall happens when you\nhave this situation that's",
    "start": "2265990",
    "end": "2276440"
  },
  {
    "text": "called heteroskedacisity. So suppose your data\nare of this kind. ",
    "start": "2276440",
    "end": "2286550"
  },
  {
    "text": "So what's happening here? You seem to have a linear model,\nbut when X is small you",
    "start": "2286550",
    "end": "2297640"
  },
  {
    "text": "have a very good model. So this means that W has a small\nvariance when X is here.",
    "start": "2297640",
    "end": "2303829"
  },
  {
    "text": "On the other hand, when X is\nthere you have a lot of randomness. This would be a situation\nin which the W's are not",
    "start": "2303830",
    "end": "2312080"
  },
  {
    "text": "identically distributed, but\nthe variance of the W's, of the noise, has something\nto do with the X's.",
    "start": "2312080",
    "end": "2320360"
  },
  {
    "text": "So with different regions of our\nx-space we have different amounts of noise. What will go wrong in\nthis situation?",
    "start": "2320360",
    "end": "2327615"
  },
  {
    "text": "Since we're trying to minimize\nsum of squared errors, we're really paying attention\nto the biggest errors.",
    "start": "2327615",
    "end": "2334080"
  },
  {
    "text": "Which will mean that we are\ngoing to pay attention to these data points, because\nthat's where the big errors",
    "start": "2334080",
    "end": "2339690"
  },
  {
    "text": "are going to be. So the linear regression\nformulas will end up building a model based on these data,\nwhich are the most noisy ones.",
    "start": "2339690",
    "end": "2349110"
  },
  {
    "text": "Instead of those data that are\nnicely stacked in order.",
    "start": "2349110",
    "end": "2354810"
  },
  {
    "text": "Clearly that's not to the\nright thing to do. So you need to change something,\nand use the fact",
    "start": "2354810",
    "end": "2361500"
  },
  {
    "text": "that the variance of W changes\nwith the X's, and there are ways of dealing with it.",
    "start": "2361500",
    "end": "2367770"
  },
  {
    "text": "It's something that one needs\nto be careful about. Another possibility of getting\ninto trouble is if you're",
    "start": "2367770",
    "end": "2374580"
  },
  {
    "text": "using multiple explanatory\nvariables that are very closely related to each other.",
    "start": "2374580",
    "end": "2381330"
  },
  {
    "text": "So for example, suppose that I\ntried to predict your GPA by",
    "start": "2381330",
    "end": "2387500"
  },
  {
    "text": "looking at your SAT the first\ntime that you took it plus",
    "start": "2387500",
    "end": "2394100"
  },
  {
    "text": "your SAT the second time that\nyou took your SATs. I'm assuming that almost\neveryone takes the",
    "start": "2394100",
    "end": "2400470"
  },
  {
    "text": "SAT more than once. So suppose that you had\na model of this kind.",
    "start": "2400470",
    "end": "2405630"
  },
  {
    "text": "Well, SAT on your first try and\nSAT on your second try are very likely to be\nfairly close.",
    "start": "2405630",
    "end": "2412480"
  },
  {
    "text": "And you could think of coming\nup with estimates in which",
    "start": "2412480",
    "end": "2417570"
  },
  {
    "text": "this is ignored. And you build a model based on\nthis, or an alternative model",
    "start": "2417570",
    "end": "2422779"
  },
  {
    "text": "in which this term is ignored,\nand you make predictions based on the second SAT. And both models are likely to be\nessentially as good as the",
    "start": "2422780",
    "end": "2431839"
  },
  {
    "text": "other one, because these\ntwo quantities are essentially the same. So in that case, your theta's\nthat you estimate are going to",
    "start": "2431840",
    "end": "2441440"
  },
  {
    "text": "be very sensitive to little\ndetails of the data. You change your data, you have\nyour data, and your data tell",
    "start": "2441440",
    "end": "2448559"
  },
  {
    "text": "you that this coefficient\nis big and that coefficient is small. You change your data just a\ntiny bit, and your theta's",
    "start": "2448560",
    "end": "2456060"
  },
  {
    "text": "would drastically change. So this is a case in which you\nhave multiple explanatory variables, but they're redundant\nin the sense that",
    "start": "2456060",
    "end": "2464110"
  },
  {
    "text": "they're very closely related\nto each other, and perhaps with a linear relation. So one must be careful about the\nsituation, and do special",
    "start": "2464110",
    "end": "2471980"
  },
  {
    "text": "tests to make sure that\nthis doesn't happen. Finally the biggest and most\ncommon blunder is that you run",
    "start": "2471980",
    "end": "2480900"
  },
  {
    "text": "your linear regression, you\nget your linear model, and then you say oh, OK.",
    "start": "2480900",
    "end": "2486760"
  },
  {
    "text": "Y is caused by X according to\nthis particular formula.",
    "start": "2486760",
    "end": "2493340"
  },
  {
    "text": "Well, all that we did was to\nidentify a linear relation between X and Y. This doesn't\ntell us anything.",
    "start": "2493340",
    "end": "2500119"
  },
  {
    "text": "Whether it's Y that causes X, or\nwhether it's X that causes Y, or maybe both X and Y are\ncaused by some other variable",
    "start": "2500120",
    "end": "2508849"
  },
  {
    "text": "that we didn't think about. So building a good linear model\nthat has small errors",
    "start": "2508850",
    "end": "2516800"
  },
  {
    "text": "does not tell us anything about\ncausal relations between the two variables.",
    "start": "2516800",
    "end": "2522320"
  },
  {
    "text": "It only tells us that there's\na close association between the two variables. If you know one you can make\npredictions about the other.",
    "start": "2522320",
    "end": "2530370"
  },
  {
    "text": "But it doesn't tell you anything\nabout the underlying physics, that there's some\nphysical mechanism that",
    "start": "2530370",
    "end": "2538120"
  },
  {
    "text": "introduces the relation between\nthose variables. OK, that's it about\nlinear regression.",
    "start": "2538120",
    "end": "2546430"
  },
  {
    "text": "Let us start the next topic,\nwhich is hypothesis testing. And we're going to continue\nwith it next time.",
    "start": "2546430",
    "end": "2555140"
  },
  {
    "text": "So here, instead of trying\nto estimate continuous parameters, we have two\nalternative hypotheses about",
    "start": "2555140",
    "end": "2561920"
  },
  {
    "text": "the distribution of the\nX random variable. So for example our random\nvariable could be either",
    "start": "2561920",
    "end": "2573620"
  },
  {
    "text": "distributed according to this\ndistribution, under H0, or it might be distributed according\nto this distribution under H1.",
    "start": "2573620",
    "end": "2582930"
  },
  {
    "text": "And we want to make a decision\nwhich distribution is the correct one?",
    "start": "2582930",
    "end": "2587990"
  },
  {
    "text": "So we're given those two\ndistributions, and some common terminologies that one of them\nis the null hypothesis--",
    "start": "2587990",
    "end": "2594290"
  },
  {
    "text": "sort of the default hypothesis,\nand we have some alternative hypotheses-- and we want to check whether\nthis one is true,",
    "start": "2594290",
    "end": "2600560"
  },
  {
    "text": "or that one is true. So you obtain a data\npoint, and you want to make a decision.",
    "start": "2600560",
    "end": "2606060"
  },
  {
    "text": "In this picture what would\na reasonable person do to make a decision? They would probably choose a\ncertain threshold, Xi, and",
    "start": "2606060",
    "end": "2615500"
  },
  {
    "text": "decide that H1 is true if your\ndata falls in this interval.",
    "start": "2615500",
    "end": "2623540"
  },
  {
    "text": "And decide that H0 is true\nif you fall on the side.",
    "start": "2623540",
    "end": "2629590"
  },
  {
    "text": "So that would be a\nreasonable way of approaching the problem. More generally you take the set\nof all possible X's, and",
    "start": "2629590",
    "end": "2639160"
  },
  {
    "text": "you divide the set of possible\nX's into two regions. One is the rejection region,\nin which you decide H1,",
    "start": "2639160",
    "end": "2651110"
  },
  {
    "text": "or you reject H0.  And the complement of that\nregion is where you decide H0.",
    "start": "2651110",
    "end": "2661640"
  },
  {
    "text": "So this is the x-space\nof your data. In this example here, x\nwas one-dimensional.",
    "start": "2661640",
    "end": "2668349"
  },
  {
    "text": "But in general X is going to\nbe a vector, where all the possible data vectors that\nyou can get, they're",
    "start": "2668350",
    "end": "2674790"
  },
  {
    "text": "divided into two types. If it falls in this set you'd\nmake one decision.",
    "start": "2674790",
    "end": "2680400"
  },
  {
    "text": "If it falls in that set, you\nmake the other decision. OK, so how would you\ncharacterize the performance",
    "start": "2680400",
    "end": "2687380"
  },
  {
    "text": "of the particular way of\nmaking a decision? Suppose I chose my threshold.",
    "start": "2687380",
    "end": "2693000"
  },
  {
    "text": "I may make mistakes of\ntwo possible types. Perhaps H0 is true, but my data\nhappens to fall here.",
    "start": "2693000",
    "end": "2703359"
  },
  {
    "text": "In which case I make a mistake,\nand this would be a false rejection of H0.",
    "start": "2703360",
    "end": "2710730"
  },
  {
    "text": "If my data falls here\nI reject H0. I decide H1.",
    "start": "2710730",
    "end": "2716890"
  },
  {
    "text": "Whereas H0 was true. The probability of\nthis happening? Let's call it alpha.",
    "start": "2716890",
    "end": "2724890"
  },
  {
    "text": "But there's another kind of\nerror that can be made. Suppose that H1 was true, but by\naccident my data happens to",
    "start": "2724890",
    "end": "2732810"
  },
  {
    "text": "falls on that side. Then I'm going to make\nan error again. I'm going to decide H0 even\nthough H1 was true.",
    "start": "2732810",
    "end": "2740540"
  },
  {
    "text": "How likely is this to occur? This would be the area under\nthis curve here.",
    "start": "2740540",
    "end": "2746420"
  },
  {
    "text": "And that's the other type of\nerror than can be made, and beta is the probability of this\nparticular type of error.",
    "start": "2746420",
    "end": "2755400"
  },
  {
    "text": "Both of these are errors. Alpha is the probability\nof error of one kind. Beta is the probability of an\nerror of the other kind.",
    "start": "2755400",
    "end": "2762109"
  },
  {
    "text": "You would like the\nprobabilities of error to be small. So you would like to\nmake both alpha and",
    "start": "2762110",
    "end": "2767550"
  },
  {
    "text": "beta as small as possible. Unfortunately that's not\npossible, there's a trade-off.",
    "start": "2767550",
    "end": "2773300"
  },
  {
    "text": "If I go to my threshold it this\nway, then alpha become smaller, but beta\nbecomes bigger.",
    "start": "2773300",
    "end": "2780760"
  },
  {
    "text": "So there's a trade-off. If I make my rejection region\nsmaller one kind of error is",
    "start": "2780760",
    "end": "2789350"
  },
  {
    "text": "less likely, but the\nother kind of error becomes more likely.",
    "start": "2789350",
    "end": "2794670"
  },
  {
    "text": "So we got this trade-off. So what do we do about it? How do we move systematically?",
    "start": "2794670",
    "end": "2801570"
  },
  {
    "text": "How do we come up with\nrejection regions? Well, what the theory basically\ntells you is it",
    "start": "2801570",
    "end": "2808900"
  },
  {
    "text": "tells you how you should\ncreate those regions. But it doesn't tell\nyou exactly how.",
    "start": "2808900",
    "end": "2817859"
  },
  {
    "text": "It tells you the general\nshape of those regions. For example here, the theory\nwho tells us that the right",
    "start": "2817860",
    "end": "2825119"
  },
  {
    "text": "thing to do would be to put\nthe threshold and make decisions one way to the right,\none way to the left.",
    "start": "2825120",
    "end": "2830910"
  },
  {
    "text": "But it might not necessarily\ntell us where to put the threshold. Still, it's useful enough to\nknow that the way to make a",
    "start": "2830910",
    "end": "2838890"
  },
  {
    "text": "good decision would\nbe in terms of a particular threshold. Let me make this\nmore specific.",
    "start": "2838890",
    "end": "2844770"
  },
  {
    "text": "We can take our inspiration\nfrom the solution of the hypothesis testing problem\nthat we had in",
    "start": "2844770",
    "end": "2849819"
  },
  {
    "text": "the Bayesian case. In the Bayesian case we just\npick the hypothesis which is more likely given the data.",
    "start": "2849820",
    "end": "2857480"
  },
  {
    "text": "The produced posterior\nprobabilities using Bayesian rule, they're written\nthis way.",
    "start": "2857480",
    "end": "2862770"
  },
  {
    "text": "And this term is the\nsame as that term. They cancel out, then let me\ncollect terms here and there.",
    "start": "2862770",
    "end": "2869500"
  },
  {
    "text": " I get an expression here. I think the version you\nhave in your handout",
    "start": "2869500",
    "end": "2876090"
  },
  {
    "text": "is the correct one.  The one on the slide was\nnot the correct one, so",
    "start": "2876090",
    "end": "2882082"
  },
  {
    "text": "I'm fixing it here. OK, so this is the form of how\nyou make decisions in the Bayesian case.",
    "start": "2882082",
    "end": "2888720"
  },
  {
    "text": "What you do in the Bayesian\ncase, you calculate this ratio. Let's call it the likelihood\nratio.",
    "start": "2888720",
    "end": "2897110"
  },
  {
    "text": "And compare that ratio\nto a threshold. And the threshold that you\nshould be using in the",
    "start": "2897110",
    "end": "2902916"
  },
  {
    "text": "Bayesian case has something\nto do with the prior probabilities of the\ntwo hypotheses.",
    "start": "2902916",
    "end": "2908000"
  },
  {
    "text": "In the non-Bayesian case we do\nnot have prior probabilities, so we do not know how to\nset this threshold.",
    "start": "2908000",
    "end": "2914690"
  },
  {
    "text": "But we're going to do is we're\ngoing to keep this particular structure anyway, and maybe use\nsome other considerations",
    "start": "2914690",
    "end": "2922690"
  },
  {
    "text": "to pick the threshold. So we're going to use a\nlikelihood ratio test, that's",
    "start": "2922690",
    "end": "2931030"
  },
  {
    "text": "how it's called in which we\ncalculate a quantity of this kind that we call the\nlikelihood, and compare it",
    "start": "2931030",
    "end": "2936830"
  },
  {
    "text": "with a threshold. So what's the interpretation\nof this likelihood? ",
    "start": "2936830",
    "end": "2943140"
  },
  {
    "text": "We ask-- the X's that I have observed,\nhow likely were they to occur",
    "start": "2943140",
    "end": "2948570"
  },
  {
    "text": "if H1 was true? And how likely were they to\noccur if H0 was true?",
    "start": "2948570",
    "end": "2954589"
  },
  {
    "text": "This ratio could be big if my\ndata are plausible they might",
    "start": "2954590",
    "end": "2960560"
  },
  {
    "text": "occur under H1. But they're very implausible,\nextremely unlikely to occur under H0.",
    "start": "2960560",
    "end": "2967380"
  },
  {
    "text": "Then my thinking would be well\nthe data that I saw are extremely unlikely to have\noccurred under H0.",
    "start": "2967380",
    "end": "2973300"
  },
  {
    "text": "So H0 is probably not true. I'm going to go for\nH1 and choose H1.",
    "start": "2973300",
    "end": "2979820"
  },
  {
    "text": "So when this ratio is big it\ntells us that the data that we're seeing are better\nexplained if we assume H1 to",
    "start": "2979820",
    "end": "2987720"
  },
  {
    "text": "be true rather than\nH0 to be true. So I calculate this quantity,\ncompare it with a threshold,",
    "start": "2987720",
    "end": "2993970"
  },
  {
    "text": "and that's how I make\nmy decision. So in this particular picture,\nfor example the way it would",
    "start": "2993970",
    "end": "2999359"
  },
  {
    "text": "go would be the likelihood ratio\nin this picture goes monotonically with my X. So\ncomparing the likelihood ratio",
    "start": "2999360",
    "end": "3007230"
  },
  {
    "text": "to the threshold would be the\nsame as comparing my x to the threshold, and we've got\nthe question of how",
    "start": "3007230",
    "end": "3012890"
  },
  {
    "text": "to choose the threshold. The way that the threshold is\nchosen is usually done by fixing one of the two\nprobabilities of error.",
    "start": "3012890",
    "end": "3021559"
  },
  {
    "text": "That is, I say, that I want my\nerror of one particular type",
    "start": "3021560",
    "end": "3026710"
  },
  {
    "text": "to be a given number,\nso I fix this alpha. And then I try to find where\nmy threshold should be.",
    "start": "3026710",
    "end": "3033160"
  },
  {
    "text": "So that this probability theta,\nprobability out there, is just equal to alpha.",
    "start": "3033160",
    "end": "3039190"
  },
  {
    "text": "And then the other probability\nof error, beta, will be whatever it turns out to be. So somebody picks alpha\nahead of time.",
    "start": "3039190",
    "end": "3048140"
  },
  {
    "text": "Based on the probability of\na false rejection based on alpha, I find where my threshold\nis going to be.",
    "start": "3048140",
    "end": "3055890"
  },
  {
    "text": "I choose my threshold, and that\ndetermines subsequently the value of beta.",
    "start": "3055890",
    "end": "3061269"
  },
  {
    "text": "So we're going to continue with\nthis story next time, and",
    "start": "3061270",
    "end": "3067340"
  },
  {
    "text": "we'll stop here. ",
    "start": "3067340",
    "end": "3109120"
  }
]