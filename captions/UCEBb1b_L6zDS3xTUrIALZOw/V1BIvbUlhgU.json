[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6859"
  },
  {
    "text": "offer high-quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6860",
    "end": "13410"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "13410",
    "end": "18440"
  },
  {
    "text": " PROFESSOR: OK. So today we're going to continue\non with some of the",
    "start": "18440",
    "end": "26790"
  },
  {
    "text": "design patterns that we started talking about last week. So to recap, there are really\nfour common steps to taking a",
    "start": "26790",
    "end": "34780"
  },
  {
    "text": "program and then parallelizing\nit. Often you're starting off with\na program that's designed or written in a sequential\nmanner.",
    "start": "34780",
    "end": "41060"
  },
  {
    "text": "And what you want to do is find\ntasks in the program -- and these are sort of\nindependent work pieces that",
    "start": "41060",
    "end": "47230"
  },
  {
    "text": "you are going to be able\nto decompose from your sequential code. You're going to group\ntasks together",
    "start": "47230",
    "end": "52520"
  },
  {
    "text": "into threads or processes. And then you'll essentially map\neach one of these threads or processes down to the\nactual hardware.",
    "start": "52520",
    "end": "59640"
  },
  {
    "text": "And that will get you,\neventually when these programs run, the concurrency\nand the performance speedups that you want.",
    "start": "59640",
    "end": "65750"
  },
  {
    "text": " So as a reminder of what I\ntalked about last week in",
    "start": "65750",
    "end": "72120"
  },
  {
    "text": "terms of finding the task or\nfinding the concurrency, you start off with an application. You come up with a block\nlevel diagram.",
    "start": "72120",
    "end": "79130"
  },
  {
    "text": "And from that you sort of try\nto understand where the time is spent in the computations\nand what are some typical",
    "start": "79130",
    "end": "85520"
  },
  {
    "text": "patterns for how the\ncomputations are carried out. So we talked about task\ndecomposition or sort of",
    "start": "85520",
    "end": "91680"
  },
  {
    "text": "independent tasks or tasks that\nmight be different that the application is\ncarrying out. So in the MPEG encoder, we\nlooked at decoding the motion",
    "start": "91680",
    "end": "98310"
  },
  {
    "text": "vectors for temporal compression\nversus spatial compression.",
    "start": "98310",
    "end": "103470"
  },
  {
    "text": "It does sort of substantially\ndifferent work. We talked about data\ndecomposition.",
    "start": "103470",
    "end": "109820"
  },
  {
    "text": "So if you're doing a process\n-- so if you have some work that's really consuming a large\nchunk of data, and you",
    "start": "109820",
    "end": "116740"
  },
  {
    "text": "realize that it's applying the\nsame kind of work to each of those data pieces, then you can\npartition your data into",
    "start": "116740",
    "end": "122640"
  },
  {
    "text": "smaller subsets and apply\nthe same function over and over again.",
    "start": "122640",
    "end": "127730"
  },
  {
    "start": "127730",
    "end": "133970"
  },
  {
    "text": "So in the motion compensation\nphase, that's one example where you can replicate the\nfunction and split up the data",
    "start": "133970",
    "end": "139590"
  },
  {
    "text": "stream in different ways\nand have these tasks proceed in parallel. So that's data decomposition.",
    "start": "139590",
    "end": "145250"
  },
  {
    "text": "And then we talked a little\nbit about sort of making a case for a pipeline\ndecomposition. So you have a data assembly\nline or producer-consumer",
    "start": "145250",
    "end": "152530"
  },
  {
    "text": "chains, and you essentially\nwant to recognize those in your computation and make it so\nthat you can exploit them",
    "start": "152530",
    "end": "159140"
  },
  {
    "text": "eventually when you're\ndoing your mapping down to actual hardware. But what does it mean for\ntwo tasks to actually be",
    "start": "159140",
    "end": "166209"
  },
  {
    "text": "concurrent? And how do you know that you\ncan safely actually run two tasks in parallel?",
    "start": "166210",
    "end": "171230"
  },
  {
    "text": "So there's something I crudely\nwent over last time. So as to make it more concrete,\nhighlighting",
    "start": "171230",
    "end": "178180"
  },
  {
    "text": "Bernstein's condition, which\nsays that given two tasks, if the input set to one task is\ndifferent from or does not",
    "start": "178180",
    "end": "186630"
  },
  {
    "text": "intersect with the output set\nof another, and vice versa, and neither task sort of\nupdates the same data",
    "start": "186630",
    "end": "193910"
  },
  {
    "text": "structures in memory, then\nthere's really no dependency issues between them. You can run them safely\nin parallel.",
    "start": "193910",
    "end": "201650"
  },
  {
    "text": "So task T1 and T2, if all the\ndata that's consumed by T1, so all the data elements that are\nread by T1 are different from",
    "start": "201650",
    "end": "209840"
  },
  {
    "text": "the ones that are read by\nT2, then you have -- you know, if T2 is running in\nparallel, there's really no",
    "start": "209840",
    "end": "217130"
  },
  {
    "text": "problem with T1 because\nit's updating the orthogonal data set. Similarly for T2 and T1, any\noutputs are different.",
    "start": "217130",
    "end": "225060"
  },
  {
    "text": "So as an example, let's say\nyou have two tasks. In T1 you're doing some\nbasic statements.",
    "start": "225060",
    "end": "232239"
  },
  {
    "text": "And these could be essentially\nmore coarse grained. There could be a lot more\ncomputation in here. I just simplified it for\nthe illustration.",
    "start": "232240",
    "end": "240390"
  },
  {
    "text": "So you have task a\nequals x plus y. And task two does b\nequals x plus z.",
    "start": "240390",
    "end": "246300"
  },
  {
    "text": "So if we look at the read set\nfor T1, these are all the variables or data structures\nor addresses these that are",
    "start": "246300",
    "end": "253170"
  },
  {
    "text": "read by the first task. So that's x and y here. And all the data that's written\nor produced by T1.",
    "start": "253170",
    "end": "259480"
  },
  {
    "text": "So here we're just producing\none data value. And that's going into\nlocation A.",
    "start": "259480",
    "end": "264830"
  },
  {
    "text": "Similarly we can come up\nwith the read set and write set for T2. And so that's shown on here.",
    "start": "264830",
    "end": "272220"
  },
  {
    "text": "So we have -- task T2 has x\nplus z in its read set. And it produces one\ndata value, b.",
    "start": "272220",
    "end": "277440"
  },
  {
    "text": "If we take the intersection of\nthe read and write sets for the different tasks,\nthen they're empty.",
    "start": "277440",
    "end": "283550"
  },
  {
    "text": "I read something completely\ndifferent than what's produced in this task and vice versa. And they write to\ntwo completely",
    "start": "283550",
    "end": "289190"
  },
  {
    "text": "different memory locations. So I can essentially parallelize\nthese or run these two tasks in parallel.",
    "start": "289190",
    "end": "294810"
  },
  {
    "text": "So you can extend\nthis analysis. And compilers can actually use\nthis condition to determine",
    "start": "294810",
    "end": "301470"
  },
  {
    "text": "when two tasks can be\nparallelized if you're doing automatic parallelization. And you'll probably hear more\nabout these later on.",
    "start": "301470",
    "end": "307960"
  },
  {
    "text": " And so what I focused on last\ntime were the finding",
    "start": "307960",
    "end": "313990"
  },
  {
    "text": "concurrency patterns. And I had identified sort of\nfour design spaces based on",
    "start": "313990",
    "end": "320630"
  },
  {
    "text": "the work that's outlined in the\nbook by Mattson, Sanders, and Massingill.",
    "start": "320630",
    "end": "326160"
  },
  {
    "text": "And so starting with two\nlarge sort of concepts.",
    "start": "326160",
    "end": "331750"
  },
  {
    "text": "The first helps you figure out\nhow you're going to actually express your algorithm. So first you find your\nconcurrency and then you",
    "start": "331750",
    "end": "338500"
  },
  {
    "text": "organize in some way. And so we're going to talk about\nthat in more detail. And then once you've organized\nyour tasks in some way that",
    "start": "338500",
    "end": "344789"
  },
  {
    "text": "actually expresses your overall\ncomputation, you need some software construction\nutilities or data structures",
    "start": "344790",
    "end": "352669"
  },
  {
    "text": "or mechanisms for actually\norchestrating computations for which they have also abstracted out some common patterns.",
    "start": "352670",
    "end": "358560"
  },
  {
    "text": "And so I'll briefly talk\nabout these as well. And so on your algorithm\nexpression side, these are",
    "start": "358560",
    "end": "363870"
  },
  {
    "text": "essentially conceptualization\nsteps that help you abstract out your problem. And you may in fact think\nabout your algorithm",
    "start": "363870",
    "end": "372490"
  },
  {
    "text": "expression in different ways to\nexpose different kinds of concurrency or to be able to\nexplore different ways of",
    "start": "372490",
    "end": "377960"
  },
  {
    "text": "mapping the concurrency\nto hardware. And so for construction it's\nmore about actual engineering",
    "start": "377960",
    "end": "383539"
  },
  {
    "text": "and implementation. So here you're actually thinking\nabout what do the data structures look like? What is the communication\npattern going to look like?",
    "start": "383540",
    "end": "390050"
  },
  {
    "text": "Am I but going to use things\nlike MPI or OpenMP? What does that help me with\nin terms of doing my",
    "start": "390050",
    "end": "396270"
  },
  {
    "text": "implementation?  So given a collection of\nconcurrent tasks -- so you've",
    "start": "396270",
    "end": "402800"
  },
  {
    "text": "done your first step in your\nfour design patterns. You know, what is\nyour next step? And that's really mapping\nthose tasks that you've",
    "start": "402800",
    "end": "409390"
  },
  {
    "text": "identified down to some sort\nof execution units. So threads are very common.",
    "start": "409390",
    "end": "414660"
  },
  {
    "text": "This is essentially what we've\nbeen using on Cell. We take our computation and we\nwrap it into SPE threads and",
    "start": "414660",
    "end": "419960"
  },
  {
    "text": "then we can execute\nthose at run time. So some things to keep in mind\n-- although you shouldn't over",
    "start": "419960",
    "end": "425900"
  },
  {
    "text": "constrain yourself in terms\nof these considerations. What is the magnitude of\nyour parallelism that",
    "start": "425900",
    "end": "432150"
  },
  {
    "text": "you're going to get? You know, do you want hundreds\nor thousands of threads? Or do you want something\non the order of tens?",
    "start": "432150",
    "end": "438630"
  },
  {
    "text": "And this is because you don't\nwant to overwhelm the intended system that you're\ngoing to run on. So we talked about yesterday on\nCell processor, if you're",
    "start": "438630",
    "end": "447010"
  },
  {
    "text": "creating a lot more than six\nthreads, then you can create problems or you essentially\ndon't get extra parallelism",
    "start": "447010",
    "end": "452660"
  },
  {
    "text": "because each thread is running\nto completion on each SPE. And contact switch overhead\nis extremely high.",
    "start": "452660",
    "end": "458139"
  },
  {
    "text": "So you don't want to spend too\nmuch engineering cost to come up with an algorithm\nimplementation that's massively scalable to hundreds\nor thousands of threads when",
    "start": "458140",
    "end": "465940"
  },
  {
    "text": "you can't actually exploit it. But that doesn't mean that you\nshould over constrain your implementation to where if now\nI want to take your code and",
    "start": "465940",
    "end": "473000"
  },
  {
    "text": "run it on a different machine, I\nessentially have to redesign or re-engineer the\ncomplete process.",
    "start": "473000",
    "end": "478410"
  },
  {
    "text": "So you want to avoid tendencies\nto over constrain the implementation. And you want to leave your code\nin a way that's malleable",
    "start": "478410",
    "end": "484340"
  },
  {
    "text": "so that you can easily make\nchanges to sort of factor in new platforms that you want\nto run on or new machine",
    "start": "484340",
    "end": "490340"
  },
  {
    "text": "architecture features that you\nmight want to exploit. So there are three major\norganization principles I'm",
    "start": "490340",
    "end": "497670"
  },
  {
    "text": "going to talk about. And none of these should be sort\nof foreign to you at this point because we've talked about\nthem in different ways",
    "start": "497670",
    "end": "504950"
  },
  {
    "text": "in the recitations or in\nprevious lectures. And it's really, what is it that\ndetermines sort of the",
    "start": "504950",
    "end": "511240"
  },
  {
    "text": "algorithm structure based on the\nset of tasks that you're actually carrying out\nin your computation? And so there's the principle\nthat says,",
    "start": "511240",
    "end": "518180"
  },
  {
    "text": "organize things by tasks. I'm going to talk to that. And then there's a principle\nthat says, well, organize",
    "start": "518180",
    "end": "523979"
  },
  {
    "text": "things by how you're doing\nthe data decomposition. So in this case how you're\nactually distributing the data",
    "start": "523980",
    "end": "530090"
  },
  {
    "text": "or how to the data is laid out\nin memory, or how you're partitioning the data to\nactually compute on it",
    "start": "530090",
    "end": "535510"
  },
  {
    "text": "dictates how you should actually\norganize your actual computation. And then there's organize\nby flow of data.",
    "start": "535510",
    "end": "541779"
  },
  {
    "text": "And this is something you'll\nhear about more in the next lecture where we're talking\nabout streaming. But in this pattern if there\nare specific sort of",
    "start": "541780",
    "end": "552209"
  },
  {
    "text": "computations that take advantage\nof high bandwidth flow of data between\ncomputations, you might want",
    "start": "552210",
    "end": "558560"
  },
  {
    "text": "to exploit that for\nconcurrency. And we'll talk about\nthat as well. OK.",
    "start": "558560",
    "end": "564100"
  },
  {
    "text": "So a design diagram\nfor how can you actually go through process. So you can ask yourself\na set of questions.",
    "start": "564100",
    "end": "571250"
  },
  {
    "text": "if I want to organize things\nby tasks, then there are essentially two main clusters or\ntwo main computations, two",
    "start": "571250",
    "end": "578529"
  },
  {
    "text": "main patterns. If the code is recursive, then\nyou essentially want to apply",
    "start": "578530",
    "end": "583700"
  },
  {
    "text": "a divide and conquer pattern\nor divide and conquer organization. If it's not recursive.",
    "start": "583700",
    "end": "589779"
  },
  {
    "text": "then you essentially want\nto do task parallelism.",
    "start": "589780",
    "end": "594990"
  },
  {
    "text": "So in task parallelism -- you know, I've listed\ntwo examples here. But really any of the things\nthat we've talked about in the past fit.",
    "start": "594990",
    "end": "601810"
  },
  {
    "text": "Ray computation, ray tracing. So here you're shooting rays\nthrough a scene to try to determine how to render it.",
    "start": "601810",
    "end": "609420"
  },
  {
    "text": "And really each ray is a\nseparate and independent computation step. In molecular dynamics you're\ntrying to determine the",
    "start": "609420",
    "end": "616070"
  },
  {
    "text": "non-bonded force calculations. There are some dependencies,\nbut really you can do each calculation for one molecule\nor for one atom",
    "start": "616070",
    "end": "623339"
  },
  {
    "text": "independent of any other. And then there are sort of the\nglobal dependence of having to update or communicate across all\nthose molecules that sort",
    "start": "623340",
    "end": "630529"
  },
  {
    "text": "of reflect new positions\nin the system. So the common factors here are\nyour tasks are associated with",
    "start": "630530",
    "end": "638500"
  },
  {
    "text": "iterations of a loop. And you can distribute, you\nknow, each process -- each processor can\ndo a different",
    "start": "638500",
    "end": "644760"
  },
  {
    "text": "iteration of the loop. And often you know sort of what\nthe tasks are before you",
    "start": "644760",
    "end": "651170"
  },
  {
    "text": "actually start your\ncomputation. Although in some cases, like\nin ray tracing, you might generate more and more threads\nas you go along, or more and",
    "start": "651170",
    "end": "659040"
  },
  {
    "text": "more computations because as\nthe ray is shooting off, you're calculating\nnew reflections.",
    "start": "659040",
    "end": "665680"
  },
  {
    "text": "And that creates sort\nof extra work. But largely you have these\nindependent tasks that you can",
    "start": "665680",
    "end": "671330"
  },
  {
    "text": "encapsulate in threads\nand you run them. And this is sort of -- it might\nappear subtle, but there",
    "start": "671330",
    "end": "677649"
  },
  {
    "text": "are algorithm classes where not\nall tasks essentially need to complete for you to\narrive at a solution.",
    "start": "677650",
    "end": "683160"
  },
  {
    "text": "You know, in some cases\nyou might convert to an acceptable solution. And you don't actually need to\ngo through and exercise all",
    "start": "683160",
    "end": "691240"
  },
  {
    "text": "the computation that's\noutstanding for you to say the program is done. So there will be a\ntricky issue -- I'll revisit this just briefly\nlater on -- is how do you",
    "start": "691240",
    "end": "698699"
  },
  {
    "text": "determine if your program\nhas actually terminated or has completed? In divide and conquer, this is\nreally for recursive programs.",
    "start": "698700",
    "end": "707910"
  },
  {
    "text": "You know, you can think of a\nwell-known sorting algorithm, merge sort, that classically\nfits into this kind of",
    "start": "707910",
    "end": "713900"
  },
  {
    "text": "picture, where you have some\nreally large array of data that you want to sort. You keep subdividing into\nsmaller and smaller chunks",
    "start": "713900",
    "end": "721330"
  },
  {
    "text": "until you can do local\nreorderings. And then you start merging\nthings together. So this gives you sort of a way\nto take a problem, divide",
    "start": "721330",
    "end": "729460"
  },
  {
    "text": "it into subproblems. And then\nyou can split the data at some point and then you join\nit back together. You merge it.",
    "start": "729460",
    "end": "735540"
  },
  {
    "text": "You might see things like fork\nand merge or fork and join used instead of split\nand join.",
    "start": "735540",
    "end": "742329"
  },
  {
    "text": "I've used the terminology that\nsort of melds well with some of the concepts we use in\nstreaming that you'll see in",
    "start": "742330",
    "end": "748209"
  },
  {
    "text": "the next lecture. And so in these kinds of\nprograms, it's not always the case that each subproblem will\nhave essentially the same",
    "start": "748210",
    "end": "755510"
  },
  {
    "text": "amount of work to do. You might need more dynamic load\nbalancing because each",
    "start": "755510",
    "end": "761050"
  },
  {
    "text": "subproblem -- how you distribute the data\nmight lead you to do more work in one problem than\nin the other.",
    "start": "761050",
    "end": "767720"
  },
  {
    "text": "So as opposed to some of the\nother mechanisms where static load balancing will work\nreally well --",
    "start": "767720",
    "end": "773220"
  },
  {
    "text": "and to remind you, static load\nbalancing essentially says, you have some work, you assign\nit to each of the processors.",
    "start": "773220",
    "end": "778610"
  },
  {
    "text": "And you're going to be\nrelatively happy with how each processor's sort of\nutilization is",
    "start": "778610",
    "end": "784000"
  },
  {
    "text": "going to be over time. Nobody's going to be too\noverwhelmed with the amount of work they have to do. In this case, you might end up\nwith needing some things for",
    "start": "784000",
    "end": "791630"
  },
  {
    "text": "dynamic load balancing that\nsays, I'm unhappy with the work performance\nor utilization.",
    "start": "791630",
    "end": "796700"
  },
  {
    "text": "Some processors are more idle\nthan the others, so you might want to essentially redistribute\nthings. So what we'll talk about --\nyou know, how does this",
    "start": "796700",
    "end": "804060"
  },
  {
    "text": "concept of divide and conquer\nparallelization pattern work",
    "start": "804060",
    "end": "809310"
  },
  {
    "text": "into the actual implementation? You know, how do I actually\nimplement a divide and conquer",
    "start": "809310",
    "end": "815310"
  },
  {
    "text": "organization? The next organization is\norganized by data.",
    "start": "815310",
    "end": "820590"
  },
  {
    "text": "So here you have some\ncomputation -- not sure why it's flickering. AUDIENCE: Check your -- maybe your VGA cables\naren't in good.",
    "start": "820590",
    "end": "827455"
  },
  {
    "text": " PROFESSOR: So in the organize by\ndata, you essentially want",
    "start": "827455",
    "end": "840449"
  },
  {
    "text": "to apply this if you have a\nlot of computation that's using a shared global data\nstructure or that's going to",
    "start": "840450",
    "end": "846040"
  },
  {
    "text": "update a central\ndata structure. So in molecular dynamics, for\nexample, you have a huge array",
    "start": "846040",
    "end": "851940"
  },
  {
    "text": "that records the position of\neach of the molecules. And while you can do the\ncoarse calculations",
    "start": "851940",
    "end": "857310"
  },
  {
    "text": "independently, eventually all\nthe parallel tasks have to communicate with the central\ndata structure and say, here",
    "start": "857310",
    "end": "863699"
  },
  {
    "text": "are the new locations for\nall the molecules. And so that has to go into\na central repository.",
    "start": "863700",
    "end": "869070"
  },
  {
    "text": "And there are different kinds\nof sort of decompositions within this organization.",
    "start": "869070",
    "end": "876160"
  },
  {
    "text": "If your data structure is\nrecursive, so a link list or a tree or a graph, then\nyou can apply the",
    "start": "876160",
    "end": "882750"
  },
  {
    "text": "recursive data pattern. If it's not, if it's linear,\nlike an array or a vector,",
    "start": "882750",
    "end": "888250"
  },
  {
    "text": "then you apply geometric\ndecomposition. And you've essentially seen\ngeometric decomposition.",
    "start": "888250",
    "end": "893560"
  },
  {
    "text": "These were some of the labs\nthat you've already done. And so the example from\nyesterday's recitation, you're",
    "start": "893560",
    "end": "899850"
  },
  {
    "text": "doing an end body simulation in\nterms of who is gravitating towards who, you're calculating\nthe forces between",
    "start": "899850",
    "end": "906650"
  },
  {
    "text": "pairs of objects. And depending on the force that\neach object feels, you calculate a new motion vector.",
    "start": "906650",
    "end": "916170"
  },
  {
    "text": "And you use that to update the\nposition of each body in your, say, galaxy that you're\nsimulating.",
    "start": "916170",
    "end": "922790"
  },
  {
    "text": "And so what we talked about\nyesterday was given an array of positions, each processor\ngets a sub-chunk of that",
    "start": "922790",
    "end": "930050"
  },
  {
    "text": "position array. And it knows how to\ncalculate sort of locally, based on that.",
    "start": "930050",
    "end": "935240"
  },
  {
    "text": "And then you might also\ncommunicate local chunks to do more scalable computations. ",
    "start": "935240",
    "end": "943930"
  },
  {
    "text": "And recursive data structure are\na little bit more tricky. So at face value you might think\nthat there's really no",
    "start": "943930",
    "end": "949910"
  },
  {
    "text": "kind of parallelism you\ncan get out of a recursive data structure. So if you're iterating over a\nlist and you want to get the",
    "start": "949910",
    "end": "955310"
  },
  {
    "text": "sum, well, you know, I just need\nto go through the list. Can I really parallelize that?",
    "start": "955310",
    "end": "961019"
  },
  {
    "text": "There are, however,\nopportunities where you can reshape the computation in\na way that exposes the",
    "start": "961020",
    "end": "967100"
  },
  {
    "text": "concurrency. And often what this comes down\nto is you're going to do more work, but it's OK because you're\ngoing to finish faster.",
    "start": "967100",
    "end": "976000"
  },
  {
    "text": "So this kind of work/concurrency\ntradeoff, I'm going to illustrate\nwith an example.",
    "start": "976000",
    "end": "981740"
  },
  {
    "text": "So in this application\nwe have some graphs.",
    "start": "981740",
    "end": "987450"
  },
  {
    "text": "And for each node in\na graph, we want to know what is its root? So this works well when you have\na forest where not all",
    "start": "987450",
    "end": "994620"
  },
  {
    "text": "the graphs are connected and\ngiven a node you want to know who is the root of this graph.",
    "start": "994620",
    "end": "999750"
  },
  {
    "text": "So what we can do is essentially\nhave more concurrency by changing the way\nwe actually think about",
    "start": "999750",
    "end": "1005700"
  },
  {
    "text": "the algorithm. So rather than starting with\neach node and then, in a directed graph, following\nits successor --",
    "start": "1005700",
    "end": "1013000"
  },
  {
    "text": "so this is essentially order\nn, because for each node we have to follow n links -- we can think about it slightly\ndifferently.",
    "start": "1013000",
    "end": "1019579"
  },
  {
    "text": "So what if rather than finding\nthe successor and then finding that successor's successor, at\neach computational step we",
    "start": "1019580",
    "end": "1025560"
  },
  {
    "text": "start with a node and\nwe say who is your successor's successor? So we can converge in this\nexample in three steps.",
    "start": "1025560",
    "end": "1034000"
  },
  {
    "text": "So from five to six we can say\nwho is this successor? So who is the successor's\nsuccessor of five?",
    "start": "1034000",
    "end": "1041199"
  },
  {
    "text": "And that would be two. And similarly you can do that\nfor seven and so on. And so you keep asking\nthe question.",
    "start": "1041200",
    "end": "1047520"
  },
  {
    "text": "So you can distribute all\nthese data structures, repeatedly ask these questions\nout of all these end nodes, and it leads you to an order log\nn solution versus an order",
    "start": "1047520",
    "end": "1054840"
  },
  {
    "text": "n solution. But what have I done\nin each step? Well, I've actually created\nmyself and I've sort of",
    "start": "1054840",
    "end": "1063850"
  },
  {
    "text": "increased the amount of work\nthat I'm doing by order n. Right there.",
    "start": "1063850",
    "end": "1069309"
  },
  {
    "text": "Right. Yes. Because I've essentially for\neach node doing n queries, you",
    "start": "1069310",
    "end": "1076680"
  },
  {
    "text": "know, who's your successor's\nsuccessor? Whereas in a sequential case,\nyou know, I just need to do it once for each node.",
    "start": "1076680",
    "end": "1082980"
  },
  {
    "text": "And that works really well. So most strategies based on\nthis pattern of actually decomposing your computation\naccording to recursive pattern",
    "start": "1082980",
    "end": "1091390"
  },
  {
    "text": "lead you to doing much more work\nor some increase in the",
    "start": "1091390",
    "end": "1096860"
  },
  {
    "text": "amount of work. But you get this back\nin because you can decrease your execution. And so this is a good\ntradeoff that you",
    "start": "1096860",
    "end": "1103260"
  },
  {
    "text": "might want to consider. AUDIENCE: In the first one\norder n was sequential? PROFESSOR: Yeah, yeah.",
    "start": "1103260",
    "end": "1108520"
  },
  {
    "text": "It's a typo. Yeah. ",
    "start": "1108520",
    "end": "1114630"
  },
  {
    "text": "So organize by flow or organize\nby flow of data. And this is essentially\nthe pipeline model.",
    "start": "1114630",
    "end": "1121560"
  },
  {
    "text": "And we talked about this again\nin some of the recitations in terms of SPE to SPE\ncommunication.",
    "start": "1121560",
    "end": "1127960"
  },
  {
    "text": "Or do you want to organize\nbased on event-based mechanisms? So what these really come down\nto is, well, how regular is",
    "start": "1127960",
    "end": "1136049"
  },
  {
    "text": "the flow of data in\nyour application? If you have regular, let's say,\none-way flow through a",
    "start": "1136050",
    "end": "1141445"
  },
  {
    "text": "stable computation path -- so I've set up my sort of\nalgorithm structure. Data is flowing through\nit at a regular rate.",
    "start": "1141445",
    "end": "1148190"
  },
  {
    "text": "The computation graph isn't\nchanging very much. Then I can essentially pipeline\nthings really well.",
    "start": "1148190",
    "end": "1153490"
  },
  {
    "text": "And this could be a linear chain\nof computation or it could be sort of nonlinear. There could be branches\nin the graph.",
    "start": "1153490",
    "end": "1160780"
  },
  {
    "text": "And I can use that in a way to\nexploit pipeline parallelism. If I don't have sort of this\nnice, regular structure, it",
    "start": "1160780",
    "end": "1169440"
  },
  {
    "text": "could be events that are\ncreated at run time. So, for example, you're a\ncar wash attendant and a",
    "start": "1169440",
    "end": "1177450"
  },
  {
    "text": "new car comes in. So you have to find a garage to\nassign to it and then turn",
    "start": "1177450",
    "end": "1182480"
  },
  {
    "text": "on the car wash machine. So the dynamic threads are\ncreated based on sensory input",
    "start": "1182480",
    "end": "1188540"
  },
  {
    "text": "that comes in, then you might\nwant to use an events-based coordination. You have irregular\ncomputation.",
    "start": "1188540",
    "end": "1194250"
  },
  {
    "text": "The computation might vary based\non the data that comes into your system. And you might have unpredictable\ndata flow.",
    "start": "1194250",
    "end": "1203460"
  },
  {
    "text": "So in the pipeline model, the\nthings to consider is the pipeline throughput versus\nthe pipeline latency.",
    "start": "1203460",
    "end": "1210610"
  },
  {
    "text": "So the amount of concurrency\nin a pipeline is really limited by the number\nof stages. This is nothing new.",
    "start": "1210610",
    "end": "1216450"
  },
  {
    "text": "You've seen this, for example,\nin super scaled pipelines. And just as in this case, as in\nthe case of an architecture",
    "start": "1216450",
    "end": "1223460"
  },
  {
    "text": "pipeline, the amount of time\nit takes you to fill the pipeline and the amount of time\nit takes you to drain the pipeline can essentially\nlimit your parallelism.",
    "start": "1223460",
    "end": "1229940"
  },
  {
    "text": "So you want those to be really\nsmall compared to the actual computation that you spend\nin your pipeline.",
    "start": "1229940",
    "end": "1235930"
  },
  {
    "text": "And the performance metric is\nusually the throughput. How much data can you\npump through your pipeline per unit time?",
    "start": "1235930",
    "end": "1243630"
  },
  {
    "text": "So in video encoding, you know,\nit's the frames per second that you can produce. And the pipeline latency,\nthough, is important,",
    "start": "1243630",
    "end": "1250150"
  },
  {
    "text": "especially in a real-time\napplication where you need a result every 10 milliseconds. You know, your pacemaker for\nexample has to produce a beep",
    "start": "1250150",
    "end": "1257240"
  },
  {
    "text": "or a signal to your heart\nat specific rates. So you need to consider what\nis your pipeline throughput",
    "start": "1257240",
    "end": "1263780"
  },
  {
    "text": "versus your pipeline latency? And that can actually determine\nhow many stages you might want to actually decompose\nor organize your",
    "start": "1263780",
    "end": "1270250"
  },
  {
    "text": "application in. And in the event-based\ncoordination, these are",
    "start": "1270250",
    "end": "1275790"
  },
  {
    "text": "interactions of tasks over\nunpredictable intervals. And you're more prone to sort\nof deadlocks in these",
    "start": "1275790",
    "end": "1282920"
  },
  {
    "text": "applications. Because you might have cyclic\ndependencies where one event can't proceed until it gets\ndata from another event.",
    "start": "1282920",
    "end": "1289500"
  },
  {
    "text": "But it can't proceed until it\ngets data from another event. You can create sort of these\ncomplex interactions that",
    "start": "1289500",
    "end": "1295000"
  },
  {
    "text": "often lead to deadlock. So you have to sort of be very\ncareful in structuring things together so you don't end up\nwith feedback loops that block",
    "start": "1295000",
    "end": "1304679"
  },
  {
    "text": "computation progress. So given sort of these three\norganizational structures that",
    "start": "1304680",
    "end": "1310510"
  },
  {
    "text": "say, you know, I can organize\nmy computation by task or by the flow of data or by sort of\nthe pipeline nature of the",
    "start": "1310510",
    "end": "1317120"
  },
  {
    "text": "computation, what are the\nsupporting structures? How do I actually\nimplement these? And so there are many\ndifferent supporting",
    "start": "1317120",
    "end": "1323510"
  },
  {
    "text": "structures. I've identified sort of four\nthat occur most often in",
    "start": "1323510",
    "end": "1328880"
  },
  {
    "text": "literature and in\nbooks and common terminology that's used. And so those are SPMD, loop\nparallelism, the master/worker",
    "start": "1328880",
    "end": "1338090"
  },
  {
    "text": "pattern, and the fork/join\npattern.  In the SPMD pattern, you're\ntalking about a single",
    "start": "1338090",
    "end": "1346070"
  },
  {
    "text": "program, multiple\ndata concept. So here you just have\none program. You write it once and then you\nassign it to each of your",
    "start": "1346070",
    "end": "1353680"
  },
  {
    "text": "processors to run. So it's the same program. It just runs on different\nmachines. Now each program or each\ninstance of the code can have",
    "start": "1353680",
    "end": "1362710"
  },
  {
    "text": "different control flow\nthat it takes. So just because they're running\nthe same program doesn't mean the computation\nis happening in lock step.",
    "start": "1362710",
    "end": "1368830"
  },
  {
    "text": "That would be a sort of a SIMD\nor vector-like computation.",
    "start": "1368830",
    "end": "1373909"
  },
  {
    "text": "In this model you can actually\ntake independent control flow. It could be different behavior\nin each instance of the code.",
    "start": "1373910",
    "end": "1379810"
  },
  {
    "text": "But you're running the\nsame code everywhere. So this is slightly different,\nfor example, from what you've seen on Cell, where you have\nthe PPE thread that creates",
    "start": "1379810",
    "end": "1389480"
  },
  {
    "text": "SPE threads. Sometimes the SPE threads are\nthe same, but it's not always the case that the PPE\nthreads and the SPE",
    "start": "1389480",
    "end": "1395559"
  },
  {
    "text": "threads are the same. So in the SPMD model there are\nreally five steps that you do.",
    "start": "1395560",
    "end": "1401110"
  },
  {
    "text": "You initialize sort of your\ncomputation in the world of sort of code instances that\nyou're going to run.",
    "start": "1401110",
    "end": "1407250"
  },
  {
    "text": "And for each one you obtain\na unique identifier. And this usually helps them\nbeing able to determine who needs to communicate with who\nor ordering dependencies.",
    "start": "1407250",
    "end": "1417250"
  },
  {
    "text": "And you run the same program\non each processor. And what you need to do in this\ncase is also distribute",
    "start": "1417250",
    "end": "1424030"
  },
  {
    "text": "your data between each\nof the different instances of your code. And once, you know, each program\nis running, it's",
    "start": "1424030",
    "end": "1430870"
  },
  {
    "text": "computing on its data,\neventually you need to finalize in some way. And so that might mean doing a\nreduction to communicate all",
    "start": "1430870",
    "end": "1437789"
  },
  {
    "text": "the data to one processor to\nactually output the value.",
    "start": "1437790",
    "end": "1443160"
  },
  {
    "text": "And so we saw in SPMD an example\nfor the numerical integration for calculating\npi.",
    "start": "1443160",
    "end": "1449120"
  },
  {
    "text": "And if you remember, so we had\nthis very simple c loop. And we showed the MPI\nimplementation of the c loop.",
    "start": "1449120",
    "end": "1459780"
  },
  {
    "text": "And so in this code, what we're\ndoing is we're trying to determine different intervals. And for each interval we're\ngoing to calculate a value and",
    "start": "1459780",
    "end": "1467980"
  },
  {
    "text": "then in the MPI program we're\nessentially deciding how big",
    "start": "1467980",
    "end": "1473419"
  },
  {
    "text": "an interval each process\nshould run. So it's the same program. It runs on every single\nmachine or",
    "start": "1473420",
    "end": "1479640"
  },
  {
    "text": "every single processor. And each processor determines\nbased on its ID which interval",
    "start": "1479640",
    "end": "1486290"
  },
  {
    "text": "of the actual integration\nto do. And so in this model\nwe're distributing",
    "start": "1486290",
    "end": "1491320"
  },
  {
    "text": "work relatively evenly. Each processor is doing a\nspecific chunk that starts at",
    "start": "1491320",
    "end": "1496400"
  },
  {
    "text": "say some index i. And if I have 10 processors,\nI'm doing 100 steps.",
    "start": "1496400",
    "end": "1501810"
  },
  {
    "text": "Then you're doing i, i plus\n10, i plus 20 and so on. But I can do a different\ndistribution.",
    "start": "1501810",
    "end": "1508320"
  },
  {
    "text": "So the first is a block\ndistribution. I can do something called\na cyclic distribution. So in a cyclic distribution, I\ndistribute work sort of in a",
    "start": "1508320",
    "end": "1515420"
  },
  {
    "text": "round robin fashion or\nsome other mechanism. So here, you know,\neach processor --",
    "start": "1515420",
    "end": "1521940"
  },
  {
    "text": "sorry. In the block distribution I sort\nof start at interval i",
    "start": "1521940",
    "end": "1528480"
  },
  {
    "text": "and I go -- sorry. So each processor gets one\nentire slice here.",
    "start": "1528480",
    "end": "1536180"
  },
  {
    "text": "So I start here and I go\nthrough to completion. I start here and go through\nto completion.",
    "start": "1536180",
    "end": "1541760"
  },
  {
    "text": "In a cyclic distribution I might\ndo smaller slices of each one of those intervals.",
    "start": "1541760",
    "end": "1547669"
  },
  {
    "text": "And so I greyed out the\ncomponents for the block distribution to show you that\nfor a contrast here.",
    "start": "1547670",
    "end": "1555039"
  },
  {
    "text": " There are some challenges\nin the SPMD model.",
    "start": "1555040",
    "end": "1561610"
  },
  {
    "text": "And that is how do you actually split your data correctly? You have to distribute your data\nin a way that, you know,",
    "start": "1561610",
    "end": "1568410"
  },
  {
    "text": "doesn't increase contention on\nyour memory system, where each actual processor that's assigned\nthe computation has",
    "start": "1568410",
    "end": "1574899"
  },
  {
    "text": "data locally to actually\noperate on. And you want to achieve an\neven work distribution.",
    "start": "1574900",
    "end": "1581480"
  },
  {
    "text": "You know, do you need a dynamic\nload balancing scheme or can you use an alternative\npattern",
    "start": "1581480",
    "end": "1586740"
  },
  {
    "text": "if that's not suitable?  So the second pattern, as\nopposed to the SPMD pattern is",
    "start": "1586740",
    "end": "1594750"
  },
  {
    "text": "loop parallelism pattern. In this case, this is the best\nsuited when you actually have a programming model or a program\nthat you can't really",
    "start": "1594750",
    "end": "1602870"
  },
  {
    "text": "change a whole lot or that\nyou don't really want to change a whole lot. Or you have a programming model\nthat allows you to sort",
    "start": "1602870",
    "end": "1608679"
  },
  {
    "text": "of identify loops that take up\nmost of the computation and then insert annotations or some\nways to automatically",
    "start": "1608680",
    "end": "1615160"
  },
  {
    "text": "parallelize those loops. So we saw in the OpenMP example,\nyou have some loops",
    "start": "1615160",
    "end": "1621660"
  },
  {
    "text": "you can insert these\npragmas that say, this loop is parallel. And the compiler in the run-time\ntime system can",
    "start": "1621660",
    "end": "1628390"
  },
  {
    "text": "automatically partition this\nloop into smaller chunks. And then each chunk can\ncompute in parallel.",
    "start": "1628390",
    "end": "1635880"
  },
  {
    "text": "And you might apply this scheme\nin different ways depending on how well you\nunderstand your code.",
    "start": "1635880",
    "end": "1641870"
  },
  {
    "text": "Are you running on a shared\nmemory machine? You can't afford to do a whole\nlot of restructuring.",
    "start": "1641870",
    "end": "1647490"
  },
  {
    "text": "Communication costs might\nbe really expensive. ",
    "start": "1647490",
    "end": "1653480"
  },
  {
    "text": "In the master/worker pattern,\nthis is really starting to get closer to what we've\ndone with the Cell",
    "start": "1653480",
    "end": "1661040"
  },
  {
    "text": "recitations in the Cell labs. You have some world of\nindependent tasks and the",
    "start": "1661040",
    "end": "1666850"
  },
  {
    "text": "master essentially running and\ndistributing each of these tasks to different processors.",
    "start": "1666850",
    "end": "1673490"
  },
  {
    "text": "So in this case you'd get\nseveral advantages that you can leverage. If each of your tasks are varied\nin nature -- and they",
    "start": "1673490",
    "end": "1680830"
  },
  {
    "text": "might finish at different\ntimes or they require different kinds of resources,\nyou can use this model to sort",
    "start": "1680830",
    "end": "1686140"
  },
  {
    "text": "of view your machine as sort of\na non-symmetric processor. Not everybody is the same.",
    "start": "1686140",
    "end": "1692090"
  },
  {
    "text": "And you can use this model\nreally well for that. So you can distribute these and\nthen you can do dynamic",
    "start": "1692090",
    "end": "1698790"
  },
  {
    "text": "load balancing. Because as processors -- as workers finish you can ship\nthem more and more data.",
    "start": "1698790",
    "end": "1705120"
  },
  {
    "text": "So it has some particularly\nrelevant properties for",
    "start": "1705120",
    "end": "1715580"
  },
  {
    "text": "heterogeneous computations, but\nit's also really good for when you have a whole lot\nof parallelism in your application.",
    "start": "1715580",
    "end": "1720790"
  },
  {
    "text": "So something called\nembarrassingly parallel problems. So ray tracing,\nmolecular dynamics, a lot of",
    "start": "1720790",
    "end": "1726200"
  },
  {
    "text": "scientific applications have\nthese massive levels of parallelism. And you can use this essentially\nwork-queue based",
    "start": "1726200",
    "end": "1732059"
  },
  {
    "text": "mechanism that says I have all\nthese tasks and I'll just dispatch them to workers\nand compute.",
    "start": "1732060",
    "end": "1738700"
  },
  {
    "text": "And as I pointed out earlier,\nyou know, when do you define your entire computation\nto have completed? You know, sometimes you're\ncomputing a result until",
    "start": "1738700",
    "end": "1745160"
  },
  {
    "text": "you've reached some result. And often you're willing to\naccept a result within some",
    "start": "1745160",
    "end": "1753080"
  },
  {
    "text": "range of error. And you might have some\nmore threads that are still in flight. Do you terminate your\ncomputation then or not?",
    "start": "1753080",
    "end": "1759860"
  },
  {
    "text": "What are some issues with\nsynchronization? If you have so many threads that\nare running together, you know, does the communication\nbetween them to send out these",
    "start": "1759860",
    "end": "1766900"
  },
  {
    "text": "control messages say, I'm done,\nstart to overwhelm you? ",
    "start": "1766900",
    "end": "1773659"
  },
  {
    "text": "In the fork/join pattern -- this is really not conceptually\ntoo different in",
    "start": "1773660",
    "end": "1780610"
  },
  {
    "text": "my mind from the master/worker\nmodel, and also very relevant",
    "start": "1780610",
    "end": "1786570"
  },
  {
    "text": "to what we've done with Cell. The main difference might be\nthat you have tasks that are",
    "start": "1786570",
    "end": "1792299"
  },
  {
    "text": "dynamically created. So in the embarrassingly\nparallel case, you actually know the world of all your\npotential task that you're",
    "start": "1792300",
    "end": "1799169"
  },
  {
    "text": "going to run in parallel. In the fork/join join model\nsome new computation might come up as a result of, say,\nan event-based mechanism.",
    "start": "1799170",
    "end": "1806720"
  },
  {
    "text": "So a task might be created\ndynamically and then later terminated or they\nmight complete.",
    "start": "1806720",
    "end": "1811760"
  },
  {
    "text": "And so new ones come\nup as a result. AUDIENCE: It almost seems like\nyou are forking the task in",
    "start": "1811760",
    "end": "1821660"
  },
  {
    "text": "the forking model. And then keep assigning\ntasks to that. The fork/join model you\njust keep forking at",
    "start": "1821660",
    "end": "1828203"
  },
  {
    "text": "first virtual box. Might not be completely\nmatched to a number of processor available. ",
    "start": "1828203",
    "end": "1843429"
  },
  {
    "text": "fork them out.  PROFESSOR: So the process that's\nequating all these",
    "start": "1843430",
    "end": "1852420"
  },
  {
    "text": "threads or that's doing all the\nforking is often known as the parent and the\ntasks that are",
    "start": "1852420",
    "end": "1858600"
  },
  {
    "text": "generated are the children. And eventually essentially the\nparent can't continue or can't",
    "start": "1858600",
    "end": "1864580"
  },
  {
    "text": "resume until its children have\nsort of completed or have reached the join point.",
    "start": "1864580",
    "end": "1870130"
  },
  {
    "text": "And so those are really some of\nthe models that we've seen",
    "start": "1870130",
    "end": "1875730"
  },
  {
    "text": "already, in a lot of cases in\nthe recitations and labs for how you run your computations. And some of you have already\ndiscovered these and actually",
    "start": "1875730",
    "end": "1883100"
  },
  {
    "text": "are thinking about how your\nprojects should be sort of parallelized for your\nactual Cell demos.",
    "start": "1883100",
    "end": "1890160"
  },
  {
    "text": "Some of the other things that\nI'm just going to talk about are communication patterns. So two lectures ago you saw,\nfor example, that you have",
    "start": "1890160",
    "end": "1897300"
  },
  {
    "text": "point to point communication\nor you have broadcast communication. So in point to point\ncommunication, you have two",
    "start": "1897300",
    "end": "1903430"
  },
  {
    "text": "tasks that need to\ncommunicate. And they can send explicit\nmessages to each other. These could be control messages\nthat say I'm done or",
    "start": "1903430",
    "end": "1909762"
  },
  {
    "text": "I'm waiting for data. Or they could be data messages\nthat actually ships you a particular data element\nthat you might need.",
    "start": "1909762",
    "end": "1915640"
  },
  {
    "text": "And again we've seen\nthis with Cell. Broadcast says, you know,\nI have some result that",
    "start": "1915640",
    "end": "1921320"
  },
  {
    "text": "everybody needs. And so I send that out to\neverybody by some mechanism. There is no real broadcast\nmechanism on Cell.",
    "start": "1921320",
    "end": "1929529"
  },
  {
    "text": "The concept I'm going to talk\nabout though is the reduction mechanism, which really is the\ninverse of the broadcast. So",
    "start": "1929530",
    "end": "1935900"
  },
  {
    "text": "in the broadcast I have a data\nelement I need to send to everybody else. In the reduction, all of you\nhave data that I need or all",
    "start": "1935900",
    "end": "1943090"
  },
  {
    "text": "of us have data that each\nsomebody else needs. So what we need to do is\ncollectively bring that data together or group it together\nand generate an end result.",
    "start": "1943090",
    "end": "1954730"
  },
  {
    "text": "So a simple example of a\nreduction, you have some array",
    "start": "1954730",
    "end": "1960160"
  },
  {
    "text": "of elements that you want\nto add together. And sort of the result\nof the collective",
    "start": "1960160",
    "end": "1965240"
  },
  {
    "text": "operation is the end sum. So you have an array of four\nelements, A0, A1, A2, and A3.",
    "start": "1965240",
    "end": "1972790"
  },
  {
    "text": "And you can do a serial\nreduction. I can take A0 and\nadd it to A1. And that gives me a result.",
    "start": "1972790",
    "end": "1979360"
  },
  {
    "text": "And I can take A2 and\nadd that to it. And I can take A3 and\nadd that to it. And so at the end I'll have\nsort of calculated the sum",
    "start": "1979360",
    "end": "1986530"
  },
  {
    "text": "from A0 to A3. So this is essentially -- the\nserial reduction applies when",
    "start": "1986530",
    "end": "1992880"
  },
  {
    "text": "your operation is\nan associative. So the addition is\nassociative. So in this case I can actually\ndo something more intelligent.",
    "start": "1992880",
    "end": "2002100"
  },
  {
    "text": "And I think we talked about\nthat last time. I'm going to show you\nsome more examples. And often sort of the end result\nfollows a broadcast. It",
    "start": "2002100",
    "end": "2009980"
  },
  {
    "text": "says, here is the end result. Who are all the people\nthat need it? I'll sort of broadcast\nthat out so that",
    "start": "2009980",
    "end": "2015030"
  },
  {
    "text": "everybody has the result. If your operation isn't\nassociative, then you're essentially limited to\na serial process.",
    "start": "2015030",
    "end": "2021789"
  },
  {
    "text": "And so that's not very good from\na performance standpoint. ",
    "start": "2021790",
    "end": "2028860"
  },
  {
    "text": "Some of the tricks you can apply\nfor actually getting performance out of your\nreduction is to go to a",
    "start": "2028860",
    "end": "2034030"
  },
  {
    "text": "tree-based reduction model. So this might be very obvious. Rather than doing A0 and A1\nand then adding A2 to that",
    "start": "2034030",
    "end": "2041010"
  },
  {
    "text": "result, I can do A0\nand A1 together. In parallel I can\ndo A2 and A3. And then I can get those results\nand add them together.",
    "start": "2041010",
    "end": "2047830"
  },
  {
    "text": "So rather than doing n steps\nI can do log n steps. So this is particularly\nattractive when only one task",
    "start": "2047830",
    "end": "2055240"
  },
  {
    "text": "needs the result. So in the MPI program when we're\ndoing the integration to calculate pi, you know, one\nprocessor needs to print out",
    "start": "2055240",
    "end": "2061919"
  },
  {
    "text": "that value of pi.  But if you have a computation\nwhere more than one process",
    "start": "2061920",
    "end": "2069020"
  },
  {
    "text": "actually needs the result of\nthe reduction, there's actually a better mechanism you\ncan use that's sort of a",
    "start": "2069020",
    "end": "2075169"
  },
  {
    "text": "better alternative to the\ntree-based reduction followed by a broadcast. So you can do a recursive doubling reduction.",
    "start": "2075170",
    "end": "2084010"
  },
  {
    "text": "So at the end here, every\nprocess will have the result of the reduction without having\ndone the broadcast. So",
    "start": "2084010",
    "end": "2090550"
  },
  {
    "text": "we can start off as with the\ntree-based and add up A0 and A1 together.",
    "start": "2090550",
    "end": "2096379"
  },
  {
    "text": "But what we do is for each\nprocess that has a value, we sort of do a local exchange.",
    "start": "2096380",
    "end": "2101960"
  },
  {
    "text": "So from here we communicate\nthe value to here. And from here we communicate\nthe value to here. And so now these two processors\nthat had the value",
    "start": "2101960",
    "end": "2109900"
  },
  {
    "text": "independently now both have\na local sum, A0 to A1. And similarly we can sort of\nmake the similar symmetric",
    "start": "2109900",
    "end": "2117140"
  },
  {
    "text": "computation on the other side. And now we can communicate data\nfrom these two processors",
    "start": "2117140",
    "end": "2123340"
  },
  {
    "text": "here to come up with\nthe end -- ",
    "start": "2123340",
    "end": "2130150"
  },
  {
    "text": "PROFESSOR: It was there. All right. Must have been lost\nin the animation. So you actually do that the\nother way as well so that you",
    "start": "2130150",
    "end": "2137000"
  },
  {
    "text": "have the sum A0 to A3 on all\nthe different processors.",
    "start": "2137000",
    "end": "2142050"
  },
  {
    "text": "Sorry about the lost\nanimation. OK. So this is better than the\ntree-based approach with a",
    "start": "2142050",
    "end": "2147870"
  },
  {
    "text": "broadcast because you\nend up with local results of your reduction.",
    "start": "2147870",
    "end": "2153410"
  },
  {
    "text": "And rather than doing the\nbroadcast following the",
    "start": "2153410",
    "end": "2158780"
  },
  {
    "text": "tree-based reduction which takes\nn steps, you end up with an order n. Everybody has a result in order\nn versus an order 2n",
    "start": "2158780",
    "end": "2166450"
  },
  {
    "text": "process for the tree-based\nplus broadcast. AUDIENCE: On the Cell processor\nbut not in general.",
    "start": "2166450",
    "end": "2173200"
  },
  {
    "text": "PROFESSOR: Not in general. It depends on sort of the\narchitectural mechanism that you have for your network.",
    "start": "2173200",
    "end": "2180859"
  },
  {
    "text": "If you actually do need to sort\nof, you know, if you have a broadcast mechanism that has\nbus-based architecture where",
    "start": "2180860",
    "end": "2186205"
  },
  {
    "text": "you can deposit a local value,\neverybody can pull that value, then, yeah, it can be\nmore efficient.",
    "start": "2186205",
    "end": "2191410"
  },
  {
    "text": "Or on optical networks, you\ncan broadcast the data and everybody can just\nfuse it out. ",
    "start": "2191410",
    "end": "2198910"
  },
  {
    "text": "OK. So summarizing all the different\npatterns, so here",
    "start": "2198910",
    "end": "2204860"
  },
  {
    "text": "these are the actual mechanisms\nthat you would use for how you would implement\nthe different patterns.",
    "start": "2204860",
    "end": "2210800"
  },
  {
    "text": "So in the SPMD you would\nwrite the same program. In loop parallelism you have\nyour program and you might",
    "start": "2210800",
    "end": "2216700"
  },
  {
    "text": "annotate sort of some pragmas\nthat tell you how to parallelize your computation. In the master/worker model you\nmight have sort of a master",
    "start": "2216700",
    "end": "2224740"
  },
  {
    "text": "that's going to create threads\nand you actually know -- you might sort of have a very\ngood idea of what is the kind",
    "start": "2224740",
    "end": "2230800"
  },
  {
    "text": "of work you're going to have\nto do in each thread. In the fork/join model you\nhave more dynamism.",
    "start": "2230800",
    "end": "2236340"
  },
  {
    "text": "So you might create threads\non the fly. And you apply these sort of\nbased on appeal or what is",
    "start": "2236340",
    "end": "2245890"
  },
  {
    "text": "more suited in terms of\nimplementation to each of the different patterns for how you\nactually organize your data.",
    "start": "2245890",
    "end": "2251190"
  },
  {
    "text": "So in the task parallelism\nmodel, this is where you have a world of threads that you know\nyou're going to calculate",
    "start": "2251190",
    "end": "2257410"
  },
  {
    "text": "or that you're going to use\nfor your computation. And really you can use largely\nany one of these models.",
    "start": "2257410",
    "end": "2263610"
  },
  {
    "text": "So I used a ranking\nsystem where four stars is really good. One star is sort of bad or no\nstar means not well suited.",
    "start": "2263610",
    "end": "2271192"
  },
  {
    "text": "AUDIENCE: Sort of in\nCell because the inherit master there. Sometimes master/worker might\nget a little bit of a biasing",
    "start": "2271192",
    "end": "2278109"
  },
  {
    "text": "than this one. PROFESSOR: Right, so -- AUDIENCE: You don't have to pay\na cost of having master",
    "start": "2278110",
    "end": "2284599"
  },
  {
    "text": "PROFESSOR: Right. Right. Although you could use the\nCell master to do regular",
    "start": "2284600",
    "end": "2290230"
  },
  {
    "text": "computations as well. But, yes. So and the divide and conquer\nmodel, you know, might be",
    "start": "2290230",
    "end": "2298700"
  },
  {
    "text": "especially well suited for a\nfork and join because you're creating all these recursive\nsubproblems They might be heterogeneous.",
    "start": "2298700",
    "end": "2304080"
  },
  {
    "text": "In the nature of the computation\nthat you do, you might have more problems\ncreated dynamically. Fork/join really works\nwell for that.",
    "start": "2304080",
    "end": "2310300"
  },
  {
    "text": "And the fact, you know, the\nsubproblem structure that I showed, the graph of\nsort of division.",
    "start": "2310300",
    "end": "2315400"
  },
  {
    "text": "And then merging works really\nwell with the fork/join model. In the recursive, in the\ngeometric decomposition --",
    "start": "2315400",
    "end": "2325260"
  },
  {
    "text": "this is essentially your lab one\nexercise and the things we went over yesterday\nin the recitation.",
    "start": "2325260",
    "end": "2330329"
  },
  {
    "text": "You're taking data and you're\npartitioning over multiple processors to actually\ncompute in parallel.",
    "start": "2330330",
    "end": "2336960"
  },
  {
    "text": "So this could be SPMD\nimplementation or it could be a loop parallelism\nimplementation,",
    "start": "2336960",
    "end": "2342570"
  },
  {
    "text": "which we didn't do. Less suitable, the master/worker\nand fork/join, often because the geometric\ndecomposition applied some",
    "start": "2342570",
    "end": "2350960"
  },
  {
    "text": "distribution to the data which\nhas static properties that you can exploit in various ways. So you don't need to\npay the overhead of",
    "start": "2350960",
    "end": "2357420"
  },
  {
    "text": "master/worker or fork/join. Recursive data structures sort\nof have very specific models",
    "start": "2357420",
    "end": "2365670"
  },
  {
    "text": "that you can run with. Largely master/worker is a\ndecent implementation choice.",
    "start": "2365670",
    "end": "2372950"
  },
  {
    "text": "SPMD is another. And you're going to hear more\nabout sort of the pipeline",
    "start": "2372950",
    "end": "2378220"
  },
  {
    "text": "mechanism in the next talk so\nI'm not going to talk about that very much. Event-based coordination,\nlargely dynamic.",
    "start": "2378220",
    "end": "2384710"
  },
  {
    "text": "So fork/join works\nreally well. So one -- AUDIENCE: When you're buffering\nthem you could do",
    "start": "2384710",
    "end": "2390870"
  },
  {
    "text": "master/worker with pipelining? PROFESSOR: Yes, so next slide. So sort of these choices or\nthese tradeoffs aren't really",
    "start": "2390870",
    "end": "2398859"
  },
  {
    "text": "orthogonal. You can actually combine\nthem in different ways. And in a lot of applications\nwhat you might find is that",
    "start": "2398860",
    "end": "2405430"
  },
  {
    "text": "the different patterns compose\nhierarchically. And you actually want that\nin various ways --",
    "start": "2405430",
    "end": "2412540"
  },
  {
    "text": "for various reasons. So in the MPEG example, you\nknow, we had tasks here within each task and identified\nsome pipeline stages.",
    "start": "2412540",
    "end": "2421720"
  },
  {
    "text": "You know, here I have some\ndata parallelism so I can apply the loop pattern here.",
    "start": "2421720",
    "end": "2427589"
  },
  {
    "text": "And what I want to do is\nactually in my computation sort of express these different\nmechanisms so I can",
    "start": "2427590",
    "end": "2432869"
  },
  {
    "text": "understand sort of different\ntradeoffs. And for really large\napplications, there might be different patterns that are\nwell suited for the actual",
    "start": "2432870",
    "end": "2440630"
  },
  {
    "text": "computation that I'm doing. So I can combine things like\npipelining with a task-based",
    "start": "2440630",
    "end": "2445859"
  },
  {
    "text": "mechanism or data parallelism\nto actually get really good performance speedups. And one of the things that\nmight strike you as well,",
    "start": "2445860",
    "end": "2453849"
  },
  {
    "text": "heck, this is a whole lot of\nwork that I have to do to actually get my code in the\nright way so that I can actually take advantage of\nmy parallel architecture.",
    "start": "2453850",
    "end": "2461050"
  },
  {
    "text": "You know, I have to conceptually\nthink about the question the right way. I have to maybe restructure my\ncomputation in different ways",
    "start": "2461050",
    "end": "2467270"
  },
  {
    "text": "to actually exploit\nparallelism. Data distribution\nis really hard. I have to get that right.",
    "start": "2467270",
    "end": "2472810"
  },
  {
    "text": "Synchronization issues\nmight be a problem. And how much buffering\ndo I need to do between different tasks?",
    "start": "2472810",
    "end": "2478020"
  },
  {
    "text": "So the thing you're going to\nhear about in the next talk is, well, what if these things\nreally fall out naturally from the way you actually write the\nprogram, and if the way you",
    "start": "2478020",
    "end": "2487800"
  },
  {
    "text": "actually write your program\nmatches really well with the intuitive, sort of natural conceptualization of the problem.",
    "start": "2487800",
    "end": "2495000"
  },
  {
    "text": "And so I'll leave Bill\nto talk about that. And I'm going to stop here.",
    "start": "2495000",
    "end": "2500190"
  },
  {
    "text": "Any questions? AUDIENCE: We can take in\nsome questions and then everybody -- ",
    "start": "2500190",
    "end": "2510650"
  },
  {
    "text": "AUDIENCE: You talked about\nfork and join. When you have a parent thread\nthat spawns off to a child",
    "start": "2510650",
    "end": "2517734"
  },
  {
    "text": "thread, how do you keep your\nparent thread from using up the SPE?",
    "start": "2517735",
    "end": "2523970"
  },
  {
    "text": "PROFESSOR: So you have a\nfork/join where you have -- AUDIENCE: Most of the parents\nit might be the PPE.",
    "start": "2523970",
    "end": "2531780"
  },
  {
    "text": "And so if you just do fork/join,\nmight not really",
    "start": "2531780",
    "end": "2537430"
  },
  {
    "text": "use PPE unless you can, you\nknow, you have some time and you let it do some of the\ntask and come back.",
    "start": "2537430",
    "end": "2542670"
  },
  {
    "text": "AUDIENCE: So for our purposes\nwe shouldn't spawn off new threads by the SPEs?",
    "start": "2542670",
    "end": "2549109"
  },
  {
    "text": "PROFESSOR: So, yeah. So most of the threads\nthat are spawned off are done by the PPE.",
    "start": "2549110",
    "end": "2554160"
  },
  {
    "text": "So you have these -- in fact a good walk through\nin recitation yesterday.",
    "start": "2554160",
    "end": "2559200"
  },
  {
    "text": "You have the PPE. Essentially it sends messages to\nthe SPEs that says, create these threads and start\nrunning them.",
    "start": "2559200",
    "end": "2565190"
  },
  {
    "text": "Here's the data for them. And then these threads\nrun on the SPEs. And they just do local\ncomputation.",
    "start": "2565190",
    "end": "2570680"
  },
  {
    "text": "And then they send messages\nback to the PPE that says, we're done. So that essentially implements\nthe join mechanism.",
    "start": "2570680",
    "end": "2576422"
  },
  {
    "text": "AUDIENCE: On the other hand,\nif you are doing something like master slave way, and\nthen the SPE can send a",
    "start": "2576422",
    "end": "2585430"
  },
  {
    "text": "message and deliver another\njob into the PPE who feeds the master.",
    "start": "2585430",
    "end": "2591829"
  },
  {
    "text": "If SPE see there's some more\ncomputers, you can say, OK, look, put this into your keybord\nand keep sending messages and so the master can\nlook at that and update it.",
    "start": "2591830",
    "end": "2599740"
  },
  {
    "text": "So, you know, it's not only\nmaster who has to fork off but the slaves also. They still can send\ninformation back.",
    "start": "2599740",
    "end": "2608470"
  },
  {
    "text": "So you can think about\nsomething like very",
    "start": "2608470",
    "end": "2614180"
  },
  {
    "text": "confident that way. There are eight -- like if six\nSPE is running and you first",
    "start": "2614180",
    "end": "2623340"
  },
  {
    "text": "get something in there and SPE\nsays divide it will take one task and run that until the\nother one to the master will",
    "start": "2623340",
    "end": "2630066"
  },
  {
    "text": "finish it and here's my ID. Send me the message\nwhen it's done.",
    "start": "2630066",
    "end": "2635560"
  },
  {
    "text": "And so you fork that\nend and wait. So you can assume you can\ndo something like that. So it's almost master/slave but\nthe coordination is there.",
    "start": "2635560",
    "end": "2645210"
  },
  {
    "text": "The trouble with normally\nfork/join is if you create too",
    "start": "2645210",
    "end": "2651214"
  },
  {
    "text": "many threads. You are in like a thread hell\nbecause there are too many things to run. I don't know, can you SPE?",
    "start": "2651215",
    "end": "2657760"
  },
  {
    "text": "PROFESSOR: No. AUDIENCE: So you can't even do\nthat because of some physical limitation. You can't get take up 1000\nthreads you run another",
    "start": "2657760",
    "end": "2668410"
  },
  {
    "text": "master/slave thing yourself\nis because 1000 threads",
    "start": "2668410",
    "end": "2674180"
  },
  {
    "text": "on top of your SPEs. And that's going to\nbe locked threads.",
    "start": "2674180",
    "end": "2679819"
  },
  {
    "text": "PROFESSOR: Yeah. Contact switching on the\nSPEs is very expensive. So on the PlayStation\n3 you have six SPEs",
    "start": "2679820",
    "end": "2687980"
  },
  {
    "text": "available to you. So if you have a lot more than\nsix threads that you've created, essentially each\none runs to completion.",
    "start": "2687980",
    "end": "2695190"
  },
  {
    "text": "And then you swap that out and\nyou bring in -- well, that terminates. You deallocate it from the SPE\nand you bring in a new thread.",
    "start": "2695190",
    "end": "2702200"
  },
  {
    "text": "If you actually want to do more\nthread-like dynamic load balancing on the SPEs, it's\nnot well suited for that.",
    "start": "2702200",
    "end": "2708720"
  },
  {
    "text": "Just because the -- AUDIENCE: The best model there\nis master/slave. Because the PPE [UNINTELLIGIBLE PHRASE]",
    "start": "2708720",
    "end": "2714990"
  },
  {
    "text": "the master part. It will run more sequential\ncode. And when there's parallel send\n-- it will give it to you and",
    "start": "2714990",
    "end": "2722670"
  },
  {
    "text": "produce the work queue model\ntype and send stuff into SPE and feed that. So work queue type models\ncan be used there.",
    "start": "2722670",
    "end": "2730750"
  },
  {
    "text": "PROFESSOR: Yeah. And the SPMD model might not\nwork really well because you have this heterogeneity in the\nactual hardware, right.",
    "start": "2730750",
    "end": "2737030"
  },
  {
    "text": "So if I'm taking the same\nprogram running on the SPE versus the PPE, that code\nmight not be -- so I",
    "start": "2737030",
    "end": "2743360"
  },
  {
    "text": "essentially have to specialize\nthe code. And that starts to deviate\naway from the SPMD model. AUDIENCE: Something I think most\nof the code you write for",
    "start": "2743360",
    "end": "2751565"
  },
  {
    "text": "Cell will probably\nbe master/worker. And if you try to do something\nother than you should think",
    "start": "2751565",
    "end": "2756995"
  },
  {
    "text": "hard why that's the case. ",
    "start": "2756995",
    "end": "2762690"
  },
  {
    "text": "PROFESSOR: You can\ndo fork/join but you know, it's -- AUDIENCE: I mean you can't -- because you don't have\nvirtualization.",
    "start": "2762690",
    "end": "2769410"
  },
  {
    "text": "If you fork too much where are\nyou going to put those? PROFESSOR: Right. Sometimes you fork --",
    "start": "2769410",
    "end": "2774790"
  },
  {
    "text": "AUDIENCE: Yeah. but in that\nsense you -- should you fork too much? To keep work you want\nthe master. You can fork things --",
    "start": "2774790",
    "end": "2780590"
  },
  {
    "text": "you can do virtual fork and send\nthe work to the master and say, here, I forked\nsomething. Here's the work.",
    "start": "2780590",
    "end": "2786350"
  },
  {
    "text": "I mean, the key thing is\ndo the simplest thing. I mean, you guys have\ntwo weeks left.",
    "start": "2786350",
    "end": "2793320"
  },
  {
    "text": "And if you try doing anything\ncomplicated, you might end up with a big mess that's\nundebuggable.",
    "start": "2793320",
    "end": "2798730"
  },
  {
    "text": "Just do simple things. And I can vouch, parallelism\nis hard.",
    "start": "2798730",
    "end": "2805400"
  },
  {
    "text": "Debugging parallel code\nis even harder. So you're sort of trying to\npush the limits on the",
    "start": "2805400",
    "end": "2811300"
  },
  {
    "text": "complexity of messages going\nall over the world and the three different types of\nparallelism all trying to",
    "start": "2811300",
    "end": "2817615"
  },
  {
    "text": "compete in there. Just do the simple thing. Just get the simple\nthing working. First get the sequential code\nworking and keep adding more",
    "start": "2817615",
    "end": "2825005"
  },
  {
    "text": "and more story. And then make sure that\neach level it works. The problem with parallelism\nis because things that",
    "start": "2825005",
    "end": "2830349"
  },
  {
    "text": "determine if some bugs\nthat might show up. Data might be hard. But design absolutely matters.",
    "start": "2830350",
    "end": "2837770"
  },
  {
    "text": "Another thing I think,\nespecially for doing demos and stuff would be nice, would be to\nhave a knob that basically",
    "start": "2837770",
    "end": "2844570"
  },
  {
    "text": "you can tune. So you can say, OK, no SPEs. Everything running in\nPPE one is two is.",
    "start": "2844570",
    "end": "2849624"
  },
  {
    "text": "So you can actually see\nhopefully in your code how how things move for the demo part.",
    "start": "2849624",
    "end": "2856692"
  },
  {
    "start": "2856693",
    "end": "2863200"
  },
  {
    "text": "PROFESSOR: You had a question? All right. We'll take a brief\nbreak and do the quizzes in the meantime.",
    "start": "2863200",
    "end": "2869380"
  },
  {
    "start": "2869380",
    "end": "2930862"
  }
]