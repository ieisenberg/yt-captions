[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "17795"
  },
  {
    "text": "at ocw.mit.edu. ",
    "start": "17795",
    "end": "22800"
  },
  {
    "text": "GILBERT STRANG: Well, OK,\nI am happy to be back, and I am really happy\nabout the project",
    "start": "22800",
    "end": "29910"
  },
  {
    "text": "proposals that are coming in. This is like, OK, this is really\na good part of the course.",
    "start": "29910",
    "end": "36840"
  },
  {
    "text": "And so keep them coming, and\nI'm happy to give whatever",
    "start": "36840",
    "end": "42360"
  },
  {
    "text": "feedback I can on\nthose proposals, and do make a start there.",
    "start": "42360",
    "end": "49120"
  },
  {
    "text": "They're really good, and\nif some are completed before the end of\nthe semester and we",
    "start": "49120",
    "end": "55230"
  },
  {
    "text": "can to offer you a\nchance to report on them,",
    "start": "55230",
    "end": "60860"
  },
  {
    "text": "that that's good too. So well done with\nthose proposals.",
    "start": "60860",
    "end": "67930"
  },
  {
    "text": "So today, I'm\njumping to part six. So part six and part\nseven are optimization",
    "start": "67930",
    "end": "75870"
  },
  {
    "text": "which is the fundamental\nalgorithm that goes into deep learning. So we've got to start\nwith optimization.",
    "start": "75870",
    "end": "82140"
  },
  {
    "text": "Everybody has to\nget that picture, and then part seven\nwill be the structure",
    "start": "82140",
    "end": "89580"
  },
  {
    "text": "of CNNs, Convolution\nNeural Nets, and all kinds of applications.",
    "start": "89580",
    "end": "97560"
  },
  {
    "text": "And so can we start\nwith optimization? So first, can I like\nget the basic facts",
    "start": "97560",
    "end": "104370"
  },
  {
    "text": "about three terms\nof a Taylor series?",
    "start": "104370",
    "end": "109840"
  },
  {
    "text": "So that's the typical. It's seldom that we would\ngo up to third derivatives",
    "start": "109840",
    "end": "115590"
  },
  {
    "text": "in optimization. So that's the most useful\napproximation to a function.",
    "start": "115590",
    "end": "121920"
  },
  {
    "text": "Everybody recognizes it. Here, I'm thinking of\nF as just one function,",
    "start": "121920",
    "end": "128729"
  },
  {
    "text": "and x as just one\nvariable, but now I really want to go\nto more variables.",
    "start": "128729",
    "end": "138180"
  },
  {
    "text": "So what do I have\nto change if F is a function of more variables?",
    "start": "138180",
    "end": "146790"
  },
  {
    "text": "So now, I'm thinking of x as-- well, now let me see.",
    "start": "146790",
    "end": "152730"
  },
  {
    "text": " Yeah, I want n variables here.",
    "start": "152730",
    "end": "162470"
  },
  {
    "text": "x is x1 up to xn. ",
    "start": "162470",
    "end": "168830"
  },
  {
    "text": "So just to get\nthe words straight so we can begin on\noptimization, so what",
    "start": "168830",
    "end": "176800"
  },
  {
    "text": "will be the similar step\nso the function F at x--",
    "start": "176800",
    "end": "182740"
  },
  {
    "text": "remember, x is n variables. OK? Now, what do I have?",
    "start": "182740",
    "end": "188350"
  },
  {
    "text": "Delta x, so what's the\npoint about delta x now? It's a vector, delta\nx1 to delta xn,",
    "start": "188350",
    "end": "197500"
  },
  {
    "text": "and what about the\nderivative of F? It's a vector too,\nthe derivative",
    "start": "197500",
    "end": "202810"
  },
  {
    "text": "of F with respect to\nx1, the derivative of F with respect to x2, and so on.",
    "start": "202810",
    "end": "209660"
  },
  {
    "text": "What do I have to\nchange about that? I know those guys are vectors,\nso it's their dot product.",
    "start": "209660",
    "end": "216310"
  },
  {
    "text": "So it's delta x transpose\nat vector times this dF/dx.",
    "start": "216310",
    "end": "224360"
  },
  {
    "text": " So now I'm replacing this\nby all the derivatives,",
    "start": "224360",
    "end": "232620"
  },
  {
    "text": "and it's the gradient.",
    "start": "232620",
    "end": "238040"
  },
  {
    "text": "So the gradient of F at\nx is the derivatives--",
    "start": "238040",
    "end": "243819"
  },
  {
    "text": " let's see.",
    "start": "243820",
    "end": "249920"
  },
  {
    "text": "It's essential to get the\nnotation straight here. Yeah, so it'll be the\npartial derivatives",
    "start": "249920",
    "end": "257140"
  },
  {
    "text": "of the function F. So grad F\nis the partial derivatives of F",
    "start": "257140",
    "end": "266290"
  },
  {
    "text": "with respect to x1 down to\npartial derivative with respect to xn.",
    "start": "266290",
    "end": "272650"
  },
  {
    "text": "OK, good. That's the linear term, and\nnow what's the quadratic term? 1/2, now delta x isn't\na scalar anymore.",
    "start": "272650",
    "end": "282009"
  },
  {
    "text": "It's a vector. So I'm going to have\ndelta x transpose",
    "start": "282010",
    "end": "287500"
  },
  {
    "text": "and a delta x, and\nwhat goes in between is the second\nderivatives, but I've",
    "start": "287500",
    "end": "295240"
  },
  {
    "text": "got a function of n variables.  So now, I have a matrix\nof second derivatives,",
    "start": "295240",
    "end": "304240"
  },
  {
    "text": "and I'll call it H. This is the\nmatrix of second derivatives,",
    "start": "304240",
    "end": "310509"
  },
  {
    "text": "Hjk is the second derivative\nof F with respect to xj and xk,",
    "start": "310510",
    "end": "319840"
  },
  {
    "text": "and what's the\nname for this guy? The Hessian, Hessian matrix.",
    "start": "319840",
    "end": "326485"
  },
  {
    "text": " How the Hessians got into\nthis picture I don't know.",
    "start": "326485",
    "end": "333260"
  },
  {
    "text": "The only Hessians\nI know are the ones who fought in the\nRevolutionary War for somebody.",
    "start": "333260",
    "end": "339530"
  },
  {
    "text": "Who? Which side were they on? I think maybe the wrong side. The French were\non our side and--",
    "start": "339530",
    "end": "347470"
  },
  {
    "text": "Anyway, Hessian\nmatrix, and what are the facts about that matrix? Well, the first fact is\nthat it's [INAUDIBLE]",
    "start": "347470",
    "end": "354220"
  },
  {
    "text": "and the key fact\nis it's symmetric. ",
    "start": "354220",
    "end": "360199"
  },
  {
    "text": "Yeah. OK, and again, it's\nan approximation.",
    "start": "360200",
    "end": "367420"
  },
  {
    "text": "And everybody recognizes\nthat if n is very large,",
    "start": "367420",
    "end": "372880"
  },
  {
    "text": "and we have a function\nof many variables. Then, we had n derivatives\nto compute here,",
    "start": "372880",
    "end": "379539"
  },
  {
    "text": "and about 1/2 n\nsquared derivatives. The 1/2 comes from the\nsymmetry, but the key point",
    "start": "379540",
    "end": "385960"
  },
  {
    "text": "is the n squared derivatives\nto compute there. So computing the\ngradient is feasible",
    "start": "385960",
    "end": "396240"
  },
  {
    "text": "if n is small or\nmoderately large. Actually, by using\nautomatic differentiation,",
    "start": "396240",
    "end": "403900"
  },
  {
    "text": "the key idea of\nback propagation, back prop, you can\nspeed up the computation",
    "start": "403900",
    "end": "414750"
  },
  {
    "text": "of derivatives quite amazingly. But still for the size\nof deep learning problems",
    "start": "414750",
    "end": "422400"
  },
  {
    "text": "that's out of reach. OK. So that's the\npicture, and then I",
    "start": "422400",
    "end": "430139"
  },
  {
    "text": "will want to use this\nto solve equations.",
    "start": "430140",
    "end": "436170"
  },
  {
    "text": "There is a parallel\npicture for a vector f.",
    "start": "436170",
    "end": "445140"
  },
  {
    "text": "So now, this is a\nvector function. This is f1 of x up to fn\nof x, an x is x1 to xn.",
    "start": "445140",
    "end": "459210"
  },
  {
    "text": " So I have n functions\nof n variables,",
    "start": "459210",
    "end": "465950"
  },
  {
    "text": "n functions of n variables. Well, that's exactly what\nI have in the gradient.",
    "start": "465950",
    "end": "471380"
  },
  {
    "text": " Think of these two as\nparallel, the parallel",
    "start": "471380",
    "end": "477230"
  },
  {
    "text": "being f corresponds\nto the gradient of F, n functions of n variables.",
    "start": "477230",
    "end": "486980"
  },
  {
    "text": " OK. ",
    "start": "486980",
    "end": "495009"
  },
  {
    "text": "Now maybe, what I'm after\nhere is to solve f equals 0. So I'm going to think about\nthe f at x plus delta x,",
    "start": "495010",
    "end": "505570"
  },
  {
    "text": "so it starts with f of x. And then we have the\ncorrection times the matrix",
    "start": "505570",
    "end": "514539"
  },
  {
    "text": "of first derivatives, and\nwhat's the name for that matrix",
    "start": "514539",
    "end": "522309"
  },
  {
    "text": "of first derivatives? Well, if I'm just\ngiven n functions--",
    "start": "522309",
    "end": "531670"
  },
  {
    "text": " yeah, what am I after here?",
    "start": "531670",
    "end": "536830"
  },
  {
    "text": "I'm looking for the Jacobian.",
    "start": "536830",
    "end": "545120"
  },
  {
    "text": "So here we'll go the Jacobian,\nJ. This is the Jacobian named",
    "start": "545120",
    "end": "550580"
  },
  {
    "text": "after Jacoby, Jacobian matrix.",
    "start": "550580",
    "end": "556320"
  },
  {
    "text": "And what are its entries? J, the jk entry\nis the derivative",
    "start": "556320",
    "end": "563060"
  },
  {
    "text": "of the J function with\nrespect to the kth variable,",
    "start": "563060",
    "end": "570779"
  },
  {
    "text": "and I'm stopping at\nfirst order there. OK, so these are sort of\nlike facts of calculus,",
    "start": "570780",
    "end": "578360"
  },
  {
    "text": "facts of 18.02 you could say. Multivariable calculus,\nthat's the point.",
    "start": "578360",
    "end": "584550"
  },
  {
    "text": "Notice that we're doing just\nlike the first half of 18.02, just do differential calculus,\nderivatives, Taylor series.",
    "start": "584550",
    "end": "592880"
  },
  {
    "text": "We're not doing\nmultiple integrals. That's not part\nof our world here. OK, so that's the background.",
    "start": "592880",
    "end": "600330"
  },
  {
    "text": "Now, I want to look\nat optimization.",
    "start": "600330",
    "end": "605840"
  },
  {
    "text": "So over here, I\nwant to optimize--",
    "start": "605840",
    "end": "612610"
  },
  {
    "text": "well, over here, let me\ntry to minimize F of x,",
    "start": "612610",
    "end": "624290"
  },
  {
    "text": "and I'll be in the\nvector case here. And over here, I want\nto solve f equals 0,",
    "start": "624290",
    "end": "638610"
  },
  {
    "text": "and of course, that\nmeans f of 1 equals 0",
    "start": "638610",
    "end": "645329"
  },
  {
    "text": "all the way along\nto fn equals 0. Here, I have n equations,\nand n unknowns.",
    "start": "645330",
    "end": "654079"
  },
  {
    "start": "654080",
    "end": "659720"
  },
  {
    "text": "Let me start with\nthat one, and I'll start with Newton's\nmethod, Newton's method",
    "start": "659720",
    "end": "665449"
  },
  {
    "text": "to solve these n\nequations and n unknowns. OK, so Newton, Newton's\nmethod which is often",
    "start": "665450",
    "end": "682440"
  },
  {
    "text": "not presented in 18.02. That's a crime, because\nthat's the big application",
    "start": "682440",
    "end": "689130"
  },
  {
    "text": "of gradients in Jacobians. OK, so I'm trying to solve\nn equations and n unknowns,",
    "start": "689130",
    "end": "697149"
  },
  {
    "text": "and so I want f at x\nplus delta x to be 0.",
    "start": "697150",
    "end": "702930"
  },
  {
    "text": "Right? So I want f of x\nplus delta x to be 0. So f at x plus delta x is--",
    "start": "702930",
    "end": "709890"
  },
  {
    "text": "I'm putting in a 0. I'm just copying that equation-- is f at where I am.",
    "start": "709890",
    "end": "718440"
  },
  {
    "text": "Let me use K for\nthe case iteration. So I'm at a point xK.",
    "start": "718440",
    "end": "724490"
  },
  {
    "text": "I want to get to\na point xK plus 1. And so I have 0 is f of\nx plus J, at that point,",
    "start": "724490",
    "end": "734889"
  },
  {
    "text": "times delta x which\nis xK plus 1 minus xK.",
    "start": "734890",
    "end": "741700"
  },
  {
    "text": "Good. That's Newton's method. Of course, 0 isn't quite true.",
    "start": "741700",
    "end": "749010"
  },
  {
    "text": "Well, 0 will be true if I'm\nconstructing xK plus 1 here.",
    "start": "749010",
    "end": "755250"
  },
  {
    "text": "I'm constructing xK plus 1. OK. So let me just rewrite that,\nand we've got Newton's method.",
    "start": "755250",
    "end": "763410"
  },
  {
    "text": "So we're looking for this\nchange, xK plus 1 minus xK.",
    "start": "763410",
    "end": "772750"
  },
  {
    "text": "I'll put it on this side\nas plus xK, so that's this.",
    "start": "772750",
    "end": "780010"
  },
  {
    "text": "Now, I have to invert\nthat and put it on the other side\nof the equation. So that will go with a minus.",
    "start": "780010",
    "end": "786280"
  },
  {
    "text": "This guy will be\ninverted and f at xK.",
    "start": "786280",
    "end": "792880"
  },
  {
    "text": " So that's Newton's methods. It's natural.",
    "start": "792880",
    "end": "799980"
  },
  {
    "text": "So let me just repeat that. You see where the xK plus\n1 minus xK is sitting?",
    "start": "799980",
    "end": "806140"
  },
  {
    "text": "Right? And I moved f of xK to the\nother side with a minus sign,",
    "start": "806140",
    "end": "812930"
  },
  {
    "text": "and then I multiplied through\nby J inverse, so I got that. So that's Newton's method\nfor a system of equations,",
    "start": "812930",
    "end": "825020"
  },
  {
    "text": "and over there, I'm going to\nwrite down Newton's method for minimizing a function. This is such basic stuff\nthat we have to begin here.",
    "start": "825020",
    "end": "833890"
  },
  {
    "text": "Let me even begin with an\nextremely straightforward example of Newton's method here.",
    "start": "833890",
    "end": "842440"
  },
  {
    "text": "Suppose my function--\nsuppose I've only got one function actually.",
    "start": "842440",
    "end": "850900"
  },
  {
    "text": "Suppose I only had one function. So suppose my function\nis x squared minus 9,",
    "start": "850900",
    "end": "861240"
  },
  {
    "text": "and I want to solve\nf of x equals 0. I want to find the\nsquare root of 9.",
    "start": "861240",
    "end": "868990"
  },
  {
    "text": "OK, so what is\nNewton's method for it? My point is just to see how\nNewton's method is written",
    "start": "868990",
    "end": "877360"
  },
  {
    "text": "and then rewrite it a little bit\nso that we see the convergence.",
    "start": "877360",
    "end": "883200"
  },
  {
    "text": "OK, so of course,\nthe Jacobian is 2x. So Newton's method\nsays that xK plus 1--",
    "start": "883200",
    "end": "891579"
  },
  {
    "text": "I'm just going to copy\nthat Newton's method-- minus 1 over 2xK.",
    "start": "891580",
    "end": "899170"
  },
  {
    "text": "Right? That's the derivative times f at\nxK which is xK squared minus 9.",
    "start": "899170",
    "end": "908680"
  },
  {
    "text": " OK. We followed the formula,\nthis determines xK plus 1,",
    "start": "908680",
    "end": "917720"
  },
  {
    "text": "and let's simplify it. So here I have xK minus\nthat looks like 1/2 of xK,",
    "start": "917720",
    "end": "926500"
  },
  {
    "text": "so I think I have\n1/2xK, and then this times this is\n9/2 of 1 over xK.",
    "start": "926500",
    "end": "940136"
  },
  {
    "text": "Is that right?  1/2 of xk from this stuff\nand plus 9/2 of 1 over xK.",
    "start": "940137",
    "end": "950100"
  },
  {
    "text": "OK.  Can I just like check that\nI know the answer is 3?",
    "start": "950100",
    "end": "959850"
  },
  {
    "text": "Can I be sure that I\nget the right answer, 3? That if xK was exactly 3, then\nof course, I expect xK plus 1",
    "start": "959850",
    "end": "970170"
  },
  {
    "text": "to stay at 3. So does that happen? So 1/2 of 3 and 9/2 of 1/3,\nwhat's that, 1/2 of 3 and 9/2",
    "start": "970170",
    "end": "984330"
  },
  {
    "text": "of 1/3? OK, that's 3/2 and 3/2. That's 6/2, and that's 3.",
    "start": "984330",
    "end": "991579"
  },
  {
    "text": "OK.  So we've checked that the method\nis consistent which just means",
    "start": "991580",
    "end": "998870"
  },
  {
    "text": "we kept the algebra straight. But then the really important\npoint about Newton's method",
    "start": "998870",
    "end": "1006430"
  },
  {
    "text": "is to discover how\nfast it converges. So now let me do\nxK plus 1 minus 3.",
    "start": "1006430",
    "end": "1015350"
  },
  {
    "text": "So now, I'm looking at the\nerror which is, I hope,",
    "start": "1015350",
    "end": "1020649"
  },
  {
    "text": "approaching 0. Is it approaching 0? How quickly is it approaching 0? These are the fundamental\nquestions of optimization.",
    "start": "1020650",
    "end": "1029470"
  },
  {
    "text": "So I'm going to subtract\n3 from both sides somehow.",
    "start": "1029470",
    "end": "1034939"
  },
  {
    "text": "OK, from here, I guess,\nI'm going to subtract 3.",
    "start": "1034940",
    "end": "1040970"
  },
  {
    "text": "So I was just checking\nthat it was correct. OK. Now, so xK plus 1\nminus 3, I'm going",
    "start": "1040970",
    "end": "1050309"
  },
  {
    "text": "to subtract 3 from both sides. I'm going to subtract 3\nthere, and then I hope that--",
    "start": "1050310",
    "end": "1058190"
  },
  {
    "text": "that box is what goes down here. Right? Subtracted 3 from both sides, so\nI'm hoping now things go to 0.",
    "start": "1058190",
    "end": "1068960"
  },
  {
    "text": "OK, so what do I have there? Let me factor out the 1 over xK.",
    "start": "1068960",
    "end": "1075330"
  },
  {
    "text": " So what do I have then left?",
    "start": "1075330",
    "end": "1081850"
  },
  {
    "text": "1 over xK, so there's a\n9/2 from there, 1 over xK.",
    "start": "1081850",
    "end": "1088750"
  },
  {
    "text": "So I really have\n1/2 of xK squared, because I've divided by an xK.",
    "start": "1088750",
    "end": "1095080"
  },
  {
    "text": "And this minus 3, I\nbetter put minus 3xK,",
    "start": "1095080",
    "end": "1101120"
  },
  {
    "text": "because I'm dividing by xK. I claim that that's-- now I've got it.",
    "start": "1101120",
    "end": "1106360"
  },
  {
    "text": " And let's see, let\nme take out the 2--",
    "start": "1106360",
    "end": "1114970"
  },
  {
    "text": "2, forget these 2s,\nand make that a 6.",
    "start": "1114970",
    "end": "1121480"
  },
  {
    "text": "So I have 1 over 2xK times\n9 plus xK squared minus 6.",
    "start": "1121480",
    "end": "1127210"
  },
  {
    "text": "Anything good about that?  We hope so. ",
    "start": "1127210",
    "end": "1136450"
  },
  {
    "text": "We hope that that is\nsomething attractive. So this is, again, the\nerror at set K plus 1,",
    "start": "1136450",
    "end": "1145730"
  },
  {
    "text": "and it's 1 over 2xK times\nthis thing in brackets--",
    "start": "1145730",
    "end": "1151940"
  },
  {
    "text": "9 plus xK squared minus 6xK. And we recognize that\nas xK minus 3 squared.",
    "start": "1151940",
    "end": "1163235"
  },
  {
    "text": " xK squared minus 6 of them plus\n9, that's xK minus 3 squared.",
    "start": "1163235",
    "end": "1174710"
  },
  {
    "text": "OK, that was the\ngoal, of course.",
    "start": "1174710",
    "end": "1180370"
  },
  {
    "text": "That's the goal that shows why\nNewton's method is fantastic. If you can execute it, if\nyou can start near enough,",
    "start": "1180370",
    "end": "1188080"
  },
  {
    "text": "notice that-- so how do I describe\nthis great equation?",
    "start": "1188080",
    "end": "1194750"
  },
  {
    "text": "It says that the error is\nsquared at every step, squared at every step.",
    "start": "1194750",
    "end": "1200770"
  },
  {
    "text": "So if I'm converging to a\nlimit, it will satisfy the--",
    "start": "1200770",
    "end": "1208132"
  },
  {
    "text": "it'll be 3, or I guess\nminus 3, is that possible?",
    "start": "1208132",
    "end": "1215049"
  },
  {
    "text": "Yeah, minus 3 is\nanother solution here. So we've got two solutions.",
    "start": "1215050",
    "end": "1220809"
  },
  {
    "text": "Newton's method\ncould converge to 3.  Am I right, it could\nconverge to minus 3?",
    "start": "1220810",
    "end": "1229190"
  },
  {
    "text": "So I'd have a similar equation\nsort of centered at minus 3,",
    "start": "1229190",
    "end": "1234919"
  },
  {
    "text": "or does it always\ndo one of those? ",
    "start": "1234920",
    "end": "1240710"
  },
  {
    "text": "It could blow up.  So there are sort of\nregions of attraction.",
    "start": "1240710",
    "end": "1248429"
  },
  {
    "text": "They're all the starting\npoints that approach 3, and the whole point\nof that equation is with quadratic\nconvergence the error",
    "start": "1248430",
    "end": "1257870"
  },
  {
    "text": "being squared at every step. It zooms in on 3. Then, there is all\nthe starting points",
    "start": "1257870",
    "end": "1264060"
  },
  {
    "text": "that would go to minus\n3, and then there are the starting points\nthat would blow up.",
    "start": "1264060",
    "end": "1270920"
  },
  {
    "text": "And those, maybe for\nthis very simple problem, the picture is not too\ndifficult to sort out",
    "start": "1270920",
    "end": "1277760"
  },
  {
    "text": "those three regions.  And this is allowing for a\nvector, two equations or n",
    "start": "1277760",
    "end": "1289010"
  },
  {
    "text": "equations, then\nwe're in n variables, and really you get\nbeautiful pictures.",
    "start": "1289010",
    "end": "1295309"
  },
  {
    "text": "You get some of the type\nof pictures that gave rise to these books on fractals,\npicture books on fractals",
    "start": "1295310",
    "end": "1305030"
  },
  {
    "text": "for these basins of attraction. Does the starting point lead\nyou to one of the solutions,",
    "start": "1305030",
    "end": "1312410"
  },
  {
    "text": "or does it lead you to infinity? Here, that would be interesting\nto just draw it for this,",
    "start": "1312410",
    "end": "1319790"
  },
  {
    "text": "but the essential point is\nthe quadratic convergence, if it's close enough.",
    "start": "1319790",
    "end": "1325670"
  },
  {
    "text": "You see that it has to be close. If x0 is pretty near 3, then\nthis is about 1/6 of that,",
    "start": "1325670",
    "end": "1335760"
  },
  {
    "text": "and there would be a good region\nof attraction in this case.",
    "start": "1335760",
    "end": "1341430"
  },
  {
    "text": "OK. So that's Newton's\nmethod for equations.",
    "start": "1341430",
    "end": "1349640"
  },
  {
    "text": "And now I want to\ndo Newton's method. I just want to convert\nall those words over to Newton's method\nfor optimization.",
    "start": "1349640",
    "end": "1358450"
  },
  {
    "text": "So remember, these boards\nwere solving f equals 0.",
    "start": "1358450",
    "end": "1365809"
  },
  {
    "text": "This board is\nminimizing capital F, and what's the\nconnection between them?",
    "start": "1365810",
    "end": "1370860"
  },
  {
    "text": "Well of course, this corresponds\nto solving the gradient",
    "start": "1370860",
    "end": "1382200"
  },
  {
    "text": "equals 0. At a minimum, if\nI'm minimizing, I'm",
    "start": "1382200",
    "end": "1387419"
  },
  {
    "text": "finding a point where all\nthe first derivatives are 0. So that will be the\nmatch between these.",
    "start": "1387420",
    "end": "1396060"
  },
  {
    "text": "This grad F in this picture is\nthe small f in that picture.",
    "start": "1396060",
    "end": "1402510"
  },
  {
    "text": "OK.  Now, I guess here I have--",
    "start": "1402510",
    "end": "1408710"
  },
  {
    "text": "and this is sort of the\nheart of our applications",
    "start": "1408710",
    "end": "1413929"
  },
  {
    "text": "to deep learning-- we have\nvery complicated loss functions to minimize,\nfunctions of thousands",
    "start": "1413930",
    "end": "1420259"
  },
  {
    "text": "or hundreds of\nthousands of variables. OK. So that means that we would\nlike to use Newton's method,",
    "start": "1420260",
    "end": "1427840"
  },
  {
    "text": "but often we can't. So I need him to put\ndown here two methods--",
    "start": "1427840",
    "end": "1434380"
  },
  {
    "text": "one that doesn't involve\nthose high second derivatives and Newton's that does.",
    "start": "1434380",
    "end": "1442750"
  },
  {
    "text": "So first, I'll write down a\nmethod that does not involve,",
    "start": "1442750",
    "end": "1450430"
  },
  {
    "text": "so method one, and this\nwill be steepest descent.",
    "start": "1450430",
    "end": "1456390"
  },
  {
    "start": "1456390",
    "end": "1465040"
  },
  {
    "text": "And what is that? That says that xK plus 1--\nthe new x is the old x minus--",
    "start": "1465040",
    "end": "1475430"
  },
  {
    "text": "steepest descent means that I\nmove in the steepest direction which is the direction\nof the gradient of F.",
    "start": "1475430",
    "end": "1485240"
  },
  {
    "text": "I move some distance,\nand I better have freedom to decide what\nthat distance should be.",
    "start": "1485240",
    "end": "1491360"
  },
  {
    "text": "So this is a step size, s, or in\nthe language of deep learning,",
    "start": "1491360",
    "end": "1501670"
  },
  {
    "text": "it's often called\nthe learning rate, so if you see learning rate. ",
    "start": "1501670",
    "end": "1511909"
  },
  {
    "text": "OK. ",
    "start": "1511910",
    "end": "1519000"
  },
  {
    "text": "So and it's natural\nto choose sK. We're going along, do you see\nwhat this right-hand side looks",
    "start": "1519000",
    "end": "1527340"
  },
  {
    "text": "like? I'm at a point in n dimensions. We're in n dimensions here.",
    "start": "1527340",
    "end": "1534640"
  },
  {
    "text": "We have functions\nof n variables. There is a vector. There is a direction to\nmove down the steepest slope",
    "start": "1534640",
    "end": "1543780"
  },
  {
    "text": "of the graph. And here is a distance to\nmove, and we will stop.",
    "start": "1543780",
    "end": "1551910"
  },
  {
    "text": "We'll have to get off\nthis step, normally.",
    "start": "1551910",
    "end": "1559350"
  },
  {
    "text": "If we stay on it,\nit will swing back, it'll take us off to infinity.",
    "start": "1559350",
    "end": "1566910"
  },
  {
    "text": "You would like to choose\nsK so that you minimize",
    "start": "1566910",
    "end": "1572450"
  },
  {
    "text": "capital F. You take the point on\nthis line, so this a line in R",
    "start": "1572450",
    "end": "1580080"
  },
  {
    "text": "n, a direction in R n. ",
    "start": "1580080",
    "end": "1588820"
  },
  {
    "text": "And for all the points on\nthat line, in that direction, F has some value, and what\nyou expect is that initially,",
    "start": "1588820",
    "end": "1599090"
  },
  {
    "text": "because you chose it sensibly,\nthe value of F will drop.",
    "start": "1599090",
    "end": "1605460"
  },
  {
    "text": "But then at a certain point,\nit will turn back on you and increase.",
    "start": "1605460",
    "end": "1611130"
  },
  {
    "text": "So that would be the\nnatural stopping point. I would call that an\nexact line search.",
    "start": "1611130",
    "end": "1617460"
  },
  {
    "text": "So I exact line search would be,\nexact line search is the best",
    "start": "1617460",
    "end": "1628929"
  },
  {
    "text": "s.  Of course, that would\ntake time to compute,",
    "start": "1628930",
    "end": "1636389"
  },
  {
    "text": "and you probably,\nin deep learning, that's time you can't afford,\nso you fix the learning rate s.",
    "start": "1636390",
    "end": "1645690"
  },
  {
    "text": "Maybe you choose 0.01\nto be pretty safe. OK, so that's method\none, steepest descent.",
    "start": "1645690",
    "end": "1652800"
  },
  {
    "text": "Now, method two will\nbe Newton's method. ",
    "start": "1652800",
    "end": "1663380"
  },
  {
    "text": "So now, we have xK plus 1\nequal to xK minus something",
    "start": "1663380",
    "end": "1673070"
  },
  {
    "text": "times delta F, and now I'm\ngoing to do the right thing.",
    "start": "1673070",
    "end": "1681809"
  },
  {
    "text": "I'm going to live right\nhere, and the right thing is the Hessian, the\nsecond derivative.",
    "start": "1681810",
    "end": "1688850"
  },
  {
    "text": "This was cheap. We just took the direction\nand went along it.",
    "start": "1688850",
    "end": "1694280"
  },
  {
    "text": "Now, we're getting really\nthe right direction by using the second derivative,\nso that's H inverse.",
    "start": "1694280",
    "end": "1703850"
  },
  {
    "text": "OK, and what I've\ndone is to set that 0.",
    "start": "1703850",
    "end": "1712429"
  },
  {
    "start": "1712430",
    "end": "1717900"
  },
  {
    "text": "Do you see that's\nNewton's method? It's totally\nparallel to this guy.",
    "start": "1717900",
    "end": "1724110"
  },
  {
    "text": "Actually, I'm really happy to\nhave these two on the board parallel to each other, because\nyou have to keep straight,",
    "start": "1724110",
    "end": "1732539"
  },
  {
    "text": "are you solving equations, or\nare you minimizing functions? And you're using different\nletters in the two problems,",
    "start": "1732540",
    "end": "1740430"
  },
  {
    "text": "but now you see how they match. The Jacobian of-- so\nagain the matches,",
    "start": "1740430",
    "end": "1748679"
  },
  {
    "text": "think of f as the\ngradient of F. That's the way you should think of it.",
    "start": "1748680",
    "end": "1756510"
  },
  {
    "text": "So the Jacobian of the\ngradient is the Hessian.",
    "start": "1756510",
    "end": "1764940"
  },
  {
    "text": "The Jacobian of the\ngradient is the Hessian, and that makes sense,\nbecause the first derivative",
    "start": "1764940",
    "end": "1770790"
  },
  {
    "text": "of the first derivative\nis the second derivative. Only we're doing matrix y, so\nthe Jacobian of the gradient--",
    "start": "1770790",
    "end": "1779640"
  },
  {
    "text": "we're doing a vector\nmatrix sentence instead of a scalar sentence--\nthe Jacobian of the gradient",
    "start": "1779640",
    "end": "1786090"
  },
  {
    "text": "is a Hessian. Yeah, right. OK, so that's what I\nwanted to start with,",
    "start": "1786090",
    "end": "1793770"
  },
  {
    "text": "just to get those\nbasic facts down. And so the basic facts were\nthe three-term Taylor series.",
    "start": "1793770",
    "end": "1803280"
  },
  {
    "text": "And then the basic algorithms\nfollowed naturally from it",
    "start": "1803280",
    "end": "1809400"
  },
  {
    "text": "by setting f F at\nthe new point to 0, if that's what you were\nsolving or by assuming",
    "start": "1809400",
    "end": "1816010"
  },
  {
    "text": "you had the minimum. Right, good, good, good, good. OK.",
    "start": "1816010",
    "end": "1821100"
  },
  {
    "text": "Now, what? Now, we have to think about\nsolving these problems,",
    "start": "1821100",
    "end": "1826380"
  },
  {
    "text": "studying. Do they converge? What rate do they converge? Well, the rate of\nconvergence is like why",
    "start": "1826380",
    "end": "1835770"
  },
  {
    "text": "I separated off this example. So the convergence rate\nfor Newton's method",
    "start": "1835770",
    "end": "1841800"
  },
  {
    "text": "will be quadratic. The error gets squared,\nand of course, that",
    "start": "1841800",
    "end": "1847980"
  },
  {
    "text": "means super-fast convergence,\nif you start and close enough.",
    "start": "1847980",
    "end": "1853160"
  },
  {
    "text": "The rate of convergence\nfor a steepest descent is, of course, not. You're not squaring errors\nhere, because you're just",
    "start": "1853160",
    "end": "1860480"
  },
  {
    "text": "taking some number\ninstead of the inverse of the correct matrix, so\nyou can't expect super speed.",
    "start": "1860480",
    "end": "1872610"
  },
  {
    "text": "So a linear rate of\nconvergence would be right. ",
    "start": "1872610",
    "end": "1878430"
  },
  {
    "text": "You would like to\nknow that the error is multiplied at every step\nby some constant below 1.",
    "start": "1878430",
    "end": "1886850"
  },
  {
    "text": "That would be a\nlinear rate compared to being squared at every step.",
    "start": "1886850",
    "end": "1895420"
  },
  {
    "text": "OK, and so this will\nbe our basic formula",
    "start": "1895420",
    "end": "1902310"
  },
  {
    "text": "that we build on for really\nlarge scale problems.",
    "start": "1902310",
    "end": "1910830"
  },
  {
    "text": "And there are\nmethods, of course, people are going to come up\nwith methods that they're sort",
    "start": "1910830",
    "end": "1917789"
  },
  {
    "text": "of a cheap Newton's method. Levenberg-Marquardt,\nand it's in the notes",
    "start": "1917790",
    "end": "1925020"
  },
  {
    "text": "at the end of this\nsection, at the end of 6.4 that we'll get to.",
    "start": "1925020",
    "end": "1931105"
  },
  {
    "text": "So Levenberg-Marquardt is a sort\nof cheap man's Newton's method.",
    "start": "1931105",
    "end": "1936130"
  },
  {
    "text": "It does not compute to\nHessian, but it says, OK,",
    "start": "1936130",
    "end": "1942180"
  },
  {
    "text": "from the gradient, I can\nsee one term in the Hessian. So it grabs that term, but\nit's not fully second order.",
    "start": "1942180",
    "end": "1954570"
  },
  {
    "text": "OK. So now, we have to\nthink about problems,",
    "start": "1954570",
    "end": "1960860"
  },
  {
    "text": "and I guess the message here\nis, at our starting point, has to be convexity.",
    "start": "1960860",
    "end": "1969100"
  },
  {
    "text": "Convexity is the key\nword for these problems,",
    "start": "1969100",
    "end": "1975090"
  },
  {
    "text": "for the function that\nwe want to minimize. If that's a convex\nfunction, well first of all,",
    "start": "1975090",
    "end": "1981800"
  },
  {
    "text": "the convex function is\nlikely to have one minimum.",
    "start": "1981800",
    "end": "1987570"
  },
  {
    "text": "And the picture that's in\nour mind of steepest descent,",
    "start": "1987570",
    "end": "1993850"
  },
  {
    "text": "this picture of a bowl,\na bowl is the graph of the convex function.",
    "start": "1993850",
    "end": "1999480"
  },
  {
    "text": "So I'm turning to convexity now. I'll leave that board there,\nbecause that's pretty crucial,",
    "start": "1999480",
    "end": "2008260"
  },
  {
    "text": "and speak about the\nidea of convexity. Convex function, convex\nset, so let's call",
    "start": "2008260",
    "end": "2019960"
  },
  {
    "text": "the function f of x,\nand a typical convex set",
    "start": "2019960",
    "end": "2025539"
  },
  {
    "text": "would be I'll call it K. OK.",
    "start": "2025540",
    "end": "2030590"
  },
  {
    "text": "So we just want to remember what\ndoes that word can convex mean, and how do you know if\nyou have a convex function",
    "start": "2030590",
    "end": "2037910"
  },
  {
    "text": "or a convex set? OK, let me start\nwith convex set. So because here is\nmy general problem,",
    "start": "2037910",
    "end": "2045290"
  },
  {
    "text": "my convex minimization,\nwhich you hope to have,",
    "start": "2045290",
    "end": "2056960"
  },
  {
    "text": "and in many applications,\nyou do have. So you minimize\na convex function",
    "start": "2056960",
    "end": "2066199"
  },
  {
    "text": "for points in a convex set.",
    "start": "2066199",
    "end": "2071330"
  },
  {
    "text": "So that's like the\nideal situation. That's the ideal\nsituation, to get something",
    "start": "2071330",
    "end": "2078230"
  },
  {
    "text": "on your side, something\npowerful, convexity. The function is convex,\nand you say, well,",
    "start": "2078230",
    "end": "2084408"
  },
  {
    "text": "let me draw a convex\nfunction, the graph. OK, so I'll draw a convex\nfunction, say a bowl.",
    "start": "2084409",
    "end": "2094980"
  },
  {
    "text": "So that's a graph of f of x,\nand then here are the x's.",
    "start": "2094980",
    "end": "2100640"
  },
  {
    "text": "Let me maybe put x1 and x2 in\nthe base and the graph of f",
    "start": "2100640",
    "end": "2108799"
  },
  {
    "text": "of 1x x2 up here. ",
    "start": "2108800",
    "end": "2116109"
  },
  {
    "text": "OK. Actually, I'm over there. I should be calling this\nfunction F, I think.",
    "start": "2116110",
    "end": "2121340"
  },
  {
    "text": " Is that right? ",
    "start": "2121340",
    "end": "2129160"
  },
  {
    "text": "Yeah, a little f would be\nthe gradient of this guy. Yeah, I think so. ",
    "start": "2129160",
    "end": "2138543"
  },
  {
    "text": "OK.  Now, I'm minimizing it over\ncertain x's, not all x's.",
    "start": "2138543",
    "end": "2149119"
  },
  {
    "text": "I might be minimizing,\nfor example,",
    "start": "2149120",
    "end": "2155750"
  },
  {
    "text": "K might be the set where\nAx equals B. K might be,",
    "start": "2155750",
    "end": "2165230"
  },
  {
    "text": "in that case, a subspace\nor a shifted subspace. I said subspace, but then 18.06\nis reminding me in my mind",
    "start": "2165230",
    "end": "2174349"
  },
  {
    "text": "that I only have a\nsubspace when B is 0.  You know the word for a subspace\nthat's sort of moved over?",
    "start": "2174350",
    "end": "2183530"
  },
  {
    "text": "Affine, so I'll just\nput that word down here. ",
    "start": "2183530",
    "end": "2190190"
  },
  {
    "text": "Bunch of words to\nlearn for this topic,",
    "start": "2190190",
    "end": "2195200"
  },
  {
    "text": "but they're worth learning. OK. So it's like a plane but not\nnecessarily through the origin.",
    "start": "2195200",
    "end": "2202700"
  },
  {
    "text": "If B is 0, it doesn't\ngo through it. If B it's not 0, it doesn't\ngo through the origin. OK. Anyway, or I have\nsome other convex set.",
    "start": "2202700",
    "end": "2209940"
  },
  {
    "text": "Let me just put this convex\nset K in the base for you,",
    "start": "2209940",
    "end": "2217519"
  },
  {
    "text": "and did I make it convex? I think pretty luckily I did.",
    "start": "2217519",
    "end": "2224060"
  },
  {
    "text": "So now what's the?  Well, the convex\nsets the constraint,",
    "start": "2224060",
    "end": "2231540"
  },
  {
    "text": "so this is the constraint set. Constraint is that x\nmust be in the set K. OK,",
    "start": "2231540",
    "end": "2243290"
  },
  {
    "text": "and I drew it as a convex blob. Here was an example\nwhere it's flat, not",
    "start": "2243290",
    "end": "2252770"
  },
  {
    "text": "a blob but a flat plane. But let me come back to\nwhat does convex mean.",
    "start": "2252770",
    "end": "2261020"
  },
  {
    "text": "What's a convex set? Yeah, we have to do that,\nshould have done that before.",
    "start": "2261020",
    "end": "2267430"
  },
  {
    "start": "2267430",
    "end": "2275140"
  },
  {
    "text": "In the notes, I had the\nfun of figuring out, if I took a triangle,\nis that a convex set?",
    "start": "2275140",
    "end": "2284870"
  },
  {
    "text": "Let's just be sure.  So what's a convex set?",
    "start": "2284870",
    "end": "2291529"
  },
  {
    "text": "That is a convex\nset, because if I take any two points in the set\nand draw the line between them,",
    "start": "2291530",
    "end": "2299120"
  },
  {
    "text": "it stays in the set. So that's convexity,\nany edge, line, from x1",
    "start": "2299120",
    "end": "2311369"
  },
  {
    "text": "to x2 stays in the set.",
    "start": "2311370",
    "end": "2317970"
  },
  {
    "text": "OK, good. So here's my little\nexercise to myself.",
    "start": "2317970",
    "end": "2323380"
  },
  {
    "text": "What if I took the\nunion of two triangles? All I want to get you to do is\njust visualize convex and not",
    "start": "2323380",
    "end": "2332760"
  },
  {
    "text": "convex possibilities. Suppose I have one triangle,\neven if it was obtuse,",
    "start": "2332760",
    "end": "2342120"
  },
  {
    "text": "that's still convex, right? No problem. But now what if I put\nthose two triangles",
    "start": "2342120",
    "end": "2348630"
  },
  {
    "text": "together, take their union? Well, if I take them sitting\nwith a big gap between,",
    "start": "2348630",
    "end": "2355740"
  },
  {
    "text": "like I've lost. I mean, I never had\na chance that way, because if it was the\nunion of these two-- well,",
    "start": "2355740",
    "end": "2364289"
  },
  {
    "text": "you know what I'm going to say. If I'm doing that point\nand that point, of course, it goes outside and stupid.",
    "start": "2364290",
    "end": "2370270"
  },
  {
    "text": "All right. ",
    "start": "2370270",
    "end": "2376140"
  },
  {
    "text": "What if what if that\ntriangle, that lower triangle,",
    "start": "2376140",
    "end": "2381150"
  },
  {
    "text": "overlaps the upper triangle? Is that a convex set? ",
    "start": "2381150",
    "end": "2387000"
  },
  {
    "text": "Everybody's right saying no. Why how do I see that the\nunion of those two triangles",
    "start": "2387000",
    "end": "2393240"
  },
  {
    "text": "is not a convex set? Guys, you tell me where\nto pick two points,",
    "start": "2393240",
    "end": "2399530"
  },
  {
    "text": "where the line goes out. Well, I take one from\nthat corner and one from that corner, and the line\nbetween them went outside.",
    "start": "2399530",
    "end": "2408260"
  },
  {
    "text": "So union is usually not convex.",
    "start": "2408260",
    "end": "2417305"
  },
  {
    "text": " Well, if I think of\nthe union of two sets,",
    "start": "2417305",
    "end": "2424240"
  },
  {
    "text": "my mind automatically goes\nto the other corresponding",
    "start": "2424240",
    "end": "2429560"
  },
  {
    "text": "possibility which is the\nintersection of the two sets.",
    "start": "2429560",
    "end": "2436540"
  },
  {
    "text": "So if I take the\nintersection of two sets. ",
    "start": "2436540",
    "end": "2443680"
  },
  {
    "text": "Now, what's the deal with that? When I had two triangles,\ntwo separated triangles,",
    "start": "2443680",
    "end": "2450670"
  },
  {
    "text": "what can we say about the\nintersection of those two triangles? AUDIENCE: [INAUDIBLE]",
    "start": "2450670",
    "end": "2456790"
  },
  {
    "text": "GILBERT STRANG: It's empty. So should we regard the\nempty set as a convex set?",
    "start": "2456790",
    "end": "2462380"
  },
  {
    "text": "Yes. Isn't it? AUDIENCE: Yeah, it's vacuous. GILBERT STRANG: Vacuous, so\nit hasn't got any problems.",
    "start": "2462380",
    "end": "2469770"
  },
  {
    "text": "Right? OK, but now the intersection\nis always convex.",
    "start": "2469770",
    "end": "2478770"
  },
  {
    "text": "I'm assuming the two sets\nthat we start with are. Now, that's an important fact,\nthat the intersection of convex",
    "start": "2478770",
    "end": "2486640"
  },
  {
    "text": "sets. Let's just draw a picture\nthat shows an example. ",
    "start": "2486640",
    "end": "2494160"
  },
  {
    "text": "So what's the intersection? Just this part and it's convex.",
    "start": "2494160",
    "end": "2499560"
  },
  {
    "text": "OK, can you give\nme a little proof that the intersection is convex?",
    "start": "2499560",
    "end": "2508670"
  },
  {
    "text": "So I take two points\nin the intersection-- let me start the proof.",
    "start": "2508670",
    "end": "2514070"
  },
  {
    "text": "To test if something's\nconvex, how do you test it? You take two points in the\nset in the intersection,",
    "start": "2514070",
    "end": "2521570"
  },
  {
    "text": "and you want to show that\nthe line between them is in the intersection.",
    "start": "2521570",
    "end": "2526850"
  },
  {
    "text": "OK, why is that? So take two points, take\nx1 in the intersection.",
    "start": "2526850",
    "end": "2534349"
  },
  {
    "text": "We've got two sets\nhere, and that's the symbol for\nintersection, and we've",
    "start": "2534350",
    "end": "2539870"
  },
  {
    "text": "got another point\nin the intersection.  And now, we want to look\nat the line between them,",
    "start": "2539870",
    "end": "2547950"
  },
  {
    "text": "the line from x1 to 2x.",
    "start": "2547950",
    "end": "2553099"
  },
  {
    "text": "What's the deal with that one?  Is that fully in K1?",
    "start": "2553100",
    "end": "2561038"
  },
  {
    "text": "AUDIENCE: Yes. GILBERT STRANG: Why\nis it fully in K1? I took two points\nin the intersection,",
    "start": "2561038",
    "end": "2568830"
  },
  {
    "text": "I'm looking at the line\nbetween them, and I'm asking, is it in the first set K1?",
    "start": "2568830",
    "end": "2574680"
  },
  {
    "text": "And the answer is yes,\nbecause those points were in K1, and K1's convex.",
    "start": "2574680",
    "end": "2582010"
  },
  {
    "text": "And is that line\nbetween them in K2? Yes, same reason, the\ntwo endpoints were in K2,",
    "start": "2582010",
    "end": "2591550"
  },
  {
    "text": "so the line between\nthem is in K2. So the intersection of\nconvex sets is always convex.",
    "start": "2591550",
    "end": "2597490"
  },
  {
    "text": "The intersection of\nconvex sets is convex.",
    "start": "2597490",
    "end": "2602990"
  },
  {
    "text": "Good.  So you'll see in the\nnote these possibilities",
    "start": "2602990",
    "end": "2609569"
  },
  {
    "text": "with two triangles. Sometimes, you can take the\nunion but not very often.",
    "start": "2609570",
    "end": "2615770"
  },
  {
    "text": "OK. Now, what's the next\nthing I have to do?",
    "start": "2615770",
    "end": "2621130"
  },
  {
    "text": "Convex functions,\nwe got convex sets, what are convex\nfunctions, and we're good.",
    "start": "2621130",
    "end": "2628590"
  },
  {
    "text": "Because this is our\nprototype of a problem, and I now want to know what\nit means for that F to be--",
    "start": "2628590",
    "end": "2637930"
  },
  {
    "text": "oh, I'm sorry. I now know what it means for\nthe set K to be convex set,",
    "start": "2637930",
    "end": "2643920"
  },
  {
    "text": "but now I have to look at the\nother often more important part of the problem. What's the function\nI'm minimizing,",
    "start": "2643920",
    "end": "2650549"
  },
  {
    "text": "and I'm looking for functions\nwith this kind of a picture. OK.",
    "start": "2650550",
    "end": "2656610"
  },
  {
    "text": "The coolest way is to connect\nthe definition of a convex",
    "start": "2656610",
    "end": "2662580"
  },
  {
    "text": "function to the definition\nof a convex set.",
    "start": "2662580",
    "end": "2668340"
  },
  {
    "text": "This is really the nicest way. It's a little quick. It just swishes by you.",
    "start": "2668340",
    "end": "2674610"
  },
  {
    "text": "But tell me, do you see a\nconvex set in that picture? [INAUDIBLE]",
    "start": "2674610",
    "end": "2680930"
  },
  {
    "text": "You see a convex\nset in that picture. That's the picture of a\ngraph of a convex function.",
    "start": "2680930",
    "end": "2686300"
  },
  {
    "text": "It's a picture of a bowl. Are the points on that\nsurface, is that a convex set?",
    "start": "2686300",
    "end": "2693400"
  },
  {
    "text": "No, certainly not. No, but where is a convex set to\nbe found here, in that picture?",
    "start": "2693400",
    "end": "2703309"
  },
  {
    "text": "Yes. AUDIENCE: The set of y, if y\nis greater than [INAUDIBLE] GILBERT STRANG: Yes, the\npoints on and above the bowl,",
    "start": "2703310",
    "end": "2711210"
  },
  {
    "text": "inside the bowl, we\ncould say, these points.",
    "start": "2711210",
    "end": "2716470"
  },
  {
    "text": "So convex function,\nyes, a function's",
    "start": "2716470",
    "end": "2721599"
  },
  {
    "text": "convex when the points on and\nabove the graph are convex set.",
    "start": "2721600",
    "end": "2737320"
  },
  {
    "start": "2737320",
    "end": "2748650"
  },
  {
    "text": "You could say,\nOK, mathematicians are just being lazy. Having got one definition\nstraight for a convex set,",
    "start": "2748650",
    "end": "2755190"
  },
  {
    "text": "now they're just using that\nto give an easy definition of a convex function.",
    "start": "2755190",
    "end": "2760930"
  },
  {
    "text": "Actually, it's quite\nuseful for functions that could maybe equal infinity,\nsort of generalized functions.",
    "start": "2760930",
    "end": "2769079"
  },
  {
    "text": "But it's not the quickest way to\ntell if the function is convex.",
    "start": "2769080",
    "end": "2775260"
  },
  {
    "text": "It's not our usual test\nfor convex functions. So now I want to\ngive such a test.",
    "start": "2775260",
    "end": "2781520"
  },
  {
    "text": "OK.  So now, the definition of convex\nfunction, of a smooth convex,",
    "start": "2781520",
    "end": "2796030"
  },
  {
    "text": "yeah.  This fact, I shouldn't\nrush off away from it,",
    "start": "2796030",
    "end": "2805190"
  },
  {
    "text": "from the definition\nof a convex function",
    "start": "2805190",
    "end": "2811560"
  },
  {
    "text": "as having a convex\nset above its graph. The really official French name\nfor the set above the graph",
    "start": "2811560",
    "end": "2819450"
  },
  {
    "text": "is the epigraph, but I won't\neven write that word down. OK.",
    "start": "2819450",
    "end": "2825690"
  },
  {
    "text": "Why do I come back\nto that for a minute? Because I would like to think\nabout two functions, F1 and F2.",
    "start": "2825690",
    "end": "2836720"
  },
  {
    "text": "Out of two functions,\nI can always create the minimum\nor the maximum.",
    "start": "2836720",
    "end": "2843140"
  },
  {
    "text": "So suppose I have to convex\nfunctions, convex function",
    "start": "2843140",
    "end": "2848795"
  },
  {
    "text": "F1 and F2. ",
    "start": "2848795",
    "end": "2854440"
  },
  {
    "text": "OK. Then, I could choose a minimum. I could choose my new function.",
    "start": "2854440",
    "end": "2861450"
  },
  {
    "text": "Shall I call it\nlittle m for minimum? m of x is the\nminimum of F1 and F2.",
    "start": "2861450",
    "end": "2871340"
  },
  {
    "text": "And I could choose\na maximum function which would be the\nmaximum of F1 of x and F2",
    "start": "2871340",
    "end": "2879329"
  },
  {
    "text": "of x at the same point x. It's just a natural to think,\nOK, I have two functions.",
    "start": "2879330",
    "end": "2887280"
  },
  {
    "text": "I've got a bowl and\nI've got another bowl, and suppose they're both convex.",
    "start": "2887280",
    "end": "2892545"
  },
  {
    "start": "2892545",
    "end": "2898030"
  },
  {
    "text": "So I'm just stretching\nyou to think here. If I've got the graphs\nof two convex functions,",
    "start": "2898030",
    "end": "2905059"
  },
  {
    "text": "and I would like to consider the\nminimum of those two functions and also the maximum\nof those two functions.",
    "start": "2905060",
    "end": "2912150"
  },
  {
    "text": "I believe life is good. One of these will be\nconvex, and the other won't.",
    "start": "2912150",
    "end": "2918830"
  },
  {
    "text": "And can you identify\nwhich one is convex and which one is not convex? ",
    "start": "2918830",
    "end": "2926310"
  },
  {
    "text": "What about the minimum? Is that a convex function? So just look at the graph.",
    "start": "2926310",
    "end": "2932470"
  },
  {
    "text": "What does the minimum look like? The minimum is this guy\nuntil they meet somehow",
    "start": "2932470",
    "end": "2938250"
  },
  {
    "text": "on some surface\nand then this guy. Is that convex? We have like one minute\nto answer that question.",
    "start": "2938250",
    "end": "2945900"
  },
  {
    "text": "Absolutely no. It's got this bad kink in it. What about the maximum\nof the two functions?",
    "start": "2945900",
    "end": "2954220"
  },
  {
    "text": "So the maximum is the\none that is above, all the points or things\nthat are above or on.",
    "start": "2954220",
    "end": "2962150"
  },
  {
    "text": " There is the maximum function.",
    "start": "2962150",
    "end": "2967770"
  },
  {
    "text": "That was the minimum function. It had a kink. The maximum function\nis like that,",
    "start": "2967770",
    "end": "2973380"
  },
  {
    "text": "and it is convex, so\nmaximum yes, minimum no.",
    "start": "2973380",
    "end": "2981529"
  },
  {
    "text": " OK, and we could have a\nmaximum of 1,500 functions.",
    "start": "2981530",
    "end": "2990740"
  },
  {
    "text": "If the 1,500 functions\nare all convex, the maximum will\nbe, because it's the part way above\neverybody's graph,",
    "start": "2990740",
    "end": "3000520"
  },
  {
    "text": "and that would be the\ngraph of the maximum. OK, good. And now finally, let me\njust say, how do you know",
    "start": "3000520",
    "end": "3011260"
  },
  {
    "text": "whether a function is convex? How to test, how of test.",
    "start": "3011260",
    "end": "3016570"
  },
  {
    "text": " OK, so let me take just a\nfunction of one variable.",
    "start": "3016570",
    "end": "3024890"
  },
  {
    "text": "What's the test you learned\nin calculus, freshman calculus actually, just show\nthat this is a convex function?",
    "start": "3024890",
    "end": "3036359"
  },
  {
    "text": "What's the test for that? AUDIENCE: Use second derivative. GILBERT STRANG: Second\nderivative should be?",
    "start": "3036360",
    "end": "3041900"
  },
  {
    "text": "AUDIENCE: Positive. GILBERT STRANG:\nPositive or possibly 0, so second derivative greater\nor equals 0 everywhere.",
    "start": "3041900",
    "end": "3053359"
  },
  {
    "text": "That's convex.  OK, final question,\nsuppose F is a vector.",
    "start": "3053360",
    "end": "3062920"
  },
  {
    "text": "So this is a vector, and so I\nhave n functions of n variable.",
    "start": "3062920",
    "end": "3069369"
  },
  {
    "text": "No, I don't. I have one, sorry,\nI've got one function, but I'm in n variables.",
    "start": "3069370",
    "end": "3075490"
  },
  {
    "text": " So this was just one.",
    "start": "3075490",
    "end": "3081680"
  },
  {
    "text": "What's the test for convexity? ",
    "start": "3081680",
    "end": "3087350"
  },
  {
    "text": "So it would be passed, for\nexample, by x1 squared plus x2",
    "start": "3087350",
    "end": "3093080"
  },
  {
    "text": "squared. Would it be passed by-- so here would be the\nquestion-- would it",
    "start": "3093080",
    "end": "3099350"
  },
  {
    "text": "be passed by x transpose\nsome symmetric matrix S?",
    "start": "3099350",
    "end": "3105830"
  },
  {
    "text": "That would be a quadratic,\na pure quadratic.",
    "start": "3105830",
    "end": "3111000"
  },
  {
    "text": "Would it be convex? What would be the test? ",
    "start": "3111000",
    "end": "3117500"
  },
  {
    "text": "I'm looking for an n\ndimensional equivalent of positive second derivative.",
    "start": "3117500",
    "end": "3124460"
  },
  {
    "text": "The n dimensional equivalent\nof positive second derivative is convexity, and we have to\nrecognize what's the test.",
    "start": "3124460",
    "end": "3132560"
  },
  {
    "text": "So I could apply it\nto this function, or I could apply it to any\nfunction of n variables.",
    "start": "3132560",
    "end": "3140090"
  },
  {
    "text": " It should be OK.",
    "start": "3140090",
    "end": "3145280"
  },
  {
    "text": " What's the test here? Here, I have a matrix\ninstead of a number.",
    "start": "3145280",
    "end": "3153340"
  },
  {
    "text": "So what's the\nrequirement going to be? Times out, yeah?",
    "start": "3153340",
    "end": "3159527"
  },
  {
    "text": "[INAUDIBLE] Positive\ndefinite or semidefinite, or semidefinite just as here.",
    "start": "3159527",
    "end": "3165440"
  },
  {
    "text": "Yeah. So the test is positive,\nsemidefinite, Hessian.",
    "start": "3165440",
    "end": "3171680"
  },
  {
    "text": " And here, the Hessian\nis actually that S,",
    "start": "3171680",
    "end": "3176830"
  },
  {
    "text": "because the second\nderivatives will produce-- I'll put a 1/2 in there--",
    "start": "3176830",
    "end": "3182220"
  },
  {
    "text": "the second derivatives will\nproduce S equal the Hessian H. So here, the S--",
    "start": "3182220",
    "end": "3188589"
  },
  {
    "text": "so positive\nsemidefinite, Hessian in general, second derivative\nmatrix for a quadratic.",
    "start": "3188590",
    "end": "3197920"
  },
  {
    "text": "OK. So its convex\nproblems that we're",
    "start": "3197920",
    "end": "3203950"
  },
  {
    "text": "going to get farther with. We run into no saddle points.",
    "start": "3203950",
    "end": "3211039"
  },
  {
    "text": "We run into no local minimum. Once we found the minimum,\nit's the global minimum.",
    "start": "3211040",
    "end": "3216950"
  },
  {
    "text": "These are the good problems. OK, again, happy\nto see you today, and I look forward to Wednesday.",
    "start": "3216950",
    "end": "3223822"
  },
  {
    "start": "3223822",
    "end": "3224322"
  }
]