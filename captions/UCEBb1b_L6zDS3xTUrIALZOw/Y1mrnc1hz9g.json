[
  {
    "start": "0",
    "end": "140000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6950"
  },
  {
    "text": "offer high-quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6950",
    "end": "13690"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw. mit.edu.",
    "start": "13690",
    "end": "19170"
  },
  {
    "text": " PROFESSOR: So last week, last\nfew lectures, you heard about",
    "start": "19170",
    "end": "28070"
  },
  {
    "text": "parallel architectures and\nstarted with lecture four on discussions of concurrency.",
    "start": "28070",
    "end": "34289"
  },
  {
    "text": "How do you take applications\nor independent actors that want to operate on the\nsame data and make",
    "start": "34290",
    "end": "40260"
  },
  {
    "text": "them run safely together? And so just recapping the last\ntwo lectures, you saw really",
    "start": "40260",
    "end": "46930"
  },
  {
    "text": "two primary classes\nof architectures. Although Saman talked\nabout a few more. There was the class of shared\nmemory processors, you know,",
    "start": "46930",
    "end": "55629"
  },
  {
    "text": "the multicores that Intel, AMD,\nand PowerPC, for example, have today, where you have\none copy of the data.",
    "start": "55630",
    "end": "63970"
  },
  {
    "text": "And that's really shared among\nall the different processors because they essentially\nshare the same memory.",
    "start": "63970",
    "end": "69770"
  },
  {
    "text": "And you need things like\natomicity and synchronization to be able to make sure that the\nsharing is done properly",
    "start": "69770",
    "end": "77770"
  },
  {
    "text": "so that you don't get into data\nrace situations where multiple processors try to\nupdate the same data element",
    "start": "77770",
    "end": "84790"
  },
  {
    "text": "and you end up with\nerroneous results. You also heard about distributed\nmemory processors.",
    "start": "84790",
    "end": "90530"
  },
  {
    "text": "So an example of that might be\nthe Cell, loosely said, where",
    "start": "90530",
    "end": "95780"
  },
  {
    "text": "you have cores that primarily\naccess their own local memory. And while you can have a single\nglobal memory address",
    "start": "95780",
    "end": "103940"
  },
  {
    "text": "space, to get data from memory\nyou essentially have to communicate with the different\nprocessors to explicitly fetch",
    "start": "103940",
    "end": "109720"
  },
  {
    "text": "data in and out. So things like data\ndistribution, where the data is, and what your communication\npattern is like",
    "start": "109720",
    "end": "116729"
  },
  {
    "text": "affect your performance.  So what I'm going to talk about\nin today's lecture is",
    "start": "116730",
    "end": "125340"
  },
  {
    "text": "programming these two\ndifferent kinds of architectures, shared memory\nprocessors and distributed memory processors, and present\nyou with some concepts for",
    "start": "125340",
    "end": "134569"
  },
  {
    "text": "commonly programming\nthese machines. So in shared memory processors,\nyou have, say, n",
    "start": "134570",
    "end": "139620"
  },
  {
    "text": "processors, 1 to n. And they're connected\nto a single memory. And if one processor asks for\nthe value stored at address X,",
    "start": "139620",
    "end": "147810"
  },
  {
    "start": "140000",
    "end": "140000"
  },
  {
    "text": "everybody knows where\nit'll go look. Because there's only one address\nX. And so different",
    "start": "147810",
    "end": "153480"
  },
  {
    "text": "processors can communicate\nthrough shared variables. And you need things like\nlocking, as I mentioned, to",
    "start": "153480",
    "end": "158620"
  },
  {
    "text": "avoid race conditions or\nerroneous computation.",
    "start": "158620",
    "end": "163900"
  },
  {
    "text": "So as an example of\nparallelization, you know, straightforward parallelization\nin a shared memory machine, would be you\nhave the simple loop that's",
    "start": "163900",
    "end": "172000"
  },
  {
    "text": "just running through an array. And you're adding elements of\narray A to elements of array",
    "start": "172000",
    "end": "178140"
  },
  {
    "text": "B. And you're going to write\nthem to some new array, C. Well, if I gave you this loop\nyou can probably recognize",
    "start": "178140",
    "end": "184879"
  },
  {
    "text": "that there's really no data\ndependencies here. I can split up this loop into\nthree chunks -- let's say I have three processors -- where\none processor does all the",
    "start": "184880",
    "end": "193010"
  },
  {
    "text": "computations for iterations\nzero through three, so the first four iterations. Second processor does the\nnext four iterations.",
    "start": "193010",
    "end": "199530"
  },
  {
    "text": "And the third processor does\nthe last four iterations. And so that's shown with the\nlittle -- should have brought",
    "start": "199530",
    "end": "206473"
  },
  {
    "text": "a laser pointer. So that's showing here. And what you might need to\ndo is some mechanism to",
    "start": "206473",
    "end": "211980"
  },
  {
    "text": "essentially tell the different\nprocessors, here's the code that you need to run and\nmaybe where to start.",
    "start": "211980",
    "end": "218110"
  },
  {
    "text": "And then you may need some way\nof sort of synchronizing these different processors that say,\nI'm done, I can move on to the",
    "start": "218110",
    "end": "223765"
  },
  {
    "text": "next computation steps. So this is an example of a data\nparallel computation.",
    "start": "223765",
    "end": "228950"
  },
  {
    "text": "The loop has no real\ndependencies and, you know, each processor can operate\non different data sets.",
    "start": "228950",
    "end": "235840"
  },
  {
    "text": "And what you could do is you\ncan have a process -- this is a single application\nthat forks off or creates what",
    "start": "235840",
    "end": "243360"
  },
  {
    "text": "are commonly called\nthe threads. And each thread goes on and\nexecutes in this case the same computation.",
    "start": "243360",
    "end": "249860"
  },
  {
    "text": "So a single process can create\nmultiple concurrent threads. And really each thread is just\na mechanism for encapsulating",
    "start": "249860",
    "end": "257290"
  },
  {
    "text": "some trace of execution,\nsome execution path. So in this case you're\nessentially encapsulating this",
    "start": "257290",
    "end": "262980"
  },
  {
    "text": "particular loop here. And maybe you parameterize\nyour start index and your",
    "start": "262980",
    "end": "268669"
  },
  {
    "text": "ending index or maybe\nyour loop bounds. And in a shared memory\nprocessor, since you're",
    "start": "268670",
    "end": "275419"
  },
  {
    "text": "communicating -- since there's only a single\nmemory, really you don't need",
    "start": "275420",
    "end": "280760"
  },
  {
    "text": "to do anything special about\nthe data in this particular example, because everybody knows\nwhere to go look for it.",
    "start": "280760",
    "end": "286270"
  },
  {
    "text": "Everybody can access it. Everything's independent. There's no real issues with\nraces or deadlocks.",
    "start": "286270",
    "end": "292849"
  },
  {
    "text": "So I just wrote down some actual\ncode for that loop that parallelize it using Pthreads,\na commonly",
    "start": "292850",
    "end": "299315"
  },
  {
    "text": "used threading mechanism. Just to give you a little bit\nof flavor for, you know, the complexity of -- the simple loop\nthat we had expands to a",
    "start": "299315",
    "end": "307580"
  },
  {
    "text": "lot more code in this case. So you have your array. It has 12 elements.",
    "start": "307580",
    "end": "313050"
  },
  {
    "text": "A, B, and C. And you have\nthe basic functions. So this is the actual code\nor computation that we",
    "start": "313050",
    "end": "318710"
  },
  {
    "text": "want to carry out. And what I've done here is\nI've parameterized where you're essentially starting\nin the array.",
    "start": "318710",
    "end": "324790"
  },
  {
    "text": "So you get this parameter. And then you calculate four\niterations' worth. And this is essentially the\ncomputation that we're",
    "start": "324790",
    "end": "331311"
  },
  {
    "text": "carrying out. And now in my main program or\nin my main function, rather, what I do is I have this concept\nof threads that I'm",
    "start": "331312",
    "end": "339320"
  },
  {
    "text": "going to create. In this case I'm going to\ncreate three of them. There are some parameters that\nI have to pass in, so some",
    "start": "339320",
    "end": "346040"
  },
  {
    "text": "attributes which are now\ngoing to get into here. But then I pass in the\nfunction pointer. This is essentially a mechanism\nthat says once I've",
    "start": "346040",
    "end": "353220"
  },
  {
    "text": "created this thread, I go to\nthis function and execute this particular code. And then some arguments\nthat are functions.",
    "start": "353220",
    "end": "359320"
  },
  {
    "text": "So here I'm just passing in an\nindex at which each loop switch starts with. And after I've created each\nthread here, implicitly in the",
    "start": "359320",
    "end": "366790"
  },
  {
    "text": "thread creation, the\ncode can just immediately start running. And then once all the threads\nhave started running, I can",
    "start": "366790",
    "end": "372800"
  },
  {
    "text": "essentially just exit\nthe program because I've completed. ",
    "start": "372800",
    "end": "379070"
  },
  {
    "text": "So what I've shown you with\nthat first example was the concept of, or example\nof, data parallelism.",
    "start": "379070",
    "end": "384670"
  },
  {
    "text": "So you're performing the same\ncomputation, but instead of operating on one big chunk of\ndata, I've partitioned the",
    "start": "384670",
    "end": "391140"
  },
  {
    "start": "391000",
    "end": "391000"
  },
  {
    "text": "data into smaller chunks\nand I've replicated the computation so that I can get\nthat kind of parallelism.",
    "start": "391140",
    "end": "398190"
  },
  {
    "text": "But there's another form of\nparallelism called control parallelism, which essentially\nuses the same model of",
    "start": "398190",
    "end": "404630"
  },
  {
    "text": "threading but doesn't\nnecessarily have to run the same function or run the same\ncomputation each thread.",
    "start": "404630",
    "end": "411379"
  },
  {
    "text": "So I've sort of illustrated\nthat in the illustration there, where these are your data\nparallel computations and",
    "start": "411380",
    "end": "418050"
  },
  {
    "text": "these are some other\ncomputations in your code.",
    "start": "418050",
    "end": "426389"
  },
  {
    "text": "So there is sort of a\nprogramming model that allows you to do this kind of\nparallelism and tries to sort",
    "start": "426390",
    "end": "433580"
  },
  {
    "text": "of help the programmer by taking\ntheir sequential code and then adding annotations that\nsay, this loop is data",
    "start": "433580",
    "end": "440770"
  },
  {
    "text": "parallel or this set of code\nis has this kind of control",
    "start": "440770",
    "end": "447500"
  },
  {
    "text": "parallelism in it. So you start with your\nparallel code. This is the same program,\nmultiple data kind of",
    "start": "447500",
    "end": "454900"
  },
  {
    "text": "parallelization. So you might have seen in the\nprevious talk and the previous lecture, it was SIMD, single\ninstruction or same",
    "start": "454900",
    "end": "460389"
  },
  {
    "text": "instruction, multiple data,\nwhich allowed you to execute the same operation, you\nknow, and add over",
    "start": "460390",
    "end": "465950"
  },
  {
    "text": "multiple data elements. So here it's a similar\nkind of terminology. There's same program, multiple\ndata, and multiple program,",
    "start": "465950",
    "end": "474510"
  },
  {
    "text": "multiple data. This talk is largely focused on\nthe SPMD model, where you",
    "start": "474510",
    "end": "479600"
  },
  {
    "text": "essentially have one central\ndecision maker or you're trying to solve one central\ncomputation.",
    "start": "479600",
    "end": "485669"
  },
  {
    "text": "And you're trying to parallelize\nthat over your architecture to get the\nbest performance. So you start off with your\nprogram and then you annotate",
    "start": "485670",
    "end": "492990"
  },
  {
    "text": "the code with what's parallel\nand what's not parallel. And you might add in some\nsynchronization directives so",
    "start": "492990",
    "end": "498990"
  },
  {
    "text": "that if you do in fact have\nsharing, you might want to use the right locking mechanism\nto guarantee safety.",
    "start": "498990",
    "end": "505320"
  },
  {
    "text": "Now, in OpenMP, there are\nsome limitations as to what it can do. So it in fact assumes\nthat the programmer",
    "start": "505320",
    "end": "512130"
  },
  {
    "text": "knows what he's doing. And the programmer is largely\nresponsible for getting the synchronization right, or that\nif they're sharing that they",
    "start": "512130",
    "end": "518680"
  },
  {
    "text": "get those dependencies\nprotected correctly. So you can take your program,\ninsert these annotations, and",
    "start": "518680",
    "end": "525850"
  },
  {
    "text": "then you go on and\ntest and debug. So a simple OpenMP example,\nagain using the simple loop --",
    "start": "525850",
    "end": "534730"
  },
  {
    "text": "now, I've thrown away some of\nthe extra code -- you're adding these two extra\npragmas in this case.",
    "start": "534730",
    "end": "540190"
  },
  {
    "text": "The first one, your parallel\npragma, I call the data",
    "start": "540190",
    "end": "545370"
  },
  {
    "text": "parallel pragma, really says\nthat you can execute as many of the following code block as\nthere are processors or as",
    "start": "545370",
    "end": "553120"
  },
  {
    "text": "many as you have thread\ncontexts. So in this case I implicitly\nmade the assumption that I",
    "start": "553120",
    "end": "558260"
  },
  {
    "text": "have three processors, so I can\nautomatically partition my code into three sets. And this transformation can sort\nof be done automatically",
    "start": "558260",
    "end": "565380"
  },
  {
    "text": "by the compiler. And then there's a for pragma\nthat says this loop is",
    "start": "565380",
    "end": "571750"
  },
  {
    "text": "parallel and you can divide up\nthe work in the mechanism that's work sharing.",
    "start": "571750",
    "end": "577230"
  },
  {
    "text": "So multiple threads can\ncollaborate to solve the same computation, but each one does\na smaller amount of work.",
    "start": "577230",
    "end": "582540"
  },
  {
    "text": "So this is in contrast to what\nI'm going to focus on a lot",
    "start": "582540",
    "end": "589759"
  },
  {
    "text": "more in the rest of our talk,\nwhich is distributed memory processors and programming\nfor distributed memories.",
    "start": "589760",
    "end": "595080"
  },
  {
    "start": "595000",
    "end": "595000"
  },
  {
    "text": "And this will feel a lot more\nlike programming for the Cell as you get more and more\ninvolved in that and your",
    "start": "595080",
    "end": "600780"
  },
  {
    "text": "projects get more intense. So in distributed memory\nprocessors, to recap the",
    "start": "600780",
    "end": "608220"
  },
  {
    "text": "previous lectures, you\nhave n processors. Each processor has\nits own memory. And they essentially share the\ninterconnection network.",
    "start": "608220",
    "end": "616180"
  },
  {
    "text": " Each processor has its own\naddress, X. So when a",
    "start": "616180",
    "end": "621670"
  },
  {
    "text": "processor, P1, asks for X it\nknows where to go look. It's going to look in its\nown local memory.",
    "start": "621670",
    "end": "627810"
  },
  {
    "text": "So if all processors are asking\nfor the same value as sort of address X, then each\none goes and looks in a",
    "start": "627810",
    "end": "633670"
  },
  {
    "text": "different place. So there are n places\nto look, really. And what's stored in those\naddresses will vary because",
    "start": "633670",
    "end": "639830"
  },
  {
    "text": "it's everybody's local memory. So if one processor, say P1,\nwants to look at the value",
    "start": "639830",
    "end": "646150"
  },
  {
    "text": "stored in processor two's\naddress, it actually has to explicitly request it. The processor two has\nto send it data.",
    "start": "646150",
    "end": "653320"
  },
  {
    "text": "And processor one has to figure\nout, you know, what to do with that copy. So it has to store\nit somewhere. ",
    "start": "653320",
    "end": "660720"
  },
  {
    "text": "So this message passing really\nexposes explicit communication",
    "start": "660720",
    "end": "666399"
  },
  {
    "text": "to exchange data. And you'll see that there are\ndifferent kinds of data",
    "start": "666400",
    "end": "671620"
  },
  {
    "text": "communications. But really the concept of what\nyou exchange has three different -- or four different,\nrather --",
    "start": "671620",
    "end": "679029"
  },
  {
    "text": "things you need to address. One is how is the data\ndescribed and what does it describe?",
    "start": "679030",
    "end": "684530"
  },
  {
    "text": "How are the processes\nidentified? So how do I identify that\nprocessor one is sending me this data? And if I'm receiving data\nhow do I know who I'm",
    "start": "684530",
    "end": "690760"
  },
  {
    "text": "receiving it from?  Are all messages the same? Well, you know, if I send a\nmessage to somebody, do I have",
    "start": "690760",
    "end": "698100"
  },
  {
    "text": "any guarantee that it's\nreceived or not? And what does it mean for a send\noperation or a receive",
    "start": "698100",
    "end": "703450"
  },
  {
    "text": "operation to be completed? You know, is there some sort\nof acknowledgment process? ",
    "start": "703450",
    "end": "710440"
  },
  {
    "text": "So an example of a message\npassing program -- and if you've started to look at the\nlab you'll see that this is",
    "start": "710440",
    "end": "715580"
  },
  {
    "text": "essentially where the\nlab came from. It's the same idea.",
    "start": "715580",
    "end": "720920"
  },
  {
    "text": "I've created -- here I have some\ntwo-dimensional space. And I have points in this\ntwo-dimensional space.",
    "start": "720920",
    "end": "727380"
  },
  {
    "text": "I have points B, which are these\nblue circles, and I have points A which I've represented\nas these yellow or",
    "start": "727380",
    "end": "734160"
  },
  {
    "text": "golden squares. And what I want to do is for\nevery point in A I want to",
    "start": "734160",
    "end": "739740"
  },
  {
    "text": "calculate the distance to all\nof the points B. So there's sort of a pair wise interaction between the two arrays.",
    "start": "739740",
    "end": "746880"
  },
  {
    "text": "So a simple loop that\nessentially does this -- and there are n squared\ninteractions, you have, you",
    "start": "746880",
    "end": "754980"
  },
  {
    "text": "know, a loop that loops over\nall the A elements, a loop that loops over all\nthe B elements. And you essentially calculate\nin this case Euclidean",
    "start": "754980",
    "end": "762350"
  },
  {
    "text": "distance which I'm\nnot showing. And you store it into\nsome new array. ",
    "start": "762350",
    "end": "767890"
  },
  {
    "text": "So if I give you two processors\nto do this work, processor one and processor\ntwo, and I give you some",
    "start": "767890",
    "end": "774180"
  },
  {
    "text": "mechanism to share between\nthe two -- so here's my CPU. Each processor has\nlocal memory.",
    "start": "774180",
    "end": "779940"
  },
  {
    "text": "What would be some approach\nfor actually parallelizing this? Anybody look at the lab yet?",
    "start": "779940",
    "end": "785240"
  },
  {
    "text": "OK, so what would you do\nwith two processors? ",
    "start": "785240",
    "end": "794734"
  },
  {
    "text": "AUDIENCE: One has half\nmemory [INAUDIBLE] PROFESSOR: Right.",
    "start": "794734",
    "end": "801120"
  },
  {
    "text": "So what was said was that you\nsplit one of the arrays in two and you can actually get that\nkind of concurrency.",
    "start": "801120",
    "end": "806589"
  },
  {
    "text": "So, you know, let's\nsay processor one already has the data. And it has some place that it's\nalready allocated where",
    "start": "806590",
    "end": "813319"
  },
  {
    "text": "it's going to write C, the\nresults of the computation, then I can break up the work\njust like it was suggested.",
    "start": "813320",
    "end": "819860"
  },
  {
    "text": "So what P1 has to do\nis send data to P2. It says here's the data. Here's the computation.",
    "start": "819860",
    "end": "826190"
  },
  {
    "text": "Go ahead and help me out. So I send the first array\nelements, and then I send half",
    "start": "826190",
    "end": "833340"
  },
  {
    "text": "of the other elements\nthat I want the calculations done for. And then P1 and P2 can\nnow sort of start",
    "start": "833340",
    "end": "840760"
  },
  {
    "text": "computing in parallel. But notice that P2 has its own\narray that it's going to store",
    "start": "840760",
    "end": "846649"
  },
  {
    "text": "results in. And so as these compute they\nactually fill in different",
    "start": "846650",
    "end": "852210"
  },
  {
    "text": "logical places or logical parts\nof the overall matrix. So what has to be done is at the\nend for P1 to have all the",
    "start": "852210",
    "end": "859720"
  },
  {
    "text": "results, P2 has to send it sort\nof the rest of the matrix to complete it. And so now P1 has\nall the results.",
    "start": "859720",
    "end": "867200"
  },
  {
    "text": "The computation is done\nand you can move on. Does that make sense? OK. So you'll get to actually do\nthis as part of your labs.",
    "start": "867200",
    "end": "876490"
  },
  {
    "text": "So in this example messaging\nprogram, you have started out with a sequential code. And we had two processors.",
    "start": "876490",
    "end": "883329"
  },
  {
    "text": "So processor one actually\nsends the code. So it is essentially a template\nfor the code you'll end up writing.",
    "start": "883330",
    "end": "889350"
  },
  {
    "text": "And it does have to work\nin the outer loop. So this n array over which it\nis iterating the A array, is",
    "start": "889350",
    "end": "897480"
  },
  {
    "text": "it's only doing half as many.  And processor two has to\nactually receive the data.",
    "start": "897480",
    "end": "906170"
  },
  {
    "start": "906000",
    "end": "906000"
  },
  {
    "text": "And it specifies where to\nreceive the data into. So I've omitted some things, for\nexample, extra information",
    "start": "906170",
    "end": "913110"
  },
  {
    "text": "sort of hidden in these\nparameters. So here you're sending all of\nA, all of B. Whereas, you",
    "start": "913110",
    "end": "918650"
  },
  {
    "text": "know, you could have specified\nextra parameters that says, you know, I'm sending you A.\nHere's n elements to read from",
    "start": "918650",
    "end": "924740"
  },
  {
    "text": "A. Here's B. Here's n by\ntwo elements to read from B. And so on.",
    "start": "924740",
    "end": "931040"
  },
  {
    "text": "But the computation is\nessentially the same except for the index at which you\nstart, in this case changed",
    "start": "931040",
    "end": "938140"
  },
  {
    "text": "for processor two. And now, when the computation is\ndone, this guy essentially",
    "start": "938140",
    "end": "943610"
  },
  {
    "text": "waits until the data\nis received. Processor two eventually sends\nit that data and now",
    "start": "943610",
    "end": "949410"
  },
  {
    "text": "you can move on. AUDIENCE: I have a question. PROFESSOR: Yeah? AUDIENCE: So would processor two\nhave to wait for the data",
    "start": "949410",
    "end": "957583"
  },
  {
    "text": "from processor one? PROFESSOR: Yeah,\nso there's a -- I'll get into that later. So what does it mean\nto receive?",
    "start": "957583",
    "end": "963980"
  },
  {
    "text": " To do this computation, I\nactually need this instruction",
    "start": "963980",
    "end": "969670"
  },
  {
    "text": "to complete. So what does it need for that\ninstruction to complete? I do have to get the data\nbecause otherwise I don't know",
    "start": "969670",
    "end": "975370"
  },
  {
    "text": "what to compute on. So there is some implicit\nsynchronization that you have to do. And in some cases\nit's explicit.",
    "start": "975370",
    "end": "981490"
  },
  {
    "text": "So I'll get into that\na little bit later. Does that sort of hint\nat the answer?",
    "start": "981490",
    "end": "987400"
  },
  {
    "text": "Are you still confused? AUDIENCE: So processor one\ndoesn't do the computation but",
    "start": "987400",
    "end": "993040"
  },
  {
    "text": "it still sends the data -- PROFESSOR: So in terms of\ntracing, processor one sends",
    "start": "993040",
    "end": "1001470"
  },
  {
    "text": "the data and then can\nimmediately start executing its code, right? Processor two, in this\nparticular example, has to",
    "start": "1001470",
    "end": "1008960"
  },
  {
    "text": "wait until it receives\nthe data. So once this receive completes,\nthen you can actually go and start executing",
    "start": "1008960",
    "end": "1014770"
  },
  {
    "text": "the rest of the code. So imagine that it essentially\nsays, wait until I have data. Wait until I have\nsomething to do.",
    "start": "1014770",
    "end": "1019950"
  },
  {
    "text": "Does that help? AUDIENCE: Can the\nmain processor [UNINTELLIGIBLE PHRASE]",
    "start": "1019950",
    "end": "1026079"
  },
  {
    "text": "PROFESSOR: Can the\nmain processor -- AUDIENCE: I mean, in Cell,\neverybody is not peers.",
    "start": "1026080",
    "end": "1031196"
  },
  {
    "text": "There is a master there. And what master can do instead\nof doing computation, master",
    "start": "1031196",
    "end": "1037760"
  },
  {
    "text": "can be basically the\nquarterback, sending data, receiving data. And SPEs can be basically\nwaiting for data, get the",
    "start": "1037760",
    "end": "1045534"
  },
  {
    "text": "computation, send it back. So in some sense in Cell you\nprobably don't want to do the",
    "start": "1045534",
    "end": "1050728"
  },
  {
    "text": "computation on the master. Because that means the\nmaster slows down. The master will do only\nthe data management.",
    "start": "1050728",
    "end": "1056608"
  },
  {
    "text": "So that might be one symmetrical\n[UNINTELLIGIBLE]",
    "start": "1056608",
    "end": "1061730"
  },
  {
    "text": "PROFESSOR: And you'll see\nthat in the example. Because the PPE in that case\nhas to send the data to two different SPEs.",
    "start": "1061730",
    "end": "1069390"
  },
  {
    "text": "Yup? AUDIENCE: In some sense\n[UNINTELLIGIBLE PHRASE]  at points seems to be\n[UNINTELLIGIBLE] sense that if",
    "start": "1069390",
    "end": "1077005"
  },
  {
    "text": "-- so have a huge array\nand you want to [UNINTELLIGIBLE PHRASE] the\ndata to receive the whole",
    "start": "1077005",
    "end": "1083090"
  },
  {
    "text": "array, then you have\nto [UNINTELLIGIBLE] PROFESSOR: Yeah, we'll\nget into that later.",
    "start": "1083090",
    "end": "1088600"
  },
  {
    "text": "Yeah, I mean, that's\na good point. You know, communication\nis not cheap. And if you sort of don't take\nthat into consideration, you",
    "start": "1088600",
    "end": "1095139"
  },
  {
    "text": "end up paying a lot\nfor overhead for parallelizing things. AUDIENCE: [INAUDIBLE] PROFESSOR: Well, you can do\nthings in software as well.",
    "start": "1095140",
    "end": "1102250"
  },
  {
    "text": "We'll get into that. OK, so some crude performance\nanalysis.",
    "start": "1102250",
    "end": "1107850"
  },
  {
    "text": "So I have to calculate\nthis distance. And given two processors, I can effectively get a 2x speedup.",
    "start": "1107850",
    "end": "1114020"
  },
  {
    "start": "1109000",
    "end": "1109000"
  },
  {
    "text": "By dividing up the work I can\nget done in half the time. Well, if you gave me four\nprocessors, I can maybe get",
    "start": "1114020",
    "end": "1120250"
  },
  {
    "text": "done four times as fast. And\nin my communication model here, I have one copy of one\narray that's essentially",
    "start": "1120250",
    "end": "1128690"
  },
  {
    "text": "sending to every processor. And there's some subset of A.\nSo I'm partitioning my other",
    "start": "1128690",
    "end": "1134060"
  },
  {
    "text": "array into smaller subsets. And I'm sending those to each\nof the different processors. So we'll get into terminology\nfor how to actually name these",
    "start": "1134060",
    "end": "1141610"
  },
  {
    "text": "communications later. But really the thing to take\naway here is that this granularity -- how I'm\npartitioning A -- affects my",
    "start": "1141610",
    "end": "1149000"
  },
  {
    "text": "performance and communication\nalmost directly. And, you know, the comment that\nwas just made is that,",
    "start": "1149000",
    "end": "1154690"
  },
  {
    "text": "you know, what do you do\nabout communication? It's not free. So all of those will\nbe addressed. So to understand performance,\nwe sort of summarize three",
    "start": "1154690",
    "end": "1162150"
  },
  {
    "text": "main concepts that you\nessentially need to understand. One is coverage, or in other\nwords, how much parallelism do",
    "start": "1162150",
    "end": "1168945"
  },
  {
    "text": "I actually have in\nmy application? And this can actually affect,\nyou know, how much work is it",
    "start": "1168945",
    "end": "1175980"
  },
  {
    "text": "worth spending on this\nparticular application? Granularity -- you know, how do you partition\nyour data among your different",
    "start": "1175980",
    "end": "1181860"
  },
  {
    "text": "processors so that you can keep\ncommunication down, so you can keep synchronization\ndown, and so on.",
    "start": "1181860",
    "end": "1186970"
  },
  {
    "text": "Locality -- so while not shown in the\nparticular example, if two processors are communicating, if\nthey are close in space or",
    "start": "1186970",
    "end": "1194840"
  },
  {
    "text": "far in space, or if the\ncommunication between two processors is far cheaper than\ntwo other processors, can I",
    "start": "1194840",
    "end": "1201399"
  },
  {
    "text": "exploit that in some way? And so we'll talk about\nthat as well.",
    "start": "1201400",
    "end": "1207370"
  },
  {
    "text": "So an example of sort of\nparallelism in an application, there are two essentially\nprojects that are doing ray",
    "start": "1207370",
    "end": "1214030"
  },
  {
    "text": "tracing, so I thought I'd\nhave this slide here. You know, how much parallelism\ndo you have in",
    "start": "1214030",
    "end": "1220890"
  },
  {
    "text": "a ray tracing program. In ray tracing what you do is\nyou essentially have some",
    "start": "1220890",
    "end": "1226170"
  },
  {
    "text": "camera source, some observer. And you're trying to figure out,\nyou know, how to color or",
    "start": "1226170",
    "end": "1233059"
  },
  {
    "text": "how to shade different pixels\nin your screen. So what you do is you shoot rays\nfrom a particular source",
    "start": "1233060",
    "end": "1238160"
  },
  {
    "text": "through your plane. And then you see how the rays\nbounce off of other objects. And that allows you to render\nscenes in various ways.",
    "start": "1238160",
    "end": "1244310"
  },
  {
    "text": "So you have different kinds of\nparallelisms. You have your primary ray that's shot in.",
    "start": "1244310",
    "end": "1250190"
  },
  {
    "text": "And if you're shooting into\nsomething like water or some very reflective surface, or some\nsurface that can actually",
    "start": "1250190",
    "end": "1257440"
  },
  {
    "text": "reflect, transmit, you can\nessentially end up with a lot",
    "start": "1257440",
    "end": "1263409"
  },
  {
    "text": "more rays that are created\nat run time. So there's dynamic parallelism\nin this particular example.",
    "start": "1263410",
    "end": "1270040"
  },
  {
    "text": "And you can shoot a lot\nof rays from here. So there's different kinds of\nparallelism you can exploit. ",
    "start": "1270040",
    "end": "1277500"
  },
  {
    "text": "Not all prior programs have this\nkind of, sort of, a lot of parallelism, or\nembarrassingly parallel",
    "start": "1277500",
    "end": "1283450"
  },
  {
    "text": "computation. You know, you saw\nsome basic code sequences in earlier lectures.",
    "start": "1283450",
    "end": "1289090"
  },
  {
    "text": "So there's a sequential part. And the reason this is\nsequential is because there are data flow dependencies\nbetween each of the different",
    "start": "1289090",
    "end": "1295020"
  },
  {
    "text": "computations. So here I calculate a, but I\nneed the result of a to do this instruction. I calculate d here and I need\nthat result to calculate e.",
    "start": "1295020",
    "end": "1303300"
  },
  {
    "text": "But then this loop really here\nis just assigning or it's initializing some big array. And I can really do\nthat in parallel.",
    "start": "1303300",
    "end": "1309940"
  },
  {
    "text": "So I have sequential parts\nand parallel parts. So how does that affect\nmy overall speedups?",
    "start": "1309940",
    "end": "1316590"
  },
  {
    "text": "And so there's this law which\nis really a demonstration of diminishing returns,\nAmdahl's Law.",
    "start": "1316590",
    "end": "1323549"
  },
  {
    "text": "It says that if, you know, you\nhave a really fast car, it's only as good to you as fast\nas you can drive it.",
    "start": "1323550",
    "end": "1330309"
  },
  {
    "text": "So if there's a lot of\ncongestion on your road or, you know, there are posted speed\nlimits or some other",
    "start": "1330310",
    "end": "1335320"
  },
  {
    "text": "mechanism, you really can't\nexploit all the speed of your car. Or in other words, you're only\nas fast as the fastest",
    "start": "1335320",
    "end": "1342440"
  },
  {
    "text": "mechanisms of the computation\nthat you can have. So to look at this in more\ndetail, your potential speedup",
    "start": "1342440",
    "end": "1351529"
  },
  {
    "text": "is really proportional to the\nfraction of the code that can be parallelized. So if I have some computation\n-- let's say it has three",
    "start": "1351530",
    "end": "1358730"
  },
  {
    "text": "parts: a sequential part that\ntakes 25 seconds, a parallel part that takes 50 seconds,\nand a sequential part that",
    "start": "1358730",
    "end": "1365120"
  },
  {
    "text": "runs in 25 seconds. So the total execution\ntime is 100 seconds. And if I have one processor,\nthat's really all I can do.",
    "start": "1365120",
    "end": "1373390"
  },
  {
    "text": "And if she gave me more than one\nprocessor -- so let's say I have five processors. Well, I can't do anything about\nthe sequential work.",
    "start": "1373390",
    "end": "1379390"
  },
  {
    "text": "So that's still going\nto take 25 seconds. And I can't do anything about\nthis sequential work either. That still takes 25 seconds.",
    "start": "1379390",
    "end": "1385850"
  },
  {
    "text": "But this parallel part I can\nessentially break up among the different processors. So five in this case.",
    "start": "1385850",
    "end": "1391470"
  },
  {
    "text": "And that gets me, you know,\nfive-way parallelism. So the 50 seconds now is\nreduced to 10 seconds.",
    "start": "1391470",
    "end": "1396970"
  },
  {
    "text": "Is that clear so far? So the overall running time in\nthat case is 60 seconds.",
    "start": "1396970",
    "end": "1402330"
  },
  {
    "text": "So what would be my speedup? Well, you calculate speedup,\nold running time divided by",
    "start": "1402330",
    "end": "1409580"
  },
  {
    "text": "the new running time. So 100 seconds divided\nby 60 seconds. Or my parallel version\nis 1.67 times faster.",
    "start": "1409580",
    "end": "1417019"
  },
  {
    "text": "So this is great. If I increase the number of\nprocessors, then I should be able to get more and\nmore parallelism. But it also means that there's\nsort of an upper bound on how",
    "start": "1417020",
    "end": "1427530"
  },
  {
    "text": "much speedup you can get. So if you look at the fraction\nof work in your application that's parallel, that's p.",
    "start": "1427530",
    "end": "1434840"
  },
  {
    "text": "And your number of processors,\nwell, your speedup is -- let's say the old running time\nis just one unit of work.",
    "start": "1434840",
    "end": "1441270"
  },
  {
    "text": "If the time it takes for the\nsequential work -- so that's 1 minus p, since p is the fraction\nof the parallel work.",
    "start": "1441270",
    "end": "1448720"
  },
  {
    "text": "And it's the time to do\nthe parallel work. And since I can parallelize\nthat fraction over n",
    "start": "1448720",
    "end": "1454480"
  },
  {
    "text": "processors, I can sort of reduce\nthat to really small amounts in the limit.",
    "start": "1454480",
    "end": "1459640"
  },
  {
    "text": "Does that make sense so far?  So the speedup can tend to 1\nover 1 minus p in the limit.",
    "start": "1459640",
    "end": "1466260"
  },
  {
    "text": "If I increase the number of\nprocessors or that gets really large, that's essentially my\nupper bound on how fast",
    "start": "1466260",
    "end": "1473820"
  },
  {
    "text": "programs can work. You know, how much can I exploit\nout of my program?",
    "start": "1473820",
    "end": "1479549"
  },
  {
    "text": "So this is great. What this law says -- the\nimplication here is if your program has a lot of inherent\nparallelism, you",
    "start": "1479550",
    "end": "1485790"
  },
  {
    "text": "can do really well. But if your program doesn't have\nany parallelism, well, there's really nothing\nyou can do. So parallel architectures\nwon't really help you.",
    "start": "1485790",
    "end": "1492679"
  },
  {
    "text": "And there's some interesting\ntrade-offs, for example, that you might consider if you're\ndesigning a chip or if you're",
    "start": "1492680",
    "end": "1498850"
  },
  {
    "text": "looking at an application or\ndomain of applications, figuring out what is the best\narchitecture to run them on.",
    "start": "1498850",
    "end": "1506080"
  },
  {
    "text": "So in terms of performance\nscalability, as I increase the number of processors,\nI have speedup.",
    "start": "1506080",
    "end": "1512890"
  },
  {
    "text": "You can define, sort of,\nan efficiency to be linear at 100%. But typically you end up in sort\nof the sublinear domain.",
    "start": "1512890",
    "end": "1520590"
  },
  {
    "text": "That's because communication\nis not often free.",
    "start": "1520590",
    "end": "1525799"
  },
  {
    "text": "But you can get super linear\nspeedups ups on real architectures because of\nsecondary and tertiary effects",
    "start": "1525800",
    "end": "1531450"
  },
  {
    "text": "that come from register\nallocation or caching effects. So they can hide a lot of\nlatency or you can take",
    "start": "1531450",
    "end": "1538120"
  },
  {
    "text": "advantage of a lot of pipelining\nmechanisms in the architecture to get super\nlinear speedups.",
    "start": "1538120",
    "end": "1543330"
  },
  {
    "text": "So you can end up in two\ndifferent domains. ",
    "start": "1543330",
    "end": "1549860"
  },
  {
    "text": "So a small, you know, overview\nof the extent of parallelism",
    "start": "1549860",
    "end": "1555000"
  },
  {
    "text": "in your program and\nhow that affects your overall execution. And the other concept\nis granularity.",
    "start": "1555000",
    "end": "1561620"
  },
  {
    "text": "So given that I have this\nmuch parallelism, how do I exploit it? There are different ways of\nexploiting it, and that comes",
    "start": "1561620",
    "end": "1566809"
  },
  {
    "text": "down to, well, how do I\nsubdivide my problem? What is the granularity of the\nsub-problems I'm going to",
    "start": "1566810",
    "end": "1572409"
  },
  {
    "text": "calculate on? And, really, granularity from\nmy perspective, is just a",
    "start": "1572410",
    "end": "1579130"
  },
  {
    "text": "qualitative measure of what is\nthe ratio of your computation to your communication?",
    "start": "1579130",
    "end": "1584950"
  },
  {
    "text": "So if you're doing a lot of\ncomputation, very little communication, you could\nbe doing really well or vice versa.",
    "start": "1584950",
    "end": "1590580"
  },
  {
    "text": "Then you could be computation\nlimited, and so you need a lot of bandwidth for example\nin your architecture. AUDIENCE: Like before, you\nreally didn't have to give",
    "start": "1590580",
    "end": "1597257"
  },
  {
    "text": "every single processor\nan entire copy of B. PROFESSOR: Right. Yeah. Good point.",
    "start": "1597258",
    "end": "1605070"
  },
  {
    "text": " And as you saw in the previous\nslides, you have --",
    "start": "1605070",
    "end": "1610960"
  },
  {
    "text": "computation stages\nare separated by communication stages. And your communication in a\nlot of cases essentially",
    "start": "1610960",
    "end": "1617070"
  },
  {
    "text": "serves as synchronization. I need everybody to get to the\nsame point before I can move on logically in my\ncomputation.",
    "start": "1617070",
    "end": "1624780"
  },
  {
    "text": "So there are two kinds of sort\nof classes of granularity. There's fine grain and, as\nyou'll see, coarse grain.",
    "start": "1624780",
    "end": "1630840"
  },
  {
    "text": "So in fine-grain parallelism,\nyou have low computation to communication ratio. And that has good properties\nin that you have a small",
    "start": "1630840",
    "end": "1638149"
  },
  {
    "text": "amount of work done between\ncommunication stages. ",
    "start": "1638150",
    "end": "1644410"
  },
  {
    "text": "And it has bad properties in\nthat it gives you less performance opportunity.",
    "start": "1644410",
    "end": "1649530"
  },
  {
    "text": " It should be more, right? More opportunity for --",
    "start": "1649530",
    "end": "1655100"
  },
  {
    "text": "AUDIENCE: No. Less. PROFESSOR: Sorry. Yeah, yeah, sorry. I didn't get enough sleep. ",
    "start": "1655100",
    "end": "1661670"
  },
  {
    "text": "So less opportunities for\nperformance enhancement, but you have high communication\nratio because essentially",
    "start": "1661670",
    "end": "1668870"
  },
  {
    "text": "you're communicating\nvery often. So these are the computations\nhere and these yellow bars are",
    "start": "1668870",
    "end": "1675120"
  },
  {
    "text": "the synchronization points. So I have to distribute\ndata or communicate. I do computations but,\nyou know, computation",
    "start": "1675120",
    "end": "1682000"
  },
  {
    "text": "doesn't last very long. And I do more communication or\nmore synchronization, and I repeat the process.",
    "start": "1682000",
    "end": "1687750"
  },
  {
    "text": "So naturally you can adjust this\ngranularity to sort of reduce the communication\noverhead. AUDIENCE:\n[UNINTELLIGIBLE PHRASE]",
    "start": "1687750",
    "end": "1694510"
  },
  {
    "text": "two things in that\noverhead part. One is the volume. So one, communication.",
    "start": "1694510",
    "end": "1701380"
  },
  {
    "text": "Also there's a large part\nof synchronization cost. Basically you get a\ncommunication goal and you have to go start the messages\nand wait until",
    "start": "1701380",
    "end": "1709022"
  },
  {
    "text": "everybody is done. So that overhead also can go. Even if you don't send that much\ndata, just the fact that",
    "start": "1709022",
    "end": "1715642"
  },
  {
    "text": "you are communicating, that\nmeans you have to do a lot of this additional bookkeeping\nstuff, that especially in the",
    "start": "1715642",
    "end": "1723076"
  },
  {
    "text": "distributed [? memory ?] [? machine is ?] pretty\nexpensive. PROFESSOR: Yeah. Thanks.",
    "start": "1723076",
    "end": "1728780"
  },
  {
    "text": "So in coarse-grain parallelism,\nyou sort of make the work chunks more and\nmore so that you do the",
    "start": "1728780",
    "end": "1735070"
  },
  {
    "text": "communication synchronization\nless and less. And so that's shown here. You do longer pieces of\nwork and have fewer",
    "start": "1735070",
    "end": "1740750"
  },
  {
    "text": "synchronization stages. So in that regime, you can have\nmore opportunities for",
    "start": "1740750",
    "end": "1749690"
  },
  {
    "text": "performance improvements, but\nthe tricky thing that you get into is what's called\nload balancing.",
    "start": "1749690",
    "end": "1755730"
  },
  {
    "text": "So if each of these different\ncomputations takes differing amounts of time to complete,\nthen what you might end up",
    "start": "1755730",
    "end": "1762130"
  },
  {
    "text": "doing is a lot of people might\nend up idle as they wait until everybody's essentially reached\ntheir finish line.",
    "start": "1762130",
    "end": "1767690"
  },
  {
    "text": "Yep? AUDIENCE: If you don't have to\nacknowledge that something's done can't you just say, [? OK, I'm done with\nyour salt ?]. Hand it to the initial\nprocessor",
    "start": "1767690",
    "end": "1773570"
  },
  {
    "text": "and keep doing whatever? PROFESSOR: So, you can do\nthat in cases where that essentially there is a\nmechanism -- or the",
    "start": "1773570",
    "end": "1780429"
  },
  {
    "text": "application allows for it. But as I'll show -- well, you\nwon't see until the next lecture -- there are\ndependencies, for example,",
    "start": "1780430",
    "end": "1786340"
  },
  {
    "text": "that might preclude you\nfrom doing that. If everybody needs to reach the\nsame point because you're updating a large data structure\nbefore you can go",
    "start": "1786340",
    "end": "1793550"
  },
  {
    "text": "on, then you might not\nbe able to do that. So think of doing molecular\ndynamics simulations.",
    "start": "1793550",
    "end": "1798810"
  },
  {
    "text": "You need everybody to calculate\na new position before you can go on and\ncalculate new kinds of coarse interactions. AUDIENCE: [UNINTELLIGIBLE]\nnothing else to calculate yet.",
    "start": "1798810",
    "end": "1804700"
  },
  {
    "text": "PROFESSOR: Right. AUDIENCE: But also there\nis pipelining. So what do you talk about\n[UNINTELLIGIBLE] because you might want to get the next data\nwhile you're computing",
    "start": "1804700",
    "end": "1810785"
  },
  {
    "text": "now so that when I'm done\nI can start sending. [UNINTELLIGIBLE PHRASE] you\ncan all have some of that.",
    "start": "1810785",
    "end": "1816659"
  },
  {
    "text": "PROFESSOR: Yep. Yeah, because communication is\nsuch an intensive part, there",
    "start": "1816660",
    "end": "1821934"
  },
  {
    "text": "are different ways of\ndealing with it. And that will be right\nafter load balancing. So the load balancing problem\nis just an illustration.",
    "start": "1821935",
    "end": "1828880"
  },
  {
    "text": "And things that appear in sort\nof this lightish pink will serve as sort of visual cues.",
    "start": "1828880",
    "end": "1834900"
  },
  {
    "start": "1830000",
    "end": "1830000"
  },
  {
    "text": "This is the same color coding\nscheme that David's using in the recitations. So this is PPU code. Things that appear in yellow\nwill be SPU code.",
    "start": "1834900",
    "end": "1842180"
  },
  {
    "text": "And these are just meant to\nessentially show you how you might do things like this on\nCell, just to help you along",
    "start": "1842180",
    "end": "1847780"
  },
  {
    "text": "in picking up more of the syntax\nand functionality you need for your programs.",
    "start": "1847780",
    "end": "1853190"
  },
  {
    "text": "So in the load balancing\nproblem, you essentially have, let's say, three different\nthreads of computation.",
    "start": "1853190",
    "end": "1863929"
  },
  {
    "text": "And so that's shown here:\nred, blue, and orange. And you've reached some\ncommunication stage. So the PPU program in this case\nis saying, send a message",
    "start": "1863930",
    "end": "1873560"
  },
  {
    "text": "to each of my SPEs, to each of\nmy different processors, that you're ready to start. And so now once every processor\ngets that message,",
    "start": "1873560",
    "end": "1881149"
  },
  {
    "text": "they can start computing. And let's assume they have data\nand so on ready to go. And what's going to happen is\neach processor is going to run",
    "start": "1881150",
    "end": "1889880"
  },
  {
    "text": "through the computation\nat different rates. Now this could be because\none processor is faster than another.",
    "start": "1889880",
    "end": "1895100"
  },
  {
    "text": "Or it could be because\none processor is more loaded than another. Or it could be just because\neach processor is assigned",
    "start": "1895100",
    "end": "1900550"
  },
  {
    "text": "sort of differing\namounts of work. So one has a short loop. One has a longer loop.",
    "start": "1900550",
    "end": "1906240"
  },
  {
    "text": "And so as the animation shows,\nsort of, execution proceeds and everybody's waiting until\nthe orange guy has completed.",
    "start": "1906240",
    "end": "1912930"
  },
  {
    "text": "But nobody could have made\nprogress until everybody's reached synchronization point\nbecause, you know, there's a",
    "start": "1912930",
    "end": "1920130"
  },
  {
    "text": "strict dependence that's being\nenforced here that says, I'm going to wait until everybody's\ntold me they're done before I go on to the\nnext step of computation.",
    "start": "1920130",
    "end": "1928130"
  },
  {
    "text": "And so you know, in Cell\nyou do that using mailboxes in this case. That clear so far?",
    "start": "1928130",
    "end": "1933610"
  },
  {
    "text": " So how do you get around this\nload balancing problem?",
    "start": "1933610",
    "end": "1939160"
  },
  {
    "start": "1938000",
    "end": "1938000"
  },
  {
    "text": "Well, there are two\ndifferent ways. There's static load balancing. I know my application\nreally, really well.",
    "start": "1939160",
    "end": "1944540"
  },
  {
    "text": "And I understand sort of\ndifferent computations. So what I can do is I can divide\nup the work and have a",
    "start": "1944540",
    "end": "1949920"
  },
  {
    "text": "static mapping of the work\nto my processors. And static mapping just means,\nyou know, in this particular",
    "start": "1949920",
    "end": "1956889"
  },
  {
    "text": "example, that I'm going to\nassign the work to different processors and that's what\nthe processors will do.",
    "start": "1956890",
    "end": "1963820"
  },
  {
    "text": "Work can't shift around\nbetween processors. And so in this case I\nhave a work queue.",
    "start": "1963820",
    "end": "1969330"
  },
  {
    "text": "Each of those bars is\nsome computation. You know, I can assign some\nchunk to P1, processor one,",
    "start": "1969330",
    "end": "1975590"
  },
  {
    "text": "some chunk to processor two. And then computation\ncan go on. Those allocations\ndon't change.",
    "start": "1975590",
    "end": "1983320"
  },
  {
    "text": "So this works well if I\nunderstand the application, well and I know the computation,\nand my cores are",
    "start": "1983320",
    "end": "1988760"
  },
  {
    "text": "relatively homogeneous and, you\nknow, there's not a lot of contention for them. So if all the cores are the\nsame, each core has an equal",
    "start": "1988760",
    "end": "1996690"
  },
  {
    "text": "amount of work -- the total\namount of work -- this works really well because nobody\nis sitting too idle. It doesn't work so well for\nheterogeneous architectures or",
    "start": "1996690",
    "end": "2004140"
  },
  {
    "text": "multicores. Because one might be faster\nthan the other. It increases the complexity of\nthe allocation I need to do.",
    "start": "2004140",
    "end": "2009470"
  },
  {
    "text": "If there's a lot of contention\nfor some resources, then that can affect the static\nload balancing.",
    "start": "2009470",
    "end": "2016470"
  },
  {
    "text": "So work distribution might\nend up being uneven. So the alternative is dynamic\nload balancing.",
    "start": "2016470",
    "end": "2023740"
  },
  {
    "text": "And you certainly could do\nsort of a hybrid load balancing, static plus\ndynamic mechanism.",
    "start": "2023740",
    "end": "2028860"
  },
  {
    "start": "2025000",
    "end": "2025000"
  },
  {
    "text": "Although I don't have\nthat in the slides. So in the dynamic load\nbalancing scheme, two",
    "start": "2028860",
    "end": "2037020"
  },
  {
    "text": "different mechanisms I'm\ngoing to illustrate. So in the first scheme, you\nstart with something like the static mechanism.",
    "start": "2037020",
    "end": "2043610"
  },
  {
    "text": "So I have some work going\nto processor one. And I have some work going\nto processor two.",
    "start": "2043610",
    "end": "2050080"
  },
  {
    "text": "But then as processor two\nexecutes and completes faster than processor one, it takes on\nsome of the additional work",
    "start": "2050080",
    "end": "2057610"
  },
  {
    "text": "from processor one. So the work that was here\nis now shifted. And so you can keep helping\nout, you know, your other",
    "start": "2057610",
    "end": "2064159"
  },
  {
    "text": "processors to compute\nthings faster. In the other scheme, you have\na work queue where you",
    "start": "2064160",
    "end": "2069629"
  },
  {
    "text": "essentially are distributing\nwork on the fly. So as things complete, you're\njust sending them",
    "start": "2069630",
    "end": "2075740"
  },
  {
    "text": "more work to do. So in this animation\nhere, I start off. I send work to two different\nprocessors.",
    "start": "2075740",
    "end": "2081820"
  },
  {
    "text": "P2 is really fast so it's just\nzipping through things. And then P1 eventually finishes\nand new work is",
    "start": "2081820",
    "end": "2088360"
  },
  {
    "text": "allocated to the two\ndifferent schemes. So dynamic load balancing is\nintended to sort of give equal",
    "start": "2088360",
    "end": "2094120"
  },
  {
    "text": "amounts of work in a different\nscheme for processors. So it really increased\nutilization and spent less and",
    "start": "2094120",
    "end": "2099880"
  },
  {
    "text": "less time being idle. OK. So load balancing was one part\nof sort of how granularity can",
    "start": "2099880",
    "end": "2108480"
  },
  {
    "text": "have a performance trade-off. The other is synchronization. So there were already some good\nquestions as to, well,",
    "start": "2108480",
    "end": "2114520"
  },
  {
    "text": "you know, how does this play\ninto overall execution? When can I wait? When can't I wait? So I'm going to illustrate it\nwith just a simple data",
    "start": "2114520",
    "end": "2121930"
  },
  {
    "start": "2116000",
    "end": "2116000"
  },
  {
    "text": "dependence graph. Although you can imagine that\nin each one of these circles there's some really heavy\nload computation.",
    "start": "2121930",
    "end": "2127550"
  },
  {
    "text": "And you'll see that in the\nnext lecture, in fact. So if I have some simple\ncomputation here -- I have some operands.",
    "start": "2127550",
    "end": "2133930"
  },
  {
    "text": "I'm doing an addition. Here I do another addition. I need both of these results\nbefore I can do this multiplication.",
    "start": "2133930",
    "end": "2140140"
  },
  {
    "text": "Here I have, you know, some\nloop that's adding through some array elements. I need all those results before\nI do final substraction",
    "start": "2140140",
    "end": "2146869"
  },
  {
    "text": "and produce my final result. So what are some synchronization\npoints here?",
    "start": "2146870",
    "end": "2152330"
  },
  {
    "text": "Well, it really depends on how\nI allocate the different instructions to processors.",
    "start": "2152330",
    "end": "2159089"
  },
  {
    "text": "So if I have an allocation that\njust says, well, let's put all these chains on one\nprocessor, put these two",
    "start": "2159090",
    "end": "2166240"
  },
  {
    "text": "chains on two different\nprocessors, well, where are my synchronization points? Well, it depends on where this\nguy is and where this guy is.",
    "start": "2166240",
    "end": "2173119"
  },
  {
    "text": "Because for this instruction\nto execute, it needs to receive data from P1 and P2. So if P1 and P2 are different\nfrom what's in that box,",
    "start": "2173120",
    "end": "2183260"
  },
  {
    "text": "somebody has to wait. And so there's a synchronization that has to happen. ",
    "start": "2183260",
    "end": "2192200"
  },
  {
    "text": "So essentially at all join\npoints there's potential for synchronization. But I can adjust the granularity\nso that I can",
    "start": "2192200",
    "end": "2197810"
  },
  {
    "text": "remove more and more\nsynchronization points. ",
    "start": "2197810",
    "end": "2206920"
  },
  {
    "text": "So if I had assigned all this\nentire sub-graph to the same processor, I really get rid of\nthe synchronization because it",
    "start": "2206920",
    "end": "2213579"
  },
  {
    "text": "is essentially local to that\nparticular processor. And there's no extra messaging\nthat would have to happen",
    "start": "2213580",
    "end": "2218819"
  },
  {
    "text": "across processors that says,\nI'm ready, or I'm ready to send you data, or you can move\non to the next step.",
    "start": "2218820",
    "end": "2224330"
  },
  {
    "text": "And so in this case the last\nsynchronization point would be at this join point. Let's say if it's allocated on\nP1 or on some other processor.",
    "start": "2224330",
    "end": "2232470"
  },
  {
    "text": "So how would I get rid of this\nsynchronization point? AUDIENCE: Do the whole thing.",
    "start": "2232470",
    "end": "2239079"
  },
  {
    "text": "PROFESSOR: Right. You put the entire thing\non a single processor. But you get no parallelism\nin this case. So the coarse-grain, fine-grain\ngrain parallelism",
    "start": "2239080",
    "end": "2246359"
  },
  {
    "text": "granularity issue\ncomes to play. So the last sort of thing I'm\ngoing to talk about in terms",
    "start": "2246360",
    "end": "2253410"
  },
  {
    "text": "of how granularity impacts\nperformance -- and this was already touched on -- is that\ncommunication is really not",
    "start": "2253410",
    "end": "2259569"
  },
  {
    "text": "cheap and can be quite\noverwhelming on a lot of architectures. And what's interesting about\nmulticores is that they're",
    "start": "2259570",
    "end": "2266600"
  },
  {
    "text": "essentially putting a lot\nmore resources closer together on a chip. So it essentially is changing\nthe factors for communication.",
    "start": "2266600",
    "end": "2275010"
  },
  {
    "text": "So rather than having, you know,\nyour parallel cluster now which is connected, say,\nby ethernet or some other",
    "start": "2275010",
    "end": "2280700"
  },
  {
    "text": "high-speed link, now you\nessentially have large clusters or will have large\nclusters on a chip.",
    "start": "2280700",
    "end": "2285920"
  },
  {
    "text": "So communication factors\nreally change. But the cost model is relatively\ncaptured by these",
    "start": "2285920",
    "end": "2295310"
  },
  {
    "text": "different parameters.  So what is the cost of\nmy communication?",
    "start": "2295310",
    "end": "2302450"
  },
  {
    "text": "Well, it's equal to, well, how\nmany messages am I sending and what is the frequency with\nwhich I'm sending them?",
    "start": "2302450",
    "end": "2310130"
  },
  {
    "text": "There's some overhead\nfor message. So I have to actually package\ndata together. I have to stick in a control\nheader and then send it out.",
    "start": "2310130",
    "end": "2317920"
  },
  {
    "text": "So that takes me some work\non the receiver side. I have to take the message. I maybe have to decode the\nheader, figure out where to",
    "start": "2317920",
    "end": "2326390"
  },
  {
    "text": "store the data that's coming\nin on the message. So there's some overhead\nassociated with that as well.",
    "start": "2326390",
    "end": "2331700"
  },
  {
    "text": "There's a network delay for\nsending a message, so putting a message on the network so that\nit can be transmitted, or",
    "start": "2331700",
    "end": "2338375"
  },
  {
    "text": "picking things up\noff the network. So there's a latency also\nassociated with how long does",
    "start": "2338375",
    "end": "2344049"
  },
  {
    "text": "it take for a message to get\nfrom point A to point B. What is the bandwidth that I\nhave across a link?",
    "start": "2344050",
    "end": "2351019"
  },
  {
    "text": "So if I have a lot of bandwidth\nthen that can really lower my communication cost. But\nif I have little bandwidth",
    "start": "2351020",
    "end": "2356670"
  },
  {
    "text": "then that can really\ncreate contention. How much data am I sending? And, you know, number\nof messages.",
    "start": "2356670",
    "end": "2362780"
  },
  {
    "text": "So this numerator here is really\nan average of the data that you're sending\nper communication.",
    "start": "2362780",
    "end": "2369590"
  },
  {
    "text": "There's a cost induced\nper contention. And then finally there's\n-- so all of these are added factors.",
    "start": "2369590",
    "end": "2375579"
  },
  {
    "text": "The higher they are, except for\nbandwidth, because it's in the denominator here,\nthe worse your communication cost becomes.",
    "start": "2375580",
    "end": "2380800"
  },
  {
    "text": "So you can try to reduce the\ncommunication cost by communicating less.",
    "start": "2380800",
    "end": "2386220"
  },
  {
    "text": "So you adjust your\ngranularity. And that can impact your\nsynchronization or what kind of data you're shipping\naround.",
    "start": "2386220",
    "end": "2392960"
  },
  {
    "text": "You can do some architectural\ntweaks or maybe some software tweaks to really get the network\nlatency down and the",
    "start": "2392960",
    "end": "2398800"
  },
  {
    "text": "overhead per message down. So on something like raw\narchitecture, which we saw in",
    "start": "2398800",
    "end": "2403880"
  },
  {
    "text": "Saman's lecture, there's a\nreally fast mechanism to communicate your nearest\nneighbor in three cycles.",
    "start": "2403880",
    "end": "2408900"
  },
  {
    "text": "So one processor can send a\nsingle operand to another reasonably fast. You know, you\ncan improve the bandwidth",
    "start": "2408900",
    "end": "2416310"
  },
  {
    "text": "again in architectural\nmechanism. You can do some tricks as to how\nyou package your data in",
    "start": "2416310",
    "end": "2421720"
  },
  {
    "text": "each message. And lastly, what I'm going to\ntalk about in a couple of",
    "start": "2421720",
    "end": "2427380"
  },
  {
    "text": "slides is, well, I can also\nimprove it using some mechanisms that try\nto increase the overlap between messages.",
    "start": "2427380",
    "end": "2433390"
  },
  {
    "text": "And what does this\nreally mean? What am I overlapping it with? And it's really the\ncommunication and computation",
    "start": "2433390",
    "end": "2440100"
  },
  {
    "text": "stages are going to somehow\nget aligned. So before I actually show you\nthat, I just want to point out",
    "start": "2440100",
    "end": "2446300"
  },
  {
    "text": "that there are two kinds\nof messages. There's data messages, and these\nare, for example, the arrays that I'm sending around\nto different processors for",
    "start": "2446300",
    "end": "2454240"
  },
  {
    "text": "the distance calculations\nbetween points in space. But there are also\ncontrol messages.",
    "start": "2454240",
    "end": "2459650"
  },
  {
    "text": "So control messages essentially\nsay, I'm done, or I'm ready to go, or is there\nany work for me to do?",
    "start": "2459650",
    "end": "2466900"
  },
  {
    "text": "So on Cell, control messages,\nyou know, you can think of using Mailboxes for those and\nthe DMAs for doing the data",
    "start": "2466900",
    "end": "2472560"
  },
  {
    "text": "communication. So data messages are relatively\nmuch larger -- you're sending a lot of data\n-- versus control messages",
    "start": "2472560",
    "end": "2479150"
  },
  {
    "text": "that are really much shorter,\njust essentially just sending you very brief information. ",
    "start": "2479150",
    "end": "2487640"
  },
  {
    "text": "So in order to get that overlap,\nwhat you can do is essentially use this concept\nof pipelining.",
    "start": "2487640",
    "end": "2493609"
  },
  {
    "text": "So you've seen pipelining\nin superscalar. Someone talked about that. And what you are essentially\ntrying to do is break up the",
    "start": "2493610",
    "end": "2500130"
  },
  {
    "text": "communication and computation\ninto different stages and then figure out a way to overlap\nthem so that you can",
    "start": "2500130",
    "end": "2505859"
  },
  {
    "text": "essentially hide the\nlatency for the sends and the receives.",
    "start": "2505860",
    "end": "2511090"
  },
  {
    "text": "So let's say you have some work\nthat you're doing, and it really requires you to\nsend the data --",
    "start": "2511090",
    "end": "2517570"
  },
  {
    "text": "somebody has to send you the\ndata or you essentially have to wait until you get it. And then after you've waited and\nthe data is there, you can",
    "start": "2517570",
    "end": "2524849"
  },
  {
    "text": "actually go on and\ndo your work. So these are color coded. So this is essentially one\niteration of the work.",
    "start": "2524850",
    "end": "2531540"
  },
  {
    "text": "And so you could overlap them\nby breaking up the work into send, wait, work stages, where\neach iteration trying to send",
    "start": "2531540",
    "end": "2541050"
  },
  {
    "text": "or request the data for the next\niteration, I wait on the data from a previous iteration\nand then I do my work.",
    "start": "2541050",
    "end": "2547420"
  },
  {
    "text": "So depending on how I partition,\nI can really get really good overlap. And so what you want to get to\nis the concept of the steady",
    "start": "2547420",
    "end": "2555320"
  },
  {
    "text": "state, where in your main loop\nbody, all you're doing is essentially pre-fetching or\nrequesting data that's going",
    "start": "2555320",
    "end": "2563589"
  },
  {
    "text": "to be used in future iterations\nfor future work. And then you're waiting on --",
    "start": "2563590",
    "end": "2569890"
  },
  {
    "text": "yeah. I think my color coding\nis a little bogus. That's good.",
    "start": "2569890",
    "end": "2575860"
  },
  {
    "text": "So here's an example of how\nyou might do this kind of buffer pipelining in Cell.",
    "start": "2575860",
    "end": "2581700"
  },
  {
    "text": "So I have some main loop that's\ngoing to do some work, that's encapsulating\nthis process data.",
    "start": "2581700",
    "end": "2587670"
  },
  {
    "text": "And what I'm going to\nuse is two buffers. So the scheme is also called\ndouble buffering.",
    "start": "2587670",
    "end": "2592750"
  },
  {
    "text": "I'm going to use this ID to\nrepresent which buffer I'm going to use. So it's either buffer\nzero or buffer one.",
    "start": "2592750",
    "end": "2598910"
  },
  {
    "text": "And this instruction here\nessentially flips the bit. So it's either zero or one. So I fetch data into buffer zero\nand then I enter my loop.",
    "start": "2598910",
    "end": "2607619"
  },
  {
    "text": "So this is essentially the first\nsend, which is trying to get me one iteration ahead.",
    "start": "2607620",
    "end": "2613329"
  },
  {
    "text": "So I enter this mail loop and\nI do some calculation to figure out where to write\nthe next data.",
    "start": "2613330",
    "end": "2620380"
  },
  {
    "text": "And then I do another request\nfor the next data item that I'm going to -- sorry, there's\nan m missing here --",
    "start": "2620380",
    "end": "2627800"
  },
  {
    "text": "I'm going to fetch data into\na different buffer, right. This is ID where I've already\nflipped the bit once.",
    "start": "2627800",
    "end": "2634359"
  },
  {
    "text": "So this get is going to write\ndata into buffer zero. And this get is going to write\ndata into buffer one.",
    "start": "2634360",
    "end": "2641730"
  },
  {
    "text": "I flip the bit again. So now I'm going to issue a wait\ninstruction that says is",
    "start": "2641730",
    "end": "2648720"
  },
  {
    "text": "the data from buffer\nzero ready? And if it is then I can go on\nand actually do my work. Does that make sense?",
    "start": "2648720",
    "end": "2655590"
  },
  {
    "text": "People are confused? Should I go over it again?  AUDIENCE: [INAUDIBLE]",
    "start": "2655590",
    "end": "2661720"
  },
  {
    "text": "PROFESSOR: So this\nis an [? x or. ?] So I could have just said buffer\nequals zero or buffer",
    "start": "2661720",
    "end": "2667710"
  },
  {
    "text": "equals one. ",
    "start": "2667710",
    "end": "2673220"
  },
  {
    "text": "Oh, sorry. This is one. Yeah. Yeah. So this is a one here.",
    "start": "2673220",
    "end": "2680950"
  },
  {
    "text": "Last-minute editing. It's right there. Did that confuse you? AUDIENCE: No. But, like, I don't\nsee [INAUDIBLE]",
    "start": "2680950",
    "end": "2687810"
  },
  {
    "text": "PROFESSOR: Oh. OK. So I'll go over it again. So this get here is going\nto write into ID zero.",
    "start": "2687810",
    "end": "2693599"
  },
  {
    "text": "So that's buffer zero. And then I'm going\nto change the ID. So imagine there's a one here.",
    "start": "2693600",
    "end": "2699400"
  },
  {
    "text": "So now the next time I use ID,\nwhich is here, I'm trying to get the data.",
    "start": "2699400",
    "end": "2704950"
  },
  {
    "text": "And I'm going to write\nit to buffer one. The DMA on the Cell processor\nessentially says I can send",
    "start": "2704950",
    "end": "2711270"
  },
  {
    "text": "this request off and I can check\nlater to see when that data is available. But that data is going to go\ninto a different buffer,",
    "start": "2711270",
    "end": "2717940"
  },
  {
    "text": "essentially B1. Whereas I'm going to work\non buffer zero. Because I changed the\nID back here.",
    "start": "2717940",
    "end": "2725920"
  },
  {
    "text": "Now you get it? So I fetch data into buffer\nzero initially before I start to loop.",
    "start": "2725920",
    "end": "2731540"
  },
  {
    "text": "And then I start working. I probably should have had\nan animation in here. So then you go into\nyour main loop.",
    "start": "2731540",
    "end": "2739430"
  },
  {
    "text": "You try to start fetching into\nbuffet one and then you try to compute out of buffer zero. But before you can start\ncomputing out of buffer zero,",
    "start": "2739430",
    "end": "2746130"
  },
  {
    "text": "you just have to make sure\nthat your data is there. And so that's what the\nsynchronization is doing here.",
    "start": "2746130",
    "end": "2752790"
  },
  {
    "text": "Hope that was clear. OK, so this kind of computation\nand communication",
    "start": "2752790",
    "end": "2758710"
  },
  {
    "text": "overlap really helps in\nhiding the latency. And it can be real useful\nin terms of improving",
    "start": "2758710",
    "end": "2764990"
  },
  {
    "text": "performance.  And there are different kinds\nof communication patterns.",
    "start": "2764990",
    "end": "2773450"
  },
  {
    "text": "So there's point to point. And you can use these both for\ndata communication or control communication.",
    "start": "2773450",
    "end": "2779060"
  },
  {
    "text": "And it just means that, you\nknow, one processor can explicitly send a message\nto another processor. There's also broadcast that\nsays, hey, I have some data",
    "start": "2779060",
    "end": "2786345"
  },
  {
    "text": "that everybody's interested in,\nso I can just broadcast it to everybody on the network. Or a reduce, which\nis the opposite.",
    "start": "2786345",
    "end": "2792930"
  },
  {
    "text": "It says everybody on the network\nhas data that I need to compute, so everybody\nsend me their data.",
    "start": "2792930",
    "end": "2799840"
  },
  {
    "text": "There's an all to all, which\nsays all processors should just do a global exchange of\ndata that they have. And then",
    "start": "2799840",
    "end": "2806410"
  },
  {
    "text": "there's a scatter\nand a gather. So a scatter and a gather are\nreally different types of broadcast. So it's one to\nseveral or one to many.",
    "start": "2806410",
    "end": "2814770"
  },
  {
    "text": "And gather, which\nis many to one. So this is useful when you're\ndoing a computation that",
    "start": "2814770",
    "end": "2820370"
  },
  {
    "text": "really is trying to pull data\nin together but only from a subset of all processors.",
    "start": "2820370",
    "end": "2826260"
  },
  {
    "text": "So it depends on how you've\npartitioned your problems.",
    "start": "2826260",
    "end": "2832250"
  },
  {
    "text": "So there's a well-known sort\nof message passing library specification called MPI that\ntries to specify all of these",
    "start": "2832250",
    "end": "2844950"
  },
  {
    "start": "2837000",
    "end": "2837000"
  },
  {
    "text": "different communications in\norder to sort of facilitate parallel programming.",
    "start": "2844950",
    "end": "2850310"
  },
  {
    "text": "Its full featured actually has\nmore types of communications and more kinds of functionality\nthan I showed on",
    "start": "2850310",
    "end": "2856660"
  },
  {
    "text": "the previous slides. But it's not a language or\na compiler specification. It's really just a library\nthat you can implement in",
    "start": "2856660",
    "end": "2863470"
  },
  {
    "text": "various ways on different\narchitectures. Again, it's same program,\nmultiple data, or supports the",
    "start": "2863470",
    "end": "2870720"
  },
  {
    "text": "SPMD model. And it works reasonably well for\nparallel architectures for",
    "start": "2870720",
    "end": "2875990"
  },
  {
    "text": "clusters, heterogeneous\nmulticores, homogeneous multicores.",
    "start": "2875990",
    "end": "2881830"
  },
  {
    "text": "Because really all it's doing\nis just abstracting out -- it's giving you a mechanism\nto abstract out all the",
    "start": "2881830",
    "end": "2888130"
  },
  {
    "text": "communication that you would\nneed in your computation. So you can have additional\nthings like precise buffer",
    "start": "2888130",
    "end": "2895280"
  },
  {
    "text": "management. You can have some collective\noperations. I'll show an example of for\ndoing things things in a",
    "start": "2895280",
    "end": "2902985"
  },
  {
    "text": "scalable manner when a lot of\nthings need to communicate with each other.",
    "start": "2902985",
    "end": "2909140"
  },
  {
    "text": "So just a brief history of\nwhere MPI came from. And, you know, very early\nwhen, you know, parallel",
    "start": "2909140",
    "end": "2915270"
  },
  {
    "text": "computers started becoming more\nand more widespread and there were these networks and\npeople had problems porting",
    "start": "2915270",
    "end": "2920720"
  },
  {
    "text": "their applications or writing\napplications for these [? came, ?] just because it was\ndifficult, as you might be",
    "start": "2920720",
    "end": "2925860"
  },
  {
    "text": "finding in terms of programming\nthings with the Cell processor. You know, there needed to be\nways to sort of address the",
    "start": "2925860",
    "end": "2932800"
  },
  {
    "text": "spectrum of communication. And it often helps to have a\nstandard because if everybody",
    "start": "2932800",
    "end": "2938330"
  },
  {
    "text": "implements the same standard\nspecification, that allows your code to be ported\naround from one",
    "start": "2938330",
    "end": "2943369"
  },
  {
    "text": "architecture to the other. And so MPI came around. The forum was organized\nin 1992.",
    "start": "2943370",
    "end": "2951130"
  },
  {
    "text": "And that had a lot of people\nparticipating in it from vendors, you know, people like\nIBM, a company like IBM,",
    "start": "2951130",
    "end": "2957000"
  },
  {
    "text": "Intel, and people who had\nexpertise in writing",
    "start": "2957000",
    "end": "2963030"
  },
  {
    "text": "libraries, users who were\ninterested in using these kinds of specifications to\ndo their computations, so",
    "start": "2963030",
    "end": "2971910"
  },
  {
    "text": "scientific people who were\nin the scientific domain. And it was finished in\nabout 18 months.",
    "start": "2971910",
    "end": "2978050"
  },
  {
    "text": "I don't know if that's a\nreasonably long time or a short time. But considering, you know, I\nthink the MPEG-4 standard took",
    "start": "2978050",
    "end": "2984339"
  },
  {
    "text": "a bit longer to do, as\na comparison point. I don't have the actual data.",
    "start": "2984340",
    "end": "2989880"
  },
  {
    "text": "So point-to-point\ncommunication -- and again, a reminder, this is\nhow you would do it on Cell.",
    "start": "2989880",
    "end": "2996590"
  },
  {
    "text": "These are SPE sends\nand receives. You have one processor\nthat's sending",
    "start": "2996590",
    "end": "3002490"
  },
  {
    "text": "it to another processor. Or you have some network\nin between. And processor A can essentially\nsend the data",
    "start": "3002490",
    "end": "3008430"
  },
  {
    "text": "explicitly to processor two. And the message in this case\nwould include how the data is",
    "start": "3008430",
    "end": "3014880"
  },
  {
    "text": "packaged, some other information\nsuch as the length of the data, destination,\npossibly some tag so you can",
    "start": "3014880",
    "end": "3020570"
  },
  {
    "text": "identify the actual\ncommunication. And, you know, there's an actual\nmapping for the actual",
    "start": "3020570",
    "end": "3026559"
  },
  {
    "text": "functions on Cell. And there's a get for the send\nand a put for the receive.",
    "start": "3026560",
    "end": "3032950"
  },
  {
    "start": "3032950",
    "end": "3039230"
  },
  {
    "text": "So there's a question of, well,\nhow do I know if my data actually got sent? How do I know if it\nwas received?",
    "start": "3039230",
    "end": "3045410"
  },
  {
    "start": "3042000",
    "end": "3042000"
  },
  {
    "text": "And there's, you know, you can\nthink of a synchronous send and a synchronous receive, or\nasynchronous communication.",
    "start": "3045410",
    "end": "3052200"
  },
  {
    "text": "So in the synchronous\ncommunication, you actually wait for notification. So this is kind of like\nyour fax machine.",
    "start": "3052200",
    "end": "3057250"
  },
  {
    "text": "You put something\ninto your fax. It goes out and you eventually\nget a beep that says your transmission was OK.",
    "start": "3057250",
    "end": "3062260"
  },
  {
    "text": "Or if it wasn't OK then, you\nknow, you get a message that says, you know, something\nwent wrong. And you can redo your\ncommunication.",
    "start": "3062260",
    "end": "3069519"
  },
  {
    "text": "An asynchronous send is\nkind of like your -- AUDIENCE: Most [UNINTELLIGIBLE]\nyou could get a reply too. PROFESSOR: Yeah, you\ncan get a reply.",
    "start": "3069520",
    "end": "3075340"
  },
  {
    "text": "Thanks. An asynchronous send, it's like\nyou write a letter, you go put it in the mailbox, and\nyou don't know whether it",
    "start": "3075340",
    "end": "3080680"
  },
  {
    "text": "actually made it into the actual\npostman's bag and it",
    "start": "3080680",
    "end": "3086425"
  },
  {
    "text": "was delivered to your\ndestination or if it was actually delivered. So you only know that the\nmessage was sent.",
    "start": "3086425",
    "end": "3094200"
  },
  {
    "text": "You know, you put it\nin the mailbox. But you don't know anything else\nabout what happened to the message along the way.",
    "start": "3094200",
    "end": "3101099"
  },
  {
    "text": "There's also the concept\nof a blocking versus a non-blocking message. So this is orthogonal really\nto synchronous versus",
    "start": "3101100",
    "end": "3108109"
  },
  {
    "text": "asynchronous. So in blocking messages, a\nsender waits until there's",
    "start": "3108110",
    "end": "3114940"
  },
  {
    "text": "some signal that says the\nmessage has been transmitted. So this is, for example if I'm\nwriting data into a buffer,",
    "start": "3114940",
    "end": "3123180"
  },
  {
    "text": "and the buffer essentially gets\ntransmitted to somebody else, we wait until the\nbuffer is empty.",
    "start": "3123180",
    "end": "3130590"
  },
  {
    "text": "And what that means is that\nsomebody has read it on the other end or somebody has\ndrained that buffer from somewhere else.",
    "start": "3130590",
    "end": "3136670"
  },
  {
    "text": "The receiver, if he's waiting on\ndata, well, he just waits. He essentially blocks until\nsomebody has put",
    "start": "3136670",
    "end": "3141850"
  },
  {
    "text": "data into the buffer. And you can get into potential\ndeadlock situations. So you saw deadlock with locks\nin the concurrency talk.",
    "start": "3141850",
    "end": "3150099"
  },
  {
    "text": "I'm going to show\nyou a different kind of deadlock example. ",
    "start": "3150100",
    "end": "3155200"
  },
  {
    "text": "An example of a blocking\nsend on Cell --",
    "start": "3155200",
    "end": "3160630"
  },
  {
    "text": "allows you to use mailboxes. Or you can sort of use\nmailboxes for that.",
    "start": "3160630",
    "end": "3167260"
  },
  {
    "text": "Mailboxes again are just for\ncommunicating short messages, really, not necessarily for\ncommunicating data messages.",
    "start": "3167260",
    "end": "3173910"
  },
  {
    "text": "So an SPE does some work, and\nthen it writes out a message, in this case to notify the PPU\nthat, let's say, it's done.",
    "start": "3173910",
    "end": "3182750"
  },
  {
    "text": "And then it goes on and\ndoes more work. And then it wants to notify\nthe PPU of something else.",
    "start": "3182750",
    "end": "3188230"
  },
  {
    "text": "So in this case this particular\nsend will block because, let's say, the PPU\nhasn't drained its mailbox.",
    "start": "3188230",
    "end": "3194930"
  },
  {
    "text": "It hasn't read the mailbox. So you essentially stop and wait\nuntil the PPU has, you",
    "start": "3194930",
    "end": "3200960"
  },
  {
    "text": "know, caught up. AUDIENCE: So all mailbox\nsends are blocking?",
    "start": "3200960",
    "end": "3206860"
  },
  {
    "text": "PROFESSOR: Yes. David says yes. ",
    "start": "3206860",
    "end": "3216680"
  },
  {
    "text": "A non-blocking send is something\nthat essentially allows you to send a message\nout and just continue on.",
    "start": "3216680",
    "end": "3224400"
  },
  {
    "text": "You don't care exactly about\nwhat's happened to the message or what's going on with\nthe receiver.",
    "start": "3224400",
    "end": "3231910"
  },
  {
    "text": "So you write the data into\nthe buffer and you just continue executing. And this really helps you in\nterms of avoiding idle times",
    "start": "3231910",
    "end": "3238740"
  },
  {
    "text": "and deadlocks, but it might\nnot always be the thing that you want. So an example of sort of a\nnon-blocking send and wait on",
    "start": "3238740",
    "end": "3245970"
  },
  {
    "text": "Cell is using the DMAs\nto ship data out. You know, you can put something,\nput in a request to",
    "start": "3245970",
    "end": "3251579"
  },
  {
    "text": "send data out on the DMA. And you could wait for it if you\nwant in terms of reading",
    "start": "3251580",
    "end": "3259190"
  },
  {
    "text": "the status bits to make\nsure it's completed. OK, so what is a source of\ndeadlock in the blocking case?",
    "start": "3259190",
    "end": "3267530"
  },
  {
    "text": "And it really comes about if you\ndon't really have enough buffering in your communication\nnetwork.",
    "start": "3267530",
    "end": "3273140"
  },
  {
    "start": "3268000",
    "end": "3268000"
  },
  {
    "text": "And often you can resolve that\nby having additional storage. So let's say I have processor\none and processor two and",
    "start": "3273140",
    "end": "3278700"
  },
  {
    "text": "they're trying to send messages\nto each other. So processor one sends a message\nat the same time",
    "start": "3278700",
    "end": "3283710"
  },
  {
    "text": "processor two sends a message. And these are going\nto go, let's say, into the same buffer.",
    "start": "3283710",
    "end": "3289200"
  },
  {
    "text": "Well, neither can make progress\nbecause somebody has to essentially drain that buffer\nbefore these receives",
    "start": "3289200",
    "end": "3295930"
  },
  {
    "text": "can execute.  So what happens with that code\nis it really depends on how",
    "start": "3295930",
    "end": "3302990"
  },
  {
    "text": "much buffering you have\nbetween the two. If you have a lot of buffering,\nthen you may never see the deadlock.",
    "start": "3302990",
    "end": "3308000"
  },
  {
    "text": "But if you have a really tiny\nbuffer, then you do a send.",
    "start": "3308000",
    "end": "3313930"
  },
  {
    "text": "The other person can't do the\nsend because the buffer hasn't been drained.",
    "start": "3313930",
    "end": "3318970"
  },
  {
    "text": "And so you end up\nwith a deadlock. And so a potential solution\nis, well, you actually increase your buffer length.",
    "start": "3318970",
    "end": "3324620"
  },
  {
    "text": "But that doesn't always\nwork because you can still get into trouble. So what you might need to do\nis essentially be more",
    "start": "3324620",
    "end": "3330040"
  },
  {
    "text": "diligent about how you order\nyour sends and receives. So if you have processor one\ndoing a send, make sure it's",
    "start": "3330040",
    "end": "3336740"
  },
  {
    "text": "matched up with a receive\non the other end. And similarly, if you're doing\na receive here, make sure",
    "start": "3336740",
    "end": "3342049"
  },
  {
    "text": "there's sort of a matching\nsend on the other end. And that helps you in sort of\nmaking sure that things are operating reasonably in lock\nstep at, you know, partially",
    "start": "3342050",
    "end": "3351160"
  },
  {
    "text": "ordered times. ",
    "start": "3351160",
    "end": "3359750"
  },
  {
    "text": "That was really examples of\npoint-to-point communication. A broadcast mechanism is\nslightly different.",
    "start": "3359750",
    "end": "3365990"
  },
  {
    "text": "It says, I have data that I\nwant to send to everybody. It could be really efficient\nfor sending short control",
    "start": "3365990",
    "end": "3371750"
  },
  {
    "text": "messages, maybe even efficient\nfor sending data messages. So as an example, if you\nremember our calculation of",
    "start": "3371750",
    "end": "3378950"
  },
  {
    "text": "distances between all points,\nthe parallelization strategy said, well, I'm going\nto send one copy of",
    "start": "3378950",
    "end": "3384900"
  },
  {
    "text": "the array A to everybody. In the two processor\ncase that was easy. But if I have n processors,\nthen rather than sending",
    "start": "3384900",
    "end": "3392730"
  },
  {
    "text": "point-to-point communication\nfrom A to everybody else, what I could do is just, say,\nbroadcast A to everybody and",
    "start": "3392730",
    "end": "3398300"
  },
  {
    "text": "they can grab it off\nthe network. So in MPI there's this\nfunction, MPI",
    "start": "3398300",
    "end": "3405210"
  },
  {
    "text": "broadcast, that does that. I'm using sort of generic\nabstract sends, receives and",
    "start": "3405210",
    "end": "3411700"
  },
  {
    "text": "broadcasts in my examples. So you can broadcast\nA to everybody. And then if I have n processors,\nthen what I might",
    "start": "3411700",
    "end": "3419550"
  },
  {
    "text": "do is distribute the m's in a\nround robin manner to each of the different processes. So you pointed this out.",
    "start": "3419550",
    "end": "3425710"
  },
  {
    "text": "I don't have to send\nB to everybody. I can just send, you\nknow, in this case, one particular element. Is that clear?",
    "start": "3425710",
    "end": "3432839"
  },
  {
    "text": " AUDIENCE: There's no\nbroadcast on Cell?",
    "start": "3432840",
    "end": "3438670"
  },
  {
    "text": "PROFESSOR: There is no\nbroadcast on Cell. There is no mechanism for\nreduction either.",
    "start": "3438670",
    "end": "3445680"
  },
  {
    "text": "And you can't quite do\nscatters and gathers. I don't think.",
    "start": "3445680",
    "end": "3452430"
  },
  {
    "text": "OK, so an example of a\nreduction, you know, I said it's the opposite of a\nbroadcast. Everybody has data",
    "start": "3452430",
    "end": "3457650"
  },
  {
    "start": "3454000",
    "end": "3454000"
  },
  {
    "text": "that needs to essentially\nget to the same point. So as an example, if everybody\nin this room had a value,",
    "start": "3457650",
    "end": "3465610"
  },
  {
    "text": "including myself, and I wanted\nto know what is the collective value of everybody in the\nroom, you all have to send me your data.",
    "start": "3465610",
    "end": "3470839"
  },
  {
    "text": "Now, this is important because\nif -- you know, in this case we're doing an addition. It's an associative operation.",
    "start": "3470840",
    "end": "3476290"
  },
  {
    "text": "So what we can do is we\ncan be smart about how the data is sent. So, you know, guys that are\nclose together can essentially",
    "start": "3476290",
    "end": "3482160"
  },
  {
    "text": "add up their numbers\nand forward me. So instead of getting\nn messages I can get log n messages. And so if every pair of you\nadded your numbers and",
    "start": "3482160",
    "end": "3488450"
  },
  {
    "text": "forwarded me that, that cuts\ndown communication by half. And so you can, you know --\nstarting from the back of",
    "start": "3488450",
    "end": "3493640"
  },
  {
    "text": "room, by the time you get to\nme, I only get two messages instead of n messages.",
    "start": "3493640",
    "end": "3498799"
  },
  {
    "text": "So a reduction combines data\nfrom all processors. In MPI, you know, there's\nthis function MPI",
    "start": "3498800",
    "end": "3504030"
  },
  {
    "text": "reduce for doing that. And the collective operations\nare things that are",
    "start": "3504030",
    "end": "3509180"
  },
  {
    "text": "associative. And subtract -- sorry. And or and -- you can read\nthem on the slide.",
    "start": "3509180",
    "end": "3519500"
  },
  {
    "text": "There is a semantic caveat here\nthat no processor can finish the reduction before all\nprocessors have at least",
    "start": "3519500",
    "end": "3525760"
  },
  {
    "text": "sent it one data or have\ncontributed, rather, a particular value.",
    "start": "3525760",
    "end": "3531790"
  },
  {
    "text": "So in many numerical algorithms,\nyou can actually use the broadcast and send to\nbroadcast and reduce in place",
    "start": "3531790",
    "end": "3540200"
  },
  {
    "text": "of sends and receives because\nit really improves the simplicity of your\ncomputation.",
    "start": "3540200",
    "end": "3546970"
  },
  {
    "text": "You don't have to do n sends\nto communicate there. You can just broadcast. It\ngives you a mechanism for essentially having a shared\nmemory abstraction on",
    "start": "3546970",
    "end": "3553970"
  },
  {
    "text": "distributed memory\narchitecture. There are things like all to all\ncommunication which would also help you in that sense.",
    "start": "3553970",
    "end": "3559730"
  },
  {
    "text": "Although I don't talk about all\nto all communication here.",
    "start": "3559730",
    "end": "3564960"
  },
  {
    "text": "So I'm going to show you an\nexample of sort of a more detailed MPI. But I also want to contrast this\nto the OpenMP programming",
    "start": "3564960",
    "end": "3572690"
  },
  {
    "text": "on shared memory processors\nbecause one might look simpler than the other.",
    "start": "3572690",
    "end": "3578470"
  },
  {
    "text": "So suppose that you have a\nnumerical integration method that essentially you're going\nto use to calculate pi.",
    "start": "3578470",
    "end": "3584990"
  },
  {
    "text": "So as you get finer and finer,\nyou can get more accurate -- as you shrink these intervals\nyou can get",
    "start": "3584990",
    "end": "3590030"
  },
  {
    "text": "better values for pi. And the code for doing\nthat is some C code.",
    "start": "3590030",
    "end": "3598690"
  },
  {
    "text": "You have some variables. And then you have a step that\nessentially tells you how many times you're going to\ndo this computation.",
    "start": "3598690",
    "end": "3604730"
  },
  {
    "text": "And for each time step you\ncalculate this particular function here. And you add it all up and in the\nend you can sort of print",
    "start": "3604730",
    "end": "3611040"
  },
  {
    "text": "out what is the value of\npi that you calculated. So clearly as, you know, as you\nshrink your intervals, you",
    "start": "3611040",
    "end": "3616599"
  },
  {
    "text": "can get more and more accurate\nmeasures of pi. So that translates to increasing\nthe number of steps",
    "start": "3616600",
    "end": "3622740"
  },
  {
    "text": "in that particular C code.",
    "start": "3622740",
    "end": "3629160"
  },
  {
    "text": "So you can use that numerical\nintegration to calculate pi with OpenMP.",
    "start": "3629160",
    "end": "3634900"
  },
  {
    "text": "And what that translates to is\n-- sorry, there should have been an animation here to ask\nyou what I should add in.",
    "start": "3634900",
    "end": "3641369"
  },
  {
    "text": "You have this particular loop. And this is computation that\nyou want to parallelize. And there is really four\nquestions that you essentially",
    "start": "3641370",
    "end": "3648890"
  },
  {
    "text": "have to go through. Are there variables\nthat are shared? Because you have to get\nthe process right.",
    "start": "3648890",
    "end": "3654420"
  },
  {
    "text": "If there are variables that\nare shared, you have to explicitly synchronize them and\nuse locks to protect them.",
    "start": "3654420",
    "end": "3661829"
  },
  {
    "text": "What values are private? So in OpenMP, things that are\nprivate are data on the stack,",
    "start": "3661830",
    "end": "3668690"
  },
  {
    "text": "things that are defined\nlexically within the scope of the computation that you\nencapsulate by an OpenMP",
    "start": "3668690",
    "end": "3676030"
  },
  {
    "text": "pragma, and what variables\nyou might want to use for a reduction. So in this case I'm doing a\nsummation, and this is the",
    "start": "3676030",
    "end": "3683490"
  },
  {
    "text": "computation that I\ncan parallelize. Then I essentially want to do\na reduction for the plus",
    "start": "3683490",
    "end": "3688770"
  },
  {
    "text": "operator since I'm doing an\naddition on this variable. This loop here is parallel.",
    "start": "3688770",
    "end": "3694760"
  },
  {
    "text": "It's data parallel. I can split it up. The for loop is also --",
    "start": "3694760",
    "end": "3701190"
  },
  {
    "text": "I can do this work\nsharing on it. So I use the parallel\nfor pragma. And the variable x\nhere is private.",
    "start": "3701190",
    "end": "3709359"
  },
  {
    "text": "It's defined here but I can\nessentially give a directive that says, this is private. You can essentially rename\nit on each processor.",
    "start": "3709360",
    "end": "3716290"
  },
  {
    "text": "Its value won't have any\neffect on the overall computation because each\ncomputation will have its own local copy.",
    "start": "3716290",
    "end": "3723910"
  },
  {
    "text": "That clear so far? So computing pi with integration\nusing MPI takes up",
    "start": "3723910",
    "end": "3729950"
  },
  {
    "text": "two slides. You know, I could fit it on one\nslide but you couldn't see it in the back.",
    "start": "3729950",
    "end": "3735170"
  },
  {
    "text": "So there's some initialization. In fact, I think there's only\nsix basic MPI commands that you need for computing.",
    "start": "3735170",
    "end": "3742119"
  },
  {
    "text": "Three of them are here and\nyou'll see the others are MPI send and MPI receive.",
    "start": "3742120",
    "end": "3747680"
  },
  {
    "text": "And there's one more that you'll\nsee on the next slide. So there's some loop\nthat says while I'm",
    "start": "3747680",
    "end": "3753170"
  },
  {
    "text": "not done keep computing. And what you do is you broadcast\nn to all the",
    "start": "3753170",
    "end": "3759120"
  },
  {
    "text": "different processors. N is really your time step. How many small intervals of\nexecution are you going to do?",
    "start": "3759120",
    "end": "3767660"
  },
  {
    "text": "And you can go through,\ndo your computation. So now this -- the MPI\nessentially encapsulates the computation over n processors.",
    "start": "3767660",
    "end": "3775290"
  },
  {
    "text": "And then you get to an MPI\nreduce command at some point that says, OK, what values\ndid everybody compute?",
    "start": "3775290",
    "end": "3781810"
  },
  {
    "text": "Do the reduction on that. Write that value into my MPI. Now what happens here is there's\nprocessor ID zero",
    "start": "3781810",
    "end": "3790700"
  },
  {
    "text": "which I'm going to consider\nthe master. So he's the one who's going to\nactually print out the value. So the reduction essentially\nsynchronizes until everybody's",
    "start": "3790700",
    "end": "3798780"
  },
  {
    "text": "communicated a value\nto processor zero. And then it can print\nout the pi.",
    "start": "3798780",
    "end": "3803990"
  },
  {
    "text": "And then you can finalize, which\nactually makes sure the computation can exit. And you can go on\nand terminate.",
    "start": "3803990",
    "end": "3810359"
  },
  {
    "start": "3810360",
    "end": "3815710"
  },
  {
    "text": "So the last concept in terms of\nunderstanding performance for parallelism is this\nnotion of locality.",
    "start": "3815710",
    "end": "3823010"
  },
  {
    "text": "And there's locality in your\ncommunication and locality in your computation. So what do I mean by that?",
    "start": "3823010",
    "end": "3830700"
  },
  {
    "text": "So in terms of communication,\nyou know, if I have two operations and let's say -- this\nis a picture or schematic",
    "start": "3830700",
    "end": "3838829"
  },
  {
    "text": "of what the MIT raw\nchip looks like. Each one of these is a core. There's some network, some basic\ncomputation elements.",
    "start": "3838830",
    "end": "3846620"
  },
  {
    "text": "And if I have, you know, an\naddition that feeds into a shift, well, I can put the\naddition here and the shift",
    "start": "3846620",
    "end": "3852910"
  },
  {
    "text": "there, but that means I have a\nreally long path that I need to go to in terms\nof communicating that data value around.",
    "start": "3852910",
    "end": "3859190"
  },
  {
    "text": "So the computation naturally\nshould just be closer together because that decreases the\nlatency that I need to",
    "start": "3859190",
    "end": "3865950"
  },
  {
    "text": "communicate. So rather than doing net\nmapping, what I might want to do is just go to somebody who is\nclose to me and available.",
    "start": "3865950",
    "end": "3872900"
  },
  {
    "text": "AUDIENCE: Also there\nare volume issues. So assume more than that. A lot of other people also\nwant to communicate.",
    "start": "3872900",
    "end": "3880308"
  },
  {
    "text": "So if [UNINTELLIGIBLE] randomly\ndistributed, you can assume there's a lot\nmore communication going into the channel.",
    "start": "3880309",
    "end": "3887880"
  },
  {
    "text": "Whereas if you put locality in\nthere then you can scale communication much better than\nscaling the network.",
    "start": "3887880",
    "end": "3896210"
  },
  {
    "text": " PROFESSOR: There's also a notion\nof locality in terms of",
    "start": "3896210",
    "end": "3902400"
  },
  {
    "text": "memory accesses. And these are potentially also\nvery important or more",
    "start": "3902400",
    "end": "3907880"
  },
  {
    "text": "important, rather, because\nof the latencies for accessing memory. So if I have, you know, this\nloop that's doing some",
    "start": "3907880",
    "end": "3915860"
  },
  {
    "text": "addition or some computation on\nan array and I distribute it, say, over four\nprocessors --",
    "start": "3915860",
    "end": "3921900"
  },
  {
    "text": "this is, again, let's assume\na data parallel loop. So what I can do is have a work\nsharing mechanism that",
    "start": "3921900",
    "end": "3927460"
  },
  {
    "text": "says, this thread here\nwill operate on the first four indices. This thread here will operate\non the next four indices and",
    "start": "3927460",
    "end": "3934135"
  },
  {
    "text": "the next four and\nthe next four. And then you essentially get to\njoin barrier and then you",
    "start": "3934135",
    "end": "3939530"
  },
  {
    "text": "can continue on. And if we consider how the\naccess patterns are going to",
    "start": "3939530",
    "end": "3944839"
  },
  {
    "text": "be generated for this particular\nloop, well, in the",
    "start": "3944840",
    "end": "3950730"
  },
  {
    "text": "sequential case I'm essentially generating them in sequence. So that allows me to exploit,\nfor example, on traditional",
    "start": "3950730",
    "end": "3956619"
  },
  {
    "text": "[? CAT ?] architecture, a notion\nof spatial locality. If I look at how things are\norganized in memory, in the",
    "start": "3956620",
    "end": "3963620"
  },
  {
    "text": "sequential case I can\nperhaps fetch an entire block at a time. So I can fetch all the\nelements of A0",
    "start": "3963620",
    "end": "3971340"
  },
  {
    "text": "to A3 in one shot. I can fetch all the elements\nof A4 to A7 in one shot.",
    "start": "3971340",
    "end": "3976819"
  },
  {
    "text": "And that allows me to\nessentially improve performance because I overlap\ncommunication.",
    "start": "3976820",
    "end": "3982080"
  },
  {
    "text": "I'm predicting that once I see\na reference, I'm going to use data that's adjacent\nto it in space.",
    "start": "3982080",
    "end": "3989140"
  },
  {
    "text": "There's also a notion of\ntemporal locality that says that if I use some particular\ndata element, I'm going to reuse it later on.",
    "start": "3989140",
    "end": "3995430"
  },
  {
    "text": "I'm not showing that here. But in the parallel case what\ncould happen is if each one of",
    "start": "3995430",
    "end": "4001190"
  },
  {
    "text": "these threads is requesting a\ndifferent data element -- and let's say execution essentially\nproceeds -- you",
    "start": "4001190",
    "end": "4009470"
  },
  {
    "text": "know, all the threads\nare requesting their data at the same time. Then all these requests are\ngoing to end up going to the",
    "start": "4009470",
    "end": "4016240"
  },
  {
    "text": "same memory bank. The first thread is requesting\nace of zero.",
    "start": "4016240",
    "end": "4022420"
  },
  {
    "text": "The next thread is requesting\nace of four, the next thread ace of eight, next\nthread ace of 12.",
    "start": "4022420",
    "end": "4028750"
  },
  {
    "text": "And all of these happen to be\nin the same memory bank. So what that means is, you\nknow, there's a lot of contention for that\none memory bank.",
    "start": "4028750",
    "end": "4034940"
  },
  {
    "text": "And in effect I've serialized\nthe computation. Right? Everybody see that?",
    "start": "4034940",
    "end": "4040620"
  },
  {
    "text": "And, you know, this can be\na problem in that you can essentially fully serialize the\ncomputation in that, you",
    "start": "4040620",
    "end": "4046920"
  },
  {
    "text": "know, there's contention on the\nfirst bank, contention on the second bank, and then\ncontention on the third bank,",
    "start": "4046920",
    "end": "4053619"
  },
  {
    "text": "and then contention on\nthe fourth bank. And so I've done absolutely\nnothing other than pay overhead for parallelization.",
    "start": "4053620",
    "end": "4059720"
  },
  {
    "text": "I've made extra work for\nmyself [? concreting ?] the threads. Maybe I've done some extra\nwork in terms of",
    "start": "4059720",
    "end": "4066810"
  },
  {
    "text": "synchronization. So I'm fully serial. ",
    "start": "4066810",
    "end": "4072980"
  },
  {
    "text": "So what you want to do is\nactually reorganize the way data is laid out in memory so\nthat you can effectively get",
    "start": "4072980",
    "end": "4079840"
  },
  {
    "text": "the benefit of parallelization. So if you have the data\norganized as is there, you can",
    "start": "4079840",
    "end": "4087420"
  },
  {
    "text": "shuffle things around. And then you end up with fully\nparallel or a layout that's",
    "start": "4087420",
    "end": "4093000"
  },
  {
    "text": "more amenable to full\nparallelism because now each thread is going to\na different bank. And that essentially gives you\na four-way parallelism.",
    "start": "4093000",
    "end": "4100650"
  },
  {
    "text": "And so you get the performance\nbenefits. ",
    "start": "4100650",
    "end": "4106480"
  },
  {
    "text": "So there are different kinds of\nsort of considerations you need to take into account for\nshared memory architectures in",
    "start": "4106480",
    "end": "4114400"
  },
  {
    "text": "terms of how the design affects\nthe memory latency. So in a uniform memory access\narchitecture, every processor",
    "start": "4114400",
    "end": "4122190"
  },
  {
    "text": "is either, you can think\nof it as being equidistant from memory. Or another way, it has the\nsame access latency for",
    "start": "4122190",
    "end": "4128295"
  },
  {
    "text": "getting data from memory. Most shared memory architectures\nare non-uniform,",
    "start": "4128295",
    "end": "4134480"
  },
  {
    "text": "also known as NUMA\narchitecture. So you have physically\npartitioned memories. And the processors can have the\nsame address space, but",
    "start": "4134480",
    "end": "4143020"
  },
  {
    "text": "the placement of data affects\nthe performance because going to one bank versus another\ncan be faster or slower.",
    "start": "4143020",
    "end": "4150859"
  },
  {
    "text": "So what kind of architecture\nis Cell? Yeah.",
    "start": "4150860",
    "end": "4159100"
  },
  {
    "text": "No guesses? AUDIENCE: It's not\na shared memory. PROFESSOR: Right. It's not a shared memory\narchitecture.",
    "start": "4159100",
    "end": "4164719"
  },
  {
    "text": " So a summary of parallel\nperformance factors.",
    "start": "4164720",
    "end": "4170500"
  },
  {
    "text": "So there's three things\nI tried to cover.  Coverage or the extent\nof parallelism in the",
    "start": "4170500",
    "end": "4176509"
  },
  {
    "text": "application. So you saw Amdahl's Law and it\nactually gave you a sort of a model that said when is\nparallelizing your application",
    "start": "4176510",
    "end": "4183989"
  },
  {
    "text": "going to be worthwhile? And it really boils down to\nhow much parallelism you actually have in your particular\nalgorithm. If your algorithm is sequential,\nthen there's",
    "start": "4183990",
    "end": "4190870"
  },
  {
    "text": "really nothing you can do for\nprogramming for performance",
    "start": "4190870",
    "end": "4196110"
  },
  {
    "text": "using parallel architectures. I talked about granularity of\nthe data partitioning and the",
    "start": "4196110",
    "end": "4202500"
  },
  {
    "text": "granularity of the work\ndistribution. You know, if you had really\nfine-grain things versus really coarse-grain things,\nhow does that translate to",
    "start": "4202500",
    "end": "4208200"
  },
  {
    "text": "different communication costs? And then last thing I\nshared was locality.",
    "start": "4208200",
    "end": "4213770"
  },
  {
    "text": "So if you have near neighbors\ntalking, that may be different than two things that are\nfurther apart in space",
    "start": "4213770",
    "end": "4219230"
  },
  {
    "text": "communicating. And there are some issues in\nterms of the memory latency and how you actually can\ntake advantage of that.",
    "start": "4219230",
    "end": "4228310"
  },
  {
    "text": "So this really is an overview\nof sort of the parallel",
    "start": "4228310",
    "end": "4233400"
  },
  {
    "text": "programming concepts and the\nperformance implications. So the next lecture will be,\nyou know, how do I actually",
    "start": "4233400",
    "end": "4239530"
  },
  {
    "text": "parallelize my program? And we'll talk about that.",
    "start": "4239530",
    "end": "4242480"
  }
]