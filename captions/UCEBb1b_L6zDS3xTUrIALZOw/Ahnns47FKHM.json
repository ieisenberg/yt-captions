[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6950"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6950",
    "end": "13500"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "13500",
    "end": "18680"
  },
  {
    "start": "18680",
    "end": "30460"
  },
  {
    "text": "PROFESSOR: And so today we have\nRavi , who is I guess a",
    "start": "30460",
    "end": "36980"
  },
  {
    "text": "performance engineer at VMware,\nwhere he basically looks at how to get really good\nperformance in many, many",
    "start": "36980",
    "end": "44940"
  },
  {
    "text": "different things in VMware. So Ravi is an MIT undergrad.",
    "start": "44940",
    "end": "51210"
  },
  {
    "text": "I guess a long time ago. RAVI SOUNDARARAJAN: Yeah, '92. PROFESSOR: '92. There you go. And then he got his PhD from\nStanford and end up in VMware",
    "start": "51210",
    "end": "59660"
  },
  {
    "text": "looking at a lot of interesting\nperformance stuff. So today, he's going to give us\na kind of a bird's eye view",
    "start": "59660",
    "end": "67780"
  },
  {
    "text": "of how these kind of\ntechnologies, what you guys are learning, is useful and\nhow it's used in industry.",
    "start": "67780",
    "end": "74510"
  },
  {
    "text": "OK, Ravi. Take over. RAVI SOUNDARARAJAN: First\nquestion, can everybody hear me? And is the mic on? OK. Good.",
    "start": "74510",
    "end": "79720"
  },
  {
    "text": "I have a bit of a sore throat. So I'm sorry if you\ncan't hear me. The other thing I want to say\nis I'm actually really glad you're here.",
    "start": "79720",
    "end": "84910"
  },
  {
    "text": "Because I remember very\ndistinctly in 1990 when I was an undergrad, I was\ntaking 6.111. And I had a bug in\nmy final project.",
    "start": "84910",
    "end": "91400"
  },
  {
    "text": "It just wasn't working. And I was going to skip\nthe lecture that day. But I was like, whatever,\nI'll go. So went to the lecture. Turns out Don Troxell pointed\nout something in that lecture.",
    "start": "91400",
    "end": "98790"
  },
  {
    "text": "I was like, that's my problem. And I went back, fixed it. Everything worked. And I was very happy. I don't think that's going\nto happen here. But it would be nice\nif it did.",
    "start": "98790",
    "end": "104520"
  },
  {
    "text": "So that's cool. So instead of just talking\nabout what a performance engineer does, I would--",
    "start": "104520",
    "end": "110100"
  },
  {
    "text": "I'll just-- Power is coming in. But-- Oh, there it is. Perfect.",
    "start": "110100",
    "end": "115210"
  },
  {
    "text": "So instead of just talking\nabout what performance engineering is about. I just wanted to go through 10\nbugs that I've actually been involved in.",
    "start": "115210",
    "end": "120740"
  },
  {
    "text": "And hopefully, they'll be\ninteresting to you. Because each one of them\nwas interesting to me. So let's start out with--",
    "start": "120740",
    "end": "127079"
  },
  {
    "text": "I just wanted to start\nthis out with a bang. So for dramatic effect\neverybody read through this slide. It's basically an email thread\nfrom one of my colleagues.",
    "start": "127080",
    "end": "135799"
  },
  {
    "text": "And raise your hand\nwhen you're done. So I'll know when you're done. ",
    "start": "135800",
    "end": "146590"
  },
  {
    "text": "OK. Anyway, so the point of this is\nthat his question ended up being, why is the percent lock\ntime in the database",
    "start": "146590",
    "end": "152640"
  },
  {
    "text": "increasing despite a lighter\nload for the thing that uses that database? Now, if you're like me, you\nread this email, and your",
    "start": "152640",
    "end": "159590"
  },
  {
    "text": "first question is what the\nhell is he talking about? So that's why titled this\nslide like that. So let's actually deconstruct\nwhat, a, that email meant.",
    "start": "159590",
    "end": "167390"
  },
  {
    "text": "And, b, what it meant to me. So in order to do that, I want\nto first talk about the system that he was talking about and\nhelp describe what the actual",
    "start": "167390",
    "end": "174969"
  },
  {
    "text": "problem he was seeing was. So here we have a standard\nmulti-tier set up. What we've got is that we've\ngot a server, which is our",
    "start": "174970",
    "end": "183060"
  },
  {
    "text": "middleware server. And it talks to a database. And then we have a bunch of\nhosts on the other end. Those are labeled by agent,\nHAL, whatever.",
    "start": "183060",
    "end": "188900"
  },
  {
    "text": "Anyway, those are hosts running\nvirtual machines. And here's the setup. You've got a client.",
    "start": "188900",
    "end": "194810"
  },
  {
    "text": "And that client issues\na command to this middleware server. And that middleware server tries\nto figure out what host",
    "start": "194810",
    "end": "200190"
  },
  {
    "text": "or what virtual machine\nneeds to be contacted. So the middleware goes\nahead and does that. And it performs an operation\non that host.",
    "start": "200190",
    "end": "207520"
  },
  {
    "text": "And after performing the\noperation, it then archives some of the information that\nit gained to the database.",
    "start": "207520",
    "end": "214170"
  },
  {
    "text": "And once it archived the\ninformation to the database, it went ahead and notified the\nclient that the operation",
    "start": "214170",
    "end": "219500"
  },
  {
    "text": "itself was done. Fairly simple. Send a request to a server. Server contacts some\nother host. Host comes back to the server\nand says it's done.",
    "start": "219500",
    "end": "226120"
  },
  {
    "text": "Server persists information to\nthe database and then notifies the client everything is done. So the actual problem that my\ncolleague was seeing was, he",
    "start": "226120",
    "end": "234770"
  },
  {
    "text": "was sending less and less load\nto this middleware server, which therefore means that less\nand less goes to the host",
    "start": "234770",
    "end": "240260"
  },
  {
    "text": "and less and less goes\nto the database. However, the percent of\ntime spent in the",
    "start": "240260",
    "end": "245780"
  },
  {
    "text": "database was higher. So despite a lighter load and\nless stuff going to the database, the percent\nof time spent in the",
    "start": "245780",
    "end": "252150"
  },
  {
    "text": "database was higher. Probably, all of you are smart\nenough to know what the actual problem is. But we'll go through\nit step by step.",
    "start": "252150",
    "end": "258489"
  },
  {
    "text": "So the deal is this. The first thing I did is I\nexamined the amount of time that each lock was held for\neach of these different",
    "start": "258490",
    "end": "263750"
  },
  {
    "text": "operations. And what I did was I varied\nthe load just as my colleague did. And what you see are six\ndifferent lines.",
    "start": "263750",
    "end": "270990"
  },
  {
    "text": "On the x-axis is each of the\ndifferent locks that we acquire in our system. And on the y-axis is the latency\nof those locks in",
    "start": "270990",
    "end": "277270"
  },
  {
    "text": "milliseconds. And basically, the\naxis for this-- the legend for this chart says\nLocked-4, which means a",
    "start": "277270",
    "end": "283610"
  },
  {
    "text": "certain type of load, all the\nway down to Locked-128. OK. And the deal is that when you\nhave lock-128, what that",
    "start": "283610",
    "end": "290660"
  },
  {
    "text": "indicates is that there's\n128 hosts per thread. Which, take my word for it, that\nmeans that you've got a lighter load.",
    "start": "290660",
    "end": "296070"
  },
  {
    "text": "So basically, the lowest\nline in the legend is the lightest load. And the highest line in the\nlegend is the highest load.",
    "start": "296070",
    "end": "302900"
  },
  {
    "text": "And as you can see, although\nit's a little bit obscured in this chart, the bottom line is\nthat this is about time you're spent holding a lock.",
    "start": "302900",
    "end": "309670"
  },
  {
    "text": "In this chart, lock-4 indicates\na heavy load. Lock-128 indicates\na lighter load. And what this chart indicates\nis that, in fact, you are",
    "start": "309670",
    "end": "316759"
  },
  {
    "text": "spending more time in the locks\nwith a heavier load, lock-4, than you are with a\nlighter load, lock-128.",
    "start": "316760",
    "end": "323479"
  },
  {
    "text": "So the latency prologue is\nhigher with a higher load, which is totally reasonable. And it turns out that if we\ntranslate my colleague's",
    "start": "323480",
    "end": "330280"
  },
  {
    "text": "question into reality, what\nhe's asking is why is the percent time spent in the\ndatabase higher, even though",
    "start": "330280",
    "end": "336610"
  },
  {
    "text": "you have a lighter load? Well, the answer ends up being\nthat when you have a lighter load, overall, you're spending\nless time in locks, which",
    "start": "336610",
    "end": "342700"
  },
  {
    "text": "means that any time you spend\nin the database, if that's a fixed cost, is going to\nbe a higher percent of the overall time.",
    "start": "342700",
    "end": "348590"
  },
  {
    "text": "So it turns out, there really\nwasn't an issue. There was nothing odd here. It's exactly what you'd expect,\nbecause the database",
    "start": "348590",
    "end": "353740"
  },
  {
    "text": "imposes essentially\na fixed amount of latency per operation. And in order to confirm that,\nI also looked at the time",
    "start": "353740",
    "end": "360020"
  },
  {
    "text": "spent contending for locks. So you really want a lock, but\nsomeone else gets it instead. And you have to wait.",
    "start": "360020",
    "end": "365170"
  },
  {
    "text": "And the trend is exactly\nthe same. When you have a lighter load,\nyou end up having a much lower",
    "start": "365170",
    "end": "370580"
  },
  {
    "text": "overall latency. But that just means that any\nlatency you spend in this database, its percent impact\non the entire operation is",
    "start": "370580",
    "end": "377229"
  },
  {
    "text": "quite a bit higher. So to take a look at this and\nto do a postmortem on this",
    "start": "377230",
    "end": "383130"
  },
  {
    "text": "case study, his first thing was\nsome random email, which kind of was nonsense.",
    "start": "383130",
    "end": "388879"
  },
  {
    "text": "But it was less nonsensical\nonce you actually tried to understand what this\nsetup was. So step one is figure out what\nis the experimental setup and",
    "start": "388880",
    "end": "395660"
  },
  {
    "text": "what are the parameters that\nare being studied. The next thing to understand\nis what's being measured? I remember what my\ncolleague said. He said the percent of time in\nthe database was pretty high.",
    "start": "395660",
    "end": "403289"
  },
  {
    "text": "Why is that? Well, OK. So we have to understand what\nthis multi-tier setup is, why you would be going to the\ndatabase, and what it means if",
    "start": "403290",
    "end": "409770"
  },
  {
    "text": "the time spent in that\ndatabase is high. And I guess the next thing you\nwant to do is you want to say, well, OK, he's complaining\nabout lock latency.",
    "start": "409770",
    "end": "416070"
  },
  {
    "text": "So I must have some sort of\nvisibility there somewhere. Let me go and look at the\nappropriate metric to figure out what's going on and why\ntime is being spent there.",
    "start": "416070",
    "end": "424080"
  },
  {
    "text": "And finally, it's appropriate\nto-- once you've got all this data, don't make any\nassumptions-- just draw the appropriate\nconclusion based on the data.",
    "start": "424080",
    "end": "429940"
  },
  {
    "text": " The problem is basically that\nwith a lighter load the impact",
    "start": "429940",
    "end": "435730"
  },
  {
    "text": "of that database becomes larger,\neven if it were the same under a heavier load. Because under a heavier load\nother aspects of the system",
    "start": "435730",
    "end": "441930"
  },
  {
    "text": "also get stressed that\ndon't get stressed in a lighter load. So, and this is basically what\nI'm saying here, yeah, the",
    "start": "441930",
    "end": "448540"
  },
  {
    "text": "percent lock time in the\ndatabase overall is higher. But the overall lock\ntime is smaller. So everything is behaving\nexactly as you would expect.",
    "start": "448540",
    "end": "457050"
  },
  {
    "text": "So this was one simple study\nand hopefully it didn't require any virtualization\nspecific knowledge.",
    "start": "457050",
    "end": "462760"
  },
  {
    "text": "What I'm going to do now is\ngo over a bunch of other different case studies that I've\nlooked at and crystallize what lessons I learned from\ndoing those studies.",
    "start": "462760",
    "end": "470560"
  },
  {
    "text": "So I guess the next case study\nI want to talk about is something that I call garbage\nin, garbage out.",
    "start": "470560",
    "end": "477080"
  },
  {
    "text": "I think that's something\nI learned on the first day of 001. So here's the deal. You've got a customer. And he wants to draw\nthis chart.",
    "start": "477080",
    "end": "484400"
  },
  {
    "text": "And this chart is basically, on\nthe x-axis is time and on the y-axis is some metric. It happens to be CPU\nusage megahertz.",
    "start": "484400",
    "end": "490979"
  },
  {
    "text": "But it doesn't really matter. OK. So he wants to get this chart. And it turns out-- if you\nremember the diagram I showed before, we have a server and a\nbunch of hosts at the end.",
    "start": "490980",
    "end": "498810"
  },
  {
    "text": "So the client simply has to send\na request to that server. And the server will\ngenerate the data, send back to the client. And it turns out that\nif any of--",
    "start": "498810",
    "end": "505070"
  },
  {
    "text": "all of you, I'm sure, are\nfamiliar with Pearl, Python, and all these different\nscripted languages. There's another one that's very\npopular in the Windows",
    "start": "505070",
    "end": "510550"
  },
  {
    "text": "community called PowerShell. And so it turns out that\nour server has a PowerShell interface. You can write a PowerShell\nscript to get any statistic",
    "start": "510550",
    "end": "516950"
  },
  {
    "text": "that you want. And then you can write that\nto some database or some spreadsheet and get\nthis charge. So here's the deal.",
    "start": "516950",
    "end": "522219"
  },
  {
    "text": "We have a user. And what he does, as I've\nindicated at the bottom, is that he wants to get the CPU\nusage for the last hour.",
    "start": "522220",
    "end": "527680"
  },
  {
    "text": "So he just writes a simple\nscript, using PowerShell. It doesn't really matter\nwhat the syntax is. But the point is,\nhe does what's called a get-stats call.",
    "start": "527680",
    "end": "533880"
  },
  {
    "text": "And he indicates, OK, I want\nthis virtual machine. I want this stat, whatever. And then he gets his\ninformation and",
    "start": "533880",
    "end": "539325"
  },
  {
    "text": "writes it to a file. No big deal. It's just a simple script\nto get some statistics. Now, here's what is actually\ninteresting.",
    "start": "539325",
    "end": "546440"
  },
  {
    "text": "I wrote that script\nusing PowerShell. Now, let's compare a PowerShell\nimplementation to a",
    "start": "546440",
    "end": "552060"
  },
  {
    "text": "highly tuned Java implementation\nof the very same script. Now, what I've done here is I've\nshown the performance of",
    "start": "552060",
    "end": "558630"
  },
  {
    "text": "that very same script when\ngetting data for one virtual machine, which is the first\nrow, six, about 40.",
    "start": "558630",
    "end": "564700"
  },
  {
    "text": "And then up to a little over\n300 virtual machines. Look at the scaling\nof this script.",
    "start": "564700",
    "end": "569870"
  },
  {
    "text": "In the middle column, I have\nthe performance for this PowerCLI script that I\njust showed you on",
    "start": "569870",
    "end": "575279"
  },
  {
    "text": "the previous slide. As you can see, it goes from a\nhealthy 9.2 seconds to over 43",
    "start": "575280",
    "end": "580530"
  },
  {
    "text": "minutes if you increase the\nsize of your inventory. In contrast, if you use a\nJava-based implementation, it",
    "start": "580530",
    "end": "586200"
  },
  {
    "text": "starts a little slower than\nPowerCLI at 14 seconds. But the scaling is\nfar superior. It ends up at 50 seconds.",
    "start": "586200",
    "end": "592110"
  },
  {
    "text": "So the question is why? What's different about these\nscripts at scale versus when I wrote the little toy script?",
    "start": "592110",
    "end": "598360"
  },
  {
    "text": "Now, to give you some color,\nwhy this matters to someone like me is that every once in a\nwhile, I'll get a call from a system admin, who wants\na very simple",
    "start": "598360",
    "end": "604000"
  },
  {
    "text": "script that does blah. And they say, I wrote this\nsimple script in my development environment,\nand it worked fine. But then I run it in my\nproduction environment, and it",
    "start": "604000",
    "end": "610410"
  },
  {
    "text": "completely breaks. What's going on? So often, problems like\nthis actually occur. Now I'm calling this garbage in,\ngarbage out, because you",
    "start": "610410",
    "end": "616589"
  },
  {
    "text": "can see that with PowerCLI and\nJava, we're getting very different results. The deal is that in PowerCLI,\nI wrote a very naive script.",
    "start": "616590",
    "end": "623190"
  },
  {
    "text": "And I'll explain how\nin a moment. And that very naive script\nhas not performed at all. Whereas in Java, I wrote a\nvery highly tuned script.",
    "start": "623190",
    "end": "630640"
  },
  {
    "text": "And it ended up taking\nabout an order of magnitude less time. So garbage in, garbage out. Good stuff in, good stuff out.",
    "start": "630640",
    "end": "636290"
  },
  {
    "text": "Let's try to understand why. OK. Here's the deal.",
    "start": "636290",
    "end": "641310"
  },
  {
    "text": "All of you are probably familiar\nwith the difference between scripting and high level\nlanguages, or low level languages or whatever you\nwant to call them. So the idea is that if you're\nrunning something using a",
    "start": "641310",
    "end": "648040"
  },
  {
    "text": "shell script or running\nPearl, or whatever. These things are usually\nmeant for ease of use. But they're not necessarily\nmeant for performance reasons.",
    "start": "648040",
    "end": "655140"
  },
  {
    "text": "And so they hide a lot of\ndetails from the users so that it's easier to come to\na quick prototype. You don't have to declare\nvariables.",
    "start": "655140",
    "end": "660330"
  },
  {
    "text": "Do all this kind of junk,\nright, if you're using scripting language. In contrast, if you're using,\nJava, C++, whatever, you have",
    "start": "660330",
    "end": "666150"
  },
  {
    "text": "Typesafe and all\nthis nonsense. You have to worry about what\nthe declarations of your variables are. Make sure they're correct\ntypes, et cetera.",
    "start": "666150",
    "end": "671180"
  },
  {
    "text": "So they tend to be more\ndifficult to get running. But once you get them running,\nyou can use a lot of advanced",
    "start": "671180",
    "end": "677000"
  },
  {
    "text": "tools that just aren't\navailable necessarily in toolkits. And we're going to talk about\nhow this difference manifested",
    "start": "677000",
    "end": "683870"
  },
  {
    "text": "itself in this particular\nproblem. I said that when I told the\ncustomer to write a simple PowerCLI script, they just\nhave to do one call.",
    "start": "683870",
    "end": "690950"
  },
  {
    "text": "Get-stat. And here's a bunch of arguments\nto tell it what statistic it wants to grab. It's a toolkit.",
    "start": "690950",
    "end": "696670"
  },
  {
    "text": "It's doing a lot of\nwork for you. What's it doing for you? Well, it turns out it's calling\nour internal APIs.",
    "start": "696670",
    "end": "702230"
  },
  {
    "text": "Our internal APIs have\nto work for a variety of different platforms. So they have to be incredibly\ngeneral. Under the covers of that single\nget-stat call, we have",
    "start": "702230",
    "end": "709949"
  },
  {
    "text": "about seven different steps\nthat are going on, ranging from asking, OK, I want a stat\nfor a virtual machine, what",
    "start": "709950",
    "end": "715500"
  },
  {
    "text": "kinds of stats does that\nvirtual machine export? How frequently does it update? All kinds of information that\nyou probably don't really care",
    "start": "715500",
    "end": "722035"
  },
  {
    "text": "about if all you want\nis a single stat. But you might care about if\nyou're writing a production quality script. So the point is get-stat\ndoes these seven calls.",
    "start": "722035",
    "end": "729080"
  },
  {
    "text": "And because you're using\na toolkit, it hides them all from you. It spares it.",
    "start": "729080",
    "end": "734260"
  },
  {
    "text": "It's very easy to use. But it turns out, this is\nobviously not the most optimal way to do this. And I illustrate that by showing\nyou conceptually",
    "start": "734260",
    "end": "741699"
  },
  {
    "text": "what's actually going on. On the right, I guess on your\nleft, I've got what the PowerCLI script was doing.",
    "start": "741700",
    "end": "747950"
  },
  {
    "text": "You call this get-stat call, or\nwhatever, and what it does that for every virtual machine\nthat you want this statistic for, it goes through and does\nall these random steps that",
    "start": "747950",
    "end": "754670"
  },
  {
    "text": "were done that I described\nin the previous slide. Figuring out what stats are\nsupported, how frequently they updated, and then actually going\nand getting the stat.",
    "start": "754670",
    "end": "761680"
  },
  {
    "text": "It turns out, as I show on the\nright, when I do this in Java, I can take advantage of the\nfact that a lot of those",
    "start": "761680",
    "end": "767730"
  },
  {
    "text": "things that I'm asking\nabout in the toolkit, they never change. You've got a good\nvirtual machine.",
    "start": "767730",
    "end": "773050"
  },
  {
    "text": "It supports things to\nbe refreshed every 20 seconds or whatever. That never changes. You want to find out what\nstatistics are supported.",
    "start": "773050",
    "end": "779260"
  },
  {
    "text": "Well, those never change once\nyou've configured it and you don't change anything. So why do that every\nsingle time?",
    "start": "779260",
    "end": "784910"
  },
  {
    "text": "So as we show in Java, you can\npull a lot of that stuff out of the main body of the loop.",
    "start": "784910",
    "end": "790370"
  },
  {
    "text": "And if you wanted information\nfor every VM in the system, you only have to make one call,\nas I show at the very bottom, for every VM.",
    "start": "790370",
    "end": "796080"
  },
  {
    "text": "You don't have to make seven\ndifferent RPC calls. In fact, you could further\noptimize, if you really understand the API, and collapse\na few more calls in",
    "start": "796080",
    "end": "803610"
  },
  {
    "text": "Java that you can't\ndo in the toolkit. Because again, the toolkit's\ntrying to talk at a high level, so any random dude can\nactually write a script.",
    "start": "803610",
    "end": "811040"
  },
  {
    "text": "OK. So the point is in this PowerCLI\nworld, a very simple toolkit world, we were making\nfive RPC calls for every VM",
    "start": "811040",
    "end": "817560"
  },
  {
    "text": "that we wanted stats for. Whereas in Java, we\nmade one call. And we can even optimize\nthat further. And I don't show that here.",
    "start": "817560",
    "end": "823690"
  },
  {
    "text": "But anyway, the bottom line is\nthat's why the performance of the Java script was\nso much faster.",
    "start": "823690",
    "end": "828810"
  },
  {
    "text": "So the idea is that with\nPowerCLI what I do is I wrote very simple, a script that\nanybody could understand and",
    "start": "828810",
    "end": "834330"
  },
  {
    "text": "use, but it's not optimized. I didn't utilize\nmulti-threading. I didn't realize that the output\nformat of the data was",
    "start": "834330",
    "end": "839660"
  },
  {
    "text": "really, really verbose. Again, that's the kind of\nthing that if you really understand the APIs you\ncan take advantage of. I also didn't realize--",
    "start": "839660",
    "end": "845949"
  },
  {
    "text": "as a customer, I wouldn't have\nrealized that I make this one call but it expands it to a\nbunch of different network requests that I didn't\nknow about.",
    "start": "845950",
    "end": "852680"
  },
  {
    "text": "In contrast, with Java, I was\nable to take advantage of the power of a language like Java,\nwhere I could thread pools.",
    "start": "852680",
    "end": "858640"
  },
  {
    "text": "I could optimize the return\nformat of the data. And I could reduce the\nnumber of RPC calls.",
    "start": "858640",
    "end": "863660"
  },
  {
    "text": "I guess the analogy I would\ngive is if you think about assembly code versus\ncompiler-generated code, nobody here wants to sit down\nand write x86 Assembly.",
    "start": "863660",
    "end": "869800"
  },
  {
    "text": "So, of course, you write C and\nyou use GCC and whatever. But maybe there are some things\nthat would actually be faster if you wrote\nhand assembly.",
    "start": "869800",
    "end": "877440"
  },
  {
    "text": "OK. So, first example we gave was\njust understanding what on earth the problem was. The second example was if I\nwrite bad code, I'm going to",
    "start": "877440",
    "end": "885010"
  },
  {
    "text": "usually get bad output. In the third case, I'm going to\ntalk a little bit about the design of an API. And I'm going to motivate\nthat by an example.",
    "start": "885010",
    "end": "891640"
  },
  {
    "text": "We have an administrator\non the right hand side. And he's talking to the\nmanagement server. This management server is\nresponsible for telling that",
    "start": "891640",
    "end": "897440"
  },
  {
    "text": "administrator what's going in\nevery host in the system, every virtual machine,\nwhatever.",
    "start": "897440",
    "end": "902899"
  },
  {
    "text": "So let's pretend that the user\nwants-- so this is a virtualized environment-- so\nthe idea is that the user",
    "start": "902900",
    "end": "907930"
  },
  {
    "text": "wants to actually access his\nor her virtual machine. They're running a Linux\nvirtual machine. They want to see its console.",
    "start": "907930",
    "end": "913390"
  },
  {
    "text": "So they can type\nat the console. Here's how that roughly works. They want to see the console of\nthe VM that I've circled.",
    "start": "913390",
    "end": "919420"
  },
  {
    "text": "So the way it works is that\nthis user talks to the management server. The management server locates\nthat virtual machine,",
    "start": "919420",
    "end": "926510"
  },
  {
    "text": "establishes a handshaking\nconnection. And that connection\nis directly given back to that user. So now the user can interact\ndirectly with that virtual",
    "start": "926510",
    "end": "933769"
  },
  {
    "text": "machine and send commands\nand do whatever. So we've all got the setup.",
    "start": "933770",
    "end": "939060"
  },
  {
    "text": "Very simple. Now here's the deal. The problem was that our poor\nlittle user, poor big user,",
    "start": "939060",
    "end": "944850"
  },
  {
    "text": "whatever, they could not start\nthis console when they were managing a large number\nof hosts.",
    "start": "944850",
    "end": "950940"
  },
  {
    "text": "To take you back to this diagram\nfor just a moment, on the right hand side\nof the screen, I show a number of hosts.",
    "start": "950940",
    "end": "956670"
  },
  {
    "text": "And I show a number of\nvirtual machines. So what I'm basically saying\nis that this administrator could not access the console of\nhis virtual machine as the",
    "start": "956670",
    "end": "964269"
  },
  {
    "text": "number of hosts increased. There should be no link\nbetween these two. But there was.",
    "start": "964270",
    "end": "969600"
  },
  {
    "text": "So I got involved\nin this problem. And here I went to\nthe client folks. And I explained the problem. And their response was that's\nobviously a server problem.",
    "start": "969600",
    "end": "977280"
  },
  {
    "text": "Next step, I go to\nthe server folks. And they said, yeah, it's\nobviously a client problem. So then I went back and thought\na little bit more",
    "start": "977280",
    "end": "983670"
  },
  {
    "text": "about the architecture. And what I knew is that whenever\nthe client needs to initiate this handshake\nconnection, it has to spawn a certain process.",
    "start": "983670",
    "end": "989080"
  },
  {
    "text": "So that process is called the\nVMRC, VMware remote console. So I talked to the client\nfolks again.",
    "start": "989080",
    "end": "994699"
  },
  {
    "text": "And said, well, I think\nthis is happening. Oh, yeah, it's a VMRC problem. OK. So that just passes the\nbuck another way.",
    "start": "994700",
    "end": "999880"
  },
  {
    "text": "So then I talk to\nthe VMRC folks. And I said, OK, I think I have\nthis problem with starting up. And they said, oh, it's probably\nauthentication.",
    "start": "999880",
    "end": "1005380"
  },
  {
    "text": "It's probably this. You have to enable verbose log\nin on your end host, blah, blah, blah. And at some point, I got\nso sick of this.",
    "start": "1005380",
    "end": "1010650"
  },
  {
    "text": "I thought, well, this\nis complete garbage. I'm actually going to\ntake a look at this a little more carefully. And so, once I got involved,\nI got a little bit more",
    "start": "1010650",
    "end": "1016910"
  },
  {
    "text": "information. And the information was actually\npretty useful. Here's the deal. When you're starting the remote\nconsole on one of these virtual machines and you want\nto connect, it turns out the",
    "start": "1016910",
    "end": "1024510"
  },
  {
    "text": "person that was having this\nproblem-- when you ask a few more questions, you get\na few more answers. Some of which are\nactually useful. So I said, well, if you\ndo this with 50 hosts,",
    "start": "1024510",
    "end": "1031289"
  },
  {
    "text": "do you see a problem? So 50 manage hosts,\nhe only wants to view one VM's console.",
    "start": "1031290",
    "end": "1037050"
  },
  {
    "text": "No problem. Then I asked him-- and then I\nactually sat at my desk, and we reproduced this. So then I said, let's try it\nwith 500 hosts, no problem.",
    "start": "1037050",
    "end": "1044579"
  },
  {
    "text": "Just a little bit slower,\nbut no problem. So again, the set up is 500\ndifferent hosts, he just wants to look at one VM.",
    "start": "1044579",
    "end": "1049900"
  },
  {
    "text": "And it's taking longer and\nlonger to view that one VM. Finally, he gets\nto 1,001 hosts.",
    "start": "1049900",
    "end": "1055110"
  },
  {
    "text": "And that's when the\nproblem occurs. That' when he cannot\nsee this console. So obviously then, you go back\nand you think, well, gee, is",
    "start": "1055110",
    "end": "1061605"
  },
  {
    "text": "there some magic number of 1,000\nhard coded anywhere, such that we exceeded it, and\nthat was the problem. And frankly, the answer is no.",
    "start": "1061605",
    "end": "1068460"
  },
  {
    "text": "So let's figure out actually\nwhat did happen. So I made a bunch of different\nobservations while I was",
    "start": "1068460",
    "end": "1073880"
  },
  {
    "text": "debugging this. When you have fewer than that\nmagic number of 1,001 hosts,",
    "start": "1073880",
    "end": "1079320"
  },
  {
    "text": "here's what was going on. Remember the sequence\nof events. I'm a client. I talked to a server.",
    "start": "1079320",
    "end": "1085340"
  },
  {
    "text": "Server talks to the\nvirtual machine. And then that virtual machine\nand I are directly connected. What I was noticing was that\nwhen there were less than",
    "start": "1085340",
    "end": "1093030"
  },
  {
    "text": "1,000 overall hosts in this\nsetup, when I would initiate the command for this console.",
    "start": "1093030",
    "end": "1098960"
  },
  {
    "text": "The CPU usage and memory usage\nof this middle management server would gradually increase,\nand increase, and",
    "start": "1098960",
    "end": "1106670"
  },
  {
    "text": "increase until the console\nwould actually show up. So you right click.",
    "start": "1106670",
    "end": "1111730"
  },
  {
    "text": "You want to pull up a console. A little window comes up. But it's totally blank. And then eventually, after the\nmanagement server consumes a lot of CPU and memory,\neventually that",
    "start": "1111730",
    "end": "1118460"
  },
  {
    "text": "console comes up. So now your question is what's\ngoing on that's causing the CPU usage of this management\nserver.",
    "start": "1118460",
    "end": "1123990"
  },
  {
    "text": "And why is that scaling\nwith the number of your virtual machines.",
    "start": "1123990",
    "end": "1129090"
  },
  {
    "text": "So the first thing I did-- so\none of the themes of this talk, I suppose, is that there's\na lot of great tools for performance debugging.",
    "start": "1129090",
    "end": "1134280"
  },
  {
    "text": "But sometimes there's no real\nsubstitute for common sense and for the things that\nare easiest to do. So the first thing I did\nis I looked at the log",
    "start": "1134280",
    "end": "1140320"
  },
  {
    "text": "file of this server. Here's the rationale. If every time I invoke this\nremote console, I'm doing more",
    "start": "1140320",
    "end": "1145550"
  },
  {
    "text": "work on the server. First, let me just see what\nthat server is doing. Now, it turns out that what I'd\nnoticed about the server",
    "start": "1145550",
    "end": "1153450"
  },
  {
    "text": "was that whenever the client\nwould contact the server and say I want the console of this\nVM, you would see a data",
    "start": "1153450",
    "end": "1159260"
  },
  {
    "text": "retrieval call from the\nclient to the server. And that data retrieval call\ngot more and more expensive",
    "start": "1159260",
    "end": "1165139"
  },
  {
    "text": "with more hosts and\nmore virtual machines in the system. So the question that I asked\nwas, why on earth are we doing",
    "start": "1165140",
    "end": "1170519"
  },
  {
    "text": "some stupid data retrieval call\nif all I wanted is to say, here's a VM. Tell me where it's located\nso I can talk to it.",
    "start": "1170520",
    "end": "1176149"
  },
  {
    "text": "So it turns out-- that was problem number one. Problem number two, that's what\nhappened when you had 50",
    "start": "1176150",
    "end": "1182780"
  },
  {
    "text": "hosts, 500, whatever, that you\nbasically spend more and more time in this data retrieval\nroutine on the server.",
    "start": "1182780",
    "end": "1188330"
  },
  {
    "text": "However, once you hit that magic\nnumber of 1,000, you would see this call start.",
    "start": "1188330",
    "end": "1193970"
  },
  {
    "text": "But you would never\nsee it end. And in fact, you wouldn't\nsee it end. You'd see some exceptions\nsomewhere.",
    "start": "1193970",
    "end": "1199970"
  },
  {
    "text": "So that to me was a clue that\nI better take a look and see what's going on at\nthat exception. So here's what I did.",
    "start": "1199970",
    "end": "1205350"
  },
  {
    "text": "Unfortunately, the exception\ngave me no information whatsoever. So I went ahead and I attached\na debugger to the server.",
    "start": "1205350",
    "end": "1211290"
  },
  {
    "text": "And when I did, I found\nout that it was an out of memory exception. So here's the deal. I'm a poor admin. I send a request to\nlook at a console.",
    "start": "1211290",
    "end": "1219260"
  },
  {
    "text": "For some reason, some data\nhandshaking code takes place between the server\nand the client. It takes longer and longer\nwith more hosts.",
    "start": "1219260",
    "end": "1225039"
  },
  {
    "text": "Eventually, when I have too many\nhosts, that thing ran out of memory and silently failed. And so my client's sitting\naround looking dumb with a",
    "start": "1225040",
    "end": "1232060"
  },
  {
    "text": "totally blank screen, because\nit has no idea that its request basically got denied. So the question is why\nis that happening?",
    "start": "1232060",
    "end": "1238650"
  },
  {
    "text": "And aha, this is where the\naha moment occurred. I happen to know that one of the\nreasons in our setup, that",
    "start": "1238650",
    "end": "1246540"
  },
  {
    "text": "one of the causes for this data\nretrieval, is that as a client, I want to know certain\nattributes of the virtual",
    "start": "1246540",
    "end": "1252490"
  },
  {
    "text": "machine that I wanted to view. For example, if I'm looking at\nthe console for a virtual machine, I might care that that\nvirtual machine has a",
    "start": "1252490",
    "end": "1259040"
  },
  {
    "text": "CD-ROM connected. And I might care if\nthat CD-ROM got disconnected or whatever. That's not something you'd\nnecessarily associate with a",
    "start": "1259040",
    "end": "1265220"
  },
  {
    "text": "virtual machine. But when I go to the server to\nconnect to it, the server wants to give me that\ninformation so that the shell,",
    "start": "1265220",
    "end": "1271560"
  },
  {
    "text": "which is showing me this console\ncan appropriately update if the virtual machine\ngets disconnected from its",
    "start": "1271560",
    "end": "1276810"
  },
  {
    "text": "CD-ROM, whatever. Anyway, the point is I looked at\nthe code that this magical VMRC routine was doing.",
    "start": "1276810",
    "end": "1282390"
  },
  {
    "text": "And here's what it was doing. It was saying, look,\nI want to start a remote console session. I need certain information like\nthe CD-ROM connectivity.",
    "start": "1282390",
    "end": "1289480"
  },
  {
    "text": "I'm going to go ahead and get\nthat information for every host and every virtual machine\nin the system. And then once that's done, I'll\ngo ahead and display the",
    "start": "1289480",
    "end": "1296200"
  },
  {
    "text": "console for this one virtual\nmachine that I care about. Well, this is clearly\njust a stupid idea. So I called the guy up and I\nsaid, you realize this is",
    "start": "1296200",
    "end": "1303519"
  },
  {
    "text": "stupid idea. And he said, well, why? It's not that expensive. Is it? And I was like, OK, we need\nto have a long discussion.",
    "start": "1303520",
    "end": "1308985"
  },
  {
    "text": "And bring a priest in here. But that's a terrible idea. So what we ended up doing is-- And so now--",
    "start": "1308985",
    "end": "1314080"
  },
  {
    "text": "OK, so that's the problem. The problem is we drew a\nhuge data retrieval. None of which we need. Now why this 1,001 host thing?",
    "start": "1314080",
    "end": "1321020"
  },
  {
    "text": "Here's the deal. It turns out that in our code,\nwe had-- for old style clients, of which this was an\nold style client-- we had a",
    "start": "1321020",
    "end": "1327870"
  },
  {
    "text": "restriction. We said if you need to serialize\nmore than 200 megabytes worth of data, we're\nnot going to handle that.",
    "start": "1327870",
    "end": "1333320"
  },
  {
    "text": "We're just going to fail. It turns out once we got 1,001\nhosts, we slightly exceeded that 200 megabyte buffer size,\nwe silently erred with an out",
    "start": "1333320",
    "end": "1341210"
  },
  {
    "text": "of memory exception. And we never returned\nto the client. And everybody was unhappy. So it had nothing to do\nwith 1,000 hosts. It has to do with the\namount of data",
    "start": "1341210",
    "end": "1347170"
  },
  {
    "text": "you're actually sending. So luckily, there were several\ndifferent fixes. So I told the VMRC folks, the\nguy that wrote this thing, I",
    "start": "1347170",
    "end": "1352510"
  },
  {
    "text": "said, please, under no\ncircumstances, should you get information for every single\nhost if you only want it for this one guy.",
    "start": "1352510",
    "end": "1358220"
  },
  {
    "text": "That's a terrible idea. And then I went back to the\nserver folks, and I said, you have to admit-- first of all, you have\nto fail correctly.",
    "start": "1358220",
    "end": "1364080"
  },
  {
    "text": "And second, you have to admit\nsensible error messages, because this should not take the\nkind of expertise that it took to figure out.",
    "start": "1364080",
    "end": "1370330"
  },
  {
    "text": "And thankfully, this made\nactually a pretty huge difference. So that was kind of cool. So the lessons that I took away\nfrom this is if you're",
    "start": "1370330",
    "end": "1377200"
  },
  {
    "text": "going to create an API, create\nan API that's very difficult to abuse and very easy to use.",
    "start": "1377200",
    "end": "1384210"
  },
  {
    "text": "Not the opposite. These poor VMRC folks, it's\nlike a three line piece of code to get information\nfor every VM. It's like eight or nine lines\nto get information",
    "start": "1384210",
    "end": "1390920"
  },
  {
    "text": "for just that VM. But those five lines make a\nboatload of difference. The other thing is\nthat this was an",
    "start": "1390920",
    "end": "1396169"
  },
  {
    "text": "entirely internal customer. In other words, it's just\nanother group within VMware. Imagine you're some external guy\nthat's using the same API.",
    "start": "1396170",
    "end": "1402530"
  },
  {
    "text": "You're screwed. So we have to be better at\neducating the people that are using our API. And don't just throw it\nover the fence and",
    "start": "1402530",
    "end": "1408350"
  },
  {
    "text": "say here's an API. Deal with it. There needs to be some\ninteraction between them to make sure that this is the\nright API for its job.",
    "start": "1408350",
    "end": "1416000"
  },
  {
    "text": "This is the next case study\nI want to talk about. This one actually was found by\na colleague of mine, who I'm",
    "start": "1416000",
    "end": "1422559"
  },
  {
    "text": "going to abbreviate as RM. Very interesting example. For those of you that took\nthe class last year, I'm recycling it. But it's a good example,\nso no worries.",
    "start": "1422560",
    "end": "1429100"
  },
  {
    "text": "So here's the deal. You've got a benchmark. And I've got two builds of\na piece of software.",
    "start": "1429100",
    "end": "1437020"
  },
  {
    "text": "When I run the benchmark\nagainst that piece of software, in case a, I was\ngetting a throughput of 100",
    "start": "1437020",
    "end": "1442470"
  },
  {
    "text": "operations per minute. In case b, I was getting\nhalf the throughput.",
    "start": "1442470",
    "end": "1447799"
  },
  {
    "text": "And what was the difference\nbetween those builds? Well, the first build, the\nfaster build, was a 32-bit",
    "start": "1447800",
    "end": "1454620"
  },
  {
    "text": "executable running on\n64-bit hardware. While in the second case, it\nwas a 64-bit executable",
    "start": "1454620",
    "end": "1461590"
  },
  {
    "text": "running on 64-bit hardware. Now if you're like me-- which hopefully you're not--\nbut if you're like me, you",
    "start": "1461590",
    "end": "1467220"
  },
  {
    "text": "think yourself, well, there\nshouldn't be much difference between 32-bit and 64-bit. In fact, 64-bit should be a\nlot faster for a lot of",
    "start": "1467220",
    "end": "1472360"
  },
  {
    "text": "reasons you're going to\nlearn about in 004. So the question is\nwhat's going on? I can distinctly remember when\nthis check-in was done,",
    "start": "1472360",
    "end": "1478060"
  },
  {
    "text": "somebody said, oh, we're making\nthe switch over, but it shouldn't be a big deal. And foolishly, like a moron, I\nactually believed that person.",
    "start": "1478060",
    "end": "1483159"
  },
  {
    "text": "Very bad idea. So this came back to haunt\nus for three months. So what's the deal? Remember, a 32-bit executable\nrunning on 64-bit code was",
    "start": "1483160",
    "end": "1490740"
  },
  {
    "text": "faster than a 64-bit executable\nrunning on a 64-bit application. So the first thing we do--",
    "start": "1490740",
    "end": "1495770"
  },
  {
    "text": "first of all, you've got\nto find the right tool for the right job. In this case, we use a profiling\ntool called Xperf. Xperf is a very powerful\nprofiler, kind of like Vtune,",
    "start": "1495770",
    "end": "1503549"
  },
  {
    "text": "or Valgrind, or Quantifier, or\na lot of the other tools you've used. The nice thing is it's\nbuilt into the OS.",
    "start": "1503550",
    "end": "1508710"
  },
  {
    "text": "It runs on Windows 2008. And it's a sampling based\nprofiler that has a lot of pretty cool attributes. If you have Windows 2008, I\nthink you ought to just go and",
    "start": "1508710",
    "end": "1515530"
  },
  {
    "text": "download it. It's free. And play with it. This gives a lot of information\nlike stack traces, caller/callee information,\net cetera.",
    "start": "1515530",
    "end": "1521510"
  },
  {
    "text": "It's potentially the perfect\ntool for this kind of job. What kind of output\ndoes it show?",
    "start": "1521510",
    "end": "1526990"
  },
  {
    "text": "In this case, remember\nthe setup. The setup is I run build A. And\nI'm getting twice as much throughput as running build B.\nSo let's take a look at the",
    "start": "1526990",
    "end": "1533310"
  },
  {
    "text": "CPU usage, just as a first cut\nto see what's going on. In the 64-bit case, which\nis what I showed here.",
    "start": "1533310",
    "end": "1538340"
  },
  {
    "text": "I showed the output of this\nXperf for 64-bit. What you can see, x-axis\nis just time. And y-axis is CPU usage.",
    "start": "1538340",
    "end": "1543870"
  },
  {
    "text": "It's totally saturated. In the 64-bit case, we're\ncompletely saturated.",
    "start": "1543870",
    "end": "1548940"
  },
  {
    "text": "Take my word for it, in the\n32-bit case, you're not. And this accounts for\nthe difference. So now let's dig deeper and find\nout why we're seeing this",
    "start": "1548940",
    "end": "1555510"
  },
  {
    "text": "difference. So first, we look at the\nsampling profiler. Now there's a lot of perils of\nusing sampling profilers.",
    "start": "1555510",
    "end": "1562970"
  },
  {
    "text": "In this particular case,\nhere's the deal. I've got a process. And that process spawns\na bunch of threads.",
    "start": "1562970",
    "end": "1568820"
  },
  {
    "text": "In this particular view, what\nyou're seeing is where is the time being spent in\nthis process.",
    "start": "1568820",
    "end": "1573840"
  },
  {
    "text": "And as you can see, most of the\ntime is spent at the root, which kind of makes sense. Because everything in some sense\nis a child of the root.",
    "start": "1573840",
    "end": "1580610"
  },
  {
    "text": "But the first thing our poor\nprocess does, it spawns a bunch of child threads. So clearly, all of the time is\nessentially going to be spent",
    "start": "1580610",
    "end": "1587600"
  },
  {
    "text": "in the routine that calls all\nof these threads, because that's the first entry point. But unfortunately, that's\nremarkably unhelpful.",
    "start": "1587600",
    "end": "1594370"
  },
  {
    "text": "So the deal is we have to dig\ndeeper and find out, where is this time actually\nbeing spent.",
    "start": "1594370",
    "end": "1600480"
  },
  {
    "text": "So like I said, just to kind of\nreiterate the point that I made on the previous slide. The point is that your\nentry point is",
    "start": "1600480",
    "end": "1605560"
  },
  {
    "text": "spawning all the threads. And if that's kind of the\nstarting point for accounting, you're not going to get any help\nif you're just looking at that starting point.",
    "start": "1605560",
    "end": "1610860"
  },
  {
    "text": "You have to dig a little\nbit deeper. And so what I'm basically trying\nto say, is that even",
    "start": "1610860",
    "end": "1617009"
  },
  {
    "text": "though this thing records\nstack traces, this stack happens to be the most\npopular stack.",
    "start": "1617010",
    "end": "1622020"
  },
  {
    "text": "But it's not actually\nthe problem. Because it's the most popular,\nbecause that's where everybody starts. So all of the time that\nis charged gets charged to that stack.",
    "start": "1622020",
    "end": "1627860"
  },
  {
    "text": "We have to look a little\nbit deeper. So in order to look a little bit\ndeeper, let's think about the problem in a slightly\ndifferent way.",
    "start": "1627860",
    "end": "1633870"
  },
  {
    "text": "Suppose I've got my root. That's where everything\nstarts. That's where all the threads\nare spawned, whatever. And suppose I have three paths\nto a tiny little function at",
    "start": "1633870",
    "end": "1641270"
  },
  {
    "text": "the very end. Maybe I'm spending all\nof my time in the path that I've indicated.",
    "start": "1641270",
    "end": "1647530"
  },
  {
    "text": "But the problem is that-- suppose you're spending all of\nyour time in that tiny little function, and suppose it's\nequally spent among these",
    "start": "1647530",
    "end": "1655360"
  },
  {
    "text": "other three paths. Well, it's not any path that's\nkind of screwed up. It's that that tiny function\nis kind of messed up.",
    "start": "1655360",
    "end": "1660700"
  },
  {
    "text": "So you got to figure out why are\nyou spending time in that tiny function? And is there something that\nyou can do to fix that?",
    "start": "1660700",
    "end": "1666090"
  },
  {
    "text": "So let's look from the\nopposite perspective. Let's look at the caller view. In this case, what I did was I\nlooked at a bunch of different",
    "start": "1666090",
    "end": "1673350"
  },
  {
    "text": "routines, a bunch of\nthese different so-called tiny functions. And I looked at what was calling\nthose tiny functions.",
    "start": "1673350",
    "end": "1678640"
  },
  {
    "text": "It turns out that here's a call,\nZwQueryVirtualMemory some tiny function. It happens to be-- it turns out\nthat we're spending, as it",
    "start": "1678640",
    "end": "1684490"
  },
  {
    "text": "indicates, 77% of our time\nin this tiny function. And just to emphasize that\nthis is not being",
    "start": "1684490",
    "end": "1691559"
  },
  {
    "text": "called from the root. So you don't see it from\nyour first glance. You can see that the number of\ntimes this tiny function was",
    "start": "1691560",
    "end": "1696590"
  },
  {
    "text": "called from the root is 0.34%. So if you were just looking from\nthe root and trying to figure out what's being\ncalled from the root.",
    "start": "1696590",
    "end": "1702520"
  },
  {
    "text": "It would be totally useless. Instead, you have\nto dig deeper. And see all the different\nfunctions that are being called. And find out where they're\nbeing called from.",
    "start": "1702520",
    "end": "1708440"
  },
  {
    "text": "So we know that this function,\nZwQueryVirtualMemory, is taking like 70% of the CPU,\nwhich is an enormously large",
    "start": "1708440",
    "end": "1713810"
  },
  {
    "text": "amount of CPU. We also know what's calling\nit are these two routines right here.",
    "start": "1713810",
    "end": "1719269"
  },
  {
    "text": "This thing, this magical\nthing called RTDynamicCast and RTtypeid. So it turns out we have two\npaths that are going to this",
    "start": "1719270",
    "end": "1726580"
  },
  {
    "text": "tiny function. And those two paths comprise\nmost of the calls to this tiny function. And it turns out we're spending\na boatload of time in",
    "start": "1726580",
    "end": "1732800"
  },
  {
    "text": "this tiny function. So let's dig deeper and find\nout what on earth these two routines are. What is RTtypeid?",
    "start": "1732800",
    "end": "1738549"
  },
  {
    "text": "What is RTDynamicCast,\nwhatever? So let's first look\nat RTDynamicCast. What I show here--",
    "start": "1738550",
    "end": "1743890"
  },
  {
    "text": "in contrast to the\nprevious view. The previous view said, I\nhave a tiny function. What are the people\nthat call me? Let's look at the reverse.",
    "start": "1743890",
    "end": "1749690"
  },
  {
    "text": "Let's go and start at one of\nthese two routines, RTtypeid. And figure out what it's calling\nthat is spending so",
    "start": "1749690",
    "end": "1755480"
  },
  {
    "text": "much time, and ultimately\nleads to us calling that ZwQueryVirtualMemory\nor whatever. And as you can see, this\nRTtypeid, it calls two",
    "start": "1755480",
    "end": "1762850"
  },
  {
    "text": "functions primarily. One is incrementing a\nreference counter. And the other is decrementing\na reference counter. Now you're thinking to yourself,\nincrementing a",
    "start": "1762850",
    "end": "1769030"
  },
  {
    "text": "reference counter, decrementing\na reference counter should be very simple,\nvery low CPU, and basically should cost you nothing. And all of those three\nassumptions are dead wrong,",
    "start": "1769030",
    "end": "1775500"
  },
  {
    "text": "unfortunately. So let's try to understand\na little bit as to why that's happening. Turns out RTtypeid is used in\norder to figure out the C++",
    "start": "1775500",
    "end": "1782520"
  },
  {
    "text": "type of an object. So we're trying to figure\nthis out on the fly. And as I said, it's kind of\nmystifying that this routine,",
    "start": "1782520",
    "end": "1787950"
  },
  {
    "text": "which calls our tiny function,\nthat you're spending 39% overall in this routine\nin that tiny function.",
    "start": "1787950",
    "end": "1793930"
  },
  {
    "text": "So let's figure out what's\ngoing on by looking at a simple example, which\nwould be IncRef. So it turns out if you're\nlooking at this code, the",
    "start": "1793930",
    "end": "1800170"
  },
  {
    "text": "dreadful line is the fact that\nwe're doing const type info reference tinfo equals\ntypeid of *this.",
    "start": "1800170",
    "end": "1806880"
  },
  {
    "text": "This is basically, you have to\ncall RTtypeid to get this kind of information. And in order to do that, you\nneed some runtime type",
    "start": "1806880",
    "end": "1813520"
  },
  {
    "text": "information. This runtime type information\nhas some pointers associated with it.",
    "start": "1813520",
    "end": "1818720"
  },
  {
    "text": "So here's the deal. Whether I'm running a 32-bit-- Remember, we even forgot the\noriginal problem, which is, gee, I'm 32-bit, 64-bit\nexecutable--",
    "start": "1818720",
    "end": "1825870"
  },
  {
    "text": "sorry 32-bit executable\non 64-bit hardware. We were slower than a 64-bit-- we were faster than a 64-bit\nexecutable on 64-bit hardware.",
    "start": "1825870",
    "end": "1831899"
  },
  {
    "text": "Let's try to figure out why. Well, remember, every time I\nincrement or decrement a reference counter, I get some\ntype information by consulting",
    "start": "1831900",
    "end": "1839310"
  },
  {
    "text": "this runtime type information. It's got a bunch of pointers. When I run a 32-bit executable\non 64-bit hardware, those",
    "start": "1839310",
    "end": "1846460"
  },
  {
    "text": "pointers are just raw\n32-bit pointers. You just look them up,\nand you're done. In 64-bit, pretty much the\nonly difference is that",
    "start": "1846460",
    "end": "1853020"
  },
  {
    "text": "instead of those pointers being\nactual pointers, they're 32-bit offsets. So you take the offset, you add\nit to the base address of",
    "start": "1853020",
    "end": "1861170"
  },
  {
    "text": "the DLL of the executable or\nwhatever, where that runtime typeid call is being made.",
    "start": "1861170",
    "end": "1866399"
  },
  {
    "text": "And then you're done. Once you've added that offset,\nyou get a 64-bit pointer. You look up that 64-bit pointer,\neverybody's happy.",
    "start": "1866400",
    "end": "1873520"
  },
  {
    "text": "So here's the deal. Remember, we said\n32-bit's faster. 32-bit is just a pointer\nlook up. And then you're done. 64-bit's slower.",
    "start": "1873520",
    "end": "1879670"
  },
  {
    "text": "It's a pointer look up-- and\nit's an addition plus a pointer look up. So I pause, because you're\nthinking to yourself, an",
    "start": "1879670",
    "end": "1886230"
  },
  {
    "text": "addition plus a pointer look up\nversus a pointer look up. Does that mean that\naddition is slow? That sounds really stupid.",
    "start": "1886230",
    "end": "1891260"
  },
  {
    "text": "And by the way, it\nis really stupid. It's not true. So the deal is this. It's not that addition\nto create a 64-bit pointer is slow.",
    "start": "1891260",
    "end": "1897809"
  },
  {
    "text": "The deal is that-- remember\nthis 2-step process. You find out the base\naddress of a module. And you add this offset\nto that base address.",
    "start": "1897810",
    "end": "1904220"
  },
  {
    "text": "Determining that base address\nis really, really slow. It turns out you have to call\na bunch of different random",
    "start": "1904220",
    "end": "1910510"
  },
  {
    "text": "routines that have to walk the\nlist of loaded modules. And you're doing this\nevery single time you call this routine.",
    "start": "1910510",
    "end": "1916130"
  },
  {
    "text": "And if you increment a reference\nwhenever you do a memory allocation, you can\nimagine that this is going to consume a lot of CPU.",
    "start": "1916130",
    "end": "1922630"
  },
  {
    "text": "So it turns out, this is\nwhat's actually slow. And remember, that we started\nthis off a little by saying, oh, we've got this\nweird function,",
    "start": "1922630",
    "end": "1927950"
  },
  {
    "text": "ZwQueryVirtualMemory, which\nseems to be being called a lot from a bunch of different\nplaces. The reason it's called a lot\nis because all of these",
    "start": "1927950",
    "end": "1934269"
  },
  {
    "text": "routines to get typeid\ninformation are calling this routine. So there's actually\ntwo solutions. And you can look on this blog\nto actually figure out that",
    "start": "1934270",
    "end": "1941355"
  },
  {
    "text": "this is a known problem that\nMicrosoft eventually fixed. And it turns out, that the two\nsolutions end up being that",
    "start": "1941355",
    "end": "1948350"
  },
  {
    "text": "you can either statically\ncompute that base address so you're not constantly\nrelooking it up every single time.",
    "start": "1948350",
    "end": "1953529"
  },
  {
    "text": "But there's actually a much\nsimpler and stupider solution, which is, use the latest runtime\nlibrary where they have actually fixed\nthis problem.",
    "start": "1953530",
    "end": "1959940"
  },
  {
    "text": "So just to summarize\nwhat was going on. We were CPU saturated.",
    "start": "1959940",
    "end": "1965670"
  },
  {
    "text": "We looked from the top down. We didn't notice anything. We looked from the bottom up. And we noticed that some bottom\nwas being called a lot.",
    "start": "1965670",
    "end": "1970929"
  },
  {
    "text": "We figured out where that bottom\nwas being called by looking at caller/callee\nviews. Once we figured out where it\nwas being called, we then",
    "start": "1970930",
    "end": "1976570"
  },
  {
    "text": "tried to figure out why that\nthing was being called. And then we fixed the problem. So to me, I guess the take-home\nlesson from this is",
    "start": "1976570",
    "end": "1983490"
  },
  {
    "text": "that when that developer said\nto me, three months before this whole debacle started,\nhe said, oh, yeah, we're switching from say\n32 to 64-bit.",
    "start": "1983490",
    "end": "1989519"
  },
  {
    "text": "That's not a big deal. You can't really take that stuff\nat face value, because a lot of little things\ncan really add up. And we could never have really\nguessed that this would happen",
    "start": "1989520",
    "end": "1996400"
  },
  {
    "text": "beforehand. So that's four. Let me go over another one.",
    "start": "1996400",
    "end": "2001799"
  },
  {
    "text": "Memory usage. So excessive-- I'm sure actually all of you\nknow that memory usage is a",
    "start": "2001800",
    "end": "2007340"
  },
  {
    "text": "big problem. Your application is going to\nslow down if you're inducing a lot of paging. And at least in 32-bit Windows,\nif you exceed two",
    "start": "2007340",
    "end": "2013700"
  },
  {
    "text": "gigabytes for the process that\nyou're running, the process is just going to crash. So you clearly do care\nabout memory usage.",
    "start": "2013700",
    "end": "2020029"
  },
  {
    "text": "And I want to differentiate\nbetween memory leaks and memory accumulation. You're wondering why would\na process take up two",
    "start": "2020030",
    "end": "2026090"
  },
  {
    "text": "gigabytes of memory. Well, one is that maybe\nyou have a leak. You allocate something. You don't remove it. That might just be\nan accumulation.",
    "start": "2026090",
    "end": "2031630"
  },
  {
    "text": "You allocate something somewhere\nin your code, you actually are freeing it. But the trouble is that routine\nnever gets called, because in your code logic you\nnever take the time to think",
    "start": "2031630",
    "end": "2039750"
  },
  {
    "text": "and say, OK, can I actually get\nrid of this at this point. And that's what I'm going to\ncall a memory accumulation.",
    "start": "2039750",
    "end": "2046340"
  },
  {
    "text": "There's a lot of different tools\nout there to analyze memory usage. And the sad part about this is\nthat almost none of them work",
    "start": "2046340",
    "end": "2051719"
  },
  {
    "text": "for the particular example that\nwe have, because they just didn't scale very well. It's fine if you've got your\ntoy, stupid little application",
    "start": "2051719",
    "end": "2057388"
  },
  {
    "text": "that does addition in a loop. Not so fine if you've got an\nenterprising middleware application. So we ended up, despite all\nthe tools that we have",
    "start": "2057389",
    "end": "2062699"
  },
  {
    "text": "available, we ended up having\nto write our own. I didn't do this,\na colleague did. But anyway, I guess\nthat distinction",
    "start": "2062699",
    "end": "2067899"
  },
  {
    "text": "doesn't really matter. But anyway, the point is that\nyou do a lot of what these other tools do. But you do it special\nto your environment. In this case, we just hooked a\nlot of the calls to malloc.",
    "start": "2067900",
    "end": "2075000"
  },
  {
    "text": "And we figure out what\npointers are live. And then we do stuff\nfrom there. This is how normal tools work.",
    "start": "2075000",
    "end": "2080408"
  },
  {
    "text": "And of course, this can be\nunusably slow if you do a ton of allocations. For example, if you're doing\nmillions of allocations per",
    "start": "2080409",
    "end": "2085739"
  },
  {
    "text": "second, you can imagine you're\ncalling this kind of routine, which itself-- if it only has a 10x overhead\nover a call without that,",
    "start": "2085739",
    "end": "2092460"
  },
  {
    "text": "you're slowing down your program\nby 10x for every single memory allocation. That means performance depends\non how many allocations you do, which is really bad.",
    "start": "2092460",
    "end": "2098800"
  },
  {
    "text": "And just to illustrate what\nI mean by a leak. My definition is basically\nthat you've got this routine, foo. And it mallocs some data.",
    "start": "2098800",
    "end": "2104599"
  },
  {
    "text": "But it returns without\nfreeing that data. And nowhere else in the\ncode log did you actually have a free. That's an actual memory leak.",
    "start": "2104600",
    "end": "2110830"
  },
  {
    "text": "An accumulation would be if you\nhave some free somewhere. But it's never actually\ncalled. So a lot of the code that we\nwrite happened-- at least in",
    "start": "2110830",
    "end": "2118330"
  },
  {
    "text": "the past, it's changing-- but\na lot of it happened to be written in C++. And all of you, I guess assume,\nyou know about new and",
    "start": "2118330",
    "end": "2124260"
  },
  {
    "text": "delete and all those kinds of\nthings that we do in C++. And it turns out, that those\nthings are notoriously bad for assigning memory leaks.",
    "start": "2124260",
    "end": "2129359"
  },
  {
    "text": "So instead, what people\ntypically do is they use reference counted objects. So every time you use something\nyou increment some",
    "start": "2129360",
    "end": "2134589"
  },
  {
    "text": "reference count. And every time you\ndelete it, you decrement a reference count. And when the decremented\nreference count is zero it",
    "start": "2134590",
    "end": "2140860"
  },
  {
    "text": "automatically gets deleted. Now of course, this only\nsolves an actual leak. It doesn't solve an\naccumulation.",
    "start": "2140860",
    "end": "2146470"
  },
  {
    "text": "So here was the problem\nthat I noticed. That was all kind of setup, just\nsaying what's a memory leak, what's an accumulation,\nwhy is memory a problem.",
    "start": "2146470",
    "end": "2152530"
  },
  {
    "text": "Let's get to actually why\nit was a problem. I had this issue. I have our little server\napplication.",
    "start": "2152530",
    "end": "2157950"
  },
  {
    "text": "And it's running out of memory\nafter several hours. Now I used Purify on a much,\nmuch smaller setup.",
    "start": "2157950",
    "end": "2165950"
  },
  {
    "text": "It was about 1/100 the size of\nthe actual setup, because larger than that, Purify would\ncrash, because it would run",
    "start": "2165950",
    "end": "2171860"
  },
  {
    "text": "out of memory. Because by the way, to track\nall the memory references, it's got to allocate memory,\nwhich ends up being a nightmare of unprecedented\nproportions.",
    "start": "2171860",
    "end": "2178260"
  },
  {
    "text": "So the deal was, was that I\ncouldn't really detect this leak very easily.",
    "start": "2178260",
    "end": "2184670"
  },
  {
    "text": "So what I did was I examined\nthe memory that was in use. Essentially, what I'm trying to\nsay is that, there wasn't so much a leak as there\nwas an accumulation.",
    "start": "2184670",
    "end": "2191750"
  },
  {
    "text": "Accumulation again being\na logical problem. Well, yeah, all of my code\nis written correctly. But it's just that free\ncall is never called.",
    "start": "2191750",
    "end": "2197470"
  },
  {
    "text": "So I examined the memory in use\nand I localized it to one operation that whenever we did\nit, memory would increase.",
    "start": "2197470",
    "end": "2203279"
  },
  {
    "text": "So since doing it only once,\nthere's a lot of noise. What I did was I said, well, OK,\nwhy don't I just do this operation hundreds of times\nat the same time.",
    "start": "2203280",
    "end": "2209530"
  },
  {
    "text": "Because then, if I see some\nallocation that's 100x from before I started this\nI can know that this was actually a problem.",
    "start": "2209530",
    "end": "2215680"
  },
  {
    "text": "And that's exactly what I did. I had this issue where\nsomething was being allocated 64 bytes. Now 64 bytes is so small that\nit's pretty much in the noise.",
    "start": "2215680",
    "end": "2222580"
  },
  {
    "text": "But if you do it a hundred\ntimes, and you're seeing 6,400 bytes. Then you do it a thousand\ntimes and you see 64 times 1,000.",
    "start": "2222580",
    "end": "2227740"
  },
  {
    "text": "Then it becomes something\neasily that you can then track. Once I noticed that this\nparticular piece of data was",
    "start": "2227740",
    "end": "2233339"
  },
  {
    "text": "being allocated like this, I\nwent through and I realized that this actually was-- it was an accumulation\nand not a leak.",
    "start": "2233340",
    "end": "2238530"
  },
  {
    "text": "So someone would allocate\nsomething whenever an incoming message came in. And there was a free somewhere,\nbut it was never being called.",
    "start": "2238530",
    "end": "2245060"
  },
  {
    "text": "So the lesson that I learned\nfrom this is that in a lot of situations with performance\ndebugging, it's very helpful",
    "start": "2245060",
    "end": "2250810"
  },
  {
    "text": "to try to find a problem-- in order to try to find a\nproblem, it's very helpful to magnify it and make\nit a lot worse.",
    "start": "2250810",
    "end": "2256390"
  },
  {
    "text": "If you think something is\nproblematic in the network, slow down your network\nif that's an option. If you think something has a\nproblem with memory usage,",
    "start": "2256390",
    "end": "2262859"
  },
  {
    "text": "either do something like what\nI've described here. Or maybe reduce the\namount of memory. When I was in graduate school,\nwe were working on a cache",
    "start": "2262860",
    "end": "2269359"
  },
  {
    "text": "coherence protocol. And I'm going to spare you\na lot of the details. But here's the thing. We basically stored a linked\nlist for every data in memory.",
    "start": "2269360",
    "end": "2275359"
  },
  {
    "text": "And we had to make sure that\nthat linked list was rock solid no matter what the size\nof the inventory was. And if you exceeded the\nlinked list, you'd",
    "start": "2275360",
    "end": "2281820"
  },
  {
    "text": "store stuff in disk. So what we did to magnify any\npossible bugs was we created it so that you could\nmake a linked list",
    "start": "2281820",
    "end": "2287190"
  },
  {
    "text": "of at most one item. That way no matter what you\ndid it would force you to overflow and go into disk. And that was a great way to\ntest a lot of overflow",
    "start": "2287190",
    "end": "2294000"
  },
  {
    "text": "conditions that in practice\nwould never have occurred. Because we do the engineering\nthing, figure out the capacity, double it, and hope\nthat nothing happens.",
    "start": "2294000",
    "end": "2299990"
  },
  {
    "text": "So it's good to exaggerate\nedge conditions. So that's one memory\nanalysis problem.",
    "start": "2299990",
    "end": "2305980"
  },
  {
    "text": "Here's another one. Here's the deal. You've got a user and they're\nusing this middleware server. And they complain that with\ntime it gets slower, and",
    "start": "2305980",
    "end": "2312590"
  },
  {
    "text": "slower, and slower. And if you look at\nall of the-- if you look at CPU network and\ndisk, you see that nothing is",
    "start": "2312590",
    "end": "2317760"
  },
  {
    "text": "being saturated there. But if you look at memory,\nit's increasing pretty dramatically. And at some point, it increases\nso dramatically that",
    "start": "2317760",
    "end": "2324810"
  },
  {
    "text": "the server application\ncrashes. So first thing, let's actually\nlook at the memory utilization",
    "start": "2324810",
    "end": "2330820"
  },
  {
    "text": "and see the timeline. What I show here is I show\nversus time, I show a chart of something in Windows that's\ncalled private bytes.",
    "start": "2330820",
    "end": "2337190"
  },
  {
    "text": "Essentially, think of\nthat as memory. What you see is that memory is\ngrowing slowly, whatever.",
    "start": "2337190",
    "end": "2342550"
  },
  {
    "text": "But at some point, near the very\nend, near when the server crashes, it starts growing at\na really, really fast clip.",
    "start": "2342550",
    "end": "2350010"
  },
  {
    "text": "Now remember, I call\nthis private bytes. What that essentially is, in\nthis particular case, it's Windows giving a process\nsome amount of memory.",
    "start": "2350010",
    "end": "2358119"
  },
  {
    "text": "It's not necessarily what\nthe process or what the application is requesting. The application will request\nsome amount.",
    "start": "2358120",
    "end": "2363260"
  },
  {
    "text": "And Windows will give\nit this much. Well, Windows keeps giving it\nmore and more and more. And it eventually just runs\nout of memory to give.",
    "start": "2363260",
    "end": "2371840"
  },
  {
    "text": "So the reason I made the\ndistinction between something that an application and\nsomething Windows is giving it is that I, as an application,\nmight be asking for",
    "start": "2371840",
    "end": "2379260"
  },
  {
    "text": "four bytes at a time. Four bytes, four bytes,\nwhatever. But what if Windows has this\nproblem where, whenever you allocate-- it doesn't, but\nsuppose it does-- suppose that",
    "start": "2379260",
    "end": "2385740"
  },
  {
    "text": "every time you ask for four\nbytes, it says, well, you know what, I'm just going to give you\n16 bytes because that's my smallest increment of memory. You can imagine if I keep asking\nfor 4 and it's dumb",
    "start": "2385740",
    "end": "2392340"
  },
  {
    "text": "enough to give me 16 every time,\nwe're going to see a huge expansion of memory, even\nthough I, as an application, didn't ask for that much.",
    "start": "2392340",
    "end": "2397730"
  },
  {
    "text": "So it's important to keep these\ndistinctions in mind. So the point is what was\nhappening in this case, was my",
    "start": "2397730",
    "end": "2402860"
  },
  {
    "text": "server's going fine, fine, and\nfine, until near the very end where it starts increasing in\nmemory use at such a dramatic rate, that it eventually\ncrashes.",
    "start": "2402860",
    "end": "2410090"
  },
  {
    "text": "So the first thing I did-- We just talked a\nlot about these reference counted objects. The fact that I keep track\nof whenever something is",
    "start": "2410090",
    "end": "2415670"
  },
  {
    "text": "allocated and whenever something\nis not being looked at anymore. When it's not being looked\nat anymore, I go ahead and remove it. If you look at the reference\ncounted objects, they were",
    "start": "2415670",
    "end": "2422470"
  },
  {
    "text": "pretty flat until you get to\nthe very end of the run, exactly the same as when the\nmemory was increasing.",
    "start": "2422470",
    "end": "2428150"
  },
  {
    "text": "And it turns out, I've\nhighlighted a few of these, and they ended up being-- it doesn't matter what they\nspecifically were-- but in",
    "start": "2428150",
    "end": "2433240"
  },
  {
    "text": "this case, they ended up being\nrelated to threads, and mutexes, and stuff associated\nwith keeping track of what was",
    "start": "2433240",
    "end": "2438339"
  },
  {
    "text": "going on in the system. So some thread-related\nobjects were increasing for some reason. Now turns out thread state in\nsome sense is very cheap.",
    "start": "2438340",
    "end": "2445690"
  },
  {
    "text": "So just because these guys were\nincreasing, it doesn't really explain why we're\ndramatically increasing in memory. But I'll tell you what\ndoes explain it.",
    "start": "2445690",
    "end": "2452799"
  },
  {
    "text": "If you take a look at a\ndifferent attribute, which is these are the total number of\nthreads in our system, we have a static pool of threads. And whenever a work item comes\nin, we allocate a thread.",
    "start": "2452800",
    "end": "2460170"
  },
  {
    "text": "Whenever that work\nitem is done, we deallocate the thread. So we keep it to a\nminimal number. And it turns out,\nwe had a cap.",
    "start": "2460170",
    "end": "2465200"
  },
  {
    "text": "We said, OK, I'm going to allow\nat most 20 operations to occur in flight. Anything beyond 20, I'm going\nto queue it up in memory.",
    "start": "2465200",
    "end": "2471690"
  },
  {
    "text": "And we'll deal with it later. Here's the deal. What you see when you look at\nthe number of threads being used is that-- it's the bottom chart,\nunfortunately, it's a little",
    "start": "2471690",
    "end": "2477950"
  },
  {
    "text": "hard to see. But the point is it goes up and\ndown, up and down, up and down with time. So stuff gets allocated. Stuff gets used.",
    "start": "2477950",
    "end": "2483530"
  },
  {
    "text": "Stuff gets removed. But then when you get to the\nvery end, you see this monotonic increase in the number\nof threads being used.",
    "start": "2483530",
    "end": "2488790"
  },
  {
    "text": "It never goes down. And at some point, you hit the\npoint where all of the threads that you've preallocated\nare completely taken.",
    "start": "2488790",
    "end": "2495460"
  },
  {
    "text": "You obviously can't allocate\nany more threads. So every incoming request,\nyou go ahead and queue.",
    "start": "2495460",
    "end": "2500940"
  },
  {
    "text": "This was the root\nof the problem. The root of the problem is\nthat in this particular situation, each of the times\nthat it was increasing was",
    "start": "2500940",
    "end": "2508049"
  },
  {
    "text": "because there was again\nan uncaught exception. That uncaught exception resulted\nin a thread not being",
    "start": "2508050",
    "end": "2514830"
  },
  {
    "text": "deallocated. At some point, we ran\nout of threads. And every single request that\ncame in was being allocated in",
    "start": "2514830",
    "end": "2520220"
  },
  {
    "text": "memory and waiting around for\na thread to become available before it could be finished. Well, if no threads are\navailable, these things can",
    "start": "2520220",
    "end": "2526570"
  },
  {
    "text": "never get pulled\noff the queue. You see a monotonic increase\nin the amount of memory. And you eventually die.",
    "start": "2526570",
    "end": "2531810"
  },
  {
    "text": "And so, it's actually\na correctness issue. Why are you not catching\nexceptions properly?",
    "start": "2531810",
    "end": "2537180"
  },
  {
    "text": "And why are you not recovering\nproperly from them. But interestingly, it manifested\nitself in a performance issue.",
    "start": "2537180",
    "end": "2542980"
  },
  {
    "text": "So in each of these cases,\nwhat we've done, is we've looked at customized profiler\ninstead of a tool that we might have gotten, Purifier,\nValgrind, or whatever--",
    "start": "2542980",
    "end": "2550860"
  },
  {
    "text": "it's nice that it's tailored\nto our application. And we can make it as\narbitrarily fast, because it understands what our application\nis doing.",
    "start": "2550860",
    "end": "2556760"
  },
  {
    "text": "And it also-- the other nice\nthing about this is this can be run in production\nenvironments. If you write your own tool,\nand a customer calls and complains, you can just say, oh,\njust twiddle this knob and",
    "start": "2556760",
    "end": "2562916"
  },
  {
    "text": "you're done. That's kind of convenient. But of course, the disadvantage\nis that now you have to-- whenever you rewrite\nyour code or do a new rev, or",
    "start": "2562916",
    "end": "2570030"
  },
  {
    "text": "whatever, you have\nto make sure that these tools work right. And you have to recompile\nthe code. If you have something like\nPurifier, Quanitify, in a lot",
    "start": "2570030",
    "end": "2575600"
  },
  {
    "text": "of situations, you can run with\nsome sort of preexisting binary-- sometimes,\nnot always. You can run it without\nhaving to recompile.",
    "start": "2575600",
    "end": "2580870"
  },
  {
    "text": "And that can be very\nconvenient. So I don't know. I was kind of stuck for what a\nlesson this was besides common",
    "start": "2580870",
    "end": "2587599"
  },
  {
    "text": "sense is useful. So I just said memory profiling\nis pretty critical. Don't ignore that when you're\nwriting your applications. And I think I also want\nto point out that--",
    "start": "2587600",
    "end": "2594760"
  },
  {
    "text": "I didn't put this here. But you might think that when\nyou're using a garbage collected language, like\nsomething like Java, that these problems go away.",
    "start": "2594760",
    "end": "2600980"
  },
  {
    "text": "But they actually don't. The problem is actually now\nmemory allocations and deallocations are more\nhidden from you. So you have to be a little bit\nmore careful about how you",
    "start": "2600980",
    "end": "2607750"
  },
  {
    "text": "analyze them. Case study number seven. So we've looked a lot of things\nrelated to CPU usage.",
    "start": "2607750",
    "end": "2612940"
  },
  {
    "text": "We've looked at a lot of things\nrelated to memory. Now let's look a little\nbit at the network. Here was the problem.",
    "start": "2612940",
    "end": "2618240"
  },
  {
    "text": "I have a user, our\nCanonical admin. The user wants to perform an\noperation on some virtual machine somewhere.",
    "start": "2618240",
    "end": "2623810"
  },
  {
    "text": "Remember the flow. Issue request, it goes\nto some server. The server goes to the host\nwhere that VM is.",
    "start": "2623810",
    "end": "2629910"
  },
  {
    "text": "Does some operation,\ncomes back. So this user wants to\nperform an operation on a virtual machine. I had setup A, In setup A, this\noperation end to end took",
    "start": "2629910",
    "end": "2639260"
  },
  {
    "text": "about eight seconds. Fine. That doesn't mean much until you\ncompare to setup B, where it took twice the\namount of time.",
    "start": "2639260",
    "end": "2645339"
  },
  {
    "text": "It took 16 seconds,\nnot 8 seconds. So now the question you're\nasking yourself is what's the difference between\nsetups A and B.",
    "start": "2645340",
    "end": "2651620"
  },
  {
    "text": "In setups A and B, the only\ndifference is that this middle server is a different\nversion number.",
    "start": "2651620",
    "end": "2657560"
  },
  {
    "text": "But everything else is\nexactly the same. So with this different version\nyou somehow manage to make your performance twice as bad,\nwhich is a really bad thing.",
    "start": "2657560",
    "end": "2665080"
  },
  {
    "text": "We got to get to the\nbottom of that. And in addition, it's not like\nit was some random thing that happened once. It happened every single time.",
    "start": "2665080",
    "end": "2670480"
  },
  {
    "text": "So now, let's take a look\nat what was going on. So again, a lot of times, I\ntend to do things in a top",
    "start": "2670480",
    "end": "2676430"
  },
  {
    "text": "down manner, where I say what's\nthe easiest information to collect. And then, OK. What's more difficult, what's\nmore difficult? And finally, if you have to use\nreally industrial strength",
    "start": "2676430",
    "end": "2683849"
  },
  {
    "text": "tools, you use them. But it might not be necessary. In this case, my first thing\nwas just to use logging and try to figure out where\ntime was being spent.",
    "start": "2683850",
    "end": "2690060"
  },
  {
    "text": "Remember, we've got a client\ntalking to a server, talking to an end host. In this case, the latency\nimposed by that client was",
    "start": "2690060",
    "end": "2695569"
  },
  {
    "text": "exactly the same in\nthese two setups. So that's not where this eight\nseconds is being lost.",
    "start": "2695570",
    "end": "2700940"
  },
  {
    "text": "The amount of time being claimed\nby the server was exactly the same. I'm a client.",
    "start": "2700940",
    "end": "2706010"
  },
  {
    "text": "I go to a server. It then goes to a host. However, the time spent on\nthe host was different.",
    "start": "2706010",
    "end": "2712710"
  },
  {
    "text": "That's where all of the\ntime was being spent. Now, you're probably looking\nat-- well, you may not be. But I'll tell you to look now.",
    "start": "2712710",
    "end": "2718220"
  },
  {
    "text": "We had setup A. Setup A was a\nclient and the server were each at some version number. And the host was at the\nsame version number.",
    "start": "2718220",
    "end": "2724710"
  },
  {
    "text": "In setup B, the client and the\nserver were at a different version number with respect\nto the host. But we just said all of\nthe time is being",
    "start": "2724710",
    "end": "2730890"
  },
  {
    "text": "spent in the host. So why is that going on? And this is where understanding\nthe architecture is important. It turns out in this case, the\nfirst thing that happens when",
    "start": "2730890",
    "end": "2737940"
  },
  {
    "text": "you have a difference in version\nnumber is that the server helpfully talks to the\nend host and says you know what, you're in a different\nversion number.",
    "start": "2737940",
    "end": "2744010"
  },
  {
    "text": "So let me give you some code\nto run, because you're in a different version number. So in fact, even though the host\nis at the same version",
    "start": "2744010",
    "end": "2750030"
  },
  {
    "text": "number, there's a small shim\nrunning on that host that makes up for the fact\nthat it's not at the same version number.",
    "start": "2750030",
    "end": "2756110"
  },
  {
    "text": "So now let's analyze this host\nand figure out where is this time being spent. I did a little bit more\nlogging on the host.",
    "start": "2756110",
    "end": "2762020"
  },
  {
    "text": "The reason I did this, because\na lot of the standard tools that you might use, like gprof\nor whatever, these things didn't exist. We have our own kernel.",
    "start": "2762020",
    "end": "2768049"
  },
  {
    "text": "Which therefore, we have our\nown tools and our own nightmares, and headaches\nassociated with it. So we had to use these tools\ninstead of commercially",
    "start": "2768050",
    "end": "2774440"
  },
  {
    "text": "available tools. When you use these tools,\nwe ended up narrowing down the time. Here's what goes on. I'm a client. And I talk to the server.",
    "start": "2774440",
    "end": "2780480"
  },
  {
    "text": "The server talks to an\nagent on the host. It says, agent, this is\nwhat I want to do. The agent then talks to the\nhardware abstraction layer and",
    "start": "2780480",
    "end": "2786980"
  },
  {
    "text": "says, dude, this is\nwhat I want to do. The hardware abstraction layer\ncomes back to the agent and says, done. Agent goes back to the server.",
    "start": "2786980",
    "end": "2792670"
  },
  {
    "text": "Sever goes back to the client,\nblah, blah, blah. It turns out that this\ninteraction on the host between the agent that accepted\nthe call from the",
    "start": "2792670",
    "end": "2799630"
  },
  {
    "text": "server and the hardware\nabstraction layer, that was where all of the time\nwas being spent. It was 10 milliseconds\nin one case.",
    "start": "2799630",
    "end": "2805640"
  },
  {
    "text": "And 20 times that length\nin the other case. And so this was a pretty big\ndifference not to be ignored.",
    "start": "2805640",
    "end": "2811280"
  },
  {
    "text": "So we wanted to look at\nwhy that was going on. So here, it's kind\nof a dumb thing. But the next the thing I did\nwas, well, let's take a look",
    "start": "2811280",
    "end": "2816980"
  },
  {
    "text": "at the configuration. Maybe I did something stupid\nthat would have caused this to happen. Here's the deal. In setup A, remember everything\nis at the same",
    "start": "2816980",
    "end": "2823220"
  },
  {
    "text": "version number. And it turns out this agent HAL\ncommunication occurs over a named pipe. OK, whatever.",
    "start": "2823220",
    "end": "2829220"
  },
  {
    "text": "It turns out in setup B-- remember the client and the\nserver were at a different version from the host, so the\nserver has to download some",
    "start": "2829220",
    "end": "2834570"
  },
  {
    "text": "shim code onto that\nhost so that everybody can talk correctly. It turns out that shim code\nwas communicating with the",
    "start": "2834570",
    "end": "2841520"
  },
  {
    "text": "hardware abstraction layer--\ninstead of over a named pipe-- it was communicating\nover TCP/IP.",
    "start": "2841520",
    "end": "2847030"
  },
  {
    "text": "Now I have to give credit\nto my colleague on this. As soon as I showed him this\ninformation, he hit a flash of",
    "start": "2847030",
    "end": "2853680"
  },
  {
    "text": "inspiration. It turns out that when you're\nusing the named pipe and it takes 10 milliseconds, the\nreason that's different from",
    "start": "2853680",
    "end": "2860810"
  },
  {
    "text": "TCP/IP communication is because\nof something called Nagle's algorithm. Nagle's algorithm basically\nsays, you want to accumulate",
    "start": "2860810",
    "end": "2867560"
  },
  {
    "text": "data before you send it so you\nreduce the number of round trips and the overhead\nof creating a connection, and whatever.",
    "start": "2867560",
    "end": "2872570"
  },
  {
    "text": "And the problem was this\nwas biting us. Essentially, we were waiting\nwithin that shim to collect enough data to send it\nback to that HAL.",
    "start": "2872570",
    "end": "2879180"
  },
  {
    "text": "And we were waiting fairly\nneedlessly, because what happens is that this algorithm\nwaits for a",
    "start": "2879180",
    "end": "2884330"
  },
  {
    "text": "certain amount of time. And then if it hasn't gotten\nmore data, it goes ahead and sends it. So we are basically stuck,\nwaiting for this time out to",
    "start": "2884330",
    "end": "2889820"
  },
  {
    "text": "occur before the data actually\ngot transferred. And there's a very simple\nway to get rid of that. You just tell it, hey, don't\nuse Nagle's algorithm.",
    "start": "2889820",
    "end": "2894990"
  },
  {
    "text": "We don't care. And that solves the\nentire problem. So the point is once you get rid\nof that, the performance",
    "start": "2894990",
    "end": "2901110"
  },
  {
    "text": "is exactly the same. It's a very simple configuration\nsetting. But that's again, a situation\nwhere somebody-- and in fact, I remember\nasking why this",
    "start": "2901110",
    "end": "2907720"
  },
  {
    "text": "particular change was made. And again, it was, well, we\ndidn't think it was going to be a big deal. And that ends up being, again,\na very bad way of doing",
    "start": "2907720",
    "end": "2913470"
  },
  {
    "text": "performance engineering. So definitely, the lesson that\nI learned here is just like the lesson with 32-bit,\n64-bit.",
    "start": "2913470",
    "end": "2919300"
  },
  {
    "text": "You might think that a little\nchange like this doesn't mean anything. But it really does. And here, it's actually\nimportant to understand the",
    "start": "2919300",
    "end": "2924650"
  },
  {
    "text": "entire stack end to end. Because otherwise, you're never\ngoing to be able to figure out where the problem\nactually was occurring. ",
    "start": "2924650",
    "end": "2931560"
  },
  {
    "text": "I already alluded to a previous\nsituation where we had a correctness problem,\nwhich masked as a performance problem. In the previous case, it\nwas because I had a",
    "start": "2931560",
    "end": "2938289"
  },
  {
    "text": "limited thread pool. And whenever I ran out of\nthreads, I'd queue everyting. And at some point, I had a\ncorrectness problem where I kept killing threads and never\nresurrecting them.",
    "start": "2938290",
    "end": "2944790"
  },
  {
    "text": "And so I ended up queuing\neverything. And everything went to pot. Now let me talk about a\ndifferent situation. Here I've got a poor little\ncustomer, in fact that",
    "start": "2944790",
    "end": "2951830"
  },
  {
    "text": "customer was me. I was powering on a\nvirtual machine. Remember it goes to the server,\ngoes to host, powers",
    "start": "2951830",
    "end": "2957015"
  },
  {
    "text": "on the virtual machine,\ntells me it's done. Every other time I would do\nthis, it took five seconds,",
    "start": "2957015",
    "end": "2962109"
  },
  {
    "text": "which is pretty OK. In comparison, every other time\nI did it, it would take about five minutes.",
    "start": "2962110",
    "end": "2967780"
  },
  {
    "text": "That's a pretty huge\nexpansion. And if a customer called me\nand said this, I would basically be in deep\ndoggy do-do.",
    "start": "2967780",
    "end": "2973150"
  },
  {
    "text": "So it was important to try\nto figure what was actually going on here. So why does powering on a\nVM have such a variable",
    "start": "2973150",
    "end": "2978530"
  },
  {
    "text": "performance? Well, the key here, again, is\nto understand what's the end to end path for things. In this case, what I know is\nthat when you power on a",
    "start": "2978530",
    "end": "2985190"
  },
  {
    "text": "virtual machine, one of the\nthings you have to do is allocate some space on\ndisk to write swap information and whatever.",
    "start": "2985190",
    "end": "2991180"
  },
  {
    "text": "So one of the first places I\nlooked was I said, well, let me understand how the disk is\nperforming in each of these cases to try to figure\nout what's going on.",
    "start": "2991180",
    "end": "2997220"
  },
  {
    "text": "It's a mixture of common sense\nand knowing what's going on, and then knowing\nwhere to look. So let me let you in\non a little secret.",
    "start": "2997220",
    "end": "3004190"
  },
  {
    "text": "In the real world, if you're\nusing some sort of disk, if your disk latency is basically\n10 milliseconds or less,",
    "start": "3004190",
    "end": "3009820"
  },
  {
    "text": "you're more or less golden. That's not a problem. If your latency is between 10\nand 20 milliseconds, maybe",
    "start": "3009820",
    "end": "3015299"
  },
  {
    "text": "that's acceptable because you\nhave a complicated topology. You're some multinational\ncorporation with sites everywhere, whatever.",
    "start": "3015300",
    "end": "3021140"
  },
  {
    "text": "If your latency is getting to\nthe point where they're 50 milliseconds, then you probably",
    "start": "3021140",
    "end": "3026350"
  },
  {
    "text": "have to start worrying. That's actually quite\na bit high. If your latency is greater\nthan 51 seconds, that is",
    "start": "3026350",
    "end": "3031869"
  },
  {
    "text": "staggeringly bad. In this chart, I show you\nthe latency versus time.",
    "start": "3031870",
    "end": "3038290"
  },
  {
    "text": "And what this chart shows you\nis that every single time step, it says in the last five\nminutes, what was the highest",
    "start": "3038290",
    "end": "3043900"
  },
  {
    "text": "disk latency that I saw. And remember our\nrules of thumb.",
    "start": "3043900",
    "end": "3049160"
  },
  {
    "text": "10 milliseconds, good. 20 milliseconds, not so good. 50 milliseconds, not so good. This one is 1,100\nmilliseconds.",
    "start": "3049160",
    "end": "3057530"
  },
  {
    "text": "That's like you could walk to\nthe disk, get the data, and bring it back faster than what\nit's actually doing.",
    "start": "3057530",
    "end": "3062940"
  },
  {
    "text": "That's when you've got\nto call a priest. OK. That's horrible. So this was the reason\nthat five seconds versus five minutes.",
    "start": "3062940",
    "end": "3068470"
  },
  {
    "text": "So now the question is\nwhy is this going on? Why am I seeing such\nunbelievably bad disk latency?",
    "start": "3068470",
    "end": "3074300"
  },
  {
    "text": "And remember, the title\nof this case study was \"Correctness Impacts\nPerformance.\" So",
    "start": "3074300",
    "end": "3080079"
  },
  {
    "text": "let's take a look. What I do here is I take a look\nat all of the events that are happening with respect\nto that disk.",
    "start": "3080080",
    "end": "3086710"
  },
  {
    "text": "You're not going to\nunderstand this. It's some junk related\nto our product. But I'm going to circle\nthe relevant portion. You'll notice a little\nline that says",
    "start": "3086710",
    "end": "3092180"
  },
  {
    "text": "lost access to volume. Here's the deal. I send a request to the disk.",
    "start": "3092180",
    "end": "3097480"
  },
  {
    "text": "It turns out the disk is\nconnected over some channel to some channel, a disk controller,\nwhich is talking",
    "start": "3097480",
    "end": "3102530"
  },
  {
    "text": "to some disk. It turns out, I kept losing\naccess to that disk. Whenever I would lose access,\nit would keep retrying.",
    "start": "3102530",
    "end": "3109599"
  },
  {
    "text": "And that's why ultimately,\nyes, it would complete. But it would take over a second\nper disk request. ",
    "start": "3109600",
    "end": "3117230"
  },
  {
    "text": "In the situations where I was\nnot seeing that loss of connectivity, things were\noccurring at the order of milliseconds and my power-on\nwould take five seconds.",
    "start": "3117230",
    "end": "3124820"
  },
  {
    "text": "So the point here is that in\nthis case, it was actually a correctness problem with\nmy controller.",
    "start": "3124820",
    "end": "3130020"
  },
  {
    "text": "And the controller would\nbasically flake out and not talk to the disk properly. And this manifested itself\nin a performance problem.",
    "start": "3130020",
    "end": "3135380"
  },
  {
    "text": "Once I changed the disk\ncontroller, performance problems went away. So if you're doing this kind\nof debugging, sometimes you",
    "start": "3135380",
    "end": "3142250"
  },
  {
    "text": "have to eliminate-- I guess it's a Sherlock\nHolmesism-- that you eliminate the obvious and whatever's\nleft must be it.",
    "start": "3142250",
    "end": "3148060"
  },
  {
    "text": "A good example of this is my\nex-officemate, he used to ask this question whenever he'd\ninterview somebody for VMware.",
    "start": "3148060",
    "end": "3153380"
  },
  {
    "text": "He'd say, OK, you have\na networking problem. Where is the first\nplace you look? And everybody that's coming\nin with a grad degree or",
    "start": "3153380",
    "end": "3160400"
  },
  {
    "text": "whatever, they're like, I\nwould look at the stack. I'd look at the CPU. I'd look at a cache counters,\nor whatever. And my officemate would just\nlook at them and say, why",
    "start": "3160400",
    "end": "3166720"
  },
  {
    "text": "don't you just check the cable\nand see if it's plugged in. And surprisingly,\nit's so dumb. But when you're talking to\ncustomers, they're like I",
    "start": "3166720",
    "end": "3173970"
  },
  {
    "text": "swear I did that. I swear I did that. Their swearing doesn't\nnecessarily mean anything, unless it's directed at you.",
    "start": "3173970",
    "end": "3179769"
  },
  {
    "text": "So that's case study nine-- eight. The last two case studies are\ngoing to be specific to",
    "start": "3179770",
    "end": "3185620"
  },
  {
    "text": "virtualization. And so what I'm going to do is\ntalk a little bit about how the CPU scheduler works in the\nVMware server class product.",
    "start": "3185620",
    "end": "3192460"
  },
  {
    "text": "And then I'll talk about the\nlast two case studies. So actually how many of you\nhave used VMware before?",
    "start": "3192460",
    "end": "3197890"
  },
  {
    "text": "Just as a hand. Thankfully, Saman has used it. That's good. So basically, the idea is that\nyou're multiplexing time on",
    "start": "3197890",
    "end": "3206960"
  },
  {
    "text": "the CPU between different\nvirtual machines that are running on the CPU. So what I show here is I\nshow ESX, which is our",
    "start": "3206960",
    "end": "3213280"
  },
  {
    "text": "server class product. Basically, think of that\nas the hardware. So you're running virtual\nmachines on hardware. That hardware has, in this\nparticular case,",
    "start": "3213280",
    "end": "3220859"
  },
  {
    "text": "four physical CPUs. No problem. You've got four physical CPUs. And you want to run virtual\nmachines on this hardware.",
    "start": "3220860",
    "end": "3228110"
  },
  {
    "text": "So you've got one\nvirtual machine. Well, that's fine. There's four CPUs. One virtual machine.",
    "start": "3228110",
    "end": "3233650"
  },
  {
    "text": "No problem. Plenty of CPU to go around. Now you got another one. Still no problem. They're not chained\nto any given CPU.",
    "start": "3233650",
    "end": "3240250"
  },
  {
    "text": "And there's plenty of\nCPU to go around. No problem. Same with three and four. You got all these four\nvirtual machines.",
    "start": "3240250",
    "end": "3246080"
  },
  {
    "text": "It's not a one to one correspondence in terms of CPU. We don't work that way. But there's plenty-- the point\nis, there's plenty of CPU to go around.",
    "start": "3246080",
    "end": "3253170"
  },
  {
    "text": "Now what happens, and when VMs\nare basically available and when there's hardware on which\nthey can run and they're using",
    "start": "3253170",
    "end": "3261210"
  },
  {
    "text": "the hardware, that's called\nthe run state. So running means I'm golden. I'm using the hardware.",
    "start": "3261210",
    "end": "3266930"
  },
  {
    "text": "Now, let's add another VM. I put it in red, because this\npoor VM, he wants to run.",
    "start": "3266930",
    "end": "3273800"
  },
  {
    "text": "But you'll see that every single\nCPU is actually taken up by another virtual machine.",
    "start": "3273800",
    "end": "3279650"
  },
  {
    "text": "So even if this virtual machine wants to run, it can't. It has to wait. The time that it spends ready to\nuse the CPU, but unable to",
    "start": "3279650",
    "end": "3288810"
  },
  {
    "text": "use the CPU, because there's\nno CPU available, that's called ready time.",
    "start": "3288810",
    "end": "3293869"
  },
  {
    "text": "In some sense, it's kind\nof the demand. Ready plus signals down. But anyway, the point is\nit's sitting around",
    "start": "3293870",
    "end": "3299160"
  },
  {
    "text": "waiting to use the CPU. But it can't, because other\npeople are using it. So it has to wait its turn. It waits its turn.",
    "start": "3299160",
    "end": "3304910"
  },
  {
    "text": "And eventually a VM, our ESX\nscheduler is smart enough to realize that it has to\nfix the situation.",
    "start": "3304910",
    "end": "3310050"
  },
  {
    "text": "So it deschedules a VM-- in\nthis case, VM 1-- and goes ahead and schedules that other\nvirtual machine that was",
    "start": "3310050",
    "end": "3315119"
  },
  {
    "text": "previously waiting. Now predictably, the VM that's\nnow ready to run but can't is",
    "start": "3315120",
    "end": "3320400"
  },
  {
    "text": "in the ready state. And the VM that formerly was\nin the ready state is now using the CPU.",
    "start": "3320400",
    "end": "3325790"
  },
  {
    "text": "So it's in the run state. It's very happy. The final thing that I should\npoint out is that VMs don't",
    "start": "3325790",
    "end": "3331140"
  },
  {
    "text": "have to be in the ready state. They don't have to be either\nusing the CPU or ready to use the CPU. Some VMs are voluntarily\ndescheduled, because they have",
    "start": "3331140",
    "end": "3337520"
  },
  {
    "text": "nothing to do. Some VMs are descheduled,\nbecause they're waiting-- they're basically waiting\nfor an IO to complete.",
    "start": "3337520",
    "end": "3343300"
  },
  {
    "text": "And so they don't need\nto use the CPU. And that's what I\ndepict by VM6. These are in what's called\nthe wait or idle states.",
    "start": "3343300",
    "end": "3350160"
  },
  {
    "text": "So hopefully, you now have\na fairly rudimentary understanding of what our\nCPU scheduler is doing.",
    "start": "3350160",
    "end": "3356079"
  },
  {
    "text": "Let's talk a little bit about\na very simple performance problem related to this. And I call this-- But It's Only a small\nprobe VM.",
    "start": "3356080",
    "end": "3362910"
  },
  {
    "text": "I'll explain what this\nmeans in a minute. Essentially, you've got two of\nthese hosts, two ESX hosts that both want to run VMs.",
    "start": "3362910",
    "end": "3369840"
  },
  {
    "text": "And I've got a user, depicted\nby the user icon. And he's talking\nto this one VM.",
    "start": "3369840",
    "end": "3375080"
  },
  {
    "text": "Let's call this VM\nthe vSphere VM. Please pardon the\nnomenclature. It doesn't really matter.",
    "start": "3375080",
    "end": "3380520"
  },
  {
    "text": "But anyway, he's talking\nto this vSphere VM on one of the hosts. Now, it turns out that that VM\nthat he's talking to, as I",
    "start": "3380520",
    "end": "3386070"
  },
  {
    "text": "depicted in the diagram, is\ntalking to another virtual machine, which I have labeled\nvSphere database.",
    "start": "3386070",
    "end": "3393420"
  },
  {
    "text": "So it turns out, this vSphere\nVM is talking to a database over what's called an\nODBC connection. That doesn't matter. Don't worry about that.",
    "start": "3393420",
    "end": "3399349"
  },
  {
    "text": "And it turns out that on that\nother host where the database virtual machine is running, we\nhave another virtual machine.",
    "start": "3399350",
    "end": "3404810"
  },
  {
    "text": "It's called the probe\nvirtual machine. The job of that probe virtual\nmachine is to sniff any",
    "start": "3404810",
    "end": "3410680"
  },
  {
    "text": "traffic going into\nthe database.  So as I've said, the vSphere VM",
    "start": "3410680",
    "end": "3416800"
  },
  {
    "text": "communicates with the database. This probe VM is monitoring any\ntraffic that's going to that database virtual machine.",
    "start": "3416800",
    "end": "3424160"
  },
  {
    "text": "And therefore, as you can\nimagine, the more traffic that's being sent between this\nvSphere VM and the database,",
    "start": "3424160",
    "end": "3429730"
  },
  {
    "text": "the more work that that\nprobe VM has to do.",
    "start": "3429730",
    "end": "3434900"
  },
  {
    "text": "Here was the complaint. My colleague called me up, and\nhe said, hey, I'm having a little bit of a problem\nwith my vSphere VM.",
    "start": "3434900",
    "end": "3441810"
  },
  {
    "text": "All of a sudden, it's completely\nunresponsive. So then I was like, OK, well,\nthis is kind of silly.",
    "start": "3441810",
    "end": "3448059"
  },
  {
    "text": "Let's try to figure out\nwhat's going on. So here's what I did. I graphed various metrics, which\nI just spoke about, the",
    "start": "3448060",
    "end": "3455720"
  },
  {
    "text": "used time and the ready\ntime, on this chart. The time is on the x-axis. And the y-axis is\nin milliseconds.",
    "start": "3455720",
    "end": "3463210"
  },
  {
    "text": "In what I'm going to call the\nland before time, the ready time of the database.",
    "start": "3463210",
    "end": "3468730"
  },
  {
    "text": "So look at the circle on the--\nlet me think, you're-- on the left. This is the ready time of that\ndatabase virtual machine.",
    "start": "3468730",
    "end": "3476440"
  },
  {
    "text": "Remember what we said,\nwhen you're accumulating ready time. It means you really want\nto use the CPU. But you can't because all the\nCPUs are already in use.",
    "start": "3476440",
    "end": "3483300"
  },
  {
    "text": "So it turns out that the first\nthing that happened when my colleague called and said I'm\nhaving a problem, is I look at",
    "start": "3483300",
    "end": "3489859"
  },
  {
    "text": "this chart, which\nis versus time. And I said, let me guess. Are you having a problem at\nabout such and such time, which corresponds to the\nmiddle of this chart?",
    "start": "3489860",
    "end": "3495870"
  },
  {
    "text": "And he said, yeah, that's when\nI'm having a problem. So you can tell that there are\ntwo very clearly demarcated regions here.",
    "start": "3495870",
    "end": "3502510"
  },
  {
    "text": "In the left, you can\nsee that this ready time, it's around 12%. Now if you're like me, a number\nthat has no context",
    "start": "3502510",
    "end": "3509610"
  },
  {
    "text": "means nothing. So 12% means nothing. However, what I'd like to point\nout is that when things got really bad, that number\nspiked up to 20%.",
    "start": "3509610",
    "end": "3519130"
  },
  {
    "text": "So here's the deal. I'm a database virtual\nmachine. I want to use the physical\nhardware.",
    "start": "3519130",
    "end": "3524500"
  },
  {
    "text": "I can use it about 90% of the\ntime, because my ready time's about 12%, whatever. But at some point, something\nhappens where I now can only",
    "start": "3524500",
    "end": "3530960"
  },
  {
    "text": "use it 80% of the time. And it turns out that that\ndifference was enough to cause this vSphere VM talking to this\ndatabase to be perturbed.",
    "start": "3530960",
    "end": "3539760"
  },
  {
    "text": "And that ultimately was the\ncustomer's problem. So the question is why. And here is the reason.",
    "start": "3539760",
    "end": "3545200"
  },
  {
    "text": "Turns out, here's the deal. Let's go back to the diagram\nso I can explain this. Remember that the vSphere VM\ntalks to the database VM.",
    "start": "3545200",
    "end": "3551810"
  },
  {
    "text": "Remember as well, that the more\ntraffic there is, the more work the probe\nVM has to do.",
    "start": "3551810",
    "end": "3557080"
  },
  {
    "text": "It turns out that what this user\ndid was at that critical juncture, he started\na workload.",
    "start": "3557080",
    "end": "3562290"
  },
  {
    "text": "In starting that workload, he\ncaused a lot of traffic between the vSphere VM and the\ndatabase, which therefore caused a lot of work\nby this probe VM.",
    "start": "3562290",
    "end": "3570460"
  },
  {
    "text": "Well, what's happening? The probe VM and the database\nVM are sharing the same underlying CPUs. So if one of them is running,\nthe other one can't.",
    "start": "3570460",
    "end": "3577470"
  },
  {
    "text": "And that's why we saw this\ngentle increase in the amount of ready time to vSphere\ndatabase. Well, this has a cascading\neffect, because it can't get",
    "start": "3577470",
    "end": "3584330"
  },
  {
    "text": "as much time to use a CPU. Any request that goes to\nit gets slowed down. Where are the requests\ncoming from?",
    "start": "3584330",
    "end": "3589960"
  },
  {
    "text": "They're coming from\nthis vSphere VM. Who's initiating\nthose requests? It's this poor admin. The bottom line is that this\nadmin is seeing poor",
    "start": "3589960",
    "end": "3595650"
  },
  {
    "text": "responsiveness because some\ndoofus put his probe VM on some other host, because he\nwanted to see what was going on in the database.",
    "start": "3595650",
    "end": "3601530"
  },
  {
    "text": "In fairness, I was\nthat doofus. But that's fine. We needed that, because\nwe had to debug other performance problems.",
    "start": "3601530",
    "end": "3607150"
  },
  {
    "text": "So the point is that one of\nthe things that's kind of important in this particular\ncase is that whenever you",
    "start": "3607150",
    "end": "3613580"
  },
  {
    "text": "introduce any sort of\nmonitoring-- it's like a Heisenberg thing-- it's going to\nhave a performance impact. And understanding\nthat performance",
    "start": "3613580",
    "end": "3619010"
  },
  {
    "text": "impact is pretty key. It might even shift where the bottlenecks are in your system.",
    "start": "3619010",
    "end": "3624119"
  },
  {
    "text": "If your thing has a high cost\nin terms of CPU, it might shift the overhead of whatever's\nrunning to the CPU where it wasn't there before.",
    "start": "3624120",
    "end": "3630230"
  },
  {
    "text": "So it's kind of important to\nunderstand what you're monitoring and how you're\nmonitoring it. OK. We're almost done.",
    "start": "3630230",
    "end": "3636230"
  },
  {
    "text": "This was a really\ncurious problem. We have a customer. And a customer was performing\na load test on a server.",
    "start": "3636230",
    "end": "3643860"
  },
  {
    "text": "And in that load test, what they\nwould do is they would just keep attaching clients\nto that end server and run some test.",
    "start": "3643860",
    "end": "3650540"
  },
  {
    "text": "At some point, they would\nattach clients and their workload suffered, their\nworkload performance suffered",
    "start": "3650540",
    "end": "3656070"
  },
  {
    "text": "even though there was no metric\nthat was saturated. And so the question was why. Why you keep adding clients,\nnothing is saturated, but",
    "start": "3656070",
    "end": "3662890"
  },
  {
    "text": "performance suffers? Well, this chart's kind\nof an eyesore. But let me explain it.",
    "start": "3662890",
    "end": "3669770"
  },
  {
    "text": "The first thing we do is we've\ngot some profiling information. On this chart, I show\ntwo things. In purple, I show CPU usage.",
    "start": "3669770",
    "end": "3676359"
  },
  {
    "text": "And in white, I show\ndisk latency. As you can see from this\nchart-- and the",
    "start": "3676360",
    "end": "3681500"
  },
  {
    "text": "y-axis is just 100%. And they're both normalized. As you can see at some point,\nthe CPU usage is steadily, steadily increasing.",
    "start": "3681500",
    "end": "3688190"
  },
  {
    "text": "And at some point, the disk\nlatency takes over and gets to be really large. You can tell that, because up\nuntil about 3/4 of the way in,",
    "start": "3688190",
    "end": "3695600"
  },
  {
    "text": "the disk latency hovers\naround zero. And all of a sudden, it\njumps much higher. So my first instinct was to tell\nthe customer you've got a",
    "start": "3695600",
    "end": "3701750"
  },
  {
    "text": "disk problem. Your disk latencies are\ngoing over a cliff. And that's why you're having\na performance problem.",
    "start": "3701750",
    "end": "3707039"
  },
  {
    "text": "Unfortunately, that's\nnot correct. Why is that not correct?",
    "start": "3707040",
    "end": "3712900"
  },
  {
    "text": "It's not correct, because, yeah,\ndisk latency gets worse at 4:00 PM. It's because of swapping and\nsome other nonsense.",
    "start": "3712900",
    "end": "3719119"
  },
  {
    "text": "We know that. However, the application latency\nactually gets worse at 3:30, not at 4:00.",
    "start": "3719120",
    "end": "3724310"
  },
  {
    "text": "It's half an hour earlier. So now the question is what's\ngoing on that causes the problem, even before disk\nkicks in and gets to be",
    "start": "3724310",
    "end": "3730140"
  },
  {
    "text": "another problem. This is where understanding\nthe difference in metrics",
    "start": "3730140",
    "end": "3735930"
  },
  {
    "text": "becomes pretty critical. Now, I'm not expecting\nvery many people to understand this chart. But I will try to explain it.",
    "start": "3735930",
    "end": "3741860"
  },
  {
    "text": "What I do here is I show a\ndifferent chart, which essentially shows CPU usage of\nall the different virtual machines and processes on\nthe underlying host.",
    "start": "3741860",
    "end": "3749310"
  },
  {
    "text": "Forget about what's in\nmost of this chart. And just focus on what\nI've circled. I have two columns\nshown there. One column is called\npercent used.",
    "start": "3749310",
    "end": "3757610"
  },
  {
    "text": "And one column is called\npercent run. You can think about it and\nthink, OK, well, I can imagine",
    "start": "3757610",
    "end": "3763970"
  },
  {
    "text": "used time means probably how\nmuch time I'm using the CPU. But then what is the runtime? Isn't runtime the same thing,\nbecause I'm using the CPU?",
    "start": "3763970",
    "end": "3771080"
  },
  {
    "text": "What's the difference? This is actually the\ncritical problem. It turns out, that when you talk\nof a percent used time,",
    "start": "3771080",
    "end": "3778050"
  },
  {
    "text": "you're normalizing that to\nthe base clock frequency. If I've got a three gigahertz\nmachine, I've normalized it to",
    "start": "3778050",
    "end": "3783920"
  },
  {
    "text": "three gigahertz. However, we all live in the\nera of power management. It turns out this percent run,\nwhich nobody understands, is",
    "start": "3783920",
    "end": "3790700"
  },
  {
    "text": "normalized to the clock\nfrequency at the time you were running. So if I did power management,\nand I was only running at two",
    "start": "3790700",
    "end": "3796840"
  },
  {
    "text": "gigahertz for a certain amount\nof time, percent run captures that. And so in this particular chart\nwhat I'm showing is that",
    "start": "3796840",
    "end": "3803075"
  },
  {
    "text": "a bunch of virtual machines are\nseeing used time different from runtime. And the runtime, which is\nnormalized to the clock",
    "start": "3803075",
    "end": "3809590"
  },
  {
    "text": "frequency of the time you're\nusing it, is actually greater than used time, which suggests\nthat the user's actually using",
    "start": "3809590",
    "end": "3815029"
  },
  {
    "text": "power management. And it turns out, that power\nmanagement was kicking in at",
    "start": "3815030",
    "end": "3820320"
  },
  {
    "text": "some point and causing their\napplication latency to go down even though the CPU was\nnever saturated.",
    "start": "3820320",
    "end": "3826240"
  },
  {
    "text": "And in this case, when we went\nback to the customer-- you look at this chart, and you\ninstantly see, OK, I know that because this is normalized and\nthis is normalized to that.",
    "start": "3826240",
    "end": "3832820"
  },
  {
    "text": "This is what's going on, you\nhave to first of all change this parameter. They changed it. And they were unbelievably\nhappy. It was pretty cool.",
    "start": "3832820",
    "end": "3838630"
  },
  {
    "text": "Now, I have to admit that\nthere's much more to this story that I'm not\nshowing you. Which I'm not allowed\nto show you. But it becomes a very\ninteresting thing.",
    "start": "3838630",
    "end": "3844540"
  },
  {
    "text": "But the point is it's very, very\nimportant to understand",
    "start": "3844540",
    "end": "3849720"
  },
  {
    "text": "your metrics. Here just understanding the\ndifference between runtime and used time made all of the\ndifference in diagnosing this",
    "start": "3849720",
    "end": "3855210"
  },
  {
    "text": "customer's performance\nrelated issue. So I just pick a smattering\nof ten random bugs.",
    "start": "3855210",
    "end": "3860880"
  },
  {
    "text": "And in each of them, I tried\nto distill one kind of conclusion you can get from\nthat particular thing.",
    "start": "3860880",
    "end": "3867170"
  },
  {
    "text": "Because all of you can read,\nI'm not going to bother repeating all of these things. But anyway, here's\nthe first five.",
    "start": "3867170",
    "end": "3873140"
  },
  {
    "text": "And you all get the slides. And here's the second five. Oops. I'll just keep that\nup for a second.",
    "start": "3873140",
    "end": "3880069"
  },
  {
    "text": "And in order to conclude this\ntalk, let me just say that",
    "start": "3880070",
    "end": "3886970"
  },
  {
    "text": "it's very important to\navoid assumptions. It's very tempting to\nsay well that's-- I'll give you a great example.",
    "start": "3886970",
    "end": "3892520"
  },
  {
    "text": "I used to interview people. And I'd say, OK, you've\ngot a UI. You've got a problem. You have a UI. And you click on something. And it takes a long time.",
    "start": "3892520",
    "end": "3897790"
  },
  {
    "text": "Why is it taking a long time? And the first thing they'd say,\nwell, I'd go and look at-- this is literally,\nnot joking. They'd say, well, I'd go to look\nat the cache messages on",
    "start": "3897790",
    "end": "3903540"
  },
  {
    "text": "the server. And I just looked them and I\nsaid, how do you know the client's not the problem? Oh, the client's\nnot a problem.",
    "start": "3903540",
    "end": "3908590"
  },
  {
    "text": "And you base this on\nwhat knowledge? Don't use assumptions. It's a very bad idea.",
    "start": "3908590",
    "end": "3913730"
  },
  {
    "text": "It's really important to\nunderstand the entire system that you're dealing with. From the client to the server to\nthe hardware to the network to the disk.",
    "start": "3913730",
    "end": "3919270"
  },
  {
    "text": "You have to understand all of\nthose different pieces, because you have to understand\nwhere to look. And what's really onerous is\nthat-- it is not so much when",
    "start": "3919270",
    "end": "3927210"
  },
  {
    "text": "it's a problem in your code,\nbecause you can be blamed for making stupid mistakes-- it's when it's problems with\nother people's code and you spend a month looking at other\npeople's code, figuring out",
    "start": "3927210",
    "end": "3933530"
  },
  {
    "text": "what's the problem That's when\nthings are really kind of irritating. And especially when it's a\nproblem with hardware.",
    "start": "3933530",
    "end": "3939240"
  },
  {
    "text": "And I think the final bullet\nI'll say is be persistent and be thorough.",
    "start": "3939240",
    "end": "3944680"
  },
  {
    "text": "I don't have to tell you\nhow to be good workers. So let's just blah,\nblah, blah. We're done. ",
    "start": "3944680",
    "end": "3955093"
  }
]