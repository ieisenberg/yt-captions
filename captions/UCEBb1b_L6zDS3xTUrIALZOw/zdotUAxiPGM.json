[
  {
    "start": "0",
    "end": "66000"
  },
  {
    "start": "0",
    "end": "14762"
  },
  {
    "text": "DAVID SONTAG: A\nthree-part lecture today, and I'm still continuing on\nthe theme of reinforcement learning.",
    "start": "14762",
    "end": "20160"
  },
  {
    "text": "Part one, I'm going\nto be speaking, and I'll be following up\non last week's discussion",
    "start": "20160",
    "end": "26212"
  },
  {
    "text": "about causal inference\nand Tuesday's discussion on reinforcement learning. And I'll be going into sort\nof one more subtlety that",
    "start": "26212",
    "end": "35160"
  },
  {
    "text": "arises there and\nwhere we can develop some nice mathematical\nmethods to help with.",
    "start": "35160",
    "end": "40650"
  },
  {
    "text": "And then I'm going\nto turn over the show to Barbra, who I'll formally\nintroduce when the time comes.",
    "start": "40650",
    "end": "47550"
  },
  {
    "text": "And she's going to both\ntalk about some of her work on developing and evaluating\ndynamic treatment regimes,",
    "start": "47550",
    "end": "56520"
  },
  {
    "text": "and then she will\nlead a discussion on the sepsis paper,\nwhich was required reading from today's class.",
    "start": "56520",
    "end": "62650"
  },
  {
    "text": "So those are the three\nparts of today's lecture. ",
    "start": "62650",
    "end": "67920"
  },
  {
    "start": "66000",
    "end": "427000"
  },
  {
    "text": "So I want you to return\nback, put yourself back in the mindset of\nTuesday's lecture where we talked about\nreinforcement learning.",
    "start": "67920",
    "end": "74510"
  },
  {
    "text": "Now, remember that the goal\nof reinforcement learning was to optimize some reward. ",
    "start": "74510",
    "end": "84930"
  },
  {
    "text": "Specifically, our goal is\nto find some policy, which",
    "start": "84930",
    "end": "90920"
  },
  {
    "text": "I can note as pi\nstar, which is the arg",
    "start": "90920",
    "end": "96909"
  },
  {
    "text": "max over all possible\npolicies pi of v of pi,",
    "start": "96910",
    "end": "105240"
  },
  {
    "text": "where just to\nremind you, v of pi is the value of the policy pi. Formally, it's defined as\nthe expectation of the sum",
    "start": "105240",
    "end": "117930"
  },
  {
    "text": "of the rewards across time. So the reason why\nI'm calling this",
    "start": "117930",
    "end": "123409"
  },
  {
    "text": "an expectation with like\nthe pi is because there's stochasticity both in the\nenvironment, and possibly pi",
    "start": "123410",
    "end": "130340"
  },
  {
    "text": "is going to be a\nstochastic policy. And this is summing\nover the time steps, because this is not just a\nsingle time step problem.",
    "start": "130340",
    "end": "138372"
  },
  {
    "text": "But we're going to be\nconsidering interventions across time of the reward\nat each point in time. And that reward function could\neither be at each point in time",
    "start": "138372",
    "end": "145909"
  },
  {
    "text": "or you might imagine that\nthis is 0 for all time steps, except for the last time step. ",
    "start": "145910",
    "end": "152019"
  },
  {
    "text": "So the first question I\nwant us to think about is, well, what are\nthe implications of this as a learning paradigm?",
    "start": "152020",
    "end": "160560"
  },
  {
    "text": "If we look what's going on\nover here, hidden in my story is also an expectation\nover x, the patient,",
    "start": "160560",
    "end": "167610"
  },
  {
    "text": "for example, or\nthe initial state. And so this\nintuitively is saying, let's try to find a policy\nthat has high expected",
    "start": "167610",
    "end": "176730"
  },
  {
    "text": "reward, average [INAUDIBLE]\nover all patients. And I just want you to think\nabout whether that is indeed",
    "start": "176730",
    "end": "184159"
  },
  {
    "text": "the right goal. Can anyone think\nabout a setting where that might not be desirable?",
    "start": "184160",
    "end": "189360"
  },
  {
    "start": "189360",
    "end": "194420"
  },
  {
    "text": "Yeah. AUDIENCE: What if the reward\nis the patient living or dying? You don't want it\nto have high ratings",
    "start": "194420",
    "end": "200172"
  },
  {
    "text": "like saving two\npatients and [INAUDIBLE] and expect the same [INAUDIBLE]. DAVID SONTAG: So what\nhappens if this reward",
    "start": "200173",
    "end": "205989"
  },
  {
    "text": "is something mission critical\nlike a patient dying?",
    "start": "205990",
    "end": "212230"
  },
  {
    "text": "You really want to try to\navoid that from happening as much as possible. Of course, there\nare other criteria",
    "start": "212230",
    "end": "217719"
  },
  {
    "text": "that we might be\ninterested in as well. And both in Frederick's lecture\non Tuesday and in the readings,",
    "start": "217720",
    "end": "223599"
  },
  {
    "text": "we talked about how there might\nbe other aspects about making sure that a patient is not\njust alive but also healthy,",
    "start": "223600",
    "end": "231397"
  },
  {
    "text": "which might play into\nyour reward functions. And there might be rewards\nassociated with those. And if you were to\njust, for example,",
    "start": "231397",
    "end": "236980"
  },
  {
    "text": "put a positive or\nnegative infinity for a patient dying,\nthat's a nonstarter,",
    "start": "236980",
    "end": "243050"
  },
  {
    "text": "right, because if you did that,\nunfortunately in this world, we're not always going to be\nable to keep patients alive.",
    "start": "243050",
    "end": "249863"
  },
  {
    "text": "And so you're going to get\ninto an infeasible optimization problem. So minus infinity\nis not an option.",
    "start": "249863",
    "end": "255130"
  },
  {
    "text": "We're going to have to\nput some number to it in this type of approach.",
    "start": "255130",
    "end": "260450"
  },
  {
    "text": "But then you're going to start\ntrading off between patients. In some cases, you might\nhave a very high reward for--",
    "start": "260450",
    "end": "271510"
  },
  {
    "text": "there are two\ndifferent solutions that you might\nimagine, one solution where the reward is somewhat\nbalanced across patients",
    "start": "271510",
    "end": "279210"
  },
  {
    "text": "and another situation\nwhere you have really small values of\nreward for some patients and a few patients with very\nlarge values and rewards.",
    "start": "279210",
    "end": "285760"
  },
  {
    "text": "And both of them could be\nthe same average, obviously. But both are not\nnecessarily equally useful.",
    "start": "285760",
    "end": "291910"
  },
  {
    "text": "We might want to say\nthat we prefer to avoid that worst-case situation. So one could imagine\nother ways of formulating",
    "start": "291910",
    "end": "298389"
  },
  {
    "text": "this optimization\nproblem, like maybe you want to control the\nworst-case reward instead",
    "start": "298390",
    "end": "303460"
  },
  {
    "text": "of the average-case reward. Or maybe you want\nto say something about different quartiles.",
    "start": "303460",
    "end": "309160"
  },
  {
    "text": "I just wanted to point that\nout, because really that's the starting place for a lot of\nthe work that we're doing here.",
    "start": "309160",
    "end": "315813"
  },
  {
    "text": "So now I want us\nto think through, OK, returning back to this goal,\nwe've done our policy iteration",
    "start": "315813",
    "end": "324870"
  },
  {
    "text": "or we've done our Q\nlearning, that is, and we get a policy out. And we might now\nwant to know what",
    "start": "324870",
    "end": "330780"
  },
  {
    "text": "is the value of that policy? So what is our estimate\nof that quantity?",
    "start": "330780",
    "end": "336050"
  },
  {
    "text": "Well, to get that,\none could just try to read it off\nfrom the results of Q learning by just\ncomputing that the pi--",
    "start": "336050",
    "end": "343900"
  },
  {
    "text": "what I'm calling v pi\nhat-- the estimate is just equal to now a\nmaximum over actions",
    "start": "343900",
    "end": "350860"
  },
  {
    "text": "a of your Q function\nevaluated at whatever your initial state is and the\noptimal choice of action a.",
    "start": "350860",
    "end": "363820"
  },
  {
    "text": "So all I'm saying here is that\nthe last step of the algorithm might be to ask, well,\nwhat is the expected",
    "start": "363820",
    "end": "369930"
  },
  {
    "text": "reward of this policy? And if you remember,\nthe Q learning algorithm is, in essence, a dynamic\nprogramming algorithm",
    "start": "369930",
    "end": "375728"
  },
  {
    "text": "working its way from the\nsort of large values of time up to the present.",
    "start": "375728",
    "end": "381160"
  },
  {
    "text": "And it is indeed actually\ncomputing this expected value that you're interested in. So you could just read\nit off from the Q values",
    "start": "381160",
    "end": "387948"
  },
  {
    "text": "at the very end. But I want to point\nout that here there's an implicit policy built in.",
    "start": "387948",
    "end": "394500"
  },
  {
    "text": "So I'm going to compare this\nin just a second to what happens under the causal\ninference scenario.",
    "start": "394500",
    "end": "400540"
  },
  {
    "text": "So just a single time\nstep in potential outcomes framework that we're used to. Notice that the value of this\npolicy, the reason why it's",
    "start": "400540",
    "end": "409590"
  },
  {
    "text": "a function of pi is\nbecause the value is a function of every\nsubsequent action",
    "start": "409590",
    "end": "417510"
  },
  {
    "text": "that you're taking as well. And so now let's\njust compare that for a second to what happens\nin the potential outcomes",
    "start": "417510",
    "end": "424740"
  },
  {
    "text": "framework.  So there, our starting place--",
    "start": "424740",
    "end": "430639"
  },
  {
    "start": "427000",
    "end": "569000"
  },
  {
    "text": "so now I'm going to turn\nour attention for just one",
    "start": "430640",
    "end": "437560"
  },
  {
    "text": "moment from reinforcement\nlearning now back to just causal inference.",
    "start": "437560",
    "end": "444240"
  },
  {
    "text": "In reinforcement learning,\nwe talked about policies. How do we find\npolicies to do well",
    "start": "444240",
    "end": "449560"
  },
  {
    "text": "in terms of some expected\nreward of this policy? But yet when we were talking\nabout causal inference,",
    "start": "449560",
    "end": "457250"
  },
  {
    "text": "we only used words like\naverage treatment effect or conditional average\ntreatment effect,",
    "start": "457250",
    "end": "464390"
  },
  {
    "text": "where for example, to estimate\nthe conditional average treatment effect,\nwhat we said is we're going to first learn, if we\nuse a covariate adjustment",
    "start": "464390",
    "end": "472430"
  },
  {
    "text": "approach, we learn\nsome function f of x comma t, which\nis intended to be",
    "start": "472430",
    "end": "479900"
  },
  {
    "text": "an approximation of the expected\nvalue of your outcome y given",
    "start": "479900",
    "end": "487520"
  },
  {
    "text": "x comma--  I'll say y of t.",
    "start": "487520",
    "end": "498860"
  },
  {
    "text": "There. So that notation. So the goal of\ncovariate adjustment was to estimate this quantity.",
    "start": "498860",
    "end": "505030"
  },
  {
    "text": "And we could use that then\nto try to construct a policy. For example, you could think\nabout the policy pi of x,",
    "start": "505030",
    "end": "517690"
  },
  {
    "text": "which simply looks to see is-- we'll say it's 1 if CATE or\nyour estimate of CATE for x",
    "start": "517690",
    "end": "530860"
  },
  {
    "text": "is positive and 0 otherwise.",
    "start": "530860",
    "end": "536290"
  },
  {
    "text": "Just remind you, the way that\nwe got the estimate of CATE",
    "start": "536290",
    "end": "542490"
  },
  {
    "text": "for an individual x\nwas just by looking at f of x comma 1\nminus f of x comma 0.",
    "start": "542490",
    "end": "551670"
  },
  {
    "start": "551670",
    "end": "570620"
  },
  {
    "start": "569000",
    "end": "2168000"
  },
  {
    "text": "So if we have a policy-- so now we're going to start\nthinking about policies in the context of\ncausal inference,",
    "start": "570620",
    "end": "576410"
  },
  {
    "text": "just like we were doing\nin reinforcement learning. And I want us to think through\nwhat would the analogous value",
    "start": "576410",
    "end": "583610"
  },
  {
    "text": "of the policy be? How good is that policy?",
    "start": "583610",
    "end": "589464"
  },
  {
    "text": "It could be another\npolicy, but right now I'm assuming I'm just going\nto focus on this policy that I show up here. ",
    "start": "589465",
    "end": "596690"
  },
  {
    "text": "Well, one approach\nto try to evaluate how good that policy is, is\nexactly analogous to what we",
    "start": "596690",
    "end": "602350"
  },
  {
    "text": "did in reinforcement learning. In essence, what\nwe're going to say is we evaluate the\nquality of the policy",
    "start": "602350",
    "end": "608470"
  },
  {
    "text": "by summing over your\nempirical data of pi of xi.",
    "start": "608470",
    "end": "622420"
  },
  {
    "text": "So this is going to be 1 if the\npolicy says to give treatment 1",
    "start": "622420",
    "end": "628459"
  },
  {
    "text": "to individual xi. In that case, we say that\nthe value is f of x comma 1.",
    "start": "628460",
    "end": "637730"
  },
  {
    "text": "Or if you gave the second-- if the policy would\ngive treatment 0,",
    "start": "637730",
    "end": "645540"
  },
  {
    "text": "the value of the policy on\nthat individual is 1 minus pi of x times f of x comma 0.",
    "start": "645540",
    "end": "653280"
  },
  {
    "text": " So I'm going to call this\nsort of an empirical estimate",
    "start": "653280",
    "end": "664250"
  },
  {
    "text": "of what you should think about\nas the reward for a policy pi.",
    "start": "664250",
    "end": "669680"
  },
  {
    "start": "669680",
    "end": "674690"
  },
  {
    "text": "And it's exactly analogous\nto the estimate of v of pie",
    "start": "674690",
    "end": "680440"
  },
  {
    "text": "that you would get from a\nreinforcement learning context. But now we're talking\nabout policies explicitly.",
    "start": "680440",
    "end": "688089"
  },
  {
    "text": "So let's try to dig\ndown a little bit deeper and think about what\nthis is actually saying. ",
    "start": "688090",
    "end": "694040"
  },
  {
    "text": "Imagine the story where you\njust have a single covariate x.",
    "start": "694040",
    "end": "700430"
  },
  {
    "text": "We'll think about x as being,\nlet's say, the patient's age.",
    "start": "700430",
    "end": "705440"
  },
  {
    "text": "And unfortunately there's\njust one color here. But I'll do my best with that.",
    "start": "705440",
    "end": "712100"
  },
  {
    "text": "And imagine that the\npotential outcome y0 as a function of\nthe patient's age x",
    "start": "712100",
    "end": "723279"
  },
  {
    "text": "looks like this. Now imagine that the\nother potential outcome y1 looked like that.",
    "start": "723280",
    "end": "734060"
  },
  {
    "text": "So I'll call this the\ny1 potential outcome. ",
    "start": "734060",
    "end": "741610"
  },
  {
    "text": "Suppose now that the policy\nthat we're defining is this. So we're going to\ngive treatment one",
    "start": "741610",
    "end": "747550"
  },
  {
    "text": "if the condition of our\ntreatment effect is positive and 0 otherwise. I want everyone to draw what\nthe value of that policy",
    "start": "747550",
    "end": "756320"
  },
  {
    "text": "is on a piece of paper. It's going to be-- ",
    "start": "756320",
    "end": "764027"
  },
  {
    "text": "I'm sorry-- I want everyone\nto write on a piece of paper what the value of the policy\nwould be for each individual.",
    "start": "764027",
    "end": "769630"
  },
  {
    "text": "So it's going to\nbe a function of x. ",
    "start": "769630",
    "end": "775649"
  },
  {
    "text": "And now I want it to be-- I'm looking for y of pi of x.",
    "start": "775650",
    "end": "783690"
  },
  {
    "text": "So I'm looking for\nyou to draw that plot.  And feel free to talk\nto your neighbor.",
    "start": "783690",
    "end": "790150"
  },
  {
    "text": " In fact, I encourage you\nto talk to your neighbor.",
    "start": "790150",
    "end": "795584"
  },
  {
    "text": "[SIDE CONVERSATION] ",
    "start": "795584",
    "end": "802792"
  },
  {
    "text": "Just to try to connect\nthis a little bit better to what I have up here, I'm\ngoing to assume that f--",
    "start": "802792",
    "end": "808304"
  },
  {
    "text": "this is f of x1,\nand this is f of x0. ",
    "start": "808304",
    "end": "819440"
  },
  {
    "text": "All right. Any guesses?  What does this plot look like?",
    "start": "819440",
    "end": "826613"
  },
  {
    "text": "Someone who hasn't spoken in\nthe last one week and a half, if possible. ",
    "start": "826613",
    "end": "838870"
  },
  {
    "text": "Yeah? AUDIENCE: Does it take like\nthe max of the functions at all point, like,\nit would be y0 up until they intersect\nand then y1 afterward?",
    "start": "838870",
    "end": "846200"
  },
  {
    "text": "DAVID SONTAG: So it would\nbe something like this until the intersection point. AUDIENCE: Yeah. DAVID SONTAG: And then\nlike that afterwards.",
    "start": "846200",
    "end": "852050"
  },
  {
    "text": "Yeah. That's exactly\nwhat I'm going for. And let's try to\nthink through why is",
    "start": "852050",
    "end": "857350"
  },
  {
    "text": "that the value of the policy? Well, here the CATE,\nwhich is looking",
    "start": "857350",
    "end": "865259"
  },
  {
    "text": "at a difference between\nthese two lines as negative-- so for every x up to\nthis crossing point,",
    "start": "865260",
    "end": "873600"
  },
  {
    "text": "the policy that we've\ndefined over there is going to perform action--",
    "start": "873600",
    "end": "879645"
  },
  {
    "text": " wait. Am I drawing this correctly?",
    "start": "879645",
    "end": "885459"
  },
  {
    "text": "Maybe it's actually\nthe opposite, right? This should be doing action one. ",
    "start": "885460",
    "end": "894100"
  },
  {
    "text": "Here. OK. So here the CATE is negative.",
    "start": "894100",
    "end": "900250"
  },
  {
    "text": "And so by my definition, the\naction performed is action 0. And so the value of the\npolicy is actually this one.",
    "start": "900250",
    "end": "907828"
  },
  {
    "text": "[INTERPOSING VOICES] DAVID SONTAG: Oh. Wait. Oh, good. [INAUDIBLE]",
    "start": "907828",
    "end": "913925"
  },
  {
    "text": "Because this is the\ngraph I have in my notes. Oh, good. OK. I was getting worried.",
    "start": "913925",
    "end": "919740"
  },
  {
    "text": "OK. So it's this action, all the\nway up until you get over here. And then over here, now the\nCATE suddenly becomes positive.",
    "start": "919740",
    "end": "928890"
  },
  {
    "text": "And so the action chosen is 1.",
    "start": "928890",
    "end": "934280"
  },
  {
    "text": "And so the value of\nthat policy is y1.",
    "start": "934280",
    "end": "941570"
  },
  {
    "text": "So one could write this a\nlittle bit differently for-- ",
    "start": "941570",
    "end": "950260"
  },
  {
    "text": "in the case of just\ntwo policies, and now I'm going to write this in a\nway that it's really clear. In the case of just\ntwo actions, one",
    "start": "950260",
    "end": "958500"
  },
  {
    "text": "could write this\nequivalently as an average",
    "start": "958500",
    "end": "964970"
  },
  {
    "text": "over the data points of\nthe maximum of fx comma 0",
    "start": "964970",
    "end": "974860"
  },
  {
    "text": "and f of x comma 1. And this simplification turning\nthis formula into this formula",
    "start": "974860",
    "end": "985660"
  },
  {
    "text": "is making the\nassumption that the pi that we're being evaluated\non is precisely this pi.",
    "start": "985660",
    "end": "991100"
  },
  {
    "text": "So this simplification\nis only for that pi. For another policy, which is not\nlooking at CATE or for example,",
    "start": "991100",
    "end": "997120"
  },
  {
    "text": "which might threshold\nCATE at a gamma, it wouldn't quite be this. It would be something else.",
    "start": "997120",
    "end": "1003279"
  },
  {
    "text": "But I've gone a\nstep further here. So what I've shown\nyou right here is not the average value but\nsort of individual values.",
    "start": "1003280",
    "end": "1010270"
  },
  {
    "text": "I have shown you\nthe max function. But what this is\nactually looking at is the expected reward, which\nis now averaging across all x.",
    "start": "1010270",
    "end": "1020390"
  },
  {
    "text": "So to truly draw a connection\nbetween this plot we're drawing and the average reward\nof that policy, what",
    "start": "1020390",
    "end": "1027356"
  },
  {
    "text": "we should be looking\nat is the average of these two functions, which is\nwe'll say something like that.",
    "start": "1027357",
    "end": "1037049"
  },
  {
    "text": "And that value is\nthe expected reward. Now, this all goes to show\nthat the expected reward",
    "start": "1037050",
    "end": "1046740"
  },
  {
    "text": "of this policy is not a\nquantity that we've considered in the previous\nlectures, at least",
    "start": "1046740",
    "end": "1052210"
  },
  {
    "text": "not in the previous lectures\nin causal inference. This is not the same as\nthe average treatment effect, for example.",
    "start": "1052210",
    "end": "1057315"
  },
  {
    "start": "1057315",
    "end": "1065840"
  },
  {
    "text": "So I've just given you\none way to think through, number one, what is\nthe policy that you",
    "start": "1065840",
    "end": "1071770"
  },
  {
    "text": "might want to derive when\nyou're doing causal inference? And number two, what\nis one way to estimate",
    "start": "1071770",
    "end": "1078759"
  },
  {
    "text": "the value of that\npolicy, which goes through the process of\nestimating potential outcomes",
    "start": "1078760",
    "end": "1087070"
  },
  {
    "text": "via covariate adjustment? But we might wonder,\njust like when",
    "start": "1087070",
    "end": "1092610"
  },
  {
    "text": "we talked about in\ncausal inference where I said there are two\napproaches or more than two, but we focused on two,\nusing covariate adjustment",
    "start": "1092610",
    "end": "1099159"
  },
  {
    "text": "and doing inverse\npropensity score weighting, you might wonder is\nthere another approach to this problem all together?",
    "start": "1099160",
    "end": "1106422"
  },
  {
    "text": "Is there an approach\nwhich wouldn't have had to go\nthrough estimating the potential outcomes?",
    "start": "1106422",
    "end": "1112242"
  },
  {
    "text": "And that's what\nI'll spend the rest of this third of the lecture\nfocused talking about.",
    "start": "1112242",
    "end": "1118960"
  },
  {
    "text": "And so to help you\npage this back in, remember that we derived\nin last Thursday's lecture",
    "start": "1118960",
    "end": "1128690"
  },
  {
    "text": "an estimator for the average\ntreatment effect, which was 1 over n times the\nsum over data points",
    "start": "1128690",
    "end": "1138230"
  },
  {
    "text": "that got treatment 1 of yi, the\nobserved outcome for that data",
    "start": "1138230",
    "end": "1149120"
  },
  {
    "text": "point, divided by\nthe propensity score, which I'm just going\nto write as ei.",
    "start": "1149120",
    "end": "1155660"
  },
  {
    "text": "So ei is equal to\nthe probability of observing t equals\n1 given the data point",
    "start": "1155660",
    "end": "1170510"
  },
  {
    "text": "xi minus a sum over data\npoint i such that ti equals",
    "start": "1170510",
    "end": "1181510"
  },
  {
    "text": "0 of yi divided by 1 minus ei.",
    "start": "1181510",
    "end": "1186540"
  },
  {
    "text": " And by the way, there was\na lot of confusion in class why do I have a 1 over\nn here, a 1 over n here,",
    "start": "1186540",
    "end": "1193810"
  },
  {
    "text": "but right now I just\ntook it out all together, and not 1 over the\nnumber of positive points",
    "start": "1193810",
    "end": "1199840"
  },
  {
    "text": "and 1 over the number\nof 0 data points. And I expanded the derivation\nthat I gave in class,",
    "start": "1199840",
    "end": "1206770"
  },
  {
    "text": "and I posted new slides\nonline after class. So if you're curious about\nthat, go to those slides",
    "start": "1206770",
    "end": "1211840"
  },
  {
    "text": "and look at the derivation. So in a very\nanalogous way now, I'm",
    "start": "1211840",
    "end": "1217850"
  },
  {
    "text": "going to give you\na new estimator for this same quantity\nthat I had over here, the expected reward of a policy.",
    "start": "1217850",
    "end": "1225179"
  },
  {
    "text": "Notice that this estimator here,\nit made sense for any policy.",
    "start": "1225180",
    "end": "1230520"
  },
  {
    "text": "It didn't have to be the\npolicy which looked at, is CATE just greater\nthan 0 or not?",
    "start": "1230520",
    "end": "1236150"
  },
  {
    "text": "This held for any policy. The simplification\nI gave was only in this particular setting.",
    "start": "1236150",
    "end": "1242107"
  },
  {
    "text": "I'm going to give you\nnow another estimator for the average value\nof a policy, which doesn't go through estimating\npotential outcomes at all.",
    "start": "1242108",
    "end": "1251040"
  },
  {
    "text": "Analogous to this is\njust going to make use of the propensity scores.",
    "start": "1251040",
    "end": "1256690"
  },
  {
    "text": "And I'll call it R hat. Now I'm going to put\na superscript IPW",
    "start": "1256690",
    "end": "1262169"
  },
  {
    "text": "for inverse propensity weighted. And it's a function of\npi, and it's given to you by the following formula--",
    "start": "1262170",
    "end": "1268200"
  },
  {
    "text": "1 over n sum over the data\npoints of an indicator",
    "start": "1268200",
    "end": "1274350"
  },
  {
    "text": "function for if the\ntreatment, which was actually given to the i-th\npatient, is equal to what",
    "start": "1274350",
    "end": "1283140"
  },
  {
    "text": "the policy would have done\nbefore the i-th patient. And by the way, here\nI'm assuming that pi",
    "start": "1283140",
    "end": "1290040"
  },
  {
    "text": "is a deterministic function. So the policy says\nfor this patient, you should do this treatment.",
    "start": "1290040",
    "end": "1296760"
  },
  {
    "text": "So we're going to\nlook at just the data points for which the\nobserved treatment is consistent with what\nthe policy would",
    "start": "1296760",
    "end": "1305054"
  },
  {
    "text": "have done for that patient. And this indicator\nfunction is 0 otherwise. And we're going to divide it by\nthe probability of ti given xi.",
    "start": "1305055",
    "end": "1322750"
  },
  {
    "text": "So the way I'm writing this,\nby the way, is very general. So this formula will hold for\nnonbinary treatments as well.",
    "start": "1322750",
    "end": "1330653"
  },
  {
    "text": "And that's one of the\nreally nice things about thinking about\npolicies, which is whereas when talking about\naverage treatment effect,",
    "start": "1330653",
    "end": "1339367"
  },
  {
    "text": "average treatment effect\nsort of makes sense in the comparative sense,\ncomparing one to another.",
    "start": "1339367",
    "end": "1344500"
  },
  {
    "text": "But when we talk about\nhow good is a policy, it's not a comparative\nstatement at all.",
    "start": "1344500",
    "end": "1349934"
  },
  {
    "text": "The policy does\nsomething for everyone. You could ask, well, what is the\naverage value of the outcomes that you get for those\nactions that we're",
    "start": "1349935",
    "end": "1355870"
  },
  {
    "text": "taking for those individuals? So that's why I'm writing a\nslightly more general fashion already here.",
    "start": "1355870",
    "end": "1361030"
  },
  {
    "text": "Times yi obviously. So this is now a new estimator.",
    "start": "1361030",
    "end": "1366667"
  },
  {
    "text": "I'm not going to derive\nit for you in class, but the derivation is\nvery similar to what we did last week when we tried\nto drive the average treatment",
    "start": "1366667",
    "end": "1372930"
  },
  {
    "text": "effect. And the critical point is we're\ndividing by that propensity",
    "start": "1372930",
    "end": "1378280"
  },
  {
    "text": "score, just like\nwe did over there. ",
    "start": "1378280",
    "end": "1384390"
  },
  {
    "text": "So this, if all of the\nassumptions made sense,",
    "start": "1384390",
    "end": "1389890"
  },
  {
    "text": "you had infinite\ndata, should give you exactly the same\nestimate as this.",
    "start": "1389890",
    "end": "1396280"
  },
  {
    "text": "But here, you're not estimating\npotential outcomes at all. So you never have to try to\nimpute the counterfactuals.",
    "start": "1396280",
    "end": "1404900"
  },
  {
    "text": "Here, all it relies\non knowing is that you have the\npropensity scores",
    "start": "1404900",
    "end": "1410110"
  },
  {
    "text": "for each of the data\npoints in your training set or in a data set. So for example,\nthis opens the door",
    "start": "1410110",
    "end": "1416380"
  },
  {
    "text": "to tons of new\nexciting directions. Imagine that you had a very\nlarge observational data set.",
    "start": "1416380",
    "end": "1424610"
  },
  {
    "text": "And you learned\na policy from it. For example, you might have\ndone covariate adjustment",
    "start": "1424610",
    "end": "1433250"
  },
  {
    "text": "and then said, OK, based\non covariate adjustment, this is my new policy.",
    "start": "1433250",
    "end": "1438970"
  },
  {
    "text": "So you might have gotten\nit via that approach. Now you want to know\nhow good is that.",
    "start": "1438970",
    "end": "1444260"
  },
  {
    "text": "Well, suppose that you then\nrun a randomized control trial. And then you run a\nrandomized control trial,",
    "start": "1444260",
    "end": "1451030"
  },
  {
    "text": "you have 100 people, maybe 200\npeople, and so not that many. So not nearly enough\npeople to have",
    "start": "1451030",
    "end": "1457090"
  },
  {
    "text": "actually estimated\nyour policy alone. You might have needed thousands\nor millions of individuals to estimate your policy.",
    "start": "1457090",
    "end": "1463197"
  },
  {
    "text": "Now you're only going to\nhave a couple individuals that you could actually afford\nto do a randomized control trial on.",
    "start": "1463197",
    "end": "1468980"
  },
  {
    "text": "For those people,\nbecause you're flipping a coin for which treatment\nthey're going to get,",
    "start": "1468980",
    "end": "1476210"
  },
  {
    "text": "suppose that were\nin a binary setting where the only two\ntreatments, then this value is always 1/2 1/2.",
    "start": "1476210",
    "end": "1482899"
  },
  {
    "text": "And what I'm giving\nyou here is going to be an unbiased estimate\nof how good that policy is,",
    "start": "1482900",
    "end": "1491129"
  },
  {
    "text": "which one can now estimate using\nthat randomized control trial. ",
    "start": "1491130",
    "end": "1497350"
  },
  {
    "text": "Now, this also might\nlead you to think",
    "start": "1497350",
    "end": "1503299"
  },
  {
    "text": "through the question of,\nwell, rather than estimating the policy through--",
    "start": "1503300",
    "end": "1510230"
  },
  {
    "text": "rather than obtaining a policy\nthrough the lens of optimizing CATE, of figuring\nhow to estimate CATE,",
    "start": "1510230",
    "end": "1517880"
  },
  {
    "text": "maybe we could have\nskipped that all together. For example, suppose that we had\nthat randomized control trial",
    "start": "1517880",
    "end": "1526170"
  },
  {
    "text": "data. Now imagine that rather\nthan 100 individuals, you had a really large\nrandomized control trial",
    "start": "1526170",
    "end": "1532500"
  },
  {
    "text": "with 10,000 individuals in it. This now opens the door\nto thinking about directly",
    "start": "1532500",
    "end": "1541010"
  },
  {
    "text": "maximizing or minimizing,\ndepending whether you want this to be large or small,\npi with respect",
    "start": "1541010",
    "end": "1546590"
  },
  {
    "text": "to this quantity, which\ncompletely bypasses the goal of estimating the\ncondition of average treatment",
    "start": "1546590",
    "end": "1554299"
  },
  {
    "text": "effect. And you'll notice how\nthis looks exactly like a classification problem.",
    "start": "1554300",
    "end": "1560390"
  },
  {
    "text": "This quantity here looks\nexactly like a 0 1 loss. And the only difference\nis that you're",
    "start": "1560390",
    "end": "1566279"
  },
  {
    "text": "weighting each of\nthe data points by this inverse propensity.",
    "start": "1566280",
    "end": "1572640"
  },
  {
    "text": "So one can reduce the\nproblem of actually finding an optimal policy here to that\nof a weighted classification",
    "start": "1572640",
    "end": "1581250"
  },
  {
    "text": "problem, in the case of a\ndiscrete set of treatments. ",
    "start": "1581250",
    "end": "1588370"
  },
  {
    "text": "There are two big caveats\nto that line of thinking. The first major\ncaveat is that you",
    "start": "1588370",
    "end": "1596790"
  },
  {
    "text": "have to know these\npropensity scores.  And so if you have data coming\nfrom randomized control trial,",
    "start": "1596790",
    "end": "1606700"
  },
  {
    "text": "you will know this\npropensity scores or if you have, for\nexample, some control over the data\ngeneration process.",
    "start": "1606700",
    "end": "1614290"
  },
  {
    "text": "For example, if you\nare an ad company and you get to choose which\nad to show to your customers,",
    "start": "1614290",
    "end": "1621860"
  },
  {
    "text": "then you look to see\nwho clicks on what, you might know what that policy\nwas that was showing things. In that case, you might exactly\nknow the propensity scores.",
    "start": "1621860",
    "end": "1629890"
  },
  {
    "text": "In health care, other than\nin randomized control trials, we typically don't\nknow this value. So we either have to have a\nlarge enough randomized control",
    "start": "1629890",
    "end": "1637330"
  },
  {
    "text": "trial that we won't over-fit\nby trying to directly minimize this or we have to work within\nan observational data setting.",
    "start": "1637330",
    "end": "1647740"
  },
  {
    "text": "But we have to estimate the\npropensity scores directly. So you would then have\na two-step procedure,",
    "start": "1647740",
    "end": "1652750"
  },
  {
    "text": "where first you estimate these\npropensity scores, for example, by doing logistic regression. And then you attempt\nto maximize or minimize",
    "start": "1652750",
    "end": "1660640"
  },
  {
    "text": "this quantity in order to\nfind the optimal policy. ",
    "start": "1660640",
    "end": "1665890"
  },
  {
    "text": "And that has a\nlot of challenges, because this quantity\nshown in the very bottom",
    "start": "1665890",
    "end": "1671370"
  },
  {
    "text": "here could be really\nsmall or really large in an observational data set\ndue to these issues of having",
    "start": "1671370",
    "end": "1678480"
  },
  {
    "text": "very small overlap\nbetween your treatments. And this being very\nsmall implies then",
    "start": "1678480",
    "end": "1685200"
  },
  {
    "text": "that the variant of this\nestimator is very, very large. And so when one wants to\nuse an approach like this,",
    "start": "1685200",
    "end": "1693570"
  },
  {
    "text": "similar to when one wants to\nuse an average treatment effect estimator, and when you're\nestimating these propensities,",
    "start": "1693570",
    "end": "1699870"
  },
  {
    "text": "often you might\nneed to do things like clipping of the\npropensity scores in order to prevent the\nvariants from being too large.",
    "start": "1699870",
    "end": "1706110"
  },
  {
    "text": "That then, however, leads to\na biased estimate typically.",
    "start": "1706110",
    "end": "1711420"
  },
  {
    "text": "I wanted to give you a\ncouple of references here. So one is Swaminathan\nand Joachims,",
    "start": "1711420",
    "end": "1725530"
  },
  {
    "text": "J-O-A-C-H-I-M-S ACML 2015.",
    "start": "1725530",
    "end": "1735250"
  },
  {
    "text": "In that paper, they\ntackle this question. They focus on the setting\nwhere the propensity scores are",
    "start": "1735250",
    "end": "1740290"
  },
  {
    "text": "known, such as do it half from\na randomized controlled trial. And they recognize\nthat you might",
    "start": "1740290",
    "end": "1746380"
  },
  {
    "text": "decide that you prefer something\nlike a biased estimator because of the fact that these\npropensity scores could be really small.",
    "start": "1746380",
    "end": "1752650"
  },
  {
    "text": "And so they use some\ngeneralization results from the machine learning\ntheory community in order",
    "start": "1752650",
    "end": "1758320"
  },
  {
    "text": "to try to control the\nvariants of the estimator as a function of these\npropensity scores.",
    "start": "1758320",
    "end": "1765440"
  },
  {
    "text": "And they then learn,\ndirectly minimize the policy which is what they\ncall counterfactual regret",
    "start": "1765440",
    "end": "1770460"
  },
  {
    "text": "minimization, in\norder to allow one to generalize as\nbest as possible",
    "start": "1770460",
    "end": "1776630"
  },
  {
    "text": "from the small amount of data\nyou might have available. A second reference that\nI want to give just",
    "start": "1776630",
    "end": "1782110"
  },
  {
    "text": "to point you into this\nliterature, if you're interested, is by Nathan\nKallus and his student,",
    "start": "1782110",
    "end": "1789030"
  },
  {
    "text": "I believe Angela Zhou,\nfrom NeurIPS 2018.",
    "start": "1789030",
    "end": "1795960"
  },
  {
    "text": "And that was a paper which was\none of the optional readings for last Thursday's class. Now, that paper they\nalso start from something",
    "start": "1795960",
    "end": "1802320"
  },
  {
    "text": "like this, from\nthis perspective. And they say that,\noh, now that we're working in this\nframework, one could",
    "start": "1802320",
    "end": "1809490"
  },
  {
    "text": "think about what happens\nif you have actually unobserved confounding.",
    "start": "1809490",
    "end": "1814820"
  },
  {
    "text": "So there, you might not actually\nknow the true propensity scores, because there are\nunobserved confounders",
    "start": "1814820",
    "end": "1820720"
  },
  {
    "text": "that you don't observe. And that you can think about\ntrying to bound how wrong",
    "start": "1820720",
    "end": "1827380"
  },
  {
    "text": "your estimator can be as\na function of how much you don't know this quantity. And they show that\nwhen you try to--",
    "start": "1827380",
    "end": "1834430"
  },
  {
    "text": "if you think about having\nsome backup strategy, like if your goal is to find\na new policy which performs",
    "start": "1834430",
    "end": "1841480"
  },
  {
    "text": "as best as possible with\nrespect to an old policy,",
    "start": "1841480",
    "end": "1846730"
  },
  {
    "text": "then it gives you a\nreally elegant framework for trying to think about a\nrobust optimization of this, even taking into consideration\nthe fact that there might",
    "start": "1846730",
    "end": "1853570"
  },
  {
    "text": "be unobserved confounding. And that works also\nin this framework.",
    "start": "1853570",
    "end": "1859040"
  },
  {
    "text": "So I'm nearly done now.  I just want to now\nfinish with a thought,",
    "start": "1859040",
    "end": "1865110"
  },
  {
    "text": "can we do the same thing\nfor policies learned by reinforcement learning? So now that we've sort\nof built up this language",
    "start": "1865110",
    "end": "1872399"
  },
  {
    "text": "that's returned\nto the RL setting. And there one can\nshow that you can",
    "start": "1872400",
    "end": "1879030"
  },
  {
    "text": "get a similar estimate\nfor the value of a policy by summing over your\nobserved sequences,",
    "start": "1879030",
    "end": "1887520"
  },
  {
    "text": "summing over the time steps\nof that sequence of the reward",
    "start": "1887520",
    "end": "1895080"
  },
  {
    "text": "observed at that time step times\na ratio of probabilities, which",
    "start": "1895080",
    "end": "1902590"
  },
  {
    "text": "is going from the\nfirst time step up to time little t\nof the probability",
    "start": "1902590",
    "end": "1913429"
  },
  {
    "text": "that you would actually take\nthe observed action t prime, given that you are in the\nobserved state t prime, divided",
    "start": "1913430",
    "end": "1922760"
  },
  {
    "text": "by the probability-- this is the analogy\nof the propensity",
    "start": "1922760",
    "end": "1928200"
  },
  {
    "text": "score, the probability under\nthe data generating process-- of seeing action a given that\nyou are in state t prime.",
    "start": "1928200",
    "end": "1941190"
  },
  {
    "text": "So if, as we\ndiscussed there, you had a deterministic\npolicy, then this pi,",
    "start": "1941190",
    "end": "1947940"
  },
  {
    "text": "it would just be\na delta function. And so this would\njust be looking at--",
    "start": "1947940",
    "end": "1954030"
  },
  {
    "text": "this estimator would\nonly be looking at sequences where the precise\nsequence of actions taken",
    "start": "1954030",
    "end": "1960960"
  },
  {
    "text": "are identical to the\nprecise sequence of actions that the policy\nwould have taken.",
    "start": "1960960",
    "end": "1967789"
  },
  {
    "text": "And the difference here\nis that now instead of having a single\npropensity score, one has a product of these\npropensity scores corresponding",
    "start": "1967790",
    "end": "1976010"
  },
  {
    "text": "to the propensity of\nobserving that action given the corresponding state at\neach point along the sequence.",
    "start": "1976010",
    "end": "1984240"
  },
  {
    "text": "And so this is nice,\nbecause this gives you one way to do what's called\noff-policy evaluation.",
    "start": "1984240",
    "end": "1989450"
  },
  {
    "start": "1989450",
    "end": "1995570"
  },
  {
    "text": "And this is an\nestimator, which is",
    "start": "1995570",
    "end": "2000639"
  },
  {
    "text": "completely analogous\nto the estimator that we got from Q learning. So if all assumptions\nwere correct,",
    "start": "2000640",
    "end": "2006670"
  },
  {
    "text": "and you had a lot of\ndata, then those two should give you precisely\nthe same answer.",
    "start": "2006670",
    "end": "2012980"
  },
  {
    "text": "But here, like in the\ncausal inference setting, we are not making the\nassumption that we can",
    "start": "2012980",
    "end": "2018170"
  },
  {
    "text": "do covariate adjustment well. Or said differently,\nwe're not assuming that we can fit the Q function well.",
    "start": "2018170",
    "end": "2025450"
  },
  {
    "text": "And this is now,\njust like there, based on the assumption\nthat we have the ability",
    "start": "2025450",
    "end": "2030640"
  },
  {
    "text": "to really accurately know what\nthe propensity scores are. So it now gives you an\nalternative approach",
    "start": "2030640",
    "end": "2035650"
  },
  {
    "text": "to do evaluation. And you could\nthink about looking at the robustness\nof your estimates from these two\ndifferent estimators.",
    "start": "2035650",
    "end": "2044340"
  },
  {
    "text": "And this is the most\nnaive of the estimators. There are many ways to try\nto make this better, such as",
    "start": "2044340",
    "end": "2052260"
  },
  {
    "text": "by doing w robust estimators. And if you want to\nlearn more, I recommend",
    "start": "2052260",
    "end": "2058739"
  },
  {
    "text": "reading this paper by Thomas\nand Emma Brunskill in ICML 2016.",
    "start": "2058739",
    "end": "2070169"
  },
  {
    "text": "And with that, I want Barbra\nto come up and get set up. And we're going to transition\nto the next part of the lecture.",
    "start": "2070170",
    "end": "2075693"
  },
  {
    "text": " Yes. AUDIENCE: Why do we sum\nover t and take the project",
    "start": "2075693",
    "end": "2082550"
  },
  {
    "text": "across all t? DAVID SONTAG: One easy\nway to think about this is suppose that you only had a\nreward of the last time step.",
    "start": "2082550",
    "end": "2089770"
  },
  {
    "text": "If you only had a reward\nof the last time step, then you wouldn't\nhave this sum over t, because the rewards in the\nearlier steps would be 0.",
    "start": "2089770",
    "end": "2095460"
  },
  {
    "text": "You would just have that\nproduct going from 0 up to capital T of last time step. The reason why you have\nit up to at each time step",
    "start": "2095460",
    "end": "2103340"
  },
  {
    "text": "is because one wants to be\nable to appropriately weigh the likelihood of seeing that\nreward at that point in time.",
    "start": "2103340",
    "end": "2111150"
  },
  {
    "text": "One could rewrite\nthis in other ways. I want to hold other\nquestions, because this part of the lecture is going to\nbe much more interesting",
    "start": "2111150",
    "end": "2117045"
  },
  {
    "text": "than my part of the lecture. And with that, I want\nintroduce Barbra. Barbra, I first met her\nwhen she invited me to give",
    "start": "2117045",
    "end": "2124279"
  },
  {
    "text": "a talk in her class last year. She's an instructor at\nHarvard Medical School--",
    "start": "2124280",
    "end": "2133550"
  },
  {
    "text": "or School of Public Health. She recently finished\nher PhD in 2018.",
    "start": "2133550",
    "end": "2139530"
  },
  {
    "text": "And her PhD looked\nat many questions related to the themes of\nthe last couple of weeks.",
    "start": "2139530",
    "end": "2145910"
  },
  {
    "text": "Since that time, in addition\ncontinuing her research, she's been really leading the\nway in creating data science",
    "start": "2145910",
    "end": "2152000"
  },
  {
    "text": "curriculum over at Harvard. So please take it away. BARBRA DICKERMAN:\nThank you so much for the introduction, David.",
    "start": "2152000",
    "end": "2157869"
  },
  {
    "text": "I'm very happy to be here\nto share some of my work on evaluating dynamic\ntreatment strategies,",
    "start": "2157870",
    "end": "2164420"
  },
  {
    "text": "which you've been talking about\nover the past few lectures. So my goals for\ntoday, I'm just going",
    "start": "2164420",
    "end": "2171130"
  },
  {
    "start": "2168000",
    "end": "2229000"
  },
  {
    "text": "to breeze over defining\ndynamic treatment strategies, as you're already\nfamiliar with it.",
    "start": "2171130",
    "end": "2176220"
  },
  {
    "text": "But I would like\nto touch on when we need a special class of\nmethods called g-methods.",
    "start": "2176220",
    "end": "2182760"
  },
  {
    "text": "And then we'll talk about\ntwo different applications, different analyses, that\nhave focused on evaluating",
    "start": "2182760",
    "end": "2188839"
  },
  {
    "text": "dynamic treatment strategies. So the first will\nbe an application of the parametric\ng-formula, which",
    "start": "2188840",
    "end": "2196010"
  },
  {
    "text": "is a powerful g-method\nto cancer research. And so the goal\nhere is to give you",
    "start": "2196010",
    "end": "2202069"
  },
  {
    "text": "my causal inference\nperspective on how we think about this task of\nsequential decision making",
    "start": "2202070",
    "end": "2208099"
  },
  {
    "text": "and then with\nwhatever time remains, we'll be discussing a recent\npublication on the AI clinician",
    "start": "2208100",
    "end": "2215029"
  },
  {
    "text": "to talk through the\nreinforcement learning perspective. So I think it'll be a really\ninteresting discussion, where",
    "start": "2215030",
    "end": "2220040"
  },
  {
    "text": "we can share these\nperspectives, talk about the relative strengths\nand limitations as well.",
    "start": "2220040",
    "end": "2226200"
  },
  {
    "text": "And please stop me if\nyou have any questions. So you already know this.",
    "start": "2226200",
    "end": "2231420"
  },
  {
    "start": "2229000",
    "end": "2355000"
  },
  {
    "text": "When it comes to\ntreatment strategies, there's three main types. There's point\ninterventions happening at a single point in time.",
    "start": "2231420",
    "end": "2236840"
  },
  {
    "text": "There's sustained interventions\nhappening over time. When it comes to clinical\ncare, this is often what we're most interested in.",
    "start": "2236840",
    "end": "2243960"
  },
  {
    "text": "Within that, there\nare static strategies, which are constant over time. And then there's\ndynamic strategies,",
    "start": "2243960",
    "end": "2249810"
  },
  {
    "text": "which we're going to focus on. And these differ in that\nthe intervention over time",
    "start": "2249810",
    "end": "2254970"
  },
  {
    "text": "depends on evolving\ncharacteristics. So for example, initiate\ntreatment at baseline",
    "start": "2254970",
    "end": "2261330"
  },
  {
    "text": "and continue it over follow\nup until a contraindication occurs, at which point\nyou may stop treatment",
    "start": "2261330",
    "end": "2267750"
  },
  {
    "text": "and decide with your\ndoctor whether you're going to switch to an\nalternate treatment. You would still be\nadhering to that strategy,",
    "start": "2267750",
    "end": "2274770"
  },
  {
    "text": "even though you quit. The comparison here being do\nnot initiate treatment over follow up, likewise unless\nan indication occurs,",
    "start": "2274770",
    "end": "2282880"
  },
  {
    "text": "at which point you may\nstart treatment and still be adhering to the strategy. So we're focusing on\nthese because they're",
    "start": "2282880",
    "end": "2287940"
  },
  {
    "text": "the most clinically relevant. And so clinicians encounter\nthese every day in practice.",
    "start": "2287940",
    "end": "2294859"
  },
  {
    "text": "So when they're making\na recommendation to their patient about a\nprevention intervention,",
    "start": "2294860",
    "end": "2300410"
  },
  {
    "text": "they're going to be\ntaking into consideration the patient's evolving\ncomorbidities. Or when they're deciding\nthe next screening interval,",
    "start": "2300410",
    "end": "2307280"
  },
  {
    "text": "they'll consider the previous\nresult from the last screening test when deciding that. Likewise for treatment, deciding\nwhether to keep the patient",
    "start": "2307280",
    "end": "2315140"
  },
  {
    "text": "on treatment or not. Is the patient\nhaving any changes in symptoms or lab values\nthat may reflect toxicity?",
    "start": "2315140",
    "end": "2323210"
  },
  {
    "text": "So one thing to note\nis that while many of the strategies that you\nmay see in clinical guidelines",
    "start": "2323210",
    "end": "2329360"
  },
  {
    "text": "and in clinical practice\nare dynamic strategies, these may not be the\noptimal strategies.",
    "start": "2329360",
    "end": "2336069"
  },
  {
    "text": "So maybe what we're\nrecommending and doing is not optimal for patients. However, the optimal\nstrategies will",
    "start": "2336070",
    "end": "2342020"
  },
  {
    "text": "be dynamic in some\nway, in that they will be adapting to\nindividuals' unique and evolving",
    "start": "2342020",
    "end": "2348859"
  },
  {
    "text": "characteristics. So that's why we\ncare about them. So what's the problem?",
    "start": "2348860",
    "end": "2356270"
  },
  {
    "start": "2355000",
    "end": "2478000"
  },
  {
    "text": "So one problem\ndeals with something called treatment\nconfounder feedback, which you may have spoken\nabout in this class.",
    "start": "2356270",
    "end": "2362510"
  },
  {
    "text": "So conventional statistical\nmethods cannot appropriately compare dynamic treatment\nstrategies in the presence",
    "start": "2362510",
    "end": "2370490"
  },
  {
    "text": "of treatment\nconfounder feedback. So this is when time\nvarying confounders are",
    "start": "2370490",
    "end": "2375560"
  },
  {
    "text": "affected by previous treatment. So if we kind of ground\nthis in a concrete example",
    "start": "2375560",
    "end": "2381620"
  },
  {
    "text": "with this causal\ndiagram, let's say we're interested in estimating\nthe effect of some intervention",
    "start": "2381620",
    "end": "2387140"
  },
  {
    "text": "A, vasopressors or it could be\nIV fluids, on some outcome Y,",
    "start": "2387140",
    "end": "2392750"
  },
  {
    "text": "which we'll call survival here. We know that vasopressors\naffect blood pressure,",
    "start": "2392750",
    "end": "2398630"
  },
  {
    "text": "and blood pressure will\naffect subsequent decisions to treat with vasopressors.",
    "start": "2398630",
    "end": "2404210"
  },
  {
    "text": "We also know that\nhypotension-- so again, blood pressure, L1,\naffects survival, based",
    "start": "2404210",
    "end": "2410570"
  },
  {
    "text": "on our clinical knowledge. And then in this DAG, we\nalso have the node U, which",
    "start": "2410570",
    "end": "2416180"
  },
  {
    "text": "represents disease severity. So these could be potentially\nunmeasured markers",
    "start": "2416180",
    "end": "2421910"
  },
  {
    "text": "of disease severity that are\naffecting your blood pressure and also affecting your\nprobability of survival.",
    "start": "2421910",
    "end": "2430260"
  },
  {
    "text": "So if we're interested\nin estimating the effect of a sustained\ntreatment strategy,",
    "start": "2430260",
    "end": "2437510"
  },
  {
    "text": "then we want to know something\nabout the total effect of treatment at all time points. We can see that L1 here is a\nconfounder for the effect of A1",
    "start": "2437510",
    "end": "2445520"
  },
  {
    "text": "on Y so we have to do\nsomething to adjust for that. And if we were to apply a\nconventional statistical",
    "start": "2445520",
    "end": "2450980"
  },
  {
    "text": "method, we would essentially\nbe conditioning on a collider and inducing a selection bias.",
    "start": "2450980",
    "end": "2456780"
  },
  {
    "text": "So an open path from\nA0 to L1 to U to Y. What's the consequence of this?",
    "start": "2456780",
    "end": "2462750"
  },
  {
    "text": "If we look in our\ndata set, we may see an association\nbetween A and Y.",
    "start": "2462750",
    "end": "2468040"
  },
  {
    "text": "But that association is not\nbecause there's necessarily an effect of A on Y.\nIt might not be causal.",
    "start": "2468040",
    "end": "2474079"
  },
  {
    "text": "It may be due to this\nselection bias that we created.",
    "start": "2474080",
    "end": "2479100"
  },
  {
    "start": "2478000",
    "end": "2514000"
  },
  {
    "text": "So this is the problem. And so in these cases, we\nneed a special type of method",
    "start": "2479100",
    "end": "2484910"
  },
  {
    "text": "that can handle these settings. And so a class of methods\nthat was designed specifically",
    "start": "2484910",
    "end": "2492260"
  },
  {
    "text": "to handle this is g-methods. And so these are sometimes\nreferred to as causal methods.",
    "start": "2492260",
    "end": "2498380"
  },
  {
    "text": "They've been developed by\nJamie Robins and colleagues and collaborators since 1986.",
    "start": "2498380",
    "end": "2503480"
  },
  {
    "text": "And they include the\nparametric g-formula, g-estimation of\nstructural nested models, and inverse\nprobability weighting",
    "start": "2503480",
    "end": "2509660"
  },
  {
    "text": "of marginal structural models. ",
    "start": "2509660",
    "end": "2515140"
  },
  {
    "start": "2514000",
    "end": "2718000"
  },
  {
    "text": "So in my research,\nwhat I do is I combine g-methods with\nlarge longitudinal databases",
    "start": "2515140",
    "end": "2522420"
  },
  {
    "text": "to try to evaluate dynamic\ntreatment strategies. So I'm particularly interested\nin bringing these methods",
    "start": "2522420",
    "end": "2529320"
  },
  {
    "text": "to cancer research,\nbecause they haven't been applied much there. So a lot of my\nresearch questions",
    "start": "2529320",
    "end": "2534420"
  },
  {
    "text": "are focused on answering\nquestions like, how and when can we intervene to\nbest prevent, detect, and treat",
    "start": "2534420",
    "end": "2541740"
  },
  {
    "text": "cancer? And so I'd like to share\none example with you, which",
    "start": "2541740",
    "end": "2548370"
  },
  {
    "text": "focused on evaluating\nthe effect of adhering to guideline-based\nphysical activity",
    "start": "2548370",
    "end": "2554940"
  },
  {
    "text": "interventions on survival\namong men with prostate cancer. So the motivation\nfor this study,",
    "start": "2554940",
    "end": "2561390"
  },
  {
    "text": "there's a large clinical\norganization, ASCO, the American Society\nof Clinical Oncology, that had actually called\nfor randomized trials",
    "start": "2561390",
    "end": "2568680"
  },
  {
    "text": "to generate these estimates\nfor several cancers. The thing with\nprostate cancer is",
    "start": "2568680",
    "end": "2574200"
  },
  {
    "text": "it's a very slowly\nprogressing disease. So the feasibility of doing\na trial to evaluate this",
    "start": "2574200",
    "end": "2579840"
  },
  {
    "text": "is very limited. The trial would have to\nbe 10 years long probably. So given that, given the absence\nof this randomized evidence,",
    "start": "2579840",
    "end": "2588390"
  },
  {
    "text": "we did the next\nbest thing that we could do to generate\nthis estimate, which was combine high-quality\nobservational data",
    "start": "2588390",
    "end": "2595230"
  },
  {
    "text": "with advanced EPI methods, in\nthis case parametric g-formula. And so we leveraged data\nfrom the Health Professionals",
    "start": "2595230",
    "end": "2602730"
  },
  {
    "text": "Follow-up Study, which is a\nwell-characterized prospective cohort study. ",
    "start": "2602730",
    "end": "2609670"
  },
  {
    "text": "So in these cases, there's\na three-step process that we take to extract the\nmost meaningful and actionable",
    "start": "2609670",
    "end": "2617090"
  },
  {
    "text": "insights from\nobservational data. So the first thing\nthat we do is we specify the protocol\nof the target trial",
    "start": "2617090",
    "end": "2624740"
  },
  {
    "text": "that we would have liked to\nconduct had it been feasible. The second thing we\ndo is we make sure",
    "start": "2624740",
    "end": "2631339"
  },
  {
    "text": "that we measure enough\ncovariates to approximately adjust for confounding\nand achieve",
    "start": "2631340",
    "end": "2637280"
  },
  {
    "text": "conditional exchangeability. And then the third\nthing we do is we apply an appropriate method\nto compare the specified",
    "start": "2637280",
    "end": "2644510"
  },
  {
    "text": "treatment strategies\nunder this assumption of conditional exchangeability.",
    "start": "2644510",
    "end": "2650670"
  },
  {
    "text": "And so in this case,\neligible men for this study had been diagnosed with\nnon-metastatic prostate cancer.",
    "start": "2650670",
    "end": "2657430"
  },
  {
    "text": "And at baseline,\nthey were free of cardiovascular and\nneurologic conditions that may limit physical ability.",
    "start": "2657430",
    "end": "2664319"
  },
  {
    "text": "For the treatment\nstrategies, men were to initiate one of\nsix physical activity strategies at diagnosis and\ncontinue it over followup",
    "start": "2664320",
    "end": "2673410"
  },
  {
    "text": "until the development\nof a condition limiting physical activity. So this is what made\nthe strategies dynamic.",
    "start": "2673410",
    "end": "2680900"
  },
  {
    "text": "The intervention\nover time depended on these evolving conditions. And so just to note,\nwe pre-specified",
    "start": "2680900",
    "end": "2688530"
  },
  {
    "text": "these strategies that\nwe were evaluating as well as the conditions.",
    "start": "2688530",
    "end": "2694040"
  },
  {
    "text": "Men were followed\nuntil diagnosis, until death, and to followup\n10 years after diagnosis",
    "start": "2694040",
    "end": "2699793"
  },
  {
    "text": "or administrative\nend to followup, whichever happened first. Our outcome of interest\nwas all cause mortality",
    "start": "2699793",
    "end": "2705140"
  },
  {
    "text": "within 10 years. And we were interested in\nestimating the per protocol effect of not just\ninitiating these strategies",
    "start": "2705140",
    "end": "2712670"
  },
  {
    "text": "but adhering to\nthem over followup. And again, we applied\nthe parametric g-formula.",
    "start": "2712670",
    "end": "2719615"
  },
  {
    "start": "2718000",
    "end": "3012000"
  },
  {
    "text": "So I think you've already\nheard about the g-formula in a previous lecture, possibly\nin a slightly different way.",
    "start": "2719615",
    "end": "2724720"
  },
  {
    "text": "So I won't spend too\nmuch time on this. So the g-formula, essentially\nthe way I think about it",
    "start": "2724720",
    "end": "2730380"
  },
  {
    "text": "is a generalization\nof standardization to time varying exposures\nand confounders.",
    "start": "2730380",
    "end": "2736380"
  },
  {
    "text": "So it's basically\na weighted average of risks, where you can\nthink of the weights being the probability density\nfunctions of the time varying",
    "start": "2736380",
    "end": "2743910"
  },
  {
    "text": "confounders, which we estimate\nusing parametric regression models. And we approximate\nthe weighted average",
    "start": "2743910",
    "end": "2750090"
  },
  {
    "text": "using Monte Carlo simulation. So practically\nhow do we do this?",
    "start": "2750090",
    "end": "2756840"
  },
  {
    "text": "So the first thing we do is\nwe fit parametric regression models for all of the\nvariables that we're",
    "start": "2756840",
    "end": "2762020"
  },
  {
    "text": "going to be studying. So for treatment confounders\nand death at each followup time.",
    "start": "2762020",
    "end": "2768690"
  },
  {
    "text": "The next thing we do is\nMonte Carlo simulation where essentially\nwhat we want to do is simulate the\noutcome distribution",
    "start": "2768690",
    "end": "2775880"
  },
  {
    "text": "under each treatment strategy\nthat we're interested in.",
    "start": "2775880",
    "end": "2781140"
  },
  {
    "text": "And then we bootstrap\nthe confidence intervals. So I'd like to show you\nkind of in a schematic what",
    "start": "2781140",
    "end": "2787495"
  },
  {
    "text": "this looks like,\nbecause it might be a little bit easier to see. So again, the idea\nis we're going to make copies of our data\nset, where in each copy",
    "start": "2787495",
    "end": "2796730"
  },
  {
    "text": "everyone is adhering\nto the strategy that we're focusing\non in that copy.",
    "start": "2796730",
    "end": "2802070"
  },
  {
    "text": "So how do we construct each of\nthese copies of the data set? We have to build them\neach from the ground up,",
    "start": "2802070",
    "end": "2808349"
  },
  {
    "text": "starting with time 0. So the values of all of the time\nvarying covariates at time 0",
    "start": "2808350",
    "end": "2814580"
  },
  {
    "text": "are sampled from their\nempirical distribution. So these are actually observed\nvalues of the covariates.",
    "start": "2814580",
    "end": "2821780"
  },
  {
    "text": "How do we get the values\nat the next time point? We use the parametric\nregression models",
    "start": "2821780",
    "end": "2827900"
  },
  {
    "text": "that I mentioned that\nwe fit in step 1. Then what we do is we force\nthe level of the intervention",
    "start": "2827900",
    "end": "2836900"
  },
  {
    "text": "variable to be whatever was\nspecified by that intervention strategy.",
    "start": "2836900",
    "end": "2843319"
  },
  {
    "text": "And then we estimate\nthe risk of the outcome at each time period\ngiven these variables,",
    "start": "2843320",
    "end": "2849890"
  },
  {
    "text": "again using the\nparametric regression model for the outcome now. And so we repeat this\nover all time periods",
    "start": "2849890",
    "end": "2856070"
  },
  {
    "text": "to estimate a cumulative risk\nunder that strategy, which",
    "start": "2856070",
    "end": "2861110"
  },
  {
    "text": "is taken as the average of\nthe subject-specific risks. So this is what I'm doing.",
    "start": "2861110",
    "end": "2866750"
  },
  {
    "text": "This is kind of\nunder the hood what's going on with this method. DAVID SONTAG: So\nmaybe we should try to put that in language of\nwhat we saw in the class.",
    "start": "2866750",
    "end": "2873890"
  },
  {
    "text": "And let me know if I'm\ngetting this wrong. So you first estimate the\nmarkup decision process,",
    "start": "2873890",
    "end": "2882410"
  },
  {
    "text": "which allows you to simulate\nfrom the underlying data distribution.",
    "start": "2882410",
    "end": "2888020"
  },
  {
    "text": "So you know that probability\nof this sort of next sequence of observations, given the\nprevious sequence and action",
    "start": "2888020",
    "end": "2895820"
  },
  {
    "text": "and previous actions,\nand then with that, then you could then intervene\nand simulate the forms.",
    "start": "2895820",
    "end": "2901930"
  },
  {
    "text": "Because that was,\nif you remember Frederick gave you\nthree different buckets of approaches.",
    "start": "2901930",
    "end": "2908040"
  },
  {
    "text": "Then he focused\non the middle one. This is the left-most bucket. The right? AUDIENCE: Yes. DAVID SONTAG: So we\ndidn't talk about it.",
    "start": "2908040",
    "end": "2914660"
  },
  {
    "text": "AUDIENCE: No, [INAUDIBLE]\nmodel based on relevance. BARBRA DICKERMAN: Yeah. Yes. DAVID SONTAG: But\nit's very sensible.",
    "start": "2914660",
    "end": "2920905"
  },
  {
    "text": "AUDIENCE: Yeah. But it seems very hard. BARBRA DICKERMAN: What's that? AUDIENCE: Sorry.",
    "start": "2920905",
    "end": "2926080"
  },
  {
    "text": "Oh, it seems very hard to\nmodel this [INAUDIBLE].. BARBRA DICKERMAN: Yeah. So that is a challenge.",
    "start": "2926080",
    "end": "2931150"
  },
  {
    "text": "That is the hardest\npart about this. And it's relying on a\nlot of assumptions, yeah. ",
    "start": "2931150",
    "end": "2939530"
  },
  {
    "text": "So the primary\nresults that kind of come out after we\ndo all of this.",
    "start": "2939530",
    "end": "2944640"
  },
  {
    "text": "So this is the estimated\nrisk of all cause mortality under several physical\nactivity interventions.",
    "start": "2944640",
    "end": "2950780"
  },
  {
    "text": "So I'm not going to focus\ntoo much on the results. I want to focus on two main\ntakeaways from this slide.",
    "start": "2950780",
    "end": "2957119"
  },
  {
    "text": "One thing to emphasize\nis we pre-specified the weekly duration\nof physical activity.",
    "start": "2957120",
    "end": "2963450"
  },
  {
    "text": "Or you can think of this like\nthe dose of the intervention. We pre-specified that. And this was based on\ncurrent guidelines.",
    "start": "2963450",
    "end": "2970730"
  },
  {
    "text": "So the third row\nof each band, we did look at some dose or\nlevel beyond the guidelines",
    "start": "2970730",
    "end": "2976610"
  },
  {
    "text": "to see if there might be\nadditional survival benefits. But these were\nall pre-specified.",
    "start": "2976610",
    "end": "2981930"
  },
  {
    "text": "We also pre-specified all of\nthe time varying covariates that made these\nstrategies dynamic.",
    "start": "2981930",
    "end": "2987890"
  },
  {
    "text": "So I mentioned that\nmen were excused from following the\nrecommended physical activity levels if they developed one\nof these listed conditions,",
    "start": "2987890",
    "end": "2996140"
  },
  {
    "text": "metastasis, MI,\nstroke, et cetera. We pre-specified all of those. It's possible that maybe\na different dependence",
    "start": "2996140",
    "end": "3004828"
  },
  {
    "text": "on a different time\nvarying covariate may have led to a\nmore optimal strategy. There was a lot that\nremained unexplored.",
    "start": "3004828",
    "end": "3010870"
  },
  {
    "text": " So we did a lot of\nsensitivity analyses",
    "start": "3010870",
    "end": "3016830"
  },
  {
    "start": "3012000",
    "end": "3232000"
  },
  {
    "text": "as part of this project. I'd like to focus, though,\non the sensitivity analyses",
    "start": "3016830",
    "end": "3021930"
  },
  {
    "text": "that we did for potential\nunmeasured confounding by chronic disease that\nmay be severe enough",
    "start": "3021930",
    "end": "3028680"
  },
  {
    "text": "to affect both physical\nactivity and survival. And so the g-formula is\nactually providing a natural way",
    "start": "3028680",
    "end": "3036870"
  },
  {
    "text": "to at least partly\naddress this by estimating the risk of these physical\nactivity interventions that",
    "start": "3036870",
    "end": "3044900"
  },
  {
    "text": "are at each time\npoint t only applied to men who are healthy enough\nto maintain a physical activity",
    "start": "3044900",
    "end": "3051650"
  },
  {
    "text": "level at that time. And so again in\nthe main analysis, we excused men from following\nthe recommended levels",
    "start": "3051650",
    "end": "3058400"
  },
  {
    "text": "if they developed one of\nthese serious conditions. So in sensitivity\nanalyses, we then",
    "start": "3058400",
    "end": "3065180"
  },
  {
    "text": "expanded this list\nof serious conditions to also include the conditions\nthat are shown in blue text.",
    "start": "3065180",
    "end": "3072589"
  },
  {
    "text": "And so this attenuated\nour estimates but didn't change\nour conclusions. One thing to point out is that\nthe validity of this approach",
    "start": "3072590",
    "end": "3081620"
  },
  {
    "text": "rests on the assumption\nthat at each time t we had available data\nneeded to identify which",
    "start": "3081620",
    "end": "3090350"
  },
  {
    "text": "men were healthy\nat that time enough to do the physical activity. Yeah. AUDIENCE: Sorry,\njust to double-check,",
    "start": "3090350",
    "end": "3096023"
  },
  {
    "text": "does excuse mean\nthat you remove them? BARBRA DICKERMAN:\nGreat question. So because the strategy\nwas pre-specified to say",
    "start": "3096023",
    "end": "3102980"
  },
  {
    "text": "that if you develop one\nof these conditions, you may essentially do whatever\nlevel of physical activity",
    "start": "3102980",
    "end": "3110089"
  },
  {
    "text": "you're able to do. So importantly-- I'm glad\nyou brought this up-- we did not censor\nmen at that time.",
    "start": "3110090",
    "end": "3116420"
  },
  {
    "text": "They were still followed,\nbecause they were still adhering to the\nstrategy as defined.",
    "start": "3116420",
    "end": "3122330"
  },
  {
    "text": "Thanks for asking. And so given that we don't\nknow whether the data contain",
    "start": "3122330",
    "end": "3129290"
  },
  {
    "text": "at each time t the\ninformation necessary to know, are these men healthy enough\nat that time, we therefore",
    "start": "3129290",
    "end": "3136070"
  },
  {
    "text": "conducted a few alternate\nanalyses in which we lagged physical activity and\ncovariate data by two years.",
    "start": "3136070",
    "end": "3142880"
  },
  {
    "text": "And we also used a\nnegative outcome control to explore potential unmeasured\nconfounding by clinical disease",
    "start": "3142880",
    "end": "3149810"
  },
  {
    "text": "or disease severity. So what's the\nrationale behind this? So in the DAGs below for\nthe original analysis,",
    "start": "3149810",
    "end": "3156770"
  },
  {
    "text": "we have physical activity\nA. We have survival Y. And this may be confounded\nby disease severity U.",
    "start": "3156770",
    "end": "3165590"
  },
  {
    "text": "So when we see an association\nbetween A and Y in our data, we want to make sure\nthat it's causal,",
    "start": "3165590",
    "end": "3171070"
  },
  {
    "text": "that it's because\nof the blue arrow, and not because of\nthis confounding bias, the red arrow.",
    "start": "3171070",
    "end": "3176640"
  },
  {
    "text": "So how can we\npotentially provide evidence for whether that\nred pathway is there?",
    "start": "3176640",
    "end": "3182480"
  },
  {
    "text": "We selected\nquestionnaire nonresponse as an alternate outcome,\ninstead of survival,",
    "start": "3182480",
    "end": "3188750"
  },
  {
    "text": "that we assumed was not directly\naffected by physical activity,",
    "start": "3188750",
    "end": "3193940"
  },
  {
    "text": "but that we thought would\nbe similarly confounded by disease severity.",
    "start": "3193940",
    "end": "3199230"
  },
  {
    "text": "And so when we\nrepeated the analysis with a negative\noutcome control, we found that physical activity\nhad a nearly null effect",
    "start": "3199230",
    "end": "3206000"
  },
  {
    "text": "on questionnaire nonresponse,\nas we would expect, which provides some support\nthat in our original analysis,",
    "start": "3206000",
    "end": "3214353"
  },
  {
    "text": "the effect of physical\nactivity on death was not confounded through\nthe pathways explored",
    "start": "3214353",
    "end": "3219380"
  },
  {
    "text": "through the negative control. So one thing to highlight\nhere is the sensitivity analyses were driven by our\nsubject matter knowledge.",
    "start": "3219380",
    "end": "3227819"
  },
  {
    "text": "And there's nothing in the\ndata that kind of drove this. ",
    "start": "3227820",
    "end": "3233700"
  },
  {
    "start": "3232000",
    "end": "3312000"
  },
  {
    "text": "And so just to\nrecap this portion. So g-methods are a\nuseful tool, because they",
    "start": "3233700",
    "end": "3239160"
  },
  {
    "text": "let us validly\nestimate the effect of pre-specified\ndynamic strategies",
    "start": "3239160",
    "end": "3245490"
  },
  {
    "text": "and estimate adjusted absolute\nrisks, which are clinically meaningful to us, and\nappropriately adjusted survival",
    "start": "3245490",
    "end": "3251520"
  },
  {
    "text": "curves, even in the presence\nof treatment confounder feedback, which occurs\noften in clinical questions.",
    "start": "3251520",
    "end": "3259770"
  },
  {
    "text": "And of course, this is under\nour typical identifiability assumptions.",
    "start": "3259770",
    "end": "3265020"
  },
  {
    "text": "So this makes it a\npowerful approach to estimate the effects\nof currently recommended or proposed strategies\nthat therefore we",
    "start": "3265020",
    "end": "3271320"
  },
  {
    "text": "can specify and write out\nprecisely as we did here. However, these\npre-specified strategies",
    "start": "3271320",
    "end": "3278280"
  },
  {
    "text": "may not be the\noptimal strategies. So again, when I was\ndoing this analysis,",
    "start": "3278280",
    "end": "3284310"
  },
  {
    "text": "I was thinking there are so\nmany different weekly durations of physical activity that\nwe're not looking at.",
    "start": "3284310",
    "end": "3290320"
  },
  {
    "text": "There are so many different\ntime-varying covariates where we could have different\ndependencies on those",
    "start": "3290320",
    "end": "3296430"
  },
  {
    "text": "for these strategies over time. And maybe those would have\nled to better survival outcomes among these men, but\nall of that was unexplored.",
    "start": "3296430",
    "end": "3305960"
  },
  {
    "start": "3305960",
    "end": "3312000"
  }
]