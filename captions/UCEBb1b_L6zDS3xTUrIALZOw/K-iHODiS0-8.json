[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "7410"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "7410",
    "end": "13960"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu",
    "start": "13960",
    "end": "23400"
  },
  {
    "text": "PROFESSOR: OK, we have\na busy day today, so let's get started. ",
    "start": "23400",
    "end": "32580"
  },
  {
    "text": "Want to go through Chernoff\nbounds and the Wald identity, which are closely related, as\nyou'll see, and that involves",
    "start": "32580",
    "end": "42770"
  },
  {
    "text": "coming back to the TG1Q a little\nbit and making use of what we did for that.",
    "start": "42770",
    "end": "49290"
  },
  {
    "text": "It also means coming\nback to hypothesis",
    "start": "49290",
    "end": "54320"
  },
  {
    "text": "testing and using that. Would probably have been better\nto start out with Wald's identity and the Chernoff\nbound and then do the",
    "start": "54320",
    "end": "64319"
  },
  {
    "text": "applications when it was at\nthe natural time for them. But anyway, this is the way it\nis this time, and next time",
    "start": "64319",
    "end": "72730"
  },
  {
    "text": "we'll probably do\nit differently.  Suppose you have a random\nvariable z.",
    "start": "72730",
    "end": "79580"
  },
  {
    "text": "It has a moment generating\nfunction. Remember, not all random\nvariables have moment generating functions.",
    "start": "79580",
    "end": "85250"
  },
  {
    "text": "It's a pretty strong\nrestriction. You need a variance. You need moments\nof all orders.",
    "start": "85250",
    "end": "91570"
  },
  {
    "text": "You need all sorts of things,\ns but we'll assume it exists in some region between\nr and r plus.",
    "start": "91570",
    "end": "101009"
  },
  {
    "text": "There's always a question,\nwith moment generating functions, if they exist up\nto some maximum value of r",
    "start": "101010",
    "end": "109240"
  },
  {
    "text": "because some of them exist at\nthat value of r and then disappear immediately after\nthat, and others just sort of",
    "start": "109240",
    "end": "119649"
  },
  {
    "text": "peter away as r approaches\nr plus from below.",
    "start": "119650",
    "end": "128210"
  },
  {
    "text": "I think in the homework this\nweek, you have an example of both of those. I mean, it's a very\nsimple issue.",
    "start": "128210",
    "end": "133900"
  },
  {
    "text": "If you have an exponential\ndistribution, then as r approaches, the rate of that\nexponential distribution,",
    "start": "133900",
    "end": "143900"
  },
  {
    "text": "obviously, the moment generating\nfunction blows up because you're taking e to the\nminus lambda x, and you're",
    "start": "143900",
    "end": "150700"
  },
  {
    "text": "multiplying it by a the r x. And when r is equal to lambda,\nbingo, you're integrating 1",
    "start": "150700",
    "end": "159280"
  },
  {
    "text": "over an infinite range, so\nyou've got infinity. If you multiply that exponential\nby something which",
    "start": "159280",
    "end": "166150"
  },
  {
    "text": "makes the integral finite when\nyou set r equal to lambda,",
    "start": "166150",
    "end": "172360"
  },
  {
    "text": "then of course, you have\nsomething which is finite at r star.",
    "start": "172360",
    "end": "177459"
  },
  {
    "text": "That is a big pain\nin the neck. It's usually not important. The notes deal with it very\ncarefully, so we're not going",
    "start": "177460",
    "end": "186240"
  },
  {
    "text": "to deal with it here. We will just assume here that\nwe're talking about r less than r plus and not worry about\nthat special case, which",
    "start": "186240",
    "end": "196040"
  },
  {
    "text": "usually is not all\nthat important. But sometimes you have\nto worry about it. OK, the Chernoff bound says\nthat the probability that",
    "start": "196040",
    "end": "203610"
  },
  {
    "text": "random variable is greater than\nor equal to alpha is less than or equal to the moment\ngenerating function evaluated",
    "start": "203610",
    "end": "212060"
  },
  {
    "text": "at some arbitrary value r times\ne to the minus r alpha.",
    "start": "212060",
    "end": "217690"
  },
  {
    "text": "And if you put it in terms of\nthe semi invariant moment generating function, the log\nof the moment generating",
    "start": "217690",
    "end": "224440"
  },
  {
    "text": "function, then the bound\nis e to the gamma z of r minus alpha r.",
    "start": "224440",
    "end": "231099"
  },
  {
    "text": " When you see something like\nthat, you ought to look at it",
    "start": "231100",
    "end": "237410"
  },
  {
    "text": "and say, gee, that looks funny\nbecause here, we're taking an arbitrary random variable and\nsaying the tails of it have to",
    "start": "237410",
    "end": "244750"
  },
  {
    "text": "go down exponentially. That's exactly what this says. It says that a z takes\non very large values.",
    "start": "244750",
    "end": "253560"
  },
  {
    "text": "This is a fixed quantity here\nfor a given value of r, and it's going down as e to the\nminus r times alpha.",
    "start": "253560",
    "end": "261699"
  },
  {
    "text": "As you make alpha larger and\nlarger, this goes down faster and faster. So what's going on?",
    "start": "261700",
    "end": "266790"
  },
  {
    "text": "How do you take an arbitrary\nrandom variable and say the tails of it is exponentially\ndecreasing?",
    "start": "266790",
    "end": "274139"
  },
  {
    "text": "That's why you have to insist\nthat the moment generating function exists because when the\nmoment generating function",
    "start": "274140",
    "end": "281450"
  },
  {
    "text": "exists for some r, it means\nthat the tail of that distribution is, in fact, going\ndown at least that fast,",
    "start": "281450",
    "end": "288870"
  },
  {
    "text": "so you get something\nthat exists. So the question is what's the\nbest bound of this sort of",
    "start": "288870",
    "end": "295310"
  },
  {
    "text": "when you optimize o for r? Then the next thing we did is\nwe said that z is a sum of",
    "start": "295310",
    "end": "302650"
  },
  {
    "text": "IID, then the semi invariant\nmoment generating function for that sum is equal to n times\nthe semi invariant moment",
    "start": "302650",
    "end": "312480"
  },
  {
    "text": "generating function for the\nunderlying random variable x.",
    "start": "312480",
    "end": "317722"
  },
  {
    "text": "S of n is n of these IID\nrandom variable. So one thing you see\nimmediately, and ought to be",
    "start": "317722",
    "end": "324180"
  },
  {
    "text": "second nature to you now, is\nthat if a random variable has a moment generating function\nover some range, the sum of a",
    "start": "324180",
    "end": "332480"
  },
  {
    "text": "bunch of those IID random\nvariables also has a moment generating function over\nthat same range.",
    "start": "332480",
    "end": "339200"
  },
  {
    "text": "You can just count on that\nbecause the semi invariant moment generating function\nis just n times this b.",
    "start": "339200",
    "end": "346000"
  },
  {
    "text": "OK, so then what we've said is\nthe probability that sn is greater than or equal to na,\nwhere na is playing the role",
    "start": "346000",
    "end": "353170"
  },
  {
    "text": "of alpha and sn is playing the\nrole of z, is just a minimum over r of e to the n times gamma\nx of r minus ra, and the",
    "start": "353170",
    "end": "364850"
  },
  {
    "text": "n is multiplying the ra\nas well as the n. OK, this is exponential\nn for a fixed a.",
    "start": "364850",
    "end": "372100"
  },
  {
    "text": "In other words, what you do in\nthis minimization, if you don't worry about the special\ncases or anything, how do you",
    "start": "372100",
    "end": "378780"
  },
  {
    "text": "minimize something? Well, obviously, you want to\nminimize the exponent here, so",
    "start": "378780",
    "end": "385590"
  },
  {
    "text": "you take the derivative of this\ngamma prime of r has to be equal to a.",
    "start": "385590",
    "end": "392330"
  },
  {
    "text": "Then n can be whatever it wants\nto be when you find that optimum r, which is where gamma\nprime of r equals a,",
    "start": "392330",
    "end": "400240"
  },
  {
    "text": "then this goes down\nexponentially with a. ",
    "start": "400240",
    "end": "405590"
  },
  {
    "text": "Now, however, we're interested\nin something else. We're interested in threshold\ncrossings.",
    "start": "405590",
    "end": "411129"
  },
  {
    "text": "We're not interested in picking\na particular value of",
    "start": "411130",
    "end": "416800"
  },
  {
    "text": "a and asking, as n gets very,\nvery big, what's the probability that the sum of\nrandom variable is greater",
    "start": "416800",
    "end": "423150"
  },
  {
    "text": "than or equal to n times a. That is exponential in n, but\nwhat we're interested in is",
    "start": "423150",
    "end": "428759"
  },
  {
    "text": "the probability that s of n is\ngreater than or equal to just some constant alpha, and what\nwe're doing, now, is instead",
    "start": "428760",
    "end": "436300"
  },
  {
    "text": "of varying n and varying\nthis with n also, we're holding the stick.",
    "start": "436300",
    "end": "442030"
  },
  {
    "text": "So we're asking as n gets very,\nvery large, but you hold this alpha fixed, what happens\non this bound over here?",
    "start": "442030",
    "end": "450340"
  },
  {
    "text": "Well, when you minimize this,\ntaking the same simple-minded view, now the n is not\nmultiplied by the ra.",
    "start": "450340",
    "end": "458090"
  },
  {
    "text": "It's just multiplied\nby the gamma x. You get n times gamma prime of\nr is equal to alpha s where",
    "start": "458090",
    "end": "465790"
  },
  {
    "text": "the minimum is so that it says\ngamma prime of r is optimized",
    "start": "465790",
    "end": "471610"
  },
  {
    "text": "when you pick gamma prime of r\nequal to alpha over and n. This quantity is minimized when\nyou pick gamma prime of r",
    "start": "471610",
    "end": "480460"
  },
  {
    "text": "equal to alpha over n. So if you look at this bound as\nn changes, what's happening",
    "start": "480460",
    "end": "486200"
  },
  {
    "text": "is, as n changes, r is changing\nalso, so this is a",
    "start": "486200",
    "end": "491710"
  },
  {
    "text": "harder thing to deal with\nfor variable n. But graphically, it's quite\neasy to deal with.",
    "start": "491710",
    "end": "499580"
  },
  {
    "text": "I'm not sure you all got the\ngraphical argument last time when we went through it, so I\nwant to go through it again.",
    "start": "499580",
    "end": "506570"
  },
  {
    "text": "Let's look at this exponent r\nminus n over alpha times gamma",
    "start": "506570",
    "end": "512409"
  },
  {
    "text": "of r, and see what\nit looks like. We'll take r, pick\nany old r, there.",
    "start": "512409",
    "end": "522000"
  },
  {
    "text": "What we want to do is show that\nthis, if you take a slope",
    "start": "522000",
    "end": "528530"
  },
  {
    "text": "of alpha over n, and take an\narbitrary r, come down to",
    "start": "528530",
    "end": "536010"
  },
  {
    "text": "gamma of x of r, draw a line\nin this slope, and look at where it hits the horizontal\naxis here, that point is r",
    "start": "536010",
    "end": "546350"
  },
  {
    "text": "plus the length of\nthis line here. The length of this line here\nis gamma of r, that's a",
    "start": "546350",
    "end": "553410"
  },
  {
    "text": "negative value, times 1 over\nthat slope of this line.",
    "start": "553410",
    "end": "558930"
  },
  {
    "text": "And 1 over the slope of this\nline is n over alpha, so when",
    "start": "558930",
    "end": "564830"
  },
  {
    "text": "I pick a particular value of r,\nthe value of the experiment I have is this value here.",
    "start": "564830",
    "end": "571100"
  },
  {
    "start": "571100",
    "end": "576930"
  },
  {
    "text": "How do I optimize this over r? How do I get the largest\nexponent here?",
    "start": "576930",
    "end": "582340"
  },
  {
    "text": "Well, I think of varying r, as\nI vary r from 0, and each",
    "start": "582340",
    "end": "588560"
  },
  {
    "text": "time, I take this straight\nline here. And I start here, draw a\nstraight line over there,",
    "start": "588560",
    "end": "594530"
  },
  {
    "text": "start here, draw a straight\nline over, start at this tangent here, draw a\nstraight line over.",
    "start": "594530",
    "end": "600320"
  },
  {
    "text": "And what happens when I come\nto larger values of r? Just because gamma of s of r is\nconvex, what happens is I",
    "start": "600320",
    "end": "609300"
  },
  {
    "text": "start taking these slope lines,\nslope alpha over n, and",
    "start": "609300",
    "end": "617500"
  },
  {
    "text": "they intercept the horizontal\naxis at a smaller value. So this is optimized over r\nat the value of r0, which",
    "start": "617500",
    "end": "628019"
  },
  {
    "text": "satisfies alpha over n equals\ngamma prime of r0. That's the same answer we got\nbefore when we just used",
    "start": "628020",
    "end": "635870"
  },
  {
    "text": "elementary calculus. Here, we're using a more\nsophisticated argument, which",
    "start": "635870",
    "end": "641330"
  },
  {
    "text": "you learned about probably\nin 10th grade. I would argue that you learned\nmostly really sophisticated",
    "start": "641330",
    "end": "647540"
  },
  {
    "text": "things when you're in high\nschool, and then when you get to study engineering in college,\nsomehow you always",
    "start": "647540",
    "end": "655370"
  },
  {
    "text": "study these mundane things. But anyway, aside from\nthat, why is this",
    "start": "655370",
    "end": "661740"
  },
  {
    "text": "geometric argument better? Well, when you look at these\nspecial cases of what happens",
    "start": "661740",
    "end": "667630"
  },
  {
    "text": "when gamma of r comes around\nlike this, and then suddenly",
    "start": "667630",
    "end": "673650"
  },
  {
    "text": "it stops in midair and just\ndoesn't exist anymore? So it comes around here, it's\nstill convex, but then",
    "start": "673650",
    "end": "681230"
  },
  {
    "text": "suddenly it goes off\nto infinity. How do you do that optimization\nthen? Well, the graphical argument\nmakes it clear how you do it,",
    "start": "681230",
    "end": "689050"
  },
  {
    "text": "and makes it perfectly rigorous\nhow to do it, whereas if you're doing it by calculus,\nyou've got a really",
    "start": "689050",
    "end": "694310"
  },
  {
    "text": "think it through, and it\nbecomes fairly tricky. OK, so anyway, now, the next\nquestion we want to ask--",
    "start": "694310",
    "end": "705040"
  },
  {
    "text": " I mean, at this point, we've\nseen how to minimize this",
    "start": "705040",
    "end": "711040"
  },
  {
    "text": "quantity over r, so we know what\nthis exponent is for a",
    "start": "711040",
    "end": "716490"
  },
  {
    "text": "particular value of n. Now, what happens\nwhen we vary n?",
    "start": "716490",
    "end": "721690"
  },
  {
    "text": "As you vary n, the thing that\nhappens is we have this tangent line here, a\nslope alpha over n.",
    "start": "721690",
    "end": "730010"
  },
  {
    "text": "When you start making n larger,\nalpha over n becomes smaller, so the slope\nbecomes smaller.",
    "start": "730010",
    "end": "738850"
  },
  {
    "text": "And as n approaches infinity,\nyou wind up going way,",
    "start": "738850",
    "end": "744209"
  },
  {
    "text": "way the heck out. As n gets smaller, you\ncome in again.",
    "start": "744210",
    "end": "749700"
  },
  {
    "text": "You keep coming in until you\nget to this point here. And what happens then? We're talking about\na line of--",
    "start": "749700",
    "end": "758010"
  },
  {
    "text": "maybe I ought to draw\nit on the board. It would be clearer, I think. ",
    "start": "758010",
    "end": "777850"
  },
  {
    "text": "As n gets smaller, you\nget a point which is tangent here, this here.",
    "start": "777850",
    "end": "785510"
  },
  {
    "text": "When you're here, the tangent\ngets right here, so we've",
    "start": "785510",
    "end": "792000"
  },
  {
    "text": "moved all the way into this\nquantity we call r star, which is the root of the equation\ngamma of r equals 0.",
    "start": "792000",
    "end": "800519"
  },
  {
    "text": "Gamma of r equals 0 typically\nhas two roots, one here, and one at 0.",
    "start": "800520",
    "end": "807920"
  },
  {
    "text": "It always has a root at 0\nbecause moment generating function evaluated is 0 is\nalways 1, so the log of",
    "start": "807920",
    "end": "816009"
  },
  {
    "text": "it is always 0. There should be another root\nbecause this is convex, unless",
    "start": "816010",
    "end": "821500"
  },
  {
    "text": "it drops off suddenly, and even\nif it drops off suddenly, you can visualize it\nas a straight line",
    "start": "821500",
    "end": "828269"
  },
  {
    "text": "going off to infinity. So when you get down to this\npoint, what happens?",
    "start": "828270",
    "end": "834240"
  },
  {
    "text": "Well, we just keep\nmoving along. ",
    "start": "834240",
    "end": "844339"
  },
  {
    "text": "So as n increases, we start\nout very large. We come in.",
    "start": "844340",
    "end": "850140"
  },
  {
    "text": "We hit this point, and then\nwe start coming out again. I mean, if you think about it,\nthat makes perfect sense",
    "start": "850140",
    "end": "858040"
  },
  {
    "text": "because what we're doing here is\nwe're imagining experiment where this random variable has\na negative expected value.",
    "start": "858040",
    "end": "867150"
  },
  {
    "text": "That's what's indicated by\nthis quantity there.",
    "start": "867150",
    "end": "873060"
  },
  {
    "text": "We're asking what's the\nprobability that the sum of a large number of IID random\nvariables with a negative",
    "start": "873060",
    "end": "880260"
  },
  {
    "text": "expected value ever rises above\nsome positive threshold? ",
    "start": "880260",
    "end": "887199"
  },
  {
    "text": "Well, the law of large numbers\nsays it's not going to do that when n is very, very large,\nand this says that, too.",
    "start": "887200",
    "end": "894100"
  },
  {
    "text": "It says the probability of\nit for n very large is extraordinarily small. It's e to the minus 10 times\nan exponent, which is very,",
    "start": "894100",
    "end": "902860"
  },
  {
    "text": "very large. So as n gets very small, it's\nnot going to happen either",
    "start": "902860",
    "end": "909660"
  },
  {
    "text": "because it doesn't have time\nto get to the threshold. So there's some intermediate\nvalue at which it's most",
    "start": "909660",
    "end": "915910"
  },
  {
    "text": "likely to cross the threshold,\nif you're going to cross the threshold, and that intermediate\nvalue is just",
    "start": "915910",
    "end": "922460"
  },
  {
    "text": "that value at which gamma of\nr star is equal to zero.",
    "start": "922460",
    "end": "928960"
  },
  {
    "text": " So the probability this union\nof terms, namely the",
    "start": "928960",
    "end": "934670"
  },
  {
    "text": "probability you ever cross\nalpha, is going to be, in some sense, approximately a to the\nminus alpha r star because",
    "start": "934670",
    "end": "945210"
  },
  {
    "text": "that's where the dominant\nterm is. The dominant term is where\nalpha over n is equal to gamma prime.",
    "start": "945210",
    "end": "952180"
  },
  {
    "text": "Blah, blah, blah, blah, blah,\nwhere'd I put that? r star satisfies gamma\nof r star equals 0.",
    "start": "952180",
    "end": "960130"
  },
  {
    "text": "When you look at the line of\nslope, gamma prime of r plus",
    "start": "960130",
    "end": "966650"
  },
  {
    "text": "of r star, that's where you get\nthis critical value of n",
    "start": "966650",
    "end": "972860"
  },
  {
    "text": "where it's most likely the\ncross the threshold. OK, I put that somewhere.",
    "start": "972860",
    "end": "978900"
  },
  {
    "text": "I thought it was on this slide,\nbut it's the n, the",
    "start": "978900",
    "end": "984410"
  },
  {
    "text": "critical n, let's call it n\ncrit, is equal to gamma prime.",
    "start": "984410",
    "end": "992194"
  },
  {
    "start": "992195",
    "end": "1001240"
  },
  {
    "text": "Is at right? Alpha over m is the\ngamma prime. Alpha over n, 1 over n crit.",
    "start": "1001240",
    "end": "1011730"
  },
  {
    "text": "n crit, this says, is alpha over\ngamma prime of r star.",
    "start": "1011730",
    "end": "1022829"
  },
  {
    "text": "OK, so that sort of nails down\neverything you want to know",
    "start": "1022830",
    "end": "1029670"
  },
  {
    "text": "about the Chernoff bound accept\nfor the fact that it is exponentially tight. The text proves that.",
    "start": "1029670",
    "end": "1035780"
  },
  {
    "text": "I'm not going to go\nthrough that here. Exponentially tight means, if\nyou take an exponent which is",
    "start": "1035780",
    "end": "1041480"
  },
  {
    "text": "just a little bit larger than\nthe one you found here, and look what happens as alpha\ngets very, very",
    "start": "1041480",
    "end": "1047660"
  },
  {
    "text": "large, then you lose. OK, let's go on, and at this\npoint, we're ready to talk",
    "start": "1047660",
    "end": "1057820"
  },
  {
    "text": "about Wald's identity. And we'll prove Wald's identity\nat the end of the",
    "start": "1057820",
    "end": "1063600"
  },
  {
    "text": "lecture today. Turns out there's a very,\nvery simple proof of it. There's hardly anything to it,\nbut it seems more important to",
    "start": "1063600",
    "end": "1075800"
  },
  {
    "text": "use it in several ways first so\nthat you get a sense that it, in fact, is sort\nof important.",
    "start": "1075800",
    "end": "1083440"
  },
  {
    "text": "OK, so we want to think about\na random walk, s sub n and",
    "start": "1083440",
    "end": "1089909"
  },
  {
    "text": "greater than or equal to n, so\nit's a sequence of sums of random variable, s sub\nn is equal to x1",
    "start": "1089910",
    "end": "1096559"
  },
  {
    "text": "plus up to x sub n. The x's are all IID. This is the thing we've been\ntalking about all term.",
    "start": "1096560",
    "end": "1104330"
  },
  {
    "text": "We have a bunch of IID\nrandom variables. We look at the partial\nsums of them.",
    "start": "1104330",
    "end": "1109600"
  },
  {
    "text": "We're interested in what happens\nto that sequence of partial sums. The question we're asking here\nis does that sequence of",
    "start": "1109600",
    "end": "1116940"
  },
  {
    "text": "partial sums ever cross\na positive threshold? And now we're asking does\nit ever cross a positive",
    "start": "1116940",
    "end": "1124670"
  },
  {
    "text": "threshold, or does it cross a\nnegative threshold, and which does it cross first?",
    "start": "1124670",
    "end": "1131230"
  },
  {
    "text": "So the probability that it\ncrosses this threshold is the probability that it\ngoes up, first.",
    "start": "1131230",
    "end": "1137340"
  },
  {
    "text": "The probability that it crosses\nthis threshold is the probability that it\ngoes down, first.",
    "start": "1137340",
    "end": "1143139"
  },
  {
    "text": "Now, what Wald's identity says\nis the following thing. We're going to assume that\nx is not identically 0.",
    "start": "1143140",
    "end": "1149480"
  },
  {
    "text": "If x is identically\n0, then it's never going to go any place. We're going to assume that it\nhas a semi invariant moment",
    "start": "1149480",
    "end": "1157070"
  },
  {
    "text": "generating function in some\nregion, r minus to r plus.",
    "start": "1157070",
    "end": "1162870"
  },
  {
    "text": "That's the same as assuming\nthat it has a generating function in that region, so it\nexists from some value less",
    "start": "1162870",
    "end": "1171230"
  },
  {
    "text": "than zero to some value\ngreater than zero. And we picked two thresholds,\none of them positive, one of",
    "start": "1171230",
    "end": "1178660"
  },
  {
    "text": "them negative, and we let j be\nthe smallest value of n. j is",
    "start": "1178660",
    "end": "1184980"
  },
  {
    "text": "a random variable, now, because\nwe've start to run this random walk.",
    "start": "1184980",
    "end": "1191070"
  },
  {
    "text": "We run it until it crosses one\nof these thresholds, and if it crosses the positive threshold,\nj is the time at",
    "start": "1191070",
    "end": "1197780"
  },
  {
    "text": "which it crosses the\npositive threshold. If it crosses the negative\nthreshold, j is the time that",
    "start": "1197780",
    "end": "1203470"
  },
  {
    "text": "it crosses the negative\nthreshold. We're only looking at the first threshold that it crosses. ",
    "start": "1203470",
    "end": "1210990"
  },
  {
    "text": "Now, notice that j is\na stopping trial. In other words, what that means\nis you can determine",
    "start": "1210990",
    "end": "1217270"
  },
  {
    "text": "whether you've crossed a\nthreshold at time n solely in terms of s1 up to s sub n.",
    "start": "1217270",
    "end": "1226039"
  },
  {
    "text": "If you see all these sums, then\nyou know that you haven't crossed a threshold\nup until time n.",
    "start": "1226040",
    "end": "1233169"
  },
  {
    "text": "You know you have crossed\nit at time n. Doesn't make any difference\nwhat happens at times greater than n.",
    "start": "1233170",
    "end": "1239700"
  },
  {
    "text": "OK, so it's a stopping trial\nin the same sense as the stopping trials we talked\nabout before.",
    "start": "1239700",
    "end": "1245330"
  },
  {
    "text": "You get the sense that Wald's\nidentity, which we're talking about here, is sort of like\nWald's equality, which we",
    "start": "1245330",
    "end": "1252020"
  },
  {
    "text": "talked about before. Both of them have to do with\nthese stopping trials.",
    "start": "1252020",
    "end": "1257620"
  },
  {
    "text": "Both of them have everything\nto do with stopping trials. Wald was a famous statistician,\nnot all that",
    "start": "1257620",
    "end": "1265390"
  },
  {
    "text": "much before your era. He didn't die too long ago.",
    "start": "1265390",
    "end": "1270910"
  },
  {
    "text": "I forget when, but he was one\nof the good statisticians. See, he was a statistician who\nrecognized that you wanted to",
    "start": "1270910",
    "end": "1279230"
  },
  {
    "text": "look at lots of different\nmodels to understand the problem, rather than a\nstatistician who only wanted",
    "start": "1279230",
    "end": "1284830"
  },
  {
    "text": "to take data and think that he\nwasn't assuming anything.",
    "start": "1284830",
    "end": "1290039"
  },
  {
    "text": "So Wald was a good guy. And what his identity says is,\nand the trouble with his",
    "start": "1290040",
    "end": "1297530"
  },
  {
    "text": "identity, is you look at\nit, and you blink. The expected value of\ne to the r s sub j.",
    "start": "1297530",
    "end": "1307950"
  },
  {
    "text": "s sub j is the value of the\nrandom walk at the time when",
    "start": "1307950",
    "end": "1313519"
  },
  {
    "text": "you cross a threshold minus\nthe time at which you've crossed a threshold\ntimes gamma of r.",
    "start": "1313520",
    "end": "1321510"
  },
  {
    "text": "So when you take the expected\nvalue of e to the this, you're averaging over j over the time\nthat you crossed the",
    "start": "1321510",
    "end": "1330410"
  },
  {
    "text": "threshold, and also at the value\nat which you crossed the threshold, so you're averaging\nover both of these things.",
    "start": "1330410",
    "end": "1337500"
  },
  {
    "text": "And Wald says this expectation\nnot is less than or equal to 1, but it's exactly 1, and it's\nexactly 1 for every r",
    "start": "1337500",
    "end": "1347870"
  },
  {
    "text": "between r minus and r plus. So it's a very surprising\nresult.",
    "start": "1347870",
    "end": "1354690"
  },
  {
    "text": "Yes? AUDIENCE: Can you\nplease explain why j cannot be defective? I don't really see it. PROFESSOR: Oh, it's because\nwe were looking at two",
    "start": "1354690",
    "end": "1362159"
  },
  {
    "text": "thresholds. If we only had one threshold,\nthen it could be defective. Since we're looking at two\nthresholds, you keep adding",
    "start": "1362160",
    "end": "1369700"
  },
  {
    "text": "random variables in, and the\nsum starts to have a larger and larger variance.",
    "start": "1369700",
    "end": "1375650"
  },
  {
    "text": "Now, even with a large variance,\nyou're not sure that you crossed a threshold,\nbut you see why you",
    "start": "1375650",
    "end": "1381090"
  },
  {
    "text": "must cross a threshold. Yes? AUDIENCE: If the MGF is defined\nat r minus, r plus,",
    "start": "1381090",
    "end": "1386475"
  },
  {
    "text": "then is that also [INAUDIBLE]\nquality? PROFESSOR: Yes.",
    "start": "1386475",
    "end": "1392690"
  },
  {
    "text": "Oh, if it's defined at r plus. ",
    "start": "1392690",
    "end": "1399240"
  },
  {
    "text": "I don't know. I don't remember, and I would\nhave to think about it hard. ",
    "start": "1399240",
    "end": "1407830"
  },
  {
    "text": "Funny things happen right at the\nends of where these moment generating functions are\ndefined, and you'll see why",
    "start": "1407830",
    "end": "1415070"
  },
  {
    "text": "when we prove it.  I can give you a clue as to how\nwe're going to prove it.",
    "start": "1415070",
    "end": "1422450"
  },
  {
    "text": "What we're going to do is, for\nthis random variable x, we're",
    "start": "1422450",
    "end": "1428860"
  },
  {
    "text": "going to define another random\nvariable which has the same distribution as x except\nit's tilted.",
    "start": "1428860",
    "end": "1438070"
  },
  {
    "text": "For large values of x, you\nmultiply it by e to the rx.",
    "start": "1438070",
    "end": "1443250"
  },
  {
    "text": "For small values of\nx, you multiply it by e to the rx also. But if r is positive, that\nmeans the positive values",
    "start": "1443250",
    "end": "1450960"
  },
  {
    "text": "could shifted up, and the small\nvalues get shifted down.",
    "start": "1450960",
    "end": "1457000"
  },
  {
    "text": " So you're taking some of the\ndensity that looks like this,",
    "start": "1457000",
    "end": "1463560"
  },
  {
    "text": "and when you shift it to this\ntilted value, you're shifting",
    "start": "1463560",
    "end": "1469640"
  },
  {
    "text": "the whole thing upward. When r is negative,\nyou're shifting the whole thing downward.",
    "start": "1469640",
    "end": "1476330"
  },
  {
    "text": "Now, what this says is that\ntilted random variable, when",
    "start": "1476330",
    "end": "1483220"
  },
  {
    "text": "it crosses the threshold, the\ntime of crossing the threshold is still a random variable.",
    "start": "1483220",
    "end": "1488640"
  },
  {
    "text": "You will see that this simply\nsays that the expected value",
    "start": "1488640",
    "end": "1493660"
  },
  {
    "text": "of that tilted random variable\nis equal to-- it says that tilted random\nvariable is, in fact, the",
    "start": "1493660",
    "end": "1500559"
  },
  {
    "text": "random variable. It's not defective. And it's the same argument as\nbefore, that has a finite",
    "start": "1500560",
    "end": "1505600"
  },
  {
    "text": "variance, and therefore, since\nit has a finite variance, it keeps expanding. It will cross one of the\nthresholds eventually.",
    "start": "1505600",
    "end": "1513960"
  },
  {
    "text": "OK, so the other thing you can\ndo here is to say, suppose",
    "start": "1513960",
    "end": "1521039"
  },
  {
    "text": "instead of crossing a threshold,\nyou just fix this stopping rule to say we'll\nstop at time 100.",
    "start": "1521040",
    "end": "1529350"
  },
  {
    "text": "If you stop at time 100, then\nwhat this says is expected value of e to the r100 minus\n100 times gamma of",
    "start": "1529350",
    "end": "1538745"
  },
  {
    "text": "r is equal to 1. But that's obvious because the\nexpected value of e to the r",
    "start": "1538745",
    "end": "1544690"
  },
  {
    "text": "is j is, in fact-- ",
    "start": "1544690",
    "end": "1554029"
  },
  {
    "text": "it's j times the expected value\nof rx, so then you're subtracting off j times\nthe log of the",
    "start": "1554030",
    "end": "1562410"
  },
  {
    "text": "expected value of rx. So it's a trivial identity\nif x is fixed.",
    "start": "1562410",
    "end": "1570080"
  },
  {
    "text": "OK, so Wald's identity\nsays this.",
    "start": "1570080",
    "end": "1575929"
  },
  {
    "text": "Let's see what it means in terms\nof crossing a threshold. We'll assume both thresholds\nare there.",
    "start": "1575930",
    "end": "1582290"
  },
  {
    "text": "Incidentally, Wald's identity\nis valid in a much broader range of circumstances than\njust where you have two",
    "start": "1582290",
    "end": "1590309"
  },
  {
    "text": "thresholds, and you're looking\nat a threshold crossing. It's just that's a particularly\nvaluable form of",
    "start": "1590310",
    "end": "1601680"
  },
  {
    "text": "the Wald identity. So that's the only thing\nwe're going to use.",
    "start": "1601680",
    "end": "1607350"
  },
  {
    "text": "But now, if we assume further\nthat this random variable x has a negative expectation,\nwhen x has a negative",
    "start": "1607350",
    "end": "1615820"
  },
  {
    "text": "expectation, gamma of r\nstarts off going down. Usually, it comes\nback up again.",
    "start": "1615820",
    "end": "1623300"
  },
  {
    "text": "We're going to assume that this\nquantity r star here,",
    "start": "1623300",
    "end": "1632000"
  },
  {
    "text": "where it crosses 0 again, we're\ngoing to assume there is",
    "start": "1632000",
    "end": "1642400"
  },
  {
    "text": "some value of r, for which\ngamma of r star equals 0. Mainly, we're going to assume\nthis typical case in which it",
    "start": "1642400",
    "end": "1650279"
  },
  {
    "text": "comes back up and crosses\nthe 0 point.",
    "start": "1650280",
    "end": "1655980"
  },
  {
    "text": "And in that case, what it says\nis the probability that sj is",
    "start": "1655980",
    "end": "1663600"
  },
  {
    "text": "greater than or equal to alpha\nis just less than or equal to e to the minus r star\ntimes alpha.",
    "start": "1663600",
    "end": "1670320"
  },
  {
    "text": "Very, very simple bound\nat this point. You look at this, and you sort\nof see why we're looking, now,",
    "start": "1670320",
    "end": "1677130"
  },
  {
    "text": "not at r in general,\nbut just r star. At r star, gamma of r\nstar is equal to 0.",
    "start": "1677130",
    "end": "1685360"
  },
  {
    "text": "So this term goes away, so we're\nonly talking about the expected value of e to the r\nsj, e to the r star sj is",
    "start": "1685360",
    "end": "1695670"
  },
  {
    "text": "equal to 1. So let's see what happens. We know that e to the r star sj\nis greater than or equal to",
    "start": "1695670",
    "end": "1704080"
  },
  {
    "text": "0 for all values of sj because\ne to the anything real is",
    "start": "1704080",
    "end": "1715159"
  },
  {
    "text": "going to be positive. OK, since e to the r star sj is\ngreater than or equal to 0,",
    "start": "1715160",
    "end": "1721669"
  },
  {
    "text": "what we can do is break this\nexpected value here, this term is 0, now, remember, break\nit into two terms.",
    "start": "1721670",
    "end": "1729760"
  },
  {
    "text": "Break it into the term where s\nsub j is bigger than alpha, and break it into the\nterm where s sub j",
    "start": "1729760",
    "end": "1740059"
  },
  {
    "text": "is less than beta. So I'm just going to ignore the\ncase where it's less than or equal to beta.",
    "start": "1740060",
    "end": "1745970"
  },
  {
    "text": "I'm going to take this\nexpected value. I'm going to write it as the\nprobability that s sub j is",
    "start": "1745970",
    "end": "1751360"
  },
  {
    "text": "greater than or equal to alpha\ntimes e times the expected value of either the r star s\nsub j given s sub j greater",
    "start": "1751360",
    "end": "1759750"
  },
  {
    "text": "than or equal to alpha. There should be another term\nin here to make this equal,",
    "start": "1759750",
    "end": "1765740"
  },
  {
    "text": "and that's the probability that\ns sub j is less than or equal to beta times e to the r\nstar s sub j, given that s sub",
    "start": "1765740",
    "end": "1773450"
  },
  {
    "text": "j is less than or\nequal to beta. We're going to ignore that, and\nthat's why we get the less",
    "start": "1773450",
    "end": "1779120"
  },
  {
    "text": "than or equal to 1 here. Now, you can lower bound\ne to the r star sj under this condition.",
    "start": "1779120",
    "end": "1787060"
  },
  {
    "text": "What's a lower bound to s sub\nj given that s sub j is greater than or equal\nto alpha?",
    "start": "1787060",
    "end": "1793351"
  },
  {
    "text": " Alpha.",
    "start": "1793352",
    "end": "1798510"
  },
  {
    "text": "OK, we're looking at all cases\nwhere s sub j is greater than or equal to alpha, and we're\ngoing to stop this experiment",
    "start": "1798510",
    "end": "1808200"
  },
  {
    "text": "at the point where it\nfirst exceeds alpha. So we're going to lower bound\nthe point where it first",
    "start": "1808200",
    "end": "1813280"
  },
  {
    "text": "exceeds alpha by alpha itself,\nso this quantity is lower",
    "start": "1813280",
    "end": "1820190"
  },
  {
    "text": "bounded, again, by taking the\nprobability that s sub j greater than or equal to alpha\ntimes e to the r star alpha,",
    "start": "1820190",
    "end": "1828980"
  },
  {
    "text": "and that whole thing is less\nthan or equal to 1. That says the probability that\nsj is greater than or equal to",
    "start": "1828980",
    "end": "1835380"
  },
  {
    "text": "alpha is less than or equal to\ne to the minus r star alpha,",
    "start": "1835380",
    "end": "1844590"
  },
  {
    "text": "which is what this inequality\nsays here. OK, so this is not\nrocket science.",
    "start": "1844590",
    "end": "1852290"
  },
  {
    "text": "This is a fairly simple result\nif you believe in Wald's",
    "start": "1852290",
    "end": "1857910"
  },
  {
    "text": "identity, which we'll\nprove later. OK, so it's valid\nfor all choices",
    "start": "1857910",
    "end": "1862970"
  },
  {
    "text": "of this lower threshold. And remember, this probability\nhere, it doesn't look like",
    "start": "1862970",
    "end": "1870260"
  },
  {
    "text": "it's a function of both alpha\nand beta, but it is because you're asking what's the\nprobability that you cross the",
    "start": "1870260",
    "end": "1881370"
  },
  {
    "text": "threshold alpha before you\ncross the threshold beta.",
    "start": "1881370",
    "end": "1886380"
  },
  {
    "text": "And if you make beta very, very\nlarge, it makes it more likely that you're going\nto cross the threshold.",
    "start": "1886380",
    "end": "1891780"
  },
  {
    "text": "If you make beta very close to\n0, then you're probably going to cross beta first, so this\ninequality here, this quantity",
    "start": "1891780",
    "end": "1902750"
  },
  {
    "text": "here, depends on beta also. But we know that this inequality\nis valid no matter",
    "start": "1902750",
    "end": "1907960"
  },
  {
    "text": "what beta is, so we can let beta\napproach minus infinity, and we can still have\nthis inequality.",
    "start": "1907960",
    "end": "1914700"
  },
  {
    "text": "There's a little bit tricky\nmath involved in that. There's an exercise in the text\nwhich goes through that",
    "start": "1914700",
    "end": "1922800"
  },
  {
    "text": "slightly tricky math, but what\nyou find is that this bound is valid with only one threshold,\nas well as with two",
    "start": "1922800",
    "end": "1929920"
  },
  {
    "text": "thresholds. But this proof here that we've\ngiven depends on a lower",
    "start": "1929920",
    "end": "1935009"
  },
  {
    "text": "threshold, which is somewhere. We don't care where. Valid for all choices of\nbeta, so it's valid",
    "start": "1935010",
    "end": "1941830"
  },
  {
    "text": "without a lower threshold. The probability that the union\noverall n of sn less than or",
    "start": "1941830",
    "end": "1950590"
  },
  {
    "text": "equal to alpha. In other words, the probability\nthat we ever crossed a threshold alpha-- AUDIENCE: It's not true equal.",
    "start": "1950590",
    "end": "1956429"
  },
  {
    "text": "PROFESSOR: What? AUDIENCE: It's supposed to\nbe sn larger [INAUDIBLE] as the last time? PROFESSOR: It's less than or\nequal to e to the minus r star",
    "start": "1956430",
    "end": "1963940"
  },
  {
    "text": "alpha, which is-- AUDIENCE: Oh, sn? sn? PROFESSOR: n.",
    "start": "1963940",
    "end": "1970194"
  },
  {
    "text": "AUDIENCE: You just [INAUDIBLE] [? the quantity? ?] PROFESSOR: Oh, it's a union\noverall n greater than or equal to 1.",
    "start": "1970194",
    "end": "1975290"
  },
  {
    "text": " OK, in other words, this\nquantity we're dealing with",
    "start": "1975290",
    "end": "1983049"
  },
  {
    "text": "here is the probability\nthat sn---",
    "start": "1983050",
    "end": "1988581"
  },
  {
    "text": "oh, I see what you're saying. This quantity here\nshould be greater",
    "start": "1988581",
    "end": "1994320"
  },
  {
    "text": "than or equal to alpha. You're right. Sorry about that.",
    "start": "1994320",
    "end": "2001289"
  },
  {
    "text": "I think it's right\nmost places. Yes, it's right. We have it right here. ",
    "start": "2001290",
    "end": "2012210"
  },
  {
    "text": "The probability of this union\nis really the same as the probability that the value of\nit, after it crosses the",
    "start": "2012210",
    "end": "2019929"
  },
  {
    "text": "threshold, is greater than\nor equal to alpha. ",
    "start": "2019930",
    "end": "2025910"
  },
  {
    "text": "OK, now, we saw before that the\nprobability that s sub n",
    "start": "2025910",
    "end": "2031150"
  },
  {
    "text": "is greater than or\nequal to alpha. Excuse me, that's the same.",
    "start": "2031150",
    "end": "2036750"
  },
  {
    "text": " When you're writing things in\nLaTeX, the symbol for less",
    "start": "2036750",
    "end": "2044029"
  },
  {
    "text": "than or equal to is so similar\nto that for greater than or equal to that's hard to\nkeep them straight.",
    "start": "2044030",
    "end": "2049109"
  },
  {
    "text": "That quantity there is a greater\nthan or equal to sign,",
    "start": "2049110",
    "end": "2054239"
  },
  {
    "text": "if you're going from right to\nleft instead of right to left. So all we're doing here is\nsimply using this, well,",
    "start": "2054239",
    "end": "2062658"
  },
  {
    "text": "greater than or equal to. OK, the corollary makes a\nstronger and cleaner statement",
    "start": "2062659",
    "end": "2067949"
  },
  {
    "text": "that the probability that you\never cross alpha is less than",
    "start": "2067949",
    "end": "2075629"
  },
  {
    "text": "or equal to-- my heavens, my evil twin got\na hold of these slides.",
    "start": "2075630",
    "end": "2082909"
  },
  {
    "text": " And let me rewrite that one.",
    "start": "2082910",
    "end": "2091760"
  },
  {
    "text": "The probability that the union\noverall n of the event s sub n",
    "start": "2091760",
    "end": "2102620"
  },
  {
    "text": "greater than or equal to alpha\nis less than or equal to e to",
    "start": "2102620",
    "end": "2109425"
  },
  {
    "text": "the minus r star alpha. OK, so we've seen from the\nChernoff bound that for every",
    "start": "2109425",
    "end": "2117350"
  },
  {
    "text": "n this bound has satisfied, this\nsays that it's not only satisfied for each n, but it\nsays it's satisfied overall n",
    "start": "2117350",
    "end": "2125740"
  },
  {
    "text": "collectively. Otherwise, if we were using the\nChernoff bound, what would we have to do to get a handle\non this quantity?",
    "start": "2125740",
    "end": "2133420"
  },
  {
    "text": "We'd have to use the union\nbound, and then when we use the union bound, we can show\nthat for every n, the",
    "start": "2133420",
    "end": "2141950"
  },
  {
    "text": "probability that sn is greater\nthan or equal to alpha is less than or equal to\nthis quantity. But then we'd have to add all\nthose terms, and we would have",
    "start": "2141950",
    "end": "2149339"
  },
  {
    "text": "to somehow diddle around with\nthem to show that there are only a few of them which are\nclose to this value, and all",
    "start": "2149340",
    "end": "2157760"
  },
  {
    "text": "the rest are negligible. And the number that are close to\nthat value is only growing",
    "start": "2157760",
    "end": "2164420"
  },
  {
    "text": "with n and goes through\na lot of headache. Here, we don't have to do this\nanymore because the Wald",
    "start": "2164420",
    "end": "2171460"
  },
  {
    "text": "identity has saved is from\nall that difficulty. ",
    "start": "2171460",
    "end": "2177780"
  },
  {
    "text": "OK, we talked about\nthe G/G/1 queue.",
    "start": "2177780",
    "end": "2184100"
  },
  {
    "text": "We're going to apply this\ncorollary to the G/G/1 queue to the queueing time, namely to\nthe time w sub i that the",
    "start": "2184100",
    "end": "2193000"
  },
  {
    "text": "i's arrival spends in\nthe queue before starting to be served.",
    "start": "2193000",
    "end": "2198420"
  },
  {
    "text": "You remember, when we looked at\nthat, we found that if we define u sub i to be equal to\nthe ith interarrival time",
    "start": "2198420",
    "end": "2208850"
  },
  {
    "text": "minus the i minus first service\ntime, those are independent of each other,\nso this is the",
    "start": "2208850",
    "end": "2215640"
  },
  {
    "text": "difference between those. So ui is the difference between\nthe i's arrival time",
    "start": "2215640",
    "end": "2222100"
  },
  {
    "text": "and the previous service time. What we showed was that this\nsequence, u sub i, the",
    "start": "2222100",
    "end": "2229280"
  },
  {
    "text": "sequence of the sums of u sub\ni as a modification of a random walk.",
    "start": "2229280",
    "end": "2236440"
  },
  {
    "text": "In other words, the sums of\nthe u sub i behave exactly like a random walk does, but\nevery time it gets down to 0,",
    "start": "2236440",
    "end": "2244280"
  },
  {
    "text": "if it crosses 0, it\nresets to 0 again. So it keeps bouncing up again.",
    "start": "2244280",
    "end": "2250700"
  },
  {
    "text": "If you look in the text, what\nit shows is that if you look",
    "start": "2250700",
    "end": "2256970"
  },
  {
    "text": "at this sequence of u sub i's,\nand you look at the sum of them, and you look at them\nbackward, if you look at the",
    "start": "2256970",
    "end": "2263799"
  },
  {
    "text": "sum of u sub i plus u sub i\nminus 1 plus u sub i minus 2,",
    "start": "2263800",
    "end": "2270380"
  },
  {
    "text": "and so forth, when you look at\nthe sum that way, it actually",
    "start": "2270380",
    "end": "2276769"
  },
  {
    "text": "becomes a random walk. Therefore, we can apply this\nbound to the random walk, and",
    "start": "2276770",
    "end": "2283290"
  },
  {
    "text": "what we find is that the\nprobability that the waiting time of n queue, of the nth\ncustomer, is probability that",
    "start": "2283290",
    "end": "2298010"
  },
  {
    "text": "it's greater than or equal to\nan arbitrary number alpha is less than or equal to the\nprobability that w sub",
    "start": "2298010",
    "end": "2305150"
  },
  {
    "text": "infinity is greater than or\nequal to alpha, and it's less than e to the minus\nr star alpha.",
    "start": "2305150",
    "end": "2311609"
  },
  {
    "text": "So again, all you have to do is\nyou have this inner arrival time x, you have this service\ntime y, you take the",
    "start": "2311610",
    "end": "2319390"
  },
  {
    "text": "difference of the two, that's a\nrandom variable, you find a moment generating function of\nthat random variable, you find",
    "start": "2319390",
    "end": "2326450"
  },
  {
    "text": "the point of r star at which\nthat moment generating function equals 1, and then\nthe bound says that the",
    "start": "2326450",
    "end": "2333580"
  },
  {
    "text": "probability that the queueing\ntime that you're going to be dealing with is less than\nor equal to this",
    "start": "2333580",
    "end": "2340840"
  },
  {
    "text": "quantity alpha here. Yes? AUDIENCE: What do you work with\nwhen you have the gamma",
    "start": "2340840",
    "end": "2346016"
  },
  {
    "text": "function go like this, and thus\nhave infinity, and you cross it there. [INAUDIBLE] points that\nwe're looking for?",
    "start": "2346016",
    "end": "2352610"
  },
  {
    "text": "PROFESSOR: For that, you\nhave to read the text. I mean, effectively, you can\nthink of it just as if gamma",
    "start": "2352610",
    "end": "2360240"
  },
  {
    "text": "of r is a convex function\nlike anything else. It just has a discontinuity in\nit, and bingo, it shoots off",
    "start": "2360240",
    "end": "2368840"
  },
  {
    "text": "to infinity. So when you take these slope\narguments, what happens is that for all slopes beyond that\npoint, they just seesaw",
    "start": "2368840",
    "end": "2377140"
  },
  {
    "text": "around at one point. But the same bound holds. ",
    "start": "2377140",
    "end": "2384020"
  },
  {
    "text": "OK, so that's the\nKingman bound. ",
    "start": "2384020",
    "end": "2390240"
  },
  {
    "text": "Then we talked about\nlarge deviations for hypothesis test. Well, actually we just talked\nabout hypothesis test, but not",
    "start": "2390240",
    "end": "2398340"
  },
  {
    "text": "large deviation for them. Let's review where\nwe were on that.",
    "start": "2398340",
    "end": "2405839"
  },
  {
    "text": "Let's let the vector y be an n\ntuple of IID random variables,",
    "start": "2405840",
    "end": "2414870"
  },
  {
    "text": "y1 up to y sub n. They're IID conditional\non hypothesis 0.",
    "start": "2414870",
    "end": "2421050"
  },
  {
    "text": "They're also IID conditional on\nhypothesis 1, so the game is nature chooses either\nhypothesis 0 or hypothesis 1.",
    "start": "2421050",
    "end": "2431965"
  },
  {
    "text": " You take n samples of some IID\nrandom variable, and those n",
    "start": "2431965",
    "end": "2440590"
  },
  {
    "text": "samples are IID conditional on\neither nature choosing 0 or nature choosing 1.",
    "start": "2440590",
    "end": "2445870"
  },
  {
    "text": "At the end of choosing those n\nsamples, you're supposed to guess whether h0 is the right\nhypothesis or 1 is a right",
    "start": "2445870",
    "end": "2454070"
  },
  {
    "text": "hypothesis. Invest in Apple stock 10 years\nago, and one hypothesis is",
    "start": "2454070",
    "end": "2459790"
  },
  {
    "text": "it's going to go broke. The other hypothesis is it's\ngoing to invent marvelous things, and your stock will\ngo up by a factor of 50.",
    "start": "2459790",
    "end": "2468030"
  },
  {
    "text": "You take some samples, you make\nyour decision on that. Fortunately, with that, you can\nmake a separate decision",
    "start": "2468030",
    "end": "2475210"
  },
  {
    "text": "each year, but that's the\nkind of thing that we're talking about. We're just restricting it to\nthis case where you have n",
    "start": "2475210",
    "end": "2484790"
  },
  {
    "text": "sample values that you're taking\none after the other, and they're all IID when the\nparticular value of the",
    "start": "2484790",
    "end": "2491380"
  },
  {
    "text": "hypothesis that happens\nto be there. OK, so we said there\nis something called",
    "start": "2491380",
    "end": "2497830"
  },
  {
    "text": "a likelihood ratio. The likelihood ratio for a\nparticular sequence y is",
    "start": "2497830",
    "end": "2505609"
  },
  {
    "text": "lambda of y is equal to the\ndensity of y given h1 divided",
    "start": "2505610",
    "end": "2513490"
  },
  {
    "text": "by the density of y given h0. Why is it h1 on the top\nand h0 on the bottom?",
    "start": "2513490",
    "end": "2520549"
  },
  {
    "text": "Purely convention,\nnothing else. The only thing that\ndistinguishes hypothesis 1",
    "start": "2520550",
    "end": "2525599"
  },
  {
    "text": "from hypothesis 0 is you choose\none and call it 1, and",
    "start": "2525600",
    "end": "2530940"
  },
  {
    "text": "you choose the other\nand call it 0. Doesn't make any difference\nhow you do it.",
    "start": "2530940",
    "end": "2536900"
  },
  {
    "text": "So after we make that choice,\nthe likelihood ratio is that ratio.",
    "start": "2536900",
    "end": "2542869"
  },
  {
    "text": "Now, the reason for using semi\ninvariant moment generating functions is that this\ndensity here is",
    "start": "2542870",
    "end": "2551170"
  },
  {
    "text": "a product of densities. This density is a product of\ndensities, and therefore when",
    "start": "2551170",
    "end": "2556710"
  },
  {
    "text": "you take the log of this ratio\nof products, you get the sum",
    "start": "2556710",
    "end": "2564380"
  },
  {
    "text": "from i equals 1 to n of this log\nlikelihood ratio for just",
    "start": "2564380",
    "end": "2571809"
  },
  {
    "text": "a single experiment. It's a single experiment that\nyou're taking based on the",
    "start": "2571810",
    "end": "2578520"
  },
  {
    "text": "fact that all n experiments\nare based on the same hypothesis, either h0 or h1.",
    "start": "2578520",
    "end": "2585290"
  },
  {
    "text": "So the game that you're playing,\nand please remember what the game is if you forget\neverything else about this",
    "start": "2585290",
    "end": "2591309"
  },
  {
    "text": "game, is the hypothesis gets\nchosen, and at the same time,",
    "start": "2591310",
    "end": "2596350"
  },
  {
    "text": "you take n sample values. All n sample values correspond\nto the same value of the",
    "start": "2596350",
    "end": "2602830"
  },
  {
    "text": "hypothesis. OK, so when you do that, we're\ngoing to call z sub i, this",
    "start": "2602830",
    "end": "2609010"
  },
  {
    "text": "logarithm here, this log\nlikelihood ratio. And then we showed last time\nthat a threshold test is--",
    "start": "2609010",
    "end": "2617370"
  },
  {
    "text": "well, we define the threshold\ntest as comparing the sum with",
    "start": "2617370",
    "end": "2623250"
  },
  {
    "text": "the logarithm of a threshold. And the threshold is equal to\np0 over p sub 1, if in fact",
    "start": "2623250",
    "end": "2631630"
  },
  {
    "text": "you're doing a maximum a\nposteriori probability test, and p0 and p1 are the\nprobabilities of hypothesis.",
    "start": "2631630",
    "end": "2641529"
  },
  {
    "text": "Remember how we did that. It was a very simple thing. You just write out what the\nprobability is of hypothesis 0",
    "start": "2641530",
    "end": "2649349"
  },
  {
    "text": "and a sequence of\nn values of y. You write out what the\nprobability is of hypotheses 1",
    "start": "2649350",
    "end": "2656260"
  },
  {
    "text": "and that same sequence of values\nwith the appropriate",
    "start": "2656260",
    "end": "2662360"
  },
  {
    "text": "probability on that sequence for\nh equals 1 and h equals 0.",
    "start": "2662360",
    "end": "2670880"
  },
  {
    "text": "And what you get out of that\nis that the threshold test",
    "start": "2670880",
    "end": "2677230"
  },
  {
    "text": "sums up all the z sub i's,\ncompares it with the threshold, and makes a choice,\nand that is the map choice.",
    "start": "2677230",
    "end": "2684990"
  },
  {
    "text": "OK, so conditional on h0, you're\ngoing to make an error",
    "start": "2684990",
    "end": "2690520"
  },
  {
    "text": "if the sum of the z sub i's\nis greater than the logarithm of eta.",
    "start": "2690520",
    "end": "2697500"
  },
  {
    "text": "And conditional on h1, you're\ngoing to make an error if the sum is less than or\nequal to log eta.",
    "start": "2697500",
    "end": "2705450"
  },
  {
    "text": "I denote these as the random\nvariable z sub i 0 to make",
    "start": "2705450",
    "end": "2711540"
  },
  {
    "text": "sure that you recognize that\nthis random variable here is conditional on h0 in this case,\nand it's conditional on",
    "start": "2711540",
    "end": "2720700"
  },
  {
    "text": "h1 in the opposite case. ",
    "start": "2720700",
    "end": "2726680"
  },
  {
    "text": "OK, so the exponential bound\nfor z sub i sub 0--",
    "start": "2726680",
    "end": "2732569"
  },
  {
    "text": "OK, so what we're doing now is\nwe're saying, OK, suppose that 0 is the actual value\nof this hypothesis.",
    "start": "2732570",
    "end": "2740900"
  },
  {
    "text": "0 is the value of\nthe hypothesis. The experimenter doesn't\nknow this.",
    "start": "2740900",
    "end": "2746700"
  },
  {
    "text": "What the experimenter does is\ndoes what the experimenter has been told to do, namely the\nexperimenter take these n",
    "start": "2746700",
    "end": "2753910"
  },
  {
    "text": "values, y1 up to y sub n, finds\nthe likelihood ratio,",
    "start": "2753910",
    "end": "2759089"
  },
  {
    "text": "compares that likelihood ratio\nwith the threshold, and if the threshold is larger than the\nthreshold, it decides 1.",
    "start": "2759090",
    "end": "2768369"
  },
  {
    "text": "If it's smaller than the\nthreshold, that decides opposite thing.",
    "start": "2768370",
    "end": "2774060"
  },
  {
    "text": "It decides 1 if it's above\nthe threshold, 0 if it's below the threshold. ",
    "start": "2774060",
    "end": "2786390"
  },
  {
    "text": "Well, first thing we want to do,\nthen, is to find the log likelihood ratio under the\nassumption that 0 is the",
    "start": "2786390",
    "end": "2796050"
  },
  {
    "text": "correct hypothesis, and\nsomething very remarkable happens here.",
    "start": "2796050",
    "end": "2801420"
  },
  {
    "text": "Gamma sub 0 of r is now the\nlogarithm because it's a semi",
    "start": "2801420",
    "end": "2806980"
  },
  {
    "text": "invariant moment generating\nfunction of the expected value",
    "start": "2806980",
    "end": "2812190"
  },
  {
    "text": "of this quantity of e to\nthe r times z sub i.",
    "start": "2812190",
    "end": "2817911"
  },
  {
    "text": "When we take the expected value,\nwe integrate over f of y given h0 times e to the r\ntimes log of f of y given h1",
    "start": "2817912",
    "end": "2827692"
  },
  {
    "text": "over f of y given h0. You look at this, and\nwhat do you get?",
    "start": "2827692",
    "end": "2832920"
  },
  {
    "text": "This quantity here is e\nto the r times log of f of y given h1.",
    "start": "2832920",
    "end": "2838920"
  },
  {
    "text": "That whole quantity in there\nis just f of y given h1 to the rth power.",
    "start": "2838920",
    "end": "2846829"
  },
  {
    "text": "So what we have is, in this\nquantity here, is f of y given",
    "start": "2846830",
    "end": "2853270"
  },
  {
    "text": "h0 to the minus r power. So this term combined with\nthis term gives us f of 1",
    "start": "2853270",
    "end": "2860100"
  },
  {
    "text": "minus r of y given h0, and this\nquantity here is f to the",
    "start": "2860100",
    "end": "2865890"
  },
  {
    "text": "r of y given h1 dy. So the semi invariant moment\ngenerating function is this",
    "start": "2865890",
    "end": "2875339"
  },
  {
    "text": "quantity here. At r equals 1, this is just f\nof y given h1, so the log of",
    "start": "2875340",
    "end": "2882910"
  },
  {
    "text": "it is equal to 0. So what we're saying is that,\nfor any old detection problem",
    "start": "2882910",
    "end": "2890420"
  },
  {
    "text": "in the world, so long as this\nmoment generating function exists, what happens is it\nstarts at 0, it comes down,",
    "start": "2890420",
    "end": "2900560"
  },
  {
    "text": "comes back up again, and\nr star is equal to 1.",
    "start": "2900560",
    "end": "2906750"
  },
  {
    "text": "That's what we've just shown. When r is equal to 1, this whole\nthing is equal to 1, so",
    "start": "2906750",
    "end": "2913140"
  },
  {
    "text": "the log of 1 is equal to 0. For every one of these problems,\nyou know where this",
    "start": "2913140",
    "end": "2918680"
  },
  {
    "text": "intercept is, you know where\nthis intercept is, one is at 0, one is at 1.",
    "start": "2918680",
    "end": "2923950"
  },
  {
    "start": "2923950",
    "end": "2939190"
  },
  {
    "text": "What we're going to do now is\ntry to find out what the probability of error is given\nthat h is 0, h equals 0, is",
    "start": "2939190",
    "end": "2950020"
  },
  {
    "text": "the correct hypothesis. So we're assuming that the\nprobabilities are actually f",
    "start": "2950020",
    "end": "2956010"
  },
  {
    "text": "of y given h0. We calculate this quantity that\nlooks like this, and we",
    "start": "2956010",
    "end": "2962450"
  },
  {
    "text": "ask what is the probability\nthat this sum of random",
    "start": "2962450",
    "end": "2972290"
  },
  {
    "text": "variables exceeds the threshold,\nexceeds the threshold eta.",
    "start": "2972290",
    "end": "2977750"
  },
  {
    "text": "So the thing that we do is we\ndraw a line, a slope, natural log of eta divided by eta.",
    "start": "2977750",
    "end": "2984840"
  },
  {
    "text": "We draw that slope along here,\nand we find that the probability of error is upper\nbounded by gamma 0 of this",
    "start": "2984840",
    "end": "2994120"
  },
  {
    "text": "quantity, defined by the slope,\nminus r0 times log of eta divided by eta.",
    "start": "2994120",
    "end": "3001850"
  },
  {
    "text": "That's all there is to it. Any questions about that? ",
    "start": "3001850",
    "end": "3008450"
  },
  {
    "text": "Seem obvious? Seem strange? ",
    "start": "3008450",
    "end": "3014820"
  },
  {
    "text": "OK, so the probability of r\nconditional on h equals 0 is e",
    "start": "3014820",
    "end": "3022760"
  },
  {
    "text": "to the n times gamma 0 of\nr0 minus r0, natural log of eta over eta.",
    "start": "3022760",
    "end": "3028940"
  },
  {
    "text": "And ql of eta is the probability\nof error given that h is equal to l.",
    "start": "3028940",
    "end": "3036790"
  },
  {
    "text": "OK, we can do the same thing\nfor hypothesis 1.",
    "start": "3036790",
    "end": "3043190"
  },
  {
    "text": "We're asking what's the\nprobability of error given that h equals 1 is the correct\nhypothesis, and given that we",
    "start": "3043190",
    "end": "3051040"
  },
  {
    "text": "choose a threshold, say\nwe know the a priori probabilities, so we choose\na threshold that way.",
    "start": "3051040",
    "end": "3058180"
  },
  {
    "text": "OK, we go through the same\nargument, z1 of s is the natural log of f of y given F1\ntimes e to the s, we're using",
    "start": "3058180",
    "end": "3068760"
  },
  {
    "text": "s in place of r here, times\nthe natural log of f of y",
    "start": "3068760",
    "end": "3074150"
  },
  {
    "text": "given h1 over f of y given h0. And this quantity, now, f of y\ngiven h1, the f of y given h1",
    "start": "3074150",
    "end": "3085380"
  },
  {
    "text": "is upstairs, so we have f of\n1 plus s of y given h1.",
    "start": "3085380",
    "end": "3091509"
  },
  {
    "text": "This quantity is down here,\nso we have f of minus s of y given h0.",
    "start": "3091510",
    "end": "3098210"
  },
  {
    "text": "And we notice that when s is\nequal to minus 1, this is again equal to 0, and we notice\nalso, if you compare",
    "start": "3098210",
    "end": "3107220"
  },
  {
    "text": "this, gamma 1 of s is equal\nto gamma 0 of r minus 1.",
    "start": "3107220",
    "end": "3113349"
  },
  {
    "text": "These two functions are the\nsame, just shifts it by one. OK, so this one of the very\nstrange things about",
    "start": "3113350",
    "end": "3122680"
  },
  {
    "text": "hypothesis testing, namely you\nare calculating these expected",
    "start": "3122680",
    "end": "3129069"
  },
  {
    "text": "values, but you're calculating\nthe expected value of a likelihood ratio.",
    "start": "3129070",
    "end": "3134319"
  },
  {
    "text": "And the likelihood ratio\ninvolves the probabilities of the hypotheses also, so when you\ncalculate that ratio, what",
    "start": "3134320",
    "end": "3142470"
  },
  {
    "text": "you get this is funny quantity\nhere, which is related to what you get when you calculate\nthe semi invariant moment",
    "start": "3142470",
    "end": "3150460"
  },
  {
    "text": "generating function given\nthe other hypothesis. So that now, what we wind up\nwith is a gamma 1 of the eta,",
    "start": "3150460",
    "end": "3158599"
  },
  {
    "text": "is e to the n times\ngamma 0 of r0. I'm using the fact that gamma 1\nof s is equal to gamma 0 of",
    "start": "3158600",
    "end": "3166610"
  },
  {
    "text": "r minus 1, s is just r shifted\nover by 1, so I can do the",
    "start": "3166610",
    "end": "3172150"
  },
  {
    "text": "same optimization for each. So what I wind up with is\nthe probability of error",
    "start": "3172150",
    "end": "3178940"
  },
  {
    "text": "conditional on hypothesis 0,\nis this quantity down here.",
    "start": "3178940",
    "end": "3185150"
  },
  {
    "start": "3185150",
    "end": "3191049"
  },
  {
    "text": "That's this one, and the\nprobability of error conditional on the other\nhypothesis, the exponent is",
    "start": "3191050",
    "end": "3198809"
  },
  {
    "text": "equal to this quantity here. OK, so what that says is that\nas you shift the threshold--",
    "start": "3198810",
    "end": "3209480"
  },
  {
    "text": "in other words, suppose instead\nof using a map test,",
    "start": "3209480",
    "end": "3216270"
  },
  {
    "text": "you say, well, I want the\nprobability of error to be small when hypothesis\n0 correct.",
    "start": "3216270",
    "end": "3224050"
  },
  {
    "text": "I want it to be small when\nhypothesis 1 is correct. I have a trade off between\nthose two.",
    "start": "3224050",
    "end": "3230450"
  },
  {
    "text": "How do I choose my threshold in\norder to get the smallest value overall?",
    "start": "3230450",
    "end": "3236609"
  },
  {
    "text": "So you say, well,\nyou're stuck. You have one exponent\nunder hypothesis 0.",
    "start": "3236610",
    "end": "3244400"
  },
  {
    "text": "You have another exponent\nunder hypothesis 1. You have this curve here.",
    "start": "3244400",
    "end": "3250350"
  },
  {
    "text": "You can take whatever value you\nwant over here, and that",
    "start": "3250350",
    "end": "3256570"
  },
  {
    "text": "sticks you with a value here. You can rock things around this\ninverted seesaw, and you",
    "start": "3256570",
    "end": "3266920"
  },
  {
    "text": "can make one probability of\nerror bigger by making the other one smaller, or you make\nthe other one bigger by making",
    "start": "3266920",
    "end": "3274550"
  },
  {
    "text": "the other one smaller. Namely, what you're doing is\nchanging the threshold, and as",
    "start": "3274550",
    "end": "3280820"
  },
  {
    "text": "you change the threshold, as\nyou make the threshold positive, what you're doing is\nmaking it harder to accept h1,",
    "start": "3280820",
    "end": "3290599"
  },
  {
    "text": "h equals 1, and easier\nto accept h equals 0. When you move the threshold the\nother way, you're making",
    "start": "3290600",
    "end": "3297350"
  },
  {
    "text": "it easier the other way. This, in fact, gives you the\nchoice between the two.",
    "start": "3297350",
    "end": "3304410"
  },
  {
    "text": "You decide you're going\nto take n tests. You can make both of these\nsmaller by making n bigger.",
    "start": "3304410",
    "end": "3310650"
  },
  {
    "text": "But there's a trade off between\nthe two, and the trade off is given by this tangent\nline to this curve here.",
    "start": "3310650",
    "end": "3321690"
  },
  {
    "text": "And you're always stuck with\nr star equals 1 and all of these problems.",
    "start": "3321690",
    "end": "3327900"
  },
  {
    "text": "So the only question is what\ndoes this curve look like? Notice that the expected value\nof the likelihood ratio given",
    "start": "3327900",
    "end": "3339350"
  },
  {
    "text": "h equals 0 is negative. The expected value given h\nequals 1 is positive, and",
    "start": "3339350",
    "end": "3351500"
  },
  {
    "text": "that's just because of the form\nof the likelihood ratio. OK, so this actually shows\nthese two exponents.",
    "start": "3351500",
    "end": "3363420"
  },
  {
    "text": "These are the exponents for\nthe two kinds of errors. You can view this as a large\ndeviation form of the Neyman",
    "start": "3363420",
    "end": "3370950"
  },
  {
    "text": "Pearson test. In the Neyman Pearson test,\nyou're doing things in a very",
    "start": "3370950",
    "end": "3376700"
  },
  {
    "text": "detailed way, and you're\ntaking a choice between",
    "start": "3376700",
    "end": "3383079"
  },
  {
    "text": "choosing different thresholds\nto make the probability of error of one type bigger or less\nthan the other one, just",
    "start": "3383080",
    "end": "3392030"
  },
  {
    "text": "the other way. Here, we're looking at the large\ndeviation form of it that becomes an upper bound\nrather than an exact",
    "start": "3392030",
    "end": "3398830"
  },
  {
    "text": "calculation, but it tells you\nmuch, much more because for most of these threshold tests,\nyou're going to do enough",
    "start": "3398830",
    "end": "3408430"
  },
  {
    "text": "experiments that your\nprobability of error is going to be very small. So the only question is where\ndo you really want the error",
    "start": "3408430",
    "end": "3417070"
  },
  {
    "text": "probability to be small? You can make it very small one\nway by shifting the curve this",
    "start": "3417070",
    "end": "3422539"
  },
  {
    "text": "way, and make it very small the\nother way by shifting the curve the other way.",
    "start": "3422540",
    "end": "3428240"
  },
  {
    "text": "And you take your choice\nof which you want.  OK, the a priori probabilities\nare usually not the essential",
    "start": "3428240",
    "end": "3437350"
  },
  {
    "text": "characteristic when you're\ndealing with this large deviation kind of result\nbecause, when you take a large",
    "start": "3437350",
    "end": "3443310"
  },
  {
    "text": "number of tests, this threshold,\nlog eta over eta",
    "start": "3443310",
    "end": "3448360"
  },
  {
    "text": "over n, when n becomes very\nlarge, when you have a large number of experiments,\nlog eta over n",
    "start": "3448360",
    "end": "3456840"
  },
  {
    "text": "becomes relatively small. So that's not the thing you're\nusually concerned with.",
    "start": "3456840",
    "end": "3462070"
  },
  {
    "text": "What you're concerned with is\nwhether one test, the patient dies, and the other tests costs\na lot of money; or one",
    "start": "3462070",
    "end": "3470640"
  },
  {
    "text": "test, the nuclear plant blows\nup, and the other test, you waste a lot of money,\nwhich you wouldn't",
    "start": "3470640",
    "end": "3476450"
  },
  {
    "text": "have had to pay otherwise.  OK, now, here's the important\npart of all of this.",
    "start": "3476450",
    "end": "3489240"
  },
  {
    "text": "So far, it looked like there\nwasn't any way to get out of this trade off between choosing\na threshold to make",
    "start": "3489240",
    "end": "3498910"
  },
  {
    "text": "the error probability small one\nway, or making the error probability small\nthe other way.",
    "start": "3498910",
    "end": "3504700"
  },
  {
    "text": "And you think, well,\nyes, there is a way to get around it. What I should do is what I do\nin real life, namely if I'm",
    "start": "3504700",
    "end": "3512980"
  },
  {
    "text": "trying to decide about\nsomething, what I'm normally going to do, I don't like to\nwaste my time deciding about",
    "start": "3512980",
    "end": "3521690"
  },
  {
    "text": "it, so as soon as the decision\nbecomes relatively straightforward, I\nmake up my mind.",
    "start": "3521690",
    "end": "3527470"
  },
  {
    "text": "If the decision is not\nstraightforward, if I don't have enough evidence, I keep\ndoing more tests, so",
    "start": "3527470",
    "end": "3533780"
  },
  {
    "text": "sequential tests are an obvious\nthing to try to do if you can do it.",
    "start": "3533780",
    "end": "3540080"
  },
  {
    "text": "What we have here, what we've\nshown, is we have two coupled random walks.",
    "start": "3540080",
    "end": "3545480"
  },
  {
    "text": "Given hypothesis h equals 0, we\nhave one random walk, and",
    "start": "3545480",
    "end": "3551119"
  },
  {
    "text": "that random walk is typically\ngoing to go down.",
    "start": "3551120",
    "end": "3556250"
  },
  {
    "text": "Given h equals 1, we have\nanother random walk. That random walk is typically\ngoing to go up.",
    "start": "3556250",
    "end": "3564640"
  },
  {
    "text": "And one is going to go down, one\nis going to go up, because we've defined the random\nvariable involved is a log of",
    "start": "3564640",
    "end": "3571934"
  },
  {
    "text": "f of y given h1 divided by f\nof y given h0, which is why",
    "start": "3571935",
    "end": "3581300"
  },
  {
    "text": "the 1 walk goes up, and\nthe 0 walk goes down. Now, the thing we're going to\ndo is do a sequential test.",
    "start": "3581300",
    "end": "3590600"
  },
  {
    "text": "We're going to keep\ndoing experiments until we cross a threshold. We're going to decide what\nthreshold is going to give us",
    "start": "3590600",
    "end": "3598560"
  },
  {
    "text": "a small enough probability of\nerror under each condition, and then we choose\nthat threshold.",
    "start": "3598560",
    "end": "3604650"
  },
  {
    "text": "And we continue to test\nuntil we get there.",
    "start": "3604650",
    "end": "3609710"
  },
  {
    "text": "So we want to find out whether\nwe've gained anything by that, how much we've gained\nif we gain something",
    "start": "3609710",
    "end": "3615210"
  },
  {
    "text": "by it, and so forth. OK, when you use two thresholds,\nalpha's going to",
    "start": "3615210",
    "end": "3624369"
  },
  {
    "text": "be bigger than 0. Beta's going to be\nless than 0. The expected value of z given\nh0 is less than 0, but the",
    "start": "3624370",
    "end": "3632109"
  },
  {
    "text": "value of z given h1\nis greater than 0. That's why the walks are\ncoupled, so we can handle each",
    "start": "3632110",
    "end": "3638809"
  },
  {
    "text": "of them separately until we can\nget the answers for one from the answers\nfor the other.",
    "start": "3638810",
    "end": "3644530"
  },
  {
    "text": "Crossing alpha is a rare event\nfor the random walk with h0",
    "start": "3644530",
    "end": "3650670"
  },
  {
    "text": "because a random walk\nwith h0, you're going to go down typically. You hardly ever go up. Yes?",
    "start": "3650670",
    "end": "3655784"
  },
  {
    "text": "AUDIENCE: Can you please\nexplain again sign of expectations? PROFESSOR: The sign of\nthe expectations? Yes, z is the log, so that when\nwe actually have h equals",
    "start": "3655784",
    "end": "3685170"
  },
  {
    "text": "1, the expected value of this\nis going to be lined up with this term on top.",
    "start": "3685170",
    "end": "3691349"
  },
  {
    "text": "We have f of y. When we have h equals 0,\nthis lined up with",
    "start": "3691350",
    "end": "3697920"
  },
  {
    "text": "the term on the bottom. I mean, actually, you have to\ngo through and actually show",
    "start": "3697920",
    "end": "3705270"
  },
  {
    "text": "that the integral of f of y\ngiven h1 of this quantity is",
    "start": "3705270",
    "end": "3712590"
  },
  {
    "text": "greater than 0, and the other\none is less than 0. We don't really have to do that\nbecause, if we calculate",
    "start": "3712590",
    "end": "3719020"
  },
  {
    "text": "this moment generating\nfunction, we can pick it off of there.",
    "start": "3719020",
    "end": "3725640"
  },
  {
    "text": "When we look at this moment\ngenerating function, that",
    "start": "3725640",
    "end": "3733380"
  },
  {
    "text": "slope there is the expected\nvalue of z conditional on h",
    "start": "3733380",
    "end": "3739109"
  },
  {
    "text": "equals 0, and because of the\nshifting property, this slope here is the expected value of\nz given h equals 1, just",
    "start": "3739110",
    "end": "3749010"
  },
  {
    "text": "because the 1 curve is shifted\nfrom the other by one unit. ",
    "start": "3749010",
    "end": "3758840"
  },
  {
    "text": "It's really because\nof that ratio. If you defined it the other\nway, you just changed the",
    "start": "3758840",
    "end": "3764010"
  },
  {
    "text": "sign, so nothing important\nwould happen. ",
    "start": "3764010",
    "end": "3771477"
  },
  {
    "text": "OK, so r start equals 1 for\nthe h0 walk, so the",
    "start": "3771478",
    "end": "3778980"
  },
  {
    "text": "probability of error, given h0,\nis less than or equal to e to the minus alpha.",
    "start": "3778980",
    "end": "3785430"
  },
  {
    "text": "Well, that's a nice simple\nresult, isn't it? In fact, that's really\nbeautifully. You just calculate this moment\ngenerating function, you find",
    "start": "3785430",
    "end": "3793740"
  },
  {
    "text": "the root of it, and\nyou're done. You have a nice bound, and in\nfact, it's an exponentially tight bound.",
    "start": "3793740",
    "end": "3800460"
  },
  {
    "text": "And on the other hand, when you\ndeal with the probability of error given h1 by symmetry,\nit's less than or equal to e",
    "start": "3800460",
    "end": "3809230"
  },
  {
    "text": "to the beta. Beta is a negative number,\nremember, so this is exponentially going\ndown as you choose",
    "start": "3809230",
    "end": "3815460"
  },
  {
    "text": "beta, smaller and smaller. So the thing that we're getting\nis we can make each of",
    "start": "3815460",
    "end": "3821140"
  },
  {
    "text": "these error probabilities as\nsmall as we want, this one, by",
    "start": "3821140",
    "end": "3828289"
  },
  {
    "text": "making alpha big. We can make this one as\nsmall as we want by making beta big negative.",
    "start": "3828290",
    "end": "3835360"
  },
  {
    "text": "There must be a cost to this. OK, but what's the cost? ",
    "start": "3835360",
    "end": "3843480"
  },
  {
    "text": "What happens when you\nmake alpha big? ",
    "start": "3843480",
    "end": "3850010"
  },
  {
    "text": "When hypothesis 1 is the correct\nhypothesis, what normally happens is that this\nrandom walk is going to go up",
    "start": "3850010",
    "end": "3859010"
  },
  {
    "text": "roughly at a slope of the\nexpected value of z given h equals 0.",
    "start": "3859010",
    "end": "3866530"
  },
  {
    "text": "So when you make alpha very,\nvery large, you're forced to make a very large number of\ntests when h is equal to 1.",
    "start": "3866530",
    "end": "3875040"
  },
  {
    "text": "When you make beta very, very\nlarge, you're forced to take a large number of tests when\nh is equal to 0.",
    "start": "3875040",
    "end": "3881840"
  },
  {
    "text": "So the trade off here is\na little bit funny. You make your error probability\nfor h equals 0",
    "start": "3881840",
    "end": "3889090"
  },
  {
    "text": "very, very small by costing more\nmoney when hypotheses 1",
    "start": "3889090",
    "end": "3894280"
  },
  {
    "text": "is the correct hypothesis\nbecause you don't make a decision until you've\nreally climb way up",
    "start": "3894280",
    "end": "3901540"
  },
  {
    "text": "on this random walk. And that means it takes a long\ntime when you have h equals 1.",
    "start": "3901540",
    "end": "3909050"
  },
  {
    "text": "Since when h is equal to 1, the\nprobability of crossing this lower threshold is it is\nalmost negligible, this",
    "start": "3909050",
    "end": "3917660"
  },
  {
    "text": "expected time that it takes\nis really just a function of h equals 1.",
    "start": "3917660",
    "end": "3923110"
  },
  {
    "text": "I'm going to show that\nin the next slide. ",
    "start": "3923110",
    "end": "3931279"
  },
  {
    "text": "When you increase alpha, it\nlowers the probability of error given h equals 0.",
    "start": "3931280",
    "end": "3939190"
  },
  {
    "text": "Excuse me, I should have h\nequals 0 instead of h sub 0. Exponentially, it increases the\nexpected number of steps",
    "start": "3939190",
    "end": "3948660"
  },
  {
    "text": "until you make a decision\ngiven h1.",
    "start": "3948660",
    "end": "3954420"
  },
  {
    "text": "Expected value of j given h1 is\neffectively equal to alpha divided by expected value\nof z given h1.",
    "start": "3954420",
    "end": "3962960"
  },
  {
    "text": "Why is that? That's essentially\nWald's equality.",
    "start": "3962960",
    "end": "3968210"
  },
  {
    "text": "Not Wald's identity, but Wald's\nequality because-- ",
    "start": "3968210",
    "end": "3985250"
  },
  {
    "text": "Yes, it says from Wald's\nequality, since alpha is essentially equal to the\nexpected value of s of j given",
    "start": "3985250",
    "end": "3993570"
  },
  {
    "text": "h equals 1, the number of\ntesting you have to take when h is equal to 1, when alpha\nis very, very large, is",
    "start": "3993570",
    "end": "4001960"
  },
  {
    "text": "effectively the amount of time\nthat it takes you to get up to the point alpha. That expected amount of time is\ntypically pretty close to",
    "start": "4001960",
    "end": "4011110"
  },
  {
    "text": "the mean value. So alpha there is close to the\nexpected value of s of j given",
    "start": "4011110",
    "end": "4021530"
  },
  {
    "text": "h equals 1. So Wald's equality, given h\nequals 1, says the expected",
    "start": "4021530",
    "end": "4027260"
  },
  {
    "text": "value of j given h1 is equal\nto the expected value of sj",
    "start": "4027260",
    "end": "4033880"
  },
  {
    "text": "given h equals 1, that's alpha,\ndivided by the expected value of z given h1,\nwhich is just the",
    "start": "4033880",
    "end": "4042550"
  },
  {
    "text": "underlying likelihood ratio. ",
    "start": "4042550",
    "end": "4047640"
  },
  {
    "text": "So to get this result, we just\nsubstitute alpha for the expected value.",
    "start": "4047640",
    "end": "4053340"
  },
  {
    "text": "And then the probability of\nerror, given h equals 0, if we write it this way, we see\nthe cost immediately.",
    "start": "4053340",
    "end": "4060680"
  },
  {
    "text": "That's the expected value\nof j given h equal to 1.",
    "start": "4060680",
    "end": "4066230"
  },
  {
    "text": "In other words, the expected\nnumber of tests given h equals 1 times the expected value of\nthe log likelihood ratio given",
    "start": "4066230",
    "end": "4074555"
  },
  {
    "text": "h equals 1. When you decrease beta, that\nlowers the probability of error given h1 exponentially,\nbut it increases the number of",
    "start": "4074555",
    "end": "4083960"
  },
  {
    "text": "tests when h0 is the\ncorrect hypothesis. So in that case, you get the\nprobability of error given h",
    "start": "4083960",
    "end": "4094010"
  },
  {
    "text": "equals 1 is effectively equal to\nthe expected value e to the",
    "start": "4094010",
    "end": "4100240"
  },
  {
    "text": "expected value of j\nequals j equals 0. This is just the number of tests\nyou have to do when h is",
    "start": "4100240",
    "end": "4107160"
  },
  {
    "text": "equal to 0. This is the expected value of\nthe log likelihood ratio when",
    "start": "4107160",
    "end": "4112449"
  },
  {
    "text": "h is equal to 0. This is very approximate, but\nthis is how you would actually",
    "start": "4112450",
    "end": "4118630"
  },
  {
    "text": "choose how big you make alpha,\nhow big do you make beta if",
    "start": "4118630",
    "end": "4123679"
  },
  {
    "text": "you want to do a test between\nthese two hypotheses. Now, this shows what you're\ngaining by the sequential test",
    "start": "4123680",
    "end": "4134520"
  },
  {
    "text": "over what you're gaining by\nthe non-sequential test. You don't have this in your\nnotes, so you might just jot",
    "start": "4134520",
    "end": "4140159"
  },
  {
    "text": "it down quickly. The expected value of z,\nconditional on h equals 0, is",
    "start": "4140160",
    "end": "4148399"
  },
  {
    "text": "this slope here, the slope of\nthe moment generating function",
    "start": "4148399",
    "end": "4154219"
  },
  {
    "text": "is z equals 0. That's the slope of the\nunderlying random variable.",
    "start": "4154220",
    "end": "4160490"
  },
  {
    "text": "Since this point is r equal to\n1, this point down here is the expected value of z\ngiven h equals 0.",
    "start": "4160490",
    "end": "4169130"
  },
  {
    "text": "That's the exponents that you\nget when h equals 0 is, in",
    "start": "4169130",
    "end": "4178620"
  },
  {
    "text": "fact, the correct exponent. When given the probability of\nerror given that h is equal to",
    "start": "4178620",
    "end": "4185818"
  },
  {
    "text": "0, namely the probability that\nyou choose hypothesis 1. Same way over here.",
    "start": "4185819",
    "end": "4191950"
  },
  {
    "text": "This slope here is the expected\nvalue of the log likelihood ratio given\nh equals 1.",
    "start": "4191950",
    "end": "4199015"
  },
  {
    "text": "This hits down here at minus\nexpected value of z given h equals 1.",
    "start": "4199015",
    "end": "4205050"
  },
  {
    "text": "So you have this exponent going\none way, you have this exponent going the other way\nwhen the thing multiplying the",
    "start": "4205050",
    "end": "4212060"
  },
  {
    "text": "exponent is not an absolute\nvalue but is, in fact, the number of tests you have to\ndo than the other test.",
    "start": "4212060",
    "end": "4220130"
  },
  {
    "text": "Now, if we do the fix test,\nwhat we're fixed with is a",
    "start": "4220130",
    "end": "4225710"
  },
  {
    "text": "test where you take a line\ntangent to this curve, which",
    "start": "4225710",
    "end": "4231550"
  },
  {
    "text": "goes from here across\nhere to there. We can see-saw it around. When we see-saw it all the way\nin the limit, we can get this",
    "start": "4231550",
    "end": "4240470"
  },
  {
    "text": "result here. But we get this result here at\nthe cost of an error, which is",
    "start": "4240470",
    "end": "4248240"
  },
  {
    "text": "almost one in the other\ncase, so that's not a very good deal.",
    "start": "4248240",
    "end": "4254059"
  },
  {
    "text": "This says that sequential\ntesting, well, it shows you how much you gain by doing\na sequential test.",
    "start": "4254060",
    "end": "4261960"
  },
  {
    "text": "I mean, it might not be\nintuitively obvious why this is happening. I mean, really the reason it's\nhappening is that the times",
    "start": "4261960",
    "end": "4270480"
  },
  {
    "text": "when you want to make the test\nvery long are those times when",
    "start": "4270480",
    "end": "4276600"
  },
  {
    "text": "if h is equal to 0, you\nnormally go down. The next most normal thing is\nyou wobble around without",
    "start": "4276600",
    "end": "4284619"
  },
  {
    "text": "doing anything for a long time,\nin which case you want to keep doing additional tests\nuntil finally it falls down,",
    "start": "4284620",
    "end": "4293210"
  },
  {
    "text": "or finally it goes up. But by taking additional\ntests, you make it very",
    "start": "4293210",
    "end": "4298810"
  },
  {
    "text": "unlikely that you're ever going\nto cross that threshold. So that's the thing\nyou're gaining.",
    "start": "4298810",
    "end": "4305800"
  },
  {
    "text": "You are gaining the fact that\nthe error is small in those",
    "start": "4305800",
    "end": "4314139"
  },
  {
    "text": "situations where the sum of\nthese random variables stays close to 0 for a long time, and\nthen you don't make errors",
    "start": "4314140",
    "end": "4322410"
  },
  {
    "text": "in those cases. ",
    "start": "4322410",
    "end": "4328120"
  },
  {
    "text": "We now have just a little\nbit of time to prove Wald's identity. I don't want to have a lot of\ntime to prove it because",
    "start": "4328120",
    "end": "4334890"
  },
  {
    "text": "proofs of theorems are things\nyou really have to look at yourselves. This one, you almost don't\nhave to look at it.",
    "start": "4334890",
    "end": "4342320"
  },
  {
    "text": "This one is almost obvious as\nsoon as you understand what a",
    "start": "4342320",
    "end": "4347460"
  },
  {
    "text": "tilted probability is. So let's suppose that x sub n is\na sequence of IID discrete",
    "start": "4347460",
    "end": "4355480"
  },
  {
    "text": "random variables. It has a moment generating\nfunction for some given r.",
    "start": "4355480",
    "end": "4362110"
  },
  {
    "text": "We're going to assume that these\nrandom variables are discrete now to make this\nargument simple.",
    "start": "4362110",
    "end": "4367740"
  },
  {
    "text": "If they're not discrete, this\nwhole argument has to be replaced with all sorts\nof [INAUDIBLE]",
    "start": "4367740",
    "end": "4374270"
  },
  {
    "text": "integrals and all\nof that stuff. It's exactly the same idea,\nbut it just is messy",
    "start": "4374270",
    "end": "4379449"
  },
  {
    "text": "mathematically. So what we're going to do is\nwe're going to define a tilted",
    "start": "4379450",
    "end": "4384540"
  },
  {
    "text": "random variable. A tilted random variable is a\nrandom variable in a different",
    "start": "4384540",
    "end": "4390590"
  },
  {
    "text": "probability space. OK, we start out with this\nprobability space that we're interested in, and then we say,\nOK, suppose that we, just",
    "start": "4390590",
    "end": "4403830"
  },
  {
    "text": "to satisfy our imaginations, we\nsuppose the probabilities",
    "start": "4403830",
    "end": "4409000"
  },
  {
    "text": "are different. We assume that the probabilities\nfor a given r is",
    "start": "4409000",
    "end": "4414090"
  },
  {
    "text": "the probability that the random\nvariable X is equal to",
    "start": "4414090",
    "end": "4419219"
  },
  {
    "text": "little x, namely this quantity\nhere, is equal to the original",
    "start": "4419220",
    "end": "4427070"
  },
  {
    "text": "probability that X is\nequal to little x. All the sample values are\nthe same, it's just the",
    "start": "4427070",
    "end": "4432980"
  },
  {
    "text": "probability's changed, times e\nto the rx minus gamma of r.",
    "start": "4432980",
    "end": "4439470"
  },
  {
    "text": "So we're taking these\nprobabilities when X is large. We're magnifying them\nwhen x is small.",
    "start": "4439470",
    "end": "4445790"
  },
  {
    "text": "We're knocking them down. What's the purpose of this? It's just a normalization\nfactor.",
    "start": "4445790",
    "end": "4451420"
  },
  {
    "text": "e to the minus gamma of r is 1\nover the moment generating",
    "start": "4451420",
    "end": "4456800"
  },
  {
    "text": "function of r, so you take\np of x, e to the rx, divide it by g of r.",
    "start": "4456800",
    "end": "4465560"
  },
  {
    "text": "So this is a probability mass\nfunction, as well as this.",
    "start": "4465560",
    "end": "4471280"
  },
  {
    "text": "This is the correct probability\nmass function for the model you're looking at.",
    "start": "4471280",
    "end": "4476320"
  },
  {
    "text": "This is an imaginary one, but\nyou can always imagine. You can say let's suppose that\nwe had this model instead of",
    "start": "4476320",
    "end": "4484070"
  },
  {
    "text": "the other model. All the sample values are the\nsame, but the probabilities are different.",
    "start": "4484070",
    "end": "4489800"
  },
  {
    "text": "So we want to see what we can\nfind out from these different probabilities in this different\nprobability model.",
    "start": "4489800",
    "end": "4498199"
  },
  {
    "text": "If you sum over x here,\nthis sum is equal to 1, as we just said.",
    "start": "4498200",
    "end": "4503940"
  },
  {
    "text": "So we'll view q sub xr of x as\nthe probability mass function",
    "start": "4503940",
    "end": "4511580"
  },
  {
    "text": "on x in a new probability\nspace. We can use all the laws of\nprobability in this new space,",
    "start": "4511580",
    "end": "4519050"
  },
  {
    "text": "and that's exactly what\nwe're going to do. And we're going to say things\nabout the new space, but then",
    "start": "4519050",
    "end": "4525790"
  },
  {
    "text": "we can always come back to the\nold space from this formula here because whatever we find\nout in the new space will work",
    "start": "4525790",
    "end": "4534230"
  },
  {
    "text": "in the old space. One thing we'd like to do is\nto be able to find the",
    "start": "4534230",
    "end": "4540000"
  },
  {
    "text": "expected value of the random\nvariable x in this new probability space, so this isn't\nthe expected value in",
    "start": "4540000",
    "end": "4548940"
  },
  {
    "text": "the old space. It's a probability\nin the new space. It's the sum over x of x\ntimes q sub xr of x.",
    "start": "4548940",
    "end": "4557130"
  },
  {
    "text": "That's what the expected\nvalue is. X is the same in both spaces. That's just the probabilities\nthat have changed.",
    "start": "4557130",
    "end": "4565030"
  },
  {
    "text": "These are p of x times z to the\nrx minus gamma of r, so",
    "start": "4565030",
    "end": "4572400"
  },
  {
    "text": "when you sum this, what you get\nis 1 over g of xr, which is that term, times the\nderivative of p sub x",
    "start": "4572400",
    "end": "4580440"
  },
  {
    "text": "of x, e to the rx. When you take this derivative,\nthen you get an x in front,",
    "start": "4580440",
    "end": "4587290"
  },
  {
    "text": "which is that x there. So you get g prime of xr\nover gx of r, which is",
    "start": "4587290",
    "end": "4593230"
  },
  {
    "text": "gamma prime of r. OK, so in terms of that graph\nwe've drawn, when you take",
    "start": "4593230",
    "end": "4598830"
  },
  {
    "text": "these tilted probabilities, you\nmove that slope, that r",
    "start": "4598830",
    "end": "4604320"
  },
  {
    "text": "equals 0, and now you're looking\nat a slope at whatever r you're looking at.",
    "start": "4604320",
    "end": "4610590"
  },
  {
    "text": "And that gives you the\nexpected value there. ",
    "start": "4610590",
    "end": "4616290"
  },
  {
    "text": "OK, if you have a joint tilted\nprobability mass function--",
    "start": "4616290",
    "end": "4623770"
  },
  {
    "text": "and don't think it gets\nany more complicated. It doesn't. I mean, you've already gone\nthrough the major complication",
    "start": "4623770",
    "end": "4630120"
  },
  {
    "text": "of this argument. The joint tilted PMF is the\nprobability of x1 to xn is the",
    "start": "4630120",
    "end": "4639690"
  },
  {
    "text": "old probability of x1 to xn\ntimes all of these tilted",
    "start": "4639690",
    "end": "4644940"
  },
  {
    "text": "factors here. If you let a of sn be the set\nof n tuples which have the",
    "start": "4644940",
    "end": "4651469"
  },
  {
    "text": "same sum, then all these terms\nbecome r times s sub n.",
    "start": "4651470",
    "end": "4657590"
  },
  {
    "text": "So what you get is that for each\nxn for which the sum is sn, this tilted probability\nbecomes the old probability",
    "start": "4657590",
    "end": "4668620"
  },
  {
    "text": "times e to the r sn minus n\ngamma of r, which says that when we look at the tilted\nprobability of the sum, namely",
    "start": "4668620",
    "end": "4678380"
  },
  {
    "text": "we said that when we tilt these\nprobabilities, we can do everything in a new space that\nwe could do in the old space.",
    "start": "4678380",
    "end": "4684710"
  },
  {
    "text": "We can do everything that\nprobability theory allows us to do, so we can look at the\nprobability of s sub n in the",
    "start": "4684710",
    "end": "4692460"
  },
  {
    "text": "new space also. The probability of sn in the\nold space, namely we're summing this quantity, overall\nxn in a of sn, so we sum up",
    "start": "4692460",
    "end": "4703949"
  },
  {
    "text": "all of those as the probability\nsub s sub n at sn times this quantity,\nwhich is fixed.",
    "start": "4703950",
    "end": "4712090"
  },
  {
    "text": "So this is the key to a lot\nof large deviation theory. Any time you're dealing with a\ndifficult problem, and you",
    "start": "4712090",
    "end": "4721090"
  },
  {
    "text": "want to see what's happening\nway, way away from the mean, you want to see what these\nsums look like for these",
    "start": "4721090",
    "end": "4728110"
  },
  {
    "text": "exceptional cases, what we do\nis we look at a new model where we tilt the probability so\nthat the region of concern",
    "start": "4728110",
    "end": "4736850"
  },
  {
    "text": "becomes the main region\nfor that tilted model.",
    "start": "4736850",
    "end": "4742320"
  },
  {
    "text": "So for r equals 0, we're\ntilting the probability towards large values, and you\ncan use the law of large",
    "start": "4742320",
    "end": "4748330"
  },
  {
    "text": "numbers, essential limit\ntheorem, whatever you want to, in that new space, then.",
    "start": "4748330",
    "end": "4755280"
  },
  {
    "text": "Now, we can prove\nWald's equality. ",
    "start": "4755280",
    "end": "4760489"
  },
  {
    "text": "What Wald's identity is is the\nstatement that when you tilt these probabilities, a stopping\nrule in this tilted",
    "start": "4760490",
    "end": "4771020"
  },
  {
    "text": "world is still the stopping\ntime is still a random",
    "start": "4771020",
    "end": "4776570"
  },
  {
    "text": "variable, namely you still\nstop with probability 1. Somebody questioned whether you\nstop with probability 1 in",
    "start": "4776570",
    "end": "4782960"
  },
  {
    "text": "the old world. Like I said, you do because\nyou have this positive variance, and the thing with two\nthresholds keeps growing",
    "start": "4782960",
    "end": "4790330"
  },
  {
    "text": "and growing. Here, you have the same thing.",
    "start": "4790330",
    "end": "4795560"
  },
  {
    "text": "I mean, the mean doesn't make\nany difference at all. I mean, you're looking at trying\nto exceed one of two",
    "start": "4795560",
    "end": "4801770"
  },
  {
    "text": "different thresholds, and\neventually, you exceed one of them no matter where\nyou set r.",
    "start": "4801770",
    "end": "4807900"
  },
  {
    "text": "So what this is saying is the\nprobability that j is equal to",
    "start": "4807900",
    "end": "4813840"
  },
  {
    "text": "n in this tilted space is equal\nto the probability that",
    "start": "4813840",
    "end": "4819610"
  },
  {
    "text": "j is equal to n in the old\nspace times z to the r sn minus gamma of r.",
    "start": "4819610",
    "end": "4826300"
  },
  {
    "text": "So this quantity is equal to the\nexpected value of e to the r sn minus gamma of r.",
    "start": "4826300",
    "end": "4832539"
  },
  {
    "text": "Given j equals n times the\nprobability that j is equal to n, you sum this over n\nand, bingo, you're",
    "start": "4832540",
    "end": "4839860"
  },
  {
    "text": "back at the Wald identity. So that's all the Wald identity\nis, is just a statement that when you tilt a\nprobability, and you have a",
    "start": "4839860",
    "end": "4848260"
  },
  {
    "text": "stopping rule on the original\nprobabilities, you then have a stopping rule on the\nnew probabilities.",
    "start": "4848260",
    "end": "4855619"
  },
  {
    "text": "And Wald's identity says-- well, Wald's identity holds\nwhenever that tilted stopping",
    "start": "4855620",
    "end": "4863500"
  },
  {
    "text": "rule is a random variable. OK, that's it for today.",
    "start": "4863500",
    "end": "4871540"
  },
  {
    "text": "We will do martingales\non Wednesday. ",
    "start": "4871540",
    "end": "4876545"
  }
]