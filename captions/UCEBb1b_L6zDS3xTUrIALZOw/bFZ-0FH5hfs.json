[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of\nMIT courses, visit MITOpenCourseWare@OCW.MIT.edu",
    "start": "12720",
    "end": "21470"
  },
  {
    "text": "PHILIPPE RIGOLLET: So today\nWE'LL actually just do a brief",
    "start": "21470",
    "end": "26669"
  },
  {
    "text": "chapter on Bayesian statistics. And there's entire courses\non Bayesian statistics, there's entire books\non Bayesian statistics,",
    "start": "26670",
    "end": "33480"
  },
  {
    "text": "there's entire careers\nin Bayesian statistics. So admittedly, I'm\nnot going to be",
    "start": "33480",
    "end": "39270"
  },
  {
    "text": "able to do it\njustice and tell you all the interesting\nthings that are happening in Bayesian statistics. But I think it's important\nas a statistician",
    "start": "39270",
    "end": "47310"
  },
  {
    "text": "to know what it\nis, how it works, because it's actually\na weapon of choice",
    "start": "47310",
    "end": "52500"
  },
  {
    "text": "for many practitioners. And because it allows them to\nincorporate their knowledge",
    "start": "52500",
    "end": "58079"
  },
  {
    "text": "about a problem in a\nfairly systematic manner. So if you look at like, say the\nBayesian statistics literature,",
    "start": "58080",
    "end": "64099"
  },
  {
    "text": "it's huge. And so here I give\nyou sort of a range",
    "start": "64099",
    "end": "69570"
  },
  {
    "text": "of what you can expect to\nsee in Bayesian statistics from your second edition of\na traditional book, something",
    "start": "69570",
    "end": "78299"
  },
  {
    "text": "that involves computation,\nsome things that involve risk thinking. And there's a lot of\nBayesian thinking.",
    "start": "78300",
    "end": "84750"
  },
  {
    "text": "There's a lot of\nthings that you know talking about sort of like\nphilosophy of thinking Bayesian.",
    "start": "84750",
    "end": "90180"
  },
  {
    "text": "This book, for example,\nseems to be one of them. This book is\ndefinitely one of them. This one represents sort of\na wide, a broad literature",
    "start": "90180",
    "end": "98880"
  },
  {
    "text": "on Bayesian statistics, for\napplications for example, in social sciences. But even in large\nscale machine learning,",
    "start": "98880",
    "end": "105380"
  },
  {
    "text": "there's a lot of Bayesian\nstatistics happening, particular using something\ncalled Bayesian parametrics, or hierarchical\nBayesian modeling.",
    "start": "105380",
    "end": "113490"
  },
  {
    "text": "So we do have some experts\nat MIT in the c-cell.",
    "start": "113490",
    "end": "119470"
  },
  {
    "text": "Tamara Broderick for\nexample, is a person who does quite a bit\nof interesting work",
    "start": "119470",
    "end": "124560"
  },
  {
    "text": "on Bayesian parametrics. And if that's something you\nwant to know more about, I urge you to go\nand talk to her.",
    "start": "124560",
    "end": "130889"
  },
  {
    "text": "So before we go into\nmore advanced things, we need to start with what\nis the Bayesian approach.",
    "start": "130889",
    "end": "137220"
  },
  {
    "text": "What do Bayesians\ndo, and how is it different from what\nwe've been doing so far?",
    "start": "137220",
    "end": "142719"
  },
  {
    "text": "So to understand the\ndifference between Bayesians and what we've been\ndoing so far is,",
    "start": "142720",
    "end": "148800"
  },
  {
    "text": "we need to first put a name on\nwhat we've been doing so far. It's called\nfrequentist statistics. Which usually Bayesian versus\nfrequentist statistics,",
    "start": "148800",
    "end": "156720"
  },
  {
    "text": "by versus I don't mean\nthat there is naturally in opposition to them. Actually, often you will\nsee the same method that",
    "start": "156720",
    "end": "163350"
  },
  {
    "text": "comes out of both approaches. So let's see how\nwe did it, right. The first thing, we had data.",
    "start": "163350",
    "end": "168930"
  },
  {
    "text": "We observed some data. And we assumed that this\ndata was generated randomly. The reason we did\nthat is because this",
    "start": "168930",
    "end": "174840"
  },
  {
    "text": "would allow us to leverage\ntools from probability. So let's say by nature,\nmeasurements, you do a survey,",
    "start": "174840",
    "end": "181019"
  },
  {
    "text": "you get some data. Then we made some assumptions\non the data generating process.",
    "start": "181020",
    "end": "186030"
  },
  {
    "text": "For example, we\nassumed they were iid. That was one of the\nrecurring things. Sometimes we assume\nit was Gaussian.",
    "start": "186030",
    "end": "191530"
  },
  {
    "text": "If you wanted to\nuse say, T-test. Maybe we did some\nnonparametric statistics. We assume it was a\nsmooth function or maybe",
    "start": "191530",
    "end": "198240"
  },
  {
    "text": "linear regression function. So those are our modeling. And this was basically\na way to say, well,",
    "start": "198240",
    "end": "204849"
  },
  {
    "text": "we're not going to allow for\nany distributions for the data that we have. But maybe a small\nset of distributions",
    "start": "204850",
    "end": "211640"
  },
  {
    "text": "that indexed by some small\nparameters, for example. Or at least remove some\nof the possibilities.",
    "start": "211640",
    "end": "218400"
  },
  {
    "text": "Otherwise, there's\nnothing we can learn. And so for example,\nthis was associated",
    "start": "218400",
    "end": "225270"
  },
  {
    "text": "to some parameter of\ninterest, say data or beta in the regression model.",
    "start": "225270",
    "end": "231269"
  },
  {
    "text": "Then we had this unknown\nproblem and this unknown thing, a known parameter.",
    "start": "231270",
    "end": "236610"
  },
  {
    "text": "And we wanted to find it. We wanted to either\nestimate it or test it, or maybe find a confidence\ninterval for the subject.",
    "start": "236610",
    "end": "242460"
  },
  {
    "text": "So, so far I should not have\nsaid anything that's new. But this last\nsentence is actually",
    "start": "242460",
    "end": "248209"
  },
  {
    "text": "what's going to be different\nfrom the Bayesian part. And particular, this\nunknown but fixed things is what's going to be changing.",
    "start": "248210",
    "end": "254280"
  },
  {
    "text": " In the Bayesian\napproach, we still assume that we observe\nsome random data.",
    "start": "254280",
    "end": "262049"
  },
  {
    "text": "But the generating process\nis slightly different. It's sort of a\ntwo later process. And there's one\nprocess that generates",
    "start": "262050",
    "end": "267320"
  },
  {
    "text": "the parameter and\nthen one process that, given this parameter\ngenerates the data. So what the first layer\ndoes, nobody really",
    "start": "267320",
    "end": "275990"
  },
  {
    "text": "believes that there's\nsome random process that's happening, about\ngenerating what is going",
    "start": "275990",
    "end": "281000"
  },
  {
    "text": "to be the true expected\nnumber of people who turn their head to\nthe right when they kiss.",
    "start": "281000",
    "end": "287060"
  },
  {
    "text": "But this is actually going to\nbe something that brings us some easiness for\nus to incorporate",
    "start": "287060",
    "end": "293270"
  },
  {
    "text": "what we call prior belief. We'll see an\nexample in a second.",
    "start": "293270",
    "end": "298640"
  },
  {
    "text": "But often, you actually\nhave prior belief of what this\nparameter should be. When we, say least\nsquares, we looked",
    "start": "298640",
    "end": "305509"
  },
  {
    "text": "over all of the vectors\nin all of R to the p, including the ones that\nhave coefficients equal",
    "start": "305510",
    "end": "311840"
  },
  {
    "text": "to 50 million. Those are things that we\nmight be able to rule out.",
    "start": "311840",
    "end": "318050"
  },
  {
    "text": "We might be able to rule out\nthat on a much smaller scale. For example, well\nI'm not an expert",
    "start": "318050",
    "end": "324650"
  },
  {
    "text": "on turning your head to\nthe right or to the left. But maybe you can\nrule out the fact",
    "start": "324650",
    "end": "330949"
  },
  {
    "text": "that almost everybody\nis turning their head in the same direction, or almost\neverybody is turning their head",
    "start": "330950",
    "end": "335990"
  },
  {
    "text": "to another direction. So we have this prior belief. And this belief is going\nto play say, hopefully",
    "start": "335990",
    "end": "343750"
  },
  {
    "text": "less and less important role as\nwe collect more and more data. But if we have a\nsmaller amount of data,",
    "start": "343750",
    "end": "349199"
  },
  {
    "text": "we might want to be able\nto use this information, rather than just\nshooting in the dark.",
    "start": "349200",
    "end": "354700"
  },
  {
    "text": "And so the idea is to\nhave this prior belief. And then, we want to\nupdate this prior belief",
    "start": "354700",
    "end": "360430"
  },
  {
    "text": "into what's called the\nposterior belief after we've seen some data. Maybe I believe that\nthere's something",
    "start": "360430",
    "end": "368050"
  },
  {
    "text": "that should be in some range. But maybe after I see data, it's\ncomforting me in my beliefs. So I'm actually having\nmaybe a belief that's more.",
    "start": "368050",
    "end": "375330"
  },
  {
    "text": "So belief encompasses\nbasically what you think and how strongly\nyou think about it. That's what I call belief.",
    "start": "375330",
    "end": "381370"
  },
  {
    "text": "So for example, if I have a\nbelief about some parameter theta, maybe my\nbelief is telling me where theta should\nbe and how strongly I",
    "start": "381370",
    "end": "388970"
  },
  {
    "text": "believe in it, in the sense\nthat I have a very narrow region where theta could be.",
    "start": "388970",
    "end": "395470"
  },
  {
    "text": "The posterior beliefs, as\nwell, you see some data. And maybe you're more confident\nor less confident about what you've seen.",
    "start": "395470",
    "end": "400499"
  },
  {
    "text": "Maybe you've shifted\nyour belief a little bit. And so that's what we're\ngoing to try to see, and how to do this in\na principal manner.",
    "start": "400499",
    "end": "408630"
  },
  {
    "text": "To understand this\nbetter, there's nothing better than an example. So let's talk about another\nstupid statistical question.",
    "start": "408630",
    "end": "416220"
  },
  {
    "text": "Which is, let's try\nto understand p. Of course, I'm not going to\ntalk about politics from now on.",
    "start": "416220",
    "end": "421430"
  },
  {
    "text": "So let's talk about p,\nthe proportion of women in the population. ",
    "start": "421430",
    "end": "435330"
  },
  {
    "text": "And so what I could do is\nto collect some data, X1, Xn",
    "start": "435330",
    "end": "441849"
  },
  {
    "text": "and assume that\nthey're Bernoulli with some parameter, p unknown. So p is in 0, 1.",
    "start": "441850",
    "end": "450810"
  },
  {
    "text": "OK, let's assume that\nthose guys are iid. So this is just an indicator\nfor each of my collected data,",
    "start": "450810",
    "end": "458190"
  },
  {
    "text": "whether the person I randomly\nsample is a woman, I get a one. If it's a man, I get a zero.",
    "start": "458190",
    "end": "463350"
  },
  {
    "text": " Now the question is, I\nsample these people randomly.",
    "start": "463350",
    "end": "469470"
  },
  {
    "text": "I do you know their gender. And the frequentist\napproach was just saying,",
    "start": "469470",
    "end": "474600"
  },
  {
    "text": "OK, let's just estimate\np hat being Xn bar. And then we could do some tests.",
    "start": "474600",
    "end": "481110"
  },
  {
    "text": "So here, there's a test. I want to test maybe if\np is equal to 0.5 or not. That sounds like a pretty\nreasonable thing to test.",
    "start": "481110",
    "end": "489710"
  },
  {
    "text": "But we want to also\nmaybe estimate p. But here, this is a case where\nwe definitely prior belief",
    "start": "489710",
    "end": "496160"
  },
  {
    "text": "of what p should be. We are pretty confident that\np is not going to be 0.7.",
    "start": "496160",
    "end": "502040"
  },
  {
    "text": "We actually believe\nthat we should be extremely close to one\nhalf, but maybe not exactly.",
    "start": "502040",
    "end": "509330"
  },
  {
    "text": "Maybe this population is not\nthe population in the world. But maybe this is the\npopulation of, say some college",
    "start": "509330",
    "end": "515659"
  },
  {
    "text": "and we want to understand if\nthis college has half women or not. Maybe we know it's going\nto be close to one half,",
    "start": "515659",
    "end": "522110"
  },
  {
    "text": "but maybe we're not quite sure.  We're going to want to\nintegrate that knowledge.",
    "start": "522110",
    "end": "529960"
  },
  {
    "text": "So I could integrate it in\na blunt manner by saying, discard the data and say\nthat p is equal to one half.",
    "start": "529960",
    "end": "535420"
  },
  {
    "text": "But maybe that's just\na little too much. So how do I do this trade\noff between adding the data",
    "start": "535420",
    "end": "541360"
  },
  {
    "text": "and combining it with\nthis prior knowledge?",
    "start": "541360",
    "end": "546760"
  },
  {
    "text": "In many instances, essentially\nwhat's going to happen is this one half is going to\nact like one new observation.",
    "start": "546760",
    "end": "554329"
  },
  {
    "text": "So if you have\nfive observations, this is just the\nsixth observation, which will play a role.",
    "start": "554330",
    "end": "560240"
  },
  {
    "text": "If you have a\nmillion observations, you're going to have\na million and one. It's not going to play\nso much of a role. That's basically how it goes.",
    "start": "560240",
    "end": "565899"
  },
  {
    "text": " But, definitely not\nalways because we'll",
    "start": "565900",
    "end": "573470"
  },
  {
    "text": "see that if I take my prior to\nbe a point minus one half here, it's basically as if I\nwas discarding my data.",
    "start": "573470",
    "end": "579290"
  },
  {
    "text": "So essentially, there's\nalso your ability to encompass how strongly\nyou believe in this prior.",
    "start": "579290",
    "end": "585520"
  },
  {
    "text": "And if you believe\ninfinitely more in the prior than you believe in\nthe data you collected, then it's not going to act\nlike one more observation.",
    "start": "585520",
    "end": "594600"
  },
  {
    "text": "The Bayesian approach\nis a tool to one, include mathematically\nour prior. And our prior belief into\nstatistical procedures.",
    "start": "594600",
    "end": "602580"
  },
  {
    "text": "Maybe I have this\nprior knowledge. But if I'm a medical\ndoctor, it's not clear to me how I'm going to turn this into\nsome principal way of building",
    "start": "602580",
    "end": "609870"
  },
  {
    "text": "estimators. And the second\ngoal is going to be to update this prior belief\ninto a posterior belief",
    "start": "609870",
    "end": "616260"
  },
  {
    "text": "by using the data. ",
    "start": "616260",
    "end": "622200"
  },
  {
    "text": "How do I do this? And at some point,\nI sort of suggested that there's two layers.",
    "start": "622200",
    "end": "628610"
  },
  {
    "text": "One is where you draw\nthe parameter at random. And two, once you\nhave the parameter,",
    "start": "628610",
    "end": "635290"
  },
  {
    "text": "conditionless parameter,\nyou draw your data. Nobody believed this actually is\nhappening, that nature is just",
    "start": "635290",
    "end": "642080"
  },
  {
    "text": "rolling dice for us and\nchoosing parameters at random. But what's happening\nis that, this idea",
    "start": "642080",
    "end": "648260"
  },
  {
    "text": "that the parameter comes\nfrom some random distribution actually captures, very\nwell, this idea that how",
    "start": "648260",
    "end": "654860"
  },
  {
    "text": "you would encompass your prior. How would you say, my\nbelief is as follows? Well here's an example about p.",
    "start": "654860",
    "end": "661870"
  },
  {
    "text": "I'm 90% sure that p is\nbetween 0.4 and 0.6.",
    "start": "661870",
    "end": "667856"
  },
  {
    "text": "And I'm 95% sure that p\nis between 0.3 and 0.8.",
    "start": "667856",
    "end": "674230"
  },
  {
    "text": "So essentially, I have\nthis possible value of p. And what I know is that, there's\n90% here between 0.4 and 0.6.",
    "start": "674230",
    "end": "695430"
  },
  {
    "text": "And then I have 0.3 and 0.8. And I know that I'm 95%\nsure that I'm in here.",
    "start": "695430",
    "end": "704200"
  },
  {
    "text": "If you remember, this sort of\nlooks like the kind of pictures that I made when I had\nsome Gaussian, for example.",
    "start": "704200",
    "end": "710110"
  },
  {
    "text": "And I said, oh here we have\n90% of the observations. And here, we have 95%\nof the observations.",
    "start": "710110",
    "end": "717105"
  },
  {
    "text": " So in a way, if I\nwere able to tell you",
    "start": "717105",
    "end": "724570"
  },
  {
    "text": "all those ranges for\nall possible values, then I would essentially\ndescribe a probability",
    "start": "724570",
    "end": "730510"
  },
  {
    "text": "distribution for p. And what I'm saying\nis that, p is going to have this kind of shape.",
    "start": "730510",
    "end": "736582"
  },
  {
    "text": "So of course, if I tell you\nonly two twice this information that there's 90% I'm here,\nand I'm between here and here.",
    "start": "736582",
    "end": "742279"
  },
  {
    "text": "And 95%, I'm between here\nand here, then there's many ways I can\naccomplish that, right. I could have something that\nlooks like this, maybe.",
    "start": "742280",
    "end": "748970"
  },
  {
    "text": " It could be like this.",
    "start": "748970",
    "end": "755830"
  },
  {
    "text": "There's many ways\nI can have this. Some of them are\ndefinitely going to be mathematically more\nconvenient than others.",
    "start": "755830",
    "end": "762279"
  },
  {
    "text": "And hopefully, we're\ngoing to have things that I can\nparameterize very well. Because if I tell\nyou this is this guy,",
    "start": "762280",
    "end": "769899"
  },
  {
    "text": "then there's basically one,\ntwo three, four, five, six, seven parameters.",
    "start": "769900",
    "end": "776554"
  },
  {
    "text": "So I probably don't\nwant something that has seven parameters. But maybe I can say, oh,\nit's a Gaussian and I all",
    "start": "776554",
    "end": "781582"
  },
  {
    "text": "I have to do is to tell\nyou where it's centered and what the standard\ndeviation is. ",
    "start": "781582",
    "end": "787250"
  },
  {
    "text": "So the idea of using\nthis two layer thing, where we think of\nthe parameter p",
    "start": "787250",
    "end": "792800"
  },
  {
    "text": "as being drawn from\nsome distribution, is really just a way for us\nto capture this information. Our prior belief\nbeing, well there's",
    "start": "792800",
    "end": "800420"
  },
  {
    "text": "this percentage of\nchances that it's there. But the percentage of\nthis chance, I'm not I'm deliberately not using\nprobability here.",
    "start": "800420",
    "end": "808730"
  },
  {
    "text": "So it's really a way\nto get close to this.  That's why I say, the true\nparameter is not random.",
    "start": "808730",
    "end": "816170"
  },
  {
    "text": "But the Bayesian approach\ndoes as if it was random. And then, just spits\nout a procedure",
    "start": "816170",
    "end": "822430"
  },
  {
    "text": "out of this thought process,\nthis thought experiment.",
    "start": "822430",
    "end": "829110"
  },
  {
    "text": "So when you practice\nBayesian statistics a lot, you start getting automatisms.",
    "start": "829110",
    "end": "837840"
  },
  {
    "text": "You start getting some things\nthat you do without really thinking about\nit. just like when you you're a statistician,\nthe first thing you do is,",
    "start": "837840",
    "end": "844860"
  },
  {
    "text": "can I think of this data as\nbeing Gaussian for example? When you're Bayesian\nyou're thinking about, OK I have a set of parameters.",
    "start": "844860",
    "end": "851400"
  },
  {
    "text": "So here, I can\ndescribe my parameter as being theta in\ngeneral, in some big space",
    "start": "851400",
    "end": "860190"
  },
  {
    "text": "parameter of theta. But what spaces\ndid we encounter? Well, we encountered\nthe real line.",
    "start": "860190",
    "end": "867089"
  },
  {
    "text": "We encountered the interval\n0, 1 for Bernoulli's And we encountered some of\nthe positive real line",
    "start": "867090",
    "end": "876320"
  },
  {
    "text": "for exponential\ndistributions, etc. And so what I'm\ngoing to need to do,",
    "start": "876320",
    "end": "882020"
  },
  {
    "text": "if I want to put some\nprior on those spaces, I'm going to have to\nhave a usual set of tools",
    "start": "882020",
    "end": "887694"
  },
  {
    "text": "for this guy, usual set\nof tools for this guy, usual sort of\ntools for this guy. And by usual set\nof tools, I mean I'm going to have to have a\nfamily of distributions that's",
    "start": "887694",
    "end": "894966"
  },
  {
    "text": "supported on this. So in particular,\nthis is the speed in which my parameter\nthat I usually denote",
    "start": "894966",
    "end": "901610"
  },
  {
    "text": "by p for Bernoulli lives. And so what I need is to find a\ndistribution on the interval 0,",
    "start": "901610",
    "end": "907840"
  },
  {
    "text": "1 just like this guy.",
    "start": "907840",
    "end": "913540"
  },
  {
    "text": "The problem with the\nGaussian is that it's not on the interval 0, 1. It's going to spill\nout in the end.",
    "start": "913540",
    "end": "920230"
  },
  {
    "text": "And it's not going to be\nsomething that works for me. And so the question is, I need\nto think about distributions",
    "start": "920230",
    "end": "925802"
  },
  {
    "text": "that are probably continuous. Why would I restrict myself\nto discrete distributions that are actually convenient and for\nBernoulli, one that's actually",
    "start": "925802",
    "end": "934060"
  },
  {
    "text": "basically the main tool\nthat everybody is using is the so-called\nbeta distribution.",
    "start": "934060",
    "end": "939670"
  },
  {
    "text": "So the beta distribution\nhas two parameters. ",
    "start": "939670",
    "end": "950680"
  },
  {
    "text": "So x follows a beta\nwith parameters",
    "start": "950680",
    "end": "956910"
  },
  {
    "text": "a and b if it has\na density, f of x",
    "start": "956910",
    "end": "965069"
  },
  {
    "text": "is equal to x to the a minus 1. 1 minus x to the b minus 1,\nif x is in the interval 0,",
    "start": "965070",
    "end": "975800"
  },
  {
    "text": "1 and 0 for all other x's.",
    "start": "975800",
    "end": "982730"
  },
  {
    "text": "OK?  Why is that a good thing?",
    "start": "982730",
    "end": "990470"
  },
  {
    "text": "Well, it's a density that's\non the interval 0, 1 for sure. But now I have these two\nparameters and a set of shapes",
    "start": "990470",
    "end": "997130"
  },
  {
    "text": "that I can get by tweaking those\ntwo parameters is incredible. ",
    "start": "997130",
    "end": "1004260"
  },
  {
    "text": "It's going to be a\nunimodal distribution. It's still fairly nice. It's not going to be something\nthat goes like this and this.",
    "start": "1004260",
    "end": "1009760"
  },
  {
    "text": "Because if you think\nabout this, what would it mean if your prior\ndistribution of the interval 0,",
    "start": "1009760",
    "end": "1015550"
  },
  {
    "text": "1 had this shape?  It would mean that, maybe\nyou think that p is here",
    "start": "1015550",
    "end": "1021934"
  },
  {
    "text": "or maybe you think\nthat p is here, or maybe you think\nthat p is here. Which essentially\nmeans that you think that p can come from\nthree different phenomena.",
    "start": "1021934",
    "end": "1030661"
  },
  {
    "text": "And there's other models\nthat are called mixers for that, that directly\naccount for the fact that maybe there are several\nphenomena that are aggregated",
    "start": "1030661",
    "end": "1039549"
  },
  {
    "text": "in your data set. But if you think that your\ndata set is sort of pure, and that everything comes\nfrom the same phenomenon,",
    "start": "1039550",
    "end": "1045650"
  },
  {
    "text": "you want something\nthat looks like this, or maybe looks like this, or\nmaybe is sort of symmetric.",
    "start": "1045650",
    "end": "1052850"
  },
  {
    "text": "You want to get all this stuff. Maybe you want something\nthat says, well if I'm talking about p being the\nprobability of the proportion",
    "start": "1052850",
    "end": "1062330"
  },
  {
    "text": "of women in the whole world, you\nwant something that's probably really spiked around one half.",
    "start": "1062330",
    "end": "1068600"
  },
  {
    "text": "Almost the point\nmath, because you know let's agree that 0.5\nis the actual number.",
    "start": "1068600",
    "end": "1074990"
  },
  {
    "text": "So you want something that\nsays, OK maybe I'm wrong. But I'm sure I'm not going\nto be really that way off.",
    "start": "1074990",
    "end": "1081299"
  },
  {
    "text": "So you want something\nthat's really pointy. But if it's something\nyou've never checked,",
    "start": "1081300",
    "end": "1086660"
  },
  {
    "text": "and again I can not make\nreferences at this point, but something where you might\nhave some uncertainty that",
    "start": "1086660",
    "end": "1093197"
  },
  {
    "text": "should be around one half. Maybe you want something\nthat a little more allows you to say, well, I think\nthere's more around one half.",
    "start": "1093197",
    "end": "1099409"
  },
  {
    "text": "But there's still some\nfluctuations that are possible. And in particular\nhere, I talk about p,",
    "start": "1099410",
    "end": "1105110"
  },
  {
    "text": "where the two parameters a\nand b are actually the same. I call them a.",
    "start": "1105110",
    "end": "1110500"
  },
  {
    "text": "One is called scale. The other one is called shape. Oh sorry, this is not a density. So it actually has\nto be normalized.",
    "start": "1110500",
    "end": "1118646"
  },
  {
    "text": "When you integrate\nthis guy, it's going to be some function\nthat depends on a and b, actually depends\non this function through the beta function.",
    "start": "1118646",
    "end": "1125427"
  },
  {
    "text": "Which is this combination\nof gamma function, so that's why it's\ncalled beta distribution.",
    "start": "1125427",
    "end": "1131515"
  },
  {
    "text": "That's the definition of\nthe beta function when you integrate this thing anyway. You just have to normalize it.",
    "start": "1131515",
    "end": "1136970"
  },
  {
    "text": "That's just a number that\ndepends on the a and b. So here, if you\ntake a equal to b, you have something\nthat essentially",
    "start": "1136970",
    "end": "1143000"
  },
  {
    "text": "is symmetric around one half. Because what does it look like? Well, so my density f of\nx, is going to be what?",
    "start": "1143000",
    "end": "1150980"
  },
  {
    "text": "It's going to be my constant\ntimes x, times one minus x",
    "start": "1150980",
    "end": "1159200"
  },
  {
    "text": "to a minus one. And this function, x times\n1 minus x looks like this.",
    "start": "1159200",
    "end": "1166080"
  },
  {
    "text": "We've drawn it before. That was something\nthat showed up as being the variance\nof my Bernoulli.",
    "start": "1166080",
    "end": "1176490"
  },
  {
    "text": "So we know it's something that\ntakes its maximum at one half.",
    "start": "1176490",
    "end": "1182240"
  },
  {
    "text": "And now I'm just taking\na power of this guy. So I'm really just\ndistorting this thing into some fairly\nsymmetric manner.",
    "start": "1182240",
    "end": "1191340"
  },
  {
    "start": "1191340",
    "end": "1196400"
  },
  {
    "text": "This distribution that\nwe actually take for p. I assume that p, the\nparameter, notice",
    "start": "1196400",
    "end": "1203030"
  },
  {
    "text": "that this is kind of weird. First of all, this is\nprobably the first time in this entire\ncourse that something",
    "start": "1203030",
    "end": "1209570"
  },
  {
    "text": "has a distribution when it's\nactually a lower case letter. That's something you\nhave to deal with, because we've been using lower\ncase letters for parameters.",
    "start": "1209570",
    "end": "1216826"
  },
  {
    "text": "And now we want them\nto have a distribution. So that's what's\ngoing to happen. This is called the\nprior distribution.",
    "start": "1216827",
    "end": "1223850"
  },
  {
    "text": "So really, I should write\nsomething like f of p is equal to a constant times\np, 1 minus p, to the n minus 1.",
    "start": "1223850",
    "end": "1235290"
  },
  {
    "text": "Well no, actually I should not\nbecause then it's confusing. One thing in terms\nof notation that I'm",
    "start": "1235290",
    "end": "1241610"
  },
  {
    "text": "going to write, when\nI have a constant here and I don't want to\nmake it explicit. And we'll see in a second why I\ndon't need to make it explicit.",
    "start": "1241610",
    "end": "1248480"
  },
  {
    "text": "I'm going to write\nthis as f of x is proportional to x 1\nminus x to the n minus 1.",
    "start": "1248480",
    "end": "1264060"
  },
  {
    "text": "That's just to say, equal to\nsome constant that does not depend on x times this thing.",
    "start": "1264060",
    "end": "1271260"
  },
  {
    "start": "1271260",
    "end": "1276320"
  },
  {
    "text": "So if we continue\nwith our experiment",
    "start": "1276320",
    "end": "1281929"
  },
  {
    "text": "where I'm drawing\nthis data, X1 to Xn, which is Bernoulli p, if\np has some distribution",
    "start": "1281930",
    "end": "1289050"
  },
  {
    "text": "it's not clear what it\nmeans to have a Bernoulli with some random parameter. So what I'm going to do is, then\nI'm going to first draw my p.",
    "start": "1289050",
    "end": "1295010"
  },
  {
    "text": "Let's say I get a number, 0.52. And then, I'm going to draw\nmy data conditionally on p.",
    "start": "1295010",
    "end": "1301100"
  },
  {
    "text": "So here comes the first and\nlast flowchart of this class. ",
    "start": "1301100",
    "end": "1309500"
  },
  {
    "text": "So nature first draws p.  p follows some data on a, a.",
    "start": "1309500",
    "end": "1318360"
  },
  {
    "text": "Then I condition on p.  And then I draw X1, Xn\nthat are iid, Bernoulli p.",
    "start": "1318360",
    "end": "1330760"
  },
  {
    "text": "Everybody understand the\nprocess of generating this data? So you first draw a\nparameter, and then you just",
    "start": "1330760",
    "end": "1336250"
  },
  {
    "text": "flip those independent biased\ncoins with this particular p. There's this layered thing.",
    "start": "1336250",
    "end": "1343230"
  },
  {
    "text": " Now conditionally p, right so\nhere I have this prior about p",
    "start": "1343230",
    "end": "1351009"
  },
  {
    "text": "which was the thing. So this is just the\nthought process again, it's not anything that\nactually happens in practice.",
    "start": "1351010",
    "end": "1356480"
  },
  {
    "text": "This is my way of thinking about\nhow the data was generated. And from this, I'm going to try\nto come up with some procedure.",
    "start": "1356480",
    "end": "1363310"
  },
  {
    "text": "Just like, if your estimator\nis the average of the data, you don't have to\nunderstand probability",
    "start": "1363310",
    "end": "1369700"
  },
  {
    "text": "to say that my estimator\nis the average of the data. Anyone outside this\nroom understands that the average\nis a good estimator",
    "start": "1369700",
    "end": "1375970"
  },
  {
    "text": "for some average behavior. And they don't need\nto think of the data",
    "start": "1375970",
    "end": "1381070"
  },
  {
    "text": "as being a random\nvariable, et cetera. So same thing, basically. ",
    "start": "1381070",
    "end": "1390759"
  },
  {
    "text": "In this case, you can see that\nthe posterior distribution is still a beta. ",
    "start": "1390760",
    "end": "1398320"
  },
  {
    "text": "What it means is that,\nI had this thing. Then, I observed my data. And then, I continue\nand here I'm",
    "start": "1398320",
    "end": "1403570"
  },
  {
    "text": "going to update my prior\ninto some posterior",
    "start": "1403570",
    "end": "1412799"
  },
  {
    "text": "distribution, pi. And here, this guy is\nactually also a beta.",
    "start": "1412800",
    "end": "1419210"
  },
  {
    "text": " My posterior\ndistribution, p, is also",
    "start": "1419210",
    "end": "1425950"
  },
  {
    "text": "a beta distribution\nwith the parameters that are on this slide. And I'll have the space\nto reproduce them.",
    "start": "1425950",
    "end": "1431669"
  },
  {
    "text": "So I start the beginning\nof this flowchart as having p, which is a prior.",
    "start": "1431670",
    "end": "1437130"
  },
  {
    "text": "I'm going to get\nsome observations and then, I'm going to\nupdate what my posterior is. ",
    "start": "1437130",
    "end": "1444530"
  },
  {
    "text": "This posterior is\nbasically something that's, in business\nstatistics was",
    "start": "1444530",
    "end": "1449690"
  },
  {
    "text": "beautiful is as soon as\nyou have this distribution, it's essentially capturing all\nthe information about the data",
    "start": "1449690",
    "end": "1457030"
  },
  {
    "text": "that you want for p. And it's not just the point. It's not just an average. It's actually an\nentire distribution",
    "start": "1457030",
    "end": "1463659"
  },
  {
    "text": "for the possible\nvalues of theta. And it's not the same\nthing as saying, well",
    "start": "1463660",
    "end": "1470740"
  },
  {
    "text": "if theta hat is equal to Xn\nbar, in the Gaussian case I know that this is some mean, mu.",
    "start": "1470740",
    "end": "1477130"
  },
  {
    "text": "And then maybe it has\nvarying sigma squared over n. That's not what I mean by, this\nis my posterior distribution.",
    "start": "1477130",
    "end": "1483550"
  },
  {
    "text": "This is not what I mean. This is going to come from\nthis guy, the Gaussian thing",
    "start": "1483550",
    "end": "1489790"
  },
  {
    "text": "and the central limit theorem. But what I mean is this guy. And this came exclusively\nfrom the prior distribution.",
    "start": "1489790",
    "end": "1498130"
  },
  {
    "text": "If I had another prior,\nI would not necessarily have a beta distribution\non the output.",
    "start": "1498130",
    "end": "1503840"
  },
  {
    "text": "So when I have the same\nfamily of distributions at the beginning and at\nthe end of this flowchart,",
    "start": "1503840",
    "end": "1511090"
  },
  {
    "text": "I say that beta is\na conjugate prior.",
    "start": "1511090",
    "end": "1516520"
  },
  {
    "text": " Meaning I put in beta as a prior\nand I get beta as [INAUDIBLE]",
    "start": "1516520",
    "end": "1527389"
  },
  {
    "text": "And that's why betas\nare so popular. Conjugate priors\nare really nice, because you know that whatever\nyou put in, what you're going",
    "start": "1527390",
    "end": "1535730"
  },
  {
    "text": "to get in the end is a beta. So all you have to think\nabout is the parameters. You don't have to check\nagain what the posterior is",
    "start": "1535730",
    "end": "1541040"
  },
  {
    "text": "going to look like, what the\nPDF of this guy is going to be. You don't have to\nthink about it. You just have to check\nwhat the parameters are.",
    "start": "1541040",
    "end": "1546649"
  },
  {
    "text": "And there's families\nof conjugate priors. Gaussian gives\nGaussian, for example. There's a bunch of them.",
    "start": "1546650",
    "end": "1552170"
  },
  {
    "text": "And this is what drives people\ninto using specific priors as",
    "start": "1552170",
    "end": "1557210"
  },
  {
    "text": "opposed to others. It has nice\nmathematical properties. Nobody believes that p is really\ndistributed according to beta.",
    "start": "1557210",
    "end": "1565910"
  },
  {
    "text": "But it's flexible enough\nand super convenient mathematically. ",
    "start": "1565910",
    "end": "1572450"
  },
  {
    "text": "Now let's see for one\nsecond, before we actually go any further. I didn't mention A and\nB are both in here,",
    "start": "1572450",
    "end": "1579789"
  },
  {
    "text": "A and B are both\npositive numbers.  They can be anything positive.",
    "start": "1579790",
    "end": "1587610"
  },
  {
    "text": "So here what I did\nis that, I updated A into a plus the sum\nof my data, and b",
    "start": "1587610",
    "end": "1594649"
  },
  {
    "text": "into b plus n minus\nthe sum of my data. So that's essentially, a becomes\na plus the number of ones.",
    "start": "1594650",
    "end": "1601910"
  },
  {
    "text": " Well, that's only\nwhen I have a and a.",
    "start": "1601910",
    "end": "1607350"
  },
  {
    "text": "So the first parameters become\nitself plus the number of ones. And the second\none becomes itself plus the number of zeros.",
    "start": "1607350",
    "end": "1612531"
  },
  {
    "text": " And so just as a sanity\ncheck, what does this mean?",
    "start": "1612531",
    "end": "1619159"
  },
  {
    "text": "If a it goes to zero, what\nis the beta when a goes to 0?",
    "start": "1619160",
    "end": "1628910"
  },
  {
    "text": "We can actually\nread this from here. ",
    "start": "1628910",
    "end": "1636920"
  },
  {
    "text": "Actually, let's take a goes to-- ",
    "start": "1636920",
    "end": "1645370"
  },
  {
    "text": "no. Sorry, let's just do this. ",
    "start": "1645370",
    "end": "1658670"
  },
  {
    "text": "I'll do it when we talk\nabout non-informative prior, because it's a little too messy. ",
    "start": "1658670",
    "end": "1667220"
  },
  {
    "text": "How do we do this? How did I get this posterior\ndistribution, given the prior? How do I update This well this\nis called Bayesian statistics.",
    "start": "1667220",
    "end": "1676070"
  },
  {
    "text": "And you've heard this\nword, Bayes before. And the way you've heard\nit is in the Bayes formula.",
    "start": "1676070",
    "end": "1682010"
  },
  {
    "text": "What was the Bayes formula? The Bayes formula\nwas telling you that the probability of A, given\nB was equal to something that",
    "start": "1682010",
    "end": "1691389"
  },
  {
    "text": "depended on the probability of\nB, given A. That's what it was. ",
    "start": "1691390",
    "end": "1696787"
  },
  {
    "text": "You can actually either\nremember the formula or you can remember\nthe definition. And this is what p of A\nand B divided by p of B.",
    "start": "1696787",
    "end": "1706000"
  },
  {
    "text": "So this is p of B, given A\ntimes p of A divided by p of B.",
    "start": "1706000",
    "end": "1715480"
  },
  {
    "text": "That's what Bayes\nformula is telling you. Agree? So now what I want is to have\nsomething that's telling me",
    "start": "1715480",
    "end": "1726200"
  },
  {
    "text": "how this is going to work. What is going to play the\nrole of those events, A and B?",
    "start": "1726200",
    "end": "1734410"
  },
  {
    "text": "Well one is going\nto be, this is going to be the distribution\nof my parameter of theta,",
    "start": "1734410",
    "end": "1741980"
  },
  {
    "text": "given that I see the data. And this is going\nto tell me, what is the distribution of the\ndata, given that I know what",
    "start": "1741980",
    "end": "1747601"
  },
  {
    "text": "my parameter if theta is. But that part, if\nthis is theta and this is the parameter of\ntheta, this is what",
    "start": "1747601",
    "end": "1753080"
  },
  {
    "text": "we've been doing all along. The distribution of the data,\ngiven the parameter here",
    "start": "1753080",
    "end": "1758720"
  },
  {
    "text": "was n iid Bernoulli p. I knew exactly what their joint\nprobability mass function is.",
    "start": "1758720",
    "end": "1767960"
  },
  {
    "text": "Then, that was what? So we said that this\nis going to be my data and this is going\nto be my parameter.",
    "start": "1767960",
    "end": "1774730"
  },
  {
    "text": " So that means that, this is\nthe probability of my data,",
    "start": "1774730",
    "end": "1780210"
  },
  {
    "text": "given the parameter. This is the probability\nof the parameter.",
    "start": "1780210",
    "end": "1785729"
  },
  {
    "text": "What is this? What did we call this? This is the prior. It's just the distribution\nof my parameter.",
    "start": "1785729",
    "end": "1793690"
  },
  {
    "text": "Now what is this? Well, this is just\nthe distribution of the data, itself.",
    "start": "1793690",
    "end": "1800340"
  },
  {
    "text": "This is essentially the\ndistribution of this,",
    "start": "1800340",
    "end": "1806799"
  },
  {
    "text": "if this was indeed\nnot conditioned on p.",
    "start": "1806800",
    "end": "1815080"
  },
  {
    "text": "So if I don't condition\non p, this data is going to be a bunch of iid,\nBernoulli with some parameter.",
    "start": "1815080",
    "end": "1823981"
  },
  {
    "text": "But the perimeter\nis random, right. So for different realization\nof this data set, I'm going to get different\nparameters for the Bernoulli.",
    "start": "1823982",
    "end": "1830170"
  },
  {
    "text": "And so that leads to\nsome sort of convolution. It's not really a\nconvolution in this case,",
    "start": "1830170",
    "end": "1836170"
  },
  {
    "text": "but it's like some sort of\ncomposition of distributions. I have the randomness that\ncomes from here and then,",
    "start": "1836170",
    "end": "1841600"
  },
  {
    "text": "the randomness that comes\nfrom realizing the Bernoulli. That's just the\nmarginal distribution. It actually might be painful to\nunderstand what this is, right.",
    "start": "1841600",
    "end": "1849820"
  },
  {
    "text": "In a way, it's sort of a\nmixture and it's not super nice. But we'll see that this\nactually won't matter for us.",
    "start": "1849820",
    "end": "1855880"
  },
  {
    "text": "This is going to be some number. It's going to be there. But it will matter\nfor us, what it is. Because it actually does\nnot depend on the parameter.",
    "start": "1855880",
    "end": "1862510"
  },
  {
    "text": "And that's all\nthat matters to us. ",
    "start": "1862510",
    "end": "1869100"
  },
  {
    "text": "Let's put some names\non those things. This was very informal. So let's put some actual\nnames on what we call prior.",
    "start": "1869100",
    "end": "1879710"
  },
  {
    "text": "So what is the formal\ndefinition of a prior, what is the formal\ndefinition of a posterior,",
    "start": "1879710",
    "end": "1884960"
  },
  {
    "text": "and what are the\nrules to update it? So I'm going to have my data,\nwhich is going to be X1, Xn.",
    "start": "1884960",
    "end": "1890100"
  },
  {
    "start": "1890100",
    "end": "1895710"
  },
  {
    "text": "Let's say they are iid, but\nthey don't actually have to. And so I'm going to\nhave given, theta.",
    "start": "1895710",
    "end": "1901260"
  },
  {
    "start": "1901260",
    "end": "1907450"
  },
  {
    "text": "And when I say\ngiven, it's either given like I did in the\nfirst part of this course in all previous chapters,\nor conditionally on.",
    "start": "1907450",
    "end": "1915940"
  },
  {
    "text": "If you're thinking like a\nBayesian, what I really mean is conditionally on\nthis random parameter.",
    "start": "1915940",
    "end": "1922250"
  },
  {
    "text": "It's as if it was\na fixed number. They're going to\nhave a distribution,",
    "start": "1922250",
    "end": "1928410"
  },
  {
    "text": "X1, Xn is going to\nhave some distribution. Let's assume for now\nit's a PDF, pn of X1, Xn.",
    "start": "1928410",
    "end": "1939260"
  },
  {
    "text": "I'm going to write\ntheta like this. So for example, what is this?",
    "start": "1939260",
    "end": "1944900"
  },
  {
    "text": "Let's say this is a PDF. It could be a PMF. Everything I say, I'm going to\nthink of them as being PDF's.",
    "start": "1944900",
    "end": "1951197"
  },
  {
    "text": "I'm going to combine\nPDF's with PDF's, but I could combine PDF it PMF, PMF\nwith PDF's or PMF with PMF.",
    "start": "1951197",
    "end": "1957440"
  },
  {
    "text": "So everywhere you see\na D could be an M. Now I have those things.",
    "start": "1957440",
    "end": "1962590"
  },
  {
    "text": "So what does it mean? So here is an example. X1, Xn or iid, and theta 1.",
    "start": "1962590",
    "end": "1973970"
  },
  {
    "text": "Now I know exactly what the\njoint PDF of this thing is. It means that pn of X1, Xn\ngiven theta is equal to what?",
    "start": "1973970",
    "end": "1983790"
  },
  {
    "text": "Well it's 1 over\n2pi to the power n",
    "start": "1983790",
    "end": "1990560"
  },
  {
    "text": "e, to the minus sum\nfrom i equal 1 to n of xi minus theta\nsquared divided by 2.",
    "start": "1990560",
    "end": "1998450"
  },
  {
    "text": "So that's just the joint\ndistribution of n iid and theta 1, random variables.",
    "start": "1998450",
    "end": "2005120"
  },
  {
    "text": "That's my pn given theta. Now this is what we denoted\nby f sub theta before.",
    "start": "2005120",
    "end": "2013310"
  },
  {
    "text": "We had the subscript before, but\nnow we just put a bar in theta because we want to remember\nthat this is actually",
    "start": "2013310",
    "end": "2018860"
  },
  {
    "text": "conditioned on theta. But this is just notation. You should just think of this\nas being, just the usual thing",
    "start": "2018860",
    "end": "2026060"
  },
  {
    "text": "that you get from some\nstatistical model. Now, that's going to be pn.",
    "start": "2026060",
    "end": "2033910"
  },
  {
    "start": "2033910",
    "end": "2051020"
  },
  {
    "text": "Theta has prior\ndistribution, pi.",
    "start": "2051020",
    "end": "2059500"
  },
  {
    "text": " For example, so think of it\nas either PDF or PMF again.",
    "start": "2059500",
    "end": "2069129"
  },
  {
    "text": "For example, pi\nof theta was what? Well it was some constant\ntimes theta to the a minus 1,",
    "start": "2069130",
    "end": "2080158"
  },
  {
    "text": "1 minus theta to a minus 1. So it has some\nprior distribution,",
    "start": "2080159",
    "end": "2085899"
  },
  {
    "text": "and that's another PMF. So now I'm given the\ndistribution of my,",
    "start": "2085900",
    "end": "2091090"
  },
  {
    "text": "x is given theta and given\nthe distribution of my theta. I'm given this guy.",
    "start": "2091090",
    "end": "2097410"
  },
  {
    "text": "That's this guy. I'm given that guy,\nwhich is my pi.",
    "start": "2097410",
    "end": "2105340"
  },
  {
    "text": "So that's my pn of\nX1, Xn given theta.",
    "start": "2105340",
    "end": "2111700"
  },
  {
    "text": "That's my pi of theta. ",
    "start": "2111700",
    "end": "2117390"
  },
  {
    "text": "Well, this is just\nthe integral of pn of X1, Xn times pi\nof theta, d theta,",
    "start": "2117390",
    "end": "2128279"
  },
  {
    "text": "over all possible sets of theta. That's just when I\nintegrate out my theta,",
    "start": "2128280",
    "end": "2133359"
  },
  {
    "text": "or I compute the\nmarginal distribution, I did this by integrating. That's just basic probability,\nconditional probabilities.",
    "start": "2133360",
    "end": "2141010"
  },
  {
    "text": "Then if I had the\nPMF, I would just sum over the values of thetas. ",
    "start": "2141010",
    "end": "2149020"
  },
  {
    "text": "Now what I want is to\nfind what's called,",
    "start": "2149020",
    "end": "2155210"
  },
  {
    "text": "so that's the\nprior distribution, and I want to find the\nposterior distribution.",
    "start": "2155210",
    "end": "2161227"
  },
  {
    "start": "2161227",
    "end": "2175110"
  },
  {
    "text": "It's pi of theta, given X1, Xn. ",
    "start": "2175110",
    "end": "2181780"
  },
  {
    "text": "If I use Bayes' rule\nI know that this is pn of X1, Xn, given\ntheta times pi of theta.",
    "start": "2181780",
    "end": "2194650"
  },
  {
    "text": "And then it's divided\nby the distribution of those guys, which I will\nwrite as integral over theta",
    "start": "2194650",
    "end": "2201070"
  },
  {
    "text": "of pn, X1, Xn, given theta\ntimes pi of theta, d theta.",
    "start": "2201070",
    "end": "2208830"
  },
  {
    "start": "2208830",
    "end": "2215360"
  },
  {
    "text": "Everybody's with me, still? If you're not\ncomfortable with this, it means that you probably need\nto go read your couple of pages",
    "start": "2215360",
    "end": "2223010"
  },
  {
    "text": "on conditional densities\nand conditional PMF's from your probably class. There's really not much there.",
    "start": "2223010",
    "end": "2228869"
  },
  {
    "text": "It's just a matter of being able\nto define those quantities, f density of x, given y.",
    "start": "2228870",
    "end": "2235288"
  },
  {
    "text": "This is just what's called\na conditional density. You need to understand\nwhat this object is and how it relates to the\njoint distribution of x and y,",
    "start": "2235289",
    "end": "2241920"
  },
  {
    "text": "or maybe the distribution of\nx or the distribution of y. ",
    "start": "2241920",
    "end": "2247400"
  },
  {
    "text": "But it's the same rules. One way to actually\nremember this is, this is exactly\nthe same rules as this.",
    "start": "2247400",
    "end": "2253730"
  },
  {
    "text": "When you see a bar, it's the\nsame thing as the probability of this and this guy. So for densities,\nit's just a comma",
    "start": "2253730",
    "end": "2260060"
  },
  {
    "text": "divided by the second the\nprobably the second guy. That's it.",
    "start": "2260060",
    "end": "2265119"
  },
  {
    "text": "So if you remember this, you can\njust do some pattern matching and see what I just wrote here. ",
    "start": "2265120",
    "end": "2273220"
  },
  {
    "text": "Now, I can compute every\nsingle one of these guys. This something I get\nfrom my modeling.",
    "start": "2273220",
    "end": "2284030"
  },
  {
    "text": "So I did not write this. It's not written in the slides.",
    "start": "2284030",
    "end": "2289130"
  },
  {
    "text": "But I give a name to this guy\nthat was my prior distribution.",
    "start": "2289130",
    "end": "2294819"
  },
  {
    "text": "And that was my\nposterior distribution. ",
    "start": "2294820",
    "end": "2302550"
  },
  {
    "text": "In chapter three, maybe\nwhat did we call this guy? ",
    "start": "2302550",
    "end": "2312119"
  },
  {
    "text": "The one that does not have a\nname and that's in the box. ",
    "start": "2312120",
    "end": "2319347"
  },
  {
    "text": "What did we call it?  AUDIENCE: [INAUDIBLE]",
    "start": "2319347",
    "end": "2326335"
  },
  {
    "text": "PHILLIPE RIGOLLET: It is the\njoint distribution of the Xi's. ",
    "start": "2326335",
    "end": "2331950"
  },
  {
    "text": "And we gave it a name. AUDIENCE: [INAUDIBLE] PHILLIPE RIGOLLET: It's\nthe likelihood, right? This is exactly the likelihood.",
    "start": "2331950",
    "end": "2337630"
  },
  {
    "text": "This was the\nlikelihood of theta. ",
    "start": "2337630",
    "end": "2343920"
  },
  {
    "text": "And this is something that's\nvery important to remember, and that really reminds you\nthat these things are really not",
    "start": "2343920",
    "end": "2350520"
  },
  {
    "text": "that different. Maximum likelihood estimation\nand Bayesian estimation, because your posterior is really\njust your likelihood times",
    "start": "2350520",
    "end": "2358859"
  },
  {
    "text": "something that's just putting\nsome weights on the thetas, depending on where you\nthink theta should be.",
    "start": "2358860",
    "end": "2366390"
  },
  {
    "text": "If I had, say a maximum\nlikelihood estimate, and my likelihood and\ntheta looked like this, but my prior and theta\nlooked like this.",
    "start": "2366390",
    "end": "2373410"
  },
  {
    "text": "I said, oh I really want\nthetas that are like this. So what's going to\nhappen is that, I'm",
    "start": "2373410",
    "end": "2378710"
  },
  {
    "text": "going to turn this into some\nposterior that looks like this. ",
    "start": "2378710",
    "end": "2384400"
  },
  {
    "text": "So I'm just really\nwaiting, this posterior, this is a constant that does\nnot depend on theta right?",
    "start": "2384400",
    "end": "2389971"
  },
  {
    "text": "Agreed? I integrated over\ntheta, so theta is gone. So forget about this guy.",
    "start": "2389971",
    "end": "2396220"
  },
  {
    "text": "I have basically, that the\nposterior distribution up to scaling, because it has to\nbe a probability density and not",
    "start": "2396220",
    "end": "2401830"
  },
  {
    "text": "just anything any\nfunction that's positive, is the product of this guy. It's a weighted version\nof my likelihood.",
    "start": "2401830",
    "end": "2406920"
  },
  {
    "text": "That's all it is. I'm just weighing\nthe likelihood, using my prior belief on theta.",
    "start": "2406920",
    "end": "2413150"
  },
  {
    "text": "And so given this guy\na natural estimator, if you follow the maximum\nlikelihood principle,",
    "start": "2413150",
    "end": "2419480"
  },
  {
    "text": "would be the maximum\nof this posterior. Agreed?",
    "start": "2419480",
    "end": "2424619"
  },
  {
    "text": "That would basically be doing\nexactly what maximum likelihood estimation is telling you.",
    "start": "2424620",
    "end": "2431740"
  },
  {
    "text": "So it turns out that you can. It's called Maximum\nA Posteriori, and I won't talk much\nabout this, or MAP.",
    "start": "2431740",
    "end": "2439370"
  },
  {
    "text": "That's Maximum a Posteriori.",
    "start": "2439370",
    "end": "2444500"
  },
  {
    "text": "So it's just the\ntheta hat is the arc max of pi theta, given X1, Xn.",
    "start": "2444500",
    "end": "2450790"
  },
  {
    "text": " And it sounds like it's OK.",
    "start": "2450790",
    "end": "2456190"
  },
  {
    "text": "I'll give you a\ndensity and you say, OK I have a density for all\nvalues of my parameters. You're asking me to\nsummarize it into one number.",
    "start": "2456190",
    "end": "2463440"
  },
  {
    "text": "I'm just going to take the most\nlikely number of those guys. But you could summarize\nit, otherwise. You could take the average.",
    "start": "2463440",
    "end": "2470770"
  },
  {
    "text": "You could take the median. You could take a\nbunch of numbers. And the beauty of\nBayesian statistics",
    "start": "2470770",
    "end": "2476080"
  },
  {
    "text": "is that, you don't have to\ntake any number in particular. You have an entire\nposterior distribution.",
    "start": "2476080",
    "end": "2481480"
  },
  {
    "text": "This is not only telling\nyou where theta is, but it's actually telling\nyou the difference",
    "start": "2481480",
    "end": "2489160"
  },
  {
    "text": "if you actually\ngive as something that gives you the posterior. Now, let's say the theta\nis p between 0 and 1.",
    "start": "2489160",
    "end": "2496270"
  },
  {
    "text": "If my posterior distribution\nlooks like this, or my posterior distribution\nlooks like this,",
    "start": "2496270",
    "end": "2503410"
  },
  {
    "text": "then those two guys\nhave one, the same mode. This is the same value.",
    "start": "2503410",
    "end": "2509200"
  },
  {
    "text": "And their symmetric, so they'll\nalso have the same mean. So these two posterior\ndistributions give me the same\nsummary into one number.",
    "start": "2509200",
    "end": "2515500"
  },
  {
    "text": "However clearly, one\nis much more confident than the other one. So I might as well just\nspit it out as a solution.",
    "start": "2515500",
    "end": "2524010"
  },
  {
    "text": "You can do even better. People actually do things,\nsuch as drawing a random number",
    "start": "2524010",
    "end": "2529559"
  },
  {
    "text": "from this distribution. Say, this is my number. That's kind of\ndangerous, but you can imagine you could do this.",
    "start": "2529560",
    "end": "2535690"
  },
  {
    "start": "2535690",
    "end": "2540730"
  },
  {
    "text": "This is what works. That's what we went through. So here, as you notice I don't\ncare so much about this part",
    "start": "2540730",
    "end": "2548650"
  },
  {
    "text": "here. Because it does not\ndepend on theta. So I know that given the\nproduct of those two things,",
    "start": "2548650",
    "end": "2555190"
  },
  {
    "text": "this thing is only the\nconstant that I need to divide so that when I integrate\nthis thing over theta, it integrates to one.",
    "start": "2555190",
    "end": "2561460"
  },
  {
    "text": "Because this has to be a\nprobability density on theta. I can write this and just\nforget about that part.",
    "start": "2561460",
    "end": "2567910"
  },
  {
    "text": "And that's what's written\non the top of this slide. This notation, this sort of\nweird alpha, or I don't know.",
    "start": "2567910",
    "end": "2577920"
  },
  {
    "text": "Infinity sign\npropped to the right. Whatever you want\nto call this thing is actually just really\nemphasizing the fact",
    "start": "2577920",
    "end": "2584700"
  },
  {
    "text": "that I don't care. I write it because I can,\nbut you know what it is.",
    "start": "2584700",
    "end": "2592490"
  },
  {
    "text": " In some instances, you have\nto compute the integral.",
    "start": "2592490",
    "end": "2599480"
  },
  {
    "text": "In some instances, you don't\nhave to compute the integral. And a lot of\nBayesian computation is about saying,\nOK it's actually",
    "start": "2599480",
    "end": "2605599"
  },
  {
    "text": "really hard to\ncompute this integral, so I'd rather not doing it. So let me try to find some\nmethods that will allow me",
    "start": "2605600",
    "end": "2611450"
  },
  {
    "text": "to sample from the\nposterior distribution, without having to compute this. And that's what's called\nMonte-Carlo Markov",
    "start": "2611450",
    "end": "2617720"
  },
  {
    "text": "chains, or MCMC, and that's\nexactly what they're doing. They're just using\nonly ratios of things, like that for different thetas.",
    "start": "2617720",
    "end": "2624130"
  },
  {
    "text": "And which means that\nif you take ratios, the normalizing constant\nis gone and you don't need to find this integral.",
    "start": "2624130",
    "end": "2630810"
  },
  {
    "text": "So we won't go into\nthose details at all. That would be the purpose\nof an entire course on Bayesian inference.",
    "start": "2630810",
    "end": "2636630"
  },
  {
    "text": "Actually, even\nBayesian computations would be an entire\ncourse on its own.",
    "start": "2636630",
    "end": "2642154"
  },
  {
    "text": "And there's some very\ninteresting things that are going on there,\nthe interface of stats and computation. ",
    "start": "2642154",
    "end": "2650054"
  },
  {
    "text": "So let's go back to our example\nand see if we can actually compute any of those things. Because it's very nice to give\nyou some data, some formulas.",
    "start": "2650054",
    "end": "2657420"
  },
  {
    "text": "Let's see if we\ncan actually do it. In particular, can I\nactually recover this claim",
    "start": "2657420",
    "end": "2663810"
  },
  {
    "text": "that the posterior associated\nto a beta prior with a Bernoulli",
    "start": "2663810",
    "end": "2671250"
  },
  {
    "text": "likelihood is actually\ngiving me a beta again? What was my prior?",
    "start": "2671250",
    "end": "2676710"
  },
  {
    "start": "2676710",
    "end": "2682670"
  },
  {
    "text": "So p was following\na beta AA, which means that p, the density.",
    "start": "2682670",
    "end": "2688320"
  },
  {
    "start": "2688320",
    "end": "2693620"
  },
  {
    "text": "That was pi of theta. Well I'm going to\nwrite this as pi of p--",
    "start": "2693620",
    "end": "2699580"
  },
  {
    "text": "was proportional to p to the\nA minus 1 times 1 minus p",
    "start": "2699580",
    "end": "2705800"
  },
  {
    "text": "to the A minus 1. So that's the first ingredient\nI need to complete my posterior.",
    "start": "2705800",
    "end": "2711430"
  },
  {
    "text": "I really need only two, if I\nwanted to bound up to constant. The second one was p hat. ",
    "start": "2711430",
    "end": "2720710"
  },
  {
    "text": "We've computed that many times. And we had even a nice\ncompact way of writing it, which was that pn of X1,\nXn, given the parameter p.",
    "start": "2720710",
    "end": "2732570"
  },
  {
    "text": "So the joint density of my data,\ngiven p, that's my likelihood. The likelihood of p was what?",
    "start": "2732570",
    "end": "2738730"
  },
  {
    "text": "Well it was p to\nthe sum of Xi's. ",
    "start": "2738730",
    "end": "2744030"
  },
  {
    "text": "1 minus p to the n\nminus some of the Xi's. ",
    "start": "2744030",
    "end": "2750990"
  },
  {
    "text": "Anybody wants me\nto parse this more? Or do you remember seeing\nthat from maximum likelihood",
    "start": "2750990",
    "end": "2756060"
  },
  {
    "text": "estimation? Yeah? AUDIENCE: [INAUDIBLE]",
    "start": "2756060",
    "end": "2762929"
  },
  {
    "text": "PHILLIPE RIGOLLET: That's\nwhat conditioning does. ",
    "start": "2762929",
    "end": "2770838"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nprevious slide. [INAUDIBLE] bottom\nthere, it says D pi of t.",
    "start": "2770838",
    "end": "2779151"
  },
  {
    "text": "Shouldn't it be dt pi of t? PHILLIPE RIGOLLET:\nSo D pi of T is",
    "start": "2779151",
    "end": "2785300"
  },
  {
    "text": "a measure theoretic notation,\nwhich I used without thinking. And I should not because\nI can see it upsets you.",
    "start": "2785300",
    "end": "2792380"
  },
  {
    "text": "D pi of T is just a\nnatural way to say that I integrate\nagainst whatever I'm",
    "start": "2792380",
    "end": "2798170"
  },
  {
    "text": "given for the prior of theta.",
    "start": "2798170",
    "end": "2803930"
  },
  {
    "text": "In particular, if theta is just\nthe mix of a PDF and a point mass, maybe I say\nthat my p takes",
    "start": "2803930",
    "end": "2811430"
  },
  {
    "text": "value 0.5 with probability 0.5. And then is uniform on the\ninterval with probability 0.5.",
    "start": "2811430",
    "end": "2818900"
  },
  {
    "text": "For this, I neither\nhave a PDF nor a PMF. But I can still talk about\nintegrating with respect",
    "start": "2818900",
    "end": "2824150"
  },
  {
    "text": "to this, right? It's going to look like, if\nI take a function f of T, D pi of T is going to be\none half of f of one half.",
    "start": "2824150",
    "end": "2834480"
  },
  {
    "text": "That's the point mass\nwith probability one half, at one half. Plus one half of the integral\nbetween 0 and 1, of f of TDT.",
    "start": "2834480",
    "end": "2843230"
  },
  {
    "text": "This is just the notation, which\nis actually funnily enough, interchangeable with pi of DT.",
    "start": "2843230",
    "end": "2849359"
  },
  {
    "text": " But if you have a\ndensity, it's really",
    "start": "2849360",
    "end": "2854890"
  },
  {
    "text": "just the density pi of TDT. If pi is really a\ndensity, but that's",
    "start": "2854890",
    "end": "2861940"
  },
  {
    "text": "when it's when pi is and\nmeasure and not a density.  Everybody else,\nforget about this.",
    "start": "2861940",
    "end": "2869700"
  },
  {
    "text": "This is not something\nyou should really worry about at this point. This is more graduate\nlevel probability classes.",
    "start": "2869700",
    "end": "2875719"
  },
  {
    "text": "But yeah, it's called\nmeasure theory. And that's when you think\nof pi as being a measure in an abstract fashion. You don't have to worry\nwhether it's a density",
    "start": "2875719",
    "end": "2881896"
  },
  {
    "text": "or not, or whether\nit has a density. ",
    "start": "2881896",
    "end": "2888350"
  },
  {
    "text": "So everybody is OK with this? ",
    "start": "2888350",
    "end": "2895530"
  },
  {
    "text": "Now I need to\ncompute my posterior. And as I said, my\nposterior is really",
    "start": "2895530",
    "end": "2903119"
  },
  {
    "text": "just the product of\nthe likelihood weighted by the prior.",
    "start": "2903120",
    "end": "2908970"
  },
  {
    "text": "Hopefully, at this stage\nof your application, you can multiply two functions.",
    "start": "2908970",
    "end": "2915390"
  },
  {
    "text": "So what's happening is,\nif I multiply this guy with this guy, p gets\nthis guy to the power",
    "start": "2915390",
    "end": "2921300"
  },
  {
    "text": "this guy plus this guy. ",
    "start": "2921300",
    "end": "2933810"
  },
  {
    "text": "And then 1 minus p gets the\npower n minus some of Xi's.",
    "start": "2933810",
    "end": "2940020"
  },
  {
    "text": "So this is always\nfrom I equal 1 to n. And then plus A minus 1 as well. ",
    "start": "2940020",
    "end": "2950010"
  },
  {
    "text": "This is up to constant, because\nI still need to solve this.",
    "start": "2950010",
    "end": "2955560"
  },
  {
    "text": "And I could try to do it. But I really don't\nhave to, because I know that if my density\nhas this form, then",
    "start": "2955560",
    "end": "2964380"
  },
  {
    "text": "it's a beta distribution. And then I can just\ngo on Wikipedia and see what should be\nthe normalization factor. But I know it's going to\nbe a beta distribution.",
    "start": "2964380",
    "end": "2971020"
  },
  {
    "text": "It's actually the\nbeta with parameter. So this is really my beta\nwith parameter, sum of Xi,",
    "start": "2971020",
    "end": "2979210"
  },
  {
    "text": "i equal 1 to n plus A minus 1. And then the second\nparameter is n minus sum",
    "start": "2979210",
    "end": "2986130"
  },
  {
    "text": "of the Xi's plus A minus 1. ",
    "start": "2986130",
    "end": "2994980"
  },
  {
    "text": "I just wrote what was here. What happened to my one?",
    "start": "2994980",
    "end": "3001580"
  },
  {
    "text": "Oh no, sorry. Beta has the power minus 1. So that's the\nparameter of the beta.",
    "start": "3001580",
    "end": "3008847"
  },
  {
    "text": "And this is the\nparameter of the beta. ",
    "start": "3008847",
    "end": "3015127"
  },
  {
    "text": "Beta is over there, right? So I just replace\nA by what I see. A is just becoming\nthis guy plus this guy",
    "start": "3015127",
    "end": "3022290"
  },
  {
    "text": "and this guy plus this guy. Everybody is comfortable\nwith this computation?",
    "start": "3022290",
    "end": "3028661"
  },
  {
    "start": "3028662",
    "end": "3034170"
  },
  {
    "text": "We just agreed that beta priors\nfor Bernoulli observations are certainly convenient.",
    "start": "3034170",
    "end": "3042540"
  },
  {
    "text": "Because they are just\nconjugate, and we know that's what is going\nto come out in the end. That's going to\nbe a beta as well.",
    "start": "3042540",
    "end": "3048899"
  },
  {
    "text": "I just claim it was convenient. It was certainly convenient\nto compute this, right? There was certainly\nsome compatibility",
    "start": "3048899",
    "end": "3055741"
  },
  {
    "text": "when I had to multiply this\nfunction by that function. And you can imagine that things\ncould go much more wrong,",
    "start": "3055741",
    "end": "3060916"
  },
  {
    "text": "than just having p to some power\nand p to some power, 1 minus p to some power, when it might\njust be some other power.",
    "start": "3060916",
    "end": "3066390"
  },
  {
    "text": "Things were nice. Now this is nice, but I can also\nquestion the following things.",
    "start": "3066390",
    "end": "3072410"
  },
  {
    "text": "Why beta, for one? The beta tells me something.",
    "start": "3072410",
    "end": "3077840"
  },
  {
    "text": "That's convenient, but\nthen how do I pick A? I know that A should definitely\ncapture the fact that where",
    "start": "3077840",
    "end": "3087500"
  },
  {
    "text": "I want to have my p\nmost likely located. But it also actually\nalso captures the variance of my beta.",
    "start": "3087500",
    "end": "3094579"
  },
  {
    "text": "And so choosing\ndifferent As is going to have different functions. If I have A and B, If I started\nwith the beta with parameter.",
    "start": "3094580",
    "end": "3103050"
  },
  {
    "text": "If I started with a B here, I\nwould just pick up the B here.",
    "start": "3103050",
    "end": "3108110"
  },
  {
    "text": "Agreed? And that would just\nbe a symmetric. But they're going to\ncapture mean and variance",
    "start": "3108110",
    "end": "3113270"
  },
  {
    "text": "of this thing. And so how do I pick those guys? If I'm a doctor and\nyou're asking me,",
    "start": "3113270",
    "end": "3119437"
  },
  {
    "text": "what do you think the\nchances of this drug working in this kind of patients is? And I have to spit out the\nparameters of a beta for you,",
    "start": "3119437",
    "end": "3126080"
  },
  {
    "text": "it might be a bit of a\ncomplicated thing to do. So how do you do this,\nespecially for problems? So by now, people\nhave actually mastered",
    "start": "3126080",
    "end": "3134750"
  },
  {
    "text": "the art of coming up with how\nto formulate those numbers. But in new problems that\ncome up, how do you do this?",
    "start": "3134750",
    "end": "3141660"
  },
  {
    "text": "What happens if you want\nto use Bayesian methods, but you actually do not\nknow what you expect to see?",
    "start": "3141660",
    "end": "3150140"
  },
  {
    "text": "To be fair, before we started\nthis class, I hope all of you had no idea whether people tend\nto bend their head to the right",
    "start": "3150140",
    "end": "3156870"
  },
  {
    "text": "or to the left before kissing. Because if you did, well\nyou have too much time on your hands and I should\ndouble your homework.",
    "start": "3156870",
    "end": "3162130"
  },
  {
    "text": " So in this case,\nmaybe you still want to use the Bayesian machinery.",
    "start": "3162130",
    "end": "3168830"
  },
  {
    "text": "Maybe you just want\nto do something nice. It's nice right, I mean\nit worked out pretty well. What if you want to do?",
    "start": "3168830",
    "end": "3174470"
  },
  {
    "text": "Well you actually want\nto use some priors that carry no information, that\nbasically do not prefer",
    "start": "3174470",
    "end": "3180170"
  },
  {
    "text": "any theta to another theta. Now, you could read\nthis slide or you",
    "start": "3180170",
    "end": "3185435"
  },
  {
    "text": "could look at this formula.  We just said that this\npi here was just here",
    "start": "3185435",
    "end": "3194920"
  },
  {
    "text": "to weigh some thetas more\nthan others, depending on their prior belief. If our prior belief\ndoes not want",
    "start": "3194920",
    "end": "3201400"
  },
  {
    "text": "to put any preference towards\nsome thetas than to others, what do I do? AUDIENCE: [INAUDIBLE]",
    "start": "3201400",
    "end": "3207655"
  },
  {
    "text": "PHILLIPE RIGOLLET:\nYeah, I remove it. And the way to remove\nsomething we multiply by, is just replace it by one. That's really what we're doing.",
    "start": "3207655",
    "end": "3215099"
  },
  {
    "text": "If this was a constant\nnot depending on theta, then that would mean that\nwe're not preferring any theta.",
    "start": "3215100",
    "end": "3221400"
  },
  {
    "text": "And we're looking\nat the likelihood. But not as a function that\nwe're trying to maximize,",
    "start": "3221400",
    "end": "3226560"
  },
  {
    "text": "but it is a function that\nwe normalize in such a way that it's actually\na distribution.",
    "start": "3226560",
    "end": "3232570"
  },
  {
    "text": "So if I have pi,\nwhich is not here, this is really just taking\nthe like likelihood, which is a positive function.",
    "start": "3232570",
    "end": "3237990"
  },
  {
    "text": "It may not integrate\nto 1, so I normalize it so that it integrates to 1. And then I just say, well this\nis my posterior distribution.",
    "start": "3237990",
    "end": "3245120"
  },
  {
    "text": "Now I could just\nmaximize this thing and spit out my maximum\nlikelihood estimator. But I can also\nintegrate and find",
    "start": "3245120",
    "end": "3250849"
  },
  {
    "text": "what the expectation\nof this guy is. I can find what the\nmedian of this guy is. I can sample data from this guy.",
    "start": "3250850",
    "end": "3256369"
  },
  {
    "text": "I can build, understand what\nthe variance of this guy is. Which is something we did\nnot do when we just did",
    "start": "3256370",
    "end": "3261829"
  },
  {
    "text": "maximum likelihood estimation\nbecause given a function, all we cared about was the\narc max of this function.",
    "start": "3261830",
    "end": "3267998"
  },
  {
    "text": " These priors are\ncalled uninformative.",
    "start": "3267998",
    "end": "3276119"
  },
  {
    "text": "This is just replacing this\nnumber by one or by a constant.",
    "start": "3276120",
    "end": "3283440"
  },
  {
    "text": "Because it still\nhas to be a density. ",
    "start": "3283440",
    "end": "3289236"
  },
  {
    "text": "If I have a bounded\nset, I'm just looking for the\nuniform distribution on this bounded set, the\none that puts constant one",
    "start": "3289236",
    "end": "3296579"
  },
  {
    "text": "over the size of this thing. But if I have an\ninvalid set, what",
    "start": "3296580",
    "end": "3301589"
  },
  {
    "text": "is the density that\ntakes a constant value on the entire real\nline, for example?",
    "start": "3301590",
    "end": "3307555"
  },
  {
    "text": "What is this density? ",
    "start": "3307555",
    "end": "3313200"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILLIPE RIGOLLET:\nDoesn't exist, right?",
    "start": "3313200",
    "end": "3318530"
  },
  {
    "text": "It just doesn't exist. The way you can think\nof it is a Gaussian with the variance going\nto infinity, maybe,",
    "start": "3318530",
    "end": "3324860"
  },
  {
    "text": "or something like this. But you can think\nof it in many ways. You can think of the limit of\nthe uniform between minus T",
    "start": "3324860",
    "end": "3332330"
  },
  {
    "text": "and T, with T going to infinity. But this thing is actually zero. There's nothing there.",
    "start": "3332330",
    "end": "3339530"
  },
  {
    "text": "You can actually\nstill talk about this. You could always talk\nabout this thing, where you think of this guy\nas being a constant,",
    "start": "3339530",
    "end": "3346549"
  },
  {
    "text": "remove this thing from this\nequation, and just say, well my posterior is\njust the likelihood divided by the integral of\nthe likelihood over theta.",
    "start": "3346550",
    "end": "3354680"
  },
  {
    "text": "And if theta is the entire\nreal line, so be it. As long as this\nintegral converges,",
    "start": "3354680",
    "end": "3360390"
  },
  {
    "text": "you can still talk\nabout this stuff.  This is what's called\nan improper prior.",
    "start": "3360390",
    "end": "3366300"
  },
  {
    "text": " An improper prior is just a\nnon-negative function defined",
    "start": "3366300",
    "end": "3371990"
  },
  {
    "text": "in theta, but it does not have\nto integrate neither to one,",
    "start": "3371990",
    "end": "3377390"
  },
  {
    "text": "nor to anything.  If I integrate the\nfunction equal to 1",
    "start": "3377390",
    "end": "3382700"
  },
  {
    "text": "on the entire real\nline, what do I get? ",
    "start": "3382700",
    "end": "3387800"
  },
  {
    "text": "Infinity.  It's not a proper prior, and\nit's called and improper prior.",
    "start": "3387800",
    "end": "3395960"
  },
  {
    "text": "And those improper\npriors are usually what you see when you start\nto want non-informative priors",
    "start": "3395960",
    "end": "3402829"
  },
  {
    "text": "on infinite sets of datas. That's just the nature of it. You should think of them as\nbeing the uniform distribution",
    "start": "3402830",
    "end": "3410019"
  },
  {
    "text": "of some infinite set, if\nthat thing were to exist. ",
    "start": "3410020",
    "end": "3416360"
  },
  {
    "text": "Let's see some examples\nabout non-informative priors. If I'm in the interval 0,\n1 this is a finite set.",
    "start": "3416360",
    "end": "3424410"
  },
  {
    "text": "So I can talk about\nthe uniform prior on the interval 0, 1 for a\nparameter, p of a Bernoulli.",
    "start": "3424410",
    "end": "3430600"
  },
  {
    "start": "3430600",
    "end": "3446380"
  },
  {
    "text": "If I want to talk\nabout this, then it means that my prior is p follows\nsome uniform on the interval",
    "start": "3446380",
    "end": "3455910"
  },
  {
    "text": "0, 1. So that means that f of\nx is 1 if x is in 0, 1.",
    "start": "3455910",
    "end": "3468940"
  },
  {
    "text": "Otherwise, there is actually\nnot even a normalization. This thing integrates to 1. And so now if I look\nat my likelihood,",
    "start": "3468940",
    "end": "3476137"
  },
  {
    "text": "it's still the same thing. So my posterior\nbecomes theta X1, Xn.",
    "start": "3476137",
    "end": "3484510"
  },
  {
    "text": "That's my posterior. I don't write the\nlikelihood again, because we still have it--",
    "start": "3484510",
    "end": "3489830"
  },
  {
    "text": "well we don't have\nit here anymore. ",
    "start": "3489830",
    "end": "3495440"
  },
  {
    "text": "The likelihood is given here. Copy, paste over there.",
    "start": "3495440",
    "end": "3500930"
  },
  {
    "text": "The posterior is just\nthis thing times 1. So you will see it in a second. So it's p to the power sum\nof the Xi's, one minus p",
    "start": "3500930",
    "end": "3508570"
  },
  {
    "text": "to the power, n minus\nsum of the Xi's. And then it's multiplied by\n1, and then divided by this",
    "start": "3508570",
    "end": "3516380"
  },
  {
    "text": "integral between 0 and\n1 of p, sum of the Xi's.",
    "start": "3516380",
    "end": "3522250"
  },
  {
    "text": "1 minus p, n minus\nsum of the Xi's.",
    "start": "3522250",
    "end": "3527870"
  },
  {
    "text": "Dp, which does not depend on p. And I really don't care\nwhat the thing actually is.",
    "start": "3527870",
    "end": "3533990"
  },
  {
    "text": " That's posterior of p.",
    "start": "3533990",
    "end": "3543549"
  },
  {
    "text": "And now I can see,\nwell what is this? It's actually just the\nbeta with parameters.",
    "start": "3543550",
    "end": "3552870"
  },
  {
    "text": "This guy plus 1. ",
    "start": "3552870",
    "end": "3559670"
  },
  {
    "text": "And this guy plus 1. ",
    "start": "3559670",
    "end": "3574430"
  },
  {
    "text": "I didn't tell you what the\nexpectation of a beta was. We don't know what the\nexpectation of a beta",
    "start": "3574430",
    "end": "3579890"
  },
  {
    "text": "is, agreed? If I wanted to find say, the\nexpectation of this thing that",
    "start": "3579890",
    "end": "3585980"
  },
  {
    "text": "would be some good\nestimator, we know that the maximum\nof this guy-- what is the maximum of this thing?",
    "start": "3585980",
    "end": "3591110"
  },
  {
    "text": " Well, it's just this thing,\nit's the average of the Xi's.",
    "start": "3591110",
    "end": "3597936"
  },
  {
    "text": "That's just the maximum\nlikelihood estimator for Bernoulli. We know it's the average. Do you think if I take the\nexpectation of this thing,",
    "start": "3597937",
    "end": "3603910"
  },
  {
    "text": "I'm going to get the average? ",
    "start": "3603910",
    "end": "3613864"
  },
  {
    "text": "So actually, I'm not\ngoing to get the average. I'm going to get this guy plus\nthis guy, divided by n plus 1.",
    "start": "3613864",
    "end": "3619790"
  },
  {
    "start": "3619790",
    "end": "3627246"
  },
  {
    "text": "Let's look at what\nthis thing is doing. It's looking at the number\nof ones and it's adding one.",
    "start": "3627246",
    "end": "3634364"
  },
  {
    "text": "And this guy is looking\nat the number of zeros and it's adding one. Why is it adding this one?",
    "start": "3634364",
    "end": "3641910"
  },
  {
    "text": "What's going on here? ",
    "start": "3641910",
    "end": "3647510"
  },
  {
    "text": "This is going to matter\nmostly when the number of ones is actually zero, or the\nnumber of zeros is zero.",
    "start": "3647510",
    "end": "3656060"
  },
  {
    "text": "Because what it does is just\npushes the zero from non-zero. And why is that something that\nthis Bayesian method actually",
    "start": "3656060",
    "end": "3663020"
  },
  {
    "text": "does for you automatically? It's because when we\nput this non-informative prior on p, which was\nuniform on the interval 0, 1.",
    "start": "3663020",
    "end": "3671169"
  },
  {
    "text": "In particular, we know\nthat the probability that p is equal to 0 is zero.",
    "start": "3671169",
    "end": "3676690"
  },
  {
    "text": "And the probability p\nis equal to 1 is zero. And so the problem\nis that if I did not",
    "start": "3676690",
    "end": "3681880"
  },
  {
    "text": "add this 1 with some\npositive probability, I wouldn't be allowed to spit\nout something that actually had",
    "start": "3681880",
    "end": "3688120"
  },
  {
    "text": "p hat, which was equal to 0. If by chance, let's say\nI have n is equal to 3,",
    "start": "3688120",
    "end": "3693279"
  },
  {
    "text": "and I get only 0, 0, 0, that\ncould happen with probability. 1 over pq, one over 1 minus pq.",
    "start": "3693280",
    "end": "3701470"
  },
  {
    "text": " That's not something\nthat I want.",
    "start": "3701470",
    "end": "3707880"
  },
  {
    "text": "And I'm using my priors. My prior is not informative,\nbut somehow it captures the fact that I don't want to\nbelieve p is going",
    "start": "3707880",
    "end": "3713549"
  },
  {
    "text": "to be either equal to 0 or 1. So that's sort of\ntaken care of here.",
    "start": "3713550",
    "end": "3719790"
  },
  {
    "text": "So let's move away a little\nbit from the Bernoulli example,",
    "start": "3719790",
    "end": "3725640"
  },
  {
    "text": "shall we? I think we've seen enough of it. And so let's talk about\nthe Gaussian model.",
    "start": "3725640",
    "end": "3730860"
  },
  {
    "text": "Let's say I want to\ndo Gaussian inference. ",
    "start": "3730860",
    "end": "3737859"
  },
  {
    "text": "I want to do inference\nin a Gaussian model, using Bayesian methods. ",
    "start": "3737859",
    "end": "3750600"
  },
  {
    "text": "What I want is that Xi,\nX1, Xn, or say 0, 1 iid.",
    "start": "3750600",
    "end": "3759840"
  },
  {
    "text": " Sorry, theta 1, iid\nconditionally on theta.",
    "start": "3759840",
    "end": "3767770"
  },
  {
    "text": " That means that pn of\nX1, Xn, given theta",
    "start": "3767770",
    "end": "3776299"
  },
  {
    "text": "is equal to exactly\nwhat I wrote before. So 1 square root to pi, to the\nn exponential minus one half",
    "start": "3776300",
    "end": "3784760"
  },
  {
    "text": "sum of Xi minus theta squared. So that's just the\njoint distribution",
    "start": "3784760",
    "end": "3791120"
  },
  {
    "text": "of my Gaussian with mean data. And the another\nquestion is, what is the posterior distribution?",
    "start": "3791120",
    "end": "3797540"
  },
  {
    "text": "Well here I said, let's use\nthe uninformative prior, which is an improper prior.",
    "start": "3797540",
    "end": "3803840"
  },
  {
    "text": "It puts weight on everyone. That's the so-called uniform\non the entire real line.",
    "start": "3803840",
    "end": "3809310"
  },
  {
    "text": "So that's certainly\nnot a density. But it can still just use this.",
    "start": "3809310",
    "end": "3814359"
  },
  {
    "text": "So all I need to do\nis get this divided",
    "start": "3814360",
    "end": "3820430"
  },
  {
    "text": "by normalizing this thing. But if you look at\nthis, essentially I",
    "start": "3820430",
    "end": "3827900"
  },
  {
    "text": "want to understand. So this is proportional\nto the exponential minus one half\nsum from I equal 1",
    "start": "3827900",
    "end": "3835040"
  },
  {
    "text": "to n of Xi minus theta squared. And now I want to see\nthis thing as a density,",
    "start": "3835040",
    "end": "3841369"
  },
  {
    "text": "not on the Xi's but on theta. ",
    "start": "3841370",
    "end": "3846420"
  },
  {
    "text": "What I want is a\ndensity on theta. So it looks like I have\nchances of getting something",
    "start": "3846420",
    "end": "3853650"
  },
  {
    "text": "that looks like a Gaussian. To have a Gaussian, I would\nneed to see minus one half.",
    "start": "3853650",
    "end": "3859500"
  },
  {
    "text": "And then I would need to\nsee theta minus something here, not just the sum of\nsomething minus thetas.",
    "start": "3859500",
    "end": "3865230"
  },
  {
    "text": "So I need to work\na little bit more, to expand the square here.",
    "start": "3865230",
    "end": "3871475"
  },
  {
    "text": "So this thing here\nis going to be equal to exponential minus\none half sum from I equal 1",
    "start": "3871475",
    "end": "3877330"
  },
  {
    "text": "to n of Xi squared minus 2Xi\ntheta plus theta squared.",
    "start": "3877330",
    "end": "3885280"
  },
  {
    "start": "3885280",
    "end": "3910590"
  },
  {
    "text": "Now what I'm going to do\nis, everything remember is up to this little sign.",
    "start": "3910590",
    "end": "3915870"
  },
  {
    "text": "So every time I see a term\nthat does not depend on theta, I can just push it in there\nand just make it disappear.",
    "start": "3915870",
    "end": "3922250"
  },
  {
    "text": "Agreed? This term here, exponential\nminus one half sum of Xi",
    "start": "3922250",
    "end": "3928420"
  },
  {
    "text": "squared, does it\ndepend on theta? No. So I'm just pushing it here. This guy, yes.",
    "start": "3928420",
    "end": "3934530"
  },
  {
    "text": "And the other one, yes. So this is proportional to\nexponential sum of the Xi.",
    "start": "3934530",
    "end": "3945020"
  },
  {
    "text": "And then I'm going to pull out\nmy theta, the minus one half canceled with the minus 2.",
    "start": "3945020",
    "end": "3950150"
  },
  {
    "text": "And then I have minus\none half sum from I",
    "start": "3950150",
    "end": "3956460"
  },
  {
    "text": "equal 1 to n of theta squared. ",
    "start": "3956460",
    "end": "3961480"
  },
  {
    "text": "Agreed? So now what this\nthing looks like, this looks very much like some\ntheta minus something squared.",
    "start": "3961480",
    "end": "3969570"
  },
  {
    "text": "This thing here is really\njust n over 2 times theta.",
    "start": "3969570",
    "end": "3975110"
  },
  {
    "text": " Sorry, times theta squared.",
    "start": "3975110",
    "end": "3981740"
  },
  {
    "text": "So now what I need to do is to\nwrite this of the form, theta minus something. Let's call it mu, squared,\ndivided by 2 sigma squared.",
    "start": "3981740",
    "end": "3991820"
  },
  {
    "text": "I want to turn this into\nthat, maybe up to terms that do not depend on theta. That's what I'm\ngoing to try to do.",
    "start": "3991820",
    "end": "3999062"
  },
  {
    "text": "So that's called\ncompleting the squaring. That's some exercises you do. You've done it probably,\nalready in the homework.",
    "start": "3999062",
    "end": "4004260"
  },
  {
    "text": "And that's something\nyou do a lot when you do Bayesian\nstatistics, in particular. So let's do this.",
    "start": "4004260",
    "end": "4010010"
  },
  {
    "text": "What is it going to\nbe the leading term? Theta squared is going to\nbe multiplied by this thing. So I'm going to pull\nout my n over 2.",
    "start": "4010010",
    "end": "4017130"
  },
  {
    "text": "And then I'm going to write\nthis as minus theta over 2.",
    "start": "4017130",
    "end": "4023069"
  },
  {
    "text": "And then I'm going to write\ntheta minus something squared. And this something is going\nto be one half of what",
    "start": "4023070",
    "end": "4028890"
  },
  {
    "text": "I see in the cross-product.  I need to actually\npull this thing out.",
    "start": "4028890",
    "end": "4034590"
  },
  {
    "text": "So let me write it\nlike that first. So that's theta squared.",
    "start": "4034590",
    "end": "4041860"
  },
  {
    "text": "And then I'm going to write it\nas minus 2 times 1 over n sum",
    "start": "4041860",
    "end": "4050680"
  },
  {
    "text": "from I equal 1 to n\nof Xi's times theta.",
    "start": "4050680",
    "end": "4056980"
  },
  {
    "text": "That's exactly just a rewriting\nof what we had before. And that should look\nmuch more familiar. ",
    "start": "4056980",
    "end": "4064990"
  },
  {
    "text": "A squared minus 2 blap A,\nand then I missed something. So this thing, I'm going\nto be able to rewrite",
    "start": "4064990",
    "end": "4071859"
  },
  {
    "text": "as theta minus Xn bar squared.",
    "start": "4071860",
    "end": "4077930"
  },
  {
    "text": "But then I need to remove\nthe square of Xn bar. Because it's not here. ",
    "start": "4077930",
    "end": "4089210"
  },
  {
    "text": "So I just complete the square. And then I actually really don't\ncare with this thing actually was, because it's going to go\nagain in the little Alpha's",
    "start": "4089210",
    "end": "4096899"
  },
  {
    "text": "sign over there. So this thing\neventually is going to be proportional\nto exponential",
    "start": "4096899",
    "end": "4104620"
  },
  {
    "text": "of minus n over 2 times theta\nof minus Xn bar squared.",
    "start": "4104620",
    "end": "4111089"
  },
  {
    "text": "And so we know that if\nthis is a density that's proportional to this guy, it has\nto be some n with mean, Xn bar.",
    "start": "4111090",
    "end": "4124100"
  },
  {
    "text": "And variance, this is supposed\nto be 1 over sigma squared. This guy over here, this n.",
    "start": "4124100",
    "end": "4129317"
  },
  {
    "text": "So that's really just 1 over n.  So the posterior\ndistribution is a Gaussian",
    "start": "4129318",
    "end": "4141740"
  },
  {
    "text": "centered at the average\nof my observations. And with variance, 1 over n.",
    "start": "4141740",
    "end": "4148429"
  },
  {
    "text": " Everybody's with me?",
    "start": "4148430",
    "end": "4154140"
  },
  {
    "text": " Why I'm saying this, this was\nthe output of some computation.",
    "start": "4154140",
    "end": "4159778"
  },
  {
    "text": "But it sort of\nmakes sense, right? It's really telling me that\nthe more observations I have, the more concentrated\nthis posterior is.",
    "start": "4159779",
    "end": "4166250"
  },
  {
    "text": "Concentrated around what? Well around this Xn bar. That looks like something\nwe've sort of seen before.",
    "start": "4166250",
    "end": "4173139"
  },
  {
    "text": "But it does not have the\nsame meaning, somehow. This is really just the\nposterior distribution. ",
    "start": "4173140",
    "end": "4180490"
  },
  {
    "text": "It's sort of a sanity check,\nthat I have this 1 over n when I have Xn bar. But it's not the\nsame thing as saying",
    "start": "4180490",
    "end": "4185680"
  },
  {
    "text": "that the variance of Xn bar was\n1 over n, like we had before. ",
    "start": "4185680",
    "end": "4195670"
  },
  {
    "text": "As an exercise,\nI would recommend if you don't get it,\njust try pi of theta",
    "start": "4195670",
    "end": "4210140"
  },
  {
    "text": "to be equal to some n mu 1.",
    "start": "4210140",
    "end": "4215290"
  },
  {
    "text": " Here, the prior that we used\nwas completely non-informative.",
    "start": "4215290",
    "end": "4222350"
  },
  {
    "text": "What happens if I take my prior\nto be some Gaussian, which is centered at mu and\nit has the same variance",
    "start": "4222350",
    "end": "4227510"
  },
  {
    "text": "as the other guys? So what's going to\nhappen here is that we're going to put a weight.",
    "start": "4227510",
    "end": "4233120"
  },
  {
    "text": "And everything\nthat's away from mu is going to actually\nget less weight.",
    "start": "4233120",
    "end": "4238469"
  },
  {
    "text": "I want to know how I'm\ngoing to be updating this prior into a posterior. ",
    "start": "4238469",
    "end": "4244520"
  },
  {
    "text": "Everybody sees what\nI'm saying here? So that means that pi of theta\nhas the density proportional",
    "start": "4244520",
    "end": "4250040"
  },
  {
    "text": "to exponential minus one\nhalf theta minus mu squared.",
    "start": "4250040",
    "end": "4255680"
  },
  {
    "text": "So I need to multiply\nmy posterior with this, and then see.",
    "start": "4255680",
    "end": "4261849"
  },
  {
    "text": "It's actually going\nto be a Gaussian. This is also a conjugate prior. It's going to spit\nout another Gaussian. You're going to have to complete\na square again, and just check",
    "start": "4261849",
    "end": "4269390"
  },
  {
    "text": "what it's actually giving you. And so spoiler alert,\nit's going to look like you get an extra\nobservation, which is actually",
    "start": "4269390",
    "end": "4274790"
  },
  {
    "text": "equal to mu.  It's going to be the average\nof n plus 1 observations.",
    "start": "4274790",
    "end": "4282440"
  },
  {
    "text": "The first n1's being X1 to Xn. And then, the last one being mu.",
    "start": "4282440",
    "end": "4287530"
  },
  {
    "text": "And it sort of makes sense. That's actually a\nfairly simple exercise.",
    "start": "4287530",
    "end": "4294699"
  },
  {
    "text": "Rather than going\ninto more computation, this is something\nyou can definitely do when you're in the\ncomfort of your room.",
    "start": "4294700",
    "end": "4301510"
  },
  {
    "text": "I want to talk about\nother types of priors. The first thing I said is,\nthere's this beta prior",
    "start": "4301510",
    "end": "4307330"
  },
  {
    "text": "that I just pulled out of my hat\nand that was just convenient. Then there was this\nnon-informative prior.",
    "start": "4307330",
    "end": "4312860"
  },
  {
    "text": "It was convenient. It was non-informative, so\nif you don't know anything else maybe that's\nwhat you want to do.",
    "start": "4312860",
    "end": "4318949"
  },
  {
    "text": "The question is, are there\nany other priors that are sort of principled\nand generic, in the sense",
    "start": "4318950",
    "end": "4324489"
  },
  {
    "text": "that the uninformative\nprior was generic, right? It was equal to 1, that's\nas generic as it gets.",
    "start": "4324490",
    "end": "4331400"
  },
  {
    "text": "So is there anything\nthat's generic as well? Well, there's this priors that\nare called Jeffrey's priors.",
    "start": "4331400",
    "end": "4337180"
  },
  {
    "text": "And Jeffrey's prior, which is\nproportional to square root of the determinant of the\nFisher information of theta.",
    "start": "4337180",
    "end": "4343290"
  },
  {
    "text": " This is actually a\nweird thing to do.",
    "start": "4343290",
    "end": "4348600"
  },
  {
    "text": "It says, look at your model. Your model is going to\nhave a Fisher information.",
    "start": "4348600",
    "end": "4354152"
  },
  {
    "text": "Let's say it exists.  Because we know it\ndoes not always exist.",
    "start": "4354152",
    "end": "4359957"
  },
  {
    "text": "For example, in the\nmultinomial model, we didn't have a\nFisher information. The determinant of\na matrix is somehow",
    "start": "4359957",
    "end": "4366670"
  },
  {
    "text": "measuring the size of a matrix. If you don't trust\nme, just think about the matrix being\nof size one by one,",
    "start": "4366670",
    "end": "4373870"
  },
  {
    "text": "then the determinant is just\nthe number that you have there. And so this is really something\nthat looks like the Fisher",
    "start": "4373870",
    "end": "4380770"
  },
  {
    "text": "information.  It's proportional to the\namount of information",
    "start": "4380770",
    "end": "4386290"
  },
  {
    "text": "that you have at\na certain point. And so what my prior\nis saying well,",
    "start": "4386290",
    "end": "4392310"
  },
  {
    "text": "I want to put more weights\non those thetas that are going to just extract more\ninformation from the data. ",
    "start": "4392310",
    "end": "4400510"
  },
  {
    "text": "You can actually\ncompute those things. In the first example,\nJeffrey's prior",
    "start": "4400510",
    "end": "4406215"
  },
  {
    "text": "is something that\nlooks like this. In one dimension,\nFisher information is essentially one\nthe word variance.",
    "start": "4406215",
    "end": "4413476"
  },
  {
    "text": "That's just 1 over the\nsquare root of the variance, because I have the square root. And when I have the Jeffrey's\nprior, when I have the Gaussian",
    "start": "4413476",
    "end": "4425770"
  },
  {
    "text": "case, this is the\nidentity matrix that I would have in\nthe Gaussian case.",
    "start": "4425770",
    "end": "4430840"
  },
  {
    "text": "The determinant of\nthe identities is 1. So square root of 1 is 1, and\nso I would basically get 1.",
    "start": "4430840",
    "end": "4436180"
  },
  {
    "text": "And that gives me my improper\nprior, my uninformative prior that I had. So the uninformative\nprior 1 is fine.",
    "start": "4436180",
    "end": "4443690"
  },
  {
    "text": "Clearly, all the thetas\ncarry the same information in the Gaussian model. Whether I translate\nit here or here,",
    "start": "4443690",
    "end": "4450200"
  },
  {
    "text": "it's pretty clear none\nof them is actually better than the other. But clearly for\nthe Bernoulli case,",
    "start": "4450200",
    "end": "4456530"
  },
  {
    "text": "the p's that are closer\nto the boundary carry",
    "start": "4456530",
    "end": "4462559"
  },
  {
    "text": "more information. I sort of like those\nguys, because they just carry more information.",
    "start": "4462560",
    "end": "4467757"
  },
  {
    "text": "So what I do is, I\ntake this function. So p1 minus p. Remember, it's something\nthat looks like this.",
    "start": "4467757",
    "end": "4474170"
  },
  {
    "text": "On the interval 0, 1.  This guy, 1 over square\nroot of p1 minus p",
    "start": "4474170",
    "end": "4480979"
  },
  {
    "text": "is something that\nlooks like this.  Agreed",
    "start": "4480979",
    "end": "4487619"
  },
  {
    "text": "What it's doing is\nsort of wants to push towards the piece that actually\ncarry more information.",
    "start": "4487620",
    "end": "4494586"
  },
  {
    "text": "Whether you want to\nbias your data that way or not, is something\nyou need to think about. When you put a prior on your\ndata, on your parameter,",
    "start": "4494586",
    "end": "4501550"
  },
  {
    "text": "you're sort of biasing\ntowards this idea your data. That's maybe not\nsuch a good idea,",
    "start": "4501550",
    "end": "4507700"
  },
  {
    "text": "when you have some p that's\nactually close to one half,",
    "start": "4507700",
    "end": "4513160"
  },
  {
    "text": "for example. You're actually\nsaying, no I don't want to see a p that's\nclose to one half. Just make a decision,\none way or another.",
    "start": "4513160",
    "end": "4518350"
  },
  {
    "text": "But just make a decision. So it's forcing you to do that. ",
    "start": "4518350",
    "end": "4523690"
  },
  {
    "text": "Jeffrey's prior, I'm\nrunning out of time so I don't want to go\ninto too much detail.",
    "start": "4523690",
    "end": "4529849"
  },
  {
    "text": "We'll probably stop\nhere, actually. ",
    "start": "4529850",
    "end": "4544570"
  },
  {
    "text": "So Jeffrey's priors have\nthis very nice property. It's that they actually do not\ncare about the parameterization",
    "start": "4544570",
    "end": "4551740"
  },
  {
    "text": "of your space. If you actually have\np and you suddenly decide that p is not the\nright parameter for Bernoulli,",
    "start": "4551740",
    "end": "4558850"
  },
  {
    "text": "but it's p squared. You could decide to\nparameterize this by p squared. Maybe your doctor is\nactually much more able",
    "start": "4558850",
    "end": "4565840"
  },
  {
    "text": "to formulate some prior\nassumption on p squared, rather than p. You never know.",
    "start": "4565840",
    "end": "4571099"
  },
  {
    "text": "And so what happens is\nthat Jeffrey's priors are an invariant in this. And the reason is because\nthe information carried by p",
    "start": "4571100",
    "end": "4578560"
  },
  {
    "text": "is the same as the information\ncarried by p squared, somehow. ",
    "start": "4578560",
    "end": "4588822"
  },
  {
    "text": "They're essentially\nthe same thing.  You need to have one to one map.",
    "start": "4588822",
    "end": "4594630"
  },
  {
    "text": "Where you basically for\neach parameter, before you have another parameter. Let's call Eta the\nnew parameters.",
    "start": "4594630",
    "end": "4600810"
  },
  {
    "text": " The PDF of the new prior\nindexed by Eta this time",
    "start": "4600810",
    "end": "4610380"
  },
  {
    "text": "is actually also\nJeffrey's prior. But this time, the\nnew Fisher information is not the Fisher information\nwith respect to theta.",
    "start": "4610380",
    "end": "4617340"
  },
  {
    "text": "But it's this Fisher\ninformation associated to this statistical\nmodel indexed by Eta.",
    "start": "4617340",
    "end": "4623130"
  },
  {
    "text": "So essentially, when you\nchange the parameterization of your model, you still\nget Jeffrey's prior",
    "start": "4623130",
    "end": "4630600"
  },
  {
    "text": "for the new parameterization. Which is, in a way,\na desirable property. ",
    "start": "4630600",
    "end": "4639410"
  },
  {
    "text": "Jeffrey's prior is just\nan uninformative priors, or priors you want\nto use when you want a systematic way without\nreally thinking about what",
    "start": "4639410",
    "end": "4646480"
  },
  {
    "text": "to pick for your mile. ",
    "start": "4646480",
    "end": "4655440"
  },
  {
    "text": "I'll finish this next time. And we'll talk about\nBayesian confidence regions. We'll talk about\nBayesian estimation.",
    "start": "4655440",
    "end": "4661620"
  },
  {
    "text": "Once I have a posterior,\nwhat do I get? And basically, the\nonly message is going to be that you\nmight want to integrate",
    "start": "4661620",
    "end": "4667860"
  },
  {
    "text": "against the posterior. Find the posterior, the\nexpectation of your posterior distribution. That's a good point\nestimator for theta.",
    "start": "4667860",
    "end": "4674010"
  },
  {
    "text": " We'll just do a\ncouple of computation.",
    "start": "4674010",
    "end": "4681020"
  },
  {
    "start": "4681020",
    "end": "4684950"
  }
]