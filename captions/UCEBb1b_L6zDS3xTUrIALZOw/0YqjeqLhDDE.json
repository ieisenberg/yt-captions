[
  {
    "text": "[SQUEAKING] [RUSTLING] [CLICKING]",
    "start": "0",
    "end": "5357"
  },
  {
    "start": "5357",
    "end": "10389"
  },
  {
    "text": "ALAN EDELMAN: So\nwelcome, everybody, to this IAP class\non matrix calculus. I'm Professor Alan Edelman.",
    "start": "10390",
    "end": "16170"
  },
  {
    "text": "And the other professor\nis Steven Johnson,",
    "start": "16170",
    "end": "21270"
  },
  {
    "text": "who's going to have-- he had to go out of town. And so we're going to take\nadvantage of Zoom technology",
    "start": "21270",
    "end": "28020"
  },
  {
    "text": "so that he could give\nlectures remotely. We did some testing of\nthe audio and the video. So hopefully it'll\nwork out just fine.",
    "start": "28020",
    "end": "35410"
  },
  {
    "text": "So maybe just a\nquick introduction. So-- oh, good, good.",
    "start": "35410",
    "end": "43059"
  },
  {
    "text": "Yes, Steven-- recording? STEVEN JOHNSON: I\nalready started it. ALAN EDELMAN: All right. Thank you for the reminder.",
    "start": "43060",
    "end": "48220"
  },
  {
    "text": "Yeah. So I even put the two\nreminders on the blackboard. Yeah, so that was\nreminder number one. Good.",
    "start": "48220",
    "end": "53270"
  },
  {
    "text": "OK. So let's see. So both of us are in\nthe math department.",
    "start": "53270",
    "end": "59590"
  },
  {
    "text": "I'm also in CSAIL. And I run the Julia\nlab over in CSAIL.",
    "start": "59590",
    "end": "65560"
  },
  {
    "text": "And my own research involves\nboth mathematics as well as",
    "start": "65560",
    "end": "71470"
  },
  {
    "text": "computing software. Steven, do you want to say a few\nwords quickly about yourself? STEVEN JOHNSON: Hello, everyone.",
    "start": "71470",
    "end": "77150"
  },
  {
    "text": "So some of you may remember\nme if you took 18.06 in the spring-- ALAN EDELMAN: How many of\nyou took 18.06 in the spring?",
    "start": "77150",
    "end": "83409"
  },
  {
    "text": "STEVEN JOHNSON: --or\nin the fall with me. ALAN EDELMAN: Or in the fall. So there's two, or\nthree, or four hands. OK.",
    "start": "83410",
    "end": "88750"
  },
  {
    "text": "STEVEN JOHNSON: Yep. Yeah. So those people who\ntook 18.06 with me will have gotten one hour of\nthese 16 hours of lectures",
    "start": "88750",
    "end": "97000"
  },
  {
    "text": "that we're going\nto do this IAP on. And that'll be familiar. But yeah.",
    "start": "97000",
    "end": "102440"
  },
  {
    "text": "So I'm also in the\nmath department. And I'm also in physics. And I come into matrix\ncalculus through a lot",
    "start": "102440",
    "end": "110750"
  },
  {
    "text": "of PDE-constrained\noptimization mainly. But I've also worked with\nJulia, which will also",
    "start": "110750",
    "end": "117500"
  },
  {
    "text": "be used on the problem sets. And so I'm sorry I can't be\nthere with you in person.",
    "start": "117500",
    "end": "124130"
  },
  {
    "text": "I'll be missing-- I'll be doing it remotely for\nthe first week and a half. And then I'll be back in person.",
    "start": "124130",
    "end": "129313"
  },
  {
    "text": " ALAN EDELMAN: OK. So let me quickly show\neverybody the GitHub site",
    "start": "129313",
    "end": "138020"
  },
  {
    "text": "that we're going to be using. Where-- did it disappear on me? Where did it go?",
    "start": "138020",
    "end": "143120"
  },
  {
    "text": "Oh, there it is. OK. So-- oh, no. That's not the right one. This was another course.",
    "start": "143120",
    "end": "149790"
  },
  {
    "text": "All right. I think we should\njust dive right in. So let me go ahead and do that. So yes, here are the lectures,\nthe problem sets-- three units.",
    "start": "149790",
    "end": "159530"
  },
  {
    "text": "And we assume people are\nfamiliar with linear algebra and not much more. OK?",
    "start": "159530",
    "end": "164690"
  },
  {
    "text": "And there's no assumption\nthat you've already used Julia before. But of course, if\nyou've taken 18.06 with Professor Johnson\nor some other courses,",
    "start": "164690",
    "end": "172400"
  },
  {
    "text": "you may already be\nsomewhat familiar. But you could easily learn. The kind of Julia\nwe're using here",
    "start": "172400",
    "end": "178100"
  },
  {
    "text": "is sort of basic\ncalculator Julia. And so you won't\nneed to know much.",
    "start": "178100",
    "end": "183570"
  },
  {
    "text": "OK? So let me delve right into where\ndoes matrix calculus fit in.",
    "start": "183570",
    "end": "189600"
  },
  {
    "text": "So if you look at MIT's\ncourse catalog, which I've copied over here-- this is replicated\nprobably in universities",
    "start": "189600",
    "end": "196760"
  },
  {
    "text": "all over the planet, right? There's single-variable\ncalculus. This is, like, the first\nsemester of calculus.",
    "start": "196760",
    "end": "202010"
  },
  {
    "text": "And it's required at MIT\nfor all undergraduates to take 18.01 and\nlearn how to take",
    "start": "202010",
    "end": "207980"
  },
  {
    "text": "the derivative or an integral\nof a function of one variable. OK? And then there's\nthat second semester",
    "start": "207980",
    "end": "214490"
  },
  {
    "text": "of the sequence, 18.02, where\nyou learn vector calculus or multivariate calculus, right?",
    "start": "214490",
    "end": "219930"
  },
  {
    "text": "And so you learn the\nbasic definitions of a gradient or a Jacobian. And that's kind of\nthis whole 18.02 thing.",
    "start": "219930",
    "end": "227040"
  },
  {
    "text": "And like I said, this\nis in every university around the world. OK? And I bet everybody\nin this room has",
    "start": "227040",
    "end": "232620"
  },
  {
    "text": "gone through these two\nclasses one way or another, or learned it on the\nstreets, or something. So it seems to me\nthat there's actually",
    "start": "232620",
    "end": "242280"
  },
  {
    "text": "a sequence that's sort of been\ncut off completely arbitrarily, right? The sequence should go\nscalar, vector, right?",
    "start": "242280",
    "end": "250260"
  },
  {
    "text": "So that's one, two\ndimensions-- matrix. Right? Scalar is zero-dimensional;\nvector is one-dimensional;",
    "start": "250260",
    "end": "255840"
  },
  {
    "text": "and matrices are\ntwo-dimensional; and then, of course, higher\ndimensional arrays, right? It just makes sense\nthat these objects are--",
    "start": "255840",
    "end": "265290"
  },
  {
    "text": "you should be able\nto do calculus on any of these objects. When you talk about programming\nlanguages, for example, some",
    "start": "265290",
    "end": "271680"
  },
  {
    "text": "but not-- every programming language\nhas two-dimensional structures and more, right-- arrays.",
    "start": "271680",
    "end": "277780"
  },
  {
    "text": "So if you're\nfamiliar with Matlab, it does not have a\none-dimensional array. People talk about n-by-1 column\nvectors and 1-by-n row vectors.",
    "start": "277780",
    "end": "287040"
  },
  {
    "text": "But they're really\njust matrices, right? Other languages have\none-dimensional arrays.",
    "start": "287040",
    "end": "292050"
  },
  {
    "text": "Some even have\nzero-dimensional arrays, right? So these are the things that you\nfind in programming languages.",
    "start": "292050",
    "end": "298514"
  },
  {
    "text": "And this is Julia. The size of a matrix, for\nexample, has two components. The size of a vector has\none component, et cetera.",
    "start": "298514",
    "end": "306220"
  },
  {
    "text": "So it seems to me-- I've been teaching at MIT now--",
    "start": "306220",
    "end": "311868"
  },
  {
    "text": "I don't even want\nto count the years. We're pushing on 30\nyears now for me. And when I started\nhere, linear algebra",
    "start": "311868",
    "end": "319230"
  },
  {
    "text": "was not what it is today. I mean, maybe you have-- I don't know if you folks\nknow, but linear algebra",
    "start": "319230",
    "end": "324480"
  },
  {
    "text": "was this thing to avoid. It was this, like,\nrequired course maybe. And a few people\nneeded it perhaps.",
    "start": "324480",
    "end": "330540"
  },
  {
    "text": "But I think because of machine\nlearning, and statistics, and lots of other reasons,\nlinear algebra has gradually",
    "start": "330540",
    "end": "338070"
  },
  {
    "text": "taken over a much bigger\npart of today's tools",
    "start": "338070",
    "end": "345000"
  },
  {
    "text": "for lots and lots\nof areas compared to when I started 30 years ago.",
    "start": "345000",
    "end": "350139"
  },
  {
    "text": "And so machine learning,\nstatistics, engineering-- everybody needs linear algebra.",
    "start": "350140",
    "end": "355920"
  },
  {
    "text": "And so it stands to reason that\nyou'd want to be able to do calculus on matrices and\nhigher-dimensional objects,",
    "start": "355920",
    "end": "363940"
  },
  {
    "text": "especially because\neverybody these days-- they're doing machine learning. I don't have to tell you\nbecause everybody in MIT",
    "start": "363940",
    "end": "370419"
  },
  {
    "text": "is already doing it, right? Machine learning-- you\nneed to take gradients. And you need to take gradients\nof complicated objects, right?",
    "start": "370420",
    "end": "377050"
  },
  {
    "text": "So you want to do\ngradient descent? You need to be able to\ndo this sort of thing. ",
    "start": "377050",
    "end": "383650"
  },
  {
    "text": "But I've always been\ninterested in matrix calculus. I always just thought it was-- I always liked\ncalculus to begin with. And I thought the idea of\ndoing calculus with matrices",
    "start": "383650",
    "end": "390430"
  },
  {
    "text": "just seemed like a-- I always liked linear algebra. I always thought it would\nbe sort of fun to marry calculus and linear algebra.",
    "start": "390430",
    "end": "396730"
  },
  {
    "text": "And I used to go to the library. And then when Google came along,\nyou'd Google \"matrix calculus.\"",
    "start": "396730",
    "end": "401770"
  },
  {
    "text": "And up until fairly\nrecently, I was rather disappointed with\nwhat I found, right?",
    "start": "401770",
    "end": "407050"
  },
  {
    "text": "Matrix calculus was-- I found, like, three books I\nthink with the title of \"matrix calculus.\" And the moment I\nopened the book,",
    "start": "407050",
    "end": "412910"
  },
  {
    "text": "I realized, no,\nno, no, this is not what I wanted at all, right? I wanted to be able to\ndo the kind of calculus",
    "start": "412910",
    "end": "419740"
  },
  {
    "text": "we did in college-- 18.01, 18.02 calculus-- but\non higher-dimensional objects.",
    "start": "419740",
    "end": "425330"
  },
  {
    "text": "OK? So here's a little bit of a\nquick question for all of you to think about if you don't\nalready know the answer.",
    "start": "425330",
    "end": "432590"
  },
  {
    "text": "You might ask\nyourself, for example, suppose you have the function\nthat squares a matrix, right?",
    "start": "432590",
    "end": "438520"
  },
  {
    "text": "What should the derivative be? In fact, if you've\nnever done this before, you might not even know what\nit should look like, right?",
    "start": "438520",
    "end": "445870"
  },
  {
    "text": "Should it be 2x,\nlike the scalar case? Well, I'll tell you right\nnow that that's not correct",
    "start": "445870",
    "end": "451960"
  },
  {
    "text": "when x is not a scalar. And then, of course,\nyou could ask about",
    "start": "451960",
    "end": "458290"
  },
  {
    "text": "what is the derivative\nof the matrix inverse, or what is the derivative of\nthe matrix inverse squared,",
    "start": "458290",
    "end": "464110"
  },
  {
    "text": "and so forth. Or yeah, is the derivative of x\ninverse the same as negative xn",
    "start": "464110",
    "end": "469520"
  },
  {
    "text": "minus 2? And the answer is no. So the first point\nI want to make",
    "start": "469520",
    "end": "474940"
  },
  {
    "text": "is, matrix calculus is not-- it's more complicated than\nscalar and vector calculus.",
    "start": "474940",
    "end": "481310"
  },
  {
    "text": "It's not a lot more\ncomplicated, but it's unfamiliar to everybody\nwho hasn't studied it.",
    "start": "481310",
    "end": "487510"
  },
  {
    "text": "That's what I want to say--\nthat you can't just say, oh, I know vector calculus,\nI know scalar calculus,",
    "start": "487510",
    "end": "493000"
  },
  {
    "text": "it's probably just some\nsimple generalization. So my first message to\nyou is, no, it's not.",
    "start": "493000",
    "end": "498216"
  },
  {
    "text": "I mean, you can master it. You'll learn it during\nthis IEP course. But it's not just a simple,\nobvious generalization.",
    "start": "498217",
    "end": "507500"
  },
  {
    "text": "OK. And so as far as applications\ngo, this is a bit of a collage",
    "start": "507500",
    "end": "512799"
  },
  {
    "text": "that Steven put together\nfrom a year or two ago. So you could just\nlook at this yourself.",
    "start": "512799",
    "end": "518919"
  },
  {
    "text": "But I think you all\nprobably-- you're all here for some reason. So you probably already\nall know the buzzwords.",
    "start": "518919",
    "end": "524500"
  },
  {
    "text": "But \"machine learning,\"\n\"parameter optimization,\" \"stochastic gradient descent,\"\n\"automatic differentiation,\"",
    "start": "524500",
    "end": "529990"
  },
  {
    "text": "\"backpropagation,\" right--\nso these are all the reasons for doing matrix calculus. And this little\ncollage here-- you",
    "start": "529990",
    "end": "535600"
  },
  {
    "text": "could just see the places\nwhere things are happening in various parts of the web.",
    "start": "535600",
    "end": "542040"
  },
  {
    "text": "OK? So you can take a\nquick look yourselves. The part that\ninterests some of us",
    "start": "542040",
    "end": "550160"
  },
  {
    "text": "are the applications\nto physical problems as well as machine\nlearning and statistics.",
    "start": "550160",
    "end": "556670"
  },
  {
    "text": "And so for example, in\nthe upper left here, you have the so-called\ntopology-optimized aircraft",
    "start": "556670",
    "end": "562730"
  },
  {
    "text": "wing, where in\nthe old days, when people used to engineer\nan airplane wing, what",
    "start": "562730",
    "end": "569502"
  },
  {
    "text": "they would do is they\nwould just pick a design. And then they would just somehow\nfigure out the aerodynamics. And if they wanted to try again,\nthey would take another design",
    "start": "569502",
    "end": "577100"
  },
  {
    "text": "and figure out the aerodynamics. But now, with faster\ncomputers and machine learning techniques, you\ncould have the computer",
    "start": "577100",
    "end": "583490"
  },
  {
    "text": "decide what is the-- you can optimize\nfor whatever you want-- minimize fuel, or\nminimize metal that you use,",
    "start": "583490",
    "end": "591240"
  },
  {
    "text": "or whatever it is that\nyou want to optimize for. And you can let the computer\nfigure out the shape. And so that becomes a\nbig optimization problem.",
    "start": "591240",
    "end": "598070"
  },
  {
    "text": "And that gets called\ntopology optimization. It happens for an airplane wing. It happens for fluid dynamics.",
    "start": "598070",
    "end": "606450"
  },
  {
    "text": "So it doesn't really\nmatter what you do. The point is that,\nin the old days, it was hard enough\nto just simulate",
    "start": "606450",
    "end": "611670"
  },
  {
    "text": "the physics around anything,\nlike an airplane wing. But now we can actually put that\nphysics inside an inner loop.",
    "start": "611670",
    "end": "618270"
  },
  {
    "text": "And we can put an optimization\nproblem around it, right? And that's happening everywhere.",
    "start": "618270",
    "end": "624630"
  },
  {
    "text": "So data science and multivariate\nstatistics, of course, is another big area, especially\nthe computer scientists.",
    "start": "624630",
    "end": "632029"
  },
  {
    "text": "They're doing this sort of stuff\nall over the place nowadays. OK. So there is-- you can\ntake a look at this video.",
    "start": "632030",
    "end": "637490"
  },
  {
    "text": "Or here's a book. And let me say some roles--",
    "start": "637490",
    "end": "642990"
  },
  {
    "text": "let me say a few words\nabout the role of autodiff, or automatic differentiation. So automatic differentiation\nis a very exciting technology.",
    "start": "642990",
    "end": "652110"
  },
  {
    "text": "And one of the things\nthat I like to say-- I mean, you can\nlook at the slide.",
    "start": "652110",
    "end": "658110"
  },
  {
    "text": "But when I first learned about\nautomatic differentiation, I was rather surprised as to\nwhat it is and what it isn't.",
    "start": "658110",
    "end": "666389"
  },
  {
    "text": "So how many of you are familiar\nwith autodiff already a little bit? OK.",
    "start": "666390",
    "end": "671840"
  },
  {
    "text": "So about maybe\nfive or six people have raised their hands--\na minority of the course, for sure.",
    "start": "671840",
    "end": "677029"
  },
  {
    "text": "So everybody at MIT is good\nat differentiating, right?",
    "start": "677030",
    "end": "683120"
  },
  {
    "text": "You didn't get into MIT if you\ncan't differentiate functions. You're all good at it. I have no doubt, right, that--",
    "start": "683120",
    "end": "688670"
  },
  {
    "text": "certainly if you\ncame to this class. But everybody at MIT can\ndifferentiate everything. I don't care if I write\nx-squared, square root, sine,",
    "start": "688670",
    "end": "695610"
  },
  {
    "text": "cosine-- arctan maybe you have\nto remind yourself. But yeah, you can\ndifferentiate everything.",
    "start": "695610",
    "end": "700800"
  },
  {
    "text": "And nowadays, automatic\ndifferentiation has become almost more\nof a compiler technology",
    "start": "700800",
    "end": "707670"
  },
  {
    "text": "than a math course, right? It's kind of become\nthis new thing. It's not numerical\ndifferentiation,",
    "start": "707670",
    "end": "714570"
  },
  {
    "text": "like if you take a\nnumerical analysis course. Or maybe you see a\nlittle bit at the end of your calculus course,\nwhere you learn to take",
    "start": "714570",
    "end": "720510"
  },
  {
    "text": "a delta y over a delta x. You remember? You take a small difference\nover a small change.",
    "start": "720510",
    "end": "727030"
  },
  {
    "text": "So it's not that. And it's not-- like, some of\nyou may have used Wolfram Alpha or Mathematica, where you let\nthe computer differentiate",
    "start": "727030",
    "end": "734340"
  },
  {
    "text": "symbolically. And it's not that either. And I think that's what makes it\ninteresting-- that it's neither of those two things, right?",
    "start": "734340",
    "end": "740760"
  },
  {
    "text": "It's something different. And we'll show you a little\nbit about how it's done.",
    "start": "740760",
    "end": "746670"
  },
  {
    "text": "And what you learn is that\nyou should never-- well, we teach in 18.01 to do things--",
    "start": "746670",
    "end": "752940"
  },
  {
    "text": "to me, it's almost like what's\nhappened with long division. Like, maybe-- how\nmany of you actually learned to do long division?",
    "start": "752940",
    "end": "759360"
  },
  {
    "text": "How many of you think you\ncould still do it today if forced to, right?",
    "start": "759360",
    "end": "764399"
  },
  {
    "text": "So to tell Steven--\nthe whole class raised their hand for the\nfirst one and the minority for the second question.",
    "start": "764400",
    "end": "771408"
  },
  {
    "text": "But of course, one\nof the good things is you kind of\nunderstand division maybe by learning long division.",
    "start": "771408",
    "end": "776500"
  },
  {
    "text": "I'm not even sure. But calculus is, I think,\nbecoming the same way, where we teach it the\nold-fashioned way.",
    "start": "776500",
    "end": "783428"
  },
  {
    "text": "And I guess that's good\nbecause you can learn it. You understand it. But as to what should--",
    "start": "783428",
    "end": "790662"
  },
  {
    "text": "we don't really think humans\nshould do long division. And it's getting to the\npoint where probably--",
    "start": "790662",
    "end": "795870"
  },
  {
    "text": "this may be heresy\nhere in Building 2 in the math-- this is home\nof mathematics at MIT.",
    "start": "795870",
    "end": "801149"
  },
  {
    "text": "But I would say that taking\nderivatives may or may not be as important--\ncertainly complicated",
    "start": "801150",
    "end": "806910"
  },
  {
    "text": "derivatives are\nbeyond human ability anyway, no matter how\ngood you are at it. The things we want to\ndifferentiate these days",
    "start": "806910",
    "end": "813390"
  },
  {
    "text": "are just too complicated. OK. So what did I put over here? Let's see.",
    "start": "813390",
    "end": "818540"
  },
  {
    "text": "So yeah. So today's courses\nare mostly symbolic. I think that's fair to\nsay-- that 18.01 is probably",
    "start": "818540",
    "end": "826329"
  },
  {
    "text": "a 90% symbolic course. ",
    "start": "826330",
    "end": "831582"
  },
  {
    "text": "And I'm saying, yeah,\ntoday's differentiation-- neither of these two things. The math is fun. We'll learn about\nit in this class.",
    "start": "831582",
    "end": "838020"
  },
  {
    "text": "OK? So-- STEVEN JOHNSON: And\nit's not just fun. ALAN EDELMAN: Wait, wait, wait. Let me put the\nmicrophone on you. STEVEN JOHNSON: Even if\nyou're using the computer",
    "start": "838020",
    "end": "844460"
  },
  {
    "text": "to take derivatives, right--\nso you write a program. You let it take the\nderivative for you.",
    "start": "844460",
    "end": "850310"
  },
  {
    "text": "To use that\neffectively, you really have to have some idea of\nwhat's going on under the hood",
    "start": "850310",
    "end": "856298"
  },
  {
    "text": "because there are-- and especially there are cases\nwhere it doesn't work well. You need to know about those. And you need to know\nsomething about the technology",
    "start": "856298",
    "end": "863030"
  },
  {
    "text": "and about what its\ncapabilities are-- to know when to use forward\nmode or reverse mode differentiation, and what a\nvector Jacobian product is",
    "start": "863030",
    "end": "870500"
  },
  {
    "text": "and why it matters-- in order to really\nbe an effective user",
    "start": "870500",
    "end": "876223"
  },
  {
    "text": "of these kinds of things. ALAN EDELMAN: Yes, yes\nSteven's getting fancy already a little bit faster\nthan I would have. But he's quite right that--",
    "start": "876223",
    "end": "882598"
  },
  {
    "text": " I mean, I would\nactually tell people",
    "start": "882598",
    "end": "888350"
  },
  {
    "text": "that it's not such a bad idea\nto know how an engine works if you want to\ndrive a car, but I guess you don't really have to.",
    "start": "888350",
    "end": "894450"
  },
  {
    "text": "And these days, cars don't\neven have engines anymore, many of them. But so what Steven's saying is\nthat automatic differentiation",
    "start": "894450",
    "end": "903897"
  },
  {
    "text": "is probably not quite as easy\nas driving a car-- that actually understanding the\ntechnology underneath it will really help you to use it.",
    "start": "903897",
    "end": "910420"
  },
  {
    "text": "And sometimes it's necessary. OK. So let me start with\nsomething you all know just",
    "start": "910420",
    "end": "920140"
  },
  {
    "text": "to establish some notation. OK? And then I think Steven in the\nsecond part of today's lecture will kind of reiterate\nand talk a little bit more",
    "start": "920140",
    "end": "928150"
  },
  {
    "text": "about the notation\nthat we'll be using. But I want to emphasize the\nconcept of linearization.",
    "start": "928150",
    "end": "937480"
  },
  {
    "text": "I just want to\nget that word out. And you'll see more\nof that pretty soon. But instead, the idea\nthat a derivative",
    "start": "937480",
    "end": "943870"
  },
  {
    "text": "is taking a nonlinear function\nand pretending at least locally that it's\na linear function--",
    "start": "943870",
    "end": "950420"
  },
  {
    "text": "I want to emphasize\nthat point of view. And of course,\nyou all know that. So looking over here\nat this expression,",
    "start": "950420",
    "end": "957640"
  },
  {
    "text": "you all know that,\nin some sense, if you're sitting at a point--",
    "start": "957640",
    "end": "964390"
  },
  {
    "text": "x0, y0-- then the\nlinear function that's tangent to the\ncurve can be written",
    "start": "964390",
    "end": "971650"
  },
  {
    "text": "as y minus y0 is-- well,\nthe linear function is equal to f prime of\nx0 x minus x0, right?",
    "start": "971650",
    "end": "978290"
  },
  {
    "text": "So if I put this down\nin the denominator, it's just saying the change\nin y over the change of x",
    "start": "978290",
    "end": "983420"
  },
  {
    "text": "is the derivative, right? So in some sense, why\nwe take derivatives--",
    "start": "983420",
    "end": "990260"
  },
  {
    "text": "this is all very trivial\nin one-dimension. But it's good to kind of keep\ntrack of this point of view so that, when we go\ninto higher dimensions,",
    "start": "990260",
    "end": "996500"
  },
  {
    "text": "you kind of remember\nthat calculus is really about pretending some\ncomplicated curved surface is",
    "start": "996500",
    "end": "1002470"
  },
  {
    "text": "just locally linear. And for one variable,\nthat's just a tangent line.",
    "start": "1002470",
    "end": "1007840"
  },
  {
    "text": "For multivariables, you\nhave planes, hyperplanes, and so forth. But they're flat. That's what a linearization is.",
    "start": "1007840",
    "end": "1014449"
  },
  {
    "text": "So here are some notations. We'll use delta to indicate\nfinite perturbations.",
    "start": "1014450",
    "end": "1021010"
  },
  {
    "text": "And so I have this\napproximate symbol over here. OK? So delta y is approximately\nf prime of x delta x.",
    "start": "1021010",
    "end": "1028359"
  },
  {
    "text": "Or you can take the\ninfinitesimal limit. And many of you are\nfamiliar with dy over dx",
    "start": "1028359",
    "end": "1034869"
  },
  {
    "text": "is f prime of x. But how many of\nyou have actually seen it written\nthis way, where--",
    "start": "1034869",
    "end": "1040675"
  },
  {
    "text": "or we're told, you're\nnot allowed to do that, where you go-- right? So everybody knows\nthis notation.",
    "start": "1040675",
    "end": "1050430"
  },
  {
    "text": " And maybe they told you that\nthis is really not a division.",
    "start": "1050430",
    "end": "1058159"
  },
  {
    "text": "Or maybe they told you\nit is kind of a division. I don't know what your\ncalculus teacher told you. But has anyone ever-- have\nyou seen it like this?",
    "start": "1058160",
    "end": "1064332"
  },
  {
    "text": "How many of you have\nseen it like this, where you're allowed to-- OK. So almost everybody. Good.",
    "start": "1064332",
    "end": "1069679"
  },
  {
    "text": "OK. So we'll talk a little bit\nabout what that really means. Here's the linearization\nthat's-- instead of just using",
    "start": "1069680",
    "end": "1077980"
  },
  {
    "text": "y's, I'm using x over here. And this one is my\nfavorite, where you write,",
    "start": "1077980",
    "end": "1085750"
  },
  {
    "text": "if y is equal to f of\nx, right, so then-- in fact, there's\nno real y at all. If you just have\na function of x,",
    "start": "1085750",
    "end": "1092380"
  },
  {
    "text": "that df is equal\nto f prime of x dx. And so we have this--",
    "start": "1092380",
    "end": "1097840"
  },
  {
    "text": " f prime is a linear\nfunction in the end.",
    "start": "1097840",
    "end": "1102850"
  },
  {
    "text": "I mean, in one variable,\nit's just a constant, right? But a constant times the\nchange of x is the change of f.",
    "start": "1102850",
    "end": "1111200"
  },
  {
    "text": "OK? So you all know this. It's one-dimensional calculus--\nbut just to set the stage.",
    "start": "1111200",
    "end": "1116520"
  },
  {
    "text": "OK. And-- STEVEN JOHNSON: Hey,\ncan I say something? So the reason we don't want\nto put dx on the other side",
    "start": "1116520",
    "end": "1124250"
  },
  {
    "text": "is we don't want\nto divide by dx. One way of thinking about it is\nthat pretty soon this-- right",
    "start": "1124250",
    "end": "1130370"
  },
  {
    "text": "now, it's a scalar. We can divide by numbers. But pretty soon, x\ncould be a vector. Or it could be some other thing.",
    "start": "1130370",
    "end": "1136009"
  },
  {
    "text": "You can't divide by a vector. So it's harder to-- it's\neasier to generalize",
    "start": "1136010",
    "end": "1141320"
  },
  {
    "text": "this kind of notation to\nother kinds of objects to where you can\nmultiply, you can operate,",
    "start": "1141320",
    "end": "1146600"
  },
  {
    "text": "but you can't divide. ALAN EDELMAN: Yep. Good.",
    "start": "1146600",
    "end": "1151710"
  },
  {
    "text": "Good. Yeah. Try dividing a\nvector by a vector.",
    "start": "1151710",
    "end": "1156790"
  },
  {
    "text": "It doesn't really mean much. ",
    "start": "1156790",
    "end": "1161980"
  },
  {
    "text": "All right. So again, more trivialities\njust to set the stage, just to go baby steps.",
    "start": "1161980",
    "end": "1168190"
  },
  {
    "text": "So here, what I'm doing is I'm\nlooking at the square function. OK?",
    "start": "1168190",
    "end": "1173310"
  },
  {
    "text": "And I'm looking at it at a\nparticular point-- the point 3, 9. And you all know that\nthe derivative is 2x.",
    "start": "1173310",
    "end": "1179490"
  },
  {
    "text": "And so if x equals 3,\n2x is equal to 6, right?",
    "start": "1179490",
    "end": "1184920"
  },
  {
    "text": "So that's the derivative. The square of 3,\nof course, is 9. And even if you do this on a\ncomputer, this is easy to do.",
    "start": "1184920",
    "end": "1192760"
  },
  {
    "text": "You don't even need\na computer, really. It's actually easy to check\nthis even on paper and pencil.",
    "start": "1192760",
    "end": "1197919"
  },
  {
    "text": "But if you get closer\nand closer to 3, as I'm doing in these four\nlines over here, of course",
    "start": "1197920",
    "end": "1203880"
  },
  {
    "text": "you get closer and closer to 9. And what we're interested\nin is the difference.",
    "start": "1203880",
    "end": "1209140"
  },
  {
    "text": "So you see that if I add\na little delta x to 3,",
    "start": "1209140",
    "end": "1215160"
  },
  {
    "text": "I get a 9 plus delta y,\nwhich is 9 plus 6 delta x plus some higher-order\nterm, which",
    "start": "1215160",
    "end": "1221980"
  },
  {
    "text": "we don't care about, right? And so we see this as delta y\nis f prime of x0 delta x, right?",
    "start": "1221980",
    "end": "1229809"
  },
  {
    "text": "delta y is 6 delta x. So the thing I want\nyou to walk away with is really that I want you\nto look at these numbers,",
    "start": "1229810",
    "end": "1238697"
  },
  {
    "text": "like this one here-- I think of this as the delta\nx and think of this red part here as the sixth\ndelta x, right?",
    "start": "1238697",
    "end": "1245360"
  },
  {
    "text": "So again, this idea that\nSteven also just said--",
    "start": "1245360",
    "end": "1250780"
  },
  {
    "text": "that this is a\nlinear functional. This is the function\nthat multiplies by 6. If I want to know the change\nin y, I have that change in x.",
    "start": "1250780",
    "end": "1259240"
  },
  {
    "text": "And I multiply by 6. That's my-- you see,\neventually this is going to be not just multiplied by 6. But it's going to be,\nyou have a vector change,",
    "start": "1259240",
    "end": "1266000"
  },
  {
    "text": "and you're going to\nmultiply by a matrix. OK? But just going slowly,\nwe make a little change. And you multiply by 6.",
    "start": "1266000",
    "end": "1271600"
  },
  {
    "text": "And that's what's going on here. So I like to think of dx and\ndy as really small numbers",
    "start": "1271600",
    "end": "1279880"
  },
  {
    "text": "on a computer. The mathematicians call\nthem infinitesimals. From-- there's a lot of rigorous\nmath that makes infinitesimals",
    "start": "1279880",
    "end": "1289690"
  },
  {
    "text": "work, which I think is sort\nof a great human achievement but of little value to\npractical computations.",
    "start": "1289690",
    "end": "1296320"
  },
  {
    "text": "I think for practical\ncomputations, to think of-- you could think of it\nany way you like, but I",
    "start": "1296320",
    "end": "1301720"
  },
  {
    "text": "like to think of dx and dy-- when I'm actually working and\nI'm not thinking theoretically,",
    "start": "1301720",
    "end": "1307570"
  },
  {
    "text": "I like to think of dx and\ndy as, like, the limit of very small numbers. And when I play on the computer,\nI type 0.0001 or 0.00001.",
    "start": "1307570",
    "end": "1318130"
  },
  {
    "text": "And I say, that's small enough. It depends on\ncontext, of course. But that's usually what I do.",
    "start": "1318130",
    "end": "1325950"
  },
  {
    "text": "OK? So all right. Now we get to go to where the\nbig boys and big girls go,",
    "start": "1325950",
    "end": "1331680"
  },
  {
    "text": "right? So we're leaving the\nworld of scalar calculus and entering this new\nworld of matrix calculus.",
    "start": "1331680",
    "end": "1340380"
  },
  {
    "text": "So to get started,\nlet me just mention a little bit of notation. So it's handy to have\nthe element-wise--",
    "start": "1340380",
    "end": "1348990"
  },
  {
    "text": "so yeah, just a little bit of\nvector and matrix notation. So I like to use\nthe Julia notation",
    "start": "1348990",
    "end": "1356000"
  },
  {
    "text": "for element-wise\nproduct of vectors. So this dot times this point--",
    "start": "1356000",
    "end": "1363860"
  },
  {
    "text": "I like to call it\npoint-wise times, right? So 2, 3 point-wise\ntimes 10, 11 is 20, 33.",
    "start": "1363860",
    "end": "1370879"
  },
  {
    "text": "If you happen to use languages-- I think Python does it,\ntoo-- and certainly Julia. They pronounce it\nas \"broadcasting.\"",
    "start": "1370880",
    "end": "1377520"
  },
  {
    "text": "I always hated that word\nbecause I don't feel it really indicates what's going on. I think point-wise multiply--",
    "start": "1377520",
    "end": "1384559"
  },
  {
    "text": "to me, a dot is like\na decimal point. It says it better. But if you're used to\nthe term \"broadcasting,\" that's fine as well.",
    "start": "1384560",
    "end": "1391210"
  },
  {
    "text": "So you see we're doing\nelement-wise multiplication. And in this little\ndemo here, you'll",
    "start": "1391210",
    "end": "1398519"
  },
  {
    "text": "also see some people use this\ndot with a circle around it to indicate point-wise multiply.",
    "start": "1398520",
    "end": "1404429"
  },
  {
    "text": "I remind you, in case linear\nalgebra was too long ago, the notion of a\ntrace of a matrix is, if you have a\nbig square matrix,",
    "start": "1404430",
    "end": "1413250"
  },
  {
    "text": "then you look at the diagonal,\nand you add up all the numbers. And that's the trace.",
    "start": "1413250",
    "end": "1419080"
  },
  {
    "text": "And if you go to this\nmatrix.calculus.org,",
    "start": "1419080",
    "end": "1424710"
  },
  {
    "text": "there was-- oh, yes. There's a story here.",
    "start": "1424710",
    "end": "1430260"
  },
  {
    "text": "Let me step back and\ntell you the story. So the reason why we started--",
    "start": "1430260",
    "end": "1435780"
  },
  {
    "text": "one of the reasons we started\nteaching this class during IEP was because there was a\nquestion on the math Piazza--",
    "start": "1435780",
    "end": "1443700"
  },
  {
    "text": "all the undergraduates have\naccess to a Piazza page,",
    "start": "1443700",
    "end": "1448980"
  },
  {
    "text": "not for an individual\nclass, but for the majors. And there was this\nquestion that arose,",
    "start": "1448980",
    "end": "1454350"
  },
  {
    "text": "which is basically\nthis question. If y is an n-by-m\nmatrix-- here, I'll",
    "start": "1454350",
    "end": "1461640"
  },
  {
    "text": "use the mouse so it'll\nget recorded properly. So if y is an n-by-m matrix,\nx is an n-by-k matrix,",
    "start": "1461640",
    "end": "1471000"
  },
  {
    "text": "and theta is also a\nmatrix-- a k-by-m matrix-- then this scalar, the\ntrace, makes sense, right?",
    "start": "1471000",
    "end": "1478919"
  },
  {
    "text": "The trace of this\nmultiply makes sense. And one could try to\nunderstand what does",
    "start": "1478920",
    "end": "1484300"
  },
  {
    "text": "it mean to take the derivative. OK? And this student on Piazza\nasked, how do you do it?",
    "start": "1484300",
    "end": "1491830"
  },
  {
    "text": "And how do you learn how\nto do things like this? And that really is the\norigin of this IEP class-- that, here's the answer.",
    "start": "1491830",
    "end": "1498400"
  },
  {
    "text": "The answer is itself a matrix. It's minus 2x transpose\ntimes y minus x theta.",
    "start": "1498400",
    "end": "1504100"
  },
  {
    "text": "And you can actually\nget the answer through this\nmatrixcalculus.org, right?",
    "start": "1504100",
    "end": "1509216"
  },
  {
    "text": "So those of you who want\nto, you could look at it. It's kind of a nice web page. You can type in some matrices.",
    "start": "1509217",
    "end": "1514930"
  },
  {
    "text": "It's got its limitations, but\nyou can look at some of these and see what's going on. OK.",
    "start": "1514930",
    "end": "1520000"
  },
  {
    "text": "But let me go back. How do I get back to my slides? I'll never find them again. Where are my slides? They're underneath\nhere, aren't they?",
    "start": "1520000",
    "end": "1525789"
  },
  {
    "text": "STEVEN JOHNSON: Ctrl-left\narrow or right arrow? ALAN EDELMAN: No, I\nthink they're underneath. Yeah, they're underneath.",
    "start": "1525790",
    "end": "1531940"
  },
  {
    "text": "Oh, you think I could\nhave gotten it that way? STEVEN JOHNSON: Full\nscreen mode, I think, yeah. ALAN EDELMAN: Oh, OK.",
    "start": "1531940",
    "end": "1537429"
  },
  {
    "text": "Let's see. Can I really get to that? Ctrl-- never mind. I don't know how I can get back.",
    "start": "1537430",
    "end": "1543400"
  },
  {
    "text": "Anyway, yeah. So you might play\nwith this a little bit and see some of the\nthings you can do. It has limitations.",
    "start": "1543400",
    "end": "1549260"
  },
  {
    "text": "For example, it\ndoesn't do a good job when the answer is higher\nthan two-dimensional, right?",
    "start": "1549260",
    "end": "1555070"
  },
  {
    "text": "But you can take a look\nand see what it will do. One of the things-- you\nmight see some matrix",
    "start": "1555070",
    "end": "1561760"
  },
  {
    "text": "calculus in some classes. For example-- I think\nthis is on the next slide, but I'll just put it\non the blackboard.",
    "start": "1561760",
    "end": "1567490"
  },
  {
    "text": "For example, you\nmight ask for, how do you take the gradient\nof x transpose x,",
    "start": "1567490",
    "end": "1574059"
  },
  {
    "text": "and things like that--\nor x transpose ax. I think it's coming up on\nthe next slide, anyway. So how do you take the\ngradient with respect",
    "start": "1574060",
    "end": "1580393"
  },
  {
    "text": "to x of objects like these? And I've seen many\nclasses at MIT do that. And what I would say\nis they would do it",
    "start": "1580393",
    "end": "1586990"
  },
  {
    "text": "the old-fashioned way. They do it variable by\nvariable by variable as opposed to the holistic way.",
    "start": "1586990",
    "end": "1593059"
  },
  {
    "text": "So one of the things that\nI'd like to kind of invite all of you to think about\nas we go through this course",
    "start": "1593060",
    "end": "1598430"
  },
  {
    "text": "is to think of a\nmatrix holistically or think of a\nvector holistically. Stop thinking of a vector as a\nbunch of elements or a matrix",
    "start": "1598430",
    "end": "1605870"
  },
  {
    "text": "as an m-by-n table, as you would\nsee in an early linear algebra class. Think of a matrix\nas sort of having--",
    "start": "1605870",
    "end": "1613505"
  },
  {
    "text": "like, we're more than our hands,\nand our feet, and our noses, and our mouths, right? We're people, right?",
    "start": "1613505",
    "end": "1618940"
  },
  {
    "text": "I want you to think\nof matrices that way-- as a holistic object. And there are ways of doing\nmatrix calculus without going",
    "start": "1618940",
    "end": "1626210"
  },
  {
    "text": "down to the element level. To kind of get that point\nacross a little bit more,",
    "start": "1626210",
    "end": "1632390"
  },
  {
    "text": "I still see this today. Though, it happened\nmore and more. I, as a professor, would\nwalk into a classroom",
    "start": "1632390",
    "end": "1639020"
  },
  {
    "text": "to start to give a lecture. And in the old days, professors\nused chalk and blackboards.",
    "start": "1639020",
    "end": "1644270"
  },
  {
    "text": "I guess they still\ndo that sometimes. And I would walk in. And I would see what's\non the blackboard",
    "start": "1644270",
    "end": "1649940"
  },
  {
    "text": "from the previous class. And as I'm erasing it, I\nwould actually look and see what would be written.",
    "start": "1649940",
    "end": "1655610"
  },
  {
    "text": "And I would see people taking-- essentially, I would stand back.",
    "start": "1655610",
    "end": "1660919"
  },
  {
    "text": "And I would look at\nthe entire blackboard. And I realized\nthat, this is just a couple of matrix multiplies\nwritten out element-wise.",
    "start": "1660920",
    "end": "1668420"
  },
  {
    "text": "But it fills up the\nwhole blackboard because, for whatever\nreason, the professor didn't use matrix notation.",
    "start": "1668420",
    "end": "1673877"
  },
  {
    "text": "They just used scalar\nnotation, with indices all over the place. And one of the\nthings you'll see is",
    "start": "1673877",
    "end": "1679130"
  },
  {
    "text": "how to kind of grow up\nfrom that point of view to actually-- sometimes it's\nuseful to work with indices.",
    "start": "1679130",
    "end": "1684920"
  },
  {
    "text": "And sometimes it's comforting. You feel like you're\ngetting the right answer. But in some sense,\nit's more elegant when you don't have to do that.",
    "start": "1684920",
    "end": "1690710"
  },
  {
    "text": "And we'll show you how\nto do that as well. OK. So let's take a look at sort\nof the various types of cases",
    "start": "1690710",
    "end": "1703730"
  },
  {
    "text": "of what it even means to\ntake a derivative, right? And so 18.01 is very much in\nthe upper-left corner, right?",
    "start": "1703730",
    "end": "1714500"
  },
  {
    "text": "So you have a scalar in, and\na scalar comes out, right? That's everybody's first\nsemester of calculus.",
    "start": "1714500",
    "end": "1721460"
  },
  {
    "text": "OK? If you go along the first\nrow over here, for example,",
    "start": "1721460",
    "end": "1727340"
  },
  {
    "text": "probably at least in physics-- but very simple physics-- you learn the idea\nthat you could",
    "start": "1727340",
    "end": "1734150"
  },
  {
    "text": "have, say, your position\nin three dimensions as a function of time, right?",
    "start": "1734150",
    "end": "1739220"
  },
  {
    "text": "So your input is time. It's a scalar. And your output is a\nposition in space, like a--",
    "start": "1739220",
    "end": "1745590"
  },
  {
    "text": "for those of you who can't\nsee what I'm doing online, I'm moving my hand to indicate\na trajectory in space.",
    "start": "1745590",
    "end": "1752309"
  },
  {
    "text": "OK? And of course, the derivative is\nnow the velocity vector, right?",
    "start": "1752310",
    "end": "1759230"
  },
  {
    "text": "That's what the derivative\nis of the function of time. It's tangent to the curve. And its magnitude tells you\nhow fast you're going, right?",
    "start": "1759230",
    "end": "1766580"
  },
  {
    "text": "And that's the derivative\nof a vector with a scalar. Now, this is not\nthe sort of thing",
    "start": "1766580",
    "end": "1772220"
  },
  {
    "text": "you would see much in previous\nclasses, I wouldn't think. But it's perfectly\nreasonable to also talk about a trajectory in\nmatrix space, right?",
    "start": "1772220",
    "end": "1780049"
  },
  {
    "text": "And so you could\nhave an m-by-n matrix that's a function of time. I could say it as every\nelement as a function of time.",
    "start": "1780050",
    "end": "1786450"
  },
  {
    "text": "And of course, you can take\nthe derivative of that thing, right, which will be\na fixed matrix, which,",
    "start": "1786450",
    "end": "1791810"
  },
  {
    "text": "if you could imagine an\nm-by-n matrix space-- mathematicians love to imagine\nhigh-dimensional matrix spaces.",
    "start": "1791810",
    "end": "1797909"
  },
  {
    "text": "high-dimensional\nspaces of all kinds. So if you can imagine\nan m-by-n matrix space, then the derivative\nwould be a tangent",
    "start": "1797910",
    "end": "1803750"
  },
  {
    "text": "in that big\nhigh-dimensional space. OK. So that's the first row. Maybe it's a good time to\nnow go down the first column.",
    "start": "1803750",
    "end": "1812994"
  },
  {
    "text": "So this blue-- it looks-- yeah,\nI guess it looks more purple",
    "start": "1812994",
    "end": "1821350"
  },
  {
    "text": "on the screen, but it's blue-- is sort of the-- this is what you do in\nmultivariate calculus--",
    "start": "1821350",
    "end": "1828070"
  },
  {
    "text": "18.02-- and machine learning\neverywhere, which is, we take gradients. So it's very typical\nwhere you have a function",
    "start": "1828070",
    "end": "1836290"
  },
  {
    "text": "with many variables going in. I'll just talk about vectors. But in fact, you could\nhave lots more variables",
    "start": "1836290",
    "end": "1841720"
  },
  {
    "text": "than just a vector. But let's just say,\nin machine learning, you have a vector going\nin and a scalar going out.",
    "start": "1841720",
    "end": "1847510"
  },
  {
    "text": "The scalar is often\ncalled the loss function in machine learning. And how many of you have heard\nthe word \"loss function\"?",
    "start": "1847510",
    "end": "1855910"
  },
  {
    "text": "Everybody. OK. I think students are learning\nit in kindergarten these days. So yeah, everybody-- right.",
    "start": "1855910",
    "end": "1862539"
  },
  {
    "text": "So you're all familiar with\nmany, many variables coming in. That's the vector, the\ninput, and the loss function,",
    "start": "1862540",
    "end": "1868703"
  },
  {
    "text": "the scalar coming out. And you want to take a\ngradient of that thing, right? And so if you actually literally\nhave a vector going in,",
    "start": "1868703",
    "end": "1876460"
  },
  {
    "text": "then the gradient is-- in fact, for every\nscalar function,",
    "start": "1876460",
    "end": "1882800"
  },
  {
    "text": "the gradient always has the\nsame shape as the inputs. So if your input is a vector,\nthen the gradient is a vector,",
    "start": "1882800",
    "end": "1889380"
  },
  {
    "text": "right? So that's often denoted\nwith this nabla notation.",
    "start": "1889380",
    "end": "1894950"
  },
  {
    "text": "In LaTeX, it's backslash nabla-- so usually pronounced\ngrad f, or gradient of f.",
    "start": "1894950",
    "end": "1901940"
  },
  {
    "text": "And in Julia, it\nwould be a vector. We don't have to talk about\ncolumn vectors and row vectors.",
    "start": "1901940",
    "end": "1908210"
  },
  {
    "text": "It's simple to\njust say \"vectors.\" It gets too confused when\nyou have column vectors,",
    "start": "1908210",
    "end": "1913620"
  },
  {
    "text": "but people are used\nto that notation. It's just a vector. But you could call\nit a column vector. But the derivative\nis the transpose.",
    "start": "1913620",
    "end": "1922860"
  },
  {
    "text": "It's a row vector, right? And Steven and I-- we\nactually went back and forth",
    "start": "1922860",
    "end": "1928649"
  },
  {
    "text": "on this a couple of years ago. And we were absolutely\nconvinced that this is the best way to do\nit-- that the gradient is",
    "start": "1928650",
    "end": "1935970"
  },
  {
    "text": "a vector, or a column\nvector, and the f prime is a row vector.",
    "start": "1935970",
    "end": "1942580"
  },
  {
    "text": "So in particular,\nI'll just give you the answer for x transpose x. So here's f of x, right?",
    "start": "1942580",
    "end": "1948450"
  },
  {
    "text": "And we're talking about\nx being a vector, right? And so the gradient\nof f is equal to 2x.",
    "start": "1948450",
    "end": "1958320"
  },
  {
    "text": "OK? But df will be 2x transpose dx.",
    "start": "1958320",
    "end": "1964350"
  },
  {
    "text": "Or f prime of x will\nbe 2x transpose. And I hope you can\nstart to appreciate",
    "start": "1964350",
    "end": "1971139"
  },
  {
    "text": "why this is a good idea. Because we want this to be\na linear operator-- that we put in a little vector change.",
    "start": "1971140",
    "end": "1979870"
  },
  {
    "text": "And we want the output\nto be a scalar, right? So it makes no sense to go--",
    "start": "1979870",
    "end": "1985659"
  },
  {
    "text": "you see, it makes no\nsense to go to 2x dx, where everything-- this\nis a little vector change.",
    "start": "1985660",
    "end": "1991453"
  },
  {
    "text": "And this is a little vector. There's no such thing as\nmultiplying vector by vector. But if we write this--",
    "start": "1991453",
    "end": "1997600"
  },
  {
    "text": "2x transpose-- then\nwhen we multiply by dx,",
    "start": "1997600",
    "end": "2002669"
  },
  {
    "text": "as we're doing here, out\ncomes a scalar, right? So I make a little\nsmall change to x--",
    "start": "2002670",
    "end": "2008070"
  },
  {
    "text": "infinitesimal or a\ntiny little change, however you want to think of it. And out comes a\nscalar change to f.",
    "start": "2008070",
    "end": "2013710"
  },
  {
    "text": "And so this actually\nmakes perfect sense. And I don't think you would\nlearn it that way in 18.02.",
    "start": "2013710",
    "end": "2022020"
  },
  {
    "text": "And so this is-- but this seems\nto lead to consistent answers. And so we're going to encourage\nyou to take that point of view.",
    "start": "2022020",
    "end": "2030030"
  },
  {
    "text": "All right. So I've covered\nfour of these boxes. Let me point out that there's\na color-coding that you might",
    "start": "2030030",
    "end": "2037340"
  },
  {
    "text": "have noticed to these boxes, if\nyou haven't noticed it already. The green is when the\nderivative is zero-dimensional.",
    "start": "2037340",
    "end": "2044120"
  },
  {
    "text": "It goes along the\ndiagonals, right? The blue is one-dimensional\nanswers, right-- the vector and the\ngradient, those two boxes.",
    "start": "2044120",
    "end": "2052369"
  },
  {
    "text": "And then the next diagonal-- it's a funny diagonal, right? It's going from\nnortheast to southwest.",
    "start": "2052370",
    "end": "2058908"
  },
  {
    "text": "But this funny\ndiagonal-- the next step is when the derivative\nitself is a matrix, right?",
    "start": "2058909",
    "end": "2065239"
  },
  {
    "text": "And so one example was in the\ntop right, when you just have a trajectory in matrix space.",
    "start": "2065239",
    "end": "2072919"
  },
  {
    "text": "Another one is in\nthe bottom left-- is, if your parameters are\ncoming into a machine learning",
    "start": "2072920",
    "end": "2080449"
  },
  {
    "text": "algorithm as an array and\nyou have a loss function, then the gradient-- as I\nsaid, the gradient of a scalar",
    "start": "2080449",
    "end": "2086270"
  },
  {
    "text": "function always matches the\nshape of the input, right? And so the gradient of\na matrix is a matrix.",
    "start": "2086270",
    "end": "2091710"
  },
  {
    "text": "OK? And then in the middle\nof these 3-by-3 arrays, I think you would have\nall seen in 18.02--",
    "start": "2091710",
    "end": "2098410"
  },
  {
    "text": "I don't know if you\nremember it anymore-- most of you probably do. Some of you may have forgotten. STEVEN JOHNSON: 18.02 doesn't\nalways cover it, unfortunately.",
    "start": "2098410",
    "end": "2105172"
  },
  {
    "text": "ALAN EDELMAN: Doesn't\ncover Jacobians? STEVEN JOHNSON: And certainly\nnot as linearization. They sometimes do\nJacobian matrices",
    "start": "2105172",
    "end": "2111180"
  },
  {
    "text": "determinants for integrals. But they don't even always\ncover that, apparently. ALAN EDELMAN: But you\nmean students in 18.02",
    "start": "2111180",
    "end": "2119610"
  },
  {
    "text": "will not see a Jacobian matrix\nin some form or another? STEVEN JOHNSON: Apparently. ALAN EDELMAN: What\nhas MIT come to?",
    "start": "2119610",
    "end": "2125508"
  },
  {
    "text": "How many of you have\nactually taken 18.02 at MIT? OK. And among those, how many of\nyou think you once learned",
    "start": "2125508",
    "end": "2133210"
  },
  {
    "text": "about a Jacobian matrix? You don't have to\nhave remembered it. OK. Well, we've got a\ngood set of students",
    "start": "2133210",
    "end": "2138369"
  },
  {
    "text": "because apparently they\nlearned Jacobian matrices. I would have thought that that\nwould just go without saying,",
    "start": "2138370",
    "end": "2143530"
  },
  {
    "text": "but-- STEVEN JOHNSON: But it's very-- often, it's only for\nmulti-dimensional integration.",
    "start": "2143530",
    "end": "2149523"
  },
  {
    "text": "So it's only determinants of\nJacobians that they ever see. So the Jacobian and\nits determinant-- they kind of get mixed together.",
    "start": "2149523",
    "end": "2157010"
  },
  {
    "text": "ALAN EDELMAN: All right. That sounds unfortunate. [CHUCKLES] But OK.",
    "start": "2157010",
    "end": "2162360"
  },
  {
    "text": "Thanks for the update on 18.02. A few years back, I actually\nlooked at the 18.02 ASE notes.",
    "start": "2162360",
    "end": "2169510"
  },
  {
    "text": "And I know that it was there. But I don't know what\nchanges have been made. OK.",
    "start": "2169510",
    "end": "2176340"
  },
  {
    "text": "But in any event, just\nto kind of set the stage, this is where you have a vector\ninput and a vector output,",
    "start": "2176340",
    "end": "2182500"
  },
  {
    "text": "right? So you have a function\nin, say, three dimensions. And the output is also a\nfunction, say, in three--",
    "start": "2182500",
    "end": "2190589"
  },
  {
    "text": "or it could be lower or\nhigher dimensions, right? And if you have n dimensions\ngoing in and m-- as in \"Mary\"--",
    "start": "2190590",
    "end": "2199860"
  },
  {
    "text": "dimensions going out, then the\nderivative is most naturally done if you do it as a matrix,\nnot as a linear operator.",
    "start": "2199860",
    "end": "2207099"
  },
  {
    "text": "But if you do it as a matrix,\nit will be an m-by-n matrix. OK? And then, of course, we get\ninto the interesting situations",
    "start": "2207100",
    "end": "2213240"
  },
  {
    "text": "as you move down this table-- the situations where\nthat web page doesn't do a very good job at it. But we're going to show\nyou how to do this--",
    "start": "2213240",
    "end": "2219673"
  },
  {
    "text": "how to think about just even\nwhat's a good notation when you start moving into\nhigher-order arrays,",
    "start": "2219673",
    "end": "2226930"
  },
  {
    "text": "right, where the derivative\nis no longer best expressed",
    "start": "2226930",
    "end": "2232960"
  },
  {
    "text": "with two-dimensional\nmatrices anymore, right? And so that's down\non the bottom right.",
    "start": "2232960",
    "end": "2238550"
  },
  {
    "text": "OK? So here are some answers\nto set the stage.",
    "start": "2238550",
    "end": "2245727"
  },
  {
    "text": "And again, we're going to\nshow you how to do this. But I think it's kind of nice to\nforeshadow a little bit of what",
    "start": "2245727",
    "end": "2250840"
  },
  {
    "text": "you're going to see. So here, I'm going to use this\nsort of operator notation,",
    "start": "2250840",
    "end": "2256565"
  },
  {
    "text": "starting with one\nyou're familiar with. The derivative of x-cubed\nis 3x-squared dx, right?",
    "start": "2256565",
    "end": "2261760"
  },
  {
    "text": "So everybody kind of knows that. And just to encourage\nyou, just to remember what I've said before--",
    "start": "2261760",
    "end": "2267369"
  },
  {
    "text": "that I want you to\nthink of this as, if I make a little small change\nto x, my change to my function",
    "start": "2267370",
    "end": "2274660"
  },
  {
    "text": "will be 3x squared times\nthat small change, right? If I'm at x equals 7, right,\nand I make a 0.01 change,",
    "start": "2274660",
    "end": "2284470"
  },
  {
    "text": "then the result\nis going to be 3 7 squared times 0.01\napproximately, right? Right?",
    "start": "2284470",
    "end": "2289870"
  },
  {
    "text": "And of course, that gets better\nas 0.01 gets closer and closer to 0. OK? So here's one that--",
    "start": "2289870",
    "end": "2299720"
  },
  {
    "text": "in fact, let's use the mouse\nagain just to emphasize. But this one here-- whoops. I didn't want to do that.",
    "start": "2299720",
    "end": "2305410"
  },
  {
    "text": "This one here is-- let's see. We have a-- I want this to be a vector\ninput and a scalar output.",
    "start": "2305410",
    "end": "2312530"
  },
  {
    "text": "So it's this box again. And it's the same thing\nI wrote over here. I'm just using prime\ninstead of transpose.",
    "start": "2312530",
    "end": "2318160"
  },
  {
    "text": "But again, I want\nto encourage you-- that if x is a vector and dx is\na small change to that vector,",
    "start": "2318160",
    "end": "2326620"
  },
  {
    "text": "then you take the\ndot product, right? So I hope everybody\nrealizes that, if you ever",
    "start": "2326620",
    "end": "2332650"
  },
  {
    "text": "go x transpose y, that's the\nsame thing as a dot product, right? That means multiply\nall the elements",
    "start": "2332650",
    "end": "2337720"
  },
  {
    "text": "and then add\neverything up, right? So this is a dot\nproduct between 2x and--",
    "start": "2337720",
    "end": "2343690"
  },
  {
    "text": "this is, I'm at the point x. And my change is a little dx. This dot product will be\nthe change to x transpose x.",
    "start": "2343690",
    "end": "2351715"
  },
  {
    "text": "OK? And here's your\nfirst matrix answer. I told you that, if\nyou square a matrix,",
    "start": "2351715",
    "end": "2360010"
  },
  {
    "text": "the answer is not,\nlike, 2x, right? What is it? Well, in fact, because\nmatrices don't commute--",
    "start": "2360010",
    "end": "2367390"
  },
  {
    "text": "the x and the dx don't commute-- in a way, you could\nthink about it this way.",
    "start": "2367390",
    "end": "2373720"
  },
  {
    "text": "Should we go x dx times\n2 or dx times x times 2? Well, in fact, there's no reason\nto prefer 1 over the other.",
    "start": "2373720",
    "end": "2381470"
  },
  {
    "text": "And in fact, the\ncorrect answer is to add the\nmultiplications both ways.",
    "start": "2381470",
    "end": "2386810"
  },
  {
    "text": "So if you make a small change\nto a matrix and you ask,",
    "start": "2386810",
    "end": "2392960"
  },
  {
    "text": "what is the change\nto the square, then this is the correct answer. This is-- and you see now\nmaybe for the first time",
    "start": "2392960",
    "end": "2401170"
  },
  {
    "text": "why we need to think of\nit as a linear operator. We can write this\nout as a big matrix.",
    "start": "2401170",
    "end": "2410140"
  },
  {
    "text": "But it's not a good idea, right? It might be comfortable,\nbut we don't recommend it. So you see, you could think\nof x as n-squared variables.",
    "start": "2410140",
    "end": "2419260"
  },
  {
    "text": "If x is an n-by-n\nmatrix, right-- ",
    "start": "2419260",
    "end": "2424900"
  },
  {
    "text": "here's x. That's n-by-n. We have n-squared variables.",
    "start": "2424900",
    "end": "2430168"
  },
  {
    "text": "And so we could\nthink of the function that squares our matrix. If you want, you could,\nlike, flatten everything",
    "start": "2430168",
    "end": "2436480"
  },
  {
    "text": "and think of it as going\nfrom n-squared variables to n-squared variables. And then the\nderivative would have",
    "start": "2436480",
    "end": "2442622"
  },
  {
    "text": "n to the fourth things in it. You could write it as an\nn-squared by n-squared matrix, but we're not recommending that.",
    "start": "2442622",
    "end": "2448400"
  },
  {
    "text": "We're saying, just think of\nit as this linear operator. OK? And that's going\nto be the answer",
    "start": "2448400",
    "end": "2454670"
  },
  {
    "text": "for the derivative of x-squared,\nOK, for a matrix square. Obviously, it reduces to\n2x dx when it scalars,",
    "start": "2454670",
    "end": "2461360"
  },
  {
    "text": "but it's much better than that. OK? So sometimes I open\nup a Julia to do this,",
    "start": "2461360",
    "end": "2469090"
  },
  {
    "text": "but I think these slides\nkind of say it all. So just to kind of hit home\non both the matrix square",
    "start": "2469090",
    "end": "2477040"
  },
  {
    "text": "and the x transpose x, I'll just\ngive you a numerical example just to make sure this\nis completely clear.",
    "start": "2477040",
    "end": "2483829"
  },
  {
    "text": "So let's consider the\nfunction x transpose x, right? So I hope you all\nrealize that's just the sum of the squares of\nthe entries of x, right?",
    "start": "2483830",
    "end": "2492080"
  },
  {
    "text": "And so if I'm at the\npoint 3, 4, right--",
    "start": "2492080",
    "end": "2497440"
  },
  {
    "text": "so I'm at a point in space,\na two-dimensional space-- 3, 4, right? So my function value is\n9 plus 16 is 25, right?",
    "start": "2497440",
    "end": "2507640"
  },
  {
    "text": "And let's make a little change. Let's say my dx is-- I'm going to go 0.001\nin my first direction",
    "start": "2507640",
    "end": "2513609"
  },
  {
    "text": "and 0.002 in the next direction. Well, you could do the math. And basically, the\nanswer is about 25.022.",
    "start": "2513610",
    "end": "2521960"
  },
  {
    "text": "OK? And so here, we're explicitly\nmaking those changes, right?",
    "start": "2521960",
    "end": "2527589"
  },
  {
    "text": "But the whole point\nof calculus is to have, like, a formula\nfor doing that, right?",
    "start": "2527590",
    "end": "2535111"
  },
  {
    "text": "The reason why we don't just\ndo the calculation that's on this line is because\nthe magic of calculus",
    "start": "2535111",
    "end": "2543549"
  },
  {
    "text": "is there's an\nexact way to do it. And this is that exact way.",
    "start": "2543550",
    "end": "2548920"
  },
  {
    "text": "If you go 2x0\ntranspose dx, you see we're going to get\nthat right answer. OK?",
    "start": "2548920",
    "end": "2554240"
  },
  {
    "text": "And that's what we\nlove about calculus. OK? And so the delta f\nis going to be that.",
    "start": "2554240",
    "end": "2561780"
  },
  {
    "text": "Any questions about that so far? OK. And I could--",
    "start": "2561780",
    "end": "2570878"
  },
  {
    "text": "I don't know. Well, let me just see. Do you want to see me\ndo this with a matrix? I don't think I put\nit in the slides.",
    "start": "2570878",
    "end": "2577270"
  },
  {
    "text": "Should I do a matrix\nquickly in Julia? We could do it. Do you want-- oh, people are\nsaying, yes, I should do it. All right.",
    "start": "2577270",
    "end": "2582660"
  },
  {
    "text": "So let's open up\na Julia quickly. I think this is\nthe sort of thing that's just easy enough I\ncould actually just do it",
    "start": "2582660",
    "end": "2588245"
  },
  {
    "text": "in the repl. Should I do it with VS Code\nor just do it in the repl? I'll just do it in the repl. OK. So let's get a Julia.",
    "start": "2588245",
    "end": "2595760"
  },
  {
    "text": "This is a bit\nimpromptu, but why not? OK. So here, let's\njust grab a Julia. ",
    "start": "2595760",
    "end": "2602650"
  },
  {
    "text": "OK. And what I'd like to\ndo is demonstrate-- so the plan here is to\ndemonstrate that dx--",
    "start": "2602650",
    "end": "2611770"
  },
  {
    "text": " how do I do this?",
    "start": "2611770",
    "end": "2617020"
  },
  {
    "text": "Like that. Dx-squared is going to\nequal dx times x plus x dx.",
    "start": "2617020",
    "end": "2624390"
  },
  {
    "text": "OK. So that's going to be my goal. Any particular size\nyou want to see?",
    "start": "2624390",
    "end": "2629720"
  },
  {
    "text": "3-by-3 good enough? I guess so. Let's just do 3-by-3. Yeah, we'll make this\na little bit bigger. So I don't even think\nI need linear algebra.",
    "start": "2629720",
    "end": "2637710"
  },
  {
    "text": "Let's see. I don't even think\nI need any packages. So should I take-- all right.",
    "start": "2637710",
    "end": "2643310"
  },
  {
    "text": "Let me hear your nine\nfavorite integers-- small. Don't make them too big. Come on.",
    "start": "2643310",
    "end": "2648380"
  },
  {
    "text": "Just shout out nine integers. AUDIENCE: 5. ALAN EDELMAN: 5. AUDIENCE: 7. ALAN EDELMAN: 7.",
    "start": "2648380",
    "end": "2653705"
  },
  {
    "text": "AUDIENCE: Negative 3. ALAN EDELMAN: Negative 3. AUDIENCE: 17. AUDIENCE: 1, 2, 3. ALAN EDELMAN: 1, 2, 3. Thank you.",
    "start": "2653705",
    "end": "2659190"
  },
  {
    "text": "OK. Hurrying along here. Three more? AUDIENCE: 6, 7, 8. AUDIENCE: 0.",
    "start": "2659190",
    "end": "2664470"
  },
  {
    "text": "ALAN EDELMAN: 6, 7, 8. All right. Just to show you that\nthere's nothing up my sleeve. OK. And let's go, y is equal\nto x-squared maybe.",
    "start": "2664470",
    "end": "2674750"
  },
  {
    "text": "So here's the matrix square. OK? So you could check\nthat yourselves. And let's take dx to be--",
    "start": "2674750",
    "end": "2683390"
  },
  {
    "text": "oh, let's just go 0.001\ntimes rand of 3,3. Or is that too ugly?",
    "start": "2683390",
    "end": "2688550"
  },
  {
    "text": "Oh, it's fine. Let's just do it. OK? So here's a bunch\nof ugly numbers.",
    "start": "2688550",
    "end": "2694180"
  },
  {
    "text": "OK? So everybody understands that\nI'm sitting at this matrix X. And I'm moving over\nto X plus dx, right?",
    "start": "2694180",
    "end": "2703950"
  },
  {
    "text": "And I want to know\nthe difference. So let's do that, right? So dy-- we'll do it\nfirst numerically--",
    "start": "2703950",
    "end": "2710339"
  },
  {
    "text": "is going to be X plus\ndx-squared minus X-squared.",
    "start": "2710340",
    "end": "2719110"
  },
  {
    "text": "OK? So there's the dy. OK? And now let's compare that\nwith this magic matrix calculus",
    "start": "2719110",
    "end": "2727170"
  },
  {
    "text": "formula. The matrix calculus formula\nis X times dx plus dx times X.",
    "start": "2727170",
    "end": "2733760"
  },
  {
    "text": "And this is where I always have\nthat fear that I made a mistake or a typo. But let's hit Enter\nand see what happens.",
    "start": "2733760",
    "end": "2742200"
  },
  {
    "text": "And look at that. You see? So this is the numerical way\nof calculating a derivative.",
    "start": "2742200",
    "end": "2748170"
  },
  {
    "text": "Just see where I am over\nthere minus where I started. Or there's this magic formula.",
    "start": "2748170",
    "end": "2754170"
  },
  {
    "text": "And just to show you\nthat these other ways you might think of doing it\nwon't work, like 2x dx--",
    "start": "2754170",
    "end": "2761339"
  },
  {
    "text": "that's not right. Or 2 times dx times X--",
    "start": "2761340",
    "end": "2766920"
  },
  {
    "text": "I don't need that times. That's not right. But the one that is right--\nhere, let's put it back up--",
    "start": "2766920",
    "end": "2776810"
  },
  {
    "text": "is clearly that one. OK? OK. So I haven't showed you\nhow to derive it yet,",
    "start": "2776810",
    "end": "2783410"
  },
  {
    "text": "but that's coming soon. But I just wanted\nyou to sort of-- really, for me-- I don't know\nif your minds work this way.",
    "start": "2783410",
    "end": "2789980"
  },
  {
    "text": "But for me, seeing this is\npretty convincing, right?",
    "start": "2789980",
    "end": "2795530"
  },
  {
    "text": "I feel like I know\nwhat's going on, right? I really like looking\nat it this way. OK?",
    "start": "2795530",
    "end": "2800870"
  },
  {
    "text": "So after all, if I\nwere doing calculus on a computer for\nscalars, I would just",
    "start": "2800870",
    "end": "2806900"
  },
  {
    "text": "take X plus dx-squared\nfor scalars. And this is what I would do. And I would get 2X.",
    "start": "2806900",
    "end": "2811940"
  },
  {
    "text": "And in fact, just to\nremind you that this really is the same thing, we could take\nX to be 3 and dx to be 0.001.",
    "start": "2811940",
    "end": "2822650"
  },
  {
    "text": "OK? And then we can go like this.",
    "start": "2822650",
    "end": "2828339"
  },
  {
    "text": "And I can compare that\nwith what everybody knows from ordinary\ncalculus, 2X times dx.",
    "start": "2828340",
    "end": "2836335"
  },
  {
    "text": "Right? And to the right number\nof digits, they match. OK? Any questions about this?",
    "start": "2836335",
    "end": "2842010"
  },
  {
    "text": "All right. So hopefully you now\nkind of believe at least s not theoretically, that\nmatrix calculus kind of works.",
    "start": "2842010",
    "end": "2850440"
  },
  {
    "text": "All right. So getting back to\nthe slides here--",
    "start": "2850440",
    "end": "2856010"
  },
  {
    "text": "so it's helpful to know\ncertain rules that you all learned in freshman\ncalculus extend to matrices.",
    "start": "2856010",
    "end": "2863990"
  },
  {
    "text": "So you might remember that\nyou learned the product rule, right? So what's the product rule say?",
    "start": "2863990",
    "end": "2870130"
  },
  {
    "text": "That-- people learn it\nwith different notations. Many people learn it as duv\nequals udv plus vdu, right?",
    "start": "2870130",
    "end": "2882430"
  },
  {
    "text": "Other people learn the product\nrule with other notations. But the thing that\nI want you to know is that the product rule just\nworks for matrices and vectors",
    "start": "2882430",
    "end": "2890829"
  },
  {
    "text": "as well as for scalars. So any time you have a\ncompatible vector product or--",
    "start": "2890830",
    "end": "2897160"
  },
  {
    "text": "like a vector dot product\nor a matrix times vector, where the sizes all\nwork out, then it's",
    "start": "2897160",
    "end": "2904720"
  },
  {
    "text": "perfectly OK to take the matrix\nrule dab to be dab plus adb.",
    "start": "2904720",
    "end": "2910580"
  },
  {
    "text": "OK? So this would-- the\nonly thing that you have to remember is\nmatrices don't commute.",
    "start": "2910580",
    "end": "2916470"
  },
  {
    "text": "So if a is on the left and b is\non the right, a must always-- so in all the formats,\na must be on the left.",
    "start": "2916470",
    "end": "2923150"
  },
  {
    "text": "And b must always\nbe on the right. Like, if-- right?",
    "start": "2923150",
    "end": "2928760"
  },
  {
    "text": "So for example, this\nwas a little bit giving you the wrong idea.",
    "start": "2928760",
    "end": "2935690"
  },
  {
    "text": "I mean, it might have been\nbetter to say udv plus duv, you see, because this will\nwork for matrices, right?",
    "start": "2935690",
    "end": "2943640"
  },
  {
    "text": "I must always keep the u on the\nleft and the v on the right. And that's the only\nrule that makes",
    "start": "2943640",
    "end": "2949190"
  },
  {
    "text": "matrices just a little bit\ndifferent from scalars. OK?",
    "start": "2949190",
    "end": "2954560"
  },
  {
    "text": "So let's apply that to\nx transpose x, right? And so if we apply\nit to x transpose x,",
    "start": "2954560",
    "end": "2960770"
  },
  {
    "text": "it says that d x transpose x\nwill be dx transpose x plus x transpose dx.",
    "start": "2960770",
    "end": "2966750"
  },
  {
    "text": "OK? And here's where\nstudents get confused. I'm going to go ahead, and\ncombine them, and violate",
    "start": "2966750",
    "end": "2975440"
  },
  {
    "text": "every rule that I just said. I'm going to say that I\ncan combine them and get 2x transpose dx.",
    "start": "2975440",
    "end": "2981230"
  },
  {
    "text": "Wait a minute. What are-- how is it that\nI can combine this here when I just told you\nthat you can't put things",
    "start": "2981230",
    "end": "2987200"
  },
  {
    "text": "in other orders? Anybody want to tell\nme why, in this case, I can get away with it?",
    "start": "2987200",
    "end": "2992720"
  },
  {
    "text": "I just said to you the\na must be on the left and the b must be on the right. And here I am. I'm telling you that dx\ntranspose x and x transpose dx",
    "start": "2992720",
    "end": "3000490"
  },
  {
    "text": "are actually the same thing. How come I'm allowed to\ndo that in this one case",
    "start": "3000490",
    "end": "3007789"
  },
  {
    "text": "and combine these two? Can anybody tell me? Yes, please, in the back. AUDIENCE: A dot\nproduct is a scalar.",
    "start": "3007790",
    "end": "3013570"
  },
  {
    "text": "ALAN EDELMAN: Yeah, the dot\nproduct is just a scalar. And it doesn't matter\nwhat order you do it in. Exactly right. Just to kind of\nshow you in Julia--",
    "start": "3013570",
    "end": "3019780"
  },
  {
    "text": "oh. Oh, you know what? [CHUCKLES] There's a tiny little\nhole where the wire comes out.",
    "start": "3019780",
    "end": "3025570"
  },
  {
    "text": "And if the chair leg falls into\nthat hole, I go down with it. [LAUGHS]",
    "start": "3025570",
    "end": "3030589"
  },
  {
    "text": "All right. So here we go. So let's say X is \"1 2 3.\"",
    "start": "3030590",
    "end": "3037070"
  },
  {
    "text": "Oh, well, actually,\nlet's put it with commas. OK? And if dx is 0.001\ntimes rand of three--",
    "start": "3037070",
    "end": "3048060"
  },
  {
    "text": "see, I think if I did it this\nway, you'd all believe it. If I go-- can I do dot in Base?",
    "start": "3048060",
    "end": "3053070"
  },
  {
    "text": "Let's see. Maybe, maybe not. \"not defined.\" OK. But anyway, I don't even\nwant to do it that way.",
    "start": "3053070",
    "end": "3059680"
  },
  {
    "text": "Let's go x transpose dx,\nOK, and dx transpose x. You see they're\nexactly the same thing.",
    "start": "3059680",
    "end": "3065850"
  },
  {
    "text": "After all, the dot\nproduct of the-- right? So my point is the dot\nproduct of x and dx",
    "start": "3065850",
    "end": "3076700"
  },
  {
    "text": "is, of course, the\nsame as dx and x.",
    "start": "3076700",
    "end": "3081950"
  },
  {
    "text": "OK? So just to remind you, a\ntimes b is not b times a.",
    "start": "3081950",
    "end": "3090660"
  },
  {
    "text": "But x transpose y\nequals y transpose x if x and y are vectors.",
    "start": "3090660",
    "end": "3099230"
  },
  {
    "text": "Understood? You understand the difference\nbetween the general case, where things don't commute,\nand the dot product,",
    "start": "3099230",
    "end": "3104690"
  },
  {
    "text": "where things do commute? OK? So students seem to get\nconfused by that sometimes. And so I just wanted to say\nthat as clearly as I know how.",
    "start": "3104690",
    "end": "3112770"
  },
  {
    "text": "OK? So one more time, using\nthe product rule on dx",
    "start": "3112770",
    "end": "3117800"
  },
  {
    "text": "transpose x-- you can use this\napproach to write it as-- you use the product rule.",
    "start": "3117800",
    "end": "3124619"
  },
  {
    "text": "OK? And then you recognize\nthat those two dot products are the same. And so you can combine\nthem into 2X transpose dx.",
    "start": "3124620",
    "end": "3131460"
  },
  {
    "text": "OK? And so in a way-- now, of course, you\ncould do it with--",
    "start": "3131460",
    "end": "3137720"
  },
  {
    "text": "to take the gradient\nof x transpose x is not hard to do\nthe old-fashioned way. And there's no-- you could\ndecide for yourselves.",
    "start": "3137720",
    "end": "3145040"
  },
  {
    "text": "But on the blackboard, let me\njust quickly remind you that-- here's the old-fashioned\nway to do it.",
    "start": "3145040",
    "end": "3151820"
  },
  {
    "text": "The old fashioned\nway is to say f of x equals x transpose x is\nthe sum of the xi-squared.",
    "start": "3151820",
    "end": "3158660"
  },
  {
    "text": "And so this is like\nthe baby way to do it, but this is the sort\nof thing that you'll see in a lot of classes, right? And you'll learn that\nthe gradient of f",
    "start": "3158660",
    "end": "3167460"
  },
  {
    "text": "is the vector that has df dx1\nall the way down to df dxn.",
    "start": "3167460",
    "end": "3174000"
  },
  {
    "text": "And so you'll look f of xi. And you'll say, oh, df\nof dxi is equal to 2xi.",
    "start": "3174000",
    "end": "3181980"
  },
  {
    "text": "And so you'll put\nthem all in a vector. And you will say, ha, the\ngradient of f is equal to 2x,",
    "start": "3181980",
    "end": "3187708"
  },
  {
    "text": "right? And there's nothing\nwrong with that. I mean, you get good\nat this sort of thing.",
    "start": "3187708",
    "end": "3193450"
  },
  {
    "text": "But I hope you could sort of\nsee the advantage of doing it holistically. I don't have to use indices.",
    "start": "3193450",
    "end": "3199050"
  },
  {
    "text": "I don't have to think about\nwhat this thing is made of. I just do this\nlittle trick here. And boom, I get the\nanswer right away, right?",
    "start": "3199050",
    "end": "3205260"
  },
  {
    "text": "It's kind of like a magic\ntrick for vectors and matrices, that-- like, if you take\ncourses where people are taking gradients, all the\nother students in the class",
    "start": "3205260",
    "end": "3213453"
  },
  {
    "text": "are going to do it this way. And you're going to\nknow the secret trick of being able to get the\ngradient this way, right? So you'll have super powers\nthat they don't have.",
    "start": "3213453",
    "end": "3219990"
  },
  {
    "text": "OK. So of course, both ways work. But OK.",
    "start": "3219990",
    "end": "3226170"
  },
  {
    "text": "OK. So here, let's see. We have du transpose v is-- ",
    "start": "3226170",
    "end": "3232630"
  },
  {
    "text": "this is sort of the point\nthat I was making already. So I think we have that. OK? Any questions?",
    "start": "3232630",
    "end": "3239030"
  },
  {
    "text": "All right.  OK. So let's-- just, again,\na little warm up.",
    "start": "3239030",
    "end": "3249360"
  },
  {
    "text": "And so sometimes it's good to\ncount how many parameters are",
    "start": "3249360",
    "end": "3256500"
  },
  {
    "text": "needed to express an answer. \"Parameters\" is sort\nof a vague term. But think about it\nas, how many numbers",
    "start": "3256500",
    "end": "3262105"
  },
  {
    "text": "do I need if I'm going to put\nsomething in a matrix-- how many elements. So let me ask you quickly,\nif I have a function that",
    "start": "3262105",
    "end": "3268950"
  },
  {
    "text": "takes n inputs to\nm outputs and I do want to express the\nderivative as a matrix,",
    "start": "3268950",
    "end": "3274380"
  },
  {
    "text": "how many numbers do I need?  So n inputs, m\noutputs-- how many",
    "start": "3274380",
    "end": "3281320"
  },
  {
    "text": "numbers do I need to\nexpress it as a matrix? AUDIENCE: m times n. ALAN EDELMAN: m times n. All right. Everybody knows.",
    "start": "3281320",
    "end": "3286460"
  },
  {
    "text": "OK. And I think this\nis my last slide. So I'll just say a few words\nabout second derivatives",
    "start": "3286460",
    "end": "3292580"
  },
  {
    "text": "quickly. And then maybe we take, like, a\nfive-minute break or something. And then Steven will speak.",
    "start": "3292580",
    "end": "3300410"
  },
  {
    "text": "So I don't know how\nmuch we're going to talk about second\nderivatives in this class.",
    "start": "3300410",
    "end": "3306650"
  },
  {
    "text": "But there's one example that\ncomes up so often that it's probably just worth mentioning.",
    "start": "3306650",
    "end": "3311809"
  },
  {
    "text": "And in a way, it-- I think when I first learned\nabout differentiating vectors,",
    "start": "3311810",
    "end": "3318140"
  },
  {
    "text": "I always got confused\nbetween the Hessian, which is a matrix-- it's a second\nderivative matrix--",
    "start": "3318140",
    "end": "3323869"
  },
  {
    "text": "and the Jacobian,\nwhich is also a matrix, but it's a first\nderivative matrix, right? So let me be clear.",
    "start": "3323870",
    "end": "3336750"
  },
  {
    "text": "If you have a function\nfrom rm to rn, you can express the answer\nas an m-by-n matrix. And that's what we call\nthe Jacobian, right?",
    "start": "3336750",
    "end": "3344102"
  },
  {
    "text": "So the big point\nI'm just saying is it's a function from\nvectors to vectors. The other case\nthat comes up very",
    "start": "3344102",
    "end": "3350030"
  },
  {
    "text": "often is if you have a function\nfrom vectors to scalars.",
    "start": "3350030",
    "end": "3357240"
  },
  {
    "text": "OK? And so examples\nlike x transpose ax",
    "start": "3357240",
    "end": "3364930"
  },
  {
    "text": "would be vector transpose\nmatrix times vector as a function of the vector. OK? And so now the--",
    "start": "3364930",
    "end": "3373300"
  },
  {
    "text": "so if it's going from\nvector to scalar, remember that the\ngradient is a vector.",
    "start": "3373300",
    "end": "3378490"
  },
  {
    "text": "It's one-dimensional. But the second derivative is\noften expressed as a matrix. And that's what we\ncall the Hessian.",
    "start": "3378490",
    "end": "3385329"
  },
  {
    "text": "So you might ask\nyourself, if we're trying to get away from\nelement-wise matrix",
    "start": "3385330",
    "end": "3391050"
  },
  {
    "text": "representations, what's\nsort of the abstraction that we will need to talk\nabout second derivatives?",
    "start": "3391050",
    "end": "3396480"
  },
  {
    "text": "And the answer in advanced\nlinear algebra classes is called a quadratic form. OK? And we'll see what that's--",
    "start": "3396480",
    "end": "3402105"
  },
  {
    "text": "So we can get past the\nclunkiness of matrices, like 18.06, and get into\nthe more abstraction.",
    "start": "3402105",
    "end": "3409560"
  },
  {
    "text": "And those are the\nquadratic forms. OK. So I think that's the end of\nwhat I'm going to say today.",
    "start": "3409560",
    "end": "3417160"
  },
  {
    "text": "Last chance for questions. Otherwise, what should we take? Should we take a\nfive-minute break, Steven? What do you think--",
    "start": "3417160",
    "end": "3422550"
  },
  {
    "text": "or a little less? STEVEN JOHNSON: Sure,\nfive minutes is good. ALAN EDELMAN: Five\nminutes is good?",
    "start": "3422550",
    "end": "3427910"
  },
  {
    "text": "All right. So I've got 12:03 on\nthe clock back here. So let's just take a\nfive-minute break-- until 12:08.",
    "start": "3427910",
    "end": "3433850"
  },
  {
    "text": "And people can get some water,\nor ask me some questions, or read their email, or\nwhatever you do in five minutes.",
    "start": "3433850",
    "end": "3439530"
  },
  {
    "text": "All right? And then, Steven, you can grab\nthe screen and set yourself up.",
    "start": "3439530",
    "end": "3445000"
  },
  {
    "start": "3445000",
    "end": "3461000"
  }
]