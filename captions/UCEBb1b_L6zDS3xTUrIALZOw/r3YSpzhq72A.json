[
  {
    "text": "[SQUEAKING] [RUSTLING] [CLICKING]",
    "start": "0",
    "end": "7776"
  },
  {
    "start": "7776",
    "end": "17510"
  },
  {
    "text": "SARA ELLISON: OK, so a\nlittle bit of a review. I want to put what\nEsther has been",
    "start": "17510",
    "end": "24020"
  },
  {
    "text": "doing the last few lectures into\na little bit broader context. And that's going to be\na sort of a nice segue",
    "start": "24020",
    "end": "30740"
  },
  {
    "text": "into talking about\nlinear regression. So what did we do for the first\nsort of half of the semester?",
    "start": "30740",
    "end": "37800"
  },
  {
    "text": "We established a\nfoundation in probability, and we proceeded to talk\nabout how to estimate",
    "start": "37800",
    "end": "43820"
  },
  {
    "text": "unknown parameters, OK. Most, if not all\nof that discussion,",
    "start": "43820",
    "end": "50480"
  },
  {
    "text": "was focused on\nestimating parameters of a univariate\ndistribution, OK.",
    "start": "50480",
    "end": "55730"
  },
  {
    "text": "So we were talking about\nestimating the mean or estimating the variance\nor something like that. Some other parameter\nthat characterizes sort",
    "start": "55730",
    "end": "63410"
  },
  {
    "text": "of a univariate distribution. But so much of what we care\nabout in social science",
    "start": "63410",
    "end": "69920"
  },
  {
    "text": "and in lots of other\nsettings, I mean, maybe most other settings,\ninvolves joint distributions,",
    "start": "69920",
    "end": "76330"
  },
  {
    "text": "though. And so we really\nhave to be concerned with how to estimate\nparameters characterizing",
    "start": "76330",
    "end": "81780"
  },
  {
    "text": "either joint distributions\nor conditional distributions or something like that, OK.",
    "start": "81780",
    "end": "86910"
  },
  {
    "text": "So Esther's discussion\nof causality was really the\nbeginning of this. And it was also we could think\nof it as a special case, OK.",
    "start": "86910",
    "end": "95400"
  },
  {
    "text": "So you can think\nof much of what she",
    "start": "95400",
    "end": "100560"
  },
  {
    "text": "did as considering the\njoint distribution of two variables where one\nwas simply a coin flip.",
    "start": "100560",
    "end": "107500"
  },
  {
    "text": "So if you flip a\ncoin, and it comes up heads, that's a treatment, that\nobservation has a treatment.",
    "start": "107500",
    "end": "114460"
  },
  {
    "text": "And if it comes up\ntails, that observation is assigned to control. And then the other\nrandom variable",
    "start": "114460",
    "end": "121620"
  },
  {
    "text": "that was defined on this sample,\nwas the outcome of interest, say, infant mortality\nor website effectiveness",
    "start": "121620",
    "end": "130020"
  },
  {
    "text": "or something like that, OK. So we didn't always talk about--",
    "start": "130020",
    "end": "135468"
  },
  {
    "text": "oh, and I should say, in\nfact, we were mostly concerned with the conditional\ndistribution of the outcome variable\nconditional on this coin flip,",
    "start": "135468",
    "end": "142020"
  },
  {
    "text": "OK. So we can and we did think of\nthe treatment and the control group sometimes as being sort\nof two separate populations.",
    "start": "142020",
    "end": "149870"
  },
  {
    "text": "And so then we asked\nthe question, well, how could we test whether\nthey had a common mean?",
    "start": "149870",
    "end": "157060"
  },
  {
    "text": "But we can also think\nof them as having one population and a\njoint distribution of two",
    "start": "157060",
    "end": "163540"
  },
  {
    "text": "random variables\non that population. And what we were really doing\nis estimating conditional means,",
    "start": "163540",
    "end": "172270"
  },
  {
    "text": "conditional on this coin\nflip random variable. So these are two\ndifferent ways to think",
    "start": "172270",
    "end": "178600"
  },
  {
    "text": "about randomized controlled\ntrials and the things that Esther was talking about\nthe last couple of lectures.",
    "start": "178600",
    "end": "186490"
  },
  {
    "text": "OK, and they're sort\nof equivalent ways to think about them.",
    "start": "186490",
    "end": "191510"
  },
  {
    "text": "Now, I want to pose a question. What if, instead of a coin\nflip, the second random variable",
    "start": "191510",
    "end": "197210"
  },
  {
    "text": "is continuous instead? So let's suppose we\nhave a random variable.",
    "start": "197210",
    "end": "202250"
  },
  {
    "text": "It can take on a\nwhole range of values. We can still think\nof it as being sort of levels of treatment\nor something like that.",
    "start": "202250",
    "end": "208687"
  },
  {
    "text": "We don't necessarily have\nto think of it that way, but you can think\nof it that way. But instead of just being a\ncoin flip treatment control,",
    "start": "208687",
    "end": "217550"
  },
  {
    "text": "it can take on many\ndifferent values.",
    "start": "217550",
    "end": "222590"
  },
  {
    "text": "How do we analyze the\nconditional distribution of our outcome variable\nconditional on something",
    "start": "222590",
    "end": "227750"
  },
  {
    "text": "like a continuous\nrandom variable? And furthermore, how do\nwe estimate the parameters",
    "start": "227750",
    "end": "232820"
  },
  {
    "text": "of that conditional\ndistribution, and that's what we're going\nto be talking about today.",
    "start": "232820",
    "end": "238370"
  },
  {
    "text": "The workhorse model we use\nis called the linear model. And the way that we estimate\nthe parameters in a linear model",
    "start": "238370",
    "end": "246110"
  },
  {
    "text": "is linear regression,\nor that's typically the way that we estimate them.",
    "start": "246110",
    "end": "251850"
  },
  {
    "text": "So why do we care about\njoint distributions in estimating the parameters\nassociated with them?",
    "start": "251850",
    "end": "257299"
  },
  {
    "text": "Well, we've seen a\nbunch of examples, but let me just throw\nout a couple more.",
    "start": "257300",
    "end": "263300"
  },
  {
    "text": "So we might be\ninterested in prediction. We might be interested\nin determining causality,",
    "start": "263300",
    "end": "269900"
  },
  {
    "text": "and we might just be\ninterested in understanding the world better. So let me give you\nexamples of each of these. ",
    "start": "269900",
    "end": "278320"
  },
  {
    "text": "OK, so let's imagine that\nI'm the type of person who",
    "start": "278320",
    "end": "285910"
  },
  {
    "text": "reads XKCD comics. And I'm also the\ntype of person who is likely to click on an\nad for a t-shirt bearing",
    "start": "285910",
    "end": "293500"
  },
  {
    "text": "the Russian cover\ndesign of Moby Dick. So who might be interested in\nwhether that statement is true?",
    "start": "293500",
    "end": "302130"
  },
  {
    "text": "AUDIENCE: Amazon. SARA ELLISON:\nAmazon or whoever's selling the t-shirts with\nthe cover of the Russian Moby",
    "start": "302130",
    "end": "309909"
  },
  {
    "text": "Dick on them. So basically anyone\nwho wants to sell me",
    "start": "309910",
    "end": "315100"
  },
  {
    "text": "that product or anyone who wants\nto serve me an ad that might get me to click on the\nad and buy the product,",
    "start": "315100",
    "end": "322240"
  },
  {
    "text": "cares about whether the\nperson who reads XKCD comics",
    "start": "322240",
    "end": "329289"
  },
  {
    "text": "is also the kind of person\nwho buys such t-shirts. It turns out I am such a person.",
    "start": "329290",
    "end": "334540"
  },
  {
    "text": "I do both of those things. And I thought I'll just show\nyou my favorite XKCD comic, and let you read\nit for a second.",
    "start": "334540",
    "end": "340760"
  },
  {
    "text": "And I did just\norder this t-shirt from Out of Print, which\nI think is a pretty",
    "start": "340760",
    "end": "346545"
  },
  {
    "text": "nice looking t-shirt. ",
    "start": "346545",
    "end": "356140"
  },
  {
    "text": "That's for all you course\nsix majors, I guess. ",
    "start": "356140",
    "end": "363020"
  },
  {
    "text": "OK, can I go ahead. OK, so anyhow,\nthere's nothing about",
    "start": "363020",
    "end": "368720"
  },
  {
    "text": "causality in this example. I didn't read the XKCD\ncomic and think, oh, now,",
    "start": "368720",
    "end": "374930"
  },
  {
    "text": "I have to go by this\nsort of t-shirt. It's just that I'm\nthe kind of person who might do both of those things.",
    "start": "374930",
    "end": "380670"
  },
  {
    "text": "So someone wanting to sell\nme that t-shirt, would like to know that,\nso they can serve me the ad when I'm reading XKCD.",
    "start": "380670",
    "end": "386255"
  },
  {
    "text": " We might want to\ndetermine causality.",
    "start": "386255",
    "end": "392870"
  },
  {
    "text": "Esther talked about a number\nof examples in this vein.",
    "start": "392870",
    "end": "398040"
  },
  {
    "text": "So let's suppose I\nwant to know the answer to the following question. If I give my dog\na treat every time",
    "start": "398040",
    "end": "404270"
  },
  {
    "text": "he does not bark at another\ndog walking by our house, will he stop barking\nat other dogs?",
    "start": "404270",
    "end": "410300"
  },
  {
    "text": "So that's the kind\nof causal question",
    "start": "410300",
    "end": "415729"
  },
  {
    "text": "I might want a answer to. The answer, by the way, is no. And the only reason I\nbrought up this example",
    "start": "415730",
    "end": "422390"
  },
  {
    "text": "is so I would have an excuse to\nshow you my sweet little dog. ",
    "start": "422390",
    "end": "429200"
  },
  {
    "text": "But I have not carried out\nsuch a statistical analysis.",
    "start": "429200",
    "end": "434990"
  },
  {
    "text": "I'm pretty sure the\nanswer is no, though. And then maybe we just care\nabout joint distributions",
    "start": "434990",
    "end": "442740"
  },
  {
    "text": "because we want to\nunderstand the world better. So if we were\nbehavioral economists,",
    "start": "442740",
    "end": "448890"
  },
  {
    "text": "we might be interested in\nthe following question. Are people only influenced\nby price, quality,",
    "start": "448890",
    "end": "455520"
  },
  {
    "text": "characteristics, and\nthe expected weather that they have in the\narea of the country where",
    "start": "455520",
    "end": "461970"
  },
  {
    "text": "they live when they're\ndeciding whether to purchase a convertible?",
    "start": "461970",
    "end": "467250"
  },
  {
    "text": "Or are they also\ninfluenced by the weather on that particular day?",
    "start": "467250",
    "end": "472770"
  },
  {
    "text": "So behavioral\neconomists might be interested in a\nquestion like this because they want\nto understand what",
    "start": "472770",
    "end": "481320"
  },
  {
    "text": "influences economic\ndecisions that agents make. Do they make rational\ndecisions, or are they",
    "start": "481320",
    "end": "488669"
  },
  {
    "text": "deciding to buy a\nconvertible because they know what the weather is going\nto be like over the next five",
    "start": "488670",
    "end": "495150"
  },
  {
    "text": "years, and that's\nthe period of time, they're going to own the car? Or are they likely to make\nthese sort of decisions",
    "start": "495150",
    "end": "502300"
  },
  {
    "text": "that are hard to justify by\nany rational economic model, like, gosh, it's\nreally sunny today.",
    "start": "502300",
    "end": "507477"
  },
  {
    "text": "Maybe I'll buy a convertible\nthat I'm going to now have for the next five years? Well, it turns\nout that they are.",
    "start": "507477",
    "end": "516469"
  },
  {
    "text": "And this was a recent\npaper in the QJE, \"The Psychological Effect of\nWeather and Car Purchases\"",
    "start": "516470",
    "end": "522700"
  },
  {
    "text": "that fairly\nconvincingly established",
    "start": "522700",
    "end": "529090"
  },
  {
    "text": "that people are,\nin fact, influenced by the weather on the day\nthat they make a purchase.",
    "start": "529090",
    "end": "535089"
  },
  {
    "text": "When the weather's\nbad, snowy they're more likely to make\na purchase of an SUV.",
    "start": "535090",
    "end": "542529"
  },
  {
    "text": "And this is controlling for the\nexpected weather going forward.",
    "start": "542530",
    "end": "549250"
  },
  {
    "text": "And likewise, if\nit's sunny, they're more likely to buy a convertible\ncontrolling for the expected",
    "start": "549250",
    "end": "554379"
  },
  {
    "text": "weather. So anyhow, I went to a talk. I saw this paper\na few years ago.",
    "start": "554380",
    "end": "560110"
  },
  {
    "text": "I was convinced by the results,\nbut I also sort of chuckled. I thought, oh,\nthat's so ridiculous.",
    "start": "560110",
    "end": "565390"
  },
  {
    "text": "How could someone\nbe so influenced? Well, turns out\nthat last summer,",
    "start": "565390",
    "end": "571300"
  },
  {
    "text": "I decided I was going\nto buy this car. [LAUGHTER] And so I was waiting for\nthe 2016 model to come out.",
    "start": "571300",
    "end": "579880"
  },
  {
    "text": "And it was a really\nhot model of car. I couldn't even test drive it. It was like really hard to--",
    "start": "579880",
    "end": "585325"
  },
  {
    "text": "and I thought, OK, I\nthought about the QJE paper, and I said, all I have to do\nis wait until the weather gets",
    "start": "585325",
    "end": "590680"
  },
  {
    "text": "bad, and then the\nmodels of this car will build up on\nthe dealer's lots.",
    "start": "590680",
    "end": "596710"
  },
  {
    "text": "And I'll just waltz in\nand be able to buy one, and maybe I'll even\nget a deal on it. Well, it turns out, as soon\nas the weather got bad,",
    "start": "596710",
    "end": "603800"
  },
  {
    "text": "I kind of lost my interest\nin buying the convertible, and I just ended up\nbuying a sedan, so.",
    "start": "603800",
    "end": "610900"
  },
  {
    "text": "[LAUGHTER]  OK, so in each of\nthese examples,",
    "start": "610900",
    "end": "617209"
  },
  {
    "text": "we had two or more\nrandom variables that had a joint distribution. And we wanted to know\nthe characteristics",
    "start": "617210",
    "end": "622740"
  },
  {
    "text": "of their joint\ndistribution in order to answer the questions\nthat we were interested in. ",
    "start": "622740",
    "end": "629700"
  },
  {
    "text": "OK, so how do we do this? Well, I told you already,\nthe workhorse model to do this is the linear model.",
    "start": "629700",
    "end": "635800"
  },
  {
    "text": "So let me introduce the\nlinear model, bivariate style. We'll generalize that later.",
    "start": "635800",
    "end": "642840"
  },
  {
    "text": "And let me just go\nthrough and define what the things in\nthis linear model are.",
    "start": "642840",
    "end": "649649"
  },
  {
    "text": "And then we'll talk\nabout properties of it, and we'll talk about how\nto estimate the parameters in it and so forth.",
    "start": "649650",
    "end": "656670"
  },
  {
    "text": "OK, well, the linear\nmodel is a model that establishes a relationship\nbetween two random variables",
    "start": "656670",
    "end": "664410"
  },
  {
    "text": "on which we have\nrepeated observations. So these are y and x.",
    "start": "664410",
    "end": "669660"
  },
  {
    "text": "So y is what we call\nthe dependent variable. Sometimes you will\nhear that referred",
    "start": "669660",
    "end": "675990"
  },
  {
    "text": "to as the explained\nvariable or the regressand. I would say hardly ever\ndo I hear those terms,",
    "start": "675990",
    "end": "682889"
  },
  {
    "text": "but those are both\nlegitimate terms, I guess, for the dependent variable.",
    "start": "682890",
    "end": "689600"
  },
  {
    "text": "And then the x\nvariable is typically called the regressor or\nexplanatory variable,",
    "start": "689600",
    "end": "696100"
  },
  {
    "text": "or sometimes called the\nindependent variable. So even though regressor and\nregressand seem to go together,",
    "start": "696100",
    "end": "703660"
  },
  {
    "text": "or independent variable\nand dependent variable seem to go together, that's not\nthe way these terms are used.",
    "start": "703660",
    "end": "709389"
  },
  {
    "text": "Typically, we refer\nto the y variable as the dependent variable,\nand typically, you'll",
    "start": "709390",
    "end": "716830"
  },
  {
    "text": "refer to the x variable\nas either the regressor or the explanatory variable.",
    "start": "716830",
    "end": "723069"
  },
  {
    "text": "And there are some\nreasons for that. But this is just\nbasically what people do, just so you're familiar\nwith the terms.",
    "start": "723070",
    "end": "732330"
  },
  {
    "text": "And then we have a\nthird random variable thrown into the mix, an\nunobserved random variable, and we call that the error.",
    "start": "732330",
    "end": "738335"
  },
  {
    "start": "738335",
    "end": "745190"
  },
  {
    "text": "We also have two\nparameters in this model, or at least two\nobvious parameters. We have another one that\nwe'll talk about later.",
    "start": "745190",
    "end": "752190"
  },
  {
    "text": "And these are parameters\nwe want to estimate, and we call these the\nregression coefficients.",
    "start": "752190",
    "end": "757520"
  },
  {
    "text": " OK, so what does this model do?",
    "start": "757520",
    "end": "763250"
  },
  {
    "text": "It allows us to consider the\nmean of a random variable y as a function of another random\nvariable x And if we obtain",
    "start": "763250",
    "end": "771190"
  },
  {
    "text": "estimates for beta 0\nand beta 1, then we have an estimated conditional\nmean function for y.",
    "start": "771190",
    "end": "777040"
  },
  {
    "text": " So let's add some\nbasic assumptions,",
    "start": "777040",
    "end": "783940"
  },
  {
    "text": "and then we will\nget to something called the classical\nlinear-regression model when we sort of pile on a\nfew more assumptions.",
    "start": "783940",
    "end": "790600"
  },
  {
    "text": "OK, first of all, let's assume\nthat our two random variables on the right side\nare uncorrelated,",
    "start": "790600",
    "end": "797980"
  },
  {
    "text": "so x sub i and epsilon sub i are\nuncorrelated random variables.",
    "start": "797980",
    "end": "805100"
  },
  {
    "text": "Let's assume what\nthis second assumption",
    "start": "805100",
    "end": "811190"
  },
  {
    "text": "called identification\nmeans is just basically, that we have some variation\nin our x variable.",
    "start": "811190",
    "end": "817790"
  },
  {
    "text": "I'm going to show you a picture\nillustrating this in a second. But basically, that just means\nthat the repeated observations",
    "start": "817790",
    "end": "829880"
  },
  {
    "text": "on the x variable are not\nall on the same point.",
    "start": "829880",
    "end": "834920"
  },
  {
    "text": "We're going to assume that\nthe expectation of our error is equal to 0.",
    "start": "834920",
    "end": "840500"
  },
  {
    "text": "We'll assume a\nhomoskedasticity, which means that the expectation\nof our error squared",
    "start": "840500",
    "end": "847820"
  },
  {
    "text": "is equal to sigma\nsquared for all i. And we're going to also\nassume that we do not",
    "start": "847820",
    "end": "853100"
  },
  {
    "text": "have serial correlation. So in particular,\nthe expectation of epsilon i epsilon j is\nequal to 0 for all i and j.",
    "start": "853100",
    "end": "862130"
  },
  {
    "text": "So let me talk about\neach one of these in turn and show you some\npictures, so you",
    "start": "862130",
    "end": "867300"
  },
  {
    "text": "understand exactly what the\nmeat of these assumptions is.",
    "start": "867300",
    "end": "873040"
  },
  {
    "text": "Oh, a couple notes, we sometimes\nimpose an alternate assumption",
    "start": "873040",
    "end": "878110"
  },
  {
    "text": "to 1 for our convenience. And that's sometimes we\nassume that instead of x being",
    "start": "878110",
    "end": "884380"
  },
  {
    "text": "a random variable, xi\nbeing a random variable, we assume that xi's\nare fixed and repeated",
    "start": "884380",
    "end": "890230"
  },
  {
    "text": "samples or nonstochastic. This makes some of\nthe proofs easier. Sometimes we do it\nfor convenience.",
    "start": "890230",
    "end": "896649"
  },
  {
    "text": "The results\nessentially go through without that assumption. But we'll go back and\nforth between them.",
    "start": "896650",
    "end": "904520"
  },
  {
    "text": "And then the other note\nis that assumptions-- we had the three different\nassumptions on the error term. And all of those\nassumptions could",
    "start": "904520",
    "end": "911290"
  },
  {
    "text": "be subsumed under one stronger\nassumption that the epsilons are iid normal 0 sigma squared.",
    "start": "911290",
    "end": "921190"
  },
  {
    "text": "So sometimes, especially\nwhen we talk about inference, we'll impose that assumption.",
    "start": "921190",
    "end": "927339"
  },
  {
    "text": "Again, that assumption is not\nnecessary for almost anything that we do in linear\nregression, but sometimes it's",
    "start": "927340",
    "end": "933740"
  },
  {
    "text": "sort of convenient. It makes the proofs easier\nand more elegant, I guess.",
    "start": "933740",
    "end": "941649"
  },
  {
    "text": "So let's look at this\nassumption number two called identification. So I have this kind of strange\nlooking equation up at the top,",
    "start": "941650",
    "end": "952690"
  },
  {
    "text": "but all that means is\nthat we're ruling out a case where all of\nour observations on x",
    "start": "952690",
    "end": "959980"
  },
  {
    "text": "are on a particular value x0. And the reason we\nrule out this case is, because this doesn't\ngive us any variation in x",
    "start": "959980",
    "end": "967360"
  },
  {
    "text": "that we need to identify the\nmean of y as a function of x. If we're trying to fit a\nsort of a regression line,",
    "start": "967360",
    "end": "975520"
  },
  {
    "text": "we just have a bunch\nof observations on a particular point,\nand we can't do it. ",
    "start": "975520",
    "end": "985029"
  },
  {
    "text": "The third assumption is\nthat the error has a 0 mean.",
    "start": "985030",
    "end": "992410"
  },
  {
    "text": "Well, so basically,\nwhat we rule out-- so here I've drawn a\npicture of an error having",
    "start": "992410",
    "end": "998380"
  },
  {
    "text": "a positive mean. And we just rule\nthis out because we don't have any information\nthat would help us separately",
    "start": "998380",
    "end": "1007950"
  },
  {
    "text": "sort out whether our\nerror just had a 0 mean,",
    "start": "1007950",
    "end": "1013710"
  },
  {
    "text": "and the regression\nline was shifted up, or rather the error\nhad a positive mean,",
    "start": "1013710",
    "end": "1021750"
  },
  {
    "text": "and our regression\nline was shifted down. We don't have any\ninformation to separately",
    "start": "1021750",
    "end": "1027119"
  },
  {
    "text": "to sort those two\npossibilities out. So we just assume that the\nerror has 0 expectation.",
    "start": "1027119",
    "end": "1034199"
  },
  {
    "text": " And I should say,\nwell, another way",
    "start": "1034200",
    "end": "1043359"
  },
  {
    "text": "to say this is that those two-- the expectation of the error and\nthe intercept of the regression",
    "start": "1043359",
    "end": "1050710"
  },
  {
    "text": "line are not\nseparately identified. That's another piece\nof terminology.",
    "start": "1050710",
    "end": "1056590"
  },
  {
    "text": "That just means we don't\nhave any way to them out with the data.",
    "start": "1056590",
    "end": "1063210"
  },
  {
    "text": "Our fourth assumption\nis homoscedasticity. So what is homoscedasticity?",
    "start": "1063210",
    "end": "1068600"
  },
  {
    "text": "It just means that\nwe're going to assume that the variance\nfor all of our errors that the variances is equal\nto sigma squared for all--",
    "start": "1068600",
    "end": "1078840"
  },
  {
    "text": "oh, that's supposed\nto be all little i, but my autocorrect made\nit capital I, I think.",
    "start": "1078840",
    "end": "1086270"
  },
  {
    "text": "So this is a picture of what\nthe opposite of homoskedasticity",
    "start": "1086270",
    "end": "1091940"
  },
  {
    "text": "might look like. This is called\nheteroskedasticity. And here I've drawn it so\nthat it looks like the error",
    "start": "1091940",
    "end": "1098990"
  },
  {
    "text": "variance for low values of x\nis much higher than the error variance for higher values of x.",
    "start": "1098990",
    "end": "1106950"
  },
  {
    "text": "And so we're going to\nrule that out for now. That's actually not\na fatal problem.",
    "start": "1106950",
    "end": "1113310"
  },
  {
    "text": "We can deal with it, but\nat least for now, we're going to assume that we have\nhomoskedasticity and not",
    "start": "1113310",
    "end": "1119279"
  },
  {
    "text": "heteroskedasticity. So right about now,\nyou're probably thinking,",
    "start": "1119280",
    "end": "1124720"
  },
  {
    "text": "what is the etymology of\nhomo and heteroskedasticity and is she even\nspelling it right?",
    "start": "1124720",
    "end": "1130780"
  },
  {
    "text": "Because in fact,\nmy autocorrect kept trying to replace\nk with c whenever I was typing homoskedasticity.",
    "start": "1130780",
    "end": "1137610"
  },
  {
    "text": "Turns out that the\nautocorrect in PowerPoint has not read this\narticle in Econometrica",
    "start": "1137610",
    "end": "1145960"
  },
  {
    "text": "on the etymology and the correct\nspelling of homoskedasticity and heteroskedasticity.",
    "start": "1145960",
    "end": "1152080"
  },
  {
    "text": "So anyhow, I've put\nthis up on the website. You can peruse this\nat your leisure.",
    "start": "1152080",
    "end": "1158090"
  },
  {
    "text": "But this will answer\nall your questions about the origin of these words. ",
    "start": "1158090",
    "end": "1166659"
  },
  {
    "text": "And then the fifth\nassumption is that we have no serial correlation.",
    "start": "1166660",
    "end": "1172100"
  },
  {
    "text": "So that means, in other\nwords, that the errors are not",
    "start": "1172100",
    "end": "1178780"
  },
  {
    "text": "correlated if they're associated\nwith different observations.",
    "start": "1178780",
    "end": "1183940"
  },
  {
    "text": "So a stronger way\nof saying this, is that the errors\nare independent across observations.",
    "start": "1183940",
    "end": "1190330"
  },
  {
    "text": "We don't need to impose that. We're just going to say\nthat they're uncorrelated.",
    "start": "1190330",
    "end": "1195640"
  },
  {
    "text": "Here is a picture. I drew a picture of what\npositive serial correlation",
    "start": "1195640",
    "end": "1201220"
  },
  {
    "text": "might look like. So here we have some positive\nerrors over on the left side.",
    "start": "1201220",
    "end": "1209510"
  },
  {
    "text": "And then if you have\none positive error, the sort of close-by errors\nare also likely to be positive.",
    "start": "1209510",
    "end": "1217429"
  },
  {
    "text": "So you have sort\nof an area where errors are mostly positive, and\nthen you get a negative error.",
    "start": "1217430",
    "end": "1222490"
  },
  {
    "text": "And then most of\nthe errors close by are likely to be negative. So this is what positive serial\ncorrelation might look like.",
    "start": "1222490",
    "end": "1230120"
  },
  {
    "text": "The errors are kind of\nclumped spatially like this. Again, this is not fatal.",
    "start": "1230120",
    "end": "1235669"
  },
  {
    "text": "We can deal with\nthis, but we're going to assume, for now,\nthat we don't have it. ",
    "start": "1235670",
    "end": "1243549"
  },
  {
    "text": "And then as I said,\nassumptions three through five could be subsumed under\na stronger assumption that epsilon i are iid\nnormal 0 sigma squared.",
    "start": "1243550",
    "end": "1253990"
  },
  {
    "text": "So we don't typically\nneed this assumption. Sometimes we impose it.",
    "start": "1253990",
    "end": "1259279"
  },
  {
    "text": "That just means that\nbasically, the error just has a little normal distribution\nthat looks like that.",
    "start": "1259280",
    "end": "1267010"
  },
  {
    "text": " So what are the\nthings that we can say",
    "start": "1267010",
    "end": "1272720"
  },
  {
    "text": "about this model first of all? What are some of the properties? Well, we can calculate what\nthe expectation of y is.",
    "start": "1272720",
    "end": "1281690"
  },
  {
    "text": "And I should say we\ncan think of this-- so right now, during\nthis calculation,",
    "start": "1281690",
    "end": "1287520"
  },
  {
    "text": "I'm making the assumption,\njust for my convenience, that the x's are nonstochastic.",
    "start": "1287520",
    "end": "1293090"
  },
  {
    "text": "And so I haven't said\nthat the expectation of y is conditional on\nx, but we can think of this as basically the\nconditional distribution of y",
    "start": "1293090",
    "end": "1301340"
  },
  {
    "text": "given x. And so, basically,\nif I'm assuming that the x's are\nnonstochastic, it",
    "start": "1301340",
    "end": "1307610"
  },
  {
    "text": "makes my life a\nlittle bit easier because I take the expectation\nof beta 0 plus beta 1",
    "start": "1307610",
    "end": "1313940"
  },
  {
    "text": "x sub i plus epsilon sub i,\nand the x's are nonstochastic, so they're just constants.",
    "start": "1313940",
    "end": "1320179"
  },
  {
    "text": "They come outside\nthe expectation, and it just makes the\ncalculations easy. And so we get, in particular,\nthat the expectation of y sub i",
    "start": "1320180",
    "end": "1328820"
  },
  {
    "text": "is equal to beta 0\nplus beta 1 x sub i.",
    "start": "1328820",
    "end": "1334220"
  },
  {
    "text": "We can do a similar calculation\nwith variance of y sub i, and we get that that is\njust equal to sigma squared.",
    "start": "1334220",
    "end": "1342899"
  },
  {
    "text": "So it's the same\nvariance as the error.",
    "start": "1342900",
    "end": "1348610"
  },
  {
    "text": "And then we can also do a\nsimilar calculation showing",
    "start": "1348610",
    "end": "1354309"
  },
  {
    "text": "that for any i and j,\nwhere i is not equal to j, the covariance between y sub i\nand y sub j is equal to zero.",
    "start": "1354310",
    "end": "1362725"
  },
  {
    "start": "1362725",
    "end": "1368059"
  },
  {
    "text": "Yeah, oh, and so I\njust want to emphasize, the betas that are parameters in\nthe conditional mean function.",
    "start": "1368060",
    "end": "1373805"
  },
  {
    "start": "1373805",
    "end": "1380570"
  },
  {
    "text": "So we've got this\nlinear function. We've got a set of\nassumptions that we're willing to impose for now.",
    "start": "1380570",
    "end": "1387260"
  },
  {
    "text": "So our next question is,\nhow do we find estimates for beta 0 and beta 1? Well, I'm just going to throw\nup three different options.",
    "start": "1387260",
    "end": "1397400"
  },
  {
    "text": "And I'll very quickly\ndismiss two of them but just so you know that\nthere are other things that you",
    "start": "1397400",
    "end": "1403190"
  },
  {
    "text": "can consider other than\nleast squares estimates. So one thing that\nwe could do is,",
    "start": "1403190",
    "end": "1409130"
  },
  {
    "text": "we could find the least\nsquares estimates. In other words, the values\nof beta 0 hat and beta 1 hat,",
    "start": "1409130",
    "end": "1418670"
  },
  {
    "text": "such that this expression\nhere is minimized.",
    "start": "1418670",
    "end": "1424220"
  },
  {
    "text": "And I'll show you\npictures of what these look like in a second. We could, instead of minimizing\nthe sum of squared errors,",
    "start": "1424220",
    "end": "1432929"
  },
  {
    "text": "which is what-- so each one of these in\nthe parentheses is a--",
    "start": "1432930",
    "end": "1442830"
  },
  {
    "text": "well, it's an error\nif beta 0 and beta 1 are the true parameters.",
    "start": "1442830",
    "end": "1448150"
  },
  {
    "text": "It's a residual if\nthey're the estimates. But what we do is, if we\nplug-in the estimates,",
    "start": "1448150",
    "end": "1455700"
  },
  {
    "text": "we call that a residual. And then if we\nsquare the residuals and sum them up and\nfind the parameters that",
    "start": "1455700",
    "end": "1462840"
  },
  {
    "text": "minimize that sum\nof squares, that's called a least\nsquares estimator. We, instead of squaring\nthem, we could just",
    "start": "1462840",
    "end": "1469680"
  },
  {
    "text": "take their absolute\nvalues instead and come up with something called the least\nabsolute deviations estimator.",
    "start": "1469680",
    "end": "1478260"
  },
  {
    "text": "That's the second possibility. Or a third possibility\nI'll throw out, is something called the reverse\nleast-squares estimator.",
    "start": "1478260",
    "end": "1487020"
  },
  {
    "text": "Let me just show you pictures\nof each of these in turn. So actually, the\nleast-squares estimator",
    "start": "1487020",
    "end": "1496530"
  },
  {
    "text": "and the least-absolute\ndeviations estimator have the same picture\nbecause what we're doing,",
    "start": "1496530",
    "end": "1501870"
  },
  {
    "text": "is for the least-squares\nestimators, we're just taking this\nquantity here and squaring it.",
    "start": "1501870",
    "end": "1508320"
  },
  {
    "text": "And for the\nleast-absolute deviations, we're not squaring it. We're just taking\nthe absolute value. So they have the same pictures.",
    "start": "1508320",
    "end": "1514840"
  },
  {
    "text": "But what we do is, for the\nleast-squares estimator, we compute all of\nthese deviations.",
    "start": "1514840",
    "end": "1523110"
  },
  {
    "text": "And we choose the\nline that minimizes those squared deviations.",
    "start": "1523110",
    "end": "1528180"
  },
  {
    "text": "Yes? AUDIENCE: So, I guess, the end\nis how did these two different [INAUDIBLE].",
    "start": "1528180",
    "end": "1534321"
  },
  {
    "text": "SARA ELLISON: So I'm going\nto talk just a little bit, I'm going to have a slide,\nmaybe two slides or something, about the properties of\nleast squares and why.",
    "start": "1534322",
    "end": "1541710"
  },
  {
    "text": "We almost always use\nleast squares, instead of least-absolute deviations. I'm not going to actually\nsay too much about them.",
    "start": "1541710",
    "end": "1549018"
  },
  {
    "text": "But when we get to\nthat slide, if you have any other\nquestions, let me know. AUDIENCE: OK. ",
    "start": "1549018",
    "end": "1555278"
  },
  {
    "text": "SARA ELLISON: So these are\nthe first two possibilities. And then the picture for\nthe third possibility just looks like this.",
    "start": "1555278",
    "end": "1560550"
  },
  {
    "text": "We're minimizing the sum of\nsquares of those deviations instead, in a reverse\nleast-squares estimator.",
    "start": "1560550",
    "end": "1569400"
  },
  {
    "text": "So why do we always just focus\non least-squares estimators? Why is that sort of\nwhat we always do?",
    "start": "1569400",
    "end": "1576830"
  },
  {
    "text": "Well, under the assumptions of\nthe classical linear-regression model that I had up\na few minutes ago,",
    "start": "1576830",
    "end": "1584629"
  },
  {
    "text": "OLS, which stands for\nOrdinary Least Squares, provides the minimum\nvariance, in other words,",
    "start": "1584630",
    "end": "1590550"
  },
  {
    "text": "the most efficient, unbiased\nestimator of beta 0 and beta 1. It is also the maximum\nlikelihood estimate",
    "start": "1590550",
    "end": "1598730"
  },
  {
    "text": "under normality of errors. And the estimates are consistent\nand asymptotically normal.",
    "start": "1598730",
    "end": "1605300"
  },
  {
    "text": "And these things are not\ntrue of the other estimators. So that's why we\ntypically always use the least-squares estimators.",
    "start": "1605300",
    "end": "1612650"
  },
  {
    "text": "Are there cases\nwhen you might want to use a least-absolute\ndeviations estimator? Maybe, I mean maybe if\nyou're particularly worried",
    "start": "1612650",
    "end": "1620389"
  },
  {
    "text": "about the credibility\nof your data, and you think you might have\nsome outliers that you don't want to take out\nof your data set,",
    "start": "1620390",
    "end": "1627380"
  },
  {
    "text": "but you don't want them\nto have undue influence on your parameter estimates. You might be able\nto justify using",
    "start": "1627380",
    "end": "1634410"
  },
  {
    "text": "a least-absolute\ndeviations estimator. But typically, people\nare going to go for the least-squares estimators\nbecause its properties are",
    "start": "1634410",
    "end": "1641610"
  },
  {
    "text": "so good. Yeah, oh, sorry, yeah, go ahead. AUDIENCE: And so is the reason\nthat that OLS is favorable,",
    "start": "1641610",
    "end": "1648929"
  },
  {
    "text": "even though they're both based\non the same distance, I guess, the sensitivity in\nthe OLS is higher",
    "start": "1648930",
    "end": "1657520"
  },
  {
    "text": "because it's just squared? SARA ELLISON: Yeah, it's more\nsensitive to observations",
    "start": "1657520",
    "end": "1663460"
  },
  {
    "text": "that are kind of out in the\ntails because it's squared. But even given that,\nit still has all",
    "start": "1663460",
    "end": "1669430"
  },
  {
    "text": "of these favorable properties. So you have a question? AUDIENCE: I guess\nnot really anymore.",
    "start": "1669430",
    "end": "1675320"
  },
  {
    "text": "I was sort of thinking the first\ncharacteristic is a little bit circular because we\ndefine efficiency",
    "start": "1675320",
    "end": "1680850"
  },
  {
    "text": "in terms of variance. So if you had a [INAUDIBLE]\npermutations here [INAUDIBLE] necessary [INAUDIBLE]. I guess what you're\nreally saying, is that it",
    "start": "1680850",
    "end": "1688072"
  },
  {
    "text": "provides the minimal barrier. SARA ELLISON: Exactly,\nyeah, so I just put in parentheses\n\"most efficient\"",
    "start": "1688072",
    "end": "1693120"
  },
  {
    "text": "to just remind you that\nthat's how we defined-- yeah.",
    "start": "1693120",
    "end": "1699150"
  },
  {
    "text": "That's the terminology we\nused basically, but yeah. Yep. AUDIENCE: Are theses\nproperties more or less--",
    "start": "1699150",
    "end": "1705930"
  },
  {
    "text": "homoskedasticity\ngoes out the window? SARA ELLISON: So good question.",
    "start": "1705930",
    "end": "1711370"
  },
  {
    "text": "I would have to think\nfor just a second. So basically, it's no\nlonger the most efficient.",
    "start": "1711370",
    "end": "1718260"
  },
  {
    "text": "It's still unbiased. It's no longer the\nmost efficient. The inference has to be--",
    "start": "1718260",
    "end": "1728640"
  },
  {
    "text": "under heteroskedasticity, the\nsort of standard inference that we'll see over the\nnext couple of lectures,",
    "start": "1728640",
    "end": "1735420"
  },
  {
    "text": "has to be modified because then\nthe standard errors, if you use the standard\ninference, are biased.",
    "start": "1735420",
    "end": "1742740"
  },
  {
    "text": "But it's still going to\nhave good properties. And we have very\ngood ways of fixing,",
    "start": "1742740",
    "end": "1749350"
  },
  {
    "text": "of recapturing the best\nproperties under the case of heteroskedasticity.",
    "start": "1749350",
    "end": "1755289"
  },
  {
    "start": "1755290",
    "end": "1761890"
  },
  {
    "text": "OK, so fine, maybe, I\nhope, I've convinced you that least-squares\nestimators are the way to go",
    "start": "1761890",
    "end": "1768857"
  },
  {
    "text": "and the way that\nwe're typically going to estimate this linear model. So we've got all of these\nsum of squared residuals.",
    "start": "1768857",
    "end": "1779600"
  },
  {
    "text": "And we're trying to choose the\nline that's minimizing those. Does that mean we have to\ndo a numerical minimization",
    "start": "1779600",
    "end": "1786010"
  },
  {
    "text": "every time we want to solve for\nour least-squares estimators? No, we don't.",
    "start": "1786010",
    "end": "1791920"
  },
  {
    "text": "We have these lovely\nclosed form solutions. So all we have to do is\nplug into these formulas",
    "start": "1791920",
    "end": "1798929"
  },
  {
    "text": "and realistically\nspeaking, all we have to do is type the command into R.",
    "start": "1798930",
    "end": "1808080"
  },
  {
    "text": "So anyhow, if you went on to\ndo more advanced econometrics,",
    "start": "1808080",
    "end": "1814890"
  },
  {
    "text": "you would, in fact,\nencounter estimators where you have to do complicated\nnumerical minimizations",
    "start": "1814890",
    "end": "1821070"
  },
  {
    "text": "every time to get\nparameter estimates. OLS is not one of them. We have these nice\nclosed-form solutions.",
    "start": "1821070",
    "end": "1827760"
  },
  {
    "text": "How did I get these? Pages of tedious\ncalculations that",
    "start": "1827760",
    "end": "1833250"
  },
  {
    "text": "are up on the website for\nyour viewing pleasure. So I have a couple\nthings to say about this.",
    "start": "1833250",
    "end": "1839860"
  },
  {
    "text": "First of all, I can't\nremember what I put it under.",
    "start": "1839860",
    "end": "1845670"
  },
  {
    "text": "I think I put in\nunder derivation. There's a page of notes under-- what's it under?",
    "start": "1845670",
    "end": "1851280"
  },
  {
    "text": "Resources, maybe,\nand it's called derivation of OLS estimators. And so you're welcome\nto look on those notes",
    "start": "1851280",
    "end": "1858570"
  },
  {
    "text": "and see the\ncalculations whereby I arrived at the\nleast-squares estimators.",
    "start": "1858570",
    "end": "1865650"
  },
  {
    "text": "The notation is slightly\ndifferent, I believe,",
    "start": "1865650",
    "end": "1870900"
  },
  {
    "text": "in those notes. Instead of having the\nintercept denoted by beta 0,",
    "start": "1870900",
    "end": "1878010"
  },
  {
    "text": "I think I used an alpha instead. And then a beta for the slope. And so the notation is\nslightly different, but anyhow,",
    "start": "1878010",
    "end": "1886409"
  },
  {
    "text": "you can look at\nthose if you want. However, I don't want\nyou to get the idea",
    "start": "1886410",
    "end": "1893040"
  },
  {
    "text": "that OLS estimators are\nhorrible complicated things. They're really very elegant,\nand they're very intuitive.",
    "start": "1893040",
    "end": "1899880"
  },
  {
    "text": "But the thing that's holding\nus back is this notation. So Ashley, next\ntime, we will derive",
    "start": "1899880",
    "end": "1908010"
  },
  {
    "text": "the least-squares estimators\nusing matrix notation, and you will see how\nbeautiful they are.",
    "start": "1908010",
    "end": "1914070"
  },
  {
    "text": "I'm not going to drag you\nthrough the derivation using the summation notation because\nit's just it's painful.",
    "start": "1914070",
    "end": "1920310"
  },
  {
    "text": "Yeah. AUDIENCE: When we go 1 over\nnc epsilon, why do you have",
    "start": "1920310",
    "end": "1925560"
  },
  {
    "text": "[INAUDIBLE]. SARA ELLISON: Doo, doo, doo,\ndoo, doo, oh, they would--",
    "start": "1925560",
    "end": "1931480"
  },
  {
    "text": "I think it's just because-- oh, yeah. I think it's just because the--",
    "start": "1931480",
    "end": "1937269"
  },
  {
    "text": "well, so that thing\non the bottom,",
    "start": "1937270",
    "end": "1942310"
  },
  {
    "text": "is like a sample variance. And so you might compute that\nseparately, and then just--",
    "start": "1942310",
    "end": "1949270"
  },
  {
    "text": "yeah. ",
    "start": "1949270",
    "end": "1954300"
  },
  {
    "text": "And as I just said, they\ncould be lovely or still if we weren't too afraid of\nusing matrix notation, which",
    "start": "1954300",
    "end": "1961049"
  },
  {
    "text": "we're not. So we'll do it next time. ",
    "start": "1961050",
    "end": "1966390"
  },
  {
    "text": "A couple of important\ndefinitions. So I already told you\njust sort of verbally,",
    "start": "1966390",
    "end": "1973620"
  },
  {
    "text": "what a residual is. But here's a\npicture of residual, and you have the formula here.",
    "start": "1973620",
    "end": "1979419"
  },
  {
    "text": "So residual is\nbasically, the deviation between an ordered pair\nof a particular x and y",
    "start": "1979420",
    "end": "1988380"
  },
  {
    "text": "and the fitted regression line. And we denote that as\nepsilon sub i hat, typically.",
    "start": "1988380",
    "end": "1999540"
  },
  {
    "text": "Sometimes we'll see\nother notation, as well. Here, I've drawn in the\nregression line, also known",
    "start": "1999540",
    "end": "2005540"
  },
  {
    "text": "as the fitted line. So that's just beta 0 hat\nplus beta 1 hat times x.",
    "start": "2005540",
    "end": "2013890"
  },
  {
    "text": "And then it's also convenient\nto define something called a fitted value. And that's basically, just\nfor any particular value",
    "start": "2013890",
    "end": "2021930"
  },
  {
    "text": "of x, the value of y on\nthe fitted line associated",
    "start": "2021930",
    "end": "2027420"
  },
  {
    "text": "with that value of x on the\nestimated regression line. So we'll come back.",
    "start": "2027420",
    "end": "2033070"
  },
  {
    "text": "We'll use this notation, the y\nsub i hat and the epsilon sub i hat notation, and\nwe'll come back",
    "start": "2033070",
    "end": "2039660"
  },
  {
    "text": "to these quantities in the\nnext couple of lectures. ",
    "start": "2039660",
    "end": "2051340"
  },
  {
    "text": "What do we always ask when we\nlearn about a new estimator? And why do we ask it?",
    "start": "2051340",
    "end": "2057379"
  },
  {
    "text": "What do we always want to\nknow about an estimator? AUDIENCE: [INAUDIBLE]",
    "start": "2057380",
    "end": "2062739"
  },
  {
    "text": "SARA ELLISON: Hmm? AUDIENCE: It's [INAUDIBLE]. SARA ELLISON: Well,\nthat's a specific case. So we want to know\nits distribution.",
    "start": "2062739",
    "end": "2068370"
  },
  {
    "text": "We always want to\nknow its distribution. And I already told you these\nare unbiased estimators,",
    "start": "2068370",
    "end": "2074370"
  },
  {
    "text": "but we want to know\nits distribution. And why do we want to\nknow its distribution? Because we're not going to\nbe able to perform inference.",
    "start": "2074370",
    "end": "2081060"
  },
  {
    "text": "We're not going to\nbe able to create a hypothesis test or\na confidence interval",
    "start": "2081060",
    "end": "2089050"
  },
  {
    "text": "if we don't know\nthe distribution, or we don't know, at least,\nthe variance of our estimators.",
    "start": "2089050",
    "end": "2096500"
  },
  {
    "text": "So let me just define\na couple of things. Let's let x bar just equal\nwhat it typically is equal,",
    "start": "2096500",
    "end": "2104060"
  },
  {
    "text": "just 1 over n times\nthe sum of the xi's. And then let's define this thing\ncalled sigma squared hat sub",
    "start": "2104060",
    "end": "2110630"
  },
  {
    "text": "x, which is 1 over n\ntimes the sum over i of xi minus x bar squared.",
    "start": "2110630",
    "end": "2117244"
  },
  {
    "text": " And if we use that\nnotation, then we",
    "start": "2117245",
    "end": "2128420"
  },
  {
    "text": "can figure out what the\nmean and the variance and the covariance of beta hat,\nbeta 0 hat, and beta 1 hat are.",
    "start": "2128420",
    "end": "2141299"
  },
  {
    "text": "And again, how did I get these? Pages of tedious calculations up\non the website for your viewing",
    "start": "2141300",
    "end": "2147150"
  },
  {
    "text": "pleasure. But you can skip them if\nyou want because as I said, we'll see a much nicer way to\ncome up with these formulas",
    "start": "2147150",
    "end": "2157530"
  },
  {
    "text": "next time. And then how did I\nget the mean actually?",
    "start": "2157530",
    "end": "2162599"
  },
  {
    "text": "I think the calculation\nof the mean of these is also up on the website.",
    "start": "2162600",
    "end": "2169570"
  },
  {
    "text": "But in fact, I didn't. I already told you they\nwere unbiased estimators. So we already knew that\nthe mean of beta hat",
    "start": "2169570",
    "end": "2175619"
  },
  {
    "text": "was equal to-- beta hat 0 was\nequal to beta 0 and same thing with beta 1.",
    "start": "2175620",
    "end": "2181320"
  },
  {
    "text": " So let me take a moment\nto talk about a little bit",
    "start": "2181320",
    "end": "2188910"
  },
  {
    "text": "of comparative statics. I think that this discussion\nwill give you a little bit more of a feel for the mechanics\nof linear regression",
    "start": "2188910",
    "end": "2197130"
  },
  {
    "text": "and how this thing works. So I list them here, and I'll\ngo through each one separately.",
    "start": "2197130",
    "end": "2206400"
  },
  {
    "text": "If we have a larger\nsigma squared, so imagine having two\ndifferent data sets,",
    "start": "2206400",
    "end": "2212250"
  },
  {
    "text": "they're identical\nexcept for one of them has a larger error\nvariance, how is that",
    "start": "2212250",
    "end": "2218550"
  },
  {
    "text": "going to affect our estimates? Well, you can just look at\nthe formulas for the variance",
    "start": "2218550",
    "end": "2227069"
  },
  {
    "text": "and the covariance and so\nforth of beta of our estimates,",
    "start": "2227070",
    "end": "2233040"
  },
  {
    "text": "and you can see that if we\nhave a larger sigma squared, that's going to mean that the\nvariance of beta hat is larger.",
    "start": "2233040",
    "end": "2241230"
  },
  {
    "text": "I'll show you a picture\nof that in a second. If we have a larger variance\nof x, so in other words,",
    "start": "2241230",
    "end": "2250040"
  },
  {
    "text": "our x's are more\nspread out, that's going to lead to a smaller\nvariance in beta hat,",
    "start": "2250040",
    "end": "2257440"
  },
  {
    "text": "and a larger n also means a\nsmaller variance of beta hat.",
    "start": "2257440",
    "end": "2262500"
  },
  {
    "text": "And then furthermore, if\nthe mean of x is positive,",
    "start": "2262500",
    "end": "2270990"
  },
  {
    "text": "then we're going to have\na negative covariance, a negative relationship between\nbeta 0 hat and beta 1 hat.",
    "start": "2270990",
    "end": "2280790"
  },
  {
    "text": "And I didn't write it down\nhere, but if x bar is negative, then that flips.",
    "start": "2280790",
    "end": "2287599"
  },
  {
    "text": "So let me show you\npictures of these, and that will make the\ncomparative statics a little clearer, I think.",
    "start": "2287600",
    "end": "2294410"
  },
  {
    "text": "Oh, and I should say, oh,\njust a point of notation, I'm often going to just start\nsliding into matrix and vector",
    "start": "2294410",
    "end": "2302210"
  },
  {
    "text": "notation. And when I say beta hat, I'm\ntalking about the vector beta 0",
    "start": "2302210",
    "end": "2308900"
  },
  {
    "text": "hat and beta 1 hat. ",
    "start": "2308900",
    "end": "2314109"
  },
  {
    "text": "So what's the first\ncomparative static? What does that suggest about\nthe mechanics of regression?",
    "start": "2314110",
    "end": "2321500"
  },
  {
    "text": "So we have two\nidentical data sets. The only difference is\nthat the error variance is greater than 1.",
    "start": "2321500",
    "end": "2327340"
  },
  {
    "text": "What does that mean? That in this case,\nwe're going to be less sure of our estimates. The variance of\nbeta hat is greater.",
    "start": "2327340",
    "end": "2334553"
  },
  {
    "text": "We're just going to have less\nconfidence in our estimates. That makes sense, right. If we have very small\nerror variances,",
    "start": "2334553",
    "end": "2342049"
  },
  {
    "text": "we're going to be\nable to estimate that linear relationship\nvery precisely. ",
    "start": "2342050",
    "end": "2349100"
  },
  {
    "text": "How about if we have the same\ndata sets, but in this case, the x's on the left\nare very spread out,",
    "start": "2349100",
    "end": "2356869"
  },
  {
    "text": "we have a lot of variation in\nthe x's and in the other case, the x's are all smushed\ntogether into one little area?",
    "start": "2356870",
    "end": "2365950"
  },
  {
    "text": "Well, in the case where\nthey're all smushed together, remember, in the\nlimit, we can't even estimate our linear\nregression coefficients.",
    "start": "2365950",
    "end": "2375147"
  },
  {
    "text": "Remember the picture I\nshowed you in the beginning where all of the observations\non x were on a single point.",
    "start": "2375147",
    "end": "2381440"
  },
  {
    "text": "We couldn't even estimate the\nlinear regression coefficients there. Here, we can estimate\nthem, but the variance",
    "start": "2381440",
    "end": "2388597"
  },
  {
    "text": "is going to be pretty big. We're going to be less sure\nof our estimates in this case because we don't have\na lot of variation",
    "start": "2388597",
    "end": "2395830"
  },
  {
    "text": "of x to help us identify the\neffect we're interested in. ",
    "start": "2395830",
    "end": "2403230"
  },
  {
    "text": "A larger n means a smaller\nvariance of beta hat. Well, I don't need to\ndraw a picture of this.",
    "start": "2403230",
    "end": "2410220"
  },
  {
    "text": "We'll just note that this\nfollows from the fact that beta hat are\nconsistent estimators.",
    "start": "2410220",
    "end": "2415320"
  },
  {
    "start": "2415320",
    "end": "2421200"
  },
  {
    "text": "And then finally, there is\nthis mechanical relationship between the two estimates.",
    "start": "2421200",
    "end": "2427029"
  },
  {
    "text": "So if we have for the mean\nof our x's is positive,",
    "start": "2427030",
    "end": "2433180"
  },
  {
    "text": "then basically,\nif we overestimate",
    "start": "2433180",
    "end": "2439859"
  },
  {
    "text": "the intercept of the\nline, we're going to underestimate the\nslope, or an overestimate",
    "start": "2439860",
    "end": "2445560"
  },
  {
    "text": "of the intercept, is associated\nwith an underestimate of the slope. An underestimate\nof the intercept,",
    "start": "2445560",
    "end": "2452160"
  },
  {
    "text": "is associated with an\noverestimate of the slope. So it's just this\nmechanical relationship. And then if x bar\nis negative, then",
    "start": "2452160",
    "end": "2460950"
  },
  {
    "text": "the mechanical relationship\nshifts, it'll flips. ",
    "start": "2460950",
    "end": "2468500"
  },
  {
    "text": "So one step further, if we\nuse the stronger assumption that the errors are iid normal 0\nsigma squared, then we obtain--",
    "start": "2468500",
    "end": "2479120"
  },
  {
    "text": "in addition, so we already\nhave the mean and the variance and the covariance of\nthese two estimates.",
    "start": "2479120",
    "end": "2485540"
  },
  {
    "text": "But we also have that they\nhave normal distributions if the errors have\nnormal distributions.",
    "start": "2485540",
    "end": "2493970"
  },
  {
    "text": "So if we're willing\nor if we feel like imposing that extra\nassumption of normality",
    "start": "2493970",
    "end": "2499849"
  },
  {
    "text": "of errors, then what\nwe get out of it is normality of our estimators.",
    "start": "2499850",
    "end": "2504890"
  },
  {
    "start": "2504890",
    "end": "2511250"
  },
  {
    "text": "One loose end, note that the\ndistributions of beta 0 hat",
    "start": "2511250",
    "end": "2518570"
  },
  {
    "text": "and beta 1 hat are\nfunctions of sigma squared. That is the error variance.",
    "start": "2518570",
    "end": "2524570"
  },
  {
    "text": "We often don't know\nthe error variance. Why would we know\nthe error variance? So we estimate it.",
    "start": "2524570",
    "end": "2531200"
  },
  {
    "text": "And we're going to use\na different estimator than estimators\nwe've seen before.",
    "start": "2531200",
    "end": "2537630"
  },
  {
    "text": "So we're going to use\nan estimator that's equal to 1 over n\nminus 2 times the sum",
    "start": "2537630",
    "end": "2545420"
  },
  {
    "text": "of the residual squared. ",
    "start": "2545420",
    "end": "2551470"
  },
  {
    "text": "And this estimator,\nit turns out, is unbiased for sigma\nsquared in the linear model.",
    "start": "2551470",
    "end": "2558170"
  },
  {
    "text": "That's why we use it. Why the minus 2 in\nthe denominator? Well, it's because we're\nestimating two parameters.",
    "start": "2558170",
    "end": "2565090"
  },
  {
    "text": "Do you remember our unbiased\nestimator for the variance when we were just estimating\nthe mean had an n minus 1",
    "start": "2565090",
    "end": "2573339"
  },
  {
    "text": "in the denominator? Well here we need the\nn minus 2 because we're estimating two parameters,\na beta 0 and a beta 1.",
    "start": "2573340",
    "end": "2580485"
  },
  {
    "text": "And it turns out that that's\nwhat we need for sigma squared to be unbiased. Yep. AUDIENCE: Can you\nremind us why that is,",
    "start": "2580485",
    "end": "2587290"
  },
  {
    "text": "or why we both need it? SARA ELLISON: So I\ndidn't do the proof. And to be honest, I don't feel\nlike I have a useful intuition",
    "start": "2587290",
    "end": "2597730"
  },
  {
    "text": "to tell you. I'm happy to give you the\nproof, and the proof of this",
    "start": "2597730",
    "end": "2603880"
  },
  {
    "text": "isn't much more complicated\nthan in the univariate case. But yeah, it just\nturns out that that's",
    "start": "2603880",
    "end": "2614330"
  },
  {
    "text": "what you need to do to\nhave an unbiased estimator of the variance. Do you have a useful intuition?",
    "start": "2614330",
    "end": "2620579"
  },
  {
    "text": "I don't know. No, OK. AUDIENCE: That applies if you\nhad to twist me three things.",
    "start": "2620580",
    "end": "2627190"
  },
  {
    "text": "[LAUGHTER] SARA ELLISON: Yes, yes, exactly. AUDIENCE: I don't\nknow why, but well--",
    "start": "2627190",
    "end": "2633725"
  },
  {
    "text": "SARA ELLISON: Mm-hmm, it does. ",
    "start": "2633725",
    "end": "2638900"
  },
  {
    "text": "So now I want to ask\nyou to think back to when we were doing\nunivariate inference,",
    "start": "2638900",
    "end": "2646880"
  },
  {
    "text": "and we replaced an\nunknown variance with an estimate\nof the variance.",
    "start": "2646880",
    "end": "2652500"
  },
  {
    "text": "So we had a sample mean,\nand the sample mean had a normal distribution, but\nwe didn't know the variance--",
    "start": "2652500",
    "end": "2661970"
  },
  {
    "text": "sorry, the standardized\nsample mean had a standard\nnormal distribution.",
    "start": "2661970",
    "end": "2668070"
  },
  {
    "text": "But we actually didn't\nknow what the variance was, and we replaced an\nestimate for the variance.",
    "start": "2668070",
    "end": "2674630"
  },
  {
    "text": "What happened? You guys remember? That's what happened,\nT distribution.",
    "start": "2674630",
    "end": "2683320"
  },
  {
    "text": "Well, same thing is\ngoing to happen here. I won't talk about\nt-tests in the context",
    "start": "2683320",
    "end": "2690820"
  },
  {
    "text": "of linear regression today. I should be able to\nget to them next time. But basically, just to give\nyou a little bit of a preview,",
    "start": "2690820",
    "end": "2697960"
  },
  {
    "text": "the same thing is\ngoing to happen here. We have, under the\nassumption of normal errors,",
    "start": "2697960",
    "end": "2703270"
  },
  {
    "text": "we have that beta\n0 hat and beta 1 hat have normal\ndistributions, but we don't",
    "start": "2703270",
    "end": "2708673"
  },
  {
    "text": "know what their variance is. We're going to have to use an\nestimated variance instead, typically. And so that's where the t-test\nin the linear regression",
    "start": "2708673",
    "end": "2717850"
  },
  {
    "text": "context comes from.  So now we have most of\nthe pieces the model,",
    "start": "2717850",
    "end": "2725859"
  },
  {
    "text": "the estimators, information\nabout the distribution of the estimators. We could proceed with inference.",
    "start": "2725860",
    "end": "2731890"
  },
  {
    "text": "But I'm going to put that\noff for a little while. And what we're going to do\nnow is take a quick detour",
    "start": "2731890",
    "end": "2739510"
  },
  {
    "text": "into something called\nanalysis of variance. And then I'm also going\nto talk about some sort of practical issues in\nestimating linear regressions",
    "start": "2739510",
    "end": "2748300"
  },
  {
    "text": "before we get to inference. We'll do inference next time. ",
    "start": "2748300",
    "end": "2754099"
  },
  {
    "text": "So we want some way, typically,\nto indicate how closely",
    "start": "2754100",
    "end": "2761000"
  },
  {
    "text": "associated x and y are. Or how much of y's variation\nis explained by x's variation.",
    "start": "2761000",
    "end": "2768830"
  },
  {
    "text": "And what we do is we perform\nan analysis of variance that I'll do in just a second.",
    "start": "2768830",
    "end": "2775020"
  },
  {
    "text": "And that's going to lead us to\na measure of goodness of fit for our linear regression.",
    "start": "2775020",
    "end": "2780660"
  },
  {
    "text": "And that's, typically, a useful\ngauge of how the regression is",
    "start": "2780660",
    "end": "2788780"
  },
  {
    "text": "doing in some sense. So let's start by defining\nthe sum of squared residuals.",
    "start": "2788780",
    "end": "2795529"
  },
  {
    "text": "So we've actually already seen\nthe sum of squared residuals. That's what we are minimizing\nchoosing beta 0 hat and beta 1",
    "start": "2795530",
    "end": "2803570"
  },
  {
    "text": "hat to minimize. Let's just define\nthis quantity here",
    "start": "2803570",
    "end": "2808940"
  },
  {
    "text": "as the sum of squared residuals.  And here's a picture of it.",
    "start": "2808940",
    "end": "2815350"
  },
  {
    "text": "So the picture can't indicate-- I mean, each one of these-- so what we do is, we\ncalculate each one of these,",
    "start": "2815350",
    "end": "2822510"
  },
  {
    "text": "we square it, and we\nadd them together. That's the sum of\nsquared residuals. ",
    "start": "2822510",
    "end": "2830100"
  },
  {
    "text": "And this is, in some sense,\na measure of goodness of fit. But it's not a very useful\nmeasure of goodness of fit",
    "start": "2830100",
    "end": "2837830"
  },
  {
    "text": "because it's not unit free,\nwhich is inconvenient. So let's say we have a data\nset and both of the variables",
    "start": "2837830",
    "end": "2845330"
  },
  {
    "text": "are measured in\npounds, British pounds.",
    "start": "2845330",
    "end": "2850530"
  },
  {
    "text": "And then we translate both\nof the variables to dollars.",
    "start": "2850530",
    "end": "2855980"
  },
  {
    "text": "Then the sum of squared\nresiduals will actually change. And that's not a very convenient\nfeature for a measure of fit",
    "start": "2855980",
    "end": "2861890"
  },
  {
    "text": "to have. But if we divide the\nsum of squared residuals",
    "start": "2861890",
    "end": "2868700"
  },
  {
    "text": "by some other sum of squares,\nthe total sum of squares, for instance, then that\ngives us a unit-free measure",
    "start": "2868700",
    "end": "2875690"
  },
  {
    "text": "because the units cancel. So what's the total\nsum of squares? Well, the total sum of\nsquares is we don't even fit a regression line.",
    "start": "2875690",
    "end": "2881900"
  },
  {
    "text": "We just take the\nsample mean of y",
    "start": "2881900",
    "end": "2887210"
  },
  {
    "text": "and just calculate all of the\ndeviations from the sample mean and square those up.",
    "start": "2887210",
    "end": "2892470"
  },
  {
    "text": "So graphically, what\ndoes that look like? So let's say this line is y bar.",
    "start": "2892470",
    "end": "2898290"
  },
  {
    "text": "So what we've done is we've\njust taken all of the deviations from y bar, squared\nthem, and sum them up.",
    "start": "2898290",
    "end": "2904900"
  },
  {
    "text": "That's the total sum of squares. So we've got the\nresidual sum of squares,",
    "start": "2904900",
    "end": "2911970"
  },
  {
    "text": "and we've got the\ntotal sum of squares. And note that that\nis going to be a unit-free measure\nof goodness of fit",
    "start": "2911970",
    "end": "2918690"
  },
  {
    "text": "that's always going\nto be between 0 and 1. Why is it always\nbetween 0 and 1?",
    "start": "2918690",
    "end": "2925770"
  },
  {
    "text": "Yep. AUDIENCE: [INAUDIBLE]\nthese are always-- right, so I guess the\nquestion is, actually, why is the SSR\nalways less than SSD?",
    "start": "2925770",
    "end": "2936930"
  },
  {
    "text": "Because they're both positive. SARA ELLISON: Yeah. AUDIENCE: Yeah, OK. SARA ELLISON: They're\nboth positive, yes. So you've got half of it right.",
    "start": "2936930",
    "end": "2942750"
  },
  {
    "text": "OK, Yep. AUDIENCE: Is this like it's\nthe maximum error you can have? SARA ELLISON: No,\nwell, no, I could draw",
    "start": "2942750",
    "end": "2951790"
  },
  {
    "text": "a line that gave me more error. But basically, you're\non the right track.",
    "start": "2951790",
    "end": "2960490"
  },
  {
    "text": "The SSR is the least\nerror you can have. Why is that? That's how we chose\nthe regression line.",
    "start": "2960490",
    "end": "2966820"
  },
  {
    "text": "To minimize that quantity. So any other line,\nit's going to give you",
    "start": "2966820",
    "end": "2973059"
  },
  {
    "text": "higher sum-of-squared errors. Anything else you do is\ngoing to give a higher sum-of-squared\nerrors than the SSR.",
    "start": "2973060",
    "end": "2981490"
  },
  {
    "text": "And so yeah, so\nyou got half of it. They're both non-negative\nby construction.",
    "start": "2981490",
    "end": "2987700"
  },
  {
    "text": "And the fact that the regression\nline is the least-squares line ensures that SSR is always\nless than or equal to SST.",
    "start": "2987700",
    "end": "2995230"
  },
  {
    "start": "2995230",
    "end": "3000520"
  },
  {
    "text": "So that could be our\ngoodness-of-fit measure. Actually, I guess we wanted a\ngoodness-of-fit measure that",
    "start": "3000520",
    "end": "3007270"
  },
  {
    "text": "had larger values when the fit\nwas better or explained more. So instead what we\ndecided is we define",
    "start": "3007270",
    "end": "3015700"
  },
  {
    "text": "our goodness-of-fit\nmeasure r squared to be equal to 1\nminus SSR over SST.",
    "start": "3015700",
    "end": "3021220"
  },
  {
    "text": "It's an arbitrary decision,\nbut that's what it is. ",
    "start": "3021220",
    "end": "3028650"
  },
  {
    "text": "And it turns out that\nthe total sum of squares can be decomposed\ninto two terms,",
    "start": "3028650",
    "end": "3033920"
  },
  {
    "text": "the residual sum of\nsquares and something called the model sum of squares. And so then r\nsquared, it could also",
    "start": "3033920",
    "end": "3042920"
  },
  {
    "text": "be written as just the\nmodel sum of squares over the total sum of squares.",
    "start": "3042920",
    "end": "3048200"
  },
  {
    "text": "Here's the model sum of squares,\nand here's a picture of it,",
    "start": "3048200",
    "end": "3054829"
  },
  {
    "text": "just the fitted\nvalue, the difference between the fitted\nvalue and y bar.",
    "start": "3054830",
    "end": "3061670"
  },
  {
    "start": "3061670",
    "end": "3067260"
  },
  {
    "text": "Oh, that's just something\nabout the decomposition. ",
    "start": "3067260",
    "end": "3072500"
  },
  {
    "text": "So we've got this\nthing called r squared, and in a bivariate regression,\nso in the regressions",
    "start": "3072500",
    "end": "3079850"
  },
  {
    "text": "we've seen so far, they just\nhave one explanatory variable, this r squared is actually\nequal to the sample correlation",
    "start": "3079850",
    "end": "3086690"
  },
  {
    "text": "coefficient for x and y. So we have, in fact,\nseen this measure before.",
    "start": "3086690",
    "end": "3092930"
  },
  {
    "text": "Why do we feel like\nwe have to define this new goodness-of-fit\nmeasure for linear regression?",
    "start": "3092930",
    "end": "3098090"
  },
  {
    "text": "Well, it is in fact, a\nmore general formulation, and it's defined\nfor linear models with more than one\nexplanatory variable.",
    "start": "3098090",
    "end": "3104850"
  },
  {
    "text": "So when we get to\nmultiple regression, we can have the same. We can define the r squared\nexactly the same way",
    "start": "3104850",
    "end": "3111320"
  },
  {
    "text": "as we just defined it.  And in addition\nto using r squared",
    "start": "3111320",
    "end": "3118310"
  },
  {
    "text": "as a basic measure of goodness\nof fit of our regression, we can also use it as the basis\nof a test of the hypothesis",
    "start": "3118310",
    "end": "3125600"
  },
  {
    "text": "that our slope is equal to zero. And when we get to\nmultiple regression,",
    "start": "3125600",
    "end": "3133369"
  },
  {
    "text": "it's going to be a test of\nthe hypothesis that beta 1-- that's sort of the slopes on\nall of our explanatory variables",
    "start": "3133370",
    "end": "3141170"
  },
  {
    "text": "are equal to zero if we have\nk explanatory variables. And so what exactly\nis this test?",
    "start": "3141170",
    "end": "3151490"
  },
  {
    "text": "Well, we create\nthis test statistic, which is just equal to n minus\n2 times r squared over 1 minus r",
    "start": "3151490",
    "end": "3160580"
  },
  {
    "text": "squared. And it turns out that this\nthing, under assumption",
    "start": "3160580",
    "end": "3165862"
  },
  {
    "text": "that the errors are\nnormally distributed, has an f distribution\nunder the null. And it's going to be large.",
    "start": "3165862",
    "end": "3176740"
  },
  {
    "text": "So we're going to reject\nthe null if it's large. So why does that make sense?",
    "start": "3176740",
    "end": "3182580"
  },
  {
    "text": "Is anyone willing to take a\nstab at explaining the intuition behind this?",
    "start": "3182580",
    "end": "3188470"
  },
  {
    "text": "No, OK. [LAUGHS] I'll take\na stab at it then. So basically, the\nidea is if our r",
    "start": "3188470",
    "end": "3196570"
  },
  {
    "text": "squared is large, that means\nthat the variation in x is explaining a lot\nof the variation in y.",
    "start": "3196570",
    "end": "3203440"
  },
  {
    "text": "We have a high measure\nfor our goodness of fit. And so this thing is\ngoing to be large.",
    "start": "3203440",
    "end": "3211900"
  },
  {
    "text": "This quantity is\ngoing to be large. And so we want to\nreject the null",
    "start": "3211900",
    "end": "3217390"
  },
  {
    "text": "that the coefficient on x,\nthis beta 1 is equal to zero.",
    "start": "3217390",
    "end": "3224170"
  },
  {
    "text": "If the x does a good job of\nexplaining-- the variation in x does a good job of explaining\nthe variation we're seeing",
    "start": "3224170",
    "end": "3230170"
  },
  {
    "text": "in y, then beta 1 isn't equal\nto 0, or with high probability,",
    "start": "3230170",
    "end": "3237460"
  },
  {
    "text": "we can reject the\nhypothesis that beta 1 is equal to 0 in that case. And so then we want to--",
    "start": "3237460",
    "end": "3246430"
  },
  {
    "text": "when this is large,\nthat's when we want to reject this hypothesis.",
    "start": "3246430",
    "end": "3252080"
  },
  {
    "text": "Does that make sense? Somewhat? We'll see the f test again.",
    "start": "3252080",
    "end": "3258910"
  },
  {
    "text": "AUDIENCE: If k represents number\nof variables or observations. SARA ELLISON: Sorry. AUDIENCE: And [INAUDIBLE]\nrepresents number of variables.",
    "start": "3258910",
    "end": "3265020"
  },
  {
    "text": "SARA ELLISON: No,\nnumber of observations, number of observations, yeah. ",
    "start": "3265020",
    "end": "3273550"
  },
  {
    "text": "So let's talk about a\nfew practical issues,",
    "start": "3273550",
    "end": "3278590"
  },
  {
    "text": "then we'll introduce\nmultiple regression, although, I don't think\nwe'll get to that this time.",
    "start": "3278590",
    "end": "3284170"
  },
  {
    "text": "And then we'll return\nto inference after that. ",
    "start": "3284170",
    "end": "3291490"
  },
  {
    "text": "So what does regression\noutput look like, and how do we interpret it? So first, let me show you some\nregression output from Stata.",
    "start": "3291490",
    "end": "3301000"
  },
  {
    "text": "So I know that you\nguys are using R, and I'll show you some\nR output in a second. But most of the\nregression output",
    "start": "3301000",
    "end": "3308080"
  },
  {
    "text": "I have sitting on my computer\nhappens to be in Stata, and it's probably good for\nyou to see the elements",
    "start": "3308080",
    "end": "3313180"
  },
  {
    "text": "in different\nstatistical packages and be able to figure out what\nall of these numbers mean.",
    "start": "3313180",
    "end": "3320050"
  },
  {
    "text": " So here in this\ndata output, we have",
    "start": "3320050",
    "end": "3325940"
  },
  {
    "text": "beta 1 hat is just listed\nunder this column that says coefficient.",
    "start": "3325940",
    "end": "3331819"
  },
  {
    "text": "And then beta 0 hat\nis at the bottom. Stata always puts the\nestimated constant beta 0 hat",
    "start": "3331820",
    "end": "3341420"
  },
  {
    "text": "at the bottom. Right after the\nestimated coefficients",
    "start": "3341420",
    "end": "3347540"
  },
  {
    "text": "for beta 1 and beta 0, we have\nthe standard errors listed.",
    "start": "3347540",
    "end": "3356630"
  },
  {
    "text": "So those are the estimated\nstandard deviations of the distributions\nof those estimators.",
    "start": "3356630",
    "end": "3364931"
  },
  {
    "text": "AUDIENCE: Where [INAUDIBLE]? ",
    "start": "3364931",
    "end": "3372140"
  },
  {
    "text": "SARA ELLISON: So I was the\none who gave lhd3rev its name.",
    "start": "3372140",
    "end": "3378230"
  },
  {
    "text": "And that's just a variable. I could tell you what's\nin this regression. This is basically a regression\ntaken from one of my papers",
    "start": "3378230",
    "end": "3385640"
  },
  {
    "text": "where we were trying to\nexplain the ratio of detail,",
    "start": "3385640",
    "end": "3393710"
  },
  {
    "text": "advertising, and\npharmaceuticals to sales trying to explain that ratio with\ncharacteristics of the market,",
    "start": "3393710",
    "end": "3403039"
  },
  {
    "text": "like revenue. So that's one of our\nmeasures of revenue. So that's a crazy\nname for a variable.",
    "start": "3403040",
    "end": "3409970"
  },
  {
    "text": "We called it something\ndifferent in the paper. But in my sort of\ncrazy state of code,",
    "start": "3409970",
    "end": "3415820"
  },
  {
    "text": "that's what it ended up being. The _cons, that's a\nname given by Stata.",
    "start": "3415820",
    "end": "3422630"
  },
  {
    "text": "So that's basically\njust saying, that's the estimator for the\nconstant, or the intercept in the regression.",
    "start": "3422630",
    "end": "3428930"
  },
  {
    "text": " So the standard errors\nare listed there.",
    "start": "3428930",
    "end": "3435220"
  },
  {
    "text": "And then we'll get to the-- I'll point out the\nt-test in a second.",
    "start": "3435220",
    "end": "3442530"
  },
  {
    "text": "But here are the results for\nthe f test that I mentioned.",
    "start": "3442530",
    "end": "3448227"
  },
  {
    "text": "Let me just say something. So typically,\nstatistical packages, when they give you the\noutput for a regression,",
    "start": "3448227",
    "end": "3455220"
  },
  {
    "text": "give you a bunch\nof stuff for free. And one of the things\nthey give you for free is this standard f\ntest I just mentioned.",
    "start": "3455220",
    "end": "3461730"
  },
  {
    "text": "So they'll just run\nit for you for free, and they'll report the results. And here are the results.",
    "start": "3461730",
    "end": "3468660"
  },
  {
    "text": "Let me take this question,\nand then I'll explain it. Yeah. AUDIENCE: So when\nyou're describing [INAUDIBLE],, what's the\nconvention for how much of this",
    "start": "3468660",
    "end": "3477290"
  },
  {
    "text": "you share? What if-- SARA ELLISON: Oh, I'm\nso glad you asked, and I will talk\nabout that next time.",
    "start": "3477290",
    "end": "3482640"
  },
  {
    "text": "So I'll have examples of\ntables that I want your--",
    "start": "3482640",
    "end": "3489299"
  },
  {
    "text": "well, they're examples\nof tables from my papers. But they're also\nexamples of tables that I want the tables in your\nempirical project to look like.",
    "start": "3489300",
    "end": "3498830"
  },
  {
    "text": "And yeah, it will be clear\nnext time so good question.",
    "start": "3498830",
    "end": "3504080"
  },
  {
    "text": "So here are the results\nof the standard f test. And what this particular\nf test says, remember,",
    "start": "3504080",
    "end": "3511160"
  },
  {
    "text": "we want to reject for large\nvalues of the test statistic.",
    "start": "3511160",
    "end": "3517220"
  },
  {
    "text": "We want to reject the\nnull that beta 1 is equal to zero if we get\nlarge values of that f test.",
    "start": "3517220",
    "end": "3524450"
  },
  {
    "text": "So they perform the f test, and\nthey also gave us the p value.",
    "start": "3524450",
    "end": "3530220"
  },
  {
    "text": "So last time, Esther\ndefined with a p value is that's basically\nthe probability that--",
    "start": "3530220",
    "end": "3536519"
  },
  {
    "text": "I hope I don't screw up\nthis verbal definition. It's the probability\nthat it's the point",
    "start": "3536520",
    "end": "3543540"
  },
  {
    "text": "at which if you were\ndoing a size p test, you would reject the null.",
    "start": "3543540",
    "end": "3549900"
  },
  {
    "text": "So what size test\ndo we usually do? We do 5% tests. We do 1% tests,\nthings like that.",
    "start": "3549900",
    "end": "3556799"
  },
  {
    "text": "Here, we have to\ndo a test as large as basically an 18% test in\norder to reject the null.",
    "start": "3556800",
    "end": "3564580"
  },
  {
    "text": "We're never going\nto do an 18% test, so we don't reject\nthe null here. So basically, if\nyou looked up 1.9,",
    "start": "3564580",
    "end": "3573519"
  },
  {
    "text": "which is the value\nof our f statistic for this particular\nregression, if you looked up",
    "start": "3573520",
    "end": "3579750"
  },
  {
    "text": "the critical values\nin an f table, you would conclude\nthat you don't",
    "start": "3579750",
    "end": "3585300"
  },
  {
    "text": "want to reject the\nnull under this case. And that's exactly what\nthe results here tell you.",
    "start": "3585300",
    "end": "3591000"
  },
  {
    "text": "They say, unless you're choosing\na ridiculously high alpha",
    "start": "3591000",
    "end": "3597810"
  },
  {
    "text": "for your test, you don't\nwant to reject the null. ",
    "start": "3597810",
    "end": "3603840"
  },
  {
    "text": "How about this one, though? For this one, we\nwould reject the null",
    "start": "3603840",
    "end": "3608970"
  },
  {
    "text": "that beta 1 is equal to zero\nif we were doing a 5% test.",
    "start": "3608970",
    "end": "3615390"
  },
  {
    "text": "If we were doing a 1% test,\nwe would not reject the null. So basically, what this\nf test is telling us,",
    "start": "3615390",
    "end": "3623009"
  },
  {
    "text": "is that if we want\nto do a three--",
    "start": "3623010",
    "end": "3628470"
  },
  {
    "text": "this is the p value. The p value tells\nus the boundary of the size of the test\nat which we would reject.",
    "start": "3628470",
    "end": "3636240"
  },
  {
    "text": " Is that clear?",
    "start": "3636240",
    "end": "3642650"
  },
  {
    "text": "And here-- we also get\nthis for free every time we run a regression --they\nperform a t-test for basically,",
    "start": "3642650",
    "end": "3652220"
  },
  {
    "text": "the hypothesis that that\nparticular coefficient is equal to zero.",
    "start": "3652220",
    "end": "3657380"
  },
  {
    "text": "And we'll talk about-- I'll make more precise next time\nexactly what these tests are.",
    "start": "3657380",
    "end": "3665150"
  },
  {
    "text": "But this is just a t-test\nfor each coefficient",
    "start": "3665150",
    "end": "3671569"
  },
  {
    "text": "being equal to zero. And in this case, we don't\nreject the null for either one.",
    "start": "3671570",
    "end": "3680030"
  },
  {
    "text": "I guess, down here, we reject\nthe null for this one at 5%.",
    "start": "3680030",
    "end": "3686390"
  },
  {
    "start": "3686390",
    "end": "3692569"
  },
  {
    "text": "So here are some R output\ncourtesy of Angela, who was up late last night\nrunning some regressions.",
    "start": "3692570",
    "end": "3701580"
  },
  {
    "text": "So it looks a little different. It has most of the\nsame elements in it. So here, instead of\ncalling it the constant,",
    "start": "3701580",
    "end": "3709550"
  },
  {
    "text": "they call it the intercept. And they put it first,\ninstead of second. This is the variable name\nthat Angela came up with.",
    "start": "3709550",
    "end": "3721250"
  },
  {
    "text": "So again, that's sort of your\nchoice when you're programming. You call your variables\nwhat you want.",
    "start": "3721250",
    "end": "3728010"
  },
  {
    "text": "And you also get\nthe standard errors, and you get the\nt-tests for free.",
    "start": "3728010",
    "end": "3735800"
  },
  {
    "text": "The f test here,\ninstead of being listed in the upper-right corner,\nlike it is in this data output,",
    "start": "3735800",
    "end": "3741890"
  },
  {
    "text": "it's listed at the bottom,\nbut it's the same information. It's basically gives you the\nvalue of the f statistic,",
    "start": "3741890",
    "end": "3748530"
  },
  {
    "text": "and then tells you\nthe p value at which-- the size of the test at which\nyou would reject the null.",
    "start": "3748530",
    "end": "3756380"
  },
  {
    "text": "And here, this is a very small p\nvalue, so any traditional test,",
    "start": "3756380",
    "end": "3761480"
  },
  {
    "text": "you would reject the null that\nbeta 1 is equal to 0 here. ",
    "start": "3761480",
    "end": "3768890"
  },
  {
    "text": "And then you can even-- well, let me just go back\nfor a second and note",
    "start": "3768890",
    "end": "3777080"
  },
  {
    "text": "that the r- squared of\nthis regression is 0.41.",
    "start": "3777080",
    "end": "3783260"
  },
  {
    "text": "And if you're wondering\nwhat a 0.41 r squared might look like\nin a scatter plot,",
    "start": "3783260",
    "end": "3789180"
  },
  {
    "text": "so this is a scatter\nplot of the variables and the fitted-regression\nline through them.",
    "start": "3789180",
    "end": "3795080"
  },
  {
    "text": " And if you're curious about\nthe particular variables,",
    "start": "3795080",
    "end": "3801600"
  },
  {
    "text": "this is just these are variables\nfrom the General Social Survey about\nattitudes on abortion",
    "start": "3801600",
    "end": "3807410"
  },
  {
    "text": "and how they have been evolving\nover the past couple of decades in the US.",
    "start": "3807410",
    "end": "3812525"
  },
  {
    "text": " OK, questions? ",
    "start": "3812525",
    "end": "3820780"
  },
  {
    "text": "So how do we interpret\nthese parameter estimates, and in particular, how do\nwe interpret the estimated",
    "start": "3820780",
    "end": "3829420"
  },
  {
    "text": "coefficient on our x variable,\nthis sort of beta 1 hat?",
    "start": "3829420",
    "end": "3834549"
  },
  {
    "text": "Well, beta 1 hat is\nthe estimated effect of y of a one unit\nincrease in x.",
    "start": "3834550",
    "end": "3841820"
  },
  {
    "text": "So the precise nuances\nof the interpretation, are going to depend a little bit\non whether we think that we've",
    "start": "3841820",
    "end": "3848530"
  },
  {
    "text": "estimated a causal relationship\nor whether we think we've estimated something else. So the language you might\nuse could be a little bit",
    "start": "3848530",
    "end": "3855880"
  },
  {
    "text": "different, but this\nis the basic idea. This is how we interpret\nwhat beta 1 hat means.",
    "start": "3855880",
    "end": "3863819"
  },
  {
    "text": "So let me show you-- these are some data that I\ndownloaded from, let's see,",
    "start": "3863820",
    "end": "3870540"
  },
  {
    "text": "the ESPN website. I did this several years ago. So this is data involving the\n2005 Major League Baseball",
    "start": "3870540",
    "end": "3879119"
  },
  {
    "text": "season. And what I did is I downloaded\nthe number of wins of each team",
    "start": "3879120",
    "end": "3885900"
  },
  {
    "text": "and the attendance, the\ncomplete season-long attendance",
    "start": "3885900",
    "end": "3891599"
  },
  {
    "text": "for each team. And wanted to see if there\nwas some relationship between attendance\nand number of wins.",
    "start": "3891600",
    "end": "3900940"
  },
  {
    "text": "So what did I do? I regressed attendance, that's\nmy dependent variable, my y",
    "start": "3900940",
    "end": "3907420"
  },
  {
    "text": "variable, on the number of\nwins that each team had. And we see here based on\nthis regression, that one",
    "start": "3907420",
    "end": "3913750"
  },
  {
    "text": "additional win is associated--\noh, and I should say, the attendance was in thousands. That was the unit I used.",
    "start": "3913750",
    "end": "3919490"
  },
  {
    "text": "So one additional\nwin is associated with an additional\n31,000 fans in attendance",
    "start": "3919490",
    "end": "3925119"
  },
  {
    "text": "over the course of the season. So we might want to\nexercise a little caution",
    "start": "3925120",
    "end": "3932410"
  },
  {
    "text": "to think that this is\na causal relationship. We'll get to that more\nlater in the semester. But at least, sort\nof this regression",
    "start": "3932410",
    "end": "3941320"
  },
  {
    "text": "suggests that an additional\nwin is associated with 31,000 fans in attendance,\nan additional 31,000",
    "start": "3941320",
    "end": "3949510"
  },
  {
    "text": "fans in attendance. ",
    "start": "3949510",
    "end": "3957680"
  },
  {
    "text": "Another practical\nissue, what if x only takes on two values, 0 and 1?",
    "start": "3957680",
    "end": "3964418"
  },
  {
    "text": "Well, we have a special name\nfor that type of variable. We call it a dummy\nvariable, or sometimes we call it an indicator variable.",
    "start": "3964418",
    "end": "3971240"
  },
  {
    "text": "And there's no problem\nat all in that. There's nothing in our\nassumptions that rule this out.",
    "start": "3971240",
    "end": "3978650"
  },
  {
    "text": "So that is not a problem for\nestimating a linear regression.",
    "start": "3978650",
    "end": "3987079"
  },
  {
    "text": "And in fact, dummy variables\ncan be quite useful. The pictures will look\na little different.",
    "start": "3987080",
    "end": "3992130"
  },
  {
    "text": "I'll show you a\npicture in a second. Here's what I mean. If our x variable only\ntakes on values 0 and 1,",
    "start": "3992130",
    "end": "3999710"
  },
  {
    "text": "then we don't have this\nnice cloud of data. We just have these two.",
    "start": "3999710",
    "end": "4004720"
  },
  {
    "text": "But nothing wrong with that. And then we estimate\nthen, sort of beta 0 hat",
    "start": "4004720",
    "end": "4011500"
  },
  {
    "text": "is interpreted as the mean\nof the group of observations",
    "start": "4011500",
    "end": "4019060"
  },
  {
    "text": "that have the dummy\nvariable equaling 0. And beta 1 hat is the\nincrement in the mean",
    "start": "4019060",
    "end": "4027760"
  },
  {
    "text": "or the effect of having\nthe dummy variable equal 1. ",
    "start": "4027760",
    "end": "4034910"
  },
  {
    "text": "So dummy variables\nserve a number of important roles\nin linear models. And we've actually\nseen one already,",
    "start": "4034910",
    "end": "4042440"
  },
  {
    "text": "randomized controlled trials. So when Esther was talking about\nhaving two groups, a treatment",
    "start": "4042440",
    "end": "4047990"
  },
  {
    "text": "and control group,\nwe could just simply have done the whole analysis\nin the linear regression",
    "start": "4047990",
    "end": "4053570"
  },
  {
    "text": "framework assigning a 0 to\none of the groups and a 1 to the other group.",
    "start": "4053570",
    "end": "4060050"
  },
  {
    "text": "So suppose we have some\ntreatment whose effect we're interested. We randomly assign a treatment\nto half of the observations.",
    "start": "4060050",
    "end": "4066440"
  },
  {
    "text": "Leave the other half untreated. We assign the treated\nobservations x equals 1 and the untreated x equals 0.",
    "start": "4066440",
    "end": "4072590"
  },
  {
    "text": " And then if we estimate\nthe regression above,",
    "start": "4072590",
    "end": "4078350"
  },
  {
    "text": "beta 1 hat will be the estimated\neffect of the treatment. ",
    "start": "4078350",
    "end": "4085920"
  },
  {
    "text": "And by the way, x need\nnot be randomly assigned half zeros and half\nones or anything",
    "start": "4085920",
    "end": "4091670"
  },
  {
    "text": "like that to be\na dummy variable. So we do use dummy variables\nor can use dummy variables",
    "start": "4091670",
    "end": "4097620"
  },
  {
    "text": "if we do have this sort of\nrandom assignment variable. But we can use it for\nlots of other things too.",
    "start": "4097620",
    "end": "4104089"
  },
  {
    "text": "Any characteristic\nof the observations that exists on some but\nnot all observations,",
    "start": "4104090",
    "end": "4109759"
  },
  {
    "text": "can be represented\nwith a dummy variable. So there's nothing\nabout a dummy variable",
    "start": "4109760",
    "end": "4114990"
  },
  {
    "text": "that we're assuming is randomly\nassigned or anything like that. And we'll see lots of other\nuses of dummy variables",
    "start": "4114990",
    "end": "4122120"
  },
  {
    "text": "when we get to\nmultiple regression because we can use\nthem to interact",
    "start": "4122120",
    "end": "4129380"
  },
  {
    "text": "with other explanatory variables\nand shift slopes around and trace out non-linear\nrelationships.",
    "start": "4129380",
    "end": "4137149"
  },
  {
    "text": "We can do all kinds of tricky\nthings with dummy variables, and we'll see that later. ",
    "start": "4137149",
    "end": "4145850"
  },
  {
    "text": "Another practical\nissue that we'll deal with in more detail\nin multiple regression,",
    "start": "4145850",
    "end": "4154830"
  },
  {
    "text": "you might be\nthinking, well, fine, the linear model\nseems pretty useful.",
    "start": "4154830",
    "end": "4160830"
  },
  {
    "text": "We can use it for\nall kinds of stuff. But isn't it really restrictive?",
    "start": "4160830",
    "end": "4165899"
  },
  {
    "text": "I mean, maybe the relationship\nbetween these variables, there is a relationship,\nbut it's just not linear.",
    "start": "4165899",
    "end": "4171599"
  },
  {
    "text": "And so we're imposing\nlinearity on here. And that seems like something\nwe wouldn't often want to do.",
    "start": "4171600",
    "end": "4179630"
  },
  {
    "text": "Well, there are a couple of\nthings I can say about this. One, is that the linear model\nis actually super flexible.",
    "start": "4179630",
    "end": "4187309"
  },
  {
    "text": "So for instance, we can\ntake non-linear functions",
    "start": "4187310",
    "end": "4194028"
  },
  {
    "text": "of both of the variables,\nx variable and y variable, and analyze the\nrelationships between",
    "start": "4194029",
    "end": "4203930"
  },
  {
    "text": "those non-linear\nfunctions of x and y within a linear framework. We can also create\ninteraction variables,",
    "start": "4203930",
    "end": "4213550"
  },
  {
    "text": "which is the product\nof two variables. And that allows for other\ntypes of non-linearity.",
    "start": "4213550",
    "end": "4221520"
  },
  {
    "text": "And we can do all of this. And like I said, we\ncan use dummy variables to allow for non-linear shapes\nby tricky multiplication.",
    "start": "4221520",
    "end": "4235469"
  },
  {
    "text": "We'll show you examples of that. But the point I\nwant to make now, is just that this is actually--\nit seems like it might",
    "start": "4235470",
    "end": "4242205"
  },
  {
    "text": "be a restrictive framework. It's actually a super\nflexible framework. And we'll see all\nkinds of examples",
    "start": "4242205",
    "end": "4248220"
  },
  {
    "text": "about how flexible the\nlinear framework actually is. One additional comment I\nwant to make about this,",
    "start": "4248220",
    "end": "4256050"
  },
  {
    "text": "is that we can do the\nnon-parametric version of a linear\nregression that Esther",
    "start": "4256050",
    "end": "4261840"
  },
  {
    "text": "talked about last time,\na kernel regression. And researchers do perform.",
    "start": "4261840",
    "end": "4270120"
  },
  {
    "text": "They do estimate\nkernel regressions, and they're useful in\na lot of situations.",
    "start": "4270120",
    "end": "4276030"
  },
  {
    "text": "But there are\nserious trade offs. And there are very good reasons\nwhy the linear regression",
    "start": "4276030",
    "end": "4281460"
  },
  {
    "text": "is the workhorse that it is. And the trade-offs mostly come\nin the form of efficiency.",
    "start": "4281460",
    "end": "4288520"
  },
  {
    "text": "A linear regression is going\nto be a much, much more efficient way to estimate a\nrelationship than a kernel",
    "start": "4288520",
    "end": "4295380"
  },
  {
    "text": "regression. So the kernel\nregression is sort of infinitely flexible\nin some sense.",
    "start": "4295380",
    "end": "4300660"
  },
  {
    "text": "But you pay for it because\nit's not an efficient estimator at all, really.",
    "start": "4300660",
    "end": "4306045"
  },
  {
    "start": "4306045",
    "end": "4312080"
  },
  {
    "text": "So questions about\nthose practical issues",
    "start": "4312080",
    "end": "4319670"
  },
  {
    "text": "concerning linear regression? So let me just get started.",
    "start": "4319670",
    "end": "4325500"
  },
  {
    "text": "I have a few minutes now. And we'll get started\non a generalization",
    "start": "4325500",
    "end": "4332030"
  },
  {
    "text": "of the linear model, the sort\nof multivariate linear model.",
    "start": "4332030",
    "end": "4337699"
  },
  {
    "text": "And this is what it looks like. So why might we be interested in\nthis more general linear model?",
    "start": "4337700",
    "end": "4350590"
  },
  {
    "text": "I mean, maybe the answer\nis kind of obvious. But I could talk\nabout a lot of--",
    "start": "4350590",
    "end": "4356890"
  },
  {
    "text": "we could talk about\na lot of examples where we're primarily interested\nin the relationship between one",
    "start": "4356890",
    "end": "4367929"
  },
  {
    "text": "particular x variable and\nour dependent or y variable, but there are other\nfactors that might",
    "start": "4367930",
    "end": "4375280"
  },
  {
    "text": "come into play that we need to\naccount for in our analysis. So that's one very\ngood reason to consider",
    "start": "4375280",
    "end": "4384160"
  },
  {
    "text": "this multivariate\nlinear model, as opposed to just the bivariate\nlinear model.",
    "start": "4384160",
    "end": "4389739"
  },
  {
    "text": "It could also be that we\nactually have many variables.",
    "start": "4389740",
    "end": "4398260"
  },
  {
    "text": "We don't have any\na priori notion about what x variables might\nbe important predictors of y.",
    "start": "4398260",
    "end": "4407230"
  },
  {
    "text": "So think back to the\nexample of a firm trying to decide who to serve ads\non that Moby Dick T-shirt to.",
    "start": "4407230",
    "end": "4419900"
  },
  {
    "text": "Well, they might not have\nany sort of a priori reason to think, well, it's\npeople who read XKCD",
    "start": "4419900",
    "end": "4426770"
  },
  {
    "text": "who might buy this t-shirt. And what they\nwould like to do is get the web browsing behavior\nof lots and lots of people",
    "start": "4426770",
    "end": "4435620"
  },
  {
    "text": "and see all kinds of\nwebsites that they visit, figure out which ones\nactually buy this t-shirt,",
    "start": "4435620",
    "end": "4442490"
  },
  {
    "text": "and then use the\nresults of that analysis to figure out who to serve\nads to in the future.",
    "start": "4442490",
    "end": "4450770"
  },
  {
    "text": "So that's another reason\nwhy we want to consider models that have lots of--",
    "start": "4450770",
    "end": "4455810"
  },
  {
    "text": "potentially, lots of regressors\nin them, not just one. So as I said\nbefore, the notation",
    "start": "4455810",
    "end": "4463780"
  },
  {
    "text": "we were using for the\nbivariate regression it was just sort of\nnot up to the task.",
    "start": "4463780",
    "end": "4469090"
  },
  {
    "text": "It was kind of clunky. It was a little awkward. And it turns out that\nall of the things that I",
    "start": "4469090",
    "end": "4478449"
  },
  {
    "text": "can say about linear regression\ncan be said much more elegantly using matrix notation.",
    "start": "4478450",
    "end": "4484840"
  },
  {
    "text": "So what we're going to do-- by the way, maybe I should\ntake a show of hands. How many people are familiar\nwith some basic matrix notation",
    "start": "4484840",
    "end": "4494170"
  },
  {
    "text": "and linear algebra facts?  Yes, OK, so there might be\na couple who aren't, but it",
    "start": "4494170",
    "end": "4502219"
  },
  {
    "text": "seems like the vast\nmajority of the class is. So I have put up on\nthe website, also",
    "start": "4502220",
    "end": "4509360"
  },
  {
    "text": "under Resources, a\nseparate handout. And I forget what it's called. Maybe it's called matrix\nnotation et cetera or something",
    "start": "4509360",
    "end": "4517010"
  },
  {
    "text": "like that. And basically, it goes through\nin greater detail than I'm going to do in lecture.",
    "start": "4517010",
    "end": "4524239"
  },
  {
    "text": "It gives you both a\nprimer for matrix notation and reminds you definitions,\nmatrix algebra, linear algebra",
    "start": "4524240",
    "end": "4534110"
  },
  {
    "text": "definitions. Reminds you of those. But then also goes through\nthe linear regression",
    "start": "4534110",
    "end": "4543260"
  },
  {
    "text": "stuff in more detail than\nI'm going to do in lecture. So you can look at\nthose notes if you want.",
    "start": "4543260",
    "end": "4549650"
  },
  {
    "text": "So this is a job\nfor matrix notation. We don't have too much time,\nbut I'll just get started here.",
    "start": "4549650",
    "end": "4555740"
  },
  {
    "text": "Let's replace this whole parade\nof x's with a vector, a row",
    "start": "4555740",
    "end": "4563460"
  },
  {
    "text": "vector. And let's say,\nlet's define a new x",
    "start": "4563460",
    "end": "4570480"
  },
  {
    "text": "that's identically equal to 1. And that's called x sub 0 1.",
    "start": "4570480",
    "end": "4576330"
  },
  {
    "text": "And the reason we do this\nis because, implicitly in the bivariate regression,\nwe had beta 0 times 1.",
    "start": "4576330",
    "end": "4584730"
  },
  {
    "text": "We never had the 1\nthere, but we sort of implicitly there was a\n1 multiplying beta 0.",
    "start": "4584730",
    "end": "4591210"
  },
  {
    "text": "And then we had beta\n1 times x sub i. Well here, we want to explicitly\nhave the 1 in that first spot.",
    "start": "4591210",
    "end": "4600040"
  },
  {
    "text": "So we can do everything\nin matrix notation. And so let's replace this\nwhole parade of x's with a row",
    "start": "4600040",
    "end": "4610710"
  },
  {
    "text": "vector. And so for each observation,\nwe have a vector of different measures\nx0 up through x sub k",
    "start": "4610710",
    "end": "4620790"
  },
  {
    "text": "for each observation. And then let's also replace\nall of the betas that",
    "start": "4620790",
    "end": "4629190"
  },
  {
    "text": "multiply all of these\nregressors by a vector. In this case, a column\nvector, a k plus 1",
    "start": "4629190",
    "end": "4637770"
  },
  {
    "text": "by 1 column vector\nof parameters beta. ",
    "start": "4637770",
    "end": "4646400"
  },
  {
    "text": "And then that lets us use a\nmuch more condensed notation",
    "start": "4646400",
    "end": "4651560"
  },
  {
    "text": "for our multivariate\nlinear model. And it's just y sub i equals x\nsub i beta plus epsilon sub i.",
    "start": "4651560",
    "end": "4660440"
  },
  {
    "text": "So this is the model for\neach individual observation.",
    "start": "4660440",
    "end": "4666290"
  },
  {
    "text": "And so we can go even further. And we can basically\nput all the observations into matrices or vectors and\ncompress our notation even",
    "start": "4666290",
    "end": "4677120"
  },
  {
    "text": "further. So now let's let y the\nentire set of observations",
    "start": "4677120",
    "end": "4685550"
  },
  {
    "text": "on our outcome variable across\nour different observations. Let's put that into an\nn by 1 column vector.",
    "start": "4685550",
    "end": "4694880"
  },
  {
    "text": "And then let's put our\nerrors also into an n by 1 column vector.",
    "start": "4694880",
    "end": "4700280"
  },
  {
    "text": "And then let's take\neach one of these row vectors that I defined\non the previous slide--",
    "start": "4700280",
    "end": "4705340"
  },
  {
    "text": "so remember, for\neach observation, we had a row vector that was\nthe measures of all of the x's for each observation.",
    "start": "4705340",
    "end": "4711599"
  },
  {
    "text": "Let's stack those up\nand put those into an n",
    "start": "4711600",
    "end": "4716940"
  },
  {
    "text": "by k plus 1 matrix. So remember this column here\nis identically equal to 1.",
    "start": "4716940",
    "end": "4727460"
  },
  {
    "text": "And then the second\ncolumn on, are all measures of our explanatory\nvariables x1 through xk.",
    "start": "4727460",
    "end": "4737449"
  },
  {
    "text": " Yes.",
    "start": "4737450",
    "end": "4742800"
  },
  {
    "text": "AUDIENCE: Is there an in there\nor is that just [INAUDIBLE]?? SARA ELLISON: Yes, it\nis supposed to be x.",
    "start": "4742800",
    "end": "4749170"
  },
  {
    "text": "So yes, the i subscript snuck in\nthere, and I apologize for it.",
    "start": "4749170",
    "end": "4755179"
  },
  {
    "text": "AUDIENCE: So each row is an x. SARA ELLISON: Yes. AUDIENCE: [INAUDIBLE] SARA ELLISON: So\neach row is x sub i.",
    "start": "4755180",
    "end": "4760448"
  },
  {
    "text": "AUDIENCE: OK. SARA ELLISON: Yes, so each row\nvector, we'll call x sub i. This entire matrix\nwe'll call it x.",
    "start": "4760448",
    "end": "4766990"
  },
  {
    "text": "So you're absolutely right. Is everyone on board\nwith this structure?",
    "start": "4766990",
    "end": "4774880"
  },
  {
    "text": "So one more slide and\nthen we'll call it a day. Then when we have defined\neverything and created",
    "start": "4774880",
    "end": "4783594"
  },
  {
    "text": "that structure, we now can\nwrite the multivariate linear",
    "start": "4783595",
    "end": "4789310"
  },
  {
    "text": "regression model or\nlinear model as y equals x beta plus epsilon,\nwith all of those dimensions",
    "start": "4789310",
    "end": "4796150"
  },
  {
    "text": "up there. So we'll start here next time. And see you guys in\na couple of days.",
    "start": "4796150",
    "end": "4805440"
  },
  {
    "start": "4805440",
    "end": "4824000"
  }
]