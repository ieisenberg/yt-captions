[
  {
    "text": " NARRATOR: The\nfollowing content is provided under a\nCreative Commons license. Your support will help MIT\nOpenCourseWare continue",
    "start": "0",
    "end": "6870"
  },
  {
    "text": "to offer high-quality\neducational resources for free. To make a donation, or\nview additional materials",
    "start": "6870",
    "end": "13330"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13330",
    "end": "27000"
  },
  {
    "text": "PROFESSOR: All right,\nwe should get started.",
    "start": "27000",
    "end": "33690"
  },
  {
    "text": "So it's good to be back. We'll be discussing\nDNA sequence motifs.",
    "start": "33690",
    "end": "41920"
  },
  {
    "text": "Oh yeah, we were,\nif you're wondering, yes, the instructors were\nat the awards on Sunday.",
    "start": "41920",
    "end": "48059"
  },
  {
    "text": "It was great. The pizza was delicious. So today, we're going to be\ntalking about DNA and protein",
    "start": "48060",
    "end": "56559"
  },
  {
    "text": "sequence motifs, which are\nessentially the building blocks of regulatory\ninformation, in a sense.",
    "start": "56560",
    "end": "67770"
  },
  {
    "text": "Before we get started,\nI wanted to just see if there are any\nquestions about material",
    "start": "67770",
    "end": "75540"
  },
  {
    "text": "that Professor Gifford covered\nfrom the past couple days? No guarantees I'll be\nable to answer them,",
    "start": "75540",
    "end": "81490"
  },
  {
    "text": "but just general things related\nto transcriptome analysis,",
    "start": "81490",
    "end": "86759"
  },
  {
    "text": "or PCA? Anything? Hopefully, you all got the\nemail that he sent out about,",
    "start": "86760",
    "end": "93020"
  },
  {
    "text": "basically, what you're\nexpected to get. So at the level of the\ndocument that's posted,",
    "start": "93020",
    "end": "102070"
  },
  {
    "text": "that's sort of what\nwe're expecting. So if you haven't\nhad linear algebra, that should still\nbe accessible--",
    "start": "102070",
    "end": "107189"
  },
  {
    "text": "not necessarily all\nthe derivations. Any questions about that?",
    "start": "107190",
    "end": "113630"
  },
  {
    "text": "OK, so as a reminder,\nteam projects, your aims are due soon.",
    "start": "113630",
    "end": "121150"
  },
  {
    "text": "We'll post a\nslightly-- there's been a request for more detailed\ninformation on what we'd like in the aims, so we'll\npost something more detailed",
    "start": "121150",
    "end": "129660"
  },
  {
    "text": "on the website this\nevening, and probably extend the deadline\na day or two, just",
    "start": "129660",
    "end": "135860"
  },
  {
    "text": "to give you a little bit\nmore time on the aims. So after you submit\nyour aims-- this",
    "start": "135860",
    "end": "142070"
  },
  {
    "text": "is students who are\ntaking the project component of the\ncourse-- then your team",
    "start": "142070",
    "end": "149650"
  },
  {
    "text": "will be assigned to one\nof the three instructors as a mentor/advisor, and\nwe will schedule a time",
    "start": "149650",
    "end": "158170"
  },
  {
    "text": "to meet with you in\nthe next week or two to discuss your\naims, just to assess",
    "start": "158170",
    "end": "163500"
  },
  {
    "text": "the feasibility of the project\nand so forth, before you launch into it.",
    "start": "163500",
    "end": "170569"
  },
  {
    "text": "All right-- any questions\nfrom past lectures?",
    "start": "170570",
    "end": "176421"
  },
  {
    "text": "All right, today\nwe're going to talk about modeling and discovery\nof sequence motifs.",
    "start": "176421",
    "end": "182280"
  },
  {
    "text": "We'll give an example of a\nparticular algorithm that's used in motif finding called\nthe Gibbs Sampling Algorithm.",
    "start": "182280",
    "end": "188870"
  },
  {
    "text": "It's not the only algorithm,\nit's not even necessarily the best algorithm. It's pretty good.",
    "start": "188870",
    "end": "195000"
  },
  {
    "text": "It works in many cases. It's an early algorithm. But it's interesting\nto talk about",
    "start": "195000",
    "end": "201700"
  },
  {
    "text": "because it illustrates\nthe problem in general, and also it's an example\nof a stochastic algorithm--",
    "start": "201700",
    "end": "206800"
  },
  {
    "text": "an algorithm where what\nit does is determined at random, to some extent.",
    "start": "206800",
    "end": "212409"
  },
  {
    "text": "And yet still often converges\nto a particular answer. So it's interesting\nfrom that point of view.",
    "start": "212410",
    "end": "218170"
  },
  {
    "text": "And we'll talk about\na few other types of motif finding algorithms. And we'll do a little bit\non statistical entropy",
    "start": "218170",
    "end": "226476"
  },
  {
    "text": "and information\ncontent, which is a handy way of\ndescribing motifs. And talk a little bit\nabout parameter estimation,",
    "start": "226476",
    "end": "234469"
  },
  {
    "text": "as well, which is critical\nwhen you have a motif and you want to\nbuild a model of it",
    "start": "234470",
    "end": "240440"
  },
  {
    "text": "to then discover additional\ninstances of that motif. So some reading for today--\nI posted some nature",
    "start": "240440",
    "end": "249480"
  },
  {
    "text": "biotechnology primers on\nmotifs and motif discovery, which are pretty easy reading.",
    "start": "249480",
    "end": "256588"
  },
  {
    "text": "The textbook, chapter 6, also\nhas some good information on motifs, I encourage\nyou to look at that.",
    "start": "256589",
    "end": "262200"
  },
  {
    "text": "And I've also posted\nthe original paper by Bailey and Elkin on\nthe MEME algorithm, which",
    "start": "262200",
    "end": "268009"
  },
  {
    "text": "is kind of related to the\nGibbs Sampling Algorithm, but is used as\nexpectation maximization.",
    "start": "268010",
    "end": "275720"
  },
  {
    "text": "And so it's a really nice\npaper-- take a look at that. And I'll also post the original\nGibbs Sampler paper later",
    "start": "275720",
    "end": "284370"
  },
  {
    "text": "today. And then on Tuesday,\nwe're going to be talking about Markov and\nhidden Markov models.",
    "start": "284370",
    "end": "290160"
  },
  {
    "text": "And so take a look at\nthe primer on HMMs,",
    "start": "290160",
    "end": "296030"
  },
  {
    "text": "as well as there\nis some information on HMMs in the text. It's not really a\ndistinct section,",
    "start": "296030",
    "end": "302410"
  },
  {
    "text": "it's kind of scattered\nthroughout the text. So the best approach is to\nlook in the index for HMMs,",
    "start": "302410",
    "end": "310086"
  },
  {
    "text": "and read the relevant parts\nthat you're interested in.",
    "start": "310086",
    "end": "316310"
  },
  {
    "text": "And if you really\nwant to understand the mechanics of HMMs, and\nhow to actually implement",
    "start": "316310",
    "end": "321430"
  },
  {
    "text": "one in depth, then I strongly\nrecommend this Rabiner tutorial on HMMs, which is posted.",
    "start": "321430",
    "end": "328050"
  },
  {
    "text": "So everyone please,\nplease read that. I will use the same notation,\nto the extent possible,",
    "start": "328050",
    "end": "336030"
  },
  {
    "text": "as the Rabiner paper\nwhen talking about some of the algorithms used\nin HMMs in lecture.",
    "start": "336030",
    "end": "342440"
  },
  {
    "text": "So it should synergize well. ",
    "start": "342440",
    "end": "348280"
  },
  {
    "text": "So what is a sequence motifs? ",
    "start": "348280",
    "end": "353335"
  },
  {
    "text": "In general, it's\na pattern that's common to a set of DNA,\nRNA, or protein sequences, that share a\nbiological property.",
    "start": "353335",
    "end": "361919"
  },
  {
    "text": "So for example, all of the\nbinding sites of the Myc transcription factor--\nthere's probably a pattern",
    "start": "361920",
    "end": "369680"
  },
  {
    "text": "that they share, and you\ncall that the motif for Myc. Can you give some examples of\nwhere you might get DNA motifs?",
    "start": "369680",
    "end": "379280"
  },
  {
    "text": "Or protein motifs? Anyone have another\nexample of a type of motif that would be interesting?",
    "start": "379280",
    "end": "385740"
  },
  {
    "text": "What about one that's\ndefined on function? Yeah, go ahead. What's your name? AUDIENCE: Dan. [INAUDIBLE] PROFESSOR: Yeah.",
    "start": "385740",
    "end": "391370"
  },
  {
    "text": "So each kinase typically\nhas a certain sequence motif",
    "start": "391370",
    "end": "397560"
  },
  {
    "text": "that determines which\nproteins it phosphorylate. Right. Other examples?",
    "start": "397560",
    "end": "402812"
  },
  {
    "text": "Yeah, so in that case, you\nmight determine it functionally. You might purify that\nprotein, incubate it with a pool of peptides, and\nsee what gets phosphorylated,",
    "start": "402812",
    "end": "410060"
  },
  {
    "text": "for example. Yeah, in the back? AUDIENCE: I'm [INAUDIBLE],\nand promonocytes.",
    "start": "410060",
    "end": "415184"
  },
  {
    "text": "PROFESSOR: What\nwas the first one? AUDIENCE: Promonocytes? Oh, that one? Oh, that was my name. PROFESSOR: Yeah, OK.",
    "start": "415184",
    "end": "421850"
  },
  {
    "text": "And as to promoter motifs, sir? Some examples? AUDIENCE: Like, [INAUDIBLE]\nin transcription mining site.",
    "start": "421850",
    "end": "431613"
  },
  {
    "text": "PROFESSOR: Ah. Yeah. And so you would\nidentify those how? AUDIENCE: By\nlooking at sequences",
    "start": "431613",
    "end": "437156"
  },
  {
    "text": "upstream of\n[INAUDIBLE], and seeing what different sequences\nhave in common?",
    "start": "437156",
    "end": "443091"
  },
  {
    "text": "PROFESSOR: Right. So I think there's at\nleast three ways-- OK, four ways I can\nthink of identifying",
    "start": "443092",
    "end": "449020"
  },
  {
    "text": "those types of motifs. That's probably one of\nthe most common types of motifs encountered\nin molecular biology.",
    "start": "449020",
    "end": "454599"
  },
  {
    "text": "So one way, you take\na bunch of genes, where you've identified the\ntranscription start site.",
    "start": "454600",
    "end": "460490"
  },
  {
    "text": "You just look for patterns--\nshort sub-sequences that they have in common. That might give you the\nTATA box, for example.",
    "start": "460490",
    "end": "469120"
  },
  {
    "text": "Another way would be, what\nabout comparative genomics? You take each\nindividual one, look to see which parts of that\npromoter are conserved.",
    "start": "469120",
    "end": "476720"
  },
  {
    "text": "That can also help you\nrefine your motifs. Protein binding, you\ncould do ChIP-Seq,",
    "start": "476720",
    "end": "483100"
  },
  {
    "text": "that could give you motifs. And what about a\nfunctional readout? You clone a bunch\nof random sequences",
    "start": "483100",
    "end": "490080"
  },
  {
    "text": "upstream of a\nluciferase reporter, see which ones actually drive\nexpression, for example.",
    "start": "490080",
    "end": "495580"
  },
  {
    "text": "So, that would be another. Yeah, absolutely, so there's\na bunch of different ways to define them.",
    "start": "495580",
    "end": "501569"
  },
  {
    "text": "In terms of when we\ntalk about motifs, there are several different\nmodels of increasing resolution",
    "start": "501570",
    "end": "508660"
  },
  {
    "text": "that people use. So people often talk about talk\nabout the consensus sequence",
    "start": "508660",
    "end": "513960"
  },
  {
    "text": "so you say the TATA\nbox, which, of course, describes the actual\nmotif-- T-A-T-A-A-A,",
    "start": "513960",
    "end": "521659"
  },
  {
    "text": "something like that. But that's really\njust the consensus",
    "start": "521659",
    "end": "526689"
  },
  {
    "text": "of a bunch of TATA box motifs. You rarely find the\nperfect consensus in real promoters-- the real,\nnaturally occurring ones",
    "start": "526690",
    "end": "534820"
  },
  {
    "text": "are usually one or\ntwo mismatches away. So that doesn't\nfully captured it.",
    "start": "534820",
    "end": "540084"
  },
  {
    "text": "So sometimes you'll have\na regular expression. So an example would be if\nyou were describing mammalian",
    "start": "540084",
    "end": "546710"
  },
  {
    "text": "5 prime splice sites, you might\ndescribe the motif as GT, A",
    "start": "546710",
    "end": "552300"
  },
  {
    "text": "or G, AGT, or sometimes\nabbreviated as GTR AGT, where",
    "start": "552300",
    "end": "560959"
  },
  {
    "text": "R is shorthand for either\nappearing nucleotide-- either A or G. In some motifs you could\nhave GT, NN, GT, or something",
    "start": "560960",
    "end": "569710"
  },
  {
    "text": "like that. Those can be captured,\noften, by regular expressions",
    "start": "569710",
    "end": "575170"
  },
  {
    "text": "in a scripting language\nlike Python or Perl. Another very common\ndescription in motifs,",
    "start": "575170",
    "end": "581740"
  },
  {
    "text": "there would be a weight matrix. So you'll see a matrix where\nthe width of the matrix",
    "start": "581740",
    "end": "587990"
  },
  {
    "text": "is the number of\nbases in the motif. And then there are\nfour rows, which",
    "start": "587990",
    "end": "593269"
  },
  {
    "text": "are the four bases-- we'll\nsee that in a moment. Sometimes these are described\nas position-specific probability",
    "start": "593270",
    "end": "599279"
  },
  {
    "text": "matrices, or position-specific\nscore matrices. We'll come to that in a moment. And then there are more\ncomplicated models.",
    "start": "599280",
    "end": "604890"
  },
  {
    "text": "So it's increasingly\nbecoming clear that the simple weight matrix\nis too limited-- it doesn't",
    "start": "604890",
    "end": "610339"
  },
  {
    "text": "capture all the information\nthat's present in motifs.",
    "start": "610340",
    "end": "615410"
  },
  {
    "text": "So we talked about where\ndo motifs come from. These are just some examples. I think I talked\nabout all of these,",
    "start": "615410",
    "end": "622740"
  },
  {
    "text": "except for in vitro binding. So in addition to doing\na CLIP-seq, where you're",
    "start": "622740",
    "end": "628010"
  },
  {
    "text": "looking at the binding of\nthe endogenous protein, you could also make recombinant\nprotein-- incubate that",
    "start": "628010",
    "end": "633019"
  },
  {
    "text": "with a random pool\nof DNA molecules, pull down, and see what\nbinds to it, for example.",
    "start": "633020",
    "end": "638750"
  },
  {
    "text": " So why are they important?",
    "start": "638750",
    "end": "644200"
  },
  {
    "text": "They're important\nfor obvious reasons-- that they can\nidentify proteins that",
    "start": "644200",
    "end": "649279"
  },
  {
    "text": "have a specific biological\nproperty of interest. For example, being\nphosphorylated by a particular",
    "start": "649280",
    "end": "654480"
  },
  {
    "text": "kinase. Or promoters that have\na particular property. That is, that they're\nlikely to be regulated by a particular transcription\nfactor, et cetera.",
    "start": "654480",
    "end": "663510"
  },
  {
    "text": "And ultimately, if\nyou're very interested in the regulation of\na particular gene,",
    "start": "663510",
    "end": "672160"
  },
  {
    "text": "knowing what motifs are upstream\nand how strong the evidence is",
    "start": "672160",
    "end": "677180"
  },
  {
    "text": "for each particular\ntranscription factor that might or might not\nbind there, can be very useful in understanding\nthe regulation of that gene.",
    "start": "677180",
    "end": "687527"
  },
  {
    "text": "And they're also going to\nbe important for efforts to model gene expression. So, a goal of\nsystems biology would",
    "start": "687527",
    "end": "693650"
  },
  {
    "text": "be to predict, from a given\nstarting point, if we introduce",
    "start": "693650",
    "end": "702100"
  },
  {
    "text": "some perturbation-- for\nexample, if we knock out or knock down a particular\ntranscription factor, or over-express it, how\nwill the system behave?",
    "start": "702100",
    "end": "711470"
  },
  {
    "text": "So you'd really want\nto be able to predict how the occupancy of\nthat transcription factor",
    "start": "711470",
    "end": "717620"
  },
  {
    "text": "would change. You'd want to know, first, where\nit is at an endogenous levels, and then how its occupancy\nat every promoter",
    "start": "717620",
    "end": "724209"
  },
  {
    "text": "will change when you\nperturb its levels. And then, what\neffects that will have",
    "start": "724210",
    "end": "729459"
  },
  {
    "text": "on expression of\ndownstream genes. So these sorts of\nmodels all require",
    "start": "729460",
    "end": "736370"
  },
  {
    "text": "really accurate\ndescriptions of motifs. OK, so these are some\nexamples of protein motifs.",
    "start": "736370",
    "end": "742230"
  },
  {
    "text": "Anyone recognize this one? What motif is that?",
    "start": "742230",
    "end": "749720"
  },
  {
    "text": "So it says X's. X's would be\ndegenerate oppositions, and C's would cysteines. And H's [INAUDIBLE].",
    "start": "749720",
    "end": "756965"
  },
  {
    "text": "What is this? What does this define? What protein has this? What can you predict\nabout its function?",
    "start": "756965",
    "end": "762857"
  },
  {
    "text": "AUDIENCE: Zinc finger. PROFESSOR: Zinc finger, right. So it's a motif commonly seen\nin genome binding transcription factors, and it\ncoordinates to zinc.",
    "start": "762857",
    "end": "772399"
  },
  {
    "text": "What about this one? Any guesses on\nwhat this motif is? This quite a short motif.",
    "start": "772400",
    "end": "780550"
  },
  {
    "text": "Yeah? AUDIENCE: That's\na phosphorylation. PROFESSOR: Phosphorylation site. Yeah. And how do you know that?",
    "start": "780550",
    "end": "786753"
  },
  {
    "text": "AUDIENCE: The [INAUDIBLE] and\nthe [INAUDIBLE] next to it means it's [INAUDIBLE].",
    "start": "786754",
    "end": "792600"
  },
  {
    "text": "PROFESSOR: OK, so you even\nknow what kinase it is, yeah. Exactly. So that's sort of the view. So, serine, threonine,\nand tirocene",
    "start": "792600",
    "end": "798922"
  },
  {
    "text": "are the residues that\nget phosphorylated. And so if you see a motif\nwith a serine in the middle, it's a good chance it's\na phosphorylation site.",
    "start": "798922",
    "end": "806139"
  },
  {
    "text": " Here are some-- you can think\nof them as DNA sequence motifs,",
    "start": "806140",
    "end": "812720"
  },
  {
    "text": "because they occur in\ngenes, but they, of course, function at that RNA level. These are the motifs that\noccur at the boundaries",
    "start": "812720",
    "end": "820310"
  },
  {
    "text": "of mammalian introns. So this first one is the\nprime splicing motif.",
    "start": "820310",
    "end": "826700"
  },
  {
    "text": "So these would be the bases\nthat occur at the last three bases of the exon. The first two of the intron\nhere, are almost always GT.",
    "start": "826700",
    "end": "835079"
  },
  {
    "text": "And then you have this position\nthat I mentioned here-- it's almost always\nA or G position. And then some positions that\nare bias for A, bias for G,",
    "start": "835079",
    "end": "842660"
  },
  {
    "text": "and then slightly\nbiased for T. And that is what you see when you\nlook at a whole bunch of five",
    "start": "842660",
    "end": "851075"
  },
  {
    "text": "prime ends of mammalian\nintrons-- they have this motif. So some will have better\nmatches, or worse,",
    "start": "851075",
    "end": "857710"
  },
  {
    "text": "to this particular pattern. And that's the average\npattern that you see. And it turns out\nthat in this case,",
    "start": "857710",
    "end": "865290"
  },
  {
    "text": "the recognition of that site\nis not by a protein, per se, but it's by a\nribonucleoprotein complex.",
    "start": "865290",
    "end": "872410"
  },
  {
    "text": "So there's actually\nan RNA called U1 snRNA that base pairs with the\nfive prime splice site.",
    "start": "872410",
    "end": "877879"
  },
  {
    "text": "And its sequence, or\npart of its sequence, is perfectly complimentary to\nthe consensus five prime splice",
    "start": "877880",
    "end": "885320"
  },
  {
    "text": "site. So we can understand why\nfive prime splice sites have this motif-- they're\nevolving to have",
    "start": "885320",
    "end": "890660"
  },
  {
    "text": "a certain degree of\ncomplementarity to U1, and in order to get\nofficially recognized by the splicing machinery.",
    "start": "890660",
    "end": "898250"
  },
  {
    "text": "Then at the three\nprime end of introns, you see this motif here. So here's the last base of\nthe intron, a G, and then",
    "start": "898250",
    "end": "905450"
  },
  {
    "text": "an A before it. Almost all introns end with AG. Then you have a\npyrimidine ahead of it.",
    "start": "905450",
    "end": "912000"
  },
  {
    "text": "Then you have basically an\nirrelevant position here at minus four, which is\nnot strongly conserved.",
    "start": "912000",
    "end": "920720"
  },
  {
    "text": "And then a stretch of\nresidues that are usually, but not always, pyrimidines--\ncalled the pyrimidine track.",
    "start": "920720",
    "end": "926340"
  },
  {
    "text": "And in this case,\nthe recognition is actually by proteins\nrather than RNA.",
    "start": "926340",
    "end": "931910"
  },
  {
    "text": "And there are two proteins. One called U2AF65 that\nbinds the pyrimidine track, and one, U2AF35 that\nbinds that last YAG motif.",
    "start": "931910",
    "end": "940550"
  },
  {
    "text": "And then there's\nan upstream motif here, that's just upstream\nof the 3 prime splice site that is quite degenerate\nand hard to find,",
    "start": "940550",
    "end": "947350"
  },
  {
    "text": "called the branch point motif. OK, so, let's take an example. So the five prime splice site\nis a nice example of a motif,",
    "start": "947350",
    "end": "955650"
  },
  {
    "text": "because you can uniquely\nalign them, right? You can sequence DNA,\nsequence genomes,",
    "start": "955650",
    "end": "961530"
  },
  {
    "text": "align the CDNA to the\ngenome, that tells you exactly where the\nsplice junctions are. And you can take the exons that\nhave a 5 prime splice site,",
    "start": "961530",
    "end": "970710"
  },
  {
    "text": "and align the sequences aligned\nto the exon/intron boundary and get a precise motif.",
    "start": "970710",
    "end": "976630"
  },
  {
    "text": "And then you can tally up\nthe frequencies of the bases, and make a table\nlike this, which we would call a\nposition-specific probability",
    "start": "976630",
    "end": "983760"
  },
  {
    "text": "matrix. And what you could then\ndo to predict additional,",
    "start": "983760",
    "end": "991899"
  },
  {
    "text": "say, five prime splice-site\nmotifs in other genes-- for example, genes where\nyou didn't get a good CDNA",
    "start": "991900",
    "end": "998009"
  },
  {
    "text": "coverage, because let's\nsay they're not expressed in the cells that you\nanalyzed-- you could then",
    "start": "998010",
    "end": "1003529"
  },
  {
    "text": "make this odds ratio here. So here we have a\ncandidate sequence.",
    "start": "1003530",
    "end": "1010920"
  },
  {
    "text": "So the motif is nine positions,\noften numbered minus 3",
    "start": "1010920",
    "end": "1016425"
  },
  {
    "text": "to minus 1, would be the\nexonic parts of this. And then plus 1 to plus\n6 would be the first six bases of the intron.",
    "start": "1016425",
    "end": "1021680"
  },
  {
    "text": "That's just the\nconvention that's used. I'm sure it's going to drive\nthe computer scientists crazy",
    "start": "1021680",
    "end": "1026689"
  },
  {
    "text": "because we're not\nstarting at 0, but that's usually what's used\nin the literature. And so we have a\nnine-based motif.",
    "start": "1026690",
    "end": "1033010"
  },
  {
    "text": "And then we're going to\ncalculate the probability of generating that\nparticular sequence as given",
    "start": "1033010",
    "end": "1039970"
  },
  {
    "text": "plus-- meaning given\nour foreground, or motif model-- as the\nproduct of the probability",
    "start": "1039970",
    "end": "1047760"
  },
  {
    "text": "of generating the first base in\nsequence, S1, using the column",
    "start": "1047760",
    "end": "1053150"
  },
  {
    "text": "probability in the\nminus 3 position. So if the first base is AC,\nfor example, that would be 0.4.",
    "start": "1053150",
    "end": "1061660"
  },
  {
    "text": "And then the probability of\ngenerating the second base in the sequence using the\nnext column, and so forth.",
    "start": "1061660",
    "end": "1066670"
  },
  {
    "text": " If you made a vector for\neach position that had a 1",
    "start": "1066670",
    "end": "1074207"
  },
  {
    "text": "for the base that\noccurred at that position, and a 0 for the other\nbases, and then you just did the dot product of that\nwith the matrix, you get this.",
    "start": "1074207",
    "end": "1082680"
  },
  {
    "text": "So, we multiply probabilities. So that is assuming\nindependence between positions.",
    "start": "1082680",
    "end": "1088420"
  },
  {
    "text": "And so that's a key assumption--\nweight matrices assume that each position in the\nmotif contributes independently",
    "start": "1088420",
    "end": "1096020"
  },
  {
    "text": "to the overall\nstrength of that motif. And that may or may not be\ntrue-- they don't assume that it's homogeneous,\nthat is you",
    "start": "1096020",
    "end": "1102190"
  },
  {
    "text": "have usually in a typical\ncase, different probabilities",
    "start": "1102190",
    "end": "1108326"
  },
  {
    "text": "in different columns,\nso it's inhomogeneous, but assumes independence. And then you often want\nto use a background model.",
    "start": "1108327",
    "end": "1115769"
  },
  {
    "text": "For example, if your\ngenome composition is 25% of each of\nthe nucleotides, you could just have a\nbackground probability that",
    "start": "1115770",
    "end": "1123549"
  },
  {
    "text": "was equally likely\nfor each of the four, and then calculate the\nprobability, S given minus,",
    "start": "1123550",
    "end": "1131300"
  },
  {
    "text": "of generating that\nparticular [INAUDIBLE] under the background model, and\ntake the ratio of those two.",
    "start": "1131300",
    "end": "1136880"
  },
  {
    "text": "And the advantage of\nthat is that then you can find sequences\nthat are-- that ratio,",
    "start": "1136880",
    "end": "1142890"
  },
  {
    "text": "it could be 100 times more\nlike a 5 prime splice site than like background--\nor 1,000 times.",
    "start": "1142890",
    "end": "1148700"
  },
  {
    "text": "Or you have some sort\nof scaling on it. Whereas, if you just\ntake the raw probability,",
    "start": "1148700",
    "end": "1154850"
  },
  {
    "text": "it's going to be something\nthat's on the order of 1/4 to a 1/9. So some very, very small\nnumber that's a little hard to",
    "start": "1154850",
    "end": "1160631"
  },
  {
    "text": "work with.  So when people\ntalk about motifs,",
    "start": "1160631",
    "end": "1167410"
  },
  {
    "text": "they often use language\nlike exact, or precise, versus degenerate, strong\nversus weak, good versus lousy,",
    "start": "1167410",
    "end": "1174310"
  },
  {
    "text": "depending on the\ncontext, who's listening. So an example of these would\nbe a restriction enzyme.",
    "start": "1174310",
    "end": "1181830"
  },
  {
    "text": "You often say\nrestriction enzymes have very precise\nsequence specificity, they only cut-- echo\nR1 only cuts a GAA TTC.",
    "start": "1181830",
    "end": "1191190"
  },
  {
    "text": "Whereas, a TATA binding protein\nis somewhat more degenerate. It'll bind to a range of things.",
    "start": "1191190",
    "end": "1198510"
  },
  {
    "text": "So I use degenerate there, you\ncould say it's a weaker motif. You'll often-- if you want\nto try to make this precise,",
    "start": "1198510",
    "end": "1204110"
  },
  {
    "text": "then the language of\nentropy information offers additional terminology,\nlike high information",
    "start": "1204110",
    "end": "1210360"
  },
  {
    "text": "content, low entropy, et cetera. So let's take a look at this as\nperhaps a more natural, or more",
    "start": "1210360",
    "end": "1219230"
  },
  {
    "text": "precise way of describing\nwhat we mean, here. So imagine you have a motif.",
    "start": "1219230",
    "end": "1225220"
  },
  {
    "text": "We're going to do a\nmotif of length one-- just keep the math\nsuper simple, but you'll",
    "start": "1225220",
    "end": "1231200"
  },
  {
    "text": "see it easily generalizes. So you have probabilities of the\nfour nucleotides that are Pk.",
    "start": "1231200",
    "end": "1240090"
  },
  {
    "text": "And you have background\nprobabilities, qk. And we're going to assume\nthose are all uniform,",
    "start": "1240090",
    "end": "1245690"
  },
  {
    "text": "they're all a quarter. So then the statistical,\nor Shannon entropy",
    "start": "1245690",
    "end": "1251920"
  },
  {
    "text": "of a probability distribution--\nor vector of probabilities,",
    "start": "1251920",
    "end": "1257520"
  },
  {
    "text": "if you will-- is defined here. So H of q, where q is a\ndistribution or, in this case,",
    "start": "1257520",
    "end": "1270590"
  },
  {
    "text": "vector, is defined as minus\nthe summation of qk log qk,",
    "start": "1270590",
    "end": "1279460"
  },
  {
    "text": "in general. And then if you wanted\nto be in units of bits, you'd use log base 2.",
    "start": "1279460",
    "end": "1285059"
  },
  {
    "text": "So how many people have\nseen this equation before?  Like half, I'm going to go with.",
    "start": "1285060",
    "end": "1292030"
  },
  {
    "text": "OK, good. So who can tell me\nwhy, first of all--",
    "start": "1292030",
    "end": "1299490"
  },
  {
    "text": "is this a positive\nquantity, negative quantity, non-negative, or what? ",
    "start": "1299490",
    "end": "1306354"
  },
  {
    "text": "Yeah, go ahead. AUDIENCE: Log qk is always\ngoing to be negative. And so therefore\nyou have to take the negative of the sum\nof all the negatives",
    "start": "1306354",
    "end": "1312482"
  },
  {
    "text": "to get a positive number. PROFESSOR: Right,\nso this, in general, is a non-negative\nquantity, because we",
    "start": "1312482",
    "end": "1317580"
  },
  {
    "text": "have this minus sign here. We're taking logs of things\nthat are between 0 and 1. So the logs are negative, right?",
    "start": "1317580",
    "end": "1324530"
  },
  {
    "text": "OK. And then what would be\nthe entropy if I say that the distribution q is\nthis-- 0100, meaning,",
    "start": "1324530",
    "end": "1337810"
  },
  {
    "text": "it's a motif that's 100% C? What is the entropy of that? ",
    "start": "1337810",
    "end": "1344536"
  },
  {
    "text": "What was your name? AUDIENCE: William. PROFESSOR: William. AUDIENCE: So the\nentropy would be 0,",
    "start": "1344536",
    "end": "1351084"
  },
  {
    "text": "because the vector is determined\nin respect of the known certainty. PROFESSOR: Right. And we do the math--\nyou'll get, for the C term,",
    "start": "1351084",
    "end": "1359215"
  },
  {
    "text": "you'll have a sum. You'll have three\nterms that are 0 log 0-- it might crash your\ncalculator, I guess.",
    "start": "1359215",
    "end": "1367410"
  },
  {
    "text": "And then you'll have one\nterm that is 1 log 1.",
    "start": "1367410",
    "end": "1372440"
  },
  {
    "text": "And so 1 log 1, that's easy. That's 0, right? This, you could\nsay, is undefined.",
    "start": "1372440",
    "end": "1378810"
  },
  {
    "text": "But using L'Hospital's rule--\nby continuity, x log x,",
    "start": "1378810",
    "end": "1384000"
  },
  {
    "text": "you take the limit,\nas x gets small, is 0. So this is defined to be\n0 in information theory.",
    "start": "1384000",
    "end": "1390350"
  },
  {
    "text": "And this is always, always 0. So that comes out to be 0. So it's deterministic.",
    "start": "1390350",
    "end": "1396090"
  },
  {
    "text": "So entropy is a\nmeasure of uncertainty, and so that makes sense-- if\nyou know what the base is,",
    "start": "1396090",
    "end": "1401340"
  },
  {
    "text": "there's no uncertainty,\nentropy is 0. So what about this vector-- 1/4,\n1/4, 25% of each of the bases,",
    "start": "1401340",
    "end": "1415290"
  },
  {
    "text": "what is H of q? Anyone?",
    "start": "1415290",
    "end": "1420700"
  },
  {
    "text": " I'm going to make\nyou show me why, so-- Anyone want\nto attempt this?",
    "start": "1420700",
    "end": "1432695"
  },
  {
    "text": "Levi? AUDIENCE: I think it's 2. PROFESSOR: 2, OK. Can you explain? AUDIENCE: Because\nthe log of the 1/4's",
    "start": "1432695",
    "end": "1438260"
  },
  {
    "text": "is going to be negative 2. And then you're\nmultiplying that by 1/4,",
    "start": "1438260",
    "end": "1444554"
  },
  {
    "text": "so you're getting 1/2 for each\nand adding it up equals 2. PROFESSOR: Right,\nin sum, there are going to be four terms that\nare 1/4 times log of a 1/4.",
    "start": "1444554",
    "end": "1453010"
  },
  {
    "text": " This is minus 2, 1/4 times\nminus 2 is minus 1/2,",
    "start": "1453010",
    "end": "1459215"
  },
  {
    "text": "4 times minus 1/2 is\nminus 2, and then you change the sign, because\nthere's this minus in front. So that equals 2.",
    "start": "1459215",
    "end": "1466670"
  },
  {
    "text": "And what about this one? ",
    "start": "1466670",
    "end": "1475770"
  },
  {
    "text": "Anyone see that one? This is a coin flip, basically. All right?",
    "start": "1475770",
    "end": "1481220"
  },
  {
    "text": "It's either A or G. [INAUDIBLE]. ",
    "start": "1481220",
    "end": "1489785"
  },
  {
    "text": "Anyone? ",
    "start": "1489785",
    "end": "1495730"
  },
  {
    "text": "Levi, want to do this one again? AUDIENCE: It's 1. PROFESSOR: OK, and why?",
    "start": "1495730",
    "end": "1502164"
  },
  {
    "text": "AUDIENCE: Because you have two\nterms of 0 log 0, which is 0. And two terms of 1/2\ntimes the log of 1/2,",
    "start": "1502165",
    "end": "1516980"
  },
  {
    "text": "which is just negative 1. So you have 2 halves. PROFESSOR: Yeah. So two terms like that. And then there's going\nto be two terms that",
    "start": "1516980",
    "end": "1523226"
  },
  {
    "text": "are something that turns\nout to be 0-- 0 log 0. And then there's\na minus in front.",
    "start": "1523226",
    "end": "1528730"
  },
  {
    "text": "So that will be 1. So a coin flip has one\nbit of information. So that's basically\nwhat we mean.",
    "start": "1528730",
    "end": "1534160"
  },
  {
    "text": "If you have a fair coin and\nyou don't know the outcome, we're going to\ncall that one bit. And so a base that could be\nany of the four equally likely",
    "start": "1534160",
    "end": "1543620"
  },
  {
    "text": "has twice as much uncertainty. All right, and this is related\nto the Boltzmann entropy",
    "start": "1543620",
    "end": "1551593"
  },
  {
    "text": "that you may be familiar with\nfrom statistical mechanics, which is the log of the number\nof states, in that if you have",
    "start": "1551593",
    "end": "1559880"
  },
  {
    "text": "N states, and they're\nall equally likely, then it turns out that the\nShannon entropy turns out",
    "start": "1559880",
    "end": "1565159"
  },
  {
    "text": "to be log of the number states. We saw that here-- four\nstates, equally likely,",
    "start": "1565160",
    "end": "1570350"
  },
  {
    "text": "comes out to be log of 4 or 2. And that's true in general.",
    "start": "1570350",
    "end": "1576130"
  },
  {
    "text": "All right, so you\ncan think of this as a generalization of Boltzmann\nentropy, if you want to.",
    "start": "1576130",
    "end": "1583010"
  },
  {
    "text": "OK, so why did he\ncall it entropy?",
    "start": "1583010",
    "end": "1588280"
  },
  {
    "text": "So it turns out\nthat Shannon, who was developing this\nin the late '40s, as developing a theory\nof communication,",
    "start": "1588280",
    "end": "1596697"
  },
  {
    "text": "scratched his head a little bit. And he talked to his friend,\nJohn von Neumann-- none other than him, involved\nin inventing computers--",
    "start": "1596697",
    "end": "1605970"
  },
  {
    "text": "and he says, \"My concern\nwas what to call it. I thought of calling\nit information. But the word was overly used.\"",
    "start": "1605970",
    "end": "1611800"
  },
  {
    "text": "OK, so back in 1949, information\nwas already overused.",
    "start": "1611800",
    "end": "1617060"
  },
  {
    "text": "\"And and so I decided\nto call it uncertainty.\" And then he discussed it\nwith John von Neumann,",
    "start": "1617060",
    "end": "1622300"
  },
  {
    "text": "and he had a better idea. He said, \"You should\ncall it entropy. In the first place,\nyour certainly function",
    "start": "1622300",
    "end": "1627704"
  },
  {
    "text": "has already been used\nin statistical mechanics under that name,\" so\nit already has a name. \"And the second place,\nand more important,",
    "start": "1627704",
    "end": "1633450"
  },
  {
    "text": "nobody knows what entropy\nreally is, so in a debate, you always have the advantage.\" So keep that in mind.",
    "start": "1633450",
    "end": "1640350"
  },
  {
    "text": "After you've taken this class,\njust start throwing it around and you will win\na lot of debates.",
    "start": "1640350",
    "end": "1645820"
  },
  {
    "text": " So how is information\nrelated to entropy?",
    "start": "1645820",
    "end": "1652530"
  },
  {
    "text": "So the way we're going\nto define it here, which is how it's\noften defined, is",
    "start": "1652530",
    "end": "1657730"
  },
  {
    "text": "information is reduction\nin uncertainty. So, if I'm dealing\nwith an unknown DNA",
    "start": "1657730",
    "end": "1667870"
  },
  {
    "text": "sequence, the\nlambda phage genome, and it has 25% of each\nbase, if you tell me,",
    "start": "1667870",
    "end": "1674929"
  },
  {
    "text": "I'm going to send you two\nbases, I have no idea. They could be any pair of bases. My uncertainty is\n2 bits per base,",
    "start": "1674930",
    "end": "1683259"
  },
  {
    "text": "or 4 bits before you\ntell me anything. If you then tell me,\nit's the TA motif,",
    "start": "1683260",
    "end": "1690360"
  },
  {
    "text": "which is always T followed\nby A, then now my uncertainty is 0, so the amount\nof information",
    "start": "1690360",
    "end": "1696990"
  },
  {
    "text": "that you just gave me is 4 bits. You reduced my uncertainty\nfrom 4 bits to 0.",
    "start": "1696990",
    "end": "1703370"
  },
  {
    "text": "So we define the information\nat a particular position as the entropy\nbefore-- before meaning",
    "start": "1703370",
    "end": "1710320"
  },
  {
    "text": "the background, the background\na sort of your null hypothesis-- minus the entropy after--\nso after you've told me",
    "start": "1710320",
    "end": "1717570"
  },
  {
    "text": "that this is an\ninstance of that motif, and it has a particular model.",
    "start": "1717570",
    "end": "1722580"
  },
  {
    "text": "So, in this case, you\ncan see the entropy",
    "start": "1722580",
    "end": "1730750"
  },
  {
    "text": "is going to be entropy before. This is just H of q\nright here, this term.",
    "start": "1730750",
    "end": "1736580"
  },
  {
    "text": "And then minus this\nterm, which is H of p.",
    "start": "1736580",
    "end": "1742049"
  },
  {
    "text": "So, if it's uniform, we said\nH of q is 2 bits per position.",
    "start": "1742050",
    "end": "1747070"
  },
  {
    "text": "And so, so the information\ncontent of the motif is just 2",
    "start": "1747070",
    "end": "1752870"
  },
  {
    "text": "minus the entropy\nof that motif model. In general, it turns out if\nthe positions in the motif",
    "start": "1752870",
    "end": "1761139"
  },
  {
    "text": "are independent, then\nthe information content of the motif is 2w\nminus H of motif,",
    "start": "1761140",
    "end": "1767610"
  },
  {
    "text": "where w is it\nwidth of the motif. So for example, the entropy\nof the motif of-- we",
    "start": "1767610",
    "end": "1777770"
  },
  {
    "text": "said the entropy of\nthis is 2 bits, right? Therefore, the information\ncontent is what?",
    "start": "1777770",
    "end": "1784160"
  },
  {
    "text": " If this is our-- let's say this\nis a P. This is our routine.",
    "start": "1784160",
    "end": "1794230"
  },
  {
    "text": "Are you starting to generate? What is its information content? AUDIENCE: 0?",
    "start": "1794230",
    "end": "1800250"
  },
  {
    "text": "PROFESSOR: 0. Why is it 0? Yeah, back row. AUDIENCE: Because the\ninformation content",
    "start": "1800250",
    "end": "1807043"
  },
  {
    "text": "of that is 0, and\nthen the information content of the known\nhypothesis, so to say, is 0. Sorry, both of them are 2.",
    "start": "1807043",
    "end": "1813071"
  },
  {
    "text": "So 2 minus 2 is 0. PROFESSOR: The entropy\nof the background is 2, and the entropy\nif this is also 2. So 2 minus 2 is 0.",
    "start": "1813072",
    "end": "1819520"
  },
  {
    "text": "And what about this? Let's say this was\nour motif, it's a motif that's either A or G.\nWe said the entropy of this",
    "start": "1819520",
    "end": "1828100"
  },
  {
    "text": "is 1 bit, so what\nis the information content of this motif?",
    "start": "1828100",
    "end": "1834125"
  },
  {
    "text": "AUDIENCE: 1. PROFESSOR: 1, and why is it 1? AUDIENCE: Background is\n2, and entropy here is 1.",
    "start": "1834125",
    "end": "1842705"
  },
  {
    "text": "PROFESSOR: Background\nis 2, entropy is 1. OK? And what about if I tell you\nit's the echo R1 restriction",
    "start": "1842705",
    "end": "1849240"
  },
  {
    "text": "enzyme? So it's GAA TTC,\na six-base motif precise-- it has\nto be those bases?",
    "start": "1849240",
    "end": "1856980"
  },
  {
    "text": "What is the information\ncontent of that motif? ",
    "start": "1856980",
    "end": "1866422"
  },
  {
    "text": "In the back? AUDIENCE: It's 12. PROFESSOR: 12-- 12 what? AUDIENCE: 12 bits. PROFESSOR: 12 bits,\nand why is that?",
    "start": "1866422",
    "end": "1872350"
  },
  {
    "text": "AUDIENCE: Because the\nbackground is 2 times 6. So 6 bases, and 2 bits for each.",
    "start": "1872350",
    "end": "1880328"
  },
  {
    "text": "And you have all the\nbases are determined at the specific\n[INAUDIBLE] enzyme site.",
    "start": "1880328",
    "end": "1886179"
  },
  {
    "text": "So the entropy of that is\n0, since 12 minus 0 is 12.",
    "start": "1886180",
    "end": "1891520"
  },
  {
    "text": "PROFESSOR: Right, the\nentropy of that motif is 0. You imagine 4,096\npossible six-mers.",
    "start": "1891520",
    "end": "1899190"
  },
  {
    "text": "One of them has probably 1. All the others have 0. You're going to\nhave that big sum.",
    "start": "1899190",
    "end": "1904730"
  },
  {
    "text": "It's going to come\nout to be 0, OK? Why is this useful\nat all, or is it? ",
    "start": "1904730",
    "end": "1912340"
  },
  {
    "text": "One of the reasons why\nit's useful-- sorry, that's on a later slide.",
    "start": "1912340",
    "end": "1919460"
  },
  {
    "text": "Well, just hang with\nme, and it will be clear why it's useful in a few slides.",
    "start": "1919460",
    "end": "1925659"
  },
  {
    "text": "But for now, we\nhave a description of information content. So the echo R1 site has\n12 bits of information,",
    "start": "1925660",
    "end": "1934940"
  },
  {
    "text": "a completely random\nposition has 0, and a short four-cutter\nrestriction enzyme",
    "start": "1934940",
    "end": "1941550"
  },
  {
    "text": "would have 2 times 4, 8\nbits of information, right, and an eight-cutter. So you can see as the\nrestriction enzyme gets",
    "start": "1941550",
    "end": "1946594"
  },
  {
    "text": "longer, more\ninformation content. ",
    "start": "1946594",
    "end": "1951894"
  },
  {
    "text": "So let's talk about the\nmotif finding problem, and then we'll return to the\nusefulness of information content.",
    "start": "1951894",
    "end": "1957090"
  },
  {
    "text": "So can everyone see\nthe motif that's present in all these sequences? ",
    "start": "1957090",
    "end": "1965710"
  },
  {
    "text": "If anyone can't,\nplease let me know. You probably can't. Now, what now? These are the same sequences,\nbut I've aligned them.",
    "start": "1965710",
    "end": "1974580"
  },
  {
    "text": "Can anyone see a motif? PROFESSOR: GGG GGG. PROFESSOR: Yeah,\nI heard some G's.",
    "start": "1974580",
    "end": "1980280"
  },
  {
    "text": "Right. so there's this motif\nthat's over here. It's pretty weak, and\npretty degenerate. There's definitely\nsome exceptions,",
    "start": "1980280",
    "end": "1985910"
  },
  {
    "text": "but you can definitely see\nthat a lot of the sequences have at least GGC,\npossibly an A after that.",
    "start": "1985910",
    "end": "1992450"
  },
  {
    "text": "Right, so this is the\nproblem we're dealing with. You have a bunch of promoters,\nand the transcription factor",
    "start": "1992450",
    "end": "2001070"
  },
  {
    "text": "that binds may be fairly\ndegenerate, maybe because it",
    "start": "2001070",
    "end": "2006867"
  },
  {
    "text": "likes to bind cooperatively\nwith several of its buddies, and so it doesn't\nhave to have a very strong instance of\nthe motif present.",
    "start": "2006867",
    "end": "2013919"
  },
  {
    "text": "And so, it can be quite\ndifficult to find. So that's why there's a real\nbio-informatics challenge.",
    "start": "2013920",
    "end": "2020790"
  },
  {
    "text": "Motif finding is not done by\nlining up sequences by hand, and drawing boxes--\nalthough that's",
    "start": "2020790",
    "end": "2026080"
  },
  {
    "text": "how the first motif was\nfound, the TATA box. That's why it's called the\nTATA box, because someone just",
    "start": "2026080",
    "end": "2031400"
  },
  {
    "text": "drew a box in a\nsequence alignment. But these days,\nyou need a computer",
    "start": "2031400",
    "end": "2036860"
  },
  {
    "text": "to find-- most motifs require\nsome sort of algorithm to find. ",
    "start": "2036860",
    "end": "2043890"
  },
  {
    "text": "Like I said, it's essentially\na local multiple alignment",
    "start": "2043890",
    "end": "2049050"
  },
  {
    "text": "problem. You want multiple alignment, but\nit doesn't have to be global. It just can be local, it can\nbe just over a sub-region.",
    "start": "2049050",
    "end": "2056033"
  },
  {
    "text": " There are basically at\nleast three different sort",
    "start": "2056034",
    "end": "2064770"
  },
  {
    "text": "of general approaches to the\nproblem of motif finding. One approach is the so-called\nenumerative, or dictionary,",
    "start": "2064770",
    "end": "2071839"
  },
  {
    "text": "approach. And so in this\napproach, you say, well, we're looking\nfor a motif of length 6",
    "start": "2071840",
    "end": "2080239"
  },
  {
    "text": "because this is a leucine zipper\ntranscription factor that we're",
    "start": "2080239",
    "end": "2085760"
  },
  {
    "text": "modeling, and they usually\nhave binding sites around 6, so we're going to guess 6. And we're going to\nenumerate all the six-mers,",
    "start": "2085760",
    "end": "2091949"
  },
  {
    "text": "there's 4,096 six-mers. We're going to count\nup their occurrences in a set of promoters that, for\nexample, are turned on when you",
    "start": "2091949",
    "end": "2101220"
  },
  {
    "text": "over-express this\nfactor, and look at those frequencies\ndivided by the frequencies",
    "start": "2101220",
    "end": "2107630"
  },
  {
    "text": "of those six-mers in\nsome background set-- either random sequences, or\npromoters that didn't turn on.",
    "start": "2107630",
    "end": "2112717"
  },
  {
    "text": "Something like that. You have two\nclasses, and you look for statistical enrichment.",
    "start": "2112717",
    "end": "2117940"
  },
  {
    "text": "This approach, this is fine. There's nothing wrong\nwith this approach. People use it all the time.",
    "start": "2117940",
    "end": "2125330"
  },
  {
    "text": "One of the downsides,\nthough, is that you're doing a lot of\nstatistical tests. You're essentially\ntesting each six-mer--",
    "start": "2125330",
    "end": "2131720"
  },
  {
    "text": "you're doing 4,096\nstatistical tests. So you have to adjust the\nstatistical significance",
    "start": "2131720",
    "end": "2136740"
  },
  {
    "text": "for the number of\ntests that you do, and that can reduce your power. So that's one main drawback.",
    "start": "2136740",
    "end": "2143520"
  },
  {
    "text": "The other reason\nis that maybe you don't see-- maybe this protein\nbinds a rather degenerate",
    "start": "2143520",
    "end": "2150750"
  },
  {
    "text": "motif, and a precise\nsix-mer is just too precise. None of them will\noccur often enough.",
    "start": "2150750",
    "end": "2156330"
  },
  {
    "text": "You really have to have\na degenerate motif that's C R Y G Y. That's really\nthe motif that it binds to,",
    "start": "2156330",
    "end": "2165869"
  },
  {
    "text": "and so you don't\nsee it unless you use something more degenerate. So you can generalize\nthis to use",
    "start": "2165870",
    "end": "2171680"
  },
  {
    "text": "regular expressions, et cetera. And it's a reasonable approach. Another approach that we'll\ntalk about in a moment",
    "start": "2171680",
    "end": "2180430"
  },
  {
    "text": "is probabilistic\noptimization, where you wander around the possible\nspace of possible motifs",
    "start": "2180430",
    "end": "2191070"
  },
  {
    "text": "until you find one\nthat looks strong. And we'll talk about that. And then they're deterministic\nversions of this, like me.",
    "start": "2191070",
    "end": "2199810"
  },
  {
    "text": " We're going to focus\ntoday on this second one.",
    "start": "2199810",
    "end": "2205360"
  },
  {
    "text": "Mostly because it's a little bit\nmore mysterious and interesting as an algorithm. ",
    "start": "2205360",
    "end": "2213210"
  },
  {
    "text": "And it's also [INAUDIBLE]. So, if the motif landscape\nlooked like this,",
    "start": "2213210",
    "end": "2218400"
  },
  {
    "text": "where imagine all\npossible motifs, you've somehow come up with a\n2D lattice of the possible motif",
    "start": "2218400",
    "end": "2225970"
  },
  {
    "text": "sequences. And then the strength of\nthat motif, or the degree to which that motif description\ncorresponds to the 2 motif",
    "start": "2225970",
    "end": "2235160"
  },
  {
    "text": "is represented by\nthe height here. Then, there's basically\none optimal motif, and the closer you get to\nthat, the better fit it is.",
    "start": "2235160",
    "end": "2244070"
  },
  {
    "text": "Then our problem is going\nto be relatively easy. But it's also possible that\nit looks something like this.",
    "start": "2244070",
    "end": "2252140"
  },
  {
    "text": "There's a lot of\nsort of decoy motifs, or weaker motifs that are\nonly slightly enriched",
    "start": "2252140",
    "end": "2257420"
  },
  {
    "text": "in the sequence space.",
    "start": "2257420",
    "end": "2262650"
  },
  {
    "text": "And so you can easily\nget tripped up, if you're wandering\naround randomly.",
    "start": "2262650",
    "end": "2269319"
  },
  {
    "text": "We don't know a priori,\nand it's probably not as simple as the first example.",
    "start": "2269320",
    "end": "2274410"
  },
  {
    "text": "And so that's one\nof the issues that motivates these\nstochastic algorithms.",
    "start": "2274410",
    "end": "2281770"
  },
  {
    "text": "So just to sort of\nput this in context-- the Gibbs motif sampler that\nwe're going to be talking about is a Monte Carlo\nalgorithm, so that just",
    "start": "2281770",
    "end": "2288380"
  },
  {
    "text": "means it's an algorithm\nthat basically does some random sampling\nsomewhere in it,",
    "start": "2288380",
    "end": "2295310"
  },
  {
    "text": "so that the outcome\nthat you get isn't necessarily deterministic. Your run it at different\ntimes, and you'll actually",
    "start": "2295310",
    "end": "2302360"
  },
  {
    "text": "get different\noutputs, which can be a little bit disconcerting\nand annoying at times.",
    "start": "2302360",
    "end": "2308190"
  },
  {
    "text": "But it turns out to be\nuseful in some cases. There's also a special case\nof a Las Vegas algorithm,",
    "start": "2308190",
    "end": "2316265"
  },
  {
    "text": "where it knows when it\ngot be optimal answer. But in general, not.",
    "start": "2316265",
    "end": "2321910"
  },
  {
    "text": "In general, you\ndon't know for sure. So Gibbs motif\nsimpler is basically",
    "start": "2321910",
    "end": "2329290"
  },
  {
    "text": "a model where you have a\nlikelihood for generating",
    "start": "2329290",
    "end": "2334880"
  },
  {
    "text": "a set of sequences,\nS. So imagine you have 40 sequences that\nare bacterial promoters, each",
    "start": "2334880",
    "end": "2344150"
  },
  {
    "text": "of 40 bases long, let's say. That's your S. And so\nwhat you want to do,",
    "start": "2344150",
    "end": "2352500"
  },
  {
    "text": "then, is consider a model that\nthere is a particular instance",
    "start": "2352500",
    "end": "2358810"
  },
  {
    "text": "of a motif you're\ntrying to discover, at a particular position in\neach one of those sequences.",
    "start": "2358810",
    "end": "2363990"
  },
  {
    "text": "Not necessarily\nthe same position, just some position\nin each sequence. And we're going to describe\nthe composition of that motif",
    "start": "2363990",
    "end": "2372529"
  },
  {
    "text": "by a weight matrix. OK, one of these matrices\nthat's of width, W, and then has the four rows specifying\nthe frequencies of the four",
    "start": "2372530",
    "end": "2381820"
  },
  {
    "text": "nucleotides at that position. The setup here is that you\nwant to calculate or think",
    "start": "2381820",
    "end": "2390860"
  },
  {
    "text": "about the probability of S comma\nA, S is the actual sequences,",
    "start": "2390860",
    "end": "2396600"
  },
  {
    "text": "and A is basically a\nvector that specifies the location of the motif\ninstance in each of those 40",
    "start": "2396600",
    "end": "2404810"
  },
  {
    "text": "sequences.  You want to calculate that,\nconditional on capital",
    "start": "2404810",
    "end": "2413260"
  },
  {
    "text": "theta-- which is\nour weight matrix. So that's going to\nbe, in this case, I think I made a motif of length\n8, and it's shown there in red.",
    "start": "2413260",
    "end": "2421850"
  },
  {
    "text": "There's going to be a\nweight matrix of length 8. And then there's going to\nbe some sort of background",
    "start": "2421850",
    "end": "2427200"
  },
  {
    "text": "frequency vector that\nmight be the background composition in the genome\nof E.coli DNA, for example.",
    "start": "2427200",
    "end": "2434270"
  },
  {
    "text": "And so then the probability\nof generating those sequences",
    "start": "2434270",
    "end": "2440220"
  },
  {
    "text": "together with that\nparticular locations",
    "start": "2440220",
    "end": "2447430"
  },
  {
    "text": "is going to be\nproportional to this. Basically, use the little\ntheta background vector",
    "start": "2447430",
    "end": "2455970"
  },
  {
    "text": "for all the positions, except\nthe specific positions that are inside the motif,\nstarting at position AK here.",
    "start": "2455970",
    "end": "2465770"
  },
  {
    "text": "And then you use the\nparticular column of the weight matrix\nfor those 8 positions, and then you go back to using\nthe background probabilities.",
    "start": "2465770",
    "end": "2475220"
  },
  {
    "text": "Question, yeah? AUDIENCE: Is this\nfor finding motifs",
    "start": "2475220",
    "end": "2480255"
  },
  {
    "text": "based on other known motifs? Or is this-- PROFESSOR: No, what\nwe're doing-- I'm sorry, I should've prefaced that.",
    "start": "2480255",
    "end": "2485545"
  },
  {
    "text": "We're doing de\nnovo motif finding. We're going to tell\nthe algorithm-- we're going to give the algorithm some\nsequences of a given length,",
    "start": "2485545",
    "end": "2492230"
  },
  {
    "text": "or it can even be\nof variable lengths, and we're going to\ngive it a guess of what the length of the motif is.",
    "start": "2492230",
    "end": "2497809"
  },
  {
    "text": "So we're going to\nsay, we think it's 8. That could come from\nstructural reasons. Or often you really\nhave no idea,",
    "start": "2497810",
    "end": "2503970"
  },
  {
    "text": "so you just guess that\nyou know, a lot of times it's kind of short, so we're\ngoing to go with 6 or 8, or you try different lengths.",
    "start": "2503970",
    "end": "2510450"
  },
  {
    "text": "Totally de novo motif finding. OK, so how does algorithm work? You have N sequences of\nlength, L. You guessed",
    "start": "2510450",
    "end": "2518619"
  },
  {
    "text": "that the motif has width, W.\nYou choose starting positions",
    "start": "2518620",
    "end": "2524330"
  },
  {
    "text": "at random-- OK, so this is\na vector, of the starting position in each\nsequence, we're going",
    "start": "2524330",
    "end": "2530210"
  },
  {
    "text": "to choose completely random\npositions within the end sequences. They have to be at\nleast W before the end--",
    "start": "2530210",
    "end": "2538760"
  },
  {
    "text": "so we'll have a whole motif,\nthat's just an accounting thing to make it simpler. And then you choose one\nof the sequence at random.",
    "start": "2538760",
    "end": "2547190"
  },
  {
    "text": "Say, the first sequence. You make a weight matrix\nmodel of width, W, from the instances in\nthe other sequences.",
    "start": "2547190",
    "end": "2555250"
  },
  {
    "text": "So for example--\nactually, I have slides on this, so we'll\njust do it with the slides, you'll see what this\nlooks like in a moment.",
    "start": "2555250",
    "end": "2562053"
  },
  {
    "text": "And so you have instances\nhere in the sequence, here in this one, here. You take all those, line them\nup, make a weight matrix out",
    "start": "2562053",
    "end": "2569930"
  },
  {
    "text": "of those, and then you score\nthe positions in sequence 1 for how well they match.",
    "start": "2569930",
    "end": "2575630"
  },
  {
    "text": "So, let me just do this. These are your motif instances. Again, totally random\nat the beginning.",
    "start": "2575630",
    "end": "2581069"
  },
  {
    "text": "Then you build a weight\nmatrix from those",
    "start": "2581070",
    "end": "2587840"
  },
  {
    "text": "by lining them up, and\njust counting frequencies. Then you pick a sequence\nat random-- yeah,",
    "start": "2587840",
    "end": "2594520"
  },
  {
    "text": "your weight matrix doesn't\ninclude that sequence, typically. And then you take\nyour theta matrix",
    "start": "2594520",
    "end": "2603690"
  },
  {
    "text": "and you slide it\nalong the sequence. You consider every\nsub-sequence of length,",
    "start": "2603690",
    "end": "2610300"
  },
  {
    "text": "W-- the one that\ngoes from 1 to W, to one that goes from\n2 to W plus 1, et",
    "start": "2610300",
    "end": "2616190"
  },
  {
    "text": "cetera, all the way\nalong the sequence, until you get to the end. And you calculate the\nprobability of that sequence,",
    "start": "2616190",
    "end": "2623760"
  },
  {
    "text": "using that likelihood\nthat I gave you before. So, it's basically the\nprobability generating sequence",
    "start": "2623760",
    "end": "2630140"
  },
  {
    "text": "where you use the background\nvector for all the positions, except for the particular\nmotif instance that you're",
    "start": "2630140",
    "end": "2637240"
  },
  {
    "text": "considering, and use the\nmotif model for that. Does that make sense? So, if you happen to have a good\nlooking occurrence of the motif",
    "start": "2637240",
    "end": "2646810"
  },
  {
    "text": "at this position,\nhere, in the sequence, then you would get\na higher likelihood.",
    "start": "2646810",
    "end": "2655560"
  },
  {
    "text": "So for example, if the motif\nwas, let's say it's 3 long,",
    "start": "2655560",
    "end": "2664010"
  },
  {
    "text": "and it happened\nto favor ACG, then",
    "start": "2664010",
    "end": "2680650"
  },
  {
    "text": "if you have a sequence\nhere that has, let's say, it's got\nTTT, that's going",
    "start": "2680650",
    "end": "2686364"
  },
  {
    "text": "to have a low probability\nin this motif. It's going to be 0.1 cubed. And then if you have an\noccurrence of, say, ACT,",
    "start": "2686364",
    "end": "2694151"
  },
  {
    "text": "that's going to have\na higher occurrence. It's going to be 0.7\ntimes 0.7 times 0.1. So, quite a bit higher.",
    "start": "2694152",
    "end": "2699260"
  },
  {
    "text": "So you start, it'll be\nlow for this triplet here-- so I'll put\na low value here.",
    "start": "2699260",
    "end": "2704369"
  },
  {
    "text": "TTA is also going to be low. TAC, also low. But ACT, that matches 2\nout of 3 to the motif.",
    "start": "2704370",
    "end": "2712540"
  },
  {
    "text": "It's going to be a lot better. And then CT is going to\nbe low again, et cetera.",
    "start": "2712540",
    "end": "2717855"
  },
  {
    "text": "So you just slide this along\nand calculate probabilities. ",
    "start": "2717855",
    "end": "2723130"
  },
  {
    "text": "And then what you do is you\nsample from this distribution. These probabilities don't\nnecessarily sum to 1.",
    "start": "2723130",
    "end": "2731500"
  },
  {
    "text": "But you re-normalize them\nso that they do sum to 1, you just add them up,\ndivide by the sum. Now they sum to 1.",
    "start": "2731500",
    "end": "2737589"
  },
  {
    "text": "And now you sample those\nsites in that sequence, according to that\nprobability distribution. ",
    "start": "2737590",
    "end": "2745560"
  },
  {
    "text": "Like I said, in this\ncase you might end up sampling-- that's the\nhighest probability site, so you might sample that.",
    "start": "2745560",
    "end": "2750770"
  },
  {
    "text": "But you also might sample\none of these other ones. It's unlikely you\nwould sample this one, because that's very low.",
    "start": "2750770",
    "end": "2756640"
  },
  {
    "text": "But you actually sometime\nsample one that's not so great.",
    "start": "2756640",
    "end": "2763650"
  },
  {
    "text": "So you sample a starting\nposition in that sequence, and you basically-- wherever\nyou would originally",
    "start": "2763650",
    "end": "2769490"
  },
  {
    "text": "assign in sequence 1, now you\nmove it to that new location. We've just changed\nthe assignment",
    "start": "2769490",
    "end": "2777380"
  },
  {
    "text": "of where we think the motif\nmight be in that sequence. And then you choose\nanother sequence",
    "start": "2777380",
    "end": "2782450"
  },
  {
    "text": "at random from your list. Often you go through the\nsequences sequentially, and then you make a new\nweight matrix model.",
    "start": "2782450",
    "end": "2789570"
  },
  {
    "text": "So how will that weight matrix\nmodel differ from the last one? Well it'll differ\nbecause the instance",
    "start": "2789570",
    "end": "2795730"
  },
  {
    "text": "of the motif in sequence 1\nis now at a new location, in general. I mean, you might have sampled\nthe exact same location you",
    "start": "2795730",
    "end": "2802776"
  },
  {
    "text": "started, but in\ngeneral it'll move. And so now, you'll got a\nslightly different weight matrix.",
    "start": "2802776",
    "end": "2808250"
  },
  {
    "text": "Most of the data going\ninto it, N minus 1, is going to be the same. But one of them is\ngoing to be different.",
    "start": "2808250",
    "end": "2815040"
  },
  {
    "text": "So it'll change a little bit. You make a new weight\nmatrix, and then you pick a new sequence.",
    "start": "2815040",
    "end": "2820810"
  },
  {
    "text": "You slide that weight\nmatrix along that sequence, you get this distribution, you\nsample from that distribution, and you keep going.",
    "start": "2820810",
    "end": "2827980"
  },
  {
    "text": "Yeah, this was described\nby Lorenz in 1993, and I'll post that paper.",
    "start": "2827980",
    "end": "2836670"
  },
  {
    "text": "OK, so you sample a\nportion with that, and you update the location. So now we sampled that\nreally high probably one,",
    "start": "2836670",
    "end": "2843130"
  },
  {
    "text": "so we moved the motif over\nto that new orange location, there.",
    "start": "2843130",
    "end": "2848742"
  },
  {
    "text": "I don't know if these\nanimations are helping at all. And then you update\nyour weight matrix. ",
    "start": "2848742",
    "end": "2857079"
  },
  {
    "text": "And then you iterate\nuntil convergence. So you typically have\na set of end sequences,",
    "start": "2857080",
    "end": "2864700"
  },
  {
    "text": "you go through them once. You have a weight matrix, and\nthen you go through them again. You go through a few times.",
    "start": "2864700",
    "end": "2870110"
  },
  {
    "text": "And maybe at a\ncertain point, you end re-sampling the\nsame sites as you did in the last iteration--\nsame exact sites.",
    "start": "2870110",
    "end": "2877910"
  },
  {
    "text": "You've converged. Or, you keep track\nof the theta matrices",
    "start": "2877910",
    "end": "2883265"
  },
  {
    "text": "that you get after going through\nthe whole set of sequences, and from one\niteration to the next,",
    "start": "2883265",
    "end": "2888460"
  },
  {
    "text": "the theta matrix hasn't\nreally changed much. You've converged. ",
    "start": "2888460",
    "end": "2896790"
  },
  {
    "text": "So let's do an example of this. Here I made up a motif, and\nthis is a representation",
    "start": "2896790",
    "end": "2903569"
  },
  {
    "text": "where the four bases have\nthese colors assigned to them. And you can see that this\nmotif is quite strong.",
    "start": "2903570",
    "end": "2909440"
  },
  {
    "text": "It really strongly\nprefers A at this position here, and et cetera. And I put it at the same\nposition in all the sequences,",
    "start": "2909440",
    "end": "2916550"
  },
  {
    "text": "just to make life simple. And then a former student\nin the lab, [INAUDIBLE],",
    "start": "2916550",
    "end": "2926010"
  },
  {
    "text": "he implemented the Gibb\nSample in Matlab, actually, and made a little video\nof what's going on.",
    "start": "2926010",
    "end": "2933730"
  },
  {
    "text": "So the upper part shows\nthe current weight matrix. Notice it's pretty\nrandom-looking",
    "start": "2933730",
    "end": "2941590"
  },
  {
    "text": "at the beginning. And the right parts\nshow where the motif",
    "start": "2941590",
    "end": "2950740"
  },
  {
    "text": "is, or the position that\nwe're currently considering. So this shows the\nposition that was last sampled in the last round.",
    "start": "2950740",
    "end": "2959800"
  },
  {
    "text": "And this shows the\nprobability density along each sequence of\nwhat's the probability that the motif occurs\nat each particular place",
    "start": "2959800",
    "end": "2968050"
  },
  {
    "text": "in the sequence. And that's what\nhappens over times. So it's obviously very\nfast, so I'll run it again",
    "start": "2968050",
    "end": "2975410"
  },
  {
    "text": "and maybe pause it partway. We're starting from a\nvery random-looking motif.",
    "start": "2975410",
    "end": "2981160"
  },
  {
    "text": " This is what you get after not\ntoo many iterations-- probably",
    "start": "2981160",
    "end": "2988420"
  },
  {
    "text": "like 100 or so. And now you can see your motif--\nyour weight matrix is now quite",
    "start": "2988420",
    "end": "2995730"
  },
  {
    "text": "biased, and now favors A at\nthis position, and so forth. And the locations of\nyour motif, most of them",
    "start": "2995730",
    "end": "3003680"
  },
  {
    "text": "are around this\nposition, around 6 or 7 in the sequence-- that's\nwhere we put the motif in.",
    "start": "3003680",
    "end": "3009450"
  },
  {
    "text": "But not all, some of them. And then you can see the\nprobabilities-- white is high, black is low-- in\nsome sequences,",
    "start": "3009450",
    "end": "3017010"
  },
  {
    "text": "it's very, very\nconfident, the motif is exactly at that position,\nlike this first sequence here. And others, it's\ngot some uncertainty",
    "start": "3017010",
    "end": "3023980"
  },
  {
    "text": "about where the motif might be. And then we let it\nrun a little bit more,",
    "start": "3023980",
    "end": "3029450"
  },
  {
    "text": "and it eventually\nconverges to being very confident that the motif\nhas the sequence, A C G T A G C",
    "start": "3029450",
    "end": "3037213"
  },
  {
    "text": "A, and that it occurs at\nthat particular position in the sequence. ",
    "start": "3037213",
    "end": "3045200"
  },
  {
    "text": "So who can tell me why\nthis actually works? We're choosing\npositions at random,",
    "start": "3045200",
    "end": "3051589"
  },
  {
    "text": "updating a weight matrix,\nwhy does that actually help you find the real motif\nthat's in these sequences?",
    "start": "3051590",
    "end": "3058646"
  },
  {
    "text": " Any ideas?",
    "start": "3058646",
    "end": "3064310"
  },
  {
    "text": "Or who can make an argument\nthat it shouldn't work? ",
    "start": "3064310",
    "end": "3069829"
  },
  {
    "text": "Yeah? What was your name again? AUDIENCE: Dan. PROFESSOR: Dan, yeah, go ahead. AUDIENCE: So, couldn't it,\nsort of, in a certain situation",
    "start": "3069830",
    "end": "3078678"
  },
  {
    "text": "have different sub-motifs\nthat are also sort of rich,",
    "start": "3078678",
    "end": "3085109"
  },
  {
    "text": "and because you're\nsampling randomly you might be stuck inside\nof those boundaries",
    "start": "3085110",
    "end": "3091960"
  },
  {
    "text": "where you're searching\nyour composition? PROFESSOR: Yeah, that's good.",
    "start": "3091960",
    "end": "3097350"
  },
  {
    "text": "So Dan's point is\nthat you can get stuck in sub-optimal\nsmaller or weaker motifs.",
    "start": "3097350",
    "end": "3105350"
  },
  {
    "text": "So that's certainly true. So you're saying, maybe\nthis example is artificial? Because I had started with\ntotally random sequences,",
    "start": "3105350",
    "end": "3111326"
  },
  {
    "text": "and I put a pretty strong\nmotif in a particular place, so there were no-- it's\nmore like that mountain,",
    "start": "3111326",
    "end": "3118079"
  },
  {
    "text": "that structure where there's\njust one motif to find. So it's perhaps an easy case.",
    "start": "3118080",
    "end": "3123580"
  },
  {
    "text": "But still, what I want to know\nis how does this algorithm, how did it actually\nfind that motif?",
    "start": "3123580",
    "end": "3129322"
  },
  {
    "text": "He implemented exactly that\nalgorithm that I described.",
    "start": "3129322",
    "end": "3135290"
  },
  {
    "text": "Why does it tend to go\ntowards [INAUDIBLE]? After a long time,\nremember it's a long time,",
    "start": "3135290",
    "end": "3142056"
  },
  {
    "text": "it's hundreds of iterations. AUDIENCE: So you're covering\na lot in the sequence,",
    "start": "3142056",
    "end": "3147614"
  },
  {
    "text": "just the random searching of\nthe sequence, when you're--",
    "start": "3147614",
    "end": "3154214"
  },
  {
    "text": "PROFESSOR: There\nare many iterations. You're considering\nmany possible locations within the sequences,\nthat's true.",
    "start": "3154214",
    "end": "3160620"
  },
  {
    "text": "But why does it eventually-- why\ndoes it converge to something?",
    "start": "3160620",
    "end": "3167238"
  },
  {
    "text": "AUDIENCE: I guess,\nbecause you're seeing your motif more\nplainly than you're",
    "start": "3167238",
    "end": "3172806"
  },
  {
    "text": "seeing other random motifs. So it will hit it more\nfrequently-- randomly.",
    "start": "3172806",
    "end": "3179610"
  },
  {
    "text": "And therefore,\nconverge [INAUDIBLE]. PROFESSOR: Yeah, that's true.",
    "start": "3179610",
    "end": "3184990"
  },
  {
    "text": "Can someone give a more\nintuition behind this?",
    "start": "3184990",
    "end": "3190760"
  },
  {
    "text": "Yeah? AUDIENCE: I just\nhave a question. Is each iteration\nan independent test? For example, if you iterate\nover the same sequence base",
    "start": "3190760",
    "end": "3200626"
  },
  {
    "text": "100 times, and you're updating\nyour weight matrix each time, does that mean it is the\nupdating the weight matrix also",
    "start": "3200626",
    "end": "3209485"
  },
  {
    "text": "taking into account\nthat the previous-- that this is the\nsame sample space? PROFESSOR: Yeah,\nthe weight matrix,",
    "start": "3209485",
    "end": "3215937"
  },
  {
    "text": "after you go through one\niteration of all the sequences, you have a weight matrix. You carry that over, you\ndon't start from scratch.",
    "start": "3215937",
    "end": "3223900"
  },
  {
    "text": "You bring that weight\nmatrix back up, and use that to score, let's\nsay, that first sequence.",
    "start": "3223900",
    "end": "3229150"
  },
  {
    "text": "Yeah, the weight matrix\njust keeps moving around.",
    "start": "3229150",
    "end": "3236089"
  },
  {
    "text": "Moves a little bit every\ntime you sample a sequence. AUDIENCE: So you constantly\nget a strong [INAUDIBLE]. PROFESSOR: Well, does it?",
    "start": "3236090",
    "end": "3242206"
  },
  {
    "text": "AUDIENCE: Well, I guess-- PROFESSOR: Would it\nconstantly get stronger? What's to make it get\nstronger or weaker?",
    "start": "3242206",
    "end": "3251190"
  },
  {
    "text": "I mean, this is sort\nof-- you're on the track. AUDIENCE: If it is\nrandom, then there's",
    "start": "3251190",
    "end": "3258008"
  },
  {
    "text": "some probability that you're\ngoing to find this motif again, at which point it\nwill get stronger. But, if it's-- given\nenough iterations,",
    "start": "3258008",
    "end": "3268234"
  },
  {
    "text": "it gets stronger as long as you\nhit different spots at random.",
    "start": "3268235",
    "end": "3275044"
  },
  {
    "text": "PROFESSOR: Yeah, yeah. That's what I'm-- I think\nthere was a comment. Jacob, yeah?",
    "start": "3275044",
    "end": "3280800"
  },
  {
    "text": "AUDIENCE: Well, you can think\nabout it as a random walk through the landscape. Eventually, it has\na high probability",
    "start": "3280800",
    "end": "3287436"
  },
  {
    "text": "of taking that motif, and\nupdating the [INAUDIBLE] direction, just from the\nprobability of [INAUDIBLE].",
    "start": "3287436",
    "end": "3294293"
  },
  {
    "text": "PROFESSOR: OK. AUDIENCE: And given\nthe [INAUDIBLE]. ",
    "start": "3294293",
    "end": "3300332"
  },
  {
    "text": "PROFESSOR: OK, let's say I\nhad 100 sequences of length,",
    "start": "3300332",
    "end": "3309240"
  },
  {
    "text": "I don't know, 30. And the width of the motif is 6. ",
    "start": "3309240",
    "end": "3320160"
  },
  {
    "text": "So here's our sequences. We choose random positions\nfor the start position,",
    "start": "3320160",
    "end": "3329800"
  },
  {
    "text": "and let's say it was\nthis example where the real motif, I put it right\nhere, and all the sequences.",
    "start": "3329800",
    "end": "3338932"
  },
  {
    "text": "That's where it starts.  Does this help? ",
    "start": "3338932",
    "end": "3346369"
  },
  {
    "text": "So it's 30 and 6, so there's\n25 possible start positions. I did that to make\nit a little easier.",
    "start": "3346370",
    "end": "3353059"
  },
  {
    "text": "So what would happen in\nthat first iteration? What w can you say\nabout what the weight matrix would look like?",
    "start": "3353060",
    "end": "3359730"
  },
  {
    "text": "It's going to be a width, W, you\nknow, columns 1, 2, 3, up to 6.",
    "start": "3359730",
    "end": "3366315"
  },
  {
    "text": " We're going to give it\n100 positions at random.",
    "start": "3366315",
    "end": "3373206"
  },
  {
    "text": "The motif is here--\nlet's say it's a very strong motif,\nthat's a 12-bit motif.",
    "start": "3373206",
    "end": "3379230"
  },
  {
    "text": "So it's 100%-- it's echo R1.",
    "start": "3379230",
    "end": "3384390"
  },
  {
    "text": "It's that.  What would that weight\nmatrix look like,",
    "start": "3384390",
    "end": "3391569"
  },
  {
    "text": "in this first iteration,\nwhen you first just sample the sites at random?",
    "start": "3391570",
    "end": "3397490"
  },
  {
    "text": "What kind of probabilities\nwould it have? AUDIENCE: [INAUDIBLE] PROFESSOR: Equal?",
    "start": "3397490",
    "end": "3404454"
  },
  {
    "text": "OK-- perfectly equal? AUDIENCE: Roughly. PROFESSOR: OK. Any box?",
    "start": "3404454",
    "end": "3410430"
  },
  {
    "start": "3410430",
    "end": "3418398"
  },
  {
    "text": "Are we likely to hit\nthe actual motif, ever, in that first encryption? AUDIENCE: No, because you\nhave a uniform probability,",
    "start": "3418398",
    "end": "3426282"
  },
  {
    "text": "of sampling. Well, uniform at each\none of the 25 positions? PROFESSOR: Right. AUDIENCE: Right now, you're\nnot sampling proportional",
    "start": "3426282",
    "end": "3434186"
  },
  {
    "text": "to the likelihood. PROFESSOR: So the chance of\nhitting the motif in any given",
    "start": "3434186",
    "end": "3439406"
  },
  {
    "text": "sequence is what? AUDIENCE: 1/25. PROFESSOR: 1/25. We have 100 sequences. AUDIENCE: So that's\nfour out of--",
    "start": "3439406",
    "end": "3445529"
  },
  {
    "text": "PROFESSOR: So on average,\nI'll hit the motif four times, right. The other 96 positions will\nbe essentially random, right?",
    "start": "3445529",
    "end": "3452680"
  },
  {
    "text": " So you initially said this was\ngoing to be uniform, right?",
    "start": "3452680",
    "end": "3460080"
  },
  {
    "text": "On average, 25% of each\nbase, plus or minus a little bit of sampling\nerror-- could be 23, 24, 26.",
    "start": "3460080",
    "end": "3467730"
  },
  {
    "text": "But now, you pointed out\nthat it's going to be four. You're going to hit the\nmotif four times, on average.",
    "start": "3467730",
    "end": "3475900"
  },
  {
    "text": "So, can you say anything more? AUDIENCE: Could you maybe\nhave a slightly bias towards G",
    "start": "3475900",
    "end": "3482737"
  },
  {
    "text": "on the first position? Slightly biased towards A\non the second and third?",
    "start": "3482737",
    "end": "3488701"
  },
  {
    "text": "Slightly biased towards T\non the fourth and fifth. And slightly biased\ntowards C in the sixth?",
    "start": "3488701",
    "end": "3494665"
  },
  {
    "text": "So it would be slightly biased-- PROFESSOR: Right, so\nremind me of your name? AUDIENCE: I'm Eric.",
    "start": "3494665",
    "end": "3500131"
  },
  {
    "text": "PROFESSOR: Eric,\nOK, so Eric says that because four\nof the sequences will have a G at\nthe first position,",
    "start": "3500132",
    "end": "3506160"
  },
  {
    "text": "because those are the ones\nwhere you sampled the motif, and the other 96 will have\neach of the four bases equally",
    "start": "3506160",
    "end": "3511809"
  },
  {
    "text": "likely, on average you have\nlike 24%-- plus 4 for G, right? Something like\n28%-- this will be",
    "start": "3511810",
    "end": "3518640"
  },
  {
    "text": "28%, plus or minus a little bit. And these other ones will be\nwhatever that works out to be,",
    "start": "3518640",
    "end": "3530520"
  },
  {
    "text": "23 or something like\nthat-- 23-ish, on average. Again, it may not\ncome out exactly",
    "start": "3530520",
    "end": "3536540"
  },
  {
    "text": "like-- G may not be number\none, but it's more often going to be number one\nthan any other base.",
    "start": "3536540",
    "end": "3541680"
  },
  {
    "text": "And on average, it'll be more\nlike 28% rather than 25%. And similarly for\nposition two, A",
    "start": "3541680",
    "end": "3548600"
  },
  {
    "text": "will be 28%, and\nthree, and et cetera. And then the sixth\nwill be-- C will",
    "start": "3548600",
    "end": "3556264"
  },
  {
    "text": "have a little bit of a bias. OK, so even in that\nfirst round, when you're sampling\nthat first sequence,",
    "start": "3556264",
    "end": "3562380"
  },
  {
    "text": "the matrix is going\nto be slightly biased toward the motif-- depending\nhow the sampling went. You might not have hit any\ninstances of motif, right?",
    "start": "3562380",
    "end": "3570100"
  },
  {
    "text": "But often, it'll\nbe a little bit-- Is that enough of\na bias to give you",
    "start": "3570100",
    "end": "3577760"
  },
  {
    "text": "a good chance of selecting the\nmotif in that first sequence?",
    "start": "3577760",
    "end": "3582994"
  },
  {
    "text": "AUDIENCE: You mean in\nthe first iteration? PROFESSOR: Let's say the first\nrandom sequence size sample.",
    "start": "3582994",
    "end": "3588155"
  },
  {
    "text": "No. You're shaking your head. Not enough of a\nbias because-- it's",
    "start": "3588155",
    "end": "3595501"
  },
  {
    "text": "0.28 over 0.25 to the\nsixth power, right?",
    "start": "3595501",
    "end": "3601955"
  },
  {
    "text": "So it's like-- AUDIENCE: The likelihood\nis still close 1. Like, that's [INAUDIBLE] ratio.",
    "start": "3601955",
    "end": "3607025"
  },
  {
    "text": "PROFESSOR: So it's something\nlike 1.1 to the sixth, or something like that. So it might be close to 2,\nmight be twice as likely.",
    "start": "3607025",
    "end": "3613809"
  },
  {
    "text": "But still, there's 25 positions. Does that make sense? So it's quite likely\nthat you won't",
    "start": "3613810",
    "end": "3621790"
  },
  {
    "text": "sample the motif in that first--\nyou'll sample something else. Which will take it away\nin some random direction.",
    "start": "3621790",
    "end": "3631440"
  },
  {
    "text": "So who can tell me how this\nactually ends up working? Why does it actually\nconverge eventually, if you get it long enough?",
    "start": "3631440",
    "end": "3637710"
  },
  {
    "start": "3637710",
    "end": "3652622"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. ",
    "start": "3652622",
    "end": "3658078"
  },
  {
    "text": "PROFESSOR: So the\ninformation content, what will happen to that? So the information content,\nif it was completely random--",
    "start": "3658078",
    "end": "3665074"
  },
  {
    "text": "we said that would be uniform. That would be zero\ninformation content, right? This matrix, which has around\n28% at six different positions,",
    "start": "3665074",
    "end": "3672329"
  },
  {
    "text": "will have an information content\nthat's low, but non-zero. It might end up being\nlike 1 bit, or something.",
    "start": "3672330",
    "end": "3679700"
  },
  {
    "text": "And if you then sample motifs\nthat are not the motif, they will tend to reduce\nthe information content,",
    "start": "3679700",
    "end": "3685640"
  },
  {
    "text": "tend to bring it\nback toward random. If you sample locations\nthat have the motif,",
    "start": "3685640",
    "end": "3694680"
  },
  {
    "text": "what will that do to\nthe information content? Boost it. So what would you expect if we\nwere to plot the information",
    "start": "3694680",
    "end": "3700620"
  },
  {
    "text": "content over time, what\nwould that look like? ",
    "start": "3700620",
    "end": "3706090"
  },
  {
    "text": "AUDIENCE: It should\ntrend upwards, but it could fluctuate. PROFESSOR: Yeah.",
    "start": "3706090",
    "end": "3711378"
  },
  {
    "text": "AUDIENCE: Over the\nnumber of iterations? ",
    "start": "3711378",
    "end": "3719929"
  },
  {
    "text": "PROFESSOR: I think\nI blocked it here. Let me see if I\ncan-- Let's try this. I think I plotted it.",
    "start": "3719929",
    "end": "3725050"
  },
  {
    "text": " OK, never mind. I wanted to keep\nit very mysterious,",
    "start": "3725050",
    "end": "3731450"
  },
  {
    "text": "so you guys have\nto figure it out. The answer is that it will--\nbasically what happens is you",
    "start": "3731450",
    "end": "3744430"
  },
  {
    "text": "start with a weight\nmatrix like this. A lot of times, because the bias\nfor the motif is quite weak,",
    "start": "3744430",
    "end": "3751390"
  },
  {
    "text": "a lot of times\nyou'll sample-- even for a sequence,\nwhat matters is-- like, if you had a sequence\nwhere the location, initially,",
    "start": "3751390",
    "end": "3757450"
  },
  {
    "text": "was not the motif, and then you\nsample another location that's not the motif, that's not\nreally going to change anything.",
    "start": "3757450",
    "end": "3762910"
  },
  {
    "text": "It'll change things\na little bit, but not in any\nparticular direction. What really matters is\nwhen you get to a sequence",
    "start": "3762910",
    "end": "3768330"
  },
  {
    "text": "where you already had the motif,\nif you now sample one that's not the motif, your information\ncontent will get weaker.",
    "start": "3768330",
    "end": "3774000"
  },
  {
    "text": "It will become more uniform. But if you have a sequence\nwhere it wasn't the motif,",
    "start": "3774000",
    "end": "3780410"
  },
  {
    "text": "but now you happen to sample the\nmotif, then it'll get stronger.",
    "start": "3780410",
    "end": "3785440"
  },
  {
    "text": "And when it gets\nstronger, it will then be more likely to pick the\nmotif in the next sequence,",
    "start": "3785440",
    "end": "3791819"
  },
  {
    "text": "and so on. So basically what happens\nto the information content is that over many\niterations-- it starts near 0.",
    "start": "3791820",
    "end": "3799440"
  },
  {
    "text": "And can occasionally\ngo up a little bit. And then once it exceeds the\nthreshold, it goes like that.",
    "start": "3799440",
    "end": "3805070"
  },
  {
    "text": " So what happens is it\nstumbles onto a few instances",
    "start": "3805070",
    "end": "3810769"
  },
  {
    "text": "of the motif that bias\nthe weight matrix. And if they don't\nbias it enough, it'll just fall off that.",
    "start": "3810770",
    "end": "3816874"
  },
  {
    "text": "It's like trying to\nclimb the mountain-- but it's walking in\na random direction. So sometimes it will turn\naround and go back down.",
    "start": "3816874",
    "end": "3822710"
  },
  {
    "text": "But then when it gets high\nenough, it'll be obvious. Once you have a, say, 20\ntimes greater likelihood",
    "start": "3822710",
    "end": "3832020"
  },
  {
    "text": "of picking that motif than\nany other sequence, most of the time you will pick it. And very soon,\nit'll be stronger.",
    "start": "3832020",
    "end": "3839576"
  },
  {
    "text": "And the next round,\nwhen it's stronger, you'll have a greater\nbias for picking the motif, and so forth.",
    "start": "3839576",
    "end": "3844770"
  },
  {
    "text": "Question? AUDIENCE: For this\nspecific example, M is much greater\nthan L minus W.",
    "start": "3844770",
    "end": "3851850"
  },
  {
    "text": "How true is that for\npractical examples? PROFESSOR: That's a\nvery good question.",
    "start": "3851850",
    "end": "3858380"
  },
  {
    "text": "There is sometimes-- depends on\nhow commonly your motif occurs",
    "start": "3858380",
    "end": "3863950"
  },
  {
    "text": "in the genome, and how\ngood your data is, really, and what the source\nof your data is.",
    "start": "3863950",
    "end": "3870370"
  },
  {
    "text": "So sometimes it can\nbe very limited, sometimes-- If you do\nChIP-Seq you might have 10,000",
    "start": "3870370",
    "end": "3877190"
  },
  {
    "text": "peaks that you're\nanalyzing, or something. So you could have a huge number. But on the other hand, if\nyou did some functional",
    "start": "3877190",
    "end": "3883500"
  },
  {
    "text": "assay that's quite laborious for\na motif that drives luciferase, or something, and you\ncan only test a few,",
    "start": "3883500",
    "end": "3890770"
  },
  {
    "text": "you might only have 10. So it varies all over the map.",
    "start": "3890770",
    "end": "3895980"
  },
  {
    "text": "So that's a good question. We'll come back to\nthat in a little bit. Simona?",
    "start": "3895980",
    "end": "3901200"
  },
  {
    "text": "AUDIENCE: If you\nhave a short motif, does it make sense,\nthen, to reduce the number of\nsequences you have? Because maybe it won't converge?",
    "start": "3901200",
    "end": "3907880"
  },
  {
    "text": "PROFESSOR: Reduce the\nnumber of sequences? What do you people\nthink about that? Is that a good\nidea or a bad idea? ",
    "start": "3907880",
    "end": "3916790"
  },
  {
    "text": "It's true that it\nmight converge faster with a smaller\nnumber of sequences,",
    "start": "3916790",
    "end": "3922640"
  },
  {
    "text": "but you also might\nnot find it all. So generally you're\nlosing information, so you want to\nhave more sequences",
    "start": "3922640",
    "end": "3930319"
  },
  {
    "text": "up to a certain point. Let's just do a couple more\nexamples, and I'll come back. Those are both good questions.",
    "start": "3930320",
    "end": "3936080"
  },
  {
    "text": "OK, so here's this weak motif. So this is the one where\nyou guys couldn't see it when I just put\nthe sequences up.",
    "start": "3936080",
    "end": "3941240"
  },
  {
    "text": "You can only see it\nwhen it's aligned-- it's this thing with GGC, here.",
    "start": "3941240",
    "end": "3947190"
  },
  {
    "text": "And here is, again,\nthe Gibbs Sampler.",
    "start": "3947190",
    "end": "3952410"
  },
  {
    "text": "And what happened? ",
    "start": "3952410",
    "end": "3960980"
  },
  {
    "text": "Who can summarize\nwhat happened here? ",
    "start": "3960980",
    "end": "3972926"
  },
  {
    "text": "Yeah, David? AUDIENCE: It didn't converge. PROFESSOR: Yeah, it\ndidn't quite converge. The motif is usually\non the right side,",
    "start": "3972926",
    "end": "3981130"
  },
  {
    "text": "and it found something\nthat's like the motif. But it's not quite right--\nit's got that A, it's G A G C,",
    "start": "3981130",
    "end": "3989329"
  },
  {
    "text": "it should be G G C. And so\nit sampled some other things, and it got off\ntrack a little bit,",
    "start": "3989330",
    "end": "3995050"
  },
  {
    "text": "because probably\nby chance, there were some things that looked\na little bit like the motif, and it was finding\nsome instances of that,",
    "start": "3995050",
    "end": "4001089"
  },
  {
    "text": "and some instances\nof the real motif. And yeah, it didn't\nquite converge. And you can see this\nprobability vectors here,",
    "start": "4001090",
    "end": "4010220"
  },
  {
    "text": "they have multiple white\ndots in many of the rows. So it doesn't know,\nit's uncertain. So it keeps bouncing around.",
    "start": "4010220",
    "end": "4015770"
  },
  {
    "text": "So it didn't really\nconverge, it was too weak, it was too challenging\nfor the algorithm.",
    "start": "4015770",
    "end": "4020930"
  },
  {
    "text": " This is just a summary of the\nGibb Sampler, how it works.",
    "start": "4020930",
    "end": "4030750"
  },
  {
    "text": "It's not guaranteed to converge\nto the same motif every time. So what you generally will want\nto do is run it several times,",
    "start": "4030750",
    "end": "4039390"
  },
  {
    "text": "and nine out of 10 times,\nyou get the same motif. You should trust that.",
    "start": "4039390",
    "end": "4045400"
  },
  {
    "text": "Go ahead. AUDIENCE: Over here, are we\noptimizing for convergence",
    "start": "4045400",
    "end": "4053062"
  },
  {
    "text": "of the value of the\ninformation content? PROFESSOR: No, the\ninformation content is just describing-- it's\njust a handy single number",
    "start": "4053062",
    "end": "4061880"
  },
  {
    "text": "description of how biased\nthe weight matrix is. So it's not actually\ndirectly being optimized.",
    "start": "4061880",
    "end": "4069200"
  },
  {
    "text": "But it turns out that\nthis way of sampling tends to increase\ninformation content.",
    "start": "4069200",
    "end": "4078349"
  },
  {
    "text": "It's sort of a self-reinforcing\nkind of a thing. ",
    "start": "4078350",
    "end": "4084366"
  },
  {
    "text": "But it's not\ndirectly doing that. However MEME, more or\nless, directly does that.",
    "start": "4084366",
    "end": "4090110"
  },
  {
    "text": "The problem with that is\nthat, where do you start? Imagine an algorithm\nlike this, but where",
    "start": "4090110",
    "end": "4096500"
  },
  {
    "text": "you deterministically--\ninstead of sampling from the positions in the\nsequence, where it might have a motif in proportion\nto probabilities,",
    "start": "4096500",
    "end": "4102310"
  },
  {
    "text": "you just chose the one that\nhad the highest probability. That's more or less\nwhat MEME does. And so what are\nthe pros and cons",
    "start": "4102310",
    "end": "4109960"
  },
  {
    "text": "of that approach,\nversus this one? ",
    "start": "4109960",
    "end": "4119443"
  },
  {
    "text": "Any ideas? ",
    "start": "4119444",
    "end": "4130312"
  },
  {
    "text": "OK, one of the disadvantages\nis that the initial choice of-- how you're initially\nseeding your matrix,",
    "start": "4130312",
    "end": "4138420"
  },
  {
    "text": "matters a lot. That slight bias-- it might\nbe that you had a slight bias,",
    "start": "4138420",
    "end": "4145290"
  },
  {
    "text": "and it didn't come out\nbeing G was number one. It was actually-- T was\nnumber one, just because",
    "start": "4145290",
    "end": "4150850"
  },
  {
    "text": "of the quirks of the sampling. So what would this\nbe, 31 or something?",
    "start": "4150850",
    "end": "4158880"
  },
  {
    "text": "Anyway, it's higher\nthan these other guys. And so then you're always\npicking the highest.",
    "start": "4158880",
    "end": "4167549"
  },
  {
    "text": "It'll become a\nself-fulfilling prophecy. So that's the problem with MEME. So the way that MEME\ngets around that,",
    "start": "4167550",
    "end": "4173210"
  },
  {
    "text": "is it uses multiple\ndifferent seeding, multiple different\nstarting points, and goes to the end\nwith all of them.",
    "start": "4173210",
    "end": "4179568"
  },
  {
    "text": "And then it evaluates, how good\na model did we get at the end? And whichever was the\nbest one, it takes that.",
    "start": "4179569",
    "end": "4184818"
  },
  {
    "text": "So it actually takes\nlonger, but you only need to run it once\nbecause it's deterministic.",
    "start": "4184819",
    "end": "4190330"
  },
  {
    "text": "You use a deterministic\nset of starting points, you run a deterministic\nalgorithm, and then you evaluate.",
    "start": "4190330",
    "end": "4197650"
  },
  {
    "text": "The Gibbs, it can\ngo off on a tangent, but because it's\nsampling so randomly,",
    "start": "4197650",
    "end": "4203590"
  },
  {
    "text": "it often will fall off,\nthen, and come back to something that's\nmore uniform. And when it's a\nuniform matrix, it's",
    "start": "4203590",
    "end": "4208640"
  },
  {
    "text": "really sampling\ncompletely randomly, exploring the space\nin an unbiased way. Tim?",
    "start": "4208640",
    "end": "4213724"
  },
  {
    "text": "AUDIENCE: For genomes that have\ninherent biases that you know going in, do you precalculate--\ndo you just recalculate",
    "start": "4213724",
    "end": "4223160"
  },
  {
    "text": "the weight matrix before, to\n[? affect  those  classes? ?]",
    "start": "4223160",
    "end": "4228730"
  },
  {
    "text": "For example, if you\nhad 80% AT content, then you're not looking\nfor-- you know, immediately,",
    "start": "4228730",
    "end": "4236895"
  },
  {
    "text": "that you're going to hit an A\nor a T off the first iteration. So how do you deal with that?",
    "start": "4236895",
    "end": "4243237"
  },
  {
    "text": "PROFESSOR: Good question. ",
    "start": "4243237",
    "end": "4248800"
  },
  {
    "text": "So these are some features\nthat affect motif finding. I think that we've now hit at\nleast a few of these-- number",
    "start": "4248800",
    "end": "4259485"
  },
  {
    "text": "of sequences, length of\nsequences, information content, and motif, and basically\nwhether the background is biased",
    "start": "4259485",
    "end": "4267140"
  },
  {
    "text": "or not. So, in general, higher\ninformation content motifs,",
    "start": "4267140",
    "end": "4273130"
  },
  {
    "text": "or lower information\ncontent, are easier to find-- who thinks higher?",
    "start": "4273130",
    "end": "4278792"
  },
  {
    "text": "Who thinks lower?  Someone, can you explain?",
    "start": "4278792",
    "end": "4286054"
  },
  {
    "text": "AUDIENCE: I don't know. I just guessed. PROFESSOR: Just a guess? OK, in back, can you explain?",
    "start": "4286055",
    "end": "4291635"
  },
  {
    "text": "Lower? AUDIENCE: Low\ninformation content is basically very uniform. PROFESSOR: Low information\nmeans nearly uniform-- right,",
    "start": "4291636",
    "end": "4298324"
  },
  {
    "text": "those are very hard to find. That's like that GGC one.",
    "start": "4298324",
    "end": "4303660"
  },
  {
    "text": "The high information\ncontent motif, those are the very strong\nones, like that first one. Those are much easier to find.",
    "start": "4303660",
    "end": "4308700"
  },
  {
    "text": "Because when you\nstumble on to them, it biases the matrix more, and\nyou rapidly converge to that.",
    "start": "4308700",
    "end": "4314870"
  },
  {
    "text": "OK, high information\nis easy to find. So if I have one\nmotif per sequence,",
    "start": "4314870",
    "end": "4321199"
  },
  {
    "text": "what about the length\nof the sequence? Is longer or shorter better? ",
    "start": "4321200",
    "end": "4327930"
  },
  {
    "text": "Is long better? Who thinks shorter is better? Shorter-- can you\nexplain why short?",
    "start": "4327930",
    "end": "4334291"
  },
  {
    "text": "AUDIENCE: Shouldn't it be\nthe smaller the search space, the fewer the problems? PROFESSOR: Exactly, the\nshorter the search space,",
    "start": "4334291",
    "end": "4341207"
  },
  {
    "text": "and your motif, there's\nless place for it to hide. You're more likely to sample it. Shorter is better.",
    "start": "4341207",
    "end": "4347510"
  },
  {
    "text": "If you think about-- if you\nhave a motif like TATA, which is typically 30\nbases from the TSS,",
    "start": "4347510",
    "end": "4355770"
  },
  {
    "text": "if you happen to know that, and\nyou give it plus 1 to minus 50, you're giving it a\nsmall region, you",
    "start": "4355770",
    "end": "4361820"
  },
  {
    "text": "can easily find the TATA box. If you give it plus 1 to\nminus 2,000 or something,",
    "start": "4361820",
    "end": "4367890"
  },
  {
    "text": "you may not find it. It's diluted, essentially. Number of sequences--\nthe more the better.",
    "start": "4367890",
    "end": "4374486"
  },
  {
    "text": "This is a little more\nsubtle, as Simona was saying. It affects convergence\ntime, and so forth. But in general, the\nmore the better.",
    "start": "4374486",
    "end": "4381870"
  },
  {
    "text": "And if you guessed the\nwrong length of your matrix, that makes it worse\nthan if you guess",
    "start": "4381870",
    "end": "4389370"
  },
  {
    "text": "the right length in\neither direction. For example, it's six-base\nmotif, you guess three.",
    "start": "4389370",
    "end": "4395690"
  },
  {
    "text": "The information content,\neven if it's a 12-bit motif, there's only six bits that\nyou could hope to find,",
    "start": "4395690",
    "end": "4401256"
  },
  {
    "text": "because you can only find\nthree of those positions. So clearly, effectively\nit's a smaller information",
    "start": "4401256",
    "end": "4407176"
  },
  {
    "text": "content, and much\nharder to find. And vice versa. ",
    "start": "4407176",
    "end": "4412719"
  },
  {
    "text": "Another thing that\noccurs in practice is what's called shifted motifs.",
    "start": "4412720",
    "end": "4419350"
  },
  {
    "text": "Your motif is G A A T T C.\nImagine in your first iteration",
    "start": "4419350",
    "end": "4424740"
  },
  {
    "text": "you happen to hit several of\nthese sequences, starting here. You hit the motif,\nbut off by two",
    "start": "4424740",
    "end": "4432330"
  },
  {
    "text": "at several different places. That'll bias first\nposition to be A, and the second position\nto be T, and so forth.",
    "start": "4432330",
    "end": "4439230"
  },
  {
    "text": "And then you tend to find other\nshifted versions of that motif. You may well converge to this--\nA T C C N N, or something",
    "start": "4439230",
    "end": "4446699"
  },
  {
    "text": "like that-- which\nis not quite right. It's close, you're very\nclose, but not quite right.",
    "start": "4446700",
    "end": "4452950"
  },
  {
    "text": "And it's not as information\nrich as the real motif. Because it's got those two N's\nat the end, instead of G A.",
    "start": "4452950",
    "end": "4459480"
  },
  {
    "text": "So one thing that's\ndone in practice is a lot of times, every so\noften, the algorithm will say,",
    "start": "4459480",
    "end": "4466020"
  },
  {
    "text": "what would happen if we\nshifted all of our positions over to the left by one or two? Or to the right by one or two?",
    "start": "4466020",
    "end": "4472410"
  },
  {
    "text": "Would the information\ncontent go up? If so, let's do that.",
    "start": "4472410",
    "end": "4477540"
  },
  {
    "text": "So basically, shifted\nversions of the motif become local, near-optimal solutions.",
    "start": "4477540",
    "end": "4483630"
  },
  {
    "text": "So you have to avoid them. And biased background\ncomposition is very difficult to deal with.",
    "start": "4483630",
    "end": "4489120"
  },
  {
    "text": "So I will just give\nyou one or two more",
    "start": "4489120",
    "end": "4494650"
  },
  {
    "text": "examples of that in a\nmoment, and continue. So in practice, I would say\nthe Gibbs Sampler is sometimes",
    "start": "4494650",
    "end": "4502340"
  },
  {
    "text": "used, or AlignACE, which is\na version of Gibbs Sampler. But probably more\noften, people use",
    "start": "4502340",
    "end": "4508530"
  },
  {
    "text": "an algorithm called MEME, which\nis this EM algorithm, which, like I said, is\ndeterministic, so you always",
    "start": "4508530",
    "end": "4514550"
  },
  {
    "text": "get the same answer,\nwhich makes you feel good. May or may not always be right,\nbut you can try it out here",
    "start": "4514550",
    "end": "4521280"
  },
  {
    "text": "at this website. And actually, the\nFraenkel Lab has a very nice website\ncalled WebMotifs",
    "start": "4521280",
    "end": "4526350"
  },
  {
    "text": "that runs several different\nmotif finders including, like I said, a MEME\nand AlignACE, which",
    "start": "4526350",
    "end": "4533080"
  },
  {
    "text": "is similar to Gibbs,\nas well as some others. And it integrates the output,\nso that's often a handy thing",
    "start": "4533080",
    "end": "4540190"
  },
  {
    "text": "to use. You can read about them there. And then I just wanted\nto say a couple words--",
    "start": "4540190",
    "end": "4548370"
  },
  {
    "text": "this is related to Tim's comment\nabout the biased background. How do you actually\ndeal with that?",
    "start": "4548370",
    "end": "4556130"
  },
  {
    "text": "And this related to this notion\nof a mean bit score of a motif.",
    "start": "4556130",
    "end": "4564360"
  },
  {
    "text": "So if I were to give you a\nmotif model, P, and a background",
    "start": "4564360",
    "end": "4570040"
  },
  {
    "text": "model, q, then the natural\nscoring system, if you wanted additives scores, instead\nof multiplicative,",
    "start": "4570040",
    "end": "4576940"
  },
  {
    "text": "you would just take the log. So log P over q, I would argue,\nis natural additive scores.",
    "start": "4576940",
    "end": "4583050"
  },
  {
    "text": "And that's often what you'll\nsee in a weight matrix-- you'll see log probabilities, or logs\nof ratios of probabilities.",
    "start": "4583050",
    "end": "4590116"
  },
  {
    "text": "And so then you\njust add them up, and it makes life a bit simpler. And so then, if you were to\ncalculate what's the mean bit",
    "start": "4590116",
    "end": "4596350"
  },
  {
    "text": "score-- if I had a bunch\nof instances of a motif, it will be given by this formula\nthat's here in the upper right.",
    "start": "4596350",
    "end": "4606579"
  },
  {
    "text": "So that's your score. And this is the mean,\nwhere you're sampling over",
    "start": "4606580",
    "end": "4611710"
  },
  {
    "text": "the probability in using the\nmotif model, probabilities.",
    "start": "4611710",
    "end": "4618150"
  },
  {
    "text": "So it turns out, then, that\nif qk, your background, is uniform, motif of width\nw-- so its probability",
    "start": "4618150",
    "end": "4626360"
  },
  {
    "text": "of any w-mer, is\n1/4 to the w, then it's true that the\nmean bit-score is",
    "start": "4626360",
    "end": "4633410"
  },
  {
    "text": "2w minus the entropy\nof the motif, which is the same as the information\ncontent of the motif,",
    "start": "4633410",
    "end": "4640270"
  },
  {
    "text": "using our previous definition. So that's just a\nhandy relationship.",
    "start": "4640270",
    "end": "4647380"
  },
  {
    "text": "And you can do a little algebra\nto show that, if you want. So basically summation Pk\nlog Pk over qk-- this log,",
    "start": "4647380",
    "end": "4663150"
  },
  {
    "text": "you turn that into\na difference-- so that summation Pk\nlog Pk minus Pk log qk.",
    "start": "4663150",
    "end": "4676889"
  },
  {
    "text": "And then you can do some\nrearrangement, and sum them up, and you'll get this formula.",
    "start": "4676890",
    "end": "4682920"
  },
  {
    "text": "I'll leave that as an exercise,\nand any questions on it, we can do it next time.",
    "start": "4682920",
    "end": "4688790"
  },
  {
    "text": "So what I wanted to get to\nis sort of this big question that I posed earlier--\nwhat's the use of knowing",
    "start": "4688790",
    "end": "4695050"
  },
  {
    "text": "the information\ncontent of a motif? And the answer is that one use\nis that it's true, in general,",
    "start": "4695050",
    "end": "4706630"
  },
  {
    "text": "that the motif with\nn bits of information will occur about once every 2 to\nthe n bases of random sequence.",
    "start": "4706630",
    "end": "4714140"
  },
  {
    "text": "So we said a six-cutter\nrestriction enzyme, echo R1,",
    "start": "4714140",
    "end": "4719190"
  },
  {
    "text": "has an information\ncontent of 12 bits. So by this rule, it should\noccur about once every",
    "start": "4719190",
    "end": "4725260"
  },
  {
    "text": "to 2 to the 12th\nbases of sequence. And if you know your powers\nof 2, which you should all commit to memory,\nthat's about 4,000.",
    "start": "4725260",
    "end": "4734370"
  },
  {
    "text": "2 to the 12th is 4 to\nthe sixth, is 4,096. So it'll occur about once\nevery 4 [? kb, ?] which",
    "start": "4734370",
    "end": "4740680"
  },
  {
    "text": "if you've ever cut E. coli\nDNA, you know is about right-- your fragments come out\nto be about 4 [? kb. ?]",
    "start": "4740680",
    "end": "4747610"
  },
  {
    "text": "So this turns out to be\nstrictly true for any motif that you can represent\nby a regular expression,",
    "start": "4747610",
    "end": "4754920"
  },
  {
    "text": "like a precise\nmotif, or something where you have a degenerate R\nor Y or N in it, still true.",
    "start": "4754920",
    "end": "4761659"
  },
  {
    "text": "And if you have a more\ngeneral motif that's described by weight matrix, then\nyou have to define a threshold,",
    "start": "4761660",
    "end": "4767490"
  },
  {
    "text": "and it's roughly\ntrue, but not exactly.",
    "start": "4767490",
    "end": "4772960"
  },
  {
    "text": "All right, so what do you\ndo when the background composition is biased,\nlike Tim was saying?",
    "start": "4772960",
    "end": "4778320"
  },
  {
    "text": "What if it's 80%, A plus T? So then, it turns out that this\nmean bit-score is a good way",
    "start": "4778320",
    "end": "4787310"
  },
  {
    "text": "to go. So like I said,\nthe mean bit-score",
    "start": "4787310",
    "end": "4792460"
  },
  {
    "text": "equals the information\ncontent in this special case, where the background is uniform.",
    "start": "4792460",
    "end": "4798720"
  },
  {
    "text": "But if the background\nis not uniform, then you can still calculate\nthis mean bit-score,",
    "start": "4798720",
    "end": "4805950"
  },
  {
    "text": "and it'll still be meaningful. But now it's called\nsomething else-- it's called relative entropy.",
    "start": "4805950",
    "end": "4812730"
  },
  {
    "text": "Actually it has several\nnames, relative entropy, Kullback-Leibler\ndistance is another, and information\nfor discrimination,",
    "start": "4812730",
    "end": "4818900"
  },
  {
    "text": "depending whether you're\nreading the Double E literature, or\nstatistics, or whatever.",
    "start": "4818900",
    "end": "4824410"
  },
  {
    "text": "And so it turns out\nthat if you have a very biased composition--\nso here's one that's 75% A T,",
    "start": "4824410",
    "end": "4829969"
  },
  {
    "text": "probability of A and T\nare 3/8, C and G are 1/8. ",
    "start": "4829970",
    "end": "4835990"
  },
  {
    "text": "If your motif is just\nC 100% of the time, your information content\nby the original formula",
    "start": "4835990",
    "end": "4843920"
  },
  {
    "text": "that I gave you,\nwould be 2 bits. However, the relative\nentropy will be 3 bits,",
    "start": "4843920",
    "end": "4853560"
  },
  {
    "text": "if you just plug in these\nnumbers into this formula, it will turn out to be 3 bits.",
    "start": "4853560",
    "end": "4860490"
  },
  {
    "text": "My question is, which\none better describes the frequency of C in\nthe background sequence?",
    "start": "4860490",
    "end": "4868400"
  },
  {
    "text": "Frequency of this\nmotif-- the motif is just a C. You can see that\nthe relative entropy says",
    "start": "4868400",
    "end": "4874050"
  },
  {
    "text": "that actually, that's\nstronger than it appears. Because it's a C, and\nthat's a rare nucleotide, it's actually stronger\nthan it appears.",
    "start": "4874050",
    "end": "4880225"
  },
  {
    "text": "And so 2 to the 3rd\nis a better estimate of its frequency than 2 squared. So relative entropy.",
    "start": "4880225",
    "end": "4885840"
  },
  {
    "text": "So what you can do\nwhen you run a motif finder in a sequence\nof biased composition, you can say, what's\nthe relative entropy",
    "start": "4885840",
    "end": "4891930"
  },
  {
    "text": "of this motif at the end? And look at the ones\nthat are strong. We'll come back to this\na little more next time.",
    "start": "4891930",
    "end": "4900580"
  },
  {
    "text": "Next time, we'll talk\nabout hidden Markov models, and please take a\nlook at the readings.",
    "start": "4900580",
    "end": "4905590"
  },
  {
    "text": "And please, those who\nare doing projects, look for more\ndetailed instructions",
    "start": "4905590",
    "end": "4910760"
  },
  {
    "text": "to be posted tonight. Thanks. ",
    "start": "4910760",
    "end": "4925694"
  }
]