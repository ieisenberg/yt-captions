[
  {
    "start": "0",
    "end": "9388"
  },
  {
    "text": "MICHALE FEE: All right, let's\ngo ahead and get started. So we're starting\na new topic today. This is actually one of\nmy favorite lectures,",
    "start": "9388",
    "end": "15450"
  },
  {
    "text": "one of my favorite subjects\nin computational neuroscience. All right, so brief recap\nof what we've been doing.",
    "start": "15450",
    "end": "23590"
  },
  {
    "text": "So we've been working on circuit\nmodels of neural networks. And we've been\nworking on what we",
    "start": "23590",
    "end": "30330"
  },
  {
    "text": "call a rate model,\nin which we replaced all the spikes of a\nneuron with, essentially,",
    "start": "30330",
    "end": "35670"
  },
  {
    "text": "a single number\nthat characterizes the rate at which\na neuron fires.",
    "start": "35670",
    "end": "41550"
  },
  {
    "text": "We introduced a simple\nnetwork in which we have an input neuron\nand an output neuron",
    "start": "41550",
    "end": "48450"
  },
  {
    "text": "with a synaptic connection\nof weight w between them. And that synaptic connection\nleads to a synaptic input",
    "start": "48450",
    "end": "56730"
  },
  {
    "text": "that's proportional\nto w times the firing rate of the input neuron. And then we talked about\nhow we can characterize",
    "start": "56730",
    "end": "63690"
  },
  {
    "text": "the output, the firing\nrate of the output neuron, as some nonlinear function\nof the total input",
    "start": "63690",
    "end": "72030"
  },
  {
    "text": "to this output neuron. We've talked about\ndifferent F-I curves.",
    "start": "72030",
    "end": "78660"
  },
  {
    "text": "We've talked about having\nwhat's called a binary threshold unit, which has zero firing\nbelow some threshold.",
    "start": "78660",
    "end": "85360"
  },
  {
    "text": "And then actually, there\nare different versions of the binary threshold unit. Sometimes the\nfiring rate is zero",
    "start": "85360",
    "end": "93869"
  },
  {
    "text": "for inputs below the threshold. And in other models,\nwe use a minus 1.",
    "start": "93870",
    "end": "100110"
  },
  {
    "text": "And then a constant firing rate\nof one above that threshold. And we also talked\nabout linear neurons,",
    "start": "100110",
    "end": "106950"
  },
  {
    "text": "where we can write down the\nfiring rate of the output neuron just as a weighted\nsum of the inputs.",
    "start": "106950",
    "end": "114000"
  },
  {
    "text": "And remember that\nthese neurons are kind of special in that they\ncan have negative firing",
    "start": "114000",
    "end": "119880"
  },
  {
    "text": "rates, which is not really\nbiophysically plausible,",
    "start": "119880",
    "end": "125049"
  },
  {
    "text": "but mathematically, it's very\nconvenient to have neurons like this.",
    "start": "125050",
    "end": "130530"
  },
  {
    "text": "So we took this simple model\nand we expanded it to the case where we have many input\nneurons and many output neurons.",
    "start": "130530",
    "end": "137790"
  },
  {
    "text": "So now we have a vector of input\nfiring rates, u, and a vector",
    "start": "137790",
    "end": "144090"
  },
  {
    "text": "of output firing rates, u. And for the case\nof linear neurons, we talked about how\nyou can write down",
    "start": "144090",
    "end": "149850"
  },
  {
    "text": "the vector of firing\nrates of the output neuron simply as a matrix product of a\nweight matrix times the vector",
    "start": "149850",
    "end": "158909"
  },
  {
    "text": "of input firing rates. And we talked about how this\ncan produce transformations",
    "start": "158910",
    "end": "165970"
  },
  {
    "text": "of this vector of\ninput firing rates. So in this high-dimensional\nspace of inputs,",
    "start": "165970",
    "end": "171430"
  },
  {
    "text": "we can imagine stretching\nthat input vector along different directions to\namplify certain directions that",
    "start": "171430",
    "end": "178960"
  },
  {
    "text": "may be more important\nthan others. We talked about how\nyou can do that, stretch in arbitrary directions,\nnot just along the axes.",
    "start": "178960",
    "end": "186350"
  },
  {
    "text": "And we talked about\nhow that vector of-- that, sorry, matrix of weights\ncan produce a rotation.",
    "start": "186350",
    "end": "194510"
  },
  {
    "text": "So we can have\nsome set of inputs where, let's say,\nwe have clusters",
    "start": "194510",
    "end": "200140"
  },
  {
    "text": "of different input\nvalues corresponding to different things. And you can rotate that\nto put certain features",
    "start": "200140",
    "end": "207430"
  },
  {
    "text": "in particular output neurons. So now you can\ndiscriminate one class of objects from another\nclass of objects",
    "start": "207430",
    "end": "213280"
  },
  {
    "text": "by looking at just\none dimension and not the whole\nhigh-dimensional space.",
    "start": "213280",
    "end": "219940"
  },
  {
    "text": "So today, we're going to look\nat a new kind of network called a recurrent neural\nnetwork, where not",
    "start": "219940",
    "end": "225909"
  },
  {
    "text": "only do we have inputs\nto our output neurons from an input layer, but\nwe also have connections",
    "start": "225910",
    "end": "234400"
  },
  {
    "text": "between the neurons\nin the output layer. So these neurons in a recurrent\nnetwork talk to each other.",
    "start": "234400",
    "end": "241750"
  },
  {
    "text": "And that imbues some really cool\nproperties onto these networks.",
    "start": "241750",
    "end": "247160"
  },
  {
    "text": "So we're going to\ndevelop the math and describe how\nthese things work to develop an intuition for\nhow recurrent networks respond",
    "start": "247160",
    "end": "255100"
  },
  {
    "text": "to their inputs. We're going to get into\nsome of the computations that recurrent networks can do.",
    "start": "255100",
    "end": "262330"
  },
  {
    "text": "They can act as amplifiers\nin particular directions. They can act as integrators, so\nthey can accumulate information",
    "start": "262330",
    "end": "270250"
  },
  {
    "text": "over time. They can generate sequences. They can act as\nshort-term memories",
    "start": "270250",
    "end": "275620"
  },
  {
    "text": "of either continuous variables\nor discrete variables. It's a very powerful kind\nof circuit architecture.",
    "start": "275620",
    "end": "283420"
  },
  {
    "text": "And on top of that, in order to\ndescribe these mathematically, we're going to use all of\nthe linear algebra tools",
    "start": "283420",
    "end": "289840"
  },
  {
    "text": "that we've been\ndeveloping so far. So, hopefully, a bunch of things\nwill kind of connect together.",
    "start": "289840",
    "end": "297190"
  },
  {
    "text": "OK, so mathematical description\nof recurrent networks. We're going to\ntalk about dynamics in these recurrent\nnetworks, and we're",
    "start": "297190",
    "end": "303700"
  },
  {
    "text": "going to start with\nthe very simplest kind of recurrent network\ncalled an autapse network.",
    "start": "303700",
    "end": "309100"
  },
  {
    "text": "Then we're going to extend\nthat to the general case of recurrent connectivity.",
    "start": "309100",
    "end": "316449"
  },
  {
    "text": "And then we're going to\ntalk about how recurrent networks store memories. So we'll start talking about\na specific circuit models",
    "start": "316450",
    "end": "325720"
  },
  {
    "text": "for storing short-term memories. And I'll touch on recurrent\nnetworks for decision-making.",
    "start": "325720",
    "end": "333280"
  },
  {
    "text": "And this will kind of lead\ninto the last few lectures",
    "start": "333280",
    "end": "338620"
  },
  {
    "text": "of the class, where\nwe get into how sort of specific cases of\nlooking at how networks",
    "start": "338620",
    "end": "345610"
  },
  {
    "text": "can store memories. OK, mathematical description. All right, so the first\nthing that we need to do is--",
    "start": "345610",
    "end": "353980"
  },
  {
    "text": "the really cool thing\nabout recurrent networks is that their activity\ncan evolve over time.",
    "start": "353980",
    "end": "361060"
  },
  {
    "text": "So we need to talk about\ndynamics, all right? The feed-forward networks\nthat we've been talking about,",
    "start": "361060",
    "end": "368170"
  },
  {
    "text": "we just put in an input. It gets weighted by\nsynaptic strength,",
    "start": "368170",
    "end": "374210"
  },
  {
    "text": "and we get a firing\nrate in the output, just sort of instantaneously. We've been thinking\nof you put an input,",
    "start": "374210",
    "end": "381500"
  },
  {
    "text": "and you get an output. In general, neural\nnetworks don't do that. You put an input, and\nthings change over time",
    "start": "381500",
    "end": "387740"
  },
  {
    "text": "until you settle at\nsome output, maybe, or it starts doing something\ninteresting, all right?",
    "start": "387740",
    "end": "393830"
  },
  {
    "text": "So the time course\nof the activity becomes very\nimportant, all right?",
    "start": "393830",
    "end": "399770"
  },
  {
    "text": "So neurons don't respond\ninstantaneously to inputs. There are synaptic delays.",
    "start": "399770",
    "end": "405199"
  },
  {
    "text": "There are integration\nof membrane potential. Things change over time. And a specific example of\nthis that we saw in the past",
    "start": "405200",
    "end": "413300"
  },
  {
    "text": "is that if you have\nan input spike, you can produce a postsynaptic\ncurrent that jumps up abruptly",
    "start": "413300",
    "end": "419480"
  },
  {
    "text": "as the synaptic\nconductance turns on. And then the\nsynaptic conductance",
    "start": "419480",
    "end": "424670"
  },
  {
    "text": "decays away as the\nneurotransmitter unbinds from the neurotransmitter\nreceptor,",
    "start": "424670",
    "end": "430670"
  },
  {
    "text": "and you get a synaptic current\nthat decays away over time, OK? So that's a simple kind of time\ndependence that you would get.",
    "start": "430670",
    "end": "439720"
  },
  {
    "text": "And that could lead\nto time dependence in the firing rate\nof the output neuron.",
    "start": "439720",
    "end": "446000"
  },
  {
    "text": "OK, dendritic\npropagation, membrane time constant, other examples\nof how things can take time",
    "start": "446000",
    "end": "453470"
  },
  {
    "text": "in a neural network. All right, so we're\ngoing to model the firing rate of our output\nneuron in the following way.",
    "start": "453470",
    "end": "459449"
  },
  {
    "text": "If we have an input\nfiring rate that's zero and then steps up to some\nconstant and then steps down,",
    "start": "459450",
    "end": "466400"
  },
  {
    "text": "we're going to model the output,\nthe firing rate of the output",
    "start": "466400",
    "end": "471740"
  },
  {
    "text": "neuron, using exactly the\nsame kind of first order linear differential\nequation that we've been using all along for\nthe membrane potential,",
    "start": "471740",
    "end": "479120"
  },
  {
    "text": "for the Hodgkin-Huxley\ngating variables. The same kind of differential\nequation that you've seen over and over again.",
    "start": "479120",
    "end": "484807"
  },
  {
    "text": "So that's the differential\nequation we're going to use. We're going to say that the\ntime derivative of the firing",
    "start": "484808",
    "end": "491330"
  },
  {
    "text": "rate of the output neuron\ntimes the time constant is just equal to minus the\nfiring rate of the output",
    "start": "491330",
    "end": "497960"
  },
  {
    "text": "non plus v infinity. And so you know that the\nsolution to this equation is that the firing rate\nof the output neuron",
    "start": "497960",
    "end": "504530"
  },
  {
    "text": "will just relax exponentially\nto some new v infinity.",
    "start": "504530",
    "end": "511280"
  },
  {
    "text": "And the v infinity\nthat we're going to use is just this non-linear function\ntimes the weighted input",
    "start": "511280",
    "end": "518870"
  },
  {
    "text": "to our neuron. So we're going to take the\nformalism that we developed",
    "start": "518870",
    "end": "527260"
  },
  {
    "text": "for our feed-forward\nnetworks to say, what is the firing\nrate of the output neuron as a function\nof the inputs?",
    "start": "527260",
    "end": "534040"
  },
  {
    "text": "And we're going to use\nthat firing rate that we've been using before as the\nv infinity for our network",
    "start": "534040",
    "end": "541810"
  },
  {
    "text": "with dynamics. Any questions about that? ",
    "start": "541810",
    "end": "547150"
  },
  {
    "text": "All right, so that becomes\nour differential equation now for this recurrent\nnetwork, all right?",
    "start": "547150",
    "end": "554390"
  },
  {
    "text": "So it's just a first order\nlinear differential equation, where the v infinity, the steady\nstate firing rate of the output",
    "start": "554390",
    "end": "561080"
  },
  {
    "text": "neuron, is just this nonlinear\nfunction times the weighted sum of all the inputs.",
    "start": "561080",
    "end": "568710"
  },
  {
    "text": "All right, and actually, for\nmost of what we do today, we're going to just take\nthe case of a linear neuron.",
    "start": "568710",
    "end": "575160"
  },
  {
    "text": "All right. ",
    "start": "575160",
    "end": "581370"
  },
  {
    "text": "So this I've already said. This I've already said. And actually, what I'm doing\nhere is just extending this.",
    "start": "581370",
    "end": "587650"
  },
  {
    "text": "So this was the case for\na single output neuron and a single input neuron. What we're doing now is\nwe're just extending this",
    "start": "587650",
    "end": "594300"
  },
  {
    "text": "to the case where we have\na vector of input neurons with a firing rate represented\nby a firing rate vector u,",
    "start": "594300",
    "end": "601949"
  },
  {
    "text": "and a vector of output\nneurons with a fine rate vector v. And we're just going\nto use this same differential",
    "start": "601950",
    "end": "608310"
  },
  {
    "text": "equation, but we're going to\nwrite it in vector notation. So each one of\nthese output neurons",
    "start": "608310",
    "end": "614040"
  },
  {
    "text": "has an equation\nlike this, and we're going to combine them all\ntogether into a single vector.",
    "start": "614040",
    "end": "621255"
  },
  {
    "text": "Does that make sense?  All right, so there\nis our vector notation",
    "start": "621255",
    "end": "628730"
  },
  {
    "text": "of the activity in\nthis recurrent network. Sorry, I forgot to put the\nrecurrent connections in there.",
    "start": "628730",
    "end": "637880"
  },
  {
    "text": "So the time dependence\nis really simple in this feed-forward\nnetwork, right?",
    "start": "637880",
    "end": "644930"
  },
  {
    "text": "So in a feed-forward\nnetwork, the dynamics just look like this. ",
    "start": "644930",
    "end": "651570"
  },
  {
    "text": "But in a recurrent\nnetwork, this thing can get really interesting and\nstart doing interesting stuff.",
    "start": "651570",
    "end": "656630"
  },
  {
    "text": "All right, so let's add\nrecurrent connections now and add these recurrent\nconnections to our equation.",
    "start": "656630",
    "end": "666075"
  },
  {
    "text": " So in addition to\nthis weight matrix",
    "start": "666075",
    "end": "672209"
  },
  {
    "text": "w that describes the\nconnections from the input layer to the output\nlayer, we're going to have another\nweight matrix that",
    "start": "672210",
    "end": "679470"
  },
  {
    "text": "describes the connections\nbetween the neurons in the output layer.",
    "start": "679470",
    "end": "685740"
  },
  {
    "text": "And this weight\nmatrix, of course, has to be able to\ndescribe a connection from any one of these neurons\nto any other of these neurons.",
    "start": "685740",
    "end": "695040"
  },
  {
    "text": "And so this weight\nmatrix is going to be a function of the\npostsynaptic neuron,",
    "start": "695040",
    "end": "701310"
  },
  {
    "text": "the weight-- the synaptic strength\nis going to be a function of the postsynaptic\nneuron and the presynaptic--",
    "start": "701310",
    "end": "709380"
  },
  {
    "text": "the identity of the\npostsynaptic neuron and the identity of\nthe presynaptic neuron. Does that make sense?",
    "start": "709380",
    "end": "715880"
  },
  {
    "text": "OK, so there are\ntwo kinds of input-- a feed-forward input\nfrom the input layer",
    "start": "715880",
    "end": "723630"
  },
  {
    "text": "and a recurrent input due to\nconnections within the output layer. ",
    "start": "723630",
    "end": "733340"
  },
  {
    "text": "Any questions about that? ",
    "start": "733340",
    "end": "741300"
  },
  {
    "text": "OK, so there is the\nequation now that describes the time rate\nof change of the firing",
    "start": "741300",
    "end": "749760"
  },
  {
    "text": "rates in the output layer. It's just this first order\nlinear differential equation.",
    "start": "749760",
    "end": "755310"
  },
  {
    "text": "And the infinity is just\nthis non-linear function",
    "start": "755310",
    "end": "763290"
  },
  {
    "text": "of the inputs, of the net input\nto this neuron, to each neuron.",
    "start": "763290",
    "end": "770220"
  },
  {
    "text": "And the net input to\nthis set of neurons is a contribution from\nthe feed-forward inputs,",
    "start": "770220",
    "end": "777090"
  },
  {
    "text": "given by this weight matrix\nw, and this contribution from the recurrent inputs,\ngiven by this weight matrix, m.",
    "start": "777090",
    "end": "787820"
  },
  {
    "text": "So that is the crux\nof it, all right?",
    "start": "787820",
    "end": "794540"
  },
  {
    "text": "So I want to make sure that\nwe understand where we are.",
    "start": "794540",
    "end": "801320"
  },
  {
    "text": "Does anybody have any\nquestions about that? No?",
    "start": "801320",
    "end": "806760"
  },
  {
    "text": "All right, then I'll push ahead. All right, so what is this?",
    "start": "806760",
    "end": "811950"
  },
  {
    "text": "So we've seen this before. This product of\nthis weight matrix",
    "start": "811950",
    "end": "817050"
  },
  {
    "text": "times this vector of\ninput firing rates just looks like this.",
    "start": "817050",
    "end": "822550"
  },
  {
    "text": "You can see that the input to\nthis neuron, this first output",
    "start": "822550",
    "end": "829800"
  },
  {
    "text": "neuron, is just the dot\nproduct of these weights onto the first\nneuron and the dot",
    "start": "829800",
    "end": "839190"
  },
  {
    "text": "product of that vector of\nweights, that row of the weight matrix, with the vector\nof input firing rates.",
    "start": "839190",
    "end": "844950"
  },
  {
    "text": " And the feed-forward\ncontribution to this neuron",
    "start": "844950",
    "end": "850590"
  },
  {
    "text": "is just the dot product of that\nrow weight of this input weight matrix with the vector of\ninput firing rates, and so on.",
    "start": "850590",
    "end": "860769"
  },
  {
    "text": "If we look at the recurrent\ninput to these neurons, the recurrent input\nto this first neuron",
    "start": "860770",
    "end": "868630"
  },
  {
    "text": "is just going to\nbe the dot product of this row of the\nrecurrent weight matrix",
    "start": "868630",
    "end": "875140"
  },
  {
    "text": "and the vector of firing\nrates in the output layer. ",
    "start": "875140",
    "end": "883580"
  },
  {
    "text": "The recurrent inputs\nto the second neuron is going to be the dot product\nof this row of the weight",
    "start": "883580",
    "end": "889930"
  },
  {
    "text": "matrix and the vector\nof firing rates. ",
    "start": "889930",
    "end": "896129"
  },
  {
    "text": "Yes? AUDIENCE: So I guess\nI'm a little confused, because I thought it was\nfrom A. Oh, to A. OK.",
    "start": "896130",
    "end": "901890"
  },
  {
    "text": "MICHALE FEE: Yeah,\nit's always post, pre. Post, pre in a weight matrix.",
    "start": "901890",
    "end": "907649"
  },
  {
    "start": "907650",
    "end": "914260"
  },
  {
    "text": "That's because we're\nusually writing down these vectors the way that\nI'm defining this notation.",
    "start": "914260",
    "end": "920890"
  },
  {
    "text": " This vector is a column\nmatrix, a column vector.",
    "start": "920890",
    "end": "931990"
  },
  {
    "text": "All right, so we're going\nto make one simplification",
    "start": "931990",
    "end": "937839"
  },
  {
    "text": "to this. When we work with the\nrecurrent networks,",
    "start": "937840",
    "end": "944020"
  },
  {
    "text": "we're usually going to\nsimplify this input. And rather than write down this\ncomplex feed-forward component,",
    "start": "944020",
    "end": "953050"
  },
  {
    "text": "writing this out as\nthis matrix product, we're just going to\nsimplify the math.",
    "start": "953050",
    "end": "959950"
  },
  {
    "text": "And rather than carry\naround this w times u, we're just going to replace\nthat with a vector of inputs",
    "start": "959950",
    "end": "970760"
  },
  {
    "text": "onto each one of\nthose neurons, OK? So we're just going to pretend\nthat the input to this neuron",
    "start": "970760",
    "end": "977380"
  },
  {
    "text": "is just coming\nfrom one input, OK? And the input to\nthis neuron is coming",
    "start": "977380",
    "end": "982960"
  },
  {
    "text": "from another single input. And so we're just going to\nreplace that feed-forward input",
    "start": "982960",
    "end": "988720"
  },
  {
    "text": "onto this network\nwith this vector h.  So that's the\nequation that we're",
    "start": "988720",
    "end": "995940"
  },
  {
    "text": "going to use moving\nforward, all right? Just simplifies\nthings a little bit so",
    "start": "995940",
    "end": "1002130"
  },
  {
    "text": "we're not carrying\naround this w u. ",
    "start": "1002130",
    "end": "1007560"
  },
  {
    "text": "So now, that's our\nequation that we're going to use to describe\nthis recurrent network.",
    "start": "1007560",
    "end": "1014350"
  },
  {
    "text": "This is a system of\ncoupled equations. What does that mean? You can see that the time\nderivative of the firing",
    "start": "1014350",
    "end": "1021540"
  },
  {
    "text": "rate of this first neuron\nis given by a contribution from the input layer\nand a contribution",
    "start": "1021540",
    "end": "1028920"
  },
  {
    "text": "from other neurons\nin the output layer. So the time rate of\nchange of this neuron",
    "start": "1028920",
    "end": "1036189"
  },
  {
    "text": "depends on the activity\nin all the other neurons in the network. And the time rate of\nchange in this neuron",
    "start": "1036190",
    "end": "1041800"
  },
  {
    "text": "depends on the activity\nof all the other neurons in the network. So that's a set of\ncoupled equations.",
    "start": "1041800",
    "end": "1048173"
  },
  {
    "text": "And that, in general, can be-- you know, it's not obvious,\nwhen you look at it,",
    "start": "1048174",
    "end": "1053230"
  },
  {
    "text": "what the solution is, all right? So we're going to develop the\ntools to solve this equation",
    "start": "1053230",
    "end": "1062200"
  },
  {
    "text": "and get some intuition about\nhow networks like this behave in response to their inputs.",
    "start": "1062200",
    "end": "1070090"
  },
  {
    "text": "So the first thing\nwe're going to do is to simplify this network\nto the case of linear neurons.",
    "start": "1070090",
    "end": "1078800"
  },
  {
    "text": "So we don't have-- so the neurons just fire.",
    "start": "1078800",
    "end": "1084080"
  },
  {
    "text": "Their firing rate is just\nlinear with their input. ",
    "start": "1084080",
    "end": "1089360"
  },
  {
    "text": "And so that's the equation\nfor the linear case. All we've done is\nwe've just gotten rid",
    "start": "1089360",
    "end": "1094400"
  },
  {
    "text": "of this non-linear function f. ",
    "start": "1094400",
    "end": "1099470"
  },
  {
    "text": "All right, so now let's\ntake a very simple case of a recurrent network\nand use this equation",
    "start": "1099470",
    "end": "1107810"
  },
  {
    "text": "to see how it\nbehaves, all right? So the simplest case\nof a recurrent network",
    "start": "1107810",
    "end": "1114080"
  },
  {
    "text": "is the case where the recurrent\nconnections within this layer",
    "start": "1114080",
    "end": "1119120"
  },
  {
    "text": "are given by-- the weight matrix is given\nby a diagonal matrix. Now, what does\nthat correspond to?",
    "start": "1119120",
    "end": "1125690"
  },
  {
    "text": "What that corresponds to is\nthis neuron making a connection onto itself with a synapse of\nweight lambda one, right there.",
    "start": "1125690",
    "end": "1136340"
  },
  {
    "text": "And that kind of\nrecurrent connection of a neuron onto itself\nis called an autapse,",
    "start": "1136340",
    "end": "1143420"
  },
  {
    "text": "like an auto synapse. And we're going to put\none of those autapses",
    "start": "1143420",
    "end": "1148760"
  },
  {
    "text": "on each one of these\nneurons in our output layer, in our recurrent layer.",
    "start": "1148760",
    "end": "1155460"
  },
  {
    "text": "So now we can write\ndown the equation for this network, all right?",
    "start": "1155460",
    "end": "1161540"
  },
  {
    "text": "And what we're going to\ndo is simply replace-- sorry, let me just bring\nup that equation again.",
    "start": "1161540",
    "end": "1168260"
  },
  {
    "text": "Sorry, there's the equation. And we're simply going to\nreplace this weight matrix",
    "start": "1168260",
    "end": "1173570"
  },
  {
    "text": "m, this recurrent weight matrix,\nwith that diagonal matrix that I just showed you.",
    "start": "1173570",
    "end": "1180510"
  },
  {
    "text": "So there it is. So that time rate of change of\nthis vector of output neurons",
    "start": "1180510",
    "end": "1185820"
  },
  {
    "text": "is just minus v plus this\ndiagonal matrix times [INAUDIBLE] plus the inputs.",
    "start": "1185820",
    "end": "1191480"
  },
  {
    "text": " So now you can see\nthat if we write out",
    "start": "1191480",
    "end": "1198630"
  },
  {
    "text": "the equation separately for each\none of these output neurons-- so here it is in\nvector notation.",
    "start": "1198630",
    "end": "1206210"
  },
  {
    "text": "We can just write that out for\neach one of our output neurons.",
    "start": "1206210",
    "end": "1212600"
  },
  {
    "text": "So there's a separate\nequation like this for each one of these neurons.",
    "start": "1212600",
    "end": "1218170"
  },
  {
    "text": "But you can see that\nthese are all uncoupled. So we can understand how\nthis network responds just by studying this equation\nfor one of those neurons.",
    "start": "1218170",
    "end": "1227990"
  },
  {
    "text": "OK, so let's do that. We have an independent equation. The firing rate change--",
    "start": "1227990",
    "end": "1234700"
  },
  {
    "text": "the time derivative of the\nfiring rate of neuron one depends only on the\nfiring rate of neuron one.",
    "start": "1234700",
    "end": "1240509"
  },
  {
    "text": "It doesn't depend on\nany other neurons. As you can see,\nit's not connected",
    "start": "1240510",
    "end": "1245790"
  },
  {
    "text": "to any of the other neurons. OK, so let's write\nthis equation. And let's see what that\nequation looks like.",
    "start": "1245790",
    "end": "1253420"
  },
  {
    "text": "So we're going to rewrite\nthis a little bit. We're just going to factor\nout the va all right here.",
    "start": "1253420",
    "end": "1260940"
  },
  {
    "text": "This parameter,\n1 minus lambda a, controls what kind of\nsolutions this equation has.",
    "start": "1260940",
    "end": "1268770"
  },
  {
    "text": "And there are three different\ncases that we need to consider. We need to consider\nthe case where 1 minus lambda is greater\nthan zero, equal to zero,",
    "start": "1268770",
    "end": "1277350"
  },
  {
    "text": "or less than zero. Those three different values of\nthat parameter 1 minus lambda",
    "start": "1277350",
    "end": "1283820"
  },
  {
    "text": "give three different kinds of\nsolutions to this equation. We're going to\nstart with the case where lambda is less than one.",
    "start": "1283820",
    "end": "1291730"
  },
  {
    "text": "And if lambda is less than\n1, then this term right here is greater than zero.",
    "start": "1291730",
    "end": "1298200"
  },
  {
    "text": "If we do that, then we\ncan rewrite this equation as follows. We're going to divide both\nsides of this equation",
    "start": "1298200",
    "end": "1305780"
  },
  {
    "text": "by 1 minus lambda, and\nthat's what we have here. And you can see that this\nequation starts looking",
    "start": "1305780",
    "end": "1312970"
  },
  {
    "text": "very familiar, very simple. We have a first order linear\ndifferential equation, where",
    "start": "1312970",
    "end": "1320559"
  },
  {
    "text": "we have a time constant here,\ntau over 1 minus lambda, and a v infinity here, which is\nthe input, the effective input",
    "start": "1320560",
    "end": "1329560"
  },
  {
    "text": "onto that neuron, divided\nby 1 minus lambda. So that's tau dv dt equals\nminus v plus v infinity.",
    "start": "1329560",
    "end": "1338482"
  },
  {
    "text": " But now you can see\nthat the time constant",
    "start": "1338482",
    "end": "1344470"
  },
  {
    "text": "and the v infinity\ndepend on lambda, depend on the strength of\nthat connection, all right?",
    "start": "1344470",
    "end": "1355450"
  },
  {
    "text": "And the solution to that we've\nseen before, to this equation. It's just exponential\nrelaxation toward v infinity.",
    "start": "1355450",
    "end": "1363690"
  },
  {
    "text": "OK, so here's our v infinity. There's our tau. True for the case\nof lambda between--",
    "start": "1363690",
    "end": "1371730"
  },
  {
    "text": "let's just look at these\nsolutions for the case of lambda between zero and one.",
    "start": "1371730",
    "end": "1378850"
  },
  {
    "text": "So I'm going to plot v as a\nfunction of time when we have",
    "start": "1378850",
    "end": "1384740"
  },
  {
    "text": "an input that goes from zero\nand then steps up and then is held constant.",
    "start": "1384740",
    "end": "1392340"
  },
  {
    "text": "All right, so let's look at\nthe case of lambda equals zero. So this lambda zero\nmeans there's no autapse.",
    "start": "1392340",
    "end": "1398540"
  },
  {
    "text": "It's just not connected. So you can see\nthat, in this case,",
    "start": "1398540",
    "end": "1403560"
  },
  {
    "text": "the solution is very simple. It's just exponential relaxation\ntoward infinity. v infinity",
    "start": "1403560",
    "end": "1409100"
  },
  {
    "text": "is just given by h, the\ninput, and tau is just",
    "start": "1409100",
    "end": "1415880"
  },
  {
    "text": "the original tau,\n1 minus 0, right? So it's just exponential\nrelaxation to h.",
    "start": "1415880",
    "end": "1423638"
  },
  {
    "text": " That make sense? ",
    "start": "1423638",
    "end": "1431100"
  },
  {
    "text": "And it relaxes with a\ntime constant tau, tau m.",
    "start": "1431100",
    "end": "1436990"
  },
  {
    "text": "We're going to now turn up\nthe synapse a little bit so that it has a\nlittle bit of strength.",
    "start": "1436990",
    "end": "1444250"
  },
  {
    "text": "You see that what happens\nwhen lambda is 0.5, that v infinity gets bigger.",
    "start": "1444250",
    "end": "1450909"
  },
  {
    "text": "v infinity goes to 2h. Why? Because it's h divided\nby 1 minus 0.5.",
    "start": "1450910",
    "end": "1456010"
  },
  {
    "text": "So it's h over 0.5, so 2h. And what happens to\nthe time constant?",
    "start": "1456010",
    "end": "1461310"
  },
  {
    "text": "Well, it becomes two tau. All right, and if we make\nlambda equal to 0.3--",
    "start": "1461310",
    "end": "1468799"
  },
  {
    "text": "sorry, 0.66. We turn it up a little bit. You can see that the response\nof this neuron gets even bigger.",
    "start": "1468800",
    "end": "1476600"
  },
  {
    "text": "So you can see that\nwhat's happening is that when we start\nletting this neuron feed back",
    "start": "1476600",
    "end": "1482890"
  },
  {
    "text": "to itself, positive feedback,\nthe response of the neuron",
    "start": "1482890",
    "end": "1487970"
  },
  {
    "text": "to a fixed input-- the input is the same\nfor all of those. The response of the\nneuron gets bigger.",
    "start": "1487970",
    "end": "1495380"
  },
  {
    "text": "And so having positive feedback\nof that neuron onto itself through an autapse just\namplifies the response",
    "start": "1495380",
    "end": "1502130"
  },
  {
    "text": "of this neuron to its input. ",
    "start": "1502130",
    "end": "1509080"
  },
  {
    "text": "Now, let's consider\nthe case where-- so positive feedback\namplifies the response.",
    "start": "1509080",
    "end": "1514630"
  },
  {
    "text": "And what also does it do? It slows the response down. The time constants are\ngetting longer, which",
    "start": "1514630",
    "end": "1521590"
  },
  {
    "text": "means the response is slower. ",
    "start": "1521590",
    "end": "1527304"
  },
  {
    "text": "All right, let's look\nat what happens when the lambdas are less than zero.",
    "start": "1527305",
    "end": "1532960"
  },
  {
    "text": "What does lambda less than\nzero correspond to here? AUDIENCE: [INAUDIBLE] MICHALE FEE: Yeah, which\nis, in neurons, what",
    "start": "1532960",
    "end": "1541470"
  },
  {
    "text": "does that correspond to? AUDIENCE: [INAUDIBLE] MICHALE FEE: Inhibition. So this neuron, when\nyou put an input in,",
    "start": "1541470",
    "end": "1548684"
  },
  {
    "text": "it tries to activate the neuron. But that neuron inhibits itself. So what do you think's\ngoing to happen?",
    "start": "1548685",
    "end": "1554580"
  },
  {
    "text": "So positive feedback\nmade the response bigger. Here, the neuron is kind\nof inhibiting itself.",
    "start": "1554580",
    "end": "1561130"
  },
  {
    "text": "So what's going to happen? You put in that same\nh that we had before, what's going to happen\nwhen we have inhibition?",
    "start": "1561130",
    "end": "1569612"
  },
  {
    "text": "AUDIENCE: Response\nis [INAUDIBLE].. MICHALE FEE: What's that? AUDIENCE: The response\nis going to be smaller. MICHALE FEE: The response will\njust be smaller, that's right.",
    "start": "1569612",
    "end": "1575060"
  },
  {
    "text": "So let's look at that. So here's firing\nrate of this neuron is a function of time\nfor a step input.",
    "start": "1575060",
    "end": "1580820"
  },
  {
    "text": "You can see for a\nlambda equals zero, we're going to respond\nwith an amount h. ",
    "start": "1580820",
    "end": "1587890"
  },
  {
    "text": "But if we put in-- in a time constant tau. If we put in a lambda\nof negative one--",
    "start": "1587890",
    "end": "1593740"
  },
  {
    "text": "that means you put\nthis input in-- that neuron starts\ninhibiting itself,",
    "start": "1593740",
    "end": "1599279"
  },
  {
    "text": "and you can see the\nresponse is smaller. But another thing\nthat's real interesting",
    "start": "1599280",
    "end": "1604290"
  },
  {
    "text": "is that you can see that\nthe response of the neuron is actually faster. ",
    "start": "1604290",
    "end": "1612100"
  },
  {
    "text": "So if the feedback-- if\nthe lambda is minus one, you can see that v infinity\nis h over 1 minus negative 1.",
    "start": "1612100",
    "end": "1620350"
  },
  {
    "text": "So it's h over 2. All right, and so on. The more we turn up that\ninhibition, the more",
    "start": "1620350",
    "end": "1626430"
  },
  {
    "text": "suppressed the\nneuron is, the weaker the response that\nneuron is to its input, but the faster it is.",
    "start": "1626430",
    "end": "1634110"
  },
  {
    "text": "So negative feedback suppresses\nthe response of the neuron and speeds up the response.",
    "start": "1634110",
    "end": "1639300"
  },
  {
    "start": "1639300",
    "end": "1646708"
  },
  {
    "text": "OK, now, there's one other\nreally important thing about recurrent networks\nin this regime, where",
    "start": "1646708",
    "end": "1652860"
  },
  {
    "text": "this lambda is less than one. And that is that\nthe activity always",
    "start": "1652860",
    "end": "1659610"
  },
  {
    "text": "relaxes back to zero when\nyou turn the input off. OK, so you put a step\ninput in, the neuron",
    "start": "1659610",
    "end": "1666659"
  },
  {
    "text": "responds, relaxing exponentially\nto sum of v infinity. But when you turn the\ninput off, the network",
    "start": "1666660",
    "end": "1673820"
  },
  {
    "text": "relaxes back to zero, OK? ",
    "start": "1673820",
    "end": "1685760"
  },
  {
    "text": "So now let's go to\nthe more general case of recurrent connections.",
    "start": "1685760",
    "end": "1692019"
  },
  {
    "text": "Oh, and first, I\njust want to show you how we actually show graphically\nhow a neuron responds--",
    "start": "1692020",
    "end": "1699870"
  },
  {
    "text": "sorry, how one of\nthese networks respond. And a typical way\nthat we do that is we",
    "start": "1699870",
    "end": "1705049"
  },
  {
    "text": "plot the firing rate of one\nneuron versus the firing rate of another neuron.",
    "start": "1705050",
    "end": "1711260"
  },
  {
    "text": "That's called a\nstate-space trajectory. And we plot that response\nas a function of time",
    "start": "1711260",
    "end": "1718820"
  },
  {
    "text": "after we put in an input. So we can put an input in\ndescribed as some vector.",
    "start": "1718820",
    "end": "1724100"
  },
  {
    "text": "So we put in some h1\nand h2, and we then plot the response\nof the neuron--",
    "start": "1724100",
    "end": "1730430"
  },
  {
    "text": "the response of the network\nin this output state space. So let me show you an example\nof what that looks like.",
    "start": "1730430",
    "end": "1737400"
  },
  {
    "text": "So here is the output\nof this little network",
    "start": "1737400",
    "end": "1744170"
  },
  {
    "text": "for different kinds of inputs. So Daniel made this nice\nlittle movie for us.",
    "start": "1744170",
    "end": "1749210"
  },
  {
    "text": " Here, you can see that if you\nput an input into neuron one,",
    "start": "1749210",
    "end": "1756250"
  },
  {
    "text": "neuron one responds. If you put a negative\ninput into neuron one, the neuron goes negative.",
    "start": "1756250",
    "end": "1761700"
  },
  {
    "text": "If you put an input into neuron\ntwo, the neuron responds. And if you put a negative input\ninto neuron two, it responds.",
    "start": "1761700",
    "end": "1769640"
  },
  {
    "text": "Now, why did it respond\nbigger in this direction than in this direction?",
    "start": "1769640",
    "end": "1774978"
  },
  {
    "text": " AUDIENCE: That's [INAUDIBLE].",
    "start": "1774978",
    "end": "1782320"
  },
  {
    "text": "MICHALE FEE: Good. Because neuron one had--",
    "start": "1782320",
    "end": "1787857"
  },
  {
    "text": "AUDIENCE: Positive? MICHALE FEE: Positive feedback. And neuron two had\nnegative feedback.",
    "start": "1787858",
    "end": "1793330"
  },
  {
    "text": "So neuron one, this neuron\none, amplified its input",
    "start": "1793330",
    "end": "1799600"
  },
  {
    "text": "and gave a big response. Neuron two suppressed the\nresponse to its input,",
    "start": "1799600",
    "end": "1805400"
  },
  {
    "text": "and so it had a weak response. ",
    "start": "1805400",
    "end": "1812750"
  },
  {
    "text": "Let's look at another\ninteresting case. Let's put an input\ninto these neurons-- not one at a time,\nbut simultaneously.",
    "start": "1812750",
    "end": "1820090"
  },
  {
    "text": " So now we're going to put an\ninput into both neurons one",
    "start": "1820090",
    "end": "1828440"
  },
  {
    "text": "and two simultaneously. ",
    "start": "1828440",
    "end": "1837570"
  },
  {
    "text": "It's like Spirograph. Did you guys play\nwith Spirograph?",
    "start": "1837570",
    "end": "1844430"
  },
  {
    "text": "It's kind of weird, right? It's like making little\nbutterflies for spring. ",
    "start": "1844430",
    "end": "1852139"
  },
  {
    "text": "So why does the output-- why does the response\nof this neuron to an input, positive input to\nboth h1 and h2, look like this?",
    "start": "1852140",
    "end": "1861530"
  },
  {
    "text": "Let's just break this down into\none of these little branches. We start at zero. We put an input into h1\nand h2, and the response",
    "start": "1861530",
    "end": "1869420"
  },
  {
    "text": "goes quickly like this and\nthen relaxes up to here.",
    "start": "1869420",
    "end": "1874930"
  },
  {
    "text": "So why is that? Lena? AUDIENCE: [INAUDIBLE] so\nthere was [INAUDIBLE] and then",
    "start": "1874930",
    "end": "1883646"
  },
  {
    "text": "because it's negative,\nit's shorter. MICHALE FEE: Yup. The response in the v2\ndirection is weak but fast.",
    "start": "1883646",
    "end": "1890965"
  },
  {
    "text": "AUDIENCE: Yeah. MICHALE FEE: So it\ngoes up quickly. And then the response\nin the v1 direction is?",
    "start": "1890965",
    "end": "1897679"
  },
  {
    "text": "AUDIENCE: Slow, but [INAUDIBLE]. MICHALE FEE: Good. That's it. It's slow, but [AUDIO OUT].",
    "start": "1897680",
    "end": "1903090"
  },
  {
    "text": "It's amplified in this\ndirection, suppressed in this direction. But the response is fast\nthis way and slow this way.",
    "start": "1903090",
    "end": "1909630"
  },
  {
    "text": "So it traces this out. Now, when you turn the input\noff, again, it relaxes.",
    "start": "1909630",
    "end": "1916420"
  },
  {
    "text": "v2 relaxes quickly back to\nzero, and v1 relaxes slowly",
    "start": "1916420",
    "end": "1922100"
  },
  {
    "text": "back to zero. So it kind of traces out\nthis kind of hysteretic loop. ",
    "start": "1922100",
    "end": "1930400"
  },
  {
    "text": "It's not really hysteresis. Then it's exactly\nmirror image when",
    "start": "1930400",
    "end": "1935820"
  },
  {
    "text": "you put in a negative input. And when you put in h1\npositive and v1 negative,",
    "start": "1935820",
    "end": "1944610"
  },
  {
    "text": "it just looks like\na mirror image. All right, so any\nquestions about that?",
    "start": "1944610",
    "end": "1950470"
  },
  {
    "text": "Yes, Lena? AUDIENCE: If there was nothing,\nlike no kind of amplified or [INAUDIBLE],, would it\njust be like a [INAUDIBLE]??",
    "start": "1950470",
    "end": "1957440"
  },
  {
    "text": "MICHALE FEE: Yeah,\nso if you took out the recurrent connections, what\nwould what would it look like?",
    "start": "1957440",
    "end": "1962964"
  },
  {
    "text": "AUDIENCE: An x? MICHALE FEE: Yeah, the output-- so let's say that you just\nliterally set those to zero.",
    "start": "1962964",
    "end": "1970059"
  },
  {
    "text": "Then the response will be\nthe identity matrix, right?",
    "start": "1970060",
    "end": "1978130"
  },
  {
    "text": "You get the output as\na function of input. Let's just go back\nto the equation. Can always, always\nget the answer",
    "start": "1978130",
    "end": "1983650"
  },
  {
    "text": "by looking at the equation. ",
    "start": "1983650",
    "end": "1990330"
  },
  {
    "text": "Too many animations. No, it's a very good question. Here we go. There it is right there.",
    "start": "1990330",
    "end": "1996430"
  },
  {
    "text": "So you're asking about-- let's\njust ask about the steady state response. So we can set dv\ndt equal to zero.",
    "start": "1996430",
    "end": "2003539"
  },
  {
    "text": "And you're asking, what is v? And you're saying, let's\nset lambda to zero, right?",
    "start": "2003540",
    "end": "2011200"
  },
  {
    "text": "We're going to set all these\ndiagonal elements to zero. And so now v equals h.",
    "start": "2011200",
    "end": "2017740"
  },
  {
    "start": "2017740",
    "end": "2027940"
  },
  {
    "text": "OK, great question. Now, let's go to the case\nof fully recurrent networks.",
    "start": "2027940",
    "end": "2034390"
  },
  {
    "text": "We've been working with\nthis simplified case of just having neurons have autapses.",
    "start": "2034390",
    "end": "2040350"
  },
  {
    "text": "And the reason we've been doing\nthat is because the answer you get for the autapse\nkind of captures",
    "start": "2040350",
    "end": "2046380"
  },
  {
    "text": "almost all the intuition\nthat you need to have. What we're going to\ndo is we're going to take a fully\nrecurrent neural network,",
    "start": "2046380",
    "end": "2054270"
  },
  {
    "text": "and we're going to do a\nmathematical trick that just turns it into\nan autapse network.",
    "start": "2054270",
    "end": "2059309"
  },
  {
    "text": " And the answer for the\nfully recurrent network",
    "start": "2059310",
    "end": "2065280"
  },
  {
    "text": "is just going to be just as\nsimple as what you saw here. All right, so let's do that.",
    "start": "2065280",
    "end": "2071280"
  },
  {
    "text": "Let's take this fully\nrecurrent network. Our weight matrix m now,\ninstead of just having",
    "start": "2071280",
    "end": "2076980"
  },
  {
    "text": "diagonal elements, also\nhas off-diagonal elements. ",
    "start": "2076980",
    "end": "2082820"
  },
  {
    "text": "And I'll say that one\nof the things that we're going to do today is just\nconsider the simplest case of this fully\nrecurrent network, where",
    "start": "2082820",
    "end": "2091239"
  },
  {
    "text": "the connections are symmetric,\nwhere a connection from v1 to v2 is equal to the connection\nfrom v2 to v1, all right?",
    "start": "2091239",
    "end": "2100180"
  },
  {
    "text": "We're going to do that\nbecause that's the next thing to do to build our\nintuition, and it's",
    "start": "2100180",
    "end": "2106150"
  },
  {
    "text": "also mathematically simpler\nthan the fully general case, OK?",
    "start": "2106150",
    "end": "2112359"
  },
  {
    "text": "So we saw how the\nbehavior of this network is very simple if m is diagonal.",
    "start": "2112360",
    "end": "2117609"
  },
  {
    "text": " So what we're going\nto do is we're going to take this\narbitrary matrix m,",
    "start": "2117610",
    "end": "2126030"
  },
  {
    "text": "and we're going to\njust make it diagonal. So let's do that.",
    "start": "2126030",
    "end": "2131140"
  },
  {
    "text": "So we're going to rewrite\nour weight matrix m as-- so we're going to rewrite m\nin this form, where this phi--",
    "start": "2131140",
    "end": "2146910"
  },
  {
    "text": "sorry, where this lambda\nis a diagonal matrix.",
    "start": "2146910",
    "end": "2152210"
  },
  {
    "text": "So we're going to\ntake this network with recurrent connections\nbetween different neurons",
    "start": "2152210",
    "end": "2158200"
  },
  {
    "text": "in the network, and we're\ngoing to transform it",
    "start": "2158200",
    "end": "2163780"
  },
  {
    "text": "into sort of an equivalent\nnetwork that just has autapses. ",
    "start": "2163780",
    "end": "2171310"
  },
  {
    "text": "So how do we write\nm in this form, with a rotation matrix\ntimes a diagonal matrix",
    "start": "2171310",
    "end": "2177310"
  },
  {
    "text": "times a rotation matrix? We just solve this\neigenvalue equation, OK?",
    "start": "2177310",
    "end": "2186170"
  },
  {
    "text": "Does that make sense? We're just going to do\nexactly the same thing we did in PCA, where we\nfind the covariance matrix.",
    "start": "2186170",
    "end": "2196569"
  },
  {
    "text": "And we rewrote the\ncovariance matrix like this. Now we're going to\ntake a weight matrix",
    "start": "2196570",
    "end": "2202300"
  },
  {
    "text": "of this recurrent\nnetwork, and we're going to rewrite it in\nexactly the same way.",
    "start": "2202300",
    "end": "2211090"
  },
  {
    "text": "So that process is called\ndiagonalizing the weight matrix.",
    "start": "2211090",
    "end": "2217860"
  },
  {
    "text": "So the elements of lambda\nhere are the eigenvalues of m.",
    "start": "2217860",
    "end": "2224040"
  },
  {
    "text": " And the columns of the phi\nare the eigenvectors of m.",
    "start": "2224040",
    "end": "2232320"
  },
  {
    "text": " And we're going to use these\nquantities, these elements,",
    "start": "2232320",
    "end": "2242589"
  },
  {
    "text": "to build a new network that\nhas the same properties as our recurrent network.",
    "start": "2242590",
    "end": "2252400"
  },
  {
    "text": "So let me just show\nyou how we do that. So remember that what\nthis eigenvalue--",
    "start": "2252400",
    "end": "2258190"
  },
  {
    "text": "this is an eigenvalue equation\nwritten in matrix notation.",
    "start": "2258190",
    "end": "2263470"
  },
  {
    "text": "What this means is this is set\nof eigenvalues equations that",
    "start": "2263470",
    "end": "2271340"
  },
  {
    "text": "have-- it's a set of\nn eigenvalue equations like this, where\nthere's one of these",
    "start": "2271340",
    "end": "2276430"
  },
  {
    "text": "for each neuron in the network. OK, so let me just\ngo through that. OK, so here's the\neigenvalue equation.",
    "start": "2276430",
    "end": "2282339"
  },
  {
    "text": "If M is a symmetric matrix,\nthen the eigenvalues are real",
    "start": "2282340",
    "end": "2288180"
  },
  {
    "text": "and phi is a rotation matrix. And the eigenvectors give us\nan orthogonal basis, all right?",
    "start": "2288180",
    "end": "2294150"
  },
  {
    "text": "So everybody remember this\nfrom a few lectures ago?  If M is symmetric--\nand this is why",
    "start": "2294150",
    "end": "2301070"
  },
  {
    "text": "we're going to, at\nthis point on, consider just the case where\nM is symmetric,",
    "start": "2301070",
    "end": "2306390"
  },
  {
    "text": "then the eigenvectors, the\ncolumns of that matrix phi, give us an orthogonal set of\nvectors and their unit vectors.",
    "start": "2306390",
    "end": "2317599"
  },
  {
    "text": "So it satisfies this\northonormal condition. And phi transpose phi is\nan identity matrix, which",
    "start": "2317600",
    "end": "2325010"
  },
  {
    "text": "means phi is a rotation matrix. OK, so now what we're\ngoing to do is rewrite.",
    "start": "2325010",
    "end": "2331670"
  },
  {
    "text": "The first thing we're going\nto do to use this trick to rewrite our\nmatrix, our network,",
    "start": "2331670",
    "end": "2337220"
  },
  {
    "text": "is to rewrite the\nvector of firing rates v in this new basis. What are we going to do?",
    "start": "2337220",
    "end": "2342950"
  },
  {
    "text": "Well take the vector and\nall we're going to do is to rewrite that vector\nin this new basis set.",
    "start": "2342950",
    "end": "2351170"
  },
  {
    "text": "We're just going to do a change\nof basis of our firing rate vector into a new\nbasis set that's",
    "start": "2351170",
    "end": "2357950"
  },
  {
    "text": "given by the columns of phi. ",
    "start": "2357950",
    "end": "2363506"
  },
  {
    "text": "Another way of saying\nit is that we're going to rotate this firing rate\nvector v using the phi rotation",
    "start": "2363507",
    "end": "2370190"
  },
  {
    "text": "matrix. So we're going to project v\nonto each one of those new basis",
    "start": "2370190",
    "end": "2375619"
  },
  {
    "text": "vectors. So there's v in\nthe standard basis. There's our new\nbasis, f1 and f2.",
    "start": "2375620",
    "end": "2382100"
  },
  {
    "text": "We're going to project\nv onto f1 and f2 and write down that scalar\nprojection, c1 and c2.",
    "start": "2382100",
    "end": "2392299"
  },
  {
    "text": "So we're going to write down\nthe scalar projection of v onto each one of\nthose basis vectors.",
    "start": "2392300",
    "end": "2399270"
  },
  {
    "text": "So we can write\nthat c sub alpha-- that's the alpha-th component-- is just v dot the\nalpha-th basis vector.",
    "start": "2399270",
    "end": "2413180"
  },
  {
    "text": "So now we can express v\nas a linear combination in this new basis. ",
    "start": "2413180",
    "end": "2421240"
  },
  {
    "text": "So it's c1 times f1 plus\nc2 times f2 plus c3-- that's supposed to be a three--",
    "start": "2421240",
    "end": "2427870"
  },
  {
    "text": "times f3 and so on.  And of course, remember,\nwe're doing all of this",
    "start": "2427870",
    "end": "2435980"
  },
  {
    "text": "because we want to\nunderstand the dynamics. So these things\nare time dependent. So v is v changes in time.",
    "start": "2435980",
    "end": "2445099"
  },
  {
    "text": "We're not going to be changing\nour basis vectors in time. So if we want to write\ndown a time dependent v,",
    "start": "2445100",
    "end": "2450260"
  },
  {
    "text": "it's really these\ncoefficients that are changing in time, right?",
    "start": "2450260",
    "end": "2456444"
  },
  {
    "text": "Does that make sense? So we can now write our vector\nv, our firing rate vector,",
    "start": "2456445",
    "end": "2463742"
  },
  {
    "text": "as a sum of contributions in\nall these different directions",
    "start": "2463742",
    "end": "2469750"
  },
  {
    "text": "corresponding to the new basis.  And each one of\nthose coefficients, c",
    "start": "2469750",
    "end": "2476170"
  },
  {
    "text": "is just the time dependent\nv projected onto one of those basis vectors.",
    "start": "2476170",
    "end": "2481940"
  },
  {
    "start": "2481940",
    "end": "2488450"
  },
  {
    "text": "And questions? No? OK. And remember, we can write\nthat in matrix notation using",
    "start": "2488450",
    "end": "2499140"
  },
  {
    "text": "this formalism that we developed\nin the lecture on basis sets. So v is just phi c, and c\nis just phi transpose v.",
    "start": "2499140",
    "end": "2507570"
  },
  {
    "text": "So we're just taking\nthis vector v, and we're rotating it\ninto a new basis set, and we can rotate it back.",
    "start": "2507570",
    "end": "2513720"
  },
  {
    "text": " All right, so now\nwhat we're going to do is we're going to take this v\nexpressed in this new basis set",
    "start": "2513720",
    "end": "2523390"
  },
  {
    "text": "and were going to rewrite our\nequation in that new basis set.",
    "start": "2523390",
    "end": "2528869"
  },
  {
    "text": " Watch this. This is so cool.",
    "start": "2528870",
    "end": "2534560"
  },
  {
    "text": "All right, you ready? We're going to take this, and\nwe're to plug it into here. ",
    "start": "2534560",
    "end": "2542140"
  },
  {
    "text": "So dv dt is phi dc dt.",
    "start": "2542140",
    "end": "2548170"
  },
  {
    "text": "V is just phi c. v is phi c, and\nh doesn't change.",
    "start": "2548170",
    "end": "2556549"
  },
  {
    "text": "So now what is that? ",
    "start": "2556550",
    "end": "2565270"
  },
  {
    "text": "Do you remember? AUDIENCE: Phi [INAUDIBLE]. MICHALE FEE: Right.",
    "start": "2565270",
    "end": "2570600"
  },
  {
    "text": "We got phi as the solution\nto the eigenvalue equation.",
    "start": "2570600",
    "end": "2577440"
  },
  {
    "text": "What was the\neigenvalue equation? The eigenvalue equation was\nm phi equals phi lambda.",
    "start": "2577440",
    "end": "2586150"
  },
  {
    "text": "So the phi here,\nthis rotation matrix, is the solution to this\nequation, all right?",
    "start": "2586150",
    "end": "2593980"
  },
  {
    "text": "So we're given m,\nand we're saying we're going to find\na phi and a lambda",
    "start": "2593980",
    "end": "2600950"
  },
  {
    "text": "such that we can write m\nphi is equal to phi lambda.",
    "start": "2600950",
    "end": "2606109"
  },
  {
    "text": "So when we take that matrix m\nand we run eig on it in Matlab,",
    "start": "2606110",
    "end": "2612220"
  },
  {
    "text": "Matlab sends us back a phi and\na lambda such that this equation is true.",
    "start": "2612220",
    "end": "2617650"
  },
  {
    "text": " So literally, we can\ntake the weight matrix",
    "start": "2617650",
    "end": "2623805"
  },
  {
    "text": "m stick it into Matlab,\nand get a phi and a lambda such that m phi is\nequal to phi lambda.",
    "start": "2623805",
    "end": "2631790"
  },
  {
    "text": "So m phi is equal to what?",
    "start": "2631790",
    "end": "2639020"
  },
  {
    "text": "Phi lambda.  That becomes this.",
    "start": "2639020",
    "end": "2646810"
  },
  {
    "text": "Now, all of a sudden, this\nthing is just going to simplify. ",
    "start": "2646810",
    "end": "2654480"
  },
  {
    "text": "So how would we\nsimplify this equation?  We can get rid of all of these\nthings, all of these phi's,",
    "start": "2654480",
    "end": "2663080"
  },
  {
    "text": "by doing what? How do you get rid of phi's? AUDIENCE: Multiply\n[INAUDIBLE] phi transpose. MICHALE FEE: You multiply\nby phi transpose, exactly.",
    "start": "2663080",
    "end": "2669710"
  },
  {
    "text": "So we're going to multiply\neach term in this equation by phi transpose.",
    "start": "2669710",
    "end": "2675760"
  },
  {
    "text": "So what do you have? Phi transpose phi, phi transpose\nphi, phi transpose phi.",
    "start": "2675760",
    "end": "2682990"
  },
  {
    "text": "What is phi transpose\nphi equal to? The identity matrix.",
    "start": "2682990",
    "end": "2688700"
  },
  {
    "text": "Because it's a rotation\nmatrix, phi transpose is just the inverse of phi.",
    "start": "2688700",
    "end": "2694730"
  },
  {
    "text": "So phi inverse phi is just\nequal to the identity matrix. And all those things disappear.",
    "start": "2694730",
    "end": "2700680"
  },
  {
    "text": "And you're left\nwith this equation-- tau dc dt equals minus c\nplus lambda c plus h, hf.",
    "start": "2700680",
    "end": "2709370"
  },
  {
    "text": "And what is hf? hf is just h rotated\ninto the new basis set. ",
    "start": "2709370",
    "end": "2716370"
  },
  {
    "text": "So this is the equation\nfor a recurrent network with just autapses,\nwhich we just understood.",
    "start": "2716370",
    "end": "2728119"
  },
  {
    "text": "We just wrote down what\nthe solution is, right? And we plotted it for\ndifferent values of lambda.",
    "start": "2728120",
    "end": "2733130"
  },
  {
    "start": "2733130",
    "end": "2740380"
  },
  {
    "text": "So now let's just look at\nwhat some of these look like. So we've rewritten our weight\nmatrix in a new basis set.",
    "start": "2740380",
    "end": "2752360"
  },
  {
    "text": "We've rebuilt our network\nand a new basis set, in a rotated basis set\nwhere everything simplifies.",
    "start": "2752360",
    "end": "2759860"
  },
  {
    "text": "So we've taken this\ncomplicated network with recurrent connections\nand we've rewritten it",
    "start": "2759860",
    "end": "2767539"
  },
  {
    "text": "in a new network, where\neach of these neurons in our new network\ncorresponds to what's",
    "start": "2767540",
    "end": "2773480"
  },
  {
    "text": "called a mode of the\nfully recurrent network.",
    "start": "2773480",
    "end": "2778820"
  },
  {
    "text": " So the activities c alpha c1\nand c2 of the network modes",
    "start": "2778820",
    "end": "2788849"
  },
  {
    "text": "represent kind of an activity\nin a linear combination of these neurons.",
    "start": "2788850",
    "end": "2795180"
  },
  {
    "text": "So we're going to go\nthrough what that means now.",
    "start": "2795180",
    "end": "2800359"
  },
  {
    "text": "So the first thing I want\nto do is just calculate what the steady state\nresponse is in this neuron.",
    "start": "2800360",
    "end": "2806960"
  },
  {
    "text": "And I'll just do\nit mathematically, and then I'll show you what\nit looks like graphically. ",
    "start": "2806960",
    "end": "2814400"
  },
  {
    "text": "So there's our original\nnetwork equation. We've rewritten it a set\nof differential equations",
    "start": "2814400",
    "end": "2820380"
  },
  {
    "text": "for the modes of this network. ",
    "start": "2820380",
    "end": "2826470"
  },
  {
    "text": "I'm just rewriting this\nby putting an I here, minus I times c.",
    "start": "2826470",
    "end": "2832040"
  },
  {
    "text": "That's the only\nchange I made here. I just rewrote it like this. ",
    "start": "2832040",
    "end": "2840450"
  },
  {
    "text": "Let's find a steady state. So we're going to set\ndc dt equal to zero. We're going to ask, what\nis c in steady state?",
    "start": "2840450",
    "end": "2848850"
  },
  {
    "text": "So we're going to call\nthat c infinity, all right? I minus lambda times c infinity\nequals phi transpose h.",
    "start": "2848850",
    "end": "2857520"
  },
  {
    "text": "OK, don't panic. It's all going to be\nvery simple in a second. c infinity is just I minus\nlambda inverse phi transpose h.",
    "start": "2857520",
    "end": "2867230"
  },
  {
    "text": "But I is diagonal. Lambda is diagonal. So I minus lambda\ninverse is just the--",
    "start": "2867230",
    "end": "2873730"
  },
  {
    "text": "it's a diagonal matrix with\nthese elements with one over all those\ndiagonal elements.",
    "start": "2873730",
    "end": "2880145"
  },
  {
    "text": " Now let's calculate v\ninfinity. v infinity",
    "start": "2880145",
    "end": "2886870"
  },
  {
    "text": "is just phi times v infinity. So here, we're multiplying\non the left by phi.",
    "start": "2886870",
    "end": "2892390"
  },
  {
    "text": "That's just v infinity. So v infinity is just this. So what is this?",
    "start": "2892390",
    "end": "2898330"
  },
  {
    "text": "This just says v\ninfinity is some matrix-- it's a rotated stretch matrix--",
    "start": "2898330",
    "end": "2903940"
  },
  {
    "text": "times the input. So v infinity is just\nthis matrix times h.",
    "start": "2903940",
    "end": "2910500"
  },
  {
    "text": "And now let's look\nat what that is.  v infinity is a matrix times h.",
    "start": "2910500",
    "end": "2917610"
  },
  {
    "text": "We're going to call that g. v infinity is a gain matrix. We're going to think of that\nas a gain times the input.",
    "start": "2917610",
    "end": "2925270"
  },
  {
    "text": "So it's just a matrix\noperation on the input.",
    "start": "2925270",
    "end": "2930800"
  },
  {
    "text": "This matrix has exactly\nthe same eigenvectors as m. And the eigenvalues are\njust 1 over 1 minus lambda.",
    "start": "2930800",
    "end": "2939289"
  },
  {
    "text": " Hang in there. So what this means is that\nif an input is parallel",
    "start": "2939290",
    "end": "2947060"
  },
  {
    "text": "to one of the eigenvectors\nof the weight matrix, that means the output is\nparallel to the input.",
    "start": "2947060",
    "end": "2952520"
  },
  {
    "text": " So if the input is\nin the direction",
    "start": "2952520",
    "end": "2959240"
  },
  {
    "text": "of one of the eigenvectors,\nv infinity is g times f.",
    "start": "2959240",
    "end": "2965720"
  },
  {
    "text": "But g times f-- f is an eigenvector\nv. And what that means",
    "start": "2965720",
    "end": "2971309"
  },
  {
    "text": "is that v infinity is parallel\nto f with a scaling factor 1 over 1 minus lambda.",
    "start": "2971310",
    "end": "2979309"
  },
  {
    "text": "All right? So hang in there. I'm going to show you\nwhat this looks like. So in steady state, the output\nwill be parallel to the input",
    "start": "2979310",
    "end": "2988480"
  },
  {
    "text": "if the input is in\nthe direction of one of the eigenvectors\nof the network. ",
    "start": "2988480",
    "end": "2997609"
  },
  {
    "text": "So if the input is in\nthe direction of one of the eigenvectors\nof the network, that means you're activating\nonly one mode of the network.",
    "start": "2997610",
    "end": "3007770"
  },
  {
    "text": "And only that one mode responds,\nand none of the other modes respond. ",
    "start": "3007770",
    "end": "3015840"
  },
  {
    "text": "The response of\nthe network will be in the direction of\nthat input, and it will be amplified or\nsuppressed by this gain factor.",
    "start": "3015840",
    "end": "3024480"
  },
  {
    "text": "And the time constant will\nalso be increased or decreased by that factor.",
    "start": "3024480",
    "end": "3030350"
  },
  {
    "text": "So now let's look at--\nso I just kind of whizzed through a bunch of math. Let's look at what this\nlooks like graphically",
    "start": "3030350",
    "end": "3036400"
  },
  {
    "text": "for a few simple cases. And then I think it will\nbecome much more clear.",
    "start": "3036400",
    "end": "3041440"
  },
  {
    "text": "Let's just look at\na simple network, where we have two neurons\nwith an excitatory connection",
    "start": "3041440",
    "end": "3047740"
  },
  {
    "text": "from neuron one to neuron\ntwo, an excitatory connection from neuron two to neuron one.",
    "start": "3047740",
    "end": "3053440"
  },
  {
    "text": "And we're going to\nmake that weight 0.8. OK, so what is the weight\nmatrix M look like?",
    "start": "3053440",
    "end": "3060220"
  },
  {
    "text": "Just tell me what the\nentries are for M. AUDIENCE: Does it\nnot have the autapse?",
    "start": "3060220",
    "end": "3065630"
  },
  {
    "text": "MICHALE FEE: No, so\nthere's no connection of any of these neurons\nonto themselves.",
    "start": "3065630",
    "end": "3073450"
  },
  {
    "text": "AUDIENCE: So you have,\nlike, zeros on the diagonal. MICHALE FEE: Zeros\non the diagonal. Good. AUDIENCE: All the diagonals.",
    "start": "3073450",
    "end": "3079840"
  },
  {
    "text": "MICHALE FEE: Good. Like that? Good. Connection from neuron\none to itself is zero.",
    "start": "3079840",
    "end": "3086510"
  },
  {
    "text": "The connection from\npost, pre is row, column.",
    "start": "3086510",
    "end": "3092330"
  },
  {
    "text": "So onto neuron one\nfrom neuron two is 0.8. Onto neuron two from\nneuron one is 0.8.",
    "start": "3092330",
    "end": "3100460"
  },
  {
    "text": "And neuron two onto\nneuron two is zero. ",
    "start": "3100460",
    "end": "3106400"
  },
  {
    "text": "So now we are just going to\ndiagonalize this weight matrix. We're going to find the\neigenvectors and eigenvalues.",
    "start": "3106400",
    "end": "3118660"
  },
  {
    "text": "The eigenvectors are\nthe columns of phi. And the eigenvalues are the\ndiagonal elements of lambda.",
    "start": "3118660",
    "end": "3124865"
  },
  {
    "text": " Let's take a look at what\nthose eigenvectors are.",
    "start": "3124865",
    "end": "3130720"
  },
  {
    "text": "So this vector here is f1. This vector here is\nanother eigenvector, f2.",
    "start": "3130720",
    "end": "3136370"
  },
  {
    "text": " And how did I get this? ",
    "start": "3136370",
    "end": "3144260"
  },
  {
    "text": "How did I get this from this? How would you do that? If I gave you this matrix,\nhow would you find phi?",
    "start": "3144260",
    "end": "3152044"
  },
  {
    "text": "AUDIENCE: Eig M. MICHALE FEE: Good,\neig of M. Now,",
    "start": "3152044",
    "end": "3157579"
  },
  {
    "text": "remember in the\nlast lecture when we were talking about some\nsimple cases of matrices",
    "start": "3157580",
    "end": "3165349"
  },
  {
    "text": "that are really easy to\nfind the eigenvectors of? If you have a symmetric matrix,\nwhere the diagonal elements are",
    "start": "3165350",
    "end": "3173350"
  },
  {
    "text": "equal to each other,\nthe eigenvectors are always 45 degrees\nhere and 45 degrees there.",
    "start": "3173350",
    "end": "3181070"
  },
  {
    "text": "And the eigenvalues are just the\ndiagonal elements plus or minus",
    "start": "3181070",
    "end": "3187000"
  },
  {
    "text": "the off-diagonal elements. So the eigenvalues here\nare 0.8 and minus 0.8.",
    "start": "3187000",
    "end": "3195460"
  },
  {
    "text": "All right, so those are the two\neigenvectors of this matrix,",
    "start": "3195460",
    "end": "3202349"
  },
  {
    "text": "of this network.  Those are the modes\nof the network.",
    "start": "3202350",
    "end": "3209860"
  },
  {
    "text": "Notice that one of the modes\ncorresponds to neuron one and neuron two firing together.",
    "start": "3209860",
    "end": "3217559"
  },
  {
    "text": "The other mode corresponds\nto neuron one and neuron two firing with opposite sign--",
    "start": "3217560",
    "end": "3223200"
  },
  {
    "text": " minus one, one.",
    "start": "3223200",
    "end": "3230020"
  },
  {
    "text": "So the lambda-- the diagonal\nelements of the lambda matrix are the eigenvalues.",
    "start": "3230020",
    "end": "3236120"
  },
  {
    "text": "They're 0.8 and minus\n0.8, a plus or minus b.",
    "start": "3236120",
    "end": "3244410"
  },
  {
    "text": "Now, this gain\nfactor, what this says is that if I have an input\nin the direction of f1,",
    "start": "3244410",
    "end": "3251960"
  },
  {
    "text": "the response is going to\nbe amplified by a gain. And remember, we just derived,\non the previous slide,",
    "start": "3251960",
    "end": "3257720"
  },
  {
    "text": "that that gain factor\nis just 1 over 1 minus the eigenvalue\nfor that eigenvector.",
    "start": "3257720",
    "end": "3266360"
  },
  {
    "text": "In this case, the eigenvalue\nfor mode one is 0.8.",
    "start": "3266360",
    "end": "3274270"
  },
  {
    "text": "So 1 over 1 minus 0.8 is 5. So the gain in this\ndirection is 5.",
    "start": "3274270",
    "end": "3283180"
  },
  {
    "text": "The gain for an input\nin this direction is 1 over 1 minus negative\n0.8, which is 1 over 1.8.",
    "start": "3283180",
    "end": "3296380"
  },
  {
    "text": "Does that makes sense? OK, let's keep going,\nbecause I think it will make even\nmore sense once we",
    "start": "3296380",
    "end": "3301960"
  },
  {
    "text": "see how the network\nresponds to its inputs. ",
    "start": "3301960",
    "end": "3310910"
  },
  {
    "text": "So zero input. Now we're going to put an input\nin the direction of this mode",
    "start": "3310910",
    "end": "3316220"
  },
  {
    "text": "one. And you can see the\nmode responds a lot. Put a negative input\nin, it responds a lot.",
    "start": "3316220",
    "end": "3323000"
  },
  {
    "text": "If we put a mode input in this\ndirection or this direction, the response is suppressed\nby an amount of about 0.5.",
    "start": "3323000",
    "end": "3337339"
  },
  {
    "text": "Because here, the gain is small. Here, the gain is big. So you see what's happening?",
    "start": "3337340",
    "end": "3343414"
  },
  {
    "text": "This network looks just\nlike an autapse network,",
    "start": "3343414",
    "end": "3350070"
  },
  {
    "text": "but where we've taken this\ninput and output space and just rotated it into a new coordinate\nsystem, into this new basis.",
    "start": "3350070",
    "end": "3360410"
  },
  {
    "text": "Yes? AUDIENCE: Why did it\nkind of loop around on the one side [INAUDIBLE]? MICHALE FEE: OK, it's because\nthese things are relaxing",
    "start": "3360410",
    "end": "3368970"
  },
  {
    "text": "exponentially back to zero. And we got a little\nbit impatient and started the next input\nbefore it had quite gone away.",
    "start": "3368970",
    "end": "3376559"
  },
  {
    "text": "OK, good question. It's just that if you really\nwait for a long time for it",
    "start": "3376560",
    "end": "3381750"
  },
  {
    "text": "to settle, then the movie\njust takes a long time. But maybe it would\nbe better to do that.",
    "start": "3381750",
    "end": "3386850"
  },
  {
    "text": "So input this way and this\nway lead to a large response, because those inputs activate\nmode one, which has a big gain.",
    "start": "3386850",
    "end": "3395280"
  },
  {
    "text": "Inputs in this direction\nand this direction have a small response,\nbecause they activate",
    "start": "3395280",
    "end": "3401450"
  },
  {
    "text": "mode two, which has small gain. But notice that when\nyou activate mode one--",
    "start": "3401450",
    "end": "3411730"
  },
  {
    "text": "when you put an input\nin this direction, it only activates mode one.",
    "start": "3411730",
    "end": "3418000"
  },
  {
    "text": "And it doesn't activate\nmode two at all. If you put an input\nin this direction,",
    "start": "3418000",
    "end": "3423830"
  },
  {
    "text": "then it only activates\nmode two, and it doesn't activate mode one at all. ",
    "start": "3423830",
    "end": "3431220"
  },
  {
    "text": "So it's just like the\nautapse network, but rotated. ",
    "start": "3431220",
    "end": "3438490"
  },
  {
    "text": "So now let's do the case\nwhere we have an input that",
    "start": "3438490",
    "end": "3447830"
  },
  {
    "text": "activates both modes. So let's say we put an\ninput in this direction.",
    "start": "3447830",
    "end": "3453300"
  },
  {
    "text": "What does that direction\ncorrespond to h up. What is that input mean\nhere in terms of h1 and h2?",
    "start": "3453300",
    "end": "3461330"
  },
  {
    "start": "3461330",
    "end": "3466560"
  },
  {
    "text": "Let's say we just put\nan input-- remember, this is a plot on\naxes h1 versus h2.",
    "start": "3466560",
    "end": "3475050"
  },
  {
    "text": "So this input\nvector h corresponds to just putting an input\non h2, into this neuron.",
    "start": "3475050",
    "end": "3484220"
  },
  {
    "text": "So you can see that when we\nput an input in this direction, we're activating--",
    "start": "3484220",
    "end": "3489500"
  },
  {
    "text": "that input has a projection\nonto mode one and mode two. So we're activating both modes.",
    "start": "3489500",
    "end": "3496070"
  },
  {
    "text": " You can see that the\ninput h has a projection",
    "start": "3496070",
    "end": "3503280"
  },
  {
    "text": "onto f1 and projection onto f2. So what you do is--",
    "start": "3503280",
    "end": "3508859"
  },
  {
    "start": "3508860",
    "end": "3514090"
  },
  {
    "text": "well, here, I'm just showing\nyou what the steady state response is mathematically.",
    "start": "3514090",
    "end": "3519490"
  },
  {
    "text": "Let me just show you\nwhat that looks like. What this says is that if we\nput an h in this direction,",
    "start": "3519490",
    "end": "3526250"
  },
  {
    "text": "it's going to activate\na little bit of mode one with a big gain and a\nlittle bit of mode two",
    "start": "3526250",
    "end": "3534940"
  },
  {
    "text": "with a very small gain. And so the steady state response\nwill be the sum of those two.",
    "start": "3534940",
    "end": "3541880"
  },
  {
    "text": "It'll be up here. So the steady state response\nto this input in this direction",
    "start": "3541880",
    "end": "3549119"
  },
  {
    "text": "is going to be over here. Why? Because that input activates\nmode one and mode two both.",
    "start": "3549120",
    "end": "3556680"
  },
  {
    "text": "But the response\nof mode one is big, and the response of mode\ntwo is really small.",
    "start": "3556680",
    "end": "3563150"
  },
  {
    "text": "And so the steady\nstate response is going to be way\nover here because",
    "start": "3563150",
    "end": "3569180"
  },
  {
    "text": "of the big response, the\namplified response of mode two, which is in this direction, OK?",
    "start": "3569180",
    "end": "3575750"
  },
  {
    "text": "So when we put an\ninput straight up, the response of\nthe network's going to be all the way over here.",
    "start": "3575750",
    "end": "3580760"
  },
  {
    "text": "How is it going to get there? Let's take a look. ",
    "start": "3580760",
    "end": "3592569"
  },
  {
    "text": "We're going to put an input-- sorry, that was first\nin this direction.",
    "start": "3592570",
    "end": "3598283"
  },
  {
    "text": "Now let's see what\nhappens when we put an input in this direction. You can see the response is\nreally big along the mode one",
    "start": "3598283",
    "end": "3606150"
  },
  {
    "text": "direction, in this\ndirection, and it's really small in this direction.",
    "start": "3606150",
    "end": "3612550"
  },
  {
    "text": "So input up in the upward\ndirection onto just this neuron",
    "start": "3612550",
    "end": "3618380"
  },
  {
    "text": "produces a large\nresponse in mode, which is this way, and\na very small response",
    "start": "3618380",
    "end": "3624020"
  },
  {
    "text": "in mode two, which is this way. The response in mode two is\nvery fast, because the lambda,",
    "start": "3624020",
    "end": "3632380"
  },
  {
    "text": "the 1 over 1 minus\nlambda, is small, which makes the\ntime constant faster",
    "start": "3632380",
    "end": "3639809"
  },
  {
    "text": "and the response smaller. So, again, it's just\nlike the response",
    "start": "3639810",
    "end": "3645990"
  },
  {
    "text": "of the autapse\nnetwork, but rotated into a new coordinate system.",
    "start": "3645990",
    "end": "3651630"
  },
  {
    "start": "3651630",
    "end": "3656670"
  },
  {
    "text": "All right, any\nquestions about that? ",
    "start": "3656670",
    "end": "3662610"
  },
  {
    "text": "So you can see we basically\nunderstood everything we needed to know about\nrecurrent networks",
    "start": "3662610",
    "end": "3669660"
  },
  {
    "text": "just by understanding simple\nnetworks with just autapses.",
    "start": "3669660",
    "end": "3677920"
  },
  {
    "text": "And all these more\ncomplicated networks are just nothing\nbut rotated versions",
    "start": "3677920",
    "end": "3685190"
  },
  {
    "text": "of the response of a\nnetwork with just autapses. ",
    "start": "3685190",
    "end": "3696998"
  },
  {
    "text": "Any questions about that?  OK, let's do another\nnetwork now where",
    "start": "3696998",
    "end": "3704350"
  },
  {
    "text": "we have inhibitory connections. That's called mutual inhibition.",
    "start": "3704350",
    "end": "3710349"
  },
  {
    "text": "And let's make that\ninhibition minus 0.8. The weight matrix is just\nzeros on the diagonals,",
    "start": "3710350",
    "end": "3715690"
  },
  {
    "text": "because there's no autapse here. And minus 0.8 on\nthe off-diagonals.",
    "start": "3715690",
    "end": "3723230"
  },
  {
    "text": "What are the eigenvectors for\nthis matrix, for this network?",
    "start": "3723230",
    "end": "3730338"
  },
  {
    "text": "AUDIENCE: The same. MICHALE FEE: Yeah,\nbecause the diagonal elements are equal\nto each other,",
    "start": "3730338",
    "end": "3735430"
  },
  {
    "text": "and the off-diagonal elements\nare equal to each other. It's a symmetric network\nwith equal diagonal elements.",
    "start": "3735430",
    "end": "3741990"
  },
  {
    "text": "The eigenvectors are\nalways at 45 degrees. And what are the eigenvalues?",
    "start": "3741990",
    "end": "3748000"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE]",
    "start": "3748000",
    "end": "3754369"
  },
  {
    "text": "MICHALE FEE: Well,\nthe two numbers are going to be the same. It's zero plus and minus 0.8,\nplus and minus negative 0.8,",
    "start": "3754370",
    "end": "3764320"
  },
  {
    "text": "which is just 0.8\nand minus 0.8, right? Good. So the eigenvalues are\njust 0.8 and minus 0.8.",
    "start": "3764320",
    "end": "3771390"
  },
  {
    "text": "But the eigenvalues correspond\nto different eigenvectors. So now the eigenvalue\nmode in the 1,",
    "start": "3771390",
    "end": "3779760"
  },
  {
    "text": "1 direction is now\nminus 0.8, which means it's suppressing the\nresponse in this direction.",
    "start": "3779760",
    "end": "3789270"
  },
  {
    "text": "And the eigenvalue for the\neigenvector in the minus 1, 1 direction is now\nclose to 1, which",
    "start": "3789270",
    "end": "3796530"
  },
  {
    "text": "means that mode has a lot\nof recurrent feedback. And so its response in this\ndirection is going to be big.",
    "start": "3796530",
    "end": "3805480"
  },
  {
    "text": "It's going to be amplified. So unlike the case where we had\npositive recurrent synapses,",
    "start": "3805480",
    "end": "3811580"
  },
  {
    "text": "where we had amplification\nin this direction, now we're going to\nhave amplification",
    "start": "3811580",
    "end": "3817920"
  },
  {
    "text": "in this direction. Does that make sense? ",
    "start": "3817920",
    "end": "3823500"
  },
  {
    "text": "Think of it this way-- if we go back to\nthis network here, you can see that when\nthese two neurons--",
    "start": "3823500",
    "end": "3831950"
  },
  {
    "text": "when this neuron is active, it\ntends to activate this neuron. And when this\nneuron is activate,",
    "start": "3831950",
    "end": "3838230"
  },
  {
    "text": "it tends to activate\nthat neuron. So this network, if you were to\nactivate one of these neurons,",
    "start": "3838230",
    "end": "3845150"
  },
  {
    "text": "it tends to drive the\nother neuron also. And so the activity of those two\nneurons likes to go together.",
    "start": "3845150",
    "end": "3853040"
  },
  {
    "text": "When one is big, the\nother one wants to be big. And that's why there's a lot\nof gain in this direction.",
    "start": "3853040",
    "end": "3861800"
  },
  {
    "text": "Does that make sense? With these recurrent\nexcitatory connections, it's hard to make\nthis neuron fire",
    "start": "3861800",
    "end": "3869859"
  },
  {
    "text": "and make that neuron not fire. And that's why the response is\nsuppressed in this direction,",
    "start": "3869860",
    "end": "3876300"
  },
  {
    "text": "OK? With this network, when\nthis neuron is active,",
    "start": "3876300",
    "end": "3881920"
  },
  {
    "text": "it's trying to\nsuppress that neuron. ",
    "start": "3881920",
    "end": "3887100"
  },
  {
    "text": "When that neuron has\npositive firing rate, it's trying to make that neuron\nhave a negative firing rate. When that neuron is\nnegative, it tries",
    "start": "3887100",
    "end": "3893850"
  },
  {
    "text": "to make that one go positive. And so this network\nlikes to have one firing positive and the\nother neuron going negative.",
    "start": "3893850",
    "end": "3904990"
  },
  {
    "text": "And so that's what happens. What you find is that if you put\nan input into the first neuron,",
    "start": "3904990",
    "end": "3916580"
  },
  {
    "text": "it tends to suppress the\nactivity in the second neuron, in v2.",
    "start": "3916580",
    "end": "3921980"
  },
  {
    "text": "If you put neuron\ninto neuron two,",
    "start": "3921980",
    "end": "3927390"
  },
  {
    "text": "it tends to suppress\nthe activity, or make v1 go negative. So it's, again, exactly\nlike the autapse network,",
    "start": "3927390",
    "end": "3936810"
  },
  {
    "text": "but just, in this case, rotated\nminus 45 degrees instead",
    "start": "3936810",
    "end": "3942590"
  },
  {
    "text": "of plus 45 degrees, OK? ",
    "start": "3942590",
    "end": "3951750"
  },
  {
    "text": "Any questions about that? All right. So now let's talk about how--",
    "start": "3951750",
    "end": "3959830"
  },
  {
    "text": "yes, Linda? AUDIENCE: So we just did, those\nwere all symmetric matrices, right? MICHALE FEE: Yes.",
    "start": "3959830",
    "end": "3965098"
  },
  {
    "text": "AUDIENCE: So [INAUDIBLE]\ncan we not do this strategy if it's not symmetric? MICHALE FEE: You can do it\nfor non-symmetric matrices,",
    "start": "3965098",
    "end": "3971690"
  },
  {
    "text": "but non-symmetric\nmatrices start doing all kinds of other\ncool stuff that",
    "start": "3971690",
    "end": "3977330"
  },
  {
    "text": "is a topic for another day. So symmetric matrices\nare special in that they",
    "start": "3977330",
    "end": "3985650"
  },
  {
    "text": "have very simple dynamics.",
    "start": "3985650",
    "end": "3991380"
  },
  {
    "text": "They just relax to a\nsteady state solution.",
    "start": "3991380",
    "end": "3997930"
  },
  {
    "text": "Weight matrices that are\nnot symmetric, or even anti-symmetric, tend to\ndo really cool things",
    "start": "3997930",
    "end": "4003230"
  },
  {
    "text": "like oscillating. And we'll get to that in\nanother lecture, all right?",
    "start": "4003230",
    "end": "4010670"
  },
  {
    "text": "OK, so now let's talk about\nusing recurrent networks to store memories.",
    "start": "4010670",
    "end": "4017150"
  },
  {
    "text": "So, remember, all of\nthe cases we've just described, all of the\nnetworks we've just described,",
    "start": "4017150",
    "end": "4023960"
  },
  {
    "text": "had the properties that the\nlambdas were less than one. So what we've been\nlooking at are",
    "start": "4023960",
    "end": "4030190"
  },
  {
    "text": "networks for which\nlambda is less than one and they're symmetric\nweight matrices.",
    "start": "4030190",
    "end": "4038320"
  },
  {
    "text": "So that was kind\nof a special case, but it's a good case\nfor building intuition about what goes on.",
    "start": "4038320",
    "end": "4044050"
  },
  {
    "text": "But now we're going\nto start branching out into more interesting behavior.",
    "start": "4044050",
    "end": "4050310"
  },
  {
    "text": " So let's take a look at what\nhappens to our equation.",
    "start": "4050310",
    "end": "4057090"
  },
  {
    "text": "This is now our equation\ndifferent modes of a network. What happens to this equation\nwhen lambda is actually",
    "start": "4057090",
    "end": "4063930"
  },
  {
    "text": "equal to one? So when lambda is equal to one,\nthis term goes to zero, right?",
    "start": "4063930",
    "end": "4072210"
  },
  {
    "text": "So we can just cross this\nout and rewrite our equation",
    "start": "4072210",
    "end": "4078170"
  },
  {
    "text": "as tau dc dt equals f1 f dot h.",
    "start": "4078170",
    "end": "4086710"
  },
  {
    "text": "So what is this? What does that look like? ",
    "start": "4086710",
    "end": "4093850"
  },
  {
    "text": "What's the solution to c for\nthis differential equation?",
    "start": "4093850",
    "end": "4101130"
  },
  {
    "text": "Does this exponentially\nrelax toward a v infinity? ",
    "start": "4101130",
    "end": "4109640"
  },
  {
    "text": "What is v infinity here? It's not even defined.",
    "start": "4109640",
    "end": "4114770"
  },
  {
    "text": "If you set dc dt equal to\nzero, there's not even a c to solve for, right?",
    "start": "4114770",
    "end": "4119899"
  },
  {
    "text": "So what is this? ",
    "start": "4119899",
    "end": "4126290"
  },
  {
    "text": "The derivative of c\nis just equal to-- if we put in an input\nthat's constant, what is c?",
    "start": "4126290",
    "end": "4135237"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] MICHALE FEE: This is\nan integrator, right?",
    "start": "4135238",
    "end": "4140289"
  },
  {
    "text": "This c, the solution\nto this equation, is that c is the\nintegral of this input.",
    "start": "4140290",
    "end": "4150960"
  },
  {
    "text": "c is some initial c plus\nthe integral over time.",
    "start": "4150960",
    "end": "4156960"
  },
  {
    "start": "4156960",
    "end": "4162278"
  },
  {
    "text": "So if we have an input-- and again, what\nwe're plotting here",
    "start": "4162279",
    "end": "4168049"
  },
  {
    "text": "is the activity of one of\nthe modes of our network, c1,",
    "start": "4168050",
    "end": "4174369"
  },
  {
    "text": "which is a function\nof the projection of the input along the\neigenvector of mode one.",
    "start": "4174370",
    "end": "4182350"
  },
  {
    "text": "So we're going to plot h, which\nis just how much the input overlaps with mode one.",
    "start": "4182350",
    "end": "4190000"
  },
  {
    "text": "And as a function of time,\nlet's start at one equals zero. What will this look like? ",
    "start": "4190000",
    "end": "4199710"
  },
  {
    "text": "This will just\nincrease linearly. And then what happens? ",
    "start": "4199710",
    "end": "4206993"
  },
  {
    "text": "What happens here? ",
    "start": "4206993",
    "end": "4213650"
  },
  {
    "text": "Raymundo? AUDIENCE: R just stays constant. MICHALE FEE: Good. We've been through that,\nlike, 100 times in this class.",
    "start": "4213650",
    "end": "4221730"
  },
  {
    "text": " Now, what's special about\nthis network is that remember,",
    "start": "4221730",
    "end": "4233600"
  },
  {
    "text": "when lambda was less\nthan one, the network would respond to the input.",
    "start": "4233600",
    "end": "4239120"
  },
  {
    "text": "And then what would it do\nwhen we took the input away? ",
    "start": "4239120",
    "end": "4244830"
  },
  {
    "text": "It would decay back to zero. But this network does\nsomething really special.",
    "start": "4244830",
    "end": "4251070"
  },
  {
    "text": "This network, you put\nan input in and then take the input away, this\nnetwork stays active.",
    "start": "4251070",
    "end": "4258360"
  },
  {
    "text": "It remembers what the input was. Whereas, if you have a network\nwhere lambda is less than one,",
    "start": "4258360",
    "end": "4266219"
  },
  {
    "text": "the network very quickly\nforgets what the input was.",
    "start": "4266220",
    "end": "4272237"
  },
  {
    "text": "All right, what happens when\nlambda is greater than one? So when lambda is greater\nthan one, this term is now--",
    "start": "4272237",
    "end": "4278920"
  },
  {
    "text": "this thing inside\nthe parentheses is negative, multiplied\nby a negative number. This whole coefficient in front\nof the c1 becomes positive.",
    "start": "4278920",
    "end": "4287290"
  },
  {
    "text": "So we're just going to write\nit as lambda minus one. And so this because positive.",
    "start": "4287290",
    "end": "4293980"
  },
  {
    "text": "And what does that\nsolution look like? Does anyone know\nwhat that looks like? dc dt equals a positive\nnumber times c.",
    "start": "4293980",
    "end": "4300760"
  },
  {
    "start": "4300760",
    "end": "4309789"
  },
  {
    "text": "Nobody? Are we all just sleepy? ",
    "start": "4309790",
    "end": "4317278"
  },
  {
    "text": "What happens?  So if this is negative, if this\ncoefficient were negative, dc--",
    "start": "4317278",
    "end": "4324950"
  },
  {
    "text": "if c is positive, then\ndc dt is negative, and it relaxes to zero, right?",
    "start": "4324950",
    "end": "4331620"
  },
  {
    "text": "Lets think about\nthis for a minute. What happens if this\nquantity is positive? So if c is positive--",
    "start": "4331620",
    "end": "4336739"
  },
  {
    "text": " cover that up. If this is positive\nand c is positive,",
    "start": "4336740",
    "end": "4344090"
  },
  {
    "text": "then dc dt is positive. So that means if c is positive,\nit just keeps getting bigger,",
    "start": "4344090",
    "end": "4351789"
  },
  {
    "text": "right? And so what happens is you\nget exponential growth. So if we now take an input and\nwe put it into this network,",
    "start": "4351790",
    "end": "4359670"
  },
  {
    "text": "where lambda is\ngreater than one, you get exponential growth. And now what happens when\nyou turn that input off?",
    "start": "4359670",
    "end": "4367000"
  },
  {
    "start": "4367000",
    "end": "4373800"
  },
  {
    "text": "Does it go away? ",
    "start": "4373800",
    "end": "4386031"
  },
  {
    "text": "What happens? ",
    "start": "4386031",
    "end": "4392420"
  },
  {
    "text": "draw with their hand\nwhat happens here. ",
    "start": "4392420",
    "end": "4398360"
  },
  {
    "text": "So just look at the equation. Again, h dot f1 is zero\nhere, so that's gone.",
    "start": "4398360",
    "end": "4406449"
  },
  {
    "text": "This is positive. c is positive. So what is dc dt? ",
    "start": "4406450",
    "end": "4413950"
  },
  {
    "text": "Good. It's positive. And so what is-- AUDIENCE: [INAUDIBLE] MICHALE FEE: It keeps growing. ",
    "start": "4413950",
    "end": "4421710"
  },
  {
    "text": "So you can see that\nthis network also remembers that it had input.",
    "start": "4421710",
    "end": "4429020"
  },
  {
    "text": " So this network\nalso has a memory.",
    "start": "4429020",
    "end": "4434550"
  },
  {
    "text": "So anytime you have lambda\nless than one the network just-- as soon as\nthe input goes away,",
    "start": "4434550",
    "end": "4441060"
  },
  {
    "text": "the network activity\ngoes to zero, and it just completely forgets\nthat it ever had input.",
    "start": "4441060",
    "end": "4446219"
  },
  {
    "text": "Whereas, as long as lambda is\nequal to or greater than one, then this network remembers\nthat it had input.",
    "start": "4446220",
    "end": "4455639"
  },
  {
    "text": "So if lambda is less than\none, then the network relaxes exponentially back to\nzero after the input goes away.",
    "start": "4455640",
    "end": "4463330"
  },
  {
    "text": "If you have lambda equal to\none, you have an integrator, and the network\nactivity persists",
    "start": "4463330",
    "end": "4469350"
  },
  {
    "text": "after the input goes away. And if you have\nexponential growth, the network activity\nalso persists",
    "start": "4469350",
    "end": "4476400"
  },
  {
    "text": "after the input goes away.  And so that right there\nis one of the best",
    "start": "4476400",
    "end": "4485020"
  },
  {
    "text": "models for short-term\nmemory in the brain.",
    "start": "4485020",
    "end": "4492560"
  },
  {
    "text": "The idea that you have\nneurons that get input,",
    "start": "4492560",
    "end": "4498440"
  },
  {
    "text": "become activated, and\nthen hold that memory by reactivating themselves and\nholding their own activity high",
    "start": "4498440",
    "end": "4508340"
  },
  {
    "text": "through recurrent excitation. But that excitation\nhas to be big enough",
    "start": "4508340",
    "end": "4514430"
  },
  {
    "text": "to either just barely\nmaintain the activity or continue increasing\ntheir activity.",
    "start": "4514430",
    "end": "4522240"
  },
  {
    "text": "OK, now, that's not\nnecessarily such a great model for a memory, right? Because we can't have\nneurons whose activity is",
    "start": "4522240",
    "end": "4528989"
  },
  {
    "text": "exploding exponentially, right? So that's not so great. But it is quite commonly\nthought that in neural networks",
    "start": "4528990",
    "end": "4540070"
  },
  {
    "text": "involved in memory, the lambda\nis actually greater than one. And how would we\nrescue this situation?",
    "start": "4540070",
    "end": "4546020"
  },
  {
    "text": "How would we save our\nnetwork from having neurons that blow up exponentially? ",
    "start": "4546020",
    "end": "4553610"
  },
  {
    "text": "Well, remember, this\nwas the solution",
    "start": "4553610",
    "end": "4559429"
  },
  {
    "text": "for a network with\nlinear neurons. But neurons in the brain are\nnot really linear, are they?",
    "start": "4559430",
    "end": "4567449"
  },
  {
    "text": "They have firing\nrates that saturate. At higher inputs, firing\nrates tend [AUDIO OUT].. Why?",
    "start": "4567450",
    "end": "4572640"
  },
  {
    "text": "Because sodium channels\nbecome inactivated, and the neurons can't\nrespond that fast, right?",
    "start": "4572640",
    "end": "4578850"
  },
  {
    "start": "4578850",
    "end": "4589430"
  },
  {
    "text": "All right, this\nI've already said. So we use what are called\nsaturating non-linearities.",
    "start": "4589430",
    "end": "4597760"
  },
  {
    "text": "So it's very common\nto write down models in which we can still\nhave neurons that are--",
    "start": "4597760",
    "end": "4605230"
  },
  {
    "text": "we can still have them\napproximately linear. So it's quite often to\nhave neurons that are",
    "start": "4605230",
    "end": "4610510"
  },
  {
    "text": "linear for small [INAUDIBLE]. They can go plus and\nminus, but they saturate on the plus side or the minus.",
    "start": "4610510",
    "end": "4617170"
  },
  {
    "text": "So now you can have\nan input to a neuron that activates the neuron.",
    "start": "4617170",
    "end": "4623230"
  },
  {
    "text": "You can see what happens is you\nstart activating this neuron.",
    "start": "4623230",
    "end": "4628670"
  },
  {
    "text": "It keeps activating itself,\neven as the input goes away.",
    "start": "4628670",
    "end": "4634730"
  },
  {
    "text": "But now, what happens\nis that activity starts getting up into the\nregime where the neuron can't",
    "start": "4634730",
    "end": "4640490"
  },
  {
    "text": "fire any faster. And so the activity becomes\nstable at some high value",
    "start": "4640490",
    "end": "4648070"
  },
  {
    "text": "of firing. Does that make sense? And this kind of\nneuron, for example, can remember a plus input, or\nit can remember a minus input.",
    "start": "4648070",
    "end": "4658330"
  },
  {
    "text": " Does that make sense? So that's how we can\nbuild a simple network",
    "start": "4658330",
    "end": "4666050"
  },
  {
    "text": "with a neuron that can\nremember its previous inputs",
    "start": "4666050",
    "end": "4672949"
  },
  {
    "text": "with a lambda that's\ngreater than one. And this right here,\nthat basic thing,",
    "start": "4672950",
    "end": "4680540"
  },
  {
    "text": "is one of the models for\nhow the hippocampus stores",
    "start": "4680540",
    "end": "4685730"
  },
  {
    "text": "memories, that you have\nhippocampal neurons that connect to each other\nwith a lot of recurrent",
    "start": "4685730",
    "end": "4691489"
  },
  {
    "text": "connections [AUDIO OUT]\nin the hippocampus has a lot of\nrecurrent connections. And the idea is that those\nneurons activate each other,",
    "start": "4691490",
    "end": "4700100"
  },
  {
    "text": "but then those neurons saturate\nso they can't fire anymore, and now you can have a stable\nmemory of some prior input.",
    "start": "4700100",
    "end": "4709230"
  },
  {
    "start": "4709230",
    "end": "4717020"
  },
  {
    "text": "And I think we\nshould stop there. But there are other\nvery interesting topics",
    "start": "4717020",
    "end": "4722080"
  },
  {
    "text": "that we're going to get to\non how these kind of networks",
    "start": "4722080",
    "end": "4730990"
  },
  {
    "text": "can also make\ndecisions and how they can store continuous memories--\nnot just discrete memories,",
    "start": "4730990",
    "end": "4738190"
  },
  {
    "text": "plus or minus, on\nor off, but can store a value for a long period\nof time using this integrator.",
    "start": "4738190",
    "end": "4747540"
  },
  {
    "text": "OK, so we'll stop there. ",
    "start": "4747540",
    "end": "4752000"
  }
]