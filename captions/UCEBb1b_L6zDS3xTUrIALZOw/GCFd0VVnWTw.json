[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "7410"
  },
  {
    "text": "offer high-quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "7410",
    "end": "13960"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at",
    "start": "13960",
    "end": "19790"
  },
  {
    "text": "ocw.mit.edu.  PROFESSOR: OK, let's get started\nagain on finite-state",
    "start": "19790",
    "end": "29950"
  },
  {
    "text": "Markov chains. Sorry I was away last week. It was a long-term commitment\nthat I had to honor.",
    "start": "29950",
    "end": "38390"
  },
  {
    "text": "But I think I will\nbe around for all the rest of the lectures. So I want to start out by\nreviewing just a little bit.",
    "start": "38390",
    "end": "49949"
  },
  {
    "text": "I'm spending a lot more time on\nfinite-state Markov chains than we usually do in this\ncourse, partly because I've",
    "start": "49950",
    "end": "57940"
  },
  {
    "text": "rewritten this section, partly\nbecause I think the material is very important.",
    "start": "57940",
    "end": "64510"
  },
  {
    "text": "It's sort of bread-and-butter\nstuff, of",
    "start": "64510",
    "end": "69850"
  },
  {
    "text": "discrete stochastic processes. You use it all the time. It's a foundation for almost\neverything else.",
    "start": "69850",
    "end": "78080"
  },
  {
    "text": "And after thinking about it\nfor a long time, it really isn't all that complicated.",
    "start": "78080",
    "end": "83800"
  },
  {
    "text": "I used to think that all these\ndetails of finding eigenvalues and eigenvectors and so on\nwas extremely tedious.",
    "start": "83800",
    "end": "92580"
  },
  {
    "text": "And it turns out that\nthere's a very nice pleasant theory there.",
    "start": "92580",
    "end": "97850"
  },
  {
    "text": "You can find all of these things\nafter you know what you're doing by very simple\ncomputer packages.",
    "start": "97850",
    "end": "106790"
  },
  {
    "text": "But they don't help if you don't\nknow what's going on. So here, we're trying to figure\nout what's going on.",
    "start": "106790",
    "end": "112930"
  },
  {
    "text": "So let's start out by reviewing\nwhat we know about ergodic unit chains and\nproceed from there.",
    "start": "112930",
    "end": "125120"
  },
  {
    "text": "An ergodic finite-state Markov\nchain has transition",
    "start": "125120",
    "end": "130710"
  },
  {
    "text": "probabilities which, if you look\nat the transition matrix",
    "start": "130710",
    "end": "137190"
  },
  {
    "text": "raised to the nth power, what\nthat gives you is the transition probabilities of\nan n-step Markov chain.",
    "start": "137190",
    "end": "144439"
  },
  {
    "text": "In other words, you start at\ntime 0, and at time n, you look at what state you're in.",
    "start": "144440",
    "end": "151310"
  },
  {
    "text": "P sub ij to the nth power is\nthen the probability that",
    "start": "151310",
    "end": "158190"
  },
  {
    "text": "you're in state j at time\nn, given that you're in state i at time 0.",
    "start": "158190",
    "end": "165910"
  },
  {
    "text": "So this has all the information\nthat you want about what happens to Markov\nchain as time gets large.",
    "start": "165910",
    "end": "173180"
  },
  {
    "text": "One of the things we're most\nconcerned with is, do you go to steady state?",
    "start": "173180",
    "end": "178439"
  },
  {
    "text": "And if you do go to steady\nstate, how fast do you go to steady state? And of course, this matrix\ntells you the whole story",
    "start": "178440",
    "end": "185830"
  },
  {
    "text": "there, because if you go to\nsteady state, and the Markov",
    "start": "185830",
    "end": "191720"
  },
  {
    "text": "chain forgets where it started,\nthen P sub ij to the",
    "start": "191720",
    "end": "204390"
  },
  {
    "text": "n goes to some constant, pi sub\nj, which is independent of the starting state, i,\nand independent of m,",
    "start": "204390",
    "end": "213190"
  },
  {
    "text": "asymptotically, as n gets big. So this pi is a strictly\npositive probability vector.",
    "start": "213190",
    "end": "221730"
  },
  {
    "text": "I shouldn't say so it is. That's something that\nwas shown last time.",
    "start": "221730",
    "end": "227590"
  },
  {
    "text": "If you multiply both sides of\nthis equation by P sub jk in",
    "start": "227590",
    "end": "235530"
  },
  {
    "text": "sum over k, then what\ndo you get? You get P sub ik to\nthe n plus 1.",
    "start": "235530",
    "end": "241580"
  },
  {
    "text": "That goes to a limit also. If the limit in n\ngoes to infin-- ",
    "start": "241580",
    "end": "247670"
  },
  {
    "text": "then the limit as n plus 1 goes\nto infinity is clearly the same thing. So this quantity here is\nthe sum over j, of pi",
    "start": "247670",
    "end": "257199"
  },
  {
    "text": "sub j, P sub jk. And this quantity is equal to\npi sub k, just by definition",
    "start": "257200",
    "end": "265730"
  },
  {
    "text": "of this quantity. So P sub k is equal\nto sum of pi j. Pjk, what does that say?",
    "start": "265730",
    "end": "272050"
  },
  {
    "text": "That's the definition of\na steady state vector. That's the definition of, if\nyour probabilities of being in",
    "start": "272050",
    "end": "280310"
  },
  {
    "text": "state k satisfy this equation,\nthen one step later, you still",
    "start": "280310",
    "end": "286490"
  },
  {
    "text": "have the same probability\nof being in state k. Two steps later, you still have\nthe same probability of",
    "start": "286490",
    "end": "292870"
  },
  {
    "text": "being in state k. So this is called the steady\nstate equation.",
    "start": "292870",
    "end": "300430"
  },
  {
    "text": "And a solution to that is called\na steady state vector. And that satisfies this.",
    "start": "300430",
    "end": "307795"
  },
  {
    "text": "In matrix terms, if you rate\nthis out, what does it say? It says the limit as n\napproaches infinity of p to",
    "start": "307795",
    "end": "314800"
  },
  {
    "text": "the n is equal to the column\nvector, e of all 1s.",
    "start": "314800",
    "end": "321210"
  },
  {
    "text": " The transpose here means\nit's a column vector.",
    "start": "321210",
    "end": "326300"
  },
  {
    "text": "So you have a column vector\ntimes a row vector. Now, you know if you have a\nrow vector times a column",
    "start": "326300",
    "end": "332760"
  },
  {
    "text": "vector, that just gives\nyou a number. If you have a column\nvector times a row",
    "start": "332760",
    "end": "338960"
  },
  {
    "text": "vector, what happens? Well, for each element\nof the column, you",
    "start": "338960",
    "end": "346510"
  },
  {
    "text": "get this whole row. And for the next element of the\ncolumn, you get the whole row down beneath it multiplied\nby the element of the column,",
    "start": "346510",
    "end": "354790"
  },
  {
    "text": "and so forth, day on. So a column vector times\na row vector is, in",
    "start": "354790",
    "end": "361760"
  },
  {
    "text": "fact, a whole matrix. It's a j by j matrix.",
    "start": "361760",
    "end": "367590"
  },
  {
    "text": "And since e is all 1s, what\nthat matrix is is a matrix",
    "start": "367590",
    "end": "373900"
  },
  {
    "text": "where every row is a steady\nstate vector pi. So we're saying not only does\nthis pi that we're talking",
    "start": "373900",
    "end": "381950"
  },
  {
    "text": "about satisfy this steady\nstate equation, but more important, it's this limiting\nvector here.",
    "start": "381950",
    "end": "389639"
  },
  {
    "text": "And as n goes to infinity,\nyou in fact do forget where you were.",
    "start": "389640",
    "end": "394730"
  },
  {
    "text": "And the entire matrix of where\nyou are at time n, given where",
    "start": "394730",
    "end": "401660"
  },
  {
    "text": "you were at time 0, goes to\njust this fixed vector pi.",
    "start": "401660",
    "end": "406690"
  },
  {
    "text": "So this is a column vector, and\npi is a row vector then. ",
    "start": "406690",
    "end": "414660"
  },
  {
    "text": "The same result almost holds\nfor ergodic unit chains. What's an ergodic unit chain?",
    "start": "414660",
    "end": "421509"
  },
  {
    "text": "An ergodic unit chain is an\nergodic set of states plus a",
    "start": "421510",
    "end": "426740"
  },
  {
    "text": "whole bunch of transient\nstates. Doesn't matter whether the\ntransient states are one class",
    "start": "426740",
    "end": "432970"
  },
  {
    "text": "of transient states or whether\nit's multiple classes of transient states. It's just transient states.",
    "start": "432970",
    "end": "439850"
  },
  {
    "text": "And there's one recurrent\nclass. And we're assuming here\nthat it's recurrent.",
    "start": "439850",
    "end": "444970"
  },
  {
    "text": "So you can almost see\nintuitively that if you start out in any one of these\ntransient states, you bum",
    "start": "444970",
    "end": "451800"
  },
  {
    "text": "around through the transient\nstates for a while. And eventually, you flop off\ninto the recurrent class.",
    "start": "451800",
    "end": "459800"
  },
  {
    "text": "And once you're in\nthe recurrent class, there's no return. So you stay there forever.",
    "start": "459800",
    "end": "466220"
  },
  {
    "text": "Now, that's something that\nhas to be proven. And it's proven in the notes. It was probably proven\nlast time.",
    "start": "466220",
    "end": "471885"
  },
  {
    "text": " But anyway, what happens then\nis that the sole difference",
    "start": "471885",
    "end": "479250"
  },
  {
    "text": "between ergodic unit chains and\njust having a completely",
    "start": "479250",
    "end": "485090"
  },
  {
    "text": "ergodic Markov chain is that the\nsteady state factor is now",
    "start": "485090",
    "end": "490669"
  },
  {
    "text": "positive for all ergodic states\nand it's 0 for all transient states.",
    "start": "490670",
    "end": "496099"
  },
  {
    "text": "And aside from that, you still\nget the same behavior still. As n gets large, you go to the\nsteady state vector, which is",
    "start": "496100",
    "end": "506139"
  },
  {
    "text": "the steady state vector\nof the ergodic chain. If you're doing this stuff by\nhand, how do you do it?",
    "start": "506140",
    "end": "515500"
  },
  {
    "text": "Well, you start out just\nwith the ergodic class. I mean, you might as well\nignore everything else,",
    "start": "515500",
    "end": "522130"
  },
  {
    "text": "because you know that eventually\nyou're in that ergodic class. And you find the steady state\nvector in that ergodic class,",
    "start": "522130",
    "end": "529830"
  },
  {
    "text": "and that's the steady\nstate vector you're going to wind up with. This is one advantage of\nunderstanding what you're",
    "start": "529830",
    "end": "536350"
  },
  {
    "text": "doing, because if you don't\nunderstand what you're doing and you're just using computer\nprograms, then you never have",
    "start": "536350",
    "end": "544110"
  },
  {
    "text": "any idea what's ergodic,\nwhat's not ergodic or anything else. You just plug it, you grind\naway, you get some answer and",
    "start": "544110",
    "end": "551180"
  },
  {
    "text": "say, ah, I'll publish a paper. And you put down exactly what\nthe computer says, but you",
    "start": "551180",
    "end": "559220"
  },
  {
    "text": "have no interpretation\nof it at all. So the other way of looking at\nthis is, when you have a bunch",
    "start": "559220",
    "end": "567700"
  },
  {
    "text": "of transient states, and you\nalso have an ergodic class,",
    "start": "567700",
    "end": "573640"
  },
  {
    "text": "you can represent a matrix if\nthe recurrent states are at",
    "start": "573640",
    "end": "580050"
  },
  {
    "text": "the end of the chain and the\ntransient states are at the beginning of the chain.",
    "start": "580050",
    "end": "585830"
  },
  {
    "text": "This matrix here is the matrix\nof transition probabilities",
    "start": "585830",
    "end": "590850"
  },
  {
    "text": "within the recurrent class. These are the probabilities for\ngoing from the transient",
    "start": "590850",
    "end": "598870"
  },
  {
    "text": "states to the recurrent class. And once you get over\nhere, the only place",
    "start": "598870",
    "end": "604810"
  },
  {
    "text": "to go is down here. ",
    "start": "604810",
    "end": "610880"
  },
  {
    "text": "And the transient class is\njust a t by t class. And the recurrent class\nis just a j minus t",
    "start": "610880",
    "end": "620035"
  },
  {
    "text": "by j minus t matrix. So the idea is that each\ntransient state eventually has",
    "start": "620035",
    "end": "625250"
  },
  {
    "text": "a transition to a recurrent\nstate, and the class of recurrent states leads to\nstudy state as before.",
    "start": "625250",
    "end": "633769"
  },
  {
    "text": "So that really, all that\nanalysis of ergodic unit chains, if you look at it\nintuitively, it's all obvious.",
    "start": "633770",
    "end": "643470"
  },
  {
    "text": "Now, as in much of mathematics,\nknowing that something is obvious does not\nrelieve you of the need to",
    "start": "643470",
    "end": "652820"
  },
  {
    "text": "prove it, because sometimes you\nfind that something that looks obvious is true\nmost of the time but",
    "start": "652820",
    "end": "658800"
  },
  {
    "text": "not all of the time. And that's the purpose of\ndoing these things.",
    "start": "658800",
    "end": "664230"
  },
  {
    "text": "There's another way to express\nthis eigenvalue, eigenvector equation we have here.",
    "start": "664230",
    "end": "670700"
  },
  {
    "text": "And that is that the transition\nmatrix minus lambda",
    "start": "670700",
    "end": "676900"
  },
  {
    "text": "times the identity matrix times\nthe column vector v is",
    "start": "676900",
    "end": "683020"
  },
  {
    "text": "equal to 0. That's the same as the equation\np times v is equal to",
    "start": "683020",
    "end": "690760"
  },
  {
    "text": "v. That's the same as\na right eigenvector. Well, this is the equation\nfor an eigenvalue 1.",
    "start": "690760",
    "end": "698879"
  },
  {
    "text": "This is an equation for an\narbitrary eigenvalue lambda. But p times v equals lambda\ntimes v is the same as p minus",
    "start": "698880",
    "end": "708610"
  },
  {
    "text": "lambda i times v equals 0. Why do we even bother to say\nsomething so obvious?",
    "start": "708610",
    "end": "715010"
  },
  {
    "text": "Well, because when you look at\nlinear algebra, how many of you have never studied any\nlinear algebra at all or have",
    "start": "715010",
    "end": "724220"
  },
  {
    "text": "only studied completely\nmathematical linear algebra,",
    "start": "724220",
    "end": "729430"
  },
  {
    "text": "where you never deal with\nn-tuples as vectors or",
    "start": "729430",
    "end": "735000"
  },
  {
    "text": "matrices or any things\nlike this? Is there anyone? ",
    "start": "735000",
    "end": "741410"
  },
  {
    "text": "If you don't have this\nbackground, pick up-- ",
    "start": "741410",
    "end": "750330"
  },
  {
    "text": "what's his name? AUDIENCE: Strang. PROFESSOR: Strang. Strang's book. It's a remarkably simple-minded\nbook which says",
    "start": "750330",
    "end": "759090"
  },
  {
    "text": "everything as clearly\nas it can be stated. And it tells you everything\nyou have to know.",
    "start": "759090",
    "end": "765040"
  },
  {
    "text": "And it does it in a very\nstraightforward way. So I highly recommend it to get\nany of the background that",
    "start": "765040",
    "end": "772860"
  },
  {
    "text": "you might need. Most of you, I'm sure, are very familiar with these things. So I'm just reminding\nyou of then.",
    "start": "772860",
    "end": "779760"
  },
  {
    "text": "Now, a square matrix is singular\nif there's a vector v, such that a times\nv is equal to 0.",
    "start": "779760",
    "end": "787639"
  },
  {
    "text": "That's just a definition\nas a singularity. Now, lambda is an eigenvalue of\na matrix p if and only if p",
    "start": "787640",
    "end": "796340"
  },
  {
    "text": "minus lambda times\ni is singular. In other words, if there's\nsome v for which p minus",
    "start": "796340",
    "end": "803600"
  },
  {
    "text": "lambda i times v is equal to\n0, that's what this says. You put p minus lambda i in\nfor a, and it says it's",
    "start": "803600",
    "end": "812150"
  },
  {
    "text": "singular if there's some v\nfor which this matrix--",
    "start": "812150",
    "end": "817580"
  },
  {
    "text": "this matrix is singular if\nthere's some v such that p minus lambda i times\nv is equal to 0.",
    "start": "817580",
    "end": "824860"
  },
  {
    "text": "So let a1 to am be\nthe columns of a. Then a is going to be\nsingular if a1 to am",
    "start": "824860",
    "end": "832430"
  },
  {
    "text": "are linearly dependent. In other words, if there's some\nset of coefficients you",
    "start": "832430",
    "end": "842279"
  },
  {
    "text": "can attach to a1 times v1 plus\na2 times v2, plus up to am",
    "start": "842280",
    "end": "849000"
  },
  {
    "text": "times vm such that that sum is\nequal to 0, that means that a1",
    "start": "849000",
    "end": "855510"
  },
  {
    "text": "to am are linearly dependent. It also means that the matrix a\ntimes that v is equal to 0.",
    "start": "855510",
    "end": "864579"
  },
  {
    "text": "So those two things say\nthe same thing again. So the square matrix, a, is\nsingular if and only if the",
    "start": "864580",
    "end": "870200"
  },
  {
    "text": "rows of a are linearly\nindependent. We set columns here.",
    "start": "870200",
    "end": "876100"
  },
  {
    "text": "Here, we're doing the\nsame thing for rows. It still holds true. And one new thing, if and only\nif the determinant of a is",
    "start": "876100",
    "end": "884339"
  },
  {
    "text": "equal to 0. One of the nice things about\ndeterminants is that determinants are 0 if the matrix\nis singular, if and",
    "start": "884340",
    "end": "894470"
  },
  {
    "text": "only if the matrix\nis singular. So the summary of all of this\nfor a matrix which is a",
    "start": "894470",
    "end": "901440"
  },
  {
    "text": "transition matrix-- namely, a stochastic matrix-- is lambda, is an eigenvalue of\np, if and only if p minus",
    "start": "901440",
    "end": "910050"
  },
  {
    "text": "lambda i is singular, if and\nonly if the determinant of p minus lambda i is equal to 0,\nif and only if p times some",
    "start": "910050",
    "end": "919870"
  },
  {
    "text": "vector v equals lambda v, and\nif and only if u times p",
    "start": "919870",
    "end": "925150"
  },
  {
    "text": "equals lambda u for some u. Yes? AUDIENCE: The second to last\nstatement is actually linearly",
    "start": "925150",
    "end": "931672"
  },
  {
    "text": "independent, you said? The second to last. Square matrix a.",
    "start": "931672",
    "end": "938364"
  },
  {
    "text": "No, above that. PROFESSOR: Oh, above that. A square matrix a is singular\nif and only if the rows of a",
    "start": "938364",
    "end": "946910"
  },
  {
    "text": "are linearly dependent, yes. AUDIENCE: Dependent. PROFESSOR: Dependent, yes. In other words, if there's\nsome vector v such that a",
    "start": "946910",
    "end": "956050"
  },
  {
    "text": "times v is equal to 0, that\nmeans that those columns are",
    "start": "956050",
    "end": "968370"
  },
  {
    "text": "linearly dependent.  So we need all of those\nrelationships.",
    "start": "968370",
    "end": "976630"
  },
  {
    "text": "It says for every stochastic\nmatrix-- oh, now this is something new.",
    "start": "976630",
    "end": "982820"
  },
  {
    "text": "For every stochastic matrix,\nP times e is equal to e.",
    "start": "982820",
    "end": "988000"
  },
  {
    "text": "Obviously, because if you sum\nup the sum of Pij over j is",
    "start": "988000",
    "end": "1006040"
  },
  {
    "text": "equal to 1. P sub ij is the probability,\ngiven that you start in state",
    "start": "1006040",
    "end": "1011230"
  },
  {
    "text": "i, that in the next step,\nyou'll be in state j. You have to be somewhere\nin the next step.",
    "start": "1011230",
    "end": "1016650"
  },
  {
    "text": "So if you sum these quantities\nup, you have to get 1, which says you have to\nbe some place.",
    "start": "1016650",
    "end": "1023230"
  },
  {
    "text": "So that's all this is saying.  That's true for every\nfinite-state Markov chain in",
    "start": "1023230",
    "end": "1030579"
  },
  {
    "text": "the world, no matter how ugly\nit is, how many sets of",
    "start": "1030579",
    "end": "1035839"
  },
  {
    "text": "recurrent states it has, how\nmuch periodicity it has. A complete generality, P\ntimes e is equal to e.",
    "start": "1035839",
    "end": "1046010"
  },
  {
    "text": "So lambda is always an\neigenvalue of a stochastic matrix, and e is always\na right eigenvector.",
    "start": "1046010",
    "end": "1055350"
  },
  {
    "text": "Well, from what we've just said,\nthat means there has to be a left eigenvector also.",
    "start": "1055350",
    "end": "1061090"
  },
  {
    "text": "So there has to be some\npi such that pi times P is equal to pi.",
    "start": "1061090",
    "end": "1067380"
  },
  {
    "text": "So suddenly, we find there's\nalso a left eigenvector. What we haven't shown yet is\nthat that pi that satisfies",
    "start": "1067380",
    "end": "1076470"
  },
  {
    "text": "this equation is a probability\nvector. Namely, we haven't shown that\nall the components of pi are",
    "start": "1076470",
    "end": "1083070"
  },
  {
    "text": "greater than or equal to 0. We still have to do that. And in fact, that's not\ncompletely trivial.",
    "start": "1083070",
    "end": "1090340"
  },
  {
    "text": "If we can find such a vector\nthat is a probability vector, the compound in sum to 1 and\nthey're not negative, then",
    "start": "1090340",
    "end": "1097960"
  },
  {
    "text": "this is the equation for\na steady state vector. So what we don't know yet\nis whether a steady",
    "start": "1097960",
    "end": "1105590"
  },
  {
    "text": "state vector exists. We do know that a left\neigenvector exists.",
    "start": "1105590",
    "end": "1111400"
  },
  {
    "text": "We're going to show later\nthat there is a steady state vector pi. In other words, a non-negative\nvector which sums to 1 for all",
    "start": "1111400",
    "end": "1120400"
  },
  {
    "text": "finite-state Markov chains. In other words, no matter how\nmessy it is, just like e, the",
    "start": "1120400",
    "end": "1126779"
  },
  {
    "text": "column vector of all 1s is\nalways a right eigenvector of eigenvalue 1.",
    "start": "1126780",
    "end": "1132480"
  },
  {
    "text": "There is always a non-negative\nvector pi whose components sum to 1, which is a left\neigenvector with eigenvalue 1.",
    "start": "1132480",
    "end": "1143590"
  },
  {
    "text": "So these two relationships\nhold everywhere. ",
    "start": "1143590",
    "end": "1150780"
  },
  {
    "text": "Incidentally, the notes\nat one point claim to have shown this.",
    "start": "1150780",
    "end": "1157030"
  },
  {
    "text": "And the notes really\ndon't show it. I'm going to show\nit to you today. I'm sorry for that.",
    "start": "1157030",
    "end": "1162400"
  },
  {
    "text": "It's something I've known for so\nlong that I find it hard to say is this true or not.",
    "start": "1162400",
    "end": "1169420"
  },
  {
    "text": "Of course it's true. But it does have to be shown,\nand I will show it",
    "start": "1169420",
    "end": "1174540"
  },
  {
    "text": "to you later on.  Chapter three of the notes is\nlargely rewritten this year.",
    "start": "1174540",
    "end": "1184410"
  },
  {
    "text": "And it has a few more typos\nin it than most of the other chapters.",
    "start": "1184410",
    "end": "1189870"
  },
  {
    "text": "And a few of the typos\nare fairly important. I'll try to point some\nof them out as we go.",
    "start": "1189870",
    "end": "1195518"
  },
  {
    "text": "But I'm sure I haven't\ncaught them all yet. Now, what is the determinant\nof an M by M matrix?",
    "start": "1195518",
    "end": "1203990"
  },
  {
    "text": "It's this very simple-looking\nbut rather messy formula, which says the determinant of\na square matrix A is the sum",
    "start": "1203990",
    "end": "1213560"
  },
  {
    "text": "over all partitions--  and then there's a plus\nminus here, which",
    "start": "1213560",
    "end": "1219000"
  },
  {
    "text": "I'll talk about later-- of the product from i equals\n1 to M. M is the number of",
    "start": "1219000",
    "end": "1224779"
  },
  {
    "text": "states of A sub i.",
    "start": "1224780",
    "end": "1230270"
  },
  {
    "text": "This is the component\nof the ij position. And we're taking A sub i.",
    "start": "1230270",
    "end": "1236299"
  },
  {
    "text": "And then the partition that\nwe're dealing with, mu sub i. So what we're doing is taking\na matrix with all sorts of",
    "start": "1236300",
    "end": "1246520"
  },
  {
    "text": "terms in it-- A11 up to A1j on to Aj1\nup to A sub jj.",
    "start": "1246520",
    "end": "1263600"
  },
  {
    "text": "And these partitions we're\ntalking about are ways of selecting one element from each\nrow and one element from",
    "start": "1263600",
    "end": "1271900"
  },
  {
    "text": "each column. Namely, that first sum there is\ntalking about one element",
    "start": "1271900",
    "end": "1280240"
  },
  {
    "text": "from each row. And then when we're talking\nabout a permutation here, we're doing something like, for\nthis row, we're looking",
    "start": "1280240",
    "end": "1289700"
  },
  {
    "text": "at, say, this element. For this row, we might be\nlooking at this element. For this row, we might be\nlooking at this element, and",
    "start": "1289700",
    "end": "1297500"
  },
  {
    "text": "so forth down, until finally,\nwe're looking at some element down here. Now, we've picked out every\ncolumn and every row in doing",
    "start": "1297500",
    "end": "1305090"
  },
  {
    "text": "this, but we only have one\nelement in each row and one element in each column.",
    "start": "1305090",
    "end": "1311399"
  },
  {
    "text": "If you've studied linear algebra\nand you're at all interested in computation, the\nfirst thing that everybody",
    "start": "1311400",
    "end": "1318190"
  },
  {
    "text": "tells you is that this is a\ngod-awful way to ever compute a determinant, because the\nnumber of permutations grows",
    "start": "1318190",
    "end": "1327760"
  },
  {
    "text": "very, very fast with the\nsize of the matrix. And therefore you don't\nwant to use this formula very often.",
    "start": "1327760",
    "end": "1334230"
  },
  {
    "text": "It's a very useful formula\nconceptually, though, because if we look at the determinant\nof p minus lambda i, if we",
    "start": "1334230",
    "end": "1343620"
  },
  {
    "text": "want to ask the question, how\nmany eigenvalues does this transition matrix have?",
    "start": "1343620",
    "end": "1350140"
  },
  {
    "text": "well, the number of eigenvalues\nit has is the number of values of lambda such\nthat the determinant of p",
    "start": "1350140",
    "end": "1356940"
  },
  {
    "text": "minus lambda i is 0.",
    "start": "1356940",
    "end": "1362269"
  },
  {
    "text": "Now, how many such\nvalues are there? Well, you look the matrix for\nthat, and you get A11 minus",
    "start": "1362270",
    "end": "1376299"
  },
  {
    "text": "lambda A12 and A22 minus lambda\nAjj minus lambda.",
    "start": "1376300",
    "end": "1393460"
  },
  {
    "text": "And none of the other elements\nhave lambda in it. So when you're looking at this\nformula for finding the",
    "start": "1393460",
    "end": "1400450"
  },
  {
    "text": "determinant, one of the\npartitions is this partition, which is a polynomial of\ndegree j in lambda.",
    "start": "1400450",
    "end": "1410520"
  },
  {
    "text": "All of the others are\npolynomials of degree less than j in lambda. And therefore this whole\nbloody mess here is a",
    "start": "1410520",
    "end": "1418750"
  },
  {
    "text": "polynomial of degree\nj and lambda.",
    "start": "1418750",
    "end": "1424140"
  },
  {
    "text": "So the equation, determinant of\np minus lambda i, which is a polynomial of degree j\nin lambda, equals 0.",
    "start": "1424140",
    "end": "1433070"
  },
  {
    "text": "How many roots does it have? Well, the fundamental theorem\nof algebra says that a",
    "start": "1433070",
    "end": "1438120"
  },
  {
    "text": "polynomial of degree j,\nof complex numbers--",
    "start": "1438120",
    "end": "1444510"
  },
  {
    "text": "and real is a special\ncase of complex-- that it has exactly j roots.",
    "start": "1444510",
    "end": "1452360"
  },
  {
    "text": "So there are exactly,\nin this case, M-- excuse me, I've been\ncalling it j",
    "start": "1452360",
    "end": "1457780"
  },
  {
    "text": "sometimes and M sometimes.  This equation here has exactly\nM roots to it.",
    "start": "1457780",
    "end": "1466809"
  },
  {
    "text": "And since it has exactly M\nroots, that's the number of eigenvalues there are.",
    "start": "1466810",
    "end": "1472370"
  },
  {
    "text": "There's one flaw in\nthat argument. And that is, some of the roots\nmight be repeated.",
    "start": "1472370",
    "end": "1480020"
  },
  {
    "text": "Say you have M roots\naltogether. Some of them appear more than\none time, so you'll have roots",
    "start": "1480020",
    "end": "1488460"
  },
  {
    "text": "of multiplicity, something\nor other. And when you add up the\nmultiplicities of each of the",
    "start": "1488460",
    "end": "1494740"
  },
  {
    "text": "distinct eigenvalues, you\nget capital M, which is the number of states.",
    "start": "1494740",
    "end": "1500700"
  },
  {
    "text": "So the number of different\neigenvalues is less than or equal to M. And the number of\ndistinct eigenvalues times the",
    "start": "1500700",
    "end": "1509940"
  },
  {
    "text": "multiplicity of each eigenvalue\nis equal to M.",
    "start": "1509940",
    "end": "1517350"
  },
  {
    "text": "That's a simple, straightforward\nfact. And it's worth remembering.",
    "start": "1517350",
    "end": "1522910"
  },
  {
    "text": "So there are M roots\nto the equation. Determinant p minus\nlambda i equals 0.",
    "start": "1522910",
    "end": "1528100"
  },
  {
    "text": "And therefore there are\nM eigenvalues of p.",
    "start": "1528100",
    "end": "1533220"
  },
  {
    "text": "And therefore you might think\nthat there are M eigenvectors. That, unfortunately, is\nnot true necessarily.",
    "start": "1533220",
    "end": "1543529"
  },
  {
    "text": "That's one of the really-- it's probably the only really\nugly thing in linear algebra.",
    "start": "1543530",
    "end": "1550380"
  },
  {
    "text": "I mean, linear algebra is\na beautiful theory.  I mean, it's like Poisson's\nstochastic processes.",
    "start": "1550380",
    "end": "1558260"
  },
  {
    "text": "Everything that can\nbe true is true. And if something isn't true,\nthere's a simple counter-example of why\nit can't be true.",
    "start": "1558260",
    "end": "1565580"
  },
  {
    "text": "This thing is just\na bloody mess. But unfortunately, if you have\nM states in a finite-state",
    "start": "1565580",
    "end": "1575570"
  },
  {
    "text": "Markov chain, you might not have\nM different eigenvectors. And that's unfortunate, but we\nwill forget about that for as",
    "start": "1575570",
    "end": "1584790"
  },
  {
    "text": "long as we can, and we'll\nfinally come back to it towards the end.",
    "start": "1584790",
    "end": "1591130"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]? ",
    "start": "1591130",
    "end": "1598600"
  },
  {
    "text": "PROFESSOR: What? AUDIENCE: Why would we care\nabout all the eigenvectors if we are only concerned with the\nones that [INAUDIBLE]?",
    "start": "1598600",
    "end": "1605789"
  },
  {
    "text": "PROFESSOR: Well, because we're\ninterested in the other ones because that tells us how fast\np to the M converges to what",
    "start": "1605790",
    "end": "1611960"
  },
  {
    "text": "it should be. I mean, all those other\neigenvalues, as we'll see, are",
    "start": "1611960",
    "end": "1618700"
  },
  {
    "text": "the error terms in p to the\nM as it approaches this",
    "start": "1618700",
    "end": "1624883"
  },
  {
    "text": "asymptotic value. And therefore we want to know\nwhat those eigenvalues are.",
    "start": "1624884",
    "end": "1630416"
  },
  {
    "text": "At least we want to\nknow what the second-biggest eigenvalue is. ",
    "start": "1630416",
    "end": "1639820"
  },
  {
    "text": "Now, let's look at just\na case of two states. Most of the things that can\nhappen will happen with two",
    "start": "1639820",
    "end": "1648020"
  },
  {
    "text": "states, except for this ugly\nthing that I told you about that can't happen\nwith two states.",
    "start": "1648020",
    "end": "1653330"
  },
  {
    "text": "And therefore two states is a\ngood thing to look at, because with two states, you can\ncalculate everything very",
    "start": "1653330",
    "end": "1658930"
  },
  {
    "text": "easily and you don't have to\nuse any linear algebra. So if we look at a Markov chain\nwith two states, P sub",
    "start": "1658930",
    "end": "1668010"
  },
  {
    "text": "ij is this set of transition\nprobabilities.",
    "start": "1668010",
    "end": "1674620"
  },
  {
    "text": "The left eigenvector equation\nis pi 1 times P11 times pi 2",
    "start": "1674620",
    "end": "1683880"
  },
  {
    "text": "times P21 is equal\nto lambda pi 1. And so this is writing out\nwhat we said before.",
    "start": "1683880",
    "end": "1692500"
  },
  {
    "text": "The vector pi times the matrix\nP is equal to lambda",
    "start": "1692500",
    "end": "1697770"
  },
  {
    "text": "times the vector pi. That covers both of\nthese equations. Since M is only 2,\nwe only have to",
    "start": "1697770",
    "end": "1703810"
  },
  {
    "text": "write things out twice. Same thing for the right\neigenvector equation.",
    "start": "1703810",
    "end": "1709250"
  },
  {
    "text": "That's this. The determinant of P minus\nlambda i, if we use this formula that we talked about\nhere, you put A11 minus",
    "start": "1709250",
    "end": "1719940"
  },
  {
    "text": "lambda, A22 minus lambda. Well, then you're done. So all you need is P11 minus\nlambda times P22 minus lambda.",
    "start": "1719940",
    "end": "1730220"
  },
  {
    "text": "That's this permutation there. And then you have an odd\npermutation, A12 times A21.",
    "start": "1730220",
    "end": "1739150"
  },
  {
    "text": "How do you know which\npermutations are even and which permutations are odd?",
    "start": "1739150",
    "end": "1744159"
  },
  {
    "text": "It's how many flips\nyou have to do. But to see that that's\nconsistent, you really have to",
    "start": "1744160",
    "end": "1749929"
  },
  {
    "text": "look at Strang or some book on\nlinear algebra, because it's not relevant here.",
    "start": "1749930",
    "end": "1755590"
  },
  {
    "text": "But anyway, that determinant\nis equal to this quantity here. That's a polynomial of\ndegree 2 in lambda.",
    "start": "1755590",
    "end": "1764850"
  },
  {
    "text": "If you solve it, you find\nout that one solution is",
    "start": "1764850",
    "end": "1770650"
  },
  {
    "text": "lambda 1 equals 1. The other solution is lambda\n2 is 1 minus P12 minus P21.",
    "start": "1770650",
    "end": "1779710"
  },
  {
    "text": "Now, there are a bunch of\ncases to look at here. If the off-diagonal transition\nprobabilities are both 0, what",
    "start": "1779710",
    "end": "1788770"
  },
  {
    "text": "does that mean? It means if you start in state\n0, you stay there. If you start in state 1,\nyou stay there forever.",
    "start": "1788770",
    "end": "1796450"
  },
  {
    "text": "If you start in state 2,\nyou stay there forever. That's a very boring Markov\nchain, but it's not very nice",
    "start": "1796450",
    "end": "1804520"
  },
  {
    "text": "for the theory. So we're going to leave that\ncase out for the time being.",
    "start": "1804520",
    "end": "1811120"
  },
  {
    "text": "But anyway, if you have that\ncase, then the chain has two recurrent classes.",
    "start": "1811120",
    "end": "1816520"
  },
  {
    "text": "Lambda equals 1, has\nmultiplicity 2. You have two eigenvalues of\nalgebraic multiplicity 2.",
    "start": "1816520",
    "end": "1827429"
  },
  {
    "text": "I mean, it's just one number,\nbut it appears twice in this determinant equation.",
    "start": "1827430",
    "end": "1832920"
  },
  {
    "text": "And it also appears twice in\nthe sense that you have two recurrent classes. And you will find that there are\ntwo linearly independent",
    "start": "1832920",
    "end": "1843930"
  },
  {
    "text": "left eigenvectors, two linearly\nindependent right eigenvectors. And how do you find those?",
    "start": "1843930",
    "end": "1850400"
  },
  {
    "text": "You use your common sense and\nyou say, well, if you start in state 1, you're always there.",
    "start": "1850400",
    "end": "1855910"
  },
  {
    "text": "If you start in state 2,\nyou're always there. Why do I even look at\nthese two states?",
    "start": "1855910",
    "end": "1861130"
  },
  {
    "text": "This is a crazy thing where\nwherever I start, I stay there and I only look at state\n1 or state 2.",
    "start": "1861130",
    "end": "1869130"
  },
  {
    "text": "It's scarcely even\na Markov chain. If P12 and P21 are both 1, what\nit means is you can never",
    "start": "1869130",
    "end": "1879630"
  },
  {
    "text": "go from state 1 to state 1. You always go from state\n1 to state 2. And you always go from\nstate 2 to state 1.",
    "start": "1879630",
    "end": "1887220"
  },
  {
    "text": "It means you have a two-state\nperiodic chain. And that's the other\ncrazy case.",
    "start": "1887220",
    "end": "1893130"
  },
  {
    "text": "The other case is not\nvery interesting. There's nothing stochastic\nabout it at all.",
    "start": "1893130",
    "end": "1898800"
  },
  {
    "text": "So the chain is periodic. And if you look at this equation\nhere, the second",
    "start": "1898800",
    "end": "1905169"
  },
  {
    "text": "eigenvalue is equal\nto minus 1. I might as well tell you that,\nin general, if you have a",
    "start": "1905170",
    "end": "1911790"
  },
  {
    "text": "periodic Markov chain, just one\nrecurrent class and it's",
    "start": "1911790",
    "end": "1917520"
  },
  {
    "text": "periodic, a period d, then the\neigenvalues turn out to be the",
    "start": "1917520",
    "end": "1924760"
  },
  {
    "text": "uniformly spaced eigenvalues\naround the unit circle. One is one of the eigenvalues.",
    "start": "1924760",
    "end": "1930920"
  },
  {
    "text": "We've already seen that. And the other d minus 1\neigenvalues are those",
    "start": "1930920",
    "end": "1935940"
  },
  {
    "text": "uniformly spaced around\nthe unit circle. So they add up to 360 degrees\nwhen you get all done with it.",
    "start": "1935940",
    "end": "1944409"
  },
  {
    "text": "So that's an easy case. Proving that is tedious.",
    "start": "1944410",
    "end": "1949780"
  },
  {
    "text": "It's done in the notes. It's not even done\nin the notes. It's done in one of\nthe exercises. And you can do it\nif you choose.",
    "start": "1949780",
    "end": "1958260"
  },
  {
    "text": " So let's look at these\neigenvector equations and the",
    "start": "1958260",
    "end": "1966539"
  },
  {
    "text": "eigenvalue equations. Incidentally, if you don't know\nwhat the eigenvalues are,",
    "start": "1966540",
    "end": "1972200"
  },
  {
    "text": "is this a linear set\nof equations? No, it's a nonlinear\nset of equations.",
    "start": "1972200",
    "end": "1979380"
  },
  {
    "text": "This is a nonlinear set\nof equations in pi 1, pi 2, and lambda.",
    "start": "1979380",
    "end": "1986930"
  },
  {
    "text": "How do you solve non-linear\nequations like that? Well, if you have much sense,\nyou first find out what lambda",
    "start": "1986930",
    "end": "1994750"
  },
  {
    "text": "is and then you solve\nlinear equations. ",
    "start": "1994750",
    "end": "2000105"
  },
  {
    "text": "And you can always do that. We've said that these solutions\nfor lambda, there",
    "start": "2000105",
    "end": "2005880"
  },
  {
    "text": "can only be M of them. And you can find\nthem by solving this polynomial equation.",
    "start": "2005880",
    "end": "2012220"
  },
  {
    "text": "Then you can solve the linear\nequation by finding the eigenvectors. There are packages to do all\nof these things, so there's",
    "start": "2012220",
    "end": "2019750"
  },
  {
    "text": "nothing you should waste\ntime on doing here. It's just knowing what the\nresults are that's important.",
    "start": "2019750",
    "end": "2029090"
  },
  {
    "text": "From now on, I'm going to assume\nthat P12 or P21 are",
    "start": "2029090",
    "end": "2035100"
  },
  {
    "text": "greater than 0. In other words, I'm going to\nassume that we don't have the periodic case and we don't have\nthe case where you have",
    "start": "2035100",
    "end": "2045010"
  },
  {
    "text": "two classes of states. In other words, I'm going to\nassume that our Markov chain",
    "start": "2045010",
    "end": "2051760"
  },
  {
    "text": "is actually ergodic. That's the assumption that\nI'm making here.",
    "start": "2051760",
    "end": "2057530"
  },
  {
    "text": "If you then solve these\nequations using lambda 1 equals 1, you'll find\nout that pi 1 is the",
    "start": "2057530",
    "end": "2067379"
  },
  {
    "text": "component sum to 1. First component is\nP21 over the sum.",
    "start": "2067380",
    "end": "2072669"
  },
  {
    "text": "Second component is\nP12 over the sum. Not very interesting.",
    "start": "2072670",
    "end": "2077950"
  },
  {
    "text": " Why is the steady state\nprobability weighted towards",
    "start": "2077950",
    "end": "2084520"
  },
  {
    "text": "the largest of these transition\nprobabilities? If P21 is bigger than P12, how\ndo you know intuitively that",
    "start": "2084520",
    "end": "2094330"
  },
  {
    "text": "you're going to be in state 1\nmore than you're in state 2? Is this intuitively\nobvious to-- yeah?",
    "start": "2094330",
    "end": "2103131"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE].  PROFESSOR: Because you make more\ntransistors from 2 to 1.",
    "start": "2103131",
    "end": "2108810"
  },
  {
    "text": "Well, actually you don't make\nmore transitions from 2 to 1. You make exactly the same number\nof transitions, but",
    "start": "2108810",
    "end": "2114890"
  },
  {
    "text": "since the probability is higher,\nit means you have to be in state 1 more\nof the time.",
    "start": "2114890",
    "end": "2120160"
  },
  {
    "text": "Good.  So these are the two.",
    "start": "2120160",
    "end": "2125520"
  },
  {
    "text": " And this is the left eigenvector\nfor the second",
    "start": "2125520",
    "end": "2135920"
  },
  {
    "text": "eigenvalue-- namely, the smaller\neigenvalue. Now, if you look at these\nequations, you'll notice that",
    "start": "2135920",
    "end": "2147000"
  },
  {
    "text": "the vector pi, the left i-th\neigenvector, multiplied by the",
    "start": "2147000",
    "end": "2154390"
  },
  {
    "text": "right j-th eigenvector, is\nalways equal to delta ij.",
    "start": "2154390",
    "end": "2159650"
  },
  {
    "text": "In other words, the left\neigenvectors are orthogonal to",
    "start": "2159650",
    "end": "2165789"
  },
  {
    "text": "the right eigenvectors. I mean, you can see this just\nby multiplying it out.",
    "start": "2165790",
    "end": "2171839"
  },
  {
    "text": "You multiply pi 1 times nu\n1, and what do you get? You get this plus this,\nwhich is 1.",
    "start": "2171840",
    "end": "2180460"
  },
  {
    "text": "Delta 11 means there's something\nwhich is 1 when i is equal j and 0 when i\nis unequal to j.",
    "start": "2180460",
    "end": "2189165"
  },
  {
    "text": "You take this and you\nmultiply it by this,",
    "start": "2189165",
    "end": "2196170"
  },
  {
    "text": "and what do you get? You get P21 times P12\nover the square. Minus P12 times P21, it's 0.",
    "start": "2196170",
    "end": "2205829"
  },
  {
    "text": "Same thing here. 1 minus 1, that vector times\nthis vector, is 0 again.",
    "start": "2205830",
    "end": "2213160"
  },
  {
    "text": "So the cross-terms are 0. The diagonal terms are 1.",
    "start": "2213160",
    "end": "2218680"
  },
  {
    "start": "2218680",
    "end": "2227150"
  },
  {
    "text": "That's the way it is.  So let's move on with this.",
    "start": "2227150",
    "end": "2233410"
  },
  {
    "text": " These right eigenvector\nequations, you can write them",
    "start": "2233410",
    "end": "2241440"
  },
  {
    "text": "in matrix form. I'm doing this slowly. I hope I'm not boring those who\nhave done a lot of linear",
    "start": "2241440",
    "end": "2247740"
  },
  {
    "text": "algebra too much. But they won't go on forever,\nand it gets us to where we",
    "start": "2247740",
    "end": "2257130"
  },
  {
    "text": "want to go. So if you take these two\nequations and you write them in matrix form, what you get\nis P times u, where u is a",
    "start": "2257130",
    "end": "2268170"
  },
  {
    "text": "matrix whose columns are\nthe vector nu 1 and",
    "start": "2268170",
    "end": "2273770"
  },
  {
    "text": "the vector nu 2. And capital lambda is the\ndiagonal matrix of the",
    "start": "2273770",
    "end": "2281090"
  },
  {
    "text": "eigenvalues. If you multiply P times the\nfirst column of u, and then",
    "start": "2281090",
    "end": "2287540"
  },
  {
    "text": "you look at the first column of\nthis matrix, what you get-- yes, that's exactly the\nright way to do it.",
    "start": "2287540",
    "end": "2293680"
  },
  {
    "text": " And if you're not doing that,\nyou're probably not",
    "start": "2293680",
    "end": "2300510"
  },
  {
    "text": "understanding it. But if you just think of\nordinary matrix vector multiplication, this\nall works out.",
    "start": "2300510",
    "end": "2309109"
  },
  {
    "text": " Because of this orthogonality\nrelationship, we see that the",
    "start": "2309110",
    "end": "2316700"
  },
  {
    "text": "matrix whose rows are the left\neigenvectors times the matrix",
    "start": "2316700",
    "end": "2332530"
  },
  {
    "text": "whose columns are the\nright eigenvectors, that's equal to i.",
    "start": "2332530",
    "end": "2339720"
  },
  {
    "text": "Namely, it's equal to\nthe identity matrix. That's what this orthogonality\nrelationship means.",
    "start": "2339720",
    "end": "2345580"
  },
  {
    "text": "This means that this matrix is\nthe inverse of this matrix.",
    "start": "2345580",
    "end": "2352730"
  },
  {
    "text": "This proves that u\nis invertible. And in fact, we've done this\njust for m equals 2.",
    "start": "2352730",
    "end": "2360250"
  },
  {
    "text": "But in fact, this proof is\ngeneral and holds for arbitrary Markov chains if the\neigenvectors span the space.",
    "start": "2360250",
    "end": "2371519"
  },
  {
    "text": "And we'll see that later. We're doing this for m equals\n2 now, so we how to proceed",
    "start": "2371520",
    "end": "2378030"
  },
  {
    "text": "when we have an arbitrary\nMarkov chain. u is invertible. u to the minus 1 has pi\n1 and pi 2 as rows.",
    "start": "2378030",
    "end": "2386790"
  },
  {
    "text": "And thus P is going\nto be equal to-- ",
    "start": "2386790",
    "end": "2392540"
  },
  {
    "text": "I guess we should-- oh, we set it up here. P times u is equal to\nu times lambda.",
    "start": "2392540",
    "end": "2399220"
  },
  {
    "text": "We've shown here that u is\ninvertible, therefore we can multiply this equation\nby u to the minus 1.",
    "start": "2399220",
    "end": "2406830"
  },
  {
    "text": "And we get the transition matrix\nP is equal to u times the diagonal matrix lambda\ntimes the matrix u",
    "start": "2406830",
    "end": "2415730"
  },
  {
    "text": "to the minus 1. What happens if we try\nto find P squared?",
    "start": "2415730",
    "end": "2421350"
  },
  {
    "text": "Well, it's u times lambda\ntimes u to the minus 1. One of the nice things about\nmatrices is you can multiply",
    "start": "2421350",
    "end": "2428470"
  },
  {
    "text": "them, if you don't\nworry about the details, almost like numbers. Times u times lambda times\nu to the minus 1.",
    "start": "2428470",
    "end": "2436540"
  },
  {
    "text": "Except you don't have\ncommutativity.",
    "start": "2436540",
    "end": "2441580"
  },
  {
    "text": "That's the only thing\nthat you don't have. But anyway, you have u times\nlambda times u to the minus 1",
    "start": "2441580",
    "end": "2447100"
  },
  {
    "text": "times u times lambda times\nu to t he minus 1. This and this turn out to be\nthe identity matrix, so you",
    "start": "2447100",
    "end": "2454840"
  },
  {
    "text": "have u times lambda times\nlambda, which is lambda squared, times u\nto the minus 1.",
    "start": "2454840",
    "end": "2460869"
  },
  {
    "text": "You still have this diagonal\nmatrix here, but the eigenvalues have all\nbeen doubled.",
    "start": "2460870",
    "end": "2466610"
  },
  {
    "text": "If you keep doing that\nrepeatedly, you find out that",
    "start": "2466610",
    "end": "2472660"
  },
  {
    "text": "P to the n-- namely, this\nlong-term transition matrix, which is the thing we're\ninterested in--",
    "start": "2472660",
    "end": "2480289"
  },
  {
    "text": "is the matrix u times this\ndiagonal matrix, lambda to the",
    "start": "2480290",
    "end": "2485860"
  },
  {
    "text": "n, times u to the minus 1. Equation 329 in the text has a\ntypo, and it should be this.",
    "start": "2485860",
    "end": "2494910"
  },
  {
    "text": "It's given as u to the minus 1\ntimes lambda to the n times u, which is not at all right.",
    "start": "2494910",
    "end": "2503029"
  },
  {
    "text": "That's probably the worst typo,\nbecause if you try to say something from that, you'll\nget very confused.",
    "start": "2503030",
    "end": "2511076"
  },
  {
    "text": "You can solve one in general if\nall the M eigenvalues are distinct as easily as\nfor M equals 2.",
    "start": "2511076",
    "end": "2517730"
  },
  {
    "text": "This is still valid\nso long as the eigenvectors span the space.",
    "start": "2517730",
    "end": "2524780"
  },
  {
    "text": "So now the thing we want to\ndo is relatively simple. This lambda to the n is\na diagonal matrix.",
    "start": "2524780",
    "end": "2534460"
  },
  {
    "text": "I can represent it as the sum\nof M different matrices.",
    "start": "2534460",
    "end": "2539900"
  },
  {
    "text": "And each of those matrices\nhas only one diagonal element, non-0.",
    "start": "2539900",
    "end": "2546590"
  },
  {
    "text": "In other words, for the case\nhere, what we're doing is taking lambda 1, 0 to the n,\n0 lambda 2 to the n, and",
    "start": "2546590",
    "end": "2560650"
  },
  {
    "text": "representing this as lambda 1 to\nthe n, 0, 0, 0, plus 0, 0,",
    "start": "2560650",
    "end": "2574089"
  },
  {
    "text": "0 lambda 2 to the n. ",
    "start": "2574090",
    "end": "2581240"
  },
  {
    "text": "So we have those trivial\nmatrices with u on the left",
    "start": "2581240",
    "end": "2587940"
  },
  {
    "text": "side and u to the minus\n1 on the right side. And we think of how to multiply\nthe matrix u, which",
    "start": "2587940",
    "end": "2597920"
  },
  {
    "text": "is a matrix whose columns are\nthe eigenvectors, times this",
    "start": "2597920",
    "end": "2603910"
  },
  {
    "text": "matrix with only one non-0\nelement, times the matrix here, whose elements are\nthe left eigenvectors.",
    "start": "2603910",
    "end": "2613619"
  },
  {
    "text": "And how do you do that? Well, if you do this for a\nwhile, and you think of what",
    "start": "2613620",
    "end": "2620200"
  },
  {
    "text": "this one element here times\na matrix whose rows are",
    "start": "2620200",
    "end": "2626720"
  },
  {
    "text": "eigenvectors does, this non-0\nterm in here picks out the",
    "start": "2626720",
    "end": "2632280"
  },
  {
    "text": "appropriate row here. And this non-0 element\npicks out the",
    "start": "2632280",
    "end": "2637310"
  },
  {
    "text": "appropriate column here. So what that gives you is p to\nthe n is equal to the sum over",
    "start": "2637310",
    "end": "2645770"
  },
  {
    "text": "the number of states in the\nMarkov chain times lambda sub i-- the i-th value to\nthe nth power--",
    "start": "2645770",
    "end": "2653780"
  },
  {
    "text": "times nu to the i times\npi to the i. pi to the i is the i-th\neigenvector of p.",
    "start": "2653780",
    "end": "2661560"
  },
  {
    "text": "nu to the i is the i-th right\neigenvector of p. They have nothing\nto do with n.",
    "start": "2661560",
    "end": "2668010"
  },
  {
    "text": "The only thing that n affects\nis this eigenvalue here.",
    "start": "2668010",
    "end": "2673370"
  },
  {
    "text": "And what this is saying is that\np to the n is just the sum of eigenvalues which are,\nif lambda is bigger than 1,",
    "start": "2673370",
    "end": "2686160"
  },
  {
    "text": "this is exploding. If lambda 1 is less than\n1, it's going to 0.",
    "start": "2686160",
    "end": "2691540"
  },
  {
    "text": "And if lambda 1 is equal to\n1, it's staying constant. If lambda 1 is complex but has\nmagnitude 1, then it's just",
    "start": "2691540",
    "end": "2700789"
  },
  {
    "text": "gradually rotating around and\nnot doing much of interest at all, but it's going away.",
    "start": "2700790",
    "end": "2706930"
  },
  {
    "text": "So that's what this\nequation means. It says that we've converted the\nproblem of finding the nth",
    "start": "2706930",
    "end": "2713730"
  },
  {
    "text": "power of p just to this problem\nof finding the nth power of these eigenvalues.",
    "start": "2713730",
    "end": "2720740"
  },
  {
    "text": "So we've made some\nreal progress. AUDIENCE: Professor, what\nis nu i right here? PROFESSOR: What? AUDIENCE: What is nu i?",
    "start": "2720740",
    "end": "2726453"
  },
  {
    "text": " PROFESSOR: nu sub i is the i-th\nof the right eigenvectors",
    "start": "2726453",
    "end": "2734680"
  },
  {
    "text": "of the matrix p. AUDIENCE: And pi i? PROFESSOR: And pi i is the\ni-th left eigenvector.",
    "start": "2734680",
    "end": "2744020"
  },
  {
    "text": "And what we've shown is that\nthese are orthogonal to each other, orthonormal.",
    "start": "2744020",
    "end": "2751554"
  },
  {
    "text": "AUDIENCE: Can you please say\nagain what happens when lambda is complex? PROFESSOR: What? AUDIENCE: When lambda is\ncomplex, what exactly happens?",
    "start": "2751554",
    "end": "2757094"
  },
  {
    "text": " PROFESSOR: Oh, if lambda i is\ncomplex and the magnitude is",
    "start": "2757094",
    "end": "2764160"
  },
  {
    "text": "less than 1, it just\ndies away. if the magnitude is bigger than\n1, it explodes, which",
    "start": "2764160",
    "end": "2769730"
  },
  {
    "text": "will be very strange. And we'll see that\ncan't happen. And if the magnitude is 1, as\nyou take powers of a complex",
    "start": "2769730",
    "end": "2777540"
  },
  {
    "text": "number of magnitude 1, I mean,\nit start out here, it goes here, then here.",
    "start": "2777540",
    "end": "2783820"
  },
  {
    "text": "I mean, it just rotates around\nin some crazy way. But it maintains its magnitude\nas being equal",
    "start": "2783820",
    "end": "2791220"
  },
  {
    "text": "to 1 all the time. ",
    "start": "2791220",
    "end": "2798290"
  },
  {
    "text": "So this is just repeating\nwhat we had before. These are the eigenvectors. ",
    "start": "2798290",
    "end": "2806350"
  },
  {
    "text": "If you calculate this very\nquickly using this and this,",
    "start": "2806350",
    "end": "2811690"
  },
  {
    "text": "and if you recognize that the\nright eigenvector, nu 2, is",
    "start": "2811690",
    "end": "2819970"
  },
  {
    "text": "the first part of it is pi sub\n2, the second part of it minus",
    "start": "2819970",
    "end": "2827060"
  },
  {
    "text": "pi sub 1, where pi is just this\nfirst eigenvector here.",
    "start": "2827060",
    "end": "2834450"
  },
  {
    "text": "So if you do this\nmultiplication, you find that nu to the 1--",
    "start": "2834450",
    "end": "2839960"
  },
  {
    "text": "oh, I thought I had all\nof these things out. This should be nu. ",
    "start": "2839960",
    "end": "2846580"
  },
  {
    "text": "The first right eigenvector\ntimes the first left",
    "start": "2846580",
    "end": "2852270"
  },
  {
    "text": "eigenvector. Oh, but this is all right,\nbecause I'm saying the first left eigenvector is a steady\nstate vector, which is the",
    "start": "2852270",
    "end": "2859080"
  },
  {
    "text": "thing we're interested in. That's pi 1, pi 2, pi 1,\npi 2, where pi 1 is",
    "start": "2859080",
    "end": "2866210"
  },
  {
    "text": "this and pi 2 is this. nu 2 times pi 2 is just this.",
    "start": "2866210",
    "end": "2873480"
  },
  {
    "text": "So when we calculate p sub n,\nwe get pi 1 plus pi 2 times",
    "start": "2873480",
    "end": "2879910"
  },
  {
    "text": "this eigenvalue to\nthe nth power. Pi 1 minus pi 1, lambda\n2 to the nth power.",
    "start": "2879910",
    "end": "2887000"
  },
  {
    "text": "pi 2 and pi 2 is what we get\nfor the main eigenvalue.",
    "start": "2887000",
    "end": "2892110"
  },
  {
    "text": "This is what we get for\nthe little eigenvalue. This little eigenvalue here is\n1 minus P12 minus P21, which",
    "start": "2892110",
    "end": "2900790"
  },
  {
    "text": "has magnitude less than 1,\nunless we either have the",
    "start": "2900790",
    "end": "2909690"
  },
  {
    "text": "situation where P12 is equal to\nP21 is equal to 0, or both of them are 1.",
    "start": "2909690",
    "end": "2915140"
  },
  {
    "text": "So these are the terms\nthat go to 0. This solution is exact. There were no approximations\nin here.",
    "start": "2915140",
    "end": "2921990"
  },
  {
    "text": "Before, when we analyzed what\nhappened to P to the n, we saw",
    "start": "2921990",
    "end": "2927140"
  },
  {
    "text": "that we converged, but\nwe didn't really see how fast we converged. Now we know how fast\nwe converge.",
    "start": "2927140",
    "end": "2933750"
  },
  {
    "text": "The rate of convergence is\nthe value of this second",
    "start": "2933750",
    "end": "2939010"
  },
  {
    "text": "eigenvalue here. And that's a pretty\ngeneral result.",
    "start": "2939010",
    "end": "2944230"
  },
  {
    "text": "You converged like the\nsecond-largest eigenvalue. And we'll see how\nthat works out.",
    "start": "2944230",
    "end": "2950900"
  },
  {
    "text": " Now, let's go on to the case\nwhere you have an arbitrary",
    "start": "2950900",
    "end": "2958810"
  },
  {
    "text": "number of states. We've almost solved that\nalready, because as we were",
    "start": "2958810",
    "end": "2963820"
  },
  {
    "text": "looking at the case with two\nstates, we were doing most of",
    "start": "2963820",
    "end": "2969870"
  },
  {
    "text": "the things in general. If you have an n state Markov\nchain, a determinant of P",
    "start": "2969870",
    "end": "2976430"
  },
  {
    "text": "minus lambda is a polynomial\nof degree M in lambda. That was what we said\na while ago.",
    "start": "2976430",
    "end": "2982790"
  },
  {
    "text": "It has M roots, eigenvalues. And here, we're going to assume\nthat those roots are",
    "start": "2982790",
    "end": "2988740"
  },
  {
    "text": "all distinct. So we don't have to worry\nabout what happens with repeated roots.",
    "start": "2988740",
    "end": "2994320"
  },
  {
    "text": "Each eigenvalue lambda sub i--\nthere are M of them now-- has a right eigenvector,\nnu sub i, and a left",
    "start": "2994320",
    "end": "3003160"
  },
  {
    "text": "eigenvector, pi sub i. And we have seen that--",
    "start": "3003160",
    "end": "3010030"
  },
  {
    "text": "well, we haven't seen it yet. We're going to show\nit in a second. pi super i times nu super\nj is equal to j for each",
    "start": "3010030",
    "end": "3018059"
  },
  {
    "text": "ij unequal to i. If you scale either this or\nthat, when you saw this",
    "start": "3018060",
    "end": "3024660"
  },
  {
    "text": "eigenvector equation, you have\na pi on both sides or a nu on",
    "start": "3024660",
    "end": "3030160"
  },
  {
    "text": "both sides, and you have a scale\nfactor which can't be determined from the eigenvector\nequation.",
    "start": "3030160",
    "end": "3037040"
  },
  {
    "text": "So you have to choose that\nscaling factor somehow. If we choose the scaling factor\nappropriately, we get",
    "start": "3037040",
    "end": "3045070"
  },
  {
    "text": "pi, the i-th left eigenvector,\ntimes the i-th right",
    "start": "3045070",
    "end": "3051810"
  },
  {
    "text": "eigenvector. This is just a number now. It's that times that. We can scale things, so\nthat's equal to 1.",
    "start": "3051810",
    "end": "3060810"
  },
  {
    "text": "Then as before, let u be the\nmatrix with columns nu 1 to nu",
    "start": "3060810",
    "end": "3065930"
  },
  {
    "text": "M, and let v have the rows, pi\n1 to pi M. Because of this",
    "start": "3065930",
    "end": "3072089"
  },
  {
    "text": "orthogonality relationship we've\nset up, v times u is equal to i.",
    "start": "3072090",
    "end": "3077530"
  },
  {
    "text": "So again, the left eigenvector\nrows forms a matrix which is",
    "start": "3077530",
    "end": "3086310"
  },
  {
    "text": "the inverse of the right\neigenvector columns. So that says v is equal\nto u to the minus 1.",
    "start": "3086310",
    "end": "3095400"
  },
  {
    "text": "Thus the eigenvector is nu, the\nfirst right eigenvector up",
    "start": "3095400",
    "end": "3101039"
  },
  {
    "text": "to the nth right eigenvector,\nthese are linearly independent.",
    "start": "3101040",
    "end": "3106100"
  },
  {
    "text": "And they span M space.  That's a very peculiar\nthing we've done.",
    "start": "3106100",
    "end": "3113480"
  },
  {
    "text": "We've said we have all these\nM right eigenvectors. We don't know anything about\nthem, but what we do know is",
    "start": "3113480",
    "end": "3122690"
  },
  {
    "text": "we also have M left\neigenvectors.",
    "start": "3122690",
    "end": "3128030"
  },
  {
    "text": "And the left eigenvectors, as\nwe're going to show in just a",
    "start": "3128030",
    "end": "3133070"
  },
  {
    "text": "second, are orthogonal to\nthe right eigenvectors. And therefore, when we look at\nthese two matrices, we can",
    "start": "3133070",
    "end": "3140500"
  },
  {
    "text": "multiply them and get\nthe identity matrix. And that means that the right\neigenvectors have to be--",
    "start": "3140500",
    "end": "3149370"
  },
  {
    "text": "when we look at the matrix of\nthe right eigenvectors, is non-singular. ",
    "start": "3149370",
    "end": "3154920"
  },
  {
    "text": "Very, very peculiar argument. I mean, we find out that those\nright eigenvectors span the",
    "start": "3154920",
    "end": "3161350"
  },
  {
    "text": "space, not by looking at the\nright eigenvectors, but by looking at how they relate\nto the left eigenvectors.",
    "start": "3161350",
    "end": "3168220"
  },
  {
    "text": "But anyway, that's perfectly\nall right. And so long as we can show\nthat we can satisfy this",
    "start": "3168220",
    "end": "3174890"
  },
  {
    "text": "orthogonality condition, then in\nfact all this works out. v",
    "start": "3174890",
    "end": "3181329"
  },
  {
    "text": "is equal to u to the minus 1. These eigenvectors are linearly\nindependent and they span M space.",
    "start": "3181330",
    "end": "3187380"
  },
  {
    "text": "Same here. ",
    "start": "3187380",
    "end": "3192980"
  },
  {
    "text": "And putting these equations\ntogether, P times u equals u times lambda. This is exactly what\nwe did before.",
    "start": "3192980",
    "end": "3199960"
  },
  {
    "text": "Post-multiplying by u to the\nminus 1, we get P equals u times lambda times\nu to the minus 1.",
    "start": "3199960",
    "end": "3207210"
  },
  {
    "text": "P to the n is then u times\nlambda to the n times u to the minus 1.",
    "start": "3207210",
    "end": "3212230"
  },
  {
    "text": "All this stuff about convergence\nis all revolving down to simply the question\nof what happens to these",
    "start": "3212230",
    "end": "3219010"
  },
  {
    "text": "eigenvalues. I mean, there's a mess first,\nfinding out what all these right eigenvectors are and\nwhat all these left",
    "start": "3219010",
    "end": "3227579"
  },
  {
    "text": "eigenvectors are. But once you do that, P to the\nn is just looking at this",
    "start": "3227580",
    "end": "3234869"
  },
  {
    "text": "quantity, breaking up\nlambda to the n the way we did before. P to the n is just\nthis sum here.",
    "start": "3234870",
    "end": "3244549"
  },
  {
    "text": "Now, each row of P sums to 1, so\ne is a right eigenvector of eigenvalue 1.",
    "start": "3244550",
    "end": "3250960"
  },
  {
    "text": "So we have a theorem that says\nthe left eigenvector pi of eigenvalue 1 is a steady state\nvector if it's normalized to",
    "start": "3250960",
    "end": "3260060"
  },
  {
    "text": "pi times e equals 1. ",
    "start": "3260060",
    "end": "3266050"
  },
  {
    "text": "So we almost did that before,\nbut now we want to be a little more careful about it.",
    "start": "3266050",
    "end": "3272010"
  },
  {
    "start": "3272010",
    "end": "3278768"
  },
  {
    "text": "Oh, excuse me. The theorem is that the left\neigenvector pi is a steady",
    "start": "3278768",
    "end": "3285640"
  },
  {
    "text": "state vector if it's normalized\nin this way. In other words, we know that\nthere is a left eigenvector",
    "start": "3285640",
    "end": "3293250"
  },
  {
    "text": "pi, which has eigenvalue 1,\nbecause there's a right eigenvector.",
    "start": "3293250",
    "end": "3298260"
  },
  {
    "text": "If there's a right eigenvector,\nthere has to be a left eigenvector. What we don't know is\nthat pi actually has",
    "start": "3298260",
    "end": "3306190"
  },
  {
    "text": "non-negative terms. So that's the thing\nwe want to show.",
    "start": "3306190",
    "end": "3311320"
  },
  {
    "text": "The proof is, there must be\na left eigenvector pi for eigenvalue 1.",
    "start": "3311320",
    "end": "3316960"
  },
  {
    "text": "We already know that. For every j, Pi sub j is equal\nto the sum over k times pi sub",
    "start": "3316960",
    "end": "3325275"
  },
  {
    "text": "k times p sub kj. We don't know whether these\nare complex or real. We don't know whether they're\npositive or negative, if",
    "start": "3325275",
    "end": "3332140"
  },
  {
    "text": "they're real. But we do know that since they\nsatisfy this eigenvector",
    "start": "3332140",
    "end": "3337960"
  },
  {
    "text": "equation, they satisfy\nthis equation. If I take the magnitudes\nof all of these",
    "start": "3337960",
    "end": "3343670"
  },
  {
    "text": "things, what do I get? The magnitude on this side\nis pi sub j magnitude.",
    "start": "3343670",
    "end": "3351440"
  },
  {
    "text": "This is less than or equal to\nthe sum of the magnitudes of these terms.",
    "start": "3351440",
    "end": "3356720"
  },
  {
    "text": "If you take two complex numbers\nand you add them up,",
    "start": "3356720",
    "end": "3363690"
  },
  {
    "text": "you get something which, in\nmagnitude, is less than or equal to the sum of\nthe magnitudes.",
    "start": "3363690",
    "end": "3370120"
  },
  {
    "text": " It might sound strange,\nbut if you look",
    "start": "3370120",
    "end": "3376030"
  },
  {
    "text": "in the complex plane-- imaginary, real--",
    "start": "3376030",
    "end": "3383480"
  },
  {
    "text": "and you look at one complex\nnumber, and you add it to another complex number, this\ndistance here is less than or",
    "start": "3383480",
    "end": "3393380"
  },
  {
    "text": "equal to this magnitude\nplus this magnitude. That's all that equation\nis saying.",
    "start": "3393380",
    "end": "3398940"
  },
  {
    "text": "And this is equal to this\ndistance plus this distance if",
    "start": "3398940",
    "end": "3404160"
  },
  {
    "text": "and only if each of these\ncomponents of the eigenvector",
    "start": "3404160",
    "end": "3411079"
  },
  {
    "text": "that we're talking about, if and\nonly if those components are all heading off in\nthe same direction",
    "start": "3411080",
    "end": "3417630"
  },
  {
    "text": "in the complex plane. Now what do we do? Well, you look at this for a\nwhile and you say, OK, what",
    "start": "3417630",
    "end": "3425950"
  },
  {
    "text": "happens if I sum this\ninequality over j?",
    "start": "3425950",
    "end": "3431031"
  },
  {
    "text": "Well, if I sum this\nover j, I get one. And therefore when I sum both\nsides over j, the sum over j",
    "start": "3431031",
    "end": "3448410"
  },
  {
    "text": "of the magnitudes of these\neigenvector components is less than or equal to the sum over\nk of the magnitude.",
    "start": "3448410",
    "end": "3456570"
  },
  {
    "text": "This is the same as this. This j is just a dummy\nindex of summation.",
    "start": "3456570",
    "end": "3462220"
  },
  {
    "text": "This is a dummy index\nof summation. Obviously, this is less\nthan or equal to this.",
    "start": "3462220",
    "end": "3467810"
  },
  {
    "text": "But what's interesting here is\nthat this is equal to this. And the only way this can be\nequal to this is if every one",
    "start": "3467810",
    "end": "3476290"
  },
  {
    "text": "of these things are satisfied\nwith equality. If any one of these are\nsatisfied with inequality,",
    "start": "3476290",
    "end": "3483720"
  },
  {
    "text": "then when you add them all up,\nthis will be satisfied with inequality also, which\nis impossible.",
    "start": "3483720",
    "end": "3490119"
  },
  {
    "text": "So all of these are satisfied\nwith equality, which says that the magnitude of pi sub j, the\nvector whose elements are the",
    "start": "3490120",
    "end": "3505060"
  },
  {
    "text": "magnitudes of this thing we\nstarted with, in fact form a",
    "start": "3505060",
    "end": "3511010"
  },
  {
    "text": "steady state vector if we\nnormalize them to 1. It says these magnitudes\nsatisfy the",
    "start": "3511010",
    "end": "3518599"
  },
  {
    "text": "steady state equation. These magnitudes are real\nand they're positive.",
    "start": "3518600",
    "end": "3525010"
  },
  {
    "text": "So when we normalize them to\nsum to 1, we have a steady state vector.",
    "start": "3525010",
    "end": "3530940"
  },
  {
    "text": "And therefore the left\neigenvector pi of eigenvalue 1 is a steady state vector if it's\nnormalized to pi times e",
    "start": "3530940",
    "end": "3537790"
  },
  {
    "text": "equals 1, which is the way we\nwant to normalize them.",
    "start": "3537790",
    "end": "3543120"
  },
  {
    "text": "So there always is a steady\nstate vector for every finite-state Markov chain.",
    "start": "3543120",
    "end": "3548839"
  },
  {
    "text": " So this is a non-negative vector\nsatisfying a steady",
    "start": "3548840",
    "end": "3555579"
  },
  {
    "text": "state vector equation. And normalizing it, we have\na steady state vector. So we've demonstrated the\nexistence of a left",
    "start": "3555580",
    "end": "3564299"
  },
  {
    "text": "eigenvector which is a\nsteady state vector. Another theorem is that every\neigenvalue satisfies lambda,",
    "start": "3564300",
    "end": "3574180"
  },
  {
    "text": "magnitude of the eigenvalue is\nless than or equal to 1. This, again, is sort of obvious,\nbecause if you have",
    "start": "3574180",
    "end": "3581370"
  },
  {
    "text": "an eigenvalue which is bigger\nthan 1 and you start taking powers of it, it starts marching\noff to infinity.",
    "start": "3581370",
    "end": "3589020"
  },
  {
    "text": "Now, you might say, maybe\nsomething else is balancing that. But since you only have a finite\nnumber of these things,",
    "start": "3589020",
    "end": "3595760"
  },
  {
    "text": "that sounds pretty weird. And in fact, it is. So the proof of this is, we want\nto assume that pi super l",
    "start": "3595760",
    "end": "3609140"
  },
  {
    "text": "is the l-th of these\neigenvectors of P. Its",
    "start": "3609140",
    "end": "3614984"
  },
  {
    "text": "eigenvalue is lambda sub l. It also is a left eigenvector of\nP to the n with eigenvalue",
    "start": "3614985",
    "end": "3625170"
  },
  {
    "text": "lambda to the n. That's what we've\nshown before. I mean, you can multiply this\nmatrix P, and all you're doing",
    "start": "3625170",
    "end": "3633070"
  },
  {
    "text": "is just taking powers\nof the eigenvalue. So if we start out with lambda\nto the n, let's forget about",
    "start": "3633070",
    "end": "3643160"
  },
  {
    "text": "the l's, because we're just\nlooking at a fixed l now. Lambda to the nth power times\nthe j-th component of pi is",
    "start": "3643160",
    "end": "3654870"
  },
  {
    "text": "equal to the sum over i of the\ni-th component of pi times Pij",
    "start": "3654870",
    "end": "3664900"
  },
  {
    "text": "to the n, for all j. ",
    "start": "3664900",
    "end": "3671079"
  },
  {
    "text": "Now I take the magnitude of\neverything is before. The magnitude of this is, again,\nless than or equal to",
    "start": "3671080",
    "end": "3677510"
  },
  {
    "text": "the magnitude of this. I want to let beta be the\nlargest of these quantities.",
    "start": "3677510",
    "end": "3685509"
  },
  {
    "text": "And when I put that maximizing\nj in here, lambda to the l",
    "start": "3685510",
    "end": "3692240"
  },
  {
    "text": "times beta is less than or equal\nto the sum over i of--",
    "start": "3692240",
    "end": "3700550"
  },
  {
    "text": "I can upper-bound\nthese by beta. So I wind up with lambda to the\nl times beta is less than",
    "start": "3700550",
    "end": "3707340"
  },
  {
    "text": "or equal to the sum over i of\nbeta times Pij to the n. I don't know what these powers\nare, but they're certainly",
    "start": "3707340",
    "end": "3714680"
  },
  {
    "text": "less than or equal to 1. So lambda sub l is less\nthan or equal to n.",
    "start": "3714680",
    "end": "3723920"
  },
  {
    "text": "That's what this said. When you take this magnitude of\nthe l-th eigenvalue, it's",
    "start": "3723920",
    "end": "3734680"
  },
  {
    "text": "less than or equal\nto this number n. Now, if this number were larger\nthan 1, if it was 1",
    "start": "3734680",
    "end": "3742310"
  },
  {
    "text": "plus 10 to the minus sixth,\nand you multiplied it by a large enough number n, that\nthis would grow to be",
    "start": "3742310",
    "end": "3751410"
  },
  {
    "text": "arbitrarily large. It can't grow to be arbitrarily\nlarge, therefore",
    "start": "3751410",
    "end": "3756880"
  },
  {
    "text": "the magnitude of lambda\nsub l has to be less than or equal to 1. Tedious proof, but\nunfortunately, the notes just",
    "start": "3756880",
    "end": "3768980"
  },
  {
    "text": "assume this.  Maybe I had some good, simple\nreason for it before.",
    "start": "3768980",
    "end": "3776609"
  },
  {
    "text": "I don't have any now, so I have\nto go through a proof. Anyway, these two theorems, if\nyou look at them, are valid",
    "start": "3776610",
    "end": "3784440"
  },
  {
    "text": "for all finite-state\nMarkov chains. There was no place that we\nused the fact that we had",
    "start": "3784440",
    "end": "3792190"
  },
  {
    "text": "anything with distinct\neigenvalues or anything. But now when we had distinct\neigenvalues, we have the nth",
    "start": "3792190",
    "end": "3800840"
  },
  {
    "text": "power of P is the sum here again\nover right eigenvectors",
    "start": "3800840",
    "end": "3808500"
  },
  {
    "text": "times left eigenvectors. When you take a right\neigenvector, which is a column",
    "start": "3808500",
    "end": "3815340"
  },
  {
    "text": "vector, times a left\neigenvector, which is a row vector, you get an\nM by M matrix.",
    "start": "3815340",
    "end": "3824080"
  },
  {
    "text": "I don't know what that matrix\nis, but it's a matrix. It's a fixed matrix\nindependent of n.",
    "start": "3824080",
    "end": "3830980"
  },
  {
    "text": "And the only thing that's\nvarying with n is these eigenvalues.",
    "start": "3830980",
    "end": "3836000"
  },
  {
    "text": "These quantities are less\nthan or equal to 1. So if the chain is an ergodic\nunit chain, we've already seen",
    "start": "3836000",
    "end": "3843270"
  },
  {
    "text": "that one eigenvalue is 1, and\nthe rest of the eigenvalues are strictly less than\n1 in magnitude.",
    "start": "3843270",
    "end": "3849260"
  },
  {
    "text": "We saw that by showing that for\nan ergodic unit chain, P to the n converged.",
    "start": "3849260",
    "end": "3856060"
  },
  {
    "text": "So the rate at which P to the\nn approaches e times pi is",
    "start": "3856060",
    "end": "3861280"
  },
  {
    "text": "going to be determined\nby the second-largest eigenvalue in here.",
    "start": "3861280",
    "end": "3867710"
  },
  {
    "text": "And that second-largest\neigenvalue is going to be less than 1, strictly less than 1.",
    "start": "3867710",
    "end": "3873880"
  },
  {
    "text": "We don't know what it is. Before, we knew this convergence\nhere for an",
    "start": "3873880",
    "end": "3879040"
  },
  {
    "text": "ergodic unit chain\nis exponential. Now we know that it's\nexponential and we know",
    "start": "3879040",
    "end": "3885380"
  },
  {
    "text": "exactly how fast it goes,\nbecause the speed of convergence is just the\nsecond-largest eigenvalue.",
    "start": "3885380",
    "end": "3892480"
  },
  {
    "text": "If you want to know how fast P\nto the n approaches e times",
    "start": "3892480",
    "end": "3898530"
  },
  {
    "text": "the steady state vector pi,\nall you have to do is find that second-largest eigenvalue,\nand that tells you",
    "start": "3898530",
    "end": "3905220"
  },
  {
    "text": "how fast the convergence is,\nexcept for calculating these things, which are just fixed.",
    "start": "3905220",
    "end": "3911015"
  },
  {
    "text": " If P is a periodic unit chain\nwith period d, then if you",
    "start": "3911015",
    "end": "3919200"
  },
  {
    "text": "read the notes-- you should read the notes-- there are d eigenvalues\nequally spaced",
    "start": "3919200",
    "end": "3924420"
  },
  {
    "text": "around the unit circle. P to the n doesn't converge. The only thing you can say here\nis, what happens if you",
    "start": "3924420",
    "end": "3933040"
  },
  {
    "text": "look at P to the d-th power? And you can imagine what happens\nif you look at P to",
    "start": "3933040",
    "end": "3939890"
  },
  {
    "text": "the d-th power without\ndoing any analysis. I mean, we know that what\nhappens in a periodic chain is",
    "start": "3939890",
    "end": "3949290"
  },
  {
    "text": "that you rotate from one set\nof states to another set of states to another set of states\nto another set of",
    "start": "3949290",
    "end": "3956220"
  },
  {
    "text": "states, and then back\nto the set of states you started with. And you keep rotating around. Now, there are d sets of states\ngoing around here.",
    "start": "3956220",
    "end": "3965520"
  },
  {
    "text": "What happens if I\ntake P to the d? P to the d is looking at\nthe d-step transitions.",
    "start": "3965520",
    "end": "3972320"
  },
  {
    "text": "So it's looking at, if you start\nhere, after d steps, you're back here again,\nafter d steps,",
    "start": "3972320",
    "end": "3979310"
  },
  {
    "text": "you're back here again. So the matrix, P to the d, is\nin fact the matrix of d",
    "start": "3979310",
    "end": "3991700"
  },
  {
    "text": "ergodic subclasses. ",
    "start": "3991700",
    "end": "3997940"
  },
  {
    "text": "And for each one of them,\nwhatever subclass you start in, you stay in that\nsubclass forever.",
    "start": "3997940",
    "end": "4004200"
  },
  {
    "text": "So the analysis of a periodic\nunit chain, really the classy way to do it is to look\nat P to the d and see",
    "start": "4004200",
    "end": "4012730"
  },
  {
    "text": "what happens there. And you see that you get\nconvergence within each",
    "start": "4012730",
    "end": "4018800"
  },
  {
    "text": "subclass, but you just keep\nrotating among subclasses. So there's nothing very\nfancy going on there.",
    "start": "4018800",
    "end": "4026060"
  },
  {
    "text": "You just rotate from one\nsubclass to another. And that's the way it is.",
    "start": "4026060",
    "end": "4032350"
  },
  {
    "text": "And P to the n doesn't\nconverge. But P to the d times\nn does converge.",
    "start": "4032350",
    "end": "4038494"
  },
  {
    "text": " Now, let's look at the next-most\ncomplicated state.",
    "start": "4038495",
    "end": "4050049"
  },
  {
    "text": "Suppose we have M states and\nwe have M independent eigenvectors.",
    "start": "4050050",
    "end": "4055180"
  },
  {
    "text": "OK, remember I told you that\nthere was a very ugly thing in linear algebra that said, when\nyou had an eigenvalue of",
    "start": "4055180",
    "end": "4063140"
  },
  {
    "text": "multiplicity k, you might not\nhave k linearly independent",
    "start": "4063140",
    "end": "4070589"
  },
  {
    "text": "eigenvectors. You might have a smaller\nnumber of them. We'll look at an example\nof that later. But here, I'm saying, let's\nforget about that case,",
    "start": "4070590",
    "end": "4078730"
  },
  {
    "text": "because it's ugly. Let's assume that whatever\nmultiplicity each of these",
    "start": "4078730",
    "end": "4084640"
  },
  {
    "text": "eigenvalues has, if you have\nan eigenvalue with multiplicity k, then you have\nk linearly independent right",
    "start": "4084640",
    "end": "4095010"
  },
  {
    "text": "eigenvectors and k linearly\nindependent left eigenvectors to correspond to that.",
    "start": "4095010",
    "end": "4100960"
  },
  {
    "text": "And then when you add up all of\nthe eigenvectors, you have",
    "start": "4100960",
    "end": "4106420"
  },
  {
    "text": "M linearly independent\neigenvectors. And what happens when you have M\nlinearly independent vectors",
    "start": "4106420",
    "end": "4116028"
  },
  {
    "text": "in a space of dimension M? If you have M linearly\nindependent vectors in a space",
    "start": "4116029",
    "end": "4122649"
  },
  {
    "text": "of dimension N, you expand the\nwhole space, which says that",
    "start": "4122649",
    "end": "4128880"
  },
  {
    "text": "the vector of these eigenvectors\nis in fact non-singular, which says, again,\nwe can do all of the",
    "start": "4128880",
    "end": "4136920"
  },
  {
    "text": "stuff we did before. There's a little bit of a trick\nin showing that the left eigenvectors and the right\neigenvectors can be made",
    "start": "4136920",
    "end": "4144460"
  },
  {
    "text": "orthogonal. But aside from that,\nP to the n is again",
    "start": "4144460",
    "end": "4150359"
  },
  {
    "text": "equal to the same form. And what this form says is, if\nall of the eigenvalues except",
    "start": "4150359",
    "end": "4163549"
  },
  {
    "text": "one are less than 1, then you're\nagain going to approach steady state.",
    "start": "4163550",
    "end": "4168649"
  },
  {
    "text": "What does that mean?  Suppose I have more than one\nergodic chain, more than one",
    "start": "4168649",
    "end": "4179729"
  },
  {
    "text": "ergodic class, or suppose I\nhave a periodic class or something else.",
    "start": "4179729",
    "end": "4185130"
  },
  {
    "text": "Is it possible to have one\neigenvalue equal to 1 and all the other eigenvalues\nbe smaller?",
    "start": "4185130",
    "end": "4192040"
  },
  {
    "text": "If there's one eigenvalue that's\nequal to 1, according to this formula here, eventually\nP to the n",
    "start": "4192040",
    "end": "4199739"
  },
  {
    "text": "converges to that one\nvalue equal to 1.",
    "start": "4199740",
    "end": "4205090"
  },
  {
    "text": "And right eigenvector\ncan be taken as e. Left eigenvector can be taken\nas a steady state vector pi.",
    "start": "4205090",
    "end": "4213230"
  },
  {
    "text": "And we have the case\nof convergence. Can you have convergence to all\nthe rows being the same if",
    "start": "4213230",
    "end": "4220830"
  },
  {
    "text": "you have multiple\nergodic classes? No.",
    "start": "4220830",
    "end": "4225900"
  },
  {
    "text": "If you have multiple ergodic\nclasses and you start out in one class, you stay there. You can't get out of it.",
    "start": "4225900",
    "end": "4232350"
  },
  {
    "text": "If you have a periodic class\nand you start out in that periodic class, you can't\nhave convergence there.",
    "start": "4232350",
    "end": "4239120"
  },
  {
    "text": "So in this situation here, where\nall the eigenvalues are",
    "start": "4239120",
    "end": "4247100"
  },
  {
    "text": "distinct, you can only have\none eigenvalue equal to 1. Here, when we're going to this\nmore general case, we might",
    "start": "4247100",
    "end": "4255270"
  },
  {
    "text": "have more than one eigenvalue\nequal to 1. But if in fact we only have one\neigenvalue equal to 1, and",
    "start": "4255270",
    "end": "4262960"
  },
  {
    "text": "all the others are strictly\nsmaller in magnitude, then in fact you're just talking about\nthis case of an ergodic unit",
    "start": "4262960",
    "end": "4269620"
  },
  {
    "text": "chain again. It's the only place\nyou can be. So let's look at an\nexample of this.",
    "start": "4269620",
    "end": "4279350"
  },
  {
    "text": "Suppose you have a Markov\nchain which has l ergodic sets of states.",
    "start": "4279350",
    "end": "4286610"
  },
  {
    "text": "You have one set of states. ",
    "start": "4286610",
    "end": "4300989"
  },
  {
    "text": "So we have one set of states\nover here, which will all go",
    "start": "4300990",
    "end": "4307610"
  },
  {
    "text": "back and forth to each other. Then another set of\nstates over here.",
    "start": "4307610",
    "end": "4312850"
  },
  {
    "start": "4312850",
    "end": "4318260"
  },
  {
    "text": "Let's let l equal\n2 in this case.",
    "start": "4318260",
    "end": "4323840"
  },
  {
    "text": "So what happens in\nthis situation? ",
    "start": "4323840",
    "end": "4336840"
  },
  {
    "text": "We'll have to work quickly\nbefore it gets up. ",
    "start": "4336840",
    "end": "4345400"
  },
  {
    "text": "Anybody with any sense, faced\nwith a Markov chain like this, would say if we start here,\nwe're going to stay here, if",
    "start": "4345400",
    "end": "4352800"
  },
  {
    "text": "we start here, we're\ngoing to stay here. Let's just analyze this first. And then after we're done\nanalyzing this,",
    "start": "4352800",
    "end": "4359390"
  },
  {
    "text": "we'll analyze this. And then we'll put the\nwhole thing together. And what we will find is\na transition matrix",
    "start": "4359390",
    "end": "4368180"
  },
  {
    "text": "which looks like this. ",
    "start": "4368180",
    "end": "4374539"
  },
  {
    "text": "And if you're here,\nyou stay here. If you're here, you stay here. We can find the eigenvalues\nand eigenvectors of this.",
    "start": "4374540",
    "end": "4381630"
  },
  {
    "text": "We can find the eigenvalues\nand eigenvectors of this. If you look at this crazy\nformula for finding",
    "start": "4381630",
    "end": "4388530"
  },
  {
    "text": "determinants, what you're stuck\nwith is permutations within here times permutations\nwithin here.",
    "start": "4388530",
    "end": "4396500"
  },
  {
    "text": "So the eigenvalues that you wind\nup with are products of the two eigenvalues.",
    "start": "4396500",
    "end": "4401960"
  },
  {
    "text": "Or any eigenvalue here is an\neigenvalue of the whole thing.",
    "start": "4401960",
    "end": "4409969"
  },
  {
    "text": "Any eigenvalue here is an\neigenvalue of the whole thing. And we just look at the sum of\nthe number of eigenvalues here",
    "start": "4409970",
    "end": "4416119"
  },
  {
    "text": "and the number there. So we have a very boring\ncase here. Each ergodic set has an\neigenvalue equal to 1, has a",
    "start": "4416120",
    "end": "4424750"
  },
  {
    "text": "right eigenvector equal to 1. When the steps of that state\nand 0 elsewhere.",
    "start": "4424750",
    "end": "4433090"
  },
  {
    "text": "There's also a steady state\nvector on that set of states. We've already seen that.",
    "start": "4433090",
    "end": "4438120"
  },
  {
    "text": "So P to the n converges to a\nblock diagonal matrix, where",
    "start": "4438120",
    "end": "4443940"
  },
  {
    "text": "for each ergodic set, the rows\nwithin that set are the same. So P to the n then\nis pi 1, pi 1.",
    "start": "4443940",
    "end": "4461400"
  },
  {
    "text": "And then here, we have\npi 2, pi 2, pi 2.",
    "start": "4461400",
    "end": "4467094"
  },
  {
    "text": " So that's all that\ncan happen here.",
    "start": "4467095",
    "end": "4474000"
  },
  {
    "text": "This is limit. ",
    "start": "4474000",
    "end": "4482090"
  },
  {
    "text": "So one message of this is that,\nafter you understand",
    "start": "4482090",
    "end": "4487219"
  },
  {
    "text": "ergodic unit chains, you\nunderstand almost everything. You still have to worry about\nperiodic unit chains.",
    "start": "4487220",
    "end": "4495310"
  },
  {
    "text": "But you just take a power of\nthem, and then you have ergodic sets of states.",
    "start": "4495310",
    "end": "4500400"
  },
  {
    "text": " one final thing.",
    "start": "4500400",
    "end": "4507250"
  },
  {
    "text": "Good, I have five minutes\nto talk about this. I don't want any more time to\ntalk about it, because I'll",
    "start": "4507250",
    "end": "4512480"
  },
  {
    "text": "get terribly confused if I do. And it's a topic which, if you\nwant to read more about it,",
    "start": "4512480",
    "end": "4521030"
  },
  {
    "text": "read about it in Strang. He obviously doesn't like\nthe topic either.",
    "start": "4521030",
    "end": "4527010"
  },
  {
    "text": "Nobody likes the topic. Strang at least was driven to\nsay something clear about it.",
    "start": "4527010",
    "end": "4533320"
  },
  {
    "text": "Most people don't even\nbother to say something clear about it. There's a theorem, due to, I\nguess, Jordan, because it's",
    "start": "4533320",
    "end": "4542190"
  },
  {
    "text": "called a Jordan form. And what Jordan said is, in\nthe nice cases we talked",
    "start": "4542190",
    "end": "4551210"
  },
  {
    "text": "about, you have this\ndecomposition of the",
    "start": "4551210",
    "end": "4557860"
  },
  {
    "text": "transition matrix in P into a\nmatrix here whose columns are",
    "start": "4557860",
    "end": "4564090"
  },
  {
    "text": "the right eigenvectors times\na matrix here, which is a",
    "start": "4564090",
    "end": "4569480"
  },
  {
    "text": "diagonal matrix with the\neigenvalues along it. And this, finally, is a matrix\nwhich is the inverse of this,",
    "start": "4569480",
    "end": "4579980"
  },
  {
    "text": "and, which properly normalized,\nis the left eigenvectors of P. And you can\nreplace this form by what's",
    "start": "4579980",
    "end": "4593400"
  },
  {
    "text": "called a Jordan form, where P\nis equal to some matrix u",
    "start": "4593400",
    "end": "4599040"
  },
  {
    "text": "times the Jordan form matrix\nj times the inverse of u.",
    "start": "4599040",
    "end": "4605720"
  },
  {
    "text": "Now, u is no longer the\nright eigenvectors. It can't be the right\neigenvectors, because when we",
    "start": "4605720",
    "end": "4612480"
  },
  {
    "text": "needed Jordan form, we don't\nhave enough right eigenvectors to span the space.",
    "start": "4612480",
    "end": "4618030"
  },
  {
    "text": "So it has to be something\nelse. And like everyone else,\nwe say, I don't care",
    "start": "4618030",
    "end": "4624450"
  },
  {
    "text": "what that matrix is. Jordan proved that there is such\na matrix, and that's all",
    "start": "4624450",
    "end": "4629940"
  },
  {
    "text": "we want to know. The important thing is that this\nmatrix j in here is as",
    "start": "4629940",
    "end": "4637230"
  },
  {
    "text": "close as you can get it. It's a matrix, which along the\nmain diagonal, has all the",
    "start": "4637230",
    "end": "4645400"
  },
  {
    "text": "eigenvalues with their\nappropriate multiplicity. Namely, lambda 1 is\nan eigenvalue with",
    "start": "4645400",
    "end": "4651670"
  },
  {
    "text": "multiplicity 2. Lambda 2 is an eigenvalue\nof multiplicity 3.",
    "start": "4651670",
    "end": "4658699"
  },
  {
    "text": "And in this situation, you have\ntwo eigenvectors here, so nothing appears up there.",
    "start": "4658700",
    "end": "4666180"
  },
  {
    "text": "With this multiplicity 3\neigenvalue, there are only two",
    "start": "4666180",
    "end": "4673530"
  },
  {
    "text": "linearly independent\neigenvectors. And therefore Jordan says, why\ndon't we stick a 1 in here and",
    "start": "4673530",
    "end": "4680640"
  },
  {
    "text": "then solve everything else? And his theorem says, if you\ndo that, it in fact works.",
    "start": "4680640",
    "end": "4688770"
  },
  {
    "text": "So every time-- well, the eigenvalue is on the\nmain diagonal, the ones on the",
    "start": "4688770",
    "end": "4697850"
  },
  {
    "text": "next diagonal up, the only place\nwould be anything non-0 is on the main diagonal in this\nform, and on the next",
    "start": "4697850",
    "end": "4705850"
  },
  {
    "text": "diagonal up, where you\noccasionally have a 1. And the 1 is to replace\nthe need for deficient",
    "start": "4705850",
    "end": "4713420"
  },
  {
    "text": "eigenvectors. So every time you have a\ndeficient eigenvector, you have some 1 appearing there.",
    "start": "4713420",
    "end": "4719260"
  },
  {
    "text": "And then there's a way\nto solve for u. And I don't have any idea what\nit is, and I don't care.",
    "start": "4719260",
    "end": "4724650"
  },
  {
    "text": "But if you get interested in it,\nI think that's wonderful. But please don't tell\nme about it.",
    "start": "4724650",
    "end": "4733075"
  },
  {
    "start": "4733075",
    "end": "4739250"
  },
  {
    "text": "Nice example of this is\nthis matrix here.",
    "start": "4739250",
    "end": "4744400"
  },
  {
    "text": "What happens if you try to\ntake the determinant of P",
    "start": "4744400",
    "end": "4750159"
  },
  {
    "text": "minus lambda i? Well, you have 1/2 minus lambda,\n1/2 minus lambda, 1",
    "start": "4750160",
    "end": "4756850"
  },
  {
    "text": "minus lambda. What are all the permutations\nhere that you can take?",
    "start": "4756850",
    "end": "4765180"
  },
  {
    "text": "There's the permutation of\nthe main diagonal itself. If I try to include that\nelement, there's nothing I can",
    "start": "4765180",
    "end": "4773380"
  },
  {
    "text": "do but have some element\ndown here. And all these elements are 0. ",
    "start": "4773380",
    "end": "4779870"
  },
  {
    "text": "So those elements don't\ncontribute to a determinant at all.",
    "start": "4779870",
    "end": "4785140"
  },
  {
    "text": "So I have one eigenvalue\nwhich is equal to 1. I have two values at\nmultiplicity 2, eigenvalue",
    "start": "4785140",
    "end": "4793020"
  },
  {
    "text": "which is 1/2. If you try to find the\neigenvector here, you find",
    "start": "4793020",
    "end": "4798070"
  },
  {
    "text": "there is only one. So in fact, this corresponds\nto a Jordan form,",
    "start": "4798070",
    "end": "4803700"
  },
  {
    "text": "where you have 1/2. ",
    "start": "4803700",
    "end": "4815300"
  },
  {
    "text": "1, and a 0, and a 1 here,\nand 0 everywhere else.",
    "start": "4815300",
    "end": "4822010"
  },
  {
    "start": "4822010",
    "end": "4829650"
  },
  {
    "text": "And now if I want to find P to\nthe n, I have u times this j",
    "start": "4829650",
    "end": "4837110"
  },
  {
    "text": "times u to the minus\n1 times u. All the u's in the middle\ncancel out, so I wind up",
    "start": "4837110",
    "end": "4842139"
  },
  {
    "text": "eventually with u times j\nto the nth power times u to the minus 1.",
    "start": "4842140",
    "end": "4848020"
  },
  {
    "text": "What is j to the nth power? What happens if I multiply this\nmatrix by itself n times?",
    "start": "4848020",
    "end": "4856260"
  },
  {
    "text": "Well, it turns out that what\nhappens is that this main diagonal here, you wind\nup with a 1/4 and",
    "start": "4856260",
    "end": "4863880"
  },
  {
    "text": "then 1/8 and so forth. This term here, it goes\ndown exponential.",
    "start": "4863880",
    "end": "4873190"
  },
  {
    "text": "Well, if you multiply this by\nitself, eventually, you can",
    "start": "4873190",
    "end": "4884270"
  },
  {
    "text": "see what's going on here more\neasily if you draw the Markov chain for it. You have state 1, state\n2, and state 3.",
    "start": "4884270",
    "end": "4894590"
  },
  {
    "text": "State 1, there's a transition\n1/2 and a transition 1/2.",
    "start": "4894590",
    "end": "4900679"
  },
  {
    "text": "State 2, there's a transition\n1/2 and a transition 1/2, And",
    "start": "4900680",
    "end": "4907530"
  },
  {
    "text": "state 3, you just stay there. So the amount of time that it\ntakes you to get to steady",
    "start": "4907530",
    "end": "4913600"
  },
  {
    "text": "state is the amount of\ntime it takes you-- you start in state 1.",
    "start": "4913600",
    "end": "4918690"
  },
  {
    "text": "You've got to make this\ntransition eventually, and then you've got to make this\ntransition eventually.",
    "start": "4918690",
    "end": "4925690"
  },
  {
    "text": "And the amount of time that it\ntakes you to do that is the sum of the amount of time it\ntakes you to go there, plus",
    "start": "4925690",
    "end": "4932170"
  },
  {
    "text": "the amount of time that\nit takes to go there. So you have two random\nvariables. One is the time to go here.",
    "start": "4932170",
    "end": "4939400"
  },
  {
    "text": "The other is the time\nto go here. Both of those are geometrically\ndecreasing",
    "start": "4939400",
    "end": "4944590"
  },
  {
    "text": "random variables. When we convolve those things\nwith each other, what we get",
    "start": "4944590",
    "end": "4950469"
  },
  {
    "text": "is an extra term n. So we get an n times\n1/2 to the n.",
    "start": "4950470",
    "end": "4960070"
  },
  {
    "text": "So the thing which is different\nin the Jordan form is, instead of having an\neigenvalue to the nth power,",
    "start": "4960070",
    "end": "4967200"
  },
  {
    "text": "you have an eigenvalue times-- if there's only a single one\nthere, there's an n there.",
    "start": "4967200",
    "end": "4974610"
  },
  {
    "text": "If there are two 1s both\ntogether, you get an n times n minus 1, and so forth.",
    "start": "4974610",
    "end": "4980230"
  },
  {
    "text": "So worst case, you've got a\npolynomial to the nth power times an eigenvalue.",
    "start": "4980230",
    "end": "4987020"
  },
  {
    "text": "For all practical purposes, this\nis still the eigenvalue going down exponentially. So for all practical purposes,\nwhat you wind up with is the",
    "start": "4987020",
    "end": "4997090"
  },
  {
    "text": "second-largest eigenvalue still\ndetermines how fast you",
    "start": "4997090",
    "end": "5002179"
  },
  {
    "text": "get convergence.  Sorry, I took eight minutes\ntalking about the Jordan form.",
    "start": "5002180",
    "end": "5009120"
  },
  {
    "text": "I wanted to take five minutes\ntalking about it. You can read more about\nit in the notes. ",
    "start": "5009120",
    "end": "5017087"
  }
]