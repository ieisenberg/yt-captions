[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13330"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13330",
    "end": "21546"
  },
  {
    "text": "PROFESSOR: All right. I want to complete\nthe discussion on volatility modeling in\nthe first part of the lecture",
    "start": "21546",
    "end": "26940"
  },
  {
    "text": "today. And last time we\naddressed the definition",
    "start": "26940",
    "end": "32950"
  },
  {
    "text": "of ARCH models, which allow\nfor time-varying volatility",
    "start": "32950",
    "end": "39420"
  },
  {
    "text": "in modeling the returns of\na financial time series. And we were looking\nlast time at modeling",
    "start": "39420",
    "end": "45039"
  },
  {
    "text": "the euro-dollar\nexchange rate returns. And we went through fitting\nARCH models to those returns,",
    "start": "45040",
    "end": "54970"
  },
  {
    "text": "and also looked at fitting the\nGARCH model to those returns. And to recap, the GARCH model\nextends upon the ARCH model",
    "start": "54970",
    "end": "67980"
  },
  {
    "text": "by adding some extra terms. So if you look at this\nexpression for the GARCH model, the first two terms for the\ntime-varying volatility sigma",
    "start": "67980",
    "end": "75950"
  },
  {
    "text": "squared t is a linear\ncombination of the past sort of residual returns squared.",
    "start": "75950",
    "end": "83750"
  },
  {
    "text": "That's the ARCH\nmodel, p of those. So the current\nvolatility depends upon what's happened in\nexcess returns over the last p",
    "start": "83750",
    "end": "90520"
  },
  {
    "text": "periods. But then we add an\nextra term, which is corresponds to q levels\nof the previous volatility.",
    "start": "90520",
    "end": "99380"
  },
  {
    "text": "And so what we're\ndoing with GARCH models is adding extra\nparameters to the ARCH,",
    "start": "99380",
    "end": "106150"
  },
  {
    "text": "but an advantage of considering\nthese extra parameters which relate basically the current\nvolatility sigma squared t",
    "start": "106150",
    "end": "113439"
  },
  {
    "text": "with the previous or lagged\nvalue sigma squared t minus j for lags\nj is that we may",
    "start": "113440",
    "end": "118940"
  },
  {
    "text": "be able to have a model\nwith many fewer parameters. So indeed, if we fit these\nmodels to the exchange rate",
    "start": "118940",
    "end": "129929"
  },
  {
    "text": "returns, what we found last\ntime-- let me go through",
    "start": "129930",
    "end": "135670"
  },
  {
    "text": "and show that--\nwas-- basically here are various fits of the\nthree cases of ARCH models.",
    "start": "135670",
    "end": "145890"
  },
  {
    "text": "ARCH orders 1, 2, and\n10, thinking we maybe need many lags to\nfit volatility.",
    "start": "145890",
    "end": "151760"
  },
  {
    "text": "And then the GARCH model 1,1,\nwhere we only have one ARCH term and one GARCH term.",
    "start": "151760",
    "end": "158890"
  },
  {
    "text": "And so basically the green line,\nor rather the blue line in this",
    "start": "158890",
    "end": "165110"
  },
  {
    "text": "graph, shows the plot of\nthe fitted GARCH(1,1) model as compared with\nthe ARCH models.",
    "start": "165110",
    "end": "171540"
  },
  {
    "text": "Now, in looking\nat this graph, one can actually see\nsome features of how",
    "start": "171540",
    "end": "178980"
  },
  {
    "text": "these models are fitting\nvolatility, which is important to understand.",
    "start": "178980",
    "end": "184090"
  },
  {
    "text": "One is that the ARCH\nmodels have a hard lower bound on the volatility.",
    "start": "184090",
    "end": "190950"
  },
  {
    "text": "Basically there's\na constant term in the volatility equation.",
    "start": "190950",
    "end": "196069"
  },
  {
    "text": "And because the additional terms\nare squared excess returns,",
    "start": "196070",
    "end": "202410"
  },
  {
    "text": "it-- basically, the volatility\ndoes have the lower bound of that intercept. So depending on what range\nyou fit the data over,",
    "start": "202410",
    "end": "209230"
  },
  {
    "text": "that lower bound is\ngoing to be defined by-- or it will be determined by\nthe data you're fitting to.",
    "start": "209230",
    "end": "217080"
  },
  {
    "text": "As you increase the ARCH\norder, you basically allow for a greater range of--\nor a lower lower bound of that.",
    "start": "217080",
    "end": "226940"
  },
  {
    "text": "And with the GARCH\nmodel you can see that this blue line\nis actually predicting",
    "start": "226940",
    "end": "232150"
  },
  {
    "text": "very different\nlevels of volatility over the entire\nrange of the series.",
    "start": "232150",
    "end": "237300"
  },
  {
    "text": "So it really is\nmuch more flexible. ",
    "start": "237300",
    "end": "242500"
  },
  {
    "text": "Now-- and in these fits, we are\nassuming Gaussian distributions",
    "start": "242500",
    "end": "247580"
  },
  {
    "text": "for the innovations\nin the return series. We'll soon pursue looking\nat alternatives to that,",
    "start": "247580",
    "end": "255000"
  },
  {
    "text": "but let me talk just\na little bit more about the GARCH model going\nback to lecture notes here.",
    "start": "255000",
    "end": "263980"
  },
  {
    "text": "So let me expand this. ",
    "start": "263980",
    "end": "271150"
  },
  {
    "text": "OK. So there's the specification. The GARCH(1,1) model. One thing to note is that this\nGARCH(1,1) model does relate",
    "start": "271150",
    "end": "283000"
  },
  {
    "text": "to an ARMA, an autoregressive\nmoving average process in the squared residuals.",
    "start": "283000",
    "end": "289410"
  },
  {
    "text": "So if we look at the top\nline, which is the equation for the GARCH(1,1) model,\nconsider eliminating sigma",
    "start": "289410",
    "end": "300200"
  },
  {
    "text": "squared t by using a new\ninnovation term, little u_t,",
    "start": "300200",
    "end": "309090"
  },
  {
    "text": "which is the difference\nbetween the squared residual and the true volatility\ngiven by the model.",
    "start": "309090",
    "end": "315060"
  },
  {
    "text": "So if you plug in the difference\nbetween our squared excess return and the\ncurrent volatility,",
    "start": "315060",
    "end": "322680"
  },
  {
    "text": "that should have mean\n0 because sigma squared",
    "start": "322680",
    "end": "327710"
  },
  {
    "text": "t, the t-th volatility squared,\nis equal to the square--",
    "start": "327710",
    "end": "333360"
  },
  {
    "text": "or is equal to the expectation\nof the squared excess residual return, epsilon_t squared.",
    "start": "333360",
    "end": "340440"
  },
  {
    "text": "So if we plug that\nin, we basically get an ARMA model for\nthe squared residuals.",
    "start": "340440",
    "end": "347330"
  },
  {
    "text": "And so epsilon_t squared\nis alpha_0 plus alpha_1 plus beta_1 the squared t minus\n1 lag plus u_t minus beta_1",
    "start": "347330",
    "end": "356400"
  },
  {
    "text": "u_t. And so what this implies is an\nARMA(1,1 model with white noise",
    "start": "356400",
    "end": "364800"
  },
  {
    "text": "that has mean 0 and variance\n2 sigma to the fourth. Just plugging things in.",
    "start": "364800",
    "end": "371150"
  },
  {
    "text": "And through our\nknowledge, understanding, of univariate time series\nmodels, ARMA models,",
    "start": "371150",
    "end": "379120"
  },
  {
    "text": "we can express this ARMA model\nfor the squared residuals as basically a polynomial\nlag of the squared residuals",
    "start": "379120",
    "end": "387660"
  },
  {
    "text": "is equal to a polynomial\nlag of the innovations.",
    "start": "387660",
    "end": "392690"
  },
  {
    "text": "And so we have this expression\nfor what the innovations are.",
    "start": "392690",
    "end": "398330"
  },
  {
    "text": "And it's required that the\nroots of this a of L operator,",
    "start": "398330",
    "end": "407020"
  },
  {
    "text": "when it thought of\non the complex plane, have roots outside\nthe unit circle, which",
    "start": "407020",
    "end": "413100"
  },
  {
    "text": "corresponds to alpha_1 plus\nbeta_1 being less than 1 in magnitude. So in order for these\nvolatility models",
    "start": "413100",
    "end": "421020"
  },
  {
    "text": "not to blow up and be\nstationary, covariance stationary, we have these\nbounds on the parameters.",
    "start": "421020",
    "end": "428480"
  },
  {
    "start": "428480",
    "end": "434790"
  },
  {
    "text": "OK, let's look at the\nunconditional volatility or long-run variance\nof the GARCH model.",
    "start": "434790",
    "end": "443259"
  },
  {
    "text": "If you take expectations on\nboth sides of the GARCH model",
    "start": "443260",
    "end": "449190"
  },
  {
    "text": "equation, you basically have\nthe expectation of sigma",
    "start": "449190",
    "end": "454270"
  },
  {
    "text": "squared sub t--\nin the long run is sigma star squared-- is alpha_0\nplus alpha_1 plus beta_1 sigma",
    "start": "454270",
    "end": "460750"
  },
  {
    "text": "star squared. So that sigma star\nsquared there is the expectation of the t\nminus 1 volatility squared",
    "start": "460750",
    "end": "477740"
  },
  {
    "text": "in the limit. And then you can\njust solve for this and see that sigma star\nsquared is equal alpha_0 over 1 minus alpha_1 minus beta_1.",
    "start": "477740",
    "end": "485050"
  },
  {
    "text": "And in terms of the stationarity\nconditions for the process,",
    "start": "485050",
    "end": "490180"
  },
  {
    "text": "if the long-run variance, in\norder for that to be finite, you need alpha_1 plus beta_1\nto be less than 1 in magnitude.",
    "start": "490180",
    "end": "500500"
  },
  {
    "text": "And if you consider the\ngeneral GARCH(p,1) model,",
    "start": "500500",
    "end": "506710"
  },
  {
    "text": "then the same argument leads to\na long-run variance being equal to alpha_0, the sort of\nintercept term in the GARCH",
    "start": "506710",
    "end": "515490"
  },
  {
    "text": "model, divided by 1 minus the\nsum of all the parameters. So these GARCH models\nlead to constraints",
    "start": "515490",
    "end": "523740"
  },
  {
    "text": "on the parameters that are\nimportant to incorporate",
    "start": "523740",
    "end": "530060"
  },
  {
    "text": "when we're doing any estimation\nof these underlying parameters. And it does complicate\nthings, actually.",
    "start": "530060",
    "end": "536650"
  },
  {
    "text": " So with maximum\nlikelihood estimation,",
    "start": "536650",
    "end": "544820"
  },
  {
    "text": "the routine for maximum\nlikelihood estimation is the same for all models. We basically want to\ndetermine the likelihood",
    "start": "544820",
    "end": "551090"
  },
  {
    "text": "function of our data given\nthe unknown parameters. And the likelihood function\nis the probability density",
    "start": "551090",
    "end": "557089"
  },
  {
    "text": "function of the data\nconditional on the parameters. So our likelihood\nfunction as a function",
    "start": "557090",
    "end": "563280"
  },
  {
    "text": "of the unknown parameters\nc, alpha, and beta is the value of the probability\ndensity, the joint density",
    "start": "563280",
    "end": "570389"
  },
  {
    "text": "of all the data conditional\non those parameters. And that joint\ndensity function can",
    "start": "570390",
    "end": "577820"
  },
  {
    "text": "be expressed as the product\nof successive conditional expectations of the time series. ",
    "start": "577820",
    "end": "585420"
  },
  {
    "text": "And those conditional densities\nare normal random variables.",
    "start": "585420",
    "end": "591600"
  },
  {
    "text": "So we can just plug\nin what we know to be the probability\ndensities of normals for the t-th\ninnovation epsilon_t.",
    "start": "591600",
    "end": "601269"
  },
  {
    "text": "And we just optimize\nthat function. Now, the challenge with\nestimating these GARCH models",
    "start": "601270",
    "end": "609050"
  },
  {
    "text": "in part is the constraints\non the underlying parameters. Those need to be enforced.",
    "start": "609050",
    "end": "616000"
  },
  {
    "text": "So we have to have that the\nalpha_i are greater than 0. Also, the beta_j\nare greater than 0.",
    "start": "616000",
    "end": "621860"
  },
  {
    "text": "And the sum of all of\nthem is between 0 and 1. ",
    "start": "621860",
    "end": "629406"
  },
  {
    "text": "Who in this class has had\ncourses in numerical analysis and done some\noptimization of functions?",
    "start": "629406",
    "end": "639680"
  },
  {
    "text": "Non-linear functions? Anybody? OK. Well, in addressing this kind\nof problem, which will come up",
    "start": "639680",
    "end": "648490"
  },
  {
    "text": "with any complex model\nthat you need to estimate, say via maximum likelihood,\nthe optimization methods",
    "start": "648490",
    "end": "656640"
  },
  {
    "text": "do really well if you're\noptimizing a convex function,",
    "start": "656640",
    "end": "662610"
  },
  {
    "text": "finding the minimum\nof a convex function. And it's always nice\nto do minimization",
    "start": "662610",
    "end": "669520"
  },
  {
    "text": "over sort of an unconstrained\nrange of underlying parameters. And one of the tricks in\nsolving these problems",
    "start": "669520",
    "end": "678630"
  },
  {
    "text": "is to transform the\nparameters to a scale where they're unlimited\nin range, basically.",
    "start": "678630",
    "end": "687480"
  },
  {
    "text": "So if you have a\npositive random variable, you might use to log of\nthat variable as the thing to be optimizing over.",
    "start": "687480",
    "end": "693860"
  },
  {
    "text": "If the variable's\nbetween 0 and 1, then you might use\nthat variable divided",
    "start": "693860",
    "end": "699050"
  },
  {
    "text": "by 1 minus that variable and\nthen take the log of that. And that's unconstrained.",
    "start": "699050",
    "end": "704740"
  },
  {
    "text": "So there are tricks for how\nyou do this optimization, which come into play. Anyway, that's the likelihood\nwith the normal distribution.",
    "start": "704740",
    "end": "713290"
  },
  {
    "text": "And we have computer\nprograms that will solve that directly\nso we don't have to worry",
    "start": "713290",
    "end": "719740"
  },
  {
    "text": "about this particular case. ",
    "start": "719740",
    "end": "725430"
  },
  {
    "text": "Once we fit this model,\nwe want to evaluate how good it is and\nthe evaluation is",
    "start": "725430",
    "end": "733260"
  },
  {
    "text": "based upon looking at the\nresiduals from the model. So what we have are\nthese innovations,",
    "start": "733260",
    "end": "739450"
  },
  {
    "text": "epsilon hat t, which should\nbe distributed with variance",
    "start": "739450",
    "end": "745250"
  },
  {
    "text": "or volatility sigma hat t. Those should be uncorrelated\nwith themselves or at least",
    "start": "745250",
    "end": "754600"
  },
  {
    "text": "to the extent that they can be. And the squared\nstandardized residuals should also be uncorrelated.",
    "start": "754600",
    "end": "760732"
  },
  {
    "text": "What we're trying to\ndo with these models is to capture the\ndependence, actually,",
    "start": "760732",
    "end": "765990"
  },
  {
    "text": "in the squared residuals, which\nis measuring the magnitude",
    "start": "765990",
    "end": "771560"
  },
  {
    "text": "of the excess returns. So those should be uncorrelated. ",
    "start": "771560",
    "end": "776690"
  },
  {
    "text": "There are various\ntest for normality. I've listed some of those that\nare the most popular here. And then there's issues of model\nselection for deciding sort",
    "start": "776690",
    "end": "786810"
  },
  {
    "text": "of which GARCH model to apply. I wanted to go\nthrough an example",
    "start": "786810",
    "end": "795399"
  },
  {
    "text": "of this analysis with the\neuro-dollar exchange rate.",
    "start": "795400",
    "end": "800630"
  },
  {
    "text": "So let me go to this\ncase study note.",
    "start": "800630",
    "end": "807480"
  },
  {
    "text": "So let's see. There's a package in R called\nrugarch for univariate GARCH",
    "start": "807480",
    "end": "817480"
  },
  {
    "text": "models, which fits various\nGARCH models with different--",
    "start": "817480",
    "end": "824420"
  },
  {
    "text": "and fits them by\nmaximum likelihood. So with this packet-- with\nthis particular library in R,",
    "start": "824420",
    "end": "832280"
  },
  {
    "text": "I fit the GARCH\nmodel after actually",
    "start": "832280",
    "end": "848320"
  },
  {
    "text": "fitting the mean process for\nthe exchange rate returns. Now, when we looked\nat things last time,",
    "start": "848320",
    "end": "854470"
  },
  {
    "text": "we basically looked at\nmodeling the squared returns. In fact, there may be an\nunderlying mean process that",
    "start": "854470",
    "end": "859560"
  },
  {
    "text": "needs to be specified as well. So in this section\nof the case note, I initially fit an\nautoregressive process",
    "start": "859560",
    "end": "869970"
  },
  {
    "text": "using the Akaike\ninformation criterion to choose the order of\nthe autoregressive process",
    "start": "869970",
    "end": "875590"
  },
  {
    "text": "and then fit a GARCH model\nwith normal GARCH terms.",
    "start": "875590",
    "end": "882200"
  },
  {
    "text": "And this is a plot of\nthe normal q-q plot",
    "start": "882200",
    "end": "887730"
  },
  {
    "text": "of the autoregressive residuals. And what you can see is\nthat the points lie along",
    "start": "887730",
    "end": "896220"
  },
  {
    "text": "a straight line sort of in\nthe middle of the range. But on the extremes, they\ndepart from that straight line.",
    "start": "896220",
    "end": "903259"
  },
  {
    "text": "This basically is a measure\nof standardized quantiles.",
    "start": "903260",
    "end": "909510"
  },
  {
    "text": "So in terms of\nstandard units away from the mean for\nthe residuals, we",
    "start": "909510",
    "end": "916120"
  },
  {
    "text": "tend to get many more high\nvalues and many more low values with the Gaussian distribution. So that really isn't\nfitting very well.",
    "start": "916120",
    "end": "924620"
  },
  {
    "text": "If we proceed and fit--\nOK, actually that plot",
    "start": "924620",
    "end": "933300"
  },
  {
    "text": "was just the simple ARCH\nmodel with no GARCH terms. And then this is the graph of\nthe q-q plot with the Gaussian",
    "start": "933300",
    "end": "946240"
  },
  {
    "text": "assumption. So here we can see that the\nresiduals from this model are suggesting that it may\ndo a pretty good job when",
    "start": "946240",
    "end": "954310"
  },
  {
    "text": "things are only a few standard\ndeviations away from the mean. Less than 2, 2.5. But when we get to\nmore extreme values,",
    "start": "954310",
    "end": "962480"
  },
  {
    "text": "this isn't modeling things well. So one alternative\nis to consider",
    "start": "962480",
    "end": "968980"
  },
  {
    "text": "a heavier-tailed distribution\nthan the normal, namely the t distribution.",
    "start": "968980",
    "end": "974920"
  },
  {
    "text": "And consider identifying\nwhat t distribution best fits the data.",
    "start": "974920",
    "end": "980000"
  },
  {
    "text": " So let's just look at what ends\nup being the maximum likelihood",
    "start": "980000",
    "end": "991620"
  },
  {
    "text": "estimate for the degrees\nof freedom parameter, which is 10 degrees of freedom.",
    "start": "991620",
    "end": "997150"
  },
  {
    "text": "This shows the q-q\nplot when you have a non-Gaussian\ndistribution that's t with 10 degrees of freedom.",
    "start": "997150",
    "end": "1005380"
  },
  {
    "text": "It basically is explaining\nthese residuals quite well, so that's accommodating the\nheavier-tailed distribution",
    "start": "1005380",
    "end": "1013690"
  },
  {
    "text": "of these values. ",
    "start": "1013690",
    "end": "1019630"
  },
  {
    "text": "With this GARCH\nmodel, let's see--",
    "start": "1019630",
    "end": "1026550"
  },
  {
    "text": "if you compare sort of estimates\nof volatility under the GARCH and ARCH models--\nthe GARCH models",
    "start": "1026550",
    "end": "1037470"
  },
  {
    "text": "with the t distribution-- sorry\nt distribution versus Gaussian.",
    "start": "1037470",
    "end": "1044039"
  },
  {
    "text": "Here's just a graph\nshowing time series plots of the estimated\nvolatility over time, which actually look quite close.",
    "start": "1044040",
    "end": "1049860"
  },
  {
    "text": "But when you look\nat the differences, there really are differences. And so it turns out that\nthe volatility function",
    "start": "1049860",
    "end": "1063220"
  },
  {
    "text": "or the volatility estimate\nGARCH models with Gaussian versus GARCH with\nt distributions",
    "start": "1063220",
    "end": "1068460"
  },
  {
    "text": "are really very, very similar. And the heavier\ntailed distribution of the t distribution\nmeans that the distribution",
    "start": "1068460",
    "end": "1078740"
  },
  {
    "text": "of actual volatility is greater. But in terms of\nestimating the volatility,",
    "start": "1078740",
    "end": "1085240"
  },
  {
    "text": "you have quite similar estimates\nof the volatility coming out. And this display--\nwhich you'll be",
    "start": "1085240",
    "end": "1093110"
  },
  {
    "text": "able to see more clearly in the\ncase notes that I'll post up-- show that these are really\nquite similar in magnitude.",
    "start": "1093110",
    "end": "1098875"
  },
  {
    "text": " And the value at risk concept\nthat was just-- by Ken couple",
    "start": "1098875",
    "end": "1111350"
  },
  {
    "text": "weeks ago in his lecture\nfrom Morgan Stanley-- concerns the issue\nof estimating what",
    "start": "1111350",
    "end": "1118500"
  },
  {
    "text": "is the likelihood of returns\nexceeding some threshold.",
    "start": "1118500",
    "end": "1124480"
  },
  {
    "text": "And if we use the t distribution\nfor measuring variability",
    "start": "1124480",
    "end": "1132309"
  },
  {
    "text": "of the excess returns, then\nthe computations in the notes",
    "start": "1132310",
    "end": "1137420"
  },
  {
    "text": "indicate how you would compute\nthese value at risk limits.",
    "start": "1137420",
    "end": "1143000"
  },
  {
    "text": "If you compare\nthe t distribution with a Gaussian distribution at\nthese nominal levels for value",
    "start": "1143000",
    "end": "1148920"
  },
  {
    "text": "at risk of like 2.5%\nor 5%, surprisingly you won't get too much difference.",
    "start": "1148920",
    "end": "1156070"
  },
  {
    "text": "It's really in looking at\nsort of the extreme tails of the distribution that\nthings come into play.",
    "start": "1156070",
    "end": "1164399"
  },
  {
    "text": "And so I wanted to show you how\nthat plays out by showing you",
    "start": "1164400",
    "end": "1173530"
  },
  {
    "text": "another graph here. Those of you who have had\na statistics course before",
    "start": "1173530",
    "end": "1180080"
  },
  {
    "text": "have heard that sort\nof a t distribution can be a good\napproximation to a normal--",
    "start": "1180080",
    "end": "1186801"
  },
  {
    "text": "or it can be approximated\nwell by a normal if the degrees of freedom\nfor the t are at some level.",
    "start": "1186801",
    "end": "1193060"
  },
  {
    "text": "And who wants to suggest\na degrees of freedom that you might\nhave before you're",
    "start": "1193060",
    "end": "1200140"
  },
  {
    "text": "comfortable approximating\na t with a normal? ",
    "start": "1200140",
    "end": "1205480"
  },
  {
    "text": "Danny? AUDIENCE: 30 or 40. PROFESSOR: 30 or 40. Sometimes people say even 25. Above 25, you can almost\nexpect the t distribution",
    "start": "1205480",
    "end": "1213210"
  },
  {
    "text": "to be a good approximation\nto the normal. Well, this is a graph the\nPDF for a standard normal",
    "start": "1213210",
    "end": "1219340"
  },
  {
    "text": "versus a standard t with\n30 degrees of freedom. And you can see that the density\nfunctions are very, very close.",
    "start": "1219340",
    "end": "1226080"
  },
  {
    "text": "The standard-- the CDFs,\nthe cumulative distribution functions, which is\nthe likelihood of being",
    "start": "1226080",
    "end": "1231190"
  },
  {
    "text": "less than or equal to the\nhorizontal value, ranges between 0 and 1, is\nalmost indistinguishable.",
    "start": "1231190",
    "end": "1237779"
  },
  {
    "text": "But if you look at the\ntails of the distribution, here I've computed the\nlog of the CDF function.",
    "start": "1237779",
    "end": "1243470"
  },
  {
    "text": "You basically have\nto move much more than two standard deviations\naway from the mean before there's really\na difference in the t",
    "start": "1243470",
    "end": "1250380"
  },
  {
    "text": "distribution with 30\ndegrees of freedom. Now I'm going to\npage up by reducing",
    "start": "1250380",
    "end": "1256009"
  },
  {
    "text": "the degrees of freedom. Let's see. If we could do a page down here.",
    "start": "1256010",
    "end": "1264480"
  },
  {
    "text": "Page down. Oh, page up. OK.",
    "start": "1264480",
    "end": "1270130"
  },
  {
    "text": "So here is 20\ndegrees of freedom.",
    "start": "1270130",
    "end": "1275520"
  },
  {
    "text": "Here's 10 degrees of\nfreedom, in our case, which turns out to be sort\nof the best fit of the t",
    "start": "1275520",
    "end": "1283010"
  },
  {
    "text": "distribution. And what you can see is that,\nin terms of standard deviation",
    "start": "1283010",
    "end": "1289240"
  },
  {
    "text": "units, up to about two standard\ndeviations below the mean, we're basically getting\nvirtually the same probability mass at the extreme below.",
    "start": "1289240",
    "end": "1297040"
  },
  {
    "text": "But as we go to four or\nsix standard deviations, then we get heavier mass\nwith the t distribution.",
    "start": "1297040",
    "end": "1305210"
  },
  {
    "text": "In discussion of\nresults in finance when you sort of fit models,\npeople talk about, oh, there",
    "start": "1305210",
    "end": "1311730"
  },
  {
    "text": "was six standard deviation\nmove or-- which is just virtually impossible to occur.",
    "start": "1311730",
    "end": "1317210"
  },
  {
    "text": "Well, with t distributions a\nsix standard deviation move occurs about 1 in 10,000\ntimes according to this fit.",
    "start": "1317210",
    "end": "1327700"
  },
  {
    "text": "And so it actually is a\ncommon [? idiomatic. ?] And so it's important to know\nthat these t distributions are",
    "start": "1327700",
    "end": "1336460"
  },
  {
    "text": "benefiting us by giving us\na much better gauge of what the tail distribution is like.",
    "start": "1336460",
    "end": "1342250"
  },
  {
    "text": "And we call these\ndistributions leptokurtic, meaning they're heavier tailed\nthan a normal distribution.",
    "start": "1342250",
    "end": "1350260"
  },
  {
    "text": "Actually, lepto means\nslender, I believe, if you're Greek or have the\nGreek origin of the word.",
    "start": "1350260",
    "end": "1360000"
  },
  {
    "text": "And you can see that\nthe blue curve, which is the t distribution, is\nsort of a bit more slender",
    "start": "1360000",
    "end": "1367070"
  },
  {
    "text": "in the center of the\ndistribution, which allows it to have heavier tails. ",
    "start": "1367070",
    "end": "1376270"
  },
  {
    "text": "All right. So t distributions\nare very useful. Let's go back to\nthis case note here",
    "start": "1376270",
    "end": "1390230"
  },
  {
    "text": "which discusses-- this\ncase note goes through, actually, fitting\nthe t distribution--",
    "start": "1390230",
    "end": "1397950"
  },
  {
    "text": "identifying the degrees of\nfreedom for this t model. And so with the\nrugarch package, we",
    "start": "1397950",
    "end": "1405060"
  },
  {
    "text": "can get the log-likelihood\nof the data fit",
    "start": "1405060",
    "end": "1410350"
  },
  {
    "text": "under the t\ndistribution assumption. And here's a graph of the\nnegative log-likelihood",
    "start": "1410350",
    "end": "1416030"
  },
  {
    "text": "versus the degrees of\nfreedom in the t model. So with maximum\nlikelihood we identify",
    "start": "1416030",
    "end": "1424240"
  },
  {
    "text": "the value which minimizes\nthe negative log likelihood. And that comes out\nas that 10 value.",
    "start": "1424240",
    "end": "1431012"
  },
  {
    "text": " All right.",
    "start": "1431012",
    "end": "1436280"
  },
  {
    "text": "Let's go back to these\nnotes and see what else we want to talk about. ",
    "start": "1436280",
    "end": "1445250"
  },
  {
    "text": "All right. ",
    "start": "1445250",
    "end": "1453380"
  },
  {
    "text": "OK, with these GARCH\nmodels we actually are able to model\nvolatility clustering.",
    "start": "1453380",
    "end": "1459860"
  },
  {
    "text": "And volatility clustering\nis where, over time, you",
    "start": "1459860",
    "end": "1466700"
  },
  {
    "text": "expect volatility to be\nhigh during some periods and to be low during\nother periods.",
    "start": "1466700",
    "end": "1473060"
  },
  {
    "text": "And the GARCH model\ncan accommodate that. So large volatilities\ntend to be followed",
    "start": "1473060",
    "end": "1478080"
  },
  {
    "text": "by large, small\nvolatilities tend to be followed by small ones. OK.",
    "start": "1478080",
    "end": "1483210"
  },
  {
    "text": "The returns have heavier tails\nthan Gaussian distributions.",
    "start": "1483210",
    "end": "1489240"
  },
  {
    "text": "Actually, even if we have\nGaussian errors in the GARCH model, it's still heavier\ntailed than a Gaussian.",
    "start": "1489240",
    "end": "1496870"
  },
  {
    "text": "The homework goes into\nthat a little bit. And the-- well, actually\none of the original papers",
    "start": "1496870",
    "end": "1506590"
  },
  {
    "text": "by Engle with Bollerslev, who\nintroduced the GARCH model, discusses these\nfeatures and how useful",
    "start": "1506590",
    "end": "1513490"
  },
  {
    "text": "they are for modeling\nfinancial time series. Now, a property of these models\nthat may be obvious, perhaps,",
    "start": "1513490",
    "end": "1525860"
  },
  {
    "text": "but it is-- OK,\nthese are models that are appropriate for modeling\ncovariance stationary time",
    "start": "1525860",
    "end": "1533309"
  },
  {
    "text": "series. So the volatility\nmeasure, which is a measure of the\nsquared excess return,",
    "start": "1533310",
    "end": "1542539"
  },
  {
    "text": "is basically a covariance\nstationary process. So what does that mean?",
    "start": "1542540",
    "end": "1548320"
  },
  {
    "text": "That means that's going\nto have a long-term mean. So with these GARCH models\nthat are covariance stationary,",
    "start": "1548320",
    "end": "1555059"
  },
  {
    "text": "there's going to be a long-term\nmean of the GARCH process. And this discussion here\ndetails how this GARCH process",
    "start": "1555060",
    "end": "1566820"
  },
  {
    "text": "is essentially a mean\nreversion of the volatility",
    "start": "1566820",
    "end": "1574389"
  },
  {
    "text": "to that value. So basically, the sort\nof excess volatility",
    "start": "1574390",
    "end": "1581110"
  },
  {
    "text": "of the squared\nresiduals relative to their long-term\naverage is some multiple",
    "start": "1581110",
    "end": "1587790"
  },
  {
    "text": "of the previous period's\nexcess volatility. So if we build forecasting\nmodels of volatility",
    "start": "1587790",
    "end": "1596770"
  },
  {
    "text": "with GARCH models,\nwhat's going to happen?",
    "start": "1596770",
    "end": "1602390"
  },
  {
    "text": "Basically, in the\nlong run we predict that any volatility\nvalue is going to revert",
    "start": "1602390",
    "end": "1608610"
  },
  {
    "text": "to this long-run average. And in the short run, it's\ngoing to move incrementally",
    "start": "1608610",
    "end": "1614600"
  },
  {
    "text": "to that value. So these GARCH models are very\ngood for describing volatility",
    "start": "1614600",
    "end": "1622080"
  },
  {
    "text": "relative to the\nlong-term average. In terms of their\nusefulness for prediction, well, they really\npredict that volatility",
    "start": "1622080",
    "end": "1629580"
  },
  {
    "text": "is going to revert back\nto the mean at some rate. And the rate at which the\nvolatility reverts back",
    "start": "1629580",
    "end": "1638840"
  },
  {
    "text": "is given by alpha_1 plus beta_1. So that number,\nwhich is less than 1",
    "start": "1638840",
    "end": "1645870"
  },
  {
    "text": "for covariance\nstationarity, is sort of measuring, basically, how\nquickly you are reverting back",
    "start": "1645870",
    "end": "1653150"
  },
  {
    "text": "to the mean. And that sum is actually\ncalled a persistence parameter in GARCH models as well.",
    "start": "1653150",
    "end": "1658820"
  },
  {
    "text": "So is volatility\npersisting or not? Well, the larger\nalpha_1 plus beta_1 is, the more persistent\nvolatility is, meaning it's",
    "start": "1658820",
    "end": "1667290"
  },
  {
    "text": "reverting back to that long-run\naverage very, very slowly. In the implementation\nof volatility estimates",
    "start": "1667290",
    "end": "1674179"
  },
  {
    "text": "with the risk\nmetrics methodology, they actually don't assume that\nthere is a long-run volatility.",
    "start": "1674180",
    "end": "1684090"
  },
  {
    "text": "And so that basically you'll\nhave alpha_1 be equal to 0 and beta_1 equal to, say, 0.95.",
    "start": "1684090",
    "end": "1694130"
  },
  {
    "text": "So or rather the alpha_0 is\n0 and the alpha_1 and beta_1",
    "start": "1694130",
    "end": "1702270"
  },
  {
    "text": "will actually sum to 1. And so you actually are tracking\na potentially non-stationary",
    "start": "1702270",
    "end": "1708220"
  },
  {
    "text": "volatility, which allows you\nto be estimating the volatility",
    "start": "1708220",
    "end": "1715549"
  },
  {
    "text": "without presuming a\nlong-run average is consistent with the past. ",
    "start": "1715550",
    "end": "1725290"
  },
  {
    "text": "There are many extensions\nof the GARCH models. And there's wide\nliterature on that.",
    "start": "1725290",
    "end": "1733162"
  },
  {
    "text": "For this course, I think\nit's important to understand the fundamentals of\nthese models in terms",
    "start": "1733162",
    "end": "1738200"
  },
  {
    "text": "of how they're specified under\nGaussian and t assumptions. Extending them can\nbe very interesting.",
    "start": "1738200",
    "end": "1747050"
  },
  {
    "text": "And there are many papers\nto look at for that.",
    "start": "1747050",
    "end": "1752650"
  },
  {
    "text": "OK.  let's pause for a minute\nand get the next topic.",
    "start": "1752650",
    "end": "1761210"
  },
  {
    "start": "1761210",
    "end": "1774921"
  },
  {
    "text": "All right. The next topic is time series,\nmultivariate time series.",
    "start": "1774921",
    "end": "1782630"
  },
  {
    "text": "In two lectures ago\nof mine, we talked about univariate time series\nand basic methodologies there.",
    "start": "1782630",
    "end": "1789270"
  },
  {
    "text": "We're now going to be\nextending that to multivariate time series.",
    "start": "1789270",
    "end": "1795040"
  },
  {
    "text": "Turns out there's a multivariate\nWold representation theorem,",
    "start": "1795040",
    "end": "1800110"
  },
  {
    "text": "extension of the univariate one. There are\nautoregressive processes for multivariate\ncases, which are vector",
    "start": "1800110",
    "end": "1807050"
  },
  {
    "text": "autoregressive processes. Least squares estimation\ncomes into play. And then we'll see where\nour regression analysis",
    "start": "1807050",
    "end": "1817460"
  },
  {
    "text": "understanding\nallows us to specify these vector autoregressive\nprocesses nicely.",
    "start": "1817460",
    "end": "1825669"
  },
  {
    "text": "There's an optimality properties\nof ordinary least squares estimates component wise, which\nwe'll highlight in about a half",
    "start": "1825670",
    "end": "1835020"
  },
  {
    "text": "an hour. And go through the maximum\nlikelihood estimation model selection methods,\nwhich are just",
    "start": "1835020",
    "end": "1844680"
  },
  {
    "text": "very straightforward\nextensions of the same concepts for univariate time series\nand univariate regressions.",
    "start": "1844680",
    "end": "1854270"
  },
  {
    "text": "So let's talk-- let's\nintroduce the notation for multivariate time series. We have a stochastic process,\nwhich now is multivariate.",
    "start": "1854270",
    "end": "1864990"
  },
  {
    "text": "So we have bold X of t is\nsome m-dimensional valued",
    "start": "1864990",
    "end": "1871320"
  },
  {
    "text": "random variable. And it's a stochastic process\nthat varies over time t.",
    "start": "1871320",
    "end": "1881170"
  },
  {
    "text": "And we can think of this\nas m different time series",
    "start": "1881170",
    "end": "1888910"
  },
  {
    "text": "corresponding to the m\ncomponents of the given process. So, say, with exchange\nrates we could",
    "start": "1888910",
    "end": "1894899"
  },
  {
    "text": "be modeling m different exchange\nrate values and want to model",
    "start": "1894900",
    "end": "1900130"
  },
  {
    "text": "those jointly as a time series. Or we could have collections\nof stocks that we're modeling.",
    "start": "1900130",
    "end": "1907450"
  },
  {
    "text": "So each of the\ncomponents individually can be treated as univariate\nseries with univariate methods.",
    "start": "1907450",
    "end": "1913680"
  },
  {
    "text": " With the multivariate\ncase, we extend",
    "start": "1913680",
    "end": "1920740"
  },
  {
    "text": "the definition of\ncovariance stationarity to correspond to finite, bounded\nfirst and second order moments.",
    "start": "1920740",
    "end": "1931390"
  },
  {
    "text": "So we need to talk\nabout the first order moment of the\nmultivariate time series.",
    "start": "1931390",
    "end": "1937820"
  },
  {
    "text": "Mu now is an m vector, which is\nthe vector of expected values",
    "start": "1937820",
    "end": "1942970"
  },
  {
    "text": "of the individual\ncomponents, which we can denote by mu_1 through mu_m. So we basically have m\nvectors for our mean.",
    "start": "1942970",
    "end": "1951534"
  },
  {
    "text": " Then for the\nvariance/covariance matrix,",
    "start": "1951535",
    "end": "1960370"
  },
  {
    "text": "let's define gamma_0 to be\nthe variance/covariance matrix",
    "start": "1960370",
    "end": "1966490"
  },
  {
    "text": "of the t-th observation of\nour multivariate process. So that's equal to the\nexpected value of X_t minus mu",
    "start": "1966490",
    "end": "1977740"
  },
  {
    "text": "X_t minus mu prime. So when we write that\ndown, we have X_t minus mu.",
    "start": "1977740",
    "end": "1990900"
  },
  {
    "text": "This is basically\nan m by 1 vector",
    "start": "1990900",
    "end": "1996160"
  },
  {
    "text": "and then X_t minus mu\nprime is a 1 by m vector.",
    "start": "1996160",
    "end": "2003530"
  },
  {
    "text": "And so the product of that\nis an m by m quantity. So the 1, 1 element of that\nproduct is the variance",
    "start": "2003530",
    "end": "2014750"
  },
  {
    "text": "of X_(1,t). And the diagonal entries\nare the variances of the components series.",
    "start": "2014750",
    "end": "2020800"
  },
  {
    "text": "And the off-diagonal\nvalues are the covariance between the i-th row series\nand the j-th column series,",
    "start": "2020800",
    "end": "2030220"
  },
  {
    "text": "as given by the i-th row of\nX and the j-th column of X",
    "start": "2030220",
    "end": "2035559"
  },
  {
    "text": "transpose. So we're just\ncollecting together all the variances/covariances\ntogether.",
    "start": "2035560",
    "end": "2042770"
  },
  {
    "text": "And the notation is very\nstraightforward and simple with the matrix\nnotation given here.",
    "start": "2042770",
    "end": "2050800"
  },
  {
    "text": "Now, the correlation matrix,\nr_0, is obtained by pre-",
    "start": "2050800",
    "end": "2061030"
  },
  {
    "text": "and post-multiplying this\ncovariance matrix gamma_0 by a diagonal matrix\nwith the square roots",
    "start": "2061030",
    "end": "2071110"
  },
  {
    "text": "of the diagonal of this matrix. Now what's a correlation? Correlation is the correlation\nbetween two random variables",
    "start": "2071110",
    "end": "2078800"
  },
  {
    "text": "where we've standardized\nthe variables to have mean 0 and variance 1.",
    "start": "2078800",
    "end": "2085579"
  },
  {
    "text": " So what we want to do is\nbasically divide through all",
    "start": "2085580",
    "end": "2093440"
  },
  {
    "text": "of these variables by\ntheir standard deviation and compute the covariance\nmatrix on that new scaling.",
    "start": "2093440",
    "end": "2102440"
  },
  {
    "text": "That's equivalent to just\npre- and post-multiplying by that diagonal of the inverse\nof the standard deviations.",
    "start": "2102440",
    "end": "2108390"
  },
  {
    "text": "So with matrix\nalgebra, that formula is-- I think it's very clear.",
    "start": "2108390",
    "end": "2118509"
  },
  {
    "text": "And this is-- now with-- the\nprevious discussion was just",
    "start": "2118510",
    "end": "2126320"
  },
  {
    "text": "looking at the sort of\ncontemporaneous covariance matrix of the time series\nvalues at the given time",
    "start": "2126320",
    "end": "2132570"
  },
  {
    "text": "t with itself. We want to look at, also, the\ncross-covariance matrices.",
    "start": "2132570",
    "end": "2138970"
  },
  {
    "text": "So how are the current values\nof the multivariate time series,",
    "start": "2138970",
    "end": "2144890"
  },
  {
    "text": "X_t-- how do they covary with\nthe k-th lag of those values?",
    "start": "2144890",
    "end": "2152000"
  },
  {
    "text": "So gamma_k is looking at how\nthe current period vector",
    "start": "2152000",
    "end": "2158340"
  },
  {
    "text": "values is covaried with the\nk-th lag of those values.",
    "start": "2158340",
    "end": "2164750"
  },
  {
    "text": "So this covariance matrix\nhas covariance elements given in this display.",
    "start": "2164750",
    "end": "2174089"
  },
  {
    "text": "And we can define the\ncross-correlation matrix by similarly pre-\nand post-multiplying",
    "start": "2174090",
    "end": "2181450"
  },
  {
    "text": "by the inverse of the\nstandard deviations. The diagonal of gamma_0\nis the covariance--",
    "start": "2181450",
    "end": "2188565"
  },
  {
    "text": "or is the matrix of diagonal\nentries of variances.",
    "start": "2188565",
    "end": "2193680"
  },
  {
    "text": "Now, properties of\nthese matrices is-- OK, gamma_0 is a symmetric\nmatrix that we had before.",
    "start": "2193680",
    "end": "2200150"
  },
  {
    "text": "But gamma k where k is\ngreater than 1 or less than-- or greater or equal to 1 or\nless than-- basically different",
    "start": "2200150",
    "end": "2206600"
  },
  {
    "text": "from 0. This is not symmetric. Basically, you may have\nlags of some variables that",
    "start": "2206600",
    "end": "2216680"
  },
  {
    "text": "are positively correlated with\nothers and not vice versa. So the off-diagonal entries\nhere aren't necessarily even",
    "start": "2216680",
    "end": "2228510"
  },
  {
    "text": "of the same sign, let\nalone equal and symmetric. So with these\ncovariance matrices,",
    "start": "2228510",
    "end": "2237800"
  },
  {
    "text": "one can look at\nhow things covary and whether they are--\nwhether there is, basically,",
    "start": "2237800",
    "end": "2245290"
  },
  {
    "text": "a dependence between them. And you can define--\nit's basically the j star",
    "start": "2245290",
    "end": "2250860"
  },
  {
    "text": "series-- the j star component\nof the multivariate time series",
    "start": "2250860",
    "end": "2256890"
  },
  {
    "text": "may lead the j-th one if the\ncovariance of the k-th lag of j",
    "start": "2256890",
    "end": "2263789"
  },
  {
    "text": "star is different from 0--\nor the covariance of j star k",
    "start": "2263790",
    "end": "2270650"
  },
  {
    "text": "lags ago is non-zero,\ncovaries with the j-th lag.",
    "start": "2270650",
    "end": "2277510"
  },
  {
    "text": "Sorry. The current lag. So X_(t, j star)\nwill lead X_(t, j). Basically, there's information\nin the lagged values",
    "start": "2277510",
    "end": "2287290"
  },
  {
    "text": "of j star for the component j.",
    "start": "2287290",
    "end": "2293110"
  },
  {
    "text": "So if we're trying to build\nmodels-- linear regression",
    "start": "2293110",
    "end": "2299100"
  },
  {
    "text": "models, even, where we're\ntrying to look at how-- trying to predict values, then if\nthere's a non-zero covariance,",
    "start": "2299100",
    "end": "2306520"
  },
  {
    "text": "then we can use those\nvariables' information to actually project what the\none variable is given the other.",
    "start": "2306520",
    "end": "2314760"
  },
  {
    "text": "Now, it can be the case that\nyou have non-zero covariance",
    "start": "2314760",
    "end": "2321900"
  },
  {
    "text": "in both directions. And so that suggests\nthat there can be sort of feedback\nbetween these variables.",
    "start": "2321900",
    "end": "2328400"
  },
  {
    "text": "It's not just that one\nvariable causes another, but there can\nactually be feedback.",
    "start": "2328400",
    "end": "2334170"
  },
  {
    "text": "In economics and\nfinance, there's a notion of Granger causality.",
    "start": "2334170",
    "end": "2340790"
  },
  {
    "text": "And basically that--\nwell, Granger and Engle got the Nobel Prize number of\nyears ago based on their work.",
    "start": "2340790",
    "end": "2348890"
  },
  {
    "text": "And that work deals\nwith identifying, in part, judgments of\ncausality between--",
    "start": "2348890",
    "end": "2358440"
  },
  {
    "text": "or Granger causality\nbetween variables in economic time series. And so Granger\ncausality basically",
    "start": "2358440",
    "end": "2364780"
  },
  {
    "text": "is sort of positive or non-zero\ncorrelation between variables",
    "start": "2364780",
    "end": "2371900"
  },
  {
    "text": "where lags of one variable will\ncause another or cause changes in another. ",
    "start": "2371900",
    "end": "2381309"
  },
  {
    "text": "All right. I want to just alert you to\nthe existence of this Wold decomposition theorem.",
    "start": "2381310",
    "end": "2387750"
  },
  {
    "text": "This is an advanced theorem,\nbut it's a useful theorem",
    "start": "2387750",
    "end": "2393060"
  },
  {
    "text": "to know exists. And this extends the univariate\nWold decomposition theorem,",
    "start": "2393060",
    "end": "2401000"
  },
  {
    "text": "which concerns the--\nwhenever we have a covariant stationary\nprocess, there",
    "start": "2401000",
    "end": "2406750"
  },
  {
    "text": "exists a representation\nof that process, which is the sum of a deterministic\nprocess and a moving",
    "start": "2406750",
    "end": "2419050"
  },
  {
    "text": "average process\nof a white noise. So if you're modeling\na time series",
    "start": "2419050",
    "end": "2427859"
  },
  {
    "text": "and you're going\nto be specifying a covariance stationary\nprocess for that,",
    "start": "2427860",
    "end": "2433890"
  },
  {
    "text": "there does exist a Wold\ndecomposition representation of that.",
    "start": "2433890",
    "end": "2439190"
  },
  {
    "text": "You can basically\ndetermine-- identify",
    "start": "2439190",
    "end": "2444660"
  },
  {
    "text": "the deterministic process\nthat the process might follow. It might be a linear trend over\ntime or an exponential trend.",
    "start": "2444660",
    "end": "2452650"
  },
  {
    "text": "And if you remove that sort\nof deterministic process V_t,",
    "start": "2452650",
    "end": "2458490"
  },
  {
    "text": "then what remains\nis a process that can be modeled with a moving\naverage of white noise, these.",
    "start": "2458490",
    "end": "2469390"
  },
  {
    "text": "Now here, everything is\nchanged from univariate case to multivariate case, so we have\nmatrices in place of constants",
    "start": "2469390",
    "end": "2476670"
  },
  {
    "text": "from before. So these-- new\nconcepts here are we",
    "start": "2476670",
    "end": "2484330"
  },
  {
    "text": "have a multivariate\nwhite noise process.  That's going to be a process\neta_t which is m-dimensional",
    "start": "2484330",
    "end": "2493819"
  },
  {
    "text": "which has mean 0. And the variance\nmatrix of this m-vector",
    "start": "2493820",
    "end": "2502020"
  },
  {
    "text": "is going to be sigma, which is\nnow m by m variance/covariance matrix of the components.",
    "start": "2502020",
    "end": "2509810"
  },
  {
    "text": "And that must be a\npositive semi-definite. And for white noise, we have\ncovariances between, say,",
    "start": "2509810",
    "end": "2518880"
  },
  {
    "text": "the current t innovation and\na lag of its value are 0. So these are uncorrelated\nmultivariate white noise",
    "start": "2518880",
    "end": "2527440"
  },
  {
    "text": "processes. And so they're uncorrelated\nwith each other at various lags.",
    "start": "2527440",
    "end": "2534130"
  },
  {
    "text": "And the innovation eta_t\nhas a covariance of 0",
    "start": "2534130",
    "end": "2539470"
  },
  {
    "text": "with the deterministic process. Actually, that's\npretty much a given",
    "start": "2539470",
    "end": "2545720"
  },
  {
    "text": "if we have a\ndeterministic process. Now, the term\npsi_k-- basically we",
    "start": "2545720",
    "end": "2552000"
  },
  {
    "text": "have this m-vector X_t is equal\nto some m-vectored process V_t plus this weighted\naverage of innovations.",
    "start": "2552000",
    "end": "2558565"
  },
  {
    "text": " What's required is that the sum\nof this-- basically each term",
    "start": "2558565",
    "end": "2568300"
  },
  {
    "text": "psi_k and its\ntranspose converges. Now, if you were to\ntake that X_t process",
    "start": "2568300",
    "end": "2574990"
  },
  {
    "text": "and say let me compute the\nvariance/covariance matrix of that representation,\nthen you would basically",
    "start": "2574990",
    "end": "2582309"
  },
  {
    "text": "get terms in the\ncovariance matrix which includes this sum of terms.",
    "start": "2582310",
    "end": "2588710"
  },
  {
    "text": "So that sum has to be\nfinite in order for this to be covariance stationary.",
    "start": "2588710",
    "end": "2595509"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. PROFESSOR: Yes? AUDIENCE: Could you define\nwhat you mean by innovation? PROFESSOR: Oh, OK.",
    "start": "2595509",
    "end": "2602960"
  },
  {
    "text": "Well, the innovation\nis-- let's see. With-- let me go back up here.",
    "start": "2602960",
    "end": "2612230"
  },
  {
    "text": "OK. ",
    "start": "2612230",
    "end": "2619240"
  },
  {
    "text": "The innovation process--\ninnovation process. ",
    "start": "2619240",
    "end": "2626329"
  },
  {
    "text": "OK, if we have, as\nin this case, we have sort of our X_t\nstochastic process.",
    "start": "2626330",
    "end": "2638760"
  },
  {
    "text": "And we have sort of, say,\nf sub t minus 1 equal",
    "start": "2638760",
    "end": "2644710"
  },
  {
    "text": "to the information on\nX_(t-1), X_(t-2)...",
    "start": "2644710",
    "end": "2655250"
  },
  {
    "text": "Basically consisting of the\ninformation set available before time t.",
    "start": "2655250",
    "end": "2661520"
  },
  {
    "text": "Then we can model X_t to be\nthe expected value of X_t given",
    "start": "2661520",
    "end": "2668810"
  },
  {
    "text": "F_(t-1) plus an innovation.",
    "start": "2668810",
    "end": "2678530"
  },
  {
    "text": "And so our objective\nin these models is to be thinking of how is that\nprocess evolving where we can",
    "start": "2678530",
    "end": "2690000"
  },
  {
    "text": "model the process as well as\npossible using information up to time before t. And then there's some\ndisturbance about that model.",
    "start": "2690000",
    "end": "2699599"
  },
  {
    "text": "There's something new that's\nhappened at time t that wasn't available before. And that's this\ninnovation process.",
    "start": "2699600",
    "end": "2707660"
  },
  {
    "text": "So this representation\nwith the Wold decomposition is converting the-- or\nrepresenting, basically,",
    "start": "2707660",
    "end": "2716600"
  },
  {
    "text": "the bits of information that\nare affecting the process that are occurring at time t and\nwasn't available prior to that.",
    "start": "2716600",
    "end": "2724294"
  },
  {
    "text": " All right. Well, let's move on to vector\nautoregressive processes.",
    "start": "2724294",
    "end": "2733690"
  },
  {
    "start": "2733690",
    "end": "2739839"
  },
  {
    "text": "OK, this representation for a\nvector autoregressive process is an extension of the\nunivariate autoregressive",
    "start": "2739840",
    "end": "2747580"
  },
  {
    "text": "process to p dimensions. Sorry, to m dimensions. And so our X_t is an m-vector.",
    "start": "2747580",
    "end": "2756630"
  },
  {
    "text": "That's going to be equal to some\nconstant vector C plus a matrix",
    "start": "2756630",
    "end": "2765119"
  },
  {
    "text": "phi_1 times lag of X_t\nfirst order, X_(t-1).",
    "start": "2765120",
    "end": "2770850"
  },
  {
    "text": "Plus another matrix, phi_2 times\nthe second lag of X_t, X_(t-2).",
    "start": "2770850",
    "end": "2779790"
  },
  {
    "text": "Up to the p-th term, which is\na phi_p, m by m matrix times,",
    "start": "2779790",
    "end": "2785040"
  },
  {
    "text": "X_(t-p) plus this\ninnovation term. So this is essentially--\nthis is basically",
    "start": "2785040",
    "end": "2791390"
  },
  {
    "text": "how a univariate\nautoregressive process extends to an m-variate case.",
    "start": "2791390",
    "end": "2797320"
  },
  {
    "text": "And what this\nallows one to do is",
    "start": "2797320",
    "end": "2803550"
  },
  {
    "text": "model how a given component\nof the multivariate series-- like how one\nexchange rate varies",
    "start": "2803550",
    "end": "2811030"
  },
  {
    "text": "depending on how other\nexchange rates might vary. Exchange rates tend to co-move\ntogether in that example.",
    "start": "2811030",
    "end": "2820490"
  },
  {
    "text": "So if we look at\nwhat this represents in terms of basically\na component series,",
    "start": "2820490",
    "end": "2828130"
  },
  {
    "text": "we can consider\nfixing j, a component of the multivariate process.",
    "start": "2828130",
    "end": "2833830"
  },
  {
    "text": "It could be the first,\nthe last, or the j-th, somewhere in the middle. And that component\ntime series-- like",
    "start": "2833830",
    "end": "2840590"
  },
  {
    "text": "a fixed exchange\nrate series or time series, whatever we're\nfocused on in our modeling--",
    "start": "2840590",
    "end": "2846920"
  },
  {
    "text": "is a generalization of the\nautoregressive model where we have the autoregressive\nterms of the j-th series on lags",
    "start": "2846920",
    "end": "2857020"
  },
  {
    "text": "of the j-th series\nup to order p. So we have the univariate\nautoregressive model,",
    "start": "2857020",
    "end": "2863410"
  },
  {
    "text": "but we also add to that\nterms corresponding to the relationship\nbetween X_j and X_(j star).",
    "start": "2863410",
    "end": "2872130"
  },
  {
    "text": "So how does X_j,\nthe j-th component, depend on other variables,\nother components",
    "start": "2872130",
    "end": "2877360"
  },
  {
    "text": "of the multivariate series. And those are given here. So it's a convenient way to\nallow for interdependence",
    "start": "2877360",
    "end": "2887890"
  },
  {
    "text": "among the components\nand model that. ",
    "start": "2887890",
    "end": "2895210"
  },
  {
    "text": "OK. This slide deals with\nrepresenting a p-th order",
    "start": "2895210",
    "end": "2905490"
  },
  {
    "text": "process as a first order process\nwith vector autoregressions.",
    "start": "2905490",
    "end": "2912720"
  },
  {
    "text": "Now the concept here is really\na very powerful concept that's",
    "start": "2912720",
    "end": "2917970"
  },
  {
    "text": "applied in time\nseries methods, which is when you are modeling\ndependence that goes back, say,",
    "start": "2917970",
    "end": "2931110"
  },
  {
    "text": "a number of lags like\np lags, the structure",
    "start": "2931110",
    "end": "2937060"
  },
  {
    "text": "can actually be re-expressed\nas simply a first order",
    "start": "2937060",
    "end": "2942450"
  },
  {
    "text": "dependence only. And so it's much easier sort\nof to deal with just a lag one",
    "start": "2942450",
    "end": "2949069"
  },
  {
    "text": "dependence than to\nconsider p lag dependence and the complications\ninvolved with that.",
    "start": "2949070",
    "end": "2957559"
  },
  {
    "text": "So-- and this\ntechnique is one where, in the early days of fitting,\nlike autoregressive moving",
    "start": "2957560",
    "end": "2966700"
  },
  {
    "text": "average processes and\nvarious smoothing methods,",
    "start": "2966700",
    "end": "2974520"
  },
  {
    "text": "the model-- basically\naccommodating",
    "start": "2974520",
    "end": "2980520"
  },
  {
    "text": "p lags complicated the\nanalysis enormously. But one can actually\nre-express it just",
    "start": "2980520",
    "end": "2986740"
  },
  {
    "text": "as a first order lag problem. So in this case, what\none does is one considers",
    "start": "2986740",
    "end": "2993859"
  },
  {
    "text": "for a vector autoregressive\nprocess of order of p, simply stacking the\nvalues of the process.",
    "start": "2993860",
    "end": "3008090"
  },
  {
    "text": "So let me just highlight\nwhat's going on there. ",
    "start": "3008090",
    "end": "3017619"
  },
  {
    "text": "So if we have basically--\nOK, so if we have X_1,",
    "start": "3017620",
    "end": "3029500"
  },
  {
    "text": "X_2, X_n, which are\nall m by 1 values,",
    "start": "3029500",
    "end": "3038680"
  },
  {
    "text": "m-vectors of the\nstochastic process. Then consider defining Z_t\nto be equal to X_t transpose,",
    "start": "3038680",
    "end": "3056769"
  },
  {
    "text": "X_(t-1) transpose up\nto X_(t-p-1) transpose.",
    "start": "3056770",
    "end": "3061889"
  },
  {
    "start": "3061889",
    "end": "3067378"
  },
  {
    "text": "Or this is t minus (p-1). So there are p terms. ",
    "start": "3067378",
    "end": "3073630"
  },
  {
    "text": "And then if we consider\nthe lagged value of that,",
    "start": "3073630",
    "end": "3080286"
  },
  {
    "text": "that's X_(t-1), X_(t-2),\nX_(t-p) transpose.",
    "start": "3080286",
    "end": "3090470"
  },
  {
    "text": "So what we've done is\nwe're considering Z_t. This is going to be m times p.",
    "start": "3090470",
    "end": "3100380"
  },
  {
    "text": "It's actually 1 by m\ntimes p in this notation.",
    "start": "3100380",
    "end": "3106891"
  },
  {
    "text": "Well, actually I guess I\nshould put transpose here. So m times p by 1.",
    "start": "3106892",
    "end": "3114740"
  },
  {
    "text": "OK, in the lecture\nnotes it actually is primed there to\nindicate the transpose.",
    "start": "3114740",
    "end": "3120640"
  },
  {
    "text": "Well, if you define Z_t\nand Z_(t-1) this way, then Z_t is equal to D\nplus A of Z_(t-1) plus F.",
    "start": "3120640",
    "end": "3130609"
  },
  {
    "text": "Where this is d, basically\nthe constant term has the C entering and then\n0's everywhere else.",
    "start": "3130610",
    "end": "3136660"
  },
  {
    "text": "And the A matrix is\nphi_1, phi_2, up to phi_p.",
    "start": "3136660",
    "end": "3143819"
  },
  {
    "text": "And so basically the Z_t\nvector transforms the Z_t--",
    "start": "3143820",
    "end": "3156410"
  },
  {
    "text": "or is the transform--\nthis linear transformation of the Z_(t-1).",
    "start": "3156410",
    "end": "3163290"
  },
  {
    "text": "And we have sort of\na very simple form for the constant term and a very\nsimple form for the F vector.",
    "start": "3163290",
    "end": "3172640"
  },
  {
    "text": "And this is-- renders the model\ninto a sort of a first order",
    "start": "3172640",
    "end": "3179700"
  },
  {
    "text": "time series model with a\nlarger multivariate series,",
    "start": "3179700",
    "end": "3186270"
  },
  {
    "text": "basically mp by 1. Now, with this representation\nwe basically have-- we",
    "start": "3186270",
    "end": "3201380"
  },
  {
    "text": "can demonstrate that the process\nis going to be stationary",
    "start": "3201380",
    "end": "3211039"
  },
  {
    "text": "if all eigenvalues of\nthe companion matrix A have modulus less than 1.",
    "start": "3211040",
    "end": "3218060"
  },
  {
    "text": "And let's see-- if we go\nback to the expression.",
    "start": "3218060",
    "end": "3223320"
  },
  {
    "text": "OK, if the eigenvalues of\nthis matrix A are less than 1,",
    "start": "3223320",
    "end": "3230750"
  },
  {
    "text": "then we won't get sort\nof an explosive behavior of the process when this\nbasically increments over time",
    "start": "3230750",
    "end": "3240319"
  },
  {
    "text": "with every previous\nvalue getting multiplied by the A matrix and\nscaling the process over time",
    "start": "3240320",
    "end": "3248420"
  },
  {
    "text": "by the A-th power. So that is required. All eigenvalues of A\nhave to be less than 1.",
    "start": "3248420",
    "end": "3254619"
  },
  {
    "text": "And equivalently, all\nroots of this equation need to be outside\nthe unit circle.",
    "start": "3254620",
    "end": "3261300"
  },
  {
    "text": "You remember there was a\nconstraint of-- or a condition for univariate\nautoregressive models",
    "start": "3261300",
    "end": "3270030"
  },
  {
    "text": "to be stationary, that the roots\nof the characteristic equation",
    "start": "3270030",
    "end": "3275100"
  },
  {
    "text": "are all outside the unit circle. And the class notes\ngo through and went",
    "start": "3275100",
    "end": "3280170"
  },
  {
    "text": "through the derivation of that. This is the extension of that\nto the multivariate case.",
    "start": "3280170",
    "end": "3286820"
  },
  {
    "text": "And so basically\none needs to solve for roots of a polynomial in\nz and determine whether those",
    "start": "3286820",
    "end": "3294100"
  },
  {
    "text": "are outside the unit circle.",
    "start": "3294100",
    "end": "3299120"
  },
  {
    "text": "Who can tell me what the\norder of the polynomial is here for this sort\nof determinant equation?",
    "start": "3299120",
    "end": "3307880"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] mp. PROFESSOR: mp. Yes. It's basically of power mp.",
    "start": "3307880",
    "end": "3313670"
  },
  {
    "text": "So in a determinant\nyou basically are taking products\nof the m components",
    "start": "3313670",
    "end": "3319910"
  },
  {
    "text": "in the matrix, various\nlinear combinations of those. So that's going to be an\nmp-dimensional polynomial.",
    "start": "3319910",
    "end": "3328610"
  },
  {
    "text": "All right. Well, the mean of the\nstationary VAR process can be computed rather\neasily by taking expectations",
    "start": "3328610",
    "end": "3337220"
  },
  {
    "text": "of this on both sides. So if we take the\nexpectation of X_t",
    "start": "3337220",
    "end": "3344720"
  },
  {
    "text": "and take expectations\nacross both sides, we get that mu is the C vector\nplus the product of the phi_k's",
    "start": "3344720",
    "end": "3357859"
  },
  {
    "text": "times mu plus 0. So mu, the unconditional\nmean of the process,",
    "start": "3357860",
    "end": "3365619"
  },
  {
    "text": "actually has this\nformula, just solving",
    "start": "3365620",
    "end": "3370640"
  },
  {
    "text": "for mu in the top-- in the\nsecond line to the third line.",
    "start": "3370640",
    "end": "3378809"
  },
  {
    "text": "So here we can see that\nbasically this expression 1",
    "start": "3378810",
    "end": "3387380"
  },
  {
    "text": "minus phi_1 through phi_p,\nthat inverse has to exist.",
    "start": "3387380",
    "end": "3393039"
  },
  {
    "text": "And actually, if we then\nplug in the value of C in terms of the\nunconditional mean,",
    "start": "3393040",
    "end": "3399049"
  },
  {
    "text": "we get this expression\nfor the original process. So the unconditional mean C,\nif we demeaned the process,",
    "start": "3399050",
    "end": "3409920"
  },
  {
    "text": "there's basically no mean term. There's 0. And so basically the\nmean-adjusted process",
    "start": "3409920",
    "end": "3417500"
  },
  {
    "text": "X follows this multivariate\nvector autoregression with no mean, which is actually\nused when this is specified.",
    "start": "3417500",
    "end": "3428430"
  },
  {
    "text": " Now, this vector\nautoregression model",
    "start": "3428430",
    "end": "3438849"
  },
  {
    "text": "can be expressed as a system\nof regression equations.",
    "start": "3438850",
    "end": "3445760"
  },
  {
    "text": "And so what we have with the\nmultivariate series, if we have",
    "start": "3445760",
    "end": "3453820"
  },
  {
    "text": "multivariate data, we'll have\nn sample observations x_t,",
    "start": "3453820",
    "end": "3458870"
  },
  {
    "text": "which is basically the m-vector\nof the multivariate process observed for n time points.",
    "start": "3458870",
    "end": "3465710"
  },
  {
    "text": "And for the\ncomputations here, we're going to assume that\nwe have p sort of-- we",
    "start": "3465710",
    "end": "3472050"
  },
  {
    "text": "have pre-sample observations\navailable to us. So we're essentially going\nto be considering models",
    "start": "3472050",
    "end": "3478980"
  },
  {
    "text": "where we condition\non the first p time points in order to facilitate\nthe estimation methodology.",
    "start": "3478980",
    "end": "3487660"
  },
  {
    "text": "Then we can set up m\nregression models corresponding to each component of\nthe m-variate series.",
    "start": "3487660",
    "end": "3496080"
  },
  {
    "text": "And so what we have\nis our original--",
    "start": "3496080",
    "end": "3512190"
  },
  {
    "text": "we have our collection of data\nvalues, which is x_1 transpose,",
    "start": "3512190",
    "end": "3519099"
  },
  {
    "text": "x_2 transpose, down\nto x_n transpose,",
    "start": "3519100",
    "end": "3525750"
  },
  {
    "text": "which is an n by m matrix.",
    "start": "3525750",
    "end": "3532290"
  },
  {
    "text": "OK, this is our\nmultivariate time series where we were\njust-- the first row corresponds to the first time\nvalues, nth row to the nth time",
    "start": "3532290",
    "end": "3539015"
  },
  {
    "text": "values.  And we can set up\nm regression models",
    "start": "3539015",
    "end": "3545579"
  },
  {
    "text": "where we're going\nto consider modeling",
    "start": "3545580",
    "end": "3551410"
  },
  {
    "text": "the j-th column of this matrix. So we're just picking out\nthe univariate time series",
    "start": "3551410",
    "end": "3559180"
  },
  {
    "text": "corresponding to\nthe j-th component. That's y j. And we're going to\nmodel that as Z beta j",
    "start": "3559180",
    "end": "3570260"
  },
  {
    "text": "plus epsilon j where Z is given\nby the vector of lagged values",
    "start": "3570260",
    "end": "3581380"
  },
  {
    "text": "of the multivariate\nprocess where there's, for the t-th-- t\nminus first value",
    "start": "3581380",
    "end": "3589040"
  },
  {
    "text": "we have that current\nvalue-- or the t minus first, t minus\nsecond, up to t minus p.",
    "start": "3589040",
    "end": "3594890"
  },
  {
    "text": "So we have basically\np m-vectors here.",
    "start": "3594890",
    "end": "3601210"
  },
  {
    "text": "And so this j-th time\nseries has elements",
    "start": "3601210",
    "end": "3609099"
  },
  {
    "text": "that follow a linear\nregression model",
    "start": "3609100",
    "end": "3614190"
  },
  {
    "text": "on the lags of the entire\nmultivariate series up to p lags with their regression\nparameter given by beta j.",
    "start": "3614190",
    "end": "3623380"
  },
  {
    "text": "And basically the beta\nj regression parameters corresponds to the various\nelements of the phi matrices.",
    "start": "3623380",
    "end": "3636049"
  },
  {
    "text": "So now there's a one-to-one one\ncorrespondence between those. ",
    "start": "3636049",
    "end": "3650800"
  },
  {
    "text": "All right. So I'm using now a notation\nwhere superscript j corresponds",
    "start": "3650800",
    "end": "3659280"
  },
  {
    "text": "to the j-th component\nof the series, of the multivariate\nstochastic process.",
    "start": "3659280",
    "end": "3667760"
  },
  {
    "text": "So we have an mp plus 1 vector\nof regression parameters for each series j, and\nwe have an epsilon j",
    "start": "3667760",
    "end": "3676160"
  },
  {
    "text": "for-- an n-vector of innovation\nerrors for each series.",
    "start": "3676160",
    "end": "3682099"
  },
  {
    "text": "And so basically if this,\nthe j-th column, is y j,",
    "start": "3682100",
    "end": "3691970"
  },
  {
    "text": "we're modeling that to be\nequal to the simple matrix Z times beta j plus epsilon\nj, where this is n by 1.",
    "start": "3691970",
    "end": "3704540"
  },
  {
    "text": "This is n by np plus 1. ",
    "start": "3704540",
    "end": "3711520"
  },
  {
    "text": "And this beta j is the mp\nplus 1 regression parameter. ",
    "start": "3711520",
    "end": "3724845"
  },
  {
    "text": "OK. ",
    "start": "3724845",
    "end": "3730140"
  },
  {
    "text": "One might think,\nOK, one can consider each of these regressions for\neach of the component series,",
    "start": "3730140",
    "end": "3737320"
  },
  {
    "text": "you could consider\nthem separately. But to consider\nthem all together,",
    "start": "3737320",
    "end": "3743940"
  },
  {
    "text": "we can define the\nmultivariate regression model,",
    "start": "3743940",
    "end": "3749270"
  },
  {
    "text": "which has the following form. We basically have the n-vectors\nfor the first component,",
    "start": "3749270",
    "end": "3760276"
  },
  {
    "text": "and then the second component\nup to nth component. So an n by p matrix of\ndependent variables,",
    "start": "3760277",
    "end": "3766730"
  },
  {
    "text": "where each column corresponds\nto a different component series,",
    "start": "3766730",
    "end": "3773540"
  },
  {
    "text": "follows a linear\nregression model with the same Z matrix\nwith different regression",
    "start": "3773540",
    "end": "3779990"
  },
  {
    "text": "coefficient parameters, beta\n1 through beta m corresponding to the different components\nof the multivariate series.",
    "start": "3779990",
    "end": "3788040"
  },
  {
    "text": "And we have epsilon 1,\nepsilon 2, up to epsilon m.",
    "start": "3788040",
    "end": "3794330"
  },
  {
    "text": "So we're thinking of taking--\nso basically the y 1, y 2,",
    "start": "3794330",
    "end": "3800670"
  },
  {
    "text": "up to y m is essentially\nthis original matrix",
    "start": "3800670",
    "end": "3806160"
  },
  {
    "text": "of our multivariate\ntime series because it's the first component\nin the first column",
    "start": "3806160",
    "end": "3815820"
  },
  {
    "text": "and the nth component\nin the nth column. And the-- this\nregression parameter",
    "start": "3815820",
    "end": "3822230"
  },
  {
    "text": "or this explanatory variables\nmatrix X, Z in this case corresponds to lags of the\nwhole process up to p lags.",
    "start": "3822230",
    "end": "3833210"
  },
  {
    "text": "So we're having\nlags of all the-- the m-variate\nprocess up to p lags.",
    "start": "3833210",
    "end": "3838230"
  },
  {
    "text": "So that's mp and then\nplus 1 for our constant. So this is the set up for a\nmultivariate regression model.",
    "start": "3838230",
    "end": "3845790"
  },
  {
    "start": "3845790",
    "end": "3852880"
  },
  {
    "text": "In terms of how\none specifies this, well, actually,\nin economic theory this is also related\nto seemingly unrelated",
    "start": "3852880",
    "end": "3860930"
  },
  {
    "text": "regressions, which you'll\nfind in econometrics. ",
    "start": "3860930",
    "end": "3866730"
  },
  {
    "text": "If we want to specify this\nmultivariate model, well,",
    "start": "3866730",
    "end": "3872850"
  },
  {
    "text": "what we could do is\nwe could actually specify each of the\ncomponent models separately because we\nbasically have sort of-- can",
    "start": "3872850",
    "end": "3881974"
  },
  {
    "text": "think of the\nunivariate regression model for each component series.",
    "start": "3881974",
    "end": "3887010"
  },
  {
    "text": "And this slide\nindicates basically what",
    "start": "3887010",
    "end": "3892580"
  },
  {
    "text": "the formulas are for that. So if we don't know anything\nabout multivariate regression",
    "start": "3892580",
    "end": "3898189"
  },
  {
    "text": "we can say, well,\nlet's start by just doing the univariate regression\nof each component series",
    "start": "3898189",
    "end": "3903619"
  },
  {
    "text": "on the lags. And so we get our beta\nhat j's least squares estimates given by the\nusual formula where",
    "start": "3903620",
    "end": "3910330"
  },
  {
    "text": "the independent variables matrix\nZ goes Z transpose Z inverse Z transpose Y are the residuals.",
    "start": "3910330",
    "end": "3917280"
  },
  {
    "text": "So these are familiar formulas. And if we did this for each\nof the component series j,",
    "start": "3917280",
    "end": "3928680"
  },
  {
    "text": "then we would actually\nget sample estimates",
    "start": "3928680",
    "end": "3933750"
  },
  {
    "text": "of the innovation process,\nthe eta_1, basically the whole eta series.",
    "start": "3933750",
    "end": "3940970"
  },
  {
    "text": "And we could actually\ndefine from these estimates",
    "start": "3940970",
    "end": "3945980"
  },
  {
    "text": "of the innovations\nour covariance matrix for the innovations as\nthe sample covariance",
    "start": "3945980",
    "end": "3952500"
  },
  {
    "text": "matrix of these etas. So all of these formulas\nare-- you're basically",
    "start": "3952500",
    "end": "3958170"
  },
  {
    "text": "applying very\nstraightforward estimation methods for the parameters\nof a linear regression",
    "start": "3958170",
    "end": "3965440"
  },
  {
    "text": "and then estimating\nvariances/covariances of these innovation terms.",
    "start": "3965440",
    "end": "3971440"
  },
  {
    "text": "So from this, we\nactually have estimates of this process in terms of\nthe sigma and the beta hats.",
    "start": "3971440",
    "end": "3980420"
  },
  {
    "text": "But it's made\nassuming that we can treat each of these component\nregressions separately.",
    "start": "3980420",
    "end": "3986755"
  },
  {
    "start": "3986755",
    "end": "3993410"
  },
  {
    "text": "A rather remarkable\nresult is that these component-wise\nregressions are actually",
    "start": "3993410",
    "end": "4000300"
  },
  {
    "text": "the optimal estimates for\nthe multivariate regression as well.",
    "start": "4000300",
    "end": "4006030"
  },
  {
    "text": "And as mathematicians,\nthis kind of result",
    "start": "4006030",
    "end": "4011840"
  },
  {
    "text": "is, I think, rather\nneat and elegant. And maybe some of you will\nthink this is very obvious,",
    "start": "4011840",
    "end": "4018900"
  },
  {
    "text": "but it actually-- it\nisn't quite obvious.",
    "start": "4018900",
    "end": "4025720"
  },
  {
    "text": "That said, this\ncomponent-wise estimation should be optimal as well. And the next section\nof the lecture notes",
    "start": "4025720",
    "end": "4033099"
  },
  {
    "text": "goes through this argument. ",
    "start": "4033100",
    "end": "4040140"
  },
  {
    "text": "And I'm going to, in\nthe interest of time, go through this-- just sort of\nhighlight what the results are.",
    "start": "4040140",
    "end": "4046590"
  },
  {
    "text": "The details are in these\nnotes that you can go through. And I will be happy to go\ninto more detail about them",
    "start": "4046590",
    "end": "4054750"
  },
  {
    "text": "during office hours. But if we're fitting a vector\nautoregression model where",
    "start": "4054750",
    "end": "4061520"
  },
  {
    "text": "there are no constraints\non the coefficient matrices phi_1\nthrough phi_p, then",
    "start": "4061520",
    "end": "4067619"
  },
  {
    "text": "these component-wise\nestimates, accounting",
    "start": "4067620",
    "end": "4072640"
  },
  {
    "text": "for arbitrary covariance matrix\nsigma for the innovations,",
    "start": "4072640",
    "end": "4080706"
  },
  {
    "text": "those basically are equal\nto the generalized least squares estimates of these\nunderlying parameters.",
    "start": "4080707",
    "end": "4086030"
  },
  {
    "text": "You'll recall we talked about\nthe Gauss-Markov theorem where we were able to extend the\nassumption of equal variances",
    "start": "4086030",
    "end": "4095370"
  },
  {
    "text": "across observations to unequal\nvariances and covariances. Well, it turns out to\nthese component-wise OLS",
    "start": "4095370",
    "end": "4103832"
  },
  {
    "text": "estimates are, in fact, the\ngeneralized least squared estimates. And under the assumption\nof Gaussian distributions",
    "start": "4103832",
    "end": "4110160"
  },
  {
    "text": "for the innovations,\nthey, in fact, are maximum\nlikelihood estimates. And this theory applies\nKronecker products.",
    "start": "4110160",
    "end": "4121210"
  },
  {
    "text": "We're not going to have\nany homework with Kronecker products. These notes really\nare for those who",
    "start": "4121210",
    "end": "4127160"
  },
  {
    "text": "have some more extensive\nbackground in linear algebra. But it's a very nice use\nof these Kronecker product",
    "start": "4127160",
    "end": "4135130"
  },
  {
    "text": "operators. Basically, this\nnotation-- I don't know, x circle, I'll\ncall it Kronecker--",
    "start": "4135130",
    "end": "4144540"
  },
  {
    "text": "is one where you take a\nmatrix A and a matrix B and you consider\nthe matrix which",
    "start": "4144540",
    "end": "4151170"
  },
  {
    "text": "takes each element of A times\nthe whole matrix B. So we start",
    "start": "4151170",
    "end": "4156250"
  },
  {
    "text": "with an m by n matrix A and\nend up with an mp by qn matrix",
    "start": "4156250",
    "end": "4162220"
  },
  {
    "text": "by taking each element of\nA times the whole matrix B. So it's, they say, has\nthis block structure.",
    "start": "4162220",
    "end": "4169009"
  },
  {
    "text": "So this is very\nsimple definition. If you look at properties of\ntransposition of matrices,",
    "start": "4169010",
    "end": "4177079"
  },
  {
    "text": "you can prove these results. These are properties of\nthe Kronecker product.",
    "start": "4177080",
    "end": "4182850"
  },
  {
    "text": "And there's a vec operator\nwhich takes a matrix",
    "start": "4182850",
    "end": "4194320"
  },
  {
    "text": "and simply stacks\nthe columns together. And in the talk last Tuesday of\nIvan's, talking about modeling",
    "start": "4194320",
    "end": "4204700"
  },
  {
    "text": "the volatility surface,\nhe basically, he was modeling a two dimensional\nsurface-- or a surface",
    "start": "4204700",
    "end": "4211410"
  },
  {
    "text": "in three dimensions,\nbut there was two dimensions explaining it.",
    "start": "4211410",
    "end": "4216830"
  },
  {
    "text": "You basically can stack\ncolumns of the matrix",
    "start": "4216830",
    "end": "4222140"
  },
  {
    "text": "and be modeling a vector\ninstead of a matrix of values. So the vectorizing operator\nallows us to manipulate terms",
    "start": "4222140",
    "end": "4232130"
  },
  {
    "text": "into a more convenient form. And this multivariate\nregression model",
    "start": "4232130",
    "end": "4239040"
  },
  {
    "text": "is one where it's set up as\nsort of a n by m matrix Y,",
    "start": "4239040",
    "end": "4250950"
  },
  {
    "text": "having that structure. It can be expressed in terms\nof the linear regression form",
    "start": "4250950",
    "end": "4257110"
  },
  {
    "text": "as y star equaling the\nvector, the vec of y.",
    "start": "4257110",
    "end": "4266380"
  },
  {
    "text": "So we basically have y 1, y\n2, down to y m all lined up.",
    "start": "4266380",
    "end": "4275340"
  },
  {
    "text": "So this is pm by 1. ",
    "start": "4275340",
    "end": "4281600"
  },
  {
    "text": "That's going to be equal to\nsome matrix plus the epsilon 1,",
    "start": "4281600",
    "end": "4290055"
  },
  {
    "text": "epsilon 2, down to epsilon n. And then there's\ngoing to be a matrix",
    "start": "4290055",
    "end": "4298850"
  },
  {
    "text": "and a regression\ncoefficient matrix beta 1, beta 2, down to beta p.",
    "start": "4298850",
    "end": "4307360"
  },
  {
    "text": "So we consider vectorizing\nthe beta matrix, vectorizing epsilon,\nand vectorizing y.",
    "start": "4307360",
    "end": "4315340"
  },
  {
    "text": "And then in order\nto define this sort of simple linear regression\nmodel, univariate regression",
    "start": "4315340",
    "end": "4323369"
  },
  {
    "text": "model, well, we need to\nhave a Z in the first column",
    "start": "4323370",
    "end": "4328750"
  },
  {
    "text": "here corresponding to beta 1 for\ny 1, and 0's everywhere else.",
    "start": "4328750",
    "end": "4334800"
  },
  {
    "text": "In the second block\nwe want to have",
    "start": "4334800",
    "end": "4340159"
  },
  {
    "text": "a Z in the second off diagonal\nwith 0's everywhere else and so",
    "start": "4340160",
    "end": "4345920"
  },
  {
    "text": "forth. So this is just re-expressing\neverything in this notation.",
    "start": "4345920",
    "end": "4350960"
  },
  {
    "text": "But the notation is very nice\nbecause, at the end of the day we basically have a regression\nmodel like we had when we were",
    "start": "4350960",
    "end": "4357536"
  },
  {
    "text": "doing our regression analysis. So all the theory we have\nfor specifying these models",
    "start": "4357536",
    "end": "4362929"
  },
  {
    "text": "plays through with\nunivariate regression. And one can go through\nthis technical argument",
    "start": "4362930",
    "end": "4370441"
  },
  {
    "text": "to show that the\ngeneralized least squares estimate is, in\nfact, the equivalent",
    "start": "4370441",
    "end": "4376900"
  },
  {
    "text": "to the component-wise values. And that's very, very good.",
    "start": "4376900",
    "end": "4383980"
  },
  {
    "text": "Maximum likelihood\nestimation with these models. Well, we actually use\nthis vectorized notation",
    "start": "4383980",
    "end": "4392130"
  },
  {
    "text": "to define the\nlikelihood function. And if these\nassumptions are made",
    "start": "4392130",
    "end": "4400559"
  },
  {
    "text": "about the linear\nregression model, we basically have\nan n times m vector",
    "start": "4400560",
    "end": "4408739"
  },
  {
    "text": "of dependent variable values,\nwhereas your multivariate",
    "start": "4408740",
    "end": "4414780"
  },
  {
    "text": "normal with mean given\nby x star beta star and then a covariance\nmatrix epsilon.",
    "start": "4414780",
    "end": "4421870"
  },
  {
    "text": "The covariance matrix of\nepsilon star is sigma star.",
    "start": "4421870",
    "end": "4427380"
  },
  {
    "text": "Well, sigma star is I_n\nKronecker product sigma. So if you go through\nthe math of this,",
    "start": "4427380",
    "end": "4434130"
  },
  {
    "text": "everything matches up in terms\nof what the assumptions are.",
    "start": "4434130",
    "end": "4439250"
  },
  {
    "text": "And the conditional probability\ndensity function of this data",
    "start": "4439250",
    "end": "4445340"
  },
  {
    "text": "is the usual functions\nof log-normal",
    "start": "4445340",
    "end": "4452260"
  },
  {
    "text": "or of a normal sample. So we have unknown\nparameters beta star sigma,",
    "start": "4452260",
    "end": "4460850"
  },
  {
    "text": "which are equal to\nthe joint density",
    "start": "4460850",
    "end": "4466960"
  },
  {
    "text": "of this normal linear\nregression model. So this corresponds to what we\nhad before in our regression",
    "start": "4466960",
    "end": "4474350"
  },
  {
    "text": "analysis. We just had this more\ncomplicated definition of the independent\nvariables matrix X star.",
    "start": "4474350",
    "end": "4480900"
  },
  {
    "text": "And a more\ncomplicated definition of our variance/covariance\nmatrix sigma star.",
    "start": "4480900",
    "end": "4487270"
  },
  {
    "text": "But the log-likelihood\nfunction ends up being equal to a\nterm proportional",
    "start": "4487270",
    "end": "4494390"
  },
  {
    "text": "to the log of the determinant\nof our sigma matrix and minus one half Q of beta\nsigma, where Q of beta sigma",
    "start": "4494390",
    "end": "4503790"
  },
  {
    "text": "is the least squares criterion\nfor each of the component",
    "start": "4503790",
    "end": "4508860"
  },
  {
    "text": "models summed up. So the component-wise\nmaximum likelihood estimation",
    "start": "4508860",
    "end": "4516660"
  },
  {
    "text": "is-- for the\nunderlying parameters, is the same as the large one.",
    "start": "4516660",
    "end": "4523260"
  },
  {
    "text": "And in terms of estimating\nthe covariance matrix,",
    "start": "4523260",
    "end": "4531880"
  },
  {
    "text": "there's a notion called the\nconcentrated log-likelihood,",
    "start": "4531880",
    "end": "4537420"
  },
  {
    "text": "which comes into play in\nmodels with many parameters.",
    "start": "4537420",
    "end": "4545199"
  },
  {
    "text": "In this model, we have\nunknown parameters-- our regression parameters\nbeta and our covariance matrix",
    "start": "4545200",
    "end": "4552230"
  },
  {
    "text": "for the innovations sigma. It turns out that our estimate\nof the regression parameter",
    "start": "4552230",
    "end": "4559850"
  },
  {
    "text": "beta is independent, doesn't\ndepend-- not statistically",
    "start": "4559850",
    "end": "4565190"
  },
  {
    "text": "independent-- but\ndoes not depend on the value of the\ncovariance matrix sigma.",
    "start": "4565190",
    "end": "4570699"
  },
  {
    "text": "So whatever sigma is, we have\nthe same maximum likelihood estimate for the betas. So we can consider\nthe log-likelihood",
    "start": "4570700",
    "end": "4579760"
  },
  {
    "text": "setting the beta parameter\nequal to its maximum likelihood estimate.",
    "start": "4579760",
    "end": "4585410"
  },
  {
    "text": "And then we have a\nfunction that just depends on the data and the\nunknown parameter sigma.",
    "start": "4585410",
    "end": "4591350"
  },
  {
    "text": "So that's a concentrated\nlikelihood function that needs to be maximized. And the maximization of the log\nof a determinant of a matrix",
    "start": "4591350",
    "end": "4600570"
  },
  {
    "text": "minus n over 2 the trace of that\nmatrix times an estimate of it,",
    "start": "4600570",
    "end": "4605690"
  },
  {
    "text": "that has been solved. It's a bit involved. But if you're interested in\nthe mathematics for how that's",
    "start": "4605690",
    "end": "4612760"
  },
  {
    "text": "actually solved and how you\ntake derivatives of determinants and so forth, there's a\npaper by Anderson and Olkin",
    "start": "4612760",
    "end": "4618152"
  },
  {
    "text": "that goes through all\nthe details of that that you can Google on the web. ",
    "start": "4618152",
    "end": "4625910"
  },
  {
    "text": "Finally, let's see. There's-- well, not finally. There's model selection\ncriteria that can be applied.",
    "start": "4625910",
    "end": "4632240"
  },
  {
    "text": "These have been applied\nbefore for regression models for univariate time series\nmodel, the Akaike Information",
    "start": "4632240",
    "end": "4638780"
  },
  {
    "text": "Criterion, the Bayes Information\nCriterion, Hannan-Quinn Criterion.",
    "start": "4638780",
    "end": "4644640"
  },
  {
    "text": "These definitions\nare all consistent with the other definitions.",
    "start": "4644640",
    "end": "4649680"
  },
  {
    "text": "They basically take\nthe likelihood function and you try to maximize that\nplus a penalty for the number",
    "start": "4649680",
    "end": "4657570"
  },
  {
    "text": "of unknown parameters. And that's given here.",
    "start": "4657570",
    "end": "4663380"
  },
  {
    "text": " OK, then the last\nsection goes through an asymptotic distribution\nof least squares estimates.",
    "start": "4663380",
    "end": "4673500"
  },
  {
    "text": "And I'll let you read\nthat on your own. Let's see. For this lecture I put\ntogether an example",
    "start": "4673500",
    "end": "4682470"
  },
  {
    "text": "of fitting vector\nautoregressions with some macroeconomic\nvariables.",
    "start": "4682470",
    "end": "4689330"
  },
  {
    "text": "And I just wanted to\npoint that out to you.",
    "start": "4689330",
    "end": "4695360"
  },
  {
    "text": "So let me go to\nthis document here.",
    "start": "4695360",
    "end": "4703738"
  },
  {
    "text": "What have we got here? ",
    "start": "4703738",
    "end": "4709594"
  },
  {
    "text": "All right. Well, OK. Modeling macroeconomic time\nseries is an important topic.",
    "start": "4709594",
    "end": "4717410"
  },
  {
    "text": "It's what sort of\ncentral bankers do. They want to\nunderstand what factors are affecting the economy in\nterms of growth, inflation,",
    "start": "4717410",
    "end": "4724670"
  },
  {
    "text": "unemployment. And what's the impact of\ninterest rate policies.",
    "start": "4724670",
    "end": "4730600"
  },
  {
    "text": "There are some really\nimportant papers by Robert Litterman and\nChristopher Sims dealing",
    "start": "4730600",
    "end": "4736750"
  },
  {
    "text": "with fitting vector\nautoregression models to a macroeconomic time series. And actually, the\nframework within which",
    "start": "4736750",
    "end": "4743420"
  },
  {
    "text": "they specified these models\nwas a Bayesian framework, which is an extension of the\nmaximum likelihood method where",
    "start": "4743420",
    "end": "4751320"
  },
  {
    "text": "you'll incorporate reasonable\nsort of prior assumptions about what the\nparameters ought to be.",
    "start": "4751320",
    "end": "4758190"
  },
  {
    "text": "But in this note,\nI sort of basically",
    "start": "4758190",
    "end": "4766130"
  },
  {
    "text": "go through collecting various\nmacroeconomic variables directly off the web\nusing the package R.",
    "start": "4766130",
    "end": "4773240"
  },
  {
    "text": "All this stuff\nis-- these are data that you can get your hands on.",
    "start": "4773240",
    "end": "4779040"
  },
  {
    "text": "Here's the unemployment\nrate from January 1946 up through this past month.",
    "start": "4779040",
    "end": "4787030"
  },
  {
    "text": "Anyone can see how that's\nvaried between much less than 4%",
    "start": "4787030",
    "end": "4792670"
  },
  {
    "text": "to over 10%, as it was recently. And there's also\nthe Fed funds rate,",
    "start": "4792670",
    "end": "4799550"
  },
  {
    "text": "which is one of\nthe key variables that the Federal Reserve Open\nMarket Committee controls,",
    "start": "4799550",
    "end": "4806610"
  },
  {
    "text": "or I should say\ncontrolled in the past, to try and affect the economy. Now that value of that\nrate is set almost at zero",
    "start": "4806610",
    "end": "4814719"
  },
  {
    "text": "and other means\nare applied to have an impact on economic growth\nand the economic situation",
    "start": "4814720",
    "end": "4824940"
  },
  {
    "text": "of the market-- of\nthe economy, rather.",
    "start": "4824940",
    "end": "4831340"
  },
  {
    "text": "Let's see. There's also-- anyway, a\nbunch of other variables. CPI, which is a\nmeasure of inflation.",
    "start": "4831340",
    "end": "4838470"
  },
  {
    "text": "What this note goes through\nis the specification",
    "start": "4838470",
    "end": "4845502"
  },
  {
    "text": "of vector autoregression\nmodels for these series.",
    "start": "4845502",
    "end": "4852070"
  },
  {
    "text": "And I use just a\nsmall set of cases. I look at unemployment\nrate, federal funds,",
    "start": "4852070",
    "end": "4858640"
  },
  {
    "text": "and the CPI, which is\na measure of inflation. And there's-- if\none goes through,",
    "start": "4858640",
    "end": "4866580"
  },
  {
    "text": "there are multivariate\nversions of the autocorrelation function, as given on\nthe top right panel here,",
    "start": "4866580",
    "end": "4874670"
  },
  {
    "text": "between these variables. And one can also do the partial\nautocorrelation function.",
    "start": "4874670",
    "end": "4880350"
  },
  {
    "text": "You'll recall that\nautocorrelation functions and partial\nautocorrelation functions are related to what kind of--\nor help us understand what kind",
    "start": "4880350",
    "end": "4889040"
  },
  {
    "text": "of order ARMA processes\nmight be appropriate for univariate series. For multivariate series,\nthen there are basically",
    "start": "4889040",
    "end": "4896750"
  },
  {
    "text": "cross lags between variables\nthat are important, and these can call be captured\nwith vector autoregression",
    "start": "4896750",
    "end": "4902750"
  },
  {
    "text": "models. So this goes through and\nshows how these things",
    "start": "4902750",
    "end": "4907820"
  },
  {
    "text": "are correlated with themselves. And let's see. At the end of this note,\nthere are some impulse",
    "start": "4907820",
    "end": "4919550"
  },
  {
    "text": "response functions\ngraphed, which are looking at what is the\nimpact of an innovation in one",
    "start": "4919550",
    "end": "4927370"
  },
  {
    "text": "of the components of the\nmultivariate time series. So like if Fed funds were to be\nincreased by a certain value,",
    "start": "4927370",
    "end": "4936570"
  },
  {
    "text": "what would the likely impact\nbe on the unemployment rate? Or on GNP?",
    "start": "4936570",
    "end": "4942240"
  },
  {
    "text": "Basically, the production\nlevel of the economy. And this looks at-- let's see.",
    "start": "4942240",
    "end": "4950790"
  },
  {
    "text": "Well, actually\nhere we're looking at the impulse function. You can look at the\nimpulse function",
    "start": "4950790",
    "end": "4956040"
  },
  {
    "text": "of innovations on any of\nthe component variables on all the others. And in this case,\non the left panel",
    "start": "4956040",
    "end": "4962150"
  },
  {
    "text": "here is-- it shows what\nhappens when unemployment",
    "start": "4962150",
    "end": "4967790"
  },
  {
    "text": "has a spike up, or unit spike. A unit impulse up. Well, this second\npanel shows what's",
    "start": "4967790",
    "end": "4975460"
  },
  {
    "text": "likely to happen to\nthe Fed funds rate. It turns out that's\nlikely to go down. And that sort of is\nindicating-- it's sort",
    "start": "4975460",
    "end": "4981670"
  },
  {
    "text": "of reflecting\nwhat, historically, was the policy of the Fed to\nbasically reduce interest rates",
    "start": "4981670",
    "end": "4989180"
  },
  {
    "text": "if unemployment was rising. And then-- so anyway, these\nimpulse response functions",
    "start": "4989180",
    "end": "4996400"
  },
  {
    "text": "correspond to essentially those\ninnovation terms on the Wold decomposition. And why are these important?",
    "start": "4996400",
    "end": "5002360"
  },
  {
    "text": "Well, this indicates a\nconnection, basically, between that sort of moving\naverage representation",
    "start": "5002360",
    "end": "5010260"
  },
  {
    "text": "and these time series models. And the way these\ngraphs are generated",
    "start": "5010260",
    "end": "5015480"
  },
  {
    "text": "is by essentially finding\nthe Wold decomposition and then incorporating\nthat into these values.",
    "start": "5015480",
    "end": "5023880"
  },
  {
    "text": "So-- OK, we'll finish\nthere for today.",
    "start": "5023880",
    "end": "5027540"
  }
]