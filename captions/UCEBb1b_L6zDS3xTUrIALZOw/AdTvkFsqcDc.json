[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu. ",
    "start": "18140",
    "end": "23169"
  },
  {
    "text": "GILBERT STRANG: OK, so kind of\na few things in mind for today. One is to answer those two\nquestions on the second line.",
    "start": "23170",
    "end": "31340"
  },
  {
    "text": " We found those two formulas\non the first line last time,",
    "start": "31340",
    "end": "38080"
  },
  {
    "text": "the derivative of a inverse. So the derivative of A\nsquared ought to be easy.",
    "start": "38080",
    "end": "43460"
  },
  {
    "text": "But if we can't do that,\nwe need to be sure we can. And then this was the\nderivative of an eigenvalue.",
    "start": "43460",
    "end": "51560"
  },
  {
    "text": "And then it's natural to\nask about the derivative of the singular value.",
    "start": "51560",
    "end": "56870"
  },
  {
    "text": "And I had a happy day\nyesterday in the snow, realizing that that\nhas a nice formula too.",
    "start": "56870",
    "end": "63380"
  },
  {
    "text": "Of course, I'm not the first. I'm sure that Wikipedia\nalready knows this formula.",
    "start": "63380",
    "end": "73260"
  },
  {
    "text": "But it was new to me. And I should say Professor\nEdelman has carried it",
    "start": "73260",
    "end": "79460"
  },
  {
    "text": "to the second derivative. Again, not new, but it's\nmore difficult to find",
    "start": "79460",
    "end": "87320"
  },
  {
    "text": "second derivatives,\nand interesting. But we'll just stay\nwith first derivatives.",
    "start": "87320",
    "end": "94740"
  },
  {
    "text": "OK, so that's my\nfirst item of sort of business from last time.",
    "start": "94740",
    "end": "101260"
  },
  {
    "text": "And then I'd like to say\nsomething about the lab homeworks and ask your advice\nand begin to say something",
    "start": "101260",
    "end": "109159"
  },
  {
    "text": "about a project. And then I will move to\nthese topics in Section 4.4",
    "start": "109160",
    "end": "118550"
  },
  {
    "text": "that you have already. And you might notice\nI skipped 4.3.",
    "start": "118550",
    "end": "126860"
  },
  {
    "text": "And the reason is\nthat on Friday, actually arriving\nat MIT tomorrow",
    "start": "126860",
    "end": "133070"
  },
  {
    "text": "is Professor Townsend,\n4.3 is all about his work.",
    "start": "133070",
    "end": "139910"
  },
  {
    "text": "And he's the best\nlecturer I know. He was here as an instructor\nand did 18.06 and was",
    "start": "139910",
    "end": "149390"
  },
  {
    "text": "a big success. Actually, he's also\njust won a prize",
    "start": "149390",
    "end": "156110"
  },
  {
    "text": "for the SIAG/LA, international\nprize for young investigators,",
    "start": "156110",
    "end": "164570"
  },
  {
    "text": "young faculty in\napplied linear algebra. So he goes to Hong Kong\nto get that prize too.",
    "start": "164570",
    "end": "173300"
  },
  {
    "text": "Anyway, he will be on the videos\nand in here in class Friday,",
    "start": "173300",
    "end": "178700"
  },
  {
    "text": "if all goes well. OK, so in order\nthen, the first thing",
    "start": "178700",
    "end": "186110"
  },
  {
    "text": "is the derivative of A squared. And you might think it's\n2A dA dt, but it's not.",
    "start": "186110",
    "end": "196750"
  },
  {
    "text": "And if you realize\nthat it's not, then you realize what it is,\nyou will get these things right",
    "start": "196750",
    "end": "202480"
  },
  {
    "text": "in the future. So the answer to the derivative\nof A squared is not 2A dA dt.",
    "start": "202480",
    "end": "212250"
  },
  {
    "text": " And why isn't it?",
    "start": "212250",
    "end": "217690"
  },
  {
    "text": "And what is the right answer? So I do that maybe\njust below here.",
    "start": "217690",
    "end": "223450"
  },
  {
    "start": "223450",
    "end": "230590"
  },
  {
    "text": "Well, I could ask you to\nguess the right answer, but why don't we do\nit systematically.",
    "start": "230590",
    "end": "235660"
  },
  {
    "text": "So how do you find\nthe derivative? It's a limit.",
    "start": "235660",
    "end": "241180"
  },
  {
    "text": "First you have a delta A, right. And then you take a limit. So I look at A plus delta\nA squared minus A squared.",
    "start": "241180",
    "end": "255700"
  },
  {
    "text": "So that's the\nchange in A squared. And I divide it by delta t.",
    "start": "255700",
    "end": "261819"
  },
  {
    "text": "And then delta t goes to 0. So that's the derivative\nI'm looking for,",
    "start": "261820",
    "end": "266919"
  },
  {
    "text": "the derivative of A squared. And now, if I write that out,\nyou'll see why this is wrong,",
    "start": "266920",
    "end": "274210"
  },
  {
    "text": "but something very close to it,\nof course-- can't be far away-- is right. So what happens if\nI write this out?",
    "start": "274210",
    "end": "281100"
  },
  {
    "text": "The A squared will\ncancel the A squared. What will I have? Will I have 2A delta A?",
    "start": "281100",
    "end": "289540"
  },
  {
    "text": "Why don't I write\n2A delta A next? ",
    "start": "289540",
    "end": "295900"
  },
  {
    "text": "Because when you're squaring\na sum of two matrices,",
    "start": "295900",
    "end": "301090"
  },
  {
    "text": "one term is A delta A, and\nanother term is delta A A.",
    "start": "301090",
    "end": "311180"
  },
  {
    "text": "And those are\ndifferent in general. And then plus delta A squared.",
    "start": "311180",
    "end": "320210"
  },
  {
    "text": "And now I divide\nit all by delta t. So you're now seeing my point\nthat now I let delta t go to 0.",
    "start": "320210",
    "end": "331550"
  },
  {
    "text": "So I'm just doing\nmatrix calculus. And it's not altogether simple,\nbut if you follow the rules,",
    "start": "331550",
    "end": "340490"
  },
  {
    "text": "it comes out right. So now what answer do I\nget as delta t goes to 0?",
    "start": "340490",
    "end": "348770"
  },
  {
    "text": "I get A dA dt-- that's the definition of the--",
    "start": "348770",
    "end": "356240"
  },
  {
    "text": "that ratio goes to dA dt. That's the whole idea\nof the derivative of A.",
    "start": "356240",
    "end": "361460"
  },
  {
    "text": "And now what's the other term? It's dA dt A. So it\nwas simply that point",
    "start": "361460",
    "end": "373190"
  },
  {
    "text": "that I wanted you to pick up on,\nthat the derivative might not",
    "start": "373190",
    "end": "380510"
  },
  {
    "text": "commute with A. Matrices\ndon't commute in general. And so you'll notice that we\nhad a similar expression there.",
    "start": "380510",
    "end": "391414"
  },
  {
    "text": " We had to pay attention to\nthe order of things there.",
    "start": "391415",
    "end": "398480"
  },
  {
    "text": "And now we get it right. It's not this, but A\ndA dt plus dA dt A. OK.",
    "start": "398480",
    "end": "411810"
  },
  {
    "text": "Good. Now, can I do the other one? Which is a little more serious,\nbut it's a beautiful formula.",
    "start": "411810",
    "end": "421199"
  },
  {
    "text": "And it's parallel to this guy. You might even guess it.",
    "start": "421200",
    "end": "427050"
  },
  {
    "text": "So I'm looking for the\nderivative of a singular value. The matrix A is changing.",
    "start": "427050",
    "end": "432780"
  },
  {
    "text": "dA dt tells me how it's changing\nat the moment, at the instant.",
    "start": "432780",
    "end": "437830"
  },
  {
    "text": "And I want to know how is sigma\nchanging at that same instant. And sort of in parallel\nwith this is a nice--",
    "start": "437830",
    "end": "446699"
  },
  {
    "text": "the nice formula-- u transpose dA dt v of t.",
    "start": "446700",
    "end": "456150"
  },
  {
    "text": "Boy, you couldn't ask for a\nnicer formula than that, right? ",
    "start": "456150",
    "end": "463310"
  },
  {
    "text": "You remember this\nis the eigenvector. And that's the eigenvector\nof A transpose.",
    "start": "463310",
    "end": "470050"
  },
  {
    "text": "So this is the\nsingular vector of A. And you could say this is a\nsingular vector of A transpose,",
    "start": "470050",
    "end": "476280"
  },
  {
    "text": "or it's the left singular vector\nof A. So that's our formula.",
    "start": "476280",
    "end": "484470"
  },
  {
    "text": "And if we can just\nrecall how to prove it, which is going to be parallel\nto the proof of that one,",
    "start": "484470",
    "end": "490259"
  },
  {
    "text": "then I'm a happy person and\nwe can get on with life. So let's remember this,\nbecause it will help us",
    "start": "490260",
    "end": "500340"
  },
  {
    "text": "to remember the other one, too. OK, so where do I start?",
    "start": "500340",
    "end": "505620"
  },
  {
    "text": "I start with a\nformula for sigma. So I believe that sigma is\nu transpose times A times",
    "start": "505620",
    "end": "515340"
  },
  {
    "text": "v. Everybody agree with that?",
    "start": "515340",
    "end": "521780"
  },
  {
    "text": "Everything's depending\non t in this formula. As time changes,\neverything changes.",
    "start": "521780",
    "end": "528890"
  },
  {
    "text": "But I didn't write\nin the parentheses, t three more times.",
    "start": "528890",
    "end": "536390"
  },
  {
    "text": "Can we just remember\nabout the SVD. The SVD says that\nA times v equals--",
    "start": "536390",
    "end": "544756"
  },
  {
    "text": "AUDIENCE: Sigma u. GILBERT STRANG: Sigma u. Thanks. Av is sigma u. That's the SVD.",
    "start": "544756",
    "end": "550410"
  },
  {
    "text": " So when I put in for\nAv, I put in sigma u.",
    "start": "550410",
    "end": "558380"
  },
  {
    "text": "Sigma is just a number. So I bring it outside. And I'm left with u transpose u.",
    "start": "558380",
    "end": "564110"
  },
  {
    "text": "And what's u transpose u? 1. So I've used these two facts.",
    "start": "564110",
    "end": "570160"
  },
  {
    "text": " Or I could have\ngone the other way",
    "start": "570160",
    "end": "575889"
  },
  {
    "text": "and said that this\nis the transpose of-- this is A transpose u transpose.",
    "start": "575890",
    "end": "583060"
  },
  {
    "text": "I could look at it\nthat way times v.",
    "start": "583060",
    "end": "589060"
  },
  {
    "text": "And if I look at\nit that way, I'm interested in what\nis A transpose u. And what is A transpose u?",
    "start": "589060",
    "end": "597370"
  },
  {
    "text": "It's sigma v. And it's\ntranspose, so sigma v",
    "start": "597370",
    "end": "604900"
  },
  {
    "text": "transpose v. And what is sigma v transpose v? AUDIENCE: Sigma.",
    "start": "604900",
    "end": "610310"
  },
  {
    "text": "GILBERT STRANG: It's\nsigma again, of course. Got sigma both ways. OK.",
    "start": "610310",
    "end": "615520"
  },
  {
    "text": "Now, I'm ready to\ntake the derivative. That's the formula\nI have for sigma,",
    "start": "615520",
    "end": "623680"
  },
  {
    "text": "completely parallel\nto the formula that we started out\nwith for lambda. The eigenvalue was\ny transpose Ax.",
    "start": "623680",
    "end": "631899"
  },
  {
    "text": "And now we've got\nu transpose Av. And, by the way, when\nwould those two formulas",
    "start": "631900",
    "end": "638170"
  },
  {
    "text": "be one and the same? When does the SVD just\ntell us nothing new",
    "start": "638170",
    "end": "645060"
  },
  {
    "text": "beyond the eigenvalue stuff for\nwhat matrices are the singular",
    "start": "645060",
    "end": "650520"
  },
  {
    "text": "values, the same as the\neigenvalues, and singular vectors the same as this\nas the eigenvectors for--",
    "start": "650520",
    "end": "657870"
  },
  {
    "text": "For? AUDIENCE: Symmetric. GILBERT STRANG: Symmetric, good. Symmetric, square,\nand-- the two words",
    "start": "657870",
    "end": "668490"
  },
  {
    "text": "that I'm always looking\nfor in this course. If you want an A in\nthis course, just",
    "start": "668490",
    "end": "673560"
  },
  {
    "text": "write down positive definite\nin the answer to any question, because sigmas are by\ndefinition positive.",
    "start": "673560",
    "end": "681509"
  },
  {
    "text": "And if they're going to agree\ntotally with the lambdas, then the lambdas\nhave to be positive. Or could be 0, so positive\nsemidefinite definite",
    "start": "681510",
    "end": "690237"
  },
  {
    "text": "would be the right answer. Anyway, this is our start. ",
    "start": "690237",
    "end": "696170"
  },
  {
    "text": "And what do we do\nwith that formula? So this was all the same,\nbecause v transpose v was 1.",
    "start": "696170",
    "end": "702340"
  },
  {
    "text": " Here I had v transpose\nv. And that's 1.",
    "start": "702340",
    "end": "708000"
  },
  {
    "text": "So it gave me sigma. Yeah, good. Everybody's with us. OK, what do I do?",
    "start": "708000",
    "end": "713580"
  },
  {
    "text": "Take the derivative. Takes the derivative of\nthat equation in the box. It's exactly what\nI did last time",
    "start": "713580",
    "end": "720569"
  },
  {
    "text": "with the corresponding\nequation for lambda. Same thing. And I'm going to get again--",
    "start": "720570",
    "end": "727140"
  },
  {
    "text": "it's a product rule, because\nI have three things multiplied on the right-hand side.",
    "start": "727140",
    "end": "732760"
  },
  {
    "text": "So I've got three terms\nfrom the product rule. So d sigma dt,\ncoming from the box,",
    "start": "732760",
    "end": "741780"
  },
  {
    "text": "is du transpose dt Av\nplus u transpose dA dt v",
    "start": "741780",
    "end": "755430"
  },
  {
    "text": "plus the third guy, which\nwill be u transpose A dv dt.",
    "start": "755430",
    "end": "761370"
  },
  {
    "text": " Did I get the three terms there? Yep.",
    "start": "761370",
    "end": "768200"
  },
  {
    "text": "And which term do I want? Which term do I believe is going\nto survive and be the answer?",
    "start": "768200",
    "end": "774300"
  },
  {
    "text": " Well, this is what I'm after.",
    "start": "774300",
    "end": "779620"
  },
  {
    "text": "So it's the middle term. The middle term is just right. ",
    "start": "779620",
    "end": "786319"
  },
  {
    "text": "And the other two terms\nhad better be zero. So that will be the proof.",
    "start": "786320",
    "end": "792200"
  },
  {
    "text": "The other two\nterms will be zero. So can we just take\none of those two terms",
    "start": "792200",
    "end": "797839"
  },
  {
    "text": "and show that it's\nzero like this one? OK, what have I got here?",
    "start": "797840",
    "end": "804139"
  },
  {
    "text": "I want to know that\nthat term is 0. So what have I got. I've got du transpose\ndt times Av.",
    "start": "804140",
    "end": "817370"
  },
  {
    "text": "And everybody says, OK, in\nplace of Av, write in sigma u.",
    "start": "817370",
    "end": "823200"
  },
  {
    "text": "And sigma's a number, so I\ndon't mind putting it there. So I've got sigma, a number of\ntimes the derivative of u times",
    "start": "823200",
    "end": "833310"
  },
  {
    "text": "u itself, the dot product-- the derivative of u\nwith dot product with u.",
    "start": "833310",
    "end": "838410"
  },
  {
    "text": "And that equals? 0, I hope, because of this.",
    "start": "838410",
    "end": "845279"
  },
  {
    "text": " Because of that. ",
    "start": "845280",
    "end": "852240"
  },
  {
    "text": "This comes from the\nderivative of that. ",
    "start": "852240",
    "end": "857579"
  },
  {
    "text": "But you see, now we've got\ndot products, ordinary dot",
    "start": "857580",
    "end": "863210"
  },
  {
    "text": "products, and a number\non the right-hand side. We're in dimension\n1, you could say.",
    "start": "863210",
    "end": "869750"
  },
  {
    "text": "So this tells me immediately\nthat the derivative of u with u plus u transpose\ntimes the derivative of u",
    "start": "869750",
    "end": "884870"
  },
  {
    "text": "is the derivative\nof 1, which is 0.",
    "start": "884870",
    "end": "891680"
  },
  {
    "text": "All I'm saying is that\nthese are the same. You know, vectors, x transpose\ny is the same as y transpose",
    "start": "891680",
    "end": "901535"
  },
  {
    "text": "x when I'm talking\nabout real numbers. If I was doing complex\nthings, which I could do,",
    "start": "901535",
    "end": "908030"
  },
  {
    "text": "then I'd have to pay attention\nand take complex conjugates",
    "start": "908030",
    "end": "916070"
  },
  {
    "text": "at the right moment. But let's not bother. So you see, this is\njust two of these.",
    "start": "916070",
    "end": "923600"
  },
  {
    "text": "And it gives me 0. So that term's gone. ",
    "start": "923600",
    "end": "930690"
  },
  {
    "text": "And similarly, totally\nsimilarly, this term is gone. This is A transpose\nu, all transpose.",
    "start": "930690",
    "end": "940819"
  },
  {
    "text": "I'm just doing the\nsame thing times dv dt. And what is A transpose u?",
    "start": "940820",
    "end": "948300"
  },
  {
    "text": "It's sigma v. So this is\nsigma v transpose dv dt.",
    "start": "948300",
    "end": "955580"
  },
  {
    "text": "And again 0, because of this. ",
    "start": "955580",
    "end": "962630"
  },
  {
    "text": "So in a way this was a\nslightly easier thing-- the last time was completely\nparallel computation.",
    "start": "962630",
    "end": "972830"
  },
  {
    "text": "But the first and third terms\nhad to cancel each other with the x's and y's.",
    "start": "972830",
    "end": "979839"
  },
  {
    "text": "Now, they disappear separately,\nleaving the right answer.",
    "start": "979840",
    "end": "989690"
  },
  {
    "text": "You might think, how did\nwe get into derivatives of singular values?",
    "start": "989690",
    "end": "995470"
  },
  {
    "text": "Well, I think if we're\ngoing to understand the SVD, then the first derivative\nof the sigma is--",
    "start": "995470",
    "end": "1005040"
  },
  {
    "text": "well, except that I've\nsurvived all these years without knowing it. So you could say it's not--",
    "start": "1005040",
    "end": "1010440"
  },
  {
    "text": " you can live without it, but\nit's a pretty nice formula.",
    "start": "1010440",
    "end": "1018330"
  },
  {
    "text": "OK, that completes\nthat Section 3.1.",
    "start": "1018330",
    "end": "1025780"
  },
  {
    "text": "And more to say about 3.2,\nwhich was the interlacing part that I introduced.",
    "start": "1025780",
    "end": "1031959"
  },
  {
    "text": "OK, so where am I? I guess I'm thinking about the\nneat topics about interlacing",
    "start": "1031960",
    "end": "1046220"
  },
  {
    "text": "of eigenvalues. So may I pick up on that theme,\ninterlacing of eigenvalues",
    "start": "1046220",
    "end": "1053810"
  },
  {
    "text": "and say what's in the notes\nand what's the general idea?",
    "start": "1053810",
    "end": "1059730"
  },
  {
    "text": "OK.  So we're leaving the\nderivatives and moving",
    "start": "1059730",
    "end": "1068480"
  },
  {
    "text": "to finite changes in the\neigenvalues and singular",
    "start": "1068480",
    "end": "1074570"
  },
  {
    "text": "values, and we are\nrecognizing that we can't get exact\nformulas for the change,",
    "start": "1074570",
    "end": "1082730"
  },
  {
    "text": "but we can get\nbounds for change. And they are pretty cool.",
    "start": "1082730",
    "end": "1087990"
  },
  {
    "text": "So let me remind you what\nthat is, what they are. So I have a matrix--",
    "start": "1087990",
    "end": "1095260"
  },
  {
    "text": "let's see, a symmetric\nmatrix S that has eigenvalues lambda 1,\ngreater equal lambda 2,",
    "start": "1095260",
    "end": "1102080"
  },
  {
    "text": "greater equal so on. Then I change S by some amount.",
    "start": "1102080",
    "end": "1108680"
  },
  {
    "text": "I think in the notes there is\na number, theta times 1 matrix.",
    "start": "1108680",
    "end": "1115520"
  },
  {
    "text": "That has eigenvalues mu\n1, greater equal mu 2, greater equal something.",
    "start": "1115520",
    "end": "1123529"
  },
  {
    "text": "And these are what I can't\ngive you an exact formula for.",
    "start": "1123530",
    "end": "1129610"
  },
  {
    "text": "You just would have\nto compute them. But I can give you\nbounds for them.",
    "start": "1129610",
    "end": "1137410"
  },
  {
    "text": "And the bounds come\nfrom the lambdas.  So this was a positive.",
    "start": "1137410",
    "end": "1144100"
  },
  {
    "text": "This is a positive change. ",
    "start": "1144100",
    "end": "1149590"
  },
  {
    "text": "So the eigenvalues will\ngo up, or stay still, but they won't go down.",
    "start": "1149590",
    "end": "1156760"
  },
  {
    "text": "So the mu's will be\nbigger than the lambdas. But the neat thing is that mu\n2 will not pass up lambda 1.",
    "start": "1156760",
    "end": "1167130"
  },
  {
    "text": "So here is the interlacing. Mu 1 is greater equal lambda 1. That says that the highest\neigenvalue, the top eigenvalue",
    "start": "1167130",
    "end": "1175350"
  },
  {
    "text": "went up, or didn't move. But mu 2 is below lambda 1.",
    "start": "1175350",
    "end": "1184639"
  },
  {
    "text": "This is the new--\neverybody's with me here? This is a new, and\nthis is the old.",
    "start": "1184640",
    "end": "1190210"
  },
  {
    "text": "New and old being old is S,\nnew is with the change in S.",
    "start": "1190210",
    "end": "1196510"
  },
  {
    "text": "And that mu 2 is\ngreater equal lambda 2.",
    "start": "1196510",
    "end": "1201540"
  },
  {
    "text": "So the second\neigenvalues went up. And then so on. ",
    "start": "1201540",
    "end": "1210909"
  },
  {
    "text": "That's a great fact. And I guess that I sent\nout a puzzle question.",
    "start": "1210910",
    "end": "1216810"
  },
  {
    "text": "Did it arrive in email? ",
    "start": "1216810",
    "end": "1225100"
  },
  {
    "text": "Did anybody see that puzzle\nquestion and think about it? It worried me for a while.",
    "start": "1225100",
    "end": "1231059"
  },
  {
    "start": "1231060",
    "end": "1236480"
  },
  {
    "text": "Suppose this is the\nsecond eigenvalue value--",
    "start": "1236480",
    "end": "1241820"
  },
  {
    "text": "eigenvector. So I'm adding on, I'm hyping\nup the second eigenvector,",
    "start": "1241820",
    "end": "1250520"
  },
  {
    "text": "hyping up the matrix\nin the direction of the second eigenvector. ",
    "start": "1250520",
    "end": "1257890"
  },
  {
    "text": "So the second\neigenvalue was lambda 2. And its mu 2, the new\nsecond eigenvalue,",
    "start": "1257890",
    "end": "1265280"
  },
  {
    "text": "is going to be bigger by theta. ",
    "start": "1265280",
    "end": "1271190"
  },
  {
    "text": "But then I lost a little\nsleep in thinking, OK, if the second eigenvalue\nis mu 2 plus theta--",
    "start": "1271190",
    "end": "1280130"
  },
  {
    "text": "sorry, if the second\neigenvalue mu 2-- so let me write it here. If mu 2, the second eigenvalue,\nis the old lambda 2 plus theta",
    "start": "1280130",
    "end": "1295460"
  },
  {
    "text": "then bad news, because theta\ncan be as big as I want.",
    "start": "1295460",
    "end": "1305390"
  },
  {
    "text": "It can be 20, 200, 2,000. And if I'm just adding theta\nto lambda 2 to get the second--",
    "start": "1305390",
    "end": "1317300"
  },
  {
    "text": "because it's a second\neigenvector that's getting pumped up, then after a\nwhile, mu 2 will pass lambda 1.",
    "start": "1317300",
    "end": "1330139"
  },
  {
    "text": "This will be totally true. I have no worries about this. The old lambda 1-- actually, the old--",
    "start": "1330140",
    "end": "1338150"
  },
  {
    "text": "I'll even have\nequality here, because for this particular change,\nit's not affecting lambda 1.",
    "start": "1338150",
    "end": "1347600"
  },
  {
    "text": "So I think mu 1\nwould be lambda 1 in my hypothetical possibility.",
    "start": "1347600",
    "end": "1354080"
  },
  {
    "text": "What I'm trying to\nget you to do is to think through what this\nmeans, because it's quite",
    "start": "1354080",
    "end": "1359210"
  },
  {
    "text": "easy to write that line there. But then when you think about\nit, you get some questions.",
    "start": "1359210",
    "end": "1366950"
  },
  {
    "text": "And it looks as\nif it might fail, because if theta is really\nbig, that mu 2 would pass up",
    "start": "1366950",
    "end": "1377110"
  },
  {
    "text": "lambda 1. And the thing would fail. And there has to be a catch.",
    "start": "1377110",
    "end": "1382570"
  },
  {
    "text": "There has to be a catch. So does anybody-- you\nsaw that in the email.",
    "start": "1382570",
    "end": "1391539"
  },
  {
    "text": "And I'll now explain\nwhat how I understood that everything can work and I'm\nnot reaching a contradiction.",
    "start": "1391540",
    "end": "1404650"
  },
  {
    "text": "And here's my thinking. So it's perfectly true that the\neigenvalue that goes with u2--",
    "start": "1404650",
    "end": "1412809"
  },
  {
    "text": "or maybe I should be calling\nthem x2, because usually I call the eigenvectors x2--",
    "start": "1412810",
    "end": "1418750"
  },
  {
    "text": "it's perfectly true that mu\n2, that that one goes up. ",
    "start": "1418750",
    "end": "1426019"
  },
  {
    "text": "But what happens when\nit reaches lambda 1?",
    "start": "1426020",
    "end": "1431900"
  },
  {
    "text": "Actually, lambda 1,\nthe first eigenvalue, is staying put, because it's\nnot getting any push from this.",
    "start": "1431900",
    "end": "1437930"
  },
  {
    "text": "But the second eigenvalue is\ngetting a push of size theta. So what happens when lambda\n2 plus theta, which is mu 2--",
    "start": "1437930",
    "end": "1447289"
  },
  {
    "text": "mu 2 is lambda 2 plus theta-- what happens when it\ncomes up to lambda 1",
    "start": "1447290",
    "end": "1452720"
  },
  {
    "text": "and I start worrying\nthat it passes lambda 1? ",
    "start": "1452720",
    "end": "1458410"
  },
  {
    "text": "Do you see what's\nhappening there? What happens when mu 2 passes--",
    "start": "1458410",
    "end": "1465250"
  },
  {
    "text": "when mu 2, which is-- I'm just going to copy here-- it's the old lambda 2 plus\nthe theta, the number.",
    "start": "1465250",
    "end": "1471875"
  },
  {
    "text": "What happens when theta gets\nbigger and bigger and bigger and this hits this thing\nand then goes beyond?",
    "start": "1471875",
    "end": "1477670"
  },
  {
    "text": "Just to see the logic here. What happens is that this lambda\n2 plus theta, which was mu 2,",
    "start": "1477670",
    "end": "1486760"
  },
  {
    "text": "mu 2 until they got here. But what is lambda 2 plus\ntheta after it passes lambda 1?",
    "start": "1486760",
    "end": "1495570"
  },
  {
    "text": "It's lambda 1 now.  It passed up, so it's\nthe top eigenvalue",
    "start": "1495570",
    "end": "1502190"
  },
  {
    "text": "of the altered matrix.",
    "start": "1502190",
    "end": "1507389"
  },
  {
    "text": "And therefore, it's just fine. It's out here. No problem.",
    "start": "1507390",
    "end": "1513740"
  },
  {
    "text": "Maybe I'll just say it again. When theta is big\nenough that mu 2 reaches",
    "start": "1513740",
    "end": "1520010"
  },
  {
    "text": "lambda 1, if I increase\ntheta beyond that, then this becomes not\nmu 2 any more, but mu 1.",
    "start": "1520010",
    "end": "1530060"
  },
  {
    "text": "And then totally\neverybody's happy.",
    "start": "1530060",
    "end": "1535130"
  },
  {
    "text": "I won't say more on that,\nbecause that's just like a way",
    "start": "1535130",
    "end": "1540260"
  },
  {
    "text": "that I found to make me think,\nwhat do these things mean? OK, enough said on\nthat small point.",
    "start": "1540260",
    "end": "1548070"
  },
  {
    "text": "But then the main point\nis, why is this true? This interlacing, which is\nreally a nice, beautiful fact.",
    "start": "1548070",
    "end": "1559240"
  },
  {
    "text": "And you could\nimagine that we have",
    "start": "1559240",
    "end": "1565500"
  },
  {
    "text": "more different perturbations\nthan just rank 1s. ",
    "start": "1565500",
    "end": "1573299"
  },
  {
    "text": "So let me tell you the\ninequality, so named",
    "start": "1573300",
    "end": "1579750"
  },
  {
    "text": "after the discoverer,\nWeyl's inequality. ",
    "start": "1579750",
    "end": "1587790"
  },
  {
    "text": "So his inequality is for\nthe eigenvalues of S plus T.",
    "start": "1587790",
    "end": "1599400"
  },
  {
    "text": "So T is the change. S is where I start. It has eigenvalues lambda.",
    "start": "1599400",
    "end": "1605340"
  },
  {
    "text": "But now, I'm looking at the\neigenvalues of S plus T. So I'm making a change.",
    "start": "1605340",
    "end": "1610860"
  },
  {
    "text": "Over here, in my\nlittle puzzle question, that was T. It was\na rank 1 change.",
    "start": "1610860",
    "end": "1616710"
  },
  {
    "text": "Now I will allow other ranks. So I want to estimate\nlambdas of S plus t",
    "start": "1616710",
    "end": "1623429"
  },
  {
    "text": "in terms of lambdas\nof S and lambdas of T.",
    "start": "1623430",
    "end": "1630880"
  },
  {
    "text": "And I want some\ninequality sign there. ",
    "start": "1630880",
    "end": "1637000"
  },
  {
    "text": "And it's supposed to be true\nfor any symmetric matrices, symmetric S and T.",
    "start": "1637000",
    "end": "1646800"
  },
  {
    "text": "And then a totally\nidentical Weyl inequality--",
    "start": "1646800",
    "end": "1652360"
  },
  {
    "text": "actually, Weyl was\none of the people who discovered singular values. And when he did it, he\nasked about his inequality.",
    "start": "1652360",
    "end": "1659350"
  },
  {
    "text": "And he found that it\nstill worked the way we've found this morning earlier. ",
    "start": "1659350",
    "end": "1667210"
  },
  {
    "text": "I haven't completed\nthat yet, because I haven't told you which\nlambdas I'm talking about.",
    "start": "1667210",
    "end": "1674790"
  },
  {
    "text": "So let me do that. So now, I'll tell you\nWeyl's inequality.",
    "start": "1674790",
    "end": "1681050"
  },
  {
    "text": "So S and T are symmetric. And so the lambdas are real. And we want to know--",
    "start": "1681050",
    "end": "1687670"
  },
  {
    "text": "we want to get them in order. OK, so here it goes. ",
    "start": "1687670",
    "end": "1695170"
  },
  {
    "text": "Weyl allowed the i-th eigenvalue\nof S and the j-th eigenvalue",
    "start": "1695170",
    "end": "1701460"
  },
  {
    "text": "of T and figured out that this\nwas bounded by that eigenvalue",
    "start": "1701460",
    "end": "1707850"
  },
  {
    "text": "of S plus T. So that's\nWeyl's great inequality, which reduces to the\none I wrote here,",
    "start": "1707850",
    "end": "1722730"
  },
  {
    "text": "if I make the right choice-- yeah, probably, if\nI take j equal to 1.",
    "start": "1722730",
    "end": "1727940"
  },
  {
    "text": "So you see the beauty of this. It tells you about\nany eigenvalues of S,",
    "start": "1727940",
    "end": "1736559"
  },
  {
    "text": "eigenvalues of T. So I'm using lambdas here. Lambda of S are the\neigenvalues of S.",
    "start": "1736560",
    "end": "1742950"
  },
  {
    "text": "I'm using lambda again for T\nand lambda again for S plus T. So you have to pay attention\nto which matrix I'm",
    "start": "1742950",
    "end": "1751830"
  },
  {
    "text": "taking the eigenvalues out of. So let me take j equal to 1.",
    "start": "1751830",
    "end": "1757269"
  },
  {
    "text": "And this says that\nlambda i, because j is 1, S plus T is less or equal to\nlambda i of S plus lambda 1,",
    "start": "1757270",
    "end": "1768210"
  },
  {
    "text": "the top eigenvalue of T.\nThis is lambda max of T.",
    "start": "1768210",
    "end": "1780169"
  },
  {
    "text": "Do you see that that's totally\nreasonable, believable? That the eigenvalue\nwhen I add on T-- let's",
    "start": "1780170",
    "end": "1789260"
  },
  {
    "text": "imagine in our minds\nthat T is positive. T is like this thing.",
    "start": "1789260",
    "end": "1796640"
  },
  {
    "text": "This could be the T, example of\na T. It's what I'm adding on.",
    "start": "1796640",
    "end": "1802280"
  },
  {
    "text": "Then the eigenvalues go up. But they don't pass that.",
    "start": "1802280",
    "end": "1809870"
  },
  {
    "text": "So that tells you how\nmuch it could go up by. So I guess that Weyl is giving\nus a less than or equal here.",
    "start": "1809870",
    "end": "1819070"
  },
  {
    "text": "Less or equal to lambda 1-- so I'm taking i to be 1--",
    "start": "1819070",
    "end": "1824450"
  },
  {
    "text": "plus theta. Yeah, so that any equality\nI've written down there--",
    "start": "1824450",
    "end": "1832590"
  },
  {
    "text": "there's some playing around\nto do to get practice. And it's not so essential for\nus to be like world grandmasters",
    "start": "1832590",
    "end": "1844310"
  },
  {
    "text": "at this thing, but\nyou should see it. And you should also\nsee j equal to 2.",
    "start": "1844310",
    "end": "1852010"
  },
  {
    "text": "Why will j equal to\n2 tell us something? I hope it will.",
    "start": "1852010",
    "end": "1858120"
  },
  {
    "text": "Let's see what it tells us. Lambda i plus 1 now-- j is 2--",
    "start": "1858120",
    "end": "1864720"
  },
  {
    "text": "of S plus T. So it's less\nthan or equal to lambda i of S",
    "start": "1864720",
    "end": "1872640"
  },
  {
    "text": "plus lambda 2 of T. I\nthink that's interesting.",
    "start": "1872640",
    "end": "1879480"
  },
  {
    "text": "And also, I think I also could\nget lambda i plus i minus 1.",
    "start": "1879480",
    "end": "1894280"
  },
  {
    "text": "Let me write it and\nsee if it's correct. Plus lambda i minus 1.",
    "start": "1894280",
    "end": "1900755"
  },
  {
    "text": "So those was add up to i plus 2. Yeah, I guess lambda i\nplus 1 plus lambda 1 of T.",
    "start": "1900755",
    "end": "1911120"
  },
  {
    "text": "That's what I got by taking-- yeah, did I do that right?",
    "start": "1911120",
    "end": "1917360"
  },
  {
    "text": " I'm taking j equal to 1.",
    "start": "1917360",
    "end": "1923695"
  },
  {
    "text": "No, well, I don't\nthink I got it right. ",
    "start": "1923696",
    "end": "1930520"
  },
  {
    "text": "What do I want to do here to\nget a bound on lambda i plus 1? I want to take j equal to 2.",
    "start": "1930520",
    "end": "1936610"
  },
  {
    "text": "I should just be sensible\nand plug in j equal to 2",
    "start": "1936610",
    "end": "1943210"
  },
  {
    "text": "and i equal to 1. ",
    "start": "1943210",
    "end": "1949510"
  },
  {
    "text": "All I want to say is that Weyl's\ninequality is the great fact",
    "start": "1949510",
    "end": "1955480"
  },
  {
    "text": "out of which all this\ninterlacing falls and more and more, because\nthe interlacing is telling me",
    "start": "1955480",
    "end": "1962650"
  },
  {
    "text": "about neighbors. And actually if I use Weyl for i\nand j, different i's and j's, I",
    "start": "1962650",
    "end": "1970790"
  },
  {
    "text": "even learn about ones\nthat are not neighbors.",
    "start": "1970790",
    "end": "1976166"
  },
  {
    "text": "And I could tell you a\nproof of Weyl's inequality. But I'll save that\nfor the notes.",
    "start": "1976166",
    "end": "1982340"
  },
  {
    "text": " So I think maybe\nthat's what I want",
    "start": "1982340",
    "end": "1989100"
  },
  {
    "text": "to do about interfacing, just\nto say what the notes have,",
    "start": "1989100",
    "end": "1994240"
  },
  {
    "text": "but not repeat it all in class. So the notes have actually two\nways to prove this interlacing.",
    "start": "1994240",
    "end": "2004230"
  },
  {
    "text": "The standard way that every\nmathematician would use would be Weyl's inequality.",
    "start": "2004230",
    "end": "2010990"
  },
  {
    "text": "But last year,\nProfessor Rao, visiting,",
    "start": "2010990",
    "end": "2016600"
  },
  {
    "text": "found a nice argument\nthat's also in the notes.",
    "start": "2016600",
    "end": "2022059"
  },
  {
    "text": "It ends up with a graph. And on that graph, you\ncan see that this is true.",
    "start": "2022060",
    "end": "2027539"
  },
  {
    "text": "So for what it's worth, two\napproaches to this interlacing",
    "start": "2027540",
    "end": "2035530"
  },
  {
    "text": "and some examples. But I really don't\nwant to spend our lives",
    "start": "2035530",
    "end": "2041710"
  },
  {
    "text": "on this eigenvalue topic. It's a beautiful fact\nabout symmetric matrices",
    "start": "2041710",
    "end": "2048940"
  },
  {
    "text": "and the corresponding fact\nis true for singular values of any matrix, but let's\nthink of leaving it there.",
    "start": "2048940",
    "end": "2058270"
  },
  {
    "text": " So now, I'm moving on\nto the new section.",
    "start": "2058270",
    "end": "2068669"
  },
  {
    "text": "The new section\ninvolves something called compressed sensing. I don't know if you've\nheard those words. ",
    "start": "2068670",
    "end": "2085949"
  },
  {
    "text": "So these are all topics in\nSection 4.4, which you have.",
    "start": "2085949",
    "end": "2092699"
  },
  {
    "text": "I think we sent it out\n10 days ago probably. ",
    "start": "2092699",
    "end": "2098660"
  },
  {
    "text": "OK, so first let me remember\nwhat the nuclear norm is",
    "start": "2098660",
    "end": "2104000"
  },
  {
    "text": "of a matrix. The nuclear norm a matrix is\nthe sum of the singular values,",
    "start": "2104000",
    "end": "2119635"
  },
  {
    "text": "the sum of the singular values. So it's like the L1\nnorm for a vector.",
    "start": "2119635",
    "end": "2129170"
  },
  {
    "text": "That's a right way\nto think about it. And do you remember\nwhat was special? We've talked about\nusing the L1 norm.",
    "start": "2129170",
    "end": "2138230"
  },
  {
    "text": "It has this special property\nthat the ordinary L2 norm absolutely does not have.",
    "start": "2138230",
    "end": "2145190"
  },
  {
    "text": "What was it special\nabout the L1 norm? If I minimize the L1 norm with\nsome constraint, like ab equal",
    "start": "2145190",
    "end": "2156080"
  },
  {
    "text": "b, what's special about the\nsolution, the minimum in the L1",
    "start": "2156080",
    "end": "2161320"
  },
  {
    "text": "norm? AUDIENCE: Sparse. GILBERT STRANG: Sparse, right. Sparse.",
    "start": "2161320",
    "end": "2166920"
  },
  {
    "text": "So this is moving\nus up to matrices. And that's where compressed\nsensing comes in.",
    "start": "2166920",
    "end": "2173670"
  },
  {
    "text": "Matrix completion comes in. So matrix completion\nwould just be--",
    "start": "2173670",
    "end": "2180580"
  },
  {
    "text": "I mentioned-- so\nthis is completion. ",
    "start": "2180580",
    "end": "2186120"
  },
  {
    "text": "And I'll remember\nthe words Netflix, which made the problem famous.",
    "start": "2186120",
    "end": "2191910"
  },
  {
    "text": "So I have the matrix A, 3, 2,\nquestion mark, question mark,",
    "start": "2191910",
    "end": "2204140"
  },
  {
    "text": "question mark, 1, 4,\n6, question mark-- ",
    "start": "2204140",
    "end": "2213390"
  },
  {
    "text": "missing data. And so I have to put\nit in something there,",
    "start": "2213390",
    "end": "2219650"
  },
  {
    "text": "because if I don't put in\nanything, then the numbers I do know are useless,\nbecause no row or no column",
    "start": "2219650",
    "end": "2227930"
  },
  {
    "text": "is complete. So it just would give up. Somebody that sent\nme the data, 3 and 2",
    "start": "2227930",
    "end": "2234710"
  },
  {
    "text": "and didn't tell me a\nranking for the third movie,",
    "start": "2234710",
    "end": "2240050"
  },
  {
    "text": "I'd have to say,\nwell, I can't use it. That's not possible. So we need to think about there.",
    "start": "2240050",
    "end": "2248990"
  },
  {
    "text": "And the idea is that the numbers\nthat minimized the nuclear norm",
    "start": "2248990",
    "end": "2256790"
  },
  {
    "text": "are a good choice,\na good choice. So that's just a connection here\nthat we will say more about,",
    "start": "2256790",
    "end": "2266540"
  },
  {
    "text": "but not-- we could have a whole\ncourse in compressed sensing",
    "start": "2266540",
    "end": "2272240"
  },
  {
    "text": "and nuclear norm. Professor Parrilo in course\n6 is an expert on this.",
    "start": "2272240",
    "end": "2279470"
  },
  {
    "text": " But you see the point that--",
    "start": "2279470",
    "end": "2286980"
  },
  {
    "text": "so you remember v1\ncame from the 0 norm.",
    "start": "2286980",
    "end": "2298900"
  },
  {
    "text": " And what is the 0\nnorm of the vector? ",
    "start": "2298900",
    "end": "2306940"
  },
  {
    "text": "Well, it's not a norm. So you could say,\nforget it, no answer. But what do we\nsymbolically mean when",
    "start": "2306940",
    "end": "2314880"
  },
  {
    "text": "I write the 0 norm of a vector? I mean the number of....?",
    "start": "2314880",
    "end": "2321070"
  },
  {
    "text": "Non-zeros. The number of non-zeros. This was the number of\nnon-zeros in the vector, in v.",
    "start": "2321070",
    "end": "2335430"
  },
  {
    "text": "But it's not a norm, because\nif I take 2 times the vector,",
    "start": "2335430",
    "end": "2343720"
  },
  {
    "text": "I have the same number\nof non-zeros, same norm. I can't have the norm\nof 2v equal the norm",
    "start": "2343720",
    "end": "2349662"
  },
  {
    "text": "of v. That would blow away\nall the properties of norms.",
    "start": "2349662",
    "end": "2355890"
  },
  {
    "text": "So v0 is not a norm. And then we move it to that sort\nof appropriate nearest norm.",
    "start": "2355890",
    "end": "2361500"
  },
  {
    "text": "And we get v1. We get the L1 norm,\nwhich is the sum of--",
    "start": "2361500",
    "end": "2367320"
  },
  {
    "text": "everybody remembers that\nthis is the sum of the vi.",
    "start": "2367320",
    "end": "2372474"
  },
  {
    "text": "And you remember my pictures\nof diamonds touching",
    "start": "2372474",
    "end": "2377950"
  },
  {
    "text": "planes at sharp points. Well, that's what\nis going on here.",
    "start": "2377950",
    "end": "2384430"
  },
  {
    "text": "That problem was\ncalled basis pursuit. And it comes back\nagain in this section.",
    "start": "2384430",
    "end": "2391599"
  },
  {
    "text": " So I minimize this norm\nsubject to the conditions.",
    "start": "2391600",
    "end": "2401570"
  },
  {
    "text": "Now, I'm just going to take\na jump to the matrix case.",
    "start": "2401570",
    "end": "2407070"
  },
  {
    "text": "What's my idea here? My idea is that for a\nmatrix, the nuclear norm",
    "start": "2407070",
    "end": "2414420"
  },
  {
    "text": "comes from what?  What's the norm that\nwe sort of start with,",
    "start": "2414420",
    "end": "2421570"
  },
  {
    "text": "but it's not a norm? And when I sort of take the--",
    "start": "2421570",
    "end": "2427280"
  },
  {
    "text": "because the requirements\nfor a norm don't fail--",
    "start": "2427280",
    "end": "2434000"
  },
  {
    "text": "they fail for what I'm\nabout to write there. I could put A 0,\nbut I don't want",
    "start": "2434000",
    "end": "2441920"
  },
  {
    "text": "the number of non-zero entries. That would be a good guess.",
    "start": "2441920",
    "end": "2447280"
  },
  {
    "text": "And probably in some\nsense it makes sense. But it's not the\nanswer I'm looking for.",
    "start": "2447280",
    "end": "2453960"
  },
  {
    "text": "What do you think is the 0 norm\nof a matrix that is not a norm,",
    "start": "2453960",
    "end": "2464030"
  },
  {
    "text": "but when I pump it up to the\nbest, to the nearest good norm,",
    "start": "2464030",
    "end": "2469400"
  },
  {
    "text": "I get the nuclear norm? So this is the question,\nit's what is A0?",
    "start": "2469400",
    "end": "2474500"
  },
  {
    "text": " And it's what?",
    "start": "2474500",
    "end": "2482160"
  },
  {
    "text": "AUDIENCE: The rank. GILBERT STRANG: The rank. The rank of matrix\nis the equivalent.",
    "start": "2482160",
    "end": "2487890"
  },
  {
    "text": " So I don't know about the zero.",
    "start": "2487890",
    "end": "2494170"
  },
  {
    "text": "Nobody else calls it A0. So I better not. It's the rank.",
    "start": "2494170",
    "end": "2499180"
  },
  {
    "text": "So again, the rank\nis not a norm, because if I double the matrix,\nI don't double the rank. ",
    "start": "2499180",
    "end": "2506600"
  },
  {
    "text": "So I have to move to a norm. And it turns out to\nbe the nuclear norm. And now, I'll just,\nwith one minute,",
    "start": "2506600",
    "end": "2512539"
  },
  {
    "text": "say it's the guess of some\npeople who are working hard",
    "start": "2512540",
    "end": "2517850"
  },
  {
    "text": "to prove it, that\nthe deep learning",
    "start": "2517850",
    "end": "2522920"
  },
  {
    "text": "algorithm of gradient\ndescent finds the solution to the minimum\nproblem in the nuclear norm.",
    "start": "2522920",
    "end": "2532850"
  },
  {
    "text": "And we don't know if\nthat's true or not yet. For related examples, like\nthis thing, it's proved.",
    "start": "2532850",
    "end": "2545539"
  },
  {
    "text": "For the exact problem of deep\nlearning, it's a conjecture.",
    "start": "2545540",
    "end": "2551870"
  },
  {
    "text": "So that's what in section 4.4. But that word lasso, you\nwant to know what that is.",
    "start": "2551870",
    "end": "2558859"
  },
  {
    "text": "Compressed sensing,\nI'll say a word about. So that will be Monday after\nAlex Townsend's lecture Friday.",
    "start": "2558860",
    "end": "2566539"
  },
  {
    "text": "So he's coming to\nspeak to computational",
    "start": "2566540",
    "end": "2571790"
  },
  {
    "text": "science students all over\nMIT tomorrow afternoon. I'll certainly go\nto that, but then he",
    "start": "2571790",
    "end": "2580099"
  },
  {
    "text": "said he would come in and\ntake this class Friday. So I'll see you Friday.",
    "start": "2580100",
    "end": "2585410"
  },
  {
    "text": "And he'll be here too. ",
    "start": "2585410",
    "end": "2587955"
  }
]