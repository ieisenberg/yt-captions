[
  {
    "start": "0",
    "end": "181000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6920"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation, or view\nadditional materials from",
    "start": "6920",
    "end": "13469"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "13470",
    "end": "18650"
  },
  {
    "text": " PROFESSOR: Ladies and gentlemen,\nwelcome to lecture number 12.",
    "start": "18650",
    "end": "24690"
  },
  {
    "text": "In this lecture, I would like\nto discuss with you solution methods for eigenproblems.",
    "start": "24690",
    "end": "29900"
  },
  {
    "text": "This is a very large field,\nand all I can do really in this lecture is to give you or\ndiscuss this you a few of the",
    "start": "29900",
    "end": "36970"
  },
  {
    "text": "solution methods that\nwe are using in finite element analysis. So it will be a very brief\nintroduction, a brief survey",
    "start": "36970",
    "end": "44710"
  },
  {
    "text": "of the methods of solution that\nwe're using for finite element eigenvalue problems.",
    "start": "44710",
    "end": "50440"
  },
  {
    "text": "The generalized eigenvalue\nproblem, K phi equals lambda M phi, is a problem that we\nencountered in the multiple",
    "start": "50440",
    "end": "57350"
  },
  {
    "text": "position analysis. Please recognize that I'm\ncalling lambda here, the",
    "start": "57350",
    "end": "63480"
  },
  {
    "text": "eigenvalue, which was our omega squared in the last lecture. This is really the eigenproblems\nthat we are",
    "start": "63480",
    "end": "69950"
  },
  {
    "text": "primarily addressing ourselves\nto finite element analysis. The standard eigenproblems\nproblem here, EVP stands for",
    "start": "69950",
    "end": "77980"
  },
  {
    "text": "eigenproblem, has M equal\nto the identity matrix. So when we're talking about\nsolution methods of this",
    "start": "77980",
    "end": "83800"
  },
  {
    "text": "problem, we're really also talk\nabout solution methods of this problem. If we have, in a [? multiple ?]\nposition",
    "start": "83800",
    "end": "89530"
  },
  {
    "text": "analysis, non-proportional\ndamping, then we would have to solve this eigenproblem, which\nis a quadratic eigenproblem.",
    "start": "89530",
    "end": "97040"
  },
  {
    "text": "I will not address myself to the\nsolution of this problem, so in what follow, we are\ntalking about a solution of",
    "start": "97040",
    "end": "102850"
  },
  {
    "text": "this problem, and of course, the\nsolution of this problem also, when M is equal to\nthe identity matrix.",
    "start": "102850",
    "end": "109030"
  },
  {
    "text": "In particular, we are interested\nin the solution of large eigenvalue problems when\nN, is say, greater than 500.",
    "start": "109030",
    "end": "116119"
  },
  {
    "text": "N, of course, being the order of\nK and M, and with the half bandwidth, I should say,\nbeing greater than 60.",
    "start": "116120",
    "end": "122900"
  },
  {
    "text": "The number of eigenvalues that\nwe will be looking for, that we want to calculate, are say,\n1 to 1/3 of N. 1/3 of N is",
    "start": "122900",
    "end": "130139"
  },
  {
    "text": "already large number\nof eigenvalues. Generally, we are talking about\n1 to and at most, 100",
    "start": "130139",
    "end": "136840"
  },
  {
    "text": "for large eigenproblems. When we have a small eigenvalue\nproblem, in other words, N being small, say 50\nor 100, there are a large",
    "start": "136840",
    "end": "145530"
  },
  {
    "text": "number of different techniques\nthat can be used to solve this eigenvalue problem, and it\nreally does not make that much",
    "start": "145530",
    "end": "151970"
  },
  {
    "text": "difference which technique\nyou are using. The cost can still be quite\nsignificant, but certainly,",
    "start": "151970",
    "end": "159610"
  },
  {
    "text": "when we're talking about very\nlarge eigenproblems, N being even larger than a 1,000 or\n2,000, then it is of up most",
    "start": "159610",
    "end": "167370"
  },
  {
    "text": "concern to select a technique\nthat is very effective. And I want to address myself\nreally now towards those",
    "start": "167370",
    "end": "175670"
  },
  {
    "text": "techniques that we feel are\nvery effectively used in finite element analysis.",
    "start": "175670",
    "end": "181959"
  },
  {
    "start": "181000",
    "end": "322000"
  },
  {
    "text": "In dynamic analysis, of course,\nwith proportional damping, this was the eigenvalue\nproblem that we",
    "start": "181960",
    "end": "187930"
  },
  {
    "text": "wanted to solve. Now, if they are 0 frequencies\npresent, in other words, when we have rigid body modes in the\nsystem, then we can use",
    "start": "187930",
    "end": "195540"
  },
  {
    "text": "the following procedure to\nbasically getting rid of these 0 frequencies. We simply add, on the left hand\nside, a mu M phi, and on",
    "start": "195540",
    "end": "203040"
  },
  {
    "text": "the right hand side,\na mu M phi. And taking these two together,\nwe are getting this",
    "start": "203040",
    "end": "208879"
  },
  {
    "text": "coefficient matrix, and\na new lambda value. That lambda value now is omega\nsquared plus mu, where omega",
    "start": "208880",
    "end": "215460"
  },
  {
    "text": "squared is equal to\nlambda minus mu. What we have been doing here,\nis basically, we have been",
    "start": "215460",
    "end": "221870"
  },
  {
    "text": "applying a shift. In other words, if we were to\nplot the characteristic polynomial, giving us\nthe eigenvalues, the",
    "start": "221870",
    "end": "230560"
  },
  {
    "text": "characteristic polynomial of\ncourse, being p of lambda being determined K bar minus\nlambda M, where K bar is equal",
    "start": "230560",
    "end": "237940"
  },
  {
    "text": "to K plus mu M. If we plug this characteristic\npolynomial, we will see this",
    "start": "237940",
    "end": "243270"
  },
  {
    "text": "curve here, schematically drawn,\nof course, where the omega squared values would start\nhere, in other words,",
    "start": "243270",
    "end": "250250"
  },
  {
    "text": "the lowest omega squared\nvalue is, in fact, 0. Whereas our lambda value\nnow starts right here.",
    "start": "250250",
    "end": "257588"
  },
  {
    "text": "This is the shift that\nwe [? imposed. ?] So, basically, we now recognize\nthat we have all",
    "start": "257589",
    "end": "265755"
  },
  {
    "text": "eigenvalues being greater than\n0, because this is our new starting value. All eigenvalues measured from\nhere are greater than 0, the",
    "start": "265755",
    "end": "273150"
  },
  {
    "text": "smallest one being mu,\nnamely when omega squared is equal to 0. And the larger ones\nbeing here.",
    "start": "273150",
    "end": "279610"
  },
  {
    "text": "It is effective to perform such\na shift when we have 0",
    "start": "279610",
    "end": "285259"
  },
  {
    "text": "eigenvalues, in other words, to\noperate on this eigenvalue, rather than on that one, because\nour techniques then",
    "start": "285260",
    "end": "294789"
  },
  {
    "text": "are very stable and effective,\nand this shift here is, in general, therefore performed.",
    "start": "294790",
    "end": "300949"
  },
  {
    "text": "So all we have to address\nourselves to then from now on, is really, the solution of\neigenvalues and eigenvectors,",
    "start": "300950",
    "end": "308390"
  },
  {
    "text": "when all of the eigenvalues\nare greater than 0. If we start off with a problem\nwhere we have a 0 frequency,",
    "start": "308390",
    "end": "315720"
  },
  {
    "text": "we apply the shift so that the\neigenvalues now of this problem here are all\ngreater than 0.",
    "start": "315720",
    "end": "323280"
  },
  {
    "start": "322000",
    "end": "417000"
  },
  {
    "text": "In buckling analysis, we\nhave this eigenproblem. We really did not talk about-- we did not really at all about\nbuckling analysis in the set",
    "start": "323280",
    "end": "331700"
  },
  {
    "text": "of lectures, but you might just\nbe interested in finding out how would we solve for\nthese eigenvalues.",
    "start": "331700",
    "end": "338139"
  },
  {
    "text": "Well, here is the characteristic\npolynomial. The smallest lambda value, of\ncourse, gives now a critical",
    "start": "338140",
    "end": "343390"
  },
  {
    "text": "load, that is the one e would\nbe interested in. It is effective here,\nto rewrite this",
    "start": "343390",
    "end": "348639"
  },
  {
    "text": "problem in this form. All we do, is we are taking\nthe lambda value onto the",
    "start": "348640",
    "end": "353970"
  },
  {
    "text": "other side. In other words, we\nare dividing this equation here by a lambda. So that one of the lambda stands\non this side, and KG",
    "start": "353970",
    "end": "361320"
  },
  {
    "text": "phi on that side. The resulting problem is this\none, where kappa is 1 over lambda and we have now\nhere, is a K matrix.",
    "start": "361320",
    "end": "368540"
  },
  {
    "text": "K is positive definite, and now\nit is more effective to",
    "start": "368540",
    "end": "374630"
  },
  {
    "text": "operate on this equation. How do we obtain now\nthe largest kappa?",
    "start": "374630",
    "end": "379820"
  },
  {
    "text": "Because the largest kappa is the\none that we are interested in, which corresponds to the\nsmallest lambda, the smallest",
    "start": "379820",
    "end": "385010"
  },
  {
    "text": "buckling load. Well, we would now perform\na shift to the right, subtracting mu K from this side,\nfrom this side, and from",
    "start": "385010",
    "end": "394190"
  },
  {
    "text": "that side, to obtain this\neigenvalue problem. That means we are really\nshifting from here up to",
    "start": "394190",
    "end": "399979"
  },
  {
    "text": "there, and now we are interested\nin solving kappa N",
    "start": "399980",
    "end": "405130"
  },
  {
    "text": "by operating on this\neigenproblems problem. It is this eigenvalue problem\nthat we can operate on directly, using the techniques\nthat I would be discussing",
    "start": "405130",
    "end": "413540"
  },
  {
    "text": "later, to determine search\nmethod and the subspace iteration method. But before we get into those\ntechniques, I would like to",
    "start": "413540",
    "end": "420990"
  },
  {
    "start": "417000",
    "end": "570000"
  },
  {
    "text": "discuss with you some\npreliminary considerations that are important\nwhen we look at",
    "start": "420990",
    "end": "427710"
  },
  {
    "text": "an eigenvalue solution. The important first point that\nI want to make is that there",
    "start": "427710",
    "end": "434030"
  },
  {
    "text": "is a traditional approach\nfor the solution of this eigenvalue problem here. Let's look at this eigenvalue\nproblem.",
    "start": "434030",
    "end": "440610"
  },
  {
    "text": "Where of course, when we're\ntalking about a buckling analysis, we would have\ndifferent matrices here, but",
    "start": "440610",
    "end": "445699"
  },
  {
    "text": "basically, we still solve\nthis eigenvalue problem. The traditional approach lies\nin factorizing the M matrix",
    "start": "445700",
    "end": "454159"
  },
  {
    "text": "into Cholesky factors. We talked about the Cholesky\nfactor when we talked about solution of equations.",
    "start": "454160",
    "end": "460230"
  },
  {
    "text": "So here it comes up again. We would factorize the M matrix\ninto Cholesky factors,",
    "start": "460230",
    "end": "465440"
  },
  {
    "text": "as shown here. We define now a new eigenvector,\nphi curl being",
    "start": "465440",
    "end": "470730"
  },
  {
    "text": "equal as shown here, L curl\ntranspose times 5. We substitute from here into\nthere, and we directly obtain",
    "start": "470730",
    "end": "479970"
  },
  {
    "text": "this eigenvalue problem here. Now we have the standard\neigenproblem.",
    "start": "479970",
    "end": "485980"
  },
  {
    "text": "Now once we have a standard\neigenproblem, there are a large number of techniques\nthat can be used.",
    "start": "485980",
    "end": "491660"
  },
  {
    "text": "For example, the QR method,\nthe standard Jacobi method, et cetera.",
    "start": "491660",
    "end": "496789"
  },
  {
    "text": "So this is, what\nwe might call. a traditional approach to\nsolve the generalized eigenproblem, this is the one\nhere that we are really",
    "start": "496790",
    "end": "504139"
  },
  {
    "text": "interested in. But it involves this\ntransformation, and this transformation here can involve\na very significant",
    "start": "504140",
    "end": "512280"
  },
  {
    "text": "cost, because we have to find\nthese Cholesky factors and then, of course, we have to\ncalculate this K curl.",
    "start": "512280",
    "end": "517929"
  },
  {
    "text": "Another way of performing\na transformation of the generalized item problem to a\nstandard form would be to",
    "start": "517929",
    "end": "523549"
  },
  {
    "text": "solve the eigenproblem\nof M first. W here and W transpose\n[? store ?]",
    "start": "523549",
    "end": "529110"
  },
  {
    "text": "the eigenvectors of the M\nmatrix, and D squared is a diagonal matrix of the\neigenvalues of M. And then",
    "start": "529110",
    "end": "537320"
  },
  {
    "text": "having factored M in this form,\nan equation similar to this one here can be written.",
    "start": "537320",
    "end": "542510"
  },
  {
    "text": "Where now, the transformation,\nof course, involves the eigenvectors, W. Well, this approach is\nsometimes still quite",
    "start": "542510",
    "end": "552860"
  },
  {
    "text": "effective, but in general, it\ncannot be recommended because there's too much cost involved\nin the factorizing of the M",
    "start": "552860",
    "end": "560420"
  },
  {
    "text": "matrix, and then particular,\nin the transformation here, and the solution of this problem\nand the, of course, to",
    "start": "560420",
    "end": "566180"
  },
  {
    "text": "get from this problem, the phi\nvector, we have to go back to this equation. A direct solution of the\ngeneralized eigenvalue problem",
    "start": "566180",
    "end": "576070"
  },
  {
    "start": "570000",
    "end": "625000"
  },
  {
    "text": "that we want to solve is\nreally more effective. And now, we are ordering the\nlambdas as shown here.",
    "start": "576070",
    "end": "583680"
  },
  {
    "text": "Remember, our smallest lambda\nnow is greater than 0, the eigonvectors are stored as shown\nhere, and for most of a",
    "start": "583680",
    "end": "592490"
  },
  {
    "text": "position analysis, we are really\ninterested in solving only for a certain number of\neigen pairs, i equals 1 to p,",
    "start": "592490",
    "end": "600550"
  },
  {
    "text": "as discussed in the last\nlecture, or i equals r to s. If we have a vibration\nexcitation problem, we really",
    "start": "600550",
    "end": "606980"
  },
  {
    "text": "only want to solve for specific\nfrequencies and corresponding mode shapes\nin a certain interval, as shown here.",
    "start": "606980",
    "end": "614300"
  },
  {
    "text": "The solution procedures on this\ngeneralized eigenvalue problem can be categorized\ninto the following forms.",
    "start": "614300",
    "end": "623690"
  },
  {
    "text": "The first solution procedure,\nor a whole class of solution",
    "start": "623690",
    "end": "629780"
  },
  {
    "start": "625000",
    "end": "781000"
  },
  {
    "text": "procedures, that I should really\nsay, which we might call the vector iteration\ntechniques.",
    "start": "629780",
    "end": "636050"
  },
  {
    "text": "The idea is the following-- if we want to solve this\nproblem, then why not solve it",
    "start": "636050",
    "end": "643160"
  },
  {
    "text": "in the following way? Start off with a vector on this\nside, that we assume,",
    "start": "643160",
    "end": "655830"
  },
  {
    "text": "calculate a new vector on this\nside, then take that new vector, plug it back on the\nright hand side, and cycle",
    "start": "655830",
    "end": "663130"
  },
  {
    "text": "through this way. This is indeed inverse\niteration. What do we do here? We are putting a vector on the\nright hand side, x1, when K is",
    "start": "663130",
    "end": "671740"
  },
  {
    "text": "equal to 1, that vector\nwe have to assume. Then we are multiplying out\nthe right hand side.",
    "start": "671740",
    "end": "678540"
  },
  {
    "text": "Now we have a set of\nsimultaneous equations that we would solve just in the same way\nas we solve the equations",
    "start": "678540",
    "end": "686460"
  },
  {
    "text": "in static analysis. We can think of this\nas a load vector. We are calculating this\ndisplacement vector, and then",
    "start": "686460",
    "end": "694100"
  },
  {
    "text": "having got this displacement\nvector, and not taking care of this lambda, or having,\nbasically, taken this lambda",
    "start": "694100",
    "end": "702390"
  },
  {
    "text": "out of this equation. Which means that this vector\nwill grow in lengths, or",
    "start": "702390",
    "end": "709090"
  },
  {
    "text": "collapse in lengths. Therefore, want to scale this\nvector down to a good length--",
    "start": "709090",
    "end": "714160"
  },
  {
    "text": "a length that does not hit\noverflow, so to say, or underflow in the machine. And that scaling is achieved\nas shown here.",
    "start": "714160",
    "end": "722260"
  },
  {
    "text": "So, the final result then, this\nis just a scalar here, as you can see, is a vector here,\nwhich we can put on to the",
    "start": "722260",
    "end": "730519"
  },
  {
    "text": "right hand side and keep\non iterating around. In fact, we find as that\niteration procedure converges",
    "start": "730520",
    "end": "738110"
  },
  {
    "text": "to the lowest eigenvector,\nprovided the starting vector is not orthogonal, is not\nM orthogonal to phi 1.",
    "start": "738110",
    "end": "748209"
  },
  {
    "text": "This is one first iteration\ntechnique. Maybe I should put\na side note down.",
    "start": "748210",
    "end": "757130"
  },
  {
    "text": "A side note that many of you\nmight have in your mind.",
    "start": "757130",
    "end": "762940"
  },
  {
    "text": "Many of you might have the\nfollowing question in your mind-- why he iterate, why does\nhe not solve directly for",
    "start": "762940",
    "end": "768350"
  },
  {
    "text": "the eigenvalues? Just like we saw a set of\nsimultaneous equations. Well, there's one very important\npoint, and this is",
    "start": "768350",
    "end": "777889"
  },
  {
    "text": "the point-- if we are solving an eigenvalue\nproblem, and shown here, then really that\nmeans the following--",
    "start": "777890",
    "end": "785350"
  },
  {
    "text": "we are saying that determinant,\nK minus lambda M",
    "start": "785350",
    "end": "790920"
  },
  {
    "text": "shall be equal to 0. Why? Because if we write\nthis equation in the following form--",
    "start": "790920",
    "end": "797080"
  },
  {
    "text": "K minus lambda M times phi\nequals 0, then we have here a",
    "start": "797080",
    "end": "804760"
  },
  {
    "text": "set of simultaneous equations\nwith a 0 vector on",
    "start": "804760",
    "end": "810390"
  },
  {
    "text": "the right hand side. And there is an important\ntheorem in linear algebra that says, only a solution to this\nset of equations when this",
    "start": "810390",
    "end": "819960"
  },
  {
    "text": "coefficient matrix\nis singular. Which means that the\ndeterminant of this coefficient matrix must be 0.",
    "start": "819960",
    "end": "827700"
  },
  {
    "text": "So, what do we want? We really want to solve p of\nlambda determinant, K minus",
    "start": "827700",
    "end": "834649"
  },
  {
    "text": "lambda M. We want to solve for\nthe 0's of this polynomial.",
    "start": "834650",
    "end": "840480"
  },
  {
    "text": "That's really what we want. Now, if we want to solve for\nthe 0's of this polynomial,",
    "start": "840480",
    "end": "847700"
  },
  {
    "text": "then we also have to understand\nthat when k is of",
    "start": "847700",
    "end": "853390"
  },
  {
    "text": "order 4, there will\nbe four 0's. The polynomial itself\nwill be of order 4.",
    "start": "853390",
    "end": "860080"
  },
  {
    "text": "If K is of order, N, then\nthere will be N 0's. So the polynomial is order N.\nHowever, if we want to find",
    "start": "860080",
    "end": "869680"
  },
  {
    "text": "the solution of a polynomial,\nor order N, then we very quickly find that, in fact, we\ncan only look up the solution",
    "start": "869680",
    "end": "880650"
  },
  {
    "text": "to such polynomials if they are\nof low enough order, lower than say, 4.",
    "start": "880650",
    "end": "886660"
  },
  {
    "text": "Above for larger order\npolynomials, there's no way you can look up the solution.",
    "start": "886660",
    "end": "893150"
  },
  {
    "text": "You have to iterate to obtain\nthe solution of the 0's of",
    "start": "893150",
    "end": "898160"
  },
  {
    "text": "this polynomial. However, since this problem\nhere, is completely equivalent",
    "start": "898160",
    "end": "904480"
  },
  {
    "text": "to this problem. To solve this problem,\nyou have to iterate. And this is the important\nconclusion.",
    "start": "904480",
    "end": "911480"
  },
  {
    "text": "It is of up most importance that\nyou understand that to solve an eigenvalue problem of\norder larger than say, 4, you",
    "start": "911480",
    "end": "920570"
  },
  {
    "text": "will have to iterate. Because you're solving for the\n0's of a polynomial of order",
    "start": "920570",
    "end": "927290"
  },
  {
    "text": "larger than 4. Only for very small order\npolynomials, we can look up the solutions in handbooks.",
    "start": "927290",
    "end": "934060"
  },
  {
    "text": "So having grasped the fact that\nwe have to iterate to solve for the 0's of this\npolynomial, to solve for the",
    "start": "934060",
    "end": "944460"
  },
  {
    "text": "eigenvalues of this problem, we\nare stuck with iteration,",
    "start": "944460",
    "end": "951390"
  },
  {
    "start": "946000",
    "end": "1134000"
  },
  {
    "text": "and since we're stuck with\niteration, we now want to come up with techniques to iterate. And the first technique to\niterate is this vector",
    "start": "951390",
    "end": "958620"
  },
  {
    "text": "iteration technique. And the basic idea, once more,\nis if you want to solve this",
    "start": "958620",
    "end": "964470"
  },
  {
    "text": "equation, well, then why not\niterate in the following way? Put some starting value in here,\nthis number is just a",
    "start": "964470",
    "end": "973150"
  },
  {
    "text": "scalar, let's neglect that\neffect for the moment. And having put a starting value\nin here, calculate this",
    "start": "973150",
    "end": "979640"
  },
  {
    "text": "one and keep on cycling\naround this way. And that's what we call\ninverse iteration.",
    "start": "979640",
    "end": "985600"
  },
  {
    "text": "Inverse iteration here, a vector\ninverse iteration, because we have to solve a set\nof equations here, with K as",
    "start": "985600",
    "end": "992680"
  },
  {
    "text": "the coefficient matrix.  There are other iteration\ntechniques.",
    "start": "992680",
    "end": "998290"
  },
  {
    "text": "There's also forward\niteration. And basically, what we're\nsaying there is-- well if we are starting off here\nand going around in this",
    "start": "998290",
    "end": "1005970"
  },
  {
    "text": "circle, why not start off here\nand go around in that circle, the red line.",
    "start": "1005970",
    "end": "1011189"
  },
  {
    "text": "And that, in fact, gives\nus forward iteration. One can prove that that forward\niteration converges to",
    "start": "1011190",
    "end": "1017970"
  },
  {
    "text": "the largest eigenvalue. We are usually interested in the\nsmallest frequencies, the",
    "start": "1017970",
    "end": "1023190"
  },
  {
    "text": "smallest eigenvalues, and\ntherefore, we are using inverse iteration\nquite commonly.",
    "start": "1023190",
    "end": "1028809"
  },
  {
    "text": "Then there is Rayleigh\nQuotient iteration. The Rayleigh Quotient iteration\nis again, an inverse",
    "start": "1028810",
    "end": "1036790"
  },
  {
    "text": "iteration, however, with an\nacceleration scheme in it, a very effective acceleration\nscheme.",
    "start": "1036790",
    "end": "1044189"
  },
  {
    "text": "This Rayleigh Quotient iteration\ncan be employed to calculate one eigenvalue and\nvector, and then, of course,",
    "start": "1044190",
    "end": "1050070"
  },
  {
    "text": "we have to deflate afterwards\nthe matrix, the coefficient matrix for the eigenvector\nthat we have calculated",
    "start": "1050070",
    "end": "1057170"
  },
  {
    "text": "already, and we then could go on\ntowards the calculation of",
    "start": "1057170",
    "end": "1062750"
  },
  {
    "text": "additional eigenvalues\nand vectors. The same deflation also would\nbe used in inverse iteration alone, and in forward\niteration alone.",
    "start": "1062750",
    "end": "1070200"
  },
  {
    "text": "In a Rayleigh Quotient\niteration, we converge to an eigen pair, but which one\nis not guaranteed.",
    "start": "1070200",
    "end": "1077440"
  },
  {
    "text": "So this is a class of\ntechniques, iteration methods that we are using.",
    "start": "1077440",
    "end": "1083560"
  },
  {
    "text": "Another class of techniques,\nagain iteration methods, we are stuck with iteration, as\nI mentioned earlier, is the",
    "start": "1083560",
    "end": "1089890"
  },
  {
    "text": "polynomial iteration methods. Here, we are saying that this\nproblem is equivalent to that, we want to solve\nfor these 0's.",
    "start": "1089890",
    "end": "1096920"
  },
  {
    "text": "Therefore, for the 0's of this\npolynomial, sketched out here, and one way is to use\nNewton iteration.",
    "start": "1096920",
    "end": "1103710"
  },
  {
    "text": "Here we have p, here\nwe have p prime. Well, of course, if we have\np here, we have to also",
    "start": "1103710",
    "end": "1109740"
  },
  {
    "text": "recognize we should have\nthe coefficients here. Well, these coefficients are\ngiven here, A0 to a n.",
    "start": "1109740",
    "end": "1117330"
  },
  {
    "text": "And we could rewrite this\npolynomial in this form, once we know the zeroes.",
    "start": "1117330",
    "end": "1124260"
  },
  {
    "text": "Now, if we don't know the\nzeroes, of course, we would be stuck on iterating on this\nmatrix, where we would have to",
    "start": "1124260",
    "end": "1132840"
  },
  {
    "text": "calculate these coefficients. However, there are now really,\ntwo different techniques.",
    "start": "1132840",
    "end": "1140039"
  },
  {
    "start": "1134000",
    "end": "1322000"
  },
  {
    "text": "The first technique is explicit\npolynomial iteration. In the explicit polynomial\niteration, we expand the",
    "start": "1140040",
    "end": "1147260"
  },
  {
    "text": "polynomial and iterate for the\n0's, by that I mean, we actually expand this\npolynomial.",
    "start": "1147260",
    "end": "1154200"
  },
  {
    "text": "However, this technique is not\nsuitable for larger problems because first of all, there's\nmuch work to obtain the ai's,",
    "start": "1154200",
    "end": "1161150"
  },
  {
    "text": "and then it's an unstable\nprocess. Unstable because small errors\nin these coefficients here,",
    "start": "1161150",
    "end": "1167940"
  },
  {
    "text": "give large errors\nin the zeroes. So this is not a suitable\ntechnique, and we are better",
    "start": "1167940",
    "end": "1174980"
  },
  {
    "text": "off using implicit polynomial\niteration. In the implicit polynomial\niteration, we evaluate the",
    "start": "1174980",
    "end": "1180409"
  },
  {
    "text": "determinant all this\nmatrix here. In others, p at mu i, via the L\nD L transpose factorization,",
    "start": "1180410",
    "end": "1190290"
  },
  {
    "text": "which we discussed in the Gauss\nelimination solution,",
    "start": "1190290",
    "end": "1195770"
  },
  {
    "text": "when we talked about Gauss\nelimination solution. To obtain the determinant of\nthis matrix here then, we need",
    "start": "1195770",
    "end": "1201530"
  },
  {
    "text": "simply to multiply all the\ndiagonal elements. This technique is accurate,\nprovided we do not encounter",
    "start": "1201530",
    "end": "1208450"
  },
  {
    "text": "large multipliers L matrix. And if we do, we have to\nshift away, we have to",
    "start": "1208450",
    "end": "1214400"
  },
  {
    "text": "change our mu i. Also we directly solve for\nlambda 1 and so on.",
    "start": "1214400",
    "end": "1219570"
  },
  {
    "text": "And for the more we can use\neffectively a secant iteration, instead of\ncalculating, in other words,",
    "start": "1219570",
    "end": "1225710"
  },
  {
    "text": "the p prime at mu. This is here approximately equal\nto p prime, at mu i.",
    "start": "1225710",
    "end": "1233929"
  },
  {
    "text": "Instead of calculating this\nderivative, we are putting this value here, which gives\nus an approximation to that",
    "start": "1233930",
    "end": "1241330"
  },
  {
    "text": "derivative, this is what we\ncall the secant iteration. Once we have calculated one\neigenvalue, of course, we will",
    "start": "1241330",
    "end": "1247260"
  },
  {
    "text": "have to deflate to calculate the\nnext eigenvalue, and this is the process that\nhe would pursue.",
    "start": "1247260",
    "end": "1253360"
  },
  {
    "text": "Pictorially, we have here,\nour p lambda, this is the polynomial.",
    "start": "1253360",
    "end": "1258470"
  },
  {
    "text": "Let me show it to you here. Here, we have the polynomial,\nhere we have lambda 1, here we have lambda 2.",
    "start": "1258470",
    "end": "1264260"
  },
  {
    "text": "In the secant iteration, we\nstart off with the value of mu i minus 1 here, mu i, these\ntwo values are given.",
    "start": "1264260",
    "end": "1271500"
  },
  {
    "text": "Usually, in a practical\nanalysis, mu 0 would be this value here, and mu 1 would be\na lower bound on lambda 1.",
    "start": "1271500",
    "end": "1280660"
  },
  {
    "text": "And plugging these two values\nin, we get mu i plus 1, and you can see that this process\nwould then converge to this",
    "start": "1280660",
    "end": "1290620"
  },
  {
    "text": "eigenvalue. Once we have lambda 1, we can\nrepeat the iteration.",
    "start": "1290620",
    "end": "1295880"
  },
  {
    "text": "However, working now on p\nlambda divided by lambda minus lambda 1.",
    "start": "1295880",
    "end": "1301110"
  },
  {
    "text": "And that means we are operating\non this polynomial here, and polynomial then gives\nus the iteration on that",
    "start": "1301110",
    "end": "1308640"
  },
  {
    "text": "polynomial then gives us\nthe second eigenvalue. So we are deflating the actual p\nlambda by the lambda 1 value",
    "start": "1308640",
    "end": "1317379"
  },
  {
    "text": "in order to obtain\na convergence to the second value.",
    "start": "1317380",
    "end": "1322940"
  },
  {
    "start": "1322000",
    "end": "1717000"
  },
  {
    "text": "Well, this is the one\nclass of methods. Another class of methods, the\nthird one, are the Sturm",
    "start": "1322940",
    "end": "1329750"
  },
  {
    "text": "sequence methods. And here, the basic idea\nis the following--",
    "start": "1329750",
    "end": "1335060"
  },
  {
    "text": "if we look at this eigenproblem,\nand I use a 4x4 matrix here, then if we take a\nshift on this eigenproblem, mu",
    "start": "1335060",
    "end": "1344230"
  },
  {
    "text": "S, and this is the shift here. Then if we factorize K minus\nmu S M, equal to L D L",
    "start": "1344230",
    "end": "1353700"
  },
  {
    "text": "transpose, again, we're using\nour Gauss elimination procedures basically. Then the number of negative\nelements in D is equal to the",
    "start": "1353700",
    "end": "1362420"
  },
  {
    "text": "number eigenvalues smaller\nthan mu S, a very important point.",
    "start": "1362420",
    "end": "1367480"
  },
  {
    "text": "Let us look at this example\nhere of the 4x4 matrix. Here we have plotted\ncharacteristic polynomials.",
    "start": "1367480",
    "end": "1376370"
  },
  {
    "text": "The characteristic polynomial of\nthis 4x4 matrix, with that M matrix, of course, is drawn\nalong here, four eigenvalues.",
    "start": "1376370",
    "end": "1385310"
  },
  {
    "text": "And I have put mu S here\nas the red line. What this theorem says is if I\nfactorize K minus mu S, M,",
    "start": "1385310",
    "end": "1395430"
  },
  {
    "text": "this being the K matrix, the\ntotal K matrix, and the 4x4 M matrix goes in here.",
    "start": "1395430",
    "end": "1401020"
  },
  {
    "text": "Then there would be one negative\nelement in D. One only because mu S is large\nthan lambda 1.",
    "start": "1401020",
    "end": "1410900"
  },
  {
    "text": "If mu S were on this side\nhere, there would be two negative elements in D. If mu\nS were on this side, there",
    "start": "1410900",
    "end": "1418640"
  },
  {
    "text": "would be three negative\nelements in D-- very important point because qw\ncan use this procedure to",
    "start": "1418640",
    "end": "1426360"
  },
  {
    "text": "directly calculate eigenvalues\nthe way I will be discussing it just now.",
    "start": "1426360",
    "end": "1431430"
  },
  {
    "text": "However, let us look a little\nbit more at why this hole is here.",
    "start": "1431430",
    "end": "1436730"
  },
  {
    "text": "Well, it derives from the\nassociated constraint problems",
    "start": "1436730",
    "end": "1441940"
  },
  {
    "text": "with the original\neigenproblem. The original eigenproblem\ncorresponds to this beam",
    "start": "1441940",
    "end": "1449440"
  },
  {
    "text": "problem, which we considered\nearlier already in the discussion of Gauss\nelimination.",
    "start": "1449440",
    "end": "1455730"
  },
  {
    "text": "If I constrain this degree of\nfreedom here, I get a new polynomial corresponding to\nthis problem, and the",
    "start": "1455730",
    "end": "1462690"
  },
  {
    "text": "important point is that the\nlowest root, the lowest value of this problem here lies in\nbetween these two-- lambda 1",
    "start": "1462690",
    "end": "1472000"
  },
  {
    "text": "and lambda 2. The second one here, lies\nbetween lambda 2 and lambda 3,",
    "start": "1472000",
    "end": "1477180"
  },
  {
    "text": "and the third one here lies\nbetween lambda 3 and lambda 4. The second associated constraint\nproblem has two",
    "start": "1477180",
    "end": "1484340"
  },
  {
    "text": "eigenvalues, and this one lies\nbetween these two, this one lies between these two.",
    "start": "1484340",
    "end": "1489370"
  },
  {
    "text": "And the final eigenvalue\nproblem here, the third associate constraint problem\nobtained by knocking out these",
    "start": "1489370",
    "end": "1494630"
  },
  {
    "text": "degrees of freedom here, has one\neigenvalue, and that one lies in between these\ntwo values here.",
    "start": "1494630",
    "end": "1501790"
  },
  {
    "text": "It's this important fact\nthat is used to",
    "start": "1501790",
    "end": "1509970"
  },
  {
    "text": "derive this final result. Now, we have not time to go\nthrough the actual details,",
    "start": "1509970",
    "end": "1518820"
  },
  {
    "text": "but this is the important\nfact here. In other words, that the\n[? i's ?] eigenvalue of this",
    "start": "1518820",
    "end": "1527470"
  },
  {
    "text": "associated constraint problem,\nfor example, the second eigenvalue of this associated\nconstraint problem here,",
    "start": "1527470",
    "end": "1535420"
  },
  {
    "text": "bisects or lies in between,\nnot bisect-- lies in between lambda 2 and\nlambda 3 of this eigenproblem",
    "start": "1535420",
    "end": "1544340"
  },
  {
    "text": "here, and so on. That result is used to derive\nthis final result, which we",
    "start": "1544340",
    "end": "1550700"
  },
  {
    "text": "use effectively then in\nthe eigen solution.",
    "start": "1550700",
    "end": "1557179"
  },
  {
    "text": "And the class of techniques then\nthat we are referring to",
    "start": "1557180",
    "end": "1563950"
  },
  {
    "text": "using this fact are the Sturm\nsequence methods. So basically, what we are saying\nis the following--",
    "start": "1563950",
    "end": "1571049"
  },
  {
    "text": "take this coefficient matrix,\nnow I make the mu x available. We factorize that coefficient\nmatrix into L D L transposed,",
    "start": "1571050",
    "end": "1580060"
  },
  {
    "text": "and the number of negative\nelements in D is equal to the number of eigenvalues smaller\nthan mu S, I. How",
    "start": "1580060",
    "end": "1589630"
  },
  {
    "text": "do we use this fact? Well, if this is here, the\npolynomial, we could start off",
    "start": "1589630",
    "end": "1594885"
  },
  {
    "text": "by putting mu S 1 into this\npoint here, and we would find",
    "start": "1594885",
    "end": "1600950"
  },
  {
    "text": "out the number of eigenvalues\nsmaller than mu S 1. Then we are taking, mu S 2, and\nwe find out the number of",
    "start": "1600950",
    "end": "1609170"
  },
  {
    "text": "eigenvalues smaller\nthan mu S 2. Of course, we would find that\nthey are one more eigenvalue",
    "start": "1609170",
    "end": "1618580"
  },
  {
    "text": "smaller than mu S 2. Then mu S 1, namely this one. So we know there's one\neigenvalue in between mu S 1",
    "start": "1618580",
    "end": "1626149"
  },
  {
    "text": "and mu S 2. Now we could bisect this\ninterval, and we would find that, in fact, there is still\none more eigenvalue smaller",
    "start": "1626150",
    "end": "1636080"
  },
  {
    "text": "than mu S 3, then there was\nsmaller than mu S 1. Because this eigenvalue\nstill lies on the left",
    "start": "1636080",
    "end": "1643750"
  },
  {
    "text": "side of mu S 3. So we bisect again,\nwe get mu S 4. Notice this distance here, is\nequal to that distance here.",
    "start": "1643750",
    "end": "1653550"
  },
  {
    "text": "And now, we would find that the\neigenvalue still lies in between these two lines, between\nmu S 3 and mu S 4.",
    "start": "1653550",
    "end": "1660780"
  },
  {
    "text": "And like that. we could\ncontinue bisecting the interval until we finally find a\nvery small interval in which",
    "start": "1660780",
    "end": "1668400"
  },
  {
    "text": "this eigenvalue must lie. We, of course, have to take care\nin the L D L transpose",
    "start": "1668400",
    "end": "1674490"
  },
  {
    "text": "factorization, because we are\nnot working on a positive definite matrix. The multipliers must be small,\nthe L, I, J elements must",
    "start": "1674490",
    "end": "1683800"
  },
  {
    "text": "remain small in the\nfactorization. But provided that is the case,\nthe round of errors are small,",
    "start": "1683800",
    "end": "1689700"
  },
  {
    "text": "and the procedure\nis very stable. However, convergence can be very\nslow, can be extremely",
    "start": "1689700",
    "end": "1695100"
  },
  {
    "text": "slow because you have to cut\ndown the intervals further and further until you finally get\na very small interval, in",
    "start": "1695100",
    "end": "1700990"
  },
  {
    "text": "which you now know the\neigenvalue does lie. Of course, at some point, you\nmight want to switch to",
    "start": "1700990",
    "end": "1706340"
  },
  {
    "text": "another iteration scheme. Once you know there is an\neigenvalue in between say mu S 3 and mu S 4, you might want\nto switch to a iteration",
    "start": "1706340",
    "end": "1714100"
  },
  {
    "text": "scheme that converges to this\nparticular value much faster. Finally, a whole class of\nmethods that I would like to",
    "start": "1714100",
    "end": "1721570"
  },
  {
    "start": "1717000",
    "end": "2054000"
  },
  {
    "text": "briefly mention to you are the\nmethods of transformations, where we are operating on\nthis eigenvalue problem,",
    "start": "1721570",
    "end": "1729230"
  },
  {
    "text": "recognizing that phi transpose\nK phi is equal to lambda. And phi transpose M phi is\nequal to I. Where phi is",
    "start": "1729230",
    "end": "1739170"
  },
  {
    "text": "defined as shown here, lambda\nis defined as shown here. This fact, of course, we use\nalready earlier in the",
    "start": "1739170",
    "end": "1745960"
  },
  {
    "text": "[? multiple ?] position analysis. Now how can we get\nphi and lambda?",
    "start": "1745960",
    "end": "1752020"
  },
  {
    "text": "That is our objective. Well, if we know that this\nmatrix diagonalizes the K",
    "start": "1752020",
    "end": "1757900"
  },
  {
    "text": "matrix, and it also diagonalizes\nthe M matrix, then why not try to construct\nit by iteration?",
    "start": "1757900",
    "end": "1765370"
  },
  {
    "text": "And this is the basic idea in\nthe transformation methods. What we do is the following-- we\ntake the original K matrix,",
    "start": "1765370",
    "end": "1773410"
  },
  {
    "text": "we are operating with\nP1 transpose K P1. On K, and on M, such as to make\nthis resultant matrix",
    "start": "1773410",
    "end": "1785059"
  },
  {
    "text": "here closer to a\ndiagonal form. Ultimately, we want to get\na diagonal matrix.",
    "start": "1785060",
    "end": "1792470"
  },
  {
    "text": "Well, one step in the iteration\nis to make this P1 transpose K P1 matrix, and P1\ntranspose M P1 matrix, both of",
    "start": "1792470",
    "end": "1800610"
  },
  {
    "text": "these matrices a little closer\nto diagonal form. Then K and M were\nby themselves.",
    "start": "1800610",
    "end": "1806030"
  },
  {
    "text": "So this is the first step. Now, we have here, let's call\nthis a K2 matrix, and this",
    "start": "1806030",
    "end": "1812360"
  },
  {
    "text": "here an M2 matrix. ",
    "start": "1812360",
    "end": "1818760"
  },
  {
    "text": "Now, we are operating with P2\ntranspose on K2, and also there's a P2 here.",
    "start": "1818760",
    "end": "1824400"
  },
  {
    "text": "So let me take a different color\nto show that now we are operating with this part here,\nand that part there.",
    "start": "1824400",
    "end": "1834020"
  },
  {
    "text": " And this, of course, we are\ndoing in such a way as to have",
    "start": "1834020",
    "end": "1841559"
  },
  {
    "text": "that P2 transposed, K2, P2\nis a little bit closer to",
    "start": "1841560",
    "end": "1847730"
  },
  {
    "text": "diagonal form than was K2. I hope you can see these\nlines-- let me put",
    "start": "1847730",
    "end": "1855320"
  },
  {
    "text": "them in once more. What I'm saying is we have K2\nafter having applied P1",
    "start": "1855320",
    "end": "1861720"
  },
  {
    "text": "transpose on P1, on K,\nand similar for M.",
    "start": "1861720",
    "end": "1867200"
  },
  {
    "text": "And now we want to make K2 still\na little bit closer to diagonal form.",
    "start": "1867200",
    "end": "1872380"
  },
  {
    "text": "Well, we are now applying P2\ntransposed on K2 and the P2 on",
    "start": "1872380",
    "end": "1877780"
  },
  {
    "text": "K2, this one here has been\nreplaced by K2, this one here has been replaced by M2.",
    "start": "1877780",
    "end": "1883240"
  },
  {
    "text": "And what we're saying is that\nthis final result, in this final result should be closer\nto diagonal form.",
    "start": "1883240",
    "end": "1890760"
  },
  {
    "text": "In fact, this should be closer\nto I matrix, and this should be closer to the lambda\nmatrix, ideally.",
    "start": "1890760",
    "end": "1896790"
  },
  {
    "text": "Well, this is the basic process\nthat is used in the generalized Jacobi method.",
    "start": "1896790",
    "end": "1903750"
  },
  {
    "text": "We can show that if\nwe choose specific matrices, P I matrices.",
    "start": "1903750",
    "end": "1910770"
  },
  {
    "text": "That zero always one [? of ?]\ndiagonal element. In other words, the P I matrix,\nzeroes a particular",
    "start": "1910770",
    "end": "1918970"
  },
  {
    "text": "off diagonal element in this\ncurrent coefficient matrix, similarly for the M2, for the\ncurrent M coefficient matrix.",
    "start": "1918970",
    "end": "1929320"
  },
  {
    "text": "Then we know in the generalized\nJacobi method that the process finally converges.",
    "start": "1929320",
    "end": "1935795"
  },
  {
    "text": "It converges and we get a\ndiagonal matrix here.",
    "start": "1935795",
    "end": "1941070"
  },
  {
    "text": "With proper scaling, it will\nbe the identity matrix. And here, we are getting also a\ndiagonal matrix, and if this",
    "start": "1941070",
    "end": "1947670"
  },
  {
    "text": "is an identity matrix, this will\nactually be the matrix storing the eigenvalues.",
    "start": "1947670",
    "end": "1953330"
  },
  {
    "text": "Well, this is the generalized\nJacobi method here. They are other techniques that\ncan also be used, of course,",
    "start": "1953330",
    "end": "1959270"
  },
  {
    "text": "this is a very common approach\ntaken when we have M being",
    "start": "1959270",
    "end": "1964990"
  },
  {
    "text": "equal to the identity matrix. However, the disadvantage of\nthis approach is that we are",
    "start": "1964990",
    "end": "1970270"
  },
  {
    "text": "calculating all eigen pairs\nsimultaneously. In finite element analysis,\nwe are only interested in",
    "start": "1970270",
    "end": "1975570"
  },
  {
    "text": "specific eigenvalues, so lowest\nP, or a certain number of eigenvalues in an interval.",
    "start": "1975570",
    "end": "1981880"
  },
  {
    "text": "Here, we have to calculate all\nof the eigenvalues stored in the lambda matrix, and all\nof the eigenvectors.",
    "start": "1981880",
    "end": "1988790"
  },
  {
    "text": "And the eigenvectors here, would\nbe these vectors here.",
    "start": "1988790",
    "end": "1994676"
  },
  {
    "text": "These are the the matrix phi,\nwhich is P1 times P2, up to P",
    "start": "1994676",
    "end": "2001692"
  },
  {
    "text": "K, after K steps of iteration. This gives us the matrix\nphi, storing all of the",
    "start": "2001692",
    "end": "2008295"
  },
  {
    "text": "eigenvectors. So the disadvantage is that we\nare calculating all of that",
    "start": "2008295",
    "end": "2013860"
  },
  {
    "text": "eigenvectors, which in finite\nelement analysis is hardly ever required. All we want is the total number\nof eigenvalues and",
    "start": "2013860",
    "end": "2023810"
  },
  {
    "text": "eigenvectors. Remember, also if K\nis of [? order ?] 1,000, this would be a\ntremendous amount of work",
    "start": "2023810",
    "end": "2031309"
  },
  {
    "text": "involved and, in fact, it would\nbe beyond the current state of computing capabilities\nto use a",
    "start": "2031310",
    "end": "2037060"
  },
  {
    "text": "generalized Jacobi method on a\nK matrix 1,000x1,000 and an M",
    "start": "2037060",
    "end": "2042700"
  },
  {
    "text": "matrix 1,000x1,000. You will certainly stretch the\ncurrent computing capabilities",
    "start": "2042700",
    "end": "2048320"
  },
  {
    "text": "tremendously by using the\ngeneralized Jacobi method on large eigenproblems.",
    "start": "2048320",
    "end": "2055080"
  },
  {
    "start": "2054000",
    "end": "2132000"
  },
  {
    "text": "For large eigenproblems, it is\nreally best to combine the",
    "start": "2055080",
    "end": "2060199"
  },
  {
    "text": "basic approaches that\nI just mentioned. Let's recall the basic\napproaches. There were iteration methods,\nvector iteration methods,",
    "start": "2060199",
    "end": "2068919"
  },
  {
    "text": "there were Sturm sequence\nmethods, there was the transformation method.",
    "start": "2068920",
    "end": "2075580"
  },
  {
    "text": "And then, of course, there were polynomial iteration methods. So really, four classes\nof methods that",
    "start": "2075580",
    "end": "2081980"
  },
  {
    "text": "we are talking about. Each of them, we find,\nhave some advantages. Well, as engineer, surely we\nwant to now combine the",
    "start": "2081980",
    "end": "2090888"
  },
  {
    "text": "advantages of each of these\nmethods to come up with optimum techniques for our\nspecific problems.",
    "start": "2090889",
    "end": "2097430"
  },
  {
    "text": "Well, there are a number\nof thoughts that we would go through. First of all, a determinant\nsearch, a polynomial iteration",
    "start": "2097430",
    "end": "2105599"
  },
  {
    "text": "method would be effective\nto get near a root. Vector iteration then could\nbe used to calculate an",
    "start": "2105600",
    "end": "2112930"
  },
  {
    "text": "eigenvalue and an eigenvector. The transformation method would\nbe good to orthogonalize",
    "start": "2112930",
    "end": "2118180"
  },
  {
    "text": "the current iteration vectors\nif we iterate with more than just one vector.",
    "start": "2118180",
    "end": "2123280"
  },
  {
    "text": "And the Sturm sequence method\nis extremely powerful to assure that we have actually\ncalculated the required",
    "start": "2123280",
    "end": "2130360"
  },
  {
    "text": "eigenvalues and eigenvectors. Well, the first method then that\nI would like to discuss",
    "start": "2130360",
    "end": "2136480"
  },
  {
    "start": "2132000",
    "end": "2477000"
  },
  {
    "text": "with you is the determinant\nsearch method. In this method, we have\ncombined some of these",
    "start": "2136480",
    "end": "2142370"
  },
  {
    "text": "techniques that I pointed\nout to you earlier in an optimum way. Here, we have the polynomial P\nlambda, we want to, of course,",
    "start": "2142370",
    "end": "2149760"
  },
  {
    "text": "solve for the roots of\nthat polynomial-- that is our objective. Here is lambda 1, there\nis lambda 2.",
    "start": "2149760",
    "end": "2155690"
  },
  {
    "text": "I also show a point,\nA, because I will refer to it just now.",
    "start": "2155690",
    "end": "2160750"
  },
  {
    "text": "The basic scheme in the\ndeterminant search method is to look for the zeroes of\nthe determinant, of the",
    "start": "2160750",
    "end": "2169140"
  },
  {
    "text": "determinant K minus mu I M. Of\ncourse, these zeroes give us R",
    "start": "2169140",
    "end": "2174260"
  },
  {
    "text": "equal to the eigenvalues. That's what we're interested\nin calculating. Well, we are calculating\nthe determinant by the",
    "start": "2174260",
    "end": "2180740"
  },
  {
    "text": "factorization of this matrix\ninto L D L transpose, and we notice that this determinant is\nsimply the product of the",
    "start": "2180740",
    "end": "2187990"
  },
  {
    "text": "[? D I I's, ?] as I\nmentioned earlier. Now, having a certain starting\nvalue, mu 0, say, and mu 1.",
    "start": "2187990",
    "end": "2196560"
  },
  {
    "text": "We can directly use the secant\niteration, that I mentioned to you earlier to get close\nto an eigenvalue.",
    "start": "2196560",
    "end": "2203700"
  },
  {
    "text": "Now, I also slipped in here\nan acceleration factor. This acceleration factor would\nbe equal to 1 in the normal",
    "start": "2203700",
    "end": "2212009"
  },
  {
    "text": "secant iteration that I showed\nto you earlier already. However, since we want to use\nthis polynomial iteration",
    "start": "2212010",
    "end": "2218540"
  },
  {
    "text": "technique only to get close to\nan eigenvalue, and we might",
    "start": "2218540",
    "end": "2224420"
  },
  {
    "text": "actually obtain with eta equal\nto 1, sometimes slow convergence. What we do is we simply\nset it larger to",
    "start": "2224420",
    "end": "2231810"
  },
  {
    "text": "accelerate the process. And this is what we're\ndoing here. If we do jump beyond an\neigenvalue because we have",
    "start": "2231810",
    "end": "2242600"
  },
  {
    "text": "accelerated the secant\niteration. In other words, if we do a jump\nacross here, then since",
    "start": "2242600",
    "end": "2249130"
  },
  {
    "text": "we are factorizing the matrix\nhere, we can also look at the",
    "start": "2249130",
    "end": "2255509"
  },
  {
    "text": "number of negative elements\nin the D matrix. These would tell us that we, in\nfact, have jumped across an",
    "start": "2255510",
    "end": "2263140"
  },
  {
    "text": "unknown eigenvalue. So, in this particular case, of\ncourse, there would be one",
    "start": "2263140",
    "end": "2269510"
  },
  {
    "text": "negative element in\nthe D matrix. And we would know now we\nhave jumped across. The jumping across an unknown\neigenvalue is not possible",
    "start": "2269510",
    "end": "2278480"
  },
  {
    "text": "when eta is equal to 1. However, it is possible when\neta is greater than 1.",
    "start": "2278480",
    "end": "2285610"
  },
  {
    "text": "And however, that jumping\ndoes not hurt us.",
    "start": "2285610",
    "end": "2291600"
  },
  {
    "text": "In fact, all we will find is\nthat the Sturm sequence count",
    "start": "2291600",
    "end": "2299540"
  },
  {
    "text": "will tell us that we have jumped\nacross an eigenvalue, and then, of course, we have to\ngo to another strategy of",
    "start": "2299540",
    "end": "2307000"
  },
  {
    "text": "solving for the eigenvalue\nmore accurately. But the important point is that\nall we want to do via",
    "start": "2307000",
    "end": "2313110"
  },
  {
    "text": "this procedure is to get into\nthe vicinity of the eigenvalue. We do not want to calculate it\naccurately, all we want to do",
    "start": "2313110",
    "end": "2320780"
  },
  {
    "text": "is get into the vicinity of\nan unknown eigenvalue. First, of course, lambda 1,\nthen lambda 2, and so on.",
    "start": "2320780",
    "end": "2329080"
  },
  {
    "text": "There's also a concern, of\ncourse, that we do not want to jump beyond A, because if we, in\nour process of looking for",
    "start": "2329080",
    "end": "2338309"
  },
  {
    "text": "lambda 1, we are jumping too\nclose to lambda 2, then via",
    "start": "2338310",
    "end": "2345090"
  },
  {
    "text": "the other solution techniques\nthat we are using later on then to calculate the eigenvalue\nmore accurately, we would actually converge\nto lambda 2.",
    "start": "2345090",
    "end": "2351430"
  },
  {
    "text": "And, of course, that we\ndon't want, we want to calculate lambda 1. ",
    "start": "2351430",
    "end": "2356940"
  },
  {
    "text": "So, the process is then to start\noff with eta equal to 1,",
    "start": "2356940",
    "end": "2362950"
  },
  {
    "text": "iterate, see how we are\nprogressing towards an unknown eigenvalue. If that progressing is\nvery slow, we are",
    "start": "2362950",
    "end": "2369410"
  },
  {
    "text": "choosing eta larger. There are specific strategies\nthat we're using to choose the",
    "start": "2369410",
    "end": "2374470"
  },
  {
    "text": "right amount all eta. And until we find that we are\nclose to an eigenvalue, either",
    "start": "2374470",
    "end": "2381670"
  },
  {
    "text": "from the left or we have jumped\nacross it already, but you should never\njumped to far.",
    "start": "2381670",
    "end": "2387940"
  },
  {
    "text": "And we are making sure that we\nnever jump beyond that point A. and having jumped, we find\nthat the Sturm sequence count",
    "start": "2387940",
    "end": "2394930"
  },
  {
    "text": "tells us about it. And now, we are going on to\nanother strategy, and that",
    "start": "2394930",
    "end": "2403309"
  },
  {
    "text": "strategy is the vector\niteration, vector inverse iteration method that I\ndiscussed with you earlier.",
    "start": "2403310",
    "end": "2409040"
  },
  {
    "text": "What we are doing now, is we\nhave a new coefficient matrix, that's the one. We are sitting here, say, and\nnow I have assumed that we",
    "start": "2409040",
    "end": "2415550"
  },
  {
    "text": "have, in fact, jumped. It's not necessary. We might have also approached\nfrom this side and come very",
    "start": "2415550",
    "end": "2420980"
  },
  {
    "text": "close to lambda J. And now, we\nare using vector iteration to",
    "start": "2420980",
    "end": "2427010"
  },
  {
    "text": "get this eigenvalue\naccurately. This coefficient here,\nor this value here",
    "start": "2427010",
    "end": "2435150"
  },
  {
    "text": "corresponds to that length. And if you are adding this value\nhere to this shift, we",
    "start": "2435150",
    "end": "2442700"
  },
  {
    "text": "are getting an approximation\nto lambda J. And that, of course, is our objective. Once we have lambda J, we have\ncalculated this eigenvalue--",
    "start": "2442700",
    "end": "2451680"
  },
  {
    "text": "the eigenvector comes out, in x\nk plus 1 as k increases, so",
    "start": "2451680",
    "end": "2456960"
  },
  {
    "text": "we have calculated this eigen\npair, and now, of course, we would use deflation method, just\nthe way I showed it to",
    "start": "2456960",
    "end": "2463230"
  },
  {
    "text": "your earlier, for the polynomial\niteration method. To deflate the polynomial of\nthis value, implicitly--",
    "start": "2463230",
    "end": "2471010"
  },
  {
    "text": "implicitly, that's important-- and we would continue our\niteration process towards that eigenvalue next.",
    "start": "2471010",
    "end": "2478640"
  },
  {
    "start": "2477000",
    "end": "2834000"
  },
  {
    "text": "In this process, we also want\nto make sure that the iteration vector is deflated\nof the previously",
    "start": "2478640",
    "end": "2484160"
  },
  {
    "text": "eigenvectors, for example,\nGram-Schmidt orthogonalization. If the convergence is slow in\nthis particular iteration",
    "start": "2484160",
    "end": "2492800"
  },
  {
    "text": "here, we're using also the\nreally Rayleigh-quotient iteration to speed up\niteration itself.",
    "start": "2492800",
    "end": "2500309"
  },
  {
    "text": "The advantage of the method--\nwell, it calculates only the eigen pairs that are actually\nrequired., there's no prior",
    "start": "2500310",
    "end": "2508260"
  },
  {
    "text": "transformation of the\neigenprobelm necessary. The disadvantage of the method\nis that we have to perform",
    "start": "2508260",
    "end": "2513970"
  },
  {
    "text": "many triangular factorizations. Therefore, the method is really\nonly effective for small banded systems.",
    "start": "2513970",
    "end": "2520760"
  },
  {
    "text": "If we want to calculate\neigenvalues of large banded systems, we really need\nan algorithm with less",
    "start": "2520760",
    "end": "2527860"
  },
  {
    "text": "factorizations, and more\nvector iterations. And that is the subspace\niteration method.",
    "start": "2527860",
    "end": "2534240"
  },
  {
    "text": "In fact, we have now extended\nthe subspace iteration method to be also most effective for\nsmall banded systems.",
    "start": "2534240",
    "end": "2540590"
  },
  {
    "text": "And I will just refer to\nit very briefly later. The basic steps of the subspace\niteration method are",
    "start": "2540590",
    "end": "2546940"
  },
  {
    "text": "the following. Here, we have the first\nequation used.",
    "start": "2546940",
    "end": "2553089"
  },
  {
    "text": "We have on the right\nhand side, the current iteration vectors. Now, notice that in this\nparticular case, we're",
    "start": "2553090",
    "end": "2558290"
  },
  {
    "text": "iterating with q vectors,\nwhere q ip larger than p simultaneously.",
    "start": "2558290",
    "end": "2564350"
  },
  {
    "text": "This gives us here, q right\nhand side vector. We are solving for\nq vectors here.",
    "start": "2564350",
    "end": "2572200"
  },
  {
    "text": "This is an inverse iteration\nstep on q vector simultaneously. Then we are calculating the\nprojections of these k and m",
    "start": "2572200",
    "end": "2580860"
  },
  {
    "text": "matrices on these vectors. Notice that this pat here,\nof course, is equal to",
    "start": "2580860",
    "end": "2587750"
  },
  {
    "text": "this right hand side. So we would not calculate\nthis here. We simply use the right hand\nside, which we have evaluated",
    "start": "2587750",
    "end": "2593290"
  },
  {
    "text": "already and plug it\nright in there. But having calculated these two\nmatrices, which are now of",
    "start": "2593290",
    "end": "2599390"
  },
  {
    "text": "order q by q, we are solving\neigenvalue problem all of these q by q matrices.",
    "start": "2599390",
    "end": "2606109"
  },
  {
    "text": "And the solution of\nthat eigenvalue problem is shown here. Notice this matrix here\nis a q by q matrix.",
    "start": "2606110",
    "end": "2614030"
  },
  {
    "text": "We're using q vectors. This is a q by q matrix storing\nthe eigenvectors of",
    "start": "2614030",
    "end": "2620970"
  },
  {
    "text": "this eigen problem here, of the\neigenproblem corresponding to these matrices.",
    "start": "2620970",
    "end": "2626510"
  },
  {
    "text": "This here is the diagonal matrix\nlisting the eigenvalues of the eigenproblem",
    "start": "2626510",
    "end": "2632830"
  },
  {
    "text": "corresponding to these matrices. Having calculated these matrices\nhere, and of course,",
    "start": "2632830",
    "end": "2638930"
  },
  {
    "text": "that one too, we are performing\nthis operation to get a new set of iteration\nvectors, which then are",
    "start": "2638930",
    "end": "2645390"
  },
  {
    "text": "plugged back into here. Notice that the solution of\nthis eigenproblem, q by q,",
    "start": "2645390",
    "end": "2652579"
  },
  {
    "text": "might be a 50x50 matrix here,\n100x100 matrix, maybe a",
    "start": "2652580",
    "end": "2657660"
  },
  {
    "text": "200x200, but that already would\nbe a large number of vectors if we're using-- a very large number\nof vectors.",
    "start": "2657660",
    "end": "2663730"
  },
  {
    "text": "This solution of this\neigenproblem can be effectively achieved via the\ngeneralized Jacobi method,",
    "start": "2663730",
    "end": "2669290"
  },
  {
    "text": "which I referred to\nbriefly earlier. Of course here, we are using\nour solution algorithm for",
    "start": "2669290",
    "end": "2675515"
  },
  {
    "text": "static analysis. There's L D L transpose\nfactorization involved here.",
    "start": "2675515",
    "end": "2681200"
  },
  {
    "text": "And here, this is a simple\nmatrix multiplication that has to be carried out.",
    "start": "2681200",
    "end": "2686880"
  },
  {
    "text": "This part here, is the\neigenproblem solution, and there's another vector multiplication carried out here.",
    "start": "2686880",
    "end": "2693980"
  },
  {
    "text": "Now, under specific conditions,\nthis eigensolution",
    "start": "2693980",
    "end": "2699320"
  },
  {
    "text": "algorithm converges and the\nvectors stored in x k plus 1,",
    "start": "2699320",
    "end": "2704690"
  },
  {
    "text": "they are q vectors now here,\nwith appropriate ordering, converge to the phi matrix.",
    "start": "2704690",
    "end": "2711750"
  },
  {
    "text": "This is now an n by q matrix,\nstoring the lowest",
    "start": "2711750",
    "end": "2717800"
  },
  {
    "text": "eigenvectorr, or I should\nsay, the q eigenvectors corresponding to the\nlowest eigenvalues.",
    "start": "2717800",
    "end": "2725020"
  },
  {
    "text": " And here, we have it written\nout, and of course, this is a",
    "start": "2725020",
    "end": "2730820"
  },
  {
    "text": "diagonal matrix. The condition that we have to\nsatisfy here is that the starting subspace spanned by\nthese vectors here must not be",
    "start": "2730820",
    "end": "2742800"
  },
  {
    "text": "orthogonal to the subspace\nspanned by the p eigenvectors",
    "start": "2742800",
    "end": "2752840"
  },
  {
    "text": "that we are interested in. Of course, here, I'm talking\nabout q eigenvectors.",
    "start": "2752840",
    "end": "2759160"
  },
  {
    "text": "I really am only interested in\np eigenvectors, in the lowest p eigenvectors.",
    "start": "2759160",
    "end": "2765510"
  },
  {
    "text": "So in general, what happens is\nthat I will only be interested in these eigenvectors here, and\nthe lowest eigenvalues.",
    "start": "2765510",
    "end": "2773079"
  },
  {
    "text": "One comes out here, I'm not\ntoo much concerned about. In fact, we are carrying these\nalong only to accelerate the",
    "start": "2773080",
    "end": "2781820"
  },
  {
    "text": "iteration scheme. And I will show you just know\nwhat the convergence rate is,",
    "start": "2781820",
    "end": "2788060"
  },
  {
    "text": "and why we are carrying\nthem along. So since we only interested in\nthe lowest p eigenvalues and",
    "start": "2788060",
    "end": "2794800"
  },
  {
    "text": "corresponding eigenvectors,\nwe want to get convergence towards those, and what that\nmeans is that our starting",
    "start": "2794800",
    "end": "2808330"
  },
  {
    "text": "vectors in here must not be m\northogonal to the eigenvectors",
    "start": "2808330",
    "end": "2817290"
  },
  {
    "text": "that we want to calculate,\nphi 1 to phi p. If that condition is satisfied,\none can show,",
    "start": "2817290",
    "end": "2822960"
  },
  {
    "text": "theoretically, and of course,\nwe see it in practice also, that this iteration scheme will\nalways converge to the",
    "start": "2822960",
    "end": "2829230"
  },
  {
    "text": "lowest eigenvectors and\ncorresponding eigenvalues. ",
    "start": "2829230",
    "end": "2836010"
  },
  {
    "start": "2834000",
    "end": "2969000"
  },
  {
    "text": "One important point here, once\nwe have calculated, say, a certain number of iteration\nvectors, and we have tested",
    "start": "2836010",
    "end": "2844560"
  },
  {
    "text": "for convergence, we are saying\nthat convergence is reached, say, when the current iterates\non the eigenvalues don't",
    "start": "2844560",
    "end": "2852670"
  },
  {
    "text": "change very much anymore. This tol might be 10 to the\nminus 6, as an example.",
    "start": "2852670",
    "end": "2858720"
  },
  {
    "text": "Then once this condition is\nsatisfied, we stop the iteration, and we really want\nto make then sure that we",
    "start": "2858720",
    "end": "2866440"
  },
  {
    "text": "really converge to the\neigenvalues of interest. And how is that accomplished? Well, if these are the\np eigenvalues, we are",
    "start": "2866440",
    "end": "2873310"
  },
  {
    "text": "calculating error bounds on\nthese eigenvalues, indicated here by the blue lines here.",
    "start": "2873310",
    "end": "2881690"
  },
  {
    "text": "And then just to the right\nof the error bound. Or we can really use that error\nbound, we apply a Sturm",
    "start": "2881690",
    "end": "2891299"
  },
  {
    "text": "sequence shift, mu S. What this\nthen says that k minus mu S factorize into L\nD L transpose.",
    "start": "2891300",
    "end": "2898150"
  },
  {
    "text": " And we must now have in the D\nmatrix, p negative elements.",
    "start": "2898150",
    "end": "2909870"
  },
  {
    "text": "And only p, not more, not less,\njust p negative elements",
    "start": "2909870",
    "end": "2915390"
  },
  {
    "text": "must be occurring\nin the D matrix. This check is absolutely sure.",
    "start": "2915390",
    "end": "2924410"
  },
  {
    "text": "If this check a satisfied, we\ncan be absolutely sure that we have calculated the eigenvalues\nof interest, the",
    "start": "2924410",
    "end": "2930970"
  },
  {
    "text": "lowest p eigenvalues, in\nthis particular case. So I repeat, after we have\nsatisfied this convergence",
    "start": "2930970",
    "end": "2937910"
  },
  {
    "text": "rate, or this conversions\nlimit, I should say--",
    "start": "2937910",
    "end": "2943510"
  },
  {
    "text": "typically 10 to the minus\n6, or 10 to the minus 8, depending on what accuracy\nyou want.",
    "start": "2943510",
    "end": "2948700"
  },
  {
    "text": "We calculating eigenvalue\nbounds, as shown here. We are putting a mu S Sturm\nsequence shift just to the",
    "start": "2948700",
    "end": "2958500"
  },
  {
    "text": "right of the last eigenvalue\nbound, and if this is the p's",
    "start": "2958500",
    "end": "2964460"
  },
  {
    "text": "eigenvalue, we must have\nhere, exactly p negative elements indeed.",
    "start": "2964460",
    "end": "2970670"
  },
  {
    "start": "2969000",
    "end": "3109000"
  },
  {
    "text": "The convergence rate of the\niteration, we find is given by lambda i divided by\nlambda q plus 1.",
    "start": "2970670",
    "end": "2978109"
  },
  {
    "text": " And the convergence rate to the\neigenvalues of interest",
    "start": "2978110",
    "end": "2984650"
  },
  {
    "text": "are just this value squared. Notice that we have\nhere, q plus 1.",
    "start": "2984650",
    "end": "2990279"
  },
  {
    "text": "So if we are interested in the\np's eigenvalue,== the black pen doesn't work too well, let\nme take the blue one here--",
    "start": "2990280",
    "end": "2996850"
  },
  {
    "text": "if you're interested in the p's\neigenvalue as the largest eigenvalue, notice that the\nconvergence rate is that",
    "start": "2996850",
    "end": "3004220"
  },
  {
    "text": "lambda p plus-- lambda p divided\nby lambda q plus 1. That is the worst convergence\nrate that we are reaching",
    "start": "3004220",
    "end": "3011030"
  },
  {
    "text": "because the other eigenvalues\nbelow lambda p are smaller, giving us better convergence\nrate.",
    "start": "3011030",
    "end": "3016740"
  },
  {
    "text": "So this is the worst that we\ncan reach, if we are only",
    "start": "3016740",
    "end": "3022110"
  },
  {
    "text": "interested in p eigenvalues\nand corresponding eigenvectors. Notice however, that lambda\nq plus 1 must be",
    "start": "3022110",
    "end": "3029850"
  },
  {
    "text": "greater than lambda p. If lambda q plus 1 is\nequal to lambda p, we would never converge.",
    "start": "3029850",
    "end": "3034880"
  },
  {
    "text": "So this is the reason why we\nare choosing q to be larger than p, substantially\nlarge than p.",
    "start": "3034880",
    "end": "3042030"
  },
  {
    "text": "For example, we might choose\nq to be typically, as an",
    "start": "3042030",
    "end": "3049640"
  },
  {
    "text": "example, q being the maximum\nof 2p and p plus 8.",
    "start": "3049640",
    "end": "3057849"
  },
  {
    "text": "This is a formula that\nwe have used with quite a lot of success. In other words, when p is equal\nto 4, we would use--",
    "start": "3057850",
    "end": "3065920"
  },
  {
    "start": "3065920",
    "end": "3071059"
  },
  {
    "text": "no, sorry, I should say\nthe minimum here. the In other words, when\np is equal to 4, we",
    "start": "3071060",
    "end": "3079099"
  },
  {
    "text": "would have 8 here. When p is equal to\n8, it's the same.",
    "start": "3079100",
    "end": "3084410"
  },
  {
    "text": "Because these two values\ngive us the same. And when p is equal to 100,\nthen we just have 8 more.",
    "start": "3084410",
    "end": "3090470"
  },
  {
    "text": "Q is equal to 108. This 8 here is a safeguard, so\nthat this cannot be equal to 1",
    "start": "3090470",
    "end": "3097870"
  },
  {
    "text": "in practice. It would only be equal to 1 if\nwe would have, basically.",
    "start": "3097870",
    "end": "3103960"
  },
  {
    "text": "9 time the same root, and of\ncourse, that is a case which we would hardly encounter\nin practice.",
    "start": "3103960",
    "end": "3110289"
  },
  {
    "start": "3109000",
    "end": "3453000"
  },
  {
    "text": "Now, regarding the choice\nof the starting values. For the vectors, there\nare two choices.",
    "start": "3110290",
    "end": "3117860"
  },
  {
    "text": "x1 is simply a vector\nwith 1 only.",
    "start": "3117860",
    "end": "3123070"
  },
  {
    "text": "xj would be a vector with zeroes\neverywhere, but with a",
    "start": "3123070",
    "end": "3130220"
  },
  {
    "text": "1 somewhere. And that one is in\nthe j's entry-- ",
    "start": "3130220",
    "end": "3135590"
  },
  {
    "text": "sorry, the k's entry. The k's entry for ek, ek has\nin the case, entry of 1.",
    "start": "3135590",
    "end": "3143050"
  },
  {
    "text": "And otherwise, everywhere\nzeroes. This would be the second to\nthe q minus first vector.",
    "start": "3143050",
    "end": "3149390"
  },
  {
    "text": "And the last vector, we\ntake a random vector. That is one choice of\nstarting vectors. The Lanczos method can also be\nused to generate starting",
    "start": "3149390",
    "end": "3156310"
  },
  {
    "text": "vectors very effectively.  Once we have selected the\nstarting vector, we go through",
    "start": "3156310",
    "end": "3165450"
  },
  {
    "text": "the iteration schemes the way\nI have been displaying it. And there, it is important, once\nagain, to perform after",
    "start": "3165450",
    "end": "3173420"
  },
  {
    "text": "the convergence- the Sturm\nsystem sequence check, and also we might want to calculate\nerror bounds on the",
    "start": "3173420",
    "end": "3180610"
  },
  {
    "text": "eigenvalues and eigenvectors. These error bounds would be\ncalculated by simply saying, well, if we want to satisfy--",
    "start": "3180610",
    "end": "3188070"
  },
  {
    "text": "or if we want to solve for\nlambdas and phis that satisfies this equation,\nthen why not see how",
    "start": "3188070",
    "end": "3197400"
  },
  {
    "text": "close we will satisfy. Make that a minus and put here\nan error, say, R, on",
    "start": "3197400",
    "end": "3203779"
  },
  {
    "text": "the right hand side. Substituting then the last\nvalues for lambda and phi that we just kind calculated, the\nlast iterative values, we get",
    "start": "3203780",
    "end": "3212220"
  },
  {
    "text": "this top line here. We're taking a norm\nthe way I've been talking about earlier.",
    "start": "3212220",
    "end": "3218310"
  },
  {
    "text": "And we are also taking a norm\nat the bottom to get,",
    "start": "3218310",
    "end": "3223730"
  },
  {
    "text": "basically, a relative value,\ntop to bottom. And then relative value\ngives us epsilon i.",
    "start": "3223730",
    "end": "3229240"
  },
  {
    "text": "So epsilon i should be of order,\n10 to the minus 3, if we want to have three-digit\naccuracy in the eigenvalues",
    "start": "3229240",
    "end": "3236740"
  },
  {
    "text": "and eigenvectors. In fact, the eigenvalues will\ngenerally be more accurate than the eigenvectors.",
    "start": "3236740",
    "end": "3242040"
  },
  {
    "text": "That is, of course, a\nvery low accuracy. Ideally, we have much\nmore accuracy after the iteration schemes.",
    "start": "3242040",
    "end": "3247330"
  },
  {
    "text": "We would have to tighten the\ntolerance, the tol value that I referred to earlier\nsufficiently to make epsilon i",
    "start": "3247330",
    "end": "3255010"
  },
  {
    "text": "small enough. Now, I like to refer, very\nbriefly also, to an accelerated subspace iteration\nmethod that we have been",
    "start": "3255010",
    "end": "3261740"
  },
  {
    "text": "developing during the last\nyears, which is a very",
    "start": "3261740",
    "end": "3266800"
  },
  {
    "text": "substantial extension of this\nsubspace iteration method. The idea here is, basically,\nthat we wanted to combine the",
    "start": "3266800",
    "end": "3276080"
  },
  {
    "text": "sum of the effective techniques\nthat we are using in the polynomial iteration\nmethod, with the standard",
    "start": "3276080",
    "end": "3284115"
  },
  {
    "text": "subspace iteration method\nthat I have discussed with you here. Basically, in the accelerated\nsubspace iteration method, we",
    "start": "3284115",
    "end": "3291710"
  },
  {
    "text": "are not keeping a constant\ncoefficient matrix, as we have done here.",
    "start": "3291710",
    "end": "3296870"
  },
  {
    "text": "The k matrix never changed. Here, we are shifting the\ncoefficient matrix, we are",
    "start": "3296870",
    "end": "3302640"
  },
  {
    "text": "shifting through the spectrum. Much in the same way as we are\npursuing it in the determined",
    "start": "3302640",
    "end": "3308859"
  },
  {
    "text": "search method. And that is number one. And number two-- we are also\nnot iterating with vectors",
    "start": "3308860",
    "end": "3315720"
  },
  {
    "text": "that are always larger than\nthe number of eigenvectors that we want to calculated. In fact, if we want to\ncalculate, say, 100",
    "start": "3315720",
    "end": "3322990"
  },
  {
    "text": "eigenvalues, we might just use\nfour iteration vectors at a time, and shift through the\nspectrum, always with four",
    "start": "3322990",
    "end": "3331420"
  },
  {
    "text": "iteration vectors, capturing\nthe eigenvalues values and eigenvectors that we are really interested in, in bundles.",
    "start": "3331420",
    "end": "3337800"
  },
  {
    "text": "So this is a way how we have\nvery substantially accelerated the standard subspace iteration\nmethod that I have",
    "start": "3337800",
    "end": "3345310"
  },
  {
    "text": "been talking to you\nabout just now. If you are interested in seeing\nsome solution times,",
    "start": "3345310",
    "end": "3352315"
  },
  {
    "text": "please we look at\nthis reference. Ladies and gentlemen, this\nthen brings me to the",
    "start": "3352315",
    "end": "3359960"
  },
  {
    "text": "conclusion of this lecture. And in fact, it also brings us\nto the conclusion of the whole",
    "start": "3359960",
    "end": "3365630"
  },
  {
    "text": "set of lectures that I have\nbeen presenting to you. These set up lectures have been\nbrief and very compact.",
    "start": "3365630",
    "end": "3374180"
  },
  {
    "text": "They've been a survey\nintroduction to the finite element method-- a very brief and compact\nintroduction and survey.",
    "start": "3374180",
    "end": "3381670"
  },
  {
    "text": "Brief because there are\nonly 12 lectures. Compact because I wanted to\npresent to you as much as",
    "start": "3381670",
    "end": "3387800"
  },
  {
    "text": "possible in these 12 lectures. And there is, of course, a lot\nof information that we could",
    "start": "3387800",
    "end": "3394390"
  },
  {
    "text": "have talked about. However, I hope that the\ninformation that I had given",
    "start": "3394390",
    "end": "3400559"
  },
  {
    "text": "in the lectures has been of\ninterest to you, and it will also be valuable to you in your\nwork and general decision",
    "start": "3400560",
    "end": "3412070"
  },
  {
    "text": "making process of whether you\nwant to use a finite element method, how you want to\nuse it, and so on.",
    "start": "3412070",
    "end": "3418119"
  },
  {
    "text": "I like to take the opportunity\nat this point to thank the Center of Advanced Engineering\nStudies for the terrific",
    "start": "3418120",
    "end": "3424670"
  },
  {
    "text": "effort they've made in\npreparing the tapes. I'm very happy that they have\nbeen cooperative with me in",
    "start": "3424670",
    "end": "3432150"
  },
  {
    "text": "the way they did, in the\npreparation of the tapes.",
    "start": "3432150",
    "end": "3437750"
  },
  {
    "text": "I also would like to thank you\nas listeners for your patience of listening to the tapes and,\nonce again, I hope you found",
    "start": "3437750",
    "end": "3445590"
  },
  {
    "text": "the tapes interesting and\nhave obtained some value information on them. Best of luck in your further\nstudies of the",
    "start": "3445590",
    "end": "3451590"
  },
  {
    "text": "finite element method. ",
    "start": "3451590",
    "end": "3453690"
  }
]