[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6920"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6920",
    "end": "13410"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "13410",
    "end": "18760"
  },
  {
    "text": " PROFESSOR RABBAH: OK, so today's\nthe last lecture day we're going to talk about\nthe raw architecture.",
    "start": "18760",
    "end": "25150"
  },
  {
    "text": "This is a processor that was\nbuilt here at MIT and essentially trailblazed a lot\nof the research in terms of",
    "start": "25150",
    "end": "30650"
  },
  {
    "text": "parallel architectures for\nmulticores, compilation for multicores, programming\nlanguage and so on.",
    "start": "30650",
    "end": "37160"
  },
  {
    "text": "So you've heard some things\nabout RAW and the parallelizing technology\nin terms of StreamIt.",
    "start": "37160",
    "end": "42200"
  },
  {
    "text": "So we're going to cover some of\nthat again here today just briefly and give you little\nbit more insight into what",
    "start": "42200",
    "end": "48460"
  },
  {
    "text": "went into the design of\nthe raw architecture. So these are RAW chips\nthey were delivered",
    "start": "48460",
    "end": "54150"
  },
  {
    "start": "50000",
    "end": "365000"
  },
  {
    "text": "in October of 2002. Each one of these has\n16 processors on it.",
    "start": "54150",
    "end": "59920"
  },
  {
    "text": "I'm going to show you sort of\na diagram on the next slide. It's really a tiled\nmicroprocessor.",
    "start": "59920",
    "end": "65900"
  },
  {
    "text": "We'll get into what that means\nand how it actually-- what does a tiled microprocessor\ngive you that",
    "start": "65900",
    "end": "71930"
  },
  {
    "text": "makes it an attractive\ndesign point in the architecture space? Each of the raw tiles-- you can\nsort of see the outline",
    "start": "71930",
    "end": "78640"
  },
  {
    "text": "here sort of replicates-- is four millimeters. It's four millimeters square.",
    "start": "78640",
    "end": "85799"
  },
  {
    "text": "It's a single-issue\n8-stage pipeline. It has local memory, so\nthere's a 32K cache.",
    "start": "85800",
    "end": "92820"
  },
  {
    "text": "And the unique aspect of the raw\nprocessor is that is has a lot of on-chip networks that you\ncould use to orchestrate",
    "start": "92820",
    "end": "98980"
  },
  {
    "text": "communication between\nprocessors. So there's two operand\nnetworks. I'm going to get into\nwhat that means and",
    "start": "98980",
    "end": "104350"
  },
  {
    "text": "what they used for. But these eventually allow\nyou to do point-to-point communication between tiles\nwith very low latency.",
    "start": "104350",
    "end": "110260"
  },
  {
    "text": "And then there's a network that\nessentially allows you to handle cache misses and input\nand output and one for message",
    "start": "110260",
    "end": "116770"
  },
  {
    "text": "passings, a more dynamic style\nof messaging, something similar to what you're\naccustomed to at the cell, for",
    "start": "116770",
    "end": "123850"
  },
  {
    "text": "example, in DMA transfers. This was built in 180\nnanometer ASIC",
    "start": "123850",
    "end": "129570"
  },
  {
    "text": "technology by IBM. It's got 100 million\ntransistors. It was designed here by\nMIT grad students.",
    "start": "129570",
    "end": "136140"
  },
  {
    "text": "It's got something like\na million gates on it. Three to four years of\ndevelopment time.",
    "start": "136140",
    "end": "142270"
  },
  {
    "text": "And what was really interesting\nhere is that this was-- because of the tiled\nnature of the architecture,",
    "start": "142270",
    "end": "147590"
  },
  {
    "text": "you could just design one tile\nand then once you have one tile, you essentially just plop\ndown more and more and",
    "start": "147590",
    "end": "152850"
  },
  {
    "text": "more of them. And so you have one, you scale\nit out to 16 tiles. And the design sort of came back\nwithout any bugs when the",
    "start": "152850",
    "end": "159680"
  },
  {
    "text": "first chip was delivered. The core frequency was expected\nto run at 425--",
    "start": "159680",
    "end": "165760"
  },
  {
    "text": "I think lower than\n425 megahertz. AUDIENCE: Designed for 250?",
    "start": "165760",
    "end": "172430"
  },
  {
    "text": "PROFESSOR RABBAH: 250 megahertz\nand came back and it ran 425 megahertz. And it's been clocked as high as\n500 megahertz at 2.2 volts.",
    "start": "172430",
    "end": "181690"
  },
  {
    "text": "The chip isn't really designed\nfor low power but the tile abstraction is really nice for\npower consumption because if",
    "start": "181690",
    "end": "188740"
  },
  {
    "text": "you're not using tiles\nyou can essentially just shut them down. So it'll allow you to sort of\nhave power efficient design",
    "start": "188740",
    "end": "195920"
  },
  {
    "text": "just by nature of the\narchitecture. But when you're using all the\ntiles, all the memories, all the networks, in a non-optimized\ndesign, you",
    "start": "195920",
    "end": "205330"
  },
  {
    "text": "consume about 18\nwatts of power. So how do you use this\ntiled processor?",
    "start": "205330",
    "end": "212379"
  },
  {
    "text": "So here's one particular\nexample. The nice thing about tile\narchitecture is that you can let applications consume as\nmany tiles as they need.",
    "start": "212380",
    "end": "220470"
  },
  {
    "text": "If you have an application with\na lot parallelism then you give it a lot of tiles. If you have an application that\ndoesn't need a lot of",
    "start": "220470",
    "end": "226110"
  },
  {
    "text": "parallelism then you don't\ngive it a lot of tiles. So it allows you to really\nexploit the mapping of your",
    "start": "226110",
    "end": "231540"
  },
  {
    "text": "application down to the\narchitecture and gives you ASIC-like behavior-- application\nspecific",
    "start": "231540",
    "end": "236990"
  },
  {
    "text": "processing technology. So one example is you have\nsome video that you're",
    "start": "236990",
    "end": "242170"
  },
  {
    "text": "recording and you want to encode\nit and stream it across the web or display it on your\nmonitor or whatever else.",
    "start": "242170",
    "end": "248409"
  },
  {
    "text": "So you can have some logic\nthat you map down. If your chips are here, you\ndo some computation. You have memories sprinkled\nacross the tile that you're",
    "start": "248410",
    "end": "255940"
  },
  {
    "text": "going to use for local store. So you can parallelize, for\nexample, the motion estimation",
    "start": "255940",
    "end": "263060"
  },
  {
    "text": "for encoding the temporal\nredundancy in a video stream.",
    "start": "263060",
    "end": "268940"
  },
  {
    "text": "You can have another application\ncompletely independent running on other\npart of the chip.",
    "start": "268940",
    "end": "274470"
  },
  {
    "text": "So here's an application that's\nusing four different tiles and it's really\nisolated. It doesn't affect what's going\non in these tiles.",
    "start": "274470",
    "end": "280449"
  },
  {
    "text": "You can have another application\nthat's running something like MPI where\nyou're doing dynamic messaging, and httpd server\nand this tile is maybe not",
    "start": "280450",
    "end": "288500"
  },
  {
    "text": "used so it's just sleeping\nor it's idle. You can have memories\nconnected off the chip, I/O devices.",
    "start": "288500",
    "end": "295009"
  },
  {
    "text": "So it's really interesting in\nthe sense that probably the most interesting aspect of it is\nyou just allow the tiles to",
    "start": "295010",
    "end": "301610"
  },
  {
    "text": "sort of be used as your\nfundamental resource. And you can scale\nthem up as your application parallelism scales.",
    "start": "301610",
    "end": "307650"
  },
  {
    "text": "This is a picture of the raw\nboard-- the raw motherboard. Actually you see it in the Stata\nCenter in the Raw Lab.",
    "start": "307650",
    "end": "313870"
  },
  {
    "text": "This is the raw chip. A lot of the peripheral\ndevice, firmware and interconnect for dealing with a\nlot of devices off the chip",
    "start": "313870",
    "end": "323630"
  },
  {
    "text": "are implemented in\nthese FPGAs, so these are Xilinx chips. There's DRAM. You have connection to a\nPCI card, USB stick.",
    "start": "323630",
    "end": "333980"
  },
  {
    "text": "Network interface so you can\nactually log into this machine and use it. ",
    "start": "333980",
    "end": "340600"
  },
  {
    "text": "And there's a real compiler. It can run real applications\non it.",
    "start": "340600",
    "end": "345890"
  },
  {
    "text": "There's actually a bigger chip\nthat we built where we take four of these raw chips and\nsort of scale them up.",
    "start": "345890",
    "end": "352800"
  },
  {
    "text": "So rather than having 16 tiles\non your motherboard, you can have four raw chips. That gives you 64 tiles. You can scale this up to a\nthousand tiles or so on.",
    "start": "352800",
    "end": "360159"
  },
  {
    "text": "Just because of the tile\nnature, everything is symmetric, homogeneous, so\nyou can really scale it up really big.",
    "start": "360160",
    "end": "366500"
  },
  {
    "start": "365000",
    "end": "619000"
  },
  {
    "text": "So what is the performance\nof raw? So looking at the overall\napplication performance, so",
    "start": "366500",
    "end": "372370"
  },
  {
    "text": "we've done a lot of\nbenchmarking. So these are numbers from a\npaper that was published in 2004, where we took a lot of\napplications-- some are",
    "start": "372370",
    "end": "379530"
  },
  {
    "text": "well-known and used in standard\nbenchmark suites-- and compiled them for raw using\nvarious raw compiler",
    "start": "379530",
    "end": "385800"
  },
  {
    "text": "that we built in-house. And we've compared them\nagainst the Pentium 3. So the Pentium 3 is sort of\na unique comparison point",
    "start": "385800",
    "end": "393170"
  },
  {
    "text": "because it sort of matches raw\nin terms of the technology that was used to fabricate\nthe two.",
    "start": "393170",
    "end": "398889"
  },
  {
    "text": "And what you're seeing here,\nthis is a lock scale. The speedup of the application\nrunning on raw compared to the",
    "start": "398890",
    "end": "405460"
  },
  {
    "text": "application running on a P3. So the higher you get, the\nbetter the performance is.",
    "start": "405460",
    "end": "410659"
  },
  {
    "text": "So these applications are sort\nof grouped into a few classes.",
    "start": "410660",
    "end": "415770"
  },
  {
    "text": "So the first class is what\nwe call ILP applications. So these are applications that\nhave essentially instruction",
    "start": "415770",
    "end": "421620"
  },
  {
    "text": "level parallelism. I'm going to talk a little\nbit more about and sort of explain it. But you've seen this early\non in the lecture--",
    "start": "421620",
    "end": "428040"
  },
  {
    "text": "in some of Saman's lectures. So here you're trying to exploit\ninherent instruction level parallelism in\nthe applications.",
    "start": "428040",
    "end": "434819"
  },
  {
    "text": "And if you have lots of ILP then\nyou map it to a lot of tiles and you can get\nparallelism that way and you",
    "start": "434820",
    "end": "440040"
  },
  {
    "text": "get better performance. These applications here-- what\nwe call the streaming applications.",
    "start": "440040",
    "end": "445530"
  },
  {
    "text": "So you saw some of these in the\nStreamIt lecture and the StreamIt parallelizer\ncompiler.",
    "start": "445530",
    "end": "452250"
  },
  {
    "text": "Some of those numbers were\ngenerated on a raw-like architecture. And then you have the server\nor sort of more traditional",
    "start": "452250",
    "end": "459129"
  },
  {
    "text": "applications that you expect\nto run in a server style or throughput-oriented.",
    "start": "459130",
    "end": "465229"
  },
  {
    "text": "And then finally you have\nbit-level applications. So doing things at the very\nlowest level of computation",
    "start": "465230",
    "end": "470750"
  },
  {
    "text": "where you're doing a lot\nof bit manipulation. So what's interesting here to\nnote is that as you get into",
    "start": "470750",
    "end": "477830"
  },
  {
    "text": "more applications that have a\nlot of inherent parallelism in them, where you want\nexplicit--",
    "start": "477830",
    "end": "483830"
  },
  {
    "text": "where you can extract a lot of\nparallelism because of the explicit nature of the\napplications-- you can really map those\nreally well through the",
    "start": "483830",
    "end": "489920"
  },
  {
    "text": "architecture. And because of the communication\nnature-- because of communication\ncapabilities of the",
    "start": "489920",
    "end": "495650"
  },
  {
    "text": "architecture, being able to\nstream data from one tile to another really fast, you can\nget really high on-chip",
    "start": "495650",
    "end": "502350"
  },
  {
    "text": "bandwidth and that gives you\nreally high performance, especially for these kinds\nof applications. ",
    "start": "502350",
    "end": "510120"
  },
  {
    "text": "There are other applications\nthat we've done. Some of the students have worked\non in the raw group. So an MPEG-2 encoder where\nyou're essentially trying to",
    "start": "510120",
    "end": "519039"
  },
  {
    "text": "do real-time encoding of a\nvideo screen at different resolutions. So 350 by 240 or 720 by 480\nwhere you're compiling down to",
    "start": "519040",
    "end": "527925"
  },
  {
    "text": "a number of tiles. One, 4, 8 sixteen, 16-- 1 and 16 are somehow missing,\nI'm not sure why.",
    "start": "527925",
    "end": "535410"
  },
  {
    "text": "And what you're looking\nfor here? Sort of scalability\nof algorithm. As you add more tiles, are\nyou getting more and more",
    "start": "535410",
    "end": "542390"
  },
  {
    "text": "performance or are you getting\nbetter and better throughput? So you could encode more frames\nper second for example.",
    "start": "542390",
    "end": "548610"
  },
  {
    "text": "So if you're doing HDTV, it's\n1080p, then you really want to sort of get--",
    "start": "548610",
    "end": "553720"
  },
  {
    "text": "there's a lot of compute\npower that you need. And so as you add more frames,\nmaybe you can get to sort of",
    "start": "553720",
    "end": "559570"
  },
  {
    "text": "the throughput that\nyou need for HDTV. So this is something that might\nbe interesting for some",
    "start": "559570",
    "end": "565450"
  },
  {
    "text": "of your projects as well. And we've talked about\nthis before. On the cell, as you're using\nmore and more SPEs, can you",
    "start": "565450",
    "end": "571649"
  },
  {
    "text": "accelerate the performance\nof your application? Can you sort of show\nthat if you're doing some visual aspect? And you can sort of\ndemonstrate it.",
    "start": "571650",
    "end": "578330"
  },
  {
    "text": "So there's a demo that is set\nup and in the lab where you can sort of crank up number of\ntiles that you're using and",
    "start": "578330",
    "end": "584703"
  },
  {
    "text": "you get better performance\nfrom the MPEG encoder. And just looking at number of\nframes per second that you can",
    "start": "584703",
    "end": "590300"
  },
  {
    "text": "get, with 64 tiles-- so the raw\nchip is 16 tiles, but you",
    "start": "590300",
    "end": "595519"
  },
  {
    "text": "can scale it up by having\nmore chips-- so you can get about\n51 frames.",
    "start": "595520",
    "end": "600920"
  },
  {
    "text": "These numbers have been\nimproved and there are different ways of optimizing\nthese performances.",
    "start": "600920",
    "end": "608279"
  },
  {
    "text": "At 352 by 4 240, the estimated\ndata rate-- estimated",
    "start": "608280",
    "end": "614990"
  },
  {
    "text": "throughput-- of 160 frames per second\nalmost. So this is really high bandwidth.",
    "start": "614990",
    "end": "620790"
  },
  {
    "start": "619000",
    "end": "808000"
  },
  {
    "text": "Another interesting thing that\nwe've done with the raw chip is taking a look at graphics\npipelines and looking at is",
    "start": "620790",
    "end": "627329"
  },
  {
    "text": "there anything we can do to\nexploit the inherent tiled architecture of the raw chip.",
    "start": "627330",
    "end": "632700"
  },
  {
    "text": "So here's a screenshot from\nCounterstrike and a simplified graphics pipeline where you have\nsome input to the screen",
    "start": "632700",
    "end": "638930"
  },
  {
    "text": "you want to render. You do some vertex shading. So these are triangles that you\nwant to figure out what",
    "start": "638930",
    "end": "643990"
  },
  {
    "text": "colors to make-- what colors to paint them. The triangle's set up\nfor pixel stage.",
    "start": "643990",
    "end": "650209"
  },
  {
    "text": "And in this screen you'll notice\nthat there are two different things that\nyou're rendering. There's essentially this part of\nthe screen which has a lot",
    "start": "650210",
    "end": "657080"
  },
  {
    "text": "of triangles that span\na relatively not-so-complex image.",
    "start": "657080",
    "end": "663470"
  },
  {
    "text": "And then you have these guys\nhere that have fewer triangle span a smaller region\nof the frame.",
    "start": "663470",
    "end": "672520"
  },
  {
    "text": "And what you might want to do\nis allocate more computer power to the pixel stage and\nless compute power to the",
    "start": "672520",
    "end": "677960"
  },
  {
    "text": "vertex stage. So that's analogous to saying,\nI want more tiles for one stage of the pipeline and\nfewer tiles for another.",
    "start": "677960",
    "end": "684259"
  },
  {
    "text": "Or maybe I want to be able to\ndynamically change how many tiles I'm allocating\nat different stages of the pipeline.",
    "start": "684260",
    "end": "690200"
  },
  {
    "text": "So that as your screens that\nyou're rendering change in terms of their complexity, you\ncan maintain the good visual",
    "start": "690200",
    "end": "698120"
  },
  {
    "text": "illusions transparently without\ncompromising the",
    "start": "698120",
    "end": "703950"
  },
  {
    "text": "utilization of the chip. So some demos that were\ndone with the graphics group it at MIT--",
    "start": "703950",
    "end": "709230"
  },
  {
    "text": "Fredo Durand's group-- phong shading. You have 132 vertices\nwith 1 light source.",
    "start": "709230",
    "end": "715790"
  },
  {
    "text": "So this is what you're\ntrying to shade. You have a lot of\nregions black. So if you're looking at a\nfixed pipeline where the",
    "start": "715790",
    "end": "724110"
  },
  {
    "text": "vertex shader is taking\nsix tiles-- this is on a 64-tile chip-- the rasterizer is taking 15\ntiles, the pixel processor has",
    "start": "724110",
    "end": "730920"
  },
  {
    "text": "15 tiles, the alpha buffer\noperations has 15 tiles, then you might not get the best\nutilization because for that",
    "start": "730920",
    "end": "738310"
  },
  {
    "text": "entire region that you're\nrendering where it's black there's nothing really\ninteresting happening there.",
    "start": "738310",
    "end": "743920"
  },
  {
    "text": "You want to shift those tiles\nto another processor, to another stage of pipeline. Or, if you can't really utilize\nthem, then you're just",
    "start": "743920",
    "end": "751160"
  },
  {
    "text": "wasting power, wasting energy,\nand so you might just want to shut them and not\nuse them at all. So with a fixed pipeline\nversus a reconfigurable",
    "start": "751160",
    "end": "758120"
  },
  {
    "text": "pipeline where I can change the\nnumber of tiles allocated to different stages of the\npipeline, I can get better",
    "start": "758120",
    "end": "764660"
  },
  {
    "text": "utilization. And, in some cases, better\nperformance. So here, fuller bars,\nand you're",
    "start": "764660",
    "end": "771060"
  },
  {
    "text": "finishing faster in time. So this is indicative also\nof what's going on in the",
    "start": "771060",
    "end": "776540"
  },
  {
    "text": "graphics industry. So the graphics card\nused to be very-- ",
    "start": "776540",
    "end": "784990"
  },
  {
    "text": "well, it had fixed resources\nallocated to different stage, which is essentially what we're\ntrying model in this",
    "start": "784990",
    "end": "790899"
  },
  {
    "text": "part of the experiment, where\nmore and more now you have unified shaders that you can use\nfor the pixel shading and",
    "start": "790900",
    "end": "796300"
  },
  {
    "text": "the vertex shading. So you're getting into more of\nthat programmable aspect. Precisely because you want to\nbe able to do this kind of",
    "start": "796300",
    "end": "801660"
  },
  {
    "text": "load balancing and exploit\ndynamisms that you see in different things that you're\ntrying to render. ",
    "start": "801660",
    "end": "809150"
  },
  {
    "start": "808000",
    "end": "852000"
  },
  {
    "text": "Another example:\nshadow volumes. You have 4 triangles,\none light source.",
    "start": "809150",
    "end": "815170"
  },
  {
    "text": "And this was rendered\nin three passes. So pass 1, pass 2, pass 3, would\nessentially take the",
    "start": "815170",
    "end": "821285"
  },
  {
    "text": "same amount of time because\nyou're doing the same computation map to a fixed\nnumber of resources.",
    "start": "821285",
    "end": "828600"
  },
  {
    "text": "But if I can change the number\nof resources that I need for different passes-- so the\nrasterizer, for example, and the alpha buffer operations,\nis really where you",
    "start": "828600",
    "end": "835690"
  },
  {
    "text": "need a lot of power. So if you go from 15 tiles for\neach to 20 tiles for each, you",
    "start": "835690",
    "end": "841910"
  },
  {
    "text": "get better execution time\nbecause you were able to exploit parallelism or match\nparallelism better to the application.",
    "start": "841910",
    "end": "847740"
  },
  {
    "text": "And so you get 40%\npercent faster in this particular case. ",
    "start": "847740",
    "end": "853459"
  },
  {
    "start": "852000",
    "end": "941000"
  },
  {
    "text": "And another interesting\napplication: this is the largest in the world\nmicrophone array.",
    "start": "853460",
    "end": "859199"
  },
  {
    "text": "It's actually in the Guinness\nBook of Records. It was build in the lab. And what it essentially has--\neach of these little boards",
    "start": "859200",
    "end": "867140"
  },
  {
    "text": "has two microphones on it. And so what you can\nuse this for is eavesdropping for example. Or you can carry this\naround if you want.",
    "start": "867140",
    "end": "875820"
  },
  {
    "text": "Pack it in the car and\ndo some spying. But somewhat more interesting\ndemos that were done with this",
    "start": "875820",
    "end": "882720"
  },
  {
    "text": "in smaller scales was that in a\nnoisy room, for example, if you want the sort of hone in.",
    "start": "882720",
    "end": "887769"
  },
  {
    "text": "Let's say everybody here was\nspeaking, but for the camera they want to record\nonly my voice.",
    "start": "887770",
    "end": "892790"
  },
  {
    "text": "They can have a microphone array\nin the back that focuses on just my voice. And the way it's done is you can\nmeasure the distance from",
    "start": "892790",
    "end": "901190"
  },
  {
    "text": "the time it takes for the sound\nwave to reach each of these different microphones\nand you can focus in on a particular source of noise\nand be able to",
    "start": "901190",
    "end": "910950"
  },
  {
    "text": "just highlight that. So there's this demo where's\nit's a noisy room-- I probably should have had these\nin here in retrospect--",
    "start": "910950",
    "end": "918470"
  },
  {
    "text": "there's a noisy room, lots of\npeople are talking, then you turn on the microphone array\nand you can hear that one",
    "start": "918470",
    "end": "924180"
  },
  {
    "text": "particular source and\nit's a lot clearer. You can also have applications\nwhere you're tracking a person",
    "start": "924180",
    "end": "930740"
  },
  {
    "text": "in a room with videos as\nwell, so you can sort of follow him around. So it's a very interesting\napplication.",
    "start": "930740",
    "end": "936339"
  },
  {
    "text": "An now I regret not having\nthe video demo in here. Actually, should I do it? It's on the Web.",
    "start": "936340",
    "end": "941550"
  },
  {
    "start": "941000",
    "end": "1061000"
  },
  {
    "text": "OK. So a case study using\nthe beamformer. So what's being done in the\nmicrophone array is you're",
    "start": "941550",
    "end": "949290"
  },
  {
    "text": "doing beamforming. So you're trying to figure out\nwhat are the different beams that are reaching\nthe microphone. You want to be able to\namplify one of them.",
    "start": "949290",
    "end": "957279"
  },
  {
    "text": "So looking at the application\nwritten natively in C running",
    "start": "957280",
    "end": "962650"
  },
  {
    "text": "on a 1 gigahertz Pentium\n, what is the operation throughput? So you're getting about\n240 MegaFLOPS.",
    "start": "962650",
    "end": "970470"
  },
  {
    "text": "And if you go down\nto an optimized-- same code but running on single\ntile raw chip, you get",
    "start": "970470",
    "end": "977700"
  },
  {
    "text": "about 19 MegaFLOPS. So a not very good\nperformance. But here, what you really want\nto do, is you have al lot of parallelism.",
    "start": "977700",
    "end": "983200"
  },
  {
    "text": "Because each of those beams\nthat's reaching individual microphones can be\ndone in parallel. So you have a lot of\nparallelism in that",
    "start": "983200",
    "end": "989170"
  },
  {
    "text": "application. So taking the C program,\nreimplementing it in StreamIt that you've seen in previous\nlectures, and not really",
    "start": "989170",
    "end": "996060"
  },
  {
    "text": "optimizing it in terms\nof doing a lot of the optimizations you saw in the\nparallelizing compiler talk,",
    "start": "996060",
    "end": "1003360"
  },
  {
    "text": "you get about 640 MegaFLOPS. So already you're beating the C\nprogram running on a pretty",
    "start": "1003360",
    "end": "1009810"
  },
  {
    "text": "fast superscalar machine. And if you really optimize the\nStreamIt code in terms of doing the fission and fusion,\nincreasing the parallelism,",
    "start": "1009810",
    "end": "1019029"
  },
  {
    "text": "doing better load balancing\nautomatically, you can get up to 1.4 GigaFLOPS. So really good performance and\nreally matching the inherent",
    "start": "1019030",
    "end": "1026419"
  },
  {
    "text": "parallelism to the\narchitecture.  So it was just a big overview of\nthe raw chip and what we've",
    "start": "1026420",
    "end": "1033510"
  },
  {
    "text": "done with it in lab. There's more in here than\nI've talked about. But what I'm going to do next\nis give you some insights as",
    "start": "1033510",
    "end": "1040430"
  },
  {
    "text": "to what is the design philosophy\nthat went into raw architecture, why was it\ndesigned the way it was.",
    "start": "1040430",
    "end": "1047000"
  },
  {
    "text": "And then I'm going to talk a\nlittle bit about the raw parallelizing compiler. And while the StreamIt language\nand compiler also has",
    "start": "1047000",
    "end": "1053550"
  },
  {
    "text": "a back end for the raw\narchitecture, we've sort of seen that in previous lectures\nso I'm not going to talk about that here.",
    "start": "1053550",
    "end": "1058580"
  },
  {
    "text": "So I'm just going to focus\non the first two bullets. And a few years ago when the\nproject got started, sort of",
    "start": "1058580",
    "end": "1067680"
  },
  {
    "start": "1061000",
    "end": "1299000"
  },
  {
    "text": "the insight in the wide issue\nprocessors and the design philosophy that was being\nfollowed in industry for",
    "start": "1067680",
    "end": "1075580"
  },
  {
    "text": "building wider superscalars,\nfaster superscalars, was really going to come to a halt\nlargely because you have",
    "start": "1075580",
    "end": "1081560"
  },
  {
    "text": "scalability issues. So if you look at sort of a\nsimplified illustration of a",
    "start": "1081560",
    "end": "1086840"
  },
  {
    "text": "wide issue microprocessor, you\nhave your program counter such as instructions. Goes into some control logic.",
    "start": "1086840",
    "end": "1092740"
  },
  {
    "text": "Control logic is then\ngoing to run. You're going to read\nsome variables from the register file. You'll have a big crossbar in\nthe middle that routes to",
    "start": "1092740",
    "end": "1099700"
  },
  {
    "text": "operands like ALUs. Yell And then you operate on\nthose and you have to send it back to the register file.",
    "start": "1099700",
    "end": "1105429"
  },
  {
    "text": "Plus you have this really big\nproblem with the network. So if you're doing some\ncomputation--",
    "start": "1105430",
    "end": "1112850"
  },
  {
    "text": "sorry, I rearranged\nthese slides. So what you have if you have n\nALUs, then the complexity of",
    "start": "1112850",
    "end": "1118290"
  },
  {
    "text": "your crossbar increases as\nn squared, because you essentially have to\nhave everybody talking to each other.",
    "start": "1118290",
    "end": "1124880"
  },
  {
    "text": "And in terms of the number of\nwires that you need out of the register file to support\neverybody being able to sort",
    "start": "1124880",
    "end": "1129970"
  },
  {
    "text": "of talk to anybody else very\nefficiently, the number of ports, the number of wires\nincreases n cubed. So that's a problem because\nyou can't clock all those",
    "start": "1129970",
    "end": "1137600"
  },
  {
    "text": "wires fast enough. The frequency becomes\nsort of limited. It grows even less\nthan linearly.",
    "start": "1137600",
    "end": "1144380"
  },
  {
    "text": "And this is a problem because\noperational routing-- operand routing, is global.",
    "start": "1144380",
    "end": "1149620"
  },
  {
    "text": "So if I have- I'm doing some\noperations and it's an add, the results of this add is fed\nto another operation to shift,",
    "start": "1149620",
    "end": "1156760"
  },
  {
    "text": "and these are going to execute\non two different ALUs. So what's going to happen?",
    "start": "1156760",
    "end": "1162860"
  },
  {
    "text": "I do the add operation. It's going to produce\na result. But there's no direct path for\nthis ALU to send this result",
    "start": "1162860",
    "end": "1170100"
  },
  {
    "text": "to this ALU. So instead what has happened is\nthe operand has to travel all the way back around through\nthe crossbar and then",
    "start": "1170100",
    "end": "1176195"
  },
  {
    "text": "back to this ALU.  So that's really just going to\ntake a long time and not",
    "start": "1176195",
    "end": "1183210"
  },
  {
    "text": "necessarily very efficient. And if you're doing this for a\nlot of ALU operations, you have a lot of parallelism in\nyour application level,",
    "start": "1183210",
    "end": "1189780"
  },
  {
    "text": "instructional level parallelism,\nand that's just creating a lot of\ncommunication. But you're not really exploiting\nthe locality of the",
    "start": "1189780",
    "end": "1195170"
  },
  {
    "text": "computation. If 2 instructions are really\nclose together, you want to be able to just have a\npoint-to-point path, for",
    "start": "1195170",
    "end": "1201889"
  },
  {
    "text": "example, or a shorter path that\nallows you to exploit where was instructions\nare in space.",
    "start": "1201890",
    "end": "1207920"
  },
  {
    "text": "And so this was the driving\ninsight for the architecture in that you want to make\noperand routing local.",
    "start": "1207920",
    "end": "1214850"
  },
  {
    "text": "So an idea is to essentially\nexploit this locality by distributing the ALUs. And rather than having that\nmassive crossbar, what you",
    "start": "1214850",
    "end": "1222570"
  },
  {
    "text": "want to do is have an on-chip\nmesh network. So rather than have one big\ncrossbar, you have lots of",
    "start": "1222570",
    "end": "1228393"
  },
  {
    "text": "smaller ones. So these become switch\nprocessors. So I can put value from this\nALU here and then have that",
    "start": "1228393",
    "end": "1234750"
  },
  {
    "text": "value routed to any other ALU. Maybe that just cost me more in\nterms of instructions that",
    "start": "1234750",
    "end": "1239990"
  },
  {
    "text": "says where this operand\nis going. We'll get into that. But here, what this allows\nme to do is exploit",
    "start": "1239990",
    "end": "1246580"
  },
  {
    "text": "that locality better. Same instruction chain, I can\nput the first operation on one",
    "start": "1246580",
    "end": "1251650"
  },
  {
    "text": "ALU, I can put the other\noperation on the second ALU. And here, rather than putting\nit for example here, which",
    "start": "1251650",
    "end": "1258240"
  },
  {
    "text": "would send the operand really\nfar across chip, what I want to do is recognize that there's\na producer/consumer",
    "start": "1258240",
    "end": "1263660"
  },
  {
    "text": "relationship here. I want to exploit that locality\nand have them close in spaces so that the routes\nremain fairly short.",
    "start": "1263660",
    "end": "1271260"
  },
  {
    "text": "You know what I can also do is\nsort of pipeline this network so that I can have the hardware\nessential match",
    "start": "1271260",
    "end": "1276770"
  },
  {
    "text": "computation flow. If one ALU is producing a lot\nof results at a lot faster",
    "start": "1276770",
    "end": "1282900"
  },
  {
    "text": "rate than for example this\ninstruction can consume them, then the hardware can take care\nof, for example, blocking",
    "start": "1282900",
    "end": "1289470"
  },
  {
    "text": "or stalling the producing\nprocessor so it doesn't get too far ahead. It gives you a nature mechanism\nfor regulating the",
    "start": "1289470",
    "end": "1296490"
  },
  {
    "text": "flow data on the chip. Well, this is better than what\nwe saw before because with the",
    "start": "1296490",
    "end": "1304679"
  },
  {
    "start": "1299000",
    "end": "1435000"
  },
  {
    "text": "crossbar you're not really\ngetting any scalability in terms of your latency transport\noperands from one",
    "start": "1304680",
    "end": "1310670"
  },
  {
    "text": "ALU to another. Whereas with on-chip network,\nif you've taken routing",
    "start": "1310670",
    "end": "1316790"
  },
  {
    "text": "classes, you know that there\nexists an algorithm that sort of allows it to route things at\nleast the square root of n,",
    "start": "1316790",
    "end": "1323029"
  },
  {
    "text": "where n is the number of things\nthat are communicating in your network. But if you're doing locality\ndriven placement then it's",
    "start": "1323030",
    "end": "1328560"
  },
  {
    "text": "essentially costing time. And in a raw chip, it's\nin fact three cycles. So you can send one operand from\none tile to another in",
    "start": "1328560",
    "end": "1335220"
  },
  {
    "text": "three cycles. And we'll get into how that\nnumber comes about. So this is much better. But what it does\nis increase the",
    "start": "1335220",
    "end": "1341450"
  },
  {
    "text": "complexity on the compiler. It says, this is my computation,\nhow do you map it efficiently so that things are\nclustered in space well so",
    "start": "1341450",
    "end": "1348960"
  },
  {
    "text": "that I don't have these really\nlong routes for communication? But then we can look at what\nelse can we distribute.",
    "start": "1348960",
    "end": "1356190"
  },
  {
    "text": "Well, we have the\nregister file. We can distribute that\nacross all the ALUs.",
    "start": "1356190",
    "end": "1361240"
  },
  {
    "text": "And that essentially decreases\nthat n cubed relationships between ALUs and register file\nports to something that's a",
    "start": "1361240",
    "end": "1367980"
  },
  {
    "text": "lot more tractable. Where it's one small\nregister per ALU.",
    "start": "1367980",
    "end": "1374170"
  },
  {
    "text": "And this is better in terms of\nscalability, but we haven't solved the entire problem in\nthat we still have one global",
    "start": "1374170",
    "end": "1379870"
  },
  {
    "text": "program counter, we have one\nglobal instruction fetch unit, one global control unified\nload/store queue for",
    "start": "1379870",
    "end": "1387240"
  },
  {
    "text": "communicating with memory. And those all have scalability\nproblems. So whereas we fixed",
    "start": "1387240",
    "end": "1393850"
  },
  {
    "text": "the problem with\nthe crossbar-- that becomes more scalable-- we haven't really fix the\nproblems with the others.",
    "start": "1393850",
    "end": "1399940"
  },
  {
    "text": "So what's the natural\nsolution to do here? Well, we'll just distribute\neverything else.",
    "start": "1399940",
    "end": "1406250"
  },
  {
    "text": "And so you start off with each\nALU here now having it's own program counter, its own\ninstruction cache, it's own",
    "start": "1406250",
    "end": "1412610"
  },
  {
    "text": "data cache. And it has its register file ALU\nand everybody-- that same sort of design pattern\nis repeated for each",
    "start": "1412610",
    "end": "1420559"
  },
  {
    "text": "one of those ALUs. So now it looks like it's\na lot scalable. I don't have any global wires.",
    "start": "1420560",
    "end": "1426320"
  },
  {
    "text": "There's no global centralized\ndata structure. And all of that means I\ncan do things more--",
    "start": "1426320",
    "end": "1432220"
  },
  {
    "text": "I can do things faster\nand more efficiently. And what you start seeing here\nis this sort of tile processor",
    "start": "1432220",
    "end": "1438990"
  },
  {
    "start": "1435000",
    "end": "1611000"
  },
  {
    "text": "coming about all. So each one of those things\nwas exactly the same. And what was done in the raw\nprocessor is that none of",
    "start": "1438990",
    "end": "1446470"
  },
  {
    "text": "those tiles was longer than\nyou can communicate in one clock cycle. So this solved essentially a\nwire delay problem as well.",
    "start": "1446470",
    "end": "1454850"
  },
  {
    "text": "So if this is the distance\nthat a wire-- that a signal can travel\nin one clock cycle, the tile is smaller.",
    "start": "1454850",
    "end": "1461970"
  },
  {
    "text": "It can fit within this circle. So that means that you're\nguaranteed-- you have better scalability\nproblems. You're solving the",
    "start": "1461970",
    "end": "1469200"
  },
  {
    "text": "issues that people are facing\nwith wire delay. And in terms of the tile\nprocessor abstraction, Michael",
    "start": "1469200",
    "end": "1476940"
  },
  {
    "text": "Taylor was is a PhD student in\nthe raw group, his thesis sort of identified the tile processor\napproach and this",
    "start": "1476940",
    "end": "1485780"
  },
  {
    "text": "aspect of the tile processor\napproach that makes it more attractive, the SON.",
    "start": "1485780",
    "end": "1490880"
  },
  {
    "text": "Which is the scalar\noperand network. And the next two slides, the\nnext part of the lecture, is",
    "start": "1490880",
    "end": "1497080"
  },
  {
    "text": "going to really focus\non what that means. He argues why the tile",
    "start": "1497080",
    "end": "1502170"
  },
  {
    "text": "processor approach is scalable. And it's scalable for the same\nreasons as multicores. You just add more and more\ncores on a chip.",
    "start": "1502170",
    "end": "1509350"
  },
  {
    "text": "But the intrinsic difference\nbetween the multicore that you see today and the raw\narchitecture is the scalar",
    "start": "1509350",
    "end": "1516580"
  },
  {
    "text": "operand network. So I'm going to ask you\nquestions about this in a few slides.",
    "start": "1516580",
    "end": "1522690"
  },
  {
    "text": "But really what you're getting\nhere is the ability to communicate from one processor\nto another very efficiently.",
    "start": "1522690",
    "end": "1528980"
  },
  {
    "text": "And the way you do this on raw\nis you have your instruction fetch d code, register file\nread stage, WALU--",
    "start": "1528980",
    "end": "1535340"
  },
  {
    "text": "your competition pipeline. But part of the registers-- the\nnew register file-- so 24",
    "start": "1535340",
    "end": "1541429"
  },
  {
    "text": "through 27 are network mapped. So what that means is, if\nI write-- if one of the",
    "start": "1541430",
    "end": "1546890"
  },
  {
    "text": "operations that I have in my\ncomputation has a destination register that's 24, 25, 26 or\n27, that value automatically",
    "start": "1546890",
    "end": "1556480"
  },
  {
    "text": "get sent to the output\nnetwork. And if I have a value-- if one of my source operands is\nregistered at 24, 25, 26 or",
    "start": "1556480",
    "end": "1564960"
  },
  {
    "text": "27, implicitly that means get\nthat value off the network. ",
    "start": "1564960",
    "end": "1572010"
  },
  {
    "text": "And so I can have add 25-- added to register 25-- so this\nis one of the network map",
    "start": "1572010",
    "end": "1578560"
  },
  {
    "text": "ports, sum two operands. So this is a picture\nof the raw chip. This is one tile.",
    "start": "1578560",
    "end": "1585100"
  },
  {
    "text": "This is the other tile. So you can sort of see the\ncomputation and the network",
    "start": "1585100",
    "end": "1590250"
  },
  {
    "text": "switch processor here. So the operand flows into the\nnetwork and then gets",
    "start": "1590250",
    "end": "1596340"
  },
  {
    "text": "transported across from\none tile to the other. And then gets injected\ninto the other tiles compute networks.",
    "start": "1596340",
    "end": "1603270"
  },
  {
    "text": "And here this instruction has\nsort a source operand that that's register map operand. So it knows where to\nget its value from.",
    "start": "1603270",
    "end": "1609730"
  },
  {
    "text": "And then you can do\nthe computation. An interesting aspect here\nis that while you've seen",
    "start": "1609730",
    "end": "1615200"
  },
  {
    "start": "1611000",
    "end": "1842000"
  },
  {
    "text": "instructions like this, just\nnormal instructions, here you also have explicit routing\ninstructions that are executed",
    "start": "1615200",
    "end": "1622220"
  },
  {
    "text": "on the switch processor. So the switch processor here\nsays take the value that's coming from my processor and\nsend it east. So each",
    "start": "1622220",
    "end": "1631990"
  },
  {
    "text": "processor can send values east,\nwest, north or south. So it can go to the tile above\nit, the tile below it, the",
    "start": "1631990",
    "end": "1637950"
  },
  {
    "text": "tile to the left of it or\ntile to the right of it. And so sending it east sends\nit along this wire here.",
    "start": "1637950",
    "end": "1644290"
  },
  {
    "text": "And then this particular switch\nprocessor says get a value from the west port and\nsend it to my processor.",
    "start": "1644290",
    "end": "1650910"
  },
  {
    "text": "Now you could have had here,\nthis process could say, this value is not for me, so I want\nto just pass through to some",
    "start": "1650910",
    "end": "1657059"
  },
  {
    "text": "other processor. So you can pass it from the west\nport to the south port or to the north port or just pass\nit through laterally to the",
    "start": "1657060",
    "end": "1664170"
  },
  {
    "text": "other east port. So it just allows you to\nessentially just have an on-chip network and not\noperand-- you can imagine",
    "start": "1664170",
    "end": "1670480"
  },
  {
    "text": "having an operand that has a\ndata packet and header that's says, I'm going to tile 10\nand the switches know",
    "start": "1670480",
    "end": "1678540"
  },
  {
    "text": "which way to send it. But the interesting aspect\nhere is that the compiler actually orchestrates the\ncommunication, so you don't",
    "start": "1678540",
    "end": "1684059"
  },
  {
    "text": "need that extra header that\nsays, I'm going to tile 10. You just have to generate a\nschedule of how to write that",
    "start": "1684060",
    "end": "1689380"
  },
  {
    "text": "data through. So we'll get into what that\nmeans for the compiler in terms of that added\ncomplexity.",
    "start": "1689380",
    "end": "1696140"
  },
  {
    "text": "So communication on multicores\nis expensive for the following reasons. And this is really sort of going\ncontrast or going to put",
    "start": "1696140",
    "end": "1704400"
  },
  {
    "text": "the scalar operand network\ninto slightly more perspective. But first, so how do you\ncommunicate between multicores",
    "start": "1704400",
    "end": "1711480"
  },
  {
    "text": "on the cell? You have the DMA transfers\nfrom one SPE to another.",
    "start": "1711480",
    "end": "1716510"
  },
  {
    "text": "You can't really ship an\noperand single value. So if I write the value x, and\nI want to send x from one SPE",
    "start": "1716510",
    "end": "1723029"
  },
  {
    "text": "to another, I can't really do\nthat very efficiently, right? So this is essentially the\ncontrasting thing between",
    "start": "1723030",
    "end": "1732140"
  },
  {
    "text": "multicore processors that\nlargely exist today and the raw processor. So I've shown you an empirical--\na quantitative--",
    "start": "1732140",
    "end": "1740210"
  },
  {
    "text": "an analytical model for\ncommunication costs before in earlier slides.",
    "start": "1740210",
    "end": "1746380"
  },
  {
    "text": "This is an illustration\nof that concept. So if I have a processor that's\ntalking to another,",
    "start": "1746380",
    "end": "1752370"
  },
  {
    "text": "that value has to travel\nacross some network and there's some transport costs\nassociated with that.",
    "start": "1752370",
    "end": "1758940"
  },
  {
    "text": "But there's also some\nadded complexities. So there were lots of terms,\nif you remember, in that really big equation\nI've shown before.",
    "start": "1758940",
    "end": "1765730"
  },
  {
    "text": "You have some overhead in terms\nof packaging the data. And you have some overhead in\nterms of unpacking the data.",
    "start": "1765730",
    "end": "1772039"
  },
  {
    "text": "So what does that look? Well, there are two components\nwe're going to break this down to: the send occupancy\nand send latency.",
    "start": "1772040",
    "end": "1779019"
  },
  {
    "text": "And I'm going to talk\nabout each of those. And similarly on the receive\nside, you have the receive latency and the receive\noccupancy.",
    "start": "1779020",
    "end": "1785640"
  },
  {
    "text": "So bear in mind, this lifetime\nof a message essentially has to flow through these\nfive components.",
    "start": "1785640",
    "end": "1792820"
  },
  {
    "text": "It has to go through the\noccupancy stage, then there's the send latency, transport,\nreceive latency and receive",
    "start": "1792820",
    "end": "1799810"
  },
  {
    "text": "occupancy before you can\nactually use it to compute on.",
    "start": "1799810",
    "end": "1804830"
  },
  {
    "text": "So what are some things\nthat you do here? Well, it's things that you've\ndone on cell for getting VME",
    "start": "1804830",
    "end": "1809900"
  },
  {
    "text": "transfers to work. You have to figure who the\ndestination is, what is the value, maybe you have an idea\nassociated with it, a tag,",
    "start": "1809900",
    "end": "1817800"
  },
  {
    "text": "things of that sort. And you have to essentially\ninject that message into the network. So there's some latency\nassociated with that.",
    "start": "1817800",
    "end": "1824210"
  },
  {
    "text": "Maybe your-- on cell you have a DMA engine\nwhich essentially hides this",
    "start": "1824210",
    "end": "1831480"
  },
  {
    "text": "latency for you. Because you can essentially just\nsend the message to the DMA, right into its queue. And you can especially forget\nabout it unless it stalls",
    "start": "1831480",
    "end": "1839530"
  },
  {
    "text": "because the DMA list is full. On the receive side, you sort\nof have a similar thing.",
    "start": "1839530",
    "end": "1845890"
  },
  {
    "start": "1842000",
    "end": "1856000"
  },
  {
    "text": "You have to get the network to\ninject that value into the processor and then you have to\ndepackage it, demultiplex it",
    "start": "1845890",
    "end": "1853005"
  },
  {
    "text": "and put it into some form that\nyou can actually use to operate on it. So this 5-tuple is gives us a\nway of sort of characterizing",
    "start": "1853005",
    "end": "1861700"
  },
  {
    "start": "1856000",
    "end": "1871000"
  },
  {
    "text": "communication patterns on\ndifferent architectures. So I can contrast, for example,\nraw versus the",
    "start": "1861700",
    "end": "1869529"
  },
  {
    "text": "traditional microprocessor. So this is a traditional\nsuperscalar.",
    "start": "1869530",
    "end": "1875460"
  },
  {
    "start": "1871000",
    "end": "1998000"
  },
  {
    "text": "A traditional superscalar\nessentially has all the sophisticated circuitry that\nallows you to essentially",
    "start": "1875460",
    "end": "1882200"
  },
  {
    "text": "bypass network. You can have an operand directly\nflowing to another ALU through all the n squared\nwires in the crossbar.",
    "start": "1882200",
    "end": "1889950"
  },
  {
    "text": "And a lot of dynamic scheduling\nis going on. So it really has no occupancy,\nlatency, you're not really",
    "start": "1889950",
    "end": "1897110"
  },
  {
    "text": "doing any packaging\nof the operands. Your transport cost is\nessentially completely hidden.",
    "start": "1897110",
    "end": "1903460"
  },
  {
    "text": "You have no complexity\non the receive side. So it's really efficient. So this is essentially what you\nwant to get to go: this",
    "start": "1903460",
    "end": "1910140"
  },
  {
    "text": "kind of 5-tuple. But as we saw before, it's\nreally not scalable because the wire complexity woes--\nwhether it's n squared or n",
    "start": "1910140",
    "end": "1917460"
  },
  {
    "text": "cubed, that's not good\nfrom an energy efficient point of view. Scalable multiprocessors-- these are on-chip\nmultiprocessors more",
    "start": "1917460",
    "end": "1925580"
  },
  {
    "text": "indicative of things that you\nhave today-- have this kind of 5-tuple where you have about\n16 cycles just to get a",
    "start": "1925580",
    "end": "1932210"
  },
  {
    "text": "message out, know roughly\n3 cycles are so to transport message. So maybe this is being done\nthrough a shared cache.",
    "start": "1932210",
    "end": "1939370"
  },
  {
    "text": "Which is how a lot of\narchitecture communicates between processors today. And you have to sort of\ndemultiplex the message on the",
    "start": "1939370",
    "end": "1946970"
  },
  {
    "text": "receive side. So that adds some latency. In raw, because you have these\nnet memory map registers on",
    "start": "1946970",
    "end": "1954580"
  },
  {
    "text": "the input side and the output\nside, you really can knock down the complexity from the\nsend side in terms of the",
    "start": "1954580",
    "end": "1964789"
  },
  {
    "text": "occupancy and latency to zero. And you just write the values\nto the register. And it looks like a normal\nregister, right?",
    "start": "1964790",
    "end": "1970490"
  },
  {
    "text": "But it just magically appears\non the network. And then from one tile to\nanother, it's one cycle to",
    "start": "1970490",
    "end": "1976380"
  },
  {
    "text": "ship the value across that one\nlink from one switch processor to the other, as long as\nit's a near neighbor.",
    "start": "1976380",
    "end": "1982020"
  },
  {
    "text": "And then two cycles to\ninject the network into the tile processor. And then you're ready\nto use it.",
    "start": "1982020",
    "end": "1988269"
  },
  {
    "text": "So in this space, where would\nyou put cell is the question? Anybody have any ideas?",
    "start": "1988270",
    "end": "1994310"
  },
  {
    "start": "1994310",
    "end": "1999670"
  },
  {
    "start": "1998000",
    "end": "2103000"
  },
  {
    "text": "What would the communication\npanel look like on cell? ",
    "start": "1999670",
    "end": "2007960"
  },
  {
    "text": "So you have to do explicit\nsends and receives. So let's look at this.",
    "start": "2007960",
    "end": "2015450"
  },
  {
    "text": "So can we get rid of this\nstage on cell which is essentially saying\npackaging up my message, is it's no, right?",
    "start": "2015450",
    "end": "2022190"
  },
  {
    "text": "Because you have to essentially\nsay where that DMA transfer is going to go to--\nwhich region of memory? So you're buildings these\ncontrol blocks.",
    "start": "2022190",
    "end": "2029670"
  },
  {
    "text": "And then the send latency here\nis roughly zero, because you have the DMA processor which\nallows that kind of",
    "start": "2029670",
    "end": "2036090"
  },
  {
    "text": "concurrency between\ncommunication and computation, so you can hide essentially that\npart of the transport--",
    "start": "2036090",
    "end": "2043559"
  },
  {
    "text": "that part of communication\ncosts. Your transport costs here, you\nhave this really massive",
    "start": "2043560",
    "end": "2049210"
  },
  {
    "text": "bandwidth, this really\nhigh bandwidth interconnect on the chip. So this makes it reasonably\nfast, but",
    "start": "2049210",
    "end": "2054520"
  },
  {
    "text": "it's still a few cycles. There's no near neighbor? Yeah, a hundred cycles to go\nnear neighbor communication.",
    "start": "2054520",
    "end": "2062419"
  },
  {
    "text": "Because you're still-- you don't have that fast\nmechanism of being able to send things points to point.",
    "start": "2062420",
    "end": "2067909"
  },
  {
    "text": "You're putting things on the\nbus and there's some complexity there.",
    "start": "2067910",
    "end": "2073690"
  },
  {
    "text": "On the receive, you have the\nsame kind of complexity that you had on the send side. You have to know that the\nmessage is coming, that can be",
    "start": "2073690",
    "end": "2079770"
  },
  {
    "text": "done in different ways. And then you have to take that\nmessage and write it into your local store.",
    "start": "2079770",
    "end": "2085379"
  },
  {
    "text": "Which also adds some overhead in\nterms of the communication",
    "start": "2085380",
    "end": "2090609"
  },
  {
    "text": "cost. So the cell would probably\nbe somewhere up here,",
    "start": "2090610",
    "end": "2097970"
  },
  {
    "text": "I would imagine. I didn't have a chance\nto get the numbers. If I do, I'll update\nthe slide later on.",
    "start": "2097970",
    "end": "2104490"
  },
  {
    "start": "2103000",
    "end": "2449000"
  },
  {
    "text": "OK, so that's essentially a\nbrief insight into the raw-- yeah? AUDIENCE: Where did you get\nthe scalable processor?",
    "start": "2104490",
    "end": "2113550"
  },
  {
    "text": "PROFESSOR RABBAH: So these are\nfrom Michael Taylor's thesis. So I believe what he's done here\nis just looked at some",
    "start": "2113550",
    "end": "2121790"
  },
  {
    "text": "existing microprocessor and\nessentially benchmarked communication latency from\none processor to another.",
    "start": "2121790",
    "end": "2127050"
  },
  {
    "text": "AUDIENCE: So this is like going\nthrough the cache on the [OBSCURED]? PROFESSOR RABBAH: That's\nin fact how you-- a lot of these multiprocessors\ntoday have shared caches,",
    "start": "2127050",
    "end": "2134310"
  },
  {
    "text": "either L-1 and more\nso now it's L-2. So if you have-- L-1s are dedicated to different\nprocessors.",
    "start": "2134310",
    "end": "2140640"
  },
  {
    "text": "But you still have to go the\nmemory to communicate. ",
    "start": "2140640",
    "end": "2145750"
  },
  {
    "text": "So the raw parallelizing\ncompiler-- yeah? Another question? AUDIENCE: You might want to\npostpone this question.",
    "start": "2145750",
    "end": "2152540"
  },
  {
    "text": "Two related questions:\nso raw has--",
    "start": "2152540",
    "end": "2157950"
  },
  {
    "text": "I guess raw has pretty well\noptimized nearest neighbor communication. But we know from, for example,\nRed's Rule in heuristic and",
    "start": "2157950",
    "end": "2168074"
  },
  {
    "text": "intellectual engineering about\nthe number of wires needed for a given area. Is that in between--",
    "start": "2168074",
    "end": "2174190"
  },
  {
    "text": "as I recall, it's the minimum\nfor a good sized circuit is",
    "start": "2174190",
    "end": "2181050"
  },
  {
    "text": "proportional to the perimeter,\nor roughly the square root of the area.",
    "start": "2181050",
    "end": "2187480"
  },
  {
    "text": "And it ranges from there to--\nnot proportional to the area.",
    "start": "2187480",
    "end": "2193070"
  },
  {
    "text": "There's something in between. Something with 3 in it. Like to the 3/2 power\nI think, perhaps.",
    "start": "2193070",
    "end": "2200180"
  },
  {
    "text": "No, something like 2/3rds,\nsomething like-- yeah, 2/3rds power. So the area to the 1/2 power or\narea to the 2/3rds power.",
    "start": "2200180",
    "end": "2207069"
  },
  {
    "text": "So Red's Rule says the number\nof wires you need is roughly in that area.",
    "start": "2207070",
    "end": "2212769"
  },
  {
    "text": "And so that sort of\npushes that-- so the minimum you need is the\nnearest communication.",
    "start": "2212770",
    "end": "2218650"
  },
  {
    "text": "And often you need\nmore than that. We know from the FPGA experience\nthat nearest",
    "start": "2218650",
    "end": "2226470"
  },
  {
    "text": "neighbor communication\nis not-- or, at least, it's good to\nhave move than nearest neighbor, and that often long\nwires followed across the",
    "start": "2226470",
    "end": "2233930"
  },
  {
    "text": "chip, in extremely high-- PROFESSOR RABBAH: So I'm going\nto actually show you an example where nearest neighbor\nis good but you might also",
    "start": "2233930",
    "end": "2240280"
  },
  {
    "text": "want some global mechanism\nfor control orchestration for example.",
    "start": "2240280",
    "end": "2245490"
  },
  {
    "text": "AUDIENCE: Not just for con--\nnot surely just for control but for broadcast, for arbitrary\nfor the computation",
    "start": "2245490",
    "end": "2251810"
  },
  {
    "text": "to use, not just for\nthe chip to use. Like why are you scaling out two\nhops, four hops, fewer and",
    "start": "2251810",
    "end": "2258970"
  },
  {
    "text": "fewer wire-- PROFESSOR RABBAH: Yes, in fact\nwhat I think is going to happen is a lot of these chip\ndesigns are going to be",
    "start": "2258970",
    "end": "2264280"
  },
  {
    "text": "heirarchical. You have some really global\ntype communication at the",
    "start": "2264280",
    "end": "2269570"
  },
  {
    "text": "highest level. And then as you get within each\none of the processors, then you see things at the\nlowest level, something that",
    "start": "2269570",
    "end": "2275609"
  },
  {
    "text": "looked like raw. So you can build sort of a\nhierarchy of communication stages that allow you to sort\nof solve that problem.",
    "start": "2275610",
    "end": "2282589"
  },
  {
    "text": "But all of that adds\ncomplexity, right? First you have to solve the\nproblem of how do you parallelize for just a fixed\nnumber of cores and then",
    "start": "2282590",
    "end": "2289120"
  },
  {
    "text": "figure out the communications. Once we understand how to\ndo that well with a nice programming model then you can\nbuild heirarchically on that.",
    "start": "2289120",
    "end": "2295745"
  },
  {
    "text": "AUDIENCE: On the other hand, it\nmight make the compiler's job easier because it's\nnot as constrained. PROFESSOR RABBAH: It\nmight give you a",
    "start": "2295745",
    "end": "2301090"
  },
  {
    "text": "nice fall back rate. It might save you in cases where\nthere are things that are hard to do.",
    "start": "2301090",
    "end": "2306360"
  },
  {
    "text": "There are some issues\nin the last two-- the second to the last\nthree slides.",
    "start": "2306360",
    "end": "2313119"
  },
  {
    "text": "We'll talk about an example of\nwhere that might be the case. AUDIENCE: Another question\nwhich [OBSCURED]",
    "start": "2313120",
    "end": "2320970"
  },
  {
    "text": "so raw, I guess, being simpled\nand tiled, I guess one of the selling points I think was that\nit really cuts down on",
    "start": "2320970",
    "end": "2327436"
  },
  {
    "text": "the engineering effort. PROFESSOR RABBAH:\nOh, absolutely. This was done a million gates\nin-house for [OBSCURED]",
    "start": "2327436",
    "end": "2334660"
  },
  {
    "text": "AUDIENCE: So a company like\nIntel has a ridiculous number of engineers. And to get a competitive edge,\nthey something they want to",
    "start": "2334660",
    "end": "2341485"
  },
  {
    "text": "apply more engineering to it. And so the question is, where\nmight you apply more engineering to try\nto squeeze more--",
    "start": "2341485",
    "end": "2347760"
  },
  {
    "text": "PROFESSOR AMARASINGHE: That's\nthe million dollar question that everybody's looking at. Because if somehow Intel thought\nthey could add more",
    "start": "2347760",
    "end": "2354188"
  },
  {
    "text": "and more engineering. And then build this very complex\nfull-scale [OBSCURED]",
    "start": "2354188",
    "end": "2359520"
  },
  {
    "text": "But separate vessels. And so I think there's still a\nlot of things that is wrong.",
    "start": "2359520",
    "end": "2366500"
  },
  {
    "text": "Meaning it's [OBSCURED]",
    "start": "2366500",
    "end": "2373090"
  },
  {
    "text": "so at Intel basically\nthey will let you do something like that. They will put a lot of engineers\ndoing each of these",
    "start": "2373090",
    "end": "2379650"
  },
  {
    "text": "components, finding very few,\nand they can get a lot more performance, a lot less power\nand stuff like that.",
    "start": "2379650",
    "end": "2387140"
  },
  {
    "text": "So depending on what you want,\nscience is not everything.",
    "start": "2387140",
    "end": "2393150"
  },
  {
    "text": "There are a lot of other\nthings [OBSCURED]",
    "start": "2393150",
    "end": "2398420"
  },
  {
    "text": "So while it makes it easier? [OBSCURED]",
    "start": "2398420",
    "end": "2408260"
  },
  {
    "text": "And the key thing is, you start\nsomething simple and as you go on, you can add more\nand more complexity.",
    "start": "2408260",
    "end": "2414220"
  },
  {
    "text": "Just, as there's more\nthings to do. PROFESSOR RABBAH: Part of the\ncomplexity might be going to--",
    "start": "2414220",
    "end": "2420680"
  },
  {
    "text": "not making all those\n[OBSCURED].",
    "start": "2420680",
    "end": "2425859"
  },
  {
    "text": "OK, so raw pushes a lot of the\ncomplexity into the compiler in that the compiler now has\nto do at least two things.",
    "start": "2425860",
    "end": "2433240"
  },
  {
    "text": "It has to distribute\nthe instructions. You have a single program and\nyou have to figure out how to parallelize it across\nmultiple cores.",
    "start": "2433240",
    "end": "2439140"
  },
  {
    "text": "But not only that, because you\nhave the scalar operand network, you have to figure out\nhow the different cores",
    "start": "2439140",
    "end": "2444480"
  },
  {
    "text": "have to talk to each other. So you have to essentially\ngenerate schedule for the switch processors as well.",
    "start": "2444480",
    "end": "2450400"
  },
  {
    "start": "2449000",
    "end": "2512000"
  },
  {
    "text": "So I'm going to talk a\nlittle bit about the raw paralyzing compiler. And this is different from\na StreamIT parallelizing",
    "start": "2450400",
    "end": "2455470"
  },
  {
    "text": "compiler which really talks\nabout a different program as an input, using a different\nlanguage.",
    "start": "2455470",
    "end": "2461450"
  },
  {
    "text": "This is work again done here\nat MIT by Walter Lee who graduated two years ago.",
    "start": "2461450",
    "end": "2467570"
  },
  {
    "text": "We have a sequential program. You inject it into raw C seed,\nraw C compiler, and you get",
    "start": "2467570",
    "end": "2474030"
  },
  {
    "text": "fine-grained Orchestrated\nParallel execution. And what the compiler does is\nworry about data distribution",
    "start": "2474030",
    "end": "2480700"
  },
  {
    "text": "just like you have to do on cell\nin terms of which memory goes into which local store. which competition\noperates on--",
    "start": "2480700",
    "end": "2487560"
  },
  {
    "text": "the raw compiler has to worry\nabout which computation operates on which data element\nand can you put that data in the right caches for each\nof the different tiles.",
    "start": "2487560",
    "end": "2496369"
  },
  {
    "text": "Instruction distribution:\nso the way this compiler essentially get parallelism,\nit's going to look at instruction level parallelism\nin your application.",
    "start": "2496370",
    "end": "2503270"
  },
  {
    "text": "And it's going to divide that up\namong the different cores. And then the last step is the\ncoordination of communication",
    "start": "2503270",
    "end": "2508810"
  },
  {
    "text": "in control flow. So I'm just going\nto briefly step through each one of those. So the data distribution really\nhas essentially trying",
    "start": "2508810",
    "end": "2516890"
  },
  {
    "start": "2512000",
    "end": "2577000"
  },
  {
    "text": "to solve the problem\nof locality. You have two instructions. A load into r1 from some\naddress and then",
    "start": "2516890",
    "end": "2524030"
  },
  {
    "text": "you're adding r1. You're incrementing\nthat value. And you might write it\nback for later on. So where would you put these\ntwo instructions?",
    "start": "2524030",
    "end": "2531109"
  },
  {
    "text": "So to exploit the locality, then\nyou want the data-- if the data is here, then you want\nthese two instructions to",
    "start": "2531110",
    "end": "2538020"
  },
  {
    "text": "be on this tile. If the data is here, then you\nwant these two instructions to be on this file.",
    "start": "2538020",
    "end": "2543420"
  },
  {
    "text": "Because it doesn't help you to\nhave the data here and the instructions here. Because what do you have\nto do in that case?",
    "start": "2543420",
    "end": "2549119"
  },
  {
    "text": "You have to send a message that\nsays, send me this data. And then you have to wait for\nit to come in and then you have to operate on it.",
    "start": "2549120",
    "end": "2555020"
  },
  {
    "text": "And then maybe you have\nto write it back. So the compiler sort of\nworries about the data distribution.",
    "start": "2555020",
    "end": "2560030"
  },
  {
    "text": "It applies some data analysis. A lot of a thing that you saw in\nSaman's lecture on classic",
    "start": "2560030",
    "end": "2565530"
  },
  {
    "text": "parallelization technology. Sort of figure out the\ninterdependencies and then they can figure out how to split\nup the data across the",
    "start": "2565530",
    "end": "2571770"
  },
  {
    "text": "different cores. And there's some other work done\nby other students in the group that tried to address\nthis problem.",
    "start": "2571770",
    "end": "2578470"
  },
  {
    "start": "2577000",
    "end": "2660000"
  },
  {
    "text": "The instruction distribution is\nperhaps as complicated and",
    "start": "2578470",
    "end": "2585020"
  },
  {
    "text": "interesting. In here, what's going\non is-- let's say you have a base block. So you take your sequential\nprogram.",
    "start": "2585020",
    "end": "2590950"
  },
  {
    "text": "You figure out what are the\ndifferent basic blocks of computation that you have and\nwithin the basic block you",
    "start": "2590950",
    "end": "2597200"
  },
  {
    "text": "have lots of instructions. So each one of these green\nboxes is a particular instruction.",
    "start": "2597200",
    "end": "2602559"
  },
  {
    "text": "And what you're seeing-- these\narrows here that connect the edges-- are operands that\nyou have to exchange.",
    "start": "2602560",
    "end": "2608090"
  },
  {
    "text": "So you might have-- ",
    "start": "2608090",
    "end": "2613190"
  },
  {
    "text": "this is an add instruction. It requires a value\ncoming from here. Multiply-- subtract instruction requires\nvalues coming in from",
    "start": "2613190",
    "end": "2619640"
  },
  {
    "text": "different areas. So how would you\ndistribute this across a number of cores-- or across a number of tiles?",
    "start": "2619640",
    "end": "2626720"
  },
  {
    "text": "Any ideas here? So you can look for, for\nexample, some chains that are",
    "start": "2626720",
    "end": "2633349"
  },
  {
    "text": "not interconnected. So you can look for clusters\nthat you can use. And say, OK, well I see no edges\nhere so maybe I can put",
    "start": "2633350",
    "end": "2640940"
  },
  {
    "text": "this on one tile. And then maybe I can put some\nof these instructions on another tile.",
    "start": "2640940",
    "end": "2646440"
  },
  {
    "text": "Because sort of the\ncommunication flow is local. So maybe one strategy might\nbe, look for the longest",
    "start": "2646440",
    "end": "2652630"
  },
  {
    "text": "single chains so you can keep\nthe communication flow. And then you apply and make and\nalgorithm, come up with a",
    "start": "2652630",
    "end": "2658630"
  },
  {
    "text": "number of clusters. Something like that\ndoes happen. And keep in mind from the\nlectures we talked about the",
    "start": "2658630",
    "end": "2666070"
  },
  {
    "start": "2660000",
    "end": "2757000"
  },
  {
    "text": "parallelizing compiler, you\nhave to worry about parallelism versus\ncommunication. Some the more you distribute\nthings, the more communication",
    "start": "2666070",
    "end": "2671800"
  },
  {
    "text": "you have to get right. So here we're showing-- what I'm showing is color\nmapping from the original",
    "start": "2671800",
    "end": "2678400"
  },
  {
    "text": "instructions in the base block\nto the same instructions, but now each color essential\nrepresents a different cluster",
    "start": "2678400",
    "end": "2684290"
  },
  {
    "text": "or essentially code that would\nmap a different thread. So blue is one thread, yellow is\nanother, green is another,",
    "start": "2684290",
    "end": "2692270"
  },
  {
    "text": "red, purple, and so on. But I have to worry about\ncommunication between the different colors because\nthey're essentially two",
    "start": "2692270",
    "end": "2698770"
  },
  {
    "text": "different threads. They're going to run on two\ndifferent processors or two different tiles. So those arrows that are\nhighlighted in dark black are",
    "start": "2698770",
    "end": "2708799"
  },
  {
    "text": "communication edges. They have to explicitly send\nthe operands around. Right?",
    "start": "2708800",
    "end": "2714310"
  },
  {
    "text": "So then I might look\nat the granularity. What is my communication cost? What is my computation cost?",
    "start": "2714310",
    "end": "2719770"
  },
  {
    "text": "And I want to worry about\nload balancing. As we saw, load balancing can\ngive you how it can better",
    "start": "2719770",
    "end": "2726870"
  },
  {
    "text": "make use of your architecture\nand give you better utilization, better\nthroughput. So you might essentially say,\nit doesn't-- it's not",
    "start": "2726870",
    "end": "2733250"
  },
  {
    "text": "worthwhile to have these running\non a different tile because there's a lot of\ncommunication going on.",
    "start": "2733250",
    "end": "2738660"
  },
  {
    "text": "So maybe I'd want to fuse\nthose together. Keep the communication local.",
    "start": "2738660",
    "end": "2743870"
  },
  {
    "text": "And essentially eliminate\ncostly communication. So there are different\nheuristics that you can apply. You can use that 5-tuple.",
    "start": "2743870",
    "end": "2751630"
  },
  {
    "text": "You can use heuristic space on\nthe 5-tuple to determine when it's profitable to break things\nup and when it's not.",
    "start": "2751630",
    "end": "2758510"
  },
  {
    "start": "2757000",
    "end": "2842000"
  },
  {
    "text": "And then you have to worry\nabout placement. So you don't quite have this\non cell in that you create",
    "start": "2758510",
    "end": "2764010"
  },
  {
    "text": "these SPE threads and\nthey can run on any SPE in the raw compiler. You can actually exploit the\nspacial characteristics of the",
    "start": "2764010",
    "end": "2770410"
  },
  {
    "text": "chip in the point-to-point\ncommunication network to say, I want to put these two threads\non tile 1 and tile 2,",
    "start": "2770410",
    "end": "2776950"
  },
  {
    "text": "where tile 1 and tile 2 are\nadjacent to each other. Because I have a well-defined\ncommunication pattern that I'm going to use.",
    "start": "2776950",
    "end": "2782640"
  },
  {
    "text": "And map to the communication\nnetwork on the chip to get really fast, really\nlow latency.",
    "start": "2782640",
    "end": "2789710"
  },
  {
    "text": "So you can take each one of\nthese colors, place it on a different tile. And now you have these wires\nthat are going across these",
    "start": "2789710",
    "end": "2796490"
  },
  {
    "text": "tiles which essentially\nrepresent communication. But now the tile has to worry\nabout, oh, I have to",
    "start": "2796490",
    "end": "2801570"
  },
  {
    "text": "essentially send these\non fixed routes. There's no arbitrary\ncommunication mechanism. So if there's data going from\nthis tile to this tile, it",
    "start": "2801570",
    "end": "2810750"
  },
  {
    "text": "actually has to be routed\nthrough a network. And that might mean getting\nrouting through somebody else's tile.",
    "start": "2810750",
    "end": "2817630"
  },
  {
    "text": "So the next stage would be\ncommunication coordination. You have to figure out which\nswitch you need to go to and",
    "start": "2817630",
    "end": "2825510"
  },
  {
    "text": "what do you do to get that\noperand to the right switch which then gets it to\nthe right processor. So here, I believe the heuristic\nis to do dimension",
    "start": "2825510",
    "end": "2832960"
  },
  {
    "text": "order routing so you send along\nthe x-dimension and then the y-dimension.",
    "start": "2832960",
    "end": "2838859"
  },
  {
    "text": "I might have those reversed. I don't know. And then finally, now you've\nfigured out your communication",
    "start": "2838860",
    "end": "2845609"
  },
  {
    "start": "2842000",
    "end": "2898000"
  },
  {
    "text": "patterns, you've figured out\nyour instructions, you do some instructions scheduling. And what you can do here,\nbecause the communication",
    "start": "2845610",
    "end": "2851359"
  },
  {
    "text": "patterns are static, you've\nsplit up the instructions so you know when you need to ship\ndata around and how.",
    "start": "2851360",
    "end": "2858110"
  },
  {
    "text": "You can guarantee deadlock\nfreedom by carefully ordering your send and receive pairs. So what you see here, every time\nyou see an instruction",
    "start": "2858110",
    "end": "2866369"
  },
  {
    "text": "that needs to ship an operand\naround, there's the equivalent of a route instruction\nthat has route east,",
    "start": "2866370",
    "end": "2871590"
  },
  {
    "text": "west, north, south. There's an equivalent route\ninstruction on the other",
    "start": "2871590",
    "end": "2876940"
  },
  {
    "text": "processors. And that allows you to\nessentially analyze code and say, OK, I've laid these things\nout carefully, I've",
    "start": "2876940",
    "end": "2884020"
  },
  {
    "text": "orchestrated my send and\nreceive pairs so I can guarantee, for example, there\nare no overlapping routes. Or that there are no deadlocks\nbecause one is trying to shift",
    "start": "2884020",
    "end": "2892540"
  },
  {
    "text": "the other while the other is\nalso trying to ship, and they both block on the shared\nnetwork link.",
    "start": "2892540",
    "end": "2899000"
  },
  {
    "start": "2898000",
    "end": "2908000"
  },
  {
    "text": "And finally, you have the\ncode representation. So this is where you package\nthings up into object files,",
    "start": "2899000",
    "end": "2904050"
  },
  {
    "text": "into essentially things\nlike threads. And then you can compile\nthem and run them. Now the question that was posed\nearlier is, well there's",
    "start": "2904050",
    "end": "2912579"
  },
  {
    "start": "2908000",
    "end": "3027000"
  },
  {
    "text": "one thing we haven't talked\nabout and that's branching. This is a sequential program,\nit executes branches.",
    "start": "2912580",
    "end": "2918700"
  },
  {
    "text": "And now I have this loop that\nI've split up across a number of tiles, how do I know who's\ngoing to do the branch?",
    "start": "2918700",
    "end": "2924990"
  },
  {
    "text": "And if one tile is doing\nthe branch, how does it communicate with\neverybody else? Or if I'm going to repeat the\nbranch on every file, does",
    "start": "2924990",
    "end": "2931734"
  },
  {
    "text": "that mean I'm redoing\ntoo much computation on every other tile? So control coordination is\nactually quite an interesting",
    "start": "2931735",
    "end": "2937960"
  },
  {
    "text": "aspect of-- adds another interesting\naspect to the parallelization for raw.",
    "start": "2937960",
    "end": "2944600"
  },
  {
    "text": "So what you have to\ndo is figure out-- there are two different\nways you can do it.",
    "start": "2944600",
    "end": "2949650"
  },
  {
    "text": "Because you have no mechanism\nfor a global message on raw,",
    "start": "2949650",
    "end": "2954750"
  },
  {
    "text": "you can't say, I've taken a\nbranch, everybody go to this program counter. You essentially have to send\neither the branch result so",
    "start": "2954750",
    "end": "2961690"
  },
  {
    "text": "one tile can do the comparison,\nit calculates the condition, and then it has to\ncommunicate x to each of the",
    "start": "2961690",
    "end": "2969490"
  },
  {
    "text": "different branches-- to each\nof the different tiles. Or every tiles has to\nessentially just replicate the",
    "start": "2969490",
    "end": "2974900"
  },
  {
    "text": "control and redo the\ncomputations. So every tile figures out what\nis the condition, what are the",
    "start": "2974900",
    "end": "2980450"
  },
  {
    "text": "conditions for the branch. They redundantly do that\ncomputation and then they can all merge at the same time--",
    "start": "2980450",
    "end": "2987769"
  },
  {
    "text": "at different times. So that gives you two ways\nof doing the branching. If each tile's doing its own\ncontrol flow calculation, then",
    "start": "2987770",
    "end": "2996720"
  },
  {
    "text": "they can essentially branch\nat different times. But if they're all going to\nwait for the result to compare, then it essentially\ngives you points where you",
    "start": "2996720",
    "end": "3002730"
  },
  {
    "text": "have to synchronize. Everybody's going to wait for\nthe result of the branch. But the latency could\nbe different.",
    "start": "3002730",
    "end": "3008320"
  },
  {
    "text": "Because if I'm sending the\nbranch condition to one tile versus another file, and if\none's closer than the other.",
    "start": "3008320",
    "end": "3013390"
  },
  {
    "text": "Then the branch that's closer to\nme-- the tile that's closer to me will take that branch\nearlier in time. So you get sort of the\neffective of a global",
    "start": "3013390",
    "end": "3020850"
  },
  {
    "text": "asynchronous branching\nin either case. Does that make sense?",
    "start": "3020850",
    "end": "3027680"
  },
  {
    "start": "3027000",
    "end": "3093000"
  },
  {
    "text": "So, in summary, the raw\narchitecture is really a tile microprocessor. It incorporates the best\nelements from superscalars in",
    "start": "3027680",
    "end": "3036340"
  },
  {
    "text": "terms of a really low latency\ncommunication network between tiles which really cuts down\non the communication costs.",
    "start": "3036340",
    "end": "3042320"
  },
  {
    "text": "And as we saw, and as probably\nyou've been learning, communication is really\nan expensive part of",
    "start": "3042320",
    "end": "3047830"
  },
  {
    "text": "parallelization on existing\nmulticore chips. And it's also getting the\nscalability of multicores in",
    "start": "3047830",
    "end": "3055670"
  },
  {
    "text": "terms of explicit parallelism\nbut also gives you implicit parallelism because the networks\nare pipelined and",
    "start": "3055670",
    "end": "3062059"
  },
  {
    "text": "they can give you\nfull control. So you're trying to get to the\npoint where you have a tile processor with scalar operand\nnetwork that allows you to do",
    "start": "3062060",
    "end": "3069650"
  },
  {
    "text": "communication with a very low\ncost. And it might be the case in the future that these chips\nwill especially be--",
    "start": "3069650",
    "end": "3076640"
  },
  {
    "text": "more complex architectures will\nsit on top of these so you'll use these as fundamental\nbuilding blocks.",
    "start": "3076640",
    "end": "3082220"
  },
  {
    "text": "And there was the 80 chip\nmulticore from Intel: there",
    "start": "3082220",
    "end": "3089425"
  },
  {
    "text": "have been rumors that that might\nactually be something like a graphics processor that\nhas something like a scalar",
    "start": "3089425",
    "end": "3094770"
  },
  {
    "start": "3093000",
    "end": "3467000"
  },
  {
    "text": "operand network because\nyou could communicate with a very fast-- with very low latency\nbetween tiles.",
    "start": "3094770",
    "end": "3101010"
  },
  {
    "text": "And in that article which came\nout a few months ago was the first time I think that I had\nseen tile architectures used",
    "start": "3101010",
    "end": "3107610"
  },
  {
    "text": "in literature or in\npublications. So I think you'll see more of\nthese kinds of designs pattern",
    "start": "3107610",
    "end": "3113359"
  },
  {
    "text": "appear as people scale out to\nmore than 2 cores, 4 cores, 8 cores and so on, where you\ncould still communication",
    "start": "3113360",
    "end": "3119559"
  },
  {
    "text": "reasonably well with caches. And that's all I prepared\nfor today. Any other questions?",
    "start": "3119560",
    "end": "3125090"
  },
  {
    "text": " And this is a list\nof people who contributed to the raw project.",
    "start": "3125090",
    "end": "3131520"
  },
  {
    "text": "A lot of students who are\nled by Anant and Saman. PROFESSOR AMARASINGHE:\n[OBSCURED]",
    "start": "3131520",
    "end": "3137590"
  },
  {
    "text": "view of what happened in our\ngroups and then how it relates",
    "start": "3137590",
    "end": "3143050"
  },
  {
    "text": "to necessary to what you need. But this is trying to take\nit to a much finer grain.",
    "start": "3143050",
    "end": "3150270"
  },
  {
    "text": "Whereas in Cell, of course, the\nmessage has to be large, you can do a lot of coarse\ngrain stuff.",
    "start": "3150270",
    "end": "3156098"
  },
  {
    "text": "But in raw, you try to do much\nmore fine grain stuff. But we're going to talk\nabout it the next lecture on the future.",
    "start": "3156098",
    "end": "3162135"
  },
  {
    "text": "[OBSCURED] AUDIENCE: [OBSCURED] Don't you need long wires\nfor the clock.",
    "start": "3162135",
    "end": "3169640"
  },
  {
    "text": "PROFESSOR RABBAH: There's\nno global clock. AUDIENCE: So you have this\nnetwork that seems to --",
    "start": "3169640",
    "end": "3176617"
  },
  {
    "text": "So that the network actually\nrequires handshaking? Or-- PROFESSOR AMARASINGHE: The way\nyou can do is, you can in",
    "start": "3176617",
    "end": "3184130"
  },
  {
    "text": "modern processors, [OBSCURED]",
    "start": "3184130",
    "end": "3189650"
  },
  {
    "text": "so since there's no long wire,\nyou can actually carry the clock with the data. So in the global world, the\nswitching here would happen",
    "start": "3189650",
    "end": "3196370"
  },
  {
    "text": "when the switching here. But since there's no big wire\nconnecting, then that's OK.",
    "start": "3196370",
    "end": "3203490"
  },
  {
    "text": "So you can deal with\nclock ticking. AUDIENCE: So this is\nnot going to be",
    "start": "3203490",
    "end": "3209187"
  },
  {
    "text": "not clock drift because-- PROFESSOR AMARASINGHE: Yeah,\nthat's clock drift. One end of the process clock\nis happening at the global",
    "start": "3209187",
    "end": "3217320"
  },
  {
    "text": "instant time at the other\nend of the processor. ",
    "start": "3217320",
    "end": "3228130"
  },
  {
    "text": "And since the wires also kind\nof go in the tree, you can deal with that.",
    "start": "3228130",
    "end": "3233359"
  },
  {
    "text": "AUDIENCE: Drift meaning\nticking at different rates, not just-- PROFESSOR AMARASINGHE:\nYeah, I know.",
    "start": "3233360",
    "end": "3238363"
  },
  {
    "text": "Basically I don't think\nI can go back to it. It has a skew. There's a clock skew going\nin between those.",
    "start": "3238363",
    "end": "3245090"
  },
  {
    "text": "AUDIENCE: So you don't need\nsynchronizers between the different tiles? PROFESSOR AMARASINGHE: No, we\ndon't need synchronizers because tiles are local. The clock would bring\nthose tiles.",
    "start": "3245090",
    "end": "3251400"
  },
  {
    "text": "The clock would bring two things\nthat communicate close enough that it fits\nit in the cycle.",
    "start": "3251400",
    "end": "3257099"
  },
  {
    "text": "But for example, if you get it\ntwo very far away branches of a tree and then if you try to\ncommunicate with them then you",
    "start": "3257100",
    "end": "3263169"
  },
  {
    "text": "have a problem. Another thing is when the tree\ngoes here, you want to use two different branches it's\nsimilar to going down.",
    "start": "3263169",
    "end": "3270170"
  },
  {
    "text": "So you can compress\nthe process. So there are all these things. I mean, modern processors\nreally really destable. ",
    "start": "3270170",
    "end": "3277310"
  },
  {
    "text": "The problem occurs when you try\nto connect directly from the far end of the branch to\nsomething that gets clocked",
    "start": "3277310",
    "end": "3284270"
  },
  {
    "text": "there to something that\nclocks at a very early end of the branch. If you're trying to connect\nthose two, then the skew might",
    "start": "3284270",
    "end": "3290070"
  },
  {
    "text": "be too long. Then you can get into\nclock trouble. AUDIENCE: [OBSCURED] I was just worried about\nthis local network.",
    "start": "3290070",
    "end": "3297283"
  },
  {
    "text": "[OBSCURED]",
    "start": "3297283",
    "end": "3304224"
  },
  {
    "text": "AUDIENCE: Another question I had\nwas in the mesh, obviously",
    "start": "3304224",
    "end": "3311386"
  },
  {
    "text": "the processors in the middle\nhave further to get to the I/O devices or to the main memory.",
    "start": "3311386",
    "end": "3318859"
  },
  {
    "text": "What do you see happening as you\nget to larger and larger processors? Are they going to just put more\nand more local memory on",
    "start": "3318860",
    "end": "3325282"
  },
  {
    "text": "the tile and [OBSCURED] it, or are they going to add\nextra memory buses on it?",
    "start": "3325282",
    "end": "3330500"
  },
  {
    "text": "PROFESSOR RABBAH: It could\nbe a combination of both. So it's not just memory,\nI/O devices.",
    "start": "3330500",
    "end": "3335950"
  },
  {
    "text": "If you're doing I/O then you\nmight to be placed at a part of the chip that has direct\naccess to an I/O device or",
    "start": "3335950",
    "end": "3342165"
  },
  {
    "text": "very close. It also comes up in the case\nof the communication orchestration.",
    "start": "3342165",
    "end": "3347470"
  },
  {
    "text": "So if this guy is doing the\nbranch then you want him essentially centrally located.",
    "start": "3347470",
    "end": "3353270"
  },
  {
    "text": "So the best patterns for\nallocating things is essentially across. It's like a plus sign where\nit branches in the middle.",
    "start": "3353270",
    "end": "3359670"
  },
  {
    "text": "PROFESSOR AMARASINGHE: But\nthat's not [OBSCURED]. You can make them uniform by\neverybody equally there.",
    "start": "3359670",
    "end": "3367420"
  },
  {
    "text": "And a lot of times people have\ndone that simple model with everybody equally there Or you\ntry to take advantage of",
    "start": "3367420",
    "end": "3376353"
  },
  {
    "text": "closeness and stuff like that. So you can't have both ways. So anytime you try to\nmake me [OBSCURED] very, very close and fast\naccess, you're are doing it by",
    "start": "3376353",
    "end": "3384580"
  },
  {
    "text": "basically making the other parts\nto have less resources",
    "start": "3384580",
    "end": "3390652"
  },
  {
    "text": "and less access. On the other hand, there are\na lot of people working on [INAUDIBLE]",
    "start": "3390652",
    "end": "3398690"
  },
  {
    "text": "things that, for example,\nthere's a thing called tree space laser. So what that does is you put a\nmirror on top of the tile, on",
    "start": "3398690",
    "end": "3405920"
  },
  {
    "text": "top of the processor. And each of these-- you can\nembed a small LED transmitter",
    "start": "3405920",
    "end": "3418920"
  },
  {
    "text": "into the chip. So basically if you want to\ncommunicate with someone, you just bounce that laser on\ntop of that and get it to the right guy.",
    "start": "3418920",
    "end": "3424660"
  },
  {
    "text": "So there are a lot of exotic\nthings that might be able to solve this thing, technological\nproblem. But in some case,\nspeed of light--",
    "start": "3424660",
    "end": "3431160"
  },
  {
    "text": "I don't think an engineer\nhas figured out how to break speed of light. Unless, of course, people go\nwith quantum computing and",
    "start": "3431160",
    "end": "3437925"
  },
  {
    "text": "stuff like that. So, I mean the key thing is, you\nhave resources, you have certain data and you just\nhave to deal with it. Getting nice uniformity\nhas a cost.",
    "start": "3437925",
    "end": "3446190"
  },
  {
    "text": "PROFESSOR RABBAH: Yeah, I\nmean, on the [OBSCURED] that are groups here at MIT\nwho are working on optical networks in the third\ndimension.",
    "start": "3446190",
    "end": "3452210"
  },
  {
    "text": "So you have a tile chip plus\nan optical network in the third dimension which allows\nyou to do things like broadcast much more\nefficiently.",
    "start": "3452210",
    "end": "3458214"
  },
  {
    "text": "OK? PROFESSOR AMARASINGHE: I guess\nwe'll take a break here and take a small, three-minute break\nand then we can go on to the next topic.",
    "start": "3458214",
    "end": "3463536"
  },
  {
    "start": "3463536",
    "end": "3468002"
  }
]