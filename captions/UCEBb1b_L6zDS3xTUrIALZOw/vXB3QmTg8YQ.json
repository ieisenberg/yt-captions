[
  {
    "text": "PROFESSOR: So the idea behind\nthe encoder is increase the",
    "start": "0",
    "end": "13560"
  },
  {
    "text": "minimum distance at the cost\nof spectral efficiency.",
    "start": "13560",
    "end": "23760"
  },
  {
    "start": "23760",
    "end": "32169"
  },
  {
    "text": "The idea behind the decoder\nwas the following.",
    "start": "32170",
    "end": "38070"
  },
  {
    "text": "You have a received signal, y. And the job of the decoder is\nto find x hat, which belongs",
    "start": "38070",
    "end": "49930"
  },
  {
    "text": "to the signal set so that\nyou minimize your probability of error.",
    "start": "49930",
    "end": "55920"
  },
  {
    "text": "And the probability of error\nx is not equal to x-hat.",
    "start": "55920",
    "end": "61839"
  },
  {
    "text": "That's the probability of\nerror for your decoder. And we saw a bunch of\nequivalence rules -- we said",
    "start": "61840",
    "end": "69090"
  },
  {
    "text": "that the minimum probability of\nerror rule, which is this rule here, is the same as the\nmaximum a posteriori rule.",
    "start": "69090",
    "end": "78900"
  },
  {
    "text": "And this just follows\nby the definition. This was the same as the maximum\nlikelihood rule.",
    "start": "78900",
    "end": "88160"
  },
  {
    "text": "And here we made one assumption\nthat all the signals in your signal set\nA are equally likely.",
    "start": "88160",
    "end": "95840"
  },
  {
    "text": "And this was shown to be\nequivalent to your minimum distance decision rule.",
    "start": "95840",
    "end": "102130"
  },
  {
    "text": "And in order to show this\nequivalence, we use the fact that the noise was White\nand Gaussian.",
    "start": "102130",
    "end": "108630"
  },
  {
    "text": "So this assumes equi-probable\nsignals. ",
    "start": "108630",
    "end": "119290"
  },
  {
    "text": "And this assumes AWGN channel. OK. ",
    "start": "119290",
    "end": "128259"
  },
  {
    "text": "In fact, the minimum distance\ndecisions has very nice geometrical properties. ",
    "start": "128259",
    "end": "137720"
  },
  {
    "text": "We basically said that the\ndecision regions had a particular shape. They were convex polytopes.",
    "start": "137720",
    "end": "143100"
  },
  {
    "text": "And they were also known\nas Voronoi regions. However, finding the exact\nexpression for the probability",
    "start": "143100",
    "end": "150370"
  },
  {
    "text": "of error seemed quite tedious\nso we decided to settle with bounds.",
    "start": "150370",
    "end": "155640"
  },
  {
    "text": "And we developed some bounds for\nthe probability of error. ",
    "start": "155640",
    "end": "162530"
  },
  {
    "text": "And the main bounds were -- we\nhad a strict upper bound, which was the summation. OK.",
    "start": "162530",
    "end": "168187"
  },
  {
    "start": "168187",
    "end": "181220"
  },
  {
    "text": "This is a strict upper bound. Well, the inequality here, say\nit's a strict upper bound.",
    "start": "181220",
    "end": "186890"
  },
  {
    "text": "We also had a union\nbound estimate. Because this is a sum of various\nQ functions, this is",
    "start": "186890",
    "end": "192910"
  },
  {
    "text": "still a very cumbersome\nexpression to evaluate. So we wanted a simpler\nexpression.",
    "start": "192910",
    "end": "198440"
  },
  {
    "text": "So we then approximated this\nby K_min of aj times Q of",
    "start": "198440",
    "end": "208650"
  },
  {
    "text": "d_min over 2 sigma. OK, and now if we take the\naverage on both sides, what we",
    "start": "208650",
    "end": "217750"
  },
  {
    "text": "get is the probability of error\nis approximately equal",
    "start": "217750",
    "end": "223650"
  },
  {
    "text": "to K_min of the constellation,\nwhich is the average number of nearest neighbors, times Q\nof d_min over 2 sigma.",
    "start": "223650",
    "end": "235890"
  },
  {
    "text": " And this expression is known as\nthe union bound estimate.",
    "start": "235890",
    "end": "242600"
  },
  {
    "start": "242600",
    "end": "250280"
  },
  {
    "text": "We call it an estimate because\nit's not a bound anymore. We have made an approximation\ngoing from here to here.",
    "start": "250280",
    "end": "256370"
  },
  {
    "text": "So this is what we came\nup with last time. Throughout the analysis today,\nwhenever we have a probability",
    "start": "256370",
    "end": "263076"
  },
  {
    "text": "of error expression, we will be\nestimating it by the union bound estimate. So this union bound estimate\nis quite important.",
    "start": "263076",
    "end": "271310"
  },
  {
    "text": "Are there any questions\non this? OK, so today we will start by\nintroducing the notion of",
    "start": "271310",
    "end": "280700"
  },
  {
    "text": "effective coding gain. ",
    "start": "280700",
    "end": "298240"
  },
  {
    "text": "And we'll use the symbol\ngamma sub f for that.",
    "start": "298240",
    "end": "303330"
  },
  {
    "text": "What do we mean by effective\ncoding gain? First you have to decide what\nregion you're operating in.",
    "start": "303330",
    "end": "308630"
  },
  {
    "text": "It's defined for both\nbandwidth-limited regime and power-limited regimes. So let us start with the\npower-limited regime.",
    "start": "308630",
    "end": "315240"
  },
  {
    "start": "315240",
    "end": "324610"
  },
  {
    "text": "Remember in the power-limited\nregime, the trade-off we care about is the probability of bit\nerror versus Eb over N_0.",
    "start": "324610",
    "end": "335740"
  },
  {
    "text": "The baseline scheme is 2-PAM. ",
    "start": "335740",
    "end": "345099"
  },
  {
    "text": "And for 2-PAM, we showed two\nlectures ago that the probability of bit error is\ngiven by Q of root 2 Eb N_0.",
    "start": "345100",
    "end": "356650"
  },
  {
    "text": " OK, it should be --",
    "start": "356650",
    "end": "364310"
  },
  {
    "text": "so now the idea is to plot your\nprobability of bit error",
    "start": "364310",
    "end": "370870"
  },
  {
    "text": "as a function of Eb N_0. ",
    "start": "370870",
    "end": "376150"
  },
  {
    "text": "Eb N_0 is always plotted\nin dB on the x-axis. The probability of bit\nerror we also plot on",
    "start": "376150",
    "end": "383790"
  },
  {
    "text": "a logarithmic scale. So this is ten to\nthe minus six. This is ten to the minus\nfive, ten to the",
    "start": "383790",
    "end": "390389"
  },
  {
    "text": "minus four and so on. The Shannon limit is at --",
    "start": "390390",
    "end": "397419"
  },
  {
    "text": "this is the Shannon limit -- is at minus 1.59 dB.",
    "start": "397420",
    "end": "405830"
  },
  {
    "text": "OK? For the power-limited regime,\nthe performance of 2-PAM,",
    "start": "405830",
    "end": "412750"
  },
  {
    "text": "which is given by this\nexpression, is here. And we basically go to say that\nwhen the probability of",
    "start": "412750",
    "end": "421260"
  },
  {
    "text": "bit error is ten to the minus\nfive, the EbN _0 that you require is 9.6 dB.",
    "start": "421260",
    "end": "431310"
  },
  {
    "text": "This is a nine.  So what's the idea behind the\neffective coding gain?",
    "start": "431310",
    "end": "439540"
  },
  {
    "text": "Well, suppose I give you a\ncertain constellation, OK, a certain signal set.",
    "start": "439540",
    "end": "444940"
  },
  {
    "text": "And you find the probability\nof bit error for that, and you plot it. And it comes out to be\nsomething like this.",
    "start": "444940",
    "end": "451970"
  },
  {
    "text": "So this is the performance for\nsome given signal set A. This",
    "start": "451970",
    "end": "457470"
  },
  {
    "text": "is your 2-PAM. ",
    "start": "457470",
    "end": "463150"
  },
  {
    "text": "And now you want to quantify how\nmuch better is the signal set over the baseline system.",
    "start": "463150",
    "end": "469390"
  },
  {
    "text": "Now, so in other words you\nwant to look at the gap between the two curves. Now clearly the gap is going\nto be a function of the",
    "start": "469390",
    "end": "475580"
  },
  {
    "text": "probability of bit\nerror, right? But the basic idea now is you\nwant to fix a certain",
    "start": "475580",
    "end": "481990"
  },
  {
    "text": "probability of bit error because\nif you're designing a system, the probability of bit\nerror is something that the",
    "start": "481990",
    "end": "487070"
  },
  {
    "text": "system tolerates and is\ngoing to be fixed throughout the analysis. So in this course, we'll\nbe fixing it at ten",
    "start": "487070",
    "end": "492320"
  },
  {
    "text": "to the minus five. You fix this probability of bit\nerror to ten to the minus five, and you look at how\nmuch a gap you have",
    "start": "492320",
    "end": "498610"
  },
  {
    "text": "between the two curves. And this gap is going\nto be your effective",
    "start": "498610",
    "end": "503629"
  },
  {
    "text": "coding gain in dB. ",
    "start": "503630",
    "end": "513049"
  },
  {
    "text": "So in other words, effective\ncoding gain, I can write here, is the gap between a given\nsignal set and concordant",
    "start": "513049",
    "end": "534340"
  },
  {
    "text": "system, 2-PAM, at a fixed\nprobability of bit error.",
    "start": "534340",
    "end": "541415"
  },
  {
    "text": " OK? So now let us try to quantify\nthis effective coding gain",
    "start": "541415",
    "end": "549470"
  },
  {
    "text": "using the union bound estimate\nthat we have here. So if I have a certain signal\nset, A, then I know from the",
    "start": "549470",
    "end": "558120"
  },
  {
    "text": "union bound estimate that the\nprobability of error is given",
    "start": "558120",
    "end": "563370"
  },
  {
    "text": "by K_min of A times\napproximately Q of",
    "start": "563370",
    "end": "573060"
  },
  {
    "text": "d_min over 2 sigma. So since I want that expression\nin terms of",
    "start": "573060",
    "end": "579199"
  },
  {
    "text": "probability of bit error,\nI will use the fact that probability of bit error is\nprobability of error per",
    "start": "579200",
    "end": "586620"
  },
  {
    "text": "symbols or the number\nof bits per symobl. Since I have endpoints in my\nsignal set, I have logM bits",
    "start": "586620",
    "end": "595730"
  },
  {
    "text": "per symbol.  And this is then K_min of A over\nlogM base 2 times Q of",
    "start": "595730",
    "end": "615980"
  },
  {
    "text": "d_min over 2 sigma. ",
    "start": "615980",
    "end": "633910"
  },
  {
    "text": "So I'm going to rewrite my\nprobability of bit error expression now in this --",
    "start": "633910",
    "end": "640620"
  },
  {
    "text": "in fact the right side\nin this quad. K_b of A times Q of root\n2 gamma_c of A",
    "start": "640620",
    "end": "656390"
  },
  {
    "text": "times Eb over N_0. So I'm just going to define\nthe right hand side in this way.",
    "start": "656390",
    "end": "661880"
  },
  {
    "text": "And I'm going to relate gamma_c\nof A and K_b of A to the parameters I have on the\nright hand side there.",
    "start": "661880",
    "end": "668620"
  },
  {
    "text": "And why do I want to\ndo it this way? Because I want to get an\nexpression as close as possible to the performance\nof an uncoded 2-PAM system",
    "start": "668620",
    "end": "677330"
  },
  {
    "text": "because that way things will be\neasier to do if I want to calculate the effective\ncoding gain, OK?",
    "start": "677330",
    "end": "684530"
  },
  {
    "text": "So how do the parameters\nrelate? Well, K_b of A is K_min of A\nover logM to the base 2.",
    "start": "684530",
    "end": "700810"
  },
  {
    "text": "And this is the average number\nof nearest neighbors per",
    "start": "700810",
    "end": "716430"
  },
  {
    "text": "information bit.",
    "start": "716430",
    "end": "724110"
  },
  {
    "text": "If I compare the arguments\ninside the Q function, what I have is 2 gamma_c of A times Eb\nN_0 is the argument d_min",
    "start": "724110",
    "end": "740570"
  },
  {
    "text": "over 2 sigma squared, which I\nwill write as d_min squared over 4 sigma squared.",
    "start": "740570",
    "end": "748959"
  },
  {
    "text": "So remember sigma squared\nis N_0 over 2. So if I simplify this\nexpression, what I will end up",
    "start": "748960",
    "end": "756397"
  },
  {
    "text": "getting is gamma_c of A is\nd_min squared over 4 Eb.",
    "start": "756397",
    "end": "765524"
  },
  {
    "text": " Gamma_c of A is known as the\nnominal coding gain.",
    "start": "765525",
    "end": "771500"
  },
  {
    "start": "771500",
    "end": "783680"
  },
  {
    "text": "It's not the same as the\neffective coding gain, which is the distance between\nthe two curves.",
    "start": "783680",
    "end": "788880"
  },
  {
    "text": "OK, are there any questions\nso far? So far what I have done is given\na constellation A, I can",
    "start": "788880",
    "end": "798440"
  },
  {
    "text": "find two parameters. I can find the nominal coding\ngain, and I can find the",
    "start": "798440",
    "end": "805490"
  },
  {
    "text": "average number of nearest\nneighbors per information bit. And using these two parameters,\nwe will try to get",
    "start": "805490",
    "end": "812630"
  },
  {
    "text": "a handle over the effective\ncoding gain. So that's the idea. So how can we plot --",
    "start": "812630",
    "end": "818490"
  },
  {
    "text": "so given this constellation\nA, we want to plot the probability of bit error\ncurve like this.",
    "start": "818490",
    "end": "824050"
  },
  {
    "text": "But instead of plotting the\nexact curve we will be plotting this curve here,\nwhich is the union bound",
    "start": "824050",
    "end": "829600"
  },
  {
    "text": "estimate of your probability\nof bit error. So we will always be plotting\nthe union bound estimate.",
    "start": "829600",
    "end": "837290"
  },
  {
    "text": "So how do we do that? So we'll again start with\na curve like that. Probability of bit error as\na function of Eb N_0.",
    "start": "837290",
    "end": "847930"
  },
  {
    "text": "This is in dB. The y-axis is also in dB.",
    "start": "847930",
    "end": "853980"
  },
  {
    "text": "Sorry, not in dB but\non the log scale. ",
    "start": "853980",
    "end": "862995"
  },
  {
    "text": "And so on. This is the Shannon\nlimit at minus 1.",
    "start": "862995",
    "end": "868870"
  },
  {
    "text": "59 dB. ",
    "start": "868870",
    "end": "874270"
  },
  {
    "text": "This is the 2-PAM performance. ",
    "start": "874270",
    "end": "879420"
  },
  {
    "text": "So if we want to plot this union\nbound estimate, you will be doing two steps.",
    "start": "879420",
    "end": "885250"
  },
  {
    "text": "First, we'll be plotting this\nQ function, a curve corresponding to\nthe Q function. And then we'll be scaling it.",
    "start": "885250",
    "end": "892110"
  },
  {
    "text": "So if I just want to plot this\nQ function, how will it compare to this curve here?",
    "start": "892110",
    "end": "898070"
  },
  {
    "start": "898070",
    "end": "905620"
  },
  {
    "text": "Well, you're just scaling the\nargument inside the Q function, right? But now because we are plotting\nthe x-axis on a dB",
    "start": "905620",
    "end": "913280"
  },
  {
    "text": "scale, it simply means that we\nare translating to the left by an amount of gamma_c in dB.",
    "start": "913280",
    "end": "920670"
  },
  {
    "text": "Is that clear? Well, let's see. What we are doing is we are\ngoing to scale the x-axis by a",
    "start": "920670",
    "end": "928500"
  },
  {
    "text": "certain constant. So this means that we will have\nto scale x-axis here. But, our Eb N_0 is being plotted\non a dB scale, right?",
    "start": "928500",
    "end": "937480"
  },
  {
    "text": "So multiplying in the linear\nscale corresponds to an addition on the logarithmic\nscale.",
    "start": "937480",
    "end": "943680"
  },
  {
    "text": "An addition on the logarithmic\nscale simply implies a shift on the x-axis so that's the\nconstant shift on the x-axis.",
    "start": "943680",
    "end": "952170"
  },
  {
    "text": "I'm going to plot it here. This curve is going to be\nparallel to the original 2-PAM",
    "start": "952170",
    "end": "958019"
  },
  {
    "text": "curve to the best\nof my abilities. OK, so this is what we get\nas your Q function.",
    "start": "958020",
    "end": "965010"
  },
  {
    "text": "Now we are going to scale the\ny-axis by this factor here, K_b of A. But remember the\ny-axis is also on a",
    "start": "965010",
    "end": "974300"
  },
  {
    "text": "logarithmic scale, rather\nthan the linear scale. So multiply [INAUDIBLE] by a\nfactor of K_b of A implies a",
    "start": "974300",
    "end": "981009"
  },
  {
    "text": "vertical shift by a\ncertain factor. So I'm going to do a vertical\nshift here.",
    "start": "981010",
    "end": "987760"
  },
  {
    "start": "987760",
    "end": "994600"
  },
  {
    "text": "So let me make that\ndarker now. So this is what I\nend up getting.",
    "start": "994600",
    "end": "1000060"
  },
  {
    "text": " OK, now note that the distance\nbetween these two curves is",
    "start": "1000060",
    "end": "1006250"
  },
  {
    "text": "not fixed anymore. If the slope is steeper, then\nthis distance is small,",
    "start": "1006250",
    "end": "1011460"
  },
  {
    "text": "meaning that this distance\nhere is large. On the other hand, if the slope\nis going to be smaller",
    "start": "1011460",
    "end": "1017829"
  },
  {
    "text": "here, then this distance\nis large. So this means that a distance\nbetween these two curves is no",
    "start": "1017830",
    "end": "1023770"
  },
  {
    "text": "longer fixed. And basically what we're saying\nhere is that if the probability of error is large,\nthen the number of nearest",
    "start": "1023770",
    "end": "1031109"
  },
  {
    "text": "neighbors matters much more. But if the probability of error\nis going to be quite small, then the more important\nfactor is your exponent, how",
    "start": "1031109",
    "end": "1039400"
  },
  {
    "text": "fast the slope decays as opposed\nto the number of nearest neighbors, OK?",
    "start": "1039400",
    "end": "1046349"
  },
  {
    "text": "This distance here is your\neffective coding gain.",
    "start": "1046349",
    "end": "1051630"
  },
  {
    "text": "And this constant horizontal\ndistance here is your nominal",
    "start": "1051630",
    "end": "1057420"
  },
  {
    "text": "coding gain in dB. ",
    "start": "1057420",
    "end": "1065570"
  },
  {
    "text": "Are there any questions\non this curve? AUDIENCE: [INAUDIBLE]",
    "start": "1065570",
    "end": "1071842"
  },
  {
    "text": "on the right of four 2-PAM\nbecause these are the logM.",
    "start": "1071842",
    "end": "1077800"
  },
  {
    "text": "PROFESSOR: So so what I'm\nreally assuming is that gamma_c of A is greater\nthan 1.",
    "start": "1077800",
    "end": "1083350"
  },
  {
    "text": "So if I'm adding it, then\nI'm going to shift to the left, right? ",
    "start": "1083350",
    "end": "1099600"
  },
  {
    "text": "OK, so a couple of remarks. ",
    "start": "1099600",
    "end": "1111980"
  },
  {
    "text": "I'm just summarizing a few\nremarks I made while plotting the curve. First is the log-log scale is\nvery convenient because any",
    "start": "1111980",
    "end": "1133430"
  },
  {
    "text": "multiplicative factor simply\ninvolves a translation along the x and y directions.",
    "start": "1133430",
    "end": "1140040"
  },
  {
    "text": "The second point is that the\naccuracy we have in the",
    "start": "1140040",
    "end": "1147510"
  },
  {
    "text": "effective coding gain in this\nway is determined by the union",
    "start": "1147510",
    "end": "1161330"
  },
  {
    "text": "bound estimate because that's\nall we are plotting. We are not really plotting\nthe exact curve.",
    "start": "1161330",
    "end": "1168160"
  },
  {
    "text": " And the third point is that\nwe have a rule of thumb.",
    "start": "1168160",
    "end": "1175380"
  },
  {
    "start": "1175380",
    "end": "1181770"
  },
  {
    "text": "Because the distance between the\ncurves is not fixed, it's still too tedious to find the\nexact numerical value of",
    "start": "1181770",
    "end": "1187720"
  },
  {
    "text": "effective coding gain. So what we can find is the value\nof the nominal coding",
    "start": "1187720",
    "end": "1192750"
  },
  {
    "text": "gain and K_b of A exactly,\ngiven a constellation. So we want to have a rule\nof thumb for the",
    "start": "1192750",
    "end": "1200320"
  },
  {
    "text": "effective coding gain. And the rule of thumb is that\nthe effective coding gain is",
    "start": "1200320",
    "end": "1206279"
  },
  {
    "text": "given by the nominal coding gain\nin dB minus 0.2 log2 of",
    "start": "1206280",
    "end": "1216050"
  },
  {
    "text": "K_b of A. This is in dB. What this means is every factor\nof two increase is K_b",
    "start": "1216050",
    "end": "1225450"
  },
  {
    "text": "results in a loss of 0.2 dB in\nthe effective coding gain. OK, and this is our probability\nof bit error of",
    "start": "1225450",
    "end": "1235520"
  },
  {
    "text": "ten to the minus five. So that's the region that we\nwill be doing our analysis in.",
    "start": "1235520",
    "end": "1242059"
  },
  {
    "text": "So this rule of thumb\nis quite helpful. Are there any questions? AUDIENCE: [INAUDIBLE]",
    "start": "1242060",
    "end": "1249340"
  },
  {
    "text": "PROFESSOR: This rule of thumb? Well, it works quite well\nin practice so you want to use that rule.",
    "start": "1249340",
    "end": "1254860"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: Right. ",
    "start": "1254860",
    "end": "1266360"
  },
  {
    "text": "If things are clear, let\nus do some examples. ",
    "start": "1266360",
    "end": "1277090"
  },
  {
    "text": "So let us start with a\n2-PAM for examples. ",
    "start": "1277090",
    "end": "1284810"
  },
  {
    "text": "The first is 2-PAM system.  What's the effective coding\ngain going to be for this",
    "start": "1284810",
    "end": "1292950"
  },
  {
    "text": "constellation? It's going to be 0 dB, right?",
    "start": "1292950",
    "end": "1298250"
  },
  {
    "text": "It just follows from\nthe definition.  I mean the effective coding gain\ntells us how much gap we",
    "start": "1298250",
    "end": "1306790"
  },
  {
    "text": "have between this new\nconstellation and 2-PAM. If your new constellation is\n2-PAM itself, we don't have",
    "start": "1306790",
    "end": "1312990"
  },
  {
    "text": "any effective coding gain. What about the 4-QAM? ",
    "start": "1312990",
    "end": "1321120"
  },
  {
    "text": "It's going to be still 0,\nbecause a 4-QAM is a Cartesian product of two 2-PAM\nconstellations.",
    "start": "1321120",
    "end": "1326820"
  },
  {
    "text": "And we argued last time that\nthere is no coding gain if you use a Cartesian product.",
    "start": "1326820",
    "end": "1331970"
  },
  {
    "text": "If you want to verify it using\nthe framework that we have just developed, say we\nhave four points.",
    "start": "1331970",
    "end": "1341420"
  },
  {
    "text": "This point is alpha, alpha. This point is minus alpha,\nalpha, minus alpha, minus",
    "start": "1341420",
    "end": "1350580"
  },
  {
    "text": "alpha, and alpha minus alpha. The nominal coding gain\nis given by d_min",
    "start": "1350580",
    "end": "1359010"
  },
  {
    "text": "squared over 4 Eb. ",
    "start": "1359010",
    "end": "1365080"
  },
  {
    "text": "The minimum distance is 3 alpha\nbetween any two points. So we have 4 alpha squared.",
    "start": "1365080",
    "end": "1372250"
  },
  {
    "text": "Then what's the energy\nper bit? Well, the energy per symbol\nis 2 alpha squared. Then we have two bits per\nsymbol, so the energy per bit",
    "start": "1372250",
    "end": "1380240"
  },
  {
    "text": "is alpha squared. So we have 4 alpha squared,\nand so that's 1. So we do not have any\nnominal coding gain.",
    "start": "1380240",
    "end": "1388059"
  },
  {
    "text": "K_b of A, what's that\ngoing to be? Well, we have two nearest\nneighbors per symbol, but we",
    "start": "1388060",
    "end": "1393870"
  },
  {
    "text": "also have two bits per symbol. So the number of nearest\nneighbors per bit is going to be one.",
    "start": "1393870",
    "end": "1398940"
  },
  {
    "text": "So your nominal coding gain is\n1, K_b of A is going to be 1.",
    "start": "1398940",
    "end": "1403960"
  },
  {
    "text": "And so we do not see\nany coding gain for the 4-QAM system.",
    "start": "1403960",
    "end": "1410880"
  },
  {
    "text": "Now let me do a slightly\ndifferent case. Let me start with a 4-QAM,\nbut I remove two points.",
    "start": "1410880",
    "end": "1417770"
  },
  {
    "text": "I remove this point, and\nI remove this point. And I only keep the\ntwo points here.",
    "start": "1417770",
    "end": "1423610"
  },
  {
    "text": "So I have a constellation, say,\nA prime, which only has",
    "start": "1423610",
    "end": "1428710"
  },
  {
    "text": "two points, one point here\nand one point here. So this point is alpha, alpha,\nand this point is minus alpha,",
    "start": "1428710",
    "end": "1435750"
  },
  {
    "text": "minus alpha. Any idea what the effective\ncoding gain will be for this case?",
    "start": "1435750",
    "end": "1441730"
  },
  {
    "text": " AUDIENCE: I think\nit's the same.",
    "start": "1441730",
    "end": "1447920"
  },
  {
    "text": "PROFESSOR: It's still\nzero, right? AUDIENCE: Yeah, because it's\nsaying that's 2-PAM. PROFESSOR: Exactly. All this is a 2-PAM\nconstellation embedded in two",
    "start": "1447920",
    "end": "1457600"
  },
  {
    "text": "dimensions. It's 2-PAM along this\nline, right? So I cannot hope to have a\nhigher coding gain for this",
    "start": "1457600",
    "end": "1464780"
  },
  {
    "text": "constellation over\nthe original one. I mean I told you the story that\nwe can take a subset of",
    "start": "1464780",
    "end": "1470340"
  },
  {
    "text": "points, and we can increase\nthe minimum distance by throwing away some points.",
    "start": "1470340",
    "end": "1475360"
  },
  {
    "text": "But one has to be careful\nin that analysis. There could be examples where\nyou're strictly improving the",
    "start": "1475360",
    "end": "1481279"
  },
  {
    "text": "minimum distance. Note that the minimum distance\nhere is going to 8 alpha",
    "start": "1481280",
    "end": "1489460"
  },
  {
    "text": "squared now. OK, so the minimum distance\nis strictly improved.",
    "start": "1489460",
    "end": "1494970"
  },
  {
    "text": "But what happens to be\nyour energy per bit? Well, the energy per symbol\nis 2 alpha squared.",
    "start": "1494970",
    "end": "1501780"
  },
  {
    "text": "But you only have one bit per\nsymbol, so the energy per bit is 2 alpha squared.",
    "start": "1501780",
    "end": "1507760"
  },
  {
    "text": "So your nominal coding gain is\nd_min squared over 4 Eb, which",
    "start": "1507760",
    "end": "1513710"
  },
  {
    "text": "follows from the relation\nwe have here. And d_min squared is\n8 alpha squared.",
    "start": "1513710",
    "end": "1519310"
  },
  {
    "text": "4 Eb is also 8 alpha squared, so\nthe nominal coding gain is 1, or 0 dB.",
    "start": "1519310",
    "end": "1524410"
  },
  {
    "text": " The average number of nearest\nneighbors, well, we only have",
    "start": "1524410",
    "end": "1530810"
  },
  {
    "text": "one nearest neighbor, and we\nhave one bit per symbol, so that's -- and this is A\nprime by the way --",
    "start": "1530810",
    "end": "1537950"
  },
  {
    "text": "is going to be 1. And so the effective coding\ngain is still 0 dB. ",
    "start": "1537950",
    "end": "1546919"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]  PROFESSOR: So probability of\nsymbol error as a function of",
    "start": "1546920",
    "end": "1554169"
  },
  {
    "text": "alpha has definitely\ndecreased, right. But the trade-off we care about\nis probability of bit error as a function of Eb N_0.",
    "start": "1554170",
    "end": "1560740"
  },
  {
    "text": "In other words, this curve\nis still here. It's not changed. I just shifted right by\nreducing my spectral",
    "start": "1560740",
    "end": "1567450"
  },
  {
    "text": "efficiency. OK? ",
    "start": "1567450",
    "end": "1575559"
  },
  {
    "text": "So let's do one last example\njust to prove that it's not always the case that you get\nnothing by removing points.",
    "start": "1575560",
    "end": "1583290"
  },
  {
    "start": "1583290",
    "end": "1655990"
  },
  {
    "text": "OK, so the contellation that\nI have in mind is the one I",
    "start": "1655990",
    "end": "1662770"
  },
  {
    "text": "introduced last time. We'll start with a 3, 4\nCartesian product of 2-PAM.",
    "start": "1662770",
    "end": "1670170"
  },
  {
    "text": "OK, so in three dimensions\nit's a cube, and the 3,4 Cartesian product will\nhave a point on each",
    "start": "1670170",
    "end": "1677840"
  },
  {
    "text": "vertex of the cube. And we only keep four points. So the points we keep are\nthese four points.",
    "start": "1677840",
    "end": "1683377"
  },
  {
    "text": "OK? ",
    "start": "1683377",
    "end": "1689799"
  },
  {
    "text": "So I write B, the coordinates\nare alpha, alpha, alpha, minus",
    "start": "1689800",
    "end": "1699300"
  },
  {
    "text": "alpha, minus alpha, alpha,\nminus alpha, alpha, minus",
    "start": "1699300",
    "end": "1704540"
  },
  {
    "text": "alpha, and alpha, minus\nalpha, minus alpha.",
    "start": "1704540",
    "end": "1711770"
  },
  {
    "text": "So these are the vertices. These are the coordinates of the\nfour points here where we",
    "start": "1711770",
    "end": "1717575"
  },
  {
    "text": "have the standard x, y and\nz-axis, which I'm not drawing because it will just\nlook confusing. So now we'll see that for this\nparticular signal set, we do",
    "start": "1717575",
    "end": "1726320"
  },
  {
    "text": "have a coding gain, OK? Let's find the minimum\ndistance. ",
    "start": "1726320",
    "end": "1734159"
  },
  {
    "text": "The minimum distance\nhere is what? It's the distance along\na diagonal.",
    "start": "1734160",
    "end": "1740010"
  },
  {
    "text": "And the diagonal is of length to\n-- each side of the cube is off-length to alpha.",
    "start": "1740010",
    "end": "1745640"
  },
  {
    "text": " So the diagonal is of\nlength to alpha.",
    "start": "1745640",
    "end": "1753930"
  },
  {
    "text": "So d_min squared is\n8 alpha squared. ",
    "start": "1753930",
    "end": "1759450"
  },
  {
    "text": "What is the energy per bit\ngoing to be in this case? ",
    "start": "1759450",
    "end": "1773029"
  },
  {
    "text": "Well, what's the energy\nper symbol now? AUDIENCE: [INAUDIBLE] PROFESSOR: 3 alpha squared\nover 2 is the Eb, right?",
    "start": "1773030",
    "end": "1780140"
  },
  {
    "text": "We have 3 alpha squared units\nof energy per symbol. We have four points, so we\nhave two bits per symbol.",
    "start": "1780140",
    "end": "1788230"
  },
  {
    "text": "So the energy per bit is\n3 alpha squared over 2.",
    "start": "1788230",
    "end": "1794900"
  },
  {
    "text": "So now if I look at my nominal\ncoding gain, that's going to",
    "start": "1794900",
    "end": "1801470"
  },
  {
    "text": "be d_min squared over 4 Eb.",
    "start": "1801470",
    "end": "1807210"
  },
  {
    "text": "d_min squared is 8\nalpha squared. 4 dB is 6 alpha squared.",
    "start": "1807210",
    "end": "1814170"
  },
  {
    "text": "So that's 4/3. On a linear scale and on a dB\nscale, this is going to be?",
    "start": "1814170",
    "end": "1821779"
  },
  {
    "start": "1821780",
    "end": "1832680"
  },
  {
    "text": "You guys should remember\nyour dB tables.  AUDIENCE: 0.2?",
    "start": "1832680",
    "end": "1839446"
  },
  {
    "text": "PROFESSOR: 12. You have to multiply by ten. So actually I think it's 1.3,\nmore like 1.3 dB if you look",
    "start": "1839446",
    "end": "1846150"
  },
  {
    "text": "at the last significant digit.  OK, so what's K_b of A going\nto be in this case?",
    "start": "1846150",
    "end": "1856760"
  },
  {
    "text": "Or K_b of B I should say. I'm calling the signal\nset B here. So what is K_b if\nB going to be?",
    "start": "1856760",
    "end": "1866140"
  },
  {
    "start": "1866140",
    "end": "1873820"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: 3 by 2, right? You have three nearest neighbors\nper point, for each",
    "start": "1873820",
    "end": "1879760"
  },
  {
    "text": "point has three nearest\nneighbors. They're all equidistant. And you have two bits\nper symbol. OK?",
    "start": "1879760",
    "end": "1885156"
  },
  {
    "text": " So if you work out your\neffective coding again using this rule of thumb here, take\ngamma_c of A in dB and",
    "start": "1885156",
    "end": "1893745"
  },
  {
    "text": "subtract 0.2 log2 you will see\nthat the effective coding gain is approximately 1 dB.",
    "start": "1893745",
    "end": "1899220"
  },
  {
    "start": "1899220",
    "end": "1915730"
  },
  {
    "text": "Are there any questions? ",
    "start": "1915730",
    "end": "1920970"
  },
  {
    "text": "OK, so it might seem that\nthis is still not a very impressive number. Yes, you had a question. AUDIENCE: What was it about this\nconstellation that gave",
    "start": "1920970",
    "end": "1927809"
  },
  {
    "text": "us coding gain? Was is the face that we went\nto higher dimensions? PROFESSOR: Right, so usually\nyou do get a coding gain if",
    "start": "1927810",
    "end": "1935240"
  },
  {
    "text": "you trade-off minimum distance\nwith spectral efficiency.",
    "start": "1935240",
    "end": "1940400"
  },
  {
    "text": "So that's usually the case. So that's why I presented you\nan example of that case.",
    "start": "1940400",
    "end": "1946200"
  },
  {
    "text": "It's just that this was a very\nspecial case where you end up with a 2-PAM constellation\nto start with. ",
    "start": "1946200",
    "end": "1956010"
  },
  {
    "text": "OK, so the comment I was going\nto make is that 1.2 dB still",
    "start": "1956010",
    "end": "1962440"
  },
  {
    "text": "might not be like a very\nimpressive number. I mean after all the hard\nwork, you just get 1 dB effective coding gain.",
    "start": "1962440",
    "end": "1968660"
  },
  {
    "text": "So the question to ask at this\npoint is are there any constellations that have much\nhigher coding gains?",
    "start": "1968660",
    "end": "1974980"
  },
  {
    "text": "In particular, other\nconstellations that come close to the Shannon limit at all.",
    "start": "1974980",
    "end": "1981340"
  },
  {
    "text": "And we'll look at one\ninteresting class of signal sets, which are known as\northogonal signal sets.",
    "start": "1981340",
    "end": "1986640"
  },
  {
    "start": "1986640",
    "end": "1998540"
  },
  {
    "text": "And you probably saw these\nexamples back in 6.450, but we're just reviewing\nit again here.",
    "start": "1998540",
    "end": "2004070"
  },
  {
    "text": "And these signal sets have a\nproperty that as you increase the number of dimensions, you\ncome arbitrarily close to the",
    "start": "2004070",
    "end": "2010960"
  },
  {
    "text": "Shannon limit. So the definition of the signal\nsets is your A --",
    "start": "2010960",
    "end": "2017110"
  },
  {
    "text": "it's a collection of signals aj,\nwhere j goes from 1 to M",
    "start": "2017110",
    "end": "2023450"
  },
  {
    "text": "such that the inner product\nbetween ai and aj is E A times",
    "start": "2023450",
    "end": "2032450"
  },
  {
    "text": "delta_i,j, meaning that if i\nis not equal to j, the two signals are orthogonal. And if i equals j, the\nenergy is E(A).",
    "start": "2032450",
    "end": "2041560"
  },
  {
    "text": "Geometrically, we\nhave two points. The two points can be\nthought of as two",
    "start": "2041560",
    "end": "2047220"
  },
  {
    "text": "points on the two axes. So this is a case\nwhen M equals 2.",
    "start": "2047220",
    "end": "2052649"
  },
  {
    "text": "When M equals 3, the three\npoints will lie on those three corresponding axes. ",
    "start": "2052650",
    "end": "2059649"
  },
  {
    "text": "So this is the case when\nM equals 3 and so on. So generally when you have\nM points, your signal",
    "start": "2059650",
    "end": "2065120"
  },
  {
    "text": "spatializes in M dimensions, and\neach point can be thought of as a point on one\nof the axes.",
    "start": "2065120",
    "end": "2071860"
  },
  {
    "text": "So if you look at the distance\nbetween any two points, it's the norm of ai minus\naj squared.",
    "start": "2071860",
    "end": "2081850"
  },
  {
    "text": "I could simplify this simply as\nthe norm of ai squared plus the norm of aj squared because\nthe end product",
    "start": "2081850",
    "end": "2090050"
  },
  {
    "text": "is going to be zero. OK, now ai squared is\ngoing to be E(A).",
    "start": "2090050",
    "end": "2095759"
  },
  {
    "text": "aj squared is going\nto be E(A). So this is 2E(A).",
    "start": "2095760",
    "end": "2101280"
  },
  {
    "text": "So what we're saying here\nis that every point is equidistant from every other\npoint, and the square of the",
    "start": "2101280",
    "end": "2106880"
  },
  {
    "text": "distance is 2 E(A). OK? ",
    "start": "2106880",
    "end": "2113440"
  },
  {
    "text": "So in other words, the average\nnumber of nearest neighbors is always going to be\na M minus 1.",
    "start": "2113440",
    "end": "2120160"
  },
  {
    "text": " So let us calculate the nominal\ncoding gain for this",
    "start": "2120160",
    "end": "2127700"
  },
  {
    "text": "constellation.  That's going to be d_min\nsquared over 4 Eb.",
    "start": "2127700",
    "end": "2138460"
  },
  {
    "text": " d_min squared is 2E(A)\nover 4 Eb.",
    "start": "2138460",
    "end": "2149160"
  },
  {
    "text": "Well, the energy per bit is the\nenergy per symbol, which is E(A) over the number of bits\nper symbol, which is log",
    "start": "2149160",
    "end": "2156585"
  },
  {
    "text": "M to the base 2. And this I'm going to write\nhere is 1/2 log",
    "start": "2156585",
    "end": "2169520"
  },
  {
    "text": "M to the base 2. So as I increase my M, my\nnumber of dimensions, my",
    "start": "2169520",
    "end": "2175621"
  },
  {
    "text": "nominal coding gain\ngoes to infinity. OK? ",
    "start": "2175622",
    "end": "2181240"
  },
  {
    "text": "What do we expect for the\neffective coding gain? Will it also go to infinity? ",
    "start": "2181240",
    "end": "2190974"
  },
  {
    "text": "AUDIENCE: It approaches\nthe Shannon limit. PROFESSOR: Well, we don't know\nwhether it approaches the Shannon limit as of yet,\nbut we know it",
    "start": "2190974",
    "end": "2196115"
  },
  {
    "text": "cannot go to infinity. It's upper bounded by\nthe Shannon limit. Right? So the effective coding gain\ncannot really become",
    "start": "2196115",
    "end": "2204090"
  },
  {
    "text": "unbounded, and what's really\nhappening here? AUDIENCE: [INAUDIBLE]",
    "start": "2204090",
    "end": "2209470"
  },
  {
    "text": " PROFESSOR: Right, but why does\nthe effective coding gain stay bounded and the nominal\ncoding gain blow up?",
    "start": "2209470",
    "end": "2216610"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: Right, the number\nof nearest neighbors also increases with M, right?",
    "start": "2216610",
    "end": "2222440"
  },
  {
    "text": "Not just the nominal\ncoding gain. ",
    "start": "2222440",
    "end": "2312329"
  },
  {
    "text": "So let's write that down. If I look at the number of\nnearest neighbors per",
    "start": "2312330",
    "end": "2317460"
  },
  {
    "text": "information bit -- yes? AUDIENCE: These don't\nhave 0 mean, right?",
    "start": "2317460",
    "end": "2324480"
  },
  {
    "text": "PROFESSOR: Right, they\ndon't have 0 mean. That's a good point. ",
    "start": "2324480",
    "end": "2331420"
  },
  {
    "text": "PROFESSOR: Let's see, what\nhappens to the mean as M goes to infinity?",
    "start": "2331420",
    "end": "2337420"
  },
  {
    "text": " If we look at what happens to\nthe mean, it will be 1 over M.",
    "start": "2337420",
    "end": "2342470"
  },
  {
    "text": "You find the mean of this guy,\nit's 1 over 2 times the distance here times distance\nhere is 1 over 2 E(A), E(A).",
    "start": "2342470",
    "end": "2349299"
  },
  {
    "text": "OK? If you look at three dimensions,\nit's 1 over 3 E(A), E(A), E(A). So in M dimensions, it's going\nto be 1 over M times all these",
    "start": "2349300",
    "end": "2356560"
  },
  {
    "text": "coordinates. If we look at the mean squared,\nit goes to 0 as M goes to infinity. So that's a good point.",
    "start": "2356560",
    "end": "2364390"
  },
  {
    "text": "So the mean does go\nto 0, and so we approach the Shannon limit. ",
    "start": "2364390",
    "end": "2370410"
  },
  {
    "text": "So we have K_b of A over\nlogM What's the",
    "start": "2370410",
    "end": "2378260"
  },
  {
    "text": "number of nearest neighbors? We saw it was M minus 1 over\nlog M. And that goes to",
    "start": "2378260",
    "end": "2387990"
  },
  {
    "text": "infinity as M goes\nto infinity. OK, so the average number\nof nearest neighbors per",
    "start": "2387990",
    "end": "2395980"
  },
  {
    "text": "information bit also\ngo to infinity as M goes to infinity.",
    "start": "2395980",
    "end": "2402930"
  },
  {
    "text": "And if we look at the mean --\njust to write down the comment that was made.",
    "start": "2402930",
    "end": "2409100"
  },
  {
    "text": "If we look at the mean of this\nconstellation, that's going to be 1 over the number of points\ntimes root E(A) root",
    "start": "2409100",
    "end": "2421960"
  },
  {
    "text": "E(A) and root E(A). So there are M coordinates.",
    "start": "2421960",
    "end": "2428170"
  },
  {
    "text": "So that's going to be the mean\nif you believe this is the coordinate axis here.",
    "start": "2428170",
    "end": "2433300"
  },
  {
    "text": "And if you we look at the norm\nof M(A) squared, then it's",
    "start": "2433300",
    "end": "2439550"
  },
  {
    "text": "going to be E(A) over M. And\nas M goes to infinity,",
    "start": "2439550",
    "end": "2446310"
  },
  {
    "text": "this goes to 0. ",
    "start": "2446310",
    "end": "2454508"
  },
  {
    "text": "And because the mean goes to\n0, it's not unreasonable to expect that the performance does\napproach to the ultimate",
    "start": "2454508",
    "end": "2460825"
  },
  {
    "text": "Shanon limit. OK, so so far we have looked at\nK_b of A and gamma_c of A.",
    "start": "2460825",
    "end": "2469060"
  },
  {
    "text": "If you look at the\neffective coding gain, it is still bounded. ",
    "start": "2469060",
    "end": "2476930"
  },
  {
    "text": "It's always bounded. It is not clear at this point\nwhat really happens to it.",
    "start": "2476930",
    "end": "2482280"
  },
  {
    "text": "And in fact, it's not quite\nstraightforward to actually show what happens to the\neffective coding gain.",
    "start": "2482280",
    "end": "2488440"
  },
  {
    "text": "If you have taken 6.450, then\nProfessor Bob Gallager really goes into the details of proving\nhow the effective",
    "start": "2488440",
    "end": "2495470"
  },
  {
    "text": "coding gain for this signal set\nindeed does approach the Shannon limit.",
    "start": "2495470",
    "end": "2500560"
  },
  {
    "text": "But it can be shown that the\neffective coding gain does",
    "start": "2500560",
    "end": "2510690"
  },
  {
    "text": "approach 11.2 dB as M\ngoes to infinity.",
    "start": "2510690",
    "end": "2516790"
  },
  {
    "text": "The main trick here is that\nunion bound is usually quite weak if the probability\nof error is small. So we replace the probability of\nerror just by 1, instead of",
    "start": "2516790",
    "end": "2524450"
  },
  {
    "text": "the union bound,\nat some point. And if you do that change, then\nthings work out nicely.",
    "start": "2524450",
    "end": "2530300"
  },
  {
    "text": "But in this course, we will\njust assert that the orthogonal signal sets\ndo achieve the",
    "start": "2530300",
    "end": "2536040"
  },
  {
    "text": "ultimate Shannon limit. And you can see 6.450\nnotes for the proof.",
    "start": "2536040",
    "end": "2542180"
  },
  {
    "start": "2542180",
    "end": "2550559"
  },
  {
    "text": "So that's the assertion that\nwe are making right now. So let's step back and\ntake a look at what's",
    "start": "2550560",
    "end": "2556910"
  },
  {
    "text": "happening so far. We have a class of signal sets\nwhich are conceptually quite",
    "start": "2556910",
    "end": "2561990"
  },
  {
    "text": "easy to describe. They are just orthogonal\nsignal sets. They are not too hard\nto analyze, and they",
    "start": "2561990",
    "end": "2567869"
  },
  {
    "text": "asymptotically approach the\nultimate Shannon limit. So this seems quite nice. So why not we just\nimplement them?",
    "start": "2567870",
    "end": "2576010"
  },
  {
    "text": "Now it turns that these are not\nvery common in practice. They are not implemented as\noften as you might think when",
    "start": "2576010",
    "end": "2581849"
  },
  {
    "text": "you first look at\nthem and because they have some drawbacks. And there are a couple of\ndrawbacks which make them very",
    "start": "2581850",
    "end": "2589610"
  },
  {
    "text": "less appealing than what you\nmight otherwise think. ",
    "start": "2589610",
    "end": "2595450"
  },
  {
    "text": "And first of all, what's the\nspectral efficiency for these signal sets? ",
    "start": "2595450",
    "end": "2603316"
  },
  {
    "text": "AUDIENCE:0, right? PROFESSOR: It goes to 0. And how does it go to 0? It's 2log M to the base 2 over\nM. As M goes to infinity,",
    "start": "2603316",
    "end": "2613370"
  },
  {
    "text": "this goes to 0. And in fact, it goes\nto 0 quite sharply.",
    "start": "2613370",
    "end": "2619150"
  },
  {
    "text": "Now that's fine, but if you\nwant to approach Shannon limit, our spectral efficiency\nshould indeed go to 0.",
    "start": "2619150",
    "end": "2625300"
  },
  {
    "text": "But if you're looking at the\nsystem design problem, what do you really want? You have a certain effective\ncoding gain that you have in",
    "start": "2625300",
    "end": "2632330"
  },
  {
    "text": "mind, and suppose you are\npresented with a list of several different signal sets\nthat achieve this effective",
    "start": "2632330",
    "end": "2637960"
  },
  {
    "text": "coding gain. And your job is to pick one of\nthese possible signal sets. Which one will you pick?",
    "start": "2637960",
    "end": "2644320"
  },
  {
    "text": "Well, there are two things\nyou will look for, right? You will look for the spectral\nefficiency that these different signal sets require\nor achieve in order to get",
    "start": "2644320",
    "end": "2652020"
  },
  {
    "text": "this effective coding gain. And the higher the spectral\nefficiency the better. And secondly, you will also\nwant to look at the",
    "start": "2652020",
    "end": "2658660"
  },
  {
    "text": "implementation complexity. You do not want something which\nreally takes a lot of time to decode.",
    "start": "2658660",
    "end": "2665110"
  },
  {
    "text": "OK? So it turns out that because the\nspectral efficiency goes to 0 so sharply, it's not very\nsurprising that there are",
    "start": "2665110",
    "end": "2671180"
  },
  {
    "text": "other codes that have the same\neffective coding gain but higher spectral efficiency.",
    "start": "2671180",
    "end": "2676480"
  },
  {
    "text": "So there are binary codes that\nhave a higher spectral",
    "start": "2676480",
    "end": "2695460"
  },
  {
    "text": "efficiency for same effective\ncoding gain.",
    "start": "2695460",
    "end": "2703050"
  },
  {
    "text": "And so they would be a natural\nchoice over the orthogonal signal set. Yes?",
    "start": "2703050",
    "end": "2708130"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nhigh-spectrum efficiency to mean bandwidth? PROFESSOR: Right, because in\npractice, bandwidth is never really infinite, right?",
    "start": "2708130",
    "end": "2714320"
  },
  {
    "text": "So if you are achieving\nthe same -- the first objective is\nto have a certain effective coding gain.",
    "start": "2714320",
    "end": "2719530"
  },
  {
    "text": "If you do have that, you want to\npick something which has a higher spectral efficiency. And indeed, the subject of\nchapters six and eight is to",
    "start": "2719530",
    "end": "2728120"
  },
  {
    "text": "come up with binary codes that\nhave a much better performance than the orthogonal signal\nset for a given effective coding gain.",
    "start": "2728120",
    "end": "2734430"
  },
  {
    "text": "Chapter six deals with the the\nblock codes, whereas chapter eight deals with convolutional\ncodes.",
    "start": "2734430",
    "end": "2739880"
  },
  {
    "text": "And chapter seven basically\ndevelops all the finite field theory that you require to study\nconvolutional codes.",
    "start": "2739880",
    "end": "2746410"
  },
  {
    "text": "So the point is, in the\nsubsequent lectures we will be focusing a lot on improving\nthe performance over",
    "start": "2746410",
    "end": "2752510"
  },
  {
    "text": "orthogonal signal sets. The second point I mentioned was\nimplementation complexity.",
    "start": "2752510",
    "end": "2757775"
  },
  {
    "start": "2757775",
    "end": "2767599"
  },
  {
    "text": "Now one particular way of\nimplementing orthogonal codes, which you saw in the first\nhomework, was using this idea",
    "start": "2767600",
    "end": "2775099"
  },
  {
    "text": "of Hadamard transforms, right? You have a Hardamard matrix,\nwhich is an M by M matrix. The rows of the Hadamard matrix\nare your orthogonal",
    "start": "2775100",
    "end": "2782260"
  },
  {
    "text": "signal sets, which\nare elements aj. And at the receiver, what you\nwould do is when you get the",
    "start": "2782260",
    "end": "2787950"
  },
  {
    "text": "signal y, you simply multiply\nby the Hadamard matrix and look at the coordinate\nwhich is the largest.",
    "start": "2787950",
    "end": "2794050"
  },
  {
    "text": "So this is something\nyou did in the first homework exercise. And how many computations did\nyou require for an orthogonal",
    "start": "2794050",
    "end": "2800680"
  },
  {
    "text": "signal set of size M? ",
    "start": "2800680",
    "end": "2806010"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] ",
    "start": "2806010",
    "end": "2811420"
  },
  {
    "text": "PROFESSOR: Sorry? OK, so I meant M being\nthe size of the orthogonal signal set. AUDIENCE: Yeah, [INAUDIBLE]",
    "start": "2811420",
    "end": "2819030"
  },
  {
    "text": "PROFESSOR: Right. AUDIENCE: 2 to the\nM by 2 to the M. PROFESSOR: If it is 2 to the M\nby 2 to the M, it is M times 2",
    "start": "2819030",
    "end": "2825330"
  },
  {
    "text": "to the M. But there are M\northogonal signal sets, then it will be M log M, where M is\nthe number of signal sets.",
    "start": "2825330",
    "end": "2837950"
  },
  {
    "text": "So that's the number\nof computations that you would require. Now, the question to ask is, is\nthis fast or is this slow?",
    "start": "2837950",
    "end": "2846640"
  },
  {
    "text": "Is this too many computations\nor is this too little computations? And if you're looking in the\npower-limited regime, what we",
    "start": "2846640",
    "end": "2852790"
  },
  {
    "text": "really want is to see the\ncomplexity per information bit because we normalize things\nper information bit.",
    "start": "2852790",
    "end": "2858810"
  },
  {
    "text": "Now, how many bits are sent\nwhen you have orthogonal signal set of size M? We have log M bits, right?",
    "start": "2858810",
    "end": "2865050"
  },
  {
    "text": "So this quantity is actually\nexponential in the number of transmitted bits that we are\nsending, and so in fact this",
    "start": "2865050",
    "end": "2870360"
  },
  {
    "text": "is quite slow. Or in other words we are saying\nit is we require M times 2 to the M computations,\nwhere M is the number of",
    "start": "2870360",
    "end": "2877640"
  },
  {
    "text": "information bits that\nwe are sending. AUDIENCE: So if M is the size\nof your [INAUDIBLE]",
    "start": "2877640",
    "end": "2884220"
  },
  {
    "text": "then you have log M bits? PROFESSOR: Right. AUDIENCE: So M log M\nover log M is M.",
    "start": "2884220",
    "end": "2889470"
  },
  {
    "text": "PROFESSOR: Why are\nyou adding by M? AUDIENCE: You need M log M\noperations to decode a symbol. PROFESSOR: To decode a symbol.",
    "start": "2889470",
    "end": "2895190"
  },
  {
    "text": "AUDIENCE: And each symbol\ngives you log M bits. So you have M operations\nper bit.",
    "start": "2895190",
    "end": "2901650"
  },
  {
    "text": "PROFESSOR: Well, when you send,\nsay, log M bits, right, you can only send one of\nthe M possible symbols. ",
    "start": "2901650",
    "end": "2909860"
  },
  {
    "text": "And when you want to decode the\nsymbol, you would end up recovering log M bits. ",
    "start": "2909860",
    "end": "2915630"
  },
  {
    "text": "So you should not be\ndividing by M here. AUDIENCE: You're dividing by log\nM because you want to do",
    "start": "2915630",
    "end": "2922030"
  },
  {
    "text": "it per bit. PROFESSOR: All right. You're right. So you're dividing by log M.\nYou have M log M bits.",
    "start": "2922030",
    "end": "2927330"
  },
  {
    "text": "You're dividing by log M,\nso you still have -- AUDIENCE: You have M\ncalculations per bit.",
    "start": "2927330",
    "end": "2935240"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2935240",
    "end": "2942869"
  },
  {
    "text": "PROFESSOR: So we are sending\n-- so I need this many computations to get\nlog M bits.",
    "start": "2942870",
    "end": "2949990"
  },
  {
    "text": " So is this expression\nexponential in log M or not?",
    "start": "2949990",
    "end": "2957359"
  },
  {
    "text": " So that's all I was saying.",
    "start": "2957359",
    "end": "2963309"
  },
  {
    "text": "AUDIENCE: I just put it another\nway that you could divide by log M. PROFESSOR: M for information,\nright.",
    "start": "2963310",
    "end": "2968440"
  },
  {
    "text": " OK, are there any questions?",
    "start": "2968440",
    "end": "2975049"
  },
  {
    "text": "I mean we are both agreeing to\nthe fact that this is too many computations than\nyou would want. And there exist other decoding\nalgorithms for which the",
    "start": "2975050",
    "end": "2983730"
  },
  {
    "text": "number of computations is much\nsmaller than exponential. ",
    "start": "2983730",
    "end": "2994290"
  },
  {
    "text": "OK, so let's next look at the\nbandwidth-limited regime. ",
    "start": "2994290",
    "end": "3012110"
  },
  {
    "text": "Well, actually before that I\nwanted to mention a couple of other signal sets as well.",
    "start": "3012110",
    "end": "3018370"
  },
  {
    "text": "So there are two other important\nclass of signal sets which are related to the\northogonal signal sets.",
    "start": "3018370",
    "end": "3024050"
  },
  {
    "text": "The first is this class of\nsimplex signal sets. ",
    "start": "3024050",
    "end": "3031580"
  },
  {
    "text": "And the other class is\nbi-orthogonal signal sets.",
    "start": "3031580",
    "end": "3041065"
  },
  {
    "start": "3041065",
    "end": "3046960"
  },
  {
    "text": "So the idea behind simplex\nsignal sets goes back to the observation that was made that\nwe have a non-zero mean for",
    "start": "3046960",
    "end": "3053299"
  },
  {
    "text": "this signal set here. So if we do, we can indeed\nsubtract of the mean and get a performance which is\nbetter than the",
    "start": "3053300",
    "end": "3060430"
  },
  {
    "text": "orthogonal signal set. So in particular if I subtract\nof the mean here, I get a",
    "start": "3060430",
    "end": "3065450"
  },
  {
    "text": "signal set, which\nis like this. This is the simplex signal\nset when M equals 2.",
    "start": "3065450",
    "end": "3074090"
  },
  {
    "text": "When M equals 3, I'm going to\nsubtract of the mean from this plane here.",
    "start": "3074090",
    "end": "3079390"
  },
  {
    "text": "The points lie on this\nparticular plane. And what I end up with is\nan equilateral triangle.",
    "start": "3079390",
    "end": "3085570"
  },
  {
    "text": "So this is the case\nwhen M equals 3. What do you think M equals\n4 would look like?",
    "start": "3085570",
    "end": "3092960"
  },
  {
    "text": "A tetrahedron right here. That's a simplex signal set.",
    "start": "3092960",
    "end": "3099930"
  },
  {
    "text": "So basically, it's also not too\nhard to write an algebraic",
    "start": "3099930",
    "end": "3105410"
  },
  {
    "text": "expression for these\nsignal sets. E of A prime is going to be\nE of A minus M of A. You",
    "start": "3105410",
    "end": "3115720"
  },
  {
    "text": "subtract off the mean from your\northogonal signal set. If I want to write it in terms\nof inner products, the inner",
    "start": "3115720",
    "end": "3123460"
  },
  {
    "text": "product between ai and aj is\ngiven by E(A) if i equals j.",
    "start": "3123460",
    "end": "3133450"
  },
  {
    "text": "And it's I believe minus of 1\nover M minus 1 times E(A) if i",
    "start": "3133450",
    "end": "3141609"
  },
  {
    "text": "is not equal to j. So this is the inner product\nbetween ai and aj. And I mean there are also many\nproperties of this simplex",
    "start": "3141610",
    "end": "3148850"
  },
  {
    "text": "signal set, but I believe that\nthat's going to be in your next homework exercise\nso I'm not going",
    "start": "3148850",
    "end": "3153960"
  },
  {
    "text": "into too many details. What' the spectral efficiency\nin this case going to be? ",
    "start": "3153960",
    "end": "3165346"
  },
  {
    "text": "AUDIENCE: I have a question. So what is a simplex? Is a simplex just a reference to\nanything with [INAUDIBLE].",
    "start": "3165346",
    "end": "3172270"
  },
  {
    "text": "PROFESSOR: Simplex signal set is\nderived from the orthogonal signal set by subtracting\noff its mean.",
    "start": "3172270",
    "end": "3177560"
  },
  {
    "text": "AUDIENCE: Right, but in general\nsimplex means anything [INAUDIBLE]? PROFESSOR: I'm not\nsure about that.",
    "start": "3177560",
    "end": "3183170"
  },
  {
    "start": "3183170",
    "end": "3188940"
  },
  {
    "text": "What's the spectral efficiency\ngoing to be? AUDIENCE: [INAUDIBLE] ",
    "start": "3188940",
    "end": "3195840"
  },
  {
    "text": "PROFESSOR: Well, almost. AUDIENCE: You use\none dimension. PROFESSOR: Right, you just\nuse one dimension.",
    "start": "3195840",
    "end": "3201285"
  },
  {
    "start": "3201285",
    "end": "3207230"
  },
  {
    "text": "OK, and there are other\nproperties that you'll be looking at it in the homework.",
    "start": "3207230",
    "end": "3212420"
  },
  {
    "text": "Now the idea behind this -- AUDIENCE: [INAUDIBLE] ",
    "start": "3212420",
    "end": "3219660"
  },
  {
    "text": "PROFESSOR: This is 2 log\nM over [INAUDIBLE]. So the idea behind the\nbi-orthogonal signal set is",
    "start": "3219660",
    "end": "3228510"
  },
  {
    "text": "you start with an orthogonal\nsignal set, and for each point, you also put the\nnegative signal of it.",
    "start": "3228510",
    "end": "3234240"
  },
  {
    "text": "So in addition to this, you will\nhave a point here, and you will have a point here. So the case in two dimensions, M\nequals 2, you will have four",
    "start": "3234240",
    "end": "3243770"
  },
  {
    "text": "points like this.",
    "start": "3243770",
    "end": "3249270"
  },
  {
    "text": "So in this case you are\nincreasing the number of",
    "start": "3249270",
    "end": "3256510"
  },
  {
    "text": "signal points by a\nfactor of two.",
    "start": "3256510",
    "end": "3265830"
  },
  {
    "text": " But this does come at\na price, right?",
    "start": "3265830",
    "end": "3272380"
  },
  {
    "text": "Because if you're increasing the\nnumber of signal points, the number of nearest neighbors\nis also going to",
    "start": "3272380",
    "end": "3277450"
  },
  {
    "text": "increase by a factor of two. In this case, you have two\nnearest neighbors now. ",
    "start": "3277450",
    "end": "3307210"
  },
  {
    "text": "So you have both of the\neffects going on. I mean ultimately, all the\nsignal sets, as M goes to",
    "start": "3307210",
    "end": "3312680"
  },
  {
    "text": "infinity, are going to approach\nthe Shannon limit. So again, but they do suffer\nfrom the same kind of",
    "start": "3312680",
    "end": "3318510"
  },
  {
    "text": "drawbacks that we saw for the\northogonal signal sets. So these are more of theoretical\ninterest as",
    "start": "3318510",
    "end": "3323830"
  },
  {
    "text": "opposed to real, practical\nimplementation points. Any questions?",
    "start": "3323830",
    "end": "3329380"
  },
  {
    "text": " AUDIENCE: Minus [INAUDIBLE]",
    "start": "3329380",
    "end": "3334790"
  },
  {
    "text": " PROFESSOR: It's a bit\ninvolved to show. I think you'll be showing it\nin the homework exercise or",
    "start": "3334790",
    "end": "3342760"
  },
  {
    "text": "for an exam problem\nat one point. You basically start with the\northogonal signal sets here,",
    "start": "3342760",
    "end": "3350710"
  },
  {
    "text": "look at the inner product here,\nand use the relation between the simplex signal sets\nand the orthogonal signal",
    "start": "3350710",
    "end": "3357110"
  },
  {
    "text": "sets to do a subtraction\nof the mean. And then you can derive in\na product expression. ",
    "start": "3357110",
    "end": "3419690"
  },
  {
    "text": "OK, so let's now move on to the\nbandwidth-limited regime. ",
    "start": "3419690",
    "end": "3438547"
  },
  {
    "text": "OK, so the main difference\nbetween -- yes? AUDIENCE: [INAUDIBLE]",
    "start": "3438548",
    "end": "3444196"
  },
  {
    "start": "3444196",
    "end": "3449220"
  },
  {
    "text": "PROFESSOR: You mean here? For each -- AUDIENCE: How far can\nyou [INAUDIBLE]? PROFESSOR: A factor of two.",
    "start": "3449220",
    "end": "3456270"
  },
  {
    "text": "I said a factor right? So if you have M points in the\northogonal signal set, you have 2M points in the\nbi-orthogonal signal set",
    "start": "3456270",
    "end": "3464690"
  },
  {
    "text": "because for each --  AUDIENCE: In the orthogonal\nsets you have 3.",
    "start": "3464690",
    "end": "3472470"
  },
  {
    "text": "PROFESSOR: And in\nthis you have 6. So let's look at the\nbandwidth-limited regime.",
    "start": "3472470",
    "end": "3480290"
  },
  {
    "text": "The trade-off we care about is\nthe probability of error for two dimensions as a function\nof SNR norm.",
    "start": "3480290",
    "end": "3486260"
  },
  {
    "text": " OK? The baseline scheme here\nis your M-PAM scheme.",
    "start": "3486260",
    "end": "3495530"
  },
  {
    "text": "And we showed a couple of\nlectures ago that the probability of error for two\ndimensions is approximately 4",
    "start": "3495530",
    "end": "3502460"
  },
  {
    "text": "Q times root 3 SNR norm. OK? ",
    "start": "3502460",
    "end": "3510420"
  },
  {
    "text": "So let's plot the performance\ngraph. ",
    "start": "3510420",
    "end": "3520576"
  },
  {
    "text": "So on the x-axis I\nplot SNR norm. On the y-axis I plot Ps of E\nagain on a log-log scale.",
    "start": "3520576",
    "end": "3527980"
  },
  {
    "start": "3527980",
    "end": "3533310"
  },
  {
    "text": "This is ten to the negative six,\nten to the negative five, ten to the negative four,\nten to the negative three and so on.",
    "start": "3533310",
    "end": "3539140"
  },
  {
    "text": "The intercept is at 0\ndB, this point here. So this is your Shannon\nlimit in the",
    "start": "3539140",
    "end": "3545420"
  },
  {
    "text": "bandwidth-limited regime. Now your performance curve is\ngoing to be something like",
    "start": "3545420",
    "end": "3552560"
  },
  {
    "text": "this for the M-PAM system. ",
    "start": "3552560",
    "end": "3562710"
  },
  {
    "text": "And we want to basically\nquantify now the effective",
    "start": "3562710",
    "end": "3568480"
  },
  {
    "text": "coding gain in this regime. So it's the same story as in\nthe power-limited regime.",
    "start": "3568480",
    "end": "3573900"
  },
  {
    "text": "We will start off with the\nprobability of error using the",
    "start": "3573900",
    "end": "3579609"
  },
  {
    "text": "union bound estimate, which is\nK_min of A times Q of d_min",
    "start": "3579610",
    "end": "3591000"
  },
  {
    "text": "over 2 sigma.  Now Ps of E is the probability\nof error for two dimensions.",
    "start": "3591000",
    "end": "3600460"
  },
  {
    "text": "Assuming we have N dimensions\nhere, it is two times the probability of error over N.\nSo this is probability of",
    "start": "3600460",
    "end": "3610400"
  },
  {
    "text": "error per symbol. We are dividing by the number of\ndimensions and multiplying by two because it is\nfor two dimensions.",
    "start": "3610400",
    "end": "3618770"
  },
  {
    "text": "And this is going to be 2 times\nK_min of A over N, times",
    "start": "3618770",
    "end": "3628980"
  },
  {
    "text": "Q of d_min over 2 sigma. ",
    "start": "3628980",
    "end": "3639530"
  },
  {
    "text": "So now let's do the same trick\nthat we did in the power-limited regime. ",
    "start": "3639530",
    "end": "3646109"
  },
  {
    "text": "We will write Ps of E\nbe approximately -- and instead of the right hand\nside, I will write it as Ks of",
    "start": "3646110",
    "end": "3653980"
  },
  {
    "text": "A times Q of root 3 SNR norm.",
    "start": "3653980",
    "end": "3661950"
  },
  {
    "text": " But because I have a factor of\nfour up there, I will also",
    "start": "3661950",
    "end": "3668900"
  },
  {
    "text": "multiply and divide by 4. That's not going to\nchange anything.",
    "start": "3668900",
    "end": "3676024"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] per two dimensions\nor by symbol. PROFESSOR: Per two dimensions. And for bandwidth-limited\nregime, we normalize",
    "start": "3676024",
    "end": "3683510"
  },
  {
    "text": "everything by two dimensions. AUDIENCE: But for N band we used\nit for symbols, right? The calculations represent --",
    "start": "3683510",
    "end": "3690790"
  },
  {
    "text": "PROFESSOR: No, it was\nfor two dimensions. ",
    "start": "3690790",
    "end": "3697720"
  },
  {
    "text": "OK?, so now let's compare\nthe two expressions. We have Ks of A, which we'll\ndefine by two times K_min of A",
    "start": "3697720",
    "end": "3710960"
  },
  {
    "text": "over N. And this is the number\nof nearest neighbors per two",
    "start": "3710960",
    "end": "3725810"
  },
  {
    "text": "dimensions. And I can write 3 times SNR norm\nis d_min squared over 4",
    "start": "3725810",
    "end": "3739780"
  },
  {
    "text": "sigma squared, OK?",
    "start": "3739780",
    "end": "3747970"
  },
  {
    "text": "Actually, I was missing\nthis factor of gamma. I knew I was missing\nsomething. It's not just 3 SNR norm.",
    "start": "3747970",
    "end": "3754970"
  },
  {
    "text": "It's 3 times this nominal coding\ngain times SNR norm. ",
    "start": "3754970",
    "end": "3762080"
  },
  {
    "text": "So 3 times SNR norm times\ngamma_c of A here.",
    "start": "3762080",
    "end": "3767550"
  },
  {
    "text": "So this is d_min squared\nover 2N_0. ",
    "start": "3767550",
    "end": "3774850"
  },
  {
    "text": "So what do I have for the\ngamma_c c o A is d_min squared",
    "start": "3774850",
    "end": "3782470"
  },
  {
    "text": "over 6 N_0 times SNR norm. ",
    "start": "3782470",
    "end": "3788740"
  },
  {
    "text": "Remember SNR norm is SNR over\n2 to the rho minus 1. It's normalized signal-to-noise\nratio.",
    "start": "3788740",
    "end": "3795730"
  },
  {
    "text": "So this is the d_min squared\ntimes 2 to the rho minus 1",
    "start": "3795730",
    "end": "3800890"
  },
  {
    "text": "over 6 times N_0 times SNR. But N_0 times SNR is just Es.",
    "start": "3800890",
    "end": "3807390"
  },
  {
    "text": "So this is 6 times Es. So this is the expression\nyou have for the nominal coding gain.",
    "start": "3807390",
    "end": "3812990"
  },
  {
    "start": "3812990",
    "end": "3827760"
  },
  {
    "text": "So now again, given a signal\nset A, I can find those two parameters, the nominal coding\ngain and the number of nearest",
    "start": "3827760",
    "end": "3839369"
  },
  {
    "text": "neighbors per two dimensions. And I can use those to\nplot on the Ps of E",
    "start": "3839370",
    "end": "3845686"
  },
  {
    "text": "versus SNR norm curve. Again, the exact same story. ",
    "start": "3845686",
    "end": "3852940"
  },
  {
    "text": "If I have a certain nominal\ncoding gain, I will simply shift my curve around the x-axis\nby that factor as a",
    "start": "3852940",
    "end": "3863460"
  },
  {
    "text": "pure translation. And then because I have a factor\nof Ks A over 4, I'm",
    "start": "3863460",
    "end": "3872120"
  },
  {
    "text": "going to plot -- how did that expression go? This expression on the top.",
    "start": "3872120",
    "end": "3877590"
  },
  {
    "text": "Remember this curve here is 4\ntimes root 3 SNR norm, right? So the multiplicative factor\nI have is Ks(A) over 4.",
    "start": "3877590",
    "end": "3884740"
  },
  {
    "text": "So I will shift my curve\nup by that factor. ",
    "start": "3884740",
    "end": "3890180"
  },
  {
    "text": "And I will get something\nwhich is like this. ",
    "start": "3890180",
    "end": "3897110"
  },
  {
    "text": "And that's my union bound\nestimate for this new constellation A.",
    "start": "3897110",
    "end": "3904470"
  },
  {
    "text": "The distance here is\ngamma effective.  This distance here is gamma_c of\nA. Are there any questions?",
    "start": "3904470",
    "end": "3919400"
  },
  {
    "text": "So now we also have a similar\nrule of thumb as in the power-limited regime.",
    "start": "3919400",
    "end": "3924450"
  },
  {
    "text": "I will write that rule\nof thumb here. We have that gamma effective is\ngoing to be gamma_c in dB",
    "start": "3924450",
    "end": "3934430"
  },
  {
    "text": "minus 0.2 times log2 times\nKs of A over 4 in dB.",
    "start": "3934430",
    "end": "3943020"
  },
  {
    "text": " OK, so again, a factor of 2NKs\nwill decrease your nominal",
    "start": "3943020",
    "end": "3951400"
  },
  {
    "text": "coding gain by a factor\nof 0.2 in dB. ",
    "start": "3951400",
    "end": "3958620"
  },
  {
    "text": "Are there any questions?  Well, so I mean this is the\nend of chapter five.",
    "start": "3958620",
    "end": "3966260"
  },
  {
    "text": "We still have like ten minutes\nremaining, so I thought I would give you a preview\nof the next chapter.",
    "start": "3966260",
    "end": "3971480"
  },
  {
    "text": "Professor Forney will be coming\nnext class, and he will be starting chapter six\nall over I believe. ",
    "start": "3971480",
    "end": "3979030"
  },
  {
    "text": "This chalk is much nicer to\nerase than that other one. ",
    "start": "3979030",
    "end": "4003000"
  },
  {
    "text": "So in chapter six, we'll\nbe looking at the",
    "start": "4003000",
    "end": "4008650"
  },
  {
    "text": "binary block codes. ",
    "start": "4008650",
    "end": "4020130"
  },
  {
    "text": "OK, and what is the main\narchitecture of these codes? Well, for an encoder, remember\nwe have K bits coming in.",
    "start": "4020130",
    "end": "4029700"
  },
  {
    "text": "And we pass it now through\na binary block code. ",
    "start": "4029700",
    "end": "4036069"
  },
  {
    "text": "And what we get out is\na binary sequence",
    "start": "4036070",
    "end": "4043220"
  },
  {
    "text": "of length N. OK? So x belongs to c, where c is\na subset of binary sequences",
    "start": "4043220",
    "end": "4051750"
  },
  {
    "text": "of length N. So we start with K bits, and\nwe convert them to N bits,",
    "start": "4051750",
    "end": "4057240"
  },
  {
    "text": "where N is going to be greater\nthan or equal to K. Once we",
    "start": "4057240",
    "end": "4062610"
  },
  {
    "text": "have this sequence of N bits,\nwhat we end up doing is we pass it through a binary\nsignal constellation.",
    "start": "4062610",
    "end": "4069820"
  },
  {
    "text": " So I'm writing in as\nbinary signaling.",
    "start": "4069820",
    "end": "4076690"
  },
  {
    "text": "And what we get out is\nS of x, which is a",
    "start": "4076690",
    "end": "4082200"
  },
  {
    "text": "sequence of N symbols. Each symbol can either take\nvalue minus alpha or alpha. So this binary signalling is\nactually quite a trivial step.",
    "start": "4082200",
    "end": "4092130"
  },
  {
    "text": "Basically, the relation is S of\n0 is going to be alpha, S of 1 is going to\nbe minus alpha.",
    "start": "4092130",
    "end": "4098500"
  },
  {
    "text": "If I have a 0 coming in, I\nwill produce an alpha. if I have a 1 coming in, I will\nproduce a minus alpha.",
    "start": "4098500",
    "end": "4104520"
  },
  {
    "text": "So this part here is trivial. All our energy will be\nfocused on will be to find a good code.",
    "start": "4104520",
    "end": "4111339"
  },
  {
    "text": "Given a sequence of input bits,\nwe want to find a good binary code in order to\noptimize the minimum",
    "start": "4111340",
    "end": "4119080"
  },
  {
    "text": "distance and so on. So that's the architecture that\nwe will be using for the binary linear codes.",
    "start": "4119080",
    "end": "4125210"
  },
  {
    "text": "Both chapters six and\neight will be only focusing on this part. And this binary signalling\nscheme, for the most part,",
    "start": "4125210",
    "end": "4130549"
  },
  {
    "text": "will be implicit in\nour architecture.  So at this point, one might ask\nis there anything to lose",
    "start": "4130550",
    "end": "4138399"
  },
  {
    "text": "in having imposed this\ntype of architecture? Remember, the nice thing about\nthis architecture is we have",
    "start": "4138399",
    "end": "4143899"
  },
  {
    "text": "coding, which is going on here,\nand we have modulation that is happening here. And the modulation step\nis quite simple.",
    "start": "4143899",
    "end": "4150960"
  },
  {
    "text": "So we have a separation between\ncoding and modulation.",
    "start": "4150960",
    "end": "4163155"
  },
  {
    "text": "OK? ",
    "start": "4163156",
    "end": "4169689"
  },
  {
    "text": "And the question is, is this\nthe right thing to do? Now, it turns out that if we\nare going to live in this binary world where we are only\nconstrained ourselves to",
    "start": "4169689",
    "end": "4176984"
  },
  {
    "text": "sending binary signals over the\nchannel, this is in fact an economical structure. There isn't much improvement we\ncan get, and that is quite",
    "start": "4176984",
    "end": "4184309"
  },
  {
    "text": "obvious, right? But it turns out that if you are\ngoing to go for non-binary signaling, particularly in the\nbandwidth-limited regime,",
    "start": "4184310",
    "end": "4191870"
  },
  {
    "text": "there is a cost to be paid for\nthis kind of separation. In fact, in the 1980's there\nwas this whole idea of",
    "start": "4191870",
    "end": "4197950"
  },
  {
    "text": "turbo-coded modulation or\ntrellis-coded modulation. And the idea there was to\ncombine coding and modulation",
    "start": "4197950",
    "end": "4203680"
  },
  {
    "text": "in one step. So this is not optimal for\nnon-binary signalling, but's",
    "start": "4203680",
    "end": "4219440"
  },
  {
    "text": "it's OK for binary.",
    "start": "4219440",
    "end": "4225270"
  },
  {
    "start": "4225270",
    "end": "4230570"
  },
  {
    "text": "So we will be focusing on this\ntype of an architecture. The other issue is OK, nobody\ntold us that you can only send",
    "start": "4230570",
    "end": "4237130"
  },
  {
    "text": "binary signals over\nthe channel. If you want to achieve the\nShannon limit, when we derived",
    "start": "4237130",
    "end": "4242270"
  },
  {
    "text": "the capacity expression -- or we just stated it but\nyou can derive it. We said that the Shannon\nlimit is always less",
    "start": "4242270",
    "end": "4249660"
  },
  {
    "text": "than log2 1 plus SNR. And we never said that we can\nonly send binary signals over",
    "start": "4249660",
    "end": "4255980"
  },
  {
    "text": "the channel. So it could be that this is some\ninherence of optimality in this type of architecture. Why send binary signals\nin the first place?",
    "start": "4255980",
    "end": "4263730"
  },
  {
    "text": "Again, it turns out that the\nregime that we are interested in here is the power-limited\nregime because the spectral",
    "start": "4263730",
    "end": "4270130"
  },
  {
    "text": "efficiency can never be more\nthan two bits per two dimensions, right? Remember, rho is 2K over N here,\nand K is going to be",
    "start": "4270130",
    "end": "4276340"
  },
  {
    "text": "less than that. So we are inherently in the\npower-limited regime.",
    "start": "4276340",
    "end": "4282130"
  },
  {
    "text": "And so a natural thing to do\nin order to understand how much fundamental loss we have\nin imposing this type of",
    "start": "4282130",
    "end": "4288120"
  },
  {
    "text": "binary constraint over the\nsignals is to not look at this expression, but to find the best\npossible performance we",
    "start": "4288120",
    "end": "4296469"
  },
  {
    "text": "can have if we constrain\nourselves to binary signals over the channel. If we impose a certain signaling\nconstraint, we can",
    "start": "4296470",
    "end": "4303909"
  },
  {
    "text": "find another fundamental limit\non the spectral efficiency.",
    "start": "4303910",
    "end": "4309330"
  },
  {
    "text": "So the point being, the best\npossible binary code can only achieve this upper bound here.",
    "start": "4309330",
    "end": "4315449"
  },
  {
    "text": "Now, this upper bound\nhas to be less than log2 1 plus SNR, right? ",
    "start": "4315450",
    "end": "4323170"
  },
  {
    "text": "Because the Shannon code assumes\nthe binary signaling is a special case. So it turns out that this\nexpression is actually quite",
    "start": "4323170",
    "end": "4330700"
  },
  {
    "text": "tedious to write out. I'm not even sure if the\nclosed-form expression exists, but you can calculate\nthis numerically.",
    "start": "4330700",
    "end": "4336820"
  },
  {
    "text": "It's just some kind of a convex\noptimization problem. And now, to understand how\nmuch loss we have, we can",
    "start": "4336820",
    "end": "4342349"
  },
  {
    "text": "compare the two expressions in\nthe power-limited regime. So I'm going to plot the curves\nfor the two cases.",
    "start": "4342350",
    "end": "4350690"
  },
  {
    "text": "I'm going to plot the spectral\nefficiency on the y-axis as a function of Eb N_0.",
    "start": "4350690",
    "end": "4356500"
  },
  {
    "text": "You can easily convert from\nSNR to Eb N_0 using the relations we discussed\nin chapter four.",
    "start": "4356500",
    "end": "4362210"
  },
  {
    "text": "Now we first plot the\nShannon limit. The ultimate Shannon limit\nis minus 1.59 dB.",
    "start": "4362210",
    "end": "4368920"
  },
  {
    "text": "This is going to be 0 dB here. And if I'm plot the Shannon\nlimit, it will be",
    "start": "4368920",
    "end": "4374100"
  },
  {
    "text": "some code like this. It increases with Eb N_0. And minus 1.59 dB is 0.",
    "start": "4374100",
    "end": "4381210"
  },
  {
    "text": "This point here is 1. Some point here is 2. So actually I want to draw this\nhorizontal asymptote at",
    "start": "4381210",
    "end": "4389199"
  },
  {
    "text": "two when rho Shannon equals 2. So this is my rho Shannon.",
    "start": "4389200",
    "end": "4395719"
  },
  {
    "text": "Now, I can now also plot\nrho binary from this expression here.",
    "start": "4395720",
    "end": "4401020"
  },
  {
    "text": "I can never exceed two bits\nper two dimensions. So I had plotted this\nhorizontal line. So as Eb N_0 goes to infinity,\nthe most I can get is two bits",
    "start": "4401020",
    "end": "4408720"
  },
  {
    "text": "per two dimensions. If Eb N_0 is smaller, I cannot\ndo better so I will be doing",
    "start": "4408720",
    "end": "4415380"
  },
  {
    "text": "worse and worse, and I will\nalways be below this line. Ultimately, it can be shown\nthat I will have the same",
    "start": "4415380",
    "end": "4421330"
  },
  {
    "text": "Shannon limit here. So this is rho binary. ",
    "start": "4421330",
    "end": "4428380"
  },
  {
    "text": "And the main observation here is\nif I'm in the power-limited regime, which is say, in this\npart of the curve --",
    "start": "4428380",
    "end": "4435150"
  },
  {
    "text": "like, say around one bit per two\ndimension -- the gap here is quite small. In fact, this gap can be shown\nto be at most 0.2 dB.",
    "start": "4435150",
    "end": "4442620"
  },
  {
    "text": " So if I want to achieve a\ncertain spectral efficiency,",
    "start": "4442620",
    "end": "4448130"
  },
  {
    "text": "if I impose a binary signaling\nconstraint, the additional Eb N_0 that I require when the\nspectral efficiency is 1 dB,",
    "start": "4448130",
    "end": "4455300"
  },
  {
    "text": "one bit per two dimensions,\nis at most 0.2 dB. So there is not much to lose by\nimposing a binary signaling",
    "start": "4455300",
    "end": "4462280"
  },
  {
    "text": "constraint. Said in different words words\nif you had the best possible binary linear code, followed\nby a binary signaling",
    "start": "4462280",
    "end": "4469150"
  },
  {
    "text": "constellation, the most you\ncan lose is 0.2 dB. And so this type of an\narchitecture does make sense.",
    "start": "4469150",
    "end": "4476070"
  },
  {
    "text": "There are a lot of details that\nthe next powerpoint will be to kind of formalize the\nnotion of a binary block code,",
    "start": "4476070",
    "end": "4484680"
  },
  {
    "text": "then specialize it to the\ncase of binary linear codes and so on. Unfortunately, the first thing\nto do is to start with some",
    "start": "4484680",
    "end": "4491370"
  },
  {
    "text": "finite field theory because\nthere's a lot of finite field algebra involved in this\nalgebraic block codes.",
    "start": "4491370",
    "end": "4497780"
  },
  {
    "text": "So how many of you have seen\nfinite field theory before? OK, if you haven't seen it,\ndon't worry about it.",
    "start": "4497780",
    "end": "4505810"
  },
  {
    "text": "We'll be starting right\nfrom the basics. But I think I will be stopping\nhere for today because I don't",
    "start": "4505810",
    "end": "4511579"
  },
  {
    "text": "want to go into those\ndetails today. ",
    "start": "4511580",
    "end": "4536017"
  }
]