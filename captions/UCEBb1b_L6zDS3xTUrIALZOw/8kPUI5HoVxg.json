[
  {
    "start": "0",
    "end": "138000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5300"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high-quality\neducational resources for free.",
    "start": "5300",
    "end": "11600"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11600",
    "end": "18100"
  },
  {
    "text": "at ocw.mit.edu. ",
    "start": "18100",
    "end": "24660"
  },
  {
    "text": "JAMES SWAN: OK. Should we begin?",
    "start": "24660",
    "end": "30280"
  },
  {
    "text": "Let me remind you,\nwe switched topics. We transitioned\nfrom linear algebra, solving systems of\nlinear equations,",
    "start": "30280",
    "end": "36880"
  },
  {
    "text": "to solving systems of\nnonlinear equations. And it turns out, linear algebra\nis at the core of the way",
    "start": "36880",
    "end": "43630"
  },
  {
    "text": "that we're going to\nsolve these equations. We need iterative approaches. These problems are complicated. We don't know how many\nsolutions there could be.",
    "start": "43630",
    "end": "50678"
  },
  {
    "text": "We have no idea where those\nsolutions could be located. We have no exact\nways of finding them. We use iterative methods to\ntransform non-linear equations",
    "start": "50679",
    "end": "59140"
  },
  {
    "text": "into simpler problems, right? Iterates of systems\nof linear equations.",
    "start": "59140",
    "end": "64849"
  },
  {
    "text": "And the key to that was\nthe Newton-Raphson method. So I'm going to pick\nup where we left off",
    "start": "64849",
    "end": "71000"
  },
  {
    "text": "with the Newton-Raphson\nmethod, and we're going find out ways of being\nless Newton-Raphson-y in order",
    "start": "71000",
    "end": "77420"
  },
  {
    "text": "to overcome some difficulties\nwith the method, shortcomings of the method. There are a number\nof them that have",
    "start": "77420",
    "end": "83823"
  },
  {
    "text": "to be overcome in various ways. And you sort of choose these\nso-called quasi- Newton-Raphson methods as you need them.",
    "start": "83823",
    "end": "90670"
  },
  {
    "text": "OK, so you'll find out. You try to solve a problem. And the Newton-Raphson method\npresents some difficulty,",
    "start": "90670",
    "end": "96799"
  },
  {
    "text": "you might resort to a quasi\nNewton-Raphson method instead. Built into MATLAB is\nnon-linear equations solver.",
    "start": "96800",
    "end": "104060"
  },
  {
    "text": "fsolve. OK, it's going to\nhappily solve systems of nonlinear equations\nfor you, and it's going to use this\nmethodology to do it.",
    "start": "104060",
    "end": "110520"
  },
  {
    "text": "It's going to use\nvarious aspects of these quasi- Newton-Raphson\nmethods to do it. I'll sort of point\nout places where",
    "start": "110520",
    "end": "116150"
  },
  {
    "text": "fsolve will take\nfrom our lecture and implement them for you. It will even use some more\ncomplicated methods that",
    "start": "116150",
    "end": "121684"
  },
  {
    "text": "we'll talk about later on in\nthe context of optimization . Somebody asked an\ninteresting question,",
    "start": "121684",
    "end": "128209"
  },
  {
    "text": "which is how many of\nthese nonlinear equations am I going to want\nto solve at once? Right?",
    "start": "128210",
    "end": "133280"
  },
  {
    "text": "Like I have a system\nof these equations. What does a big system of\nnonlinear equations look like?",
    "start": "133280",
    "end": "138709"
  },
  {
    "start": "138000",
    "end": "266000"
  },
  {
    "text": "And just like with\nlinear equations, it's as big as you can imagine. So one case you could think\nabout is trying to solve,",
    "start": "138710",
    "end": "146390"
  },
  {
    "text": "for example, the steady\nNavia-Stokes equations. That's a nonlinear\npartial differential",
    "start": "146390",
    "end": "151489"
  },
  {
    "text": "equation for the velocity field\nand the pressure in a fluid. And a at Reynolds number,\nthat non-linearity",
    "start": "151490",
    "end": "158480"
  },
  {
    "text": "is going to present itself\nin terms of inertial terms that may even dominate the flow\ncharacteristics in many places.",
    "start": "158480",
    "end": "166190"
  },
  {
    "text": "We'll learn ways of discretizing\npartial differential equations like that. And so then, at each point in\nthe fluid we're interested in,",
    "start": "166190",
    "end": "173424"
  },
  {
    "text": "we're going to have a non-linear\nequation that we have to solve. So there's going to be a system\nof these non-linear equations",
    "start": "173424",
    "end": "178631"
  },
  {
    "text": "that are coupled together. How many points are\nthere going to be? That's up to you, OK? And so you're going to\nneed methods like this",
    "start": "178631",
    "end": "185030"
  },
  {
    "text": "to solve that. It sounds very complicated. So a lot of times\nin fluid mechanics, we have better ways of\ngoing about doing it.",
    "start": "185030",
    "end": "190470"
  },
  {
    "text": "But in principle, we've have any\nnumber of nonlinear equations that we want to solve. ",
    "start": "190470",
    "end": "198791"
  },
  {
    "text": "We discussed last time, the\nnew Newton-Raphson method, which was based around\nthe idea of linearization. We have these\nnonlinear equations.",
    "start": "198791",
    "end": "205732"
  },
  {
    "text": "We don't know what\nto do with them. So let's linearize them, right? If we have some guess for the\nsolution, which isn't perfect,",
    "start": "205732",
    "end": "212260"
  },
  {
    "text": "but it's our best\npossible guess. Let's look at the function\nand find a linearized form of the function and see where\nthat linearized form has",
    "start": "212260",
    "end": "219250"
  },
  {
    "text": "an intercept. And we just have an Ansatz. We guess that this\nis a better solution",
    "start": "219250",
    "end": "225160"
  },
  {
    "text": "than the one we had before. And we iterate. It turns out you can\nprove that this sort",
    "start": "225160",
    "end": "231520"
  },
  {
    "text": "of a strategy-- this\nNewton-Raphson strategy is locally convergent. If I start with a guess\nsufficiently close to the root,",
    "start": "231520",
    "end": "239710"
  },
  {
    "text": "you can prove mathematically\nthat this procedure will terminate with a\nsolution at the root, right?",
    "start": "239710",
    "end": "245980"
  },
  {
    "text": "It's going to approach after\nan infinite number of iterates, the root. That's wonderful. It's locally convergent,\nnot globally convergent.",
    "start": "245980",
    "end": "253200"
  },
  {
    "text": "So this is one of those\nproblems that we discussed. Take a second here, right?",
    "start": "253200",
    "end": "258519"
  },
  {
    "text": "Here's your\nNewton-Raphson formula. You've got it on your slides. Take a second here and--",
    "start": "258519",
    "end": "263664"
  },
  {
    "text": "this is sort of interesting. Derive the Babylonian\nmethod, right? Turns out the Babylonians\ndidn't know anything about Newton-Raphson but they\nhad some good guesses for how",
    "start": "263664",
    "end": "270608"
  },
  {
    "start": "266000",
    "end": "630000"
  },
  {
    "text": "to find square roots, right? Find the roots of an\nequation like this. See that you understand the\nnew Newton-Raphson method",
    "start": "270608",
    "end": "276370"
  },
  {
    "text": "by deriving the\nBabylonian method, right? The iterative method for\nfinding the square root of s",
    "start": "276370",
    "end": "281979"
  },
  {
    "text": "as the root of this equation. Can you do it? [SIDE CONVERSATION] ",
    "start": "281980",
    "end": "366880"
  },
  {
    "text": "JAMES SWAN: Yes, you know\nhow to do this, right? So calculate the derivative. The derivative is 2x.",
    "start": "366880",
    "end": "372460"
  },
  {
    "text": "Here's our formula for\nthe iterative method. Right? So it's f of x\nover f prime of x.",
    "start": "372460",
    "end": "378939"
  },
  {
    "text": "That sets the\nmagnitude of the stop. The direction is\nminus this magnitude. It's in one d, so we either\ngo left or we go right.",
    "start": "378940",
    "end": "386470"
  },
  {
    "text": "Minus sets the direction. We add that to our\nprevious guess. And we have our\nnew iterate, right?",
    "start": "386470",
    "end": "391560"
  },
  {
    "text": "You substitute f and f prime,\nand you can simplify this down to the Babylonian\nmethod, which said take",
    "start": "391560",
    "end": "398110"
  },
  {
    "text": "the average of x and s over x. If I'm at the\nroot, both of these should be square root of s.",
    "start": "398110",
    "end": "405510"
  },
  {
    "text": "And this quantity should\nbe zero exactly, right? And you'll get your solution.",
    "start": "405510",
    "end": "410550"
  },
  {
    "text": "So that's the Babylonian\nmethod, right? It's just an extension of\nthe Newton-Raphson method.",
    "start": "410550",
    "end": "415580"
  },
  {
    "text": "It was pretty good\nback in the day, right? Quadratic convergence to\nthe square root of a number. I mentioned early on\ncomputers got really good",
    "start": "415580",
    "end": "423220"
  },
  {
    "text": "in computing square\nroots at one point, because somebody did\nsomething kind of magic.",
    "start": "423220",
    "end": "429380"
  },
  {
    "text": "They came up with a scheme for\ngetting good initial guesses for the square root. This iterative method has to\nstart with some initial guess.",
    "start": "429380",
    "end": "435949"
  },
  {
    "text": "If it starts far away,\nit'll take more iterations to get there. It'll get there, but it's\ngoing to take more iterations",
    "start": "435950",
    "end": "441620"
  },
  {
    "text": "to get there. That's undesirable if you're\ntrying to do fast calculations. So somebody came up\nwith some magic scheme,",
    "start": "441620",
    "end": "446840"
  },
  {
    "text": "using floating point\nmathematics, right? They masked some of the bits\nin the digits of these numbers.",
    "start": "446840",
    "end": "452719"
  },
  {
    "text": "A special number\nto mask those bits. They found that using\noptimization, it turns out.",
    "start": "452720",
    "end": "457880"
  },
  {
    "text": "And they got really\ngood initial guesses, and then it would\ntake one or two iterations with the\nNewton-Raphson method",
    "start": "457880",
    "end": "463039"
  },
  {
    "text": "to get 16 digits of accuracy. That's pretty good. But good initial\nguesses are important.",
    "start": "463040",
    "end": "469620"
  },
  {
    "text": "We'll talk about that\nnext week on Wednesday. Where do those good\ninitial guesses come from? But sometimes we don't\nhave those available to us.",
    "start": "469620",
    "end": "476052"
  },
  {
    "text": "So what are some other\nways that we can improve the Newton-Raphson method? That will be the topic\nof today's lecture.",
    "start": "476052",
    "end": "482701"
  },
  {
    "text": "What's the Newton-Raphson\nmethod look like graphically in many dimensions. We talked about this Jacobian.",
    "start": "482701",
    "end": "488130"
  },
  {
    "text": "Right, when we're\ntrying to find the roots of a non-linear equation\nwhere our function has",
    "start": "488130",
    "end": "493890"
  },
  {
    "text": "more than one dimension-- let's\nsay it has two dimensions. So we have an f 1 and an f 2. And our unknowns\nour x 1 and x 2,",
    "start": "493890",
    "end": "501270"
  },
  {
    "text": "they live in the\nx1 x2 plane, right? f1 might be this say\nbowl-shaped function.",
    "start": "501270",
    "end": "507017"
  },
  {
    "text": "I've sketched out in red, right? It's three dimensional. It's some surface here. Right? We have some initial\nguess for the solution.",
    "start": "507017",
    "end": "514349"
  },
  {
    "text": "We go up to the function, and\nwe find a linearization of it, which is not a line but a plane.",
    "start": "514350",
    "end": "520200"
  },
  {
    "text": "And that plane intersects\nthe x 1, x 2 plane at a line. And our next best guess is going\nto live somewhere on this line.",
    "start": "520200",
    "end": "528210"
  },
  {
    "text": "Where on this line depends\non the linearization of f 2. Right? So we got to draw the\nsame picture for f 2,",
    "start": "528210",
    "end": "533980"
  },
  {
    "text": "but I'm not going\nto do that for you. So let's say, this is where\nthe equivalent line from f 2",
    "start": "533980",
    "end": "539400"
  },
  {
    "text": "intersects the line\nfrom f 1, right? So the two linearizations\nintersect here.",
    "start": "539400",
    "end": "544650"
  },
  {
    "text": "That's our next best guess. We go back up to the curve. We find the plane that's\ntangent to the curve.",
    "start": "544650",
    "end": "550680"
  },
  {
    "text": "We figure out where\nit intersects. The x 1 x 2 plane. That's a line. We find the point on the line\nthat's our next best guess,",
    "start": "550680",
    "end": "557279"
  },
  {
    "text": "and continue. Finding that\nintersection in the plane is the act of computing\nJacobian inverse times f.",
    "start": "557280",
    "end": "564938"
  },
  {
    "text": "OK? ",
    "start": "564938",
    "end": "570380"
  },
  {
    "text": "If we project down to\njust the x 1 x 2 plane, and we draw the curves where f\n1 equals 0, and f 2 equals zero,",
    "start": "570380",
    "end": "579750"
  },
  {
    "text": "right? Then each of these iterates,\nwe start with an initial guess. We find the planes that are\ntangent to these curves,",
    "start": "579750",
    "end": "586550"
  },
  {
    "text": "or to these surfaces. And where they intersect\nthe x 1 x 2 plane. Those give us these lines.",
    "start": "586550",
    "end": "591745"
  },
  {
    "text": "And the intersection\nof the lines give us our next approximation. And so our function steps\nalong in the x1 and x2 plane.",
    "start": "591745",
    "end": "598160"
  },
  {
    "text": "It takes some path\nthrough that plane. And eventually it will approach\nthis locally unique solution.",
    "start": "598160",
    "end": "603692"
  },
  {
    "text": "So that's what this iterative\nmethod is doing, right? It's navigating this\nmultidimensional space, right?",
    "start": "603692",
    "end": "608750"
  },
  {
    "text": "It moves where it\nhas to to satisfy these linearized\nequations, right?",
    "start": "608750",
    "end": "614930"
  },
  {
    "text": "Producing ever better\napproximations for a root. Start close. It'll converge fast.",
    "start": "614930",
    "end": "621230"
  },
  {
    "text": "How fast? Quadratically. And you can prove this. I'll prove it in 1D.",
    "start": "621230",
    "end": "626620"
  },
  {
    "text": "You might think about the\nmultidimensional case, but I'll show you\nin one dimension. So the Newton-Raphson\nmethod said,",
    "start": "626620",
    "end": "632360"
  },
  {
    "start": "630000",
    "end": "841000"
  },
  {
    "text": "xi plus 1 is equal to xi minus\nf of xi over f prime of xi.",
    "start": "632360",
    "end": "638459"
  },
  {
    "text": "I'm going to subtract the root,\nthe exact root from both sides of this equation.",
    "start": "638460",
    "end": "644320"
  },
  {
    "text": "So this is the absolute error\nin the i plus 1 approximation.",
    "start": "644320",
    "end": "649900"
  },
  {
    "text": "It's equal to this. And we're going to do\na little trick, OK?",
    "start": "649900",
    "end": "654940"
  },
  {
    "text": "The value of the function at the\nroot is exactly equal to zero, and I'm going to expand\nthis as a Taylor series,",
    "start": "654940",
    "end": "663090"
  },
  {
    "text": "about the point xy. So f of xi plus f prime of\nxi times x star minus xi,",
    "start": "663090",
    "end": "671040"
  },
  {
    "text": "plus this second\norder term as well. Plus cubic terms in this\nTaylor expansion, right?",
    "start": "671040",
    "end": "676310"
  },
  {
    "text": "All of those need to sum\nup and be equal to 0, Because f of x star\nby definition is zero. x star is the root.",
    "start": "676310",
    "end": "684000"
  },
  {
    "text": "And buried in this\nexpression here is a quantity which can be\nrelated to xi minus f of xi",
    "start": "684000",
    "end": "693209"
  },
  {
    "text": "over f prime minus x star. It's right here, right? xi minus x star,\nxi minus x star.",
    "start": "693210",
    "end": "699250"
  },
  {
    "text": "I've got to divide\nthrough by f prime. Divide through by f prime,\nand I get f over f prime.",
    "start": "699250",
    "end": "704550"
  },
  {
    "text": "That's this guy here. Those things are equal\nin magnitude then,",
    "start": "704550",
    "end": "709890"
  },
  {
    "text": "to this second order term here. So they are equal\nin magnitude to 1/2, the second derivative of f,\ndivided by f prime, times xi",
    "start": "709890",
    "end": "718380"
  },
  {
    "text": "minus x star squared. And then these cubic terms,\nwell, they're still around. But they're going to be small as\nI get close to the actual root.",
    "start": "718380",
    "end": "725610"
  },
  {
    "text": "So they're negligible, right? Compared to these second order\nterms, they can be neglected.",
    "start": "725610",
    "end": "731115"
  },
  {
    "text": "And you should convince\nyourself that I can apply some of\nthe norm properties that we used before, OK?",
    "start": "731115",
    "end": "736740"
  },
  {
    "text": "To the absolute value. The absolute values is\nthe norm of a scalar. So these norm properties\ntell me that this quantity",
    "start": "736740",
    "end": "743550"
  },
  {
    "text": "has to be less than\nor equal to, right? This ratio of\nderivatives multiplied",
    "start": "743550",
    "end": "750150"
  },
  {
    "text": "by the absolute error\nin step i squared. And I'll divide by that absolute\nerror in step i squared.",
    "start": "750150",
    "end": "757589"
  },
  {
    "text": "So taking the limit\nis i goes to infinity, this ratio here is\nbound by a constant.",
    "start": "757590",
    "end": "765520"
  },
  {
    "text": "This is a definition for\nthe rate of convergence. It says I take the absolute\nerror in step i plus 1.",
    "start": "765520",
    "end": "771930"
  },
  {
    "text": "I divide it by the absolute\nerror in step i squared. And it will always be\nsmaller than some constant,",
    "start": "771930",
    "end": "777540"
  },
  {
    "text": "as i goes to infinity. So it converges\nquadratically, right? If the relative error in step\ni was order 10 to the minus 1,",
    "start": "777540",
    "end": "789667"
  },
  {
    "text": "then the relative\nerror in step i plus 1 will be order 10 to the minus 2. Because they got to be\nbound by this constant.",
    "start": "789667",
    "end": "796410"
  },
  {
    "text": "If the relative error in\nstep i was 10 to the minus 2, the relative error\nin step i plus 1,",
    "start": "796410",
    "end": "801589"
  },
  {
    "text": "has got to be order 10 to the\nminus 4, or smaller, right? Because I square the\nquantity down here.",
    "start": "801590",
    "end": "807810"
  },
  {
    "text": "I get to double the\nnumber of accurate digits with each iteration. ",
    "start": "807810",
    "end": "814050"
  },
  {
    "text": "And this will hold so long\nas the derivative evaluated at the root is\nnot equal to zero.",
    "start": "814050",
    "end": "819660"
  },
  {
    "text": "If the derivative evaluated\nat the root is equal to zero, this analysis\nwasn't really valid. You can't divide by zero\nin various places, OK?",
    "start": "819660",
    "end": "829259"
  },
  {
    "text": "It turns out the\nsame thing is true if we do the\nmultidimensional case. I'll leave it to you to\ninvestigate that case.",
    "start": "829260",
    "end": "834960"
  },
  {
    "text": "I think it's interesting for\nyou to try and explore that. It follows the 1D model\nI showed you before. But the absolute error\nin iterate i plus 1,",
    "start": "834960",
    "end": "842930"
  },
  {
    "start": "841000",
    "end": "933000"
  },
  {
    "text": "divided by the absolute\nerror in iterate i-- here's a small typo here. Cross out that plus 1, right? The absolute error\nin iterate i squared",
    "start": "842930",
    "end": "850917"
  },
  {
    "text": "is going to be\nbound by a constant.  And this will be true so\nlong as the determinant",
    "start": "850917",
    "end": "857670"
  },
  {
    "text": "at the Jacobian at the\nroot is not equal to zero. We know the determinant\nof the Jacobian plays the role of the\nderivative in the 1D case.",
    "start": "857670",
    "end": "866194"
  },
  {
    "text": "When the Jacobian\nis singular, you can show that linear convergence\nis going to occur instead. So it will still converge.",
    "start": "866194",
    "end": "872540"
  },
  {
    "text": "It's not necessarily a\nproblem that the Jacobian becomes singular at the root. But you're going to lose your\nrate of quadratic convergence.",
    "start": "872540",
    "end": "879120"
  },
  {
    "text": " And this rate of\nconvergence is only guaranteed if we start\nsufficiently close to the root.",
    "start": "879120",
    "end": "886390"
  },
  {
    "text": "So good initial guesses,\nthat's important. We have a locally\nconvergent method. Bad initial guesses?",
    "start": "886390",
    "end": "892490"
  },
  {
    "text": "Well, who knows where\nthis iterative method is going to go. There's nothing to\nguarantee that it's going to converge even.",
    "start": "892490",
    "end": "898430"
  },
  {
    "text": "Right It may run away someplace. Here are a few examples of\nwhere things can go wrong.",
    "start": "898430",
    "end": "903540"
  },
  {
    "text": "So if I have a local\nminima or maxima, I might have an iterate where\nI evaluate the linearization,",
    "start": "903540",
    "end": "910940"
  },
  {
    "text": "and it tells me my\nnext best approximation is on the other side of\nthis minima or maxima.",
    "start": "910940",
    "end": "916140"
  },
  {
    "text": "And then I go up, and I\nget the linearization here. And it tells me, oh, my\nnext best approximation is on the other side.",
    "start": "916140",
    "end": "921545"
  },
  {
    "text": "And this method could\nbounce back and forth in here for as long\nas we sit and wait.",
    "start": "921545",
    "end": "927839"
  },
  {
    "text": "It's locally convergent,\nnot globally convergent. It can get hung up in\nsituations like this.",
    "start": "927839",
    "end": "933330"
  },
  {
    "start": "933000",
    "end": "1010000"
  },
  {
    "text": "Asymptotes are a problem. I have an asymptote,\nwhich presumably has an effective root\nsomewhere out here at infinity.",
    "start": "933330",
    "end": "940800"
  },
  {
    "text": "Well, my solution\nwould like to follow the linearization, the\nsuccessive linearizations all the way out along\nthis asymptote, right?",
    "start": "940800",
    "end": "947580"
  },
  {
    "text": "So my iterates may blow up\nin an uncontrolled fashion. You can also end\nup with funny cases",
    "start": "947580",
    "end": "954399"
  },
  {
    "text": "where our Newton-Raphson\nsteps continually overshoot the roots.",
    "start": "954400",
    "end": "960090"
  },
  {
    "text": "So they can be functions who\nhave a power loss scaling",
    "start": "960090",
    "end": "965490"
  },
  {
    "text": "right near the root, such that\nthe derivative doesn't exist.",
    "start": "965490",
    "end": "971640"
  },
  {
    "text": "OK? So here the derivative of this\nthing, if s is smaller than 1,",
    "start": "971640",
    "end": "977080"
  },
  {
    "text": "and x equals zero, it\nwon't exist, right? There isn't a derivative\nthat's defined there.",
    "start": "977080",
    "end": "982579"
  },
  {
    "text": "And in those cases, you can\noften wind up with overshoot. So I'll take a linearization,\nand I'll shoot over the root.",
    "start": "982580",
    "end": "989320"
  },
  {
    "text": "And I'll go up and I'll\ntake my next linearization, I'll shoot back on the\nother side of the root. And depending on the power of s\nassociated with this function,",
    "start": "989320",
    "end": "996530"
  },
  {
    "text": "it may diverge, right? I may get further and\nfurther away from the root, or it may slowly converge\ntowards that root.",
    "start": "996530",
    "end": "1004140"
  },
  {
    "text": "But it can be problematic.  Here's another\nproblem that crops up.",
    "start": "1004140",
    "end": "1011910"
  },
  {
    "text": "Sometimes people talk\nabout basins of attraction. So here's a two-dimensional,\nnon-linear equation",
    "start": "1011910",
    "end": "1018180"
  },
  {
    "text": "I want to find the roots for. It's cubic in\nnature, so it's got three roots, which are indicated\nby the stars in the x1 x 2",
    "start": "1018180",
    "end": "1024730"
  },
  {
    "text": "plane. And I've taken a number of\ndifferent initial guesses from all over the\nplane and I've asked--",
    "start": "1024730",
    "end": "1033160"
  },
  {
    "text": "given that initial guess, using\nthe Newton-Raphson method, which root do I find?",
    "start": "1033160",
    "end": "1040000"
  },
  {
    "text": "So if you see a dark\nblue color like this, that means initial guesses\nthere found this root.",
    "start": "1040000",
    "end": "1045189"
  },
  {
    "text": "If you see a medium\nblue color, that means they found this root. See a light blue color, that\nmeans they found this root.",
    "start": "1045190",
    "end": "1051110"
  },
  {
    "text": "And this is a relatively\nsimple function, relatively low",
    "start": "1051110",
    "end": "1056140"
  },
  {
    "text": "dimension, but the plane\nhere is tilled by-- it's not tiled. It's filled with a fractal.",
    "start": "1056140",
    "end": "1061960"
  },
  {
    "text": "These basins of attraction\nare fractal in nature. Which means that I\ncould think that I'm",
    "start": "1061960",
    "end": "1067120"
  },
  {
    "text": "starting with a\nsolution rate here that should converge to this\ngreen root because it's close. But it actually goes over here.",
    "start": "1067120",
    "end": "1073875"
  },
  {
    "text": "And if I change that initial\nguess by a little bit, it actually pops up to this\nroot over here instead.",
    "start": "1073875",
    "end": "1079870"
  },
  {
    "text": "It's quite difficult to\npredict which solution you're going to converge to.",
    "start": "1079870",
    "end": "1087060"
  },
  {
    "text": "Yes? AUDIENCE: And in this case, you\nknew how many roots there are. JAMES SWAN: Yes. AUDIENCE: Often\nyou wouldn't know. So you find one,\nand you're happy.",
    "start": "1087060",
    "end": "1094630"
  },
  {
    "text": "Right? You're happy because\n[INAUDIBLE] physical. Might be the wrong one.",
    "start": "1094630",
    "end": "1100000"
  },
  {
    "text": "JAMES SWAN: So this the problem. ",
    "start": "1100000",
    "end": "1107980"
  },
  {
    "text": "I think this is about the\nminimum level of complexity you need. Which is not very complex\nat all in a function",
    "start": "1107980",
    "end": "1113320"
  },
  {
    "text": "to get these sorts of\nbasins of attraction. Polynomial equations\nare ones that really suffer from\nthis especially,",
    "start": "1113320",
    "end": "1120059"
  },
  {
    "text": "but it's a problem in general. You often don't know. I'll show you quasi\nNewton-Raphson methods",
    "start": "1120060",
    "end": "1126250"
  },
  {
    "text": "that help fix some\nof these problems. How about other problems? It's good to know where\nthe weaknesses are.",
    "start": "1126250",
    "end": "1132240"
  },
  {
    "text": "Newton-Raphson sounds great,\nbut where are the weaknesses? Let's see. The Jacobian-- might not be\neasy to calculate analytically,",
    "start": "1132240",
    "end": "1139890"
  },
  {
    "text": "right? So far we've written\ndown analytical forms for the Jacobian. We've had simple functions.",
    "start": "1139890",
    "end": "1146070"
  },
  {
    "text": "But maybe it's not easy\nto calculate analytically. You should think about\nwhat are the sources for this function,\nf of x, that we're",
    "start": "1146070",
    "end": "1152400"
  },
  {
    "text": "trying to find the roots for. Also we got to\ninvert the Jacobian, and we know that's a matrix. And matrices which have a\nlot of dimensions in them",
    "start": "1152400",
    "end": "1159570"
  },
  {
    "text": "are complicated to invert. There's a huge\namount of complexity, computational complexity,\nin doing those inversions.",
    "start": "1159570",
    "end": "1165675"
  },
  {
    "text": "It can take a long\ntime to do them. It may undesirable to have\nto constantly be solving",
    "start": "1165675",
    "end": "1172350"
  },
  {
    "text": "a system of linear equations. So might think about some\noptions for mitigating this.",
    "start": "1172350",
    "end": "1179500"
  },
  {
    "text": "Sometimes it won't\nconverge at all? Or to the nearest root. This is this overshoot, or\nbasin of attraction problem.",
    "start": "1179500",
    "end": "1185904"
  },
  {
    "text": "And we'll talk about\nthese modifications to correct these issues. They come with a penalty though. OK?",
    "start": "1185904",
    "end": "1190969"
  },
  {
    "text": "So Newton-Raphson was\nbased around the idea of linearization. If we modify that\nlinearization, we're",
    "start": "1190969",
    "end": "1197010"
  },
  {
    "text": "going to lose some of\nthese great benefits of the Newton-Raphson method,\nnamely that it's quadratically",
    "start": "1197010",
    "end": "1202570"
  },
  {
    "text": "convergent, right? We're going to make some\nchanges to the method, and it's not going to converge\nquadratically anymore.",
    "start": "1202570",
    "end": "1208149"
  },
  {
    "text": "It's going to slow\ndown, but maybe we'll be able to rein in the method\nand make it converge either",
    "start": "1208150",
    "end": "1213280"
  },
  {
    "text": "to the roots we want it to\nconverge to or converge more reliably than it would before.",
    "start": "1213280",
    "end": "1219250"
  },
  {
    "text": "Maybe we'll be able to actually\ndo the calculation faster, even though it may\nrequire more iterations.",
    "start": "1219250",
    "end": "1226180"
  },
  {
    "text": "Maybe we can make each\niteration much faster using some of these methods. OK so here are the three things\nthat we're going to talk about.",
    "start": "1226180",
    "end": "1233190"
  },
  {
    "text": "We're going to talk about\napproximating the Jacobian with finite differences. We're talking about\nBroyden's method",
    "start": "1233190",
    "end": "1238730"
  },
  {
    "text": "for approximating the\ninverse of the Jacobian. And we're going to talk\nabout something called damped Newton-Raphson methods.",
    "start": "1238730",
    "end": "1243981"
  },
  {
    "text": "Those will be the three\ntopics of the day.  So here's what I said before.",
    "start": "1243981",
    "end": "1249980"
  },
  {
    "text": "Analytical calculations\nof Jacobian requires analytical\nformulas for f. And for functions of a\nfew dimensions, right?",
    "start": "1249980",
    "end": "1256070"
  },
  {
    "start": "1251000",
    "end": "1417000"
  },
  {
    "text": "These calculations\nare not too tough. For functions of\nmany dimensions, this is tedious at best.",
    "start": "1256070",
    "end": "1263710"
  },
  {
    "text": "Error prone, at worst. Think about even something like\n10 equations for 10 unknowns.",
    "start": "1263710",
    "end": "1269190"
  },
  {
    "text": "If your error rate is\n1%, well, you're shot. There's a pretty good\nchance that you missed",
    "start": "1269190",
    "end": "1274550"
  },
  {
    "text": "one element of the Jacobian. You made a mistake\nsomewhere in there. And now you're not\ndoing Newton-Raphson You're doing some other\niterative method that isn't",
    "start": "1274550",
    "end": "1281654"
  },
  {
    "text": "the one that you intended. There are a lot of times\nwhere you-- maybe you",
    "start": "1281654",
    "end": "1286745"
  },
  {
    "text": "have an analytical formula for\nsome of these f's, but not all of them.",
    "start": "1286745",
    "end": "1291980"
  },
  {
    "text": "So where can these\nfunctionalities come from? We've seen some cases, where\nyou have physical models.",
    "start": "1291980",
    "end": "1297950"
  },
  {
    "text": "Thermodynamic models that\nyou can write down by hand. But where are other places\nthat these functions come from?",
    "start": "1297950",
    "end": "1303220"
  },
  {
    "text": " Ideas? AUDIENCE: [INAUDIBLE]",
    "start": "1303220",
    "end": "1310426"
  },
  {
    "text": "JAMES SWAN: Oh, good. AUDIENCE: [INAUDIBLE] JAMES SWAN: Beautiful. So this is going to be the\nmost common case, right?",
    "start": "1310426",
    "end": "1316754"
  },
  {
    "text": "Maybe you want to use some\nsort of simulation code, right? To model something. It's somebody else's\nsimulation code.",
    "start": "1316754",
    "end": "1323260"
  },
  {
    "text": "They're an expert at doing\nfinite element modeling. But the output is this f\nthat you're interested,",
    "start": "1323260",
    "end": "1329900"
  },
  {
    "text": "and the input to the\nsimulation are these x's. And you want to find the roots\nassociated with this problem",
    "start": "1329900",
    "end": "1335960"
  },
  {
    "text": "that you're solving via\nthe simulation code, right? This is pretty important\nbeing able to connect different pieces of\nsoftware together.",
    "start": "1335960",
    "end": "1343420"
  },
  {
    "text": "Well, there's no analytical\nformula for f there. OK?",
    "start": "1343420",
    "end": "1348600"
  },
  {
    "text": "You're shot. So it may come from\nresults of simulations. This is extremely common.",
    "start": "1348600",
    "end": "1353710"
  },
  {
    "text": "It could come from\ninterpretation of data. So you may have a\nbunch of data that's being generated by some physical\nmeasurement or a process,",
    "start": "1353710",
    "end": "1361780"
  },
  {
    "text": "either continuously or\nyou just have a data set that's available to you.",
    "start": "1361780",
    "end": "1367030"
  },
  {
    "text": "But these function\nvalues are often not, they're not things that\nyou know analytically.",
    "start": "1367030",
    "end": "1372250"
  },
  {
    "text": "It may also be the case that,\noh, man, even Aspen, you're going to wind up solving\nsystems of nonlinear equations.",
    "start": "1372250",
    "end": "1379157"
  },
  {
    "text": "It's going to use the\nNewton-Raphson method. Aspen's going to have lots\nof these formulas in it for functions.",
    "start": "1379157",
    "end": "1385250"
  },
  {
    "text": "Whose going in by\nhand and computing the derivatives of all\nthese functions for aspen?",
    "start": "1385250",
    "end": "1391009"
  },
  {
    "text": "MATLAB has a nonlinear\nequation solver in it. You give it the\nfunction, and it'll find the root of the\nequation, given a guess.",
    "start": "1391010",
    "end": "1398000"
  },
  {
    "text": "It's going to use the\nNewton-Raphson method. Whose computing the\nJacobian for MATLAB?",
    "start": "1398000",
    "end": "1403621"
  },
  {
    "text": "You can. You can compute it by\nhand, and give it an input. Sometimes that's a\nreally good thing to do.",
    "start": "1403621",
    "end": "1408987"
  },
  {
    "text": "But sometimes, we don't\nhave that available to us. So we need alternative ways\nof computing the Jacobian. The simplest one is a finite\ndifference approximation.",
    "start": "1408987",
    "end": "1417409"
  },
  {
    "start": "1417000",
    "end": "1873000"
  },
  {
    "text": "So you recall the definition\nof the derivative. It's the limit of this\ndifference, f of x plus epsilon",
    "start": "1417410",
    "end": "1425570"
  },
  {
    "text": "minus f of x divided by epsilon,\nas epsilon goes to zero. There's an error in\nthis approximation",
    "start": "1425570",
    "end": "1432260"
  },
  {
    "text": "for the derivative with a\nfinite value for epsilon, which is proportional\nto epsilon.",
    "start": "1432260",
    "end": "1438320"
  },
  {
    "text": "So choose a small\nvalue of epsilon. You'll get a good approximation\nfor the derivative.",
    "start": "1438320",
    "end": "1444080"
  },
  {
    "text": "It turns out the accuracy\ndepends on epsilon, but kind of in a\nnon-intuitive way. And here's a simple example. So let's compute the\nderivative of f of x equals e",
    "start": "1444080",
    "end": "1452270"
  },
  {
    "text": "the x, which is e to the x. Let's evaluate it at x equals 1.",
    "start": "1452270",
    "end": "1457670"
  },
  {
    "text": "So F prime of 1 is e the 1,\nwhich should approximately be e to the 1 plus epsilon\nminus e to the 1 over epsilon.",
    "start": "1457670",
    "end": "1466740"
  },
  {
    "text": "And here I've done\nthis calculation. And I've asked, what's\nthe absolute error",
    "start": "1466740",
    "end": "1472600"
  },
  {
    "text": "in this calculation by taking\nthe difference between this and this, for different\nvalues of epsilon.",
    "start": "1472600",
    "end": "1479290"
  },
  {
    "text": "You can see initially,\nas epsilon gets smaller, the absolute error goes down\nin proportion to epsilon.",
    "start": "1479290",
    "end": "1484980"
  },
  {
    "text": "10 to the minus 3,\n10 to the minus 3. 10 to the minus 4,\n10 to the minus 4. 10 to the minus 8,\n10 to the minus 8.",
    "start": "1484980",
    "end": "1491379"
  },
  {
    "text": "10 to the minus 9. 10 to the minus 7. 10 to the minus 10. And 10 to the minus 6. So it went down,\nand it came back up.",
    "start": "1491380",
    "end": "1498860"
  },
  {
    "text": "But that's not what this formula\ntold us should happen, right? Yes? AUDIENCE: So just to be sure.",
    "start": "1498860",
    "end": "1504316"
  },
  {
    "text": "That term in that\ncolumn on the right? JAMES SWAN: Yes? AUDIENCE: It says\nexponential 1, but it",
    "start": "1504316",
    "end": "1510290"
  },
  {
    "text": "represents the approximation? JAMES SWAN: Exponential 1 is\nexponential 1. f prime of 1 is our approximation here.",
    "start": "1510291",
    "end": "1516310"
  },
  {
    "text": "AUDIENCE: Oh, OK. JAMES SWAN: Sorry\nthat that's unclear. Yes, so this is the absolute\nerror in this approximation.",
    "start": "1516310",
    "end": "1523370"
  },
  {
    "text": "So it goes down,\nand then it goes up. Is that clear now? Good. OK, why does it go down?",
    "start": "1523370",
    "end": "1528730"
  },
  {
    "text": "It goes down because our\ndefinition of the derivative says it should go down. At some point, I've actually\ngot to do these calculations",
    "start": "1528730",
    "end": "1537610"
  },
  {
    "text": "with high enough accuracy\nto be able to perceive the difference between e to\nthe 1 plus, 10 to the minus 9,",
    "start": "1537610",
    "end": "1544450"
  },
  {
    "text": "and e to the 1. So there is a truncation\nerror in the calculation",
    "start": "1544450",
    "end": "1549640"
  },
  {
    "text": "of this difference that reduces\nmy accuracy at a certain level.",
    "start": "1549640",
    "end": "1554810"
  },
  {
    "text": "There's a heuristic\nyou can use here, OK? You want to set\nthis epsilon when you do this finite\ndifference approximation,",
    "start": "1554810",
    "end": "1560920"
  },
  {
    "text": "to be the square\nroot of the machine precision times the magnitude\nof x, the point at which you're",
    "start": "1560920",
    "end": "1566830"
  },
  {
    "text": "trying to calculate\nthis derivative. So that's, usually\nwe're double precision. So this is something\nlike 10 to the minus 8",
    "start": "1566830",
    "end": "1572200"
  },
  {
    "text": "times the magnitude of x. That's pretty good. That holds true here. OK? You can test it out on\nsome other functions.",
    "start": "1572200",
    "end": "1580140"
  },
  {
    "text": "If x is 0, or very small. We don't want a\nrelative tolerance. We've got to choose an\nabsolute tolerance instead.",
    "start": "1580140",
    "end": "1586110"
  },
  {
    "text": "Just like we talked about\nwith the step norm criteria. So one has to be a\nlittle bit careful in how you implement this.",
    "start": "1586110",
    "end": "1591850"
  },
  {
    "text": "But this is a good guide, OK? A good way to think about how\nthe error is going to go down,",
    "start": "1591850",
    "end": "1597690"
  },
  {
    "text": "and where it's going to\nstart to come back up. Make sense? Good.",
    "start": "1597690",
    "end": "1603880"
  },
  {
    "text": "OK, so how do you compute\nelements of the Jacobian then? Well, those are all\njust partial derivatives of the function with respect to\none of the unknown variables.",
    "start": "1603880",
    "end": "1613100"
  },
  {
    "text": "So partial f i with\nrespect to x j is just f i at x plus some\nepsilon deviation of x",
    "start": "1613100",
    "end": "1623310"
  },
  {
    "text": "in its j-th component only. So this is like a unit\nvector in the j direction,",
    "start": "1623310",
    "end": "1628710"
  },
  {
    "text": "or associated with the j-th\nelement of this vector. Minus f i of x divided\nby this epsilon.",
    "start": "1628710",
    "end": "1635475"
  },
  {
    "text": " Equivalently, you'd\nhave to do this for f i.",
    "start": "1635475",
    "end": "1641590"
  },
  {
    "text": "You can compute all the\ncolumns of the Jacobian very quickly by calling\nf of x plus epsilon",
    "start": "1641590",
    "end": "1648220"
  },
  {
    "text": "minus f of x over epsilon. Just evaluate your vector-valued\nfunction at these different",
    "start": "1648220",
    "end": "1653710"
  },
  {
    "text": "x's. Take the difference,\nand that will give you column j of your Jacobian.",
    "start": "1653710",
    "end": "1660940"
  },
  {
    "text": "So how many function\nevaluations does it take to calculate the\nJacobian at a single point?",
    "start": "1660940",
    "end": "1667549"
  },
  {
    "text": "How many times do I have\nto evaluate my function? ",
    "start": "1667549",
    "end": "1676492"
  },
  {
    "text": "Yeah? AUDIENCE: 2 n. JAMES SWAN: 2n, right. So if I have n, if I\nhave n elements to x,",
    "start": "1676492",
    "end": "1685530"
  },
  {
    "text": "I've got to make two function\ncalls per column of j. There's going to\nbe n columns in j.",
    "start": "1685530",
    "end": "1691986"
  },
  {
    "text": "So 2n function evaluations\nto compute the Jacobian at a single point.",
    "start": "1691986",
    "end": "1697290"
  },
  {
    "text": "Is that really true though? Not quite. f of x is f of x. I don't have to\ncompute it every time.",
    "start": "1697290",
    "end": "1702790"
  },
  {
    "text": "I just compute f of x once. So it's really like n plus\n1 that I have to do, right?",
    "start": "1702790",
    "end": "1708460"
  },
  {
    "text": "N plus a function evaluations\nto compute this thing. I actually got to\ncompute them though.",
    "start": "1708460",
    "end": "1713710"
  },
  {
    "text": "Function evaluations\nmay be really expensive. Suppose you're doing some sort\nof complicated simulation,",
    "start": "1713710",
    "end": "1720136"
  },
  {
    "text": "like a finite\nelement simulation. Maybe it takes minutes to\ngenerate a function evaluation. So it can be\nexpensive to compute",
    "start": "1720136",
    "end": "1726340"
  },
  {
    "text": "the Jacobian in this way. Just be expensive to\ncompute the Jacobian.",
    "start": "1726340",
    "end": "1731500"
  },
  {
    "text": "How is approximation\nof Jacobian going to affect the convergence? ",
    "start": "1731500",
    "end": "1738289"
  },
  {
    "text": "What's going to happen to\nthe rate of convergence of our method? It's going to go down, right?",
    "start": "1738289",
    "end": "1744890"
  },
  {
    "text": "It's probably not\ngoing to be linear. It's not going to be quadratic. It's going to be some\nsuper linear factor.",
    "start": "1744890",
    "end": "1750230"
  },
  {
    "text": "It's going to depend on how\naccurate the Jacobian is. How sensitive the\nfunction is near the root.",
    "start": "1750230",
    "end": "1757960"
  },
  {
    "text": "But it's going to reduce\nthe accuracy of the method, or the convergence rate of\nthe method by a little bit.",
    "start": "1757960",
    "end": "1763879"
  },
  {
    "text": "That's OK. So this is what MATLAB does. It uses a finite\ndifference approximation",
    "start": "1763880",
    "end": "1769460"
  },
  {
    "text": "for your Jacobian. When you give it a function,\nand you don't tell it the Jacobian explicitly. ",
    "start": "1769460",
    "end": "1775990"
  },
  {
    "text": "Here's an example of how\nto implement this yourself. So I've got to have\nsome function that does whatever this\nfunction is supposed to do.",
    "start": "1775990",
    "end": "1782736"
  },
  {
    "text": "It takes as input x and\nit gives an output f. And then the Jacobian, right? It's a matrix.",
    "start": "1782736",
    "end": "1788740"
  },
  {
    "text": "So we initialize this matrix. We loop over each\nof the columns.",
    "start": "1788740",
    "end": "1793990"
  },
  {
    "text": "We compute the\ndisplacement right? The deviation from\nx for each of these.",
    "start": "1793990",
    "end": "1800512"
  },
  {
    "text": "And then we compute\nthis difference and divide it by epsilon. I haven't done everything\nperfect here, right?",
    "start": "1800512",
    "end": "1806590"
  },
  {
    "text": "Here's an extra\nfunction evaluation. I could just calculate\nthe value of the function at x before doing the loop.",
    "start": "1806590",
    "end": "1812486"
  },
  {
    "text": "I've also only used a\nrelative tolerance here. I'm going to be in\ntrouble if xi is 0. It's going to be a problem\nwith this algorithm.",
    "start": "1812486",
    "end": "1818350"
  },
  {
    "text": "These are the little details\none has to pay attention to. But it's a simple enough\ncalculation to do.",
    "start": "1818350",
    "end": "1824559"
  },
  {
    "text": "Loop over the columns, right? Compute these differences. Divide by epsilon. You have your approximation\nfor the Jacobian.",
    "start": "1824560",
    "end": "1830980"
  },
  {
    "text": "I've got to do that at\nevery iteration, right? Every time x is updated, I've\ngot to recompute my Jacobian.",
    "start": "1830980",
    "end": "1836680"
  },
  {
    "text": " That's it though. All right, that's one way\nof approximating a Jacobian.",
    "start": "1836680",
    "end": "1844309"
  },
  {
    "text": "There s a method that's used in\none dimension called the Secant method.",
    "start": "1844310",
    "end": "1849380"
  },
  {
    "text": "It's a special case of\nthe Newton-Raphson method and uses a coarser approximation\nfor the derivative. It says, I was taking these\nsteps from xi minus 1 to xi.",
    "start": "1849380",
    "end": "1859600"
  },
  {
    "text": "And I knew the\nfunction values there. Maybe I should just\ncompute the slope of the line that goes through\nthose points, and say,",
    "start": "1859600",
    "end": "1865400"
  },
  {
    "text": "that's my approximation\nfor the derivative. Why not? I have the data available to me. It seems like a\nsensible thing to do.",
    "start": "1865400",
    "end": "1872420"
  },
  {
    "text": "So we replace f prime at x1 with\nf of xi minus f of x i minus 1.",
    "start": "1872420",
    "end": "1880590"
  },
  {
    "text": "Down here we put xi\nminus xi minus 1 up here. That's our approximation\nfor the derivative,",
    "start": "1880590",
    "end": "1885910"
  },
  {
    "text": "or the inverse of\nthe derivative. This can work, it\ncan work just fine.",
    "start": "1885910",
    "end": "1891460"
  },
  {
    "text": "Can it be extended\nto many dimensions? That's an interesting\nquestion, though? This is simple. In many dimensions,\nnot so obvious right?",
    "start": "1891460",
    "end": "1897880"
  },
  {
    "text": "If I know xi, xi minus 1.\nf of xi, f of si minus 1. Can I approximate the Jacobian?",
    "start": "1897880",
    "end": "1906841"
  },
  {
    "text": "What do you think? ",
    "start": "1906841",
    "end": "1915033"
  },
  {
    "text": "Does it strike you\nas though there might be some fundamental\ndifficulty to doing that? ",
    "start": "1915034",
    "end": "1929704"
  },
  {
    "text": "Yeah? AUDIENCE: Could you\napproximate the gradient? [INAUDIBLE] gradient of f at x.",
    "start": "1929704",
    "end": "1939533"
  },
  {
    "text": "JAMES SWAN: OK. AUDIENCE: But I'm sure\nif you whether you can go backwards from the\ngradient in the Jacobian.",
    "start": "1939533",
    "end": "1947440"
  },
  {
    "text": "JAMES SWAN: OK. So, let's-- go ahead. AUDIENCE: Perhaps\nthe difficulty is,",
    "start": "1947440",
    "end": "1953534"
  },
  {
    "text": "I mean when they're\njust single values-- JAMES SWAN: Yeah. AUDIENCE: You can think of\n[INAUDIBLE] derivative, right? JAMES SWAN: Yeah.",
    "start": "1953534",
    "end": "1959190"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nget really big, you get a vector of\na function at xi, a vector of a function of\nxi minus 1 or whatever.",
    "start": "1959190",
    "end": "1967289"
  },
  {
    "text": "Vectors of these x's. And so if you're [INAUDIBLE]",
    "start": "1967290",
    "end": "1972377"
  },
  {
    "text": "JAMES SWAN: Yeah, so how\ndo I divide these things? That's a good question. The Jacobian-- how much\ninformation content",
    "start": "1972377",
    "end": "1978210"
  },
  {
    "text": "is in the Jacobian? Or how many\nindependent quantities are built into the Jacobian?",
    "start": "1978210",
    "end": "1984009"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] JAMES SWAN: And squared. And how much data do I\nhave to work with here?",
    "start": "1984010",
    "end": "1990159"
  },
  {
    "text": "You know, order n data. To figure out order\nn squared quantities. This is the division problem\nyou're describing, right?",
    "start": "1990160",
    "end": "1996490"
  },
  {
    "text": "So it seems like this is\nan underdetermined sort of problem. And it is. OK?",
    "start": "1996490",
    "end": "2001740"
  },
  {
    "text": "So there isn't a direct\nanalog to the Secant method in dimensions.",
    "start": "2001740",
    "end": "2007110"
  },
  {
    "text": "We can write down\nsomething that makes sense. So this is the 1D\nSecant approximation.",
    "start": "2007110",
    "end": "2012990"
  },
  {
    "text": "That the value of the\nderivative multiplied by the step between\ni minus 1 and i",
    "start": "2012990",
    "end": "2018750"
  },
  {
    "text": "is approximated\nby the difference in the values of the function.",
    "start": "2018750",
    "end": "2023840"
  },
  {
    "text": "The equivalent is the value\nof the Jacobian multiplied by the step between\ni minus 1 and i",
    "start": "2023840",
    "end": "2029120"
  },
  {
    "text": "is equal to the difference\nbetween the values of the functions.",
    "start": "2029120",
    "end": "2034220"
  },
  {
    "text": "But now this is\nan equation for n squared elements\nof the Jacobian,",
    "start": "2034220",
    "end": "2040260"
  },
  {
    "text": "in terms of n elements\nof the function, right? So it's massively,\nmassively underdetermined.",
    "start": "2040260",
    "end": "2047690"
  },
  {
    "text": "OK? Here we have an equation for-- we have one equation\nfor one unknown.",
    "start": "2047690",
    "end": "2054280"
  },
  {
    "text": "The derivative, right? Think about how it was moving\nthrough space before, right? The difference here, xi minus 1,\nthat's some sort of linear path",
    "start": "2054280",
    "end": "2064119"
  },
  {
    "text": "that I'm moving\nalong through space. How am I supposed to figure out\nwhat the tangent curves to all",
    "start": "2064120",
    "end": "2070419"
  },
  {
    "text": "these functions are\nfrom this linear path through multidimensional\nspace, right? That's not going to work.",
    "start": "2070420",
    "end": "2077260"
  },
  {
    "text": "So there's\nunderdetermined problems. It's not so-- that's\nnot so bad, actually. Right? Doesn't mean\nthere's no solution.",
    "start": "2077260",
    "end": "2082719"
  },
  {
    "text": "In fact, it means there's\nall a lot of solutions. So we can pick whichever\none we think is suitable. And Broyden's method\nis a method for picking",
    "start": "2082719",
    "end": "2090179"
  },
  {
    "text": "one of these\npotential solutions to this underdetermined problem. We don't have enough information\nto calculate the Jacobian",
    "start": "2090179",
    "end": "2096780"
  },
  {
    "text": "exactly. But maybe we can construct a\nsuitable approximation for it. And here's what's done.",
    "start": "2096780",
    "end": "2104200"
  },
  {
    "start": "2103000",
    "end": "2666000"
  },
  {
    "text": "So here's the Secant\napproximation. It says the Jacobian\ntimes the step size, or the\nNewton-Raphson step,",
    "start": "2104200",
    "end": "2110480"
  },
  {
    "text": "should be the difference\nin the functions. And Newton's method for x\ni, said xi minus xi minus 1",
    "start": "2110480",
    "end": "2120660"
  },
  {
    "text": "was equal-- times\nthe Jacobian, was equal to minus f of xi minus 1. This is just Newton's method. Invert the Jacobian, and\nput it on the other side",
    "start": "2120660",
    "end": "2127710"
  },
  {
    "text": "of the equation. Broyden's method said,\ni-- there's a trick here. Take the difference\nbetween these things.",
    "start": "2127710",
    "end": "2133970"
  },
  {
    "text": "I get the same left-hand side\non both of these equations. So take the difference,\nand I can figure out how the Jacobian should change\nfrom one step to the next.",
    "start": "2133970",
    "end": "2141894"
  },
  {
    "text": "So maybe I have a good\napproximation to the Jacobian at xi minus i, I might\nbe able to use this still",
    "start": "2141894",
    "end": "2148380"
  },
  {
    "text": "underdetermined problem\nto figure out how to update that Jacobian, right? So Broyden's method is what's\nreferred to as the rank one",
    "start": "2148380",
    "end": "2156270"
  },
  {
    "text": "update. You should convince\nyourself that letting the Jacobian at xi minus\nthe Jacobian at xi minus 1",
    "start": "2156270",
    "end": "2164500"
  },
  {
    "text": "be equal to this is\none possible solution of this underdetermined\nequation.",
    "start": "2164500",
    "end": "2170500"
  },
  {
    "text": "There are others. This is one possible solution. It turns out to be a\ngood one to choose.",
    "start": "2170500",
    "end": "2177820"
  },
  {
    "text": "So there's an\niterative approximation now for the Jacobian. ",
    "start": "2177820",
    "end": "2184615"
  },
  {
    "text": "Does this strategy make sense? It's a little weird, right? There's something tricky here. You got to know to do this.",
    "start": "2184615",
    "end": "2190600"
  },
  {
    "text": "Right, so somebody\nhas to have in mind already that they're looking\nfor differences in the Jacobian that they're going\nto update over time.",
    "start": "2190600",
    "end": "2196510"
  },
  {
    "text": " So this tells me the Jacobian,\nhow the Jacobian is updated.",
    "start": "2196510",
    "end": "2202780"
  },
  {
    "text": " Really we need the\nJacobian inverse,",
    "start": "2202780",
    "end": "2209480"
  },
  {
    "text": "and the reason for choosing\nthis rank one update approximation is it's\npossible to write",
    "start": "2209480",
    "end": "2217100"
  },
  {
    "text": "the inverse of j of xi in\nterms of the inverse of j at xi minus 1 when this\nupdate formula is true.",
    "start": "2217100",
    "end": "2225100"
  },
  {
    "text": "So it's something called the\nSherman Morrison Formula, which says the inverse of a matrix\nplus the dyadic product of two",
    "start": "2225100",
    "end": "2231440"
  },
  {
    "text": "vectors can be\nwritten in this form. We don't need to derive\nthis, but this is true.",
    "start": "2231440",
    "end": "2238119"
  },
  {
    "text": "This matrix plus dyadic\nproduct is exactly this. We have dyadic product\nbetween f and the step",
    "start": "2238120",
    "end": "2244660"
  },
  {
    "text": "from xi minus 1 to x. And so we can apply that\nSherman Morrison Formula",
    "start": "2244660",
    "end": "2249910"
  },
  {
    "text": "to the rank one update. And not only can we update\nthe Jacobian iteratively, but we can update\nthe Jacobian inverse.",
    "start": "2249910",
    "end": "2256280"
  },
  {
    "text": "So if I know j inverse\nat some previous time, I know j inverse at\nsome later time too.",
    "start": "2256280",
    "end": "2261899"
  },
  {
    "text": "I don't have to\ncompute these things. I don't have to solve these\nsystems of equations, right? I just update this matrix.",
    "start": "2261899",
    "end": "2267220"
  },
  {
    "text": " Update this matrix,\nand I can very rapidly do these computations.",
    "start": "2267220",
    "end": "2274639"
  },
  {
    "text": "So not only do we have\nan iterative formula for the steps, right? From x 0 to x1 to x 2,\nall the way up to our",
    "start": "2274639",
    "end": "2281381"
  },
  {
    "text": "converged solution,\nbut we can have a formula for the\ninverse of the Jacobian. We give up accuracy.",
    "start": "2281382",
    "end": "2287170"
  },
  {
    "text": "But that's paid for in\nterms of the amount of time we have to spend doing\nthese calculations.",
    "start": "2287170",
    "end": "2292930"
  },
  {
    "text": "Does it pay off? It depends on the\nproblem, right? We try to solve problems\nin different ways. This is a pretty common way\nto approximate the Jacobian.",
    "start": "2292930",
    "end": "2302630"
  },
  {
    "text": "Questions about this?  No.",
    "start": "2302631",
    "end": "2308130"
  },
  {
    "text": "OK. Broyden's method.  All right, here's the last one.",
    "start": "2308130",
    "end": "2313910"
  },
  {
    "text": " The Damped\nNewton-Raphson method. We'll do this in one dimension.",
    "start": "2313910",
    "end": "2320329"
  },
  {
    "text": "So the Newton-Raphson method,\nNewton and Raphson told us, take a step from xi to xi\nplus 1 that is this big.",
    "start": "2320330",
    "end": "2326780"
  },
  {
    "text": " xi to xi plus 1, it's this big.",
    "start": "2326780",
    "end": "2333569"
  },
  {
    "text": "Sometimes you'll take\nthat step, and you'll find that the value of\nthe function at xi plus 1",
    "start": "2333570",
    "end": "2339360"
  },
  {
    "text": "is even bigger than the\nvalue of the function at xi. There was nothing about the\nNewton-Raphson method that",
    "start": "2339360",
    "end": "2344429"
  },
  {
    "text": "told us the function value was\nalways going to be decreasing. But actually, our goal is to\nmake the function value go",
    "start": "2344429",
    "end": "2349440"
  },
  {
    "text": "to 0 in absolute value. So it seems like this step,\nnot a very good one, right?",
    "start": "2349440",
    "end": "2356370"
  },
  {
    "text": "What are Newton and\nRaphson thinking here. This is not a good idea. The function value went up. ",
    "start": "2356370",
    "end": "2364829"
  },
  {
    "text": "Far from a root, OK? The Newton-Raphson\nmethod is going to give these sorts\nof erratic responses. Who knows what direction\nit's going to go?",
    "start": "2364830",
    "end": "2371370"
  },
  {
    "text": "And it's only\nlocally convergent. It tells us a\ndirection to move in,",
    "start": "2371370",
    "end": "2377191"
  },
  {
    "text": "but it doesn't always give\nthe right sort of magnitude associated with that step. And so you take these\nsteps and you can find out",
    "start": "2377191",
    "end": "2383310"
  },
  {
    "text": "the value of your function, the\nnormed value of your functions. It's bigger than\nwhere you started. It seems like you're getting\nfurther away from the root.",
    "start": "2383310",
    "end": "2389670"
  },
  {
    "text": "Our ultimate goal is to\ndrive this norm to 0. So steps like that you might\neven call unacceptable.",
    "start": "2389670",
    "end": "2395309"
  },
  {
    "text": "Right? Why would I ever take a\nstep in that direction? Maybe I should use\na different method. When I take a step that's\nso big my function value",
    "start": "2395310",
    "end": "2401550"
  },
  {
    "text": "grows in norm value.  So what one does, oftentimes,\nis introduce a damping factor,",
    "start": "2401550",
    "end": "2408920"
  },
  {
    "text": "right? We said that this\nratio, or equivalently, the Jacobian inverse times\nthe value of the function,",
    "start": "2408920",
    "end": "2415760"
  },
  {
    "text": "gives us the right\ndirection to step in. But how big a step\nshould we take?",
    "start": "2415760",
    "end": "2421760"
  },
  {
    "text": "It's clear a step like\nthis is a good one. It reduced the value\nof the function.",
    "start": "2421760",
    "end": "2427320"
  },
  {
    "text": " And it's better than\nthe one we took before,",
    "start": "2427320",
    "end": "2432480"
  },
  {
    "text": "which was given by the\nlinear approximation. So if I draw the tangent\nline, it intercepts here.",
    "start": "2432480",
    "end": "2438150"
  },
  {
    "text": "If I take a step\nin this direction, but I reduce the\nslope by having some damping factor that's\nsmaller than 1,",
    "start": "2438150",
    "end": "2443990"
  },
  {
    "text": "I get closer to the root. Ideally we'd like to\nchoose that damping factor",
    "start": "2443990",
    "end": "2450710"
  },
  {
    "text": "to be the one that minimizes\nthe value of the function at xi plus 1.",
    "start": "2450710",
    "end": "2457280"
  },
  {
    "text": "So it's the argument\nthat minimizes the value of the function at xi\nplus 1 or at xi minus alpha, f",
    "start": "2457280",
    "end": "2465530"
  },
  {
    "text": "over f prime. Solving that\noptimization problem, what's hard as finding\nthe root itself.",
    "start": "2465530",
    "end": "2472150"
  },
  {
    "text": "So ideally this is true. But practically you're not\ngoing to be able to do it. So we have to come up with some\napproximate methods of solving",
    "start": "2472150",
    "end": "2480540"
  },
  {
    "text": "this optimization problem. Actually we don't even care\nabout getting it exact. We know Newton-Raphson\ndoes a pretty good job.",
    "start": "2480540",
    "end": "2485670"
  },
  {
    "text": "We want some sort\nof guess that's respectable for this alpha so\nthat we get close to this root.",
    "start": "2485670",
    "end": "2492215"
  },
  {
    "text": "Once we get close,\nwe'll probably choose alpha equal to 1. We'll just take the\nNewton-Raphson steps all the way down to the root.",
    "start": "2492216",
    "end": "2500099"
  },
  {
    "text": "So here it is in\nmany dimensions. Modify the Newton-Raphson\nstep by some value alpha,",
    "start": "2500100",
    "end": "2505960"
  },
  {
    "text": "choose alpha to be\nthe argument that minimizes the norm value of\nthe function at xi plus 1.",
    "start": "2505960",
    "end": "2512269"
  },
  {
    "text": " Here's one way of doing this.",
    "start": "2512270",
    "end": "2517340"
  },
  {
    "text": "So this is called the\nArmijo line search. See? Line search. Start by letting\nalpha equal to 1.",
    "start": "2517340",
    "end": "2523400"
  },
  {
    "text": "Take the full Newton-Raphson\nstep, and check. Was the value of my function\nsmaller than where I started?",
    "start": "2523400",
    "end": "2529000"
  },
  {
    "text": "If it is, let's take the step. It's getting us-- we're\naccomplishing our goal. We re reducing the value\nof the function in norm.",
    "start": "2529000",
    "end": "2536180"
  },
  {
    "text": "Maybe we're headed towards z. That's good. Accept it. If no, let's replace\nalpha with alpha over 2.",
    "start": "2536180",
    "end": "2544440"
  },
  {
    "text": "Let's take a shorter step.  We take a shorter\nstep, and we repeat.",
    "start": "2544440",
    "end": "2550190"
  },
  {
    "text": "Right? Take the shorter step. Check whether the value of the\nfunction with the shorter step is acceptable.",
    "start": "2550190",
    "end": "2555740"
  },
  {
    "text": "If yes, let's take\nit, and let's move on. And if no, replace alpha with\nalpha over 2, and continue.",
    "start": "2555740",
    "end": "2562590"
  },
  {
    "text": "So we have our step\nsize every time. We don't just have to have it. We could choose different\nfactors to reduce it by.",
    "start": "2562590",
    "end": "2569930"
  },
  {
    "text": "But we try to take\nshorter and shorter steps until we accomplish our goal\nof having a function which",
    "start": "2569930",
    "end": "2576710"
  },
  {
    "text": "is smaller in norm\nat our next iterate than where we were before. It's got-- the function\nvalue will be reduced.",
    "start": "2576710",
    "end": "2583910"
  },
  {
    "text": "The Newton-Raphson\nmethod picks a direction that wants to bring the\nfunction value closer to 0.",
    "start": "2583910",
    "end": "2590170"
  },
  {
    "text": "We linearize the\nfunction, and we found the direction we needed\nto go to make that linearization go to 0. So there is a step size\nfor which the function",
    "start": "2590170",
    "end": "2598309"
  },
  {
    "text": "value will be reduced. And because of that,\nthis Armijo line search",
    "start": "2598310",
    "end": "2603631"
  },
  {
    "text": "of the Damped\nNewton-Raphson method is actually globally\nconvergent, right? The iterative method\nwill terminate.",
    "start": "2603632",
    "end": "2610369"
  },
  {
    "text": "You can guarantee it. Here's what it looks\nlike graphically. I take my big step, my\nalpha equals 1 step.",
    "start": "2610370",
    "end": "2615790"
  },
  {
    "text": "I check the value\nof the function. It's bigger in absolute\nvalue than where I started. So I go back. I take half that step size.",
    "start": "2615790",
    "end": "2622830"
  },
  {
    "text": "OK? I look at the value\nof the function. It's still bigger. Let's reject it, and go back. I take half that\nstep size again.",
    "start": "2622830",
    "end": "2628719"
  },
  {
    "text": "The value of the\nfunction here is now smaller in absolute value. So I accept it. And I put myself pretty\nclose to the root.",
    "start": "2628719",
    "end": "2637200"
  },
  {
    "text": "So it's convergent,\nglobally convergent. That's nice. It's not globally convergent\nto roots, which is a pain.",
    "start": "2637200",
    "end": "2645712"
  },
  {
    "text": "But it's globally convergent. It will terminate eventually. You'll get to a\npoint where you won't",
    "start": "2645712",
    "end": "2651780"
  },
  {
    "text": "be able to advance\nyour steps any further. It may converge to minima\nor maxima of a function.",
    "start": "2651780",
    "end": "2658050"
  },
  {
    "text": "Or it may converge to roots. But it will converge. ",
    "start": "2658050",
    "end": "2663290"
  },
  {
    "text": "I showed you this example before\nwith basins of attraction. So here we have different\nbasins of attraction.",
    "start": "2663290",
    "end": "2668690"
  },
  {
    "start": "2666000",
    "end": "3038000"
  },
  {
    "text": "They're all colored in. They show you which\nroots you approach. Here I've applied the\nDamped Newton-Raphson method",
    "start": "2668690",
    "end": "2674680"
  },
  {
    "text": "to the same system of equations. And you can see the\nbasins of attraction are shrunk because\nof the damping.",
    "start": "2674680",
    "end": "2681380"
  },
  {
    "text": "What happens when you're\nvery close to places where the determinant of the\nJacobian is singular,",
    "start": "2681380",
    "end": "2686640"
  },
  {
    "text": "you take all sorts\nof wild steps. You go to places where\nthe value of the function",
    "start": "2686640",
    "end": "2691700"
  },
  {
    "text": "is bigger than\nwhere you started. And then you've got to\nstep down from there to try to find the root.",
    "start": "2691700",
    "end": "2697430"
  },
  {
    "text": "Who knows where\nthose locations are? It's a very complicated,\ngeometrically complicated space",
    "start": "2697430",
    "end": "2702650"
  },
  {
    "text": "that you're moving through. And the Damped\nNewton-Raphson method is forcing the steps\nto always reduce",
    "start": "2702650",
    "end": "2709784"
  },
  {
    "text": "the value of the\nfunction, so they reduce the size of these\nbasins of attraction.",
    "start": "2709784",
    "end": "2715030"
  },
  {
    "text": "So this is often a\nnice way to supplement the Newton-Raphson method\nwhen your guesses aren't",
    "start": "2715030",
    "end": "2720520"
  },
  {
    "text": "very good to begin with. When you start to get close\nto root you're, always just going to accept alpha equals 1.",
    "start": "2720520",
    "end": "2725650"
  },
  {
    "text": "The first step will\nbe the best step, and then you'll converge\nvery rapidly to the solution.",
    "start": "2725650",
    "end": "2730825"
  },
  {
    "text": "Do we have to do any\nextra work actually to do this Damped\nNewton-Raphson method. Does it require\nextra calculations?",
    "start": "2730825",
    "end": "2737310"
  },
  {
    "start": "2737310",
    "end": "2743010"
  },
  {
    "text": "What do you think? A lot of extra-- a lot\nof extra calculation? How many extra calculations\ndoes it require? Of course it requires extra.",
    "start": "2743010",
    "end": "2748990"
  },
  {
    "text": "How many? AUDIENCE: [INAUDIBLE]",
    "start": "2748990",
    "end": "2755076"
  },
  {
    "text": "JAMES SWAN: What do you think? AUDIENCE: [INAUDIBLE]",
    "start": "2755076",
    "end": "2767180"
  },
  {
    "text": "JAMES SWAN: It's--\nthat much is true. So let's talk about\ntaking one step.",
    "start": "2767180",
    "end": "2773160"
  },
  {
    "text": "How many more-- how many\nmore calculations do I have to pay to do\nthis sort of a step? Or even write the\nmultidimensional step?",
    "start": "2773160",
    "end": "2781490"
  },
  {
    "text": " For each of these\ntimes around this loop,",
    "start": "2781490",
    "end": "2787184"
  },
  {
    "text": "do I have to recompute? Do I have to solve the\nsystem of equations?",
    "start": "2787185",
    "end": "2792420"
  },
  {
    "text": "No. Right? You precompute this, right? This is the basic\nNewton-Raphson step. You compute that first.",
    "start": "2792420",
    "end": "2797518"
  },
  {
    "text": "You've got to do it once. And then it's pretty\ncheap after that. I've got to do some extra\nfunction evaluations,",
    "start": "2797518",
    "end": "2803430"
  },
  {
    "text": "but I don't actually have to\nsolve the system of equations. Remember this is order n cubed.",
    "start": "2803430",
    "end": "2809040"
  },
  {
    "text": "If we solve it exactly, maybe\norder n squared or order n if we do it iteratively.",
    "start": "2809040",
    "end": "2815160"
  },
  {
    "text": "And the Jacobian\nis sparse somehow, and we know about\nit sparsity pattern. This is expensive.",
    "start": "2815160",
    "end": "2820320"
  },
  {
    "text": "Function evaluations, those\nare order n to compute. Relatively cheap by comparison.",
    "start": "2820320",
    "end": "2825490"
  },
  {
    "text": "So you compute\nyour initial step. That's expensive. But all of this down\nhere is pretty cheap.",
    "start": "2825490",
    "end": "2833500"
  },
  {
    "text": "Yeah? AUDIENCE: You're also assuming\nthat your function evaluations are reasonably true. JAMES SWAN: This is true. AUDIENCE: [INAUDIBLE]",
    "start": "2833500",
    "end": "2839474"
  },
  {
    "text": "JAMES SWAN: It's true. Well, Jacobian is also very\nexpensive to compute then too. So, if--",
    "start": "2839474",
    "end": "2844830"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] JAMES SWAN: Sure, sure, sure. No, I don't disagree.",
    "start": "2844830",
    "end": "2851210"
  },
  {
    "text": "I think one has to pick\nthe method you're going to use to suit the problem. But turns out this doesn't\ninvolve much extra calculation.",
    "start": "2851210",
    "end": "2857900"
  },
  {
    "text": "So by default, for\nexample, fsolve in MATLAB is going to do this for you. Or some version of this.",
    "start": "2857900",
    "end": "2863450"
  },
  {
    "text": "it's going to try to take\nsteps that aren't too big. It will limit the\nstep size for you, so that it keeps the value\nof the function reducing",
    "start": "2863450",
    "end": "2870830"
  },
  {
    "text": "in magnitude. It's a pretty good\ngeneral strategy. Yes?",
    "start": "2870830",
    "end": "2875997"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] so why\ndo we just pick one value for",
    "start": "2875998",
    "end": "2883468"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "2883468",
    "end": "2888500"
  },
  {
    "text": "JAMES SWAN: I see. So why-- ask that one more time. This is a good question. Can you say it a little\nlouder so everyone can hear?",
    "start": "2888500",
    "end": "2894410"
  },
  {
    "text": "AUDIENCE: So why, instead of\nhaving just one value of alpha and not having several\nvalues of alpha [INAUDIBLE]",
    "start": "2894410",
    "end": "2901700"
  },
  {
    "text": "JAMES SWAN: I see. So the question is, yeah,\nwe used a scalar alpha here,",
    "start": "2901700",
    "end": "2906740"
  },
  {
    "text": "right? If we wanted to, we could\nreduce the step size and also change direction.",
    "start": "2906740",
    "end": "2912259"
  },
  {
    "text": "We would use a matrix to\ndo that, instead, right? It would transform the step\nand change its direction. And maybe we would\nchoose different alphas",
    "start": "2912260",
    "end": "2919183"
  },
  {
    "text": "along different\ndirections, for example. So diagonal matrix\nwith different alphas. We could potentially do that.",
    "start": "2919184",
    "end": "2926150"
  },
  {
    "text": "we're probably going to\nneed some extra information to decide how to set the\nscaling in different directions.",
    "start": "2926150",
    "end": "2935400"
  },
  {
    "text": "One thing we know for sure is\nthat the Newton-Raphson step will reduce the value\nof the function.",
    "start": "2935400",
    "end": "2941550"
  },
  {
    "text": "If we take a small\nenough step size, it will bring the value\nof the function down. We know that because\nwe did the Taylor",
    "start": "2941550",
    "end": "2947869"
  },
  {
    "text": "Expansion of the function\nto determine that step size. And that Taylor expansion\nwas going to be--",
    "start": "2947870",
    "end": "2953950"
  },
  {
    "text": "that Taylor expansion is\nnearly exact in the limit of very, very small step sizes. So there will always\nbe some small step",
    "start": "2953950",
    "end": "2961220"
  },
  {
    "text": "in this direction,\nwhich will reduce the value of the function. In other directions,\nwe may reduce the value",
    "start": "2961220",
    "end": "2967840"
  },
  {
    "text": "of the function faster. We don't know which\ndirections to choose, OK? Actually I shouldn't say that.",
    "start": "2967840",
    "end": "2973810"
  },
  {
    "text": "When we take very small step\nsizes in this direction, it's reducing the value\nof the function fastest. There isn't a faster\ndirection to go in.",
    "start": "2973810",
    "end": "2979480"
  },
  {
    "text": "When we take impossibly small,\nvanishingly small step sizes. But in principle, if I\nhad some extra information",
    "start": "2979480",
    "end": "2986500"
  },
  {
    "text": "on the problem, I might be\nable to choose step sizes along different directions. I may know that one\nof these directions",
    "start": "2986500",
    "end": "2991600"
  },
  {
    "text": "is more ill-behaved\nthan the other ones. And choose a different\ndamping factor for it.",
    "start": "2991600",
    "end": "2997416"
  },
  {
    "text": "That's a possibility. But we actually have\nto know something about the details of\nthe problem we're trying to solve if we're going to do--",
    "start": "2997416",
    "end": "3002611"
  },
  {
    "text": "it's a wonderful question. I mean, you could\nthink about ways of making this more,\npotentially more robust.",
    "start": "3002611",
    "end": "3008015"
  },
  {
    "text": "I'll show you an alternative\nway of doing this when we talk about optimization. In optimization we'll\ndo-- we'll solve",
    "start": "3008016",
    "end": "3014280"
  },
  {
    "text": "systems of nonlinear equations\nto solve these optimization problems. There's another way of doing\nthe same sort of strategy that's",
    "start": "3014280",
    "end": "3020454"
  },
  {
    "text": "more along what\nyou're describing. Maybe there's a\ndifferent direction to choose instead that\ncould be preferable. This is something called\nthe dogleg method.",
    "start": "3020454",
    "end": "3027640"
  },
  {
    "text": "Great question. Anything else? No.",
    "start": "3027640",
    "end": "3032760"
  },
  {
    "text": " So globally convergent, right?",
    "start": "3032760",
    "end": "3039720"
  },
  {
    "start": "3038000",
    "end": "3082000"
  },
  {
    "text": "Converges to roots,\nlocal minima or maxima. There are other modifications\nthat are possible.",
    "start": "3039720",
    "end": "3044820"
  },
  {
    "text": "We'll talk about\nthem in optimization. There's always a\npenalty to doing this. The penalty is in the\nrate of convergence.",
    "start": "3044820",
    "end": "3050550"
  },
  {
    "text": "So it will converge more slowly. But maybe you speed the\ncalculations along anyways, right? Maybe it requires fewer\niterations overall",
    "start": "3050550",
    "end": "3056370"
  },
  {
    "text": "to get there because\nyou tame the locally convergent properties of\nthe Newton-Raphson method. Or you shortcut some of\nthe expensive calculations,",
    "start": "3056370",
    "end": "3065160"
  },
  {
    "text": "like getting your Jacobian\nor calculating your Jacobian inverse. All right?",
    "start": "3065160",
    "end": "3070280"
  },
  {
    "text": "So Monday we're going to review\nsort of topics up til now. Professor Green will run\nthe lecture on Monday. And then after\nthat, we'll pick up",
    "start": "3070280",
    "end": "3076650"
  },
  {
    "text": "with optimization, which will\nfollow right on from what we've done so far. Thanks. ",
    "start": "3076650",
    "end": "3082587"
  }
]