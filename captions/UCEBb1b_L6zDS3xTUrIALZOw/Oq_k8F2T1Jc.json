[
  {
    "start": "0",
    "end": "122000"
  },
  {
    "start": "0",
    "end": "16313"
  },
  {
    "text": "MICHALE FEE: OK. All right, let's go\nahead and get started. OK, so we're going\nto continue talking",
    "start": "16313",
    "end": "21689"
  },
  {
    "text": "about the topic of\nneural networks. Last time, we introduced\na new framework",
    "start": "21690",
    "end": "28619"
  },
  {
    "text": "for thinking about neural\nnetwork interactions, using a rate model to describe\nthe interactions of neurons",
    "start": "28620",
    "end": "37740"
  },
  {
    "text": "and develop a mathematical\nframework for how to combine\ncollections of neurons",
    "start": "37740",
    "end": "43079"
  },
  {
    "text": "to study their behavior. So, last time, we introduced\nthe notion of a perceptron",
    "start": "43080",
    "end": "50760"
  },
  {
    "text": "as a way of building\na neural network that can classify its inputs.",
    "start": "50760",
    "end": "57510"
  },
  {
    "text": "And we started talking about the\nnotion of a perceptron learning",
    "start": "57510",
    "end": "63300"
  },
  {
    "text": "rule, and we're going\nto flesh that idea out in more detail today.",
    "start": "63300",
    "end": "68580"
  },
  {
    "text": "We're going to then talk about\nthe idea of using networks to perform logic with neurons.",
    "start": "68580",
    "end": "77250"
  },
  {
    "text": "We're going to talk about the\nidea of linear separability and invariance. Then we're going to\nintroduce more complex",
    "start": "77250",
    "end": "84240"
  },
  {
    "text": "feed-forward networks,\nwhere instead of having a single\noutput neuron, we have multiple output neurons.",
    "start": "84240",
    "end": "92040"
  },
  {
    "text": "Then we're going to turn to\na more fully developed view",
    "start": "92040",
    "end": "97800"
  },
  {
    "text": "of the math that we use to\ndescribe neural networks, and matrix operations\nbecome extremely important",
    "start": "97800",
    "end": "105450"
  },
  {
    "text": "in neural network theory. And then, finally,\nwe're going to turn",
    "start": "105450",
    "end": "111960"
  },
  {
    "text": "to some of the kinds\nof transformations that are performed by\nmatrix multiplication",
    "start": "111960",
    "end": "118290"
  },
  {
    "text": "and by the kinds of-- by\nfeed-forward neural networks. OK, so we've been considering\na kind of neural network called",
    "start": "118290",
    "end": "128160"
  },
  {
    "text": "a rate model that uses firing\nrates rather than spike trains. So we introduced\nthe idea that we",
    "start": "128160",
    "end": "133440"
  },
  {
    "text": "have an output neuron\nwith firing rate v that receives input\nfrom an input neuron that",
    "start": "133440",
    "end": "139830"
  },
  {
    "text": "has firing rate u. The input neuron synapses\nonto the output neuron with a synapse of weight w.",
    "start": "139830",
    "end": "146490"
  },
  {
    "text": "And we described\nhow we can think of the input neuron producing a\nsynaptic input into the output",
    "start": "146490",
    "end": "154110"
  },
  {
    "text": "neuron that has a\nmagnitude of the firing",
    "start": "154110",
    "end": "159600"
  },
  {
    "text": "rate times the strength of\nthe synaptic connection. So the input to the output\nneuron here is w times u.",
    "start": "159600",
    "end": "168550"
  },
  {
    "text": "And then we talked about how we\ncan convert that input current, let's say, into\nour output neuron",
    "start": "168550",
    "end": "175330"
  },
  {
    "text": "into a firing rate of the output\nneuron through some function f, which is what's called the\nF-I curve of the neuron that",
    "start": "175330",
    "end": "185050"
  },
  {
    "text": "relates the input to the\nfiring rate of the neuron. And we talked about\nseveral different kinds",
    "start": "185050",
    "end": "191260"
  },
  {
    "text": "of F-I firing rate versus input\nfunctions that can be useful. We then extended our network\nfrom a single input neuron",
    "start": "191260",
    "end": "200950"
  },
  {
    "text": "synapsing onto a\nsingle output neuron by having multiple\ninput neurons.",
    "start": "200950",
    "end": "206290"
  },
  {
    "text": "Again, the output neuron\nhas a firing rate, and our input neurons have a\nvector of firing rates now--",
    "start": "206290",
    "end": "214090"
  },
  {
    "text": "u1, u2, u3, u4, and so on-- that we can combine\ntogether into a vector, u.",
    "start": "214090",
    "end": "222940"
  },
  {
    "text": "Each one of those input neurons\nhas a synaptic strength w onto our output neuron.",
    "start": "222940",
    "end": "228469"
  },
  {
    "text": "So we have a vector\nof synaptic strengths. And now we can write down the\ninput current to our output",
    "start": "228470",
    "end": "236590"
  },
  {
    "text": "neuron as a sum of the\ncontributions from each of those input neurons-- so w1,\nu1 plus w2, u2, plus w3, u3,",
    "start": "236590",
    "end": "247150"
  },
  {
    "text": "and so on. So we can now write\nthe input current to our output neuron as\na sum of contributions",
    "start": "247150",
    "end": "256810"
  },
  {
    "text": "that we can then write\nas a dot product-- w dot u. OK, any questions about that?",
    "start": "256810",
    "end": "262930"
  },
  {
    "text": " And so, in general, we have\nthe firing rate of our output",
    "start": "262930",
    "end": "270570"
  },
  {
    "text": "neuron is just\nthis F-I function, this input-output function\nof our output neuron acting",
    "start": "270570",
    "end": "277500"
  },
  {
    "text": "on the total input,\nwhich is w dot u. And then we talked\nabout different kinds",
    "start": "277500",
    "end": "282690"
  },
  {
    "text": "of functions that are\nuseful computationally for this function f.",
    "start": "282690",
    "end": "287819"
  },
  {
    "text": "So in the context of the\nintegrate and fire neuron, we talked about F-I curves that\nare zero below some threshold",
    "start": "287820",
    "end": "296440"
  },
  {
    "text": "and then are linear above\nthat threshold current. We talked last time about\na binary threshold known",
    "start": "296440",
    "end": "305640"
  },
  {
    "text": "that has zero firing\nrate below some threshold and then steps up abruptly to\na constant output firing rate",
    "start": "305640",
    "end": "312390"
  },
  {
    "text": "one. And then we also introduced,\nlast time, the notion of a linear neuron,\nwhose firing rate is",
    "start": "312390",
    "end": "319050"
  },
  {
    "text": "just proportional\nto the input current and has positive and\nnegative firing rates.",
    "start": "319050",
    "end": "324300"
  },
  {
    "text": "And we talked about the\nidea that although it's biophysically implausible\nto have neurons that have negative\nfiring rates, that this",
    "start": "324300",
    "end": "331650"
  },
  {
    "text": "is a particularly useful\nsimplification of neurons. Because we can just\nuse linear algebra",
    "start": "331650",
    "end": "339990"
  },
  {
    "text": "to describe the properties of\nnetworks of linear neurons. And we can do some\nreally interesting things",
    "start": "339990",
    "end": "346980"
  },
  {
    "text": "with that kind of\nmathematical simplification.",
    "start": "346980",
    "end": "352270"
  },
  {
    "text": "We're going to get to\nsome of that today. And that allows\nyou to really build",
    "start": "352270",
    "end": "357419"
  },
  {
    "text": "an intuition for what\nneural networks can do.",
    "start": "357420",
    "end": "362750"
  },
  {
    "text": "OK, so let's come back to what\nperceptron is and introduce",
    "start": "362750",
    "end": "368570"
  },
  {
    "text": "this perceptron learning role. So we talked about the idea\nthat a perceptron carries out",
    "start": "368570",
    "end": "374690"
  },
  {
    "start": "370000",
    "end": "637000"
  },
  {
    "text": "a classification\nof its inputs that represent different features. So we talked about classifying\nanimals into dogs and non-dogs",
    "start": "374690",
    "end": "382580"
  },
  {
    "text": "based on two\nfeatures of animals. We talked about\nthe fact that you",
    "start": "382580",
    "end": "390110"
  },
  {
    "text": "can't make that classification\nbetween dogs and non-dogs just on the basis of\none of those features,",
    "start": "390110",
    "end": "396350"
  },
  {
    "text": "because these two categories\noverlap in this feature and in this feature.",
    "start": "396350",
    "end": "402060"
  },
  {
    "text": "And so in order to properly\nseparate those categories, you need a decision\nboundary that's",
    "start": "402060",
    "end": "407780"
  },
  {
    "text": "actually a combination\nof those two features. And we talked about\nhow you can implement",
    "start": "407780",
    "end": "414290"
  },
  {
    "text": "that using a simple\nnetwork, called a perceptron, that has an output\nneuron and two input neurons.",
    "start": "414290",
    "end": "422570"
  },
  {
    "text": "Each one of those input neurons\nrepresents the magnitude of those two different\nfeatures for each object",
    "start": "422570",
    "end": "430070"
  },
  {
    "text": "that you're trying to classify. So u1 here and u2 are the\ndimensions on which we're",
    "start": "430070",
    "end": "439580"
  },
  {
    "text": "performing this classification. And so we talked about the fact\nthat that decision boundary",
    "start": "439580",
    "end": "448840"
  },
  {
    "text": "between those two\nclassifications is determined by\nthis weight matrix w.",
    "start": "448840",
    "end": "455470"
  },
  {
    "text": "And then we used a\nbinary threshold neuron for making the actual decision. Binary threshold neurons are\ngreat for making decisions,",
    "start": "455470",
    "end": "462370"
  },
  {
    "text": "because unlike a linear\nneuron-- so a linear neuron just responds more if\nits input is larger,",
    "start": "462370",
    "end": "468850"
  },
  {
    "text": "and it responds less if\nits input is smaller. Binary threshold neurons\nhave a very clear threshold",
    "start": "468850",
    "end": "477220"
  },
  {
    "text": "below which the\nneuron doesn't spike and above which the\nneuron does spike. So, in this case, this network,\nthis output neuron here,",
    "start": "477220",
    "end": "484300"
  },
  {
    "text": "will fire, will have\na firing rate of one, for any input that's on this\nside of the decision boundary",
    "start": "484300",
    "end": "491530"
  },
  {
    "text": "and will have a\nfiring rate of zero for any input that's on this\nside of the decision boundary,",
    "start": "491530",
    "end": "496940"
  },
  {
    "text": "OK? All right, so we talked about\nhow we can, in two dimensions,",
    "start": "496940",
    "end": "504560"
  },
  {
    "text": "just write down a decision\nboundary that will separate, let's say, green objects\nfrom red objects.",
    "start": "504560",
    "end": "512870"
  },
  {
    "text": "So you can see that\nif you sat down and you looked at this drawing\nof green dots and red dots,",
    "start": "512870",
    "end": "519770"
  },
  {
    "text": "that it would be very simple\nto just look at that picture and see that if you put\na decision boundary right",
    "start": "519770",
    "end": "526010"
  },
  {
    "text": "there, that you would be able\nto separate the green dots from the red dots.",
    "start": "526010",
    "end": "531350"
  },
  {
    "text": "How would you actually\ncalculate the weight vector that that corresponds\nto in a perceptron?",
    "start": "531350",
    "end": "537029"
  },
  {
    "text": "Well, it's very simple. You can just look at where\nthat decision boundary crosses",
    "start": "537030",
    "end": "542300"
  },
  {
    "text": "the axes-- so you can see here, that\ndecision boundary crosses the u1 axis at point A, crosses\nthe u2 axis at, I should say,",
    "start": "542300",
    "end": "553080"
  },
  {
    "text": "a value of B. And then we can\nuse those numbers to actually calculate the w.",
    "start": "553080",
    "end": "559100"
  },
  {
    "text": "So, remember, u is\nthe input space. w is a weight vector\nthat we're trying",
    "start": "559100",
    "end": "564230"
  },
  {
    "text": "to calculate in order\nto place the decision boundary at that point. Is that clear what\nwe're trying to do here?",
    "start": "564230",
    "end": "572380"
  },
  {
    "text": "OK, so we can calculate\nthat weight vector. We assume that just\ndata is some number.",
    "start": "572380",
    "end": "577709"
  },
  {
    "text": "Let's just call it one. We have an equation for a\nline-- w dot u equals theta.",
    "start": "577710",
    "end": "584760"
  },
  {
    "text": "That's the equation for\nthat decision boundary. We have two knowns, the two\npoints on the decision boundary",
    "start": "584760",
    "end": "592079"
  },
  {
    "text": "that we can just\nread off by eye. And we have two unknowns-- the\nsynaptic weights, w1 and w2.",
    "start": "592080",
    "end": "598020"
  },
  {
    "text": "And so we have two equations-- ua dot w equals theta,\nub dot w equals theta.",
    "start": "598020",
    "end": "606020"
  },
  {
    "text": "And we can just\nsolve for w1 and w2, and that's what you got, OK? So the weight vector that gives\nyou that decision boundary",
    "start": "606020",
    "end": "613560"
  },
  {
    "text": "is 1 over a and 1 over b, OK? Those are the two weights. Any questions about that?",
    "start": "613560",
    "end": "621699"
  },
  {
    "text": "OK. So in two dimensions, that's\nvery easy to do, right?",
    "start": "621700",
    "end": "627630"
  },
  {
    "text": "You can just look at\nthat cloud of points, decide where to draw a line\nthat best separates the two",
    "start": "627630",
    "end": "634589"
  },
  {
    "text": "categories that you're\ninterested in separating. But in higher dimensions,\nthat's really hard.",
    "start": "634590",
    "end": "640870"
  },
  {
    "start": "637000",
    "end": "868000"
  },
  {
    "text": "So in high dimensions,\nfor example, we're trying to separate\nimages, for example.",
    "start": "640870",
    "end": "647720"
  },
  {
    "text": "So we can have a bunch\nof images of dogs, a bunch of images of cats. Each pixel in that\nimage corresponds",
    "start": "647720",
    "end": "654029"
  },
  {
    "text": "to a different input to\nour classification unit. And now how do you decide\nwhat all of those weights",
    "start": "654030",
    "end": "660959"
  },
  {
    "text": "should be from all of\nthose different pixels onto our output neuron that\nseparates images of one class",
    "start": "660960",
    "end": "668760"
  },
  {
    "text": "from images of another class? So there's just no way to do\nthat by eye in high dimensions.",
    "start": "668760",
    "end": "674639"
  },
  {
    "text": "So you need an\nalgorithm that helps you choose that set of\nweights that allows you",
    "start": "674640",
    "end": "680130"
  },
  {
    "text": "to separate different classes-- you know, a bunch of images\nof one class from a bunch",
    "start": "680130",
    "end": "685740"
  },
  {
    "text": "of images of another class. And so we're going to\nintroduce a method called",
    "start": "685740",
    "end": "693540"
  },
  {
    "text": "the perceptron learning rule\nthat is a category of learning",
    "start": "693540",
    "end": "700709"
  },
  {
    "text": "rules called supervised learning\nrules that allow you to take",
    "start": "700710",
    "end": "707910"
  },
  {
    "text": "a bunch of objects that\nyou know-- so if you have a bunch of\npictures of dogs,",
    "start": "707910",
    "end": "713160"
  },
  {
    "text": "you know that they're dogs. If you have a bunch of pictures\nof cats, you know they're cats. So you label those images.",
    "start": "713160",
    "end": "718920"
  },
  {
    "text": "You feed those inputs, those\nimages, into your network, and you tell the network\nwhat the answer was.",
    "start": "718920",
    "end": "726870"
  },
  {
    "text": "And through an\niterative process, it finds all of the weights that\noptimally separate those two",
    "start": "726870",
    "end": "733410"
  },
  {
    "text": "different categories. So that's called the\nperceptron learning rule. So let me just set up\nhow that actually works.",
    "start": "733410",
    "end": "739240"
  },
  {
    "text": "So you have a bunch of\nobservations of the input. So in this case, I'm drawing\nthese in two dimensions,",
    "start": "739240",
    "end": "745960"
  },
  {
    "text": "but you should think about each\none of these dots as being, let's say, an image of a\ndog in very high dimensions,",
    "start": "745960",
    "end": "752520"
  },
  {
    "text": "where instead of just u1 and\nu2, you have u1 through u1000,",
    "start": "752520",
    "end": "757920"
  },
  {
    "text": "where each one of those is\nthe value of a different pixel in your image.",
    "start": "757920",
    "end": "764190"
  },
  {
    "text": "So you have a bunch of images. Each one of those corresponds\nto an image of a dog.",
    "start": "764190",
    "end": "770220"
  },
  {
    "text": "Each one of those corresponds\nto an image of a cat. And we have a whole bunch\nof different observations",
    "start": "770220",
    "end": "776279"
  },
  {
    "text": "or images of those\ndifferent categories. Any questions about that? ",
    "start": "776280",
    "end": "783800"
  },
  {
    "text": "All right, so we have n\nof those observations. And for each one of\nthose observations,",
    "start": "783800",
    "end": "788880"
  },
  {
    "text": "we say that the\ninput is equal to one of those observations for one\niteration of this learning",
    "start": "788880",
    "end": "795930"
  },
  {
    "text": "process, OK? And so with each\nobservation, we're told whether this\ninput corresponds",
    "start": "795930",
    "end": "801810"
  },
  {
    "text": "to one category or another,\nso a dog or a non-dog. And our output, we're asking--",
    "start": "801810",
    "end": "807959"
  },
  {
    "text": "we want to choose\nthis set of weights such that the output\nof our network is equal to some known value.",
    "start": "807960",
    "end": "817680"
  },
  {
    "text": "So t sub i, where if it's a dog,\nthen the answer is one for yes.",
    "start": "817680",
    "end": "823410"
  },
  {
    "text": "If it's a non-dog, the answer\nis no for that's not a dog.",
    "start": "823410",
    "end": "828449"
  },
  {
    "text": "And we have n of those answers. We have n images and labels\nthat tell us what category",
    "start": "828450",
    "end": "836760"
  },
  {
    "text": "that image belongs to. So for all of\nthese, t equals one. For all of these, t equals zero.",
    "start": "836760",
    "end": "843300"
  },
  {
    "text": "And we want to find\na set of weights such that when we take the dot\nproduct of that weight factor",
    "start": "843300",
    "end": "850020"
  },
  {
    "text": "into each one of those\nobservations minus theta",
    "start": "850020",
    "end": "857970"
  },
  {
    "text": "that we get an answer\nthat is equal to t",
    "start": "857970",
    "end": "863339"
  },
  {
    "text": "for each observation. Does that make sense?",
    "start": "863340",
    "end": "868360"
  },
  {
    "start": "868000",
    "end": "1016000"
  },
  {
    "text": "So how do we do that? All right, so each observation,\nwe have two things--",
    "start": "868360",
    "end": "877240"
  },
  {
    "text": "the input and the\ndesired output. And that gives us\ninformation that we",
    "start": "877240",
    "end": "883150"
  },
  {
    "text": "can use to construct\nthis weight vector. So, again, that's called\nsupervised learning. And we're going to use an\nupdate rule, or a learning rule,",
    "start": "883150",
    "end": "892300"
  },
  {
    "text": "that allows us to\nchange the weight vector on as a result\nof each estimate,",
    "start": "892300",
    "end": "898180"
  },
  {
    "text": "depending on whether we got\nthe answer right or not. So how do we do this? What we're going to\ndo is we're going",
    "start": "898180",
    "end": "903912"
  },
  {
    "text": "to start with a random set\nof weights, w1 and w2, OK? And we're going to\nput in an input.",
    "start": "903912",
    "end": "911579"
  },
  {
    "text": "So there's a space of inputs. We're going to start\nwith some random weight, and I started with some random\nvector in this direction.",
    "start": "911580",
    "end": "918230"
  },
  {
    "text": "You can see that that gives you\na classification boundary here. And you can see that that\nclassification boundary is not",
    "start": "918230",
    "end": "924339"
  },
  {
    "text": "very good for separating the\ngreen dots from the red dots. Why? Because it will assign\na one to everything",
    "start": "924340",
    "end": "931060"
  },
  {
    "text": "on this side of that\ndecision boundary and a zero to everything on that side. But you can see\nthat that does not",
    "start": "931060",
    "end": "936520"
  },
  {
    "text": "correspond to the\nassignment of green and red to each of those dots, OK? So how do we update that w in\norder to get the right answer?",
    "start": "936520",
    "end": "947523"
  },
  {
    "text": "So what we're going\nto do is we're going to put in one of these\ninputs on each iteration",
    "start": "947523",
    "end": "953710"
  },
  {
    "text": "and ask whether the network\ngot the answer right or not. So we're going to put\nin one of those inputs.",
    "start": "953710",
    "end": "962610"
  },
  {
    "text": "So let's pick that\ninput right there. We're going to put\nthat into our network. And we see that the answer\nwe get from the network",
    "start": "962610",
    "end": "969730"
  },
  {
    "text": "is one, because it's on the\npositive side of the decision",
    "start": "969730",
    "end": "974769"
  },
  {
    "text": "boundary. And so one was the right\nanswer in this case. So what do we do?",
    "start": "974770",
    "end": "979840"
  },
  {
    "text": "We don't do anything. We say the change in weight is\ngoing to be zero if we already",
    "start": "979840",
    "end": "985270"
  },
  {
    "text": "get the right answer. So if we got lucky and\nour initial weight vector was in the right direction,\nso our perceptron",
    "start": "985270",
    "end": "992260"
  },
  {
    "text": "already classified\nthe answer, then the weight vector is\nnever going to change, because it was already\nthe right answer.",
    "start": "992260",
    "end": "999400"
  },
  {
    "text": "OK, so let's put it\nin another input-- a red input. You can see that the\ncorrect answer is a zero.",
    "start": "999400",
    "end": "1005970"
  },
  {
    "text": "The network gave us\na zero, because it's on the negative side of the\nweight vector of the decision",
    "start": "1005970",
    "end": "1013380"
  },
  {
    "text": "boundary. And so, again, delta w is zero. But let's put in\nanother input now such",
    "start": "1013380",
    "end": "1018780"
  },
  {
    "text": "that we get the wrong answer. So let's put in this\ninput right here. So you can see that the answer\nhere, the correct answer",
    "start": "1018780",
    "end": "1026339"
  },
  {
    "text": "is one, but the network is\ngoing to give us a zero.",
    "start": "1026339",
    "end": "1032339"
  },
  {
    "text": "So what do we do to\nupdate that weight vector? So if the output is not\nequal to the correct answer,",
    "start": "1032339",
    "end": "1039329"
  },
  {
    "text": "then we're wrong. So now we update w. And the perceptron learning\nrule is very simple.",
    "start": "1039329",
    "end": "1046140"
  },
  {
    "text": "We introduce a change in\nw that looks like this. It's a little change, so\neps eta is a learning rate.",
    "start": "1046140",
    "end": "1055620"
  },
  {
    "text": "It's generally going\nto be smaller than one. So we're going to put in\na small change in w that's",
    "start": "1055620",
    "end": "1063510"
  },
  {
    "text": "in the direction of the\ninput that was wrong if the correct answer is a one.",
    "start": "1063510",
    "end": "1071580"
  },
  {
    "text": "We're going to\nmake a small change to w in the opposite\ndirection of that input",
    "start": "1071580",
    "end": "1077910"
  },
  {
    "text": "if the correct answer was zero. Does that make sense? So we're going to\nchange w in a way that",
    "start": "1077910",
    "end": "1086430"
  },
  {
    "text": "depends on what the\ninput was and what",
    "start": "1086430",
    "end": "1091930"
  },
  {
    "text": "the correct answer was. ",
    "start": "1091930",
    "end": "1096970"
  },
  {
    "text": "So let's walk through this. So we put it in an input here. The correct answer is a one,\nand we got the answer wrong.",
    "start": "1096970",
    "end": "1105130"
  },
  {
    "text": "The network gave us a zero, but\nthe correct answer is a one. So we're in this region here.",
    "start": "1105130",
    "end": "1111880"
  },
  {
    "text": "The answer was incorrect,\nso we're going to update w. The correct answer was a one,\nso we're going to change delta--",
    "start": "1111880",
    "end": "1118299"
  },
  {
    "text": "we're going to change w in\nthe direction of that input. So that input is there.",
    "start": "1118300",
    "end": "1123760"
  },
  {
    "text": "So we're going to add a little\nbit to w in this direction.",
    "start": "1123760",
    "end": "1130530"
  },
  {
    "text": "So if we add that little\nbit of vector to the w, it's going to move the w vector\nin this direction, right?",
    "start": "1130530",
    "end": "1138280"
  },
  {
    "text": "So let's do that. So there's our new w. Our new w is the\nold plus delta w,",
    "start": "1138280",
    "end": "1145310"
  },
  {
    "text": "which is in the direction\nof this incorrectly classified input.",
    "start": "1145310",
    "end": "1151880"
  },
  {
    "text": "So there's our new decision\nboundary, all right? And let's put in another input--",
    "start": "1151880",
    "end": "1158340"
  },
  {
    "text": "let's say this one right here. You can see that this input is\nalso incorrectly classified,",
    "start": "1158340",
    "end": "1163610"
  },
  {
    "text": "because the correct\nanswer is a zero. It's a red dot. But the network since\nit's on the positive side",
    "start": "1163610",
    "end": "1170800"
  },
  {
    "text": "of the decision boundary. So the network\nclassifies it as a one. OK, good. So the network classified it\nas a one and the correct answer",
    "start": "1170800",
    "end": "1179049"
  },
  {
    "text": "was a zero, so we were wrong. So we're going to\nupdate w, and we're going to update it in the\nopposite direction of the input",
    "start": "1179050",
    "end": "1187060"
  },
  {
    "text": "if the correct answer was\nzero, which is the case. So we're going to update w.",
    "start": "1187060",
    "end": "1193360"
  },
  {
    "text": "And that's the input xi. Minus xi is in this direction.",
    "start": "1193360",
    "end": "1199310"
  },
  {
    "text": "So we're going to update\nw in that direction. So we're going to add those\ntwo vectors to get our new w.",
    "start": "1199310",
    "end": "1206530"
  },
  {
    "text": "And when we do that,\nthat's what we get. There's our new w. There's our new\ndecision boundary.",
    "start": "1206530",
    "end": "1212360"
  },
  {
    "text": "And you can see that that\ndecision boundary is now perfectly oriented to separate\nthe red and the green dots.",
    "start": "1212360",
    "end": "1222159"
  },
  {
    "text": "So that's Rosenblatt's\nperceptron learning rule. Yes, Rebecca? AUDIENCE: How do you\nchange the learning rate?",
    "start": "1222160",
    "end": "1229100"
  },
  {
    "text": "Because what if it's too big? You'll sort of get not\nhelpful [INAUDIBLE].. MICHALE FEE: Yeah, that's right.",
    "start": "1229100",
    "end": "1234400"
  },
  {
    "text": "So if the learning\nrate were too big, you could see this\nfirst correction. So let's say that we corrected\nw but made a correction that",
    "start": "1234400",
    "end": "1241929"
  },
  {
    "text": "was too far in this direction. So now the new w\nwould point up here.",
    "start": "1241930",
    "end": "1248350"
  },
  {
    "text": "And that would give us,\nagain, the wrong answer. What happens, generally,\nis that if your learning rate is too high, then your\nweight vector bounces around.",
    "start": "1248350",
    "end": "1259810"
  },
  {
    "text": "It oscillates around. So it'll jump too far\nthis way, and then it'll get an error\nover here, and it'll",
    "start": "1259810",
    "end": "1266530"
  },
  {
    "text": "jump too far that way. And then you'll get\nan error over there, and it'll just keep\nbouncing back and forth. So you generally\nchoose learning rates",
    "start": "1266530",
    "end": "1273460"
  },
  {
    "text": "that-- the process of\nchoosing learning rates can be a little\ntricky Basically,",
    "start": "1273460",
    "end": "1278500"
  },
  {
    "text": "the answer is start small and\nincrease it until it breaks. ",
    "start": "1278500",
    "end": "1286780"
  },
  {
    "text": "OK, any questions about that?  So you can see it's a\nvery simple algorithm that",
    "start": "1286780",
    "end": "1296430"
  },
  {
    "text": "provides a way of changing w\nthat is guaranteed to converge toward the best answer\nin separating these two",
    "start": "1296430",
    "end": "1305400"
  },
  {
    "text": "classes of inputs. ",
    "start": "1305400",
    "end": "1312270"
  },
  {
    "text": "All right, so let's go\na little bit further into single layer\nbinary networks",
    "start": "1312270",
    "end": "1319770"
  },
  {
    "text": "and see what they can do. So these kinds of networks\nare very good for actually",
    "start": "1319770",
    "end": "1326100"
  },
  {
    "text": "implementing logic operations. So you can see that-- let's say\nthat we have a perceptron that looks like this.",
    "start": "1326100",
    "end": "1332110"
  },
  {
    "text": "Let's give it a threshold\nof 0.5 and give it",
    "start": "1332110",
    "end": "1337210"
  },
  {
    "text": "a weight vector that's 1 and 1. So you can see that\nthis perceptron",
    "start": "1337210",
    "end": "1344710"
  },
  {
    "text": "gives an answer of zero. The output neuron\nhas zero firing rate for an input that's zero.",
    "start": "1344710",
    "end": "1352320"
  },
  {
    "text": "But any input that's on the\nother side of the decision",
    "start": "1352320",
    "end": "1358009"
  },
  {
    "text": "boundary produces an\noutput firing rate of one. What that means is that if\nthe input a, or u1, is a 1,",
    "start": "1358010",
    "end": "1370250"
  },
  {
    "text": "0, then the output\nneuron will fire. If the input is 0, 1, the\noutput neuron will fire.",
    "start": "1370250",
    "end": "1377720"
  },
  {
    "text": "And if the input is 1, 1,\nthe output neuron will fire. So, basically, any input\nabove some threshold",
    "start": "1377720",
    "end": "1387610"
  },
  {
    "text": "will make the\noutput neuron fire. So this perceptron\nimplements an OR gate.",
    "start": "1387610",
    "end": "1393600"
  },
  {
    "text": "If it's input a or input\nb, the output neuron spikes, as long as those inputs\nare above some threshold value.",
    "start": "1393600",
    "end": "1402330"
  },
  {
    "text": "So that's very much\nlike a logical OR gate. ",
    "start": "1402330",
    "end": "1408130"
  },
  {
    "text": "Now let's see if we can\nimplement an AND gate. So it turns out that\nimplementing an AND gate",
    "start": "1408130",
    "end": "1413340"
  },
  {
    "text": "is almost exactly\nlike an OR gate. We just need-- what would\nwe change about this network",
    "start": "1413340",
    "end": "1420420"
  },
  {
    "text": "to implement an AND gate? AUDIENCE: A larger [INAUDIBLE]. MICHALE FEE: What's that? AUDIENCE: A larger theta?",
    "start": "1420420",
    "end": "1425760"
  },
  {
    "text": "MICHALE FEE: Yeah,\na larger theta. So all we have to do is\nmove this line up to here.",
    "start": "1425760",
    "end": "1432670"
  },
  {
    "text": "And now one of\nthose inputs is not enough to make the\noutput neuron fire.",
    "start": "1432670",
    "end": "1437830"
  },
  {
    "text": "The other input is not enough\nto make the output neuron fire. Only when you have both. So that implements an AND gate.",
    "start": "1437830",
    "end": "1444520"
  },
  {
    "text": "We just increase the\nthreshold a little bit. Does that make sense?",
    "start": "1444520",
    "end": "1449950"
  },
  {
    "text": "So we just increase the\nthreshold here to 1.5. And now when either input\nis on at a value of one,",
    "start": "1449950",
    "end": "1457870"
  },
  {
    "text": "that's not enough to make\nthe output neuron fire. If this input's on,\nit's not enough. If that output is\non, it's not enough.",
    "start": "1457870",
    "end": "1465790"
  },
  {
    "text": "Only when both inputs are\non do you get enough input to this output neuron to make\nit have a non-zero firing rate,",
    "start": "1465790",
    "end": "1473010"
  },
  {
    "text": "to get it above threshold. Now, there's another very common\nlogic operation that cannot be",
    "start": "1473010",
    "end": "1482080"
  },
  {
    "text": "solved by a simple perceptron. That's called an\nexclusive OR, where",
    "start": "1482080",
    "end": "1491680"
  },
  {
    "text": "this neuron, this\nnetwork, we want it to fire only if input a is on\nor input b is on, but not both.",
    "start": "1491680",
    "end": "1505890"
  },
  {
    "text": "Why is it that that\ncan't be solved by the kind of perceptron\nthat we've been describing?",
    "start": "1505890",
    "end": "1512010"
  },
  {
    "text": "Anybody have some\nintuition about that? ",
    "start": "1512010",
    "end": "1520022"
  },
  {
    "text": "AUDIENCE: I mean, it's\nobviously [INAUDIBLE] separable. MICHALE FEE: Yeah, that's right. The keyword there is separable.",
    "start": "1520022",
    "end": "1527320"
  },
  {
    "text": "If you look at this set of\ndots, there's no single line,",
    "start": "1527320",
    "end": "1533210"
  },
  {
    "text": "there's no single boundary\nthat separates all the red dots from off the green dots, OK?",
    "start": "1533210",
    "end": "1540940"
  },
  {
    "text": "And so that set of inputs\nis called non-separable. And sets of inputs that are not\nseparable cannot be classified",
    "start": "1540940",
    "end": "1552700"
  },
  {
    "text": "correctly by a simple perceptron\nof the type we've been talking",
    "start": "1552700",
    "end": "1558159"
  },
  {
    "text": "about. So how do you\nsolve that problem? So this is a set of inputs\nthat's non-separable.",
    "start": "1558160",
    "end": "1566132"
  },
  {
    "text": "You can see that you can\nsolve this problem now if you have two\nseparate perceptrons.",
    "start": "1566132",
    "end": "1571309"
  },
  {
    "text": "So watch this. We can build one\nperceptive one that fires, that has a positive output\nwhen this input is on.",
    "start": "1571310",
    "end": "1581590"
  },
  {
    "text": "We can have a separate\nperceptron that is active when that input is on.",
    "start": "1581590",
    "end": "1589300"
  },
  {
    "text": "And then what would we do? If we had one\nneuron that's active if that input is\non another input",
    "start": "1589300",
    "end": "1595990"
  },
  {
    "text": "that's active when\nthat input is on?  We would or them\ntogether, that's right.",
    "start": "1595990",
    "end": "1603260"
  },
  {
    "text": "So this is what's known as\na multi-layer perceptron. We have two inputs, one\nthat represents activity",
    "start": "1603260",
    "end": "1610040"
  },
  {
    "text": "in a, another that\nrepresents activity in b. And we have one neuron\nin what's called",
    "start": "1610040",
    "end": "1617840"
  },
  {
    "text": "the intermediate layer\nof our perceptron that has a weight\nvector of 1 minus 1.",
    "start": "1617840",
    "end": "1624929"
  },
  {
    "text": "What that means is this neuron\nwill be active if input a is on but not input b.",
    "start": "1624930",
    "end": "1634750"
  },
  {
    "text": "This one will be active. This neuron has a different\nweight vector-- minus 1, 1.",
    "start": "1634750",
    "end": "1640576"
  },
  {
    "text": "This neuron will be active if\ninput b is on but not input a.",
    "start": "1640576",
    "end": "1647770"
  },
  {
    "text": " And the output neuron\nimplements an OR operation",
    "start": "1647770",
    "end": "1654120"
  },
  {
    "text": "that will be active when this\nintermediate neuron is on or that intermediate\nneuron is on, OK?",
    "start": "1654120",
    "end": "1662820"
  },
  {
    "text": "And so that network altogether\nimplements this exclusive OR function.",
    "start": "1662820",
    "end": "1668549"
  },
  {
    "text": "Does that make sense? Any questions about that? ",
    "start": "1668550",
    "end": "1676690"
  },
  {
    "text": "So this problem\nof separability is extremely important in\nclassifying inputs in general.",
    "start": "1676690",
    "end": "1685820"
  },
  {
    "text": "So if you think about\nclassifying an image,",
    "start": "1685820",
    "end": "1691419"
  },
  {
    "text": "like a number or\na letter, you can see that in high-dimensional\nspace, images",
    "start": "1691420",
    "end": "1701429"
  },
  {
    "text": "that are all threes,\nlet's say, are all",
    "start": "1701430",
    "end": "1708590"
  },
  {
    "text": "very similar to each other. But they're actually not\nseparable in this linear space.",
    "start": "1708590",
    "end": "1714000"
  },
  {
    "text": "And that's because in the\nhigh dimensional space they exist on what's\ncalled a manifold",
    "start": "1714000",
    "end": "1720920"
  },
  {
    "text": "in this high-dimensional\nspace, OK? They're like all lined\nup on some sheet, OK?",
    "start": "1720920",
    "end": "1728180"
  },
  {
    "text": "So this is an\nexample of rotations, and you can see that all these\ndifferent threes kind of sit",
    "start": "1728180",
    "end": "1734930"
  },
  {
    "text": "along a manifold in this\nhigh-dimensional space that are separate from all\nthe other numbers.",
    "start": "1734930",
    "end": "1741605"
  },
  {
    "text": " So all those numbers\nexist on what's",
    "start": "1741605",
    "end": "1748310"
  },
  {
    "text": "called an invariant\ntransformation, OK? Now, how would we\nseparate those images",
    "start": "1748310",
    "end": "1756600"
  },
  {
    "text": "of threes from all the\nother numbers or letters?",
    "start": "1756600",
    "end": "1762059"
  },
  {
    "text": "How would we do that? Well, we could imagine building\na multi-layer perceptron that--",
    "start": "1762060",
    "end": "1770034"
  },
  {
    "text": "so here, I'm\nshowing that there's no single line that separates\nthe threes on this manifold",
    "start": "1770035",
    "end": "1775040"
  },
  {
    "text": "from all the other\ndigits over here. We can solve that\nproblem by implementing",
    "start": "1775040",
    "end": "1780650"
  },
  {
    "text": "a multi-layer perceptron that\nwhile one of those perceptrons detects these objects,\nanother perceptron detects",
    "start": "1780650",
    "end": "1789140"
  },
  {
    "text": "these objects, and then we\ncan OR those all together. So that's a kind of\nnetwork that can now",
    "start": "1789140",
    "end": "1798380"
  },
  {
    "text": "detect all of these three,\nseparate them from non-threes.",
    "start": "1798380",
    "end": "1803990"
  },
  {
    "text": "Does that make sense? So we can think of objects that\nwe recognize, like this three",
    "start": "1803990",
    "end": "1810520"
  },
  {
    "text": "that we recognize, even\nthough it has different-- we can recognize it\nwith different rotations or transformations\nor scale changes.",
    "start": "1810520",
    "end": "1820730"
  },
  {
    "text": "You can also think of the\nproblem of separating images from dogs and cats as\nalso solving this problem,",
    "start": "1820730",
    "end": "1828250"
  },
  {
    "text": "that the space of\ndogs, of dog images, somehow lives on a manifold\nin the high dimensional space",
    "start": "1828250",
    "end": "1836679"
  },
  {
    "text": "of inputs that we\ncan distinguish from the set of\nimages of cats that's",
    "start": "1836680",
    "end": "1843070"
  },
  {
    "text": "some other manifold in this\nhigh-dimensional space.",
    "start": "1843070",
    "end": "1848570"
  },
  {
    "text": "So it turns out that you need\nmore than just a single layer",
    "start": "1848570",
    "end": "1853789"
  },
  {
    "text": "perceptron. You need more than just\na two-layer perceptron. In general, the\nkinds of networks",
    "start": "1853790",
    "end": "1859820"
  },
  {
    "text": "that are good for separating\ndifferent kinds of images, like dogs and cats and\ncars and houses and faces,",
    "start": "1859820",
    "end": "1866240"
  },
  {
    "start": "1866000",
    "end": "2347000"
  },
  {
    "text": "look more like this. So this is work from\nJim DiCarlo's lab,",
    "start": "1866240",
    "end": "1871250"
  },
  {
    "text": "where they found evidence that\nnetworks in the brain that do",
    "start": "1871250",
    "end": "1876770"
  },
  {
    "text": "image classification--\nfor example, in the visual pathway-- look a lot like very deep\nneural networks, where",
    "start": "1876770",
    "end": "1885690"
  },
  {
    "text": "you have the retina on the\nleft side here sending inputs",
    "start": "1885690",
    "end": "1891419"
  },
  {
    "text": "to another letter\nin the thalamus, sending inputs to v1, to v2,\nto v4, and so on, up to IT.",
    "start": "1891420",
    "end": "1900300"
  },
  {
    "text": "And that we can think\nof this as being, essentially, many stacked\nlayers of perceptrons",
    "start": "1900300",
    "end": "1908100"
  },
  {
    "text": "that sort of unravel\nthese manifolds in this high-dimensional\nspace to allow",
    "start": "1908100",
    "end": "1914550"
  },
  {
    "text": "neurons here at the very\nend to separate dogs from cats from\nbuildings from faces.",
    "start": "1914550",
    "end": "1922065"
  },
  {
    "text": " And there are\nlearning rules that can be used to train\nnetworks like this",
    "start": "1922065",
    "end": "1929310"
  },
  {
    "text": "by putting in a bunch of\ndifferent images of people",
    "start": "1929310",
    "end": "1934440"
  },
  {
    "text": "and other different\ncategories that you might want to separate. And then each one\nof those images",
    "start": "1934440",
    "end": "1939720"
  },
  {
    "text": "has a label, just like our\nperceptron learning rule. And we can use the image\nand the correct label--",
    "start": "1939720",
    "end": "1947010"
  },
  {
    "text": "face or dog-- and\ntrain that network",
    "start": "1947010",
    "end": "1952640"
  },
  {
    "text": "by projecting that information\ninto these intermediate layers",
    "start": "1952640",
    "end": "1958560"
  },
  {
    "text": "to train that network\nto properly classify those different stimuli, OK? This is, basically,\nthe kind of technology",
    "start": "1958560",
    "end": "1967770"
  },
  {
    "text": "that's currently\nbeing used to train-- this is being used in AI.",
    "start": "1967770",
    "end": "1973470"
  },
  {
    "text": "It's being used to\ntrain driverless cars. All kinds of\ntechnological advances",
    "start": "1973470",
    "end": "1982350"
  },
  {
    "text": "are based on this kind\nof technology here. Any questions about that? Aditi?",
    "start": "1982350",
    "end": "1988054"
  },
  {
    "text": "AUDIENCE: So in\nactual neurons, I assume it's not linear, right? MICHALE FEE: Yes.",
    "start": "1988054",
    "end": "1994230"
  },
  {
    "text": "These are all nonlinear neurons. They're more like these\nbinary threshold units",
    "start": "1994230",
    "end": "1999960"
  },
  {
    "text": "than they are like\nlinear neurons. That's right. AUDIENCE: But then do\nyou there's, like--",
    "start": "1999960",
    "end": "2005795"
  },
  {
    "text": "because right now, I\nimagine that models we make have to have way more\nperceptron units. MICHALE FEE: Yes.",
    "start": "2005795",
    "end": "2011190"
  },
  {
    "text": "AUDIENCE: We use our\nsimplified [INAUDIBLE].. But then our brain\nis sometimes-- I mean, it's at, like,\na much faster level,",
    "start": "2011190",
    "end": "2018610"
  },
  {
    "text": "like way faster, right? So you think it'd be like--\nif we examine what functions",
    "start": "2018610",
    "end": "2026000"
  },
  {
    "text": "neurons might be using, in a\nway that would let us reduce the number of units needed?",
    "start": "2026000",
    "end": "2031759"
  },
  {
    "text": "Because right now, for\nexample, [INAUDIBLE] be a bunch of lines. But maybe in the brain, there's\nsome other function it's using,",
    "start": "2031760",
    "end": "2038690"
  },
  {
    "text": "which is smoother. MICHALE FEE: Yeah. OK, so let me just\nmake sure I understand.",
    "start": "2038690",
    "end": "2044330"
  },
  {
    "text": "You're not talking about the\nF-I curve of the neurons? Is that correct?",
    "start": "2044330",
    "end": "2049540"
  },
  {
    "text": "You're talking about the\nway that you figure out these weights. Is that what you're\nasking about?",
    "start": "2049540",
    "end": "2054888"
  },
  {
    "text": "AUDIENCE: No. I'm asking if we use a\nmore accurate F-I curve,",
    "start": "2054889",
    "end": "2060033"
  },
  {
    "text": "we'll need less units. MICHALE FEE: OK, so\nthat's a good question. I don't actually know the\nanswer to the question",
    "start": "2060034",
    "end": "2066230"
  },
  {
    "text": "of how the specific\nchoice of F-I curve affects the performance of this.",
    "start": "2066230",
    "end": "2071658"
  },
  {
    "text": "The big problem that people\nare trying to figure out in terms of how\nthese are trained",
    "start": "2071659",
    "end": "2079488"
  },
  {
    "text": "is the challenge that in\norder to train these networks, you actually need thousands\nand thousands, maybe millions,",
    "start": "2079489",
    "end": "2087419"
  },
  {
    "text": "of examples of different objects\nhere and the answer here.",
    "start": "2087420",
    "end": "2094138"
  },
  {
    "text": "So you have to put\nin many thousands of example images and\nthe answer in order",
    "start": "2094139",
    "end": "2100620"
  },
  {
    "text": "to train these networks. And that's not the way\npeople actually learn.",
    "start": "2100620",
    "end": "2106080"
  },
  {
    "text": "We don't walk around the\nworld when we're one-year-old and our mother saying,\ndog, cat, person, house.",
    "start": "2106080",
    "end": "2112550"
  },
  {
    "text": "You know, it would be... in\norder to give a person as many labeled examples as you\nneed to give these networks,",
    "start": "2112550",
    "end": "2119069"
  },
  {
    "text": "you would just be doing nothing,\nbut your parents would be pointing things out to you and\ntelling you one-word answers",
    "start": "2119070",
    "end": "2127770"
  },
  {
    "text": "of what those are. Instead, what happens is\nwe just observe the world and figure out\nkind of categories",
    "start": "2127770",
    "end": "2134970"
  },
  {
    "text": "based on other sorts of learning\nrules that are unsupervised. We figure out, oh, that's a kind\nof thing, and then mom says,",
    "start": "2134970",
    "end": "2140610"
  },
  {
    "text": "that's a dog. And then we know that\nthat category is a dog. And we sometimes\nmake mistakes, right?",
    "start": "2140610",
    "end": "2147510"
  },
  {
    "text": "Like a kid might look\nat a bear and say, dog.",
    "start": "2147510",
    "end": "2152820"
  },
  {
    "text": "And then dad says, no,\nno, that's not a dog, son. ",
    "start": "2152820",
    "end": "2159930"
  },
  {
    "text": "So the learning by which\npeople train their networks to do classification\nof inputs is",
    "start": "2159930",
    "end": "2166560"
  },
  {
    "text": "quite different from the way\nthese deep neural networks work. And that's a very important\nand active area of research.",
    "start": "2166560",
    "end": "2175340"
  },
  {
    "text": "Yes? AUDIENCE: Is the fact that\n[INAUDIBLE] use unsupervised learning, as well,\nto train a computer",
    "start": "2175340",
    "end": "2182690"
  },
  {
    "text": "to recognize an image\nof a turtle as a gun, but humans can't do\nthat [INAUDIBLE]..",
    "start": "2182690",
    "end": "2188040"
  },
  {
    "text": "MICHALE FEE: Recognize\na turtle if what? AUDIENCE: Like I saw this\nthing where it was like at MIT, they used an AI.",
    "start": "2188040",
    "end": "2193910"
  },
  {
    "text": "They manipulated\npixels in images and convinced the computer\nthat it was something that it was not actually.",
    "start": "2193910",
    "end": "2199170"
  },
  {
    "text": "MICHALE FEE: I see. Yeah. AUDIENCE: So like you would\nsee a picture of a turtle, but the computer\nwould get that picture and say it was,\nlike, a machine gun.",
    "start": "2199170",
    "end": "2205200"
  },
  {
    "text": "MICHALE FEE: Just by\nmanipulating a few pixels and kind of screwing\nwith its mind. AUDIENCE: Yes. So it's [INAUDIBLE].",
    "start": "2205200",
    "end": "2210990"
  },
  {
    "text": " MICHALE FEE: Yeah. Well, people can be tricked\nby different things.",
    "start": "2210990",
    "end": "2217722"
  },
  {
    "text": " The answer is, yes,\nit's related to that.",
    "start": "2217722",
    "end": "2225490"
  },
  {
    "text": "The problem is after\nyou do this training, we actually don't\nreally understand what's going on in the\nguts of this network.",
    "start": "2225490",
    "end": "2234089"
  },
  {
    "text": "It's very hard to look at\nthe inside of this network after it's trained and\nunderstand what it's doing.",
    "start": "2234090",
    "end": "2242089"
  },
  {
    "text": "And so we don't\nknow the answer why it is that you can fool\none of these networks",
    "start": "2242090",
    "end": "2248569"
  },
  {
    "text": "by changing a few pixels. Something goes wrong in here,\nand we don't know what it is. It may very well have to do\nwith the way it's trained,",
    "start": "2248570",
    "end": "2255920"
  },
  {
    "text": "rather than building categories\nin an unsupervised way, which",
    "start": "2255920",
    "end": "2261829"
  },
  {
    "text": "could be much more\ngeneralizable. So good question. I don't really know the answer.",
    "start": "2261830",
    "end": "2267340"
  },
  {
    "text": " Yes? AUDIENCE: Sorry,\ncan you explain what",
    "start": "2267340",
    "end": "2272372"
  },
  {
    "text": "you mean [INAUDIBLE] the\nneural network needs an answer? They're not categorized and\nthen tell the user dogs?",
    "start": "2272372",
    "end": "2280310"
  },
  {
    "text": "MICHALE FEE: Yeah,\nso no, in order to train one of these networks,\nyou have to give it a data set,",
    "start": "2280310",
    "end": "2285390"
  },
  {
    "text": "a labeled data set. So a set of images that\nalready has the answer",
    "start": "2285390",
    "end": "2291269"
  },
  {
    "text": "that was labeled by a person. AUDIENCE: So you\ncan't just give it",
    "start": "2291270",
    "end": "2296710"
  },
  {
    "text": "a set of photos of\npuppies and snakes and it'll categorize\nthem into two groups? MICHALE FEE: No, nobody\nknows how to do that.",
    "start": "2296710",
    "end": "2303195"
  },
  {
    "text": " People are working on that,\nbut it's not known yet.",
    "start": "2303195",
    "end": "2311220"
  },
  {
    "text": "Yes, Jasmine?  AUDIENCE: [INAUDIBLE]\nbut I see [INAUDIBLE] I",
    "start": "2311220",
    "end": "2321079"
  },
  {
    "text": "can't separate them and like\nadding an additional feature to raise it to a higher\ndimensional space, where",
    "start": "2321080",
    "end": "2327874"
  },
  {
    "text": "it's separable? MICHALE FEE: Sorry, I\ndidn't quite understand. Can you say it again?",
    "start": "2327874",
    "end": "2333806"
  },
  {
    "text": "AUDIENCE: I think I\nremember reading somewhere about how when the scenes\nare nonlinearly separable--",
    "start": "2333806",
    "end": "2342182"
  },
  {
    "text": "MICHALE FEE: Yes. AUDIENCE: --you can add in\nanother feature to [INAUDIBLE].. MICHALE FEE: Yeah, yeah. So let me show you\nan example of that.",
    "start": "2342182",
    "end": "2349090"
  },
  {
    "start": "2347000",
    "end": "2411000"
  },
  {
    "text": "So coming back to\nthe exclusive OR. So one thing that\nyou can do, you",
    "start": "2349090",
    "end": "2354130"
  },
  {
    "text": "can see that the reason this is\nlinearly inseparable-- it's not linearly separable-- is\nbecause all these points are",
    "start": "2354130",
    "end": "2360970"
  },
  {
    "text": "in a plane. So there's no line\nthat separates them.",
    "start": "2360970",
    "end": "2366620"
  },
  {
    "text": "But one way, one sort\nof trick you can do, is to add noise to this. So that now, some of\nthese points move.",
    "start": "2366620",
    "end": "2373930"
  },
  {
    "text": "You can add another dimension. So now let's say\nthat we add noise, and we just, by chance, happen\nto move the green dots this way",
    "start": "2373930",
    "end": "2381790"
  },
  {
    "text": "and the red dots,\nwell, that way. And now there's a plane that\nwill separate the red dots",
    "start": "2381790",
    "end": "2387400"
  },
  {
    "text": "from the green dots. So that's advanced\nbeyond the scope of what",
    "start": "2387400",
    "end": "2395170"
  },
  {
    "text": "we're talking about here. But yes, there are\ntricks that you can play to get around\nthis exclusive OR",
    "start": "2395170",
    "end": "2402070"
  },
  {
    "text": "problem, this linear\nseparability problem, OK? All right, great question.",
    "start": "2402070",
    "end": "2408940"
  },
  {
    "text": "All right, let's push on. So let's talk about\nmore general two-layer",
    "start": "2408940",
    "end": "2418000"
  },
  {
    "start": "2411000",
    "end": "2779000"
  },
  {
    "text": "feed-forward networks. So this is referred to as a\ntwo-layer network-- an input",
    "start": "2418000",
    "end": "2425800"
  },
  {
    "text": "layer and an output layer. And in this case, we had\na single input neuron",
    "start": "2425800",
    "end": "2431070"
  },
  {
    "text": "and a single output neuron. We generalized that to having\nmultiple input neurons and one",
    "start": "2431070",
    "end": "2436780"
  },
  {
    "text": "output neuron. We saw that we can write\ndown the input current to this output neuron as\nw, the vector of weights,",
    "start": "2436780",
    "end": "2443500"
  },
  {
    "text": "dotted into the vector\nof input firing rates to give us an expression for\nthe firing rate of the output",
    "start": "2443500",
    "end": "2449310"
  },
  {
    "text": "neuron. And now we can\ngeneralize that further to the case of multiple\noutput neurons.",
    "start": "2449310",
    "end": "2454520"
  },
  {
    "text": "So we have multiple input\nneurons, multiple output neurons. You can see that\nwe have a vector",
    "start": "2454520",
    "end": "2460510"
  },
  {
    "text": "of firing rates of\nthe input neurons and a vector of firing\nrates of the output neurons.",
    "start": "2460510",
    "end": "2467099"
  },
  {
    "text": "So we used to just have one\nof these output neurons, and now we've got a\nwhole bunch of them. And so we have to write\ndown a vector of fire rates",
    "start": "2467100",
    "end": "2474520"
  },
  {
    "text": "in the output layer. And now we can write down\nthe firing rate of our output",
    "start": "2474520",
    "end": "2479560"
  },
  {
    "text": "neurons as follows. So the firing rate\nof this neuron here is going to be a\ndot product of the vector",
    "start": "2479560",
    "end": "2488170"
  },
  {
    "text": "of weights onto it. So the firing rate\nof output neuron one is the vector of weights onto\nthat first output neuron dotted",
    "start": "2488170",
    "end": "2499180"
  },
  {
    "text": "into the vector of\ninput firing rates. And the same for the\nnext output neuron.",
    "start": "2499180",
    "end": "2506380"
  },
  {
    "text": "The firing rate of\noutput neuron two is dot product of the weights\nonto that output neuron two",
    "start": "2506380",
    "end": "2512349"
  },
  {
    "text": "and onto the vector\nof input firing rates. Same for neuron three.",
    "start": "2512350",
    "end": "2517900"
  },
  {
    "text": "And we can write\nthat down as follows. So the eighth output--\nthe firing rate",
    "start": "2517900",
    "end": "2523600"
  },
  {
    "text": "of the eighth output\nneuron is the weight vector onto the eighth output neuron\ndotted into the input firing",
    "start": "2523600",
    "end": "2529390"
  },
  {
    "text": "rate vector, OK? And we can write\nthat down as follows, where we've now introduced\na new thing here,",
    "start": "2529390",
    "end": "2535810"
  },
  {
    "text": "which is a matrix of weights. So it's called\nthe weight matrix.",
    "start": "2535810",
    "end": "2543300"
  },
  {
    "text": "And it essentially\nis a matrix of all of these synaptic weights, from\nthe input layer onto the output",
    "start": "2543300",
    "end": "2552900"
  },
  {
    "text": "layer. And now if we had\na linear neuron, we can write down the firing\nrate of the output neuron.",
    "start": "2552900",
    "end": "2560900"
  },
  {
    "text": "The firing rate vector\nof output neuron is just this weight matrix times\nthe vector of input fire rates.",
    "start": "2560900",
    "end": "2572610"
  },
  {
    "text": "So now, we've\nrewritten this problem of finding the vector\nof output firing rates",
    "start": "2572610",
    "end": "2579869"
  },
  {
    "text": "as a matrix multiplication. And we're going to spend\nsome time talking about what",
    "start": "2579870",
    "end": "2585490"
  },
  {
    "text": "that means and what that does. So our feed-forward\nnetwork implements a matrix",
    "start": "2585490",
    "end": "2592590"
  },
  {
    "text": "multiplication. All right, so let's take\na closer look at what this weight matrix looks like.",
    "start": "2592590",
    "end": "2600780"
  },
  {
    "text": "So we have a weight matrix w sub\na comma b that looks like this.",
    "start": "2600780",
    "end": "2606340"
  },
  {
    "text": "So we have four input neurons\nand four output neurons. We have a weight for each input\nneuron onto each output neuron.",
    "start": "2606340",
    "end": "2614670"
  },
  {
    "text": "The columns here correspond\nto different input neurons.",
    "start": "2614670",
    "end": "2620280"
  },
  {
    "text": "The rows correspond to\ndifferent output neurons. Remember, for a\nmatrix, the elements",
    "start": "2620280",
    "end": "2626549"
  },
  {
    "text": "are listed as w sub a, b,\nwhere a is the output neuron.",
    "start": "2626550",
    "end": "2634713"
  },
  {
    "text": "b is the input neuron. On so it's w postsynaptic,\npresynaptic-- post, pre.",
    "start": "2634713",
    "end": "2641759"
  },
  {
    "text": "Rows, columns. So the rows are the\ndifferent output neurons.",
    "start": "2641760",
    "end": "2647400"
  },
  {
    "text": "The columns are the\ndifferent input neurons.  So it can be a little\ntricky to remember.",
    "start": "2647400",
    "end": "2655980"
  },
  {
    "text": "I just remember that it's rows--",
    "start": "2655980",
    "end": "2661030"
  },
  {
    "text": "a matrix is labeled\nby rows and columns. And weight matrices are\npostsynaptic, presynaptic--",
    "start": "2661030",
    "end": "2668000"
  },
  {
    "text": "post, pre.  AUDIENCE: [INAUDIBLE]\ncomment of [INAUDIBLE]??",
    "start": "2668000",
    "end": "2675160"
  },
  {
    "text": "MICHALE FEE: I think\nthat's standard. I'm pretty sure\nthat's very standard.",
    "start": "2675160",
    "end": "2681049"
  },
  {
    "text": "If you find any\nexceptions let me know. OK, we can think of\neach row of this matrix",
    "start": "2681050",
    "end": "2689710"
  },
  {
    "text": "as being the vector of weights\nonto one output neuron. ",
    "start": "2689710",
    "end": "2696890"
  },
  {
    "text": "That row is a vector of weights\nonto that output neuron--",
    "start": "2696890",
    "end": "2701960"
  },
  {
    "text": "that row, that output neuron;\nthat row, that output neuron. Does that makes sense? ",
    "start": "2701960",
    "end": "2709590"
  },
  {
    "text": "All right, so let's flesh out\nthis matrix multiplication. The vector of\noutput firing rates,",
    "start": "2709590",
    "end": "2715838"
  },
  {
    "text": "we're going to write it\nas a column vector, where the first number is\nthis firing rate. That number is that firing rate.",
    "start": "2715838",
    "end": "2722440"
  },
  {
    "text": "That number represents\nthat firing rate, OK? That's equal to\nthis weight matrix times the vector of\ninput firing rates,",
    "start": "2722440",
    "end": "2731849"
  },
  {
    "text": "again, written as\na column vector. And in order to calculate the\nfiring rate of the first output",
    "start": "2731850",
    "end": "2740320"
  },
  {
    "text": "neuron, we take the dot product\nof the first row of the weight matrix and the column vector\nof input firing rates.",
    "start": "2740320",
    "end": "2753020"
  },
  {
    "text": "And that gives us this\nfirst firing rate, OK?",
    "start": "2753020",
    "end": "2759070"
  },
  {
    "text": "To get the second\nfiring rate, we take the dot product of\nthe second row of weights with the vector of firing\nrates, and that gives us",
    "start": "2759070",
    "end": "2766570"
  },
  {
    "text": "this second firing rate. Any questions about that? Just a brief reminder of\nmatrix multiplication.",
    "start": "2766570",
    "end": "2776740"
  },
  {
    "text": "All right, no questions? All right, so let's take\na step back and go quickly",
    "start": "2776740",
    "end": "2786910"
  },
  {
    "start": "2779000",
    "end": "3274000"
  },
  {
    "text": "through some basic\nmatrix algebra. I know most of you have\nprobably seen this,",
    "start": "2786910",
    "end": "2792670"
  },
  {
    "text": "but many haven't, so we're\njust going to go through it. All right, so just\nas vectors are--",
    "start": "2792670",
    "end": "2800109"
  },
  {
    "text": "you can think of them as\na collection of numbers that you write down. So let's say that you are\nmaking a measurement of two",
    "start": "2800110",
    "end": "2807970"
  },
  {
    "text": "different things-- let's say temperature\nand humidity. So you can write down a vector\nthat represents those two",
    "start": "2807970",
    "end": "2815980"
  },
  {
    "text": "quantities. So matrices you can think of\nas collections of vectors. So let's say we take\nthose two measurements",
    "start": "2815980",
    "end": "2823870"
  },
  {
    "text": "at different times, at\nthree different times. So now we have a vector one, a\nvector two, and a vector three",
    "start": "2823870",
    "end": "2831910"
  },
  {
    "text": "that measure those two\nquantities at three different times, all right? So we can now write all\nof those measurements",
    "start": "2831910",
    "end": "2839350"
  },
  {
    "text": "down as a matrix,\nwhere we collect each one of those vectors\nas a column in our matrix,",
    "start": "2839350",
    "end": "2847900"
  },
  {
    "text": "like that. Any questions about that? And there's a bit of MATLAB\ncode that calculates this matrix",
    "start": "2847900",
    "end": "2857170"
  },
  {
    "text": "by writing three\ndifferent column vectors and then concatenating\nthem into a matrix. ",
    "start": "2857170",
    "end": "2865130"
  },
  {
    "text": "All right, and you can\nsee that in this matrix, the columns are just\nthe original vectors,",
    "start": "2865130",
    "end": "2872069"
  },
  {
    "text": "and the rows are-- you can think of\nthose as a time series of our first measurement,\nlet's say temperature.",
    "start": "2872070",
    "end": "2879010"
  },
  {
    "text": "So that's temperature\nas a function of time. This is temperature and\nhumidity at one time.",
    "start": "2879010",
    "end": "2888005"
  },
  {
    "text": "Does that make sense?  All right, so, again, we\ncan write down this matrix.",
    "start": "2888005",
    "end": "2894180"
  },
  {
    "text": "Remember, this is\nthe first measurement at time two, the first\nmeasurement at time three. We have two rows\nand three columns.",
    "start": "2894180",
    "end": "2901650"
  },
  {
    "text": "We can also write\ndown what's known as the transpose of a matrix\nthat just flips the rows",
    "start": "2901650",
    "end": "2907080"
  },
  {
    "text": "and columns. So we can write\ntranspose, which is indicated by this\ncapital super scripted t.",
    "start": "2907080",
    "end": "2913860"
  },
  {
    "text": "And here, we're just flipping\nthe rows and columns. So the first row of this\nmatrix becomes the first column",
    "start": "2913860",
    "end": "2921510"
  },
  {
    "text": "of the transposed matrix. So we have three\nrows and two columns.",
    "start": "2921510",
    "end": "2927450"
  },
  {
    "text": "A symmetric matrix-- I'm just defining\nsome terms now. A symmetric matrix\nis a matrix where",
    "start": "2927450",
    "end": "2934359"
  },
  {
    "text": "the off-diagonal elements--\nso let me just define, that's the diagonal,\nthe matrix diagonal.",
    "start": "2934360",
    "end": "2941799"
  },
  {
    "text": "And a symmetric matrix\nhas the property that the off-diagonal\nelements are zero.",
    "start": "2941800",
    "end": "2948130"
  },
  {
    "text": "And a symmetric matrix\nhas the property that the transpose of that\nmatrix is equal to the matrix,",
    "start": "2948130",
    "end": "2954970"
  },
  {
    "text": "OK? That is only\npossible, of course, if the matrix has the same\nnumber of rows and columns,",
    "start": "2954970",
    "end": "2963016"
  },
  {
    "text": "if it's what's called\na square matrix. ",
    "start": "2963017",
    "end": "2968990"
  },
  {
    "text": "Let me just remind\nyou, in general about matrix multiplication. We can write down the\nproduct of two matrices.",
    "start": "2968990",
    "end": "2976820"
  },
  {
    "text": "And we do that multiplication\nby taking the dot product of each row in the first\nmatrix with each column",
    "start": "2976820",
    "end": "2984590"
  },
  {
    "text": "in the second matrix. So here's the product of\nmatrix A and matrix B.",
    "start": "2984590",
    "end": "2989930"
  },
  {
    "text": "So there's the product. If this matrix, if\nmatrix A, is an m by k--",
    "start": "2989930",
    "end": "2996020"
  },
  {
    "text": "m rows by k columns-- and matrix B has k\nrows by n columns,",
    "start": "2996020",
    "end": "3005090"
  },
  {
    "text": "then the product of\nthose two matrices will have m by n\nrows and columns.",
    "start": "3005090",
    "end": "3014180"
  },
  {
    "text": "And you can see that in order\nfor matrix multiplication to work, the number of\ncolumns of the first matrix",
    "start": "3014180",
    "end": "3023510"
  },
  {
    "text": "equal the number of rows\nin the second matrix. You can see that this k has to\nbe the same for both matrices.",
    "start": "3023510",
    "end": "3030890"
  },
  {
    "text": "Does that make sense? So, again, in order to compute\nthis element right here,",
    "start": "3030890",
    "end": "3037300"
  },
  {
    "text": "we take the dot product\nof the first row of A and the first column of B.\nThat's just 1 times 4, is 4.",
    "start": "3037300",
    "end": "3046450"
  },
  {
    "text": "Plus negative 2\ntimes 7 is minus 14. Plus 0 times minus 1 is 0.",
    "start": "3046450",
    "end": "3051490"
  },
  {
    "text": "Add those up and\nyou get minus 10. So you get this number. You multiply this\nrow dot product",
    "start": "3051490",
    "end": "3057040"
  },
  {
    "text": "this row with this\ncolumn and so on. ",
    "start": "3057040",
    "end": "3062710"
  },
  {
    "text": "Notice, A times B is\nnot equal to B times A. In fact, in cases of rectangular\nmatrices, matrices that aren't",
    "start": "3062710",
    "end": "3071470"
  },
  {
    "text": "square, you can't\neven do this, often do this, multiplication\nin a different order.",
    "start": "3071470",
    "end": "3078760"
  },
  {
    "text": "Mathematically, it\ndoesn't make sense. So let's say that we\nhave a matrix of vectors,",
    "start": "3078760",
    "end": "3087099"
  },
  {
    "text": "and we want to take\nthe dot product of each one of those vectors\nx with some other vector v. So",
    "start": "3087100",
    "end": "3095420"
  },
  {
    "text": "let's just write that down. The way to do that is\nto say the answer here, the dot product of each\none of those column vectors",
    "start": "3095420",
    "end": "3104130"
  },
  {
    "text": "in our matrix with\nthis other vector v we do by taking\nthe transpose of v,",
    "start": "3104130",
    "end": "3109580"
  },
  {
    "text": "which takes a column vector\nand turns it into a row vector. And we can now multiply\nthat by our data matrix x",
    "start": "3109580",
    "end": "3116660"
  },
  {
    "text": "by taking the dot product\nof v with that column of x.",
    "start": "3116660",
    "end": "3121700"
  },
  {
    "text": "And that gives us a matrix. So this matrix here, that\nvector is a one by two matrix.",
    "start": "3121700",
    "end": "3129750"
  },
  {
    "text": "This is a two by three matrix. The product of those is\na one by three matrix.",
    "start": "3129750",
    "end": "3136010"
  },
  {
    "text": "Any questions about that? OK. We can do this a different way.",
    "start": "3136010",
    "end": "3141859"
  },
  {
    "text": "Notice that the result\nof this multiplication here is a row vector, y.",
    "start": "3141860",
    "end": "3147578"
  },
  {
    "text": "We can do this a different way. We can take dot product. We can also compute this\nas y equals x transpose v.",
    "start": "3147578",
    "end": "3155350"
  },
  {
    "text": "So here, we've taken the\ntranspose of the data matrix times this\ncolumn vector v.",
    "start": "3155350",
    "end": "3160790"
  },
  {
    "text": "And again, we take the\ndot product of this, this with this,\nand that with that. And now we get a\ncolumn vector that",
    "start": "3160790",
    "end": "3167859"
  },
  {
    "text": "has the same entries\nthat we had over here. ",
    "start": "3167860",
    "end": "3173980"
  },
  {
    "text": "All right, so I'm just\nshowing you different ways that you can manipulate\na vector in a matrix",
    "start": "3173980",
    "end": "3180920"
  },
  {
    "text": "to compute the dot product\nof elements of vectors",
    "start": "3180920",
    "end": "3188119"
  },
  {
    "text": "within a data matrix\nand other vectors that you're interested in.",
    "start": "3188120",
    "end": "3193490"
  },
  {
    "text": " All right, identity matrix.",
    "start": "3193490",
    "end": "3199160"
  },
  {
    "text": "So when you're multiplying\nnumbers together, the number one has\nthe special property",
    "start": "3199160",
    "end": "3204370"
  },
  {
    "text": "that you can multiply\nany real number by one and get the same number back. ",
    "start": "3204370",
    "end": "3213930"
  },
  {
    "text": "You have the same kind\nof element in matrices.",
    "start": "3213930",
    "end": "3219030"
  },
  {
    "text": "So is there a matrix that when\nmultiplied by A gives you A? And the answer is yes. It's called the identity matrix.",
    "start": "3219030",
    "end": "3225640"
  },
  {
    "text": "So it's given by the\nsymbol I, usually. A times I equals A. What\ndoes that matrix look like?",
    "start": "3225640",
    "end": "3234540"
  },
  {
    "text": "Again, the identity\nmatrix looks like this. It's a square matrix that\nhas ones along the diagonal",
    "start": "3234540",
    "end": "3241320"
  },
  {
    "text": "and zero everywhere else.  So you can see here that if\nyou take an arbitrary vector x,",
    "start": "3241320",
    "end": "3249180"
  },
  {
    "text": "multiplied by the\nidentity matrix, you can see that this product\nis x1, x2 dotted into 1,",
    "start": "3249180",
    "end": "3258630"
  },
  {
    "text": "0, which gives you x1. x1, x2 dotted into\n0, 1, gives you x2.",
    "start": "3258630",
    "end": "3265230"
  },
  {
    "text": "And so the answer looks\nlike that, which is just x. So the identity matrix\ntimes an arbitrary vector x",
    "start": "3265230",
    "end": "3272450"
  },
  {
    "text": "gives you x back. Another very useful\napplication of linear algebra,",
    "start": "3272450",
    "end": "3280559"
  },
  {
    "start": "3274000",
    "end": "3448000"
  },
  {
    "text": "linear algebra tools, is to\nsolve systems of equations. So let me show you\nwhat that looks like.",
    "start": "3280560",
    "end": "3286240"
  },
  {
    "text": "So let's say we want to solve\na simple equation, ax equals c.",
    "start": "3286240",
    "end": "3292230"
  },
  {
    "text": "So, in this case, how\ndo you solve for x? Well, you're just going to\ndivide both sides by a, right?",
    "start": "3292230",
    "end": "3297599"
  },
  {
    "text": "So if you divide\nboth sides by a, you get that x equals\n1 over a times c.",
    "start": "3297600",
    "end": "3304019"
  },
  {
    "text": "So it turns out that there\nis a matrix equivalent of that, that allows you to\nsolve systems of equations.",
    "start": "3304020",
    "end": "3311800"
  },
  {
    "text": "So if you have a\npair of equations-- x minus 2y equals 3 and\n3x plus y equals 5--",
    "start": "3311800",
    "end": "3318570"
  },
  {
    "text": "you can write this down\nas a matrix equation, where you have a\nmatrix 1, minus 2,",
    "start": "3318570",
    "end": "3323910"
  },
  {
    "text": "3, 1, which correspond to\nthe coefficients of x and y in these equations. Times a vector xy is equal\nto 3, 5, another vector 3, 5.",
    "start": "3323910",
    "end": "3336120"
  },
  {
    "text": "So you can write this\ndown as ax equals c-- that's kind of nice--",
    "start": "3336120",
    "end": "3342420"
  },
  {
    "text": "where this matrix A is\ngiven by these coefficients and this vector c is\ngiven by these terms",
    "start": "3342420",
    "end": "3349650"
  },
  {
    "text": "on this side of the equation, on\nthe right side of the equation. Now, how do we solve this?",
    "start": "3349650",
    "end": "3355990"
  },
  {
    "text": "Well, can we just divide both\nsides of that matrix equation,",
    "start": "3355990",
    "end": "3362510"
  },
  {
    "text": "that vector equation, by a? So division is not really\ndefined for matrices,",
    "start": "3362510",
    "end": "3368450"
  },
  {
    "text": "but we can use another trick. We can multiply both\nsides of this equation by something that\nmakes the a go away.",
    "start": "3368450",
    "end": "3377590"
  },
  {
    "text": "And so that magical thing\nis called the inverse of A.",
    "start": "3377590",
    "end": "3382760"
  },
  {
    "text": "So we take the\ninverse of matrix A, denoted by A with this\nsuperscript minus 1.",
    "start": "3382760",
    "end": "3388420"
  },
  {
    "text": "And that's the standard notation\nfor identifying the inverse. It has the property\nthat A inverse times",
    "start": "3388420",
    "end": "3394220"
  },
  {
    "text": "A equals the identity matrix. So you can sort of\nthink about this",
    "start": "3394220",
    "end": "3399780"
  },
  {
    "text": "as A equals the identity matrix\nover A. Anyway, don't really",
    "start": "3399780",
    "end": "3405090"
  },
  {
    "text": "think of it like that. So to solve this system\nof equations ax equals c,",
    "start": "3405090",
    "end": "3411270"
  },
  {
    "text": "we multiply both sides\nby that A inverse matrix.",
    "start": "3411270",
    "end": "3416420"
  },
  {
    "text": "And so that looks like this-- A inverse A times x\nequals A inverse c.",
    "start": "3416420",
    "end": "3423240"
  },
  {
    "text": "A inverse A is just what? The identity matrix times\nx equals A inverse c.",
    "start": "3423240",
    "end": "3430920"
  },
  {
    "text": "And we just saw before that\nidentity matrix times x is just x.",
    "start": "3430920",
    "end": "3435930"
  },
  {
    "text": "All right, so\nthere's the solution to this system of equations.",
    "start": "3435930",
    "end": "3444140"
  },
  {
    "text": "All right, any\nquestions about that? ",
    "start": "3444140",
    "end": "3450220"
  },
  {
    "start": "3448000",
    "end": "3599000"
  },
  {
    "text": "So how do you find the\ninverse of a matrix? What is this A inverse? How do you get it in real life?",
    "start": "3450220",
    "end": "3457900"
  },
  {
    "text": "So in real life, what\nyou usually do is you would just use the matrix\ninverse function in Matlab.",
    "start": "3457900",
    "end": "3464250"
  },
  {
    "text": "Because for any matrices\nother than a two-by-two, it's really annoying to\nget a matrix inverse.",
    "start": "3464250",
    "end": "3470160"
  },
  {
    "text": "But for a two-by-two matrix,\nit's actually pretty easy. You can almost just get the\nanswer by looking at the matrix",
    "start": "3470160",
    "end": "3476340"
  },
  {
    "text": "and writing down the inverse. It looks like this. The inverse of a two-by-two\nsquare matrix is just given",
    "start": "3476340",
    "end": "3483360"
  },
  {
    "text": "by a slight reordering\nof the coefficients, of the entries of that matrix,\ndivided by what's called",
    "start": "3483360",
    "end": "3489599"
  },
  {
    "text": "the determinant of A. So\nwhat you do is you flip-- in a two-by-two matrix,\nyou flip the A and the D,",
    "start": "3489600",
    "end": "3498090"
  },
  {
    "text": "and then you multiply the\ndiagonal elements by minus 1.",
    "start": "3498090",
    "end": "3504990"
  },
  {
    "text": "Now, what is this determinant? The determinant is given by\na times d minus b times c.",
    "start": "3504990",
    "end": "3513059"
  },
  {
    "text": "And you can prove\nthat that actually is the inverse, because if we\ntake this and multiply it by A,",
    "start": "3513060",
    "end": "3519940"
  },
  {
    "text": "what you find when you multiply\nthat out is that that's just equal to the identity matrix.",
    "start": "3519940",
    "end": "3528369"
  },
  {
    "text": "So a matrix has an\ninverse if and only if the determinant\nis not equal to zero.",
    "start": "3528370",
    "end": "3535360"
  },
  {
    "text": "If the determinant\nis equal to zero, you can see that\nthis thing blows up, and there's no inverse.",
    "start": "3535360",
    "end": "3542250"
  },
  {
    "text": "We're going to spend\na little bit of time later talking about what\nthat means when a matrix has",
    "start": "3542250",
    "end": "3547630"
  },
  {
    "text": "an inverse and what the\ndeterminant actually corresponds to in a matrix\nmultiplication context.",
    "start": "3547630",
    "end": "3558920"
  },
  {
    "text": "If the determinant\nis equal to zero, we say that that\nmatrix is singular.",
    "start": "3558920",
    "end": "3564260"
  },
  {
    "text": "And in that case, you can't\nactually find an inverse, and you can't solve this\nequation right here,",
    "start": "3564260",
    "end": "3572240"
  },
  {
    "text": "this system of equations. ",
    "start": "3572240",
    "end": "3578720"
  },
  {
    "text": "All right, so let's actually\ngo through this example. So here's our\nequation, ax equals c.",
    "start": "3578720",
    "end": "3585530"
  },
  {
    "text": "We're going to use the\nsame matrix we had before and the same c. The determinant is\njust the product",
    "start": "3585530",
    "end": "3592910"
  },
  {
    "text": "of those minus the product of\nthose, so 1 minus negative 6. So the determinant is 7.",
    "start": "3592910",
    "end": "3598549"
  },
  {
    "text": "So there is an inverse\nof this matrix. And we can just write\nthat down as follows.",
    "start": "3598550",
    "end": "3603809"
  },
  {
    "text": "Again, we've flipped those\ntwo and multiplied those by minus 1. So we can solve for x just by\ntaking that inverse times c,",
    "start": "3603810",
    "end": "3613550"
  },
  {
    "text": "A inverse times c. And if you multiply\nthat out, you see that there's the inverse.",
    "start": "3613550",
    "end": "3619418"
  },
  {
    "text": "It's just a vector. ",
    "start": "3619418",
    "end": "3624680"
  },
  {
    "text": "That's it. That's how you solve a system\nof equations, all right?",
    "start": "3624680",
    "end": "3631400"
  },
  {
    "text": "Any questions about that? So this process of solving\nsystems of equations",
    "start": "3631400",
    "end": "3643590"
  },
  {
    "text": "and using matrices\nand their inverses",
    "start": "3643590",
    "end": "3649250"
  },
  {
    "text": "to solve systems of equations\nis a very important concept that we're going to use\nover and over again.",
    "start": "3649250",
    "end": "3655820"
  },
  {
    "text": " All right, let's\nturn to the topic",
    "start": "3655820",
    "end": "3661039"
  },
  {
    "text": "of matrix transformations. All right, so you can see\nfrom this problem of solving",
    "start": "3661040",
    "end": "3666710"
  },
  {
    "text": "this system of equations that\nthat matrix A transformed",
    "start": "3666710",
    "end": "3672099"
  },
  {
    "text": "a vector x into a vector c, OK? So we have this vector x, which\nwas 3/7 minus 4/7 a vector.",
    "start": "3672100",
    "end": "3681290"
  },
  {
    "text": "When we multiplied that by\nA, we got another vector, c.",
    "start": "3681290",
    "end": "3686940"
  },
  {
    "text": " And the vector A inverse\ntransforms this vector",
    "start": "3686940",
    "end": "3694960"
  },
  {
    "text": "c back into vector x, right? So we can take that vector\nc, multiply it by A inverse,",
    "start": "3694960",
    "end": "3704170"
  },
  {
    "text": "and get back to x. Does that make sense?",
    "start": "3704170",
    "end": "3709339"
  },
  {
    "text": "So, in general, a\nmatrix A maps a set",
    "start": "3709340",
    "end": "3716480"
  },
  {
    "text": "of vectors in this whole space. So if you have a\ntwo-by-two vector,",
    "start": "3716480",
    "end": "3721730"
  },
  {
    "text": "it maps a set of vectors\nin R2 onto a different set",
    "start": "3721730",
    "end": "3728619"
  },
  {
    "text": "of vectors in R2. So you can take\nany vector here-- a vector from the\norigin into here--",
    "start": "3728620",
    "end": "3736360"
  },
  {
    "text": "multiply that vector\nby A, and it gives you a different vector. And if you multiply that\nother vector by A inverse,",
    "start": "3736360",
    "end": "3743220"
  },
  {
    "text": "you go back to the\noriginal vector. So this vector A\nimplements some kind",
    "start": "3743220",
    "end": "3751860"
  },
  {
    "text": "of transformation on this\nspace of real numbers into a different space\nof real numbers, OK?",
    "start": "3751860",
    "end": "3762120"
  },
  {
    "text": "And you can only do this\ninverse if the determinant of A is not equal to zero.",
    "start": "3762120",
    "end": "3767250"
  },
  {
    "text": " So I just want to show you\nwhat different kinds of matrix",
    "start": "3767250",
    "end": "3775260"
  },
  {
    "text": "transformations look like. ",
    "start": "3775260",
    "end": "3780980"
  },
  {
    "text": "So let's start with the\nsimplest matrix transformation-- the identity matrix.",
    "start": "3780980",
    "end": "3786260"
  },
  {
    "text": "So if we take a\nvector x, multiply it by the identity matrix,\nyou get another vector y,",
    "start": "3786260",
    "end": "3792710"
  },
  {
    "text": "which is equal to x. So what we're going to do is\nwe're going to kind of riff off",
    "start": "3792710",
    "end": "3798650"
  },
  {
    "text": "of a theme here, and\nwe're going to take slight perturbations\nof the identity matrix",
    "start": "3798650",
    "end": "3806400"
  },
  {
    "text": "and see what that new matrix\ndoes to a set of input vectors, OK?",
    "start": "3806400",
    "end": "3811490"
  },
  {
    "text": "So let me show you how\nwe're going to do that. We're going to take it the\nidentity matrix 1, 0, 0, 1.",
    "start": "3811490",
    "end": "3817050"
  },
  {
    "text": "And we're going to add\na little perturbation to the diagonal elements. ",
    "start": "3817050",
    "end": "3823900"
  },
  {
    "text": "And we're going to see what that\ndoes to a set of input vectors. So let me show you\nwhat we're doing here.",
    "start": "3823900",
    "end": "3829810"
  },
  {
    "text": "We have each one\nof these red dots. So what I did was I generated\na bunch of random numbers",
    "start": "3829810",
    "end": "3838410"
  },
  {
    "text": "in a 2D space. So this is a 2D space. And I just randomly\nselected a bunch of numbers, a bunch of\npoints on that plane.",
    "start": "3838410",
    "end": "3847320"
  },
  {
    "text": "And each one of those\nis an input vector x. And then I multiplied\nthat vector",
    "start": "3847320",
    "end": "3853359"
  },
  {
    "text": "times this slightly\nperturbed identity matrix. ",
    "start": "3853360",
    "end": "3862030"
  },
  {
    "text": "And then I get a bunch\nof output vectors y. Input vectors x\nare the red dots. The output vectors y are the\nother end of this blue line.",
    "start": "3862030",
    "end": "3871800"
  },
  {
    "text": "Does that make sense? So for every vector x,\nmultiplying it by this matrix",
    "start": "3871800",
    "end": "3879600"
  },
  {
    "text": "gives me another vector\nthat's over here. Does that make sense?",
    "start": "3879600",
    "end": "3884930"
  },
  {
    "text": "So you can see that\nwhat this matrix does is it takes this space,\nthis cloud of points,",
    "start": "3884930",
    "end": "3892599"
  },
  {
    "text": "and stretches them\nequally in all directions. So it takes any vector\nand just makes it longer,",
    "start": "3892600",
    "end": "3900760"
  },
  {
    "text": "stretches it out. No matter which\ndirection it's pointing, it just makes that\nvector slightly longer.",
    "start": "3900760",
    "end": "3906210"
  },
  {
    "text": " And here's that\nlittle bit of code that I used to\ngenerate those vectors.",
    "start": "3906210",
    "end": "3917670"
  },
  {
    "text": "OK, so let's take\nanother example. Let's say that we take\nthe identity matrix and we just add a little\nperturbation to one element",
    "start": "3917670",
    "end": "3926019"
  },
  {
    "text": "of the identity matrix, OK? So what does that do? It stretches the vectors\nout in the x direction,",
    "start": "3926020",
    "end": "3937400"
  },
  {
    "text": "but it doesn't do anything\nto the y direction. So the vector with a\ncomponent in the x direction,",
    "start": "3937400",
    "end": "3945200"
  },
  {
    "text": "the x component gets increased\nby an by a factor 1 plus delta.",
    "start": "3945200",
    "end": "3951250"
  },
  {
    "text": "The components of each of these\nvectors in the y direction don't change, all right?",
    "start": "3951250",
    "end": "3957720"
  },
  {
    "text": "So we're going to take\nthis cloud of points, and we're going to stretch\nit in the x direction. What about this matrix here?",
    "start": "3957720",
    "end": "3965540"
  },
  {
    "text": "What's that going to do? AUDIENCE: Stretch it\nin the y direction. MICHALE FEE: Good. It's going to stretch it\nout in the y direction.",
    "start": "3965540",
    "end": "3972346"
  },
  {
    "text": "Good. So that's kind of cute. ",
    "start": "3972346",
    "end": "3979000"
  },
  {
    "text": "And you can see that this\nearlier matrix that we looked at right here stretches\nin the x direction",
    "start": "3979000",
    "end": "3987560"
  },
  {
    "text": "and stretches in\nthe y direction. And that's why that\ncloud of vectors",
    "start": "3987560",
    "end": "3992960"
  },
  {
    "text": "just stretched out\nequally in all directions. ",
    "start": "3992960",
    "end": "4000340"
  },
  {
    "text": "Out this. What is that going to do? AUDIENCE: It would stretch in\nthe x direction and compress",
    "start": "4000340",
    "end": "4006864"
  },
  {
    "text": "in the y direction MICHALE FEE: Right. This perturbation here\nis making this component,",
    "start": "4006864",
    "end": "4012500"
  },
  {
    "text": "the x component larger. This perturbation here--\nand delta here is small.",
    "start": "4012500",
    "end": "4018860"
  },
  {
    "text": "It's less than one. Here, it's making the\ny component smaller.",
    "start": "4018860",
    "end": "4023930"
  },
  {
    "text": "And so what that looks like\nis the y component of each one of these vectors gets smaller. The x component gets larger.",
    "start": "4023930",
    "end": "4030740"
  },
  {
    "text": "And so we're squeezing\nin one direction and stretching in\nthe other direction.",
    "start": "4030740",
    "end": "4037740"
  },
  {
    "text": "Imagine we took\na block of sponge and we grabbed it\nand stretched it out,",
    "start": "4037740",
    "end": "4043735"
  },
  {
    "text": "and it gets skinny\nin this direction and stretches out\nin that direction. All right, that's kind of cool.",
    "start": "4043735",
    "end": "4050050"
  },
  {
    "text": " What is this going to do?",
    "start": "4050050",
    "end": "4056060"
  },
  {
    "text": "Here, I'm not making a\nsmall perturbation of this, but I'm flipping the\nsign of one of those.",
    "start": "4056060",
    "end": "4062880"
  },
  {
    "text": "What happens there? What is that going to do? ",
    "start": "4062880",
    "end": "4068470"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] MICHALE FEE: Good. What do we call that?",
    "start": "4068470",
    "end": "4074240"
  },
  {
    "text": "There's a term for it. What do you-- yeah, it's\ncalled a mirror reflection.",
    "start": "4074240",
    "end": "4082400"
  },
  {
    "text": "So every point that's on\nthis side of the origin gets reflected over to\nthis side of the origin.",
    "start": "4082400",
    "end": "4090370"
  },
  {
    "text": "And every point\nthat's over here-- sorry, of this axis. Every point that's on\nthis side of the y-axis",
    "start": "4090370",
    "end": "4095980"
  },
  {
    "text": "gets reflected\nover to this side. So that's called a\nmirror reflection.",
    "start": "4095980",
    "end": "4103410"
  },
  {
    "text": "What is this? What is that going to do? ",
    "start": "4103410",
    "end": "4115430"
  },
  {
    "text": "Abiba? AUDIENCE: Reflect\nit [INAUDIBLE].. MICHALE FEE: Right. It's going to reflect it\nthrough the origin, like this.",
    "start": "4115430",
    "end": "4123399"
  },
  {
    "text": "So every point that's over\nhere, on one side of the origin, is going to reflect\nthrough to the other side.",
    "start": "4123399",
    "end": "4130270"
  },
  {
    "text": "That's pretty neat. Inversion of the origin. OK?",
    "start": "4130270",
    "end": "4136870"
  },
  {
    "text": "So we have symmetric\nperturbations in the x and y components\nof the identity matrix.",
    "start": "4136870",
    "end": "4144299"
  },
  {
    "text": "We have a stretch transformation\nthat stretches along one axis,",
    "start": "4144300",
    "end": "4150199"
  },
  {
    "text": "but not the other. Stretch around the other axis,\nthe y-axis, but not the x-axis.",
    "start": "4150200",
    "end": "4157130"
  },
  {
    "text": "Stretch along x and\ncompression along y. Mirror reflection\nthrough the y-axis.",
    "start": "4157130",
    "end": "4164990"
  },
  {
    "text": "Inversion through the origin. These are examples of\ndiagonal matrices, OK?",
    "start": "4164990",
    "end": "4171740"
  },
  {
    "text": "So the only thing\nwe've done so far-- we've gotten all these\nreally cool transformations,",
    "start": "4171740",
    "end": "4176778"
  },
  {
    "text": "but the only thing\nwe've done so far are change these two\ndiagonal elements. ",
    "start": "4176779",
    "end": "4183778"
  },
  {
    "text": "So there's a lot\nmore crazy stuff to happen if we start messing\nwith the other components.",
    "start": "4183779",
    "end": "4191309"
  },
  {
    "text": "Oh, and I should mention\nthat we can invert any one of these transformations\nthat we just did by finding",
    "start": "4191310",
    "end": "4201060"
  },
  {
    "text": "the inverse of this matrix. The inverse of a diagonal matrix\nis very simple to calculate.",
    "start": "4201060",
    "end": "4206805"
  },
  {
    "text": "It's just one over\nthose diagonal elements. ",
    "start": "4206805",
    "end": "4213470"
  },
  {
    "text": "All right, how about this?  What is that going to do?",
    "start": "4213470",
    "end": "4218910"
  },
  {
    "text": "Anybody? ",
    "start": "4218910",
    "end": "4228969"
  },
  {
    "text": "When you take a vector\nand you multiply it by that, what's going to happen? This part is going to give\nyou the original vector back.",
    "start": "4228970",
    "end": "4236800"
  },
  {
    "text": "This part is going to take a\nlittle bit of the y component and add it to the x component.",
    "start": "4236800",
    "end": "4245630"
  },
  {
    "text": "So what does that do? That produces what's\nknown as a shear. So points up here,\nwe're going to take",
    "start": "4245630",
    "end": "4253340"
  },
  {
    "text": "a little bit of the y component\nand add it to the x component. So if something has\na big y component,",
    "start": "4253340",
    "end": "4260300"
  },
  {
    "text": "it's going to be shifted in x. If something has a\nnegative y component,",
    "start": "4260300",
    "end": "4266710"
  },
  {
    "text": "it's going to shift\nthis way in x. If something has a\npositive y component, it's going to shift\nthis way an x.",
    "start": "4266710",
    "end": "4272500"
  },
  {
    "text": "And it's going to produce\nwhat's called a shear. So we're pushing\nthese points this way,",
    "start": "4272500",
    "end": "4280099"
  },
  {
    "text": "pushing those points this way. ",
    "start": "4280100",
    "end": "4285230"
  },
  {
    "text": "Shear is very important in\nthings like the flow of liquid. So when you have liquid\nflowing over a surface,",
    "start": "4285230",
    "end": "4292699"
  },
  {
    "text": "you have forces, frictional\nforces to the liquid down here that prevent it from moving.",
    "start": "4292700",
    "end": "4299750"
  },
  {
    "text": "Liquid up here\nmoves more quickly, and it produces a shear in the\npattern of velocity profiles.",
    "start": "4299750",
    "end": "4308250"
  },
  {
    "text": "OK, that's pretty cool. What about this? ",
    "start": "4308250",
    "end": "4316520"
  },
  {
    "text": "It's going to just\nproduce a shear along the other direction. That's right. So now components that have a--",
    "start": "4316520",
    "end": "4323250"
  },
  {
    "text": "vectors that have a\nlarge x component acquire a negative projection in y.",
    "start": "4323250",
    "end": "4330900"
  },
  {
    "start": "4330900",
    "end": "4337159"
  },
  {
    "text": "OK, what does this look like? It's pretty cool. ",
    "start": "4337160",
    "end": "4350599"
  },
  {
    "text": "We're going to get some\nshear in this direction,",
    "start": "4350600",
    "end": "4356680"
  },
  {
    "text": "get some shear in\nthis direction. What's it going to do?",
    "start": "4356680",
    "end": "4362136"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] MICHALE FEE: Good.",
    "start": "4362137",
    "end": "4367420"
  },
  {
    "text": "Good guess. That's exactly right,\nproduces a rotation.",
    "start": "4367420",
    "end": "4372840"
  },
  {
    "text": "Not exactly a rotation,\nbut very close. ",
    "start": "4372840",
    "end": "4381470"
  },
  {
    "text": "So that's how you actually\nproduce a rotation. So notice, for small angles\ntheta, these are close to one,",
    "start": "4381470",
    "end": "4390000"
  },
  {
    "text": "so it's close to\nan identity matrix. These are close to zero,\nbut this is negative",
    "start": "4390000",
    "end": "4397090"
  },
  {
    "text": "and this is positive,\nor the other way around. So if we have diagonals close\nto one and the off-diagonals one",
    "start": "4397090",
    "end": "4407560"
  },
  {
    "text": "positive and one negative,\nthen that produces a rotation. That, formally, is\na rotation matrix.",
    "start": "4407560",
    "end": "4413760"
  },
  {
    "text": "Yes? AUDIENCE: On the\nprevious slide, is there a reason you chose to represent\nthe delta on the x-axis as",
    "start": "4413760",
    "end": "4419858"
  },
  {
    "text": "negative? MICHALE FEE: No. It goes either way. So if you have a rotation\nangle that's positive,",
    "start": "4419858",
    "end": "4425600"
  },
  {
    "text": "then this is negative\nand this is positive. If your rotation angle\nis the other sign,",
    "start": "4425600",
    "end": "4430840"
  },
  {
    "text": "then this is positive\nand this is negative. So, for example, if we want to\nproduce a 45-degree rotation,",
    "start": "4430840",
    "end": "4440260"
  },
  {
    "text": "then we have 1, 1, minus 1, 1. And of course, all those\nthings have a square root",
    "start": "4440260",
    "end": "4447040"
  },
  {
    "text": "of 2, 1 over square\nroot of 2, in them. And so that looks like this. So if you have, let's say,\ntheta equals 10 degrees,",
    "start": "4447040",
    "end": "4454179"
  },
  {
    "text": "we can produce a 10-degree\nrotation of all the vectors. If theta is 25\ndegrees, you can see",
    "start": "4454180",
    "end": "4460180"
  },
  {
    "text": "that the rotation is further. Theta 45, that's\nthis case right here.",
    "start": "4460180",
    "end": "4465560"
  },
  {
    "text": "You can see that you get a\n45-degree rotation of all of those vectors\naround the origin.",
    "start": "4465560",
    "end": "4471440"
  },
  {
    "text": "And if theta is 90 degrees,\nyou can see that, OK?",
    "start": "4471440",
    "end": "4477850"
  },
  {
    "text": "Pretty cool, right?  OK, what is the inverse\nof this rotation matrix?",
    "start": "4477850",
    "end": "4486880"
  },
  {
    "text": "So if we have a\nrotation-- oh, and I just want to point out\none more thing.",
    "start": "4486880",
    "end": "4493140"
  },
  {
    "text": "In this formulation of\nthe rotation matrix, positive angles correspond\nto rotating counterclockwise.",
    "start": "4493140",
    "end": "4500969"
  },
  {
    "text": " Negative angles\ncorrespond to rotation",
    "start": "4500970",
    "end": "4507640"
  },
  {
    "text": "in the clockwise direction, OK? So there's a big hint. What is the inverse of\nour rotation matrix?",
    "start": "4507640",
    "end": "4517230"
  },
  {
    "text": "If we have a rotation\nof 10 degrees this way,",
    "start": "4517230",
    "end": "4522940"
  },
  {
    "text": "what is the inverse of that? AUDIENCE: [INAUDIBLE] MICHALE FEE: Right. AUDIENCE: [INAUDIBLE]",
    "start": "4522940",
    "end": "4528909"
  },
  {
    "text": "MICHALE FEE: That's right. Remember, matrix multiplication\nimplements a transformation.",
    "start": "4528910",
    "end": "4535870"
  },
  {
    "text": "The inverse of\nthat transformation just takes you back\nwhere you were.",
    "start": "4535870",
    "end": "4541420"
  },
  {
    "text": "So if you have a rotation\nmatrix that you implemented a 20-degree rotation\nin the plus direction,",
    "start": "4541420",
    "end": "4547750"
  },
  {
    "text": "then the inverse of that\nis a 20-degree rotation in the minus direction.",
    "start": "4547750",
    "end": "4553120"
  },
  {
    "text": "So the inverse of\nthis matrix you can get just by putting in\na minus sign into the theta.",
    "start": "4553120",
    "end": "4558830"
  },
  {
    "text": "And you can see that\ncosine of minus theta is just cosine of theta. But sine of minus theta\nis negative sine of theta.",
    "start": "4558830",
    "end": "4566500"
  },
  {
    "text": " So the inverse of this\nmatrix is just this.",
    "start": "4566500",
    "end": "4573100"
  },
  {
    "text": "You change the sign\nof those diagonals, which just makes the shear go in\nthe opposite direction, right?",
    "start": "4573100",
    "end": "4579620"
  },
  {
    "text": " OK, so a rotation\nby angle plus theta",
    "start": "4579620",
    "end": "4586679"
  },
  {
    "text": "followed by a rotation\nof angle minus theta puts everything\nback where it was. So rotation matrix phi of\nminus theta times phi of theta",
    "start": "4586680",
    "end": "4597590"
  },
  {
    "text": "is equal to the identity matrix. So those two are\ninverses of each other. ",
    "start": "4597590",
    "end": "4604410"
  },
  {
    "text": "And the inverse of a--\nnotice that the inverse of this rotation matrix\nis also just the transpose",
    "start": "4604410",
    "end": "4611850"
  },
  {
    "text": "of the rotation matrix.  All right, so what\nyou can see is",
    "start": "4611850",
    "end": "4618190"
  },
  {
    "text": "that these different\ncool transformations that these matrix\nmultiplications can do",
    "start": "4618190",
    "end": "4627489"
  },
  {
    "text": "are just examples of what our\nfeed-forward network can do. Because the feed-m\nforward network",
    "start": "4627490",
    "end": "4633460"
  },
  {
    "text": "just implements\nmatrix multiplication. So this feed-forward\nnetwork takes",
    "start": "4633460",
    "end": "4638949"
  },
  {
    "text": "a set of vectors, a\nset of input vectors, and transforms them into a set\nof output vectors, all right?",
    "start": "4638950",
    "end": "4646060"
  },
  {
    "text": "And you can understand what\nthat transformation does just by understanding the different\nkinds of transformations",
    "start": "4646060",
    "end": "4652059"
  },
  {
    "text": "you can get from\nmatrix multiplication.",
    "start": "4652060",
    "end": "4657550"
  },
  {
    "text": "All right, we'll\ncontinue next time. ",
    "start": "4657550",
    "end": "4667000"
  }
]