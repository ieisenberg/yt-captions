[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6950"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6950",
    "end": "13500"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "13500",
    "end": "19380"
  },
  {
    "text": " PROFESSOR: So today, we're going\nto talk a bit more about",
    "start": "19380",
    "end": "26840"
  },
  {
    "start": "23000",
    "end": "52000"
  },
  {
    "text": "parallelism and about how you\nget performance out of",
    "start": "26840",
    "end": "32360"
  },
  {
    "text": "parallel codes. And also, we're going to take\na little bit of a tour underneath the Cilk++ runtime\nsystem so you can get an idea",
    "start": "32360",
    "end": "41100"
  },
  {
    "text": "of what's going on underneath\nand why it is that when you code stuff, how it is that it\ngets mapped, scheduled on the",
    "start": "41100",
    "end": "49540"
  },
  {
    "text": "processors.  So when people talk about\nparallelism, one of the first",
    "start": "49540",
    "end": "57230"
  },
  {
    "start": "52000",
    "end": "304000"
  },
  {
    "text": "things that often comes up is\nwhat's called Amdahl's Law. Gene Amdahl was the architect\nof the IBM360 computers who",
    "start": "57230",
    "end": "69780"
  },
  {
    "text": "then left IBM and formed his\nown company that made",
    "start": "69780",
    "end": "75400"
  },
  {
    "text": "competing machines and he made\nthe following observation",
    "start": "75400",
    "end": "82440"
  },
  {
    "text": "about parallel computing,\nhe said-- and I'm paraphrasing here--",
    "start": "82440",
    "end": "88250"
  },
  {
    "text": "half your application is\nparallel and half is serial. You can't get more than a factor\nof two speed up, no",
    "start": "88250",
    "end": "95310"
  },
  {
    "text": "matter how many processors\nit runs on. So if you think about it, if\nit's half parallel and you",
    "start": "95310",
    "end": "102050"
  },
  {
    "text": "managed to make that parallel\npart run in zero time, still",
    "start": "102050",
    "end": "108390"
  },
  {
    "text": "the serial part will be half of\nthe time and you only get a factor of two speedup.",
    "start": "108390",
    "end": "114980"
  },
  {
    "text": "You can generalize that to say\nif some fraction alpha can be run in parallel and the rest\nmust be run serially, the",
    "start": "114980",
    "end": "121420"
  },
  {
    "text": "speedup is at most 1\nover 1 minus alpha. OK, so this was used in the\n1980s in particular to say why",
    "start": "121420",
    "end": "135170"
  },
  {
    "text": "it was that parallel computing\nhad no future, because you simply weren't going to be able\nto get very much speedups",
    "start": "135170",
    "end": "143090"
  },
  {
    "text": "from parallel computing. You're going to spend extra\nhardware on the parallel parts",
    "start": "143090",
    "end": "148890"
  },
  {
    "text": "of the system and yet you might\nbe limited in terms of",
    "start": "148890",
    "end": "154580"
  },
  {
    "text": "how much parallelism there is\nin a particular application and you wouldn't get\nvery much speedup.",
    "start": "154580",
    "end": "160060"
  },
  {
    "text": "You wouldn't get the bang for\nthe buck, if you will. So things have changed today\nthat make that not quite the",
    "start": "160060",
    "end": "166650"
  },
  {
    "text": "same story. The first thing is that with\nmulticore computers, it is",
    "start": "166650",
    "end": "173489"
  },
  {
    "text": "pretty much just as inexpensive\nto produce a p",
    "start": "173490",
    "end": "180600"
  },
  {
    "text": "processor right now, like six\nprocessor machine as it is a one processor machine. so it's not like you're actually\npaying for those",
    "start": "180600",
    "end": "187440"
  },
  {
    "text": "extra processing cores. They come for free. Because what else are you're\ngoing to use that silicon for?",
    "start": "187440",
    "end": "196330"
  },
  {
    "text": "And the other thing is that\nwe've had a large growth of understanding of problems\nfor which there's ample",
    "start": "196330",
    "end": "203510"
  },
  {
    "text": "parallelism, where that\namount of time is, in fact, quite small.",
    "start": "203510",
    "end": "210830"
  },
  {
    "text": "And the main place these things\ncome from, it turns out, this analysis is kind of a\nthroughput kind of analysis.",
    "start": "210830",
    "end": "218170"
  },
  {
    "text": "OK, it says, gee, I only get\n50% speedup for that application, but what most\npeople care about in most",
    "start": "218170",
    "end": "226920"
  },
  {
    "text": "interactive applications, at\nleast for a client side programming, is response time.",
    "start": "226920",
    "end": "233020"
  },
  {
    "text": "And for any problem that you\nhave that has a response time",
    "start": "233020",
    "end": "238120"
  },
  {
    "text": "that's too long and its compute\nintensive, using parallelism to make it so that\nthe response is much zippier",
    "start": "238120",
    "end": "247590"
  },
  {
    "text": "is definitely worthwhile. And so this is true, even for\nthings like game programs.",
    "start": "247590",
    "end": "254520"
  },
  {
    "text": "So in game programs, they don't\nhave quite a response time problem, they have what's\ncalled a time box problem,",
    "start": "254520",
    "end": "260180"
  },
  {
    "text": "where you have a certain\namount of time-- 13 milliseconds typically-- because you need some slop to\nmake sure that you can go from",
    "start": "260180",
    "end": "269729"
  },
  {
    "text": "one frame to another, but about\n13 milliseconds to do a rendering of whatever the frame\nis that the game player",
    "start": "269730",
    "end": "277540"
  },
  {
    "text": "is going to see on his computer\nor her computer. And so in that time, you want to\ndo as much as you possibly",
    "start": "277540",
    "end": "287560"
  },
  {
    "text": "can, and so there's a big\nopportunity there to take advantage of parallelism in\norder to do more, have more",
    "start": "287560",
    "end": "295530"
  },
  {
    "text": "quality graphics, have better\nAI, have better physics and all the other components that\nmake up a game engine.",
    "start": "295530",
    "end": "302245"
  },
  {
    "text": " But one of the issues\nwith Amdahl's Law--",
    "start": "302245",
    "end": "310759"
  },
  {
    "text": "and this analysis is a cogent\nanalysis that Amdahl made-- but one of the issues here is\nthat it doesn't really say",
    "start": "310760",
    "end": "320270"
  },
  {
    "text": "anything about how fast\nyou can expect your application to run.",
    "start": "320270",
    "end": "325750"
  },
  {
    "text": "In other words, this is a nice\nsort of thing, but who really can decompose their application\ninto the serial",
    "start": "325750",
    "end": "332910"
  },
  {
    "text": "part and the part that\ncan be parallel? Well fortunately, there's been a\nlot of work in the theory of",
    "start": "332910",
    "end": "341940"
  },
  {
    "text": "parallel systems to answer this\nquestion, and we're going to go over some of that really\noutstanding research that",
    "start": "341940",
    "end": "350800"
  },
  {
    "text": "helps us understand what\nparallelism is. So we're going to talk a\nlittle bit about what",
    "start": "350800",
    "end": "356740"
  },
  {
    "start": "355000",
    "end": "395000"
  },
  {
    "text": "parallelism is and come up with\na very specific measure",
    "start": "356740",
    "end": "362800"
  },
  {
    "text": "of parallelism, quantify\nparallelism, OK? We're also going to talk a\nlittle bit about scheduling",
    "start": "362800",
    "end": "368720"
  },
  {
    "text": "theory and how the Cilk++\nruntime system works. And then we're going to have\na little chess lesson.",
    "start": "368720",
    "end": "375900"
  },
  {
    "text": "So who here plays chess? Nobody plays chess anymore. Who plays Angry Birds?",
    "start": "375900",
    "end": "382410"
  },
  {
    "text": "[LAUGHTER] OK. ",
    "start": "382410",
    "end": "389940"
  },
  {
    "text": "So you don't have to know\nanything about chess to learn this chess lesson, that's OK.",
    "start": "389940",
    "end": "395699"
  },
  {
    "start": "395000",
    "end": "474000"
  },
  {
    "text": "So we'll start out with\nwhat is parallelism? So let's recall first the\nbasics of Cilk++.",
    "start": "395700",
    "end": "400979"
  },
  {
    "text": "So here's the example of the\nlousy Fibonacci that everybody parallelizes because it's\ngood didactically.",
    "start": "400980",
    "end": "408110"
  },
  {
    "text": "We have the Cilk spawn statement\nthat says that the child can execute in parallel\nwith the parent caller and the",
    "start": "408110",
    "end": "414169"
  },
  {
    "text": "sync that says don't go past\nthis point until all your spawn children have returned. And that's a local sync,\nthat's just a",
    "start": "414170",
    "end": "420240"
  },
  {
    "text": "sync for that function. It's not a sync across\nthe whole machine. So some of you may have had\nexperience with open MP",
    "start": "420240",
    "end": "426840"
  },
  {
    "text": "barriers, for example,\nthat's a sync across the whole machine. This is not, this is just a\nlocal sync for this function",
    "start": "426840",
    "end": "433110"
  },
  {
    "text": "saying when I sync, make sure\nall my children have returned before going past this point.",
    "start": "433110",
    "end": "438199"
  },
  {
    "text": "And just remember also that Cilk\nkeywords grant permission for parallel execution. They don't command parallel\nexecution.",
    "start": "438200",
    "end": "444890"
  },
  {
    "text": "OK so we can always execute\nour code serially if we choose to. ",
    "start": "444890",
    "end": "451764"
  },
  {
    "text": "Yes? AUDIENCE: [UNINTELLIGIBLE] Can't this runtime figure that\nspawning an extra child would",
    "start": "451765",
    "end": "458444"
  },
  {
    "text": "be more expensive? Can't it like look at\nthis and be like-- PROFESSOR: We'll go into it.",
    "start": "458444",
    "end": "463450"
  },
  {
    "text": "I'll show you how it works\nlater in the lecture. I'll show you how it works and\nthen we can talk about what",
    "start": "463450",
    "end": "470169"
  },
  {
    "text": "knobs you have to tune, OK? So it's helpful to have\nan execution model for",
    "start": "470170",
    "end": "480350"
  },
  {
    "start": "474000",
    "end": "800000"
  },
  {
    "text": "something like this. And so we're going to look at\nan abstract execution model, which is basically asking what\ndoes the instruction trace",
    "start": "480350",
    "end": "489389"
  },
  {
    "text": "look like for this program? So normally when you execute a\nprogram, you can imagine one instruction executing\nafter the other.",
    "start": "489390",
    "end": "495940"
  },
  {
    "text": "And if it's a serial program,\nall those instructions essentially form a long chain.",
    "start": "495940",
    "end": "502470"
  },
  {
    "text": "Well there's a similar thing for\nparallel computers, which is that instead of a chain as\nyou'll see, it gets bushier",
    "start": "502470",
    "end": "510620"
  },
  {
    "text": "and it's going to be a directed\nacyclic graph. So let's take a look\nat how we do this. So we'll the example\nof fib of four.",
    "start": "510620",
    "end": "516534"
  },
  {
    "text": " So what we're going to do\nis start out here with a",
    "start": "516535",
    "end": "524920"
  },
  {
    "text": "rectangle here that I want you\nthink about as sort of a function call activation\nrecord.",
    "start": "524920",
    "end": "531810"
  },
  {
    "text": "So it's a record on a stack. It's got variables associated\nwith it. The only variable I'm going to\nkeep track of is n, so that's",
    "start": "531810",
    "end": "539270"
  },
  {
    "text": "what the four is there. OK, so we're going to\ndo fib of four. So we've got in this activation\nframe, we have the",
    "start": "539270",
    "end": "545390"
  },
  {
    "text": "variable four and now what I've\ndone is I've color coded the fib function here and into\nthe parts that are all serial.",
    "start": "545390",
    "end": "556100"
  },
  {
    "text": "So there's a serial part up to\nwhere it spawns, then there's",
    "start": "556100",
    "end": "562100"
  },
  {
    "text": "recursively calling the fib and\nthen there's returning. So there's sort of three parts\nto this function, each of",
    "start": "562100",
    "end": "567830"
  },
  {
    "text": "which is, in fact, a chain\nof serial instruction. I'm going to collapse those\nchains into a single circle",
    "start": "567830",
    "end": "574570"
  },
  {
    "text": "here that I'm going\nto call a strand. OK, now what we do is we execute\nthe strand, which",
    "start": "574570",
    "end": "580410"
  },
  {
    "text": "corresponds to executing the\ninstructions and advancing the program calendar up until\nthe point we hit this",
    "start": "580410",
    "end": "586899"
  },
  {
    "text": "fib of n minus 1. At that point, I basically\ncall fib of n minus 1.",
    "start": "586900",
    "end": "593699"
  },
  {
    "text": "So in this case, it's now\ngoing to be fib of 3. So that means I create a child\nand start executing in the",
    "start": "593700",
    "end": "603100"
  },
  {
    "text": "child, this prefix part\nof the function. However, unlike I were doing an\nordinary function call, I",
    "start": "603100",
    "end": "611140"
  },
  {
    "text": "would make this call and then\nthis guy would just sit here and wait until this\nframe was done.",
    "start": "611140",
    "end": "619650"
  },
  {
    "text": "But since it's a spawn, what\nhappens is I'm actually going to continue executing in the\nparent and execute, in fact,",
    "start": "619650",
    "end": "628070"
  },
  {
    "text": "the green part.  So in this case, evaluating\nthe arguments, etc.",
    "start": "628070",
    "end": "635640"
  },
  {
    "text": "Then it's going to spawn here,\nbut this guy, in fact, is going to what it does when it\ngets here is it evaluates n",
    "start": "635640",
    "end": "642490"
  },
  {
    "text": "minus 2, it does a call\nof fib of n minus 2.",
    "start": "642490",
    "end": "648520"
  },
  {
    "text": "So I've indicated that this was\na called frame by showing it in a light color.",
    "start": "648520",
    "end": "653580"
  },
  {
    "text": "So these are spawn, spawn,\ncall, meanwhile this thing is going. So at this point, we now have\none, two, three things that",
    "start": "653580",
    "end": "662780"
  },
  {
    "text": "are operating in parallel\nat the same time. We keep going on, OK?",
    "start": "662780",
    "end": "669420"
  },
  {
    "text": "So this guy that does a spawn\nand has a continuation, this one does a call, but while\nhe's doing a call, he's",
    "start": "669420",
    "end": "676510"
  },
  {
    "text": "waiting for the return\nso he doesn't start executing the successor. He stalled at the\nCilk sink here.",
    "start": "676510",
    "end": "682920"
  },
  {
    "text": "And we keep executing and so\nas you can see, what's",
    "start": "682920",
    "end": "688550"
  },
  {
    "text": "happening is we're actually\ncreating a directed acyclic graph of these strands.",
    "start": "688550",
    "end": "693839"
  },
  {
    "text": "So here basically, this guy was\nable to execute because both of the children, one that\nhe had spawned and one that he",
    "start": "693840",
    "end": "700875"
  },
  {
    "text": "had called, have returned. And so this fella, therefore,\nis able then to execute the return.",
    "start": "700875",
    "end": "707880"
  },
  {
    "text": "OK, so the addition of x plus y\nin particular, and then the return to the parent.",
    "start": "707880",
    "end": "713190"
  },
  {
    "text": "And so what we end up with is of\nall these serial chains of instructions that are\nrepresented by these strands,",
    "start": "713190",
    "end": "719890"
  },
  {
    "text": "all these circles, they're\nembedded in the call tree like you would have in an ordinary\nserial execution.",
    "start": "719890",
    "end": "727670"
  },
  {
    "text": "You have a call tree that you\nexecute up and down, you walk it like a stack normally.",
    "start": "727670",
    "end": "734370"
  },
  {
    "text": "Now, in fact, what we have is\nembedded in there is the parallel execution which form a\nDAG, directed acyclic graph.",
    "start": "734370",
    "end": "746120"
  },
  {
    "text": " So when you start thinking in\nparallel, you have to start",
    "start": "746120",
    "end": "751670"
  },
  {
    "text": "thinking about the DAG as your\nexecution model, not a chain of instructions.",
    "start": "751670",
    "end": "757900"
  },
  {
    "text": " And the nice thing about this\nparticular execution model we're going to be looking at is\nnowhere did I say how many",
    "start": "757900",
    "end": "765190"
  },
  {
    "text": "processors we were running on. This is a processor\noblivious model. It doesn't know how many\nprocessors you're running on.",
    "start": "765190",
    "end": "772509"
  },
  {
    "text": " We simply in the execution\nmodel, are thinking about",
    "start": "772510",
    "end": "782110"
  },
  {
    "text": "abstractly what can run in\nparallel, not what actually",
    "start": "782110",
    "end": "787420"
  },
  {
    "text": "does run in parallel\nin an execution. So any questions about\nthis execution model?",
    "start": "787420",
    "end": "793620"
  },
  {
    "start": "793620",
    "end": "799260"
  },
  {
    "text": "OK. So just so that we have some\nterminology, so the parallel",
    "start": "799260",
    "end": "810620"
  },
  {
    "start": "800000",
    "end": "912000"
  },
  {
    "text": "instruction stream is a DAG\nwith vertices and edges. ",
    "start": "810620",
    "end": "816399"
  },
  {
    "text": "Each vertex is a strand, OK? Which is a sequence of\ninstructions not containing a",
    "start": "816400",
    "end": "822570"
  },
  {
    "text": "call spawn sync, a return or\nthrown exception, if you're doing exceptions.",
    "start": "822570",
    "end": "829480"
  },
  {
    "text": "We're not going to really talk\nabout exceptions much. So they are supported in the\nsoftware that we'll be using,",
    "start": "829480",
    "end": "838079"
  },
  {
    "text": "but for most part, we're\nnot going to have to worry about them.",
    "start": "838080",
    "end": "843170"
  },
  {
    "text": "OK so there's an initial strand\nwhere you start, and a final strand where you end.",
    "start": "843170",
    "end": "849990"
  },
  {
    "text": "Then each edge is a spawn or\na call or return or what's",
    "start": "849990",
    "end": "856950"
  },
  {
    "text": "called a continue edge or a\ncontinuation edge, which goes from the parent, when a parent\nspawns something to the next",
    "start": "856950",
    "end": "867430"
  },
  {
    "text": "instruction after the spawn. ",
    "start": "867430",
    "end": "874380"
  },
  {
    "text": "So we can classify the edges\nin that fashion. And I've only explain this for\nspawm and sync, as you recall",
    "start": "874380",
    "end": "883639"
  },
  {
    "text": "from last time, we also talked\nabout Cilk four. It turns out Cilk four is\nconverted to spawns and syncs",
    "start": "883640",
    "end": "889949"
  },
  {
    "text": "using a recursive divide\nand conquer approach. We'll talk about that next\ntime on Thursday.",
    "start": "889950",
    "end": "896515"
  },
  {
    "text": "So we'll talk more about Cilk\nfour and how it's implemented and the implications of how\nloop parallelism works.",
    "start": "896515",
    "end": "903390"
  },
  {
    "text": "So at the fundamental level,\nthe runtime system is only concerned about spawns\nand syncs. ",
    "start": "903390",
    "end": "912190"
  },
  {
    "start": "912000",
    "end": "1330000"
  },
  {
    "text": "Now given that we have a DAG,\nso I've taken away the call tree and just left the strands\nof a computation.",
    "start": "912190",
    "end": "918480"
  },
  {
    "text": "It's actually not the same as\nthe computation we saw before. We would like to understand,\nis this a good parallel",
    "start": "918480",
    "end": "926260"
  },
  {
    "text": "program or not? Based on if I understand\nthe logical parallelism that I've exposed.",
    "start": "926260",
    "end": "932690"
  },
  {
    "text": "So how much parallelism do\nyou think is in here? ",
    "start": "932690",
    "end": "937850"
  },
  {
    "text": "Give me a number. How many processors does it\nmake sense to run this on?",
    "start": "937850",
    "end": "943470"
  },
  {
    "text": "Five?  That's as parallel as it gets.",
    "start": "943470",
    "end": "949080"
  },
  {
    "text": "Let's take a look. We're going to do an analysis. At the end of it, we'll know\nwhat the answer is.",
    "start": "949080",
    "end": "955620"
  },
  {
    "text": "So for that, let tp be the\nexecution time on p processors",
    "start": "955620",
    "end": "961560"
  },
  {
    "text": "for this particular program. ",
    "start": "961560",
    "end": "966600"
  },
  {
    "text": "It turns but there\nare two measures that are really important. The first is called the work. OK, so of course, we know\nthat real machines",
    "start": "966600",
    "end": "974900"
  },
  {
    "text": "have caches, etc. Let's forget all of that. Just very simple algorithmic\nmodel where every strand,",
    "start": "974900",
    "end": "982320"
  },
  {
    "text": "let's say, costs us unit time\nas opposed to in practice, they may be many instructions\nand so forth.",
    "start": "982320",
    "end": "988880"
  },
  {
    "text": "We can take that into account. Let's take that into\naccount separately. So T1 is the work.",
    "start": "988880",
    "end": "995390"
  },
  {
    "text": "It's the time it if I had to\nexecute it on one processor, I've got to do all the\nwork that's in here.",
    "start": "995390",
    "end": "1003450"
  },
  {
    "text": "So what's the work of this\nparticular computation? ",
    "start": "1003450",
    "end": "1011850"
  },
  {
    "text": "I think it's 18, right? Yeah, 18. So T1 is the work. So even though I'm executing a\nparallel, I could it execute",
    "start": "1011850",
    "end": "1019279"
  },
  {
    "text": "it serially and then T1 is the\namount of work it would take.",
    "start": "1019280",
    "end": "1026439"
  },
  {
    "text": "The other measure is called the\nspan, and sometimes called critical path length or\ncomputational depth.",
    "start": "1026440",
    "end": "1033490"
  },
  {
    "text": "And it corresponds to the\nlongest path of dependencies in the DAG.",
    "start": "1033490",
    "end": "1040349"
  },
  {
    "text": "We call it T infinity because\neven if you had an infinite number of processors, you still\ncan't do this one until",
    "start": "1040349",
    "end": "1047159"
  },
  {
    "text": "you finish that one. You can't do this one until you\nfinish that one, can't do this one till you've finished\nthat one and so forth.",
    "start": "1047160",
    "end": "1053800"
  },
  {
    "text": "So even with an infinite number\nof processors, I still wouldn't go faster\nthan the span. So that's why we denote\nby T infinity.",
    "start": "1053800",
    "end": "1060345"
  },
  {
    "text": " So these are the two\nimportant measures. Now what we're really\ninterested in is",
    "start": "1060345",
    "end": "1066250"
  },
  {
    "text": "Tp for a given p. As you'll see, we actually can\nget some bounds on the",
    "start": "1066250",
    "end": "1074080"
  },
  {
    "text": "performance on p processors just\nby looking at the work, the span and the number of\nprocessors we're executing on.",
    "start": "1074080",
    "end": "1082660"
  },
  {
    "text": "So the first bound is\nthe following, it's called the Work Law. The Work Law says that the time\non p processors is at",
    "start": "1082660",
    "end": "1089909"
  },
  {
    "text": "least the time on one processor\ndivided by p. So why does that Work\nLaw make sense?",
    "start": "1089910",
    "end": "1097190"
  },
  {
    "text": "What's that saying? ",
    "start": "1097190",
    "end": "1102879"
  },
  {
    "text": "Sorry? AUDIENCE: Like work is\nconserved sort of? I mean, you have to do the\nsame amount of work.",
    "start": "1102880",
    "end": "1108910"
  },
  {
    "text": "PROFESSOR: You have to do the\nsame amount of work, so on every time step, you can get\np pieces of work done.",
    "start": "1108910",
    "end": "1115350"
  },
  {
    "text": "So if you're running for fewer\nthan T1 over p steps, you've done less than T1 work\nover and time Tp.",
    "start": "1115350",
    "end": "1126440"
  },
  {
    "text": "So you won't have done all\nthe work if you run for less than this. So the time must be at least\nTp, time Tp must be at",
    "start": "1126440",
    "end": "1135250"
  },
  {
    "text": "least T1 over p. You only get to do p\nwork on one step.",
    "start": "1135250",
    "end": "1140365"
  },
  {
    "text": "Is that pretty clear? The second one should be even\nclearer, the Span Law.",
    "start": "1140365",
    "end": "1146880"
  },
  {
    "text": "On p processors, you're not\ngoing to go faster than if you had an infinite number of\nprocessors because the",
    "start": "1146880",
    "end": "1153330"
  },
  {
    "text": "infinite processor could always\nuse fewer processors if it's scheduled. Once again, this is a\nvery simple model.",
    "start": "1153330",
    "end": "1158800"
  },
  {
    "text": "We're not taking into account\nscheduling, we're not taking into account overheads or\nwhatever, just a simple",
    "start": "1158800",
    "end": "1164190"
  },
  {
    "text": "conceptual model for\nunderstanding parallelism. So any questions about\nthese two laws?",
    "start": "1164190",
    "end": "1170535"
  },
  {
    "text": " There's going to be a couple\nof formulas in this lecture",
    "start": "1170535",
    "end": "1176470"
  },
  {
    "text": "today that you should write\ndown and play with. So these two, they may seem\nsimple, but these are hugely",
    "start": "1176470",
    "end": "1184280"
  },
  {
    "text": "important formulas. So you should know that Tp is at\nleast T1 over p, that's the",
    "start": "1184280",
    "end": "1189860"
  },
  {
    "text": "Work Law and that Tp is\nat least T infinity. Those are bounds on how fast\nyou could execute.",
    "start": "1189860",
    "end": "1195480"
  },
  {
    "text": "Do I have a question\nin that back there? OK so let's see what happens\nto work in span in terms of",
    "start": "1195480",
    "end": "1207330"
  },
  {
    "text": "how we can understand our\nprograms and decompose them. So suppose that I have a\ncomputation A followed by",
    "start": "1207330",
    "end": "1215200"
  },
  {
    "text": "computation B and I connect\nthem in series. What happens to the work?",
    "start": "1215200",
    "end": "1221580"
  },
  {
    "text": "How does the work of all this\nwhole thing correspond to the",
    "start": "1221580",
    "end": "1226690"
  },
  {
    "text": "work of A and the work of B? ",
    "start": "1226690",
    "end": "1232840"
  },
  {
    "text": "What's that? AUDIENCE: [UNINTELLIGIBLE] PROFESSOR: Yeah, add\nthem together. You get T1 of A plus T1 of B.\nTake the work of this and the",
    "start": "1232840",
    "end": "1241649"
  },
  {
    "text": "work of this. OK, that's pretty easy. What about the span? ",
    "start": "1241650",
    "end": "1250289"
  },
  {
    "text": "So the span is the longest\npath of dependencies. ",
    "start": "1250290",
    "end": "1255850"
  },
  {
    "text": "What happens to the span\nwhen I connect two things in a series? ",
    "start": "1255850",
    "end": "1261130"
  },
  {
    "text": "Yeah, it just sums as well\nbecause I take whatever the longest path is from here to\nhere and then the longest one",
    "start": "1261130",
    "end": "1268050"
  },
  {
    "text": "from here to here,\nit just adds. But now let's look at parallel\ncomposition, So now suppose",
    "start": "1268050",
    "end": "1275370"
  },
  {
    "text": "that I can execute these\ntwo things in parallel. What happens to the work? ",
    "start": "1275370",
    "end": "1285160"
  },
  {
    "text": "It just adds, just as before. The work always adds. The work is easy because\nit's additive.",
    "start": "1285160",
    "end": "1292909"
  },
  {
    "text": "What happens to the span? ",
    "start": "1292910",
    "end": "1300560"
  },
  {
    "text": "What's that? AUDIENCE: [UNINTELLIGIBLE] PROFESSOR: It's the\nmax of the spans. Right, so whatever is the\nlongest, whichever one of",
    "start": "1300560",
    "end": "1309860"
  },
  {
    "text": "these ones has a longer span,\nthat's going to be the span of the total.",
    "start": "1309860",
    "end": "1315669"
  },
  {
    "text": "Does that give you some\nIntuition So we're going to see when we analyze the spans of\nthings that in fact, we're",
    "start": "1315670",
    "end": "1324390"
  },
  {
    "text": "going to see maxes occurring\nall over the place. ",
    "start": "1324390",
    "end": "1329570"
  },
  {
    "text": "So speedup is defined\nto be T1 over Tp.",
    "start": "1329570",
    "end": "1337809"
  },
  {
    "start": "1330000",
    "end": "1410000"
  },
  {
    "text": "So speedup is how much faster am\nI on p processors than I am on one processor?",
    "start": "1337810",
    "end": "1343360"
  },
  {
    "text": " Pretty easy. So if T1 over Tp is equal to\np, we say we have perfect",
    "start": "1343360",
    "end": "1351050"
  },
  {
    "text": "linear speedup, or\nlinear speedup. ",
    "start": "1351050",
    "end": "1356840"
  },
  {
    "text": "That's good, right? Because if I put on use p\nprocessors, I'd like to have",
    "start": "1356840",
    "end": "1363970"
  },
  {
    "text": "things go p times faster. OK, that would be\nthe ideal world.",
    "start": "1363970",
    "end": "1369530"
  },
  {
    "text": "If T1 over Tp, which is the\nspeedup, is greater than p,",
    "start": "1369530",
    "end": "1375720"
  },
  {
    "text": "that says we have super\nlinear speedup. And in our model, we don't get\nthat because of the work law.",
    "start": "1375720",
    "end": "1381590"
  },
  {
    "text": "Because the work law says Tp is\ngreater than or equal to T1 over p and just do a little\nalgebra here, you get T1 over",
    "start": "1381590",
    "end": "1389580"
  },
  {
    "text": "Tp must be less than\nor equal to p. So you can't get super\nlinear speedup.",
    "start": "1389580",
    "end": "1395500"
  },
  {
    "text": "In practice, there are\nsituations where you can get super linear speedup due\nto caching effects and",
    "start": "1395500",
    "end": "1401419"
  },
  {
    "text": "a variety of things. We'll talk about some\nof those things. But in this simple model,\nwe don't get",
    "start": "1401420",
    "end": "1407470"
  },
  {
    "text": "that kind of behavior. And of course, the case I left\nout is the common case, which",
    "start": "1407470",
    "end": "1414390"
  },
  {
    "start": "1410000",
    "end": "1965000"
  },
  {
    "text": "is the T1 over Tp is less than\np, and that's very common people write code which doesn't",
    "start": "1414390",
    "end": "1420370"
  },
  {
    "text": "give them linear speedup. We're mostly interested in\ngetting linear speedup here.",
    "start": "1420370",
    "end": "1427059"
  },
  {
    "text": "That's our goal. So that we're getting the most\nbang for the buck out of the processors we're using.",
    "start": "1427060",
    "end": "1435110"
  },
  {
    "text": "OK, parallelism. So we're finally to the point\nwhere I can talk about parallelism and give a\nquantitative definition of",
    "start": "1435110",
    "end": "1444050"
  },
  {
    "text": "parallelism. So the Span Law says that Tp is\nat least T infinity, right?",
    "start": "1444050",
    "end": "1451950"
  },
  {
    "text": "The time on p processors is at\nleast the time on an infinite number of processors. So the maximum possible speedup,\nthat's T1 over Tp,",
    "start": "1451950",
    "end": "1461390"
  },
  {
    "text": "given T1 and T infinity\nis T1 over T infinity. ",
    "start": "1461390",
    "end": "1469020"
  },
  {
    "text": "And we call that the\nparallelism.  It's the maximum amount\nof speedup we",
    "start": "1469020",
    "end": "1477050"
  },
  {
    "text": "could possibly attain.  So we have the speedup and the\nspeedup by the Span Law that",
    "start": "1477050",
    "end": "1485220"
  },
  {
    "text": "says this is the maximum amount\nwe can get, we could also view it as if I look along\nthe critical path of the",
    "start": "1485220",
    "end": "1493659"
  },
  {
    "text": "computation. It's sort of what's the average\namount of work at every level.",
    "start": "1493660",
    "end": "1499302"
  },
  {
    "text": "The work, the total amount of\nstuff here divided by that length there that sort of tells\nus the width, what's the",
    "start": "1499302",
    "end": "1505500"
  },
  {
    "text": "average amount of stuff that's\ngoing on in every step. So for this example,\nwhat is the--",
    "start": "1505500",
    "end": "1512700"
  },
  {
    "text": "I forgot to put this\non my slide-- what is the parallelism of\nthis particular DAG here?",
    "start": "1512700",
    "end": "1521770"
  },
  {
    "start": "1521770",
    "end": "1526830"
  },
  {
    "text": "Two, right? So the span has length nine--\nthis is assuming everything",
    "start": "1526830",
    "end": "1532460"
  },
  {
    "text": "was unit time-- obviously in reality, when you\nhave more instructions, you in",
    "start": "1532460",
    "end": "1537779"
  },
  {
    "text": "fact would make it be whatever\nthe length of this was in",
    "start": "1537780",
    "end": "1542920"
  },
  {
    "text": "terms of number of instructions\nor what have you, of execution time of\nall these things.",
    "start": "1542920",
    "end": "1548160"
  },
  {
    "text": "So this is length 9, there's\n18 things here, parallelism is 2.",
    "start": "1548160",
    "end": "1554660"
  },
  {
    "text": "So we can quantify parallelism\nprecisely. We'll see why it's important\nto quantify it. So that the maximum speedup\nwe're going to get when we run",
    "start": "1554660",
    "end": "1562210"
  },
  {
    "text": "this application. Here's another example\nwe did before. Fib of four.",
    "start": "1562210",
    "end": "1569929"
  },
  {
    "text": "So let's assume again that\neach strand takes unit time to execute.",
    "start": "1569930",
    "end": "1576320"
  },
  {
    "text": "So what is the work in this\nparticular computation?",
    "start": "1576320",
    "end": "1582225"
  },
  {
    "start": "1582225",
    "end": "1591679"
  },
  {
    "text": "Assume every strand takes unit\ntime to execute, which of course it doesn't, but-- ",
    "start": "1591680",
    "end": "1608190"
  },
  {
    "text": "anybody care to hazard\na guess?  17, yeah, because there's four\nnodes here that have 3 plus 5.",
    "start": "1608190",
    "end": "1619320"
  },
  {
    "text": "So 3 times 4 plus 5 is 17. So the work is 17.",
    "start": "1619320",
    "end": "1626669"
  },
  {
    "text": "OK, what's the span? ",
    "start": "1626670",
    "end": "1632590"
  },
  {
    "text": "This one's tricky. ",
    "start": "1632590",
    "end": "1641370"
  },
  {
    "text": "Too bad it's not a little\nbit more focused. ",
    "start": "1641370",
    "end": "1647269"
  },
  {
    "text": "What the span? AUDIENCE: 8. PROFESSOR: 8, that's correct.",
    "start": "1647270",
    "end": "1652610"
  },
  {
    "text": "Who got 7? Yeah, so I got 7 when I did this\nand then I looked harder",
    "start": "1652610",
    "end": "1659450"
  },
  {
    "text": "and it was 8. It's 8, so here it is. Here's the span.",
    "start": "1659450",
    "end": "1666410"
  },
  {
    "text": "There is goes. Ooh that little sidestep there,\nthat's what makes it 8.",
    "start": "1666410",
    "end": "1671667"
  },
  {
    "text": " OK so basically, it comes down\nhere and I had gone down like",
    "start": "1671667",
    "end": "1679159"
  },
  {
    "text": "that when I did it, but in fact,\nyou've got to go over and back up. So it's actually 8.",
    "start": "1679160",
    "end": "1686450"
  },
  {
    "text": "So that says that the\nparallelism is a little bit",
    "start": "1686450",
    "end": "1692620"
  },
  {
    "text": "more than 2, 2 and 1/8. What that says is that if\nI use many more than two",
    "start": "1692620",
    "end": "1699370"
  },
  {
    "text": "processors, I can't get linear\nspeedup anymore.",
    "start": "1699370",
    "end": "1704970"
  },
  {
    "text": "I'm only going to get marginal\nperformance gains. If I use more than 2, because\nthe maximum speedup I can get",
    "start": "1704970",
    "end": "1711530"
  },
  {
    "text": "is like 2.125 if I had an\ninfinite number of processors. ",
    "start": "1711530",
    "end": "1719120"
  },
  {
    "text": "So any questions about this? So this by the way deceptively\nsimple and yet, if you don't",
    "start": "1719120",
    "end": "1726960"
  },
  {
    "text": "play around with it a little\nbit, you can get confused very easily.",
    "start": "1726960",
    "end": "1733350"
  },
  {
    "text": "Deceptively simple,\nvery powerful to be able to do this. ",
    "start": "1733350",
    "end": "1741740"
  },
  {
    "text": "So here we have for the analysis\nof parallelism, one of the things that we have going\nfor us in using the Cilk",
    "start": "1741740",
    "end": "1749170"
  },
  {
    "text": "tool suite is a program called\nCilkview, which has a scalability analyzer.",
    "start": "1749170",
    "end": "1757110"
  },
  {
    "text": "And it is like the race detector\nthat I talked to you about last time in that it uses\ndynamic instrumentation.",
    "start": "1757110",
    "end": "1763780"
  },
  {
    "text": "So you run it under Cilkview,\nit's like running it under [? Valgrhen ?] for example, or what have you.",
    "start": "1763780",
    "end": "1770960"
  },
  {
    "text": "So basically you run your\nprogram under it, and it analyzes your program\nfor scalability.",
    "start": "1770960",
    "end": "1777040"
  },
  {
    "text": "It computes the work and span of\nyour program to derive some upper bounds on parallel\nperformance and it also",
    "start": "1777040",
    "end": "1784630"
  },
  {
    "text": "estimates a scheduling overhead\nto compute what's called a burden span\nfor lower bounds. ",
    "start": "1784630",
    "end": "1792710"
  },
  {
    "text": "So let's take a look. So here's, for example, here's\na quick sort program.",
    "start": "1792710",
    "end": "1798690"
  },
  {
    "text": "So let's just see this\nis a c++ program. So here we're using a template\nso that the type of items that",
    "start": "1798690",
    "end": "1806260"
  },
  {
    "text": "I'm sorting I can make\nbe a variable. So tightening-- can we shut\nthe back door there?",
    "start": "1806260",
    "end": "1813000"
  },
  {
    "text": "One of the TAs? Somebody run up to--  thank you.",
    "start": "1813000",
    "end": "1818460"
  },
  {
    "text": " So we have the variable T And\nwe're going to quick sort from",
    "start": "1818460",
    "end": "1825380"
  },
  {
    "text": "the beginning to the\nend of the array. And what we do is, just as\nyou're familiar with quick",
    "start": "1825380",
    "end": "1831340"
  },
  {
    "text": "sort, if there's actually\nsomething to be sorted, more than one thing, then we find the\nmiddle by partitioning the",
    "start": "1831340",
    "end": "1839590"
  },
  {
    "text": "thing and this is a bit\nof a c++ magic to find the middle element.",
    "start": "1839590",
    "end": "1845200"
  },
  {
    "text": "And then the important part\nfrom our point of view is after we've done this partition,\nwe quick sort the",
    "start": "1845200",
    "end": "1850539"
  },
  {
    "text": "first part of the array, from\nbeginning to middle and then from the beginning plus 1 or\nthe middle, whichever is",
    "start": "1850540",
    "end": "1860170"
  },
  {
    "text": "greater to the end. And then we sync. So what we're doing is quick\nsort where we're spawning off",
    "start": "1860170",
    "end": "1867350"
  },
  {
    "text": "the two sub problems to\nbe solved in parallel recursively. So they're going to execute in\nparallel and they're going to",
    "start": "1867350",
    "end": "1874429"
  },
  {
    "text": "execute in parallel\nand so forth. So a fairly natural thing to\ndivide, to do divide and",
    "start": "1874430",
    "end": "1880480"
  },
  {
    "text": "conquer on quick sort because\nthe two some problems can be operated on independently.",
    "start": "1880480",
    "end": "1885580"
  },
  {
    "text": "We just sort them recursively. But we can sort them\nin parallel. OK, so suppose that we are\nsorting 100,000 numbers.",
    "start": "1885580",
    "end": "1894010"
  },
  {
    "text": "How much parallelism do you\nthink is in this code? ",
    "start": "1894010",
    "end": "1906770"
  },
  {
    "text": "So remember that we're getting\nthis recursive stuff done. How many people think--",
    "start": "1906770",
    "end": "1913854"
  },
  {
    "text": "well, it's not going\nto be more than 100,000, I promise you. So how many people think more\nthan a million parallels?",
    "start": "1913854",
    "end": "1920500"
  },
  {
    "text": "Raise your hand, more\nthan a million? And how many people think\nmore than 100,000?",
    "start": "1920500",
    "end": "1929370"
  },
  {
    "text": "And how many people think\nmore than 10,000? OK, between the two.",
    "start": "1929370",
    "end": "1934720"
  },
  {
    "text": " More than 1,000?",
    "start": "1934720",
    "end": "1941050"
  },
  {
    "text": "OK, how about more than 100? 100 to 1,000? How about 10 to 100?",
    "start": "1941050",
    "end": "1946790"
  },
  {
    "text": " How about between 1 and 10? ",
    "start": "1946790",
    "end": "1954380"
  },
  {
    "text": "So a lot of people think\nbetween 1 and 10. Why do you think that there's\nso little parallels in this? ",
    "start": "1954380",
    "end": "1962700"
  },
  {
    "text": "You don't have to justify\nyourself, OK. Well let's see how much there\nis according to Cilkview.",
    "start": "1962700",
    "end": "1969760"
  },
  {
    "start": "1965000",
    "end": "2152000"
  },
  {
    "text": "So here's the type of output\nthat you'll get. You'll get a graphical curve. You'll also get a\ntextual output.",
    "start": "1969760",
    "end": "1975429"
  },
  {
    "text": "But this is sort of the\ngraphical output. And this is basically showing\nwhat the running time here is.",
    "start": "1975430",
    "end": "1980909"
  },
  {
    "text": "So the first thing it shows is\nit will actually run your program, benchmark your program,\non in this case, up",
    "start": "1980910",
    "end": "1986950"
  },
  {
    "text": "to 8 course. We ran it. So we ran up to 8 course and\ngive you what your measured",
    "start": "1986950",
    "end": "1995260"
  },
  {
    "text": "speedup is. So the second thing is it\ntells you the parallels. If you can't read that\nit's, 11.21.",
    "start": "1995260",
    "end": "2004760"
  },
  {
    "text": "So we get about 11. Why do you think it's\nnot higher?",
    "start": "2004760",
    "end": "2010140"
  },
  {
    "start": "2010140",
    "end": "2015880"
  },
  {
    "text": "What's that? AUDIENCE: It's the log. PROFESSOR: What's the log? AUDIENCE: [UNINTELLIGIBLE]",
    "start": "2015880",
    "end": "2021140"
  },
  {
    "text": " PROFESSOR: Yeah, but you're\ndoing the two things in",
    "start": "2021140",
    "end": "2027570"
  },
  {
    "text": "parallel, right? We'll actually analyze this. So it has to do with the fact\nthat the partition routine is",
    "start": "2027570",
    "end": "2033500"
  },
  {
    "text": "a serial piece of code\nand it's big. So the initial partitioning\ntakes you 100,000--",
    "start": "2033500",
    "end": "2040236"
  },
  {
    "text": "sorry, 100 million steps\nof doing a partition-- before you get to do any\nparallelism at all.",
    "start": "2040236",
    "end": "2046789"
  },
  {
    "text": "And we'll see that\nin just a minute. So it gives you the\nparallelism. It also plots this.",
    "start": "2046790",
    "end": "2052260"
  },
  {
    "text": "So this is the parallelism. Notice that's the same\nnumber, 11.21 is plotted as this bound.",
    "start": "2052260",
    "end": "2060260"
  },
  {
    "text": "So it tells you the span law and\nit tells you the work law. This is the linear speedup.",
    "start": "2060260",
    "end": "2065980"
  },
  {
    "text": "If you were having linear\nspeedup, this is what your program would give you. So it gives you these two\nbounds, the work law and span",
    "start": "2065980",
    "end": "2073250"
  },
  {
    "text": "law on your speedup. And then it also computes\nwhat's called a burden",
    "start": "2073250",
    "end": "2079460"
  },
  {
    "text": "parallelism, estimating\nscheduling overheads to sort of give you a lower bound.",
    "start": "2079460",
    "end": "2084699"
  },
  {
    "text": "Now that's not to say that\nyour numbers can't fall outside this range. But when they do, it will tell\nyou essentially what the",
    "start": "2084699",
    "end": "2091989"
  },
  {
    "text": "issues are with your program. And we'll discuss how you\ndiagnose some of those issues.",
    "start": "2091989",
    "end": "2097849"
  },
  {
    "text": "Actually that's in one of the\nhandouts that we've provided.",
    "start": "2097850",
    "end": "2105400"
  },
  {
    "text": "I think that's in one\nof the handouts. If not, we'll make sure it's\namong the handouts. So basically, this gives you a\nrange for what you can expect.",
    "start": "2105400",
    "end": "2113609"
  },
  {
    "text": "So the important thing here is\nto notice here for example, that we're losing performance,\nbut it's not due to the",
    "start": "2113610",
    "end": "2120200"
  },
  {
    "text": "parallelism, to the work law. Basically, in some sense, what's\nhappening is we are",
    "start": "2120200",
    "end": "2128230"
  },
  {
    "text": "losing it because the Span Law\nbecause we're starting to approach the point where the\nspan is going to be the issue.",
    "start": "2128230",
    "end": "2135529"
  },
  {
    "text": "So we'll talk more about this. So the main thing is you have\na tool that can tell you the work and span and so that you\ncan analyze your own programs",
    "start": "2135530",
    "end": "2143549"
  },
  {
    "text": "to understand are you bounded\nby parallelism, for example, in particular, in the code\nthat you've written.",
    "start": "2143550",
    "end": "2153040"
  },
  {
    "text": "OK let's do a theoretical\nanalysis of this to understand why that number is small.",
    "start": "2153040",
    "end": "2159589"
  },
  {
    "text": "So the main thing here is that\nthe expected work, as you recall, of quick sort\nis order n log n.",
    "start": "2159590",
    "end": "2165900"
  },
  {
    "text": "You tend to do order n log n\nwork, you partition and then you're solving two problems\nof the same size.",
    "start": "2165900",
    "end": "2171470"
  },
  {
    "text": "If you actually draw out the\nrecursion tree, it's log height with linear amount of\nwork on every level for n log",
    "start": "2171470",
    "end": "2177130"
  },
  {
    "text": "end total work. The expected span, however, is\norder n because the partition",
    "start": "2177130",
    "end": "2184610"
  },
  {
    "text": "routine is a serial program that\npartitions up the thing of size n in order n time.",
    "start": "2184610",
    "end": "2192090"
  },
  {
    "text": "So when you compute the\nparallelism, you get parallelism of order log n\nand log n is kind of puny",
    "start": "2192090",
    "end": "2198830"
  },
  {
    "text": "parallelism, and that's our\ntechnical word for it. So puny parallelism is what\nwe get out of quick sort.",
    "start": "2198830",
    "end": "2204769"
  },
  {
    "text": " So it turns out there\nare lots of things",
    "start": "2204770",
    "end": "2210560"
  },
  {
    "text": "that you can analyze. Here's just a selection of\nsome of the interesting practical algorithms and the\nkinds of analyses that you can",
    "start": "2210560",
    "end": "2218200"
  },
  {
    "text": "do showing that, for example,\nwith merge sort you can do it with work n log n.",
    "start": "2218200",
    "end": "2223380"
  },
  {
    "text": "You can get a span of log qn and\nso then the parallelism is",
    "start": "2223380",
    "end": "2229009"
  },
  {
    "text": "the ratio of the two.  In fact, you can actually\ntheoretically get log squared",
    "start": "2229010",
    "end": "2235400"
  },
  {
    "text": "n span, but that's not as\npractical an algorithm as the one that gives you\nlog cubed n.",
    "start": "2235400",
    "end": "2240930"
  },
  {
    "text": "And you can go through and there\nare a whole bunch of algorithms for which you can\nget very good parallelism.",
    "start": "2240930",
    "end": "2249080"
  },
  {
    "text": "So all of these, if you look\nat the ratio of these, the parallelism is quite high. ",
    "start": "2249080",
    "end": "2255510"
  },
  {
    "text": "So let's talk a little bit\nabout what's going on underneath and why parallelism\nis important.",
    "start": "2255510",
    "end": "2261579"
  },
  {
    "text": "So when you describe your\nprogram in Cilk, you express",
    "start": "2261580",
    "end": "2268020"
  },
  {
    "text": "the potential parallelism\nof your application.",
    "start": "2268020",
    "end": "2273810"
  },
  {
    "text": "You don't say exactly how it's\ngoing to be scheduled, that's done by the Cilk++ scheduler,\nwhich maps the strands",
    "start": "2273810",
    "end": "2280980"
  },
  {
    "text": "dynamically onto the processors\nat run time. So it's going to do the load\nbalancing and everything",
    "start": "2280980",
    "end": "2288040"
  },
  {
    "text": "necessary to balance your\ncomputation off the number of processors. We want to understand how that\nprocess works, because that's",
    "start": "2288040",
    "end": "2295890"
  },
  {
    "text": "going to help us to understand\nhow it is that we can build codes that will map very\neffectively on to the number",
    "start": "2295890",
    "end": "2303069"
  },
  {
    "text": "of processors. Now it turns out that the theory\nof the distributed schedulers such as is in\nCilk++ is complicated.",
    "start": "2303070",
    "end": "2313150"
  },
  {
    "text": "I'll wave my hands about it\ntowards the end, but the analysis of it is advanced.",
    "start": "2313150",
    "end": "2320280"
  },
  {
    "text": "You have to take a graduate\ncourse to get that stuff. So instead, we're going to\nexplore the ideas with a",
    "start": "2320280",
    "end": "2326560"
  },
  {
    "text": "centralized, much simpler,\nscheduler which serves as a",
    "start": "2326560",
    "end": "2332600"
  },
  {
    "text": "surrogate for understanding\nwhat's going on. ",
    "start": "2332600",
    "end": "2338010"
  },
  {
    "start": "2336000",
    "end": "2522000"
  },
  {
    "text": "So the basic idea of almost all\nscheduling theory in this",
    "start": "2338010",
    "end": "2343380"
  },
  {
    "text": "domain is greedy scheduling. And so this is-- by the way,\nwe're coming to the second",
    "start": "2343380",
    "end": "2349359"
  },
  {
    "text": "thing you have to understand\nreally well in order to be able to generate good code,\nthe second sort of theoretical thing--",
    "start": "2349360",
    "end": "2355369"
  },
  {
    "text": "so the idea of a greedy\nscheduler is you want to do as much work as possible\non each step.",
    "start": "2355370",
    "end": "2364700"
  },
  {
    "text": "So the idea here is let's take\na look, for example, suppose",
    "start": "2364700",
    "end": "2371490"
  },
  {
    "text": "that we've executed this part\nof the DAG already. Then there are certain number\nof strands that are ready to",
    "start": "2371490",
    "end": "2378829"
  },
  {
    "text": "execute, meaning all their\npredecessors have exited. How many strands are ready\nto execute on this DAG?",
    "start": "2378830",
    "end": "2386680"
  },
  {
    "text": "Five, right? These guys. So those five strands are\nready to execute.",
    "start": "2386680",
    "end": "2394560"
  },
  {
    "text": "So the idea is-- and let me\nillustrate for p equals 3--",
    "start": "2394560",
    "end": "2399810"
  },
  {
    "text": "the idea is to understand the\nexecution in terms of two types of steps.",
    "start": "2399810",
    "end": "2406119"
  },
  {
    "text": "So in a greed schedule, you\nalways do as much as possible. So is what would be called a\ncomplete step because I can",
    "start": "2406120",
    "end": "2412120"
  },
  {
    "text": "schedule all three processors\nto have some work to do on that step.",
    "start": "2412120",
    "end": "2418600"
  },
  {
    "text": "So which are the best three guys\nto be able to execute?",
    "start": "2418600",
    "end": "2423970"
  },
  {
    "text": " Yes, so I'm not sure what the\nbest three are, but for sure,",
    "start": "2423970",
    "end": "2430100"
  },
  {
    "text": "you want to get this guy\nand this guy, right? Maybe that guy's not,\nbut this guy, you definitely want to execute.",
    "start": "2430100",
    "end": "2438050"
  },
  {
    "text": "And these guys, I guess, OK. So in a greedy schedule, no,\nyou're not allowed to look to",
    "start": "2438050",
    "end": "2443200"
  },
  {
    "text": "see which ones are\nthe best execute. You don't know what the future\nis, the scheduler isn't going to know what the future is so it\njust executes any p course.",
    "start": "2443200",
    "end": "2454309"
  },
  {
    "start": "2454310",
    "end": "2462630"
  },
  {
    "text": "You just execute any p course. In this case, I executed\nthe p strand.",
    "start": "2462630",
    "end": "2467830"
  },
  {
    "text": "In this case, I executed these\nthree guys even though they weren't necessarily the best.",
    "start": "2467830",
    "end": "2472910"
  },
  {
    "text": "And in a greedy scheduler, it\ndoesn't look to see what's the best one to execute, it\njust executes as many",
    "start": "2472910",
    "end": "2480500"
  },
  {
    "text": "as it can this case. In this case, it's p. Now we have what's called\nan incomplete step. Notice nothing got enabled.",
    "start": "2480500",
    "end": "2485960"
  },
  {
    "text": "That was sort of too bad. So there's only two guys\nthat are ready to go. What do you think happens if\nI have an incomplete step,",
    "start": "2485960",
    "end": "2494630"
  },
  {
    "text": "namely p strands are\nready, fewer than p strands are ready? I just to execute all of\nthem, as many as I can.",
    "start": "2494630",
    "end": "2502390"
  },
  {
    "text": "Run all of them. So that's what a greedy\nscheduler does. Just at every step, it executes\nas many as it can and",
    "start": "2502390",
    "end": "2510090"
  },
  {
    "text": "we can classify the steps as\nones which are complete, meaning we used all our\nprocessors versus incomplete,",
    "start": "2510090",
    "end": "2517930"
  },
  {
    "text": "meaning we only used a subset\nof our processors in scheduling it.",
    "start": "2517930",
    "end": "2523540"
  },
  {
    "start": "2522000",
    "end": "2742000"
  },
  {
    "text": "So that's what a greedy\nscheduler does. Now the important thing,\nwhich is the",
    "start": "2523540",
    "end": "2529440"
  },
  {
    "text": "analysis of this program. And this is, by the way, the\nsingle most important thing in scheduling theory but you're\ngoing to ever learn is this",
    "start": "2529440",
    "end": "2538599"
  },
  {
    "text": "particular theory. It goes all the way back to\n1968 and what it basically",
    "start": "2538600",
    "end": "2545290"
  },
  {
    "text": "says it is any greedy scheduler\nachieves a bound of",
    "start": "2545290",
    "end": "2552850"
  },
  {
    "text": "T1 over p plus T infinity. So why is that an interesting\nupper bound?",
    "start": "2552850",
    "end": "2560589"
  },
  {
    "text": "Yeah? AUDIENCE: That says that it's\ngot the refinement of what you",
    "start": "2560590",
    "end": "2565761"
  },
  {
    "text": "said before, even if you add as\nmany processors as you can, basically you're bounded\nby T infinity.",
    "start": "2565761",
    "end": "2571560"
  },
  {
    "text": "PROFESSOR: Yeah. AUDIENCE: It's compulsory. ",
    "start": "2571560",
    "end": "2578266"
  },
  {
    "text": "PROFESSOR: So basically, each\nof these, this term here is the term in the Work Law. This is the term in the Span\nLaw, and we're saying you can",
    "start": "2578266",
    "end": "2585640"
  },
  {
    "text": "always achieve the sum of those\ntwo lower bounds as an upper bound.",
    "start": "2585640",
    "end": "2592030"
  },
  {
    "text": "So let's see how we do this and\nthen we'll look at some of the implications. Question, do you have\na question?",
    "start": "2592030",
    "end": "2597534"
  },
  {
    "text": "No? So here's the proof that\nyou meet this.",
    "start": "2597534",
    "end": "2602780"
  },
  {
    "text": "So that the proof says--\nand I'll illustrate for P equals 3-- how many complete steps\ncould we have?",
    "start": "2602780",
    "end": "2611570"
  },
  {
    "text": "So I'll argue that the number\nof complete steps is at most T1 over p.",
    "start": "2611570",
    "end": "2617140"
  },
  {
    "text": "Why is that? Every complete step\nperforms p work.",
    "start": "2617140",
    "end": "2623240"
  },
  {
    "text": "So if I had more complete steps\nthan T1 over p, I'd be doing more than T1 work.",
    "start": "2623240",
    "end": "2631280"
  },
  {
    "text": "But I only have T1 work to do. OK, so the maximum number of\ncomplete steps I could have is",
    "start": "2631280",
    "end": "2638220"
  },
  {
    "text": "at most T1 over p. Do people follow that? So the trickier part of the\nproof, which is not all that",
    "start": "2638220",
    "end": "2645960"
  },
  {
    "text": "tricky but it's a little bit\ntrickier, is the other side. How many incomplete steps\ncould I have?",
    "start": "2645960",
    "end": "2652119"
  },
  {
    "text": "So we execute those. So I claim that the number of\nincomplete steps is bounded by",
    "start": "2652120",
    "end": "2659000"
  },
  {
    "text": "the critical path length,\nby the span. Why is that?",
    "start": "2659000",
    "end": "2664440"
  },
  {
    "text": "Well let's take a look at\nthe part of DAG that has yet to be executed. So that this gray part here.",
    "start": "2664440",
    "end": "2671230"
  },
  {
    "text": "There's some span associated\nwith that. In this case, it's this\nlongest path.",
    "start": "2671230",
    "end": "2677440"
  },
  {
    "text": "When I execute all of the ready\nthreads that are ready",
    "start": "2677440",
    "end": "2686460"
  },
  {
    "text": "to go, I guarantee to reduce\nthe span of that unexecuted",
    "start": "2686460",
    "end": "2692530"
  },
  {
    "text": "DAG by at least one. ",
    "start": "2692530",
    "end": "2698300"
  },
  {
    "text": "So as I do here, so I reduce\nit by one when I execute. So if I have a complete step,\nI don't guaranteed to reduce",
    "start": "2698300",
    "end": "2707022"
  },
  {
    "text": "the span of the unexecuted DAG,\nbecause I may execute",
    "start": "2707022",
    "end": "2713200"
  },
  {
    "text": "things as I showed you in this\nexample, you don't actually advance anything. But I execute all the ready\nthreads on an incomplete step,",
    "start": "2713200",
    "end": "2723770"
  },
  {
    "text": "and that's going to\nreduce it by one. So the number of incomplete\nsteps is at most infinity. So the total number of steps\nis at most the sum.",
    "start": "2723770",
    "end": "2732650"
  },
  {
    "text": "So as I say, this proof you\nshould understand in your sleep because it's the most\nimportant scheduling theory",
    "start": "2732650",
    "end": "2739380"
  },
  {
    "text": "proof that you're going to\nprobably see in your lifetime. It's very old, and really, very,\nvery simple and yet,",
    "start": "2739380",
    "end": "2748180"
  },
  {
    "text": "there's a huge amount of\nscheduling theory if you have a look at scheduling theory,\nthat comes out of this just",
    "start": "2748180",
    "end": "2754560"
  },
  {
    "text": "making this same problem more\ncomplicated and more real and more interesting and so forth.",
    "start": "2754560",
    "end": "2760339"
  },
  {
    "text": "But this is really the crux\nof what's going on. Any questions about\nthis proof?",
    "start": "2760340",
    "end": "2767510"
  },
  {
    "text": "So one corollary of the greedy\nscheduling algorithm is that",
    "start": "2767510",
    "end": "2773370"
  },
  {
    "text": "any greedy scheduler achieves\nwithin a factor of two of optimal scheduling. ",
    "start": "2773370",
    "end": "2780279"
  },
  {
    "text": "So let's see why that is. So it's guaranteed as an upper\nbound to get within a factor of two of optimal.",
    "start": "2780280",
    "end": "2786220"
  },
  {
    "text": "So here's the proof. So let's Tp star be the\nexecution time produced by the",
    "start": "2786220",
    "end": "2791700"
  },
  {
    "text": "optimal scheduler. This is the schedule that knows\nthe whole DAG in advance and can schedule things exactly\nwhere they need to be",
    "start": "2791700",
    "end": "2798000"
  },
  {
    "text": "scheduled to minimize the\ntotal amount of time. Now even though the optimal\nscheduler can schedule very",
    "start": "2798000",
    "end": "2804550"
  },
  {
    "text": "officially, it's still\nbound by the Work Law and the Span Law.",
    "start": "2804550",
    "end": "2810170"
  },
  {
    "text": "So therefore, Tp star has still\ngot to be greater than T1 over p and greater than\nT infinity by the",
    "start": "2810170",
    "end": "2816730"
  },
  {
    "text": "Work and Span Laws. Even though it's optimal, every\nscheduler must obey the",
    "start": "2816730",
    "end": "2821850"
  },
  {
    "text": "Work Laws and Spam Law. So then we have, by the greedy\nscheduling theorem, Tp is at",
    "start": "2821850",
    "end": "2828680"
  },
  {
    "text": "most T1 over p plus\nT infinity. Well that's at most twice the\nmaximum of these two values,",
    "start": "2828680",
    "end": "2835660"
  },
  {
    "text": "whichever is larger. I've just plugged in to get the\nmaximum of those two and",
    "start": "2835660",
    "end": "2840880"
  },
  {
    "text": "that's at most, by\nthis equation, twice the optimal time. ",
    "start": "2840880",
    "end": "2849059"
  },
  {
    "text": "So this is a very simple\ncorollary says oh, greedy scheduling is actually\npretty good.",
    "start": "2849060",
    "end": "2855110"
  },
  {
    "text": "It's not optimal,\nin fact, optimal scheduling is mP complete. Very hard problem to solve.",
    "start": "2855110",
    "end": "2861010"
  },
  {
    "text": "But getting within a factor\nof two, you just do greedy scheduling, it works\njust fine. ",
    "start": "2861010",
    "end": "2867460"
  },
  {
    "text": "More importantly is the next\ncorollary, which has to do is",
    "start": "2867460",
    "end": "2872770"
  },
  {
    "text": "when do you get linear\nspeedup? And this is, I think, the\nmost important thing to get out of this. So any greedy scheduler achieves\nnear perfect linear",
    "start": "2872770",
    "end": "2881820"
  },
  {
    "text": "speedup whenever-- what's this thing on\nthe left-hand side? What's the name we\ncall that?--",
    "start": "2881820",
    "end": "2888940"
  },
  {
    "text": "the parallelism, right? That's the parallelism, is much\nbigger than the number of processors you're running on.",
    "start": "2888940",
    "end": "2896299"
  },
  {
    "text": "So if the number of processors\nare running on is smaller than the parallelism of your code\nsays you can expect near",
    "start": "2896300",
    "end": "2903400"
  },
  {
    "text": "perfect linear speedup. OK, so what does that say you\nwant to do in your program?",
    "start": "2903400",
    "end": "2909140"
  },
  {
    "text": "You want to make sure you have\nample parallelism and then the scheduler will be able to\nschedule it so that you get",
    "start": "2909140",
    "end": "2917210"
  },
  {
    "text": "near perfect linear speedup. Let's see why that's true. So T1 over T infinity is much\nbigger than p is equivalent to",
    "start": "2917210",
    "end": "2926470"
  },
  {
    "text": "saying that T infinity is much\nless than T1 over p. That's just algebra.",
    "start": "2926470",
    "end": "2933960"
  },
  {
    "text": "Well what does that mean? The greedy scheduling theorem\nsays Tp is at most T1 over p plus T infinity.",
    "start": "2933960",
    "end": "2939700"
  },
  {
    "text": "We just said that if we have\nthis condition, then T infinity is very small compared\nto T1 over p.",
    "start": "2939700",
    "end": "2948020"
  },
  {
    "text": "So if this is negligible,\nthen the whole thing is about T1 over p.",
    "start": "2948020",
    "end": "2953195"
  },
  {
    "text": " Well that just says that\nthe speedup is about p.",
    "start": "2953195",
    "end": "2959617"
  },
  {
    "text": " So the name of the game is to\nmake sure that your span is",
    "start": "2959617",
    "end": "2967920"
  },
  {
    "text": "relatively short compared to\nthe amount of work per processor that you're doing.",
    "start": "2967920",
    "end": "2974082"
  },
  {
    "text": "And in that case, you'll\nget linear speedup. And that happens when you've\ngot enough parallelism",
    "start": "2974082",
    "end": "2980050"
  },
  {
    "text": "compared to the number\nprocessors you're running on. Any questions about this? This is like the most important\nthing you're going",
    "start": "2980050",
    "end": "2990000"
  },
  {
    "text": "to learn about parallel\ncomputing. ",
    "start": "2990000",
    "end": "2997410"
  },
  {
    "text": "Everything else we're going to\ndo is going to be derivatives of this, so if you don't\nunderstand this, you have a",
    "start": "2997410",
    "end": "3002430"
  },
  {
    "text": "hard time with the\nother stuff. So in some sense, it's\ndeceptively simple, right?",
    "start": "3002430",
    "end": "3008359"
  },
  {
    "text": "We just have a few variables,\nT1, Tp, T infinity, p, there's",
    "start": "3008360",
    "end": "3013730"
  },
  {
    "text": "not much else going on. But there are these bounds and\nthese elegant theorems that",
    "start": "3013730",
    "end": "3019589"
  },
  {
    "text": "tell us something about how no\nmatter what the shape of the",
    "start": "3019590",
    "end": "3025430"
  },
  {
    "text": "DAG is or whatever, these two\nvalues, the work and the span, really characterize very closely\nwhere it is that you",
    "start": "3025430",
    "end": "3033890"
  },
  {
    "text": "can expect to get\nlinear speedup. Any questions?",
    "start": "3033890",
    "end": "3039660"
  },
  {
    "text": "OK, good. ",
    "start": "3039660",
    "end": "3046500"
  },
  {
    "text": "So the quantity T1 over PT\ninfinity, so what is that? That's just the parallelism\ndivided by p.",
    "start": "3046500",
    "end": "3056310"
  },
  {
    "text": "That's called the parallel\nslackness. So this parallel slackness is\n10, means you have 10 times",
    "start": "3056310",
    "end": "3065200"
  },
  {
    "text": "more parallelism than\nprocessors. So if you have high slackness,\nyou can expect",
    "start": "3065200",
    "end": "3070330"
  },
  {
    "text": "to get linear speedup. If you have low slackness,\ndon't expect to get linear speedup. ",
    "start": "3070330",
    "end": "3077660"
  },
  {
    "start": "3076000",
    "end": "3223000"
  },
  {
    "text": "OK. Now the scheduler we're using\nis not a greedy scheduler.",
    "start": "3077660",
    "end": "3086920"
  },
  {
    "text": "It's better in many ways,\nbecause it's a distributed,",
    "start": "3086920",
    "end": "3093530"
  },
  {
    "text": "what's called work stealing\nscheduler and I'll show you how it works in a little bit. But it's based on\nthe same theory.",
    "start": "3093530",
    "end": "3101070"
  },
  {
    "text": "Even though it's a more\ncomplicated scheduler from an",
    "start": "3101070",
    "end": "3106340"
  },
  {
    "text": "analytical point of view, it's\nreally based on the same theory as greedy scheduling. It guarantees that the time on\np processors is at most T1",
    "start": "3106340",
    "end": "3117110"
  },
  {
    "text": "over p plus order T infinity. So there's a constant here.",
    "start": "3117110",
    "end": "3122310"
  },
  {
    "text": "And it's a randomized scheduler,\nso it actually only guarantees this in\nexpectation.",
    "start": "3122310",
    "end": "3128119"
  },
  {
    "text": "It actually guarantees very\nclose to this with high probability. OK so the difference is the big\nO, but if you look at any",
    "start": "3128120",
    "end": "3139190"
  },
  {
    "text": "of the formulas that we did with\nthe greedy scheduler, the fact that there's a constant\nthere doesn't really matter.",
    "start": "3139190",
    "end": "3144480"
  },
  {
    "text": "You get the same effect, it just\nmeans that the slackness that you need to get linear\nspeedup has to not only",
    "start": "3144480",
    "end": "3150579"
  },
  {
    "text": "overcome the T infinity, it's\nalso got to overcome the constant there.",
    "start": "3150580",
    "end": "3156200"
  },
  {
    "text": "And empirically, it actually\nturns out this is not bad as an estimate using the\ngreedy bound.",
    "start": "3156200",
    "end": "3164040"
  },
  {
    "text": "Not bad as an estimate, so this\nis sort of a model that we'll take as if we're\ndoing things",
    "start": "3164040",
    "end": "3169450"
  },
  {
    "text": "with a greedy scheduler. And that will be very close for\nwhat we're actually going to see in practice with\nthe Cilk++ scheduler.",
    "start": "3169450",
    "end": "3178790"
  },
  {
    "text": "So once again, it means near\nperfect linear speedup as long as p is much less than T1 over\nT infinity generally.",
    "start": "3178790",
    "end": "3186330"
  },
  {
    "text": "And so Cilkview allows us to\nmeasure T1 and T infinity. So that's going to be good,\nbecause then we can figure out",
    "start": "3186330",
    "end": "3193320"
  },
  {
    "text": "what our parallelism is and look\nto see how we're running on typically 12 cores, how much\nparallels do we have?",
    "start": "3193320",
    "end": "3201910"
  },
  {
    "text": "If our parallelism is 12, we\ndon't have a lot of slackness. We won't get very\ngood speedup.",
    "start": "3201910",
    "end": "3207440"
  },
  {
    "text": "But if we have a parallelism\nof say, 10 times more, say 120, we should get very, very\ngood parallelism, very, very",
    "start": "3207440",
    "end": "3216200"
  },
  {
    "text": "good speedup on 12 cores. We should get close to\nperfect speedup. ",
    "start": "3216200",
    "end": "3225099"
  },
  {
    "text": "So let's talk about the runtime\nsystem and how this work stealing scheduler works,\nbecause it different",
    "start": "3225100",
    "end": "3230740"
  },
  {
    "text": "from the other one. And this will be helpful also\nfor understanding when you",
    "start": "3230740",
    "end": "3236160"
  },
  {
    "text": "program these things what\nyou can expect. So the basic idea of the\nschedule is there's two",
    "start": "3236160",
    "end": "3247730"
  },
  {
    "text": "strategies the people have\nexplored for doing scheduling. One is called work sharing,\nwhich is not what Cilk++ does.",
    "start": "3247730",
    "end": "3256110"
  },
  {
    "text": "But let me explain what work\nsharing is because it's helpful to contrast it\nwith work stealing.",
    "start": "3256110",
    "end": "3262200"
  },
  {
    "text": "So in works sharing, what you do\nis when you spawn off some work, you say let me go find\nsome low utilized processor",
    "start": "3262200",
    "end": "3272280"
  },
  {
    "text": "and put that worked there\nfor it to operate on.",
    "start": "3272280",
    "end": "3277450"
  },
  {
    "text": "The problem with work sharing\nis that you have to do some communication and\nsynchronization every time you",
    "start": "3277450",
    "end": "3285599"
  },
  {
    "text": "do a spawn. Every time you do a spawn,\nyou're going to go out. This is kind of what\nPthreads does, when",
    "start": "3285600",
    "end": "3292290"
  },
  {
    "text": "you do Pthread create. It goes out and says OK, let me\ncreate all of the things it",
    "start": "3292290",
    "end": "3298120"
  },
  {
    "text": "needs to do and get it schedule\nthen on a processor. Work stealing, on the\nother hand, takes",
    "start": "3298120",
    "end": "3306410"
  },
  {
    "text": "the opposite approach. Whenever it spawns work, it's\njust going to keep that work",
    "start": "3306410",
    "end": "3311780"
  },
  {
    "text": "local to it, but make it\navailable for stealing. A processor that runs out of\nwork is going to go looking",
    "start": "3311780",
    "end": "3321220"
  },
  {
    "text": "for work to steal,\nto bring back. The advantage of work stealing\nis that the processor doesn't",
    "start": "3321220",
    "end": "3331540"
  },
  {
    "text": "do any synchronization\nexcept when it's actually load balancing. So if all of the processors have\nample work to do, then",
    "start": "3331540",
    "end": "3342849"
  },
  {
    "text": "what happens is there's no\noverhead for scheduling whatsoever.",
    "start": "3342850",
    "end": "3348380"
  },
  {
    "text": "They all just crank away. And so you get very, very low\noverheads when there's ample",
    "start": "3348380",
    "end": "3356120"
  },
  {
    "text": "work to do on each processor. So let's see how this works. So the particular way that\nit maintains it is that",
    "start": "3356120",
    "end": "3364119"
  },
  {
    "text": "basically, each processor\nmaintains a work deck. So a deck is a double-ended\nqueue of the ready strands.",
    "start": "3364120",
    "end": "3373750"
  },
  {
    "text": "It manipulates the bottom of\nthe deck like a stack. So what that says is, for\nexample, here, we had a spawn",
    "start": "3373750",
    "end": "3381020"
  },
  {
    "text": "followed by two calls. And basically, it's operating\njust as it would have to",
    "start": "3381020",
    "end": "3386810"
  },
  {
    "text": "operate in an ordinary stack,\nan ordinary call stack.",
    "start": "3386810",
    "end": "3396210"
  },
  {
    "text": "So, for example, this guy says\ncall, well it pushes a frame on the bottom of the call\nstack just like normal.",
    "start": "3396210",
    "end": "3404460"
  },
  {
    "text": "It says spawn, it pushes\na spawn frame on the bottom of the deck. ",
    "start": "3404460",
    "end": "3412910"
  },
  {
    "text": "In fact, of course, it's running\nin parallel, so you can have a bunch of guys that\nare both calling and spawning",
    "start": "3412910",
    "end": "3418420"
  },
  {
    "text": "and they all push whatever\ntheir frames are. When somebody says return,\nyou just pop it off.",
    "start": "3418420",
    "end": "3425380"
  },
  {
    "text": "So in the common case, each of\nthese guys is just executing",
    "start": "3425380",
    "end": "3430420"
  },
  {
    "text": "the code serially the way\nthat it would normally executing in C or C++. ",
    "start": "3430420",
    "end": "3438119"
  },
  {
    "text": "However, if somebody runs out\nof work, then it becomes a",
    "start": "3438120",
    "end": "3445370"
  },
  {
    "text": "thief and it looks for a victim\nand the strategy that's",
    "start": "3445370",
    "end": "3453310"
  },
  {
    "text": "used by Cilk++ is to\nlook at random. It says let me just go to\nany other processor",
    "start": "3453310",
    "end": "3463119"
  },
  {
    "text": "or any other workers-- I call these workers-- ",
    "start": "3463120",
    "end": "3468880"
  },
  {
    "text": "and grab away some\nof their work. But when it grabs it away, what\nit does is it steals it",
    "start": "3468880",
    "end": "3476170"
  },
  {
    "text": "from the opposite end of the\ndeck from where this",
    "start": "3476170",
    "end": "3482990"
  },
  {
    "text": "particular victim is actually\ndoing its work. So it steals the oldest\nstuff first.",
    "start": "3482990",
    "end": "3489650"
  },
  {
    "text": "So it moves that over and now\nhere what it's doing is it's stealing up to the point\nthat it spawns.",
    "start": "3489650",
    "end": "3494710"
  },
  {
    "text": "So it steals from the top of the\ndeck down to where there's a spawn on top. Yes? AUDIENCE: Is there always\na spawn on the",
    "start": "3494710",
    "end": "3501970"
  },
  {
    "text": "top of every deck?  PROFESSOR: Close,\nalmost always.",
    "start": "3501970",
    "end": "3508140"
  },
  {
    "text": "Yes, so I think that you could\nsay that there are. So the initial deck does not\nhave a spawn on top of it, but",
    "start": "3508140",
    "end": "3514289"
  },
  {
    "text": "you could imagine that it did. And then when you steal, you're\nalways stealing from the top down to a spawn.",
    "start": "3514290",
    "end": "3521920"
  },
  {
    "text": "If there isn't something, if\nthis is just a call here, this",
    "start": "3521920",
    "end": "3527990"
  },
  {
    "text": "cannot any longer be stolen. There's no work there to be\nstolen because this is just a single execution, there's\nnothing that's been spawned",
    "start": "3527990",
    "end": "3534990"
  },
  {
    "text": "off at this point. This is the result of having\nbeen spawned as opposed to",
    "start": "3534990",
    "end": "3540200"
  },
  {
    "text": "that it's doing a spawn. So yes, basically\nyou're right.",
    "start": "3540200",
    "end": "3545230"
  },
  {
    "text": "There's a spawn on the top. So it basically steals that\noff and then it resumes execution afterwards and starts\nthen operating just",
    "start": "3545230",
    "end": "3555540"
  },
  {
    "text": "like an ordinary deck. So the theorem that you can\nprove for this type of",
    "start": "3555540",
    "end": "3564550"
  },
  {
    "text": "scheduler is that if you have\nsufficient parallelism, so you all know what parallelism is at\nthis point, you can prove",
    "start": "3564550",
    "end": "3571910"
  },
  {
    "text": "that the workers steal\ninfrequently. So in a a typical execution, you\nmight have a few hundred",
    "start": "3571910",
    "end": "3580880"
  },
  {
    "text": "load balancing operations of\nthis nature for something which is doing billions and\nbillions of instructions.",
    "start": "3580880",
    "end": "3588420"
  },
  {
    "text": "So you steal infrequently. If you're stealing infrequently\nand all the rest",
    "start": "3588420",
    "end": "3593859"
  },
  {
    "text": "of the time you're just\nexecuting like the C or C++,",
    "start": "3593860",
    "end": "3599970"
  },
  {
    "text": "hey, now you've got linear\nspeedup because you've got all of these guys working\nall the time. ",
    "start": "3599970",
    "end": "3608140"
  },
  {
    "text": "And so as I say, the main thing\nto understand is that there's this work stealing\nscheduler running underneath.",
    "start": "3608140",
    "end": "3614550"
  },
  {
    "text": "It's more complicated to\nanalyze then the greedy scheduler, but it gives you\npretty much the same",
    "start": "3614550",
    "end": "3620210"
  },
  {
    "text": "qualitative kinds of results. And the idea then is that the\nstealing occurs infrequently",
    "start": "3620210",
    "end": "3631450"
  },
  {
    "text": "so you get linear speedup. So the idea then is just as with\ngreedy scheduling, make sure you have enough\nparallelism, because then the",
    "start": "3631450",
    "end": "3637540"
  },
  {
    "text": "load balancing is a small\nfraction of the time these processors are spending\nexecuting the code.",
    "start": "3637540",
    "end": "3645890"
  },
  {
    "text": "Because whenever it's doing\nthings like work stealing, it's not working on your code\nexecuting, making it go fast.",
    "start": "3645890",
    "end": "3652460"
  },
  {
    "text": "It's doing bookkeeping and\noverhead and stuff.",
    "start": "3652460",
    "end": "3657790"
  },
  {
    "text": "So you want to make sure\nthat stays low. So any questions about that?",
    "start": "3657790",
    "end": "3663289"
  },
  {
    "text": "So specifically, we\nhave these bounds. You have achieved this expected\nrunning time, which I",
    "start": "3663290",
    "end": "3669750"
  },
  {
    "text": "mentioned before. Let me give you a pseudo-proof\nof this. So this is not a real proof\nbecause it ignores things like",
    "start": "3669750",
    "end": "3679030"
  },
  {
    "text": "independence of probabilities. So when you do a probability\nanalysis, you're not allowed to multiply probabilities unless\nthey're independent.",
    "start": "3679030",
    "end": "3687310"
  },
  {
    "text": "So anyway, here I'm multiplying\nprobabilities that are independent. So the idea is you can\nview a processor as",
    "start": "3687310",
    "end": "3694960"
  },
  {
    "text": "either working or stealing. So it goes into one\nof two modes. It's going to be stealing\nif it's run out of work,",
    "start": "3694960",
    "end": "3702220"
  },
  {
    "text": "otherwise it's working. So the total time all processors\nspend working is T1, hooray, that's\nat least a bound.",
    "start": "3702220",
    "end": "3711900"
  },
  {
    "text": "Now it turns out that every\nsteal has a 1 over p chance of reducing the span by one.",
    "start": "3711900",
    "end": "3719550"
  },
  {
    "text": "So you can prove that of all of\nthe work that's in the top of all those decks that those\nare where any of the ready",
    "start": "3719550",
    "end": "3727720"
  },
  {
    "text": "threads are going to be there\nare in a position of reducing",
    "start": "3727720",
    "end": "3735890"
  },
  {
    "text": "the span if you execute them. And so whenever you steal, you\nhave a 1 over p chance of",
    "start": "3735890",
    "end": "3741780"
  },
  {
    "text": "hitting the guy that matters\nfor the span",
    "start": "3741780",
    "end": "3748580"
  },
  {
    "text": "of unexecuted DAG. So the same kind of thing\nas in theory. You have a 1 over p chance.",
    "start": "3748580",
    "end": "3754050"
  },
  {
    "text": "So the expected cost of all\nsteals is order PT infinity.",
    "start": "3754050",
    "end": "3759580"
  },
  {
    "text": "So this is true, but not\nfor this reason. But it's kind, the intuition\nis right.",
    "start": "3759580",
    "end": "3765960"
  },
  {
    "text": " So therefore the cost of all\nsteals is PT infinity and the",
    "start": "3765960",
    "end": "3772070"
  },
  {
    "text": "cost of the work is T1, so\nthat's the total amount of work and time spent stealing\nby all the p processors.",
    "start": "3772070",
    "end": "3780480"
  },
  {
    "text": "So to get the time spent doing\nthat, we divide by p, because they're p processors.",
    "start": "3780480",
    "end": "3788220"
  },
  {
    "text": "And when I do that, I get T1\nover p plus order T infinity. So that's kind of where that\nbound is coming from.",
    "start": "3788220",
    "end": "3795549"
  },
  {
    "text": "So you can see what's important\nhere is that the term, that order T infinity\nterm, this the one where all",
    "start": "3795550",
    "end": "3802730"
  },
  {
    "text": "the overhead of scheduling\nand synchronization is. There's no overhead for\nscheduling and synchronization",
    "start": "3802730",
    "end": "3808119"
  },
  {
    "text": "in the T1 over p term. The only overhead there is to do\nthings like mark the frames",
    "start": "3808120",
    "end": "3813849"
  },
  {
    "text": "as being a steel frame or\na spawn frame and do the",
    "start": "3813850",
    "end": "3819140"
  },
  {
    "text": "bookkeeping of the deck as\nyou're executing so the spawn can be implemented\nvery cheaply.",
    "start": "3819140",
    "end": "3828070"
  },
  {
    "text": "Now in addition to the\nscheduling things, there are",
    "start": "3828070",
    "end": "3835130"
  },
  {
    "text": "some other things to understand\na little bit about the scheduler and that is that\nit supports the C, C++ rule",
    "start": "3835130",
    "end": "3842960"
  },
  {
    "text": "for pointers. So remember in C and C++, you\ncan pass a pointer to stack",
    "start": "3842960",
    "end": "3848130"
  },
  {
    "text": "space down, but you can't pass\na pointer to stack space back to your parent, right?",
    "start": "3848130",
    "end": "3853430"
  },
  {
    "text": "Because it popped off.  So if you think about a C or\nC++ execution, let's say we",
    "start": "3853430",
    "end": "3862990"
  },
  {
    "text": "have this call structure here. A really cannot see any of the\nstack space of B,C,D or E. So",
    "start": "3862990",
    "end": "3870819"
  },
  {
    "text": "this is what A gets to see. And B, meanwhile, can see A\nspace, because that's down on",
    "start": "3870820",
    "end": "3876349"
  },
  {
    "text": "the stack, but it can't see C, D\nor E. Particularly if you're executing this serially, it\ncan't see C because C hasn't",
    "start": "3876350",
    "end": "3882490"
  },
  {
    "text": "executed yet when B executes. However, C, it turns out,\nthe same thing.",
    "start": "3882490",
    "end": "3889190"
  },
  {
    "text": "I can't see any of the variables\nthat might be allocated in the space\nfor B when I'm",
    "start": "3889190",
    "end": "3894270"
  },
  {
    "text": "executing here on a stack. You can see them in a heap, but\nnot on the stack, because B has been popped off at that\npoint and so forth.",
    "start": "3894270",
    "end": "3902250"
  },
  {
    "text": "So this is basically the normal\nrule, the normal views of stack that you\nget in C or C++.",
    "start": "3902250",
    "end": "3909790"
  },
  {
    "text": "In Cilk++, you get exactly the\nsame behavior except that multiple ones of these views\nmay exist at the same time.",
    "start": "3909790",
    "end": "3920259"
  },
  {
    "text": "So if, for example, B and C are\nboth executing at the same time, they each will\nsee their own stack",
    "start": "3920260",
    "end": "3926690"
  },
  {
    "text": "space and a stack space. And so the cactus stack\nmaintains that fiction that",
    "start": "3926690",
    "end": "3934220"
  },
  {
    "text": "you can sort of look at your\nancestors and see your ancestors, but now\nit's maintained. It's called a cactus stack\nbecause it's kind of like a",
    "start": "3934220",
    "end": "3941890"
  },
  {
    "text": "tree structure upside down, like\na what's the name of that",
    "start": "3941890",
    "end": "3947240"
  },
  {
    "text": "big cactus out West? Yes, saguaro. The saguaro cactus, yep.",
    "start": "3947240",
    "end": "3952490"
  },
  {
    "text": "This kind of looks like that\nif you look at the stacks. ",
    "start": "3952490",
    "end": "3958110"
  },
  {
    "text": "This leads to a very powerful\nbound on how much space your",
    "start": "3958110",
    "end": "3964070"
  },
  {
    "text": "program is using. So normally, if you do a greedy\nscheduler, you could end up using gobs more space\nthen you would in a serial",
    "start": "3964070",
    "end": "3971950"
  },
  {
    "text": "execution, gobs more\nstack space. In Cilk++ programs,\nyou have a bound.",
    "start": "3971950",
    "end": "3978600"
  },
  {
    "text": "It's p times s1 is the maximum\namount of stack space you'll ever use where s1 is\nthe stack space",
    "start": "3978600",
    "end": "3985420"
  },
  {
    "text": "used by serial execution. So if you can keep your serial\nexecution to use a reasonable amount of stack space--\nand usually it does--",
    "start": "3985420",
    "end": "3992270"
  },
  {
    "text": "then in parallel, you don't\nuse more than p times that amount of stack space. And the proof for that is sort\nof by induction, which",
    "start": "3992270",
    "end": "3999890"
  },
  {
    "text": "basically says there's a\nproperty called the Busy Leaves Property that says that\nif you have a leaf that's",
    "start": "3999890",
    "end": "4010530"
  },
  {
    "text": "being worked on but hasn't\nbeen completed-- so I've indicated those by the\npurple and pink ones--",
    "start": "4010530",
    "end": "4017720"
  },
  {
    "text": "then if it's a leaf, it has\na worker executing on it. And so therefore, if you look at\nhow much stack space you're",
    "start": "4017720",
    "end": "4026609"
  },
  {
    "text": "using, each of these guys can\ntrace up and they may double count the stack space, but it'll\nstill be bounded by p",
    "start": "4026610",
    "end": "4034550"
  },
  {
    "text": "times the depth that they're\nat, or p times s1, which is the maximum amount.",
    "start": "4034550",
    "end": "4040620"
  },
  {
    "text": "So it has good space bounds. That's not so crucial for you\nfolks to know as a practical",
    "start": "4040620",
    "end": "4046099"
  },
  {
    "text": "matter, but it would be\nif this didn't hold. If this didn't hold, then you\nwould have more programming",
    "start": "4046100",
    "end": "4052410"
  },
  {
    "text": "problems than you'll have.  The implications of this work\nstealing scheduler is",
    "start": "4052410",
    "end": "4060780"
  },
  {
    "text": "interesting from the linguistic\npoint of view,",
    "start": "4060780",
    "end": "4065920"
  },
  {
    "text": "because you can write a code\nlike this, so for i gets one to a billion, spawn some\nsub-routine foo",
    "start": "4065920",
    "end": "4074740"
  },
  {
    "text": "of i and then sync. So one way of executing this,\nthe way that the work sharing",
    "start": "4074740",
    "end": "4081920"
  },
  {
    "text": "schedulers tend to do this,\nis they say oh, I've got a billion tasks to do.",
    "start": "4081920",
    "end": "4088599"
  },
  {
    "text": "So let me create a billion tasks\nand now schedule them and the space just vrooms to\nstore all those billion tasks,",
    "start": "4088600",
    "end": "4096180"
  },
  {
    "text": "it gets to be huge. Now of course, they have some\nstrategies they can use to reduce it by bunching tasks\ntogether and so forth.",
    "start": "4096180",
    "end": "4102939"
  },
  {
    "text": "But in principle, you got a\nbillion pieces of work to do even if you execute\non one processor.",
    "start": "4102939",
    "end": "4110630"
  },
  {
    "text": "Whereas in the work stealing\ntype execution, what happens is you execute this in\nfact depth research.",
    "start": "4110630",
    "end": "4118180"
  },
  {
    "text": "So basically, you're going to\nexecute foo of 1 and then you'll return.",
    "start": "4118180",
    "end": "4124620"
  },
  {
    "text": "And then you'll increment i and\nyou'll execute foo of 2, and you'll return.",
    "start": "4124620",
    "end": "4129670"
  },
  {
    "text": "At no time are you using more\nthan in this case two stack frames, one for this routine\nhere and one for foo because",
    "start": "4129670",
    "end": "4138028"
  },
  {
    "text": "you basically keep going up. You're using your stack up on\ndemand, rather than creating",
    "start": "4138029",
    "end": "4143430"
  },
  {
    "text": "all the work up front\nto be scheduled.  So the work stealing scheduler\nis very good from",
    "start": "4143430",
    "end": "4149839"
  },
  {
    "text": "that point of view. The tricky thing for people\nto understand is that if executing on multiple\nprocessors, when you do Cilk",
    "start": "4149840",
    "end": "4156520"
  },
  {
    "text": "spawn, the processor, the worker\nthat you're running on,",
    "start": "4156520",
    "end": "4161568"
  },
  {
    "text": "is going to execute foo of 1. The next statement--",
    "start": "4161569",
    "end": "4166859"
  },
  {
    "text": "which would basically\nbe incrementing the counter and so forth-- is executed by whatever\nprocessor comes in and steals",
    "start": "4166859",
    "end": "4173910"
  },
  {
    "text": "that continuation. ",
    "start": "4173910",
    "end": "4180089"
  },
  {
    "text": "So if you had two processors,\nthey're each going to basically be executing. The first processor isn't the\none that excuse everything in",
    "start": "4180090",
    "end": "4187620"
  },
  {
    "text": "this function. This function has its execution\nshared, the strands are going to be shared where the\nfirst part of it would be",
    "start": "4187620",
    "end": "4193909"
  },
  {
    "text": "done by processor one and the\nlatter part of it would be done by processor two. And then when processor one\nfinishes this off, it might go",
    "start": "4193910",
    "end": "4201160"
  },
  {
    "text": "back and steal back from\nprocessor two. ",
    "start": "4201160",
    "end": "4207150"
  },
  {
    "text": "So the important thing there is\nit's generating its stack needs sort of on demand rather\nthan all up front, and that",
    "start": "4207150",
    "end": "4216240"
  },
  {
    "text": "keeps the amount of stack space\nsmall as it executes.",
    "start": "4216240",
    "end": "4221250"
  },
  {
    "text": "So the moral is it's better to\nsteal your parents from their children than stealing children\nfrom their parents.",
    "start": "4221250",
    "end": "4226780"
  },
  {
    "text": " So that's the advantage of\ndoing this sort of parent",
    "start": "4226780",
    "end": "4233710"
  },
  {
    "text": "stealing, because you're always\ndoing the frame which is an ancestor of where that\nworker is working and that",
    "start": "4233710",
    "end": "4240310"
  },
  {
    "text": "means resuming a function\nright in the middle on a different processor. That's kind of the magic of the\ntechnologies is how do you",
    "start": "4240310",
    "end": "4247110"
  },
  {
    "text": "actually move a stack frame from\none place to another and resume it in the middle? ",
    "start": "4247110",
    "end": "4255560"
  },
  {
    "text": "Let's finish up here with\na chess lesson. I promised a chess lesson,\nso we might as well do some fun and games.",
    "start": "4255560",
    "end": "4262440"
  },
  {
    "text": "We have a lot of experience at\nMIT with chess programs. We've had a lot of success,\nprobably our closest one was",
    "start": "4262440",
    "end": "4274770"
  },
  {
    "text": "Star Socrates 2.0, which took\nsecond place in the world computer chess championship\nrunning on an 1824 node Intel",
    "start": "4274770",
    "end": "4283849"
  },
  {
    "text": "Paragon, so a big supercomputer\nrunning with a Cilk scheduler.",
    "start": "4283850",
    "end": "4288980"
  },
  {
    "text": "We actually almost won that\ncompetition, and it's a sad",
    "start": "4288980",
    "end": "4293989"
  },
  {
    "text": "story that maybe be sometime\naround dinner or something I will tell you the sad story\nbehind it, but I'm not going",
    "start": "4293990",
    "end": "4301540"
  },
  {
    "text": "to tell you why we didn't\ntake first place. And we've had a bunch of other\nsuccesses over the years.",
    "start": "4301540",
    "end": "4310030"
  },
  {
    "text": "Right now our chess programming\nis dormant, we're not doing that in my group\nanymore, but in the past, we",
    "start": "4310030",
    "end": "4315680"
  },
  {
    "text": "had some very strong chess\nplaying programs.  So what we did with Star\nSocrates, which is one of our",
    "start": "4315680",
    "end": "4325990"
  },
  {
    "text": "programs, was we wanted to\nunderstand the Cilk scheduler.",
    "start": "4325990",
    "end": "4331880"
  },
  {
    "text": "And so what we did is we ran\na whole bunch of different positions on different numbers\nof processors which ran for",
    "start": "4331880",
    "end": "4339000"
  },
  {
    "text": "different amounts of time. We wanted to plot them all on\nthe same chart, and here's our",
    "start": "4339000",
    "end": "4345640"
  },
  {
    "text": "strategy for doing it. What decided to do was do a\nstandard speedup curve.",
    "start": "4345640",
    "end": "4351230"
  },
  {
    "text": "So a standard speedup curve says\nlet's plot the number of processors along this axis and\nthe speed up along that axis.",
    "start": "4351230",
    "end": "4360700"
  },
  {
    "text": "But in order to fit all these\nthings on the same processor curve, what we did was we\nnormalize the speedup.",
    "start": "4360700",
    "end": "4368110"
  },
  {
    "text": "So what's the maximum pot? So here's the speedup. If you look the numerator\nhere, this is the speedup, T1 over Tp.",
    "start": "4368110",
    "end": "4374280"
  },
  {
    "text": "What we did is we normalized\nby the parallelism. So we said what fraction of\nperfect speedup can we get?",
    "start": "4374280",
    "end": "4384360"
  },
  {
    "text": "So here one says that I got\nexactly a speedup, this is the",
    "start": "4384360",
    "end": "4392480"
  },
  {
    "text": "maximum possible speed up that\nI can get because the maximum possible value of T1 over\np is T1 over T infinity.",
    "start": "4392480",
    "end": "4400190"
  },
  {
    "text": "So that's sort of the maximum. On this axis, we said how\nmany processors are",
    "start": "4400190",
    "end": "4405560"
  },
  {
    "text": "you running on it? Well, we looked at\nthat relative to essentially the slackness. So notice by normalizing, we\nessentially have here the",
    "start": "4405560",
    "end": "4413929"
  },
  {
    "text": "inverse of the slackness. So 1 here says that I'm running\non exactly the same",
    "start": "4413930",
    "end": "4419130"
  },
  {
    "text": "number of processors\nas my parallelism. A tenth here says I've got a\nslackness of 10, I'm running",
    "start": "4419130",
    "end": "4426949"
  },
  {
    "text": "on 10 times fewer processors\nthen parallelism. Out here, I'm saying I got way\nmore processors than I have",
    "start": "4426950",
    "end": "4435310"
  },
  {
    "text": "parallelism. So I plotted all the points. So it doesn't show up very well\nhere, but all those green",
    "start": "4435310",
    "end": "4441040"
  },
  {
    "text": "points, there are a lot of green\npoints here, that's our performance, measured\nperformance.",
    "start": "4441040",
    "end": "4447199"
  },
  {
    "text": "You can sort of see they're\ngreen there, not the best color for this projector. ",
    "start": "4447200",
    "end": "4453909"
  },
  {
    "text": "So we plot on this essentially\nthe Work Law and the Span Law.",
    "start": "4453910",
    "end": "4460090"
  },
  {
    "text": "So this is the Work Law, it says\nlinear speedup, and this is the Span Law. And you can see that we're\ngetting very close to perfect",
    "start": "4460090",
    "end": "4468360"
  },
  {
    "text": "linear speedup as long as our\nslackness is 10 or greater.",
    "start": "4468360",
    "end": "4475600"
  },
  {
    "text": "See that? It's hugging that curve\nreally tightly. As we approach a slackness of 1,\nyou can see that it starts",
    "start": "4475600",
    "end": "4488570"
  },
  {
    "text": "to go away from the linear\nspeedup curve.  So for this program, if you\nlook, it says, gee, if we were",
    "start": "4488570",
    "end": "4496060"
  },
  {
    "text": "running with 10 time, slackness\nof 10, 10 times more parallelism than processors,\nwe're getting almost perfect",
    "start": "4496060",
    "end": "4503550"
  },
  {
    "text": "linear speedup in the number of\nprocessors we're running on across a wide range of number\nof processors, wide range of",
    "start": "4503550",
    "end": "4509440"
  },
  {
    "text": "benchmarks for this\nchess program. And in fact, this curve\nis the curve.",
    "start": "4509440",
    "end": "4515040"
  },
  {
    "text": "This is not an interpolation\nhere, but rather it is just the greedy scheduling curve,\nand you can see it does a pretty good job of going through\nall the points here.",
    "start": "4515040",
    "end": "4524810"
  },
  {
    "text": "Greedy scheduling does a pretty\ngood job of predicting the performance. The other thing you should\nnotice is that although things",
    "start": "4524810",
    "end": "4529920"
  },
  {
    "text": "are very tight down here, as\nyou approach up here, they start getting more spread.",
    "start": "4529920",
    "end": "4535360"
  },
  {
    "text": "And the reason is that as you\nstart having more of the span mattering in the calculation,\nthat's where all the",
    "start": "4535360",
    "end": "4542480"
  },
  {
    "text": "synchronization, communication,\nall the overhead of actually doing the\nmechanics of moving a frame",
    "start": "4542480",
    "end": "4549750"
  },
  {
    "text": "from one processor to another\ntake into account, so you get a lot more spread as\nyou go up here.",
    "start": "4549750",
    "end": "4556360"
  },
  {
    "text": "So that's just the first\npart of the lesson. The first part was, oh, the\ntheory works out in practice",
    "start": "4556360",
    "end": "4565560"
  },
  {
    "text": "for real programs. You have like 10 times more\nparallelisms than processors,",
    "start": "4565560",
    "end": "4572929"
  },
  {
    "text": "you're going to do a\npretty good job of getting linear speedup. So that says you guys should be\nshooting for parallelisms",
    "start": "4572930",
    "end": "4579619"
  },
  {
    "text": "on the order of 100 for\nrunning on 12 cores.",
    "start": "4579620",
    "end": "4586660"
  },
  {
    "text": "Somewhere in that vicinity you\nshould be doing pretty well if you've got parallelism\nof 100 when you measure it for your codes.",
    "start": "4586660",
    "end": "4593530"
  },
  {
    "text": "So we normalize by the\nparallel there.  Now the real lesson though was\nunderstanding how to use",
    "start": "4593530",
    "end": "4603840"
  },
  {
    "text": "things like work and span to\nmake decisions in the design of our program.",
    "start": "4603840",
    "end": "4609730"
  },
  {
    "text": "So as it turned out, Socrates\nfor this particular competition was to run on a\n512 processor connection",
    "start": "4609730",
    "end": "4617750"
  },
  {
    "text": "machine at the University\nof Illinois. So this was in the mid\nin the early 1990's.",
    "start": "4617750",
    "end": "4628950"
  },
  {
    "text": "It was one of the most powerful\nmachines in the world, and this thing is\nprobably more powerful today.",
    "start": "4628950",
    "end": "4637270"
  },
  {
    "text": "But in those days, it was a\npretty powerful machine. I don't know whether this\nthing is, but this thing probably I'm pretty sure\nis more powerful.",
    "start": "4637270",
    "end": "4645610"
  },
  {
    "text": "So this was a big machine. However here at MIT, we didn't\nhave a great big",
    "start": "4645610",
    "end": "4651300"
  },
  {
    "text": "machine like that. We only had a 32\nprocessor CM5. ",
    "start": "4651300",
    "end": "4657800"
  },
  {
    "text": "So we were developing on a\nlittle machine expecting to run on a big machine. ",
    "start": "4657800",
    "end": "4665050"
  },
  {
    "text": "So one of the developers\nproposed to change the program that produced a speedup of over\n20% on the MIT machine.",
    "start": "4665050",
    "end": "4674310"
  },
  {
    "text": "So we said, oh that's pretty\ngood, 25% improvement. But we did a back of the\nenvelope calculation and",
    "start": "4674310",
    "end": "4680990"
  },
  {
    "text": "rejected that improvement\nbecause we were able to use work and span to predict the\nbehavior on the big machine.",
    "start": "4680990",
    "end": "4690645"
  },
  {
    "text": " So let's see how that worked\nout, why that worked out.",
    "start": "4690645",
    "end": "4696670"
  },
  {
    "text": "So I've fudged these numbers so\nthat they're easy to do the math on and easy\nto understand.",
    "start": "4696670",
    "end": "4702780"
  },
  {
    "text": "The real numbers actually though\ndid sort out very, very similar to what I'm saying,\njust they weren't round",
    "start": "4702780",
    "end": "4708610"
  },
  {
    "text": "numbers like I'm going\nto give you. So the original program\nran for let's say 65",
    "start": "4708610",
    "end": "4714610"
  },
  {
    "text": "seconds on 32 cores. The proposed program ran for\n40 seconds on 32 cores.",
    "start": "4714610",
    "end": "4721480"
  },
  {
    "text": "Sounds like a good improvement\nto me. Let's go for the\nfaster program.",
    "start": "4721480",
    "end": "4726790"
  },
  {
    "text": "Well, let's hold your horses. Let's take a look at our\nperformance model based on",
    "start": "4726790",
    "end": "4732480"
  },
  {
    "text": "greedy scheduling. That Tp is T1 over\np plus infinity.",
    "start": "4732480",
    "end": "4737500"
  },
  {
    "text": "What component we really need to\nunderstand the scale this, what component of each of\nthese things is work",
    "start": "4737500",
    "end": "4743830"
  },
  {
    "text": "and which is span? Because that's how we're going\nto be able to predict what's going to happen on\nthe big machine.",
    "start": "4743830",
    "end": "4749930"
  },
  {
    "text": "So indeed, this original program\nhad a work of 2048",
    "start": "4749930",
    "end": "4755360"
  },
  {
    "text": "seconds and a span\nof one second. Now chess, it turns out, is a\nnon-deterministic type of",
    "start": "4755360",
    "end": "4763820"
  },
  {
    "text": "program where you use\nspeculative parallelism, and so in order to get more\nparallelism, you can sacrifice",
    "start": "4763820",
    "end": "4772205"
  },
  {
    "text": "and do more work versus\nless work. So this one over here that we\nimproved it to had less work",
    "start": "4772205",
    "end": "4779250"
  },
  {
    "text": "on the benchmark, but it\nhad a longer span. ",
    "start": "4779250",
    "end": "4786280"
  },
  {
    "text": "So it had less work\nbut a longer span. So when we actually were going\nto run this, well first of",
    "start": "4786280",
    "end": "4792730"
  },
  {
    "text": "all, we did the calculation\nand it actually came out pretty close. I was kind of surprised how\nclose the theory matched.",
    "start": "4792730",
    "end": "4800620"
  },
  {
    "text": "We actually on 32 processors\nwhen you do the work spanned calculation, you get the 65\nseconds on a 32 processor",
    "start": "4800620",
    "end": "4808920"
  },
  {
    "text": "machine, here we\nhad 40 seconds. But now what happens when we\nscale this to the big machine?",
    "start": "4808920",
    "end": "4820200"
  },
  {
    "text": "Here we scaled it\nto 512 cores. So now we take the work divided\nby the number of processors, 512, plus 1, that's\n5 seconds for this.",
    "start": "4820200",
    "end": "4829160"
  },
  {
    "text": "Here we have the work but we now\nhave a much larger span. So we have two seconds of work\nfor processor, but now eight",
    "start": "4829160",
    "end": "4836790"
  },
  {
    "text": "seconds of span for a\ntotal of 10 seconds.",
    "start": "4836790",
    "end": "4842130"
  },
  {
    "text": "So had we made this quote\n\"improvement,\" our code would have been half as fast.",
    "start": "4842130",
    "end": "4848035"
  },
  {
    "text": " It would not have scaled. ",
    "start": "4848035",
    "end": "4855020"
  },
  {
    "text": "And so the point is that work\nand span typically will beat",
    "start": "4855020",
    "end": "4861300"
  },
  {
    "text": "running times for predicting\nscalability of performance. So you can measure a particular\nthing, but what you",
    "start": "4861300",
    "end": "4867440"
  },
  {
    "text": "really want to know is this\nthing this going to scale and how is it going to scale\ninto the future.",
    "start": "4867440",
    "end": "4872860"
  },
  {
    "text": "So people building multicore\napplications today want to know that they coded up. They don't want to be told in\ntwo years that they've got to",
    "start": "4872860",
    "end": "4880450"
  },
  {
    "text": "recode it all because the\nnumber of cores doubled. They want to have some\nfuture-proof notion that hey,",
    "start": "4880450",
    "end": "4887369"
  },
  {
    "text": "there's a lot of parallelism\nin this program.",
    "start": "4887370",
    "end": "4893490"
  },
  {
    "text": "So work and span, work\nand span, eat it, drink it, sleep it.",
    "start": "4893490",
    "end": "4899560"
  },
  {
    "text": "Work and span, work and span,\nwork and span, work and span,",
    "start": "4899560",
    "end": "4905740"
  },
  {
    "text": "work and span, OK? Work and span. ",
    "start": "4905740",
    "end": "4908760"
  }
]