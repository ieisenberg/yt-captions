[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6330"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6330",
    "end": "13320"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "13320",
    "end": "18370"
  },
  {
    "text": " JOHN W. ROBERTS: We'll\nbe talking about-- ",
    "start": "18370",
    "end": "29670"
  },
  {
    "text": "I heard last time I\nhad bad handwriting. And I guess this isn't\nmuch improved yet, but I will try to be more\ndeliberate if not more skilled.",
    "start": "29670",
    "end": "39150"
  },
  {
    "text": "Stochastic-- all right.",
    "start": "39150",
    "end": "53200"
  },
  {
    "text": "So as you may\nremember last time,",
    "start": "53200",
    "end": "59650"
  },
  {
    "text": "we were talking about\ndifferent assumptions that we've used in all\nthe techniques we've applied so far.",
    "start": "59650",
    "end": "65018"
  },
  {
    "start": "65019",
    "end": "71829"
  },
  {
    "text": "Now, we assume that we\nhave a model of the system,",
    "start": "71830",
    "end": "79780"
  },
  {
    "text": "that the system\nis deterministic-- ",
    "start": "79780",
    "end": "86232"
  },
  {
    "text": "that's not really any\nbetter handwriting, but this is the one\nthat last time we talked about getting\nrid of anyway, right?",
    "start": "86232",
    "end": "91300"
  },
  {
    "text": "Stochastic system,\nstochastic dynamics. So that's what we\ntried to remove.",
    "start": "91300",
    "end": "96503"
  },
  {
    "text": "And then the state is known. ",
    "start": "96503",
    "end": "103060"
  },
  {
    "text": "So sort of already\ngotten rid of this one in some of our discussions. And today we're going\nto talk about what",
    "start": "103060",
    "end": "109690"
  },
  {
    "text": "you do if you don't a model. And this is something\nthat's actually very important in a lot\nof interesting systems.",
    "start": "109690",
    "end": "116080"
  },
  {
    "text": "And the systems that we work\non the lab, some of them we try to model, but some\nof them help us to model.",
    "start": "116080",
    "end": "121870"
  },
  {
    "text": "So this, dealing without\nthis, is a very useful thing to be able to do.",
    "start": "121870",
    "end": "127600"
  },
  {
    "text": "So hopefully, you'll\nall at the end appreciate the tremendous power\nof model-free reinforcement",
    "start": "127600",
    "end": "134730"
  },
  {
    "text": "learning.  So the basic idea is,\nagain, we have this policy",
    "start": "134730",
    "end": "142480"
  },
  {
    "text": "parameterization alpha, which\nsomehow defines our policy.",
    "start": "142480",
    "end": "148840"
  },
  {
    "text": "And the problem sets that you\nrecently did, it's open loop, so you just have one\nalpha for every time step. You also can imagine these are\ngains on a feedback policy,",
    "start": "148840",
    "end": "156760"
  },
  {
    "text": "entries of the K matrix,\nor PD gains, any way you want to parameterize it.",
    "start": "156760",
    "end": "162400"
  },
  {
    "text": "And you think about you use-- these parameters-- now, this is\nthe most simple interpretation.",
    "start": "162400",
    "end": "169300"
  },
  {
    "text": "There's a lot more complicated\nways of looking at it, but I'm going to look at\nthe most simple way first. You send this into your system.",
    "start": "169300",
    "end": "174800"
  },
  {
    "text": "So you can run your system\nwith these parameters. Now, this is,\nagain, sort of like what you did in the problem set.",
    "start": "174800",
    "end": "180579"
  },
  {
    "text": "You have a fixed initial\ncondition, fixed cost function, you give it a policy, you run\nit, and you see how it does.",
    "start": "180580",
    "end": "186870"
  },
  {
    "text": "And so what you\nget, you get J. You",
    "start": "186870",
    "end": "191950"
  },
  {
    "text": "get the cost of\nrunning that policy. So the question is\nthat, previously we've",
    "start": "191950",
    "end": "197883"
  },
  {
    "text": "talked about, OK, if you\nhave a model of the system, there's a lot of\nthings you can do. You can do back prop to\nget the specific gradient;",
    "start": "197883",
    "end": "203650"
  },
  {
    "text": "do something like SNOPT, you can\ndo gradient descent using that. You can do, depending\non the dimensionality,",
    "start": "203650",
    "end": "209725"
  },
  {
    "text": "you can do value iteration. So there's a lot of options\nwhen you have a model. But if you don't, if you don't\nknow how the system works, if it's really just a black\nbox where I have a policy",
    "start": "209725",
    "end": "216400"
  },
  {
    "text": "parameterization, I get a cost,\nhow do we achieve anything",
    "start": "216400",
    "end": "222189"
  },
  {
    "text": "in that context? We don't have any\nsort of information about how these things\nrelate to each other. Well, the thing is we do have\nsome information in that we",
    "start": "222190",
    "end": "227800"
  },
  {
    "text": "can execute this black box. We can test it, right? We can run our policy\nand see how well it does. So what would you say\nis the crudest thing",
    "start": "227800",
    "end": "235840"
  },
  {
    "text": "you could do if you had a\nsystem like this, a black box? You give it an open loop\ntape, let's say, you run it,",
    "start": "235840",
    "end": "242587"
  },
  {
    "text": "and it tells you the cost.  What could we do?",
    "start": "242587",
    "end": "248170"
  },
  {
    "text": "AUDIENCE: SNOPT could also-- well, not that we'll\n[INAUDIBLE] SNOPT. But SNOPT could also-- or you have methods for\nestimating the gradient.",
    "start": "248170",
    "end": "254680"
  },
  {
    "text": "JOHN W. ROBERTS: You can do\nfinite differences, right? AUDIENCE: Yeah, do finite-- JOHN W. ROBERTS: So finite\ndifferences, exactly. So what you can\ndo is you can say,",
    "start": "254680",
    "end": "260989"
  },
  {
    "text": "let's talk about, again,\nthis is a notation. [? So ?] what we're using is simple.",
    "start": "260990",
    "end": "266350"
  },
  {
    "text": "A lot of times they parameterize\nit in a different way. But yeah. So you have pretty\nmuch in this context-- let's say we have a\ndeterministic cost.",
    "start": "266350",
    "end": "272380"
  },
  {
    "text": "So we don't have\na random system. We'll talk about\nrandom systems later. Let's say we have a\ndeterministic cost, which",
    "start": "272380",
    "end": "277690"
  },
  {
    "text": "is a function of our alpha. So what are our parameters,\nour parameter vector?",
    "start": "277690",
    "end": "282919"
  },
  {
    "text": "So what we do is we\nsay, OK, let's say I have a 2D system\nalpha 1, alpha 2.",
    "start": "282920",
    "end": "296115"
  },
  {
    "text": "Now, we don't know what\nthis function is right now. But let's just say it's a simple\nfunction like this, convex,",
    "start": "296115",
    "end": "302710"
  },
  {
    "text": "where these are the contour\nlines, and what we want is we want to get to the middle.",
    "start": "302710",
    "end": "308310"
  },
  {
    "text": "So this is sort of the local\nmin, and we start here. Now, what-- how SNOPT\ncould get these gradients,",
    "start": "308310",
    "end": "314440"
  },
  {
    "text": "and sort of the simplest\nthing you can imagine doing, would be, all right, well-- one of the simplest things\nyou can imagine doing",
    "start": "314440",
    "end": "320800"
  },
  {
    "text": "is actually another\nvery simple thing. You measure here. So you run the system. You get J at this point.",
    "start": "320800",
    "end": "327440"
  },
  {
    "text": "So you run the system,\nyou get J at this point. You run the system, you\nget J at this point. But you take these differences,\ndivide by your displacement,",
    "start": "327440",
    "end": "334480"
  },
  {
    "text": "and what you get is you get some\nestimate of the local gradient. And if those distances are\nsmall enough and your evaluation",
    "start": "334480",
    "end": "339880"
  },
  {
    "text": "sort of nice enough, you\ncan get arbitrarily close to the true gradient there. And this will tell you, OK, you\nwant to move in this direction,",
    "start": "339880",
    "end": "346225"
  },
  {
    "text": "right? Now, the problem with this is\nthat you have to do n plus 1, where n is the number of\ndimensions, evaluations,",
    "start": "346225",
    "end": "353169"
  },
  {
    "text": "to get this. So you sort of have\nto evaluate at alpha and at alpha plus delta\n0, 0, 0, dot, dot, dot,",
    "start": "353170",
    "end": "362800"
  },
  {
    "text": "alpha plus 0 delta 0, 0,\ndot, dot, dot, et cetera. Now, obviously,\nthese things just",
    "start": "362800",
    "end": "368470"
  },
  {
    "text": "have to be linearly\nindependent, actually. But you might as\nwell do it this way. Do these finite\ndifferences, you get",
    "start": "368470",
    "end": "374170"
  },
  {
    "text": "an estimate of the gradient. You can hand it\nto SNOPT and SNOPT can try to do fancier things. Or you can do gradient descent,\nwhere you get this gradient,",
    "start": "374170",
    "end": "380260"
  },
  {
    "text": "you compute it, and then you\ndo an update where I say, OK, now my alpha at n plus 1 equals\nalpha at n plus some delta",
    "start": "380260",
    "end": "394060"
  },
  {
    "text": "alpha. And you can say, OK, delta\nalpha equals negative eta",
    "start": "394060",
    "end": "402520"
  },
  {
    "text": "and then dJ d alpha. That's a vector.",
    "start": "402520",
    "end": "408165"
  },
  {
    "text": "And so this is\nour learning rate. That says, OK, we have\nthe gradient here. How far are we going to move? Setting that can be an issue.",
    "start": "408165",
    "end": "414290"
  },
  {
    "text": "But you update your\nalpha like this. And you can just keep doing\nthat over and over again, keep evaluating it\nover and over again.",
    "start": "414290",
    "end": "420170"
  },
  {
    "text": "And eventually, you\nshould move in to the 0.",
    "start": "420170",
    "end": "425380"
  },
  {
    "text": "You should get to a local min. The thing is that doing n\nplus 1 evaluations every time",
    "start": "425380",
    "end": "432850"
  },
  {
    "text": "is expensive. Now, you could say you\ncould cut that down a bit if you were to reuse\nsome evaluations and stuff like that. But the point is that you have\nto do a lot of evaluations",
    "start": "432850",
    "end": "439540"
  },
  {
    "text": "to get this local information. And if you move very\nfar, you sort of have to discard those and do\nall those evaluations again.",
    "start": "439540",
    "end": "445468"
  },
  {
    "text": "So short of doing\na lot of evaluation to get sort of an accurate\nestimate of the gradient right here. Then you're throwing a lot\nof it away when you move,",
    "start": "445468",
    "end": "451447"
  },
  {
    "text": "and the gradient could change. And so in that sense, doing\nall these evaluations maybe is wasteful, because\nyou're getting more--",
    "start": "451447",
    "end": "456580"
  },
  {
    "text": "you're sort of being more\ncareful than you have to. And then you're just going\nto lose that information once you move somewhere else\nand have to evaluate it again.",
    "start": "456580",
    "end": "462368"
  },
  {
    "text": "So there's another thing. This one, you could\nsay, is even more crude. This is, at least in\nevolutionary algorithm",
    "start": "462368",
    "end": "467919"
  },
  {
    "text": "[? screen, ?] I think they\ncall it just hill climbing. I mean, all these things are\nsort of hill climbing or valley descending.",
    "start": "467920",
    "end": "474040"
  },
  {
    "text": "But what you can\nalso imagine doing is just having a point here,\nand now we just randomly perturb that.",
    "start": "474040",
    "end": "479188"
  },
  {
    "text": "So I don't do this. This is deterministic, right? I could randomly perturb\nand just be like, OK, well, what if I'm here?",
    "start": "479188",
    "end": "484607"
  },
  {
    "text": "That's worse, right? The cost is higher. So you just throw it\nout, don't use it. Do it again here, that's better.",
    "start": "484607",
    "end": "491090"
  },
  {
    "text": "So now we just keep this. And we just do this over and\nover, discarding bad ones, keeping good ones,\nuntil we get back there.",
    "start": "491090",
    "end": "500927"
  },
  {
    "text": "But the thing is,\nis that there you're doing all these evaluations. When they get worse, you're\njust throwing them out, and you're acting like they\ngive you no information.",
    "start": "500927",
    "end": "507310"
  },
  {
    "text": "But there is\ninformation in that. Even if it gets worse, and\nby how much it gets worse, how much it gets better, there's\ninformation in all of that.",
    "start": "507310",
    "end": "513157"
  },
  {
    "text": "And you're getting information\nwhen you do the evaluation. So I throw it away and just sort\nof like cast out these things",
    "start": "513158",
    "end": "518260"
  },
  {
    "text": "that do worse. So that's sort of the idea\nof the stochastic gradient",
    "start": "518260",
    "end": "524290"
  },
  {
    "text": "descent, is that we're\ngoing to follow this sort of like random kind of idea. Well, instead of doing this\ndeterministic evaluation",
    "start": "524290",
    "end": "530529"
  },
  {
    "text": "of the local gradient, we're\ngoing to randomize the system, and we're going to get\nan estimate the gradient",
    "start": "530530",
    "end": "535690"
  },
  {
    "text": "stochastically, and then\nwe're going to follow that. And we're going to get\nas much information out of that as possible. That's one of the\nimportant thing,",
    "start": "535690",
    "end": "541587"
  },
  {
    "text": "is generally these systems,\nthis evaluation is all the cost. Pretty much everything\nis dominated by checking your cost of\nthe [INAUDIBLE] policy.",
    "start": "541587",
    "end": "548980"
  },
  {
    "text": "So you want to get as much\nas you can out of each one. And stochastic\ngradient descent is sort of a powerful way of doing\nthat when you have no model.",
    "start": "548980",
    "end": "555610"
  },
  {
    "text": "That's definitely more\nefficient than hill climbing. So now the question is, what\nis the appropriate process",
    "start": "555610",
    "end": "568450"
  },
  {
    "text": "for doing this? How do we randomly\nsample these guys and actually improve our policy?",
    "start": "568450",
    "end": "573847"
  },
  {
    "text": "So I'm going to\nwrite down an update. This is a common update. It's the weight\nperturbation update. It also shows up in an\nidentical form in reinforce,",
    "start": "573848",
    "end": "580165"
  },
  {
    "text": "if you see any of those. We'll talk about all those. But when you can look at the\n[? performance ?] update-- ",
    "start": "580165",
    "end": "587829"
  },
  {
    "text": "is my handwriting\nat all legible? [INTERPOSING VOICES] JOHN W. ROBERTS: Yeah? OK.",
    "start": "587830",
    "end": "594022"
  },
  {
    "text": "So you want to look at this\n[? performance ?] update. ",
    "start": "594022",
    "end": "601690"
  },
  {
    "text": "Take my word for now\nthat this makes sense. ",
    "start": "601690",
    "end": "607450"
  },
  {
    "text": "Well, changing the alpha bits. ",
    "start": "607450",
    "end": "614600"
  },
  {
    "text": "OK. So I'm saying,\nchange your alpha. Here we have the\nsame learning rate. So this is like in the\ndeterministic gradient descent.",
    "start": "614600",
    "end": "621850"
  },
  {
    "text": "And then here's\nwhere you evaluate. And this z is noise. So when you perturb\nyour policy, this is sort of the vector of how\nyou perturb that alpha vector.",
    "start": "621850",
    "end": "629256"
  },
  {
    "text": "So this is sort of-- this is a z, this\nis a z, this is a z. Those z's are these\nperturbations to this.",
    "start": "629256",
    "end": "638097"
  },
  {
    "text": "So what you can do is\nwe can say-- a simple and a very common way is to\nhave the vector z is distributed",
    "start": "638098",
    "end": "645380"
  },
  {
    "text": "as a multivariate Gaussian,\nso where each element of z is iid with the same\nstandard deviation, mean 0.",
    "start": "645380",
    "end": "656480"
  },
  {
    "text": "And so you sort of\ndraw this z from your-- you draw a sort of\nsample z, you evaluate",
    "start": "656480",
    "end": "661880"
  },
  {
    "text": "how well it does, you\nevaluate how well you do with your sort\nof nominal policy right now, calculate\nthis difference,",
    "start": "661880",
    "end": "666980"
  },
  {
    "text": "and then you move in\nthe direction of z. So I'll try to draw this in 1D\nand then 2D so it makes sense.",
    "start": "666980",
    "end": "679089"
  },
  {
    "text": "So here in 1D, you can\nsay this is our 1 alpha. If this is J, so here's\nour cost function.",
    "start": "679090",
    "end": "689940"
  },
  {
    "text": "So we'll be here. Now, our z in this\ncase is just a scalar. But so our z is\ngoing to be mean 0.",
    "start": "689940",
    "end": "696560"
  },
  {
    "text": "And it's going to have\na Gaussian distribution. But when you sample from\nthis, you evaluate--",
    "start": "696560",
    "end": "703948"
  },
  {
    "text": "I should actually probably\nkeep that update up at the same time. ",
    "start": "703948",
    "end": "714686"
  },
  {
    "text": "So you sample, you\nget this change. So this is sort of my J alpha. ",
    "start": "714686",
    "end": "723010"
  },
  {
    "text": "This right here is\nmy J alpha plus z. And imagine this change. That's going to say,\nOK, the cost went up.",
    "start": "723010",
    "end": "730330"
  },
  {
    "text": "It went up by some amount. That's the difference. I'm going to move in\nthe direction of z.",
    "start": "730330",
    "end": "735910"
  },
  {
    "text": "So z is just a scalar. Here it's just going\nto be sort of the sign and the magnitude of it. And then I'm going to move\nsort of opposite this.",
    "start": "735910",
    "end": "741732"
  },
  {
    "text": "So I perturb z. z went\nin this direction. It got bigger. That change, then,\nis a positive number.",
    "start": "741732",
    "end": "747860"
  },
  {
    "text": "So we're going to move down by\namount sort of eta that change, right?",
    "start": "747860",
    "end": "753310"
  },
  {
    "text": "And so if it gets a lot\nworse, we move down farther. If it gets a bit worse,\nwe move down a bit.",
    "start": "753310",
    "end": "758529"
  },
  {
    "text": "Does that makes sense? And so when you're\nmeasuring here, you're going to get small\nchange for the same z.",
    "start": "758530",
    "end": "765220"
  },
  {
    "text": "When you, again, you draw\nyour Gaussian around that, if you get a small\nchange, you're just going to move a bit.",
    "start": "765220",
    "end": "770590"
  },
  {
    "text": "When I'm here, where\nit's really steep, I'll get the same perturbation,\nI'm going to get bigger change. I'm going to move even farther.",
    "start": "770590",
    "end": "776890"
  },
  {
    "text": "And I'll update here. And so if I do this\na bunch of times, you can imagine I\ndescend into local min.",
    "start": "776890",
    "end": "783875"
  },
  {
    "text": "Does that make sense? And this is every time you're\ndrawing the stochastically. So you're not doing this\n[INAUDIBLE] term thing.",
    "start": "783875",
    "end": "788920"
  },
  {
    "text": "Every time you do it,\nyou could be updating, you could try worse,\nyou could try better. But stochastically, you\ncan sort of intuitively",
    "start": "788920",
    "end": "794860"
  },
  {
    "text": "see why it's going\nto sort of descend. Does that make sense? AUDIENCE: This is heavily\ndepending on the fact",
    "start": "794860",
    "end": "800085"
  },
  {
    "text": "that the function is sort\nof [INAUDIBLE] direction? JOHN W. ROBERTS:\nIt's sort of what? AUDIENCE: It's like the\nfunction that you're looking at,",
    "start": "800085",
    "end": "807163"
  },
  {
    "text": "if you're looking-- if you increase,\nlike in this case, like alpha in one\ndirection versus other one,",
    "start": "807163",
    "end": "813149"
  },
  {
    "text": "the changes are sort of\nsimilar in both ways. JOHN W. ROBERTS: No. I mean, that can affect the\nperformance of the algorithm.",
    "start": "813149",
    "end": "819770"
  },
  {
    "text": "But yeah, I can draw that. These are sort of common\npathological cases.",
    "start": "819770",
    "end": "824807"
  },
  {
    "text": "Let's look at in 2D. So this is what\nyou're saying, right? Now, the ideal one would be--",
    "start": "824807",
    "end": "830060"
  },
  {
    "text": "again, we can draw\na contour map again. Now, you'd be-- this is\nabout the same, right?",
    "start": "830060",
    "end": "835740"
  },
  {
    "text": "You're saying this is about\nsort of isotropic or whatever. You're here, you perturb\nyourself randomly",
    "start": "835740",
    "end": "841464"
  },
  {
    "text": "so your Gaussian is going\nto put you anywhere here. And you measure somewhere. You get better, so you're going\nto move in that direction,",
    "start": "841465",
    "end": "847200"
  },
  {
    "text": "depending on what eta is,\nand I'll get an update. You're saying, well, what\nhappens if we're actually in trouble, and\nwe have something",
    "start": "847200",
    "end": "862623"
  },
  {
    "text": "that looks like this, right?  Saying that's a problem?",
    "start": "862623",
    "end": "868300"
  },
  {
    "text": "Well, that can hurt\nthe convergence of it. It can be slower. But it still works. Because you can see--\nlike, let's say I'm here.",
    "start": "868300",
    "end": "875078"
  },
  {
    "text": "Now, it's really steep here,\nand it's really shallow here. So what's going to happen\nis when I perturb it, I'm still going to--",
    "start": "875078",
    "end": "881015"
  },
  {
    "text": "my perturbation\nin this direction is going have an effect--\nmaybe it's relatively shallow-- but then in this direction it's\ngoing to be very sensitive.",
    "start": "881015",
    "end": "886120"
  },
  {
    "text": "And so when I move it more\nin this direction and I move very far, and I'm going\nto go down here first-- I'm going to descend\nthe steep part--",
    "start": "886120",
    "end": "891942"
  },
  {
    "text": "and then slowly converge\nin on the shallow part. That's sort of called the,\nI think, the banana problem,",
    "start": "891942",
    "end": "897182"
  },
  {
    "text": "where you sort of have\nthis massive bowl, and you go really quick right\ndown here, and then really slowly. And so the thing is that\nif it's all very shallow,",
    "start": "897182",
    "end": "904330"
  },
  {
    "text": "that's not a problem. You can make your\nlearning rate bigger, you can make your\nsample further out, and then it sort of just\ndoesn't matter, right?",
    "start": "904330",
    "end": "909788"
  },
  {
    "text": "But this asymmetry in\nthese things is an issue. Now, there are some\nways of dealing with that if you have an\nidea of how asymmetric it is.",
    "start": "909788",
    "end": "916650"
  },
  {
    "text": "We can talk about this later. But it'll still descend. And actually, you\ncan show, and I'm",
    "start": "916650",
    "end": "922230"
  },
  {
    "text": "about to show, that\nthis update actually follows an expectation\nit moves in the direction",
    "start": "922230",
    "end": "928320"
  },
  {
    "text": "of the true gradient. So, I mean, randomly it\ncan bounce all around.",
    "start": "928320",
    "end": "934230"
  },
  {
    "text": "But in expectation, it will\nmove in the right direction. And if you're having\ndeterministic evaluations--",
    "start": "934230",
    "end": "939755"
  },
  {
    "text": "well, we're going to do a\nlinear analysis at first. But you actually can\nshow that it'll always",
    "start": "939755",
    "end": "945720"
  },
  {
    "text": "move within 90 degrees\nof the true gradient if you have\ndeterministic valuations. So you'll never\nactually get worse.",
    "start": "945720",
    "end": "952380"
  },
  {
    "text": "You can move parallel\nand not improve, but you'll never move\nsort of the wrong sign",
    "start": "952380",
    "end": "957990"
  },
  {
    "text": "of [? those two ?] parameters. All right. ",
    "start": "957990",
    "end": "965020"
  },
  {
    "text": "So then yeah. Let's look at why that\nis in some detail then. ",
    "start": "965020",
    "end": "974210"
  },
  {
    "text": "So again, our delta\nalpha, same up there.",
    "start": "974210",
    "end": "981310"
  },
  {
    "text": "So I just won't waste\ntime rewriting it. And let's do-- let's look at\nit in a first-order Taylor",
    "start": "981310",
    "end": "986550"
  },
  {
    "text": "expansion of our cost function. So look at it locally\nwhere you look at sort of like the linear term.",
    "start": "986550",
    "end": "994530"
  },
  {
    "text": "So our J-- and we\nlinearize around alpha.",
    "start": "994530",
    "end": "999840"
  },
  {
    "text": "So our J of alpha\nplus z, well, that",
    "start": "999840",
    "end": "1008350"
  },
  {
    "text": "is approximately equal to-- for small z-- alpha plus\ndJ d alpha transpose z.",
    "start": "1008350",
    "end": "1026410"
  },
  {
    "text": " So that's the first-order\nTaylor expansion.",
    "start": "1026410",
    "end": "1033279"
  },
  {
    "text": "Now, if we examine\nthis then, we plug this in for J alpha plus\nz in that update,",
    "start": "1033280",
    "end": "1039880"
  },
  {
    "text": "we're going to cancel out of\nthis term, our J alpha term, and we're going to get that\ndelta alpha approximately",
    "start": "1039880",
    "end": "1051130"
  },
  {
    "text": "negative eta dJ d alpha z z.",
    "start": "1051130",
    "end": "1063356"
  },
  {
    "start": "1063356",
    "end": "1068440"
  },
  {
    "text": "So now, what does\nthis look like? This is sort of\nlike a dot product between the gradient\nwith respect",
    "start": "1068440",
    "end": "1073910"
  },
  {
    "text": "to alpha and our noise vector. All right?",
    "start": "1073910",
    "end": "1079690"
  },
  {
    "text": "So and this is going to be\nabout equal to negative eta.",
    "start": "1079690",
    "end": "1084789"
  },
  {
    "text": "This thing we can then write\n[INAUDIBLE] i is 1 to N",
    "start": "1084790",
    "end": "1090670"
  },
  {
    "text": "of dJ d alpha i\nzi times vector z.",
    "start": "1090670",
    "end": "1104290"
  },
  {
    "start": "1104290",
    "end": "1115050"
  },
  {
    "text": "All right. So here then, if we\nmultiply that out,",
    "start": "1115050",
    "end": "1122289"
  },
  {
    "text": "so we're going to get this\nvector and eta, because you're multiplying that coefficient\ntimes each term individually.",
    "start": "1122290",
    "end": "1129867"
  },
  {
    "text": "You're going to get the vector. ",
    "start": "1129868",
    "end": "1148800"
  },
  {
    "text": "And then the same thing. This one's going to be\nsome dJ d alpha zi zN.",
    "start": "1148800",
    "end": "1158620"
  },
  {
    "text": " Now, if we take\nexpectation of this,",
    "start": "1158620",
    "end": "1165893"
  },
  {
    "text": "we get another distribution. We know that each zi is iid. Do you know iid? That they're all--\nthey're all distributed",
    "start": "1165893",
    "end": "1172530"
  },
  {
    "text": "the exact same distribution,\nall the sort of mean 0, Gaussian, standard\ndeviation sigma. And they're all independent.",
    "start": "1172530",
    "end": "1178060"
  },
  {
    "text": "So if we do that, we can then\ntake the expectation of delta",
    "start": "1178060",
    "end": "1186775"
  },
  {
    "text": "alpha.  We can pull that eta out front,\nbecause expectation is linear.",
    "start": "1186775",
    "end": "1195030"
  },
  {
    "text": "And what you'll get is you'll\nget the [INAUDIBLE] again,",
    "start": "1195030",
    "end": "1201510"
  },
  {
    "text": "dJ d alpha-- i is not\na random variable. So pull that out.",
    "start": "1201510",
    "end": "1207920"
  },
  {
    "text": "dJ d alpha i, again the sum z--",
    "start": "1207920",
    "end": "1217410"
  },
  {
    "text": "sorry.  Expectation of zi then z1.",
    "start": "1217410",
    "end": "1229980"
  },
  {
    "text": "Now, this sum goes\nthrough all the i's. But the first one only\nhas that z1, right?",
    "start": "1229980",
    "end": "1237480"
  },
  {
    "text": "Now, zi, z1, they're\nindependent, mean 0. So you can sort\nof split these up,",
    "start": "1237480",
    "end": "1243750"
  },
  {
    "text": "and you're going to get that\nthey're 0 for every term, except for the term\nwhere i equals 1,",
    "start": "1243750",
    "end": "1249780"
  },
  {
    "text": "and the second one where\nequals 2, et cetera, right? All the other terms\nare going to go to 0. So it's easy, then. To get the expectations,\nyou go through the sum,",
    "start": "1249780",
    "end": "1257178"
  },
  {
    "text": "and you're going to\nsee that you only have the one where you have\nexpectation of z1 squared, expectation of z2 squared.",
    "start": "1257178",
    "end": "1262900"
  },
  {
    "text": " Now, the expectation-- again,\nmaybe you remember variance",
    "start": "1262900",
    "end": "1275280"
  },
  {
    "text": "equals expected value of x\nsquared minus expected value",
    "start": "1275280",
    "end": "1281430"
  },
  {
    "text": "of x squared, right? Now, we're mean 0, so this is 0.",
    "start": "1281430",
    "end": "1287190"
  },
  {
    "text": "Our variance is sigma\nsquared, So our expected value of x squared is sigma squared. So that means that each\none of these expectations",
    "start": "1287190",
    "end": "1293710"
  },
  {
    "text": "is going to be sigma squared. So you're going to end up where\nyou have negative eta-- now,",
    "start": "1293710",
    "end": "1299070"
  },
  {
    "text": "they all the same sigma,\nso we can pull that out-- sigma squared. And you're going to have the\nvector of this dJ d alpha 1,",
    "start": "1299070",
    "end": "1305670"
  },
  {
    "text": "dJ d alpha 2, et cetera. So you're going\nto get dJ d alpha.",
    "start": "1305670",
    "end": "1314616"
  },
  {
    "text": "So yeah, so the\nexpectation, this update, when we look at it in\nthis sort of linear sense, is eta sigma squared-- so\njust these are scalars.",
    "start": "1314616",
    "end": "1321540"
  },
  {
    "text": "They just change\nthe magnitude of it. But it's in the direction\nof the gradient. And eta is sort\nof our parameter.",
    "start": "1321540",
    "end": "1327070"
  },
  {
    "text": "We can control it.  That makes sense?",
    "start": "1327070",
    "end": "1332644"
  },
  {
    "text": "AUDIENCE: Is that sigma squared? JOHN W. ROBERTS: Yes, yes. Sorry. ",
    "start": "1332644",
    "end": "1337950"
  },
  {
    "text": "Yeah, sorry. Yeah. So the noise you\nuse pops out here. Comment-- I actually oftentimes\nin one of the other--",
    "start": "1337950",
    "end": "1344679"
  },
  {
    "text": "when we look at this\nalgorithm in a different way, they write the\nupdate where it's eta over sigma squared, your noise.",
    "start": "1344680",
    "end": "1351615"
  },
  {
    "text": "And then that cancels\nout that sigma squared, and you purely just\nget eta dJ d alpha. So you can put that in, too,\nif you wanted to really just be",
    "start": "1351615",
    "end": "1357990"
  },
  {
    "text": "eta times your true gradient. But the important\nthing is that you'll",
    "start": "1357990",
    "end": "1363570"
  },
  {
    "text": "move an expectation\nin the true direction. ",
    "start": "1363570",
    "end": "1371860"
  },
  {
    "text": "So a couple of interesting\nproperties to this.",
    "start": "1371860",
    "end": "1376929"
  },
  {
    "text": "Here, you see we\nstill have to do-- we still have to do two\nevaluations to give rid of the update, right?",
    "start": "1376930",
    "end": "1382870"
  },
  {
    "text": "If we want to cancel\nout that J alpha term, we're going to have\nto evaluate it twice.",
    "start": "1382870",
    "end": "1388240"
  },
  {
    "text": "Now, it doesn't matter\nfor three-dimensional, and we only have to\nevaluate it twice. But we still have to\nevaluate it two times.",
    "start": "1388240",
    "end": "1393422"
  },
  {
    "text": "And the question is,\nwell, what happens if you don't evaluate it at J alpha?",
    "start": "1393422",
    "end": "1399580"
  },
  {
    "text": "What happens if you\nonly evaluate it once? Well, that's a very common\nthing to do, actually, and doesn't actually affect\nyour expectation at all.",
    "start": "1399580",
    "end": "1407572"
  },
  {
    "text": "Lots of times,\ninstead of this sort of like your perfect baseline\nwhere you evaluate it, people sometimes average\nthe last several evaluations",
    "start": "1407572",
    "end": "1413625"
  },
  {
    "text": "to get that\nbaseline-- oh, sorry. I don't think I\ndefined baseline. This right here, whatever\nit is, is your baseline.",
    "start": "1413625",
    "end": "1421107"
  },
  {
    "text": " Now, there doesn't\nhave to be J of alpha.",
    "start": "1421107",
    "end": "1426640"
  },
  {
    "text": "It can be an exponentially\ndecaying average of your last\nseveral evaluations. That's going be\napproximately J of alpha.",
    "start": "1426640",
    "end": "1433780"
  },
  {
    "text": "And it won't be\nperfect, but the point is that it's not\ngoing to affect it, and we're going to see that.",
    "start": "1433780",
    "end": "1439269"
  },
  {
    "text": "Maybe you'd expect that you\nneed to get rid of that term for you're still moving in the\ndirection [? of ?] [? your ?] gradient, because you can\nimagine if you don't have that,",
    "start": "1439270",
    "end": "1446919"
  },
  {
    "text": "if you don't know that\nterm, you could evaluate-- if it's always\npositive, you'll--",
    "start": "1446920",
    "end": "1452398"
  },
  {
    "text": "I'll draw a diagram,\nmake this clear.  If you don't have that, and\nyou're here, if I-- let's say",
    "start": "1452398",
    "end": "1461850"
  },
  {
    "text": "I just make that 0. I'm going to\nevaluate here, that's going to be a positive number. So I'm moving in the\nopposite direction.",
    "start": "1461850",
    "end": "1468207"
  },
  {
    "text": "If I evaluate here, it's\ngoing to be positive number. So you're going to move\nin the opposite direction. So maybe you think like,\noh, without that baseline we could be in bad shape.",
    "start": "1468207",
    "end": "1474679"
  },
  {
    "text": "But actually, you'll move\nmore in this direction when you do that sample than\nyou move in this direction when you do the other sample.",
    "start": "1474680",
    "end": "1481269"
  },
  {
    "text": "And so that scale\nhere, the fact that you move proportional to how big\nthe change is in your cost,",
    "start": "1481270",
    "end": "1487210"
  },
  {
    "text": "it means that in\nexpectation, you'll still move in the direction\nof the true gradient. Now, in practice you\nwon't do as well.",
    "start": "1487210",
    "end": "1492679"
  },
  {
    "text": "It makes sense that\nyou won't do as well. Really, when you\nthink about it, that's going to be bouncing\nall around crazily. But it'll still move in\nthe direction the gradient.",
    "start": "1492680",
    "end": "1498770"
  },
  {
    "text": "And you don't just have\nto take my word for that. If you look at\nthis update again,",
    "start": "1498770",
    "end": "1510730"
  },
  {
    "text": "now we can do linear\nexpansion again, and you'll get this dJ d alpha\nz plus, say, some scalar--",
    "start": "1510730",
    "end": "1524440"
  },
  {
    "text": "this is uncorrelated\nwith the noise. That's an important\nthing, though. It's uncorrelated\nwith the noise z.",
    "start": "1524440",
    "end": "1530620"
  },
  {
    "text": "Now, use expectation again. Expectation is linear. So we have expectation\nof this term. That's the same\nas it was before.",
    "start": "1530620",
    "end": "1536530"
  },
  {
    "text": "That's the gradient. And then we have expectation\nof negative eta dz.",
    "start": "1536530",
    "end": "1543920"
  },
  {
    "text": " Now, E is uncorrelated\nwith the noise. These are both scalar, so you\ncan actually pull them out.",
    "start": "1543920",
    "end": "1551295"
  },
  {
    "text": "Expectation of z, it's mean 0. So this won't affect it at all. So really, your expected update\nwill not depend it all on what",
    "start": "1551295",
    "end": "1557770"
  },
  {
    "text": "you use here. So you could put\na constant there. You could put in the exact one. You could put in some decaying\naverage, anything you want.",
    "start": "1557770",
    "end": "1567310"
  },
  {
    "text": "It will still move,\nin expectation, in the right direction. But in practice, it\ncan a huge difference. I don't know if anyone's\nimplemented these things on--",
    "start": "1567310",
    "end": "1575140"
  },
  {
    "text": "but a good baseline can be\nthe difference between success and getting completely stuck\nand not moving anywhere.",
    "start": "1575140",
    "end": "1580990"
  },
  {
    "text": "So if you do small updates,\nyou should still be OK. But performance depends a\nlot on getting a baseline.",
    "start": "1580990",
    "end": "1587802"
  },
  {
    "text": "Or it can depend a lot. Sometimes it doesn't matter.  Right.",
    "start": "1587802",
    "end": "1594460"
  },
  {
    "text": "So the-- yeah. So again, a common thing to use\nhere is that you're evaluating,",
    "start": "1594460",
    "end": "1601270"
  },
  {
    "text": "you're updating. Let's say every time I do\none evaluation, I update. If I took my last 10 of\nthem, I averaged them",
    "start": "1601270",
    "end": "1607037"
  },
  {
    "text": "with decaying sort of weight\nso that the most recent one is the most heavily\nweighted, then I'm sure you'll get an\napproximation of how much",
    "start": "1607037",
    "end": "1612990"
  },
  {
    "text": "should it be around here. And then I update based on that. And that way you don't have to\nevaluate it twice every time.",
    "start": "1612990",
    "end": "1618020"
  },
  {
    "text": "And so that way,\nyou can actually get sort of improved\nperformance. And it's still going to work. And another cool\nthing, this is sort",
    "start": "1618020",
    "end": "1623590"
  },
  {
    "text": "of when we go back\nto our assumptions about deterministic. It doesn't have to be\ndeterministic, either. Let's say in the same way\nwe put in this, instead",
    "start": "1623590",
    "end": "1632230"
  },
  {
    "text": "let's say we put in noise like,\nagain, like a scalar noise to evaluation w.",
    "start": "1632230",
    "end": "1639412"
  },
  {
    "text": "Oh, I just got [? color. ?] Now, that's going to\nshow up in here again.",
    "start": "1639412",
    "end": "1645670"
  },
  {
    "text": "Now, it's not a-- now it's a random variable,\nso it has an expectation. But if they're uncorrelated,\nwe can split them up.",
    "start": "1645670",
    "end": "1652780"
  },
  {
    "text": "We can-- that'll be equal to\nnegative eta expectation w",
    "start": "1652780",
    "end": "1660100"
  },
  {
    "text": "expectation z. Now, we know that\nz is mean 0 again.",
    "start": "1660100",
    "end": "1665200"
  },
  {
    "text": "That's 0. So it's not going\nto affect either. We're still going\nto get this term. And so you can add sort\nof additive random noise,",
    "start": "1665200",
    "end": "1671360"
  },
  {
    "text": "and you'll still move that\nthrough expected direction, the gradient. So that's sort of cool. This is quite robust. You can have these\nerrors in this baseline.",
    "start": "1671360",
    "end": "1678142"
  },
  {
    "text": "You can have noisy evaluations. You can have all\nsorts of these things. And still, expectation will\nmove in the right direction.",
    "start": "1678142",
    "end": "1684625"
  },
  {
    "text": "So that's nice.  We're going to see that that\nhas a lot of practical benefits.",
    "start": "1684625",
    "end": "1695130"
  },
  {
    "text": "Is everybody with me here? I don't know if I went\nthrough this quickly or if--",
    "start": "1695130",
    "end": "1701268"
  },
  {
    "text": "everyone's sort of being quiet. They look sort of-- AUDIENCE: w is baseline there? JOHN W. ROBERTS: No, no, sorry. This w I change it to noise.",
    "start": "1701268",
    "end": "1707160"
  },
  {
    "text": "Sorry, this is a noise. Maybe you'd prefer it to be\ncalled like xi or something",
    "start": "1707160",
    "end": "1712310"
  },
  {
    "text": "like that. But this is just added noise. So you could say that\nz is drawn from--",
    "start": "1712310",
    "end": "1719282"
  },
  {
    "text": "it doesn't really\nmatter the distribution as long as it's uncorrelated. We could say it's drawn\nfrom some other Gaussian.",
    "start": "1719282",
    "end": "1728690"
  },
  {
    "text": "And so it's expectation-- I mean, expectation of\nthis really can be 0, too. Because if it's not non-zero--\nit's not mean 0 noise,",
    "start": "1728690",
    "end": "1734223"
  },
  {
    "text": "then you might as well just\nput that in your cost function and make it mean 0 again, right? Yes?",
    "start": "1734223",
    "end": "1739669"
  },
  {
    "text": "AUDIENCE: So the idea is to\nadd this into the term J alpha? Or replace the term J alpha\nwith different baseline?",
    "start": "1739670",
    "end": "1745610"
  },
  {
    "text": "JOHN W. ROBERTS:\nReplace it, right. AUDIENCE: OK. And then so what\ncancels-- so when we talk about a Taylor expansion?",
    "start": "1745610",
    "end": "1750649"
  },
  {
    "text": "What cancels-- what-- JOHN W. ROBERTS: Nothing. Nothing cancels it. You see, that's the thing. Yeah, so I put an E here--",
    "start": "1750650",
    "end": "1757490"
  },
  {
    "text": "maybe I'm reusing\ntoo many things. AUDIENCE: Oh, is it J alpha\nis also uncorrelated to z?",
    "start": "1757490",
    "end": "1763649"
  },
  {
    "text": "JOHN W. ROBERTS:\nWell, J alpha, J alpha is just a scalar, right? I mean, it is some number. So it is--",
    "start": "1763650",
    "end": "1769010"
  },
  {
    "text": "AUDIENCE: z is your mean, so. JOHN W. ROBERTS: Yeah,\nso z is your mean. So whether-- we\ncould put in J alpha. We could put an estimate of\nJ alpha that has some error.",
    "start": "1769010",
    "end": "1774920"
  },
  {
    "text": "And then our J\nalpha minus this is going to be some\nnumber-- doesn't matter. If we just put in\nnothing at all, then",
    "start": "1774920",
    "end": "1780350"
  },
  {
    "text": "our error is sort of\nthat J alpha term. That J alpha term\nis just, again, some number that's\nuncorrelated, gets rid of it.",
    "start": "1780350",
    "end": "1786165"
  },
  {
    "text": "Does that make sense? Everyone looks sort of just--",
    "start": "1786165",
    "end": "1791253"
  },
  {
    "text": "AUDIENCE: So it's actually,\nputting another constant in that equation for the\nupdate makes you move more in some random z-direction.",
    "start": "1791253",
    "end": "1797370"
  },
  {
    "text": "But on average, you're still\ngoing down the gradient the same way. JOHN W. ROBERTS: Yeah. I mean, you can move more. Yeah.",
    "start": "1797370",
    "end": "1802440"
  },
  {
    "text": "I mean, if you put some-- if\nyou put some giant constant every time you update, maybe\nyou'll bounce around farther.",
    "start": "1802440",
    "end": "1807779"
  },
  {
    "text": "But on average, you'll still\nmove in the right direction. Because you'll move farther in\nthe right direction than you move in the wrong direction. So they sort of cancel out.",
    "start": "1807780",
    "end": "1813605"
  },
  {
    "text": " So everybody is on board here?",
    "start": "1813605",
    "end": "1818770"
  },
  {
    "text": "OK. I just really want you to-- AUDIENCE: Why wouldn't you\ninclude the actual J alpha? JOHN W. ROBERTS: Well,\nbecause if you get it",
    "start": "1818770",
    "end": "1826270"
  },
  {
    "text": "by evaluating the function,\nif you run a policy, it can be expensive to\nget that J alpha, right?",
    "start": "1826270",
    "end": "1831820"
  },
  {
    "text": "Because for example, I\nuse this in some work I did where we had\nthis flapping thing. I'll show you videos of it. Maybe I'll start setting\nthat up right now.",
    "start": "1831820",
    "end": "1838600"
  },
  {
    "text": "But so we have this\nflapping system. And we get-- we sort\nof have souped it",
    "start": "1838600",
    "end": "1843601"
  },
  {
    "text": "up now so it's a bit quicker. But it used to be\nevery time I wanted to evaluate the function, I\nhad to sit there for 4 minutes",
    "start": "1843602",
    "end": "1849880"
  },
  {
    "text": "and have this sort of\nplate flap in this water and measure how quickly it\nwas going, all these things. And so to evaluate that function\nonce, it took me 4 minutes.",
    "start": "1849880",
    "end": "1858220"
  },
  {
    "text": "And so avoiding\nevaluations is important. And so if you can just take your\nseveral previous evaluations,",
    "start": "1858220",
    "end": "1865695"
  },
  {
    "text": "average them\ntogether-- now, it's not going to be a\nperfect assignment, but maybe it's an OK\nestimate, and then you don't have to\nspend any more time.",
    "start": "1865695",
    "end": "1871730"
  },
  {
    "text": "And so in that sense,\nit's sort of cheaper. Please ask as many questions\nas possible, because this is--",
    "start": "1871730",
    "end": "1878910"
  },
  {
    "text": "AUDIENCE: But at some point\nyou have to measure every time, right? JOHN W. ROBERTS: You have to. Yeah, you have to\nmeasure every time",
    "start": "1878910",
    "end": "1883990"
  },
  {
    "text": "when you want to do an update. But the thing is that-- here. ",
    "start": "1883990",
    "end": "1890910"
  },
  {
    "text": "Let's say i, a tiny one--",
    "start": "1890910",
    "end": "1897540"
  },
  {
    "text": "but the question is, if I\nhave some estimate of that-- let's say my current\nsort of alpha is here.",
    "start": "1897540",
    "end": "1903840"
  },
  {
    "text": "Now, I need to randomly\nsample something, so I have to do that evaluation. Now, the question is, do I\nhave to evaluate it here, too?",
    "start": "1903840",
    "end": "1910679"
  },
  {
    "text": "Because this is my J alpha. Do I evaluate that? ",
    "start": "1910680",
    "end": "1917280"
  },
  {
    "text": "Now, I could estimate\nthis, because have a bunch of other evaluations\nfrom however I got here, right?",
    "start": "1917280",
    "end": "1922797"
  },
  {
    "text": "So I've already evaluated. If I average those\ntogether, I'll get a pretty good\nidea of what this is. If I wanted to get\nit exactly, I'd",
    "start": "1922797",
    "end": "1928950"
  },
  {
    "text": "have to run my system here,\nand then run it again here. And so every update would\nrequire two evaluations",
    "start": "1928950",
    "end": "1936240"
  },
  {
    "text": "as opposed to just one. Now, sometimes it\nstill makes sense to do that evaluation, though. Depending on how your system\nis, if it's really noisy,",
    "start": "1936240",
    "end": "1943470"
  },
  {
    "text": "if you have to do really\nbig updates, it makes sense. AUDIENCE: [INAUDIBLE] using\nthis delta alpha would you",
    "start": "1943470",
    "end": "1949411"
  },
  {
    "text": "calculate [INAUDIBLE]? JOHN W. ROBERTS:\nPardon [INAUDIBLE]?? AUDIENCE: Yes. JOHN W. ROBERTS: I'm sorry,\nI didn't hear what you said. AUDIENCE: This new\nalpha that we have,",
    "start": "1949411",
    "end": "1955648"
  },
  {
    "text": "that we have the\n[INAUDIBLE] before-- JOHN W. ROBERTS: This one? Yeah. AUDIENCE: You calculate it\nby having a previous alpha, and then we did\nthis thing, and--",
    "start": "1955648",
    "end": "1962230"
  },
  {
    "text": "JOHN W. ROBERTS: And I moved\nin that direction, right. AUDIENCE: Right. But you're saying that you\ndon't want to calculate the value for this new alpha.",
    "start": "1962230",
    "end": "1967980"
  },
  {
    "text": "Instead we use\nlike, for example, 10 past history of J of alpha,\nand use that as your estimate.",
    "start": "1967980",
    "end": "1974340"
  },
  {
    "text": "JOHN W. ROBERTS: Yeah. You're saying that\ndoesn't make sense to you? AUDIENCE: It does make sense. In some cases I can think\n[INAUDIBLE] actually",
    "start": "1974340",
    "end": "1981460"
  },
  {
    "text": "[INAUDIBLE] if the change-- a small change in alpha would\nhave a huge effect on the end",
    "start": "1981460",
    "end": "1987880"
  },
  {
    "text": "value [INAUDIBLE] from J-- like, if you have a very\ndiscrete-- like, [INAUDIBLE]",
    "start": "1987880",
    "end": "1993860"
  },
  {
    "text": "condition pass over [INAUDIBLE]. JOHN W. ROBERTS: If you\nmove very violently, yeah. So I mean, that's a good\nexample in practice.",
    "start": "1993860",
    "end": "1999340"
  },
  {
    "text": "I mean, there's\nthings that we have in the theory like\nthis expectation stuff. And there's things that I've\napplied to several systems.",
    "start": "1999340",
    "end": "2004950"
  },
  {
    "text": "And in practice, when you\nhave like sort of really bad policies, and you need\nto move really far in state space-- let's say\nthat right now you're",
    "start": "2004950",
    "end": "2010908"
  },
  {
    "text": "trying to swing up a cart-pole,\nand you're not going anywhere near the top. And your reward function doesn't\nhave very smooth gradients,",
    "start": "2010908",
    "end": "2016980"
  },
  {
    "text": "and so you can't just sort of\nswing up a bit by bit by bit. Well, a good thing is, is to\nput in place possibly very",
    "start": "2016980",
    "end": "2022230"
  },
  {
    "text": "big noise, a very big eta, and\nthen do these two evaluations. Because if you-- it's going\nto change so much every time",
    "start": "2022230",
    "end": "2028440"
  },
  {
    "text": "you do it. Like for example, if you\njump and suddenly you're doing a lot better, then\nyour previous average is not going to\nbe representative.",
    "start": "2028440",
    "end": "2033973"
  },
  {
    "text": "And then you can\nactually bounce around. You can bounce\naround so violently in this big space of policies\nthat you never improve, right?",
    "start": "2033973",
    "end": "2041160"
  },
  {
    "text": "I don't-- maybe I should draw\na diagram to make this more clear, what I'm saying. But the key thing\nis, is that, yeah,",
    "start": "2041160",
    "end": "2046693"
  },
  {
    "text": "if you're moving these\nreally big jumps, and your cost is changing\na lot every time, and you still want to sort of\nmove in the right direction,",
    "start": "2046693",
    "end": "2052906"
  },
  {
    "text": "doing two evaluations\ncan make sense. Because if you're\nstuck to where you don't have good gradients\nin your cost function,",
    "start": "2052906",
    "end": "2058290"
  },
  {
    "text": "a bunch of little updates\nwhich slowly would climb aren't going to give you\nanything, because maybe they're not even differentiable. Maybe you have some sort of\ndiscrete way of measuring",
    "start": "2058290",
    "end": "2065148"
  },
  {
    "text": "a reward, like how many time\nsteps you spend in some goal region, or something, and\nyou don't have any time steps there, there's no\ngradient at all right now.",
    "start": "2065148",
    "end": "2071649"
  },
  {
    "text": "And so you need to be violent\nenough in sort of your policy changes that you\neventually get it to where you're into that goal region. And once you get in\nthat goal region,",
    "start": "2071650",
    "end": "2077340"
  },
  {
    "text": "now you have some gradients\nand you're in good shape. So that's actually another thing\nthat I was going to talk about.",
    "start": "2077340",
    "end": "2082477"
  },
  {
    "text": "But designing your cost\nfunction is extremely important. There are cost functions\nthat can be extremely poor",
    "start": "2082477",
    "end": "2088590"
  },
  {
    "text": "and doing this can\nwork really poorly on. And there's cost functions\nthat can make it a lot easier. So if you have a cost function\nwhich is relatively smooth,",
    "start": "2088590",
    "end": "2096899"
  },
  {
    "text": "if it's-- ideally it doesn't\nhave this sort of banana problem. If it's relatively same in\nall the different parameters,",
    "start": "2096900",
    "end": "2102730"
  },
  {
    "text": "it can work a lot better. And you can sort of formulate\nthe same task lots of time, since lots of times your cost\nfunction isn't what you really",
    "start": "2102730",
    "end": "2109512"
  },
  {
    "text": "want to optimize. It's just of a proxy for\ntrying to get something done. That's what Russ talked about\nhe didn't care about optimality. It's like, here's a\ncost function that",
    "start": "2109512",
    "end": "2115993"
  },
  {
    "text": "gives us a means of\nsolving how to do this. And so there's sort of a\nwhole bunch of cost functions you can imagine coming up that\ntry to encapsulate that task.",
    "start": "2115993",
    "end": "2123069"
  },
  {
    "text": "Now, if you come up with-- for\nthe perch one, for example, this plane perching, which\nis a difficult problem,",
    "start": "2123070",
    "end": "2128535"
  },
  {
    "text": "and a problem where the\nmodels are very bad-- I mean, the aerodynamic\nmodels of this plane flying like that\nare extremely poor.",
    "start": "2128535",
    "end": "2134110"
  },
  {
    "text": "And we have-- we actually\nhave some decent ones. We spent a lot of work\ntrying to get decent ones. But sort of the\nhigh-fidelity kind of region,",
    "start": "2134110",
    "end": "2139410"
  },
  {
    "text": "where you really want\nto just get at the end, it's hard to model that. So the thing is that,\nwhat if you had a cost",
    "start": "2139410",
    "end": "2145092"
  },
  {
    "text": "function, like what\nwe really care about is hitting that perch. So let's say that we give\nyou a 1 if you hit the perch and a 0 everywhere else.",
    "start": "2145093",
    "end": "2151164"
  },
  {
    "text": "Now, that means until\nwe hit the perch, we're getting no information. We could be getting\nreally close, we could be really far away. It's not going to\ntell us anything.",
    "start": "2151165",
    "end": "2157079"
  },
  {
    "text": "Now, a lot of actually\nreinforcement learning has these sort of rewards, like\nthese sort of delayed rewards",
    "start": "2157080",
    "end": "2162630"
  },
  {
    "text": "where you get it\nhere, and then you have to sort of\npropagate that back. When you're trying to\naccomplish a task like that, that doesn't necessarily\nwork that well.",
    "start": "2162630",
    "end": "2168937"
  },
  {
    "text": "If you measure\nsomething like distance from the perch of distance\nfrom your desired state, if you get a little bit\ncloser to your desired state,",
    "start": "2168937",
    "end": "2174160"
  },
  {
    "text": "you sort of get a\nlittle bit better. And then you can\nmeasure the gradient. And so that will make a\nbig difference, right?",
    "start": "2174160",
    "end": "2179970"
  },
  {
    "text": "And so if you had\nsomething where you have region\nof state where you have a good gradient\nin your cost function,",
    "start": "2179970",
    "end": "2185609"
  },
  {
    "text": "and you're out here, and\nnot getting a gradient, the little perturbations\nyou're going to have to random walk\nsort of have made you",
    "start": "2185610",
    "end": "2190650"
  },
  {
    "text": "no update at all, because\nyou may get no change. But if you do really\nbig ones, maybe you'll bounce into where you\nget this region where",
    "start": "2190650",
    "end": "2195750"
  },
  {
    "text": "you're getting some reward. And in that case,\nthese updates are so big that averaging\ndoesn't make sense, a baseline still gives\nyou a big advantage,",
    "start": "2195750",
    "end": "2201932"
  },
  {
    "text": "and maybe two\nevaluations is worth it. In some of the\nflapping stuff I did, I did two evaluations,\nbecause when",
    "start": "2201932",
    "end": "2207660"
  },
  {
    "text": "I was moving very\nviolently, because averaging didn't work that well. And getting a good baseline\nwas worth the extra time.",
    "start": "2207660",
    "end": "2213973"
  },
  {
    "text": "But when we ended up\ngetting it working, we put it online, and we\nactually-- we update it every time we flapped.",
    "start": "2213973",
    "end": "2219750"
  },
  {
    "text": "So it was just 1 second,\nflap, update, flap, update. And that way, we\npretty much were able to sort of cut our time in\nhalf, because our policies were",
    "start": "2219750",
    "end": "2226470"
  },
  {
    "text": "very similar, our average\nwas a pretty good estimate. It's so noisy that one\nevaluation, anyway, isn't necessarily that great of\nan estimate of your local value",
    "start": "2226470",
    "end": "2233265"
  },
  {
    "text": "function. And so yeah. We just did an average baseline. And that's sort of half\nthe running time, right?",
    "start": "2233265",
    "end": "2238529"
  },
  {
    "text": "And so it can be a big one. And so there's a lot of\ndetails when you implement it about the right way to\nsort of put this together,",
    "start": "2238530",
    "end": "2244540"
  },
  {
    "text": "and depending what your cost\nfunction is, and how good of an initial policy, what\nyour initial condition and your policy is. But yeah, there's a lot\nof factors like that.",
    "start": "2244540",
    "end": "2251490"
  },
  {
    "text": " All right. So now we can do some of--",
    "start": "2251490",
    "end": "2262530"
  },
  {
    "text": " sorry. ",
    "start": "2262530",
    "end": "2271180"
  },
  {
    "text": "I can do example of this. So I keep on talking about\nthis flapping system.",
    "start": "2271180",
    "end": "2276430"
  },
  {
    "text": "That's what I worked on\nfor my master's thesis.",
    "start": "2276430",
    "end": "2282460"
  },
  {
    "text": "And so that's sort of what\nmy brain always goes back to, particularly since we\nused all these methods. But all right.",
    "start": "2282460",
    "end": "2287980"
  },
  {
    "text": "So now I wonder if I\ncan do Russ' thing where he makes the font really big.",
    "start": "2287980",
    "end": "2294822"
  },
  {
    "text": "That's also-- the\nthing I'm about to run, it's this relatively simple\nlumped parameter simulation",
    "start": "2294822",
    "end": "2301240"
  },
  {
    "text": "of the flapping system. This is a lumped parameter\nmodel of-- let me show you,",
    "start": "2301240",
    "end": "2306319"
  },
  {
    "text": "it's pretty cool-- of this system which I guy\nin NYU named Jun [? Zhang ?]",
    "start": "2306320",
    "end": "2312580"
  },
  {
    "text": "built this robot that\neffectively models flapping flight.",
    "start": "2312580",
    "end": "2317650"
  },
  {
    "text": "It's a very simple model. I'll show it to you in a second. But it has a lot of\nthe same dynamics and a lot of the same\nissues as sort of a bird.",
    "start": "2317650",
    "end": "2326109"
  },
  {
    "text": "So the system, it's a\nsort of a rigid plate.",
    "start": "2326110",
    "end": "2331840"
  },
  {
    "text": "Well, the one you see here, we\nattached a rubber tail to it.",
    "start": "2331840",
    "end": "2337250"
  },
  {
    "text": "But the one-- most\nof these results are on actually a rigid plate,\nwhere it heaves up and down,",
    "start": "2337250",
    "end": "2345450"
  },
  {
    "text": "and what we can do is control\nthe motion it follows.",
    "start": "2345450",
    "end": "2351470"
  },
  {
    "text": "I hope that the\ncamera can see it. ",
    "start": "2351470",
    "end": "2360816"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nmoonlight [INAUDIBLE].. JOHN W. ROBERTS: Mood? AUDIENCE: Moonlight. JOHN W. ROBERTS: Oh, moonlight. I was like, mood lighting?",
    "start": "2360816",
    "end": "2366840"
  },
  {
    "text": "OK. Make my lecture more enjoyable. All right.",
    "start": "2366840",
    "end": "2372360"
  },
  {
    "text": "So this is the system. You can see we drive\nit up and down. That big cylindrical disk\nright there is the load cell.",
    "start": "2372360",
    "end": "2381120"
  },
  {
    "text": "So that measures the\nforce we're applying. And then what we do is we\ncontrol this vertical motion. How we control it is--\nthat's an important thing.",
    "start": "2381120",
    "end": "2387223"
  },
  {
    "text": "I talked about how the cost\nfunction matters a lot. Well, another thing\nthat matters a lot is the parameterization\nof your policy.",
    "start": "2387223",
    "end": "2393930"
  },
  {
    "text": "Now, in the last few problems\nwe had open-loop policies, which are pretty simple. You have like 251 parameters\nor something like that, right?",
    "start": "2393930",
    "end": "2401580"
  },
  {
    "text": "Now, when you're doing\ngradient descent using back prop or SNOPT, you have\nthe exact gradient.",
    "start": "2401580",
    "end": "2406682"
  },
  {
    "text": "It's cheap to compute\nthe exact gradient, so you can sort of follow\nthis pretty nicely. But When you do stochastic\ngradient descent,",
    "start": "2406682",
    "end": "2413130"
  },
  {
    "text": "the probability of\nbeing perpendicular sort of to your gradient, or nearly\nperpendicular to the gradient, increases the number\nof parameters goes up.",
    "start": "2413130",
    "end": "2420180"
  },
  {
    "text": "So you can think, if you're on-- if you're doing a\n1D thing, you're always going to move\npretty much-- it doesn't",
    "start": "2420180",
    "end": "2425490"
  },
  {
    "text": "matter if you move in\nthe right direction or the wrong direction. That's one of the benefits\nof this instead of that hill climbing. But you're always trying to get\nmoving in the right direction",
    "start": "2425490",
    "end": "2431370"
  },
  {
    "text": "to get this measurement. Does that make sense? If you think in 2D,\nyou have the circle.",
    "start": "2431370",
    "end": "2436470"
  },
  {
    "text": "You're going to\nbe moving around. You're going to be along-- close to the direction of\nyour gradient pretty often.",
    "start": "2436470",
    "end": "2442110"
  },
  {
    "text": "A sphere, it's a lot easier\nto be pretty far away. I mean, sort of a lot\nmore of the samples you do",
    "start": "2442110",
    "end": "2448770"
  },
  {
    "text": "are going to be\nrelatively perpendicular to your true gradient. And as your dimensionality\ngets very high,",
    "start": "2448770",
    "end": "2455070"
  },
  {
    "text": "a lot of your samples are\nrelatively perpendicular. And the thing is\nthat whether you go in the right direction or\nwrong direction doesn't matter.",
    "start": "2455070",
    "end": "2460180"
  },
  {
    "text": "You'll get the same\ninformation either way. Going perpendicular\nto the gradient gives you no information. Because you'll get no change,\nand there's no update.",
    "start": "2460180",
    "end": "2467220"
  },
  {
    "text": "So it's still-- the\n[? cross ?] dimensionality is alive and well.",
    "start": "2467220",
    "end": "2472410"
  },
  {
    "text": "And very\nhigh-dimensional policies can be slower to learn. And so those 251\ndimensional policies you use",
    "start": "2472410",
    "end": "2478380"
  },
  {
    "text": "may not be the best\nrepresentation, because they sort of-- I mean, you probably don't\nneed that many parameters",
    "start": "2478380",
    "end": "2485580"
  },
  {
    "text": "to represent what\nyou want to do. So for this, what we had-- and this made a\nbig difference, we",
    "start": "2485580",
    "end": "2490710"
  },
  {
    "text": "tried different things, this\none worked really nicely-- was a spline. So we said, all right,\nif you have time,",
    "start": "2490710",
    "end": "2499050"
  },
  {
    "text": "I'm going to set\nthe final time here. Now that's a parameter, too. Then this is the z height.",
    "start": "2499050",
    "end": "2505620"
  },
  {
    "text": "It's in millimeters\nor whatever you want. And I'm going to\nsay, OK, I'm going to force it to be at the\nbeginning, in the middle,",
    "start": "2505620",
    "end": "2511920"
  },
  {
    "text": "and at the end-- wow, that's\nnowhere near the middle, is it? ",
    "start": "2511920",
    "end": "2518089"
  },
  {
    "text": "I shouldn't be a\ncarpenter in the 1200s. So what do we do then?",
    "start": "2518090",
    "end": "2523500"
  },
  {
    "text": "We then have five\nparameters-- now, we've done several versions, but\nsimple one right here-- five parameters that define a spline.",
    "start": "2523500",
    "end": "2529200"
  },
  {
    "text": "So this is going to be smooth. You can enforce to be a\nperiodic spline, which means that the knot at the\nend, the connection here, is",
    "start": "2529200",
    "end": "2536700"
  },
  {
    "text": "continuously\ndifferentiable as well. And then we force that this\nparameter-- so this number p1,",
    "start": "2536700",
    "end": "2542040"
  },
  {
    "text": "this one is going to\nbe the opposite of it.",
    "start": "2542040",
    "end": "2547180"
  },
  {
    "text": "So it's a negative p1. And that's true for all these. So this way, we have this\nrelatively rich policy class",
    "start": "2547180",
    "end": "2555930"
  },
  {
    "text": "that has sort of the\nright kind of properties. But we do it with\nonly five parameters. So you can imagine,\nif we want it",
    "start": "2555930",
    "end": "2561330"
  },
  {
    "text": "to be asymmetric top\nand bottom, that would double our parameters. And we probably wouldn't\nwant to tie this guy to 0, so we'd even add one more.",
    "start": "2561330",
    "end": "2567450"
  },
  {
    "text": "And when we have\nthe amplitude, you can either fix it\nor make it free. I can add another parameter.",
    "start": "2567450",
    "end": "2572640"
  },
  {
    "text": "So you can see that as\nyou add this richness, you're going to add all\nthese different parameters. But getting-- using a\nspline rather than--",
    "start": "2572640",
    "end": "2577718"
  },
  {
    "text": "this is the height right now,\nthis is the height right then-- it's a huge advantage. Because what's the\nchance that you're going to want it to move very\nviolently on a sort of like 1",
    "start": "2577718",
    "end": "2584698"
  },
  {
    "text": "dt time scale? And if you try to do\nthat, you could actually damage your system. Some of the policies\nthat I-- when I was working on this\nparameterization,",
    "start": "2584698",
    "end": "2591030"
  },
  {
    "text": "I had the load cell break off\nand fall into the tank once. Luckily it broke off\nthe wires and lost",
    "start": "2591030",
    "end": "2596250"
  },
  {
    "text": "its electric connection before\nit fell in there, but yeah. So if you come up with\na parameterization that",
    "start": "2596250",
    "end": "2604890"
  },
  {
    "text": "appropriately captures the kind\nof behaviors you expect to see,",
    "start": "2604890",
    "end": "2610890"
  },
  {
    "text": "it can be a lot faster to learn. Now, sort of the warning,\nthen, is that you're only",
    "start": "2610890",
    "end": "2616494"
  },
  {
    "text": "going to be optimal-- the only thing, you're going get\nto a local minimum in this sort of parameterization space. So if you parameterize-- if\nI were to parameterize this",
    "start": "2616495",
    "end": "2623730"
  },
  {
    "text": "by saying, OK, well I'm only\ngoing to let it be some-- let's say I was going to do like\na Fourier series kind of thing",
    "start": "2623730",
    "end": "2630885"
  },
  {
    "text": "and say, OK, it's add\nthis, this, and this--",
    "start": "2630885",
    "end": "2636750"
  },
  {
    "text": "now, that's not very rich. It's only three parameters. That's good. But I'm going to do all sorts\nof things that are probably extremely sub-optimal.",
    "start": "2636750",
    "end": "2642407"
  },
  {
    "text": "Now, it's still going to find\nthe best kind of behavior, or the locally best\nkind of behavior can using this kind of policy.",
    "start": "2642407",
    "end": "2649920"
  },
  {
    "text": "But it could be quite bad. So the actual optimum\ncould be very different. So your policy class, you'd\nlike it to include the optimum.",
    "start": "2649920",
    "end": "2659640"
  },
  {
    "text": "And so that sort\nof is-- it depends on what the question is. You sort of have to just have a\nfeel for what is a good policy class.",
    "start": "2659640",
    "end": "2664710"
  },
  {
    "text": "How do I get [? my ?]\ndimension as low as possible, while still having the richness\nto represent a wide variety of viable policies?",
    "start": "2664710",
    "end": "2672440"
  },
  {
    "text": "So when you're trying to\nimplement these things, that can make a big difference.",
    "start": "2672440",
    "end": "2678910"
  },
  {
    "text": "So yeah. So we set up that. And we could control\nthe shape of that curve.",
    "start": "2678910",
    "end": "2684920"
  },
  {
    "text": "And so that is the policy\nparameterization we chose. So going back to this code here.",
    "start": "2684920",
    "end": "2691960"
  },
  {
    "text": "Now, I think I can\njust run this here.",
    "start": "2691960",
    "end": "2698859"
  },
  {
    "text": "This is going to be doing\nthat bit we talked about on-- again, a simple lumped parameter\nmodel of that flapping system.",
    "start": "2698860",
    "end": "2706650"
  },
  {
    "text": "So here's our curve. It's this, you see\nthis-- well, this",
    "start": "2706650",
    "end": "2713485"
  },
  {
    "text": "is the forward motion of\nthe thing as it's flapping. This is the vertical motion.",
    "start": "2713485",
    "end": "2718820"
  },
  {
    "text": "So this is sort of the\nwaveform it's following. This is where it\nis in x position. You can see it sort of\ngoes fast, bounces around--",
    "start": "2718820",
    "end": "2724120"
  },
  {
    "text": "sorry, this is the\nspeed, not the position. So you can see it\naccelerates from 0, and then as it's pumping,\nit sort of oscillates a bit.",
    "start": "2724120",
    "end": "2729730"
  },
  {
    "text": "In practice, there's more\ninertia and everything, so you don't see these\nhigh-frequency oscillations. But this is just a relatively\nsimple, explicit model.",
    "start": "2729730",
    "end": "2736450"
  },
  {
    "text": "This is the shape we follow. So we're following that curve. And we have a little\nbit of noise to it.",
    "start": "2736450",
    "end": "2742039"
  },
  {
    "text": "And let me-- so now we're going\nto perturb it, measure again.",
    "start": "2742040",
    "end": "2747065"
  },
  {
    "text": "Try to measure again,\nand boom, here we are. We got a little bit better.",
    "start": "2747065",
    "end": "2752090"
  },
  {
    "text": "This is our reward, and\nthen we did another sample, and that's our reward. Let's do it again, better.",
    "start": "2752090",
    "end": "2757119"
  },
  {
    "text": " You see we improve quite nicely.",
    "start": "2757120",
    "end": "2764560"
  },
  {
    "text": "And also, notice,\nrelatively monotonically. Now, you might be\nsurprised by that.",
    "start": "2764560",
    "end": "2769840"
  },
  {
    "text": "Because even though\nwe're moving-- we have this sort\nof guarantee we'll move within 90 degrees\nof the gradient.",
    "start": "2769840",
    "end": "2774910"
  },
  {
    "text": "That's what I was talking\nabout sort of with, you'll always be within 90\ndegrees if it's deterministic.",
    "start": "2774910",
    "end": "2780036"
  },
  {
    "text": "And this is deterministic. But it also sort of is this\nlinear kind of interpretation, right? So as you run it, you'd imagine\nthat you could perturb yourself",
    "start": "2780037",
    "end": "2789130"
  },
  {
    "text": "far enough that you got worse. Now, the reason\nthat's not happening is because I'm perturbing\nmyself very small amounts,",
    "start": "2789130",
    "end": "2795309"
  },
  {
    "text": "and I'm updating\nvery small amounts. So all this sort of linear\nanalysis is appropriate. And actually, you can see\nwhat I talked about that,",
    "start": "2795310",
    "end": "2800380"
  },
  {
    "text": "that you always get pretty close\nto the true gradient is there. Sometimes it moves up a\nlot, sometimes it's steep, sometimes it moves up shallowly,\nbut it does a pretty good job.",
    "start": "2800380",
    "end": "2808090"
  },
  {
    "text": "Now, we can change that and\ntry to sabotage our little code here. Or sometimes you're\nOK, actually.",
    "start": "2808090",
    "end": "2814180"
  },
  {
    "text": "That's the thing, is that\nin practice lots of times it's OK if it gets\nworse sometimes, because allowing\nit to get worse, being violent\nenough to get worse,",
    "start": "2814180",
    "end": "2819917"
  },
  {
    "text": "it'll reach the\noptimum a lot faster. So here, this is\nour eta parameter. Let's make it bigger a factor\nof-- let's make it 20.5.",
    "start": "2819917",
    "end": "2830470"
  },
  {
    "text": "I don't want to risk-- [INAUDIBLE] not get worse. AUDIENCE: Is that the noise or-- JOHN W. ROBERTS: Pardon? No, that is the update. So the noise is the same.",
    "start": "2830470",
    "end": "2836320"
  },
  {
    "text": "This noise is still local. But now we're\njumping really far. And so you can imagine,\nwe're measuring the gradient. We're moving really far.",
    "start": "2836320",
    "end": "2841848"
  },
  {
    "text": "And now where we've\nmoved to, that gradient may be a poor\nmeasurement of sort of the update over\nthat long of a scale.",
    "start": "2841848",
    "end": "2848090"
  },
  {
    "text": "So let's do this again. This is always fun. Oh, there you go, already.",
    "start": "2848090",
    "end": "2855650"
  },
  {
    "text": "That's better.  See now-- but you see,\nthat's a huge increase then.",
    "start": "2855650",
    "end": "2861110"
  },
  {
    "text": "That's what I'm talking\nabout, is that there's sort of a sweet spot. And you don't necessarily\nwant monotonic increasing. Like, there's limitations\non how violent",
    "start": "2861110",
    "end": "2867637"
  },
  {
    "text": "you want it to be in\npractice, because on a robot, a very violent policy\ncould break your load cell of and have it\nalmost cost you $400.",
    "start": "2867637",
    "end": "2875137"
  },
  {
    "text": "So you don't do something crazy. But there's also the willingness\nthat-- oh, that's ugly. But you see, I mean, if\nyou bounce pretty far,",
    "start": "2875137",
    "end": "2882500"
  },
  {
    "text": "you can also get\nhuge improvements. And so there's sort of this-- monotonicity in your\nincreasing reward",
    "start": "2882500",
    "end": "2889910"
  },
  {
    "text": "is not necessarily the best\nway to learn, I suppose. That's from the trenches.",
    "start": "2889910",
    "end": "2895400"
  },
  {
    "text": "I learned that the hard way\nthrough many, many hours sitting in front of a machine. So then the other thing\nthat we can do is this eta.",
    "start": "2895400",
    "end": "2904115"
  },
  {
    "text": "Let's decrease eta. And now let's make\nour sigma really big. Now, this is going to be\nreally crazy stuff probably.",
    "start": "2904115",
    "end": "2910700"
  },
  {
    "text": "But you see, now we're\ngoing to measure so far. And we're going to\nget this sort of-- we're going to try to\nmeasure the gradient, but it's going to be just\nway off, because it's",
    "start": "2910700",
    "end": "2916903"
  },
  {
    "text": "moving so far that the local\nstructure is completely ignored. ",
    "start": "2916903",
    "end": "2922910"
  },
  {
    "text": "Yeah. I probably don't have to be\nnearly as dramatic as this to make my point. But, you know, it's just\ncompletely falling apart.",
    "start": "2922910",
    "end": "2930700"
  },
  {
    "text": "Yeah. That's doing as badly\nas it can, I guess. I think it's like almost\n[? no net ?] motion, so.",
    "start": "2930700",
    "end": "2936040"
  },
  {
    "text": "Yeah. So the sweet spot,\nthen, is somewhere in between, where maybe you\nwant an eta of, say, 3,",
    "start": "2936040",
    "end": "2942805"
  },
  {
    "text": "and sigma, I don't know, 0.1.",
    "start": "2942805",
    "end": "2955006"
  },
  {
    "text": "Oh, that's probably\nstill too violent.  Yeah, definitely.",
    "start": "2955007",
    "end": "2961940"
  },
  {
    "text": "But I think that is-- that's the sort of\ngame you have to play. And how big all these things are\ndepend on a number of factors",
    "start": "2961940",
    "end": "2969950"
  },
  {
    "text": "specific to your system. Like, if your system--",
    "start": "2969950",
    "end": "2975390"
  },
  {
    "text": "if the change is very\nsmall in magnitude, if your cost function\nis such that it's changed between 10 to\nthe negative fifth and 10",
    "start": "2975390",
    "end": "2980450"
  },
  {
    "text": "to the negative\nfifth plus 1 times 10 to the negative sixth, that's\nchanging by very small amounts,",
    "start": "2980450",
    "end": "2986090"
  },
  {
    "text": "right? You could need a very large eta\njust to make up for the fact that your change is so small. So a big eta-- like, there's\nno absolute perception",
    "start": "2986090",
    "end": "2993500"
  },
  {
    "text": "on what is a big eta. It's not like 10,000\nis a huge eta. 10,000 could be very\nsmall eta, depending on what your rewards are.",
    "start": "2993500",
    "end": "2999453"
  },
  {
    "text": "Same thing with sigma. It depends on how big\nyour parameters are. Because, I mean,\nmy parameters here",
    "start": "2999453",
    "end": "3005680"
  },
  {
    "text": "are of order one, which\nis sort of convenient. Yeah. So there. ",
    "start": "3005680",
    "end": "3017130"
  },
  {
    "text": "Yeah. So here we're learning\npretty quickly. And so all those sort\nof things, that's sort of a disadvantage\nof this technique,",
    "start": "3017130",
    "end": "3023240"
  },
  {
    "text": "is that there's a lot of tuning\nto sort solve these things, is that-- where SNOPT you\ndon't have to set--",
    "start": "3023240",
    "end": "3029300"
  },
  {
    "text": "SNOPT you don't have\nto set a learning rate, here you have to\nset a learning rate. You have to set your sigma.",
    "start": "3029300",
    "end": "3035660"
  },
  {
    "text": "And when you have really\nsort of hard problems, there's even more\nthings you have to do. Like, your policy\nparameterization could affect a lot of things.",
    "start": "3035660",
    "end": "3041250"
  },
  {
    "text": "There's a lot of issues. But sometimes, that's--\nsometimes it's the only sort of route you have. Like, the best this can\never do is gradient descent.",
    "start": "3041250",
    "end": "3048050"
  },
  {
    "text": "It's never going to do\nbetter than gradient descent. And so there's of a lot of\nfancy packages out there. When you have better\nmodels and stuff like that,",
    "start": "3048050",
    "end": "3054250"
  },
  {
    "text": "you can do better\nthan gradient descent. But while even\nthough you're only going to be able to\nachieve gradient descent, you can achieve it\ndespite the fact",
    "start": "3054250",
    "end": "3060290"
  },
  {
    "text": "that you know nothing\nabout your system, your system is stochastic,\nand it's noisy, like that. And so in those cases,\nit can be a big win.",
    "start": "3060290",
    "end": "3069119"
  },
  {
    "text": "AUDIENCE: So when you were\ndoing this in real life, instead of in space\neach time, you were sitting for 4 minutes\nin front of a flapping--",
    "start": "3069120",
    "end": "3075963"
  },
  {
    "text": "JOHN W. ROBERTS: I automated\npretty much everything, yeah. So I was-- but yeah. I mean, this is a\nlittle simulation. AUDIENCE: Every interval was\nlike actually it running and--",
    "start": "3075963",
    "end": "3082730"
  },
  {
    "text": "JOHN W. ROBERTS: Oh, yeah. When I pressed\nSpace, it actually does two-- because this\nis using a true baseline. I didn't put in the\naverage baseline.",
    "start": "3082730",
    "end": "3088410"
  },
  {
    "text": "So this is running it twice\nevery time I press Space. But yeah. You can imagine every\ntime I'm doing Space, it does this one update and\ngives me that new point.",
    "start": "3088410",
    "end": "3094107"
  },
  {
    "text": "What I was doing is I sat\nthere, and it would run, and I'd babysit to make\nsure it wasn't broken.",
    "start": "3094107",
    "end": "3100490"
  },
  {
    "text": "And it would throw\nup the curve as it was running so I could make sure\nthat the encoders weren't off, just sort of sitting\nthere keeping track of all these things.",
    "start": "3100490",
    "end": "3106160"
  },
  {
    "text": "I was like a nuclear\nsafety technician. I just eat some doughnuts and\ngo to Moe's, and I would have",
    "start": "3106160",
    "end": "3111656"
  },
  {
    "text": "been a good sitcom character. But yeah. So I mean, pretty much\njust babysitting it. But yeah, every time you\ndid it, every time you",
    "start": "3111656",
    "end": "3117319"
  },
  {
    "text": "got a new update-- like,\nevery one of these points cost me 6 minutes or\nsomething, because it was like a 3-minute\nrun for-- basically a 3-minute run for an update.",
    "start": "3117320",
    "end": "3123319"
  },
  {
    "text": "Because I wasn't using\naveraged baseline then either. I was trying to be more violent. But yeah. And so that's the\nthing, is that that",
    "start": "3123320",
    "end": "3130067"
  },
  {
    "text": "is the perfect\nencapsulation of why you want to use this information\nas carefully as possible. It's because it's very\nexpensive to get a point.",
    "start": "3130067",
    "end": "3135890"
  },
  {
    "text": "Like, here it cost nothing. If I were to turn\noff the pause, like, this thing would\nclimb up like that. ",
    "start": "3135890",
    "end": "3143108"
  },
  {
    "text": "If you're running\non a robot, like we want to use this on the\nglider, every time you watch that glider, you have\nto set up the glider,",
    "start": "3143108",
    "end": "3148242"
  },
  {
    "text": "fire it off, take all this\ndata, and reset it by hand, and launch it again. So getting a data point there\nis going be extremely expensive.",
    "start": "3148242",
    "end": "3154130"
  },
  {
    "text": "And so we've actually done\nsome work on the right ways to sample. You can imagine trying to\ncome up with the right ways",
    "start": "3154130",
    "end": "3159260"
  },
  {
    "text": "to have a policy. But sampling intelligently\ncan save you a lot of time. We sort of look at the\nsignal-to-noise ratio",
    "start": "3159260",
    "end": "3164750"
  },
  {
    "text": "of these updates. I don't know if anyone-- some people here probably at\nleast heard about that stuff",
    "start": "3164750",
    "end": "3170450"
  },
  {
    "text": "since they're in my group. But probably talk about\nthat maybe tomorrow.",
    "start": "3170450",
    "end": "3176432"
  },
  {
    "text": "But there's these\nthings you can do that can improve the quality\nof your performance a lot. And actually, I test\non this exact system.",
    "start": "3176432",
    "end": "3183080"
  },
  {
    "text": "I got put on the\nsystem, and I ran it with the sort of results\nwe had that's just, this is a better way to\nsample, and then just",
    "start": "3183080",
    "end": "3189740"
  },
  {
    "text": "with a naive Gaussian kind of\nsampling, and you learn faster. And in the context of me sitting\nthere and spending my days",
    "start": "3189740",
    "end": "3197150"
  },
  {
    "text": "in New York City huddled in\nfront of a computer, that's a big win. So anyway. AUDIENCE: So when you\nsay change the sampling,",
    "start": "3197150",
    "end": "3203647"
  },
  {
    "text": "you can just change the\nvariance like you would do to a non-Gaussian distribution? JOHN W. ROBERTS: Right, yeah. So that-- yeah.",
    "start": "3203647",
    "end": "3209000"
  },
  {
    "text": "In fact, we used a very\ndifferent kind of description overall. You can still-- the linear\nanalysis will still work.",
    "start": "3209000",
    "end": "3215990"
  },
  {
    "text": "But it's just a local-- but yeah, there's work\nwhere they change--",
    "start": "3215990",
    "end": "3221083"
  },
  {
    "text": "We also have something where\nyou change the variance [? to ?] the Gaussian, but\nyour different directions",
    "start": "3221083",
    "end": "3226730"
  },
  {
    "text": "have different variances. And so if you sort of need\nan estimate of the gradient, then-- but you just estimate the\ngradient to bias your sampling",
    "start": "3226730",
    "end": "3233873"
  },
  {
    "text": "more in the directions where\nyou think the gradient is, so that more of your sampling is\nalong the directions you think",
    "start": "3233873",
    "end": "3239120"
  },
  {
    "text": "are most interesting. And so that can be a win when\nyou have a lot of parameters",
    "start": "3239120",
    "end": "3244760"
  },
  {
    "text": "that aren't well correlated. Like if you imagine\nif you had a feedback policy that was dependent on-- a parameter is active\nin a certain state--",
    "start": "3244760",
    "end": "3250910"
  },
  {
    "text": "like, if I was at negative\n2 to negative 5, I do this, and let's say I\nnever get there, then",
    "start": "3250910",
    "end": "3256370"
  },
  {
    "text": "that parameter has nothing to\ndo with how well I perform. And so if you know\nthat, you can sort of-- there's something called\nan eligibility you can track.",
    "start": "3256370",
    "end": "3263210"
  },
  {
    "text": "And you cannot update\nthat parameter. There's no reason to\nsort of be fooling around with that parameter when it's\nnot affecting your output. And if you know that, you\ncan do things like that.",
    "start": "3263210",
    "end": "3269773"
  },
  {
    "text": "And we sort of have a way,\na more careful way of, shaping all these-- of shaping this Gaussian\nto learn faster.",
    "start": "3269773",
    "end": "3276530"
  },
  {
    "text": "And it can. And also, just completely very\ndifferent kind of sampling. Like, it's-- well, maybe\nI'll try to talk about it.",
    "start": "3276530",
    "end": "3283133"
  },
  {
    "text": "Because I think it's\npretty interesting stuff. The math is a little\nbit nasty, but I'll skip the really ugly steps.",
    "start": "3283133",
    "end": "3289760"
  },
  {
    "text": "And actually, the one with\nthe different distribution isn't even that nasty. But yeah. I mean, we ran it here and\nit [INAUDIBLE] improvement.",
    "start": "3289760",
    "end": "3297205"
  },
  {
    "text": "Yeah, so. Did I answer your question? Yeah. It's not just changing\nthe variances. It's more complicated than that.",
    "start": "3297205",
    "end": "3303050"
  },
  {
    "text": "Although changing the\nvariances can be a big win. For example, if you knew\nyou had this anisotropy, and if you were to have\ndifferent etas in different--",
    "start": "3303050",
    "end": "3310010"
  },
  {
    "text": "if you were to scale\neverything in your sigma, you could effectively make\nit squashed in, right? I mean, just a rescaling\nof this anisotropic bowl",
    "start": "3310010",
    "end": "3317790"
  },
  {
    "text": "will make it right. So if you can evaluate\nthat, you can fix it. But you sort have to know\nthat that's going on.",
    "start": "3317790",
    "end": "3323713"
  },
  {
    "text": "That's about the times\nyou have adaptive learning rates and stuff. Gradient descent, like if you\nkeep moving the same direction, you have a bigger learning rate.",
    "start": "3323713",
    "end": "3330000"
  },
  {
    "text": "You can have different\nlearning rates, you have different parameters. This one, as you get\nclose to a local min, you'll decrease your\nlearning rate and your noise,",
    "start": "3330000",
    "end": "3335100"
  },
  {
    "text": "because you want to\nsort of bounce around. You don't want to be\njumping all across this min. So-- AUDIENCE: [INAUDIBLE] talked\nabout a basically policy",
    "start": "3335100",
    "end": "3342626"
  },
  {
    "text": "gradient when we\nwere [INAUDIBLE].. JOHN W. ROBERTS: Yeah, no. Yeah. I mean, there is--",
    "start": "3342626",
    "end": "3347700"
  },
  {
    "text": "it's definitely exactly that. It's just stochastic gradient. But yeah, it's all\npolicy gradient ideas.",
    "start": "3347700",
    "end": "3352710"
  },
  {
    "text": "Because we don't-- I mean, these\nthings don't have a critic, right? But you can combine this\nwith some policy evaluation",
    "start": "3352710",
    "end": "3359280"
  },
  {
    "text": "techniques. And you can turn them into\nactor-critic algorithms. A very simple-- do people know\nabout actor-critic algorithms?",
    "start": "3359280",
    "end": "3364830"
  },
  {
    "text": "That's going to be\na subject I think Russ talks about at the end. But the thing is that\nright now-- well, I'll motivate in a completely\ndifferent way.",
    "start": "3364830",
    "end": "3371007"
  },
  {
    "text": "We talked about how\nthis baseline can affect your performance a lot, right? Now, a good baseline can\nmake you do a lot better.",
    "start": "3371007",
    "end": "3377670"
  },
  {
    "text": "Now, the thing is\nthat, what happens if-- here we start with the same\ninitial condition every time.",
    "start": "3377670",
    "end": "3383260"
  },
  {
    "text": "But let's say that I\nactually could be in one of two initial conditions. I can measure this,\nand then I run it.",
    "start": "3383260",
    "end": "3389160"
  },
  {
    "text": "And the system behaves\nvery differently, or the costs are very\ndifferent depending on my initial condition.",
    "start": "3389160",
    "end": "3394530"
  },
  {
    "text": "But I want sort of the same\npolicy to cover both of these. So the thing is, if I just did\nthis and I had one baseline",
    "start": "3394530",
    "end": "3399570"
  },
  {
    "text": "for both of them, and I could\nrandomly be putting these in [? different initial ?]\nconditions or whatever-- or I mean, I could--",
    "start": "3399570",
    "end": "3405503"
  },
  {
    "text": "there's probably a more\nsensible way of saying this, but I don't want to\nconfuse the issue. So if you could have\ndifferent initial conditions,",
    "start": "3405503",
    "end": "3412758"
  },
  {
    "text": "you can make your\nbaseline a function of your initial condition. Does that makes sense? Instead of just having B,\ninstead of evaluating it twice,",
    "start": "3412758",
    "end": "3419410"
  },
  {
    "text": "I could have my B of x. And if my x is here,\nI'm going to say, OK, my cost should be like this.",
    "start": "3419410",
    "end": "3425490"
  },
  {
    "text": "And if my x is here,\nthen it's like, oh, my cost should be like this. And when I evaluate my cost,\nwhen I perturb my policy,",
    "start": "3425490",
    "end": "3430890"
  },
  {
    "text": "I have a better idea\nof how well I'm doing. Does that makes sense? It probably doesn't, so.",
    "start": "3430890",
    "end": "3437640"
  },
  {
    "text": "All right. So let's say-- now,\nthis is phase space now.",
    "start": "3437640",
    "end": "3445500"
  },
  {
    "text": " Now let's say that I can\nstart in either of these.",
    "start": "3445500",
    "end": "3453810"
  },
  {
    "text": "And let's say that\nI'm trying to get to-- let's draw this here.",
    "start": "3453810",
    "end": "3458822"
  },
  {
    "text": "I'm trying to get to 0. That's my goal. And I can measure this. But then one of them, I'm\ngoing to go [WHOOSH] like that.",
    "start": "3458822",
    "end": "3466270"
  },
  {
    "text": "And the other one I'm going\nto have to go, I don't know, through whatever\ntorque, [? limited ?] reasons like that or something.",
    "start": "3466270",
    "end": "3472860"
  },
  {
    "text": "So this one always costs more\nthan this one, all right? It doesn't matter how\ngood my policy is. Like, you can imagine just\nhave a feedback policy.",
    "start": "3472860",
    "end": "3479713"
  },
  {
    "text": "It doesn't matter how bad\nit is, how good it is. I mean, the same policy is\nalways going to do worse here. Now, if you believe that a good\nbaseline improves performance--",
    "start": "3479713",
    "end": "3487060"
  },
  {
    "text": "and trust me, it does-- then I don't want\nthe same baseline. I don't want the same B for\nboth of these situations.",
    "start": "3487060",
    "end": "3492120"
  },
  {
    "text": "Because this guy should\nalways be around 50, and this guy should always\nbe around 20, right? So what I could\ndo is I could have",
    "start": "3492120",
    "end": "3497850"
  },
  {
    "text": "my baseline be a function of x. And I'm going to be like,\nOK, here my baseline is 50, here my baseline is 20.",
    "start": "3497850",
    "end": "3503932"
  },
  {
    "text": "And let's say I don't\nknow that from the start. I can learn my baseline\nwhile I'm learning my policy.",
    "start": "3503932",
    "end": "3510690"
  },
  {
    "text": "So I can use the same\npolicy for both situations. And then over here\nI measure my state, and I'm like, oh, over here\nI'm doing bad all the time.",
    "start": "3510690",
    "end": "3516950"
  },
  {
    "text": "So my baseline is\ngoing to be high. And over here I'm\nalways doing well, so my baseline is\ngoing to be low. And so in that way you can\ntake that into account.",
    "start": "3516950",
    "end": "3528400"
  },
  {
    "text": "Does that makes sense? it does look like-- AUDIENCE: [INAUDIBLE] this\nis basically Monte-Carlo",
    "start": "3528400",
    "end": "3534200"
  },
  {
    "text": "sampling and learning. Because each time\nthat you set your--",
    "start": "3534200",
    "end": "3539910"
  },
  {
    "text": "so your policy is defined\nby a set of alphas. And then you fix it,\nyou run it, and you get a sample that says what\nis the value associated",
    "start": "3539910",
    "end": "3547200"
  },
  {
    "text": "with this starting point\ngiven this [INAUDIBLE] policy. JOHN W. ROBERTS: Are you\ntalking about Monte-Carlo for policy evaluation?",
    "start": "3547200",
    "end": "3552809"
  },
  {
    "text": "Because Monte-Carlo [INAUDIBLE]. That's like TD infinity\nor whatever it is. And that's for\npolicy evaluation. That's how you make a critic.",
    "start": "3552810",
    "end": "3560099"
  },
  {
    "text": "The policy is different, right? The policy, you're\ndoing this update, then you're advancing it a bit. Your critic, the\nway I just described",
    "start": "3560100",
    "end": "3566082"
  },
  {
    "text": "making the baseline\nfor this, that would be a Monte-Carlo interpretation. You could do it with t, lambda,\nor anything you wanted to.",
    "start": "3566082",
    "end": "3572099"
  },
  {
    "text": "But yeah. So the important thing is--",
    "start": "3572100",
    "end": "3577350"
  },
  {
    "text": "I mean, it looks like\nthe sort of blank faces after I talked about that. But Russ, I think, is going\nto go into more detail",
    "start": "3577350",
    "end": "3585480"
  },
  {
    "text": "into actor-critic. But maybe I can talk about\nthat more tomorrow if you want.",
    "start": "3585480",
    "end": "3592720"
  },
  {
    "text": "Yeah. I mean, the important thing\nis that right now this is a very simple kind of\nidea we've talked about, where you run the alpha, and\nthen if you ran the same alpha,",
    "start": "3592720",
    "end": "3599640"
  },
  {
    "text": "it would always do the same. Or maybe it just has a\nlittle bit of additive noise. But If actually running the same\nalpha from different states--",
    "start": "3599640",
    "end": "3607140"
  },
  {
    "text": "which happens a lot\nin a lot of systems-- the different states could have\ndifferent expected performance.",
    "start": "3607140",
    "end": "3612492"
  },
  {
    "text": "And so while you'll still\nlearn without the baseline, having a good\nbaseline everywhere will make you learn faster. And so it's worth\nlearning a baseline",
    "start": "3612492",
    "end": "3619410"
  },
  {
    "text": "and learning the\npolicy simultaneously. And sort of the thing we\ntalked about, where you just",
    "start": "3619410",
    "end": "3625770"
  },
  {
    "text": "average your last several\nsamples to get your baseline, that's already we're\nlearning a baseline, right? We're just learning it for\neverywhere in state space.",
    "start": "3625770",
    "end": "3632310"
  },
  {
    "text": "We're saying this is the\nsame everywhere, right? ",
    "start": "3632310",
    "end": "3638320"
  },
  {
    "text": "AUDIENCE: That idea\nof sampling, can you do something like [? smarter ?]\nusing Gaussian processes",
    "start": "3638320",
    "end": "3644230"
  },
  {
    "text": "to do active\nlearning on top of it to sample in areas that\nare more promising?",
    "start": "3644230",
    "end": "3649840"
  },
  {
    "text": "Instead of just randomly\nmoving somewhere? JOHN W. ROBERTS:\nI mean, there are ways of biasing your\nsampling based on what",
    "start": "3649840",
    "end": "3655870"
  },
  {
    "text": "you think the gradient is. I mean, that's one of\nthe things we worked on with signal-to-noise ratio.",
    "start": "3655870",
    "end": "3662410"
  },
  {
    "text": "I'm not sure exactly what-- AUDIENCE: I know some people\nworked on Aibos walking,",
    "start": "3662410",
    "end": "3668230"
  },
  {
    "text": "and they wanted to\nfind a gain which maximizes the speed of the\nAibos when they're walking. JOHN W. ROBERTS: I think\nI read that paper, yeah.",
    "start": "3668230",
    "end": "3674502"
  },
  {
    "text": "AUDIENCE: Yeah, and there\nare like 12 or 13 dimensions. And it seems like\na similar problem--",
    "start": "3674502",
    "end": "3680085"
  },
  {
    "text": "JOHN W. ROBERTS:\nNo, I think they use a very similar algorithm. I think they had a\ndifferent update, though. It was the same kind of idea.",
    "start": "3680085",
    "end": "3685869"
  },
  {
    "text": "I think that the update\nstructure was maybe different than that. Yeah. So I won't dwell\non critic stuff.",
    "start": "3685870",
    "end": "3691813"
  },
  {
    "text": "That's, I think, the\nlast lecture in the class or something like that. But yeah.",
    "start": "3691813",
    "end": "3697420"
  },
  {
    "text": " So here, I mean, this is\nsort of the sample system.",
    "start": "3697420",
    "end": "3702819"
  },
  {
    "text": "And you can see how this thing\nis robust to really noisy systems in practice. Because when I ran it on the\nflapping thing down at NYU,",
    "start": "3702820",
    "end": "3712180"
  },
  {
    "text": "the consecutive evaluations\ncould be very different--",
    "start": "3712180",
    "end": "3717280"
  },
  {
    "text": "not because of any\nchange in policy, You run the same policy,\nyou get a big variance. So that's just\nbecause you're running on this physical robot\nwith this fluid system",
    "start": "3717280",
    "end": "3723370"
  },
  {
    "text": "and you're measuring the\nforces in an analog sensor. And so it's just very noisy. But it's robust to that.",
    "start": "3723370",
    "end": "3728560"
  },
  {
    "text": "And that's what's so nice. ",
    "start": "3728560",
    "end": "3737349"
  },
  {
    "text": "Put that here.  So look at that.",
    "start": "3737350",
    "end": "3744079"
  },
  {
    "text": "I mean, this one--\nthese, luckily, didn't take 3 minutes anymore. They took 1 second. So it wasn't nearly as bad.",
    "start": "3744080",
    "end": "3751342"
  },
  {
    "text": "But, I mean, look how\nmuch it's changing. It's changing a significant\npercentage every time, right? AUDIENCE: These are all with\nthe same [? taping loop? ?]",
    "start": "3751342",
    "end": "3756560"
  },
  {
    "text": "JOHN W. ROBERTS: Yeah. Yeah-- I mean, no, this\nis playing a different-- this is learning. So the thing is that-- I mean, I showed you how\nit wasn't monotonic before.",
    "start": "3756560",
    "end": "3762890"
  },
  {
    "text": "But this, you can\nrun the same tape. I mean, up there it's pretty\nmuch running the same tape. So up there you get an idea of\nwhat the noise looks like when",
    "start": "3762890",
    "end": "3768619"
  },
  {
    "text": "you're running the same policy. Right. And so you can imagine-- yes. AUDIENCE: Just [INAUDIBLE]\nwent with blue and red.",
    "start": "3768620",
    "end": "3775295"
  },
  {
    "text": "JOHN W. ROBERTS:\nOh, blue and red are different ways of\nkeeping track of my baseline. All right.",
    "start": "3775295",
    "end": "3781668"
  },
  {
    "text": "So I mean, I don't worry about\nthe different blue and red. They're just sort\nof an internal test to see the right way to make\nthese things-- we determined",
    "start": "3781668",
    "end": "3787190"
  },
  {
    "text": "that it didn't\nmake a difference. But yeah. AUDIENCE: It looks like\nthe red is much smoother.",
    "start": "3787190",
    "end": "3792270"
  },
  {
    "text": "JOHN W. ROBERTS: I don't know. It may be plotting. I may have plotted blue on\ntop of red or something, too, you know? I don't know.",
    "start": "3792270",
    "end": "3797750"
  },
  {
    "text": "I remember we decided it didn't\nmake much of a difference. Yeah. I see what you're saying. It does look like the\nvariance is a bit less,",
    "start": "3797750",
    "end": "3804305"
  },
  {
    "text": "but I don't think it was. But these are trials\non the bottom. So that's, every second we\nsort of did another flap,",
    "start": "3804305",
    "end": "3810013"
  },
  {
    "text": "we did another update. So this is update\nfrom the bottom. And yeah. This is-- we actually have a\nreward instead of cost here. So it's going to go\nup instead of down.",
    "start": "3810013",
    "end": "3816085"
  },
  {
    "text": "But yeah. So despite the fact\nthis is really noisy, despite the fact that we had\nthis average baseline, which",
    "start": "3816085",
    "end": "3821797"
  },
  {
    "text": "I was talking about--\nso our baseline wasn't perfect-- it still learned. It learned pretty quickly. I mean, 400 samples maybe\ndoesn't seem very good.",
    "start": "3821797",
    "end": "3828500"
  },
  {
    "text": "But that's also less\nthan 10 minutes. So that's like 7 minutes.",
    "start": "3828500",
    "end": "3833630"
  },
  {
    "text": "So it in practice can\nwork pretty darn well. And solving this thing\nwith other techniques",
    "start": "3833630",
    "end": "3839520"
  },
  {
    "text": "would be very tricky. Well, I mean, you could build\na model like this model we have and stuff like that, you can try\nto solve it with a simulation.",
    "start": "3839520",
    "end": "3845270"
  },
  {
    "text": "That's generally how they\nsolve a lot of these problems, is to do the\noptimization on a model. So there's this fly.",
    "start": "3845270",
    "end": "3853310"
  },
  {
    "text": " Jane Wang at Cornell tries\nto optimize the stroke form",
    "start": "3853310",
    "end": "3858799"
  },
  {
    "text": "for a fly, like a fruit fly. I think it's a fruit fly scale. And so she just built a\nsort of pretty fancy model",
    "start": "3858800",
    "end": "3865099"
  },
  {
    "text": "of this thing and\nthen simulates it. It does the optimization on a\ncomputational fluid dynamics simulation.",
    "start": "3865100",
    "end": "3871280"
  },
  {
    "text": "And so that's some way\nwe can-- and there you can get the gradients, you\ncan do all the sort of things we've already talked about. Because you have the model,\nyou can do all these things",
    "start": "3871280",
    "end": "3877910"
  },
  {
    "text": "explicitly. But the model takes\na long time to run. I think the optimization\ntook months of computer time.",
    "start": "3877910",
    "end": "3883589"
  },
  {
    "text": "So if you-- that's\nthe thing here, is that the full simulation\nof this system, where it took me 1 second\nto get an update,",
    "start": "3883590",
    "end": "3890900"
  },
  {
    "text": "it takes, I think,\nabout an hour per flap. So an hour on a\ncomputing cluster to get one full safety\nsimulation of one flap.",
    "start": "3890900",
    "end": "3898640"
  },
  {
    "text": "And that's even the simpler one. We're working on\nother ones, too, that have sort of\naeroelastic effects, which are where sort of the\nbody deforms in response",
    "start": "3898640",
    "end": "3905367"
  },
  {
    "text": "to the fluid forces. And simulating those\nis even harder. And so where it takes an\nhour to get an update,",
    "start": "3905367",
    "end": "3911660"
  },
  {
    "text": "I can get it in a second. And the thing is my update\nis going to be noisier and I don't get\nthe true gradient. But when you can get\n3,600 updates per update,",
    "start": "3911660",
    "end": "3919707"
  },
  {
    "text": "you're going to win. I mean, I'll get\none flap in the time takes me to optimize and sit\nthere for most of an hour,",
    "start": "3919707",
    "end": "3925250"
  },
  {
    "text": "you know? So you can see in\nthose kind of problems, it can be a big win, especially\nwhen a simulation is extremely",
    "start": "3925250",
    "end": "3931458"
  },
  {
    "text": "expensive, or computing\nthe gradient is extremely expensive, but you have the\nrobot right in front of you. You can just take that\ndata, accept the noise,",
    "start": "3931458",
    "end": "3938120"
  },
  {
    "text": "do model-free gradient descent. ",
    "start": "3938120",
    "end": "3943610"
  },
  {
    "text": "I think that's what I\nwanted to talk about. If you have any questions or\nanything didn't make sense",
    "start": "3943610",
    "end": "3949790"
  },
  {
    "text": "at all, please let me know. Otherwise, maybe I'll\nintroduce something that I'm trying to\ntalk about tomorrow,",
    "start": "3949790",
    "end": "3955205"
  },
  {
    "text": "a different interpretation. I'll just try to get your\nbrain ready for it, I guess. But if there are any other\nquestions on this, please ask.",
    "start": "3955205",
    "end": "3961850"
  },
  {
    "text": " AUDIENCE: What was the reward\nfunction for [INAUDIBLE]??",
    "start": "3961850",
    "end": "3966860"
  },
  {
    "text": "JOHN W. ROBERTS:\nThe reward function for this was the integral of\nvelocity, of spin velocity, over the integral\nof power input.",
    "start": "3966860",
    "end": "3974270"
  },
  {
    "text": "So it measured the force\non it, multiplied that by the vertical velocity.",
    "start": "3974270",
    "end": "3979640"
  },
  {
    "text": "That gives you\nthe rate of power. That gives you power,\nwhich is the rate of work.",
    "start": "3979640",
    "end": "3985550"
  },
  {
    "text": "And then it just sort of\ncalculates the distance. And so that ratio is what\nwe tried to optimize. So it tries to figure out sort\nof the minimum energy per unit",
    "start": "3985550",
    "end": "3993830"
  },
  {
    "text": "distance. And so it spins\naround in a circle, but it's a model of\nit going forward. So we did it for\nan angle, but you",
    "start": "3993830",
    "end": "3999260"
  },
  {
    "text": "can do it just as easily for\nif you had a linear test. It's just harder experimentally. And so it's try-- it's sort\nof an efficiency metric.",
    "start": "3999260",
    "end": "4006100"
  },
  {
    "text": " Yeah? ",
    "start": "4006100",
    "end": "4013520"
  },
  {
    "text": "All right. Turn the lights back up.",
    "start": "4013520",
    "end": "4019670"
  },
  {
    "text": " Make sure I crossed all\nmy Ts, dotted my Is.",
    "start": "4019670",
    "end": "4027550"
  },
  {
    "start": "4027550",
    "end": "4032900"
  },
  {
    "text": "Oh, yeah. And actually, there's\none story, too, before I get into that thing.",
    "start": "4032900",
    "end": "4038180"
  },
  {
    "text": "So a lot of these\nthings originated, like a lot of the things we've\nseen for neural networks--",
    "start": "4038180",
    "end": "4044990"
  },
  {
    "text": "like back prop, like\ngradient descent. I mean, we learned\nthat [INAUDIBLE] originated in the context of\nneural networks, RTRL did.",
    "start": "4044990",
    "end": "4053210"
  },
  {
    "text": "And a lot of this did,\nthe reinforce algorithm, which is the thing we're going\nto talk about-- originated with neural networks.",
    "start": "4053210",
    "end": "4060030"
  },
  {
    "text": "And one of the reasons\nthey found so appealing, particularly like this\nkind of stochastic work, is that it seemed\nbiologically plausible.",
    "start": "4060030",
    "end": "4066230"
  },
  {
    "text": "That it could be like, what is\nthe chance that a human brain is doing back prop? I mean, it could be doing some\nsort of approximate back prop",
    "start": "4066230",
    "end": "4071750"
  },
  {
    "text": "or something like that. I actually don't know that\nmuch about neuroscience. But the thing is that these\nsort of computationally involved",
    "start": "4071750",
    "end": "4078860"
  },
  {
    "text": "techniques for\nsolving these problems don't seem like they're\nreasonable as sort",
    "start": "4078860",
    "end": "4085250"
  },
  {
    "text": "of postulations on how the\nhuman brain or how neurons solve these problems.",
    "start": "4085250",
    "end": "4090960"
  },
  {
    "text": "But this one, you can\nsee, it's so simple. And the little randomness\nbeing part of it and just the sort of like\nsimple update structure does",
    "start": "4090960",
    "end": "4096979"
  },
  {
    "text": "seem biologically plausible. Just sort of intuitively,\nit makes more sense. But even more than that,\nthere's examples of--",
    "start": "4096979",
    "end": "4104089"
  },
  {
    "text": "there's data and\nevidence that suggests that these kind of things could\nbe one of the aspects of how",
    "start": "4104090",
    "end": "4109159"
  },
  {
    "text": "animals learn. And the coolest one, I\nthink, is there's these song birds that learn how to sing.",
    "start": "4109160",
    "end": "4114255"
  },
  {
    "text": "Like, they don't-- they're\nnot born knowing a certain way to sing. But they hear their parents\nsing as they're growing up,",
    "start": "4114255",
    "end": "4119647"
  },
  {
    "text": "and they start\nsinging more and more. And they get better and better. And actually, you can hear\nthem getting better until they sing like their parents did.",
    "start": "4119647",
    "end": "4124880"
  },
  {
    "text": "And you can raise\nthem in captivity and play them\nElvis all the time, and they'll do like a song\nbird impression of Elvis, which",
    "start": "4124880",
    "end": "4133759"
  },
  {
    "text": "I'm surprised you can't buy the\nCD of that on late night TV. But the-- right.",
    "start": "4133760",
    "end": "4142422"
  },
  {
    "text": "But so a really\ncool thing, though, is that there's this\npart of the brain where, if you measure sort\nof the signals,",
    "start": "4142422",
    "end": "4148849"
  },
  {
    "text": "they seem to be\ncompletely random. Like, they just seem\nto be random noise. And so it's like-- it's\nstrange that there's not",
    "start": "4148850",
    "end": "4154640"
  },
  {
    "text": "this structure. It's like, what could this\npart of the brain be doing? Why would it need to be\nproducing random noise? What they did--\nand this is your--",
    "start": "4154640",
    "end": "4161509"
  },
  {
    "text": "maybe you bird lovers\nout there won't like it-- they took one of these birds. And while it was\nlearning-- like,",
    "start": "4161510",
    "end": "4166785"
  },
  {
    "text": "they waited till a bird\nlearned like the full song. And then they deactivated,\nthrough some means,",
    "start": "4166785",
    "end": "4171828"
  },
  {
    "text": "the part of the brain that\nproduces random noise. And nothing happened. The bird-- apparently, the bird\nwasn't like entirely the same.",
    "start": "4171828",
    "end": "4177380"
  },
  {
    "text": "But it still could\nsing the songs fine, everything like that. Then they took a bird who was\nin the process of learning",
    "start": "4177380",
    "end": "4182429"
  },
  {
    "text": "the song-- had learned some of it\nbut wasn't perfect yet and was still getting better--",
    "start": "4182430",
    "end": "4187729"
  },
  {
    "text": "and they deactivated\nthat part of the brain. And it started just\nsinging the same song.",
    "start": "4187729",
    "end": "4193609"
  },
  {
    "text": "Like, how it'd been\nsinging, it kept singing. It didn't get any better. And so that's some\nsort of proxy evidence",
    "start": "4193609",
    "end": "4199820"
  },
  {
    "text": "that this random noise was\nrelated towards the ability to improve, that it's\nnot storing the signal, that it's not necessarily\nlike the descent itself.",
    "start": "4199820",
    "end": "4207722"
  },
  {
    "text": "But just this random\nnoise could be how it's screwing up\nits song in an effort",
    "start": "4207722",
    "end": "4212938"
  },
  {
    "text": "to get better and better. It screws up a bit, listens, and\nmaybe it's a little bit better, and it does that. So that's sort of a pretty--",
    "start": "4212938",
    "end": "4218930"
  },
  {
    "text": "I mean, and sometimes\ncompelling evidence that, I mean, biology\ncould at least use this as some aspect of\nits improvement,",
    "start": "4218930",
    "end": "4225260"
  },
  {
    "text": "that you shut down the random\nnoise and it stops learning. I mean, if you get\nthe variance to 0,",
    "start": "4225260",
    "end": "4231010"
  },
  {
    "text": "you're not going to\nget worse, you're just not going to do anything. You're going to keep\nsinging the same song.",
    "start": "4231010",
    "end": "4236269"
  },
  {
    "text": "So that's sort of cool, I think. Right, so just give you\nsomething to chew on.",
    "start": "4236270",
    "end": "4245330"
  },
  {
    "text": "There's another\ninterpretation of this. So here we sort of\ntalked about this one.",
    "start": "4245330",
    "end": "4250400"
  },
  {
    "text": "Here I think our idea was\nthis sort of sampling,",
    "start": "4250400",
    "end": "4258090"
  },
  {
    "text": "where we have some\nnominal policy. ",
    "start": "4258090",
    "end": "4266560"
  },
  {
    "text": "We perturb it, measure how\ngood we did, how well we did,",
    "start": "4266560",
    "end": "4280280"
  },
  {
    "text": "measure performance, and update.",
    "start": "4280280",
    "end": "4288139"
  },
  {
    "text": " So this is pretty\nmuch what we have,",
    "start": "4288140",
    "end": "4293180"
  },
  {
    "text": "is we have got some policy\nthat we're working at. We add this z to it\nthat changes it a bit. We run it, and then we update.",
    "start": "4293180",
    "end": "4300170"
  },
  {
    "text": "There's a different\ninterpretation-- my performance got too long. There's a stochastic\npolicy interpretation.",
    "start": "4300170",
    "end": "4307899"
  },
  {
    "start": "4307899",
    "end": "4314390"
  },
  {
    "text": "Now, in this, the way\nyou think about it isn't that we have\nsome nominal policy",
    "start": "4314390",
    "end": "4319822"
  },
  {
    "text": "and we're adding noise to it. It's that your policy\nitself acts stochastically. So actions are random.",
    "start": "4319822",
    "end": "4329420"
  },
  {
    "text": " Doesn't mean that they're\ncompletely random.",
    "start": "4329420",
    "end": "4335000"
  },
  {
    "text": "I mean, they're random\nwith some distribution. But you're not saying\nexactly what you do. And so you can imagine this\nis sort of like if you're",
    "start": "4335000",
    "end": "4344630"
  },
  {
    "text": "playing Liar's Poker--\nyou know, where you hold the card above your head. And then you can see\nanyone else's card but not",
    "start": "4344630",
    "end": "4350690"
  },
  {
    "text": "your own, and then you sort\nof bet on these things. Do you know the game? Maybe that game doesn't have\nenough cultural penetration to be a good example.",
    "start": "4350690",
    "end": "4356247"
  },
  {
    "text": "But if you're playing normal\npoker, any sort of gambling games, if every time\nyou had the same cards, if you made the exact same\nbet, people could eventually",
    "start": "4356247",
    "end": "4363739"
  },
  {
    "text": "sort of figure that\nout, and maybe they could use it to beat you. There's plenty of games like\nthat, where, say, every time I",
    "start": "4363740",
    "end": "4370400"
  },
  {
    "text": "have a certain card,\nI always bet this. Then if I bet that\nway, they're going to be like, oh,\nhe has good cards. I'm going to fold.",
    "start": "4370400",
    "end": "4375405"
  },
  {
    "text": "Or, oh, he always bluffs\nwhen he has this card. So this sort of deterministic\npolicy doesn't make sense. The stochastic policy\nis exactly what you do.",
    "start": "4375405",
    "end": "4382065"
  },
  {
    "text": "So your policy is\ngoing to be like, oh, I've got like pocket kings, 95%\nof the time I'm going to raise whatever, and [INAUDIBLE]\ntime I'm going to check--",
    "start": "4382065",
    "end": "4389863"
  },
  {
    "text": "those kind of things, where\nyou're sort of-- there's some noise in what you do. Now, you can question\nwhether or not",
    "start": "4389863",
    "end": "4395810"
  },
  {
    "text": "that makes sense as whether\noptimal policies would be stochastic in the kind\nof problems we look at. But the important thing\nis just to realize",
    "start": "4395810",
    "end": "4402320"
  },
  {
    "text": "that your policy, don't think of\nit as it's doing these things. It is these sort of\ndistributions of what you do.",
    "start": "4402320",
    "end": "4408810"
  },
  {
    "text": "All right? So the parameterization,\nthen, controls distribution.",
    "start": "4408810",
    "end": "4426440"
  },
  {
    "text": " Ooh. My fifth grade teacher\nwould not have liked that.",
    "start": "4426440",
    "end": "4433710"
  },
  {
    "text": "But.  So what you do, then, you\ncan imagine you control,",
    "start": "4433710",
    "end": "4440400"
  },
  {
    "text": "perhaps, the mean\nof a distribution. So where over here we had this-- you can think of it as really\nsort of exactly the same, where",
    "start": "4440400",
    "end": "4446420"
  },
  {
    "text": "my other interpretation\nsaid, OK, my policy is alpha, and then I add\nrandom noise z to it.",
    "start": "4446420",
    "end": "4453620"
  },
  {
    "text": "While here, my policy is\nparameterized by alpha, and my action is the same thing.",
    "start": "4453620",
    "end": "4459060"
  },
  {
    "text": "It's just that it's not,\nthis is what I'm doing and I'm sampling something else. It's that, this is\nactually my policy.",
    "start": "4459060",
    "end": "4464300"
  },
  {
    "text": "If I ran the same policy, I\nwould just do all these things with these probabilities.",
    "start": "4464300",
    "end": "4469700"
  },
  {
    "text": "So it's your actions\nare stochastic. Now, that's sort of something\nthat isn't always completely--",
    "start": "4469700",
    "end": "4479570"
  },
  {
    "text": "well, when I first saw it,\nit wasn't really easy for me to get my head around\nall what that meant. But yeah.",
    "start": "4479570",
    "end": "4485630"
  },
  {
    "text": "So this is-- we're\ngoing to look at this. Yeah, I won't go\ninto more detail. But tomorrow we'll\nlook at this sort",
    "start": "4485630",
    "end": "4491210"
  },
  {
    "text": "of different interpretation\nof how to do this. And you can get\nthe same learning. we'll actually show that\nthe update's the same, the behavior's very similar.",
    "start": "4491210",
    "end": "4497243"
  },
  {
    "text": "But the properties are\na little bit different. And the big thing\nis that you don't have to do this linearization.",
    "start": "4497243",
    "end": "4502402"
  },
  {
    "text": "Here we did this sort\nof linear expansion and we say, OK, so\nthis is true locally. When you look at\nit in this context,",
    "start": "4502402",
    "end": "4508010"
  },
  {
    "text": "you can show that\nyou'll always follow the gradient of the expected\nvalue of the policy. All right?",
    "start": "4508010",
    "end": "4513679"
  },
  {
    "text": "And so that's a big\ndifference, right? Here we're saying, OK, we're\nlooking at the local gradient, we're going to follow\nthe local gradient here.",
    "start": "4513680",
    "end": "4519080"
  },
  {
    "text": "But let's say that you\nhave a very broad policy or a very sort of\nviolent value function. Let's look at this 1D one again,\nwhere my value function has",
    "start": "4519080",
    "end": "4525530"
  },
  {
    "text": "something like this-- extremely violent. Well, when I put this\nrandom stochastic policy,",
    "start": "4525530",
    "end": "4531950"
  },
  {
    "text": "that smooths it out. And so even though--\nit's because I have a stochastic policy. Running my policy, the cost\nis a random variable now,",
    "start": "4531950",
    "end": "4539545"
  },
  {
    "text": "depending on what\nmy actions are. Even if my dynamics\nare deterministic, because my policy is stochastic,\nmy cost is stochastic,",
    "start": "4539545",
    "end": "4547250"
  },
  {
    "text": "I'm going to get some-- if you look at this,\nthere's some expected cost for running this policy on this.",
    "start": "4547250",
    "end": "4552608"
  },
  {
    "text": "And you do this,\nyou're going to sort of-- you can imagine\nsort of smoothing out some of this, right? Sort of averaging\nover all of these.",
    "start": "4552608",
    "end": "4558020"
  },
  {
    "text": "And what you follow\nwhen you do this-- and the update is\nreally identical. Like, it's actually just\nthe exact same update.",
    "start": "4558020",
    "end": "4564857"
  },
  {
    "text": "Possibly, there's a coefficient\nup front that you could put in or not. But the structure is the same. And the thing is\nthat it'll follow",
    "start": "4564857",
    "end": "4571159"
  },
  {
    "text": "the expected value\nof the performance of the stochastic policy. So it's sort of a\ndifferent way of thinking.",
    "start": "4571160",
    "end": "4577440"
  },
  {
    "text": "I think that this way is sort of\nthe easier way to sort of first think about it. But tomorrow will be more\nprobability kind of things.",
    "start": "4577440",
    "end": "4582889"
  },
  {
    "text": "And we'll talk about\nthe stochastic policy interpretation and some of\nthe ramifications of that.",
    "start": "4582890",
    "end": "4588570"
  },
  {
    "text": "Yeah. And maybe some other\ninteresting side notes. So yeah.",
    "start": "4588570",
    "end": "4595510"
  },
  {
    "start": "4595510",
    "end": "4596458"
  }
]