[
  {
    "start": "0",
    "end": "0"
  },
  {
    "start": "8019",
    "end": "8019"
  },
  {
    "text": "Let's discuss the method Ashenfelter used\nto build his model, linear regression.",
    "start": "8019",
    "end": "13759"
  },
  {
    "text": "We'll start with one-variable linear regression,\nwhich",
    "start": "13759",
    "end": "17520"
  },
  {
    "text": "just uses one independent variable to predict\nthe dependent variable.",
    "start": "17520",
    "end": "23630"
  },
  {
    "text": "This figure shows a plot of one of the independent\nvariables,",
    "start": "23630",
    "end": "27199"
  },
  {
    "text": "average growing season temperature,\nand the dependent variable, wine price.",
    "start": "27200",
    "end": "32800"
  },
  {
    "text": "The goal of linear regression is to create\na predictive line",
    "start": "32800",
    "end": "36140"
  },
  {
    "text": "through the data.",
    "start": "36140",
    "end": "37890"
  },
  {
    "text": "There are many different lines that\ncould be drawn to predict wine price using",
    "start": "37890",
    "end": "41940"
  },
  {
    "text": "average\ngrowing season temperature.",
    "start": "41940",
    "end": "44559"
  },
  {
    "text": "A simple option would be a flat line at the\naverage price,",
    "start": "44560",
    "end": "49260"
  },
  {
    "text": "in this case 7.07.",
    "start": "49260",
    "end": "52460"
  },
  {
    "text": "The equation for this line is y equals 7.07.",
    "start": "52460",
    "end": "58739"
  },
  {
    "text": "This linear regression model would predict\n7.07 regardless",
    "start": "58740",
    "end": "63719"
  },
  {
    "text": "of the temperature.",
    "start": "63719",
    "end": "65969"
  },
  {
    "text": "But it looks like a better line would\nhave a positive slope, such as this line in",
    "start": "65969",
    "end": "71280"
  },
  {
    "text": "blue.",
    "start": "71280",
    "end": "73530"
  },
  {
    "text": "The equation for this line is y equals 0.5*(AGST)\n-1.25.",
    "start": "73530",
    "end": "85539"
  },
  {
    "text": "This linear regression model would predict\na higher price",
    "start": "85539",
    "end": "88729"
  },
  {
    "text": "when the temperature is higher.",
    "start": "88729",
    "end": "91340"
  },
  {
    "text": "Let's make this idea a little more formal.",
    "start": "91340",
    "end": "94328"
  },
  {
    "start": "94000",
    "end": "94000"
  },
  {
    "text": "In general form a one-variable linear regression\nmodel",
    "start": "94329",
    "end": "98319"
  },
  {
    "text": "is a linear equation to predict the dependent\nvariable, y,",
    "start": "98319",
    "end": "102810"
  },
  {
    "text": "using the independent variable, x.",
    "start": "102810",
    "end": "105618"
  },
  {
    "text": "Beta 0 is the intercept term or intercept\ncoefficient,",
    "start": "105619",
    "end": "110439"
  },
  {
    "text": "and Beta 1 is the slope of the line or coefficient\nfor the independent variable, x.",
    "start": "110439",
    "end": "116759"
  },
  {
    "text": "For each observation, i, we have data\nfor the dependent variable Yi and data",
    "start": "116759",
    "end": "123060"
  },
  {
    "text": "for the independent variable, Xi.",
    "start": "123060",
    "end": "126849"
  },
  {
    "text": "Using this equation we make a prediction beta\n0 plus Beta",
    "start": "126849",
    "end": "131569"
  },
  {
    "text": "1 times Xi for each data point, i.",
    "start": "131569",
    "end": "135730"
  },
  {
    "text": "This prediction is hopefully close to the\ntrue outcome, Yi.",
    "start": "135730",
    "end": "140819"
  },
  {
    "text": "But since the coefficients have to be the\nsame for all data",
    "start": "140820",
    "end": "144670"
  },
  {
    "text": "points, i, we often make a small error,\nwhich we'll call epsilon i.",
    "start": "144670",
    "end": "150810"
  },
  {
    "text": "This error term is also often called a residual.",
    "start": "150810",
    "end": "155480"
  },
  {
    "text": "Our errors will only all be 0 if all our points\nlie perfectly",
    "start": "155480",
    "end": "159860"
  },
  {
    "text": "on the same line.",
    "start": "159860",
    "end": "161720"
  },
  {
    "text": "This rarely happens, so we know that our model\nwill probably",
    "start": "161720",
    "end": "164990"
  },
  {
    "text": "make some errors.",
    "start": "164990",
    "end": "167040"
  },
  {
    "text": "The best model or best choice of coefficients\nBeta 0 and Beta 1",
    "start": "167040",
    "end": "172239"
  },
  {
    "text": "has the smallest error terms or smallest residuals.",
    "start": "172240",
    "end": "184220"
  },
  {
    "start": "184000",
    "end": "184000"
  },
  {
    "text": "This figure shows the blue line that we drew\nin the beginning.",
    "start": "184220",
    "end": "188130"
  },
  {
    "text": "We can compute the residuals or errors\nof this line for each data point.",
    "start": "188130",
    "end": "193610"
  },
  {
    "text": "For example, for this point the actual value\nis about 6.2.",
    "start": "193610",
    "end": "199760"
  },
  {
    "text": "Using our regression model we predict about\n6.5.",
    "start": "199760",
    "end": "204420"
  },
  {
    "text": "So the error for this data point is negative\n0.3,",
    "start": "204420",
    "end": "208640"
  },
  {
    "text": "which is the actual value minus our prediction.",
    "start": "208640",
    "end": "212770"
  },
  {
    "text": "As another example for this point,\nthe actual value is about 8.",
    "start": "212770",
    "end": "219180"
  },
  {
    "text": "Using our regression model we predict about\n7.5.",
    "start": "219180",
    "end": "224200"
  },
  {
    "text": "So the error for this data point is about\n0.5.",
    "start": "224200",
    "end": "228560"
  },
  {
    "text": "Again the actual value minus our prediction.",
    "start": "228560",
    "end": "233819"
  },
  {
    "text": "One measure of the quality of a regression\nline",
    "start": "233820",
    "end": "236790"
  },
  {
    "text": "is the sum of squared errors, or SSE.",
    "start": "236790",
    "end": "241110"
  },
  {
    "text": "This is the sum of the squared residuals or\nerror terms.",
    "start": "241110",
    "end": "245850"
  },
  {
    "text": "Let n equal the number of data points that\nwe have in our data",
    "start": "245850",
    "end": "249740"
  },
  {
    "text": "set.",
    "start": "249740",
    "end": "252400"
  },
  {
    "text": "Then the sum of squared errors is\nequal to the error we make on the first data",
    "start": "252400",
    "end": "257338"
  },
  {
    "text": "point squared\nplus the error we make on the second data",
    "start": "257339",
    "end": "261370"
  },
  {
    "text": "point squared\nplus the errors that you make on all data",
    "start": "261370",
    "end": "264800"
  },
  {
    "text": "points\nup to the n-th data point squared.",
    "start": "264800",
    "end": "272978"
  },
  {
    "text": "We can compute the sum of squared errors\nfor both the red line and the blue line.",
    "start": "272979",
    "end": "278210"
  },
  {
    "text": "As expected the blue line is a better fit\nthan the red line",
    "start": "278210",
    "end": "282138"
  },
  {
    "text": "since it has a smaller sum of squared errors.",
    "start": "282139",
    "end": "285930"
  },
  {
    "text": "The line that gives the minimum sum of squared\nerrors",
    "start": "285930",
    "end": "288900"
  },
  {
    "text": "is shown in green.",
    "start": "288900",
    "end": "290680"
  },
  {
    "text": "This is the line that our regression model\nwill find.",
    "start": "290680",
    "end": "294830"
  },
  {
    "text": "Although sum of squared errors allows us to\ncompare lines",
    "start": "294830",
    "end": "298210"
  },
  {
    "start": "296000",
    "end": "296000"
  },
  {
    "text": "on the same data set, it's hard to interpret\nfor two reasons.",
    "start": "298210",
    "end": "303289"
  },
  {
    "text": "The first is that it scales with n, the number\nof data points.",
    "start": "303289",
    "end": "307849"
  },
  {
    "text": "If we built the same model with twice as much\ndata,",
    "start": "307849",
    "end": "311449"
  },
  {
    "text": "the sum of squared errors might be twice as\nbig.",
    "start": "311449",
    "end": "314180"
  },
  {
    "text": "But this doesn't mean it's a worse model.",
    "start": "314180",
    "end": "317419"
  },
  {
    "text": "The second is that the units are hard to understand.",
    "start": "317419",
    "end": "320270"
  },
  {
    "text": "Some of squared errors is in squared units\nof the dependent variable.",
    "start": "320270",
    "end": "326270"
  },
  {
    "text": "Because of these problems, Root Means Squared\nError, or RMSE,",
    "start": "326270",
    "end": "331039"
  },
  {
    "text": "is often used.",
    "start": "331039",
    "end": "332819"
  },
  {
    "text": "This divides sum of squared errors by n\nand then takes a square root.",
    "start": "332819",
    "end": "337699"
  },
  {
    "text": "So it's normalized by n and is in the same\nunits",
    "start": "337699",
    "end": "341180"
  },
  {
    "text": "as the dependent variable.",
    "start": "341180",
    "end": "344129"
  },
  {
    "text": "Another common error measure for linear regression\nis R squared.",
    "start": "344129",
    "end": "348759"
  },
  {
    "text": "This error measure is nice because it compares\nthe best",
    "start": "348759",
    "end": "351699"
  },
  {
    "text": "model to a baseline model, the model that\ndoes not",
    "start": "351699",
    "end": "355308"
  },
  {
    "text": "use any variables, or the red line from before.",
    "start": "355308",
    "end": "359479"
  },
  {
    "text": "The baseline model predicts the average value\nof the dependent variable regardless",
    "start": "359479",
    "end": "365370"
  },
  {
    "text": "of the value of the independent variable.",
    "start": "365370",
    "end": "368979"
  },
  {
    "text": "We can compute that the sum of squared errors\nfor the best fit",
    "start": "368979",
    "end": "372279"
  },
  {
    "text": "line or the green line is 5.73.",
    "start": "372279",
    "end": "376949"
  },
  {
    "text": "And the sum of squared errors for the baseline\nor the red line is 10.15.",
    "start": "376949",
    "end": "383080"
  },
  {
    "text": "The sum of squared errors for the baseline\nmodel",
    "start": "383080",
    "end": "385819"
  },
  {
    "text": "is also known as the total sum of squares,\ncommonly referred",
    "start": "385819",
    "end": "389880"
  },
  {
    "text": "to as SST.",
    "start": "389880",
    "end": "392590"
  },
  {
    "text": "Then the formula for R squared is\nR squared equals 1 minus sum of squared errors",
    "start": "392590",
    "end": "400860"
  },
  {
    "text": "divided\nby total sum of squares.",
    "start": "400860",
    "end": "404599"
  },
  {
    "text": "In this case it equals 1 minus 5.73\ndivided by 10.15 which equals 0.44.",
    "start": "404599",
    "end": "416400"
  },
  {
    "text": "R squared is nice because it captures\nthe value added from using a linear regression",
    "start": "416400",
    "end": "421719"
  },
  {
    "start": "417000",
    "end": "417000"
  },
  {
    "text": "model over just predicting the average outcome\nfor every data",
    "start": "421719",
    "end": "426129"
  },
  {
    "text": "point.",
    "start": "426129",
    "end": "427319"
  },
  {
    "text": "So what values do we expect to see for R squared?",
    "start": "427319",
    "end": "430610"
  },
  {
    "text": "Well both the sum of squared errors\nand the total sum of squares have",
    "start": "430610",
    "end": "436520"
  },
  {
    "text": "to be greater than or equal to zero because\nthey're",
    "start": "436520",
    "end": "439430"
  },
  {
    "text": "the sum of squared terms so they can't be\nnegative.",
    "start": "439430",
    "end": "443680"
  },
  {
    "text": "Additionally the sum of squared errors has\nto be less than",
    "start": "443680",
    "end": "447809"
  },
  {
    "text": "or equal to the total sum of squares.",
    "start": "447809",
    "end": "451270"
  },
  {
    "text": "This is because our linear regression model\ncould just",
    "start": "451270",
    "end": "454460"
  },
  {
    "text": "set the coefficient for the independent variable\nto 0",
    "start": "454460",
    "end": "458749"
  },
  {
    "text": "and then we would have the baseline model.",
    "start": "458749",
    "end": "461720"
  },
  {
    "text": "So our linear regression model will never\nbe worse than the baseline model.",
    "start": "461720",
    "end": "466110"
  },
  {
    "text": "So in the worst case the sum of squares errors\nequals the total sum of squares, and our R",
    "start": "466110",
    "end": "472960"
  },
  {
    "text": "squared is equal to 0.",
    "start": "472960",
    "end": "475449"
  },
  {
    "text": "So this means no improvement over the baseline.",
    "start": "475449",
    "end": "479529"
  },
  {
    "text": "In the best case our linear regression model\nmakes no errors, and the sum of squared errors",
    "start": "479529",
    "end": "485699"
  },
  {
    "text": "is equal to 0.",
    "start": "485699",
    "end": "487809"
  },
  {
    "text": "And then our R squared is equal to 1.",
    "start": "487809",
    "end": "491029"
  },
  {
    "text": "So an R squared equal to 1 or close to 1\nmeans a perfect or almost perfect predictive",
    "start": "491029",
    "end": "497689"
  },
  {
    "text": "model.",
    "start": "497689",
    "end": "500089"
  },
  {
    "text": "R squared is nice because it's unitless and\ntherefore",
    "start": "500089",
    "end": "503069"
  },
  {
    "text": "universally interpretable between problems.",
    "start": "503069",
    "end": "506469"
  },
  {
    "text": "However, it can still be hard to compare between\nproblems.",
    "start": "506469",
    "end": "511060"
  },
  {
    "text": "Good models for easy problems will\nhave an R squared close to 1.",
    "start": "511060",
    "end": "515549"
  },
  {
    "text": "But good models for hard problems\ncan still have an R squared close to zero.",
    "start": "515549",
    "end": "521570"
  },
  {
    "text": "Throughout this course we will see\nexamples of both types of problems.",
    "start": "521570",
    "end": "525660"
  }
]