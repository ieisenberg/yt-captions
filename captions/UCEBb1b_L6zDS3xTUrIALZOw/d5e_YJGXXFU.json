[
  {
    "start": "0",
    "end": "51000"
  },
  {
    "text": " VOICEOVER: The following content\nis provided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT Open Courseware continue to offer high-quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu.  JULIAN SHUN: Good\nafternoon, everyone.",
    "start": "18140",
    "end": "24500"
  },
  {
    "text": "So today we're going to talk\nabout storage allocation.",
    "start": "24500",
    "end": "29780"
  },
  {
    "text": "This is a continuation\nfrom last lecture where we talked about\nserial storage allocation.",
    "start": "29780",
    "end": "35345"
  },
  {
    "text": "Today we'll also talk\na little bit more about serial allocation. But then I'll talk more about\nparallel allocation and also",
    "start": "35345",
    "end": "44020"
  },
  {
    "text": "garbage collection. So I want to just do a review\nof some memory allocation",
    "start": "44020",
    "end": "49870"
  },
  {
    "text": "primitives. So recall that you can use\nmalloc to allocate memory",
    "start": "49870",
    "end": "56890"
  },
  {
    "start": "51000",
    "end": "51000"
  },
  {
    "text": "from the heap. And if you call malloc\nwith the size of s,",
    "start": "56890",
    "end": "62140"
  },
  {
    "text": "it's going to\nallocate and return a pointer to a block of memory\ncontaining at least s bytes.",
    "start": "62140",
    "end": "68630"
  },
  {
    "text": "So you might actually\nget more than s bytes, even though you\nasked for s bytes. But it's guaranteed to\ngive you at least s bytes.",
    "start": "68630",
    "end": "76990"
  },
  {
    "text": "The return values avoid star,\nbut good programming practice is to typecast this\npointer to whatever type",
    "start": "76990",
    "end": "83620"
  },
  {
    "text": "you're using this memory\nfor when you receive this from the malloc call. ",
    "start": "83620",
    "end": "90550"
  },
  {
    "text": "There's also aligned allocation. So you can do aligned\nallocation with memalign,",
    "start": "90550",
    "end": "96790"
  },
  {
    "text": "which takes two arguments, a\nsize a as well as a size s.",
    "start": "96790",
    "end": "101880"
  },
  {
    "text": "And a has to be an\nexact power of 2, and it's going to\nallocate and return a pointer to a block of\nmemory again containing",
    "start": "101880",
    "end": "108490"
  },
  {
    "text": "at least s bytes. But this time this\nmemory is going to be aligned to\na multiple of a,",
    "start": "108490",
    "end": "114310"
  },
  {
    "text": "so the address is going\nto be a multiple of a, where this memory block starts. ",
    "start": "114310",
    "end": "120400"
  },
  {
    "text": "So does anyone know why we might\nwant to do an aligned memory allocation?",
    "start": "120400",
    "end": "125590"
  },
  {
    "text": " Yeah? STUDENT: [INAUDIBLE]",
    "start": "125590",
    "end": "135630"
  },
  {
    "text": "JULIAN SHUN: Yeah,\nso one reason is that you can align\nmemories so that they're aligned to cache lines, so that\nwhen you access an object that",
    "start": "135630",
    "end": "144920"
  },
  {
    "text": "fits within the\ncache line, it's not going to cross two cache lines. And you'll only get one\ncache axis instead of two.",
    "start": "144920",
    "end": "152490"
  },
  {
    "text": "So one reason is that\nyou want to align the memory to cache\nlines to reduce",
    "start": "152490",
    "end": "158330"
  },
  {
    "text": "the number of cache misses. You get another reason is that\nthe vectorization operators",
    "start": "158330",
    "end": "165530"
  },
  {
    "text": "also require you to have\nmemory addresses that are aligned to some power of 2. So if you align your memory\nallocation with memalign,",
    "start": "165530",
    "end": "173480"
  },
  {
    "text": "then that's also good\nfor the vector units. We also talked\nabout deallocations.",
    "start": "173480",
    "end": "179180"
  },
  {
    "text": "You can free memory back to the\nheap with the free function.",
    "start": "179180",
    "end": "184890"
  },
  {
    "text": "So if you pass at a point of\np to some block of memory, it's going to\ndeallocate this block",
    "start": "184890",
    "end": "191510"
  },
  {
    "text": "and return it to the\nstorage allocator. And we also talked about\nsome anomalies of freeing.",
    "start": "191510",
    "end": "200150"
  },
  {
    "text": "So what is it called\nwhen you fail to free some memory that you allocated? ",
    "start": "200150",
    "end": "209381"
  },
  {
    "text": "Yes? Yeah, so If you fail to freeze\nsomething that you allocated, that's called a memory leak.",
    "start": "209381",
    "end": "215960"
  },
  {
    "text": "And this can cause your program\nto use more and more memory. And eventually your\nprogram is going",
    "start": "215960",
    "end": "223102"
  },
  {
    "text": "to use up all the\nmemory on your machine, and it's going to crash. We also talked about freeing\nsomething more than once.",
    "start": "223102",
    "end": "230599"
  },
  {
    "text": "Does anyone remember\nwhat that's called? Yeah? Yeah, so that's\ncalled double freeing.",
    "start": "230600",
    "end": "236659"
  },
  {
    "text": "Double freeing is when you\nfree something more than once. And the behavior is\ngoing to be undefined.",
    "start": "236660",
    "end": "242050"
  },
  {
    "text": "You might get a seg\nfault immediately, or you'll free something\nthat was allocated",
    "start": "242050",
    "end": "247760"
  },
  {
    "text": "for some other purpose. And then later down\nthe road your program is going to have some\nunexpected behavior.",
    "start": "247760",
    "end": "253319"
  },
  {
    "text": " OK.",
    "start": "253320",
    "end": "258370"
  },
  {
    "text": "I also want to talk about m map. So m map is a system call.",
    "start": "258370",
    "end": "264520"
  },
  {
    "text": "And usually m map is used\nto treat some file on disk as part of memory,\nso that when you",
    "start": "264520",
    "end": "271420"
  },
  {
    "text": "write to that memory region,\nit also backs it up on disk.",
    "start": "271420",
    "end": "276490"
  },
  {
    "text": "In this context\nhere, I'm actually using m map to allocate\nvirtual memory without having",
    "start": "276490",
    "end": "282190"
  },
  {
    "start": "279000",
    "end": "279000"
  },
  {
    "text": "any backing file. So So our map has a whole\nbunch of parameters here. The second to the last\nparameter indicates",
    "start": "282190",
    "end": "289330"
  },
  {
    "text": "the file I want to map,\nand if I pass a negative 1, that means there's\nno backing file. So I'm just using this to\nallocate some virtual memory.",
    "start": "289330",
    "end": "297130"
  },
  {
    "text": "The first argument is where\nI want to allocate it. And 0 means that I don't care.",
    "start": "297130",
    "end": "302160"
  },
  {
    "text": "The size in terms\nof number of bytes has how much memory\nI want to allocate.",
    "start": "302160",
    "end": "307720"
  },
  {
    "text": "Then there's also permissions. So here it says I can read\nand write this memory region.",
    "start": "307720",
    "end": "315280"
  },
  {
    "text": "s private means that\nthis memory region is private to the process\nthat's allocating it.",
    "start": "315280",
    "end": "320860"
  },
  {
    "text": "And then map anon means that\nthere is no name associated with this memory region.",
    "start": "320860",
    "end": "326470"
  },
  {
    "text": "And then as I said,\nnegative 1 means that there's no backing file. And the last parameter is just\n0 if there's no backing file.",
    "start": "326470",
    "end": "333880"
  },
  {
    "text": "Normally it would be\nan offset into the file that you're trying to map. But here there's\nno backing file.",
    "start": "333880",
    "end": "339430"
  },
  {
    "text": "And what m map does is it finds\na contiguous unused region in the address space of the\napplication that's large enough",
    "start": "339430",
    "end": "346000"
  },
  {
    "text": "to hold size bytes. And then it updates\nthe page table so that it now contains\nan entry for the pages",
    "start": "346000",
    "end": "354430"
  },
  {
    "text": "that you allocated. And then it creates a necessary\nvirtual memory management structures within\nthe operating system",
    "start": "354430",
    "end": "361630"
  },
  {
    "text": "to make it so that users\naccesses to this area are legal, and accesses\nwon't result in a seg fault.",
    "start": "361630",
    "end": "371650"
  },
  {
    "text": "If you try to access some\nregion of memory without using--",
    "start": "371650",
    "end": "376660"
  },
  {
    "text": "without having OS\nset these parameters, then you might get a set fault\nbecause the program might not",
    "start": "376660",
    "end": "384310"
  },
  {
    "text": "have permission to\naccess that area. But m map is going to make\nsure that the user can access",
    "start": "384310",
    "end": "389380"
  },
  {
    "text": "this area of virtual memory. And m map is a system\ncall, whereas malloc,",
    "start": "389380",
    "end": "395080"
  },
  {
    "text": "which we talked about last\ntime, is a library call. So these are two\ndifferent things. And malloc actually uses\nm map under the hood",
    "start": "395080",
    "end": "402520"
  },
  {
    "text": "to get more memory from\nthe operating system.",
    "start": "402520",
    "end": "407810"
  },
  {
    "text": "So let's look at some\nproperties of m map. So m map is lazy.",
    "start": "407810",
    "end": "414190"
  },
  {
    "start": "409000",
    "end": "409000"
  },
  {
    "text": "So when you request a\ncertain amount of memory, it doesn't immediately\nallocate physical memory",
    "start": "414190",
    "end": "420370"
  },
  {
    "text": "for the requested allocation. Instead it just\npopulates the page table",
    "start": "420370",
    "end": "426070"
  },
  {
    "text": "with entries pointing\nto a special 0 page. And then it marks these\npages as read only.",
    "start": "426070",
    "end": "432500"
  },
  {
    "text": "And then the first time\nyou write to such a page, it will cause a page\nfault. And at that point,",
    "start": "432500",
    "end": "438340"
  },
  {
    "text": "the OS is going to\nmodify the page table, get the appropriate\nphysical memory,",
    "start": "438340",
    "end": "445400"
  },
  {
    "text": "and store the mapping from\nthe virtual address space to physical address space\nfor the particular page",
    "start": "445400",
    "end": "452055"
  },
  {
    "text": "that you touch. And then it will\nrestart the instructions so that it can\ncontinue to execute.",
    "start": "452055",
    "end": "457120"
  },
  {
    "text": " You can-- turns out\nthat you can actually m map a terabyte\nof virtual memory,",
    "start": "457120",
    "end": "464470"
  },
  {
    "text": "even on a machine with\njust a gigabyte of d ram. Because when you call m\nmap, it doesn't actually",
    "start": "464470",
    "end": "471340"
  },
  {
    "text": "allocate the physical memory. But then you should be careful,\nbecause a process might",
    "start": "471340",
    "end": "477310"
  },
  {
    "text": "die from running out\nof physical memory well after you call m map. Because m map is going to\nallocate this physical memory",
    "start": "477310",
    "end": "484930"
  },
  {
    "text": "whenever you first touch it. And this could be much\nlater than when you actually made the call to m map.",
    "start": "484930",
    "end": "492250"
  },
  {
    "text": "So any questions so far? ",
    "start": "492250",
    "end": "499319"
  },
  {
    "text": "OK.  So what's the difference\nbetween malloc and m map?",
    "start": "499320",
    "end": "506050"
  },
  {
    "start": "502000",
    "end": "502000"
  },
  {
    "text": "So as I said, malloc\nis a library call. And it's part of--malloc and\nfree are part of the memory",
    "start": "506050",
    "end": "513610"
  },
  {
    "text": "allocation interface of the\nheat-management code in the c library.",
    "start": "513610",
    "end": "518950"
  },
  {
    "text": "And the heat-management code\nuses the available system facilities, including\nthe m map function",
    "start": "518950",
    "end": "524860"
  },
  {
    "text": "to get a virtual address space\nfrom the operating system.",
    "start": "524860",
    "end": "531250"
  },
  {
    "text": "And then the\nheat-management code is going-- within\nmalloc-- is going to attempt to satisfy user\nrequests for heat storage",
    "start": "531250",
    "end": "537910"
  },
  {
    "text": "by reusing the memory\nthat it got from the OS as much as possible until\nit can't do that anymore.",
    "start": "537910",
    "end": "544210"
  },
  {
    "text": "And then it will\ngo and call m map to get more memory from\nthe operating system.",
    "start": "544210",
    "end": "550990"
  },
  {
    "text": "So the malloc implementation\ninvokes m map and other system calls to expand the size\nof the users heap storage.",
    "start": "550990",
    "end": "558430"
  },
  {
    "text": "And the responsibility\nof malloc is to reuse the memory, such that\nyour fragmentation is reduced,",
    "start": "558430",
    "end": "565510"
  },
  {
    "text": "and you have good\ntemporal locality, whereas the\nresponsibility of m map",
    "start": "565510",
    "end": "571000"
  },
  {
    "text": "is actually getting this memory\nfrom the operating system.",
    "start": "571000",
    "end": "576190"
  },
  {
    "text": "So any questions\non the differences between malloc and m map? ",
    "start": "576190",
    "end": "584560"
  },
  {
    "text": "So one question is,\nwhy don't we just call m map up all the time,\ninstead of just using malloc?",
    "start": "584560",
    "end": "590800"
  },
  {
    "text": "Why don't we just\ndirectly call m map? ",
    "start": "590800",
    "end": "602800"
  },
  {
    "text": "Yes. STUDENT: [INAUDIBLE] JULIAN SHUN: Yes,\nso one answer is",
    "start": "602800",
    "end": "609850"
  },
  {
    "text": "that you might have\nfree storage from before that you would want to reuse.",
    "start": "609850",
    "end": "618009"
  },
  {
    "text": "And it turns out that m map\nis relatively heavy weight. So it works on a\npage granularity.",
    "start": "618010",
    "end": "625600"
  },
  {
    "text": "So if you want to do\na small allocation, it's quite wasteful to\nallocate an entire page",
    "start": "625600",
    "end": "630730"
  },
  {
    "text": "for that allocation\nand not reuse it. You'll get very bad\nexternal fragmentation.",
    "start": "630730",
    "end": "636940"
  },
  {
    "text": "And when you call\nm map, it has to go through all of the overhead\nof the security of the OS and updating the\npage table and so on.",
    "start": "636940",
    "end": "644900"
  },
  {
    "text": "Whereas, if you use\nmalloc, it's actually pretty fast for\nmost allocations,",
    "start": "644900",
    "end": "650350"
  },
  {
    "text": "and especially if you have\ntemporal locality where you allocate something that\nyou just recently freed.",
    "start": "650350",
    "end": "656470"
  },
  {
    "text": "So your program\nwould be pretty slow if you used m map all the time,\neven for small allocations.",
    "start": "656470",
    "end": "662230"
  },
  {
    "text": "For big allocations, it's fine. But for small allocations,\nyou should use malloc.",
    "start": "662230",
    "end": "669730"
  },
  {
    "text": "Any questions on m\nmap versus malloc? ",
    "start": "669730",
    "end": "677850"
  },
  {
    "text": "OK, so I just want to do\na little bit of review on how address\ntranslation works.",
    "start": "677850",
    "end": "684102"
  },
  {
    "start": "679000",
    "end": "679000"
  },
  {
    "text": "So some of you might have seen\nthis before in your computer architecture course. So how it works is, when\nyou access memory location,",
    "start": "684102",
    "end": "694870"
  },
  {
    "text": "you access it via\nthe virtual address. And the virtual address can be\ndivided into two parts, where",
    "start": "694870",
    "end": "700569"
  },
  {
    "text": "the lower order bits\nstore the offset, and the higher order bits\nstore the virtual page number.",
    "start": "700570",
    "end": "706970"
  },
  {
    "text": "And in order to get the\nphysical address associated with this virtual\naddress, the hardware",
    "start": "706970",
    "end": "713019"
  },
  {
    "text": "is going to look up this\nvirtual page number in what's called the page table.",
    "start": "713020",
    "end": "719870"
  },
  {
    "text": "And then if it finds\na corresponding entry for the virtual page\nnumber in the page table, that will tell us the\nphysical frame number.",
    "start": "719870",
    "end": "728320"
  },
  {
    "text": "And then the\nphysical frame number corresponds to where this\nfiscal memory is in d ram.",
    "start": "728320",
    "end": "736060"
  },
  {
    "text": "So you can just take\nthe frame number, and then use the\nsame offset as before",
    "start": "736060",
    "end": "742240"
  },
  {
    "text": "to get the appropriate offset\ninto the physical memory frame. ",
    "start": "742240",
    "end": "750589"
  },
  {
    "text": "So if the virtual page\nthat you're looking for doesn't reside in\nphysical memory, then a page fault\nis going to occur.",
    "start": "750590",
    "end": "757910"
  },
  {
    "text": "And when a page fault occurs,\neither the operating system will see that the\nprocess actually",
    "start": "757910",
    "end": "763330"
  },
  {
    "text": "has permissions to look\nat that memory region, and it will set the\npermissions and place the entry",
    "start": "763330",
    "end": "769420"
  },
  {
    "text": "into the page table\nso that you can get the appropriate\nphysical address.",
    "start": "769420",
    "end": "775510"
  },
  {
    "text": "But otherwise, the\noperating system might see that this\nprocess actually can't access that region\nmemory, and then you'll",
    "start": "775510",
    "end": "780870"
  },
  {
    "text": "get a segmentation fault. It turns out that the page\ntable search, also called a page",
    "start": "780870",
    "end": "787300"
  },
  {
    "text": "walk, is pretty expensive. And that's why we have the\ntranslation look, a side",
    "start": "787300",
    "end": "794079"
  },
  {
    "text": "buffer or TLB,\nwhich is essentially a cache for the page table.",
    "start": "794080",
    "end": "799390"
  },
  {
    "text": "So the hardware uses a TLB\nto cache the recent page table look ups into this\nTLB so that later on when",
    "start": "799390",
    "end": "807670"
  },
  {
    "text": "you access the same\npage, it doesn't have to go all the\nway to the page table to find the physical address.",
    "start": "807670",
    "end": "813330"
  },
  {
    "text": "It can first look\nin the TLB to see if it's been recently accessed.",
    "start": "813330",
    "end": "818810"
  },
  {
    "text": "So why would you\nexpect to see something that it recently has accessed?",
    "start": "818810",
    "end": "824075"
  },
  {
    "text": " So what's one\nproperty of a program",
    "start": "824075",
    "end": "829110"
  },
  {
    "text": "that will make it so that\nyou get a lot of TLB hits? Yes? STUDENT: Well, usually\n[INAUDIBLE] nearby one another,",
    "start": "829110",
    "end": "839083"
  },
  {
    "text": "which means they're probably in\nthe same page or [INAUDIBLE]..",
    "start": "839083",
    "end": "844570"
  },
  {
    "text": "JULIAN SHUN: Yeah,\nso that's correct. So the page table\nstores pages, which",
    "start": "844570",
    "end": "849810"
  },
  {
    "text": "are typically four kilobytes. Nowadays there are\nalso huge pages, which can be a couple of megabytes.",
    "start": "849810",
    "end": "855640"
  },
  {
    "text": "And most of the\naccesses in your program are going to be near each other. So they're likely\ngoing to reside",
    "start": "855640",
    "end": "862710"
  },
  {
    "text": "on the same page for\naccesses that have been done close together in time.",
    "start": "862710",
    "end": "869620"
  },
  {
    "text": "So therefore you'll expect that\nmany of your recent accesses",
    "start": "869620",
    "end": "874650"
  },
  {
    "text": "are going to be\nstored in the TLB if your program has locality,\neither spatial or temporal",
    "start": "874650",
    "end": "880500"
  },
  {
    "text": "locality or both.  So how this architecture works\nis that the processor is first",
    "start": "880500",
    "end": "887608"
  },
  {
    "text": "going to check whether\nthe virtual address you're looking for is in TLB. If it's not, it's going to go to\nthe page table and look it up.",
    "start": "887608",
    "end": "895470"
  },
  {
    "text": "And then if it finds\nthat there, then it's going to store that\nentry into the TLB. And then next it's going to\ngo get this physical address",
    "start": "895470",
    "end": "902720"
  },
  {
    "text": "that it found from the TLB and\nlook it up into the CPU cache.",
    "start": "902720",
    "end": "908027"
  },
  {
    "text": "And if it finds it\nthere, it gets it. If it doesn't, then it goes to\nd ram to satisfy the request.",
    "start": "908028",
    "end": "913410"
  },
  {
    "text": "Most modern machines\nactually have an optimization that allow you to do TLB access\nin parallel with the L1 cache",
    "start": "913410",
    "end": "920250"
  },
  {
    "text": "access. So the L1 cache actually uses\nvirtual addresses instead of fiscal addresses,\nand this reduces",
    "start": "920250",
    "end": "926850"
  },
  {
    "text": "the latency of a memory access.",
    "start": "926850",
    "end": "932069"
  },
  {
    "text": "So that's a brief review\nof address translation. All right, so let's\ntalk about stacks.",
    "start": "932070",
    "end": "938055"
  },
  {
    "text": " So when you execute a\nserial c and c++ program,",
    "start": "938055",
    "end": "948430"
  },
  {
    "start": "941000",
    "end": "941000"
  },
  {
    "text": "you're using a stack to keep\ntrack of the function calls and local variables\nthat you have to save.",
    "start": "948430",
    "end": "956740"
  },
  {
    "text": "So here, let's say we\nhave this invocation tree, where function a calls\nFunction b, which then returns.",
    "start": "956740",
    "end": "963460"
  },
  {
    "text": "And then a calls\nfunction c, which calls d, returns, calls e,\nreturns, and then returns",
    "start": "963460",
    "end": "969490"
  },
  {
    "text": "again. Here are the different views of\nthe stack at different points",
    "start": "969490",
    "end": "974529"
  },
  {
    "text": "of the execution. So initially when we call a,\nwe have a stack frame for a.",
    "start": "974530",
    "end": "981029"
  },
  {
    "text": "And then when a\ncalls b, we're going to place a stack\nframe for b right",
    "start": "981030",
    "end": "986050"
  },
  {
    "text": "below the stack frame of a. So these are going to\nbe linearly ordered. When we're done with b,\nthen this part of the stack",
    "start": "986050",
    "end": "994420"
  },
  {
    "text": "is no longer going to\nbe used, the part for b. And then when it calls c, It's\ngoing to allocate a stack frame",
    "start": "994420",
    "end": "1001079"
  },
  {
    "text": "below a on the stack. And this space is actually going\nto be the same space as what",
    "start": "1001080",
    "end": "1007230"
  },
  {
    "text": "b was using before. But this is fine, because we're\nalready done with the call to b.",
    "start": "1007230",
    "end": "1012720"
  },
  {
    "text": "Then when c calls d, we're going\nto create a stack frame for d right below c. When it returns, we're not going\nto use that space any more,",
    "start": "1012720",
    "end": "1020340"
  },
  {
    "text": "so then we can reuse it for\nthe stack frame when we call e. And then eventually all\nof these will pop back up.",
    "start": "1020340",
    "end": "1029290"
  },
  {
    "text": "And all of these views\nhere share the same view of the stack frame for a.",
    "start": "1029290",
    "end": "1036299"
  },
  {
    "text": "And then for c, d, and e, they\nall stare share the same view of this stack for c.",
    "start": "1036300",
    "end": "1044369"
  },
  {
    "text": "So this is how a traditional\nlinear stack works when you call a serial c or c++ program.",
    "start": "1044369",
    "end": "1050520"
  },
  {
    "text": "And you can view this as a\nserial walk over the invocation tree. ",
    "start": "1050520",
    "end": "1059540"
  },
  {
    "text": "There's one rule for pointers. With traditional\nlinear stacks is that a parent can pass\npointers to its stack variables",
    "start": "1059540",
    "end": "1067610"
  },
  {
    "text": "down to its children. But not the other way around. A child can't pass a pointer\nto some local variable",
    "start": "1067610",
    "end": "1074750"
  },
  {
    "text": "back to its parent. So if you do that, you'll\nget a bug in your program. How many of you have\ntried doing that before?",
    "start": "1074750",
    "end": "1081820"
  },
  {
    "text": "Yeah, so a lot of you. So let's see why that\ncauses a problem.",
    "start": "1081820",
    "end": "1089240"
  },
  {
    "text": "So if I'm calling-- if I call b, and I pass a\npointer to some local variable",
    "start": "1089240",
    "end": "1096260"
  },
  {
    "text": "in b stack to a, and\nthen now when a calls c,",
    "start": "1096260",
    "end": "1101360"
  },
  {
    "text": "It's going to overwrite\nthe space that b was using. And if b's local variable\nwas stored in the space",
    "start": "1101360",
    "end": "1106760"
  },
  {
    "text": "that c has now\noverwritten, then you're just going to see garbage. And when you try to\naccess that, you're",
    "start": "1106760",
    "end": "1113480"
  },
  {
    "text": "not going to get\nthe correct value. So you can pass a pointer\nto a's local variable",
    "start": "1113480",
    "end": "1119090"
  },
  {
    "text": "down to any of these\ndescendant function calls, because they all see\nthe same view of a stack.",
    "start": "1119090",
    "end": "1125480"
  },
  {
    "text": "And that's not going\nto be overwritten while these descendant\nfunction calls are proceeding.",
    "start": "1125480",
    "end": "1131360"
  },
  {
    "text": "But if you pass it the\nother way, then potentially the variable that\nyou had a pointer to",
    "start": "1131360",
    "end": "1136370"
  },
  {
    "text": "is going to be overwritten. So here's one question.",
    "start": "1136370",
    "end": "1142640"
  },
  {
    "text": "If you want to pass memory from\na child back to the parent, where would you allocate it? ",
    "start": "1142640",
    "end": "1151380"
  },
  {
    "text": "So you can allocate\nit on the parent. What's another option?",
    "start": "1151380",
    "end": "1156510"
  },
  {
    "text": "Yes? Yes, so another way to do this\nis to allocate it on the heap.",
    "start": "1156510",
    "end": "1161643"
  },
  {
    "text": "If you allocate it on\nthe heap, even after you return from the function\ncall, that memory is going to persist.",
    "start": "1161643",
    "end": "1167549"
  },
  {
    "text": "You can also allocate it in the\nparent's stack, if you want. In fact, some programs\nare written that way.",
    "start": "1167550",
    "end": "1174720"
  },
  {
    "text": "And one of the reasons why\nmany c functions require",
    "start": "1174720",
    "end": "1179850"
  },
  {
    "text": "you to pass in memory to\nthe function where it's going to store the\nreturn value is",
    "start": "1179850",
    "end": "1186630"
  },
  {
    "text": "to try to avoid an expensive\nheap allocation in the child. Because if the parent allocates\nthis space to store the result,",
    "start": "1186630",
    "end": "1194260"
  },
  {
    "text": "the child can just\nput whatever it wants to compute in that space. And the parent will see it.",
    "start": "1194260",
    "end": "1200560"
  },
  {
    "text": "So then the responsibility\nis up to the parent to figure out whether it\nwants to allocate the memory",
    "start": "1200560",
    "end": "1207270"
  },
  {
    "text": "on the stack or on the heap. So this is one of\nthe reasons why you'll see many c functions,\nwhere one of the arguments",
    "start": "1207270",
    "end": "1214200"
  },
  {
    "text": "is a memory location where\nthe result should be stored. ",
    "start": "1214200",
    "end": "1223710"
  },
  {
    "text": "OK, so that was the serial case. What happens in parallel?",
    "start": "1223710",
    "end": "1229310"
  },
  {
    "text": "So in parallel, we\nhave what's called a cactus stack where we\ncan support multiple views",
    "start": "1229310",
    "end": "1234570"
  },
  {
    "text": "of the stack in parallel. So let's say we have a program\nwhere it calls function",
    "start": "1234570",
    "end": "1240720"
  },
  {
    "text": "a, and then a spawns b and c. So b and c are going to\nbe running potentially",
    "start": "1240720",
    "end": "1245730"
  },
  {
    "text": "in parallel. And then c spawns d and\ne, which can potentially be running in parallel. So for this program, we could\nhave functions b, d and e all",
    "start": "1245730",
    "end": "1254820"
  },
  {
    "text": "executing in parallel. And a cactus stack\nis going to allow us to have all of\nthese functions",
    "start": "1254820",
    "end": "1261840"
  },
  {
    "text": "see the same view of\nthis stack as they would have if this program\nwere executed serially.",
    "start": "1261840",
    "end": "1271040"
  },
  {
    "text": "And the silk runtime\nsystem supports the cactus stack to make it easy\nfor writing parallel programs.",
    "start": "1271040",
    "end": "1278940"
  },
  {
    "text": "Because now when you're\nwriting programs, you just have to obey the same\nrules for programming in serial",
    "start": "1278940",
    "end": "1285480"
  },
  {
    "text": "c and c++ with\nregards to the stack, and then you'll still get\nthe intended behavior.",
    "start": "1285480",
    "end": "1291830"
  },
  {
    "text": " And it turns out that there's\nno copying of the stacks here.",
    "start": "1291830",
    "end": "1300059"
  },
  {
    "text": "So all of these\ndifferent views are seeing the same virtual\nmemory addresses for a.",
    "start": "1300060",
    "end": "1308580"
  },
  {
    "text": "But now there is\nan issue of how do we implement this cactus stack?",
    "start": "1308580",
    "end": "1314250"
  },
  {
    "text": "Because in the serial case, we\ncould have these later stacks overwriting the earlier stacks.",
    "start": "1314250",
    "end": "1320400"
  },
  {
    "text": "But in parallel,\nhow can we do this? So does anyone have\nany simple ideas",
    "start": "1320400",
    "end": "1327240"
  },
  {
    "text": "on how we can implement\na cactus stack? Yes? ",
    "start": "1327240",
    "end": "1335370"
  },
  {
    "text": "STUDENT: You could just have\neach child's stack start",
    "start": "1335370",
    "end": "1342367"
  },
  {
    "text": "in like a separate stack,\nor just have references to the [INAUDIBLE].",
    "start": "1342367",
    "end": "1348470"
  },
  {
    "text": "JULIAN SHUN: Yeah,\nso one way to do this is to have each thread\nuse a different stack.",
    "start": "1348470",
    "end": "1355940"
  },
  {
    "text": "And then store pointers to\nthe different stack frames across the different stacks.",
    "start": "1355940",
    "end": "1362480"
  },
  {
    "text": "There's actually another way\nto do this, which is easier. ",
    "start": "1362480",
    "end": "1369830"
  },
  {
    "text": "OK, yes? STUDENT: If the stack\nframes have a maximum--",
    "start": "1369830",
    "end": "1375158"
  },
  {
    "text": "fixed maximum size--\nthen you could put them all in the same stack\nseparated by that fixed size.",
    "start": "1375158",
    "end": "1384430"
  },
  {
    "text": "JULIAN SHUN: Yeah,\nso if the stacks all have a maximum depth,\nthen you could just allocate a whole bunch of\nstacks, which are separated",
    "start": "1384430",
    "end": "1392500"
  },
  {
    "text": "by this maximum depth.  There's actually\nanother way to do this,",
    "start": "1392500",
    "end": "1400510"
  },
  {
    "text": "which is to not use the stack. So yes? STUDENT: Could you memory\nmap it somewhere else--",
    "start": "1400510",
    "end": "1406510"
  },
  {
    "text": "each of the different threads? JULIAN SHUN: Yes, that's\nactually one way to do it. The easiest way to do it is just\nto allocate it off the heap.",
    "start": "1406510",
    "end": "1415000"
  },
  {
    "text": "So instead of allocating\nthe frames on the stack, you just do a heap allocation\nfor each of these stack frames.",
    "start": "1415000",
    "end": "1422690"
  },
  {
    "text": "And then each of\nthese stack frames has a pointer to the\nparent stack frame.",
    "start": "1422690",
    "end": "1429840"
  },
  {
    "text": "So whenever you do\na function call, you're going to do a memory\nallocation from the heap",
    "start": "1429840",
    "end": "1435780"
  },
  {
    "text": "to get a new stack frame. And then when you\nfinish a function, you're going to pop\nsomething off of this stack,",
    "start": "1435780",
    "end": "1441480"
  },
  {
    "text": "and free it back to the heap. In fact, a lot of early systems\nfor parallel programming",
    "start": "1441480",
    "end": "1449040"
  },
  {
    "text": "use this strategy of\nheap-based cactus stacks.",
    "start": "1449040",
    "end": "1455160"
  },
  {
    "text": "Turns out that you can actually\nminimize the performance impact using this strategy if\nyou optimize the code enough.",
    "start": "1455160",
    "end": "1461970"
  },
  {
    "text": "But there is actually\na bigger problem with using a heap-based\ncactus stack, which doesn't",
    "start": "1461970",
    "end": "1467610"
  },
  {
    "text": "have to do with performance. Does anybody have any guesses\nof what this potential issue is?",
    "start": "1467610",
    "end": "1474390"
  },
  {
    "text": " Yeah?",
    "start": "1474390",
    "end": "1479583"
  },
  {
    "text": "STUDENT: It requires you to\nallocate the heap in parallel. JULIAN SHUN: Yeah, so\nlet's assume that we can do parallel heap allocation.",
    "start": "1479583",
    "end": "1485487"
  },
  {
    "text": "And we'll talk about that. So assuming that we\ncan do that correctly, what's the issue\nwith this approach?",
    "start": "1485487",
    "end": "1491600"
  },
  {
    "text": " Yeah? STUDENT: It's that you don't\nknow how big the stack is",
    "start": "1491600",
    "end": "1498071"
  },
  {
    "text": "going to be? JULIAN SHUN: So\nlet's assume that you can get whatever stack frames\nyou need from the heap,",
    "start": "1498071",
    "end": "1503840"
  },
  {
    "text": "so you don't actually need to\nput an upper bound on this. ",
    "start": "1503840",
    "end": "1512550"
  },
  {
    "text": "Yeah? STUDENT: We don't know\nthe maximum depth.  JULIAN SHUN: Yeah.",
    "start": "1512550",
    "end": "1518140"
  },
  {
    "text": "So we don't know\nthe maximum depth, but let's say we\ncan make that work.",
    "start": "1518140",
    "end": "1523283"
  },
  {
    "text": "So you don't actually need\nto know the maximum depth if you're allocating\noff the heap. ",
    "start": "1523283",
    "end": "1532270"
  },
  {
    "text": "Any other guesses? ",
    "start": "1532270",
    "end": "1545590"
  },
  {
    "text": "Yeah? STUDENT: Something\nto do with returning from the stack that is\nallocated on the heap to one of the original stacks.",
    "start": "1545590",
    "end": "1552570"
  },
  {
    "text": "JULIAN SHUN: So let's say we\ncould get that to work as well. ",
    "start": "1552570",
    "end": "1558049"
  },
  {
    "text": "So what happens if I try\nto run some program using this heap-based cactus\nstack with something",
    "start": "1558050",
    "end": "1563380"
  },
  {
    "text": "that's using the regular stack? So let's say I have\nsome old legacy code that was already\ncompiled using",
    "start": "1563380",
    "end": "1570490"
  },
  {
    "start": "1565000",
    "end": "1565000"
  },
  {
    "text": "the traditional linear stack. So there's a problem with\ninteroperability here.",
    "start": "1570490",
    "end": "1576660"
  },
  {
    "text": "Because the traditional\ncode is assuming that, when you make\na function call,",
    "start": "1576660",
    "end": "1581718"
  },
  {
    "text": "the stack frame for\nthe function call is going to appear right\nafter the stack frame for the particular\ncall e function.",
    "start": "1581718",
    "end": "1589380"
  },
  {
    "text": "So if you try to mix code that\nuses the traditional stack as well as this heap-based\ncactus stack approach,",
    "start": "1589380",
    "end": "1597059"
  },
  {
    "text": "then it's not going\nto work well together. One approach is\nthat you can just recompile all your code to use\nthis heap-based cactus stack.",
    "start": "1597060",
    "end": "1608310"
  },
  {
    "text": "Even if you could do that,\neven if all of the source codes were available, there\nare some legacy programs",
    "start": "1608310",
    "end": "1614390"
  },
  {
    "text": "that actually in\nthe source code, they do some manipulations\nwith the stack,",
    "start": "1614390",
    "end": "1619470"
  },
  {
    "text": "because they assume that you're\nusing the traditional stack, and those programs\nwould no longer work if you're using a\nheap-based cactus stack.",
    "start": "1619470",
    "end": "1626700"
  },
  {
    "text": "So the problem is\ninteroperability with legacy code.",
    "start": "1626700",
    "end": "1632070"
  },
  {
    "text": "Turns out that you can\nfix this using an approach called thread local\nmemory mapping. So one of the students\nmentioned memory mapping.",
    "start": "1632070",
    "end": "1640200"
  },
  {
    "text": "But that requires changes\nto the operating system. So it's not general purpose.",
    "start": "1640200",
    "end": "1645810"
  },
  {
    "text": "But the heap-based cactus stack\nturns out to be very simple. And we can prove\nnice bounds about it.",
    "start": "1645810",
    "end": "1653110"
  },
  {
    "text": "So besides the\ninteroperability issue, heap-based cactus stacks\nare pretty good in practice,",
    "start": "1653110",
    "end": "1660120"
  },
  {
    "text": "as well as in theory. So we can actually\nprove a space bound of a cilk program that uses\nthe heap-based cactus stack.",
    "start": "1660120",
    "end": "1669750"
  },
  {
    "start": "1663000",
    "end": "1663000"
  },
  {
    "text": "So let's say s 1 is the\nstack space required by a serial execution\nof a cilk program.",
    "start": "1669750",
    "end": "1676620"
  },
  {
    "text": "Then the stack space\nof p worker execution using a heap-based cactus\nstack is going to be",
    "start": "1676620",
    "end": "1681809"
  },
  {
    "text": "upper bounded by p times s 1. So s p is the space for\na p worker execution,",
    "start": "1681810",
    "end": "1687300"
  },
  {
    "text": "and that's less than or\nequal to p times s 1. To understand how\nthis works, we need",
    "start": "1687300",
    "end": "1694560"
  },
  {
    "text": "to understand a little bit\nabout how the cilks works stealing algorithm works.",
    "start": "1694560",
    "end": "1699820"
  },
  {
    "text": "So in the cilk\nwork-stealing algorithm, whenever you spawn\nsomething of work, or that spawns a new task,\nis going to work on the task",
    "start": "1699820",
    "end": "1708310"
  },
  {
    "text": "that it spawned. So therefore, for any leaf\nin the invocation tree that",
    "start": "1708310",
    "end": "1715110"
  },
  {
    "text": "currently exists,\nthere's always going to be a worker working on it. There's not going to be\nany leaves in the tree",
    "start": "1715110",
    "end": "1720330"
  },
  {
    "text": "where there's no\nworker working on it. Because when a worker spawns\na task, it creates a new leaf.",
    "start": "1720330",
    "end": "1725610"
  },
  {
    "text": "But then it works\nimmediately on that leaf. So here we have a--",
    "start": "1725610",
    "end": "1731970"
  },
  {
    "text": "we have a invocation tree. And for all of the leaves, we\nhave a processor working on it.",
    "start": "1731970",
    "end": "1738570"
  },
  {
    "text": "And with this busy\nleaves property, we can easily show\nthis space bound.",
    "start": "1738570",
    "end": "1745090"
  },
  {
    "text": "So for each one of\nthese processors, the maximum stack\nspace it's using is going to be upper\nbounded by s 1,",
    "start": "1745090",
    "end": "1751830"
  },
  {
    "text": "because that's maximum stock\nspace across a serial execution that executes the whole program.",
    "start": "1751830",
    "end": "1757980"
  },
  {
    "text": "And then since we have\np of these leaves, we just multiply s 1\nby p, and that gives us",
    "start": "1757980",
    "end": "1763050"
  },
  {
    "text": "an upper bound on the overall\nspace used by a p worker execution.",
    "start": "1763050",
    "end": "1768690"
  },
  {
    "text": "This can be a loose upper\nbound, because we're double counting here. There's some part of\nthis memory that we're",
    "start": "1768690",
    "end": "1774780"
  },
  {
    "text": "counting more than once,\nbecause they're shared among the different processors.",
    "start": "1774780",
    "end": "1779850"
  },
  {
    "text": "But that's why we have the\nless than or equal to here. So it's upper bounded\nby p times s 1.",
    "start": "1779850",
    "end": "1787035"
  },
  {
    "text": "So this is one of\nthe nice things about using a heap-based\ncactus stack is that you get this good space bound.",
    "start": "1787035",
    "end": "1794169"
  },
  {
    "text": "Any questions on the\nspace bound here? ",
    "start": "1794170",
    "end": "1803640"
  },
  {
    "text": "So let's try to apply this\ntheorem to a real example. So this is the divide and\nconquer matrix multiplication",
    "start": "1803640",
    "end": "1810679"
  },
  {
    "start": "1809000",
    "end": "1809000"
  },
  {
    "text": "code that we saw in\na previous lecture. ",
    "start": "1810680",
    "end": "1815759"
  },
  {
    "text": "So this is-- in this code, we're\nmaking eight recursive calls",
    "start": "1815760",
    "end": "1821780"
  },
  {
    "text": "to a divide and\nconquer function. Each of size n over 2. And before we make\nany of these calls,",
    "start": "1821780",
    "end": "1828950"
  },
  {
    "text": "we're doing a malloc to\nget some temporary space. And this is of size\norder and squared.",
    "start": "1828950",
    "end": "1835140"
  },
  {
    "text": "And then we free this\ntemporary space at the end. And notice here\nthat the allocations of the temporary matrix\nobey a stack discipline.",
    "start": "1835140",
    "end": "1843290"
  },
  {
    "text": "So we're allocating stuff\nbefore we make recursive calls. And we're freeing it\nafter, or right before we",
    "start": "1843290",
    "end": "1849980"
  },
  {
    "text": "return from the function. So all this stack-- all the allocations\nare nested, and they follow a stack discipline.",
    "start": "1849980",
    "end": "1856430"
  },
  {
    "text": "And it turns out that even\nif you're allocating off the heap, if you follow\na stack discipline, you can still use the space\nbound from the previous slide",
    "start": "1856430",
    "end": "1864140"
  },
  {
    "text": "to upper bound the\np worker space. ",
    "start": "1864140",
    "end": "1869590"
  },
  {
    "text": "OK, so let's try to analyze\nthe space of this code here. So first let's look at\nwhat the work and span are.",
    "start": "1869590",
    "end": "1877300"
  },
  {
    "text": "So this is just\ngoing to be review. What's the work of this divide\nand conquer matrix multiply? So it's n cubed.",
    "start": "1877300",
    "end": "1882740"
  },
  {
    "text": "So it's n cubed because we\nhave eight solve problems",
    "start": "1882740",
    "end": "1889260"
  },
  {
    "text": "of size n over 2. And then we have\nto do linear work to add together the matrices.",
    "start": "1889260",
    "end": "1895320"
  },
  {
    "text": " So our recurrence is\ngoing to be t 1 of n",
    "start": "1895320",
    "end": "1904190"
  },
  {
    "text": "is equal to eight times t 1 of\nn over 2 plus order n squared. And that solves to order n\ncubed if you just pull out",
    "start": "1904190",
    "end": "1911990"
  },
  {
    "text": "your master theorem card. ",
    "start": "1911990",
    "end": "1917150"
  },
  {
    "text": "What about the span?  So what's the recurrence here?",
    "start": "1917150",
    "end": "1922950"
  },
  {
    "text": "Yeah, so the span\nt infinity of n is equal to t\ninfinitive of n over 2",
    "start": "1922950",
    "end": "1929960"
  },
  {
    "text": "plus a span of the addition. And what's the span\nof the addition?",
    "start": "1929960",
    "end": "1935570"
  },
  {
    "text": "STUDENT: [INAUDIBLE] JULIAN SHUN: No,\nlet's assume that we have a parallel addition.",
    "start": "1935570",
    "end": "1940870"
  },
  {
    "text": "We have nested silk four loops. Right, so then the span of\nthat is just going of be log n.",
    "start": "1940870",
    "end": "1949260"
  },
  {
    "text": "Since the span of 1\nsilk four loop is log n and when you nest them, you\njust add together the span.",
    "start": "1949260",
    "end": "1955440"
  },
  {
    "text": "So it's going to\nbe t infinity of n is equal to t infinity of\nn over 2 plus order log n.",
    "start": "1955440",
    "end": "1961330"
  },
  {
    "text": "And what does that solve to?  Yeah, so it's going to solve\nto order log squared n.",
    "start": "1961330",
    "end": "1968529"
  },
  {
    "text": "Again you can pull out\nyour master theorem card, and look at one of\nthe three cases.",
    "start": "1968530",
    "end": "1974910"
  },
  {
    "text": "OK, so now let's\nlook at the space. What's going to be the\nrecurrence for the space?",
    "start": "1974910",
    "end": "1980140"
  },
  {
    "start": "1980140",
    "end": "1985680"
  },
  {
    "text": "Yes. STUDENT: [INAUDIBLE]",
    "start": "1985680",
    "end": "1994980"
  },
  {
    "text": "JULIAN SHUN: The only place\nwe're generating new space is when we call\nthis malloc here.",
    "start": "1994980",
    "end": "2001500"
  },
  {
    "text": "So they're all seeing\nthe same original matrix. ",
    "start": "2001500",
    "end": "2007990"
  },
  {
    "text": "So what would the recurrence be? ",
    "start": "2007990",
    "end": "2016270"
  },
  {
    "text": "Yeah? STUDENT: [INAUDIBLE] JULIAN SHUN: Yeah. ",
    "start": "2016270",
    "end": "2025292"
  },
  {
    "text": "STUDENT: [INAUDIBLE]",
    "start": "2025292",
    "end": "2032080"
  },
  {
    "text": "JULIAN SHUN: So the n\nsquare term is right. Do we actually need eight\nsubproblems of size n over 2?",
    "start": "2032080",
    "end": "2039100"
  },
  {
    "text": "What happens after we finish\none of these sub problems? Are we still going to\nuse the space for it?",
    "start": "2039100",
    "end": "2046279"
  },
  {
    "text": "STUDENT: Yeah, you free the\nmemory after the [INAUDIBLE].. JULIAN SHUN: Right. So you can actually\nreuse the memory.",
    "start": "2046280",
    "end": "2051849"
  },
  {
    "text": "Because you free the\nmemory you allocated after each one of\nthese recursive calls.",
    "start": "2051850",
    "end": "2057638"
  },
  {
    "start": "2057000",
    "end": "2057000"
  },
  {
    "text": "So therefore the recurrence is\njust going to be s of n over 2",
    "start": "2057639",
    "end": "2063429"
  },
  {
    "text": "plus theta n squared. And what does that solve to?",
    "start": "2063429",
    "end": "2070743"
  },
  {
    "text": "STUDENT: [INAUDIBLE] JULIAN SHUN: N squared.",
    "start": "2070744",
    "end": "2077360"
  },
  {
    "text": "Right. So here the n squared\nterm actually dominates.",
    "start": "2077360",
    "end": "2082589"
  },
  {
    "text": "You have a decreasing\ngeometric series. So it's dominated at the root,\nand you get theta of n squared.",
    "start": "2082590",
    "end": "2090090"
  },
  {
    "text": "And therefore by using the busy\nleaves property and the theorem for the space bound, this\ntells us that on p processors,",
    "start": "2090090",
    "end": "2097530"
  },
  {
    "text": "the space is going to be\nbounded by p times n squared. And this is actually pretty good\nsince we have a bound on this.",
    "start": "2097530",
    "end": "2107200"
  },
  {
    "text": "It turns out that we can\nactually prove a stronger bound for this particular example.",
    "start": "2107200",
    "end": "2112770"
  },
  {
    "text": "And I'll walk you through how we\ncan prove this stronger bound. Here's the order p times n\nsquared is already pretty good.",
    "start": "2112770",
    "end": "2118980"
  },
  {
    "text": "But we can actually do better\nif we look internally at how this algorithm is structured.",
    "start": "2118980",
    "end": "2124761"
  },
  {
    "text": " So on each level of recursion,\nwe're branching eight ways.",
    "start": "2124761",
    "end": "2132150"
  },
  {
    "text": "And most of the\nspace is going to be used near the top of\nthis recursion tree.",
    "start": "2132150",
    "end": "2138599"
  },
  {
    "text": "So if I branch as\nmuch as possible near the top of\nmy recursion tree, then that's going to give me\nmy worst case space bound.",
    "start": "2138600",
    "end": "2145560"
  },
  {
    "text": "Because the space is\ndecreasing geometrically as I go down the tree.",
    "start": "2145560",
    "end": "2150690"
  },
  {
    "text": "So I'm going to\nbranch eight ways until I get to some level\nk in the recursion tree",
    "start": "2150690",
    "end": "2155880"
  },
  {
    "text": "where I have p nodes. And at that point, I'm not going\nto branch anymore because I've already used up all p nodes.",
    "start": "2155880",
    "end": "2162119"
  },
  {
    "text": "And that's the number\nof workers I have.",
    "start": "2162120",
    "end": "2167350"
  },
  {
    "text": "So let's say I have this level\nk here, where I have p nodes.",
    "start": "2167350",
    "end": "2175580"
  },
  {
    "text": "So what would be\nthe value of k here? If I branch eight ways\nhow many levels do",
    "start": "2175580",
    "end": "2181380"
  },
  {
    "text": "I have to go until\nI get to p nodes? ",
    "start": "2181380",
    "end": "2188420"
  },
  {
    "start": "2188000",
    "end": "2188000"
  },
  {
    "text": "Yes. STUDENT: It's log base 8 of p. JULIAN SHUN: Yes. It's log base 8 of p.",
    "start": "2188420",
    "end": "2196380"
  },
  {
    "text": "So we have eight,\nthe k, equal p, because we're branching k ways.",
    "start": "2196380",
    "end": "2202310"
  },
  {
    "text": "And then using some\nalgebra, you can get it so that k is equal to log base\n8 of p, which is equal to log",
    "start": "2202310",
    "end": "2208850"
  },
  {
    "text": "base 2 of p divided by 3. And then at this\nlevel k downwards,",
    "start": "2208850",
    "end": "2217430"
  },
  {
    "text": "it's going to decrease\ngeometrically. So the space is going to be\ndominant at this level k.",
    "start": "2217430",
    "end": "2223550"
  },
  {
    "text": "So the space decreases\ngeometrically as you go down from level k, and\nalso as you go up from level k.",
    "start": "2223550",
    "end": "2231410"
  },
  {
    "text": "So therefore we can just look at\nwhat the space is at this level k here.",
    "start": "2231410",
    "end": "2237950"
  },
  {
    "text": "So the space is going to be\np times the size of each one",
    "start": "2237950",
    "end": "2243200"
  },
  {
    "text": "of these nodes squared. And the size of each\none of these nodes is going to be n over 2 to\nthe log base 2 of p over 3.",
    "start": "2243200",
    "end": "2251840"
  },
  {
    "text": "And then we square that\nbecause we're using n squared temporary space. So if you solve that, that gives\nyou p to the one-third times n",
    "start": "2251840",
    "end": "2260839"
  },
  {
    "text": "squared, which is better\nthan the upper bound we saw earlier of order\np times n squared.",
    "start": "2260840",
    "end": "2268430"
  },
  {
    "text": "So you can work out the\ndetails for this example. Not all the details are\nshown on this slide.",
    "start": "2268430",
    "end": "2274970"
  },
  {
    "text": "You need to show that the\nlevel k here actually dominates",
    "start": "2274970",
    "end": "2281290"
  },
  {
    "text": "all the other levels\nin the recursion tree. But in general, if you know what\nthe structure of the algorithm,",
    "start": "2281290",
    "end": "2287150"
  },
  {
    "text": "is you can potentially prove a\nstronger space bound than just applying the general theorem we\nshowed on the previous slide.",
    "start": "2287150",
    "end": "2293420"
  },
  {
    "text": " So any questions on this? ",
    "start": "2293420",
    "end": "2310630"
  },
  {
    "text": "OK, so as I said before, the\nproblem with heap-based linkage is that parallel functions\nfail to interoperate",
    "start": "2310630",
    "end": "2317440"
  },
  {
    "text": "with legacy and third-party\nserial binaries. Yes, was there a question? STUDENT: I actually\ndo have a question.",
    "start": "2317440",
    "end": "2323835"
  },
  {
    "text": "JULIAN SHUN: Yes. STUDENT: [INAUDIBLE]",
    "start": "2323835",
    "end": "2331130"
  },
  {
    "text": "JULIAN SHUN: Yes. STUDENT: How do we know\nthat the workers don't split",
    "start": "2331130",
    "end": "2337160"
  },
  {
    "text": "along the path of the\n[INAUDIBLE] instead of across",
    "start": "2337160",
    "end": "2343400"
  },
  {
    "text": "or horizontal. JULIAN SHUN: Yes. So you don't actually know that. But this turns out\nto be the worst case. So if it branches any\nother way, the space",
    "start": "2343400",
    "end": "2350450"
  },
  {
    "text": "is just going to be lower. So you have to argue that this\nis going to be the worst case,",
    "start": "2350450",
    "end": "2356480"
  },
  {
    "text": "and it's going to be-- intuitively it's the\nworst case, because you're using most of the memory near\nthe root of the recursion tree.",
    "start": "2356480",
    "end": "2363920"
  },
  {
    "text": "So if you can get all p nodes as\nclose as possible to the root, that's going to make your\nspace as high as possible.",
    "start": "2363920",
    "end": "2371820"
  },
  {
    "text": "It's a good question.  So parallel functions\nfail to interoperate",
    "start": "2371820",
    "end": "2378970"
  },
  {
    "start": "2375000",
    "end": "2375000"
  },
  {
    "text": "with legacy and third-party\nserial binaries. Even if you can recompile\nall of this code, which",
    "start": "2378970",
    "end": "2384520"
  },
  {
    "text": "isn't always\nnecessarily the case, you can still have\nissues if the legacy code",
    "start": "2384520",
    "end": "2389560"
  },
  {
    "text": "is taking advantage of the\ntraditional linear stack inside the source code.",
    "start": "2389560",
    "end": "2395230"
  },
  {
    "text": "So our implementation of\ncilk uses a less space efficient strategy that is\ninteroperable with legacy code.",
    "start": "2395230",
    "end": "2404370"
  },
  {
    "text": "And it uses a pool of\nlinear stacks instead of a heap-based strategy.",
    "start": "2404370",
    "end": "2409880"
  },
  {
    "text": "So we're going to maintain a\npool of linear stacks lying around. There's going to be more\nthan p stacks lying around.",
    "start": "2409880",
    "end": "2417340"
  },
  {
    "text": "And whenever a worker\ntries to steal something, it's going to try to\nacquire one of these tasks",
    "start": "2417340",
    "end": "2423130"
  },
  {
    "text": "from this pool of linear tasks. And when it's done, it\nwill return it back.",
    "start": "2423130",
    "end": "2428380"
  },
  {
    "text": "But when it finds\nthat there's no more linear stacks in\nthis pool, then it's not going to steal anymore.",
    "start": "2428380",
    "end": "2433599"
  },
  {
    "text": "So this is still going to\npreserve the space bound, as long as the number of stocks\nis a constant times the number",
    "start": "2433600",
    "end": "2440050"
  },
  {
    "text": "of processors. But it will affect\nthe time bounds of the work-stealing algorithm. Because now when\na worker is idle,",
    "start": "2440050",
    "end": "2446620"
  },
  {
    "text": "it might not necessarily\nhave the chance to steal if there are no\nmore stacks lying around.",
    "start": "2446620",
    "end": "2452770"
  },
  {
    "text": "This strategy doesn't\nrequire any changes to the operating system. There is a way where you\ncan preserve the space",
    "start": "2452770",
    "end": "2459370"
  },
  {
    "text": "and the time bounds using\nthread local memory mapping. But this does require changes\nto the operating system.",
    "start": "2459370",
    "end": "2467470"
  },
  {
    "text": "So our implementation of cilk\nuses a pool of linear stacks, and it's based on the\nIntel implementation.",
    "start": "2467470",
    "end": "2474845"
  },
  {
    "text": " OK. ",
    "start": "2474845",
    "end": "2481520"
  },
  {
    "text": "All right, so we\ntalked about stacks, and that we just reduce the\nproblem to heap allocation.",
    "start": "2481520",
    "end": "2487170"
  },
  {
    "text": "So now we have to\ntalk about heaps. So let's review some\nbasic properties of heap-storage allocators.",
    "start": "2487170",
    "end": "2496250"
  },
  {
    "start": "2495000",
    "end": "2495000"
  },
  {
    "text": "So here's a definition. The allocator\nspeed is the number of allocations and d\nallocations per second",
    "start": "2496250",
    "end": "2502400"
  },
  {
    "text": "that the allocator can sustain. ",
    "start": "2502400",
    "end": "2507813"
  },
  {
    "text": "And here's a question. Is it more important to\nmaximize the allocator speed for large blocks\nor small blocks?",
    "start": "2507813",
    "end": "2513440"
  },
  {
    "start": "2513440",
    "end": "2521359"
  },
  {
    "text": "Yeah? STUDENT: Small blocks? JULIAN SHUN: So small blocks. Here's another question.",
    "start": "2521360",
    "end": "2527440"
  },
  {
    "text": "Why?  Yes?",
    "start": "2527440",
    "end": "2532526"
  },
  {
    "text": "STUDENT: So you're going to\nbe doing a lot of [INAUDIBLE].. JULIAN SHUN: Yes,\nso one answer is",
    "start": "2532526",
    "end": "2538299"
  },
  {
    "text": "that you're going to be\ndoing a lot more allocations and deallocations of small\nblocks than large blocks.",
    "start": "2538300",
    "end": "2546730"
  },
  {
    "text": "There's actually a\nmore fundamental reason why it's more important to\noptimize for small blocks.",
    "start": "2546730",
    "end": "2552760"
  },
  {
    "text": "So anybody? Yeah? STUDENT: [INAUDIBLE]\nbasically not being",
    "start": "2552760",
    "end": "2560970"
  },
  {
    "text": "able to make use of pages. JULIAN SHUN: Yeah, so\nthat's another reason for small blocks.",
    "start": "2560970",
    "end": "2566490"
  },
  {
    "text": "It's more likely that it\nwill lead to fragmentation if you don't optimize\nfor small blocks.",
    "start": "2566490",
    "end": "2572579"
  },
  {
    "text": "What's another reason? Yes. STUDENT: Wouldn't\nit just take longer to allocate larger\nblocks anyway?",
    "start": "2572580",
    "end": "2577619"
  },
  {
    "text": "So the overhead is going to\nbe more noticeable if you have a big overhead when you\nallocate small blocks",
    "start": "2577620",
    "end": "2584790"
  },
  {
    "text": "versus large blocks? JULIAN SHUN: Yeah. So the reason-- the main reason\nis that when you're allocating",
    "start": "2584790",
    "end": "2592319"
  },
  {
    "text": "a large-- when you're allocating\na block, a user program is typically going to write\nto all the bytes in the block.",
    "start": "2592320",
    "end": "2598805"
  },
  {
    "text": "And therefore,\nfor a large block, it takes so much time to\nwrite that the allocator time has little effect on\nthe overall running time.",
    "start": "2598805",
    "end": "2606600"
  },
  {
    "text": "Whereas if a program\nallocates many small blocks, the amount of\nwork-- useful work--",
    "start": "2606600",
    "end": "2611970"
  },
  {
    "text": "it's actually doing on\nthe block is going to be-- it can be comparable to the\noverhead for the allocation.",
    "start": "2611970",
    "end": "2620400"
  },
  {
    "text": "And therefore, all of\nthe allocation overhead can add up to a significant\namount for small blocks.",
    "start": "2620400",
    "end": "2627630"
  },
  {
    "text": "So essentially for\nlarge blocks, you can amortize away the overheads\nfor storage allocation, whereas for small, small\nblocks, it's harder to do that.",
    "start": "2627630",
    "end": "2634890"
  },
  {
    "text": "Therefore, it's important to\noptimize for small blocks. ",
    "start": "2634890",
    "end": "2641539"
  },
  {
    "text": "Here's another definition. So the user footprint\nis the maximum over time of the\nnumber u of bytes",
    "start": "2641540",
    "end": "2648770"
  },
  {
    "text": "in use by the user program. And these are the bytes that\nare allocated and not freed.",
    "start": "2648770",
    "end": "2654710"
  },
  {
    "text": "And this is measuring\nthe peak memory usage. It's not necessarily equal\nto the sum of the sizes",
    "start": "2654710",
    "end": "2660349"
  },
  {
    "text": "that you have allocated\nso far, because you might have reused some of that. So the user footprint is the\npeak memory usage and number",
    "start": "2660350",
    "end": "2668480"
  },
  {
    "text": "of bytes. And the allocator\nfootprint is the maximum over time of the\nnumber of a bytes",
    "start": "2668480",
    "end": "2673610"
  },
  {
    "text": "that the memory\nprovided to the locator by the operating system. And the reason why the allocator\nfootprint could be larger",
    "start": "2673610",
    "end": "2680738"
  },
  {
    "text": "than the user\nfootprint, is that when you ask the OS for some\nmemory, it could give you more than what you asked for.",
    "start": "2680738",
    "end": "2686000"
  },
  {
    "text": " And similarly, if you ask malloc\nfor some amount of memory,",
    "start": "2686000",
    "end": "2691579"
  },
  {
    "text": "it can also give you more\nthan what you asked for. And the fragmentation is\ndefined to be a divided by u.",
    "start": "2691580",
    "end": "2699200"
  },
  {
    "text": "And a program with\nlow fragmentation will keep this ratio\nas low as possible, so keep the allocator\nfootprint as close as",
    "start": "2699200",
    "end": "2706910"
  },
  {
    "text": "possible to the user footprint. And in the best case, this\nratio is going to be one. So you're using\nall of the memory",
    "start": "2706910",
    "end": "2712490"
  },
  {
    "text": "that the operating\nsystem allocated. ",
    "start": "2712490",
    "end": "2718050"
  },
  {
    "text": "One remark is that the\nallocator footprint a usually gross monotonically\nfor many allocators.",
    "start": "2718050",
    "end": "2725590"
  },
  {
    "text": "So it turns out\nthat many allocators do m maps to get more memory.",
    "start": "2725590",
    "end": "2730950"
  },
  {
    "text": "But they don't always free\nthis memory back to the OS. And you can actually free\nmemory using something called",
    "start": "2730950",
    "end": "2737640"
  },
  {
    "text": "m unmap, which is the\nopposite of m map, to give memory back to the OS. But this turns out to\nbe pretty expensive.",
    "start": "2737640",
    "end": "2745380"
  },
  {
    "text": "In modern operating systems,\ntheir implementation is not very efficient. So many allocators\ndon't use m unmap.",
    "start": "2745380",
    "end": "2754020"
  },
  {
    "text": "You can also use\nsomething called m advise. And what m advise does is it\ntells the operating system",
    "start": "2754020",
    "end": "2760440"
  },
  {
    "text": "that you're not going to\nbe using this page anymore but to keep it around\nin virtual memory.",
    "start": "2760440",
    "end": "2765940"
  },
  {
    "text": "So this has less\noverhead, because it doesn't have to clear this\nentry from the page table. It just has to mark\nthat the program isn't",
    "start": "2765940",
    "end": "2773280"
  },
  {
    "text": "using this page anymore. So some allocators use m\nadvise with the option,",
    "start": "2773280",
    "end": "2778289"
  },
  {
    "text": "don't need, to free memory. But a is usually still growing\nmonotonically over time,",
    "start": "2778290",
    "end": "2786900"
  },
  {
    "text": "because allocators\ndon't necessarily free all of the things back\nto the OS that they allocated.",
    "start": "2786900",
    "end": "2792139"
  },
  {
    "text": " Here's a theorem that we proved\nin last week's lecture, which",
    "start": "2792139",
    "end": "2800519"
  },
  {
    "text": "says that the fragmentation\nfor binned free list is order log base 2 of\nu, or just order log u.",
    "start": "2800520",
    "end": "2809340"
  },
  {
    "text": "And the reason for\nthis is that you're can have log-based 2 of u bins.",
    "start": "2809340",
    "end": "2815039"
  },
  {
    "text": "And for each bin\nit can basically contain u bytes of storage.",
    "start": "2815040",
    "end": "2822420"
  },
  {
    "text": "So overall you can use-- overall, you could\nhave allocated u times log u storage, and\nonly be using u of those bytes.",
    "start": "2822420",
    "end": "2831510"
  },
  {
    "text": "So therefore the\nfragmentation is order log u. ",
    "start": "2831510",
    "end": "2839440"
  },
  {
    "text": "Another thing to note is that\nmodern 64-bit processors only",
    "start": "2839440",
    "end": "2844480"
  },
  {
    "text": "provide about 2 to 48 bytes\nof virtual address space. So this is sort of news\nbecause you would probably",
    "start": "2844480",
    "end": "2852070"
  },
  {
    "text": "expect that, for a\n64-bit processor, you have to the 64 bytes\nof virtual address space.",
    "start": "2852070",
    "end": "2859160"
  },
  {
    "text": "But that turns out\nnot to be the case. So they only support\nto the 48 bytes. And that turns out to be\nenough for all of the programs",
    "start": "2859160",
    "end": "2866470"
  },
  {
    "text": "that you would want to write. And that's also going to be much\nmore than the physical memory",
    "start": "2866470",
    "end": "2872860"
  },
  {
    "text": "you would have on a machine. So nowadays, you\ncan get a big server with a terabyte of memory,\nor to the 40th bytes",
    "start": "2872860",
    "end": "2879910"
  },
  {
    "text": "of physical memory,\nwhich is still much lower than the number of\nbytes in the virtual address",
    "start": "2879910",
    "end": "2885440"
  },
  {
    "text": "space.  Any questions?",
    "start": "2885440",
    "end": "2891004"
  },
  {
    "start": "2891004",
    "end": "2898920"
  },
  {
    "text": "OK, so here's some\nmore definitions. So the space overhead\nof an allocator",
    "start": "2898920",
    "end": "2904530"
  },
  {
    "text": "is a space used for bookkeeping. So you could store--",
    "start": "2904530",
    "end": "2909750"
  },
  {
    "text": "perhaps you could store\nheaders with the blocks that you allocate to\nkeep track of the size and other information.",
    "start": "2909750",
    "end": "2915630"
  },
  {
    "text": "And that would contribute\nto the space overhead",
    "start": "2915630",
    "end": "2920869"
  },
  {
    "text": "Internal fragmentation\nis a waste due to allocating larger\nblocks in the user request.",
    "start": "2920870",
    "end": "2927720"
  },
  {
    "text": "So you can get\ninternal fragmentation if, when you call\nmalloc, you get back a block that's actually larger\nthan what the user requested.",
    "start": "2927720",
    "end": "2935180"
  },
  {
    "text": "We saw on the bin\nfree list algorithm, we're rounding up to the\nnearest power of 2's. If you allocate\nnine bytes, you'll",
    "start": "2935180",
    "end": "2941359"
  },
  {
    "text": "actually get back 16 bytes in\nour binned-free list algorithm from last lecture. So that contributes to\ninternal fragmentation.",
    "start": "2941360",
    "end": "2950690"
  },
  {
    "text": "It turns out that not\nall binned-free list implementations use powers of 2. So some of them use other\npowers that are smaller than 2",
    "start": "2950690",
    "end": "2958220"
  },
  {
    "text": "in order to reduce the\ninternal fragmentation.",
    "start": "2958220",
    "end": "2963525"
  },
  {
    "start": "2963000",
    "end": "2963000"
  },
  {
    "text": "Then there's an\nexternal fragmentation, which is the waste due to\nthe inability to use storage because it's not contiguous.",
    "start": "2963525",
    "end": "2970950"
  },
  {
    "text": "So for example, if I allocated\na whole bunch of one byte things consecutively in memory, then\nI freed every other byte.",
    "start": "2970950",
    "end": "2978710"
  },
  {
    "text": "And now I want to\nallocate a 2-byte thing, I don't actually have contiguous\nmammary to satisfy that",
    "start": "2978710",
    "end": "2985460"
  },
  {
    "text": "request, because all\nof my free memory-- all of my free bytes\nare in one-bite chunks,",
    "start": "2985460",
    "end": "2990860"
  },
  {
    "text": "and they're not\nnext to each other. So this is one example of how\nexternal fragmentation can",
    "start": "2990860",
    "end": "2996320"
  },
  {
    "text": "happen after you allocate\nstuff and free stuff. Then there's blow up.",
    "start": "2996320",
    "end": "3003480"
  },
  {
    "text": "And this is for a\nparallel locator. The additional space beyond what\na serial locator would require.",
    "start": "3003480",
    "end": "3011470"
  },
  {
    "text": "So if a serial locator\nrequires s space, and a parallel allocator\nrequires t space,",
    "start": "3011470",
    "end": "3019690"
  },
  {
    "text": "then it's just going\nto be t over s. That's the blow up. ",
    "start": "3019690",
    "end": "3026200"
  },
  {
    "text": "OK, so now let's look at\nsome parallel heap allocation strategies. ",
    "start": "3026200",
    "end": "3032859"
  },
  {
    "text": "So the first strategy\nis to use a global heap. And this is how the\ndefault c allocator works.",
    "start": "3032860",
    "end": "3040819"
  },
  {
    "start": "3034000",
    "end": "3034000"
  },
  {
    "text": "So if you just use a default\nc allocator out of the box, this is how it's implemented.",
    "start": "3040820",
    "end": "3046380"
  },
  {
    "text": "It uses a global heap\nwhere all the accesses to this global heap\nare protected by mutex.",
    "start": "3046380",
    "end": "3053940"
  },
  {
    "text": "You can also use lock-free\nsynchronization primitives to implement this. We'll actually talk about\nsome of these synchronization",
    "start": "3053940",
    "end": "3060599"
  },
  {
    "text": "primitives later\non in the semester. And this is done to\npreserve atomicity because you can have\nmultiple threads trying",
    "start": "3060600",
    "end": "3066660"
  },
  {
    "text": "to access the global\nheap at the same time. And you need to ensure that\nraces are handled correctly.",
    "start": "3066660",
    "end": "3073260"
  },
  {
    "text": " So what's the blow\nup for this strategy?",
    "start": "3073260",
    "end": "3080714"
  },
  {
    "text": " How much more space am I using\nthan just a serial allocator?",
    "start": "3080715",
    "end": "3090250"
  },
  {
    "text": "Yeah. STUDENT: [INAUDIBLE] JULIAN SHUN: Yeah, so\nthe blow up is one. Because I'm not actually\nusing any more space",
    "start": "3090250",
    "end": "3097627"
  },
  {
    "text": "than the serial allocator. Since I'm just maintaining\none global heap, and everybody is going to that heap to do\nallocations and deallocations.",
    "start": "3097627",
    "end": "3104710"
  },
  {
    "text": " But what's the potential\nissue with this approach?",
    "start": "3104710",
    "end": "3109900"
  },
  {
    "start": "3109900",
    "end": "3116869"
  },
  {
    "text": "Yeah? STUDENT: Performance hit\nfor that block coordination. JULIAN SHUN: Yeah,\nso you're going",
    "start": "3116870",
    "end": "3123109"
  },
  {
    "text": "to take a performance hit for\ntrying to acquire this lock.",
    "start": "3123110",
    "end": "3128450"
  },
  {
    "text": "So basically every time you do\na allocation or deallocation, you have to acquire this lock.",
    "start": "3128450",
    "end": "3133970"
  },
  {
    "text": "And this is pretty\nslow, and it gets slower as you increase\nthe number of processors.",
    "start": "3133970",
    "end": "3140360"
  },
  {
    "text": "Roughly speaking,\nacquiring a lock to perform is similar to an\nL2 cache access.",
    "start": "3140360",
    "end": "3146750"
  },
  {
    "text": "And if you just run\na serial allocator, many of your requests are\ngoing to be satisfied just",
    "start": "3146750",
    "end": "3152270"
  },
  {
    "text": "by going into the L1 cache. Because you're going\nto be allocating things that you recently\nfreed, and those things",
    "start": "3152270",
    "end": "3158300"
  },
  {
    "text": "are going to be\nresiding in L1 cache. But here, before you\neven get started, you have to grab a lock.",
    "start": "3158300",
    "end": "3164300"
  },
  {
    "text": "And you have to pay\na performance hit similar to an L2 cache access. So that's bad.",
    "start": "3164300",
    "end": "3170600"
  },
  {
    "text": "And it gets worse\nas you increase the number of processors.",
    "start": "3170600",
    "end": "3175790"
  },
  {
    "text": "So the contention\nincreases as you increase the number of threads. And then you can't--",
    "start": "3175790",
    "end": "3181280"
  },
  {
    "text": "you're not going to be able\nto get good scalability. ",
    "start": "3181280",
    "end": "3186450"
  },
  {
    "start": "3186000",
    "end": "3186000"
  },
  {
    "text": "So ideally, as the number of\nthreads or processors grows, the time to perform an\nallocation or deallocation",
    "start": "3186450",
    "end": "3193320"
  },
  {
    "text": "shouldn't increase. But in fact, it does. And the most common reason\nfor loss of scalability",
    "start": "3193320",
    "end": "3199589"
  },
  {
    "text": "is lock contention. So here all of the\nprocesses are trying",
    "start": "3199590",
    "end": "3205020"
  },
  {
    "text": "to acquire the same lock, which\nis the same memory address. And if you recall from\nthe caching lecture,",
    "start": "3205020",
    "end": "3213518"
  },
  {
    "text": "or the multicore\nprogramming lecture, every time you acquire\na memory location, you have to bring that cache\nline into your own cache,",
    "start": "3213518",
    "end": "3220560"
  },
  {
    "text": "and then invalidate\nthe same cache line in other processors' caches. So if all the processors\nare doing this,",
    "start": "3220560",
    "end": "3226260"
  },
  {
    "text": "then this cache line is\ngoing to be bouncing around among all of the\nprocessors' caches, and this could lead to\nvery bad performance.",
    "start": "3226260",
    "end": "3234474"
  },
  {
    "text": "So here's a question. Is lock contention more of\na problem for large blocks or small blocks? ",
    "start": "3234475",
    "end": "3246700"
  },
  {
    "text": "Yes. STUDENT: So small blocks.",
    "start": "3246700",
    "end": "3251779"
  },
  {
    "text": "JULIAN SHUN: Here's\nanother question. Why? Yes. STUDENT: Because by\nthe time it takes",
    "start": "3251780",
    "end": "3258140"
  },
  {
    "text": "to finish using the\nsmall block, then the allocator is usually small.",
    "start": "3258140",
    "end": "3263330"
  },
  {
    "text": "So you do many allocations\nand deallocations, which means you have to go\nthrough the lock multiple times. JULIAN SHUN: Yeah.",
    "start": "3263330",
    "end": "3269020"
  },
  {
    "text": "So one of the\nreasons is that when you're doing small\nallocations, that",
    "start": "3269020",
    "end": "3275950"
  },
  {
    "text": "means that your request rate\nis going to be pretty high. And your processors are going\nto be spending a lot of time",
    "start": "3275950",
    "end": "3281950"
  },
  {
    "text": "acquiring this lock. And this can exacerbate\nthe lock contention.",
    "start": "3281950",
    "end": "3289210"
  },
  {
    "text": "And another reason is that when\nyou allocate a large block, you're doing a lot of work,\nbecause you have to write--",
    "start": "3289210",
    "end": "3295540"
  },
  {
    "text": "most of the time you're\ngoing to write to all the bytes in that large block. And therefore you can\namortize the overheads",
    "start": "3295540",
    "end": "3302290"
  },
  {
    "text": "of the storage allocator\nacross all of the work that you're doing. Whereas for small\nblocks, in addition to",
    "start": "3302290",
    "end": "3308950"
  },
  {
    "text": "increasing this rate of\nmemory requests, it's also--",
    "start": "3308950",
    "end": "3314260"
  },
  {
    "text": "there's much less work to\namortized to overheads across. ",
    "start": "3314260",
    "end": "3320010"
  },
  {
    "text": "So any questions? ",
    "start": "3320010",
    "end": "3326960"
  },
  {
    "text": "OK, good. All right. So here's another strategy,\nwhich is to use local heaps.",
    "start": "3326960",
    "end": "3333460"
  },
  {
    "start": "3330000",
    "end": "3330000"
  },
  {
    "text": "So each thread is going\nto maintain its own heap. And it's going to allocate\nout of its own heap.",
    "start": "3333460",
    "end": "3341800"
  },
  {
    "text": "And there's no locking\nthat's necessary. So when you allocate something,\nyou get it from your own heap. And when you free something, you\nput it back into your own heap.",
    "start": "3341800",
    "end": "3348770"
  },
  {
    "text": "So there's no\nsynchronization required. So that's a good thing. It's very fast.",
    "start": "3348770",
    "end": "3354579"
  },
  {
    "text": "What's a potential issue\nwith this approach? ",
    "start": "3354580",
    "end": "3364900"
  },
  {
    "text": "Yes. STUDENT: It's using\na lot of extra space. JULIAN SHUN: Yes,\nso this approach, you're going to be using\na lot of extra space.",
    "start": "3364900",
    "end": "3373380"
  },
  {
    "text": "So first of all,\nbecause you have to maintain multiple heaps. And what's one\nphenomenon that you",
    "start": "3373380",
    "end": "3378609"
  },
  {
    "text": "might see if you're\nexecuting a program with this local-heap approach?",
    "start": "3378610",
    "end": "3385250"
  },
  {
    "text": "So it's a space-- could the space potentially\nkeep growing over time?",
    "start": "3385250",
    "end": "3390276"
  },
  {
    "start": "3390276",
    "end": "3396970"
  },
  {
    "text": "Yes. STUDENT: You could\nmaybe like allocate every one process [INAUDIBLE].",
    "start": "3396970",
    "end": "3402720"
  },
  {
    "text": "JULIAN SHUN: Yeah. Yeah, so you could actually\nhave an unbounded blow up. Because if you do all of\nthe allocations in one heap,",
    "start": "3402720",
    "end": "3409820"
  },
  {
    "text": "and you free everything\nin another heap, then whenever the first\nheap does an allocation,",
    "start": "3409820",
    "end": "3415160"
  },
  {
    "text": "there's actually free space\nsitting around in another heap. But it's just going to grab\nmore memory from the operating system.",
    "start": "3415160",
    "end": "3420310"
  },
  {
    "text": "So you're blow up\ncan be unbounded. And this phenomenon, it's\nwhat's called memory drift.",
    "start": "3420310",
    "end": "3425840"
  },
  {
    "text": "So blocks allocated\nby one thread are freed by another thread. And if you run your\nprogram for long enough,",
    "start": "3425840",
    "end": "3433350"
  },
  {
    "text": "your memory consumption\ncan keep increasing. And this is sort of\nlike a memory leak. So you might see that if you\nhave a memory drift problem,",
    "start": "3433350",
    "end": "3440540"
  },
  {
    "text": "your program running\non multiple processors could run out of\nmemory eventually. Whereas if you just run\nit on a single core,",
    "start": "3440540",
    "end": "3449000"
  },
  {
    "text": "it won't run out of memory. And here it's because the\nallocator isn't smart enough to reuse things in other heaps.",
    "start": "3449000",
    "end": "3455990"
  },
  {
    "text": " So what's another strategy you\ncan use to try to fix this?",
    "start": "3455990",
    "end": "3462380"
  },
  {
    "text": " Yes? STUDENT: [INAUDIBLE]",
    "start": "3462380",
    "end": "3469868"
  },
  {
    "text": "JULIAN SHUN: Sorry, can\nyou repeat your question? STUDENT: [INAUDIBLE]",
    "start": "3469868",
    "end": "3477017"
  },
  {
    "text": "JULIAN SHUN: Because\nif you keep allocating from one thread, if you\ndo all of your allocations",
    "start": "3477018",
    "end": "3482230"
  },
  {
    "text": "in one thread, and do\nall of your deallocations on another thread,\nevery time you allocate from the\nfirst thread, there's",
    "start": "3482230",
    "end": "3488319"
  },
  {
    "text": "actually memory sitting\naround in the system. But the first thread isn't\ngoing to see it, because it only",
    "start": "3488320",
    "end": "3493510"
  },
  {
    "text": "sees its own heap. And it's just going\nto keep grabbing more memory from the OS. And then the second\nthread actually",
    "start": "3493510",
    "end": "3499747"
  },
  {
    "text": "has this extra memory\nsitting around. But it's not using it. Because it's only\ndoing the freeze. It's not doing allocate.",
    "start": "3499748",
    "end": "3505180"
  },
  {
    "text": "And if we recall the\ndefinition of blow up is, how much more\nspace you're using compared to a serial\nexecution of a program.",
    "start": "3505180",
    "end": "3511810"
  },
  {
    "text": "If you executed this\nprogram on a single core, you would only have a single\nheap that does the allocations",
    "start": "3511810",
    "end": "3519400"
  },
  {
    "text": "and frees. So you're not going to-- your memory isn't\ngoing to blow up. It's just going to be\nconstant over time.",
    "start": "3519400",
    "end": "3525250"
  },
  {
    "text": "Whereas if you use two\nthreads to execute this, the memory could just\nkeep growing over time.",
    "start": "3525250",
    "end": "3532030"
  },
  {
    "text": "Yes? STUDENT: [INAUDIBLE]",
    "start": "3532030",
    "end": "3540089"
  },
  {
    "text": "JULIAN SHUN: So, it just-- so if you remember the\nbinned-free list approach,",
    "start": "3540090",
    "end": "3547540"
  },
  {
    "text": "let's say we're using that. Then all you have to\ndo is set some pointers in your binned-free\nlists data structure,",
    "start": "3547540",
    "end": "3554292"
  },
  {
    "text": "as well as the block\nthat you're freeing, so that it appears in\none of the linked lists. So you can do that even if\nsome other processor allocated",
    "start": "3554292",
    "end": "3561740"
  },
  {
    "text": "that block.  OK, so what what's another\nstrategy that can avoid",
    "start": "3561740",
    "end": "3569550"
  },
  {
    "text": "this issue of memory drift? Yes? STUDENT: Periodically shuffle\nthe free memory that's",
    "start": "3569550",
    "end": "3574800"
  },
  {
    "text": "being used on different heaps. JULIAN SHUN: Yeah. So that's a good idea. You could periodically\nrebalance the memory.",
    "start": "3574800",
    "end": "3581579"
  },
  {
    "text": "What's a simpler approach\nto solve this problem? ",
    "start": "3581580",
    "end": "3588760"
  },
  {
    "text": "Yes? STUDENT: Make it all know\nall of the free memory? JULIAN SHUN: Sorry,\ncould you repeat that?",
    "start": "3588760",
    "end": "3595140"
  },
  {
    "text": "STUDENT: Make them all know\nall of the free memory?",
    "start": "3595140",
    "end": "3601312"
  },
  {
    "text": "JULIAN SHUN: Yes. So you could have\nall of the processors know all the free memory. And then every time\nit grabs something,",
    "start": "3601312",
    "end": "3608059"
  },
  {
    "text": "it looks in all the other heaps. That does require a lot of\nsynchronization overhead. Might not perform that well.",
    "start": "3608060",
    "end": "3614780"
  },
  {
    "text": "What's an easier way\nto solve this problem? Yes.",
    "start": "3614780",
    "end": "3620639"
  },
  {
    "text": "STUDENT: [INAUDIBLE] JULIAN SHUN: So you could\nrestructure your program",
    "start": "3620639",
    "end": "3626920"
  },
  {
    "text": "so that the same thread\ndoes the allocation and frees for the\nsame memory block.",
    "start": "3626920",
    "end": "3632060"
  },
  {
    "text": "But what if you didn't want\nto restructure your program? How can you change\nthe allocator?",
    "start": "3632060",
    "end": "3638890"
  },
  {
    "text": "So we want the\nbehavior that you said, but we don't want to\nchange our program. Yes.",
    "start": "3638890",
    "end": "3643930"
  },
  {
    "text": "STUDENT: You could have\na single free list that's protected by synchronization. JULIAN SHUN: Yeah, so you\ncould have a single free list.",
    "start": "3643930",
    "end": "3649790"
  },
  {
    "text": "But that gets back\nto the first strategy of having a global heap. And then you have high\nsynchronization overheads.",
    "start": "3649790",
    "end": "3658030"
  },
  {
    "text": "Yes. STUDENT: You could have\nthe free map to the thread",
    "start": "3658030",
    "end": "3663320"
  },
  {
    "text": "that it came from or for the\npointer that corresponds to--",
    "start": "3663320",
    "end": "3671752"
  },
  {
    "text": "that allocated it. JULIAN SHUN: So you're\nsaying free back to the thread that allocated it?",
    "start": "3671752",
    "end": "3679580"
  },
  {
    "text": "Yes, so that that's\nexactly right. So here each object,\nwhen you allocate it,",
    "start": "3679580",
    "end": "3685080"
  },
  {
    "text": "it's labeled with an owner. And then whenever\nyou free it, you return it back to the owner.",
    "start": "3685080",
    "end": "3690240"
  },
  {
    "text": "So the objects\nthat are allocated will eventually go back\nto the owner's heap",
    "start": "3690240",
    "end": "3695880"
  },
  {
    "text": "if they're not in use. And they're not going\nto be free lying around in somebody else's heap.",
    "start": "3695880",
    "end": "3702809"
  },
  {
    "text": "The advantage of\nthis approach is that you get fast allocation\nand freeing of local objects.",
    "start": "3702810",
    "end": "3707940"
  },
  {
    "text": "Local objects are objects\nthat you allocated. However, free remote objects\nrequire some synchronization.",
    "start": "3707940",
    "end": "3716400"
  },
  {
    "text": "Because you have to coordinate\nwith the other threads' heap that you're sending the\nmemory object back to.",
    "start": "3716400",
    "end": "3724619"
  },
  {
    "text": "But this synchronization isn't\nas bad as having a global heap, since you only have to talk to\none other thread in this case.",
    "start": "3724620",
    "end": "3733860"
  },
  {
    "text": "You can also bound\nthe blow up by p. So the reason why the blow\nup is upper bounded by p",
    "start": "3733860",
    "end": "3742470"
  },
  {
    "text": "is that, let's say the\nserial allocator uses at most x memory.",
    "start": "3742470",
    "end": "3748350"
  },
  {
    "text": "In this case, each of the\nheaps can use at most x memory, because that's how much the\nserial program would have used.",
    "start": "3748350",
    "end": "3755730"
  },
  {
    "text": "And you have p of these\nheaps, so overall you're using p times x memory. And therefore the ratio\nis upper bounded by p.",
    "start": "3755730",
    "end": "3763650"
  },
  {
    "text": "Yes? STUDENT: [INAUDIBLE]",
    "start": "3763650",
    "end": "3771830"
  },
  {
    "text": "JULIAN SHUN: So when you\nfree an object, it goes-- if you allocated that object,\nit goes back to your own heap.",
    "start": "3771830",
    "end": "3779120"
  },
  {
    "text": "If your heap is\nempty, it's actually going to get more memory\nfrom the operating system. It's not going to take something\nfrom another thread's heap.",
    "start": "3779120",
    "end": "3787220"
  },
  {
    "text": "But the maximum amount of memory\nthat you're going to allocate is going to be\nupper bounded by x.",
    "start": "3787220",
    "end": "3792260"
  },
  {
    "text": "Because the sequential serial\nprogram took that much. STUDENT: [INAUDIBLE]",
    "start": "3792260",
    "end": "3797450"
  },
  {
    "text": "JULIAN SHUN: Yeah. So the upper bound\nfor the blow up is p.",
    "start": "3797450",
    "end": "3803960"
  },
  {
    "text": "Another advantage\nof this approach is that it's resilience-- it has resilience\nto false sharing.",
    "start": "3803960",
    "end": "3809030"
  },
  {
    "text": " So let me just talk a little\nbit about false sharing.",
    "start": "3809030",
    "end": "3815210"
  },
  {
    "text": "So true sharing is\nwhen two processors are trying to access the\nsame memory location.",
    "start": "3815210",
    "end": "3822380"
  },
  {
    "text": "And false sharing is when\nmultiple processors are accessing different\nmemory locations, but those locations happen\nto be on the same cache line.",
    "start": "3822380",
    "end": "3831000"
  },
  {
    "text": "So here's an example. Let's say we have two\nvariables, x and y. And the compiler happens to\nplace x and y on the same cache",
    "start": "3831000",
    "end": "3839180"
  },
  {
    "text": "line. Now, when the first\nprocessor writes to x, it's going to bring this\ncache line into its cache.",
    "start": "3839180",
    "end": "3848869"
  },
  {
    "text": "When the other\nprocessor writes to y, since it's on the\nsame cache line, it's going to bring this\ncache line to y's cache.",
    "start": "3848870",
    "end": "3857120"
  },
  {
    "text": "And then now, the first\nprocessor writes x, it's going to bring\nthis cache line back to the first processor's cache.",
    "start": "3857120",
    "end": "3864080"
  },
  {
    "text": "And then you can keep-- you can see this\nphenomenon keep happening. So here, even though\nthe processors",
    "start": "3864080",
    "end": "3870320"
  },
  {
    "text": "are writing to different\nmemory locations, because they happen to be\non the same cache line,",
    "start": "3870320",
    "end": "3876470"
  },
  {
    "text": "the cache line is going to\nbe bouncing back and forth on the machine between the\ndifferent processors' caches.",
    "start": "3876470",
    "end": "3884270"
  },
  {
    "text": "And this problem gets\nworse if more processors are accessing this cache line. ",
    "start": "3884270",
    "end": "3893040"
  },
  {
    "text": "So in this-- this can\nbe quite hard to debug. Because if you're using\njust variables on the stack,",
    "start": "3893040",
    "end": "3900260"
  },
  {
    "text": "you don't actually\nknow necessarily where the compiler is going to\nplace these memory locations.",
    "start": "3900260",
    "end": "3906120"
  },
  {
    "text": "So the compiler\ncould just happen to place x and y in\nthe same cache block.",
    "start": "3906120",
    "end": "3911420"
  },
  {
    "text": "And then you'll get\nthis performance hit, even though it seems like you're\naccessing different memory locations.",
    "start": "3911420",
    "end": "3918920"
  },
  {
    "text": "If you're using the heap\nfor memory allocation, you have more knowledge. Because if you\nallocate a huge block,",
    "start": "3918920",
    "end": "3925339"
  },
  {
    "text": "you know that all of\nthe memory locations are contiguous in\nphysical memory. So you can just space your--",
    "start": "3925340",
    "end": "3931700"
  },
  {
    "text": "you can space the accesses\nfar enough apart so that different\nprocesses aren't going to touch the same cache line.",
    "start": "3931700",
    "end": "3937910"
  },
  {
    "start": "3937910",
    "end": "3944140"
  },
  {
    "text": "A more general approach\nis that you can actually pad the object. So first, you can\nalign the object",
    "start": "3944140",
    "end": "3950109"
  },
  {
    "text": "on a cache line boundary. And then you pad out the\nremaining memory locations of the objects so that it\nfills up the entire cache line.",
    "start": "3950110",
    "end": "3958450"
  },
  {
    "text": "And now there's only one\nthing on that cache line. But this does lead\nto a waste of space",
    "start": "3958450",
    "end": "3965619"
  },
  {
    "text": "because you have this\nwasted padding here. So program can\ninduce false sharing",
    "start": "3965620",
    "end": "3971470"
  },
  {
    "text": "by having different\nthreads process nearby objects, both on\nthe stack and on the heap.",
    "start": "3971470",
    "end": "3978100"
  },
  {
    "text": "And then an allocator can\nalso induce false sharing in two ways. So it can actively\ninduce false sharing.",
    "start": "3978100",
    "end": "3985330"
  },
  {
    "text": "And this is when the allocator\nsatisfies memory requests from different threads\nusing the same cache block.",
    "start": "3985330",
    "end": "3992110"
  },
  {
    "text": "And it can also\ndo this passively. And this is when the program\npasses objects lying around in the same cache line.",
    "start": "3992110",
    "end": "3998010"
  },
  {
    "text": "So different threads,\nand then the allocator reuses the object\nstorage after the objects",
    "start": "3998010",
    "end": "4003330"
  },
  {
    "text": "are free to satisfy requests\nfrom those different threads. And the local ownership\napproach tends",
    "start": "4003330",
    "end": "4011280"
  },
  {
    "text": "to reduce false sharing\nbecause the thread that allocates an object\nis eventually",
    "start": "4011280",
    "end": "4017130"
  },
  {
    "text": "going to get it back. You're not going to have it so\nthat an object is permanently split among multiple\nprocessors' heaps.",
    "start": "4017130",
    "end": "4025320"
  },
  {
    "text": "So even if you see false\nsharing in local ownership, it's usually temporary.",
    "start": "4025320",
    "end": "4030990"
  },
  {
    "text": "Eventually it's\ngoing-- the object is going to go back to the heap\nthat it was allocated from,",
    "start": "4030990",
    "end": "4036510"
  },
  {
    "text": "and the false sharing\nis going to go away. Yes? STUDENT: Are the local heaps\njust three to five regions in",
    "start": "4036510",
    "end": "4046852"
  },
  {
    "text": "[INAUDIBLE]? JULIAN SHUN: I mean, you can\nimplement it in various ways. I mean can have each one of\nthem have a binned-free list",
    "start": "4046852",
    "end": "4054359"
  },
  {
    "text": "allocator, so there's\nno restriction on where they have to\nappear in physical memory.",
    "start": "4054360",
    "end": "4059859"
  },
  {
    "text": "There are many different\nways where you can-- you can basically plug-in\nany serial locator",
    "start": "4059860",
    "end": "4064890"
  },
  {
    "text": "for the local heap. ",
    "start": "4064890",
    "end": "4070280"
  },
  {
    "text": "So let's go back to\nparallel heap allocation. So I talked about three\napproaches already.",
    "start": "4070280",
    "end": "4076900"
  },
  {
    "text": "Here's a fourth approach. This is called the\nhoard allocator.",
    "start": "4076900",
    "end": "4082599"
  },
  {
    "text": "And this was actually\na pretty good allocator when it was introduced\nalmost two decades ago.",
    "start": "4082600",
    "end": "4088900"
  },
  {
    "text": "And it's inspired a\nlot of further research on how to improve\nparallel-memory allocation.",
    "start": "4088900",
    "end": "4093970"
  },
  {
    "text": "So let me talk about\nhow this works. So in the hoard allocator, we're\ngoing to have p local heaps.",
    "start": "4093970",
    "end": "4101020"
  },
  {
    "text": "But we're also going\nto have a global heap. The memory is going\nto be organized",
    "start": "4101020",
    "end": "4106960"
  },
  {
    "text": "into large super\nblocks of size s. And s is usually a\nmultiple of the page size.",
    "start": "4106960",
    "end": "4114520"
  },
  {
    "text": "So this is the\ngranularity at which objects are going to be moved\naround in the allocator.",
    "start": "4114520",
    "end": "4120250"
  },
  {
    "text": "And then you can move super\nblocks between the local heaps and the global heaps.",
    "start": "4120250",
    "end": "4126130"
  },
  {
    "text": "So when a local heap becomes-- has a lot of super blocks\nthat are not being fully used",
    "start": "4126130",
    "end": "4132770"
  },
  {
    "text": "and you can move it\nto the global heap, and then when a local heap\ndoesn't have enough memory, it can go to the global\nheap to get more memory.",
    "start": "4132770",
    "end": "4139721"
  },
  {
    "text": "And then when the global heap\ndoesn't have any more memory, then it gets more memory\nfrom the operating system.",
    "start": "4139722",
    "end": "4146778"
  },
  {
    "text": "So this is sort of a\ncombination of the approaches that we saw before.",
    "start": "4146779",
    "end": "4152139"
  },
  {
    "text": "The advantages are that this\nis a pretty fast allocator. It's also scalable. As you add more processors,\nthe performance improves.",
    "start": "4152140",
    "end": "4160450"
  },
  {
    "text": "You can also bound the blow up. And it also has resilience\nto false sharing,",
    "start": "4160450",
    "end": "4166390"
  },
  {
    "text": "because it's using local heaps. So let's look at how an\nallocation using the hoard",
    "start": "4166390",
    "end": "4173080"
  },
  {
    "text": "allocator works. So let's just assume\nwithout loss of generality that all the blocks\nare the same size.",
    "start": "4173080",
    "end": "4178759"
  },
  {
    "text": "So we have fixed-size\nallocation. So let's say we call\nmalloc in our program.",
    "start": "4178760",
    "end": "4186160"
  },
  {
    "text": "And let's say thread\ni calls the malloc. So what we're going\nto do is we're going to check if there\nis a free object in heap i",
    "start": "4186160",
    "end": "4196030"
  },
  {
    "text": "that can satisfy this request. And if so, we're\ngoing to get an object from the fullest non-full\nsuper block in i's heap.",
    "start": "4196030",
    "end": "4205360"
  },
  {
    "text": "Does anyone know why we want to\nget the object from the fullest non-full super block?",
    "start": "4205360",
    "end": "4210580"
  },
  {
    "text": " Yes. STUDENT: [INAUDIBLE]",
    "start": "4210580",
    "end": "4217478"
  },
  {
    "text": "JULIAN SHUN: Right. So when a super block\nneeds to be moved, it's as dense as possible. And more importantly, this is to\nreduce external fragmentation.",
    "start": "4217478",
    "end": "4225500"
  },
  {
    "text": "Because as we saw\nin the last lecture, if you skew the distribution\nof allocated memory objects",
    "start": "4225500",
    "end": "4232430"
  },
  {
    "text": "to as few pages,\nor in this case, as few super blocks\nas possible, that reduces your external\nfragmentation.",
    "start": "4232430",
    "end": "4240840"
  },
  {
    "text": "OK, so if it finds\nit in its own heap, then it's going to allocate\nan object from there.",
    "start": "4240840",
    "end": "4246690"
  },
  {
    "text": "Otherwise, it's going to\ncheck the global heap. And if there's something\nin the global heap--",
    "start": "4246690",
    "end": "4253320"
  },
  {
    "text": "so here it says, if the\nglobal heap is empty, then it's going to get a\nnew super block from the OS.",
    "start": "4253320",
    "end": "4259130"
  },
  {
    "text": "Otherwise, we can get a super\nblock from the global heap, and then use that one.",
    "start": "4259130",
    "end": "4265860"
  },
  {
    "text": "And then finally\nwe set the owner of the block we got either from\nthe OS or from the global heap",
    "start": "4265860",
    "end": "4272000"
  },
  {
    "text": "to i, and then we return that\nfree object to the program.",
    "start": "4272000",
    "end": "4278210"
  },
  {
    "text": "So this is how a malloc works\nusing the hoard allocator. And now let's look at\nhoard deallocation.",
    "start": "4278210",
    "end": "4286770"
  },
  {
    "text": "Let use of i be the in\nuse storage in heap i. This is the heap for thread i.",
    "start": "4286770",
    "end": "4293565"
  },
  {
    "text": "And let a sub i be the\nstorage owned by heap i.",
    "start": "4293565",
    "end": "4299162"
  },
  {
    "text": "The hoard allocator maintains\nthe following invariant for all heaps i. And the invariant is as follows.",
    "start": "4299162",
    "end": "4304650"
  },
  {
    "text": "So u sub i is always\ngoing to be greater than or equal to the min\nof a sub i minus 2 times s.",
    "start": "4304650",
    "end": "4310940"
  },
  {
    "text": "Recall s is the\nsuper block size. And a sub i over 2.",
    "start": "4310940",
    "end": "4318110"
  },
  {
    "text": "So how it implements\nthis is as follows. When we call free of x, let's\nsay x is owned by thread i,",
    "start": "4318110",
    "end": "4326500"
  },
  {
    "text": "then we're going to\nput x back into heap i, and then we're going to check\nif the n u storage in heap i,",
    "start": "4326500",
    "end": "4333230"
  },
  {
    "text": "u sub i is less than the\nmin of a sub i minus 2 s and a sub i over 2.",
    "start": "4333230",
    "end": "4340510"
  },
  {
    "text": "And what this condition\nsays, if it's true, it means that your heap\nis, at most, half utilized.",
    "start": "4340510",
    "end": "4350570"
  },
  {
    "text": "Because if it's\nsmaller than this, it has to be smaller\nthan a sub i over 2. That means there's\ntwice as much allocated",
    "start": "4350570",
    "end": "4357050"
  },
  {
    "text": "than used in the local heap i. And therefore there\nmust be some super block that's at least half empty.",
    "start": "4357050",
    "end": "4363139"
  },
  {
    "text": "And you move that super block,\nor one of those super blocks, to the global heap. ",
    "start": "4363140",
    "end": "4371060"
  },
  {
    "text": "So any questions on how the\nallocation and deallocation works? So since we're maintaining\nthis invariant,",
    "start": "4371060",
    "end": "4378960"
  },
  {
    "text": "it's going to allow us to\napprove a bound on the blow up. And I'll show you that\non the next slide. But before I go on, are\nthere any questions?",
    "start": "4378960",
    "end": "4385718"
  },
  {
    "text": " OK, so let's look at how\nwe can bound the blow up",
    "start": "4385718",
    "end": "4391000"
  },
  {
    "text": "of the hoard allocator. So there is actually\na lemma that we're going to use and not prove. The lemma is that the\nmaximum storage allocated",
    "start": "4391000",
    "end": "4398110"
  },
  {
    "text": "in the global heap is at most\na maximum storage allocated in the local heaps. So we just need to analyze\nhow much storage is",
    "start": "4398110",
    "end": "4405070"
  },
  {
    "text": "allocated in the local heaps. Because the total\namount of storage is going to be, at\nmost, twice as much,",
    "start": "4405070",
    "end": "4410889"
  },
  {
    "text": "since the global heap storage\nis dominated by the local heap storage.",
    "start": "4410890",
    "end": "4416170"
  },
  {
    "text": "So you can prove this\nlemma by case analysis. And there's the\nhoard paper that's",
    "start": "4416170",
    "end": "4421520"
  },
  {
    "text": "available on learning modules. And you're free to look\nat that if you want to look at how this is proved. But here I'm just\ngoing to use this lemma",
    "start": "4421520",
    "end": "4427600"
  },
  {
    "text": "to prove this theorem, which\nsays that, let u be the user footprint for a program.",
    "start": "4427600",
    "end": "4433840"
  },
  {
    "text": "And let a be the hoard's\nallocator footprint.",
    "start": "4433840",
    "end": "4438940"
  },
  {
    "text": "We have that a as upper\nbounded by order u plus s p.",
    "start": "4438940",
    "end": "4444340"
  },
  {
    "text": "And therefore, a divided\nby u, which is a blowup, is going to be 1 plus\norder s p divided by u.",
    "start": "4444340",
    "end": "4451510"
  },
  {
    "text": " OK, so let's see how\nthis proof works.",
    "start": "4451510",
    "end": "4458809"
  },
  {
    "text": "So we're just going to analyze\nthe storage in the local heaps. Now recall that we're always\nsatisfying this invariant here,",
    "start": "4458810",
    "end": "4466940"
  },
  {
    "text": "where u sub i is greater than\nthe min of a sub i minus 2 s and a sub i over 2.",
    "start": "4466940",
    "end": "4472420"
  },
  {
    "text": "So the first term\nsays that we can have 2 s on utilized\nstorage per heap.",
    "start": "4472420",
    "end": "4479410"
  },
  {
    "text": "So it's basically giving\ntwo super blocks for free to each heap. And they don't have to use it.",
    "start": "4479410",
    "end": "4485980"
  },
  {
    "text": "They can basically use\nit as much as they want. And therefore, the total\namount of storage contributed",
    "start": "4485980",
    "end": "4493270"
  },
  {
    "text": "by the first term\nis going to be order s p, because each processor has\nup to 2 s unutilized storage.",
    "start": "4493270",
    "end": "4501250"
  },
  {
    "text": "So that's where the second\nterm comes from here. And the second term,\na sub i over 2--",
    "start": "4501250",
    "end": "4511180"
  },
  {
    "text": "this will give us the\nfirst-term order u. So this says that\nthe allocated storage",
    "start": "4511180",
    "end": "4516970"
  },
  {
    "text": "is at most twice the\nuse storage for-- and then if you sum up\nacross all the processors,",
    "start": "4516970",
    "end": "4524110"
  },
  {
    "text": "then there's a total of order\nuse storage that's allocated. Because the allocated\nstorage can be at most",
    "start": "4524110",
    "end": "4530530"
  },
  {
    "text": "twice the used storage.  OK, so that's the proof\nof the blow up for hoard.",
    "start": "4530530",
    "end": "4539410"
  },
  {
    "text": "And this is pretty good. It's 1 plus some\nlower order term. ",
    "start": "4539410",
    "end": "4546620"
  },
  {
    "text": "OK, so-- now these are\nsome other allocators that people use.",
    "start": "4546620",
    "end": "4552429"
  },
  {
    "text": "So jemalloc is a\npretty popular one. Has a few differences\nwith hoard. It has a separate global lock\nfor each different allocation",
    "start": "4552430",
    "end": "4559870"
  },
  {
    "text": "size. It allocates the object\nwith the smallest address among all the objects\nof the requested size.",
    "start": "4559870",
    "end": "4565630"
  },
  {
    "text": "And it releases empty\npages using m advise, which we talked about-- I talked about earlier.",
    "start": "4565630",
    "end": "4572650"
  },
  {
    "text": "And it's pretty popular because\nit has good performance, and it's pretty robust to\ndifferent allocation traces.",
    "start": "4572650",
    "end": "4580826"
  },
  {
    "text": "There's also another one\ncalled SuperMalloc, which is an up and coming contender. And it was developed\nby Bradley Kuszmaul.",
    "start": "4580827",
    "end": "4587240"
  },
  {
    "text": " Here are some allocator\nspeeds for the allocators",
    "start": "4587240",
    "end": "4593130"
  },
  {
    "text": "that we looked at for\nour particular benchmark. And for this\nparticular benchmark,",
    "start": "4593130",
    "end": "4599730"
  },
  {
    "text": "we can see that SuperMalloc\nactually does really well. It's more than three times\nfaster than jemalloc, and jemalloc is more than\ntwice as fast as hoard.",
    "start": "4599730",
    "end": "4608160"
  },
  {
    "text": "And then the default\nallocator, which uses a global heap is\npretty slow, because it",
    "start": "4608160",
    "end": "4613620"
  },
  {
    "text": "can't get good speed up. And all these experiments\nare in 32 threads.",
    "start": "4613620",
    "end": "4620180"
  },
  {
    "text": "I also have the lines of code. So we see that\nSuperMalloc actually has very few lines\nof code compared",
    "start": "4620180",
    "end": "4626324"
  },
  {
    "text": "to the other allocators. So it's relatively simple. OK so, I also have some\nslides in Garbage Collection.",
    "start": "4626325",
    "end": "4633969"
  },
  {
    "text": "But since we're out\nof time, I'll just put these slides online\nand you can read them.",
    "start": "4633970",
    "end": "4639530"
  },
  {
    "start": "4639530",
    "end": "4640669"
  }
]