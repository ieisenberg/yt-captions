[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "text": "NARRATOR: The\nfollowing content is provided by MIT OpenCourseWare\nunder a Creative Commons license.",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "Additional information\nabout our license and MIT OpenCourseWare\nin general is available at ocw.mit.edu.",
    "start": "6090",
    "end": "14370"
  },
  {
    "text": "PROFESSOR: So I'm thinking\nof large sparse matrices.",
    "start": "14370",
    "end": "21540"
  },
  {
    "text": " So the point about A -- the\nkind of matrices that we've been",
    "start": "21540",
    "end": "29280"
  },
  {
    "text": "writing down, maybe\ntwo-dimensional, maybe three-dimensional\ndifference matrices.",
    "start": "29280",
    "end": "35620"
  },
  {
    "text": "So it might be five or seven\nnon-zeros on a typical row",
    "start": "35620",
    "end": "42570"
  },
  {
    "text": "and that means that you don't\nwant to invert such a matrix. These are big matrices.",
    "start": "42570",
    "end": "49470"
  },
  {
    "text": "The order could\neasily be a million. What you can do fast is\nmultiply A by a vector,",
    "start": "49470",
    "end": "56989"
  },
  {
    "text": "because multiplying a\nsparse matrix by a vector, you only have maybe five\nnon-zeros times the n --",
    "start": "56990",
    "end": "65720"
  },
  {
    "text": "the vector of length n -- so say\n5n operations, so that's fast.",
    "start": "65720",
    "end": "71240"
  },
  {
    "text": " I'm thinking that we are at\na size where elimination,",
    "start": "71240",
    "end": "81840"
  },
  {
    "text": "even with a good ordering,\nis too expensive in time",
    "start": "81840",
    "end": "87960"
  },
  {
    "text": "or in storage. So I'm going from direct\nelimination methods",
    "start": "87960",
    "end": "93640"
  },
  {
    "text": "to iterative methods. Iterative means that\nI never actually get",
    "start": "93640",
    "end": "99640"
  },
  {
    "text": "to the perfect answer x, but I\nget close and what I want to do",
    "start": "99640",
    "end": "106390"
  },
  {
    "text": "is get close quickly. So a lot of thought\nhas gone into creating",
    "start": "106390",
    "end": "113170"
  },
  {
    "text": "iterative methods. I'll write down\none key word here.",
    "start": "113170",
    "end": "119710"
  },
  {
    "text": "Part of the decision is to\nchoose a good preconditioner.",
    "start": "119710",
    "end": "126130"
  },
  {
    "text": "I'll call that matrix P. So it's\na matrix that's supposed to be",
    "start": "126130",
    "end": "132920"
  },
  {
    "text": "-- that we have some freedom to\nchoose and it very much speeds up -- so the decisions are,\nwhat preconditioner to choose?",
    "start": "132920",
    "end": "141640"
  },
  {
    "text": "A preconditioner that's\nsomehow like the matrix A, but hopefully a lot simpler.",
    "start": "141640",
    "end": "148640"
  },
  {
    "start": "148640",
    "end": "156540"
  },
  {
    "text": "There are a bunch\nof iterative methods and that's what today's\nlecture is about.",
    "start": "156540",
    "end": "162770"
  },
  {
    "text": "Multigrid is an idea that\njust keeps developing.",
    "start": "162770",
    "end": "169040"
  },
  {
    "text": "It really is producing answers\nto very large problems very",
    "start": "169040",
    "end": "174629"
  },
  {
    "text": "fast, so that will be\nthe following lectures. And then come these methods --\nyou may not be familiar with",
    "start": "174630",
    "end": "184560"
  },
  {
    "text": "that word Krylov. Let me mention that in this\nfamily, the best known method",
    "start": "184560",
    "end": "190360"
  },
  {
    "text": "it is called\nconjugate gradients. The conjugate gradient method. So I'll just write that\ndown, conjugate gradient.",
    "start": "190360",
    "end": "199120"
  },
  {
    "text": "So that will come next week. ",
    "start": "199120",
    "end": "204620"
  },
  {
    "text": "That's a method\nthat's a giant success for symmetric, positive\ndefinite problem.",
    "start": "204620",
    "end": "212040"
  },
  {
    "text": "So I don't assume\nin general that A is symmetric or positive definite.",
    "start": "212040",
    "end": "217420"
  },
  {
    "text": "Often it is if it comes\nfrom a finite difference or finite element approximation.",
    "start": "217420",
    "end": "227250"
  },
  {
    "text": "If it is, then\nconjugate gradients is highly recommended.",
    "start": "227250",
    "end": "233690"
  },
  {
    "text": "I'll start with the\ngeneral picture. Let me put the general\npicture of what",
    "start": "233690",
    "end": "240160"
  },
  {
    "text": "an iteration looks like. So an iteration\ncomputes the new x.",
    "start": "240160",
    "end": "246170"
  },
  {
    "text": "Let me call it x-new or\nx_(k+1) from the known x.",
    "start": "246170",
    "end": "257380"
  },
  {
    "text": "So we start with x_0,\nany x_0, In principle.",
    "start": "257380",
    "end": "263570"
  },
  {
    "text": "It's not too important\nfor x_0 to be close. In linear methods, you\ncould start even at 0.",
    "start": "263570",
    "end": "273110"
  },
  {
    "text": "In nonlinear methods,\nNewton methods, you do want to be close,\nand a lot of attention",
    "start": "273110",
    "end": "278910"
  },
  {
    "text": "goes into a good start. Here the start\nisn't too important; it's the steps that you taking.",
    "start": "278910",
    "end": "285300"
  },
  {
    "text": "Let me -- so I guess\nI'm going to --",
    "start": "285300",
    "end": "293300"
  },
  {
    "text": "I'm trying to solve A*x equal b.  I'm going to split\nthat equation into --",
    "start": "293300",
    "end": "305910"
  },
  {
    "text": "I'm just going to write\nthe same equation this way. I could write it as x\nequals I minus A x plus b.",
    "start": "305910",
    "end": "318195"
  },
  {
    "text": "That would be the\nsame equation, right? Because x and x cancel. A*x comes over here\nand I have A*x equal b.",
    "start": "318195",
    "end": "325200"
  },
  {
    "text": " I've split up the equation and\nnow actually, let me bring --",
    "start": "325200",
    "end": "334640"
  },
  {
    "text": "so that's without\npreconditioner. You don't see a matrix P. That's\njust an ordinary iteration,",
    "start": "334640",
    "end": "344189"
  },
  {
    "text": "no preconditioner. The preconditioner will come --\nI'll have P and it'll go here",
    "start": "344190",
    "end": "352230"
  },
  {
    "text": "too. Then, of course, I have\nto change that to a P.",
    "start": "352230",
    "end": "360080"
  },
  {
    "text": "So that's still the\nsame equation, right? P*x is on both sides, cancels\n-- and I have A*x equal b.",
    "start": "360080",
    "end": "367080"
  },
  {
    "text": "So that's the same equation,\nbut it suggests the iteration",
    "start": "367080",
    "end": "372590"
  },
  {
    "text": "that I want to analyze.  On the right-hand side is\nthe known approximation.",
    "start": "372590",
    "end": "388900"
  },
  {
    "text": "On the left side is the\nnext approximation, x_(k+1). ",
    "start": "388900",
    "end": "396100"
  },
  {
    "text": "I multiply by P so I -- the\nidea is, P should be close to A.",
    "start": "396100",
    "end": "403800"
  },
  {
    "text": "Of course, if it was\nexactly A -- if P equaled A, then this wouldn't be here,\nand I would just be solving,",
    "start": "403800",
    "end": "412569"
  },
  {
    "text": "in one step, A*x equal b. So P equal A is one extreme.",
    "start": "412570",
    "end": "421490"
  },
  {
    "text": "I mean, it's totally,\nperfectly preconditioned, but the point is\nthat if P equals A,",
    "start": "421490",
    "end": "429520"
  },
  {
    "text": "we don't want to\nsolve that system, we're trying to escape\nfrom solving that system.",
    "start": "429520",
    "end": "435790"
  },
  {
    "text": "But we're willing to solve a\nrelated system that is simpler.",
    "start": "435790",
    "end": "443450"
  },
  {
    "text": "Why simpler? Here are some possible\npreconditioners. P equals the diagonal.",
    "start": "443450",
    "end": "452060"
  },
  {
    "start": "447000",
    "end": "687000"
  },
  {
    "text": "Just take the diagonal\nof A and that choice",
    "start": "452060",
    "end": "457550"
  },
  {
    "text": "is named after Jacobi. That's Jacobi's method.",
    "start": "457550",
    "end": "463670"
  },
  {
    "text": "Actually, it's\nalready a good idea, because the effect of that\nis somehow to properly scale",
    "start": "463670",
    "end": "474550"
  },
  {
    "text": "the equations. When I had the identity\nhere, I had no idea whether the identity and A\nare in the right scaling.",
    "start": "474550",
    "end": "484180"
  },
  {
    "text": "A may be divided by delta\nx squared or something. It could be totally\ndifferent units, but P --",
    "start": "484180",
    "end": "493060"
  },
  {
    "text": "if I take the diagonal of A,\nat least they're comparable. Then you see what -- we'll\nstudy Jacobi's method.",
    "start": "493060",
    "end": "502190"
  },
  {
    "text": "So that's a very simple choice. Takes no more work than\nthe identity, really.",
    "start": "502190",
    "end": "508650"
  },
  {
    "text": "Second choice would be -- so\nthat would be P for Jacobi. A second choice, that you\nwould think of pretty fast,",
    "start": "508650",
    "end": "516279"
  },
  {
    "text": "is the diagonal and the\nlower triangular part.",
    "start": "516280",
    "end": "522460"
  },
  {
    "text": "So it's the -- I'll\nsay the triangular -- I'm going to say\nlower triangular,",
    "start": "522460",
    "end": "528980"
  },
  {
    "text": "triangular part of A,\nincluding the diagonal.",
    "start": "528980",
    "end": "534209"
  },
  {
    "text": " This is associated with\nGauss's great name and Seidel.",
    "start": "534210",
    "end": "545389"
  },
  {
    "text": "So that's the\nGauss-Seidel choice. ",
    "start": "545390",
    "end": "551470"
  },
  {
    "text": "So why triangular? We'll analyze the\neffect of this choice.",
    "start": "551470",
    "end": "558980"
  },
  {
    "text": "If P is triangular,\nthen we can solve --",
    "start": "558980",
    "end": "567600"
  },
  {
    "text": "I'm never going to write\nout an inverse matrix. That's understood, but\nI can compute the new x",
    "start": "567600",
    "end": "575340"
  },
  {
    "text": "from the old x just\nby back substitution.",
    "start": "575340",
    "end": "581380"
  },
  {
    "text": "I find the first component\nof x_(k+1), then the second,",
    "start": "581380",
    "end": "587100"
  },
  {
    "text": "then the third. It's just because the first\nequation -- if P is triangular,",
    "start": "587100",
    "end": "596750"
  },
  {
    "text": "the first equation will\nonly have one term, one new term on the left side\nand everything else will be",
    "start": "596750",
    "end": "603860"
  },
  {
    "text": "over there. The second equation will\nhave a diagonal and something",
    "start": "603860",
    "end": "609850"
  },
  {
    "text": "that uses the number I just\nfound for the first component",
    "start": "609850",
    "end": "617579"
  },
  {
    "text": "and onward. So in other words,\ntriangular matrices are good.",
    "start": "617580",
    "end": "623320"
  },
  {
    "text": "Then people thought about\ncombinations of these. ",
    "start": "623320",
    "end": "630400"
  },
  {
    "text": "A big lot of activity\nwent in something called overrelaxation,\nwhere you did a --",
    "start": "630400",
    "end": "639520"
  },
  {
    "text": "you had a weighting of these\nand it turned out to be if you adjusted the weight, you\ncould speed up the method,",
    "start": "639520",
    "end": "649900"
  },
  {
    "text": "sometimes called SOR--\nsuccessive overrelaxation.",
    "start": "649900",
    "end": "655230"
  },
  {
    "text": "So when I was in graduate\nschool, that was a very --",
    "start": "655230",
    "end": "660889"
  },
  {
    "text": "that was sort of the beginning\nof progress beyond Jacobi and Gauss-Seidel.",
    "start": "660890",
    "end": "667740"
  },
  {
    "text": "Then after that came\na very natural --",
    "start": "667740",
    "end": "672959"
  },
  {
    "text": "ILU is the next one that I want\n-- the I stands for incomplete,",
    "start": "672960",
    "end": "680260"
  },
  {
    "text": "incomplete LU.  So what are L and U?",
    "start": "680260",
    "end": "687520"
  },
  {
    "start": "687000",
    "end": "1135000"
  },
  {
    "text": "That's the letters I always\nuse as a signal for the lower triangular and upper triangular\nfactors of A, the ones",
    "start": "687520",
    "end": "698600"
  },
  {
    "text": "that exact elimination\nwould produce. But we don't want to\ndo exact elimination. Why?",
    "start": "698600",
    "end": "704449"
  },
  {
    "text": "Because non-zeros fill in. Non-zeros appear in those\nspaces where A itself is 0.",
    "start": "704450",
    "end": "715230"
  },
  {
    "text": "So the factors are much --\nhave many more non-zeros than",
    "start": "715230",
    "end": "721630"
  },
  {
    "text": "the original A.\nThey're not as sparse. So the idea of incomplete LU is\n-- and I'll come back to it --",
    "start": "721630",
    "end": "730470"
  },
  {
    "text": "the idea of incomplete\nLU, you might say, should have occurred\nto people earlier --",
    "start": "730470",
    "end": "736400"
  },
  {
    "text": "only keep the non-zeros that\nare non-zeros in the original A.",
    "start": "736400",
    "end": "744010"
  },
  {
    "text": "Don't allow fill-in or allow\na certain amount of fill-in. You could have a tolerance and\nyou would include a non-zero",
    "start": "744010",
    "end": "752209"
  },
  {
    "text": "if it was big enough, but the\nmany, many small non-zeros that just fill in and\nmake the elimination long,",
    "start": "752210",
    "end": "763060"
  },
  {
    "text": "you throw them out. So P is -- in this\ncase, P is L --",
    "start": "763060",
    "end": "768870"
  },
  {
    "text": "is an approximate L\ntimes an approximate U.",
    "start": "768870",
    "end": "776290"
  },
  {
    "text": "So it's close to A, but\nbecause we've refused some",
    "start": "776290",
    "end": "785149"
  },
  {
    "text": "of the fill-in,\nit's not exactly A. And you have a tolerance there\nwhere if you set the tolerance",
    "start": "785150",
    "end": "792520"
  },
  {
    "text": "high, then these are exact, but\nyou've got all that fill-in;",
    "start": "792520",
    "end": "797930"
  },
  {
    "text": "if you set the tolerance to 0,\nyou've got the other extreme.",
    "start": "797930",
    "end": "805540"
  },
  {
    "text": "So that would give you an\nidea of preconditioners, of reasonable choices.",
    "start": "805540",
    "end": "812660"
  },
  {
    "text": "Now I'm going to think\nabout -- first, how do --",
    "start": "812660",
    "end": "819600"
  },
  {
    "text": "how to decide whether the x's\napproach the correct answer?",
    "start": "819600",
    "end": "827300"
  },
  {
    "text": "I need an equation for the error\nand because my equations are linear, I'm just\ngoing to subtract --",
    "start": "827300",
    "end": "835290"
  },
  {
    "text": "I'll subtract the upper\nequation from the lower one and I'll call the -- so the\nerror e_k will be x minus",
    "start": "835290",
    "end": "844850"
  },
  {
    "text": "the true answer. This is x exact. Minus x, my k-th approximation.",
    "start": "844850",
    "end": "853110"
  },
  {
    "text": "So when I subtract\nthat from that, I have e_(k+1) and here\nI have P minus A --",
    "start": "853110",
    "end": "862510"
  },
  {
    "text": "when I subtract that\nfrom that, I have e_k, and when I subtract\nthat from that, 0.",
    "start": "862510",
    "end": "868280"
  },
  {
    "text": "So very simple error equation. So this is an equation, this\nwould be the error equation.",
    "start": "868280",
    "end": "883339"
  },
  {
    "text": "You see the b has\ndisappeared, because I'm only",
    "start": "883340",
    "end": "888620"
  },
  {
    "text": "looking at differences now. ",
    "start": "888620",
    "end": "894440"
  },
  {
    "text": "Let me write that\nover on this board, even slightly differently.",
    "start": "894440",
    "end": "901380"
  },
  {
    "text": "Now I will multiply\nthrough by P inverse. That really shows\nit just so clearly.",
    "start": "901380",
    "end": "908670"
  },
  {
    "text": "So the new error is, if\nI multiply by P inverse,",
    "start": "908670",
    "end": "915600"
  },
  {
    "text": "P inverse times P is I, and\nI have P inverse A, e_k.",
    "start": "915600",
    "end": "921170"
  },
  {
    "start": "921170",
    "end": "929500"
  },
  {
    "text": "So what does that say? That says that at\nevery iteration,",
    "start": "929500",
    "end": "935130"
  },
  {
    "text": "I multiply the error\nby that matrix.",
    "start": "935130",
    "end": "940380"
  },
  {
    "text": "Of course, if P\nequals A, if I pick the perfect preconditioner, then\nP inverse A is the identity,",
    "start": "940380",
    "end": "950920"
  },
  {
    "text": "this is the zero matrix, and the\nerror in the first step is 0. I'm solving exactly.",
    "start": "950920",
    "end": "957140"
  },
  {
    "text": "I don't plan to do that. I plan to have a P that's\nclose to A, so in some way,",
    "start": "957140",
    "end": "965820"
  },
  {
    "text": "this should be close to I.\nThis whole matrix should be close to 0,\nclose in some sense.",
    "start": "965820",
    "end": "974130"
  },
  {
    "text": "Let me give a name\nfor that matrix. Let's call that\nmatrix M. So that's",
    "start": "974130",
    "end": "981730"
  },
  {
    "text": "the iteration matrix, which\nwe're never going to display.",
    "start": "981730",
    "end": "988970"
  },
  {
    "text": "I mean, it would be\nmadness to display. We don't want to compute P\ninverse explicitly or display",
    "start": "988970",
    "end": "996140"
  },
  {
    "text": "it, but to understand it -- to\nunderstand convergence or not convergence, it's M that\ncontrols everything.",
    "start": "996140",
    "end": "1005120"
  },
  {
    "text": "So this is what I would\ncall pure iteration or maybe",
    "start": "1005120",
    "end": "1011730"
  },
  {
    "text": "another word that's\nused instead of pure is stationary iteration. What does stationary mean?",
    "start": "1011730",
    "end": "1018430"
  },
  {
    "text": "It means that you do the\nsame thing all the time. At every step, you just use\nthe same equation, where --",
    "start": "1018430",
    "end": "1030810"
  },
  {
    "text": "these methods will be smarter. They'll adapt.",
    "start": "1030810",
    "end": "1036529"
  },
  {
    "text": "They'll use more information. They'll converge faster, but\nthis is the simple one --",
    "start": "1036530",
    "end": "1042449"
  },
  {
    "text": "and this one plays\na part in those. So what's the story\non convergence now?",
    "start": "1042450",
    "end": "1052309"
  },
  {
    "text": "What about that matrix M? If I take a starting\nvector, e_0 --",
    "start": "1052310",
    "end": "1058940"
  },
  {
    "text": "my initial error\ncould be anything. I multiply by M to get e_1.",
    "start": "1058940",
    "end": "1065620"
  },
  {
    "text": "Then I multiply by\nM again to get e_2. Every step, I multiply\nby M. So after k steps,",
    "start": "1065620",
    "end": "1074580"
  },
  {
    "text": "I've multiplied k times\nby M. I want to know --",
    "start": "1074580",
    "end": "1081240"
  },
  {
    "text": "so the key question\nis, does that go to 0? So the question, does it go to\n0, and if it does, how fast?",
    "start": "1081240",
    "end": "1088899"
  },
  {
    "start": "1088900",
    "end": "1094460"
  },
  {
    "text": "The two parts, does it\ngo to 0 and how fast, are pretty much controlled\nby the same property of M. So",
    "start": "1094460",
    "end": "1104350"
  },
  {
    "text": "what's the answer? When does -- if I take -- I\ndon't know anything about e_0,",
    "start": "1104350",
    "end": "1111360"
  },
  {
    "text": "so what property of M is going\nto decide whether its powers",
    "start": "1111360",
    "end": "1117260"
  },
  {
    "text": "get small, go to 0, so\nthat the error goes to 0? That's convergence.",
    "start": "1117260",
    "end": "1124550"
  },
  {
    "text": "Or, maybe blow up, or\nmaybe stay away from 0.",
    "start": "1124550",
    "end": "1132790"
  },
  {
    "text": "What controls it? One word is the answer.",
    "start": "1132790",
    "end": "1139090"
  },
  {
    "start": "1135000",
    "end": "1967000"
  },
  {
    "text": "The eigenvalues. It's the eigenvalues of\nM, and in particular, it's the biggest one.",
    "start": "1139090",
    "end": "1146650"
  },
  {
    "text": "Because if I have an eigenvalue\nof M, as far as I know,",
    "start": "1146650",
    "end": "1152240"
  },
  {
    "text": "e_naught could be\nthe eigenvector, and then every\nstep would multiply",
    "start": "1152240",
    "end": "1157610"
  },
  {
    "text": "by that eigenvalue lambda. So the condition is that\nall eigenvalues of M",
    "start": "1157610",
    "end": "1167860"
  },
  {
    "text": "have to be smaller than\n1, in magnitude of course,",
    "start": "1167860",
    "end": "1172931"
  },
  {
    "text": "because they could be negative,\nthey could be complex. ",
    "start": "1172931",
    "end": "1178890"
  },
  {
    "text": "So that's the total\ncondition, that's exactly the requirement\non M. How do",
    "start": "1178890",
    "end": "1185990"
  },
  {
    "text": "we say how fast they go to 0? How fast -- if all the\neigenvalues are below 1,",
    "start": "1185990",
    "end": "1193000"
  },
  {
    "text": "then the slowest\nconvergence is going to come",
    "start": "1193000",
    "end": "1199470"
  },
  {
    "text": "for the eigenvalue\nthat's the closest to 1. So really it will be -- and this\nis called the spectral radius",
    "start": "1199470",
    "end": "1211720"
  },
  {
    "text": "and it's usually denoted\nby a Greek rho of M,",
    "start": "1211720",
    "end": "1216890"
  },
  {
    "text": "which is the maximum\nof these guys. It's the max of the eigenvalues.",
    "start": "1216890",
    "end": "1223710"
  },
  {
    "text": "And that has to be below 1. So it's this\nnumber, that number,",
    "start": "1223710",
    "end": "1231090"
  },
  {
    "text": "because it's that number\nwhich really says what I'm multiplying by at every step.",
    "start": "1231090",
    "end": "1237710"
  },
  {
    "text": "Because most likely, e_naught,\nmy initial unknown error, has some component in the\ndirection of this biggest",
    "start": "1237710",
    "end": "1247890"
  },
  {
    "text": "eigenvalue, in the direction\nof its eigenvector. Then that component will\nmultiply at every step",
    "start": "1247890",
    "end": "1255960"
  },
  {
    "text": "by lambda, but that one will\nbe the biggest guy, rho. ",
    "start": "1255960",
    "end": "1262909"
  },
  {
    "text": "So it's the maximum eigenvalue. That's really what it comes to. What is the maximum eigenvalue?",
    "start": "1262910",
    "end": "1269230"
  },
  {
    "text": "Let me take Jacobi. Can we figure out the maximum\neigenvalue for Jacobi?",
    "start": "1269230",
    "end": "1277440"
  },
  {
    "text": "Of course, if I don't know\nanything about the matrix, I certainly don't want\nto compute eigenvalues.",
    "start": "1277440",
    "end": "1284280"
  },
  {
    "text": " I could run Jacobi's method.",
    "start": "1284280",
    "end": "1289450"
  },
  {
    "text": "So again, Jacobi's\nmethod, I'm taking P to be the diagonal part. Let me make that explicit\nand let me take the matrix",
    "start": "1289450",
    "end": "1298309"
  },
  {
    "text": "that we know the best. I'll call it K. So k --\nor A, this is the A --",
    "start": "1298310",
    "end": "1305850"
  },
  {
    "text": "is my friend with 2's on the\ndiagonal, minus 1's above.",
    "start": "1305850",
    "end": "1311820"
  },
  {
    "text": "I apologize -- I don't\nreally apologize, but I'll pretend to apologize\n-- for bringing this matrix",
    "start": "1311820",
    "end": "1322390"
  },
  {
    "text": "in so often.  Do we remember its eigenvalues?",
    "start": "1322390",
    "end": "1328180"
  },
  {
    "start": "1328180",
    "end": "1334780"
  },
  {
    "text": "We computed them last semester,\nbut that's another world.",
    "start": "1334780",
    "end": "1342180"
  },
  {
    "text": "I'll say what they are. The eigenvalues of A\n-- this isn't M now --",
    "start": "1342180",
    "end": "1349830"
  },
  {
    "text": "the eigenvalues of this matrix\nA -- I see 2's on the diagonal.",
    "start": "1349830",
    "end": "1356140"
  },
  {
    "text": "So that's a 2. That accounts for the diagonal\npart, 2 times the identity.",
    "start": "1356140",
    "end": "1361240"
  },
  {
    "text": "I have a minus and then the\npart that comes from these 1's,",
    "start": "1361240",
    "end": "1367580"
  },
  {
    "text": "which are forward and back. What would von Neumann say? What would von Neumann\ndo with that matrix?",
    "start": "1367580",
    "end": "1373580"
  },
  {
    "text": "Of course, he would test it on\nexponentials and he would get 2",
    "start": "1373580",
    "end": "1383590"
  },
  {
    "text": "minus e to the i*k minus\ne to the minus i*k.",
    "start": "1383590",
    "end": "1391049"
  },
  {
    "text": "He would put the two\nexponentials together into cosines and\nthat's the answer.",
    "start": "1391050",
    "end": "1396640"
  },
  {
    "text": "It's 2 minus 2 cosine\nof whatever angle theta. Those are the eigenvalues.",
    "start": "1396640",
    "end": "1403590"
  },
  {
    "text": "The j-th eigenvalue is --\nthe cosine is at some angle",
    "start": "1403590",
    "end": "1411250"
  },
  {
    "text": "theta_j. It's worth since -- maybe just\nto take again a minute on this",
    "start": "1411250",
    "end": "1417190"
  },
  {
    "text": "matrix. So the eigenvalues\nare between 0 and 4.",
    "start": "1417190",
    "end": "1422670"
  },
  {
    "text": "The eigenvalues never\ngo negative, right? Because the cosine\ncan't be bigger than 1.",
    "start": "1422670",
    "end": "1428890"
  },
  {
    "text": "Actually, for this matrix\nthe cosine doesn't reach 1. So the smallest eigenvalue\nis 2 minus 2 times",
    "start": "1428890",
    "end": "1437340"
  },
  {
    "text": "that cosine close to 1. ",
    "start": "1437340",
    "end": "1443210"
  },
  {
    "text": "It's too close for\ncomfort, but it is below 1,",
    "start": "1443210",
    "end": "1449710"
  },
  {
    "text": "so the eigenvalues are positive. And they're between 0 and 4.",
    "start": "1449710",
    "end": "1457560"
  },
  {
    "text": "At the other extreme,\ntheta is near pi,",
    "start": "1457560",
    "end": "1462620"
  },
  {
    "text": "the cosine is near minus 1,\nand we have something near 4. So the eigenvalues of that --\nif you look at that matrix,",
    "start": "1462620",
    "end": "1468890"
  },
  {
    "text": "you should see. Special matrix eigenvalues\nin this interval, 0 to 4.",
    "start": "1468890",
    "end": "1478650"
  },
  {
    "text": "The key point is,\nhow close to 0? Now what about M --\nwhat about P first?",
    "start": "1478650",
    "end": "1485210"
  },
  {
    "text": "What's P? For Jacobi, so P for Jacobi\nis the diagonal part,",
    "start": "1485210",
    "end": "1496289"
  },
  {
    "text": "which is exactly 2I. That's a very simple\ndiagonal part, very simple P.",
    "start": "1496290",
    "end": "1507620"
  },
  {
    "text": "It produces the matrix\nM. You remember, M is the identity matrix minus\nP inverse A. That's beautiful.",
    "start": "1507620",
    "end": "1522789"
  },
  {
    "text": "So P is just 2I. So I'm just dividing A by\n2, which puts a 1 there,",
    "start": "1522790",
    "end": "1533450"
  },
  {
    "text": "and then subtracting\nfrom the identity. So it has 0's on the diagonal. That's what we expect.",
    "start": "1533450",
    "end": "1538659"
  },
  {
    "text": "It will be 0's on the diagonal\nand off the diagonal, what",
    "start": "1538660",
    "end": "1547990"
  },
  {
    "text": "do I have? Minus -- with that minus,\nP is 2, so it's 1/2.",
    "start": "1547990",
    "end": "1553150"
  },
  {
    "text": "It's 1/2 off the diagonal\nand otherwise, all 0's.",
    "start": "1553150",
    "end": "1562370"
  },
  {
    "text": "That's the nice matrix that\nwe get for M in one dimension.",
    "start": "1562370",
    "end": "1571750"
  },
  {
    "text": "I have to say, of\ncourse, as you know, we wouldn't be doing any of this\nfor this one-dimensional matrix",
    "start": "1571750",
    "end": "1579180"
  },
  {
    "text": "A. It's tridiagonal. We can't ask for more than that. Direct elimination\nwould be top speed,",
    "start": "1579180",
    "end": "1588090"
  },
  {
    "text": "but the 2D case is what we\nunderstand that it has a big",
    "start": "1588090",
    "end": "1596059"
  },
  {
    "text": "bandwidth because\nit's two-dimensional, three-dimensional even bigger\n-- and the eigenvalues will come",
    "start": "1596060",
    "end": "1603870"
  },
  {
    "text": "directly from the\neigenvalues of this 1D case. So if we understand\nthe 1D case, we'll",
    "start": "1603870",
    "end": "1609590"
  },
  {
    "text": "knock out the two- and\nthree-dimensional easily. So I'll stay with this\none-dimensional case,",
    "start": "1609590",
    "end": "1616559"
  },
  {
    "text": "where the matrix M\n-- and I'll even, maybe should write down exactly\nwhat the iteration does --",
    "start": "1616560",
    "end": "1626360"
  },
  {
    "text": "but if I'm looking\nnow at convergence, do I have or don't I have\nconvergence, and how fast?",
    "start": "1626360",
    "end": "1633980"
  },
  {
    "text": "What are the eigenvalues\nand what's the biggest one? The eigenvalues of A are\nthis and P is just --",
    "start": "1633980",
    "end": "1644230"
  },
  {
    "text": "P inverse is just 1/2\ntimes the identity.",
    "start": "1644230",
    "end": "1649950"
  },
  {
    "text": " So what are the eigenvalues?",
    "start": "1649950",
    "end": "1655360"
  },
  {
    "text": "Again, I mean, I know\nthe eigenvalues of A, so I divide by 2.",
    "start": "1655360",
    "end": "1662919"
  },
  {
    "text": "Let me write down what\nI'm going to get now for the eigenvalues of M,\nremembering this connection.",
    "start": "1662920",
    "end": "1673230"
  },
  {
    "text": "So the eigenvalues of M are 1\nminus 1/2 times the eigenvalues of A. Better write that down.",
    "start": "1673230",
    "end": "1680080"
  },
  {
    "text": "1 minus 1/2 times\nthe eigenvalues of A. Of course, it's\nvery special that we",
    "start": "1680080",
    "end": "1690000"
  },
  {
    "text": "have this terrific\nexample matrix where",
    "start": "1690000",
    "end": "1695500"
  },
  {
    "text": "we know the eigenvalues and\nreally can see what's happening and we know the matrix.",
    "start": "1695500",
    "end": "1701670"
  },
  {
    "text": "So what happens? I take half of that --\nthat makes that a 1.",
    "start": "1701670",
    "end": "1707140"
  },
  {
    "text": "When I subtract it\nfrom that 1, it's gone. 1/2 of that, that 2 is cancelled\n-- I just get cos theta.",
    "start": "1707140",
    "end": "1716980"
  },
  {
    "text": "Theta sub -- whatever\nthat angle was. ",
    "start": "1716980",
    "end": "1723910"
  },
  {
    "text": "It's a cosine and\nit's less than 1. ",
    "start": "1723910",
    "end": "1731190"
  },
  {
    "text": "So convergence is going to\nhappen and convergence is going to be slow or fast according as\nthis is very near 1 or not --",
    "start": "1731190",
    "end": "1741950"
  },
  {
    "text": "and unfortunately, it is pretty\nnear 1, because the first, the largest one -- I could\ntell you what the angles are.",
    "start": "1741950",
    "end": "1751040"
  },
  {
    "text": "That's -- to complete\nthis information, I should have put in what --\nand von Neumann got them right.",
    "start": "1751040",
    "end": "1758610"
  },
  {
    "text": "That's j -- is there\nmaybe a pi over N plus 1?",
    "start": "1758610",
    "end": "1767610"
  },
  {
    "text": "I think so.  Those are the\nangles that come in,",
    "start": "1767610",
    "end": "1773159"
  },
  {
    "text": "just the equally spaced\nangles in the finite case. So we have N eigenvalues.",
    "start": "1773160",
    "end": "1780940"
  },
  {
    "text": "And the biggest one is\ngoing to be when j is 1. So the biggest one, rho,\nthe maximum of these guys,",
    "start": "1780940",
    "end": "1790780"
  },
  {
    "text": "of these lambdas, will be\nthe cosine of, when j is 1,",
    "start": "1790780",
    "end": "1796040"
  },
  {
    "text": "pi over N plus 1. That's the one that's\nslowing us down.",
    "start": "1796040",
    "end": "1801860"
  },
  {
    "text": " Notice the eigenvector\nthat's slowing us down,",
    "start": "1801860",
    "end": "1810390"
  },
  {
    "text": "because it's the\neigenvector that goes with that eigenvalue that's\nthe biggest eigenvalue that's",
    "start": "1810390",
    "end": "1817270"
  },
  {
    "text": "going to hang around the\nlongest, decay the slowest. That eigenvector is very smooth.",
    "start": "1817270",
    "end": "1825980"
  },
  {
    "text": "It's the low frequency. It's frequency 1. So this is j equal to 1.",
    "start": "1825980",
    "end": "1832180"
  },
  {
    "text": "This is the low frequency,\nj equal 1, low frequency. ",
    "start": "1832180",
    "end": "1839030"
  },
  {
    "text": "The eigenvector is just\nsamples of the sine,",
    "start": "1839030",
    "end": "1849070"
  },
  {
    "text": "of a discrete sine. Lowest frequency that\nthe grid can sustain.",
    "start": "1849070",
    "end": "1857140"
  },
  {
    "text": " Why do I emphasize\nwhich eigenvector it is?",
    "start": "1857140",
    "end": "1864250"
  },
  {
    "text": "Because when you know which\neigenvector is slowing you down -- and here it's the eigenvector\nyou would think would be",
    "start": "1864250",
    "end": "1872330"
  },
  {
    "text": "the simplest one to handle\n-- this is the easiest eigenvector, higher-frequency\neigenvectors have eigenvalues",
    "start": "1872330",
    "end": "1881830"
  },
  {
    "text": "that start dropping. Cosine of 2*pi, cosine of 3*pi\nover N plus 1 is going to be",
    "start": "1881830",
    "end": "1889809"
  },
  {
    "text": "further away from 1. Now I have to admit that\nthe very highest frequency,",
    "start": "1889810",
    "end": "1898630"
  },
  {
    "text": "when j is capital N -- so when j\nis capital N I have to admit it",
    "start": "1898630",
    "end": "1908600"
  },
  {
    "text": "happens to come back\nwith a minus sign, but the magnitude is\nstill just as bad.",
    "start": "1908600",
    "end": "1914820"
  },
  {
    "text": "The cosine of N*pi over N plus\n1 is just the negative of that",
    "start": "1914820",
    "end": "1923039"
  },
  {
    "text": "one. So actually, we\nhave two eigenvalues that are both hitting\nthe spectral radius",
    "start": "1923040",
    "end": "1933070"
  },
  {
    "text": "and both of them are the\nworst, the slowest converging",
    "start": "1933070",
    "end": "1940350"
  },
  {
    "text": "eigenvectors. This eigenvector\nlooks very smooth.",
    "start": "1940350",
    "end": "1947360"
  },
  {
    "text": "The eigenvector for\nthat high-frequency one is the fastest oscillation.",
    "start": "1947360",
    "end": "1952760"
  },
  {
    "text": "If I graph the eigenvectors,\neigenvalues, I will. I'll graph the eigenvalues.",
    "start": "1952760",
    "end": "1959950"
  },
  {
    "text": "You'll see that it's the\nfirst and the last that are nearest in magnitude to 1.",
    "start": "1959950",
    "end": "1966160"
  },
  {
    "text": " I was just going to\nsay, can I anticipate",
    "start": "1966160",
    "end": "1972020"
  },
  {
    "start": "1967000",
    "end": "2088000"
  },
  {
    "text": "multigrid for a minute?  So multigrid does -- is going\nto try to get these guys to go",
    "start": "1972020",
    "end": "1981679"
  },
  {
    "text": "faster. Now this one can be\nhandled, you'll see, by just a simple\nadjustment of Jacobi.",
    "start": "1981680",
    "end": "1989650"
  },
  {
    "text": "I just weight\nJacobi a little bit. Instead of 2I, I take 3I or 4I.",
    "start": "1989650",
    "end": "1999350"
  },
  {
    "text": "3I would be very good\nfor the preconditioner. That will have the effect\non this high frequency one",
    "start": "1999350",
    "end": "2008440"
  },
  {
    "text": "that it'll be well\ndown now, below 1. This will still be the\nslowpoke in converging.",
    "start": "2008440",
    "end": "2017980"
  },
  {
    "text": "That's where multigrid\nsays, that's low frequency.",
    "start": "2017980",
    "end": "2024460"
  },
  {
    "text": "That's too low. That's not converging quickly. Take a different grid\non which that same thing",
    "start": "2024460",
    "end": "2032380"
  },
  {
    "text": "looks higher frequency. We'll see that tomorrow.",
    "start": "2032380",
    "end": "2038360"
  },
  {
    "text": "So if I stay with Jacobi,\nwhat have I learned? I've learned that this is rho.",
    "start": "2038360",
    "end": "2045410"
  },
  {
    "text": "For this matrix,\nthe spectral radius is that number and let's\njust see how big it is.",
    "start": "2045410",
    "end": "2051550"
  },
  {
    "text": "How big is -- that's a\ncosine of a small number. So the cosine of a small\nnumber, theta near 0,",
    "start": "2051550",
    "end": "2060319"
  },
  {
    "text": "the cosine of theta is 1\nminus -- so it's below 1,",
    "start": "2060320",
    "end": "2065600"
  },
  {
    "text": "of course -- 1/2 theta squared. Theta is pi over\nN plus 1 squared.",
    "start": "2065600",
    "end": "2073369"
  },
  {
    "text": "All these simple\nestimates really tell you what happens when\nyou do Jacobi iteration.",
    "start": "2073370",
    "end": "2084230"
  },
  {
    "text": "You'll see it on the output\nfrom -- I mean, you can --",
    "start": "2084230",
    "end": "2092450"
  },
  {
    "text": "I hope will -- code\nup Jacobi's method,",
    "start": "2092450",
    "end": "2099690"
  },
  {
    "text": "taking P to be a multiple of\nthe identity, so very fast,",
    "start": "2099690",
    "end": "2106710"
  },
  {
    "text": "and you'll see the convergence\nrate, you can graph. Here's an interesting point\nand I'll make it again.",
    "start": "2106710",
    "end": "2113900"
  },
  {
    "text": "What should you\ngraph in seeing --",
    "start": "2113900",
    "end": "2119970"
  },
  {
    "text": "to display convergence\nand rate of convergence? Let me draw a\npossible graph here.",
    "start": "2119970",
    "end": "2128400"
  },
  {
    "text": "Let me draw two possible graphs. I could graph -- this is\nK. As I take more steps,",
    "start": "2128400",
    "end": "2138660"
  },
  {
    "text": "I could graph the error,\nwell, the size of the error. It's a vector, so\nI take its length.",
    "start": "2138660",
    "end": "2147980"
  },
  {
    "text": "I don't know where I\nstart -- at e_naught. This is k equals 0. This is the initial guess. ",
    "start": "2147980",
    "end": "2155380"
  },
  {
    "text": "If I use -- I'm going\nto draw a graph,",
    "start": "2155380",
    "end": "2160619"
  },
  {
    "text": "which you'll draw much better\nby actually getting MATLAB to do it. ",
    "start": "2160620",
    "end": "2168690"
  },
  {
    "text": "Before I draw it, let me mention\nthe other graph we could draw. This tells us the error\nin -- this is the error",
    "start": "2168690",
    "end": "2182349"
  },
  {
    "text": "in the solution. If I think that's the\nnatural thing to draw, it is, but we could\nalso draw the error",
    "start": "2182350",
    "end": "2192360"
  },
  {
    "text": "in the equation,\nthe residual error. So I'll just put up here\nwhat I mean by that.",
    "start": "2192360",
    "end": "2199730"
  },
  {
    "text": "If I put in -- so\nA*x equals b exactly.",
    "start": "2199730",
    "end": "2208220"
  },
  {
    "text": "A*x_k is close. ",
    "start": "2208220",
    "end": "2216990"
  },
  {
    "start": "2214000",
    "end": "2266000"
  },
  {
    "text": "The residual is -- let me call\nit r, as everybody does -- is the difference A*x minus\nA*x_k It's how close we are",
    "start": "2216990",
    "end": "2233820"
  },
  {
    "text": "to b, so it's b minus A*x_k.",
    "start": "2233820",
    "end": "2238850"
  },
  {
    "text": "The true A*x gets\nb exactly right. The wrong A*x gets\nb nearly right. ",
    "start": "2238850",
    "end": "2248030"
  },
  {
    "text": "So you see, because we\nhave this linear problem, it's A times the error.\nx minus x_k is e_k.",
    "start": "2248030",
    "end": "2258860"
  },
  {
    "text": "So this is the residual\nand I could graph that.",
    "start": "2258860",
    "end": "2269770"
  },
  {
    "start": "2266000",
    "end": "2638000"
  },
  {
    "text": "That's how close my answer\ncomes to getting b right,",
    "start": "2269770",
    "end": "2275470"
  },
  {
    "text": "how close the equation is. Where this is how\nclose the x_k is.",
    "start": "2275470",
    "end": "2281440"
  },
  {
    "text": "So that would be the\nother thing I could graph. ",
    "start": "2281440",
    "end": "2286570"
  },
  {
    "text": "r_k, the size of the\nresidual, which is A*e_k.",
    "start": "2286570",
    "end": "2292680"
  },
  {
    "start": "2292680",
    "end": "2299670"
  },
  {
    "text": "So now I have to remember what\nI think these graphs look like, but of course, it's a little\nabsurd for me to draw them when",
    "start": "2299670",
    "end": "2306305"
  },
  {
    "text": "-- I think what we see is that\nthe residual drops pretty fast.",
    "start": "2306305",
    "end": "2315040"
  },
  {
    "text": "The residual drops pretty\nfast and keeps going,",
    "start": "2315040",
    "end": "2320660"
  },
  {
    "text": "for, let's say, Jacobi's method,\nor, in a minute or maybe next",
    "start": "2320660",
    "end": "2329010"
  },
  {
    "text": "time, another\niterative method -- but the error in the\nsolution starts down fast",
    "start": "2329010",
    "end": "2337000"
  },
  {
    "text": "and what's happening here is the\nhigh-frequency components are getting killed, but those\nlow-frequency components are",
    "start": "2337000",
    "end": "2348950"
  },
  {
    "text": "not disappearing, because we\nfigured out that they give the spectral radius,\nthe largest eigenvalue.",
    "start": "2348950",
    "end": "2356410"
  },
  {
    "text": "So somehow it slopes\noff and it's --",
    "start": "2356410",
    "end": "2367839"
  },
  {
    "text": "the price of such a simple\nmethod is that you don't get great convergence.",
    "start": "2367840",
    "end": "2373020"
  },
  {
    "text": "Of course, if I put\nthis on a log-log plot,",
    "start": "2373020",
    "end": "2378520"
  },
  {
    "text": "the slope would be\nexactly connected to the spectral radius.",
    "start": "2378520",
    "end": "2384310"
  },
  {
    "text": "That's the rate of decay.  It's the -- e_k is\napproximately --",
    "start": "2384310",
    "end": "2393320"
  },
  {
    "text": "the size of e_k is approximately\nthat worst eigenvalue to the k-th power.",
    "start": "2393320",
    "end": "2398710"
  },
  {
    "start": "2398710",
    "end": "2404830"
  },
  {
    "text": "Do you see how this can happen?  You see, this can\nbe quite small,",
    "start": "2404830",
    "end": "2411830"
  },
  {
    "text": "the residual quite small. I think it's very\nimportant to see the two different quantities.",
    "start": "2411830",
    "end": "2417760"
  },
  {
    "text": "The residual can be quite small,\nbut then, from the residual,",
    "start": "2417760",
    "end": "2422780"
  },
  {
    "text": "if I wanted to get\nback to the error, I'd have to multiply\nby A inverse and A inverse is not small.",
    "start": "2422780",
    "end": "2430730"
  },
  {
    "text": "Our matrix A is pretty --\nhas eigenvalues close to 0.",
    "start": "2430730",
    "end": "2436090"
  },
  {
    "text": "So A inverse is pretty big. A inverse times A*e_k -- this\nis small but A inverse picks up",
    "start": "2436090",
    "end": "2448170"
  },
  {
    "text": "those smallest\neigenvalues of A -- picks up those low\nfrequencies and it's bigger,",
    "start": "2448170",
    "end": "2459109"
  },
  {
    "text": "slower to go to 0 as\nwe see in this picture. So it's this picture\nthat is our problem.",
    "start": "2459110",
    "end": "2468080"
  },
  {
    "text": "That shows where Jacobi\nis not good enough.",
    "start": "2468080",
    "end": "2474580"
  },
  {
    "text": "So what more to\nsay about Jacobi? For this matrix K, we\nknow its eigenvalues.",
    "start": "2474580",
    "end": "2481840"
  },
  {
    "text": " The eigenvalues of\nM we know exactly.",
    "start": "2481840",
    "end": "2487559"
  },
  {
    "text": "The matrix M we know exactly. Do you see in that matrix\n-- now if I had said,",
    "start": "2487560",
    "end": "2493740"
  },
  {
    "text": "look at that matrix and\ntell me something about its eigenvalues, what would you say? ",
    "start": "2493740",
    "end": "2501369"
  },
  {
    "text": "You would say, it's symmetric,\nso the eigenvalues are real and you would say that every\neigenvalue is smaller than 1.",
    "start": "2501370",
    "end": "2509869"
  },
  {
    "text": "Why? Because every eigenvalue is --\nif you like to remember this",
    "start": "2509870",
    "end": "2515119"
  },
  {
    "text": "business of silly circles,\nevery eigenvalue is in a circle",
    "start": "2515120",
    "end": "2521540"
  },
  {
    "text": "centered at this number -- in\nother words, centered at 0 -- and its radius is the\nsum of those, which is 1.",
    "start": "2521540",
    "end": "2530090"
  },
  {
    "text": "So we know that every\neigenvalue is in the unit circle",
    "start": "2530090",
    "end": "2536180"
  },
  {
    "text": "and actually, it comes\ninside the unit circle,",
    "start": "2536180",
    "end": "2541510"
  },
  {
    "text": "but not very much --\nonly by 1 over N squared. What do I learn from\nthis 1 over N squared?",
    "start": "2541510",
    "end": "2548610"
  },
  {
    "text": " How many iterations\ndo I have to take",
    "start": "2548610",
    "end": "2553950"
  },
  {
    "text": "to reduce the error by, let's\nsay, 10 or e or whatever?",
    "start": "2553950",
    "end": "2559630"
  },
  {
    "text": "If the eigenvalues are 1 minus\na constant over N squared,",
    "start": "2559630",
    "end": "2564900"
  },
  {
    "text": "I'll have to take\nthe N squared power. So if I have a\nmatrix of order 100,",
    "start": "2564900",
    "end": "2570900"
  },
  {
    "text": "the number of iterations\nI'm going to need is 100 squared, 10,000 steps.",
    "start": "2570900",
    "end": "2579600"
  },
  {
    "text": "Every step is fast, especially\nwith the Jacobi choice. ",
    "start": "2579600",
    "end": "2589420"
  },
  {
    "text": "Here's my iteration -- and P is\njust a multiple of the identity",
    "start": "2589420",
    "end": "2594920"
  },
  {
    "text": "here.  So every step is certainly fast.",
    "start": "2594920",
    "end": "2600830"
  },
  {
    "text": "Every step involves a\nmatrix multiplication and that's why I began\nthe lecture by saying,",
    "start": "2600830",
    "end": "2605850"
  },
  {
    "text": "matrix multiplications are fast. A times x_k -- that's the\nwork and it's not much.",
    "start": "2605850",
    "end": "2612860"
  },
  {
    "text": "It's maybe five\nnon-zeros in A -- so 5N. That's fast, but if I\nhave to do it 10,000 times",
    "start": "2612860",
    "end": "2620890"
  },
  {
    "text": "just to reduce the error\nby some constant factor, that's not good.",
    "start": "2620890",
    "end": "2627210"
  },
  {
    "text": "So that's why people think,\nokay, Jacobi gives us the idea.",
    "start": "2627210",
    "end": "2632530"
  },
  {
    "text": "It does knock off the high\nfrequencies quite quickly, but it leaves those\nlow frequencies.",
    "start": "2632530",
    "end": "2640349"
  },
  {
    "start": "2638000",
    "end": "2922000"
  },
  {
    "text": "Well, I was going to\nmention weighed Jacobi. Weighted Jacobi is to\ntake, instead of p --",
    "start": "2640350",
    "end": "2652040"
  },
  {
    "text": "put in a weight. So weighted Jacobi -- let me put\nthis here and we'll come back",
    "start": "2652040",
    "end": "2658970"
  },
  {
    "text": "to it. So weighted Jacobi\nsimply takes P",
    "start": "2658970",
    "end": "2667330"
  },
  {
    "text": "to be the diagonal\nover a factor omega.",
    "start": "2667330",
    "end": "2675390"
  },
  {
    "text": " For example, omega equals\n2/3 is a good choice.",
    "start": "2675390",
    "end": "2689300"
  },
  {
    "text": "Let me draw the -- you\ncan imagine that if I -- this is 2 times the identity,\nso it's just 2 over omega I --",
    "start": "2689300",
    "end": "2698020"
  },
  {
    "text": "I'm just choosing a\ndifferent constant. I'm just choosing a\ndifferent constant in P",
    "start": "2698020",
    "end": "2703540"
  },
  {
    "text": "and it has a simple affect\non M. I'll draw the picture.",
    "start": "2703540",
    "end": "2709360"
  },
  {
    "text": " The eigenvalues of\nM are these cosines.",
    "start": "2709360",
    "end": "2716180"
  },
  {
    "text": "So I'm going to draw those\ncosines, the eigenvalues of M. So they start -- I'll just draw\nthe picture of cos theta if I",
    "start": "2716180",
    "end": "2723290"
  },
  {
    "text": "can. What does a graph of\ncos theta look like? ",
    "start": "2723290",
    "end": "2732060"
  },
  {
    "text": "From 0 to pi? So I'm going to see --\nthis is theta equals 0.",
    "start": "2732060",
    "end": "2738910"
  },
  {
    "text": "This is theta equal pi. If I just draw cos theta\n-- please tell me if this",
    "start": "2738910",
    "end": "2745670"
  },
  {
    "text": "isn't it. Is it something like that? ",
    "start": "2745670",
    "end": "2752619"
  },
  {
    "text": "Now where are these theta_1\n-- the bad one -- theta_2,",
    "start": "2752620",
    "end": "2758940"
  },
  {
    "text": "theta_3, theta_4, theta_5,\nequally bad, down here. So there's cos --\nthere's the biggest one.",
    "start": "2758940",
    "end": "2767180"
  },
  {
    "text": "That's rho and you see\nit's pretty near 1.  Here I'm going to hit\n-- at this frequency,",
    "start": "2767180",
    "end": "2774890"
  },
  {
    "text": "I'm going to hit minus rho. It'll be, again, near 1.",
    "start": "2774890",
    "end": "2780730"
  },
  {
    "text": "So the spectral radius\nis that, that's rho.",
    "start": "2780730",
    "end": "2787180"
  },
  {
    "text": "The cosine of that smallest\nangle, theta over N plus 1.",
    "start": "2787180",
    "end": "2794160"
  },
  {
    "text": "What happens when I\nbring in these weights? It turns out that -- so,\nnow the eigenvalues of M,",
    "start": "2794160",
    "end": "2803090"
  },
  {
    "text": "with the weights, will\nbe 1 -- it turns out -- 1 minus omega plus\nomega cos theta.",
    "start": "2803090",
    "end": "2810740"
  },
  {
    "text": " You'll see it in the\nnotes, no problem.",
    "start": "2810740",
    "end": "2816720"
  },
  {
    "text": "So it's simply\ninfluenced by omega. If omega is 2/3 --\nif omega is 2/3,",
    "start": "2816720",
    "end": "2824660"
  },
  {
    "text": "then I have 1 minus omega\nis 1/3 plus 2/3 cos theta.",
    "start": "2824660",
    "end": "2830599"
  },
  {
    "start": "2830600",
    "end": "2835620"
  },
  {
    "text": "Instead of graphing cos\ntheta, which I've done -- let me draw -- let me complete\nthis graph for cos theta.",
    "start": "2835620",
    "end": "2843220"
  },
  {
    "text": "The eigenvalues unweighted,\nwith weight omega equal 1. Now it's this thing I draw.",
    "start": "2843220",
    "end": "2851450"
  },
  {
    "text": "So what happens? What's going on here? For a small theta,\ncosine is near 1.",
    "start": "2851450",
    "end": "2860890"
  },
  {
    "text": "I'm again near 1. In fact, I'm probably even\nlittle worse, probably up here.",
    "start": "2860890",
    "end": "2866099"
  },
  {
    "start": "2866100",
    "end": "2871230"
  },
  {
    "text": "I think it goes down like that.",
    "start": "2871230",
    "end": "2877410"
  },
  {
    "text": "So it starts at near 1, but\nit ends when theta is pi.",
    "start": "2877410",
    "end": "2883180"
  },
  {
    "text": "This is 1/3 minus 2/3. It ends at minus 1/3. You see, that's a lot smarter.",
    "start": "2883180",
    "end": "2890150"
  },
  {
    "text": "This one ended at minus 1,\nthat one ended at near minus 1, but this one will end with omega\n-- this is the omega equal 2/3,",
    "start": "2890150",
    "end": "2901130"
  },
  {
    "text": "the weighted one. All you've done is fix\nthe high frequency.",
    "start": "2901130",
    "end": "2906829"
  },
  {
    "text": "When I say, oh, that\nwas a good thing to do. The high frequencies\nare no longer a problem.",
    "start": "2906830",
    "end": "2914060"
  },
  {
    "text": "The low frequencies still are. So the only way we'll\nget out of that problem",
    "start": "2914060",
    "end": "2919800"
  },
  {
    "text": "is moving to multigrid. Can I, in the remaining\nminute, report on Gauss-Seidel",
    "start": "2919800",
    "end": "2929970"
  },
  {
    "start": "2922000",
    "end": "3010000"
  },
  {
    "text": "and then I'll come back to it? But quick report\non Gauss-Seidel. So what's the difference\nin Gauss-Seidel?",
    "start": "2929970",
    "end": "2938150"
  },
  {
    "text": "What does Gauss-Seidel do? It uses the latest information. As soon as you compute\nan x, you use it.",
    "start": "2938150",
    "end": "2945770"
  },
  {
    "text": "Jacobi computed all -- had to\nkeep all these x's until it",
    "start": "2945770",
    "end": "2954350"
  },
  {
    "text": "found all the new x's, but\nthe Gauss-Seidel idea is, as soon as you have a better\nx, put it in the system.",
    "start": "2954350",
    "end": "2962010"
  },
  {
    "text": "So the storage is cut in\nhalf, because you're not saving a big old vector while\nyou compute the new one.",
    "start": "2962010",
    "end": "2972380"
  },
  {
    "text": "I'll write down\nGauss-Seidel again. So it's more up to\ndate and the effect",
    "start": "2972380",
    "end": "2978650"
  },
  {
    "text": "is that the Jacobi\neigenvalues get squared.",
    "start": "2978650",
    "end": "2984079"
  },
  {
    "text": "So you're squaring numbers\nthat are less than 1.",
    "start": "2984080",
    "end": "2990380"
  },
  {
    "text": "So Gauss-Seidel, this gives\nthe Jacobi eigenvalues squared.",
    "start": "2990380",
    "end": "3001460"
  },
  {
    "text": "The result is that if I draw\nthe same curve for Gauss-Seidel, I'll be below, right?",
    "start": "3001460",
    "end": "3008590"
  },
  {
    "text": " Essentially, a Gauss-Seidel\nstep is worth two Jacobi steps,",
    "start": "3008590",
    "end": "3015589"
  },
  {
    "start": "3010000",
    "end": "3093000"
  },
  {
    "text": "because eigenvalues\nget squared as they would do in two Jacobi steps. If I do Jacobi twice, I\nsquare its eigenvalues.",
    "start": "3015590",
    "end": "3023460"
  },
  {
    "text": "So it seems like\nabsolutely a smart move. It's not brilliant, because\nthis becomes cosine squared.",
    "start": "3023460",
    "end": "3035320"
  },
  {
    "text": "I lose the 1/2, but I'm\nstill very, very close to 1.",
    "start": "3035320",
    "end": "3041960"
  },
  {
    "text": "It still takes order N squared\niterations to get anywhere.",
    "start": "3041960",
    "end": "3047150"
  },
  {
    "text": "So Jacobi is, you\ncould say, replaced by Gauss-Seidel as\nbeing one step better,",
    "start": "3047150",
    "end": "3053760"
  },
  {
    "text": "but it's not good enough. And that's why the\nsubject kept going.",
    "start": "3053760",
    "end": "3062950"
  },
  {
    "text": "This gives rho as\n1 minus order of 1 over N, first power, where these\nwere 1 minus 1 over N squared.",
    "start": "3062950",
    "end": "3074920"
  },
  {
    "text": "So this is definitely\nfurther from 1 and then this can be\nway further from 1.",
    "start": "3074920",
    "end": "3080460"
  },
  {
    "text": "So you can see some of\nthe numerical experiments and analysis that's coming.",
    "start": "3080460",
    "end": "3086340"
  },
  {
    "text": "I'll see you Wednesday to finish\nthis and move into multigrid, which uses these guys.",
    "start": "3086340",
    "end": "3093300"
  },
  {
    "start": "3093300",
    "end": "3093860"
  }
]