[
  {
    "text": "[SQUEAKING] [RUSTLING] [CLICKING]",
    "start": "0",
    "end": "5976"
  },
  {
    "start": "5976",
    "end": "17300"
  },
  {
    "text": "SARA ELLISON: So where\nwere we last time? We had started, we\nhad talked about,",
    "start": "17300",
    "end": "22580"
  },
  {
    "text": "started a general\nconversation about estimation. And we had seen a couple\nof different examples",
    "start": "22580",
    "end": "29900"
  },
  {
    "text": "of estimators that\nseemed to have of just-- like we just thought of them\noff the top of our head.",
    "start": "29900",
    "end": "36440"
  },
  {
    "text": "And both estimators,\nremember we were estimating theta in a\nuniform 0 theta distribution.",
    "start": "36440",
    "end": "45350"
  },
  {
    "text": "And both estimators\nseemed quite reasonable. And so then, the\nnext obvious question is, well, how do we\nknow which one to use?",
    "start": "45350",
    "end": "52350"
  },
  {
    "text": "How do we decide\nbetween estimators if we have more than\none estimator available? And last time, we saw\nthe first criterion",
    "start": "52350",
    "end": "59780"
  },
  {
    "text": "that one might want to use\nto choose among estimators. And that was unbiased,\nunbiased ones.",
    "start": "59780",
    "end": "66979"
  },
  {
    "text": "So an estimator is\nunbiased for theta if the expectation\nof the estimator",
    "start": "66980",
    "end": "72020"
  },
  {
    "text": "is equal to the\nparameter we're trying to estimate for all\nvalues of the parameter,",
    "start": "72020",
    "end": "77520"
  },
  {
    "text": "all possible values\nof the parameter. And then I drew a picture. And I said that here\nis-- so remember,",
    "start": "77520",
    "end": "86010"
  },
  {
    "text": "an estimator is a function\nof random variables. So it itself is a\nrandom variable. So it has a distribution.",
    "start": "86010",
    "end": "91840"
  },
  {
    "text": "So we can talk about\ncharacteristics of that distribution. Here is a particular\ndistribution",
    "start": "91840",
    "end": "97920"
  },
  {
    "text": "of an estimator theta hat. And this one, I've drawn\nso that it's unbiased. Here's a particular distribution\nof a different theta hat, say.",
    "start": "97920",
    "end": "106920"
  },
  {
    "text": "And I've drawn it\nso it's biased. So the expectation for that.",
    "start": "106920",
    "end": "112390"
  },
  {
    "text": "The second estimator over\nthere is not equal to theta, so it's a biased estimator.",
    "start": "112390",
    "end": "119490"
  },
  {
    "text": "So let's see an example. So we've seen this\nparticular estimator before. Remember, this is uniform.",
    "start": "119490",
    "end": "126090"
  },
  {
    "text": "The random sample\nis uniform 0 theta. And we came up with the\nidea that we could just",
    "start": "126090",
    "end": "133470"
  },
  {
    "text": "compute the sample mean\nfrom that random sample, multiply it by 2. And that would be a reasonable\nestimator for theta hat,",
    "start": "133470",
    "end": "140610"
  },
  {
    "text": "or for theta, sorry. So let's see if this one is,\nin fact, biased or unbiased.",
    "start": "140610",
    "end": "147569"
  },
  {
    "text": "Well, we can just\ncompute the expectation by using the properties\nof expectation",
    "start": "147570",
    "end": "152970"
  },
  {
    "text": "that we saw a few lectures ago. So we don't have to go back to\nthe definition of expectation.",
    "start": "152970",
    "end": "159180"
  },
  {
    "text": "We just say, well, the\nexpectation of theta hat is equal to--",
    "start": "159180",
    "end": "164640"
  },
  {
    "text": "by properties of\nexpectation, we can pull out",
    "start": "164640",
    "end": "170430"
  },
  {
    "text": "the 2 and the 1/n\nfrom the expectation and from the summation.",
    "start": "170430",
    "end": "177960"
  },
  {
    "text": "And then we plug\nin the expectation",
    "start": "177960",
    "end": "183730"
  },
  {
    "text": "of x sub i, which is just\nequal to theta over 2. Everyone knows where\nthat came from.",
    "start": "183730",
    "end": "190420"
  },
  {
    "text": "And then we just\ndo this summation. And we get the 2's cancel.",
    "start": "190420",
    "end": "195700"
  },
  {
    "text": "And we get that this\nis equal to theta. So unbiased estimator,\nso that's a good thing.",
    "start": "195700",
    "end": "202150"
  },
  {
    "start": "202150",
    "end": "208390"
  },
  {
    "text": "How about our other estimators? So the other option\nthat we had talked about was just using the\nnth order statistic.",
    "start": "208390",
    "end": "215290"
  },
  {
    "text": "And that also seemed reasonable. So let's see what we can say\nabout whether this one is",
    "start": "215290",
    "end": "221349"
  },
  {
    "text": "biased or unbiased. Any guesses? ",
    "start": "221350",
    "end": "227530"
  },
  {
    "text": "STUDENT: It becomes more\nunbiased as n increases.",
    "start": "227530",
    "end": "234090"
  },
  {
    "text": "SARA ELLISON: So you\nhave the right intuition, but you haven't expressed\nit in the right way. So basically, what's happening\nis that it will be biased.",
    "start": "234090",
    "end": "244800"
  },
  {
    "text": "We're going to see that. And the amount of the bias\nis going to get smaller as n gets larger.",
    "start": "244800",
    "end": "250850"
  },
  {
    "text": " And do you want to do you want\nto share your intuition for why",
    "start": "250850",
    "end": "257160"
  },
  {
    "text": "it's biased? STUDENT: We talked\nabout it last time. We could take more draws\nthan the probability",
    "start": "257160",
    "end": "262290"
  },
  {
    "text": "that the highest draw is\nclosest to theta increases. SARA ELLISON: That's\nright, that's right.",
    "start": "262290",
    "end": "268009"
  },
  {
    "text": "So this estimator is\nsort of getting close. But it's but it's\nalways going to be biased for a finite sample\nbecause the nth order",
    "start": "268010",
    "end": "279000"
  },
  {
    "text": "statistic is always less\nthan or equal to theta. It's never greater than theta. It's always less than\nor equal to theta.",
    "start": "279000",
    "end": "285639"
  },
  {
    "text": "And it's equal to theta\nwith 0 probability. So that's sort of\nthe way that you",
    "start": "285640",
    "end": "292425"
  },
  {
    "text": "know this is going to be a\nbiased estimator, the intuition I guess, that I use. So let's verify this.",
    "start": "292425",
    "end": "300060"
  },
  {
    "text": "We can't just use the\nproperties of expectation like we did on the last slide\nto calculate the expectation",
    "start": "300060",
    "end": "307500"
  },
  {
    "text": "of this estimator. And so we're going\nto do it directly, using the definition\nof expectation.",
    "start": "307500",
    "end": "313180"
  },
  {
    "text": "And in order to\ndo that, we first need the PDF of the\nnth order statistic. Well, we've seen that before.",
    "start": "313180",
    "end": "319530"
  },
  {
    "text": "So we'll just go back\nto previous lecture and recall that this\nis the general--",
    "start": "319530",
    "end": "328800"
  },
  {
    "text": "that's the general formula\nfor the PDF of the nth order statistic from some\ndistribution, F. Now,",
    "start": "328800",
    "end": "337710"
  },
  {
    "text": "we have a specific distribution\nhere, uniform 0 theta.",
    "start": "337710",
    "end": "343360"
  },
  {
    "text": "So we'll plug in that specific\nPDF, plug in the uniform PDF",
    "start": "343360",
    "end": "350189"
  },
  {
    "text": "here and the uniform CDF here. And then we get this. ",
    "start": "350190",
    "end": "356220"
  },
  {
    "text": "And then it just\nsimplifies to this. So now we have the PDF of\nthe nth order statistic.",
    "start": "356220",
    "end": "363360"
  },
  {
    "text": "We'll be able to calculate\nits expectation with that. And so what we did is\nI just took the PDF",
    "start": "363360",
    "end": "370140"
  },
  {
    "text": "from the previous\nslide and plugged it into the definition\nof expectation.",
    "start": "370140",
    "end": "376020"
  },
  {
    "text": "Remember, the expectation\nof a random variable is just equal to-- that's continuous. It's just equal to the\nintegral of x times the PDF.",
    "start": "376020",
    "end": "384939"
  },
  {
    "text": "So that's exactly\nwhat I did there. I solved the integral. And just a couple\nof steps of math",
    "start": "384940",
    "end": "393150"
  },
  {
    "text": "later, I get that it's equal\nto n over n plus 1 times theta. ",
    "start": "393150",
    "end": "399610"
  },
  {
    "text": "So not only is our\nintuition that it's biased",
    "start": "399610",
    "end": "405819"
  },
  {
    "text": "is confirmed here,\nbut also our intuition that the bias is going down\nas the sample size is getting",
    "start": "405820",
    "end": "411160"
  },
  {
    "text": "bigger is also confirmed.  This makes sense?",
    "start": "411160",
    "end": "418450"
  },
  {
    "text": "So having an unbiased\nestimator is great. Is that the only\nthing we care about?",
    "start": "418450",
    "end": "424020"
  },
  {
    "text": "I think you guessed that\nthe answer is probably no. It's not the only\nthing we care about. Oh, so yeah, I guess this\nis what I already said.",
    "start": "424020",
    "end": "430710"
  },
  {
    "text": "Not surprising-- the estimator\nwill always be less than or equal to theta.",
    "start": "430710",
    "end": "436870"
  },
  {
    "text": "Oh, sorry. Before we get to the\nsecond criterion, I just want to give you a\ncouple of useful results",
    "start": "436870",
    "end": "446470"
  },
  {
    "text": "about unbiased estimators. So the first result\nis that if we",
    "start": "446470",
    "end": "452680"
  },
  {
    "text": "have a sample mean\nfor an iid sample, then that sample mean\nis an unbiased estimator",
    "start": "452680",
    "end": "458590"
  },
  {
    "text": "for the population mean, for\nthe mean of the underlying population.",
    "start": "458590",
    "end": "464780"
  },
  {
    "text": "So that's always\ngoing to be true. For an iid sample,\nthe sample mean",
    "start": "464780",
    "end": "470150"
  },
  {
    "text": "is unbiased for the\npopulation mean. Now, we actually\nalready knew this. We didn't use the word unbiased.",
    "start": "470150",
    "end": "476870"
  },
  {
    "text": "But remember when we\nintroduced sample mean, we calculated the expectation\nof the sample mean.",
    "start": "476870",
    "end": "483250"
  },
  {
    "text": "You guys remember\nthat calculation? We calculated the expectation\nof the sample mean, and it was equal to\nmu, the underlying",
    "start": "483250",
    "end": "491610"
  },
  {
    "text": "mean or the population mean. So in fact, we\nalready knew that.",
    "start": "491610",
    "end": "496890"
  },
  {
    "text": "Another result that we haven't\nseen before because I haven't even defined what a sample\nvariance is, but it's",
    "start": "496890",
    "end": "504060"
  },
  {
    "text": "a useful result nonetheless, is\nthat if we have an iid sample",
    "start": "504060",
    "end": "509070"
  },
  {
    "text": "and we compute this\nestimator here, it's called the sample variance.",
    "start": "509070",
    "end": "515179"
  },
  {
    "text": "This is going to be unbiased\nfor the population variance. ",
    "start": "515179",
    "end": "521460"
  },
  {
    "text": "So I am leaving out\nthe proof of that. It's kind of a little\nbit of a fussy proof.",
    "start": "521460",
    "end": "529080"
  },
  {
    "text": "And I don't think\nit's very instructive. But if anyone's interested,\nhappy to show you.",
    "start": "529080",
    "end": "536270"
  },
  {
    "text": "Yep? STUDENT: Before we\nmove on, I wanted to ask a question\nabout this, the sample mean for the examples\nof unbiased population.",
    "start": "536270",
    "end": "543828"
  },
  {
    "text": "Does the fact that\nit's unbiased mean that you're getting the\nactual population mean when you do the sample mean, or no?",
    "start": "543828",
    "end": "550227"
  },
  {
    "text": "I don't think that's\nwhat's happening. SARA ELLISON: No. So what it means is that the\nsample mean has a distribution.",
    "start": "550227",
    "end": "557710"
  },
  {
    "text": "And that distribution,\nthe expectation of that distribution\nis the population mean.",
    "start": "557710",
    "end": "563010"
  },
  {
    "text": "So here, there's\nsome distribution. Maybe it's uniform, maybe\nit's normal, whatever.",
    "start": "563010",
    "end": "572600"
  },
  {
    "text": "There's some distribution\nthat our random sample is governed by, a random sample\nis drawn by or drawn from.",
    "start": "572600",
    "end": "581990"
  },
  {
    "text": "And then, we get\na random sample. We compute the sample mean. And the distribution\nof the sample",
    "start": "581990",
    "end": "590300"
  },
  {
    "text": "mean-- so if we think of the\nsample mean as an estimator,",
    "start": "590300",
    "end": "597860"
  },
  {
    "text": "it's going to have\nexpectation, the sample mean. Any particular realization\nof that estimator--",
    "start": "597860",
    "end": "605339"
  },
  {
    "text": "so like if we have the\nnumbers, the realizations for the random sample\nand we plug them",
    "start": "605340",
    "end": "612110"
  },
  {
    "text": "in to the function\nfor the estimator, then basically,\nwhat we're doing is we're getting some realization\nfrom this distribution.",
    "start": "612110",
    "end": "619470"
  },
  {
    "text": "So depending on how\ntightly distributed it is around the\npopulation mean,",
    "start": "619470",
    "end": "628190"
  },
  {
    "text": "it could be very close to\nthis with high probability. ",
    "start": "628190",
    "end": "637839"
  },
  {
    "text": "Questions about this? So these are just\ntwo useful facts that we'll refer\nto in the future.",
    "start": "637840",
    "end": "645010"
  },
  {
    "text": " So now, is unbiasness all we\ncare about in an estimator?",
    "start": "645010",
    "end": "653170"
  },
  {
    "text": "The answer is no. So in particular,\nthere are other aspects",
    "start": "653170",
    "end": "658720"
  },
  {
    "text": "of distributions\nof the estimators that we might care about. We might care about-- I just alluded to\nthis-- how tightly",
    "start": "658720",
    "end": "666009"
  },
  {
    "text": "distributed it is around\nthe unknown parameter. We could have an\nestimator that has a very diffuse distribution.",
    "start": "666010",
    "end": "673000"
  },
  {
    "text": "And that estimator, even if its\nexpectation is the parameter",
    "start": "673000",
    "end": "678730"
  },
  {
    "text": "that we're trying to estimate,\neven if that's its expectation, in other words, even\nif it's unbiased, it might not be\nvery useful to us",
    "start": "678730",
    "end": "685360"
  },
  {
    "text": "if it has lots of\nprobability far away from the real parameter. And that's what\nefficiency is about.",
    "start": "685360",
    "end": "692829"
  },
  {
    "text": "So given two unbiased\nestimators, theta-1 hat and theta-2 hat, theta-1 hat is\nmore efficient than theta-2 hat",
    "start": "692830",
    "end": "700480"
  },
  {
    "text": "if for a given sample size,\nthe variance of theta-1 hat",
    "start": "700480",
    "end": "705550"
  },
  {
    "text": "is less than the\nvariance of theta-2 hat. So let me show you a picture.",
    "start": "705550",
    "end": "711569"
  },
  {
    "text": "So here, we have two\nunbiased estimators. We have an efficient\none, whose distribution",
    "start": "711570",
    "end": "718640"
  },
  {
    "text": "is pretty tightly\ndistributed or concentrated around the unknown parameter. And we have an inefficient\none, whose distribution",
    "start": "718640",
    "end": "725630"
  },
  {
    "text": "is much more spread out. And we're going to prefer\nthe efficient estimator. ",
    "start": "725630",
    "end": "734130"
  },
  {
    "text": "Is that clear to\neveryone, why we prefer-- ",
    "start": "734130",
    "end": "739500"
  },
  {
    "text": "so note that this definition\nhas defined efficiency here just for unbiased estimators.",
    "start": "739500",
    "end": "744840"
  },
  {
    "text": "The notion of efficiency\nis broader than that. So it can exist for, sort of\nbroader classes of estimators",
    "start": "744840",
    "end": "750210"
  },
  {
    "text": "as well. I'm not going to give\nyou a formal definition. But just think that if\nan estimator has a larger",
    "start": "750210",
    "end": "756509"
  },
  {
    "text": "variance, it's less\nefficient than an estimator with a smaller variance,\nand we typically",
    "start": "756510",
    "end": "763230"
  },
  {
    "text": "define that in different\nclasses of estimators. ",
    "start": "763230",
    "end": "770860"
  },
  {
    "text": "So fine. Suppose we have an\nefficient estimator",
    "start": "770860",
    "end": "776510"
  },
  {
    "text": "and/or an estimator that\nhas a small variance. And then we have another\nestimator it has a larger",
    "start": "776510",
    "end": "782750"
  },
  {
    "text": "variance, but it's unbiased. So sorry, the first one is\nbiased, has a small variance.",
    "start": "782750",
    "end": "790310"
  },
  {
    "text": "The second one is unbiased\nand has a large variance. How do we know\nwhich one to choose?",
    "start": "790310",
    "end": "795370"
  },
  {
    "text": "Well, there's no right answer.  I can't of tell you the\ntheorem that says, well,",
    "start": "795370",
    "end": "802960"
  },
  {
    "text": "you always need to choose this\nestimator over the other one. But there might\nbe sort of other--",
    "start": "802960",
    "end": "811960"
  },
  {
    "text": "aside from just kind of picking\nyour favorite estimator, there might be kind of\na more routinized way",
    "start": "811960",
    "end": "818920"
  },
  {
    "text": "to trade off bias and\nefficiency in estimation. And one way to do\nthis is minimum",
    "start": "818920",
    "end": "827110"
  },
  {
    "text": "or is mean squared error. So I'm going to define\nmean squared error",
    "start": "827110",
    "end": "835000"
  },
  {
    "text": "as the expectation of theta\nhat minus theta squared.",
    "start": "835000",
    "end": "840610"
  },
  {
    "text": "And that can be rewritten as\nthe variance of theta hat. So we can think\nof that basically,",
    "start": "840610",
    "end": "847060"
  },
  {
    "text": "as the efficiency, a measure of\nthe efficiency of the estimator plus the expectation of theta\nhat minus theta quantity",
    "start": "847060",
    "end": "855800"
  },
  {
    "text": "squared. And that is in fact, the bias. That is, sort of this difference\nhere is defined as the bias,",
    "start": "855800",
    "end": "865880"
  },
  {
    "text": "so that last term\nis the bias squared. So for unbiased estimators,\nthat quantity is going to be 0.",
    "start": "865880",
    "end": "872600"
  },
  {
    "text": "For biased estimators,\nit's going to be positive. So mean squared error\nis a way to trade off",
    "start": "872600",
    "end": "881150"
  },
  {
    "text": "bias and efficiency. You can choose an estimator\nthat is the minimum mean squared error estimator.",
    "start": "881150",
    "end": "887060"
  },
  {
    "text": "And then, that sort\nof allows, basically allows this explicit trade off\nbetween bias and efficiency.",
    "start": "887060",
    "end": "894050"
  },
  {
    "text": " So I should point out,\nit is a reasonable way",
    "start": "894050",
    "end": "901769"
  },
  {
    "text": "to trade off bias\nand efficiency. It's not the only one. And like I said,\nthere's no right answer",
    "start": "901770",
    "end": "908070"
  },
  {
    "text": "when it comes to choosing\namong estimators that",
    "start": "908070",
    "end": "913800"
  },
  {
    "text": "are not dominated in all ways. ",
    "start": "913800",
    "end": "922100"
  },
  {
    "text": "And I'll finally mention\none additional criterion. ",
    "start": "922100",
    "end": "929930"
  },
  {
    "text": "Theta hat is a consistent\nestimator for theta if the following is true--",
    "start": "929930",
    "end": "935790"
  },
  {
    "text": "you take the absolute value,\nThe absolute difference between theta and theta hat.",
    "start": "935790",
    "end": "942880"
  },
  {
    "text": "And the probability of that\nabsolute difference being less than some small number\ndelta, goes to 1",
    "start": "942880",
    "end": "954760"
  },
  {
    "text": "as n goes to infinity. So practically speaking,\nwhat does this mean?",
    "start": "954760",
    "end": "959960"
  },
  {
    "text": "Well, first of all, let\nme draw you a picture, So this is what a\nconsistent estimator looks",
    "start": "959960",
    "end": "967690"
  },
  {
    "text": "like as n goes to infinity. So basically, you have\na consistent estimator. I've drawn this one to\nbe unbiased for all n.",
    "start": "967690",
    "end": "975580"
  },
  {
    "text": "It doesn't have to be unbiased. But that's just the way\nI drew the picture here. So basically, you\nhave an estimator,",
    "start": "975580",
    "end": "981310"
  },
  {
    "text": "and it has a distribution. And as n goes to\ninfinity, the distribution",
    "start": "981310",
    "end": "986770"
  },
  {
    "text": "is getting more and\nmore concentrated around the true parameter. And in fact, in the\nlimit, it collapses",
    "start": "986770",
    "end": "995079"
  },
  {
    "text": "to a single point at\nthe true parameter as n goes to infinity. So this is what's known\nas a consistent estimator.",
    "start": "995080",
    "end": "1005230"
  },
  {
    "text": "So let me just emphasize,\nI've drawn this picture for an estimator\nthat's unbiased.",
    "start": "1005230",
    "end": "1012990"
  },
  {
    "text": "A consistent estimator\ndoesn't have to be unbiased. I could have drawn all of these\ndistributions having bias,",
    "start": "1012990",
    "end": "1020220"
  },
  {
    "text": "but the bias is going to 0. ",
    "start": "1020220",
    "end": "1027470"
  },
  {
    "text": "So in the interest of time, I\nhaven't included these examples",
    "start": "1027470",
    "end": "1035569"
  },
  {
    "text": "in the lecture notes. But I will tell you,\nin case you're curious, that the two estimators that\nwe looked at for the uniform 0",
    "start": "1035569",
    "end": "1043880"
  },
  {
    "text": "theta random sample,\nthe unbiased estimator,",
    "start": "1043880",
    "end": "1050030"
  },
  {
    "text": "the one that was 2\ntimes the sample mean, was much less efficient than\nthe biased estimator, the one",
    "start": "1050030",
    "end": "1059360"
  },
  {
    "text": "that the nth order statistic. And so in that case, you might\nwant to choose the nth order",
    "start": "1059360",
    "end": "1067550"
  },
  {
    "text": "statistic, just because it's for\nreasonably-sized random sample.",
    "start": "1067550",
    "end": "1073560"
  },
  {
    "text": "It's just its\ndistribution is going to be much more\ntightly distributed",
    "start": "1073560",
    "end": "1079460"
  },
  {
    "text": "near theta than 2 times\nthe random sample or two times the sample mean. ",
    "start": "1079460",
    "end": "1092230"
  },
  {
    "text": "So the criteria that\nI've gone through, they are probably the\nmost important reasons for choosing one\nestimator over another.",
    "start": "1092230",
    "end": "1100419"
  },
  {
    "text": "But they don't necessarily have\nto be the only considerations. So sometimes estimators can be\nreally difficult to compute.",
    "start": "1100420",
    "end": "1109660"
  },
  {
    "text": "They can be used, sort of hours\nof computer time or something like that. And so you might choose\nan estimator, just",
    "start": "1109660",
    "end": "1115410"
  },
  {
    "text": "based on computational\nsimplicity. And that's not an\nunreasonable thing to do.",
    "start": "1115410",
    "end": "1122440"
  },
  {
    "text": "You might also\nchoose an estimator based on how robust it is. What do I mean by robust?",
    "start": "1122440",
    "end": "1128950"
  },
  {
    "text": "Well basically, an\nestimator is robust if it still will\ndo a decent job,",
    "start": "1128950",
    "end": "1135940"
  },
  {
    "text": "perform well for estimating\nthe unknown parameter, even if some of your\nassumptions, underlying",
    "start": "1135940",
    "end": "1141490"
  },
  {
    "text": "assumptions are incorrect. So for instance, let's suppose\nthat we want to estimate mu.",
    "start": "1141490",
    "end": "1154640"
  },
  {
    "text": "And we think that\nthe random sample",
    "start": "1154640",
    "end": "1160150"
  },
  {
    "text": "is drawn from a distribution\nthat looks like this. But in fact, it's drawn\nfrom a distribution",
    "start": "1160150",
    "end": "1167170"
  },
  {
    "text": "that looks like this. ",
    "start": "1167170",
    "end": "1172890"
  },
  {
    "text": "So this is still the mean. Mu is still the mean\nof this distribution. But we've assumed\nthat we're drawing",
    "start": "1172890",
    "end": "1181078"
  },
  {
    "text": "from this distribution,\nwhen in fact, we're drawing from that one. A robust estimator will still\ndo a good job of estimating mu,",
    "start": "1181078",
    "end": "1188360"
  },
  {
    "text": "even if our assumptions about\nthe underlying distribution are wrong. And so sometimes we\nchoose estimators",
    "start": "1188360",
    "end": "1196700"
  },
  {
    "text": "that are robust to\ndifferent assumptions if we don't have a good-- if we don't have a lot of\nconfidence about our underlying",
    "start": "1196700",
    "end": "1203840"
  },
  {
    "text": "assumptions. ",
    "start": "1203840",
    "end": "1209250"
  },
  {
    "text": "And yeah, so I guess this is an\nexample of a robust estimator that we talked about\nlast time briefly.",
    "start": "1209250",
    "end": "1216060"
  },
  {
    "text": "It turns out that the 2 times\nthe sample median estimator from the uniform example will\nhave less bias than the 2 times",
    "start": "1216060",
    "end": "1227130"
  },
  {
    "text": "the sample mean\nestimator if we've misspecified the\ntail probabilities",
    "start": "1227130",
    "end": "1232470"
  },
  {
    "text": "in that uniform example. So that's an example of an\nestimator that might actually",
    "start": "1232470",
    "end": "1242830"
  },
  {
    "text": "not be as good as the\nother two estimators we saw in terms of\nbias and efficiency,",
    "start": "1242830",
    "end": "1247870"
  },
  {
    "text": "but might have\nthis other property of robustness that could\ncome in handy at some point.",
    "start": "1247870",
    "end": "1254390"
  },
  {
    "text": "Yes? STUDENT: Wouldn't\nscale probabilities be underestimated or\noverestimated for that to hold?",
    "start": "1254390",
    "end": "1262850"
  },
  {
    "text": "Or does it not matter? SARA ELLISON: So\nrobustness is basically, it's just a kind of a\nproperty of an estimator that",
    "start": "1262850",
    "end": "1272390"
  },
  {
    "text": "says it still performs well if\nyour assumptions are incorrect. And it could be\nthat the estimator",
    "start": "1272390",
    "end": "1281390"
  },
  {
    "text": "is only robust to\nmisspecifications in one direction and\nnot the other direction.",
    "start": "1281390",
    "end": "1288240"
  },
  {
    "text": "That's possible. But just the sort of\ngeneral definition of robust",
    "start": "1288240",
    "end": "1294560"
  },
  {
    "text": "is that it's not sensitive to-- STUDENT: The second\npart of this slide",
    "start": "1294560",
    "end": "1300770"
  },
  {
    "text": "is it's showing that that's\njust an example of what could happen. It's not necessarily\nalways true.",
    "start": "1300770",
    "end": "1306950"
  },
  {
    "text": "SARA ELLISON:\nExactly, that's right. Yes? STUDENT: In that example, what\nwould be the wrong distribution",
    "start": "1306950",
    "end": "1316263"
  },
  {
    "text": "[INAUDIBLE]? SARA ELLISON: So let's\nsuppose-- so we came up",
    "start": "1316263",
    "end": "1323130"
  },
  {
    "text": "with this 2 times the\nsample median estimator in the uniform 0\nto theta example.",
    "start": "1323130",
    "end": "1329620"
  },
  {
    "text": "So let's suppose instead\nof uniform 0 to theta, let's suppose the distribution\nlooked like that instead.",
    "start": "1329620",
    "end": "1348120"
  },
  {
    "text": "And we carried through all of\nthe analysis of the estimator as if this were\nthe distribution.",
    "start": "1348120",
    "end": "1356250"
  },
  {
    "text": "Then if we use the 2 times\nthe sample median estimator, we're probably going to do\na better job estimating.",
    "start": "1356250",
    "end": "1363780"
  },
  {
    "text": "I would have to work\nit out to be sure. But in that example,\nwe're almost certainly",
    "start": "1363780",
    "end": "1369030"
  },
  {
    "text": "going to do a better job of\nestimating theta with the 2 times the median estimator than\nthe 2 times the sample mean",
    "start": "1369030",
    "end": "1375059"
  },
  {
    "text": "estimator. ",
    "start": "1375060",
    "end": "1384600"
  },
  {
    "text": "So now, we know, at\nleast roughly speaking, how to figure out if\nan estimator is good",
    "start": "1384600",
    "end": "1390720"
  },
  {
    "text": "once we have one. So we have lots of different\ncriteria to consider and some good guidelines.",
    "start": "1390720",
    "end": "1399150"
  },
  {
    "text": "But how do we get an\nestimator in the first place? So the example that I went over\nin class a couple lectures ago,",
    "start": "1399150",
    "end": "1408730"
  },
  {
    "text": "we just kind of dreamed them\nup off the top of our head, which it turns\nout, can sometimes",
    "start": "1408730",
    "end": "1417029"
  },
  {
    "text": "result in quite\nreasonable estimators. But it might be a\nlittle comforting to know that you\ndon't always just have",
    "start": "1417030",
    "end": "1423660"
  },
  {
    "text": "to be able to dream things\nup off the top of your head. So there are two main frameworks\nfor deriving estimators.",
    "start": "1423660",
    "end": "1430320"
  },
  {
    "text": "One is called the\nMethod of Moments. And the second is called\nMaximum Likelihood Estimation.",
    "start": "1430320",
    "end": "1436919"
  },
  {
    "text": "And in fact, we've\nseen examples of both. So 2 times the sample mean\nin the uniform example",
    "start": "1436920",
    "end": "1445620"
  },
  {
    "text": "is a method of\nmoments estimator. And the nth order statistic\nin the uniform 0 theta",
    "start": "1445620",
    "end": "1454650"
  },
  {
    "text": "distribution is a Maximum\nLikelihood estimator. ",
    "start": "1454650",
    "end": "1460500"
  },
  {
    "text": "And then as I said\nbefore, a third framework you can think of\nis to just come up",
    "start": "1460500",
    "end": "1465690"
  },
  {
    "text": "with something clever\noff the top of your head. And that sometimes works. ",
    "start": "1465690",
    "end": "1475980"
  },
  {
    "text": "So let me just give\nyou, a sort of sketch out what the method\nof moments framework",
    "start": "1475980",
    "end": "1484560"
  },
  {
    "text": "for deriving estimators is,\ngive you an example of one, and then do the same\nfor maximum likelihood.",
    "start": "1484560",
    "end": "1492120"
  },
  {
    "text": "So the Method of Moments\nwas developed in 1894",
    "start": "1492120",
    "end": "1497250"
  },
  {
    "text": "by Karl Pearson, that\nmany, many people consider to be the father\nof mathematical statistics.",
    "start": "1497250",
    "end": "1504460"
  },
  {
    "text": "There's a picture of him. And first, to tell you what\nMethod of Moments estimation",
    "start": "1504460",
    "end": "1512200"
  },
  {
    "text": "is, I've got to define\na couple of things. So I've got to define\npopulation moments. And I've got to\ndefine sample moments.",
    "start": "1512200",
    "end": "1519070"
  },
  {
    "text": "So population moment, we haven't\nused the terminology population",
    "start": "1519070",
    "end": "1524470"
  },
  {
    "text": "moment. But we've already seen the\nfirst population moment about the origin, which\nis just the expectation.",
    "start": "1524470",
    "end": "1531730"
  },
  {
    "text": "The second population\nmoment about the origin is the expectation of x\nsquared, third population",
    "start": "1531730",
    "end": "1538540"
  },
  {
    "text": "moment, expectation of x cubed. So for any distribution,\nwe can just",
    "start": "1538540",
    "end": "1544780"
  },
  {
    "text": "know we can calculate these as\nfunctions of the parameters. Or we can look them\nup on Wikipedia.",
    "start": "1544780",
    "end": "1552010"
  },
  {
    "text": "Or we can look them up in\nthe back of our statistics book or whatever. But we can get the\npopulation moments",
    "start": "1552010",
    "end": "1561159"
  },
  {
    "text": "as functions of the parameters\nof the distributions. So as a function of parameters\nof the distribution,",
    "start": "1561160",
    "end": "1569350"
  },
  {
    "text": "what's the first\npopulation moment of a normal distribution?",
    "start": "1569350",
    "end": "1574750"
  },
  {
    "text": "STUDENT: Mu. SARA ELLISON: Mu, how\nabout of a Poisson? Does anyone remember that? STUDENT: Lambda. SARA ELLISON: Lambda.",
    "start": "1574750",
    "end": "1580330"
  },
  {
    "text": "How about of a\nuniform 0 to theta?",
    "start": "1580330",
    "end": "1585909"
  },
  {
    "text": "STUDENT: Theta over 2 SARA ELLISON: Theta\nover 2, right. So you guys get the idea. The sample moments,\nwe've seen obviously,",
    "start": "1585910",
    "end": "1594240"
  },
  {
    "text": "the first sample moment. It's just the sample mean. The second sample moment\nis the same thing.",
    "start": "1594240",
    "end": "1602040"
  },
  {
    "text": "But we're going to square\nall of the observations instead and so forth.",
    "start": "1602040",
    "end": "1608120"
  },
  {
    "text": "You have a question? STUDENT: Going back\nto the sample variance that you showed us, that\nwas divided by n minus 1.",
    "start": "1608120",
    "end": "1615260"
  },
  {
    "text": "So is there any inconsistency? Or how do you reconcile the fact\nthat this sample moment is odd?",
    "start": "1615260",
    "end": "1624200"
  },
  {
    "text": "SARA ELLISON: So\nthe sample moment that I showed you was\nnot a sample moment.",
    "start": "1624200",
    "end": "1630600"
  },
  {
    "text": "Well, first of all, it\nwas not about the origin. It was about the mean.",
    "start": "1630600",
    "end": "1637050"
  },
  {
    "text": "So I'm defining sample\nmoments here as-- I left it out.",
    "start": "1637050",
    "end": "1642320"
  },
  {
    "text": "But I should have said sample\nmoments about the origin. So there are two different\ntypes of moments, sample moments",
    "start": "1642320",
    "end": "1648127"
  },
  {
    "text": "for a distribution. One is about the mean, and\none is about the origin. These ones are about the origin. And the other thing\nis that to be honest,",
    "start": "1648128",
    "end": "1655760"
  },
  {
    "text": "I'm not sure the sample\nvariance may not--",
    "start": "1655760",
    "end": "1662990"
  },
  {
    "text": "I believe that actually, the\ndefinition of the second sample",
    "start": "1662990",
    "end": "1668660"
  },
  {
    "text": "moment about the mean is 1/n. It's not 1 over n minus 1.",
    "start": "1668660",
    "end": "1674000"
  },
  {
    "text": "So the sample variance\nis defined differently than the second sample\nmoment about the mean.",
    "start": "1674000",
    "end": "1681030"
  },
  {
    "text": "And the reason why is\nthat the second sample moment about the mean is\nactually a biased estimator",
    "start": "1681030",
    "end": "1686070"
  },
  {
    "text": "of the variance. Yeah, thanks for asking\nthat question, but yeah,",
    "start": "1686070",
    "end": "1692100"
  },
  {
    "text": "there's no inconsistency. So we've got these\npopulation moments.",
    "start": "1692100",
    "end": "1698655"
  },
  {
    "text": "You can just think of\nthese as like functions of the parameters. And then, we've got\nthe sample moments.",
    "start": "1698655",
    "end": "1704350"
  },
  {
    "text": "And we're going to use those\ntogether to derive estimators.",
    "start": "1704350",
    "end": "1709919"
  },
  {
    "text": "So to estimate a parameter,\nwe equate the first population moment, which is, as I said,\na function of the parameter,",
    "start": "1709920",
    "end": "1716900"
  },
  {
    "text": "to the first sample moment. And then we just solve\nfor the parameter. So let's do a quick example.",
    "start": "1716900",
    "end": "1724390"
  },
  {
    "text": "So this is an example,\nwe've already seen. The first population moment,\nexpectation of x of a uniform 0",
    "start": "1724390",
    "end": "1731940"
  },
  {
    "text": "to theta, is just theta over 2. And the first sample moment\nis just 1 over n times the sum",
    "start": "1731940",
    "end": "1737940"
  },
  {
    "text": "of the x's. So we equate those two things. We have to now stick\na hat on the theta,",
    "start": "1737940",
    "end": "1745110"
  },
  {
    "text": "because once we\nequate the sample moment and the\npopulation moment,",
    "start": "1745110",
    "end": "1750450"
  },
  {
    "text": "now we're solving for an\nestimator, not the parameter. So we stick a hat on it, and\nthen we solve for theta hat.",
    "start": "1750450",
    "end": "1758850"
  },
  {
    "text": "So that's exactly the\nestimator we saw before. And this is just exhibiting\nthat it is, in fact,",
    "start": "1758850",
    "end": "1764049"
  },
  {
    "text": "a method of moments estimator. ",
    "start": "1764050",
    "end": "1772490"
  },
  {
    "text": "What if you have more than\none parameter estimate? So for instance, what if you\nhave a normal distribution,",
    "start": "1772490",
    "end": "1779330"
  },
  {
    "text": "and you want to estimate of\nboth the mean and the variance",
    "start": "1779330",
    "end": "1785630"
  },
  {
    "text": "from the normal distribution? Or you have a\nbinomial distribution,",
    "start": "1785630",
    "end": "1792780"
  },
  {
    "text": "you want n and p or\nsomething like that. Well, no problem. So the Method of Moments\ncan accommodate that.",
    "start": "1792780",
    "end": "1800160"
  },
  {
    "text": "You just use as many\nsample and population moments as necessary.",
    "start": "1800160",
    "end": "1805240"
  },
  {
    "text": "So each pair of a sample\nmoment and a population moment, each one of those is\ncalled a moment condition.",
    "start": "1805240",
    "end": "1813510"
  },
  {
    "text": "And then, if you have k\nparameters to estimate, you just have k\nmoment conditions.",
    "start": "1813510",
    "end": "1818789"
  },
  {
    "text": "And then that\nbasically means you'll have k equations in k\nunknowns, the k unknowns",
    "start": "1818790",
    "end": "1825750"
  },
  {
    "text": "being all of the parameters\nwith hats on them. You're solving for estimators\nfor all those parameters.",
    "start": "1825750",
    "end": "1831360"
  },
  {
    "text": "And so then, you just solve\nthose k moment conditions for the k estimators.",
    "start": "1831360",
    "end": "1837460"
  },
  {
    "start": "1837460",
    "end": "1842770"
  },
  {
    "text": "So the second framework-- the idea for Maximum\nLikelihood Estimation",
    "start": "1842770",
    "end": "1849970"
  },
  {
    "text": "has been around for\na very long time. And if you try to figure\nout what its origin is,",
    "start": "1849970",
    "end": "1856570"
  },
  {
    "text": "it's really a little unclear. I think that a lot of\nhistorians of statistics",
    "start": "1856570",
    "end": "1865360"
  },
  {
    "text": "would attribute the idea\nto Lagrange in the 1770s. But the basic idea\nprobably predated that.",
    "start": "1865360",
    "end": "1874120"
  },
  {
    "text": "And sort of the analytics like\nproving that Maximum Likelihood Estimation was a\nreasonable thing to do,",
    "start": "1874120",
    "end": "1879910"
  },
  {
    "text": "that sort of came much later\naround 1930 with RA Fisher.",
    "start": "1879910",
    "end": "1885400"
  },
  {
    "text": "And there's those guys. You can probably figure\nout which one is from 1770s and which one is from the 1930s.",
    "start": "1885400",
    "end": "1893740"
  },
  {
    "text": "So what is a Maximum\nLikelihood Estimator. Well, it's the value theta hat,\nwhich most likely would have",
    "start": "1893740",
    "end": "1902110"
  },
  {
    "text": "generated the observed sample. That's why it's called\nmaximum likelihood.",
    "start": "1902110",
    "end": "1909630"
  },
  {
    "text": "So what exactly does this mean? How do we find it? ",
    "start": "1909630",
    "end": "1916650"
  },
  {
    "text": "So let's think of it this way. So this is not-- this is just a motivation.",
    "start": "1916650",
    "end": "1922590"
  },
  {
    "text": "This is not exactly. This is not, in fact, how\nyou find a maximum likelihood estimate, but this is going\nto serve as motivation.",
    "start": "1922590",
    "end": "1930120"
  },
  {
    "text": "So let's suppose we\nhave a random sample. And we know that\nour random sample",
    "start": "1930120",
    "end": "1939930"
  },
  {
    "text": "is from some particular\ndistribution. But we don't know which member\nor some particular family",
    "start": "1939930",
    "end": "1946182"
  },
  {
    "text": "of distributions. But we don't know\nwhich member it is. So in other words, we know\nit has a beta distribution. But we don't know\nwhat the parameter",
    "start": "1946182",
    "end": "1952710"
  },
  {
    "text": "of the beta\ndistribution is, or we know it's a normal\ndistribution, but we don't know what mu and sigma squared are.",
    "start": "1952710",
    "end": "1959730"
  },
  {
    "text": "So you can think of being\nable to create a histogram",
    "start": "1959730",
    "end": "1964740"
  },
  {
    "text": "with our random sample. And this is sort of the\nempirical counterpart of the PDF. ",
    "start": "1964740",
    "end": "1973200"
  },
  {
    "text": "On the other side, you\ncan think of taking this family of distributions\nthat we're assuming",
    "start": "1973200",
    "end": "1979230"
  },
  {
    "text": "or that we know that our\nrandom sample has come from, and varying the\nparameter or parameters",
    "start": "1979230",
    "end": "1986100"
  },
  {
    "text": "in that distribution, that\nfamily of distributions. And we get all kinds of\ndifferent possibilities here.",
    "start": "1986100",
    "end": "1993160"
  },
  {
    "text": "And presumably, we get a\ncontinuum of possibilities. I've just drawn-- what is this? I've drawn five different\npossibilities here.",
    "start": "1993160",
    "end": "2000440"
  },
  {
    "text": "But we have sort of an infinite\nnumber of possibilities as we vary the\nparameter or parameters",
    "start": "2000440",
    "end": "2006290"
  },
  {
    "text": "of these distributions. So where did we get these? Well, as I said, we're\nassuming a particular family",
    "start": "2006290",
    "end": "2014210"
  },
  {
    "text": "and then just varying\nthe parameters.  So how do we choose\nwhich one of those?",
    "start": "2014210",
    "end": "2022380"
  },
  {
    "text": "Well, just intuitively\nor graphically, we want to choose the one that\nsort of fits our histogram",
    "start": "2022380",
    "end": "2028110"
  },
  {
    "text": "the best, so that's what we do.",
    "start": "2028110",
    "end": "2034130"
  },
  {
    "text": "We choose the\nmember of the family of distributions that's\nmost likely to have produced",
    "start": "2034130",
    "end": "2041380"
  },
  {
    "text": "our data. So if we chose a member of the\nfamily whose shape was much different than our\nhistogram, then it's",
    "start": "2041380",
    "end": "2047320"
  },
  {
    "text": "unlikely that that\nparameter would have produced the data that we saw.",
    "start": "2047320",
    "end": "2053560"
  },
  {
    "text": "The one that fits our\nhistogram most closely, that's the one that's\nmost likely to have given",
    "start": "2053560",
    "end": "2059049"
  },
  {
    "text": "rise to our data. ",
    "start": "2059050",
    "end": "2065790"
  },
  {
    "text": "So theta hat, our maximum\nlikelihood estimator, is the value of the\nparameter associated",
    "start": "2065790",
    "end": "2072600"
  },
  {
    "text": "with this particular\nbest fit member of the family of distributions.",
    "start": "2072600",
    "end": "2079039"
  },
  {
    "text": "So is that, sort\nof concept clear, even if you don't know\nhow to find that yet?",
    "start": "2079040",
    "end": "2084888"
  },
  {
    "text": " So conceptually, it makes sense,\nor at least I hope it does.",
    "start": "2084889",
    "end": "2093120"
  },
  {
    "text": "Operationally, how\ndo we go about this? How do we find one\nof a bunch of PDFs",
    "start": "2093120",
    "end": "2098790"
  },
  {
    "text": "that's most likely to\nhave produced our data? ",
    "start": "2098790",
    "end": "2104390"
  },
  {
    "text": "Well, the key is\nthat we have to think about the joint PDF of our data\nin a somewhat different way.",
    "start": "2104390",
    "end": "2112760"
  },
  {
    "text": "So we know how to-- if\nwe have a random sample,",
    "start": "2112760",
    "end": "2117780"
  },
  {
    "text": "we know how to get the joint\nPDF of that random sample.",
    "start": "2117780",
    "end": "2122990"
  },
  {
    "text": "We're just it's just the product\nof the individual PDFs of all",
    "start": "2122990",
    "end": "2128090"
  },
  {
    "text": "of the random variables\nin our random sample. Now we think of this joint PDF\nas a function of the parameters",
    "start": "2128090",
    "end": "2136940"
  },
  {
    "text": "now. We'd always sort of taken\nthe parameters as given. They were just constants\nin our function",
    "start": "2136940",
    "end": "2143180"
  },
  {
    "text": "that we sort of took as given. Now we think of that\njoint PDF as a function of those parameters.",
    "start": "2143180",
    "end": "2149120"
  },
  {
    "text": "And we're going to\nmaximize that joint PDF",
    "start": "2149120",
    "end": "2154280"
  },
  {
    "text": "over the possible values\nof the parameters. ",
    "start": "2154280",
    "end": "2161850"
  },
  {
    "text": "Yes, of course. And we'll see a couple\nof examples as well. But I'm happy to go over this,\nbecause it's a little subtle.",
    "start": "2161850",
    "end": "2170010"
  },
  {
    "text": "It's a little confusing. So basically, what\nwe do-- actually,",
    "start": "2170010",
    "end": "2175170"
  },
  {
    "text": "let me go to the next slide. So in other words, what we\ndo is we take the joint PDF",
    "start": "2175170",
    "end": "2183330"
  },
  {
    "text": "of our random sample. Instead of calling\nit a joint PDF, we're now going to call\nit a likelihood function.",
    "start": "2183330",
    "end": "2190350"
  },
  {
    "text": "That's just suggestive\nto us that we have to think of it\nin a different way. It's exactly the same thing.",
    "start": "2190350",
    "end": "2195480"
  },
  {
    "text": "The likelihood function is the\njoint PDF of a random sample. How do we get this\nlikelihood function?",
    "start": "2195480",
    "end": "2202740"
  },
  {
    "text": "Well, we just take the\nproduct of the individual PDFs of all of the guys\nin our random sample.",
    "start": "2202740",
    "end": "2211010"
  },
  {
    "text": "Do you guys remember that? What's a random sample? It's independent identically\ndistributed random variables.",
    "start": "2211010",
    "end": "2217590"
  },
  {
    "text": "To get the joint distribution\nof independent identically distributed random\nvariables, you just",
    "start": "2217590",
    "end": "2223000"
  },
  {
    "text": "take their distribution, and\nyou multiply them together because they're independent.",
    "start": "2223000",
    "end": "2228070"
  },
  {
    "text": "So this is the joint PDF\nof our random sample. We're going to relabel that\nthe likelihood function.",
    "start": "2228070",
    "end": "2235780"
  },
  {
    "text": "And we're going to think\nof the likelihood function as being a function of theta,\ngiven the data, as opposed",
    "start": "2235780",
    "end": "2244540"
  },
  {
    "text": "to the other way around. And then we're going to maximize\nthat likelihood function with respect to these\nparameters theta.",
    "start": "2244540",
    "end": "2251020"
  },
  {
    "start": "2251020",
    "end": "2256060"
  },
  {
    "text": "So like I said, I have\na couple of examples. ",
    "start": "2256060",
    "end": "2262750"
  },
  {
    "text": "And this actually just\nemphasizes what we said before. So we've got this\nlikelihood function.",
    "start": "2262750",
    "end": "2267789"
  },
  {
    "text": "We just maximize it over all\npossible values of theta. And one thing to keep\nin mind, for this class,",
    "start": "2267790",
    "end": "2276400"
  },
  {
    "text": "you don't really have\nto worry about this. But if you go out of this\nclass and you encounter",
    "start": "2276400",
    "end": "2282339"
  },
  {
    "text": "a maximum likelihood\nin the larger world, then you're going\nto have to realize",
    "start": "2282340",
    "end": "2287920"
  },
  {
    "text": "that a lot of times what we\ndo, instead of maximizing the likelihood function, is\nwe maximize the log likelihood",
    "start": "2287920",
    "end": "2294579"
  },
  {
    "text": "function. So that's a perfectly\nfine thing to do. Whatever maximizes the\nlikelihood function",
    "start": "2294580",
    "end": "2300490"
  },
  {
    "text": "is going to maximize any\nmonotonic transformation of the likelihood function. So it's fine to take a log. The reason we do it\nis computationally,",
    "start": "2300490",
    "end": "2307359"
  },
  {
    "text": "it just is much\neasier, typically, to maximize the log\nlikelihood function So you go out into\nthe broader world,",
    "start": "2307360",
    "end": "2314740"
  },
  {
    "text": "you will encounter log\nlikelihood functions, not likelihood functions,\nand that's why. ",
    "start": "2314740",
    "end": "2326790"
  },
  {
    "text": "So it's good practice. I'm not necessarily saying\nyou guys have to do it.",
    "start": "2326790",
    "end": "2333990"
  },
  {
    "text": "But it's good practice\nif you're sort of interested in these\nkinds of techniques,",
    "start": "2333990",
    "end": "2342089"
  },
  {
    "text": "to write down some joint\nPDFs, maybe take the logs to make computation easier.",
    "start": "2342090",
    "end": "2347370"
  },
  {
    "text": "Take the derivative\nwith respect to theta. Set the derivatives equal to 0. Solve for the maximum\nlikelihood estimators.",
    "start": "2347370",
    "end": "2354990"
  },
  {
    "text": "You certainly can do that. We're not going to do it here. What I'm going to\ndo instead, is I'm going to do a couple of\nexamples that are both unusual",
    "start": "2354990",
    "end": "2365340"
  },
  {
    "text": "in that they can't be solved\nthis way because their maxima are not at a point where\nthe likelihood function is",
    "start": "2365340",
    "end": "2372750"
  },
  {
    "text": "differentiable. But then also, we don't\nhave to get bogged down with all the computation.",
    "start": "2372750",
    "end": "2378250"
  },
  {
    "text": "So I think they're kind of\neasier examples to show you, I think. ",
    "start": "2378250",
    "end": "2385060"
  },
  {
    "text": "STUDENT: So do we guess what we\nthink the PDFs are going to be, and then we--",
    "start": "2385060",
    "end": "2390533"
  },
  {
    "text": "SARA ELLISON: Good question. So the question was, do\nwe guess what the PDF is? So sometimes we may know.",
    "start": "2390533",
    "end": "2399400"
  },
  {
    "text": "Why do we know? I don't know. Maybe we're engineers,\nand we've sort of worked in a particular field\nfor our entire careers.",
    "start": "2399400",
    "end": "2408220"
  },
  {
    "text": "And we know that the random\ndraws from this particular--",
    "start": "2408220",
    "end": "2414300"
  },
  {
    "text": "from some random\nevent that happens with this particular\nmachine always has an exponential distribution.",
    "start": "2414300",
    "end": "2419830"
  },
  {
    "text": "And we just don't know\nwhat the parameter is. But we just know from\nvast years of experience,",
    "start": "2419830",
    "end": "2425500"
  },
  {
    "text": "it's going to be exponential. Or maybe we have a\ntheoretical reason",
    "start": "2425500",
    "end": "2431140"
  },
  {
    "text": "to believe that there is a\nparticular distribution that's giving rise to\nthe random sample.",
    "start": "2431140",
    "end": "2438279"
  },
  {
    "text": "Or sometimes we just guess. So it just depends a\nlot on the problem,",
    "start": "2438280",
    "end": "2444040"
  },
  {
    "text": "whether how confident we\nare about the distribution, the underlying distribution.",
    "start": "2444040",
    "end": "2449470"
  },
  {
    "text": " So I said, we'll do\na couple of examples.",
    "start": "2449470",
    "end": "2455900"
  },
  {
    "text": "But I'm going to do\nones that don't involve the standard, sort of taking the\nlog of the likelihood function,",
    "start": "2455900",
    "end": "2464510"
  },
  {
    "text": "taking derivative\nwith respect to theta, setting the derivatives\nequal to 0, et cetera.",
    "start": "2464510",
    "end": "2470060"
  },
  {
    "text": "I'm going to do some kind of\nnon-standard examples instead. ",
    "start": "2470060",
    "end": "2475180"
  },
  {
    "text": "So back to this sort of\nuniform 0 theta example--",
    "start": "2475180",
    "end": "2481420"
  },
  {
    "text": "remember, what the\nmaximum likelihood estimate from what the--",
    "start": "2481420",
    "end": "2490630"
  },
  {
    "text": "remember, I told you. I didn't show you, obviously. But remember what the\nmaximum likelihood estimate for this is?",
    "start": "2490630",
    "end": "2496495"
  },
  {
    "text": " The nth order statistic. STUDENT: Over n plus 1?",
    "start": "2496495",
    "end": "2502120"
  },
  {
    "text": "SARA ELLISON: That's the\nexpectation of that estimator.",
    "start": "2502120",
    "end": "2507250"
  },
  {
    "text": "So the maximum\nlikelihood estimate, I just said it verbally,\nit wasn't on a slide. But I said that in fact,\nit's the nth order statistic.",
    "start": "2507250",
    "end": "2514540"
  },
  {
    "text": " So I think I did.",
    "start": "2514540",
    "end": "2520300"
  },
  {
    "text": "So let's figure that out. Let's verify that, in fact,\nthat is the maximum likelihood estimator.",
    "start": "2520300",
    "end": "2526390"
  },
  {
    "text": "So what do we do? We first have to write down\nthe likelihood function. How do we get the\nlikelihood function?",
    "start": "2526390",
    "end": "2532840"
  },
  {
    "text": "It's just the joint PDF\nof our random sample. So we have each of\nthe individual members",
    "start": "2532840",
    "end": "2542530"
  },
  {
    "text": "of the random sample\nhas this distribution, 1 over theta for x in 0 to\ntheta and 0 otherwise.",
    "start": "2542530",
    "end": "2552780"
  },
  {
    "text": "Everyone agrees to that. So let's think back\nto the definition",
    "start": "2552780",
    "end": "2559460"
  },
  {
    "text": "of the maximum\nlikelihood estimator, it being the parameter that's\nmost likely to have given rise",
    "start": "2559460",
    "end": "2568640"
  },
  {
    "text": "to the sample that we observed.",
    "start": "2568640",
    "end": "2573980"
  },
  {
    "text": "Why? So I say here, I claim\nhere that we obviously would not pick\nany theta hat less",
    "start": "2573980",
    "end": "2581839"
  },
  {
    "text": "than the nth order statistic. Why is that, given\nthe definition of the maximum\nlikelihood estimator?",
    "start": "2581840",
    "end": "2590770"
  },
  {
    "text": "STUDENT: Well I guess, theta\nis supposed to be the maximum. So if there's a\ngreater value that",
    "start": "2590770",
    "end": "2597530"
  },
  {
    "text": "shows up in the random sample. SARA ELLISON: It\ncannot have happened. It's a probability 0 event.",
    "start": "2597530",
    "end": "2603890"
  },
  {
    "text": "So let me just draw a quick\npicture to illustrate. ",
    "start": "2603890",
    "end": "2613859"
  },
  {
    "text": "So here's our distribution. Theta is unknown. If we were to choose--",
    "start": "2613860",
    "end": "2621150"
  },
  {
    "text": "so we have sort of a random\nsample chosen from this. The entire random sample\nhas to be less than theta,",
    "start": "2621150",
    "end": "2628830"
  },
  {
    "text": "just has to be. I mean, that's how\nthe problem is set up. So let's suppose we have--",
    "start": "2628830",
    "end": "2634680"
  },
  {
    "text": " actually, maybe what I'll\ndo is erase this part.",
    "start": "2634680",
    "end": "2641140"
  },
  {
    "text": "And we have an nth\norder statistic.",
    "start": "2641140",
    "end": "2646740"
  },
  {
    "text": "We would never choose\na guess for theta below the nth order statistic,\nbecause that would suggest",
    "start": "2646740",
    "end": "2658010"
  },
  {
    "text": "a situation like this\nthat we know can't happen Does everyone understand that?",
    "start": "2658010",
    "end": "2663320"
  },
  {
    "text": " So theta always has to be at\nleast as big as the nth order",
    "start": "2663320",
    "end": "2669670"
  },
  {
    "text": "statistic or theta hat. ",
    "start": "2669670",
    "end": "2675140"
  },
  {
    "text": "So now, we've got the PDF\nof each individual guy in the random sample from that.",
    "start": "2675140",
    "end": "2680630"
  },
  {
    "text": "Since we know that\nthey're independent, we can write down the\nlikelihood function, which is also the joint\nPDF of the random sample.",
    "start": "2680630",
    "end": "2688170"
  },
  {
    "text": "So remember, it's the product\nof all of the individual PDFs.",
    "start": "2688170",
    "end": "2693349"
  },
  {
    "text": "So it's just 1 over theta raised\nto the n for all x or for x sub",
    "start": "2693350",
    "end": "2699260"
  },
  {
    "text": "i's in this interval. And it's going to\nbe 0 otherwise.",
    "start": "2699260",
    "end": "2705090"
  },
  {
    "text": "Now, we can actually\nrestate this condition here. Instead of saying x sub i in 0\nto theta for i equals 1 to n,",
    "start": "2705090",
    "end": "2716040"
  },
  {
    "text": "we can just say instead, that's\nthe same as the nth order",
    "start": "2716040",
    "end": "2722790"
  },
  {
    "text": "statistic being less than\ntheta, just equivalent. It's an equivalent statement.",
    "start": "2722790",
    "end": "2728660"
  },
  {
    "text": "You can say all of the x's are\nin the interval 0 to theta.",
    "start": "2728660",
    "end": "2733849"
  },
  {
    "text": "Or the biggest x\nis less than theta. So we can say we can say\nit like that instead.",
    "start": "2733850",
    "end": "2740190"
  },
  {
    "text": " And so what is the value that\nmaximizes this likelihood",
    "start": "2740190",
    "end": "2750800"
  },
  {
    "text": "function? It is, in fact, the\nnth order statistic. So graphically, let me show you.",
    "start": "2750800",
    "end": "2758240"
  },
  {
    "text": "It might not be 100% clear\nwhen you see it in this form. I hope when I show\nyou graphically,",
    "start": "2758240",
    "end": "2764090"
  },
  {
    "text": "it will become more clear. Why is it that this function\nis maximized at the nth order",
    "start": "2764090",
    "end": "2770930"
  },
  {
    "text": "statistic? So this likelihood\nfunction is equal to 0 for all values up until\nthe nth order statistic.",
    "start": "2770930",
    "end": "2781010"
  },
  {
    "text": "Think of the\nlikelihood function. Think of this now,\ninstead of being a joint PDF of\nthe random sample,",
    "start": "2781010",
    "end": "2786960"
  },
  {
    "text": "this is a function of theta. So basically what it's saying\nis the function is equal to this",
    "start": "2786960",
    "end": "2794280"
  },
  {
    "text": "if theta is greater than\nthe nth order statistic, and it's equal to 0 otherwise.",
    "start": "2794280",
    "end": "2799500"
  },
  {
    "text": "So for any values of theta less\nthan the nth order statistic, this function is equal to 0.",
    "start": "2799500",
    "end": "2806210"
  },
  {
    "text": "So off to the left,\nit's equal to 0. Then starting at the\nnth order statistic-- let me flip back again--",
    "start": "2806210",
    "end": "2812150"
  },
  {
    "text": "it's just equal to 1\nover theta to the n. So when we graph it in\ntheta space like this,",
    "start": "2812150",
    "end": "2821040"
  },
  {
    "text": "we get that it's equal to 0\nuntil you get to the nth order statistic. Then it jumps up to 1\nover theta to the n.",
    "start": "2821040",
    "end": "2827600"
  },
  {
    "text": "And then it declines\nas theta gets larger.",
    "start": "2827600",
    "end": "2833300"
  },
  {
    "text": "So this is maximized at\nthe nth order statistic. ",
    "start": "2833300",
    "end": "2839980"
  },
  {
    "text": "Questions? This make sense? STUDENT: N has to\nbe greater than 1?",
    "start": "2839980",
    "end": "2849040"
  },
  {
    "text": "SARA ELLISON: N? Yeah, exactly. ",
    "start": "2849040",
    "end": "2855200"
  },
  {
    "text": "Because it's just\nthe sample size. Oh, I guess the other\nthing I want to say about",
    "start": "2855200",
    "end": "2860300"
  },
  {
    "text": "is, you can probably see now\nby looking at the picture why we didn't solve this by\nwriting down the likelihood",
    "start": "2860300",
    "end": "2867859"
  },
  {
    "text": "function, taking the derivative\nwith respect to theta, setting the derivative\nequal to 0, et cetera.",
    "start": "2867860",
    "end": "2874280"
  },
  {
    "text": "Because in fact, it's not\ndifferentiable at the max. So if we had tried\nto do that, we would have run\ninto some trouble.",
    "start": "2874280",
    "end": "2880505"
  },
  {
    "text": " Second example-- let x sub i be\niid uniform 0, or sorry, theta",
    "start": "2880505",
    "end": "2893040"
  },
  {
    "text": "minus 1/2 to theta plus 1/2. So here, we again have\na uniform distribution.",
    "start": "2893040",
    "end": "2905250"
  },
  {
    "text": "It's not the length of the\ninterval that's unknown. We know that the\ninterval is length 1.",
    "start": "2905250",
    "end": "2910820"
  },
  {
    "text": "It's just the location of\nthe interval that's unknown. So the parameter\ntheta is determining",
    "start": "2910820",
    "end": "2916490"
  },
  {
    "text": "the location of this interval. so this is theta minus\n1/2 up to theta plus 1/2.",
    "start": "2916490",
    "end": "2935000"
  },
  {
    "text": "So this is our\nuniform distribution. We're interested in estimating\nthe unknown parameter theta.",
    "start": "2935000",
    "end": "2942560"
  },
  {
    "text": "The unknown parameter theta\njust determines the location of this interval.",
    "start": "2942560",
    "end": "2949490"
  },
  {
    "text": "And since the\ninterval is length 1, then the PDF is always\nequal to 1 in the interval.",
    "start": "2949490",
    "end": "2957710"
  },
  {
    "text": "It's 0 otherwise.  So let's write down the\nlikelihood function.",
    "start": "2957710",
    "end": "2963930"
  },
  {
    "text": "Well, this one we have to\nbe a little clever about. We have to think about\nit when we write it down.",
    "start": "2963930",
    "end": "2970960"
  },
  {
    "text": "So obviously, the\nlikelihood function-- so 1 raised to the nth\npower is just equal to 1.",
    "start": "2970960",
    "end": "2977309"
  },
  {
    "text": "But then what we have\nto be clever about is these bounds here.",
    "start": "2977310",
    "end": "2983550"
  },
  {
    "text": "And so let me go through\nthe explanation of how we get those. ",
    "start": "2983550",
    "end": "2990670"
  },
  {
    "text": "So we're writing them in\nterms of order statistics. And let me just point out that\nactually once I convinced you",
    "start": "2990670",
    "end": "2998829"
  },
  {
    "text": "that these bounds\nare correct, which I hope to do in just a moment,\nthen the maximum likelihood",
    "start": "2998830",
    "end": "3004770"
  },
  {
    "text": "estimate is going to be\nany value in that interval. Because any value in\nthis interval maximizes",
    "start": "3004770",
    "end": "3013380"
  },
  {
    "text": "the likelihood function. It's just flat\nover that interval. It's equal to 1.",
    "start": "3013380",
    "end": "3018420"
  },
  {
    "text": "So any value in that\ninterval is going to maximize the\nlikelihood function. So now I just have to convince\nyou that this sort of condition",
    "start": "3018420",
    "end": "3028190"
  },
  {
    "text": "is correct.  So let's look at\nthis one graphically.",
    "start": "3028190",
    "end": "3033490"
  },
  {
    "text": "So let's suppose we've\ngot a random sample. The interval that the\nunderlying distribution",
    "start": "3033490",
    "end": "3040380"
  },
  {
    "text": "has a uniform, where the\ninterval is length 1 and it's centered at theta.",
    "start": "3040380",
    "end": "3046350"
  },
  {
    "text": "And it's got to\nbe here somewhere. This is our random sample,\nso all of these observations",
    "start": "3046350",
    "end": "3053280"
  },
  {
    "text": "have to live in that interval. It's got to encompass\nall the data.",
    "start": "3053280",
    "end": "3058299"
  },
  {
    "text": "So what are the\npossibilities Well, the interval could be there, it\ncould be there, could be there,",
    "start": "3058300",
    "end": "3067310"
  },
  {
    "text": "could be there. And in fact, there's\na whole continuum.",
    "start": "3067310",
    "end": "3072529"
  },
  {
    "text": "And all of those possibilities\nare equally likely. ",
    "start": "3072530",
    "end": "3078569"
  },
  {
    "text": "Can it be there? No, no, way. Because if it's over\nhere, then these happened,",
    "start": "3078570",
    "end": "3085830"
  },
  {
    "text": "these occurred\nwith probability 0. Or they couldn't occur.",
    "start": "3085830",
    "end": "3091840"
  },
  {
    "text": "Likewise, it can't be there. So I think I've convinced\nyou that there's sort of a range of different\npossibilities, that",
    "start": "3091840",
    "end": "3099850"
  },
  {
    "text": "for the location of this\ninterval of length 1, and the only question is, how do\nwe express what that range is?",
    "start": "3099850",
    "end": "3110110"
  },
  {
    "text": "Well, the theta can\nbe, at most, 1/2",
    "start": "3110110",
    "end": "3115600"
  },
  {
    "text": "above the first order statistic. Let me just flip back.",
    "start": "3115600",
    "end": "3121680"
  },
  {
    "text": "So over here is the\nfirst order statistic. We know that the left\nedge of the interval",
    "start": "3121680",
    "end": "3129980"
  },
  {
    "text": "can't be any higher than\nthe first order statistic. So therefore, theta can't be\nany higher than 1/2 above that.",
    "start": "3129980",
    "end": "3138380"
  },
  {
    "text": "Then the same reasoning goes\nwith the nth order statistic.",
    "start": "3138380",
    "end": "3143660"
  },
  {
    "text": "Theta can be at most, 1/2\nbelow the nth order statistic. It can't be any further down.",
    "start": "3143660",
    "end": "3149569"
  },
  {
    "text": "Or we're going to start missing\nthe observations at the top. ",
    "start": "3149570",
    "end": "3155130"
  },
  {
    "text": "And so that gives us\nsort of a little region",
    "start": "3155130",
    "end": "3160680"
  },
  {
    "text": "in which theta can live. And all the values\nin theta, in that",
    "start": "3160680",
    "end": "3165780"
  },
  {
    "text": "little region in that window\nare equally likely, because we wrote down the\nlikelihood functions. It's just equal to\n1 in that region.",
    "start": "3165780",
    "end": "3173160"
  },
  {
    "text": "So in fact, I can\nflip back if you want. But this is, in fact,\nthe condition that I",
    "start": "3173160",
    "end": "3181200"
  },
  {
    "text": "had on the likelihood function. So let me just--",
    "start": "3181200",
    "end": "3186870"
  },
  {
    "text": "that theta is in the nth\norder statistic minus 1/2,",
    "start": "3186870",
    "end": "3193560"
  },
  {
    "text": "up to the first order\nstatistic plus 1/2. There should be an overlap, and\nthat the overlap of those two",
    "start": "3193560",
    "end": "3201720"
  },
  {
    "text": "sort of conditions, gives us\nthe region where theta can live.",
    "start": "3201720",
    "end": "3207530"
  },
  {
    "text": "Does that make sense? ",
    "start": "3207530",
    "end": "3214710"
  },
  {
    "text": "So maximum likelihood\nestimators, sort of it's",
    "start": "3214710",
    "end": "3220140"
  },
  {
    "text": "a very popular framework\nfor deriving estimators.",
    "start": "3220140",
    "end": "3225660"
  },
  {
    "text": "And part of it is\nbecause they have some very favorable properties. Well, part of it is because\nthey're kind of always",
    "start": "3225660",
    "end": "3232780"
  },
  {
    "text": "available. If you're willing\nto take a stand, make an assumption about what\nthe underlying distribution is,",
    "start": "3232780",
    "end": "3238410"
  },
  {
    "text": "then you can write down a\nmaximum likelihood estimator. Of course, that's also true for\nmethod of moments estimator.",
    "start": "3238410",
    "end": "3244710"
  },
  {
    "text": "But sort of maximum\nlikelihood estimators are sort of available if you're\nsort of willing to write down",
    "start": "3244710",
    "end": "3251640"
  },
  {
    "text": "a distribution of\nthe underlying data. But they also have some\nvery favorable properties.",
    "start": "3251640",
    "end": "3258250"
  },
  {
    "text": "So if your assumptions\nare correct, so this is all assuming\nthat you haven't",
    "start": "3258250",
    "end": "3264030"
  },
  {
    "text": "made a mistake in assuming\nthe underlying distribution, then the maximum\nlikelihood estimator",
    "start": "3264030",
    "end": "3270170"
  },
  {
    "text": "is going to have the\nfollowing properties. If there is an\nefficient estimator in a class of\nconsistent estimators,",
    "start": "3270170",
    "end": "3276010"
  },
  {
    "text": "maximum likelihood\nestimation will produce it. So that tells you\nright there, you",
    "start": "3276010",
    "end": "3282130"
  },
  {
    "text": "don't have to check for the\nefficiency of your estimator. If you're using\nmaximum likelihood,",
    "start": "3282130",
    "end": "3290349"
  },
  {
    "text": "it's going to have that a\ngood property in that respect. ",
    "start": "3290350",
    "end": "3296020"
  },
  {
    "text": "Also, there is a central\nlimit theorem-type",
    "start": "3296020",
    "end": "3301600"
  },
  {
    "text": "result having to do with\nmaximum likelihood estimators. So under certain\nregularity conditions,",
    "start": "3301600",
    "end": "3309490"
  },
  {
    "text": "if the maximum\nlikelihood estimator is not on some boundary of some\nspace or something like that,",
    "start": "3309490",
    "end": "3316870"
  },
  {
    "text": "then asymptotically, sort of\nas your sample size is getting",
    "start": "3316870",
    "end": "3322120"
  },
  {
    "text": "bigger and bigger, maximum\nlikelihood estimate are going to have a normal\nor approximately normal distribution.",
    "start": "3322120",
    "end": "3327589"
  },
  {
    "text": "So just like we had the central\nlimit theorem for the sample mean, there is a central\nlimit theorem-like result",
    "start": "3327590",
    "end": "3334600"
  },
  {
    "text": "for maximum\nlikelihood estimators. And that can be very\nuseful for inference, which",
    "start": "3334600",
    "end": "3341320"
  },
  {
    "text": "we'll see after spring break. ",
    "start": "3341320",
    "end": "3347970"
  },
  {
    "text": "So does this mean that maximum\nlikelihood estimation is always the right thing to do?",
    "start": "3347970",
    "end": "3354610"
  },
  {
    "text": "Well, no, in fact. So we saw an example\nwhere even given",
    "start": "3354610",
    "end": "3366200"
  },
  {
    "text": "that our assumptions\nwere correct, the maximum likelihood\nestimator was biased.",
    "start": "3366200",
    "end": "3372380"
  },
  {
    "text": "So that was the example,\nwhere we were estimating theta from the uniform 0\ntheta distribution.",
    "start": "3372380",
    "end": "3378440"
  },
  {
    "text": "And our maximum likelihood\nestimate for any finite sample size was going to be biased.",
    "start": "3378440",
    "end": "3385160"
  },
  {
    "text": "Now, in fact, in that\nparticular example, we could fix the bias, because\nthe bias is a function of n.",
    "start": "3385160",
    "end": "3391790"
  },
  {
    "text": "And we know how big\nour sample size is, so we can actually\nundo that bias.",
    "start": "3391790",
    "end": "3396950"
  },
  {
    "text": "But there are lots of\nexamples where you don't know the size of the bias.",
    "start": "3396950",
    "end": "3401970"
  },
  {
    "text": "And so you can't always undo it. So they can be biased. And we saw an example. And sometimes,\nwe're not interested",
    "start": "3401970",
    "end": "3409460"
  },
  {
    "text": "in biased estimators. They might be\ndifficult to compute.",
    "start": "3409460",
    "end": "3414650"
  },
  {
    "text": "And then finally,\nthey can be sensitive to incorrect assumptions about\nthe underlying distribution",
    "start": "3414650",
    "end": "3422090"
  },
  {
    "text": "and more so than\nother estimators. So the method of\nmoments estimators",
    "start": "3422090",
    "end": "3428180"
  },
  {
    "text": "tend to be more robust to\nthe underlying assumptions.",
    "start": "3428180",
    "end": "3437030"
  },
  {
    "text": "Because they rely less. So the maximum\nlikelihood estimators really rely on the entire\nshape of the distribution",
    "start": "3437030",
    "end": "3443990"
  },
  {
    "text": "of the random sample. The method of moments\nestimators don't. They just rely on the moments.",
    "start": "3443990",
    "end": "3449430"
  },
  {
    "text": "And so you can imagine just kind\nof intuitively that something that relies on the entire\nshape of a distribution,",
    "start": "3449430",
    "end": "3456530"
  },
  {
    "text": "if you're wrong about that\nshape of a distribution, it might be sensitive that\nsomething can go wrong.",
    "start": "3456530",
    "end": "3462680"
  },
  {
    "text": "And that's, in general, true. Maximum likelihood\nestimators can be sensitive to\nincorrect assumptions",
    "start": "3462680",
    "end": "3469130"
  },
  {
    "text": "about the underlying\ndistributions.  Questions?",
    "start": "3469130",
    "end": "3474710"
  },
  {
    "text": " No? So I would like to just go\nthrough a summary of what we've",
    "start": "3474710",
    "end": "3484010"
  },
  {
    "text": "done so far this semester. And feel free to chime\nin with questions",
    "start": "3484010",
    "end": "3490400"
  },
  {
    "text": "and ask about if you have\nquestions about the exam,",
    "start": "3490400",
    "end": "3497464"
  },
  {
    "text": "relative to this, what you might\nbe responsible for in the exam. I'm happy to answer\nthose questions.",
    "start": "3497465",
    "end": "3504850"
  },
  {
    "text": "So we started out the semester\nwith some probability basics.",
    "start": "3504850",
    "end": "3511150"
  },
  {
    "text": "So we introduced the\nconcept of probability and talked about simple sample\nspaces, independent events,",
    "start": "3511150",
    "end": "3517960"
  },
  {
    "text": "conditional probabilities,\nand Bayes' rule. ",
    "start": "3517960",
    "end": "3523619"
  },
  {
    "text": "And in some sense, that\nwas kind of foundation",
    "start": "3523620",
    "end": "3529240"
  },
  {
    "text": "for sort of going on\nand studying probability",
    "start": "3529240",
    "end": "3534490"
  },
  {
    "text": "with the kind of\nmathematical construct of the random variable. But do note that we didn't\nstudy Bayes' rule in the context",
    "start": "3534490",
    "end": "3545589"
  },
  {
    "text": "of random variables. You can actually\nformulate a Bayes' rule",
    "start": "3545590",
    "end": "3552100"
  },
  {
    "text": "using random variables,\nwhich we didn't do. But Bayes' rules are\nvery important concept,",
    "start": "3552100",
    "end": "3558940"
  },
  {
    "text": "that aside from, I guess,\nsort of the one concept",
    "start": "3558940",
    "end": "3565047"
  },
  {
    "text": "that we covered in\nthe beginning that wasn't sort of just foundation\nfor the rest of what we're doing.",
    "start": "3565048",
    "end": "3571750"
  },
  {
    "text": "Then we introduced\nrandom variables. And we defined what a\nrandom variable was.",
    "start": "3571750",
    "end": "3578860"
  },
  {
    "text": "We discussed ways to\nrepresent distributions. So a probability function, a\nprobability density function,",
    "start": "3578860",
    "end": "3585740"
  },
  {
    "text": "so those were kind of\nanalogous, sort of things. But one was for discrete\nrandom variables.",
    "start": "3585740",
    "end": "3591579"
  },
  {
    "text": "And one was for continuous\nrandom variables. And then we also\ntalked about how the same information in PFs and\nPDFs can be embodied in a CDF.",
    "start": "3591580",
    "end": "3601060"
  },
  {
    "text": "And that's the\ndefinition, of a CDF is the same for both continuous\nand discrete random variables.",
    "start": "3601060",
    "end": "3608490"
  },
  {
    "text": "And then we also covered sort\nof the random variable analogs",
    "start": "3608490",
    "end": "3614400"
  },
  {
    "text": "to the sort of stuff we\ncovered in probability basics. So we had talked about\nindependent events.",
    "start": "3614400",
    "end": "3620567"
  },
  {
    "text": "Then we talked about\nindependent random variables. We talked about\nconditional probabilities. We then talked about\nconditional PDFs.",
    "start": "3620567",
    "end": "3627010"
  },
  {
    "text": " And then, Esther covered\nsome, sort of I don't know,",
    "start": "3627010",
    "end": "3637220"
  },
  {
    "text": "empirical counterparts\nto some of the things that we had been talking\nabout theoretically in class. So she talked about\nboth, histogram,",
    "start": "3637220",
    "end": "3645260"
  },
  {
    "text": "defined what a histogram was,\nshowed us some nice examples, and then also talked about\nkernel density estimates",
    "start": "3645260",
    "end": "3652759"
  },
  {
    "text": "and what the difference between\na histogram and a kernel density estimate\nis and showed us",
    "start": "3652760",
    "end": "3658520"
  },
  {
    "text": "some pictures of those as well. ",
    "start": "3658520",
    "end": "3663890"
  },
  {
    "text": "And we saw lots of\nexamples that we hope gave you a sense for\nhow these theoretical objects",
    "start": "3663890",
    "end": "3672820"
  },
  {
    "text": "function and what\ninformation they tell us and how to interpret them. Then we started talking about\nfunctions of random variables.",
    "start": "3672820",
    "end": "3680450"
  },
  {
    "text": "So we saw some basic\nstrategies, or I guess, actually\none basic strategy",
    "start": "3680450",
    "end": "3686140"
  },
  {
    "text": "for figuring out the\ndistribution of a function of random variables.",
    "start": "3686140",
    "end": "3691180"
  },
  {
    "text": "And then I also did-- I went through several\nimportant examples. And so I think it's\nuseful to go back and be",
    "start": "3691180",
    "end": "3700390"
  },
  {
    "text": "comfortable with the example. So what were some of the\nexamples that we went through? Well, we did sort\nof the probability",
    "start": "3700390",
    "end": "3707350"
  },
  {
    "text": "integral transformation. And we did a convolution.",
    "start": "3707350",
    "end": "3712420"
  },
  {
    "text": "We did linear transformations\nof random variables. We did order statistics.",
    "start": "3712420",
    "end": "3718420"
  },
  {
    "text": "Maybe that's it. I'm not sure. Maybe there was one more. I don't know. Those are sort of, those\nare all important examples",
    "start": "3718420",
    "end": "3724839"
  },
  {
    "text": "and all things that we\nsort of have drawn on since and will continue to draw on. ",
    "start": "3724840",
    "end": "3734090"
  },
  {
    "text": "Then we sort of introduced\nmoments of distributions. So we defined mean variance.",
    "start": "3734090",
    "end": "3742430"
  },
  {
    "text": "We defined a couple\nother moments. But we mostly focused on\nthe mean and variance.",
    "start": "3742430",
    "end": "3747500"
  },
  {
    "text": "And then in addition\nto learning how to-- or expectation if you'd rather,\nexpectation and variance.",
    "start": "3747500",
    "end": "3754100"
  },
  {
    "text": "In addition to learning\nhow to directly compute the expectation and variance,\ngiven the distribution",
    "start": "3754100",
    "end": "3760520"
  },
  {
    "text": "of a random variable,\nwe also learned a lot of techniques and\nproperties to help",
    "start": "3760520",
    "end": "3765920"
  },
  {
    "text": "compute moments of functions\nof random variables. So you have a random variable,\nmaybe you know its moments.",
    "start": "3765920",
    "end": "3775160"
  },
  {
    "text": "But what you're\nreally concerned with is a function of\nthat random variable. Do you have to figure out how\nthat function is distributed",
    "start": "3775160",
    "end": "3784130"
  },
  {
    "text": "and then compute the\nmoments directly? You can do that. Usually you don't.",
    "start": "3784130",
    "end": "3789290"
  },
  {
    "text": "Oftentimes, we can just rely\non properties of expectation and properties of variance,\nproperties of covariance,",
    "start": "3789290",
    "end": "3796920"
  },
  {
    "text": "et cetera, to figure\nout the characteristics of distributions of functions\nof random variables,",
    "start": "3796920",
    "end": "3806720"
  },
  {
    "text": "Then Esther went\nthrough a whole lecture discussing special\ndistributions.",
    "start": "3806720",
    "end": "3813890"
  },
  {
    "text": "And these are all\ndistributions that are useful. They're distributions\nthat will re-enter",
    "start": "3813890",
    "end": "3822110"
  },
  {
    "text": "your life at some point in\nhigh probability, I would say. They're distributions that\nwe will continue to discuss",
    "start": "3822110",
    "end": "3832460"
  },
  {
    "text": "throughout the semester. And so it's good if\nyou have some level of comfort with them.",
    "start": "3832460",
    "end": "3838260"
  },
  {
    "text": "We're not interested\nin having you memorize PDFs of special distributions. We're not interested\nin having you",
    "start": "3838260",
    "end": "3844190"
  },
  {
    "text": "memorize the expectation\nand the variance of special distributions.",
    "start": "3844190",
    "end": "3850460"
  },
  {
    "text": "Sometimes that just happens\nbecause you use them so much. So on the exam, if\nyou feel comfortable,",
    "start": "3850460",
    "end": "3857070"
  },
  {
    "text": "you can write them out\non your page of notes that you bring in. But we'll also provide\nthings like that on the exam.",
    "start": "3857070",
    "end": "3862735"
  },
  {
    "text": " And then finally, in the\nlast couple of lectures,",
    "start": "3862735",
    "end": "3870450"
  },
  {
    "text": "we started talking\nabout estimation. So we started our segue into\nestimation, in some sense,",
    "start": "3870450",
    "end": "3879300"
  },
  {
    "text": "was the central limit theorem. So we've been talking about\nfunctions of random variables. And we introduced a special\nparticularly useful function",
    "start": "3879300",
    "end": "3889020"
  },
  {
    "text": "of random variables,\ncalled the sample mean. And we computed what the\nexpectation of that function",
    "start": "3889020",
    "end": "3896430"
  },
  {
    "text": "was, what the expectation\nof the sample mean is. We computed what the variance\nof that sample mean was.",
    "start": "3896430",
    "end": "3901680"
  },
  {
    "text": "But we wanted to know even\nmore about its characteristics and how it was distributed.",
    "start": "3901680",
    "end": "3907080"
  },
  {
    "text": "And the central limit\ntheorem told us that. The central limit theorem had\nthis sort of remarkable result,",
    "start": "3907080",
    "end": "3912510"
  },
  {
    "text": "that no matter what\nkind of distribution you're drawing from and\ncomputing the sample mean of, if your sample\nsize is big enough",
    "start": "3912510",
    "end": "3920070"
  },
  {
    "text": "from that crazy\ndistribution, then the sample mean is going\nto have approximately",
    "start": "3920070",
    "end": "3925510"
  },
  {
    "text": "a normal distribution. And that's just super useful.",
    "start": "3925510",
    "end": "3930550"
  },
  {
    "text": "The full usefulness\nof it has not occurred to you yet because we\nhaven't gotten to inference.",
    "start": "3930550",
    "end": "3937330"
  },
  {
    "text": "But we will after spring\nbreak, get to inference. And then you will know. So we talked about\nthe central limit.",
    "start": "3937330",
    "end": "3943720"
  },
  {
    "text": "We talked about the sample mean. So that's kind of the\nmost important estimator that we talked about.",
    "start": "3943720",
    "end": "3948970"
  },
  {
    "text": "We talked about the\ncentral limit theorem, which told us some important\nfacts about how the sample",
    "start": "3948970",
    "end": "3955420"
  },
  {
    "text": "mean behaved. We had sort of a general\ndiscussion about estimation.",
    "start": "3955420",
    "end": "3962920"
  },
  {
    "text": "And then, the last\ntwo topics we covered, these are topics that we will\nnot test you on the exam.",
    "start": "3962920",
    "end": "3969640"
  },
  {
    "text": "The last two topics we\ncovered were criteria for assessing estimators. So whether an\nestimator is unbiased,",
    "start": "3969640",
    "end": "3977920"
  },
  {
    "text": "whether it's efficient,\nwhether it's a minimum mean squared error estimator,\nwhether it's consistent,",
    "start": "3977920",
    "end": "3983950"
  },
  {
    "text": "and then we also\ntalked about frameworks for deriving estimators,\nmethod of moments estimation",
    "start": "3983950",
    "end": "3990610"
  },
  {
    "text": "and maximum\nlikelihood estimation. ",
    "start": "3990610",
    "end": "4014000"
  }
]