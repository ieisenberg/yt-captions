[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6029"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6030",
    "end": "12680"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at fsae@mit.edu.",
    "start": "12680",
    "end": "18270"
  },
  {
    "start": "18270",
    "end": "37340"
  },
  {
    "text": "PATRICK WINSTON: It was in\n2010, yes, that's right. It was in 2010.",
    "start": "37340",
    "end": "44630"
  },
  {
    "text": "We were having our\nannual discussion about what we would dump fro\n6034 in order to make room",
    "start": "44630",
    "end": "49970"
  },
  {
    "text": "for some other stuff. And we almost killed\noff neural nets. That might seem strange\nbecause our heads",
    "start": "49970",
    "end": "58010"
  },
  {
    "text": "are stuffed with neurons. If you open up your skull\nand pluck them all out, you don't think anymore.",
    "start": "58010",
    "end": "63870"
  },
  {
    "text": "So it would seem\nthat neural nets would be a fundamental\nand unassailable topic.",
    "start": "63870",
    "end": "72610"
  },
  {
    "text": "But many of us felt that\nthe neural models of the day weren't much in the way\nof faithful models of what",
    "start": "72610",
    "end": "83380"
  },
  {
    "text": "actually goes on\ninside our heads. And besides that,\nnobody had ever made a neural net that was\nworth a darn for doing anything.",
    "start": "83380",
    "end": "90270"
  },
  {
    "text": "So we almost killed it off. But then we said,\nwell, everybody would feel cheated\nif they take a course",
    "start": "90270",
    "end": "96360"
  },
  {
    "text": "in artificial intelligence,\ndon't learn anything about neural nets,\nand then they'll go off and invent\nthem themselves. And they'll waste\nall sorts of time.",
    "start": "96360",
    "end": "102240"
  },
  {
    "text": "So we kept the subject in. Then two years\nlater, Jeff Hinton",
    "start": "102240",
    "end": "108420"
  },
  {
    "text": "from the University of\nToronto stunned the world with some neural network\nhe had done on recognizing",
    "start": "108420",
    "end": "116450"
  },
  {
    "text": "and classifying pictures. And he published\na paper from which I am now going to show\nyou a couple of examples.",
    "start": "116450",
    "end": "124910"
  },
  {
    "text": "Jeff's neural net, by the\nway, had 60 million parameters in it. And its purpose was to determine\nwhich of 1,000 categories",
    "start": "124910",
    "end": "135110"
  },
  {
    "text": "best characterized a picture. ",
    "start": "135110",
    "end": "142900"
  },
  {
    "text": "So there it is. There's a sample of things\nthat the Toronto neural net",
    "start": "142900",
    "end": "151280"
  },
  {
    "text": "was able to recognize\nor make mistakes on. I'm going to blow\nthat up a little bit.",
    "start": "151280",
    "end": "157040"
  },
  {
    "text": "I think I'm going\nto look particularly at the example labeled\ncontainer ship.",
    "start": "157040",
    "end": "162930"
  },
  {
    "text": "So what you see here is that\nthe program returned its best estimate of what it was\nranked, first five, according",
    "start": "162930",
    "end": "171630"
  },
  {
    "text": "to the likelihood,\nprobability, or the certainty that it felt that a\nparticular class was",
    "start": "171630",
    "end": "179239"
  },
  {
    "text": "characteristic of the picture. And so you can see this\none is extremely confident that it's a container ship.",
    "start": "179240",
    "end": "186260"
  },
  {
    "text": "It also was fairly\nmoved by the idea that it might be a lifeboat.",
    "start": "186260",
    "end": "193590"
  },
  {
    "text": "Now, I'm not sure about you,\nbut I don't think this looks much like a lifeboat.",
    "start": "193590",
    "end": "198666"
  },
  {
    "text": "But it does look like\na container ship. So if I look at only the best\nchoice, it looks pretty good. Here are the other things\nthey did pretty well,",
    "start": "198666",
    "end": "205760"
  },
  {
    "text": "got the right answer\nis the first choice-- is this first choice. So over on the left,\nyou see that it's",
    "start": "205760",
    "end": "211840"
  },
  {
    "text": "decided that the picture\nis a picture of a mite. The mite is not anywhere near\nthe center of the picture,",
    "start": "211840",
    "end": "217700"
  },
  {
    "text": "but somehow it managed to find\nit-- the container ship again. There is a motor scooter, a\ncouple of people sitting on it.",
    "start": "217700",
    "end": "224110"
  },
  {
    "text": "But it correctly characterized\nthe picture as a motor scooter. And then on the\nright, a Leopard.",
    "start": "224110",
    "end": "230160"
  },
  {
    "text": "And everything else\nis a cat of some sort. So it seems to be\ndoing pretty well. In fact, it does do pretty well.",
    "start": "230160",
    "end": "236930"
  },
  {
    "text": "But anyone who does\nthis kind of work has an obligation\nto show you some of the stuff that\ndoesn't work so well on",
    "start": "236930",
    "end": "243110"
  },
  {
    "text": "or doesn't get quite right. And so these pictures also\noccurred in Hinton's paper.",
    "start": "243110",
    "end": "250010"
  },
  {
    "text": "So the first one is\ncharacterized as a grill. But the right answer was\nsupposed to be convertible.",
    "start": "250010",
    "end": "255910"
  },
  {
    "text": "Oh, no, yes, yeah, right\nanswer was convertible. In the second case,\nthe characterization",
    "start": "255910",
    "end": "262430"
  },
  {
    "text": "is of a mushroom. And the alleged right\nanswer is agaric.",
    "start": "262430",
    "end": "268320"
  },
  {
    "text": "Is that pronounced right? It turns out that's a kind of\nmushroom-- so no problem there.",
    "start": "268320",
    "end": "273729"
  },
  {
    "text": "In the next case, it\nsaid it was a cherry. But it was supposed\nto be a dalmatian. Now, I think a dalmatian is\na perfectly legitimate answer",
    "start": "273730",
    "end": "281199"
  },
  {
    "text": "for that particular picture--\nso hard to fault it for that. And the last case,\nthe correct answer",
    "start": "281200",
    "end": "287639"
  },
  {
    "text": "was not in any of the top five. I'm not sure if you've\never seen a Madagascar cap.",
    "start": "287640",
    "end": "293100"
  },
  {
    "text": "But that's a picture of one. And it's interesting\nto compare that with the first choice of the\nprogram, the squirrel monkey.",
    "start": "293100",
    "end": "298970"
  },
  {
    "text": "This is the two side by side. So in a way, it's not\nsurprising that it",
    "start": "298970",
    "end": "304190"
  },
  {
    "text": "thought that the\nMadagascar cat was a picture of a squirrel\nmonkey-- so pretty impressive.",
    "start": "304190",
    "end": "310789"
  },
  {
    "text": "It blew away the competition. It did so much better the\nsecond place wasn't even close.",
    "start": "310790",
    "end": "316224"
  },
  {
    "text": "And for the first time, it\ndemonstrated that a neural net could actually do something. And since that time, in the\nthree years since that time,",
    "start": "316225",
    "end": "323810"
  },
  {
    "text": "there's been an enormous\namount of effort put into neural net technology,\nwhich some say is the answer.",
    "start": "323810",
    "end": "331470"
  },
  {
    "text": "So what we're going to\ndo today and tomorrow is have a look at this stuff\nand ask ourselves why it works,",
    "start": "331470",
    "end": "339130"
  },
  {
    "text": "when it might not work,\nwhat needs to be done, what has been done, and all\nthose kinds of questions will emerge.",
    "start": "339130",
    "end": "344470"
  },
  {
    "start": "344470",
    "end": "351520"
  },
  {
    "text": "So I guess the first thing to\ndo is think about what it is that we are being inspired by.",
    "start": "351520",
    "end": "357970"
  },
  {
    "text": "We're being inspired\nby those things that are inside our head-- all\n10 to the 11th of them.",
    "start": "357970",
    "end": "364650"
  },
  {
    "text": "And so if we take one of those\n10 to the 11th and look at it,",
    "start": "364650",
    "end": "369755"
  },
  {
    "text": "you know from 700 something\nor other approximately what a neuron looks like. And by the way, I'm going\nto teach you in this lecture",
    "start": "369755",
    "end": "377760"
  },
  {
    "text": "how to answer questions\nabout neurobiology with an 80% probability that\nyou will give the same answer",
    "start": "377760",
    "end": "383170"
  },
  {
    "text": "as a neurobiologist. So let's go.",
    "start": "383170",
    "end": "388310"
  },
  {
    "text": "So here's a neuron. It's got a cell body. And there is a nucleus.",
    "start": "388310",
    "end": "393530"
  },
  {
    "text": "And then out here is\na long thingamajigger which divides maybe a\nlittle bit, but not much.",
    "start": "393530",
    "end": "401230"
  },
  {
    "text": "And we call that the axon.  So then over here, we've got\nthis much more branching type",
    "start": "401230",
    "end": "410099"
  },
  {
    "text": "of structure that looks\nmaybe a little bit like so. ",
    "start": "410100",
    "end": "422143"
  },
  {
    "text": "Maybe like that-- and this\nstuff branches a whole lot. And that part is called\nthe dendritic tree. ",
    "start": "422144",
    "end": "435400"
  },
  {
    "text": "Now, there are a\ncouple of things we can note about this is that\nthese guys are connected axon",
    "start": "435400",
    "end": "441350"
  },
  {
    "text": "to dendrite. So over here, they'll be\na so-called pre-synaptic",
    "start": "441350",
    "end": "448430"
  },
  {
    "text": "thickening. And over here will be some\nother neuron's dendrite. And likewise, over here\nsome other neuron's axon",
    "start": "448430",
    "end": "458220"
  },
  {
    "text": "is coming in here and hitting\nthe dendrite of our the one",
    "start": "458220",
    "end": "465855"
  },
  {
    "text": "that occupies most\nof our picture. ",
    "start": "465855",
    "end": "473220"
  },
  {
    "text": "So if there is enough\nstimulation from this side in the axonal tree,\nor the dendritic tree,",
    "start": "473220",
    "end": "481060"
  },
  {
    "text": "then a spike will\ngo down that axon. It acts like a\ntransmission line.",
    "start": "481060",
    "end": "487820"
  },
  {
    "text": "And then after that\nhappens, the neuron will go quiet for a while as\nit's recovering its strength.",
    "start": "487820",
    "end": "495000"
  },
  {
    "text": "That's called the\nrefractory period.  Now, if we look at that\nconnection in a little more",
    "start": "495000",
    "end": "503230"
  },
  {
    "text": "detail, this little piece right\nhere sort of looks like this.",
    "start": "503230",
    "end": "510000"
  },
  {
    "text": "Here's the axon coming in. It's got a whole bunch\nof little vesicles in it.",
    "start": "510000",
    "end": "515870"
  },
  {
    "text": "And then there's a\ndendrite over here. And when the axon is stimulated,\nit dumps all these vesicles",
    "start": "515870",
    "end": "522729"
  },
  {
    "text": "into this inner synaptic space. For a long time, it wasn't\nknown whether those things were actually separated.",
    "start": "522730",
    "end": "529430"
  },
  {
    "text": "I think it was\nRaamon and Cahal who demonstrated that one\nneuron is actually not part of the next one.",
    "start": "529430",
    "end": "536029"
  },
  {
    "text": "They're actually separated\nby these synaptic gaps.",
    "start": "536030",
    "end": "541610"
  },
  {
    "text": "So there it is. How can we model,\nthat sort of thing?",
    "start": "541610",
    "end": "546770"
  },
  {
    "text": "Well, here's what's\nusually done. Here's what is done in\nthe neural net literature. ",
    "start": "546770",
    "end": "557070"
  },
  {
    "text": "First of all, we've got\nsome kind of binary input, because these things either\nfire or they don't fire.",
    "start": "557070",
    "end": "563370"
  },
  {
    "text": "So it's an all-or-none\nkind of situation. So over here, we have\nsome kind of input value.",
    "start": "563370",
    "end": "569510"
  },
  {
    "text": "We'll call it x1. And is either a 0 or 1. So it comes in here.",
    "start": "569510",
    "end": "575910"
  },
  {
    "text": "And then it gets multiplied\ntimes some kind of weight. We'll call it w1.",
    "start": "575910",
    "end": "582635"
  },
  {
    "text": " So this part here is modeling\nthis synaptic connection.",
    "start": "582635",
    "end": "589910"
  },
  {
    "text": "It may be more or less strong. And if it's more strong,\nthis weight goes up. And if it's less strong,\nthis weight goes down.",
    "start": "589910",
    "end": "596509"
  },
  {
    "text": "So that reflects the\ninfluence of the synapse on whether or not the whole\naxon decides it's stimulated.",
    "start": "596510",
    "end": "605899"
  },
  {
    "text": "Then we got other inputs down\nhere-- x sub n, also 0 or 1.",
    "start": "605900",
    "end": "612110"
  },
  {
    "text": "It's also multiplied\nby a weight. We'll call that w sub n.",
    "start": "612110",
    "end": "617570"
  },
  {
    "text": "And now, we have to\nsomehow represent the way in which these inputs\nare collected together--",
    "start": "617570",
    "end": "626960"
  },
  {
    "text": "how they have collective force. And we're going to\nmodel that very, very simply just by saying, OK,\nwe'll run it through a summer",
    "start": "626960",
    "end": "637610"
  },
  {
    "text": "like so. But then we have to decide if\nthe collective influence of all",
    "start": "637610",
    "end": "643389"
  },
  {
    "text": "those inputs is sufficient\nto make the neuron fire.",
    "start": "643390",
    "end": "648630"
  },
  {
    "text": "So we're going to\ndo that by running this guy through a\nthreshold box like so.",
    "start": "648630",
    "end": "656110"
  },
  {
    "text": "Here is what the box looks like\nin terms of the relationship between input and the output.",
    "start": "656110",
    "end": "662540"
  },
  {
    "text": "And what you can see\nhere is that nothing happens until the input\nexceeds some threshold t.",
    "start": "662540",
    "end": "669040"
  },
  {
    "text": "If that happens, then\nthe output z is a 1. Otherwise, it's a 0.",
    "start": "669040",
    "end": "676270"
  },
  {
    "text": "So binary, binary out-- we\nmodel the synaptic weights by these multipliers.",
    "start": "676270",
    "end": "681529"
  },
  {
    "text": "We model the cumulative effect\nof all that input to the neuron",
    "start": "681530",
    "end": "686850"
  },
  {
    "text": "by a summer. We decide if it's going to be\nan all-or-none 1 by running it through this threshold\nbox and seeing",
    "start": "686850",
    "end": "693290"
  },
  {
    "text": "if the sum of the products add\nup to more than the threshold.",
    "start": "693290",
    "end": "699370"
  },
  {
    "text": "If so, we get a 1. So what, in the end,\nare we in fact modeling?",
    "start": "699370",
    "end": "706820"
  },
  {
    "text": "Well, with this model,\nwe have number 1, all",
    "start": "706820",
    "end": "714900"
  },
  {
    "text": "or none-- number 2, cumulative\ninfluence-- number 3, oh, I,",
    "start": "714900",
    "end": "732890"
  },
  {
    "text": "suppose synaptic weight. ",
    "start": "732890",
    "end": "740432"
  },
  {
    "text": "But that's not all\nthat there might be to model in a real neuron. We might want to deal with\nthe refractory period.",
    "start": "740432",
    "end": "747142"
  },
  {
    "start": "747142",
    "end": "759180"
  },
  {
    "text": "In these biological models that\nwe build neural nets out of, we might want to model\naxonal bifurcation.",
    "start": "759180",
    "end": "765384"
  },
  {
    "start": "765385",
    "end": "773280"
  },
  {
    "text": "We do get some division\nin the axon of the neuron. And it turns out that that\npulse will either go down",
    "start": "773280",
    "end": "779070"
  },
  {
    "text": "one branch or the other. And which branch it\ngoes down depends on electrical activity in\nthe vicinity of the division.",
    "start": "779070",
    "end": "786870"
  },
  {
    "text": "So these things might actually\nbe a fantastic coincidence detectors. But we're not modeling that.",
    "start": "786870",
    "end": "792006"
  },
  {
    "text": "We don't know how it works. So axonal bifurcation\nmight be modeled.",
    "start": "792006",
    "end": "797290"
  },
  {
    "text": "We might also have a\nlook at time patterns. ",
    "start": "797290",
    "end": "806402"
  },
  {
    "text": "See, what we don't\nknow is we don't know if the timing of the\narrival of these pulses",
    "start": "806402",
    "end": "812240"
  },
  {
    "text": "in the dendritic\ntree has anything to do with what that neuron\nis going to recognize--",
    "start": "812240",
    "end": "818050"
  },
  {
    "text": "so a lot of unknowns here. And now, I'm going\nto show you how to answer a question\nabout neurobiology",
    "start": "818050",
    "end": "825010"
  },
  {
    "text": "with 80% probability\nyou'll get it right. Just say, we don't know.",
    "start": "825010",
    "end": "831910"
  },
  {
    "text": "And that will be with\n80% probability what the neurobiologist would say. ",
    "start": "831910",
    "end": "838700"
  },
  {
    "text": "So this is a model inspired\nby what goes on in our heads. But it's far from clear\nif what we're modeling",
    "start": "838700",
    "end": "847240"
  },
  {
    "text": "is the essence of why those guys\nmake possible what we can do.",
    "start": "847240",
    "end": "853279"
  },
  {
    "text": "Nevertheless, that's where\nwe're going to start. That's where we're going to go. So we've got this model\nof what a neuron does.",
    "start": "853280",
    "end": "860500"
  },
  {
    "text": "So what about what does a\ncollection of these neurons do? Well, we can think of your skull\nas a big box full of neurons.",
    "start": "860500",
    "end": "871830"
  },
  {
    "start": "871830",
    "end": "877680"
  },
  {
    "text": "Maybe a better way\nto think of this is that your head\nis full of neurons. And they in turn are full of\nweights and thresholds like so.",
    "start": "877680",
    "end": "891490"
  },
  {
    "text": "So into this box come a variety\nof inputs x1 through xm.",
    "start": "891490",
    "end": "896675"
  },
  {
    "text": " And these find their\nway to the inside",
    "start": "896676",
    "end": "901780"
  },
  {
    "text": "of this gaggle of neurons. And out here come a bunch\nof outputs c1 through zn.",
    "start": "901780",
    "end": "913320"
  },
  {
    "text": "And there a whole bunch\nof these maybe like so. And there are a lot\nof inputs like so.",
    "start": "913320",
    "end": "919680"
  },
  {
    "text": "And somehow these inputs\nthrough the influence of the weights of the thresholds\ncome out as a set of outputs.",
    "start": "919680",
    "end": "929260"
  },
  {
    "text": "So we can write\nthat down a little fancier by just saying\nthat z is a vector, which",
    "start": "929260",
    "end": "936769"
  },
  {
    "text": "is a function of, certainly\nthe input vector, but also",
    "start": "936770",
    "end": "942220"
  },
  {
    "text": "the weight vector and\nthe threshold vector. So that's all a neural net is.",
    "start": "942220",
    "end": "947780"
  },
  {
    "text": "And when we train\na neural net, all we're going to be able to\ndo is adjust those weights and thresholds so that what\nwe get out is what we want.",
    "start": "947780",
    "end": "957430"
  },
  {
    "text": "So a neural net is a\nfunction approximator. It's good to think about that. It's a function approximator.",
    "start": "957430",
    "end": "963478"
  },
  {
    "text": " So maybe we've got some sample\ndata that gives us an output",
    "start": "963478",
    "end": "971560"
  },
  {
    "text": "vector that's desired as\nanother function of the input,",
    "start": "971560",
    "end": "977865"
  },
  {
    "text": "forgetting about what the\nweights and the thresholds are. That's what we want to get out. And so how well we're\ndoing can be figured out",
    "start": "977865",
    "end": "984990"
  },
  {
    "text": "by comparing the desired\nvalue with the actual value.",
    "start": "984990",
    "end": "991440"
  },
  {
    "text": "So we might think\nthen that we can get a handle on how well\nwe're doing by constructing",
    "start": "991440",
    "end": "998450"
  },
  {
    "text": "some performance function, which\nis determined by the desired",
    "start": "998450",
    "end": "1005770"
  },
  {
    "text": "vector and the input\nvector-- sorry,",
    "start": "1005770",
    "end": "1010960"
  },
  {
    "text": "the desired vector and\nthe actual output vector for some particular input\nor for some set of inputs.",
    "start": "1010960",
    "end": "1018370"
  },
  {
    "text": "And the question is what\nshould that function be? How should we\nmeasure performance",
    "start": "1018370",
    "end": "1023770"
  },
  {
    "text": "given that we have\nwhat we want out here and what we actually\ngot out here?",
    "start": "1023770",
    "end": "1029669"
  },
  {
    "text": "Well, one simple\nthing to do is just to measure the magnitude\nof the difference.",
    "start": "1029670",
    "end": "1036520"
  },
  {
    "text": "That makes sense. But of course, that would give\nus a performance function that",
    "start": "1036520",
    "end": "1044180"
  },
  {
    "text": "is a function of the\ndistance between those vectors would look like this. ",
    "start": "1044180",
    "end": "1052000"
  },
  {
    "text": "But this turns out\nto be mathematically inconvenient in the end. So how do you think we're going\nto turn it up a little bit?",
    "start": "1052000",
    "end": "1058730"
  },
  {
    "text": "AUDIENCE: Normalize it? PATRICK WINSTON: What's that? AUDIENCE: Normalize it? PATRICK WINSTON:\nWell, I don't know.",
    "start": "1058730",
    "end": "1063750"
  },
  {
    "text": "How about just we square it? And that way we're going to go\nfrom this little sharp point",
    "start": "1063750",
    "end": "1069889"
  },
  {
    "text": "down there to something\nthat looks more like that. So it's best when the\ndifference is 0, of course.",
    "start": "1069890",
    "end": "1078300"
  },
  {
    "text": "And it gets worse as\nyou move away from 0. But what we're\ntrying to do here is",
    "start": "1078300",
    "end": "1084080"
  },
  {
    "text": "we're trying to get\nto a minimum value. And I hope you'll forgive me.",
    "start": "1084080",
    "end": "1089360"
  },
  {
    "text": "I just don't like\nthe direction we're going here, because I like to\nthink in terms of improvement as going uphill\ninstead of down hill.",
    "start": "1089360",
    "end": "1096740"
  },
  {
    "text": "So I'm going to dress this up\none more step-- put a minus sign out there.",
    "start": "1096740",
    "end": "1102580"
  },
  {
    "text": "And then our performance\nfunction looks like this. It's always negative. And the best value it\ncan possibly be is zero.",
    "start": "1102580",
    "end": "1109640"
  },
  {
    "text": "So that's what we're going to\nuse just because I am who I am. And it doesn't matter, right? Still, you're trying to\neither minimize or maximize",
    "start": "1109640",
    "end": "1117260"
  },
  {
    "text": "some performance function. OK, so what do we got to do? I guess what we could do is we\ncould treat this thing-- well,",
    "start": "1117260",
    "end": "1126630"
  },
  {
    "text": "we already know what to do. I'm not even sure why we're\ndevoting our lecture to this,",
    "start": "1126630",
    "end": "1131860"
  },
  {
    "text": "because it's clear that\nwhat we're trying to do is we're trying to take our\nweights and our thresholds",
    "start": "1131860",
    "end": "1139840"
  },
  {
    "text": "and adjust them so as\nto maximize performance. So we can make a\nlittle contour map here",
    "start": "1139840",
    "end": "1145570"
  },
  {
    "text": "with a simple neural net\nwith just two weights in it. And maybe it looks like\nthis-- contour map.",
    "start": "1145570",
    "end": "1151562"
  },
  {
    "text": " And at any given time\nwe've got a particular w1",
    "start": "1151562",
    "end": "1158809"
  },
  {
    "text": "and particular w2. And we're trying to\nfind a better w1 and w2. So here we are right now.",
    "start": "1158810",
    "end": "1166549"
  },
  {
    "text": "And there's the contour map. And it's a 6034. So what do we do?",
    "start": "1166550",
    "end": "1171633"
  },
  {
    "text": "AUDIENCE: Climb. PATRICK WINSTON: Simple matter\nof hill climbing, right? So we'll take a step\nin every direction.",
    "start": "1171634",
    "end": "1178360"
  },
  {
    "text": "If we take a step in that\ndirection, not so hot. That actually goes pretty bad.",
    "start": "1178360",
    "end": "1184100"
  },
  {
    "text": "These two are really ugly. Ah, but that one--\nthat one takes us up the hill a little bit.",
    "start": "1184100",
    "end": "1190430"
  },
  {
    "text": "So we're done,\nexcept that I just mentioned that\nHinton's neural net had",
    "start": "1190430",
    "end": "1195870"
  },
  {
    "text": "60 million parameters in it. So we're not going to hill\nclimb with 60 million parameters because it explodes\nexponentially",
    "start": "1195870",
    "end": "1203812"
  },
  {
    "text": "in the number of\nweights you've got to deal with-- the number\nof steps you can take.",
    "start": "1203812",
    "end": "1208936"
  },
  {
    "text": "So this approach is\ncomputationally intractable.  Fortunately, you've all taken\n1801 or the equivalent thereof.",
    "start": "1208936",
    "end": "1219280"
  },
  {
    "text": "So you have a better idea. Instead of just taking a\nstep in every direction, what",
    "start": "1219280",
    "end": "1224769"
  },
  {
    "text": "we're going to do is\nwe're going to take some partial derivatives.",
    "start": "1224770",
    "end": "1230150"
  },
  {
    "text": "And we're going to\nsee what they suggest to us in terms of how we're\ngoing to get around in space.",
    "start": "1230150",
    "end": "1236720"
  },
  {
    "text": "So we might have a partial\nof that performance function up there with respect to w1.",
    "start": "1236720",
    "end": "1242969"
  },
  {
    "text": "And we might also take\na partial derivative of that guy with respect to w2.",
    "start": "1242969",
    "end": "1248450"
  },
  {
    "text": "And these will tell us\nhow much improvement we're getting by making a little\nmovement in those directions,",
    "start": "1248450",
    "end": "1253510"
  },
  {
    "text": "right? How much a change is\ngiven that we're just going right along the axis.",
    "start": "1253510",
    "end": "1258532"
  },
  {
    "text": " So maybe what we ought\nto do is if this guy is",
    "start": "1258532",
    "end": "1267020"
  },
  {
    "text": "much bigger than\nthis guy, it would suggest we mostly want to\nmove in this direction,",
    "start": "1267020",
    "end": "1272120"
  },
  {
    "text": "or to put it in 1801\nterms, what we're going to do is we're going\nto follow the gradient. And so the change\nin the w vector",
    "start": "1272120",
    "end": "1281320"
  },
  {
    "text": "is going to equal to this\npartial derivative times i plus this partial\nderivative times j.",
    "start": "1281320",
    "end": "1290010"
  },
  {
    "text": "So what we're going to end up\ndoing in this particular case by following that formula is\nmoving off in that direction",
    "start": "1290010",
    "end": "1296610"
  },
  {
    "text": "right up to the steepest\npart of the hill. And how much we\nmove is a question.",
    "start": "1296610",
    "end": "1303840"
  },
  {
    "text": "So let's just have a rate\nconstant R that decides how big our step is going to be.",
    "start": "1303840",
    "end": "1310110"
  },
  {
    "text": "And now you think we were done. Well, too bad for our side.",
    "start": "1310110",
    "end": "1315740"
  },
  {
    "text": "We're not done. There's a reason\nwhy we can't use-- create ascent, or in the case\nthat I've drawn our gradient,",
    "start": "1315740",
    "end": "1324214"
  },
  {
    "text": "descent if we take the\nperformance function the other way. Why can't we use it? AUDIENCE: Local maxima.",
    "start": "1324214",
    "end": "1330086"
  },
  {
    "text": " PATRICK WINSTON: The\nremark is local maxima. And that is certainly true.",
    "start": "1330086",
    "end": "1335683"
  },
  {
    "text": "But it's not our first obstacle.  Why doesn't gradient\nascent work?",
    "start": "1335683",
    "end": "1341892"
  },
  {
    "text": " AUDIENCE: So you're\nusing a step function.",
    "start": "1341892",
    "end": "1348250"
  },
  {
    "text": "PATRICK WINSTON: Ah,\nthere's something wrong with our function. That's right. It's non-linear, but\nrather, it's discontinuous.",
    "start": "1348250",
    "end": "1355590"
  },
  {
    "text": "So gradient ascent requires\na continuous space, continuous surface.",
    "start": "1355590",
    "end": "1360870"
  },
  {
    "text": "So too bad our side. It isn't.",
    "start": "1360870",
    "end": "1366010"
  },
  {
    "text": "So what to do? Well, nobody knew what\nto do for 25 years.",
    "start": "1366010",
    "end": "1372840"
  },
  {
    "text": "People were screwing around\nwith training neural nets for 25 years before Paul\nWerbos sadly at Harvard in 1974",
    "start": "1372840",
    "end": "1381340"
  },
  {
    "text": "gave us the answer. And now I want to tell\nyou what the answer is. The first part of the answer is\nthose thresholds are annoying.",
    "start": "1381340",
    "end": "1389920"
  },
  {
    "text": "They're just extra\nbaggage to deal with.",
    "start": "1389920",
    "end": "1395440"
  },
  {
    "text": "What we really like instead of\nc being a function of xw and t was we'd like c prime\nto be a function f",
    "start": "1395440",
    "end": "1403465"
  },
  {
    "text": "prime of x and the weights. But we've got to account\nfor the threshold somehow.",
    "start": "1403465",
    "end": "1410000"
  },
  {
    "text": "So here's how you do that. What you do is\nyou say let us add",
    "start": "1410000",
    "end": "1416220"
  },
  {
    "text": "another input to this neuron. And it's going to\nhave a weight w0.",
    "start": "1416220",
    "end": "1424514"
  },
  {
    "text": " And it's going to be\nconnected to an input that's",
    "start": "1424515",
    "end": "1432120"
  },
  {
    "text": "always minus 1. You with me so far? Now what we're\ngoing to do is we're",
    "start": "1432120",
    "end": "1438190"
  },
  {
    "text": "going to say, let w0 equal t.",
    "start": "1438190",
    "end": "1444070"
  },
  {
    "text": " What does that do to the\nmovement of the threshold? ",
    "start": "1444070",
    "end": "1451660"
  },
  {
    "text": "What it does is it\ntakes that threshold and moves it back to 0. So this little trick here\ntakes this pink threshold",
    "start": "1451660",
    "end": "1459750"
  },
  {
    "text": "and redoes it so that the new\nthreshold box looks like this. ",
    "start": "1459750",
    "end": "1470370"
  },
  {
    "text": "Think about it. If this is t, and this is\nminus 1, then this is minus t.",
    "start": "1470370",
    "end": "1475929"
  },
  {
    "text": "And so this thing ought to\nfire if everything's over-- if the sum is over 0. So it makes sense.",
    "start": "1475930",
    "end": "1481080"
  },
  {
    "text": "And it gets rid of the\nthreshold thing for us. So now we can just\nthink about weights.",
    "start": "1481080",
    "end": "1486920"
  },
  {
    "text": "But still, we've got\nthat step function there.",
    "start": "1486920",
    "end": "1493740"
  },
  {
    "text": "And that's not good. So what we're going\nto do is we're going to smooth that guy out.",
    "start": "1493740",
    "end": "1500700"
  },
  {
    "text": "So this is trick number two. Instead of a step\nfunction, we're going to have this\nthing we lovingly",
    "start": "1500700",
    "end": "1507610"
  },
  {
    "text": "call a sigmoid\nfunction, because it's kind of from an s-type shape. And the function we're going\nto use is this one-- one,",
    "start": "1507610",
    "end": "1518280"
  },
  {
    "text": "well, better make it a little\nbit different-- 1 over 1 plus",
    "start": "1518280",
    "end": "1523710"
  },
  {
    "text": "e to the minus\nwhatever the input is. Let's call the input alpha.",
    "start": "1523710",
    "end": "1530070"
  },
  {
    "text": "Does that makes sense? Is alpha is 0, then it's 1\nover 1 plus 1 plus one half.",
    "start": "1530070",
    "end": "1537559"
  },
  {
    "text": "If alpha is extremely big,\nthen even the minus alpha is extremely small. And it becomes one.",
    "start": "1537560",
    "end": "1544100"
  },
  {
    "text": "It goes up to an asymptotic\nvalue of one here. On the other hand, if alpha\nis extremely negative,",
    "start": "1544100",
    "end": "1550510"
  },
  {
    "text": "than the minus alpha\nis extremely positive. And it goes to 0 asymptotically.",
    "start": "1550510",
    "end": "1556470"
  },
  {
    "text": "So we got the right\nlook to that function. It's a very convenient function.",
    "start": "1556470",
    "end": "1561679"
  },
  {
    "text": "Did God say that neurons\nought to be-- that threshold ought to work like that?",
    "start": "1561680",
    "end": "1568080"
  },
  {
    "text": "No, God didn't say so. Who said so? The math says so.",
    "start": "1568080",
    "end": "1573540"
  },
  {
    "text": "It has the right shape\nand look and the math. And it turns out to\nhave the right math,",
    "start": "1573540",
    "end": "1579522"
  },
  {
    "text": "as you'll see in a moment.  So let's see. Where are we?",
    "start": "1579522",
    "end": "1585450"
  },
  {
    "text": "We decided that\nwhat we'd like to do is take these\npartial derivatives. We know that it was awkward\nto have those thresholds.",
    "start": "1585450",
    "end": "1591230"
  },
  {
    "text": "So we got rid of them. And we noted that it was\nimpossible to have the step function. So we got rid of it.",
    "start": "1591230",
    "end": "1596450"
  },
  {
    "text": "Now, we're a situation\nwhere we can actually take those partial derivatives,\nand see if it gives us a way of training\nthe neural net so as",
    "start": "1596450",
    "end": "1603180"
  },
  {
    "text": "to bring the actual output into\nalignment with what we desire. ",
    "start": "1603180",
    "end": "1608525"
  },
  {
    "text": "So to deal with\nthat, we're going to have to work with the\nworld's simplest neural net.",
    "start": "1608525",
    "end": "1614120"
  },
  {
    "text": "Now, if we've got one\nneuron, it's not a net. But if we've got two-word\nneurons, we've got a net.",
    "start": "1614120",
    "end": "1620020"
  },
  {
    "text": "And it turns out that's the\nworld's simplest neuron. So we're going to look at it--\nnot 60 million parameters,",
    "start": "1620020",
    "end": "1625880"
  },
  {
    "text": "but just a few, actually,\njust two parameters.",
    "start": "1625880",
    "end": "1631390"
  },
  {
    "text": "So let's draw it out. We've got input x. That goes into a multiplier.",
    "start": "1631390",
    "end": "1638559"
  },
  {
    "text": "And it gets multiplied times w1. And that goes into a\nsigmoid box like so.",
    "start": "1638560",
    "end": "1647700"
  },
  {
    "text": "We'll call this p1, by the\nway, product number one. Out here comes y.",
    "start": "1647700",
    "end": "1653270"
  },
  {
    "text": "Y gets multiplied\ntimes another weight. We'll call that w2.",
    "start": "1653270",
    "end": "1660630"
  },
  {
    "text": "The neck produces another\nproduct which we'll call p2. And that goes into\na sigmoid box.",
    "start": "1660630",
    "end": "1669200"
  },
  {
    "text": "And then that comes out as z. And z is the number\nthat we use to determine",
    "start": "1669200",
    "end": "1674230"
  },
  {
    "text": "how well we're doing. And our performance\nfunction p is",
    "start": "1674230",
    "end": "1680269"
  },
  {
    "text": "going to be one\nhalf minus one half, because I like\nthings are going in a direction, times the\ndifference between the desired",
    "start": "1680270",
    "end": "1688330"
  },
  {
    "text": "output and the actual\noutput squared. ",
    "start": "1688330",
    "end": "1694480"
  },
  {
    "text": "So now let's decide what\nthose partial derivatives are going to be. ",
    "start": "1694480",
    "end": "1705220"
  },
  {
    "text": "Let me do it over here. ",
    "start": "1705220",
    "end": "1712976"
  },
  {
    "text": "So what are we\ntrying to compute? Partial of the performance\nfunction p with respect to w2.",
    "start": "1712976",
    "end": "1719100"
  },
  {
    "text": " OK. ",
    "start": "1719100",
    "end": "1727970"
  },
  {
    "text": "Well, let's see. We're trying to figure\nout how much this wiggles when we wiggle that.",
    "start": "1727970",
    "end": "1734536"
  },
  {
    "text": " But you know it goes\nthrough this variable p2.",
    "start": "1734536",
    "end": "1741176"
  },
  {
    "text": "And so maybe what we\ncould do is figure out how much this wiggles--\nhow much z wiggles when we wiggle p2\nand then how much p2",
    "start": "1741176",
    "end": "1748830"
  },
  {
    "text": "wiggles when we wiggle w2. I just multiplied\nthose together.",
    "start": "1748830",
    "end": "1755580"
  },
  {
    "text": "I forget. What's that called? N180-- something or other. AUDIENCE: The chain rule",
    "start": "1755580",
    "end": "1761310"
  },
  {
    "text": "PATRICK WINSTON: The chain rule. So what we're going\nto do is we're going to rewrite that partial\nderivative using chain rule.",
    "start": "1761310",
    "end": "1767230"
  },
  {
    "text": "And all it's doing is\nsaying that there's an intermediate variable. And we can compute how much\nthat end wiggles with respect",
    "start": "1767230",
    "end": "1775380"
  },
  {
    "text": "how much that end\nwiggles by multiplying how much the other guys wiggle.",
    "start": "1775380",
    "end": "1781545"
  },
  {
    "text": "Let me write it down. It makes more sense\nin mathematics. So that's going to be\nable to the partial of p",
    "start": "1781545",
    "end": "1788260"
  },
  {
    "text": "with respect to z times the\npartial of z with respect",
    "start": "1788260",
    "end": "1798650"
  },
  {
    "text": "to p2. ",
    "start": "1798650",
    "end": "1804139"
  },
  {
    "text": "Keep me on track here. Partial of z with respect to w2.",
    "start": "1804140",
    "end": "1809490"
  },
  {
    "text": " Now, I'm going to do something\nfor which I will hate myself.",
    "start": "1809490",
    "end": "1815920"
  },
  {
    "text": "I'm going to erase\nsomething on the board. I don't like to do that. But you know what I'm\ngoing to do, don't you?",
    "start": "1815920",
    "end": "1821900"
  },
  {
    "text": "I'm going to say this is\ntrue by the chain rule.",
    "start": "1821900",
    "end": "1827910"
  },
  {
    "text": "But look, I can\ntake this guy here and screw around with it\nwith the chain rule too.",
    "start": "1827910",
    "end": "1834060"
  },
  {
    "text": "And in fact, what\nI'm going to do is I'm going to replace\nthat with partial of z",
    "start": "1834060",
    "end": "1839996"
  },
  {
    "text": "with respect to p2 and partial\nof p2 with respect to w2.",
    "start": "1839996",
    "end": "1848138"
  },
  {
    "text": "So I didn't erase it after all. But you can see what\nI'm going to do next. Now, I'm going to\ndo same thing with",
    "start": "1848139",
    "end": "1853610"
  },
  {
    "text": "the other partial derivative. But this time, instead of\nwriting down and writing over,",
    "start": "1853610",
    "end": "1858890"
  },
  {
    "text": "I'm just going to expand it\nall out in one go, I think. ",
    "start": "1858890",
    "end": "1865200"
  },
  {
    "text": "So partial of p\nwith respect to w1",
    "start": "1865200",
    "end": "1870620"
  },
  {
    "text": "is equal to the partial\nof p with respect to z, the partial of z with respect\nto p2, the partial of p2",
    "start": "1870620",
    "end": "1881810"
  },
  {
    "text": "with respect to what? Y? Partial of y with respect\nto p1-- partial of p1",
    "start": "1881810",
    "end": "1895170"
  },
  {
    "text": "with respect to w1. So that's going like a zipper\ndown that string of variables",
    "start": "1895170",
    "end": "1903679"
  },
  {
    "text": "expanding each by\nusing the chain rule until we got to the end. So there are some\nexpressions that provide",
    "start": "1903680",
    "end": "1910330"
  },
  {
    "text": "those partial derivatives. ",
    "start": "1910330",
    "end": "1916660"
  },
  {
    "text": "But now, if you'll\nforgive me, it",
    "start": "1916660",
    "end": "1923030"
  },
  {
    "text": "was convenient to write\nthem out that way. That matched the\nintuition in my head. But I'm just going\nto turn them around.",
    "start": "1923030",
    "end": "1928674"
  },
  {
    "text": " It's just a product. I'm just going to\nturn them around.",
    "start": "1928674",
    "end": "1934789"
  },
  {
    "text": "So partial p2, partial\nw2, times partial of z,",
    "start": "1934790",
    "end": "1942610"
  },
  {
    "text": "partial p2, times the\npartial of p with respect",
    "start": "1942610",
    "end": "1948360"
  },
  {
    "text": "to z-- same thing. And now, this one. Keep me on track, because\nif there's a mutation here,",
    "start": "1948360",
    "end": "1954190"
  },
  {
    "text": "it will be fatal. Partial of p1-- partial\nof w1, partial of y,",
    "start": "1954190",
    "end": "1961860"
  },
  {
    "text": "partial p1, partial of p2,\npartial of y, partial of z.",
    "start": "1961860",
    "end": "1972179"
  },
  {
    "text": "There's a partial of p2,\npartial of a performance function with respect to z.",
    "start": "1972180",
    "end": "1978142"
  },
  {
    "text": " Now, all we have to do is figure\nout what those partials are.",
    "start": "1978142",
    "end": "1984740"
  },
  {
    "text": "And we have solved\nthis simple neural net. So it's going to be easy.",
    "start": "1984740",
    "end": "1989750"
  },
  {
    "text": " Where is my board space?",
    "start": "1989750",
    "end": "1995880"
  },
  {
    "text": "Let's see, partial of p2\nwith respect to-- what?",
    "start": "1995880",
    "end": "2002360"
  },
  {
    "text": "That's the product. The partial of z-- the\nperformance function with respect to z. Oh, now I can see why I\nwrote it down this way.",
    "start": "2002360",
    "end": "2010201"
  },
  {
    "text": "Let's see. It's going to be d minus e. We can do that one in our head. ",
    "start": "2010201",
    "end": "2021110"
  },
  {
    "text": "What about the partial\nof p2 with respect to w2. ",
    "start": "2021110",
    "end": "2026520"
  },
  {
    "text": "Well, p2 is equal to y\ntimes w2, so that's easy. That's just y. ",
    "start": "2026520",
    "end": "2037830"
  },
  {
    "text": "Now, all we have to do\nis figure out the partial of z with respect to p2. Oh, crap, it's going\nthrough this threshold box.",
    "start": "2037830",
    "end": "2047020"
  },
  {
    "text": "So I don't know exactly what\nthat partial derivative is. So we'll have to\nfigure that out, right?",
    "start": "2047020",
    "end": "2053780"
  },
  {
    "text": "Because the function relating\nthem is this guy here. And so we have to figure out\nthe partial of that with respect",
    "start": "2053780",
    "end": "2060955"
  },
  {
    "text": "to alpha. All right, so we got to do it.",
    "start": "2060955",
    "end": "2066120"
  },
  {
    "text": "There's no way around it. So we have to destroy something.",
    "start": "2066120",
    "end": "2072620"
  },
  {
    "text": "OK, we're going to\ndestroy our neuron. ",
    "start": "2072620",
    "end": "2089989"
  },
  {
    "text": "So the function\nwe're dealing with is, we'll call it\nbeta, equal to 1 over 1",
    "start": "2089989",
    "end": "2095620"
  },
  {
    "text": "plus e to the minus alpha. And what we want\nis the derivative",
    "start": "2095620",
    "end": "2102711"
  },
  {
    "text": "with respect to alpha of beta. And that's equal to d by\nd alpha of-- you know,",
    "start": "2102711",
    "end": "2113080"
  },
  {
    "text": "I can never remember\nthose quotient formulas. So I am going to rewrite\nit a little different way.",
    "start": "2113080",
    "end": "2119260"
  },
  {
    "text": "I am going to write it as 1\nminus e to the minus alpha to the minus 1, because I\ncan't remember the formula",
    "start": "2119260",
    "end": "2128340"
  },
  {
    "text": "for differentiating a quotient. OK, so let's differentiate it. So that's equal to 1 minus e to\nthe minus alpha to the minus 2.",
    "start": "2128340",
    "end": "2145572"
  },
  {
    "text": " And we got that minus comes\nout of that part of it.",
    "start": "2145572",
    "end": "2151140"
  },
  {
    "text": "Then we got to differentiate\nthe inside of that expression.",
    "start": "2151140",
    "end": "2156660"
  },
  {
    "text": "And when we differentiate the\ninside of that expression, we get e to the minus alpha. AUDIENCE: Dr. Winston--",
    "start": "2156660",
    "end": "2162142"
  },
  {
    "text": "PATRICK WINSTON: Yeah? AUDIENCE: That should be 1 plus. PATRICK WINSTON: Oh,\nsorry, thank you. That was one of those fatal\nmistakes you just prevented.",
    "start": "2162142",
    "end": "2169183"
  },
  {
    "text": "So that's 1 plus. That's 1 plus here too. OK, so we've\ndifferentiated that.",
    "start": "2169183",
    "end": "2175589"
  },
  {
    "text": "We've turned that\ninto a minus 2. We brought the\nminus sign outside. Then we're differentiating\nthe inside.",
    "start": "2175590",
    "end": "2181319"
  },
  {
    "text": "The derivative and the\nexponential is an exponential. Then we got to\ndifferentiate that guy. And that just helps us\nget rid of the minus",
    "start": "2181320",
    "end": "2187770"
  },
  {
    "text": "sign we introduced. So that's the derivative. I'm not sure how much\nthat helps except that I'm",
    "start": "2187770",
    "end": "2196640"
  },
  {
    "text": "going to perform a parlor\ntrick here and rewrite that expression thusly.",
    "start": "2196640",
    "end": "2203510"
  },
  {
    "text": "We want to say\nthat's going to be e to the minus alpha over\n1 plus e to the minus",
    "start": "2203510",
    "end": "2213988"
  },
  {
    "text": "alpha times 1 over 1 plus\ne to the minus alpha.",
    "start": "2213988",
    "end": "2221415"
  },
  {
    "text": "That OK? I've got a lot of\nnodding heads here. So I think I'm on safe ground.",
    "start": "2221415",
    "end": "2228464"
  },
  {
    "text": "But now, I'm going to\nperform another parlor trick. ",
    "start": "2228465",
    "end": "2233700"
  },
  {
    "text": "I am going to add 1, which\nmeans I also have to subtract 1.",
    "start": "2233700",
    "end": "2239770"
  },
  {
    "text": " All right?",
    "start": "2239770",
    "end": "2244840"
  },
  {
    "text": "That's legitimate isn't it? So now, I can rewrite\nthis as 1 plus e",
    "start": "2244840",
    "end": "2252540"
  },
  {
    "text": "to the minus alpha over 1\nplus e to the minus alpha",
    "start": "2252540",
    "end": "2258820"
  },
  {
    "text": "minus 1 over 1 plus e to the\nminus alpha times 1 over 1 plus",
    "start": "2258820",
    "end": "2268085"
  },
  {
    "text": "e to the minus alpha. Any high school\nkid could do that.",
    "start": "2268085",
    "end": "2273200"
  },
  {
    "text": "I think I'm on safe ground. Oh, wait, this is beta.",
    "start": "2273200",
    "end": "2282150"
  },
  {
    "text": "This is beta. AUDIENCE: That's the wrong side. PATRICK WINSTON: Oh,\nsorry, wrong side.",
    "start": "2282150",
    "end": "2288440"
  },
  {
    "text": "Better make this\nbeta and this 1. Any high school kid could do it.",
    "start": "2288440",
    "end": "2293964"
  },
  {
    "text": "OK, so what we've\ngot then is that this is equal to 1 minus\nbeta times beta.",
    "start": "2293964",
    "end": "2302310"
  },
  {
    "text": "That's the derivative. And that's weird\nbecause the derivative of the output with\nrespect to the input",
    "start": "2302310",
    "end": "2307960"
  },
  {
    "text": "is given exclusively\nin terms of the output. It's strange.",
    "start": "2307960",
    "end": "2313020"
  },
  {
    "text": "It doesn't really matter. But it's a curiosity. And what we get out of this is\nthat partial derivative there--",
    "start": "2313020",
    "end": "2319560"
  },
  {
    "text": "that's equal to well,\nthe output is p2.",
    "start": "2319560",
    "end": "2327680"
  },
  {
    "text": "No, the output is z. So it's z time 1 minus e. So whenever we see\nthe derivative of one",
    "start": "2327680",
    "end": "2334380"
  },
  {
    "text": "of these sigmoids with\nrespect to its input, we can just write the output\ntimes one minus alpha,",
    "start": "2334380",
    "end": "2339500"
  },
  {
    "text": "and we've got it. So that's why it's\nmathematically convenient. It's mathematically\nconvenient because when we do this differentiation, we\nget a very simple expression",
    "start": "2339500",
    "end": "2348640"
  },
  {
    "text": "in terms of the output. We get a very simple expression. That's all we really need. ",
    "start": "2348640",
    "end": "2356049"
  },
  {
    "text": "So would you like to\nsee a demonstration? It's a demonstration of\nthe world's smallest neural",
    "start": "2356050",
    "end": "2362800"
  },
  {
    "text": "net in action. ",
    "start": "2362800",
    "end": "2371079"
  },
  {
    "text": "Where is neural nets? Here we go. ",
    "start": "2371080",
    "end": "2377707"
  },
  {
    "text": "So there's our neural net. And what we're\ngoing to do is we're going to train it to\ndo absolutely nothing. What we're going to do is\ntrain it to make the output",
    "start": "2377707",
    "end": "2384308"
  },
  {
    "text": "the same as the input. Not what I'd call a fantastic\nleap of intelligence.",
    "start": "2384308",
    "end": "2389660"
  },
  {
    "text": "But let's see what happens. ",
    "start": "2389660",
    "end": "2398930"
  },
  {
    "text": "Wow! Nothing's happening. ",
    "start": "2398930",
    "end": "2407050"
  },
  {
    "text": "Well, it finally\ngot to the point where the maximum error,\nnot the performance,",
    "start": "2407050",
    "end": "2412530"
  },
  {
    "text": "but the maximum error\nwent below a threshold that I had previously\ndetermined. So if you look at the\ninput here and compare that",
    "start": "2412530",
    "end": "2418810"
  },
  {
    "text": "with the desired output\non the far right, you see it produces an output,\nwhich compared with the desired",
    "start": "2418810",
    "end": "2424060"
  },
  {
    "text": "output, is pretty close. So we can test the\nother way like so.",
    "start": "2424060",
    "end": "2429070"
  },
  {
    "text": "And we can see that\nthe desired output is pretty close to the actual\noutput in that case too.",
    "start": "2429070",
    "end": "2434300"
  },
  {
    "text": "And it took 694 iterations\nto get that done. Let's try it again. ",
    "start": "2434300",
    "end": "2456089"
  },
  {
    "text": "To 823-- of course, this is all\na consequence of just starting off with random weights.",
    "start": "2456090",
    "end": "2461265"
  },
  {
    "text": "By the way, if you started with\nall the weights being the same, what would happen? Nothing because it would\nalways stay the same.",
    "start": "2461265",
    "end": "2467495"
  },
  {
    "text": "So you've got to put\nsome randomization in in the beginning. So it took a long time.",
    "start": "2467495",
    "end": "2472670"
  },
  {
    "text": "Maybe the problem is our\nrate constant is too small. So let's crank up the\nrate counts a little bit",
    "start": "2472670",
    "end": "2478106"
  },
  {
    "text": "and see what happens.  That was pretty fast.",
    "start": "2478106",
    "end": "2483730"
  },
  {
    "text": "Let's see if it was a\nconsequence of random chance. ",
    "start": "2483730",
    "end": "2489510"
  },
  {
    "text": "Run. No, it's pretty fast there--\n57 iterations-- third try-- 67.",
    "start": "2489510",
    "end": "2498109"
  },
  {
    "text": "So it looks like at my initial\nrate constant was too small. So if 0.5 was not\nas good as 5.0,",
    "start": "2498110",
    "end": "2505240"
  },
  {
    "text": "why don't we crank it up\nto 50 and see what happens. ",
    "start": "2505240",
    "end": "2511830"
  },
  {
    "text": "Oh, in this case, 124--\nlet's try it again. ",
    "start": "2511830",
    "end": "2518470"
  },
  {
    "text": "Ah, in this case 117-- so\nit's actually gotten worse. And not only has\nit gotten worse.",
    "start": "2518470",
    "end": "2523920"
  },
  {
    "text": "You'll see there's a little a\nbit of instability showing up",
    "start": "2523920",
    "end": "2529270"
  },
  {
    "text": "as it courses along its\nway toward a solution. So what it looks like is that\nif you've got a rate constant",
    "start": "2529270",
    "end": "2535200"
  },
  {
    "text": "that's too small,\nit takes forever. If you've get a rate\nconstant that's too big, it can of jump too far, as in\nmy diagram which is somewhere",
    "start": "2535200",
    "end": "2545310"
  },
  {
    "text": "underneath the board, you can\ngo all the way across the hill and get to the other side. So you have to be careful\nabout the rate constant.",
    "start": "2545310",
    "end": "2551900"
  },
  {
    "text": "So what you really\nwant to do is you want your rate constant\nto vary with what is happening as you progress\ntoward an optimal performance.",
    "start": "2551900",
    "end": "2563920"
  },
  {
    "text": "So if your performance is going\ndown when you make the jump, you know you've got a rate\nconstant that's too big. If your performance is going\nup when you make a jump,",
    "start": "2563920",
    "end": "2570780"
  },
  {
    "text": "maybe you want to\nincrease-- bump it up a little bit until it\ndoesn't look so good.",
    "start": "2570780",
    "end": "2577460"
  },
  {
    "text": "So is that all there is to it? Well, not quite, because\nthis is the world's simplest",
    "start": "2577460",
    "end": "2583010"
  },
  {
    "text": "neural net. And maybe we ought to\nlook at the world's second simplest neural net.",
    "start": "2583010",
    "end": "2588450"
  },
  {
    "text": "Now, let's call this--\nwell, let's call this x.",
    "start": "2588450",
    "end": "2593982"
  },
  {
    "text": "What we're going to do is we're\ngoing to have a second input. And I don't know.",
    "start": "2593982",
    "end": "2599849"
  },
  {
    "text": "Maybe this is screwy. I'm just going to\nuse color coding here to differentiate between\nthe two inputs and the stuff",
    "start": "2599850",
    "end": "2606734"
  },
  {
    "text": "they go through. ",
    "start": "2606734",
    "end": "2614010"
  },
  {
    "text": "Maybe I'll call this z2 and\nthis z1 and this x1 and x2.",
    "start": "2614010",
    "end": "2619666"
  },
  {
    "text": " Now, if I do that-- if I've\ngot two inputs and two outputs,",
    "start": "2619666",
    "end": "2625140"
  },
  {
    "text": "then my performance\nfunction is going to have two numbers in it-- the\ntwo desired values and the two",
    "start": "2625140",
    "end": "2631800"
  },
  {
    "text": "actual values. And I'm going to\nhave two inputs. But it's the same stuff.",
    "start": "2631800",
    "end": "2637680"
  },
  {
    "text": "I just repeat what I did in\nwhite, only I make it orange. ",
    "start": "2637680",
    "end": "2647440"
  },
  {
    "text": "Oh, but what happens if--\nwhat happens if I do this?",
    "start": "2647440",
    "end": "2652654"
  },
  {
    "start": "2652654",
    "end": "2668850"
  },
  {
    "text": "Say put little cross\nconnections in there. So these two streams\nare going to interact.",
    "start": "2668850",
    "end": "2675030"
  },
  {
    "text": "And then there might\nbe some-- this y can go into another multiplier\nhere and go into a summer here.",
    "start": "2675030",
    "end": "2683290"
  },
  {
    "text": "And likewise, this\ny can go up here and into a multiplier like so.",
    "start": "2683290",
    "end": "2690920"
  },
  {
    "text": "And there are weights all\nover the place like so.",
    "start": "2690920",
    "end": "2701329"
  },
  {
    "text": "This guy goes up in here. And now what happens?",
    "start": "2701330",
    "end": "2706430"
  },
  {
    "text": "Now, we've got a\ndisaster on our hands, because there are all kinds\nof paths through this network.",
    "start": "2706430",
    "end": "2711900"
  },
  {
    "text": "And you can imagine that if this\nwas not just two neurons deep, but three neurons\ndeep, what I would find",
    "start": "2711900",
    "end": "2719170"
  },
  {
    "text": "is expressions that\nlook like that. But you could go this way,\nand then down through, and out",
    "start": "2719170",
    "end": "2725890"
  },
  {
    "text": "here. Or you could go this way and\nthen back up through here.",
    "start": "2725890",
    "end": "2733150"
  },
  {
    "text": "So it looks like there is an\nexponentially growing number of paths through that network.",
    "start": "2733150",
    "end": "2739910"
  },
  {
    "text": "And so we're back to\nan exponential blowup. And it won't work. ",
    "start": "2739910",
    "end": "2750890"
  },
  {
    "text": "Yeah, it won't\nwork except that we need to let the math\nsing to us a little bit. And we need to look\nat the picture.",
    "start": "2750890",
    "end": "2757670"
  },
  {
    "text": "And the reason I turned\nthis guy around was actually because from a point of view\nof letting the math sing to us,",
    "start": "2757670",
    "end": "2766579"
  },
  {
    "text": "this piece here is the\nsame as this piece here. So part of what we\nneeded to do to calculate",
    "start": "2766580",
    "end": "2773570"
  },
  {
    "text": "the partial derivative\nwith respect to w1 has already been done\nwhen we calculated the partial derivative\nwith respect to w2.",
    "start": "2773570",
    "end": "2782550"
  },
  {
    "text": "And not only that,\nif we calculated the partial wit respect\nto these green w's",
    "start": "2782550",
    "end": "2789200"
  },
  {
    "text": "at both levels, what\nwe would discover is that sort of repetition\noccurs over and over again.",
    "start": "2789200",
    "end": "2797839"
  },
  {
    "text": "And now, I'm going to try\nto give you an intuitive idea of what's going on here\nrather than just write down",
    "start": "2797840",
    "end": "2804070"
  },
  {
    "text": "the math and salute it. And here's a way to think\nabout it from an intuitive",
    "start": "2804070",
    "end": "2809980"
  },
  {
    "text": "point of view.  Whatever happens to this\nperformance function",
    "start": "2809980",
    "end": "2816960"
  },
  {
    "text": "that's back of these p's\nhere, the stuff over there can",
    "start": "2816960",
    "end": "2824440"
  },
  {
    "text": "influence p only\nby going through, and influence performance\nonly going through this column",
    "start": "2824440",
    "end": "2829829"
  },
  {
    "text": "of p's. And there's a fixed\nnumber of those. So it depends on the width,\nnot the depth of the network.",
    "start": "2829830",
    "end": "2836335"
  },
  {
    "text": " So the influence of that\nstuff back there on p",
    "start": "2836335",
    "end": "2846030"
  },
  {
    "text": "is going to end up going\nthrough these guys. And it's going to end\nup being so that we're",
    "start": "2846030",
    "end": "2854840"
  },
  {
    "text": "going to discover that a lot of\nwhat we need to compute in one column has already been computed\nin the column on the right.",
    "start": "2854840",
    "end": "2863150"
  },
  {
    "text": "So it isn't going to\nexplode exponentially, because the influence-- let\nme say it one more time.",
    "start": "2863150",
    "end": "2870642"
  },
  {
    "text": " The influences of changes of\nchanges in p on the performance",
    "start": "2870643",
    "end": "2878440"
  },
  {
    "text": "is all we care about when\nwe come back to this part of the network, because\nthis stuff cannot influence",
    "start": "2878440",
    "end": "2885450"
  },
  {
    "text": "the performance except by going\nthrough this column of p's. So it's not going to\nblow up exponentially.",
    "start": "2885450",
    "end": "2891650"
  },
  {
    "text": "We're going to be able to\nreuse a lot of the computation. So it's the reuse principle.",
    "start": "2891650",
    "end": "2897040"
  },
  {
    "text": "Have we ever seen the reuse\nprinciple at work before. Not exactly.",
    "start": "2897040",
    "end": "2902109"
  },
  {
    "text": "But you remember\nthat little business about the extended list? We know that we've\nseen-- we know",
    "start": "2902109",
    "end": "2911184"
  },
  {
    "text": "we've seen something before. So we can stop computing. It's like that. We're going to be able\nto reuse the computation.",
    "start": "2911184",
    "end": "2917870"
  },
  {
    "text": "We've already done it to\nprevent an exponential blowup. By the way, for those of\nyou who know about fast Fourier transform-- same\nkind of idea-- reuse",
    "start": "2917870",
    "end": "2925880"
  },
  {
    "text": "of partial results. So in the end, what can\nwe say about this stuff?",
    "start": "2925880",
    "end": "2932590"
  },
  {
    "text": "In the end, what we can say\nis that it's linear in depth.",
    "start": "2932590",
    "end": "2942725"
  },
  {
    "text": " That is to say if we\nincrease the number of layers",
    "start": "2942725",
    "end": "2948680"
  },
  {
    "text": "to so-called depth,\nthen we're going to increase the\namount of computation necessary in a linear way,\nbecause the computation we",
    "start": "2948680",
    "end": "2955990"
  },
  {
    "text": "need in any column\nis going to be fixed. What about how it goes\nwith respect to the width?",
    "start": "2955990",
    "end": "2966900"
  },
  {
    "text": " Well, with respect to\nthe width, any neuron",
    "start": "2966900",
    "end": "2973500"
  },
  {
    "text": "here can be connected to\nany neuron in the next row. So the amount of work\nwe're going to have to do",
    "start": "2973500",
    "end": "2978860"
  },
  {
    "text": "will be proportional to\nthe number of connections. So with respect to width,\nit's going to be w-squared.",
    "start": "2978860",
    "end": "2987210"
  },
  {
    "text": "But the fact is that in the end,\nthis stuff is readily computed.",
    "start": "2987210",
    "end": "2992260"
  },
  {
    "text": "And this, phenomenally enough,\nwas overlooked for 25 years.",
    "start": "2992260",
    "end": "2998120"
  },
  {
    "text": "So what is it in the end? In the end, it's an\nextremely simple idea. All great ideas are simple.",
    "start": "2998120",
    "end": "3003670"
  },
  {
    "text": "How come there\naren't more of them? Well, because frequently,\nthat simplicity involves finding\na couple of tricks",
    "start": "3003670",
    "end": "3009730"
  },
  {
    "text": "and making a couple\nof observations. So usually, we humans\nare hardly ever",
    "start": "3009730",
    "end": "3014950"
  },
  {
    "text": "go beyond one trick\nor one observation. But if you cascade\na few together, sometimes something\nmiraculous falls out",
    "start": "3014950",
    "end": "3020270"
  },
  {
    "text": "that looks in retrospect\nextremely simple. So that's why we got the\nreuse principle at work--",
    "start": "3020270",
    "end": "3025856"
  },
  {
    "text": "and our reuse computation. In this case, the\nmiracle was a consequence of two tricks plus\nan observation.",
    "start": "3025856",
    "end": "3031590"
  },
  {
    "text": "And the overall idea\nis all great ideas are simple and easy to\noverlook for a quarter century.",
    "start": "3031590",
    "end": "3037500"
  },
  {
    "start": "3037500",
    "end": "3042463"
  }
]