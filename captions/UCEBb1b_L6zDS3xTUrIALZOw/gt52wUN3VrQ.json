[
  {
    "start": "0",
    "end": "14250"
  },
  {
    "text": "MICHALE FEE: Today,\nwe're going to finish up with recurrent neural networks. So as you remember, we've\nbeen talking about the case",
    "start": "14250",
    "end": "23610"
  },
  {
    "text": "where we have a layer of\nneurons in which we have recurrent connections\nbetween neurons in the output",
    "start": "23610",
    "end": "30480"
  },
  {
    "text": "layer of our network. And we've been developing\nthe mathematical tools",
    "start": "30480",
    "end": "36750"
  },
  {
    "text": "to describe the behavior\nof these networks and describe how they\nrespond to their inputs.",
    "start": "36750",
    "end": "43593"
  },
  {
    "text": "And we've been talking about the\ndifferent kinds of computations that recurrent neural\nnetworks can perform.",
    "start": "43593",
    "end": "50140"
  },
  {
    "text": "So you may recall that we\nstarted talking about-- we introduced the\nmath or the concept",
    "start": "50140",
    "end": "57809"
  },
  {
    "text": "of how to study\nrecurrent neural networks by looking at the\nsimplest recurrent network that has a single--",
    "start": "57810",
    "end": "63838"
  },
  {
    "text": "it's a single neuron with a\nrecurrent connection called an autapse. A recurrent connection\nhas a strength lambda.",
    "start": "63838",
    "end": "71020"
  },
  {
    "text": "And we can write down-- let's see. So we can write\ndown the equation for this, the response\nof this neuron,",
    "start": "71020",
    "end": "77700"
  },
  {
    "text": "without a recurrent\nconnection as tau dv dt equals minus v. The minus v is\nessentially a leak term,",
    "start": "77700",
    "end": "86000"
  },
  {
    "text": "so that if you put\ninput into the neuron, the response of the neuron\njumps up and then decays",
    "start": "86000",
    "end": "91920"
  },
  {
    "text": "exponentially in\nresponse to an input, h. If we have a recurrent\nconnection lambda,",
    "start": "91920",
    "end": "98430"
  },
  {
    "text": "then there's an additional\ninput to the neuron that's proportional to the\nfiring rate of the neuron.",
    "start": "98430",
    "end": "105780"
  },
  {
    "text": "We can rewrite that\nequation now as tau dv dt equals minus quantity\none minus lambda times",
    "start": "105780",
    "end": "113579"
  },
  {
    "text": "v plus the input. And the behavior of this\nsimple recurrent neural network",
    "start": "113580",
    "end": "119759"
  },
  {
    "text": "depends strongly on the\nvalue of this coefficient one",
    "start": "119760",
    "end": "125400"
  },
  {
    "text": "minus lambda. And we've talked about\nthree different cases. We've talked about case where\nlambda is less than one,",
    "start": "125400",
    "end": "131410"
  },
  {
    "text": "where lambda is equal to one-- in which case, this\ncoefficient is zero--",
    "start": "131410",
    "end": "137250"
  },
  {
    "text": "and when lambda is\ngreater than one. So let's look at those three\ncases again for this equation.",
    "start": "137250",
    "end": "143049"
  },
  {
    "text": "So when lambda is\nless than one, you can see that this quantity\nright here, this coefficient in front of the v is negative.",
    "start": "143050",
    "end": "150340"
  },
  {
    "text": "And what that means is that\nthe firing rate of this neuron relaxes exponentially\ntoward some h infinity--",
    "start": "150340",
    "end": "160350"
  },
  {
    "text": "sorry, some v infinity. And then when the input\ngoes away, the neuron--",
    "start": "160350",
    "end": "165739"
  },
  {
    "text": "the firing rate decays\nexponentially towards zero. OK, so in the case where\nlambda is equal to one,",
    "start": "165740",
    "end": "173629"
  },
  {
    "text": "you can see that this\ncoefficient is zero. And now you can see\nthat the derivative",
    "start": "173630",
    "end": "178670"
  },
  {
    "text": "of the firing rate of the neuron\nis just equal to the input. What that means is that the\nfiring rate of the neuron",
    "start": "178670",
    "end": "184219"
  },
  {
    "text": "essentially\nintegrates the input. And you can see, if you put\na step input into this neuron",
    "start": "184220",
    "end": "190040"
  },
  {
    "text": "with this recurrent connection\nof lambda equal one, that the response of the neuron\nsimply ramps up linearly,",
    "start": "190040",
    "end": "198200"
  },
  {
    "text": "which corresponds to\nintegrating that step input. And then when the\ninput is turned off",
    "start": "198200",
    "end": "203360"
  },
  {
    "text": "and goes back to\nzero, you can see that the firing rate of\nthe neuron stays constant. And that's because\nthe leak is exactly",
    "start": "203360",
    "end": "211579"
  },
  {
    "text": "balanced by this excitatory\nrecurrent input from the neuron",
    "start": "211580",
    "end": "216680"
  },
  {
    "text": "onto itself. So you can see that for the\ncase for lambda equals one,",
    "start": "216680",
    "end": "221970"
  },
  {
    "text": "there's persistent\nactivity after you put an input into this neuron. And we talked about how this\nforms a short-term memory that",
    "start": "221970",
    "end": "233550"
  },
  {
    "text": "can be used for a bunch\nof different things. It's a short-term\nmemory of a scalar,",
    "start": "233550",
    "end": "239220"
  },
  {
    "text": "or a continuous quantity,\nlike I position. Or we talked about short-term\nmemory integration being used",
    "start": "239220",
    "end": "247770"
  },
  {
    "text": "for path integration or for\naccumulating evidence across",
    "start": "247770",
    "end": "253470"
  },
  {
    "text": "noisy-- over long exposure\nto a noisy stimulus. ",
    "start": "253470",
    "end": "260489"
  },
  {
    "text": "So today, we're going to focus\non networks where this lambda is greater than one.",
    "start": "260490",
    "end": "265620"
  },
  {
    "text": "And in that case, you can see\nthat the differential equation looks like this. So if lambda is\ngreater than one,",
    "start": "265620",
    "end": "271910"
  },
  {
    "text": "then the quantity inside the\nparentheses here is negative. But that's multiplied\nby a minus one.",
    "start": "271910",
    "end": "279400"
  },
  {
    "text": "So the coefficient in\nfront of the v is positive. So if v itself is\na positive number,",
    "start": "279400",
    "end": "287860"
  },
  {
    "text": "then dv dt is also positive. So if v is positive\nand dv dt is positive,",
    "start": "287860",
    "end": "294160"
  },
  {
    "text": "then what that means is that\nthe firing rate of that neuron is growing and, in this case,\nis growing exponentially.",
    "start": "294160",
    "end": "302130"
  },
  {
    "text": "So that when you put an input\nin, the response of the neuron grows exponentially. But when you turn the input off,\nthe firing rate of the neuron",
    "start": "302130",
    "end": "310980"
  },
  {
    "text": "continues to grow exponentially,\nwhich is a little bit crazy.",
    "start": "310980",
    "end": "317090"
  },
  {
    "text": "You know that neurons\nin the brain, of course, don't have firing\nrates that just keep growing exponentially.",
    "start": "317090",
    "end": "323790"
  },
  {
    "text": "So we're going to solve that\nproblem by using nonlinearities in the firing F-I\ncurve of neurons.",
    "start": "323790",
    "end": "331030"
  },
  {
    "text": "But the key point here is\nthat this kind of network actually remembers that there\nwas an input, as opposed",
    "start": "331030",
    "end": "337840"
  },
  {
    "text": "to this kind of network, where\nthe when the input goes away, the activity of the network\njust decays back to zero.",
    "start": "337840",
    "end": "345490"
  },
  {
    "text": "This kind of network\nhas no memory that there was an input\nlong ago in the past.",
    "start": "345490",
    "end": "351230"
  },
  {
    "text": "Whereas, this kind\nof network remembers that there was an input. And so that kind of property\nwhen lambda is greater than one",
    "start": "351230",
    "end": "360250"
  },
  {
    "text": "is useful for storing memories. So we're going to\nexpand on that idea.",
    "start": "360250",
    "end": "366370"
  },
  {
    "text": "In particular, we're\ngoing to use that theme to build networks that\nhave attractors, that",
    "start": "366370",
    "end": "374020"
  },
  {
    "text": "have stable states\nthat they can go to, that depend on prior\ninputs, but also can be used",
    "start": "374020",
    "end": "381610"
  },
  {
    "text": "to store long-term memories. All right? We're going to see how\nthat kind of network",
    "start": "381610",
    "end": "387850"
  },
  {
    "text": "can also be used to produce a\nwinner-take-all network that is sensitive to\nwhich of two inputs",
    "start": "387850",
    "end": "395020"
  },
  {
    "text": "are stronger and stores a\nmemory of preceding inputs where one input is\nstronger than the other.",
    "start": "395020",
    "end": "401740"
  },
  {
    "text": "Or it ends up in a different\nstate when, let's say, input",
    "start": "401740",
    "end": "407990"
  },
  {
    "text": "one is stronger\nthan input 2, and it lands in a different\nstate when input 2 is stronger than input one.",
    "start": "407990",
    "end": "414660"
  },
  {
    "text": "We're going to then describe\na particular model, called a Hopfield model, for how\nattractor networks can",
    "start": "414660",
    "end": "421470"
  },
  {
    "text": "store long-term memories. We're going to introduce\nthe idea of an energy landscape, which is a\nproperty of networks that",
    "start": "421470",
    "end": "429510"
  },
  {
    "text": "have symmetric connections,\nof which the Hopfield model is an example. And then we're going\nto end by talking",
    "start": "429510",
    "end": "435990"
  },
  {
    "text": "about how many memories such\na network can actually store,",
    "start": "435990",
    "end": "441180"
  },
  {
    "text": "known as the capacity problem. OK, so let's start\nwith recurrent networks",
    "start": "441180",
    "end": "447520"
  },
  {
    "text": "with lambda greater than one. So let's start with our autapse. Let's put lambda equal to 2.",
    "start": "447520",
    "end": "453550"
  },
  {
    "text": "And again, you can see that\nif we rewrite this equation with lambda greater\nthan one, we can",
    "start": "453550",
    "end": "460270"
  },
  {
    "text": "write tau dv dt equals lambda\nminus one times v plus h.",
    "start": "460270",
    "end": "465759"
  },
  {
    "text": "You can see that\nthe value of zero,",
    "start": "465760",
    "end": "472020"
  },
  {
    "text": "at the firing rate of\nzero, is an unstable fixed point of the network. Why is that? Because at v equals zero,\nthen dv dt equals zero.",
    "start": "472020",
    "end": "482950"
  },
  {
    "text": "So what that means is that\nif the firing rate is exactly zero, that's a fixed\npoint of the system.",
    "start": "482950",
    "end": "489889"
  },
  {
    "text": "But if v deviates very\nslightly from zero, v becomes very\nslightly positive,",
    "start": "489890",
    "end": "496900"
  },
  {
    "text": "then dv dt is positive, and\nthe firing rate of the neuron starts running away.",
    "start": "496900",
    "end": "502240"
  },
  {
    "text": " So what you can see is if you\nstart the fire rate at zero",
    "start": "502240",
    "end": "509430"
  },
  {
    "text": "and have the input at\nzero, then dv dt is zero, and the network will\nstay at zero firing rate.",
    "start": "509430",
    "end": "517210"
  },
  {
    "text": "But if you put in a very\nslight, a very small input, then dv dt goes positive, and the\nnetwork activity runs away.",
    "start": "517210",
    "end": "526590"
  },
  {
    "text": " Now, let's put in an input\nof the opposite sign.",
    "start": "526590",
    "end": "533260"
  },
  {
    "text": "So now let's start\nwith v equals zero and put in a very\ntiny negative input.",
    "start": "533260",
    "end": "539130"
  },
  {
    "text": "What's the network going to do? So tau dv dt equals v. So v\nis very slightly negative,",
    "start": "539130",
    "end": "546720"
  },
  {
    "text": "or if h is very slightly\nnegative and v is zero, then dv dt will be negative,\nand the network will run away",
    "start": "546720",
    "end": "554130"
  },
  {
    "text": "in the negative direction. So this network actually\ncan produce two memories.",
    "start": "554130",
    "end": "560310"
  },
  {
    "text": "It can produce a memory that a\npreceding input was positive,",
    "start": "560310",
    "end": "566779"
  },
  {
    "text": "or it can store a memory that\na preceding input was negative. So it has two\nconfigurations after you've",
    "start": "566780",
    "end": "575990"
  },
  {
    "text": "put in an input that is\npositive or negative, right? It can produce a positive output\nor a negative output that's",
    "start": "575990",
    "end": "583190"
  },
  {
    "text": "persistent for a long time. Yes? AUDIENCE: Is the [INAUDIBLE]\nof a negative firing rate [INAUDIBLE]?",
    "start": "583190",
    "end": "590210"
  },
  {
    "text": "MICHALE FEE: Yeah. So you can basically\nreformulate everything",
    "start": "590210",
    "end": "595790"
  },
  {
    "text": "that we've been talking\nabout for neurons that have zero, that can't\nhave negative firing rates.",
    "start": "595790",
    "end": "603350"
  },
  {
    "text": "But in this case, we've been\nworking with linear neurons. And it seems like the\nnegative fire rates are pretty",
    "start": "603350",
    "end": "610339"
  },
  {
    "text": "non-physical, non-intuitive. But it's a pretty standard way\nto do the mathematical analysis",
    "start": "610340",
    "end": "618110"
  },
  {
    "text": "for neurons like this, is\nto treat them as linear. But you can sort of reformulate\nall of these networks",
    "start": "618110",
    "end": "624170"
  },
  {
    "text": "in a way that don't have\nthat non-physical property. So for now, let's just bear\nwith this slightly uncomfortable",
    "start": "624170",
    "end": "632510"
  },
  {
    "text": "situation of having neurons\nthat have negative firing rates. Generally, we're going to\nassociate negative firing rates",
    "start": "632510",
    "end": "637880"
  },
  {
    "text": "as inhibition, OK?",
    "start": "637880",
    "end": "643280"
  },
  {
    "text": "But don't worry about that here. All right, so we're going\nto solve this problem",
    "start": "643280",
    "end": "648980"
  },
  {
    "text": "that these neurons have firing\nrates that are kind of running away exponentially by adding a\nnonlinear activation function.",
    "start": "648980",
    "end": "657710"
  },
  {
    "text": "So a typical\nnonlinear activation function that you might\nuse for linear neurons,",
    "start": "657710",
    "end": "663079"
  },
  {
    "text": "like for networks of the\ntype we've been considering, is a symmetric F-I curve,\nwhere if the input is",
    "start": "663080",
    "end": "669710"
  },
  {
    "text": "positive and small, the\nfiring rate of the neuron grows linearly, until you reach\na point where it saturates.",
    "start": "669710",
    "end": "678110"
  },
  {
    "text": "And larger inputs don't\nproduce any larger firing rate of the neuron.",
    "start": "678110",
    "end": "683550"
  },
  {
    "text": "So most neurons actually have\nkind of a saturating F-I curve, like this, like the\nHodgkin-Huxley neurons begin",
    "start": "683550",
    "end": "691190"
  },
  {
    "text": "to saturate. Why is that? Because the sodium channels\nbegin to inactivate,",
    "start": "691190",
    "end": "696410"
  },
  {
    "text": "and it can't fire\nany faster than the--",
    "start": "696410",
    "end": "702000"
  },
  {
    "text": "there's a time\nbetween spikes that's sort of the closest that\nthe neuron-- the fastest",
    "start": "702000",
    "end": "708260"
  },
  {
    "text": "that the neuron can\nspike because of sodium channel inactivation. And then on the minus\nside, if the input",
    "start": "708260",
    "end": "714410"
  },
  {
    "text": "is small and negative, then\nthe firing rate of the neuron goes negative\nlinearly for a while",
    "start": "714410",
    "end": "719660"
  },
  {
    "text": "and then saturates\nat some value. And we typically have the\nneuron saturating between one",
    "start": "719660",
    "end": "725390"
  },
  {
    "text": "and minus one. So now, if you start your\nneuron at zero firing rate",
    "start": "725390",
    "end": "730890"
  },
  {
    "text": "and you put in a\nlittle positive input, what's the neuron going to do? ",
    "start": "730890",
    "end": "737110"
  },
  {
    "text": "Any guesses? AUDIENCE: [INAUDIBLE] MICHALE FEE: Yeah. It's going to start\nrunning up exponentially,",
    "start": "737110",
    "end": "745029"
  },
  {
    "text": "but then it's going\nto saturate up here. And so the firing rate\nwill run up and sit at one.",
    "start": "745030",
    "end": "754810"
  },
  {
    "text": "And if we put in a negative\ninput, a small negative input, then the neuron--",
    "start": "754810",
    "end": "760450"
  },
  {
    "text": "then this little\nrecurrent network will go negative and\nsaturate at minus one, OK?",
    "start": "760450",
    "end": "768490"
  },
  {
    "text": "So you can see that\nthis network actually has one unstable fixed point,\nwhere if it sits exactly",
    "start": "768490",
    "end": "776880"
  },
  {
    "text": "at zero, it will stay\nat zero, until you give a little bit of\ninput in either direction.",
    "start": "776880",
    "end": "784560"
  },
  {
    "text": "And then the network will run up\nand sit at another fixed point here of one.",
    "start": "784560",
    "end": "790350"
  },
  {
    "text": "If you put in a\nbig negative input, you can drive it to\nanother fixed point. And these two are\nstable fixed points,",
    "start": "790350",
    "end": "796470"
  },
  {
    "text": "because once they're\nin that state, if you give little\nperturbations to the network,",
    "start": "796470",
    "end": "802530"
  },
  {
    "text": "it will deviate a little\nbit from that value. If you give a small\nnegative input,",
    "start": "802530",
    "end": "809130"
  },
  {
    "text": "you can cause this to\ndecrease a little bit. But then when the input goes\naway, it will relax back. So this is an\nunstable fixed point,",
    "start": "809130",
    "end": "816510"
  },
  {
    "text": "and these are two\nstable fixed points. Now, we're going to come back\nto this in more detail later.",
    "start": "816510",
    "end": "823510"
  },
  {
    "text": "But we often think\nabout networks like this as sort of\nlike a ball on a hill.",
    "start": "823510",
    "end": "835170"
  },
  {
    "text": "So you can imagine that you\ncan describe this network using",
    "start": "835170",
    "end": "840570"
  },
  {
    "text": "what's called an\nenergy landscape, where if you start this system\nat some point on this sort",
    "start": "840570",
    "end": "847200"
  },
  {
    "text": "of valley-shaped\nhill, all right,",
    "start": "847200",
    "end": "853500"
  },
  {
    "text": "the network sort of-- it's like\na ball that rolls downhill. So if you start the network\nexactly at the peak,",
    "start": "853500",
    "end": "860490"
  },
  {
    "text": "the ball will sit there. But if you give it a\nlittle bit of a nudge, it will roll downhill toward\none of these stable points, OK?",
    "start": "860490",
    "end": "870300"
  },
  {
    "text": "If you start it slightly\non the other side, it will roll this way, OK?",
    "start": "870300",
    "end": "875370"
  },
  {
    "text": "And those stable fixed\npoints are called attractors. And this particular\nnetwork has two tractors--",
    "start": "875370",
    "end": "881550"
  },
  {
    "text": "one with a firing\nrate of one and one at a firing rate of minus one. Yes, Appolonia? AUDIENCE: The\nstable fixed points",
    "start": "881550",
    "end": "887778"
  },
  {
    "text": "of the top graph, where'd\nyou say they were? MICHALE FEE: The\nstable fixed point",
    "start": "887778",
    "end": "892829"
  },
  {
    "text": "is here, because once you-- if the system is\nin this state, you can give slight\nperturbations and the system",
    "start": "892830",
    "end": "899040"
  },
  {
    "text": "returns to that fixed point. This is an unstable fixed\npoint, because if you start the system\nthere and give it",
    "start": "899040",
    "end": "906090"
  },
  {
    "text": "a little nudge in either\ndirection, the state runs away. Does that makes sense? AUDIENCE: Yeah.",
    "start": "906090",
    "end": "911737"
  },
  {
    "text": "MICHALE FEE: Any\nquestions about that? Yes? AUDIENCE: How is the shape of\nthe curve [INAUDIBLE] points",
    "start": "911737",
    "end": "917861"
  },
  {
    "text": "determined based on like-- MICHALE FEE: So\nI'm going to get-- I'm going to come back\nto how you actually",
    "start": "917861",
    "end": "923900"
  },
  {
    "text": "calculate this energy\nlandscape more formally. There's a very precise\nmathematical definition",
    "start": "923900",
    "end": "934870"
  },
  {
    "text": "of how you define\nthis energy landscape. All right, so this was all\nfor the case of one neuron,",
    "start": "934870",
    "end": "942529"
  },
  {
    "text": "all right? So now let's extend it to\nthe case of multiple neurons. So let's just take two\nneurons with an autapse.",
    "start": "942530",
    "end": "952149"
  },
  {
    "text": "One of these autapses have\na value strength of two,",
    "start": "952150",
    "end": "958990"
  },
  {
    "text": "and the other autapse have\na strength of minus two. So this one is recurrent\nand excitatory.",
    "start": "958990",
    "end": "964089"
  },
  {
    "text": "This one is recurrent\nand inhibitory. So now what we're\ngoing to do is we can plot the state of the network.",
    "start": "964090",
    "end": "970540"
  },
  {
    "text": "Now, instead of being\nthe state of the network in one dimension, v, we're\nnow going to have v1 and v2.",
    "start": "970540",
    "end": "979180"
  },
  {
    "text": "So the state of\nthe system is going to be a point in a plane\ngiven by v1 and v2.",
    "start": "979180",
    "end": "986140"
  },
  {
    "text": " So now, by looking\nat this network,",
    "start": "986140",
    "end": "993950"
  },
  {
    "text": "you can see immediately\nthat this particular neuron, this neuron with a\nfiring rate of v2,",
    "start": "993950",
    "end": "1000810"
  },
  {
    "text": "looks like the kind of\nnetwork that we've already studied, right? It has a stable\nfixed point at zero.",
    "start": "1000810",
    "end": "1010170"
  },
  {
    "text": "And this network has two\nstable fixed points-- one at one and the\nother one at minus one.",
    "start": "1010170",
    "end": "1018630"
  },
  {
    "text": "So you can see that\nthis system will also have two stable fixed points--",
    "start": "1018630",
    "end": "1024099"
  },
  {
    "text": "one there and one there, right? Because if I take\nthe input away, this neuron is either\ngoing to one or minus one,",
    "start": "1024099",
    "end": "1032500"
  },
  {
    "text": "and this neuron is\ngoing to go to zero. So there's one and minus\none on the v1 axis.",
    "start": "1032500",
    "end": "1037819"
  },
  {
    "text": "And those two states have zero\nfiring rate on the v2 axis. Is that clear?",
    "start": "1037819",
    "end": "1044429"
  },
  {
    "text": "So now what's going to happen\nif we made this autapse have",
    "start": "1044430",
    "end": "1050400"
  },
  {
    "text": "a strength of two? Anybody want to take a guess? AUDIENCE: That's,\nlike, four attractors?",
    "start": "1050400",
    "end": "1058048"
  },
  {
    "text": "MICHALE FEE: Right. Why is that? AUDIENCE: Because that will\nalso have stable fixed points",
    "start": "1058048",
    "end": "1066424"
  },
  {
    "text": "at [INAUDIBLE]. MICHALE FEE: Right. So this one will have\nstable fixed points",
    "start": "1066424",
    "end": "1073090"
  },
  {
    "text": "at one and minus one. This will also have\nstable fixed points at one and minus one, right?",
    "start": "1073090",
    "end": "1078460"
  },
  {
    "text": "And the system can be in\nany one of four states-- 0, 0. Sorry, 1, 1; minus 1, minus\n1; 1 minus 1; and minus 1, 1.",
    "start": "1078460",
    "end": "1088809"
  },
  {
    "text": "That's right. All right, so I just want to\nmake one other point here,",
    "start": "1088810",
    "end": "1094140"
  },
  {
    "text": "which is that no\nmatter where you start the system for\nthis network, it's",
    "start": "1094140",
    "end": "1099820"
  },
  {
    "text": "going to evolve towards one\nof these stable fixed points, unless I started it exactly\nright there at zero.",
    "start": "1099820",
    "end": "1106549"
  },
  {
    "text": "That will be\nanother fixed point, but that's an\nunstable fixed point. OK, so this system will--",
    "start": "1106550",
    "end": "1113890"
  },
  {
    "text": "no matter where\nI start the state of that system, other than\nthat exact point right there,",
    "start": "1113890",
    "end": "1119720"
  },
  {
    "text": "the network will evolve toward\none of those two attractors. That's why they're\ncalled attractors,",
    "start": "1119720",
    "end": "1124750"
  },
  {
    "text": "because they attract the\nstate of the system toward one of those two points.",
    "start": "1124750",
    "end": "1130809"
  },
  {
    "text": "Yes? AUDIENCE: So are the attractors\ndetermined by the nonlinear",
    "start": "1130810",
    "end": "1136351"
  },
  {
    "text": "activation function? MICHALE FEE: They are. So if this non-linear activation\nfunction saturated at two",
    "start": "1136351",
    "end": "1143840"
  },
  {
    "text": "and minus two, then these two\npoints would be up here at two and minus two.",
    "start": "1143840",
    "end": "1148909"
  },
  {
    "start": "1148910",
    "end": "1153990"
  },
  {
    "text": "So you could see that this\nnetwork has two eigenvalues, right?",
    "start": "1153990",
    "end": "1159179"
  },
  {
    "text": "If we think of it\nas a linear network, this network has\ntwo eigenvalues. The connection matrix is given\nby a diagonal matrix with a two",
    "start": "1159180",
    "end": "1169019"
  },
  {
    "text": "and a minus two along\nthe diagonals, right? So let's take a look at\nthis kind of network. Now, instead of an\nautapse network,",
    "start": "1169020",
    "end": "1177300"
  },
  {
    "text": "we have recurrent connections\nof strength minus 2 and minus 2. So what does that\nweight matrix look like?",
    "start": "1177300",
    "end": "1186045"
  },
  {
    "text": "AUDIENCE: 0, minus\n2; minus 2, 0. MICHALE FEE: 0, minus\n2; minus 2, 0, right?",
    "start": "1186045",
    "end": "1191990"
  },
  {
    "text": "Well, what are the\neigenvalues of this network?",
    "start": "1191990",
    "end": "1197700"
  },
  {
    "text": "Anybody remember that? AUDIENCE: [INAUDIBLE] MICHALE FEE: Right. It's a plus b and a minus b.",
    "start": "1197700",
    "end": "1205409"
  },
  {
    "text": "And so the eigenvalues\nof this network are 0 plus negative 2\nand 0 minus negative 2.",
    "start": "1205410",
    "end": "1213540"
  },
  {
    "text": "So it's 2 and minus 2, right? So this network here will have\nexactly the same eigenvalues",
    "start": "1213540",
    "end": "1220950"
  },
  {
    "text": "as this network. But what's going\nto be different? What are the eigenvectors?",
    "start": "1220950",
    "end": "1227165"
  },
  {
    "text": "AUDIENCE: The 45. MICHALE FEE: The 45 degrees. So the eigenvectors of this\nnetwork are the x- and y-axes.",
    "start": "1227165",
    "end": "1233940"
  },
  {
    "text": "The eigenvectors of this\nnetwork are the 45-degree lines. So anybody want to\ntake a guess as to what",
    "start": "1233940",
    "end": "1239670"
  },
  {
    "text": "the stable states of this-- ",
    "start": "1239670",
    "end": "1245080"
  },
  {
    "text": "it's just this network\nrotated by 45 degrees, right? ",
    "start": "1245080",
    "end": "1251929"
  },
  {
    "text": "So those are now the attractors\nof this network, right? And that makes sense, right?",
    "start": "1251930",
    "end": "1257750"
  },
  {
    "text": "This neuron can be\npositive, but that's going to be strongly driving\nthis neuron negative.",
    "start": "1257750",
    "end": "1265460"
  },
  {
    "text": "But if this neuron\nis negative, that's going to be strongly driving\nthis neuron positive, right?",
    "start": "1265460",
    "end": "1271370"
  },
  {
    "text": "And so this network will\nwant to sit out here on this line in this direction\nor in this direction.",
    "start": "1271370",
    "end": "1280279"
  },
  {
    "text": "And because of the saturation-- if there were no saturation,\nif this were a linear network,",
    "start": "1280280",
    "end": "1285880"
  },
  {
    "text": "the activity of this\nneuron would just be running exponentially\nup these 45-degree lines.",
    "start": "1285880",
    "end": "1292660"
  },
  {
    "text": "But because of\nthe saturation, it gets stuck here at 1, minus 1. Or rather, minus\n1, 1 or 1, minus 1.",
    "start": "1292660",
    "end": "1300880"
  },
  {
    "text": "Any questions about that? Yeah, Jasmine? ",
    "start": "1300880",
    "end": "1306412"
  },
  {
    "text": "AUDIENCE: So the two\nfixed points right now, like it's [INAUDIBLE]?",
    "start": "1306412",
    "end": "1312590"
  },
  {
    "text": "MICHALE FEE: Yeah. It'll be one in this direction\nand one in that direction.",
    "start": "1312590",
    "end": "1318912"
  },
  {
    "text": "AUDIENCE: So why [INAUDIBLE]? MICHALE FEE: Because\nthis neuron is saturated.",
    "start": "1318912",
    "end": "1323970"
  },
  {
    "text": "Because the saturation is acting\nat the level of the individual neurons. AUDIENCE: OK.",
    "start": "1323970",
    "end": "1329797"
  },
  {
    "text": "MICHALE FEE: So each\nneuron will go up to its own saturation point. OK?",
    "start": "1329797",
    "end": "1337400"
  },
  {
    "text": "All right.  So this kind of network\nis actually pretty cool.",
    "start": "1337400",
    "end": "1344400"
  },
  {
    "text": "This network can\nimplement decision-making. It can decide, for example,\nwhether one input is bigger",
    "start": "1344400",
    "end": "1350280"
  },
  {
    "text": "than the other, all right? So if we have an input-- so let's start our\nnetwork right here",
    "start": "1350280",
    "end": "1355890"
  },
  {
    "text": "at this unstable fixed\npoint, all right? We've carefully balanced\nthe ball on top of the hill, and it just sits there.",
    "start": "1355890",
    "end": "1362240"
  },
  {
    "text": "And now let's put an input\nthat is in this direction h, so that it's slightly\npointing to the right",
    "start": "1362240",
    "end": "1369710"
  },
  {
    "text": "of this diagonal line. So what's going to happen? It's going to kick the\nstate of the network up in this direction, right?",
    "start": "1369710",
    "end": "1377300"
  },
  {
    "text": "But we've already discussed\nhow if the network state is",
    "start": "1377300",
    "end": "1382670"
  },
  {
    "text": "anywhere on either\nside of that line, it will evolve toward\nthe fixed point.",
    "start": "1382670",
    "end": "1389000"
  },
  {
    "text": "If the h is on\nthe other side, it will kick the network\nunstable fixed point",
    "start": "1389000",
    "end": "1395970"
  },
  {
    "text": "into this part of\nthe state space. And then the network will evolve\ntoward this fixed point, OK?",
    "start": "1395970",
    "end": "1403470"
  },
  {
    "text": "These half planes\nhere, this region here, is called the attractor\nbasin for this attractor.",
    "start": "1403470",
    "end": "1412930"
  },
  {
    "text": "And on this side, it's\ncalled attractor basin for that attractor, OK?",
    "start": "1412930",
    "end": "1418950"
  },
  {
    "text": "And you can see that\nthis network will be very sensitive to\nwhichever input, h1 or h2,",
    "start": "1418950",
    "end": "1426660"
  },
  {
    "text": "is slightly larger.  So let me show you\nwhat that looks",
    "start": "1426660",
    "end": "1432000"
  },
  {
    "text": "like in this little movie. So we're going to\nstart with our network",
    "start": "1432000",
    "end": "1440450"
  },
  {
    "text": "exactly at the zero point. And we're going to give an\ninput in this direction. And you can see that we've\nkicked the network slightly",
    "start": "1440450",
    "end": "1447500"
  },
  {
    "text": "this way. And now the network evolves\ntoward the fixed point, and it stays there. Now if we give a\nbig input this way,",
    "start": "1447500",
    "end": "1453860"
  },
  {
    "text": "we can push network\nover, push it to the other side of this\ndividing line between the two",
    "start": "1453860",
    "end": "1461000"
  },
  {
    "text": "basins of attraction,\nand now the network sits here at this fixed point. We can kick it again with\nanother input and push it back.",
    "start": "1461000",
    "end": "1468800"
  },
  {
    "text": "So it's kind of like\na flip-flop, right? It's pretty cool. It detects which\ninput was larger,",
    "start": "1468800",
    "end": "1477740"
  },
  {
    "text": "pushes the network into an\nattractor that then remembers which input was\nlarger for, basically,",
    "start": "1477740",
    "end": "1483710"
  },
  {
    "text": "as long as the network-- as long as you allow the\nnetwork to sit there. OK? ",
    "start": "1483710",
    "end": "1490540"
  },
  {
    "text": "All right, any\nquestions about that? ",
    "start": "1490540",
    "end": "1495740"
  },
  {
    "text": "Yes, Rebecca? AUDIENCE: Sorry. So the basin is just like\neach side of that [INAUDIBLE]?? MICHALE FEE: That's right.",
    "start": "1495740",
    "end": "1500870"
  },
  {
    "text": "That's the basin of\nattraction for this attractor. If you start the network\nanywhere in this half plane,",
    "start": "1500870",
    "end": "1508340"
  },
  {
    "text": "the network will evolve\ntoward that attractor. And you can use that as a\nwinner-take-all decision-making",
    "start": "1508340",
    "end": "1517670"
  },
  {
    "text": "network by starting the\nnetwork right there at zero. And small kicks in\neither direction",
    "start": "1517670",
    "end": "1524510"
  },
  {
    "text": "will cause the network to relax\ninto one of these attractors and maintain that memory.",
    "start": "1524510",
    "end": "1530450"
  },
  {
    "text": " Now let's talk about sort\nof a formal implementation",
    "start": "1530450",
    "end": "1540659"
  },
  {
    "text": "of a system for producing\nmemories, long-term memories, all right? And that's called\na Hopfield model.",
    "start": "1540660",
    "end": "1547530"
  },
  {
    "text": "And the Hopfield\nmodel is actually one of the best current\nmodels for understanding",
    "start": "1547530",
    "end": "1556259"
  },
  {
    "text": "how memory systems like\nthe hippocampus work. So the basic idea\nis that we have",
    "start": "1556260",
    "end": "1563639"
  },
  {
    "text": "neurons in the hippocampus, in\nparticular in the CA3 region of the hippocampus, that\nhave very prominent-- a lot",
    "start": "1563640",
    "end": "1571200"
  },
  {
    "text": "of recurrent connectivity\nbetween those neurons, all right? And so you have input\nfrom entorhinal cortex",
    "start": "1571200",
    "end": "1579070"
  },
  {
    "text": "and from the dentate\ngyrus that sort of serve as the stimuli that come\ninto that network and form--",
    "start": "1579070",
    "end": "1587320"
  },
  {
    "text": "and burn memories into\nthat part of the network by changing the synaptic\nweights within that network.",
    "start": "1587320",
    "end": "1594630"
  },
  {
    "text": "[INAUDIBLE] that\nsome time later, when similar inputs\ncome in, they can reactivate the memory\nin the hippocampus.",
    "start": "1594630",
    "end": "1603040"
  },
  {
    "text": "And you recognize and remember\nthat pattern of stimuli.",
    "start": "1603040",
    "end": "1608980"
  },
  {
    "text": "All right, so we're going to-- actually, so an example of\nhow this looks when you record",
    "start": "1608980",
    "end": "1615790"
  },
  {
    "text": "neurons in the hippocampus,\nit looks like this. So here's a mouse or a rat with\nelectrodes in its hippocampus.",
    "start": "1615790",
    "end": "1624400"
  },
  {
    "text": "If you put it in a\nlittle arena like this, it will run around and\nexplore for a while.",
    "start": "1624400",
    "end": "1630190"
  },
  {
    "text": "You can record where the rat\nis in that arena [AUDIO OUT] from neurons.",
    "start": "1630190",
    "end": "1636730"
  },
  {
    "text": "And measure when the\nneurons spike and look",
    "start": "1636730",
    "end": "1641740"
  },
  {
    "text": "at how the firing\nrate of those neurons relates to the\nposition of the animal. So the black trace here shows\nall of the locations where",
    "start": "1641740",
    "end": "1650710"
  },
  {
    "text": "the rat was when it was\nrunning around the maze, and the red dot shows where\none of these neurons in CA3",
    "start": "1650710",
    "end": "1657250"
  },
  {
    "text": "of the hippocampus generated a\nspike, where the rat was when",
    "start": "1657250",
    "end": "1662890"
  },
  {
    "text": "that neuron generates a spike. And those are shown\nwith red dots here. And you can see that\nthis neuron generates",
    "start": "1662890",
    "end": "1671679"
  },
  {
    "text": "spiking when the animal is in\na particular restricted region",
    "start": "1671680",
    "end": "1676900"
  },
  {
    "text": "of the cage, of its environment. And different neurons show\ndifferent localized regions.",
    "start": "1676900",
    "end": "1685040"
  },
  {
    "text": "So these regions are\ncalled place fields, because they are the places\nin the environment where",
    "start": "1685040",
    "end": "1690430"
  },
  {
    "text": "that neurons spikes. Different neurons have\ndifferent place fields. You can actually record\nfrom many of these neurons--",
    "start": "1690430",
    "end": "1697480"
  },
  {
    "text": "and looking at the pattern\nof neurons that are spiking, you can actually figure\nout where the rat was or is",
    "start": "1697480",
    "end": "1705370"
  },
  {
    "text": "at any given moment,\njust by looking at which of these neurons is spiking. That's pretty obvious, right? If this neuron is spiking\nand this neuron isn't, all",
    "start": "1705370",
    "end": "1713650"
  },
  {
    "text": "these other neurons,\nthen the animal is going to be-- you know\nthat the animal is somewhere in that location right there.",
    "start": "1713650",
    "end": "1720430"
  },
  {
    "text": " All right, so in a sense,\nthe activity of these neurons",
    "start": "1720430",
    "end": "1727140"
  },
  {
    "text": "reflects the animal remembering,\nor sort of remembering, that it's in a\nparticular location.",
    "start": "1727140",
    "end": "1734340"
  },
  {
    "text": "It's in a cage. It looks at the walls\nof the environment. It sees a little-- they use colored\ncards on the wall",
    "start": "1734340",
    "end": "1741780"
  },
  {
    "text": "to give the animal\ncues as to where it is. So they look around and they\nsay, oh, yeah, I'm here.",
    "start": "1741780",
    "end": "1747158"
  },
  {
    "text": "In my environment, there's a\nred card there and a yellow card there, and that's\nwhere I am right now.",
    "start": "1747158",
    "end": "1753690"
  },
  {
    "text": "So that's the way you think\nabout these hippocampal place fields as being like a memory.",
    "start": "1753690",
    "end": "1758730"
  },
  {
    "text": "On top of that, this\npart of the hippocampus is necessary for the actual\nformation of memories",
    "start": "1758730",
    "end": "1766950"
  },
  {
    "text": "in a broader sense-- not\njust spatial locations, but more generally in terms\nof life events, right?",
    "start": "1766950",
    "end": "1776520"
  },
  {
    "text": "For humans, the hippocampus\nis an essential part of the brain for\nstoring memories.",
    "start": "1776520",
    "end": "1782129"
  },
  {
    "text": " All right, so let's\ncome back to this idea",
    "start": "1782130",
    "end": "1788309"
  },
  {
    "text": "of our recurrent network. And what we're\ngoing to do is we're going to start adding\nmore and more neurons to our recurrent network.",
    "start": "1788310",
    "end": "1795785"
  },
  {
    "text": "All right, so here's\nwhat the attractor looked like for\nthe case where we have one eigenvalue in the\nsystem that's greater than one,",
    "start": "1795785",
    "end": "1802180"
  },
  {
    "text": "another one that's\nless than one. If we now make both\nof these neurons",
    "start": "1802180",
    "end": "1808070"
  },
  {
    "text": "have recurrent connections\nthat are stronger than one, now we're going to have\nfour attractors, right?",
    "start": "1808070",
    "end": "1814759"
  },
  {
    "text": "Each one of these has\ntwo stable fixed points-- a one and minus one.",
    "start": "1814760",
    "end": "1820790"
  },
  {
    "text": "So here, for these\ntwo states, v1 is one. And for these two\nstates, v1 is negative 1.",
    "start": "1820790",
    "end": "1828529"
  },
  {
    "text": "For these two states, v2\nis 1, and these two states, v2 is negative one, all right?",
    "start": "1828530",
    "end": "1833540"
  },
  {
    "text": "So you can see every time\nwe add another neuron or another neuron to our\nnetwork that has an autapse,",
    "start": "1833540",
    "end": "1841880"
  },
  {
    "text": "every time we add another\nneuron with another eigenvalue, we add more possible\nstates of the network, OK?",
    "start": "1841880",
    "end": "1851740"
  },
  {
    "text": "So if we had two neurons,\nwe have one neuron with an eigenvalue with an\nautapse greater than one,",
    "start": "1851740",
    "end": "1858730"
  },
  {
    "text": "we have two states. If we have two, we\nhave four states. If we have three of those,\nwe have eight states.",
    "start": "1858730",
    "end": "1865130"
  },
  {
    "text": "So you can see that if we\nhave n of these neurons with recurrent excitation with\na lambda of greater than one,",
    "start": "1865130",
    "end": "1873620"
  },
  {
    "text": "we have 2 to the\nn possible states that that system can be in, OK?",
    "start": "1873620",
    "end": "1878840"
  },
  {
    "text": "So I don't know exactly how\nmany neurons are in CA3.",
    "start": "1878840",
    "end": "1883919"
  },
  {
    "text": "It has to be several\nmillion, maybe 10 million. We don't know the exact number. But 2 to that is a lot of\npossible states, right?",
    "start": "1883920",
    "end": "1893500"
  },
  {
    "text": " So the problem is that--",
    "start": "1893500",
    "end": "1900120"
  },
  {
    "text": "so let's think about how\nthis thing acts as a memory. So it turns out that this little\ndevice that we've built here",
    "start": "1900120",
    "end": "1907940"
  },
  {
    "text": "is actually a lot like\na computer memory. It's like a register,\nwhere we can write a value.",
    "start": "1907940",
    "end": "1918184"
  },
  {
    "text": " So we can write in\nhere a 1, minus 1, 1.",
    "start": "1918185",
    "end": "1924830"
  },
  {
    "text": "And as long as we leave\nthat network alone, it will store that value.",
    "start": "1924830",
    "end": "1930030"
  },
  {
    "text": " Or we can write a 1, 1, 1,\nand it will store that value.",
    "start": "1930030",
    "end": "1937300"
  },
  {
    "text": "But that's not really\nwhat we mean when we talk about memories, right? We have a memory of meeting\nsomebody for lunch yesterday,",
    "start": "1937300",
    "end": "1947350"
  },
  {
    "text": "right? That is a particular\nconfiguration of sensory inputs",
    "start": "1947350",
    "end": "1955980"
  },
  {
    "text": "that we experienced.  So the other way to think about\nthis is this kind of network",
    "start": "1955980",
    "end": "1963760"
  },
  {
    "text": "is just a short-term memory. We can program in some values-- 1, 1, 1. But if we were to turn the\nactivity of these neurons off,",
    "start": "1963760",
    "end": "1973190"
  },
  {
    "text": "we'd erase the memory, right? How do we build\ninto this network",
    "start": "1973190",
    "end": "1979020"
  },
  {
    "text": "a long-term memory,\nsomething that we can turn all these\nneurons off and then",
    "start": "1979020",
    "end": "1985429"
  },
  {
    "text": "the network sort of goes back\ninto the remembered state? You do that by\nbuilding connections",
    "start": "1985430",
    "end": "1992510"
  },
  {
    "text": "between these neurons,\nsuch that only some of these possible\nstates are actually",
    "start": "1992510",
    "end": "1999559"
  },
  {
    "text": "stable states, all right? So let me give you\nan example of this. So if you have a whole\nbunch of neurons--",
    "start": "1999560",
    "end": "2006410"
  },
  {
    "text": "n neurons. You've got 2 to the\nn possible states that that network can sit in.",
    "start": "2006410",
    "end": "2013440"
  },
  {
    "text": "What we want is for\nonly some of those",
    "start": "2013440",
    "end": "2018529"
  },
  {
    "text": "to actually be stable\nstates of the system. ",
    "start": "2018530",
    "end": "2024280"
  },
  {
    "text": "So, for example, when we\nwake up in the morning and we see the dresser or maybe\nthe nightstand next to the bed,",
    "start": "2024280",
    "end": "2036040"
  },
  {
    "text": "we want to remember\nthat's our bedroom. We want that to be a particular\nconfiguration of inputs",
    "start": "2036040",
    "end": "2043950"
  },
  {
    "text": "that we recall, right? So what you want is you\nwant a set of neurons",
    "start": "2043950",
    "end": "2050899"
  },
  {
    "text": "that have particular states\nthat the system evolves toward that are stable\nstates of the system.",
    "start": "2050900",
    "end": "2060780"
  },
  {
    "text": "So the way you do\nthat is you take this network with\nrecurrent autapses",
    "start": "2060780",
    "end": "2067290"
  },
  {
    "text": "and you build cross-connections\nbetween them that make particular of\nthose possible states",
    "start": "2067290",
    "end": "2075210"
  },
  {
    "text": "actual stable states\nof the system. We want to restrict the number\nof stable states in the system.",
    "start": "2075210",
    "end": "2080830"
  },
  {
    "text": "So take a look at\nthis network here. So here we have two neurons.",
    "start": "2080830",
    "end": "2086069"
  },
  {
    "text": "You know that if you had\nautapses between these-- of these neurons to\nthemselves, there",
    "start": "2086070",
    "end": "2092638"
  },
  {
    "text": "would be four possible\nstable states. But if we now build\nexcitatory cross-connections",
    "start": "2092639",
    "end": "2098790"
  },
  {
    "text": "between those neurons,\ntwo of those states actually are no\nlonger stable states.",
    "start": "2098790",
    "end": "2104880"
  },
  {
    "text": "They become unstable. And only these two remain\nstable states of this system,",
    "start": "2104880",
    "end": "2112410"
  },
  {
    "text": "remain attractors. ",
    "start": "2112410",
    "end": "2118550"
  },
  {
    "text": "If we put inhibitory connections\nbetween those neurons, then we can make these\ntwo states the attractors",
    "start": "2118550",
    "end": "2125690"
  },
  {
    "text": "of the system, OK? All right. Does that make sense?",
    "start": "2125690",
    "end": "2131540"
  },
  {
    "start": "2131540",
    "end": "2137460"
  },
  {
    "text": "All right, so let's\nactually flesh out the mathematics of how you\ntake a network of neurons",
    "start": "2137460",
    "end": "2145560"
  },
  {
    "text": "and program it to\nhave particular states that are tractors of\nthe system, all right?",
    "start": "2145560",
    "end": "2151920"
  },
  {
    "text": "So we've been using this\nkind of dynamical equation. We're going to simplify that.",
    "start": "2151920",
    "end": "2157920"
  },
  {
    "text": "We're going to follow\nthe construction that John Hopfield\nused when he analyzed",
    "start": "2157920",
    "end": "2163109"
  },
  {
    "text": "these recurrent networks. And instead of writing\ndown a continuous update so",
    "start": "2163110",
    "end": "2171270"
  },
  {
    "text": "that we update the-- in the\nformulation we've been using, we update the firing\nrate of our neuron using this\ndifferential equation.",
    "start": "2171270",
    "end": "2178710"
  },
  {
    "text": "We're going to simplify\nit by just writing down the state of the network\nat time t plus 1.",
    "start": "2178710",
    "end": "2185769"
  },
  {
    "text": "That's a function of\nthe state of the network of the previous time step. So we're going to\ndiscretize time.",
    "start": "2185770",
    "end": "2193060"
  },
  {
    "text": "We're going to say v,\nthe state of the network, the firing rates of all the\nneurons at time t plus 1, is just a function\nof a weight matrix",
    "start": "2193060",
    "end": "2201079"
  },
  {
    "text": "that connects all the neurons\ntimes the state of the system, times the firing rate vector.",
    "start": "2201080",
    "end": "2207310"
  },
  {
    "text": "And then this can also have\nan input into it, all right? ",
    "start": "2207310",
    "end": "2214480"
  },
  {
    "text": "All right. And here, I'm just writing\nout exactly what that matrix",
    "start": "2214480",
    "end": "2220190"
  },
  {
    "text": "multiplication looks like. It's the state of the i-th\n[vector?] after we update",
    "start": "2220190",
    "end": "2226610"
  },
  {
    "text": "the state of the network is just\na sum over all of the different",
    "start": "2226610",
    "end": "2232020"
  },
  {
    "text": "inputs coming from all\nof the other neurons, all the j other neurons.",
    "start": "2232020",
    "end": "2238920"
  },
  {
    "text": "And we're going to simplify\nour neuronal activation function to just make it into\na binary threshold neuron.",
    "start": "2238920",
    "end": "2245849"
  },
  {
    "text": "So if the input is positive,\nthen the firing rate of neuron will be positive. If the input is negative,\nthe firing rate of the neuron",
    "start": "2245850",
    "end": "2252710"
  },
  {
    "text": "will be negative. All right? And that's the sine function.",
    "start": "2252710",
    "end": "2257780"
  },
  {
    "text": "It's 1 if x is greater\nthan 0 and minus 1 if x is less than or equal to 0.",
    "start": "2257780",
    "end": "2263498"
  },
  {
    "text": "All right, so the goal is\nto build a network that can store any memory we\nwant, any pattern we want,",
    "start": "2263498",
    "end": "2271910"
  },
  {
    "text": "and turn that into\na stable state. So we're going to\nbuild a network that",
    "start": "2271910",
    "end": "2277160"
  },
  {
    "text": "will evolve toward a particular\npattern that we want. And xi is just a pattern\nof ones and minus ones",
    "start": "2277160",
    "end": "2285859"
  },
  {
    "text": "that describes that\nmemory that we're building into the network, all right?",
    "start": "2285860",
    "end": "2292100"
  },
  {
    "text": "So xi is just a one or\nminus one for every neuron in the network. So xi i is one or minus\none for the i-th neuron.",
    "start": "2292100",
    "end": "2299960"
  },
  {
    "text": " Now, we want xi to be\nan attractor, right?",
    "start": "2299960",
    "end": "2307530"
  },
  {
    "text": "We want to build a network\nsuch that xi is an attractor. And what that means is that--",
    "start": "2307530",
    "end": "2312930"
  },
  {
    "text": "what does building\na network mean? When we say build a network,\nwhat are we actually doing?",
    "start": "2312930",
    "end": "2319829"
  },
  {
    "text": "What is it here that we're\nactually trying to decide? AUDIENCE: The seminal roots.",
    "start": "2319830",
    "end": "2326060"
  },
  {
    "text": "MICHALE FEE: Yeah, which is? AUDIENCE: Like the matrix M. MICHALE FEE: The M, right. So when I say build a\nnetwork that does this,",
    "start": "2326060",
    "end": "2332300"
  },
  {
    "text": "I mean choose a set of M's\nthat has this property. So what we want is we want\nto find a weight matrix M",
    "start": "2332300",
    "end": "2342270"
  },
  {
    "text": "such that if the network\nis in a stable state, is in this desired\nstate, that when",
    "start": "2342270",
    "end": "2349730"
  },
  {
    "text": "we multiply that state\ntimes the matrix M and we take the\nsine of that sum,",
    "start": "2349730",
    "end": "2357720"
  },
  {
    "text": "you're going to get\nthe same state back. In other words, you start\nthe network in this state, it's going to end up\nin the same state.",
    "start": "2357720",
    "end": "2363095"
  },
  {
    "text": "That's what it means to\nhave an attractor, OK? That's what it means to say\nthat it's a stable state.",
    "start": "2363095",
    "end": "2371390"
  },
  {
    "text": "OK, so we're going to\ntry a particular matrix.  And I'm going to describe\nwhat this actually",
    "start": "2371390",
    "end": "2378970"
  },
  {
    "text": "looks like in more detail. But the matrix that\nprograms a pattern",
    "start": "2378970",
    "end": "2384820"
  },
  {
    "text": "xi into the network\nas an attractor is this weight\nmatrix right here.",
    "start": "2384820",
    "end": "2390170"
  },
  {
    "text": "So if we have a pattern\nxi, our weight matrix is some constant times\nthe outer product",
    "start": "2390170",
    "end": "2398370"
  },
  {
    "text": "of that pattern with itself. I'm going to explain\nwhat that means. What that means is that\nif neuron i and neuron",
    "start": "2398370",
    "end": "2408060"
  },
  {
    "text": "j are both active\nin this pattern, both have a firing\nrate of one, then",
    "start": "2408060",
    "end": "2416860"
  },
  {
    "text": "those two neurons are going\nto be connected to each other, right? They're going to have a\nconnection between them that",
    "start": "2416860",
    "end": "2423880"
  },
  {
    "text": "has a value of one, or alpha. If one of those neurons\nhas a firing rate of one",
    "start": "2423880",
    "end": "2430340"
  },
  {
    "text": "and the other neuron has\na firing rate of zero, then what weight do\nwe want between them?",
    "start": "2430340",
    "end": "2435885"
  },
  {
    "text": "If one of them has\na firing rate of one and the other has a\nfiring rate of minus one, the strength of the connection\nwe want between them",
    "start": "2435885",
    "end": "2441620"
  },
  {
    "text": "is minus one. So if one neuron is active\nand another neuron is active, we want them to excite\neach other to maintain",
    "start": "2441620",
    "end": "2447770"
  },
  {
    "text": "that as a stable state. If one neuron is plus and\nthe other one is minus, we want them to\ninhibit each other,",
    "start": "2447770",
    "end": "2454970"
  },
  {
    "text": "because that will make\nthat configuration stable. OK, notice that's\na symmetric matrix.",
    "start": "2454970",
    "end": "2461660"
  },
  {
    "text": "So let's actually take our\ndynamical equation that says how we go from\nthe state at time t",
    "start": "2461660",
    "end": "2467829"
  },
  {
    "text": "to the state of time t plus 1\nand put in this weight matrix",
    "start": "2467830",
    "end": "2473380"
  },
  {
    "text": "and see whether this pattern\nxi is actually a stable state.",
    "start": "2473380",
    "end": "2480861"
  },
  {
    "text": "So let's do that, Let's take this M and stick\nit in there, substitute it in. Notice this is a sum over j,\nso we can pull the xi i out.",
    "start": "2480862",
    "end": "2491140"
  },
  {
    "text": "And now, you see that\nv at t plus 1 is this.",
    "start": "2491140",
    "end": "2496299"
  },
  {
    "text": "And it's the sine of\na times xi i times the sum of j of xi j, xi k.",
    "start": "2496300",
    "end": "2503550"
  },
  {
    "text": "Now, what is that? Any idea what that is? So the elements of xi are what?",
    "start": "2503550",
    "end": "2510180"
  },
  {
    "text": "They're just ones or minus ones. So xi j times xi j has to be?",
    "start": "2510180",
    "end": "2520320"
  },
  {
    "text": "AUDIENCE: One. MICHALE FEE: One. And we're summing\nover n neurons.",
    "start": "2520320",
    "end": "2525339"
  },
  {
    "text": "So this sum has to\nhave a value N. So",
    "start": "2525340",
    "end": "2532920"
  },
  {
    "text": "you can see that the\nstate at time t plus 1-- if we start to network\nin this stored state,",
    "start": "2532920",
    "end": "2541830"
  },
  {
    "text": "it's just this-- sine of a N xi. But a is positive.",
    "start": "2541830",
    "end": "2548010"
  },
  {
    "text": "N is just a positive\ninteger, number of neurons. So this equal xi.",
    "start": "2548010",
    "end": "2555650"
  },
  {
    "text": "So if we have this\nweight matrix, we start to network\nin that stored state,",
    "start": "2555650",
    "end": "2562610"
  },
  {
    "text": "the state at the next time\nstep will be the same state. So it's a stable fixed point.",
    "start": "2562610",
    "end": "2569830"
  },
  {
    "text": "All right, so let's just\ngo through an example. That is the prescription\nfor programming a memory",
    "start": "2569830",
    "end": "2577800"
  },
  {
    "text": "into a Hopfield network, OK? And notice that it's just-- it's essentially a\nHebbian learning rule.",
    "start": "2577800",
    "end": "2585090"
  },
  {
    "text": "So the way you do this is\nyou activate the neurons with a particular pattern, and\nany two neurons that are active",
    "start": "2585090",
    "end": "2592370"
  },
  {
    "text": "together form a positive\nexcitatory connection between them.",
    "start": "2592370",
    "end": "2598040"
  },
  {
    "text": "Any two neurons\nwhere one is positive and the other is negative\nform a symmetric inhibitory",
    "start": "2598040",
    "end": "2605510"
  },
  {
    "text": "connection, all right? ",
    "start": "2605510",
    "end": "2614853"
  },
  {
    "text": "All right, so let's take\na particular example. Let's make a\nthree-neuron network that",
    "start": "2614853",
    "end": "2620280"
  },
  {
    "text": "stores a pattern 1, 1, minus 1. And again, the notation\nhere is xi, xi transpose.",
    "start": "2620280",
    "end": "2626340"
  },
  {
    "text": "That's an outer\nproduct, just like you use to compute the covariance\nmatrix of a data matrix.",
    "start": "2626340",
    "end": "2636990"
  },
  {
    "text": "So there's a pattern\nwe're going to program in. The weight matrix\nis xi, xi transpose,",
    "start": "2636990",
    "end": "2643410"
  },
  {
    "text": "but it's 1, 1, minus\n1 times 1, 1, minus 1. You can see that's going\nto give you this matrix",
    "start": "2643410",
    "end": "2649650"
  },
  {
    "text": "here, all right? So that element\nthere is 1 times 1. That element there. So here are two neurons.",
    "start": "2649650",
    "end": "2656860"
  },
  {
    "text": "These two neurons storing this\npattern, these two neurons--",
    "start": "2656860",
    "end": "2662170"
  },
  {
    "text": "sorry, this neuron has a\nfiring rate of minus one. So the connection between\nthat neuron and itself",
    "start": "2662170",
    "end": "2670840"
  },
  {
    "text": "is a one, right? It's just the product\nof that times that.",
    "start": "2670840",
    "end": "2676660"
  },
  {
    "text": "All right any questions about\nhow we got this weight matrix? I think it's pretty\nstraightforward.",
    "start": "2676660",
    "end": "2685040"
  },
  {
    "text": "So is that a stable point? Let's just multiply it out. We take this vector and\nmultiply it by this matrix.",
    "start": "2685040",
    "end": "2693800"
  },
  {
    "text": "There's our stored pattern. There's our matrix that\nstores that pattern. And we're just going\nto multiply this out.",
    "start": "2693800",
    "end": "2699950"
  },
  {
    "text": "You can see that 1\ntimes 1 plus 1 times 1 plus minus 1 times minus 1 is 3.",
    "start": "2699950",
    "end": "2707120"
  },
  {
    "text": "You just do that for\neach of the neurons. ",
    "start": "2707120",
    "end": "2712670"
  },
  {
    "text": "Take the sine of that. And you can see that\nthat's just 1, 1, minus 1. So 1, 1, minus 1 is\na stable fixed point.",
    "start": "2712670",
    "end": "2720170"
  },
  {
    "text": "Now let's see if it's\nactually an attractor. So when a state is an\nattractor, what that means is",
    "start": "2720170",
    "end": "2726380"
  },
  {
    "text": "if we start to network\nat a state that's a little bit different from\nthat and advance the network one",
    "start": "2726380",
    "end": "2732530"
  },
  {
    "text": "time step, it will converge\ntoward the attractor. So into our network that stores\nthis pattern 1, 1, minus 1,",
    "start": "2732530",
    "end": "2741590"
  },
  {
    "text": "let's put in a different\npattern and see what happens. So we're going to take\nthat weight matrix,",
    "start": "2741590",
    "end": "2747470"
  },
  {
    "text": "multiply it by this initial\nstate, multiply it out,",
    "start": "2747470",
    "end": "2752750"
  },
  {
    "text": "and you can see\nthat next state is going to be the sine\nof 3, 3, minus 3.",
    "start": "2752750",
    "end": "2760040"
  },
  {
    "text": "And one time step advanced,\nthe network is now in the state",
    "start": "2760040",
    "end": "2765440"
  },
  {
    "text": "that we've programmed in. Does that make sense?",
    "start": "2765440",
    "end": "2770530"
  },
  {
    "text": "So that state is a stable fixed\npoint and it's an attractor.",
    "start": "2770530",
    "end": "2776272"
  },
  {
    "text": "I'm just going to go\nthrough this very quickly. I'm just going to prove that xi\nis an attractor of the network",
    "start": "2776272",
    "end": "2782760"
  },
  {
    "text": "if we write down the network\nas the outer product of this. The matrix elements\nare the outer product",
    "start": "2782760",
    "end": "2790470"
  },
  {
    "text": "of the stored state, OK? So what we're going\nto do is we're going to calculate the total\ninput onto the i-th neuron",
    "start": "2790470",
    "end": "2797770"
  },
  {
    "text": "if we start from an\narbitrary state, v. So k",
    "start": "2797770",
    "end": "2803530"
  },
  {
    "text": "is the input to all\nthe neurons, right? And it's just that matrix\ntimes the initial state.",
    "start": "2803530",
    "end": "2812880"
  },
  {
    "text": "So v j is the firing\nrate of the j-th neuron, and k is just M times v.\nThat's the pattern of inputs",
    "start": "2812880",
    "end": "2820520"
  },
  {
    "text": "to all of our neurons. So what is that? k equals-- we're just going to\nput this weight matrix",
    "start": "2820520",
    "end": "2826490"
  },
  {
    "text": "into this equation, all right? We can pull the xi i\noutside of the sum,",
    "start": "2826490",
    "end": "2833300"
  },
  {
    "text": "because it doesn't depend on j. The sum is over j. Now let's just write\nout this sum, OK?",
    "start": "2833300",
    "end": "2840480"
  },
  {
    "text": "Now, you can see\nthat if you start out with an initial state that has\nsome number of neurons that",
    "start": "2840480",
    "end": "2847470"
  },
  {
    "text": "have the correct sign that\nare already overlapping",
    "start": "2847470",
    "end": "2852570"
  },
  {
    "text": "with the memorized state\nand some number of neurons in that initial\nstate don't overlap",
    "start": "2852570",
    "end": "2857940"
  },
  {
    "text": "with the memorized\nstate, we can write out this sum as two terms. We can write it as a sum\nover some of the neurons that",
    "start": "2857940",
    "end": "2867000"
  },
  {
    "text": "are already in the correct state\nand a sum over neurons that are not in the correct state.",
    "start": "2867000",
    "end": "2873080"
  },
  {
    "text": " So if these neurons\nin that initial state",
    "start": "2873080",
    "end": "2879490"
  },
  {
    "text": "have the right sign, that means\nthese two have the same sign. And so the sum over\nxi j vj for neurons",
    "start": "2879490",
    "end": "2888040"
  },
  {
    "text": "where v has the\nright sign is just the number of neurons\nthat has the correct sign.",
    "start": "2888040",
    "end": "2893680"
  },
  {
    "text": "And this sum over\nincorrect neurons means these neurons have the\nopposite sign of the desired",
    "start": "2893680",
    "end": "2900010"
  },
  {
    "text": "memory. And so those will be one,\nand those will be minus one. Or those will be minus\none, and those will be one.",
    "start": "2900010",
    "end": "2906820"
  },
  {
    "text": "And so this will be minus the\nnumber of incorrect neurons. So you can see that\nthe input of the neuron",
    "start": "2906820",
    "end": "2913660"
  },
  {
    "text": "will have the right sign if\nthe number of correct neurons",
    "start": "2913660",
    "end": "2918789"
  },
  {
    "text": "is more than the number of\nincorrect neurons, all right? So what that means is that\nif you program a pattern",
    "start": "2918790",
    "end": "2926810"
  },
  {
    "text": "into this network\nand then I drive an input into the network,\nwhere most of the inputs drive--",
    "start": "2926810",
    "end": "2938050"
  },
  {
    "text": "if the input drives most of the\nneurons with the right sign,",
    "start": "2938050",
    "end": "2945570"
  },
  {
    "text": "then the inputs will\ncause the network to evolve toward the memorized\npattern in the next timestamp.",
    "start": "2945570",
    "end": "2955580"
  },
  {
    "text": "OK, so let me say that again,\nbecause I felt like that didn't come out very clearly. We program a pattern\ninto our network.",
    "start": "2955580",
    "end": "2963050"
  },
  {
    "text": "If we start to network at some-- let's say at zero.",
    "start": "2963050",
    "end": "2968299"
  },
  {
    "text": "And then we put in a pattern\ninto the network such that just the majority\nof the neurons",
    "start": "2968300",
    "end": "2977150"
  },
  {
    "text": "are activated in a way that\nlooks like the stored pattern,",
    "start": "2977150",
    "end": "2982490"
  },
  {
    "text": "then in the next time\nstep, all of the neurons will have this stored pattern. So let me show you\nwhat that looks like.",
    "start": "2982490",
    "end": "2988640"
  },
  {
    "text": " Let me actually go\nahead and show you--",
    "start": "2988640",
    "end": "2994280"
  },
  {
    "start": "2994280",
    "end": "3000930"
  },
  {
    "text": "OK, so here's an\nexample of that. So you can use Hopfield\nnetworks to store",
    "start": "3000930",
    "end": "3006080"
  },
  {
    "text": "many different kinds of things,\nincluding images, all right? So this is a network\nwhere each pixel",
    "start": "3006080",
    "end": "3011630"
  },
  {
    "text": "is being represented by a\nneuron in a Hopfield network. And a particular image\nwas stored in that network",
    "start": "3011630",
    "end": "3020330"
  },
  {
    "text": "by setting up the pattern\nof synaptic weights just using that xi, xi transpose\nlearning rule for the weight",
    "start": "3020330",
    "end": "3029900"
  },
  {
    "text": "matrix M, OK? Now, what you can do is you\ncan [INAUDIBLE] that network from a random initial condition.",
    "start": "3029900",
    "end": "3038960"
  },
  {
    "text": "And then let the network\nevolve over time, all right? And what you see is that\nthe network converges",
    "start": "3038960",
    "end": "3046880"
  },
  {
    "text": "toward the pattern that was\nstored in the synaptic [?], OK?",
    "start": "3046880",
    "end": "3055154"
  },
  {
    "text": "Does that make sense? ",
    "start": "3055155",
    "end": "3061190"
  },
  {
    "text": "Got that? So, basically, as long\nas that initial pattern",
    "start": "3061190",
    "end": "3071226"
  },
  {
    "text": "has some overlap with\nthe stored pattern, the network will evolve\ntoward the stored pattern.",
    "start": "3071226",
    "end": "3077120"
  },
  {
    "start": "3077120",
    "end": "3083710"
  },
  {
    "text": "All right, so let me\ndefine a little bit better what we mean by\nthe energy landscape",
    "start": "3083710",
    "end": "3089975"
  },
  {
    "text": "and how it's actually defined.  OK, so you remember that\nif we start our network",
    "start": "3089975",
    "end": "3097360"
  },
  {
    "text": "in a particular pattern v,\nthe recurrent connections will drive inputs into all\nthe neurons in the network.",
    "start": "3097360",
    "end": "3107840"
  },
  {
    "text": "And those inputs\nwill then determine the pattern of activity\nat the next time step.",
    "start": "3107840",
    "end": "3113280"
  },
  {
    "text": "So if we have a state\nof the network v, the inputs to the network, to\nall the neurons in the network,",
    "start": "3113280",
    "end": "3122110"
  },
  {
    "text": "from the currently\nactive neurons is given by the\nconnection matrix times v.",
    "start": "3122110",
    "end": "3129400"
  },
  {
    "text": "So we can just write that\nout as a sum like this. So you define the energy of the\nnetwork as the dot product--",
    "start": "3129400",
    "end": "3139490"
  },
  {
    "text": "basically, the\namount of overlap-- between the current\nstate of the network",
    "start": "3139490",
    "end": "3146480"
  },
  {
    "text": "and the inputs to\nall of the neurons that drive the activity\nin the next step, OK?",
    "start": "3146480",
    "end": "3155270"
  },
  {
    "text": "And the energy is minus, OK? So what that means is if the\nnetwork is in a state that",
    "start": "3155270",
    "end": "3162110"
  },
  {
    "text": "has a big overlap with the\npattern of inputs to all",
    "start": "3162110",
    "end": "3167450"
  },
  {
    "text": "the other neurons,\nthen the energy will be very negative, right? And remember, the system likes\nto evolve toward low energies.",
    "start": "3167450",
    "end": "3175940"
  },
  {
    "text": "In physics, you have\na ball on a hill. It rolls downhill, right, to\nlower gravitational energies.",
    "start": "3175940",
    "end": "3184150"
  },
  {
    "text": "So you start the ball\nanywhere on the hill, and it will roll downhill. So these networks\ndo the same thing.",
    "start": "3184150",
    "end": "3190000"
  },
  {
    "text": "They evolve downward\non this energy surface. They evolve towards\nstates that have",
    "start": "3190000",
    "end": "3198280"
  },
  {
    "text": "a high overlap with the inputs\nthat drive the next state.",
    "start": "3198280",
    "end": "3203440"
  },
  {
    "text": "Does that make sense? So if you're in a state\nwhere the pattern right now",
    "start": "3203440",
    "end": "3211600"
  },
  {
    "text": "has a high overlap with\nwhat the pattern is going to be in the next time step,\nthen you're in an attractor,",
    "start": "3211600",
    "end": "3217539"
  },
  {
    "text": "right?  OK, so it looks like that.",
    "start": "3217540",
    "end": "3225050"
  },
  {
    "text": "So this energy is just\nnegative of the overlap of the current state of the\nnetwork with the pattern",
    "start": "3225050",
    "end": "3231860"
  },
  {
    "text": "of inputs to all the neurons. Yes, Rebecca? AUDIENCE: So [INAUDIBLE] to\nsay [INAUDIBLE] with the weight",
    "start": "3231860",
    "end": "3237789"
  },
  {
    "text": "matrix, since that's sort of\nthe goal of the next time step, and it will evolve towards\nthe matrix [INAUDIBLE]??",
    "start": "3237790",
    "end": "3242819"
  },
  {
    "text": "MICHALE FEE: Yeah. So the only difference is\nthat the state of the network is this vector, right?",
    "start": "3242820",
    "end": "3249750"
  },
  {
    "text": "And the weight matrix tells us\nhow that state will drive input into all the other neurons.",
    "start": "3249750",
    "end": "3256980"
  },
  {
    "text": "And so if you're in a state\nthat drives a pattern of inputs",
    "start": "3256980",
    "end": "3263050"
  },
  {
    "text": "to all the neurons that looks\nexactly like the current state, then you're going to stay\nin that state, right?",
    "start": "3263050",
    "end": "3271190"
  },
  {
    "text": "And so the energy is just\ndefined as that dot product, the overlap of the current\nstate, or the state",
    "start": "3271190",
    "end": "3278140"
  },
  {
    "text": "that you're calculating\nthe energy of, and the inputs to the network\nin the next time step.",
    "start": "3278140",
    "end": "3284515"
  },
  {
    "text": "All right, so let me show\nyou what that looks like.  And so the energy is\nlowest, current state",
    "start": "3284515",
    "end": "3292130"
  },
  {
    "text": "has a high overlap\nwith the synaptic drive to the next step. So let's just take a look at\nthis particular network here.",
    "start": "3292130",
    "end": "3299190"
  },
  {
    "text": "I've rewritten this\ndot product as-- so k is just M times\nv. This dot product",
    "start": "3299190",
    "end": "3304700"
  },
  {
    "text": "can just be written as\nv transpose times Mv.",
    "start": "3304700",
    "end": "3309829"
  },
  {
    "text": "So that's the energy. Let's take a look at this\nmatrix, this network here--",
    "start": "3309830",
    "end": "3315400"
  },
  {
    "text": "0, minus 2, minus 2, 0. So it's this mutually\ninhibitory network. You know that that\ninhibitory network",
    "start": "3315400",
    "end": "3321500"
  },
  {
    "text": "has attractors that are here\nat minus 1, 1 and 1, minus 1.",
    "start": "3321500",
    "end": "3330220"
  },
  {
    "text": "So let's actually\ncalculate the energy. So you can actually\ntake these states-- 1, minus 1-- multiply\nit by that M,",
    "start": "3330220",
    "end": "3338670"
  },
  {
    "text": "and then take the dot\nproduct with 1, minus 1. And do that for each\none of those states",
    "start": "3338670",
    "end": "3343920"
  },
  {
    "text": "and write down the energy. You can see that the\nenergy here is minus 1.",
    "start": "3343920",
    "end": "3349160"
  },
  {
    "text": "The energy here is minus 1,\nand the energy here is 0. So if you start the network\nhere, at an energy zero,",
    "start": "3349160",
    "end": "3357240"
  },
  {
    "text": "it's going to roll\ndownhill to this state. ",
    "start": "3357240",
    "end": "3365920"
  },
  {
    "text": "Or it can roll\ndownhill to this state, depending on the\ninitial condition, OK?",
    "start": "3365920",
    "end": "3374030"
  },
  {
    "text": " So you can also think about the\nenergy as a function of firing",
    "start": "3374030",
    "end": "3384500"
  },
  {
    "text": "rates continuously. You can calculate that energy,\nnot just for these points on this grid.",
    "start": "3384500",
    "end": "3390560"
  },
  {
    "text": "And what you see is\nthat there's basically-- in high dimensions,\nthere are sort of valleys that\ndescribe the attractor",
    "start": "3390560",
    "end": "3399710"
  },
  {
    "text": "basin of these different\nattractors, all right? And if you project that energy\nalong an axis like this,",
    "start": "3399710",
    "end": "3408350"
  },
  {
    "text": "you can see that you sort of-- let's say, take a slice\nthrough this energy function.",
    "start": "3408350",
    "end": "3415340"
  },
  {
    "text": "You can see that this\nlooks just like the energy surface, the energy function,\nthat we described before",
    "start": "3415340",
    "end": "3421190"
  },
  {
    "text": "for the 1D factor, the single\nneuron with two attractors,",
    "start": "3421190",
    "end": "3426819"
  },
  {
    "text": "right? This corresponds to\na valley and a valley and a peak between them.",
    "start": "3426820",
    "end": "3433200"
  },
  {
    "text": "And then the energy gets\nbig outside of that. And questions about that?",
    "start": "3433200",
    "end": "3440115"
  },
  {
    "text": "Yes, [INAUDIBLE]. AUDIENCE: [INAUDIBLE] vector 1/2\nbecause-- in this case, right?",
    "start": "3440115",
    "end": "3446680"
  },
  {
    "text": "MICHALE FEE: That's the general\ndefinition, minus 1/2 v dot k. ",
    "start": "3446680",
    "end": "3457390"
  },
  {
    "text": "It actually doesn't really-- this 1/2 doesn't really matter. It actually comes out of\nthe derivative of something,",
    "start": "3457390",
    "end": "3465190"
  },
  {
    "text": "as I recall. But a scaling factor\ndoesn't matter. The network always evolves\ntoward a minimum of the energy.",
    "start": "3465190",
    "end": "3471550"
  },
  {
    "text": "And so this 1/2\ncould be anything. ",
    "start": "3471550",
    "end": "3478170"
  },
  {
    "text": "All right, so the point is that\nstarting the network anywhere",
    "start": "3478170",
    "end": "3484680"
  },
  {
    "text": "with a sensory input, the system\nwill evolve toward the nearest memory, OK?",
    "start": "3484680",
    "end": "3490580"
  },
  {
    "text": " And I already showed you this.",
    "start": "3490580",
    "end": "3495780"
  },
  {
    "text": "OK, so now, a very\ninteresting question is, how many memories can you\nactually store in a network?",
    "start": "3495780",
    "end": "3503500"
  },
  {
    "text": "And there's a very simple way\nof calculating the capacity of the Hopfield network.",
    "start": "3503500",
    "end": "3509680"
  },
  {
    "text": "And I'm just going to show\nyou the outlines of it. And that actually\ngives us some insight",
    "start": "3509680",
    "end": "3515220"
  },
  {
    "text": "into what kinds of\nmemories you can store. Basically, the idea is that\nwhen you store memories",
    "start": "3515220",
    "end": "3522630"
  },
  {
    "text": "in a network, you want\nthe different memories to be as uncorrelated with\neach other as possible.",
    "start": "3522630",
    "end": "3528267"
  },
  {
    "text": "You don't want to try\nto store memories that are very similar to each other.",
    "start": "3528267",
    "end": "3533690"
  },
  {
    "text": "And you'll see why in a second\nwhen we look at the map. So let's say that we want\nto store multiple memories",
    "start": "3533690",
    "end": "3540760"
  },
  {
    "text": "in our network. So instead of just\nstoring one pattern, xi,",
    "start": "3540760",
    "end": "3546100"
  },
  {
    "text": "we want to store a bunch\nof different patterns xi. And so let's say we're going\nto store P different patterns.",
    "start": "3546100",
    "end": "3552530"
  },
  {
    "text": "So we have a\nparameter variable mu. An index mu addresses each\nof the different patterns",
    "start": "3552530",
    "end": "3559670"
  },
  {
    "text": "we want to store. So we're going to store zero to\np patterns, p minus 1 patterns.",
    "start": "3559670",
    "end": "3565730"
  },
  {
    "text": "So what we do, the\nway we do that is we compute the\ncontribution to the weight",
    "start": "3565730",
    "end": "3571330"
  },
  {
    "text": "make matrix from each of\nthose different patterns. So we calculate a weight\nmatrix using the outer product",
    "start": "3571330",
    "end": "3578440"
  },
  {
    "text": "for each of the patterns we\nwant to store in the network, all right? And then we add all\nof those together.",
    "start": "3578440",
    "end": "3586420"
  },
  {
    "text": "We're going to essentially sort\nof average together the network",
    "start": "3586420",
    "end": "3594280"
  },
  {
    "text": "that we would make for\neach pattern separately. Does that makes sense?",
    "start": "3594280",
    "end": "3600480"
  },
  {
    "text": "So there is the equation\nfor the weight matrix that stores p different patterns\nin our memory, in our network.",
    "start": "3600480",
    "end": "3610060"
  },
  {
    "text": " And that's how we got\nthis kind of network",
    "start": "3610060",
    "end": "3616690"
  },
  {
    "text": "here, where we store\nmultiple memories, all right? ",
    "start": "3616690",
    "end": "3622920"
  },
  {
    "text": "So let me just show\nyou an example of what happens when you do that. So I found these\nnice videos online.",
    "start": "3622920",
    "end": "3628260"
  },
  {
    "text": "So here is a representation\nof a network that stores",
    "start": "3628260",
    "end": "3633540"
  },
  {
    "text": "a five by five array of pixels. And this network was trained on\nthese three different patterns.",
    "start": "3633540",
    "end": "3642740"
  },
  {
    "text": "And what this\nlittle demo shows is that if you start the network\nfrom different configurations",
    "start": "3642740",
    "end": "3648740"
  },
  {
    "text": "here and then evolve the\nnetwork-- you start running it. That means you run the\ndynamic update for each neuron",
    "start": "3648740",
    "end": "3655760"
  },
  {
    "text": "one at a time, and\nyou can see how this system evolves over time. ",
    "start": "3655760",
    "end": "3663500"
  },
  {
    "text": "So this is a little\nGUI-based thing. You can flip the\nstate and then run it. And you can see that if\nyou change those, now it--",
    "start": "3663500",
    "end": "3673532"
  },
  {
    "start": "3673532",
    "end": "3679940"
  },
  {
    "text": "I think he was trying to\nmake it look like that. But when you run it, it actually\nevolved toward this one.",
    "start": "3679940",
    "end": "3687390"
  },
  {
    "start": "3687390",
    "end": "3693750"
  },
  {
    "text": "He's going to really\nmake it look like that. And you can see it\nevolves toward that one.",
    "start": "3693750",
    "end": "3700010"
  },
  {
    "text": "All right, any\nquestions about that? You can see it stored\nthree separate memories. You've given an\ninput, and the network",
    "start": "3700010",
    "end": "3707300"
  },
  {
    "text": "evolves toward whatever memory\nwas closest to the input. So that's called a content\n[INAUDIBLE] memory.",
    "start": "3707300",
    "end": "3714140"
  },
  {
    "text": "You can actually\nrecall a memory-- not by pointing to an address,\nlike you do in a computer,",
    "start": "3714140",
    "end": "3720049"
  },
  {
    "text": "but by putting in\nsomething that looks a little bit like the memory. And then the system\nevolves right to the memory",
    "start": "3720050",
    "end": "3729079"
  },
  {
    "text": "that was closest to the input. So it's also called an\nauto-associative memory.",
    "start": "3729080",
    "end": "3737010"
  },
  {
    "text": "It automatically associates\nwith the nearest-- with a pattern that's\nnearest to the input.",
    "start": "3737010",
    "end": "3744660"
  },
  {
    "text": "So here's another example. It's just kind of\nmore of the same. This is a network\nsimilar to this.",
    "start": "3744660",
    "end": "3752400"
  },
  {
    "text": "Instead of black and\nwhite, it's red and purple, but it's got a lot more pixels.",
    "start": "3752400",
    "end": "3757770"
  },
  {
    "text": "And you'll see the\nthree different images that are stored in there--",
    "start": "3757770",
    "end": "3764700"
  },
  {
    "text": "so a face, a world,\nand a penguin. So then what they're doing\nhere is they add noise.",
    "start": "3764700",
    "end": "3771090"
  },
  {
    "text": "And then you run the\nnetwork, and it [AUDIO OUT] one of the patterns\nthat you stored in it. ",
    "start": "3771090",
    "end": "3780960"
  },
  {
    "text": "So here's the penguin. Add noise. Add a little bit of noise.",
    "start": "3780960",
    "end": "3786140"
  },
  {
    "text": "Here, he's coloring it\nin, I guess, to make it. And then you run\nthe network, and it",
    "start": "3786140",
    "end": "3792260"
  },
  {
    "text": "remembers the [AUDIO OUT]. ",
    "start": "3792260",
    "end": "3798010"
  },
  {
    "text": "OK, so that's interesting. So he ran it. He or she ran the network.",
    "start": "3798010",
    "end": "3803540"
  },
  {
    "text": "And you see that it kind\nof recovered a face, but there's some penguin\nhead stuck on top.",
    "start": "3803540",
    "end": "3811160"
  },
  {
    "text": "So what goes wrong there? Something bad happened, right? The network was trained with a\nface, a globe, and a penguin.",
    "start": "3811160",
    "end": "3820490"
  },
  {
    "text": "And you run it most of\nthe time, and it works. And then, suddenly, you run\nit, and it recovers a face",
    "start": "3820490",
    "end": "3826280"
  },
  {
    "text": "with a penguin head\nsticking out of it. What happened? So we'll explain what happens.",
    "start": "3826280",
    "end": "3831640"
  },
  {
    "text": "What happened was\nthat that this network was trained in a\nway that has what's",
    "start": "3831640",
    "end": "3838660"
  },
  {
    "text": "called a spurious attractor. And that often happens\nwhen you train a network with too many memories,\nwhen you exceed",
    "start": "3838660",
    "end": "3845170"
  },
  {
    "text": "the capacity of the\nnetwork to store memories. So let me show you what actually\ngoes wrong mathematically",
    "start": "3845170",
    "end": "3851510"
  },
  {
    "text": "there. ",
    "start": "3851510",
    "end": "3857970"
  },
  {
    "text": "All right, so we're going\nto do the same analysis we did before. We're going to take a matrix.",
    "start": "3857970",
    "end": "3864030"
  },
  {
    "text": "We're going to build a network\nthat stores multiple memories.",
    "start": "3864030",
    "end": "3869130"
  },
  {
    "text": " This was the matrix\nto build one memory. Let's see what I did here.",
    "start": "3869130",
    "end": "3874970"
  },
  {
    "text": " So in order for--",
    "start": "3874970",
    "end": "3881619"
  },
  {
    "text": " Yeah. Sorry. This was the matrix\nfor multiple memories.",
    "start": "3881620",
    "end": "3888630"
  },
  {
    "text": "We're summing mu. I just didn't write\nthe mu equals 0 to p. So we're going to program\np different memories",
    "start": "3888630",
    "end": "3895470"
  },
  {
    "text": "by summing up this outer product\nfor all the different patterns that we're wanting\nto store, all right?",
    "start": "3895470",
    "end": "3902460"
  },
  {
    "text": "We're going to ask\nwhether one of those-- under what conditions is one\nof those patterns, the xi 0,",
    "start": "3902460",
    "end": "3911580"
  },
  {
    "text": "actually a stable\nstate of the network? So we're going to\nbuild a network",
    "start": "3911580",
    "end": "3917579"
  },
  {
    "text": "with multiple patterns\nstored, and we're just going to ask a simple question. Under what conditions is xi\n0 going to evolve to xi 0?",
    "start": "3917580",
    "end": "3928040"
  },
  {
    "text": "And if xi 0 evolves toward\nxi 0, or stays at xi 0, then it's a stable point.",
    "start": "3928040",
    "end": "3935060"
  },
  {
    "text": "All right, so let's do that. We're going to take\nthat weight matrix, and we're going to plug-in\nour multiple memory weight",
    "start": "3935060",
    "end": "3941240"
  },
  {
    "text": "matrix, all right? You can see that we\ncan pull out the xi",
    "start": "3941240",
    "end": "3948080"
  },
  {
    "text": "i out of this sum over j. And the next step is we're\ngoing to separate this",
    "start": "3948080",
    "end": "3956920"
  },
  {
    "text": "into a sum over mu equals\nzero and a separate sum for mu",
    "start": "3956920",
    "end": "3962500"
  },
  {
    "text": "not equal to 0, all right? So this is a sum\nover all the mu's,",
    "start": "3962500",
    "end": "3968140"
  },
  {
    "text": "but we're going to\npull out the mu zero term as a separate sum over j.",
    "start": "3968140",
    "end": "3973650"
  },
  {
    "text": "Is that clear? Anyway, this is just for fun. You don't have to reproduce\nthis, so don't worry.",
    "start": "3973650",
    "end": "3981310"
  },
  {
    "text": " So we're going to pull out\nthe mu equals zero term.",
    "start": "3981310",
    "end": "3987280"
  },
  {
    "text": "And what does that look like? It's xi i0, sum over\nj of xi j0, xi j0.",
    "start": "3987280",
    "end": "3995079"
  },
  {
    "text": "So what is that? That's just N, right,\nthe number of neurons.",
    "start": "3995080",
    "end": "4000259"
  },
  {
    "text": "We're summing over j equals\n1 to N, number of neurons. I should add those limits here.",
    "start": "4000260",
    "end": "4005990"
  },
  {
    "text": "So you can see that that's N.\nSo this is just sine of xi i0",
    "start": "4005990",
    "end": "4014220"
  },
  {
    "text": "plus a bunch of other stuff. So you can see right away that\nif all of this other stuff",
    "start": "4014220",
    "end": "4021950"
  },
  {
    "text": "is really small, then\nthis is a fixed point. Because if all this\nstuff is small,",
    "start": "4021950",
    "end": "4028370"
  },
  {
    "text": "the system will evolve toward\nthe sine of xi [INAUDIBLE],, which is just xi i0.",
    "start": "4028370",
    "end": "4035000"
  },
  {
    "text": "So let's take a look\nat all of this stuff and see what can go wrong\nto make this not small.",
    "start": "4035000",
    "end": "4042970"
  },
  {
    "text": "All right, so let's zoom in\non this particular term right here. So what is this? This is sum over\nj, xi mu j, xi 0 j.",
    "start": "4042970",
    "end": "4053150"
  },
  {
    "text": "So what is that? Anybody know what that is? ",
    "start": "4053150",
    "end": "4059030"
  },
  {
    "text": "It's a vector operation. What is that? AUDIENCE: The dot\nproduct between one image",
    "start": "4059030",
    "end": "4064870"
  },
  {
    "text": "and then zero. MICHALE FEE: Exactly. It's a dot product between\nthe image that we're asking",
    "start": "4064870",
    "end": "4070850"
  },
  {
    "text": "is it a stable fixed point\nand all the other images in the network.",
    "start": "4070850",
    "end": "4076829"
  },
  {
    "text": "Sorry, and the mu-th image.  So what this is saying\nis that if our image is",
    "start": "4076830",
    "end": "4087130"
  },
  {
    "text": "orthogonal to all the\nother images in the network that we've tried to store,\nthen this thing is zero.",
    "start": "4087130",
    "end": "4094375"
  },
  {
    "start": "4094375",
    "end": "4101810"
  },
  {
    "text": "So this is referred\nto as crosstalk between the stored memories. So if our pattern, xi\n0, is orthogonal to all",
    "start": "4101810",
    "end": "4110930"
  },
  {
    "text": "the other patterns, then\nit will be a fixed point. So the capacity of the\nnetwork, the crosstalk--",
    "start": "4110930",
    "end": "4117500"
  },
  {
    "text": "the capacity of\nthe network depends on how much overlap there is\nbetween our stored pattern",
    "start": "4117500",
    "end": "4123470"
  },
  {
    "text": "and all the other patterns\nin the network, all right? ",
    "start": "4123470",
    "end": "4130756"
  },
  {
    "text": "So if all the memories\nare orthogonal, if all the patterns\nare orthogonal, then they're all\nstable attractors.",
    "start": "4130757",
    "end": "4137670"
  },
  {
    "text": "But if one of those memories,\nxi 1-- let's take xi 1-- is close to xi 0,\nthen xi 0 dot xi 1--",
    "start": "4137670",
    "end": "4147000"
  },
  {
    "text": "the two patterns\nare very similar-- then the dot product is\ngoing to be N, right?",
    "start": "4147000",
    "end": "4153219"
  },
  {
    "text": "And when you plug\nthat, if that's N, then you can see that this\nbecomes xi 1 i, right?",
    "start": "4153220",
    "end": "4162390"
  },
  {
    "text": "So what happens is that\nthese other memories that",
    "start": "4162390",
    "end": "4167850"
  },
  {
    "text": "are similar to our\nmemorized pattern-- ",
    "start": "4167850",
    "end": "4173009"
  },
  {
    "text": "then when you sum that,\nwhen you compute that sum, some of these terms\nget big enough so",
    "start": "4173010",
    "end": "4178649"
  },
  {
    "text": "that the memory in the next\nstep is not that stored memory.",
    "start": "4178649",
    "end": "4184560"
  },
  {
    "text": "It's a combination. All right? So what happens is-- so the\nway the capacity of the network",
    "start": "4184560",
    "end": "4192318"
  },
  {
    "text": "is stored. So you can't actually choose all\nyour memories to be orthogonal. But a pretty good way of making\nmemory is nearly orthogonal",
    "start": "4192319",
    "end": "4200720"
  },
  {
    "text": "is to store them as\nrandom [AUDIO OUT].. So a lot of the\nthinking that goes",
    "start": "4200720",
    "end": "4208580"
  },
  {
    "text": "into how you would\nbuild a network that stores a lot of patterns\nis to take your memories",
    "start": "4208580",
    "end": "4214730"
  },
  {
    "text": "and sort of convert them in a\nway that makes them maximally orthogonal to each other.",
    "start": "4214730",
    "end": "4220190"
  },
  {
    "text": "You can use things\nlike lateral inhibition to orthogonalize\ndifferent inputs.",
    "start": "4220190",
    "end": "4226800"
  },
  {
    "text": "So once you make your\npatterns sort of noisy, then it turns out\nyou can actually",
    "start": "4226800",
    "end": "4232710"
  },
  {
    "text": "calculate that if\nthe values of xi sort of look like\nrandom numbers,",
    "start": "4232710",
    "end": "4238199"
  },
  {
    "text": "that you can store\nup to about 15% of the number of neurons worth\nof memories in your network.",
    "start": "4238200",
    "end": "4244929"
  },
  {
    "text": "So if I have 100\nneurons in my network, I should be able to store\nabout 15 different states",
    "start": "4244930",
    "end": "4253240"
  },
  {
    "text": "in that network\nbefore they start to interfere with each other,\nbefore you have a sufficiently",
    "start": "4253240",
    "end": "4259210"
  },
  {
    "text": "high probability that\ntwo of those memories are next to each other.",
    "start": "4259210",
    "end": "4264409"
  },
  {
    "text": "And as soon as that\nhappens, then you start getting crosstalk\nbetween those memories that",
    "start": "4264410",
    "end": "4270430"
  },
  {
    "text": "causes the state of\nthe system to evolve in a way that doesn't recall\none of your stored memories,",
    "start": "4270430",
    "end": "4279579"
  },
  {
    "text": "all right? And what that looks like\nin the energy landscape is when you build a network\nwith, let's say, five memories,",
    "start": "4279580",
    "end": "4291260"
  },
  {
    "text": "there will be five minima in\nthe network that sort of have equal low values of energy.",
    "start": "4291260",
    "end": "4301220"
  },
  {
    "text": "But when you start sticking too\nmany memories in your network, you end up with what are called\nspurious attractors, sort",
    "start": "4301220",
    "end": "4307150"
  },
  {
    "text": "of local minima\nthat aren't at the--",
    "start": "4307150",
    "end": "4314241"
  },
  {
    "text": "that don't correspond to\none of the stored memories. And so as the system evolves,\nit can be going downhill",
    "start": "4314242",
    "end": "4321280"
  },
  {
    "text": "and get stuck in one\nof those minima that look like a combination of\ntwo of the stored memories.",
    "start": "4321280",
    "end": "4329020"
  },
  {
    "text": "And that's what went wrong here\nwith the guy with the penguin sticking out of his head. ",
    "start": "4329020",
    "end": "4338360"
  },
  {
    "text": "Who knows? Maybe that's what happens\nwhen you look at something and you're confused\nabout what youre seeing.",
    "start": "4338360",
    "end": "4344273"
  },
  {
    "text": "We don't know if that's\nactually what happens, but it would be an\ninteresting thing to test. ",
    "start": "4344273",
    "end": "4351770"
  },
  {
    "text": "Any questions? All right, so that's-- so you can see that these\nare long-term memories.",
    "start": "4351770",
    "end": "4358820"
  },
  {
    "text": "These don't depend on activity\nin the network to store, right? Those are programmed into\nthe synaptic connections",
    "start": "4358820",
    "end": "4365770"
  },
  {
    "text": "between the neurons. So you can shut off\nall the activity. And if you just put in up\na pattern of input that",
    "start": "4365770",
    "end": "4374740"
  },
  {
    "text": "reminds you of\nsomething, the network will recover the\nfull memory for you.",
    "start": "4374740",
    "end": "4380310"
  },
  {
    "start": "4380310",
    "end": "4386000"
  }
]