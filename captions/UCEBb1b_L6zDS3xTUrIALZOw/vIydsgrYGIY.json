[
  {
    "text": "The following content it's\nprovided by MIT OpenCourseWare, under a Creative\nCommons license.",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "Additional information about our\nlicense and MIT OpenCourseWare, in general, is available\nat ocw.mit.edu.",
    "start": "6090",
    "end": "11930"
  },
  {
    "text": " PROFESSOR: Several\npieces of good news.",
    "start": "11930",
    "end": "17430"
  },
  {
    "text": "Since Brett is back now,\nall of chapters five and six",
    "start": "17430",
    "end": "23890"
  },
  {
    "text": "of my notes, that is to say,\nall that we've covered in class,",
    "start": "23890",
    "end": "29930"
  },
  {
    "text": "plus a little more, are\non the web page now. So, I was cautious at the\nbeginning of the semester",
    "start": "29930",
    "end": "39039"
  },
  {
    "text": "and didn't put that up, and then\nforgot that it wasn't there. So now you have\nsomething to work with,",
    "start": "39040",
    "end": "45360"
  },
  {
    "text": "particularly on any\nexperiments, for example,",
    "start": "45360",
    "end": "52330"
  },
  {
    "text": "minimum degree ordering. Oh and also,\nthere's a movie now.",
    "start": "52330",
    "end": "60040"
  },
  {
    "text": "Tim Davis from Florida and\n[? Perils ?] [? Person ?], my friend here, created a little\nmovie to show the order that",
    "start": "60040",
    "end": "70950"
  },
  {
    "text": "elimination occurs. And I had a lot of\nemail with Tim Davis.",
    "start": "70950",
    "end": "76760"
  },
  {
    "text": " On that one-page handout,\nfor 2D Laplace's equation,",
    "start": "76760",
    "end": "86200"
  },
  {
    "text": "it mentions N cubed for the\nnested dissection, where",
    "start": "86200",
    "end": "92829"
  },
  {
    "text": "you cut the region in half, and\nhalf, and half, by separators.",
    "start": "92830",
    "end": "98020"
  },
  {
    "text": "I believe, and I think\nexperiments will show, that minimum degree\nis even better.",
    "start": "98020",
    "end": "105310"
  },
  {
    "text": "But the analysis is\nextremely weak, apparently, on minimum degree.",
    "start": "105310",
    "end": "110560"
  },
  {
    "text": "So anyway, one possible\nproject either for now",
    "start": "110560",
    "end": "117390"
  },
  {
    "text": "or for the final project\nwould be some experiments",
    "start": "117390",
    "end": "124570"
  },
  {
    "text": "to see what the actual\nconvergence is like.",
    "start": "124570",
    "end": "131310"
  },
  {
    "text": "And maybe a little\nunderstanding of minimum degree.",
    "start": "131310",
    "end": "136660"
  },
  {
    "text": "So we'll talk more about that. Anyway, there's a movie\nand a lot more material",
    "start": "136660",
    "end": "143590"
  },
  {
    "text": "there on the website.  It'll get updated and\ncorrected as I do the edits.",
    "start": "143590",
    "end": "154600"
  },
  {
    "text": "Second bit of good news. I made a mistake.",
    "start": "154600",
    "end": "160340"
  },
  {
    "text": "That's not normally good\nnews, but this time, I reported a little\ncalculation for multigrid,",
    "start": "160340",
    "end": "170150"
  },
  {
    "text": "and I computed the wrong thing. You remember that I computed\nM. M was the Jacobi matrix.",
    "start": "170150",
    "end": "180680"
  },
  {
    "text": "So M was the Jacobi matrix, the\niteration matrix with Jacobi,",
    "start": "180680",
    "end": "186799"
  },
  {
    "text": "which is I minus D\ninverse A This is for the 1D second differences.",
    "start": "186800",
    "end": "197300"
  },
  {
    "text": "In fact, this was\nthe 5 by 5 matrix",
    "start": "197300",
    "end": "204140"
  },
  {
    "text": "that we've been talking about. And now this is multiplied\nby D inverse which is a half,",
    "start": "204140",
    "end": "212000"
  },
  {
    "text": "but then with a weighting\nfactor, that becomes a third.",
    "start": "212000",
    "end": "217970"
  },
  {
    "text": "So that's weighted Jacobi now. And this is what we would get\nfor ordinary, simple iteration.",
    "start": "217970",
    "end": "225690"
  },
  {
    "text": "And it's not satisfactory. Remember it had an eigenvalue\nlambda_max, the spectra radius,",
    "start": "225690",
    "end": "232630"
  },
  {
    "text": "was about 0.9. Then I reported about\nthe eigenvalues of M*S,",
    "start": "232630",
    "end": "245189"
  },
  {
    "text": "the multigrid matrix S, M, and\nI was a little disappointed",
    "start": "245190",
    "end": "252680"
  },
  {
    "text": "in its eigenvalues. The reason is, I should\nhave been doing I",
    "start": "252680",
    "end": "258060"
  },
  {
    "text": "minus S M. When I do that,\nI'm not disappointed anymore.",
    "start": "258060",
    "end": "264960"
  },
  {
    "text": " What are the\neigenvalues of this?",
    "start": "264960",
    "end": "270900"
  },
  {
    "text": "You remember, S is the\nmatrix that tells us",
    "start": "270900",
    "end": "277100"
  },
  {
    "text": "how much error we're capturing. S is the matrix that came\nfrom-- the error we captured",
    "start": "277100",
    "end": "284800"
  },
  {
    "text": "at the end of\nmultigrid, was S times the error that we entered with.",
    "start": "284800",
    "end": "292909"
  },
  {
    "text": "And so the error that's\nleft, the remaining error,",
    "start": "292910",
    "end": "300750"
  },
  {
    "text": "the new error, is the rest. It's the part we don't get,\nand that's when I forgot.",
    "start": "300750",
    "end": "307780"
  },
  {
    "text": "That's I minus S e. So that's why I should\nhave been using I minus S.",
    "start": "307780",
    "end": "314780"
  },
  {
    "text": "So then, the step one of\nmultigrid does a smoother. Steps two, three, four\nleave me I minus S.",
    "start": "314780",
    "end": "324480"
  },
  {
    "text": "And step five did\nanother smoother. And this is just with one Jacobi\nstep, which you would probably",
    "start": "324480",
    "end": "333660"
  },
  {
    "text": "do three. Anyway, the eigenvalues\nof this were, well --",
    "start": "333660",
    "end": "340630"
  },
  {
    "text": "this has two zero\neigenvalues, and three 1's.",
    "start": "340630",
    "end": "345760"
  },
  {
    "text": "So this matrix has\neigenvalues 0, 0, 1, 1, 1.",
    "start": "345760",
    "end": "351484"
  },
  {
    "text": " Because, why's that?",
    "start": "351484",
    "end": "358819"
  },
  {
    "text": "Well, we get two 0's-- you\nremember the eigenvalues had to be 0 or 1, because\nS squared equaled S,",
    "start": "358820",
    "end": "365250"
  },
  {
    "text": "and I minus S squared\nequals I minus S. So. We remember that.",
    "start": "365250",
    "end": "370469"
  },
  {
    "text": "Now, the question\nis, what are these? And the answer is 1/9\nis a triple eigenvalue.",
    "start": "370470",
    "end": "378600"
  },
  {
    "text": "I was astonished to discover\n1/9-- of course, 0, 0 survived.",
    "start": "378600",
    "end": "384970"
  },
  {
    "text": "The rank is 3. And this is now\nlooking much better.",
    "start": "384970",
    "end": "392220"
  },
  {
    "text": "What I reported last time\nwas some more like 7/8, or something, and I\nthought oh, multigrid,",
    "start": "392220",
    "end": "398900"
  },
  {
    "text": "it's not showing\nits true colors. But actually it does.",
    "start": "398900",
    "end": "404650"
  },
  {
    "text": "This is a kind eigenvalue\nwe expect, like 1/10, for a multigrid cycle.",
    "start": "404650",
    "end": "414240"
  },
  {
    "text": "So much better. If we just did three\nM's, for example, I would get down to 0.9\ncubed, which is more than 0.7,",
    "start": "414240",
    "end": "424210"
  },
  {
    "text": "where by doing multigrid\ninstead, we're down to 0.1.",
    "start": "424210",
    "end": "429520"
  },
  {
    "text": "OK. So that's the good news. And, of course, if you\nexperiment a little,",
    "start": "429520",
    "end": "436730"
  },
  {
    "text": "you'll find different\nnumbers for different sizes and really see what's happening.",
    "start": "436730",
    "end": "443810"
  },
  {
    "text": "And, in fact, you could do 2D. I mentioned all\nthis partly for now,",
    "start": "443810",
    "end": "449710"
  },
  {
    "text": "if you're wanting to do it\nnow, or partly for eventually,",
    "start": "449710",
    "end": "455340"
  },
  {
    "text": "later. Oh, and thinking about projects,\njust one word to repeat,",
    "start": "455340",
    "end": "460729"
  },
  {
    "text": "that the Monday\nafter spring break, I think that's April\nthe third, no class.",
    "start": "460730",
    "end": "468919"
  },
  {
    "text": "So it'll be Wednesday\nthat I'll see you again. But Mr. [? Cho ?]\nwill be available.",
    "start": "468920",
    "end": "476620"
  },
  {
    "text": "He has office hours,\nso that's April 3,",
    "start": "476620",
    "end": "482270"
  },
  {
    "text": "if you wanted to discuss\nissues with your project with Mr. [? Cho ?], that would\nbe at class time in his office",
    "start": "482270",
    "end": "491200"
  },
  {
    "text": "2-130, at one o'clock.",
    "start": "491200",
    "end": "498460"
  },
  {
    "text": "Otherwise, take an\nextra spring break, and I'll see you Wednesday.",
    "start": "498460",
    "end": "503620"
  },
  {
    "text": "OK. So that's various\nbits of good news. Oh. One other thing that I guess I\ndidn't finish in that lecture,",
    "start": "503620",
    "end": "512700"
  },
  {
    "text": "was to identify the\ntwo eigenvalues.",
    "start": "512700",
    "end": "520570"
  },
  {
    "text": " Well, eigenvectors.",
    "start": "520570",
    "end": "526130"
  },
  {
    "text": "Those will be in the notes too. I guess I found one\neigenvector, [1, 2, 2, 2, 1]. ",
    "start": "526130",
    "end": "532930"
  },
  {
    "text": "And glancing at the matrix,\nI didn't spot the other one, but it has one oscillation.",
    "start": "532930",
    "end": "538860"
  },
  {
    "text": "It's [1, 2, 0, -2, -1]. Anyway, this is the main thing.",
    "start": "538860",
    "end": "545110"
  },
  {
    "text": "A much better result.\nSo that's multigrid,",
    "start": "545110",
    "end": "552029"
  },
  {
    "text": "which we now can use\nas our iteration. ",
    "start": "552030",
    "end": "559260"
  },
  {
    "text": "Or we can use it as\na preconditioner. Many of the\nrecommended methods use",
    "start": "559260",
    "end": "569630"
  },
  {
    "text": "multigrid as a preconditioner\nbefore something even more powerful.",
    "start": "569630",
    "end": "576147"
  },
  {
    "text": "Well, I don't know about\neven more powerful. Multigrid people would\nsay not possible. But some situations, we may\nwant to go to a different method",
    "start": "576147",
    "end": "589410"
  },
  {
    "text": "but we want a preconditioner,\nand multigrid excellent.",
    "start": "589410",
    "end": "595329"
  },
  {
    "text": "By multigrid, I\ninclude the smoother. So it'd be that. That would be a\npossible preconditioner",
    "start": "595330",
    "end": "602070"
  },
  {
    "text": "for something else. OK. So where are we now? We're into the new section\nstarting this moment.",
    "start": "602070",
    "end": "610170"
  },
  {
    "text": "And what's the section about? It's about things associated\nwith this name, Krylov.",
    "start": "610170",
    "end": "621029"
  },
  {
    "text": "So, I'm going to use the letter\nK for the things that are associated-- and I'm always\nsolving A*u equals b.",
    "start": "621030",
    "end": "626740"
  },
  {
    "start": "626740",
    "end": "632620"
  },
  {
    "text": "There's a Krylov matrix\nthat's created exactly as you see here.",
    "start": "632620",
    "end": "638660"
  },
  {
    "text": "This is the j-th-- the\none with j columns. b, A*b, A squared b, up to\nA to the the j minus 1 b.",
    "start": "638660",
    "end": "648019"
  },
  {
    "text": "And the Krylov space is the\ncombinations of those vectors.",
    "start": "648020",
    "end": "655830"
  },
  {
    "text": "That's what this\nword, span, means. I could erase span, and\nsay, \"all combinations of\".",
    "start": "655830",
    "end": "662660"
  },
  {
    "text": "Maybe that even more familiar. So \"spanned by\"\nis the same thing",
    "start": "662660",
    "end": "668160"
  },
  {
    "text": "is saying \"all linear\ncombinations of\".",
    "start": "668160",
    "end": "676110"
  },
  {
    "text": "Of those j vectors. And, of course, that's\nthe same as saying",
    "start": "676110",
    "end": "681790"
  },
  {
    "text": "it's the column\nspace of the matrix, because the column space\nis all combinations. OK.",
    "start": "681790",
    "end": "687390"
  },
  {
    "text": "So, why am I interested?",
    "start": "687390",
    "end": "693170"
  },
  {
    "text": "Why was Krylov interested? Why is everybody interested\nin these vectors?",
    "start": "693170",
    "end": "698200"
  },
  {
    "text": "Because actually, that's what an\niteration like Jacobi produces.",
    "start": "698200",
    "end": "705470"
  },
  {
    "text": "If I use Jacobi's method-- or\nGauss-Seidel, any of those--",
    "start": "705470",
    "end": "713120"
  },
  {
    "text": "after one step, I've got b. After two steps, there's a\nmultiplication by A in there,",
    "start": "713120",
    "end": "721371"
  },
  {
    "text": "right? And some combination\nis taken, depending on the particular method.",
    "start": "721371",
    "end": "726710"
  },
  {
    "text": "So after two steps, I've got\na combination of b and A*b. After three steps, Jacobi\nproduces some combination of b,",
    "start": "726710",
    "end": "735760"
  },
  {
    "text": "A*b, A squared b. In other words, all of\nthose iterative methods are picking their\nj-th approximation",
    "start": "735760",
    "end": "746420"
  },
  {
    "text": "in this space K_j, actually. I should put a little j on it,\nto indicate that that's-- so we",
    "start": "746420",
    "end": "754190"
  },
  {
    "text": "have these spaces are growing. They grow by one\ndimension, by one new basis",
    "start": "754190",
    "end": "761010"
  },
  {
    "text": "vector at each iteration. And the point is, Jacobi\nmakes a particular choice",
    "start": "761010",
    "end": "770970"
  },
  {
    "text": "of x_j or u_j, the\napproximation after j steps.",
    "start": "770970",
    "end": "780860"
  },
  {
    "text": "But does it make\nthe best choice? Probably not. In fact, pretty definitely not.",
    "start": "780860",
    "end": "788340"
  },
  {
    "text": "And so the idea is let's\nmake the best choice.",
    "start": "788340",
    "end": "795180"
  },
  {
    "text": "Let's not just use\na simple iteration that builds in a choice that\nmight not be so terrific.",
    "start": "795180",
    "end": "804790"
  },
  {
    "text": "Let's make the best choice. There are a whole lot of Krylov\nmethods, which all choose x_j--",
    "start": "804790",
    "end": "816160"
  },
  {
    "text": "since I think this\nsection will use x, I'm going to change that to x. ",
    "start": "816160",
    "end": "823569"
  },
  {
    "text": "They'll all be iterative. They'll all start\nwith x_0 as 0, say.",
    "start": "823569",
    "end": "828810"
  },
  {
    "text": "And then the first\nguess will be b, maybe. Or maybe not. Maybe some multiple of b.",
    "start": "828810",
    "end": "836050"
  },
  {
    "text": "Actually, a good\nKrylov method will take the best multiple\nof b as the first guess,",
    "start": "836050",
    "end": "842380"
  },
  {
    "text": "and not necessarily 1 times b. And onwards.",
    "start": "842380",
    "end": "848430"
  },
  {
    "text": "So we have this word,\nbest, coming up.",
    "start": "848430",
    "end": "854010"
  },
  {
    "text": "What's the best\nvector in that space? And there are different\nmethods depending",
    "start": "854010",
    "end": "860690"
  },
  {
    "text": "on what I mean by best. ",
    "start": "860690",
    "end": "865790"
  },
  {
    "text": "Oh, let me tell you\nthe name of the method that I'm going to concentrate\non first and most.",
    "start": "865790",
    "end": "873180"
  },
  {
    "text": "Will be the conjugate\ngradient method. CG.",
    "start": "873180",
    "end": "878279"
  },
  {
    "text": "The conjugate gradient method. ",
    "start": "878280",
    "end": "893500"
  },
  {
    "text": "What determines x_j? It chooses x_j in K_j,\nthe space that we always",
    "start": "893500",
    "end": "908320"
  },
  {
    "text": "look for our approximation in. Let me not forget to say: these\nvectors, b, A*b, A squared b,",
    "start": "908320",
    "end": "921940"
  },
  {
    "text": "and so on, they're\neasy to compute, because each one is just a\nmatrix multiplication from",
    "start": "921940",
    "end": "930920"
  },
  {
    "text": "the previous one. And the matrix is--\nwe're assuming we're working with sparse matrices.",
    "start": "930920",
    "end": "938810"
  },
  {
    "text": "And mostly, and\nespecially, sometimes, especially symmetric ones.",
    "start": "938810",
    "end": "944590"
  },
  {
    "text": "So just let me put in\nhere, before I even finish that sentence,\nthat CG, this",
    "start": "944590",
    "end": "950790"
  },
  {
    "text": "is for A transpose\nequal A symmetric. It's only for those.",
    "start": "950790",
    "end": "956990"
  },
  {
    "text": "And positive definite so\nsymmetric, positive definite.",
    "start": "956990",
    "end": "962149"
  },
  {
    "start": "962150",
    "end": "967390"
  },
  {
    "text": "So it's a limited class of\nproblems, but a highly, highly important class.",
    "start": "967390",
    "end": "973399"
  },
  {
    "text": "And you may say what do\nwe do if the matrix is symmetric, but indefinite?",
    "start": "973400",
    "end": "980740"
  },
  {
    "text": "Well, that comes next. Or what if the matrix is\nnot to symmetric at all.",
    "start": "980740",
    "end": "987500"
  },
  {
    "text": "Well, if you're brave, you might\ntry conjugate gradients anyway. But if you're cautious,\nthen you would",
    "start": "987500",
    "end": "994800"
  },
  {
    "text": "use one of the other\nmethods like MINRES. Maybe I'll just mention one\nmore on our eventual list.",
    "start": "994800",
    "end": "1005580"
  },
  {
    "text": "Actually that tells us\nprobably what choice it makes.",
    "start": "1005580",
    "end": "1010970"
  },
  {
    "text": "MINRES chooses the x_j\nto minimize the residual.",
    "start": "1010970",
    "end": "1019930"
  },
  {
    "text": "Minimum r. Remember r is b minus A*x.",
    "start": "1019930",
    "end": "1028490"
  },
  {
    "text": "The residual r\nwill always denote the error in the equation.",
    "start": "1028490",
    "end": "1033880"
  },
  {
    "text": "And so it's minimum residual.",
    "start": "1033880",
    "end": "1039860"
  },
  {
    "text": "OK. So that's a natural\nchoice, but you'll see that this is a\nfantastic method.",
    "start": "1039860",
    "end": "1051190"
  },
  {
    "text": "Superior, just quicker than\nMINRES, for a nice reason.",
    "start": "1051190",
    "end": "1059429"
  },
  {
    "text": "So it chooses x_j in K_j,\nand I think the rule it uses",
    "start": "1059430",
    "end": "1065680"
  },
  {
    "text": "is that I think x_j should be\northogonal-- I'll go and check",
    "start": "1065680",
    "end": "1080090"
  },
  {
    "text": "it in my notes-- to\nthe residual r_j. Let me just be sure-- if I'm\ngoing to write that down,",
    "start": "1080090",
    "end": "1088409"
  },
  {
    "text": "I better be sure I\nsaid it correctly. ",
    "start": "1088410",
    "end": "1093910"
  },
  {
    "text": "I actually, I didn't\nsay it correctly. r_j is orthogonal to\nthe whole space K_j.",
    "start": "1093910",
    "end": "1110800"
  },
  {
    "text": "It turns out that\nwe can choose x_j--",
    "start": "1110800",
    "end": "1116520"
  },
  {
    "text": "when I make a choice\nof x_j, it's in K_j. Now when I compute r,\nthere's a multiplication by A",
    "start": "1116520",
    "end": "1123790"
  },
  {
    "text": "to get the residual, so that\nmoves us up to the next space, and I'm going to\nmake a choice where",
    "start": "1123790",
    "end": "1130230"
  },
  {
    "text": "this is orthogonal to\nthe whole space K_j. You have to do\nconjugate gradient.",
    "start": "1130230",
    "end": "1137539"
  },
  {
    "text": "And I can't start on\nthat for a good reason. I have to start with\nsomething called Arnoldi.",
    "start": "1137540",
    "end": "1144490"
  },
  {
    "text": "And and what's Arnoldi about?",
    "start": "1144490",
    "end": "1151230"
  },
  {
    "text": "Well. Let me come back\nto these vectors. ",
    "start": "1151230",
    "end": "1160380"
  },
  {
    "text": "Quick to compute. But what's the other property?",
    "start": "1160380",
    "end": "1165970"
  },
  {
    "text": "Everything in applied\nnumerical mathematics, you're choosing basis\nvectors, and you're",
    "start": "1165970",
    "end": "1172140"
  },
  {
    "text": "looking for two properties. And I guess this is like\na general rule, whenever",
    "start": "1172140",
    "end": "1177690"
  },
  {
    "text": "you meet a whole new\nproblem, in the end, you're going to construct\nsome basis vectors",
    "start": "1177690",
    "end": "1184190"
  },
  {
    "text": "if you're going to compute. And what properties\nare you after?",
    "start": "1184190",
    "end": "1189350"
  },
  {
    "text": "You're after speed. So they have to be quick\nto compute and work with.",
    "start": "1189350",
    "end": "1195230"
  },
  {
    "text": "Well these are. Multiplying by A is sparse\nmatrix multiplication,",
    "start": "1195230",
    "end": "1200400"
  },
  {
    "text": "you can't beat that. But the other\nproperty that you need is some decent independence.",
    "start": "1200400",
    "end": "1209800"
  },
  {
    "text": "And not just barely independent. You want the condition\nnumber of the basis,",
    "start": "1209800",
    "end": "1215840"
  },
  {
    "text": "somehow-- if I use\nthat word-- to be good.",
    "start": "1215840",
    "end": "1221650"
  },
  {
    "text": "Not enormous. And best of all, you would\nlike the basis vectors",
    "start": "1221650",
    "end": "1228059"
  },
  {
    "text": "to be orthonormal. That's a condition\nnumber of one.",
    "start": "1228060",
    "end": "1234279"
  },
  {
    "text": "That's the best basis\nyou can hope for. And the point is,\nthat this construction",
    "start": "1234280",
    "end": "1241980"
  },
  {
    "text": "produces a lousy basis from\nthe point of view of condition. So Arnoldi's job is\northogonalize the Krylov basis,",
    "start": "1241980",
    "end": "1257539"
  },
  {
    "text": "which is b, A*b, and so on,\nproducing orthonormal basis",
    "start": "1257540",
    "end": "1274230"
  },
  {
    "text": "q_1, q_2, up to q_j. ",
    "start": "1274230",
    "end": "1285160"
  },
  {
    "text": "It's just beautiful. So Arnoldi's taking\nlike a preliminary step",
    "start": "1285160",
    "end": "1291000"
  },
  {
    "text": "that Krylov has to make to\nget something that numerically",
    "start": "1291000",
    "end": "1298400"
  },
  {
    "text": "reasonable to work with. Then if it's fast, and Arnoldi\ndoesn't-- we have to do",
    "start": "1298400",
    "end": "1308300"
  },
  {
    "text": "a multiplication by A,\nand you'll see one here in the middle of Arnoldi--\nand then, of course,",
    "start": "1308300",
    "end": "1315670"
  },
  {
    "text": "if you look at those\nten lines of MATLAB,",
    "start": "1315670",
    "end": "1323150"
  },
  {
    "text": "and we could go through\nthem a little carefully, but let's just\ntake a first look.",
    "start": "1323150",
    "end": "1329250"
  },
  {
    "text": "The first step is like\nGram-Schmidt, right?",
    "start": "1329250",
    "end": "1334380"
  },
  {
    "text": "It's sort of a\nGram-Schmidt idea. You take that first vector\nb, you accept that direction,",
    "start": "1334380",
    "end": "1341990"
  },
  {
    "text": "and the only step remaining\nis to normalize it. Divide by its length.",
    "start": "1341990",
    "end": "1347740"
  },
  {
    "text": "Then you go on to the next. Then you have some\ntrial vector t,",
    "start": "1347740",
    "end": "1357000"
  },
  {
    "text": "which would in this case be\nA*b, in the direction of A*b,",
    "start": "1357000",
    "end": "1362580"
  },
  {
    "text": "which won't be orthogonal\nto the original b, right? So this won't be\northogonal to that one.",
    "start": "1362580",
    "end": "1370350"
  },
  {
    "text": " So, of course,\nyou've got to compute",
    "start": "1370350",
    "end": "1377190"
  },
  {
    "text": "an inner product between them. And subtract off\nthe right multiple",
    "start": "1377190",
    "end": "1385680"
  },
  {
    "text": "of the previous one\nfrom the current t to get the improved t.",
    "start": "1385680",
    "end": "1391430"
  },
  {
    "text": "And then you're going\nto normalize it again. ",
    "start": "1391430",
    "end": "1398510"
  },
  {
    "text": "So you compute its length\nand divide by the length. You see that overall\npattern in Arnoldi?",
    "start": "1398510",
    "end": "1405650"
  },
  {
    "text": "It's the familiar idea\nof Gram-Schmidt of,",
    "start": "1405650",
    "end": "1411180"
  },
  {
    "text": "take a new vector,\nfind its projections",
    "start": "1411180",
    "end": "1417050"
  },
  {
    "text": "onto the ones that\nare already set, subtract those components\noff, you're left with a piece,",
    "start": "1417050",
    "end": "1426700"
  },
  {
    "text": "and you find its\nlength and normalize that to be a unit factor. It's very familiar.",
    "start": "1426700",
    "end": "1432059"
  },
  {
    "text": "Its exactly the\ntype of algorithm that you would write\ndown immediately.",
    "start": "1432060",
    "end": "1439910"
  },
  {
    "text": "And it just involved the\none multiplication by A so that it's not\ngoing to cost us",
    "start": "1439910",
    "end": "1450279"
  },
  {
    "text": "a lot to make the\nvectors orthonormal.",
    "start": "1450280",
    "end": "1455610"
  },
  {
    "text": "Well. Wait a minute. ",
    "start": "1455610",
    "end": "1460630"
  },
  {
    "text": "Is it going to be\nexpensive or not? That's like the key question.",
    "start": "1460630",
    "end": "1466220"
  },
  {
    "text": "It's certainly going to\nproduce a good result. Producing orthonormal vectors is\ngoing to be a good thing to do.",
    "start": "1466220",
    "end": "1473940"
  },
  {
    "text": "But is it expensive or not? That depends. ",
    "start": "1473940",
    "end": "1480519"
  },
  {
    "text": "It's certainly not expensive\nif the new trial vector",
    "start": "1480520",
    "end": "1489780"
  },
  {
    "text": "t has only components\nin one or two",
    "start": "1489780",
    "end": "1495910"
  },
  {
    "text": "of the already\nsettled directions. In other words, if I\nonly have to subtract off",
    "start": "1495910",
    "end": "1504410"
  },
  {
    "text": "a couple of earlier\ncomponents, then I'm golden.",
    "start": "1504410",
    "end": "1512400"
  },
  {
    "text": "And that's the case\nwhen A is symmetric. So that will be\nthe key point here.",
    "start": "1512400",
    "end": "1518210"
  },
  {
    "text": "And I just make it here. If A is symmetric, A transpose\nequals A, then I only need,",
    "start": "1518210",
    "end": "1532309"
  },
  {
    "text": "in this subtracting off, where\nI'm headed for the new q,",
    "start": "1532310",
    "end": "1539330"
  },
  {
    "text": "I only need to subtract off\nh_(j, j), multiplying the q_j,",
    "start": "1539330",
    "end": "1547620"
  },
  {
    "text": "the q that was just\nset, and the one before. ",
    "start": "1547620",
    "end": "1553230"
  },
  {
    "text": "h_(j-1, j).  I'll call that a\nshort recurrence.",
    "start": "1553230",
    "end": "1559130"
  },
  {
    "text": "It's short because it\nonly has two terms. And then there'll\nbe a new h_(j+1, j),",
    "start": "1559130",
    "end": "1568730"
  },
  {
    "text": "which is just the length. So there'll be one of these\nmagic things in so many parts",
    "start": "1568730",
    "end": "1577140"
  },
  {
    "text": "of mathematical analysis, a\nthree-term recurrence relation",
    "start": "1577140",
    "end": "1582390"
  },
  {
    "text": "will hold-- in fact,\nI guess the reason we see three-term recurrence\nrelations in all-- Legendre",
    "start": "1582390",
    "end": "1591139"
  },
  {
    "text": "polynomials,\nChebyshev polynomials, all those classical\nthings-- the reason",
    "start": "1591140",
    "end": "1597410"
  },
  {
    "text": "is actually the\nsame as it is here. That something in the\nbackground is symmetric.",
    "start": "1597410",
    "end": "1605150"
  },
  {
    "start": "1605150",
    "end": "1610760"
  },
  {
    "text": "I want to put a few\nnumbers on the board so you see what is typical\ninput and output to Arnoldi.",
    "start": "1610760",
    "end": "1618390"
  },
  {
    "text": "And you'll see it's\nsymmetric and then you'll see it's short recurrence,\nand then we want to see what?",
    "start": "1618390",
    "end": "1626080"
  },
  {
    "text": "OK. So I worked out a\ntypical Arnoldi example.",
    "start": "1626080",
    "end": "1634090"
  },
  {
    "text": "Not that one. Here. OK. ",
    "start": "1634090",
    "end": "1642330"
  },
  {
    "text": "I think this is a good\nexample to look at. So matrix A is not only\nsymmetric, it's diagonal.",
    "start": "1642330",
    "end": "1649120"
  },
  {
    "text": "So of course, to find A\ninverse b is not difficult.",
    "start": "1649120",
    "end": "1655760"
  },
  {
    "text": "But we're going to do it\nthrough conjugate gradients. So that means that we\ndon't figure out A inverse,",
    "start": "1655760",
    "end": "1663419"
  },
  {
    "text": "which, of course,\nwe easily could. We just use A, very sparse.",
    "start": "1663420",
    "end": "1669350"
  },
  {
    "text": "Here's our right-hand side. Here's our Krylov matrix.",
    "start": "1669350",
    "end": "1675440"
  },
  {
    "text": "It has b in its first column. It has A times b in\nthe second column.",
    "start": "1675440",
    "end": "1680770"
  },
  {
    "text": "Multiply by A again to\nget the third column. And A one more time to\nget the fourth column. So that's K_4, right?",
    "start": "1680770",
    "end": "1689500"
  },
  {
    "text": "And for a 4 by 4\nproblem, that's the end.",
    "start": "1689500",
    "end": "1696770"
  },
  {
    "text": "So actually here I'm carrying\nKrylov to the end of it.",
    "start": "1696770",
    "end": "1706100"
  },
  {
    "text": "There's no more to do. Because once I've got this\nK_j, the combinations of those,",
    "start": "1706100",
    "end": "1714350"
  },
  {
    "text": "are all of R^4, all of\nfour-dimensional space.",
    "start": "1714350",
    "end": "1719780"
  },
  {
    "text": "Oh, yeah. That raises an important\npoint about how we're improving on iterations.",
    "start": "1719780",
    "end": "1726049"
  },
  {
    "text": "If I was using Jacobi,\nor Gauss-Seidel, or one of those others,\nthen-- well, I won't",
    "start": "1726050",
    "end": "1732910"
  },
  {
    "text": "say for this particular A, but\nusually-- after four steps,",
    "start": "1732910",
    "end": "1740310"
  },
  {
    "text": "I am by no means finished. I've taken four steps,\nI've gotten closer,",
    "start": "1740310",
    "end": "1745899"
  },
  {
    "text": "but I haven't got\nthe exact answer. But now, in this the\nworld of Krylov methods,",
    "start": "1745900",
    "end": "1752559"
  },
  {
    "text": "after four steps, I will\nhave the exact answer. Why? Because Krylov methods take\nthe best x_4 out of the space",
    "start": "1752560",
    "end": "1766710"
  },
  {
    "text": "spanned by those four columns. Well, the whole space, R^4\nis spanned by those columns",
    "start": "1766710",
    "end": "1772100"
  },
  {
    "text": "and taking the best\none must be the answer. So x_4 will actually\nbe A inverse",
    "start": "1772100",
    "end": "1780200"
  },
  {
    "text": "b, the answer we're looking for. So these methods are on the\none hand, direct methods.",
    "start": "1780200",
    "end": "1791950"
  },
  {
    "text": "They finish after\nfour steps, finish. You've got the answer.",
    "start": "1791950",
    "end": "1797260"
  },
  {
    "text": "Now. That's actually\nhow they were born. As direct methods\nthat gave the answer",
    "start": "1797260",
    "end": "1804250"
  },
  {
    "text": "from an interesting\niteration and they nearly",
    "start": "1804250",
    "end": "1811060"
  },
  {
    "text": "died for that reason. They were born and died,\nor were on death's door,",
    "start": "1811060",
    "end": "1817590"
  },
  {
    "text": "as direct methods. Because as direct methods,\nthere are better ones.",
    "start": "1817590",
    "end": "1824570"
  },
  {
    "text": "Then some years later,\npeople went back to it, back",
    "start": "1824570",
    "end": "1832669"
  },
  {
    "text": "to conjugate\ngradients, and noticed that thought of as\njust going partway,",
    "start": "1832670",
    "end": "1842580"
  },
  {
    "text": "gave very successful answers. So we're going to think\nof Krylov-- we're planning",
    "start": "1842580",
    "end": "1850260"
  },
  {
    "text": "to stop before j equals n. In this picture, j\nequals n equal 4 here.",
    "start": "1850260",
    "end": "1857170"
  },
  {
    "text": " But, we plan to stop\nfor j much below n.",
    "start": "1857170",
    "end": "1867679"
  },
  {
    "text": "We're thinking of applications\nwhere n is 10 the fifth, and we're going to stop at\n10 squared, 100 steps, maybe.",
    "start": "1867680",
    "end": "1875910"
  },
  {
    "text": "Or ten steps. So that meant a total\nreconsideration,",
    "start": "1875910",
    "end": "1883790"
  },
  {
    "text": "reanalysis of\nconjugate gradients, and it is now back\nto a big success.",
    "start": "1883790",
    "end": "1890100"
  },
  {
    "text": "Well it's a rather unusual\nthing that a method that people",
    "start": "1890100",
    "end": "1897419"
  },
  {
    "text": "have put aside gets picked up\nagain and becomes a favorite, as conjugate gradients have.",
    "start": "1897420",
    "end": "1903400"
  },
  {
    "start": "1903400",
    "end": "1909970"
  },
  {
    "text": "First of all, I was saying that\nthe basis, those basis vectors",
    "start": "1909970",
    "end": "1916150"
  },
  {
    "text": "are not very independent. ",
    "start": "1916150",
    "end": "1923740"
  },
  {
    "text": "That's not a good basis. Of course, it's a\ndetractive basis and maybe, do you know the\nname for a matrix",
    "start": "1923740",
    "end": "1929920"
  },
  {
    "text": "that has this particular form? The columns are constant, then,\nthat's the key column, there,",
    "start": "1929920",
    "end": "1939590"
  },
  {
    "text": "and then it's first powers,\nsecond powers, third powers. Do you remember whose name is\nassociated with that matrix?",
    "start": "1939590",
    "end": "1947990"
  },
  {
    "text": "It comes up in interpolation,\nand it starts with a V? ",
    "start": "1947990",
    "end": "1955900"
  },
  {
    "text": "Anybody know? Vandermonde, yeah. Vandermonde. So I could call it\nV for Vandermonde.",
    "start": "1955900",
    "end": "1964139"
  },
  {
    "text": "And Vandermonde matrices are\nnot very well conditioned. ",
    "start": "1964140",
    "end": "1974620"
  },
  {
    "text": "So now a little timeout to\nsay, because it's so important,",
    "start": "1974620",
    "end": "1980420"
  },
  {
    "text": "how do you judge the good or\npoor conditioning of a matrix?",
    "start": "1980420",
    "end": "1988160"
  },
  {
    "text": "Those vectors are\nlinearly independent. The determinant\nof V is not zero.",
    "start": "1988160",
    "end": "1994640"
  },
  {
    "text": " The matrix is not singular,\nbut it's too close to singular,",
    "start": "1994640",
    "end": "2002960"
  },
  {
    "text": "and how do you test\nthe-- suppose you",
    "start": "2002960",
    "end": "2008480"
  },
  {
    "text": "have a basis, as we have here.",
    "start": "2008480",
    "end": "2013700"
  },
  {
    "text": "And you want to know\nis it good or not? So, of course, you always\nput your bases vectors,",
    "start": "2013700",
    "end": "2021929"
  },
  {
    "text": "let me call them-- well,I don't\nwant to call them v because --",
    "start": "2021930",
    "end": "2027760"
  },
  {
    "text": "well I guess I\ncould call them v, they come out of Vandermonde.\nv_1, v_2, v_3, v_4. ",
    "start": "2027760",
    "end": "2035370"
  },
  {
    "text": "That's our matrix. Essentially I want\nits condition number. And to find its\ncondition number,",
    "start": "2035370",
    "end": "2043110"
  },
  {
    "text": "I-- it's not symmetric,\nso I can't just take the eigenvalues of that.",
    "start": "2043110",
    "end": "2048850"
  },
  {
    "text": "You might say, look at\nlambda_max and lambda_min. ",
    "start": "2048850",
    "end": "2054010"
  },
  {
    "text": "The condition\nnumber is associated with max divided by min.",
    "start": "2054010",
    "end": "2059610"
  },
  {
    "text": "But when the matrix\nisn't symmetric,",
    "start": "2059610",
    "end": "2065200"
  },
  {
    "text": "just taking its eigenvalues\ndirectly is not cool. It's not reliable.",
    "start": "2065200",
    "end": "2073899"
  },
  {
    "text": "I could have a matrix\nthat's badly conditioned, but all its eigenvalues were 1. ",
    "start": "2073900",
    "end": "2080869"
  },
  {
    "text": "I mean a matrix with\n1's on the diagonal and zillions up\nabove the diagonal would have eigenvalues of 1, but\nit would be badly conditioned.",
    "start": "2080870",
    "end": "2089379"
  },
  {
    "text": "So the right way to take\nit is V transpose V. Look at v_1 transpose,\nv_2 transpose...",
    "start": "2089380",
    "end": "2099470"
  },
  {
    "text": "As always, if a matrix\nisn't symmetric,",
    "start": "2099470",
    "end": "2106060"
  },
  {
    "text": "if the matrix V\nis not symmetric, good idea to form V transpose\nV. That is symmetric.",
    "start": "2106060",
    "end": "2114940"
  },
  {
    "text": "It does have\npositive eigenvalues. And those eigenvalues, the\neigenvalues of V transpose V,",
    "start": "2114940",
    "end": "2124720"
  },
  {
    "text": "are the singular values, or\nrather the singular values",
    "start": "2124720",
    "end": "2130760"
  },
  {
    "text": "squared, of V. So\nI guess I'm saying,",
    "start": "2130760",
    "end": "2137090"
  },
  {
    "text": "you can't trust the\neigenvalues of V. It's the singular\nvalues you can trust. And the way to find\nsingular values",
    "start": "2137090",
    "end": "2143870"
  },
  {
    "text": "is form V transpose\nV, that gives you a symmetric matrix,\nits eigenvalues--",
    "start": "2143870",
    "end": "2151280"
  },
  {
    "text": "so the i-th eigenvalue\nwould be the i-th singular value squared. ",
    "start": "2151280",
    "end": "2168260"
  },
  {
    "text": "So the condition number, is\nsigma_max over sigma_min.",
    "start": "2168260",
    "end": "2175480"
  },
  {
    "text": " And, well, it's not enormous\nfor this 4 by 4 matrix,",
    "start": "2175480",
    "end": "2184910"
  },
  {
    "text": "but if I go up to 10\nby 10 or 100 by 100. 100 by 100 would just\ntotally wipe me out.",
    "start": "2184910",
    "end": "2191550"
  },
  {
    "text": "10 by 10 would\nalready be disastrous. Completely disastrous actually.",
    "start": "2191550",
    "end": "2197080"
  },
  {
    "text": "10 by 10 Vandermonde matrix,\nwith 1, 2 3, 4, up to 10",
    "start": "2197080",
    "end": "2204740"
  },
  {
    "text": "as the points, would be--\nwell, it would have entries--",
    "start": "2204740",
    "end": "2211180"
  },
  {
    "text": "if that ended in a\n10 and I had 10 rows, would that be something\nlike 10 to the ninth power.",
    "start": "2211180",
    "end": "2217660"
  },
  {
    "text": "I mean the dynamics\nscale would be terrible. So finally, just\nto-- V transpose V",
    "start": "2217660",
    "end": "2226820"
  },
  {
    "text": "is called the Gram matrix. So that guy Gram is coming back\nin as giving the good measure.",
    "start": "2226820",
    "end": "2241790"
  },
  {
    "text": "So the point was then, that\nthis measures the dependence",
    "start": "2241790",
    "end": "2246870"
  },
  {
    "text": "or independence of the v's.",
    "start": "2246870",
    "end": "2253240"
  },
  {
    "text": "That ratio. The bigger that ratio is,\nthe more dependency they are.",
    "start": "2253240",
    "end": "2259690"
  },
  {
    "text": "What's the ratio if\nthey're orthonormal? What's the ratio of the q's?",
    "start": "2259690",
    "end": "2266700"
  },
  {
    "text": "What's the condition\nnumber of the q's? If we've run Arnoldi and\ngot a good basis, it's one.",
    "start": "2266700",
    "end": "2277990"
  },
  {
    "text": "What's the Gram matrix? If this is Q transpose Q, with\nthe q's, the orthonormal basis",
    "start": "2277990",
    "end": "2287000"
  },
  {
    "text": "in the columns, then Q\ntranspose Q is the identity.",
    "start": "2287000",
    "end": "2293330"
  },
  {
    "text": "So it's a Gram matrix. So Q transpose Q\nwould be the identity,",
    "start": "2293330",
    "end": "2300340"
  },
  {
    "text": "and its condition number\nis the best possible one.",
    "start": "2300340",
    "end": "2305880"
  },
  {
    "text": "Lambda_max is 1,\nlambda_min is 1. The condition number is one.",
    "start": "2305880",
    "end": "2310960"
  },
  {
    "text": "OK. So that's just some comments\nabout why Arnoldi gets brought",
    "start": "2310960",
    "end": "2318980"
  },
  {
    "text": "in to fix the situation. OK. So I'll just leave those ten\nArnoldi steps on the board.",
    "start": "2318980",
    "end": "2327720"
  },
  {
    "text": "The notes give ten comments. I'm quite proud of-- my\nMATLAB is unreliable.",
    "start": "2327720",
    "end": "2338300"
  },
  {
    "text": "Actually you'll\nfind a few errors like after end, I\naccidentally put",
    "start": "2338300",
    "end": "2344180"
  },
  {
    "text": "a semicolon, which was absurd. But the comments\nI'm pleased with,",
    "start": "2344180",
    "end": "2351650"
  },
  {
    "text": "and then the\nnumerical example that runs through one cycle of\nthis, with these numbers,",
    "start": "2351650",
    "end": "2363850"
  },
  {
    "text": "to see what q_2 is,\nis, I hope, useful. ",
    "start": "2363850",
    "end": "2373070"
  },
  {
    "text": "Why do we have a\nshort recurrence? This is a key point here.",
    "start": "2373070",
    "end": "2381240"
  },
  {
    "text": "And in this example, if I work\nout the h's, I'll discover sure enough, h_(1, 3) will be\n0. h_(1, 4) will be 0.",
    "start": "2381240",
    "end": "2393280"
  },
  {
    "text": " Here's the key equation.",
    "start": "2393280",
    "end": "2400410"
  },
  {
    "text": "Here's Arnoldi in\nmatrix language.",
    "start": "2400410",
    "end": "2410700"
  },
  {
    "text": "Let me see if I can remember\nArnoldi in matrix language. So, Arnoldi is taking\nthe matrix-- yeah.",
    "start": "2410700",
    "end": "2423920"
  },
  {
    "text": "So Arnoldi in matrix\nlanguage is going to be this. It's going to A*Q equals Q*H.\nI can't write out all of Q.",
    "start": "2423920",
    "end": "2448869"
  },
  {
    "text": "So that's the big equation.",
    "start": "2448870",
    "end": "2457420"
  },
  {
    "text": "Its a very important\nequation that we now have all the pieces for.",
    "start": "2457420",
    "end": "2465210"
  },
  {
    "text": "So A is our original\nmatrix that we were given. Symmetric let's say.",
    "start": "2465210",
    "end": "2471450"
  },
  {
    "text": "Q is our basis out of Arnoldi\nand H is the multipliers",
    "start": "2471450",
    "end": "2479930"
  },
  {
    "text": "that gave that basis. So this Q*H is a little\nbit like Gram-Schmidt.",
    "start": "2479930",
    "end": "2486660"
  },
  {
    "text": "Do you remember, Gram-Schmidt\nis described by Q times R. Q,",
    "start": "2486660",
    "end": "2493280"
  },
  {
    "text": "again, is orthonormal. So it's an orthogonal matrix.",
    "start": "2493280",
    "end": "2499530"
  },
  {
    "text": "In Gram-Schmidt R\nis upper triangular. Here it's not.",
    "start": "2499530",
    "end": "2505860"
  },
  {
    "text": "Here it's Hessenberg. So H stands for Hessenberg. I'll write down\nwhat the actual H",
    "start": "2505860",
    "end": "2512660"
  },
  {
    "text": "is for these for these numbers. I won't write down the\nQ. I'll just write down",
    "start": "2512660",
    "end": "2520700"
  },
  {
    "text": "what H turned out to\nbe for those numbers if I did it right. Five halves.",
    "start": "2520700",
    "end": "2527820"
  },
  {
    "text": "Oh, interesting. The lengths all turned\nout to be five halves. ",
    "start": "2527820",
    "end": "2535760"
  },
  {
    "text": "And this turned out\nto be root 5 on 2. This turned out to be the\nsquare root of 4 over 5,",
    "start": "2535760",
    "end": "2544400"
  },
  {
    "text": "and this turned out to be\nthe square root of 9 over 20.",
    "start": "2544400",
    "end": "2549480"
  },
  {
    "text": "And the point is from here, this\none is just below the diagonal.",
    "start": "2549480",
    "end": "2559220"
  },
  {
    "text": "And it will show\nup as symmetric. Root 5 over 2, root\n4/5 and root 9/20.",
    "start": "2559220",
    "end": "2570630"
  },
  {
    "text": "OK. So, what am I seeing from\nthat particular H which",
    "start": "2570630",
    "end": "2578870"
  },
  {
    "text": "somehow can't be an accident? It must be that it's built in.",
    "start": "2578870",
    "end": "2589210"
  },
  {
    "text": "It's the fact that H is\nsymmetric and tridiagonal.",
    "start": "2589210",
    "end": "2598099"
  },
  {
    "start": "2598100",
    "end": "2604900"
  },
  {
    "text": "And what does that\ntridiagonal tell us? It tells us that we\nhave short recurrences.",
    "start": "2604900",
    "end": "2609920"
  },
  {
    "text": "It's the three-term\nrecurrence relation,",
    "start": "2609920",
    "end": "2617559"
  },
  {
    "text": "is what I'm seeing here\nin matrix language, because there were three\nnon-zeros in the columns of H.",
    "start": "2617560",
    "end": "2627140"
  },
  {
    "text": "All right. I was going to write\nout-- I was going to try to understand this one\nby looking at the first column.",
    "start": "2627140",
    "end": "2635490"
  },
  {
    "text": "What if I take the first\ncolumn of both sides? That'll be A times Q_1, right?",
    "start": "2635490",
    "end": "2641540"
  },
  {
    "text": " Q_1 is the first column vector\nin the-- first basis vector.",
    "start": "2641540",
    "end": "2649440"
  },
  {
    "text": "And what do I have here\nwhen I take q's times h's? Do you do matrix multiplication\na column at a time?",
    "start": "2649440",
    "end": "2659290"
  },
  {
    "text": "You should. OK. So this says take 5/2\nof the first column.",
    "start": "2659290",
    "end": "2665310"
  },
  {
    "text": " And this says take that factor\ntimes the second column.",
    "start": "2665310",
    "end": "2672790"
  },
  {
    "text": " And I could track back\nand see, yes that's",
    "start": "2672790",
    "end": "2680690"
  },
  {
    "text": "what Arnoldi has produced.",
    "start": "2680690",
    "end": "2686050"
  },
  {
    "text": "And then the second one, the\nnext one, would be an A*q_2.",
    "start": "2686050",
    "end": "2691220"
  },
  {
    "text": "the next would an A*q_3. ",
    "start": "2691220",
    "end": "2696710"
  },
  {
    "text": "Well, look, here it is. I want to show that H\nis symmetric when A is.",
    "start": "2696710",
    "end": "2705960"
  },
  {
    "text": "Could you do that? We know what the\nproperty of Q is here.",
    "start": "2705960",
    "end": "2711430"
  },
  {
    "text": "We know that Q is, because\nArnoldi made it that way, has Q transpose Q equal\nI. And I can, in one step,",
    "start": "2711430",
    "end": "2722420"
  },
  {
    "text": "show that H is symmetric\nif A is symmetric. How do you do that? ",
    "start": "2722420",
    "end": "2729859"
  },
  {
    "text": "I guess we need a\nformula for H, so I just multiply by Q inverse.",
    "start": "2729860",
    "end": "2736609"
  },
  {
    "start": "2736610",
    "end": "2742540"
  },
  {
    "text": "So that's good. And even better is to\nrecognize what Q inverse is.",
    "start": "2742540",
    "end": "2749420"
  },
  {
    "text": "So what is Q inverse here? It's Q transpose. Anytime we see Q, that's\nmy letter, always,",
    "start": "2749420",
    "end": "2757030"
  },
  {
    "text": "for an orthogonal matrix. So this is Q transpose\nA Q. And now what?",
    "start": "2757030",
    "end": "2765849"
  },
  {
    "text": "The argument's finished. We're here.",
    "start": "2765850",
    "end": "2770940"
  },
  {
    "text": "If A is symmetric, what can\nyou say about that combination?",
    "start": "2770940",
    "end": "2778490"
  },
  {
    "text": "If A is a symmetric matrix? It is symmetric.",
    "start": "2778490",
    "end": "2786860"
  },
  {
    "text": "Right? That's how you get the\nsymmetric matrices. You start with one.",
    "start": "2786860",
    "end": "2792210"
  },
  {
    "text": "You multiply on one side by a\nmatrix, and on the other side by its transpose. The thing has to be\nsymmetric, because if I",
    "start": "2792210",
    "end": "2800430"
  },
  {
    "text": "transpose this whole\nthing, what will happen? To transpose things\ntheir transposes",
    "start": "2800430",
    "end": "2808020"
  },
  {
    "text": "come in the opposite order. So Q, its transpose comes first.",
    "start": "2808020",
    "end": "2813099"
  },
  {
    "text": "A, its transpose comes in\nthe middle, but what's that? The transpose of A is A. We're\nassuming A to be symmetric.",
    "start": "2813100",
    "end": "2821780"
  },
  {
    "text": "And then the transpose\nof Q transpose is? Is Q.",
    "start": "2821780",
    "end": "2826960"
  },
  {
    "text": "So we got this back\nagain when we transpose, so it's symmetric. So H is symmetric.",
    "start": "2826960",
    "end": "2835000"
  },
  {
    "text": "So the conclusion is,\nH equals H transposed.",
    "start": "2835000",
    "end": "2840190"
  },
  {
    "text": "And then we know immediately\nthat it's tridiagonal, because every H from\nArnoldi is Hessenberg.",
    "start": "2840190",
    "end": "2851390"
  },
  {
    "text": "We know that these\nzeros are here. The Arnoldi cycle ended\nproduced h i j in column j",
    "start": "2851390",
    "end": "2865200"
  },
  {
    "text": "but it's stopped one\nbelow the diagonal. So we know these are\nzero, but now, if we know",
    "start": "2865200",
    "end": "2872810"
  },
  {
    "text": "the matrix is symmetric,\nthen we know these are zero. And, of course, it\nworks out that way.",
    "start": "2872810",
    "end": "2880140"
  },
  {
    "text": "So the conclusion is that we\ncan orthogonalize the Krylov",
    "start": "2880140",
    "end": "2888390"
  },
  {
    "text": "basis, quickly, easily,\nand work with that basis,",
    "start": "2888390",
    "end": "2898099"
  },
  {
    "text": "either explicitly\nby computing it or by implicitly by\nkeeping things orthogonal",
    "start": "2898100",
    "end": "2904829"
  },
  {
    "text": "and that's what conjugate\ngradients will do. So next time, I'm\ngoing to make an effort",
    "start": "2904830",
    "end": "2910720"
  },
  {
    "text": "to describe the conjugate\ngradients method. I'll pick the highlights of it. It has fantastic properties,\nand to verify those properties",
    "start": "2910720",
    "end": "2923150"
  },
  {
    "text": "in full detail is often\nmore confusing than not. If you see today's\nlecture, you're",
    "start": "2923150",
    "end": "2933049"
  },
  {
    "text": "seeing the important\npoints: the role of symmetry of A and the Arnoldi algorithm.",
    "start": "2933050",
    "end": "2943790"
  },
  {
    "text": "OK. so that's our first\nlecture on the Krylov ideas.",
    "start": "2943790",
    "end": "2952250"
  },
  {
    "text": "And next time, we'll\nprobably complete that topic, and it will be on the web.",
    "start": "2952250",
    "end": "2959400"
  },
  {
    "text": "So I'll see you Wednesday\nfor the end of Krylov.",
    "start": "2959400",
    "end": "2965049"
  },
  {
    "text": "Thanks. Good. ",
    "start": "2965050",
    "end": "2971938"
  }
]