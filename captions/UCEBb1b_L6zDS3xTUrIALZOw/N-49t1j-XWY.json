[
  {
    "start": "0",
    "end": "16990"
  },
  {
    "text": "MICHALE FEE: OK, let's\ngo ahead and get started. So today we're turning\nto a new topic called",
    "start": "16990",
    "end": "23060"
  },
  {
    "text": "that basically focused\non principal components analysis, which is a very\ncool way of analyzing",
    "start": "23060",
    "end": "29750"
  },
  {
    "text": "high-dimensional data. Along the way, we're going\nto learn a little bit",
    "start": "29750",
    "end": "36110"
  },
  {
    "text": "more linear algebra. So today, I'm going to talk\nto you about eigenvectors",
    "start": "36110",
    "end": "41810"
  },
  {
    "text": "and eigenvalues which are one\nof the most fundamental concepts in linear algebra.",
    "start": "41810",
    "end": "48710"
  },
  {
    "text": "And it's extremely important\nand widely applicable to a lot of different things.",
    "start": "48710",
    "end": "55490"
  },
  {
    "text": "So eigenvalues and eigenvectors\nare important for everything from understanding energy\nlevels and quantum mechanics",
    "start": "55490",
    "end": "65030"
  },
  {
    "text": "to understanding the vibrational\nmodes of a musical instrument, to analyzing the\ndynamics of differential",
    "start": "65030",
    "end": "71870"
  },
  {
    "text": "equations of the sort that\nyou find that describe",
    "start": "71870",
    "end": "77540"
  },
  {
    "text": "neural circuits in the brain,\nand also for analyzing data",
    "start": "77540",
    "end": "85280"
  },
  {
    "text": "and doing dimensionality\nreduction. So understanding\neigenvectors and eigenvalues",
    "start": "85280",
    "end": "90980"
  },
  {
    "text": "are very important\nfor doing things like principal\ncomponents analysis.",
    "start": "90980",
    "end": "96049"
  },
  {
    "text": "So along the way, we're going\nto talk a little bit more about variance.",
    "start": "96050",
    "end": "101930"
  },
  {
    "text": "We're going to extend\nthe notion of variance that we're all familiar\nwith in one dimension,",
    "start": "101930",
    "end": "107840"
  },
  {
    "text": "like the width of a Gaussian\nor the width of a distribution of data to the case of\nmultivariate Gaussian",
    "start": "107840",
    "end": "116120"
  },
  {
    "text": "distributions or multivariate-- which means, it's\nbasically the same thing",
    "start": "116120",
    "end": "121280"
  },
  {
    "text": "as high-dimensional data. We're going to talk about how\nto compute a covariance matrix",
    "start": "121280",
    "end": "128300"
  },
  {
    "text": "from data which describes\nhow the different dimensions",
    "start": "128300",
    "end": "134420"
  },
  {
    "text": "of the data are correlated\nwith each other, what the variance in\ndifferent dimensions is, and how those different\ndimensions are correlated",
    "start": "134420",
    "end": "141380"
  },
  {
    "text": "with each other. And finally, we'll\ngo through actually how to implement principal\ncomponents analysis, which",
    "start": "141380",
    "end": "148310"
  },
  {
    "text": "is useful for a huge\nnumber of things. I'll come back to many of\nthe different applications",
    "start": "148310",
    "end": "155569"
  },
  {
    "text": "of principal components\nanalysis at the end. But I just want to mention\nthat it's very commonly used",
    "start": "155570",
    "end": "161840"
  },
  {
    "text": "in understanding\nhigh-dimensional data and neural circuits.",
    "start": "161840",
    "end": "167030"
  },
  {
    "text": "So it's a very important\nway of describing how the state of the brain\nevolves as a function of time.",
    "start": "167030",
    "end": "174420"
  },
  {
    "text": "So nowadays, you can\nrecord from hundreds or even thousands or tens\nof thousands of neurons",
    "start": "174420",
    "end": "180080"
  },
  {
    "text": "simultaneously. And if you just look\nat all that data, it just looks like\na complete mess.",
    "start": "180080",
    "end": "186710"
  },
  {
    "text": "But somehow, underneath\nof all of that, the circuitry in\nthe brain is going",
    "start": "186710",
    "end": "193430"
  },
  {
    "text": "through discrete trajectories\nin some low-dimensional space within that high-dimensional\nmess of data.",
    "start": "193430",
    "end": "204120"
  },
  {
    "text": "So our brains have something\nlike 100 billion neurons",
    "start": "204120",
    "end": "209810"
  },
  {
    "text": "in them-- about the same as the\nnumber of stars in our galaxy--",
    "start": "209810",
    "end": "215780"
  },
  {
    "text": "and yet, somehow all of\nthose different neurons communicate with each\nother in a way that",
    "start": "215780",
    "end": "220850"
  },
  {
    "text": "constrains the state of\nthe brain to evolve along the low-dimensional trajectories\nthat are our thoughts",
    "start": "220850",
    "end": "228890"
  },
  {
    "text": "and perceptions. And so it's important to be able\nto visualize those trajectories",
    "start": "228890",
    "end": "235400"
  },
  {
    "text": "in order to understand how\nthat machine is working. ",
    "start": "235400",
    "end": "242390"
  },
  {
    "text": "OK, and then one more comment\nabout principal components analysis, it's not\nactually the best way",
    "start": "242390",
    "end": "251930"
  },
  {
    "text": "often of doing this kind of\ndimensionality reduction. But the basic idea\nof how principal",
    "start": "251930",
    "end": "258470"
  },
  {
    "text": "components analysis works\nis so fundamental to all of the other techniques.",
    "start": "258470",
    "end": "264259"
  },
  {
    "text": "It's sort of the base on which\nall of those other techniques are built conceptually.",
    "start": "264260",
    "end": "270890"
  },
  {
    "text": "So that's why we're going\nto spend a lot of time talking about this.",
    "start": "270890",
    "end": "276260"
  },
  {
    "text": "OK, so let's start with\neigenvectors and eigenvalues. So remember, we've been\ntalking about the idea that matrix multiplication\nperforms a transformation.",
    "start": "276260",
    "end": "285500"
  },
  {
    "text": "So we can have a vector\nx that we multiply it by matrix A. It transforms\nthat set of vectors",
    "start": "285500",
    "end": "292810"
  },
  {
    "text": "x into some other\nset of vectors y. And we can go from y back to x\nby multiplying by A inverse--",
    "start": "292810",
    "end": "301510"
  },
  {
    "text": "if the determinant of that\nmatrix A is not equal to zero. So we've talked about a number\nof different kinds of matrix",
    "start": "301510",
    "end": "308620"
  },
  {
    "text": "transformations by\nintroducing perturbations on the identity matrix. So if we have diagonal matrices,\nwhere one of the elements",
    "start": "308620",
    "end": "316750"
  },
  {
    "text": "is slightly larger than 1,\nthe other diagonal element",
    "start": "316750",
    "end": "323950"
  },
  {
    "text": "is equal to 1, you get a stretch\nof this set of input vectors",
    "start": "323950",
    "end": "329200"
  },
  {
    "text": "along the x-axis. Now, that process of\nstretching vectors",
    "start": "329200",
    "end": "336669"
  },
  {
    "text": "along a particular\ndirection has built into it the idea that there are special\ndirections in this matrix",
    "start": "336670",
    "end": "346000"
  },
  {
    "text": "transformation. So what do I mean by that? So most of these vectors here,\neach one of these red dots",
    "start": "346000",
    "end": "353200"
  },
  {
    "text": "is one of those x's, one\nof those initial vectors-- if you look at\nthe transformation from x to y going--",
    "start": "353200",
    "end": "362150"
  },
  {
    "text": "so that's the x that we put\ninto this matrix transformation. When we multiply by y,\nwe see that that vector",
    "start": "362150",
    "end": "368919"
  },
  {
    "text": "has been stretched\nalong the x direction. So for most of these\nvectors, that stretch",
    "start": "368920",
    "end": "376000"
  },
  {
    "text": "involves a change in the\ndirection of the vector. Going from x to y means that\nthe vector has been rotated.",
    "start": "376000",
    "end": "383380"
  },
  {
    "text": "So you can see that the green\nvector is at a different angle",
    "start": "383380",
    "end": "388800"
  },
  {
    "text": "than the red vector. So there's been a rotation,\nas well as a stretch.",
    "start": "388800",
    "end": "393970"
  },
  {
    "text": "So you can see that's true\nfor that vector, that vector, and so on. So you can see, though, that\nthere are other directions that",
    "start": "393970",
    "end": "402220"
  },
  {
    "text": "are not rotated. So here's another. I just drew that same\npicture over again.",
    "start": "402220",
    "end": "407470"
  },
  {
    "text": "But now, let's look at\nthis particular vector, this particular red vector. You can see that when\nthat red vector is",
    "start": "407470",
    "end": "414340"
  },
  {
    "text": "stretched by this\nmatrix, it's not rotated. It's simply scaled.",
    "start": "414340",
    "end": "422169"
  },
  {
    "text": "Same for this vector right here. That vector is not rotated. It's just scaled,\nin this case, by 1.",
    "start": "422170",
    "end": "428710"
  },
  {
    "text": " But let's take a look at\nthis other transformation.",
    "start": "428710",
    "end": "433870"
  },
  {
    "text": "So this transformation produces\na stretch in the y direction",
    "start": "433870",
    "end": "439990"
  },
  {
    "text": "and a compression\nin the x direction. So I'm just showing you a\nsubset of those vectors now.",
    "start": "439990",
    "end": "447520"
  },
  {
    "text": "You can see that,\nagain, this vector is rotated by that transformation.",
    "start": "447520",
    "end": "453520"
  },
  {
    "text": "This vector is rotated\nby that transformation. But other vectors\nare not rotated.",
    "start": "453520",
    "end": "458590"
  },
  {
    "text": "So again, this\nvector is compressed. It's simply scaled,\nbut it's not rotated.",
    "start": "458590",
    "end": "465430"
  },
  {
    "text": "And this vector is stretched. It's scaled but not rotated.",
    "start": "465430",
    "end": "471289"
  },
  {
    "text": "Does that make sense? OK, so these\ntransformations here",
    "start": "471290",
    "end": "476910"
  },
  {
    "text": "are given by a diagonal matrices\nwhere the off-diagonal elements are zero. And the diagonal elements\nare just some constant.",
    "start": "476910",
    "end": "483780"
  },
  {
    "start": "483780",
    "end": "489720"
  },
  {
    "text": "So for all diagonal matrices,\nthese special directions,",
    "start": "489720",
    "end": "495540"
  },
  {
    "text": "the directions on which\nvectors are simply scaled but not rotated by that\nmatrix by that transformation,",
    "start": "495540",
    "end": "505680"
  },
  {
    "text": "it's the vectors along\nthe axes that are scaled and not rotated-- along the x-axis or the y-axis.",
    "start": "505680",
    "end": "512370"
  },
  {
    "text": " And you can see that by\ntaking this matrix A,",
    "start": "512370",
    "end": "519229"
  },
  {
    "text": "this general diagonal\nmatrix, multiplying it by a vector along\nthe x-axis, and you",
    "start": "519230",
    "end": "527420"
  },
  {
    "text": "can see that that is\njust a constant, lambda 1, times that vector.",
    "start": "527420",
    "end": "533190"
  },
  {
    "text": "So we take this times\nthis, plus this times this, is equal to lambda 1.",
    "start": "533190",
    "end": "539120"
  },
  {
    "text": "This times this plus this\ntimes this is equal to zero. So you can see that A times\nthat vector in the x direction",
    "start": "539120",
    "end": "545420"
  },
  {
    "text": "is simply a scaled version of\nthe vector in the x direction. And the scaling factor\nis simply the constant",
    "start": "545420",
    "end": "552712"
  },
  {
    "text": "that's on the diagonal. ",
    "start": "552712",
    "end": "557950"
  },
  {
    "text": "So we can write this\nin matrix notation as this lambda, this stretch\nvector, this diagonal matrix,",
    "start": "557950",
    "end": "568810"
  },
  {
    "text": "times a unit vector\nin the x direction. That's the standard\nbasis vector,",
    "start": "568810",
    "end": "575390"
  },
  {
    "text": "the first standard basis vector. So that's a unit vector\nin the x direction is equal to lambda 1 times\na vector in the x direction.",
    "start": "575390",
    "end": "584486"
  },
  {
    "text": " And if we do that\nsame multiplication for a vector in\nthe y direction, we",
    "start": "584486",
    "end": "592320"
  },
  {
    "text": "see that we get a constant times\nthat vector in the y direction.",
    "start": "592320",
    "end": "597370"
  },
  {
    "text": "So we have another equation. So this particular matrix,\nthis diagonal matrix,",
    "start": "597370",
    "end": "604080"
  },
  {
    "text": "has two vectors that are in\nspecial directions in the sense",
    "start": "604080",
    "end": "611000"
  },
  {
    "text": "that they aren't rotated. They're just stretched. So diagonal matrices\nhave the property",
    "start": "611000",
    "end": "618690"
  },
  {
    "text": "that they map any vector\nparallel to the standard basis",
    "start": "618690",
    "end": "623830"
  },
  {
    "text": "into another vector\nalong the standard basis. ",
    "start": "623830",
    "end": "630680"
  },
  {
    "text": "So that now is a general\nn-dimensional diagonal matrix",
    "start": "630680",
    "end": "637130"
  },
  {
    "text": "with these lambdas,\nwhich are just scalar constants along the diagonal.",
    "start": "637130",
    "end": "643069"
  },
  {
    "text": "And there are n\nequations that look like this that say that\nthis matrix times a vector",
    "start": "643070",
    "end": "651649"
  },
  {
    "text": "in the direction of a\nstandard basis vector is equal to a\nconstant times that",
    "start": "651650",
    "end": "657050"
  },
  {
    "text": "vector in the standard\nbasis direction. Any questions about that?",
    "start": "657050",
    "end": "662660"
  },
  {
    "text": "Everything else just flows\nfrom this very easily. So if you have any questions\nabout that, just ask.",
    "start": "662660",
    "end": "670930"
  },
  {
    "text": "OK, that equation is called\nthe eigenvalue equation.",
    "start": "670930",
    "end": "677110"
  },
  {
    "text": "And it describes a property\nof this matrix lambda.",
    "start": "677110",
    "end": "685560"
  },
  {
    "text": " So any vector v that's\nmapped by a matrix A",
    "start": "685560",
    "end": "695060"
  },
  {
    "text": "onto a parallel vector\nlambda v is called an eigenvector of this matrix.",
    "start": "695060",
    "end": "704660"
  },
  {
    "text": "So we're going to generalize\nnow from diagonal matrices that look like this to an arbitrary\nmatrix A. So the statement",
    "start": "704660",
    "end": "717670"
  },
  {
    "text": "is that any vector, that\nwhen you multiply it by a matrix A that gets\ntransformed into a vector",
    "start": "717670",
    "end": "727190"
  },
  {
    "text": "parallel to v, it's called\nan eigenvector of A.",
    "start": "727190",
    "end": "732600"
  },
  {
    "text": "And the one vector that\nthis is true for that",
    "start": "732600",
    "end": "738600"
  },
  {
    "text": "isn't called an eigenvector\nis the zero vector because you can see that a zero\nvector here times any matrix",
    "start": "738600",
    "end": "749910"
  },
  {
    "text": "is equal to zero. OK, so we exclude\nthe zero vector.",
    "start": "749910",
    "end": "755520"
  },
  {
    "text": "We don't call the zero\nvector an eigenvector. ",
    "start": "755520",
    "end": "763220"
  },
  {
    "text": "So typically a matrix,\nan n-dimensional matrix,",
    "start": "763220",
    "end": "770509"
  },
  {
    "text": "has n eigenvectors\nand n eigenvalues. Oh, and I forgot to say\nthat the scale factor lambda is called the eigenvalue\nassociated with that vector v.",
    "start": "770510",
    "end": "784860"
  },
  {
    "text": "So now, let's take\na look at a matrix that's a little more complicated\nthan our diagonal matrix.",
    "start": "784860",
    "end": "792380"
  },
  {
    "text": "Let's take one of these\nrotated stretch matrices. So remember, in\nthe last class, we",
    "start": "792380",
    "end": "799370"
  },
  {
    "text": "built a matrix like\nthis that produces a stretch of a factor of\n2 along a 45-degree axis.",
    "start": "799370",
    "end": "806690"
  },
  {
    "text": "And we built that matrix\nby multiplying it together",
    "start": "806690",
    "end": "811790"
  },
  {
    "text": "by basically taking\nthis set of vectors, rotating them, stretching them,\nand then rotating them back.",
    "start": "811790",
    "end": "820670"
  },
  {
    "text": "So we did that by three\nseparate transformations that we applied successively.",
    "start": "820670",
    "end": "827329"
  },
  {
    "text": "And we did that by multiplying\nphi transpose lambda and then",
    "start": "827330",
    "end": "833270"
  },
  {
    "text": "phi. So let's see what the special\ndirections are for this matrix",
    "start": "833270",
    "end": "840889"
  },
  {
    "text": "transformation. So you can see that most\nof these vectors that we've",
    "start": "840890",
    "end": "846139"
  },
  {
    "text": "multiplied by this\nmatrix get rotated. ",
    "start": "846140",
    "end": "853320"
  },
  {
    "text": "And you can see\nthat even vectors along the standard basis\ndirections get rotated.",
    "start": "853320",
    "end": "860940"
  },
  {
    "text": "So what are the special\ndirections for this matrix? Well, they're going to be\nthese vectors right here.",
    "start": "860940",
    "end": "867460"
  },
  {
    "text": "So this vector along\nthis 45-degree line gets transformed.",
    "start": "867460",
    "end": "872910"
  },
  {
    "text": "It's not rotated. It gets stretched\nby a factor of 1. And this vector\nhere gets stretched.",
    "start": "872910",
    "end": "879240"
  },
  {
    "text": " OK, so you can see that\nthis matrix has eigenvectors",
    "start": "879240",
    "end": "887329"
  },
  {
    "text": "that are along this 45-degree\naxis and that 45-degree axis. ",
    "start": "887330",
    "end": "895510"
  },
  {
    "text": "So in general,\nlet's calculate what are the eigenvectors\nand eigenvalues",
    "start": "895510",
    "end": "905150"
  },
  {
    "text": "for a general rotated\ntransformation matrix. ",
    "start": "905150",
    "end": "912620"
  },
  {
    "text": "So let's do that. Let's take this matrix A and\nmultiply it by a vector x.",
    "start": "912620",
    "end": "919990"
  },
  {
    "text": "And we're going to\nask what vectors x satisfy the properties that,\nwhen they're multiplied by A,",
    "start": "919990",
    "end": "927100"
  },
  {
    "text": "are equal to a constant times x. So we're going to ask what are\nthe eigenvectors of this matrix",
    "start": "927100",
    "end": "933700"
  },
  {
    "text": "A that we've constructed\nin this form? ",
    "start": "933700",
    "end": "940533"
  },
  {
    "text": "So what we're going\nto do is we're going to replace A with this\nproduct of matrices, of three",
    "start": "940533",
    "end": "947620"
  },
  {
    "text": "matrices. We're going to multiply\nthis equation on both sides",
    "start": "947620",
    "end": "953300"
  },
  {
    "text": "by phi transpose on the\nleft side, by phi transpose. OK, so phi transpose times\nthis, is equal to A sabai, x",
    "start": "953300",
    "end": "965980"
  },
  {
    "text": "subai, times 5\ntranspose on the left. What happens here?",
    "start": "965980",
    "end": "971320"
  },
  {
    "text": " Remember phi is a\nrotation matrix.",
    "start": "971320",
    "end": "977030"
  },
  {
    "text": "What is phi transpose phi? Anybody remember?",
    "start": "977030",
    "end": "982819"
  },
  {
    "text": "Good. Because for rotation\nmatrix, the inverse, the transpose of a rotation\nmatrix, is its inverse.",
    "start": "982820",
    "end": "991160"
  },
  {
    "text": "And so phi transpose phi is just\nequal to the identity matrix. So that goes away.",
    "start": "991160",
    "end": "997399"
  },
  {
    "text": "And we're left with\nlambda phi transpose x equals A phi transpose x.",
    "start": "997400",
    "end": "1003490"
  },
  {
    "start": "1003490",
    "end": "1009380"
  },
  {
    "text": "So remember that\nwe just wrote down",
    "start": "1009380",
    "end": "1015030"
  },
  {
    "text": "that if we have\na diagonal matrix lambda, that the eigenvectors\nare the standard basis vectors.",
    "start": "1015030",
    "end": "1025064"
  },
  {
    "start": "1025065",
    "end": "1032779"
  },
  {
    "text": "So what does that mean? If we look at this\nequation here, and we look at\nthis equation here,",
    "start": "1032780",
    "end": "1044930"
  },
  {
    "text": "it seems like phi transpose x is\nan eigenvector of this equation",
    "start": "1044930",
    "end": "1051850"
  },
  {
    "text": "as long as phi transpose\nx is equal to one of the standard basis vectors.",
    "start": "1051850",
    "end": "1058170"
  },
  {
    "text": "Does that make sense?  So we know this\nsolution is satisfied",
    "start": "1058170",
    "end": "1066290"
  },
  {
    "text": "by phi transpose x is equal\nto one of the standard basis vectors. Does that make sense?",
    "start": "1066290",
    "end": "1072320"
  },
  {
    "start": "1072320",
    "end": "1081429"
  },
  {
    "text": "So if we replace phi transpose\nx with one of the standard basis vectors, then that\nsolves this equation.",
    "start": "1081430",
    "end": "1088404"
  },
  {
    "start": "1088405",
    "end": "1093610"
  },
  {
    "text": "So what that means\nis that the solution to this eigenvalue equation\nis that the eigenvalues",
    "start": "1093610",
    "end": "1102570"
  },
  {
    "text": "A are simply the diagonal\nelements of this lambda here. ",
    "start": "1102570",
    "end": "1110790"
  },
  {
    "text": "And the eigenvectors\nare just x, where x is equal to phi times\nthe standard basis vectors.",
    "start": "1110790",
    "end": "1120799"
  },
  {
    "text": "We just solve for x by\nmultiplying both sides by phi transpose inverse.",
    "start": "1120800",
    "end": "1127900"
  },
  {
    "text": "What's phi transpose inverse? phi. ",
    "start": "1127900",
    "end": "1133080"
  },
  {
    "text": "So we multiply\nboth sides by phi. This becomes the\nidentity matrix.",
    "start": "1133080",
    "end": "1138700"
  },
  {
    "text": "And we have x equals phi times\nthis set of standard basis vectors.",
    "start": "1138700",
    "end": "1145760"
  },
  {
    "text": "Any questions about that? That probably went\nby pretty fast.",
    "start": "1145760",
    "end": "1151570"
  },
  {
    "text": "But does everyone believe this?",
    "start": "1151570",
    "end": "1157080"
  },
  {
    "text": "We went through that. We went through both examples\nof how this equation is",
    "start": "1157080",
    "end": "1162930"
  },
  {
    "text": "true for the case where\nlambda is a diagonal matrix and the e's are the\nstandard basis vectors.",
    "start": "1162930",
    "end": "1172179"
  },
  {
    "text": "And if we solve for the\neigenvectors of this equation",
    "start": "1172180",
    "end": "1178320"
  },
  {
    "text": "where A has this form of\nphi lambda phi transpose, you can see that\nthe eigenvectors",
    "start": "1178320",
    "end": "1185880"
  },
  {
    "text": "are given by this matrix\ntimes a standard basis vector.",
    "start": "1185880",
    "end": "1194890"
  },
  {
    "text": "So any standard basis\nvector times phi will give you an eigenvector\nof this equation here.",
    "start": "1194890",
    "end": "1200049"
  },
  {
    "start": "1200050",
    "end": "1213460"
  },
  {
    "text": "Let's push on. And the eigenvalues are\njust these diagonal elements",
    "start": "1213460",
    "end": "1219640"
  },
  {
    "text": "of this lambda. ",
    "start": "1219640",
    "end": "1227730"
  },
  {
    "text": "What are these? So now, we're going to figure\nout what these things are,",
    "start": "1227730",
    "end": "1232890"
  },
  {
    "text": "and how to just\nsee what they are. These eigenvectors\nhere are given by phi",
    "start": "1232890",
    "end": "1243149"
  },
  {
    "text": "times a standard basis vector. So phi is a rotation\nmatrix, right?",
    "start": "1243150",
    "end": "1251200"
  },
  {
    "text": "So phi times a standard\nbasis vector is just what? It's just a standard\nbasis vector rotated.",
    "start": "1251200",
    "end": "1257950"
  },
  {
    "text": " So let's just solve\nfor these two x's.",
    "start": "1257950",
    "end": "1266350"
  },
  {
    "text": "We're going to take phi, which\nwas this 45-degree rotation matrix, and we're\ngoing to multiply it",
    "start": "1266350",
    "end": "1272270"
  },
  {
    "text": "by the standard basis\nvector in the x direction.",
    "start": "1272270",
    "end": "1277860"
  },
  {
    "text": "So what is that? Just multiply this out. You'll see that this is just a\nvector along a 45-degree line.",
    "start": "1277860",
    "end": "1285870"
  },
  {
    "text": " So this eigenvector, this\nfirst eigenvector here,",
    "start": "1285870",
    "end": "1294370"
  },
  {
    "text": "is just a vector on the\n45-degree line, 1 over root 2. It's a unit vector.",
    "start": "1294370",
    "end": "1300130"
  },
  {
    "text": "That's why it's got the\n1 over root 2 in it. The second eigenvector\nis just phi times e2.",
    "start": "1300130",
    "end": "1307550"
  },
  {
    "text": "So it's a rotated version of the\ny standard basis vector, which",
    "start": "1307550",
    "end": "1315580"
  },
  {
    "text": "is 1 over root 2 minus 1, 1. That's this vector.",
    "start": "1315580",
    "end": "1321520"
  },
  {
    "text": " So our two eigenvectors we\nderived for this matrix that",
    "start": "1321520",
    "end": "1332020"
  },
  {
    "text": "produces this stretch along\na 45-degree line, the two eigenvectors are the\nvector, 45-degree vector",
    "start": "1332020",
    "end": "1341110"
  },
  {
    "text": "in this quadrant, and\nthe 45-degree vector in that quadrant. Notice it's just a\nrotated basis set.",
    "start": "1341110",
    "end": "1349330"
  },
  {
    "start": "1349330",
    "end": "1356799"
  },
  {
    "text": "So notice that the\neigenvectors are just the columns of our\nrotated matrix.",
    "start": "1356800",
    "end": "1369140"
  },
  {
    "text": "So let me recap.  If you have a matrix that\nyou've constructed like this,",
    "start": "1369140",
    "end": "1378460"
  },
  {
    "text": "as a matrix that produces a\nstretch in a rotated frame,",
    "start": "1378460",
    "end": "1387460"
  },
  {
    "text": "the eigenvalues are just the\ndiagonal elements of the lambda matrix that you put in\nthere to build that thing,",
    "start": "1387460",
    "end": "1394510"
  },
  {
    "text": "to build that matrix. And the eigenvectors\nare just the columns",
    "start": "1394510",
    "end": "1401020"
  },
  {
    "text": "of the rotation matrix. ",
    "start": "1401020",
    "end": "1409420"
  },
  {
    "text": "OK, so let me summarize. A symmetric matrix can\nalways be written like this,",
    "start": "1409420",
    "end": "1418510"
  },
  {
    "text": "where phi is a rotation matrix. And lambda is a\ndiagonal matrix that tells you how much the\ndifferent axes are stretched.",
    "start": "1418510",
    "end": "1425870"
  },
  {
    "text": " The eigenvectors of this matrix\nA are the columns of phi.",
    "start": "1425870",
    "end": "1433150"
  },
  {
    "text": "They are the basis vectors,\nthe new basis vectors, in this rotated basis set.",
    "start": "1433150",
    "end": "1440190"
  },
  {
    "text": " So remember, we can\n[AUDIO OUT] this rotation",
    "start": "1440190",
    "end": "1447150"
  },
  {
    "text": "matrix as a set of basis\nvectors, as the columns.",
    "start": "1447150",
    "end": "1454100"
  },
  {
    "text": "And that set of basis\nvectors are the eigenvectors of any matrix that you\nconstruct like this.",
    "start": "1454100",
    "end": "1463690"
  },
  {
    "text": "And the eigenvalues are just the\ndiagonal elements of the lambda that you put in there.",
    "start": "1463690",
    "end": "1470390"
  },
  {
    "text": "All right, any\nquestions about that?  For the most part,\nwe're going to be",
    "start": "1470390",
    "end": "1478960"
  },
  {
    "text": "working with matrices\nthat are symmetric,",
    "start": "1478960",
    "end": "1484299"
  },
  {
    "text": "that can be built like this. ",
    "start": "1484300",
    "end": "1500090"
  },
  {
    "text": "So eigenvectors are not unique. So if x eigenvector of A,\nthen any scaled version of x",
    "start": "1500090",
    "end": "1515030"
  },
  {
    "text": "is also an eigenvector. Remember, an\neigenvector is a vector",
    "start": "1515030",
    "end": "1521240"
  },
  {
    "text": "that when you multiply\nit by a matrix just gets stretched\nand not rotated.",
    "start": "1521240",
    "end": "1527900"
  },
  {
    "text": "What that means is that any\nvector in that direction will also be stretched\nand not rotated.",
    "start": "1527900",
    "end": "1533960"
  },
  {
    "text": "So eigenvectors are not unique. Any scaled version\nof an eigenvector",
    "start": "1533960",
    "end": "1539210"
  },
  {
    "text": "is also an eigenvector. When we write down\neigenvectors of a matrix,",
    "start": "1539210",
    "end": "1545780"
  },
  {
    "text": "we usually write\ndown unit vectors to avoid this ambiguity.",
    "start": "1545780",
    "end": "1551960"
  },
  {
    "text": " So we usually write\neigenvectors as unit vectors.",
    "start": "1551960",
    "end": "1559919"
  },
  {
    "text": "For matrices of n\ndimensions, there are typically n different\nunit eigenvectors--",
    "start": "1559920",
    "end": "1566820"
  },
  {
    "text": "n different vectors in\ndifferent directions that have the special properties\nthat they're just stretched",
    "start": "1566820",
    "end": "1572520"
  },
  {
    "text": "and not rotated.  So for our two-dimensional\nmatrices that produce stretch",
    "start": "1572520",
    "end": "1581770"
  },
  {
    "text": "in one direction, the\nspecial directions are-- ",
    "start": "1581770",
    "end": "1587440"
  },
  {
    "text": "sorry, so here is a\ntwo-dimensional, two-by-two matrix that produces a\nstretch in this direction.",
    "start": "1587440",
    "end": "1594630"
  },
  {
    "text": "There are two eigenvectors,\ntwo unit eigenvectors, one in this direction and\none in that direction.",
    "start": "1594630",
    "end": "1600392"
  },
  {
    "text": " And notice, that because\nthe eigenvectors are",
    "start": "1600392",
    "end": "1609140"
  },
  {
    "text": "the columns of this\nrotation matrix, the eigenvectors form a\ncomplete orthonormal basis set.",
    "start": "1609140",
    "end": "1619110"
  },
  {
    "text": "And that is true. That statement is true\nonly for symmetric matrices",
    "start": "1619110",
    "end": "1625310"
  },
  {
    "text": "that are constructed like this. ",
    "start": "1625310",
    "end": "1637100"
  },
  {
    "text": "So now, let's calculate\nwhat the eigenvalues are for a general two-dimensional\nmatrix A. So here's our matrix",
    "start": "1637100",
    "end": "1648040"
  },
  {
    "text": "A. That's an eigenvector. Any vector x that\nsatisfies that equation",
    "start": "1648040",
    "end": "1655220"
  },
  {
    "text": "is called an eigenvector. And that's the\neigenvalue associated with that eigenvector.",
    "start": "1655220",
    "end": "1661670"
  },
  {
    "text": "We can rewrite this\nequation as A times x equals lambda i times x--",
    "start": "1661670",
    "end": "1669000"
  },
  {
    "text": "just like A equals b,\nthen equals 1 times b.",
    "start": "1669000",
    "end": "1674600"
  },
  {
    "text": " We can subtract that\nfrom both sides,",
    "start": "1674600",
    "end": "1681700"
  },
  {
    "text": "and we get A minus lambda\ni times x equals zero. So that is a different way of\nwriting an eigenvalue equation.",
    "start": "1681700",
    "end": "1689905"
  },
  {
    "text": " Now, what we're to\ndo is we're going to solve for lambdas that\nsatisfy this equation.",
    "start": "1689905",
    "end": "1699650"
  },
  {
    "text": "And we only want solutions\nwhere x is not equal to zero. ",
    "start": "1699650",
    "end": "1712160"
  },
  {
    "text": "So this is just a matrix. A minus lambda i\nis just a matrix.",
    "start": "1712160",
    "end": "1718490"
  },
  {
    "text": "So how do we know whether\nthis matrix has solutions",
    "start": "1718490",
    "end": "1728290"
  },
  {
    "text": "where x is not equal to zero? ",
    "start": "1728290",
    "end": "1736590"
  },
  {
    "text": "Any ideas? [INAUDIBLE] AUDIENCE: [INAUDIBLE]",
    "start": "1736590",
    "end": "1742210"
  },
  {
    "text": "MICHALE FEE: Is, so what do\nwe need the determinant to do? AUDIENCE: [INAUDIBLE]",
    "start": "1742210",
    "end": "1750970"
  },
  {
    "text": "MICHALE FEE: Has to be zero. If the determinant of this\nmatrix is not equal to zero,",
    "start": "1750970",
    "end": "1761540"
  },
  {
    "text": "then the only solution to this\nequation is x equals zero. OK, so we solve this equation.",
    "start": "1761540",
    "end": "1769100"
  },
  {
    "text": "We ask what values of\nlambda give us a zero",
    "start": "1769100",
    "end": "1774679"
  },
  {
    "text": "determinant in this matrix. So let's write down\nan arbitrary A,",
    "start": "1774680",
    "end": "1780980"
  },
  {
    "text": "an arbitrary two-dimensional\nmatrix A, 2D, 2 by 2.",
    "start": "1780980",
    "end": "1786720"
  },
  {
    "text": "We can write A minus\nlambda i like this. Remember, lambda i is just\nlambdas on the diagonals.",
    "start": "1786720",
    "end": "1797170"
  },
  {
    "text": "The determinant of\nA minus lambda i is just the product of\nthe diagonal elements",
    "start": "1797170",
    "end": "1803200"
  },
  {
    "text": "minus the product of the\noff-diagonal elements. And we set that equal to zero.",
    "start": "1803200",
    "end": "1810510"
  },
  {
    "text": "And we solve for lambda.  And that just looks\nlike a polynomial.",
    "start": "1810510",
    "end": "1818390"
  },
  {
    "start": "1818390",
    "end": "1826650"
  },
  {
    "text": "OK, so the solutions\nto that polynomial solve what's called the\ncharacteristic equation",
    "start": "1826650",
    "end": "1833559"
  },
  {
    "text": "of this matrix A. And\nthose are the eigenvalues",
    "start": "1833560",
    "end": "1839620"
  },
  {
    "text": "of this arbitrary matrix A,\nthis 2D, two-by-two matrix.",
    "start": "1839620",
    "end": "1845710"
  },
  {
    "text": "So there is\ncharacteristic equation. There is the\ncharacteristic polynomial. We can solve for lambda just\nby using the quadratic formula.",
    "start": "1845710",
    "end": "1856340"
  },
  {
    "text": "And those are the eigenvalues\nof A. Notice, first of all,",
    "start": "1856340",
    "end": "1866559"
  },
  {
    "text": "there are two of them\ngiven by the two roots of this quadratic equation.",
    "start": "1866560",
    "end": "1875309"
  },
  {
    "text": "And notice that they\ncan be real or complex.",
    "start": "1875310",
    "end": "1881250"
  },
  {
    "text": "They can be complex. They are complex in general. ",
    "start": "1881250",
    "end": "1889710"
  },
  {
    "text": "And they can be\nreal, or imaginary, or have real and\nimaginary components.",
    "start": "1889710",
    "end": "1896110"
  },
  {
    "text": " And that just depends on\nthis quantity right here.",
    "start": "1896110",
    "end": "1903730"
  },
  {
    "text": "If what's inside this\nsquare root is negative, then eigenvalues\nwill be complex.",
    "start": "1903730",
    "end": "1911020"
  },
  {
    "text": "If what's inside the\nsquare root is positive, then the eigenvector\nwill be real.",
    "start": "1911020",
    "end": "1917179"
  },
  {
    "text": " So let's find the eigenvalues\nfor a symmetric matrix.",
    "start": "1917180",
    "end": "1925830"
  },
  {
    "text": "a, d on the diagonals and\nb on the off-diagonals.",
    "start": "1925830",
    "end": "1930929"
  },
  {
    "text": "So let's see what happens. Let's plug these\ninto this equation. The 4bc becomes 4b squared.",
    "start": "1930930",
    "end": "1939440"
  },
  {
    "text": "And you can see\nthat this thing has to be greater than zero\nbecause a minus d squared has",
    "start": "1939440",
    "end": "1947570"
  },
  {
    "text": "[INAUDIBLE] has to be positive. And b squared has\nto be positive.",
    "start": "1947570",
    "end": "1953810"
  },
  {
    "text": "And so that quantity has\nto be greater than zero. And so what we find is\nthat the eigenvalues",
    "start": "1953810",
    "end": "1959500"
  },
  {
    "text": "of a symmetric matrix\nare always real. ",
    "start": "1959500",
    "end": "1966270"
  },
  {
    "text": "So let's just take\nthis particular-- just an example-- and let's\nplug those into this equation.",
    "start": "1966270",
    "end": "1974800"
  },
  {
    "text": "And what we find is\nthat the eigenvalues are 1 plus or minus root 2 over 2.",
    "start": "1974800",
    "end": "1985370"
  },
  {
    "text": "So two real eigenvalues. ",
    "start": "1985370",
    "end": "1992070"
  },
  {
    "text": "So let's consider a special\ncase of a symmetric matrix. Let's consider a matrix where\nthe diagonal elements are",
    "start": "1992070",
    "end": "2000470"
  },
  {
    "text": "equal, and the off-diagonal\nelements are equal. ",
    "start": "2000470",
    "end": "2006570"
  },
  {
    "text": "So we can update this\nequation for the case where the diagonal\nelements are equal.",
    "start": "2006570",
    "end": "2012670"
  },
  {
    "text": "So a equals d. And what you find is\nthat the eigenvalues are",
    "start": "2012670",
    "end": "2018020"
  },
  {
    "text": "just a plus b and a minus b--\nso a plus b and a minus b.",
    "start": "2018020",
    "end": "2024920"
  },
  {
    "text": "And the eigenvectors\ncan be found just",
    "start": "2024920",
    "end": "2030180"
  },
  {
    "text": "by plugging these eigenvalues\ninto the eigenvalue equation",
    "start": "2030180",
    "end": "2035220"
  },
  {
    "text": "and solving for\nthe eigenvectors. So I'll just go through\nthat real quick--",
    "start": "2035220",
    "end": "2041159"
  },
  {
    "text": "a times x. So we found two\neigenvalues, so there are going to be two eigenvectors.",
    "start": "2041160",
    "end": "2047460"
  },
  {
    "text": "We can just plug that\nfirst eigenvalue into here, call it lambda plus.",
    "start": "2047460",
    "end": "2052619"
  },
  {
    "text": "And now, we can solve for\nthe eigenvector associated with that eigenvalue.",
    "start": "2052620",
    "end": "2058290"
  },
  {
    "text": "Just plug that in, solve for x. What you find is that the x\nassociated with that eigenvalue",
    "start": "2058290",
    "end": "2065190"
  },
  {
    "text": "is 1, 1-- if you just\ngo through the algebra.",
    "start": "2065190",
    "end": "2070349"
  },
  {
    "text": "So that's the\neigenvector associated with that eigenvalue. And that is the\neigenvector associated",
    "start": "2070350",
    "end": "2077908"
  },
  {
    "text": "with that eigenvalue.  So I'll just give you a hint.",
    "start": "2077909",
    "end": "2085059"
  },
  {
    "text": "In most of the\nproblems that I'll give you to deal with\non an exam or many",
    "start": "2085060",
    "end": "2093810"
  },
  {
    "text": "of the ones in the\nproblem sets, I think, in the\nproblem set will have",
    "start": "2093810",
    "end": "2099650"
  },
  {
    "text": "a form like this and\n[INAUDIBLE] eigenvectors along a 45-degree axis.",
    "start": "2099650",
    "end": "2106520"
  },
  {
    "text": "So if you see a\nmatrix like that, you don't have to\nplug it into MATLAB",
    "start": "2106520",
    "end": "2111589"
  },
  {
    "text": "to extract the eigenvalues. You just know that\nthe eigenvectors",
    "start": "2111590",
    "end": "2116869"
  },
  {
    "text": "are on the 45-degree axis. ",
    "start": "2116870",
    "end": "2124310"
  },
  {
    "text": "So the process of writing\na matrix as phi lambda phi",
    "start": "2124310",
    "end": "2131200"
  },
  {
    "text": "transpose is called\neigen-decomposition of this matrix A. So\nif you have a matrix",
    "start": "2131200",
    "end": "2140140"
  },
  {
    "text": "that you can write\ndown like this, that you can write\nin that form, it's called eigen-decomposition.",
    "start": "2140140",
    "end": "2149099"
  },
  {
    "text": "And the lambdas, the diagonal\nelements of this lambda matrix,",
    "start": "2149100",
    "end": "2154660"
  },
  {
    "text": "are real. And they're the eigenvalues. The columns of phi\nare the eigenvalues,",
    "start": "2154660",
    "end": "2162460"
  },
  {
    "text": "and they form an\northogonal basis set. ",
    "start": "2162460",
    "end": "2171190"
  },
  {
    "text": "And this, if you\ntake this equation and you multiply it\non both sides by phi,",
    "start": "2171190",
    "end": "2176950"
  },
  {
    "text": "you can write down that equation\nin a slightly different form-- A times phi equals phi lambda.",
    "start": "2176950",
    "end": "2184180"
  },
  {
    "text": "This is a matrix way,\na matrix equivalent,",
    "start": "2184180",
    "end": "2190900"
  },
  {
    "text": "to the set of equations\nthat we wrote down earlier. So remember, we wrote down\nthis eigenvalue equation that",
    "start": "2190900",
    "end": "2200670"
  },
  {
    "text": "describes that when you\nmultiply this matrix A times an eigenvector equals lambda\ntimes the eigenvector,",
    "start": "2200670",
    "end": "2210210"
  },
  {
    "text": "this is equivalent to writing\ndown this matrix equation.",
    "start": "2210210",
    "end": "2215530"
  },
  {
    "text": "So you'll often\nsee this equation to describe the form of\nthe eigenvalue equation",
    "start": "2215530",
    "end": "2222549"
  },
  {
    "text": "rather than this form. Why? Because it's more compact. ",
    "start": "2222550",
    "end": "2228238"
  },
  {
    "text": "Any questions about that? We've just piled up all of\nthese different f vectors into the columns of this\nrotation matrix phi.",
    "start": "2228238",
    "end": "2237730"
  },
  {
    "text": " So if you see an\nequation like that,",
    "start": "2237730",
    "end": "2244240"
  },
  {
    "text": "you'll know that\nyou're just looking at an eigenvalue\nequation just like this.",
    "start": "2244240",
    "end": "2250410"
  },
  {
    "text": "Now in general, when you want\nto do eigen-decomposition, when you have a symmetric\nmatrix that you want",
    "start": "2250410",
    "end": "2256980"
  },
  {
    "text": "to write down in this form. It's really simple. You don't have to go\nthrough all of this stuff",
    "start": "2256980",
    "end": "2264450"
  },
  {
    "text": "with the characteristic\nequation, and solve for the eigenvalues,\nand then plug them in here,",
    "start": "2264450",
    "end": "2273180"
  },
  {
    "text": "and solve for the eigenvectors. You can do that if\nyou really want to. But most people don't because in\ntwo dimensions, you can do it.",
    "start": "2273180",
    "end": "2282400"
  },
  {
    "text": "But in higher dimensions,\nit's very hard or impossible.",
    "start": "2282400",
    "end": "2288010"
  },
  {
    "text": "So what you typically do is just\nuse the eig function in MATLAB. If you just use this\nfunction eig on a matrix,",
    "start": "2288010",
    "end": "2295050"
  },
  {
    "text": "it will return the\neigenvectors and eigenvalues. So here, I'm just\nconstructing a matrix A--",
    "start": "2295050",
    "end": "2301290"
  },
  {
    "text": "1.5, 0.5, 0.5, and\n1.5, like that.",
    "start": "2301290",
    "end": "2308220"
  },
  {
    "text": "And if you just use\nthe eig function, it returns the eigenvectors\nas the columns of the matrix",
    "start": "2308220",
    "end": "2316700"
  },
  {
    "text": "and the eigenvalues as the\ndiagonals of this matrix. So you have to pass it.",
    "start": "2316700",
    "end": "2322400"
  },
  {
    "text": "Arguments F and V\nequals eig of A. And it returns eigenvectors\nand eigenvalues.",
    "start": "2322400",
    "end": "2331490"
  },
  {
    "text": "Any questions about that? ",
    "start": "2331490",
    "end": "2338950"
  },
  {
    "text": "So let's push on toward doing\nprincipal components analysis.",
    "start": "2338950",
    "end": "2346220"
  },
  {
    "text": "So this is just the\nmachinery that you use. Oh, and I think I had one more\npanel here just to show you",
    "start": "2346220",
    "end": "2353210"
  },
  {
    "text": "that if you take F and\nV, you can reconstruct A. So A is just F, V, F transpose.",
    "start": "2353210",
    "end": "2362030"
  },
  {
    "text": "F is just phi in the\nprevious equation. And V is the lambda.",
    "start": "2362030",
    "end": "2368240"
  },
  {
    "text": "Sorry, they didn't\nhave phi and lambda, and they're not options.",
    "start": "2368240",
    "end": "2373670"
  },
  {
    "text": "For variable names,\nI used F and V. And you can see that F, V, F\ntranspose is just equal to A.",
    "start": "2373670",
    "end": "2385990"
  },
  {
    "text": "Any questions about that? No? All right, so let's\nturn to how do",
    "start": "2385990",
    "end": "2393930"
  },
  {
    "text": "you use eigenvectors and\neigenvalues to describe data. ",
    "start": "2393930",
    "end": "2401730"
  },
  {
    "text": "So I'm going to briefly\nreview the notion of variance, what that means in\nhigher dimensions,",
    "start": "2401730",
    "end": "2408270"
  },
  {
    "text": "and how you use a covariance\nmatrix to describe data in high dimensions.",
    "start": "2408270",
    "end": "2414870"
  },
  {
    "text": "So let's say that we have\na bunch of observations of a variable x-- so this is now just a scaler.",
    "start": "2414870",
    "end": "2422760"
  },
  {
    "text": "So, we have m\ndifferent observations, x superscript j is the j-th\nobservation of that data.",
    "start": "2422760",
    "end": "2431160"
  },
  {
    "text": "And you can see that if you make\na bunch of measurements of most things in the world,\nyou'll find a distribution",
    "start": "2431160",
    "end": "2438480"
  },
  {
    "text": "of those measurements. Often, they will be\ndistributed in a bump.",
    "start": "2438480",
    "end": "2445605"
  },
  {
    "text": " You can write down the\nmean of that distribution",
    "start": "2445605",
    "end": "2452790"
  },
  {
    "text": "just as the average value\noverall distributions by summing together\nall those distributions",
    "start": "2452790",
    "end": "2458490"
  },
  {
    "text": "and dividing by the\nnumber of observations. You can also write down the\nvariance of that distribution",
    "start": "2458490",
    "end": "2466320"
  },
  {
    "text": "by subtracting the mean from\nall of those observations, squaring that difference\nfrom the mean,",
    "start": "2466320",
    "end": "2474330"
  },
  {
    "text": "summing up over\nall observations, and dividing by m. ",
    "start": "2474330",
    "end": "2482580"
  },
  {
    "text": "So let's say that we now have\nm different observations of two",
    "start": "2482580",
    "end": "2488940"
  },
  {
    "text": "variables, pressure\nand temperature. ",
    "start": "2488940",
    "end": "2496540"
  },
  {
    "text": "We have a distribution\nof those quantities.",
    "start": "2496540",
    "end": "2502860"
  },
  {
    "text": "We can describe that observation\nof x1 and x2 as a vector.",
    "start": "2502860",
    "end": "2509970"
  },
  {
    "text": "And we have m different\nobservations of that vector. You can write down the mean\nand variance of x1 and x2.",
    "start": "2509970",
    "end": "2518760"
  },
  {
    "text": "So for x1, we can write\ndown the mean as mu1. We can write down\nthe variance of x1.",
    "start": "2518760",
    "end": "2525330"
  },
  {
    "text": "We can write down the\nmean and variance of x2, of the x2 observation.",
    "start": "2525330",
    "end": "2531290"
  },
  {
    "text": " And sometimes,\nthat will give you a pretty good description\nof this two-dimensional",
    "start": "2531290",
    "end": "2540630"
  },
  {
    "text": "observation. But sometimes, it won't.",
    "start": "2540630",
    "end": "2546420"
  },
  {
    "text": "So in many cases, those\nvariables, x1 and x2,",
    "start": "2546420",
    "end": "2551569"
  },
  {
    "text": "are not correlated\nwith each other. They're independent variables. In many cases, though, x1 and\nx2 are dependent on each other.",
    "start": "2551570",
    "end": "2562120"
  },
  {
    "text": "The observations of x1 and x2\nare correlated with each other, so that if x1 is big,\nx2 also tends to be big.",
    "start": "2562120",
    "end": "2569320"
  },
  {
    "text": " In these two cases, x1 can\nhave the same variance.",
    "start": "2569320",
    "end": "2576000"
  },
  {
    "text": " x2 can have the same variance.",
    "start": "2576000",
    "end": "2582600"
  },
  {
    "text": "But there's clearly\nsomething different here. So we need something\nmore than just describing",
    "start": "2582600",
    "end": "2588270"
  },
  {
    "text": "the variance of x1 and x2\nto describe these data. And that thing is\nthe covariance.",
    "start": "2588270",
    "end": "2596680"
  },
  {
    "text": "It just says how do\nx1 and x2 covary? If x1 is big, does x2\nalso tend to be big?",
    "start": "2596680",
    "end": "2605730"
  },
  {
    "text": "In this case, the\ncovariance is zero. In this case, the\ncovariance is positive.",
    "start": "2605730",
    "end": "2611040"
  },
  {
    "text": "So we're taking if a\nfluctuation of x1 above the mean is associated with a fluctuation\nof x2 above the mean,",
    "start": "2611040",
    "end": "2618840"
  },
  {
    "text": "then these points will produce\na positive contribution to the covariance. And these points here will also\nproduce a positive contribution",
    "start": "2618840",
    "end": "2625920"
  },
  {
    "text": "to the covariance. And the covariance here will be\nsome number greater than zero.",
    "start": "2625920",
    "end": "2633000"
  },
  {
    "text": "That's closely related\nto the correlation, just the Pearson correlation\ncoefficient, which is the covariance divided\nby the geometric mean",
    "start": "2633000",
    "end": "2641940"
  },
  {
    "text": "of the individual variances. ",
    "start": "2641940",
    "end": "2647480"
  },
  {
    "text": "I'm assuming most of you have\nseen this many times, but just to get us up to speed.",
    "start": "2647480",
    "end": "2654619"
  },
  {
    "text": "So if you have data, a\nbunch of observations,",
    "start": "2654620",
    "end": "2660720"
  },
  {
    "text": "you can very easily fit\nthose data to a Gaussian. And you do that simply by\nmeasuring the mean and variance",
    "start": "2660720",
    "end": "2670319"
  },
  {
    "text": "of your data. And that turns out to be\nthe best fit to a Gaussian.",
    "start": "2670320",
    "end": "2676859"
  },
  {
    "text": "So if you have a bunch of\nobservations in one dimension, you measure the mean and\nvariance of that set of data.",
    "start": "2676860",
    "end": "2683880"
  },
  {
    "text": "That turns out to be a best\nfit in the least squared sense to a Gaussian probability\ndistribution defined",
    "start": "2683880",
    "end": "2693850"
  },
  {
    "text": "by a mean and a variance. ",
    "start": "2693850",
    "end": "2698880"
  },
  {
    "text": "So this is easy\nin one dimension. ",
    "start": "2698880",
    "end": "2707860"
  },
  {
    "text": "What we're interested in\ndoing is understanding data in higher dimensions. So how do we describe\ndata in higher dimensions?",
    "start": "2707860",
    "end": "2715850"
  },
  {
    "text": "How do we describe a Gaussian\nin higher dimensions? So that's what we're\ngoing to turn to now.",
    "start": "2715850",
    "end": "2723228"
  },
  {
    "text": "And the reason we're\ngoing to do this is not because every\ntime we have data, we're really trying to\nfit a Gaussian into it.",
    "start": "2723228",
    "end": "2731830"
  },
  {
    "text": "It's just that it's a powerful\nway of thinking about data,",
    "start": "2731830",
    "end": "2739150"
  },
  {
    "text": "of describing data\nin terms of variances in different directions.",
    "start": "2739150",
    "end": "2745880"
  },
  {
    "text": "And so we often think\nabout what we're doing when we are looking\nat high-dimensional data",
    "start": "2745880",
    "end": "2750970"
  },
  {
    "text": "is understanding\nits distribution in different dimensions as\nkind of a Gaussian cloud",
    "start": "2750970",
    "end": "2758470"
  },
  {
    "text": "that optimally best fits the\ndata that we're looking at. And mostly because\nit just gives us",
    "start": "2758470",
    "end": "2764770"
  },
  {
    "text": "an intuitive about how to\nbest represent or think about data in high dimensions.",
    "start": "2764770",
    "end": "2772930"
  },
  {
    "text": "So we're going to get\ninsights into how to think about high-dimensional data. We're going to develop that\ndescription using the vector",
    "start": "2772930",
    "end": "2780340"
  },
  {
    "text": "and matrix notation that\nwe've been developing all along because\nvectors and matrices",
    "start": "2780340",
    "end": "2787095"
  },
  {
    "text": "provide a natural\nway of manipulating data sets, of doing\ntransformations of basis,",
    "start": "2787095",
    "end": "2794250"
  },
  {
    "text": "rotations, so on. It's very compact. And those manipulations\nare really",
    "start": "2794250",
    "end": "2799590"
  },
  {
    "text": "trivial in MATLAB or Python.",
    "start": "2799590",
    "end": "2805900"
  },
  {
    "text": "So let's build up a Gaussian\ndistribution in two dimensions.",
    "start": "2805900",
    "end": "2811109"
  },
  {
    "text": "So we have, again, our Gaussian\nrandom variables, x1 and x2.",
    "start": "2811110",
    "end": "2816880"
  },
  {
    "text": "We have a Gaussian distribution,\nwhere the probability distribution is proportional\nto e to the minus 1/2",
    "start": "2816880",
    "end": "2822655"
  },
  {
    "text": "of x1 squared.  We have probability\ndistribution for x2--",
    "start": "2822655",
    "end": "2830140"
  },
  {
    "text": "again, probability of x2.  We can write down\nthe probability",
    "start": "2830140",
    "end": "2835900"
  },
  {
    "text": "of x1 and x2, the joint\nprobability distribution, assuming these are independent.",
    "start": "2835900",
    "end": "2842510"
  },
  {
    "text": "We can write that as\nthe product of p-- the product of the two\nprobability distributions p",
    "start": "2842510",
    "end": "2848530"
  },
  {
    "text": "of x1 and p of x2. And we have some Gaussian cloud,\nsome Gaussian distribution",
    "start": "2848530",
    "end": "2856359"
  },
  {
    "text": "in two dimensions that we\ncan write down like this. That's simply the product. So the product of\nthese two distributions",
    "start": "2856360",
    "end": "2863500"
  },
  {
    "text": "is e to the minus\n1/2 x1 squared times e to the minus 1/2 x2 squared.",
    "start": "2863500",
    "end": "2870160"
  },
  {
    "text": "And then, there's\na constant in front that just normalizes, so that\nthe total area under that curve",
    "start": "2870160",
    "end": "2875260"
  },
  {
    "text": "is just 1.  We can write this as\ne to the minus 1/2 x1",
    "start": "2875260",
    "end": "2882620"
  },
  {
    "text": "squared plus x2 squared. And that's e to the minus\n1/2 times some distance",
    "start": "2882620",
    "end": "2890970"
  },
  {
    "text": "from the origin. So it falls off exponentially\nin a way that depends only",
    "start": "2890970",
    "end": "2898049"
  },
  {
    "text": "on the distance from the\norigin or from the mean",
    "start": "2898050",
    "end": "2904110"
  },
  {
    "text": "of the distribution. In this case, we set\nthe mean to be zero.",
    "start": "2904110",
    "end": "2909200"
  },
  {
    "text": "Now, we can write that distance\nsquared using vector notation.",
    "start": "2909200",
    "end": "2916380"
  },
  {
    "text": "It's just the square\nmagnitude of that vector x. So if we have a vector x\nsitting out here somewhere,",
    "start": "2916380",
    "end": "2922069"
  },
  {
    "text": "we can measure the distance\nfrom the center of the Gaussian as the square magnitude of\nx, which is just x dot x,",
    "start": "2922070",
    "end": "2930350"
  },
  {
    "text": "or x transpose x. ",
    "start": "2930350",
    "end": "2936339"
  },
  {
    "text": "So we're going to\nuse this notation to find the distance of a vector\nfrom the center of the Gaussian",
    "start": "2936340",
    "end": "2944200"
  },
  {
    "text": "distribution. So you're going to see a\nlot of x [INAUDIBLE] axis. ",
    "start": "2944200",
    "end": "2950760"
  },
  {
    "text": "So this distribution\nthat we just built is called an isotropic\nmultivariate Gaussian",
    "start": "2950760",
    "end": "2955920"
  },
  {
    "text": "distribution.  And that distance d is called\nthe Mahalanobis distance,",
    "start": "2955920",
    "end": "2965940"
  },
  {
    "text": "which I'm going to say\nas little as possible.  So that distribution now\ndescribes how these points--",
    "start": "2965940",
    "end": "2978589"
  },
  {
    "text": "the probability of finding\nthese different points drawn from that distribution\nas a function of their position",
    "start": "2978590",
    "end": "2985280"
  },
  {
    "text": "in this space. So you're going to\ndraw a lot of points here in the middle\nand fewer points",
    "start": "2985280",
    "end": "2991640"
  },
  {
    "text": "as you go away at\nlarger distances. So this particular\ndistribution that I made here",
    "start": "2991640",
    "end": "3001800"
  },
  {
    "text": "has one more word in it. It's an isotopic multivariate\nGaussian distribution",
    "start": "3001800",
    "end": "3007280"
  },
  {
    "text": "of unit variance. And what we're\ngoing to do now is we're going to build up all\npossible Gaussian distributions",
    "start": "3007280",
    "end": "3017569"
  },
  {
    "text": "from this distribution by simply\ndoing matrix transformations. ",
    "start": "3017570",
    "end": "3025040"
  },
  {
    "text": "So we're going to start by\ntaking that unit variance Gaussian distribution and\nbuild an isotopic Gaussian",
    "start": "3025040",
    "end": "3033440"
  },
  {
    "text": "distribution that has\nan arbitrary variance-- that means an arbitrary width.",
    "start": "3033440",
    "end": "3039640"
  },
  {
    "text": "We're then going to build a\nGaussian distribution that can be stretched arbitrarily\nalong these two axes, y1",
    "start": "3039640",
    "end": "3052900"
  },
  {
    "text": "and y2. And we're going to do that\nby using a transformation",
    "start": "3052900",
    "end": "3059620"
  },
  {
    "text": "with a diagonal matrix. And then, what we're going to do\nis build an arbitrary Gaussian",
    "start": "3059620",
    "end": "3067480"
  },
  {
    "text": "distribution that can\nbe stretched and worked in any direction by using a\ntransformation matrix called",
    "start": "3067480",
    "end": "3078740"
  },
  {
    "text": "a covariance matrix,\nwhich just tells you how that distribution\nis stretched",
    "start": "3078740",
    "end": "3084540"
  },
  {
    "text": "in different directions. So we can stretch it in\nany direction we want.",
    "start": "3084540",
    "end": "3089700"
  },
  {
    "text": "Yes. AUDIENCE: Why is [INAUDIBLE]? ",
    "start": "3089700",
    "end": "3094790"
  },
  {
    "text": "MICHALE FEE: OK,\nthe distance squared is the square of magnitude. And the square of magnitude\nis x dot x, the dot product.",
    "start": "3094790",
    "end": "3105640"
  },
  {
    "text": "But remember, we can write\ndown the dot product in matrix notation as x transpose x.",
    "start": "3105640",
    "end": "3111470"
  },
  {
    "text": "So if we have row vector\ntimes a column vector,",
    "start": "3111470",
    "end": "3117900"
  },
  {
    "text": "you get the dot product. Yes, Lina. AUDIENCE: What does\nisotropic mean?",
    "start": "3117900",
    "end": "3123458"
  },
  {
    "text": "MICHALE FEE: OK, isotropic\njust means the same in all directions. Sorry, I should\nhave defined that.",
    "start": "3123458",
    "end": "3129540"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nwhen you stretched it, it's not isotropic? MICHALE FEE: Yes, these are\nnon-isotropic distributions",
    "start": "3129540",
    "end": "3138000"
  },
  {
    "text": "because they're different. They have different variances\nin different directions.",
    "start": "3138000",
    "end": "3143020"
  },
  {
    "text": "So you can see that this\nhas a large variance in the y1 direction and a small\nvariance in the y2 direction.",
    "start": "3143020",
    "end": "3149130"
  },
  {
    "text": "So it's non-isotropic.  Yes, [INAUDIBLE]. AUDIENCE: Why do\nyou [INAUDIBLE]??",
    "start": "3149130",
    "end": "3156230"
  },
  {
    "text": "MICHALE FEE: Right here. OK, think about this. Variance, you put into\nthis Gaussian distribution",
    "start": "3156230",
    "end": "3164240"
  },
  {
    "text": "as the distance squared\nover the variance squared. It's distance squared\nover a variance, which",
    "start": "3164240",
    "end": "3171830"
  },
  {
    "text": "is sigma squared. Here it's distance\nsquared over a variance.",
    "start": "3171830",
    "end": "3176870"
  },
  {
    "text": "Here it's distance\nsquared over a variance. Does that makes sense?",
    "start": "3176870",
    "end": "3182880"
  },
  {
    "text": "It's just that in\norder to describe these complex stretching and\nrotation of this Gaussian",
    "start": "3182880",
    "end": "3190650"
  },
  {
    "text": "distribution in\nhigh-dimensional space, we need a matrix to do that. ",
    "start": "3190650",
    "end": "3198000"
  },
  {
    "text": "And that covariance matrix\ndescribes the variances in the different direction\nand essentially the rotation.",
    "start": "3198000",
    "end": "3207390"
  },
  {
    "text": "Remember, this distribution\nhere is just a distribution that's stretched and rotated.",
    "start": "3207390",
    "end": "3214170"
  },
  {
    "text": "Well, we learned how to build\nexactly such a transformation",
    "start": "3214170",
    "end": "3219359"
  },
  {
    "text": "by taking the product of\nphi transpose lambda phi.",
    "start": "3219360",
    "end": "3224370"
  },
  {
    "text": "So we're going to use this to\nbuild these arbitrary Gaussian",
    "start": "3224370",
    "end": "3229890"
  },
  {
    "text": "distributions.  OK, so I'll just go\nthrough this quickly.",
    "start": "3229890",
    "end": "3235930"
  },
  {
    "text": "If we have an isotopic unit\nvariance Gaussian distribution",
    "start": "3235930",
    "end": "3244520"
  },
  {
    "text": "as a function of\nthis vector x, we can build a Gaussian\ndistribution",
    "start": "3244520",
    "end": "3249859"
  },
  {
    "text": "of arbitrary variance by\nwriting down a y that's simply sigma times x.",
    "start": "3249860",
    "end": "3256310"
  },
  {
    "text": "We're going to\ntransform x into y,",
    "start": "3256310",
    "end": "3262380"
  },
  {
    "text": "so that we can write\ndown a distribution that has an arbitrary variance.",
    "start": "3262380",
    "end": "3268410"
  },
  {
    "text": "Here this is variance 1. Here this is sigma squared.",
    "start": "3268410",
    "end": "3274020"
  },
  {
    "text": "So let's make just a change\nof variables y equals sigma x.",
    "start": "3274020",
    "end": "3280900"
  },
  {
    "text": "So now, what's the\nprobability distribution as a function of y? Well, there's\nprobability distribution",
    "start": "3280900",
    "end": "3286349"
  },
  {
    "text": "as a function of x. We're simply going to\nsubstitute y equals sigma x with x equals sigma inverse y.",
    "start": "3286350",
    "end": "3294240"
  },
  {
    "text": "We're going to substitute\nthis into here. The Mahalanobis\ndistance is just x",
    "start": "3294240",
    "end": "3299370"
  },
  {
    "text": "transpose x, which is just\nsigma inverse y transpose sigma inverse y.",
    "start": "3299370",
    "end": "3306420"
  },
  {
    "text": "And when you do that, you\nfind that the distance squared is just y transpose\nsigma to the minus 2y.",
    "start": "3306420",
    "end": "3314030"
  },
  {
    "text": " So there is our\nGaussian distribution",
    "start": "3314030",
    "end": "3321320"
  },
  {
    "text": "for this distribution. There's the expression for\nthis Gaussian distribution",
    "start": "3321320",
    "end": "3328010"
  },
  {
    "text": "with a variance sigma.  We can rewrite this\nin different ways.",
    "start": "3328010",
    "end": "3335030"
  },
  {
    "text": "Now, let's build a\nGaussian distribution that stretched arbitrarily in\ndifferent directions, x and y.",
    "start": "3335030",
    "end": "3345000"
  },
  {
    "text": "We're going to do\nthe same trick. We're simply going to make\na transformation y equals",
    "start": "3345000",
    "end": "3350520"
  },
  {
    "text": "matrix, diagonal matrix, s\ntimes x and substitute this",
    "start": "3350520",
    "end": "3358650"
  },
  {
    "text": "into our expression\nfor a Gaussian. So x equals s inverse y.",
    "start": "3358650",
    "end": "3365530"
  },
  {
    "text": "The Mahalanobis distance\nis given by x transpose x, which we can just get down here.",
    "start": "3365530",
    "end": "3371309"
  },
  {
    "text": "Let's do that with\nthis substitution. ",
    "start": "3371310",
    "end": "3376800"
  },
  {
    "text": "And we get an s squared\nhere, s inverse squared, which we're just going to\nwrite as lambda inverse.",
    "start": "3376800",
    "end": "3383875"
  },
  {
    "start": "3383875",
    "end": "3390170"
  },
  {
    "text": "And you can see that\nyou have these variances along the diagonal.",
    "start": "3390170",
    "end": "3395480"
  },
  {
    "text": "So if that's lambda\ninverse, then lambda is just a matrix of\nvariances along the diagonal.",
    "start": "3395480",
    "end": "3403970"
  },
  {
    "text": "So sigma 1 squared is the\nvariance in this direction.",
    "start": "3403970",
    "end": "3409040"
  },
  {
    "text": "Sigma 2 squared is the\nvariance in this direction. I'm just showing you how\nyou make a transformation",
    "start": "3409040",
    "end": "3417869"
  },
  {
    "text": "to this vector x\ninto another vector y to build up a representation\nof this effective distance",
    "start": "3417870",
    "end": "3427170"
  },
  {
    "text": "from the center of distribution\nfor different kinds of Gaussian distributions.",
    "start": "3427170",
    "end": "3432180"
  },
  {
    "text": " And now finally, let's\nbuild up an expression",
    "start": "3432180",
    "end": "3439900"
  },
  {
    "text": "for a Gaussian distribution\nwith arbitrary variance and covariance.",
    "start": "3439900",
    "end": "3445690"
  },
  {
    "text": "So we're going to\nmake a transformation of x into a new vector y using\nthis rotated stretch matrix.",
    "start": "3445690",
    "end": "3458140"
  },
  {
    "text": " We're going to substitute this\nin, calculate the Mahalanobis",
    "start": "3458140",
    "end": "3466600"
  },
  {
    "text": "distance-- is now x transpose x. Substitute this and solve\nfor the Mahalanobis distance.",
    "start": "3466600",
    "end": "3476340"
  },
  {
    "text": "And what you find is\nthat distance squared is just y transpose phi lambda\ninverse phi transpose times y.",
    "start": "3476340",
    "end": "3485559"
  },
  {
    "text": "And we just write that as y\ntranspose sigma inverse y. ",
    "start": "3485560",
    "end": "3494570"
  },
  {
    "text": "So that is now an expression\nfor an arbitrary Gaussian",
    "start": "3494570",
    "end": "3499640"
  },
  {
    "text": "distribution in\nhigh-dimensional space. ",
    "start": "3499640",
    "end": "3506089"
  },
  {
    "text": "And that distribution is\ndefined by this matrix of variances and covariances.",
    "start": "3506090",
    "end": "3515840"
  },
  {
    "text": "Again, I'm just writing down\nthe definition of sigma inverse here. We can take the inverse\nof that, and we see that",
    "start": "3515840",
    "end": "3524860"
  },
  {
    "text": "our covariance-- this is\ncalled a covariance matrix-- it describes the\nvariance and correlations",
    "start": "3524860",
    "end": "3534480"
  },
  {
    "text": "of those different\ndimensions as a matrix.",
    "start": "3534480",
    "end": "3540810"
  },
  {
    "text": "That's just this\nrotated stretch matrix that we been working with.",
    "start": "3540810",
    "end": "3548890"
  },
  {
    "text": "And that's just the same\nas this covariance matrix",
    "start": "3548890",
    "end": "3555809"
  },
  {
    "text": "that we described\nfor distribution.",
    "start": "3555810",
    "end": "3562385"
  },
  {
    "text": "I feel like all that didn't\ncome out quite as clearly as I'd hoped. But let me just\nsummarize for you.",
    "start": "3562385",
    "end": "3569980"
  },
  {
    "text": "So we started with an isotopic\nGaussian of unit variance.",
    "start": "3569980",
    "end": "3576800"
  },
  {
    "text": "And we multiplied that vector,\nwe transformed that vector x, by multiplying it by sigma\nso that we could write down",
    "start": "3576800",
    "end": "3585710"
  },
  {
    "text": "a Gaussian distribution\nof arbitrary variance. We transformed that vector\nx with a diagonal covariance",
    "start": "3585710",
    "end": "3594560"
  },
  {
    "text": "matrix to get arbitrary\nstretches along the axes.",
    "start": "3594560",
    "end": "3601010"
  },
  {
    "text": "And then, we made another\nkind of transformation with an arbitrary stretch\nand rotation matrix",
    "start": "3601010",
    "end": "3608680"
  },
  {
    "text": "so that we can now write down\na Gaussian distribution that has arbitrary stretch and\nrotation of its variances",
    "start": "3608680",
    "end": "3616000"
  },
  {
    "text": "in different directions. So this is the punch\nline right here--",
    "start": "3616000",
    "end": "3624430"
  },
  {
    "text": "that you can write down\nthe Gaussian distribution with arbitrary\nvariances in this form.",
    "start": "3624430",
    "end": "3636080"
  },
  {
    "text": "And that sigma right there\nis just the covariance matrix",
    "start": "3636080",
    "end": "3641320"
  },
  {
    "text": "that describes how wide\nthat distribution is in the different directions\nand how correlated",
    "start": "3641320",
    "end": "3647859"
  },
  {
    "text": "those different directions are. ",
    "start": "3647860",
    "end": "3654779"
  },
  {
    "text": "I think this just summarizes\nwhat I've already said. ",
    "start": "3654780",
    "end": "3661940"
  },
  {
    "text": "So now, let's compute the\ncovariance matrix from data. So now, I've shown\nyou how to represent",
    "start": "3661940",
    "end": "3669710"
  },
  {
    "text": "Gaussians in high\ndimensions that have these arbitrary variances.",
    "start": "3669710",
    "end": "3675390"
  },
  {
    "text": "Now, let's say that I\nactually have some data. How do I fit one of\nthese Gaussians to it?",
    "start": "3675390",
    "end": "3682330"
  },
  {
    "text": "And it turns out that\nit's really simple. It's just a matter\nof calculating",
    "start": "3682330",
    "end": "3687520"
  },
  {
    "text": "this covariance matrix. So let's do that.",
    "start": "3687520",
    "end": "3692630"
  },
  {
    "text": "So here is some\nhigh-dimensional data.",
    "start": "3692630",
    "end": "3699099"
  },
  {
    "text": "Remember that to fit a\nGaussian to a bunch of data,",
    "start": "3699100",
    "end": "3704190"
  },
  {
    "text": "all we need to do\nis to find the mean and variants in one dimension.",
    "start": "3704190",
    "end": "3709740"
  },
  {
    "text": "For higher dimensions,\nwe just need to find the mean and\nthe covariance matrix.",
    "start": "3709740",
    "end": "3717060"
  },
  {
    "text": "So that's simple. So here's our set\nof observations. Now, instead of being\nscalars, they're vectors.",
    "start": "3717060",
    "end": "3725070"
  },
  {
    "text": "First thing we do is\nsubtract the mean. So we calculate\nthe mean by summing all of those observations,\ndividing those numbers,",
    "start": "3725070",
    "end": "3733200"
  },
  {
    "text": "divide by m. So there we find the mean. We compute a new data set\nwith the mean subtracted.",
    "start": "3733200",
    "end": "3742560"
  },
  {
    "text": "So from every one of\nthese observations, we subtract the mean.",
    "start": "3742560",
    "end": "3747630"
  },
  {
    "text": "And we're going to call that z. ",
    "start": "3747630",
    "end": "3753580"
  },
  {
    "text": "So there is our mean\nsubtracted here. I've subtracted the mean. So those are the x's.",
    "start": "3753580",
    "end": "3760210"
  },
  {
    "text": "Subtract the mean. Those are now our z's,\nour mean-subtracted data. ",
    "start": "3760210",
    "end": "3767556"
  },
  {
    "text": "Does that makes sense? Now, we're going to calculate\nthis covariance matrix.",
    "start": "3767556",
    "end": "3773650"
  },
  {
    "text": "Well, all we do is\nwe find the variance in each direction\nand the covariances.",
    "start": "3773650",
    "end": "3782930"
  },
  {
    "text": "So it's going to be a matrix\nin low-dimensional data. It's a two-by-two matrix.",
    "start": "3782930",
    "end": "3790609"
  },
  {
    "text": "So we're going to find the\nvariance in the z1 direction. It's just z1 times z1, summed\nover all the observations,",
    "start": "3790610",
    "end": "3799060"
  },
  {
    "text": "divided by m.  Th variance in the z2\ndirection is just the sum",
    "start": "3799060",
    "end": "3807560"
  },
  {
    "text": "of z2, j, z2, j divided by m. The covariance is\njust the cross terms,",
    "start": "3807560",
    "end": "3815569"
  },
  {
    "text": "z1 one times z2 and z2 times z1. Of course, those are\nequal to each other.",
    "start": "3815570",
    "end": "3822000"
  },
  {
    "text": "So in a covariance\nmatrix, it's symmetric.",
    "start": "3822000",
    "end": "3827410"
  },
  {
    "text": "So how do we calculate this? It turns out that in MATLAB,\nthis is super-duper easy.",
    "start": "3827410",
    "end": "3833350"
  },
  {
    "text": " So if this is our vector,\nthat's our vector, one",
    "start": "3833350",
    "end": "3840539"
  },
  {
    "text": "of our observations, we can\ncompute the inner product",
    "start": "3840540",
    "end": "3847170"
  },
  {
    "text": "z transpose z. So the inner product\nis just z transpose z, which is z1, z2, z1, z2.",
    "start": "3847170",
    "end": "3854369"
  },
  {
    "text": "That's the square\nmagnitude of z. There's another kind of product\ncalled the outer product.",
    "start": "3854370",
    "end": "3864070"
  },
  {
    "text": "Remember that. So the outer product\nlooks like this.",
    "start": "3864070",
    "end": "3869640"
  },
  {
    "text": "This is a 1 by 2. That's a rho vector\ntimes a column vector is equal to a scalar.",
    "start": "3869640",
    "end": "3876060"
  },
  {
    "text": "1 by 2 times 2 by 1 equals by\n1 by 1-- two rows, one column--",
    "start": "3876060",
    "end": "3881700"
  },
  {
    "text": "times 1 by 2, gives you a 2 by\n2 matrix that looks like this.",
    "start": "3881700",
    "end": "3889470"
  },
  {
    "text": "z1 times z1, z1,\nz2, z1, z2, z2, z2. Why?",
    "start": "3889470",
    "end": "3894630"
  },
  {
    "text": "It's z1 times z1 equals that. z1 times z2, z2 z1, one z2 z2.",
    "start": "3894630",
    "end": "3907050"
  },
  {
    "text": "So that outer product\nalready gives us the components to compute\nthe correlation matrix.",
    "start": "3907050",
    "end": "3916890"
  },
  {
    "text": "So what we do is we\njust take this vector, z the j-th observation\nof this vector z,",
    "start": "3916890",
    "end": "3925320"
  },
  {
    "text": "and multiply it by the j-th\nobservation of this vector z transpose.",
    "start": "3925320",
    "end": "3930690"
  },
  {
    "text": "And that gives us this matrix. And we sum over all this.",
    "start": "3930690",
    "end": "3938250"
  },
  {
    "text": "And you see that is exactly\nthe covariance matrix. ",
    "start": "3938250",
    "end": "3948450"
  },
  {
    "text": "So if we have m\nobservations of vector z,",
    "start": "3948450",
    "end": "3955079"
  },
  {
    "text": "we put them in matrix form. So we have a big,\nlong data matrix.",
    "start": "3955080",
    "end": "3960450"
  },
  {
    "text": "Like this. There are m observations of\nthis two-dimensional vector z.",
    "start": "3960450",
    "end": "3966510"
  },
  {
    "text": " The data dimension, the data\nvector has, mentioned 2.",
    "start": "3966510",
    "end": "3974560"
  },
  {
    "text": "Their are m observations. So m is the number of samples. So this is an n-by-m matrix.",
    "start": "3974560",
    "end": "3981485"
  },
  {
    "text": " So if you want to compute\nthe covariance matrix,",
    "start": "3981485",
    "end": "3987690"
  },
  {
    "text": "you just in MATLAB,\nliterally do z. This big matrix z times\nthat matrix transpose.",
    "start": "3987690",
    "end": "3996850"
  },
  {
    "text": "And that automatically finds\nthe covariance matrix for you in one line of MATLAB.",
    "start": "3996850",
    "end": "4002920"
  },
  {
    "text": " There's a little trick to\nsubtract the mean easily.",
    "start": "4002920",
    "end": "4009480"
  },
  {
    "text": "So remember, your original\nobservations are x. You compute the mean\nacross the rows.",
    "start": "4009480",
    "end": "4017510"
  },
  {
    "text": "Thus, you're going you're going\nto sum across columns to give",
    "start": "4017510",
    "end": "4022880"
  },
  {
    "text": "you a mean for each row. That gives you a mean of that\nfirst component of your vector,",
    "start": "4022880",
    "end": "4030530"
  },
  {
    "text": "mean of the second component. That's really easy in the lab. mu is the mean of x summing\ncross the second component.",
    "start": "4030530",
    "end": "4043490"
  },
  {
    "text": "That gives you a\nmean vector and then you use repmat to fill that\nmean out in all of the columns",
    "start": "4043490",
    "end": "4050030"
  },
  {
    "text": "and [INAUDIBLE] subtract\nthis mean from x to get this data matrix z. ",
    "start": "4050030",
    "end": "4058590"
  },
  {
    "text": "So now, let's apply those\ntools to actually do some principal\ncomponents analysis.",
    "start": "4058590",
    "end": "4064200"
  },
  {
    "text": " So principal components\nanalysis is really amazing.",
    "start": "4064200",
    "end": "4071150"
  },
  {
    "text": "If you look at single nucleotide\npolymorphisms and populations of people, there are\nlike hundreds of genes",
    "start": "4071150",
    "end": "4078860"
  },
  {
    "text": "that you can look at. You can look at different\nvariations of a gene",
    "start": "4078860",
    "end": "4085220"
  },
  {
    "text": "across hundreds of genes. But it's this enormous data set. And you can find\nout which directions",
    "start": "4085220",
    "end": "4091940"
  },
  {
    "text": "in that space of genes\ngive you information about the genome of people.",
    "start": "4091940",
    "end": "4097229"
  },
  {
    "text": "And for example, if you\nlook at a number of genes across people with\ndifferent backgrounds,",
    "start": "4097229",
    "end": "4103640"
  },
  {
    "text": "you can see that they're\nactually clusters corresponding to people with\ndifferent backgrounds.",
    "start": "4103640",
    "end": "4109700"
  },
  {
    "text": "You can do\nsingle-cell profiling. So you can do the same thing in\ndifferent cells with a tissue.",
    "start": "4109700",
    "end": "4115790"
  },
  {
    "text": "So you look at RNA\ntranscriptional profiling. You see what are the\ngenes that are being",
    "start": "4115790",
    "end": "4124460"
  },
  {
    "text": "expressed in individual cells. You can do principal\ncomponents analysis of those different\ngenes and find",
    "start": "4124460",
    "end": "4130460"
  },
  {
    "text": "clusters for different\ncell types within a tissue. This is now being applied very\ncommonly in brain tissue now",
    "start": "4130460",
    "end": "4140028"
  },
  {
    "text": "to extract different cell types. You can use images and find out\nwhich components of an image",
    "start": "4140029",
    "end": "4147460"
  },
  {
    "text": "actually give you information\nabout different faces. So you can find a bunch\nof different faces,",
    "start": "4147460",
    "end": "4156130"
  },
  {
    "text": "find the covariance\nmatrix of those images, take that, do eigendecomposition\non that covariance matrix.",
    "start": "4156130",
    "end": "4166439"
  },
  {
    "text": "And extract what are\ncalled eigenfaces. These are dimensions on which\nthe images carry information",
    "start": "4166439",
    "end": "4174028"
  },
  {
    "text": "about face identity. You can use principal\ncomponents analysis",
    "start": "4174029",
    "end": "4180359"
  },
  {
    "text": "to decompose spike waveforms\ninto different spikes.",
    "start": "4180359",
    "end": "4185460"
  },
  {
    "text": "This is a very common way\nof doing spike sorting. So when you stick an\nelectrode in the brain, you'd record from\ndifferent cells",
    "start": "4185460",
    "end": "4191580"
  },
  {
    "text": "at the end of the electrode. Each one of those has\na different way of form and you can use this method to\nextract the different waveforms",
    "start": "4191580",
    "end": "4199560"
  },
  {
    "text": "people have even\nrecently used this now to understand the\nlow-dimensional trajectories",
    "start": "4199560",
    "end": "4207060"
  },
  {
    "text": "of movements. So if you take a movie-- SPEAKER: After tracking,\na reconstruction",
    "start": "4207060",
    "end": "4214092"
  },
  {
    "text": "of the global trajectory can\nbe made from the stepper motor movements, while the local\nshape changes of the worm",
    "start": "4214092",
    "end": "4219780"
  },
  {
    "text": "can be seen in detail. ",
    "start": "4219780",
    "end": "4224930"
  },
  {
    "text": "MICHALE FEE: OK, so here\nyou see a c elegans, a worm moving along.",
    "start": "4224930",
    "end": "4230130"
  },
  {
    "text": "This is an image, so it's\na very high-dimensional. There are 1,000\npixels in this image.",
    "start": "4230130",
    "end": "4236640"
  },
  {
    "text": "And you can decompose that\nimage into a trajectory",
    "start": "4236640",
    "end": "4246030"
  },
  {
    "text": "in a low-dimensional space. And it's been used to\ndescribe the movements",
    "start": "4246030",
    "end": "4252060"
  },
  {
    "text": "in a low-dimensional\nspace and relate to a representation\nof the neural activity",
    "start": "4252060",
    "end": "4259320"
  },
  {
    "text": "in low dimensions as well. OK, so it's a very\npowerful technique.",
    "start": "4259320",
    "end": "4265520"
  },
  {
    "text": "So let me just first demonstrate\nPCA on just some simple 2D data.",
    "start": "4265520",
    "end": "4271200"
  },
  {
    "text": "So here's a cloud\nof points given by a Gaussian distribution.",
    "start": "4271200",
    "end": "4277000"
  },
  {
    "text": "So those are a\nbunch of vectors x. We can transform those vectors\nx using phi s phi transpose",
    "start": "4277000",
    "end": "4283630"
  },
  {
    "text": "to produce a Gaussian, a cloud\nof points with a Gaussian",
    "start": "4283630",
    "end": "4289090"
  },
  {
    "text": "distribution, rotated\nat 45 degrees, and stretched by 1.7-ish along\none axis and compressed by that",
    "start": "4289090",
    "end": "4298330"
  },
  {
    "text": "amount along another axis. So we can build this rotation\nmatrix, this stretch matrix,",
    "start": "4298330",
    "end": "4306190"
  },
  {
    "text": "and build a\ntransformation matrix-- r, s, r transpose.",
    "start": "4306190",
    "end": "4311551"
  },
  {
    "text": "Multiply that by x. And that gives us\nthis data set here. OK, so we're going\nto take that data",
    "start": "4311551",
    "end": "4317070"
  },
  {
    "text": "set and do principal\ncomponents analysis on it. And what that's\ngoing to do is it's going to find the dimensions\nin this data set that",
    "start": "4317070",
    "end": "4327130"
  },
  {
    "text": "have the highest variance. It's basically going\nto extract the variance in the different dimensions.",
    "start": "4327130",
    "end": "4332600"
  },
  {
    "text": "So we take that set of points. We just compute the\ncovariance matrix",
    "start": "4332600",
    "end": "4337989"
  },
  {
    "text": "by taking z, z transpose,\ntimes 1 over m.",
    "start": "4337990",
    "end": "4343030"
  },
  {
    "text": "That computes that\ncovariance matrix. And then, we're going to use\nthe eig function in MATLAB",
    "start": "4343030",
    "end": "4348370"
  },
  {
    "text": "to extract the eigenvectors\nand eigenvalues of the covariance\nmatrix OK, so q--",
    "start": "4348370",
    "end": "4356945"
  },
  {
    "text": "we're going to call q\nis the variable name for the covariance matrix\nit's zz transpose over m.",
    "start": "4356945",
    "end": "4364550"
  },
  {
    "text": "Call eig of q.  That returns the\nrotation matrix.",
    "start": "4364550",
    "end": "4373880"
  },
  {
    "text": "And that rotation matrix,\nthe columns of which are the eigenvectors, it returns\nthe matrix of eigenvalues,",
    "start": "4373880",
    "end": "4382130"
  },
  {
    "text": "the diagonal elements\nare the eigenvalues. Sometimes, you need to\ndo a flip-left-right",
    "start": "4382130",
    "end": "4389480"
  },
  {
    "text": "because I sometimes return\nthe lowest eigenvalues first. But I generally want to plot put\nthe largest eigenvalue first.",
    "start": "4389480",
    "end": "4398570"
  },
  {
    "text": "So there's the largest one,\nthere's the smallest one. ",
    "start": "4398570",
    "end": "4403920"
  },
  {
    "text": "And now, what we do,\nis we simply rotate. We [AUDIO OUT] basis.",
    "start": "4403920",
    "end": "4410070"
  },
  {
    "text": "We can rotate this data\nset using the rotation matrix that the principal\ncomponents analysis found.",
    "start": "4410070",
    "end": "4421540"
  },
  {
    "text": "OK, so we compute the\ncovariance matrix. Find the eigenvectors\nand eigenvalues",
    "start": "4421540",
    "end": "4426910"
  },
  {
    "text": "of the covariance\nmatrix right there. And then, we just\nrotate the data",
    "start": "4426910",
    "end": "4433920"
  },
  {
    "text": "set into that new basis of\neigenvectors and eigenvalues.",
    "start": "4433920",
    "end": "4439469"
  },
  {
    "text": " It's useful for clustering. So if we have two clusters,\nwe can take the clusters,",
    "start": "4439470",
    "end": "4449320"
  },
  {
    "text": "compute the covariance matrix. Find the eigenvectors\nand eigenvalues of that covariance matrix.",
    "start": "4449320",
    "end": "4456810"
  },
  {
    "text": "And then, rotate the\ndata set into a basis set",
    "start": "4456810",
    "end": "4462140"
  },
  {
    "text": "in which the dimensions\nin the data on which",
    "start": "4462140",
    "end": "4467460"
  },
  {
    "text": "variances largest are along\nthe standard basis vectors.",
    "start": "4467460",
    "end": "4474935"
  },
  {
    "start": "4474935",
    "end": "4480900"
  },
  {
    "text": "Let's look at a problem\nin the time domain. So here we have a couple\nof time-dependent signals.",
    "start": "4480900",
    "end": "4488400"
  },
  {
    "text": "So this is some amplitude\nas a function of time.",
    "start": "4488400",
    "end": "4493530"
  },
  {
    "text": "These are signals\nthat I constructed. They're some wiggly function\nthat I added noise to.",
    "start": "4493530",
    "end": "4502240"
  },
  {
    "text": "What we do is we take each\none of those times series, and we stack them up\nin a bunch of columns.",
    "start": "4502240",
    "end": "4508410"
  },
  {
    "text": "So our vector is now a\nset of 100 time samples.",
    "start": "4508410",
    "end": "4515210"
  },
  {
    "text": "So there is a vector of\n100 different time points. Does that make sense?",
    "start": "4515210",
    "end": "4521630"
  },
  {
    "text": "And we have 200 observations of\nthose 100-dimensional vectors.",
    "start": "4521630",
    "end": "4528440"
  },
  {
    "text": "So we have a data vector\nx that has columns.",
    "start": "4528440",
    "end": "4534270"
  },
  {
    "text": "That are hundreds dimensional. And we have 200 of\nthose observations. So it's 100-by-200 matrix.",
    "start": "4534270",
    "end": "4540800"
  },
  {
    "text": "100-by-200 matrix. We do the means subtraction\nwe subtract the mean using",
    "start": "4540800",
    "end": "4547140"
  },
  {
    "text": "that trick that I showed you. Compute the covariance matrix.",
    "start": "4547140",
    "end": "4552710"
  },
  {
    "text": "So there we compute the mean. We subtract the\nmean using repmat. Subtract the mean from\nthe data to get z.",
    "start": "4552710",
    "end": "4560110"
  },
  {
    "text": "Compute the covariance\nmatrix Q. That's what the covariance matrix\nlooks like for those data.",
    "start": "4560110",
    "end": "4568610"
  },
  {
    "text": "And now, we plug it into eig\nto extract the eigenvectors",
    "start": "4568610",
    "end": "4574190"
  },
  {
    "text": "and eigenvalues. OK, so extract F and V. If\nwe look at the eigenvalues,",
    "start": "4574190",
    "end": "4583410"
  },
  {
    "text": "you can see that\nthere are hundreds eigenvalues because those\ndata have 100 dimensions.",
    "start": "4583410",
    "end": "4590050"
  },
  {
    "text": "So there are\nhundreds eigenvalues. You could see that two of\nthose eigenvalues are big,",
    "start": "4590050",
    "end": "4595860"
  },
  {
    "text": "and the rest are small. This is on a log scale. What that says is\nthat almost all",
    "start": "4595860",
    "end": "4604230"
  },
  {
    "text": "of the variance in these data\nexist in just two dimensions. It's 100-dimensional space.",
    "start": "4604230",
    "end": "4610650"
  },
  {
    "text": "But the data are living\nin two dimensions. And all the rest is noise.",
    "start": "4610650",
    "end": "4615809"
  },
  {
    "text": " Does that makes sense? ",
    "start": "4615810",
    "end": "4623410"
  },
  {
    "text": "So what you'll typically\ndo is take some data, compute the covariance\nmatrix, find the eigenvalues,",
    "start": "4623410",
    "end": "4630010"
  },
  {
    "text": "and look at the\nspectrum of eigenvalues. And you'll very\noften see that there",
    "start": "4630010",
    "end": "4635520"
  },
  {
    "text": "is a lot of variance in a\nsmall subset of eigenvalues. Then, it tells you that\nthe data are really",
    "start": "4635520",
    "end": "4642250"
  },
  {
    "text": "living in a\nlower-dimensional subspace",
    "start": "4642250",
    "end": "4647320"
  },
  {
    "text": "than the full\ndimensionality of the data. So that's where your signal is.",
    "start": "4647320",
    "end": "4652329"
  },
  {
    "text": "And all the rest\nof that is noise. You can plot the\ncumulative of this. And you can say\nthat the first two",
    "start": "4652330",
    "end": "4658119"
  },
  {
    "text": "components explain over 60% of\nthe total variance in the data.",
    "start": "4658120",
    "end": "4665080"
  },
  {
    "text": "So since there are\ntwo large eigenvalues, let's look at the eigenvectors\nassociated with those.",
    "start": "4665080",
    "end": "4670560"
  },
  {
    "text": "And we can find those. Those are just the first\ntwo columns of this matrix F",
    "start": "4670560",
    "end": "4676380"
  },
  {
    "text": "that the eig function\nreturned to us. And that's what those two\neigenvectors look like.",
    "start": "4676380",
    "end": "4682050"
  },
  {
    "text": " That's what the original\ndata looked like.",
    "start": "4682050",
    "end": "4687250"
  },
  {
    "text": "The eigenvectors, the\ncolumns of the F matrix, are an orthogonal basis set.",
    "start": "4687250",
    "end": "4693330"
  },
  {
    "text": "A new basis set. So those are the first\ntwo eigenvectors.",
    "start": "4693330",
    "end": "4701300"
  },
  {
    "text": "And you can see that\nthe signal lives in this low-dimensional space\nof these two eigenvectors.",
    "start": "4701300",
    "end": "4707320"
  },
  {
    "text": "All of the other\neigenvectors are just noise. ",
    "start": "4707320",
    "end": "4714330"
  },
  {
    "text": "So we can do is we can project\nthe data into this new basis",
    "start": "4714330",
    "end": "4722330"
  },
  {
    "text": "set. So let's do that. We simply do a change of basis.",
    "start": "4722330",
    "end": "4729370"
  },
  {
    "text": "The f is a rotation matrix. We can project our data\nz into this new basis set",
    "start": "4729370",
    "end": "4736420"
  },
  {
    "text": "and see what it looks like. Turns out, that's\nwhat it looks like. There are two clusters in\nthose data corresponding",
    "start": "4736420",
    "end": "4746210"
  },
  {
    "text": "to the two different wave forms\nthat you could see in the data. ",
    "start": "4746210",
    "end": "4752317"
  },
  {
    "text": "Right there, you can\nsee that there are kind of two wave forms in the data. ",
    "start": "4752317",
    "end": "4758470"
  },
  {
    "text": "If you projected data into\nthis low-dimensional space, you can see that there\nare two clusters there. ",
    "start": "4758470",
    "end": "4765940"
  },
  {
    "text": "If you projected data into other\nprojections, you don't see it.",
    "start": "4765940",
    "end": "4772690"
  },
  {
    "text": "It's only in this\nparticular projection that you have these two\nvery distinct clusters",
    "start": "4772690",
    "end": "4777965"
  },
  {
    "text": "corresponding to the\ntwo different wave forms in the data. Now, almost all\nof the variance is",
    "start": "4777965",
    "end": "4787050"
  },
  {
    "text": "in the space of the first\ntwo principal components. So what you can\nactually do is, you",
    "start": "4787050",
    "end": "4792060"
  },
  {
    "text": "can project the data into these\nfirst two principal components, set all of the other\nprincipal components to zero,",
    "start": "4792060",
    "end": "4800309"
  },
  {
    "text": "and then rotate back to\nthe original basis set. That is that you're setting\nas much of the noise",
    "start": "4800310",
    "end": "4806490"
  },
  {
    "text": "to zero as you can. You're getting rid\nof most of the noise. And then, when you rotate back\nto the original basis set,",
    "start": "4806490",
    "end": "4814050"
  },
  {
    "text": "you've gotten rid of\nmost of the noise. And that's called principal\ncomponents filtering.",
    "start": "4814050",
    "end": "4819090"
  },
  {
    "text": "So here's before filtering\nand here's after filtering. OK, so youve found the\nlow-dimensional space,",
    "start": "4819090",
    "end": "4827400"
  },
  {
    "text": "in which all the data sits,\nin which the signal sits, everything outside of\nthat space is noise.",
    "start": "4827400",
    "end": "4834429"
  },
  {
    "text": "So you rotate the data\ninto a new basis set.",
    "start": "4834430",
    "end": "4840030"
  },
  {
    "text": "You can filter out all\nthe other dimensions that just have noise. You filter back.",
    "start": "4840030",
    "end": "4845880"
  },
  {
    "text": "And you just keep the signal. And that's it. So that's sort of a brief\nintro to principal component",
    "start": "4845880",
    "end": "4853400"
  },
  {
    "text": "analysis. But there are a lot of\nthings you can use it for. It's a lot of fun. And it's a great\nintro to all the other",
    "start": "4853400",
    "end": "4860450"
  },
  {
    "text": "amazing dimensionality reduction\ntechniques that there are. I apologize for going over.",
    "start": "4860450",
    "end": "4866530"
  },
  {
    "start": "4866530",
    "end": "4878000"
  }
]