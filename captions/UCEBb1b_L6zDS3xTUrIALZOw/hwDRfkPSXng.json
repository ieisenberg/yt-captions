[
  {
    "start": "0",
    "end": "340000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu.  GILBERT STRANG: I'm determined\nto tell you something",
    "start": "18140",
    "end": "24690"
  },
  {
    "text": "about the convolution rule. I just get close to it, but\nhaven't quite got there.",
    "start": "24690",
    "end": "31290"
  },
  {
    "text": "And actually, I'd like\nto say something also about why convolution\nis so important.",
    "start": "31290",
    "end": "39000"
  },
  {
    "text": "I mentioned here a paper about\nimages in deep learning by--",
    "start": "39000",
    "end": "49950"
  },
  {
    "text": "it has three authors, and\nthese are two of them. Maybe you recognize\nHinton's name. He's originally English.",
    "start": "49950",
    "end": "56790"
  },
  {
    "text": "He was in San Diego\nfor quite a few years, and now he's in Canada.",
    "start": "56790",
    "end": "63530"
  },
  {
    "text": "So Toronto and Montreal are big\ncenters now for deep learning. And he's really one of the\nleaders, and so is Sutskever.",
    "start": "63530",
    "end": "75710"
  },
  {
    "text": "So maybe you know that the sort\nof progress of deep learning",
    "start": "75710",
    "end": "81810"
  },
  {
    "text": "can often be measured in\nthese competitions that are held about every\nyear for how well does--",
    "start": "81810",
    "end": "92250"
  },
  {
    "text": "people design and execute\na whole neural net.",
    "start": "92250",
    "end": "102390"
  },
  {
    "text": "And this was a\ncompetition about images. So that is really demanding,\nbecause, as I said last time,",
    "start": "102390",
    "end": "112650"
  },
  {
    "text": "an image has so many\nsamples, so many pixels that the computational\nproblem is enormous.",
    "start": "112650",
    "end": "120600"
  },
  {
    "text": "And that's when you would go to\nconvolution neural nets, CNN,",
    "start": "120600",
    "end": "125700"
  },
  {
    "text": "because a convolutional net\ntakes fewer weights, because of the same weight as\nappearing along diagonals.",
    "start": "125700",
    "end": "133440"
  },
  {
    "text": "It doesn't need a full\nmatrix of weights, just one top row of weights.",
    "start": "133440",
    "end": "139480"
  },
  {
    "text": "Anyway, so this is one\nof the historical papers in the history of deep learning.",
    "start": "139480",
    "end": "146890"
  },
  {
    "text": "I'll just read a\ncouple of sentences. We trained-- so this\nis the abstract.",
    "start": "146890",
    "end": "153100"
  },
  {
    "text": "We trained a large deep\nconvolutional neural network.",
    "start": "153100",
    "end": "158300"
  },
  {
    "text": "I'll just say that it ran\nfor five days on two GPUs.",
    "start": "158300",
    "end": "165760"
  },
  {
    "text": "So it was an enormous\nproblem, as we'll see. So we trained a large\ndeep network, CNN,",
    "start": "165760",
    "end": "173920"
  },
  {
    "text": "to classify 1.2 million\nhigh res images in ImageNet.",
    "start": "173920",
    "end": "179200"
  },
  {
    "text": "So ImageNet is a source\nof millions of images.",
    "start": "179200",
    "end": "184540"
  },
  {
    "text": "And on the test data, they-- well, the last sentence\nis maybe a key.",
    "start": "184540",
    "end": "192520"
  },
  {
    "text": "We entered a variant\nof this model in the competition,\n2012 competition,",
    "start": "192520",
    "end": "198760"
  },
  {
    "text": "and we achieved a winning top\nfive test error rate of 15%",
    "start": "198760",
    "end": "206019"
  },
  {
    "text": "compared to 26% for\nthe second place team. So 15% error.",
    "start": "206020",
    "end": "212860"
  },
  {
    "text": "They got 26% was the best that\nthe rest of the world did. And so that-- and when\nhe shows the network,",
    "start": "212860",
    "end": "223890"
  },
  {
    "text": "you realize what's gone into it. It has convolution layers,\nand it has some normal layers,",
    "start": "223890",
    "end": "235000"
  },
  {
    "text": "and it has max pooling layers\nto cut the dimension down",
    "start": "235000",
    "end": "240580"
  },
  {
    "text": "a little bit.  And half the samples go on\none GPU and half another.",
    "start": "240580",
    "end": "249970"
  },
  {
    "text": "And at certain points,\nlayers interconnect between the two GPUs.",
    "start": "249970",
    "end": "255220"
  },
  {
    "text": "And so to reduce overfitting--\nyou remember that.",
    "start": "255220",
    "end": "260370"
  },
  {
    "text": "It's a key problem is\nto reduce overfitting in the fully connected layers.",
    "start": "260370",
    "end": "265750"
  },
  {
    "text": "Those are the ordinary layers\nwith full weight matrices.",
    "start": "265750",
    "end": "271480"
  },
  {
    "text": "We employed a recently developed\nregularization called dropout. So dropout is a tool which,\nif you're in this world, you--",
    "start": "271480",
    "end": "280615"
  },
  {
    "text": "I think Hinton\nproposed it, again, by seeing that it worked.",
    "start": "280615",
    "end": "286389"
  },
  {
    "text": "It's just a careful dropout\nof some of the data.",
    "start": "286390",
    "end": "291760"
  },
  {
    "text": "It reduces the amount\nof data, and it doesn't harm the problem.",
    "start": "291760",
    "end": "300340"
  },
  {
    "text": "So the neural network has\n60 million parameters. 60 million.",
    "start": "300340",
    "end": "305830"
  },
  {
    "text": "With 650,000 neurons, five\nconvolutional layers, and three",
    "start": "305830",
    "end": "312250"
  },
  {
    "text": "fully connected layers. I just mention this.",
    "start": "312250",
    "end": "318490"
  },
  {
    "text": "If you just Google these\ntwo names on the web,",
    "start": "318490",
    "end": "323560"
  },
  {
    "text": "this paper would come up.  So we're talking about\nthe real thing here.",
    "start": "323560",
    "end": "331440"
  },
  {
    "text": "Convolution is something\neverybody wants to understand. And I'd like to-- since I've started\nseveral days ago,",
    "start": "331440",
    "end": "341170"
  },
  {
    "start": "340000",
    "end": "556000"
  },
  {
    "text": "and I'd like to remember\nwhat convolution means. Let me-- so if I\nconvolve two vectors",
    "start": "341170",
    "end": "352230"
  },
  {
    "text": "and I look for the k-th\ncomponent of the answer, the formula is I add up\nall the c's times d's where",
    "start": "352230",
    "end": "364930"
  },
  {
    "text": "are index i plus j adds to k.",
    "start": "364930",
    "end": "370330"
  },
  {
    "text": "Why do you do such a thing? Because c might be\nrepresented by a polynomial,",
    "start": "370330",
    "end": "377620"
  },
  {
    "text": "say x plus c Nx to\nthe N. And d might",
    "start": "377620",
    "end": "383949"
  },
  {
    "text": "be represented by another 1d 1x\nplus dm x to the m, let's say.",
    "start": "383950",
    "end": "390745"
  },
  {
    "text": " And convolution arises when\nI multiply those polynomials.",
    "start": "390745",
    "end": "400200"
  },
  {
    "text": "Because for a typical-- and then collect terms. Because a typical power\nof x, say x to the k,",
    "start": "400200",
    "end": "409860"
  },
  {
    "text": "the coefficients are-- ",
    "start": "409860",
    "end": "415410"
  },
  {
    "text": "well, how do we get x to\nthe k in multiplying these? I multiply c0 times a dk.",
    "start": "415410",
    "end": "422460"
  },
  {
    "text": "Somewhere in here would\nbe a dk x to the k. So a c0 times a dk would\ngive me an x to the k term.",
    "start": "422460",
    "end": "433050"
  },
  {
    "text": "And a c1 times-- everybody sees this coming now? c1 has an x in it already.",
    "start": "433050",
    "end": "440520"
  },
  {
    "text": "So over there, we would look\nat dk minus 1 with one less x.",
    "start": "440520",
    "end": "445530"
  },
  {
    "text": "So it would be c1 dk minus 1. This is just what you do when\nyou multiply a polynomial.",
    "start": "445530",
    "end": "453780"
  },
  {
    "text": "And the point is that the\nway we recognize those terms is that the exponents 0 and k,\nthe exponents 1 and k minus 1,",
    "start": "453780",
    "end": "465900"
  },
  {
    "text": "always add to k. So that's where this\nformula comes from. We take a c times a d hiding\nbehind our cx to the i and a dj",
    "start": "465900",
    "end": "477240"
  },
  {
    "text": "x to the j and when i plus\nj is k, this is x to the k.",
    "start": "477240",
    "end": "484830"
  },
  {
    "text": "And that's the term\nwe're capturing. So this is the\ncoefficient of that term.",
    "start": "484830",
    "end": "492610"
  },
  {
    "text": "And let me write it as a\nslightly different way, where",
    "start": "492610",
    "end": "497759"
  },
  {
    "text": "you actually see even more\nclearly convolution operating.",
    "start": "497760",
    "end": "503130"
  },
  {
    "text": "So j is k minus i, right? So it's the sum of cidj, but\nthe j has to be k minus i.",
    "start": "503130",
    "end": "515700"
  },
  {
    "text": "So this is the way to\nremember the formula",
    "start": "515700",
    "end": "522059"
  },
  {
    "text": "for the coefficients in c\nstar d in the convolution.",
    "start": "522059",
    "end": "529900"
  },
  {
    "text": "You look at c's times d's. It's a form of multiplication. It comes from ordinary\nmultiplication of polynomials.",
    "start": "529900",
    "end": "539660"
  },
  {
    "text": "And when you collect\nterms, you're collecting c, the i-th c and the k minus id,\nand you're taking all possible",
    "start": "539660",
    "end": "548930"
  },
  {
    "text": "i's. So it's a sum over\nall possible i's there to give you\nthe k-th answer.",
    "start": "548930",
    "end": "556880"
  },
  {
    "start": "556000",
    "end": "1281000"
  },
  {
    "text": "Well, just to see\nif you got the idea, what would be the\nconvolution of two functions?",
    "start": "556880",
    "end": "562820"
  },
  {
    "text": "Suppose I have a\nfunction f of x. And I want I convolve that\nwith a function g of x.",
    "start": "562820",
    "end": "570410"
  },
  {
    "text": " OK. And notice that I have\nnot circled this symbol.",
    "start": "570410",
    "end": "579680"
  },
  {
    "text": "So I'm not doing\nperiodic convolution. I'm just doing\nstraightforward convolution. So what are we going to have\nin the case of two functions?",
    "start": "579680",
    "end": "588740"
  },
  {
    "text": "What would that mean, a\nconvolution of functions? I'm in parallel here with a\nconvolution of two vectors.",
    "start": "588740",
    "end": "596430"
  },
  {
    "text": "So think of these now\nhave become functions. The case component has become--",
    "start": "596430",
    "end": "602690"
  },
  {
    "text": "really, I should\nsay f star g at x.",
    "start": "602690",
    "end": "607730"
  },
  {
    "text": "That's really the\nparallel to this. So let me.",
    "start": "607730",
    "end": "612920"
  },
  {
    "text": " So I'm telling you\nthe answer at x.",
    "start": "612920",
    "end": "620900"
  },
  {
    "text": "Here I told you the answer at k. The k-th component\nlooks like that.",
    "start": "620900",
    "end": "629270"
  },
  {
    "text": "What does the x value\nof the convolution look like for functions?",
    "start": "629270",
    "end": "635390"
  },
  {
    "text": "OK, I'm just going to do this. I'm going to do\nthe same as this. Instead of summing,\nwhat will I do?",
    "start": "635390",
    "end": "643200"
  },
  {
    "text": "Integrate.  Instead of c sub i,\nI'll have f of x.",
    "start": "643200",
    "end": "652199"
  },
  {
    "text": " The index i is changing over\nto the continuous variable x.",
    "start": "652200",
    "end": "661300"
  },
  {
    "text": "And now g instead of dk\nminus i, what do I have here?",
    "start": "661300",
    "end": "670390"
  },
  {
    "text": "So it's the k minus i component.",
    "start": "670390",
    "end": "676330"
  },
  {
    "text": "That will go to-- let me just write it down-- t minus x.",
    "start": "676330",
    "end": "683320"
  },
  {
    "text": "So in this translation, f\nis being translated to c.",
    "start": "683320",
    "end": "689380"
  },
  {
    "text": "Or sorry, f corresponds to c. g corresponds to d.",
    "start": "689380",
    "end": "696540"
  },
  {
    "text": "k corresponds to x. Oh no, sorry. i\ncorresponds to x.",
    "start": "696540",
    "end": "705160"
  },
  {
    "text": "And k minus i\ncorresponds to t minus x.",
    "start": "705160",
    "end": "712940"
  },
  {
    "text": "So k corresponds to t. This would be the\nconvolution of two functions.",
    "start": "712940",
    "end": "721370"
  },
  {
    "text": "Oh, it's a function of t. Bad notation.",
    "start": "721370",
    "end": "726470"
  },
  {
    "text": " The t is sort of\nthe amount of shift.",
    "start": "726470",
    "end": "734630"
  },
  {
    "text": "See, I've shifted g. I've reversed it. I've flipped it and shifting\nit by different amounts t.",
    "start": "734630",
    "end": "743360"
  },
  {
    "text": "It's what you have in a filter. It's just also always\npresent in signal processing.",
    "start": "743360",
    "end": "752310"
  },
  {
    "text": "So that that would\nbe a definition. Or I could, if you like, if you\nwant an x variable to come out,",
    "start": "752310",
    "end": "760850"
  },
  {
    "text": "let me make an x variable come\nout by exchanging t and x.",
    "start": "760850",
    "end": "766579"
  },
  {
    "text": "So this would be x minus t dt.",
    "start": "766580",
    "end": "772600"
  },
  {
    "text": "I like that, actually,\na little better. And it's the integral over\nt minus infinity to infinity",
    "start": "772600",
    "end": "779480"
  },
  {
    "text": "if our functions were\non the whole line.",
    "start": "779480",
    "end": "784670"
  },
  {
    "text": "So there will be a\nconvolution rule for that. ",
    "start": "784670",
    "end": "790279"
  },
  {
    "text": "This will connect to the Fourier\ntransform of the two functions.",
    "start": "790280",
    "end": "795830"
  },
  {
    "text": "Over here, I'm connecting\nit to the discrete Fourier transform of the two functions.",
    "start": "795830",
    "end": "801589"
  },
  {
    "text": "And I've been making\nthe convolution cyclic.",
    "start": "801590",
    "end": "807110"
  },
  {
    "text": "So what does-- can\nI add cyclic now? This is ordinary convolution.",
    "start": "807110",
    "end": "812210"
  },
  {
    "text": "This is what you had\nin the first lab, I think, from Raj Rao.",
    "start": "812210",
    "end": "818600"
  },
  {
    "text": "The first lab, you\nremember you had to figure out how many\ncomponents the convolution",
    "start": "818600",
    "end": "827329"
  },
  {
    "text": "would have? And you didn't make it cyclic. So a cyclic convolution,\nif this has n components",
    "start": "827330",
    "end": "837080"
  },
  {
    "text": "and this has n components,\nthen the convolution has n components.",
    "start": "837080",
    "end": "842490"
  },
  {
    "text": "Because keeping n\nis the key number there, the length of the period.",
    "start": "842490",
    "end": "850940"
  },
  {
    "text": "And similarly,\nover here, if f is 2 pi periodic and\ng is 2 pi periodic,",
    "start": "850940",
    "end": "856940"
  },
  {
    "text": "then we might want to do\na periodic convolution and bring it--",
    "start": "856940",
    "end": "864860"
  },
  {
    "text": "get an answer that also\nhas 2 pi period 2 pi.",
    "start": "864860",
    "end": "871589"
  },
  {
    "text": "So you could compute\nthe convolution of sine x with cos\nx, for example.",
    "start": "871590",
    "end": "878850"
  },
  {
    "text": "OK, let's stick with vectors. So what's the deal\nwhen I make it cyclic?",
    "start": "878850",
    "end": "889510"
  },
  {
    "text": "When I make it cyclic, then\nin this multiplication, I really should use--",
    "start": "889510",
    "end": "895939"
  },
  {
    "text": "I've introduced w as\nthat instead of x. So cyclic. ",
    "start": "895940",
    "end": "904900"
  },
  {
    "text": "x becomes this number w, which\nis e to the 2 pi i over n",
    "start": "904900",
    "end": "914350"
  },
  {
    "text": "and has the property\nthen that w to the n-th is 1 so that all vectors\nof length greater than n",
    "start": "914350",
    "end": "922120"
  },
  {
    "text": "can be folded back using this\nrule to a vector of length n.",
    "start": "922120",
    "end": "927640"
  },
  {
    "text": "So we get a cyclic guy. So how does that\nchange the answer?",
    "start": "927640",
    "end": "936100"
  },
  {
    "text": "Well, I only want k\ngoing from 0 to n minus 1",
    "start": "936100",
    "end": "941319"
  },
  {
    "text": "in the cyclic case. I don't want infinitely\nmany components.",
    "start": "941320",
    "end": "946810"
  },
  {
    "text": "I've got to bring\nthem back again. And let me just say\nwhat the rule would be.",
    "start": "946810",
    "end": "956380"
  },
  {
    "text": "You just ask, say, i plus j.",
    "start": "956380",
    "end": "963460"
  },
  {
    "text": "You would look at that modulo n. That's what a number theory\nperson would call it.",
    "start": "963460",
    "end": "972790"
  },
  {
    "text": "We only look at the remainder\nwhen we divide by n.",
    "start": "972790",
    "end": "978399"
  },
  {
    "text": "So now the sums go only\nfrom 0 to n minus 1,",
    "start": "978400",
    "end": "983590"
  },
  {
    "text": "and I only get an answer\nfrom 0 to n minus 1.",
    "start": "983590",
    "end": "988756"
  },
  {
    "text": "Well, I've done\nthat pretty quickly. That's if I wanted\nto do justice to--",
    "start": "988756",
    "end": "997350"
  },
  {
    "text": "So the difference\nbetween non-periodic. ",
    "start": "997350",
    "end": "1003949"
  },
  {
    "text": "So non-periodic and periodic\nwill be the difference",
    "start": "1003950",
    "end": "1014790"
  },
  {
    "text": "between-- so I have some number t0 on the\ndiagonals. t1, t2, t minus 1,",
    "start": "1014790",
    "end": "1025150"
  },
  {
    "text": "t minus 2, and so on. Constant diagonals. So the key name\nthere is Toeplitz.",
    "start": "1025150",
    "end": "1030449"
  },
  {
    "text": " And if it's periodic, then\nI have, I'll say, c, c, c.",
    "start": "1030450",
    "end": "1040929"
  },
  {
    "text": "And then the next one will be\nc1, c1, coming around to c1. And c2 coming around.",
    "start": "1040930",
    "end": "1047290"
  },
  {
    "text": "So it's n by n period n. So it's a circulant matrix.",
    "start": "1047290",
    "end": "1054485"
  },
  {
    "text": " N by N.",
    "start": "1054485",
    "end": "1060549"
  },
  {
    "text": "OK. That's the big picture. And I think in\nthat first lab, you",
    "start": "1060550",
    "end": "1068950"
  },
  {
    "text": "were asked to do the\nnon-circulant case. Because that's the one where you\nhave to do a little patience.",
    "start": "1068950",
    "end": "1074559"
  },
  {
    "text": "What will be the length? Yeah, what would be the\nlength of a non-circulant?",
    "start": "1074560",
    "end": "1080740"
  },
  {
    "text": "So not circulant. Now, suppose the c\nvector has p components",
    "start": "1080740",
    "end": "1090310"
  },
  {
    "text": "and the d vector\nhas q components. How many components\nin their convolution?",
    "start": "1090310",
    "end": "1096600"
  },
  {
    "text": "Shall I write that\nquestion down? Because that brings out\nthe difference here.",
    "start": "1096600",
    "end": "1102580"
  },
  {
    "text": "So if I have p, if c has P\ncomponents, d has q components,",
    "start": "1102580",
    "end": "1119149"
  },
  {
    "text": "then the convolution of\nc and d has how many? ",
    "start": "1119150",
    "end": "1126299"
  },
  {
    "text": "So I'm multiplying. So it's really this corresponds\nto a polynomial of degree",
    "start": "1126300",
    "end": "1132060"
  },
  {
    "text": "p minus 1, right? Polynomials of degree p minus 1.",
    "start": "1132060",
    "end": "1141039"
  },
  {
    "text": "And this guy would\nbe degree q minus 1. Degree q minus 1.",
    "start": "1141040",
    "end": "1147390"
  },
  {
    "text": "And when I multiply\nthem, what's the degree? Just add.",
    "start": "1147390",
    "end": "1153230"
  },
  {
    "text": "And how many coefficients? Well, one more I\nhave to remember for that stupid 0 order term.",
    "start": "1153230",
    "end": "1160710"
  },
  {
    "text": "So this would have p plus\nq minus 1 components.",
    "start": "1160710",
    "end": "1166294"
  },
  {
    "text": " So that would have\nbeen the number",
    "start": "1166294",
    "end": "1171910"
  },
  {
    "text": "that you've somehow had to\nwork out in that first lab. So that if this had n\ncomponents and this had n,",
    "start": "1171910",
    "end": "1179620"
  },
  {
    "text": "this would have 2n minus 1. It's just what you\nwould have-- like you say 3 plus x times 1 plus 2x.",
    "start": "1179620",
    "end": "1191390"
  },
  {
    "text": "In this case, p is 2, q is two,\ntwo components, two components.",
    "start": "1191390",
    "end": "1197470"
  },
  {
    "text": "And if I multiply those, I get\n3x and 6x is 7x and 2x squared.",
    "start": "1197470",
    "end": "1207159"
  },
  {
    "text": "And so I have 2 plus 2\nminus 1 equals 3 components.",
    "start": "1207160",
    "end": "1218230"
  },
  {
    "text": "The constant x and x squared. Yeah, clear, right. Yeah, so that's not the--",
    "start": "1218230",
    "end": "1224590"
  },
  {
    "text": "that's what I would get if I\nmultiplied these matrices, if I had a two diagonal\nmatrix, Toeplitz matrix,",
    "start": "1224590",
    "end": "1232030"
  },
  {
    "text": "times a two diagonal Toeplitz\nmatrix, that would give me a three diagonal answer.",
    "start": "1232030",
    "end": "1239650"
  },
  {
    "text": "But if I am doing\nit periodically, I would only have two.",
    "start": "1239650",
    "end": "1246040"
  },
  {
    "text": "That 2x squared would\ncome back if I-- come back as a 2.",
    "start": "1246040",
    "end": "1253420"
  },
  {
    "text": "so I just have 5 plus 7x. Right, good, good, good.",
    "start": "1253420",
    "end": "1259130"
  },
  {
    "text": "OK. So that's a reminder\nof what convolution is.",
    "start": "1259130",
    "end": "1264740"
  },
  {
    "text": "Cyclic and non-cyclic,\nvectors and functions. OK, then eigenvalues and\neigenvectors are the next step,",
    "start": "1264740",
    "end": "1275149"
  },
  {
    "text": "and then the convolution\nrule is the last step. So eigenvectors.",
    "start": "1275150",
    "end": "1281980"
  },
  {
    "start": "1281000",
    "end": "1519000"
  },
  {
    "text": "Eigenvectors of the circulant.  Of course, I can only\ndo square matrices.",
    "start": "1281980",
    "end": "1288857"
  },
  {
    "text": " So I'm doing the periodic case.",
    "start": "1288858",
    "end": "1295430"
  },
  {
    "text": "So the eigenvectors\nare the columns of the eigenvector matrix.",
    "start": "1295430",
    "end": "1305950"
  },
  {
    "text": "And I'm going to call\nit F for Fourier. So F is-- the first\neigenvector is all 1s.",
    "start": "1305950",
    "end": "1314410"
  },
  {
    "text": "An x eigenvector is\nthe fourth root of 1, then the square root of\n1, i6, i8, i fourth, i6,",
    "start": "1314410",
    "end": "1327380"
  },
  {
    "text": "and finally, 1 i\ncubed i sixth i ninth.",
    "start": "1327380",
    "end": "1332640"
  },
  {
    "text": "OK, that's F. Those are the four\neigenvectors of the permutation",
    "start": "1332640",
    "end": "1341160"
  },
  {
    "text": "p and of any polynomial in p. So my circulant is some\nc0 i plus c 1p plus c",
    "start": "1341160",
    "end": "1353759"
  },
  {
    "text": "2p squared and c3 pq. OK.",
    "start": "1353760",
    "end": "1358980"
  },
  {
    "text": "And finally, this is the step\nwe've been almost ready to do",
    "start": "1358980",
    "end": "1364110"
  },
  {
    "text": "but didn't quite do. What are the eigenvectors-- what eigenvectors\nare its eigenvectors?",
    "start": "1364110",
    "end": "1374995"
  },
  {
    "text": " So those are the\neigenvectors of p.",
    "start": "1374995",
    "end": "1382730"
  },
  {
    "text": "And now we have just\na combination of p's. So I think the eigenvectors\nI just multiply.",
    "start": "1382730",
    "end": "1392500"
  },
  {
    "text": "I take that same combination\nof the eigenvectors. ",
    "start": "1392500",
    "end": "1404049"
  },
  {
    "text": "Does that look right? So sorry.",
    "start": "1404050",
    "end": "1409180"
  },
  {
    "text": " I'm sorry.",
    "start": "1409180",
    "end": "1414450"
  },
  {
    "text": "Its eigenvectors,\nthey're the columns of f. The question I meant to ask\nis what are its eigenvalues?",
    "start": "1414450",
    "end": "1427120"
  },
  {
    "text": " That's the key question.",
    "start": "1427120",
    "end": "1432809"
  },
  {
    "text": "What are the eigenvalues? And I think that if I\njust multiply F times c,",
    "start": "1432810",
    "end": "1438240"
  },
  {
    "text": "I get the eigenvalues\nof the matrix C.",
    "start": "1438240",
    "end": "1443880"
  },
  {
    "text": "That's the beauty. That's the nice formula. ",
    "start": "1443880",
    "end": "1449280"
  },
  {
    "text": "If my matrix is just P alone,\nthen this is 0, 1, 0, 0,",
    "start": "1449280",
    "end": "1454920"
  },
  {
    "text": "and I get 1, i, i\nsquared, i cubed. But if c is some other\ncombination of the p's, then I",
    "start": "1454920",
    "end": "1462330"
  },
  {
    "text": "take the same combination\nof the eigenvectors to see--",
    "start": "1462330",
    "end": "1470130"
  },
  {
    "text": "yeah. Do you see it? So I'm claiming that I'll\nget four eigenvalues of C",
    "start": "1470130",
    "end": "1481080"
  },
  {
    "text": "from this multiplication. So of course, if\nthere's only c0,",
    "start": "1481080",
    "end": "1489140"
  },
  {
    "text": "then I only get c0, c0, c0, c0.",
    "start": "1489140",
    "end": "1494430"
  },
  {
    "text": "It's four times repeated. But if it's this combination,\nthen that matrix multiplication",
    "start": "1494430",
    "end": "1502520"
  },
  {
    "text": "takes the same combination of-- this is a combination\nof the eigenvectors.",
    "start": "1502520",
    "end": "1511950"
  },
  {
    "text": "And that gives us\nthe right thing.",
    "start": "1511950",
    "end": "1517120"
  },
  {
    "text": "OK. ",
    "start": "1517120",
    "end": "1522730"
  },
  {
    "start": "1519000",
    "end": "1719000"
  },
  {
    "text": "Now I just have one more step\nfor this convolution rule, and then I'm happy. ",
    "start": "1522730",
    "end": "1538370"
  },
  {
    "text": "Really, the convolution\nrule is stating what we--",
    "start": "1538370",
    "end": "1544610"
  },
  {
    "text": "it's stating a relation\nbetween multiplication, which we saw here, and\nthe convolution, which",
    "start": "1544610",
    "end": "1553520"
  },
  {
    "text": "we saw for the coefficients. So the convolution rule is a\nconnection between multiplying",
    "start": "1553520",
    "end": "1560269"
  },
  {
    "text": "and convolution. And so let me say what\nthat convolution rule is",
    "start": "1560270",
    "end": "1568180"
  },
  {
    "text": "and let me write it correctly. ",
    "start": "1568180",
    "end": "1575000"
  },
  {
    "text": "So here I take a\ncyclic convolution.",
    "start": "1575000",
    "end": "1581060"
  },
  {
    "text": "I'm dealing with\nsquare matrices. Everything is cyclic here. And then I get--",
    "start": "1581060",
    "end": "1588440"
  },
  {
    "text": "if I multiply by F,\nwhat do I have now? What does that represent?",
    "start": "1588440",
    "end": "1594500"
  },
  {
    "text": "This was c and d,\nand I convolve them. So I got another\ncirculant matrix.",
    "start": "1594500",
    "end": "1603290"
  },
  {
    "text": "So up here, the multiplication\nof matrices is C times D.",
    "start": "1603290",
    "end": "1610784"
  },
  {
    "text": "I want to connect multiplying\nthose matrices with convolving",
    "start": "1610784",
    "end": "1615970"
  },
  {
    "text": "the c's. I want to make that connection.",
    "start": "1615970",
    "end": "1622570"
  },
  {
    "text": "And that connection is\nthe convolution rule. ",
    "start": "1622570",
    "end": "1628120"
  },
  {
    "text": "So this would be the\neigenvalues of CD.",
    "start": "1628120",
    "end": "1635360"
  },
  {
    "text": " Let's just pause there.",
    "start": "1635360",
    "end": "1641080"
  },
  {
    "text": "Why am I looking at\nthe eigenvalues of CD? Because if I do\nthat multiplication,",
    "start": "1641080",
    "end": "1647150"
  },
  {
    "text": "I get another Toeplitz\nmatrix, C times D.",
    "start": "1647150",
    "end": "1652390"
  },
  {
    "text": "And the polynomial-- the\ncoefficients associated on the diagonals of C times\nD are the coefficients",
    "start": "1652390",
    "end": "1662170"
  },
  {
    "text": "of the convolution. So its diagonals come from\nconvolving c with d cyclically.",
    "start": "1662170",
    "end": "1680390"
  },
  {
    "text": "OK. Now I want to find the same\neigenvalues in a second way",
    "start": "1680390",
    "end": "1687160"
  },
  {
    "text": "and match-- and the equation will\nbe the convolution rule. So how can I find the\neigenvalues of CD?",
    "start": "1687160",
    "end": "1694720"
  },
  {
    "text": "Well, amazingly, they\nare the eigenvalues of C times the eigenvalues\nof D. I'm going",
    "start": "1694720",
    "end": "1714539"
  },
  {
    "text": "to test this rule on 2 by 2. So you'll see\neverything happening.",
    "start": "1714540",
    "end": "1719820"
  },
  {
    "start": "1719000",
    "end": "1803000"
  },
  {
    "text": "So this is the main-- this is\nthe fact that I want to use. ",
    "start": "1719820",
    "end": "1726880"
  },
  {
    "text": "Because C and D commute. C and D commute. They have the same eigenvectors.",
    "start": "1726880",
    "end": "1732970"
  },
  {
    "text": "And then the eigenvalues\njust multiply. So I can multiply.",
    "start": "1732970",
    "end": "1738760"
  },
  {
    "text": "I can get that in a second way\nby taking the eigenvalues of c",
    "start": "1738760",
    "end": "1744850"
  },
  {
    "text": "and multiplying those\nby the eigenvalues of d.",
    "start": "1744850",
    "end": "1750039"
  },
  {
    "text": "And I multiply\ncomponent by component. I multiply the eigenvalue\nfor the all 1s vector",
    "start": "1750040",
    "end": "1758560"
  },
  {
    "text": "by the eigenvalue for\nthe all 1s vector. Do you know this MATLAB command?",
    "start": "1758560",
    "end": "1765760"
  },
  {
    "text": "Component by component\nmultiplication? This is an important one. There's a guy's name is\nalso associated with that.",
    "start": "1765760",
    "end": "1775110"
  },
  {
    "text": "So that's a vector. That's a vector. And what comes out\nof that operation?",
    "start": "1775110",
    "end": "1780850"
  },
  {
    "text": "If I have a vector\nwith three components. So n is 3 here. And I do point star or dot star.",
    "start": "1780850",
    "end": "1789580"
  },
  {
    "text": "I'm not sure what\npeople usually say. Component by component, a three\ncomponent vector times a three",
    "start": "1789580",
    "end": "1795820"
  },
  {
    "text": "component vector, I get\na three component vector,",
    "start": "1795820",
    "end": "1801860"
  },
  {
    "text": "just like that. So this is the convolution rule.",
    "start": "1801860",
    "end": "1807833"
  },
  {
    "text": "That's the convolution rule.  And the proof is the fact\nthat when matrices commute,",
    "start": "1807833",
    "end": "1818390"
  },
  {
    "text": "the eigenvalues of\nthe product are just these eigenvalues times\nthese eigenvalues,",
    "start": "1818390",
    "end": "1824360"
  },
  {
    "text": "because they have the same-- the eigenvectors\nare always the same here for all these circulants.",
    "start": "1824360",
    "end": "1831649"
  },
  {
    "text": "So there's the convolution rule\nthat I can convolve and then",
    "start": "1831650",
    "end": "1836930"
  },
  {
    "text": "transform. Or I can transform\nseparately and then multiply.",
    "start": "1836930",
    "end": "1845490"
  },
  {
    "text": "So I just maybe better\nright that convolution rule.",
    "start": "1845490",
    "end": "1850700"
  },
  {
    "text": "Let's call it the C rule.  Convolve then transform by F.\nOr transform separately by F.",
    "start": "1850700",
    "end": "1884650"
  },
  {
    "text": "And then multiply point one.",
    "start": "1884650",
    "end": "1894390"
  },
  {
    "text": " Element by element.",
    "start": "1894390",
    "end": "1899950"
  },
  {
    "text": "Component by component. ",
    "start": "1899950",
    "end": "1905250"
  },
  {
    "text": "OK. So that's the convolution rule.",
    "start": "1905250",
    "end": "1911870"
  },
  {
    "start": "1911000",
    "end": "1964000"
  },
  {
    "text": "And why is it sort of--\nwhy is it so important? Because transforming\nby F, multiplying",
    "start": "1911870",
    "end": "1920500"
  },
  {
    "text": "by the Fourier matrix, is\nextremely fast by the FFT.",
    "start": "1920500",
    "end": "1925630"
  },
  {
    "text": "So it's useful because of\nthe FFT, the Fast Fourier",
    "start": "1925630",
    "end": "1936760"
  },
  {
    "text": "Transform, to multiply. ",
    "start": "1936760",
    "end": "1946570"
  },
  {
    "text": "Or to transform. Whichever. Equal to transform.",
    "start": "1946570",
    "end": "1952659"
  },
  {
    "text": "Multiply by F transform. ",
    "start": "1952660",
    "end": "1963370"
  },
  {
    "text": "So it's the presence of\nthe FFT that makes this--",
    "start": "1963370",
    "end": "1968980"
  },
  {
    "start": "1964000",
    "end": "2053000"
  },
  {
    "text": "it gives us really two\ndifferent ways to do it. In fact, which is\nthe faster way?",
    "start": "1968980",
    "end": "1976990"
  },
  {
    "text": "So we can produce the same\nresult this way or this way.",
    "start": "1976990",
    "end": "1986520"
  },
  {
    "text": " And if I don't count the cost\nof-- if the cost of multiplying",
    "start": "1986520",
    "end": "1994700"
  },
  {
    "text": "by F is low, because I have\nthe FFT, which would you do?",
    "start": "1994700",
    "end": "2000399"
  },
  {
    "text": "Which would you do? So let me just think aloud\nbefore we answer that question,",
    "start": "2000400",
    "end": "2006010"
  },
  {
    "text": "and then we're good.  So my vectors have n components.",
    "start": "2006010",
    "end": "2014429"
  },
  {
    "text": "So one way I can do\nis to do convolution. How many steps is that?",
    "start": "2014430",
    "end": "2020920"
  },
  {
    "text": "If I take a vector\nwith n components and I convolve with a\nvector with n components, how many little multiplications\ndo I have to do?",
    "start": "2020920",
    "end": "2030100"
  },
  {
    "text": "N squared, right? Because each of the c's has\nto multiply each of the d's.",
    "start": "2030100",
    "end": "2035830"
  },
  {
    "text": "So that takes N squared.  And Fourier is cheap.",
    "start": "2035830",
    "end": "2042799"
  },
  {
    "text": "It's N log N. Log to base 2.",
    "start": "2042800",
    "end": "2050881"
  },
  {
    "text": "So the left hand side\nis effectively N cubed.",
    "start": "2050881",
    "end": "2056090"
  },
  {
    "start": "2053000",
    "end": "2200000"
  },
  {
    "text": "What about this one? How many to do these two guys?",
    "start": "2056090",
    "end": "2063050"
  },
  {
    "text": " To find the Fourier transform\nto multiply by the matrix F. OK,",
    "start": "2063050",
    "end": "2072690"
  },
  {
    "text": "those are fast again. That's just I've got two\nmultiplications by F. So that's 2 N log N. And\nwhat's the cost of this?",
    "start": "2072690",
    "end": "2083653"
  },
  {
    "text": " I have a vector\nwith n components.",
    "start": "2083654",
    "end": "2090850"
  },
  {
    "text": "Dot star vector. Another vector\nwith n components. How many little\nmultiplications do I have to do for a Hadamard\nproduct or a component",
    "start": "2090850",
    "end": "2100420"
  },
  {
    "text": "by component product? N, only n. ",
    "start": "2100420",
    "end": "2106770"
  },
  {
    "text": "Plus N. Yeah, maybe I\nshould have made that plus.",
    "start": "2106770",
    "end": "2113100"
  },
  {
    "text": "I had two. No, I had one N\nlog N. Plus it took N squared to find that vector.",
    "start": "2113100",
    "end": "2120280"
  },
  {
    "text": "And then N log N. So it's\neffectively N squared.",
    "start": "2120280",
    "end": "2125700"
  },
  {
    "text": "But this one where I do the\nN log N twice and then it",
    "start": "2125700",
    "end": "2132300"
  },
  {
    "text": "only takes me N more. So this is the fast way. ",
    "start": "2132300",
    "end": "2141260"
  },
  {
    "text": "So if you wanted to\nmultiply two really big, long integers, as you would want\nto do in cryptography, if you",
    "start": "2141260",
    "end": "2150860"
  },
  {
    "text": "had two long integers, say,\nof length 125, 126, 128",
    "start": "2150860",
    "end": "2158360"
  },
  {
    "text": "components, to\nmultiply those, you would be better off\nto separately take",
    "start": "2158360",
    "end": "2165760"
  },
  {
    "text": "the cyclic transform of\neach of those 128 guys",
    "start": "2165760",
    "end": "2171770"
  },
  {
    "text": "and do it this way. ",
    "start": "2171770",
    "end": "2177590"
  },
  {
    "text": "Take the transforms, do the\ncomponent by component product, and then transform\nback to get that.",
    "start": "2177590",
    "end": "2184650"
  },
  {
    "text": " The convolution rule\nis what makes that go.",
    "start": "2184650",
    "end": "2191140"
  },
  {
    "text": " Oh, one more thought,\nI guess, about all",
    "start": "2191140",
    "end": "2198190"
  },
  {
    "text": "this convolution stuff. Suppose we're in 2D. We have to think what is a\ntwo dimensional convolution?",
    "start": "2198190",
    "end": "2208059"
  },
  {
    "start": "2200000",
    "end": "2838000"
  },
  {
    "text": "What does this become\nin two dimensions? Suppose we have functions.",
    "start": "2208060",
    "end": "2214400"
  },
  {
    "text": "So now I'm gonna do 2D\nfunctions of x and y.",
    "start": "2214400",
    "end": "2221020"
  },
  {
    "text": "Periodic or not periodic. But what's a convolution?",
    "start": "2221020",
    "end": "2227050"
  },
  {
    "text": "What's the operation we have\nto do in two dimensions? Well, it's a double\nintegral, of course.",
    "start": "2227050",
    "end": "2234490"
  },
  {
    "text": "t and u. We would do f of t and u times\ng of x minus ty minus u dtdu.",
    "start": "2234490",
    "end": "2249025"
  },
  {
    "text": " And that would\nproduce a function.",
    "start": "2249025",
    "end": "2256510"
  },
  {
    "text": "So I'm convolving a\nfunction of x and y with another\nfunction of x and y.",
    "start": "2256510",
    "end": "2263410"
  },
  {
    "text": "And again, I'm looking for this. This is the key to watch for.",
    "start": "2263410",
    "end": "2269230"
  },
  {
    "text": "x minus t. y minus u. That's the signal of a\nconvolution integral.",
    "start": "2269230",
    "end": "2277420"
  },
  {
    "text": "So that's what we\nwould have in 2D. In general, So maybe\nnow my final thought",
    "start": "2277420",
    "end": "2286700"
  },
  {
    "text": "is to move to think about\ntwo dimensional matrices",
    "start": "2286700",
    "end": "2293450"
  },
  {
    "text": "and their products and so on. And this is why you need them. Because if you have two\ndimensional signals,",
    "start": "2293450",
    "end": "2301190"
  },
  {
    "text": "then the components\nfit into a matrix. And we just have to\noperate in both dimensions.",
    "start": "2301190",
    "end": "2310960"
  },
  {
    "text": " So the key operation\nin 2D is in MATLAB.",
    "start": "2310960",
    "end": "2326545"
  },
  {
    "text": " The MATLAB command that\nyou need to know to get--",
    "start": "2326545",
    "end": "2335980"
  },
  {
    "text": "if you know what you're doing in\n1D and you want to do it in 2D, the MATLAB command is Kron.",
    "start": "2335980",
    "end": "2343579"
  },
  {
    "text": "So imagine we have one\ndimensional matrices A and B.",
    "start": "2343580",
    "end": "2352250"
  },
  {
    "text": "And so those are in 1D, and we\nwant to produce a natural two",
    "start": "2352250",
    "end": "2359570"
  },
  {
    "text": "dimensional matrix. So these will be N\nby N. And we want",
    "start": "2359570",
    "end": "2367770"
  },
  {
    "text": "the sort of natural\nproduct, let me call it K for Kron, which will be\nN squared by N squared.",
    "start": "2367770",
    "end": "2375080"
  },
  {
    "text": " I want to create a 2D matrix\nconnected to an image that's",
    "start": "2375080",
    "end": "2387859"
  },
  {
    "text": "N in each direction. So it has N squared pixels.",
    "start": "2387860",
    "end": "2393720"
  },
  {
    "text": "These are 1D signals,\nand K is a 2D one.",
    "start": "2393720",
    "end": "2399180"
  },
  {
    "text": "And this K would be the-- this is the operation to know.",
    "start": "2399180",
    "end": "2406938"
  },
  {
    "text": "Given two one dimensional\nn by n matrices,",
    "start": "2406938",
    "end": "2413520"
  },
  {
    "text": "Kron produces an N squared\nby N squared matrix.",
    "start": "2413520",
    "end": "2419040"
  },
  {
    "text": "It's the operation to know. So I'll just write it, and\nif you know what Kron is,",
    "start": "2419040",
    "end": "2425220"
  },
  {
    "text": "then you know it\nbefore I write it. So I want to produce a big\nmatrix, N squared by N squared.",
    "start": "2425220",
    "end": "2433859"
  },
  {
    "text": " Somehow appropriately\nmultiplying these two guys.",
    "start": "2433860",
    "end": "2441859"
  },
  {
    "text": "And the appropriate way to do it\nis to take a11 and multiply it",
    "start": "2441860",
    "end": "2448805"
  },
  {
    "text": "by B. So there, what do I have? What size have I\ngot there already",
    "start": "2448805",
    "end": "2455030"
  },
  {
    "text": "just in that one corner? N by N, right? It's a number of times\nan N by N matrix.",
    "start": "2455030",
    "end": "2463100"
  },
  {
    "text": "Then a12 times B. That's\nanother N by N matrix. Up to a1N times\nB. So I have now--",
    "start": "2463100",
    "end": "2474529"
  },
  {
    "text": "sorry, cap N. So I have cap N\nmatrices in a row.",
    "start": "2474530",
    "end": "2482720"
  },
  {
    "text": "Each of those\nmatrices is N by N. So that row has\nlength n squared.",
    "start": "2482720",
    "end": "2487789"
  },
  {
    "text": "And of course, the next row is-- ",
    "start": "2487790",
    "end": "2497330"
  },
  {
    "text": "I've allowed myself\nto number from 1 to N, but very often that numbering\nshould be 0 to n minus 1.",
    "start": "2497330",
    "end": "2506210"
  },
  {
    "text": "And finally on down here\ndown to an1 B to aNN B. So so",
    "start": "2506210",
    "end": "2519710"
  },
  {
    "text": "that's the N squared\nby N squared matrix that you would need to know. For example, if you wanted to\ndo a two dimensional Fourier",
    "start": "2519710",
    "end": "2527990"
  },
  {
    "text": "transform, that would be-- ",
    "start": "2527990",
    "end": "2535130"
  },
  {
    "text": "yeah, so what would a two\ndimensional Fourier transform produce? What matrix?",
    "start": "2535130",
    "end": "2541340"
  },
  {
    "text": "Is this the matrix you\nwould use for a 2D? ",
    "start": "2541340",
    "end": "2550280"
  },
  {
    "text": "I haven't sort of got\nstarted properly on 2D Fourier transforms. So would it be F times F?",
    "start": "2550280",
    "end": "2556760"
  },
  {
    "text": " So let me write down the\nfull name of this guy.",
    "start": "2556760",
    "end": "2563330"
  },
  {
    "text": "Kronecker. So it's called the\nKronecker product. ",
    "start": "2563330",
    "end": "2571500"
  },
  {
    "text": "It's just the\nright thing to know in moving from one\ndimension to two dimensions.",
    "start": "2571500",
    "end": "2577920"
  },
  {
    "text": "For example. Let me do an example. ",
    "start": "2577920",
    "end": "2587708"
  },
  {
    "text": "Oops, that's full. Have I got one board left? Yeah. ",
    "start": "2587708",
    "end": "2596770"
  },
  {
    "text": "So here's a standard matrix. Call it A. 2s and minus 1s.",
    "start": "2596770",
    "end": "2605410"
  },
  {
    "text": "So that corresponds\nto a second derivative",
    "start": "2605410",
    "end": "2612010"
  },
  {
    "text": "or actually minus a\nsecond derivative. Now, suppose I have\nanother, the same matrix,",
    "start": "2612010",
    "end": "2619210"
  },
  {
    "text": "corresponding to second\nderivatives in the y direction.",
    "start": "2619210",
    "end": "2627086"
  },
  {
    "text": "Same.  And what I really\nwant to do is both.",
    "start": "2627086",
    "end": "2633150"
  },
  {
    "text": "I want to have a matrix\nK that corresponds to minus the second\nin the x direction",
    "start": "2633150",
    "end": "2643380"
  },
  {
    "text": "minus the second in the y. So this is the Laplace.",
    "start": "2643380",
    "end": "2650329"
  },
  {
    "text": "Laplacian.  Which is all over the place\nin differential equations.",
    "start": "2650330",
    "end": "2658930"
  },
  {
    "text": "At a typical point, I\nwant to do minus one of these, two of these minus\none of those in the x direction,",
    "start": "2658930",
    "end": "2667110"
  },
  {
    "text": "and I want to add\nto that minus 1. Now that 2 becomes a 4 and\nminus 1 in the y direction.",
    "start": "2667110",
    "end": "2674550"
  },
  {
    "text": "So I'm looking for\nthe 2 by 2 matrix-- sorry, the two dimensional\nmatrix that takes--",
    "start": "2674550",
    "end": "2683080"
  },
  {
    "text": "that does that\nfive point scheme. Five weights at each point.",
    "start": "2683080",
    "end": "2689520"
  },
  {
    "text": "It takes four of the-- on the diagonal and minus\n1 on the four neighbors.",
    "start": "2689520",
    "end": "2696690"
  },
  {
    "text": "And the operation that\nwould do that would be you would use Kron.",
    "start": "2696690",
    "end": "2703230"
  },
  {
    "text": "It wouldn't be Kron of\nA B. That would just-- K of A B is not what I--",
    "start": "2703230",
    "end": "2710089"
  },
  {
    "text": "a Kron of A B is\nnot what I want. ",
    "start": "2710090",
    "end": "2717390"
  },
  {
    "text": "Yeah, that would do one\nand then the other one. And then that would probably\nproduce nine non-zeroes.",
    "start": "2717390",
    "end": "2724650"
  },
  {
    "text": "I want something that adds here. So I want Kron of A\ntimes the identity.",
    "start": "2724650",
    "end": "2732060"
  },
  {
    "text": "That gives me the\ntwo dimensional thing for this part.",
    "start": "2732060",
    "end": "2738390"
  },
  {
    "text": "And then I'll add on Kron of I\nB for the vertical derivative,",
    "start": "2738390",
    "end": "2750279"
  },
  {
    "text": "the derivatives in\nthe y direction. So that's called\na Kronecker sum.",
    "start": "2750280",
    "end": "2756180"
  },
  {
    "text": " The other was a\nKronecker product.",
    "start": "2756180",
    "end": "2762420"
  },
  {
    "text": "So that would be a\nKronecker product. This would be another\nKronecker product, and the total is called\nthe Kronecker sum.",
    "start": "2762420",
    "end": "2769290"
  },
  {
    "text": " OK. I wanted just to get\nthose notations out.",
    "start": "2769290",
    "end": "2777380"
  },
  {
    "text": "Because really,\nFourier transforming is such a central operation\nin all of applied math,",
    "start": "2777380",
    "end": "2787590"
  },
  {
    "text": "and especially in\nsignal processing. OK, so I'm good for it today.",
    "start": "2787590",
    "end": "2793180"
  },
  {
    "text": "Let's see. I've got one volunteer so\nfar to talk about a project.",
    "start": "2793180",
    "end": "2799079"
  },
  {
    "text": "Can I encourage an email from\nanybody that doesn't-- you don't have to be a superstar.",
    "start": "2799080",
    "end": "2806950"
  },
  {
    "text": "You're just willing to do it.",
    "start": "2806950",
    "end": "2812130"
  },
  {
    "text": "Tell us something about\nwhat you've learned. Get comments from the audience. And 10 or 15 minutes is\nall I'm thinking about.",
    "start": "2812130",
    "end": "2822960"
  },
  {
    "text": "OK, I'll let you send me an\nemail if you'd like to tell us that and get some feedback.",
    "start": "2822960",
    "end": "2829530"
  },
  {
    "text": "OK, good. So I'll see you Wednesday. ",
    "start": "2829530",
    "end": "2835590"
  },
  {
    "text": "Thanks. ",
    "start": "2835590",
    "end": "2838220"
  }
]