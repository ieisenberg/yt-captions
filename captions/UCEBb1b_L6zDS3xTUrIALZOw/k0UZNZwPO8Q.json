[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "7410"
  },
  {
    "text": "offer high-quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "7410",
    "end": "13960"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at",
    "start": "13960",
    "end": "19790"
  },
  {
    "text": "ocw.mit.edu.  PROFESSOR: OK, let's\nget started.",
    "start": "19790",
    "end": "25630"
  },
  {
    "text": "We'll start a minute early and\nthen maybe we can finish a minute early if we're lucky.",
    "start": "25630",
    "end": "31230"
  },
  {
    "text": "Today we want to talk about\nthe laws of large numbers. We want to talk about\nconvergence a little bit.",
    "start": "31230",
    "end": "38310"
  },
  {
    "text": "We will not really get into\nthe strong law of large numbers, which we're going to\ndo later, because that's a",
    "start": "38310",
    "end": "45820"
  },
  {
    "text": "kind of a mysterious,\ndifficult topic. And I wanted to put off really\ntalking about that until we",
    "start": "45820",
    "end": "53120"
  },
  {
    "text": "got to the point where we could\nmake some use of it, which is not quite yet. So first, I want to review\nwhat we've done",
    "start": "53120",
    "end": "59970"
  },
  {
    "text": "just a little bit.  We've said that probability\nmodels are very natural things",
    "start": "59970",
    "end": "68890"
  },
  {
    "text": "for real-world situations,\nparticularly those that are repeatable.",
    "start": "68890",
    "end": "75600"
  },
  {
    "text": "And by repeatable, I mean they\nuse trials, which have",
    "start": "75600",
    "end": "82940"
  },
  {
    "text": "essentially the same\ninitial conditions. They're essentially isolated\nfrom each other.",
    "start": "82940",
    "end": "88130"
  },
  {
    "text": "When I say they're isolated from\neach other, I mean there isn't any apparent\ncontact that they",
    "start": "88130",
    "end": "93469"
  },
  {
    "text": "have with each other. So for example, when you're\nflipping coins, there's not",
    "start": "93470",
    "end": "99969"
  },
  {
    "text": "one very unusual coin,\nand that's the coin you use all the time. And then you try to use\nthose results to be",
    "start": "99970",
    "end": "105909"
  },
  {
    "text": "typical of all coins. Have a fixed set of possible\noutcomes for",
    "start": "105910",
    "end": "110920"
  },
  {
    "text": "these multiple trials. And they have an essentially\nrandom individual outcomes.",
    "start": "110920",
    "end": "116430"
  },
  {
    "text": "Now, you'll see there's a real\nproblem here when I use the word \"random\" there and\n\"probability models\" there",
    "start": "116430",
    "end": "124270"
  },
  {
    "text": "because there is something\ninherently circular in this argument. It's something that always\nhappens when you get into",
    "start": "124270",
    "end": "131190"
  },
  {
    "text": "modeling where you're trying to\ntake the messy real-world and turn it into a nice, clean,\nmathematical model.",
    "start": "131190",
    "end": "139090"
  },
  {
    "text": "So that really, what we\nall do, and we do this instinctively, is after we\nstart getting used to a",
    "start": "139090",
    "end": "146590"
  },
  {
    "text": "particular model, we assume that\nthe real-world is like that model.",
    "start": "146590",
    "end": "151840"
  },
  {
    "text": "If you don't think you\ndo that, think again. Because I think everyone does.",
    "start": "151840",
    "end": "158390"
  },
  {
    "text": "So you really have that problem\nof trying to figure out what's wrong with models,\nhow to go to better models,",
    "start": "158390",
    "end": "165010"
  },
  {
    "text": "and we do this all the time. OK, for any model, an extended\nmodel-- in other words, an",
    "start": "165010",
    "end": "170030"
  },
  {
    "text": "extended mathematical model, for\na sequence or an n-tuple of independent identically\ndistributed repetitions is",
    "start": "170030",
    "end": "180030"
  },
  {
    "text": "always well-defined\nmathematically. We haven't proven that,\nit's not trivial.",
    "start": "180030",
    "end": "185230"
  },
  {
    "text": "But in fact, it's true. Relative frequencies and\nsample averages.",
    "start": "185230",
    "end": "192050"
  },
  {
    "text": "Relative frequencies apply to\nevents, and you can represent",
    "start": "192050",
    "end": "197240"
  },
  {
    "text": "events in terms of indicator\nfunctions and then use everything you know about\nrandom variables to deal with them.",
    "start": "197240",
    "end": "204330"
  },
  {
    "text": "Therefore, you can use\nsample averages. In this extended model,\nessentially become",
    "start": "204330",
    "end": "209909"
  },
  {
    "text": "deterministic. And that's what the laws of\nlarge numbers say in various different ways.",
    "start": "209910",
    "end": "216210"
  },
  {
    "text": "And beyond knowing that they\nbecome deterministic, our problem today is to decide\nexactly what that means.",
    "start": "216210",
    "end": "223700"
  },
  {
    "text": " The laws of large numbers\nspecify what \"become",
    "start": "223700",
    "end": "231280"
  },
  {
    "text": "deterministic\" means. They only operates within\nthe extended model. In other words, laws of large\nnumbers don't apply to the",
    "start": "231280",
    "end": "239390"
  },
  {
    "text": "real world. Well, we hope they apply to the\nreal world, but they only apply to the real world when the\nmodel is good because you",
    "start": "239390",
    "end": "247140"
  },
  {
    "text": "can only prove the laws of\nlarge numbers within this model domain. Probability theory provides an\nawful lot of consistency",
    "start": "247140",
    "end": "255220"
  },
  {
    "text": "checks and ways to avoid\nexperimentation. In other words, I'm not claiming\nhere that you have to",
    "start": "255220",
    "end": "261250"
  },
  {
    "text": "do experimentation with a large\nnumber of so-called independent trials very often\nbecause you have so many ways",
    "start": "261250",
    "end": "270150"
  },
  {
    "text": "of checking on things. But every once in a while, you\nhave to do experimentation. And when you do, somehow or\nother the idea of this large",
    "start": "270150",
    "end": "279520"
  },
  {
    "text": "number of trials, and either IID\ntrials or trials which are",
    "start": "279520",
    "end": "285300"
  },
  {
    "text": "somehow isolated from\neach other. ",
    "start": "285300",
    "end": "290409"
  },
  {
    "text": "And we will soon get to talk\nabout Markov models. We will see that with Markov\nmodels, you don't have the IID",
    "start": "290410",
    "end": "297730"
  },
  {
    "text": "property, but you have enough\nindependence over time that you can still get these\nsorts of results.",
    "start": "297730",
    "end": "304920"
  },
  {
    "text": "So anyway, the determinism in\nthis large number of trials",
    "start": "304920",
    "end": "310800"
  },
  {
    "text": "really underlies much of the\nvalue of probability. OK, in other words, you\ndon't need to use this",
    "start": "310800",
    "end": "316970"
  },
  {
    "text": "experimentation very often. But when you do, you really need\nit because that's what",
    "start": "316970",
    "end": "322919"
  },
  {
    "text": "you use to resolve conflicts,\nand to settle on things, and to have different people who are\nall trying to understand",
    "start": "322920",
    "end": "330960"
  },
  {
    "text": "what's going on have some\nidea of something they can agree on.",
    "start": "330960",
    "end": "337210"
  },
  {
    "text": "OK, so that's enough for\nprobability models. That's enough for philosophy. We will come back to this\nwith little bits and",
    "start": "337210",
    "end": "344740"
  },
  {
    "text": "pieces now and then. But at this point, we're really\ngoing into talking",
    "start": "344740",
    "end": "350669"
  },
  {
    "text": "about the mathematical\nmodels themselves. OK, so let's talk about the\nMarkov bound, the Chebyshev",
    "start": "350670",
    "end": "358310"
  },
  {
    "text": "bound, and the Chernoff bound. You should be reading the notes,\nso I hope you know what all these things are\nso I can go through",
    "start": "358310",
    "end": "365360"
  },
  {
    "text": "them relatively quickly. ",
    "start": "365360",
    "end": "370730"
  },
  {
    "text": "If you think that using these\nlectures slides that I'm passing out plus doing the\nproblems is sufficient for",
    "start": "370730",
    "end": "378000"
  },
  {
    "text": "understanding this\ncourse, you're really kidding yourself. I mean, the course is based on\nthis text, which explains",
    "start": "378000",
    "end": "387070"
  },
  {
    "text": "things much more fully. It still has errors in it. It still has typos in It.",
    "start": "387070",
    "end": "392639"
  },
  {
    "text": "But a whole lot fewer than these\nlecture slides, so you should be reading them and then\nusing them try to get a",
    "start": "392640",
    "end": "400800"
  },
  {
    "text": "better idea of what these\nlectures mean, and using that",
    "start": "400800",
    "end": "405810"
  },
  {
    "text": "to get the better idea\nof what the exercises you're doing mean.",
    "start": "405810",
    "end": "411090"
  },
  {
    "text": "Doing the exercises does not\ndo you any good whatsoever.",
    "start": "411090",
    "end": "416560"
  },
  {
    "text": "The only thing that does you\nsome good is to do an exercise and then think about what it\nhas to do with anything.",
    "start": "416560",
    "end": "423740"
  },
  {
    "text": "And if you don't do that second\npart, then all you're doing is you're building\na very,",
    "start": "423740",
    "end": "429290"
  },
  {
    "text": "very second rate computer. Your abilities as a computer\nare about the same for the",
    "start": "429290",
    "end": "436190"
  },
  {
    "text": "most part as the computer\nin a coffee maker. ",
    "start": "436190",
    "end": "442750"
  },
  {
    "text": "You are really not up to what\na TV set does anymore. I mean, TV sets can do so much\ncomputation that they're way",
    "start": "442750",
    "end": "451200"
  },
  {
    "text": "beyond your abilities\nat this point. So the only edge you have, the\nonly thing you can do to try",
    "start": "451200",
    "end": "458699"
  },
  {
    "text": "to make yourself worthwhile is\nto understand these things because computers cannot\ndo any of that",
    "start": "458700",
    "end": "465570"
  },
  {
    "text": "understanding at all. So you're way ahead\nof them there. OK, so what is the\nMarkov model?",
    "start": "465570",
    "end": "471735"
  },
  {
    "text": " What it says is, if y is a\nnon-negative random variable--",
    "start": "471735",
    "end": "479220"
  },
  {
    "text": "in other words, it's a random\nvariable, which only takes one non-negative sample values.",
    "start": "479220",
    "end": "484570"
  },
  {
    "text": "If it has an expectation,\nexpectation of y. And for any real y greater than\n0, the probability that Y",
    "start": "484570",
    "end": "493010"
  },
  {
    "text": "is greater than or equal to\nlittle y is the expected value of Y divided by little y.",
    "start": "493010",
    "end": "499820"
  },
  {
    "text": "The proof of it is by picture. If you don't like proofs by\npictures, you should get used",
    "start": "499820",
    "end": "505980"
  },
  {
    "text": "to it because we will prove a\ngreat number of things by pictures here. And I claim that a proof by\npicture is better than a proof",
    "start": "505980",
    "end": "513380"
  },
  {
    "text": "by algebra because if there's\nanything wrong with it, you can see from looking\nat it what it is.",
    "start": "513380",
    "end": "519309"
  },
  {
    "text": "So we know that the expected\nvalue of Y is the integral under the complimentary\ndistribution function.",
    "start": "519309",
    "end": "527680"
  },
  {
    "text": "This square in here, this area\nof Y times probability of Y greater or equal to Y. The\nprobability of Y greater than",
    "start": "527680",
    "end": "535770"
  },
  {
    "text": "or equal, capital Y, the random\nvariable y greater than or equal to the number,\nlittle y, is just that",
    "start": "535770",
    "end": "544030"
  },
  {
    "text": "point right up there. This is the point y. This is the point probability\nof capital Y",
    "start": "544030",
    "end": "550570"
  },
  {
    "text": "greater than little y. It doesn't make any difference\nwhen you're integrating whether you use a greater than\nor equal to sign or a",
    "start": "550570",
    "end": "560840"
  },
  {
    "text": "greater than sign. If you have a discontinuity,\nthe integral is the same no",
    "start": "560840",
    "end": "566260"
  },
  {
    "text": "matter which way\nyou look at it. So this area here is y times\nthe probability that random",
    "start": "566260",
    "end": "575740"
  },
  {
    "text": "variable y is greater than\nor equal to number y. And all that the Markov bound\nsays is that this little",
    "start": "575740",
    "end": "583380"
  },
  {
    "text": "rectangle here is less than or\nequal to the integral under that curve.",
    "start": "583380",
    "end": "588519"
  },
  {
    "text": "That's a perfectly\nrigorous proof. ",
    "start": "588520",
    "end": "593970"
  },
  {
    "text": "We don't really care about\nrigorous proofs here, anyway since we're trying to get at\nthe issue of how you use",
    "start": "593970",
    "end": "602530"
  },
  {
    "text": "probability, but we don't want\nto have proofs which mislead",
    "start": "602530",
    "end": "609050"
  },
  {
    "text": "you about things. In other words, proofs\nwhich aren't right. So we try to be right, and I\nwant you to learn to be right.",
    "start": "609050",
    "end": "618490"
  },
  {
    "text": "But I don't want you to start\nto worry too much about looking like a mathematician\nwhen you prove things.",
    "start": "618490",
    "end": "627020"
  },
  {
    "text": "OK, the Chebyshev inequality. If Z has a mean--",
    "start": "627020",
    "end": "632570"
  },
  {
    "text": "and when you say Z has a mean,\nwhat you really mean is the",
    "start": "632570",
    "end": "638690"
  },
  {
    "text": "expected value of the absolute\nvalue of Z is finite.",
    "start": "638690",
    "end": "644450"
  },
  {
    "text": "And it has a variance sigma\nsquared of Z. That's saying a little more than just\nhaving a mean.",
    "start": "644450",
    "end": "651520"
  },
  {
    "text": "Then, for any epsilon\ngreater than 0. In other words, this bound\nworks for any epsilon.",
    "start": "651520",
    "end": "657310"
  },
  {
    "text": "The probability that the\nabsolute value of Z less than the mean. In other words, that it's\nfurther away from the mean by",
    "start": "657310",
    "end": "665529"
  },
  {
    "text": "more than epsilon, is less than\nor equal to the variance of Z divided by epsilon\nsquared.",
    "start": "665530",
    "end": "673139"
  },
  {
    "text": "Again, this is a very weak\nbound, but it's very general. And therefore, it's\nvery useful.",
    "start": "673140",
    "end": "680220"
  },
  {
    "text": "And the proof is simplicity\nitself. You define a new random variable\ny, which is Z minus",
    "start": "680220",
    "end": "689670"
  },
  {
    "text": "the expected value of\nZ quantity squared. The expected value of Y then is\nthe expected value of this,",
    "start": "689670",
    "end": "697990"
  },
  {
    "text": "which is just the\nvariance of Z. So for any y greater than 0,\nwe'll use the Markov bound,",
    "start": "697990",
    "end": "705300"
  },
  {
    "text": "which says the probability\nthat random variable Y is greater than or equal to number\nlittle y is less than",
    "start": "705300",
    "end": "712090"
  },
  {
    "text": "or equal to sigma\nof Z squared. Namely, the expected value of\nrandom variable Y divided by",
    "start": "712090",
    "end": "718740"
  },
  {
    "text": "number Y. That's just\nthe Markov bound. And then, random variable Y is\ngreater than or equal to",
    "start": "718740",
    "end": "726560"
  },
  {
    "text": "number y if and only if the\npositive square root of capital Y-- we're dealing only\nwith positive non-negative",
    "start": "726560",
    "end": "734140"
  },
  {
    "text": "things here-- is greater than or equal to the\nsquare root of number y. And that's less than or equal\nto sigma Z squared over y.",
    "start": "734140",
    "end": "743700"
  },
  {
    "text": "And square root of Y is\njust the magnitude of Z minus Z bar.",
    "start": "743700",
    "end": "749820"
  },
  {
    "text": "We're setting epsilon equal to\nsquare root of y yields the Chebyshev bound.",
    "start": "749820",
    "end": "756350"
  },
  {
    "text": "Now, that's something which\nI don't believe",
    "start": "756350",
    "end": "764139"
  },
  {
    "text": "in memorizing proofs. I think that's a\nterrible idea. But that's something so simple\nand so often used that you",
    "start": "764140",
    "end": "773080"
  },
  {
    "text": "just ought to think\nin those terms. You ought to be able to see\nthat diagram of the Markov inequality.",
    "start": "773080",
    "end": "778800"
  },
  {
    "text": "You ought to be able to\nsee why it is true. And you ought to understand\nit well enough",
    "start": "778800",
    "end": "783980"
  },
  {
    "text": "that you can use it. In other words, there's a big\ndifference in mathematics",
    "start": "783980",
    "end": "789480"
  },
  {
    "text": "between knowing what a theorem\nsays and knowing that the theorem is true, and\nreally having a gut",
    "start": "789480",
    "end": "797020"
  },
  {
    "text": "feeling for that theorem. I mean, you know this when\nyou deal with numbers. You know it when you deal\nwith integration or",
    "start": "797020",
    "end": "803030"
  },
  {
    "text": "differentiation, or any of\nthose things you've known about for a long time.",
    "start": "803030",
    "end": "808040"
  },
  {
    "text": "There's a big difference between\nthe things that you can really work with because\nyou understand them and you see them and those things that\nyou just know as something you",
    "start": "808040",
    "end": "819490"
  },
  {
    "text": "don't really understand.  This is something that you\nreally ought to understand and",
    "start": "819490",
    "end": "828120"
  },
  {
    "text": "be able to see it. OK, the Chernoff bound\nis the last of these. We will use this a great deal.",
    "start": "828120",
    "end": "834690"
  },
  {
    "text": "It's a generating\nfunction bound. And it says, for any number,\npositive number z, and any",
    "start": "834690",
    "end": "842990"
  },
  {
    "text": "positive number r greater than\n0, such that the moment generating function-- the moment\ngenerating function of",
    "start": "842990",
    "end": "849620"
  },
  {
    "text": "a random variable z is a\nfunction given the random variable z.",
    "start": "849620",
    "end": "855600"
  },
  {
    "text": "It's a function of\na real number r. And that function is the\nexpected value of e to the rZ.",
    "start": "855600",
    "end": "864709"
  },
  {
    "text": "It's called the generating\nfunction because if you start taking derivatives of this and\nevaluate them, that r equals",
    "start": "864710",
    "end": "870879"
  },
  {
    "text": "0, what you get is the\nvarious moments of z. You've probably seen\nthat at some point.",
    "start": "870880",
    "end": "876410"
  },
  {
    "text": "If you haven't seen it, it's not\nimportant here because we don't use that at all.",
    "start": "876410",
    "end": "881580"
  },
  {
    "text": "What we really use is the fact\nthat this is a function. It's a function, which is\nincreasing as r increases",
    "start": "881580",
    "end": "890130"
  },
  {
    "text": "because you put-- well, it just does.",
    "start": "890130",
    "end": "896100"
  },
  {
    "text": "And what it says is the\nprobability that this random variable is greater than or\nequal to the number z--",
    "start": "896100",
    "end": "901430"
  },
  {
    "text": "I should really use different\nletters for these things. It's hard to talk about them-- is less than or equal to the\nmoment generating function",
    "start": "901430",
    "end": "908560"
  },
  {
    "text": "times e to the minus rZ. And the proof is exactly the\nsame as the proof before.",
    "start": "908560",
    "end": "914340"
  },
  {
    "text": "You might get the picture that\nyou can prove many, many different things from the\nMarkov inequality.",
    "start": "914340",
    "end": "919420"
  },
  {
    "text": "And in fact, you can. You just put in whatever you\nwant to and you get a new",
    "start": "919420",
    "end": "924580"
  },
  {
    "text": "inequality. And you can call it after\nyourself if you want.",
    "start": "924580",
    "end": "930070"
  },
  {
    "text": "I mean, Chernoff. Chernoff is still alive. Chernoff is a faculty\nmember at Harvard.",
    "start": "930070",
    "end": "937459"
  },
  {
    "text": "And this is kind of curious\nbecause he sort of slipped this in in a paper that he wrote\nwhere he was trying to",
    "start": "937460",
    "end": "943089"
  },
  {
    "text": "prove something difficult. And this is a relationship that\nmany mathematicians have",
    "start": "943090",
    "end": "949740"
  },
  {
    "text": "used over many, many years. And it's so simple that they\ndidn't make any fuss about it.",
    "start": "949740",
    "end": "955400"
  },
  {
    "text": "And he didn't make any\nfuss about it. And he was sort of embarrassed\nthat many engineers, starting",
    "start": "955400",
    "end": "961800"
  },
  {
    "text": "with Claude Shannon, found this\nto be extraordinarily useful, and started calling\nit the Chernoff bound.",
    "start": "961800",
    "end": "967610"
  },
  {
    "text": "He was slightly embarrassed of\nhaving this totally trivial thing suddenly be\nnamed after him.",
    "start": "967610",
    "end": "974300"
  },
  {
    "text": "But anyway, that's the\nway it happened. And now it's a widely used tool\nthat we use all the time.",
    "start": "974300",
    "end": "982020"
  },
  {
    "text": "So it's the same proof that we\nhad before for any y greater 0, Markov says this.",
    "start": "982020",
    "end": "988740"
  },
  {
    "text": "And therefore, you get that. This decreases exponentially\nwith Z, and",
    "start": "988740",
    "end": "994510"
  },
  {
    "text": "that's why it's useful. I mean, the Markov inequality\nonly decays as",
    "start": "994510",
    "end": "1000730"
  },
  {
    "text": "1 over little y. The Chebyshev inequality decays\nas 1 over y squared.",
    "start": "1000730",
    "end": "1008880"
  },
  {
    "text": "This decays exponentially\nwith y. And therefore, when you start\ndealing with large deviations,",
    "start": "1008880",
    "end": "1015209"
  },
  {
    "text": "trying to talk about things that\nare very, very unlikely when you get very, very far from\nthe mean, this is a very",
    "start": "1015210",
    "end": "1023420"
  },
  {
    "text": "useful way to do it. And it's sort of the standard\nway of doing it at this point. We won't use it right now, but\nthis is the right time to talk",
    "start": "1023420",
    "end": "1032530"
  },
  {
    "text": "about it a little bit. OK, next topic we want to take\nup is really these laws of",
    "start": "1032530",
    "end": "1037579"
  },
  {
    "text": "large numbers, and something\nabout convergence. We want to understand a\nlittle bit about that.",
    "start": "1037579",
    "end": "1045579"
  },
  {
    "text": "And this picture that we've seen\nbefore, we take X1, up to",
    "start": "1045579",
    "end": "1053390"
  },
  {
    "text": "Xn as n independent identically distributed random variables.",
    "start": "1053390",
    "end": "1060210"
  },
  {
    "text": "They each have mean expected\nvalue of X. They each have variance sigma squared.",
    "start": "1060210",
    "end": "1065880"
  },
  {
    "text": "You let Sn be the sum\nof all of them. What we want to understand is,\nhow does S sub n behave?",
    "start": "1065880",
    "end": "1073400"
  },
  {
    "text": "And more particularly,\nhow does Sn over n-- namely, the relative--",
    "start": "1073400",
    "end": "1080003"
  },
  {
    "text": "not the relative frequency, but\nthe sample average of x",
    "start": "1080003",
    "end": "1085230"
  },
  {
    "text": "behave when you take\nn samples? So this curve shows the\ndistribution function of S4,",
    "start": "1085230",
    "end": "1093520"
  },
  {
    "text": "of S20, and of S50 when you have\na binary random variable",
    "start": "1093520",
    "end": "1100700"
  },
  {
    "text": "with probability of 1 equal to\na quarter, probability of 0 equal to 3/4.",
    "start": "1100700",
    "end": "1106970"
  },
  {
    "text": "And what you see graphically\nis what you can see mathematically very\neasily, too.",
    "start": "1106970",
    "end": "1113080"
  },
  {
    "text": "The mean value of S sub\nn is n times X bar.",
    "start": "1113080",
    "end": "1118570"
  },
  {
    "text": "So the center point in these\ncurves is moving out within. You see the center point here.",
    "start": "1118570",
    "end": "1124650"
  },
  {
    "text": "Center point somewhere\naround there. Actually, the center point is\nat-- yeah, just about what it",
    "start": "1124650",
    "end": "1132950"
  },
  {
    "text": "looks like. And the center point here is\nout somewhere around there.",
    "start": "1132950",
    "end": "1140179"
  },
  {
    "text": "And you see the variance, you\nsee the variance going up linearly with n.",
    "start": "1140180",
    "end": "1146700"
  },
  {
    "text": "Not with n squared, but with\nn, which means the standard deviation is going up with\nthe square root of n.",
    "start": "1146700",
    "end": "1154750"
  },
  {
    "text": "That's sort of why the law\nof large numbers works. It's because the standard\ndeviation of these random--",
    "start": "1154750",
    "end": "1162220"
  },
  {
    "text": "of these sums only goes up with\nthe square root of n. So these curves, along with\nmoving out, become relatively",
    "start": "1162220",
    "end": "1171519"
  },
  {
    "text": "more compressed relative to\nhow far out they are. ",
    "start": "1171520",
    "end": "1180940"
  },
  {
    "text": "This curve here is relatively\nmore compressed for its mean",
    "start": "1180940",
    "end": "1186519"
  },
  {
    "text": "than this one is here. And that's more compressed\nrelative to this one. We get this a lot more\neasily if we look",
    "start": "1186520",
    "end": "1195210"
  },
  {
    "text": "at the sample average. Namely, S sub n over n. This is a random variable\nof mean X bar.",
    "start": "1195210",
    "end": "1203250"
  },
  {
    "text": "That's a random variable of\nvariance sigma squared over n. That's something that you ought\nto just recognize and",
    "start": "1203250",
    "end": "1210769"
  },
  {
    "text": "have very close to the top of\nyour consciousness because that again, is sort\nof why the sample",
    "start": "1210770",
    "end": "1216510"
  },
  {
    "text": "average starts to converge. So what happens then is for n\nequals 4, you get this very",
    "start": "1216510",
    "end": "1224880"
  },
  {
    "text": "\"blech\" curve. For n equals 20, it starts\nlooking a little more reasonable.",
    "start": "1224880",
    "end": "1230490"
  },
  {
    "text": "For n equals 50, it's starting\nto scrunch in and start to look like a unit step.",
    "start": "1230490",
    "end": "1238270"
  },
  {
    "text": "And what we'll find is that the\nintuitive way of looking at the law of large numbers, or\none of the more intuitive",
    "start": "1238270",
    "end": "1248090"
  },
  {
    "text": "ways of looking at it,\nis that the sample average starts to look-- starts to have a distribution\nfunction, which",
    "start": "1248090",
    "end": "1257000"
  },
  {
    "text": "looks like a unit step. And that step occurs\nat the mean. So this curve here keeps\nscrunching in.",
    "start": "1257000",
    "end": "1265310"
  },
  {
    "text": "This part down here is\nmoving over that way. This part over here is\nmoving over that way.",
    "start": "1265310",
    "end": "1271789"
  },
  {
    "text": "And it all gets close\nto a unit step. OK, the variance of Sn over n,\nas we've said, is equal to",
    "start": "1271790",
    "end": "1280350"
  },
  {
    "text": "sigma squared over n. The limit of the variance as n\ngoes to infinity takes the",
    "start": "1280350",
    "end": "1287960"
  },
  {
    "text": "limit of that. Don't even have to know the\ndefinition of a limit, can see that when n gets large,\nthis get small.",
    "start": "1287960",
    "end": "1296070"
  },
  {
    "text": "And this goes to 0. So the limit of this\ngoes to 0.",
    "start": "1296070",
    "end": "1301400"
  },
  {
    "text": "Now, here's the important\nthing. This equation says a whole lot\nmore than this equation says.",
    "start": "1301400",
    "end": "1312240"
  },
  {
    "text": "Because this equation says how\nquickly that approaches 0.",
    "start": "1312240",
    "end": "1318090"
  },
  {
    "text": "All this says is it\napproaches 0. So we've thrown away a lot that\nwe know, and now all we",
    "start": "1318090",
    "end": "1326150"
  },
  {
    "text": "know is this. This 3 says that the convergence\nis as 1/n.",
    "start": "1326150",
    "end": "1335720"
  },
  {
    "text": "This doesn't say that. This just says that\nit converges. Why would anyone in their right\nmind want to replace an",
    "start": "1335720",
    "end": "1343660"
  },
  {
    "text": "informative statement like this\nwith a not informative statement like this?",
    "start": "1343660",
    "end": "1350240"
  },
  {
    "text": "Any ideas of why you might\nwant to do that? Any suggestions? ",
    "start": "1350240",
    "end": "1358789"
  },
  {
    "text": "AUDIENCE: Convenience. PROFESSOR: What? AUDIENCE: Convenience. Convenience. Sometimes you don't need to--",
    "start": "1358790",
    "end": "1364130"
  },
  {
    "text": "PROFESSOR: Well, yes,\nconvenience. But there's a much\nstronger reason. ",
    "start": "1364130",
    "end": "1371750"
  },
  {
    "text": "This is a statement for\nIID random variables. This law of large numbers, we\nwant it to apply to as many",
    "start": "1371750",
    "end": "1379500"
  },
  {
    "text": "different situations\nas possible. To things that aren't\nquite IID. To things that don't\nhave a variance.",
    "start": "1379500",
    "end": "1388150"
  },
  {
    "text": "And this statement here\nis going to apply more",
    "start": "1388150",
    "end": "1395030"
  },
  {
    "text": "generally than this. You can have situations where\nthe variance goes to 0 more",
    "start": "1395030",
    "end": "1400610"
  },
  {
    "text": "slowly than 1/n if these random\nvariables are not independent.",
    "start": "1400610",
    "end": "1405990"
  },
  {
    "text": "But you still have\nthis statement. And this statement is what we\nreally need, so this really",
    "start": "1405990",
    "end": "1415170"
  },
  {
    "text": "says something, which is called convergence and mean square.",
    "start": "1415170",
    "end": "1420230"
  },
  {
    "text": "Why mean square? Because this is the\nmean squared. So obvious terminology.",
    "start": "1420230",
    "end": "1427380"
  },
  {
    "text": "Mathematicians aren't always\nvery good at choosing terminology that makes sense\nwhen you look at it,",
    "start": "1427380",
    "end": "1434240"
  },
  {
    "text": "but this one does. Definition is a sequence of\nrandom variables Y1, Y2, Y3,",
    "start": "1434240",
    "end": "1440800"
  },
  {
    "text": "and so forth. Converges in mean square to a\nrandom variable Y if this",
    "start": "1440800",
    "end": "1446559"
  },
  {
    "text": "limit here is equal to 0. So in this case, Y, this random\nvariable Y, is really a",
    "start": "1446560",
    "end": "1457130"
  },
  {
    "text": "deterministic random variable,\nwhich is just the deterministic value, expected\nvalue of X. This random",
    "start": "1457130",
    "end": "1465290"
  },
  {
    "text": "variable here is this relative\nfrequency here. And this is saying that the\nexpected value of the relative",
    "start": "1465290",
    "end": "1473390"
  },
  {
    "text": "frequency relative to the\nexpected value of X-- this is going to 0.",
    "start": "1473390",
    "end": "1479659"
  },
  {
    "text": "This isn't saying\nanything extra. This is just saying, if you're\nnot interested in the law of",
    "start": "1479660",
    "end": "1484670"
  },
  {
    "text": "large numbers, you might be\ninterested in how a bunch of random variables approach some\nother random variable.",
    "start": "1484670",
    "end": "1493030"
  },
  {
    "text": "Now, if you look at a set of\nreal numbers and you say, does that set of real numbers\napproach something?",
    "start": "1493030",
    "end": "1502500"
  },
  {
    "text": "I mean, you have sort of a\ncomplicated looking definition for that, which really\nsays that the numbers",
    "start": "1502500",
    "end": "1510590"
  },
  {
    "text": "approach this constant. But a set of numbers is so much\nmore simple-minded than a",
    "start": "1510590",
    "end": "1518860"
  },
  {
    "text": "set of random variables. I mean, a set of random\nvariables is-- I mean, not even their\ndistribution functions really",
    "start": "1518860",
    "end": "1528100"
  },
  {
    "text": "explain what they are. There's also the relationship\nbetween the distribution functions.",
    "start": "1528100",
    "end": "1533390"
  },
  {
    "text": "So you're not going to find\nanything very easy that says",
    "start": "1533390",
    "end": "1538550"
  },
  {
    "text": "random variables converge. And you can expect to find the\nnumber of different kinds of statements about convergence.",
    "start": "1538550",
    "end": "1547020"
  },
  {
    "text": "And this is just going\nto be one of them. This is something called\nconvergence and mean square--",
    "start": "1547020",
    "end": "1553010"
  },
  {
    "text": "yes? AUDIENCE: Going from 3 to 4,\nwe don't need IID anymore?",
    "start": "1553010",
    "end": "1562296"
  },
  {
    "text": "So they can be just-- PROFESSOR: You can certainly\nfind examples where it's not",
    "start": "1562296",
    "end": "1568790"
  },
  {
    "text": "IID, and this doesn't hold\nand this does hold. The most interesting case where\nthis doesn't hold and",
    "start": "1568790",
    "end": "1576200"
  },
  {
    "text": "this does hold is where you-- no, you still need a variance\nfor this to hold.",
    "start": "1576200",
    "end": "1583800"
  },
  {
    "text": " Yeah, so I guess I can't really\nconstruct any nice",
    "start": "1583800",
    "end": "1590929"
  },
  {
    "text": "examples of n where this holds\nand this doesn't hold.",
    "start": "1590930",
    "end": "1597300"
  },
  {
    "text": "But there are some if you talk\nabout random variables that are not IID. ",
    "start": "1597300",
    "end": "1604580"
  },
  {
    "text": "I ought to have a problem\nthat does that. But so far, I don't. ",
    "start": "1604580",
    "end": "1612690"
  },
  {
    "text": "Now, the fact that this sample\naverage converges in mean square doesn't tell us directly\nwhat might be more",
    "start": "1612690",
    "end": "1620640"
  },
  {
    "text": "interesting. I mean, you look at that\nstatement and it doesn't really tell you what this\ncomplementary distribution",
    "start": "1620640",
    "end": "1629549"
  },
  {
    "text": "function looks like. I mean, to me the thing that\nis closest to what I would",
    "start": "1629550",
    "end": "1636360"
  },
  {
    "text": "think of as convergence is that\nthis sequence of random",
    "start": "1636360",
    "end": "1642429"
  },
  {
    "text": "variables minus the random\nvariable, the convergence of",
    "start": "1642430",
    "end": "1647680"
  },
  {
    "text": "that difference, approaches a\ndistribution function, which",
    "start": "1647680",
    "end": "1653250"
  },
  {
    "text": "is the unit step. Which means that the probability\nthat you're anywhere off of that center\npoint is going to 0.",
    "start": "1653250",
    "end": "1664380"
  },
  {
    "text": "I mean, that's a very easy\nto interpret statement. The fact that the variance is\ngoing to 0, I don't quite know",
    "start": "1664380",
    "end": "1673549"
  },
  {
    "text": "how do interpret it, except\nthrough Chebyshev's law, which gets me to the other\nstatement.",
    "start": "1673550",
    "end": "1679520"
  },
  {
    "text": "So what I'm saying here is if\nwe apply Chebyshev to that",
    "start": "1679520",
    "end": "1685680"
  },
  {
    "text": "statement before number 3-- this one-- which says what the\nvariance is.",
    "start": "1685680",
    "end": "1692540"
  },
  {
    "text": "If we apply Chebyshev, then what\nwe get is the probability",
    "start": "1692540",
    "end": "1697840"
  },
  {
    "text": "that the relative frequency\nminus the mean, the absolute",
    "start": "1697840",
    "end": "1704049"
  },
  {
    "text": "value of that is greater than\nor equal to epsilon. That probability is less than or\nequal to sigma squared over",
    "start": "1704050",
    "end": "1711529"
  },
  {
    "text": "n times epsilon squared. You'll notice this is\na very peculiar",
    "start": "1711530",
    "end": "1717030"
  },
  {
    "text": "statement in terms of epsilon. Because if you want to make\nepsilon very small, so you get",
    "start": "1717030",
    "end": "1724700"
  },
  {
    "text": "something strong here,\nthis term blows up.",
    "start": "1724700",
    "end": "1731409"
  },
  {
    "text": "So the way you have to look at\nthis is pick some epsilon you're happy with.",
    "start": "1731410",
    "end": "1736980"
  },
  {
    "text": " I mean, you might want these two\nthings to be within 1% of",
    "start": "1736980",
    "end": "1744760"
  },
  {
    "text": "each other. Then, epsilon squared\nhere is 10,000.",
    "start": "1744760",
    "end": "1754020"
  },
  {
    "text": "But by making n big enough,\nthat gets submerged. So excuse me.",
    "start": "1754020",
    "end": "1761019"
  },
  {
    "text": "Epsilon squared is 1/10,000\nso 1 over",
    "start": "1761020",
    "end": "1766920"
  },
  {
    "text": "epsilon squared is 10,000. So you need to make\nn very large. Yes? AUDIENCE: So that's why at times\nwhen n is too small and",
    "start": "1766920",
    "end": "1774406"
  },
  {
    "text": "epsilon is too small as well,\nyou can get obvious things, like it's less than or equal\nto a number greater than 1? PROFESSOR: Yes.",
    "start": "1774406",
    "end": "1779526"
  },
  {
    "text": "And this inequality is not much\ngood because there's a very obvious inequality\nthat works.",
    "start": "1779526",
    "end": "1785120"
  },
  {
    "text": "Yes. But the other thing is this\nis a very weak inequality.",
    "start": "1785120",
    "end": "1790580"
  },
  {
    "text": "So all this is doing is\ngiving you a bound. All it's doing is saying that\nwhen n gets big enough, this",
    "start": "1790580",
    "end": "1799230"
  },
  {
    "text": "number gets as small as\nyou want it to be. So you can get an arbitrary\naccuracy of epsilon between",
    "start": "1799230",
    "end": "1806040"
  },
  {
    "text": "sample average and mean. You can get that with\na probability 1 minus this quantity.",
    "start": "1806040",
    "end": "1813450"
  },
  {
    "text": "You can make that as close\nto 1 as you wish if you increase n.",
    "start": "1813450",
    "end": "1818679"
  },
  {
    "text": "So that gives us the law of\nlarge numbers, and I haven't stated it formally, all the\nformal jazz as in the notes.",
    "start": "1818680",
    "end": "1827450"
  },
  {
    "text": "But it says, if you have IID\nrandom variables with a finite variance, the limit of the\nprobability that Sn over n",
    "start": "1827450",
    "end": "1835799"
  },
  {
    "text": "minus x bar, the absolute value\nof that is greater than or equal to epsilon is equal to\n0 in the limit, no matter",
    "start": "1835800",
    "end": "1843360"
  },
  {
    "text": "how you choose epsilon. Namely, this is one of those\npeculiar things in mathematics.",
    "start": "1843360",
    "end": "1849760"
  },
  {
    "text": "It depends on who gets\nthe first choice. If I get to choose epsilon\nand you get to",
    "start": "1849760",
    "end": "1856420"
  },
  {
    "text": "choose n, then you win. You can make this go to 0. If you choose n and then I\nchoose epsilon, you lose.",
    "start": "1856420",
    "end": "1864430"
  },
  {
    "text": "So it's only when you choose\nfirst that you win. But still, this statement\nworks. For every epsilon greater\nthan 0, this limit",
    "start": "1864430",
    "end": "1871940"
  },
  {
    "text": "here is equal to 0. Now, let's go immediately a\ncouple of pages beyond and",
    "start": "1871940",
    "end": "1882266"
  },
  {
    "text": "look at this figure a little\nbit because I think this figure tells what's going\non, I think better",
    "start": "1882266",
    "end": "1889289"
  },
  {
    "text": "than anything else. You have the mean of x, which is\nright in the center of this distribution function.",
    "start": "1889290",
    "end": "1896740"
  },
  {
    "text": "As n gets larger and larger,\nthis distribution function here is going to be scrunching\nin, which we sort of know",
    "start": "1896740",
    "end": "1904060"
  },
  {
    "text": "because the variance\nis going to 0. And we also sort of know it\nbecause of what this weak law",
    "start": "1904060",
    "end": "1910210"
  },
  {
    "text": "of large numbers tells us. And we have these.",
    "start": "1910210",
    "end": "1915390"
  },
  {
    "text": " If we pick some given epsilon,\nthen we have--",
    "start": "1915390",
    "end": "1922220"
  },
  {
    "text": "if we look at a range of two\nepsilon, epsilon on one side of the mean, epsilon on the\nother side of the mean, then",
    "start": "1922220",
    "end": "1931029"
  },
  {
    "text": "we can ask the question, how\nwell does this distribution",
    "start": "1931030",
    "end": "1936140"
  },
  {
    "text": "function conform\nto a unit step? Well, one easy way of looking at\nthat is saying, if we draw",
    "start": "1936140",
    "end": "1943830"
  },
  {
    "text": "a rectangle here of width 2\nepsilon around X bar, when",
    "start": "1943830",
    "end": "1949149"
  },
  {
    "text": "does this distribution function\nget inside that rectangle and when does it\nleave the rectangle?",
    "start": "1949150",
    "end": "1957280"
  },
  {
    "text": "And what the weak law of large\nnumbers says is that if you",
    "start": "1957280",
    "end": "1962930"
  },
  {
    "text": "pick epsilon and hold it fixed,\nthen delta 1 is going to 0 and delta 2\nis going to 0.",
    "start": "1962930",
    "end": "1971340"
  },
  {
    "text": "And eventually-- ",
    "start": "1971340",
    "end": "1976862"
  },
  {
    "text": "I think this is dying out. Well, no problem. ",
    "start": "1976863",
    "end": "1984509"
  },
  {
    "text": "What this says is that as n gets\nlarger and larger, this quantity shrinks down to 0.",
    "start": "1984510",
    "end": "1991660"
  },
  {
    "text": "That quantity up there\nshrinks down to 0. And suddenly, you have something\nwhich is, for all",
    "start": "1991660",
    "end": "1997950"
  },
  {
    "text": "practical purposes,\na unit step.",
    "start": "1997950",
    "end": "2003120"
  },
  {
    "text": "Namely, if you think about it a\nlittle bit, how can you take an increasing curve, which\nincreases from 0 to 1, and say",
    "start": "2003120",
    "end": "2010450"
  },
  {
    "text": "that's close to a unit step? Isn't this a nice\nway of doing it? ",
    "start": "2010450",
    "end": "2018590"
  },
  {
    "text": "I mean, the function is\nincreasing so it can't do anything after it crosses\nthis point here.",
    "start": "2018590",
    "end": "2025450"
  },
  {
    "text": "All it can do is increase, and\neventually it leaves again.",
    "start": "2025450",
    "end": "2030990"
  },
  {
    "text": "Now, another thing. When you think about the weak\nlaw of large numbers and you",
    "start": "2030990",
    "end": "2039020"
  },
  {
    "text": "don't state it formally, one of\nthe important things is you",
    "start": "2039020",
    "end": "2044890"
  },
  {
    "text": "can't make epsilon 0 here and\nyou can't make delta 0.",
    "start": "2044890",
    "end": "2050908"
  },
  {
    "text": "You need both an epsilon and\na delta in this argument. And you can see that just by\nlooking at a reasonable",
    "start": "2050909",
    "end": "2056789"
  },
  {
    "text": "distribution function. If you make epsilon equal to 0,\nthen you're asking, what's",
    "start": "2056790",
    "end": "2063550"
  },
  {
    "text": "the probability that this sample\naverage is exactly",
    "start": "2063550",
    "end": "2068940"
  },
  {
    "text": "equal to the mean? And in most cases, that's\nequal to 0.",
    "start": "2068940",
    "end": "2075919"
  },
  {
    "text": "Namely, you can't win\non that argument. And if you try to make\ndelta equal to 0.",
    "start": "2075920",
    "end": "2085800"
  },
  {
    "text": "In other words, you ask-- ",
    "start": "2085800",
    "end": "2090850"
  },
  {
    "text": "then suddenly you're stuck over\nhere, and you're stuck way over there, and you can't\nmake epsilon small.",
    "start": "2090850",
    "end": "2098820"
  },
  {
    "text": "So trying to say that a curve\nlooks like a step function, you really need two fudge\nfactors to do that.",
    "start": "2098820",
    "end": "2107290"
  },
  {
    "text": "So the weak law of\nlarge numbers. In terms of dealing with how\nclose you are to a step",
    "start": "2107290",
    "end": "2118119"
  },
  {
    "text": "function, the weak law of large\nnumbers says about the only thing you can\nreasonably say.",
    "start": "2118120",
    "end": "2124609"
  },
  {
    "text": "OK, now let's go back\nto the slide before. The weak law of large numbers\nsays that the limit as n goes",
    "start": "2124610",
    "end": "2131290"
  },
  {
    "text": "to infinity of the probability\nthat the sample average is greater than or equal\nto epsilon equals 0.",
    "start": "2131290",
    "end": "2139180"
  },
  {
    "text": "And it says that for every\nepsilon greater than 0. ",
    "start": "2139180",
    "end": "2145160"
  },
  {
    "text": "An equivalent statement is\nthis statement here. ",
    "start": "2145160",
    "end": "2152350"
  },
  {
    "text": "The probability that Sn over n\nminus x bar is greater than or equal to epsilon is a\ncomplicated looking animal,",
    "start": "2152350",
    "end": "2160130"
  },
  {
    "text": "but it's just a number. It's just a number\nbetween 0 and 1. It's a probability. So for every n, you get\na number up there.",
    "start": "2160130",
    "end": "2168460"
  },
  {
    "text": "And what this is saying is that\nthat sequence of numbers",
    "start": "2168460",
    "end": "2173680"
  },
  {
    "text": "is approaching 0. Another way to say that a\nsequence of numbers approaches",
    "start": "2173680",
    "end": "2179559"
  },
  {
    "text": "0 is this way down here. It says that for every epsilon\ngreater than 0 and every delta",
    "start": "2179560",
    "end": "2187270"
  },
  {
    "text": "greater than 0, the probability\nthat this quantity",
    "start": "2187270",
    "end": "2193000"
  },
  {
    "text": "is less than or equal\nto delta is--",
    "start": "2193000",
    "end": "2198450"
  },
  {
    "text": "this probability is less than\nor equal to delta for all large enough n.",
    "start": "2198450",
    "end": "2203910"
  },
  {
    "text": "In other words, these funny\nlittle things on the edge here for this next slide, the delta\n1 and delta 2 are going to 0.",
    "start": "2203910",
    "end": "2216930"
  },
  {
    "text": "So it's important to understand\nthis both ways. ",
    "start": "2216930",
    "end": "2224930"
  },
  {
    "text": "And now again, this quantity\nhere looks very much like--",
    "start": "2224930",
    "end": "2230579"
  },
  {
    "start": "2230580",
    "end": "2241740"
  },
  {
    "text": "these two equations look\nvery much alike. Except this one tells you\nsomething more about",
    "start": "2241740",
    "end": "2247470"
  },
  {
    "text": "convergence than\nthis one does. This says how this goes to 0. This only says that\nit goes to 0.",
    "start": "2247470",
    "end": "2254260"
  },
  {
    "text": "So again, we have\nthe same thing. The weak law of large numbers\nsays this weaker thing, and it",
    "start": "2254260",
    "end": "2261800"
  },
  {
    "text": "says this weaker thing\nbecause sometimes you need the weaker thing. And in this case, there\nis a good example.",
    "start": "2261800",
    "end": "2269200"
  },
  {
    "text": "The weak law of large numbers\nis true, even if you don't have a variance.",
    "start": "2269200",
    "end": "2274490"
  },
  {
    "text": "It's true under the\nsingle condition that you have a mean. There's a nice proof in\nthe text about that.",
    "start": "2274490",
    "end": "2283539"
  },
  {
    "text": "It's a proof that does\nsomething, which we're going",
    "start": "2283540",
    "end": "2288770"
  },
  {
    "text": "to do many, many times. You look at a random variable. You can't say what you want\nto say about it, so",
    "start": "2288770",
    "end": "2295990"
  },
  {
    "text": "you truncate it. You say, let me--",
    "start": "2295990",
    "end": "2301910"
  },
  {
    "text": "I mean, if you think a problem\nis too hard, you look at a",
    "start": "2301910",
    "end": "2307520"
  },
  {
    "text": "simpler problem. If you're drunk and you drop\na coin, you look for it underneath a light.",
    "start": "2307520",
    "end": "2313010"
  },
  {
    "text": "You don't look for it where\nit's dark, even though you dropped it where it's dark. So all of us do that.",
    "start": "2313010",
    "end": "2318619"
  },
  {
    "text": "If we can't solve a problem,\nwe try to pose a simpler, similar problem that\nwe can solve.",
    "start": "2318620",
    "end": "2324360"
  },
  {
    "text": "So you truncate this\nrandom variable. When you truncate a random\nvariable, I mean you just take",
    "start": "2324360",
    "end": "2330190"
  },
  {
    "text": "its distribution function\nand you chop it off to a certain point.",
    "start": "2330190",
    "end": "2335470"
  },
  {
    "text": "And what happens then? Well. at that point you\nhave a variance. You have a moment generating\nfunction.",
    "start": "2335470",
    "end": "2341010"
  },
  {
    "text": "You have all the things\nyou want. Nothing peculiar can happen\nbecause the thing is bounded.",
    "start": "2341010",
    "end": "2347160"
  },
  {
    "text": "So then the trick in proving the\nweak law of large numbers under these more general\ncircumstances is to first",
    "start": "2347160",
    "end": "2354620"
  },
  {
    "text": "truncate the random variable. You then have the weak\nlaw of large numbers. And then the thing that you do\nis in a very ticklish way, you",
    "start": "2354620",
    "end": "2362890"
  },
  {
    "text": "start increasing n,\nand you increase the truncation parameter. And if you do this in just the\nright way, you wind up proving",
    "start": "2362890",
    "end": "2371550"
  },
  {
    "text": "the theorem you want to prove. Now, I'm not saying you ought\nto read that proof now. ",
    "start": "2371550",
    "end": "2378710"
  },
  {
    "text": "If you're sailing along with no\nproblems at all, you ought to read that proof now.",
    "start": "2378710",
    "end": "2383990"
  },
  {
    "text": "If you don't quite have the\nkind of mathematical background that I seem to be\noften assuming in this course,",
    "start": "2383990",
    "end": "2391670"
  },
  {
    "text": "you ought to skip that. You will have many opportunities\nto understand",
    "start": "2391670",
    "end": "2396740"
  },
  {
    "text": "the technique later. It's the technique which is\nimportant, it's not the--",
    "start": "2396740",
    "end": "2401900"
  },
  {
    "text": "I mean, it's not so much\nthe actual proof. ",
    "start": "2401900",
    "end": "2409530"
  },
  {
    "text": "Now, the thing we didn't talk\nabout, about this picture here, is we say that a sequence\nof random variables--",
    "start": "2409530",
    "end": "2418770"
  },
  {
    "text": "Y1, Y2, et cetera-- converges in probability to a\nrandom variable y if for every",
    "start": "2418770",
    "end": "2425350"
  },
  {
    "text": "epsilon greater than 0 and every\ndelta greater than 0,",
    "start": "2425350",
    "end": "2430740"
  },
  {
    "text": "the probability that the n-th\nrandom variable minus this",
    "start": "2430740",
    "end": "2436440"
  },
  {
    "text": "funny random variable is greater\nthan or equal to epsilon, is less than\nor equal to delta. That's saying the same thing\nas this picture says.",
    "start": "2436440",
    "end": "2445460"
  },
  {
    "text": "In this picture here, you can\ndraw each one of the Y sub n's minus Y. You think of\nY sub n minus Y as a",
    "start": "2445460",
    "end": "2452840"
  },
  {
    "text": "single random variable. And then you get this kind of\ncurve here and the same",
    "start": "2452840",
    "end": "2458780"
  },
  {
    "text": "interpretation works. So again, what you're saying\nwith convergence and",
    "start": "2458780",
    "end": "2464160"
  },
  {
    "text": "probability is that the\ndistribution function of Yn",
    "start": "2464160",
    "end": "2469380"
  },
  {
    "text": "minus Y is approaching a unit\nstep as n gets big.",
    "start": "2469380",
    "end": "2475710"
  },
  {
    "text": "So that's really the meaning\nof convergence and probability. I mean, you get this unit step\nas n gets bigger and bigger.",
    "start": "2475710",
    "end": "2483720"
  },
  {
    "text": " OK, so let's review all\nof what we've done in",
    "start": "2483720",
    "end": "2490860"
  },
  {
    "text": "the last half hour. If a random variable, generic\nrandom variable x, has a",
    "start": "2490860",
    "end": "2498470"
  },
  {
    "text": "standard deviation-- in other words, if it has\na finite variance. And if X1, X2 are IID with that\nstandard deviation, then",
    "start": "2498470",
    "end": "2509580"
  },
  {
    "text": "the standard deviation of the\nrelative frequency is equal to the standard deviation\nof x divided by the",
    "start": "2509580",
    "end": "2516840"
  },
  {
    "text": "square root of n. So the standard deviation\nis the relative",
    "start": "2516840",
    "end": "2522610"
  },
  {
    "text": "of the sample average. Excuse me, sample average,\nnot relative frequency. Of the sample average is going\nto 0 as n gets big.",
    "start": "2522610",
    "end": "2532079"
  },
  {
    "text": "In the same way, if you have a\nsequence of arbitrary random",
    "start": "2532080",
    "end": "2537530"
  },
  {
    "text": "variables, which are converging\nto Y in mean square, then Chebyshev shows\nthat it converges in",
    "start": "2537530",
    "end": "2545820"
  },
  {
    "text": "probability. OK so mean square convergence\nimplies convergence in",
    "start": "2545820",
    "end": "2551530"
  },
  {
    "text": "probability. Mean square convergence is a\nfunny statement which says that this sequence of random\nvariables has a standard",
    "start": "2551530",
    "end": "2564470"
  },
  {
    "text": "deviation, which\nis going to 0. And it's hard to see exactly\nwhat that means because that",
    "start": "2564470",
    "end": "2571500"
  },
  {
    "text": "standard deviation is a\ncomplicated integral. And I don't know\nwhat it means.",
    "start": "2571500",
    "end": "2578039"
  },
  {
    "text": "But if you use the Chebyshev\ninequality, then it means this very simple statement, which\nsays that this sequence has to",
    "start": "2578040",
    "end": "2590610"
  },
  {
    "text": "converge in probability to Y. Mean square convergence then\nimplies convergence in",
    "start": "2590610",
    "end": "2596600"
  },
  {
    "text": "probability. Reverse isn't true because--",
    "start": "2596600",
    "end": "2602420"
  },
  {
    "text": "and I can't give you an example\nof it now, but I've already told you something\nabout it.",
    "start": "2602420",
    "end": "2608450"
  },
  {
    "text": "Because I've said that's the\nweak law of large numbers continues to hold if the generic\nrandom variable has a",
    "start": "2608450",
    "end": "2619279"
  },
  {
    "text": "mean, but doesn't have a\nvariance because of this truncation argument. ",
    "start": "2619280",
    "end": "2628440"
  },
  {
    "text": "Well, I mean, what it says\nthen is a variance is not required for the weak law of\nlarge numbers to hold.",
    "start": "2628440",
    "end": "2637030"
  },
  {
    "text": "And if the variance doesn't\nhold, then you certainly don't have convergence\nin mean square. So we have an example even\nthough you haven't proven that",
    "start": "2637030",
    "end": "2647150"
  },
  {
    "text": "that example works. You have an example where the\nweak law of large numbers holds, but convergence in mean\nsquare does not hold.",
    "start": "2647150",
    "end": "2659290"
  },
  {
    "text": "OK, and the final thing is\nconvergence in probability",
    "start": "2659290",
    "end": "2664510"
  },
  {
    "text": "really means that the\ndistribution of Yn minus Y",
    "start": "2664510",
    "end": "2669730"
  },
  {
    "text": "approaches the unit step. Yes? AUDIENCE: So in general,\nconvergence in probability",
    "start": "2669730",
    "end": "2675117"
  },
  {
    "text": "doesn't imply convergence\nin distribution. But it holds in this special\ncase because-- PROFESSOR: It does imply\nconvergence in distribution.",
    "start": "2675117",
    "end": "2680745"
  },
  {
    "text": "We haven't talked about\nconvergence in distribution yet. ",
    "start": "2680746",
    "end": "2688040"
  },
  {
    "text": "Except it does not imply\nconvergence in mean square, which is a thing that\nrequires a variance.",
    "start": "2688040",
    "end": "2695190"
  },
  {
    "text": "So you can have convergence\nin probability without convergence in mean square,\nbut not the other way.",
    "start": "2695190",
    "end": "2702570"
  },
  {
    "text": "I mean, convergence in mean\nsquare, you just apply Chebyshev to it,\nand suddenly-- presto, you have convergence\nin probability.",
    "start": "2702570",
    "end": "2709994"
  },
  {
    "start": "2709995",
    "end": "2716960"
  },
  {
    "text": "And incidentally, I\nwish all of you would ask more questions. Because we're taking this video,\nwhich is going to be",
    "start": "2716960",
    "end": "2724720"
  },
  {
    "text": "shown to many people in many\ndifferent countries. And they ask themselves, would\nit be better if I came to MIT",
    "start": "2724720",
    "end": "2732830"
  },
  {
    "text": "and then I could sit-in class\nand ask questions? And then they see these videos\nand they say, ah, it doesn't make any difference, nobody\nask questions anyway.",
    "start": "2732830",
    "end": "2739590"
  },
  {
    "text": " And because of that, MIT\nwill simply wither",
    "start": "2739590",
    "end": "2745750"
  },
  {
    "text": "away at some point. So it's very important for you\nto ask questions now and then. ",
    "start": "2745750",
    "end": "2755470"
  },
  {
    "text": "Now, let's go on to the\ncentral limit theorem. ",
    "start": "2755470",
    "end": "2763900"
  },
  {
    "text": "This sum of n IID\nrandom variables minus n times the mean--",
    "start": "2763900",
    "end": "2772110"
  },
  {
    "text": "in other words, we just\nnormalized it to 0 mean. S sub n minus n x bar is a\n0 mean random variable.",
    "start": "2772110",
    "end": "2780945"
  },
  {
    "text": "And it has variance n\ntimes sigma squared. It also has second moment\nn times sigma squared.",
    "start": "2780945",
    "end": "2788140"
  },
  {
    "text": "And what that means is that you\ntake Sn minus n times the",
    "start": "2788140",
    "end": "2795220"
  },
  {
    "text": "mean of x and divide it by the\nsquare root of n times sigma. What you get is something\nwhich is 0",
    "start": "2795220",
    "end": "2801490"
  },
  {
    "text": "mean and unit variance. So as you keep increasing n,\nthis random variable here, Sn",
    "start": "2801490",
    "end": "2809069"
  },
  {
    "text": "minus n x bar over the square\nroot of n sigma, just sits there rock solid with the same\nmean and the same variance,",
    "start": "2809070",
    "end": "2817720"
  },
  {
    "text": "nothing ever happens to it. Except it has a distribution\nfunction, and the distribution",
    "start": "2817720",
    "end": "2823670"
  },
  {
    "text": "function changes. I mean, you see the distribution\nfunction changing here as you let n get\nlarger and larger.",
    "start": "2823670",
    "end": "2832380"
  },
  {
    "text": "In some sense, these steps are\ngetting smaller and smaller. So it looks like you're\napproaching some particular",
    "start": "2832380",
    "end": "2839680"
  },
  {
    "text": "curve and when we looked\nat the Bernoulli case--",
    "start": "2839680",
    "end": "2846470"
  },
  {
    "text": "I guess it was just last time. When we looked at the Bernoulli\ncase, what we saw is",
    "start": "2846470",
    "end": "2852080"
  },
  {
    "text": "that these steps here were\ngoing as e to the minus",
    "start": "2852080",
    "end": "2857610"
  },
  {
    "text": "difference from the mean squared\ndivided by 2 times sigma squared.",
    "start": "2857610",
    "end": "2863570"
  },
  {
    "text": "Bunch of stuff, but what we saw\nwas that these steps were proportional to the density\nof a Gaussian.",
    "start": "2863570",
    "end": "2872790"
  },
  {
    "text": "In other words, this curve that\nwe're converging to is proportional to the distribution\nfunction of the",
    "start": "2872790",
    "end": "2880170"
  },
  {
    "text": "Gaussian random variable. We didn't completely prove that\nbecause all we did was to",
    "start": "2880170",
    "end": "2886120"
  },
  {
    "text": "show what happened to the PMF. We didn't really integrate\nthese things.",
    "start": "2886120",
    "end": "2891500"
  },
  {
    "text": "We didn't really deal with all\nof the small quantities.",
    "start": "2891500",
    "end": "2896830"
  },
  {
    "text": "We said they weren't\nimportant. But you sort of got the picture\nof exactly why this",
    "start": "2896830",
    "end": "2903620"
  },
  {
    "text": "convergence to a normal\ndistribution function takes place.",
    "start": "2903620",
    "end": "2909900"
  },
  {
    "text": "And the theorem says this more\ngeneral thing that this convergence does, in\nfact, take place.",
    "start": "2909900",
    "end": "2917340"
  },
  {
    "text": "And that is what the central\nlimit theorem says. It says that that happens not\nonly for the Bernoulli case,",
    "start": "2917340",
    "end": "2925460"
  },
  {
    "text": "but it happens for all\nrandom variables, which have a variance.",
    "start": "2925460",
    "end": "2931210"
  },
  {
    "text": "And the convergence is\nrelatively good if the random variables have a certain\nmoment that",
    "start": "2931210",
    "end": "2937109"
  },
  {
    "text": "can be awful otherwise. ",
    "start": "2937110",
    "end": "2948450"
  },
  {
    "text": "So this expression on top then\nis really the expression of the central limit theorem.",
    "start": "2948450",
    "end": "2955320"
  },
  {
    "text": "It says not only does the\nnormalized sample average--",
    "start": "2955320",
    "end": "2964320"
  },
  {
    "text": "I'll call this whole thing the\nnormalized sample average because Sn minus n X bar has\nvariance square root of n",
    "start": "2964320",
    "end": "2973700"
  },
  {
    "text": "times sigma sub x. So this normalized sample\naverage has mean 0 and",
    "start": "2973700",
    "end": "2980900"
  },
  {
    "text": "standard deviation 1. Not only does it have mean 0\nand variance 1, but it also",
    "start": "2980900",
    "end": "2986370"
  },
  {
    "text": "becomes closer and closer to\nthis Gaussian distribution. Why is that important?",
    "start": "2986370",
    "end": "2991870"
  },
  {
    "text": " Well, if you start studying\nnoise and things like that,",
    "start": "2991870",
    "end": "2997020"
  },
  {
    "text": "it's very important. Because it says that if you have\nthe sum of lots and lots",
    "start": "2997020",
    "end": "3003290"
  },
  {
    "text": "of very, very small, unimportant\nthings, then what",
    "start": "3003290",
    "end": "3009450"
  },
  {
    "text": "those things add up to if\nthey're relatively independent is something which is\nalmost Gaussian.",
    "start": "3009450",
    "end": "3015310"
  },
  {
    "text": "You pick up a book on noise\ntheory or you pick up a book",
    "start": "3015310",
    "end": "3022080"
  },
  {
    "text": "which is on communication, or\nwhich is on control, or something like that, and after\nyou read a few chapters, you",
    "start": "3022080",
    "end": "3030430"
  },
  {
    "text": "get the idea that all random\nvariables are Gaussian.",
    "start": "3030430",
    "end": "3035550"
  },
  {
    "text": "This is particularly true\nif you look at books on statistics. Many, many books on statistics,\nparticularly for",
    "start": "3035550",
    "end": "3042160"
  },
  {
    "text": "undergraduates, the only random\nvariable they ever talk about is the normal\nrandom variable.",
    "start": "3042160",
    "end": "3048610"
  },
  {
    "text": "For some reason or other, you're\nled to believe that all random variables are Gaussian.",
    "start": "3048610",
    "end": "3053859"
  },
  {
    "text": "Well, of course, they aren't. But this says that a lot of\nrandom variables, which are",
    "start": "3053860",
    "end": "3059080"
  },
  {
    "text": "sums of large numbers of little\nthings, in fact are close to Gaussian.",
    "start": "3059080",
    "end": "3064540"
  },
  {
    "text": "But we're interested in it\nhere for another reason. ",
    "start": "3064540",
    "end": "3071540"
  },
  {
    "text": "And we'll come to that\nin a little bit. But let me make the comment that\nthe proofs that I gave",
    "start": "3071540",
    "end": "3079160"
  },
  {
    "text": "you about the central\nlimit theorem for the Bernoulli case--",
    "start": "3079160",
    "end": "3084650"
  },
  {
    "text": "and if you fill-in those\nepsilons and deltas there, that really was a valid proof.",
    "start": "3084650",
    "end": "3090520"
  },
  {
    "text": "That technique does not work\nat all when you have a non-Bernoulli situation.",
    "start": "3090520",
    "end": "3096180"
  },
  {
    "text": "Because the situation is\nvery, very complicated. You wind up--",
    "start": "3096180",
    "end": "3101400"
  },
  {
    "text": "I mean, if you have a Bernoulli\ncase, you wind up with this nice, nice\ndistribution, which says that",
    "start": "3101400",
    "end": "3106800"
  },
  {
    "text": "every step in the Bernoulli\ndistribution, you have terms that are increasing and then\nterms that are decreasing.",
    "start": "3106800",
    "end": "3114420"
  },
  {
    "text": "If you look at what happens\nfor a discrete random variable, which is not binary,\nyou have the most god awful",
    "start": "3114420",
    "end": "3121870"
  },
  {
    "text": "distribution if you\ntry to look at the probability mass function.",
    "start": "3121870",
    "end": "3127130"
  },
  {
    "text": "It is just awful. And the only thing which\nlooks nice is the",
    "start": "3127130",
    "end": "3132140"
  },
  {
    "text": "distribution function. The distribution function\nlooks relatively nice. And why is hard to tell.",
    "start": "3132140",
    "end": "3138713"
  },
  {
    "text": "And if you look at proofs of\nit, it goes through Fourier",
    "start": "3138713",
    "end": "3144109"
  },
  {
    "text": "transforms. In probability theory, Fourier\ntransforms are called characteristics functions, but\nit's really the same thing.",
    "start": "3144110",
    "end": "3152300"
  },
  {
    "text": "And you go through this very\ncomplicated argument. I've been through it\na number of times.",
    "start": "3152300",
    "end": "3158720"
  },
  {
    "text": "And to me, it's all algebra. And I'm not a person that just\naccepts the fact that",
    "start": "3158720",
    "end": "3164339"
  },
  {
    "text": "something is all\nalgebra easily. I keep trying to find ways of\nmaking sense out of it.",
    "start": "3164340",
    "end": "3170170"
  },
  {
    "text": "And I've never been able to make\nsense out of it, but I'm convinced that it's true. So you just have to sort\nof live with that.",
    "start": "3170170",
    "end": "3178180"
  },
  {
    "text": "So anyway, the central limit\ntheorem does apply to the",
    "start": "3178180",
    "end": "3186550"
  },
  {
    "text": "distribution function. Namely, exactly what\nthat says. The distribution function of\nthis normalized sample average",
    "start": "3186550",
    "end": "3196620"
  },
  {
    "text": "does go into the distribution\nfunction of the Gaussian. The PMFs do not converge\nat all.",
    "start": "3196620",
    "end": "3206240"
  },
  {
    "text": "And nothing else converges,\nbut just that one thing. OK, a sequence of random\nvariables converges in",
    "start": "3206240",
    "end": "3214780"
  },
  {
    "text": "distribution-- this is what someone was asking\nabout just a second ago, but I don't think you were\nreally asking about that.",
    "start": "3214780",
    "end": "3223079"
  },
  {
    "text": "But this is what convergence\nin distribution means. It means it's the limit of the\ndistribution functions of a",
    "start": "3223080",
    "end": "3231609"
  },
  {
    "text": "sequence of random variables. Turns into the distribution\nfunction of some",
    "start": "3231610",
    "end": "3239140"
  },
  {
    "text": "other random variable. ",
    "start": "3239140",
    "end": "3246529"
  },
  {
    "text": "Then you say that these random\nvariables converge in distribution to Z. And that's\na nice, useful thing.",
    "start": "3246530",
    "end": "3254930"
  },
  {
    "text": "And the CLT, the Central Limit\nTheorem, then says that this normalized sample average\nconverges in distribution to",
    "start": "3254930",
    "end": "3264990"
  },
  {
    "text": "the distribution of a normal\nrandom variable. Many people call that density\ncity phi and call the normal",
    "start": "3264990",
    "end": "3273280"
  },
  {
    "text": "distribution phi. I don't know why. I mean, you've got to call it\nsomething, so many people call",
    "start": "3273280",
    "end": "3279600"
  },
  {
    "text": "it the same thing.  This convergence and\ndistribution is really almost",
    "start": "3279600",
    "end": "3286660"
  },
  {
    "text": "a misnomer. Because when random variables\nconverge in distribution to",
    "start": "3286660",
    "end": "3292240"
  },
  {
    "text": "another random variable, I mean,\nif you say something converges, usually you have the\nidea that the thing which",
    "start": "3292240",
    "end": "3303290"
  },
  {
    "text": "is converging to something else\nis getting close to it in some sense. And the random variables aren't\ngetting close at all,",
    "start": "3303290",
    "end": "3311240"
  },
  {
    "text": "it's only the distribution\nfunctions that are getting close. If I take a sequence of IID\nrandom variables, all of them",
    "start": "3311240",
    "end": "3321680"
  },
  {
    "text": "have the same distribution\nfunction. And therefore, a\nsequence of IID",
    "start": "3321680",
    "end": "3327430"
  },
  {
    "text": "random variables converges. And in fact, it's converged\nright from the beginning to",
    "start": "3327430",
    "end": "3335040"
  },
  {
    "text": "the same random variable,\nto the same generic random variable. But they're not at all\nclose to each other.",
    "start": "3335040",
    "end": "3341329"
  },
  {
    "text": "But you still call this\nconvergence in distribution. Why do we make such a big fuss\nabout convergence in",
    "start": "3341330",
    "end": "3348819"
  },
  {
    "text": "distribution? Well, primarily because of the\ncentral limit theorem because",
    "start": "3348820",
    "end": "3353850"
  },
  {
    "text": "you would like to see that a\nsequence of random variables, in fact, starts to look like\nsomething that is interesting,",
    "start": "3353850",
    "end": "3361600"
  },
  {
    "text": "which is the Gaussian random\nvariable after a while. It says we can do these\ncrazy things that",
    "start": "3361600",
    "end": "3368340"
  },
  {
    "text": "statisticians do, and that-- well, fortunately, most\ncommunication theorists are a",
    "start": "3368340",
    "end": "3374390"
  },
  {
    "text": "little more careful than\nstatisticians. Somebody's going to hit me\nfor saying that, but",
    "start": "3374390",
    "end": "3380980"
  },
  {
    "text": "I think it's true. But the central limit theorem,\nin fact, does say that many of",
    "start": "3380980",
    "end": "3389420"
  },
  {
    "text": "these sums of random variables\nyou look at can be reasonably approximated as being\nGaussian.",
    "start": "3389420",
    "end": "3395135"
  },
  {
    "text": " So what we have now is\nconvergence in probability",
    "start": "3395135",
    "end": "3404730"
  },
  {
    "text": "implies convergence\nin distribution.  And the proof, I will--",
    "start": "3404730",
    "end": "3411620"
  },
  {
    "text": "on the slides, I always\nabbreviate proof by Pf.",
    "start": "3411620",
    "end": "3416680"
  },
  {
    "text": "And sometimes Pf is just what\nit sounds like it, it's \"poof.\" It is not quite a proof,\nand you have to look at",
    "start": "3416680",
    "end": "3425430"
  },
  {
    "text": "those to get the actual proof. But this says the convergence\nis a sequence of Yn's in",
    "start": "3425430",
    "end": "3431530"
  },
  {
    "text": "probability means that it\nconverges to a unit step. That's exactly what convergence",
    "start": "3431530",
    "end": "3437359"
  },
  {
    "text": "in probability mean. It converges to a unit step,\nand ti converges everywhere",
    "start": "3437360",
    "end": "3444859"
  },
  {
    "text": "but at the step itself. If you look at the definition\nof convergence in",
    "start": "3444860",
    "end": "3450040"
  },
  {
    "text": "distribution, and I might not\nhave said it carefully enough when I defined it back here.",
    "start": "3450040",
    "end": "3458630"
  },
  {
    "text": "Oh, yes, I did. Remarkable. Often I make up these slides\nwhen I'm half asleep, and they",
    "start": "3458630",
    "end": "3466130"
  },
  {
    "text": "don't always say what I\nintended them to say. And my evil twin brother comes\nin and changes them later.",
    "start": "3466130",
    "end": "3473290"
  },
  {
    "text": "But here I said it right. A sequence of random variables\nconverges in distribution to",
    "start": "3473290",
    "end": "3478660"
  },
  {
    "text": "another random variable Z if the\nlimit of the distribution",
    "start": "3478660",
    "end": "3485910"
  },
  {
    "text": "function is equal\nto the limit-- if the limit of the distribution\nfunction of the Z",
    "start": "3485910",
    "end": "3494530"
  },
  {
    "text": "sub n is equal to the\ndistribution function of Z. But it only says for all Z\nwhere this distribution",
    "start": "3494530",
    "end": "3503470"
  },
  {
    "text": "function is continuous. You can't really expect much\nmore than that because if",
    "start": "3503470",
    "end": "3513840"
  },
  {
    "text": "you're looking at\na distribution-- if you're looking at a\nlimiting distribution function, which looks like this,\nespecially for the law",
    "start": "3513840",
    "end": "3523990"
  },
  {
    "text": "of large numbers, all we've\nbeen able to show is that",
    "start": "3523990",
    "end": "3530080"
  },
  {
    "text": "these distributions come in down\nhere very close, go up",
    "start": "3530080",
    "end": "3536930"
  },
  {
    "text": "and get out very\nclose up there. We haven't said anything\nabout where they cross this actual line.",
    "start": "3536930",
    "end": "3543680"
  },
  {
    "text": "And there's nothing in the\nargument about the weak law of large numbers, which says\nanything about what happens",
    "start": "3543680",
    "end": "3551970"
  },
  {
    "text": "right exactly at the mean. But that's something\nthat's the central",
    "start": "3551970",
    "end": "3558700"
  },
  {
    "text": "limit theorem says-- Yes? AUDIENCE: What's the Zn? That's not the sample mean?",
    "start": "3558700",
    "end": "3565400"
  },
  {
    "start": "3565400",
    "end": "3571369"
  },
  {
    "text": "PROFESSOR: When we use the Zn's\nfor the central limit",
    "start": "3571370",
    "end": "3577530"
  },
  {
    "text": "theorem, then what I mean by\nthe Z sub n's here is those normalized random variables Sn\nminus n x bar over square root",
    "start": "3577530",
    "end": "3587380"
  },
  {
    "text": "of n times sigma.  And in all of these definitions\nof convergence,",
    "start": "3587380",
    "end": "3594080"
  },
  {
    "text": "the random variables, which are\nconverging to something, are always rather peculiar.",
    "start": "3594080",
    "end": "3599890"
  },
  {
    "text": "Sometimes they're the\nsample averages. Sometimes they're the normalized\nsample averages.",
    "start": "3599890",
    "end": "3606839"
  },
  {
    "text": "God knows what they are.  But what mathematicians\nlike to do--",
    "start": "3606840",
    "end": "3613430"
  },
  {
    "text": "and there's a good reason for\nwhat they like to do-- is they like to define different\nkinds of convergence",
    "start": "3613430",
    "end": "3620339"
  },
  {
    "text": "in general terms, and then apply\nthem to the specific",
    "start": "3620340",
    "end": "3625620"
  },
  {
    "text": "thing that you're\ninterested in. OK, so the central limit\ntheorem says that this",
    "start": "3625620",
    "end": "3631620"
  },
  {
    "text": "normalized sum converges in\ndistribution to phi, but it",
    "start": "3631620",
    "end": "3638610"
  },
  {
    "text": "only has to converge where the\ndistribution function is continuous. Yes?",
    "start": "3638610",
    "end": "3645275"
  },
  {
    "text": "AUDIENCE: So the theorem applies\nto the distribution. Why doesn't it apply to PMF?",
    "start": "3645275",
    "end": "3652730"
  },
  {
    "text": "PROFESSOR: Well, if you look at\nthe example we have here,",
    "start": "3652730",
    "end": "3662560"
  },
  {
    "text": "if you look at the PDF for\nthis normalized random",
    "start": "3662560",
    "end": "3670400"
  },
  {
    "text": "variable, you find something\nwhich is jumping up, jumping up.",
    "start": "3670400",
    "end": "3675550"
  },
  {
    "text": "If we look at it for n equals\n50, it's still jumping up. The jumps are smaller.",
    "start": "3675550",
    "end": "3680760"
  },
  {
    "text": "But if you look at the\nPDF for this-- well, if you look at the\ndistribution function for the",
    "start": "3680760",
    "end": "3688809"
  },
  {
    "text": "normal, it has a density. ",
    "start": "3688810",
    "end": "3694260"
  },
  {
    "text": "This PDF function for the\nthings which you want to",
    "start": "3694260",
    "end": "3701130"
  },
  {
    "text": "approach a limit never\nhave a density. All the time they have a PDF.",
    "start": "3701130",
    "end": "3707760"
  },
  {
    "text": "The steps are getting\nsmaller and smaller. And you can see that here as\nyou're up to n equals 50.",
    "start": "3707760",
    "end": "3714390"
  },
  {
    "text": "You can see these little\ntiny steps here. But you still have a PMF.",
    "start": "3714390",
    "end": "3723350"
  },
  {
    "text": "You'll want to look at it\nin terms of density. You have to look at in\nterms of impulses.",
    "start": "3723350",
    "end": "3729980"
  },
  {
    "text": "And there's no way you can say\nan impulse is starting to approach a smooth curve. ",
    "start": "3729980",
    "end": "3756640"
  },
  {
    "text": "OK, so we have this proof that\nconverges in probability, implies convergence\nin distribution.",
    "start": "3756640",
    "end": "3763770"
  },
  {
    "text": "And since convergence in mean\nsquare implies convergence in probability, and convergence\nin probability implies",
    "start": "3763770",
    "end": "3771430"
  },
  {
    "text": "convergence in distribution,\nwe suddenly have the convergence in mean square\nimplies convergence in",
    "start": "3771430",
    "end": "3778030"
  },
  {
    "text": "distribution also. And you have this nice picture\nin the book of all the things",
    "start": "3778030",
    "end": "3783740"
  },
  {
    "text": "that converge in distribution. Inside of that-- this is distribution.",
    "start": "3783740",
    "end": "3790250"
  },
  {
    "text": "Inside of that is all the\nthings that converge in probability. ",
    "start": "3790250",
    "end": "3797440"
  },
  {
    "text": "And inside of that is\nall the things that converge in mean square. ",
    "start": "3797440",
    "end": "3811510"
  },
  {
    "text": "Now, there's a paradox here. And what the paradox is, is that\nthe central limit theorem",
    "start": "3811510",
    "end": "3818589"
  },
  {
    "text": "says something very, very strong\nabout how Sn over n-- namely, the sample average--",
    "start": "3818590",
    "end": "3825520"
  },
  {
    "text": "converges to the mean. The convergence in distribution\nis a very weak",
    "start": "3825520",
    "end": "3830640"
  },
  {
    "text": "form of convergence. So how is this weak form of\nconvergence telling you something that says specific\nabout how a sample average",
    "start": "3830640",
    "end": "3842460"
  },
  {
    "text": "converges to the mean? It tells you much more\nthan the weak law of large numbers does. Because it tells you if you at\nthis thing, it's starting to",
    "start": "3842460",
    "end": "3849780"
  },
  {
    "text": "approach a normal distribution\nfunction. And the resolution\nof that paradox--",
    "start": "3849780",
    "end": "3855970"
  },
  {
    "text": "and this is important\nI think-- is that the random variables\nconverge in distribution to",
    "start": "3855970",
    "end": "3861470"
  },
  {
    "text": "the central limit theorem\nare these normalized random variables.",
    "start": "3861470",
    "end": "3868859"
  },
  {
    "text": "The ones that converge in\nprobability are the things",
    "start": "3868860",
    "end": "3876830"
  },
  {
    "text": "which are normalizing in terms\nof the mean, but you're not normalizing them in\nterms of variance.",
    "start": "3876830",
    "end": "3883330"
  },
  {
    "text": "So when you look at one curve\nrelative to the other curve, one curve is a squashed down\nversion of the other curve.",
    "start": "3883330",
    "end": "3891500"
  },
  {
    "text": "I mean, look at those pictures\nwe have for that example. ",
    "start": "3891500",
    "end": "3899230"
  },
  {
    "text": "If you look at a sequence of\ndistribution functions for S sub n over n, what you find is\nthings which are squashing",
    "start": "3899230",
    "end": "3909230"
  },
  {
    "text": "down into a unit step. If you look at what you have\nfor the normalized random",
    "start": "3909230",
    "end": "3917100"
  },
  {
    "text": "variables, normalized to unit\nvariance, what you have is",
    "start": "3917100",
    "end": "3924990"
  },
  {
    "text": "something which is not squashing\ndown at all. It gives the whole shape\nof the thing.",
    "start": "3924990",
    "end": "3930309"
  },
  {
    "text": "You can get from one curve to\nthe other just by squashing or expanding on the x-axis.",
    "start": "3930310",
    "end": "3937290"
  },
  {
    "text": "That's the only difference\nbetween them. So the central limit theorem\nsays when you don't squash,",
    "start": "3937290",
    "end": "3944750"
  },
  {
    "text": "you get this nice Gaussian\ndistribution function. The weak law of large numbers\nsays when you do squash, you",
    "start": "3944750",
    "end": "3954040"
  },
  {
    "text": "get a unit step. Now, which tells you more? Well, if you have the central\nlimit theorem, it tells you a",
    "start": "3954040",
    "end": "3961609"
  },
  {
    "text": "lot more because it says, if\nyou look at this unit step",
    "start": "3961610",
    "end": "3968900"
  },
  {
    "text": "here, and you expand it out by\na factor of square root of n,",
    "start": "3968900",
    "end": "3974160"
  },
  {
    "text": "what you're going to get is\nsomething that goes like this.",
    "start": "3974160",
    "end": "3979339"
  },
  {
    "text": "The central limit theorem tells\nyou exactly what the distribution function\nis at x bar.",
    "start": "3979340",
    "end": "3985780"
  },
  {
    "text": "It tells you that that's\nconverging to what?",
    "start": "3985780",
    "end": "3990960"
  },
  {
    "text": "What's the probability that the\nsum of a large number of",
    "start": "3990960",
    "end": "3996750"
  },
  {
    "text": "random variables is greater\nthan n times the mean? ",
    "start": "3996750",
    "end": "4002670"
  },
  {
    "text": "What is it approximately? AUDIENCE: 1/2. PROFESSOR: 1/2. ",
    "start": "4002670",
    "end": "4007880"
  },
  {
    "text": "That's what this says. This is a distribution\nfunction. It's converging to\nthe distribution",
    "start": "4007880",
    "end": "4014400"
  },
  {
    "text": "function of the normal. It hits that point, the\nnormal is centered",
    "start": "4014400",
    "end": "4020500"
  },
  {
    "text": "on this x bar here. And it hits that point\nexactly at 1/2. This says the probability of\nbeing on that side is 1/2.",
    "start": "4020500",
    "end": "4028800"
  },
  {
    "text": "The probability of being\non this side is 1/2. So you see, the central limit\ntheorem is telling you a whole",
    "start": "4028800",
    "end": "4035500"
  },
  {
    "text": "lot more about how this is\nconverging than the weak law of large numbers is.",
    "start": "4035500",
    "end": "4041980"
  },
  {
    "text": "Now, I come back to the\nquestion I asked you a long time ago. ",
    "start": "4041980",
    "end": "4047760"
  },
  {
    "text": "Why is the weak law\nof large numbers-- why do you see it used more\noften than the central limit",
    "start": "4047760",
    "end": "4055170"
  },
  {
    "text": "theorem since it's so\nmuch less powerful?  Well, it's the same\nanswer as before.",
    "start": "4055170",
    "end": "4062010"
  },
  {
    "text": "It's less powerful, but it\napplies to a much larger number of cases.",
    "start": "4062010",
    "end": "4067390"
  },
  {
    "text": "And in many situations, all\nyou want is that weaker statement that tells you\neverything you want to know,",
    "start": "4067390",
    "end": "4074210"
  },
  {
    "text": "but it tells you that weaker\nstatement for this enormous variety of different\nsituations.",
    "start": "4074210",
    "end": "4081819"
  },
  {
    "text": "Mean square convergence applies\nto fewer things. Well, of course, convergence\nin distribution applies for",
    "start": "4081820",
    "end": "4088570"
  },
  {
    "text": "even more things. But we saw that when you're\ndealing with the central limit theorem, all bets are off on\nthat because it's talking",
    "start": "4088570",
    "end": "4098120"
  },
  {
    "text": "about a different sequence of\nrandom variables, which might or might not converge.",
    "start": "4098120",
    "end": "4103429"
  },
  {
    "text": "OK, so finally, convergence\nwith probability 1.",
    "start": "4103430",
    "end": "4109520"
  },
  {
    "text": " Many people call convergence\nwith probability 1 convergence",
    "start": "4109520",
    "end": "4118060"
  },
  {
    "text": "almost surely, or convergence\nalmost everywhere. You will see this almost\neverywhere.",
    "start": "4118060",
    "end": "4125640"
  },
  {
    "text": "Now, why do I want to use\nconvergence with probability, and why is that a dangerous\nthing to do?",
    "start": "4125640",
    "end": "4134109"
  },
  {
    "text": "When you say things are\nconverging with probability 1, it sounds very much like you're\nsaying they converge in",
    "start": "4134109",
    "end": "4141770"
  },
  {
    "text": "probability because you're\nusing the word \"probability\" in each. The two are very, very\ndifferent concepts.",
    "start": "4141770",
    "end": "4148499"
  },
  {
    "text": " And therefore, it would seem\nlike you should avoid the word",
    "start": "4148499",
    "end": "4155729"
  },
  {
    "text": "\"probability\" in this second one\nand say convergence almost surely or convergence\nalmost everywhere.",
    "start": "4155729",
    "end": "4163689"
  },
  {
    "text": "And the reason I don't like\nthose is they don't make any sense, unless you understand\nmeasure theory.",
    "start": "4163689",
    "end": "4170799"
  },
  {
    "text": "And we're not assuming\nthat you understand measure theory here. If you wanted to do that first\nproblem in the last problem",
    "start": "4170800",
    "end": "4177720"
  },
  {
    "text": "set, you had to understand\nmeasure theory. And I apologize for that, I\ndidn't mean to do that to you.",
    "start": "4177720",
    "end": "4185554"
  },
  {
    "text": "But this notion of convergence\nwith probability 1, I think",
    "start": "4185555",
    "end": "4192390"
  },
  {
    "text": "you can understand that. I think you can get a good sense\nof what it means without knowing any measure theory.",
    "start": "4192390",
    "end": "4197949"
  },
  {
    "text": "And at least that's what\nwe're trying to do. OK, so let's go on.",
    "start": "4197950",
    "end": "4204179"
  },
  {
    "text": " We've already said that a random\nvariable is a lot more",
    "start": "4204180",
    "end": "4209410"
  },
  {
    "text": "complicated thing than\na number is. I think those of you who\nthought you understood",
    "start": "4209410",
    "end": "4216060"
  },
  {
    "text": "probability theory pretty well,\nI probably managed to confuse you enough to get you\nto the point where you think",
    "start": "4216060",
    "end": "4223790"
  },
  {
    "text": "you're not on totally\nsafe ground talking about random variables. And you're certainly not on very\nsafe ground talking about",
    "start": "4223790",
    "end": "4231310"
  },
  {
    "text": "how random variables converge\nto each other. And that's good because to reach\na greater understanding",
    "start": "4231310",
    "end": "4238960"
  },
  {
    "text": "of something, you have to get\nto the point where you're a little bit confused first.",
    "start": "4238960",
    "end": "4244090"
  },
  {
    "text": "So I've intentionally\ntried to-- well, I haven't tried\nto make this more confusing than necessary.",
    "start": "4244090",
    "end": "4251140"
  },
  {
    "text": "But in fact, it's not as simple\nas what elementary",
    "start": "4251140",
    "end": "4256490"
  },
  {
    "text": "courses would make\nyou believe. OK, this notion of convergence\nwith probability 1, which we",
    "start": "4256490",
    "end": "4264139"
  },
  {
    "text": "abbreviate WP1, is something\nthat we're not going to talk",
    "start": "4264140",
    "end": "4273140"
  },
  {
    "text": "about a great deal until we\ncome to renewal processes. And the reason is we won't need\nit a great deal until we",
    "start": "4273140",
    "end": "4279880"
  },
  {
    "text": "come to renewal processes. But you ought to know that\nthere's something like that",
    "start": "4279880",
    "end": "4285150"
  },
  {
    "text": "hanging out there. And you ought to have some\nidea of what it is. So here's the definition\nof it.",
    "start": "4285150",
    "end": "4291890"
  },
  {
    "text": "A sequence of random variables\nconvergence with probability 1",
    "start": "4291890",
    "end": "4297120"
  },
  {
    "text": "to some other random\nvariable Z all in the same sample space.",
    "start": "4297120",
    "end": "4303190"
  },
  {
    "text": "If the probability of sample\npoints in omega-- now remember, that a sample\npoint implies a value for each",
    "start": "4303190",
    "end": "4313750"
  },
  {
    "text": "one of these random variables. So in a sense, you can think of\na sample point as, more or",
    "start": "4313750",
    "end": "4320270"
  },
  {
    "text": "less, equivalent to a sample\npath of this sequence of random variables here.",
    "start": "4320270",
    "end": "4326340"
  },
  {
    "text": "OK, so for omega and capital\nOmega, it says the limit as n",
    "start": "4326340",
    "end": "4331810"
  },
  {
    "text": "goes to infinity of these random\nvariables at the point omega is equal to what this\nextra random variable is at",
    "start": "4331810",
    "end": "4345690"
  },
  {
    "text": "the point-- and it says that the probability\nof that whole thing is equal to 1.",
    "start": "4345690",
    "end": "4351050"
  },
  {
    "text": "Now, how many of you can\nlook at that statement and see what it means? Well, I'm sure some of you can\nbecause you've seen it before.",
    "start": "4351050",
    "end": "4359550"
  },
  {
    "text": "But understanding what that\nstatement means, even though it's a very simple statement,\nis not very easy.",
    "start": "4359550",
    "end": "4367810"
  },
  {
    "text": "So there's the statement\nup there. Let's try to parse it. In other words, break it down\ninto what it's talking about.",
    "start": "4367810",
    "end": "4376470"
  },
  {
    "text": "For each sample point omega,\nthat sample point is going to map into a sequence of--",
    "start": "4376470",
    "end": "4384960"
  },
  {
    "start": "4384960",
    "end": "4401680"
  },
  {
    "text": "so each sample point maps into\nthis sample path of values for",
    "start": "4401680",
    "end": "4409390"
  },
  {
    "text": "this sequence of random\nvariables. ",
    "start": "4409390",
    "end": "4415219"
  },
  {
    "text": "Some of those sequences-- OK, this now is a sequence\nof numbers.",
    "start": "4415220",
    "end": "4420985"
  },
  {
    "start": "4420985",
    "end": "4433880"
  },
  {
    "text": "So each omega goes into some\nsequence of numbers. And that also is unquestionably\nclose to this",
    "start": "4433880",
    "end": "4446630"
  },
  {
    "text": "final generic random variable,\ncapital Z evaluated at omega.",
    "start": "4446630",
    "end": "4453460"
  },
  {
    "text": "Now, some of these sequences\nhere, sequences of real numbers, we all know what a\nlimit of real numbers is.",
    "start": "4453460",
    "end": "4464470"
  },
  {
    "text": "I hope you do. I know that many of you don't. And we'll talk about it later\nwhen we start talking about",
    "start": "4464470",
    "end": "4471230"
  },
  {
    "text": "the strong law of\nlarge numbers. But this does, perhaps,\nhave a limit.",
    "start": "4471230",
    "end": "4478600"
  },
  {
    "text": "It perhaps doesn't\nhave a limit. If you look at a sequence 1, 2,\n1, 2, 1, 2, 1, 2, forever,",
    "start": "4478600",
    "end": "4486280"
  },
  {
    "text": "that doesn't have a limit\nbecause it doesn't start to get close to anything. It keeps wandering\naround forever.",
    "start": "4486280",
    "end": "4493060"
  },
  {
    "text": "If you look at a sequence which\nis 1 for 10 terms, then",
    "start": "4493060",
    "end": "4500670"
  },
  {
    "text": "it's 2 the 11th term, and then\nit's 1 for 100 terms, 2 for 1",
    "start": "4500670",
    "end": "4508199"
  },
  {
    "text": "more term, 1 for 1,000 terms,\nthen 2 for the next term, and",
    "start": "4508200",
    "end": "4513900"
  },
  {
    "text": "so forth, that's a much\nmore tricky case. Because in that sequence,\npretty soon",
    "start": "4513900",
    "end": "4521130"
  },
  {
    "text": "all you see is 1's. You look for an awful long way\nand you don't see any 2's. That does not converge just\nbecause the definition of",
    "start": "4521130",
    "end": "4528340"
  },
  {
    "text": "convergence. And when you work with\nconvergence for a long time,",
    "start": "4528340",
    "end": "4533640"
  },
  {
    "text": "after a while you're very\nhappy that that doesn't converge because it would\nplay all sorts of",
    "start": "4533640",
    "end": "4539230"
  },
  {
    "text": "havoc with all of analysis. So anyway, there is this idea\nthat these numbers either",
    "start": "4539230",
    "end": "4545730"
  },
  {
    "text": "converge or they\ndon't converge. When these numbers converge,\nthey might or might not",
    "start": "4545730",
    "end": "4551600"
  },
  {
    "text": "converge to this. So for every omega in this\nsample space, you have this",
    "start": "4551600",
    "end": "4558350"
  },
  {
    "text": "sequence here. That sequence might converge. If it does converge, it might\nconverge to this or it might",
    "start": "4558350",
    "end": "4565460"
  },
  {
    "text": "converge to something else. And what this is saying here is\nyou take this entire set of",
    "start": "4565460",
    "end": "4573090"
  },
  {
    "text": "sequences here. Namely, you take the entire\nset of omega. And for each one of those\nomegas, this might or might",
    "start": "4573090",
    "end": "4581440"
  },
  {
    "text": "not converge. You look at the set of\nomega for which this",
    "start": "4581440",
    "end": "4586699"
  },
  {
    "text": "sequence does converge. And for which it does converge\nto Z of omega. ",
    "start": "4586700",
    "end": "4594210"
  },
  {
    "text": "And now you look at that set\nand what convergence with",
    "start": "4594210",
    "end": "4601950"
  },
  {
    "text": "probability 1 means is that that\nset turns out to be an",
    "start": "4601950",
    "end": "4607010"
  },
  {
    "text": "event, and that event turns\nout to have probability 1.",
    "start": "4607010",
    "end": "4613719"
  },
  {
    "text": "Which says that for almost\neverything that happens, you look at this sample sequence\nand it has a",
    "start": "4613720",
    "end": "4621500"
  },
  {
    "text": "limit, which is this. And that's true in\nprobability.",
    "start": "4621500",
    "end": "4628050"
  },
  {
    "text": "It's not true for\nmost sequences. Let me give you a very quick\nand simple example.",
    "start": "4628050",
    "end": "4635179"
  },
  {
    "text": "Look at this Bernoulli case, and\nsuppose the probability of",
    "start": "4635180",
    "end": "4640620"
  },
  {
    "text": "a 1 is one quarter and the\nprobability of a 0 is 3/4. Look at what happens when you\ntake an extraordinarily large",
    "start": "4640620",
    "end": "4648450"
  },
  {
    "text": "number of trials and you ask\nfor those sample sequences",
    "start": "4648450",
    "end": "4656620"
  },
  {
    "text": "that you take. What's going to happen\nto them?",
    "start": "4656620",
    "end": "4661800"
  },
  {
    "text": "Well, this says that if you\nlook at the relative frequency of 1's--",
    "start": "4661800",
    "end": "4670329"
  },
  {
    "text": "well, if they converge in this\nsense, if the set of relative",
    "start": "4670330",
    "end": "4677220"
  },
  {
    "text": "frequencies-- excuse me. If the set of sample averages\nconverges in this sense, then",
    "start": "4677220",
    "end": "4686370"
  },
  {
    "text": "it says with probability 1, that\nsample average is going",
    "start": "4686370",
    "end": "4692960"
  },
  {
    "text": "to converge to one quarter. Now, that doesn't mean that most\nsequences are going to",
    "start": "4692960",
    "end": "4699340"
  },
  {
    "text": "converge that way. Because most sequences are\ngoing to converge to 1/2.",
    "start": "4699340",
    "end": "4706810"
  },
  {
    "text": "There are many more sequences\nwith half 1's and half 0's than there are with\nthree quarter 1's",
    "start": "4706810",
    "end": "4713369"
  },
  {
    "text": "and one quarter 0's-- with three quarter 0's\nand one quarter 1's.",
    "start": "4713370",
    "end": "4720270"
  },
  {
    "text": "So there are many more of\none than the other. But those particular sequences,\nwhich have",
    "start": "4720270",
    "end": "4728090"
  },
  {
    "text": "probability-- those particular sequences which\nhave relative frequency",
    "start": "4728090",
    "end": "4734650"
  },
  {
    "text": "one quarter are much more likely\nthan those which have",
    "start": "4734650",
    "end": "4740110"
  },
  {
    "text": "relative frequency one half. Because the ones with\nrelative frequency one half are so unlikely.",
    "start": "4740110",
    "end": "4746135"
  },
  {
    "text": " That's a complicated\nset of ideas.",
    "start": "4746135",
    "end": "4751670"
  },
  {
    "text": "You sort of know that it has\nto be true because of these other laws of large numbers.",
    "start": "4751670",
    "end": "4756960"
  },
  {
    "text": "And this is simply extending\nthose laws of large numbers one more step to say not only\ndoes the distribution function",
    "start": "4756960",
    "end": "4765870"
  },
  {
    "text": "of that sample average converge\nto what it should converge to, but also for\nthese sequences with",
    "start": "4765870",
    "end": "4773750"
  },
  {
    "text": "probability 1, they converge. If you look at the sequence for\nlong enough, the sample",
    "start": "4773750",
    "end": "4780449"
  },
  {
    "text": "average is going to converge to\nwhat it should for that one particular sample sequence.",
    "start": "4780450",
    "end": "4785525"
  },
  {
    "text": " OK, the strong law of large\nnumbers then says that if X1,",
    "start": "4785525",
    "end": "4795510"
  },
  {
    "text": "X2, and so forth are IID random\nvariables, and they",
    "start": "4795510",
    "end": "4802059"
  },
  {
    "text": "have an expected value, which\nis less than infinity, then the sample average converges\nto the actual average with",
    "start": "4802060",
    "end": "4811950"
  },
  {
    "text": "probability 1. In other words, it says with\nprobability 1, you look at this sequence forever.",
    "start": "4811950",
    "end": "4818640"
  },
  {
    "text": "I don't know how you look\nat a sequence forever. I've never figured that out. But if you could look at it\nforever, then with probability",
    "start": "4818640",
    "end": "4825930"
  },
  {
    "text": "1, it would come out with the\nright relative frequency. It'll take a lot of investment\nof time when we get to chapter",
    "start": "4825930",
    "end": "4834870"
  },
  {
    "text": "4 to try to sort that out. Wanted to tell you a little bit\nabout it, read about in",
    "start": "4834870",
    "end": "4840765"
  },
  {
    "text": "notes a little bit in chapter\n1, and we will come back to it. And I think you will\nthen understand it.",
    "start": "4840765",
    "end": "4848520"
  },
  {
    "text": "OK, with that, we are\ndone with chapter 1.",
    "start": "4848520",
    "end": "4853770"
  },
  {
    "text": "Next time we will go into\nPoisson processes. If you're upset by all of the\nabstraction in chapter 1, you",
    "start": "4853770",
    "end": "4863230"
  },
  {
    "text": "will be very happy when we get\ninto Poisson processes because there's nothing abstract\nthere at all.",
    "start": "4863230",
    "end": "4868869"
  },
  {
    "text": "Everything you could reasonably\nsay about Poisson processes is either obviously\ntrue or obviously false.",
    "start": "4868870",
    "end": "4876790"
  },
  {
    "text": "I mean, there's nothing that's\nstrange there at all. After you understand it,\neverything works.",
    "start": "4876790",
    "end": "4883100"
  },
  {
    "text": "So we'll do that next time. ",
    "start": "4883100",
    "end": "4887255"
  }
]