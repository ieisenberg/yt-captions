[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6330"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6330",
    "end": "13320"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13320",
    "end": "21870"
  },
  {
    "text": "RUSS TEDRAKE: Welcome back. So today we get to\nfinish our discussion",
    "start": "21870",
    "end": "29600"
  },
  {
    "text": "on at least the first wave\nof value-based methods for trying to find\noptimal control",
    "start": "29600",
    "end": "35777"
  },
  {
    "text": "policies, without a model. So we started last time talking\nabout these model-free methods.",
    "start": "35777",
    "end": "47040"
  },
  {
    "text": "And just to make sure\nwe're all synced up here, so big picture is\nthat we're trying",
    "start": "47040",
    "end": "61090"
  },
  {
    "text": "to learn an optimal policy,\napproximate optimal policy,",
    "start": "61090",
    "end": "74920"
  },
  {
    "text": "without a model, by just\nlearning an approximate value",
    "start": "74920",
    "end": "85350"
  },
  {
    "text": "function. ",
    "start": "85350",
    "end": "96619"
  },
  {
    "text": "And the claim is that value\nfunctions are a good thing",
    "start": "96620",
    "end": "101720"
  },
  {
    "text": "to learn for a\ncouple of reasons. First of all, they\nshould describe",
    "start": "101720",
    "end": "107060"
  },
  {
    "text": "everything you need to know\nabout the optimal control.",
    "start": "107060",
    "end": "113720"
  },
  {
    "text": "Second of all, they're\nactually fairly compact. I'm going to say more\nabout that in a minute.",
    "start": "113720",
    "end": "119060"
  },
  {
    "text": "But if you think about it,\nprobably a value function might actually be\na simpler thing to represent than the policy, a\nsmaller thing to represent it,",
    "start": "119060",
    "end": "128449"
  },
  {
    "text": "because it's just a scalar\nvalue over all states. ",
    "start": "128449",
    "end": "134480"
  },
  {
    "text": "And the third big motivation\nI tried to give last time was that these temporal\ndifference methods which",
    "start": "134480",
    "end": "143390"
  },
  {
    "text": "bootstrap based on\nprevious experience, they're, like value iteration\nand dynamic programming,",
    "start": "143390",
    "end": "149040"
  },
  {
    "text": "can be very efficient in terms\nof reusing the computation or reusing the samples\nthat you've gotten by using",
    "start": "149040",
    "end": "155720"
  },
  {
    "text": "estimates that you've\nalready made with your value function to make better,\nfast estimates of your value",
    "start": "155720",
    "end": "160959"
  },
  {
    "text": "function as you [INAUDIBLE]. These are all going\nto come up again. But that's just the\nhigh-level motivation",
    "start": "160960",
    "end": "167209"
  },
  {
    "text": "for why we care about trying\nto learn value functions. And then first thing\nwe did-- it was really all we did last time.",
    "start": "167210",
    "end": "174950"
  },
  {
    "text": "The first thing\nwe had to achieve was just estimate a value\nfunction for a fixed policy,",
    "start": "174950",
    "end": "199996"
  },
  {
    "text": "which we called J pi, right? And we did it just from\nsample trajectories.",
    "start": "199996",
    "end": "206582"
  },
  {
    "text": " In discrete state and action,\nwe called them s and a.",
    "start": "206582",
    "end": "214910"
  },
  {
    "text": "Take a bunch of\ntrajectories, and you would be able from\nthose trajectories",
    "start": "214910",
    "end": "222170"
  },
  {
    "text": "to try to back out J pi. And those trajectories are\ngenerated using policy pi.",
    "start": "222170",
    "end": "229118"
  },
  {
    "text": " So I actually tried to argue\nthat that was useful even",
    "start": "229118",
    "end": "234180"
  },
  {
    "text": "in the-- so if you just want-- if you have a robot out\nthere that's already executing a policy, or a passive\nwalker or something like this",
    "start": "234180",
    "end": "239989"
  },
  {
    "text": "that doesn't have a\npolicy, and you just want to see how well it's\ndoing, estimate its stability",
    "start": "239990",
    "end": "246260"
  },
  {
    "text": "by example, then\nyou can actually-- this might be enough for you. You might just try to evaluate\nhow well that policy is doing.",
    "start": "246260",
    "end": "255020"
  },
  {
    "text": "We call that policy evaluation. ",
    "start": "255020",
    "end": "263188"
  },
  {
    "text": "What we're actually\ninterested it's not that. That's just the first step.",
    "start": "263188",
    "end": "268840"
  },
  {
    "text": "What we care about now is,\ngiven if we can estimate the value for a\nstationary policy,",
    "start": "268840",
    "end": "277720"
  },
  {
    "text": "can we now do something\nsmarter and more involved, and try to estimate what\nthe optimal value function.",
    "start": "277720",
    "end": "285490"
  },
  {
    "text": "Or you might think\nof it as continuing to estimate the value\nfunction as we change",
    "start": "285490",
    "end": "291490"
  },
  {
    "text": "pi towards the [INAUDIBLE]\nthe optimal cost, the optimal policy. ",
    "start": "291490",
    "end": "299220"
  },
  {
    "text": "We talked about a couple of ways\nto estimate the value function for a fixed pi, right?",
    "start": "299220",
    "end": "304590"
  },
  {
    "text": "Even for function approximation,\nwe did first Markov chains,",
    "start": "304590",
    "end": "311820"
  },
  {
    "text": "and then we went to\nfunction approximation. ",
    "start": "311820",
    "end": "321720"
  },
  {
    "text": "And we have convergence\nresults for linear function",
    "start": "321720",
    "end": "331290"
  },
  {
    "text": "approximators. ",
    "start": "331290",
    "end": "336425"
  },
  {
    "text": "And we went back and looked up\n[INAUDIBLE] it had a question about whether they used\nlambda in your update,",
    "start": "336425",
    "end": "342335"
  },
  {
    "text": "if it always got to\nthe same estimate of J. And I think the answer\nwas, yes, it always gets--",
    "start": "342335",
    "end": "347850"
  },
  {
    "text": "the convergence proof\nhas an error bound. And that error bound\ndoes depend on lambda,",
    "start": "347850",
    "end": "353030"
  },
  {
    "text": "if you remember\n[INAUDIBLE] discussion. But if you said your learning\nrate gets smaller and smaller",
    "start": "353030",
    "end": "358970"
  },
  {
    "text": "and you go, it should converge\nto the-- they should all converge to the same\nestimate of J pi.",
    "start": "358970",
    "end": "364658"
  },
  {
    "start": "364658",
    "end": "371300"
  },
  {
    "text": "So if you think about\nit, learning J pi shouldn't involve any\nnew machinery, right?",
    "start": "371300",
    "end": "378419"
  },
  {
    "text": "If I'm just experiencing cost,\nand I'm experiencing states, and I'm trying to learn a\nfunction of cost-to-go given",
    "start": "378420",
    "end": "384440"
  },
  {
    "text": "states, that should just be able\nto do a least squares function. It's just a standard\nfunction approximation task.",
    "start": "384440",
    "end": "390449"
  },
  {
    "text": "I could just do just a least\nsquares function approximation,",
    "start": "390450",
    "end": "397765"
  },
  {
    "text": "least squares\nestimation, and what",
    "start": "397765",
    "end": "405230"
  },
  {
    "text": "we call the Monte-Carlo error.  Just run a bunch of\ntrials, figure out",
    "start": "405230",
    "end": "412918"
  },
  {
    "text": "the estimates of what the\ncost-to-go was at every time, and then just do least\nsquares estimation.",
    "start": "412918",
    "end": "419100"
  },
  {
    "text": "The machinery we\ndeveloped last time was because it's actually\na lot faster using",
    "start": "419100",
    "end": "426078"
  },
  {
    "text": "bootstrapping algorithms. [INAUDIBLE] much faster\nthan [INAUDIBLE]..",
    "start": "426078",
    "end": "432152"
  },
  {
    "start": "432152",
    "end": "443460"
  },
  {
    "text": "Right. So we talked about the\nTD lambda algorithm,",
    "start": "443460",
    "end": "449120"
  },
  {
    "text": "including for function\napproximation. ",
    "start": "449120",
    "end": "454165"
  },
  {
    "text": "The only reason we had to\ndevelop any new machinery is because we wanted to be able\nto essentially do the least",
    "start": "454165",
    "end": "459630"
  },
  {
    "text": "squares estimation,\nbut we wanted to reuse our current estimate\nas we build up the estimate.",
    "start": "459630",
    "end": "464840"
  },
  {
    "text": "And that's why it's not just a\nstandard function approximation task we did all the [INAUDIBLE]\nsomething [INAUDIBLE]",
    "start": "464840",
    "end": "474778"
  },
  {
    "text": "presented it. OK. ",
    "start": "474778",
    "end": "482520"
  },
  {
    "text": "So that's the simple\npolicy evaluation story. ",
    "start": "482520",
    "end": "490420"
  },
  {
    "text": "Now the question is, how\ndo we use the ability to do policy evaluation to get\ntowards a more optimal policy?",
    "start": "490420",
    "end": "498004"
  },
  {
    "text": " So today, given the\nnew policy evaluation,",
    "start": "498004",
    "end": "514789"
  },
  {
    "text": "we want to improve the policy. ",
    "start": "514789",
    "end": "530920"
  },
  {
    "text": "And the idea of this-- the first idea you have to\nhave in your head, very, very simple.",
    "start": "530920",
    "end": "536255"
  },
  {
    "text": " And it's called\npolicy iteration.",
    "start": "536255",
    "end": "542410"
  },
  {
    "start": "542410",
    "end": "552629"
  },
  {
    "text": "So given I start off with some\ninitial guess for a policy, and I run it for a little while,\nI could do policy evaluation.",
    "start": "552630",
    "end": "562840"
  },
  {
    "text": "So I'm converged on a nice\nestimate to get J pi 1.",
    "start": "562840",
    "end": "568570"
  },
  {
    "text": " And now I'd like to take\nJ pi 1, my estimate,",
    "start": "568570",
    "end": "576690"
  },
  {
    "text": "and come up with a new pi 2.",
    "start": "576690",
    "end": "583810"
  },
  {
    "text": "We've talked about how the\nvalue function infers a policy.",
    "start": "583810",
    "end": "589670"
  },
  {
    "text": "And if I repeat, and\nI do it properly,",
    "start": "589670",
    "end": "594740"
  },
  {
    "text": "then if all goes well,\nI should find myself-- if it's always increasing\nin performance,",
    "start": "594740",
    "end": "600820"
  },
  {
    "text": "and we can show that, then I\nshould find myself eventually",
    "start": "600820",
    "end": "606730"
  },
  {
    "text": "at the optimal policy and\noptimal value function, right?",
    "start": "606730",
    "end": "613888"
  },
  {
    "start": "613888",
    "end": "619760"
  },
  {
    "text": "So we said TD lambda\nwas a candidate for sitting there and\nevaluating policy, which I've",
    "start": "619760",
    "end": "625515"
  },
  {
    "text": "talked about a couple\nof different ways to do policy evaluation. So the question now is,\nhow do we this, then?",
    "start": "625515",
    "end": "631127"
  },
  {
    "text": "That's the first question. ",
    "start": "631127",
    "end": "636660"
  },
  {
    "text": "So given your policy,\ngiven your value function, how do you compute\na new policy that's",
    "start": "636660",
    "end": "642890"
  },
  {
    "text": "at least as good as your\nown policy but maybe better? ",
    "start": "642890",
    "end": "662000"
  },
  {
    "text": "AUDIENCE: Maybe stochastic\ngradient descent? RUSS TEDRAKE: Do something like\nstochastic gradient descent? You have to be careful\nwith stochastic gradient.",
    "start": "662000",
    "end": "667440"
  },
  {
    "text": "You have to make sure\nit's always going down, and things like that. It's a good idea.",
    "start": "667440",
    "end": "674610"
  },
  {
    "text": "In fact, that's sort of-- actually, [INAUDIBLE]. We combine stochastic gradient\ndescent and evaluation",
    "start": "674610",
    "end": "682839"
  },
  {
    "text": "to do actor-critic [INAUDIBLE]. But there's a\nsimpler sort of idea.",
    "start": "682840",
    "end": "689618"
  },
  {
    "text": " I guess the thing it requires-- I didn't even think about this\nwhen I was making the notes.",
    "start": "689618",
    "end": "696148"
  },
  {
    "text": "But I guess it requires\nan observation that-- ",
    "start": "696148",
    "end": "701870"
  },
  {
    "text": "so the optimal value function\nand the optimal policy",
    "start": "701870",
    "end": "707810"
  },
  {
    "text": "have a property that\nthe policy is going, taking the fastest descent\ndown the value function.",
    "start": "707810",
    "end": "714187"
  },
  {
    "text": "Your job is to go down the value\nfunction as fast as possible. ",
    "start": "714187",
    "end": "719510"
  },
  {
    "text": "But if you're not optimal yet,\nI've got some random policy, and I figure out my value of\nexecuting that policy, that's",
    "start": "719510",
    "end": "729009"
  },
  {
    "text": "actually not true yet. So what I need to say is, if\nyou start giving your value",
    "start": "729010",
    "end": "735830"
  },
  {
    "text": "function, you come up\nwith a new policy which tries to be as\naggressive as possible",
    "start": "735830",
    "end": "741050"
  },
  {
    "text": "on this value function, which\nin our continuous sense, is going down the gradient\nof the value function",
    "start": "741050",
    "end": "746450"
  },
  {
    "text": "as fast as possible. And that should be\nat least as good-- in the case of the optimal\npolicy, it should be the same.",
    "start": "746450",
    "end": "753380"
  },
  {
    "text": "It should return the\noptimal policy again. But in the case where the\nvalue estimates from another,",
    "start": "753380",
    "end": "759259"
  },
  {
    "text": "original policy gets\nyou to do better. ",
    "start": "759260",
    "end": "764755"
  },
  {
    "text": "So the basic story-- that's the continuous\ngradient-- is you want to come up with a\ngreedy policy that moves down,",
    "start": "764755",
    "end": "777050"
  },
  {
    "text": "that does the best it\ncan with this J pi. ",
    "start": "777050",
    "end": "786660"
  },
  {
    "text": "So pi 2, let's say, which is\na function of s, should be,",
    "start": "786660",
    "end": "792899"
  },
  {
    "text": "for instance, [INAUDIBLE]\nthe discrete sense here, discrete state and action,\nminimize the expected value",
    "start": "792900",
    "end": "801263"
  },
  {
    "text": "[INAUDIBLE] expected value\nfirst, by one-step error plus--",
    "start": "801263",
    "end": "807030"
  },
  {
    "start": "807030",
    "end": "833430"
  },
  {
    "text": "So I've got the cost\nthat I incur here plus the long-term cost here.",
    "start": "833430",
    "end": "839190"
  },
  {
    "text": "I want to pick the\nnew min over a. ",
    "start": "839190",
    "end": "844480"
  },
  {
    "text": "The best thing I can\ndo given that estimate of the value function. And that's going to give me\na new policy, actually, pi",
    "start": "844480",
    "end": "851530"
  },
  {
    "text": "2, which is greedy with respect\nto this estimate of the value function. ",
    "start": "851530",
    "end": "860500"
  },
  {
    "text": "What does that look\nlike to you guys? AUDIENCE: [INAUDIBLE]",
    "start": "860500",
    "end": "865600"
  },
  {
    "text": "RUSS TEDRAKE: Yeah. OK. So value iteration, or\ndynamic programming, is exactly policy\niteration in the case",
    "start": "865600",
    "end": "874440"
  },
  {
    "text": "where you do a sweep\nthrough your entire state space every time, and then you\nupdate, sweep your entire state",
    "start": "874440",
    "end": "880860"
  },
  {
    "text": "space, you do the update. ",
    "start": "880860",
    "end": "915670"
  },
  {
    "text": "Absolutely. But it's a more general idea\nthan just value iteration. You don't have to\nactually evaluate all s.",
    "start": "915670",
    "end": "922330"
  },
  {
    "text": "You might call it\nasynchronous value-- [INAUDIBLE]? AUDIENCE: Shouldn't that\nbe argmin [INAUDIBLE]??",
    "start": "922330",
    "end": "927891"
  },
  {
    "text": "RUSS TEDRAKE: Oh, good. Thank you, yeah. This is argmin. Good catch.",
    "start": "927892",
    "end": "934258"
  },
  {
    "text": "Yeah. AUDIENCE: This is\nlike g [INAUDIBLE]..",
    "start": "934258",
    "end": "940117"
  },
  {
    "text": "RUSS TEDRAKE: Right. I always minimize this. So g is [? bad. ?] Well, I\ndon't promise that I will never",
    "start": "940117",
    "end": "951040"
  },
  {
    "text": "make a mistake with\nthe signs, because I try to use reinforcement\n[INAUDIBLE] notation with costs, and I can sometimes\nget myself into trouble.",
    "start": "951040",
    "end": "957580"
  },
  {
    "text": "I never write \"arg.\" It's always g.",
    "start": "957580",
    "end": "963390"
  },
  {
    "text": "OK. So so this would be argmin.",
    "start": "963390",
    "end": "968620"
  },
  {
    "text": "The min is the value estimated\nin the case of value iteration. But in general, you don't\nhave to wait till you",
    "start": "968620",
    "end": "974410"
  },
  {
    "text": "sweep the entire state space. You can just take\na single trajectory through, update\nyour value J. Or you",
    "start": "974410",
    "end": "982490"
  },
  {
    "text": "take lots of trajectories\nthrough [INAUDIBLE] get an improved estimate\nfor J, and then do this",
    "start": "982490",
    "end": "990300"
  },
  {
    "text": "and get a new policy, right? In this policy iteration,\nthe original idea",
    "start": "990300",
    "end": "995730"
  },
  {
    "text": "is you should really do\nthis policy evaluation step until your estimate of J pi\nconvergence, and then move on.",
    "start": "995730",
    "end": "1003480"
  },
  {
    "text": "But in fact, value\niteration and other-- many algorithms\nshow that you can-- it actually is still\nstable when you",
    "start": "1003480",
    "end": "1008930"
  },
  {
    "text": "don't wait for it to converge. ",
    "start": "1008930",
    "end": "1014660"
  },
  {
    "text": "But there's a problem\nwith what I wrote here. I don't think there's\na technical problem.",
    "start": "1014660",
    "end": "1020130"
  },
  {
    "text": "But why is that not quite what\nwe need for today's lecture? Yeah. AUDIENCE: I just had\na quick question.",
    "start": "1020130",
    "end": "1025170"
  },
  {
    "text": "So if you're going to\nbe gradient with respect to the value function\nthat you evaluate,",
    "start": "1025170",
    "end": "1030548"
  },
  {
    "text": "you can't do that\nwith a value function if you have a model, right? So you need a-- RUSS TEDRAKE: That's\nactually exactly-- you're answering the\nquestion that I was asking.",
    "start": "1030549",
    "end": "1037480"
  },
  {
    "text": "That's perfect. So from, as I said, model-free,\nmodel-free, model-free,",
    "start": "1037480",
    "end": "1042949"
  },
  {
    "text": "but then I wrote\ndown a model here. So how can I-- even in the steepest descent\nsort of continuous sense,",
    "start": "1042950",
    "end": "1052940"
  },
  {
    "text": "this is absurd. In the discrete\nsense, argmin over a is typically done with a\nsearch over all actions",
    "start": "1052940",
    "end": "1057980"
  },
  {
    "text": "in the continuous\nstate and action. I think it was finding the\ngradient down the slope.",
    "start": "1057980",
    "end": "1063809"
  },
  {
    "text": "But right. Both of those require\na model to actually do that policy [INAUDIBLE].",
    "start": "1063810",
    "end": "1069570"
  },
  {
    "text": "So the first question\nfor today is, how do we come up with\na gradient policy,",
    "start": "1069570",
    "end": "1074750"
  },
  {
    "text": "basically, without any model? [INAUDIBLE] going to say it.",
    "start": "1074750",
    "end": "1080880"
  },
  {
    "text": "[INAUDIBLE] know\nthis, but that's the-- ",
    "start": "1080880",
    "end": "1086172"
  },
  {
    "text": "what do you think? [INAUDIBLE] haven't read all\nthe [INAUDIBLE] algorithms. What do you think?",
    "start": "1086172",
    "end": "1091190"
  },
  {
    "text": "What's the-- how\ncould I possibly come up with a new policy\nwithout having a model?",
    "start": "1091190",
    "end": "1101004"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE] s n plus\n[INAUDIBLE] sample directly?",
    "start": "1101005",
    "end": "1108418"
  },
  {
    "text": "RUSS TEDRAKE: Good. You could sample. You can start to do\nsome local search to come up with [INAUDIBLE].",
    "start": "1108418",
    "end": "1113690"
  },
  {
    "text": " Turns out-- I mean,\nI didn't actually ask the question in a way that\nanybody would have answered it",
    "start": "1113690",
    "end": "1120492"
  },
  {
    "text": "in the way I wanted, so. So it turns out if\nwe changed the thing we store just a little bit,\nthen it turns out to contribute",
    "start": "1120492",
    "end": "1129830"
  },
  {
    "text": "to do model-free greedy policy. ",
    "start": "1129830",
    "end": "1138929"
  },
  {
    "text": "OK. So the way we do\nthat is a Q function. ",
    "start": "1138930",
    "end": "1145640"
  },
  {
    "text": "We need to find a Q function. It's a lot like\na value function.",
    "start": "1145640",
    "end": "1151750"
  },
  {
    "text": "But now it's a function\nof state and action. ",
    "start": "1151750",
    "end": "1170245"
  },
  {
    "text": "And we'll say this is\nstill [INAUDIBLE] this way.",
    "start": "1170245",
    "end": "1175720"
  },
  {
    "start": "1175720",
    "end": "1197270"
  },
  {
    "text": "OK. So what's a Q function? A Q function is the\ncost you should expect",
    "start": "1197270",
    "end": "1202610"
  },
  {
    "text": "to take, to incur, given\nyou're in a current state and you take a\nparticular action.",
    "start": "1202610",
    "end": "1209480"
  },
  {
    "text": "So it's a lot like\na value function. But now you're actually\nlearning a function over both state and actions. So in any state,\nQ pi is the cost",
    "start": "1209480",
    "end": "1220430"
  },
  {
    "text": "I should expect to incur given\nI take action a for one step and I follow a policy pi\nfor the rest of the time.",
    "start": "1220430",
    "end": "1228670"
  },
  {
    "text": "That make sense? So I could have my acrobot\ncontroller or something",
    "start": "1228670",
    "end": "1236130"
  },
  {
    "text": "like this. And in a current state, I've\ngot a policy that mostly gets me up, but I'm learning more than\njust what that policy would",
    "start": "1236130",
    "end": "1243560"
  },
  {
    "text": "do from this state. I'm learning what\nthat policy would have done if I had\nfor one step executed any random action\non the function,",
    "start": "1243560",
    "end": "1249483"
  },
  {
    "text": "for any random action. And then what would\nI do from the-- beginning I ran that\ncontroller for the rest of it.",
    "start": "1249483",
    "end": "1255970"
  },
  {
    "text": "Algebraically, it's going\nto make a lot of sense why we would store this. But it's actually interesting\nto think a little bit",
    "start": "1255970",
    "end": "1261695"
  },
  {
    "text": "about what that Q\nfunction should look like. And if you have a Q\nfunction, you certainly",
    "start": "1261695",
    "end": "1268179"
  },
  {
    "text": "could also get the\nvalue function,",
    "start": "1268180",
    "end": "1282215"
  },
  {
    "text": "because you can look up\nfor a given pi what action that policy would have taken.",
    "start": "1282215",
    "end": "1288690"
  },
  {
    "text": "You can always pull out your\ncurrent value function from Q.",
    "start": "1288690",
    "end": "1296514"
  },
  {
    "text": "But you can also-- ",
    "start": "1296515",
    "end": "1302050"
  },
  {
    "text": "[INAUDIBLE] simple relationship\nhere in the [INAUDIBLE].. ",
    "start": "1302050",
    "end": "1314278"
  },
  {
    "text": "And for the optimal\n[INAUDIBLE],, I should actually do that search over a.",
    "start": "1314278",
    "end": "1320680"
  },
  {
    "text": "I almost wrote minus. That can be your\njob for the day, make sure I don't\nflip any signs.",
    "start": "1320680",
    "end": "1326163"
  },
  {
    "start": "1326163",
    "end": "1334039"
  },
  {
    "text": "OK. We're roboticists in this room. What does it mean to\nlearn a Q function?",
    "start": "1334040",
    "end": "1340690"
  },
  {
    "text": "What are the implications\nof learning a Q function? ",
    "start": "1340690",
    "end": "1346065"
  },
  {
    "text": "Well, I guess I didn't say. So given the Q function pi\n[INAUDIBLE] having a Q function",
    "start": "1346065",
    "end": "1351879"
  },
  {
    "text": "makes action collection easy.",
    "start": "1351880",
    "end": "1357850"
  },
  {
    "text": " Pi 2 of s is now just a min\nover a Q pi s and a, where",
    "start": "1357850",
    "end": "1373482"
  },
  {
    "text": "Q pi was [INAUDIBLE] with pi 1. ",
    "start": "1373482",
    "end": "1384354"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] RUSS TEDRAKE: It's\nargmin [INAUDIBLE].. ",
    "start": "1384354",
    "end": "1395220"
  },
  {
    "text": "But I was willing to-- the reason to learn a\nQ function in one case here is that it tells me\nabout the other actions",
    "start": "1395220",
    "end": "1403350"
  },
  {
    "text": "I could have taken. And if I want to now\nimprove my policy, then I'll just look\nat my Q function.",
    "start": "1403350",
    "end": "1409830"
  },
  {
    "text": "At every state I'm in, instead\nof taking the one at pi a,",
    "start": "1409830",
    "end": "1414870"
  },
  {
    "text": "I'll go ahead and\ntake the best one. If pi 1 was optimal,\nthat I would just",
    "start": "1414870",
    "end": "1421269"
  },
  {
    "text": "get back the same policy. But if pi 1 wasn't\noptimal, then I'll",
    "start": "1421270",
    "end": "1426840"
  },
  {
    "text": "get back something better, given\nmy current estimate of Q. OK.",
    "start": "1426840",
    "end": "1434010"
  },
  {
    "text": "But what does it mean to run Q? And this is actually\nall you need to learn to do model-free value\n[INAUDIBLE] optimal policy.",
    "start": "1434010",
    "end": "1443884"
  },
  {
    "text": " That's actually really big.",
    "start": "1443884",
    "end": "1450529"
  },
  {
    "text": "So it's a little\nbit more to learn",
    "start": "1450530",
    "end": "1455890"
  },
  {
    "text": "than learning a value function. ",
    "start": "1455890",
    "end": "1464523"
  },
  {
    "text": "And you're learning\nabout your [INAUDIBLE].. ",
    "start": "1464524",
    "end": "1472110"
  },
  {
    "text": "If I had to learn J\npi, how big is that? If I'm going to say\nI've got n dimensional",
    "start": "1472110",
    "end": "1481260"
  },
  {
    "text": "states and m dimensional u--",
    "start": "1481260",
    "end": "1493104"
  },
  {
    "text": "I'll just think about\nthese two new cases, even though [INAUDIBLE] this. If I have to learn J\npi, how big is that?",
    "start": "1493104",
    "end": "1499460"
  },
  {
    "text": "What's that function\nmapping for? ",
    "start": "1499460",
    "end": "1509745"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nscalar learning. RUSS TEDRAKE: Learning\na scalar function over the state space to R1, just\nlearning a scalar function.",
    "start": "1509745",
    "end": "1523850"
  },
  {
    "text": "If I was learning a policy,\nhow big would that be? If I was learning a stationary\npolicy, it might be that.",
    "start": "1523850",
    "end": "1533720"
  },
  {
    "text": " So how bad is it to learn Q?",
    "start": "1533720",
    "end": "1539270"
  },
  {
    "text": " What's Q?",
    "start": "1539270",
    "end": "1544460"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE]\nasymptote [INAUDIBLE]..",
    "start": "1544460",
    "end": "1552472"
  },
  {
    "text": "RUSS TEDRAKE: Let's keep it a\ndeterministic policy for now. ",
    "start": "1552472",
    "end": "1557852"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] RUSS TEDRAKE: Yeah. ",
    "start": "1557852",
    "end": "1566962"
  },
  {
    "text": "Now I've suddenly got to\nlearn something over-- ",
    "start": "1566962",
    "end": "1573500"
  },
  {
    "text": "sorry, [INAUDIBLE] here. AUDIENCE: [INAUDIBLE]. Yeah, there.",
    "start": "1573500",
    "end": "1578519"
  },
  {
    "text": "RUSS TEDRAKE: OK. And for [INAUDIBLE],,\nwhat would it be",
    "start": "1578520",
    "end": "1585620"
  },
  {
    "text": "used to learn a modeled system? ",
    "start": "1585620",
    "end": "1590680"
  },
  {
    "text": "If I wanted to use this idea. What's that model? ",
    "start": "1590680",
    "end": "1597544"
  },
  {
    "text": "[INTERPOSING VOICES] RUSS TEDRAKE: Yeah. So f and then n plus m to Rn.",
    "start": "1597544",
    "end": "1605580"
  },
  {
    "text": " So let's just think about\nhow much you have to learn.",
    "start": "1605580",
    "end": "1611110"
  },
  {
    "text": "So the easiness of\nlearning this is not only related to the size. But it does matter.",
    "start": "1611110",
    "end": "1618270"
  },
  {
    "text": "So most of the time, as\ncontrol guys, as robotics guys we would probably try to\nlearn a model first, and then",
    "start": "1618270",
    "end": "1627390"
  },
  {
    "text": "do model-based control. The last few days\nI've been saying, let's try to do some things\nwithout learning a model.",
    "start": "1627390",
    "end": "1632580"
  },
  {
    "text": "Here's one interesting\nreason why. It's actually-- learning a\nmodel is sort of a tall order.",
    "start": "1632580",
    "end": "1640015"
  },
  {
    "text": "It's a lot to learn, right? You've got to learn from every\npossible state and action what's my x dot [INAUDIBLE].",
    "start": "1640015",
    "end": "1648780"
  },
  {
    "text": "This is only learning from\nevery possible state and action. What's the expected\ncost-to-go [INAUDIBLE]??",
    "start": "1648780",
    "end": "1656250"
  },
  {
    "text": "[INAUDIBLE] a scalar. So this is learning one\nalgorithm for all m.",
    "start": "1656250",
    "end": "1661510"
  },
  {
    "text": "And the beautiful thing\nabout optimal control, with this sort of\nadditive cost functions",
    "start": "1661510",
    "end": "1666980"
  },
  {
    "text": "and everything like\nthat, the beautiful thing is that this is all you need to\nknow to make optimal decisions.",
    "start": "1666980",
    "end": "1675190"
  },
  {
    "text": "You don't need to\nknow your model. That model is extra information. All you need to know to make\noptimal decisions, given",
    "start": "1675190",
    "end": "1683950"
  },
  {
    "text": "these additive cost\nfunctions [INAUDIBLE] is given [INAUDIBLE] a state\nand then a given action,",
    "start": "1683950",
    "end": "1689529"
  },
  {
    "text": "how much do I expect to\nincur cost [INAUDIBLE]?? It's a beautiful thing.",
    "start": "1689530",
    "end": "1696429"
  },
  {
    "text": "So if we make it stochastic,\nit gets even sort of-- learning a stochastic model,\nif your dynamics are variable",
    "start": "1696430",
    "end": "1701847"
  },
  {
    "text": "and that's important,\nyou want to do stochastic optimal control. Learning a stochastic model is\nprobably even harder than that.",
    "start": "1701847",
    "end": "1708730"
  },
  {
    "text": "Maybe I have to\nlearn the mean of x dot plus the covariance\nmatrix of x dot or something",
    "start": "1708730",
    "end": "1714820"
  },
  {
    "text": "like this. When I use a\nstochastic model, it would be even more expensive.",
    "start": "1714820",
    "end": "1720909"
  },
  {
    "text": "Q, in the Q sense-- I left it off in the first\npass just to keep it clean,",
    "start": "1720910",
    "end": "1726650"
  },
  {
    "text": "but Q is just going to be the\nexpected value around this. ",
    "start": "1726650",
    "end": "1738440"
  },
  {
    "text": "So Q is always going\nto be a scalar, even in the stochastic\noptimal control sense.",
    "start": "1738440",
    "end": "1744019"
  },
  {
    "text": " So maybe this is the biggest\npoint of optimal control,",
    "start": "1744020",
    "end": "1749529"
  },
  {
    "text": "honestly-- optimal control\nrelated to learning-- is that if you're willing to\ndo these additive expected",
    "start": "1749530",
    "end": "1759880"
  },
  {
    "text": "value optimization\nproblems, which I think you've seen lots\nof interesting problems",
    "start": "1759880",
    "end": "1764980"
  },
  {
    "text": "that fall into that\ncategory, then all you need to know to make decisions\nis to be able to-- the value",
    "start": "1764980",
    "end": "1771370"
  },
  {
    "text": "function, the Q function here. The expected value\nof future penalties.",
    "start": "1771370",
    "end": "1778220"
  },
  {
    "text": "And for everything\nelse, [INAUDIBLE]..  Important point. ",
    "start": "1778220",
    "end": "1786309"
  },
  {
    "text": "Now, just to soften it a\nlittle bit, in practice, you might not get\naway with only that.",
    "start": "1786309",
    "end": "1792080"
  },
  {
    "text": "If you have to somehow build an\nobserver to do state estimation or to estimate Q,\nand you've got-- there might be other\nreasons floating around",
    "start": "1792080",
    "end": "1799010"
  },
  {
    "text": "in your robot that might\nrequire you to learn this. But in a pure sense, that's\nreally what you need to know.",
    "start": "1799010",
    "end": "1809478"
  },
  {
    "text": "AUDIENCE: Hey, Russ? RUSS TEDRAKE: Yeah. AUDIENCE: You put x and u. Shouldn't that be s and-- RUSS TEDRAKE: Right.",
    "start": "1809478",
    "end": "1815710"
  },
  {
    "text": "We could have-- I could have said n is\nthe number of states. ",
    "start": "1815710",
    "end": "1823539"
  },
  {
    "text": "AUDIENCE: I just meant,\nshould it be s and a? RUSS TEDRAKE: Right. I would have-- I wrote the dimension of\nx, and I called it Rn,m.",
    "start": "1823540",
    "end": "1831120"
  },
  {
    "text": "So that's what I meant. If you want to make\nan analogy back here, then it would actually\nbe just the number",
    "start": "1831120",
    "end": "1836220"
  },
  {
    "text": "of elements in s and a. But I wanted to sort of be\na roboticist [INAUDIBLE] for a little bit. AUDIENCE: OK.",
    "start": "1836220",
    "end": "1841500"
  },
  {
    "text": "RUSS TEDRAKE: This is just\nthe computer scientist that did this [INAUDIBLE]. But it does make this easier,\nso I still [INAUDIBLE]..",
    "start": "1841500",
    "end": "1848513"
  },
  {
    "text": "So I meant to do that. ",
    "start": "1848513",
    "end": "1856340"
  },
  {
    "text": "So that [INAUDIBLE]. OK. So now, how do we learn Q? I told you how to learn J.\nQ looks pretty close to J.",
    "start": "1856340",
    "end": "1865510"
  },
  {
    "text": "How do I learn Q? I told you about temporal\ndifferent learning, probably wouldn't\nhave wasted your time talking about\ntemporal difference learning if it\nwasn't also relevant",
    "start": "1865510",
    "end": "1871938"
  },
  {
    "text": "for what we needed to do these\nmodel-free value methods.",
    "start": "1871938",
    "end": "1878050"
  },
  {
    "text": "So let's just see that temporal\ndifference learning also works for learning\nthese functions. OK? That's just some [INAUDIBLE].",
    "start": "1878050",
    "end": "1886760"
  },
  {
    "text": "Let's do just a\nsimple case first, where I'm just doing-- remember,\nTD0 was just bootstrapping.",
    "start": "1886760",
    "end": "1894860"
  },
  {
    "text": "It wasn't carrying\naround long-term rewards. It was just saying\n[INAUDIBLE] one step, and then I'm going to use my\nvalue estimate for the rest",
    "start": "1894860",
    "end": "1902060"
  },
  {
    "text": "of the time as my new update.  And I'll go ahead, since we're--",
    "start": "1902060",
    "end": "1908394"
  },
  {
    "text": "we talked about last time\nhow a function approximator [INAUDIBLE] reduce it to\nthe Markov chain case,",
    "start": "1908395",
    "end": "1913440"
  },
  {
    "text": "let's just do it like--  let's say [INAUDIBLE] of s is\nalpha i phi i, linear function",
    "start": "1913440",
    "end": "1927520"
  },
  {
    "text": "approximators. Or we could, in fact,\nreduce alpha t phi s, a.",
    "start": "1927520",
    "end": "1936780"
  },
  {
    "text": " OK. Then the TD lambda\nupdate it going to be--",
    "start": "1936780",
    "end": "1945049"
  },
  {
    "text": "TD0 update is just\ngoing to be alpha plus gamma call that hat\njust to be careful here--",
    "start": "1945050",
    "end": "1963855"
  },
  {
    "text": "pi s transpose.",
    "start": "1963855",
    "end": "1970290"
  },
  {
    "start": "1970290",
    "end": "1998220"
  },
  {
    "text": "These really are supposed\nto be s n and a n. I get a lot of [INAUDIBLE]\nfor my sloppy [INAUDIBLE]..",
    "start": "1998220",
    "end": "2006331"
  },
  {
    "text": "OK. And Q pi-- or the gradient\nhere in the linear function",
    "start": "2006332",
    "end": "2013530"
  },
  {
    "text": "approximator case,\nis just phi s, a. ",
    "start": "2013530",
    "end": "2021960"
  },
  {
    "text": "So if you look back\nin your notes, that's exactly what we had before,\nwhere this used to be J.",
    "start": "2021960",
    "end": "2029700"
  },
  {
    "text": "We're going to use\nis our new update-- we're going to say that\nour new estimate for J",
    "start": "2029700",
    "end": "2035730"
  },
  {
    "text": "is basically the one-step cost\nplus the long-term look-ahead. a n plus 1 in the case\nof doing an on-policy--",
    "start": "2035730",
    "end": "2045470"
  },
  {
    "text": "if I'm just trying to\ndo policy evaluation, it's going to be pi s n plus 1.",
    "start": "2045470",
    "end": "2053558"
  },
  {
    "text": "We'll use that\none-step prediction minus my current prediction\nand try to make that go to 0.",
    "start": "2053558",
    "end": "2060298"
  },
  {
    "text": "And in order to do it in a\nfunction approximator sense, that means multiplying that\nerror, the temporal difference error, by the gradient.",
    "start": "2060298",
    "end": "2066840"
  },
  {
    "text": "And that was something\nlike gradient descent on your temporal\ndifference policy.",
    "start": "2066840",
    "end": "2072149"
  },
  {
    "text": "But not exactly, because it's\nthis whole recursive dependence",
    "start": "2072150",
    "end": "2077888"
  },
  {
    "text": "thing.  People get why-- do\npeople get that it's not",
    "start": "2077889",
    "end": "2085149"
  },
  {
    "text": "quite gradient descent\nbut kind of this?",
    "start": "2085150",
    "end": "2090449"
  },
  {
    "text": "This looks a lot\nlike what I would get if I was trying to do\ngradient descent [INAUDIBLE],,",
    "start": "2090449",
    "end": "2098638"
  },
  {
    "text": "right?  But only in the case of TD1\nwas actually gradient descent.",
    "start": "2098638",
    "end": "2105190"
  },
  {
    "text": "But normally if I\nhave a y minus f of x,",
    "start": "2105190",
    "end": "2110319"
  },
  {
    "text": "I'm trying to do the gradient\nwith respect to this, I've got to minimize this.",
    "start": "2110320",
    "end": "2116400"
  },
  {
    "text": " And I'll get something-- if I\ntake the gradient with respect",
    "start": "2116400",
    "end": "2122452"
  },
  {
    "text": "to alpha, I get the error\nalpha x [INAUDIBLE]..",
    "start": "2122452",
    "end": "2130404"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE] RUSS TEDRAKE: Because what we\ngot here, this is our error.",
    "start": "2130405",
    "end": "2137730"
  },
  {
    "text": "If we assume that this\nis just my desired and this is my actual, then\nthis is gradient descent.",
    "start": "2137730",
    "end": "2145380"
  },
  {
    "text": "But it's not quite that, because\nthis depends on an alpha-- these all depend on alpha. So by virtue of having\nthis one in the alpha,",
    "start": "2145380",
    "end": "2152069"
  },
  {
    "text": "it's not exactly gradient\ndescent algorithm. But it still works.",
    "start": "2152070",
    "end": "2157460"
  },
  {
    "text": "People proved that it works. Is that OK? And actually, in the\ncase where TD is one,",
    "start": "2157460",
    "end": "2163920"
  },
  {
    "text": "these things actually\ngo through it and cancel each other out with\nwhatever is a gradient descent",
    "start": "2163920",
    "end": "2170650"
  },
  {
    "text": "algorithm [INAUDIBLE].  But I want you to\nsee, this is my error",
    "start": "2170650",
    "end": "2177180"
  },
  {
    "text": "I'm trying to make Q and\nmy current state and action look like my one-step cost plus\nQ of my next state and action.",
    "start": "2177180",
    "end": "2184823"
  },
  {
    "text": "And I would do that by\nmultiplying my error by my gradient in a gradient\ndescent kind of idea. ",
    "start": "2184823",
    "end": "2195890"
  },
  {
    "text": "OK. You can still do TD\nlambda if you like also.",
    "start": "2195890",
    "end": "2203615"
  },
  {
    "text": " Q functions And\nthe big idea there",
    "start": "2203615",
    "end": "2220650"
  },
  {
    "text": "was to use an\neligibility trace, which in the function\napproximator case,",
    "start": "2220650",
    "end": "2230080"
  },
  {
    "text": "was gamma lambda ei\nn plus [INAUDIBLE]..",
    "start": "2230080",
    "end": "2238610"
  },
  {
    "start": "2238610",
    "end": "2254480"
  },
  {
    "text": "And then my update\nis the same thing-- alpha-- because this is my\nbig temporal difference error.",
    "start": "2254480",
    "end": "2261470"
  },
  {
    "text": "And instead of multiplying\nby the gradient [INAUDIBLE] this eligibility trace.",
    "start": "2261470",
    "end": "2268110"
  },
  {
    "text": "And magically through\nan algebraic trick, remembering the gradient\ncomputes the bootstrapping case",
    "start": "2268110",
    "end": "2278380"
  },
  {
    "text": "when lambda is 0, and the Monte\nCarlo case when lambda is 1, and something in between\nwhen lambda is [INAUDIBLE]..",
    "start": "2278380",
    "end": "2285258"
  },
  {
    "start": "2285258",
    "end": "2292700"
  },
  {
    "text": "OK. So you'd still do\ntemporal difference there. Big point number two--",
    "start": "2292700",
    "end": "2298670"
  },
  {
    "text": " big idea number one is we\nhave to use Q functions to do",
    "start": "2298670",
    "end": "2308359"
  },
  {
    "text": "action selection.  Big point number two is\noff-policy policy evaluation.",
    "start": "2308360",
    "end": "2323600"
  },
  {
    "text": "Once we start using Q,\nyou could do this trick that I mentioned first thing\nwe're doing value methods.",
    "start": "2323600",
    "end": "2333140"
  },
  {
    "text": "And that is to execute policy pi\n1 but learn Q pi 2 [INAUDIBLE]..",
    "start": "2333140",
    "end": "2352598"
  },
  {
    "start": "2352598",
    "end": "2359542"
  },
  {
    "text": "Can you see how we do that? ",
    "start": "2359542",
    "end": "2380550"
  },
  {
    "text": "By virtue of having\nthis extra dimension, we know we're\nlearning-- bless you-- not only what happens when I\ntake policy pi from state s.",
    "start": "2380550",
    "end": "2390329"
  },
  {
    "text": "I'm learning what happens when\nI take any action in state s. That gives me a lot more power.",
    "start": "2390330",
    "end": "2399830"
  },
  {
    "text": "Because for instance, when I'm\nmaking my temporal difference error, I don't need\nto necessarily use",
    "start": "2399830",
    "end": "2406950"
  },
  {
    "text": "my one-step prediction\nas the current policy.",
    "start": "2406950",
    "end": "2413180"
  },
  {
    "text": "I can just look up what\nwould policy 2 [INAUDIBLE].. ",
    "start": "2413180",
    "end": "2422700"
  },
  {
    "text": "Because I'm storing\nevery state-action pair,",
    "start": "2422700",
    "end": "2427950"
  },
  {
    "text": "it's more to learn, more work. But it means I can say,\nI'd like my new Q pi 2",
    "start": "2427950",
    "end": "2435090"
  },
  {
    "text": "to be the one-step policy\nI got from taking a plus the long-term cost of\ntaking policy pi 2.",
    "start": "2435090",
    "end": "2441838"
  },
  {
    "text": " And then all the same equations\nplay out, and you get--",
    "start": "2441838",
    "end": "2449710"
  },
  {
    "text": " you get an estimate\nfor policy 2.",
    "start": "2449710",
    "end": "2455359"
  },
  {
    "text": " AUDIENCE: Does it count-- ",
    "start": "2455360",
    "end": "2462930"
  },
  {
    "text": "RUSS TEDRAKE: Yeah? AUDIENCE: Does it count\nmore than the first step of the policy 2 and then take\nyour cost-to-go of policy 1?",
    "start": "2462930",
    "end": "2471294"
  },
  {
    "text": "Or does it somehow--  RUSS TEDRAKE: So\nI can't switch--",
    "start": "2471294",
    "end": "2479157"
  },
  {
    "text": "we'll talk about whether you\ncan switch halfway through. But once I commit\nto learning Q pi 2,",
    "start": "2479157",
    "end": "2486660"
  },
  {
    "text": "then actually this\nwhole thing is built up of experience of\nexecuting policy 2,",
    "start": "2486660",
    "end": "2492540"
  },
  {
    "text": "even though I've only generated\nsample paths for policy 1.",
    "start": "2492540",
    "end": "2497710"
  },
  {
    "text": "So it's a completely consistent\nestimator of Q pi 2, right? If I halfway through\ndecided I wanted",
    "start": "2497710",
    "end": "2504090"
  },
  {
    "text": "to start evaluating\npi 3, then I'm going to have to wait\nfor those cancel out, or we play some\ntricks to do that.",
    "start": "2504090",
    "end": "2509890"
  },
  {
    "text": "But it actually\nrecursively builds up in the estimator of pi 2.",
    "start": "2509890",
    "end": "2515370"
  },
  {
    "text": "AUDIENCE: Can I ask a question? RUSS TEDRAKE: Of course. AUDIENCE: [INAUDIBLE] have that\n[INAUDIBLE] function like this,",
    "start": "2515370",
    "end": "2523510"
  },
  {
    "text": "we can substitute y-- RUSS TEDRAKE: You're\ntalking about this? AUDIENCE: Yes. You can substitute y\nby [? g or ?] gamma,",
    "start": "2523510",
    "end": "2529570"
  },
  {
    "text": "and then execute the-- RUSS TEDRAKE: Yeah. AUDIENCE: And then\ntake the derivative?",
    "start": "2529571",
    "end": "2534960"
  },
  {
    "text": "RUSS TEDRAKE: Yes. AUDIENCE: Why [INAUDIBLE]? RUSS TEDRAKE: So why isn't\nit true gradient descent? ",
    "start": "2534960",
    "end": "2543040"
  },
  {
    "text": "That's exactly what\nI proposed to do. But the only problem is,\nthis isn't what we have. What we actually\nhave is this, which",
    "start": "2543040",
    "end": "2552410"
  },
  {
    "text": "means that this is not the\ntrue gradient [INAUDIBLE] term for partial y partial\nfrom over here. AUDIENCE: That's\nwhat I'm suggesting.",
    "start": "2552410",
    "end": "2558382"
  },
  {
    "text": "So instead of y alpha, we\ncan actually [INAUDIBLE] g plus gamma-- an\napproximation of y.",
    "start": "2558382",
    "end": "2563510"
  },
  {
    "text": "So this g plus gamma\nQ is [INAUDIBLE] approximation for y, right? RUSS TEDRAKE: I'm\ntrying to perfectly make",
    "start": "2563510",
    "end": "2570150"
  },
  {
    "text": "the analogy that this looks like\nthat, and this looks like that.",
    "start": "2570150",
    "end": "2577719"
  },
  {
    "text": "AUDIENCE: Right. But when we're taking the\nderivative from that function, we assume that y is constant. RUSS TEDRAKE: Yes. AUDIENCE: And then solve this.",
    "start": "2577719",
    "end": "2583610"
  },
  {
    "text": "RUSS TEDRAKE: Yes. AUDIENCE: We can actually assume\nthat y is dependent on alpha and the derivative of that term\nwith respect to alpha as well,",
    "start": "2583610",
    "end": "2590060"
  },
  {
    "text": "and then solve it. RUSS TEDRAKE: Yes. So you could do that. So you're saying why\ndon't we actually have a different update which has\nthe gradient [INAUDIBLE]??",
    "start": "2590060",
    "end": "2597079"
  },
  {
    "text": "OK, good. So in the case of TD0-- TD1, you actually do have that.",
    "start": "2597080",
    "end": "2603089"
  },
  {
    "text": "And I think that's true. I worked this out a\nnumber of years ago. But I think it's true that\nif you start including that,",
    "start": "2603090",
    "end": "2610280"
  },
  {
    "text": "if you look at the sum over a\nchain, for this standard update",
    "start": "2610280",
    "end": "2616100"
  },
  {
    "text": "with TD0, for instance,\nthat those terms, this term now will actually\ncancel itself out on this term",
    "start": "2616100",
    "end": "2623302"
  },
  {
    "text": "here, for instance. It doesn't work. It doesn't work nicely. It would give you-- it gives\nyou back the Monte Carlo error.",
    "start": "2623302",
    "end": "2629663"
  },
  {
    "text": "It doesn't do\ntemporal difference. It doesn't do the bootstrapping. So basically, you\nstart including that,",
    "start": "2629663",
    "end": "2635550"
  },
  {
    "text": "then you do get a least\nsquares algorithm, of course. But it's effectively\ndoing Monte Carlo.",
    "start": "2635550",
    "end": "2644220"
  },
  {
    "text": "You have to sort of ignore\nthat do to temporal difference learning. You're actually saying, I'm\ngoing to believe this estimate",
    "start": "2644220",
    "end": "2653021"
  },
  {
    "text": "in order to do that. OK? Temporal difference,\nif you actually",
    "start": "2653021",
    "end": "2658550"
  },
  {
    "text": "want to prove any\nof these things, I have one example\nof it in the note. I think that I put in TD1 is\ngradient descent in the notes,",
    "start": "2658550",
    "end": "2666539"
  },
  {
    "text": "just so you see an example. A story-- rule of the game in\ntemporal difference learning, derivations, and proofs, is\nyou start expanding these sums,",
    "start": "2666540",
    "end": "2675420"
  },
  {
    "text": "and terms from time\nn and terms from time n plus 1 cancel each other\nout in a gradient way.",
    "start": "2675420",
    "end": "2683370"
  },
  {
    "text": "And you're left with\nsomething much more compact. That's why [? everybody ?]\ncalls it an algebraic trick, why",
    "start": "2683370",
    "end": "2688560"
  },
  {
    "text": "these things work. But because these are not random\nsamples drawn one at a time,",
    "start": "2688560",
    "end": "2696835"
  },
  {
    "text": "they're actually\ndirectly related to each other, that's why it\nmakes it more complicated.",
    "start": "2696835",
    "end": "2701857"
  },
  {
    "start": "2701857",
    "end": "2707700"
  },
  {
    "text": "OK. So we said off-policy\nevaluation says, execute policy pi 1 to get pi 1\ngenerates s n a n trajectories.",
    "start": "2707700",
    "end": "2720230"
  },
  {
    "text": "But you're going to do\nthe update alpha plus--",
    "start": "2720230",
    "end": "2726508"
  },
  {
    "text": "I'm going to just write quickly\nhere-- g s, a plus gamma Q pi--",
    "start": "2726508",
    "end": "2733810"
  },
  {
    "text": "this is going to be\nestimator Q pi 2-- s n plus 1 pi 2.",
    "start": "2733810",
    "end": "2741620"
  },
  {
    "text": "What would pi 2 have done\nin kind of s n plus 1? ",
    "start": "2741620",
    "end": "2757505"
  },
  {
    "text": "In general, we'll multiply\nit by [INAUDIBLE].. ",
    "start": "2757505",
    "end": "2764127"
  },
  {
    "text": "That's a really,\nreally nice trick. Let's learn about\npolicy 1-- or policy 2 while we execute policy 1.",
    "start": "2764127",
    "end": "2771320"
  },
  {
    "text": "OK. So what policy 2\nshould we learn about? ",
    "start": "2771320",
    "end": "2779515"
  },
  {
    "text": "And again, these are-- I'm asking the questions\nin bizarre ways, and there's a specific answer.",
    "start": "2779515",
    "end": "2785220"
  },
  {
    "text": "But [INAUDIBLE]\nask that question. Our ultimate goal\nis not to learn",
    "start": "2785220",
    "end": "2791430"
  },
  {
    "text": "about some arbitrary pi 2. I want to learn about\nthe optimal policy. ",
    "start": "2791430",
    "end": "2798180"
  },
  {
    "text": "I don't have the optimal policy. But I have an estimate of it.",
    "start": "2798180",
    "end": "2803430"
  },
  {
    "text": "So actually, a perfectly\nreasonable update to do, and the way you might\ndescribe it is, let's execute--",
    "start": "2803430",
    "end": "2813869"
  },
  {
    "text": "I'm putting it in\nquotes, because it's not entirely accurate, but\nit's the right idea-- ",
    "start": "2813870",
    "end": "2820890"
  },
  {
    "text": "execute policy 1 but learn\nabout the optimal policy.",
    "start": "2820890",
    "end": "2831005"
  },
  {
    "start": "2831006",
    "end": "2836370"
  },
  {
    "text": "And how would we do that? Well-- this is now my\nshorthand Q star here.",
    "start": "2836370",
    "end": "2852756"
  },
  {
    "text": "Estimate of Q star\nis s n plus 1--",
    "start": "2852756",
    "end": "2858565"
  },
  {
    "text": "I should have-- ",
    "start": "2858565",
    "end": "2898570"
  },
  {
    "text": "It makes total sense. Might as well, as\nI'm learning, always try to learn about policy\nwhich is optimal with respect",
    "start": "2898570",
    "end": "2905990"
  },
  {
    "text": "to my current\nestimate J [? hat. ?] And this algorithm\nis called Q-learning.",
    "start": "2905990",
    "end": "2911962"
  },
  {
    "start": "2911962",
    "end": "2917510"
  },
  {
    "text": "OK. It's the crown jewel of\nthe value-based methods",
    "start": "2917510",
    "end": "2926750"
  },
  {
    "text": "[INAUDIBLE]. I would say it was the gold\nstandard until probably about [? '90-- ?] something like that.",
    "start": "2926750",
    "end": "2932860"
  },
  {
    "text": "When people started to do policy\ngradient stuff more often. Even probably halfway\nthrough the '90s,",
    "start": "2932860",
    "end": "2938720"
  },
  {
    "text": "people were still mostly\n[INAUDIBLE] papers about Q-learning.",
    "start": "2938720",
    "end": "2944300"
  },
  {
    "text": "and there was a movement\nin policy gradient. AUDIENCE: So is your\ncurrent estimate",
    "start": "2944300",
    "end": "2949340"
  },
  {
    "text": "Q star not based on pi 1? ",
    "start": "2949340",
    "end": "2956070"
  },
  {
    "text": "RUSS TEDRAKE: It is\nbased on data from pi 1. But if I always make my\nupdate, making it this update,",
    "start": "2956070",
    "end": "2963859"
  },
  {
    "text": "then it really is\nlearning about pi 2. ",
    "start": "2963860",
    "end": "2971060"
  },
  {
    "text": "AUDIENCE: Isn't pi 2 what you're\ncomputing with this update? ",
    "start": "2971060",
    "end": "2980708"
  },
  {
    "text": "RUSS TEDRAKE: Good. There's a couple of\nways that I can do this. So in the policy-- in the simple\npolicy iteration, we",
    "start": "2980708",
    "end": "2988635"
  },
  {
    "text": "use [INAUDIBLE] evaluate\nfor a long time, and then you make an, update,\nyou evaluate for a long time,",
    "start": "2988635",
    "end": "2994641"
  },
  {
    "text": "and you make an update. AUDIENCE: This is dynamically-- RUSS TEDRAKE: This is always\nsort of updating, right? [INAUDIBLE]",
    "start": "2994642",
    "end": "3000220"
  },
  {
    "text": "And you can prove that it's\nstill a sound algorithm despite [INAUDIBLE]. This is always sort of\nupdating its policy as it goes.",
    "start": "3000220",
    "end": "3010922"
  },
  {
    "text": "Compared to this,\nwhich is more of the-- learn about pi 2 for a\nwhile, stop, [INAUDIBLE]",
    "start": "3010922",
    "end": "3016780"
  },
  {
    "text": "pi 3 for a while,\nstop, this is trying to go straight through pi. ",
    "start": "3016780",
    "end": "3027440"
  },
  {
    "text": "OK, good. So what is it-- what's required for a Q-learning\nalgorithm to converge?",
    "start": "3027440",
    "end": "3036270"
  },
  {
    "text": "So even for this algorithm to\nconverge, in order for pi 1 to really teach me everything\nthere is to know about pi 2,",
    "start": "3036270",
    "end": "3043660"
  },
  {
    "text": "there's some important feature,\nwhich is that pi 1 and pi 2",
    "start": "3043660",
    "end": "3050319"
  },
  {
    "text": "had better pick the same actions\nwith some old probability. ",
    "start": "3050320",
    "end": "3056690"
  },
  {
    "text": "So off-policy works. ",
    "start": "3056690",
    "end": "3061852"
  },
  {
    "text": "Let's just even think about-- I'll even [INAUDIBLE] first in\nthe discrete state and discrete",
    "start": "3061852",
    "end": "3067760"
  },
  {
    "text": "actions and the Markov--  MDP formulations.",
    "start": "3067760",
    "end": "3073770"
  },
  {
    "text": "Off-policy works if pi 1 takes\nin general all state-action",
    "start": "3073770",
    "end": "3086840"
  },
  {
    "text": "pairs with some\nsmall probability.",
    "start": "3086840",
    "end": "3091922"
  },
  {
    "start": "3091922",
    "end": "3099940"
  },
  {
    "text": "If pi 2 took action [INAUDIBLE]\nstate 1 and pi 1 never did,",
    "start": "3099940",
    "end": "3107890"
  },
  {
    "text": "there's no way I'm\ngoing to learn really what pi 2 is all about.",
    "start": "3107890",
    "end": "3113356"
  },
  {
    "text": "[INAUDIBLE] show you\nthose two [INAUDIBLE]..",
    "start": "3113356",
    "end": "3118540"
  },
  {
    "text": "OK. So how do you do that? If you just-- if you're\nthinking about greedy policies on a robot, and you've got your\ncurrent estimate of the value,",
    "start": "3118540",
    "end": "3127060"
  },
  {
    "text": "and you do the most aggressive\naction on the acrobot,",
    "start": "3127060",
    "end": "3132310"
  },
  {
    "text": "I'll tell you what's\ngoing to happen. You're going to visit the\nstates near the bottom,",
    "start": "3132310",
    "end": "3137470"
  },
  {
    "text": "and you start learning a lot. And you're never going to\nvisit the states up at the top when you're learning.",
    "start": "3137470",
    "end": "3144728"
  },
  {
    "text": "So how are you going to get\naround that on the acrobot? And the acrobot is\ntough, actually. But the idea is, you'd\nbetter add some randomness",
    "start": "3144728",
    "end": "3153829"
  },
  {
    "text": "so that you explore more\nand more state and actions. And the hope is that if you add\nenough for a long enough time,",
    "start": "3153830",
    "end": "3159288"
  },
  {
    "text": "you're going to learn\nbetter and better policies, you're going to find\nyour way up to the top.",
    "start": "3159288",
    "end": "3164330"
  },
  {
    "text": "So the acrobot is\nactually almost as hard as it gets with these\nthings, where you really have to find your\nway into this region",
    "start": "3164330",
    "end": "3170779"
  },
  {
    "text": "to learn about the region. In fact, my beef with\nreinforcement learning",
    "start": "3170780",
    "end": "3176090"
  },
  {
    "text": "community is that\nthey learn only to swing up to some threshold. They never actually\n[INAUDIBLE] to the top.",
    "start": "3176090",
    "end": "3182960"
  },
  {
    "text": "If you look, there's lots\nof papers, countless papers, written about reinforcement\nlearning, Q-learning for the acrobot and\nthings like this,",
    "start": "3182960",
    "end": "3189040"
  },
  {
    "text": "and they never actually\nsolve the problem. They just try to\n[INAUDIBLE] at the top. But they don't do it.",
    "start": "3189040",
    "end": "3194210"
  },
  {
    "text": "They just get up this high. Because it is sort\nof a tough case. ",
    "start": "3194210",
    "end": "3199583"
  },
  {
    "text": "So how do you do this? So you get-- like\nI said, in order to start exploring\nthat space, you'd better add some randomness.",
    "start": "3199583",
    "end": "3207140"
  },
  {
    "text": "So one of the\nstandard approaches is to use epsilon\ngreedy algorithms.",
    "start": "3207140",
    "end": "3212649"
  },
  {
    "start": "3212649",
    "end": "3222240"
  },
  {
    "text": "So I said, let's make pi 2\nexactly the minimizing thing.",
    "start": "3222240",
    "end": "3227280"
  },
  {
    "text": "That's true. But if you execute that, you're\nprobably going to [INAUDIBLE]",
    "start": "3227280",
    "end": "3233160"
  },
  {
    "text": "much better to\nexecute a policy pi s,",
    "start": "3233160",
    "end": "3243589"
  },
  {
    "text": "where the-- let's say\nthe policy I care about, I'm going to execute\nwith probability--",
    "start": "3243590",
    "end": "3253119"
  },
  {
    "text": "sort of, you flip a coin,\nyou pull a random number between 0 and 1.",
    "start": "3253120",
    "end": "3258983"
  },
  {
    "text": "If it's greater\nthan epsilon, then go ahead and execute\npolicy you're trying to learn\nabout, but execute",
    "start": "3258983",
    "end": "3264910"
  },
  {
    "text": "some random action otherwise.",
    "start": "3264910",
    "end": "3272822"
  },
  {
    "start": "3272822",
    "end": "3282900"
  },
  {
    "text": "OK. So every time I-- every dt I'm going to\nflip a coin, keep it--",
    "start": "3282900",
    "end": "3288020"
  },
  {
    "text": "well, not a coin. A hundred-sided coin, a 100 to-- 0 to 1, a continuous thing.",
    "start": "3288020",
    "end": "3293750"
  },
  {
    "text": "If it comes out\nless than epsilon, I'm going to do a random action.",
    "start": "3293750",
    "end": "3299240"
  },
  {
    "text": "Just forget about my current\npolicy, pick a random action. It's a uniform\ndistribution over actions.",
    "start": "3299240",
    "end": "3305240"
  },
  {
    "text": "Otherwise, I'll take this,\nthe action from my policy. And by virtue of having a\nsoft policy learning thing",
    "start": "3305240",
    "end": "3314135"
  },
  {
    "text": "is I can still learn\nabout pi 2, even if I'm taking this pi epsilon.",
    "start": "3314135",
    "end": "3319858"
  },
  {
    "text": "But I have the\nadvantage of exploring all the state-actions. ",
    "start": "3319858",
    "end": "3326109"
  },
  {
    "text": "Good. I'm missing a page. ",
    "start": "3326110",
    "end": "3332908"
  },
  {
    "text": "AUDIENCE: Is that\nthe most randomness you can produce since\nthey'll [INAUDIBLE] converge?",
    "start": "3332908",
    "end": "3339369"
  },
  {
    "text": "RUSS TEDRAKE: There's a couple\nof different candidates. The softmax is another\none that people use a lot. ",
    "start": "3339370",
    "end": "3347300"
  },
  {
    "text": "And a lot of-- I mean, in the off-policy sense,\nit's actually quite robust. So a lot of people talk about\nusing behavioral policy, which",
    "start": "3347300",
    "end": "3355470"
  },
  {
    "text": "is just sort of something\nto try-- it's designed to explore the state space. Actually, my candidate\nfor behavioral policy",
    "start": "3355470",
    "end": "3360820"
  },
  {
    "text": "is something like RRT. We should really\ntry to do something that gets me into all areas\nof state space, for instance.",
    "start": "3360820",
    "end": "3368600"
  },
  {
    "text": "And then maybe that's\na good way to design, to sample these\nstate-action pairs. And all the while, I\ntry to learn about pi 2.",
    "start": "3368600",
    "end": "3376100"
  },
  {
    "text": "So it is robust in that sense. ",
    "start": "3376100",
    "end": "3381940"
  },
  {
    "text": "When I say it works here, I\nhave to be a little careful. This is only for the MDP\ncase that it's really",
    "start": "3381940",
    "end": "3388559"
  },
  {
    "text": "guaranteed to work. There's more recent work\ndoing off-policy and function",
    "start": "3388560",
    "end": "3394589"
  },
  {
    "text": "approximators. And you can do that. ",
    "start": "3394590",
    "end": "3408450"
  },
  {
    "text": "I don't want to bury you\nguys with random detail. But you can do off-policy with\nlinear function approximators",
    "start": "3408450",
    "end": "3419220"
  },
  {
    "text": "safely, using an important\n[INAUDIBLE] when you're dealing",
    "start": "3419220",
    "end": "3428004"
  },
  {
    "text": "[INAUDIBLE]. ",
    "start": "3428004",
    "end": "3438970"
  },
  {
    "text": "And that's work by Doina Precup. ",
    "start": "3438970",
    "end": "3448789"
  },
  {
    "text": "The basic idea is you have to-- if your policy is changing,\nlike these things,",
    "start": "3448790",
    "end": "3454310"
  },
  {
    "text": "it's changing over time, you'd\nbetter weight your updates based on the relative-- ",
    "start": "3454310",
    "end": "3464444"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nnecessarily. But that's-- RUSS TEDRAKE: But what you're\nlearning about is the state--",
    "start": "3464444",
    "end": "3470940"
  },
  {
    "text": "the probability of picking\nthis action for one step and then executing pi 2. And that's still [INAUDIBLE]\neven pi 2 would never",
    "start": "3470940",
    "end": "3476586"
  },
  {
    "text": "take that action. AUDIENCE: Oh, yeah. Because it's-- OK. RUSS TEDRAKE: So\nI think it's good. The thing that has to\nhappen is that pi 2",
    "start": "3476586",
    "end": "3482869"
  },
  {
    "text": "has to be well-defined\nfor every possible state. AUDIENCE: OK. So keeping the Q pi 2 is\ntake a certain [INAUDIBLE]",
    "start": "3482870",
    "end": "3489470"
  },
  {
    "text": "take certain action, then\n[INAUDIBLE] pi [INAUDIBLE].. RUSS TEDRAKE: Yes. AUDIENCE: OK. Sorry, I lost that [INAUDIBLE]. RUSS TEDRAKE: OK, good.",
    "start": "3489470",
    "end": "3494630"
  },
  {
    "text": "Sorry. Thank you for clarifying it. Yeah, so cool. So I think that still works. ",
    "start": "3494630",
    "end": "3502790"
  },
  {
    "text": "OK. So this is good. So let me tell you\nwhere you are so far.",
    "start": "3502790",
    "end": "3508470"
  },
  {
    "text": "We've now switched from doing\ntemporal difference learning on value functions and\ntemporal difference",
    "start": "3508470",
    "end": "3513560"
  },
  {
    "text": "learning on Q functions. And a major thing\nwe got out of that was that we can do this\noff-policy learning.",
    "start": "3513560",
    "end": "3521186"
  },
  {
    "text": " You put it all together.",
    "start": "3521186",
    "end": "3526779"
  },
  {
    "text": "[INAUDIBLE] back into my\npolicy iteration diagram, and what we have, we've defined\nthe policy evaluation, that's",
    "start": "3526780",
    "end": "3533890"
  },
  {
    "text": "the TD lambda. We defined our\nupdate, which could be this in the general sense.",
    "start": "3533890",
    "end": "3539710"
  },
  {
    "text": "This one is-- if\nI used pi 1 again, If I really did on-policy,\nif I used pi 1 everywhere",
    "start": "3539710",
    "end": "3549004"
  },
  {
    "text": "while I'm executing\npi 1, then this would be called SARSA,\n[INAUDIBLE] sort",
    "start": "3549004",
    "end": "3555166"
  },
  {
    "text": "of on-policy Q-learning,\non-policy updating.",
    "start": "3555166",
    "end": "3560480"
  },
  {
    "text": "And Q-learning is this, where\nyou use the middle gradient. And what we know, what\npeople have proven,",
    "start": "3560480",
    "end": "3567930"
  },
  {
    "text": "the algorithms were in use\nfor years and years and years before it was actually proven,\neven in the tabular case,",
    "start": "3567930",
    "end": "3573580"
  },
  {
    "text": "where you have finite\nstate and actions. But now we know that\nthis thing is guaranteed",
    "start": "3573580",
    "end": "3579010"
  },
  {
    "text": "to converge to the\noptimal policy, that policy iteration, even\nif it's updated at every step,",
    "start": "3579010",
    "end": "3584319"
  },
  {
    "text": "is going to converge\nto the optimal policy and the optimal\nQ function, given",
    "start": "3584320",
    "end": "3591079"
  },
  {
    "text": "that all state-action pairs\nare [INAUDIBLE] in the tabular case. ",
    "start": "3591080",
    "end": "3601840"
  },
  {
    "text": "If we go to function\napproximation, if you just do policy\nevaluation but not update,",
    "start": "3601840",
    "end": "3609480"
  },
  {
    "text": "then we have an example\nwhere this is actually in '02 or something like that.",
    "start": "3609480",
    "end": "3616250"
  },
  {
    "text": "It'd be '01 or '02, 2001.",
    "start": "3616250",
    "end": "3621840"
  },
  {
    "text": "We finally proved that\noff-policy with linear function approximation would converge\nwhen the policy is not",
    "start": "3621840",
    "end": "3629530"
  },
  {
    "text": "changing-- no control. So the thing I need to\ngive you before we consider",
    "start": "3629530",
    "end": "3636410"
  },
  {
    "text": "this a complete story here is,\ncan we do off-policy learning?",
    "start": "3636410",
    "end": "3643589"
  },
  {
    "text": "Can we do our\npolicy improvement, update stably with\nfunction approximation?",
    "start": "3643590",
    "end": "3650310"
  },
  {
    "text": "And the algorithm\nthat we have for that is our least squares\npolicy iteration.",
    "start": "3650310",
    "end": "3655454"
  },
  {
    "start": "3655454",
    "end": "3695980"
  },
  {
    "text": "Do you remember least squares\ntemporal difference learning? Sort of the idea was that if we\nlook at the stationary update--",
    "start": "3695980",
    "end": "3705150"
  },
  {
    "text": "maybe I should\nwrite it down again. [INAUDIBLE] find it. ",
    "start": "3705150",
    "end": "3711642"
  },
  {
    "text": "If I look at the\nstationary update, if I were to run an\nentire batch of-- I mean, the big idea is when\nyou're doing the least squares,",
    "start": "3711642",
    "end": "3718370"
  },
  {
    "text": "is that we're going to\ntry to reuse old data. We're not going to just make\na single update, spit it out, throw9t away.",
    "start": "3718370",
    "end": "3723490"
  },
  {
    "text": "We're going to remember a bunch\nof old state-action pairs, trying to make a least squares\nupdate with just the same thing",
    "start": "3723490",
    "end": "3729405"
  },
  {
    "text": "as [INAUDIBLE]. In the Monte Carlo\nsense, it's easy. It's just function\napproximation,",
    "start": "3729405",
    "end": "3734540"
  },
  {
    "text": "where with this TD term\nfloating around it's harder. So we had to come up least\nsquares temporal difference learning.",
    "start": "3734540",
    "end": "3741430"
  },
  {
    "text": "And in the LSTD\ncase, the story was",
    "start": "3741430",
    "end": "3747970"
  },
  {
    "text": "we could build up a matrix using\nsomething that looked like phi",
    "start": "3747970",
    "end": "3762460"
  },
  {
    "text": "of s gamma phi transpose--",
    "start": "3762460",
    "end": "3771650"
  },
  {
    "text": "so it's ik. Let me just write ik in here--",
    "start": "3771650",
    "end": "3778309"
  },
  {
    "text": "ik plus 1 minus phi of ik. ",
    "start": "3778310",
    "end": "3785715"
  },
  {
    "text": "times my parameter vector. And b-- some of\nthese terms was e,",
    "start": "3785715",
    "end": "3797069"
  },
  {
    "text": "which were phi ik times\nour reward times--",
    "start": "3797070",
    "end": "3803291"
  },
  {
    "start": "3803291",
    "end": "3809135"
  },
  {
    "text": "And if I did this least\nsquares solution-- or I could invert that\ncarefully with SVD or something",
    "start": "3809135",
    "end": "3815810"
  },
  {
    "text": "like that-- then what I get out is the-- ",
    "start": "3815810",
    "end": "3823313"
  },
  {
    "text": "it jumps immediately to\nsteady-state solution TD",
    "start": "3823313",
    "end": "3835822"
  },
  {
    "text": "lambda. ",
    "start": "3835822",
    "end": "3845666"
  },
  {
    "text": "So this is essentially\nthe big piece of the TD lambda update\nbroken into the part that",
    "start": "3845666",
    "end": "3853650"
  },
  {
    "text": "depends on alpha, the part\nthat doesn't depend on alpha. I could write my\nbatch TD lambda update",
    "start": "3853650",
    "end": "3859755"
  },
  {
    "text": "as alpha equals alpha\nplus gamma a alpha plus b.",
    "start": "3859755",
    "end": "3866859"
  },
  {
    "text": "And I could solve that at\nsteady-state [INAUDIBLE].. ",
    "start": "3866860",
    "end": "3872690"
  },
  {
    "text": "All right. So least squares policy, least\nsquares temporal difference learning, is about reusing\nlots of trajectories",
    "start": "3872690",
    "end": "3879190"
  },
  {
    "text": "to make a single update that was\ngoing to jump right to the case where the temporal difference\nlearning we've gotten through.",
    "start": "3879190",
    "end": "3887598"
  },
  {
    "text": "We just replayed it\na bunch of times.  Now the question\nis, the policy is",
    "start": "3887598",
    "end": "3895420"
  },
  {
    "text": "going to be moving\nwhile we're doing this. How can we do this sort\nof least squares method",
    "start": "3895420",
    "end": "3903750"
  },
  {
    "text": "to do the policy\niteration up here? ",
    "start": "3903750",
    "end": "3912780"
  },
  {
    "text": "Again, the trick\nis pretty simple. We've just got to learn\nthe Q function instead,",
    "start": "3912780",
    "end": "3919555"
  },
  {
    "text": "[INAUDIBLE] biggest trick. ",
    "start": "3919555",
    "end": "3924760"
  },
  {
    "text": "So in order to do-- control not just\nevaluate a single policy but actually try to\nfind the optimal policy,",
    "start": "3924760",
    "end": "3945535"
  },
  {
    "text": "first thing we have\nto do is figure out how to do LSTD on a Q function. ",
    "start": "3945535",
    "end": "3953268"
  },
  {
    "text": "And it turns out it's no-- yeah, what's up? [INAUDIBLE]",
    "start": "3953268",
    "end": "3958680"
  },
  {
    "text": " It turns out if you\nkeep along, [INAUDIBLE]",
    "start": "3958680",
    "end": "3967000"
  },
  {
    "text": "exactly the same form as\nwe did in least squares temporal difference learning,\nbut now we do everything",
    "start": "3967000",
    "end": "3974710"
  },
  {
    "text": "with functions of s and t. ",
    "start": "3974710",
    "end": "4010554"
  },
  {
    "text": "[INAUDIBLE] transpose on it. Now I do the-- you said form of the\n[INAUDIBLE] too much.",
    "start": "4010554",
    "end": "4017964"
  },
  {
    "text": "I just want you to\nknow the big idea here. ",
    "start": "4017964",
    "end": "4033769"
  },
  {
    "text": "Then I do gamma\n[INAUDIBLE] a inverse b, this whole time we're\nrepresenting our Q function.",
    "start": "4033769",
    "end": "4040446"
  },
  {
    "text": " Q hat s, a is now a linear\ncombination of nonlinear basis",
    "start": "4040446",
    "end": "4050720"
  },
  {
    "text": "functions on s and a. AUDIENCE: Shouldn't that\nbe transpose [INAUDIBLE]??",
    "start": "4050720",
    "end": "4056904"
  },
  {
    "text": "RUSS TEDRAKE: I put-- I tried to put a\ntranspose with my poorly-- throughout everything.",
    "start": "4056905",
    "end": "4063140"
  },
  {
    "text": "So you're saying this one\nshouldn't be transpose? AUDIENCE: [INAUDIBLE]\nshould be a [INAUDIBLE]?? RUSS TEDRAKE: Yeah.",
    "start": "4063140",
    "end": "4068920"
  },
  {
    "text": "Good. So I'm going to-- but this one, I wrote\nthis whole update as the transpose of the\nother-- of what I just",
    "start": "4068920",
    "end": "4075110"
  },
  {
    "text": "wrote over there. ",
    "start": "4075110",
    "end": "4087070"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nwrite alpha? [INAUDIBLE] If it was\na plus b some stuff?",
    "start": "4087070",
    "end": "4092538"
  },
  {
    "text": "RUSS TEDRAKE: Yeah. And there's an alpha, sorry. Thank you. ",
    "start": "4092538",
    "end": "4099439"
  },
  {
    "text": "Well, actually, it's-- the\nalpha is really not there. Yeah, I should have\nwritten it here.",
    "start": "4099439",
    "end": "4106170"
  },
  {
    "text": "It's a times alpha. That's what it\nmakes the update on. So we get [INAUDIBLE] alpha.",
    "start": "4106170",
    "end": "4112896"
  },
  {
    "text": "So that is actually [INAUDIBLE]. This is the one I\ndid [INAUDIBLE].. OK, good.",
    "start": "4112896",
    "end": "4118562"
  },
  {
    "text": "So it turns out you can\nlearn a Q function just like you can learn\na value function, with storing up\nthese matrices, which",
    "start": "4118562",
    "end": "4125318"
  },
  {
    "text": "are what TD learning would\nhave done in a batch sense,",
    "start": "4125319",
    "end": "4130719"
  },
  {
    "text": "and then just taking\na one-step shot to get directly to the solution\nfor temporal difference learning for Q function.",
    "start": "4130720",
    "end": "4136965"
  },
  {
    "text": " And again, I put\nin here s prime.",
    "start": "4136966",
    "end": "4146620"
  },
  {
    "text": "And I left this a\nlittle bit ambiguous. So I can evaluate any\npolicy by just putting",
    "start": "4146620",
    "end": "4155559"
  },
  {
    "text": "in that policy in here\nand doing the replay. ",
    "start": "4155560",
    "end": "4163410"
  },
  {
    "text": "And it turns out if I now do\nthis in a policy iteration",
    "start": "4163410",
    "end": "4171649"
  },
  {
    "text": "sense, LSPI--",
    "start": "4171649",
    "end": "4183560"
  },
  {
    "text": "Least Squares Policy Iteration-- basically, you start off\nwith an initial guess,",
    "start": "4183560",
    "end": "4189170"
  },
  {
    "text": "we do LSTDQ [INAUDIBLE]\nto get Q pi 1.",
    "start": "4189170",
    "end": "4198080"
  },
  {
    "text": "And then you repeat, yeah? Then this thing, that's\nenough to get you to--",
    "start": "4198080",
    "end": "4209370"
  },
  {
    "text": "it converges. Now, be careful about\nhow it converges. It converges with some error\nbound to pi star Q star.",
    "start": "4209370",
    "end": "4231976"
  },
  {
    "text": " The error bound depends\non a couple of parameters.",
    "start": "4231976",
    "end": "4238120"
  },
  {
    "text": "So technically, it could\nbe close to your solution and oscillate, or\nsomething like that. But it's a pretty strong\nconvergence result",
    "start": "4238120",
    "end": "4244290"
  },
  {
    "text": "for this for policy improvement\nwith approximate value",
    "start": "4244290",
    "end": "4250162"
  },
  {
    "text": "function.  In a pure sense, it is--",
    "start": "4250162",
    "end": "4257480"
  },
  {
    "text": "you should run this\nfor a while until you get a good estimate for LSTD\nand you get your new Q pi.",
    "start": "4257480",
    "end": "4268510"
  },
  {
    "text": "But by virtue of using Q\nfunctions, when you do switch",
    "start": "4268510",
    "end": "4274150"
  },
  {
    "text": "to your new policy,\npi 2, let's say, you don't have to throw\naway all your old data.",
    "start": "4274150",
    "end": "4279710"
  },
  {
    "text": "You just take your old tapes\nand actually regenerate a and b",
    "start": "4279710",
    "end": "4285700"
  },
  {
    "text": "as if you had played off\nthose old tapes with-- as if you had seen the old\ntapes executing the new policy.",
    "start": "4285700",
    "end": "4293020"
  },
  {
    "text": " And you can reuse\nall your old data",
    "start": "4293020",
    "end": "4299400"
  },
  {
    "text": "and make an efficient update\nto get Q pi [INAUDIBLE].. ",
    "start": "4299400",
    "end": "4312870"
  },
  {
    "text": "Least squares policy iteration. Pretty simple. OK. I know that was a little dry\nand a little bit-- and a lot.",
    "start": "4312870",
    "end": "4319697"
  },
  {
    "text": "But let's make sure we know\nhow we got where we got. So there's another route\nbesides pure policy search",
    "start": "4319697",
    "end": "4331130"
  },
  {
    "text": "to do model-free learning. All you have to do is take\na bunch of trajectories,",
    "start": "4331130",
    "end": "4336930"
  },
  {
    "text": "learn a value function\nfor those trajectories. You don't even actually\nhave to take the perfect--",
    "start": "4336930",
    "end": "4344130"
  },
  {
    "text": "your best controller yet. You could take some RRT\ncontroller, something that's going to explore the space and\ntry to learn about your value",
    "start": "4344130",
    "end": "4350739"
  },
  {
    "text": "function.  Learn Q pi through\nthese LSTD algorithms,",
    "start": "4350740",
    "end": "4359190"
  },
  {
    "text": "you could do a pretty efficient\nupdate for doing Q pi. You can improve efficiently\nby just looking over",
    "start": "4359190",
    "end": "4364810"
  },
  {
    "text": "the min over Q,\nand pretty quickly iterate to an optimal\npolicy and optimal value",
    "start": "4364810",
    "end": "4371977"
  },
  {
    "text": "function, only storing-- the only thing you have to\nstore in that whole process",
    "start": "4371977",
    "end": "4377159"
  },
  {
    "text": "is the Q function. And in the LS case, the LSTD\ncase, you remember the tape--",
    "start": "4377160",
    "end": "4383480"
  },
  {
    "text": "the history of tapes, just\nyou use them more efficiently. Yeah? AUDIENCE: So could you have\nused this on the flapper",
    "start": "4383480",
    "end": "4391680"
  },
  {
    "text": "that John showed? Or what's the-- RUSS TEDRAKE: Good.",
    "start": "4391680",
    "end": "4397280"
  },
  {
    "text": "Very good question.  That's an excellent question.",
    "start": "4397280",
    "end": "4402590"
  },
  {
    "text": "So in fact, the last day of\nclass, what we're going to do is going to-- the last day I present in\nclass, we're going to--",
    "start": "4402590",
    "end": "4408780"
  },
  {
    "text": "I'm going to go through\na couple of sort of case studies and\ndifferent problems that people have had\nsuccess and tell you why we picked the algorithm\nwe picked, things like that.",
    "start": "4408780",
    "end": "4415760"
  },
  {
    "text": "So why didn't we do\nthis on a flapper? The simplest reason is that\nwe don't know the state space.",
    "start": "4415760",
    "end": "4422730"
  },
  {
    "text": "It's infinite\ndimensional in general. So that would have\nbeen a big thing",
    "start": "4422730",
    "end": "4428180"
  },
  {
    "text": "to represent a Q function for. It doesn't mean-- it\ndoesn't make it invalid.",
    "start": "4428180",
    "end": "4433489"
  },
  {
    "text": "We could have\nlearned, we could have tried to approximate the state\nspace with even a handful of features, learned a very\napproximate Q function,",
    "start": "4433490",
    "end": "4443060"
  },
  {
    "text": "and done something\nlike actor-critic like we're going\nto do next time. But I think in cases where you\ndon't know the state space,",
    "start": "4443060",
    "end": "4448730"
  },
  {
    "text": "or the state space is\na very, very large, and you can write a\nsimple controller, then it makes more sense\nto parameterize the policy.",
    "start": "4448730",
    "end": "4456360"
  },
  {
    "text": "It really goes down to that\ngame, that accounting game, in some ways, of how many\ndimensions things are.",
    "start": "4456360",
    "end": "4462830"
  },
  {
    "text": "But in a fluids case, you could\nhave a pretty simple policy from sensors to actions\nwhich we could twiddle.",
    "start": "4462830",
    "end": "4469960"
  },
  {
    "text": "We couldn't have an\nefficient value function.",
    "start": "4469960",
    "end": "4475230"
  },
  {
    "text": "Now, there are other cases\nwhere the opposite is true. The opposite is true, where\nyou have a small state space,",
    "start": "4475230",
    "end": "4481860"
  },
  {
    "text": "let's say, but the\nresulting policies would require a lot of\nfeatures to parameterize.",
    "start": "4481860",
    "end": "4488250"
  },
  {
    "text": "But I think in general, the\nstrength of these algorithms is that they are efficient\nwith reusing data.",
    "start": "4488250",
    "end": "4494760"
  },
  {
    "text": "The weakness is that-- well, the weakness\na few years ago would have been that they\nbig blow up the time.",
    "start": "4494760",
    "end": "4501188"
  },
  {
    "text": "But algorithms have gotten\nbetter as we [INAUDIBLE] we have some\nconvergence guarantees. Not the general [INAUDIBLE].",
    "start": "4501188",
    "end": "4507190"
  },
  {
    "text": "I never told you\nthat it converged if you have a nonlinear\nfunction approximator. We'd love to have that\nresult [INAUDIBLE]..",
    "start": "4507190",
    "end": "4513040"
  },
  {
    "text": "We won't have it for a while. But in linear function\napproximator sense, we have both.",
    "start": "4513040",
    "end": "4520290"
  },
  {
    "text": "But there's a lot\nof success stories. These are the kind of\nalgorithms that we're used to play backgammon.",
    "start": "4520290",
    "end": "4526230"
  },
  {
    "text": "There are examples\nof them working on things like the [INAUDIBLE].",
    "start": "4526230",
    "end": "4531460"
  },
  {
    "text": "But in the domains that I\ncare about most in my lab, we tend to do more policy\ngradient sort of things.",
    "start": "4531460",
    "end": "4537739"
  },
  {
    "start": "4537740",
    "end": "4539000"
  }
]