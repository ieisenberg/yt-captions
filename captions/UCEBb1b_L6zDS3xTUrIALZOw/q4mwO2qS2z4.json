[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6660"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6660",
    "end": "13640"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13640",
    "end": "23575"
  },
  {
    "text": "PROFESSOR: So today, we're going\nto talk about the probability that a random variable\ndeviates by a certain amount",
    "start": "23575",
    "end": "30770"
  },
  {
    "text": "from its expectation. Now, we've seen examples\nwhere a random variable",
    "start": "30770",
    "end": "36050"
  },
  {
    "text": "is very unlikely to deviate\nmuch from its expectation. For example, if you\nflip 100 mutually",
    "start": "36050",
    "end": "41910"
  },
  {
    "text": "independent fair\ncoins, you're very likely to wind up\nwith close to 50 heads, very unlikely to\nwind up with 25 or fewer",
    "start": "41910",
    "end": "50990"
  },
  {
    "text": "heads, for example. We've also seen examples\nof distributions where you are very likely to\nbe far from your expectation,",
    "start": "50990",
    "end": "59780"
  },
  {
    "text": "for example, that problem\nwhen we had the communications channel, and we were measuring\nthe latency of a packet",
    "start": "59780",
    "end": "64890"
  },
  {
    "text": "crossing the channel. There, most of the\ntime, your latency would be 10 milliseconds.",
    "start": "64890",
    "end": "71359"
  },
  {
    "text": "But the expected\nlatency was infinite. So you're very likely to deviate\na lot from your expectation",
    "start": "71360",
    "end": "77850"
  },
  {
    "text": "in that case. Last time, we looked\nat the variance. And we saw how that gave us\nsome feel for the likelihood",
    "start": "77850",
    "end": "86130"
  },
  {
    "text": "of being far from\nthe expectation-- high variance meaning you're\nmore likely to deviate",
    "start": "86130",
    "end": "92190"
  },
  {
    "text": "from the expectation. Today, we're going to\ndevelop specific tools for bounding or limiting\nthe probability you",
    "start": "92190",
    "end": "99750"
  },
  {
    "text": "deviate by a specified\namount from the expectation. And the first tool is\nknown as Markov's theorem.",
    "start": "99750",
    "end": "106090"
  },
  {
    "text": "Markov's theorem says that if\nthe random variable is always non-negative, then it is\nunlikely to greatly exceed",
    "start": "106090",
    "end": "115200"
  },
  {
    "text": "its expectation. In particular, if R is a\nnon-negative random variable,",
    "start": "115200",
    "end": "140820"
  },
  {
    "text": "then for all x bigger\nthan 0, the probability",
    "start": "140820",
    "end": "148770"
  },
  {
    "text": "that R is at least x is at\nmost the expected value of R,",
    "start": "148770",
    "end": "155705"
  },
  {
    "text": "the mean, divided by x. So in other words, if R is\nnever negative-- for example,",
    "start": "155705",
    "end": "165200"
  },
  {
    "text": "say the expected\nvalue is smaller. Then the probability R is\nlarge will be a small number.",
    "start": "165200",
    "end": "172239"
  },
  {
    "text": "Because I'll have a small\nnumber over a big number. So it says that you are\nunlikely to greatly exceed",
    "start": "172240",
    "end": "179640"
  },
  {
    "text": "the expected value. So let's prove that. ",
    "start": "179640",
    "end": "185050"
  },
  {
    "text": "Now, from the theorem\nof total expectation that you did in\nrecitation last week,",
    "start": "185050",
    "end": "191230"
  },
  {
    "text": "we can compute the\nexpected value of R by looking at two cases-- the\ncase when R is at least x,",
    "start": "191230",
    "end": "208690"
  },
  {
    "text": "and the case when\nR is less than x. ",
    "start": "208690",
    "end": "221860"
  },
  {
    "text": "That's from the theorem\nof total expectation. I look at two cases. R is bigger than x. Take the expected value\nthere times the probability",
    "start": "221860",
    "end": "229250"
  },
  {
    "text": "of this case happening plus\nthe case when R is less than x.",
    "start": "229250",
    "end": "234900"
  },
  {
    "text": "OK, now since R is non-negative,\nthis is at least 0.",
    "start": "234900",
    "end": "241950"
  },
  {
    "text": "R can't ever be negative. So the expectation\ncan't be negative. A probability can't be negative. So this is at least 0.",
    "start": "241950",
    "end": "250209"
  },
  {
    "text": "And this is\ntrivially at least x.",
    "start": "250210",
    "end": "255500"
  },
  {
    "text": "Because I'm taking the\nexpected value of R in the case when\nR is at least x.",
    "start": "255500",
    "end": "260850"
  },
  {
    "text": "So R is always at\nleast x in this case. So its expected\nvalue is at least x.",
    "start": "260850",
    "end": "267680"
  },
  {
    "text": "So that means that the\nexpected value of R is at least x times\nthe probability",
    "start": "267680",
    "end": "273780"
  },
  {
    "text": "R is greater than x, R\nis greater or equal to x.",
    "start": "273780",
    "end": "279700"
  },
  {
    "text": "And now I can get the theorem\nby just dividing by x. ",
    "start": "279700",
    "end": "290230"
  },
  {
    "text": "I'm less than or equal\nto the expected value of R divided by x.",
    "start": "290230",
    "end": "297980"
  },
  {
    "text": "So it's a very easy\ntheorem to prove. But it's going to have\namazing consequences that we're going to build up\nthrough a series of results",
    "start": "297980",
    "end": "305479"
  },
  {
    "text": "today. Any questions about Markov's\ntheorem and the proof? ",
    "start": "305480",
    "end": "314090"
  },
  {
    "text": "All right, there's a simple\ncorollary, which is useful. ",
    "start": "314090",
    "end": "320710"
  },
  {
    "text": "Again, if R is a\nnon-negative random variable,",
    "start": "320710",
    "end": "329930"
  },
  {
    "text": "then for all c bigger\nthan 0, the probability",
    "start": "329930",
    "end": "336110"
  },
  {
    "text": "that R is at least c times its\nexpected value is at most 1",
    "start": "336110",
    "end": "343409"
  },
  {
    "text": "and c. So the probability you're\ntwice your expected value is at most 1/2.",
    "start": "343410",
    "end": "350190"
  },
  {
    "text": "And the proof is very easy. We just set x to be equal\nto c times the expected",
    "start": "350190",
    "end": "357540"
  },
  {
    "text": "value of R in the theorem. ",
    "start": "357540",
    "end": "364020"
  },
  {
    "text": "So I just plug in x is c\ntimes the expected value of R.",
    "start": "364020",
    "end": "372569"
  },
  {
    "text": "And I get expected value of R\nover c times the expected value of R, which is 1/c.",
    "start": "372570",
    "end": "378330"
  },
  {
    "text": "So you just plug in that\nvalue in Markov's theorem, and it comes out.",
    "start": "378330",
    "end": "384794"
  },
  {
    "text": "All right, let's\ndo some examples. ",
    "start": "384794",
    "end": "391110"
  },
  {
    "text": "Let's let R be the\nweight of a random person",
    "start": "391110",
    "end": "398870"
  },
  {
    "text": "uniformly selected. ",
    "start": "398870",
    "end": "404030"
  },
  {
    "text": "And I don't know what the\ndistribution of weights is in the country.",
    "start": "404030",
    "end": "409930"
  },
  {
    "text": "But suppose that the\nexpected value of R, which is the average\nweight, is 100 pounds.",
    "start": "409930",
    "end": "417790"
  },
  {
    "text": "So if I average over all people,\ntheir weight is 100 pounds.",
    "start": "417790",
    "end": "423810"
  },
  {
    "text": "And suppose I want to\nknow the probability that the random person\nweighs at least 200 pounds.",
    "start": "423810",
    "end": "430090"
  },
  {
    "text": " What can I say about\nthat probability? ",
    "start": "430090",
    "end": "440820"
  },
  {
    "text": "Do I know it exactly? ",
    "start": "440820",
    "end": "445892"
  },
  {
    "text": "I don't think so. Because I don't know what the\ndistribution of weights is. But I can still get an upper\nbound on this probability.",
    "start": "445892",
    "end": "453530"
  },
  {
    "text": "What bound can I get\non the probability that a random\nperson has a weight of 200 given the facts here? Yeah.",
    "start": "453530",
    "end": "459302"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] ",
    "start": "459302",
    "end": "466050"
  },
  {
    "text": "PROFESSOR: Yes, well,\nit's 100 over 200, right. It's at most the expected\nvalue, which is 100,",
    "start": "466050",
    "end": "471880"
  },
  {
    "text": "over the x, which is 200. And that's equal to 1/2.",
    "start": "471880",
    "end": "477390"
  },
  {
    "text": "So the probability that a\nrandom person weighs 200 pounds or more is at most 1/2. Or I could plug it in here.",
    "start": "477390",
    "end": "484940"
  },
  {
    "text": "The expected value is 100. 200 is twice that. So c would be 2 here.",
    "start": "484940",
    "end": "491698"
  },
  {
    "text": "So the probability of\nbeing twice the expectation is at most 1/2. Now of course,\nI'm using the fact",
    "start": "491699",
    "end": "497220"
  },
  {
    "text": "that weight is never negative. That's obviously true. But it is implicitly\nbeing used here.",
    "start": "497220",
    "end": "505400"
  },
  {
    "text": "So what fraction\nof the population now can weigh at\nleast 200 pounds?",
    "start": "505400",
    "end": "512909"
  },
  {
    "text": "Slightly different question. Before I asked you, if\nI take a random person, what's the probability they\nweigh at least 200 pounds?",
    "start": "512909",
    "end": "520428"
  },
  {
    "text": "Now I'm asking, what\nfraction of the population can weigh at least 200\npounds if the average is 100?",
    "start": "520429",
    "end": "531207"
  },
  {
    "text": " What is it?",
    "start": "531207",
    "end": "536594"
  },
  {
    "text": "Yeah? AUDIENCE: At most 1/2. PROFESSOR: At most 1/2. In fact, it's the same answer.",
    "start": "536594",
    "end": "543300"
  },
  {
    "text": "And why?",
    "start": "543300",
    "end": "548450"
  },
  {
    "text": "Why can't everybody\nweigh 200 pounds, so it would be\nall the population weighs 200 pounds at least?",
    "start": "548450",
    "end": "555460"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: Probability would\nbe 1, and that can't happen.",
    "start": "555460",
    "end": "560690"
  },
  {
    "text": "And in fact,\nintuitively, if everybody weighs at least 200\npounds, the average",
    "start": "560690",
    "end": "566102"
  },
  {
    "text": "is going to be at\nleast 200 pounds. And we said the average was 100. And this is illustrating\nthis interesting thing",
    "start": "566102",
    "end": "573370"
  },
  {
    "text": "that probability implies things\nabout averages and fractions.",
    "start": "573370",
    "end": "580000"
  },
  {
    "text": "Because it's really the\nsame thing in disguise. The connection is, if I've\ngot a bunch of people, say, in the country, I can\nconvert a fraction",
    "start": "580000",
    "end": "587760"
  },
  {
    "text": "that have some property\ninto a probability by just selecting\na random person. Yeah.",
    "start": "587760",
    "end": "592775"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] ",
    "start": "592775",
    "end": "601584"
  },
  {
    "text": "PROFESSOR: No, the\nvariance could be very big. Because I might have a person\nthat weighs a million pounds,",
    "start": "601584",
    "end": "609890"
  },
  {
    "text": "say. So you have to get into that. But it gets a little\nbit more complicated.",
    "start": "609890",
    "end": "615120"
  },
  {
    "text": "Yeah. AUDIENCE: [INAUDIBLE]  PROFESSOR: No,\nthere's nothing being",
    "start": "615120",
    "end": "621310"
  },
  {
    "text": "assumed about the distribution,\nnothing at all, OK?",
    "start": "621310",
    "end": "626360"
  },
  {
    "text": "So that's the beauty\nof Markov's theorem. Well, I've assumed one thing. I assume that there\nis no negative values.",
    "start": "626360",
    "end": "633254"
  },
  {
    "text": "That's it. AUDIENCE: [INAUDIBLE] ",
    "start": "633254",
    "end": "639037"
  },
  {
    "text": "PROFESSOR: That's correct. They can distribute it any\nway with positive values. But we have a fact here we've\nused, that the average was 100.",
    "start": "639037",
    "end": "647490"
  },
  {
    "text": "So that does limit\nyour distribution. In other words, you couldn't\nhave a distribution where everybody weighs 200 pounds.",
    "start": "647490",
    "end": "654520"
  },
  {
    "text": "Because then the average\nwould be 200, not 100. But anything else where\nthey're all positive",
    "start": "654520",
    "end": "661060"
  },
  {
    "text": "and they average 100, you know\nthat at most half can be 200.",
    "start": "661060",
    "end": "666080"
  },
  {
    "text": "Because if you\npick a random one, the probability of getting\none that's 200 is at most 1/2,",
    "start": "666080",
    "end": "672200"
  },
  {
    "text": "which follows from\nMarkov's theorem. And that's partly\nwhy it's so powerful. You didn't know anything about\nthe distribution, really,",
    "start": "672200",
    "end": "679340"
  },
  {
    "text": "except its expectation and\nthat it was non-negative.",
    "start": "679340",
    "end": "684470"
  },
  {
    "text": "Any other questions about this? I'll give you some\nmore examples.",
    "start": "684470",
    "end": "689690"
  },
  {
    "text": "All right, here's\nanother example. Is it possible on the final\nexam for everybody in the class",
    "start": "689690",
    "end": "696069"
  },
  {
    "text": "to do better than\nthe mean score? No, of course not. Because if they did, the\nmean would be higher.",
    "start": "696070",
    "end": "702760"
  },
  {
    "text": "Because the mean is the average. OK, let's do another example.",
    "start": "702760",
    "end": "708430"
  },
  {
    "text": "Remember the Chinese\nappetizer problem? You're at the restaurant,\nbig circular table. There's n people at the table.",
    "start": "708430",
    "end": "715010"
  },
  {
    "text": "Everybody has one\nappetizer in front of them. And then the joker\nspins the thing in the middle of the table.",
    "start": "715010",
    "end": "720519"
  },
  {
    "text": "So it goes around and around. And it stops in a\nrandom uniform position. And we wanted to know, what's\nthe expected number of people",
    "start": "720520",
    "end": "727860"
  },
  {
    "text": "to get the right appetizer back? What was the answer? Does anybody remember?",
    "start": "727860",
    "end": "733325"
  },
  {
    "text": "One. So you expect one person to\nget the right appetizer back. Well, say I want to know the\nprobability that all n people",
    "start": "733325",
    "end": "741589"
  },
  {
    "text": "got the right appetizer back. What does Markov tell\nyou about the probability",
    "start": "741590",
    "end": "747300"
  },
  {
    "text": "that all n people get\nthe right appetizer back? 1/n.",
    "start": "747300",
    "end": "752370"
  },
  {
    "text": "The expected value is 1. And now you're asking\nthe probability that you get R is at least n.",
    "start": "752370",
    "end": "759705"
  },
  {
    "text": "So x is n. So it's 1 in n. And what was the probability, or\nwhat is the actual probability?",
    "start": "759705",
    "end": "765914"
  },
  {
    "text": "In this case, you\nknow the distribution, that everybody gets the\nright appetizer back, all n.",
    "start": "765914",
    "end": "771300"
  },
  {
    "text": "1 in n. So in the case of the\nChinese appetizer problem,",
    "start": "771300",
    "end": "776840"
  },
  {
    "text": "Markov's bound is actually the\nright answer, right on target, which gives you an example\nwhere you can't improve it.",
    "start": "776840",
    "end": "785550"
  },
  {
    "text": "By itself, if you just\nknow the expected value, there's no stronger\ntheorem that way.",
    "start": "785550",
    "end": "791290"
  },
  {
    "text": "Because Chinese appetizer is\nan example where the bound you get, 1/n, of n people\ngetting the right appetizer",
    "start": "791290",
    "end": "797347"
  },
  {
    "text": "is in fact the true probability.  OK, what about the\nhat check problem?",
    "start": "797347",
    "end": "804630"
  },
  {
    "text": "Remember that? So there's n men put the\nhats in the coat closet. They get uniformly\nrandomly scrambled.",
    "start": "804630",
    "end": "811829"
  },
  {
    "text": "So it's a random permutation\napplied to the hats. Now each man gets a hat back.",
    "start": "811830",
    "end": "818490"
  },
  {
    "text": "What's the expected number of\nmen to get the right hat back? ",
    "start": "818490",
    "end": "824320"
  },
  {
    "text": "One, same as the other one. Because you've got n men\neach with a 1 in n chance, so it's 1.",
    "start": "824320",
    "end": "830130"
  },
  {
    "text": "Markov says the probability that\nn men get the right hat back is at most 1 in n, same as before.",
    "start": "830130",
    "end": "838680"
  },
  {
    "text": "What's the actual\nprobability that all n men get the right hat back? AUDIENCE: [INAUDIBLE]",
    "start": "838680",
    "end": "845070"
  },
  {
    "text": "PROFESSOR: 1 in n factorial. So in this case, Markov\nis way off the mark.",
    "start": "845070",
    "end": "850120"
  },
  {
    "text": "It says 1 in n. But in fact the real\nbound is much smaller.",
    "start": "850120",
    "end": "855480"
  },
  {
    "text": "So Markov is not always tight. It's always an upper bound. But it sometimes is\nnot the right answer.",
    "start": "855480",
    "end": "863110"
  },
  {
    "text": "And to get the right\nanswer, often you need to know more\nabout the distribution. ",
    "start": "863110",
    "end": "870350"
  },
  {
    "text": "OK, what if R can be negative?",
    "start": "870350",
    "end": "875579"
  },
  {
    "text": "Is it possible that Markov's\ntheorem holds there?  Because I use the\nassumption in the theorem.",
    "start": "875580",
    "end": "884240"
  },
  {
    "text": "Can anybody give me an\nexample where it doesn't work if R can be negative?",
    "start": "884240",
    "end": "890545"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah,\ngood, so for example,",
    "start": "890545",
    "end": "896320"
  },
  {
    "text": "say probability R\nequals 1,000 is 1/2,",
    "start": "896320",
    "end": "902660"
  },
  {
    "text": "and the probability R\nequals minus 1,000 is 1/2.",
    "start": "902660",
    "end": "908500"
  },
  {
    "text": "Then the expected\nvalue of R is 0. And say we asked the probability\nthat R is at least 1,000.",
    "start": "908500",
    "end": "918310"
  },
  {
    "text": "Well, that's going to be 1/2. But that does not equal the\nexpected value of R/1,000,",
    "start": "918310",
    "end": "927730"
  },
  {
    "text": "which would be 0. So Markov's theorem really does\nneed that R to be non-negative.",
    "start": "927730",
    "end": "936970"
  },
  {
    "text": "In fact, let's see if we saw\nwhere we used it in the proof. Anybody see where we use\nthat fact in the proof,",
    "start": "936970",
    "end": "942920"
  },
  {
    "text": "that R can't be negative? What is it? AUDIENCE: [INAUDIBLE]",
    "start": "942920",
    "end": "949340"
  },
  {
    "text": "PROFESSOR: Well, no,\nbecause x is positive. We said x is positive.",
    "start": "949340",
    "end": "955080"
  },
  {
    "text": "So it's not used there. But that's a good\none to look at. Yeah?",
    "start": "955080",
    "end": "961319"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] is\ngreater than or equal to 0.",
    "start": "961320",
    "end": "966630"
  },
  {
    "text": "PROFESSOR: Yeah, if\nR can be negative, then this is not necessarily\na positive number.",
    "start": "966630",
    "end": "973270"
  },
  {
    "text": "It could be a negative number. And then this\ninequality doesn't hold. ",
    "start": "973270",
    "end": "980740"
  },
  {
    "text": "OK, good.  All right, now it\nturns out there",
    "start": "980740",
    "end": "987940"
  },
  {
    "text": "is a variation of\nMarkov's theorem you can use when R is negative. Yeah.",
    "start": "987940",
    "end": "993378"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\nbut would it be OK just to shift everything up? PROFESSOR: Yeah,\nyeah, that's great.",
    "start": "993379",
    "end": "999880"
  },
  {
    "text": "If R has a limit on\nhow negative it can be, then you make an R prime, which\njust adds that limit to R,",
    "start": "999880",
    "end": "1007430"
  },
  {
    "text": "makes it positive\nor non-negative. And now use Markov's\ntheorem there. And that is now an analogous\nform of Markov's theorem",
    "start": "1007430",
    "end": "1015540"
  },
  {
    "text": "when R can be negative, but\nthere's a lower limit to it. And I won't stay to\nimprove that here.",
    "start": "1015540",
    "end": "1021500"
  },
  {
    "text": "But that's in the\ntext and something you want to be familiar with.",
    "start": "1021500",
    "end": "1026770"
  },
  {
    "text": "What I do want to do in\nclass is another case where you can use\nMarkov's theorem",
    "start": "1026770",
    "end": "1033750"
  },
  {
    "text": "to analyze the probability or\nupper bound the probability that R is very small,\nless than its expectation.",
    "start": "1033750",
    "end": "1039770"
  },
  {
    "text": "And it's the same idea\nas you just suggested. So let's state that. ",
    "start": "1039770",
    "end": "1049610"
  },
  {
    "text": "If R is upper bounded, has a\nhard limit on the upper bound, by u for some u in\nthe real numbers,",
    "start": "1049610",
    "end": "1062460"
  },
  {
    "text": "then for all x less than\nu, the probability that R",
    "start": "1062460",
    "end": "1070210"
  },
  {
    "text": "is less than or equal\nto x is at most u",
    "start": "1070210",
    "end": "1076490"
  },
  {
    "text": "minus the expected value\nof R over u minus x.",
    "start": "1076490",
    "end": "1083960"
  },
  {
    "text": "So in this case, we're\ngetting a probability that R is less than\nsomething instead of R",
    "start": "1083960",
    "end": "1089770"
  },
  {
    "text": "is bigger than something. And we're going to do it using\na simple trick that we'll be",
    "start": "1089770",
    "end": "1098620"
  },
  {
    "text": "sort of using all day, really. The probability that R is\nless than x, this event,",
    "start": "1098620",
    "end": "1106690"
  },
  {
    "text": "R is less than x, is the\nsame as the event u minus R",
    "start": "1106690",
    "end": "1112669"
  },
  {
    "text": "is at least u minus x. ",
    "start": "1112670",
    "end": "1118660"
  },
  {
    "text": "So what have I done? I put negative R over here,\nsubtract x from each side,",
    "start": "1118660",
    "end": "1124740"
  },
  {
    "text": "add u to each side. I've got to put a less\nthan or equal to here.",
    "start": "1124740",
    "end": "1131889"
  },
  {
    "text": "So R is less than or\nequal to x if and only if u minus r is at\nleast u minus x.",
    "start": "1131890",
    "end": "1138840"
  },
  {
    "text": "It's simple math there. And now I'm going to\napply Markov to this.",
    "start": "1138840",
    "end": "1147250"
  },
  {
    "text": "I'm going to apply Markov\nto this random variable. And this will be the\nvalue I would have had",
    "start": "1147250",
    "end": "1152680"
  },
  {
    "text": "for x up in Markov's theorem. Why is it OK to apply\nMarkov to u minus R?",
    "start": "1152680",
    "end": "1158658"
  },
  {
    "text": " AUDIENCE: You could just\ndefine the new random variable",
    "start": "1158658",
    "end": "1165870"
  },
  {
    "text": "to be u minus R. PROFESSOR: Yeah, so I got\na new random variable.",
    "start": "1165870",
    "end": "1170905"
  },
  {
    "text": "But what do I need to know\nabout that new random variable to apply Markov? AUDIENCE: u is always\ngreater than R.",
    "start": "1170905",
    "end": "1176000"
  },
  {
    "text": "PROFESSOR: u is always greater\nthan R, or at least as big as R. So u minus R is\nalways non-negative.",
    "start": "1176000",
    "end": "1181650"
  },
  {
    "text": "So I can apply Markov now. And when I apply Markov, I'll\nget this is at most-- maybe",
    "start": "1181650",
    "end": "1192750"
  },
  {
    "text": "I'll go over here.  The probability that--\nooh, not R here.",
    "start": "1192750",
    "end": "1200660"
  },
  {
    "text": "This is probability. The probability that u minus\nR is at least u minus x",
    "start": "1200660",
    "end": "1210250"
  },
  {
    "text": "is at most the expected\nvalue of that random variable",
    "start": "1210250",
    "end": "1217960"
  },
  {
    "text": "over this value. ",
    "start": "1217960",
    "end": "1223210"
  },
  {
    "text": "And well now I just use the\nlinearity of expectation. I've got a scalar here. So this is u minus the expected\nvalue of R over u minus x.",
    "start": "1223210",
    "end": "1233346"
  },
  {
    "text": " So I've used Markov's theorem to\nget a different version of it.",
    "start": "1233346",
    "end": "1240052"
  },
  {
    "text": " All right, let's do an example.",
    "start": "1240052",
    "end": "1245480"
  },
  {
    "text": "Say I'm looking at test scores.  And I'll let R be the\nscore of a random student",
    "start": "1245480",
    "end": "1256860"
  },
  {
    "text": "uniformly selected. ",
    "start": "1256860",
    "end": "1263389"
  },
  {
    "text": "And say that the\nmax score is 100.",
    "start": "1263390",
    "end": "1270920"
  },
  {
    "text": "So that's u. All scores are at most 100. And say that I tell\nyou the class average,",
    "start": "1270920",
    "end": "1278530"
  },
  {
    "text": "or the expected\nvalue of R, is 75.",
    "start": "1278530",
    "end": "1284183"
  },
  {
    "text": " And now I want to know,\nwhat's the probability",
    "start": "1284183",
    "end": "1291929"
  },
  {
    "text": "that a random student\nscores 50 or below? ",
    "start": "1291930",
    "end": "1301750"
  },
  {
    "text": "Can we figure that out? I don't know anything\nabout the distribution, just that the max score is 100\nand the average score is 75.",
    "start": "1301750",
    "end": "1311160"
  },
  {
    "text": "What's the probability\nthat a random student scores 50 or less?",
    "start": "1311160",
    "end": "1316440"
  },
  {
    "text": "I want to upper bound that. So we just plug it\ninto the formula.",
    "start": "1316440",
    "end": "1324860"
  },
  {
    "text": "u is 100. The expected value is 75.",
    "start": "1324860",
    "end": "1331300"
  },
  {
    "text": "u is 100. And x is 50.",
    "start": "1331300",
    "end": "1336960"
  },
  {
    "text": "And that's 25 over 50, is 1/2. ",
    "start": "1336960",
    "end": "1343560"
  },
  {
    "text": "So at most half the class\ncan score 50 or below. And state it as a probability\nquestion or deterministic fact",
    "start": "1343560",
    "end": "1352160"
  },
  {
    "text": "if I know the average is\n75 and the max is 100. Of course, another way\nof thinking about that is if more than half\nthe class scored 50",
    "start": "1352160",
    "end": "1358980"
  },
  {
    "text": "or below, your\naverage would have had to be lower, even\nif everybody else was right at 100.",
    "start": "1358980",
    "end": "1364386"
  },
  {
    "text": "It wouldn't average out to 75.  All right, any\nquestions about that?",
    "start": "1364386",
    "end": "1374410"
  },
  {
    "text": "OK, so sometimes Markov\nis dead on right,",
    "start": "1374410",
    "end": "1381410"
  },
  {
    "text": "gives the right answer. For example, half the\nclass could have scored 50, and half could have gotten\n100 to make it be 75.",
    "start": "1381410",
    "end": "1388250"
  },
  {
    "text": "And sometimes it's way off,\nlike in the hat check problem. Now, if you know more\nabout the distribution,",
    "start": "1388250",
    "end": "1395270"
  },
  {
    "text": "then you can get better\nbounds, especially the cases when you're far off.",
    "start": "1395270",
    "end": "1400429"
  },
  {
    "text": "For example, if you know\nthe variance in addition to the expectation, or\naside from the expectation,",
    "start": "1400430",
    "end": "1406510"
  },
  {
    "text": "then you can get better\nbounds on the probability that the random\nvariable is large. And in this case, the result is\nknown as Chebyshev's theorem.",
    "start": "1406510",
    "end": "1416980"
  },
  {
    "text": "I'll do that over here.  And it's the analog of Markov's\ntheorem based on variance.",
    "start": "1416980",
    "end": "1425945"
  },
  {
    "text": " It says, for all\nx bigger than 0,",
    "start": "1425945",
    "end": "1434170"
  },
  {
    "text": "and any random variable R--\ncould even be negative--",
    "start": "1434170",
    "end": "1441230"
  },
  {
    "text": "the probability that R deviates\nfrom its expected value in either direction\nby at least x",
    "start": "1441230",
    "end": "1451340"
  },
  {
    "text": "is at most of the variance\nof R divided by x squared.",
    "start": "1451340",
    "end": "1458850"
  },
  {
    "text": "So this is like\nMarkov's theorem, except that we're now\nbounding the deviation",
    "start": "1458850",
    "end": "1464640"
  },
  {
    "text": "in either direction. Instead of expected\nvalue, you have variance. Instead of x, you've got x\nsquared, but the same idea.",
    "start": "1464640",
    "end": "1472930"
  },
  {
    "text": "In fact, the proof\nuses Markov's theorem. ",
    "start": "1472930",
    "end": "1483330"
  },
  {
    "text": "Well, the probability\nthat R deviates from its expected\nvalue by at least x,",
    "start": "1483330",
    "end": "1492820"
  },
  {
    "text": "this is the same event,\nor happens if and only if R minus expected value\nsquared is at least x squared.",
    "start": "1492820",
    "end": "1501450"
  },
  {
    "text": "I'm just going to\nsquare both sides here. ",
    "start": "1501450",
    "end": "1514960"
  },
  {
    "text": "OK, I square both sides. And since this is positive\nand this is positive, I can square both sides and\nmaintain the inequality.",
    "start": "1514960",
    "end": "1520456"
  },
  {
    "text": " Now I'm going to\napply Markov's theorem",
    "start": "1520456",
    "end": "1528050"
  },
  {
    "text": "to that random variable. It's a random variable. It's R minus expected\nvalue squared.",
    "start": "1528050",
    "end": "1533520"
  },
  {
    "text": "So it's a random variable. And what's nice about\nthis random variable that lets me apply Markov's theorem?",
    "start": "1533520",
    "end": "1540920"
  },
  {
    "text": "It's a square. So it's always non-negative. So I can apply Markov's theorem.",
    "start": "1540920",
    "end": "1546910"
  },
  {
    "text": "And my Markov's theorem,\nthis probability is at most the expected value\nof that divided by this.",
    "start": "1546910",
    "end": "1564179"
  },
  {
    "text": "That's what Markov's theorem\nsays as long as this is always non-negative.",
    "start": "1564180",
    "end": "1570029"
  },
  {
    "text": "All right, what's a\nsimpler expression for this, the expected value\nof the square of the deviation",
    "start": "1570030",
    "end": "1579070"
  },
  {
    "text": "of a random variable? That's the variance. That's the definition\nof variance. ",
    "start": "1579070",
    "end": "1586470"
  },
  {
    "text": "So that is just the variance\nof R over x squared.",
    "start": "1586470",
    "end": "1591760"
  },
  {
    "text": "And we're done. So Chebyshev's theorem is\nreally just another version",
    "start": "1591760",
    "end": "1597590"
  },
  {
    "text": "of Markov's theorem. But now it's based\non the variance. ",
    "start": "1597590",
    "end": "1604450"
  },
  {
    "text": "OK, any questions?  OK, so there's a nice corollary\nfor this, just as with",
    "start": "1604450",
    "end": "1614500"
  },
  {
    "text": "Markov's theorem. It says the probability that the\nabsolute value, the deviation,",
    "start": "1614500",
    "end": "1625020"
  },
  {
    "text": "is at least c times the\nstandard deviation of R.",
    "start": "1625020",
    "end": "1632940"
  },
  {
    "text": "So I'm looking at\nthe probability that R differs from\nits expectation by at least some scalar c\ntimes the standard deviation.",
    "start": "1632940",
    "end": "1642760"
  },
  {
    "text": "Well, what's that? Well, that's the variance of R\nover the square of this thing--",
    "start": "1642760",
    "end": "1654490"
  },
  {
    "text": "c squared times the\nstandard deviation squared.",
    "start": "1654490",
    "end": "1660590"
  },
  {
    "text": "What's the square of\nthe standard deviation? That's the variance.",
    "start": "1660590",
    "end": "1665930"
  },
  {
    "text": "They cancel, so it's\njust 1 over c squared. ",
    "start": "1665930",
    "end": "1671460"
  },
  {
    "text": "So the probability of more than\ntwice the standard deviation",
    "start": "1671460",
    "end": "1677840"
  },
  {
    "text": "off the expectation is\nat most 1/4, for example. ",
    "start": "1677840",
    "end": "1683779"
  },
  {
    "text": "All right, let's do\nsome examples of that. ",
    "start": "1683780",
    "end": "1688960"
  },
  {
    "text": "Maybe we'll leave\nMarkov up there. ",
    "start": "1688960",
    "end": "1702059"
  },
  {
    "text": "OK, say we're looking at IQs. ",
    "start": "1702060",
    "end": "1707539"
  },
  {
    "text": "In this case, we're going to let\nR be the IQ of a random person.",
    "start": "1707540",
    "end": "1712940"
  },
  {
    "start": "1712940",
    "end": "1718340"
  },
  {
    "text": "All right, now we're\ngoing to assume-- and this actually is the case--\nthat R is always at least 0,",
    "start": "1718340",
    "end": "1729580"
  },
  {
    "text": "despite the fact that\nprobably most of you have somebody you know who\nyou think has a negative IQ.",
    "start": "1729580",
    "end": "1735540"
  },
  {
    "text": "They can't be negative. They have to be non-zero. In fact, IQs are adjusted.",
    "start": "1735540",
    "end": "1742970"
  },
  {
    "text": "So the expected IQ is\nsupposed to be 100, although actually the\naverages may be in the 90's.",
    "start": "1742970",
    "end": "1751170"
  },
  {
    "text": "And it's set up so that the\nstandard deviation of IQ is supposed to be 15.",
    "start": "1751170",
    "end": "1757690"
  },
  {
    "text": "So we're just going to\nassume those are facts on IQ. And that's what\nit's meant to be.",
    "start": "1757690",
    "end": "1763260"
  },
  {
    "text": "And now we want to know, what's\nthe probability a random person has an IQ of at least 250?",
    "start": "1763260",
    "end": "1771050"
  },
  {
    "text": "Now Marilyn, from \"Ask Marilyn,\"\nhas an IQ pretty close to 250.",
    "start": "1771050",
    "end": "1776070"
  },
  {
    "text": "And she thinks that's\npretty special, pretty rare. So what can we say about that?",
    "start": "1776070",
    "end": "1782669"
  },
  {
    "text": "In particular, say\nwe used Markov. What could you say about\nthe probability of having",
    "start": "1782670",
    "end": "1790690"
  },
  {
    "text": "an IQ of at least 250? ",
    "start": "1790690",
    "end": "1796800"
  },
  {
    "text": "What does Markov tell us? ",
    "start": "1796800",
    "end": "1805858"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: What is it? AUDIENCE: [INAUDIBLE] PROFESSOR: Not quite 1 in 25,\nbut you're on the right track.",
    "start": "1805858",
    "end": "1811602"
  },
  {
    "text": " It's not quite 2/3.",
    "start": "1811602",
    "end": "1816789"
  },
  {
    "text": "It's the expected\nvalue, which is 100,",
    "start": "1816790",
    "end": "1822080"
  },
  {
    "text": "over the x value, which is 250. So it's 1 in 2.5, or 0.4.",
    "start": "1822080",
    "end": "1832740"
  },
  {
    "text": "So the probability\nis at most 0.4, so 40% chance it could\nhappen, potentially, but no",
    "start": "1832740",
    "end": "1839320"
  },
  {
    "text": "bigger than that. What about Chebyshev? ",
    "start": "1839320",
    "end": "1844884"
  },
  {
    "text": "See if you can figure\nout what Chebyshev says about the probability of\nhaving an IQ of at least 250.",
    "start": "1844884",
    "end": "1859115"
  },
  {
    "text": "It's a little tricky. You've got to sort of\nplug it into the equation there and get it to\nfit in the right form.",
    "start": "1859115",
    "end": "1864300"
  },
  {
    "text": " Chebyshev says that-- let's\nget in the right form.",
    "start": "1864300",
    "end": "1875870"
  },
  {
    "text": "I've got probability\nthat R is at least 250. I've got to get it into\nthat form up there.",
    "start": "1875870",
    "end": "1883670"
  },
  {
    "text": "So that's the probability\nthat-- well, first R minus 100",
    "start": "1883670",
    "end": "1889480"
  },
  {
    "text": "is at least 150. So I've got R minus\nthe expected value.",
    "start": "1889480",
    "end": "1894550"
  },
  {
    "text": "I'm sort of getting it ready\nto apply Chebyshev here. And then 150-- how many\nstandard deviations is 150?",
    "start": "1894550",
    "end": "1904630"
  },
  {
    "text": "10, all right? So this is the probability that\nR minus the expected value of R",
    "start": "1904630",
    "end": "1913580"
  },
  {
    "text": "is at least 10\nstandard deviations. That's what I'm asking.",
    "start": "1913580",
    "end": "1919480"
  },
  {
    "text": "I'm not quite there. I'm going to use\nthe corollary there. But I've got to get that\nabsolute value thing in.",
    "start": "1919480",
    "end": "1925110"
  },
  {
    "text": "But it's upper bounded\nby the probability of the absolute value of\nR minus expected value",
    "start": "1925110",
    "end": "1934740"
  },
  {
    "text": "bigger than or equal to\n10 standard deviations. Because this allows\nfor two cases.",
    "start": "1934740",
    "end": "1942730"
  },
  {
    "text": "R is 10 standard\ndeviations high, and R is 10 standard\ndeviations low or more.",
    "start": "1942730",
    "end": "1949169"
  },
  {
    "text": "So this is upper\nbounded by that. And now I can plug in Chebyshev\nin the corollary form.",
    "start": "1949170",
    "end": "1955248"
  },
  {
    "text": "And what's the answer\nwhen I do that? ",
    "start": "1955249",
    "end": "1960880"
  },
  {
    "text": "1 in 100-- the\nprobability of being off by 10 standard deviations\nor more is at most 1 in 100,",
    "start": "1960880",
    "end": "1966590"
  },
  {
    "text": "1 in 10 squared. So it's a lot better bound. It's 1% instead of 40%.",
    "start": "1966590",
    "end": "1975149"
  },
  {
    "text": "So knowing the variance\nof the standard deviation gives you a lot more information\nand generally gives you",
    "start": "1975150",
    "end": "1980800"
  },
  {
    "text": "much better bounds\non the probability of deviating from the mean.",
    "start": "1980800",
    "end": "1986272"
  },
  {
    "text": "And the reason it\ngives you better bounds is because the variance\nis squaring deviations.",
    "start": "1986272",
    "end": "1992250"
  },
  {
    "text": "So they count a lot more.  All right, now let's look at\nthis step a little bit more.",
    "start": "1992250",
    "end": "2002020"
  },
  {
    "start": "2002020",
    "end": "2016491"
  },
  {
    "text": "All right, let's\nsay here is a line, and here's the\nexpected value of R.",
    "start": "2016491",
    "end": "2023679"
  },
  {
    "text": "And say here's 10 standard\ndeviations high here.",
    "start": "2023680",
    "end": "2030340"
  },
  {
    "text": "So this will be more than\n10 standard deviations. And this will be 10 standard\ndeviations on the low side.",
    "start": "2030340",
    "end": "2037850"
  },
  {
    "text": "So here, I'm low. Now, this line here\nwith the absolute value",
    "start": "2037850",
    "end": "2045580"
  },
  {
    "text": "is figuring out the probability\nof being low or high. This is the probability\nthat the absolute value",
    "start": "2045580",
    "end": "2053929"
  },
  {
    "text": "of R minus its expected\nvalue is at least",
    "start": "2053929",
    "end": "2059060"
  },
  {
    "text": "10 standard deviations.  What we really wanted\nto know for bound",
    "start": "2059060",
    "end": "2067280"
  },
  {
    "text": "was just the high side. ",
    "start": "2067280",
    "end": "2072699"
  },
  {
    "text": "Now, is it true that then, since\nthe probability of high or low is 1 in 100, the probability\nof being high is at most 1",
    "start": "2072699",
    "end": "2081100"
  },
  {
    "text": "in 200, half? Is that true?",
    "start": "2081100",
    "end": "2088239"
  },
  {
    "text": "Yeah? AUDIENCE: [INAUDIBLE] ",
    "start": "2088239",
    "end": "2096609"
  },
  {
    "text": "PROFESSOR: Yeah, it is\nnot necessarily true that the high and\nthe low are equal, and therefore the high\nis half the total.",
    "start": "2096610",
    "end": "2103625"
  },
  {
    "text": "It might be true, but\nnot necessarily true. And that's a mistake\nthat often gets made where you'll take this\nfact as being less than 100",
    "start": "2103625",
    "end": "2115180"
  },
  {
    "text": "to conclude that's\nless than 1 in 200. And that you can't do,\nunless the distribution is symmetric around\nthe expected value.",
    "start": "2115180",
    "end": "2122950"
  },
  {
    "text": "Then you could do it, if\nit's a symmetric distribution around the expected value. But usually it's not.",
    "start": "2122950",
    "end": "2128093"
  },
  {
    "text": " Now, there is something\nbetter you can say.",
    "start": "2128093",
    "end": "2135660"
  },
  {
    "text": "So let me tell you what it is. But we won't prove it. I think we might\nprove it in the text.",
    "start": "2135660",
    "end": "2140720"
  },
  {
    "text": "I'm not sure. If you just want the high side\nor just want the low side,",
    "start": "2140720",
    "end": "2145960"
  },
  {
    "text": "you can do slightly better\nthan 1 in c squared. ",
    "start": "2145960",
    "end": "2154290"
  },
  {
    "text": "That's the following theorem. ",
    "start": "2154290",
    "end": "2159559"
  },
  {
    "text": "For any random variable\nR, the probability",
    "start": "2159560",
    "end": "2167320"
  },
  {
    "text": "that R is on the high side\nby c standard deviations",
    "start": "2167320",
    "end": "2176190"
  },
  {
    "text": "is at most 1 over\nc squared plus 1. So it's not 1 over 2c squared.",
    "start": "2176190",
    "end": "2182640"
  },
  {
    "text": "It's 1 over c squared\nplus 1, and the same thing for the probability of\nbeing on the low side. ",
    "start": "2182640",
    "end": "2192300"
  },
  {
    "text": "Let's see, have I\nwritten this right?",
    "start": "2192300",
    "end": "2197474"
  },
  {
    "text": "Hmm, I want to get this as\nless than or equal to negative",
    "start": "2197474",
    "end": "2204050"
  },
  {
    "text": "c times the standard deviation. So here I'm high by c or\nmore standard deviations.",
    "start": "2204050",
    "end": "2210940"
  },
  {
    "text": "Here I'm low. So R is less than the\nexpected value by at least c standard deviations.",
    "start": "2210940",
    "end": "2217070"
  },
  {
    "text": "And that is also 1\nover c squared plus 1. And it is possible\nto find distributions",
    "start": "2217070",
    "end": "2224740"
  },
  {
    "text": "that hit these targets--\nnot both at the same time, but one or the other,\nhit those targets.",
    "start": "2224740",
    "end": "2230310"
  },
  {
    "text": "So that's the best you\ncan say in general.  All right, so using\nthis bound, what's",
    "start": "2230310",
    "end": "2237300"
  },
  {
    "text": "the probability that\na random person has an IQ of at least 250? It's a little better\nthan 1 in 100.",
    "start": "2237300",
    "end": "2246106"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah, 1/101. So in fact, the best we can\nsay without knowing any more",
    "start": "2246106",
    "end": "2251920"
  },
  {
    "text": "information about IQs is\nthat it's at most 1/101,",
    "start": "2251920",
    "end": "2259000"
  },
  {
    "text": "slightly better.  Now in fact, with IQs, they know\nmore about the distribution.",
    "start": "2259000",
    "end": "2267030"
  },
  {
    "text": "And the probability\nis a lot less. Because you know more\nabout the distribution than we've assumed here.",
    "start": "2267030",
    "end": "2273670"
  },
  {
    "text": "In fact, I don't think\nanybody has an IQ over 250 as far as I know.",
    "start": "2273670",
    "end": "2278890"
  },
  {
    "text": "Any questions about this?",
    "start": "2278890",
    "end": "2284859"
  },
  {
    "text": "OK, all right, say\nwe give the exam. What fraction of the class\ncan score more than two",
    "start": "2284860",
    "end": "2294010"
  },
  {
    "text": "standard deviations, get two\nstandard deviations or more, away from the average,\nabove or below?",
    "start": "2294010",
    "end": "2299940"
  },
  {
    "text": " Could half the class be\ntwo standard deviations",
    "start": "2299940",
    "end": "2306630"
  },
  {
    "text": "off the mean? No? What's the biggest fraction\nthat that could happen?",
    "start": "2306630",
    "end": "2311831"
  },
  {
    "text": " What do I do?",
    "start": "2311832",
    "end": "2316910"
  },
  {
    "text": "What fraction of the class\ncan be two standard deviations or more from the mean?",
    "start": "2316910",
    "end": "2323662"
  },
  {
    "text": " What is it? AUDIENCE: 1/4.",
    "start": "2323662",
    "end": "2329030"
  },
  {
    "text": "PROFESSOR: 1/4, because c is 2. You don't even know\nwhat the mean is. You don't know what the\nstandard deviation is.",
    "start": "2329030",
    "end": "2334704"
  },
  {
    "text": "You don't need to. I just asked, you're\ntwo standard deviations off or more. At most, 1/4.",
    "start": "2334704",
    "end": "2340080"
  },
  {
    "text": " How many could be two\nstandard deviations high or better at most?",
    "start": "2340080",
    "end": "2349080"
  },
  {
    "text": "1/5-- 1 over 4 plus 1, good. OK, this holds\ntrue no matter what",
    "start": "2349080",
    "end": "2357030"
  },
  {
    "text": "the distribution\nof test scores is. Yeah? AUDIENCE: [INAUDIBLE] PROFESSOR: Which one?",
    "start": "2357030",
    "end": "2362190"
  },
  {
    "text": "This one? AUDIENCE: Yeah. PROFESSOR: Oh, that's\nmore complicated. That'll take us several\nboards to do, to prove that.",
    "start": "2362190",
    "end": "2368849"
  },
  {
    "text": "And I forget if we put\nit in the text or not. It might be in the\ntext, to prove that.",
    "start": "2368850",
    "end": "2376702"
  },
  {
    "text": "Any other questions?  OK so Markov and Chebyshev are\nsometimes close, sometimes not.",
    "start": "2376702",
    "end": "2388856"
  },
  {
    "text": " Now, for the rest\nof today, we're",
    "start": "2388856",
    "end": "2394360"
  },
  {
    "text": "going to talk about a much\nmore powerful technique. But it only works\nin a special case.",
    "start": "2394360",
    "end": "2400244"
  },
  {
    "text": "Now, the good news\nis this special case happens all the\ntime in practice. And it's the case when you're\nanalyzing a random variable",
    "start": "2400245",
    "end": "2408550"
  },
  {
    "text": "that itself is\nthe sum of a bunch of other random variables. And we've seen already\nexamples like that.",
    "start": "2408550",
    "end": "2415599"
  },
  {
    "text": "And the other random\nvariables have to be mutually independent. And in this case,\nyou get a bound",
    "start": "2415600",
    "end": "2421730"
  },
  {
    "text": "that's called a Chernoff bound. And this is the same\nChernoff who figured out how to beat the lottery.",
    "start": "2421730",
    "end": "2428390"
  },
  {
    "text": "And it's interesting. Long after we started\nteaching this, originally this stuff was only\ntaught, for Chernoff bounds,",
    "start": "2428390",
    "end": "2435820"
  },
  {
    "text": "for graduate students. And now we teach it here. Because it's so important. And it really is accessible.",
    "start": "2435820",
    "end": "2441559"
  },
  {
    "text": "It'll be probably the\nmost complicated proof we've done to establish\na Chernoff bound. But Chernoff himself,\nwhen he discovered this,",
    "start": "2441560",
    "end": "2449400"
  },
  {
    "text": "thought it was no big deal. In fact, he couldn't\nfigure out why everybody in computer science\nwas always writing papers with Chernoff bounds in them.",
    "start": "2449400",
    "end": "2456290"
  },
  {
    "text": "And that's because he\ndidn't put any emphasis on the bounds in his work. But computer scientists\nwho came later",
    "start": "2456290",
    "end": "2461800"
  },
  {
    "text": "found all sorts of\nimportant applications. And we'll see some\nof those today.",
    "start": "2461800",
    "end": "2466920"
  },
  {
    "text": "So let me tell you\nwhat the bound is. And the nice thing\nis it really is",
    "start": "2466920",
    "end": "2472370"
  },
  {
    "text": "Markov's theorem\nagain in disguise, just a little more complicated. ",
    "start": "2472370",
    "end": "2491230"
  },
  {
    "text": "Theorem-- it's called\na Chernoff bound. ",
    "start": "2491230",
    "end": "2502470"
  },
  {
    "text": "Let T1, T2, up to Tn be\nany mutually independent--",
    "start": "2502470",
    "end": "2515619"
  },
  {
    "text": "that's really important--\nrandom variables",
    "start": "2515620",
    "end": "2521090"
  },
  {
    "text": "such that each of them takes\nvalues only between 0 and 1.",
    "start": "2521090",
    "end": "2532110"
  },
  {
    "text": "And if they don't, just\nnormalize them so they do. So we're going to take a\nbunch of random variables",
    "start": "2532110",
    "end": "2538290"
  },
  {
    "text": "that are mutually independent. And they are all\nbetween 0 and 1. ",
    "start": "2538290",
    "end": "2546120"
  },
  {
    "text": "Then we're going to look at the\nsum of those random variables,",
    "start": "2546120",
    "end": "2556060"
  },
  {
    "text": "call that T. Then\nfor any c at least 1,",
    "start": "2556060",
    "end": "2568250"
  },
  {
    "text": "the probability that the sum\nrandom variable is at least c times its expected value.",
    "start": "2568250",
    "end": "2576059"
  },
  {
    "text": "So it's going to be the high\nside here-- is at most e to the minus z, and I'll tell\nyou what that is in a minute,",
    "start": "2576060",
    "end": "2583599"
  },
  {
    "text": "times the expected value of T\nwhere z is c natural log of c",
    "start": "2583600",
    "end": "2599130"
  },
  {
    "text": "plus 1 minus c. And it turns out if c is bigger\nthan 1, this is positive.",
    "start": "2599130",
    "end": "2604450"
  },
  {
    "text": " So that's a lot,\none of the longest",
    "start": "2604450",
    "end": "2611880"
  },
  {
    "text": "theorems we wrote down here. But what it says is\nthat probability were high is exponentially small.",
    "start": "2611880",
    "end": "2622610"
  },
  {
    "text": "As the expected value is\nbig, the chance of being high gets really, really tiny.",
    "start": "2622610",
    "end": "2628339"
  },
  {
    "text": "Now, I'm going to\nprove it in a minute. But let's just plug\nin some examples to see what's going on here.",
    "start": "2628340",
    "end": "2634367"
  },
  {
    "start": "2634367",
    "end": "2645200"
  },
  {
    "text": "So for example, suppose the\nexpected value of T is 100.",
    "start": "2645200",
    "end": "2660079"
  },
  {
    "text": "And suppose c is 2.",
    "start": "2660080",
    "end": "2665230"
  },
  {
    "text": "So we expect to have\n100 come out of the sum. The probability we get\nat least 200-- well,",
    "start": "2665230",
    "end": "2673940"
  },
  {
    "text": "let's figure out what that is. c being 2 we can evaluate z now.",
    "start": "2673940",
    "end": "2679450"
  },
  {
    "text": "It's 2 natural log\nof 2 plus 1 minus 2. And that's close to but a\nlittle larger than 0.38.",
    "start": "2679450",
    "end": "2688990"
  },
  {
    "text": "So we can plug z in,\nthe exponent up there, and find that the probability\nthat T is at least twice",
    "start": "2688990",
    "end": "2694330"
  },
  {
    "text": "its expected value,\nnamely at least 200, is at most e to the\nminus 0.38 times 100,",
    "start": "2694330",
    "end": "2704560"
  },
  {
    "text": "which is e to the minus 38,\nwhich is just really small.",
    "start": "2704560",
    "end": "2711038"
  },
  {
    "text": " So that's just way better\nthan any results you get with Markov or Chebyshev.",
    "start": "2711038",
    "end": "2717849"
  },
  {
    "text": " So if you have a bunch of random\nvariables between 0 and 1,",
    "start": "2717850",
    "end": "2723320"
  },
  {
    "text": "and they're mutually\nindependent, you add them up. If you expect 100 as\nthe answer, the chance",
    "start": "2723320",
    "end": "2728780"
  },
  {
    "text": "of getting 200 or more-- forget\nabout it, not going to happen. ",
    "start": "2728780",
    "end": "2735040"
  },
  {
    "text": "Now, of course Chernoff doesn't\napply to all distributions. It has to be this type.",
    "start": "2735040",
    "end": "2740160"
  },
  {
    "text": "This is a pretty broad class. In fact, it contains the class\nof all Bernoulli distributions.",
    "start": "2740160",
    "end": "2747200"
  },
  {
    "text": "So I have binomial\ndistributions. Because remember a binomial\ndistribution-- well,",
    "start": "2747200",
    "end": "2754619"
  },
  {
    "text": "remember binomial distributions? That's where T is\nthe sum of Ti's. In binomial, you\nhave Tj is 0 or 1.",
    "start": "2754620",
    "end": "2762690"
  },
  {
    "text": "It can't be in between. And with binomial, all Tj's\nhave the same distribution.",
    "start": "2762690",
    "end": "2769660"
  },
  {
    "text": "With Chernoff, they\ncan all be different. So Chernoff is much\nbroader than binomial.",
    "start": "2769660",
    "end": "2775730"
  },
  {
    "text": "The individual guys here can\nhave different distributions and attain values\nanywhere between 0 and 1,",
    "start": "2775730",
    "end": "2781640"
  },
  {
    "text": "as opposed to just\none or the other. Any questions about this theorem\nand what it says in the terms",
    "start": "2781640",
    "end": "2794433"
  },
  {
    "text": "there?  One nice thing about it is\nthe number of random variables",
    "start": "2794433",
    "end": "2801349"
  },
  {
    "text": "doesn't even show up\nin the answer here. n doesn't even appear. Yeah.",
    "start": "2801350",
    "end": "2807190"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: Does\nnot apply to what? AUDIENCE: [INAUDIBLE]",
    "start": "2807190",
    "end": "2812590"
  },
  {
    "text": "PROFESSOR: Yeah, when c equals\n1, what happens is z is 0.",
    "start": "2812590",
    "end": "2817700"
  },
  {
    "text": "Because I have a log of 1\nis 0, and 1 minus 1 is 0. And if z is 0, it\nsays your probability",
    "start": "2817700",
    "end": "2824540"
  },
  {
    "text": "is upper bounded by 1. Well, not too interesting,\nbecause any probability",
    "start": "2824540",
    "end": "2830080"
  },
  {
    "text": "is upper bounded by 1. So it doesn't give\nyou any information when c is 0, none at all.",
    "start": "2830080",
    "end": "2836590"
  },
  {
    "text": "But as soon as c starts\nbeing-- sorry, if c is 1. As soon as c starts being bigger\nthan 1, which is sort of a case",
    "start": "2836590",
    "end": "2842480"
  },
  {
    "text": "you're interested in, you're\nbigger than your expectation, then it gives very\npowerful results.",
    "start": "2842480",
    "end": "2848720"
  },
  {
    "text": "Yeah. AUDIENCE: [INAUDIBLE]  PROFESSOR: Yeah, you can.",
    "start": "2848720",
    "end": "2854480"
  },
  {
    "text": "It's true for n\nequals 1 as well. Now, it doesn't give you\na lot of information.",
    "start": "2854480",
    "end": "2860450"
  },
  {
    "text": "Because if c is bigger\nthan 1 and n was 1, so it's using one variable,\nwhat's the probability",
    "start": "2860450",
    "end": "2867060"
  },
  {
    "text": "that a random variable\nexceeds its expectation, c",
    "start": "2867060",
    "end": "2873560"
  },
  {
    "text": "times its expectation? AUDIENCE: [INAUDIBLE]",
    "start": "2873560",
    "end": "2878850"
  },
  {
    "text": "PROFESSOR: Yeah, let's see now. Maybe it does give\nyou information. Because the random variable\nhas a distribution on 0, 1.",
    "start": "2878850",
    "end": "2885930"
  },
  {
    "text": " That's right, so it does\ngive you some information.",
    "start": "2885930",
    "end": "2891860"
  },
  {
    "text": "But I don't think\nit gives you a lot. I have to think about that.",
    "start": "2891860",
    "end": "2896996"
  },
  {
    "text": "What happens when\nthere's just one guy? Because the same thing is true. It's just now for a single\nrandom variable on 0,",
    "start": "2896996",
    "end": "2902720"
  },
  {
    "text": "1 the chance that your\ntwice the expected value.",
    "start": "2902720",
    "end": "2908675"
  },
  {
    "text": "I have to think about that. That's a good question. Does it do anything\ninteresting there?",
    "start": "2908675",
    "end": "2914400"
  },
  {
    "text": "OK, all right, so\nlet's do an example",
    "start": "2914400",
    "end": "2920420"
  },
  {
    "text": "of how you might apply this. ",
    "start": "2920420",
    "end": "2936400"
  },
  {
    "text": "Say that you're playing Pick\n4, and 10 million people",
    "start": "2936400",
    "end": "2941549"
  },
  {
    "text": "are playing. ",
    "start": "2941550",
    "end": "2951140"
  },
  {
    "text": "And say in this\nversion of Pick 4, you're picking a four digit\nnumber, four single digits.",
    "start": "2951140",
    "end": "2957820"
  },
  {
    "text": "And you win if you\nget an exact match. So the probability of\nwinning, a person winning,",
    "start": "2957820",
    "end": "2965820"
  },
  {
    "text": "well, they've got to get\nall four digits right. That's 1 in 10,000,\n10 to the fourth. ",
    "start": "2965820",
    "end": "2974510"
  },
  {
    "text": "What's the expected\nnumber of winners? ",
    "start": "2974510",
    "end": "2980620"
  },
  {
    "text": "If I got 10 million\npeople, what's the expected number of winners?",
    "start": "2980620",
    "end": "2985693"
  },
  {
    "text": " What is it? ",
    "start": "2985693",
    "end": "2991829"
  },
  {
    "text": "We've got 10 million\nover 10,000, right?",
    "start": "2991830",
    "end": "3000892"
  },
  {
    "text": "Because what I'm doing here\nis the number of winners, T, is going to be the sum of\n10 million indicator variables.",
    "start": "3000892",
    "end": "3007405"
  },
  {
    "start": "3007405",
    "end": "3014060"
  },
  {
    "text": "And the probability that\nany one of these guys wins is 1 in 10,000.",
    "start": "3014060",
    "end": "3021839"
  },
  {
    "text": "So the expected\nnumber of winners is 1 in 10,000 added 10\nmillion times, which is this.",
    "start": "3021840",
    "end": "3028720"
  },
  {
    "text": " Is that OK? Everybody should be\nreally familiar with how",
    "start": "3028720",
    "end": "3034823"
  },
  {
    "text": "to whip these things out. This for sure will have\nprobably at least a couple questions where\nyou're going to need",
    "start": "3034823",
    "end": "3040290"
  },
  {
    "text": "to be able to do that kind\nof stuff on the final. ",
    "start": "3040290",
    "end": "3046710"
  },
  {
    "text": "All right, say I want to know\nthe probability of getting at least 2,000 winners,\nand I want to upper bound",
    "start": "3046710",
    "end": "3065940"
  },
  {
    "text": "that just with the\ninformation I've given you. ",
    "start": "3065940",
    "end": "3072319"
  },
  {
    "text": "Well, any thoughts\nabout an upper bound?",
    "start": "3072320",
    "end": "3077920"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: What's that? AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah, that's\na good upper bound.",
    "start": "3077920",
    "end": "3085970"
  },
  {
    "text": "What did you have to\nassume to get there? ",
    "start": "3085970",
    "end": "3091330"
  },
  {
    "text": "e to the minus 380\nis a great bound. Because you're going to plug\nin expected value is 1,000.",
    "start": "3091330",
    "end": "3099069"
  },
  {
    "text": "And we're asking for more\nthan twice the expected value. So it's e to the minus\n0.38 times 1,000.",
    "start": "3099070",
    "end": "3104640"
  },
  {
    "text": "And that for sure is--\nso you computed this.",
    "start": "3104640",
    "end": "3110789"
  },
  {
    "text": "And that equals e\nto the minus 380. So that's really small.",
    "start": "3110790",
    "end": "3116279"
  },
  {
    "text": "But what did you have to\nassume to apply Chernoff?",
    "start": "3116280",
    "end": "3121670"
  },
  {
    "text": "Mutual independence. Mutual independence of what? AUDIENCE: [INAUDIBLE]",
    "start": "3121670",
    "end": "3126880"
  },
  {
    "text": "PROFESSOR: The\nnumbers people picked. And we already know, if\npeople are picking numbers,",
    "start": "3126880",
    "end": "3133249"
  },
  {
    "text": "they don't tend to be\nmutually independent. They tend to gang up. But if you had a computer\npicking the numbers randomly",
    "start": "3133249",
    "end": "3140500"
  },
  {
    "text": "and mutually independently, then\nyou would be e to the minus 380",
    "start": "3140500",
    "end": "3147010"
  },
  {
    "text": "by Chernoff if mutually\nindependent picks.",
    "start": "3147010",
    "end": "3153726"
  },
  {
    "text": " Everybody see why we did that?",
    "start": "3153726",
    "end": "3159270"
  },
  {
    "text": "Because it's a probability\nof twice your expectation. The total number of winners is\nthe sum of 10 million indicator",
    "start": "3159270",
    "end": "3168080"
  },
  {
    "text": "variables. And indicator\nvariables are 0 or 1. So they fit that\ndefinition up there.",
    "start": "3168080",
    "end": "3173599"
  },
  {
    "text": " And so we already figured\nout z is at least 0.38.",
    "start": "3173600",
    "end": "3183079"
  },
  {
    "text": "And you're multiplying by\nthe expected value of 1,000. That's e to the minus 380,\nso very, very unlikely.",
    "start": "3183080",
    "end": "3190610"
  },
  {
    "text": "What if they weren't\nmutually independent? Can you say anything about this,\nanything at all better than 1,",
    "start": "3190610",
    "end": "3199630"
  },
  {
    "text": "which we know for\nany probability? Yeah? AUDIENCE: It's\npossible that everyone",
    "start": "3199630",
    "end": "3205310"
  },
  {
    "text": "chose the same numbers.  PROFESSOR: Yes, everyone could\nhave chosen the same number.",
    "start": "3205310",
    "end": "3211290"
  },
  {
    "text": "But that number only comes\nup with a 1 in 10,000 chance.",
    "start": "3211290",
    "end": "3220950"
  },
  {
    "text": "So you can say something. AUDIENCE: You can use Markov. PROFESSOR: Use Markov. What does Markov give you?",
    "start": "3220950",
    "end": "3226647"
  },
  {
    "start": "3226647",
    "end": "3234667"
  },
  {
    "text": "What does Markov give you?  1/2, yeah.",
    "start": "3234667",
    "end": "3240480"
  },
  {
    "text": "Because you've got\nthe expected value is 1,000 divided by the\nbound threshold, is 2,000,",
    "start": "3240480",
    "end": "3251400"
  },
  {
    "text": "is 1/2 by Markov. And that holds true without\nany independence assumption. ",
    "start": "3251400",
    "end": "3259230"
  },
  {
    "text": "Now, there is an enormous\ndifference between 1/2 and e to the minus 380.",
    "start": "3259230",
    "end": "3264810"
  },
  {
    "text": "Independence really\nmakes a huge difference in the bound you can compute. ",
    "start": "3264810",
    "end": "3273700"
  },
  {
    "text": "OK, now there's another way\nwe could've gone about this.",
    "start": "3273700",
    "end": "3279050"
  },
  {
    "text": "What kind of distribution\ndoes T have in this case? ",
    "start": "3279050",
    "end": "3285952"
  },
  {
    "text": "It's binomial. Because it's the sum of\nindicator random variables, 0,",
    "start": "3285952",
    "end": "3291369"
  },
  {
    "text": "1's. Each of these is 0, 1. And they're all the\nsame distribution. There's a 1 in 10,000 chance of\nwinning for each one of them.",
    "start": "3291370",
    "end": "3300160"
  },
  {
    "text": "So it's a binomial. So we could have\ngone back and used the formulas we had for\nthe binomial distribution,",
    "start": "3300160",
    "end": "3305859"
  },
  {
    "text": "plugged it all in, and we'd have\ngotten something pretty similar",
    "start": "3305860",
    "end": "3311140"
  },
  {
    "text": "here. But Chernoff is so much easier. Remember that pain\nwe would go through with a binomial distribution,\nthe approximation, Stirling's",
    "start": "3311140",
    "end": "3318579"
  },
  {
    "text": "formula, [INAUDIBLE] whatever,\nthe factorials and stuff? And that's a nightmare.",
    "start": "3318580",
    "end": "3324410"
  },
  {
    "text": "This was easy. e to the minus 380 was\nvery easy to compute. And really at that\npoint it doesn't",
    "start": "3324410",
    "end": "3330520"
  },
  {
    "text": "matter if it's minus 381\nor minus 382 or whatever. Because it's really small.",
    "start": "3330520",
    "end": "3335869"
  },
  {
    "text": "So often, even when you have\na binomial distribution, well, Chernoff will apply. And that's a great way to go.",
    "start": "3335870",
    "end": "3342560"
  },
  {
    "text": "Because it gives you\ngood bounds generally. All right, let's figure\nout the probability",
    "start": "3342560",
    "end": "3349170"
  },
  {
    "text": "of at least 1,100\nwinners instead of 1,000.",
    "start": "3349170",
    "end": "3356599"
  },
  {
    "text": "So let's look at the probability\nof at least 100 extra winners",
    "start": "3356600",
    "end": "3362110"
  },
  {
    "text": "over what we expect\nout of 10 million. We've got 10 million people. You expect 1,000.",
    "start": "3362110",
    "end": "3367730"
  },
  {
    "text": "We're going to analyze\nthe probability of 1,100. What's c in this case? We're going to use Chernoff.",
    "start": "3367730",
    "end": "3375530"
  },
  {
    "text": "1.1. So this is 1.1 times 1,000.",
    "start": "3375530",
    "end": "3381090"
  },
  {
    "text": "And that means that z is\n1.1 times the natural log",
    "start": "3381090",
    "end": "3387150"
  },
  {
    "text": "of 1.1 plus 1 minus 1.1. And that is close to\nbut at least 0.0048.",
    "start": "3387150",
    "end": "3399020"
  },
  {
    "text": "So this probability is\nat most, by Chernoff, e to the minus 0.0048 times\nthe expected number of winners",
    "start": "3399020",
    "end": "3410150"
  },
  {
    "text": "is 1,000. So that is e to the minus\n4.8, which is less than 1%,",
    "start": "3410150",
    "end": "3419930"
  },
  {
    "text": "1 in 100. So that's pretty powerful. It says, you've got 10\nmillion people who could win.",
    "start": "3419930",
    "end": "3428720"
  },
  {
    "text": "The chance of even having 100\nmore than the 1,000 you expect is 1% chance at most--\nvery, very powerful.",
    "start": "3428720",
    "end": "3437880"
  },
  {
    "text": "It says you really\nexpect to get really close to the mean\nin this situation.",
    "start": "3437880",
    "end": "3445660"
  },
  {
    "text": "OK, a lot better--\nMarkov here gives you, what, 1,000 over 1,100.",
    "start": "3445660",
    "end": "3452380"
  },
  {
    "text": "It says your probability\ncould be 90% or something-- not very useful. Chebyshev won't give\nyou much here either.",
    "start": "3452380",
    "end": "3458760"
  },
  {
    "text": "So if you're in a situation\nto apply Chernoff, always go there. It gives you the best bounds.",
    "start": "3458760",
    "end": "3464800"
  },
  {
    "text": "Any questions?  This of course is why computer\nscientists use it all the time.",
    "start": "3464800",
    "end": "3470700"
  },
  {
    "text": " OK, actually, before\nI do more examples,",
    "start": "3470700",
    "end": "3476990"
  },
  {
    "text": "let me prove the theorem\nin a special case to give you a feel\nfor what's involved.",
    "start": "3476990",
    "end": "3483050"
  },
  {
    "text": "The full proof is in the text. I'm going to prove it\nin the special case where the Tj are 0, 1.",
    "start": "3483050",
    "end": "3491089"
  },
  {
    "text": "So they're indicator\nrandom variables. But they don't have to\nhave the same distribution. So it's still more\ngeneral than you get",
    "start": "3491090",
    "end": "3497730"
  },
  {
    "text": "with a binomial distribution. All right, so we're going\nto do a proof of Chernoff",
    "start": "3497730",
    "end": "3508420"
  },
  {
    "text": "for the special case where\nthe Tj are either 0 or 1.",
    "start": "3508420",
    "end": "3517524"
  },
  {
    "text": "So they're indicator variables. ",
    "start": "3517524",
    "end": "3522980"
  },
  {
    "text": "OK, so the first step is going\nto seem pretty mysterious. But we've been doing\nsomething like it all day.",
    "start": "3522980",
    "end": "3528130"
  },
  {
    "text": " I'm trying to compute the\nprobability T is bigger",
    "start": "3528130",
    "end": "3534320"
  },
  {
    "text": "than c times its expectation. Well, what I'm going to do is\nexponentiate both of these guys",
    "start": "3534320",
    "end": "3542500"
  },
  {
    "text": "and compute the\nprobability that c to the T is at least c to the c times\nthe expected value of T.",
    "start": "3542500",
    "end": "3553119"
  },
  {
    "text": "Now, this is not the first\nthing you'd expect to do, probably, if you were\ntrying to prove this.",
    "start": "3553120",
    "end": "3558579"
  },
  {
    "text": "So it's one of those\ndivine insights that you'd make this step. ",
    "start": "3558580",
    "end": "3564920"
  },
  {
    "text": "And then I'm going to\napply Markov, like we've been doing all day, to this.",
    "start": "3564920",
    "end": "3570960"
  },
  {
    "text": "Now, since T is positive and c\nis positive, these are equal. And this is never non-negative.",
    "start": "3570960",
    "end": "3579520"
  },
  {
    "text": "So now by Markov, this\nis simply upper bounded by the expected value of that,\nexpected value of c to the T,",
    "start": "3579520",
    "end": "3590190"
  },
  {
    "text": "divided by this. ",
    "start": "3590190",
    "end": "3597130"
  },
  {
    "text": "And that's by Markov. So everything we've done today\nis really Markov in disguise. ",
    "start": "3597130",
    "end": "3605500"
  },
  {
    "text": "Any questions so far? You start looking at\nthis, you go, oh my god, I got the random variable\nand the exponent.",
    "start": "3605500",
    "end": "3611700"
  },
  {
    "text": "This is looking\nlike a nightmare. What is the expected\nvalue of c to the T, and this kind of stuff? But we're going to\nhack through it.",
    "start": "3611700",
    "end": "3618579"
  },
  {
    "text": "Because it gives you just\nan amazingly powerful result when you're done. ",
    "start": "3618580",
    "end": "3627880"
  },
  {
    "text": "All right, so we've got to\nevaluate the expected value of c to the T. And we're going\nto use the fact that T is",
    "start": "3627880",
    "end": "3636140"
  },
  {
    "text": "the sum of the Tj's. ",
    "start": "3636140",
    "end": "3645510"
  },
  {
    "text": "And that means that c to the\nT equals c to the T1 times",
    "start": "3645510",
    "end": "3652110"
  },
  {
    "text": "c to the T2 times c to the Tn. The weird thing about this\nproof is that every step sort of",
    "start": "3652110",
    "end": "3659480"
  },
  {
    "text": "makes it more\ncomplicated looking until we get to the end. So it's one of those that's hard\nto figure out the first time.",
    "start": "3659480",
    "end": "3668750"
  },
  {
    "text": "All right, that means the\nexpected value of c to the T is the expected value of\nthe product of these things.",
    "start": "3668750",
    "end": "3678150"
  },
  {
    "start": "3678150",
    "end": "3686309"
  },
  {
    "text": "Now I'm going to use the\nproduct rule for expectation. ",
    "start": "3686310",
    "end": "3699569"
  },
  {
    "text": "Now, why can I use\nthe product rule? What am I assuming to\nbe able to do that? ",
    "start": "3699570",
    "end": "3707160"
  },
  {
    "text": "That they are\nmutually independent, that the c to the\nTj's are mutually independent of each other.",
    "start": "3707160",
    "end": "3713630"
  },
  {
    "text": "And that follows, because the\nTj's are mutually independent. So if a bunch of random variable\nare mutually independent,",
    "start": "3713630",
    "end": "3720870"
  },
  {
    "text": "then their exponentiations\nare mutually independent. So this is by product\nrule for expectation",
    "start": "3720870",
    "end": "3732319"
  },
  {
    "text": "and mutual independence. ",
    "start": "3732320",
    "end": "3738800"
  },
  {
    "text": "OK, so now we've got to\nevaluate the expected value of c to the Tj.",
    "start": "3738800",
    "end": "3745345"
  },
  {
    "start": "3745345",
    "end": "3752139"
  },
  {
    "text": "And this is where\nwe're going to make it simpler by assuming that Tj\nis just a 0, 1 random variable.",
    "start": "3752139",
    "end": "3760069"
  },
  {
    "text": "So the simplification\ncomes in here. ",
    "start": "3760070",
    "end": "3767400"
  },
  {
    "text": "So the expected value of\nTj-- well, there's two cases.",
    "start": "3767400",
    "end": "3773420"
  },
  {
    "text": "Tj is 1, or it's 0. Because we made\nthis simplification. If it's 1, I get\nc to the 1-- ooh,",
    "start": "3773420",
    "end": "3781110"
  },
  {
    "text": "expected value of c to the Tj. Let's get that right. ",
    "start": "3781110",
    "end": "3787599"
  },
  {
    "text": "It could be 1, in which case\nI get a contribution of c to the 1 times the probability\nTj equals 1 plus the case at 0.",
    "start": "3787600",
    "end": "3797590"
  },
  {
    "text": "So I get c to the 0 times\nthe probability Tj is 0. ",
    "start": "3797590",
    "end": "3804230"
  },
  {
    "text": "Well, c to the 1 is just c. ",
    "start": "3804230",
    "end": "3812369"
  },
  {
    "text": "c to the 0 is 1. And I'm going to\nrewrite Tj being",
    "start": "3812370",
    "end": "3817470"
  },
  {
    "text": "0 as 1 minus the\nprobability Tj is 1. ",
    "start": "3817470",
    "end": "3825020"
  },
  {
    "text": "All right, this equals that. And of course the 1 cancels. Now I'm going to\ncollect terms here",
    "start": "3825020",
    "end": "3831750"
  },
  {
    "text": "to get 1 plus c minus 1 times\nthe probability Tj equals 1.",
    "start": "3831750",
    "end": "3840984"
  },
  {
    "start": "3840984",
    "end": "3846680"
  },
  {
    "text": "OK, then I'm going to\ndo one more step here. This is 1 plus c minus 1 times\nthe expected value of Tj.",
    "start": "3846680",
    "end": "3857730"
  },
  {
    "text": "Because if I have an\nindicator random variable, the expected value is the same\nas the probability that it's 1.",
    "start": "3857730",
    "end": "3864880"
  },
  {
    "text": "Because in the\nother case it's 0. And now I'm going to use\nthe trick from last time.",
    "start": "3864880",
    "end": "3871720"
  },
  {
    "text": "Remember 1 plus x is always at\nmost e to the x from last time?",
    "start": "3871720",
    "end": "3877564"
  },
  {
    "text": "None of these steps is\nobvious why we're doing them. But we're going\nto do them anyway.",
    "start": "3877564",
    "end": "3883560"
  },
  {
    "text": "So this is at most e to this,\nc minus 1 expected value of Tj.",
    "start": "3883560",
    "end": "3893416"
  },
  {
    "text": " Because 1 plus anything is at\nmost the exponential of that.",
    "start": "3893416",
    "end": "3901730"
  },
  {
    "text": "And I'm doing this step because\nI got a product of these guys.",
    "start": "3901730",
    "end": "3906830"
  },
  {
    "text": "And I want to put\nthem in the exponent so I can then sum\nthem so it gets easy. ",
    "start": "3906830",
    "end": "3923569"
  },
  {
    "text": "OK, now we just plug\nthis back in here.",
    "start": "3923570",
    "end": "3932880"
  },
  {
    "text": "So that means that the\nexpected value of c to the T",
    "start": "3932880",
    "end": "3941230"
  },
  {
    "text": "is at most a product\nof expected value of e",
    "start": "3941230",
    "end": "3951020"
  },
  {
    "text": "to the cTj is this-- e to the\nc minus 1 expected value of Tj.",
    "start": "3951020",
    "end": "3959280"
  },
  {
    "text": "And now I can convert this\nto a sum in the exponent. ",
    "start": "3959280",
    "end": "3972630"
  },
  {
    "text": "And this is j equals 1 to n. And what do I do\nto simplify that?",
    "start": "3972630",
    "end": "3978061"
  },
  {
    "start": "3978061",
    "end": "3983760"
  },
  {
    "text": "Linearity of expectation.  c minus 1 times the sum j equals\n1 to n expected value of Tj.",
    "start": "3983760",
    "end": "3995895"
  },
  {
    "start": "3995895",
    "end": "4001060"
  },
  {
    "text": "Ooh, let's see, did I? Actually, I used\nlinearity coming out. I already used linearity.",
    "start": "4001060",
    "end": "4007510"
  },
  {
    "text": "I screwed up here. So here I used the linearity\nwhen I took the sum up here",
    "start": "4007510",
    "end": "4014520"
  },
  {
    "text": "inside the expectation. I've already used linearity. What is the sum of the Tj's?",
    "start": "4014520",
    "end": "4019810"
  },
  {
    "text": "T-- yeah, that's what\nI needed to do here. ",
    "start": "4019810",
    "end": "4027130"
  },
  {
    "text": "OK, we're now almost done. We've got now an upper bound\non the expected value of c to the T. And it is this.",
    "start": "4027130",
    "end": "4034450"
  },
  {
    "text": "And we just plug\nthat in back up here. ",
    "start": "4034450",
    "end": "4040750"
  },
  {
    "text": "So now this is at most e to the\nc minus 1 expected value of T",
    "start": "4040750",
    "end": "4048870"
  },
  {
    "text": "over c to the c times\nthe expected value of t. And now I just do manipulation.",
    "start": "4048870",
    "end": "4056390"
  },
  {
    "text": "c to something is the\nsame as e to the log of c times that something. So this is e to the minus c ln\nc expected value of T plus that.",
    "start": "4056390",
    "end": "4070560"
  },
  {
    "start": "4070560",
    "end": "4077220"
  },
  {
    "text": "And then I'm\nrunning out of room. That equals-- I can just pull\nout the expected values of T. I",
    "start": "4077220",
    "end": "4085350"
  },
  {
    "text": "get e to the minus c log of\nc plus c minus 1 expected",
    "start": "4085350",
    "end": "4094590"
  },
  {
    "text": "value of T. And that's e to the\nminus z expected value of T.",
    "start": "4094590",
    "end": "4103707"
  },
  {
    "text": "All right, so that's\na marathon proof. It's the worst proof I think. Well, maybe minimum\nspanning tree was worse.",
    "start": "4103707",
    "end": "4108950"
  },
  {
    "text": "But this is one of the worst\nproofs we've seen this year. But I wanted to show it to you.",
    "start": "4108950",
    "end": "4114383"
  },
  {
    "text": "Because it's one of the\nmost important results that we cover, certainly\nin probability, that can be very\nuseful in practice.",
    "start": "4114384",
    "end": "4121352"
  },
  {
    "text": "And it gives you some\nfeel for, hey, this wasn't so obvious to\ndo it the first time, and also some of the\ntechniques that are used,",
    "start": "4121352",
    "end": "4127639"
  },
  {
    "text": "which is really\nMarkov's theorem. Any questions?",
    "start": "4127640",
    "end": "4133068"
  },
  {
    "text": "Yeah. AUDIENCE: Over there, you\ndefine z as 1 minus c.",
    "start": "4133069",
    "end": "4138380"
  },
  {
    "text": "PROFESSOR: Did I do it wrong? AUDIENCE: c natural\nlog of c, 1 minus c. Maybe it's plus c? PROFESSOR: Oh, I've\ngot to change the sign.",
    "start": "4138381",
    "end": "4144479"
  },
  {
    "text": "Because I pulled a\nnegative out in front. So it's got to be\nnegative c minus 1, which means negative c plus 1.",
    "start": "4144479",
    "end": "4151990"
  },
  {
    "text": "Yeah, good. Yeah, this was OK. I just made the\nmistake going to there.",
    "start": "4151990",
    "end": "4158043"
  },
  {
    "text": "Any other questions? ",
    "start": "4158043",
    "end": "4164670"
  },
  {
    "text": "OK, so the common theme here in\nusing Markov to get Chebyshev,",
    "start": "4164670",
    "end": "4173778"
  },
  {
    "text": "to get Chernoff, to get\nthe Markov extensions, is always the same. And let me show you\nwhat that theme is.",
    "start": "4173779",
    "end": "4180649"
  },
  {
    "start": "4180649",
    "end": "4186299"
  },
  {
    "text": "Because you can use it to\nget even other results. ",
    "start": "4186300",
    "end": "4192339"
  },
  {
    "text": "When we're trying to figure\nout the probability that T is at least c times its\nexpected value, or actually",
    "start": "4192340",
    "end": "4201380"
  },
  {
    "text": "even in general,\neven more generally than that, the probability\nthat A is bigger than B,",
    "start": "4201380",
    "end": "4208030"
  },
  {
    "text": "even more generally,\nwell, that's the same as the probability that\nf of A is bigger than f of B",
    "start": "4208030",
    "end": "4216150"
  },
  {
    "text": "as long as you\ndon't change signs. And then by Markov, this is at\nmost the expected value of that",
    "start": "4216150",
    "end": "4225099"
  },
  {
    "text": "as long as it's\nnon-negative over that. ",
    "start": "4225100",
    "end": "4231600"
  },
  {
    "text": "In Chebyshev, what\nfunction f did we use for Chebyshev in\nderiving Chebyshev's theorem?",
    "start": "4231600",
    "end": "4239540"
  },
  {
    "text": "What was f doing in Chebyshev? Actually I probably\njust erased it. ",
    "start": "4239540",
    "end": "4246043"
  },
  {
    "text": "What operation were we\ndoing with Chebyshev? AUDIENCE: Variance. PROFESSOR: Variance. And that meant we\nwere squaring it.",
    "start": "4246043",
    "end": "4254500"
  },
  {
    "text": "So the technique used\nto prove Chebyshev was f was the square function.",
    "start": "4254500",
    "end": "4259650"
  },
  {
    "text": "For Chernoff, f is the\nexponentiation function, which turns out to be--\nin fact, when we did it",
    "start": "4259650",
    "end": "4266400"
  },
  {
    "text": "for Chernoff, that's the\noptimal choice of functions to get good bounds. ",
    "start": "4266400",
    "end": "4273776"
  },
  {
    "text": "All right, any\nquestions on that? All right, let's do one more\nexample here with numbers.",
    "start": "4273776",
    "end": "4282451"
  },
  {
    "text": " And this is a load\nbalancing application",
    "start": "4282452",
    "end": "4289480"
  },
  {
    "text": "for example you might\nhave with web servers. Say you've got to build\na load balancing device,",
    "start": "4289480",
    "end": "4296080"
  },
  {
    "text": "and it's got to\nbalance N jobs, B1, B2,",
    "start": "4296080",
    "end": "4302480"
  },
  {
    "text": "to BN, across a set of M\nservers, S1, S2, to SN.",
    "start": "4302480",
    "end": "4315910"
  },
  {
    "text": "And say you're doing this\nfor a decent sized website. So maybe N is 100,000.",
    "start": "4315910",
    "end": "4323290"
  },
  {
    "text": "You get 100,000\nrequests a minute. And say you've got 10 servers\nto handle those requests.",
    "start": "4323290",
    "end": "4331700"
  },
  {
    "text": "And say the requests are--\nthe time for the j-th request",
    "start": "4331700",
    "end": "4336740"
  },
  {
    "text": "is, say, Bj takes the j-th job. The j-th request takes Lj time.",
    "start": "4336740",
    "end": "4345600"
  },
  {
    "text": "And the time is the\nsame on any server. The servers are all equivalent. And let's assume it's normalized\nso that Lj is between 0 and 1.",
    "start": "4345600",
    "end": "4354850"
  },
  {
    "text": "Maybe the worst job takes\na second to do, let's say.",
    "start": "4354850",
    "end": "4360490"
  },
  {
    "text": "And say that if you sum up\nthe length of all the jobs, you get L. Total workload\nis the sum of all of them.",
    "start": "4360490",
    "end": "4373780"
  },
  {
    "text": "j equals 1 to N. And we're going to assume that\nthe average job length is 1/4",
    "start": "4373780",
    "end": "4381480"
  },
  {
    "text": "second. So we're going to assume\nthat the total amount of work",
    "start": "4381480",
    "end": "4388820"
  },
  {
    "text": "is 25,000 seconds, say. So the average job\nlength is 1/4 second.",
    "start": "4388820",
    "end": "4396580"
  },
  {
    "text": "And the job is to assign these\ntasks to the 10 servers so that hopefully every server\nis doing L/M work,",
    "start": "4396580",
    "end": "4405970"
  },
  {
    "text": "which would be 25,000/10, or\n2,500 milliseconds of work,",
    "start": "4405970",
    "end": "4413360"
  },
  {
    "text": "something like that. I don't know. Because when you're\ndoing load balancing, you want to take your load and\nspread it evenly and equally",
    "start": "4413360",
    "end": "4420130"
  },
  {
    "text": "among all the servers.  Any questions about the problem?",
    "start": "4420130",
    "end": "4426720"
  },
  {
    "text": "You've got a bunch of\njobs, a bunch of servers. You want to assign the\njobs to the servers to balance the load.",
    "start": "4426720",
    "end": "4433570"
  },
  {
    "text": "Well, what is the\nsimplest algorithm you could think of to do this? AUDIENCE: [INAUDIBLE]",
    "start": "4433570",
    "end": "4440134"
  },
  {
    "text": "PROFESSOR: That's a good\nalgorithm to do this. In practice, the\nfirst thing people do is, well, take the first N/M\njobs, put them on server one,",
    "start": "4440134",
    "end": "4451910"
  },
  {
    "text": "the next N/M on server two. Or they'll use something called\nround robin-- first job goes",
    "start": "4451910",
    "end": "4457970"
  },
  {
    "text": "here, second here, third here,\n10th here, back and start over. And they hope that it\nwill balance the load.",
    "start": "4457970",
    "end": "4465230"
  },
  {
    "text": "But it might well not. Because maybe every\n10th job is a big one.",
    "start": "4465230",
    "end": "4471486"
  },
  {
    "text": "So what's much better\nto do in practice is to assign them randomly. So a job comes in.",
    "start": "4471486",
    "end": "4477370"
  },
  {
    "text": "You don't even pay\nattention to how hard it is, how much time\nyou think it'll take. You might not even know before\nyou start the job how long it's",
    "start": "4477370",
    "end": "4484530"
  },
  {
    "text": "going to take to complete. Give it to a random server. Don't even look at how\nmuch work that server has.",
    "start": "4484530",
    "end": "4490989"
  },
  {
    "text": "Just give it to a random one. And it turns out this\ndoes very, very well.",
    "start": "4490990",
    "end": "4496550"
  },
  {
    "text": "Without knowing anything,\njust that simple approach does great in practice. And today, state of the\nart load balancers do this.",
    "start": "4496550",
    "end": "4505395"
  },
  {
    "text": " We've been doing randomized\nkinds of thing like this",
    "start": "4505395",
    "end": "4511119"
  },
  {
    "text": "at Akamai now for a decade. And it's just stunning\nhow well it works.",
    "start": "4511120",
    "end": "4516820"
  },
  {
    "text": "And so let's see why that is. ",
    "start": "4516820",
    "end": "4524730"
  },
  {
    "text": "Of course we're going to use\nthe Chernoff bound to do it. So let's let Rij be the load\non server Si from job Bj.",
    "start": "4524730",
    "end": "4547079"
  },
  {
    "text": "Now, if Bj is not assigned\nto Si, it's zero load.",
    "start": "4547080",
    "end": "4554470"
  },
  {
    "text": "Because it's not even\ndoing the work there. So we know that Rij\nequals the load of Bj",
    "start": "4554470",
    "end": "4563950"
  },
  {
    "text": "if it's assigned to Si. ",
    "start": "4563950",
    "end": "4571020"
  },
  {
    "text": "And that happens\nwith probability 1/M. The job picks one of\nthe M servers at random.",
    "start": "4571020",
    "end": "4579610"
  },
  {
    "text": "And otherwise, the load is 0. Because it's not\nassigned to that server.",
    "start": "4579610",
    "end": "4585770"
  },
  {
    "text": "And that is probability\n1 minus 1/M.",
    "start": "4585770",
    "end": "4591860"
  },
  {
    "text": "Now let's look at\nhow much load gets assigned by this random\nalgorithm to server i.",
    "start": "4591860",
    "end": "4599140"
  },
  {
    "text": "So we'll let Ri be the sum\nof all the load assigned",
    "start": "4599140",
    "end": "4608890"
  },
  {
    "text": "to server i. ",
    "start": "4608890",
    "end": "4618830"
  },
  {
    "text": "So we've got this indicator\nwhere the random variables are not 0, 1. They're 0 and whatever\nthis load happens to be",
    "start": "4618830",
    "end": "4625760"
  },
  {
    "text": "for the j-th job, at most 1. And we sum up the value\nfor the contribution",
    "start": "4625760",
    "end": "4632180"
  },
  {
    "text": "to Si over all the jobs. ",
    "start": "4632180",
    "end": "4637360"
  },
  {
    "text": "So now we compute\nthe expected value of Ri, the expected\nload on the i-th server.",
    "start": "4637360",
    "end": "4642628"
  },
  {
    "start": "4642628",
    "end": "4648500"
  },
  {
    "text": "So the expected load\non the i-th server is-- well, we use\nlinearity of expectation.",
    "start": "4648500",
    "end": "4655975"
  },
  {
    "start": "4655975",
    "end": "4664870"
  },
  {
    "text": "And the expected value\nof Rij-- well, 0 or Lj.",
    "start": "4664870",
    "end": "4670070"
  },
  {
    "text": "It's Lj with\nprobability 1/M. This is just now the sum of Lj over\nM. And the sum of Lj is just L.",
    "start": "4670070",
    "end": "4686060"
  },
  {
    "text": "So the expected load\nof the i-th server is the total load\ndivided by the number",
    "start": "4686060",
    "end": "4691739"
  },
  {
    "text": "of servers, which is perfect. It's optimal-- can't\ndo better than that.",
    "start": "4691740",
    "end": "4696910"
  },
  {
    "text": " It makes sense. If you assign all\nthe jobs randomly,",
    "start": "4696910",
    "end": "4703600"
  },
  {
    "text": "every server is expecting to\nget 1/M of the total load. ",
    "start": "4703600",
    "end": "4709402"
  },
  {
    "text": "Now we want to know\nthe probability it deviates from that,\nthat you have too much load on the i-th server.",
    "start": "4709402",
    "end": "4714995"
  },
  {
    "start": "4714996",
    "end": "4731460"
  },
  {
    "text": "All right, so the probability\nthat the i-th server has c times the optimal load\nis at most, by Chernoff,",
    "start": "4731460",
    "end": "4742530"
  },
  {
    "text": "if the jobs are independent,\nminus zL over M, minus z times the\nexpected load where z is c",
    "start": "4742530",
    "end": "4752780"
  },
  {
    "text": "ln c plus 1 minus c. This is Chernoff\nnow, just straight",
    "start": "4752780",
    "end": "4758366"
  },
  {
    "text": "from the formula of Chernoff,\nas long as these loads are mutually independent. All right, so we know that when\nc gets to be-- I don't know,",
    "start": "4758367",
    "end": "4769320"
  },
  {
    "text": "you pick 10% above\noptimal, c equals 1.1,",
    "start": "4769320",
    "end": "4776969"
  },
  {
    "text": "well, we know that this is\ngoing to be a very small number. L/M is 2,500.",
    "start": "4776970",
    "end": "4782670"
  },
  {
    "text": "And z, in this case,\nwe found was 0.0048.",
    "start": "4782670",
    "end": "4788730"
  },
  {
    "text": "So we get e to the minus\n0.0048 times 2,500.",
    "start": "4788730",
    "end": "4796970"
  },
  {
    "text": "And that is really tiny. That's less than 1 in 160,000.",
    "start": "4796970",
    "end": "4804730"
  },
  {
    "text": "So Chernoff tells\nus the probability that any server, a\nparticular server, gets 10% load more than\nyou expect is minuscule.",
    "start": "4804730",
    "end": "4815219"
  },
  {
    "text": "Now, we're not quite done. That tells us the probability\nthe first server gets",
    "start": "4815220",
    "end": "4821150"
  },
  {
    "text": "10% too much load or the problem\nthe second server got 10% too",
    "start": "4821150",
    "end": "4826830"
  },
  {
    "text": "much load, and so forth. But what we really care\nabout is the worst server.",
    "start": "4826830",
    "end": "4835030"
  },
  {
    "text": "If all of them are\ngood except for one, you're still in trouble. Because the one ruined your day.",
    "start": "4835030",
    "end": "4840570"
  },
  {
    "text": "Because it didn't\nget the work done. So what do you do to\nbound the probability",
    "start": "4840570",
    "end": "4845720"
  },
  {
    "text": "that any of the servers got\ntoo much load, any of the 10?",
    "start": "4845720",
    "end": "4852510"
  },
  {
    "text": "So what I really want to\nknow is the probability that the worst server\nof M takes more than cL",
    "start": "4852510",
    "end": "4863724"
  },
  {
    "text": "over M. Well, that's the\nprobability that the first one",
    "start": "4863725",
    "end": "4870190"
  },
  {
    "text": "has more than cL over M union\nthe second one has more than cL",
    "start": "4870190",
    "end": "4876320"
  },
  {
    "text": "over M union the M-th one. ",
    "start": "4876320",
    "end": "4885850"
  },
  {
    "text": "What do I do to get that\nprobability, the probability of a union of events,\nupper bounded?",
    "start": "4885850",
    "end": "4893510"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: Upper bounded by\nthe sum of the individual guys.",
    "start": "4893510",
    "end": "4898560"
  },
  {
    "text": "It's the sum i equals 1 to M\nprobability Ri greater than",
    "start": "4898560",
    "end": "4904456"
  },
  {
    "text": "or equal to cL over M.\nAnd so that, each of these is at most 1 in 160,000.",
    "start": "4904456",
    "end": "4910739"
  },
  {
    "text": "This is at most M/160,000.",
    "start": "4910740",
    "end": "4917320"
  },
  {
    "text": "And that is equal\nto 1 in 16,000.",
    "start": "4917320",
    "end": "4922899"
  },
  {
    "text": "All right, so now\nwe have the answer. The chance that any server\ngot 10% load or more is 1 in 16,000 at most, which\nis why randomized load balancing",
    "start": "4922899",
    "end": "4934790"
  },
  {
    "text": "is used a lot in practice. Now tomorrow, you're going to\ndo a real world example where",
    "start": "4934790",
    "end": "4943250"
  },
  {
    "text": "people use this\nkind of analysis, and it led to utter disaster.",
    "start": "4943250",
    "end": "4948550"
  },
  {
    "text": "And the reason was that the\ncomponents they were looking at were not independent.",
    "start": "4948550",
    "end": "4955281"
  },
  {
    "text": "And the example has to do with\nthe subprime mortgage disaster. And I don't have time\ntoday to go through it all. But it's in the text, and\nyou'll see it tomorrow.",
    "start": "4955282",
    "end": "4962059"
  },
  {
    "text": "But basically what\nhappened is that they took a whole bunch of\nloans, subprime loans,",
    "start": "4962060",
    "end": "4967950"
  },
  {
    "text": "put them into these\nthings called bonds, and then did an analysis\nabout how many failures",
    "start": "4967950",
    "end": "4972969"
  },
  {
    "text": "they'd expect to have. And they assumed the loans\nwere all mutually independent.",
    "start": "4972970",
    "end": "4978300"
  },
  {
    "text": "And they applied\ntheir Chernoff bounds. And that concluded\nthat the chances of being off from the\nexpectation were nil, like e",
    "start": "4978300",
    "end": "4985270"
  },
  {
    "text": "to the minus 380. In reality, the loans\nwere highly dependent.",
    "start": "4985270",
    "end": "4990950"
  },
  {
    "text": "When one failed, a\nlot tended to fail. And that led to disaster. And you'll go through\nsome of the math on that tomorrow in recitation.",
    "start": "4990950",
    "end": "4998220"
  },
  {
    "start": "4998220",
    "end": "5001995"
  }
]