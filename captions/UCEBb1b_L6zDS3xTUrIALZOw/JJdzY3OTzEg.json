[
  {
    "start": "0",
    "end": "70000"
  },
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13350"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13350",
    "end": "27440"
  },
  {
    "text": "PROFESSOR: Today we're\ngoing to dig a little deeper into the system that\nwe've been talking about.",
    "start": "27440",
    "end": "32820"
  },
  {
    "text": "So we've already talked about\nsource coding and source decoding.",
    "start": "32820",
    "end": "38520"
  },
  {
    "text": "And then we talked\nabout channel coding as we've just finished talking\nabout block codes and Viterbi--",
    "start": "38520",
    "end": "44989"
  },
  {
    "text": "convolutional codes\nand Viterbi decoding. So that's the coding\nhere and the decoding.",
    "start": "44990",
    "end": "50360"
  },
  {
    "text": "And now we're going to\ndrill down to the next level to start to talk about\nthe actual signals going",
    "start": "50360",
    "end": "56300"
  },
  {
    "text": "across physical channels. So this is going to\nactually extend over the entire next\nmodule of the course.",
    "start": "56300",
    "end": "62630"
  },
  {
    "text": "I want to describe this in\nthe context of something you're going to be doing\nin labs 4 through 6.",
    "start": "62630",
    "end": "70052"
  },
  {
    "start": "70000",
    "end": "70000"
  },
  {
    "text": "You're actually\ngoing to experiment with a specific channel. What you'll have is bits coming\nin, code words coming in,",
    "start": "70052",
    "end": "78860"
  },
  {
    "text": "being translated to signals. In this case,\ndiscrete time signals,",
    "start": "78860",
    "end": "84290"
  },
  {
    "text": "and I'll give you an\nexample shortly of that. The signals will then be\nadapted through the modulator",
    "start": "84290",
    "end": "90380"
  },
  {
    "text": "for transmission on\nan analog channel. So there's a modulation\nprocess and there's",
    "start": "90380",
    "end": "97250"
  },
  {
    "text": "a digital-to-analog\nconversion process. You'll be generating\nwaveform that you apply",
    "start": "97250",
    "end": "102500"
  },
  {
    "text": "to the speaker in your laptop. That's going to\nbe a transmitter. The channel is going to\nbe just the air around you",
    "start": "102500",
    "end": "112070"
  },
  {
    "text": "with all the disturbances\nof room acoustics and noise and all of that, all the\ndistortions from that.",
    "start": "112070",
    "end": "117223"
  },
  {
    "text": "And then you'll\npick up the signal on the microphone on your\nlaptop or an external microphone if you want.",
    "start": "117223",
    "end": "123080"
  },
  {
    "text": "Conversion from analog\nto digital, demodulation and filtering to\nundo the modulation,",
    "start": "123080",
    "end": "129289"
  },
  {
    "text": "and we'll be talking\nabout this in more detail to get another\nsequence of samples.",
    "start": "129289",
    "end": "134690"
  },
  {
    "text": "After which you\nhave a decision rule that then looks at the samples\nand says, did I get a 0 or a 1? And you spit out the\nbits of your code word.",
    "start": "134690",
    "end": "142470"
  },
  {
    "text": "OK, so this is what we're\ngoing to be looking at. So here is what you\nmight be sending",
    "start": "142470",
    "end": "149170"
  },
  {
    "start": "146000",
    "end": "146000"
  },
  {
    "text": "at the transmitting end. You've got the bits coming in. You're going to convert\nthem to signals,",
    "start": "149170",
    "end": "155240"
  },
  {
    "text": "and we're going to think\nof discrete time signals. So this is a signal x of\nn-- n takes integer value,",
    "start": "155240",
    "end": "161520"
  },
  {
    "text": "so that's my\ndiscrete time clock. And the typical waveform\nmight look like this.",
    "start": "161520",
    "end": "166620"
  },
  {
    "text": "I might decide just very simply\nto have levels held at 0.5 for,",
    "start": "166620",
    "end": "173360"
  },
  {
    "text": "let's say, 16 samples\nper bit, and then held at 0 for 16 samples to denote\na 1 and a 0 respectively.",
    "start": "173360",
    "end": "181110"
  },
  {
    "text": "So here's a 1, a 00, 111, 0101.",
    "start": "181110",
    "end": "187550"
  },
  {
    "text": "So we're converting two samples. This is a sample number,\nand then the next step",
    "start": "187550",
    "end": "193280"
  },
  {
    "text": "will be to actually-- in your computer you'll\nsend this to your digital to analog converter which will--",
    "start": "193280",
    "end": "199100"
  },
  {
    "text": "with a particular clock cycle,\nconvert this to real time.",
    "start": "199100",
    "end": "204548"
  },
  {
    "text": "What you might imagine is\nthat the actual waveform that goes out on the\nchannel is somehow related to the\ncontinuous waveform",
    "start": "204548",
    "end": "210709"
  },
  {
    "text": "that you get by just connecting\nthe tops of these discrete time values.",
    "start": "210710",
    "end": "217400"
  },
  {
    "text": "The actual mechanism for\ntransmission through the air we'll talk about next time. So right now we're\njust going to focus",
    "start": "217400",
    "end": "223069"
  },
  {
    "text": "on the level of the\ndiscrete time signals. And at the other end, after\nyou've done your transmissions",
    "start": "223070",
    "end": "230780"
  },
  {
    "text": "through the channel and you've\ndemodulated and filtered, you get a sequence\nwhich ideally is",
    "start": "230780",
    "end": "235819"
  },
  {
    "text": "a replication of the\nsequence that you sent in. It can't have a scale factor,\nscale factors don't worry us.",
    "start": "235820",
    "end": "243880"
  },
  {
    "text": "In this case, you see that\nthe amplitude is divided by 2. But basically you see\nthe trace of what was",
    "start": "243880",
    "end": "249459"
  },
  {
    "text": "sent at the transmitting end. There's some distortion\nthat's introduced",
    "start": "249460",
    "end": "254530"
  },
  {
    "text": "by the dynamics of\nthe channel, and we'll be talking about that\nin more detail later. So we aren't getting\nquite the straight edges.",
    "start": "254530",
    "end": "261398"
  },
  {
    "text": "But after a brief\ntransient period, the waveform seems to\nsettle to the constant value that we had of the input.",
    "start": "261399",
    "end": "268240"
  },
  {
    "text": "So this is our received\nset of samples. Now in this figure, I've\nassumed that there's no noise, only the distortion.",
    "start": "268240",
    "end": "274630"
  },
  {
    "text": "This lecture is going\nto be about the noise. I wanted you to get the sense\nof what distortion does,",
    "start": "274630",
    "end": "279750"
  },
  {
    "text": "and then we'll park that issue\nand come back to it next time and actually for several\nlectures after that.",
    "start": "279750",
    "end": "285030"
  },
  {
    "text": "But this lecture we're\ngoing to focus on noise. Before we look at noise,\nthis is what a noise-free",
    "start": "285030",
    "end": "291070"
  },
  {
    "text": "received signal might look like\nwith just the distortion in it. ",
    "start": "291070",
    "end": "296360"
  },
  {
    "text": "OK. And now you've got to\nconvert to a bit sequence.",
    "start": "296360",
    "end": "301629"
  },
  {
    "text": "So a simple way to do that\nis pick an appropriate point in each bit slot. Each slot of 16 samples long.",
    "start": "301630",
    "end": "308580"
  },
  {
    "text": "Pick an appropriate\npoint, taking account of these\ntransient effects and so on, and then sample.",
    "start": "308580",
    "end": "314610"
  },
  {
    "text": "And if the sample value\nis above a threshold, you'll declare a 1.",
    "start": "314610",
    "end": "319690"
  },
  {
    "text": "If the sample values below the\nthreshold, you'll declare a 0. And so you reconstruct\nthe sequence that went in.",
    "start": "319690",
    "end": "327380"
  },
  {
    "text": "So we have the sample and\nthreshold feature here. So we're just taking one of\nthe samples in the bit period,",
    "start": "327380",
    "end": "334060"
  },
  {
    "text": "comparing with the threshold,\nand making a declaration. That's a very simple-minded\ndecision rule. ",
    "start": "334060",
    "end": "341210"
  },
  {
    "text": "OK. So we'll come back\nto distortion. Today I want to\ntalk about noise,",
    "start": "341210",
    "end": "348920"
  },
  {
    "text": "and I want to then\nsuppress distortion. So let's forget\nabout distortion. Let's assume that\nthe received signal",
    "start": "348920",
    "end": "354530"
  },
  {
    "text": "yn is exactly what was sent\nexcept for some additive noise.",
    "start": "354530",
    "end": "359840"
  },
  {
    "text": "So what we're\nimagining is you send a nice clean set of samples\nhere into your digital-to-analog",
    "start": "359840",
    "end": "376750"
  },
  {
    "text": "converter, and what\ncomes out ideally",
    "start": "376750",
    "end": "382660"
  },
  {
    "text": "would be the same\nset of samples,",
    "start": "382660",
    "end": "388160"
  },
  {
    "text": "but actually what happens is\nthat each of these samples is perturbed by noise. And so you get something\nthat might look like this.",
    "start": "388160",
    "end": "399000"
  },
  {
    "start": "399000",
    "end": "404290"
  },
  {
    "text": "OK, so this is y, then, and\nwhat we had before was x of n.",
    "start": "404290",
    "end": "410185"
  },
  {
    "text": " OK.",
    "start": "410185",
    "end": "415580"
  },
  {
    "text": "So nominally you'd\nget the same thing. The only thing\nthat's different now is you've got an additive noise.",
    "start": "415580",
    "end": "421849"
  },
  {
    "text": "We're going to assume\nthat this noise sample wn is independent from\none sample to the next.",
    "start": "421850",
    "end": "430130"
  },
  {
    "text": "So when the channel\nand the processing and so on decides to put\na noise sample on this,",
    "start": "430130",
    "end": "435710"
  },
  {
    "text": "it doesn't pay attention\nto what noise sample was out of on either side. So every noise sample\nis picked independently.",
    "start": "435710",
    "end": "442138"
  },
  {
    "text": "And it's picked from\nthe same distribution. That's with the identically\ndistributed part of this mean.",
    "start": "442138",
    "end": "447270"
  },
  {
    "text": "So the characteristics\nof the noise are the same right\nthrough our signal.",
    "start": "447270",
    "end": "452310"
  },
  {
    "text": "That's what we're assuming. That's the identically\ndistributed part. It's a statement about the\nstationarity of the noise characteristics.",
    "start": "452310",
    "end": "460070"
  },
  {
    "text": "All of this can be\ngeneralized, but this is where we're going\nto have our story",
    "start": "460070",
    "end": "465099"
  },
  {
    "text": "and that's all we're\ngoing to consider. OK, a key metric,\nthen, is what's",
    "start": "465100",
    "end": "472940"
  },
  {
    "text": "the signal-to-noise ratio? This is something that you see\nall over the place, the SNR. Usually what people\nmean is signal power,",
    "start": "472940",
    "end": "481490"
  },
  {
    "text": "and power is usually the\nsquare of a signal-- that's what you're thinking of. If you think of\nvoltages, for instance,",
    "start": "481490",
    "end": "487633"
  },
  {
    "text": "the square of the voltage gives\nyou power in the resistor. So you think of the\nsignal as being x, Its power as being x squared.",
    "start": "487633",
    "end": "495002"
  },
  {
    "text": "Except you've got\nto decide, do you want to talk about the peak\npower or the time average power or some other measurement\nof the signal power?",
    "start": "495002",
    "end": "502560"
  },
  {
    "text": "So that's the signal\npart of this ratio. And then the noise part of the\nratio is the noise variance.",
    "start": "502560",
    "end": "511130"
  },
  {
    "text": "So we have a noise\ncomponent wn, it's the expected squared\namplitude of that.",
    "start": "511130",
    "end": "516349"
  },
  {
    "text": "Oh, by the way, I didn't-- this is on my slide,\nbut I didn't say it yet. I'm going to assume\nthe noise is zero mean.",
    "start": "516350",
    "end": "522960"
  },
  {
    "text": "Which means that these\nexcursions from what you expect on average are at 0.",
    "start": "522960",
    "end": "531260"
  },
  {
    "text": "If there was a systematic\nbias to the noise, if I knew that there\nwas a non-zero mean,",
    "start": "531260",
    "end": "536810"
  },
  {
    "text": "I could just factor\nthat into my processing and think of my\nexpected received signal as taking account of\nthat non-zero mean.",
    "start": "536810",
    "end": "544200"
  },
  {
    "text": "So there's no loss of\ngenerality, really. I'm assuming a zero mean noise.",
    "start": "544200",
    "end": "550010"
  },
  {
    "text": "OK. Now when you come to\nactually computing numbers, this is another example--",
    "start": "550010",
    "end": "556575"
  },
  {
    "start": "554000",
    "end": "554000"
  },
  {
    "text": "showing another\nkind of waveform, this is the sum of\nsinusoids, I assume, to which you're\nadding some noise.",
    "start": "556575",
    "end": "562910"
  },
  {
    "text": "And in this\nparticular simulation, by tweaking the value\nof A there, that's the--",
    "start": "562910",
    "end": "568029"
  },
  {
    "text": "it's a gain factor\non the signal. You can actually vary\nthe signal-to-noise ratio and get a feel for\nwhat difference",
    "start": "568030",
    "end": "575030"
  },
  {
    "text": "signal-to-noise\nratio is represented. So at high\nsignal-to-noise ratio,",
    "start": "575030",
    "end": "580600"
  },
  {
    "text": "the noise isn't perturbing\nwhat went down very much. But when you get the lower\nsignal-to-noise ratios,",
    "start": "580600",
    "end": "586150"
  },
  {
    "text": "the noise is actually\ndistorting the signal that you started with\nquite substantially.",
    "start": "586150",
    "end": "592190"
  },
  {
    "text": "Now the SNR here is\ndescribed in dB, decibels.",
    "start": "592190",
    "end": "598930"
  },
  {
    "text": "And so let me just\nsay a word about that. That's a unit you'll\nsee all the time.",
    "start": "598930",
    "end": "604338"
  },
  {
    "text": "You've seen all the time.  So we're really\ntrying to measure",
    "start": "604338",
    "end": "610510"
  },
  {
    "text": "a signal-to-noise ratio. So this is what you\nwould normally think of. But in many applications,\na logarithmic scale",
    "start": "610510",
    "end": "615879"
  },
  {
    "text": "is really what you\nwant to deal with. For instance, if\nyou're measuring the response of the ear\nto noise intensities,",
    "start": "615880",
    "end": "624760"
  },
  {
    "text": "it turns out there's a\nlogarithmic feature built into our sensors. So usually want to be measuring\npower and power ratios",
    "start": "624760",
    "end": "631750"
  },
  {
    "text": "in terms of a log scale. That should have had\na capital B there. So here's the definition\nof what a ratio is on dB.",
    "start": "631750",
    "end": "640600"
  },
  {
    "text": "It's the ratio log to\nthe base 10 times 10. ",
    "start": "640600",
    "end": "646360"
  },
  {
    "text": "One caution here. I told you that when\nwe talk about powers, that's the square\nof the amplitude.",
    "start": "646360",
    "end": "652100"
  },
  {
    "text": "So if you're going to\ncompare amplitudes, ratio of amplitudes\non a log scale, then actually what you end\nup doing is taking 20",
    "start": "652100",
    "end": "659199"
  },
  {
    "text": "log 10 ratio of amplitudes. So you'll sometimes see\nthis definition as 20 log to the base 10\nratio of amplitudes,",
    "start": "659200",
    "end": "666310"
  },
  {
    "text": "and what people are doing, then,\nis comparing amplitude ratios, not power ratios. You have a question?",
    "start": "666310",
    "end": "671505"
  },
  {
    "text": "AUDIENCE: Why do we define\npower as amplitude squared? PROFESSOR: In sum--\nso the question was,",
    "start": "671505",
    "end": "677080"
  },
  {
    "text": "why do we define power\nas amplitude squared? ",
    "start": "677080",
    "end": "683530"
  },
  {
    "text": "If you think of an\nelectrical circuit",
    "start": "683530",
    "end": "689350"
  },
  {
    "text": "with some signal\napplied across it, a voltage, the\ninstantaneous power dissipated in the\nresistor is given by that.",
    "start": "689350",
    "end": "698560"
  },
  {
    "text": "So people start to think of\nsquare of a quantity as power. In the continuous\ntime domain that's",
    "start": "698560",
    "end": "704200"
  },
  {
    "text": "very natural in signals\nthat come from physics, and that terminology is\njust being carried over",
    "start": "704200",
    "end": "709510"
  },
  {
    "text": "to this kind of a\ndiscrete time setting. So when people say power, they\nmean square of the signal.",
    "start": "709510",
    "end": "715105"
  },
  {
    "start": "714000",
    "end": "714000"
  },
  {
    "text": "It could've been\ncalled something else.  OK.",
    "start": "715105",
    "end": "721860"
  },
  {
    "text": "So you can actually span\nhuge ratios in power on this log scale with much\nmore better behaved numbers.",
    "start": "721860",
    "end": "730210"
  },
  {
    "text": "0 dB, then, is a ratio of 1. 3 dB, this is good to\ncarry around in your head.",
    "start": "730210",
    "end": "736730"
  },
  {
    "text": "3 dB, it's actually\n3.01-something, but 3 dB is a factor of\n2 on the power ratio,",
    "start": "736730",
    "end": "743270"
  },
  {
    "text": "or square root of 2\non an amplitude ratio. So let's actually go\nback to what I showed you",
    "start": "743270",
    "end": "750050"
  },
  {
    "text": "on the previous slide. So here, for instance,\nis an SNR of 0.4 dB.",
    "start": "750050",
    "end": "756470"
  },
  {
    "text": "If I figure that\nthat's close to 0 dB, then I should expect that\nthe noise power and signal",
    "start": "756470",
    "end": "761660"
  },
  {
    "text": "power are about equal, and\nthe noise amplitude and signal amplitude are about equal.",
    "start": "761660",
    "end": "767400"
  },
  {
    "text": "So what I expect to\nsee is perturbations of the original signal\nthat are comparable with the signal\nvalues themselves,",
    "start": "767400",
    "end": "773750"
  },
  {
    "text": "and that's sort of\nwhat we see here. The shape of the signal is\npretty distorted at this point",
    "start": "773750",
    "end": "779180"
  },
  {
    "text": "because the typical\namplitude of the noise sample is comparable with the signal\nsample that I'm interested in.",
    "start": "779180",
    "end": "786460"
  },
  {
    "text": "OK, so when you\nget to 0 dB, you're starting to get quite\ndisturbed-looking waveforms.",
    "start": "786460",
    "end": "792529"
  },
  {
    "text": "When you have 20 dB in\npower, that's actually 100-- ratio of 100--\nsorry, what is that?",
    "start": "792530",
    "end": "798777"
  },
  {
    "text": "Yeah, that's a ratio\nof 100, isn't it? On par? So it's a ratio of\n10 on amplitudes,",
    "start": "798777",
    "end": "804300"
  },
  {
    "text": "and that's what you're seeing. The noise excursions\nare about a 10th of what the signal amplitudes are.",
    "start": "804300",
    "end": "811625"
  },
  {
    "text": "All right. It takes a little getting used\nto, but it's fairly standard.",
    "start": "811625",
    "end": "818050"
  },
  {
    "text": "OK. So now we want to figure\nout how to describe noise and work with it. ",
    "start": "818050",
    "end": "824780"
  },
  {
    "text": "So let's look at a typical\nrun of a noise sequence. What I've done is just\nextracted the noise piece",
    "start": "824780",
    "end": "833380"
  },
  {
    "text": "of a typical received signal. So it's got excursions\nabove and below 0. Remember, I said it was a zero\nmean random variable that we're",
    "start": "833380",
    "end": "840550"
  },
  {
    "text": "thinking of, zero mean noise. And you can describe how these\nvalues are distributed by just",
    "start": "840550",
    "end": "848080"
  },
  {
    "start": "845000",
    "end": "845000"
  },
  {
    "text": "doing a simple histogram. And if you only take a few\nvalues like 100 samples,",
    "start": "848080",
    "end": "853120"
  },
  {
    "text": "you get a pretty\nmessy-looking histogram, it doesn't seem to\nhave much structure.",
    "start": "853120",
    "end": "858167"
  },
  {
    "text": "But as you take more\nand more samples, you'll typically find that the\nhistogram actually settles out to a nice shape, to some\nsubtle kind of shape.",
    "start": "858167",
    "end": "867980"
  },
  {
    "text": "Normalizing this to\nhave unit area under it gives you what's called\nthe probability density function for the noise.",
    "start": "867980",
    "end": "874430"
  },
  {
    "text": "So this is a term-- kind of notion that's critical\nin working with noise.",
    "start": "874430",
    "end": "880040"
  },
  {
    "text": "So here's a step\nof idealization. We're stepping back from\nthinking about histograms to just a mathematical\nway of talking",
    "start": "880040",
    "end": "886990"
  },
  {
    "start": "882000",
    "end": "882000"
  },
  {
    "text": "about how random quantities\ndistribute themselves. So we'll talk about-- by the way, we've been\nusing W for the noise",
    "start": "886990",
    "end": "895300"
  },
  {
    "text": "and X for the signal, but if\nyou look in probability books, the first variable that people--\nthe first symbol people reach",
    "start": "895300",
    "end": "901150"
  },
  {
    "text": "for and they want to talk\nabout a random variable is X, and I got stuck with a\nwhole bunch of figures that had X in them, so I didn't\nwant to change it to W.",
    "start": "901150",
    "end": "908830"
  },
  {
    "text": "This is anything. We're going to\napply it to our W, but for now it's some capital\nX. The other convention",
    "start": "908830",
    "end": "915080"
  },
  {
    "text": "when you talk about\nrandom variables as you tend to use a\ncapital letter to denote the random variable.",
    "start": "915080",
    "end": "921010"
  },
  {
    "text": "OK. So we say that X is a\nrandom variable governed by a particular probability\ndensity function.",
    "start": "921010",
    "end": "927550"
  },
  {
    "text": "If you can compute\nthe probability that X lies in some\nparticular interval by taking the corresponding\narea under that PDF.",
    "start": "927550",
    "end": "935529"
  },
  {
    "text": "So the PDF is the object\nthat gives you probabilities from areas under the integrals.",
    "start": "935530",
    "end": "941300"
  },
  {
    "text": "So if you want the probability\nthat the quantity X, take the numerical values\nin this range, X1 to X2,",
    "start": "941300",
    "end": "947380"
  },
  {
    "text": "then you integrate\nthe PDF from X1 to X2, and this area is what you call--",
    "start": "947380",
    "end": "953320"
  },
  {
    "text": "that area is the probability. And the total area under\nthe PDF, of course,",
    "start": "953320",
    "end": "958420"
  },
  {
    "text": "has to be 1 because\nthe probability that X lies somewhere is 1. The probability that X\ntakes some value is 1.",
    "start": "958420",
    "end": "967070"
  },
  {
    "text": "So this is how we\nwork with PDFs. Again, you'll find when\npeople want to sketch a PDF,",
    "start": "967070",
    "end": "973210"
  },
  {
    "text": "the reflex is to sketch one\nof these bell-shaped things. And it turns out there's\nactually a reason for that.",
    "start": "973210",
    "end": "978520"
  },
  {
    "text": " This bell-shaped thing or a\nspecific bell-shaped thing",
    "start": "978520",
    "end": "984040"
  },
  {
    "text": "called the Gaussian\ntends to arise in all sorts of\napplications, and that's a consequence of something\ncalled the central limit",
    "start": "984040",
    "end": "990760"
  },
  {
    "text": "theorem. This is considered one of\nthe most important results",
    "start": "990760",
    "end": "997823"
  },
  {
    "text": "in probability theory. It actually dates back to about\nthe 1730s as a conjecture,",
    "start": "997823",
    "end": "1004290"
  },
  {
    "text": "but it was Laplace who-- in I guess the late 1700s, early\n1800s who actually proved it.",
    "start": "1004290",
    "end": "1012978"
  },
  {
    "text": "And it wasn't actually called\nthe central limit theorem until much more recently,\ntill about 1930 or so. And was called that because\nit was the limit theorem that",
    "start": "1012978",
    "end": "1020220"
  },
  {
    "text": "was central to all\nof probability, that was the thinking. So here is the\ncentral limit theorem. It says that if you\nsum up a whole bunch",
    "start": "1020220",
    "end": "1028530"
  },
  {
    "text": "of little random quantities that\nare not necessarily Gaussian, and if they each have finite\nmean and finite variance,",
    "start": "1028530",
    "end": "1036699"
  },
  {
    "text": "the sum is going to have a\ndistribution that's going to look increasingly Gaussian. So you could start,\nfor instance,",
    "start": "1036700",
    "end": "1042760"
  },
  {
    "text": "with a random variable that's\ndescribed by this triangular",
    "start": "1042760",
    "end": "1049430"
  },
  {
    "text": "PDF.  Take a whole bunch of\nrandom variables generated",
    "start": "1049430",
    "end": "1054880"
  },
  {
    "text": "according to that PDF. When I say generated according\nto that PDF, what I mean is that the probability that\nyou get a value between any two",
    "start": "1054880",
    "end": "1061390"
  },
  {
    "text": "limits here is the area under\nthat piece of the triangle.",
    "start": "1061390",
    "end": "1066640"
  },
  {
    "text": "Generate a whole bunch of\nthese and sum them together, you find that the\nresulting histogram starts to look Gaussian.",
    "start": "1066640",
    "end": "1074350"
  },
  {
    "text": "You can start with another\nkind of distribution, and again, it starts\nto look Gaussian. And the more of these you add,\nthe more it looks Gaussian.",
    "start": "1074350",
    "end": "1081403"
  },
  {
    "text": "And so this can be\nactually made very precise. There's a very\nprecise sense in which the limiting distribution\nin a situation like this",
    "start": "1081403",
    "end": "1088211"
  },
  {
    "text": "is a Gaussian.  So what is a Gaussian? I've got to describe\nthat for you.",
    "start": "1088212",
    "end": "1094210"
  },
  {
    "text": "I'll do it in more\ndetail in a second.  First, let me tell you\nhow we defined these two",
    "start": "1094210",
    "end": "1102030"
  },
  {
    "start": "1099000",
    "end": "1099000"
  },
  {
    "text": "key parameters. These are things that from\nother sorts of contexts. The mean and the standard\ndeviation of the variance,",
    "start": "1102030",
    "end": "1108465"
  },
  {
    "text": "you know it from\nquiz scores at least, but here is the mathematical\ndefinition in terms of a PDF.",
    "start": "1108465",
    "end": "1114390"
  },
  {
    "text": "So if you have a PDF for a\nrandom variable capital X, the mean value of capital X is--",
    "start": "1114390",
    "end": "1122600"
  },
  {
    "text": "it's basically the\naverage value of X weighted by the probability,\nwhich is what you expect.",
    "start": "1122600",
    "end": "1128400"
  },
  {
    "text": "So it's X times\nthe PDF integrated over all possible values. That's the definition\nof the expected value.",
    "start": "1128400",
    "end": "1135060"
  },
  {
    "text": "And what we do when we\ntake the expected value of the mean value on a quiz is\na sort of discrete time version",
    "start": "1135060",
    "end": "1141240"
  },
  {
    "text": "of this. So we're seeing how many\npeople in a particular bin and multiply by the score\nfor the people in that bin",
    "start": "1141240",
    "end": "1148510"
  },
  {
    "text": "and sum over all possible bins. That's one way to think\nof what this is doing, assuming you've got the right\nnormalization of the PDF.",
    "start": "1148510",
    "end": "1155590"
  },
  {
    "text": " And the variance is the\nexpected squared deviation",
    "start": "1155590",
    "end": "1162200"
  },
  {
    "text": "from the mean value. So here's a deviation\nfrom the mean value. You square it, and now you want\nto take its expected value,",
    "start": "1162200",
    "end": "1170070"
  },
  {
    "text": "so you weight it by the\nPDF of X and that gives you the variance. So the variance is the\nexpected squared deviation",
    "start": "1170070",
    "end": "1177230"
  },
  {
    "text": "from the mean value. OK, so the PDF is valuable\nin getting all of this.",
    "start": "1177230",
    "end": "1182750"
  },
  {
    "text": " And to get a sense of what\nmeans and standard deviations",
    "start": "1182750",
    "end": "1191150"
  },
  {
    "text": "and variances do-- I don't know if I said from\nthe previous slide, by the way, that standard deviation is the\nsquare root of the variance.",
    "start": "1191150",
    "end": "1197120"
  },
  {
    "text": "Did I say that? Maybe not. But I have it at the\nbottom of the slide, right?",
    "start": "1197120",
    "end": "1202470"
  },
  {
    "text": "OK.  OK.",
    "start": "1202470",
    "end": "1208050"
  },
  {
    "text": "So shifting the mean\nof a random variable, if I define a new random\nvariable with the same PPF",
    "start": "1208050",
    "end": "1215127"
  },
  {
    "text": "except for a different\nmean, what that means is that-- what that signifies\nis that the PDF has just shifted over by that amount.",
    "start": "1215127",
    "end": "1221040"
  },
  {
    "text": "So changing the mean\nand nothing else will just shift the PDF over\nto the corresponding position.",
    "start": "1221040",
    "end": "1228029"
  },
  {
    "text": "Changing the variance from a\nsmall value to a large value will spread out the\nPDF because you're",
    "start": "1228030",
    "end": "1233240"
  },
  {
    "text": "the variance is capturing the\nexpected squared deviation from the mean. So a higher variance PDF has\ngot to have a larger spread.",
    "start": "1233240",
    "end": "1242040"
  },
  {
    "text": "But because the areas\nnormalized to 1, if it spreads out this way,\nit's got to come down on top, and that's what\nyou're seeing here.",
    "start": "1242040",
    "end": "1248760"
  },
  {
    "text": "All these pictures\nactually turn out to be drawn for the\nGaussian, but my statements are more general here.",
    "start": "1248760",
    "end": "1256028"
  },
  {
    "start": "1255000",
    "end": "1255000"
  },
  {
    "text": "But here's the Gaussian itself.  So now I'm going\nback to my notation",
    "start": "1256028",
    "end": "1262300"
  },
  {
    "text": "W. We're going to think\nof a random variable W which is going to be typical\nof all my noise samples.",
    "start": "1262300",
    "end": "1269997"
  },
  {
    "text": "It's going to have some\nmean which we'll be taking to be 0 and our examples.",
    "start": "1269997",
    "end": "1275049"
  },
  {
    "text": "It's got a variance\nsigma squared. So if a random variable\nhas this particular PDF,",
    "start": "1275050",
    "end": "1281860"
  },
  {
    "text": "we call it Gaussian. That's the definition of a\nGaussian random variable. The number here,\nwhile you've got",
    "start": "1281860",
    "end": "1287830"
  },
  {
    "text": "to remember it at some\npoint, but all it's doing is normalizing to unit area.",
    "start": "1287830",
    "end": "1294740"
  },
  {
    "text": "So the key thing\nabout a Gaussian is that it's an exponential\nwith a negative sign there",
    "start": "1294740",
    "end": "1300730"
  },
  {
    "text": "of the squared deviation\nfrom the mean normalized by the variance with that\nextra factor 2 there.",
    "start": "1300730",
    "end": "1306490"
  },
  {
    "text": " So different choices of variance\nwill give you different shapes",
    "start": "1306490",
    "end": "1314790"
  },
  {
    "text": "here. So the smaller\nvariances correspond to the more peaked and\nmore sharply-falling PDFs.",
    "start": "1314790",
    "end": "1321049"
  },
  {
    "text": " So let's see. How many standard deviations\naway from the mean",
    "start": "1321050",
    "end": "1326620"
  },
  {
    "text": "you have to go before you\nhave very low probability of reaching there? ",
    "start": "1326620",
    "end": "1336430"
  },
  {
    "text": "Anyone? There's no unique answer\nto this, but yeah? AUDIENCE: 3? PROFESSOR: 3 is not about idea.",
    "start": "1336430",
    "end": "1342820"
  },
  {
    "text": "So let's see. Let's take sigma\nsquared equals 1. That's variance of 1, so\nthe standard deviation is 1.",
    "start": "1342820",
    "end": "1348950"
  },
  {
    "text": "So for the red\ntrace, by the time we get out to the number 3,\nwe expect to actually see",
    "start": "1348950",
    "end": "1355990"
  },
  {
    "text": "a very low value for the PDF. So 3 sounds about right. Does that hold up\nfor the blue one?",
    "start": "1355990",
    "end": "1362320"
  },
  {
    "text": "Sigma squared is 0.25. So the square root of that\nis a standard deviation,",
    "start": "1362320",
    "end": "1367970"
  },
  {
    "text": "which is 0.5, so 3 times that. So when we get out to about 1.5,\nwe should be essentially at 0.",
    "start": "1367970",
    "end": "1374440"
  },
  {
    "text": "So don't forget the square root. The other thing-- actually, I\nshould have commented on this",
    "start": "1374440",
    "end": "1380320"
  },
  {
    "text": "earlier, let me show it to you-- on this slide that I\nhad, I labeled this arrow",
    "start": "1380320",
    "end": "1388540"
  },
  {
    "text": "here just schematically\nto show you that it's a measure of width. But the tag I put on it\nis standard deviation.",
    "start": "1388540",
    "end": "1394973"
  },
  {
    "text": "Standard deviation\nis the thing that you want to use when you\nwant to measure width on a distribution. That has the right units.",
    "start": "1394973",
    "end": "1401290"
  },
  {
    "text": "Standard deviation, the\nsquare root of variance has the same units as\nX. If X is a voltage,",
    "start": "1401290",
    "end": "1407200"
  },
  {
    "text": "the standard deviation\nis units of voltage. It would be a mistake to label\na spread here by the variance.",
    "start": "1407200",
    "end": "1412600"
  },
  {
    "text": "You want to think in terms\nof standard deviation when you're thinking\nabout spread.",
    "start": "1412600",
    "end": "1418410"
  },
  {
    "text": "So you define the variance\nand then take the square root to get the standard deviation.",
    "start": "1418410",
    "end": "1424210"
  },
  {
    "text": "OK. So for our noise in\nthis kind of setting,",
    "start": "1424210",
    "end": "1429220"
  },
  {
    "text": "in our communications\nsetting, we're going to assume that\nevery noise sample was",
    "start": "1429220",
    "end": "1434350"
  },
  {
    "text": "drawn from a Gaussian\ndistribution with zero mean. Just the same kind of\ndistribution that I showed you. So the only thing that's going\nto change from one example",
    "start": "1434350",
    "end": "1442870"
  },
  {
    "text": "to another will be the variance. But for a given case, we're\ntalking about IID noise. You're going to fix the\nvariance, have zero mean,",
    "start": "1442870",
    "end": "1450162"
  },
  {
    "text": "and all your noise\nsamples will be pulled from that same distribution. ",
    "start": "1450162",
    "end": "1456910"
  },
  {
    "text": "If you were actually looking at\ndata here for these excursions, if you were actually\nlooking at what",
    "start": "1456910",
    "end": "1463690"
  },
  {
    "text": "the excursions from the\nbaseline are, and you wanted in a numerical experiment--\nin a simulation setting,",
    "start": "1463690",
    "end": "1470530"
  },
  {
    "text": "for instance, or in\na physical experiment to get an estimate of what\nthe mean and variance are,",
    "start": "1470530",
    "end": "1475830"
  },
  {
    "start": "1471000",
    "end": "1471000"
  },
  {
    "text": "well, we've got very\nfamiliar expressions. You would take the sample\nmean or the sample variance.",
    "start": "1475830",
    "end": "1483437"
  },
  {
    "text": "The square root of\nthe sample variance would then be your estimate\nof the standard deviation. So we can come at\nthe same objects--",
    "start": "1483437",
    "end": "1491029"
  },
  {
    "text": "well, we have the PDF, which\nis the mathematical construct, but in an experimental\nsetting, this",
    "start": "1491030",
    "end": "1497040"
  },
  {
    "text": "is how you would go\nabout estimating these. And there's a whole big\ntheory of estimation that tells you whether these\nare good estimates or not",
    "start": "1497040",
    "end": "1504580"
  },
  {
    "text": "and offers alternatives,\nand we're not getting into any of that. We're staying\nclose to the basics",
    "start": "1504580",
    "end": "1509740"
  },
  {
    "text": "and close to what\nmakes sense intuitively and what's essentially\nused all over.",
    "start": "1509740",
    "end": "1515950"
  },
  {
    "text": " So now we have the\ntask at the receiver",
    "start": "1515950",
    "end": "1526150"
  },
  {
    "text": "of getting a bunch\nof samples like this and then trying to decide\nwhether what we're seeing is a reflection of a 1 or a 0.",
    "start": "1526150",
    "end": "1534640"
  },
  {
    "text": "If we had 0's sent\nfrom here, what",
    "start": "1534640",
    "end": "1540760"
  },
  {
    "text": "we're going to see after we\nreceive the noisy signal is",
    "start": "1540760",
    "end": "1546880"
  },
  {
    "text": "perturbed samples. And so we're going to look\nat a particular sample and try and decide whether in\nthat bit slot what was sent",
    "start": "1546880",
    "end": "1555580"
  },
  {
    "text": "was a 0 or a 1.  I'm going to actually use a\nscheme for illustration here",
    "start": "1555580",
    "end": "1564409"
  },
  {
    "text": "that's not the scheme\nthat I've suggested here. Here, I suggested\nsomething that's sending 0.",
    "start": "1564410",
    "end": "1569980"
  },
  {
    "text": "If I'm communicating a 0 and\nI'm sending some other voltage level when I want to communicate\na 1, I'm going between 0 and 1.",
    "start": "1569980",
    "end": "1579130"
  },
  {
    "text": "It turns out on the\nphysical channel, if you've got a transmitter\nwith a certain peak power,",
    "start": "1579130",
    "end": "1585280"
  },
  {
    "text": "you're probably better\noff using a plus V to indicate a 1 and\na minus V for a 0",
    "start": "1585280",
    "end": "1590680"
  },
  {
    "text": "because you're using that\ntransmitter at full power all the time. So you're actually trying\nto overcome the noise",
    "start": "1590680",
    "end": "1596980"
  },
  {
    "text": "as strongly as possible. So that's the scheme\nI'm going to consider. I'm going to consider\nthat when you want to signal a 1, what you're\ndoing at the transmitting end",
    "start": "1596980",
    "end": "1605650"
  },
  {
    "text": "is sending out L samples at\nplus some peak voltage Vp.",
    "start": "1605650",
    "end": "1611460"
  },
  {
    "text": "And when you want to signal a 0,\nyou send L samples at minus Vp. So this is what we refer to\nas a bipolar signaling scheme.",
    "start": "1611460",
    "end": "1621160"
  },
  {
    "start": "1621160",
    "end": "1632280"
  },
  {
    "text": "So it would be\nsomething like this. ",
    "start": "1632280",
    "end": "1641690"
  },
  {
    "text": "This is the xn. And this is what I'm\nusing to signal a 1,",
    "start": "1641690",
    "end": "1648460"
  },
  {
    "text": "and this is what I'm\nusing to signal a 0. But in terms of\nactual voltage levels,",
    "start": "1648460",
    "end": "1655440"
  },
  {
    "text": "this is minus Vp and Vp here. ",
    "start": "1655440",
    "end": "1672800"
  },
  {
    "text": "And on the receiving\nend, what I'm getting at any\nparticular samples--",
    "start": "1672800",
    "end": "1678139"
  },
  {
    "text": "so I pick one particular\nsample to look at, and when I look at that sample--\nlet's say at sample n sub j.",
    "start": "1678140",
    "end": "1685730"
  },
  {
    "text": "So maybe I'm looking\nin the j-th bit slot and I pick one particular sample\ntime, let we call that n sub j.",
    "start": "1685730",
    "end": "1692000"
  },
  {
    "text": "And I have to decide, am I\nlooking at plus Vp with noise or am I looking at\nminus Vp with noise?",
    "start": "1692000",
    "end": "1699500"
  },
  {
    "text": "That's a decision. I know the Vp's-- ",
    "start": "1699500",
    "end": "1704527"
  },
  {
    "text": "assume that we've taken\ncare of the scaling and so on across the channel. And I know the\ncharacteristics of the noise.",
    "start": "1704527",
    "end": "1710779"
  },
  {
    "text": "I know that the noise samples\nare Gaussian, zero mean, and some variance. ",
    "start": "1710780",
    "end": "1719460"
  },
  {
    "text": "So if I draw a picture\nthat's turned sideways here in terms of the\nreceived signal, let's see.",
    "start": "1719460",
    "end": "1730510"
  },
  {
    "text": " I might get something\ncentered around",
    "start": "1730510",
    "end": "1735820"
  },
  {
    "text": "minus Vp or something\ncentered around Vp.",
    "start": "1735820",
    "end": "1741009"
  },
  {
    "text": "If a minus Vp was sent, then\nit's got a noise added to it. The noise has a\nGaussian distribution.",
    "start": "1741010",
    "end": "1747880"
  },
  {
    "text": " So this is the\ndistribution of values",
    "start": "1747880",
    "end": "1753549"
  },
  {
    "text": "I expect if a 0 was sent. So this is-- let me call it--",
    "start": "1753550",
    "end": "1760350"
  },
  {
    "text": "it's the distribution of Y-- I'm not going to put all\nthe attachments here--",
    "start": "1760350",
    "end": "1765730"
  },
  {
    "text": "if a 0 was sent. Because my shorthand notation\nfor the density of Y assuming",
    "start": "1765730",
    "end": "1773050"
  },
  {
    "text": "a 0 was sent. I haven't drawn a very good\nGaussian, but you get the idea.",
    "start": "1773050",
    "end": "1778720"
  },
  {
    "text": "And here's the distribution\nof Y if a 1 was sent.",
    "start": "1778720",
    "end": "1786740"
  },
  {
    "start": "1786740",
    "end": "1791950"
  },
  {
    "text": "So what I'm actually measuring\nis some number out here. I get some number. ",
    "start": "1791950",
    "end": "1800640"
  },
  {
    "text": "And I've got to\ndecide, did that come from having sent a 0 and\ngetting this much noise",
    "start": "1800640",
    "end": "1808260"
  },
  {
    "text": "or did it come from sending a\n1 and getting this much noise? ",
    "start": "1808260",
    "end": "1814688"
  },
  {
    "text": "That's the problem.  So if 0's and 1's\nare equally likely,",
    "start": "1814688",
    "end": "1821000"
  },
  {
    "text": "what do you think is\na sensible rule here? ",
    "start": "1821000",
    "end": "1826620"
  },
  {
    "text": "Just pick a threshold\nwhere these two cross. Threshold in the middle.",
    "start": "1826620",
    "end": "1832350"
  },
  {
    "text": "So if the sample is above the\nthreshold, you declare a 1. If it's below the\nthreshold, you declare a 0.",
    "start": "1832350",
    "end": "1839310"
  },
  {
    "text": "What if 0's and 1's\nwere not equally likely? Suppose it was much more\nlikely that you would get a 1.",
    "start": "1839310",
    "end": "1844590"
  },
  {
    "text": " And suppose we're still thinking\nin terms of threshold rules,",
    "start": "1844590",
    "end": "1849650"
  },
  {
    "text": "what might you want to do? Suppose it's much more\nlikely that we get a 1. AUDIENCE: Move the\nthreshold to --",
    "start": "1849650",
    "end": "1855690"
  },
  {
    "text": "PROFESSOR: Sorry? AUDIENCE: Move the\nthreshold to the left. PROFESSOR: Move it to the left. So you want to actually\nallow for the fact",
    "start": "1855690",
    "end": "1863130"
  },
  {
    "text": "that most of the time\nyou're getting 1's, and so you really have\nto get close to the 0",
    "start": "1863130",
    "end": "1868140"
  },
  {
    "text": "before you going to declare a 0. So your bias kind\nof gets built in. Now this is just thinking as\nan engineer what you might do.",
    "start": "1868140",
    "end": "1875679"
  },
  {
    "text": "It turns out that\nfor Gaussian noise, the optimum decision\nrule in terms of minimizing the\nprobability of error",
    "start": "1875680",
    "end": "1882330"
  },
  {
    "text": "is exactly a threshold\nrule of this kind. And the analysis will tell you\nwhere that threshold should be.",
    "start": "1882330",
    "end": "1887909"
  },
  {
    "text": "So we're not\ngetting into proving that this is the\noptimum, but it turns out with Gaussian noise, the minimum\nprobability of error decision",
    "start": "1887910",
    "end": "1894990"
  },
  {
    "text": "rule for this kind of\na hypothesis test-- this is a classic\nhypothesis test--",
    "start": "1894990",
    "end": "1901380"
  },
  {
    "text": "is to pick a threshold. Now that's not true\nnecessarily for other sorts of distributions, it's\nnot true for the settings,",
    "start": "1901380",
    "end": "1907530"
  },
  {
    "text": "but for the Gaussian it turns\nout it's what you have to do. So let's just assume\nequal prior probabilities.",
    "start": "1907530",
    "end": "1914590"
  },
  {
    "text": "So 0's and 1's come at you\nwith equal probability, and we now have to figure out\nwhat the probability of error",
    "start": "1914590",
    "end": "1921149"
  },
  {
    "text": "is. So there's a slide here\nwith some computation.",
    "start": "1921150",
    "end": "1927299"
  },
  {
    "start": "1924000",
    "end": "1924000"
  },
  {
    "text": "Let me just walk\nyou through that. We don't have to follow\nall the details and you can study it and more--",
    "start": "1927300",
    "end": "1932399"
  },
  {
    "text": "I mean, you can\nstudy it at leisure, but it's the same\npicture I showed.",
    "start": "1932400",
    "end": "1937580"
  },
  {
    "text": "OK? AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah? AUDIENCE: [? Sorry ?] [? to ?]\n[? interrupt, but I ?] have a question-- PROFESSOR: Yeah. AUDIENCE: --the Gaussian.",
    "start": "1937580",
    "end": "1943150"
  },
  {
    "text": "PROFESSOR: [? About ?]\n[INAUDIBLE]?? AUDIENCE: [INAUDIBLE]. PROFESSOR: Yeah. AUDIENCE: Is that true\nwhen the two Gaussians have different variances?",
    "start": "1943150",
    "end": "1948570"
  },
  {
    "text": "PROFESSOR: No. OK, I'm assuming-- OK,\nthe question-- the comment was that this rule of the\nthreshold being the optimum",
    "start": "1948570",
    "end": "1955980"
  },
  {
    "text": "is not necessarily true\nif the Gaussians have unequal variances.",
    "start": "1955980",
    "end": "1961980"
  },
  {
    "text": "But I'm assuming IID noise. I'm assuming Independent\nIdentically Distributed noise. So the noise samples are\ngoverned by the same Gaussian",
    "start": "1961980",
    "end": "1969000"
  },
  {
    "text": "right through, and\nthen this turns out to be the optimum rule. Thanks for catching that.",
    "start": "1969000",
    "end": "1976110"
  },
  {
    "text": "So you can imagine\nthe picture with-- suppose the noise\nis very sharply",
    "start": "1976110",
    "end": "1983670"
  },
  {
    "text": "peaked for one of these\ncases and very shallow",
    "start": "1983670",
    "end": "1990270"
  },
  {
    "text": "for the other one. So there's high variance for\nthe 1's and there's low variance for the 0's.",
    "start": "1990270",
    "end": "1996090"
  },
  {
    "text": "You might then anticipate that\nif you got a signal way over to the left here, you're\ngoing to call it a 1, not a 0.",
    "start": "1996090",
    "end": "2003440"
  },
  {
    "text": "So each case needs to be\ndealt with separately. But assuming these are\nequal variance, which",
    "start": "2003440",
    "end": "2010250"
  },
  {
    "text": "goes with the IID case,\nthis is the optimum rule. OK.",
    "start": "2010250",
    "end": "2015800"
  },
  {
    "text": "So let me just\nstep through this. What we're saying\nnow is that what's the probability of\nmaking an error?",
    "start": "2015800",
    "end": "2021613"
  },
  {
    "text": "Well, let me actually write\ndown an expression here. ",
    "start": "2021613",
    "end": "2030050"
  },
  {
    "text": "So the probability of an error-- this is the general expression.",
    "start": "2030050",
    "end": "2036130"
  },
  {
    "text": "It's the probability\nthat I send a 0-- let me just say that this is\nthe probability of sending",
    "start": "2036130",
    "end": "2044169"
  },
  {
    "text": "0 times the probability\nof declaring 1",
    "start": "2044170",
    "end": "2056210"
  },
  {
    "text": "given that 0 was sent.  And then there's the\nother possibility.",
    "start": "2056210",
    "end": "2062530"
  },
  {
    "text": "The probability that\nI sent a 1, and here's the probability of declaring\na 0 given 1 was sent.",
    "start": "2062530",
    "end": "2072169"
  },
  {
    "text": " So it turns out these\nare the only two ways you can make an error, and\nthese are mutually exclusive,",
    "start": "2072170",
    "end": "2080933"
  },
  {
    "text": "and so what you're\ndoing is adding the probabilities of the\ntwo ways of making an error. You can either have a 0\nsent, and then the question",
    "start": "2080933",
    "end": "2088169"
  },
  {
    "text": "is, what's the probability of\ndeclaring a 1 if a 0 was sent? And then you have the\ncorresponding term",
    "start": "2088170",
    "end": "2093540"
  },
  {
    "text": "on the other side. If P0 equals P1--",
    "start": "2093540",
    "end": "2098660"
  },
  {
    "text": "in other words, if\nboth of them are 0.5, this is going to be 1 minus P0.",
    "start": "2098660",
    "end": "2104720"
  },
  {
    "text": "If they're both 0.5, then\nyou can pull that out, and what you're looking at\nfor the probability of error",
    "start": "2104720",
    "end": "2110720"
  },
  {
    "text": "is just the sum of the\nareas under these two tails.",
    "start": "2110720",
    "end": "2115982"
  },
  {
    "text": "Oh sorry, not the\nsum of the areas. If these are both 0.5,\nyou pull out 0.5-- yeah.",
    "start": "2115982",
    "end": "2121260"
  },
  {
    "text": "It's the sum of those two areas. OK. So 0.5 times the sum\nof those two areas.",
    "start": "2121260",
    "end": "2128170"
  },
  {
    "text": "Well in the symmetric case,\nthese two areas are the same. The area to the right of this\nthreshold under the Gaussian",
    "start": "2128170",
    "end": "2136309"
  },
  {
    "text": "here is the probability\nof declaring a 1 given that a 0 was sent. The area under the\ntail to the left",
    "start": "2136310",
    "end": "2142010"
  },
  {
    "text": "here is the probability\nof declaring a 0 given that a 1 was sent. Those two areas are the same.",
    "start": "2142010",
    "end": "2147750"
  },
  {
    "text": "So you'll discover that\nthe probability of error is just the area under\none of these tails.",
    "start": "2147750",
    "end": "2153715"
  },
  {
    "text": "Just the area under\none of those tails. So that's all you\nhave to compute. ",
    "start": "2153715",
    "end": "2159290"
  },
  {
    "text": "So how do we do that? Well, as the area\nunder a Gaussian. We write down the Gaussian.",
    "start": "2159290",
    "end": "2165920"
  },
  {
    "text": "Let's pretend that this\nwas 0 and this was Vp. It doesn't make a difference as\nfar as the computation of areas",
    "start": "2165920",
    "end": "2172430"
  },
  {
    "text": "goes, but it makes the\nexpressions easier to write. So I'm saying that the\narea under the table",
    "start": "2172430",
    "end": "2183580"
  },
  {
    "text": "here is equal to the area\nunder the tail there.",
    "start": "2183580",
    "end": "2196810"
  },
  {
    "text": "I can do it either way. I can either center the\nGaussian at minus Vp and look at the area to the right of 0,\nor I can center the Gaussian at",
    "start": "2196810",
    "end": "2204730"
  },
  {
    "text": "0 and look at the area\nto the right of Vp. And the way the expression\nis written here,",
    "start": "2204730",
    "end": "2209930"
  },
  {
    "text": "it chooses to do\nit the second way. So what we're saying is,\nhere is the Gaussian.",
    "start": "2209930",
    "end": "2216789"
  },
  {
    "text": "It's centered at\n0, so I don't have to subtract any term off\nthat term in the numerator.",
    "start": "2216790",
    "end": "2222130"
  },
  {
    "text": "Here's the 2 sigma squared\nin the denominator. And I integrate it\nfrom Vp onwards.",
    "start": "2222130",
    "end": "2227859"
  },
  {
    "text": "There's this\nnotation introduced. Vp is square root of ES. The reason is that\nwe're thinking",
    "start": "2227860",
    "end": "2233589"
  },
  {
    "text": "in terms of the energy\nof a single sample-- or the power of a\nsingle sample, they",
    "start": "2233590",
    "end": "2239560"
  },
  {
    "text": "turn out to be the same\nthing because it's just a single sample. So it's just the notation\nthat's traditionally used.",
    "start": "2239560",
    "end": "2245060"
  },
  {
    "text": "But what we're talking\nabout as Vp there. So the area from Vp to\ninfinity under the Gaussian",
    "start": "2245060",
    "end": "2252250"
  },
  {
    "text": "with the normalization\nfactor here. Now this is not an integral you\ncan evaluate in closed form.",
    "start": "2252250",
    "end": "2259460"
  },
  {
    "text": "It is a tabulated integral. Tabulated most conveniently\nin terms of something called the error function.",
    "start": "2259460",
    "end": "2265708"
  },
  {
    "text": "And so when you work\nthrough the calculus-- and I won't show it to\nhere, it's in the book, you might do it\nin recitation, you",
    "start": "2265708",
    "end": "2271280"
  },
  {
    "text": "discover that the\nprobability of error is this error function of the\nsquare root of ES over N0.",
    "start": "2271280",
    "end": "2279079"
  },
  {
    "text": "N0's notation for\n2 sigma squared. If I translate that back to\nnotation we've been using,",
    "start": "2279080",
    "end": "2284240"
  },
  {
    "text": "it's just Vp over sigma. So the error performance,\nthe probability of error",
    "start": "2284240",
    "end": "2289670"
  },
  {
    "text": "is a function of the ratio\nof the peak amplitude on the signal to the standard\ndeviation of the noise.",
    "start": "2289670",
    "end": "2297710"
  },
  {
    "text": "That's sort of the\nsquare root of the SNR. The SNR would be\nsquare of the amplitude",
    "start": "2297710",
    "end": "2303560"
  },
  {
    "text": "to square of the\nstandard deviation. So this is the square\nroot of the SNR.",
    "start": "2303560",
    "end": "2309109"
  },
  {
    "text": "And what does this\nfunction look like? We can plot it.",
    "start": "2309110",
    "end": "2315440"
  },
  {
    "start": "2314000",
    "end": "2314000"
  },
  {
    "text": "So that's exactly\nthat computation. This is a simulation on the\ntheory overlaid on each other,",
    "start": "2315440",
    "end": "2323270"
  },
  {
    "text": "but we have 0.5. This function is called the\ncomplementary error function. The C is for complementary,\nerf is for error function,",
    "start": "2323270",
    "end": "2332250"
  },
  {
    "text": "and here's the square\nroot of ES over N0 which we had in the\nprevious expression. So you're really thinking\nof signal-to-noise ratio",
    "start": "2332250",
    "end": "2341119"
  },
  {
    "text": "along this axis in dB and\nthe probability of error",
    "start": "2341120",
    "end": "2346850"
  },
  {
    "text": "on a logarithmic\nscale down here. So as the signal-to-noise\nratio increases,",
    "start": "2346850",
    "end": "2352130"
  },
  {
    "text": "as a signal becomes more\npowerful relative to the noise, the probability of\nerror decreases.",
    "start": "2352130",
    "end": "2358670"
  },
  {
    "text": "Visually what's going on? Let's go back to this picture. ",
    "start": "2358670",
    "end": "2365680"
  },
  {
    "text": "When the noise decreases\nrelative to the signal, what's happening is that these\nGaussians are getting",
    "start": "2365680",
    "end": "2371050"
  },
  {
    "text": "more peaked and they're\npulling in more tightly, and so there's less chance\nof confusing the two cases.",
    "start": "2371050",
    "end": "2376300"
  },
  {
    "text": "So it's as simple as that. It's the separation between\nthese two levels divided",
    "start": "2376300",
    "end": "2381730"
  },
  {
    "text": "by standard deviation of\nthe noise that's really going to determine performance. How far apart are\nthese two cases",
    "start": "2381730",
    "end": "2388070"
  },
  {
    "text": "relative to the standard\ndeviation of the noise? That's the square root of\nthe signal-to-noise ratio.",
    "start": "2388070",
    "end": "2393557"
  },
  {
    "text": "That's what determines\nthe probability of error in this case. ",
    "start": "2393557",
    "end": "2399500"
  },
  {
    "text": "OK. So are we done or could\nwe be doing better?",
    "start": "2399500",
    "end": "2407140"
  },
  {
    "text": "If you think of what we did,\nwe looked at the samples in a bit slice, in a bit slot.",
    "start": "2407140",
    "end": "2413800"
  },
  {
    "text": "We took one of those\nsamples and we carried out this decision rule on it. Could we be doing\nbetter than that?",
    "start": "2413800",
    "end": "2420920"
  },
  {
    "text": "Yeah? AUDIENCE: We could look\nat one more sample? PROFESSOR: We could look\nat more than one sample. This was a little bit arbitrary.",
    "start": "2420920",
    "end": "2426700"
  },
  {
    "text": "It was conservative. Why you often do that is because\nthe number of samples in a bit",
    "start": "2426700",
    "end": "2432640"
  },
  {
    "text": "slot is small and you don't\nwant to get near the edges because you're little\nworried about the transience. You've got a long enough--",
    "start": "2432640",
    "end": "2439750"
  },
  {
    "text": "if you've got enough samples in\na bit slot and the transience have died out, then maybe\nyou can just pick out",
    "start": "2439750",
    "end": "2445120"
  },
  {
    "text": "a bigger chunk in the middle. And so that's what we're\ngoing to think to do here.",
    "start": "2445120",
    "end": "2451320"
  },
  {
    "text": "OK. So it's the same\nsetting, but we're",
    "start": "2451320",
    "end": "2456470"
  },
  {
    "text": "going to average M samples. We've got L samples per bit.",
    "start": "2456470",
    "end": "2463290"
  },
  {
    "text": "We may not be confident\ncapturing all of those were averaging because there's\nsome stuff at the edges, so let's pick M of them.",
    "start": "2463290",
    "end": "2469340"
  },
  {
    "text": "Maybe less than L. Take M of\nthem and compute the average.",
    "start": "2469340",
    "end": "2476795"
  },
  {
    "text": "And I'm doing this just\nfor one of the cases. You'd have to do the same\nthing for the minus Vp case.",
    "start": "2476795",
    "end": "2482520"
  },
  {
    "text": "So the question is, what\ndoes the average do? So why did you want\nto average them?",
    "start": "2482520",
    "end": "2487700"
  },
  {
    "text": "What was your intuition? AUDIENCE: Because that would--\nit [? would be ?] [INAUDIBLE].. PROFESSOR: OK.",
    "start": "2487700",
    "end": "2493080"
  },
  {
    "text": "So here's the key thing. If you've got independent noise\nsamples and you average them, you're going to\ndecrease the variance.",
    "start": "2493080",
    "end": "2499730"
  },
  {
    "text": "If you've got M independent\nnoise samples from an IID process, you decrease\nthe variance by M.",
    "start": "2499730",
    "end": "2505740"
  },
  {
    "text": "This doesn't hold if the noise\nsamples are not independent. In fact, if one noise sample\nequals the other, then",
    "start": "2505740",
    "end": "2514050"
  },
  {
    "text": "when you add the two,\nyou get something whose variances is four\ntimes rather than just twice.",
    "start": "2514050",
    "end": "2519960"
  },
  {
    "text": "So it's critical that\nthese be independent. So if we've got\nindependent samples--",
    "start": "2519960",
    "end": "2525960"
  },
  {
    "text": "independent noise\nsamples from one sample to the next and we\naverage them-- well, let's",
    "start": "2525960",
    "end": "2531090"
  },
  {
    "text": "just average both\nsides of this equation. We've got the average\nof Y going to be the average of these\nvalues, which is just",
    "start": "2531090",
    "end": "2536790"
  },
  {
    "text": "going to be Vp again\nbecause it's constant at Vp, plus the average of W.",
    "start": "2536790",
    "end": "2542112"
  },
  {
    "text": "Here's the other\ninteresting thing. We're not going to\ntry proving this, but it turns out\nthat the average",
    "start": "2542112",
    "end": "2549810"
  },
  {
    "text": "of a sum of independent\nGaussians is, again, Gaussian. You might believe that if you\nthink of the central limit",
    "start": "2549810",
    "end": "2555440"
  },
  {
    "text": "theorem. You think of each\nof these Gaussians being approximated by\nsums of random variables. So the sum of these\nGaussians is then",
    "start": "2555440",
    "end": "2561859"
  },
  {
    "text": "a sum of just a larger\nnumber of random variables that should still be Gaussian. So the sum of an\nindependent set of Gaussians",
    "start": "2561860",
    "end": "2568030"
  },
  {
    "text": "is, again, Gaussian. So all I need to know\nfor this average W",
    "start": "2568030",
    "end": "2573710"
  },
  {
    "text": "since it's Gaussian\nis what is its mean and what is its variance? It turns out if you\nadd up a bunch of zero",
    "start": "2573710",
    "end": "2579273"
  },
  {
    "text": "mean random variables,\nyou get something with zero mean, no surprise. And if you add--",
    "start": "2579273",
    "end": "2585200"
  },
  {
    "text": "if you take the average,\nthen the variance actually drops by that factor M.",
    "start": "2585200",
    "end": "2592353"
  },
  {
    "text": "So what you're\ngoing to do is take the average of the signal,\naverage of the noise. That shrinks the\nnoise component.",
    "start": "2592353",
    "end": "2598995"
  },
  {
    "text": "You have the same\nkind of picture but now with a higher\nsignal-to-noise ratio. Now what you've got in the\nnumerator instead of ES",
    "start": "2598995",
    "end": "2606640"
  },
  {
    "text": "is EB, which is M times ES. You're summing the\nenergies of all the samples that you've taken.",
    "start": "2606640",
    "end": "2612550"
  },
  {
    "text": "And that's what we refer to as\nEB, it's the energy of the bit. ",
    "start": "2612550",
    "end": "2620730"
  },
  {
    "text": "All right. It turns out that that has\nall sorts of implications.",
    "start": "2620730",
    "end": "2626390"
  },
  {
    "text": "You certainly want\nto be averaging if you've got this kind of\nsetting, because otherwise you're leaving all these\nsamples on the table",
    "start": "2626390",
    "end": "2633590"
  },
  {
    "text": "and not making good use of them. So if you're really\ngetting ambitious, you really want to be\nextracting all of that.",
    "start": "2633590",
    "end": "2639589"
  },
  {
    "text": " Also, if you want to maintain\nthe same error performance",
    "start": "2639590",
    "end": "2646150"
  },
  {
    "text": "and the noise\nintensity increases, then you're going to want to\nhave more samples per bit. You may want to slow\ndown your signaling rate",
    "start": "2646150",
    "end": "2652415"
  },
  {
    "text": "so you can put more\nsamples per bit. It turns out in the deep\nspace probe examples",
    "start": "2652415",
    "end": "2658460"
  },
  {
    "text": "that we've been\ntalking about, that's exactly what's happening. If you look at Voyager\n2, it was transmitting",
    "start": "2658460",
    "end": "2665420"
  },
  {
    "text": "at 115 kilobits in 1979. That's the year, I\njoined the faculty, that's a long time ago.",
    "start": "2665420",
    "end": "2672680"
  },
  {
    "text": "That was near Jupiter. Last month-- I mean, it's\ngone past Jupiter, Saturn.",
    "start": "2672680",
    "end": "2680492"
  },
  {
    "text": "The other planet\nI only like to say the Greek name of because it\ncomes out wrong when I say it. It's Ouranos.",
    "start": "2680492",
    "end": "2685550"
  },
  {
    "text": "And then Neptune. So it went past all of these.",
    "start": "2685550",
    "end": "2691340"
  },
  {
    "text": "And now it's about 9\nbillion miles away. It's twice as far away\nfrom the sun as Pluto is. But look at the\ntransmission rate now.",
    "start": "2691340",
    "end": "2697400"
  },
  {
    "text": "It's 160 bits per second. So it's greatly reduced. And the reason is that over\nthis extended interval,",
    "start": "2697400",
    "end": "2707720"
  },
  {
    "text": "the energy per sample\nthat arrives at Earth is just minuscule. I mean, it was small enough\nto begin with from Jupiter",
    "start": "2707720",
    "end": "2714850"
  },
  {
    "text": "and look at what it does now. So it's about 1,000\ntimes less in power and you've gone down 1,000\ntimes less more or less",
    "start": "2714850",
    "end": "2723290"
  },
  {
    "text": "in your signaling\nrate because you're trying to put that much\nmore time in the signal.",
    "start": "2723290",
    "end": "2730640"
  },
  {
    "text": "So these trade-offs\nare driven by trying to get the same energy\nper bit for a given noise",
    "start": "2730640",
    "end": "2737450"
  },
  {
    "text": "to maintain the performance.  As I was reading\nup on this, there",
    "start": "2737450",
    "end": "2743520"
  },
  {
    "start": "2742000",
    "end": "2742000"
  },
  {
    "text": "were little references to\nthings that went wrong. The only a handful\nof things that are listed as having gone\nwrong, but they turn out",
    "start": "2743520",
    "end": "2750333"
  },
  {
    "text": "to be related to decoding. So there was a command that was\nincorrectly decoded and kept",
    "start": "2750333",
    "end": "2757609"
  },
  {
    "text": "some heaters on for very long\nand caused some malfunction. Here was a flipped bit.",
    "start": "2757610",
    "end": "2763680"
  },
  {
    "text": "This is one of only-- these are a few of\nonly a small list of things that are listed\nas having gone wrong.",
    "start": "2763680",
    "end": "2770210"
  },
  {
    "text": "But a flipped bit\nhere caused a problem. You've got very few bits in\nthese computers to begin with.",
    "start": "2770210",
    "end": "2777230"
  },
  {
    "text": "Remember the numbers\nwe had last time. So a flipped bit\ncan cause trouble.",
    "start": "2777230",
    "end": "2782780"
  },
  {
    "text": "OK. Let's do one last piece here. ",
    "start": "2782780",
    "end": "2789380"
  },
  {
    "text": "We're going to try and be\neven less conservative. So suppose I know\nthat when a 1 is sent,",
    "start": "2789380",
    "end": "2805570"
  },
  {
    "text": "what I receive is a waveform\nof a particular type. So the piece of the response\ncorresponding to this",
    "start": "2805570",
    "end": "2812140"
  },
  {
    "text": "has some particular shape. Suppose I know that. ",
    "start": "2812140",
    "end": "2818860"
  },
  {
    "text": "OK. So nothing is constant here. This is the actual\ny of n sequence.",
    "start": "2818860",
    "end": "2824690"
  },
  {
    "text": "And then to this,\nI'm adding noise. ",
    "start": "2824690",
    "end": "2831867"
  },
  {
    "text": "So here's the thing. I've got a yn which is no longer\njust a constant plus noise,",
    "start": "2831867",
    "end": "2837650"
  },
  {
    "text": "it's some known\nprofile plus noise. That known profile\nis actually what the xn is going\nto look like when",
    "start": "2837650",
    "end": "2843200"
  },
  {
    "text": "it goes through the channel. I should perhaps have\ncalled it y0 of n, but let's stick to x0 of n.",
    "start": "2843200",
    "end": "2849680"
  },
  {
    "text": "So x0 of n is known,\nand we've got the noise. The question is, do you want\nto just be averaging or do",
    "start": "2849680",
    "end": "2857510"
  },
  {
    "text": "you want to try something else? If I've got this kind\nof signal received",
    "start": "2857510",
    "end": "2864620"
  },
  {
    "text": "and I've got the same amount\nof noise added to each sample, which of these samples\nis more trustworthy?",
    "start": "2864620",
    "end": "2870650"
  },
  {
    "text": "Which sample do you\nwant to weight more? ",
    "start": "2870650",
    "end": "2875849"
  },
  {
    "text": "I've got some amount\nof noise adding into all of these\nsamples, so there's",
    "start": "2875850",
    "end": "2881460"
  },
  {
    "text": "some standard deviations'\nworth on each of these. ",
    "start": "2881460",
    "end": "2887890"
  },
  {
    "text": "Which is the most\ntrustworthy sample here? Yeah? AUDIENCE: The one on the right?",
    "start": "2887890",
    "end": "2893610"
  },
  {
    "text": "PROFESSOR: Yeah. It's the one on the\nright because it's got the largest amplitude. By itself it has the largest\nsignal-to-noise ratio.",
    "start": "2893610",
    "end": "2899170"
  },
  {
    "text": "So if you're going to\ncombine these samples, you would think\nthat you would want to put more weight on the\nsample that was larger.",
    "start": "2899170",
    "end": "2905920"
  },
  {
    "text": "So you can actually\nformulate that analytically. So we're going to combine\nthe received samples with some set of weights an.",
    "start": "2905920",
    "end": "2913115"
  },
  {
    "text": "Here's what it's going to\ndo on the right-hand side of that equation. Again, when you take\na weighted combination",
    "start": "2913115",
    "end": "2919540"
  },
  {
    "text": "of zero mean Gaussians, as\nyou get a zero mean Gaussian. So all you need to know is\nwhat's the variance of a scaled",
    "start": "2919540",
    "end": "2928180"
  },
  {
    "text": "Gaussian? So let's see. If I have a wn having\nvariance sigma squared,",
    "start": "2928180",
    "end": "2938680"
  },
  {
    "text": "what do you think is the\nvariance of 3 times wn? ",
    "start": "2938680",
    "end": "2948930"
  },
  {
    "text": "3 times wn means the\nexcursions are scaled by 3, so what's the variance? 9.",
    "start": "2948930",
    "end": "2954310"
  },
  {
    "start": "2954310",
    "end": "2960080"
  },
  {
    "text": "So scaling by a\nparticular number scales the variance by\nthe square of that number.",
    "start": "2960080",
    "end": "2966510"
  },
  {
    "text": "So the Gaussian\nyou're adding in here has a variance which\nis sigma squared",
    "start": "2966510",
    "end": "2972049"
  },
  {
    "text": "times the sum of the W squared. Sorry. The sigma squared times\nto sum of the A squareds.",
    "start": "2972050",
    "end": "2978320"
  },
  {
    "text": "That's what the variance\nof the Gaussian is. So you can actually write a very\nsimple optimization problem.",
    "start": "2978320",
    "end": "2984950"
  },
  {
    "text": "What choice of weights maximizes\nthe signal-to-noise ratio? And you discover,\nindeed, exactly",
    "start": "2984950",
    "end": "2990890"
  },
  {
    "text": "that you're going to put the\nlargest weight on the largest sample.",
    "start": "2990890",
    "end": "2997510"
  },
  {
    "text": "And when you do that, the\nresulting signal-to-noise ratio is, again, energy of the\nsignal that was transmitted",
    "start": "2997510",
    "end": "3004540"
  },
  {
    "text": "divided by the variance. So if you do the optimum\nprocessing with this so-called",
    "start": "3004540",
    "end": "3009580"
  },
  {
    "text": "matched filtering, you're\ngoing to get to energy of the sample--",
    "start": "3009580",
    "end": "3014799"
  },
  {
    "start": "3012000",
    "end": "3012000"
  },
  {
    "text": "sorry, energy of the\nbit over the noise variance governing\nthe performance. So it's the bit energy\nover the noise variance",
    "start": "3014800",
    "end": "3022420"
  },
  {
    "text": "that's going to\ndetermine performance provided you milk that bit\nslot for everything it's",
    "start": "3022420",
    "end": "3027700"
  },
  {
    "text": "worth by doing the\nmatch filtering. OK. We'll leave it at\nthat for today. ",
    "start": "3027700",
    "end": "3036000"
  }
]