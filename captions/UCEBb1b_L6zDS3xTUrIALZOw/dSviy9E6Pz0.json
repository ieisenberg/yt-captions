[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6590"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or to view\nadditional materials from",
    "start": "6590",
    "end": "13060"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13060",
    "end": "21570"
  },
  {
    "text": "PROFESSOR: OK. Just to review where we are,\nwe've been talking about source coding as one of the\ntwo major parts of digital",
    "start": "21570",
    "end": "30230"
  },
  {
    "text": "communication. Remember, you take sources,\nyou turn them into bits. Then you take bits and you\ntransmit them over channels.",
    "start": "30230",
    "end": "38109"
  },
  {
    "text": "And that sums up the\nwhole course. This is the part where you\ntransmit over channels.",
    "start": "38110",
    "end": "44420"
  },
  {
    "text": "This is the part where you\nprocess the sources. We're concentrating now on the\nsource side of things.",
    "start": "44420",
    "end": "51740"
  },
  {
    "text": "Partly because by concentrating\non the source side of things, we will build\nup the machinery we need to",
    "start": "51740",
    "end": "58300"
  },
  {
    "text": "look at the channel\nside of things. The channel side is really more\ninteresting, I think.",
    "start": "58300",
    "end": "63760"
  },
  {
    "text": "Although there's been a great\ndeal of work on both of them. They're both important.",
    "start": "63760",
    "end": "69070"
  },
  {
    "text": "And we said that we could\nseparate source coding into three pieces. If you start out with a waveform\nsource, the typical",
    "start": "69070",
    "end": "77030"
  },
  {
    "text": "thing to do, and almost the only\nthing to do, is to turn those waveforms into sequences\nof numbers.",
    "start": "77030",
    "end": "84430"
  },
  {
    "text": "Those sequences might\nbe samples. They might be numbers\nin an expansion.",
    "start": "84430",
    "end": "91079"
  },
  {
    "text": "They might be whatever. But the first thing you almost\nalways do is turn waveforms",
    "start": "91080",
    "end": "96439"
  },
  {
    "text": "into a sequence of numbers. Because waveforms are just too\ncomplicated to deal with.",
    "start": "96440",
    "end": "102600"
  },
  {
    "text": "The next thing we do with\na sequence of numbers is quantize them. After we quantize them\nwe wind up with a",
    "start": "102600",
    "end": "108439"
  },
  {
    "text": "finite set of symbols. And the next thing\nwe do is, we take that sequence of symbols.",
    "start": "108440",
    "end": "114270"
  },
  {
    "text": "And --  and what we do at that point\nis to do data compression.",
    "start": "114270",
    "end": "121960"
  },
  {
    "text": "So that we try to represent\nthose symbols with as small as possible a number of binary\ndigits per source symbol.",
    "start": "121960",
    "end": "129950"
  },
  {
    "text": "We want to do that in such\na way that we can receive it the other end.",
    "start": "129950",
    "end": "135760"
  },
  {
    "text": "So let's review a little bit\nabout what we've done in the last couple of lectures.",
    "start": "135760",
    "end": "142230"
  },
  {
    "text": "We talked about the\nKraft inequality. And the Kraft inequality, you\nremember, says that the",
    "start": "142230",
    "end": "150819"
  },
  {
    "text": "lengths of the codewords in any\nprefix-free code have to satisfy this funny\ninequality here.",
    "start": "150820",
    "end": "158120"
  },
  {
    "text": "And this funny inequality, in\nsome sense, says if you want to make one codeword short, by\nmaking that one codeword",
    "start": "158120",
    "end": "167550"
  },
  {
    "text": "short, it eats up a large\npart of this fraction.",
    "start": "167550",
    "end": "173590"
  },
  {
    "text": "Since this sum has to be less\nthan or equal to 1. If, for example, you make l sub\n1 equal to 1, then that",
    "start": "173590",
    "end": "181150"
  },
  {
    "text": "uses up a half of this\nsum right there. And all the other codewords\nhave to be much longer.",
    "start": "181150",
    "end": "186989"
  },
  {
    "text": "So what this is saying,\nessentially, and we proved it, and we did a bunch of things\nwith it, and your homework you",
    "start": "186990",
    "end": "193760"
  },
  {
    "text": "worked with it, we have\nshown that that inequality has to hold.",
    "start": "193760",
    "end": "200380"
  },
  {
    "text": "The next thing we did is, given\na set of probabilities on a source, for example, p1\nup to p sub m, by this time",
    "start": "200380",
    "end": "209120"
  },
  {
    "text": "you should feel very comfortable\nin realizing that what you call these symbols\ndoesn't make any difference",
    "start": "209120",
    "end": "215020"
  },
  {
    "text": "whatsoever as far\nas any matter of encoding sources is concerned.",
    "start": "215020",
    "end": "221180"
  },
  {
    "text": "The first thing you can do, if\nyou like to, is take whatever name somebody has given to a set\nof symbols, replace them",
    "start": "221180",
    "end": "227940"
  },
  {
    "text": "with your own symbols, and the\neasiest set of symbols to use is the integers 1 to m.",
    "start": "227940",
    "end": "235020"
  },
  {
    "text": "And that's what we\nwill usually do. So, given this set of\nprobabilities, and they have",
    "start": "235020",
    "end": "241820"
  },
  {
    "text": "to add up to 1, the Huffman\nalgorithm is this ingenious algorithm, very, very, clever,\nwhich constructs a prefix-free",
    "start": "241820",
    "end": "250409"
  },
  {
    "text": "code of minimum expected\nlength. And that minimum expected length\nis just defined as the",
    "start": "250410",
    "end": "258560"
  },
  {
    "text": "sum over i, of p sub\ni times l sub i. And the trick in the algorithm\nis to find that set of l sub",
    "start": "258560",
    "end": "265340"
  },
  {
    "text": "i's that satisfy this inequality\nbut minimize this",
    "start": "265340",
    "end": "270570"
  },
  {
    "text": "expected value. Next thing we started to talk\nabout was a discrete memoryless source.",
    "start": "270570",
    "end": "276789"
  },
  {
    "text": "A discrete memoryless source\nis really a toy source. It's a toy source where you\nassume that each letter in the",
    "start": "276790",
    "end": "283770"
  },
  {
    "text": "sequence is independent, and\nequally independent, and identically distributed.",
    "start": "283770",
    "end": "289820"
  },
  {
    "text": "In other words, every\nletter is the same. Every letter is independent\nof every other letter.",
    "start": "289820",
    "end": "295000"
  },
  {
    "text": "That's more appropriate for a\ngambling game than it is for real sources. But, on the other hand, by\nunderstanding this problem,",
    "start": "295000",
    "end": "302830"
  },
  {
    "text": "we're starting to see that we\nunderstand the whole problem of source coding. So we'll get on with\nthat as we go.",
    "start": "302830",
    "end": "310050"
  },
  {
    "text": "But, anyway, when we have a\ndiscrete memoryless source, what we found -- first we\ndefined the entropy of such a",
    "start": "310050",
    "end": "317780"
  },
  {
    "text": "source as h of x, which is the\nsum over i of minus p sub i,",
    "start": "317780",
    "end": "324460"
  },
  {
    "text": "of logarithm of p sub i. And that was just something that\ncame out of trying to do",
    "start": "324460",
    "end": "330569"
  },
  {
    "text": "this optimization not the way\nthat Huffman did it but the way that Shannon did it.",
    "start": "330570",
    "end": "335940"
  },
  {
    "text": "Namely, Shannon looked at this\noptimization in terms of dealing with entropy and\nthings like that.",
    "start": "335940",
    "end": "342600"
  },
  {
    "text": "Huffman dealt with it in terms\nof finding the optimal code. One of the very surprising\nthings is that the way Huffman",
    "start": "342600",
    "end": "349099"
  },
  {
    "text": "looked at it, in terms of\nentropy, is the way this really valuable, even though\nit doesn't come up with an",
    "start": "349100",
    "end": "355120"
  },
  {
    "text": "optimal code. I mean, here was poor Huffman,\nwho generated this beautiful",
    "start": "355120",
    "end": "360420"
  },
  {
    "text": "algorithm, which is\nextraordinarily simple, which solved what looked like\na hard problem.",
    "start": "360420",
    "end": "366349"
  },
  {
    "text": "But yet, as far as information\ntheory is concerned, he used that algorithm, sure.",
    "start": "366350",
    "end": "371940"
  },
  {
    "text": "But as far as all the\ngeneralizations are concerned, it has almost nothing\nto do with anything.",
    "start": "371940",
    "end": "377960"
  },
  {
    "text": "But, anyway, when you look at\nthis entropy, what comes out of it is the fact that the\nentropy of the source is less",
    "start": "377960",
    "end": "385710"
  },
  {
    "text": "than or equal to the minimum\nnumber of bits per source symbol that you can come up with\nin a prefix-free code,",
    "start": "385710",
    "end": "393840"
  },
  {
    "text": "which is less than\nh of x plus 1. And the way we did that was just\nto try to look at this",
    "start": "393840",
    "end": "399340"
  },
  {
    "text": "minimization. And by looking at the\nminimization, we usually showed it had to be greater\nthan or equal to H of x.",
    "start": "399340",
    "end": "405900"
  },
  {
    "text": "And by looking at any code\nwhich satisfied this inequality with any\nset of length --",
    "start": "405900",
    "end": "411050"
  },
  {
    "text": "well, after we looked at this,\nthis said that what we really wanted to do was to make the\nlength of each codeword minus",
    "start": "411050",
    "end": "419680"
  },
  {
    "text": "log of p sub i. That's not an integer. So the thing we did to get this\ninequality is said, OK,",
    "start": "419680",
    "end": "426139"
  },
  {
    "text": "if it's not an integer, we'll\nraise it up the next value. Make it an integer. And as soon as we do that, the\nKraft inequality is satisfied.",
    "start": "426140",
    "end": "434810"
  },
  {
    "text": "And you can generate a code\nwith that set of lengths. So that's where you get\nthese two bounds.",
    "start": "434810",
    "end": "441710"
  },
  {
    "text": "This bound here is usually\nvery, very weak. Can anybody suggest the almost\nunique example where this is",
    "start": "441710",
    "end": "451030"
  },
  {
    "text": "almost tight? It's a simplistic sample\nyou can think of.",
    "start": "451030",
    "end": "456600"
  },
  {
    "text": "It's a binary source. And what binary source has the\nproperty that this is almost",
    "start": "456600",
    "end": "464840"
  },
  {
    "text": "equal to this? ",
    "start": "464840",
    "end": "472000"
  },
  {
    "text": "Anybody out there? AUDIENCE: [UNINTELLIGIBLE]",
    "start": "472000",
    "end": "477540"
  },
  {
    "text": "PROFESSOR: Make it almost\nprobability 0 and probability 1. You can't quite do that because\nas soon as you make",
    "start": "477540",
    "end": "485390"
  },
  {
    "text": "the probability of the 0, 0,\nthen you don't have to represent it. And you just know that it's\na sequence of all 1's.",
    "start": "485390",
    "end": "493259"
  },
  {
    "text": "So you're all done. And you don't need any\nbits to represent it. But if there's just some very\ntiny epsilon probability of a",
    "start": "493260",
    "end": "501740"
  },
  {
    "text": "0 and a big probability of a 1,\nthen the entropy is almost",
    "start": "501740",
    "end": "506910"
  },
  {
    "text": "equal to 0. And this 1 here is needed\nbecause l bar min is 1.",
    "start": "506910",
    "end": "514039"
  },
  {
    "text": "You can't make it any\nsmaller than that. So, that's where that\ncomes from.",
    "start": "514040",
    "end": "522590"
  },
  {
    "text": "Let's talk about entropy\njust a little bit. If we have an alphabet which\nhas size m, which is the",
    "start": "522590",
    "end": "533450"
  },
  {
    "text": "symbol we'll usually use\nfor the alphabet, x. And the probability that\nx equals i, is p sub i.",
    "start": "533450",
    "end": "541670"
  },
  {
    "text": "In other words, again, we're\nusing this convention so we're going to call the symbols the\nintegers 1 to capital M. Then",
    "start": "541670",
    "end": "548850"
  },
  {
    "text": "the entropy is equal to this. And a nice way of representing\nthis is that the entropy is",
    "start": "548850",
    "end": "555050"
  },
  {
    "text": "equal to the expected value\nof minus the logarithm. Logarithms are always to the\nbase 2, in this course.",
    "start": "555050",
    "end": "562089"
  },
  {
    "text": "When we want natural logarithms\nwe'll write ln, in other words it's log\nto the base 2. So it's log to the base\n2 of the probability",
    "start": "562090",
    "end": "570960"
  },
  {
    "text": "of the symbol x. We call this the log pmf\nrandom variable.",
    "start": "570960",
    "end": "577629"
  },
  {
    "text": "We started out with x being\na chance variable. I mean, we happen to have\nturned it into a random",
    "start": "577630",
    "end": "584800"
  },
  {
    "text": "variable because we've\ngiven it numbers. But that's irrelevant. We really want to think of\nit as a chance variable.",
    "start": "584800",
    "end": "592050"
  },
  {
    "text": "But now this quantity is a\nnumerical function of the symbol which comes out\nof the source.",
    "start": "592050",
    "end": "599300"
  },
  {
    "text": "And, therefore, this\nquantity is a well-defined random variable. And we call it the log\npmf random variable.",
    "start": "599300",
    "end": "606690"
  },
  {
    "text": "Some people call it\nself-information. We'll find out why later. I don't particularly\nlike that word.",
    "start": "606690",
    "end": "613180"
  },
  {
    "text": "One, because what we're dealing\nwith here has nothing to do with information.",
    "start": "613180",
    "end": "618300"
  },
  {
    "text": "Probably the thought that this\nall has something to do with information, namely, that\ninformation theory has",
    "start": "618300",
    "end": "623350"
  },
  {
    "text": "something to do with\ninformation, probably held up the field for about\nfive years.",
    "start": "623350",
    "end": "628730"
  },
  {
    "text": "Because everyone tried to figure\nout, what does this have to do with information. And, of course, it had nothing\nto do with information.",
    "start": "628730",
    "end": "636050"
  },
  {
    "text": "It really only had\nto do with data. And with probabilities of\nvarious things in the data.",
    "start": "636050",
    "end": "641260"
  },
  {
    "text": "So, anyway. Some people call it\nself-information and we'll see why later.",
    "start": "641260",
    "end": "647529"
  },
  {
    "text": "But this is the quantity\nwe're interested in. And we call it log pmf,\nwe sort of forget about the minus sign.",
    "start": "647530",
    "end": "654750"
  },
  {
    "text": "It's not good to forget\nabout the minus sign, but I always do it. So I sort of expect other\npeople to do it, too.",
    "start": "654750",
    "end": "662660"
  },
  {
    "text": "One of the properties of entropy\nis, it has to be greater than or equal to 0. Why is it greater than\nor equal to 0?",
    "start": "662660",
    "end": "669050"
  },
  {
    "text": "Well, because these\nprobabilities here have to be less than or equal to 1.",
    "start": "669050",
    "end": "674120"
  },
  {
    "text": "And the logarithm of something\nless than or equal to 1 is negative. And therefore minus the\nlogarithm has to be greater",
    "start": "674120",
    "end": "681480"
  },
  {
    "text": "than or equal to 0. This entropy is also less than\nor equal to log M, log capital",
    "start": "681480",
    "end": "686970"
  },
  {
    "text": "M. I'm not going to\nprove that here. It's proven in the notes, it's\na trivial thing to do.",
    "start": "686970",
    "end": "692370"
  },
  {
    "text": "Or maybe it's proven in one\nof the problems, I forget. But, anyway, you can do it using\nthe inequality log of x",
    "start": "692370",
    "end": "699860"
  },
  {
    "text": "is less than or equal\nto x minus 1. Just like all inequalities\ncan be proven with that",
    "start": "699860",
    "end": "704890"
  },
  {
    "text": "inequality. So there's a quality here\nif x is equiprobable.",
    "start": "704890",
    "end": "712240"
  },
  {
    "text": "Which is pretty clear, because\nif all of these probabilities are equal to 1 over M, and you\ntake the expected value of",
    "start": "712240",
    "end": "720200"
  },
  {
    "text": "logarithm of M, you get\nlogarithm of M. So there's nothing very surprising here.",
    "start": "720200",
    "end": "727030"
  },
  {
    "text": "Now, the next thing -- and\nhere's where what we're going to do is, on one hand very\nsimple, and on the other hand",
    "start": "727030",
    "end": "735640"
  },
  {
    "text": "very confusing. After you get the picture of\nit, it becomes very simple.",
    "start": "735640",
    "end": "741480"
  },
  {
    "text": "Before that, it all looks\nrather strange. If you have two independent\nchance variables, say x and y,",
    "start": "741480",
    "end": "750680"
  },
  {
    "text": "then the choice where the sample\nvalue of the chance",
    "start": "750680",
    "end": "757029"
  },
  {
    "text": "variable x, and the choice of\nthe sample value y, together that's a pair of sample values\nwhich we can view as one",
    "start": "757030",
    "end": "764899"
  },
  {
    "text": "sample value. In other words, we can view xy\nas a chance variable all in",
    "start": "764900",
    "end": "771540"
  },
  {
    "text": "its own right. This isn't the sequence x,\nfollowed by y, where you can",
    "start": "771540",
    "end": "777540"
  },
  {
    "text": "think of it that way. But we want to think\nof this here as a chance variable itself.",
    "start": "777540",
    "end": "782730"
  },
  {
    "text": "Which takes on different\nvalues. And the values it takes on are\npairs of sample values, 1 from",
    "start": "782730",
    "end": "791400"
  },
  {
    "text": "x, 1 from ensemble y, and xy\ntakes on the sample value of",
    "start": "791400",
    "end": "801290"
  },
  {
    "text": "little xy with the probability\np of x times p of y.",
    "start": "801290",
    "end": "808500"
  },
  {
    "text": "As we move one with this course,\nwe'll become much less careful about putting these\nsubscripts down, which talk",
    "start": "808500",
    "end": "814520"
  },
  {
    "text": "about random variables. And the arguments which talk\nabout sample values of those",
    "start": "814520",
    "end": "820509"
  },
  {
    "text": "random variables. I want to keep doing it for a\nwhile because most courses in",
    "start": "820510",
    "end": "826500"
  },
  {
    "text": "probability, even 6.041, which\nis the first course in probability, almost deliberately\nfudges the",
    "start": "826500",
    "end": "833910"
  },
  {
    "text": "difference between sample values\nand random variables. And most people who work\nwith probability do",
    "start": "833910",
    "end": "839800"
  },
  {
    "text": "this all the time. And you never know when you're\ntalking about a random variable and when you're talking\nabout a sample value",
    "start": "839800",
    "end": "846210"
  },
  {
    "text": "of a random variable. And that's convenient for\ngetting insight about thing. And you do it for a while and\nthen pretty soon you wonder,",
    "start": "846210",
    "end": "853769"
  },
  {
    "text": "what the heck is going on. Because you have no idea what's\na random variable any",
    "start": "853770",
    "end": "859055"
  },
  {
    "text": "more, and what's a sample\nvalue of it. So, this entropy, H, of the\nchance variable, xy, is then",
    "start": "859055",
    "end": "868190"
  },
  {
    "text": "expected value of minus the\nlogarithm of the probability of the chance variable, xy.",
    "start": "868190",
    "end": "877480"
  },
  {
    "text": "Mainly, when you take the\nexpected value, you're taking the expected value of\na random variable.",
    "start": "877480",
    "end": "883610"
  },
  {
    "text": "And for the random variable\nhere, in the chance variables,",
    "start": "883610",
    "end": "889750"
  },
  {
    "text": "xy and here. This is the expected value of\nminus the logarithm of p of x",
    "start": "889750",
    "end": "895180"
  },
  {
    "text": "times the probability p of x. Which is the expected value. Since they're independent of\neach other it's the sum.",
    "start": "895180",
    "end": "903320"
  },
  {
    "text": "And that gives you H of x y is\nequal to H of x plus H of y.",
    "start": "903320",
    "end": "908380"
  },
  {
    "text": "This is really the reason why\nyou're interested in these chance variables which are\nlogarithms of probabilities.",
    "start": "908380",
    "end": "914870"
  },
  {
    "text": "Because when you have\nindependent chance variables then you have the situation that\nprobabilities multiply",
    "start": "914870",
    "end": "922780"
  },
  {
    "text": "and therefore log probabilities\nadd. All of the major theorems in\nprobability theory, in",
    "start": "922780",
    "end": "929779"
  },
  {
    "text": "particular the law of large\nnumbers, which is the most important result in probability,\nsimple though it",
    "start": "929780",
    "end": "935700"
  },
  {
    "text": "might be, that's the key to\neverything in probability. That particular result talks\nabout sums of random variables",
    "start": "935700",
    "end": "944700"
  },
  {
    "text": "and not about products\nof random variables. So that's why Shannon did\neverything in terms",
    "start": "944700",
    "end": "950400"
  },
  {
    "text": "of these log PMF. And we will soon be doing\neverything in",
    "start": "950400",
    "end": "955720"
  },
  {
    "text": "terms of log PMF also. ",
    "start": "955720",
    "end": "961120"
  },
  {
    "text": "So now let's get back to\ndiscrete memoryless sources.",
    "start": "961120",
    "end": "966710"
  },
  {
    "text": "If you now have a block of n\nchance variables, x1 to xn,",
    "start": "966710",
    "end": "972710"
  },
  {
    "text": "and they're all IID, again we\ncan do this whole block as one",
    "start": "972710",
    "end": "978500"
  },
  {
    "text": "big monster chance variable. If each one of these takes on\nm possible values, then this",
    "start": "978500",
    "end": "987100"
  },
  {
    "text": "monster chance variable\ncan take on m to the n possible values.",
    "start": "987100",
    "end": "992400"
  },
  {
    "text": "Namely, each possible string\nof symbols, each possible string of n symbols where each\none is a choice from the",
    "start": "992400",
    "end": "1001060"
  },
  {
    "text": "integers 1 to capital M. So\nwe're talking about tuples of numbers now.",
    "start": "1001060",
    "end": "1006420"
  },
  {
    "text": "And those are the values that\nthis particular chance variable, x sub n, takes on.",
    "start": "1006420",
    "end": "1012420"
  },
  {
    "text": "So it takes on these\nprobabilities, takes on these values with the probability p\nof x n is the product from i",
    "start": "1012420",
    "end": "1022145"
  },
  {
    "text": "equals 1 to n, of the individual\nprobabilities. In other words, again, when you\nhave independent chance",
    "start": "1022145",
    "end": "1028569"
  },
  {
    "text": "variables, the probabilities\nmultiply. Which is all I'm saying here. So the chance variable x sub\nn has the entropy H of x n,",
    "start": "1028570",
    "end": "1037730"
  },
  {
    "text": "which is the expected value of\nminus the logarithm of that probability. Which is the expected value of\nminus the logarithm of the",
    "start": "1037730",
    "end": "1045260"
  },
  {
    "text": "product of probabilities, which\nis the expected value of the sum of minus the log of the\nprobabilities, which is n",
    "start": "1045260",
    "end": "1052590"
  },
  {
    "text": "times H of x. If you compare this with the\nprevious slide, you'll see I",
    "start": "1052590",
    "end": "1057650"
  },
  {
    "text": "haven't said anything new. This argument and this\nargument are",
    "start": "1057650",
    "end": "1064420"
  },
  {
    "text": "really exactly the same. All I did was do it for two\nchance variables first.",
    "start": "1064420",
    "end": "1069910"
  },
  {
    "text": "And then observe. But it generalizes,\nto an arbitrary number of chance variables.",
    "start": "1069910",
    "end": "1076510"
  },
  {
    "text": "You can say that it generalizes\nto an infinite number of chance\nvariables also. And in some sense it does.",
    "start": "1076510",
    "end": "1082170"
  },
  {
    "text": "And I would advise you\nnot to go there. Because you just get tangled up\nwith a lot of mathematics",
    "start": "1082170",
    "end": "1088510"
  },
  {
    "text": "that you don't need.  So the next thing is, how\ndo we fix the variable",
    "start": "1088510",
    "end": "1096409"
  },
  {
    "text": "prefix-free codes and what\ndo we gain by it? So the thing we're going to do\nnow, instead of trying to",
    "start": "1096410",
    "end": "1104170"
  },
  {
    "text": "compress one symbol at a time\nfrom the source, we're going to segment the source into\nblocks of n symbols each.",
    "start": "1104170",
    "end": "1112270"
  },
  {
    "text": "And after we segment it into\nblocks of n symbols each, we're going to encode the\nblock of n symbols.",
    "start": "1112270",
    "end": "1119620"
  },
  {
    "text": "Now, what's new there? Nothing whatsoever is new. A block of n symbols is just\na chance variable.",
    "start": "1119620",
    "end": "1128230"
  },
  {
    "text": "We know how to do optimal\nencoding of chance variables. Namely, we use the Huffman\nalgorithm.",
    "start": "1128230",
    "end": "1133430"
  },
  {
    "text": "You can do that here\non these n blocks. We also have this nice theorem,\nwhich says that the",
    "start": "1133430",
    "end": "1140520"
  },
  {
    "text": "entropy -- well, first the\nentropy of x n as n times the",
    "start": "1140520",
    "end": "1146020"
  },
  {
    "text": "entropy of x. So, in other words, when you\nhave independent identically distributed chance variables,\nthis entropy is just n times",
    "start": "1146020",
    "end": "1155559"
  },
  {
    "text": "this entropy. But the important thing\nis this result of doing the encoding.",
    "start": "1155560",
    "end": "1161180"
  },
  {
    "text": "Which is the same result\nwe had before. Namely, this is the result of\nwhat happens when you take a",
    "start": "1161180",
    "end": "1167800"
  },
  {
    "text": "set -- when you take an\nalphabet, and the alphabet can be anything whatsoever.",
    "start": "1167800",
    "end": "1173780"
  },
  {
    "text": "And you encode that alphabet in\nan optimal way, according to the probabilities of each\nsymbol within the alphabet.",
    "start": "1173780",
    "end": "1182039"
  },
  {
    "text": "And the result that you get is\nthe entropy of this big chance variable x sub n is less than\nor equal to the minimum --",
    "start": "1182040",
    "end": "1192420"
  },
  {
    "text": "well, it's less than or equal\nto the expected value of the",
    "start": "1192420",
    "end": "1197730"
  },
  {
    "text": "length of a code, whatever\ncode you have. But when you put the minimum on\nhere, this is less than the",
    "start": "1197730",
    "end": "1205090"
  },
  {
    "text": "entropy of the chance variable\nx super n plus 1. That's the same theorem\nthat we proved before.",
    "start": "1205090",
    "end": "1213270"
  },
  {
    "text": "There's nothing new here. Now, if you divide this by n,\nthis by n, and this by n, you",
    "start": "1213270",
    "end": "1220220"
  },
  {
    "text": "still have a valid inequality. When you divide this by\nn, what do you get?",
    "start": "1220220",
    "end": "1225710"
  },
  {
    "text": "You get H of x. When you divide this by n,\nby definition L bar --",
    "start": "1225710",
    "end": "1234110"
  },
  {
    "text": "what we mean is the number of\nbits per source symbol. We have n source symbols here.",
    "start": "1234110",
    "end": "1242700"
  },
  {
    "text": "So when we divide by n, we get\nthe number of bits per source symbol in this monster symbol.",
    "start": "1242700",
    "end": "1251300"
  },
  {
    "text": "So l min is equal to this. When we divide this\nby n, we get this. When we divide this by\nn, we get H of x.",
    "start": "1251300",
    "end": "1259530"
  },
  {
    "text": "And now the whole reason for\ndoing this is, this silly little 1 here, which we're\ntrying very hard to think of",
    "start": "1259530",
    "end": "1266900"
  },
  {
    "text": "as being negligible or\nunimportant, has suddenly become 1 over n.",
    "start": "1266900",
    "end": "1272090"
  },
  {
    "text": "And by making n big enough,\nthis truly is unimportant. ",
    "start": "1272090",
    "end": "1278610"
  },
  {
    "text": "If you're thinking in terms of\nencoding a binary source, this 1 here is very important.",
    "start": "1278610",
    "end": "1285000"
  },
  {
    "text": " In other words, when you're\ntrying to encode a binary",
    "start": "1285000",
    "end": "1290860"
  },
  {
    "text": "source, if you're encoding one\nletter at a time, there's nothing you can do.",
    "start": "1290860",
    "end": "1296110"
  },
  {
    "text": "You're absolutely stuck. Because if both of those\nletters have non-zero probabilities, and you want a\nuniquely decodable code, and",
    "start": "1296110",
    "end": "1304870"
  },
  {
    "text": "you want to find codewords for\neach of those two symbols, the best you can do is to have\nan expected length of 1.",
    "start": "1304870",
    "end": "1312940"
  },
  {
    "text": "Namely, you need 1 to encode 1,\nand you need 0 to encode 0.",
    "start": "1312940",
    "end": "1318529"
  },
  {
    "text": "And there's nothing else,\nthere's no freedom at all that you have. So you say, OK, in that\nsituation, I really have to go",
    "start": "1318530",
    "end": "1326929"
  },
  {
    "text": "to longer blocks. And when I go to longer\nblocks, I can get this resolved.",
    "start": "1326930",
    "end": "1332330"
  },
  {
    "text": "And I know how to do it. I use Huffman's algorithm\nor whatever. So, suddenly, I can start to\nget the expected number of",
    "start": "1332330",
    "end": "1341110"
  },
  {
    "text": "bits per source symbol. Down as close to h of x\nas I want to make it.",
    "start": "1341110",
    "end": "1347440"
  },
  {
    "text": "And I can't make\nit any smaller. Which says that H of x now has\na very clear interpretation,",
    "start": "1347440",
    "end": "1355080"
  },
  {
    "text": "at least for prefix-free codes,\nof being the number of bits you need for encoding\nprefix-free codes when you",
    "start": "1355080",
    "end": "1362430"
  },
  {
    "text": "allow the possibility\nof encoding them a block at a time.",
    "start": "1362430",
    "end": "1367570"
  },
  {
    "text": "We're going to find later today\nthat the significance of this is far greater\nthan that, even.",
    "start": "1367570",
    "end": "1373640"
  },
  {
    "text": "Because why use prefix-free\ncodes, we could use any old kind of code. When we study the Lempel-Ziv\ncodes tomorrow, we'll find out",
    "start": "1373640",
    "end": "1380840"
  },
  {
    "text": "they aren't prefix-free\ncodes at all. They're really variable length\nof variable length codes.",
    "start": "1380840",
    "end": "1387450"
  },
  {
    "text": "So they aren't fixed\nto variable length. And they do some pretty fancy\nand tricky things.",
    "start": "1387450",
    "end": "1393050"
  },
  {
    "text": "But they're still limited\nto this same inequality. You can never beat the\nentropy bound.",
    "start": "1393050",
    "end": "1398390"
  },
  {
    "text": "If you want something to be\nuniquely decodable, you're stuck with this bound. And we'll see why in a very\nstraightforward way, later.",
    "start": "1398390",
    "end": "1406620"
  },
  {
    "text": "It's a very straightforward way\nwhich I can guarantee all of you are going to look at it\nand say, yes, that's obvious.",
    "start": "1406620",
    "end": "1415960"
  },
  {
    "text": "And tomorrow you will look at it\nand say, I don't understand that at all. And the next day you'll\nlook at it again and",
    "start": "1415960",
    "end": "1421500"
  },
  {
    "text": "say, well, of course. And the day after that you'll\nsay, I don't understand that. And you'll go back and forth\nlike that for about two weeks.",
    "start": "1421500",
    "end": "1428690"
  },
  {
    "text": "Don't be frustrated, because\nit is simple. But at the same time it's\nvery sophisticated.",
    "start": "1428690",
    "end": "1434950"
  },
  {
    "text": " So, let's now review the weak\nlaw of large numbers.",
    "start": "1434950",
    "end": "1445170"
  },
  {
    "text": "I usually just call it the\nlaw of large numbers. I bridle a little bit when\npeople call it weak because,",
    "start": "1445170",
    "end": "1452670"
  },
  {
    "text": "in fact it's the centerpiece\nof probability theory. But there is this other thing\ncalled the strong law of large",
    "start": "1452670",
    "end": "1459309"
  },
  {
    "text": "numbers, which mathematicians\nlove because it lets them look at all kinds of mathematical\nminutiae.",
    "start": "1459310",
    "end": "1467630"
  },
  {
    "text": "It's also important,\nI shouldn't play it down too much. And there are places where\nyou truly need it.",
    "start": "1467630",
    "end": "1473529"
  },
  {
    "text": "For what we'll be doing, we\ndon't need it at all. And the weak law of large\nnumbers does in fact hold in",
    "start": "1473530",
    "end": "1479470"
  },
  {
    "text": "many places where the strong\nlaw doesn't hold. So if you know what the strong\nlaw, is temporarily forget it",
    "start": "1479470",
    "end": "1488130"
  },
  {
    "text": "for the for the rest\nof the term. And just focus on\nthe weak law. And the weak law is not\nterribly complicated.",
    "start": "1488130",
    "end": "1496130"
  },
  {
    "text": "We have a sequence of\nrandom variables. And each of them has\na mean y bar.",
    "start": "1496130",
    "end": "1504230"
  },
  {
    "text": "And each of them has a variance\nsigma sub y squared. And let's assume that they're\nindependent and identically",
    "start": "1504230",
    "end": "1510950"
  },
  {
    "text": "distributed for the\ntime being. Just to avoid worrying\nabout anything. If we look at the sum of those\nrandom variables, namely a is",
    "start": "1510950",
    "end": "1520020"
  },
  {
    "text": "equal to y1 up to y sub n. Then the expected value of a is\nthe expected value of this",
    "start": "1520020",
    "end": "1527570"
  },
  {
    "text": "plus the expected valuable\nof y2, and so forth. So the expected value of\na is n times y bar.",
    "start": "1527570",
    "end": "1535270"
  },
  {
    "text": "And I think in one of the\nhomework problems, you found the variance of a. And the variance of a, well, the\neasiest thing to do is to",
    "start": "1535270",
    "end": "1546090"
  },
  {
    "text": "reduce this to its\nfluctuation. Reduce all of these to\ntheir fluctuation.",
    "start": "1546090",
    "end": "1551790"
  },
  {
    "text": "Then look at the variance of the\nfluctuation, which is just the expected value\nof this squared.",
    "start": "1551790",
    "end": "1556960"
  },
  {
    "text": "Which is the expected value of\nthis squared plus the expected value of this squared,\nand so forth.",
    "start": "1556960",
    "end": "1562250"
  },
  {
    "text": "So the variance of a is n times\nsigma sub y squared. I want all of you know that.",
    "start": "1562250",
    "end": "1568529"
  },
  {
    "text": "That's sort of day two of\na probability course. As soon as you start talking\nabout random variables, that's",
    "start": "1568530",
    "end": "1575700"
  },
  {
    "text": "one of the key things\nthat you do. One of the most important\nthings you do.",
    "start": "1575700",
    "end": "1581320"
  },
  {
    "text": "The thing that we're interested\nin here is more the sample average of y1\nup to y sub n.",
    "start": "1581320",
    "end": "1586600"
  },
  {
    "text": "And the sample average,\nby definition, is the sum divided by n.",
    "start": "1586600",
    "end": "1592049"
  },
  {
    "text": "So in other words, the thing\nthat you're interested in here is to add all of these\nrandom variables up.",
    "start": "1592050",
    "end": "1599990"
  },
  {
    "text": "Take one over n times it. Which is a thing we\ndo all the time. I mean, we sum up a lot of\nevents, we divide by n, and we",
    "start": "1599990",
    "end": "1610269"
  },
  {
    "text": "hope by doing that to get some\nsort of typical value. And, usually there is some sort\nof typical value that",
    "start": "1610270",
    "end": "1618210"
  },
  {
    "text": "arises from that. What the law of large numbers\nsays is that there in fact is a typical value that arises.",
    "start": "1618210",
    "end": "1625620"
  },
  {
    "text": "So this sample value is\na over n, which is the sum divided by n. And we call that the\nsample average.",
    "start": "1625620",
    "end": "1632630"
  },
  {
    "text": "The mean of the sample average\nis just the mean of a, which",
    "start": "1632630",
    "end": "1638340"
  },
  {
    "text": "is n times y bar,\ndivided by n. So the mean of the sample\naverage is y bar itself.",
    "start": "1638340",
    "end": "1647780"
  },
  {
    "text": "The variance of the sample\nvariance, --",
    "start": "1647780",
    "end": "1657190"
  },
  {
    "text": "the variance of the sample\naverage, OK, that's, --",
    "start": "1657190",
    "end": "1662289"
  },
  {
    "text": " I'm talking too fast.",
    "start": "1662290",
    "end": "1668880"
  },
  {
    "text": "The sample average here, you\nwould like to think of it as",
    "start": "1668880",
    "end": "1675600"
  },
  {
    "text": "something which is known\nand specific, like the expected value. It, in fact, is a\nrandom variable.",
    "start": "1675600",
    "end": "1682150"
  },
  {
    "text": "It changes with different\nsample values. It can change from almost\nnothing to very large",
    "start": "1682150",
    "end": "1687840"
  },
  {
    "text": "quantities. And what we're interested in\nsaying is that most of the time, it's close to the\nexpected value.",
    "start": "1687840",
    "end": "1694480"
  },
  {
    "text": "And that's what we're\naiming at here. And that's what the law\nof large numbers says. The sample average here, the\nvariance of this, is now equal",
    "start": "1694480",
    "end": "1702970"
  },
  {
    "text": "to the variance of a divided\nby n squared.",
    "start": "1702970",
    "end": "1708000"
  },
  {
    "text": "In other words, we're trying to\ntake the expected value of this quantity squared.",
    "start": "1708000",
    "end": "1713080"
  },
  {
    "text": "So there's a 1 over n squared\nthat comes in here. When you take the 1 over n\nsquared here, this variance",
    "start": "1713080",
    "end": "1720800"
  },
  {
    "text": "then becomes sigma -- ",
    "start": "1720800",
    "end": "1726610"
  },
  {
    "text": "I don't know why I\nhave the n there. Just take that n out,\nif you will.",
    "start": "1726610",
    "end": "1732100"
  },
  {
    "text": "I don't have my red\npen with me. ",
    "start": "1732100",
    "end": "1737290"
  },
  {
    "text": "And so it's the variance\nof the random variable",
    "start": "1737290",
    "end": "1743390"
  },
  {
    "text": "y, divided by n. In other words, the limit as n\ngoes to infinity of the of the",
    "start": "1743390",
    "end": "1752169"
  },
  {
    "text": "variance of the sum is\nequal to infinity. And the variance of the sample\naverage as n goes to infinity",
    "start": "1752170",
    "end": "1761630"
  },
  {
    "text": "is equal to 0. And that's because of this 1\nover n squared effect here.",
    "start": "1761630",
    "end": "1767890"
  },
  {
    "text": "When you add up a lot of\nindependent random variables, what you wind up with is the\nsample average has a variance,",
    "start": "1767890",
    "end": "1775990"
  },
  {
    "text": "which is going to 0. And the sum has a variance which\nis going to infinity.",
    "start": "1775990",
    "end": "1784149"
  },
  {
    "text": "That's important. Aside from all of the theorems\nyou've ever heard, this is",
    "start": "1784150",
    "end": "1789820"
  },
  {
    "text": "sort of the gross, simple-minded\nthing which you always ought to keep foremost\nin your mind.",
    "start": "1789820",
    "end": "1797690"
  },
  {
    "text": "This is what's happening\nin probability theory. When you talk about sample\naverages, this variance is",
    "start": "1797690",
    "end": "1803350"
  },
  {
    "text": "getting small. Let's look at a picture\nof this.",
    "start": "1803350",
    "end": "1809710"
  },
  {
    "text": "Let's look at the distribution\nfunction of this random variable. The sample average as\na random variable.",
    "start": "1809710",
    "end": "1818110"
  },
  {
    "text": "And what we're finding here\nis that this distribution function, if we look at it for\nsome modest value of n, we get",
    "start": "1818110",
    "end": "1827509"
  },
  {
    "text": "something which looks like\nthis upper curve here. Which is then the lower\ncurve here.",
    "start": "1827510",
    "end": "1833360"
  },
  {
    "text": "It's spread out more, so it\nhas a larger variance. Namely, the sample average\nhas a larger variance.",
    "start": "1833360",
    "end": "1840179"
  },
  {
    "text": "When you make n bigger, what's\nhappening to the variance?",
    "start": "1840180",
    "end": "1845360"
  },
  {
    "text": "The variance is getting\nsmaller. The variance is getting smaller\nby a factor of 1/2.",
    "start": "1845360",
    "end": "1851850"
  },
  {
    "text": "So this quantity is supposed\nto have a variance which is equal to 1/2 of the\nvariance of this.",
    "start": "1851850",
    "end": "1859200"
  },
  {
    "text": "How you find a variance\nin a distribution function is your problem. But you know that if something\nhas a small variance, it's",
    "start": "1859200",
    "end": "1868310"
  },
  {
    "text": "very closely tucked\nin around this. In other words, as the variance\ngoes to 0, and the",
    "start": "1868310",
    "end": "1874150"
  },
  {
    "text": "mean is y bar, you have a\ndistribution function which approaches a unit step.",
    "start": "1874150",
    "end": "1880910"
  },
  {
    "text": "And all that just comes from\nthis very, very simple argument that says, when you\nhave a sum of IID random",
    "start": "1880910",
    "end": "1887260"
  },
  {
    "text": "variables and you take the\nsample average of it, namely, you divide by n, the\nvariance goes to 0.",
    "start": "1887260",
    "end": "1894780"
  },
  {
    "text": "Which says, no matter how you\nlook at it, you wind up with something that looks\nlike a unit step.",
    "start": "1894780",
    "end": "1900770"
  },
  {
    "text": "Now, the Chebyshev inequality,\nwhich is one of the simpler inequalities in probability\ntheory, and I don't prove it",
    "start": "1900770",
    "end": "1909500"
  },
  {
    "text": "because it's something\nyou've all seen. I don't know of any course in\nprobability which avoids the",
    "start": "1909500",
    "end": "1915800"
  },
  {
    "text": "Chebyshev inequality. And what it says is, for any\nepsilon greater than 0, the",
    "start": "1915800",
    "end": "1922190"
  },
  {
    "text": "probability that the difference\nbetween the sample average and the true mean, the\nprobability that that quantity",
    "start": "1922190",
    "end": "1929350"
  },
  {
    "text": "and magnitude is greater than\nor equal to epsilon, is less than or equal to sigma\nsub y squared divided",
    "start": "1929350",
    "end": "1937070"
  },
  {
    "text": "by n epsilon squared. Oh, incidentally that thing that\nwas called sigma sub n",
    "start": "1937070",
    "end": "1942340"
  },
  {
    "text": "before was really\nsigma squared. That's mainly the\nvariance of y.",
    "start": "1942340",
    "end": "1951420"
  },
  {
    "text": "I hope it's right\nin the notes. Might not be. It is? Good.",
    "start": "1951420",
    "end": "1957180"
  },
  {
    "text": " So, that's what this\ninequality says.",
    "start": "1957180",
    "end": "1962519"
  },
  {
    "text": "There's an easy way to derive\nthis on the fly. Namely, if you're wondering what\nall these constants are",
    "start": "1962520",
    "end": "1969480"
  },
  {
    "text": "here, here's a way to do it. What we're looking at, in this\ncurve here, is we're trying to",
    "start": "1969480",
    "end": "1978980"
  },
  {
    "text": "say, how much probability is\nthere outside of these plus",
    "start": "1978980",
    "end": "1984250"
  },
  {
    "text": "and minus epsilon limits. And the Chebyshev inequality\nsays there can't be too much",
    "start": "1984250",
    "end": "1989549"
  },
  {
    "text": "probability out here. And there can't be too much\nprobability out here.",
    "start": "1989550",
    "end": "1994780"
  },
  {
    "text": "So, one way to get at this is\nto say, OK, suppose I have some given probability\nout here.",
    "start": "1994780",
    "end": "2002970"
  },
  {
    "text": "And some given probability\nout here. And suppose I want to minimize\nthe variance of a random",
    "start": "2002970",
    "end": "2009380"
  },
  {
    "text": "variable which has that much\nprobability out here and that much probability out here.",
    "start": "2009380",
    "end": "2015049"
  },
  {
    "text": "How do I do it? Well, the variance deals with\nthe square of how far you were",
    "start": "2015050",
    "end": "2020700"
  },
  {
    "text": "away from the mean. So if I want to have a certain\namount of probability out here, I minimize my variance by\nmaking this come straight,",
    "start": "2020700",
    "end": "2029750"
  },
  {
    "text": "come up here with a little step,\nthen go across here. Go up here.",
    "start": "2029750",
    "end": "2036050"
  },
  {
    "text": "And then, oops. Go up here.",
    "start": "2036050",
    "end": "2042160"
  },
  {
    "text": "I wish I had my red pencil. Does anybody have a red pen? That will write on this stuff?",
    "start": "2042160",
    "end": "2048480"
  },
  {
    "start": "2048480",
    "end": "2053869"
  },
  {
    "text": "Yes? No? ",
    "start": "2053870",
    "end": "2061139"
  },
  {
    "text": "Oh, great. Thank you. I will return it. ",
    "start": "2061140",
    "end": "2067220"
  },
  {
    "text": "So what we want is something\nwhich goes over here. Comes up here.",
    "start": "2067220",
    "end": "2073169"
  },
  {
    "text": "Goes across here. Goes up here. ",
    "start": "2073170",
    "end": "2079399"
  },
  {
    "text": "Goes across here, and\ngoes up again. That's the smallest you\ncan make the variance.",
    "start": "2079400",
    "end": "2084534"
  },
  {
    "text": "It's squeezing everything\nin as far as it can be squeezed in. Namely, everything out\nhere gets squeezed",
    "start": "2084535",
    "end": "2090829"
  },
  {
    "text": "in to y minus epsilon. Everything in here gets\nsqueezed into 0.",
    "start": "2090830",
    "end": "2095909"
  },
  {
    "text": "And everything out here gets\nsqueezed into epsilon. OK, calculate the\nvariance there.",
    "start": "2095910",
    "end": "2101570"
  },
  {
    "text": "And that satisfies\nthe Chebyshev inequality with equality. So that's all the Chebyshev\ninequality is.",
    "start": "2101570",
    "end": "2110410"
  },
  {
    "text": "And it's a loose inequality\nusually, because usually these curves look very nice. Usually this looks like a\nGaussian distribution",
    "start": "2110410",
    "end": "2117410"
  },
  {
    "text": "function, and the central limit\ntheorem says that we don't need the central limit\ntheorem here, and we don't",
    "start": "2117410",
    "end": "2123809"
  },
  {
    "text": "want the central limit theorem\nhere, because this thing is an inequality that says, life can't\nbe any worse than this.",
    "start": "2123810",
    "end": "2131490"
  },
  {
    "text": "And all the central limit\ntheorem is, is an approximation. And then we have to worry\nabout when it's a good",
    "start": "2131490",
    "end": "2137550"
  },
  {
    "text": "approximation and when it's\nnot a good approximation. So this says, when we carry it\none piece further, it's for",
    "start": "2137550",
    "end": "2145160"
  },
  {
    "text": "any epsilon and delta\ngreater than 0. If we make n large enough --\nin other words, substitute",
    "start": "2145160",
    "end": "2151500"
  },
  {
    "text": "delta for this. And then, when you make n small\nenough, this quantity is smaller than delta.",
    "start": "2151500",
    "end": "2157600"
  },
  {
    "text": "And that says that the\nprobability that s and y differ by more than epsilon is\nless than or equal to delta",
    "start": "2157600",
    "end": "2165240"
  },
  {
    "text": "when we make n big enough. So it says, you can make this\nas small as you want.",
    "start": "2165240",
    "end": "2170960"
  },
  {
    "text": "You can make this as\nsmall as you want. And all you need to do\nis make a sequence which is long enough.",
    "start": "2170960",
    "end": "2177540"
  },
  {
    "text": "Now, the thing which is\nmystifying about the law of large numbers is, you\nneed both the",
    "start": "2177540",
    "end": "2184720"
  },
  {
    "text": "epsilon and the delta. You can't get rid of\neither of them. In other words, you\ncan't say --",
    "start": "2184720",
    "end": "2193260"
  },
  {
    "text": "you can't reduce this to 0. Because it won't\nmake any sense.",
    "start": "2193260",
    "end": "2198670"
  },
  {
    "text": "In other words, this\ncurve here tends to spread out on its tails.",
    "start": "2198670",
    "end": "2204520"
  },
  {
    "text": "And therefore, there's always a\nprobability of error there. You can't move epsilon into 0\nbecause, for no finite end, do",
    "start": "2204520",
    "end": "2214160"
  },
  {
    "text": "you really get a step\nfunction here. So you need some wiggle\nroom on both end.",
    "start": "2214160",
    "end": "2220580"
  },
  {
    "text": "You need wiggle room here, and\nyou need wiggle room here.",
    "start": "2220580",
    "end": "2225950"
  },
  {
    "text": "And once you recognize that you\nneed those two pieces of wiggle room. Namely, you cannot talk about\nthe probability that this is",
    "start": "2225950",
    "end": "2233830"
  },
  {
    "text": "equal to y bar, because\nthat's usually 0.",
    "start": "2233830",
    "end": "2239230"
  },
  {
    "text": "And you cannot talk about\nreducing this to 0 either.",
    "start": "2239230",
    "end": "2245390"
  },
  {
    "text": "So both of those are needed. Why did I go through\nall of this? Well, partly because\nit's important.",
    "start": "2245390",
    "end": "2251780"
  },
  {
    "text": "But partly because I want to\ntalk about something which is called the asymptotic\nequipartition property.",
    "start": "2251780",
    "end": "2259890"
  },
  {
    "text": "And because of those long words\nyou believe this has to be very complicated.",
    "start": "2259890",
    "end": "2265980"
  },
  {
    "text": "I hope to convince you that\nwhat the asymptotic equipartition property is, is\nsimply the week law of large",
    "start": "2265980",
    "end": "2272579"
  },
  {
    "text": "numbers applied to\nthe log pmf. Because that, in fact,\nis all it is.",
    "start": "2272580",
    "end": "2281030"
  },
  {
    "text": "But it says some unusual\nand fascinating things. So let's suppose that x1, x2,\nand so forth is the output",
    "start": "2281030",
    "end": "2291020"
  },
  {
    "text": "from a discrete memoryless\nsource. Look at the log pmf\nof each of these.",
    "start": "2291020",
    "end": "2298180"
  },
  {
    "text": "Namely, they each have the same\ndistribution function. So w of f x is going to be equal\nto minus the logarithm",
    "start": "2298180",
    "end": "2306090"
  },
  {
    "text": "of p sub x of x, for each of\nthese chance variables.",
    "start": "2306090",
    "end": "2311790"
  },
  {
    "text": "w of x maps source symbols\ninto real numbers. So there's a random variable,\ncapital W of x sub j, which is",
    "start": "2311790",
    "end": "2320849"
  },
  {
    "text": "a random variable. We have a random variable for\neach one of these symbols to",
    "start": "2320850",
    "end": "2326140"
  },
  {
    "text": "come out of the source. So, for each one of these\nsymbols, there's this log pmf random variable, which takes\non different values.",
    "start": "2326140",
    "end": "2335900"
  },
  {
    "text": "So the expected value of this\nlog pmf, for the j'th symbol out of the source is the sum\nof p sub x of x, namely the",
    "start": "2335900",
    "end": "2345820"
  },
  {
    "text": "probability that the source\ntakes on the value x, times minus the logarithm\nof p sub x.",
    "start": "2345820",
    "end": "2352290"
  },
  {
    "text": "And that's just the entropy. So, the strange feeling about\nthis log pmf random variable",
    "start": "2352290",
    "end": "2358770"
  },
  {
    "text": "is its expected value\nis entropy. And w of x1, w of x2, and so\nforth, are a sequence of IID",
    "start": "2358770",
    "end": "2367440"
  },
  {
    "text": "random variables, each one of\nthem which has a mean, which is the entropy.",
    "start": "2367440",
    "end": "2372570"
  },
  {
    "text": " So it's just exactly the\nsituation we had before.",
    "start": "2372570",
    "end": "2378560"
  },
  {
    "text": "Instead of y bar, we have\nthe entropy of x. And instead of the random\nvariable y sub j, we have this",
    "start": "2378560",
    "end": "2388839"
  },
  {
    "text": "random variable w of x sub j,\nwhich is defined by the symbol in an alphabet.",
    "start": "2388840",
    "end": "2394830"
  },
  {
    "start": "2394830",
    "end": "2400170"
  },
  {
    "text": "And just to review this, but\nit's what we said before, if capital X1, this little x1,\nnamely, if little x1 is the",
    "start": "2400170",
    "end": "2409900"
  },
  {
    "text": "sample value for the chance\nvariable x1 and if x2 is a",
    "start": "2409900",
    "end": "2415650"
  },
  {
    "text": "sample value for the chance\nvariable X2, then the outcome for w of x1 plus w of x2 --",
    "start": "2415650",
    "end": "2425660"
  },
  {
    "text": " very hard to keep all these\nlittle letters and capital",
    "start": "2425660",
    "end": "2431200"
  },
  {
    "text": "letters straight.  Is w of x1 plus w of x2 is minus\nthe logarithm of the",
    "start": "2431200",
    "end": "2439850"
  },
  {
    "text": "probability of x1 minus the\nlogarithm of the probability of x2, which is minus the\nlogarithm of the product,",
    "start": "2439850",
    "end": "2447790"
  },
  {
    "text": "which is minus the logarithm of\nthe joint probability of x1 and x2, which is the random\nvariable w of x1 x2.",
    "start": "2447790",
    "end": "2457610"
  },
  {
    "text": "So the sum here is the random\nvariable corresponding to log",
    "start": "2457610",
    "end": "2463870"
  },
  {
    "text": "pmf of the joint outputs\nx1 and x2.",
    "start": "2463870",
    "end": "2470650"
  },
  {
    "text": "So w of x1 x2 is the log pmf of\nthe event, but this joint",
    "start": "2470650",
    "end": "2478109"
  },
  {
    "text": "chance variable takes\non the value x1 x2. And the random variable x1 x2\nis the sum of x1 and x2.",
    "start": "2478110",
    "end": "2487820"
  },
  {
    "text": "So, what have I done here? I said this at the end of the\nlast slide, and you won't believe me.",
    "start": "2487820",
    "end": "2494050"
  },
  {
    "text": "So, again this is one of these\nthings where tomorrow you",
    "start": "2494050",
    "end": "2499690"
  },
  {
    "text": "won't believe me. And you'll have to go back\nand look at that. But, anyway, x1 x2 is\na chance variable.",
    "start": "2499690",
    "end": "2505630"
  },
  {
    "text": "And probabilities multiply in\nlog pmf's add, which is what we've been saying for a\ncouple of days now.",
    "start": "2505630",
    "end": "2512020"
  },
  {
    "text": " So. If I look at the sum of n of\nthese random variables, the",
    "start": "2512020",
    "end": "2526430"
  },
  {
    "text": "sum of these log probabilities\nis the sum of the log of pmf's, which is minus the\nlogarithm of the probability",
    "start": "2526430",
    "end": "2536370"
  },
  {
    "text": "of the entire sequence. That's just saying the same\nthing we said before, for two",
    "start": "2536370",
    "end": "2542109"
  },
  {
    "text": "random variables. The sample average of a log\npmf's is the sum of the w's",
    "start": "2542110",
    "end": "2548140"
  },
  {
    "text": "divided by n, which is minus\nthe logarithm of the probability divided by n.",
    "start": "2548140",
    "end": "2553830"
  },
  {
    "text": "The weak law of large numbers\napplies, and the probability that this sample average minus\nthe expected value of w of x",
    "start": "2553830",
    "end": "2562839"
  },
  {
    "text": "is greater than or equal to\nepsilon is less than or equal to this quantity here. This quantity is minus the\nlogarithm of the probability",
    "start": "2562840",
    "end": "2571609"
  },
  {
    "text": "of x sub n, divided by n, minus\nH of x, greater than or",
    "start": "2571610",
    "end": "2577740"
  },
  {
    "text": "equal to epsilon. ",
    "start": "2577740",
    "end": "2587210"
  },
  {
    "text": "So this is the thing that\nwe really want. ",
    "start": "2587210",
    "end": "2595609"
  },
  {
    "text": "I'm going to spend a few\nslides trying to say what this means. But let's try to just look\nat it now, and see",
    "start": "2595610",
    "end": "2602170"
  },
  {
    "text": "what it must mean. It says that with high\nprobability, this quantity is",
    "start": "2602170",
    "end": "2609349"
  },
  {
    "text": "almost the same as\nthis quantity. It says that with high\nprobability, the thing which",
    "start": "2609350",
    "end": "2615809"
  },
  {
    "text": "comes out of the source is going\nto have a probability, a",
    "start": "2615810",
    "end": "2622630"
  },
  {
    "text": "log probability, divided by n,\nwhich is close to the entropy.",
    "start": "2622630",
    "end": "2627930"
  },
  {
    "text": "It says in some sense that with\nhigh probability, the probability of what\ncomes out of the",
    "start": "2627930",
    "end": "2634240"
  },
  {
    "text": "source is almost a constant.  And that's amazing.",
    "start": "2634240",
    "end": "2642060"
  },
  {
    "text": "That's what you'll wake up in\nthe morning and say, I don't believe that. ",
    "start": "2642060",
    "end": "2647900"
  },
  {
    "text": "But it's true. But you have to be careful\nto interpret it right. ",
    "start": "2647900",
    "end": "2655450"
  },
  {
    "text": "So, we're going to define\nthe typical set. Namely, this is the typical set\nof x's, which come out of",
    "start": "2655450",
    "end": "2662680"
  },
  {
    "text": "the source. Namely, the typical\nset of blocks of n symbols out of the source.",
    "start": "2662680",
    "end": "2669490"
  },
  {
    "text": "And when you talk about a\ntypical set, you want something which includes most\nof the probability.",
    "start": "2669490",
    "end": "2676180"
  },
  {
    "text": "So what I'm going to include in\nthis typical set is all of these things that we talked\nabout before.",
    "start": "2676180",
    "end": "2683160"
  },
  {
    "text": "Namely, we showed that the\nprobability that this quantity is greater than or equal to\nepsilon is very small.",
    "start": "2683160",
    "end": "2689960"
  },
  {
    "text": "So, with high probability what\ncomes out of the source satisfies this inequality\nhere.",
    "start": "2689960",
    "end": "2697030"
  },
  {
    "text": "So I can write down the\ndistribution function of this random variable here.",
    "start": "2697030",
    "end": "2702480"
  },
  {
    "text": "It's just this w --",
    "start": "2702480",
    "end": "2709070"
  },
  {
    "text": " this is a random variable w.",
    "start": "2709070",
    "end": "2714750"
  },
  {
    "text": "I'm looking at the distribution\nof that random variable w.",
    "start": "2714750",
    "end": "2720170"
  },
  {
    "text": "And this quantity in here is\nthe probability of this",
    "start": "2720170",
    "end": "2725339"
  },
  {
    "text": "typical set. In other words, when I draw this\ndistribution function for",
    "start": "2725340",
    "end": "2731090"
  },
  {
    "text": "this combined random variable,\nI've defined this typical set to be all the sequences which\nlie between this point and",
    "start": "2731090",
    "end": "2740020"
  },
  {
    "text": "this point. Namely, this is H\nminus epsilon. And this is H plus epsilon,\nmoving H out here.",
    "start": "2740020",
    "end": "2747359"
  },
  {
    "text": "So these are all the sequences\nwhich satisfy this inequality here.",
    "start": "2747360",
    "end": "2752510"
  },
  {
    "text": "So that's what I mean\nby the typical set. It's all things which are\nclustered around H in this",
    "start": "2752510",
    "end": "2759290"
  },
  {
    "text": "distribution function.  And as n approaches infinity,\nthis typical set approaches",
    "start": "2759290",
    "end": "2767320"
  },
  {
    "text": "probability 1. In the same way that the\nlaw of large numbers behaves that way.",
    "start": "2767320",
    "end": "2772420"
  },
  {
    "text": "The probability that x sub n\nis in this typical set is",
    "start": "2772420",
    "end": "2778089"
  },
  {
    "text": "greater than or equal to 1 minus\nsigma squared divided by",
    "start": "2778090",
    "end": "2783180"
  },
  {
    "text": "n epsilon squared. ",
    "start": "2783180",
    "end": "2790800"
  },
  {
    "text": "Let's try to express that in\na bunch of other ways. ",
    "start": "2790800",
    "end": "2800400"
  },
  {
    "text": "If you're getting lost,\nplease ask questions. But I hope to come back to this\nin a little bit, after we",
    "start": "2800400",
    "end": "2809799"
  },
  {
    "text": "finish a little more\nof the story. So, another way of expressing\nthis typical set -- let me",
    "start": "2809800",
    "end": "2823060"
  },
  {
    "text": "look at that as the\ntypical set. If I take this inequality here\nand I rewrite this, namely,",
    "start": "2823060",
    "end": "2830920"
  },
  {
    "text": "this is the set of x's for\nwhich this is less than",
    "start": "2830920",
    "end": "2836190"
  },
  {
    "text": "epsilon plus H of x\nand greater than H of x minus epsilon.",
    "start": "2836190",
    "end": "2843330"
  },
  {
    "text": "So that's what I've\nexpressed here. It's the set of x's for which\nn times H of x minus epsilon",
    "start": "2843330",
    "end": "2851880"
  },
  {
    "text": "is less than this logarithm of\nprobability is great less than n times H of x plus epsilon.",
    "start": "2851880",
    "end": "2858830"
  },
  {
    "text": "Namely, I'm looking at this\nrange of epsilon around H, which is this and this.",
    "start": "2858830",
    "end": "2866980"
  },
  {
    "text": "If I write it again, if I\nexponentiate all of this, it's the set of x's for which 2 to\nthe minus n, H of x, plus",
    "start": "2866980",
    "end": "2875810"
  },
  {
    "text": "epsilon, that's this term\nexponentiated, is less than this is less than this\nterm exponentiated.",
    "start": "2875810",
    "end": "2883170"
  },
  {
    "text": "And what's going on here\nis, I've taken care of the minus sign also. And if you can follow that in\nyour head, you're a better",
    "start": "2883170",
    "end": "2890400"
  },
  {
    "text": "person than I am. But, anyway, it works. And if you fiddle around with\nthat, you'll see that that's",
    "start": "2890400",
    "end": "2896790"
  },
  {
    "text": "what it is. So this typical set is a bound\non the probabilities of all",
    "start": "2896790",
    "end": "2904300"
  },
  {
    "text": "these typical sequences. The typical sequences all are\nenclosed in this range of",
    "start": "2904300",
    "end": "2911980"
  },
  {
    "text": "probabilities.  So the typical elements are\napproximately equiprobable, in",
    "start": "2911980",
    "end": "2919440"
  },
  {
    "text": "this strange sense above. Why do I say this is\na strange sense?",
    "start": "2919440",
    "end": "2925099"
  },
  {
    "text": "Well, as n gets large,\nwhat happens here? This is 2 to the minus\nn times H of x.",
    "start": "2925100",
    "end": "2932810"
  },
  {
    "text": "Which is the important\npart of this. This epsilon here is\nmultiplied by n.",
    "start": "2932810",
    "end": "2939820"
  },
  {
    "text": "And we're trying to say, as n\ngets very, very big, we can make epsilon very, very small. But we really can't make n times\nepsilon very negligible.",
    "start": "2939820",
    "end": "2949130"
  },
  {
    "text": "But the point is, the important\nthing here is, 2 to the minus n times H of x.",
    "start": "2949130",
    "end": "2955680"
  },
  {
    "text": "So, in some sense, this\nis close to 2 to the minus n H of x.",
    "start": "2955680",
    "end": "2961750"
  },
  {
    "text": "In what sense is it true? Well, it's true in that sense.",
    "start": "2961750",
    "end": "2968140"
  },
  {
    "text": "Where that, in fact is,\na valid inequality. Namely in terms of\nsample averages,",
    "start": "2968140",
    "end": "2975130"
  },
  {
    "text": "these things are close. When I do the exponentiation and\nget rid of the n and all",
    "start": "2975130",
    "end": "2980210"
  },
  {
    "text": "that stuff, they aren't\nvery close. But saying this sort of thing is\nsort of like saying that 10",
    "start": "2980210",
    "end": "2988760"
  },
  {
    "text": "to the minus 23 is approximately\nequal to 10 to the minus 25.",
    "start": "2988760",
    "end": "2994950"
  },
  {
    "text": "And they're approximately equal\nbecause they're both very, very small. And that's the kind of thing\nthat's going on here.",
    "start": "2994950",
    "end": "3002510"
  },
  {
    "text": "And you're trying to distinguish\n10 to the minus 23 and 10 to the minus 25 from 10\nto the minus 60th and from 10",
    "start": "3002510",
    "end": "3010540"
  },
  {
    "text": "to the minus three. So that's the kind of\napproximations we're using.",
    "start": "3010540",
    "end": "3016500"
  },
  {
    "text": "Namely, we're using\napproximations on a log scale, instead of approximations\nof ordinary numbers.",
    "start": "3016500",
    "end": "3023510"
  },
  {
    "text": "But, still it's convenient to\nthink of these typical x's, typical sequences, as being\nsequences which are",
    "start": "3023510",
    "end": "3031270"
  },
  {
    "text": "constrained in probability\nin this way. And this is the thing which\nis easy to work with.",
    "start": "3031270",
    "end": "3037289"
  },
  {
    "text": "The atypical set of strings,\nnamely, the compliment to this set, the thing we know about\nthat is the entire set doesn't",
    "start": "3037290",
    "end": "3045990"
  },
  {
    "text": "have much probability. Namely, if you fix epsilon and\nyou let n get bigger and",
    "start": "3045990",
    "end": "3053080"
  },
  {
    "text": "bigger, this atypical set\nbecomes totally negligible. And you can ignore it.",
    "start": "3053080",
    "end": "3059110"
  },
  {
    "text": " So let's plow ahead.",
    "start": "3059110",
    "end": "3066940"
  },
  {
    "text": "Stop for an example pretty\nsoon, but --",
    "start": "3066940",
    "end": "3072220"
  },
  {
    "text": "If I have a sequence which is\nin the typical set, we then know that its probability is\ngreater than 2 to the minus n",
    "start": "3072220",
    "end": "3080400"
  },
  {
    "text": "times H of x plus epsilon. That's what we said before.",
    "start": "3080400",
    "end": "3086150"
  },
  {
    "text": "And, therefore, when I use\nthis inequality, the probability of x to the n, for\nsomething in the typical set,",
    "start": "3086150",
    "end": "3094900"
  },
  {
    "text": "is greater than this\nquantity here. In other words, this is\ngreater than that.",
    "start": "3094900",
    "end": "3107950"
  },
  {
    "text": "For everything in\na typical set. So now I'm heading over things\nin a typical set.",
    "start": "3107950",
    "end": "3113640"
  },
  {
    "text": "So I need to include the\nnumber of things in a typical set. So what I have is this sum.",
    "start": "3113640",
    "end": "3121190"
  },
  {
    "text": "And what is this sum? This is the probability\nof the typical set. Because I'm adding overall\nelements in the typical set.",
    "start": "3121190",
    "end": "3128960"
  },
  {
    "text": "And it's greater than or equal\nto the number of elements in a typical set times these\nsmall probabilities.",
    "start": "3128960",
    "end": "3135660"
  },
  {
    "text": "If I turn this around, it says\nthat the number of elements in a typical set is less\nthan 2 to the n",
    "start": "3135660",
    "end": "3142460"
  },
  {
    "text": "times H of x plus epsilon. For any epsilon, no matter how\nsmall I want to make it.",
    "start": "3142460",
    "end": "3150000"
  },
  {
    "text": "Which says that the elements\nin a typical set have probabilities which are about 2\nto the minus n times H of x.",
    "start": "3150000",
    "end": "3158200"
  },
  {
    "text": "And the number of them is\napproximately 2 to the n times H of x.",
    "start": "3158200",
    "end": "3164109"
  },
  {
    "text": "In other words, what it says is\nthat this typical set is a bunch of essentially uniform\nprobabilities.",
    "start": "3164110",
    "end": "3173900"
  },
  {
    "text": "So what I've done is to take\nthis very complicated source. And when I look at these very\nhumongous chance variables,",
    "start": "3173900",
    "end": "3185360"
  },
  {
    "text": "which are very large sequences\nout of the source, what I find",
    "start": "3185360",
    "end": "3190670"
  },
  {
    "text": "is that there's a bunch of\nthings which collectively have zilch probability.",
    "start": "3190670",
    "end": "3196410"
  },
  {
    "text": "There's a bunch of other things\nwhich all have equal probability. And a number of them is\nenough to add up to y.",
    "start": "3196410",
    "end": "3204650"
  },
  {
    "text": "So I have turned this source,\nwhen I look at it over a long enough sequence, into a source\nof equiprobable events.",
    "start": "3204650",
    "end": "3218080"
  },
  {
    "text": "And each of those events has\nthis probability here. Now, we know how to encode\nequiprobable events.",
    "start": "3218080",
    "end": "3226540"
  },
  {
    "text": "And that's the whole\npoint of this.  So, this is less than\nor equal to that.",
    "start": "3226540",
    "end": "3235820"
  },
  {
    "text": "On the other side, we know that\n1 minus delta is less than or equal to this\nprobability of a typical set.",
    "start": "3235820",
    "end": "3244970"
  },
  {
    "text": "And this is less than the\nnumber of elements in a typical set times 2 to the minus\nn h of x minus epsilon.",
    "start": "3244970",
    "end": "3253860"
  },
  {
    "text": "This is an upper\nbound on this. This is less than this.",
    "start": "3253860",
    "end": "3264240"
  },
  {
    "text": " So I just add all these things\nup and I get this bound.",
    "start": "3264240",
    "end": "3270570"
  },
  {
    "text": "So it says, the size of the\ntypical set is greater than 1 minus delta, times\nthis quantity.",
    "start": "3270570",
    "end": "3277359"
  },
  {
    "text": "In other words, this is a pretty\nexact sort of thing. If you don't mind dealing\nwith this 2 to the n",
    "start": "3277360",
    "end": "3284870"
  },
  {
    "text": "epsilon factor here. If you agree that that's\nnegligible in some strange",
    "start": "3284870",
    "end": "3290150"
  },
  {
    "text": "sense, the all of this\nmakes good sense. And if it is negligible, let me\nstart talking about source",
    "start": "3290150",
    "end": "3297760"
  },
  {
    "text": "coding, which is why\nthis all works out. So the summary is that the\nprobability of the complement",
    "start": "3297760",
    "end": "3305460"
  },
  {
    "text": "of the typical set\nis essentially 0.",
    "start": "3305460",
    "end": "3310650"
  },
  {
    "text": "The number of elements in a\ntypical set is approximately 2 to the n times h of x.",
    "start": "3310650",
    "end": "3316130"
  },
  {
    "text": "I'm getting rid of all the\ndeltas and epsilons here, to get sort of the broad view\nof what's important here.",
    "start": "3316130",
    "end": "3322380"
  },
  {
    "text": "Each of the elements in a\ntypical set has probability 2 to the minus n times H of x.",
    "start": "3322380",
    "end": "3328170"
  },
  {
    "text": "So I've turned a source\ninto a source of equiprobable elements.",
    "start": "3328170",
    "end": "3334230"
  },
  {
    "text": "And there are 2 to the n\ntimes h of x of them. ",
    "start": "3334230",
    "end": "3343100"
  },
  {
    "text": "Let's do an example of this. It's an example that you'll work\non more in the homework",
    "start": "3343100",
    "end": "3348890"
  },
  {
    "text": "and do it a little\nmore cleanly. Let's look at a binary discrete\nmemoryless source,",
    "start": "3348890",
    "end": "3357120"
  },
  {
    "text": "where the probability that x is\nequal to 1 is p, which is",
    "start": "3357120",
    "end": "3362310"
  },
  {
    "text": "less than 1/2. And the probability of 0\nis greater than 1/2. So, this is what you get when\nyou have a biased coin.",
    "start": "3362310",
    "end": "3372640"
  },
  {
    "text": "And the biased coin has a\n1 on one side and a 0 on the other side.",
    "start": "3372640",
    "end": "3379339"
  },
  {
    "text": "And it's more likely to\ncome up 0's than 1's. I always used to wonder how\nto make a biased coin.",
    "start": "3379340",
    "end": "3386080"
  },
  {
    "text": "And I can give you a little\nexperiment which shows you you can make a biased coin. I mean, a biased is a little\nround thing which is flat on",
    "start": "3386080",
    "end": "3394140"
  },
  {
    "text": "the top and bottom. Suppose instead of that you\nmake a triangular coin.",
    "start": "3394140",
    "end": "3400070"
  },
  {
    "text": "And instead of making it flat on\ntop and bottom, you turn it into a tetrahedron.",
    "start": "3400070",
    "end": "3405799"
  },
  {
    "text": "So in fact, what this is now is\na coin which is built up on one side into a very\nmassive thing.",
    "start": "3405800",
    "end": "3414089"
  },
  {
    "text": "And is flat on the other side. Since it's a tetrahedron\nand it's an equilateral",
    "start": "3414090",
    "end": "3419700"
  },
  {
    "text": "tetrahedron, the probability of\n1 is going to be 1/4, and",
    "start": "3419700",
    "end": "3424730"
  },
  {
    "text": "the probability of 0\nis going to be 3/4. So you can make biased coins.",
    "start": "3424730",
    "end": "3430760"
  },
  {
    "text": "So when you get into\ncoin-tossing games with people, watch the coin\nthat they're using. It probably won't be a\ntetrahedron, but anyway.",
    "start": "3430760",
    "end": "3439119"
  },
  {
    "text": " So the entropy here, the log pmf\nrandom variable, takes on",
    "start": "3439120",
    "end": "3448520"
  },
  {
    "text": "the value of minus log\np with probability p. And it takes on the value minus\nlog 1 minus p, with",
    "start": "3448520",
    "end": "3455950"
  },
  {
    "text": "probability 1 minus p. This is a probability of a 1. This is a probability of a 0.",
    "start": "3455950",
    "end": "3462700"
  },
  {
    "text": "So, the entropy is\nequal to this. Used to be that in information\ntheory courses, people would",
    "start": "3462700",
    "end": "3468980"
  },
  {
    "text": "almost memorize what this\ncurve looked like. And they'd draw pictures\nof it. There were famous curves\nof this function,",
    "start": "3468980",
    "end": "3476140"
  },
  {
    "text": "which looks like this. ",
    "start": "3476140",
    "end": "3487280"
  },
  {
    "text": "0, 1, 1.",
    "start": "3487280",
    "end": "3497620"
  },
  {
    "text": "Turns out, that's not all that\nimportant a distribution. It's a nice example\nto talk about.",
    "start": "3497620",
    "end": "3504510"
  },
  {
    "text": "The typical set, t epsilon n,\nis the set of strings with about p n1's and about 1\nminus p times n 0's.",
    "start": "3504510",
    "end": "3514710"
  },
  {
    "text": "In other words, that's the\ntypical thing to happen. And it's the typical thing in\nterms of this law of large",
    "start": "3514710",
    "end": "3521900"
  },
  {
    "text": "numbers here. Because you get 1's with\nprobability p. And therefore in a long\nsequence, you're going to get",
    "start": "3521900",
    "end": "3528700"
  },
  {
    "text": "about pn 1's and\n1 minus p 0's. The probability of a typical\nstring is, if you get a string",
    "start": "3528700",
    "end": "3538520"
  },
  {
    "text": "with this many 1's and\nthis many 0's, it's probability is p.",
    "start": "3538520",
    "end": "3544500"
  },
  {
    "text": "Namely, the probability of a 1\ntimes the number of 1's you get, which is pn.",
    "start": "3544500",
    "end": "3550610"
  },
  {
    "text": "Times the probability\nof a 0, times the number of 0's you get.",
    "start": "3550610",
    "end": "3556210"
  },
  {
    "text": "And if you look at what this\nis, if you take p up in the exponent and 1 minus the p up in\nthe exponent, this becomes",
    "start": "3556210",
    "end": "3562849"
  },
  {
    "text": "2 to the minus n times h of x,\njust like what it should be. So these typical strings, with\nabout pn 1's and 1 minus pn",
    "start": "3562850",
    "end": "3571780"
  },
  {
    "text": "0's, are in fact typical\nin the sense we've been talking about. The number of n strings with pn\n1's is n factorial divided",
    "start": "3571780",
    "end": "3583100"
  },
  {
    "text": "by pn factorial divided by n\ntimes 1 minus p factorial. ",
    "start": "3583100",
    "end": "3592070"
  },
  {
    "text": "I mean I hope you learned that\na long time ago, but you should learn it in probability\nanyway. It's just very simple\ncombinatorics.",
    "start": "3592070",
    "end": "3601260"
  },
  {
    "text": "So you have that many\ndifferent strings. So what I'm trying to get across\nhere is, there are a",
    "start": "3601260",
    "end": "3607430"
  },
  {
    "text": "bunch of different things\ngoing on here. We can talk about the random\nvariable which is the number",
    "start": "3607430",
    "end": "3613600"
  },
  {
    "text": "of 1's that occur in\nthis long sequence. And with high probability, the\nnumber of 1's that occur is",
    "start": "3613600",
    "end": "3620460"
  },
  {
    "text": "close to pn. But if pn 1's occur, there's\nstill an awful lot of",
    "start": "3620460",
    "end": "3626470"
  },
  {
    "text": "randomness left. Because we have to worry about\nwhere those pn 1's appear.",
    "start": "3626470",
    "end": "3633309"
  },
  {
    "text": "And those are the sequences\nwe're talking about. So, there are this many\nsequences, all of which have",
    "start": "3633310",
    "end": "3641519"
  },
  {
    "text": "that many 1's in them. And there's a similar number of\nsequences for all similar",
    "start": "3641520",
    "end": "3648849"
  },
  {
    "text": "numbers of 1's. Namely, if you take pn plus 1\nand pn plus 2, pn minus 1, pn",
    "start": "3648850",
    "end": "3654510"
  },
  {
    "text": "minus 2, you get similar\nnumbers here. So those are the typical\nsequences.",
    "start": "3654510",
    "end": "3660890"
  },
  {
    "text": "Now, the important thing to\nobserve here is that you really have 2 to the n binary\nstrings altogether.",
    "start": "3660890",
    "end": "3668890"
  },
  {
    "text": "And what this result is saying\nis that collectively those don't make any difference.",
    "start": "3668890",
    "end": "3674490"
  },
  {
    "text": "The law of large numbers says,\nOK, there's just a humongous number of strings.",
    "start": "3674490",
    "end": "3680080"
  },
  {
    "text": "You get the largest number\nstrings which have about half 1's and half 0's.",
    "start": "3680080",
    "end": "3685510"
  },
  {
    "text": "But their probability\nis zilch. So the thing which is probable\nis getting pn 1's",
    "start": "3685510",
    "end": "3692540"
  },
  {
    "text": "and 1 minus pn 0's. Now, we have this typical set. What is the most likely sequence\nof all, in this",
    "start": "3692540",
    "end": "3701410"
  },
  {
    "text": "experiment?  How do I maximize the\nprobability of",
    "start": "3701410",
    "end": "3708130"
  },
  {
    "text": "a particular sequence? The probability of the sequence\nis p to the i times 1",
    "start": "3708130",
    "end": "3723910"
  },
  {
    "text": "minus p to the n minus i. And 1 minus p is the\nprobability of 0.",
    "start": "3723910",
    "end": "3731049"
  },
  {
    "text": "And p is the probability\nof a 1. How do I choose i to\nmaximize this? Yeah?",
    "start": "3731050",
    "end": "3736300"
  },
  {
    "text": "AUDIENCE: [UNINTELLIGIBLE]\nall 0's. PROFESSOR: You make\nthem all 0's. So the most likely sequence\nis all 0's.",
    "start": "3736300",
    "end": "3743750"
  },
  {
    "text": "But that's not a typical\nsequence. ",
    "start": "3743750",
    "end": "3749700"
  },
  {
    "text": "Why isn't it a typical\nsequence? Because we chose to define\ntypical sequence in a",
    "start": "3749700",
    "end": "3756060"
  },
  {
    "text": "different way. Namely is only one of those, and\nthere are only n of them",
    "start": "3756060",
    "end": "3761180"
  },
  {
    "text": "with only a single one. So, in other words, what's going\non is that we have an",
    "start": "3761180",
    "end": "3766920"
  },
  {
    "text": "enormous number of sequences\nwhich have around half 1's and half 0's. ",
    "start": "3766920",
    "end": "3773430"
  },
  {
    "text": "But they don't have\nany probability. And collectively they don't\nhave any probability. We have a very small number of\nsequences which have a very",
    "start": "3773430",
    "end": "3781380"
  },
  {
    "text": "large number of 0's. But there aren't enough of those\nto make any difference.",
    "start": "3781380",
    "end": "3787960"
  },
  {
    "text": "And, therefore, the things that\nmake a difference are these typical things which\nhave about np 1's",
    "start": "3787960",
    "end": "3794710"
  },
  {
    "text": "and 1 minus pn 0's. And that all sounds\nvery strange.",
    "start": "3794710",
    "end": "3800680"
  },
  {
    "text": "But if I phrase this a different\nway, you would all say that's exactly the way\nyou ought to do things.",
    "start": "3800680",
    "end": "3807470"
  },
  {
    "text": "Because, in fact, when we look\nat very, very long sequences, you know with extraordinarily\nhigh probability what's going",
    "start": "3807470",
    "end": "3815175"
  },
  {
    "text": "to come out of the source is\nsomething with about pn 1's and about 1 minus\np times n 0's.",
    "start": "3815175",
    "end": "3822430"
  },
  {
    "text": "So that's the likely set of\nthings to have happen. And it's just that there\nare an enormous",
    "start": "3822430",
    "end": "3827589"
  },
  {
    "text": "number of those things. There are this many of them. So, here what we're dealing with\nis a balance between the",
    "start": "3827590",
    "end": "3836150"
  },
  {
    "text": "number of elements of a\nparticular type, and the probability of them.",
    "start": "3836150",
    "end": "3843520"
  },
  {
    "text": "And it turns out that this\nnumber and its probability balance out to say that usually\nwhat you get is about",
    "start": "3843520",
    "end": "3850650"
  },
  {
    "text": "pn 1's and 1 minus\np times n 0's. Which is what the law of large\nnumbers said to begin with.",
    "start": "3850650",
    "end": "3856730"
  },
  {
    "text": "All we're doing is interpreting\nthat here. But the thing that you see from\nthis example is, all of",
    "start": "3856730",
    "end": "3865210"
  },
  {
    "text": "these things with exactly pn 1's\nin them, assuming that pn is an integer, are\nall equiprobable.",
    "start": "3865210",
    "end": "3871270"
  },
  {
    "text": "They're all exactly\nequiprobable. So what we're doing when we're\ntalking about this typical",
    "start": "3871270",
    "end": "3877990"
  },
  {
    "text": "set, is first throwing out all\nthe things which have to many 1's are or too few\n1's in them.",
    "start": "3877990",
    "end": "3884569"
  },
  {
    "text": "We're keeping only the ones\nwhich are typical in the sense that they obey the law\nof large numbers.",
    "start": "3884570",
    "end": "3890920"
  },
  {
    "text": "And in this case, they obey the\nlaw of large numbers for log pmf's also.",
    "start": "3890920",
    "end": "3896730"
  },
  {
    "text": "And then all of those things\nare about equally probable.",
    "start": "3896730",
    "end": "3901770"
  },
  {
    "text": "So the idea in source coding\nis, one of the ways to deal with source coding is, you want\nto assign codewords to",
    "start": "3901770",
    "end": "3910430"
  },
  {
    "text": "only these typical things. Now, maybe you might want to\nassign codewords to something",
    "start": "3910430",
    "end": "3916240"
  },
  {
    "text": "like all 0's also. Because it hardly\ncosts anything. And a Huffman code would\ncertainly do that.",
    "start": "3916240",
    "end": "3923810"
  },
  {
    "text": "But it's not very important\nwhether you do or not. The important thing is, you\nassign codewords to all of",
    "start": "3923810",
    "end": "3930300"
  },
  {
    "text": "these typical sequences. ",
    "start": "3930300",
    "end": "3937770"
  },
  {
    "text": "So let's go back to\nfixed-to-fixed length source codes. We talked a little bit about\nfixed-to-fixed length source",
    "start": "3937770",
    "end": "3945500"
  },
  {
    "text": "codes before. Do you remember what we did\nwith fixed-to-fixed length source codes before?",
    "start": "3945500",
    "end": "3950720"
  },
  {
    "text": "We said we have an alphabet\nof size m. We want something which\nis uniquely decodable.",
    "start": "3950720",
    "end": "3956250"
  },
  {
    "text": "And since we want something\nwhich is uniquely decodable, we have to provide codewords\nfor everything.",
    "start": "3956250",
    "end": "3962510"
  },
  {
    "text": "And, therefore, if we want to\nchoose a block length of n,",
    "start": "3962510",
    "end": "3967780"
  },
  {
    "text": "we've got to generate m\nto the n codewords. Here we say, wow, maybe we\ndon't have to provide",
    "start": "3967780",
    "end": "3974700"
  },
  {
    "text": "codewords for everything. Maybe we're willing to tolerate\na certain small",
    "start": "3974700",
    "end": "3980520"
  },
  {
    "text": "probability that the whole\nthing fails and falls on its face. ",
    "start": "3980520",
    "end": "3987040"
  },
  {
    "text": "Now, does that make any sense? Well, view things the\nfollowing way.",
    "start": "3987040",
    "end": "3992330"
  },
  {
    "text": "We said, when we started out\nall of this, that we were going to look at prefix-free\ncodes.",
    "start": "3992330",
    "end": "3998880"
  },
  {
    "text": "Where some codewords had a\nlonger length and some codewords had a shorter\nlength.",
    "start": "3998880",
    "end": "4004730"
  },
  {
    "text": "And we were thinking of encoding\neither single letters at a time, or a small block\nof letters at a time.",
    "start": "4004730",
    "end": "4012340"
  },
  {
    "text": "So think of encoding, say,\n10 letters at a time. And think of doing this for\n10 to the 20th letters.",
    "start": "4012340",
    "end": "4022250"
  },
  {
    "text": "So you have the source here\nwhich is pumping out letters at a regular rate.",
    "start": "4022250",
    "end": "4028280"
  },
  {
    "text": "You're blocking them into\nn letters at a time. You're encoding in a\nprefix-free code.",
    "start": "4028280",
    "end": "4035540"
  },
  {
    "text": "Out comes something. What comes is not coming\nout at a regular right.",
    "start": "4035540",
    "end": "4042560"
  },
  {
    "text": "What is coming out, sometimes\nyou get a lot of bits out. Sometimes a small number\nof bits out.",
    "start": "4042560",
    "end": "4048450"
  },
  {
    "text": "So, in other words, if you want\nto send things over a channel, you need a buffer\nthere to save things.",
    "start": "4048450",
    "end": "4054970"
  },
  {
    "text": "If, in fact, we decide that the\nexpected number of bits per source letter is, say, five\nbits per source letter,",
    "start": "4054970",
    "end": "4063960"
  },
  {
    "text": "then we expect over a very long\ntime to be producing five bits per source letter.",
    "start": "4063960",
    "end": "4070830"
  },
  {
    "text": "And if we turn our channel on\nfor one year, to transmit all of these things, what's going\nto happen is this very",
    "start": "4070830",
    "end": "4079010"
  },
  {
    "text": "unlikely sequence occurs. Which in fact requires not one\nyear to transmit, but two",
    "start": "4079010",
    "end": "4085910"
  },
  {
    "text": "years to transmit. In fact, what do we do if it\ntakes one year and five",
    "start": "4085910",
    "end": "4093150"
  },
  {
    "text": "minutes to transmit instead\nof one year? Well, we've got a failure.",
    "start": "4093150",
    "end": "4099049"
  },
  {
    "text": "Somehow or other, the network\nis going to fail us. I mean we all know that networks\nfail all the time",
    "start": "4099050",
    "end": "4105350"
  },
  {
    "text": "despite what engineers say. I mean, all of us who use\nnetworks know that they do",
    "start": "4105350",
    "end": "4112119"
  },
  {
    "text": "crazy things. And one of those crazy things\nis that unusual things sometimes happen.",
    "start": "4112120",
    "end": "4118270"
  },
  {
    "text": "So, we develop this very nice\ntheory of prefix-free codes. But prefix-free codes,\nin fact, fail also.",
    "start": "4118270",
    "end": "4126580"
  },
  {
    "text": "And they fail also because\nbuffers overflow. In other words, we are counting\non encoding things",
    "start": "4126580",
    "end": "4134159"
  },
  {
    "text": "with a certain number of\nbits per source symbol. And if these unusual things\noccur, and we have too many",
    "start": "4134160",
    "end": "4140770"
  },
  {
    "text": "bits per source symbol,\nthen we fail. So the idea that we're trying\nto get at now is that",
    "start": "4140770",
    "end": "4148960"
  },
  {
    "text": "prefix-free codes and\nfixed-to-fixed length source codes which only encode\ntypical things.",
    "start": "4148960",
    "end": "4156640"
  },
  {
    "text": "In fact, are sort of the same\nif you look at them over a very, very large sequence\nlength.",
    "start": "4156640",
    "end": "4162859"
  },
  {
    "text": "In other words, if you look at\na prefix-free code which is dealing with blocks of 10\nletters, and you look at a",
    "start": "4162860",
    "end": "4171190"
  },
  {
    "text": "fixed-to-fixed length code which\nis only dealing with typical things but is looking at\na length of 10 to the 20th,",
    "start": "4171190",
    "end": "4179319"
  },
  {
    "text": "then over that length of 10 to\nthe 20th, your variable length code is going to have a bunch of\nthings which are about the",
    "start": "4179320",
    "end": "4187020"
  },
  {
    "text": "length they ought to be. And a bunch of other\nthings which are extraordinarily long.",
    "start": "4187020",
    "end": "4193089"
  },
  {
    "text": "The bunch of things which are\nextraordinarily long are extraordinarily unpopular, but\nthere are an extraordinarily",
    "start": "4193090",
    "end": "4199909"
  },
  {
    "text": "large number of them. Just like with a fixed-to-fixed\nlength code,",
    "start": "4199910",
    "end": "4205760"
  },
  {
    "text": "you are going to fail. And you're going to fail on\nan extraordinary number of different sequences.",
    "start": "4205760",
    "end": "4212500"
  },
  {
    "text": "But, collectively, that set of\nsequences don't have any probability.",
    "start": "4212500",
    "end": "4217850"
  },
  {
    "text": "So the point that I'm trying to\nget across is that, really, these two situations come\ntogether when we look very",
    "start": "4217850",
    "end": "4224020"
  },
  {
    "text": "long lengths. Namely, prefix-free codes are\njust a way of generating codes",
    "start": "4224020",
    "end": "4230030"
  },
  {
    "text": "that work for typical sequences\nand over a very large, long period of time, will\ngenerate about the right",
    "start": "4230030",
    "end": "4237389"
  },
  {
    "text": "number of symbols. And that's what I'm trying\nto get at here.",
    "start": "4237390",
    "end": "4242420"
  },
  {
    "text": "Or what I'm trying to get\nat in the next slide. So the fixed-to-fixed length\nsource code, I'm going to pick",
    "start": "4242420",
    "end": "4250650"
  },
  {
    "text": "some epsilon and some delta. Namely, that epsilon and delta\nwhich appeared in the law of",
    "start": "4250650",
    "end": "4255770"
  },
  {
    "text": "large numbers. I'm going to make n as big as\nI have to make it for that",
    "start": "4255770",
    "end": "4261400"
  },
  {
    "text": "epsilon and that delta. And calculate how large it\nhas to be, but we won't.",
    "start": "4261400",
    "end": "4267120"
  },
  {
    "text": "Then I'm going to assign fixed\nlength codewords to each",
    "start": "4267120",
    "end": "4272150"
  },
  {
    "text": "sequence in the typical set. Now, am I going to really build something which does this?",
    "start": "4272150",
    "end": "4278410"
  },
  {
    "text": "Of course not. I mean, I'm talking about\ntruly humongous lengths. So, this is really a conceptual\ntool to understand",
    "start": "4278410",
    "end": "4285620"
  },
  {
    "text": "what's going on. It's not something we're\ngoing to implement. So I'm going to assign\ncodewords to all",
    "start": "4285620",
    "end": "4292490"
  },
  {
    "text": "these typical elements. And then what I find is that\nsince the typical set, since",
    "start": "4292490",
    "end": "4300900"
  },
  {
    "text": "the number of elements in it is\nless than 2 to the n times H of x plus epsilon, if I choose\nL bar, namely, the",
    "start": "4300900",
    "end": "4311200"
  },
  {
    "text": "number of bits I'm going to use\nfor encoding these things,",
    "start": "4311200",
    "end": "4316980"
  },
  {
    "text": "it's going to have to be H of\nx plus epsilon in length. Because I need to provide\ncodewords for",
    "start": "4316980",
    "end": "4322190"
  },
  {
    "text": "each of these things. And it needs to be an extra 1\nover n because of this integer",
    "start": "4322190",
    "end": "4328930"
  },
  {
    "text": "constraint that we've been\ndealing with all along, which doesn't make any difference.",
    "start": "4328930",
    "end": "4334120"
  },
  {
    "text": "So if I choose L bar, that big,\nin other words, if I make it just a little bit bigger\nthan the entropy, the",
    "start": "4334120",
    "end": "4341670"
  },
  {
    "text": "probability of failure\nis going to be less than or equal to delta. And I can make delta -- and I\ncan make the probability of",
    "start": "4341670",
    "end": "4347910"
  },
  {
    "text": "failure as small as I want. So I can make this epsilon here\nwhich is the extra bits",
    "start": "4347910",
    "end": "4352960"
  },
  {
    "text": "per source symbol as\nsmall as I want. So it says I can come as close\nto the entropy bound in doing",
    "start": "4352960",
    "end": "4359790"
  },
  {
    "text": "this, and come as close to\nunique decodability as I want in doing this.",
    "start": "4359790",
    "end": "4365140"
  },
  {
    "text": "And I have a fixed-to-fixed\nlength code, which after one year is going to stop.",
    "start": "4365140",
    "end": "4370880"
  },
  {
    "text": "And I can turn my decoder off. I can turn my encoder off.",
    "start": "4370880",
    "end": "4375950"
  },
  {
    "text": "I can go buy a new encoder\nand a new decoder, which presumably works a little\nbit better.",
    "start": "4375950",
    "end": "4381770"
  },
  {
    "text": "And there isn't any problem\nabout when to turn it off. Because I know I can\nturn it off. Because everything will\nhave come in by then.",
    "start": "4381770",
    "end": "4389630"
  },
  {
    "text": "Here's a more interesting\nstory. Suppose I choose the number of\nbits per source symbol that",
    "start": "4389630",
    "end": "4398250"
  },
  {
    "text": "I'm going to use to be less than\nor equal to the entropy",
    "start": "4398250",
    "end": "4403390"
  },
  {
    "text": "minus 2 epsilon. Why 2 epsilon? Well, just wait a second.",
    "start": "4403390",
    "end": "4409110"
  },
  {
    "text": "I mean, 2 epsilon is small\nand epsilon is small. But I want to compare with this\nother epsilon and my law",
    "start": "4409110",
    "end": "4414145"
  },
  {
    "text": "of large numbers. And I'm going to pick\nn large enough.",
    "start": "4414145",
    "end": "4419429"
  },
  {
    "text": "The number of typical sequences,\nwe said before, was greater than 1 minus delta times\n2 to the n times h of x",
    "start": "4419430",
    "end": "4428300"
  },
  {
    "text": "minus epsilon. I'm going to make this epsilon\nthe same as that epsilon, which is why I wanted this\nto be 2 epsilon.",
    "start": "4428300",
    "end": "4434170"
  },
  {
    "text": " So my typical set is this big\nwhen I choose n large enough.",
    "start": "4434170",
    "end": "4441680"
  },
  {
    "text": "And this says that most\nof the typical set can't be assigned codewords.",
    "start": "4441680",
    "end": "4447440"
  },
  {
    "text": "In other words, this number\nhere is humongously larger",
    "start": "4447440",
    "end": "4455510"
  },
  {
    "text": "then 2 to the l bar, which is in\nthe order of 2 to the nh of",
    "start": "4455510",
    "end": "4475869"
  },
  {
    "text": "x minus 2 epsilon n.",
    "start": "4475870",
    "end": "4482200"
  },
  {
    "text": "So the fraction of typical\nelements that I can provide codewords for, between this and\nthis, I can only provide",
    "start": "4482200",
    "end": "4492040"
  },
  {
    "text": "codewords for a fraction\n2 to the minus epsilon n of the codewords.",
    "start": "4492040",
    "end": "4498670"
  },
  {
    "text": "We have this big sea of\ncodewords, which are all essentially equally likely.",
    "start": "4498670",
    "end": "4504200"
  },
  {
    "text": "And I can't provide codewords\nfor even a small fraction of them.",
    "start": "4504200",
    "end": "4509860"
  },
  {
    "text": "So the probability of failure is\ngoing to be 1 minus delta. The 1 minus delta's the\nprobability that I get",
    "start": "4509860",
    "end": "4515460"
  },
  {
    "text": "something atypical. Plus, well, minus in this case,\n2 to the minus epsilon",
    "start": "4515460",
    "end": "4524190"
  },
  {
    "text": "n, which is the probability that\nI can't encode a typical codeword that comes out.",
    "start": "4524190",
    "end": "4530670"
  },
  {
    "text": "And this quantity goes to 1. So this says that if I'm willing\nto use a number of",
    "start": "4530670",
    "end": "4537994"
  },
  {
    "text": "bits bigger than the entropy, I\ncan succeed with probability very close to 1.",
    "start": "4537995",
    "end": "4545010"
  },
  {
    "text": "And if I want to use a smaller\nnumber of bits, I fail with probability 1. ",
    "start": "4545010",
    "end": "4552810"
  },
  {
    "text": "Which is the same as saying that\nI'm using a prefix-free code, I'm going to run out of\nbuffer space eventually if I",
    "start": "4552810",
    "end": "4561949"
  },
  {
    "text": "run long enough. If I have something that\nI'm encoding --",
    "start": "4561950",
    "end": "4571650"
  },
  {
    "text": "well, just erase that. I'll say it more carefully\nlater. ",
    "start": "4571650",
    "end": "4578150"
  },
  {
    "text": "I do want to talk a little bit\nabout this Kraft inequality for unique decodability.",
    "start": "4578150",
    "end": "4583610"
  },
  {
    "text": "You remember we proved the\nKraft inequality for prefix-free codes.",
    "start": "4583610",
    "end": "4589460"
  },
  {
    "text": "I now want to talk about the\nKraft inequality for uniquely decodable codes.",
    "start": "4589460",
    "end": "4596060"
  },
  {
    "text": "And you might think that I've\ndone all of this development of the AEP, the asymptotic\nequipartition property.",
    "start": "4596060",
    "end": "4605990"
  },
  {
    "text": "Incidentally, you now know where\nthose words come from. It's asymptotic because this\nresult is valid asymptotically",
    "start": "4605990",
    "end": "4613500"
  },
  {
    "text": "as n goes to infinity. It's equipartition because\neverything is equally likely.",
    "start": "4613500",
    "end": "4621260"
  },
  {
    "text": "And its property, because\nit's a property. So it's the asymptotic\nequipartition property.",
    "start": "4621260",
    "end": "4628489"
  },
  {
    "text": "And I didn't do it so I could\nprove the Kraft inequality. It's just that that's an extra\nbonus that we get.",
    "start": "4628490",
    "end": "4634849"
  },
  {
    "text": "And by understanding why the\nKraft inequality has to hold",
    "start": "4634850",
    "end": "4640070"
  },
  {
    "text": "for uniquely decodable codes, if\nis one application for AEP",
    "start": "4640070",
    "end": "4648889"
  },
  {
    "text": "which lets you see a little\nbit about how to use it. OK, so the argument is an\nargument by contradiction.",
    "start": "4648890",
    "end": "4656520"
  },
  {
    "text": "Suppose you generate a set\nof lengths for codewords.",
    "start": "4656520",
    "end": "4663010"
  },
  {
    "text": "And you want this -- yeah? ",
    "start": "4663010",
    "end": "4675250"
  },
  {
    "text": "And the thing you would like to\ndo is to assign codewords of these lengths.",
    "start": "4675250",
    "end": "4681219"
  },
  {
    "text": "And what we want to do is to\nset this equal to some quantity b. In other words, suppose we beat\nthe Kraft inequality.",
    "start": "4681220",
    "end": "4689020"
  },
  {
    "text": "Suppose we can make the lengths\neven shorter than Kraft says we can make them.",
    "start": "4689020",
    "end": "4695730"
  },
  {
    "text": "I mean, he was only a graduate\nstudent, so we've got to be able to beat his inequality\nsomehow.",
    "start": "4695730",
    "end": "4701480"
  },
  {
    "text": "So we're going to try to\nmake this equal to b. We're going to assume that\nb is greater than 1.",
    "start": "4701480",
    "end": "4707930"
  },
  {
    "text": "And then what we're going to\ndo is to show that we get a contradiction here. And this same argument can\nwork whether we have a",
    "start": "4707930",
    "end": "4716090"
  },
  {
    "text": "discrete memoryless source or\na source with memory, or anything else. It can work with blocks, it can\nwork with variable length",
    "start": "4716090",
    "end": "4722830"
  },
  {
    "text": "to variable length codes. It's all essentially\nthe same argument.",
    "start": "4722830",
    "end": "4729560"
  },
  {
    "text": "So what I want to do is to\nget a contradiction. I'm going to choose a discrete\nmemoryless source.",
    "start": "4729560",
    "end": "4736230"
  },
  {
    "text": "And I'm going to make the\nprobabilities equal to 1 over b times 2 to the minus li.",
    "start": "4736230",
    "end": "4742300"
  },
  {
    "text": "In other words, I can generate\na discrete memoryless source for talking about it with\nany probabilities I want to give it.",
    "start": "4742300",
    "end": "4748800"
  },
  {
    "text": "So I'm going to generate one\nwith these probabilities. So the lengths are going to\nbe equal to minus log of",
    "start": "4748800",
    "end": "4756530"
  },
  {
    "text": "b times p sub i. Which says that the expected\nlength of the codewords is",
    "start": "4756530",
    "end": "4762920"
  },
  {
    "text": "equal to the sum of p sub i l\nsub i, which is equal to the entropy minus the\nlogarithm of b.",
    "start": "4762920",
    "end": "4771780"
  },
  {
    "text": "Which means I can get an\nexpected length which is a little bit less than\nthe entropy.",
    "start": "4771780",
    "end": "4777440"
  },
  {
    "text": "So now what I'm going to do is\nto consider strings of n source letters. I'm going to make these string\nvery, very long.",
    "start": "4777440",
    "end": "4783460"
  },
  {
    "text": " When I concatenate all these\ncodewords, I'm going to wind",
    "start": "4783460",
    "end": "4790430"
  },
  {
    "text": "up with a length that's less\nthan n times H of x minus b over 2, minus log b over 2\nwith high probability.",
    "start": "4790430",
    "end": "4799400"
  },
  {
    "start": "4799400",
    "end": "4813510"
  },
  {
    "text": "And as a fixed-length code of\nthis length it's going to have",
    "start": "4813510",
    "end": "4818940"
  },
  {
    "text": "a low failure probability. And, therefore, what this says\nis I can, using this",
    "start": "4818940",
    "end": "4826740"
  },
  {
    "text": "remarkable code with unique\ndecodability, and generating",
    "start": "4826740",
    "end": "4832670"
  },
  {
    "text": "very long strings from it, I\ncan generate a fixed-length code which has a low failure\nprobability.",
    "start": "4832670",
    "end": "4841550"
  },
  {
    "text": "And I just showed you\nin the last slide that I can't do that. The probability of failure with\nsuch a code has to be",
    "start": "4841550",
    "end": "4849829"
  },
  {
    "text": "essentially 1. So that's a contradiction that\nsays you can't have these",
    "start": "4849830",
    "end": "4854870"
  },
  {
    "text": "unique decodable codes. If you didn't get that in what\nI said, don't be surprised.",
    "start": "4854870",
    "end": "4861670"
  },
  {
    "text": "Because all I'm trying to do is\nto steer you towards how to look at the section in the\nnotes that does that.",
    "start": "4861670",
    "end": "4869610"
  },
  {
    "text": "It was a little too fast\nand a little too late. But, anyway, that is the Kraft\ninequality for unique",
    "start": "4869610",
    "end": "4875570"
  },
  {
    "text": "decodability. OK, thanks. ",
    "start": "4875570",
    "end": "4878628"
  }
]