[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu.  ALAN EDELMAN: Hi, everybody.",
    "start": "18140",
    "end": "23630"
  },
  {
    "start": "22000",
    "end": "205000"
  },
  {
    "text": "I'm Alan Edelman. And helped a little bit to\nteach this class last year.",
    "start": "23630",
    "end": "29970"
  },
  {
    "text": "But happy to see that it's\ngoing great this year. So Professor Strang\ncame to teach 18.06.",
    "start": "29970",
    "end": "37040"
  },
  {
    "text": "Some of you may know the\nintroductory linear algebra course. And Professor Strang\ncame by and gave",
    "start": "37040",
    "end": "44180"
  },
  {
    "text": "this great demonstration\nabout the row rank equals the column rank. And I'm wondering if you did\nthat in this class at any time.",
    "start": "44180",
    "end": "50600"
  },
  {
    "text": "Or would they have seen that? AUDIENCE: It's in the notes. ALAN EDELMAN: It's in the notes. Well, in any event, just as\nProfessor Strang walked out--",
    "start": "50600",
    "end": "59960"
  },
  {
    "text": "so here, I'll just grab this. This is true. I was actually going to\nstart writing the code to do the quadrilateral, but\nI didn't have enough time.",
    "start": "59960",
    "end": "66710"
  },
  {
    "text": "You can see me starting. But here's the 0.5 for the\ntriangle, which was easy. So what's the story with\nJulia in this class?",
    "start": "66710",
    "end": "73689"
  },
  {
    "text": "Have they used it a\nlittle, or a lot, or-- AUDIENCE: In the labs. ALAN EDELMAN: In the\nlabs you've used Julia. But that was just MATLAB. So that was-- but OK.",
    "start": "73690",
    "end": "81800"
  },
  {
    "text": " So Professor Strang showed\nthis proof where he would--",
    "start": "81800",
    "end": "88540"
  },
  {
    "text": "he put down a 3 by 3 matrix. It had rank two. And he took the columns\nthat were-- the first two",
    "start": "88540",
    "end": "95299"
  },
  {
    "text": "columns were independent. And it was easy to show the row\nrank equals the column rank.",
    "start": "95300",
    "end": "101270"
  },
  {
    "text": "After Professor Strang\nwent out, I asked, would that work for\nthe zero matrix?",
    "start": "101270",
    "end": "107210"
  },
  {
    "text": "So here's the zero matrix. And since I'm not really\ntelling you the proof, I'll just say, if I were to\nmake a matrix of the literally",
    "start": "107210",
    "end": "117680"
  },
  {
    "text": "independent columns of this\nmatrix, what would I do? So zero might seem tricky.",
    "start": "117680",
    "end": "124613"
  },
  {
    "text": "It's not really that tricky. But this is what I did\nthe moment you walked out.",
    "start": "124613",
    "end": "130490"
  },
  {
    "text": "So yeah, so I've got the 3 by 3\nmatrix, and I need to make a-- first step in whatever\nthe proof was,",
    "start": "130490",
    "end": "137300"
  },
  {
    "text": "I needed to take the\ncolumns, the literally independent columns\nof this matrix,",
    "start": "137300",
    "end": "143210"
  },
  {
    "text": "and place them in a\nmatrix of their own. What would I do?",
    "start": "143210",
    "end": "148403"
  },
  {
    "text": "It would be an empty matrix. What would be the size\nof this empty matrix?",
    "start": "148403",
    "end": "153450"
  },
  {
    "text": "Not exactly zero by zero. Because where every column\nis still in our 3, you see.",
    "start": "153450",
    "end": "160700"
  },
  {
    "text": "So the right answer-- I hope this makes sense-- is a 3 by 0 empty matrix.",
    "start": "160700",
    "end": "166210"
  },
  {
    "text": "And that's a concept that\nexists in MATLAB, and in Julia, and in Python, I\nthink, I'm sure,",
    "start": "166210",
    "end": "171410"
  },
  {
    "text": "and in any computer language. So if you had a full\nrank 3 by 3 matrix, the linear independent\ncolumns would be 3 by 3.",
    "start": "171410",
    "end": "178200"
  },
  {
    "text": "If you had a rank two,\nit would be 3 by 2. If I had a rank one\nmatrix, it would be 3 by 1.",
    "start": "178200",
    "end": "183330"
  },
  {
    "text": "So if I had no columns,\n3 by 0 makes sense. And to finish the proof,\nagain, I'm not telling these",
    "start": "183330",
    "end": "189830"
  },
  {
    "text": "students-- it's in\nthe notes apparently-- the next matrix\nwould be random 0, 3. And of course you multiply\nit and you get a 3",
    "start": "189830",
    "end": "196020"
  },
  {
    "text": "by 3 matrix of zeros. So it's fun to see\nthat that proof still works, even for the zero\nmatrix, without any real edge.",
    "start": "196020",
    "end": "202230"
  },
  {
    "text": "So that was just today. ",
    "start": "202230",
    "end": "207610"
  },
  {
    "start": "205000",
    "end": "770000"
  },
  {
    "text": "The other thing is let me-- can I say a word or two about\nrecent stuff about Julia?",
    "start": "207610",
    "end": "214920"
  },
  {
    "text": "So I started to put\ntogether a talk. It's not really ready yet, but\nI'll share it with you anyway.",
    "start": "214920",
    "end": "221280"
  },
  {
    "text": "So Google did the Julia\nworld a big favor last week. I mean, this is huge. So you all know machine\nlearning is hot.",
    "start": "221280",
    "end": "228930"
  },
  {
    "text": "That's probably why\nyou're here in this class. I probably don't\nhave to tell you.",
    "start": "228930",
    "end": "234599"
  },
  {
    "text": "And yet I wouldn't be\nsurprised if a number of you wished this whole class was in\nPython or something, or maybe",
    "start": "234600",
    "end": "241710"
  },
  {
    "text": "MATLAB or something. I don't doubt that\nsome of you might have wanted that to happen.",
    "start": "241710",
    "end": "248130"
  },
  {
    "text": "And we get sort of bombarded\nwith, you know, why not Python.",
    "start": "248130",
    "end": "254580"
  },
  {
    "text": "Not so much MATLAB anymore. But you know, why not Python is\nsort of the issue that comes up a lot.",
    "start": "254580",
    "end": "260910"
  },
  {
    "text": "And I could talk till\nthe cows come home, but nobody would believe me. But Google came out\nlast week and said",
    "start": "260910",
    "end": "268405"
  },
  {
    "text": "that when it comes\nto machine learning, there really are two languages\nthat are powerful enough to do machine learning--",
    "start": "268405",
    "end": "275610"
  },
  {
    "text": "to do machine learning kinds\nof things that you want to do. And in some sense, the\nrest of today's lecture",
    "start": "275610",
    "end": "281310"
  },
  {
    "text": "that I'm going to give will be\nmaybe illustrations of this. But if you want, you can go\nand look at their blog here.",
    "start": "281310",
    "end": "289169"
  },
  {
    "text": "What they do is they basically\nsort of start the race with a whole bunch of\nprogramming languages. There's Python.",
    "start": "289170",
    "end": "294180"
  },
  {
    "text": "There's R. Java, JavaScript. They sort of look at\nall of these languages.",
    "start": "294180",
    "end": "300288"
  },
  {
    "text": "And if you read the\nblog, you'll see. But we're going to filter\nthem out on technical merits. And right away, a lot\nof them disappear,",
    "start": "300288",
    "end": "306810"
  },
  {
    "text": "including Python and Java. And if you go to\nthe blog, you'll see they spend a great deal\nof time on the Python story.",
    "start": "306810",
    "end": "314255"
  },
  {
    "text": "Because they know\nthat people are going to want to hear that one. I mean, people want\nto be convinced. And so there's actually\nmultiple screens",
    "start": "314255",
    "end": "320940"
  },
  {
    "text": "full on the reason why\nPython is just not good enough for machine learning. So they leave four\nlanguages left--",
    "start": "320940",
    "end": "328160"
  },
  {
    "text": "Julia, Swift, C++, and Rust. And then if you go to the\nnext part of the blog,",
    "start": "328160",
    "end": "336120"
  },
  {
    "text": "they filter on usability. And then two more\nsort of bite the dust. So C++ and Rust disappeared.",
    "start": "336120",
    "end": "342930"
  },
  {
    "text": "And then they go on to say\nthat these are the only two languages they feel are\nappropriate for machine",
    "start": "342930",
    "end": "349600"
  },
  {
    "text": "learning. And they put this\nnice quote that it shares many common values.",
    "start": "349600",
    "end": "355680"
  },
  {
    "text": "And they actually go on about\nwhat machine learning really needs. And I'd recommend\nyou look at it.",
    "start": "355680",
    "end": "361680"
  },
  {
    "text": "And then finally,\nof course, they're going to push Swift,\nwhich they should.",
    "start": "361680",
    "end": "367259"
  },
  {
    "text": "So they had somewhere-- blah,\nblah-- about more people are using Swift. Maybe it's true. I don't know.",
    "start": "367260",
    "end": "373290"
  },
  {
    "text": "So they really said is they're\nmore familiar with Swift than Julia, which, you\nknow, if I was speaking,",
    "start": "373290",
    "end": "379990"
  },
  {
    "text": "I'd say I'm more familiar\nwith Julia than Swift. So maybe it's fair. And then I started to\nput a little cartoon",
    "start": "379990",
    "end": "386108"
  },
  {
    "text": "on the psychology of\nprogramming languages, just because it's\nsort of something that I bump into\nwith all the time.",
    "start": "386108",
    "end": "391860"
  },
  {
    "text": "People always say all\nlanguages are equally good. It doesn't really matter. But the truth is if you mention\na language that you're not",
    "start": "391860",
    "end": "399900"
  },
  {
    "text": "using yet, you're\ngoing to tune it out, at least until\nGoogle comes along. So that that's where we are.",
    "start": "399900",
    "end": "406320"
  },
  {
    "text": "OK, enough about-- I just put this together. I was testing it out on you.",
    "start": "406320",
    "end": "411930"
  },
  {
    "text": "All right. So now let me do two\nmore mathematical things.",
    "start": "411930",
    "end": "417880"
  },
  {
    "text": "So the first thing I\nwant to do is talk to you about forward mode\nautomatic differentiation. So have you done any automatic\ndifferentiation in this course?",
    "start": "417880",
    "end": "425275"
  },
  {
    "text": "AUDIENCE: Very little. ALAN EDELMAN: OK, so I\nthink this is pretty fun. I hope you'll like it. I have a notebook in\nJulia on forward mode",
    "start": "425275",
    "end": "431526"
  },
  {
    "text": "automatic differentiation. And this notebook came\ntogether because I",
    "start": "431527",
    "end": "437099"
  },
  {
    "text": "was trying to understand\nwhat the big deal was for a long time. And I had a little trouble.",
    "start": "437100",
    "end": "442560"
  },
  {
    "text": "I mean, it's the usual story\nwhere on a line-by-line level, it's easy to understand. But what's the big deal part?",
    "start": "442560",
    "end": "448620"
  },
  {
    "text": "That's sometimes the\nharder thing to grasp. And the first notebook\nI'm going to show you is sort of the\nresult of my trying",
    "start": "448620",
    "end": "456560"
  },
  {
    "text": "to grasp what was the\nreal picture here. And the second thing-- I think I'll just do\nit on the blackboard.",
    "start": "456560",
    "end": "462530"
  },
  {
    "text": "It's not even really ready yet,\nbut I'll sort of unleash it on you folks anyway-- is to show you how to do a\nparticular example of backward",
    "start": "462530",
    "end": "470600"
  },
  {
    "text": "mode automatic\ndifferentiation, the that you see in the neural net. And I guess you have seen\nsome neural nets here?",
    "start": "470600",
    "end": "476287"
  },
  {
    "text": "AUDIENCE: Yep. ALAN EDELMAN So I think by now\neverybody's seen neural nets. I think two years from now,\nit'll be in high schools.",
    "start": "476287",
    "end": "482288"
  },
  {
    "text": "And three years from there,\nit will be in kindergarten. I don't know. Neural nets seem to be sort of--",
    "start": "482288",
    "end": "487699"
  },
  {
    "text": "they're not that\nhard to understand. OK, so let me start things off.",
    "start": "487700",
    "end": "493550"
  },
  {
    "text": "And really, the two things that\nI'd love to convince you of is--",
    "start": "493550",
    "end": "499099"
  },
  {
    "text": "let me just find-- here's\nmy auto diff thing. The two things that I\nreally want to convince you",
    "start": "499100",
    "end": "504260"
  },
  {
    "text": "of-- and maybe you already\nbelieve some of this-- is one, that-- well, maybe\nyou don't believe this yet--",
    "start": "504260",
    "end": "512479"
  },
  {
    "text": "but that the language matters\nin a mathematical sense. ",
    "start": "512480",
    "end": "518614"
  },
  {
    "text": "The right computer language\ncan do more for you than just take some algorithm on\na blackboard and implement it.",
    "start": "518614",
    "end": "526113"
  },
  {
    "text": "It could do much more. And this is something that I\nhope to give a few examples of.",
    "start": "526113",
    "end": "531470"
  },
  {
    "text": "And the other thing that\nI bet you all believe now, because you've\nbeen in this class,",
    "start": "531470",
    "end": "537589"
  },
  {
    "text": "is that linear algebra is\nthe basis for everything. Every course should start\nwith linear algebra. I mean, to me, it feels like a\nunfortunate accident of history",
    "start": "537590",
    "end": "547220"
  },
  {
    "text": "that linear algebra came too\nlate for too many reasons. And so very often, things\nthat would be better done",
    "start": "547220",
    "end": "554629"
  },
  {
    "text": "with linear algebra are not. And I mean, to me, it feels like\ndoing physics without calculus.",
    "start": "554630",
    "end": "560390"
  },
  {
    "text": "I just don't get it. I know high schools do that. But it just seems wrong. To me, all of\nengineering, all of-- it",
    "start": "560390",
    "end": "566710"
  },
  {
    "text": "should all be linear algebra. I mean, I just believe\nthat-- almost all. Maybe not all. But quite a lot.",
    "start": "566710",
    "end": "572250"
  },
  {
    "text": "More than most people\nrealize, I would say. OK, so let me start with\nautomatic differentiation.",
    "start": "572250",
    "end": "580022"
  },
  {
    "text": "So I'm going to\nstart by this story by telling you that I\nwould go to conferences. I would go to numerical\nanalysis conferences.",
    "start": "580022",
    "end": "586490"
  },
  {
    "text": "I would hear people talk about\nautomatic differentiation. I'm going to be honest. I was reading my email. I was tuned out. Like, who cares about--",
    "start": "586490",
    "end": "592190"
  },
  {
    "text": "I know calculus. You could teach a\ncomputer to do it. It seems pretty easy to me.",
    "start": "592190",
    "end": "597260"
  },
  {
    "text": "I mean, I'm sure there\nare technical details. But it didn't seem that\ninteresting to teach a computer to differentiate.",
    "start": "597260",
    "end": "603312"
  },
  {
    "text": "I sort of figured that\nit was the same calculus that I learned when I\ntook a calculus class. You know, you\nmemorize this table.",
    "start": "603312",
    "end": "609529"
  },
  {
    "text": "You teach it to a computer. You learn the chain rule,\nand the product rule, and the quotient rule. And bump, the computer\nis doing just what I",
    "start": "609530",
    "end": "616430"
  },
  {
    "text": "would do with paper and pencil. So big deal. I didn't pay attention. And in any event, there\nwas this little neuron",
    "start": "616430",
    "end": "623360"
  },
  {
    "text": "in the back of my brain that\nsaid, hey, maybe I'm wrong. Maybe it's doing\nfinite differences, you know, the sort\nof thing where",
    "start": "623360",
    "end": "629840"
  },
  {
    "text": "you take the dy by the dx. In some numerical way, you\ndo the finite differences.",
    "start": "629840",
    "end": "635240"
  },
  {
    "text": "And in numerical\nanalysis, they're supposed to tell you if h is too\nbig, you get truncation error. If you have h too small,\nyou get round off error.",
    "start": "635240",
    "end": "642252"
  },
  {
    "text": "And the truth is nobody ever\ntells you what's a good h. But you go to a\nnumerical analysis class hoping somebody would tell you.",
    "start": "642252",
    "end": "648810"
  },
  {
    "text": "But in any event,\nso I thought maybe it was that kind of a\nnumerical finite difference.",
    "start": "648810",
    "end": "654200"
  },
  {
    "text": "And I think the\nbig surprise for me was that automatic\ndifferentiation was neither",
    "start": "654200",
    "end": "659930"
  },
  {
    "text": "the first nor the second\nthing, that there's actually a third thing, something\ndifferent, that's neither the first or the second.",
    "start": "659930",
    "end": "666019"
  },
  {
    "text": "And I found that fascinating. And maybe I'll even tell you\nhow it hit me in the head",
    "start": "666020",
    "end": "671960"
  },
  {
    "text": "that this was the story. Because I really wasn't\npaying attention. But I love the singular\nvalue decomposition.",
    "start": "671960",
    "end": "679190"
  },
  {
    "text": "I'm glad to see that people are\ndrawing parabolas and quarter circles and figuring out what\nthe minimum SVD value is.",
    "start": "679190",
    "end": "686840"
  },
  {
    "text": "The singular value is just-- it's just God's gift to mankind. It's just a good factorization.",
    "start": "686840",
    "end": "694880"
  },
  {
    "text": "One of the things I was\nplaying with with Julia was to calculate the\nJacobian matrix for the SVD.",
    "start": "694880",
    "end": "703460"
  },
  {
    "text": "So you know, all\nmatrix factorizations are just changes of variables. So if you have a square\nmatrix, n by n, the SVD--",
    "start": "703460",
    "end": "712760"
  },
  {
    "text": "I'm sure you know this-- is the\nU matrix is really n times n mass 1 over 2 variable. So is the V. And the\nsigma has got n variables.",
    "start": "712760",
    "end": "720505"
  },
  {
    "text": "Put it all together,\nyou got n squared. So it's just a\nchange of variables. And every time you\nchange variables,",
    "start": "720505",
    "end": "725720"
  },
  {
    "text": "you can form that big matrix, n\nsquared by n squared of dy dx,",
    "start": "725720",
    "end": "731019"
  },
  {
    "text": "compute its determinant,\nand get an answer. And I wanted to know-- I actually knew the\ntheoretical answer for that.",
    "start": "731020",
    "end": "737330"
  },
  {
    "text": "And I wanted to see\na computer confirm that theoretical answer. And I spoke to some\npeople who wrote auto diff",
    "start": "737330",
    "end": "744870"
  },
  {
    "text": "in non-Julia languages. And I was surprised\nby the answer. They said, oh, yeah.",
    "start": "744870",
    "end": "749900"
  },
  {
    "text": "We could teach the\nanswer to our system. I said, what do you mean\nyou could teach the answer? Why doesn't it just\ncompute the answer?",
    "start": "749900",
    "end": "755240"
  },
  {
    "text": "Why do you have to\nteach the answer? I thought that was all wrong. Because in Julia, we\ndidn't have to teach it. It would actually calculate it.",
    "start": "755240",
    "end": "762837"
  },
  {
    "text": "And then I started to\nunderstand a little bit more about what auto diff was doing\nand what Julia was doing. And so this is how this\nnotebook came to be.",
    "start": "762837",
    "end": "769759"
  },
  {
    "text": "So let me start-- I'm saying too much. Let me start with an example\nthat might kind of hit home.",
    "start": "769760",
    "end": "775870"
  },
  {
    "start": "770000",
    "end": "1196000"
  },
  {
    "text": "So I'm going to\ncompute the square root of x, a real simple example. You know, a square\nroot's pretty easy.",
    "start": "775870",
    "end": "781120"
  },
  {
    "text": "I'm going to take one of\nthe oldest algorithms known to mankind, the Babylonian\nsquare root algorithm. It says start with\na starting guess t.",
    "start": "781120",
    "end": "788230"
  },
  {
    "text": "Maybe it's a little bit too\nlow for the square root of x. Get x over t. So that would be too large.",
    "start": "788230",
    "end": "793270"
  },
  {
    "text": "Go ahead and take the\naverage, and repeat. OK. This is equivalent\nto a Newton's method for taking the square root.",
    "start": "793270",
    "end": "798980"
  },
  {
    "text": "And it's been known for\nmillennia to mankind. So it's not the latest\nresearch, by any means,",
    "start": "798980",
    "end": "806770"
  },
  {
    "text": "for computing square roots. But it works very effectively. And here's a little\nJulia code that",
    "start": "806770",
    "end": "813610"
  },
  {
    "text": "actually will implement it. It probably looks like\ncode in any language.",
    "start": "813610",
    "end": "819050"
  },
  {
    "text": "So I'm going to start off at 1. So literally, I'm\njust going to take 1 plus the starting value\nof x and divide by 2.",
    "start": "819050",
    "end": "827320"
  },
  {
    "text": "And then I'm going to repeat. OK? And we can check that\nthe algorithm works.",
    "start": "827320",
    "end": "833200"
  },
  {
    "text": "Here's alpha is pi. And so I'll take the\nBabylonian algorithm and compare it to\nJulia's built in.",
    "start": "833200",
    "end": "838240"
  },
  {
    "text": "And you see it gives\nthe right answer. Here it is with the\nsquare root of two. It's always good to\ncheck your code works.",
    "start": "838240",
    "end": "845680"
  },
  {
    "text": "OK? I like to see\nthings graphically, so I ran the algorithm\nfor lots of values of x.",
    "start": "845680",
    "end": "854529"
  },
  {
    "text": "And I love doing this. I kind of wish that\nin the previous talk--",
    "start": "854530",
    "end": "859940"
  },
  {
    "text": "if I'd only worked\nfast enough, I wanted to build a\nlittle GUI where I can move the points\nin front of your eyes.",
    "start": "859940",
    "end": "865352"
  },
  {
    "text": "Maybe you have one in MATLAB. I bet you do. But I wanted to build it. But I didn't get\nthere fast enough.",
    "start": "865352",
    "end": "870519"
  },
  {
    "text": "But here this is\nthe sort of thing. And I like to see\nthe convergence. And so you could see\nthe digits converging,",
    "start": "870520",
    "end": "876270"
  },
  {
    "text": "the parabola on the bottom. The block is the\nsquare root, of course. So there it is.",
    "start": "876270",
    "end": "881290"
  },
  {
    "text": "There's the\nBabylonian algorithm. I would like to get the\nderivative of square root.",
    "start": "881290",
    "end": "887020"
  },
  {
    "text": "But the rules of the\ngame are I'm not going to type method 1 or method 2. I'm not going to do-- you'll\nnever see me type 1/2 x",
    "start": "887020",
    "end": "893650"
  },
  {
    "text": "to the minus 1/2. Right? You all know that's\nthe derivative. I will not type that. I will not. It's not going to come\nanywhere from Julia.",
    "start": "893650",
    "end": "899910"
  },
  {
    "text": "OK. And the second thing\nis I'm not going to do a finite difference. All right? I'm going to get\nthat square root, but not by sort of\neither of the two things",
    "start": "899910",
    "end": "907420"
  },
  {
    "text": "that I'm sure you\nwould think of. Right? Here's how I'm going to do it. And I'm going to do a\nlittle bit of Julia code.",
    "start": "907420",
    "end": "913935"
  },
  {
    "text": "There'll be eight\nlines of Julia. But I'm not going to completely\nsay how it works yet. I'll keep you in suspense\nfor maybe about five minutes.",
    "start": "913935",
    "end": "920200"
  },
  {
    "text": "And then I'll tell\nyou how it works. All right? So here's eight lines of\nJulia code that will get me",
    "start": "920200",
    "end": "925900"
  },
  {
    "text": "the square root. So in these three lines, I'm\ngoing to create a Julia type. I'm going to call it a D\nfor a dual number, which",
    "start": "925900",
    "end": "934660"
  },
  {
    "text": "is a name that goes back at\nleast a century, maybe more. So I'm going to create a D type.",
    "start": "934660",
    "end": "941440"
  },
  {
    "text": "And all this is is\na pair of floats. So it's a tuple with\na pair of floats.",
    "start": "941440",
    "end": "947590"
  },
  {
    "text": "It's going to be some\nsort of numerical function and derivative pair. So three of my eight lines is to\ncreate a D. In Julia language,",
    "start": "947590",
    "end": "956860"
  },
  {
    "text": "this means to use a\nsubtype of a number, so we're going to\ntreat it like a number. Right? We want to be able\nto add, multiply,",
    "start": "956860",
    "end": "961900"
  },
  {
    "text": "and divide these ordered pairs. But it's just a pair of numbers. Don't let the Julia scare you.",
    "start": "961900",
    "end": "967870"
  },
  {
    "text": "It's just a function\nderivative numerical pair. OK? And what's these\nother five lines? Well, I want to teach it the\nsum rule and the quotient rule.",
    "start": "967870",
    "end": "975460"
  },
  {
    "text": "So you all remember\nthe same rule. I guess that's the easy one. The quotient rule--",
    "start": "975460",
    "end": "981279"
  },
  {
    "text": "I still have my teacher\nfrom high school ringing in the back of my ear. The denominator times the\nderivative of the numerator",
    "start": "981280",
    "end": "986450"
  },
  {
    "text": "and minus the\nnumerator-- you all have that jingle\nin your brain, too? I bet you do. divided by the\ndenominator squared.",
    "start": "986450",
    "end": "991860"
  },
  {
    "text": "Can't even get it\nout of my head.  So there's the quotient rule.",
    "start": "991860",
    "end": "998089"
  },
  {
    "text": "And so what are we doing\nin these five lines? Well, first of all, I want\nto overlook plus and divide",
    "start": "998090",
    "end": "1005230"
  },
  {
    "text": "and a few other things. And Julia wants me\nto say, are you sure? So the way you say\nare you sure is",
    "start": "1005230",
    "end": "1010690"
  },
  {
    "text": "that I'm going to\nimport plus and divide. Because it would be\ndangerous to play with plus. So here I'm going to\nplus two dual numbers.",
    "start": "1010690",
    "end": "1017613"
  },
  {
    "text": "We're going to add the\nfunction and the derivatives. Divide two dual numbers. We're going to divide the\nfunction values and denominator",
    "start": "1017613",
    "end": "1023260"
  },
  {
    "text": "times the numerator, blah,\nblah, blah, you get it. OK. That's six of the eight lines.",
    "start": "1023260",
    "end": "1029260"
  },
  {
    "text": "The seventh line is, if\nI have a dual number, I wanted to convert it. You know how the wheels are\nembedded in the complexes?",
    "start": "1029260",
    "end": "1036949"
  },
  {
    "text": "We have to tell Julia\nto take the dual number and stick a zero in. And then dual numbers\nand regular numbers",
    "start": "1036950",
    "end": "1043480"
  },
  {
    "text": "can play nicely together. And this actually\nis the thing that actually says, if I have\na dual number and a number",
    "start": "1043480",
    "end": "1050080"
  },
  {
    "text": "in operation, promote them so\nthey'll work as dual numbers-- so eight lines of code.",
    "start": "1050080",
    "end": "1055850"
  },
  {
    "text": "So the first thing I'm\ngoing to tell you is I'm going to remind you I never\ntyped 1/2 x to the minus 1/2.",
    "start": "1055850",
    "end": "1062280"
  },
  {
    "text": "Do you agree? No one-- I'm not\nimporting any packages. It's not like it's\ncoming in from the-- I'm not sneaking it\nin from the side.",
    "start": "1062280",
    "end": "1068380"
  },
  {
    "text": "There's no one half\nx to the minus 1/2. And there's certainly not\nany numerical derivatives,",
    "start": "1068380",
    "end": "1076120"
  },
  {
    "text": "either, right? Arguably, a rule\nthat almost feels symbolic, the quotient\nrule and the addition rule.",
    "start": "1076120",
    "end": "1081683"
  },
  {
    "text": "But no numerical finite\ndifferences at all here. OK.",
    "start": "1081683",
    "end": "1086929"
  },
  {
    "text": "So first of all,\nlet me show you here that I'm applying the Babylonian\nalgorithm without rewriting",
    "start": "1086930",
    "end": "1096490"
  },
  {
    "text": "code to a dual number now. Before we applied it to numbers. But now I'm going to play\nit to this dual number",
    "start": "1096490",
    "end": "1101890"
  },
  {
    "text": "that I just invented. I'm going to apply it at 49,\n1, because I know the answer. And then I'm going\nto compare it with--",
    "start": "1101890",
    "end": "1108009"
  },
  {
    "text": "I'm taking one half\nof the square root of x just for\ncomparison purposes and not in my own algorithm.",
    "start": "1108010",
    "end": "1113529"
  },
  {
    "text": "And of course, you see\nthat I'm getting magically the right answer without ever-- so you should wonder,\nhow did I do that?",
    "start": "1113530",
    "end": "1120160"
  },
  {
    "text": "How did I get the derivative? We could take any\nnumber you like. Here's 100. If you prefer to see a number\nlike pi, we can do that.",
    "start": "1120160",
    "end": "1127870"
  },
  {
    "text": "I mean, we can do\nwhatever you like. It's going to work. So there you see this is\nthe square root of pi.",
    "start": "1127870",
    "end": "1133870"
  },
  {
    "text": "And this would be 1/2 over the\nsquare root of pi numerically. So when you see it matches\nthese numbers to enough digits,",
    "start": "1133870",
    "end": "1140500"
  },
  {
    "text": "in fact, all the\ndigits, actually. Yeah. So the thing magically worked. You should all be wondering,\nhow did that happen?",
    "start": "1140500",
    "end": "1147870"
  },
  {
    "text": "I didn't rewrite any code. I actually wrote a code to\njust compute the square root. I never wrote a code to compute\nthe root of a square root.",
    "start": "1147870",
    "end": "1155350"
  },
  {
    "text": "And by the way, this is a\nlittle bit of the Julia magic that we're pushing numerically. That very often in\nthis world, people",
    "start": "1155350",
    "end": "1163743"
  },
  {
    "text": "will write a code\nto do something, and then if you want\nto do something more, like get a derivative,\nsomebody writes another code.",
    "start": "1163743",
    "end": "1169540"
  },
  {
    "text": "With Julia, very\noften, you can actually keep to the original code. And if you just use it\nproperly and intelligently,",
    "start": "1169540",
    "end": "1175990"
  },
  {
    "text": "you can do magic things\nwithout writing new codes. And you'll see this\nagain in a little bit. But here's the derivative of--",
    "start": "1175990",
    "end": "1183280"
  },
  {
    "text": "this is the plot of 1/2 over\nthe square root of x in black. And again, you could see\nthe convergence over here.",
    "start": "1183280",
    "end": "1189050"
  },
  {
    "text": "All right. Well, I'm still not going to\nshow you why it works just yet. I promise I will in just\nprobably a few minutes more.",
    "start": "1189050",
    "end": "1195770"
  },
  {
    "text": "But what I will do first is\nI'd like to show you something that most people\nwill never look at.",
    "start": "1195770",
    "end": "1201490"
  },
  {
    "start": "1196000",
    "end": "1272000"
  },
  {
    "text": "I never look at it. I want to show you-- here's\nthe same Babylonian code. I want to show you the\nassembler for the computation",
    "start": "1201490",
    "end": "1210220"
  },
  {
    "text": "of the derivative. So I'm going to run\nBabylonian on a dual number. And we're going to look here.",
    "start": "1210220",
    "end": "1217360"
  },
  {
    "text": "And I don't know if anybody\nhere reads assembler. I'm betting there is zero\nor one of you actually",
    "start": "1217360",
    "end": "1222652"
  },
  {
    "text": "reads this stuff. How many of you read assembler? OK. It wasn't 0, 1.",
    "start": "1222652",
    "end": "1228200"
  },
  {
    "text": "We had a half. Right there's half. He's kind of going like this. Here's zero. Here's one. He's like this. OK.",
    "start": "1228200",
    "end": "1233520"
  },
  {
    "text": "So I think 0, 1 is\nlike the record. But I'll bet you'll believe\nme if I tell you that, when",
    "start": "1233520",
    "end": "1239900"
  },
  {
    "text": "you have short assembler like\nthis and it's not very long, then you have efficient code. It's very tight.",
    "start": "1239900",
    "end": "1245190"
  },
  {
    "text": "It will run very fast. So whatever this thing\nis doing, it's short. And this you won't get\nfrom any other language.",
    "start": "1245190",
    "end": "1252023"
  },
  {
    "text": "If you did try to do the\nsame thing in Python, I promise you there\nwould be screens and screens and\nscreens full of stuff,",
    "start": "1252023",
    "end": "1257630"
  },
  {
    "text": "even if you could get it. So here's the Babylonian\nalgorithm on the dual number.",
    "start": "1257630",
    "end": "1264470"
  },
  {
    "text": "And here it is in\nassembler, and it's short. So the other thing\nthat I'm saying is not only does it work, but\nJulia also makes it efficient.",
    "start": "1264470",
    "end": "1272120"
  },
  {
    "start": "1272000",
    "end": "1358000"
  },
  {
    "text": "So before I finally tell\nyou what's really going on and why it works,\nI'm going to grab",
    "start": "1272120",
    "end": "1277760"
  },
  {
    "text": "a Python symbolic package, which\nwill work nicely with Julia. And I'm going to run the\nsame code through the Python",
    "start": "1277760",
    "end": "1287640"
  },
  {
    "text": "symbolic and show you what-- these are the\niterations that you get. So you actually\nsee the iterations",
    "start": "1287640",
    "end": "1294420"
  },
  {
    "text": "towards the square root. And here are the iterations\nof the derivative that's actually being calculated.",
    "start": "1294420",
    "end": "1299580"
  },
  {
    "text": "And the key point here\nis, of course, this is a symbolic computation. We're not doing a\nsymbolic computation.",
    "start": "1299580",
    "end": "1306420"
  },
  {
    "text": "This is mathematically\nequivalent to the function we would get if we were to,\nlike, plot it or something.",
    "start": "1306420",
    "end": "1312309"
  },
  {
    "text": "But of course,\nsymbolic computation is very inefficient. I mean, you get these\nbig coefficients. I mean, look at this number.",
    "start": "1312310",
    "end": "1318387"
  },
  {
    "text": "What is this? 5 million or something? Anyway, you get\nthese big numbers, these even bigger numbers here.",
    "start": "1318387",
    "end": "1324350"
  },
  {
    "text": "Look at these huge\nnumbers, right? It takes a lot of storage\ndragging these x's along.",
    "start": "1324350",
    "end": "1329970"
  },
  {
    "text": "There's a big drag on memory. I mean, this is\nnot the way that-- this is why we do\nnumerical computation.",
    "start": "1329970",
    "end": "1335320"
  },
  {
    "text": "But the Babylonian algorithm,\nin the absence of any round off, is equivalent to computing--",
    "start": "1335320",
    "end": "1341350"
  },
  {
    "text": "above the line, it's computing\nthe square root here. And then below here,\nthese are the iterates",
    "start": "1341350",
    "end": "1347070"
  },
  {
    "text": "towards the derivative. So it's not actually calculating\n1/2 x to the minus 1/2.",
    "start": "1347070",
    "end": "1352440"
  },
  {
    "text": "It's actually doing\nsomething iterative that is approximating\n1/2 x to the minus 1/2.",
    "start": "1352440",
    "end": "1357700"
  },
  {
    "text": "All right. Well, let me tell you now. Let me sort of reveal\nwhat's going on, just so that I can\nkind of show you",
    "start": "1357700",
    "end": "1363823"
  },
  {
    "start": "1358000",
    "end": "1497000"
  },
  {
    "text": "how it's getting the answer. And like I said, it was the\nSVD that sort of convinced me how this was happening.",
    "start": "1363823",
    "end": "1369750"
  },
  {
    "text": "Because the SVD is also\nan iterative algorithm, like this Babylonian\nsquare root. But it's easier to show you\nthe point with the Babylonian",
    "start": "1369750",
    "end": "1375625"
  },
  {
    "text": "square root. So I'm going to do something\nthat I would never want to do, which is explicitly write\na derivative Babylonian",
    "start": "1375625",
    "end": "1382320"
  },
  {
    "text": "algorithm. And what I'm doing\nis I'm going to take the derivative in respect to\nx of every line on my code.",
    "start": "1382320",
    "end": "1387780"
  },
  {
    "text": "So if every even or odd line-- I never know what's\neven or odd anymore. But the original line of\ncode had 1 plus x over 2.",
    "start": "1387780",
    "end": "1394788"
  },
  {
    "text": "Now I'm going to\ntake the derivative. I'll get a half. Here I had this line of code.",
    "start": "1394788",
    "end": "1400140"
  },
  {
    "text": "If I take the derivative\nI'll, use the quotient rule, and this would be\nthe derivative.",
    "start": "1400140",
    "end": "1406529"
  },
  {
    "text": "If I run this code, what\nI'm effectively doing is I'm just using good old\nplus and times and divide,",
    "start": "1406530",
    "end": "1413280"
  },
  {
    "text": "nothing fancy. There's not a square\nroot to be seen. But what I'm doing is,\nas I run my algorithm,",
    "start": "1413280",
    "end": "1419460"
  },
  {
    "text": "I'm also running-- I'm actually computing\nthe derivative as I go.",
    "start": "1419460",
    "end": "1424507"
  },
  {
    "text": "So if I have this\ninfinite algorithm that's going to converge\nto the square roots, the derivative algorithm will\nconverge to the derivative",
    "start": "1424508",
    "end": "1430980"
  },
  {
    "text": "of the square roots. But I'm not using anything\nother than plus, minus, times,",
    "start": "1430980",
    "end": "1436590"
  },
  {
    "text": "and divide to make that happen. So if you rewrite\nany code at all, you could have any\ncode-- iterative,",
    "start": "1436590",
    "end": "1442740"
  },
  {
    "text": "finite, it doesn't matter. If you just take\nthe derivatives back to your variable of\nevery line of your code,",
    "start": "1442740",
    "end": "1448180"
  },
  {
    "text": "then you can get\na derivative out. And as I said, it's not\na symbolic derivative, like, you know, all\nof 18.01, or whatever,",
    "start": "1448180",
    "end": "1454920"
  },
  {
    "text": "wherever we teach\ncalculus these days. And it's not a\nnumerical derivative like in the numerical courses,\nthe 18.3, axyz's, whatever.",
    "start": "1454920",
    "end": "1462650"
  },
  {
    "text": "It's a different beast. It's using the quotient\nrule and the addition rule",
    "start": "1462650",
    "end": "1469049"
  },
  {
    "text": "at every step of the\nway to get the answer. ",
    "start": "1469050",
    "end": "1474059"
  },
  {
    "text": "Here's this\ndBabylonian algorithm. You could see it running. It gives the right answer. Oop, I have to execute the code\nfirst to get the right answer.",
    "start": "1474060",
    "end": "1481140"
  },
  {
    "text": "But if you see, it\ngives the right answer. Oh, I was just in Istanbul and\nthey challenged me to do sine.",
    "start": "1481140",
    "end": "1487799"
  },
  {
    "text": "I forget about that. It's still in my notebook. I did it in front of everybody. It worked. I got a cosine.",
    "start": "1487800",
    "end": "1492980"
  },
  {
    "text": "OK. But let me pass all of that. ",
    "start": "1492980",
    "end": "1498820"
  },
  {
    "text": "So let me go back and tell you\nthen how is this all working. Well, what's happening--\nlet's go back",
    "start": "1498820",
    "end": "1504490"
  },
  {
    "text": "to the eight lines of\ncode, and now, maybe, you can see what's happening.",
    "start": "1504490",
    "end": "1509650"
  },
  {
    "text": "Where's my eight lines of\ncode from the very beginning? And I've got to watch the time. I want to show you this\none other thing, too.",
    "start": "1509650",
    "end": "1515485"
  },
  {
    "text": "So hopefully, I'll have\nenough time to do that. But here, let's see. Where are my eight\nlines of code?",
    "start": "1515485",
    "end": "1521710"
  },
  {
    "text": "Where are they? Here we go. Here are the eight\nlines of code. So what I'm doing is, instead\nof rewriting all your code",
    "start": "1521710",
    "end": "1531070"
  },
  {
    "text": "by taking the derivative of\nevery line the human way, I'm saying that why\ncan't the software just",
    "start": "1531070",
    "end": "1536410"
  },
  {
    "text": "do this in some automatic way? And this is where the automatic\ndifferentiation comes in. And in the old, old days,\nwhen people-- and all",
    "start": "1536410",
    "end": "1542260"
  },
  {
    "text": "the numerical code\nwas in Fortran, there would be the source\nto source translators that would actually input code\nand output derivatives of code.",
    "start": "1542260",
    "end": "1552289"
  },
  {
    "text": "The Julia way, the\nmore modern way, is to let the git compiler\nkind of do that for you. So here, I needed\nplus and divide.",
    "start": "1552290",
    "end": "1559720"
  },
  {
    "text": "Of course, I would want\nto add minus and times. But you just add a couple\nof things and then bump,",
    "start": "1559720",
    "end": "1567670"
  },
  {
    "text": "you don't have to\nrewrite the dBabylonian. Because the Babylonian,\nwith this type, will just do the work for you.",
    "start": "1567670",
    "end": "1574880"
  },
  {
    "text": "OK? And that's where the magic\nof a good piece of software will have it. So you don't have to\nwrite a translator.",
    "start": "1574880",
    "end": "1580030"
  },
  {
    "text": "You don't have to hand write it. You just give the rules and\nyou let the computer do it.",
    "start": "1580030",
    "end": "1585840"
  },
  {
    "text": "Right? And that's what computers\nare supposed to be good at. So that's what's happening. All right.",
    "start": "1585840",
    "end": "1591010"
  },
  {
    "start": "1591000",
    "end": "1661000"
  },
  {
    "text": "So that's forward mode\nautomatic differentiation. I've got 10 minutes\nto go backwards.",
    "start": "1591010",
    "end": "1597410"
  },
  {
    "text": "But let me see if\nthere's any-- anybody have any questions about this? It's really magic, right? But it's pretty wonderful magic.",
    "start": "1597410",
    "end": "1603470"
  },
  {
    "text": "And I don't know what you've\nheard about machine learning, but to be honest, machine\nlearning these days,",
    "start": "1603470",
    "end": "1611710"
  },
  {
    "text": "it's forgetting about whether\nhumans will be useless, which I don't believe by the way.",
    "start": "1611710",
    "end": "1617830"
  },
  {
    "text": "But the big thing\nabout machine learning is that it's really\njust a big optimization.",
    "start": "1617830",
    "end": "1622950"
  },
  {
    "text": "That's all it is, right? One big minimum maximum\nproblem where you've all known from calculus\nthat what you need to do",
    "start": "1622950",
    "end": "1629260"
  },
  {
    "text": "is take derivatives. You know, set them\nto zero, right? In the case of multivariate,\nit's a gradient. You set it to zero.",
    "start": "1629260",
    "end": "1635290"
  },
  {
    "text": "And so really all of\nthis machine learning, all the big stories and\neverything in the end",
    "start": "1635290",
    "end": "1641080"
  },
  {
    "text": "comes down to automatic\ndifferentiation. It's sort of like the\nworkhorse of the whole thing. And so if we could have\na language that gives you",
    "start": "1641080",
    "end": "1647980"
  },
  {
    "text": "that workhorse in a good way,\nthen machine learning really sort of benefits from that. So I hope you all see the big\npicture of machine learning.",
    "start": "1647980",
    "end": "1656260"
  },
  {
    "text": "It really does come down\nto taking derivatives. That's the end-- that's\nhow you optimize.",
    "start": "1656260",
    "end": "1663340"
  },
  {
    "text": "Any quick questions? Otherwise, I'm going\nto switch topics, and I'm going to move\nto the blackboard. Yeah?",
    "start": "1663340",
    "end": "1668640"
  },
  {
    "text": "AUDIENCE: Does the same\nthing happen for second order derivatives as well? ALAN EDELMAN: There is a\ntrick that basically lets",
    "start": "1668640",
    "end": "1674110"
  },
  {
    "text": "you go to higher orders, yeah. You can basically make it\na combo of two first order derivatives.",
    "start": "1674110",
    "end": "1680200"
  },
  {
    "text": "So yeah, it can be done. Did you have a question? AUDIENCE: Yeah. Is this notation of\n[INAUDIBLE],, and is this",
    "start": "1680200",
    "end": "1686257"
  },
  {
    "text": "only really used for computing\ndifferent orders of derivatives or are there other examples? ALAN EDELMAN: Well,\nfor using types?",
    "start": "1686258",
    "end": "1692270"
  },
  {
    "text": "AUDIENCE: Or\nspecifically, I guess, the way that you did through\nthis whole presentation, just this generalized other--",
    "start": "1692270",
    "end": "1699760"
  },
  {
    "text": "ALAN EDELMAN: So it's the\nbiggest trick in the world. It's not this little thing.",
    "start": "1699760",
    "end": "1704950"
  },
  {
    "text": "The idea of making a\ntype to do what you-- I mean, did you see Kronecker\nproducts in this class?",
    "start": "1704950",
    "end": "1711455"
  },
  {
    "text": "AUDIENCE: No. ALAN EDELMAN: No? AUDIENCE: [INAUDIBLE]. ALAN EDELMAN: OK. Let me see. What would you\nhave seen in this?",
    "start": "1711456",
    "end": "1719070"
  },
  {
    "start": "1714000",
    "end": "1875000"
  },
  {
    "text": "Did you see tridiagonal\nmatrices, your favorite? OK. So here.",
    "start": "1719070",
    "end": "1724260"
  },
  {
    "text": "So here's a built in type. Let's say n is-- oh, n doesn't have to be 4.",
    "start": "1724260",
    "end": "1730010"
  },
  {
    "text": "I'm going to create\na strang matrix, if I could spell it right.",
    "start": "1730010",
    "end": "1735360"
  },
  {
    "text": "And it's going to be a\nSymTridiagonal, which is a Julia type.",
    "start": "1735360",
    "end": "1740580"
  },
  {
    "text": "And we will create two\ntimes ones of n and minus",
    "start": "1740580",
    "end": "1746669"
  },
  {
    "text": "ones of n minus 1. Here's a type. I mean, this is built in.",
    "start": "1746670",
    "end": "1752160"
  },
  {
    "text": "But you could have created\nit yourself just as easily. And I don't like calling this--",
    "start": "1752160",
    "end": "1758429"
  },
  {
    "text": "it's certainly not\na dense matrix. And I don't like calling\nit a sparse matrix. I prefer to call it\na structured matrix.",
    "start": "1758430",
    "end": "1764780"
  },
  {
    "text": "Though the word sparse,\nit's a little tricky here. But the reason why I don't like\nto call this a sparse matrix",
    "start": "1764780",
    "end": "1770790"
  },
  {
    "text": "is because we're not\nstoring indices in any-- I mean, there a lot of fancy\nschemes for storing indices",
    "start": "1770790",
    "end": "1776640"
  },
  {
    "text": "for sparse matrices. Well, all we store\nis a diagonal vector. There's the 2s on the diagonal.",
    "start": "1776640",
    "end": "1783179"
  },
  {
    "text": "There's this 4 vector\nwith four twos. And here's a three vector\nfor the off diagonal.",
    "start": "1783180",
    "end": "1788537"
  },
  {
    "text": "And you know, you don't\nhave it twice, by the way. Most sparse matrix structures\nwould have the minus vector",
    "start": "1788537",
    "end": "1796620"
  },
  {
    "text": "twice, the super and the sub. But really, only the core\ninformation that's needed",
    "start": "1796620",
    "end": "1802050"
  },
  {
    "text": "is stored. And in a way, one uses types\nin Julia to basically--",
    "start": "1802050",
    "end": "1809430"
  },
  {
    "text": "you only store what\nyou need, not more. And then you define\nyour operations to work.",
    "start": "1809430",
    "end": "1814870"
  },
  {
    "text": "So for example, if I were to\ntake a strang inverse times,",
    "start": "1814870",
    "end": "1820809"
  },
  {
    "text": "oh, anything, times a random 4. I'm going to do a linear solve.",
    "start": "1820810",
    "end": "1825980"
  },
  {
    "text": "You would want to use a\nspecial [INAUDIBLE] that knew that the matrix was\na symmetric tridiagonal.",
    "start": "1825980",
    "end": "1831149"
  },
  {
    "text": "So it's a big story of being\nable to create types and use them for your own purposes\nwithout any wastage.",
    "start": "1831150",
    "end": "1840240"
  },
  {
    "text": "And this is the sort of thing\nthat while you can do it in languages like Python,\nin MATLAB, if you were",
    "start": "1840240",
    "end": "1846660"
  },
  {
    "text": "able the assembler-- and MATLAB\nwould never let you, Python, you just would\nregret it-- but you",
    "start": "1846660",
    "end": "1852480"
  },
  {
    "text": "would see just how much\noverhead there is in doing this. So there would be\nno performance gain.",
    "start": "1852480",
    "end": "1857800"
  },
  {
    "text": "But in a way, this is\nwhat you want to do. You want to use these things\nto match the mathematics.",
    "start": "1857800",
    "end": "1864270"
  },
  {
    "text": "And so that's really the\nnice thing to be able to do. All right. I only have five minutes. I don't know if I'm\ngoing to pull this off.",
    "start": "1864270",
    "end": "1870292"
  },
  {
    "text": "But let me see if\nI could give you the main idea in five\nminutes of over immersed mode",
    "start": "1870292",
    "end": "1875460"
  },
  {
    "start": "1875000",
    "end": "2290000"
  },
  {
    "text": "differentiations. But here, as long as you are\nfamiliar with neural networks, let me see if I can\ndo this very quickly.",
    "start": "1875460",
    "end": "1882690"
  },
  {
    "text": "I'm going to start with scalars. OK? I'm going to do a neural\nnetwork of all scalars. But only for simplicity,\nfor starters,",
    "start": "1882690",
    "end": "1889086"
  },
  {
    "text": "but I think you're going to\nsee that this can generalize to vectors and matrices, which\nare real neural networks.",
    "start": "1889087",
    "end": "1894790"
  },
  {
    "text": "So what I'm going to\ndo is I want to imagine that we have our inputs.",
    "start": "1894790",
    "end": "1901290"
  },
  {
    "text": "We'll have a bunch of\nscalar weights and biases. So here's W1, and I'll\ngo up to wn and bn.",
    "start": "1901290",
    "end": "1909610"
  },
  {
    "text": "All right? So we have a bunch of\nweights and biases here. OK? And we'll also have an x1,\nwhich will sort of start off",
    "start": "1909610",
    "end": "1920669"
  },
  {
    "text": "our neural network. And we're going to compute-- I'll write it in sort of\nJulia-like or MATLAB-like",
    "start": "1920670",
    "end": "1925740"
  },
  {
    "text": "notation, for i\nequals 1 through n. I will update x by taking some\nfunction of my current input,",
    "start": "1925740",
    "end": "1939150"
  },
  {
    "text": "maybe something like this. And what function h to use? I don't really care too much.",
    "start": "1939150",
    "end": "1944730"
  },
  {
    "text": "In the old days,\npeople used to talk about the sigmoid function. Nowadays, it's the\nmaximum of 0 and t",
    "start": "1944730",
    "end": "1953309"
  },
  {
    "text": "that gets used all the time. It's got this\nridiculous name RELU,",
    "start": "1953310",
    "end": "1958770"
  },
  {
    "text": "which I really can't stand. But anyway, the\nrectified linear unit.",
    "start": "1958770",
    "end": "1963840"
  },
  {
    "text": "But in any event,\nI mean, it's just the function that's t of t is\ngreater than or equal to 0.",
    "start": "1963840",
    "end": "1970570"
  },
  {
    "text": "0, if not. ",
    "start": "1970570",
    "end": "1976350"
  },
  {
    "text": "But whatever function you like. And here I'm just\ngoing to update.",
    "start": "1976350",
    "end": "1981520"
  },
  {
    "text": "OK. And then ultimately, you\nmight also have some data y. And you would like\nto, if everything's",
    "start": "1981520",
    "end": "1987150"
  },
  {
    "text": "a scalar, like I\nsaid, this could be generalized pretty quickly. But what we can do is\nwe can minimize, say,",
    "start": "1987150",
    "end": "1994530"
  },
  {
    "text": "1/2 y minus xm squared. And you're going to want\nto find the data that",
    "start": "1994530",
    "end": "2000200"
  },
  {
    "text": "would minimize that. All this generalizes to\nmatrices and vectors, which is what most neural nets do.",
    "start": "2000200",
    "end": "2007110"
  },
  {
    "text": "OK? And since I'm not going\nto have a lot of time, maybe I can just sort\nof cut to the chase.",
    "start": "2007110",
    "end": "2014000"
  },
  {
    "text": "If I were to differentiate\nthe key line here, I got a little\nbit of Julia here.",
    "start": "2014000",
    "end": "2019170"
  },
  {
    "text": "But if I were to differentiate\nthe key line, what would I write? I would write--\nwell, here, actually, let me use the usual notation.",
    "start": "2019170",
    "end": "2026310"
  },
  {
    "text": "Let me have delta I be the\nh prime of wxi plus bi.",
    "start": "2026310",
    "end": "2032490"
  },
  {
    "text": "OK? So that's delta i. And then you can see that\nthe dxi plus 1 is delta i.",
    "start": "2032490",
    "end": "2041330"
  },
  {
    "text": "And I'll have dwi xi plus\ndxi wi plus dbi would",
    "start": "2041330",
    "end": "2049310"
  },
  {
    "text": "be the differential. This would be how-- so I'm almost done,\nthat's the good news.",
    "start": "2049310",
    "end": "2055760"
  },
  {
    "text": "So if I make a little change-- I like to think of this\nas, like, 0.001 changes. I don't like infinitesimals.",
    "start": "2055760",
    "end": "2061830"
  },
  {
    "text": "I like 0.001. That's how I think of them. But you make a little change\nhere, a little change here, a little change here. You get a change here.",
    "start": "2061830",
    "end": "2067429"
  },
  {
    "text": " You'll get this linear\nthis linear function",
    "start": "2067429",
    "end": "2073580"
  },
  {
    "text": "of the perturbations here\ngives you perturbations here. OK? Well, I've only got one minute.",
    "start": "2073580",
    "end": "2079530"
  },
  {
    "text": "So I'm going to write all\nthis out with linear algebra, because everything is\nbetter when written out with linear algebra. So I'm going to\nwrite down that--",
    "start": "2079530",
    "end": "2087919"
  },
  {
    "text": "I'm going to write\ndown that I'm actually interested in the last element.",
    "start": "2087920",
    "end": "2093800"
  },
  {
    "text": "But dx dn plus 1\nis going to equal and I'm going to have a\ncouple of matrices here.",
    "start": "2093800",
    "end": "2099500"
  },
  {
    "text": "Let me just sort of get\nthe structure right. This will dx2, dxn plus 1 again.",
    "start": "2099500",
    "end": "2106369"
  },
  {
    "text": "Sorry for the squishing. But here-- in fact, I'd\nlike to use block matrices",
    "start": "2106370",
    "end": "2114860"
  },
  {
    "text": "a little bit. So here I'm going\nto have dw1 db1. I'm going to put the bias\ntogether-- sorry for the mess.",
    "start": "2114860",
    "end": "2122650"
  },
  {
    "text": "But dwn dbn. And Julia lets you\nmake block matrices.",
    "start": "2122650",
    "end": "2127755"
  },
  {
    "text": "And you can actually\nuse them directly. There'd be a special\ntype right there. OK? And then what goes\nhere you could actually",
    "start": "2127755",
    "end": "2133730"
  },
  {
    "text": "see what it would be. It would be-- I hope I'm doing it right. But there'd be a delta\n1x1 and a delta NxN",
    "start": "2133730",
    "end": "2140109"
  },
  {
    "text": "And this would be\na diagonal matrix. OK? And then what do\nI have over here? Here I'd have the delta w's.",
    "start": "2140110",
    "end": "2148990"
  },
  {
    "text": "And if you check you'll\nsee that this will be-- I'm not going to get the indices\nright, and I don't have time. So I'm just going to\nwrite it like this.",
    "start": "2148990",
    "end": "2155599"
  },
  {
    "text": "And now I'm just\ngoing to give you the end of the story,\nbecause I've run out of time. You could write\nall this as dx is",
    "start": "2155600",
    "end": "2161090"
  },
  {
    "text": "equal to a diagonal matrix\ntimes the derivative of the parameters plus a\nlower triangle or matrix",
    "start": "2161090",
    "end": "2167660"
  },
  {
    "text": "times the x again. And so if you want\nto solve this, linear algebra just\ndoes the propagation.",
    "start": "2167660",
    "end": "2173359"
  },
  {
    "text": "You have I minus\nL dx is DdP or dx",
    "start": "2173360",
    "end": "2180650"
  },
  {
    "text": "will be I minus L inverse DDP. And if I only want the\nlast element-- let's",
    "start": "2180650",
    "end": "2187190"
  },
  {
    "text": "say en is the vector that\npulls out the last element, then this is all I'm going to\nneed to get all my derivatives.",
    "start": "2187190",
    "end": "2194273"
  },
  {
    "text": "And what's the\nmoral of the story? I apologize for going\none minute over. But the moral of the story is\ninstead of back propagating",
    "start": "2194273",
    "end": "2200930"
  },
  {
    "text": "through your own hard\nwork, you probably know that when you solve\na lower triangular matrix,",
    "start": "2200930",
    "end": "2207830"
  },
  {
    "text": "people will read written code\nthat back solves the lower triangular matrix. The back, the big back\npiece, has already",
    "start": "2207830",
    "end": "2214670"
  },
  {
    "text": "been implemented for you. Why reinvent the wheel in if\nthe back-- if linear algebra",
    "start": "2214670",
    "end": "2220340"
  },
  {
    "text": "already has the back, you see? And so if you just do\nthis, and you do it in a language that lets\nyou get full performance,",
    "start": "2220340",
    "end": "2227240"
  },
  {
    "text": "you don't need to do\nyour own backpropagation. Because a simple backslash\nwill do it for you.",
    "start": "2227240",
    "end": "2232750"
  },
  {
    "text": "So I apologize for going over. I don't know if Professor\nStrang had some final words. But anyway, linear algebra\nis the secret to everything.",
    "start": "2232750",
    "end": "2240369"
  },
  {
    "text": "That's the big message. AUDIENCE: OK. [APPLAUSE]",
    "start": "2240370",
    "end": "2246390"
  },
  {
    "text": " GILBERT STRANG: Well, since it's\nour last two minutes, or minus",
    "start": "2246390",
    "end": "2254770"
  },
  {
    "text": "two minutes of 18.065. I hope you guys enjoyed it.",
    "start": "2254770",
    "end": "2260900"
  },
  {
    "text": "I certainly enjoyed\nit, as you could tell. Teaching this class,\nseeing how it would go,",
    "start": "2260900",
    "end": "2267579"
  },
  {
    "text": "and writing about it. So I'll let you know\nas about the writing.",
    "start": "2267580",
    "end": "2273250"
  },
  {
    "text": "And meanwhile, I'll\nget your writing on the projects, which\nI appreciate very much.",
    "start": "2273250",
    "end": "2278950"
  },
  {
    "text": "And of course, grades are\ngoing to come out well. And I hope you've enjoyed it.",
    "start": "2278950",
    "end": "2284859"
  },
  {
    "text": "So thank you all. You're right. Thanks. [APPLAUSE] ",
    "start": "2284860",
    "end": "2290949"
  }
]