[
  {
    "text": " The following\ncontent is provided under a Creative\nCommons license. Your support will help MIT\nOpenCourseWare continue",
    "start": "0",
    "end": "6870"
  },
  {
    "text": "to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6870",
    "end": "13339"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13340",
    "end": "20365"
  },
  {
    "text": "PROFESSOR: We'll begin this time\nby looking at some probability distribution that you\nshould be familiar",
    "start": "20365",
    "end": "27310"
  },
  {
    "text": "with from this\nperspective, starting with a Gaussian distribution\nfor one variable.",
    "start": "27310",
    "end": "33780"
  },
  {
    "start": "33780",
    "end": "42100"
  },
  {
    "text": "We're focused on a\nvariable that takes real values in the interval\nminus infinity to infinity",
    "start": "42100",
    "end": "48670"
  },
  {
    "text": "and the Gaussian has the\nform exponential that is centered around some\nvalue, let's call it lambda,",
    "start": "48670",
    "end": "57450"
  },
  {
    "text": "and has fluctuations around this\nvalue parameterized by sigma. And the integral of\nthis p over the interval",
    "start": "57450",
    "end": "67940"
  },
  {
    "text": "should be normalized\nto unity, giving you this hopefully very\nfamily of form.",
    "start": "67940",
    "end": "76780"
  },
  {
    "text": "Now, if you want to characterize\nthe characteristic function,",
    "start": "76780",
    "end": "83540"
  },
  {
    "text": "all we need to do is to\nFourier transform this. So I have the integral\ndx e the minus ikx.",
    "start": "83540",
    "end": "96580"
  },
  {
    "text": "So this-- let's remind\nyou alternatively was the expectation value\nof e to the minus ikx.",
    "start": "96580",
    "end": "102560"
  },
  {
    "text": " minus x minus lambda squared\nover 2 sigma squared,",
    "start": "102560",
    "end": "111310"
  },
  {
    "text": "which is the probably\ndistribution. And you should know what\nthe answer to that is,",
    "start": "111310",
    "end": "119220"
  },
  {
    "text": "but I will remind. You can change variables to\nx minus lambda [INAUDIBLE] y.",
    "start": "119220",
    "end": "125730"
  },
  {
    "text": "So from here we will\nget the factor of 2 to the minus ik lambda.",
    "start": "125730",
    "end": "130780"
  },
  {
    "text": "You have then the\nintegral over y minus y",
    "start": "130780",
    "end": "139050"
  },
  {
    "text": "squared over 2 sigma squared. And then what we need to do is\nto complete this square over",
    "start": "139050",
    "end": "149530"
  },
  {
    "text": "here. And you can do\nthat, essentially,",
    "start": "149530",
    "end": "154890"
  },
  {
    "text": "by adding and\nsubtracting a minus",
    "start": "154890",
    "end": "160300"
  },
  {
    "text": "k squared sigma squared over 2.",
    "start": "160300",
    "end": "165320"
  },
  {
    "text": "So that if I change variable\nto y plus ik sigma squared,",
    "start": "165320",
    "end": "173210"
  },
  {
    "text": "let's call that z, then I\nhave outside the integral e",
    "start": "173210",
    "end": "180910"
  },
  {
    "text": "to the minus ik lambda minus k\nsquared sigma squared over 2.",
    "start": "180910",
    "end": "190595"
  },
  {
    "text": "And the remainder I can\nwrite as a full square. ",
    "start": "190595",
    "end": "201800"
  },
  {
    "text": "And this is just a\nnormalized Gaussian integral that comes to 1.",
    "start": "201800",
    "end": "209500"
  },
  {
    "text": "So as you well know, a Fourier\ntransform of a Gaussian",
    "start": "209500",
    "end": "217260"
  },
  {
    "text": "is itself a Gaussian, and\nthat's what we've established. E to the minus ik lambda minus\nk squared sigma squared over 2m.",
    "start": "217260",
    "end": "228350"
  },
  {
    "text": "And if I haven't made a mistake\nwhen I said k equals to 0, the answer should be 1 because\nk equals to 0 expectation",
    "start": "228350",
    "end": "236070"
  },
  {
    "text": "value of 1 just amounts\nto normalization. ",
    "start": "236070",
    "end": "241100"
  },
  {
    "text": "Now what we said was that\na more interesting function",
    "start": "241100",
    "end": "246850"
  },
  {
    "text": "is obtained by taking\nthe log of this. So from here we go to the\nlog of [INAUDIBLE] of k.",
    "start": "246850",
    "end": "256970"
  },
  {
    "text": "Log of [INAUDIBLE] of k is\nvery simple for the Gaussian.",
    "start": "256970",
    "end": "262090"
  },
  {
    "start": "262090",
    "end": "268610"
  },
  {
    "text": "And what we had said\nwas that by definition",
    "start": "268610",
    "end": "275580"
  },
  {
    "text": "this log of the\ncharacteristic function generates cumulants\nthrough the series",
    "start": "275580",
    "end": "283509"
  },
  {
    "text": "minus ik to the power of n over\nn factorial, the end cumulant.",
    "start": "283510",
    "end": "289990"
  },
  {
    "text": " So looking at this,\nwe can immediately",
    "start": "289990",
    "end": "297509"
  },
  {
    "text": "see that the Guassian\nis characterized by a first cumulant, which is\nthe coefficient of minus ik.",
    "start": "297510",
    "end": "307030"
  },
  {
    "text": "It's lambda. ",
    "start": "307030",
    "end": "312440"
  },
  {
    "text": "It is characterized by\na second cumulant, which is the coefficient\nof minus ik squared.",
    "start": "312440",
    "end": "320479"
  },
  {
    "text": "This, you know, is the variance. And we can explicitly\nsee that the coefficient of minus ik squared\nover 2 factorial",
    "start": "320480",
    "end": "329040"
  },
  {
    "text": "is simply sigma squared. So this is reputation.",
    "start": "329040",
    "end": "334920"
  },
  {
    "text": "But one thing that\nis interesting is that our series\nhas now terminates,",
    "start": "334920",
    "end": "340880"
  },
  {
    "text": "which means that\nif I were to look at the third cumulant, if I were\nto look at the fourth cumulant,",
    "start": "340880",
    "end": "348280"
  },
  {
    "text": "and so forth, for the\nGuassian, they're all 0. So the Gaussian is\nthe distribution",
    "start": "348280",
    "end": "354730"
  },
  {
    "text": "that is completely\ncharacterized just by its first and second\ncumulants, all the rest",
    "start": "354730",
    "end": "360940"
  },
  {
    "text": "being 0.  So, now our last\ntime we developed",
    "start": "360940",
    "end": "369669"
  },
  {
    "text": "some kind of a graphical method. We said that I can graphically\ndescribe the first cumulant",
    "start": "369670",
    "end": "377840"
  },
  {
    "text": "as a bag with one point in it. Second cumulant with something\nthat has two points in it.",
    "start": "377840",
    "end": "386121"
  },
  {
    "text": "A third cumulant\nwith three three, fourth cumulant with four\nand four points and so forth.",
    "start": "386122",
    "end": "396220"
  },
  {
    "text": "This is just rewriting. Now, the interesting\nthing was that we said that the various moments\nwe could express graphically.",
    "start": "396220",
    "end": "407490"
  },
  {
    "text": "So that, for example,\nthe second moment is either this or this,\nwhich then graphically",
    "start": "407490",
    "end": "416760"
  },
  {
    "text": "is the same thing as lambda\nsquared plus sigma squared because this is indicated\nby sigma squared.",
    "start": "416760",
    "end": "426440"
  },
  {
    "text": "Now, x cubed you\nwould say is either",
    "start": "426440",
    "end": "432790"
  },
  {
    "text": "three things by themselves\nor put two of them together and then one separate.",
    "start": "432790",
    "end": "439610"
  },
  {
    "text": "And this I could do in\nthree different ways. And in general, for a\ngeneral distribution,",
    "start": "439610",
    "end": "446090"
  },
  {
    "text": "I would have had another\nterm, which is a triangle. But the triangle is 0.",
    "start": "446090",
    "end": "452710"
  },
  {
    "text": "So for the Gaussian,\nthis terminates here. I have lambda cubed plus\n3 lambda sigma squared.",
    "start": "452710",
    "end": "460780"
  },
  {
    "text": "If I want to calculate\nx to the fourth, maybe the old way\nof doing it would",
    "start": "460780",
    "end": "466560"
  },
  {
    "text": "have been too multiply the\nGaussian distribution against x to the fourth and try\nto do the integration.",
    "start": "466560",
    "end": "473620"
  },
  {
    "text": "And you would ultimately be able\nto do that rearranging things",
    "start": "473620",
    "end": "479060"
  },
  {
    "text": "and looking at the various\npowers of the Gaussian integrated from minus\ninfinity to infinity.",
    "start": "479060",
    "end": "485565"
  },
  {
    "text": "But you can do it graphically. You can say, OK. It's either this or\nI can have-- well,",
    "start": "485566",
    "end": "494160"
  },
  {
    "text": "I cannot put one aside\nand three together, because that doesn't exist. I could have two together\nand two not together.",
    "start": "494160",
    "end": "505330"
  },
  {
    "text": "And this I can do in\nsix different ways. You can convince\nyourself of that.",
    "start": "505330",
    "end": "511130"
  },
  {
    "text": "What I could do two pairs, which\nI can do three different ways",
    "start": "511130",
    "end": "516210"
  },
  {
    "text": "because I can either do one,\ntwo; one, three; one, four and then the other is satisfied.",
    "start": "516210",
    "end": "522360"
  },
  {
    "text": "So this is lambda to the\nfourth plus 6 lambda squared sigma squared plus 3\nsigma to the fourth.",
    "start": "522360",
    "end": "533130"
  },
  {
    "text": "And you can keep going and\ndoing different things. ",
    "start": "533130",
    "end": "539800"
  },
  {
    "text": "OK. Question?  Yeah?",
    "start": "539800",
    "end": "545278"
  },
  {
    "text": "AUDIENCE: Is the\nsecond-- [INAUDIBLE]. ",
    "start": "545278",
    "end": "553182"
  },
  {
    "text": "PROFESSOR: There? AUDIENCE: Because-- so you said\nthat the second cumulative-- PROFESSOR: Oh. AUDIENCE: --x sqaured.",
    "start": "553182",
    "end": "559089"
  },
  {
    "text": "Yes. PROFESSOR: Yes. So that's the wrong-- AUDIENCE: [INAUDIBLE]. PROFESSOR: The coefficient of k\nsquared is the second cumulant.",
    "start": "559090",
    "end": "566520"
  },
  {
    "text": "The additional 2 was a mistake. ",
    "start": "566520",
    "end": "574178"
  },
  {
    "text": "OK. Anything else? ",
    "start": "574178",
    "end": "580790"
  },
  {
    "text": "All right. Let's take a look at couple\nof other distributions,",
    "start": "580790",
    "end": "586979"
  },
  {
    "text": "this time discrete. ",
    "start": "586979",
    "end": "592760"
  },
  {
    "text": "So the binomial distribution\nis repeat a binary random",
    "start": "592760",
    "end": "607170"
  },
  {
    "text": "variable. And what does this mean?",
    "start": "607170",
    "end": "613110"
  },
  {
    "text": "It means two outcomes\nthat's binary,",
    "start": "613110",
    "end": "621500"
  },
  {
    "text": "let's call them\nA and B. And if I",
    "start": "621500",
    "end": "627970"
  },
  {
    "text": "have a coin that's head\nor tails, it's binary. Two possibilities.",
    "start": "627970",
    "end": "633389"
  },
  {
    "text": "And I can assign probabilities\nto the two outcomes",
    "start": "633390",
    "end": "638890"
  },
  {
    "text": "to PA and PB, which\nhas to be 1 minus PA.",
    "start": "638890",
    "end": "644854"
  },
  {
    "text": " And the question\nis if you repeat",
    "start": "644854",
    "end": "652160"
  },
  {
    "text": "this binary random\nvariables n times,",
    "start": "652160",
    "end": "663500"
  },
  {
    "text": "what is the probability\nof NA outcomes of A?",
    "start": "663500",
    "end": "677360"
  },
  {
    "start": "677360",
    "end": "683640"
  },
  {
    "text": "And I forget to say\nsomething important, so I write it in red. This should be independent.",
    "start": "683640",
    "end": "690160"
  },
  {
    "text": " That is the outcome of\na coin toss at, say,",
    "start": "690160",
    "end": "701140"
  },
  {
    "text": "the fifth time\nshould not influence the sixth time and future times. OK.",
    "start": "701140",
    "end": "706630"
  },
  {
    "text": "So this is easy. The probability to have NA\noccurrences of A in N trials.",
    "start": "706630",
    "end": "716010"
  },
  {
    "text": "So it has to be index yn. Is that within the n times that\nI tossed, A came up NA times.",
    "start": "716010",
    "end": "727490"
  },
  {
    "text": "So it has to be proportional\nto the probability of A independently multiplied\nby itself NA times.",
    "start": "727490",
    "end": "738870"
  },
  {
    "text": "But if I have exactly\nNA occurrences of A, all the other times\nI had B occurring.",
    "start": "738870",
    "end": "750950"
  },
  {
    "text": "So I have the probability\nof B for the remainder, which is N minus NA.",
    "start": "750950",
    "end": "757459"
  },
  {
    "text": "Now this is the probability\nfor a specific occurrence, like the first NA times that\nI threw the coin I will get A.",
    "start": "757460",
    "end": "765509"
  },
  {
    "text": "The remaining times\nI would get B. But the order is not\nimportant and the number",
    "start": "765510",
    "end": "771550"
  },
  {
    "text": "of ways that I can\nshuffle the order and have a total of NA out of\nN times is the binomial factor.",
    "start": "771550",
    "end": "779480"
  },
  {
    "start": "779480",
    "end": "788310"
  },
  {
    "text": "Fine. Again, well known. Let's look at its\ncharacteristic function. ",
    "start": "788310",
    "end": "795470"
  },
  {
    "text": "So p tilde, which is\nnow a function of k,",
    "start": "795470",
    "end": "800529"
  },
  {
    "text": "is expectation value\nof e to the minus ik NA, which means that I\nhave to weigh e minus ik",
    "start": "800530",
    "end": "812060"
  },
  {
    "text": "NA against the probability\nof occurrences of NA times,",
    "start": "812060",
    "end": "819660"
  },
  {
    "text": "which is this binomial factor\nPA to the power of NA, PB",
    "start": "819660",
    "end": "833004"
  },
  {
    "text": "to the power of N minus NA. And of course, I have to sum\nover all possible values of NA",
    "start": "833005",
    "end": "841450"
  },
  {
    "text": "that go all the way from 0 to N.",
    "start": "841450",
    "end": "849500"
  },
  {
    "text": "So what we have\nhere is something which is this combination,\nPA e to the minus ik",
    "start": "849500",
    "end": "858505"
  },
  {
    "text": "raised to the power of NA. PB raised to the complement\nN minus NA multiplied",
    "start": "858505",
    "end": "866399"
  },
  {
    "text": "by the binomial factor summed\nover all possible values. So this is just the definition\nof the binomial expansion of PA",
    "start": "866400",
    "end": "876610"
  },
  {
    "text": "e to the minus ik plus PB\nraised to the power of N.",
    "start": "876610",
    "end": "886870"
  },
  {
    "text": "And again, let's check. If I set k equals to\n0, I have PA plus PB, which is 1 raised to the\npower of N. So things are OK.",
    "start": "886870",
    "end": "894580"
  },
  {
    "text": "So this is the\ncharacteristic function. At this stage, the only thing\nthat I will note about this",
    "start": "894580",
    "end": "902600"
  },
  {
    "text": "is that if I look at the\ncharacteristic function",
    "start": "902600",
    "end": "909069"
  },
  {
    "text": "I will get N times. So this is-- actually,\nlet's make sure",
    "start": "909070",
    "end": "915420"
  },
  {
    "text": "that we maintain the\nindex N. So this is the characteristic function\nappropriate to N trials.",
    "start": "915420",
    "end": "923940"
  },
  {
    "text": "And what I get is that\nup to factor of N, I will get the\ncharacteristic function",
    "start": "923940",
    "end": "929320"
  },
  {
    "text": "that would be\nappropriate to one trial. ",
    "start": "929320",
    "end": "935600"
  },
  {
    "text": "So what that means\nif I were to look at powers of k, the expectation\nvalue of some cumulant,",
    "start": "935600",
    "end": "946649"
  },
  {
    "text": "if I go to repeat things\nN times-- so this carries",
    "start": "946650",
    "end": "952120"
  },
  {
    "text": "an index N. It is going to be\nsimply N times what I would",
    "start": "952120",
    "end": "958820"
  },
  {
    "text": "have had in a single trial. ",
    "start": "958820",
    "end": "965470"
  },
  {
    "text": "So for a single\ntrial, you really have two outcomes-- 0 or 1\noccurrences of this object.",
    "start": "965470",
    "end": "973330"
  },
  {
    "text": "So for a binary variable,\nyou can really easily compute these quantities\nand then you can",
    "start": "973330",
    "end": "979139"
  },
  {
    "text": "calculate the corresponding\nones for N trials simply",
    "start": "979140",
    "end": "984640"
  },
  {
    "text": "multiplying by N.\nAnd we will see that this is\ncharacteristic, essentially,",
    "start": "984640",
    "end": "990949"
  },
  {
    "text": "of anything that is repeated N\ntimes, not just the binomial.",
    "start": "990950",
    "end": "997090"
  },
  {
    "text": "So this form that you have\nN independent objects, you would get N\ntimes what you would",
    "start": "997090",
    "end": "1002620"
  },
  {
    "text": "have for one object\nis generally valid and actually something\nthat we will build",
    "start": "1002620",
    "end": "1008020"
  },
  {
    "text": "a lot of statistical\nmechanics on because we are interested in\nthe [INAUDIBLE].",
    "start": "1008020",
    "end": "1013710"
  },
  {
    "text": "So we will see that shortly. But rather than\nfollowing this, let's",
    "start": "1013710",
    "end": "1019070"
  },
  {
    "text": "look at a third distribution\nthat is closely related, which is the Poisson.",
    "start": "1019070",
    "end": "1024089"
  },
  {
    "start": "1024089",
    "end": "1029990"
  },
  {
    "text": "And the question\nthat we are asking is-- we have an interval.",
    "start": "1029990",
    "end": "1037819"
  },
  {
    "text": "And the question is,\nwhat is the probability",
    "start": "1037819",
    "end": "1044422"
  },
  {
    "text": "of m events in an\ninterval from 0 to T?",
    "start": "1044422",
    "end": "1059070"
  },
  {
    "text": "And I kind of\nexpressed it this way because prototypical Poisson\ndistribution is, let's say,",
    "start": "1059070",
    "end": "1067680"
  },
  {
    "text": "the radioactivity. And you can be waiting for\nsome time interval from 0",
    "start": "1067680",
    "end": "1072860"
  },
  {
    "text": "to 1 minute and asking within\nthat time interval, what's the probability that you will\nsee m radioactive decay events?",
    "start": "1072860",
    "end": "1082410"
  },
  {
    "text": "So what is the probability\nif two things happen?",
    "start": "1082410",
    "end": "1088470"
  },
  {
    "text": "One is that the probability of 1\nand only 1 event in interval dt",
    "start": "1088470",
    "end": "1108090"
  },
  {
    "text": "is alpha dt as dt goes to 0. ",
    "start": "1108090",
    "end": "1114830"
  },
  {
    "text": "OK? So basically if you look\nat this over 1 minute,",
    "start": "1114831",
    "end": "1120760"
  },
  {
    "text": "the chances are that you\nwill see so many events, so many radioactivities.",
    "start": "1120760",
    "end": "1127540"
  },
  {
    "text": "If you shorten the\ninterval, the chances that you would see events\nwould become less and less.",
    "start": "1127540",
    "end": "1133330"
  },
  {
    "text": "If you make your\nevent infinitesimal, most of the time\nnothing would happen",
    "start": "1133330",
    "end": "1138649"
  },
  {
    "text": "with very small\nprobability that vanishes. As the size of the interval\ngoes to 0, you will see 1 event.",
    "start": "1138650",
    "end": "1148090"
  },
  {
    "text": "So this is one condition. And the second condition is\nevents in different intervals",
    "start": "1148090",
    "end": "1156589"
  },
  {
    "text": "are independent. ",
    "start": "1156589",
    "end": "1163430"
  },
  {
    "text": "And since I wrote\nindependent in red up there, let me write it in red here\nbecause it sort of harks back",
    "start": "1163430",
    "end": "1171084"
  },
  {
    "text": "to the same condition.  And so this is the question.",
    "start": "1171084",
    "end": "1178013"
  },
  {
    "text": "What is this probability?  And the to get the\nanswer, what we do",
    "start": "1178014",
    "end": "1187630"
  },
  {
    "text": "is to subdivide our\nbig interval into N,",
    "start": "1187630",
    "end": "1202240"
  },
  {
    "text": "which is big T divided by\nthe small dt subintervals.",
    "start": "1202240",
    "end": "1208130"
  },
  {
    "start": "1208130",
    "end": "1213330"
  },
  {
    "text": "So basically, originally\nlet's say on the time axis,",
    "start": "1213330",
    "end": "1219515"
  },
  {
    "text": "we were covering a distance\nthat went from 0 to big T and we were asking\nwhat happens here.",
    "start": "1219515",
    "end": "1227240"
  },
  {
    "text": "So what we are\ndoing now is we are sort of dividing this interval\nto lots of subintervals,",
    "start": "1227240",
    "end": "1237320"
  },
  {
    "text": "the size of each one\nof them being dt. And therefore, the total\nnumber is big T over dt.",
    "start": "1237320",
    "end": "1246950"
  },
  {
    "text": "And ultimately,\nclearly, I want to sit dt going to 0 so that this\ncondition is satisfied.",
    "start": "1246950",
    "end": "1255100"
  },
  {
    "text": "So also because of the second\ncondition, each one of these will independently tell me\nwhether or not I have an event.",
    "start": "1255100",
    "end": "1265220"
  },
  {
    "text": "And so if I want to count\nthe total number of events, I have to add things\nthat are occurring",
    "start": "1265220",
    "end": "1271404"
  },
  {
    "text": "in different intervals. And we can see that\nthis problem now became identical to that\nproblem because each one",
    "start": "1271405",
    "end": "1278790"
  },
  {
    "text": "of these intervals has two\npossible outcomes-- nothing happens with probability\n1 minus alpha dt,",
    "start": "1278790",
    "end": "1286370"
  },
  {
    "text": "something happens with\nprobability alpha dt. So no event is probability\n1 minus alpha dt.",
    "start": "1286370",
    "end": "1298210"
  },
  {
    "text": "One event means\nprobability alpha dt. So this is a binomial process.",
    "start": "1298210",
    "end": "1304570"
  },
  {
    "start": "1304570",
    "end": "1310880"
  },
  {
    "text": "So we can calculate,\nfor example,",
    "start": "1310880",
    "end": "1315930"
  },
  {
    "text": "the characteristic function. And I will indicate\nthat we are looking",
    "start": "1315930",
    "end": "1322509"
  },
  {
    "text": "at some interval of\nsize T and parameterized by this straight\nalpha, we'll see",
    "start": "1322510",
    "end": "1329679"
  },
  {
    "text": "that only the\nproduct will occur.  So this is this before\nFourier variable.",
    "start": "1329680",
    "end": "1338230"
  },
  {
    "text": "We said that it's\na binary, so it is one of the probabilities\nplus e to the minus ik",
    "start": "1338230",
    "end": "1347419"
  },
  {
    "text": "plus the other probability\nraised to the power of N. Now we just substitute\nthe probabilities that we",
    "start": "1347420",
    "end": "1353690"
  },
  {
    "text": "have over here. So the probability of not having\nan event is 1 minus alpha dt.",
    "start": "1353690",
    "end": "1363809"
  },
  {
    "text": "The probability of having\nan event is alpha dt. So alpha dt is going\nto appear here as e",
    "start": "1363810",
    "end": "1373500"
  },
  {
    "text": "to the minus ik minus 1. So alpha st, e to the\nminus ik, is from here.",
    "start": "1373500",
    "end": "1382620"
  },
  {
    "text": "From PB I will get\n1 minus alpha dt. And I bunched\ntogether the two terms",
    "start": "1382620",
    "end": "1389470"
  },
  {
    "text": "that are proportional\nto alpha dt. And then I have to raise\nto the power of N, which",
    "start": "1389470",
    "end": "1395480"
  },
  {
    "text": "is T divided by dt. ",
    "start": "1395480",
    "end": "1401190"
  },
  {
    "text": "And this whole prescription\nis valid in the limit",
    "start": "1401190",
    "end": "1406230"
  },
  {
    "text": "where dt is going to 0. So what you have is 1\nplus an infinitesimal",
    "start": "1406230",
    "end": "1415160"
  },
  {
    "text": "raised to a huge power. And this limiting procedure\nis equivalent to taking",
    "start": "1415160",
    "end": "1422039"
  },
  {
    "text": "the exponential. So basically this\nis the same thing as exponential of what is here\nmultiplied by what is here.",
    "start": "1422040",
    "end": "1430770"
  },
  {
    "text": "The dt's cancel each\nother out and the answer is alpha T e to the\nminus ik minus 1.",
    "start": "1430770",
    "end": "1438670"
  },
  {
    "text": " So the characteristic\nfunction for this process",
    "start": "1438670",
    "end": "1445490"
  },
  {
    "text": "that we described is\nsimply given by this form. ",
    "start": "1445490",
    "end": "1460169"
  },
  {
    "text": "You say, wait. I didn't ask for the\ncharacteristic function. I wanted the probability.",
    "start": "1460170",
    "end": "1465970"
  },
  {
    "text": "Well, I say, OK. Characteristic function is\nsimply the Fourier transform. So let me Fourier\ntransform back,",
    "start": "1465970",
    "end": "1473160"
  },
  {
    "text": "and I would say that the\nprobability along not",
    "start": "1473160",
    "end": "1479850"
  },
  {
    "text": "the Fourier axis\nbut the actual axis is open by the inverse\nFourier process.",
    "start": "1479850",
    "end": "1485539"
  },
  {
    "text": "So I have to do an\nintegral dk over 2 pi e to the ikx times the\ncharacteristic function.",
    "start": "1485540",
    "end": "1494040"
  },
  {
    "text": "And the characteristic function\nis e to minus-- what was it?",
    "start": "1494040",
    "end": "1499510"
  },
  {
    "text": "e to the alpha T. E to\nthe minus ik k minus 1.",
    "start": "1499510",
    "end": "1511330"
  },
  {
    "text": " Well, there is an\nequal to minus alpha",
    "start": "1511330",
    "end": "1517770"
  },
  {
    "text": "T that I can simply take\noutside the integration. I have the integration\nover k e to that ikx.",
    "start": "1517770",
    "end": "1527950"
  },
  {
    "text": " And then what I will do\nis I have this factor of e",
    "start": "1527950",
    "end": "1534050"
  },
  {
    "text": "to the something in the-- e to\nthe alpha T to the minus ik. I will use the expansion\nof the exponential.",
    "start": "1534050",
    "end": "1542529"
  },
  {
    "text": "So the expansion\nof the exponential is a sum over m running\nfrom 0 to infinity.",
    "start": "1542530",
    "end": "1548340"
  },
  {
    "text": "The exponent raised\nto the m-th power. So I have alpha T raised\nto the m-th power,",
    "start": "1548340",
    "end": "1554480"
  },
  {
    "text": "e to the minus ik raised\nto the m-th power divided m factor here. ",
    "start": "1554480",
    "end": "1564149"
  },
  {
    "text": "So now I reorder the\nsum and the integration. The sum is over m, the\nintegration is over k.",
    "start": "1564150",
    "end": "1571900"
  },
  {
    "text": "I can reorder them. So on the things\nthat go outside I have a sum over m running from\n0 to infinity e to the minus",
    "start": "1571900",
    "end": "1582270"
  },
  {
    "text": "alpha T, alpha T to the power\nof m divided by m factorial.",
    "start": "1582270",
    "end": "1588800"
  },
  {
    "text": "Then I have the integral\nover k over 2 pi e to the ik.",
    "start": "1588800",
    "end": "1595790"
  },
  {
    "text": "Well, I had the x here and I\nhave e to the minus ikm here. So I have x minus m.",
    "start": "1595790",
    "end": "1602610"
  },
  {
    "text": " And then I say, OK, this is\nan integral that I recognize.",
    "start": "1602610",
    "end": "1610970"
  },
  {
    "text": "The integral of e to\nthe ik times something is simply a delta function.",
    "start": "1610970",
    "end": "1617940"
  },
  {
    "text": "So this whole thing\nis a delta function that's says, oh, x\nhas to be an integer.",
    "start": "1617940",
    "end": "1627320"
  },
  {
    "text": "Because I kind of did something\nthat maybe, in retrospect,",
    "start": "1627320",
    "end": "1632429"
  },
  {
    "text": "you would have said\nwhy are you doing this. Because along how many\ntimes things have occurred,",
    "start": "1632430",
    "end": "1640600"
  },
  {
    "text": "they have either occurred\n0 times, 1 times, 2 decays, 3 decays. I don't have 2.5 decays.",
    "start": "1640600",
    "end": "1648040"
  },
  {
    "text": "So I treated x as a\ncontinuous variable, but the mathematics was really\nclever enough to say that, no.",
    "start": "1648040",
    "end": "1655800"
  },
  {
    "text": "The only places that you can\nhave are really integer values. And the probability that you\nhave some particular value",
    "start": "1655800",
    "end": "1665230"
  },
  {
    "text": "integer m is simply what we have\nover here, e to the minus alpha",
    "start": "1665230",
    "end": "1671020"
  },
  {
    "text": "T alpha T to the power of m\ndivided by m factorial, which",
    "start": "1671020",
    "end": "1676954"
  },
  {
    "text": "is the Poisson distribution. ",
    "start": "1676954",
    "end": "1687809"
  },
  {
    "text": "OK. ",
    "start": "1687810",
    "end": "1693750"
  },
  {
    "text": "But fine. So this is the\nPoisson distribution, but really we go\nthrough the root",
    "start": "1693750",
    "end": "1701450"
  },
  {
    "text": "of the characteristic\nfunction in order to use this machinery\nthat we developed earlier",
    "start": "1701450",
    "end": "1708430"
  },
  {
    "text": "for cumulants et cetera. So let's look at the\ncumulant generating function.",
    "start": "1708430",
    "end": "1714620"
  },
  {
    "text": "So I have to take the\nlog of the function",
    "start": "1714620",
    "end": "1720490"
  },
  {
    "text": "that I had calculated there. It is nicely in the\nexponential, so I",
    "start": "1720490",
    "end": "1726850"
  },
  {
    "text": "get alpha T e to the\nminus ik minus 1.",
    "start": "1726850",
    "end": "1731950"
  },
  {
    "text": " So now I can make an expansion\nof this in powers of k",
    "start": "1731950",
    "end": "1741509"
  },
  {
    "text": "so I can expand the exponential. The first term vanishes\nbecause this starts with one.",
    "start": "1741510",
    "end": "1747940"
  },
  {
    "text": "So really I have alpha\nT sum running from 1",
    "start": "1747940",
    "end": "1753440"
  },
  {
    "text": "to infinity or N running\nfrom 1 to infinity minus ik",
    "start": "1753440",
    "end": "1758509"
  },
  {
    "text": "to the power of n\nover n factorial. ",
    "start": "1758510",
    "end": "1768230"
  },
  {
    "text": "So my task for\nidentifying the cumulants",
    "start": "1768230",
    "end": "1775679"
  },
  {
    "text": "is to look at the\nexpansion of this log and read off powers of minus\nikn to the power of n factorial.",
    "start": "1775680",
    "end": "1784510"
  },
  {
    "text": "So what do we see? We see that the first cumulant\nof the Poisson is alpha T,",
    "start": "1784510",
    "end": "1793809"
  },
  {
    "text": "but all the coefficients\nare the same thing. The expectation value-- sorry.",
    "start": "1793810",
    "end": "1799260"
  },
  {
    "text": "The second cumulant is\nalpha T. The third cumulant, the fourth cumulant,\nall the other cumulants",
    "start": "1799260",
    "end": "1806309"
  },
  {
    "text": "are also alpha T.",
    "start": "1806310",
    "end": "1815090"
  },
  {
    "text": "So the average number of decays\nthat you see in the interval is simply alpha T. But\nthere are fluctuations,",
    "start": "1815090",
    "end": "1823259"
  },
  {
    "text": "and if somebody\nshould, for example, ask you what's the average\nnumber cubed of events,",
    "start": "1823260",
    "end": "1830660"
  },
  {
    "text": "you would say, OK. I'm going to use the\nrelationship between moments",
    "start": "1830660",
    "end": "1835750"
  },
  {
    "text": "and cumulants. I can either have\nthree first objects",
    "start": "1835750",
    "end": "1842830"
  },
  {
    "text": "or I can put one of\nthem separate in three different factions.",
    "start": "1842830",
    "end": "1850010"
  },
  {
    "text": "But this is a case where\nthe triangle is allowed,",
    "start": "1850010",
    "end": "1856280"
  },
  {
    "text": "so diagrammatically\nall three are possible. And so the answer for the\nfirst term is alpha T cubed.",
    "start": "1856280",
    "end": "1863640"
  },
  {
    "text": " For the second term,\nit is a factor of 3.",
    "start": "1863640",
    "end": "1869620"
  },
  {
    "text": "Both this variance and the mean\ngive me a factor of alpha T, so I will get alpha T squared.",
    "start": "1869620",
    "end": "1877110"
  },
  {
    "text": "And the third term, which\nis the third cumulant, is also alpha T. So the\nanswer is simply of this form.",
    "start": "1877110",
    "end": "1886163"
  },
  {
    "text": " Again, m is an integer.",
    "start": "1886163",
    "end": "1891620"
  },
  {
    "text": "Alpha T is dimensionless. So there is no dimension problem\nby it having different powers.",
    "start": "1891620",
    "end": "1897985"
  },
  {
    "text": " OK. Any questions?",
    "start": "1897985",
    "end": "1903550"
  },
  {
    "start": "1903550",
    "end": "1910050"
  },
  {
    "text": "All right. So that's what I wanted\nto say about one variable.",
    "start": "1910050",
    "end": "1918010"
  },
  {
    "text": "Now let's go and look at\ncorresponding definitions",
    "start": "1918010",
    "end": "1923685"
  },
  {
    "text": "when you have\nmultiple variables. ",
    "start": "1923685",
    "end": "1933140"
  },
  {
    "text": "So for many random variables,\nthe set of possible outcomes,",
    "start": "1933140",
    "end": "1947250"
  },
  {
    "text": "let's say, has variables x1, x2.",
    "start": "1947250",
    "end": "1952650"
  },
  {
    "text": "Let's be precise. Let's end it a xn. And if these are\ndistributed, each one of them",
    "start": "1952650",
    "end": "1961090"
  },
  {
    "text": "continuously over the\ninterval, to each point we can characterize some kind\nof a probability density.",
    "start": "1961090",
    "end": "1970380"
  },
  {
    "text": "So this entity is called the\njoint probability density",
    "start": "1970380",
    "end": "1977940"
  },
  {
    "text": "function. And its definition would be to\nlook at probability of outcome",
    "start": "1977940",
    "end": "1994950"
  },
  {
    "text": "in some interval that\nis between, say, x1, x1",
    "start": "1994950",
    "end": "2002360"
  },
  {
    "text": "plus dx1 in one, x2, x2 plus\ndx2 in the second variable.",
    "start": "2002360",
    "end": "2010770"
  },
  {
    "text": "xn plus dxn in\nthe last variable.",
    "start": "2010770",
    "end": "2016750"
  },
  {
    "text": "So you sort of look at\nthe particular point in this multi-dimensional\nspace that you are interested.",
    "start": "2016750",
    "end": "2024420"
  },
  {
    "text": "You build a little\ncube around it. You ask, what's the\nprobability to being that cube?",
    "start": "2024420",
    "end": "2031580"
  },
  {
    "text": "And then you divide by\nthe volume of that cube. So dx1, dx2, dxn,\nwhich is the same thing",
    "start": "2031580",
    "end": "2041960"
  },
  {
    "text": "that you would be doing in\nconstructing any density and by ultimately taking the\nlimit that all of the x's",
    "start": "2041960",
    "end": "2050719"
  },
  {
    "text": "go to 0. ",
    "start": "2050719",
    "end": "2061250"
  },
  {
    "text": "All right. So this is the joint\nprobability distribution.",
    "start": "2061250",
    "end": "2066469"
  },
  {
    "text": "You can construct, now, the\njoint characteristic function. ",
    "start": "2066469",
    "end": "2084319"
  },
  {
    "text": "Now how do you do that? Well, again, just like you\nwould do for your transform with multiple variables.",
    "start": "2084319",
    "end": "2092030"
  },
  {
    "text": "So you would go\nfor each variable to a conjugate variable.",
    "start": "2092030",
    "end": "2098310"
  },
  {
    "text": "So x1 would go to k1. x2 would go to k2. xn would go to kn.",
    "start": "2098310",
    "end": "2105330"
  },
  {
    "text": "And this would\nmathematically amount to calculating the\nexpectation value of e",
    "start": "2105330",
    "end": "2111670"
  },
  {
    "text": "to the minus i k1x1 x1,\nk2x2 and so forth, which",
    "start": "2111670",
    "end": "2126470"
  },
  {
    "text": "you would obtain by integrating\nover all of these variables, e",
    "start": "2126470",
    "end": "2138849"
  },
  {
    "text": "to the minus ik alpha x alpha,\nagainst the probability of x1",
    "start": "2138850",
    "end": "2146280"
  },
  {
    "text": "through xn. ",
    "start": "2146280",
    "end": "2153140"
  },
  {
    "text": "Question?  AUDIENCE: It's a\nbit hard to read.",
    "start": "2153140",
    "end": "2159775"
  },
  {
    "text": "It's getting really small.  PROFESSOR: OK.",
    "start": "2159775",
    "end": "2167856"
  },
  {
    "text": "[LAUGHTER]  PROFESSOR: But it's just\nmulti-dimensional integral.",
    "start": "2167856",
    "end": "2175020"
  },
  {
    "text": "OK? All right. So this is, as\nthe case of one, I",
    "start": "2175020",
    "end": "2185270"
  },
  {
    "text": "think the problem is not the\nsize but the angle I see. I can't do much for that.",
    "start": "2185270",
    "end": "2190875"
  },
  {
    "text": "You have to move to the center. OK. So what we can look at\nnow is joint moment.",
    "start": "2190875",
    "end": "2200600"
  },
  {
    "start": "2200600",
    "end": "2206310"
  },
  {
    "text": "So you can-- when\nwe had one variable, we could look at something\nlike the expectation",
    "start": "2206310",
    "end": "2212970"
  },
  {
    "text": "value of x to the m.  That would be the m-th moment.",
    "start": "2212970",
    "end": "2220990"
  },
  {
    "text": "But if you have two\nvariable, we can raise x1 to some other, x2\nto another power,",
    "start": "2220990",
    "end": "2228270"
  },
  {
    "text": "and actually xn\nto another power. So this is a joint moment.",
    "start": "2228270",
    "end": "2233980"
  },
  {
    "text": " Now the thing is, that\nthe same way that moments",
    "start": "2233980",
    "end": "2243310"
  },
  {
    "text": "for one variable\ncould be generated by expanding the\ncharacteristic function,",
    "start": "2243310",
    "end": "2248770"
  },
  {
    "text": "if I were to expand this\nfunction in powers of k, you can see that in meeting\nthe expectation value,",
    "start": "2248770",
    "end": "2255819"
  },
  {
    "text": "I will get various powers\nof x1 to some power, x2 to some power, et cetera.",
    "start": "2255820",
    "end": "2261400"
  },
  {
    "text": "So by appropriate\nexpansion of that function, I can generate all of-- read\noff all of these moments.",
    "start": "2261400",
    "end": "2269200"
  },
  {
    "text": "Now, a more common way of\ngenerating the Taylor series expansion is\nthrough derivatives.",
    "start": "2269200",
    "end": "2276619"
  },
  {
    "text": "So what I can do is I can\ntake a derivative with respect",
    "start": "2276620",
    "end": "2282810"
  },
  {
    "text": "to, say, ik1. ",
    "start": "2282810",
    "end": "2288280"
  },
  {
    "text": "If I take a derivative\nwith respect to ik1 here, what happens\nis I will bring down",
    "start": "2288280",
    "end": "2294580"
  },
  {
    "text": "a factor of minus x alpha. So actually let me\nput the minus so it becomes a factor of x alpha.",
    "start": "2294580",
    "end": "2302339"
  },
  {
    "text": "And if I integrate x\nalpha against this,",
    "start": "2302340",
    "end": "2309060"
  },
  {
    "text": "I will be generating\nthe expectation value of x alpha provided that\nultimately I set all of the k's",
    "start": "2309060",
    "end": "2317990"
  },
  {
    "text": "to 0. So I will calculate\nthe derivative of this function with respect\nto all of these arguments.",
    "start": "2317990",
    "end": "2325160"
  },
  {
    "text": "At the end of the day, I\nwill set k equals to 0. That will give me the\nexpectation value of x1.",
    "start": "2325160",
    "end": "2331720"
  },
  {
    "text": "But I don't want x1, I want\nx1 raised to the power of m1.",
    "start": "2331720",
    "end": "2337550"
  },
  {
    "text": "So I do this. Each time I take a derivative\nwith respect to minus ik,",
    "start": "2337550",
    "end": "2342589"
  },
  {
    "text": "I will bring down the factor\nof the corresponding x. And I can do this with\nmultiple different things.",
    "start": "2342590",
    "end": "2350240"
  },
  {
    "text": "So d by the ik2 raised\nto the power of m2",
    "start": "2350240",
    "end": "2357119"
  },
  {
    "text": "minus d by the ikn,\nthe whole thing",
    "start": "2357120",
    "end": "2363310"
  },
  {
    "text": "raised to the power of mn. ",
    "start": "2363310",
    "end": "2369650"
  },
  {
    "text": "So I can either\ntake this function of multiple variables--\nk1 through kn--",
    "start": "2369650",
    "end": "2375430"
  },
  {
    "text": "and expand it and read off the\nappropriate powers of k1k2. Or I can say that the\nterms in this expansion",
    "start": "2375430",
    "end": "2383090"
  },
  {
    "text": "are generated through taking\nappropriate derivative. Yes? AUDIENCE: Is there\nany reason why",
    "start": "2383090",
    "end": "2388520"
  },
  {
    "text": "you're choosing to take\na derivative with respect to ikj instead of simply\nputting the i in the numerator?",
    "start": "2388520",
    "end": "2397406"
  },
  {
    "text": "Or are there-- are there\nthings that I'm not-- PROFESSOR: No. No. There is no reason. So you're saying why didn't I\nwrite this as this like this?",
    "start": "2397406",
    "end": "2405341"
  },
  {
    "text": "i divided? AUDIENCE: Yeah. PROFESSOR: I think\nI just visually saw that it was kind\nof more that way.",
    "start": "2405341",
    "end": "2413830"
  },
  {
    "text": "But it's exactly the same thing. Yes. OK. ",
    "start": "2413830",
    "end": "2422660"
  },
  {
    "text": "All right. Now the interesting\nobject, of course, to us",
    "start": "2422660",
    "end": "2427930"
  },
  {
    "text": "is more the joint cumulants. ",
    "start": "2427930",
    "end": "2434800"
  },
  {
    "text": "So how do we generate\njoint cumulants? Well previously, essentially\nwe had a bunch of objects",
    "start": "2434800",
    "end": "2444310"
  },
  {
    "text": "for one variable\nthat was some moment. And in order to\nmake them cumulants,",
    "start": "2444310",
    "end": "2450770"
  },
  {
    "text": "we just put a sub C here. So we do that and we are done. But what did\noperationally happen",
    "start": "2450770",
    "end": "2458390"
  },
  {
    "text": "was that we did the\nexpansion rather than for the\ncharacteristic function",
    "start": "2458390",
    "end": "2464400"
  },
  {
    "text": "for the log of the\ncharacteristic function. So all I need to do\nis to do precisely",
    "start": "2464400",
    "end": "2472630"
  },
  {
    "text": "this set of derivatives\napplied rather than to the joint\ncharacteristic function",
    "start": "2472630",
    "end": "2481290"
  },
  {
    "text": "to the log of the joint\ncharacteristic function. And at the end, set\nall of the case to 0.",
    "start": "2481290",
    "end": "2487849"
  },
  {
    "text": " OK? ",
    "start": "2487849",
    "end": "2497920"
  },
  {
    "text": "So by looking at these two\ndefinitions and the expansion",
    "start": "2497920",
    "end": "2507650"
  },
  {
    "text": "of the log, for example, you\ncan calculate various things. Like, for example, x1x2 with\na C is the expectation value",
    "start": "2507650",
    "end": "2520730"
  },
  {
    "text": "of x1x2. This joint moment minus x1x2,\njust as you would have thought,",
    "start": "2520730",
    "end": "2533599"
  },
  {
    "text": "would be the appropriate\ngeneralization of the variance. And this is the covariance.",
    "start": "2533600",
    "end": "2538835"
  },
  {
    "text": " And you can construct\nappropriate extensions.",
    "start": "2538835",
    "end": "2548560"
  },
  {
    "text": "OK. ",
    "start": "2548560",
    "end": "2563410"
  },
  {
    "text": "Now we made a lot of use of the\nrelationship between moments",
    "start": "2563410",
    "end": "2570966"
  },
  {
    "text": "and cumulants. We just-- so the\nidea, really, was that the essence of a\nprobability distribution",
    "start": "2570967",
    "end": "2578110"
  },
  {
    "text": "is characterized\nin the cumulants. Moments kind of depend on\nhow you look at things.",
    "start": "2578110",
    "end": "2583910"
  },
  {
    "text": "The essence is in the cumulants,\nbut sometimes the moments are more usefully\ncomputed, and there",
    "start": "2583910",
    "end": "2590020"
  },
  {
    "text": "was a relationship between\nmoments and cumulants, we can generalize that\ngraphical relation",
    "start": "2590020",
    "end": "2596025"
  },
  {
    "text": "to the case joint moments\nand joint cumulants. So graphical relation\napplies as long",
    "start": "2596025",
    "end": "2609119"
  },
  {
    "text": "as points are labeled\nby appropriate",
    "start": "2609120",
    "end": "2619490"
  },
  {
    "text": "or by corresponding variable.",
    "start": "2619490",
    "end": "2625040"
  },
  {
    "text": " So suppose I wanted to calculate\nsome kind of a moment that",
    "start": "2625040",
    "end": "2635970"
  },
  {
    "text": "is x1 squared. Let's say x2, x3.",
    "start": "2635970",
    "end": "2645490"
  },
  {
    "text": "This may generate\nfor me many diagrams, so let's stop from here.",
    "start": "2645490",
    "end": "2651180"
  },
  {
    "text": "So what I can do is\nI can have points that I label 1, 1, and 2.",
    "start": "2651180",
    "end": "2660150"
  },
  {
    "text": "And have them separate\nfrom each other. Or I can start\npairing them together.",
    "start": "2660150",
    "end": "2667800"
  },
  {
    "text": "So one possibility is that I\nput the 1's together and the 2",
    "start": "2667800",
    "end": "2673000"
  },
  {
    "text": "starts separately. Another possibility is that\nI can group the 1 and the 2",
    "start": "2673000",
    "end": "2680470"
  },
  {
    "text": "together. And then the other\n1 starts separately.",
    "start": "2680470",
    "end": "2686500"
  },
  {
    "text": "But I had a choice of\ntwo ways to do this, so this comes-- this diagram\nwith an overall factor of 2.",
    "start": "2686500",
    "end": "2693180"
  },
  {
    "text": "And then there's the\npossibility to put all of them",
    "start": "2693180",
    "end": "2698770"
  },
  {
    "text": "in the same bag. And so mathematically,\nthat means that the third-- this\nparticular joint moment",
    "start": "2698770",
    "end": "2708609"
  },
  {
    "text": "is obtained by taking average\nof x1 squared x2 average, which",
    "start": "2708610",
    "end": "2717880"
  },
  {
    "text": "is the first term. The second term is\nthe variance of x1.",
    "start": "2717880",
    "end": "2726450"
  },
  {
    "text": "And then multiplied by x2. ",
    "start": "2726450",
    "end": "2732030"
  },
  {
    "text": "The third term is twice the\ncovariance of x1 and x2 times",
    "start": "2732030",
    "end": "2740060"
  },
  {
    "text": "the mean of x1. ",
    "start": "2740060",
    "end": "2745960"
  },
  {
    "text": "And the final term is\njust the third cumulant. ",
    "start": "2745960",
    "end": "2753034"
  },
  {
    "text": "So again, you would need to\ncompute these, presumably, from the law of the\ncharacteristic function",
    "start": "2753034",
    "end": "2760200"
  },
  {
    "text": "and then you would be done. ",
    "start": "2760200",
    "end": "2787790"
  },
  {
    "text": "Couple of other definitions. One of them is an\nunconditional probability.",
    "start": "2787790",
    "end": "2802150"
  },
  {
    "text": " So very soon we will be talking\nabout, say, probabilities",
    "start": "2802150",
    "end": "2809960"
  },
  {
    "text": "appropriate to the\ngas in this room. And the particles in\nthe gas in this room will be characterized\nwhere they are,",
    "start": "2809960",
    "end": "2818020"
  },
  {
    "text": "some position vector q, and\nhow fast they are moving, some momentum vector p.",
    "start": "2818020",
    "end": "2825210"
  },
  {
    "text": "And there would be some kind\nof a probability density associated with finding a\nparticle with some momentum",
    "start": "2825210",
    "end": "2833750"
  },
  {
    "text": "at some location in space. But sometimes I\nsay, well, I really",
    "start": "2833750",
    "end": "2840650"
  },
  {
    "text": "don't care about where\nthe particles are, I just want to know how\nfast they are moving.",
    "start": "2840650",
    "end": "2847140"
  },
  {
    "text": "So what I really care\nis the probability that I have a particle\nmoving with some momentum p,",
    "start": "2847140",
    "end": "2855180"
  },
  {
    "text": "irrespective of where it is. Then all I need to\ndo is to integrate",
    "start": "2855180",
    "end": "2862119"
  },
  {
    "text": "over the position the joint\nprobability distribution. ",
    "start": "2862120",
    "end": "2868230"
  },
  {
    "text": "And the check that\nthis is correct is that if I first do not\nintegrate this over p,",
    "start": "2868230",
    "end": "2874360"
  },
  {
    "text": "this would be integrated\nover the entire space and the joint\nprobabilities appropriately",
    "start": "2874360",
    "end": "2879930"
  },
  {
    "text": "normalized so that the joint\nintegration will give me one. So this is a correct\nnormalized probability.",
    "start": "2879930",
    "end": "2889400"
  },
  {
    "text": "And more generally, if I'm\ninterested in, say, a bunch of",
    "start": "2889400",
    "end": "2894930"
  },
  {
    "text": "coordinates x1 through xs, out\nof a larger list of coordinates",
    "start": "2894930",
    "end": "2901540"
  },
  {
    "text": "that spans x1 through xs all\nthe way to something else,",
    "start": "2901540",
    "end": "2909510"
  },
  {
    "text": "all I need to do to get\nunconditional probability is to integrate over the variables\nthat I'm not interested.",
    "start": "2909510",
    "end": "2919770"
  },
  {
    "text": "Again, check is that it's\na good properly normalized. ",
    "start": "2919770",
    "end": "2926660"
  },
  {
    "text": "Now, this is to\nbe contrasted with the conditional probability. ",
    "start": "2926660",
    "end": "2935470"
  },
  {
    "text": "The conditional\nprobability, let's say we would be interested in\ncalculating the pressure that",
    "start": "2935470",
    "end": "2942954"
  },
  {
    "text": "is exerted on the board. The pressure is exerted\nby the particles that impinge on the board\nand then go away,",
    "start": "2942955",
    "end": "2950339"
  },
  {
    "text": "so I'm interested in the\nmomentum of particles right at the board, not\nanywhere else in space.",
    "start": "2950340",
    "end": "2957290"
  },
  {
    "text": "So if I'm interested in\nthe momentum of particles",
    "start": "2957290",
    "end": "2963760"
  },
  {
    "text": "at the particular location,\nwhich could in principle depend",
    "start": "2963760",
    "end": "2968870"
  },
  {
    "text": "on location-- so now q is a\nparameter p is the variable,",
    "start": "2968870",
    "end": "2976440"
  },
  {
    "text": "but the probability\ndistribution could depend on q. How do we obtain this?",
    "start": "2976440",
    "end": "2983020"
  },
  {
    "text": "This, again. is going to be proportional\nto the probability",
    "start": "2983020",
    "end": "2988330"
  },
  {
    "text": "that I will find\na particle both at this location with momentum p.",
    "start": "2988330",
    "end": "2993430"
  },
  {
    "text": "So I need to have that. But it's not\nexactly that there's a normalization involved.",
    "start": "2993430",
    "end": "3001309"
  },
  {
    "text": "And the way to get\nnormalization is to note that if I integrate this\nprobability over its variable p",
    "start": "3001310",
    "end": "3013589"
  },
  {
    "text": "but not over the parameter\nq, the answer should be 1.",
    "start": "3013590",
    "end": "3024050"
  },
  {
    "text": "So this is going to\nbe, if I apply it to the right-hand side,\nthe integral over p",
    "start": "3024050",
    "end": "3032760"
  },
  {
    "text": "of p of p and q,\nwhich we recognize",
    "start": "3032760",
    "end": "3039850"
  },
  {
    "text": "as an example of an\nunconditional probability to find something at position 1.",
    "start": "3039850",
    "end": "3048970"
  },
  {
    "text": "So the normalization is going to\nbe this so that the ratio is 1.",
    "start": "3048970",
    "end": "3055480"
  },
  {
    "text": "So most generally, we\nfind that the probability",
    "start": "3055480",
    "end": "3062070"
  },
  {
    "text": "to have some subset\nof variables, given",
    "start": "3062070",
    "end": "3068610"
  },
  {
    "text": "that the location of the\nother variables in the list are somewhat fixed, is given\nby the joint probability of all",
    "start": "3068610",
    "end": "3079109"
  },
  {
    "text": "of the variables x1\nthrough xn divided",
    "start": "3079110",
    "end": "3084970"
  },
  {
    "text": "by the unconditional\nprobability that that applies to the parameters of our fixed.",
    "start": "3084970",
    "end": "3091530"
  },
  {
    "text": " And this is called\nBayes' theorem.",
    "start": "3091530",
    "end": "3097260"
  },
  {
    "start": "3097260",
    "end": "3117510"
  },
  {
    "text": "By the way, if variables\nare independent,",
    "start": "3117510",
    "end": "3125240"
  },
  {
    "text": "which actually does apply\nto the case of the particles in this room as far as their\nmomentum and position is",
    "start": "3125240",
    "end": "3131300"
  },
  {
    "text": "concerned, then the\njoint probability is going to be the\nproduct of one that",
    "start": "3131300",
    "end": "3138190"
  },
  {
    "text": "is appropriate to\nthe position and one that is appropriate\nto the momentum. ",
    "start": "3138190",
    "end": "3145990"
  },
  {
    "text": "And if you have\nthis independence, then what you'll\nfind is that there",
    "start": "3145990",
    "end": "3151835"
  },
  {
    "text": "is no difference between\nconditional and unconditional probabilities. And when you go\nthrough this procedure,",
    "start": "3151835",
    "end": "3158609"
  },
  {
    "text": "you will find that all the\njoint cumulants-- but not the joint moments, naturally--\nall the joint cumulants",
    "start": "3158610",
    "end": "3164780"
  },
  {
    "text": "will be 0. ",
    "start": "3164780",
    "end": "3172050"
  },
  {
    "text": "OK. Any questions? Yes? AUDIENCE: Could you explain\nhow the condition of p--",
    "start": "3172050",
    "end": "3180286"
  },
  {
    "text": "PROFESSOR: How\nthis was obtained? Or the one above? AUDIENCE: Yeah. The condition you applied\nthat the integral is 1.",
    "start": "3180286",
    "end": "3187624"
  },
  {
    "text": "PROFESSOR: OK. So first of all, what\nI want to look at",
    "start": "3187624",
    "end": "3194950"
  },
  {
    "text": "is the probability that\nis appropriate to one random variable at\nthe fixed value of all",
    "start": "3194950",
    "end": "3202240"
  },
  {
    "text": "the other random variables. Like you say, in general I\nshould specify the probability",
    "start": "3202240",
    "end": "3207890"
  },
  {
    "text": "as a function of momentum and\nposition throughout space. But I'm really interested\nonly at this point.",
    "start": "3207890",
    "end": "3213790"
  },
  {
    "text": "I don't really care\nabout other points. However, the answer may depend\nwhether I'm looking at here",
    "start": "3213790",
    "end": "3220820"
  },
  {
    "text": "or I'm looking at here. So the answer for the\nprobability of momentum is parametrized by q.",
    "start": "3220820",
    "end": "3227710"
  },
  {
    "text": "On the other hand,\nI say that I know the probability over\nthe entire space",
    "start": "3227710",
    "end": "3233370"
  },
  {
    "text": "to be a disposition\nwith the momentum p as given by this\njoint probability.",
    "start": "3233370",
    "end": "3239720"
  },
  {
    "text": "But if I just set\nthat equal to this, the answer is not\ncorrect because the way",
    "start": "3239720",
    "end": "3245820"
  },
  {
    "text": "that this quantity is normalized\nis if I first integrate over all possible values\nof its variable, p.",
    "start": "3245820",
    "end": "3255170"
  },
  {
    "text": "The answer should be 1,\nirrespective of what q is.",
    "start": "3255170",
    "end": "3260200"
  },
  {
    "text": "So I can define a conditional\nprobability for momentum here, a conditional\nprobability for momentum there.",
    "start": "3260200",
    "end": "3268630"
  },
  {
    "text": "In both cases the momentum\nwould be the variable it. And integrating over all\npossible values of momentum",
    "start": "3268630",
    "end": "3276230"
  },
  {
    "text": "should give me one for a\nproperly normalized probability distribution. AUDIENCE: [INAUDIBLE].",
    "start": "3276230",
    "end": "3282575"
  },
  {
    "text": "PROFESSOR: Given\nthat q is something. So q could be some--\nnow here q can",
    "start": "3282575",
    "end": "3288510"
  },
  {
    "text": "be regarded as some parameter. So the condition is that this\nintegration should give me 1.",
    "start": "3288510",
    "end": "3295680"
  },
  {
    "text": "I said that on\nphysical grounds, I expect this\nconditional probability to be the joint probability\nup to some normalization",
    "start": "3295680",
    "end": "3303819"
  },
  {
    "text": "that I don't know. OK. So what is that normalization?",
    "start": "3303820",
    "end": "3309390"
  },
  {
    "text": "The whole answer should be 1. What I have to do\nis an integration over momentum of the\njoint probability.",
    "start": "3309390",
    "end": "3317380"
  },
  {
    "text": "I have said that an integration\nover some set of variables",
    "start": "3317380",
    "end": "3322845"
  },
  {
    "text": "of a joint probability\nwill give me the unconditional probability\nfor all the others.",
    "start": "3322845",
    "end": "3328860"
  },
  {
    "text": "So integrating over all momentum\nof this joint probability will give me the unconditional\nprobability for position.",
    "start": "3328860",
    "end": "3337310"
  },
  {
    "text": "So the normalization of 1 is\nthe unconditional probability for position divided by n.",
    "start": "3337310",
    "end": "3344220"
  },
  {
    "text": "So n-- this has to be this. And in general, it would\nhave to be this in order",
    "start": "3344220",
    "end": "3353200"
  },
  {
    "text": "to ensure that if\nI integrate over this first set of variables\nof the joint probability",
    "start": "3353200",
    "end": "3359180"
  },
  {
    "text": "distribution which would\ngive me the unconditional, cancels the unconditional in\nthe denominator to give me 1.",
    "start": "3359180",
    "end": "3367090"
  },
  {
    "start": "3367090",
    "end": "3375324"
  },
  {
    "text": "Other questions? ",
    "start": "3375324",
    "end": "3380900"
  },
  {
    "text": "OK. So I'm going to\nerase this last board",
    "start": "3380900",
    "end": "3388980"
  },
  {
    "text": "to be underneath that\ntop board in looking",
    "start": "3388980",
    "end": "3395530"
  },
  {
    "text": "at the joint Gaussian\ndistribution. So that was the\nGaussian, and we want",
    "start": "3395530",
    "end": "3401950"
  },
  {
    "text": "to look at the joint Gaussian. ",
    "start": "3401950",
    "end": "3413950"
  },
  {
    "text": "So we want to\ngeneralize the formula that we have over there for one\nvariable to multiple variables.",
    "start": "3413950",
    "end": "3421855"
  },
  {
    "text": " So what I have there\ninitially is a factor, which",
    "start": "3421855",
    "end": "3429200"
  },
  {
    "text": "is exponential of minus\n1/2, x minus lambda squared.",
    "start": "3429200",
    "end": "3443530"
  },
  {
    "text": "I can write this x\nminus lambda squared as x minus lambda\nx minus lambda.",
    "start": "3443530",
    "end": "3449680"
  },
  {
    "text": "And then put the variance. Let's call it is 1 over sigma\nrather than a small sigma",
    "start": "3449680",
    "end": "3459850"
  },
  {
    "text": "squared or something like this. Actually, let me\njust write it as 1 over sigma squared\nfor the time being.",
    "start": "3459850",
    "end": "3466110"
  },
  {
    "text": "And then the normalization was\n1 over root 2 pi sigma squared.",
    "start": "3466110",
    "end": "3473480"
  },
  {
    "text": "But you say, well, I\nhave multiple variables, so maybe this is what I\nwould give for my B variable.",
    "start": "3473480",
    "end": "3479079"
  },
  {
    "start": "3479080",
    "end": "3484090"
  },
  {
    "text": "And then I would sum over\nall N, running from 1 to N.",
    "start": "3484090",
    "end": "3490410"
  },
  {
    "text": "So this is essentially\nthe form that I would have for an independent\nGaussian variables.",
    "start": "3490410",
    "end": "3498240"
  },
  {
    "text": "And then I would\nhave to multiply here factors of 2 pi sigma squared,\nso I would have 2 pi to the N",
    "start": "3498240",
    "end": "3505990"
  },
  {
    "text": "over 2. And I would have\nproduct of-- actually, let's write it as 2 pi\nto the N square root.",
    "start": "3505990",
    "end": "3514369"
  },
  {
    "text": "I would have the product\nof sigma i squared. ",
    "start": "3514370",
    "end": "3520890"
  },
  {
    "text": "But that's just too\nlimiting a form. The most general form that these\nquadratic will allow me to have",
    "start": "3520890",
    "end": "3530700"
  },
  {
    "text": "also cross terms where it is\nnot only the diagonal terms",
    "start": "3530700",
    "end": "3535980"
  },
  {
    "text": "x1 and x1 that's are multiplying\neach other, but x2 and x3, et cetera.",
    "start": "3535980",
    "end": "3541140"
  },
  {
    "text": "So I would have a sum over both\nm and n running from 1 to n.",
    "start": "3541140",
    "end": "3547069"
  },
  {
    "text": "And then to coefficient\nhere, rather than just being a number,\nwould be the variables",
    "start": "3547070",
    "end": "3554890"
  },
  {
    "text": "that would be like a matrix. Because for each pair m and\nn, I would have some number.",
    "start": "3554890",
    "end": "3565070"
  },
  {
    "text": "And I will call them the\ninverse of some matrix C.",
    "start": "3565070",
    "end": "3572420"
  },
  {
    "text": "And if you, again, think\nof the problem as a matrix,",
    "start": "3572420",
    "end": "3577710"
  },
  {
    "text": "if I have the diagonal\nmatrix, then the product of elements along the\ndiagonal is the same thing",
    "start": "3577710",
    "end": "3583990"
  },
  {
    "text": "as the determinant. If I were to rotate the matrix\nto have off diagonal elements,",
    "start": "3583990",
    "end": "3589780"
  },
  {
    "text": "the determinant will\nalways be there. So this is really\nthe determinant of C",
    "start": "3589780",
    "end": "3596365"
  },
  {
    "text": "that will appear here.  Yes? AUDIENCE: So are you inverting\nthe individual elements of C",
    "start": "3596365",
    "end": "3607045"
  },
  {
    "text": "or are you inverting the matrix\nC and taking its elements? PROFESSOR: Actually\na very good point.",
    "start": "3607045",
    "end": "3614940"
  },
  {
    "text": "I really wanted to write it\nas the inverse of the matrix and then peak the\nmn [INAUDIBLE].",
    "start": "3614940",
    "end": "3622350"
  },
  {
    "text": "So we imagine that\nwe have the matrix. And these are the\nelements of some--",
    "start": "3622350",
    "end": "3630960"
  },
  {
    "text": "so I could have called\nthis whatever I want. So I could have called the\ncoefficients of x and n.",
    "start": "3630960",
    "end": "3637150"
  },
  {
    "text": "I have chosen to regard\nthem as the inverse",
    "start": "3637150",
    "end": "3644609"
  },
  {
    "text": "of some other matrix C.\nAnd the reason for that becomes shortly clear,\nbecause the covariances will",
    "start": "3644610",
    "end": "3652350"
  },
  {
    "text": "be related to the\ninverse of this matrix. And hence, that's the\nappropriate way to look at it.",
    "start": "3652350",
    "end": "3658731"
  },
  {
    "text": "AUDIENCE: Can [INAUDIBLE]\nwhat C means up there? PROFESSOR: OK. So let's forget\nabout this lambdas.",
    "start": "3658731",
    "end": "3665845"
  },
  {
    "text": "So I would have in\ngeneral for two variables some coefficient for x1\nsquared, some coefficient",
    "start": "3665845",
    "end": "3672640"
  },
  {
    "text": "for x2 squared, and some\ncoefficient for x1, x2.",
    "start": "3672640",
    "end": "3677730"
  },
  {
    "text": "So I could call this a11. I could call this a22.",
    "start": "3677730",
    "end": "3683580"
  },
  {
    "text": "I could call this 2a12. Or actually I\ncould, if I wanted, just write it as\na12 plus a21 x2x1",
    "start": "3683580",
    "end": "3693250"
  },
  {
    "text": "or do a1 to an a21\nwould be the same. So what I could then\nregard this is as x1 2.",
    "start": "3693250",
    "end": "3702349"
  },
  {
    "text": "The matrix a11, a12,\na21, a22, x1, x2.",
    "start": "3702350",
    "end": "3710440"
  },
  {
    "text": "So this is exactly\nthe same as that. All right?",
    "start": "3710440",
    "end": "3715570"
  },
  {
    "text": "So these objects\nhere are the elements",
    "start": "3715570",
    "end": "3721490"
  },
  {
    "text": "of this matrix C inverse. So I could call this x1,\nx2 some matrix A x1 x2.",
    "start": "3721490",
    "end": "3732790"
  },
  {
    "text": "That A is 2 by 2 matrix. The name I have given to that\n2 by 2 matrix in C inverse.",
    "start": "3732790",
    "end": "3739080"
  },
  {
    "text": " Yes? AUDIENCE: The matrix\nis required to be",
    "start": "3739080",
    "end": "3745403"
  },
  {
    "text": "symmetric though, isn't it? PROFESSOR: The\nmatrix is required to be symmetric for\nany quadrant form. Yes. So when I wrote it\ninitially, I wrote as 2 a12.",
    "start": "3745404",
    "end": "3753920"
  },
  {
    "text": "And then I said, well,\nI can also write it this fashion provided the\ntwo of them are the same. Yes?",
    "start": "3753920",
    "end": "3759230"
  },
  {
    "text": "AUDIENCE: How did you know\nthe determinant of C belonged there? PROFESSOR: Pardon? AUDIENCE: How did you\nknow that the determinant",
    "start": "3759230",
    "end": "3765019"
  },
  {
    "text": "of C [INAUDIBLE]? PROFESSOR: OK. How do I know the\ndeterminant of C? Let's say I give you this form.",
    "start": "3765020",
    "end": "3771329"
  },
  {
    "text": "And then I don't know\nwhat the normalization is. What I can do is I can do a\nchange of variables from x1 x2",
    "start": "3771330",
    "end": "3780390"
  },
  {
    "text": "to something like y1 y2 such\nthat when I look at y1 and y2,",
    "start": "3780390",
    "end": "3786690"
  },
  {
    "text": "the matrix becomes diagonal. So I can rotate the matrix.",
    "start": "3786690",
    "end": "3793130"
  },
  {
    "text": "So any matrix I can imagine\nthat I will find some U such",
    "start": "3793130",
    "end": "3798730"
  },
  {
    "text": "that A U U dagger is this\ndiagonal matrix lambda.",
    "start": "3798730",
    "end": "3804869"
  },
  {
    "text": "Now under these procedures,\none thing that does not change is the determinant.",
    "start": "3804870",
    "end": "3810970"
  },
  {
    "text": "It's always the product\nof the eigenvalues. The way that I set\nup the problem,",
    "start": "3810970",
    "end": "3815980"
  },
  {
    "text": "I said that if I hadn't made\nthe problem to have cross terms,",
    "start": "3815980",
    "end": "3821480"
  },
  {
    "text": "I knew the answers to be the\nproduct of that eigenvalues. So if you like, I can start from\nthere and then do a rotation",
    "start": "3821480",
    "end": "3829870"
  },
  {
    "text": "and have the more general form. The answer would stay\nas the determinant. Yes?",
    "start": "3829870",
    "end": "3835571"
  },
  {
    "text": "AUDIENCE: The matrix should\nbe positive as well or no? PROFESSOR: The matrix should\nbe positive definite in order for the probability to be\nwell-defined and exist, yes.",
    "start": "3835571",
    "end": "3844619"
  },
  {
    "text": "OK. So if you like, by stating\nthat this is a probability,",
    "start": "3844620",
    "end": "3850380"
  },
  {
    "text": "I have imposed a\nnumber of conditions such as symmetry, as\nwell as positivity.",
    "start": "3850380",
    "end": "3855559"
  },
  {
    "text": " Yes.",
    "start": "3855560",
    "end": "3860750"
  },
  {
    "text": "OK. But this is just linear algebra. ",
    "start": "3860750",
    "end": "3869460"
  },
  {
    "text": "I will assume that you\nknow linear algebra. ",
    "start": "3869460",
    "end": "3874710"
  },
  {
    "text": "OK. So this property normalized\nGaussian joint probability.",
    "start": "3874710",
    "end": "3880790"
  },
  {
    "text": "We are interested in the\ncharacteristic function. So what we are interested is the\njoint Gaussian characteristic.",
    "start": "3880790",
    "end": "3888256"
  },
  {
    "start": "3888256",
    "end": "3894640"
  },
  {
    "text": "And so again we\nsaw the procedure was that I have to do\nthe Fourier transform.",
    "start": "3894640",
    "end": "3900636"
  },
  {
    "text": " So I have to take this\nprobability that I",
    "start": "3900636",
    "end": "3908660"
  },
  {
    "text": "have over there and do\nan integration product, say, alpha running\nfrom 1 to N dx",
    "start": "3908660",
    "end": "3917470"
  },
  {
    "text": "alpha of e to the\nminus ik alpha x alpha.",
    "start": "3917470",
    "end": "3927750"
  },
  {
    "text": "This product exists\nfor all values. Then I have to multiply\nwith this probability",
    "start": "3927750",
    "end": "3934810"
  },
  {
    "text": "that I have up there,\nwhich would appear here. ",
    "start": "3934810",
    "end": "3941338"
  },
  {
    "text": "OK.  Now, again maybe an\neasy way to imagine",
    "start": "3941339",
    "end": "3949570"
  },
  {
    "text": "is what I was saying\nto previously. Let's imagine that\nI have rotated",
    "start": "3949570",
    "end": "3954880"
  },
  {
    "text": "into a basis where\neverything is diagonal. Then in the rotated\nbasis, all you need to do",
    "start": "3954880",
    "end": "3963350"
  },
  {
    "text": "is to essentially do product\nof characteristic functions",
    "start": "3963350",
    "end": "3971980"
  },
  {
    "text": "such as what we have over here. So the corresponding\nproduct to this first term",
    "start": "3971980",
    "end": "3980559"
  },
  {
    "text": "would be exponential of\nminus i sum over N running",
    "start": "3980560",
    "end": "3986250"
  },
  {
    "text": "from 1 to N k\nalpha lambda alpha. ",
    "start": "3986250",
    "end": "3992910"
  },
  {
    "text": "kn lambda n. I guess I'm using n\nas the variable here.",
    "start": "3992910",
    "end": "3999140"
  },
  {
    "text": "And as long as things\nwould be diagonal, the next ordered\nterm would be a sum",
    "start": "3999140",
    "end": "4006670"
  },
  {
    "text": "over alpha kn squared the\ncorresponding eigenvalue",
    "start": "4006670",
    "end": "4012260"
  },
  {
    "text": "inverted. So remember that in the diagonal\nform, each one of these sigmas",
    "start": "4012260",
    "end": "4020560"
  },
  {
    "text": "would appear as the diagonal. If I do my rotation,\nessentially this term",
    "start": "4020560",
    "end": "4027950"
  },
  {
    "text": "would not be affected. The next term would give me\nminus 1/2 sum over m and n",
    "start": "4027950",
    "end": "4035180"
  },
  {
    "text": "rather than just having k1\nsquared k2 squared, et cetera. Just like here, I\nwould have km kn.",
    "start": "4035180",
    "end": "4045890"
  },
  {
    "text": "What happened previously\nwas that each eigenvalue would get inverted.",
    "start": "4045890",
    "end": "4052460"
  },
  {
    "text": "If you think about rotating a\nmatrix, all of its eigenvalues are inverted, you are really\nrotating the inverse matrix.",
    "start": "4052460",
    "end": "4061310"
  },
  {
    "text": "So this here would be the\ninverse of whatever matrix I have here. So this would be C mn.",
    "start": "4061310",
    "end": "4066426"
  },
  {
    "text": " So I did will leave you to\ndo the corresponding linear",
    "start": "4066426",
    "end": "4074960"
  },
  {
    "text": "algebra here, but the\nanswer is correct. So the answer is that the\ngenerator of cumulants",
    "start": "4074960",
    "end": "4090630"
  },
  {
    "text": "for a joint Gaussian\ndistribution has a form which has a bunch of\nthe linear terms-- kn lambda n.",
    "start": "4090630",
    "end": "4107479"
  },
  {
    "text": "And a bunch of\nsecond order terms, so we will have minus\n1/2 sum over m and n km",
    "start": "4107479",
    "end": "4115910"
  },
  {
    "text": "kn times some coefficient. ",
    "start": "4115910",
    "end": "4122549"
  },
  {
    "text": "And the series terminates here. So for the joint Gaussian,\nyou have first cumulant.",
    "start": "4122550",
    "end": "4132710"
  },
  {
    "text": "So the expectation\nvalue of nm cumulant is the same thing as lambda m.",
    "start": "4132710",
    "end": "4141430"
  },
  {
    "text": "You have covariances or second\ncumulants xm, xn, C is Cmn.",
    "start": "4141430",
    "end": "4149149"
  },
  {
    "text": "And in particular,\nthe diagonal elements would correspond\nto the variances.",
    "start": "4149149",
    "end": "4156429"
  },
  {
    "text": "And all the higher orders\nare 0 because there's",
    "start": "4156430",
    "end": "4163490"
  },
  {
    "text": "no further term\nin the expansion. ",
    "start": "4163490",
    "end": "4179750"
  },
  {
    "text": "So for example, if I were to\ncalculate this thing that I",
    "start": "4179750",
    "end": "4187299"
  },
  {
    "text": "have on the board here for\nthe case of a Gaussian,",
    "start": "4187300",
    "end": "4195099"
  },
  {
    "text": "for the case of the Gaussian, I\nwould not have this third term.",
    "start": "4195100",
    "end": "4202150"
  },
  {
    "text": "So the answer that\nI would write down for the case of the\nthird term would be something that\ndidn't have this.",
    "start": "4202150",
    "end": "4209860"
  },
  {
    "text": "And in the way that we\nhave written things, the answer would\nhave been x1 squared",
    "start": "4209860",
    "end": "4216120"
  },
  {
    "text": "x2-- would be just a\nlambda 1 squared lambda",
    "start": "4216120",
    "end": "4222020"
  },
  {
    "text": "2 plus sigma 1 squared,\nor let's call it",
    "start": "4222020",
    "end": "4227050"
  },
  {
    "text": "C11, times lambda 2\nplus 2 lambda 1 C12.",
    "start": "4227050",
    "end": "4236239"
  },
  {
    "text": "And that's it. ",
    "start": "4236240",
    "end": "4261670"
  },
  {
    "text": "So there is something\nthat follows from this that it is used a\nlot in field theory.",
    "start": "4261670",
    "end": "4273070"
  },
  {
    "text": "And it's called Wick's theorem. So that's just a\nparticular case of this,",
    "start": "4273070",
    "end": "4280140"
  },
  {
    "text": "but let's state it anyway. ",
    "start": "4280140",
    "end": "4292410"
  },
  {
    "text": "So for Gaussian distributed\nvariables of 0 mean,",
    "start": "4292410",
    "end": "4311000"
  },
  {
    "text": "following condition applies. I can take the first\nvariable raised",
    "start": "4311000",
    "end": "4316665"
  },
  {
    "text": "to power n1, the\nsecond variable to n2, the last variable\nto some other nN",
    "start": "4316665",
    "end": "4323592"
  },
  {
    "text": "and look at a joint\nexpectation value such as this. And this is 0 if sum\nover alpha and alpha",
    "start": "4323592",
    "end": "4336219"
  },
  {
    "text": "is odd and is called sum\nover all pairwise contraction",
    "start": "4336220",
    "end": "4357120"
  },
  {
    "text": "if a sum over alpha\nand alpha is even. ",
    "start": "4357120",
    "end": "4367920"
  },
  {
    "text": "So actually, I have right\nhere an example of this. If I have a Gaussian\nvariable-- jointly",
    "start": "4367920",
    "end": "4376110"
  },
  {
    "text": "distributed Gaussian variables\nwhere the means are all 0-- so if I say that lambda\n1 and lambda 2 are 0,",
    "start": "4376110",
    "end": "4385110"
  },
  {
    "text": "then this is an odd\npower, x1 squared x2. Because of the symmetry\nit has to be 0,",
    "start": "4385110",
    "end": "4390300"
  },
  {
    "text": "but you explicitly see\nthat every term that I have will be multiplying some\npower of [INAUDIBLE].",
    "start": "4390300",
    "end": "4396840"
  },
  {
    "text": "Whereas if for\nother than this, I was looking at something\nlike x1 squared x2 x3",
    "start": "4396840",
    "end": "4407150"
  },
  {
    "text": "where the net power\nis even, then I",
    "start": "4407150",
    "end": "4412239"
  },
  {
    "text": "could sort of\nimagine putting them into these kinds of diagrams.",
    "start": "4412240",
    "end": "4417329"
  },
  {
    "text": "Or alternatively, I can\nimagine pairing these things",
    "start": "4417330",
    "end": "4423950"
  },
  {
    "text": "in all possible ways. So one pairing would\nbe this with this, this with this, which would have\ngiven me x1 squared C x2 x3 C.",
    "start": "4423950",
    "end": "4437730"
  },
  {
    "text": "Another pairing would\nhave been x1 with x2. And then, naturally, x1 with x3.",
    "start": "4437730",
    "end": "4445650"
  },
  {
    "text": "So I would have gotten\nx1 with x2 covariance x1",
    "start": "4445650",
    "end": "4451495"
  },
  {
    "text": "with extreme x3 covariance.  But I could have connected\nthe x1 to x2 or the second x1",
    "start": "4451495",
    "end": "4461690"
  },
  {
    "text": "to x2. So this comes in 2 variance.",
    "start": "4461690",
    "end": "4466920"
  },
  {
    "text": "And so the answer here would\nbe C11 C23 plus 2 C1 2 C13.",
    "start": "4466920",
    "end": "4478320"
  },
  {
    "text": "Yes? AUDIENCE: In your writing\nof x1 to the n1 [INAUDIBLE].",
    "start": "4478320",
    "end": "4485930"
  },
  {
    "text": "It should be the\ncumulant, right? Or is it the moment? PROFESSOR: This is the moment. AUDIENCE: OK. PROFESSOR: The contractions\nare the covariances.",
    "start": "4485930",
    "end": "4494420"
  },
  {
    "text": "AUDIENCE: OK. PROFESSOR: So the point is\nthat the Gaussian distribution",
    "start": "4494420",
    "end": "4500580"
  },
  {
    "text": "is completely characterized\nin terms of its covariances. Once you know the\ncovariances, essentially you",
    "start": "4500580",
    "end": "4507665"
  },
  {
    "text": "know everything. And in particular, you may be\ninterested in some particular",
    "start": "4507665",
    "end": "4513080"
  },
  {
    "text": "combination of x's. And then you use to\nexpress that in terms",
    "start": "4513080",
    "end": "4518489"
  },
  {
    "text": "of all possible\npairwise contractions, which are the covariances.",
    "start": "4518490",
    "end": "4524480"
  },
  {
    "text": "And essentially, in\nall of field theory,",
    "start": "4524480",
    "end": "4529730"
  },
  {
    "text": "you expand around some kind\nof a Gaussian background or Gaussian 0 toward the result.",
    "start": "4529730",
    "end": "4535870"
  },
  {
    "text": "And then in your\nperturbation theory you need various\npowers of your field or some combination of\npowers, and you express them",
    "start": "4535870",
    "end": "4543079"
  },
  {
    "text": "through these kinds\nof relationships. ",
    "start": "4543079",
    "end": "4555400"
  },
  {
    "text": "Any questions? ",
    "start": "4555400",
    "end": "4561440"
  },
  {
    "text": "OK. ",
    "start": "4561440",
    "end": "4568329"
  },
  {
    "text": "This is fine. Let's get rid of this. ",
    "start": "4568329",
    "end": "4582480"
  },
  {
    "text": "OK.  Now there is one result that\nall of statistical mechanics",
    "start": "4582480",
    "end": "4593500"
  },
  {
    "text": "hangs on. So I expect that\nas I get old and I",
    "start": "4593500",
    "end": "4599230"
  },
  {
    "text": "get infirm or whatever\nand my memory vanishes, the last thing that I\nwill remember before I die",
    "start": "4599230",
    "end": "4606272"
  },
  {
    "text": "would be the central\nlimit theorem. ",
    "start": "4606272",
    "end": "4618750"
  },
  {
    "text": "And why is this important\nis because you end up in statistical physics\nadding lots of things.",
    "start": "4618750",
    "end": "4625960"
  },
  {
    "text": "So really, the question that\nyou have or you should be asking is thermodynamics is\na very precise thing.",
    "start": "4625960",
    "end": "4633280"
  },
  {
    "text": "It says that heat\ngoes from the higher temperature to\nlower temperature. It doesn't say it does that 50%\nof the time or 95% of the time.",
    "start": "4633280",
    "end": "4641610"
  },
  {
    "text": "It's a definite statement. If I am telling you\nthat ultimately I'm going to express everything\nin terms of probabilities,",
    "start": "4641610",
    "end": "4649660"
  },
  {
    "text": "how does that jive? The reason that it jives\nis because of this theorem. It's because in order to go from\nthe probabilistic description,",
    "start": "4649660",
    "end": "4660290"
  },
  {
    "text": "you will be dealing with\nso many different-- so many large number of variables--\nthat probabilistic statements",
    "start": "4660290",
    "end": "4666630"
  },
  {
    "text": "actually become precise\ndeterministic statements. And that's captured\nby this theorem, which",
    "start": "4666630",
    "end": "4672199"
  },
  {
    "text": "says that let's look at the\nsum of N random variables.",
    "start": "4672200",
    "end": "4681160"
  },
  {
    "start": "4681160",
    "end": "4686550"
  },
  {
    "text": "And I will indicate the sum\nby x and my random variables",
    "start": "4686550",
    "end": "4694332"
  },
  {
    "text": "as small x's. And let's say that this is, for\nthe individual set of things",
    "start": "4694332",
    "end": "4703230"
  },
  {
    "text": "that I'm adding up\ntogether, some kind of a joint probability\ndistribution",
    "start": "4703230",
    "end": "4708690"
  },
  {
    "text": "out of which I take\nthese random variables. So each instance of this sum is\nselected from this joint PDF,",
    "start": "4708690",
    "end": "4720230"
  },
  {
    "text": "so x itself is a random variable\nbecause of possible choices",
    "start": "4720230",
    "end": "4725510"
  },
  {
    "text": "of different xi from this\nprobability distribution. ",
    "start": "4725510",
    "end": "4731720"
  },
  {
    "text": "So what I'm interested is what\nis the probability for the sum? So what is the p that\ndetermines this sum?",
    "start": "4731720",
    "end": "4743770"
  },
  {
    "start": "4743770",
    "end": "4749410"
  },
  {
    "text": "I will go by the root of these\ncharacteristic functions. I will say, OK, what's the\nexpectation value of-- well,",
    "start": "4749410",
    "end": "4759210"
  },
  {
    "text": "let's-- what's the Fourier\ntransform of this probability distribution?",
    "start": "4759210",
    "end": "4766730"
  },
  {
    "text": "If we transform, by definition\nit is the expectation of e to the minus ik this big X,\nwhich is the sum over all off",
    "start": "4766730",
    "end": "4777320"
  },
  {
    "text": "the small x's. ",
    "start": "4777320",
    "end": "4787640"
  },
  {
    "text": "Do I have that\ndefinition somewhere? I erased it. Basically, what is this?",
    "start": "4787640",
    "end": "4796290"
  },
  {
    "text": "If this k was, in fact,\ndifferent k's-- if I had a k1",
    "start": "4796290",
    "end": "4802740"
  },
  {
    "text": "multiplying x1,\nk2 multiplying x2, that would be the definition\nof the joint characteristics",
    "start": "4802740",
    "end": "4808790"
  },
  {
    "text": "function for this joint\nprobability distribution. So what this is is you take the\njoint characteristic function,",
    "start": "4808790",
    "end": "4818470"
  },
  {
    "text": "which depends on k1\nk2, all the way to kn.",
    "start": "4818470",
    "end": "4827010"
  },
  {
    "text": "And you set all of\nthem to be the same. ",
    "start": "4827010",
    "end": "4838460"
  },
  {
    "text": "So take the joint\ncharacteristic function depends on N Fourier variables.",
    "start": "4838460",
    "end": "4844090"
  },
  {
    "text": "Put all of them the same k\nand you have that for the sum. ",
    "start": "4844090",
    "end": "4850230"
  },
  {
    "text": "So I can certainly do\nthat by adding a log here.",
    "start": "4850230",
    "end": "4855970"
  },
  {
    "text": " Nothing has changed.",
    "start": "4855970",
    "end": "4862720"
  },
  {
    "text": "I know that the log is the\ngenerator of the cumulants.",
    "start": "4862720",
    "end": "4869340"
  },
  {
    "text": "So this is a sum\nover, let's say, n running from 1 to\ninfinity minus ik",
    "start": "4869340",
    "end": "4876690"
  },
  {
    "text": "to the power of n\nover n factorial, the joint cumulant of the sum.",
    "start": "4876690",
    "end": "4882990"
  },
  {
    "start": "4882990",
    "end": "4889640"
  },
  {
    "text": "So what is the\nexpansion that I would have for log of the joint\ncharacteristic function?",
    "start": "4889640",
    "end": "4897580"
  },
  {
    "text": "Well, typically I would say have\nat the lowest order k1 times",
    "start": "4897580",
    "end": "4903150"
  },
  {
    "text": "the mean of the first\nvariable, k2 times the mean of the second variable. But all of them are the same.",
    "start": "4903150",
    "end": "4909440"
  },
  {
    "text": "So the first order,\nI would get minus i the same k sum over n\nof the first cumulant",
    "start": "4909440",
    "end": "4918480"
  },
  {
    "text": "of the N-th variable. ",
    "start": "4918480",
    "end": "4926179"
  },
  {
    "text": "Typically, this\nsecond order term, I would have all\nkinds of products. I would have k1 k3 k2 k4,\nas well as k1 squared.",
    "start": "4926180",
    "end": "4935920"
  },
  {
    "text": "But now all of them\nbecome the same, and so what I will have\nis a minus ik squared.",
    "start": "4935920",
    "end": "4943780"
  },
  {
    "text": "But then I have all possible\npairings mn of xm xn cumulants.",
    "start": "4943780",
    "end": "4954260"
  },
  {
    "text": " AUDIENCE: Question. PROFESSOR: Yes? AUDIENCE: [INAUDIBLE]\nexpression you probably",
    "start": "4954260",
    "end": "4960170"
  },
  {
    "text": "should use different\nindices when you're summing over elements\nof Taylor serious and when you're summing\nover your [INAUDIBLE]",
    "start": "4960170",
    "end": "4965828"
  },
  {
    "text": "random variables. Just-- it gets confusing\nwhen both indexes are n.",
    "start": "4965828",
    "end": "4973163"
  },
  {
    "text": "PROFESSOR: This here, you\nwant me to right here, say, i? AUDIENCE: Yeah.",
    "start": "4973163",
    "end": "4978752"
  },
  {
    "text": "PROFESSOR: OK. And here I can write i and j. ",
    "start": "4978752",
    "end": "4992020"
  },
  {
    "text": "So I think there's\nstill a 2 factorial. And then there's higher orders.",
    "start": "4992020",
    "end": "4998680"
  },
  {
    "text": "Essentially then, matching\nthe coefficients of minus ik from the left minus\nik from the right",
    "start": "4998680",
    "end": "5007470"
  },
  {
    "text": "will enable me to calculate\nrelationships between cumulants",
    "start": "5007470",
    "end": "5015440"
  },
  {
    "text": "of the sum and cumulants of\nthe individual variables. This first one of them is\nnot particularly surprising.",
    "start": "5015440",
    "end": "5024070"
  },
  {
    "text": "You would say that\nthe mean of the sum is sum of the means of\nthe individual variables.",
    "start": "5024070",
    "end": "5029540"
  },
  {
    "start": "5029540",
    "end": "5034890"
  },
  {
    "text": "The second statement is\nthat the variance of the sum",
    "start": "5034890",
    "end": "5042290"
  },
  {
    "text": "really involves a pair, i\nand j, running from 1 to N.",
    "start": "5042290",
    "end": "5051410"
  },
  {
    "text": "So if these variable\nwere independent, you would be just\nadding the variances.",
    "start": "5051410",
    "end": "5058230"
  },
  {
    "text": "Since they are\npotentially dependent, you have to also keep\ntrack of covariances.",
    "start": "5058230",
    "end": "5065610"
  },
  {
    "text": " And this kind of summation\nextends to higher and higher",
    "start": "5065610",
    "end": "5073429"
  },
  {
    "text": "cumulants, essentially\nincluding more and more powers of cumulants that you\nwould put on that side.",
    "start": "5073430",
    "end": "5080905"
  },
  {
    "start": "5080905",
    "end": "5087170"
  },
  {
    "text": "And what we do\nwith that, I guess we'll start next time around. ",
    "start": "5087170",
    "end": "5092529"
  }
]