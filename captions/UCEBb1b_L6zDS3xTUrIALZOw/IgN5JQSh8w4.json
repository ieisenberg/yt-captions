[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": "SPEAKER: The following content\nis provided under a creative commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6600"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation, or to view\nadditional materials from",
    "start": "6600",
    "end": "12780"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "12780",
    "end": "17990"
  },
  {
    "text": " PROFESSOR: The AEP is probably\none of the most difficult",
    "start": "17990",
    "end": "26450"
  },
  {
    "start": "20000",
    "end": "215000"
  },
  {
    "text": "concepts we talk about\nin this course. It seems simple to start with.",
    "start": "26450",
    "end": "32369"
  },
  {
    "text": "As I said before, it's one of\nthose things where you think you understand it, and\nthen you think you don't understand it.",
    "start": "32370",
    "end": "38240"
  },
  {
    "text": "When Shannon first came out with\nthis theory, there were a lot of very, very good\nprofessional mathematicians",
    "start": "38240",
    "end": "44670"
  },
  {
    "text": "who spent a long time trying\nto understand this. And who, in fact, blew it.",
    "start": "44670",
    "end": "49920"
  },
  {
    "text": "Because, in fact, they were\ntrying to look at it strictly in terms of mathematics.",
    "start": "49920",
    "end": "55930"
  },
  {
    "text": "They were looking for strict\nmathematical theorems, they weren't looking to try to\nget the insight from it.",
    "start": "55930",
    "end": "61590"
  },
  {
    "text": "Because of that they\ncouldn't absorb it. There were a lot of engineers\nwho looked at it, and couldn't absorb it because they couldn't\nmatch it with any of",
    "start": "61590",
    "end": "69690"
  },
  {
    "text": "the mathematics, and therefore\nthey started thinking there was more there then there really\nwas so, so this is, in",
    "start": "69690",
    "end": "75530"
  },
  {
    "text": "fact, tricky. What we're looking at is a\nsequence of chance variables",
    "start": "75530",
    "end": "84820"
  },
  {
    "text": "coming from a discrete\nmemoryless source. In other words, a discrete\nmemoryless source is something",
    "start": "84820",
    "end": "91650"
  },
  {
    "text": "that spits out symbols and each\nsymbol is independent of each other symbol, each symbol\nhas the same probability mass",
    "start": "91650",
    "end": "99810"
  },
  {
    "text": "function as every\nother symbol. We said it's a neat thing to\nlook at this log pmf random",
    "start": "99810",
    "end": "106380"
  },
  {
    "text": "variable, and a log pmf random\nvariable is minus log of the probability of the particular\nsymbol.",
    "start": "106380",
    "end": "115550"
  },
  {
    "text": "So we have a sequence now\nof random variables. And the expected value of that\nrandom variable, for each of",
    "start": "115550",
    "end": "124430"
  },
  {
    "text": "the random variables, is the\nexpected value of the log pmf",
    "start": "124430",
    "end": "132280"
  },
  {
    "text": "which is, as we said before, is\nthis thing we've called the entropy, which we're trying\nto get some insight into.",
    "start": "132280",
    "end": "138240"
  },
  {
    "text": "So we have this log pmf random\nvariable, we have the entropy, which is the expected\nvalue of it.",
    "start": "138240",
    "end": "143970"
  },
  {
    "text": "We then talked about\nthe sequence of these random variables. We talked about the sample\naverage of them, and the whole",
    "start": "143970",
    "end": "151989"
  },
  {
    "text": "reason you want to look at this\nrandom variable is the sample average of a lot of these\npmf's, since logs add",
    "start": "151990",
    "end": "160439"
  },
  {
    "text": "when the thing you're taking\nthe log of multiplies. What you wind up with is the sum\nof these log pmf's is, in",
    "start": "160440",
    "end": "169870"
  },
  {
    "text": "fact, equal to minus the lof\nof the probability of the whole sequence. In other words, you look at this\nwhole sequence as a big,",
    "start": "169870",
    "end": "179200"
  },
  {
    "text": "giant chance variable, which\nhas capital X to the n",
    "start": "179200",
    "end": "185599"
  },
  {
    "text": "different possible values. Namely, every possible\nsequence of length n. When you're talking about source\ncoding, you have to",
    "start": "185600",
    "end": "192420"
  },
  {
    "text": "find the code word for\neach of those m to",
    "start": "192420",
    "end": "197680"
  },
  {
    "text": "the n different sequences. So what you're doing here\nis trying to look at the",
    "start": "197680",
    "end": "203260"
  },
  {
    "text": "probability of each of those\nm to the n sequences. We can then use Huffman coding,\nor whatever we choose",
    "start": "203260",
    "end": "210130"
  },
  {
    "text": "to use, to try to encode\nthose things. ",
    "start": "210130",
    "end": "216849"
  },
  {
    "start": "215000",
    "end": "367000"
  },
  {
    "text": "OK, the weak law of large\nnumbers applies here, and says that the probability that this\nsample average of the log pmf",
    "start": "216850",
    "end": "226450"
  },
  {
    "text": "is close to the expected\nvalue of the log pmf. The probability that that's\ngreater than or equal to",
    "start": "226450",
    "end": "233370"
  },
  {
    "text": "epsilon is less than or equal to\nthe variance of the log pmf random variable, divided by\nn times epsilon squared.",
    "start": "233370",
    "end": "242850"
  },
  {
    "text": "Now, we are going to take the\nviewpoint that n epsilon",
    "start": "242850",
    "end": "248090"
  },
  {
    "text": "squared, is very small. Excuse me, it's very large. Epsilon we're thinking as a\nsmall number, and we're",
    "start": "248090",
    "end": "255970"
  },
  {
    "text": "thinking of as a large number,\nbut the game we always play here is to first pick some\nepsilong, as small as you want",
    "start": "255970",
    "end": "265789"
  },
  {
    "text": "to make it, and then you make\nn larger and larger. And as n gets bigger and bigger,\neventually this number",
    "start": "265790",
    "end": "274640"
  },
  {
    "text": "gets very small. So that's the game that\nwe're playing. It's y n times epsilon squared,\nwe're thinking of it",
    "start": "274640",
    "end": "281070"
  },
  {
    "text": "is a large number. So what this says is the\nprobability that this log pmf",
    "start": "281070",
    "end": "287590"
  },
  {
    "text": "of the entire sequence, well the\nsample value of it, namely",
    "start": "287590",
    "end": "292600"
  },
  {
    "text": "the log pmf divided by n,\nus close to H of X. The",
    "start": "292600",
    "end": "297780"
  },
  {
    "text": "probability of that is less\nthan or equal to this. We then define the typical\nset, T sub epsilon of n.",
    "start": "297780",
    "end": "306180"
  },
  {
    "text": "This is the set of all typical\nsequences out of the source.",
    "start": "306180",
    "end": "311240"
  },
  {
    "text": "So we're defining typical\nsequences in this way. It's a set of all sequences\nwhich are",
    "start": "311240",
    "end": "318000"
  },
  {
    "text": "what we put into there. Well, it's what we put into\nthere, but we're looking at the compliment of this set.",
    "start": "318000",
    "end": "324340"
  },
  {
    "text": "These are the exceptional\nthings. These are the typical things.",
    "start": "324340",
    "end": "329790"
  },
  {
    "text": "We're saying that the\nexceptional things here have a small probability when you\nmake n big enough.",
    "start": "329790",
    "end": "335650"
  },
  {
    "text": "This is saying that the\nexceptional things don't amount to anything, and the\ntypical things fill up the",
    "start": "335650",
    "end": "342680"
  },
  {
    "text": "entire probability space. So what were saying is the\nprobability that this sequence",
    "start": "342680",
    "end": "349880"
  },
  {
    "text": "is actually typical is greater\nthan or equal to 1 minus something small.",
    "start": "349880",
    "end": "355490"
  },
  {
    "text": "It says when n gets big enough\nthe probability that you get a",
    "start": "355490",
    "end": "360940"
  },
  {
    "text": "typical sequence out of the\nsource is going to wan. ",
    "start": "360940",
    "end": "368580"
  },
  {
    "start": "367000",
    "end": "462000"
  },
  {
    "text": "OK. We drew this in terms of a\nprobability distribution, which I hope makes things look a\nlittle more straightforward.",
    "start": "368580",
    "end": "376530"
  },
  {
    "text": "This is the distribution\nfunction of the sample average of this log pmf random\nvariable.",
    "start": "376530",
    "end": "383319"
  },
  {
    "text": "And what we're saying is that\nas n gets larger and larger, the thing that's going to happen\nbecause of the fact",
    "start": "383320",
    "end": "390949"
  },
  {
    "text": "that the variance of the sample\naverage -- the sample average is always a\nrandom variable.",
    "start": "390950",
    "end": "398229"
  },
  {
    "text": "The average is not a random\nvariable, it's a fixed number. And what this is saying is\nthat as n gets larger and",
    "start": "398230",
    "end": "405150"
  },
  {
    "text": "larger, this distribution\nfunction here gets closer and closer to a stack.",
    "start": "405150",
    "end": "412530"
  },
  {
    "text": "In other words, the thing that's\nhappening is that as n gets big, you go along here,\nnothing happens suddenly you",
    "start": "412530",
    "end": "420580"
  },
  {
    "text": "move up here and suddenly\nyou move across there. That's what happens in the\nlimit, mainly the sample",
    "start": "420580",
    "end": "427270"
  },
  {
    "text": "average is, at that point,\nalways equal to the average.",
    "start": "427270",
    "end": "433050"
  },
  {
    "text": "So as n goes to infinity,\na typical set approaches probability 1. We express that in terms of the\nChebyshev inequality in",
    "start": "433050",
    "end": "440740"
  },
  {
    "text": "this way, but the picture says\nyou can interpret it in any one of a hundred\ndifferent ways.",
    "start": "440740",
    "end": "446889"
  },
  {
    "text": "Because the real essence of\nthis is not the Chebyshev inequality, the real essence of\nit is saying that as n gets",
    "start": "446890",
    "end": "453970"
  },
  {
    "text": "bigger and bigger,\nthis distribution function becomes a stack. So that's a nice way of\nthinking about it.",
    "start": "453970",
    "end": "459970"
  },
  {
    "text": " Let's summarized what\nwe did with that.",
    "start": "459970",
    "end": "466570"
  },
  {
    "start": "462000",
    "end": "846000"
  },
  {
    "text": "All of the major results\nabout typical sets. The first of them,\nis this one.",
    "start": "466570",
    "end": "473170"
  },
  {
    "text": "It's a bound on the number of\nelements in a typical set, I'm not going to rederive that\nagain, but it comes from",
    "start": "473170",
    "end": "479259"
  },
  {
    "text": "looking at the fact that all of\nthe typical elements have roughly the same probability.",
    "start": "479260",
    "end": "485789"
  },
  {
    "text": "All of them collectively fill\nup the whole space, and therefore you can find the\nprobability of each of them by",
    "start": "485790",
    "end": "491760"
  },
  {
    "text": "taking one and dividing by the\nprobability of each of them. When you do that, you\nget two bounds.",
    "start": "491760",
    "end": "497139"
  },
  {
    "text": "One of them says that this\nmagnitude is less than 2 to the n times H of\nX plus epsilon.",
    "start": "497140",
    "end": "503380"
  },
  {
    "text": "The other one says it's greater\nthan 2 to the n H of X minus epsilon. And you have to throw in this\nlittle fudge factor here",
    "start": "503380",
    "end": "511130"
  },
  {
    "text": "because of the fact that the\ntypical set doesn't quite fill up the whole space.",
    "start": "511130",
    "end": "516490"
  },
  {
    "text": "What that all says is when n\ngets large the number of typical elements is\napproximately equal to 2 to",
    "start": "516490",
    "end": "523000"
  },
  {
    "text": "the n times H of X.\nAgain, think hard about what this means.",
    "start": "523000",
    "end": "528990"
  },
  {
    "text": "This number here really isn't\nall like 2 to the n H of X.",
    "start": "528990",
    "end": "534529"
  },
  {
    "text": "This n times epsilon, is going\nto be a big number. As n gets bigger, it gets\nbigger and bigger.",
    "start": "534530",
    "end": "542610"
  },
  {
    "text": "But, at the same time, when you\nlook at 2 to the n times H of X plus epsilon, and you know\nthat H of X is something",
    "start": "542610",
    "end": "550090"
  },
  {
    "text": "substantial and you know that\nepsilon is very small, in some sense it still says this.",
    "start": "550090",
    "end": "557600"
  },
  {
    "text": "And in source coding terms,\nit very much says this. Because in source coding terms,\nwhat you're always",
    "start": "557600",
    "end": "563050"
  },
  {
    "text": "looking at is these exponents. Because if you have n different\nthings you're trying",
    "start": "563050",
    "end": "569060"
  },
  {
    "text": "to encode, it takes you log\nof n bits to encode them. So when you take the log of\nthis, you see that the number",
    "start": "569060",
    "end": "575890"
  },
  {
    "text": "of extra bits you needs to\nencode these sequences is on the order of n times epsilon.",
    "start": "575890",
    "end": "582029"
  },
  {
    "text": "Which is some number of bits,\nbut the real number bits you're looking at is n times\nH of X. So that's the major",
    "start": "582030",
    "end": "589110"
  },
  {
    "text": "term, and this is just fiddly. The next thing we found is that\nthe probability of an",
    "start": "589110",
    "end": "594630"
  },
  {
    "text": "element in a typical set is\nbetween 2 the minus n times H of X plus epsilon and 2\nto the minus n times",
    "start": "594630",
    "end": "601280"
  },
  {
    "text": "H of X minus epsilon. Which is saying almost the same\nthing as this is saying.",
    "start": "601280",
    "end": "607180"
  },
  {
    "text": "And again, there's this\napproximation which says this is about equal to 2 to the minus\nn of H of X. And this is",
    "start": "607180",
    "end": "616130"
  },
  {
    "text": "an approximation in exactly the\nsame sense that that's an approximation.",
    "start": "616130",
    "end": "621440"
  },
  {
    "text": "Finally, the last statement is\nthat the probability that this typical --",
    "start": "621440",
    "end": "627890"
  },
  {
    "text": "the probability that you get a\ntypical sequence is greater than or equal to 1 minus this\nvariance divided by n times",
    "start": "627890",
    "end": "635140"
  },
  {
    "text": "epsilon squared, with the same\nkind of approximation. The probability that\nyou get the typical",
    "start": "635140",
    "end": "641250"
  },
  {
    "text": "sequence is about 1. So what this is saying is\nthere are hardly any",
    "start": "641250",
    "end": "646600"
  },
  {
    "text": "exceptions in terms\nof probability. There are a huge number\nof exceptions. But they're all extraordinarily\nsmall in",
    "start": "646600",
    "end": "652970"
  },
  {
    "text": "probability. So most of the space is all\ntucked into this typical set.",
    "start": "652970",
    "end": "660440"
  },
  {
    "text": "Most of these, all of these\ntypical sequences, have about the same probability.",
    "start": "660440",
    "end": "665699"
  },
  {
    "text": "And the number of typical\nsequences is about 2 to the n times H of X.",
    "start": "665700",
    "end": "671450"
  },
  {
    "text": "Does that say that the entropy\nhas any significance? It sure does.",
    "start": "671450",
    "end": "677020"
  },
  {
    "text": "Because the entropy is really\nwhat's determining everything about this distribution,\nas far as",
    "start": "677020",
    "end": "682270"
  },
  {
    "text": "probabilities are concerned. There's nothing left\nbeyond that. If you're going to look at very\nlong sequences, and in",
    "start": "682270",
    "end": "689250"
  },
  {
    "text": "source coding we want\nto look at long sequences, that's the story.",
    "start": "689250",
    "end": "694460"
  },
  {
    "text": "So the entropy tells a story. Despite what Huffman said.",
    "start": "694460",
    "end": "700470"
  },
  {
    "text": "Huffman totally ignored the idea\nof entropy and because of that he came up with the optimal\nalgorithm, but his",
    "start": "700470",
    "end": "708140"
  },
  {
    "text": "optimal algorithm didn't\ntell him anything about what was going on. This is not the [UNINTELLIGIBLE] Huffman, I think his algorithm\nis one of the neatest things",
    "start": "708140",
    "end": "718260"
  },
  {
    "text": "I've ever seen, because\nhe came up with it out of nowhere. Just pure thought,\nwhich is nice.",
    "start": "718260",
    "end": "724070"
  },
  {
    "text": "We then start to talk about\nfixed lenght to fized length source coding. This is not a practical thing.",
    "start": "724070",
    "end": "730140"
  },
  {
    "text": "This is not something I would\nadvise trying to do. It's something which is\nconceptually useful, because",
    "start": "730140",
    "end": "737560"
  },
  {
    "text": "it's talks about anything you\ncan do eventually, when you look at an almost infinite\nsequence of symbols from the",
    "start": "737560",
    "end": "745279"
  },
  {
    "text": "source and you look at how\nmany bits it takes you to represent them.",
    "start": "745280",
    "end": "750310"
  },
  {
    "text": "Ultimately you need to turn that\nencoder on, at some point in pre-history, and pre-history\nin our present day",
    "start": "750310",
    "end": "758700"
  },
  {
    "text": "is about one year ago. And you have to turn it off\nsometime in the distant future, which is maybe\nsix months from now.",
    "start": "758700",
    "end": "767300"
  },
  {
    "text": "During that time you have to\nencode all these bits. So, in fact, when you get it\nall done and look at the",
    "start": "767300",
    "end": "773160"
  },
  {
    "text": "overall picture, it's fixed\nlength to fixed length. And all of these algorithms\nare just ways of doing the",
    "start": "773160",
    "end": "780560"
  },
  {
    "text": "fixed length to fixed length\nwithout too much delay involved in them.",
    "start": "780560",
    "end": "786890"
  },
  {
    "text": " I didn't say everything there.",
    "start": "786890",
    "end": "794350"
  },
  {
    "text": "What this typical set picture\ngives us there, is you can achieve an expected number of\nbits per source symbol, which",
    "start": "794350",
    "end": "803640"
  },
  {
    "text": "is about n times H of X, with\nvery rare failures. And if you try to achieve H of\nX minus epsilon bits per",
    "start": "803640",
    "end": "812190"
  },
  {
    "text": "symbol, the interesting thing\nwe found last time, was that the fraction of sequences you\ncan encode was zilch.",
    "start": "812190",
    "end": "822949"
  },
  {
    "text": "In other words, there is a very\nrapid transition here. If you try to get by with too\nfew bits per symbol, you die",
    "start": "822950",
    "end": "831320"
  },
  {
    "text": "very, very quickly. It's not that your error\nprobability is large, your error probability is\nasymptotically equal to 1.",
    "start": "831320",
    "end": "839399"
  },
  {
    "text": "You always screw up. So that's the picture.",
    "start": "839400",
    "end": "847890"
  },
  {
    "start": "846000",
    "end": "935000"
  },
  {
    "text": "We want to go onto\nMarkow sources. Let me explain why I want\nto do this first.",
    "start": "847890",
    "end": "853080"
  },
  {
    "text": " When we're talking about the\ndiscrete memoryless sources,",
    "start": "853080",
    "end": "859100"
  },
  {
    "text": "it should be obvious that that's\ntotally a toy problem.",
    "start": "859100",
    "end": "864529"
  },
  {
    "text": "There aren't any sources I can\nimagine where you would want to encode their output where\nyou can reasonably conclude",
    "start": "864530",
    "end": "872640"
  },
  {
    "text": "that they were discrete\nand memoryless. Namely, that each symbol\nwas independent of each other symbol.",
    "start": "872640",
    "end": "878250"
  },
  {
    "text": "The only possibility I can\nthink of is where you're trying to report the results\nof gambling or something.",
    "start": "878250",
    "end": "884890"
  },
  {
    "text": "And gambling is so dishonest\nthat they probably aren't independent anyway. You could use this is a way\nof showing they aren't",
    "start": "884890",
    "end": "891730"
  },
  {
    "text": "independent, but it's not\na very useful thing. So somehow we want to be able\nto talk about how do you",
    "start": "891730",
    "end": "899300"
  },
  {
    "text": "encode sources with memory. Well Markow sources are\nthe easiest kind of",
    "start": "899300",
    "end": "904860"
  },
  {
    "text": "sources with memory. And they have the nice property\nthat you can include",
    "start": "904860",
    "end": "910260"
  },
  {
    "text": "as much statistics in\nthem as you want to. You can make them include as\nmuch of the structure you can",
    "start": "910260",
    "end": "916710"
  },
  {
    "text": "find as anything else will do. So that people talk about much\nmore general classes of",
    "start": "916710",
    "end": "924290"
  },
  {
    "text": "sources, but these are\nreally sufficient. These are sufficient to talk\nabout everything useful.",
    "start": "924290",
    "end": "930060"
  },
  {
    "text": "Not necessarily the nicest way\nto think about useful things but it's sufficient.",
    "start": "930060",
    "end": "935990"
  },
  {
    "start": "935000",
    "end": "1109000"
  },
  {
    "text": "So a finite state in Markov\nchain, I assume you're somewhat familiar with Markov\nchaings from taking a",
    "start": "935990",
    "end": "941690"
  },
  {
    "text": "probability courses. If not, you should probably\nreview it there, because the",
    "start": "941690",
    "end": "946820"
  },
  {
    "text": "notes go through it\npretty quickly. There's nothing terribly\ncomplicated there, but a finite state in a Markov chain\nis a sequence of discrete",
    "start": "946820",
    "end": "955820"
  },
  {
    "text": "chance variables, in that sense\nit's exactly like the discrete memoryless sources\nwe were looking at.",
    "start": "955820",
    "end": "962700"
  },
  {
    "text": "The letters come from some\nfinite alphabet, so in that sense it's like the discrete\nmemoryless sources.",
    "start": "962700",
    "end": "970590"
  },
  {
    "text": "But here the difference is that\neach letter depends on",
    "start": "970590",
    "end": "976740"
  },
  {
    "text": "the letter before. Namely, before the Markov chain\nchanges from one state",
    "start": "976740",
    "end": "983579"
  },
  {
    "text": "to another state in one of these\nsteps, it looks at the state it's in and decides where\nit's going to go next.",
    "start": "983580",
    "end": "991579"
  },
  {
    "text": "So we have a transition\nprobability matrix, you can",
    "start": "991580",
    "end": "996960"
  },
  {
    "text": "think of this as a matrix,\nit's something which has values for every S in the state\nspace, and for everys S",
    "start": "996960",
    "end": "1003350"
  },
  {
    "text": "prime in the state space. Which represents the probability\nthat the state at",
    "start": "1003350",
    "end": "1009800"
  },
  {
    "text": "time n is equal the this state\nS, and the state of time n minus 1 is equal to the\nstates S prime.",
    "start": "1009800",
    "end": "1019000"
  },
  {
    "text": "So this tells you what's the\nprobabilities are of going from one state to another.",
    "start": "1019000",
    "end": "1024069"
  },
  {
    "text": "The important thing here is\nthat this single step",
    "start": "1024070",
    "end": "1029439"
  },
  {
    "text": "transition incorporates all of\nthe statistical knowledge. In other words, this is also\nequal to the probability that",
    "start": "1029440",
    "end": "1038600"
  },
  {
    "text": "S n is equal to this state S,\ngiven that S n minus 1 is equal to the state S prime, and\nalso that S n minus 2 is",
    "start": "1038600",
    "end": "1048120"
  },
  {
    "text": "equal to any given state of time\nn minus 2, and all the way back to S minus zero being\nany old state at time S zero.",
    "start": "1048120",
    "end": "1057650"
  },
  {
    "text": "So it says that this source\nloses memory, except for the first thing back.",
    "start": "1057650",
    "end": "1064370"
  },
  {
    "text": "I like to think of this as a\nblind frog who has very good sensory perception, jumping\naround on lily pads.",
    "start": "1064370",
    "end": "1072360"
  },
  {
    "text": "In other words, he can't see\nthe lily pad, he can only sense the nearby lily pads.",
    "start": "1072360",
    "end": "1077530"
  },
  {
    "text": "So he jumps from one lily pad\nto the next and when he gets to the next lily pad, he then\ndecides which lily pad he's",
    "start": "1077530",
    "end": "1083960"
  },
  {
    "text": "going go to go to next. ",
    "start": "1083960",
    "end": "1090250"
  },
  {
    "text": "That'll wake you up, anyway.  And we also want to define\nsome initial pmf on the",
    "start": "1090250",
    "end": "1100120"
  },
  {
    "text": "initial state. So you like to think of Markov\nchains as starting at some time and then proceeding forever\ninto the future.",
    "start": "1100120",
    "end": "1107880"
  },
  {
    "text": " That seems to indicate that all\nwe've done is to replace",
    "start": "1107880",
    "end": "1114990"
  },
  {
    "start": "1109000",
    "end": "1395000"
  },
  {
    "text": "one trivial problem with another\ntrivial problem. In other words, on step memory\nis not enough to deal with",
    "start": "1114990",
    "end": "1122020"
  },
  {
    "text": "things like English text\nand things like that. Or any other language\nthat you like.",
    "start": "1122020",
    "end": "1128960"
  },
  {
    "text": "So the idea of a Markov source\nis that you create whatever",
    "start": "1128960",
    "end": "1135630"
  },
  {
    "text": "set of states that you want,\nyou can have a very large state space, but you associate\nthe output of the source not",
    "start": "1135630",
    "end": "1144480"
  },
  {
    "text": "with the states, but with\nthe transitions from one state to another.",
    "start": "1144480",
    "end": "1149750"
  },
  {
    "text": "So this gives an example of it,\nit's easier to explain the idea by simply looking\nat an example.",
    "start": "1149750",
    "end": "1157370"
  },
  {
    "text": "In this particular example,\nwhich I think is the same as the one in the notes, what\nyou're looking at is a memory",
    "start": "1157370",
    "end": "1165299"
  },
  {
    "text": "of the two previous states. So this is a binary source, it\nproduces binary digits, just",
    "start": "1165300",
    "end": "1170590"
  },
  {
    "text": "either zero or 1. If the previous two digits were\nzero zero, it says that",
    "start": "1170590",
    "end": "1178190"
  },
  {
    "text": "the next digit is going to be\na 1 with probability 0.1 and",
    "start": "1178190",
    "end": "1183240"
  },
  {
    "text": "the next is going to be a zero\nwith probability 0.9. If you're in states 0,0 and the\nnext thing that comes out",
    "start": "1183240",
    "end": "1193280"
  },
  {
    "text": "is a zero, then at that point,\nnamely at that point one advanced into the future, you\nhave a zero as your last",
    "start": "1193280",
    "end": "1201179"
  },
  {
    "text": "digit, a zero as a previous\ndigit, and a zero as the digit before that. So your state has move from xn\nminus 2 and xn minus 1, to xn",
    "start": "1201180",
    "end": "1215020"
  },
  {
    "text": "minus 1 and xn. In other words, any time you\nmake a transition, this digit",
    "start": "1215020",
    "end": "1223830"
  },
  {
    "text": "here, which is the previous\ndigit, has to always become that digit.",
    "start": "1223830",
    "end": "1230590"
  },
  {
    "text": "You'll notice the same thing\nin all of these. When you go from here to there,\nthis last digit becomes",
    "start": "1230590",
    "end": "1237750"
  },
  {
    "text": "the first digit. When you go from here to here,\nthe last digit because the",
    "start": "1237750",
    "end": "1243379"
  },
  {
    "text": "first digit, and so forth. And that's a characteristic of\nthis particular structure of",
    "start": "1243380",
    "end": "1248970"
  },
  {
    "text": "having the memory represented\nby the last two digits. So the kind of output you can\nget from this source --",
    "start": "1248970",
    "end": "1256730"
  },
  {
    "text": "I mean you see that it doesn't\ndo anything very interesting. When you have two zeroes in the\npast, it tends to produce",
    "start": "1256730",
    "end": "1265040"
  },
  {
    "text": "a lot more zeroes, it tends to\nget stuck with lots of zeroes. If you got a single 1 that goes\nover into this state, and",
    "start": "1265040",
    "end": "1274240"
  },
  {
    "text": "from there it can either\ngo there or there. Once it gets here, it tends to\nproduce a large number of 1's.",
    "start": "1274240",
    "end": "1282390"
  },
  {
    "text": "So what's happening here is you\nhave a Markov chain which goes from long sequences of\nzeroes, to transition regions",
    "start": "1282390",
    "end": "1289530"
  },
  {
    "text": "where there are a bunch of zeros\nand 1's, and finally gets trapped into either the all\nzero state again, or the",
    "start": "1289530",
    "end": "1295640"
  },
  {
    "text": "all 1 state again. And moves on from there. So the letter is what's in this\ncase the source output.",
    "start": "1295640",
    "end": "1307460"
  },
  {
    "text": "If you know the old state, the\nsource output specifies in this state. If you know the old state and\nthe new state, that specifies",
    "start": "1307460",
    "end": "1315730"
  },
  {
    "text": "the letter. In other words, one of the\ncurious things about this chain is I've arranged it so\nthat, since we have a binary",
    "start": "1315730",
    "end": "1322410"
  },
  {
    "text": "output, they're only\ntwo possible transitions from each state. Which says that the state plus\nthe sequence of source outputs",
    "start": "1322410",
    "end": "1331840"
  },
  {
    "text": "specifies the state at every\npoint in time, the stayed at every point in time specifies\nthe source sequence.",
    "start": "1331840",
    "end": "1339490"
  },
  {
    "text": "In other words, the two are\nisomorphic to each other. One specifies the other. Since one specifies the other,\nyou can pretty much forget",
    "start": "1339490",
    "end": "1348240"
  },
  {
    "text": "about the sequence and\nlook at the state chain, if you want to.",
    "start": "1348240",
    "end": "1353890"
  },
  {
    "text": "And therefore everything you\nknow about Markov chains is useful here. Or if you like to think about\nthe real source, you can think",
    "start": "1353890",
    "end": "1360650"
  },
  {
    "text": "about the real source as\nproducing these letters. So either way is fine\nbecause either one",
    "start": "1360650",
    "end": "1366039"
  },
  {
    "text": "specifies the other. When you don't have that\nproperty, and you look at a",
    "start": "1366040",
    "end": "1371140"
  },
  {
    "text": "sequence of letters from this\nsource, these are called partially specified\nMarkov chains.",
    "start": "1371140",
    "end": "1378710"
  },
  {
    "text": "And they're awful things\nto deal with. You can write theses about, but\nyou don't get any insight",
    "start": "1378710",
    "end": "1384940"
  },
  {
    "text": "about them. There's hardly anything that\nyou would like to be true which is true.",
    "start": "1384940",
    "end": "1390540"
  },
  {
    "text": "And these are just awful\nthings to look at. So we won't look them.",
    "start": "1390540",
    "end": "1396710"
  },
  {
    "text": "One of the nice things about\nengineering is you can create your own models. Mathematicians have to look\nat the crazy models that",
    "start": "1396710",
    "end": "1404420"
  },
  {
    "text": "engineers suggest to them. They have no choice. That's their job.",
    "start": "1404420",
    "end": "1409620"
  },
  {
    "text": "But as an engineer, we can only\nlook at the nice models. We can play and the\nmathematicians have to work.",
    "start": "1409620",
    "end": "1416160"
  },
  {
    "text": "So it's nicer to be an\nengineer, I think.",
    "start": "1416160",
    "end": "1421650"
  },
  {
    "text": "Although famous mathematicians\nonly look at the engineering problems that appeal to them,\nso in fact, when they become",
    "start": "1421650",
    "end": "1428870"
  },
  {
    "text": "famous the two groups come\nback together again. Because the good engineers are\nalso good mathematician, so",
    "start": "1428870",
    "end": "1436100"
  },
  {
    "text": "they become sort of\nthe same group. These transitions, mainly the\ntransition lines that we draw",
    "start": "1436100",
    "end": "1445950"
  },
  {
    "text": "on a graph like this, always\nindicate positive probability. In other words, that there's\nzero probability from going to",
    "start": "1445950",
    "end": "1452540"
  },
  {
    "text": "here to here. You don't clutter up the diagram\nby putting a line in, which allows you to just look at\nthese transitions to figure",
    "start": "1452540",
    "end": "1460650"
  },
  {
    "text": "out what's going on. One of the things that you\nlearned when you study finite",
    "start": "1460650",
    "end": "1467490"
  },
  {
    "text": "state Markov chains, is that a\nstate s is accessible from",
    "start": "1467490",
    "end": "1473730"
  },
  {
    "text": "some other state s prime, if the\ngraph has some path from s prime to s.",
    "start": "1473730",
    "end": "1479360"
  },
  {
    "text": "In other words, it's not\nsaying you can go there in one step. It's saying that there's some\nway you can get there if you",
    "start": "1479360",
    "end": "1485740"
  },
  {
    "text": "go long enough. Which means there's some\nprobability of getting there. And the fact that there's a\nprobability of getting there",
    "start": "1485740",
    "end": "1494490"
  },
  {
    "text": "pretty much means that you're\ngoing to get there eventually. That's not an obvious statement\nbut let's see what",
    "start": "1494490",
    "end": "1502200"
  },
  {
    "text": "that means here. This state is accessible from\nthis state, in the sense that",
    "start": "1502200",
    "end": "1508210"
  },
  {
    "text": "you can get from here to there\nby going over here and then going to here.",
    "start": "1508210",
    "end": "1515890"
  },
  {
    "text": "If we look at the states which\nare accessible from each other, you get some set of\nstates, and if you're in that",
    "start": "1515890",
    "end": "1523640"
  },
  {
    "text": "set of states, you can\nnever get out of it. Therefore, every one of those\nstates remains with positive",
    "start": "1523640",
    "end": "1530290"
  },
  {
    "text": "probability and you keep\nrotating back and forth between them in some way, but\nyou never get out of them.",
    "start": "1530290",
    "end": "1538950"
  },
  {
    "text": "In other words, a Markov chain\nwhich doesn't have this property would be the following\nMarkov chain.",
    "start": "1538950",
    "end": "1546700"
  },
  {
    "text": " That's the simplest one\nI can think of.",
    "start": "1546700",
    "end": "1551880"
  },
  {
    "text": "If you start out in this\nstate you stay there. If you start out in this state,\nyou stay there, That's",
    "start": "1551880",
    "end": "1560100"
  },
  {
    "text": "not a very nice chain. Is this a decent model for\nan engineering study?",
    "start": "1560100",
    "end": "1565390"
  },
  {
    "text": "No. Because when you're looking at\nengineering, the thing that",
    "start": "1565390",
    "end": "1571120"
  },
  {
    "text": "you're interested in, is you're\nlooking at something that happens over a long\nperiod of time.",
    "start": "1571120",
    "end": "1576240"
  },
  {
    "text": "Back at time infinity you can\ndecide whether you're here, or whether you're here, and you\nmight as well not worry a",
    "start": "1576240",
    "end": "1584940"
  },
  {
    "text": "whole lot about what happened\nback at time minus infinity, as far as building your model.",
    "start": "1584940",
    "end": "1590800"
  },
  {
    "text": "You may as well just build a\nmodel for this, or build a model for that. There's another thing here,\nwhich is periodicity.",
    "start": "1590800",
    "end": "1599030"
  },
  {
    "text": "In some chains, you can go from\nthis state to this state",
    "start": "1599030",
    "end": "1605980"
  },
  {
    "text": "in one step, well\none, two steps. Or you can go there in one,\ntwo, three, four steps.",
    "start": "1605980",
    "end": "1614350"
  },
  {
    "text": "Or you can go there in one, two,\nthree steps and so forth.",
    "start": "1614350",
    "end": "1620650"
  },
  {
    "text": "Which says, in terms of m the\nperiod of s is the greatest",
    "start": "1620650",
    "end": "1625690"
  },
  {
    "text": "common denominator\nof path lengths from s back to s again.",
    "start": "1625690",
    "end": "1631250"
  },
  {
    "text": "If that period is equal to 1,\nmainly if there's not some",
    "start": "1631250",
    "end": "1636850"
  },
  {
    "text": "periodic structure which says\nthe only way you can get back to a state is by coming back\nevery two steps or every three",
    "start": "1636850",
    "end": "1647000"
  },
  {
    "text": "steps or something, then\nagain it's not a very nice Markov chain. Because if you're modeling it,\nyou might as well just model",
    "start": "1647000",
    "end": "1656060"
  },
  {
    "text": "things over two states instead\nof over single states. So the upshot of that is you\ndefine these nice Markov",
    "start": "1656060",
    "end": "1665510"
  },
  {
    "text": "chains, which are aperiodic,\nwhich don't have any of this periodic structure, and every\nstate is accessible from every",
    "start": "1665510",
    "end": "1673470"
  },
  {
    "text": "other state. And you call them ergodic\nMarkov chains.",
    "start": "1673470",
    "end": "1678790"
  },
  {
    "text": "And what ergodic means, sort\nof and as a more general",
    "start": "1678790",
    "end": "1684600"
  },
  {
    "text": "principle, is that the\nprobabilities of things are",
    "start": "1684600",
    "end": "1695140"
  },
  {
    "text": "equal to the relative\nfrequency of things. Namely, if you look at a very\nlong sequence of things out of",
    "start": "1695140",
    "end": "1700210"
  },
  {
    "text": "a Markov chain, what you see\nin that very long sequence should be representative of the\nprobabilities of that very",
    "start": "1700210",
    "end": "1708420"
  },
  {
    "text": "long sequence. The probabilities of transitions\nat various times should be the same from\none time to another.",
    "start": "1708420",
    "end": "1715380"
  },
  {
    "text": "And that's whate ergodicity\nmeans. It means that the thing\nis stationery. You look at it at one time, it\nbehaves the same way as at",
    "start": "1715380",
    "end": "1722080"
  },
  {
    "text": "another time. It doesn't have any periodic\nstructure, which means if you look at it at even times, it\nbehaves differently from",
    "start": "1722080",
    "end": "1729110"
  },
  {
    "text": "looking at it at odd times. That's the kind of Markov chain\nyou would think you would have, unless you look at\nthese odd ball examples of",
    "start": "1729110",
    "end": "1736890"
  },
  {
    "text": "other things. Everything we do is going to\nbe based on the idea of ergodic Markov chains,\nbecause they're the",
    "start": "1736890",
    "end": "1743580"
  },
  {
    "text": "nicest models to use. A Markov source then is\na sequence of labeled",
    "start": "1743580",
    "end": "1749250"
  },
  {
    "text": "transitions on an and ergodic\nMarkov chain. Those are the only things\nwe want to look at.",
    "start": "1749250",
    "end": "1754529"
  },
  {
    "text": "But that's general enough\nto do most of the things we want to do. ",
    "start": "1754530",
    "end": "1761860"
  },
  {
    "start": "1760000",
    "end": "2045000"
  },
  {
    "text": "And once you have ergodic Markov\nchains, there are a lot of nice things that happen. ",
    "start": "1761860",
    "end": "1769690"
  },
  {
    "text": "Mainly, if you try to solve this\nset of equations, namely",
    "start": "1769690",
    "end": "1778460"
  },
  {
    "text": "if you want to say, well suppose\nthere is some pmf function which gives\nme the relative",
    "start": "1778460",
    "end": "1785090"
  },
  {
    "text": "frequency of a given state. Namely, if I look at an enormous\nnumber of states, I",
    "start": "1785090",
    "end": "1791470"
  },
  {
    "text": "would like the state little\ns to come up with",
    "start": "1791470",
    "end": "1797799"
  },
  {
    "text": "some relative frequency. All the time that I do it. Namely, I wouldn't like to have\none sample path which",
    "start": "1797800",
    "end": "1805610"
  },
  {
    "text": "comes up with a relative\nfrequency 1/2, and another sample path of almost infinite\nlength which comes up with a",
    "start": "1805610",
    "end": "1812050"
  },
  {
    "text": "different relative frequency. Because it would mean that\ndifferent sequences of states",
    "start": "1812050",
    "end": "1817720"
  },
  {
    "text": "are not typical of\nthe Markov chain. That's another way of looking\nat what ergodicity means.",
    "start": "1817720",
    "end": "1826460"
  },
  {
    "text": "It means that infinite\nlength sequences are not typical anymore.",
    "start": "1826460",
    "end": "1831670"
  },
  {
    "text": "It depends on when they\nstart, when they stop. It depends on whether\nyou start at an even",
    "start": "1831670",
    "end": "1838020"
  },
  {
    "text": "time or an odd time. Depends on all of these things,\nthat -- all of these",
    "start": "1838020",
    "end": "1843920"
  },
  {
    "text": "things that real sources\nshouldn't depend on. So, if you have relative\nfrequencies then you should",
    "start": "1843920",
    "end": "1851220"
  },
  {
    "text": "have those relative frequencies\nat time n and at time n minus 1. And probability of a particular\nsymbol s, if the",
    "start": "1851220",
    "end": "1862030"
  },
  {
    "text": "probabilities of the previous\nsymbol s prime were the same values, q of s prime.",
    "start": "1862030",
    "end": "1867590"
  },
  {
    "text": "We know the transition\nprobabilities, that's q of s given s prime. The sum over s prime, of q of s\nprime, given capital Q of s",
    "start": "1867590",
    "end": "1878440"
  },
  {
    "text": "given s prime, what is that? That's the probability of x.",
    "start": "1878440",
    "end": "1884340"
  },
  {
    "text": "In other words, if you start\nout at time n minus 1 with something pmf function on the\nstates at time n minus 1, this",
    "start": "1884340",
    "end": "1893100"
  },
  {
    "text": "is the formula you would use to\ncalculate the probability the pmf function for\nstates at time s.",
    "start": "1893100",
    "end": "1902560"
  },
  {
    "text": "This is the probability mass\nfunction for the states at the next unit of time, time n.",
    "start": "1902560",
    "end": "1910640"
  },
  {
    "text": "If this probability distribution\nis the same as this probability distribution,\nthen you say that you're in",
    "start": "1910640",
    "end": "1917380"
  },
  {
    "text": "steady state, because you do\nthis again, you plug this into here, it's the same thing.",
    "start": "1917380",
    "end": "1922409"
  },
  {
    "text": "You get the same answer\nat time n plus 1. You plug it in again you got the\nsame answer at time n plus",
    "start": "1922410",
    "end": "1928350"
  },
  {
    "text": "2, and so on forever. So you stay in steady state.",
    "start": "1928350",
    "end": "1934250"
  },
  {
    "text": "The question is, if you have a\nmatrix here, can you solve this equation?",
    "start": "1934250",
    "end": "1940400"
  },
  {
    "text": "Is it easy to solve? And what's the solution?",
    "start": "1940400",
    "end": "1945840"
  },
  {
    "text": "There's a nice theorem that\nsays, if the chain is ergodic, namely if it has these nice\nproperties of transition",
    "start": "1945840",
    "end": "1953000"
  },
  {
    "text": "probabilities, that corresponds\nto a particular kind of matrix here. If you have that kind of matrix,\nthis is just a vector",
    "start": "1953000",
    "end": "1961789"
  },
  {
    "text": "matrix equation. That vector matrix equation has\na unique solution then for",
    "start": "1961790",
    "end": "1969230"
  },
  {
    "text": "this probability little q, in\nterms of this transition",
    "start": "1969230",
    "end": "1974549"
  },
  {
    "text": "probability. It also has the nice property\nthat if you start out with any old distribution, and you grind\nthis thing away for a",
    "start": "1974550",
    "end": "1982790"
  },
  {
    "text": "number of times, this q of x\nis going to approach the",
    "start": "1982790",
    "end": "1988840"
  },
  {
    "text": "steady state solution. Which means if you start a\nMarkov chain out in some known",
    "start": "1988840",
    "end": "1994090"
  },
  {
    "text": "state, after awhile the\nprobability that you're in state s is going to become this\nsteady state probability.",
    "start": "1994090",
    "end": "2002190"
  },
  {
    "text": "It gets closer and closer\nto it exponentially as time goes on.",
    "start": "2002190",
    "end": "2007730"
  },
  {
    "text": "So that's just arithmetic. These steady state probabilities\nare approached",
    "start": "2007730",
    "end": "2013520"
  },
  {
    "text": "asymptotically from any starting\nstate, i.e. for all s and s prime and s.",
    "start": "2013520",
    "end": "2018790"
  },
  {
    "text": "The limit of the probability\nthat s sub n, the state at time n, is equal to a given\nstate s, given that the state",
    "start": "2018790",
    "end": "2026320"
  },
  {
    "text": "at time zero was equal\nto s prime. This probability is equal to q\nof s and the limit as n goes",
    "start": "2026320",
    "end": "2033370"
  },
  {
    "text": "to infinity. All of you know those things\nfrom studying Markov chains. I hope, because those\nare the main facts",
    "start": "2033370",
    "end": "2041670"
  },
  {
    "text": "about Markov chains. Incidentally I'm not\ninterested in Markov chains here.",
    "start": "2041670",
    "end": "2047220"
  },
  {
    "start": "2045000",
    "end": "2323000"
  },
  {
    "text": "We're not going to do anything\nwith them, the only thing I want to do with them is to show\nyou that there are ways",
    "start": "2047220",
    "end": "2054700"
  },
  {
    "text": "of modeling real sources, and\ncoming as close to good models for real sources as\nyou want to come.",
    "start": "2054700",
    "end": "2062480"
  },
  {
    "text": "That's the whole\napproach here. How do you do coding\nfor Markov sources?",
    "start": "2062480",
    "end": "2068770"
  },
  {
    "text": "The simplest approach, which\ndoesn't work very well, is to use a separate prefix-free code\nfor each prior state.",
    "start": "2068770",
    "end": "2077590"
  },
  {
    "text": "Namely, if I look at this Markov\nchain that I had, it",
    "start": "2077590",
    "end": "2087225"
  },
  {
    "text": "says that when I'm in the state\nI want to somehow encode the next state that I go to, or\nthe next letter that comes",
    "start": "2087225",
    "end": "2093929"
  },
  {
    "text": "out of the Markov source. The things that can come out\nof the Markov source are either a 1 or a zero.",
    "start": "2093930",
    "end": "2101530"
  },
  {
    "text": "Now you see the whole problem\nwith this approach. As soon as you look at an\nexample, it sort of blows the",
    "start": "2101530",
    "end": "2107869"
  },
  {
    "text": "cover on this. What's the best prefix-free code\nto encode a 1 and a zero?",
    "start": "2107870",
    "end": "2115340"
  },
  {
    "text": "Where one appears with\nprobability 0.9 and the other one appears with probability\n0.1.",
    "start": "2115340",
    "end": "2120480"
  },
  {
    "text": "What's the Huffman encoder do? It assigns one of those symols\nto 1 and one of them to zero.",
    "start": "2120480",
    "end": "2127640"
  },
  {
    "text": "You might as well encode\n1 to this one, and zero to that one. Which means that all of the\ntheory, all it does is",
    "start": "2127640",
    "end": "2138260"
  },
  {
    "text": "generate the same symbols\nthat you had before. You're not doing any\ncompression at all.",
    "start": "2138260",
    "end": "2144040"
  },
  {
    "text": "It's a nice thing\nto think about.  In other words, by thinking\nabout these things you then",
    "start": "2144040",
    "end": "2150300"
  },
  {
    "text": "see the solution. And our solution before to these\nproblems was that if you",
    "start": "2150300",
    "end": "2155740"
  },
  {
    "text": "don't get anywhere by using a\nprefix-free code on a single digit, take a block of n digits\nand use a prefix-free",
    "start": "2155740",
    "end": "2163230"
  },
  {
    "text": "code there. So that's the approach\nwe will take here.",
    "start": "2163230",
    "end": "2168240"
  },
  {
    "text": "The general idea for single\nletters is this prefix-free",
    "start": "2168240",
    "end": "2173300"
  },
  {
    "text": "code we're going to generate\nsatisfies a Kraft inequality, you can use the Huffman\nalgorithm, you get this",
    "start": "2173300",
    "end": "2179980"
  },
  {
    "text": "property here. And this entropy, which is now\na function of the particular",
    "start": "2179980",
    "end": "2187940"
  },
  {
    "text": "state that we were in to\nstart with, is just this entropy here. So this is a conditional\nentropy, which we get a",
    "start": "2187940",
    "end": "2196540"
  },
  {
    "text": "different conditional\nentropy for each possible previous state. And that conditional entropy\nfor each possible previous",
    "start": "2196540",
    "end": "2203250"
  },
  {
    "text": "state, is what tells us exactly\nwhat we can do as far as generating a Huffman code\nfor that next state.",
    "start": "2203250",
    "end": "2210320"
  },
  {
    "text": "This would work fine if you had\na symbol alphabet of size 10,000 or something.",
    "start": "2210320",
    "end": "2215730"
  },
  {
    "text": "It just doesn't work\nwell when your symbol alphabet is binary. ",
    "start": "2215730",
    "end": "2229990"
  },
  {
    "text": "If we start out in a steady\nstate, then all of these probability stay in\nsteady state.",
    "start": "2229990",
    "end": "2236420"
  },
  {
    "text": "When we look at the number of\nbinary digits per source symbol and we average them\nover all of the initial",
    "start": "2236420",
    "end": "2244280"
  },
  {
    "text": "states, the initial states\noccur with these probabilities q of s. We have these best\nHuffman code.",
    "start": "2244280",
    "end": "2252040"
  },
  {
    "text": "So the number of binary digits\nwe're using per source symbol is really this average here.",
    "start": "2252040",
    "end": "2259920"
  },
  {
    "text": "Because this is averaging\nover all the states you're going to go into. And the entropy of the source\noutput, conditional on the",
    "start": "2259920",
    "end": "2272000"
  },
  {
    "text": "chance variable s, is now,\nin fact, defined just as that average.",
    "start": "2272000",
    "end": "2278280"
  },
  {
    "text": "So they encoder transmits s\nzero, followed by the code word for s 1 using s zero.",
    "start": "2278280",
    "end": "2285150"
  },
  {
    "text": "That specifies s 1, and\nthen you encode x 2, using s 1 and so forth.",
    "start": "2285150",
    "end": "2291480"
  },
  {
    "text": "And the decoder is sitting there\nand the decoder does exactly the same thing. Namely, the decoder first sees\nwhat s zero is, then it uses",
    "start": "2291480",
    "end": "2301220"
  },
  {
    "text": "the code for s zero to decide\nwhat x 1 was, then it uses the code for s 1, which is\ndetermined by s 1, s one is",
    "start": "2301220",
    "end": "2314050"
  },
  {
    "text": "determined by x 1, and it goes\non and on like that. ",
    "start": "2314050",
    "end": "2323180"
  },
  {
    "start": "2323000",
    "end": "2610000"
  },
  {
    "text": "Let me review a little bit about\nconditional entropy.",
    "start": "2323180",
    "end": "2328460"
  },
  {
    "text": "I'm going pretty fast here and\nI'm not deriving these things because they're in the notes.",
    "start": "2328460",
    "end": "2334810"
  },
  {
    "text": "And it's almost as if I want\nyou to get some kind of a pattern sensitivity to these\nthings, without the idea that",
    "start": "2334810",
    "end": "2343299"
  },
  {
    "text": "we're going to use them a lot. You ought to have a general idea\nof waht the results are. We're not going to spend a lot\nof time on this because, as I",
    "start": "2343300",
    "end": "2350860"
  },
  {
    "text": "said before, the only thing I\nwant you to recognize is that if you ever want to model a\nsource, this, in fact, gives",
    "start": "2350860",
    "end": "2357390"
  },
  {
    "text": "you a general way of doing it. So this general entropy then is\nusing what we had before.",
    "start": "2357390",
    "end": "2365120"
  },
  {
    "text": "It's the sum over all the\nstates, and the sum over all of the source outputs of these\nlog pmf probabilities.",
    "start": "2365120",
    "end": "2376880"
  },
  {
    "text": "This general entropy of both a\nsymbol and a state is equal to",
    "start": "2376880",
    "end": "2383049"
  },
  {
    "text": "this combined thing. Which is equal, not\nsurprisingly, to the entropy",
    "start": "2383050",
    "end": "2388730"
  },
  {
    "text": "of the state. Namely, first you want to know\nwhat the state is that has this entropy.",
    "start": "2388730",
    "end": "2395059"
  },
  {
    "text": "And then given the state, this\nis the entropy of the next letter, conditional\non the state.",
    "start": "2395060",
    "end": "2401060"
  },
  {
    "text": " Since this joint entropy is less\nthan or equal to H of s",
    "start": "2401060",
    "end": "2411140"
  },
  {
    "text": "plus H of x, I think you just\nproved that in the homework, didn't you? I hope so.",
    "start": "2411140",
    "end": "2417620"
  },
  {
    "text": "That says that the entropy of\nx conditional on s, is less",
    "start": "2417620",
    "end": "2422660"
  },
  {
    "text": "than or equal to the entropy of\nx, which is not surprising.",
    "start": "2422660",
    "end": "2428270"
  },
  {
    "text": "It says that if you use the\nprevious state in trying to do source encoding, you're going\nto do better than if",
    "start": "2428270",
    "end": "2434660"
  },
  {
    "text": "you don't use it.  I mean the whole theory\nwould be pretty",
    "start": "2434660",
    "end": "2440010"
  },
  {
    "text": "stupid if you didn't. ",
    "start": "2440010",
    "end": "2447010"
  },
  {
    "text": "That's what that says. As I told you before, the only\nway we can make all of this",
    "start": "2447010",
    "end": "2452380"
  },
  {
    "text": "work is to use\nn-to-variable-length codes for each state. In other words, you encode n\nletters at the same time.",
    "start": "2452380",
    "end": "2461390"
  },
  {
    "text": "If you look at the entropy of\nthe first n states given a",
    "start": "2461390",
    "end": "2467059"
  },
  {
    "text": "starting state, turns out to\nbe n times H of x given s.",
    "start": "2467060",
    "end": "2472350"
  },
  {
    "text": "By the same kind of rule that\nyou were using before. ",
    "start": "2472350",
    "end": "2487020"
  },
  {
    "text": "The same argument that you\nused to show that this is equal to that, you can use to\nshow that this is equal to H",
    "start": "2487020",
    "end": "2496460"
  },
  {
    "text": "of S 1, given S zero, plus H of\nS 2, given S 1, plus H of S",
    "start": "2496460",
    "end": "2501940"
  },
  {
    "text": "3, given S 2 and so forth. And by the stationarity that\nwe have here, these are all",
    "start": "2501940",
    "end": "2507070"
  },
  {
    "text": "equal, so you wind up\nwith n times times this conditional entropy.",
    "start": "2507070",
    "end": "2513260"
  },
  {
    "text": "And since the source outputs\nspecified the states, and the states specified the source\noutputs, you can then convince",
    "start": "2513260",
    "end": "2521220"
  },
  {
    "text": "yourself that this entropy is\nalso equal to n times H of X, given S. And once you do that,\nyou're back in the same",
    "start": "2521220",
    "end": "2529760"
  },
  {
    "text": "position we were in when we\nlooked at n-to-variable-length coding when we were looking at\ndiscrete memoryless sources.",
    "start": "2529760",
    "end": "2538220"
  },
  {
    "text": "Namely, the only thing that\nhappens when you're looking at n-to-variable length coding,\nis it that one fudge factor",
    "start": "2538220",
    "end": "2546680"
  },
  {
    "text": "becomes a 1 over n. When you have a small symbol\nspace by going two blocks, you",
    "start": "2546680",
    "end": "2553330"
  },
  {
    "text": "get rid of this, you can make\nthe expected length close to H of X, given S. Which means, in\nfact, that all of the memory",
    "start": "2553330",
    "end": "2562490"
  },
  {
    "text": "is taking into account and it\nstill is this one parameter, the entropy, that\nsays everything.",
    "start": "2562490",
    "end": "2570829"
  },
  {
    "text": "The AEP holds -- I mean if you want to, you can\nsit down and just see that it",
    "start": "2570830",
    "end": "2576530"
  },
  {
    "text": "holds, once you see what these\nentropies are, I mean the entropy you're using log pmf's,\nyou're just looking at",
    "start": "2576530",
    "end": "2586109"
  },
  {
    "text": "products of probabilities, which\nare sums of log pmf's, and everything is the\nsame as before.",
    "start": "2586110",
    "end": "2593170"
  },
  {
    "text": "And again, if you're using\nn-to-variable-length codes, you just can't achieve an\nexpected length less than H of",
    "start": "2593170",
    "end": "2602010"
  },
  {
    "text": "X, given S. So H of X, given\nS gives the whole story. ",
    "start": "2602010",
    "end": "2611140"
  },
  {
    "start": "2610000",
    "end": "3130000"
  },
  {
    "text": "You should read those notes,\nbecause I've gone through that very, very fast, partly because\nsome of you are",
    "start": "2611140",
    "end": "2618160"
  },
  {
    "text": "already very familiar with\nMarkov chains some of you are probably less familiar with it,\nso you should check that",
    "start": "2618160",
    "end": "2624490"
  },
  {
    "text": "out a little bit on your own. I want to talk about the\nLempel Ziv universal",
    "start": "2624490",
    "end": "2630980"
  },
  {
    "text": "algorithm, which was rather\nsurprising to many people.",
    "start": "2630980",
    "end": "2636270"
  },
  {
    "text": " Jacob Ziv is one of the\ngreat theorists",
    "start": "2636270",
    "end": "2643119"
  },
  {
    "text": "of information theory. Before this time, he wrote\na lot of very,",
    "start": "2643120",
    "end": "2648300"
  },
  {
    "text": "very powerful papers. Which were quite hard to\nread in many cases.",
    "start": "2648300",
    "end": "2654500"
  },
  {
    "text": "So it was a real surprise to\npeople when he came up with this beautiful idea, which was\na lovely, simple algorithm.",
    "start": "2654500",
    "end": "2661750"
  },
  {
    "text": "Some people, because of that,\nthought it was Abe Lempel, who spends part of his year at\nBrandeis, who was really the",
    "start": "2661750",
    "end": "2668760"
  },
  {
    "text": "genius behind it. In fact, it wasn't Abe Lempel,\nit was Jacob Ziv who was the",
    "start": "2668760",
    "end": "2674160"
  },
  {
    "text": "genius behind it. Abe Lempel was pretty much the\none who really implemented it,",
    "start": "2674160",
    "end": "2682390"
  },
  {
    "text": "because once you see what the\nalgorithm is, it's still not trivial to try to find how\nto implement it in a",
    "start": "2682390",
    "end": "2689650"
  },
  {
    "text": "simple, easy way. If you look at all the articles\nabout it, the authors",
    "start": "2689650",
    "end": "2696350"
  },
  {
    "text": "are Ziv and Lempel, instead of\nLempel and Ziv, so why it got called Lempel Ziv is a\nmystery to everyone.",
    "start": "2696350",
    "end": "2704020"
  },
  {
    "text": "Anyway, they came up with\ntwo algorithms. One which they came up with in\n1977, and people looked at",
    "start": "2704020",
    "end": "2712770"
  },
  {
    "text": "their 1977 algorithm and said,\noh that's much too complicated to implement.",
    "start": "2712770",
    "end": "2718210"
  },
  {
    "text": "So they went back to the drawing\nboard, came up with another one in 1978, which\npeople said, ah, we can",
    "start": "2718210",
    "end": "2724890"
  },
  {
    "text": "implement that. So people started implementing\nthe LZ78 and of course, by",
    "start": "2724890",
    "end": "2731750"
  },
  {
    "text": "that time, all the technology\nwas much better. You could do things faster and\ncheaper then you could before,",
    "start": "2731750",
    "end": "2739380"
  },
  {
    "text": "and what happened then is that\na few years after that people",
    "start": "2739380",
    "end": "2744920"
  },
  {
    "text": "were implementing LZ77,\nwhich turned out to work much better.",
    "start": "2744920",
    "end": "2750190"
  },
  {
    "text": "Which is often the way\nthis field works. People do something interesting\ntheoretically, people say, no you can't do\nit, so they simplify it,",
    "start": "2750190",
    "end": "2759580"
  },
  {
    "text": "thereby destroying some of\nits best characteristics. And then a few years later\npeople are doing the more",
    "start": "2759580",
    "end": "2766210"
  },
  {
    "text": "sophisticated thing, which they\nshould have started out doing at the beginning. ",
    "start": "2766210",
    "end": "2773730"
  },
  {
    "text": "What is a Universal Data\nCompression algorithm? A Universal Data Compression\nalgorithm, is an algorithm",
    "start": "2773730",
    "end": "2781330"
  },
  {
    "text": "which doesn't have any\nprobabilities tucked into it. In other words, the algorithm\nitself simply looks at a",
    "start": "2781330",
    "end": "2788690"
  },
  {
    "text": "sequence of letters from\nan alphabet, and encodes it in some way.",
    "start": "2788690",
    "end": "2794060"
  },
  {
    "text": "And what you would like to be\nable to do is somehow measure what the statistics are, and\nat the same time as you're",
    "start": "2794060",
    "end": "2802400"
  },
  {
    "text": "measuring the statistics, you\nwant to encode the digits. You don't care too\nmuch about delay.",
    "start": "2802400",
    "end": "2809630"
  },
  {
    "text": "In fact, one way to do this -- I mean, if you're surprised\nthat you can build a good universal encoder,\nyou shouldn't be.",
    "start": "2809630",
    "end": "2816880"
  },
  {
    "text": "Because you could just take the\nfirst million letters out of the source, go through all\nthe statistical analysis you",
    "start": "2816880",
    "end": "2823510"
  },
  {
    "text": "want to, model the source in\nwhatever way makes the best",
    "start": "2823510",
    "end": "2828560"
  },
  {
    "text": "sense to you, and then build a\nHuffman encoder, which, in fact, encodes things according\nto that model that you have.",
    "start": "2828560",
    "end": "2836780"
  },
  {
    "text": "Of course you then have to send\nthe decoder the first million digits, and the decoder\ngoes through the same",
    "start": "2836780",
    "end": "2843560"
  },
  {
    "text": "statistical analysis, and\ntherefore finds out what code you're going to use, and then\nthe encoder encodes, the",
    "start": "2843560",
    "end": "2850410"
  },
  {
    "text": "decoder decodes and you have\nthis million symbols of overhead in the algorithm, that\nif you use the algorithm",
    "start": "2850410",
    "end": "2858460"
  },
  {
    "text": "for a billion letters instead of\na million letters, then it all works pretty well. So there's a little bit of\nthat flavor here, but the",
    "start": "2858460",
    "end": "2866609"
  },
  {
    "text": "other part of it is, it's\na neat algorithm. And the algorithm measures\nthings in a faster way then",
    "start": "2866610",
    "end": "2873550"
  },
  {
    "text": "you would believe. And as you look at it later\nyou say, gee, this makes a",
    "start": "2873550",
    "end": "2879080"
  },
  {
    "text": "great deal of sense even if\nthere isn't much statistical structure here.",
    "start": "2879080",
    "end": "2884690"
  },
  {
    "text": "In other words, you can show\nthat if the source really is a",
    "start": "2884690",
    "end": "2889859"
  },
  {
    "text": "Markov source, then this\nalgorithm will behave just as",
    "start": "2889860",
    "end": "2895220"
  },
  {
    "text": "well, asymptotically, as the\nbest algorithm you can design",
    "start": "2895220",
    "end": "2900740"
  },
  {
    "text": "for that Markov source. Namely, it's so good that it\nwill in fact measure the",
    "start": "2900740",
    "end": "2908450"
  },
  {
    "text": "statistics in that Markov model\nand implement them.",
    "start": "2908450",
    "end": "2913970"
  },
  {
    "text": "But it does something\nbetter than that. And the thing which is better\nis that, if you're going to",
    "start": "2913970",
    "end": "2920280"
  },
  {
    "text": "look at this first million\nsymbols and your objective then is to build a Markov model,\nafter you build the",
    "start": "2920280",
    "end": "2928920"
  },
  {
    "text": "Markov model for that million\nsymbols, one of the things that you always question about\nis, should I have used a",
    "start": "2928920",
    "end": "2934890"
  },
  {
    "text": "Markov model or should I have\nused some other kind of model? And that's a difficult\nquestion to ask.",
    "start": "2934890",
    "end": "2941330"
  },
  {
    "text": "You go through all of the\ndifferent possibilities, and one of the nice things about the\nLempel Ziv algorithm is,",
    "start": "2941330",
    "end": "2947460"
  },
  {
    "text": "in a sense, it just does\nthis automatically. If there's some kind of\nstatistical structure there,",
    "start": "2947460",
    "end": "2953620"
  },
  {
    "text": "it's going to find it. If it's not Markov, if it's some\nother kind of structure,",
    "start": "2953620",
    "end": "2959040"
  },
  {
    "text": "it will find it. The question is how does it\nfind this statistical structure without knowing what\nkind of model you should use",
    "start": "2959040",
    "end": "2967800"
  },
  {
    "text": "to start with? And that's the genius of things\nwhich are universal, because they don't assume that\nyou have to measure particular",
    "start": "2967800",
    "end": "2975430"
  },
  {
    "text": "things in some model that\nyou believe in. It just does the whole\nthing all at once.",
    "start": "2975430",
    "end": "2981090"
  },
  {
    "text": "If it's running along and the\nstatistics change, bing, it changes, too.",
    "start": "2981090",
    "end": "2988070"
  },
  {
    "text": "And suddenly it will start\nproducing more binary digits per source symbol, or fewer,\nbecause that's",
    "start": "2988070",
    "end": "2994590"
  },
  {
    "text": "what it has to do. And that's just the\nway it works. But it does have all these\nnice properties.",
    "start": "2994590",
    "end": "3002510"
  },
  {
    "text": "It has instantaneous\ndecodability. In a sense, it is a prefix-free\ncode, although you",
    "start": "3002510",
    "end": "3009770"
  },
  {
    "text": "have to interpret\npretty carefully what you mean by that. We'll understand that\nin a little bit.",
    "start": "3009770",
    "end": "3016010"
  },
  {
    "text": "But in fact, it does do all\nof these neat things. And there are better algorithms\nout there now,",
    "start": "3016010",
    "end": "3025420"
  },
  {
    "text": "whether they're better in terms\nof the trade-off between complexity and compressability,\nI don't know.",
    "start": "3025420",
    "end": "3034690"
  },
  {
    "text": "But anyway, the people who do\nresearch on these things have to have something to\nkeep them busy.",
    "start": "3034690",
    "end": "3040670"
  },
  {
    "text": "And they have to have some kind\nof results to get money for, and therefore they claim\nthat the new algorithms are",
    "start": "3040670",
    "end": "3048160"
  },
  {
    "text": "better than the old\nalgorithms. And they probably are,\nbut I'm not sure.",
    "start": "3048160",
    "end": "3053620"
  },
  {
    "text": "Anyway, this is a very\ncute algorithm. ",
    "start": "3053620",
    "end": "3060539"
  },
  {
    "text": "So what you're trying to do\nhere, the objective, one objective which is achieved, is\nif you observe the output",
    "start": "3060540",
    "end": "3069500"
  },
  {
    "text": "from the given probability\nmodel, say a Markov source, and I build the best code I can\nfor that Markov source,",
    "start": "3069500",
    "end": "3077810"
  },
  {
    "text": "then we know how many bits\nwe need per symbol. Number of bits we need per\nsymbol is this entropy of a",
    "start": "3077810",
    "end": "3083800"
  },
  {
    "text": "letter of a symbol\ngiven the state. That's the best we can do.",
    "start": "3083800",
    "end": "3089750"
  },
  {
    "text": "How well does the Lempel\nZiv algorithm do? Asymptotically, when you make\neverything large it will",
    "start": "3089750",
    "end": "3097289"
  },
  {
    "text": "encode using a number of bits\nper symbol, which is H of X, given S. So it'll do just as\nwell as the best thing does",
    "start": "3097290",
    "end": "3105769"
  },
  {
    "text": "which happens to know the\nmodel to start with. As I said before the algorithm\nalso compresses in the absence",
    "start": "3105770",
    "end": "3113520"
  },
  {
    "text": "of any ordinary kind of\nstatistical structure. Whatever kind of structure\nis there, this",
    "start": "3113520",
    "end": "3118860"
  },
  {
    "text": "algorithm sorts it out. It should deal with gradually\nchanging statistics.",
    "start": "3118860",
    "end": "3124079"
  },
  {
    "text": "It does that also, but perhaps\nnot in the best way. We'll talk about that later. ",
    "start": "3124080",
    "end": "3131730"
  },
  {
    "text": "Let's describe it\na little bit. ",
    "start": "3131730",
    "end": "3139349"
  },
  {
    "text": "If we let x 1, x 2 blah blah\nblah, be the output of the source, and the alphabet is some\nalphabet capital X, which",
    "start": "3139350",
    "end": "3149610"
  },
  {
    "text": "has size m, let's just as\nnotation, let x sub m super n",
    "start": "3149610",
    "end": "3161720"
  },
  {
    "text": "denote the string xm,\nxm plus 1, up to xn.",
    "start": "3161720",
    "end": "3166780"
  },
  {
    "text": "In other words, in describing\nthis algorithm we are, all the time talking about strings of\nletters taken out of this",
    "start": "3166780",
    "end": "3174309"
  },
  {
    "text": "infinite length string that\ncomes out of the source. We want to have a nice notation\nfor talking about a",
    "start": "3174310",
    "end": "3180800"
  },
  {
    "text": "sub string of the\nactual sequence. We're going to use a window\nin this algorithm.",
    "start": "3180800",
    "end": "3188300"
  },
  {
    "text": "We want the window to have a\nsize with the power of 2. Typical values for the window\nrange from about a thousand up",
    "start": "3188300",
    "end": "3197290"
  },
  {
    "text": "to about a million. Maybe they're even bigger\nnow, I don't know.",
    "start": "3197290",
    "end": "3203830"
  },
  {
    "text": "But as we'll see later, there's some constraints there. What the Lempel Ziv algorithm\ndoes, this LZ77 algorithm, is",
    "start": "3203830",
    "end": "3213950"
  },
  {
    "text": "it matches the longest string\nof yet unencoded -- this is unencoded also, isn't it,\nthat's simple enough --",
    "start": "3213950",
    "end": "3227310"
  },
  {
    "text": "of yet unencoded symbols\nby using strings starting in the window.",
    "start": "3227310",
    "end": "3232370"
  },
  {
    "text": "So it takes this sequence of\nstuff we haven't observed yet, it tries to find the longest\nstring starting there which it",
    "start": "3232370",
    "end": "3240750"
  },
  {
    "text": "can match with something that's\nalready in the window. If it can find something which\nmatches with something in the",
    "start": "3240750",
    "end": "3247580"
  },
  {
    "text": "window, what does it do? It's going to first say how long\nthe match was, and then",
    "start": "3247580",
    "end": "3253940"
  },
  {
    "text": "it's going to say where in\nthe window it found it. And the decoder is sitting\nthere, the decoder has this",
    "start": "3253940",
    "end": "3259830"
  },
  {
    "text": "window which it observes also,\nso the decoder can find the same match which is\nin the window.",
    "start": "3259830",
    "end": "3267200"
  },
  {
    "text": "Why does it work? Well, it works because with all\nof these AEP properties",
    "start": "3267200",
    "end": "3273740"
  },
  {
    "text": "that we're thinking of, you tend\nto have typical sequences sitting there in the window.",
    "start": "3273740",
    "end": "3279790"
  },
  {
    "text": "And you tend to have typical\nsequences which come out of the source. So the thing we're trying to\nencode is some typical",
    "start": "3279790",
    "end": "3286300"
  },
  {
    "text": "sequence -- well you can you can think of\nshort typical sequences and",
    "start": "3286300",
    "end": "3291470"
  },
  {
    "text": "longer typical sequences. We try to find the longest\ntypical sequence that we can.",
    "start": "3291470",
    "end": "3296980"
  },
  {
    "text": "And we're looking back into\nthis window, and there are enormous number of typical\nsequences there.",
    "start": "3296980",
    "end": "3302290"
  },
  {
    "text": "If we make the typical sequences\nshort enough, there aren't too many of them, and\nmost of them are sitting there",
    "start": "3302290",
    "end": "3307430"
  },
  {
    "text": "in the window. This'll become clearer\nas we go.",
    "start": "3307430",
    "end": "3313150"
  },
  {
    "text": "Let's go on and actually\nexplain what the algorithm does. ",
    "start": "3313150",
    "end": "3320130"
  },
  {
    "text": "So here's the algorithm. First, you take this large W,\nthis large window size, and",
    "start": "3320130",
    "end": "3329849"
  },
  {
    "text": "we're going to encode the\nfirst thought W symbols. We're not even going to use any\ncompression, that's just",
    "start": "3329850",
    "end": "3335500"
  },
  {
    "text": "lost stuff. So we encode this first million\nsymbols, we grin and",
    "start": "3335500",
    "end": "3341820"
  },
  {
    "text": "bear it, and then the decoder\nhas this window of a million symbols.",
    "start": "3341820",
    "end": "3347170"
  },
  {
    "text": "We at the encoder have this\nwindow of a million symbols, and we proceed from there.",
    "start": "3347170",
    "end": "3353350"
  },
  {
    "text": "So it gets amortized,\nso we don't care. So we then have a pointer, and\nwe set the pointer to W.",
    "start": "3353350",
    "end": "3361349"
  },
  {
    "text": "So the pointer is the last\nthing that we encoded. So we have all this encoded\nstuff starting at time P,",
    "start": "3361350",
    "end": "3370190"
  },
  {
    "text": "everything beyond there\nis as yet unencoded. That's the first step\nin the algorithm.",
    "start": "3370190",
    "end": "3377200"
  },
  {
    "text": "So far, so good. ",
    "start": "3377200",
    "end": "3382740"
  },
  {
    "text": "The next step is to find the\nlargest n, greater than or equal to 2, I'll explain why\ngreater than or equal to 2",
    "start": "3382740",
    "end": "3389780"
  },
  {
    "text": "later, such that the string x\nsub p plus 1 up to p plus n,",
    "start": "3389780",
    "end": "3398160"
  },
  {
    "text": "what is that? It's the string which starts\nright beyond the pointer,",
    "start": "3398160",
    "end": "3403819"
  },
  {
    "text": "namely the string that starts\nhere, what we're trying to do is find the largest n, in other\nwords the longest string",
    "start": "3403820",
    "end": "3410559"
  },
  {
    "text": "starting here, which we can\nmatch with something that's in the window. Now we look at a, a is in the\nwindow, we look at a b, a b as",
    "start": "3410560",
    "end": "3420430"
  },
  {
    "text": "in the window. We look at a b a, a b a as in\nthe window. a b a b, a b a b",
    "start": "3420430",
    "end": "3430900"
  },
  {
    "text": "is not in the window. At least I hope it's not in the\nwindow or I screwed up.",
    "start": "3430900",
    "end": "3435960"
  },
  {
    "text": "Yeah, it's not in the window. So the longest thing we can find\nwhich matches with what's in the window is this match\nof length three.",
    "start": "3435960",
    "end": "3443430"
  },
  {
    "text": " So this is finding the longest\nmatch which matches with",
    "start": "3443430",
    "end": "3449420"
  },
  {
    "text": "something here. This next example, I think\nthe only way I can regard",
    "start": "3449420",
    "end": "3456900"
  },
  {
    "text": "that is as a hack. It's a kind of hack that\nprogrammers like.",
    "start": "3456900",
    "end": "3462410"
  },
  {
    "text": "It's very mysterious, but it's\nalso the kind of hack that mathematicians like because in\nthis case this particular hack",
    "start": "3462410",
    "end": "3469840"
  },
  {
    "text": "makes the analysis\nmuch easier. So this is another\nkind of match.",
    "start": "3469840",
    "end": "3475420"
  },
  {
    "text": "It's looking for the longest\nstring here, starting at this pointer.",
    "start": "3475420",
    "end": "3480510"
  },
  {
    "text": "a b a b so forth, which matches\nthings starting here. Starting somewhere\nin the window.",
    "start": "3480510",
    "end": "3487150"
  },
  {
    "text": "So it finds a match a\nb here. a b a here. But now it looks for a b\na b, a match of four.",
    "start": "3487150",
    "end": "3497050"
  },
  {
    "text": "Where do we find it? We can start back here, which\nis still in the window, and what we see is a b a b.",
    "start": "3497050",
    "end": "3505190"
  },
  {
    "text": "So these four digits match\nthese four digits. Well you might say foul ball,\nbecause if I tell you there's",
    "start": "3505190",
    "end": "3514410"
  },
  {
    "text": "a match of four and I tell you\nwhere it is, in fact, all the",
    "start": "3514410",
    "end": "3520740"
  },
  {
    "text": "poor decoder knows is this. If I tell you there's a match\nof four and it starts here,",
    "start": "3520740",
    "end": "3528180"
  },
  {
    "text": "what's the poor decoder\ngoing to do? The poor decoder says, ok, so\na is that digit two digits",
    "start": "3528180",
    "end": "3536950"
  },
  {
    "text": "ago, so that gives me the a. So I know there's an a\nthere. b is the next",
    "start": "3536950",
    "end": "3542000"
  },
  {
    "text": "digit, so b is there. And then I know the first two\ndigits beyond the window, and",
    "start": "3542000",
    "end": "3548420"
  },
  {
    "text": "therefore this third digit is a,\nso that must be that digit. The fourth digit is\nthis digit, which",
    "start": "3548420",
    "end": "3555339"
  },
  {
    "text": "must be that digit. OK? If you didn't catch that, you\ncan just think about it, it'll",
    "start": "3555340",
    "end": "3561220"
  },
  {
    "text": "become clear. I mean it really is a hack. It's not very important. It won't change the way\nthis thing bahaves.",
    "start": "3561220",
    "end": "3568000"
  },
  {
    "text": "But it does change the\nway you analyze it",
    "start": "3568000",
    "end": "3573470"
  },
  {
    "text": "So that's the first\nthing you do, you look for these matches. Next thing we're going to do\nis we're going to try to",
    "start": "3573470",
    "end": "3580230"
  },
  {
    "text": "encode the matches. Namely, we're going to try to\nencode the thing the we found",
    "start": "3580230",
    "end": "3586640"
  },
  {
    "text": "in the window. How do we encode what we\nfound in the window? Well the first thing we\nhave to do -- yeah?",
    "start": "3586640",
    "end": "3592480"
  },
  {
    "text": "AUDIENCE: What if you don't\nfind any matches? PROFESSOR: I'm going to\ntalk about that later. If you don't find any matches,\nI mean what I was looking for",
    "start": "3592480",
    "end": "3600030"
  },
  {
    "text": "was matches of two or more. If you don't find any matches of\ntwo or more, what you do is",
    "start": "3600030",
    "end": "3606440"
  },
  {
    "text": "you just take the first letter\nin the window and you encode that without any compression.",
    "start": "3606440",
    "end": "3612390"
  },
  {
    "text": " I mean our strategy here is to\nalways send the length of the",
    "start": "3612390",
    "end": "3621180"
  },
  {
    "text": "match first. If you say the length of the\nmatch is one, than the decoder knows to look for uncompressed\nsymbols, instead of looking",
    "start": "3621180",
    "end": "3628850"
  },
  {
    "text": "for something in the window. So it takes care of the case\nwhere there haven't been any",
    "start": "3628850",
    "end": "3633880"
  },
  {
    "text": "occurrences of symbol anywhere\nin the window. So you only look for matches\nof length two or more.",
    "start": "3633880",
    "end": "3641030"
  },
  {
    "text": " So then you use something called\na unary-binary code.",
    "start": "3641030",
    "end": "3650349"
  },
  {
    "text": "Theoriticians always copy\neverybody else's work. The unary-binary code was due\nto Peter Elias, who was the",
    "start": "3650350",
    "end": "3661030"
  },
  {
    "text": "head of this department\nfor a long time. He just died about\nsix months ago.",
    "start": "3661030",
    "end": "3667609"
  },
  {
    "text": "He was here up until\nhis death. He used to organize department\ncolloquia.",
    "start": "3667610",
    "end": "3674290"
  },
  {
    "text": "He was so essential that since\nhe died, nobody's taken over the department colloquia.",
    "start": "3674290",
    "end": "3679430"
  },
  {
    "text": "He was my thesis adviser, so I\ntend to think very kindly of him And he was lots\nof other things.",
    "start": "3679430",
    "end": "3686350"
  },
  {
    "text": "But anyway, he invented this\nunary-binary code, which is a way of encoding the integers,\nwhich has a lot of nice",
    "start": "3686350",
    "end": "3693980"
  },
  {
    "text": "properties. And they're universal\nproperties, as you will see.",
    "start": "3693980",
    "end": "3699230"
  },
  {
    "text": "The idea is to encode the\nintegers, there are infinite number of integers.",
    "start": "3699230",
    "end": "3704549"
  },
  {
    "text": "What you'd like to do, somehow\nor other, is have shorter code words for lower integers,\nand longer code",
    "start": "3704550",
    "end": "3711530"
  },
  {
    "text": "words for longer integers. In this particular Lempel Ziv\nalgorithm, it's particularly",
    "start": "3711530",
    "end": "3718140"
  },
  {
    "text": "important to have the lenght of\nthe code words growing as the logarithm of n.",
    "start": "3718140",
    "end": "3724790"
  },
  {
    "text": "Because then anytime you find\na really long match, and you got a very large n, you're\nencoding a whole lot of",
    "start": "3724790",
    "end": "3731560"
  },
  {
    "text": "letters, and therefore you\ndon't care if there's an overhead which is proportional\nto log n.",
    "start": "3731560",
    "end": "3739080"
  },
  {
    "text": "So you don't mind that. And if there's a very small\nnumber of letters encoded, you want something very\nefficient then.",
    "start": "3739080",
    "end": "3747280"
  },
  {
    "text": "So it does that. And the way it does it is, first\nyou generate a prefix,",
    "start": "3747280",
    "end": "3753950"
  },
  {
    "text": "and then you have a\nrepresentation in base 2. Namely base 2 expansion.",
    "start": "3753950",
    "end": "3760450"
  },
  {
    "text": "So the number n, the prefix\nhere, I think I said it here,",
    "start": "3760450",
    "end": "3766750"
  },
  {
    "text": "the positive integer n is\nencoded into the binary representation of n, preceded by\na prefix of integer part of",
    "start": "3766750",
    "end": "3776070"
  },
  {
    "text": "log to the base 2 of n zero. Now what's the integer part\nof log to the base 2 of 1?",
    "start": "3776070",
    "end": "3783930"
  },
  {
    "text": "Log to the base 2\nof 1 is zero. It was a prefix of zero zeros.",
    "start": "3783930",
    "end": "3790320"
  },
  {
    "text": "So zero zeros is nothing. So the prefix is nothing, the\nexpansion of 1, in a base 2",
    "start": "3790320",
    "end": "3797890"
  },
  {
    "text": "expansion or any other\nexpansion, is 1. So the code word for 1 is 1.",
    "start": "3797890",
    "end": "3805830"
  },
  {
    "text": "If you have the number 2, log\nto the base 2 of 2 is 1.",
    "start": "3805830",
    "end": "3811820"
  },
  {
    "text": "The integer part of 1 is 1, so\nyou start out with a single zero and then you have\nthe expansion, 2 is",
    "start": "3811820",
    "end": "3818550"
  },
  {
    "text": "expanded as 1 zero. And so forth. Oh and then 3 is expanded as 1\n1, again with a prefix of 1.",
    "start": "3818550",
    "end": "3826700"
  },
  {
    "text": "Four is encoded as\n1 zero zero, blah blah blah and so forth.",
    "start": "3826700",
    "end": "3832599"
  },
  {
    "text": "Why don't you just leave\nthe prefix out? ",
    "start": "3832600",
    "end": "3840090"
  },
  {
    "text": "Anybody figure out why I\nneed the prefix there? AUDIENCE: If without those\nprefixes, you don't a",
    "start": "3840090",
    "end": "3847250"
  },
  {
    "text": "prefix-free code. PROFESSOR: Yeah Right.",
    "start": "3847250",
    "end": "3852430"
  },
  {
    "text": "If I left them out, everything\nwould start with 1. I would get to the 1, I would\nsay, gee, is that the end of",
    "start": "3852430",
    "end": "3858680"
  },
  {
    "text": "it or isn't it the end of it? I wouldn't know. But with this, if I see 1, the\nonly code word that starts",
    "start": "3858680",
    "end": "3866840"
  },
  {
    "text": "with 1 is this one. If it's 2 or 3, it starts with\nzero and then there's a 1,",
    "start": "3866840",
    "end": "3874780"
  },
  {
    "text": "which says it's on that next\nbranch which has probably 1/4. I have a 1 0, 1 1, a prefix of\n0 0, followed by a 1, put me",
    "start": "3874780",
    "end": "3885380"
  },
  {
    "text": "off on another branch. And so forth. So, yes, this is a\nprefix-free code.",
    "start": "3885380",
    "end": "3891809"
  },
  {
    "text": "And it's a prefix-free code\nwhich has this nice property that the number of digits in the\ncode word is approximately",
    "start": "3891810",
    "end": "3902890"
  },
  {
    "text": "2 times log n. Namely, it goes up\nboth ways here.",
    "start": "3902890",
    "end": "3908940"
  },
  {
    "text": "The number of zeros I need is\nlog to the base 2 of n. The number of digits in the base\n2 expansion, is also the",
    "start": "3908940",
    "end": "3915910"
  },
  {
    "text": "integer part of log to\nthe base 2 of n. So it works both ways.",
    "start": "3915910",
    "end": "3921820"
  },
  {
    "text": "And there's always this\n1 in the middle.",
    "start": "3921820",
    "end": "3926920"
  },
  {
    "text": "Again, it's a hack. It's a hack that works\nvery nicely when you try to analyze this.",
    "start": "3926920",
    "end": "3932349"
  },
  {
    "text": " OK so if the size of the match\nis bigger than one, we're",
    "start": "3932350",
    "end": "3941480"
  },
  {
    "text": "going to encode the positive\ninteger u. u was where the match\noccurred.",
    "start": "3941480",
    "end": "3947750"
  },
  {
    "text": "How far back do you have\nto count before you find this match? You're going to encode that\ninteger u into a fixed length",
    "start": "3947750",
    "end": "3955020"
  },
  {
    "text": "code of length log of w bits.",
    "start": "3955020",
    "end": "3960530"
  },
  {
    "text": "In other words, you\nhave a window of size 2 to the twentieth.",
    "start": "3960530",
    "end": "3965780"
  },
  {
    "text": "You can encode any point in\nthere with 20 binary digits.",
    "start": "3965780",
    "end": "3970860"
  },
  {
    "text": "The 20 binary digits say how far\ndo you have to go back to find this code word.",
    "start": "3970860",
    "end": "3976060"
  },
  {
    "text": " So first we're encoding n, by\nthis unary-binary code, then",
    "start": "3976060",
    "end": "3983849"
  },
  {
    "text": "we're encoding w just with\nthis simple minded way of encoding log w bits.",
    "start": "3983850",
    "end": "3989780"
  },
  {
    "text": "And that tells us where\nthe match is. The decoder goes back, there\nfinds the match, pumps it out",
    "start": "3989780",
    "end": "4000110"
  },
  {
    "text": "if n is equal to 1, here's the\nanswer to your question, you encode the single letter\nwithout compression.",
    "start": "4000110",
    "end": "4005840"
  },
  {
    "text": "And that takes care of the case,\neither where you have a match to that single letter,\nor there isn't any match to",
    "start": "4005840",
    "end": "4012890"
  },
  {
    "text": "the single letter. The next thing, as you might\nimagine, is you set the",
    "start": "4012890",
    "end": "4018140"
  },
  {
    "text": "pointer to P plus n, because\nyou've encoded n digits, and",
    "start": "4018140",
    "end": "4023569"
  },
  {
    "text": "you go to step two. Namely, you keep iterating\nforever. Until the source wears out, or\nuntil the encoder wears out,",
    "start": "4023570",
    "end": "4031420"
  },
  {
    "text": "or until the decoder\nwears out. You just keep going. That's all the algorithm is.",
    "start": "4031420",
    "end": "4037790"
  },
  {
    "text": "Yeah? AUDIENCE: Can you throw out\nthe first n bits, when you",
    "start": "4037790",
    "end": "4043692"
  },
  {
    "text": "reset the pointer, because you\nonly have w bits that say where n was for the\nnext iteration?",
    "start": "4043692",
    "end": "4049102"
  },
  {
    "text": " PROFESSOR: No, I throw out the\nn oldest bits in the window.",
    "start": "4049102",
    "end": "4055730"
  },
  {
    "text": "AUDIENCE: Well, those are\nthe first n bits. PROFESSOR: Yes the first n bits\nout of the window and I",
    "start": "4055730",
    "end": "4060790"
  },
  {
    "text": "keep all of the more\nrecent bits. I tend to think of the first\nones as the things closest to",
    "start": "4060790",
    "end": "4067530"
  },
  {
    "text": "the pointer, but you think of it\neither way, which is fine.",
    "start": "4067530",
    "end": "4073600"
  },
  {
    "text": "So as you do it, the window\nkeeps sliding along. ",
    "start": "4073600",
    "end": "4080790"
  },
  {
    "text": "That's what it does. ",
    "start": "4080790",
    "end": "4087650"
  },
  {
    "text": "Why do you think this works?\n/ There's a half analysis in the notes.",
    "start": "4087650",
    "end": "4093520"
  },
  {
    "text": " i'd like to say a little bit\nabout how that analysis is",
    "start": "4093520",
    "end": "4100529"
  },
  {
    "text": "cheating, because it's not\nquite a fair analysis. If you look at the window, there\nare w different starting",
    "start": "4100530",
    "end": "4109460"
  },
  {
    "text": "points in the window. So let's write this down.",
    "start": "4109460",
    "end": "4114630"
  },
  {
    "text": " w starting points.",
    "start": "4114630",
    "end": "4120779"
  },
  {
    "start": "4120780",
    "end": "4128589"
  },
  {
    "text": "So for any given n, there\nare there are w",
    "start": "4128590",
    "end": "4136859"
  },
  {
    "text": "springs of length n.",
    "start": "4136860",
    "end": "4142049"
  },
  {
    "start": "4142050",
    "end": "4147179"
  },
  {
    "text": "We don't know how long this\nmatch is going to be, but what I would like to do, if I'm\nthinking of a Markov source,",
    "start": "4147180",
    "end": "4154849"
  },
  {
    "text": "is to say, OK, let's make n\nlarge enough so that the size",
    "start": "4154850",
    "end": "4160810"
  },
  {
    "text": "of the typical set is about w. ok So choose n to be about w\ndivided by H of X given S. And",
    "start": "4160810",
    "end": "4185910"
  },
  {
    "text": "the size of the typical set is\nthen going to be 2 to the n, wait a minute.",
    "start": "4185910",
    "end": "4192659"
  },
  {
    "start": "4192660",
    "end": "4200400"
  },
  {
    "text": "n is equal to log w. ",
    "start": "4200400",
    "end": "4212000"
  },
  {
    "text": "So the size of the typical set\nthen is T sub epsilon, is going to be roughly from what we\nsaid 2 to the n times H of",
    "start": "4212000",
    "end": "4223239"
  },
  {
    "text": "X given S. So what I'm going to\ndo is to set w equal to T",
    "start": "4223240",
    "end": "4232550"
  },
  {
    "text": "of epsilon. ",
    "start": "4232550",
    "end": "4239090"
  },
  {
    "text": "I'm going to focus on a match\nlength which I'm hoping to",
    "start": "4239090",
    "end": "4245080"
  },
  {
    "text": "achieve, of log w over H of X\ngiven S. The typical set,",
    "start": "4245080",
    "end": "4250470"
  },
  {
    "text": "then, is a size 2 to the n times\nH of x give S. And if",
    "start": "4250470",
    "end": "4255675"
  },
  {
    "text": "the typical set is of this size,\nand I look at these w strings in the window, yeah,\nI'm going to have some",
    "start": "4255675",
    "end": "4264030"
  },
  {
    "text": "duplicates but roughly I'm going\nto have a large enough number of things in the window\nto represent all of these",
    "start": "4264030",
    "end": "4272780"
  },
  {
    "text": "typical strings. Or most of them. If I try to choose an n which is\na little bigger than that,",
    "start": "4272780",
    "end": "4279680"
  },
  {
    "text": "let's call this n star. If I try to make n a little bit\nbigger than this typical",
    "start": "4279680",
    "end": "4286190"
  },
  {
    "text": "match size, I don't have a\nprayer of a chance, because the typical set then is just\nvery much larger than w, so",
    "start": "4286190",
    "end": "4294340"
  },
  {
    "text": "I'd be very, very lucky if I\nfound anything in the window. So that can't work.",
    "start": "4294340",
    "end": "4300310"
  },
  {
    "text": "If I make n a good deal smaller,\nthen I'm going to succeed with great probability\nit seems, because I'm even",
    "start": "4300310",
    "end": "4309030"
  },
  {
    "text": "allowing for many, many\nduplicates of each of these typical sets to be\nin the window.",
    "start": "4309030",
    "end": "4315540"
  },
  {
    "text": "So what this is saying is\nthere ought to be some critical length when the\nwindow was very large,",
    "start": "4315540",
    "end": "4320600"
  },
  {
    "text": "critical match length, and most\nof the time the match is going to be somewhere around\nthis value here.",
    "start": "4320600",
    "end": "4328309"
  },
  {
    "text": "And as w becomes truly\nhumongous, and as the match size becomes large --",
    "start": "4328310",
    "end": "4333989"
  },
  {
    "text": "you remember for these typical\nsets to make any sense, this number has to be large.",
    "start": "4333990",
    "end": "4339690"
  },
  {
    "text": "And when this number gets large,\nthe size of the typical set it humongous.",
    "start": "4339690",
    "end": "4345750"
  },
  {
    "text": "Which says, that for this\nasymptotic analysis, a window of 2 to the twentieth, probably",
    "start": "4345750",
    "end": "4352370"
  },
  {
    "text": "isn't nearly big enough. So the asymptotic analysis is\nreally saying, when you have",
    "start": "4352370",
    "end": "4358530"
  },
  {
    "text": "really humongous windows\nthis is going to work. ",
    "start": "4358530",
    "end": "4364630"
  },
  {
    "text": "You don't make windows that\nlarge, so you have to have some faith that this theoretical\nargument is going",
    "start": "4364630",
    "end": "4370560"
  },
  {
    "text": "to work here. But that tells you roughly what\nthe size of these matches is going to be.",
    "start": "4370560",
    "end": "4376280"
  },
  {
    "text": "If the size of the matches is\nthat, and you use log w bits",
    "start": "4376280",
    "end": "4385020"
  },
  {
    "text": "plus 2 log n bits, to encode\neach match, what happens? ",
    "start": "4385020",
    "end": "4394130"
  },
  {
    "text": "Encode match. ",
    "start": "4394130",
    "end": "4403350"
  },
  {
    "text": "You use log w, plus\n2 log n star.",
    "start": "4403350",
    "end": "4416180"
  },
  {
    "text": "That's the number of bits it\ntakes you, this is the number of bits it takes you to encode\nwhat the match size is.",
    "start": "4416180",
    "end": "4422770"
  },
  {
    "text": "You still have to encode that. This is the number of bits it\ntakes you to encode where the",
    "start": "4422770",
    "end": "4428790"
  },
  {
    "text": "match occurs. Now how big is this\nrelative to this?",
    "start": "4428790",
    "end": "4434290"
  },
  {
    "text": "Well n star is on the order of\nlog w, so we're taking log w",
    "start": "4434290",
    "end": "4440700"
  },
  {
    "text": "plus 2 times log of log of w. So in an approximate analysis,\nyou say, I don't even care",
    "start": "4440700",
    "end": "4448860"
  },
  {
    "text": "about that. You wind up with encoding\na match with log w bits.",
    "start": "4448860",
    "end": "4457460"
  },
  {
    "text": "So you encode n star bits, you\nuse log w bits to do it, how",
    "start": "4457460",
    "end": "4462530"
  },
  {
    "text": "many bits are you using\nper symbol? H of X given S. That's roughly\nthe idea of why the Lempel Ziv",
    "start": "4462530",
    "end": "4483724"
  },
  {
    "text": "algorithm works. Can anybody spot any problems\nwith that analysis?",
    "start": "4483725",
    "end": "4488880"
  },
  {
    "text": "Yeah. AUDIENCE: You don't know the\nprobabilities beforehand, so",
    "start": "4488880",
    "end": "4495643"
  },
  {
    "text": "how do you pick w? PROFESSOR: Good one.",
    "start": "4495643",
    "end": "4502350"
  },
  {
    "text": "You picked w by saying, I have\na computer which will go at a",
    "start": "4502350",
    "end": "4509320"
  },
  {
    "text": "certain speed. My data rate is coming in at a\ncertain speed, and I'm going",
    "start": "4509320",
    "end": "4514389"
  },
  {
    "text": "to pick w as large as\nI can keep up with. With the best algorithm\nI can think of for",
    "start": "4514390",
    "end": "4519710"
  },
  {
    "text": "doing string matching. And string matching a hard thing\nto do, but it's not a",
    "start": "4519710",
    "end": "4526690"
  },
  {
    "text": "terribly easy thing\nto do either. So you make w as large\nas you can. ",
    "start": "4526690",
    "end": "4533830"
  },
  {
    "text": "And if it's not large\nenough, tough. You got matches which are\nsomewhat smaller -- all this",
    "start": "4533830",
    "end": "4539760"
  },
  {
    "text": "argument about typical sets\nstill work except for the epsilon and deltas that\nare tucked into there.",
    "start": "4539760",
    "end": "4548690"
  },
  {
    "text": "So it's just that the epsilons\nand the deltas get too big when you're strings are\nnot long enough.",
    "start": "4548690",
    "end": "4554410"
  },
  {
    "text": "Yeah? AUDIENCE: So your w is just\nmake your processing time",
    "start": "4554410",
    "end": "4559931"
  },
  {
    "text": "equal the time [UNINTELLIGIBLE]? PROFESSOR: Yeah. That's what the determines w.",
    "start": "4559931",
    "end": "4565290"
  },
  {
    "text": "It's how fast you can do a\nstring search over this long, long window.",
    "start": "4565290",
    "end": "4570410"
  },
  {
    "text": "You're not going to just search\neverything one by one, you're going to build some kind\nof data structure there",
    "start": "4570410",
    "end": "4576300"
  },
  {
    "text": "that makes these searches\nrun fast. Can anybody think of why\nyou might not want",
    "start": "4576300",
    "end": "4581870"
  },
  {
    "text": "to make w too large? ",
    "start": "4581870",
    "end": "4588150"
  },
  {
    "text": "This isn't a theoretical\nreason, this is more practical thing. AUDIENCE: Is it if the\nprobabilities change, it's",
    "start": "4588150",
    "end": "4593993"
  },
  {
    "text": "slow to react to\nthose changes? PROFESSOR: If the probabilities\nchange, it's slow to react to them, because\nit's got this humongous window",
    "start": "4593993",
    "end": "4601900"
  },
  {
    "text": "here, and it's not until the\nwindow fills up with all of this new stuff, that it\nstarts to work well.",
    "start": "4601900",
    "end": "4613239"
  },
  {
    "text": "And before it fills up, you're\nusing an effective small window, but you're using a\nnumber of bits which is",
    "start": "4613240",
    "end": "4620909"
  },
  {
    "text": "proportionate to log of a large\nwindow, and therefore you're wasting bits.",
    "start": "4620910",
    "end": "4626370"
  },
  {
    "text": "So another thing that that\ndetermines how big you want w to be --",
    "start": "4626370",
    "end": "4631630"
  },
  {
    "text": "I mean the main thing that's\ndetermines it is just that you can't run that fast. Because you'd like to\nmake it pretty big.",
    "start": "4631630",
    "end": "4639130"
  },
  {
    "text": "Another question. How about this matter of wasting\nw symbols at the",
    "start": "4639130",
    "end": "4645139"
  },
  {
    "text": "beginning to fill\nup the window? what do you do about that? ",
    "start": "4645140",
    "end": "4651420"
  },
  {
    "text": "I mean, that's a pretty\nstupid thing, right? ",
    "start": "4651420",
    "end": "4657900"
  },
  {
    "text": "Anybody suggest a solution\nto that? If you're building this\nyourself, how",
    "start": "4657900",
    "end": "4664719"
  },
  {
    "text": "would you handle it? It's the same argument as if\nthe statistics change in",
    "start": "4664720",
    "end": "4670969"
  },
  {
    "text": "mid-stream.  I mean, you don't measure that\nthe statistics have changed,",
    "start": "4670970",
    "end": "4676889"
  },
  {
    "text": "and throw out what's\nin the window. ",
    "start": "4676890",
    "end": "4685610"
  },
  {
    "text": "What? AUDIENCE: Can you not assume\nthat you already have a typical sequence? PROFESSOR: Yes, and you\ndon't care whether",
    "start": "4685610",
    "end": "4690810"
  },
  {
    "text": "it's right or wrong. You could assume that the\ntypical sequence is all zeros,",
    "start": "4690810",
    "end": "4696239"
  },
  {
    "text": "so you fill up the window\nwith all zeros. The decoder also fills it up\nwith all zeros, because this",
    "start": "4696240",
    "end": "4701340"
  },
  {
    "text": "is the way you always start. And then you just start running\na log, looking for matches and encoding things.",
    "start": "4701340",
    "end": "4708300"
  },
  {
    "text": "And as w builds up, you\nstart matching things.",
    "start": "4708300",
    "end": "4715530"
  },
  {
    "text": "You could even be smarter, and\nknow that you're window wasn't very big and let your\nwindow grow also.",
    "start": "4715530",
    "end": "4721780"
  },
  {
    "text": "If you wanted to really\nbe fancy about this. So if you want to encode this\nyou can have a lot of fun, and",
    "start": "4721780",
    "end": "4728880"
  },
  {
    "text": "a lot of people over the years\nhave had a lot of fun trying to encode these things. It's a neat thing to do.",
    "start": "4728880",
    "end": "4734820"
  },
  {
    "start": "4734820",
    "end": "4737822"
  }
]