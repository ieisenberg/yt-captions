[
  {
    "start": "0",
    "end": "15920"
  },
  {
    "text": "PETER SZOLOVITS: OK,\nso a little over a year ago, I got a call\nfrom this committee.",
    "start": "15920",
    "end": "24620"
  },
  {
    "text": "NASEM is the National Academy\nof Science, Engineering, and Medicine.",
    "start": "24620",
    "end": "30199"
  },
  {
    "text": "So this is an august body of old\npeople with lots of gray hair",
    "start": "30200",
    "end": "36830"
  },
  {
    "text": "who have done something\nimportant enough to get elected to these academies.",
    "start": "36830",
    "end": "41870"
  },
  {
    "text": "And their research arm is called\nthe National Research Council and has a bunch of\ndifferent committees.",
    "start": "41870",
    "end": "48050"
  },
  {
    "text": "One of them is this Committee\non Science, Technology, and the Law. It's a very\ninteresting committee.",
    "start": "48050",
    "end": "53450"
  },
  {
    "text": "It's chaired by\nDavid Baltimore, who used to be an MIT professor\nuntil he went and became",
    "start": "53450",
    "end": "60260"
  },
  {
    "text": "president of Caltech. And he also happens to have\na Nobel Prize in his pocket",
    "start": "60260",
    "end": "65990"
  },
  {
    "text": "and he's a pretty famous guy. And Judge David\nTatel is a member",
    "start": "65990",
    "end": "73220"
  },
  {
    "text": "of the US Court of Appeals\nfor the District of Columbia circuit, so this is probably the\nmost important circuit court.",
    "start": "73220",
    "end": "81260"
  },
  {
    "text": "It's one level below\nthe Supreme Court. And he happens to\nsit in the seat",
    "start": "81260",
    "end": "86360"
  },
  {
    "text": "that Ruth Bader\nGinsburg occupied before she was elevated\nto the Supreme Court",
    "start": "86360",
    "end": "92120"
  },
  {
    "text": "from that Court of Appeals,\nso this is a pretty big deal. So these are heavy hitters.",
    "start": "92120",
    "end": "98970"
  },
  {
    "text": "And they convened a meeting to\ntalk about the set of topics",
    "start": "98970",
    "end": "105510"
  },
  {
    "text": "that I've listed here. So blockchain and distributed\ntrust, artificial intelligence",
    "start": "105510",
    "end": "111240"
  },
  {
    "text": "and decision making, which is\nobviously the part that I got invited to talk about,\nprivacy and informed consent",
    "start": "111240",
    "end": "118140"
  },
  {
    "text": "in an era of big data,\nscience curricula for law schools, emerging issues, and\nscience, technology, and law.",
    "start": "118140",
    "end": "126340"
  },
  {
    "text": "The issue of using litigation\nto target scientists who have opinions that you don't like.",
    "start": "126340",
    "end": "132379"
  },
  {
    "text": "And the more general\nissue of how do you communicate advances in life\nsciences to a skeptical public.",
    "start": "132380",
    "end": "140730"
  },
  {
    "text": "So this is dealing with the\nsort of anti-science tenor of the times.",
    "start": "140730",
    "end": "147030"
  },
  {
    "text": "So the group of us that talked\nabout AI and decision making, I was a little bit\nsurprised by the focus",
    "start": "147030",
    "end": "154410"
  },
  {
    "text": "because Hank really is a law\nschool professor at Stanford who's done a lot of work\non fairness and prejudice",
    "start": "154410",
    "end": "163290"
  },
  {
    "text": "in health care. Cherise Burdee is at something\ncalled the Pretrial Justice",
    "start": "163290",
    "end": "171540"
  },
  {
    "text": "Institute, and her\nissue is a legal one which is that there are\nnow a lot of companies that",
    "start": "171540",
    "end": "178770"
  },
  {
    "text": "have software that predict,\nif you get bail while you're awaiting trial, are you\nlikely to skip bail or not?",
    "start": "178770",
    "end": "187450"
  },
  {
    "text": "And so this is influential\nin the decision that judges make about\nhow much bail to impose",
    "start": "187450",
    "end": "193650"
  },
  {
    "text": "and whether to let\nyou out on bail at all or to keep you in prison,\nawaiting your trial.",
    "start": "193650",
    "end": "201209"
  },
  {
    "text": "Matt Lundgren is a radiology\nprofessor at Stanford and has done some of\nthe really cool work",
    "start": "201210",
    "end": "208020"
  },
  {
    "text": "on building convolutional\nneural network models to detect pulmonary\nemboli and various other things",
    "start": "208020",
    "end": "216140"
  },
  {
    "text": "in imaging data. You know the next guy, and\nSuresh Venkatasubramanian",
    "start": "216140",
    "end": "227130"
  },
  {
    "text": "is a professor. He was originally a theorist\nat the University of Utah",
    "start": "227130",
    "end": "232830"
  },
  {
    "text": "but has also gotten into\nthinking a lot about privacy and fairness.",
    "start": "232830",
    "end": "238290"
  },
  {
    "text": "And so that that was our panel,\nand we each gave a brief talk and then had a very\ninteresting discussion.",
    "start": "238290",
    "end": "246160"
  },
  {
    "text": "One of the things that I was\nvery surprised by is somebody raised the question of shouldn't\nTatel as a judge on the Circuit",
    "start": "246160",
    "end": "255390"
  },
  {
    "text": "Court of Appeals\nhire people like you guys to be clerks in his court?",
    "start": "255390",
    "end": "262830"
  },
  {
    "text": "So people like you\nguys who also happen to go to law school, of\nwhich there are a number now",
    "start": "262830",
    "end": "269250"
  },
  {
    "text": "of people who are trained\nin computational methods and machine learning but also\nhave the legal background.",
    "start": "269250",
    "end": "279300"
  },
  {
    "text": "And he said something\nvery interesting to me. He said, no, he\nwouldn't want people like that, which\nkind of shocked me.",
    "start": "279300",
    "end": "288585"
  },
  {
    "text": "And so we quizzed him\na little bit on why, and he said, well, because he\nviews the role of the judge",
    "start": "288585",
    "end": "298830"
  },
  {
    "text": "not to be an expert\nbut to be a judge. To be a balancer of arguments\non both sides of an issue.",
    "start": "298830",
    "end": "307590"
  },
  {
    "text": "And he was afraid that\nif he had a clerk who had a strong\ntechnical background,",
    "start": "307590",
    "end": "313530"
  },
  {
    "text": "that person would have strong\ntechnical opinions which would bias his decision\none way or another.",
    "start": "313530",
    "end": "320110"
  },
  {
    "text": "So this reminded me-- my wife was a lawyer,\nand I remember, when she was in law school, she\nwould tell me about the classes",
    "start": "320110",
    "end": "327870"
  },
  {
    "text": "that she was taking. And it became obvious\nthat studying law",
    "start": "327870",
    "end": "335250"
  },
  {
    "text": "was learning how to win, not\nlearning how to find the truth.",
    "start": "335250",
    "end": "341220"
  },
  {
    "text": "And there's this\nphilosophical notion in the law that says that\nthe truth will come out",
    "start": "341220",
    "end": "347010"
  },
  {
    "text": "from spirited argument on\ntwo sides of a question, but your duty as a lawyer is\nto argue as hard as you can",
    "start": "347010",
    "end": "355500"
  },
  {
    "text": "for your side of the argument. And in fact, in law school,\nthey teach them, like in debate,",
    "start": "355500",
    "end": "362460"
  },
  {
    "text": "that you should be able to\ntake either side of any case and be able to make a\ncogent argument for it.",
    "start": "362460",
    "end": "369750"
  },
  {
    "text": "And so Tatel sort of\nreinforced that notion in what he said, which I\nthought was interesting.",
    "start": "369750",
    "end": "377520"
  },
  {
    "text": "Well, just to talk a little\nbit about the justice area because this is the one\nthat has gotten the most",
    "start": "377520",
    "end": "384180"
  },
  {
    "text": "public attention, governments\nuse decision automation",
    "start": "384180",
    "end": "389970"
  },
  {
    "text": "for determining eligibility\nfor various kinds of services, evaluating where to deploy\nhealth inspectors and law",
    "start": "389970",
    "end": "397230"
  },
  {
    "text": "enforcement personnel,\ndefining boundaries along voting districts. So all of the gerrymandering\ndiscussion that you hear about",
    "start": "397230",
    "end": "406820"
  },
  {
    "text": "is all about using\ncomputers and actually machine learning techniques\nto try to figure out how to--",
    "start": "406820",
    "end": "413910"
  },
  {
    "text": "your objective function is to\nget Republicans or Democrats elected, depending on who's in\ncharge of the redistricting.",
    "start": "413910",
    "end": "422009"
  },
  {
    "text": "And then you tailor these\ngerrymandered districts in order to maximize the\nprobability that you're",
    "start": "422010",
    "end": "430710"
  },
  {
    "text": "going to have the majority in\nwhatever congressional races",
    "start": "430710",
    "end": "436139"
  },
  {
    "text": "or state legislative races.",
    "start": "436140",
    "end": "441300"
  },
  {
    "text": "So in the law, people are\nin favor of these ideas",
    "start": "441300",
    "end": "447110"
  },
  {
    "text": "to the extent that they\ninject clarity and precision into bail, parole, and\nsentencing decisions.",
    "start": "447110",
    "end": "453170"
  },
  {
    "text": "Algorithmic technologies\nmay minimize harms that are the\nproducts of human judgment. So we know that people\nare in fact prejudiced,",
    "start": "453170",
    "end": "461840"
  },
  {
    "text": "and so there are prejudices\nby judges and by juries that play into the decisions\nmade in the legal system.",
    "start": "461840",
    "end": "472020"
  },
  {
    "text": "So by formalizing\nit, you might win. However, conversely,\nthe use of technology",
    "start": "472020",
    "end": "479120"
  },
  {
    "text": "to determine whose liberty\nis deprived and on what terms raises significant\nconcerns about transparency",
    "start": "479120",
    "end": "486139"
  },
  {
    "text": "and interpretability. So next week, we're going to\ntalk some about transparency and interpretability,\nbut today's is really",
    "start": "486140",
    "end": "494000"
  },
  {
    "text": "about fairness. So here is an article from\nOctober of last year--",
    "start": "494000",
    "end": "501165"
  },
  {
    "text": "no, September of last year,\nsaying that as of October of this year, if you get\narrested in California,",
    "start": "501165",
    "end": "508160"
  },
  {
    "text": "the decision of whether\nyou get bail or not is going to be made by\na computer algorithm,",
    "start": "508160",
    "end": "513289"
  },
  {
    "text": "not by a human being, OK? So it's not 100%.",
    "start": "513289",
    "end": "521169"
  },
  {
    "text": "There is some discretion on the\npart of this county official",
    "start": "521169",
    "end": "526970"
  },
  {
    "text": "who will make a recommendation,\nand the judge ultimately decides, but I suspect\nthat until there",
    "start": "526970",
    "end": "533990"
  },
  {
    "text": "are some egregious\noutcomes from doing this, it will probably be\nquite commonly used.",
    "start": "533990",
    "end": "542420"
  },
  {
    "text": " Now, the critique of\nthese bail algorithms",
    "start": "542420",
    "end": "550230"
  },
  {
    "text": "is based on a number\nof different factors. One is that the algorithms\nreflect a severe racial bias.",
    "start": "550230",
    "end": "562960"
  },
  {
    "text": "So for example, if you are two\nidentical people but one of you",
    "start": "562960",
    "end": "568620"
  },
  {
    "text": "happens to be white and one\nof you happens to be black, the chances of you\ngetting bail are",
    "start": "568620",
    "end": "574740"
  },
  {
    "text": "much lower if you're black\nthan if you're white. Now, you say, well,\nhow could that",
    "start": "574740",
    "end": "581180"
  },
  {
    "text": "be given that we're learning\nthis algorithmically? Well, it's a complicated\nfeedback loop",
    "start": "581180",
    "end": "586970"
  },
  {
    "text": "because the algorithm is\nlearning from historical data, and if historically, judges have\nbeen less likely to grant bail",
    "start": "586970",
    "end": "596420"
  },
  {
    "text": "to an African-American than\nto a Caucasian-American,",
    "start": "596420",
    "end": "601730"
  },
  {
    "text": "then the algorithm will learn\nthat that's the right thing to do and will\nnicely incorporate",
    "start": "601730",
    "end": "607850"
  },
  {
    "text": "exactly that prejudice. And then the second\nproblem, which",
    "start": "607850",
    "end": "613040"
  },
  {
    "text": "I consider to be\nreally horrendous, is that in this\nparticular field,",
    "start": "613040",
    "end": "618860"
  },
  {
    "text": "the algorithms are\ndeveloped privately by private companies\nwhich will not tell you",
    "start": "618860",
    "end": "624980"
  },
  {
    "text": "what their algorithm is. You can just pay them and\nthey will tell you the answer,",
    "start": "624980",
    "end": "631890"
  },
  {
    "text": "but they won't tell you\nhow they compute it. They won't tell\nyou what data they used to train the algorithm.",
    "start": "631890",
    "end": "637740"
  },
  {
    "text": "And so it's really a black box. And so you have no idea\nwhat's going on in that box",
    "start": "637740",
    "end": "643190"
  },
  {
    "text": "other than by looking\nat its decisions. ",
    "start": "643190",
    "end": "649690"
  },
  {
    "text": "And so the data\ncollection system is flawed in the same way as\nthe judicial system itself.",
    "start": "649690",
    "end": "654810"
  },
  {
    "text": " So not only are\nthere algorithms that",
    "start": "654810",
    "end": "663030"
  },
  {
    "text": "decide whether you\nget bail or not, which is after all a\nrelatively temporary question",
    "start": "663030",
    "end": "668220"
  },
  {
    "text": "until your trial comes\nup, although that may be a long time,\nbut there are also algorithms that advise on\nthings like sentencing.",
    "start": "668220",
    "end": "677139"
  },
  {
    "text": "So they say, how likely is this\npatient to be a recidivist? Somebody who, when\nthey get out of jail,",
    "start": "677140",
    "end": "683610"
  },
  {
    "text": "they're going to offend again. And therefore, they deserve\na longer jail sentence because you want to keep\nthem off the streets.",
    "start": "683610",
    "end": "690000"
  },
  {
    "text": " Well, so this is a particular\nstory about a particular person",
    "start": "690000",
    "end": "698730"
  },
  {
    "text": "in Wisconsin, and shockingly,\nthe state Supreme Court",
    "start": "698730",
    "end": "703980"
  },
  {
    "text": "ruled against this guy,\nsaying that knowledge of the algorithm's output\nwas a sufficient level",
    "start": "703980",
    "end": "710610"
  },
  {
    "text": "of transparency in order\nto not violate his rights, which I think many\npeople consider to be",
    "start": "710610",
    "end": "718140"
  },
  {
    "text": "kind of an outrageous decision. I'm sure it'll be appealed\nand maybe overturned.",
    "start": "718140",
    "end": "725279"
  },
  {
    "text": "Conversely-- I keep doing on\nthe one hand and on the other--",
    "start": "725280",
    "end": "731070"
  },
  {
    "text": "algorithms could help\nkeep people out of jail. So there's a Wired\narticle not long ago",
    "start": "731070",
    "end": "738450"
  },
  {
    "text": "that says we can use algorithms\nto analyze people's cases",
    "start": "738450",
    "end": "746490"
  },
  {
    "text": "and say, oh, this person\nlooks like they're really in need of psychiatric help\nrather than in need of jail",
    "start": "746490",
    "end": "754110"
  },
  {
    "text": "time, and so perhaps\nwe can divert him from the penal system\ninto psychiatric care",
    "start": "754110",
    "end": "763110"
  },
  {
    "text": "and keep him out of prison\nand get him help and so on. So that's the\npositive side of being",
    "start": "763110",
    "end": "770280"
  },
  {
    "text": "able to use these\nkinds of algorithms. Now, it's not only\nin criminality.",
    "start": "770280",
    "end": "776310"
  },
  {
    "text": "There is also a\nlong discussion-- you can find this\nall over the web--",
    "start": "776310",
    "end": "781470"
  },
  {
    "text": "of, for example,\ncan an algorithm hire better than a human being.",
    "start": "781470",
    "end": "786660"
  },
  {
    "text": "So if you're a big company\nand you have a lot of people that you're trying to\nhire for various jobs,",
    "start": "786660",
    "end": "793610"
  },
  {
    "text": "it's very tempting\nto say, hey, I've made lots and lots\nof hiring decisions",
    "start": "793610",
    "end": "799160"
  },
  {
    "text": "and we have some outcome data. I know which people have\nturned out to be good employees",
    "start": "799160",
    "end": "804500"
  },
  {
    "text": "and which people have turned\nout to be bad employees, and therefore, we can base\na first-cut screening method",
    "start": "804500",
    "end": "814400"
  },
  {
    "text": "on learning such an\nalgorithm and using it on people who apply for jobs\nand say, OK, these are the ones",
    "start": "814400",
    "end": "823130"
  },
  {
    "text": "that we're going to interview\nand maybe hire because they look like they're a better bet.",
    "start": "823130",
    "end": "828470"
  },
  {
    "text": "Now, I have to tell\nyou a personal story. When I was an\nundergraduate at Caltech,",
    "start": "828470",
    "end": "834730"
  },
  {
    "text": "the Caltech faculty\ndecided that they wanted to include student\nmembers of all the faculty",
    "start": "834730",
    "end": "841009"
  },
  {
    "text": "committees. And so I was lucky\nenough that I served for three years as a member of\nthe Undergraduate Admissions",
    "start": "841010",
    "end": "847760"
  },
  {
    "text": "Committee at Caltech. And in those days, Caltech\nonly took about 220,",
    "start": "847760",
    "end": "854550"
  },
  {
    "text": "230 students a year. It's a very small school. And we would actually fly\naround the country and interview",
    "start": "854550",
    "end": "862490"
  },
  {
    "text": "about the top half of all the\napplicants in the applicant pool. So we would talk not\nonly to the students",
    "start": "862490",
    "end": "868790"
  },
  {
    "text": "but also to their teachers\nand their counselors and see what the\nenvironment was like,",
    "start": "868790",
    "end": "874879"
  },
  {
    "text": "and I think we got a very good\nsense of how good a student was likely to be based on that.",
    "start": "874880",
    "end": "880790"
  },
  {
    "text": "So one day, after the admissions\ndecisions have been made,",
    "start": "880790",
    "end": "887089"
  },
  {
    "text": "one of the professors, kind\nof as a thought experiment,",
    "start": "887090",
    "end": "892160"
  },
  {
    "text": "said here's what we ought to do. We ought to take the 230\npeople that we've just",
    "start": "892160",
    "end": "898310"
  },
  {
    "text": "offered admission to and\nwe should reject them all and take the next\n230 people, and then",
    "start": "898310",
    "end": "905510"
  },
  {
    "text": "see whether the faculty notices. Because it seemed like a\nfairly flat distribution.",
    "start": "905510",
    "end": "912769"
  },
  {
    "text": "Now, of course, I\nand others argued that this would be\nunfair and unethical",
    "start": "912770",
    "end": "919010"
  },
  {
    "text": "and would be a waste\nof all the time that we had put into\nselecting these people,",
    "start": "919010",
    "end": "924890"
  },
  {
    "text": "so we didn't do that. But then this guy\nwent out and he looked at the data we had\non people's ranking class,",
    "start": "924890",
    "end": "933590"
  },
  {
    "text": "SAT scores, grade point\naverage, the checkmarks",
    "start": "933590",
    "end": "939320"
  },
  {
    "text": "on their recommendation\nletters about whether they were truly exceptional\nor merely outstanding.",
    "start": "939320",
    "end": "945050"
  },
  {
    "text": " And he built a linear\nregression model",
    "start": "945050",
    "end": "954110"
  },
  {
    "text": "that predicted the person's\nsophomore level grade point average, which seemed\nlike a reasonable thing",
    "start": "954110",
    "end": "960170"
  },
  {
    "text": "to try to predict. And he got a\nreasonably good fit, but what was\ndisturbing about it is",
    "start": "960170",
    "end": "966920"
  },
  {
    "text": "that in the Caltech\npopulation of students,",
    "start": "966920",
    "end": "974120"
  },
  {
    "text": "it turned out that the beta for\nyour SAT English performance",
    "start": "974120",
    "end": "982190"
  },
  {
    "text": "was negative. So if you did particularly\nwell in English on the SAT,",
    "start": "982190",
    "end": "990670"
  },
  {
    "text": "you were likely to do worse\nas a sophomore at Caltech than if you didn't do as well.",
    "start": "990670",
    "end": "997069"
  },
  {
    "text": "And so we thought\nabout that a lot, and of course, we\ndecided that that would be really unfair\nto penalize somebody",
    "start": "997070",
    "end": "1003780"
  },
  {
    "text": "for being good at something,\nespecially when the school had this philosophical\norientation that",
    "start": "1003780",
    "end": "1010410"
  },
  {
    "text": "said we ought to look for\npeople with broad educations.",
    "start": "1010410",
    "end": "1015519"
  },
  {
    "text": "So that's just an example. And more, Science\nFriday had a nice show",
    "start": "1015520",
    "end": "1022230"
  },
  {
    "text": "that you can listen\nto about this issue. So let me ask you, what\ndo you mean by fairness?",
    "start": "1022230",
    "end": "1031040"
  },
  {
    "text": "If we're going to define\nthe concept, what is fair?",
    "start": "1031040",
    "end": "1041459"
  },
  {
    "start": "1041460",
    "end": "1046869"
  },
  {
    "text": "What characteristics\nwould you like to have an algorithm\nhave that judges you for some particular purpose?",
    "start": "1046869",
    "end": "1053990"
  },
  {
    "text": "Yeah? AUDIENCE: It's impossible to\npin down sort of, at least might in my opinion,\none specific definition,",
    "start": "1053990",
    "end": "1059620"
  },
  {
    "text": "but for the pre-trial\nsuccess rate for example, I think having the error rates\nbe similar across populations,",
    "start": "1059620",
    "end": "1066010"
  },
  {
    "text": "across the covariants you\nmight care about, for example, fairness, I think\nis a good start.",
    "start": "1066010",
    "end": "1071430"
  },
  {
    "start": "1071430",
    "end": "1077490"
  },
  {
    "text": "PETER SZOLOVITS: OK, so similar\nerror rates is definitely one of the criteria that people\nuse in talking about fairness.",
    "start": "1077490",
    "end": "1086399"
  },
  {
    "text": "And you'll see later Irene-- where's Irene? Right there. Irene is a master of\nthat notion of fairness.",
    "start": "1086400",
    "end": "1095265"
  },
  {
    "text": " Yeah? AUDIENCE: When the model says\nsome sort of observation that",
    "start": "1095265",
    "end": "1102029"
  },
  {
    "text": "causally shouldn't\nbe true, and what I want society to look like",
    "start": "1102030",
    "end": "1108240"
  },
  {
    "text": "PETER SZOLOVITS:\nSo I'm not sure how to capture that\nin a short phrase. ",
    "start": "1108240",
    "end": "1116730"
  },
  {
    "text": "Societal goals. ",
    "start": "1116730",
    "end": "1123490"
  },
  {
    "text": "But that's tricky, right? I mean, suppose that I\nwould like it to be the case",
    "start": "1123490",
    "end": "1128830"
  },
  {
    "text": "that the fraction of people\nof different ethnicity who are criminals\nshould be the same.",
    "start": "1128830",
    "end": "1136650"
  },
  {
    "text": "That seems like a good\ngoal for fairness. ",
    "start": "1136650",
    "end": "1142570"
  },
  {
    "text": "How do I achieve that? I mean, I could pretend\nthat it's the same, but it isn't the same\ntoday objectively,",
    "start": "1142570",
    "end": "1150600"
  },
  {
    "text": "and the data wouldn't\nsupport that. So that's an issue. Yeah?",
    "start": "1150600",
    "end": "1157070"
  },
  {
    "text": "AUDIENCE: People who are similar\nshould be treated similarly, so engaged sort of\nindependent of the [INAUDIBLE]",
    "start": "1157070",
    "end": "1165540"
  },
  {
    "text": "attributes or independent\nof your covariate. PETER SZOLOVITS:\nSimilar people should",
    "start": "1165540",
    "end": "1171019"
  },
  {
    "text": "lead to similar treatment.",
    "start": "1171020",
    "end": "1176240"
  },
  {
    "text": "Yeah, I like that. AUDIENCE: I didn't make it up.",
    "start": "1176240",
    "end": "1181769"
  },
  {
    "text": "PETER SZOLOVITS: I know. It's another of the classic\nsort of notions of fairness.",
    "start": "1181770",
    "end": "1187230"
  },
  {
    "text": " That puts a lot of weight on\nthe distance function, right?",
    "start": "1187230",
    "end": "1196350"
  },
  {
    "text": "In what way are\nto people similar? And what characteristics--\nyou obviously",
    "start": "1196350",
    "end": "1201650"
  },
  {
    "text": "don't want to use the\nsensitive characteristics, the forbidden characteristics\nin order to decide similarity,",
    "start": "1201650",
    "end": "1208700"
  },
  {
    "text": "because then people will\nbe dissimilar in ways that you don't want, but\ndefining that function",
    "start": "1208700",
    "end": "1215480"
  },
  {
    "text": "is a challenge. All right, well, let me show\nyou a more technical approach",
    "start": "1215480",
    "end": "1225409"
  },
  {
    "text": "to thinking about this. So we all know about biases like\nselection bias, sampling bias,",
    "start": "1225410",
    "end": "1231470"
  },
  {
    "text": "reporting bias, et cetera. These are in the conventional\nsense of the term bias.",
    "start": "1231470",
    "end": "1237410"
  },
  {
    "text": "But I'll show you an example\nthat I got involved in. Raj Manrai was a MIT\nHarvard HST student,",
    "start": "1237410",
    "end": "1250460"
  },
  {
    "text": "and he started looking at the\nquestion of the genetics that",
    "start": "1250460",
    "end": "1257240"
  },
  {
    "text": "was used in order to\ndetermine whether somebody is at risk for cardiomyopathy,\nhypertrophic cardiomyopathy.",
    "start": "1257240",
    "end": "1267169"
  },
  {
    "text": "That's a big word. It means that your\nheart gets too big and it becomes sort of flabby\nand it stops pumping well,",
    "start": "1267170",
    "end": "1275480"
  },
  {
    "text": "and eventually, you die of this\ndisease at a relatively young age, if, in fact, you have it.",
    "start": "1275480",
    "end": "1280669"
  },
  {
    "text": " So what happened\nis that there was",
    "start": "1280670",
    "end": "1289890"
  },
  {
    "text": "a study that was done mostly\nwith European populations",
    "start": "1289890",
    "end": "1294920"
  },
  {
    "text": "where they discovered that a lot\nof people who had this disease had a certain genetic variant.",
    "start": "1294920",
    "end": "1302130"
  },
  {
    "text": "And they said, well, that must\nbe the cause of this disease, and so it became accepted\nwisdom that if you",
    "start": "1302130",
    "end": "1309720"
  },
  {
    "text": "had that genetic variant, people\nwould counsel you to not plan",
    "start": "1309720",
    "end": "1315539"
  },
  {
    "text": "on living a long life. And this has all\nkinds of consequences.",
    "start": "1315540",
    "end": "1321420"
  },
  {
    "text": "Imagine if you're\nthinking about having a kid when you're\nin your early 40s,",
    "start": "1321420",
    "end": "1326850"
  },
  {
    "text": "and your life expectancy is 55. Would you want to die\nwhen you have a teenager",
    "start": "1326850",
    "end": "1332370"
  },
  {
    "text": "that you leave to your spouse? So this was a consequential\nset of decisions that people have to make.",
    "start": "1332370",
    "end": "1340080"
  },
  {
    "text": "Now, what happened\nis that in the US, there were tests\nof this sort done,",
    "start": "1340080",
    "end": "1347850"
  },
  {
    "text": "but the problem was that a lot\nof African and African-American populations turned out to have\nthis genetic variant frequently",
    "start": "1347850",
    "end": "1357810"
  },
  {
    "text": "without developing\nthis terrible disease, but they were all told that they\nwere going to die, basically.",
    "start": "1357810",
    "end": "1366730"
  },
  {
    "text": "And it was only after\nyears when people noticed that these\npeople who were supposed",
    "start": "1366730",
    "end": "1371910"
  },
  {
    "text": "to die genetically weren't\ndying that they said,",
    "start": "1371910",
    "end": "1377220"
  },
  {
    "text": "maybe we misunderstood\nsomething. And what they misunderstood\nwas that the population that",
    "start": "1377220",
    "end": "1384090"
  },
  {
    "text": "was used to develop the\nmodel was a European ancestry",
    "start": "1384090",
    "end": "1389549"
  },
  {
    "text": "population and not an\nAfrican ancestry population.",
    "start": "1389550",
    "end": "1394620"
  },
  {
    "text": "So you go, well, we must\nhave learned that lesson. So this paper was\npublished in 2016,",
    "start": "1394620",
    "end": "1401160"
  },
  {
    "text": "and this was one of\nthe first in this area. Here's a paper\nthat was published",
    "start": "1401160",
    "end": "1406950"
  },
  {
    "text": "three weeks ago in Nature\nScientific Reports that says,",
    "start": "1406950",
    "end": "1412860"
  },
  {
    "text": "genetic risk factors\nidentified in populations of European descent do\nnot improve the prediction",
    "start": "1412860",
    "end": "1419580"
  },
  {
    "text": "of osteoporotic fracture\nand bone mineral density in Chinese populations.",
    "start": "1419580",
    "end": "1425410"
  },
  {
    "text": "So it's the same story. It's exactly the same story. Different disease,\nthe consequence",
    "start": "1425410",
    "end": "1431070"
  },
  {
    "text": "is probably less\ndire because being told that you're going to break\nyour bones when you're old",
    "start": "1431070",
    "end": "1437039"
  },
  {
    "text": "is not as bad as being told\nthat your heart's going to stop working when you're in\nyour 50s, but there we have it.",
    "start": "1437040",
    "end": "1445590"
  },
  {
    "text": "OK, so technically, where\ndoes bias come from? Well, I mentioned\nthe standard sources,",
    "start": "1445590",
    "end": "1453580"
  },
  {
    "text": "but here is an\ninteresting analysis. This comes from\nConstantine Aliferis",
    "start": "1453580",
    "end": "1458670"
  },
  {
    "text": "from a number of\nyears ago, 2006, and he says, well, look,\nin a perfect world,",
    "start": "1458670",
    "end": "1469550"
  },
  {
    "text": "if I give you a\ndata set, there's an uncountably infinite\nnumber of models",
    "start": "1469550",
    "end": "1475240"
  },
  {
    "text": "that might possibly explain\nthe relationships in that data. ",
    "start": "1475240",
    "end": "1482670"
  },
  {
    "text": "I cannot enumerate an\nuncountable number of models,",
    "start": "1482670",
    "end": "1487710"
  },
  {
    "text": "and so what I'm going to do is\nchoose some family of models to try to fit, and then I'm\ngoing to use some fitting",
    "start": "1487710",
    "end": "1495450"
  },
  {
    "text": "technique, like stochastic\ngradient descent or something, that will find maybe a global\noptimum, but maybe not.",
    "start": "1495450",
    "end": "1505440"
  },
  {
    "text": "Maybe it'll find\nthe local optimum. And then there is noise.",
    "start": "1505440",
    "end": "1511370"
  },
  {
    "text": "And so his observation\nis that if you count O as the optimal possible\nmodel over all possible model",
    "start": "1511370",
    "end": "1520610"
  },
  {
    "text": "families, and if you count\nL as the best model that's learnable by a particular\nlearning mechanism,",
    "start": "1520610",
    "end": "1529580"
  },
  {
    "text": "and you call A the actual\nmodel that's learned,",
    "start": "1529580",
    "end": "1534799"
  },
  {
    "text": "then the bias is\nessentially O minus L, so its limitation\nof learning method",
    "start": "1534800",
    "end": "1541910"
  },
  {
    "text": "related to the target model. The variance is\nlike L minus A, it's",
    "start": "1541910",
    "end": "1549140"
  },
  {
    "text": "the error that's due to the\nparticular way in which you learned things, like\nsampling and so on,",
    "start": "1549140",
    "end": "1557550"
  },
  {
    "text": "and you can estimate the\nsignificance of differences between different models\nby just permuting the data,",
    "start": "1557550",
    "end": "1565720"
  },
  {
    "text": "randomizing, essentially, the\nrelationships in the data. And then you get a curve of\nperformance of those models,",
    "start": "1565720",
    "end": "1573740"
  },
  {
    "text": "and if yours lies outside\nthe 95% confidence interval,",
    "start": "1573740",
    "end": "1579179"
  },
  {
    "text": "then you have a P\nequal 0.05 result that this model is not random.",
    "start": "1579180",
    "end": "1586950"
  },
  {
    "text": "So that's the typical\nway of going about this. ",
    "start": "1586950",
    "end": "1594170"
  },
  {
    "text": "Now, you might say, but isn't\ndiscrimination the very reason",
    "start": "1594170",
    "end": "1602020"
  },
  {
    "text": "we do machine learning? Not discrimination\nin the legal sense,",
    "start": "1602020",
    "end": "1607269"
  },
  {
    "text": "but discrimination in\nthe sense of separating different populations. And so you could say,\nwell, yes, but some basis",
    "start": "1607270",
    "end": "1615610"
  },
  {
    "text": "for differentiation\nare justified and some basis for\ndifferentiation are not justified.",
    "start": "1615610",
    "end": "1622750"
  },
  {
    "text": "So they're either\npractically irrelevant, or we decide for\nsocietal goals that we",
    "start": "1622750",
    "end": "1630760"
  },
  {
    "text": "want them to be irrelevant\nand we're not going to take them into account.",
    "start": "1630760",
    "end": "1636429"
  },
  {
    "text": "So one lesson from people who\nhave studied this for a while",
    "start": "1636430",
    "end": "1641470"
  },
  {
    "text": "is that discrimination\nis domain specific. So you can't define\na universal notion",
    "start": "1641470",
    "end": "1649809"
  },
  {
    "text": "of what it means to discriminate\nbecause it's very much tied to these questions of\nwhat is practically",
    "start": "1649810",
    "end": "1656860"
  },
  {
    "text": "and morally irrelevant in the\ndecisions that you're making. And so it's going to be\ndifferent in criminal law",
    "start": "1656860",
    "end": "1663830"
  },
  {
    "text": "than it is in medicine, than\nit is in hiring, than it is in various other\nfields, college admissions,",
    "start": "1663830",
    "end": "1670660"
  },
  {
    "text": "for example. And it's\nfeature-specific as well, so you have to take\nthe individual features",
    "start": "1670660",
    "end": "1677200"
  },
  {
    "text": "into account. Well, historically,\nthe government",
    "start": "1677200",
    "end": "1682420"
  },
  {
    "text": "has tried to regulate\nthese domains, and so credit is regulated by\nthe Equal Credit Opportunity",
    "start": "1682420",
    "end": "1691480"
  },
  {
    "text": "Act, education by\nthe Civil Rights Act and various amendments,\nemployment by the Civil Rights",
    "start": "1691480",
    "end": "1698860"
  },
  {
    "text": "Act, housing by the\nFair Housing Act, public accommodation by\nthe Civil Rights Act,",
    "start": "1698860",
    "end": "1704980"
  },
  {
    "text": "more recently, marriage\nis regulated originally",
    "start": "1704980",
    "end": "1710110"
  },
  {
    "text": "by the Defense of\nMarriage Act, which as you might tell\nfrom its title, was against things like\npeople being able to marry who",
    "start": "1710110",
    "end": "1719170"
  },
  {
    "text": "were not a traditional marriage\nthat they wanted to defend,",
    "start": "1719170",
    "end": "1725620"
  },
  {
    "text": "but it was struck down\nby the Supreme Court about six years ago as\nbeing discriminatory.",
    "start": "1725620",
    "end": "1733990"
  },
  {
    "text": "It's interesting, if you look\nback to probably before you guys were born in\n1967, until 1967,",
    "start": "1733990",
    "end": "1742400"
  },
  {
    "text": "it was illegal for an\nAfrican-American and a white to marry each other in Virginia.",
    "start": "1742400",
    "end": "1750140"
  },
  {
    "text": "It was literally illegal. If you went to get a marriage\nlicense, you were denied,",
    "start": "1750140",
    "end": "1757100"
  },
  {
    "text": "and if you got married out\nof state and came back, you could be arrested.",
    "start": "1757100",
    "end": "1762620"
  },
  {
    "text": "This happened much later. Trevor Noah, if you know\nhim from The Daily Show,",
    "start": "1762620",
    "end": "1768440"
  },
  {
    "text": "wrote a book called\nBorn a Crime, I think, and his father\nis white Swiss guy",
    "start": "1768440",
    "end": "1775550"
  },
  {
    "text": "and his mother is a\nSouth African black, and so it was literally\nillegal for him",
    "start": "1775550",
    "end": "1783110"
  },
  {
    "text": "to exist under the apartheid\nlaws that they had.",
    "start": "1783110",
    "end": "1788270"
  },
  {
    "text": "He had to pretend to be-- his mother was his caretaker\nrather than his mother",
    "start": "1788270",
    "end": "1796490"
  },
  {
    "text": "in order to be able\nto go out in public, because otherwise, they\nwould get arrested.",
    "start": "1796490",
    "end": "1802070"
  },
  {
    "text": "So this has recently, of\ncourse, also disappeared, but these are some of\nthe regulatory issues.",
    "start": "1802070",
    "end": "1810429"
  },
  {
    "text": "So here are some of the legally\nrecognized protected classes, race, color, sex, religion,\nnational origin, citizenship,",
    "start": "1810430",
    "end": "1818880"
  },
  {
    "text": "age, pregnancy, familial status,\ndisability, veteran status, and more recently,\nsexual orientation",
    "start": "1818880",
    "end": "1827159"
  },
  {
    "text": "in certain jurisdictions,\nbut not everywhere around the country. ",
    "start": "1827160",
    "end": "1834880"
  },
  {
    "text": "OK, so given those\nexamples, there are two legal doctrines about\ndiscrimination, and one of them",
    "start": "1834880",
    "end": "1844330"
  },
  {
    "text": "talks about disparate\ntreatment, which is sort of related to this one.",
    "start": "1844330",
    "end": "1852540"
  },
  {
    "text": "And the other talks\nabout disparate impact and says, no matter\nwhat the mechanism",
    "start": "1852540",
    "end": "1858500"
  },
  {
    "text": "is, if the outcome is very\ndifferent for different racial groups typically or\ngender groups, then there",
    "start": "1858500",
    "end": "1867030"
  },
  {
    "text": "is prima facie evidence that\nthere is something not right, that there is some\nsort of discrimination.",
    "start": "1867030",
    "end": "1874710"
  },
  {
    "text": "Now, the problem is, how do\nyou defend yourself against,",
    "start": "1874710",
    "end": "1881789"
  },
  {
    "text": "for example, a disparate\nimpact argument? Well, you say, in order to\nbe disparate impact that's",
    "start": "1881790",
    "end": "1891590"
  },
  {
    "text": "illegal, it has to be\nunjustified or avoidable. So for example,\nsuppose I'm trying",
    "start": "1891590",
    "end": "1899240"
  },
  {
    "text": "to hire people to climb\n50-story buildings that",
    "start": "1899240",
    "end": "1905330"
  },
  {
    "text": "are under construction, and\nyou apply, but it turns out you have a medical\ncondition which",
    "start": "1905330",
    "end": "1911450"
  },
  {
    "text": "is that you get\ndizzy at times, I might say, you know what,\nI don't want to hire you,",
    "start": "1911450",
    "end": "1919179"
  },
  {
    "text": "because I don't want\nyou plopping off the 50th floor of a building\nthat's under construction,",
    "start": "1919180",
    "end": "1925059"
  },
  {
    "text": "and that's probably\na reasonable defense. If I brought suit\nagainst you and said,",
    "start": "1925060",
    "end": "1930530"
  },
  {
    "text": "hey, you're\ndiscriminating against me on the basis of this\nmedical disability,",
    "start": "1930530",
    "end": "1936430"
  },
  {
    "text": "a perfectly good defense\nis, yeah, it's true, but it's relevant to the job.",
    "start": "1936430",
    "end": "1944030"
  },
  {
    "text": "So that's one way\nof dealing with it. Now, how do you demonstrate\ndisparate impact?",
    "start": "1944030",
    "end": "1950170"
  },
  {
    "text": "Well, the court has\ndecided that you need to be able to show\nabout a 20% difference",
    "start": "1950170",
    "end": "1956230"
  },
  {
    "text": "in order to call something\ndisparate impact. ",
    "start": "1956230",
    "end": "1964110"
  },
  {
    "text": "So the question, of course,\nis can we change our hiring policies or whatever\npolicies we're using in order",
    "start": "1964110",
    "end": "1971840"
  },
  {
    "text": "to achieve the same\ngoals, but with less of a disparity in the impact.",
    "start": "1971840",
    "end": "1978140"
  },
  {
    "text": "So that's the challenge. ",
    "start": "1978140",
    "end": "1983220"
  },
  {
    "text": "Now, what's interesting is\nthat disparate treatment and disparate impact are really\nin conflict with each other.",
    "start": "1983220",
    "end": "1992490"
  },
  {
    "text": "And you'll find that\nthis is true in almost everything in this domain.",
    "start": "1992490",
    "end": "1997930"
  },
  {
    "text": "So disparate impact is\nabout distributive justice and minimizing\nequality of outcome.",
    "start": "1997930",
    "end": "2005240"
  },
  {
    "text": "Disparate treatment is\nabout procedural fairness and equality of opportunity,\nand those don't always mesh.",
    "start": "2005240",
    "end": "2012170"
  },
  {
    "text": "In other words, it may well be\nthat equality of opportunity still leads to\ndifferences in outcome,",
    "start": "2012170",
    "end": "2020930"
  },
  {
    "text": "and you can't square\nthat circle easily. ",
    "start": "2020930",
    "end": "2030670"
  },
  {
    "text": "Well, there's a lot\nof discrimination that keeps persisting. There's plenty of evidence\nin the literature.",
    "start": "2030670",
    "end": "2037900"
  },
  {
    "text": "And one of the problems\nis that, for example,",
    "start": "2037900",
    "end": "2043400"
  },
  {
    "text": "take an issue like the disparity\nbetween different races",
    "start": "2043400",
    "end": "2051610"
  },
  {
    "text": "or different ethnicities. It turns out that we don't\nhave a nicely balanced",
    "start": "2051610",
    "end": "2056649"
  },
  {
    "text": "set where the number of\npeople of European descent",
    "start": "2056650",
    "end": "2061869"
  },
  {
    "text": "is equal to the number of\npeople of African-American, or Hispanic, or\nAsian, or whatever",
    "start": "2061870",
    "end": "2067750"
  },
  {
    "text": "population you choose\ndescent, and therefore, we tend to know a lot more\nabout the majority class",
    "start": "2067750",
    "end": "2075250"
  },
  {
    "text": "than we know about\nthese minority classes, and just that additional data\nand that additional knowledge",
    "start": "2075250",
    "end": "2081819"
  },
  {
    "text": "might mean that we're able to\nreduce the error rate simply because we have a\nlarger sample size.",
    "start": "2081820",
    "end": "2089980"
  },
  {
    "text": "OK, so if you want\nto formalize this, this is Moritz Hardt's\npart of the tutorial",
    "start": "2089980",
    "end": "2099130"
  },
  {
    "text": "that I'm stealing\nfrom in this talk. ",
    "start": "2099130",
    "end": "2106059"
  },
  {
    "text": "This was given at KDD about a\nyear and a half ago, I think.",
    "start": "2106060",
    "end": "2111430"
  },
  {
    "text": "And Moritz is a\nprofessor at Berkeley who actually teaches an\nentire semester-long course",
    "start": "2111430",
    "end": "2118150"
  },
  {
    "text": "on fairness in machine learning,\nso there's a lot of material here.",
    "start": "2118150",
    "end": "2123190"
  },
  {
    "text": "And so he formalizes\nthe problem this way. He says, look, a decision\nproblem, a model, in our terms,",
    "start": "2123190",
    "end": "2133920"
  },
  {
    "text": "is that we have some X,\nwhich is the set of features we know about an individual,\nand we have some said A, which",
    "start": "2133920",
    "end": "2141960"
  },
  {
    "text": "is the set of\nprotected features, like your race, or your\ngender, or your age,",
    "start": "2141960",
    "end": "2148049"
  },
  {
    "text": "or whatever it is we're trying\nto prevent from discriminating on, and then we have either\na classifier or some score",
    "start": "2148050",
    "end": "2156210"
  },
  {
    "text": "or predictive function\nthat's a function of X and A",
    "start": "2156210",
    "end": "2162089"
  },
  {
    "text": "in either case, and then we have\nsome Y, which is the outcome that we're interested\nin predicting.",
    "start": "2162090",
    "end": "2169530"
  },
  {
    "text": "So now you can begin to tease\napart some different notions of fairness by looking\nat the relationships",
    "start": "2169530",
    "end": "2177210"
  },
  {
    "text": "between these elements. So there are three criteria\nthat appear in the literature.",
    "start": "2177210",
    "end": "2183520"
  },
  {
    "text": "One of them is the\nnotion of independence of the scoring function\nfrom sensitive attributes.",
    "start": "2183520",
    "end": "2189270"
  },
  {
    "text": "So this says that R is\nindependent from A. Remember,",
    "start": "2189270",
    "end": "2194430"
  },
  {
    "text": "on the previous slide, I said\nthat R is a function of--",
    "start": "2194430",
    "end": "2200099"
  },
  {
    "text": "oops. R is a function of X and A,\nso obviously, that criterion",
    "start": "2200100",
    "end": "2206759"
  },
  {
    "text": "says that it can't be a\nfunction of A. Null function.",
    "start": "2206760",
    "end": "2213530"
  },
  {
    "text": "Another notion is\nseparation of score and the sensitive attribute\ngiven the outcome.",
    "start": "2213530",
    "end": "2220050"
  },
  {
    "text": "So this is the one that says\nthe different groups are going to be treated similarly.",
    "start": "2220050",
    "end": "2225710"
  },
  {
    "text": "In other words, if I tell\nyou the group, the outcome,",
    "start": "2225710",
    "end": "2231530"
  },
  {
    "text": "the people who did\nwell at the job and the people who\ndid poorly at the job, then the scoring function is\nindependent of the protected",
    "start": "2231530",
    "end": "2241849"
  },
  {
    "text": "attribute. So that allows a\nlittle more wiggle room",
    "start": "2241850",
    "end": "2247660"
  },
  {
    "text": "because it says that the\nprotected attribute can still predict something\nabout the outcome,",
    "start": "2247660",
    "end": "2254120"
  },
  {
    "text": "it's just that you can't use it\nin the scoring function given the category of which\noutcome category",
    "start": "2254120",
    "end": "2261700"
  },
  {
    "text": "that individual belongs to. And then sufficiency\nis the inverse of that.",
    "start": "2261700",
    "end": "2266980"
  },
  {
    "text": "It says that given\nthe scoring function, the outcome is independent\nof the protected attribute.",
    "start": "2266980",
    "end": "2274420"
  },
  {
    "text": "So that says, can we\nbuild a fair scoring function that separates the\noutcome from the protected",
    "start": "2274420",
    "end": "2282280"
  },
  {
    "text": "attribute? So here's some detail on those. If you look at independence--",
    "start": "2282280",
    "end": "2288750"
  },
  {
    "text": "this is also called by\nvarious other names-- basically, what it says\nis that the probability",
    "start": "2288750",
    "end": "2296039"
  },
  {
    "text": "of a particular\nresult, R equal 1, is the same whether\nyou're in class A or class",
    "start": "2296040",
    "end": "2303180"
  },
  {
    "text": "B in the protected attribute.",
    "start": "2303180",
    "end": "2308880"
  },
  {
    "text": "So what does that tell you? That tells you that the\nscoring function has to be universal over\nthe entire data set",
    "start": "2308880",
    "end": "2318080"
  },
  {
    "text": "and has to not distinguish\nbetween people in class A versus class B. That's a\npretty strong requirement.",
    "start": "2318080",
    "end": "2329920"
  },
  {
    "text": "And then you can operationalize\nthe notion of unfairness",
    "start": "2329920",
    "end": "2335770"
  },
  {
    "text": "either by looking for\nan absolute difference between those probabilities.",
    "start": "2335770",
    "end": "2340930"
  },
  {
    "text": "If it's greater\nthan some epsilon, then you have evidence that this\nis not a fair scoring function,",
    "start": "2340930",
    "end": "2347380"
  },
  {
    "text": "or a ratio test that says,\nwe look at the ratio,",
    "start": "2347380",
    "end": "2352579"
  },
  {
    "text": "and if it differs\nfrom 1 significantly, then you have evidence that this\nis an unfair scoring function.",
    "start": "2352580",
    "end": "2360140"
  },
  {
    "text": "And by the way, this\nrelates to the 4/5 rule, because if you make\nepsilon 20%, then that's",
    "start": "2360140",
    "end": "2369130"
  },
  {
    "text": "the same as the 4/5 rule. Now, the problem-- there are\nproblems with this notion",
    "start": "2369130",
    "end": "2376300"
  },
  {
    "text": "of independence. So it only requires\nequal rates of decisions",
    "start": "2376300",
    "end": "2382089"
  },
  {
    "text": "for hiring, or giving somebody\na liver for transplant, or whatever topic\nyou're interested in.",
    "start": "2382090",
    "end": "2389840"
  },
  {
    "text": "And so what if hiring is based\non a good score in group A,",
    "start": "2389840",
    "end": "2394900"
  },
  {
    "text": "but random in B? So for example, what if we\nknow a lot more information",
    "start": "2394900",
    "end": "2401860"
  },
  {
    "text": "about group A than\nwe do about group B, so we have a better\nway of scoring them",
    "start": "2401860",
    "end": "2408000"
  },
  {
    "text": "than we do of scoring group\nB. So you might wind up with a situation\nwhere you wind up",
    "start": "2408000",
    "end": "2415630"
  },
  {
    "text": "hiring the same\nnumber of people, the same ratio of people in\nboth groups, but in one group,",
    "start": "2415630",
    "end": "2422800"
  },
  {
    "text": "you've done a good\njob of selecting out the good candidates,\nand in the other group, you've essentially\ndone it at random.",
    "start": "2422800",
    "end": "2431290"
  },
  {
    "text": "Well, the outcomes are likely\nto be better for a group A than for group B, which means\nthat you're developing more",
    "start": "2431290",
    "end": "2438940"
  },
  {
    "text": "data for the future\nthat says, we really ought to be hiring\npeople in group A",
    "start": "2438940",
    "end": "2444430"
  },
  {
    "text": "because they have\nbetter outcomes. So there's this feedback loop.",
    "start": "2444430",
    "end": "2450170"
  },
  {
    "text": "Or alternatively--\nwell, of course, it could be caused\nby malice also. ",
    "start": "2450170",
    "end": "2457250"
  },
  {
    "text": "I could just decide as\na hiring manager I'm not hiring enough\nAfrican-Americans so I'm just",
    "start": "2457250",
    "end": "2463010"
  },
  {
    "text": "going to take some random\nsample of African-Americans and hire them, and then\nmaybe they'll do badly,",
    "start": "2463010",
    "end": "2469710"
  },
  {
    "text": "and then I'll have more\ndata to demonstrate that this was a bad idea. So that would be malicious.",
    "start": "2469710",
    "end": "2478150"
  },
  {
    "text": "There's also a\ntechnical problem, which is it's possible that\nthe category, the group",
    "start": "2478150",
    "end": "2484450"
  },
  {
    "text": "is a perfect predictor\nof the outcome, in which case, of course, they\ncan't be separated.",
    "start": "2484450",
    "end": "2489760"
  },
  {
    "text": "They can't be independent\nof each other.",
    "start": "2489760",
    "end": "2494820"
  },
  {
    "text": "Now, how do you\nachieve independence? Well, there are a number\nof different techniques.",
    "start": "2494820",
    "end": "2499920"
  },
  {
    "text": "One of them is-- there's this article\nby Zemel about learning",
    "start": "2499920",
    "end": "2506690"
  },
  {
    "text": "fair representations,\nand what it says is you create a new\nworld representation, Z,",
    "start": "2506690",
    "end": "2515810"
  },
  {
    "text": "which is some\ncombination of X and A, and you do this by maximizing\nthe mutual information",
    "start": "2515810",
    "end": "2524290"
  },
  {
    "text": "between X and Z\nand by minimizing the mutual information\nbetween the A and Z.",
    "start": "2524290",
    "end": "2532410"
  },
  {
    "text": "So this is an idea\nthat I've seen used in machine learning\nfor robustness rather",
    "start": "2532410",
    "end": "2540150"
  },
  {
    "text": "than for fairness,\nwhere people say, the problem is that given\na particular data set,",
    "start": "2540150",
    "end": "2546210"
  },
  {
    "text": "you can overfit to that data\nset, and so one of the ideas is to do a Gann-like\nmethod where you say,",
    "start": "2546210",
    "end": "2554700"
  },
  {
    "text": "I want to train my\nclassifier, let's say, not only to work well on\ngetting the right answer,",
    "start": "2554700",
    "end": "2562050"
  },
  {
    "text": "but also to work as poorly as\npossible on identifying which",
    "start": "2562050",
    "end": "2567680"
  },
  {
    "text": "data set my example came from. So this is the\nsame sort of idea.",
    "start": "2567680",
    "end": "2572960"
  },
  {
    "text": "It's a representation\nlearning idea. And then you build\nyour predictor, R,",
    "start": "2572960",
    "end": "2578780"
  },
  {
    "text": "based on this representation,\nwhich is perhaps not perfectly independent of\nthe protected attribute,",
    "start": "2578780",
    "end": "2586940"
  },
  {
    "text": "but is as independent\nas possible. And usually, there are knobs\nin these learning algorithms,",
    "start": "2586940",
    "end": "2592790"
  },
  {
    "text": "and depending on how\nyou turn the knob, you can affect\nwhether you're going",
    "start": "2592790",
    "end": "2598260"
  },
  {
    "text": "to get a better classifier\nthat's more discriminatory or a worse classifier\nthat's less discriminatory.",
    "start": "2598260",
    "end": "2605910"
  },
  {
    "text": "So you can do that\nin pre-processing. You can do some kind of\nincorporating in the loss",
    "start": "2605910",
    "end": "2614490"
  },
  {
    "text": "function a dependence notion or\nan independence notion and say,",
    "start": "2614490",
    "end": "2620880"
  },
  {
    "text": "we're going to train on\na particular data set, imposing this notion of\nwanting this independence",
    "start": "2620880",
    "end": "2628260"
  },
  {
    "text": "between A and R as\npart of our desiderata.",
    "start": "2628260",
    "end": "2633600"
  },
  {
    "text": "And so you, again,\nare making trade-offs against other characteristics.",
    "start": "2633600",
    "end": "2638640"
  },
  {
    "text": "Or you can do post-processing. So suppose I've built an\noptimal R, not worrying",
    "start": "2638640",
    "end": "2645600"
  },
  {
    "text": "about discrimination, then\nI can do another learning problem that says I'm now going\nto build a new F, which takes",
    "start": "2645600",
    "end": "2653850"
  },
  {
    "text": "R and the protected\nattribute into account, and it's going to minimize the\ncost of misclassifications.",
    "start": "2653850",
    "end": "2662660"
  },
  {
    "text": "And again, there's a knob where\nyou can say, how much do I want to emphasize\nmisclassifications",
    "start": "2662660",
    "end": "2668900"
  },
  {
    "text": "for the protected\nattribute or based on the protected attribute? ",
    "start": "2668900",
    "end": "2676620"
  },
  {
    "text": "So this was still talking\nabout independence. The next notion is separation,\nthat says given the outcome,",
    "start": "2676620",
    "end": "2685890"
  },
  {
    "text": "I want to separate A and R.\nSo that graphical model shows",
    "start": "2685890",
    "end": "2692190"
  },
  {
    "text": "that the protected attribute\nis only related to the scoring function through the outcome.",
    "start": "2692190",
    "end": "2700150"
  },
  {
    "text": "So there's nothing else that you\ncan learn from one to the other than through the outcome.",
    "start": "2700150",
    "end": "2707530"
  },
  {
    "text": "So this recognizes that\nthe protected attribute may, in fact, be correlated\nwith the target variable.",
    "start": "2707530",
    "end": "2714150"
  },
  {
    "text": " An example might be\ndifferent success rates",
    "start": "2714150",
    "end": "2720119"
  },
  {
    "text": "in a drug trial for\ndifferent ethnic populations. There are now some cardiac\ndrugs where the manufacturer has",
    "start": "2720120",
    "end": "2731940"
  },
  {
    "text": "determined that\nthis drug works much better in certain\nsubpopulations than it does",
    "start": "2731940",
    "end": "2738180"
  },
  {
    "text": "in other populations,\nand the FDA has actually approved the\nmarketing of that drug",
    "start": "2738180",
    "end": "2744420"
  },
  {
    "text": "to those subpopulations. So you're not\nsupposed to market it to the people for whom\nit doesn't work as well,",
    "start": "2744420",
    "end": "2753690"
  },
  {
    "text": "but you're allowed to\nmarket it specifically for the people for\nwhom it does work well.",
    "start": "2753690",
    "end": "2758890"
  },
  {
    "text": "And if you think about\nthe personalized medicine idea, which we've\ntalked about earlier.",
    "start": "2758890",
    "end": "2766230"
  },
  {
    "text": "The populations that\nwe're interested in becomes smaller and smaller\nuntil it may just be you.",
    "start": "2766230",
    "end": "2772770"
  },
  {
    "text": "And so there might be a drug\nthat works for you and not for anybody else in the\nclass, but it's exactly",
    "start": "2772770",
    "end": "2779910"
  },
  {
    "text": "the right drug for you,\nand we may get to the point where that will happen and\nwhere we can build such drugs",
    "start": "2779910",
    "end": "2786870"
  },
  {
    "text": "and where we can approve their\nuse in human populations.",
    "start": "2786870",
    "end": "2792960"
  },
  {
    "text": "Now, the idea here\nis that if I have two populations, blue and green,\nand I draw ROC curves for both",
    "start": "2792960",
    "end": "2804290"
  },
  {
    "text": "of these populations,\nthey're not going to be the same, because\nthe drug will work differently",
    "start": "2804290",
    "end": "2810500"
  },
  {
    "text": "for those two populations. But on the other hand, I can\ndraw them on the same axes,",
    "start": "2810500",
    "end": "2816140"
  },
  {
    "text": "and I can say, look any place\nwithin this colored region",
    "start": "2816140",
    "end": "2821460"
  },
  {
    "text": "can be a fair region\nin that I'm going to get the same outcome\nfor both populations.",
    "start": "2821460",
    "end": "2828130"
  },
  {
    "text": "So I can't achieve this\noutcome for the blue population or this outcome for\nthe green population,",
    "start": "2828130",
    "end": "2835109"
  },
  {
    "text": "but I can achieve any of these\noutcomes for both populations simultaneously.",
    "start": "2835110",
    "end": "2840880"
  },
  {
    "text": "And so that's one way of\ngoing about satisfying this requirement when it\nis not easily satisfied.",
    "start": "2840880",
    "end": "2850200"
  },
  {
    "text": "So the advantage of\nseparation over independence is that it allows\ncorrelation between R",
    "start": "2850200",
    "end": "2855720"
  },
  {
    "text": "and Y, even a perfect\npredictor, so R could be a perfect\npredictor for Y.",
    "start": "2855720",
    "end": "2862320"
  },
  {
    "text": "And it gives you incentives\nto learn to reduce the errors in all groups. So that issue about\nrandomly choosing",
    "start": "2862320",
    "end": "2870450"
  },
  {
    "text": "members of the minority\ngroup doesn't work here because that would suppress\nthe ROC curve to the point",
    "start": "2870450",
    "end": "2877890"
  },
  {
    "text": "where there would be no feasible\nregion that you would like. So for example, if\nit's a coin flip,",
    "start": "2877890",
    "end": "2884430"
  },
  {
    "text": "then you'd have\nthe diagonal line and the only feasible region\nwould be below that diagonal,",
    "start": "2884430",
    "end": "2890760"
  },
  {
    "text": "no matter how good the predictor\nwas for the other class. So that's a nice characteristic.",
    "start": "2890760",
    "end": "2897770"
  },
  {
    "text": "And then the final\ncriterion is sufficiency, which flips R and Y. So\nit says that the regressor",
    "start": "2897770",
    "end": "2906000"
  },
  {
    "text": "or the predictive variable can\ndepend on the protected class, but the protected class is\nseparated from the outcome.",
    "start": "2906000",
    "end": "2914910"
  },
  {
    "text": "So for example, the\nprobability in a binary case",
    "start": "2914910",
    "end": "2920150"
  },
  {
    "text": "of a true outcome\nof Y given that R is some particular value, R\nand A is a particular class,",
    "start": "2920150",
    "end": "2930470"
  },
  {
    "text": "is the same as the probability\nof that same outcome given the same R value, but\nthe different class.",
    "start": "2930470",
    "end": "2940740"
  },
  {
    "text": "So that's related to the\nsort of similar people, similar treatment notion,\nqualitative notion, again.",
    "start": "2940740",
    "end": "2951270"
  },
  {
    "text": " So it requires a parody\nof both the positive",
    "start": "2951270",
    "end": "2957750"
  },
  {
    "text": "and the negative predictive\nvalues across different groups.",
    "start": "2957750",
    "end": "2963230"
  },
  {
    "text": "So that's another popular\nway of looking at this.",
    "start": "2963230",
    "end": "2968310"
  },
  {
    "text": "So for example, if the scoring\nfunction is a probability, or the set of all instances\nassigned the score R has an R",
    "start": "2968310",
    "end": "2976220"
  },
  {
    "text": "fraction of positive\ninstances among them, then the scoring function is\nsaid to be well-calibrated.",
    "start": "2976220",
    "end": "2982550"
  },
  {
    "text": "So we've talked about\nthat before in the class. If it turns out that R\nis not well-calibrated,",
    "start": "2982550",
    "end": "2990150"
  },
  {
    "text": "you can hack it and you can make\nit well-calibrated by putting it through a logistic function\nthat will then approximate",
    "start": "2990150",
    "end": "2998569"
  },
  {
    "text": "the appropriately\ncalibrated score, and then you hope that that\ncalibration will give--",
    "start": "2998570",
    "end": "3007030"
  },
  {
    "text": "or the degree of\ncalibration will give you a good approximation to\nthis notion of sufficiency.",
    "start": "3007030",
    "end": "3014319"
  },
  {
    "text": "These guys in the\ntutorial also point out that some data sets actually\nlead to good calibration",
    "start": "3014320",
    "end": "3024369"
  },
  {
    "text": "without even trying very hard. So for example, this is\nthe UCI census data set,",
    "start": "3024370",
    "end": "3031060"
  },
  {
    "text": "and it's a binary prediction\nof whether somebody makes more than $50,000 a year if\nyou have any income at all",
    "start": "3031060",
    "end": "3038500"
  },
  {
    "text": "and if you're over 16 years old. And the feature, there are 14\nfeatures, age, type of work,",
    "start": "3038500",
    "end": "3045369"
  },
  {
    "text": "weight of sample is\nsome statistical hack from the Census Bureau,\nyour education level,",
    "start": "3045370",
    "end": "3051549"
  },
  {
    "text": "marital status, et\ncetera, and what you see is that the calibration\nfor males and females",
    "start": "3051550",
    "end": "3059230"
  },
  {
    "text": "is pretty decent. It's almost exactly\nalong the 45 degree line",
    "start": "3059230",
    "end": "3064540"
  },
  {
    "text": "without having done anything\nparticularly dramatic in order to achieve that.",
    "start": "3064540",
    "end": "3070000"
  },
  {
    "text": "On the other hand, if you\nlook at the calibration curve by race for whites\nversus blacks,",
    "start": "3070000",
    "end": "3076690"
  },
  {
    "text": "the whites, not surprisingly,\nare reasonably well-calibrated, and the blacks are not\nas well-calibrated.",
    "start": "3076690",
    "end": "3083410"
  },
  {
    "text": "So you could imagine building\nsome kind of a transformation function to improve\nthat calibration,",
    "start": "3083410",
    "end": "3089710"
  },
  {
    "text": "and that would get\nyou separation. Now, there's a\nterrible piece of news,",
    "start": "3089710",
    "end": "3095569"
  },
  {
    "text": "which is that you can prove,\nas they do in this tutorial, that it's not possible\nto jointly achieve",
    "start": "3095570",
    "end": "3103510"
  },
  {
    "text": "any pair of these conditions. So you have three\nreasonable technical notions",
    "start": "3103510",
    "end": "3109660"
  },
  {
    "text": "of what fairness\nmeans, and they're incompatible with each other\nexcept in some trivial cases.",
    "start": "3109660",
    "end": "3117570"
  },
  {
    "text": "This is not good.  And I'm not going to\nhave time to go into it,",
    "start": "3117570",
    "end": "3124599"
  },
  {
    "text": "but there's a very\nnice thing from Google where they illustrate\nthe results of adopting",
    "start": "3124600",
    "end": "3132960"
  },
  {
    "text": "one or another of these\nnotions of fairness on a synthesized\npopulation of people,",
    "start": "3132960",
    "end": "3140609"
  },
  {
    "text": "and you can see how\nthe trade-offs vary and what the results\nare of choosing",
    "start": "3140610",
    "end": "3145650"
  },
  {
    "text": "different notions of fairness. So it's a kind of\nnice graphical hack. Again, it'll be on\nthe slides, and I",
    "start": "3145650",
    "end": "3151980"
  },
  {
    "text": "urge you to check\nthat out, but I'm not going to have time\nto go into it. There is one other problem\nthat they point out",
    "start": "3151980",
    "end": "3159370"
  },
  {
    "text": "which is interesting. So this was a\nscenario where you're",
    "start": "3159370",
    "end": "3165520"
  },
  {
    "text": "trying to hire\ncomputer programmers, and you don't want to take\ngender into account because we",
    "start": "3165520",
    "end": "3172240"
  },
  {
    "text": "know that women are\nunderrepresented among computer people, and so we\nwould like that not",
    "start": "3172240",
    "end": "3178420"
  },
  {
    "text": "to be an allowed\nattribute in order to decide to hire someone. So they say, well,\nthere are two scenarios.",
    "start": "3178420",
    "end": "3186910"
  },
  {
    "text": "One of them is that gender,\nA, influences whether you're",
    "start": "3186910",
    "end": "3192099"
  },
  {
    "text": "a programmer or not. And this is empirically true. There are fewer women\nwho are programmers.",
    "start": "3192100",
    "end": "3199630"
  },
  {
    "text": "It turns out that visiting\nPinterest is slightly more common among women than men.",
    "start": "3199630",
    "end": "3207330"
  },
  {
    "text": "Who knew? And then visiting GitHub is much\nmore common among programmers",
    "start": "3207330",
    "end": "3215430"
  },
  {
    "text": "than among non-programmers. That one's pretty obvious.",
    "start": "3215430",
    "end": "3221590"
  },
  {
    "text": "So what they say is, if you\nwant an optimal predictor",
    "start": "3221590",
    "end": "3227880"
  },
  {
    "text": "of whether somebody's\ngoing to get hired, it should actually take both\nPinterest visits and GitHub",
    "start": "3227880",
    "end": "3234630"
  },
  {
    "text": "visits into account, but\nbecause those go back",
    "start": "3234630",
    "end": "3242789"
  },
  {
    "text": "to gender, which is\nan unusable attribute,",
    "start": "3242790",
    "end": "3249860"
  },
  {
    "text": "they don't like this model. And so they say, well, we could\nuse an optimal separated score,",
    "start": "3249860",
    "end": "3258140"
  },
  {
    "text": "because now, being a programmer\nseparates your gender from the scoring function.",
    "start": "3258140",
    "end": "3265440"
  },
  {
    "text": "And so we can create\na different score which is not the same\nas the optimal score,",
    "start": "3265440",
    "end": "3271490"
  },
  {
    "text": "but is permitted because it's\nno longer dependent on your sex,",
    "start": "3271490",
    "end": "3279380"
  },
  {
    "text": "on your gender. Here's another scenario that,\nagain, starts with gender",
    "start": "3279380",
    "end": "3286140"
  },
  {
    "text": "and says, look, we know that\nthere are more men than women",
    "start": "3286140",
    "end": "3291510"
  },
  {
    "text": "who obtain college degrees\nin computer science, and so there's an\ninfluence there,",
    "start": "3291510",
    "end": "3298200"
  },
  {
    "text": "and computer scientists\nare much more likely to be programmers than\nnon-computer science majors.",
    "start": "3298200",
    "end": "3303885"
  },
  {
    "text": " If you're were a woman--",
    "start": "3303885",
    "end": "3310240"
  },
  {
    "text": "has anybody visited the Grace\nMurray Hopper Conference? A couple, a few of you.",
    "start": "3310240",
    "end": "3315900"
  },
  {
    "text": "So this is a really\ncool conference. Grace Murray Hopper invented\nthe notion bug or the term bug",
    "start": "3315900",
    "end": "3322349"
  },
  {
    "text": "and was a really famous\ncomputer scientist starting back in the 1940s when there\nwere very few of them,",
    "start": "3322350",
    "end": "3329170"
  },
  {
    "text": "and there is a yearly\nconference for women computer scientists in her honor.",
    "start": "3329170",
    "end": "3334500"
  },
  {
    "text": "So clearly, the probability that\nyou visited the Grace Hopper Conference is dependent\non your gender.",
    "start": "3334500",
    "end": "3342359"
  },
  {
    "text": "It's also dependent on whether\nyou're a computer scientist, because if you're\na historian, you're not likely to be interested\nin going to that conference.",
    "start": "3342360",
    "end": "3351869"
  },
  {
    "text": "And so in this story,\nthe optimal score is going to depend basically\non whether you have a computer",
    "start": "3351870",
    "end": "3359850"
  },
  {
    "text": "science degree or not,\nbut the separated score",
    "start": "3359850",
    "end": "3365960"
  },
  {
    "text": "will depend only on\nyour gender, which is kind of funny, because\nthat's the protected attribute.",
    "start": "3365960",
    "end": "3372590"
  },
  {
    "text": "And what these guys point out is\nthat despite the fact that you have these two scenarios,\nit could well turn out",
    "start": "3372590",
    "end": "3381500"
  },
  {
    "text": "that the numerical data, the\nstatistics from which you estimate these models\nare absolutely identical.",
    "start": "3381500",
    "end": "3389850"
  },
  {
    "text": "In other words, the\nsame fraction of people are men and women, the\nsame fraction of people",
    "start": "3389850",
    "end": "3397220"
  },
  {
    "text": "are programmers, they\nhave the same relationship to those other factors, and\nso from a purely observational",
    "start": "3397220",
    "end": "3404780"
  },
  {
    "text": "viewpoint, you can't tell\nwhich of these styles of model",
    "start": "3404780",
    "end": "3411250"
  },
  {
    "text": "is correct or which version of\nfairness your data can support.",
    "start": "3411250",
    "end": "3420640"
  },
  {
    "text": "So that's a problem\nbecause we know that these different\nnotions of fairness",
    "start": "3420640",
    "end": "3426760"
  },
  {
    "text": "are in conflict with each other. So I wanted to finish by showing\nyou a couple of examples.",
    "start": "3426760",
    "end": "3433190"
  },
  {
    "text": "So this was a paper\nbased on Irene's work. So Irene, shout if I'm\nbutchering the discussion.",
    "start": "3433190",
    "end": "3445720"
  },
  {
    "text": "I got an invitation last year\nfrom the American Medical",
    "start": "3445720",
    "end": "3453790"
  },
  {
    "text": "Association's Journal of Ethics,\nwhich I didn't know existed,",
    "start": "3453790",
    "end": "3459040"
  },
  {
    "text": "to write a think piece for\nthem about fairness in machine learning, and I decided that\nrather than just bloviate,",
    "start": "3459040",
    "end": "3468339"
  },
  {
    "text": "I wanted to present\nsome real work, and Irene had been\ndoing some real work.",
    "start": "3468340",
    "end": "3473920"
  },
  {
    "text": "And so Marcia, who was\none of my students, and I convinced her\nto get into this,",
    "start": "3473920",
    "end": "3480890"
  },
  {
    "text": "and we started looking\nat the question of how these machine learning models\ncan identify and perhaps reduce",
    "start": "3480890",
    "end": "3489800"
  },
  {
    "text": "disparities in general\nmedical and mental health.",
    "start": "3489800",
    "end": "3495110"
  },
  {
    "text": "Now, why those two areas? Because we had access\nto data in those areas. So the general medical was\nactually not that general.",
    "start": "3495110",
    "end": "3503000"
  },
  {
    "text": "It's intensive care\ndata from MIMIC, and mental health\ncare is some data that we had access to from Mass\nGeneral and McLean's hospital",
    "start": "3503000",
    "end": "3512119"
  },
  {
    "text": "here in Boston, which both\nhave big psychiatric clinics. ",
    "start": "3512120",
    "end": "3517830"
  },
  {
    "text": "So yeah, this is\nwhat I just said. So the question we were\nasking is, is there",
    "start": "3517830",
    "end": "3524910"
  },
  {
    "text": "bias based on race,\ngender, and insurance type? So we were really interested\nin socioeconomic status,",
    "start": "3524910",
    "end": "3531700"
  },
  {
    "text": "but we didn't have\nthat in the database, but the type of insurance you\nhave correlates pretty well",
    "start": "3531700",
    "end": "3537900"
  },
  {
    "text": "with whether you're\nrich or poor. If you have Medicaid insurance,\nfor example, you're poor,",
    "start": "3537900",
    "end": "3543780"
  },
  {
    "text": "and if you have\nprivate insurance, the first approximation,\nyou're rich. So we did that, and then\nwe looked at the notes.",
    "start": "3543780",
    "end": "3552119"
  },
  {
    "text": "So we wanted to see\nnot the coded data, but whether the things that\nnurses and doctors said",
    "start": "3552120",
    "end": "3559050"
  },
  {
    "text": "about you as you\nwere in the hospital were predictive of readmission,\nof 30-day readmission,",
    "start": "3559050",
    "end": "3567210"
  },
  {
    "text": "of whether you were likely\nto come back to the hospital. So these are some of the topics.",
    "start": "3567210",
    "end": "3572900"
  },
  {
    "text": "We used LDA, standard\ntopic modeling framework.",
    "start": "3572900",
    "end": "3578220"
  },
  {
    "text": "And the topics, as usual,\ninclude some garbage, but also include a lot of\nrecognizably useful topics.",
    "start": "3578220",
    "end": "3586750"
  },
  {
    "text": "So for example, mass,\ncancer, metastatic, clearly associated with\ncancer, Afib, atrial, Coumadin,",
    "start": "3586750",
    "end": "3594089"
  },
  {
    "text": "fibrillation, associated with\nheart function, et cetera,",
    "start": "3594090",
    "end": "3599850"
  },
  {
    "text": "in the ICU domain. In the psychiatric\ndomain, you have things like bipolar, lithium,\nmanic episode, clearly",
    "start": "3599850",
    "end": "3608280"
  },
  {
    "text": "associated with bipolar disease,\npain, chronic, milligrams,",
    "start": "3608280",
    "end": "3613620"
  },
  {
    "text": "the drug quantity, associated\nwith chronic pain, et cetera. So these were the\ntopics that we used.",
    "start": "3613620",
    "end": "3622970"
  },
  {
    "text": "And so we said,\nwhat happens when you look at the\ndifferent topics,",
    "start": "3622970",
    "end": "3632180"
  },
  {
    "text": "how often the\ndifferent topics arise in different subpopulations? And so what we found is that,\nfor example, white patients",
    "start": "3632180",
    "end": "3641330"
  },
  {
    "text": "have more topics that\nare enriched for anxiety and chronic pain, whereas black,\nHispanic, and Asian patients",
    "start": "3641330",
    "end": "3649760"
  },
  {
    "text": "had higher topic\nenrichment for psychosis. ",
    "start": "3649760",
    "end": "3655460"
  },
  {
    "text": "It's interesting. Male patients had more\nsubstance abuse problems.",
    "start": "3655460",
    "end": "3660890"
  },
  {
    "text": "Female patients had\nmore general depression and treatment-resistant\ndepression.",
    "start": "3660890",
    "end": "3666650"
  },
  {
    "text": "So if you want to create a\nstereotype, men are druggies",
    "start": "3666650",
    "end": "3673039"
  },
  {
    "text": "and women are depressed,\naccording to this data.",
    "start": "3673040",
    "end": "3678530"
  },
  {
    "text": "What about insurance type? Well, private insurance\npatients had higher levels",
    "start": "3678530",
    "end": "3684590"
  },
  {
    "text": "of anxiety and depression,\nand poorer patients or public insurance\npatients had more problems",
    "start": "3684590",
    "end": "3691310"
  },
  {
    "text": "with substance abuse. Again, another stereotype\nthat you could form.",
    "start": "3691310",
    "end": "3697069"
  },
  {
    "text": "And then you could look at-- that was in the\npsychiatric population.",
    "start": "3697070",
    "end": "3704420"
  },
  {
    "text": "In the ICU population, men still\nhave substance abuse problems.",
    "start": "3704420",
    "end": "3711770"
  },
  {
    "text": "Women have more\npulmonary disease. And we were\nspeculating on how this",
    "start": "3711770",
    "end": "3717829"
  },
  {
    "text": "relates to sort of known\ndata about underdiagnosis of COPD in women.",
    "start": "3717830",
    "end": "3724400"
  },
  {
    "text": "By race, Asian patients have\na lot of discussion of cancer,",
    "start": "3724400",
    "end": "3729920"
  },
  {
    "text": "black patients have\na lot of discussion of kidney problems,\nHispanics of liver problems,",
    "start": "3729920",
    "end": "3735800"
  },
  {
    "text": "and whites have\natrial fibrillation. So again, stereotypes\nof what's most common",
    "start": "3735800",
    "end": "3742309"
  },
  {
    "text": "in these different groups. ",
    "start": "3742310",
    "end": "3749780"
  },
  {
    "text": "And by insurance type,\nthose with public insurance often have multiple\nchronic conditions.",
    "start": "3749780",
    "end": "3756530"
  },
  {
    "text": "And so public insurance patients\nhave atrial fibrillation, pacemakers, dialysis.",
    "start": "3756530",
    "end": "3763130"
  },
  {
    "text": "These are indications\nof chronic heart disease",
    "start": "3763130",
    "end": "3768950"
  },
  {
    "text": "and chronic kidney disease. And private insurance patients\nhave higher topic enrichment",
    "start": "3768950",
    "end": "3774560"
  },
  {
    "text": "values for fractures. So maybe they're richer,\nthey play more sports and break their\narms or something.",
    "start": "3774560",
    "end": "3782299"
  },
  {
    "text": "Lymphoma and aneurysms.  Just reporting the data.",
    "start": "3782300",
    "end": "3788609"
  },
  {
    "text": "Just the facts. So these results are\nactually consistent with lots of analysis that have been\ndone of this kind of data.",
    "start": "3788610",
    "end": "3797880"
  },
  {
    "text": "Now, what I really\nwanted to look at was this question of, can\nwe get similar error rates,",
    "start": "3797880",
    "end": "3803640"
  },
  {
    "text": "or how similar are the\nerror rates that we get, and the answer is, not so much.",
    "start": "3803640",
    "end": "3809609"
  },
  {
    "text": "So for example, if you\nlook at the ICU data,",
    "start": "3809610",
    "end": "3814710"
  },
  {
    "text": "we find that the error rates\non a zero-one loss metric",
    "start": "3814710",
    "end": "3819830"
  },
  {
    "text": "are much lower for men than they\nare for women, statistically",
    "start": "3819830",
    "end": "3825100"
  },
  {
    "text": "significantly lower. So we're able to more accurately\nmodel male response or male",
    "start": "3825100",
    "end": "3832900"
  },
  {
    "text": "prediction of 30-day\nreadmission than we are-- sorry, of ICU mortality for\nthe ICU than we are for women.",
    "start": "3832900",
    "end": "3842350"
  },
  {
    "text": "Similarly, we have\nmuch tighter ability",
    "start": "3842350",
    "end": "3848680"
  },
  {
    "text": "to predict outcomes for\nprivate insurance patients than for public\ninsurance patients",
    "start": "3848680",
    "end": "3855910"
  },
  {
    "text": "with a huge gap\nin the confidence intervals between them.",
    "start": "3855910",
    "end": "3861290"
  },
  {
    "text": "So this indicates that\nthere is, in fact, a racial bias in the\ndata that we have",
    "start": "3861290",
    "end": "3867640"
  },
  {
    "text": "and in the models\nthat we're building. These are particularly\nsimple models.",
    "start": "3867640",
    "end": "3873579"
  },
  {
    "text": "In psychiatry, when you\nlook at the comparison",
    "start": "3873580",
    "end": "3879250"
  },
  {
    "text": "for different\nethnic populations, you see a fair\namount of overlap.",
    "start": "3879250",
    "end": "3884559"
  },
  {
    "text": "One reason we\nspeculate is that we have a lot less data\nabout psychiatric patients",
    "start": "3884560",
    "end": "3890770"
  },
  {
    "text": "than we do about ICU patients. So the models are\nnot going to give us as accurate predictions.",
    "start": "3890770",
    "end": "3896859"
  },
  {
    "text": "But you still see, for example,\na statistically significant",
    "start": "3896860",
    "end": "3902320"
  },
  {
    "text": "difference between blacks\nand whites and other races,",
    "start": "3902320",
    "end": "3910530"
  },
  {
    "text": "although there's a\nlot of overlap here. Again, between\nmales and females,",
    "start": "3910530",
    "end": "3916400"
  },
  {
    "text": "we get fewer errors in\nmaking predictions for males, but there is not a 95%\nconfidence separation",
    "start": "3916400",
    "end": "3926120"
  },
  {
    "text": "between them. And for private versus\npublic insurance, we do see that separation\nwhere for some reason,",
    "start": "3926120",
    "end": "3933810"
  },
  {
    "text": "in fact, we're able to\nmake better predictions for the people on\nMedicare than we are-- or Medicaid than\nwe are for patients",
    "start": "3933810",
    "end": "3941270"
  },
  {
    "text": "in private insurance. So just to wrap that up, this is\nnot a solution to the problem,",
    "start": "3941270",
    "end": "3949040"
  },
  {
    "text": "but it's an examination\nof the problem. And this Journal of\nEthics considered",
    "start": "3949040",
    "end": "3955940"
  },
  {
    "text": "it interesting enough to publish\njust a couple of months ago.",
    "start": "3955940",
    "end": "3961339"
  },
  {
    "text": "The last thing I\nwant to talk about is some work of\nWillie's, so I'm taking",
    "start": "3961340",
    "end": "3967100"
  },
  {
    "text": "the risk of speaking\nbefore the people who actually did the work here\nand embarrassing myself.",
    "start": "3967100",
    "end": "3974960"
  },
  {
    "text": "So this is modeling mistrust\nin end-of-life care, and it's based on\nWillie's master's thesis",
    "start": "3974960",
    "end": "3982190"
  },
  {
    "text": "and on some papers that\ncame as a result of that. ",
    "start": "3982190",
    "end": "3987800"
  },
  {
    "text": "So here's the interesting data.",
    "start": "3987800",
    "end": "3992960"
  },
  {
    "text": "If you look at\nAfrican-American patients, and these are patients in the\nMIMIC data set, what you find",
    "start": "3992960",
    "end": "4002260"
  },
  {
    "text": "is that for mechanical\nventilation,",
    "start": "4002260",
    "end": "4007720"
  },
  {
    "text": "blacks are on\nmechanical ventilation a lot longer than\nwhites on average,",
    "start": "4007720",
    "end": "4014109"
  },
  {
    "text": "and there's a pretty\ndecent separation at the P equal\n0.05 level, so 1/2%",
    "start": "4014110",
    "end": "4020230"
  },
  {
    "text": "level between those\ntwo populations. So there's something going\non where black patients are",
    "start": "4020230",
    "end": "4027220"
  },
  {
    "text": "kept on mechanical ventilation\nlonger than white patients. Now, of course, we\ndon't know exactly why.",
    "start": "4027220",
    "end": "4034270"
  },
  {
    "text": "We don't know whether\nit's because there is a physiological\ndifference, or because it has something to do\nwith their insurance,",
    "start": "4034270",
    "end": "4041320"
  },
  {
    "text": "or because God knows. It could be any of a lot\nof different factors, but that's the case.",
    "start": "4041320",
    "end": "4047770"
  },
  {
    "text": "The eICU data set\nwe've mentioned, it's a larger, but less\ndetailed data set, also of",
    "start": "4047770",
    "end": "4054430"
  },
  {
    "text": "intensive care patients, that\nwas donated to Roger Marks' Lab by Phillips Corporation.",
    "start": "4054430",
    "end": "4061960"
  },
  {
    "text": "And there, we see,\nagain, a separation of mechanical ventilation\nduration roughly",
    "start": "4061960",
    "end": "4069430"
  },
  {
    "text": "comparable to what we saw\nin the MIMIC data set. So these are consistent\nwith each other.",
    "start": "4069430",
    "end": "4075380"
  },
  {
    "text": "On the other hand, if you look\nat the use of vasopressors, blacks versus whites, at\nthe P equal 0.12 level,",
    "start": "4075380",
    "end": "4084250"
  },
  {
    "text": "you say, well,\nthere's a little bit of evidence, but\nnot strong enough to reach any conclusions.",
    "start": "4084250",
    "end": "4089650"
  },
  {
    "text": "Or in the eICU\ndata, P equal 0.42 is clearly quite\ninsignificant, so we're not",
    "start": "4089650",
    "end": "4097899"
  },
  {
    "text": "making any claims there. So the question that\nWillie was asking, which I think is a\nreally good question, is,",
    "start": "4097899",
    "end": "4107299"
  },
  {
    "text": "could this difference be due\nnot to physiological differences",
    "start": "4107300",
    "end": "4113409"
  },
  {
    "text": "or even these sort of\nsocioeconomic or social differences, but to a difference\nin the degree of trust",
    "start": "4113410",
    "end": "4121299"
  },
  {
    "text": "between the patient\nand their doctors? It's an interesting idea.",
    "start": "4121300",
    "end": "4128009"
  },
  {
    "text": "And of course, I wouldn't\nbe telling you about this if the answer were no.",
    "start": "4128010",
    "end": "4134580"
  },
  {
    "text": "And so the approach\nthat he took was to look for cases where\nthere's clearly mistrust.",
    "start": "4134580",
    "end": "4142259"
  },
  {
    "text": "So there are red flags\nif you read the notes. For example, if a patient\nleaves the hospital",
    "start": "4142260",
    "end": "4149670"
  },
  {
    "text": "against medical advice, that\nis a pretty good indication that they don't trust\nthe medical system.",
    "start": "4149670",
    "end": "4157979"
  },
  {
    "text": "If the family-- if the\nperson dies and the family refuses to allow them\nto do an autopsy,",
    "start": "4157979",
    "end": "4166240"
  },
  {
    "text": "this is another\nindication that maybe they don't trust the medical system. So there are these sort of red\nletter indicators of mistrust.",
    "start": "4166240",
    "end": "4176009"
  },
  {
    "text": "For example, patient\nrefused to sign ICU consent and expressed\nwishes to be do not",
    "start": "4176010",
    "end": "4182790"
  },
  {
    "text": "resuscitate, do not\nintubate, seemingly very frustrated and mistrusting\nof the health care system,",
    "start": "4182790",
    "end": "4189479"
  },
  {
    "text": "also with a history of\npoor medication compliance and follow-up. So that's a pretty\nclear indication.",
    "start": "4189479",
    "end": "4196080"
  },
  {
    "text": "And you can build a\nrelatively simple extraction or interpretation model that\nidentifies those clear cases.",
    "start": "4196080",
    "end": "4206590"
  },
  {
    "text": "This is what I was\nsaying about autopsies. So the problem, of course,\nis that not every patient",
    "start": "4206590",
    "end": "4212890"
  },
  {
    "text": "has such an obvious label. In fact, most of them don't. And so Willie's idea\nwas, can we learn a model",
    "start": "4212890",
    "end": "4221500"
  },
  {
    "text": "from these obvious\nexamples and then apply them to the\nless obvious examples",
    "start": "4221500",
    "end": "4227679"
  },
  {
    "text": "in order to get a kind\nof a bronze standard or remote supervision notion\nof a larger population that",
    "start": "4227680",
    "end": "4237250"
  },
  {
    "text": "has a tendency to be mistrustful\naccording to our model without having as explicit\na clear case of mistrust,",
    "start": "4237250",
    "end": "4246850"
  },
  {
    "text": "as in those examples. And so if you look at chart\nevents in MIMIC, for example,",
    "start": "4246850",
    "end": "4255300"
  },
  {
    "text": "you discover that\nassociated with those cases of obvious mistrust are\nfeatures like the person was",
    "start": "4255300",
    "end": "4264480"
  },
  {
    "text": "in restraints. They were literally\nlocked down to their bed because the nurses\nwere afraid they would",
    "start": "4264480",
    "end": "4271890"
  },
  {
    "text": "get up and do something bad. Not necessarily\nlike attack a nurse,",
    "start": "4271890",
    "end": "4277710"
  },
  {
    "text": "but more like fall out of bed\nor go wandering off the floor",
    "start": "4277710",
    "end": "4282810"
  },
  {
    "text": "or something like that. If a person is in pain, that\ncorrelated with these mistrust",
    "start": "4282810",
    "end": "4289920"
  },
  {
    "text": "measures as well. And conversely, if you saw that\nsomebody had their hair washed",
    "start": "4289920",
    "end": "4296280"
  },
  {
    "text": "or that there was a discussion\nof their status and comfort, then they were probably\nless likely to be",
    "start": "4296280",
    "end": "4303420"
  },
  {
    "text": "mistrustful of the system. And so the approach\nthat Willie took",
    "start": "4303420",
    "end": "4309120"
  },
  {
    "text": "was to say, well, let's code\nthese 620 binary indicators of trust and build a\nlogistic regression",
    "start": "4309120",
    "end": "4317370"
  },
  {
    "text": "model to the labeled\nexamples and then apply it to the unlabeled\nexamples of people for whom",
    "start": "4317370",
    "end": "4324420"
  },
  {
    "text": "we don't have such\na clear indication, and this gives us another\npopulation of people who",
    "start": "4324420",
    "end": "4331380"
  },
  {
    "text": "are likely to be\nmistrustful and therefore, enough people that we can\ndo further analysis on it.",
    "start": "4331380",
    "end": "4339260"
  },
  {
    "text": "So if you look at\nthe mistrust metrics, you have things like if\nthe patient is agitated",
    "start": "4339260",
    "end": "4347050"
  },
  {
    "text": "on some agitation scale, they're\nmore likely to be mistrustful.",
    "start": "4347050",
    "end": "4352690"
  },
  {
    "text": "If, conversely,\nthey're alert, they're less likely to be mistrustful. So that means they're in\nsome better mental shape.",
    "start": "4352690",
    "end": "4360340"
  },
  {
    "text": "If they're not in\npain, they're less likely to be\nmistrustful, et cetera.",
    "start": "4360340",
    "end": "4366050"
  },
  {
    "text": "And if the patient\nwas restrained,",
    "start": "4366050",
    "end": "4371860"
  },
  {
    "text": "then trustful\npatients have no pain, or they have a spokesperson\nwho is their health care proxy,",
    "start": "4371860",
    "end": "4381130"
  },
  {
    "text": "or there is a lot of\nfamily communication, but conversely, if restraints\nhad to be reapplied,",
    "start": "4381130",
    "end": "4389770"
  },
  {
    "text": "or if there are various\nother factors, then",
    "start": "4389770",
    "end": "4395710"
  },
  {
    "text": "they're more likely\nto be mistrustful. So if you look at that\nprediction, what you find",
    "start": "4395710",
    "end": "4407010"
  },
  {
    "text": "is that for both predicting the\nuse of mechanical ventilation and vasopressors, the\ndisparity between a population",
    "start": "4407010",
    "end": "4416040"
  },
  {
    "text": "of black and white\npatients is actually less significant\nthan the disparity",
    "start": "4416040",
    "end": "4423480"
  },
  {
    "text": "between a population of high\ntrust and low trust patients.",
    "start": "4423480",
    "end": "4428920"
  },
  {
    "text": "So what this suggests is that\nthe fundamental feature here that may be leading\nto that difference",
    "start": "4428920",
    "end": "4435480"
  },
  {
    "text": "is, in fact, not\nrace, but is something that correlates with\nrace because blacks",
    "start": "4435480",
    "end": "4441840"
  },
  {
    "text": "are more likely\nto be distrustful of the medical\nsystem than whites. Now, why might that be?",
    "start": "4441840",
    "end": "4447807"
  },
  {
    "text": "What do you know about history?  I mean, you took the\ncity training course",
    "start": "4447808",
    "end": "4455579"
  },
  {
    "text": "that had you read the Belmont\nReport talking about things like the Tuskegee experiment.",
    "start": "4455580",
    "end": "4462570"
  },
  {
    "text": "I'm sure that leaves a\nsignificant impression in people's minds about how\nthe health care system is going",
    "start": "4462570",
    "end": "4470100"
  },
  {
    "text": "to treat people of their race. I'm Jewish. My mother barely lived\nthrough Auschwitz,",
    "start": "4470100",
    "end": "4479070"
  },
  {
    "text": "and so I understand some\nof the strong family feelings that happened\nas a result of some",
    "start": "4479070",
    "end": "4485760"
  },
  {
    "text": "of these historical events. And there were medical people\ndoing experiments on prisoners",
    "start": "4485760",
    "end": "4492600"
  },
  {
    "text": "in the concentration\ncamps as well, so I would expect that\npeople in my status",
    "start": "4492600",
    "end": "4498390"
  },
  {
    "text": "might also have similar\nissues of mistrust.",
    "start": "4498390",
    "end": "4503860"
  },
  {
    "text": "Now, it turns out,\nyou might ask, well, is mistrust, in fact,\njust a proxy for severity?",
    "start": "4503860",
    "end": "4510219"
  },
  {
    "text": "Are sicker people\nsimply more mistrustful, and is what we're seeing\njust a reflection of the fact",
    "start": "4510220",
    "end": "4517390"
  },
  {
    "text": "that they're sicker? And the answer seems\nto be, not so much. So if you look at these severity\nscores like OASIS and SAPS",
    "start": "4517390",
    "end": "4527619"
  },
  {
    "text": "and look at their correlation\nwith noncompliance in autopsy, those are pretty low\ncorrelation values,",
    "start": "4527620",
    "end": "4534370"
  },
  {
    "text": "so they're not explanatory\nof this phenomenon. And then in the population,\nyou see that, again, there",
    "start": "4534370",
    "end": "4543440"
  },
  {
    "text": "is a significant\ndifference in sentiment expressed in the notes between\nblack and white patients.",
    "start": "4543440",
    "end": "4556330"
  },
  {
    "text": "The autopsy derived\nmistrust metrics don't show a strong relationship, a\nstrong difference between them,",
    "start": "4556330",
    "end": "4565070"
  },
  {
    "text": "but the noncompliance\nderived mistrust metrics do.",
    "start": "4565070",
    "end": "4571340"
  },
  {
    "text": "So I'm out of time. I'll just leave you\nwith a final word.",
    "start": "4571340",
    "end": "4577390"
  },
  {
    "text": "There is a lot more work that\nneeds to be done in this area, and it's a very rich area\nboth for technical work",
    "start": "4577390",
    "end": "4587180"
  },
  {
    "text": "and for trying to understand\nwhat the desiderata are and how to match them to\nthe technical capabilities.",
    "start": "4587180",
    "end": "4594500"
  },
  {
    "text": "There are these\nvarious conferences. One of the people\nactive in this area, one",
    "start": "4594500",
    "end": "4601100"
  },
  {
    "text": "of the pairs of people, Mike\nKearns and Aaron Roth at Penn are coming out with a book\ncalled The Ethical Algorithm,",
    "start": "4601100",
    "end": "4609180"
  },
  {
    "text": "which is coming out this fall. It's a popular pressbook. I've not read it,\nbut it looks like it",
    "start": "4609180",
    "end": "4615650"
  },
  {
    "text": "should be quite interesting. And then we're starting\nto see whole classes",
    "start": "4615650",
    "end": "4620720"
  },
  {
    "text": "in fairness popping up at\ndifferent universities. University of Pennsylvania has\nthe science of Data ethics,",
    "start": "4620720",
    "end": "4628460"
  },
  {
    "text": "and I've mentioned already this\nfairness in machine learning class at Berkeley.",
    "start": "4628460",
    "end": "4633510"
  },
  {
    "text": "This is, in fact, one of the\ntopics we've talked about. I'm on a committee\nthat is planning",
    "start": "4633510",
    "end": "4639260"
  },
  {
    "text": "the activities of\nthe new Schwarzman College of Computing,\nand this notion",
    "start": "4639260",
    "end": "4644510"
  },
  {
    "text": "of infusing ideas about\nfairness and ethics into the technical curriculum\nis one of the things",
    "start": "4644510",
    "end": "4651230"
  },
  {
    "text": "that we've been discussing. The college obviously\nhasn't started yet, so we don't have anything\nother than this lecture",
    "start": "4651230",
    "end": "4658700"
  },
  {
    "text": "and a few other things\nlike that in the works, but the plan is there to\nexpand more in this area.",
    "start": "4658700",
    "end": "4665949"
  },
  {
    "start": "4665950",
    "end": "4675000"
  }
]