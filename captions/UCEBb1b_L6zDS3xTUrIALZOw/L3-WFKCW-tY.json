[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11610",
    "end": "18140"
  },
  {
    "text": "at ocw.mit.edu.  GILBERT STRANG:\nOK, so actually, I",
    "start": "18140",
    "end": "26860"
  },
  {
    "text": "know where people are\nworking on projects, and you're not responsible\nfor any new material",
    "start": "26860",
    "end": "34030"
  },
  {
    "text": "in the lectures. Thank you for coming. But I do have something,\nan important topic,",
    "start": "34030",
    "end": "42290"
  },
  {
    "text": "which is a revised version\nabout the construction of neural nets,\nthe basic structure",
    "start": "42290",
    "end": "48910"
  },
  {
    "text": "that we're working with. So that's on the open\nweb at section 7.1,",
    "start": "48910",
    "end": "60310"
  },
  {
    "text": "so Construction of Neural Nets.",
    "start": "60310",
    "end": "68980"
  },
  {
    "text": " Really, it's a construction\nof the learning function, F.",
    "start": "68980",
    "end": "83869"
  },
  {
    "text": "So that's the function\nthat you optimize by gradient descent or\nstochastic gradient descent,",
    "start": "83870",
    "end": "92150"
  },
  {
    "text": "and you apply to the training\ndata to minimize the loss.",
    "start": "92150",
    "end": "100560"
  },
  {
    "text": "So I'm just thinking about\nit in a more organized way, because I wrote that section\nbefore I knew anything",
    "start": "100560",
    "end": "109130"
  },
  {
    "text": "more than how to\nspell neural nets, but now I've thought\nabout it more.",
    "start": "109130",
    "end": "116170"
  },
  {
    "text": "So the key point maybe, compared\nto what I had in the past,",
    "start": "116170",
    "end": "125820"
  },
  {
    "text": "is that I now think of\nthis as a function of two sets of variables, x and\nv. So x are the weights,",
    "start": "125820",
    "end": "141239"
  },
  {
    "text": "and v are the feature vectors,\nthe sample feature vectors.",
    "start": "141240",
    "end": "149775"
  },
  {
    "start": "149775",
    "end": "156489"
  },
  {
    "text": "So those come from the training\ndata, either one at a time,",
    "start": "156490",
    "end": "162030"
  },
  {
    "text": "if we're doing\nstochastic gradient descent with mini-batch size 1.",
    "start": "162030",
    "end": "167160"
  },
  {
    "text": "Or B at a time, if we're\ndoing mini-batch of size B, or the whole thing,\na whole epoch",
    "start": "167160",
    "end": "174190"
  },
  {
    "text": "at once, if we're doing\nfull-scale gradient descent.",
    "start": "174190",
    "end": "179490"
  },
  {
    "text": "So those are the\nfeature vectors, and these are the numbers in\nthe linear steps, the weights.",
    "start": "179490",
    "end": "192670"
  },
  {
    "text": "So they're the matrices AK that\nyou multiply by, multiply v by.",
    "start": "192670",
    "end": "204360"
  },
  {
    "text": "And also the bias vectors\nbK that you add on",
    "start": "204360",
    "end": "214270"
  },
  {
    "text": "to shift the origin. OK.",
    "start": "214270",
    "end": "219640"
  },
  {
    "text": "It's these that you optimize,\nthose are to optimize. ",
    "start": "219640",
    "end": "229010"
  },
  {
    "text": "And what's the structure of the\nwhole of the learning function,",
    "start": "229010",
    "end": "237620"
  },
  {
    "text": "and how do you use it? What does a neural\nnet look like? So you take F of a\nfirst set of weights,",
    "start": "237620",
    "end": "249700"
  },
  {
    "text": "so F of the first set of\nweights would be A1 and B1,",
    "start": "249700",
    "end": "257500"
  },
  {
    "text": "so that's x part. And the actual sample\nvector, the sample vectors",
    "start": "257500",
    "end": "267190"
  },
  {
    "text": "are v0 in the iteration.",
    "start": "267190",
    "end": "272320"
  },
  {
    "text": "And then you do the nonlinear\nstep to each component,",
    "start": "272320",
    "end": "278140"
  },
  {
    "text": "and that produces v1. So there is a typical--",
    "start": "278140",
    "end": "284620"
  },
  {
    "text": "I could write out what this\nis here, A1 v0 plus b1.",
    "start": "284620",
    "end": "291620"
  },
  {
    "text": " The two steps are\nthe linear step.",
    "start": "291620",
    "end": "297950"
  },
  {
    "text": "The endpoint is v0. You take the linear step using\nthe first weights, A1 and b1.",
    "start": "297950",
    "end": "306240"
  },
  {
    "text": "Then, you takes a nonlinear\nstep, and that gives you v1. So that really better\nthan my line above,",
    "start": "306240",
    "end": "314790"
  },
  {
    "text": "so I'll erase that line above. Yeah. ",
    "start": "314790",
    "end": "325340"
  },
  {
    "text": "So that produces v1 from\nv0 and the first weights.",
    "start": "325340",
    "end": "332070"
  },
  {
    "text": "And then the next\nlevel inputs v1, so I'll just call\nthis vK or vK minus 1,",
    "start": "332070",
    "end": "343550"
  },
  {
    "text": "and I'll call this one vK. OK, so K equal to 1 up\nto however many layers,",
    "start": "343550",
    "end": "351320"
  },
  {
    "text": "you are l layers. ",
    "start": "351320",
    "end": "356789"
  },
  {
    "text": "So the input was v0. So this v is really\nv0, you could say.",
    "start": "356790",
    "end": "363420"
  },
  {
    "text": "And this is the neural net, and\nthis is the input and output",
    "start": "363420",
    "end": "372120"
  },
  {
    "text": "from each layer. And then vl is the final\noutput from the final layer.",
    "start": "372120",
    "end": "380050"
  },
  {
    "text": "So let's just do a picture here. Here is v0, a sample\nvector, or if we're",
    "start": "380050",
    "end": "389820"
  },
  {
    "text": "doing image processing, it's\nall the pixels in the data,",
    "start": "389820",
    "end": "397320"
  },
  {
    "text": "in the training. From one sample, this\nis one training sample.",
    "start": "397320",
    "end": "403440"
  },
  {
    "start": "403440",
    "end": "410340"
  },
  {
    "text": "And then you multiply\nby A1, and you add b1.",
    "start": "410340",
    "end": "415470"
  },
  {
    "text": "And you take ReLU of that\nvector, and that gives you v1.",
    "start": "415470",
    "end": "424190"
  },
  {
    "text": "That gives you v1,\nand then you iterate",
    "start": "424190",
    "end": "429480"
  },
  {
    "text": "to finally vl, the last layer. You don't do ReLU\nat the last layer,",
    "start": "429480",
    "end": "436680"
  },
  {
    "text": "so it's just Al vl\nminus 1 plus bl.",
    "start": "436680",
    "end": "443430"
  },
  {
    "text": "And you may not do a bias\nvector also at that layer, but you might, and this\nis the finally the output.",
    "start": "443430",
    "end": "452310"
  },
  {
    "text": " So that picture\nis clearer for me",
    "start": "452310",
    "end": "457800"
  },
  {
    "text": "than it was previously\nto distinguish between the weights.",
    "start": "457800",
    "end": "464370"
  },
  {
    "text": "So in the gradient\ndescent algorithm,",
    "start": "464370",
    "end": "469410"
  },
  {
    "text": "it's these x's that\nyou're choosing. The v's are given by\nthe training data.",
    "start": "469410",
    "end": "475979"
  },
  {
    "text": "That's not part of\nthe optimization part. It's x in chapter\n6, where you're",
    "start": "475980",
    "end": "482850"
  },
  {
    "text": "finding the optimal weights. So this x really stands\nfor all the weights",
    "start": "482850",
    "end": "492980"
  },
  {
    "text": "that you compute up\nto Al, bl, so that's",
    "start": "492980",
    "end": "506370"
  },
  {
    "text": "a collection of all the weights. And the important part for\napplications for practice is",
    "start": "506370",
    "end": "512099"
  },
  {
    "text": "to realize that there are\noften more weights and more components in the weights\nthan there are components",
    "start": "512100",
    "end": "519299"
  },
  {
    "text": "in the feature vectors, in\nthe samples, in the v's. So often, the size of x is\ngreater than the size of v's",
    "start": "519299",
    "end": "528870"
  },
  {
    "text": "which is an interesting and\nsort of unexpected situation.",
    "start": "528870",
    "end": "534720"
  },
  {
    "text": "So often, I'll just write that. Often, the x's are the weights.",
    "start": "534720",
    "end": "543290"
  },
  {
    "text": " x's are underdetermined, because\nthe number of x's exceeds,",
    "start": "543290",
    "end": "563930"
  },
  {
    "text": "and often far exceeds,\nthe number of v's, the number of the cardinality,\nthe number of weights.",
    "start": "563930",
    "end": "572490"
  },
  {
    "text": "This is in the A's\nand b's, and these",
    "start": "572490",
    "end": "579380"
  },
  {
    "text": "are in the samples\nin the training set, the number of features of\nall the samples in the training",
    "start": "579380",
    "end": "597010"
  },
  {
    "text": "set. So I'll get that new\nsection 7.1 up hopefully",
    "start": "597010",
    "end": "605710"
  },
  {
    "text": "this week on the open-- that's the open set-- and\nI'll email to you on Stellar.",
    "start": "605710",
    "end": "615250"
  },
  {
    "text": "Is there more I\nshould say about this? You see here, I can\ndraw the picture,",
    "start": "615250",
    "end": "621250"
  },
  {
    "text": "but of course, a\nhand-drawn picture is far inferior to a\nmachine-drawn picture",
    "start": "621250",
    "end": "630850"
  },
  {
    "text": "an online picture,\nbut let me just do it. So there is v, the training\nsample has some components,",
    "start": "630850",
    "end": "640090"
  },
  {
    "text": "and then they're multiplied. Now, here is going to be v1,\nthe first hidden layer, and that",
    "start": "640090",
    "end": "651970"
  },
  {
    "text": "can have a different number of\ncomponents in the first layer,",
    "start": "651970",
    "end": "662620"
  },
  {
    "text": "a different number of neurons. And then each one\ncomes from the v's--",
    "start": "662620",
    "end": "670959"
  },
  {
    "text": "so I will keep going here,\nbut you see the picture.",
    "start": "670960",
    "end": "676780"
  },
  {
    "text": "So that describes a\nmatrix A1 that tells you what the weights are on\nthose, and then there's",
    "start": "676780",
    "end": "684100"
  },
  {
    "text": "a b1 that's added. The bias vector is added\nto all those to get the v1.",
    "start": "684100",
    "end": "694290"
  },
  {
    "text": "so v1 is A1 v0 plus\nb1, and then onwards.",
    "start": "694290",
    "end": "702220"
  },
  {
    "text": "So this is the spot\nwhere drawing it by hand is clearly inferior to any\nother possible way to do it.",
    "start": "702220",
    "end": "713560"
  },
  {
    "text": "OK. So now, I haven't yet put into\nthe picture the loss function.",
    "start": "713560",
    "end": "723430"
  },
  {
    "text": "So that's the function\nthat you want to minimize. So what is the loss function?",
    "start": "723430",
    "end": "729675"
  },
  {
    "text": " So we're choosing x to--",
    "start": "729675",
    "end": "738740"
  },
  {
    "text": "that's all the A's and b's-- to minimize the\nloss, function L.",
    "start": "738740",
    "end": "747250"
  },
  {
    "text": "OK. So it's this part that Professor\nSra's lecture was about.",
    "start": "747250",
    "end": "754540"
  },
  {
    "text": "So he said, L is often a\nfinite sum over all of F.",
    "start": "754540",
    "end": "765490"
  },
  {
    "text": "So what would that be? F of x, vi, so this is the\noutput from with weights",
    "start": "765490",
    "end": "780210"
  },
  {
    "text": "in x from sample number i. And if we're doing batch\nprocessing-- that is,",
    "start": "780210",
    "end": "786600"
  },
  {
    "text": "we're doing the\nwhole batch at once-- then we compute that for all i. And that's the computation\nthat's ridiculously expensive,",
    "start": "786600",
    "end": "794970"
  },
  {
    "text": "and you go instead to\nstochastic gradient. And you just choose one\nof those, or b of those,",
    "start": "794970",
    "end": "802260"
  },
  {
    "text": "a small number b, like\n32 or 128 of these F's.",
    "start": "802260",
    "end": "807430"
  },
  {
    "text": "But full-scale gradient\ndescent chooses the weights",
    "start": "807430",
    "end": "814290"
  },
  {
    "text": "x to minimize the loss. Now, so I haven't got\nthe loss here yet.",
    "start": "814290",
    "end": "820500"
  },
  {
    "text": "This function, the loss would\nbe minus the true result",
    "start": "820500",
    "end": "829860"
  },
  {
    "text": "from sample i. I haven't got a good\nnotation for that. I'm open to suggestions.",
    "start": "829860",
    "end": "836400"
  },
  {
    "text": "So how do I want\nto write the error?  So that would be--",
    "start": "836400",
    "end": "843330"
  },
  {
    "text": "if it was least squares, I\nwould maybe be squaring that. ",
    "start": "843330",
    "end": "849480"
  },
  {
    "text": "So it would be a sum\nof squares of errors squared over all the samples.",
    "start": "849480",
    "end": "855930"
  },
  {
    "text": "Or if I'm doing stochastic\ngradient descent, I would minimize. I guess I'm minimizing this.",
    "start": "855930",
    "end": "861870"
  },
  {
    "text": "But the question is, do\nI use the whole function L at each iteration, or do\nI just pick one, or only b,",
    "start": "861870",
    "end": "872580"
  },
  {
    "text": "of the samples to look\nat iteration number K?",
    "start": "872580",
    "end": "877830"
  },
  {
    "text": "So this is the L of x then.",
    "start": "877830",
    "end": "883890"
  },
  {
    "text": "I've added up over all the v's. So just to keep the\nnotation straight,",
    "start": "883890",
    "end": "891510"
  },
  {
    "text": "I have this function\nof x and v's. I find it's output. ",
    "start": "891510",
    "end": "898380"
  },
  {
    "text": "This is what the\nneural net produces. It's supposed to be\nclose to the true.",
    "start": "898380",
    "end": "906300"
  },
  {
    "text": "We don't want it to be exactly-- we don't expect this\nto be exactly 0, but it could be, because we have\nlots of weight to achieve that.",
    "start": "906300",
    "end": "915540"
  },
  {
    "text": " So anyway, that would\nbe the loss we minimize,",
    "start": "915540",
    "end": "922020"
  },
  {
    "text": "and it'd be squared\nfor square loss. I guess I haven't really\nspoken about loss functions.",
    "start": "922020",
    "end": "930720"
  },
  {
    "text": "Let me just put those\nhere, and actually these",
    "start": "930720",
    "end": "938519"
  },
  {
    "text": "are popular loss functions. One would be the one we know\nbest, square loss, and number",
    "start": "938520",
    "end": "949890"
  },
  {
    "text": "two, I've never seen it\nused quite this directly, would be the l1 loss,\nmaybe the sum of L1 norms.",
    "start": "949890",
    "end": "963329"
  },
  {
    "text": "This is sum of these errors\nsquared in the L2 norm.",
    "start": "963330",
    "end": "970250"
  },
  {
    "text": "The L1 loss could be the\nsum over i of the L1 losses.",
    "start": "970250",
    "end": "976080"
  },
  {
    "start": "976080",
    "end": "982510"
  },
  {
    "text": "Well, this comes into specific\nother problems like Lasso",
    "start": "982510",
    "end": "988150"
  },
  {
    "text": "and other important problems\nyou're minimizing an L1 norm but not in deep learning.",
    "start": "988150",
    "end": "996550"
  },
  {
    "text": "Now, and three\nwould be Hinge loss. ",
    "start": "996550",
    "end": "1003959"
  },
  {
    "text": "Probably some of you know\nbetter than I the formula and the background\nbehind hinge losses.",
    "start": "1003960",
    "end": "1012390"
  },
  {
    "text": "This is for the minus 1,\n1 classification problems.",
    "start": "1012390",
    "end": "1017565"
  },
  {
    "start": "1017565",
    "end": "1025349"
  },
  {
    "text": "That would be appropriate\nfor regression, so this would be\nfor a regression.",
    "start": "1025349",
    "end": "1031079"
  },
  {
    "text": " And then finally, the most\nimportant for neural nets,",
    "start": "1031079",
    "end": "1038359"
  },
  {
    "text": "is cross-entropy loss. ",
    "start": "1038359",
    "end": "1048099"
  },
  {
    "text": "This is for neural nets. ",
    "start": "1048099",
    "end": "1054799"
  },
  {
    "text": "So this is really the most\nused loss function in the setup",
    "start": "1054800",
    "end": "1062690"
  },
  {
    "text": "that we are mostly\nthinking of, and I'll try to say more about that\nbefore the course ends.",
    "start": "1062690",
    "end": "1071510"
  },
  {
    "text": "So is that-- I don't know. For me, I hadn't got this\nstraight until rewriting",
    "start": "1071510",
    "end": "1080450"
  },
  {
    "text": "that section, and it's\nnow in better form, but comments are welcome.",
    "start": "1080450",
    "end": "1087735"
  },
  {
    "text": "OK.  So that just completes\nwhat I wanted to say,",
    "start": "1087735",
    "end": "1094570"
  },
  {
    "text": "and you'll see the new section. ",
    "start": "1094570",
    "end": "1099750"
  },
  {
    "text": "Any comment on that before I go\nto a different topic entirely?",
    "start": "1099750",
    "end": "1105610"
  },
  {
    "text": "OK. Oh, any questions before\nI go to this topic?",
    "start": "1105610",
    "end": "1111190"
  },
  {
    "text": "Which I'll tell you what it is. ",
    "start": "1111190",
    "end": "1116880"
  },
  {
    "text": "It's a short section in the\nbook about distance matrices,",
    "start": "1116880",
    "end": "1127230"
  },
  {
    "text": "and the question is. ",
    "start": "1127230",
    "end": "1138540"
  },
  {
    "text": "We have a bunch of points\nin space, and what we know",
    "start": "1138540",
    "end": "1147160"
  },
  {
    "text": "is we know the distances\nbetween the points,",
    "start": "1147160",
    "end": "1159700"
  },
  {
    "text": "and it's convenient to talk\nabout distances squared here. ",
    "start": "1159700",
    "end": "1168050"
  },
  {
    "text": "OK. And how would we know\nof these distances? Maybe by radar or\nany measurement.",
    "start": "1168050",
    "end": "1179820"
  },
  {
    "text": "They might be sensors,\nwhich we've placed around,",
    "start": "1179820",
    "end": "1187110"
  },
  {
    "text": "and we can measure the\ndistances between them. And the question is,\nwhat's their position?",
    "start": "1187110",
    "end": "1194190"
  },
  {
    "text": "So that's the question.",
    "start": "1194190",
    "end": "1199450"
  },
  {
    "text": "So let me talk a little\nbit about this question and then pause. Find positions in, well,\nin space, but I don't know.",
    "start": "1199450",
    "end": "1214200"
  },
  {
    "text": "We don't know\nahead of time maybe whether the space is ordinary\n3D space, or whether these",
    "start": "1214200",
    "end": "1220500"
  },
  {
    "text": "are sensors in a plane,\nor whether we have to go to higher dimensions.",
    "start": "1220500",
    "end": "1225610"
  },
  {
    "text": "I'll just put d, and\nalso, I'll just say then, we're also finding d.",
    "start": "1225610",
    "end": "1231820"
  },
  {
    "start": "1231820",
    "end": "1237570"
  },
  {
    "text": "And what are these positions? These are positions x,\nxi, so that the distance",
    "start": "1237570",
    "end": "1244410"
  },
  {
    "text": "between xi minus xj\nsquared is the given dij.",
    "start": "1244410",
    "end": "1253920"
  },
  {
    "text": " So we're given\ndistances between them,",
    "start": "1253920",
    "end": "1259110"
  },
  {
    "text": "and we want to find\ntheir positions. So we know distances, and\nwe want to find positions.",
    "start": "1259110",
    "end": "1265799"
  },
  {
    "text": "That's the question. It's just a neat math\nquestion that is solved,",
    "start": "1265800",
    "end": "1271650"
  },
  {
    "text": "and you'll see a\ncomplete solution.  And it has lots of applications,\nand it's just a nice question.",
    "start": "1271650",
    "end": "1282680"
  },
  {
    "text": "So it occupies a\nsection of the book, but that section is\nonly two pages long.",
    "start": "1282680",
    "end": "1288100"
  },
  {
    "text": "It's just a straightforward\nsolution to that question. Given the distances,\nfind the positions.",
    "start": "1288100",
    "end": "1296900"
  },
  {
    "text": "Given the distances,\nfind the excess. ",
    "start": "1296900",
    "end": "1303610"
  },
  {
    "text": "OK. So I'm going to\nspeak about that. ",
    "start": "1303610",
    "end": "1310350"
  },
  {
    "text": "I had a suggestion, a\ngood suggestion, by email. Well, questions about\nthe projects coming in?",
    "start": "1310350",
    "end": "1318340"
  },
  {
    "text": "Projects are beginning\nto come in, and at least at the beginning--",
    "start": "1318340",
    "end": "1324930"
  },
  {
    "text": "well, in all cases,\nbeginning and end, I'll read them carefully. And as long as I\ncan, I'll send back",
    "start": "1324930",
    "end": "1332290"
  },
  {
    "text": "suggestions for a final\nrewrite, and as I said,",
    "start": "1332290",
    "end": "1339190"
  },
  {
    "text": "a print out is great. You could leave it in the\nenvelope outside my office,",
    "start": "1339190",
    "end": "1344320"
  },
  {
    "text": "but of course, online is\nwhat everybody's doing.",
    "start": "1344320",
    "end": "1350330"
  },
  {
    "text": "So those are just\nbeginning to come in, and if we can get them\nin by a week from today,",
    "start": "1350330",
    "end": "1356230"
  },
  {
    "text": "I'm really, really happy. Yeah, and just feel\nfree to email me.",
    "start": "1356230",
    "end": "1361940"
  },
  {
    "text": "I would email me about projects,\nnot Jonathan and not anonymous",
    "start": "1361940",
    "end": "1369450"
  },
  {
    "text": "Stellar. I think you'd probably do better\njust to ask me the question.",
    "start": "1369450",
    "end": "1375280"
  },
  {
    "text": "That's fine, and I'll try\nto answer in a useful way.",
    "start": "1375280",
    "end": "1380410"
  },
  {
    "text": "Yeah, and I'm always\nopen to questions. So you could email me like how\nlong should this project be?",
    "start": "1380410",
    "end": "1391240"
  },
  {
    "text": "My tutor in Oxford\nsaid something like-- when you were writing essays.",
    "start": "1391240",
    "end": "1399310"
  },
  {
    "text": "That's the Oxford system\nis to write an essay-- and he said, just\nstart where it starts,",
    "start": "1399310",
    "end": "1405070"
  },
  {
    "text": "and end when it finishes. So that's the idea, certainly\nnot enormously long.",
    "start": "1405070",
    "end": "1413230"
  },
  {
    "text": "And then a question was raised--\nand I can ask you if you are",
    "start": "1413230",
    "end": "1419740"
  },
  {
    "text": "interested in that-- the question was, what\ncourses after this one",
    "start": "1419740",
    "end": "1425490"
  },
  {
    "text": "are natural to\ntake to go forward?",
    "start": "1425490",
    "end": "1431200"
  },
  {
    "text": "And I don't know how many of you\nare thinking to take, have time to take, other MIT courses in\nthis area of deep learning,",
    "start": "1431200",
    "end": "1442510"
  },
  {
    "text": "machine learning, optimization,\nall the topics we've had here.",
    "start": "1442510",
    "end": "1448270"
  },
  {
    "text": "Anybody expecting to take more\ncourses, just stick up a hand. Yeah, and you already\nknow like what MIT offers?",
    "start": "1448270",
    "end": "1455539"
  },
  {
    "text": " So that was the question\nthat came to me,",
    "start": "1455540",
    "end": "1461019"
  },
  {
    "text": "what does MIT offer\nin this direction? And I haven't looked up to see\nthe number of Professor Sra's",
    "start": "1461020",
    "end": "1469299"
  },
  {
    "text": "course, S-R-A, in course 6. It's 6 point high number,\nand after his good lecture,",
    "start": "1469300",
    "end": "1477850"
  },
  {
    "text": "I think that's got\nto be worthwhile. So I looked in course 6.",
    "start": "1477850",
    "end": "1484390"
  },
  {
    "text": "I didn't find really\nan institute-wide list.",
    "start": "1484390",
    "end": "1490450"
  },
  {
    "text": "Maybe course 6 feels that\nthey are the Institute, but there are other\ncourses around.",
    "start": "1490450",
    "end": "1495549"
  },
  {
    "start": "1495550",
    "end": "1500570"
  },
  {
    "text": "But I found in the\noperations research site, ORC, the Operations Research\nCenter, let me just put there.",
    "start": "1500570",
    "end": "1511390"
  },
  {
    "text": "This is just in\ncase you would like to think about any\nof these things.",
    "start": "1511390",
    "end": "1517544"
  },
  {
    "start": "1517545",
    "end": "1524280"
  },
  {
    "text": "As I write that, so I heard\nthe lecture by Tim Berners-Lee.",
    "start": "1524280",
    "end": "1529740"
  },
  {
    "text": "Did others hear that\na week or so ago? He created the web.",
    "start": "1529740",
    "end": "1536710"
  },
  {
    "text": "So that's pretty amazing-- it wasn't Al Gore, after all,\nand do you know his name?",
    "start": "1536710",
    "end": "1545580"
  },
  {
    "text": "Well, he's now Sir\nTim Berners-Lee. ",
    "start": "1545580",
    "end": "1553960"
  },
  {
    "text": "So that double name makes you\nsuspect that he's from England, and he is.",
    "start": "1553960",
    "end": "1560520"
  },
  {
    "text": "So anyway, I was\ngoing to say, I hold him responsible for\nthese excessive letters",
    "start": "1560520",
    "end": "1567840"
  },
  {
    "text": "in the address, in the URL.",
    "start": "1567840",
    "end": "1574260"
  },
  {
    "text": "I mean, he's made us\nall say W-W-W for years. Find some other way to say\nit, but it's not easy to say,",
    "start": "1574260",
    "end": "1583650"
  },
  {
    "text": "I think. OK, whatever. This is the OR Center\nat MIT, and then it's",
    "start": "1583650",
    "end": "1596070"
  },
  {
    "text": "academics or something,\nand then it's something",
    "start": "1596070",
    "end": "1602750"
  },
  {
    "text": "like course offerings. That's approximately right. ",
    "start": "1602750",
    "end": "1612080"
  },
  {
    "text": "And since they do\napplied optimization, under the heading of data\nanalytics or statistics,",
    "start": "1612080",
    "end": "1619940"
  },
  {
    "text": "there's optimization, there's\nOR, Operations Research,",
    "start": "1619940",
    "end": "1625789"
  },
  {
    "text": "other lists but a good list of\ncourses from many departments,",
    "start": "1625790",
    "end": "1633020"
  },
  {
    "text": "especially course 6.",
    "start": "1633020",
    "end": "1639200"
  },
  {
    "text": "Course 15 which is where\nthe operation and research center is, course 18,\nand there are others",
    "start": "1639200",
    "end": "1646190"
  },
  {
    "text": "in course 2 and elsewhere. Yeah. ",
    "start": "1646190",
    "end": "1654390"
  },
  {
    "text": "Would somebody like to say\nwhat course you have in mind to take next, after this one?",
    "start": "1654390",
    "end": "1660250"
  },
  {
    "text": "If you looked ahead to next\nyear, any suggestions of what",
    "start": "1660250",
    "end": "1667260"
  },
  {
    "text": "looks like a good course?  I sat in once on 6.036,\nthe really basic course,",
    "start": "1667260",
    "end": "1676289"
  },
  {
    "text": "and you would want to go higher. OK. ",
    "start": "1676290",
    "end": "1684180"
  },
  {
    "text": "Maybe this is just\nto say, I'd be interested to know what you do\nnext, what your experience is,",
    "start": "1684180",
    "end": "1690809"
  },
  {
    "text": "or I'd be happy to give advice. But maybe my general\nadvice is that that's",
    "start": "1690810",
    "end": "1698250"
  },
  {
    "text": "a useful list of courses. ",
    "start": "1698250",
    "end": "1703440"
  },
  {
    "text": "OK? Back to distance matrices.",
    "start": "1703440",
    "end": "1709000"
  },
  {
    "text": "OK, so here's the problem. Yeah.",
    "start": "1709000",
    "end": "1714039"
  },
  {
    "text": "OK, I'll probably\nhave to erase that, but I'll leave it for a minute.",
    "start": "1714040",
    "end": "1720080"
  },
  {
    "text": "OK. So we know these distances,\nand we want to find the x's, so",
    "start": "1720080",
    "end": "1726550"
  },
  {
    "text": "let's call this dij maybe. ",
    "start": "1726550",
    "end": "1733240"
  },
  {
    "text": "So we have a D matrix, and we\nwant to find a position matrix, let me just see what notation.",
    "start": "1733240",
    "end": "1740060"
  },
  {
    "text": "So this is section 3.9, no 4.9,\npreviously 3.9, but chapters 3",
    "start": "1740060",
    "end": "1750460"
  },
  {
    "text": "and 4 got switched. Maybe actually, yeah, I\nthink it's 8 or 9 or 10,",
    "start": "1750460",
    "end": "1759250"
  },
  {
    "text": "other topics are trying\nto find their way in.",
    "start": "1759250",
    "end": "1764640"
  },
  {
    "text": "OK. So that's the\nreference on the web, and I'll get these\nsections onto Stellar.",
    "start": "1764640",
    "end": "1772480"
  },
  {
    "text": "OK. So the question is, can\nwe recover the positions",
    "start": "1772480",
    "end": "1777760"
  },
  {
    "text": "from the distances? In fact, there's\nalso a question, are there always positions\nfrom given distances?",
    "start": "1777760",
    "end": "1788400"
  },
  {
    "text": "And I mentioned\nseveral applications. I've already spoken about\nwireless sensor networks, where",
    "start": "1788400",
    "end": "1796809"
  },
  {
    "text": "you can measure travel\ntimes between them, between the sensors.",
    "start": "1796810",
    "end": "1802120"
  },
  {
    "text": "And then that gives\nyou the distances, and then you use this\nneat little bit of math",
    "start": "1802120",
    "end": "1809559"
  },
  {
    "text": "to find the positions. Well, of course, you can't\nfind the positions uniquely.",
    "start": "1809560",
    "end": "1817270"
  },
  {
    "text": "Clearly, you could any rigid\nmotion of all the positions.",
    "start": "1817270",
    "end": "1823780"
  },
  {
    "text": "If I have a set\nof positions, what am I going to call that, x?",
    "start": "1823780",
    "end": "1830410"
  },
  {
    "text": "So I'll write here, and\nso I'm given the D matrix. ",
    "start": "1830410",
    "end": "1840440"
  },
  {
    "text": "That's distances, and the job\nis to find the X matrix which",
    "start": "1840440",
    "end": "1849450"
  },
  {
    "text": "gives the positions.  And what I'm just going\nto say, and you already",
    "start": "1849450",
    "end": "1857780"
  },
  {
    "text": "saw it your mind-- that if\nI have a set of positions, I could do a translation.",
    "start": "1857780",
    "end": "1865880"
  },
  {
    "text": "The distances\nwouldn't change, or I could do a rigid motion,\na rigid rotation.",
    "start": "1865880",
    "end": "1873980"
  },
  {
    "text": "So positions are not unique,\nbut I can come closer by saying,",
    "start": "1873980",
    "end": "1881840"
  },
  {
    "text": "put the centroid at the\norigin, or something like that. That will take out the\ntranslations at least.",
    "start": "1881840",
    "end": "1889680"
  },
  {
    "text": "OK. So find the X matrix. That's the job. OK, and I was going to--\nbefore I started on that--",
    "start": "1889680",
    "end": "1896390"
  },
  {
    "text": "the shapes of molecules\nis another application. Nuclear magnetic resonance\ngives distances, gives d,",
    "start": "1896390",
    "end": "1905260"
  },
  {
    "text": "and then we find\nthe positions x. And of course, there's\na noise in there,",
    "start": "1905260",
    "end": "1911770"
  },
  {
    "text": "and sometimes missing entries. And machine learning\ncould be just described",
    "start": "1911770",
    "end": "1918100"
  },
  {
    "text": "also as you're given\na whole lot of points in space, feature vectors\nin a high-dimensional space.",
    "start": "1918100",
    "end": "1924760"
  },
  {
    "text": "Actually, this is a big deal. You're given a\nwhole lot of points with in high-dimensional\nspace, and those are related.",
    "start": "1924760",
    "end": "1937210"
  },
  {
    "text": "They sort of come\ntogether naturally, so they tend to fit on a surface\nin high-dimensional space,",
    "start": "1937210",
    "end": "1945620"
  },
  {
    "text": "a low-dimensional surface\nin high-dimensional space. And really, a lot\nof mathematics is",
    "start": "1945620",
    "end": "1951290"
  },
  {
    "text": "devoted to finding that\nlow-dimensional, that subspace,",
    "start": "1951290",
    "end": "1956720"
  },
  {
    "text": "but it could be curved. So subspace is not\nthe correct word.",
    "start": "1956720",
    "end": "1962630"
  },
  {
    "text": "Really, manifold,\ncurved manifold is what a geometer would say.",
    "start": "1962630",
    "end": "1969230"
  },
  {
    "text": "That is close to all the-- it's smooth and close\nto all the points,",
    "start": "1969230",
    "end": "1975110"
  },
  {
    "text": "and you could linearize it. You could flatten\nit out, and then you",
    "start": "1975110",
    "end": "1981830"
  },
  {
    "text": "have a much reduced problem. The dimension is reduced\nfrom the original dimension",
    "start": "1981830",
    "end": "1987320"
  },
  {
    "text": "of where the points\nlie with a lot of data",
    "start": "1987320",
    "end": "1993049"
  },
  {
    "text": "to the true dimension of the\nproblem which, of course, sets of points were\nall on a straight line.",
    "start": "1993050",
    "end": "1999290"
  },
  {
    "text": "The true dimension of\nthe problem would be 1. So we have to discover this.",
    "start": "1999290",
    "end": "2005065"
  },
  {
    "text": " We also have to find\nthat dimension d.",
    "start": "2005065",
    "end": "2013520"
  },
  {
    "text": "OK, so how do we do it?  So it's a classical problem.",
    "start": "2013520",
    "end": "2020420"
  },
  {
    "text": "It just has a neat answer. OK. ",
    "start": "2020420",
    "end": "2026870"
  },
  {
    "text": "All right, so let's\nrecognize the connection",
    "start": "2026870",
    "end": "2033170"
  },
  {
    "text": "between distances and positions. So dij is the square\ndistance between them,",
    "start": "2033170",
    "end": "2044090"
  },
  {
    "text": "so that is xi dot xi minus xi\nto xj minus xj, xi plus xj, xj.",
    "start": "2044090",
    "end": "2059648"
  },
  {
    "text": " OK. ",
    "start": "2059649",
    "end": "2066210"
  },
  {
    "text": "Is that right? Yes. OK.",
    "start": "2066210",
    "end": "2073600"
  },
  {
    "text": "So those are the\ndij's in a matrix, and these are entries\nin the matrix D. OK.",
    "start": "2073600",
    "end": "2085020"
  },
  {
    "text": " Well, these entries\ndepend only on i.",
    "start": "2085020",
    "end": "2097560"
  },
  {
    "text": "They're the same for every j. So this is going to be-- this\nwill this part will produce--",
    "start": "2097560",
    "end": "2104200"
  },
  {
    "text": "I'll rank one matrix. ",
    "start": "2104200",
    "end": "2109690"
  },
  {
    "text": "Because things depend not\nonly on the row but not on j, the column number,\nso columns repeated.",
    "start": "2109690",
    "end": "2120565"
  },
  {
    "start": "2120565",
    "end": "2127970"
  },
  {
    "text": "Yeah, and this\nproduces similarly",
    "start": "2127970",
    "end": "2134420"
  },
  {
    "text": "something that depends only on\nj, only on the column number.",
    "start": "2134420",
    "end": "2139549"
  },
  {
    "text": "So the rows are all the\nsame, so this is also a rank one matrix with all\nrepeated, all the same rows.",
    "start": "2139550",
    "end": "2155730"
  },
  {
    "text": "Because if I change i,\nnothing changes in a product.",
    "start": "2155730",
    "end": "2161180"
  },
  {
    "text": "So really, these\nare the terms that",
    "start": "2161180",
    "end": "2166630"
  },
  {
    "text": "produce most of the matrix, the\nsignificant part of the matrix.",
    "start": "2166630",
    "end": "2173900"
  },
  {
    "text": "OK. So what do we do with those?",
    "start": "2173900",
    "end": "2180440"
  },
  {
    "start": "2180440",
    "end": "2186710"
  },
  {
    "text": "So let's see, did I give\na name for the matrix that I'm looking for? I think in the notes I call\nit X. So I'm given D, find X.",
    "start": "2186710",
    "end": "2209930"
  },
  {
    "text": "And what I'll actually find-- you can see it coming here-- is actually find X transpose X.\nBecause what I'm given is dot",
    "start": "2209930",
    "end": "2226490"
  },
  {
    "text": "products of X's. ",
    "start": "2226490",
    "end": "2231530"
  },
  {
    "text": "So I would like to\ndiscover out of all this",
    "start": "2231530",
    "end": "2237430"
  },
  {
    "text": "what xi dotted with xj is. That'll be the\ncorrect dot product.",
    "start": "2237430",
    "end": "2243380"
  },
  {
    "text": "Let's call this matrix G\nfor the dot product matrix,",
    "start": "2243380",
    "end": "2249470"
  },
  {
    "text": "and then find X from G.",
    "start": "2249470",
    "end": "2260780"
  },
  {
    "text": "So this is a nice argument. So what this tells me is some\ninformation about dot products.",
    "start": "2260780",
    "end": "2270230"
  },
  {
    "text": "So this is telling me something\nabout the G matrix, the X transpose X matrix.",
    "start": "2270230",
    "end": "2276110"
  },
  {
    "text": "And then once I know G, then\nit's a separate step to find X.",
    "start": "2276110",
    "end": "2281390"
  },
  {
    "text": "And of course, this is the\npoint at which X is not unique.",
    "start": "2281390",
    "end": "2286640"
  },
  {
    "text": "If I put it in a rotation\ninto X, then that rotation q,",
    "start": "2286640",
    "end": "2291799"
  },
  {
    "text": "I'll see a q transpose\nq, and it'll disappear. So I'm free to rotate\nthe X's, because that",
    "start": "2291800",
    "end": "2299720"
  },
  {
    "text": "doesn't change the dot product. So it's G that I want to know,\nand this tells me something",
    "start": "2299720",
    "end": "2305990"
  },
  {
    "text": "about G, and this tells\nme something about G.",
    "start": "2305990",
    "end": "2311252"
  },
  {
    "text": "And so does that, but\nthat's what I have to see.",
    "start": "2311252",
    "end": "2316480"
  },
  {
    "text": "So what do those tell me? Let's see. Let me write down\nwhat I have here.",
    "start": "2316480",
    "end": "2323560"
  },
  {
    "start": "2323560",
    "end": "2330060"
  },
  {
    "text": "So let's say a diagonal matrix\nwith Dii as the inner product",
    "start": "2330060",
    "end": "2340940"
  },
  {
    "text": "xi with xi that we're getting\npartial information from here.",
    "start": "2340940",
    "end": "2350119"
  },
  {
    "text": "So is that OK? I'm introducing that\nnotation, because this is now",
    "start": "2350120",
    "end": "2355630"
  },
  {
    "text": "going to tell me\nthat my D matrix is-- so what is that?",
    "start": "2355630",
    "end": "2362390"
  },
  {
    "text": "So this is the diagonal matrix. Maybe it's just a\nvector, I should say.",
    "start": "2362390",
    "end": "2367703"
  },
  {
    "text": " Yeah. ",
    "start": "2367704",
    "end": "2379160"
  },
  {
    "text": "Yeah, so can I write down the\nequation that is fundamental",
    "start": "2379160",
    "end": "2385220"
  },
  {
    "text": "here, and then we'll\nfigure out what it means. So it's an equation for G,\nfor the dot product matrix.",
    "start": "2385220",
    "end": "2398600"
  },
  {
    "text": "OK, let me make space\nfor that equation. ",
    "start": "2398600",
    "end": "2405410"
  },
  {
    "text": "I believe that we can\nget the dot product matrix which I'm calling G as-- ",
    "start": "2405410",
    "end": "2414380"
  },
  {
    "text": "according to this, it's\nminus 1/2 of the D matrix",
    "start": "2414380",
    "end": "2420680"
  },
  {
    "text": "plus 1/2 of the 1's times\nthe d, the diagonal d.",
    "start": "2420680",
    "end": "2433069"
  },
  {
    "text": "And it's plus 1/2 of\nthe d times the 1's.",
    "start": "2433070",
    "end": "2443410"
  },
  {
    "start": "2443410",
    "end": "2448460"
  },
  {
    "text": "That's a matrix\nwith constant rows. ",
    "start": "2448460",
    "end": "2455390"
  },
  {
    "text": "This here is coming from there. This is a matrix with always\nthe same columns, or let me see.",
    "start": "2455390",
    "end": "2471020"
  },
  {
    "text": "No, I haven't got\nthose right yet. I mean, I want these to be rank\n1 matrices, so it's this one.",
    "start": "2471020",
    "end": "2481100"
  },
  {
    "text": "Let me fix that. ",
    "start": "2481100",
    "end": "2492780"
  },
  {
    "text": "1, 1, 1, 1 times d transpose,\nso it's column times row,",
    "start": "2492780",
    "end": "2500090"
  },
  {
    "text": "and this one is also column\ntimes row with the d here.",
    "start": "2500090",
    "end": "2509720"
  },
  {
    "text": " OK, now let me look\nat that properly.",
    "start": "2509720",
    "end": "2516150"
  },
  {
    "text": "So every row in this guy is\na multiple of 1, 1, 1, 1.",
    "start": "2516150",
    "end": "2523349"
  },
  {
    "text": "So what is that telling me? That all columns are\nthe same, this part",
    "start": "2523350",
    "end": "2529920"
  },
  {
    "text": "is reflecting these ones,\nwhere the columns are repeated.",
    "start": "2529920",
    "end": "2535859"
  },
  {
    "text": "This one is reflecting this,\nwhere the rows are repeated. The d is just the\nset of d numbers.",
    "start": "2535860",
    "end": "2542100"
  },
  {
    "text": "Let's call that di, and this\nis dj, and here's the D matrix.",
    "start": "2542100",
    "end": "2554455"
  },
  {
    "start": "2554455",
    "end": "2561090"
  },
  {
    "text": "So part of the D matrix\nis this bit and this bit,",
    "start": "2561090",
    "end": "2566760"
  },
  {
    "text": "each giving a rank 1. Now, it's this part that\nI have to understand,",
    "start": "2566760",
    "end": "2572970"
  },
  {
    "text": "so while you're\nchecking on that, let me look again at this.",
    "start": "2572970",
    "end": "2582089"
  },
  {
    "start": "2582090",
    "end": "2587590"
  },
  {
    "text": "Yeah. ",
    "start": "2587590",
    "end": "2593590"
  },
  {
    "text": "Let's just see where\nwe are if this is true. If this is true, I'm\ngiven the D matrix,",
    "start": "2593590",
    "end": "2600589"
  },
  {
    "text": "and then these dot\nproducts I can find.",
    "start": "2600590",
    "end": "2608810"
  },
  {
    "text": "So I can find these,\nso in other words, this is the key\nequation that tells me",
    "start": "2608810",
    "end": "2614164"
  },
  {
    "text": "D. That's the key\nequation, and it's going to come just from\nthat simple identity,",
    "start": "2614164",
    "end": "2623299"
  },
  {
    "text": "just from checking each term. This term we identified,\nthat last term we identified, and now this term\nis D. Well, of, course it's D.",
    "start": "2623300",
    "end": "2634860"
  },
  {
    "text": "So I have two of\nthose, and I'm going to take half of that\nto get D, I think.",
    "start": "2634860",
    "end": "2643460"
  },
  {
    "start": "2643460",
    "end": "2650150"
  },
  {
    "text": "Yeah, and we'll look. ",
    "start": "2650150",
    "end": "2655900"
  },
  {
    "text": "Yeah. ",
    "start": "2655900",
    "end": "2662339"
  },
  {
    "text": "So I guess I'm not seeing right\naway why this 1/2 is in here,",
    "start": "2662340",
    "end": "2669300"
  },
  {
    "text": "but I think I had it right,\nand there's a reason. You see that this matrix\nthis, X transpose X matrix,",
    "start": "2669300",
    "end": "2677010"
  },
  {
    "text": "is coming from these rank 1\npieces and these pieces which are the cross product.",
    "start": "2677010",
    "end": "2687430"
  },
  {
    "text": "Oh, I see. I see. What that equation\nis really saying",
    "start": "2687430",
    "end": "2693060"
  },
  {
    "text": "is that the D matrix is this-- ",
    "start": "2693060",
    "end": "2699340"
  },
  {
    "text": "if I just read that along\nand translate it and put it in matrix language-- is this 1,\n1, 1, 1, d1 to d4, let's say,",
    "start": "2699340",
    "end": "2714070"
  },
  {
    "text": "transpose is this rank 1 matrix. And the other one is\nthe d's times the 1, 1",
    "start": "2714070",
    "end": "2724900"
  },
  {
    "text": "which is a transpose of that. And then the other\none was a minus 2",
    "start": "2724900",
    "end": "2730650"
  },
  {
    "text": "of the cross product matrices. I see. Yeah. So when I write that\nequation in matrix language,",
    "start": "2730650",
    "end": "2738520"
  },
  {
    "text": "I just get that.  And now, when I solve for X--",
    "start": "2738520",
    "end": "2744611"
  },
  {
    "text": "oh, minus 2 X transpose X. Yeah.",
    "start": "2744611",
    "end": "2753200"
  },
  {
    "text": "Sorry, cross products, the X's. So I had one set\nof cross products,",
    "start": "2753200",
    "end": "2759910"
  },
  {
    "text": "and then this is the same\nas this, so I have minus 2 of them. So now, I'm just rewriting that.",
    "start": "2759910",
    "end": "2766420"
  },
  {
    "text": "When I rewrite that\nequation, I have that. Do you see that? I put that on this side.",
    "start": "2766420",
    "end": "2772359"
  },
  {
    "text": "I put the d over\nhere as a minus d. I divide by 2, and then\nthat's the formula.",
    "start": "2772360",
    "end": "2780250"
  },
  {
    "text": "So ultimately, this\nsimple identity",
    "start": "2780250",
    "end": "2786240"
  },
  {
    "text": "just looked at-- because\nthese pieces were so simple, just rank one pieces,\nand these pieces",
    "start": "2786240",
    "end": "2793860"
  },
  {
    "text": "were exactly what we want, the\nX transpose X pieces, the G.",
    "start": "2793860",
    "end": "2799500"
  },
  {
    "text": "That equation told us\nthe D. All this is known.",
    "start": "2799500",
    "end": "2805110"
  },
  {
    "text": " Well, so what's known\nis D and this and this.",
    "start": "2805110",
    "end": "2813260"
  },
  {
    "text": "So now, we have the equation for\nX transpose X is minus 1/2 of D",
    "start": "2813260",
    "end": "2821310"
  },
  {
    "text": "minus these rank 1's. ",
    "start": "2821310",
    "end": "2831530"
  },
  {
    "text": "Sorry to make it look messy. I remember Raj Rao talking\nabout it last spring,",
    "start": "2831530",
    "end": "2839060"
  },
  {
    "text": "also the algebra got flustered.",
    "start": "2839060",
    "end": "2845330"
  },
  {
    "text": "So we get it. So we know X transpose\nX, that matrix.",
    "start": "2845330",
    "end": "2851450"
  },
  {
    "text": "Now, can we just do four\nminutes of linear algebra at the end today?",
    "start": "2851450",
    "end": "2858620"
  },
  {
    "text": " Given X transpose X,\nfind X. This is n by n.",
    "start": "2858620",
    "end": "2871170"
  },
  {
    "start": "2871170",
    "end": "2876589"
  },
  {
    "text": "How would you do that? Could you do it? Would there be just one X?",
    "start": "2876590",
    "end": "2883190"
  },
  {
    "text": "No. So if you had one X,\nmultiply that by a rotation,",
    "start": "2883190",
    "end": "2891380"
  },
  {
    "text": "by an orthogonal matrix,\nyou'd have another one. So this is finding X up to\nan orthogonal transformation,",
    "start": "2891380",
    "end": "2899690"
  },
  {
    "text": "but how would you\nactually do that? What do we know about this\nmatrix, X transpose X?",
    "start": "2899690",
    "end": "2907025"
  },
  {
    "text": "It's symmetric, clearly,\nand what we especially know is that it is also?",
    "start": "2907025",
    "end": "2912470"
  },
  {
    "text": "AUDIENCE: Positive. GILBERT STRANG: Positive\nor semidefinite, so this is semidefinite.",
    "start": "2912470",
    "end": "2918890"
  },
  {
    "text": " So I'm given a\nsemidefinite matrix,",
    "start": "2918890",
    "end": "2924260"
  },
  {
    "text": "and I want to find a\nsquare root, you could say. That matrix is\nthe X transpose X,",
    "start": "2924260",
    "end": "2930890"
  },
  {
    "text": "and I want to find X. I\nthink there are two leading candidates.",
    "start": "2930890",
    "end": "2936920"
  },
  {
    "text": "There are many candidates,\nbecause if you find one, then any QX is OK.",
    "start": "2936920",
    "end": "2949730"
  },
  {
    "text": "Because if I put a Q transpose\nQ in there, it's the identity. OK. So one way is to use\neigenvalues of X transpose X,",
    "start": "2949730",
    "end": "2964579"
  },
  {
    "text": "and the other way would be to\nuse elimination on X transpose",
    "start": "2964580",
    "end": "2971320"
  },
  {
    "text": "X. So I'll put use.",
    "start": "2971320",
    "end": "2977280"
  },
  {
    "text": "So if I use\neigenvalues of X, if I find the eigenvalues\nof X transpose X,",
    "start": "2977280",
    "end": "2983500"
  },
  {
    "text": "then I'm writing this a-- it's a symmetric,\npositive definition-- I'm writing it as Q\nlambda Q transpose.",
    "start": "2983500",
    "end": "2991410"
  },
  {
    "text": "Right? That's the fundamental\nmost important theorem in linear algebra,\nyou could say.",
    "start": "2991410",
    "end": "2997130"
  },
  {
    "text": "That a symmetric, positive,\nsemidefinite matrix has greater eigenvalues,\ngreater or equal to 0,",
    "start": "2997130",
    "end": "3006339"
  },
  {
    "text": "and eigenvectors\nthat are orthogonal. So now, if I know\nthat, what's a good X?",
    "start": "3006340",
    "end": "3012579"
  },
  {
    "text": "Then, take X to be what? ",
    "start": "3012580",
    "end": "3020300"
  },
  {
    "text": "So I've got the eigenvalues and\neigenvectors of X transpose X, and I'm looking for\nan X that will work.",
    "start": "3020300",
    "end": "3027589"
  },
  {
    "text": "And one idea is just to\ntake the same eigenvectors,",
    "start": "3027590",
    "end": "3033800"
  },
  {
    "text": "and take the square\nroots of the eigenvalues. ",
    "start": "3033800",
    "end": "3040660"
  },
  {
    "text": "That's symmetric now. This is equal to X\ntranspose, and that's",
    "start": "3040660",
    "end": "3051859"
  },
  {
    "text": "a square root symbol, or a\nlambda to the 1/2, I could say.",
    "start": "3051860",
    "end": "3057150"
  },
  {
    "text": "So when I multiply that-- X transpose X is\njust X squared here.",
    "start": "3057150",
    "end": "3063500"
  },
  {
    "text": "When I square it,\nthe Q transpose Q multiplies itself to\ngive the identity.",
    "start": "3063500",
    "end": "3070700"
  },
  {
    "text": "The square root of lambda times\nthe square root of lambda, those are diagonal\nmatrices that give lambda,",
    "start": "3070700",
    "end": "3077750"
  },
  {
    "text": "and I get the right answer. So one way is, in\na few words, take",
    "start": "3077750",
    "end": "3083510"
  },
  {
    "text": "the square roots\nof the eigenvalues and keep the eigenvectors. So that's the\neigenvalue construction.",
    "start": "3083510",
    "end": "3090440"
  },
  {
    "text": "So that's producing an\nX that is symmetric, positive, semidefinite.",
    "start": "3090440",
    "end": "3096650"
  },
  {
    "text": "That might be what you want. It's a little work, because\nyour computing eigenvalues",
    "start": "3096650",
    "end": "3102260"
  },
  {
    "text": "and eigenvectors to do\nit, but that's one choice. Now, I believe that elimination\nwould give us another choice.",
    "start": "3102260",
    "end": "3111200"
  },
  {
    "text": "So elimination produces\nwhat factorization of this? This is still our symmetric,\npositive, definite matrix.",
    "start": "3111200",
    "end": "3120020"
  },
  {
    "text": "If you do elimination\non that, you usually expect L, a lower triangular,\ntimes D, the pivots, times U,",
    "start": "3120020",
    "end": "3132980"
  },
  {
    "text": "the upper triangle. That's the usual result\nof elimination, LDU.",
    "start": "3132980",
    "end": "3138620"
  },
  {
    "text": "I'm factoring out the\npivots, so they're 1's on the diagonals\nof L and U. But now,",
    "start": "3138620",
    "end": "3145460"
  },
  {
    "text": "if it's a symmetric\nmatrix, what's up? We zipped by\nelimination, regarding",
    "start": "3145460",
    "end": "3152830"
  },
  {
    "text": "that as a 18.06 trivial\nbit of linear algebra,",
    "start": "3152830",
    "end": "3160120"
  },
  {
    "text": "but of course, it's\nhighly important. So what's the situation here\nwhen the matrix is actually",
    "start": "3160120",
    "end": "3167290"
  },
  {
    "text": "symmetric?  So I want something\nto look symmetric.",
    "start": "3167290",
    "end": "3173810"
  },
  {
    "text": "How do I make that\nlook symmetric? The U gets replaced\nby L transpose.",
    "start": "3173810",
    "end": "3179059"
  },
  {
    "text": " If I'm working on a\npositive definite--",
    "start": "3179060",
    "end": "3185920"
  },
  {
    "text": "say positive definite matrix-- then I get positive pivots,\nand L and lower triangular",
    "start": "3185920",
    "end": "3195020"
  },
  {
    "text": "and upper triangular are\ntransposes of each other. So now, what is then the X?",
    "start": "3195020",
    "end": "3200960"
  },
  {
    "text": " It's just like that. I'll use L square root\nof the D L transpose.",
    "start": "3200960",
    "end": "3211990"
  },
  {
    "text": "Is that right? Oh, wait a minute.",
    "start": "3211990",
    "end": "3217260"
  },
  {
    "text": "What's up? No, that's not going\nto work, because I don't have L transpose L.\nWhere I had Q transpose Q,",
    "start": "3217260",
    "end": "3225310"
  },
  {
    "text": "it was good. No, sorry. Let's get that totally erased.",
    "start": "3225310",
    "end": "3232400"
  },
  {
    "text": "The X part should just be the\nsquare root of DL transpose.",
    "start": "3232400",
    "end": "3237559"
  },
  {
    "text": " The X is now a\ntriangular matrix,",
    "start": "3237560",
    "end": "3242840"
  },
  {
    "text": "the square root of the pivots,\nand the L transpose part. And now, when I\ndo X transpose X,",
    "start": "3242840",
    "end": "3250220"
  },
  {
    "text": "then you see X transpose\nX coming correctly. X transpose will be L transpose.",
    "start": "3250220",
    "end": "3258950"
  },
  {
    "text": "Transpose will give me\nthe L. Square root of D will be square root of\nD. We'll give the D,",
    "start": "3258950",
    "end": "3264079"
  },
  {
    "text": "and then the L\ntranspose is right. So this is called the-- ",
    "start": "3264080",
    "end": "3270290"
  },
  {
    "text": "do I try to write it here? This is my last word for today-- the Cholesky.",
    "start": "3270290",
    "end": "3275615"
  },
  {
    "text": " This is the Cholesky\nFactorization,",
    "start": "3275615",
    "end": "3282140"
  },
  {
    "text": "named after a French guy,\na French soldier actually.",
    "start": "3282140",
    "end": "3287430"
  },
  {
    "text": "So LDL transpose is\nCholesky, and that's",
    "start": "3287430",
    "end": "3294470"
  },
  {
    "text": "easy to compute, much\nfaster to compute than the eigenvalue square root. But this square\nroot is triangular.",
    "start": "3294470",
    "end": "3301460"
  },
  {
    "text": "This square root is symmetric. Those are the two pieces of\nlinear algebra to find things,",
    "start": "3301460",
    "end": "3308810"
  },
  {
    "text": "to reduce things\nto triangular form, or to reduce them to connect\nthem with symmetric matrices.",
    "start": "3308810",
    "end": "3316350"
  },
  {
    "text": "OK, thank you for\nattention today. So today, we did the\ndistance matrices,",
    "start": "3316350",
    "end": "3326180"
  },
  {
    "text": "and this was the final\nstep to get the X. And also, most\nimportant was to get",
    "start": "3326180",
    "end": "3335840"
  },
  {
    "text": "the structure of a\nneural net straight, separating the v's,\nthe sample vectors,",
    "start": "3335840",
    "end": "3342500"
  },
  {
    "text": "from the x's, the weights. OK, so Friday, I've\ngot one volunteer",
    "start": "3342500",
    "end": "3349580"
  },
  {
    "text": "to talk about a project, and I'm\ndesperately looking for more. Please just send me an email.",
    "start": "3349580",
    "end": "3356750"
  },
  {
    "text": " It'd would be appreciated,\nor I'll send you an email,",
    "start": "3356750",
    "end": "3362660"
  },
  {
    "text": "if necessary. OK, thanks. ",
    "start": "3362660",
    "end": "3367007"
  }
]