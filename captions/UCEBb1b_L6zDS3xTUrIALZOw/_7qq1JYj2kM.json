[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue offer",
    "start": "0",
    "end": "6920"
  },
  {
    "text": "high quality educational\nresources for free. To make a donation or to view\nadditional materials from",
    "start": "6920",
    "end": "12780"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "12780",
    "end": "19270"
  },
  {
    "text": "PROFESSOR: I just started\ntalking a little bit last time",
    "start": "19270",
    "end": "26460"
  },
  {
    "text": "about viewing L2, namely this\nset of functions that are",
    "start": "26460",
    "end": "33230"
  },
  {
    "text": "square integrable as\na vector space. And I want to reveal a\nlittle bit about why",
    "start": "33230",
    "end": "39230"
  },
  {
    "text": "we want to do that. Because after spending so long\nworrying about all these",
    "start": "39230",
    "end": "45320"
  },
  {
    "text": "questions of measurability and\nall that, you must wonder, why do we want to look at it\na different way now.",
    "start": "45320",
    "end": "53460"
  },
  {
    "text": "Well, a part of it is we want\nto be able to look at",
    "start": "53460",
    "end": "59590"
  },
  {
    "text": "orthogonal expansions\ngeometrically. In other words, we would\nlike to be able to draw pictures of them.",
    "start": "59590",
    "end": "66390"
  },
  {
    "text": "You can draw pictures of a\nfunction, but you're drawing a picture of one function. And you can't draw anything\nabout the relationship between",
    "start": "66390",
    "end": "74420"
  },
  {
    "text": "different functions, except\nby drawing the two different functions. You get all the detail\nthere, and you can't",
    "start": "74420",
    "end": "80990"
  },
  {
    "text": "abstract it at all. So somehow we want to start\nto abstract some of this information.",
    "start": "80990",
    "end": "87740"
  },
  {
    "text": "And we want to be able to draw\npictures which look more like vector pictures than\nlike functions.",
    "start": "87740",
    "end": "93490"
  },
  {
    "text": "And you'll see why that becomes\nimportant in a while. The other thing is, when you\ndraw a function as a function",
    "start": "93490",
    "end": "100090"
  },
  {
    "text": "of time, the only thing you see\nit how it behaves in time. When you take the Fourier\ntransform and you draw it in",
    "start": "100090",
    "end": "107490"
  },
  {
    "text": "frequency, the only thing\nyou see is how it behaves in frequency. And, again, you don't see what\nthe relationship is between",
    "start": "107490",
    "end": "115110"
  },
  {
    "text": "different functions. So you lose all of that. When you take this signal space\nviewpoint, what you're",
    "start": "115110",
    "end": "121780"
  },
  {
    "text": "trying to do there is to not\nstress time or frequency so",
    "start": "121780",
    "end": "128890"
  },
  {
    "text": "much, but more to look\nat the relationship between different functions. Why do we want to do that?",
    "start": "128890",
    "end": "134780"
  },
  {
    "text": "Well, because as soon as we\nstart looking at noise and things like that, we want to\nbe able to tell something",
    "start": "134780",
    "end": "140620"
  },
  {
    "text": "about how distinguishable\ndifferent functions are from each other. So the critical question there\nyou want to ask, when you ask",
    "start": "140620",
    "end": "149319"
  },
  {
    "text": "how different functions are,\nis how do you look at two functions both at\nthe same time.",
    "start": "149320",
    "end": "154870"
  },
  {
    "text": "So, again, that's -- all of this is coming back\nto the same thing.",
    "start": "154870",
    "end": "161450"
  },
  {
    "text": "So we want to be able to draw\npictures of functions which show how functions are related,\nrather than show all",
    "start": "161450",
    "end": "168270"
  },
  {
    "text": "the individual detail\nof function. Finally, what we'll see at the\nend of today that it gives us",
    "start": "168270",
    "end": "175640"
  },
  {
    "text": "a lot of capability for\nunderstanding much, much better what's going on when\nwe look at conversions of",
    "start": "175640",
    "end": "183040"
  },
  {
    "text": "orthogonal series. This is something we haven't had\nany way to look at before",
    "start": "183040",
    "end": "189040"
  },
  {
    "text": "we talked about the Fourier\nseries, the discrete time",
    "start": "189040",
    "end": "194500"
  },
  {
    "text": "Fourier transform, and\nthings like this. But we haven't been able to\nreally say something general.",
    "start": "194500",
    "end": "202280"
  },
  {
    "text": "Which, again, is what\nwe want to do now. ",
    "start": "202280",
    "end": "208020"
  },
  {
    "text": "I'm going to go very quickly\nthrough these axioms of a vector space.",
    "start": "208020",
    "end": "214410"
  },
  {
    "text": "Most of you have seen\nthem before. Those of you who haven't seen\naxiomatic treatments of",
    "start": "214410",
    "end": "221430"
  },
  {
    "text": "various things in mathematics\nare going to be puzzled by it anyway. And you're going to have to sit\nat home or somewhere on",
    "start": "221430",
    "end": "229510"
  },
  {
    "text": "your own and puzzle this out. But I just wanted to put them\nup so I could start to say",
    "start": "229510",
    "end": "235720"
  },
  {
    "text": "what it is that we're trying\nto do with these axioms. What we're trying to do is to\nsay everything we know about",
    "start": "235720",
    "end": "245700"
  },
  {
    "text": "n-tuples, which we've been\nusing all of our lives.",
    "start": "245700",
    "end": "251430"
  },
  {
    "text": "All of these tricks that we use,\nmost of them we can use to deal with functions also.",
    "start": "251430",
    "end": "258109"
  },
  {
    "text": "The pictures, we can use. All of the ideas about\ndimension, all of the ideas",
    "start": "258110",
    "end": "263900"
  },
  {
    "text": "about expansions. All of this stuff becomes\nuseful again.",
    "start": "263900",
    "end": "269660"
  },
  {
    "text": "But, there are a lot of things\nthat are true about functions that aren't true\nabout vectors.",
    "start": "269660",
    "end": "276740"
  },
  {
    "text": "There are lots of things which\nare true about n-dimensional",
    "start": "276740",
    "end": "282150"
  },
  {
    "text": "vectors that aren't true\nabout functions. And you want to be able to go\nback to these axioms when you",
    "start": "282150",
    "end": "288750"
  },
  {
    "text": "have to, and say, well, is this\nproperty we're looking at something which is a consequence\nof the axioms.",
    "start": "288750",
    "end": "296160"
  },
  {
    "text": "Namely, is this an inherent\nproperty of vector spaces, or is it in fact something else\nwhich is just because of the",
    "start": "296160",
    "end": "305830"
  },
  {
    "text": "particular kind of thing\nwe're looking at. So, vector spaces. Important thing is, there's\nan addition operation.",
    "start": "305830",
    "end": "313690"
  },
  {
    "text": "You can add any two vectors. You can't multiply\nthem, by the way. You can only add them.",
    "start": "313690",
    "end": "320230"
  },
  {
    "text": "You can multiply by scalars,\nwhich we'll talk about in the next slide. But you can only add the\nvectors themselves.",
    "start": "320230",
    "end": "328360"
  },
  {
    "text": "And the addition is commutative,\njust like ordinary addition of real\nnumbers or complex numbers is.",
    "start": "328360",
    "end": "334730"
  },
  {
    "text": "It's associative, which says\nv, u plus w in parentheses.",
    "start": "334730",
    "end": "341630"
  },
  {
    "text": "Now, you see why we're doing\nthis is, we've said by definition that for any two\nvectors, u and w, there's",
    "start": "341630",
    "end": "350990"
  },
  {
    "text": "another vector, which\nis called u plus w. In other words, this axiomatic\nsystem says, whenever you add",
    "start": "350990",
    "end": "357699"
  },
  {
    "text": "two vectors you have\nto get a vector. There's no way around that. So u plus w has to\nbe a vector.",
    "start": "357700",
    "end": "364720"
  },
  {
    "text": "And that says that v plus u\nplus w has to be a vector. Associativity says that you\nget the same vector if you",
    "start": "364720",
    "end": "373520"
  },
  {
    "text": "look at it the other way,\nif first adding v and u and then adding w. So, anything what you call\na vector space has",
    "start": "373520",
    "end": "381940"
  },
  {
    "text": "to have this property. Of course, the real numbers\nhave this property. The complex numbers have\nthis property.",
    "start": "381940",
    "end": "387340"
  },
  {
    "text": "All the n-tuples you\ndeal with all the time have this property. Functions have this property.",
    "start": "387340",
    "end": "392780"
  },
  {
    "text": "Sequences, infinite length\nsequences have this property, so there's no big\ndeal about it.",
    "start": "392780",
    "end": "398370"
  },
  {
    "text": "But that is one of the axioms. There's a unique vector 0, such\nthat when you add 0 to a",
    "start": "398370",
    "end": "404330"
  },
  {
    "text": "vector, you get the\nsame vector again. Now, this is not the 0 in\nthe real number system.",
    "start": "404330",
    "end": "412379"
  },
  {
    "text": "It's not the 0 in a complex\nnumber system. In terms of vectors, if you're\nthinking of n-tuples, this is",
    "start": "412380",
    "end": "420040"
  },
  {
    "text": "the n-tuple which is all 0's. In terms of other vectors,\nspaces, it",
    "start": "420040",
    "end": "425840"
  },
  {
    "text": "might be other things. Like, in terms of functions, 0\nmeans the function which is 0",
    "start": "425840",
    "end": "431480"
  },
  {
    "text": "everywhere. And finally, there's the unique\nvector minus v, which",
    "start": "431480",
    "end": "437400"
  },
  {
    "text": "has the property that v plus\nminus v -- in other words, v minus v -- is equal to 0.",
    "start": "437400",
    "end": "443250"
  },
  {
    "text": "And in a sense that defines subtraction as well as addition. So we have addition and\nsubtraction, but we don't have",
    "start": "443250",
    "end": "450710"
  },
  {
    "text": "multiplication. ",
    "start": "450710",
    "end": "458479"
  },
  {
    "text": "And there's scalar\nmultiplication. Which means you can multiply\na vector by a scalar.",
    "start": "458480",
    "end": "464070"
  },
  {
    "text": "And the vector spaces we're\nlooking at here only have two",
    "start": "464070",
    "end": "470330"
  },
  {
    "text": "kinds of scalars. One is real scalars and the\nother is complex scalars. The two are quite different,\nas you know.",
    "start": "470330",
    "end": "477790"
  },
  {
    "text": "And so when we're talking about\na vector space we have to say what the scalar field is\nthat we're talking about.",
    "start": "477790",
    "end": "486340"
  },
  {
    "text": "So, for every vector. And also for every scalar,\nthere's another vector which",
    "start": "486340",
    "end": "493800"
  },
  {
    "text": "is the scalar times\nthe vector. Which means, you have scalar\nmultiplication. You can multiply vectors by\nscalars in terms of n-tuples.",
    "start": "493800",
    "end": "504509"
  },
  {
    "text": "When you're multiplying a scalar\nby an n-tuple, you just multiply every component\nby that scalar.",
    "start": "504510",
    "end": "510100"
  },
  {
    "text": " When you take the scalar 1, and\nin a field there's always",
    "start": "510100",
    "end": "517430"
  },
  {
    "text": "an element 1, there's\nalways an element 0. When you take the element 1,\nand you multiply it by a",
    "start": "517430",
    "end": "523180"
  },
  {
    "text": "vector, you got the\nsame vector. Which, of course, is what you\nwould expect if you were",
    "start": "523180",
    "end": "528230"
  },
  {
    "text": "looking at functions. You multiply a function. For every t you multiply it by\n1, and you get the same thing",
    "start": "528230",
    "end": "536100"
  },
  {
    "text": "back again. For an n-tuple, you multiply\nevery component by 1, you get the same thing back again.",
    "start": "536100",
    "end": "542480"
  },
  {
    "text": "So that's -- well, it's just another axiom. Then we have these distributive\nlaws.",
    "start": "542480",
    "end": "549500"
  },
  {
    "text": "And I won't read them\nout to you, they're just what they say. I want to talk about them in\na second a little bit.",
    "start": "549500",
    "end": "554710"
  },
  {
    "text": " And as we said, the simplistic\nexample of a vector space, and",
    "start": "554710",
    "end": "562630"
  },
  {
    "text": "the one you've been using for\nyears, partly because it saves you a lot of notation.",
    "start": "562630",
    "end": "568839"
  },
  {
    "text": "Incidentally, one of the reasons\nI didn't list for talking about L2 as a vector\nspace is, it saves you a lot",
    "start": "568840",
    "end": "577800"
  },
  {
    "text": "of writing. Also, just like when you're\ntalking about n-tuples it saves you writing. It's a nice, convenient\nnotational device.",
    "start": "577800",
    "end": "586639"
  },
  {
    "text": "I don't think any part of\nmathematics becomes popular with everyone, unless it saves\nnotation as well as well as",
    "start": "586640",
    "end": "595920"
  },
  {
    "text": "simplifying things. So we have these n-tuples, which\nis what we mean by r sub",
    "start": "595920",
    "end": "602870"
  },
  {
    "text": "n or c sub n. In other words, when I talk\nabout r sub n, I don't mean just a vector space\nof dimension n.",
    "start": "602870",
    "end": "610320"
  },
  {
    "text": "I haven't even defined\ndimension yet. And when you talk in this\ngenerality, you have to think",
    "start": "610320",
    "end": "618960"
  },
  {
    "text": "a little bit about\nwhat it means. So we're just talking about\nn-tuples now. r sub n is",
    "start": "618960",
    "end": "625670"
  },
  {
    "text": "n-tuples of real numbers.\nc sub n is n-tuples of complex numbers.",
    "start": "625670",
    "end": "632080"
  },
  {
    "text": "Addition and scalar\nmultiplication are component by component, which\nwe just said. In other words, w equals u plus\nv means this, for all i.",
    "start": "632080",
    "end": "642449"
  },
  {
    "text": "Scalar multiplication\nmeans this. The unit vector, e sub i, is a\n1 in the i'th position, a 0",
    "start": "642450",
    "end": "651779"
  },
  {
    "text": "everywhere else. And what that means is that\nany vector v which is an",
    "start": "651780",
    "end": "657820"
  },
  {
    "text": "n-tuple, n rn or n cn, can\nalso be expressed as the equals summation of these\ncoefficients,",
    "start": "657820",
    "end": "666040"
  },
  {
    "text": "times these unit factors. That looks like we're giving up\nthe simple notation we have",
    "start": "666040",
    "end": "672230"
  },
  {
    "text": "and going to more complex\nnotation. This is a very useful\nidea here.",
    "start": "672230",
    "end": "678390"
  },
  {
    "text": "That you can take even something\nas simple as n-tuples and express every\nvector as a sum of these",
    "start": "678390",
    "end": "685820"
  },
  {
    "text": "simple vectors. And when you start drawing\npictures, you start to see why this makes sense.",
    "start": "685820",
    "end": "691090"
  },
  {
    "text": "I'm going to do this\non the next slide. So let's do it. ",
    "start": "691090",
    "end": "704810"
  },
  {
    "text": "And on the slide here, what I've\ndone is to draw a diagram which is one you've seen\nmany times, I'm sure.",
    "start": "704810",
    "end": "710940"
  },
  {
    "text": "Except for the particular\nthings on it. Of two-dimensional space, where\nyou take a 2-tuple and",
    "start": "710940",
    "end": "716700"
  },
  {
    "text": "you think of the first element\nin the 2-tuple, v1, a being the horizontal axis.",
    "start": "716700",
    "end": "722500"
  },
  {
    "text": "The second element, v2, as\nbeing the vertical axis. And then you can draw any\n2-tuple by going over v1 and",
    "start": "722500",
    "end": "732250"
  },
  {
    "text": "then up v2, which is what you've\ndone all your lives. The reason I'm drawing this is\nto show you that you can take",
    "start": "732250",
    "end": "739800"
  },
  {
    "text": "any two vectors. First vector, v. Second\nvector, u.",
    "start": "739800",
    "end": "746670"
  },
  {
    "text": "One thing that -- I'm sure you all traditionally\ndo is you view a",
    "start": "746670",
    "end": "752380"
  },
  {
    "text": "vector in two ways. One, you view it as a point\nin two-dimensional space.",
    "start": "752380",
    "end": "758610"
  },
  {
    "text": "And the other is you view it as\na line from 0 up to v. And",
    "start": "758610",
    "end": "765890"
  },
  {
    "text": "when you put a little arrow on\nit, it looks like a vector, and we call it a vector. So it must be a vector, OK?",
    "start": "765890",
    "end": "772880"
  },
  {
    "text": "So either the point or the\nline represents a vector. Here's another vector, u.",
    "start": "772880",
    "end": "778779"
  },
  {
    "text": "I can take the difference\nbetween two vectors, u minus v, just by drawing a line\nfrom v up to u.",
    "start": "778780",
    "end": "787560"
  },
  {
    "text": "And thinking of that\nas a vector also. So this vector really means a\nvector starting here, going",
    "start": "787560",
    "end": "796790"
  },
  {
    "text": "parallel to this, up to this\npoint, which is what w is. But it's very convenient to draw\nit this way, which lets",
    "start": "796790",
    "end": "803610"
  },
  {
    "text": "you know what's going on. Namely, you represent u as\nthe sum of v plus w.",
    "start": "803610",
    "end": "810950"
  },
  {
    "text": "Or you represent w as a\ndifference between u and v. And all of this is trivial.",
    "start": "810950",
    "end": "817029"
  },
  {
    "text": "I just want to say it explicitly\nso you start to see what the connection is between\nthese axioms, which if I don't",
    "start": "817030",
    "end": "826200"
  },
  {
    "text": "talk about them a little bit,\nI'm sure you're just going to blow them off and decide, eh, I\nknow all of that, I can look",
    "start": "826200",
    "end": "833339"
  },
  {
    "text": "at things in two dimensions\nand I don't have to think about them at all. And then, in a few days, when\nwe're doing the same thing for",
    "start": "833340",
    "end": "843340"
  },
  {
    "text": "functions, you're suddenly going\nto become very confused. So you ought to think about\nit in these simple terms.",
    "start": "843340",
    "end": "849230"
  },
  {
    "text": "Now, when I take a scalar\nmultiple of the vector, in",
    "start": "849230",
    "end": "854730"
  },
  {
    "text": "terms of these diagrams, what\nwe're doing is taking something on this\nsame line here.",
    "start": "854730",
    "end": "861060"
  },
  {
    "text": "I can take scalar multiples\nwhich go all the way up here, all the way down there. I can take scalar multiples\nof u which go up here.",
    "start": "861060",
    "end": "869130"
  },
  {
    "text": "And down here. The interesting thing here is\nwhen I scale v down by alpha",
    "start": "869130",
    "end": "877560"
  },
  {
    "text": "and I scale u down by alpha, I\ncan also scale w down by alpha",
    "start": "877560",
    "end": "882970"
  },
  {
    "text": "and I connect it just like that,\nfrom alpha v to alpha u. Which axiom is that?",
    "start": "882970",
    "end": "888250"
  },
  {
    "start": "888250",
    "end": "894360"
  },
  {
    "text": "I mean, this is the\ncanonic example of one of those axioms.",
    "start": "894360",
    "end": "901360"
  },
  {
    "text": "AUDIENCE: [UNINTELLIGIBLE] PROFESSOR: What? AUDIENCE: Distributed. PROFESSOR: Distributed, good. This is saying that alpha times\nthe quantity u minus v",
    "start": "901360",
    "end": "910029"
  },
  {
    "text": "is equal to alpha u minus alpha\nv. That's so trivial",
    "start": "910030",
    "end": "915770"
  },
  {
    "text": "that it's hard to see it, but\nthat's what it's saying. So that the distributive law\nreally says the triangles",
    "start": "915770",
    "end": "922540"
  },
  {
    "text": "maintain their shape.  Maybe it's easier to just\nsay distributive",
    "start": "922540",
    "end": "927860"
  },
  {
    "text": "law, I don't know. ",
    "start": "927860",
    "end": "935959"
  },
  {
    "text": "So, for the space of L2 complex\nfunctions, we're going to define u plus v as w,\nwhere w of t equals u",
    "start": "935960",
    "end": "946790"
  },
  {
    "text": "of t plus v of t. Now, I've done a bunch of\nthings here all at once. One of them is to say what we\nused to call a function, u of",
    "start": "946790",
    "end": "956350"
  },
  {
    "text": "t, we're now referring to with\na single letter, boldface u.",
    "start": "956350",
    "end": "962250"
  },
  {
    "text": "What's the advantage of that? Well, one advantage of it is\nwhen you talk about a function",
    "start": "962250",
    "end": "968460"
  },
  {
    "text": "as u of t, you're really\ntalking about two different things. One, you're talking about the\nvalue of the function at a",
    "start": "968460",
    "end": "976530"
  },
  {
    "text": "particular argument, t. And the other is,\nyou're talking about the whole function.",
    "start": "976530",
    "end": "982860"
  },
  {
    "text": "I mean, sometimes you\nwant to say a function has some property. A function is L2. Well, u of t at a particular\nt is just a number.",
    "start": "982860",
    "end": "990970"
  },
  {
    "text": "It's not a function. So this gives us a nice way\nof distinguishing between",
    "start": "990970",
    "end": "996779"
  },
  {
    "text": "functions and between the\nvalue of functions for particular arguments.",
    "start": "996780",
    "end": "1002310"
  },
  {
    "text": "So that's one more notational\nadvantage you get by talking about vectors here.",
    "start": "1002310",
    "end": "1009310"
  },
  {
    "text": "We're going to define the sum\nof two functions just as the point y sum.",
    "start": "1009310",
    "end": "1015340"
  },
  {
    "text": "In other words, these\ntwo functions are defined at each t. w defined at a t is the\nsum of this and that.",
    "start": "1015340",
    "end": "1024390"
  },
  {
    "text": "The scalar multiplication is\njust defined by, at every t, u",
    "start": "1024390",
    "end": "1029680"
  },
  {
    "text": "of t is equal to alpha\ntimes v of t. Just what you'd expect.",
    "start": "1029680",
    "end": "1034809"
  },
  {
    "text": "There's nothing strange here. I just want to be explicit in\nsaying how everything follows",
    "start": "1034810",
    "end": "1040600"
  },
  {
    "text": "from these axioms here. And I won't say all\nof it because there's too much of it.",
    "start": "1040600",
    "end": "1047720"
  },
  {
    "text": "With this addition and scalar\nmultiplication, L2, the set of finite energy measurable\nfunctions is in fact the",
    "start": "1047720",
    "end": "1056389"
  },
  {
    "text": "complex vector space. And it's trivial to go back\nand check through all the axioms with what we said\nhere, and I'm not",
    "start": "1056390",
    "end": "1063519"
  },
  {
    "text": "going to do it now. There's only one of those\naxioms which is a bit questionable.",
    "start": "1063520",
    "end": "1069670"
  },
  {
    "text": "And that is, when you add up two\nfunctions which are square integrable, is the sum going to\nbe square integrable also.",
    "start": "1069670",
    "end": "1077880"
  },
  {
    "text": " Well, you nod and say yes, but\nit's worthwhile proving it",
    "start": "1077880",
    "end": "1084059"
  },
  {
    "text": "once in your lives. So the question is, is this\nfunction here less than",
    "start": "1084060",
    "end": "1089790"
  },
  {
    "text": "infinity if the integral of u of\nt squared and the integral",
    "start": "1089790",
    "end": "1096040"
  },
  {
    "text": "of v of t squared are\nboth integrable. Well, it's a useful inequality,\nwhich looks like a",
    "start": "1096040",
    "end": "1102340"
  },
  {
    "text": "very weak inequality but it,\nin fact, is not weak. It says that u of\nt plus v of t.",
    "start": "1102340",
    "end": "1108660"
  },
  {
    "text": "This is just at a particular\nvalue of t. So this is just an inequality\nfor real",
    "start": "1108660",
    "end": "1114320"
  },
  {
    "text": "numbers and complex numbers. It says that this u of t plus\nv of t, quantity squared,",
    "start": "1114320",
    "end": "1120860"
  },
  {
    "text": "magnitude squared, is less than\nor equal to 2 times u of t squared plus 2 times\nv of t squared.",
    "start": "1120860",
    "end": "1127780"
  },
  {
    "text": "You wonder, at first, what's\nthe 2 doing in there. But then think of making\nv of t equal to u of t.",
    "start": "1127780",
    "end": "1135309"
  },
  {
    "text": "You have 2 times u of t\nsquared, which is 4 times u of t squared.",
    "start": "1135310",
    "end": "1141400"
  },
  {
    "text": "Well, that's what you need\nhere to make this true. So, in that example\nthis inequality is satisfied with equality.",
    "start": "1141400",
    "end": "1149350"
  },
  {
    "text": "To verify the inequality\nin general, you just multiply this out. It's u of t squared plus v of t\nsquared plus two cross-terms",
    "start": "1149350",
    "end": "1157790"
  },
  {
    "text": "and it all works. ",
    "start": "1157790",
    "end": "1163250"
  },
  {
    "text": "This vector space here is not\nquite the vector space we want to talk about.",
    "start": "1163250",
    "end": "1168900"
  },
  {
    "text": "But let's put off that question\nfor a while and we'll come to it later.",
    "start": "1168900",
    "end": "1174000"
  },
  {
    "text": "We will come up with a vector\nspace which is just slightly different than that. ",
    "start": "1174000",
    "end": "1181650"
  },
  {
    "text": "The main thing you can do with\nvector spaces is talk about their dimension.",
    "start": "1181650",
    "end": "1188760"
  },
  {
    "text": "Well, there are a lot of other\nthings you can do, but this is one of the main things\nwe can do. And it's an important thing\nwhich we have to talk about",
    "start": "1188760",
    "end": "1197080"
  },
  {
    "text": "before going into inner product\nspaces, which is what we're really interested in.",
    "start": "1197080",
    "end": "1204400"
  },
  {
    "text": "So we need a bunch of\ndefinitions here. All of this, I'm sure is\nfamiliar to most of you.",
    "start": "1204400",
    "end": "1210120"
  },
  {
    "text": "For those of you that it's not\nfamiliar to, you just have to spend a little longer with it.",
    "start": "1210120",
    "end": "1215480"
  },
  {
    "text": "There's not a whole\nlot involved here. If you have a set of vectors,\nwhich are in a vector space,",
    "start": "1215480",
    "end": "1225000"
  },
  {
    "text": "you say that they span the\nvector space if in fact every vector in this vector\nspace is a linear",
    "start": "1225000",
    "end": "1232590"
  },
  {
    "text": "combination of those vectors. In other words, any vector, u,\ncan be made up as some sum of",
    "start": "1232590",
    "end": "1242890"
  },
  {
    "text": "alpha i times v sub i. Now, notice we've gone a long\nway here beyond those axioms.",
    "start": "1242890",
    "end": "1250980"
  },
  {
    "text": "Because we're talking about\nscalar multiplications. And then we're talking about\na sum of a lot of scalar",
    "start": "1250980",
    "end": "1258340"
  },
  {
    "text": "multiplications. Each scalar multiplication\nis a vector. The sum of a bunch of vectors,\nby the associative law is, in",
    "start": "1258340",
    "end": "1265330"
  },
  {
    "text": "fact, another vector. So every one of these\nsums is a vector.",
    "start": "1265330",
    "end": "1271710"
  },
  {
    "text": "And by definition, a set of\nvectors spans a vector space",
    "start": "1271710",
    "end": "1277110"
  },
  {
    "text": "if every vector can be\nrepresented as some linear combination of them.",
    "start": "1277110",
    "end": "1282356"
  },
  {
    "text": "In other words, there isn't\nsomething outside of here sitting there waiting to\nbe discovered later.",
    "start": "1282356",
    "end": "1289000"
  },
  {
    "text": "You really understand everything\nthat's there. And we say that v is finite\ndimensional if it is spanned",
    "start": "1289000",
    "end": "1296230"
  },
  {
    "text": "by a finite set of vectors. So that's another definition. That's what you mean by\nfinite dimensional.",
    "start": "1296230",
    "end": "1302390"
  },
  {
    "text": "You have to be able to find a\nfinite set of vectors such that linear combinations of\nthose vectors gives you",
    "start": "1302390",
    "end": "1309020"
  },
  {
    "text": "everything. It doesn't mean you have a\nfinite set of vectors. You have an infinite set of\nvectors because you have an",
    "start": "1309020",
    "end": "1315530"
  },
  {
    "text": "infinite set of scalars. In fact, you'd have an\nuncountably infinite set of vectors because of\nthese scalars.",
    "start": "1315530",
    "end": "1321820"
  },
  {
    "text": " Second definition. The vector v1 to vn are linearly\nindependent -- and",
    "start": "1321820",
    "end": "1329940"
  },
  {
    "text": "this is a mouthful -- if u\nequals the sum of alpha sub i",
    "start": "1329940",
    "end": "1335149"
  },
  {
    "text": "v sub i equals 0, only\nfor alpha sub i equal",
    "start": "1335150",
    "end": "1340280"
  },
  {
    "text": "to 0 for each i. In other words, you can't take\nany linear combination of the",
    "start": "1340280",
    "end": "1346900"
  },
  {
    "text": "v sub i's and get 0 unless that\nlinear combination is",
    "start": "1346900",
    "end": "1352130"
  },
  {
    "text": "using all 0's. You can't, in any non-trivial\nway, add up a bunch of",
    "start": "1352130",
    "end": "1357330"
  },
  {
    "text": "vectors and get 0. To put it another way, none of\nthese basis vectors is a",
    "start": "1357330",
    "end": "1363660"
  },
  {
    "text": "linear combination\nof the others. That's usually a more convenient\nway to put it.",
    "start": "1363660",
    "end": "1370130"
  },
  {
    "text": "Although it takes\nmore writing. Now, we say that a set of\nvectors are a basis for v if",
    "start": "1370130",
    "end": "1375820"
  },
  {
    "text": "they're both linearly\nindependent and if they span v. When we first talked about\nspanning, we didn't say",
    "start": "1375820",
    "end": "1382850"
  },
  {
    "text": "anything about these vectors\nbeing linearly independent, so we might have had many more\nof them than we needed.",
    "start": "1382850",
    "end": "1390110"
  },
  {
    "text": "Now, when we're talking about a\nbasis, we restrict ourselves to just the set we need\nto span the space.",
    "start": "1390110",
    "end": "1398330"
  },
  {
    "text": "And then the theorem, which I'm\nnot going to prove, but it's standard Theorem One of\nany linear algebra book --",
    "start": "1398330",
    "end": "1408590"
  },
  {
    "text": "well, it's probably Theorem One,\nTwo, Three and Four all put together -- but anyway, if\nit says if v1 and v sub n span",
    "start": "1408590",
    "end": "1416990"
  },
  {
    "text": "v, then a subset of these\nvectors is the basis of b.",
    "start": "1416990",
    "end": "1422100"
  },
  {
    "text": "In other words, if you have a\nset of vectors which span a space, you can find the basis by\nsystematically eliminating",
    "start": "1422100",
    "end": "1430460"
  },
  {
    "text": "vectors which are linear\ncombinations of the others, until you get to a point where\nyou can't do that any further.",
    "start": "1430460",
    "end": "1437039"
  },
  {
    "text": "So this theorem has an algorithm\ntied into it. Given any set of vectors which\nspan a space, you can find the",
    "start": "1437040",
    "end": "1443280"
  },
  {
    "text": "basis from it by perhaps\nthrowing out some of those vectors.",
    "start": "1443280",
    "end": "1449510"
  },
  {
    "text": "The next part of it is, if v is\na finite dimensional space,",
    "start": "1449510",
    "end": "1454920"
  },
  {
    "text": "then every basis has\nthe same size. This, in fact, is a thing which\ntakes a little bit of",
    "start": "1454920",
    "end": "1460030"
  },
  {
    "text": "work proving it. And, also, any linearly\nindependent set, v1 to v sub",
    "start": "1460030",
    "end": "1465460"
  },
  {
    "text": "n, is part of the basis. In other words, here's another\nalgorithm you can use.",
    "start": "1465460",
    "end": "1471150"
  },
  {
    "text": "You have this big, finite\ndimensional space. You have a few vectors which\nare linearly independent.",
    "start": "1471150",
    "end": "1478960"
  },
  {
    "text": "You can build a basis starting\nwith these, and you just experiment. You experiment to find new\nvectors, which are not linear",
    "start": "1478960",
    "end": "1488250"
  },
  {
    "text": "combinations of that set. As soon as you find one, you\nadd it to the basis. You keep on going.",
    "start": "1488250",
    "end": "1494200"
  },
  {
    "text": "And the theorem says, by\ntime you get to n of them, you're done. So that, in a sense, spanning\nsets are too big.",
    "start": "1494200",
    "end": "1505540"
  },
  {
    "text": "Linearly independent\nsets are too small. And what you want to do is add\nthe linearly independent sets,",
    "start": "1505540",
    "end": "1511720"
  },
  {
    "text": "shrink the spanning sets, and\ncome up with a bases. And all bases have the same\nnumber of vectors.",
    "start": "1511720",
    "end": "1519240"
  },
  {
    "text": "There many different bases you\ncome up with, but they all have the same number\nof vectors. ",
    "start": "1519240",
    "end": "1526980"
  },
  {
    "text": "Not going to talk at all about\ninfinite dimensional spaces until the last slide today.",
    "start": "1526980",
    "end": "1532750"
  },
  {
    "text": "Because the only way I know\nto understand infinite dimensional vector spaces is by\nthinking of them, in some",
    "start": "1532750",
    "end": "1542169"
  },
  {
    "text": "sort of limiting way, as finite\ndimensional spaces. And I think that's the only\nway you can do it.",
    "start": "1542170",
    "end": "1550630"
  },
  {
    "text": "A vector space in itself has\nno sense of distance",
    "start": "1550630",
    "end": "1557260"
  },
  {
    "text": "or angles in it. When I drew that picture before\nfor you -- let me put",
    "start": "1557260",
    "end": "1562880"
  },
  {
    "text": "it up again -- ",
    "start": "1562880",
    "end": "1571820"
  },
  {
    "text": "it almost looked like\nthere was some sense",
    "start": "1571820",
    "end": "1579019"
  },
  {
    "text": "of distance in here. Because when we took scalar\nmultiples, we scaled these things down.",
    "start": "1579020",
    "end": "1586400"
  },
  {
    "text": "When I took these triangles, we\nscaled down the triangle, and the triangle\nwas maintained.",
    "start": "1586400",
    "end": "1591590"
  },
  {
    "text": "But, in fact, I could do this\njust as well if I took v and moved it down here almost down\non this axis, and if I took u",
    "start": "1591590",
    "end": "1600000"
  },
  {
    "text": "and moved that almost\ndown on the axis. I have the same picture, the\nsame kind of distributive law.",
    "start": "1600000",
    "end": "1608040"
  },
  {
    "text": "And everything else. You can't -- I mean, one of the troubles with\nn-dimensional space, to",
    "start": "1608040",
    "end": "1615029"
  },
  {
    "text": "study what a vector space is\nabout, is it's very hard to think of n-tuples without\nthinking of distance.",
    "start": "1615030",
    "end": "1624650"
  },
  {
    "text": "And without thinking of things\nbeing orthogonal to each other, and of all\nof these things. None of that is talked about at\nall in any of these axioms.",
    "start": "1624650",
    "end": "1634520"
  },
  {
    "text": "And, in fact, you need some new\naxioms to be able to talk about ideas of distance, or\nangle, or any of these things.",
    "start": "1634520",
    "end": "1643370"
  },
  {
    "text": "And that's what we\nwant to add here. ",
    "start": "1643370",
    "end": "1652800"
  },
  {
    "text": "So we need some new axioms. And what we need is a new\noperation on the vector space,",
    "start": "1652800",
    "end": "1658870"
  },
  {
    "text": "before the only -- the only\noperations we have are addition and scalar\nmultiplication.",
    "start": "1658870",
    "end": "1663960"
  },
  {
    "text": "So that vector spaces\nare really incredibly simple animals. There's very little you\ncan do with them.",
    "start": "1663960",
    "end": "1671050"
  },
  {
    "text": "And this added thing is called\nan inner product. An inner product is\na scalar valued",
    "start": "1671050",
    "end": "1678110"
  },
  {
    "text": "function of two vectors. And it's represented by\nthese little brackets.",
    "start": "1678110",
    "end": "1685580"
  },
  {
    "text": "And the axioms that these inner\nproducts have to satisfy",
    "start": "1685580",
    "end": "1691600"
  },
  {
    "text": "is, if you're dealing with\na complex vector space. In other words, where the\nscalars are complex numbers,",
    "start": "1691600",
    "end": "1698500"
  },
  {
    "text": "then this inner product, when\nyou switch it around, you have",
    "start": "1698500",
    "end": "1704530"
  },
  {
    "text": "Hermitian symmetry. Which means that this inner\nproduct is equal to u v",
    "start": "1704530",
    "end": "1711460"
  },
  {
    "text": "complex conjugate. We've already seen that\nsort of thing in taking Fourier series.",
    "start": "1711460",
    "end": "1716840"
  },
  {
    "text": "And the fact that when you're\ndealing with complex numbers,",
    "start": "1716840",
    "end": "1722419"
  },
  {
    "text": "symmetry doesn't usually hold,\nand you usually need some kind of Hermitian symmetry in most\nof the things you do.",
    "start": "1722420",
    "end": "1731720"
  },
  {
    "text": "The next axiom is something\ncalled bilinearity. It says that if you take a\nvector which is alpha times a",
    "start": "1731720",
    "end": "1739919"
  },
  {
    "text": "vector v, plus beta times a\nvector u, take the inner product of that with w, it\nsplits up as alpha times v w",
    "start": "1739920",
    "end": "1749210"
  },
  {
    "text": "plus beta times u w. How about if I do it\nthe other way? See if you understand what I'm\ntalking about at all here.",
    "start": "1749210",
    "end": "1759299"
  },
  {
    "text": "Suppose we take w alpha u plus\nbeta v. What's that equal to?",
    "start": "1759300",
    "end": "1770850"
  },
  {
    "start": "1770850",
    "end": "1776380"
  },
  {
    "text": "Well, it's equal to alpha\nsomething w u plus beta w v.",
    "start": "1776380",
    "end": "1791180"
  },
  {
    "text": "Except that's wrong, It's right\nfor real vector spaces, it's wrong for complex\nvector spaces.",
    "start": "1791180",
    "end": "1796779"
  },
  {
    "text": "What am I missing here? I need complex conjugates here\nand complex conjugates here.",
    "start": "1796780",
    "end": "1804770"
  },
  {
    "text": "I wanted to talk about that,\nbecause when you're dealing with inner products, I don't\nknow whether you're like me,",
    "start": "1804770",
    "end": "1810760"
  },
  {
    "text": "but every time I start dealing\nwith inner products and I get in a hurry writing things down,\nI forgot to put those",
    "start": "1810760",
    "end": "1816880"
  },
  {
    "text": "damn complex conjugates\nin them. And, just be careful.",
    "start": "1816880",
    "end": "1822150"
  },
  {
    "text": "Because you need them. At least go back after you're\nall done and put the complex conjugates in.",
    "start": "1822150",
    "end": "1827940"
  },
  {
    "text": "If you're dealing with real\nvectors, of course you don't need to worry about\nany of that.",
    "start": "1827940",
    "end": "1833510"
  },
  {
    "text": "And all the pictures you draw\nare always of real vectors. ",
    "start": "1833510",
    "end": "1840340"
  },
  {
    "text": "Think of trying to draw\nthis picture. This is the simplest picture\nyou can draw.",
    "start": "1840340",
    "end": "1845520"
  },
  {
    "text": "Of two-dimensional vectors. This is really a picture of a\nvector space of dimension two,",
    "start": "1845520",
    "end": "1852450"
  },
  {
    "text": "for real vectors. What happens if you try to\ndraw a picture of complex",
    "start": "1852450",
    "end": "1858700"
  },
  {
    "text": "two-dimensional vector space? ",
    "start": "1858700",
    "end": "1864070"
  },
  {
    "text": "Well, it becomes very\ndifficult to do. Because you're really talking\nabout the real part of, if",
    "start": "1864070",
    "end": "1877270"
  },
  {
    "text": "you're dealing with a basis\nwhich consists of two complex vectors, then instead of v1, you\nneed a real part of v1 and",
    "start": "1877270",
    "end": "1888550"
  },
  {
    "text": "imaginary part of v1. Instead of v2, you need\nreal part of v2 and",
    "start": "1888550",
    "end": "1893800"
  },
  {
    "text": "imaginary part of v2. And you can always draw this\nin four dimensions.",
    "start": "1893800",
    "end": "1899090"
  },
  {
    "text": "And I even have trouble drawing\nin three dimensions, because somehow my pen doesn't\nmake marks in three",
    "start": "1899090",
    "end": "1904990"
  },
  {
    "text": "dimensions. And in four dimensions,\nI'm a blinking 12.",
    "start": "1904990",
    "end": "1912520"
  },
  {
    "text": "And have no idea\nof what to do. So you have to remember this.",
    "start": "1912520",
    "end": "1918440"
  },
  {
    "text": " If you're dealing with rn or cn,\nalmost always you define",
    "start": "1918440",
    "end": "1927070"
  },
  {
    "text": "the inner product of v and u as\nthe sum of the components",
    "start": "1927070",
    "end": "1932090"
  },
  {
    "text": "with the second component\ncomplex conjugated. This is just a standard thing\nthat we do all the time.",
    "start": "1932090",
    "end": "1939690"
  },
  {
    "text": "When we do this, and we use\nunit vectors, the inner product of v with the i'th unit\nvector is just v sub i.",
    "start": "1939690",
    "end": "1949509"
  },
  {
    "text": "That's what this formula says. Because e sub i is this vector\nu, in which there's a 1 only",
    "start": "1949510",
    "end": "1957020"
  },
  {
    "text": "in the i'th position, and\na 0 everywhere else. So v e i is always the v i,\nand e i v is always v i",
    "start": "1957020",
    "end": "1966020"
  },
  {
    "text": "complex conjugate. Again, this Hermitian nonsense\nthat comes up to",
    "start": "1966020",
    "end": "1972160"
  },
  {
    "text": "plague us all the time. And from that, if you make v\nequal to e sub j or e sub i,",
    "start": "1972160",
    "end": "1979179"
  },
  {
    "text": "you get the inner product of two\nof these basis vectors is equal to 0 for i unequal to j.",
    "start": "1979180",
    "end": "1987050"
  },
  {
    "text": "In other words, the standard way\nof drawing pictures when you make it into an inner\nproduct space, those unit",
    "start": "1987050",
    "end": "1996059"
  },
  {
    "text": "vectors become orthonormal. Because that's the way you\nlike to draw things. You like to draw one\nhere and one there.",
    "start": "1996060",
    "end": "2003230"
  },
  {
    "text": "And that's what we mean by\nperpendicular, which the two-dimensional or\nthree-dimensional word for",
    "start": "2003230",
    "end": "2012160"
  },
  {
    "text": "orthogonal.  So we have a couple\nof definitions.",
    "start": "2012160",
    "end": "2020010"
  },
  {
    "text": "The inner product of v with\nitself is called inner product",
    "start": "2020010",
    "end": "2025160"
  },
  {
    "text": "v squared, which is called the\nsquared norm of the vector.",
    "start": "2025160",
    "end": "2031110"
  },
  {
    "text": "The squared norm has to be\nnon-negative, by axiom. It has to be greater than 0,\nunless this vector, v, is in",
    "start": "2031110",
    "end": "2040649"
  },
  {
    "text": "fact a 0 vector. And the length is\njust the square",
    "start": "2040650",
    "end": "2047049"
  },
  {
    "text": "root of the norm squared. In other words, the length and\nthe norm are the same thing. The norm of a vector is the\nlength of the vector.",
    "start": "2047050",
    "end": "2055850"
  },
  {
    "text": "I've always called it the\nlength, but a lot of people like to call it the norm.",
    "start": "2055850",
    "end": "2061190"
  },
  {
    "text": "v and u are orthogonal if the\ninner product of v and u is",
    "start": "2061190",
    "end": "2066530"
  },
  {
    "text": "equal to 0. How did I get that? I defined it that why.",
    "start": "2066530",
    "end": "2071730"
  },
  {
    "text": "Everybody defines it that way. That's what you mean\nby orthogonality.",
    "start": "2071730",
    "end": "2077099"
  },
  {
    "text": "Now we have to go back and see\nif it makes any sense in terms of these nice simple diagrams.",
    "start": "2077100",
    "end": "2083919"
  },
  {
    "text": "But first I'm going to do\nsomething called the one-dimensional projection\ntheorem.",
    "start": "2083920",
    "end": "2090819"
  },
  {
    "text": "Which is something you all know\nbut you probably have never thought about.",
    "start": "2090820",
    "end": "2096149"
  },
  {
    "text": "And what it says is, if you have\nto vectors, v and u, you",
    "start": "2096150",
    "end": "2101670"
  },
  {
    "text": "can always break v up\ninto two parts. One of which is on the\nsame line with u.",
    "start": "2101670",
    "end": "2109529"
  },
  {
    "text": "In other words, is\ncolinear with u. I'm drawing a picture here for\nreal spaces, but when I say",
    "start": "2109530",
    "end": "2115330"
  },
  {
    "text": "colinear, when I'm dealing with\ncomplex spaces, I mean it's u times some scalar,\nwhich could be complex.",
    "start": "2115330",
    "end": "2124140"
  },
  {
    "text": "So it's somewhere\non this line. And the other part is\nperpendicular to this line.",
    "start": "2124140",
    "end": "2132270"
  },
  {
    "text": "And this theorem says in any\nold inner product space at all, no matter how many\ndimensions you have, infinite",
    "start": "2132270",
    "end": "2139810"
  },
  {
    "text": "dimensional, finite dimensional,\nanything, if it's an inner product space on either\nthe scalars r or the",
    "start": "2139810",
    "end": "2146880"
  },
  {
    "text": "scalars c, you can always take\nany old two vectors at all.",
    "start": "2146880",
    "end": "2152119"
  },
  {
    "text": "And you can break one vector up\ninto a part that's colinear with the other, and another\npart which is orthogonal.",
    "start": "2152120",
    "end": "2159730"
  },
  {
    "text": "And you can always draw\na picture of it. If you don't mind just drawing\nthe picture for real vectors",
    "start": "2159730",
    "end": "2166210"
  },
  {
    "text": "instead of complex vectors. This is an important idea.",
    "start": "2166210",
    "end": "2171520"
  },
  {
    "text": "Because what we're going to use\nit for in a while, is to be able to talk about functions\nwhich are these",
    "start": "2171520",
    "end": "2178450"
  },
  {
    "text": "incredibly complicated\nobjects. And we're going to talk about\ntwo different functions.",
    "start": "2178450",
    "end": "2183760"
  },
  {
    "text": "And we want to be able to draw\na picture in which those two functions are represented\njust as points in a",
    "start": "2183760",
    "end": "2190990"
  },
  {
    "text": "two-dimensional picture. And we're going to do that by\nsaying, OK, I take one of",
    "start": "2190990",
    "end": "2196060"
  },
  {
    "text": "those functions. And I can represent it as partly\nbeing colinear with",
    "start": "2196060",
    "end": "2202109"
  },
  {
    "text": "this other function. And partly being orthogonal\nto the other function.",
    "start": "2202110",
    "end": "2207980"
  },
  {
    "text": "Which says, you can forget about\nall of these functions which extend to infinity, in\ntime extend to infinity, in",
    "start": "2207980",
    "end": "2217010"
  },
  {
    "text": "frequency and everything else. And, so long as you're only\ninterested in some small set",
    "start": "2217010",
    "end": "2222400"
  },
  {
    "text": "of functions, you can just deal\nwith them as a finite dimensional vector space.",
    "start": "2222400",
    "end": "2228330"
  },
  {
    "text": "You can get rid of all the mess,\nand just think of them in this very simple sense.",
    "start": "2228330",
    "end": "2234570"
  },
  {
    "text": "That's really why this vector\nspace idea, which is called signal space, is so popular\namong engineers.",
    "start": "2234570",
    "end": "2242070"
  },
  {
    "text": "It lets than get rid of all the\nmess and think in terms of very, very simple things.",
    "start": "2242070",
    "end": "2247900"
  },
  {
    "text": "So let's see why this\ncomplicated theorem is in fact true.",
    "start": "2247900",
    "end": "2257869"
  },
  {
    "text": "Let me state the\ntheorem first. It says for any inner products\nspace, v. And any vectors, u",
    "start": "2257870",
    "end": "2263800"
  },
  {
    "text": "and v in v, with u\nunequal to 0 -- I hope I said that in the notes\n-- the vector v can be",
    "start": "2263800",
    "end": "2273400"
  },
  {
    "text": "broken up into a colinear term\nplus an orthogonal term, where",
    "start": "2273400",
    "end": "2279720"
  },
  {
    "text": "the colinear term is equal\nto a scalar times u. That's what we mean\nby colinear.",
    "start": "2279720",
    "end": "2285980"
  },
  {
    "text": "That's just the definition\nof colinear. And the other vector\nis orthogonal to u.",
    "start": "2285980",
    "end": "2293210"
  },
  {
    "text": "And alpha is uniquely given by\nthe inner product v u divided",
    "start": "2293210",
    "end": "2298770"
  },
  {
    "text": "by the norm squared of u. Now, there's one thing\nugly about this.",
    "start": "2298770",
    "end": "2304400"
  },
  {
    "text": "You see that norm squared, you\nsay, what is that doing there. It just looks like it's making\nthe formula complicated.",
    "start": "2304400",
    "end": "2311730"
  },
  {
    "text": "Forgot about that for\nthe time being. We will get into it in a minute\nand explain why we have",
    "start": "2311730",
    "end": "2317480"
  },
  {
    "text": "that problem. But, for the moment, let's\njust prove this and see what it says.",
    "start": "2317480",
    "end": "2323540"
  },
  {
    "text": "So what we're going to do is\nsay, well, if I don't look at what the theorem is saying, what\nI'd like to do is look at",
    "start": "2323540",
    "end": "2331810"
  },
  {
    "text": "some generic element which is\na scalar multiple times u.",
    "start": "2331810",
    "end": "2340520"
  },
  {
    "text": "So I'll say, OK let v parallel\nto u be alpha times u.",
    "start": "2340520",
    "end": "2346780"
  },
  {
    "text": "I don't know what alpha is yet,\nbut alpha's going to be whatever it has to be. We're going to choose alpha so\nthat this other vector, v",
    "start": "2346780",
    "end": "2355570"
  },
  {
    "text": "minus v u, is a thing we're\ncalling v perp. So that that, the inner product\nof that and u, is",
    "start": "2355570",
    "end": "2362680"
  },
  {
    "text": "equal to 0. So what I'm trying to do\nis to find a vector -- ",
    "start": "2362680",
    "end": "2369570"
  },
  {
    "text": "strategy here, is to take any\nold vector along this line and",
    "start": "2369570",
    "end": "2375070"
  },
  {
    "text": "try to choose the scalar alpha\nin such a way that the difference between this point\nand this point is orthogonal",
    "start": "2375070",
    "end": "2383610"
  },
  {
    "text": "to this line. That's why I started out\nwith alpha unknown. Alpha unknown just\nsays we have any",
    "start": "2383610",
    "end": "2390140"
  },
  {
    "text": "point along this line. Now I'm going to find out\nwhat alpha has to be. I hope I will find out that it\nhas to be only one thing, and",
    "start": "2390140",
    "end": "2398180"
  },
  {
    "text": "it's uniquely chosen. And that's what we're\ngoing to find.",
    "start": "2398180",
    "end": "2404550"
  },
  {
    "text": "So v minus this projection\nterm, this is called a",
    "start": "2404550",
    "end": "2410230"
  },
  {
    "text": "projection of v on u, is equal\nto v u minus a projection",
    "start": "2410230",
    "end": "2420260"
  },
  {
    "text": "inner product with u. So it's equal to this\ndifference here. This is equal to the\ninner product.",
    "start": "2420260",
    "end": "2427870"
  },
  {
    "text": "Since we have chosen this term\nto be alpha times u, we can bring the alpha out.",
    "start": "2427870",
    "end": "2433410"
  },
  {
    "text": "So it's alpha times the inner\nproduct of u with itself. That's the norm squared\nof alpha.",
    "start": "2433410",
    "end": "2439020"
  },
  {
    "text": "So this is inner product of v\nand u minus alpha times the norm squared.",
    "start": "2439020",
    "end": "2444100"
  },
  {
    "text": " This is 0 if and only if you\nset this equal to 0.",
    "start": "2444100",
    "end": "2450870"
  },
  {
    "text": "And the only value alpha can\nhave to make this 0 is the inner product of v and u divided\nby the norm squared.",
    "start": "2450870",
    "end": "2461000"
  },
  {
    "text": "So I would think that if I ask\nyou to prove this without",
    "start": "2461000",
    "end": "2470120"
  },
  {
    "text": "knowing the projection theorem,\nI would hope that if you weren't afraid of it or\nsomething, and you just sat",
    "start": "2470120",
    "end": "2476640"
  },
  {
    "text": "down and tried to do it,\nyou would all do it in about half an hour. It would probably take most of\nyou longer than that, because",
    "start": "2476640",
    "end": "2484620"
  },
  {
    "text": "everybody gets screwed up in the\nnotation when they first try to do this. But, in fact, this is not\na complicated thing.",
    "start": "2484620",
    "end": "2490900"
  },
  {
    "text": "This is not rocket science. ",
    "start": "2490900",
    "end": "2498170"
  },
  {
    "text": "Now. What is this norm squared\ndoing here? The thing we have just proven is\nthat with any two vectors v",
    "start": "2498170",
    "end": "2505100"
  },
  {
    "text": "and u, the projection of v on\nu, namely that vector, there",
    "start": "2505100",
    "end": "2511390"
  },
  {
    "text": "which has the property that v\nminus v u is perpendicular to u, we showed, is this inner\nproduct divided by the norm",
    "start": "2511390",
    "end": "2521510"
  },
  {
    "text": "squared times u. Now, let me break up\nthis norm squared. Which is just some\npositive number.",
    "start": "2521510",
    "end": "2527790"
  },
  {
    "text": "It's a positive real number. Into the length times\nthe length.",
    "start": "2527790",
    "end": "2532970"
  },
  {
    "text": "Namely, the norm u is just\nsome real number. And write it this way, as the\ninner product of v with u",
    "start": "2532970",
    "end": "2541950"
  },
  {
    "text": "divided by the length of\nu times u divided by",
    "start": "2541950",
    "end": "2547930"
  },
  {
    "text": "the length of u. Now, what is the vector u\ndivided by the length of u?",
    "start": "2547930",
    "end": "2553539"
  },
  {
    "text": "AUDIENCE: [UNINTELLIGIBLE] PROFESSOR: What? AUDIENCE: [UNINTELLIGIBLE]",
    "start": "2553540",
    "end": "2558619"
  },
  {
    "text": "PROFESSOR: It's the same\ndirection of u, but it has length 1. In other words, this is the\nnormalized form of u.",
    "start": "2558620",
    "end": "2566360"
  },
  {
    "text": " And I have the normalized\nform of u in here also.",
    "start": "2566360",
    "end": "2572430"
  },
  {
    "text": "So what this is saying is that\nthis projection is also equal",
    "start": "2572430",
    "end": "2579559"
  },
  {
    "text": "to the projection of v on the\nnormalized form of u, times",
    "start": "2579560",
    "end": "2586640"
  },
  {
    "text": "the normalized form of u. Which says that it doesn't make\nany difference what the",
    "start": "2586640",
    "end": "2593530"
  },
  {
    "text": "length of u is. This projection is a function\nonly of the direction of u.",
    "start": "2593530",
    "end": "2601670"
  },
  {
    "text": "I mean, this is obvious from\nthe picture, isn't it? ",
    "start": "2601670",
    "end": "2608900"
  },
  {
    "text": "But again, since we can't draw\npictures for complex valued things, it's nice to be able\nto see it analytically.",
    "start": "2608900",
    "end": "2616200"
  },
  {
    "text": "If I shorten u, or lengthen u,\nthis projection is still going",
    "start": "2616200",
    "end": "2621380"
  },
  {
    "text": "to be exactly the same thing. ",
    "start": "2621380",
    "end": "2626980"
  },
  {
    "text": "And that's what the norm squared\nof u is doing here. The norm squared of u is simply\nsitting there so it",
    "start": "2626980",
    "end": "2634060"
  },
  {
    "text": "does this normalization\nfunction for us. It makes this projection equal\nto the inner product of v with",
    "start": "2634060",
    "end": "2643520"
  },
  {
    "text": "the normalized form\nof u, times the normalized vector for u. ",
    "start": "2643520",
    "end": "2651780"
  },
  {
    "text": "The Pythagorean theorem, which\ndoesn't follow from this, it's something else, but it's simple\n-- in fact, we can do",
    "start": "2651780",
    "end": "2660170"
  },
  {
    "text": "it right away -- it says if v\nand u are orthogonal, then the norm squared of u plus\nv is equal to u",
    "start": "2660170",
    "end": "2669319"
  },
  {
    "text": "squared plus v squared. I mean, this is something\nyou use geometrically all the time.",
    "start": "2669320",
    "end": "2675090"
  },
  {
    "text": "And you're familiar with this. And the argument is, you just\nbreak this up into the norm squared of u plus the norm\nsquared of v, plus the two",
    "start": "2675090",
    "end": "2683450"
  },
  {
    "text": "cross-products, the inner\nproduct of u times the inner product of u with v, which is 0,\nand the inner product of v",
    "start": "2683450",
    "end": "2692410"
  },
  {
    "text": "with u, which is 0. So the two cross-terms\ngo away and you're left with just this.",
    "start": "2692410",
    "end": "2699700"
  },
  {
    "text": "And for any v and u, the Schwarz\ninequality says that the inner product, the magnitude\nof the inner product",
    "start": "2699700",
    "end": "2706720"
  },
  {
    "text": "of v and u, is less than or\nequal to the length of v times",
    "start": "2706720",
    "end": "2712140"
  },
  {
    "text": "the length of u.  The Schwarz inequality is\nprobably -- well, I'm not sure",
    "start": "2712140",
    "end": "2720290"
  },
  {
    "text": "that it's probably, but it's\nperhaps the most used inequality in mathematics.",
    "start": "2720290",
    "end": "2725680"
  },
  {
    "text": "Any time you use vectors, you\nuse this all the time. And it's extremely useful.",
    "start": "2725680",
    "end": "2731020"
  },
  {
    "text": "I'm not going to prove it here\nbecause it's in the notes. It's a two-step proof from\nwhat we've done.",
    "start": "2731020",
    "end": "2737940"
  },
  {
    "text": "And the trouble is watching\ntwo-step proofs in class.",
    "start": "2737940",
    "end": "2743020"
  },
  {
    "text": "At a certain point you\nsaturate on them. And I have more important things\nI want to do later",
    "start": "2743020",
    "end": "2748160"
  },
  {
    "text": "today, so I don't want you\nto saturate on this. You can read this at\nyour leisure and",
    "start": "2748160",
    "end": "2753590"
  },
  {
    "text": "see why this is true.  I did want to say something\nabout it, though.",
    "start": "2753590",
    "end": "2760620"
  },
  {
    "text": "If you divide the left side by\nthe right side, you can write",
    "start": "2760620",
    "end": "2765880"
  },
  {
    "text": "this as the magnitude of the\ninner product of the normalized form of v with the\nnormalized form of u.",
    "start": "2765880",
    "end": "2775780"
  },
  {
    "text": "If we're talking about real\nvector space, this in fact is the cosine of the angle\nbetween v and u.",
    "start": "2775780",
    "end": "2786390"
  },
  {
    "text": "It's less than or equal to 1. So for real two-dimensional\nvectors, the fact that the",
    "start": "2786390",
    "end": "2791410"
  },
  {
    "text": "cosine is less than or equal to\n1 is really equivalent to the Schwarz inequality.",
    "start": "2791410",
    "end": "2796920"
  },
  {
    "text": "And this is the appropriate\ngeneralization for any old vectors at all. ",
    "start": "2796920",
    "end": "2805900"
  },
  {
    "text": "And the notes would say more\nabout that if that went a",
    "start": "2805900",
    "end": "2811500"
  },
  {
    "text": "little too quickly. OK, the inner product space of\ninterest to us is this thing",
    "start": "2811500",
    "end": "2821430"
  },
  {
    "text": "we've called signal space. Namely, it's a space of\nfunctions which are measurable",
    "start": "2821430",
    "end": "2828550"
  },
  {
    "text": "n square integrals. In other words, finite value\nwhen you take the square and",
    "start": "2828550",
    "end": "2835730"
  },
  {
    "text": "integrate it. And we want to be able to talk\nabout the set of either real",
    "start": "2835730",
    "end": "2840789"
  },
  {
    "text": "or complex L2 functions. So, either one of them, we're\ngoing to define the inner",
    "start": "2840790",
    "end": "2848290"
  },
  {
    "text": "product in the same\nway for each. It really is the only natural\nway to define an inner product here.",
    "start": "2848290",
    "end": "2854260"
  },
  {
    "text": "And you'll see this more later\nas we start doing other things with these inner products.",
    "start": "2854260",
    "end": "2860089"
  },
  {
    "text": "But just like what when you're\ndealing with n-tuples, there's only one sensible way to define\nan inner product.",
    "start": "2860090",
    "end": "2867830"
  },
  {
    "text": "You can define inner products\nin other ways. But it's just a little\nbizarre to do so.",
    "start": "2867830",
    "end": "2874480"
  },
  {
    "text": "And here it's a little\nbizarre also. There's a big technical\nproblem here.",
    "start": "2874480",
    "end": "2881500"
  },
  {
    "text": "And if you look at, it can\nanybody spot what it -- no, no, of course you can't spot\nwhat it is unless you've read the notes and you\nknow what it is.",
    "start": "2881500",
    "end": "2889990"
  },
  {
    "text": "One of the axioms of an inner\nproduct space is that the only",
    "start": "2889990",
    "end": "2897070"
  },
  {
    "text": "vector in the space which has an\ninner product with itself,",
    "start": "2897070",
    "end": "2902310"
  },
  {
    "text": "a squared norm equal to zero\nis the zero vector.",
    "start": "2902310",
    "end": "2907810"
  },
  {
    "text": "Now, we have all these crazy\nvector we've been talking about, which are zero, except\non a set of measures zero.",
    "start": "2907810",
    "end": "2915880"
  },
  {
    "text": "In other words, vectors'\nfunctions which have zero energy but just pop up at\nvarious isolated points and",
    "start": "2915880",
    "end": "2925430"
  },
  {
    "text": "have values there. Which we really can't get rid\nof if we view a function as something which is defined\nat every value of t.",
    "start": "2925430",
    "end": "2934320"
  },
  {
    "text": "You have to accept those things\nas part of what you're dealing with. As soon as you started\nintegrating things, those",
    "start": "2934320",
    "end": "2939780"
  },
  {
    "text": "things all disappear. But the trouble is, those\nfunctions, which are zero",
    "start": "2939780",
    "end": "2945930"
  },
  {
    "text": "almost everywhere,\nare not zero. They're only zero almost\neverywhere. They're zero for all engineering\npurposes.",
    "start": "2945930",
    "end": "2953240"
  },
  {
    "text": "But they're not zero, they're\nonly zero almost everywhere. Well, if you define the inner\nproduct in this way and you",
    "start": "2953240",
    "end": "2964329"
  },
  {
    "text": "want to satisfy the axioms of an\ninner product space, you're out of luck.",
    "start": "2964330",
    "end": "2970250"
  },
  {
    "text": "There's no way you can\ndo it, because this axiom just gets violated.",
    "start": "2970250",
    "end": "2975529"
  },
  {
    "text": "So what do you do? Well, you do what we've\nbeen doing all along. We've been sort of squinting a\nlittle bit and saying, well,",
    "start": "2975530",
    "end": "2981940"
  },
  {
    "text": "really, these functions of\nmeasure 0 are really 0 for all",
    "start": "2981940",
    "end": "2987060"
  },
  {
    "text": "practical purposes. Mathematically, what we have\nto say is, we want to talk",
    "start": "2987060",
    "end": "2992819"
  },
  {
    "text": "about an equivalence\nclass of functions. And two functions are in the\nsame equivalence class if",
    "start": "2992820",
    "end": "3000510"
  },
  {
    "text": "their difference has 0 energy. Which is equivalent to saying\ntheir difference is zero",
    "start": "3000510",
    "end": "3008339"
  },
  {
    "text": "almost everywhere. It's one of these bizarre\nfunctions which just jumps up at isolated points and doesn't\ndo anything else.",
    "start": "3008340",
    "end": "3015890"
  },
  {
    "text": "Not impulses at isolated points,\njust non-zero at isolated points.",
    "start": "3015890",
    "end": "3021450"
  },
  {
    "text": "Impulses are not really\nfunctions at all. So we're talking about things\nthat are functions, but",
    "start": "3021450",
    "end": "3027010"
  },
  {
    "text": "they're these bizarre functions\nwhich we talked about and we've said they're\nunimportant. So, but they are there.",
    "start": "3027010",
    "end": "3035190"
  },
  {
    "text": "So the solution is to associate\nvectors with equivalence classes.",
    "start": "3035190",
    "end": "3040880"
  },
  {
    "text": "And d of t and u of t are\nequivalent if the v of t minus u of t is zero almost\neverywhere.",
    "start": "3040880",
    "end": "3047750"
  },
  {
    "text": "In other words, when we talk\nabout a vector, u, what we're talking about is an",
    "start": "3047750",
    "end": "3053050"
  },
  {
    "text": "equivalence class of functions. It's the equivalence class of\nfunctions for which two",
    "start": "3053050",
    "end": "3061640"
  },
  {
    "text": "functions are in the same\nequivalence class if they differ only on a set\nof measure zero.",
    "start": "3061640",
    "end": "3068020"
  },
  {
    "text": "In other words, these are the\nthings that gave us trouble when we were talking about\nFourier transforms. These are the things that gave\nus trouble when we were",
    "start": "3068020",
    "end": "3074380"
  },
  {
    "text": "talking about Fourier series. When you take anything in the\nsame equivalence class,",
    "start": "3074380",
    "end": "3081230"
  },
  {
    "text": "time-limited functions, and\nyou form a Fourier series, what happens?",
    "start": "3081230",
    "end": "3086830"
  },
  {
    "text": "All of the things in the same\nequivalence class have the same Fourier coefficients. ",
    "start": "3086830",
    "end": "3093710"
  },
  {
    "text": "But when you go back from the\nFourier series coefficients back to the function, then you\nmight go back in a bunch of",
    "start": "3093710",
    "end": "3100529"
  },
  {
    "text": "different ways. So, we started using this limit\nin the mean notation and all of that stuff.",
    "start": "3100530",
    "end": "3107250"
  },
  {
    "text": "And what we're doing here now\nis, for these vectors, we're just saying, let's represent\na vector as this whole",
    "start": "3107250",
    "end": "3114630"
  },
  {
    "text": "equivalence class. ",
    "start": "3114630",
    "end": "3123299"
  },
  {
    "text": "While we're talking about\nvectors, we're almost always interested in orthogonal\nexpansions.",
    "start": "3123300",
    "end": "3129460"
  },
  {
    "text": "And when we're interested in\northogonal expansions, the coefficients in the orthogonal\nexpansions are found as",
    "start": "3129460",
    "end": "3138400"
  },
  {
    "text": "integrals with the function. And the integrals with different\nfunctions in the",
    "start": "3138400",
    "end": "3143520"
  },
  {
    "text": "same equivalence class\nare identical. In other words, any two\nfunctions in the same",
    "start": "3143520",
    "end": "3148710"
  },
  {
    "text": "equivalence class have the\nsame coefficients in any orthogonal expansion.",
    "start": "3148710",
    "end": "3156430"
  },
  {
    "text": "So if you talk only about the\northogonal expansion, and leave out these detailed notions\nof what the function",
    "start": "3156430",
    "end": "3164645"
  },
  {
    "text": "is doing at individual times,\nthen you don't have to worry",
    "start": "3164645",
    "end": "3171310"
  },
  {
    "text": "about equivalence classes. In other words, when you -- I'm going to say this again more\ncarefully later, but let",
    "start": "3171310",
    "end": "3179020"
  },
  {
    "text": "me try to say it now a\nlittle bit crudely. One of the things we're\ninterested in doing this",
    "start": "3179020",
    "end": "3184500"
  },
  {
    "text": "taking this class of functions,\nmapping each function into a set of\ncoefficients, where the set of",
    "start": "3184500",
    "end": "3192660"
  },
  {
    "text": "coefficients are the\ncoefficients in some particular orthogonal expansion\nthat we're using.",
    "start": "3192660",
    "end": "3198400"
  },
  {
    "text": "Namely, that's the whole way\nthat we're using to get from waveforms to sequences.",
    "start": "3198400",
    "end": "3204610"
  },
  {
    "text": "It's the whole -- it's the entire thing we're\ndoing when we start out on a",
    "start": "3204610",
    "end": "3210620"
  },
  {
    "text": "channel with a sequence of\nbinary digits and then a sequence of symbols.",
    "start": "3210620",
    "end": "3216790"
  },
  {
    "text": "And we modulate it\ninto a waveform. Again, it's the mapping from\nsequence to waveform.",
    "start": "3216790",
    "end": "3226310"
  },
  {
    "text": "Now, the important thing here\nabout these equivalence classes is, you can't tell\nany of the members of the",
    "start": "3226310",
    "end": "3234790"
  },
  {
    "text": "equivalence class apart within\nthe sequence that we're dealing with.",
    "start": "3234790",
    "end": "3240900"
  },
  {
    "text": "Everything we're interested\nin has to do with these sequences. I mean, if we could we'd just\nignore the waveforms",
    "start": "3240900",
    "end": "3248180"
  },
  {
    "text": "altogether. Because all the processing that\nwe do is with sequences.",
    "start": "3248180",
    "end": "3254970"
  },
  {
    "text": "So the only reason we have these\nequivalence classes is because we need them to really\ndefine what the functions are.",
    "start": "3254970",
    "end": "3263970"
  },
  {
    "text": "So, we will come back\nto that later. ",
    "start": "3263970",
    "end": "3271780"
  },
  {
    "text": "Boy, I think I'm going\nto get done today. That's surprising. ",
    "start": "3271780",
    "end": "3277130"
  },
  {
    "text": "The next idea that\nwe want to talk about is vector subspaces. Again, that's an idea you've\nprobably heard of,",
    "start": "3277130",
    "end": "3285130"
  },
  {
    "text": "for the most part. A subspace of a vector space\nis a subset of the vector",
    "start": "3285130",
    "end": "3291250"
  },
  {
    "text": "space such that that subspace\nis a vector space in its own right.",
    "start": "3291250",
    "end": "3298440"
  },
  {
    "text": "An equivalent definition is,\nfor all vectors u and v, in",
    "start": "3298440",
    "end": "3304230"
  },
  {
    "text": "the subspace alpha\ntimes u plus beta times v is in s also.",
    "start": "3304230",
    "end": "3311390"
  },
  {
    "text": "In other words, a subspace is\nsomething which you can't get out of by linear combinations.",
    "start": "3311390",
    "end": "3317140"
  },
  {
    "text": " If I take one of these diagrams\nback here that I keep",
    "start": "3317140",
    "end": "3325760"
  },
  {
    "text": "looking at -- I'll use this one. ",
    "start": "3325760",
    "end": "3334590"
  },
  {
    "text": "If I want to form a subspace of\nthis subspace, if I want to form a subspace of this\ntwo-dimensional vector space",
    "start": "3334590",
    "end": "3342160"
  },
  {
    "text": "here, one of the subspaces\nincludes u, but not v, perhaps.",
    "start": "3342160",
    "end": "3348500"
  },
  {
    "text": "Now, if I want to make a\none-dimensional subspace including u, what is\nthat subspace?",
    "start": "3348500",
    "end": "3355760"
  },
  {
    "text": "It's just all the\nscalars times u. In other words, it's this line\nthat goes through the origin",
    "start": "3355760",
    "end": "3361119"
  },
  {
    "text": "and through that vector, u. And a subspace has to include\nthe whole line.",
    "start": "3361120",
    "end": "3367640"
  },
  {
    "text": "That's what we're saying. It's all scalar multiples\nof u. If I want a subspace that\nincludes u and v, where u and",
    "start": "3367640",
    "end": "3374760"
  },
  {
    "text": "v are just arbitrary vectors\nI've chosen out of my hat,",
    "start": "3374760",
    "end": "3379790"
  },
  {
    "text": "then I have this two-dimensional\nsubspace, which is what I've drawn here.",
    "start": "3379790",
    "end": "3384880"
  },
  {
    "text": "In other words, this\nidea is something we've been using already. It's just that we didn't need\nto be explicit about it.",
    "start": "3384880",
    "end": "3392010"
  },
  {
    "text": "The subspace which includes\nu and v --",
    "start": "3392010",
    "end": "3397440"
  },
  {
    "text": "well, a subspace which includes\nu and v, is the subspace of all linear\ncombinations of u and v. And",
    "start": "3397440",
    "end": "3404610"
  },
  {
    "text": "nothing else. So, it's all vectors\nalong here. It's all vectors along here. And you fill it all in with\nanything here added to",
    "start": "3404610",
    "end": "3413050"
  },
  {
    "text": "anything here. So you get this two-dimensional\nspace. ",
    "start": "3413050",
    "end": "3418900"
  },
  {
    "text": "Is 0 always in a subspace? Of course it is.",
    "start": "3418900",
    "end": "3424140"
  },
  {
    "text": "I mean, you multiply\nany vector by 0 and you get the 0 vector.",
    "start": "3424140",
    "end": "3429640"
  },
  {
    "text": "So you sort of get\nit as a linear -- ",
    "start": "3429640",
    "end": "3437410"
  },
  {
    "text": "what more can I say?  If we have a vector space which\nis an inner product",
    "start": "3437410",
    "end": "3445750"
  },
  {
    "text": "space; in other words, if we\nadd this inner product definition to our vector space,\nand I take a subspace",
    "start": "3445750",
    "end": "3453310"
  },
  {
    "text": "of it, that can be defined as\nan inner product space also,",
    "start": "3453310",
    "end": "3459880"
  },
  {
    "text": "with the same definition\nof inner product that I had before.",
    "start": "3459880",
    "end": "3465400"
  },
  {
    "text": "Because I can't get out of it\nby linear combinations, and the inner product is defined for\nevery pair of vectors in",
    "start": "3465400",
    "end": "3473450"
  },
  {
    "text": "that space. So we still have a nice,\nwell-defined vector space,",
    "start": "3473450",
    "end": "3479770"
  },
  {
    "text": "which is an inner product space,\nand which is a subspace",
    "start": "3479770",
    "end": "3485850"
  },
  {
    "text": "of the space we started with. Everything I do from now on, I'm\ngoing to assume that v is",
    "start": "3485850",
    "end": "3492280"
  },
  {
    "text": "an inner product space. And want to look at how\nwe normalize vectors.",
    "start": "3492280",
    "end": "3498569"
  },
  {
    "text": "We've already talked\nabout that.  If I have a vector in this\nvector space that's",
    "start": "3498570",
    "end": "3505400"
  },
  {
    "text": "normalized, if its\nnorm equals 1. We already decided how to\nnormalize a vector.",
    "start": "3505400",
    "end": "3511950"
  },
  {
    "text": "We took an arbitrary vector,\nu, divided by its norm. And as soon as we divide by\nits norm, that vector, v,",
    "start": "3511950",
    "end": "3521380"
  },
  {
    "text": "divided by the norm of v, the\nnorm of that is just 1.",
    "start": "3521380",
    "end": "3528539"
  },
  {
    "text": "So the projection, what the\nprojection theorem says, and all I need here is a\none-dimensional projection",
    "start": "3528540",
    "end": "3535020"
  },
  {
    "text": "theorem, it says that v, in the\ndirection of this vector",
    "start": "3535020",
    "end": "3540770"
  },
  {
    "text": "phi, is equal to the\ninner product of u with phi times phi.",
    "start": "3540770",
    "end": "3546000"
  },
  {
    "text": "That's what we said. As soon as we normalized these\nvectors, that ugly denominator here disappears.",
    "start": "3546000",
    "end": "3551230"
  },
  {
    "text": "Because the norm of\nphi is equal to -- because the norm\nis equal to 1.",
    "start": "3551230",
    "end": "3558760"
  },
  {
    "text": "So, an orthonormal set of\nvectors is a set such that",
    "start": "3558760",
    "end": "3563920"
  },
  {
    "text": "each pair of vectors is\northogonal to each other, and where each vector\nis normalized.",
    "start": "3563920",
    "end": "3570330"
  },
  {
    "text": "In other words, it has norm\nsquared equal to 1. So, the inner product of these\nvectors is just delta sub j k.",
    "start": "3570330",
    "end": "3580690"
  },
  {
    "text": " If I have an orthogonal set, v\nsub j, say, then phi sub j is",
    "start": "3580690",
    "end": "3590390"
  },
  {
    "text": "an orthonormal set just by\ntaking each of these vectors and normalizing it.",
    "start": "3590390",
    "end": "3595890"
  },
  {
    "text": "In other words, it's no big deal\nto take an orthogonal set of functions and turn\nthem into an",
    "start": "3595890",
    "end": "3601440"
  },
  {
    "text": "orthonormal set of functions. The Fourier series was natural\nto define that, and most",
    "start": "3601440",
    "end": "3607060"
  },
  {
    "text": "people define it, in such a way\nthat it's not orthonormal.",
    "start": "3607060",
    "end": "3612070"
  },
  {
    "text": "Because we're defining it over\nsome interval of time, t, that has a norm squared of t and,\ntherefore, you have to divide",
    "start": "3612070",
    "end": "3620250"
  },
  {
    "text": "by square root of t\nto normalize it. If you want to put everything\nin a common framework, it's",
    "start": "3620250",
    "end": "3627309"
  },
  {
    "text": "nice to deal with orthonormal\nseries. And therefore, that's what we're\ngoing to be stressing",
    "start": "3627310",
    "end": "3633510"
  },
  {
    "text": "from now on.  So I want to go on so the\nreal projection theorem.",
    "start": "3633510",
    "end": "3642670"
  },
  {
    "text": "Actually, there are three\nprojection theorems. There's the one-dimensional\nprojection theorem.",
    "start": "3642670",
    "end": "3648370"
  },
  {
    "text": "There's the n-dimensional\nprojection theorem, which is what this is. And then there's an\ninfinite-dimensional",
    "start": "3648370",
    "end": "3655200"
  },
  {
    "text": "projection theorem, which is\nnot general for all inner product spaces, but is certainly\ngeneral for L2.",
    "start": "3655200",
    "end": "3664859"
  },
  {
    "text": "So I'm going to assume that\nphi 1 to phi sub n is an",
    "start": "3664860",
    "end": "3670070"
  },
  {
    "text": "orthonormal basis for an\nn-dimensional subspace, s, which is a subspace of v. How\ndo I know there is such an",
    "start": "3670070",
    "end": "3677490"
  },
  {
    "text": "orthonormal basis? Well, I don't know that yet, and\nI'm going to come back to",
    "start": "3677490",
    "end": "3682820"
  },
  {
    "text": "that later. But for the time being, I'm just\ngoing to assume that as part of the theorem.",
    "start": "3682820",
    "end": "3688510"
  },
  {
    "text": "Assume I have some particular\nsubspace which has the",
    "start": "3688510",
    "end": "3695170"
  },
  {
    "text": "property that it has an\northonormal basis. So this is an orthonormal basis\nfor this n-dimensional",
    "start": "3695170",
    "end": "3703180"
  },
  {
    "text": "subspace, s and v. For each\nvector in v, s now is some",
    "start": "3703180",
    "end": "3710760"
  },
  {
    "text": "small subspace. v is a big subspace\nout around s.",
    "start": "3710760",
    "end": "3717430"
  },
  {
    "text": "What I want to do now is, I want\nto take some vector in the big subspace. I want to project it\nonto the subspace.",
    "start": "3717430",
    "end": "3724170"
  },
  {
    "text": " By projecting it onto the\nsubspace, what I mean is, I",
    "start": "3724170",
    "end": "3730410"
  },
  {
    "text": "want to find some vector in\nthe subspace such that the",
    "start": "3730410",
    "end": "3738210"
  },
  {
    "text": "difference between v and that\npoint in the subspace is orthogonal to the\nsubspace itself.",
    "start": "3738210",
    "end": "3745980"
  },
  {
    "text": "In other words, it's the same\nidea as we used before for",
    "start": "3745980",
    "end": "3753020"
  },
  {
    "text": "this over-used picture. Which I keep looking at.",
    "start": "3753020",
    "end": "3759910"
  },
  {
    "text": "Here, the subspace that I'm\nlooking at is just the subspace of vectors\ncolinear with u.",
    "start": "3759910",
    "end": "3768000"
  },
  {
    "text": "And what I'm trying to do here\nis to find, from v I'm trying to drop a perpendicular to this\nsubspace, which is just a",
    "start": "3768000",
    "end": "3776880"
  },
  {
    "text": "straight line. In general, what I'm trying\nto do is, I have an",
    "start": "3776880",
    "end": "3782490"
  },
  {
    "text": "n-dimensional subspace.  We can sort of visualize a\ntwo-dimensional subspace if",
    "start": "3782490",
    "end": "3789990"
  },
  {
    "text": "you think of this in\nthree dimensions. And think of replacing u\nwith some space, some",
    "start": "3789990",
    "end": "3796950"
  },
  {
    "text": "two-dimensional space, which\nis going through 0. And now I have this vector,\nv, which is",
    "start": "3796950",
    "end": "3803410"
  },
  {
    "text": "outside of that plane. And what I'm trying to do here\nis to drop a perpendicular",
    "start": "3803410",
    "end": "3809710"
  },
  {
    "text": "from v onto that subspace. The projection is where that\nperpendicular lands.",
    "start": "3809710",
    "end": "3816850"
  },
  {
    "text": "So, in other words, v, minus the\nprojection, is this v perp",
    "start": "3816850",
    "end": "3822260"
  },
  {
    "text": "we're talking about. And what I want to do is exactly\nthe same thing that we",
    "start": "3822260",
    "end": "3827520"
  },
  {
    "text": "did before with the one-dimensional projection theorem. And that's what we're\ngoing to do.",
    "start": "3827520",
    "end": "3834160"
  },
  {
    "text": "And it works. And it isn't really any more complicated, except for notation.",
    "start": "3834160",
    "end": "3842070"
  },
  {
    "text": "So the theorem says, assume\nyou have this orthonormal basis for this subspace.",
    "start": "3842070",
    "end": "3849180"
  },
  {
    "text": "And then you take any old v in\nthe entire vector space.",
    "start": "3849180",
    "end": "3854609"
  },
  {
    "text": "This can be an infinite\ndimensional vector space or anything else. And what we really want it to\nbe is some element in L2,",
    "start": "3854610",
    "end": "3863470"
  },
  {
    "text": "which is some\ninfinite-dimensional element. It says there's a unique\nprojection in the subspace s.",
    "start": "3863470",
    "end": "3874950"
  },
  {
    "text": "And it's given by the inner\nproduct of v with each of",
    "start": "3874950",
    "end": "3881220"
  },
  {
    "text": "those basis vectors. That inner product,\ntimes v sub j.",
    "start": "3881220",
    "end": "3886270"
  },
  {
    "text": "For the case of a\none-dimensional vector, you take the sum out and that\nwas exactly the",
    "start": "3886270",
    "end": "3891799"
  },
  {
    "text": "projection we had before. Now we just have the\nmulti-dimensional projection.",
    "start": "3891800",
    "end": "3897540"
  },
  {
    "text": "And it has a property that v\nis equal to this projection",
    "start": "3897540",
    "end": "3902820"
  },
  {
    "text": "plus the orthogonal thing. And the orthogonal thing, the\ninner product of that with s",
    "start": "3902820",
    "end": "3911599"
  },
  {
    "text": "equal to 0 for all s\nin the subspace. In other words, it's just what\nwe got in this picture we were",
    "start": "3911600",
    "end": "3918940"
  },
  {
    "text": "just trying to construct, of\na two-dimensional plane. You drop a perpendicular\ntwo-dimensional plane.",
    "start": "3918940",
    "end": "3925410"
  },
  {
    "text": "And when I drop a perpendicular\nto a two-dimensional plane, and I\ntake any old vector in this",
    "start": "3925410",
    "end": "3931040"
  },
  {
    "text": "two-dimensional plane,\nwe still have the perpendicularity.",
    "start": "3931040",
    "end": "3936890"
  },
  {
    "text": "In other words, you have the\nnotion of this vector being",
    "start": "3936890",
    "end": "3944529"
  },
  {
    "text": "perpendicular to a plane if it's\nperpendicular whichever way you look at it.",
    "start": "3944530",
    "end": "3950600"
  },
  {
    "text": "Let me outline the\nproof of this. Actually, it's pretty much\na complete proof. But with a couple small\ndetails left out.",
    "start": "3950600",
    "end": "3961970"
  },
  {
    "text": "We're going to start out the\nsame way I did before. Namely, I don't know how\nto choose this vector.",
    "start": "3961970",
    "end": "3968170"
  },
  {
    "text": "But I know I want to choose\nit to be in this subspace. And any element in this\nsubspace is a linear",
    "start": "3968170",
    "end": "3974940"
  },
  {
    "text": "combination of these\np sub i's. ",
    "start": "3974940",
    "end": "3980560"
  },
  {
    "text": "So this is just a generic\nelement in s. And I want to find out what\nelement I have to use.",
    "start": "3980560",
    "end": "3988040"
  },
  {
    "text": "I want to find the conditions\non these coefficients here such that v minus the projection\n-- in other words,",
    "start": "3988040",
    "end": "3996310"
  },
  {
    "text": "this v perp, as we've been\ncalling it -- is orthogonal to each phi sub i.",
    "start": "3996310",
    "end": "4002680"
  },
  {
    "text": "Now, if it's orthogonal to\neach phi sub i, it's also orthogonal to each linear\ncombination",
    "start": "4002680",
    "end": "4008400"
  },
  {
    "text": "of the phi sub i's. So in fact, that solved\nour problem for us. So what I want to do is, I want\nto set 0 equal to v minus",
    "start": "4008400",
    "end": "4018310"
  },
  {
    "text": "this projection. Where I don't yet know how to\nmake the projection, because I don't know what the\nalpha sub i's are.",
    "start": "4018310",
    "end": "4024890"
  },
  {
    "text": "But I'm trying to choose these\nso that this, minus the projection, the inner product of\nthat with phi j is equal to",
    "start": "4024890",
    "end": "4034589"
  },
  {
    "text": "0 for every j. This inner product here is equal\nto the inner product of",
    "start": "4034590",
    "end": "4045750"
  },
  {
    "text": "v with phi sub j. I have this difference here,\nso the inner product is the",
    "start": "4045750",
    "end": "4052420"
  },
  {
    "text": "inner product of v with phi sub\nj minus the inner product",
    "start": "4052420",
    "end": "4057460"
  },
  {
    "text": "of this with phi sub j. Let me write that here.",
    "start": "4057460",
    "end": "4064130"
  },
  {
    "text": " v minus sum alpha i phi sub i\ncomma phi sub j is equal to v",
    "start": "4064130",
    "end": "4083019"
  },
  {
    "text": "phi sub j minus summation\nof i alpha i t sub i.",
    "start": "4083020",
    "end": "4095370"
  },
  {
    "text": "phi sub j. Which is equal to this.",
    "start": "4095370",
    "end": "4100730"
  },
  {
    "text": "All of these terms are 0 except\nwhere j is equal to i. ",
    "start": "4100730",
    "end": "4110270"
  },
  {
    "text": "Where i is equal to j. So, alpha sub j has to be equal\nto the inner product of",
    "start": "4110270",
    "end": "4118589"
  },
  {
    "text": "v with this basis vector here. And, therefore, this projection\nis equal, which we",
    "start": "4118590",
    "end": "4127240"
  },
  {
    "text": "said was sum of alpha i phi sub\ni, that's really the sum of v phi sub j, this inner\nproduct, times p sub j,",
    "start": "4127240",
    "end": "4138339"
  },
  {
    "text": "Now, if you really use your\nimagination and you really think hard about the formula we\nwere using for the Fourier",
    "start": "4138340",
    "end": "4145579"
  },
  {
    "text": "series coefficients, was\nreally the same formula",
    "start": "4145580",
    "end": "4151350"
  },
  {
    "text": "without the normalization\nin it. It's simplified by being already\nnormalized for us. We don't have that 1 over t in\nhere, which we had in the",
    "start": "4151350",
    "end": "4159470"
  },
  {
    "text": "Fourier series because\nnow we've gone to orthonormal functions.",
    "start": "4159470",
    "end": "4164569"
  },
  {
    "text": " So, in fact that sort of\nproves the theorem.",
    "start": "4164570",
    "end": "4170400"
  },
  {
    "start": "4170400",
    "end": "4182199"
  },
  {
    "text": "If we express v as some linear\ncombination of these orthonormal vectors, then if I\ntake the norm squared of v,",
    "start": "4182200",
    "end": "4192960"
  },
  {
    "text": "this is something we've done\nmany times already. I just express the norm squared,\njust by expanding",
    "start": "4192960",
    "end": "4199570"
  },
  {
    "text": "this the sum of -- well, here\nI've done it this way, so",
    "start": "4199570",
    "end": "4208530"
  },
  {
    "text": "let's do it this way again. When I take the inner product\nof v with all of these terms here, I get the sum of alpha\nsub j complex conjugated,",
    "start": "4208530",
    "end": "4217640"
  },
  {
    "text": "times the inner product\nof v with phi sub j. But the inner product of v with\nphi sub j is just alpha",
    "start": "4217640",
    "end": "4225940"
  },
  {
    "text": "sub j times 1. So it's a sum of alpha\nsub j squared.",
    "start": "4225940",
    "end": "4231670"
  },
  {
    "text": "OK, this is this energy\nrelationship we've been using all along. We've been using it for\nthe Fourier series.",
    "start": "4231670",
    "end": "4237400"
  },
  {
    "text": "We've been using it for\neverything we've been doing. It's just a special case of\nthis relationship here, in",
    "start": "4237400",
    "end": "4245910"
  },
  {
    "text": "this n-dimensional projection,\nexcept that there we were dealing with infinite dimensions\nand here we're",
    "start": "4245910",
    "end": "4251050"
  },
  {
    "text": "dealing with finite\ndimensions. But it's the same formula, and\nyou'll see how it generalizes",
    "start": "4251050",
    "end": "4256500"
  },
  {
    "text": "in a little bit. We still have the Pythagorean\ntheorem, which in this case",
    "start": "4256500",
    "end": "4262400"
  },
  {
    "text": "says that the norm squared of\nvector v is equal to the norm squared of a projection, plus\nthe norm squared of the",
    "start": "4262400",
    "end": "4271199"
  },
  {
    "text": "perpendicular part. In other words, when I start\nto represent this vector outside of the space by a vector\ninside the space, by",
    "start": "4271200",
    "end": "4283639"
  },
  {
    "text": "this projection, I wind\nup with two things. I wind up both with the part\nthat's outside of the space",
    "start": "4283640",
    "end": "4290410"
  },
  {
    "text": "entirely, and is orthogonal to\nthe space, plus the part which is in the space.",
    "start": "4290410",
    "end": "4295510"
  },
  {
    "text": "And each of those has a certain\namount of energy. When I expand this by this\nrelationship here --",
    "start": "4295510",
    "end": "4305630"
  },
  {
    "text": "I'm not doing that yet -- what\nI'm doing here is what's called a norm bound.",
    "start": "4305630",
    "end": "4311930"
  },
  {
    "text": "Which says both of these\nterms are non-negative.",
    "start": "4311930",
    "end": "4317230"
  },
  {
    "text": "This term is non-negative in\nparticular, and therefore the difference between this and this\nis always positive, or",
    "start": "4317230",
    "end": "4325780"
  },
  {
    "text": "non-negative. Which says that 0 has to be less\nthan or equal to this,",
    "start": "4325780",
    "end": "4331200"
  },
  {
    "text": "because it's non-negative. And this has to be less than or\nequal to the norm of v. In",
    "start": "4331200",
    "end": "4337385"
  },
  {
    "text": "other words, the projection\nalways has less energy then the vector itself.",
    "start": "4337385",
    "end": "4345500"
  },
  {
    "text": "Which is not very surprising. So the norm bound\nis no big deal. When I substitute this for the\nactual value, what I get is",
    "start": "4345500",
    "end": "4356500"
  },
  {
    "text": "the sum j equals 1 to n of the\nnorm of the inner product of",
    "start": "4356500",
    "end": "4363310"
  },
  {
    "text": "v, with each one of these\nbasis vectors, magnitude",
    "start": "4363310",
    "end": "4368690"
  },
  {
    "text": "squared, that's less than or\nequal to the energy in v. In other words, if we start to\nexpand, as n gets bigger and",
    "start": "4368690",
    "end": "4377369"
  },
  {
    "text": "bigger, and we look at these\nterms, we take these inner products, square them.",
    "start": "4377370",
    "end": "4383780"
  },
  {
    "text": "No matter how many terms I take\nhere, the sum is always less than or equal to the energy\nin v. That's called",
    "start": "4383780",
    "end": "4392620"
  },
  {
    "text": "Bessel's inequality, and it's a\nnice, straightforward thing.",
    "start": "4392620",
    "end": "4401030"
  },
  {
    "text": "And finally, the last\nof these things --",
    "start": "4401030",
    "end": "4407435"
  },
  {
    "start": "4407436",
    "end": "4412500"
  },
  {
    "text": "well, I'll use the other\none if I need it.",
    "start": "4412500",
    "end": "4418520"
  },
  {
    "text": "The last is this thing\ncalled the mean square error property. ",
    "start": "4418520",
    "end": "4424670"
  },
  {
    "text": "It says that if you take the\ndifference between the vector and its projection onto this\nspace, this is less than or",
    "start": "4424670",
    "end": "4432130"
  },
  {
    "text": "equal to the difference between\nthe vector and the other s in the space. Any other --",
    "start": "4432130",
    "end": "4440670"
  },
  {
    "text": "I can always represent v as\nbeing equal to this plus the",
    "start": "4440670",
    "end": "4447050"
  },
  {
    "text": "orthogonal component. So I wind up with a sum\nhere of two terms.",
    "start": "4447050",
    "end": "4453010"
  },
  {
    "text": "One is the difference between\n-- well, it's the --",
    "start": "4453010",
    "end": "4458909"
  },
  {
    "text": " it's the length squared\nof the projection.",
    "start": "4458910",
    "end": "4469180"
  },
  {
    "text": "Write it out.  v minus v s -- let me write this\nterm out. v minus s is",
    "start": "4469180",
    "end": "4491410"
  },
  {
    "text": "equal to v, the projection, plus\nv perpendicular to the",
    "start": "4491410",
    "end": "4504220"
  },
  {
    "text": "subspace s minus\nthis vector s. ",
    "start": "4504220",
    "end": "4512820"
  },
  {
    "text": "This is perpendicular to this\nand this, so -- ah,",
    "start": "4512820",
    "end": "4519155"
  },
  {
    "text": "to hell with it. Excuse my language.",
    "start": "4519155",
    "end": "4524310"
  },
  {
    "text": "I mean, this is proven\nin the notes. I'm not going to go through it\nnow because I want to finish",
    "start": "4524310",
    "end": "4530600"
  },
  {
    "text": "these other things. I don't want to play\naround with it.",
    "start": "4530600",
    "end": "4536410"
  },
  {
    "text": "We left something out of the n-dimensional projection theorem.",
    "start": "4536410",
    "end": "4542690"
  },
  {
    "text": "How do you find an orthonormal\nbasis to start with? And there's this neat thing\ncalled Gram-Schmidt, which I",
    "start": "4542690",
    "end": "4550630"
  },
  {
    "text": "suspect most of you have\nseen before also. Which is pretty simple\nnow in terms of",
    "start": "4550630",
    "end": "4557310"
  },
  {
    "text": "the projection theorem. Gram-Schmidt is really a\nbootstrap operation starting",
    "start": "4557310",
    "end": "4562890"
  },
  {
    "text": "with the one-dimensional\nprojection theorem, working your way up to larger and\nlarger dimensions.",
    "start": "4562890",
    "end": "4568830"
  },
  {
    "text": "And each case winding up with an\northonormal basis for what you started with. Let's see how that happens.",
    "start": "4568830",
    "end": "4576420"
  },
  {
    "text": "I start out with a basis for\nan inner product subspace. So, s1 up to s sub n\nas a basis for this",
    "start": "4576420",
    "end": "4584390"
  },
  {
    "text": "inner product space. First thing I do is, I\nstart out with s1.",
    "start": "4584390",
    "end": "4589970"
  },
  {
    "text": "I find the normalized\nversion of s1. I call that phi 1. So phi 1 is now an orthonormal\nbasis for the subspace whose",
    "start": "4589970",
    "end": "4604740"
  },
  {
    "text": "basis is just phi 1 itself. ",
    "start": "4604740",
    "end": "4610280"
  },
  {
    "text": "phi 1 is the basis for the\nsubspace of all linear combinations of s1.",
    "start": "4610280",
    "end": "4617469"
  },
  {
    "text": "So it's just a straight\nline in space. The next thing I do\nis, I take s2.",
    "start": "4617470",
    "end": "4624520"
  },
  {
    "text": "I find the projection of\ns2 on this subspace s1. I can do that.",
    "start": "4624520",
    "end": "4631480"
  },
  {
    "text": "So I find a part which\nis colinear with s1. I find the part which\nis orthogonal.",
    "start": "4631480",
    "end": "4638240"
  },
  {
    "text": "I take the orthogonal\npart, and that's orthogonal, to phi 1. And I normalize it.",
    "start": "4638240",
    "end": "4645300"
  },
  {
    "text": "So I then have two vectors, phi\n1 and phi 2, which span the space of functions\nof linear",
    "start": "4645300",
    "end": "4652810"
  },
  {
    "text": "combinations of s1 and s2.  And I call that subspace\nS2, capital S2.",
    "start": "4652810",
    "end": "4665040"
  },
  {
    "text": "And then I go on. So, given any orthonormal basis,\nphi 1 up to phi sub k",
    "start": "4665040",
    "end": "4670930"
  },
  {
    "text": "of the subspace s k generated\nby s1 to s k, I'm going to project s k plus 1 onto this\nsubspace s sub k, and then I'm",
    "start": "4670930",
    "end": "4680730"
  },
  {
    "text": "going to normalize it. And by going through this\nprocedure, I can in fact find",
    "start": "4680730",
    "end": "4687210"
  },
  {
    "text": "an orthonormal basis to any set\nof vectors that I want to, to any subspace that\nI want to.",
    "start": "4687210",
    "end": "4692809"
  },
  {
    "text": "Why is this important, is this\nsomething you want to do? Well, it's something you can\nprogram a computer to do",
    "start": "4692810",
    "end": "4700310"
  },
  {
    "text": "almost trivially. But that's not why\nwe want it here. The thing we want it here is to\nsay that there's no reason",
    "start": "4700310",
    "end": "4708780"
  },
  {
    "text": "to deal with bases other\nthan orthonormal bases. We can generate orthonormal\nbases easily.",
    "start": "4708780",
    "end": "4715280"
  },
  {
    "text": "The projection theorem now is\nvalid for any n-dimensional space because for any\nn-dimensional space we can",
    "start": "4715280",
    "end": "4722230"
  },
  {
    "text": "form this basis that we want. Let me just go on and finish\nthis, so we can start dealing",
    "start": "4722230",
    "end": "4733480"
  },
  {
    "text": "with channels next time. So far, the projection\ntheorem is just for",
    "start": "4733480",
    "end": "4741369"
  },
  {
    "text": "finite dimensional vectors. We want to now extend it to\ninfinite dimensional vectors.",
    "start": "4741370",
    "end": "4748660"
  },
  {
    "text": "To accountably infinite\nset of vectors. So I'm given any orthogonal set\nof functions, status sub",
    "start": "4748660",
    "end": "4756790"
  },
  {
    "text": "i, we can first generate\northonormal functions as phi sub i, which are normalized.",
    "start": "4756790",
    "end": "4763870"
  },
  {
    "text": "And that's old stuff. I can now think of doing the\nsame thing that we did before.",
    "start": "4763870",
    "end": "4770750"
  },
  {
    "text": "Namely, starting out, taking any\nold vector I want to, and",
    "start": "4770750",
    "end": "4776800"
  },
  {
    "text": "projecting it first on\nto the subspace with only phi 1 in it.",
    "start": "4776800",
    "end": "4781860"
  },
  {
    "text": "Then the subspace generated by\nphi 1 and phi 2, then the subspace generated by phi 1, phi\n2 and phi 3, and so forth.",
    "start": "4781860",
    "end": "4791219"
  },
  {
    "text": "When I do that successively,\nwhich is successive approximations in a Fourier\nexpansion, or in any",
    "start": "4791220",
    "end": "4800420"
  },
  {
    "text": "orthonormal expansion, what I'm\ngoing to wind up with is",
    "start": "4800420",
    "end": "4806100"
  },
  {
    "text": "the following theorem that says,\nlet phi sub m be a set of orthonormal functions.",
    "start": "4806100",
    "end": "4812090"
  },
  {
    "text": "Let v be any L2 vector. Then there exists an L2 vector,\nu, such that v minus u",
    "start": "4812090",
    "end": "4820140"
  },
  {
    "text": "is orthogonal to\neach phi sub n. In other words, this is the\nprojection theorem, carried on",
    "start": "4820140",
    "end": "4826719"
  },
  {
    "text": "as n goes to infinity. But I can't quite state it\nin the way I did before. I need a limit in here.",
    "start": "4826720",
    "end": "4833180"
  },
  {
    "text": "Which says the limit as n goes\nto infinity of u, namely what",
    "start": "4833180",
    "end": "4840280"
  },
  {
    "text": "is now going to be this\nprojection, minus this term",
    "start": "4840280",
    "end": "4845610"
  },
  {
    "text": "here, which is the term\nin the subspace of these orthonormal functions.",
    "start": "4845610",
    "end": "4851570"
  },
  {
    "text": "This difference goes to zero. What does this say? It doesn't say that I can take\nany function, v, and expand it",
    "start": "4851570",
    "end": "4860360"
  },
  {
    "text": "in an orthonormal expansion. I couldn't say that, because I\nhave nothing to know whether",
    "start": "4860360",
    "end": "4866840"
  },
  {
    "text": "this arbitrary orthonormal\nexpansion I started with actually spans L2 or not.",
    "start": "4866840",
    "end": "4873130"
  },
  {
    "text": "And without knowing that,\nI can't state a theorem like this.",
    "start": "4873130",
    "end": "4878210"
  },
  {
    "text": "But what the theorem does say\nis, you take any orthonormal expansion you want to.",
    "start": "4878210",
    "end": "4883250"
  },
  {
    "text": "Like the Fourier series, which\nonly spans functions which are time limited.",
    "start": "4883250",
    "end": "4888480"
  },
  {
    "text": "I take an arbitrary function. I expand them in this\nFourier series. And, bingo, what I get is a\nfunction u, which is the part",
    "start": "4888480",
    "end": "4896780"
  },
  {
    "text": "of v, which is within these time\nlimits and what's left over, which is orthogonal, is\nthe stuff outside of those",
    "start": "4896780",
    "end": "4903930"
  },
  {
    "text": "time limits. So this is the theorem that says\nthat in fact you can --",
    "start": "4903930",
    "end": "4911420"
  },
  {
    "text": "AUDIENCE: [UNINTELLIGIBLE] PROFESSOR: What? AUDIENCE: [UNINTELLIGIBLE] PROFESSOR: It's similar\nto Plancherel.",
    "start": "4911420",
    "end": "4919290"
  },
  {
    "text": "Plancherel is done for the\nFourier integral, and in",
    "start": "4919290",
    "end": "4924300"
  },
  {
    "text": "Plancherel you need this limit\nin the mean on both sides. Here we're just dealing\nwith a series.",
    "start": "4924300",
    "end": "4933280"
  },
  {
    "text": "I mean, we still have a limit in\nthe mean sort of thing, but we have a weaker theorem in\nthe sense that we're not",
    "start": "4933280",
    "end": "4940720"
  },
  {
    "text": "asserting that this orthonormal\nseries actually spans all of L2.",
    "start": "4940720",
    "end": "4946750"
  },
  {
    "text": "I mean, we found a couple of\northonormal expansions that do span L2.",
    "start": "4946750",
    "end": "4952380"
  },
  {
    "text": "So it's lacking in that. OK, going to stop there. ",
    "start": "4952380",
    "end": "4957759"
  }
]