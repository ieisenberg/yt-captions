[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "In discussing out-of-order superscalar pipelined\nCPUs we commented that the costs grow very",
    "start": "880",
    "end": "6690"
  },
  {
    "text": "quickly relative to the performance gains,\nleading to the cost-performance curve shown",
    "start": "6690",
    "end": "11470"
  },
  {
    "text": "here.",
    "start": "11470",
    "end": "12820"
  },
  {
    "text": "If we move down the curve, we can arrive at\nmore efficient architectures that give, say,",
    "start": "12820",
    "end": "17109"
  },
  {
    "text": "1/2 the performance at a 1/4 of the cost.",
    "start": "17109",
    "end": "20189"
  },
  {
    "text": "When our applications involve independent\ncomputations that can be performed in a parallel,",
    "start": "20189",
    "end": "24980"
  },
  {
    "text": "it may be that we would be able to use two\ncores to provide the same performance as the",
    "start": "24980",
    "end": "29859"
  },
  {
    "text": "original expensive core, but a fraction of\nthe cost.",
    "start": "29859",
    "end": "34399"
  },
  {
    "text": "If the available parallelism allows us to\nuse additional cores, we'll see a linear relationship",
    "start": "34400",
    "end": "39590"
  },
  {
    "text": "between increased performance vs. increased\ncost.",
    "start": "39590",
    "end": "43740"
  },
  {
    "text": "The key, of course, is that desired computations\ncan be divided into multiple tasks that can",
    "start": "43740",
    "end": "48290"
  },
  {
    "text": "run independently, with little or no need\nfor communication or coordination between",
    "start": "48290",
    "end": "53070"
  },
  {
    "text": "the tasks.",
    "start": "53070",
    "end": "55239"
  },
  {
    "text": "What is the optimal tradeoff between core\ncost and the number of cores?",
    "start": "55240",
    "end": "60160"
  },
  {
    "text": "If our computation is arbitrarily divisible\nwithout incurring additional overhead,",
    "start": "60160",
    "end": "64760"
  },
  {
    "text": "then we would continue to move down the curve\nuntil we found the cost-performance point",
    "start": "64760",
    "end": "69000"
  },
  {
    "text": "that gave us the desired performance at the\nleast cost.",
    "start": "69000",
    "end": "72380"
  },
  {
    "text": "In reality, dividing the computation across\nmany cores does involve some overhead, e.g.,",
    "start": "72380",
    "end": "78259"
  },
  {
    "text": "distributing the data and code, then collecting\nand aggregating the results, so the optimal",
    "start": "78260",
    "end": "83499"
  },
  {
    "text": "tradeoff is harder to find.",
    "start": "83499",
    "end": "85729"
  },
  {
    "text": "Still, the idea of using a larger number of\nsmaller, more efficient cores seems attractive.",
    "start": "85729",
    "end": "92159"
  },
  {
    "text": "Many applications have some computations that\ncan be performed in parallel, but also have",
    "start": "92159",
    "end": "96770"
  },
  {
    "text": "computations that won't benefit from parallelism.",
    "start": "96770",
    "end": "100390"
  },
  {
    "text": "To understand the speedup we might expect\nfrom exploiting parallelism, it's useful to",
    "start": "100390",
    "end": "104530"
  },
  {
    "text": "perform the calculation proposed by computer\nscientist Gene Amdahl in 1967, now known as",
    "start": "104530",
    "end": "110899"
  },
  {
    "text": "Amdahl's Law.",
    "start": "110899",
    "end": "113119"
  },
  {
    "text": "Suppose we're considering an enhancement that\nspeeds up some fraction F of the task at hand",
    "start": "113119",
    "end": "117429"
  },
  {
    "text": "by a factor of S.\nAs shown in the figure, the gray portion of",
    "start": "117429",
    "end": "121899"
  },
  {
    "text": "the task now takes F/S of the time that it\nused to require.",
    "start": "121899",
    "end": "127299"
  },
  {
    "text": "Some simple arithmetic lets us calculate the\noverall speedup we get from using the enhancement.",
    "start": "127299",
    "end": "134060"
  },
  {
    "text": "One conclusion we can draw is that we'll benefit\nthe most from enhancements that affect a large",
    "start": "134060",
    "end": "138920"
  },
  {
    "text": "portion of the required computations, i.e.,\nwe want to make F as large a possible.",
    "start": "138920",
    "end": "145580"
  },
  {
    "text": "What's the best speedup we can hope for if\nwe have many cores that can be used to speed",
    "start": "145580",
    "end": "149980"
  },
  {
    "start": "146000",
    "end": "146000"
  },
  {
    "text": "up the parallel part of the task?",
    "start": "149980",
    "end": "152700"
  },
  {
    "text": "Here's the speedup formula based on F and\nS, where in this case F is the parallel fraction",
    "start": "152700",
    "end": "158700"
  },
  {
    "text": "of the task.",
    "start": "158700",
    "end": "160580"
  },
  {
    "text": "If we assume that the parallel fraction of\nthe task can be speed up arbitrarily by using",
    "start": "160580",
    "end": "164610"
  },
  {
    "text": "more and more cores, we see that the best\npossible overall speed up is 1/(1-F).",
    "start": "164610",
    "end": "169220"
  },
  {
    "text": "For example, you write a program that can\ndo 90% of its work in parallel, but the other",
    "start": "169220",
    "end": "177020"
  },
  {
    "text": "10% must be done sequentially.",
    "start": "177020",
    "end": "180230"
  },
  {
    "text": "The best overall speedup that can be achieved\nis a factor of 10, no matter how many cores",
    "start": "180230",
    "end": "185319"
  },
  {
    "text": "you have at your disposal.",
    "start": "185320",
    "end": "188250"
  },
  {
    "text": "Turning the question around, suppose you have\na 1000-core machine which you hope to be able",
    "start": "188250",
    "end": "192230"
  },
  {
    "text": "to use to achieve a speedup of 500 on your\ntarget application.",
    "start": "192230",
    "end": "196959"
  },
  {
    "text": "You would need to be able parallelize 99.8%\nof the computation in order to reach your",
    "start": "196960",
    "end": "202970"
  },
  {
    "text": "goal!",
    "start": "202970",
    "end": "204290"
  },
  {
    "text": "Clearly multicore machines are most useful\nwhen the target task has lots of natural parallelism.",
    "start": "204290",
    "end": "212170"
  },
  {
    "start": "212000",
    "end": "212000"
  },
  {
    "text": "Using multiple independent cores to execute\na parallel task is called thread-level parallelism",
    "start": "212170",
    "end": "217440"
  },
  {
    "text": "(TLP), where each core executes a separate\ncomputation \"thread\".",
    "start": "217440",
    "end": "222180"
  },
  {
    "text": "The threads are independent programs, so the\nexecution model is potentially more flexible",
    "start": "222180",
    "end": "227000"
  },
  {
    "text": "than the lock-step execution provided by vector\nmachines.",
    "start": "227000",
    "end": "232120"
  },
  {
    "text": "When there are a small number of threads,\nyou often see the cores sharing a common main",
    "start": "232120",
    "end": "236299"
  },
  {
    "text": "memory, allowing the threads to communicate\nand synchronize by sharing a common address",
    "start": "236300",
    "end": "241030"
  },
  {
    "text": "space.",
    "start": "241030",
    "end": "242030"
  },
  {
    "text": "We'll discuss this further in the next section.",
    "start": "242030",
    "end": "245230"
  },
  {
    "text": "This is the approach used in current multicore\nprocessors, which have between 2 and 12 cores.",
    "start": "245230",
    "end": "252489"
  },
  {
    "text": "Shared memory becomes a real bottleneck when\nthere 10's or 100's of cores, since collectively",
    "start": "252490",
    "end": "257380"
  },
  {
    "text": "they quickly overwhelm the available memory\nbandwidth.",
    "start": "257380",
    "end": "260489"
  },
  {
    "text": "In these architectures, threads communicate\nusing a communication network to pass messages",
    "start": "260490",
    "end": "266250"
  },
  {
    "text": "back and forth.",
    "start": "266250",
    "end": "268210"
  },
  {
    "text": "We discussed possible network topologies in\nan earlier lecture.",
    "start": "268210",
    "end": "272470"
  },
  {
    "text": "A cost-effective on-chip approach is to use\na nearest-neighbor mesh network, which supports",
    "start": "272470",
    "end": "278170"
  },
  {
    "text": "many parallel point-to-point communications,\nwhile still allowing multi-hop communication",
    "start": "278170",
    "end": "283790"
  },
  {
    "text": "between any two cores.",
    "start": "283790",
    "end": "286870"
  },
  {
    "text": "Message passing is also used in computing\nclusters, where many ordinary CPUs collaborate",
    "start": "286870",
    "end": "292460"
  },
  {
    "text": "on large tasks.",
    "start": "292460",
    "end": "294560"
  },
  {
    "text": "There's a standardized message passing interface\n(MPI) and",
    "start": "294560",
    "end": "298590"
  },
  {
    "text": "specialized, very high throughput, low latency\nmessage-passing communication networks (e.g.,",
    "start": "298590",
    "end": "304750"
  },
  {
    "text": "Infiniband) that make it easy to build high-performance\ncomputing clusters.",
    "start": "304750",
    "end": "309310"
  },
  {
    "text": "In the next couple of sections we'll look\nmore closely at some of the issues involved",
    "start": "309310",
    "end": "313970"
  },
  {
    "text": "in building shared-memory multicore processors.",
    "start": "313970",
    "end": "316650"
  }
]