[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5580"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high-quality\neducational resources for free.",
    "start": "5580",
    "end": "12270"
  },
  {
    "text": "To make a donation or\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "12270",
    "end": "18830"
  },
  {
    "text": "at ocw.mit.edu. LORENZO ROSASCO:\nIf you remember, we did the local\nmethod bias-variance.",
    "start": "18830",
    "end": "26420"
  },
  {
    "text": "Then we passed to global\nregularization methods-- least squares, linear least\nsquares, kernel least squares,",
    "start": "26420",
    "end": "31790"
  },
  {
    "text": "computations modeling. And that's where we were at. And then we moved on\nand started to think about more intractable\nmodel, and we",
    "start": "31790",
    "end": "40070"
  },
  {
    "text": "were starting to think of the\nproblem of variable selection, OK?",
    "start": "40070",
    "end": "45110"
  },
  {
    "text": "And the way we posed it is\nthat, yeah, that you are going to consider a linear model.",
    "start": "45110",
    "end": "50180"
  },
  {
    "text": "And I use the weights\nassociated to each variable as the strength of the\ncorresponding variable",
    "start": "50180",
    "end": "57260"
  },
  {
    "text": "that you can view\nas measurements. And your game is\nnot only to build good predictional\nmeasurements, but also",
    "start": "57260",
    "end": "62360"
  },
  {
    "text": "to tell which measurements\nare interesting, OK? And so here, the term\n\"relevant variable\"",
    "start": "62360",
    "end": "71330"
  },
  {
    "text": "is going to be related to the\nproductivity, the contribution",
    "start": "71330",
    "end": "77870"
  },
  {
    "text": "to the productivity of the\ncorresponding function, OK? So that's how we measure\nrelevance of a variable.",
    "start": "77870",
    "end": "84600"
  },
  {
    "text": "So we looked at the funny name. And then we kind of\nagreed that somewhat it",
    "start": "84600",
    "end": "91610"
  },
  {
    "text": "seems that there is a\ndefault approach, which",
    "start": "91610",
    "end": "97340"
  },
  {
    "text": "is basically based on trying\nall possible subsets, OK? So this is also called\nbest subset selection.",
    "start": "97340",
    "end": "102869"
  },
  {
    "text": "Variable selection is\nalso sometimes called best subset selection. And this gives you a feeling\nthat what you should do is try all possible subsets\nand check the one which",
    "start": "102869",
    "end": "110420"
  },
  {
    "text": "is best with respect to your\ndata, which, again, would be like a trade-off between\nhow well you fit the data",
    "start": "110420",
    "end": "116540"
  },
  {
    "text": "and how many variables\nyou have, OK? And what I told you last\nwas you could actually",
    "start": "116540",
    "end": "124820"
  },
  {
    "text": "see that this trying\nall possible subsets is related to a form\nof regularization,",
    "start": "124820",
    "end": "131780"
  },
  {
    "text": "where it looks very\nsimilar to the one we saw until a minute ago. The main difference\nhere, I put fw,",
    "start": "131780",
    "end": "137220"
  },
  {
    "text": "but fw is just the\nusual linear function. The only difference\nis that here, rather than the square\nnorm, we put this functional",
    "start": "137220",
    "end": "144380"
  },
  {
    "text": "that is called the\n0 norm, which if, as a functional,\nthat given a vector returns the number of\nentries in the vector which",
    "start": "144380",
    "end": "151880"
  },
  {
    "text": "are different from 0, OK? It turns out that if you\nwere to minimize this,",
    "start": "151880",
    "end": "157550"
  },
  {
    "text": "you are solving the best\nsubset selection problem. Issue here is that--\nanother manifestation",
    "start": "157550",
    "end": "165080"
  },
  {
    "text": "of the complexity of\nthe problem is the fact that this functional\nhere is non-convex. So there is no\npolynomial algorithm",
    "start": "165080",
    "end": "171500"
  },
  {
    "text": "to actually find a solution. ",
    "start": "171500",
    "end": "177209"
  },
  {
    "text": "Come to my mind that\nsomebody made the comment during the break. Notice that here I'm\na bit passing quickly",
    "start": "177210",
    "end": "185100"
  },
  {
    "text": "over a refinement of the\nquestion of best subset selection, which is\nrelated to, is there a unique subset which is good?",
    "start": "185100",
    "end": "192600"
  },
  {
    "text": "Is there more than one? And if there's more than one,\nwhich one should I pick, OK? In practice, these\nquestions arise immediately",
    "start": "192600",
    "end": "199260"
  },
  {
    "text": "because if you have\ntwo measurements that are very correlated,\nor even more, if they're perfectly\ncorrelated, if you just",
    "start": "199260",
    "end": "205950"
  },
  {
    "text": "build the measurements,\nyou might build out of two measurements, a\nthird measurement, which is completely just a linear\ncombination of the first two.",
    "start": "205950",
    "end": "214187"
  },
  {
    "text": "So at that point, what\nwould you want to do? Do you want to keep\nthe minimum number of variables, the biggest\npossible number of variables?",
    "start": "214187",
    "end": "220560"
  },
  {
    "text": "And you have to decide,\nbecause all these variables are, to some extent,\ncompletely dependent, OK?",
    "start": "220560",
    "end": "227430"
  },
  {
    "text": "So for now, we just\nkeep to the case where we don't really\nworry about this, OK? We just say, among the good\nones, we want to pick one.",
    "start": "227430",
    "end": "235470"
  },
  {
    "text": "A harder question would\nbe pick all of them or pick one of them. And if you wanted to pick one\nof them, you have to tell me",
    "start": "235470",
    "end": "241406"
  },
  {
    "text": "which one you want, according\nto which criterion, OK?  So the problem we're concerned\nwith now is one of, OK, now",
    "start": "241406",
    "end": "251250"
  },
  {
    "text": "that we know that we\nmight want to do this,",
    "start": "251250",
    "end": "256390"
  },
  {
    "text": "how can you do it in\nan approximate way that will be good enough, and what\ndoes it mean, good enough?",
    "start": "256390",
    "end": "263850"
  },
  {
    "text": "So the simplest\nway, again, we can try to think about\nit together, is",
    "start": "263850",
    "end": "270410"
  },
  {
    "text": "kind of a greedy version of\nthe brute force approach. So the brute force\napproach was start",
    "start": "270410",
    "end": "277520"
  },
  {
    "text": "from all single variables,\nthen all couples, then all triplets, and\nblah, blah, blah, blah, OK?",
    "start": "277520",
    "end": "283380"
  },
  {
    "text": "And this doesn't\nwork computationally. So just to wake\nup, I don't know,",
    "start": "283380",
    "end": "289620"
  },
  {
    "text": "how could you\ntwist this approach to make it approximate, but\ncomputationally feasible?",
    "start": "289620",
    "end": "298010"
  },
  {
    "text": "Let's keep the same spirit. So let's start from few, and\nthen let's try to add more.",
    "start": "298010",
    "end": "303480"
  },
  {
    "text": "So the general\nidea is I pick one. And once I pick one, I\npick another one, just",
    "start": "303480",
    "end": "309030"
  },
  {
    "text": "keeping the one\nI already picked. And then I pick another one,\nand then I pick another one, and then I pick another one. So this, of course, will\nnot be the exhaustive search",
    "start": "309030",
    "end": "318060"
  },
  {
    "text": "I've done before. It's probably doable. There is a bunch of\ndifferent ways you can do it.",
    "start": "318060",
    "end": "323550"
  },
  {
    "text": "And you can hopefully-- you can hope that\nunder some condition you might be able to prove\nthat it's not too far",
    "start": "323550",
    "end": "329100"
  },
  {
    "text": "away from the brute\nforce approach, at least under some condition.",
    "start": "329100",
    "end": "334590"
  },
  {
    "text": "And this is kind of what\nwe would want to do, OK? So we will have a\nnotion of residual.",
    "start": "334590",
    "end": "341360"
  },
  {
    "text": "At the first\niteration, the residual will be just the output. So just think of\nthe first iteration.",
    "start": "341360",
    "end": "348300"
  },
  {
    "text": "You get the output vectors,\nand you want to explain it. You want to predict it well, OK?",
    "start": "348300",
    "end": "354090"
  },
  {
    "text": "So what you do is that you first\ncheck the one variable that",
    "start": "354090",
    "end": "359880"
  },
  {
    "text": "gives you the best\nprediction of this guy, and then you compute\nthe prediction. Then what you want to\ndo at the next round,",
    "start": "359880",
    "end": "366629"
  },
  {
    "text": "you want to discount what\nyou have already explained. And what you do is\nthat basically you take the actual output\nminus your prediction,",
    "start": "366630",
    "end": "373870"
  },
  {
    "text": "and you find the residual. And then you try\nto explain that. That's what's left\nto explain, OK?",
    "start": "373870",
    "end": "378930"
  },
  {
    "text": "So now you check\nfor the variables that best explain\nthis remaining bit.",
    "start": "378930",
    "end": "384840"
  },
  {
    "text": "Then you add this variable\nto the one you already have, and you have a new\nnotion of residual, which is what you explained\nin the first round, what",
    "start": "384840",
    "end": "392160"
  },
  {
    "text": "you added in explanation\nof the second round. And then there's still something\nleft, and you keep on going.",
    "start": "392160",
    "end": "397980"
  },
  {
    "text": "If you let this thing\ngo for enough time, you will have the\nleast squares solution. At the end of the day, you\nwill have explained everything.",
    "start": "397980",
    "end": "406500"
  },
  {
    "text": "And each round, notice\nthat you might or not decide to put the\nvariables back in, OK?",
    "start": "406500",
    "end": "413742"
  },
  {
    "text": "So you might have\nthat at each step you have just one\nvariable, or you might have that you\ntake multiple steps, but you have fewer variables\nthan number of steps.",
    "start": "413742",
    "end": "420840"
  },
  {
    "text": "No matter what,\nthe number of steps would be related to the\nnumber of variables that",
    "start": "420840",
    "end": "426030"
  },
  {
    "text": "are active in your model, OK? Does it makes sense? This is the wordy version,\nbut now we go in details, OK?",
    "start": "426030",
    "end": "433606"
  },
  {
    "text": "But this is roughly\nthe speaking. So first round, you try\nto explain something, then you see what's\nleft to explain.",
    "start": "433606",
    "end": "439199"
  },
  {
    "text": "And you keep the variables\nthat will explain the rest, and then you need to write. I'm not sure I used the\nword, but it's important.",
    "start": "439200",
    "end": "446160"
  },
  {
    "text": "The key word here\nis \"sparsity,\" OK? The fact that I'm\nassuming my model to depend on just\na few vectors--",
    "start": "446160",
    "end": "453040"
  },
  {
    "text": "sorry a few entries. So it's a vector with\nmany zero entries. Sparsity is the key word to\nexplain this property, which",
    "start": "453040",
    "end": "460327"
  },
  {
    "text": "is a property of the problem. And so I build\nalgorithm that will try to find sparse solution\nexplaining my data,",
    "start": "460327",
    "end": "469710"
  },
  {
    "text": "and this is one way. So let's look at this list. So you define the notion\nof residual as this thing",
    "start": "469710",
    "end": "476293"
  },
  {
    "text": "that you want to try to\nexplain, and the first round will be the output. The second round will be\nwhat's left to explain after your prediction.",
    "start": "476294",
    "end": "482900"
  },
  {
    "text": "You have a coefficient\nvector, OK, because we're building\na linear function. And then you have\nan index set, which",
    "start": "482900",
    "end": "489560"
  },
  {
    "text": "is the set of variables which\nare important at that stage. So these are the three objects\nthat you have to initialize.",
    "start": "489560",
    "end": "495505"
  },
  {
    "text": "So at the first round,\nthe coefficient vector is going to be 0. The index set is\ngoing to be empty. And the residual\nat the first round",
    "start": "495505",
    "end": "502010"
  },
  {
    "text": "is just going to be the\noutput, the output vector.",
    "start": "502010",
    "end": "507880"
  },
  {
    "text": "Then you find the\nbest single variable, and then you update\nthe index set.",
    "start": "507880",
    "end": "514150"
  },
  {
    "text": "So you add that variable\nto the index set. To include such variables, you\ncompute the coefficient vector.",
    "start": "514150",
    "end": "521559"
  },
  {
    "text": "And then you update\nthe residual, and then you start again, OK? ",
    "start": "521559",
    "end": "527282"
  },
  {
    "text": "If you want, here I show\nyou the first exam-- just to give you an idea.",
    "start": "527282",
    "end": "533180"
  },
  {
    "text": "Suppose that this is-- so first of all,\nnotice this, OK?",
    "start": "533180",
    "end": "538976"
  },
  {
    "text": "Oh, it's so boring.  Forget about anything\nthat's written here.",
    "start": "538976",
    "end": "544680"
  },
  {
    "text": "Just look at this matrix. The output vector is of the\nsame length of the column",
    "start": "544680",
    "end": "551080"
  },
  {
    "text": "of the matrix, right? So each column of\nthe matrix will be related to one variable.",
    "start": "551080",
    "end": "558372"
  },
  {
    "text": "So what you're going\nto do is that you're going to try to see\nwhich of these best",
    "start": "558372",
    "end": "563860"
  },
  {
    "text": "explain my output\nvector, and then you're going to try to define\nthe residual and keep on going,",
    "start": "563860",
    "end": "570380"
  },
  {
    "text": "OK?  So in this case,\nfor example, you",
    "start": "570380",
    "end": "576270"
  },
  {
    "text": "can ask, which of the two\ndirections, X1 and X2, best explain the vector Y, OK?",
    "start": "576270",
    "end": "582010"
  },
  {
    "text": "So this is the case\nwhere it's simple. I basically have\nthis one direction.",
    "start": "582010",
    "end": "587080"
  },
  {
    "text": "One variable is this one. Another variable is this one. And then I have that vector. I want to know which\ndirection I should",
    "start": "587080",
    "end": "592634"
  },
  {
    "text": "pick to best explain my Y. Which\none do you think I should pick? AUDIENCE: X1.",
    "start": "592634",
    "end": "598066"
  },
  {
    "text": "LORENZO ROSASCO:\nI should pick X1? OK. This projection here will be\nthe weight I have to put to X1",
    "start": "598066",
    "end": "604330"
  },
  {
    "text": "to get a good prediction. And then what's the residual? Well, I have to take this X1. I have to-- I have to take Yn.",
    "start": "604330",
    "end": "611650"
  },
  {
    "text": "I have to subtract that, and\nthis is what I have left. So this is the simple terms, OK? So that's what we want to do.",
    "start": "611650",
    "end": "619460"
  },
  {
    "text": "So we said it with hands. We say it with words. Here is, more or less,\nthe pseudocode, OK?",
    "start": "619460",
    "end": "627870"
  },
  {
    "text": "It's a bit boring to read. You can see that it's\nfour lines of code anyway.",
    "start": "627870",
    "end": "633670"
  },
  {
    "text": "Now that we've said it\n15 times, probably it won't be that hard\nto read because what you see is that you have\na notion of residual.",
    "start": "633670",
    "end": "639460"
  },
  {
    "text": "You have the coefficient vector,\nand you have the index set. This is empty. This is all 0's.",
    "start": "639460",
    "end": "644564"
  },
  {
    "text": "And this is the first round. It's just the output. Then what you do\nis that you start. And the free parameter here is\nT, the number of iterations,",
    "start": "644564",
    "end": "652930"
  },
  {
    "text": "OK? It's going to be\nlambda, so to say. What you do is-- OK, you have for each j--",
    "start": "652930",
    "end": "660340"
  },
  {
    "text": "so just notation. j runs over the\nvariables, and capital Xj",
    "start": "660340",
    "end": "666190"
  },
  {
    "text": "will be the column of the\ndata matrix, the one that corresponds to j's variable, OK?",
    "start": "666190",
    "end": "672399"
  },
  {
    "text": "And then what you\ndo in this line, here I expand it, is to\nfind the coefficient--",
    "start": "672400",
    "end": "679130"
  },
  {
    "text": "sorry, find the error\nthat corresponds to the best variable, OK? If you look, it turns\nout that it is--",
    "start": "679130",
    "end": "687009"
  },
  {
    "text": "if you assume-- oh, it\nis equivalent to find the column best\ncorrelated with the output",
    "start": "687010",
    "end": "693490"
  },
  {
    "text": "is equivalent to find the\ncolumn that best explains the output, or the residual. These two things are the same.",
    "start": "693490",
    "end": "699800"
  },
  {
    "text": "So here, I write equivalence. So pick the one\nthat you prefer, OK? Either you say I find the\ncolumn that is best correlated",
    "start": "699800",
    "end": "706750"
  },
  {
    "text": "with the output or the\nresidual, or you find the column that best explains the residual\nin the sense of least squares,",
    "start": "706750",
    "end": "713871"
  },
  {
    "text": "OK? These two things are equivalent. Pick the one that you like. And that's the\ncontent of this one. And then you select the\nindex of that column.",
    "start": "713871",
    "end": "720760"
  },
  {
    "text": "So you solve this\nproblem for each column. It's an easy problem. It's a one-dimensional problem. And then you pick the\none column that you",
    "start": "720760",
    "end": "727120"
  },
  {
    "text": "like the most, which is\nthe one that gives you the best correlation, a.k.a.",
    "start": "727120",
    "end": "733440"
  },
  {
    "text": "least square error. Then you add this\nk to the index set.",
    "start": "733440",
    "end": "740230"
  },
  {
    "text": "And then, in this\ncase, it's very simple. I'm just going to-- I'm not going to\nrecompute anything.",
    "start": "740230",
    "end": "745240"
  },
  {
    "text": "So what I do is that\nsuppose that at-- you remember the\ncoefficient vector, where it was all 0's, OK?",
    "start": "745240",
    "end": "751774"
  },
  {
    "text": "Then at the first round,\nI compute one number, the solution with, say, the\nfirst coordinate, for example.",
    "start": "751774",
    "end": "756850"
  },
  {
    "text": "And then I add a number\nin that entry, OK? So this is the\northonormal basis, OK?",
    "start": "756850",
    "end": "763449"
  },
  {
    "text": "So it has just all 0's,\nbut 1 in the position k. So here I put this number.",
    "start": "763450",
    "end": "769210"
  },
  {
    "text": "This is just a typo. And then what you do is\nthat you sum them up, OK? So you have all 0's.",
    "start": "769210",
    "end": "774367"
  },
  {
    "text": "Just one number here, the first\niteration, then the other one. And then you add this one there,\nand you keep on going, OK?",
    "start": "774367",
    "end": "779880"
  },
  {
    "text": "This is the simplest\npossible version. And once you have this,\nnow you have this vector. This is a long vector.",
    "start": "779880",
    "end": "786310"
  },
  {
    "text": "You multiply this--\nsorry, this should be Xn. Maybe we should take\nnote of the typos",
    "start": "786310",
    "end": "792340"
  },
  {
    "text": "because I'm never going\nto remember all of them. And then what you\ndo is that you just discount what you explained\nso far to the solution.",
    "start": "792340",
    "end": "799390"
  },
  {
    "text": "So you already explained\nsome part of the residual. Now you discount this\nnew part, and you define the new residual,\nand then you go back.",
    "start": "799390",
    "end": "807070"
  },
  {
    "text": "This method is-- so\ngreedy approaches is one name, as it often\nhappens in machine learning",
    "start": "807070",
    "end": "813730"
  },
  {
    "text": "and statistics and\nother fields, things get reinvented constantly, a\nbit because people just come",
    "start": "813730",
    "end": "820954"
  },
  {
    "text": "to them from a\ndifferent perspective, a bit because people just\ndecide studying and reading is not priority sometimes.",
    "start": "820954",
    "end": "828589"
  },
  {
    "text": "And so this one algorithm\nis often called greedy.",
    "start": "828590",
    "end": "834680"
  },
  {
    "text": "It's one example of\ngreedy approaches. It's sometimes called\nmatching pursuit. It's very much related to\nso-called forward stagewise",
    "start": "834680",
    "end": "841630"
  },
  {
    "text": "regression. That's how it's\ncalled in statistics. And well, it has a\nbunch of other names.",
    "start": "841630",
    "end": "848360"
  },
  {
    "text": "Now, this one version is\njust the basic version. It's the simplex-- it's\nthe simplest version.",
    "start": "848360",
    "end": "855110"
  },
  {
    "text": "This step typically remains. These two steps can be\nchanged slightly, OK?",
    "start": "855110",
    "end": "860980"
  },
  {
    "text": "For example, can you think\nof another way of doing this? Let me just give you a hint. In this case, what you do is\nthat you select a variable.",
    "start": "860980",
    "end": "868053"
  },
  {
    "text": "You compute the coefficient. Then you select\nanother variable. You compute the coefficient\nfor the second variable, but you keep the coefficient\nyou already computed",
    "start": "868053",
    "end": "874151"
  },
  {
    "text": "for the first variable. They never knew that\nyou took another one because you didn't take it yet.",
    "start": "874151",
    "end": "880990"
  },
  {
    "text": "So from this comment,\ndo you see how you could change this method\nto somewhat fix this aspect?",
    "start": "880990",
    "end": "891620"
  },
  {
    "text": "Do you see what I'm saying? I would like to change\nthis one line where I compute the coefficient\nand this one line",
    "start": "891620",
    "end": "898340"
  },
  {
    "text": "even, perhaps, where I compute\nthe residual to account for the fact that this method\nbasically never updated",
    "start": "898340",
    "end": "905660"
  },
  {
    "text": "the weights that\nit computed before. You only add a new one. And this seems potentially\nnot a good idea,",
    "start": "905660",
    "end": "912920"
  },
  {
    "text": "because when you\nhave two variables, it's better to compute the\nsolution with both of them.",
    "start": "912920",
    "end": "918435"
  },
  {
    "text": "So what could you do? AUDIENCE: [INAUDIBLE] ",
    "start": "918435",
    "end": "924052"
  },
  {
    "text": "LORENZO ROSASCO: Right. So what you could\ndo is essentially what is called orthogonal\nmatching pursuit.",
    "start": "924052",
    "end": "929360"
  },
  {
    "text": "So you would take\nthis set, and now you would solve a least\nsquare problem with the variables that\nare in the index set up",
    "start": "929360",
    "end": "936500"
  },
  {
    "text": "to that point, all of them. You recompute everything.",
    "start": "936500",
    "end": "941576"
  },
  {
    "text": "And now you have to solve not\na one-dimensional problem, but n times k-dimensional\nproblem, where the k is the--",
    "start": "941577",
    "end": "947220"
  },
  {
    "text": "I don't know, k is a bad\nname-- with the dimension of the set that could\nbe T or more than T, OK?",
    "start": "947220",
    "end": "953780"
  },
  {
    "text": "And then at that point, you\nalso want to redefine this, because you're not\ndiscounting anymore",
    "start": "953780",
    "end": "960850"
  },
  {
    "text": "what you already explained,\nbut each time you're recomputing everything. So you just want to do Yn\nminus the prediction, OK?",
    "start": "960851",
    "end": "968216"
  },
  {
    "text": "So this algorithm\nis the one that actually has better properties. It works better. You pay the price,\nbecause each time",
    "start": "968216",
    "end": "974240"
  },
  {
    "text": "you have to recompute the\nleast squares solution. And when you have more\nthan one variable inside,",
    "start": "974240",
    "end": "979659"
  },
  {
    "text": "then the problems\nbecome big and big. So if you stop after a few\niterations, it's great. But if you start to have\nmany iterations each time,",
    "start": "979659",
    "end": "985040"
  },
  {
    "text": "you have to solve\na linear system. So the complexity\nis much higher. This one here, as you can\nimagine, is super fast.",
    "start": "985040",
    "end": "990709"
  },
  {
    "text": "So that's it. So it turns out that this\nmethod is, as I told you,",
    "start": "990710",
    "end": "996639"
  },
  {
    "text": "is called matching pursuit,\nor if not matching pursuit, forward stagewise regression,\nis one way to approximate a zero",
    "start": "996639",
    "end": "1003130"
  },
  {
    "text": "solution. And one can prove\nexactly in which sense you can approximate it, OK?",
    "start": "1003130",
    "end": "1011020"
  },
  {
    "text": "So I think this is the\none that we might give you this afternoon, right? AUDIENCE: Orthogonal. LORENZO ROSASCO: Oh, yeah,\nthe orthogonal version,",
    "start": "1011020",
    "end": "1017513"
  },
  {
    "text": "the nicer version. The other way of doing this is\nthe one that basically says,",
    "start": "1017513",
    "end": "1024409"
  },
  {
    "text": "look, here what you're doing\nis that you're just counting the number of entries\ndifferent from 0's.",
    "start": "1024410",
    "end": "1031180"
  },
  {
    "text": "If you now were to replace\nthis with something that does a bit more-- what it\ndoes is it not only counts,",
    "start": "1031180",
    "end": "1037089"
  },
  {
    "text": "but it actually\nsums up the weights. So if you want, in one\ncase, you just check.",
    "start": "1037089",
    "end": "1043689"
  },
  {
    "text": "If a weight is bigger\nthan 0, you count it 1. Otherwise, you count it 0. Here you actually take\nthe absolute value.",
    "start": "1043690",
    "end": "1049340"
  },
  {
    "text": "So instead of summing\nup binary values, you sum up real numbers, OK? This is what is\ncalled the L1 norm.",
    "start": "1049340",
    "end": "1055810"
  },
  {
    "text": "So each weight doesn't\ncount for its sign, but it actually counts\nfor each absolute value.",
    "start": "1055810",
    "end": "1062630"
  },
  {
    "text": "So it turns out that this one\nterm, which you can imagine-- the absolute value\nlooks like this, right,",
    "start": "1062630",
    "end": "1068500"
  },
  {
    "text": "and now you're just\nsumming them up. So that thing is\nactually convex. So it turns out that you're\nsumming up two convex terms,",
    "start": "1068500",
    "end": "1076019"
  },
  {
    "text": "and the overall\nfunctional is convex. And if you want, you\ncan think of this a bit",
    "start": "1076020",
    "end": "1081070"
  },
  {
    "text": "as a relaxation\nof the zero norm. As we say, relaxation\nin this sense is the strict requirement.",
    "start": "1081070",
    "end": "1087748"
  },
  {
    "text": "So I talked about relaxation\nbefore when I said instead of binary values, it\ntakes real values, and you optimize over the reals\ninstead of the binary values.",
    "start": "1087749",
    "end": "1095649"
  },
  {
    "text": "Here is kind of the same thing. Instead of restricting yourself\nto this functional, which is binary-valued, now you\nallow yourself to relax and get",
    "start": "1095649",
    "end": "1101740"
  },
  {
    "text": "real numbers. And what you gain is\nthat this algorithm, the corresponding optimization\nproblem is convex,",
    "start": "1101740",
    "end": "1109783"
  },
  {
    "text": "and you can try to solve it. It is not still something\nthat we can do-- we cannot do what we did before.",
    "start": "1109784",
    "end": "1114910"
  },
  {
    "text": "We cannot just take derivatives\nand set them equal to 0 because this term is not smooth.",
    "start": "1114910",
    "end": "1121030"
  },
  {
    "text": "The absolute value\nlooks like this, which means that\nhere, around the kink, is not differentiable.",
    "start": "1121030",
    "end": "1126559"
  },
  {
    "text": "But we can still\nuse convex analysis to try to get the solution,\nand actually the solution doesn't look too complicated.",
    "start": "1126560",
    "end": "1132310"
  },
  {
    "text": "Getting there requires a\nbit of convex analysis, but there are techniques.",
    "start": "1132310",
    "end": "1137855"
  },
  {
    "text": "And the ones that\nare trendy these days are called forward-backward\nsplitting or proximal method to solve this problem.",
    "start": "1137855",
    "end": "1144334"
  },
  {
    "text": "And apparently,\nI'm not even going to show them to you because\nyou don't even see them. But essentially, it's\nnot too complicated.",
    "start": "1144334",
    "end": "1149360"
  },
  {
    "text": "Just to tell you in\none word what they do is that they do the gradient\ndescent of the first term,",
    "start": "1149360",
    "end": "1156309"
  },
  {
    "text": "and then at each step of\nthe gradient they threshold. So they take a step of the\ngradient, get a vector,",
    "start": "1156310",
    "end": "1161380"
  },
  {
    "text": "look inside the vector. If something is smaller than\na threshold that depends",
    "start": "1161380",
    "end": "1166840"
  },
  {
    "text": "on lambda, I set it equal to 0. Otherwise, I let it go, OK? So I didn't put it,\nI don't know why,",
    "start": "1166840",
    "end": "1173140"
  },
  {
    "text": "because it's really\na one-line algorithm. It's a bit harder to derive,\nbut it's very simple to check.",
    "start": "1173140",
    "end": "1178717"
  },
  {
    "text": "So let's talk one second\nabout this picture, and then let me tell you about\nwhat I'm not telling you.",
    "start": "1178717",
    "end": "1185170"
  },
  {
    "text": "So hiding behind\neverything I said so far, there is a linear system, right?",
    "start": "1185170",
    "end": "1191320"
  },
  {
    "text": "There is a linear system\nthat is n by p or d or whatever you want to\ncall it, with the number",
    "start": "1191320",
    "end": "1196810"
  },
  {
    "text": "of p of variables. And our game so far\nhas always been, look, we have a linear system\nthat can be not invertible,",
    "start": "1196810",
    "end": "1204220"
  },
  {
    "text": "or even if it is, it might\nhave bad condition number, and I want to try to find a\nway to stabilize the problem.",
    "start": "1204220",
    "end": "1209980"
  },
  {
    "text": "The first round, we\nbasically replace the inverse with an approximate inverse. That's the classical\nway of doing it.",
    "start": "1209980",
    "end": "1215320"
  },
  {
    "text": "Here, we're making\nanother assumption. We're basically saying,\nlook, this vector, it does look very long, so that\nthis problem seems ill-posed.",
    "start": "1215320",
    "end": "1224610"
  },
  {
    "text": "But in fact, if only a few\nentries were different from 0 and if you were able to\ntell me which one they are,",
    "start": "1224610",
    "end": "1230730"
  },
  {
    "text": "you can go in, delete\nall these entries, delete all the\ncorresponding columns, and then you will have a matrix.",
    "start": "1230730",
    "end": "1235930"
  },
  {
    "text": "Now he looks short and large. He will look skinny\nand tall, OK?",
    "start": "1235930",
    "end": "1241599"
  },
  {
    "text": "And that probably would\nbe easier to solve. It would be the case\nwhere the problem is one of the linear systema\nthat we know how to solve.",
    "start": "1241599",
    "end": "1248169"
  },
  {
    "text": "So what we described\nso far is a way to find solution of\nlinear systems that are--",
    "start": "1248170",
    "end": "1255167"
  },
  {
    "text": "with the number of equations\nwhich is smaller than the number of unknowns, and, by\ndefinition, cannot be solved,",
    "start": "1255167",
    "end": "1260610"
  },
  {
    "text": "under the extra\nassumption that, in fact, there are fewer unknowns\nthan what it looks like.",
    "start": "1260610",
    "end": "1266460"
  },
  {
    "text": "It's just that I'm not telling\nyou which one they are, OK? You see, if I\ncould tell you, you",
    "start": "1266460",
    "end": "1271839"
  },
  {
    "text": "would just get back to a\nvery easy problem, where the number of unknowns\nis much smaller, OK? ",
    "start": "1271839",
    "end": "1279779"
  },
  {
    "text": "So this is a\nmathematical effect, OK?  And the odds are open, because--",
    "start": "1279780",
    "end": "1286372"
  },
  {
    "text": "now they're not\nbecause people have been talking about this stuff\nfor 10 years constantly. But one question is, how much\ndoes this assumption buy you?",
    "start": "1286372",
    "end": "1295120"
  },
  {
    "text": "For example, could you prove\nthat in certain situations, even if you don't know\nthe entry of this,",
    "start": "1295120",
    "end": "1301030"
  },
  {
    "text": "you could actually solve\nthis problem exactly? So if I give them to you,\nyou can do it, right?",
    "start": "1301030",
    "end": "1307399"
  },
  {
    "text": "But is there a way to try\nto guess them in some way so that you can do almost as\ngood or with high probability",
    "start": "1307400",
    "end": "1313810"
  },
  {
    "text": "as well as if I tell\nthem in advance? And it turned out that\nthe answer is yes, OK?",
    "start": "1313810",
    "end": "1319870"
  },
  {
    "text": "And the answer is basically that\nif the number of entries that are different from\n0 is small enough",
    "start": "1319870",
    "end": "1326410"
  },
  {
    "text": "and the columns corresponding\nto those variables are not too correlated,\nare not too collinear,",
    "start": "1326410",
    "end": "1333299"
  },
  {
    "text": "so they're\ndistinguishable enough that when you\nperturb the problem a little bit nothing\nchanges, then",
    "start": "1333300",
    "end": "1338800"
  },
  {
    "text": "you can solve the\nproblem exactly, OK? ",
    "start": "1338800",
    "end": "1344100"
  },
  {
    "text": "So this, on the one hand, is\nexactly the kind of theory that tells you why using greedy\nmethods and convex relaxation",
    "start": "1344100",
    "end": "1350580"
  },
  {
    "text": "will give you a good\napproximation to L0, because that's basically\nwhat this story tells you.",
    "start": "1350580",
    "end": "1355860"
  },
  {
    "text": "People have been using this--\nand so this is interesting for us-- people have been using\nthis observation in a slightly",
    "start": "1355860",
    "end": "1362670"
  },
  {
    "text": "different context, which\nis the context where--",
    "start": "1362670",
    "end": "1367920"
  },
  {
    "text": "you see, for us, Y and\nX, we don't choose. We get. And whatever they are, they are.",
    "start": "1367920",
    "end": "1373200"
  },
  {
    "text": "And if it's correlate-- if\nthe columns are nice, nice. But if they're not nice, sorry,\nyou have to live with it, OK?",
    "start": "1373200",
    "end": "1380071"
  },
  {
    "text": "But there are settings where\nyou can think of the following. Suppose that you have\na signal, and you want",
    "start": "1380071",
    "end": "1386789"
  },
  {
    "text": "to be able to reconstruct it. So the classical Shannon\nsampling theorem results",
    "start": "1386790",
    "end": "1392226"
  },
  {
    "text": "basically tell you\nthat, I don't know, if you have something\nwhich has been limited, you have to sample twice\nthe maximum frequency.",
    "start": "1392227",
    "end": "1397929"
  },
  {
    "text": "But this is kind of\nworst case because it's assuming that all the bands,\nall the frequencies, are full.",
    "start": "1397929",
    "end": "1403049"
  },
  {
    "text": "Suppose that now we play-- it's an analogy, OK? But I tell you, oh,\nlook, it's true, the maximum frequency of this.",
    "start": "1403050",
    "end": "1408370"
  },
  {
    "text": "But there's only another\nfrequency, this one. Do you really need to\nsample that much, or you can do much less?",
    "start": "1408370",
    "end": "1414890"
  },
  {
    "text": "And so it turns out that\nbasically the story here, as you're answering\nthis question, it turns out that, yes,\nyou can do much less.",
    "start": "1414890",
    "end": "1420950"
  },
  {
    "text": "Ideally, what you would\nlike to say, well, instead of being twice\nthe maximum frequency, if I have just four\nfrequencies different from 0,",
    "start": "1420950",
    "end": "1427730"
  },
  {
    "text": "I'd have to do\neight samples, OK? That would be ideal,\nbut you would have to know which one they are.",
    "start": "1427730",
    "end": "1434070"
  },
  {
    "text": "You don't, so you pay a price,\nbut it's just logarithmic. So you basically say\nthat you can almost--",
    "start": "1434070",
    "end": "1439740"
  },
  {
    "text": "you have a new sampling\ntheorem that tells you that you don't need\nto sample that much.",
    "start": "1439740",
    "end": "1445530"
  },
  {
    "text": "You don't need to sample\nthat low either, which would be, say, the\nmaximum frequency is d.",
    "start": "1445530",
    "end": "1451520"
  },
  {
    "text": "The number of non-zero\nfrequencies is s. So with classical, you\nwould have to say 2d.",
    "start": "1451520",
    "end": "1458340"
  },
  {
    "text": "Ideally, we would\nlike to say 2s. Actually, what you can say\nis something like 2s log d.",
    "start": "1458340",
    "end": "1463890"
  },
  {
    "text": "So you have a log d price\nthat you pay because you didn't know where they are. But still, it's much less than\nbeing linear in the dimension.",
    "start": "1463890",
    "end": "1472170"
  },
  {
    "text": "So essentially, the field\nof compressed sensing has been built around\nthis observation, and the focus is\nslightly different.",
    "start": "1472170",
    "end": "1478710"
  },
  {
    "text": "Instead of saying I want to do\na statistical estimation where I can just build this, what\nyou do is say I have a signal.",
    "start": "1478710",
    "end": "1484990"
  },
  {
    "text": "And now I view this\nas a sensing matrix that I design with\nthe property that I know will allow me to\ndo this estimation well.",
    "start": "1484990",
    "end": "1493320"
  },
  {
    "text": "And so you basically assume that\nyou can choose those vectors in certain ways,\nand then you can prove that you can reconstruct\nwith much fewer samples, OK?",
    "start": "1493320",
    "end": "1501809"
  },
  {
    "text": "And this has been\nused, for example-- I never remember for, as\nyou call in MEG-- in what?",
    "start": "1501810",
    "end": "1507532"
  },
  {
    "text": "No, MRI, MRI. Two things I didn't\ntell you about, but it's worth\nmentioning are, suppose",
    "start": "1507532",
    "end": "1513390"
  },
  {
    "text": "that what I tell\nyou is actually it's not individual entries that are\n0, but group of entries that",
    "start": "1513390",
    "end": "1520170"
  },
  {
    "text": "are 0, for example, because each\nentry is a biological process. So I have genes, but\ngenes are actually",
    "start": "1520170",
    "end": "1526980"
  },
  {
    "text": "involved in biological process. So there is a group of genes\nthat is doing something. I have a group of genes\nthat are doing something,",
    "start": "1526980",
    "end": "1532461"
  },
  {
    "text": "and I want to select is not\nindividual genes, but groups. Can you twist this stuff in such\na way that you select groups?",
    "start": "1532461",
    "end": "1538740"
  },
  {
    "text": "Yes. What if the groups are\nactually overlapping? How do you want to\ndeal with the overlaps? Do you want to keep the overlap?",
    "start": "1538740",
    "end": "1543889"
  },
  {
    "text": "Do you want to cut the overlap? What if you have a\ntree structure, OK? What do you do with this?",
    "start": "1543889",
    "end": "1549159"
  },
  {
    "text": "So first of all, who\ngives it information, OK? And then if you have the\ninformation, how do you use it, and how are you going to use it?",
    "start": "1549160",
    "end": "1555073"
  },
  {
    "text": "See, this is the whole\nfield of structure sparsity. It's the whole industry\nof building penalties",
    "start": "1555073",
    "end": "1560910"
  },
  {
    "text": "other than L1 that would\nallow you to incorporate this kind of prior information. And if you want as the\nplace, as in kernel methods,",
    "start": "1560910",
    "end": "1568799"
  },
  {
    "text": "the kernel was the\nplace where you could incorporate prior information. This is the case\nwhere, in this field,",
    "start": "1568800",
    "end": "1574620"
  },
  {
    "text": "you can do that by designing\na suitable regularizer.",
    "start": "1574620",
    "end": "1579789"
  },
  {
    "text": "And then a lot of\nthe reason is this. So here we'll translate\nwith these new regularizers.",
    "start": "1579789",
    "end": "1585720"
  },
  {
    "text": "The last bit is that, with\na bit of a twist, some of the idea that I show\nyou now that are basically",
    "start": "1585720",
    "end": "1591810"
  },
  {
    "text": "related to vectors\nand sparsity translate to more general context, in\nparticular that of matrices",
    "start": "1591810",
    "end": "1599050"
  },
  {
    "text": "that have low rank, OK? The classical example is\nsuppose that I give you-- it's matrix completion, OK?",
    "start": "1599050",
    "end": "1605500"
  },
  {
    "text": "I give you a matrix,\nbut I actually delete most of the\nentries of the matrix. And I tell you, OK, estimate\nthe original matrix.",
    "start": "1605500",
    "end": "1614164"
  },
  {
    "text": "Well, how can I do that, right? Well, it turns out that\nif the matrix itself had very low rank, so that many\nof the columns and rows you saw",
    "start": "1614164",
    "end": "1623760"
  },
  {
    "text": "were actually related\nto each other, you might actually\nbe able to do that. And the way you chose the\nentries to delete or select",
    "start": "1623760",
    "end": "1630540"
  },
  {
    "text": "was not malicious,\nthen you would be able to fill in the\nmissing entries, OK?",
    "start": "1630540",
    "end": "1636360"
  },
  {
    "text": "And the theory\nbehind this is very similar to the\ntheory that allows to fill in the right\nentries of the vector, OK?",
    "start": "1636360",
    "end": "1643840"
  },
  {
    "text": "Last bit-- PCA in 15 minutes. So what we've seen so far\nwas a very hard problem",
    "start": "1643840",
    "end": "1653789"
  },
  {
    "text": "of variable selection. It is still a\nsupervised problem, where I give you labels, OK? The last bit I\nwant to show you is",
    "start": "1653790",
    "end": "1659640"
  },
  {
    "text": "PCA, which is the case where\nI don't give you labels. And what you try to answer\nis actually-- perhaps it's like the simpler question.",
    "start": "1659640",
    "end": "1665490"
  },
  {
    "text": "Because you don't want to\nselect one of the directions, but you would like\nto know if there are directions that matter.",
    "start": "1665490",
    "end": "1670950"
  },
  {
    "text": "So you allow\nyourself, for example, to combine the different\ndirections in your data, OK?",
    "start": "1670950",
    "end": "1677039"
  },
  {
    "text": "So this question is interesting\nfor many, many reasons. One is data visualization,\nfor example.",
    "start": "1677040",
    "end": "1683106"
  },
  {
    "text": "You have stuff that you cannot\nlook at because you have, for example, digits in\nvery high dimensions. You would like to look at them.",
    "start": "1683107",
    "end": "1689220"
  },
  {
    "text": "How do you do it? Well, you like to\nfind directions. The first direction\nto project everything, the second direction, three\ndirection, because then you",
    "start": "1689220",
    "end": "1695171"
  },
  {
    "text": "can plot and look at them, OK? And this is one visualization\nof these images here.",
    "start": "1695171",
    "end": "1701952"
  },
  {
    "text": "And I'll remember\nnow the code now. It's written here. You have different\ncolors, and what you see is that this actually\ndid a good job.",
    "start": "1701952",
    "end": "1708210"
  },
  {
    "text": "Because what you expect is that\nif you do a nice visualization, what you would like to have\nis that similar numbers",
    "start": "1708210",
    "end": "1713460"
  },
  {
    "text": "or same numbers are\nin the same regions, and perhaps similar\nnumbers are close, OK?",
    "start": "1713460",
    "end": "1718920"
  },
  {
    "text": "So this is one reason\nwhy you want to do this. One reason why you\nmight want to do this",
    "start": "1718920",
    "end": "1724230"
  },
  {
    "text": "is also because you\nmight want to reduce the dimensionality of your\ndata just to compress them or because you might hope that\ncertain dimensions don't matter",
    "start": "1724230",
    "end": "1731010"
  },
  {
    "text": "or are simply noise. And so you just want\nto get rid of that because this could be good\nfor statistical reasons.",
    "start": "1731010",
    "end": "1738720"
  },
  {
    "text": "OK, so the game is going\nto be the following. X is the data space,\nwhich is going to be RD.",
    "start": "1738720",
    "end": "1745230"
  },
  {
    "text": "And we want to define a map M\nthat sends vectors of length D",
    "start": "1745230",
    "end": "1750240"
  },
  {
    "text": "into vectors of length k. So k is going to be my\nreduced dimensionality.",
    "start": "1750240",
    "end": "1760094"
  },
  {
    "text": "And what we're going\nto do is that we're going to build a basic method\nto do this, which is PCA, and we're going to give a purely\ngeometric view of PCA, OK?",
    "start": "1760094",
    "end": "1769289"
  },
  {
    "text": "And this is going\nto be done by taking first the case where k is equal\nto 1 and then iterate to go up.",
    "start": "1769290",
    "end": "1774485"
  },
  {
    "text": "So at the first\ncase, we're going to ask, if I give you vectors\nwhich are D dimensional, how can I project them in\none dimension with respect",
    "start": "1774485",
    "end": "1781230"
  },
  {
    "text": "to some criterion\nof optimality, OK?",
    "start": "1781230",
    "end": "1786540"
  },
  {
    "text": "And here what we ask is, we want\nto project the data in the one dimension that would give\nme the best possible error.",
    "start": "1786540",
    "end": "1797170"
  },
  {
    "text": "So I think I had it before. Do I have it-- no, no.",
    "start": "1797170",
    "end": "1803278"
  },
  {
    "start": "1803279",
    "end": "1809440"
  },
  {
    "text": "This was done for another\nreason, but it's useful now. If you have this\nvector and you want",
    "start": "1809440",
    "end": "1815440"
  },
  {
    "text": "to project in this direction,\nand this is a unit vector, what do you do?",
    "start": "1815440",
    "end": "1822130"
  },
  {
    "text": "I want to know how to write this\nvector here, the projection. What you do is that you take the\ninner product between Yn and X.",
    "start": "1822130",
    "end": "1831850"
  },
  {
    "text": "You get the number, and that\nnumber is the length you want to assign to X1, OK? ",
    "start": "1831850",
    "end": "1840550"
  },
  {
    "text": "So suppose that w\nis the direction. And I have a vector\nx, and I want to give the projection, OK?",
    "start": "1840550",
    "end": "1846970"
  },
  {
    "text": "What do I do? I take the inner\nproduct of x and w, and this is the length I have\nto assign to the vector w, which",
    "start": "1846970",
    "end": "1854770"
  },
  {
    "text": "is unit norm, OK? So this is the\nbest approximation",
    "start": "1854770",
    "end": "1859930"
  },
  {
    "text": "of xi in the direction of w. Does it make sense? ",
    "start": "1859930",
    "end": "1867670"
  },
  {
    "text": "I fix a w, and I want to know\nhow well I can describe x. I project x in that\ndirection, and then I",
    "start": "1867670",
    "end": "1875090"
  },
  {
    "text": "take the difference between\nx and the projection, OK? And then what I do is that\nI sum over all points.",
    "start": "1875090",
    "end": "1882620"
  },
  {
    "text": "And then I check, among\nall possible directions, the one that give\nme the best error. ",
    "start": "1882620",
    "end": "1889070"
  },
  {
    "text": "So suppose that\nis your data set. Which direction\nyou think is going to give me the best error?",
    "start": "1889070",
    "end": "1894983"
  },
  {
    "text": " AUDIENCE: Keep going.",
    "start": "1894983",
    "end": "1900850"
  },
  {
    "text": "LORENZO ROSASCO: Well, if\nyou go in this direction, you can explain most\nof the stuff, OK? You can reconstruct it best.",
    "start": "1900850",
    "end": "1907029"
  },
  {
    "text": "So this is going\nto be the solution. So the question here is really,\nhow do you solve this problem?",
    "start": "1907030",
    "end": "1917140"
  },
  {
    "text": "You could try to minimize\nwith respect to w. But in fact, it's not clear what\nkind of computation you have.",
    "start": "1917140",
    "end": "1924040"
  },
  {
    "text": "And if we massage\nthis a little bit, it turns out that it is actually\nexactly an eigenvalue problem. So that's what we\nwant to do next.",
    "start": "1924040",
    "end": "1930160"
  },
  {
    "text": "So conceptually, what we want\nto do is what I said here and nothing more. I want to find the single\nindividual direction that",
    "start": "1930160",
    "end": "1936159"
  },
  {
    "text": "allows me to reconstruct best,\non average, all the training set points. And now what we want\nto do is just to check",
    "start": "1936160",
    "end": "1942790"
  },
  {
    "text": "what kind of computation\nthese entail and learn a bit more about this, OK?",
    "start": "1942790",
    "end": "1948049"
  },
  {
    "text": " So this notation is just to\nsay that the vector is norm 1",
    "start": "1948050",
    "end": "1954640"
  },
  {
    "text": "so that I don't have to fumble\nwith the size of the vector.",
    "start": "1954640",
    "end": "1960800"
  },
  {
    "text": "OK, so let's do a\ncouple of computations. This is ideal after lunch. So you just take this\nsquare and develop it, OK?",
    "start": "1960800",
    "end": "1969100"
  },
  {
    "text": "And remember that\nw is unit norm. So when you do w\ntranspose w, you get 1.",
    "start": "1969100",
    "end": "1976690"
  },
  {
    "text": "And then if you-- and if you don't forget\nto put your square and if you just\ndevelop this, you'll",
    "start": "1976690",
    "end": "1981770"
  },
  {
    "text": "see that this is\nan equality, OK? There is a square missing here. So you have xi square.",
    "start": "1981770",
    "end": "1987529"
  },
  {
    "text": "Then you would have\nthe product of xi and this, which will be\nw transpose xi square.",
    "start": "1987530",
    "end": "1993710"
  },
  {
    "text": "And then you would\nalso have this square, but this square is w transpose\nxi and then w transpose w,",
    "start": "1993710",
    "end": "2000570"
  },
  {
    "text": "which is 1. And so what you see is\nthat this would create-- instead of three terms\nwe have two because two",
    "start": "2000570",
    "end": "2006070"
  },
  {
    "text": "cancel out-- not cancel out. They balance each other. OK. ",
    "start": "2006070",
    "end": "2012820"
  },
  {
    "text": "So then I'd argue that if\ninstead of minimizing this,",
    "start": "2012820",
    "end": "2018250"
  },
  {
    "text": "because this is equal to this,\ninstead of minimizing this, you can maximize this. ",
    "start": "2018250",
    "end": "2026050"
  },
  {
    "text": "Why? Well, because this\nis just a constant. It doesn't depend\non w at all, so I",
    "start": "2026050",
    "end": "2031290"
  },
  {
    "text": "can drop it from my functional. The solution, the minimum,\nthe minimum will be different, but the minimizer, the w\nthat solves the problem,",
    "start": "2031290",
    "end": "2038520"
  },
  {
    "text": "will be the same, OK? And then here is\nminimizing something with a minus, which is the same\nas maximizing the same thing",
    "start": "2038520",
    "end": "2045810"
  },
  {
    "text": "without the minus, OK? I don't ask, so far, so\ngood, because I'm scared.",
    "start": "2045810",
    "end": "2053190"
  },
  {
    "text": "So what you see now\nis that basically if the data were centered,\nbasically this would just",
    "start": "2053190",
    "end": "2060929"
  },
  {
    "text": "be a variance. If the data are centered,\nso there is a minus 0 here, maybe you can interpret this\nas measuring the variance",
    "start": "2060929",
    "end": "2069360"
  },
  {
    "text": "in one direction. And so you have\nanother interpretation of PCA, which is the one\nwhere instead of picking the single direction with the\nbest possible reconstruction,",
    "start": "2069360",
    "end": "2077369"
  },
  {
    "text": "you're picking the direction\nwhere the variance of the data is bigger, OK?",
    "start": "2077370",
    "end": "2082568"
  },
  {
    "text": "And these two points of view\nare completely equivalent. Essentially, whenever\nyou have a square norm, thinking about\nmaximizing the variance",
    "start": "2082569",
    "end": "2088469"
  },
  {
    "text": "or minimizing the\nreconstruction are two complementary dual ideas, OK?",
    "start": "2088469",
    "end": "2093540"
  },
  {
    "text": "So that's what you\nwill be doing here. One more bit. What about computation? So this is-- so we can\nthink about reconstruction.",
    "start": "2093540",
    "end": "2100640"
  },
  {
    "text": "You can think about\nvariance, if you like. What about this computation? What kind of\ncomputation is this, OK?",
    "start": "2100640",
    "end": "2107180"
  },
  {
    "text": "If we massage it\na little bit, we see that is just an\neigenvalue problem. So this is how you do it.",
    "start": "2107180",
    "end": "2112540"
  },
  {
    "text": "This actually look-- so it's\nannoying, but it's very simple. So I wrote all the passages.",
    "start": "2112540",
    "end": "2118109"
  },
  {
    "text": "This is a square, so it's\nsomething times itself. This whole thing is\nsymmetric, so you can swap the order of\nthis multiplication.",
    "start": "2118110",
    "end": "2126420"
  },
  {
    "text": "So you get w transpose\nxi, xi transpose w. But then this is\njust the sum that was",
    "start": "2126420",
    "end": "2133740"
  },
  {
    "text": "going to involve these terms. So I can let the sum enter,\nand this is what you get.",
    "start": "2133740",
    "end": "2140310"
  },
  {
    "text": "So you get w transpose\n1/n xi xi transpose w.",
    "start": "2140310",
    "end": "2147270"
  },
  {
    "text": "So this is just a number. w\ntranspose xi is just a number.",
    "start": "2147270",
    "end": "2153247"
  },
  {
    "text": "But the moment you look at\nsomething that looks like xi, xi transpose, what is that? Well, just look at\ndimensionality, OK?",
    "start": "2153247",
    "end": "2160310"
  },
  {
    "text": "1 times d times d times 1 gives\nyou a number, which is 1 by 1. Now you're doing the\nother way around.",
    "start": "2160310",
    "end": "2166720"
  },
  {
    "text": "So what is this? AUDIENCE: It's a matrix. LORENZO ROSASCO: It's a-- AUDIENCE: Matrix. LORENZO ROSASCO: It's a matrix. And it's a matrix which is d\nby d, and it's of rank 1, OK?",
    "start": "2166720",
    "end": "2175589"
  },
  {
    "text": "And what you do now is\nthat you sum them all up, and what you have is\nthat this quantity here becomes what is called\nthe quadratic form.",
    "start": "2175590",
    "end": "2182180"
  },
  {
    "text": "It is a matrix C, which\njust looks like this. And it's squeezed in between\ntwo vectors, w transpose and w.",
    "start": "2182180",
    "end": "2190810"
  },
  {
    "text": "So now what you want\nto do is that you can rewrite this just this\nway as maximizing the w--",
    "start": "2190810",
    "end": "2198840"
  },
  {
    "text": "sorry, finding the\nunit norm vector w that maximizes this quadratic form.",
    "start": "2198840",
    "end": "2204475"
  },
  {
    "text": "And at this point,\nyou can still ask me who cares, because\nit's just keeping on rewriting the same problem.",
    "start": "2204475",
    "end": "2210530"
  },
  {
    "text": "But it turns out that\nessentially using Lagrange theorem, it is\nrelatively simple to do,",
    "start": "2210530",
    "end": "2217390"
  },
  {
    "text": "you can check that-- oh, so boring-- that the\nsolution of this problem",
    "start": "2217390",
    "end": "2223740"
  },
  {
    "text": "is the maximum eigenvector\nof this matrix, OK? So this you can\nleave as an exercise.",
    "start": "2223740",
    "end": "2230490"
  },
  {
    "text": "Essentially, you take the\nLagrangian of this and use a little bit of duality, and\nyou show that the [INAUDIBLE]",
    "start": "2230490",
    "end": "2236610"
  },
  {
    "text": "of this problem is just\nthe maximum eigen-- so the eigenvalues-- ugh,\nthe eigenvector corresponding",
    "start": "2236610",
    "end": "2243930"
  },
  {
    "text": "to the maximum eigenvalue\nof the matrix C. So finding this\ndirection is just",
    "start": "2243930",
    "end": "2250800"
  },
  {
    "text": "solving an eigenvalue problem. OK. ",
    "start": "2250800",
    "end": "2256274"
  },
  {
    "text": "I think just do last few of\nthose slide, kind of cute. It's pretty simple, OK?",
    "start": "2256274",
    "end": "2262860"
  },
  {
    "text": "So the one part,\nthis line after lunch is a bit there for\nbecause I'm nice.",
    "start": "2262860",
    "end": "2268689"
  },
  {
    "text": "But really, the only one part\nwhich is a bit more complicated is this one here. The rest is really just\nvery simple algebra.",
    "start": "2268689",
    "end": "2276630"
  },
  {
    "text": "So what about k equal 2? I run out of time. But it turns out that\nwhat you want to do",
    "start": "2276630",
    "end": "2283349"
  },
  {
    "text": "is basically if you say that\nyou want to look for a second-- so you look at the\nfirst direction.",
    "start": "2283350",
    "end": "2288840"
  },
  {
    "text": "You solve it, and you know that\nit's the first eigenvector-- the first eigenvector. And then let's say that\nyou add the constraint",
    "start": "2288840",
    "end": "2294206"
  },
  {
    "text": "that the second\ndirection you find has to be orthogonal\nto the first direction. You might not want\nto do this, OK?",
    "start": "2294206",
    "end": "2300540"
  },
  {
    "text": "But if you do, if\nyou say you add the orthogonality constraint,\nthen what you can check",
    "start": "2300540",
    "end": "2305670"
  },
  {
    "text": "is that you can repeat-- sorry, I didn't do it. It's on my note, the one\nthat I have on the website,",
    "start": "2305670",
    "end": "2311700"
  },
  {
    "text": "and the computation\nis kind of cute. And what you see is that\nthe solution of this problem looks exactly like\nthe one before,",
    "start": "2311700",
    "end": "2318660"
  },
  {
    "text": "only with this\nadditional constraint, is exactly the\neigenvector corresponding",
    "start": "2318660",
    "end": "2324060"
  },
  {
    "text": "to the second largest\neigenvalue, OK? And so you can keep on going.",
    "start": "2324060",
    "end": "2329439"
  },
  {
    "text": "And so now this\ngives you a way to go from k equal to 1\nto k bigger than 1, and you can keep on going, OK?",
    "start": "2329439",
    "end": "2336530"
  },
  {
    "text": "So if you're looking\nfor the direction that maximizes the variance\nor the reconstruction, they turn out to be\nthe biggest eigen--",
    "start": "2336530",
    "end": "2344338"
  },
  {
    "text": "the vectors-- ugh, the\neigenvectors corresponding to the biggest eigenvalues of\nthis matrix C, which what you",
    "start": "2344339",
    "end": "2349430"
  },
  {
    "text": "can call it as a second\nmoment or covariance matrix of the data. OK, so this is more\nor less the end.",
    "start": "2349430",
    "end": "2357349"
  },
  {
    "text": "This is the basic, basic,\nbasic version of this. You can mix this with pretty\nmuch all the other stuff",
    "start": "2357350",
    "end": "2364970"
  },
  {
    "text": "we said today. So one is, how about trying to\nuse kernels to do a nonlinear extension of this?",
    "start": "2364970",
    "end": "2370784"
  },
  {
    "text": "So here we just looked at\nthe linear reconstruction. How about nonlinear\nreconstruction? So what you would do\nis that you would first",
    "start": "2370784",
    "end": "2376250"
  },
  {
    "text": "map the data in\nsome way and then try to find some kind of\nnonlinear dimensionality reduction.",
    "start": "2376250",
    "end": "2381390"
  },
  {
    "text": "You see that what I'm\ndoing here is that I'm just using this linear--",
    "start": "2381390",
    "end": "2386840"
  },
  {
    "text": "it's just a linear\ndimensionality reduction, just a linear operator. But what about\nsomething nonlinear?",
    "start": "2386840",
    "end": "2392620"
  },
  {
    "text": "What if my data lie on\nsome kind of structure that looked like that-- our beloved machine\nlearning Swiss roll.",
    "start": "2392620",
    "end": "2401210"
  },
  {
    "text": "Well, if you do\nPCA, well, you're just going to find a plane that\ncuts that thing somewhere, OK?",
    "start": "2401210",
    "end": "2407950"
  },
  {
    "text": "But if you try to embed the\ndata in some nonlinear way, you could try to resolve this. And this has been\nmuch of the research",
    "start": "2407950",
    "end": "2413829"
  },
  {
    "text": "done in the direction\nof manifold learning. Here there are just\na few keywords--",
    "start": "2413830",
    "end": "2421789"
  },
  {
    "text": "kernel PCA is the easiest\nversion, Laplacian map, eigenmaps, diffusion\nmaps, and so on, OK?",
    "start": "2421790",
    "end": "2428870"
  },
  {
    "text": "I only touch quickly\nupon random projection. There is a whole\nliterature about those.",
    "start": "2428870",
    "end": "2434309"
  },
  {
    "text": "The idea is, again, that\nby multiplying the data by random vectors, you can keep\nthe information in the data",
    "start": "2434310",
    "end": "2441570"
  },
  {
    "text": "and might be able\nto reconstruct them as well as to\npreserve distances. Also, you can combine ideas from\nsparsity with ideas from PCA.",
    "start": "2441570",
    "end": "2452780"
  },
  {
    "text": "For example, you can say, what\nif I want to know not only-- I want to find something\nlike an eigenvector,",
    "start": "2452780",
    "end": "2458000"
  },
  {
    "text": "but I would like the entries\nof the eigenvector to be 0. So can I add here a\nconstraint which basically",
    "start": "2458000",
    "end": "2463520"
  },
  {
    "text": "says, among all\nthe unit vectors, find the one whose\nentries are most-- so I want to add an\nL0 norm or L1 norm.",
    "start": "2463520",
    "end": "2470810"
  },
  {
    "text": "So how can you do that, OK? And this leads to sparse PCA\nand other structured matrix",
    "start": "2470810",
    "end": "2475930"
  },
  {
    "text": "estimation problems, OK? So this is, again, something\nI'm not going to tell you about, but that's kind\nof the beginning.",
    "start": "2475930",
    "end": "2481640"
  },
  {
    "text": "And this, more or less, brings\nus to the desert island,",
    "start": "2481640",
    "end": "2486650"
  },
  {
    "text": "and I'm done. ",
    "start": "2486650",
    "end": "2501995"
  }
]