[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13339"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13340",
    "end": "22189"
  },
  {
    "text": "PROFESSOR: OK. Well, last time I\nwas lecturing, we were talking about\nregression analysis.",
    "start": "22190",
    "end": "29380"
  },
  {
    "text": "And we finished up talking\nabout estimation methods for fitting regression models.",
    "start": "29380",
    "end": "34730"
  },
  {
    "text": "I want to recap the method\nof maximum likelihood, because this is really\nthe primary estimation",
    "start": "34730",
    "end": "42010"
  },
  {
    "text": "method in statistical\nmodeling that you start with. And so let me just\nreview where we were.",
    "start": "42010",
    "end": "49946"
  },
  {
    "text": "We have a normal linear\nregression model. A dependent variable\ny is explained",
    "start": "49946",
    "end": "55100"
  },
  {
    "text": "by a linear combination\nof independent variables given by a regression\nparameter beta.",
    "start": "55100",
    "end": "60710"
  },
  {
    "text": "And we assume that there are\nerrors about all the cases which are independent\nidentically distributed normal random variables.",
    "start": "60710",
    "end": "67440"
  },
  {
    "text": "So because of that relationship,\nthe dependent variable vector y, which is an\nn-vector, for n cases,",
    "start": "67440",
    "end": "75630"
  },
  {
    "text": "is a multivariate\nnormal random variable. Now, the likelihood function is\nequal to the density function",
    "start": "75630",
    "end": "86490"
  },
  {
    "text": "for the data. And there's some\nambiguity really",
    "start": "86490",
    "end": "92400"
  },
  {
    "text": "about how one manipulates\nthe likelihood function. The likelihood function\nbecomes defined once we've",
    "start": "92400",
    "end": "98780"
  },
  {
    "text": "observed a sample of data. So in this expression for\nthe likelihood function",
    "start": "98780",
    "end": "105390"
  },
  {
    "text": "as a function of beta\nand sigma squared, we're considering evaluating\nthe probability density",
    "start": "105390",
    "end": "110800"
  },
  {
    "text": "function for the\ndata conditional on the unknown parameters.",
    "start": "110800",
    "end": "117040"
  },
  {
    "text": "So if this were simply a\nunivariate normal distribution",
    "start": "117040",
    "end": "122540"
  },
  {
    "text": "with some unknown mean\nand variance, then what we would have is\njust a bell curve for mu",
    "start": "122540",
    "end": "130880"
  },
  {
    "text": "centered around a\nsingle observation y, if you look at the\nlikelihood function and how it varies with\nthe underlying mean",
    "start": "130880",
    "end": "139150"
  },
  {
    "text": "of the normal distribution. So this likelihood\nfunction is-- well,",
    "start": "139150",
    "end": "148180"
  },
  {
    "text": "the challenge really\nin maximum estimation is really calculating\nand computing",
    "start": "148180",
    "end": "154840"
  },
  {
    "text": "the likelihood function. And with normal linear\nregression models, it's very easy.",
    "start": "154840",
    "end": "160440"
  },
  {
    "text": "Now, the maximum\nlikelihood estimates are those values that\nmaximize this function.",
    "start": "160440",
    "end": "167490"
  },
  {
    "text": "And the question is, why\nare those good estimates of the underlying parameters?",
    "start": "167490",
    "end": "174840"
  },
  {
    "text": "Well, what those\nestimates do is they are the parameter values for\nwhich the observed data is",
    "start": "174840",
    "end": "183150"
  },
  {
    "text": "most likely. So we're able to scale\nthe unknown parameters",
    "start": "183150",
    "end": "189170"
  },
  {
    "text": "by how likely those parameters\ncould have generated these data values.",
    "start": "189170",
    "end": "195500"
  },
  {
    "text": "So let's look at the\nlikelihood function for this normal linear\nregression model.",
    "start": "195500",
    "end": "203360"
  },
  {
    "text": "These first two lines here are\nhighlighting-- the first line",
    "start": "203360",
    "end": "208520"
  },
  {
    "text": "is highlighting that\nour response variable values are independent.",
    "start": "208520",
    "end": "215310"
  },
  {
    "text": "They're conditionally\nindependent given the unknown parameters. And so the density of the\nfull vector of y's is simply",
    "start": "215310",
    "end": "223180"
  },
  {
    "text": "the product of the density\nfunctions for those components.",
    "start": "223180",
    "end": "228290"
  },
  {
    "text": "And because this is a normal\nlinear regression model, each of the y_i's is\nnormally distributed.",
    "start": "228290",
    "end": "235350"
  },
  {
    "text": "So what's in there\nis simply the density function of a normal random\nvariable with mean given",
    "start": "235350",
    "end": "241330"
  },
  {
    "text": "by the beta sum of independent\nvariables for each i,",
    "start": "241330",
    "end": "246960"
  },
  {
    "text": "case i, given by the\nregression parameters. And that expression\nbasically can be expressed",
    "start": "246960",
    "end": "258320"
  },
  {
    "text": "in matrix form this way. And what we have is\nthe likelihood function",
    "start": "258320",
    "end": "268910"
  },
  {
    "text": "ends up being a function\nof our Q of beta, which was our least squares criteria.",
    "start": "268910",
    "end": "275610"
  },
  {
    "text": "So the least squares\nestimation is equivalent to maximum likelihood\nestimation for the regression",
    "start": "275610",
    "end": "282930"
  },
  {
    "text": "parameters if we have a normal\nlinear regression model.",
    "start": "282930",
    "end": "288509"
  },
  {
    "text": "And there's this\nextra term, minus n. Well, actually, if we're going\nto maximize the likelihood",
    "start": "288510",
    "end": "294820"
  },
  {
    "text": "function, we can also maximize\nthe log of the likelihood function, because that's\njust a monotone function",
    "start": "294820",
    "end": "300010"
  },
  {
    "text": "of the likelihood. And it's easier to maximize the\nlog of the likelihood function which is expressed here.",
    "start": "300010",
    "end": "306430"
  },
  {
    "text": "And so we're able to\nmaximize over beta",
    "start": "306430",
    "end": "311460"
  },
  {
    "text": "by minimizing Q of beta. And then we can maximize\nover sigma squared",
    "start": "311460",
    "end": "318280"
  },
  {
    "text": "given our estimate for beta. And that's achieved by\ntaking the derivative",
    "start": "318280",
    "end": "325120"
  },
  {
    "text": "of the log-likelihood with\nrespect to sigma squared.",
    "start": "325120",
    "end": "331169"
  },
  {
    "text": "So we basically have this\nfirst order condition that finds the\nmaximum because things are appropriately convex.",
    "start": "331170",
    "end": "339830"
  },
  {
    "text": "And taking that derivative\nand solving for zero,",
    "start": "339830",
    "end": "345199"
  },
  {
    "text": "we basically get\nthis expression. So this is just\ntaking the derivative",
    "start": "345200",
    "end": "350380"
  },
  {
    "text": "of the log-likelihood with\nrespect to sigma squared. And you'll notice\nhere I'm taking",
    "start": "350380",
    "end": "355857"
  },
  {
    "text": "the derivative with\nrespect to sigma squared as a parameter, not sigma. ",
    "start": "355857",
    "end": "361870"
  },
  {
    "text": "And that gives us that\nthe maximum likelihood estimate of the error variance\nis Q of beta hat over n.",
    "start": "361870",
    "end": "370700"
  },
  {
    "text": "So this is the sum of the\nsquared residuals divided by n.",
    "start": "370700",
    "end": "377090"
  },
  {
    "text": "Now, I emphasize here\nthat that's biased. Who can tell me\nwhy that's biased",
    "start": "377090",
    "end": "384612"
  },
  {
    "text": "or why it ought to be biased? ",
    "start": "384612",
    "end": "390554"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE].  PROFESSOR: OK.",
    "start": "390554",
    "end": "396350"
  },
  {
    "text": "Well, it should be n\nminus 1 if we're actually",
    "start": "396350",
    "end": "402530"
  },
  {
    "text": "estimating one parameter. So if the independent variables\nwere, say, a constant, 1,",
    "start": "402530",
    "end": "414050"
  },
  {
    "text": "so we're just estimating a\nsample from a normal with mean beta 1 corresponding to\nthe units vector of the X,",
    "start": "414050",
    "end": "423030"
  },
  {
    "text": "then we would have a one\ndegree of freedom correction",
    "start": "423030",
    "end": "431410"
  },
  {
    "text": "to the residuals to get\nan unbiased estimator. But what if we\nhave p parameters?",
    "start": "431410",
    "end": "437150"
  },
  {
    "text": "Well, let me ask you this. What if we had n parameters\nin our regression model?",
    "start": "437150",
    "end": "443280"
  },
  {
    "text": "What would happen if\nwe had a full rank n independent variable matrix\nand n independent observations?",
    "start": "443280",
    "end": "450760"
  },
  {
    "text": " AUDIENCE: [INAUDIBLE]. PROFESSOR: Yes, you'd have\nan exact fit to the data.",
    "start": "450760",
    "end": "458410"
  },
  {
    "text": "So this estimate would be 0.",
    "start": "458410",
    "end": "463560"
  },
  {
    "text": "And so clearly, if\nthe data do arise from a normal linear regression\nmodel, 0 is not unbiased.",
    "start": "463560",
    "end": "472059"
  },
  {
    "text": "And you need to have\nsome correction. Turns out you need\nto divide by n",
    "start": "472059",
    "end": "478220"
  },
  {
    "text": "minus the rank of the X\nmatrix, the degrees of freedom in the model, to get\na biased estimate.",
    "start": "478220",
    "end": "485630"
  },
  {
    "text": "So this is an important\nissue, highlights how the more parameters you add\nin the model, the more precise",
    "start": "485630",
    "end": "491879"
  },
  {
    "text": "your fitted values are. In a sense, there's\ndangers of curve fitting which you want to avoid.",
    "start": "491880",
    "end": "498370"
  },
  {
    "text": "But the maximum likelihood\nestimates, in fact, are biased.",
    "start": "498370",
    "end": "505070"
  },
  {
    "text": "You just have to\nbe aware of that. And when you're using\ndifferent software, fitting different\nmodels, you need",
    "start": "505070",
    "end": "510170"
  },
  {
    "text": "to know whether there are\nvarious corrections be made for biasedness or not. ",
    "start": "510170",
    "end": "518370"
  },
  {
    "text": "So this solves the\nestimation problem for normal linear\nregression models.",
    "start": "518370",
    "end": "524790"
  },
  {
    "text": "And when we have normal\nlinear regression models, the theorem we\nwent through last time--",
    "start": "524790",
    "end": "530470"
  },
  {
    "text": "this is very important. Let me just go back and\nhighlight that for you. ",
    "start": "530470",
    "end": "542430"
  },
  {
    "text": "This theorem right here. This is really a very\nimportant theorem",
    "start": "542430",
    "end": "550010"
  },
  {
    "text": "indicating what is the\ndistribution of the least squares, now the maximum\nlikelihood estimates",
    "start": "550010",
    "end": "555800"
  },
  {
    "text": "of our regression model? They are normally distributed. And the residuals, sum\nof squares, have a chi",
    "start": "555800",
    "end": "565570"
  },
  {
    "text": "squared distribution\nwith degrees of freedom given by n minus p. And we can look at how\nmuch signal to noise",
    "start": "565570",
    "end": "574769"
  },
  {
    "text": "there is in estimating\nour regression parameters by calculating a t\nstatistic, which is take away",
    "start": "574770",
    "end": "580589"
  },
  {
    "text": "from an estimate its\nexpected value, its mean, and divide through by an\nestimate of the variability",
    "start": "580590",
    "end": "588330"
  },
  {
    "text": "in standard deviation units. And that will have\na t distribution. So that's a critical\nway to assess",
    "start": "588330",
    "end": "596800"
  },
  {
    "text": "the relevance of different\nexplanatory variables in our model. And this approach will apply\nwith maximum likelihood",
    "start": "596800",
    "end": "606060"
  },
  {
    "text": "estimation in all\nkinds of models apart from normal linear\nregression models. It turns out maximum\nlikelihood estimates generally",
    "start": "606060",
    "end": "613970"
  },
  {
    "text": "are asymptotically\nnormally distributed. And so these properties here\nwill apply for those models",
    "start": "613970",
    "end": "621630"
  },
  {
    "text": "as well. So let's finish up these\nnotes on estimation",
    "start": "621630",
    "end": "627470"
  },
  {
    "text": "by talking about\ngeneralized M estimation.",
    "start": "627470",
    "end": "632589"
  },
  {
    "text": "So what we want to consider is\nestimating unknown parameters",
    "start": "632590",
    "end": "639020"
  },
  {
    "text": "by minimizing some\nfunction, Q of beta,",
    "start": "639020",
    "end": "644630"
  },
  {
    "text": "which is a sum of evaluations\nof another function h,",
    "start": "644630",
    "end": "649890"
  },
  {
    "text": "evaluated for each of\nthe individual cases. And choosing h to take on\ndifferent functional forms",
    "start": "649890",
    "end": "659980"
  },
  {
    "text": "will define different\nkinds of estimators. We've seen how when h\nis simply the square",
    "start": "659980",
    "end": "668440"
  },
  {
    "text": "of the case minus its\nregression prediction,",
    "start": "668440",
    "end": "673880"
  },
  {
    "text": "that leads to least squares,\nand in fact, maximum likelihood",
    "start": "673880",
    "end": "678980"
  },
  {
    "text": "estimation, as we saw before. Rather than taking the\nsquare of the residual,",
    "start": "678980",
    "end": "687340"
  },
  {
    "text": "the fitted residual,\nwe could take simply the modulus of that.",
    "start": "687340",
    "end": "693509"
  },
  {
    "text": "And so that would be the\nmean absolute deviation. So rather than summing\nthe squared deviations",
    "start": "693510",
    "end": "699040"
  },
  {
    "text": "from the mean, we could\nsum the absolute deviations from the mean. Now, from a\nmathematical standpoint,",
    "start": "699040",
    "end": "706710"
  },
  {
    "text": "if we want to solve\nfor those estimates, how would you go\nabout doing that?",
    "start": "706710",
    "end": "712450"
  },
  {
    "text": " What methodology would you\nuse to maximize this function?",
    "start": "712450",
    "end": "721949"
  },
  {
    "text": "Well, we try and apply\nbasically the same principles of if this is a\nconvex function, then",
    "start": "721950",
    "end": "729690"
  },
  {
    "text": "we just want to take derivatives\nof that and solve for that being equal to 0. So what happens when\nyou take the derivative",
    "start": "729690",
    "end": "737079"
  },
  {
    "text": "of the modulus of y minus xi\nbeta with respect to beta? ",
    "start": "737080",
    "end": "744748"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. PROFESSOR: What did you say?",
    "start": "744749",
    "end": "750779"
  },
  {
    "text": "What did you say? AUDIENCE: Yeah, it's\nnot [INAUDIBLE].",
    "start": "750780",
    "end": "756783"
  },
  {
    "text": "The first [INAUDIBLE]\nderivative is not continuous. ",
    "start": "756783",
    "end": "765460"
  },
  {
    "text": "PROFESSOR: OK. Well, this is not\na smooth function.",
    "start": "765460",
    "end": "770940"
  },
  {
    "text": "But let me just plot x_i beta\nhere, and y_i minus that.",
    "start": "770940",
    "end": "786290"
  },
  {
    "text": "Basically, this is going\nto be a function that",
    "start": "786290",
    "end": "795060"
  },
  {
    "text": "has slope 1 when it's positive\nand slope minus 1 when it's negative.",
    "start": "795060",
    "end": "800449"
  },
  {
    "text": "And so that will be true,\ncomponent-wise, or for the y.",
    "start": "800450",
    "end": "806260"
  },
  {
    "text": "So what we end up\nwanting to do is find the value of the\nregression estimate that minimizes the\nsum of predictions",
    "start": "806260",
    "end": "816680"
  },
  {
    "text": "that are below the estimate plus\nthe sum of the predictions that are above the estimate given\nby the regression line.",
    "start": "816680",
    "end": "823240"
  },
  {
    "text": "And that solves the problem. Now, with the maximum\nlikelihood estimation,",
    "start": "823240",
    "end": "830960"
  },
  {
    "text": "one can plug in minus log the\ndensity of y_i given beta, x and sigma_i squared.",
    "start": "830960",
    "end": "837730"
  },
  {
    "text": "And that function simply sums\nto the log of the joint density",
    "start": "837730",
    "end": "844399"
  },
  {
    "text": "for all the data. So that works as well. With robust M estimators, we can\nconsider another function chi",
    "start": "844400",
    "end": "853520"
  },
  {
    "text": "which can be defined to have\ngood properties with estimates. And there's a whole theory\nof robust estimation--",
    "start": "853520",
    "end": "861065"
  },
  {
    "text": "it's very rich-- which\ntalks about how best to specify this chi function.",
    "start": "861065",
    "end": "867400"
  },
  {
    "text": "Now, one of the problems\nwith least squares estimation",
    "start": "867400",
    "end": "873130"
  },
  {
    "text": "is that the squares\nof very large values are very, very\nlarge in magnitude.",
    "start": "873130",
    "end": "880210"
  },
  {
    "text": "So there's perhaps\nan undue influence of very large values, very large\nresiduals under least squares",
    "start": "880210",
    "end": "887649"
  },
  {
    "text": "estimation and maximum\n[INAUDIBLE] estimation. So robust estimators\nallow you to control that",
    "start": "887650",
    "end": "893600"
  },
  {
    "text": "by defining the\nfunction differently. Finally, there are\nquantile estimators,",
    "start": "893600",
    "end": "900830"
  },
  {
    "text": "which extend the mean\nabsolute deviation criterion.",
    "start": "900830",
    "end": "907410"
  },
  {
    "text": "And so if we consider\nthe h function to be basically a\nmultiple of the deviation",
    "start": "907410",
    "end": "916269"
  },
  {
    "text": "if the residual is positive\nand a different multiple,",
    "start": "916270",
    "end": "923460"
  },
  {
    "text": "a complementary multiple if\nthe derivation, the residual, is less than 0,\nthen by varying tau,",
    "start": "923460",
    "end": "930910"
  },
  {
    "text": "you end up getting\nquantile estimators, where what you're doing is minimizing\nthe estimate of the tau",
    "start": "930910",
    "end": "938920"
  },
  {
    "text": "quantile. ",
    "start": "938921",
    "end": "947510"
  },
  {
    "text": "So this general\nclass of M estimators encompasses most\nestimators that we will",
    "start": "947510",
    "end": "954730"
  },
  {
    "text": "encounter in fitting models. So that finishes the technical\nor the mathematical discussion",
    "start": "954730",
    "end": "963130"
  },
  {
    "text": "of regression analysis. Let me highlight for you--\nthere's a case study that I",
    "start": "963130",
    "end": "991070"
  },
  {
    "text": "dragged to the desktop here. And I wanted to find that.",
    "start": "991070",
    "end": "997532"
  },
  {
    "text": "Let me find that. ",
    "start": "997532",
    "end": "1006970"
  },
  {
    "text": "There's a case study that's been\nadded to the course website.",
    "start": "1006970",
    "end": "1014300"
  },
  {
    "text": "And this first one is on\nlinear regression models for asset pricing.",
    "start": "1014300",
    "end": "1020370"
  },
  {
    "text": "And I want you to\nread through that just to see how it applies to\nfitting various simple linear",
    "start": "1020370",
    "end": "1028099"
  },
  {
    "text": "regression models. And enter full screen. ",
    "start": "1028099",
    "end": "1037900"
  },
  {
    "text": "This case study begins by\nintroducing the capital asset pricing model, which\nbasically suggests",
    "start": "1037900",
    "end": "1044670"
  },
  {
    "text": "that if you look at the\nreturns on any stocks in an efficient\nmarket, then those",
    "start": "1044670",
    "end": "1050720"
  },
  {
    "text": "should depend on the return\nof the overall market",
    "start": "1050720",
    "end": "1056830"
  },
  {
    "text": "but scaled by how\nrisky the stock is. And so if one looks\nat basically what",
    "start": "1056830",
    "end": "1065170"
  },
  {
    "text": "the return is on the\nstock on the right scale, you should have a simple\nlinear regression model. So here, we just look at\na time series for GE stock",
    "start": "1065170",
    "end": "1074110"
  },
  {
    "text": "in the S&P 500. And the case study guide\nthrough how you can actually collect this data\non the web using R.",
    "start": "1074110",
    "end": "1081790"
  },
  {
    "text": "And so the case notes\nprovide those details.",
    "start": "1081790",
    "end": "1086845"
  },
  {
    "text": " There's also the\nthree-month treasury rate",
    "start": "1086845",
    "end": "1091929"
  },
  {
    "text": "which is collected. And so if you're\nthinking about return on the stock versus return\non the index, well, what's",
    "start": "1091930",
    "end": "1099539"
  },
  {
    "text": "really of interest is the excess\nreturn over a risk-free rate.",
    "start": "1099540",
    "end": "1104940"
  },
  {
    "text": "And the efficient\nmarkets models, basically the excess\nreturn of a stock",
    "start": "1104940",
    "end": "1111389"
  },
  {
    "text": "is related to the excess\nreturn of the market as given by a linear\nregression model.",
    "start": "1111390",
    "end": "1117250"
  },
  {
    "text": "So we can fit this model. And here's a plot of the excess\nreturns on a daily basis for GE",
    "start": "1117250",
    "end": "1126360"
  },
  {
    "text": "stock versus the market. So that looks like a\nnice sort of point cloud",
    "start": "1126360",
    "end": "1132444"
  },
  {
    "text": "for which a linear\nmodel might fit well. And it does. ",
    "start": "1132444",
    "end": "1139400"
  },
  {
    "text": "Well, there are\nregression diagnostics, which I'll get to-- well, there\nare regression diagnostics",
    "start": "1139400",
    "end": "1145300"
  },
  {
    "text": "which are detailed in the\nproblem set, where we're looking at how influential are\nindividual observations, what's",
    "start": "1145300",
    "end": "1152420"
  },
  {
    "text": "their impact on\nregression parameters.  This display here\nbasically highlights",
    "start": "1152420",
    "end": "1160160"
  },
  {
    "text": "with a very simple\nlinear regression model what are the\ninfluential data points.",
    "start": "1160160",
    "end": "1165770"
  },
  {
    "text": "And so I've highlighted\nin red those values which are influential. Now, if you look at the\ndefinition of leverage",
    "start": "1165770",
    "end": "1174060"
  },
  {
    "text": "in a linear model,\nit's very simple. A simple linear model is\njust those observations that",
    "start": "1174060",
    "end": "1179130"
  },
  {
    "text": "are very far from the\nmean have large leverage. And so you can confirm\nthat with your answers",
    "start": "1179130",
    "end": "1186059"
  },
  {
    "text": "to the problem set. This x indicates a\nsignificantly influential point",
    "start": "1186060",
    "end": "1192710"
  },
  {
    "text": "in terms of the\nregression parameters given by Cook's distance. And that definition is also\ngiven in the case notes.",
    "start": "1192710",
    "end": "1199955"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE].  PROFESSOR: By computing\nthe individual",
    "start": "1199956",
    "end": "1206630"
  },
  {
    "text": "leverages with a function\nthat's given here, and by selecting out those\nthat exceed a given magnitude.",
    "start": "1206630",
    "end": "1213385"
  },
  {
    "text": " Now, with this very,\nvery simple model",
    "start": "1213385",
    "end": "1220530"
  },
  {
    "text": "of stocks depending\non one unknown factor, risk factor given the market.",
    "start": "1220530",
    "end": "1226110"
  },
  {
    "text": "In modeling equity\nreturns, there are many different factors that\ncan have an impact on returns.",
    "start": "1226110",
    "end": "1233679"
  },
  {
    "text": "So what I've done\nin the case study is to look at adding\nanother factor which is just",
    "start": "1233680",
    "end": "1248660"
  },
  {
    "text": "the return on crude oil. And so-- I need to go down here.",
    "start": "1248660",
    "end": "1255210"
  },
  {
    "start": "1255210",
    "end": "1264090"
  },
  {
    "text": "So let me highlight\nsomething for you here.",
    "start": "1264090",
    "end": "1270260"
  },
  {
    "text": "With GE stock, what would you\nexpect the impact of, say, a high return on crude oil to\nbe on the return of GE stock?",
    "start": "1270260",
    "end": "1279260"
  },
  {
    "text": "Would you expect it to\nbe positively related or negatively related? ",
    "start": "1279260",
    "end": "1290910"
  },
  {
    "text": "OK.  Well, GE is a stock that's\njust a broad stock invested",
    "start": "1290910",
    "end": "1299610"
  },
  {
    "text": "in many different industries. And it really reflects the\noverall market, to some extent.",
    "start": "1299610",
    "end": "1305389"
  },
  {
    "text": "Many years ago,\n10, 15 years ago, GE represented maybe 3% of\nthe GNP of the US market.",
    "start": "1305390",
    "end": "1311960"
  },
  {
    "text": "So it was really highly related\nto how well the market does. Now, crude oil is a commodity.",
    "start": "1311960",
    "end": "1319700"
  },
  {
    "text": "And oil is used to drive cars,\nto fuel energy production.",
    "start": "1319700",
    "end": "1327010"
  },
  {
    "text": "So if you have an\nincrease in oil prices, then the cost of essentially\ndoing business goes up.",
    "start": "1327010",
    "end": "1333770"
  },
  {
    "text": "So it is associated with\nan inflation factor.",
    "start": "1333770",
    "end": "1338870"
  },
  {
    "text": "Prices are rising. So if you can see here,\nthe regression estimate,",
    "start": "1338870",
    "end": "1345730"
  },
  {
    "text": "if we add in a factor of\nthe return on crude oil, it's negative 0.03.",
    "start": "1345730",
    "end": "1352120"
  },
  {
    "text": "And it has a t value\nof minus 3.561. So in fact, the market, in\na sense, over this period,",
    "start": "1352120",
    "end": "1361330"
  },
  {
    "text": "for this analysis, was not\nefficient in explaining the return on GE; crude oil\nis another independent factor",
    "start": "1361330",
    "end": "1369730"
  },
  {
    "text": "that helps explain returns. So that's useful to know.",
    "start": "1369730",
    "end": "1375850"
  },
  {
    "text": "And if you are clever about\ndefining and identifying",
    "start": "1375850",
    "end": "1381429"
  },
  {
    "text": "and evaluating\ndifferent factors, you can build\nfactor asset pricing",
    "start": "1381430",
    "end": "1387549"
  },
  {
    "text": "models that are\nvery, very useful for investing and trading.",
    "start": "1387550",
    "end": "1393390"
  },
  {
    "text": "Now, as a comparison\nto this case study,",
    "start": "1393390",
    "end": "1398710"
  },
  {
    "text": "also applied the same\nanalysis to Exxon Mobil.",
    "start": "1398710",
    "end": "1406039"
  },
  {
    "text": "Now, Exxon Mobil\nis an oil company. So let me highlight this here.",
    "start": "1406040",
    "end": "1415529"
  },
  {
    "text": "We basically are\nfitting this model. Now let's highlight it. ",
    "start": "1415530",
    "end": "1423150"
  },
  {
    "text": "Here, if we consider\nthis two-factor model,",
    "start": "1423150",
    "end": "1428960"
  },
  {
    "text": "the regression\nparameter corresponding to the crude oil factor is\nplus 0.13 with a t value of 16.",
    "start": "1428960",
    "end": "1437840"
  },
  {
    "text": "So crude oil definitely\nhas an impact on the return of Exxon Mobil,\nbecause it goes up and down",
    "start": "1437840",
    "end": "1446370"
  },
  {
    "text": "with oil prices. ",
    "start": "1446370",
    "end": "1456299"
  },
  {
    "text": "This case study closes\nwith a scatter plot of the independent variables\nand highlighting where",
    "start": "1456300",
    "end": "1462950"
  },
  {
    "text": "the influential values are. And so just in the same way that\nwith a simple linear regression",
    "start": "1462950",
    "end": "1468650"
  },
  {
    "text": "it was those that were far\naway from the mean of the data were influential, in a\nmultivariate setting-- here,",
    "start": "1468650",
    "end": "1475920"
  },
  {
    "text": "it's bivariate-- the\ninfluential observations are those that are very\nfar away from the centroid.",
    "start": "1475920",
    "end": "1481240"
  },
  {
    "text": "And if you look at one of the\nproblems in the problem set, it actually goes\nthrough and you can see where these\nleveraged values are",
    "start": "1481240",
    "end": "1488930"
  },
  {
    "text": "and how it indicates influences\nassociated with the Mahalanobis distance of cases\nfrom the centroid",
    "start": "1488930",
    "end": "1496659"
  },
  {
    "text": "of the independent variables. So if you're a visual\ntype mathematician as",
    "start": "1496660",
    "end": "1502009"
  },
  {
    "text": "opposed to an algebraic\ntype mathematician, I think these\nkinds of graphs are very helpful in understanding\nwhat is really going on.",
    "start": "1502010",
    "end": "1510970"
  },
  {
    "text": "And the degree of influence\nis associated with the fact",
    "start": "1510970",
    "end": "1516179"
  },
  {
    "text": "that we're basically taking\nleast squares estimates,",
    "start": "1516180",
    "end": "1521380"
  },
  {
    "text": "so we have the quadratic\nform associated with the overall process. ",
    "start": "1521380",
    "end": "1528800"
  },
  {
    "text": "There's another\ncase study that I'll",
    "start": "1528800",
    "end": "1533950"
  },
  {
    "text": "be happy to discuss after\nclass or during office hours.",
    "start": "1533950",
    "end": "1540054"
  },
  {
    "text": "I don't think we have time\ntoday during the lecture. But it concerns\nexchange rate regimes.",
    "start": "1540054",
    "end": "1545650"
  },
  {
    "text": "And the second case study\nlooks at the Chinese yuan,",
    "start": "1545650",
    "end": "1551310"
  },
  {
    "text": "which was basically pegged\nto the dollar for many years. And then I guess through\npolitical influence",
    "start": "1551310",
    "end": "1560190"
  },
  {
    "text": "from other countries,\nthey started to let the yuan vary\nfrom the dollar,",
    "start": "1560190",
    "end": "1566172"
  },
  {
    "text": "but perhaps pegged\nit to some basket of securities-- of currencies. And so how would you determine\nwhat that basket of currencies",
    "start": "1566172",
    "end": "1573539"
  },
  {
    "text": "is? Well, there are\nregression methods that have been\ndeveloped by economists",
    "start": "1573540",
    "end": "1579490"
  },
  {
    "text": "that help you do that. And that case study goes\nthrough the analysis of that. So check that out to see how\nyou can get immediate access",
    "start": "1579490",
    "end": "1586770"
  },
  {
    "text": "to currency data and be\nfitting these regression models and looking at the\ndifferent results and trying to evaluate those.",
    "start": "1586770",
    "end": "1592458"
  },
  {
    "start": "1592458",
    "end": "1598720"
  },
  {
    "text": "So let's turn now\nto the main topic--",
    "start": "1598720",
    "end": "1608169"
  },
  {
    "text": "let's see here-- which\nis time series analysis.",
    "start": "1608170",
    "end": "1614200"
  },
  {
    "start": "1614200",
    "end": "1621250"
  },
  {
    "text": "Today in the rest\nof the lecture, I want to talk about univariate\ntime series analysis.",
    "start": "1621250",
    "end": "1629039"
  },
  {
    "text": "And so we're thinking of\nbasically a random variable that is observed over time and\nit's a discrete time process.",
    "start": "1629040",
    "end": "1637720"
  },
  {
    "text": "And we'll introduce you\nto the Wold representation",
    "start": "1637720",
    "end": "1643140"
  },
  {
    "text": "theorem and definitions\nof stationarity and its relationship there.",
    "start": "1643140",
    "end": "1648340"
  },
  {
    "text": "Then, look at the classic\nmodels of autoregressive moving average models.",
    "start": "1648340",
    "end": "1654120"
  },
  {
    "text": "And then extending those\nto non-stationarity with integrated autoregressive\nmoving average models.",
    "start": "1654120",
    "end": "1660430"
  },
  {
    "text": "And then finally, talk about\nestimating stationary models and how we test\nfor stationarity.",
    "start": "1660430",
    "end": "1667629"
  },
  {
    "text": "So let's begin from\nbasically first principles.",
    "start": "1667630",
    "end": "1674740"
  },
  {
    "text": "We have a stochastic process,\na discrete time stochastic process, X, which consists\nof random variables indexed",
    "start": "1674740",
    "end": "1684880"
  },
  {
    "text": "by time. And we're thinking\nnow discrete time. The stochastic behavior\nof this sequence",
    "start": "1684880",
    "end": "1691820"
  },
  {
    "text": "is determined by specifying\nthe density or probability mass functions for all finite\ncollections of time indexes.",
    "start": "1691820",
    "end": "1702220"
  },
  {
    "text": "And so if we could specify\nall finite.dimensional distributions of\nthis process, we",
    "start": "1702220",
    "end": "1708130"
  },
  {
    "text": "would specify this\nprobability model for the stochastic process.",
    "start": "1708130",
    "end": "1715200"
  },
  {
    "text": "Now, this stochastic process\nis strictly stationary",
    "start": "1715200",
    "end": "1720500"
  },
  {
    "text": "if the density function for\nany collection of times,",
    "start": "1720500",
    "end": "1728760"
  },
  {
    "text": "t_1 through t_m, is equal to\nthe density function for a tau",
    "start": "1728760",
    "end": "1735780"
  },
  {
    "text": "translation of that. So the density function for any\nfinite-dimensional distribution",
    "start": "1735780",
    "end": "1743000"
  },
  {
    "text": "is stationary, is constant\nunder arbitrary translations.",
    "start": "1743000",
    "end": "1748300"
  },
  {
    "text": "So that's a very\nstrong property. But it's a reasonable\nproperty to ask for if you're",
    "start": "1748300",
    "end": "1756620"
  },
  {
    "text": "doing statistical modeling. And what do you want to do\nwhen you're estimating models? You want to estimate\nthings that are constant.",
    "start": "1756620",
    "end": "1764080"
  },
  {
    "text": "Constants are nice\nthings to estimate. And parameters of\nmodels are constant. So we really want the underlying\nstructure of the distributions",
    "start": "1764080",
    "end": "1772930"
  },
  {
    "text": "to be the same. ",
    "start": "1772930",
    "end": "1784960"
  },
  {
    "text": "That was strict\nstationarity, which requires knowledge of\nthe entire distribution",
    "start": "1784960",
    "end": "1791510"
  },
  {
    "text": "of the stochastic process. We're now going to introduce\na weaker definition, which",
    "start": "1791510",
    "end": "1797340"
  },
  {
    "text": "is covariance stationarity. And a covariance\nstationary process",
    "start": "1797340",
    "end": "1802960"
  },
  {
    "text": "has a constant mean,\nmu; a constant variance,",
    "start": "1802960",
    "end": "1808330"
  },
  {
    "text": "sigma squared; and a\ncovariance over increments tau,",
    "start": "1808330",
    "end": "1815630"
  },
  {
    "text": "given by a function gamma of\ntau, that is also constant. Gamma isn't a constant function,\nbut basically for all t,",
    "start": "1815630",
    "end": "1826960"
  },
  {
    "text": "covariance of X_t, X_(t+tau)\nis this gamma of tau function. And we also can introduce\nthe autocorrelation function",
    "start": "1826960",
    "end": "1838080"
  },
  {
    "text": "of the stochastic\nprocess, rho of tau. And so the correlation\nof two random variables",
    "start": "1838080",
    "end": "1849120"
  },
  {
    "text": "is the covariance of those\nrandom variables divided by the square root of the\nproduct of the variances.",
    "start": "1849120",
    "end": "1857340"
  },
  {
    "text": "And Choongbum I think\nintroduced that a bit. in one of his lectures,\nwhere we were talking",
    "start": "1857340",
    "end": "1862680"
  },
  {
    "text": "about the correlation function. But essentially, the\ncorrelation function",
    "start": "1862680",
    "end": "1869810"
  },
  {
    "text": "is if you standardize the\ndata or the random variables",
    "start": "1869810",
    "end": "1875400"
  },
  {
    "text": "to have mean 0-- so\nsubtract off the means and then divide through by\ntheir standard deviations.",
    "start": "1875400",
    "end": "1881039"
  },
  {
    "text": "So those translated variables\nhave mean 0 and variance 1.",
    "start": "1881040",
    "end": "1886410"
  },
  {
    "text": "Then the correlation\ncoefficient is the covariance between those standardized\nrandom variables. ",
    "start": "1886410",
    "end": "1895019"
  },
  {
    "text": "So this is going to come up\nagain and again in time series analysis.",
    "start": "1895020",
    "end": "1900080"
  },
  {
    "text": "Now, the Wold\nrepresentation theorem is a very, very powerful theorem\nabout covariance stationary",
    "start": "1900080",
    "end": "1907350"
  },
  {
    "text": "processes.  It basically states that if\nwe have a zero-mean covariance",
    "start": "1907350",
    "end": "1915049"
  },
  {
    "text": "stationary time\nseries, then it can be decomposed into two\ncomponents with a very",
    "start": "1915050",
    "end": "1923520"
  },
  {
    "text": "nice structure. Basically, X_t can be\ndecomposed into V_t plus S_t.",
    "start": "1923520",
    "end": "1931430"
  },
  {
    "text": "V_t is going to be a linearly\ndeterministic process, meaning",
    "start": "1931430",
    "end": "1938470"
  },
  {
    "text": "that past values of\nV_t perfectly predict what V_t is going to be.",
    "start": "1938470",
    "end": "1944590"
  },
  {
    "text": "So this could be like a\nlinear trend or some fixed function of past values.",
    "start": "1944590",
    "end": "1949660"
  },
  {
    "text": "It's basically a\ndeterministic process. So there's nothing\nrandom in V_t.",
    "start": "1949660",
    "end": "1954690"
  },
  {
    "text": "It's something that's\nfixed, without randomness.",
    "start": "1954690",
    "end": "1960710"
  },
  {
    "text": "And S_t is a sum\nof coefficients,",
    "start": "1960710",
    "end": "1966510"
  },
  {
    "text": "psi_i times eta_(t-i), where\nthe eta_t's are linearly",
    "start": "1966510",
    "end": "1976650"
  },
  {
    "text": "unpredictable white noise. So what we have is S_t\nis a weighted average",
    "start": "1976650",
    "end": "1983890"
  },
  {
    "text": "of white noise with\ncoefficients given by the psi_i.",
    "start": "1983890",
    "end": "1989850"
  },
  {
    "text": "And the coefficients psi_i\nare such that psi_0 is 1,",
    "start": "1989850",
    "end": "1996169"
  },
  {
    "text": "and the sum of the\nsquared psi_i's is finite. ",
    "start": "1996170",
    "end": "2001340"
  },
  {
    "text": "And the white noise\neta_t-- what's white noise?",
    "start": "2001340",
    "end": "2006539"
  },
  {
    "text": "It has expectation zero. It has variance, given by\nsigma squared, that's constant.",
    "start": "2006540",
    "end": "2015120"
  },
  {
    "text": "And it has covariance across\ndifferent white noise elements that's 0 for all t and s.",
    "start": "2015120",
    "end": "2022490"
  },
  {
    "text": "So eta_t's are uncorrelated\nwith themselves, and of course, they\nare uncorrelated",
    "start": "2022490",
    "end": "2027750"
  },
  {
    "text": "with the deterministic process. So this is really a very,\nvery powerful concept.",
    "start": "2027750",
    "end": "2038010"
  },
  {
    "text": "If you are modeling\na process and it has covariance\nstationarity, then there",
    "start": "2038010",
    "end": "2045030"
  },
  {
    "text": "exists a representation\nlike this of the function. So it's a very\ncompelling structure,",
    "start": "2045030",
    "end": "2055750"
  },
  {
    "text": "which we'll see how it applies\nin different circumstances. Now, before getting into the\ndefinition of autoregressive",
    "start": "2055750",
    "end": "2065649"
  },
  {
    "text": "moving average\nmodels, I just want to give you an intuitive\nunderstanding of what's going",
    "start": "2065650",
    "end": "2073820"
  },
  {
    "text": "on with the Wold decomposition. And this, I think,\nwill help motivate",
    "start": "2073820",
    "end": "2081030"
  },
  {
    "text": "why the Wold\ndecomposition should exist from a mathematical standpoint.",
    "start": "2081030",
    "end": "2088169"
  },
  {
    "text": "So consider just some\nunivariate stochastic process,",
    "start": "2088170",
    "end": "2093549"
  },
  {
    "text": "some time series X_t\nthat we want to model. And we believe that it's\ncovariance stationary.",
    "start": "2093550",
    "end": "2100010"
  },
  {
    "text": "And so we want to\nspecify essentially the Wold decomposition of that. Well, what we could\ndo is initialize",
    "start": "2100010",
    "end": "2107680"
  },
  {
    "text": "a parameter p, the number\nof past observations, in the linearly\ndeterministic term.",
    "start": "2107680",
    "end": "2115309"
  },
  {
    "text": "And then estimate the linear\nprojection of X_t on the last p",
    "start": "2115310",
    "end": "2124420"
  },
  {
    "text": "lag values. And so what I want to do\nis consider estimating",
    "start": "2124420",
    "end": "2131490"
  },
  {
    "text": "that relationship using\na sample of size n with some ending point t_0\nless than or equal to T.",
    "start": "2131490",
    "end": "2142660"
  },
  {
    "text": "And so we can consider y\nvalues like a response variable",
    "start": "2142660",
    "end": "2150010"
  },
  {
    "text": "being given by the successive\nvalues of our time series.",
    "start": "2150010",
    "end": "2157760"
  },
  {
    "text": "And so our response variables\ny_j can be considered to be x t_0 minus n plus j.",
    "start": "2157760",
    "end": "2166040"
  },
  {
    "text": "And define a y vector and\na Z matrix as follows.",
    "start": "2166040",
    "end": "2174350"
  },
  {
    "start": "2174350",
    "end": "2180140"
  },
  {
    "text": "So we have values of our\nstochastic process in y.",
    "start": "2180140",
    "end": "2185890"
  },
  {
    "text": "And then our Z matrix,\nwhich is essentially a matrix of\nindependent variables, is just the lagged\nvalues of this process.",
    "start": "2185890",
    "end": "2196000"
  },
  {
    "text": "So let's apply\nordinary least squares to specify the projection. This projection matrix\nshould be familiar now.",
    "start": "2196000",
    "end": "2203810"
  },
  {
    "text": "And that basically gives\nus a prediction of y hat",
    "start": "2203810",
    "end": "2209160"
  },
  {
    "text": "depending on p lags. And we can compute the\nprojection residual",
    "start": "2209160",
    "end": "2214750"
  },
  {
    "text": "from that fit.  Well, we can conduct\ntime series methods",
    "start": "2214750",
    "end": "2223450"
  },
  {
    "text": "to analyze these residuals,\nwhich we'll be introducing here",
    "start": "2223450",
    "end": "2228470"
  },
  {
    "text": "in a few minutes, to specify\na moving average model. We can then have estimates of\nthe underlying coefficients",
    "start": "2228470",
    "end": "2236180"
  },
  {
    "text": "psi and estimates of\nthese residuals eta_t.",
    "start": "2236180",
    "end": "2242700"
  },
  {
    "text": "And then we can evaluate whether\nthis is a good model or not. What does it mean to be\nan appropriate model?",
    "start": "2242700",
    "end": "2249430"
  },
  {
    "text": "Well, the residual should\nbe orthogonal to longer lags",
    "start": "2249430",
    "end": "2255250"
  },
  {
    "text": "than t minus s, or\nlonger lags than p. So we basically shouldn't\nhave any dependence",
    "start": "2255250",
    "end": "2262850"
  },
  {
    "text": "of our residuals on lags\nof the stochastic process",
    "start": "2262850",
    "end": "2269390"
  },
  {
    "text": "that weren't included\nin the model. Those should be orthogonal.",
    "start": "2269390",
    "end": "2274850"
  },
  {
    "text": "And the eta_t hats should be\nconsistent with white noise.",
    "start": "2274850",
    "end": "2281070"
  },
  {
    "text": "So those issues\ncan be evaluated. And if there's\nevidence otherwise,",
    "start": "2281070",
    "end": "2287619"
  },
  {
    "text": "then we can change the\nspecification of the model. We can add additional lags.",
    "start": "2287620",
    "end": "2293090"
  },
  {
    "text": "We can add additional\ndeterministic variables if we can identify\nwhat those might be.",
    "start": "2293090",
    "end": "2301570"
  },
  {
    "text": "And proceed with this process. But essentially that is\nhow the Wold decomposition",
    "start": "2301570",
    "end": "2308490"
  },
  {
    "text": "could be implemented. And theoretically, as\nour sample gets large,",
    "start": "2308490",
    "end": "2315250"
  },
  {
    "text": "if we're observing this time\nseries for a long time, then",
    "start": "2315250",
    "end": "2322320"
  },
  {
    "text": "well certainly the\nlimit of the projections as p, the number of lags\nwe include, gets large,",
    "start": "2322320",
    "end": "2329110"
  },
  {
    "text": "should be essentially\nthe projection of our data on its history.",
    "start": "2329110",
    "end": "2335269"
  },
  {
    "text": "And that, in fact, is the\nprojection corresponding to,",
    "start": "2335270",
    "end": "2340490"
  },
  {
    "text": "defining, the\ncoefficient's psi_i. And so in the limit, that\nprojection will converge",
    "start": "2340490",
    "end": "2349400"
  },
  {
    "text": "and it will converge\nin the sense that the coefficients of\nthe projection definition",
    "start": "2349400",
    "end": "2355070"
  },
  {
    "text": "correspond to the psi_i. And now if p goes to\ninfinity is required,",
    "start": "2355070",
    "end": "2366600"
  },
  {
    "text": "now p means that there's\nbasically a long term dependence in the process. ",
    "start": "2366600",
    "end": "2374310"
  },
  {
    "text": "Basically, it doesn't\nstop at a given lag. The dependence\npersists over time.",
    "start": "2374310",
    "end": "2381410"
  },
  {
    "text": "Then we may require\nthat p goes to infinity. Now, what happens when\np goes to infinity?",
    "start": "2381410",
    "end": "2387360"
  },
  {
    "text": "Well, if you let p go\nto infinity too quickly, you run out of\ndegrees of freedom to estimate your models.",
    "start": "2387360",
    "end": "2393520"
  },
  {
    "text": "And so from an\nimplementation standpoint, you need to let p/n\ngo to 0 so that you",
    "start": "2393520",
    "end": "2401340"
  },
  {
    "text": "have essentially more\ndata than parameters",
    "start": "2401340",
    "end": "2409180"
  },
  {
    "text": "that you're estimating. And so that is required. And in time series\nmodeling, what we",
    "start": "2409180",
    "end": "2418860"
  },
  {
    "text": "look for are models where\nfinite values of p are required.",
    "start": "2418860",
    "end": "2426609"
  },
  {
    "text": "So we're only estimating a\nfinite number of parameters. Or if we have a moving\naverage model which",
    "start": "2426609",
    "end": "2431920"
  },
  {
    "text": "has coefficients that\nare infinite in number, perhaps those can be defined by\na small number of parameters.",
    "start": "2431920",
    "end": "2440430"
  },
  {
    "text": "So we'll be looking for\nthat kind of feature in different models. ",
    "start": "2440430",
    "end": "2449230"
  },
  {
    "text": "Let's turn to talking\nabout the lag operator. The lag operator is\na fundamental tool",
    "start": "2449230",
    "end": "2456250"
  },
  {
    "text": "in time series models. We consider the operator L\nthat shifts a time series back",
    "start": "2456250",
    "end": "2464180"
  },
  {
    "text": "by one time increment. And applying this\noperator recursively,",
    "start": "2464180",
    "end": "2469210"
  },
  {
    "text": "we get, if it's operating\n0 times, there's no lag,",
    "start": "2469210",
    "end": "2474400"
  },
  {
    "text": "one time, there's\none lag, two times, two lags-- doing\nthat iteratively. And in thinking of these,\nwhat we're dealing with",
    "start": "2474400",
    "end": "2482470"
  },
  {
    "text": "is like a transformation on\ninfinite dimensional space, where it's like\nthe identity matrix",
    "start": "2482470",
    "end": "2489150"
  },
  {
    "text": "sort of shifted by\none element-- or not the identity, but an element.",
    "start": "2489150",
    "end": "2495319"
  },
  {
    "text": "It's like the identity\nmatrix shifted by one column or two columns.",
    "start": "2495320",
    "end": "2501520"
  },
  {
    "text": "So anyway, inverses\nof these operators are well defined in terms\nof what we get from them.",
    "start": "2501520",
    "end": "2509440"
  },
  {
    "text": "So we can represent\nthe Wold representation in terms of these lag\noperators by saying",
    "start": "2509440",
    "end": "2518140"
  },
  {
    "text": "that our stochastic\nprocess X_t is equal to V_t plus this\npsi of L function,",
    "start": "2518140",
    "end": "2530030"
  },
  {
    "text": "basically a\nfunctional of the lag operator, which is a potentially\ninfinite-order polynomial",
    "start": "2530030",
    "end": "2538570"
  },
  {
    "text": "of the lags. So this notation is\nsomething that you",
    "start": "2538570",
    "end": "2543770"
  },
  {
    "text": "need to get very\nfamiliar with if you're going to be comfortable with\nthe different models that are introduced with\nARMA and ARIMA models.",
    "start": "2543770",
    "end": "2553840"
  },
  {
    "text": "Any questions about that? ",
    "start": "2553840",
    "end": "2562230"
  },
  {
    "text": "Now relating to\nthis-- let me just introduce now, because this\nwill come up somewhat later.",
    "start": "2562230",
    "end": "2567550"
  },
  {
    "text": "But there's the impulse\nresponse function of the covariance\nstationary process.",
    "start": "2567550",
    "end": "2573010"
  },
  {
    "text": "If we have a stochastic process\nX_t which is given by this Wold",
    "start": "2573010",
    "end": "2578630"
  },
  {
    "text": "representation, then\nyou can ask yourself",
    "start": "2578630",
    "end": "2585950"
  },
  {
    "text": "what happens to the innovation\nat time t, which is eta_t,",
    "start": "2585950",
    "end": "2591320"
  },
  {
    "text": "how does that affect\nthe process over time? And so, OK, pretend that you are\nchairman of the Federal Reserve",
    "start": "2591320",
    "end": "2601590"
  },
  {
    "text": "Bank. And you're interested in the GNP\nor basically economic growth.",
    "start": "2601590",
    "end": "2609600"
  },
  {
    "text": "And you're considering\nchanging interest rates to help the economy.",
    "start": "2609600",
    "end": "2616340"
  },
  {
    "text": "Well, you'd like to\nknow what an impact is of your change in\nthis factor, how",
    "start": "2616340",
    "end": "2622609"
  },
  {
    "text": "that's going to affect the\nvariable of interest, perhaps GNP.",
    "start": "2622610",
    "end": "2628130"
  },
  {
    "text": "Now, in this case,\nwe're thinking of just a simple covariance\nstationary stochastic process.",
    "start": "2628130",
    "end": "2635140"
  },
  {
    "text": "It's basically a process that\nis a random-- a weighted sum,",
    "start": "2635140",
    "end": "2640164"
  },
  {
    "text": "a moving average of\ninnovations eta_t. But the question is, basically\nany covariance stationary",
    "start": "2640165",
    "end": "2646130"
  },
  {
    "text": "process could be\nrepresented in this form. And the impulse\nresponse function",
    "start": "2646130",
    "end": "2651630"
  },
  {
    "text": "relates to what is\nthe impact of eta_t. What's its impact over time?",
    "start": "2651630",
    "end": "2658120"
  },
  {
    "text": "Basically, it affects\nthe process at time t. That, because of the\nmoving average process,",
    "start": "2658120",
    "end": "2664359"
  },
  {
    "text": "it affects it at t plus\n1, affects it at t plus 2. And so this impulse\nresponse is basically",
    "start": "2664360",
    "end": "2673810"
  },
  {
    "text": "the derivative of the\nvalue of the process with the j-th previous\ninnovation is given by psi_j.",
    "start": "2673810",
    "end": "2684210"
  },
  {
    "text": "So the different\ninnovations have an impact on the current value given by\nthis impulse response function.",
    "start": "2684210",
    "end": "2691200"
  },
  {
    "text": "So looking backward,\nthat definition is pretty well defined. But you can also\nthink about how does",
    "start": "2691200",
    "end": "2696630"
  },
  {
    "text": "an impact of the\ninnovation affect the process going forward. And the long-run\ncumulative response",
    "start": "2696630",
    "end": "2703430"
  },
  {
    "text": "is essentially what is the\nimpact of that innovation in the process ultimately?",
    "start": "2703430",
    "end": "2711350"
  },
  {
    "text": "And eventually, it's\nnot going to change the value of the process. But what is the value to\nwhich the process is moving",
    "start": "2711350",
    "end": "2718710"
  },
  {
    "text": "because of that one innovation? And so the long run\ncumulative response is given by basically the\nsum of these individual ones.",
    "start": "2718710",
    "end": "2728900"
  },
  {
    "text": "And it's given by the\nsum of the psi_i's. So that's the polynomial of\npsi with lag operator, where we",
    "start": "2728900",
    "end": "2737295"
  },
  {
    "text": "replace the lag operator by 1. ",
    "start": "2737295",
    "end": "2743540"
  },
  {
    "text": "We'll see this\nagain when we talk about vector\nautoregressive processes",
    "start": "2743540",
    "end": "2750546"
  },
  {
    "text": "with multivariate time series. ",
    "start": "2750546",
    "end": "2756020"
  },
  {
    "text": "Now, the Wold\nrepresentation, which is a infinite-order moving\naverage, possibly infinite order, can have an\nautoregressive representation.",
    "start": "2756020",
    "end": "2764466"
  },
  {
    "text": " Suppose that there is\nanother polynomial psi_i",
    "start": "2764466",
    "end": "2777579"
  },
  {
    "text": "star of the lags, which we're\ngoing to call psi inverse of L,",
    "start": "2777580",
    "end": "2783240"
  },
  {
    "text": "which satisfies the fact if you\nmultiply that with psi of L,",
    "start": "2783240",
    "end": "2789860"
  },
  {
    "text": "you get the identity lag 0. Then this psi inverse,\nif that exists,",
    "start": "2789860",
    "end": "2797819"
  },
  {
    "text": "is basically the\ninverse of the psi of L.",
    "start": "2797820",
    "end": "2807060"
  },
  {
    "text": "So if we start with psi of\nL, if that's invertible, then there exists\na psi inverse of L,",
    "start": "2807060",
    "end": "2812510"
  },
  {
    "text": "with coefficients psi_i star. And one can basically take\nour original expression",
    "start": "2812510",
    "end": "2822130"
  },
  {
    "text": "for the stochastic process,\nwhich is as this moving average of the eta's, and express it\nas this essentially moving",
    "start": "2822130",
    "end": "2833250"
  },
  {
    "text": "averages of the X's. And so we've essentially\ninverted the process",
    "start": "2833250",
    "end": "2840730"
  },
  {
    "text": "and shown that the\nstochastic process can",
    "start": "2840730",
    "end": "2847500"
  },
  {
    "text": "be expressed as an infinite\norder autoregressive",
    "start": "2847500",
    "end": "2855570"
  },
  {
    "text": "representation. And so this infinite order\nautoregressive representation",
    "start": "2855570",
    "end": "2860760"
  },
  {
    "text": "corresponds to that intuitive\nunderstanding of how the Wold representation exists.",
    "start": "2860760",
    "end": "2866280"
  },
  {
    "text": "And it actually works with the--\nthe regression coefficients",
    "start": "2866280",
    "end": "2871330"
  },
  {
    "text": "in that projection several\nslides back corresponds to this inverse operator. ",
    "start": "2871330",
    "end": "2879030"
  },
  {
    "text": "So let's turn to some\nspecific time series",
    "start": "2879030",
    "end": "2884160"
  },
  {
    "text": "models that are widely used. The class of autoregressive\nmoving average processes",
    "start": "2884160",
    "end": "2891670"
  },
  {
    "text": "has this mathematical\ndefinition. We define the X_t to be equal\nto a linear combination of lags",
    "start": "2891670",
    "end": "2902360"
  },
  {
    "text": "of X, going back p\nlags, with coefficients phi_1 through phi_p.",
    "start": "2902360",
    "end": "2910210"
  },
  {
    "text": "And then there are\nresiduals which",
    "start": "2910210",
    "end": "2915500"
  },
  {
    "text": "are expressed in terms of a\nq-th order moving average.",
    "start": "2915500",
    "end": "2920720"
  },
  {
    "text": "So in this framework, the\neta_t's are white noise.",
    "start": "2920720",
    "end": "2925990"
  },
  {
    "text": "And white noise, to reiterate,\nhas mean 0, constant variance, zero covariance between those.",
    "start": "2925990",
    "end": "2933455"
  },
  {
    "text": " In this representation, I've\nsimplified things a little bit",
    "start": "2933456",
    "end": "2943470"
  },
  {
    "text": "by subtracting off the\nmean from all of the X's.",
    "start": "2943470",
    "end": "2949400"
  },
  {
    "text": "And that just makes the formulas\na little bit more simpler.",
    "start": "2949400",
    "end": "2955400"
  },
  {
    "text": "Now, with lag operators, we\ncan write this ARMA model as phi of L, p-th order\npolynomial of lag L given",
    "start": "2955400",
    "end": "2966810"
  },
  {
    "text": "with coefficients 1,\nphi_1 up to phi_p, and theta of L given\nby 1, theta_1, theta_2,",
    "start": "2966810",
    "end": "2977627"
  },
  {
    "text": "up to theta_q. ",
    "start": "2977627",
    "end": "2992869"
  },
  {
    "text": "This is basically\na representation of the ARMA time series model.",
    "start": "2992870",
    "end": "2999170"
  },
  {
    "text": "Basically, we're\ntaking a set of lags of the values of the stochastic\nprocess up to order p.",
    "start": "2999170",
    "end": "3009530"
  },
  {
    "text": "And that's equal to a weighted\naverage of the eta_t's.  If we multiply by the inverse\nof phi of L, if that exists,",
    "start": "3009530",
    "end": "3021600"
  },
  {
    "text": "then we get this\nrepresentation here, which is simply the\nWold decomposition. So the ARMA models basically\nhave a Wold decomposition",
    "start": "3021600",
    "end": "3034150"
  },
  {
    "text": "if this phi of L is invertible. ",
    "start": "3034150",
    "end": "3042849"
  },
  {
    "text": "And we'll explore\nthese by looking at simpler cases\nof the ARMA models",
    "start": "3042850",
    "end": "3049160"
  },
  {
    "text": "by just focusing on\nautoregressive models first and then moving\naverage processes second so that\nyou'll get a better",
    "start": "3049160",
    "end": "3056089"
  },
  {
    "text": "feel for how these things are\nmanipulated and interpreted. So let's move on to the p-th\norder autoregressive process.",
    "start": "3056090",
    "end": "3064540"
  },
  {
    "text": "So we're going to consider\nARMA models that just have autoregressive terms in them.",
    "start": "3064540",
    "end": "3070100"
  },
  {
    "start": "3070100",
    "end": "3076000"
  },
  {
    "text": "So we have phi of L X_t\nminus mu is equal to eta_t, which is white noise.",
    "start": "3076000",
    "end": "3081990"
  },
  {
    "text": "So a linear combination of\nthe series is white noise.",
    "start": "3081990",
    "end": "3088970"
  },
  {
    "text": "And X_t follows then a linear\nregression model on explanatory",
    "start": "3088970",
    "end": "3094730"
  },
  {
    "text": "variables, which are\nlags of the process X.",
    "start": "3094730",
    "end": "3101330"
  },
  {
    "text": "And this could be expressed\nas X_t equal to c plus the sum",
    "start": "3101330",
    "end": "3106760"
  },
  {
    "text": "from 1 to p of phi_j X_(t-j),\nwhich is a linear regression model with regression\nparameters phi_j.",
    "start": "3106760",
    "end": "3113700"
  },
  {
    "text": "And c, the constant term, is\nequal to mu times phi of 1.",
    "start": "3113700",
    "end": "3121390"
  },
  {
    "text": "Now, if you basically take\nexpectations of the process,",
    "start": "3121390",
    "end": "3130920"
  },
  {
    "text": "you basically have\ncoefficients of mu coming in from all the terms. And phi of 1 times mu is the\nregression coefficient there.",
    "start": "3130920",
    "end": "3142220"
  },
  {
    "text": " So with this\nautoregressive model,",
    "start": "3142220",
    "end": "3147319"
  },
  {
    "text": "we now want to go over what are\nthe stationarity conditions. Certainly, this\nautoregressive model",
    "start": "3147320",
    "end": "3155020"
  },
  {
    "text": "is one where, well,\na simple random walk",
    "start": "3155020",
    "end": "3160790"
  },
  {
    "text": "follows an autoregressive\nmodel but is not stationary. We'll highlight that\nin a minute as well.",
    "start": "3160790",
    "end": "3167650"
  },
  {
    "text": "But if you think\nit, that's true. And so stationarity is something\nto be understood and evaluated.",
    "start": "3167650",
    "end": "3175400"
  },
  {
    "start": "3175400",
    "end": "3183160"
  },
  {
    "text": "This polynomial\nfunction phi, where",
    "start": "3183160",
    "end": "3188680"
  },
  {
    "text": "if we replace the\nlag operator L by z, a complex variable, the\nequation phi of z equal to 0",
    "start": "3188680",
    "end": "3200970"
  },
  {
    "text": "is the characteristic\nequation associated with this autoregressive model.",
    "start": "3200970",
    "end": "3207020"
  },
  {
    "text": "And it turns out that we'll\nbe interested in the roots",
    "start": "3207020",
    "end": "3213190"
  },
  {
    "text": "of this characteristic equation. Now, if we consider\nwriting phi of L",
    "start": "3213190",
    "end": "3220705"
  },
  {
    "text": "as a function of the\nroots of the equation, we get this expression\nwhere you'll",
    "start": "3220705",
    "end": "3229130"
  },
  {
    "text": "notice if you multiply\nall those terms out, the 1's all multiply out\ntogether, and you get 1.",
    "start": "3229130",
    "end": "3235730"
  },
  {
    "text": "And with the lag operator\nL to the p-th power, that would be the product\nof 1 over lambda_1",
    "start": "3235730",
    "end": "3243210"
  },
  {
    "text": "times 1 over lambda_2,\nor actually negative 1 over lambda_1 times\nnegative 1 over lambda_2,",
    "start": "3243210",
    "end": "3249680"
  },
  {
    "text": "and so forth-- negative\n1 over lambda_p. Basically, if there are\np roots to this equation,",
    "start": "3249680",
    "end": "3255819"
  },
  {
    "text": "this is how it would\nbe written out. And the process\nX_t is covariance",
    "start": "3255820",
    "end": "3267069"
  },
  {
    "text": "stationary if and\nonly if all the roots of this characteristic equation\nlie outside the unit circle.",
    "start": "3267070",
    "end": "3273630"
  },
  {
    "text": "So what does that mean? That means that the norm\nmodulus of the complex z",
    "start": "3273630",
    "end": "3281240"
  },
  {
    "text": "is greater than 1. So they're outside\nthe unit circle where it's less\nthan or equal to 1.",
    "start": "3281240",
    "end": "3287150"
  },
  {
    "text": "And the roots, if they are\noutside the unit circle,",
    "start": "3287150",
    "end": "3296809"
  },
  {
    "text": "then the modulus of the\nlambda_j's is greater than 1. ",
    "start": "3296810",
    "end": "3305400"
  },
  {
    "text": "And if we then consider\ntaking a complex number",
    "start": "3305400",
    "end": "3312160"
  },
  {
    "text": "lambda, basically\nthe root, and have an expression for 1 minus\n1 over lambda L inverse,",
    "start": "3312160",
    "end": "3320600"
  },
  {
    "text": "we can get this series\nexpression for that inverse. And that series will exist and\nbe bounded if the lambda_i are",
    "start": "3320600",
    "end": "3334860"
  },
  {
    "text": "greater than 1 in magnitude.  So we can actually compute\nan inverse of phi of L",
    "start": "3334860",
    "end": "3346210"
  },
  {
    "text": "by taking the inverse\nof each of the component products in that polynomial.",
    "start": "3346210",
    "end": "3352240"
  },
  {
    "text": "So in introductory\ntime series courses,",
    "start": "3352240",
    "end": "3357800"
  },
  {
    "text": "they talk about\nstationarity and unit roots, but they don't\nreally get into it, because people don't\nknow complex math,",
    "start": "3357800",
    "end": "3364490"
  },
  {
    "text": "don't know about roots. So anyway, but this\nis just very simply",
    "start": "3364490",
    "end": "3369619"
  },
  {
    "text": "how that framework is applied. So we have a\npolynomial equation,",
    "start": "3369620",
    "end": "3377830"
  },
  {
    "text": "the characteristic equation,\nwhose roots we're looking for. Those roots have to\nbe outside the unit circle for stationarity\nof the process.",
    "start": "3377830",
    "end": "3386170"
  },
  {
    "text": "Well, it's basically\nconditions for invertibility",
    "start": "3386170",
    "end": "3391869"
  },
  {
    "text": "of the process, of the\nautoregressive process. And that invertibility renders\nthe process an infinite-order",
    "start": "3391870",
    "end": "3400440"
  },
  {
    "text": "moving average process. ",
    "start": "3400440",
    "end": "3406210"
  },
  {
    "text": "So let's go through\nthese results for the autoregressive\nprocess of order one,",
    "start": "3406210",
    "end": "3412840"
  },
  {
    "text": "where things-- always start\nwith the simplest cases to understand things.",
    "start": "3412840",
    "end": "3418420"
  },
  {
    "text": "The characteristic equation\nfor this model is just 1 minus phi z. The root is 1/phi.",
    "start": "3418420",
    "end": "3423600"
  },
  {
    "text": " So lambda is greater than\n1-- if the modulus of lambda",
    "start": "3423600",
    "end": "3432381"
  },
  {
    "text": "is greater than 1,\nmeaning the root is outside the unit circle,\nthen phi is less than 1. So for covariance stationarity\nof this autoregressive process,",
    "start": "3432382",
    "end": "3441160"
  },
  {
    "text": "we need the magnitude of phi\nto be less than 1 in magnitude. ",
    "start": "3441160",
    "end": "3450089"
  },
  {
    "text": "The expected value of X is mu. The variance of X\nis sigma squared X.",
    "start": "3450090",
    "end": "3456460"
  },
  {
    "text": "This has this form, sigma\nsquared over 1 minus phi. That expression is\nbasically obtained",
    "start": "3456460",
    "end": "3464960"
  },
  {
    "text": "by looking at the infinite order\nmoving average representation.",
    "start": "3464960",
    "end": "3470109"
  },
  {
    "text": "But notice that if\nphi is positive,",
    "start": "3470110",
    "end": "3476760"
  },
  {
    "text": "then the variance\nof X is actually",
    "start": "3476760",
    "end": "3483710"
  },
  {
    "text": "greater than the variance\nof the innovations. ",
    "start": "3483710",
    "end": "3490440"
  },
  {
    "text": "And if phi is less than 0,\nthen it's going to be smaller.",
    "start": "3490440",
    "end": "3497280"
  },
  {
    "text": "So the innovation variance\nbasically is scaled up a bit",
    "start": "3497280",
    "end": "3503100"
  },
  {
    "text": "in the autoregressive process. The covariance matrix is\nphi times sigma squared X. You'll be going through\nthis in the problem set.",
    "start": "3503100",
    "end": "3511980"
  },
  {
    "text": "And the covariance of X is phi\nto the j power sigma squared X.",
    "start": "3511980",
    "end": "3520160"
  },
  {
    "text": "And these expressions can\nall be easily evaluated by simply writing out the\ndefinition of these covariances",
    "start": "3520160",
    "end": "3527490"
  },
  {
    "text": "in terms of the original\nmodel and looking at what terms are independent,\ncancel out, and that proceeds.",
    "start": "3527490",
    "end": "3534250"
  },
  {
    "start": "3534250",
    "end": "3544510"
  },
  {
    "text": "Let's just go\nthrough these cases. Let's show it all here. So we have if phi\nis between 0 and 1,",
    "start": "3544510",
    "end": "3556630"
  },
  {
    "text": "then the process experiences\nexponential mean reversion to mu.",
    "start": "3556630",
    "end": "3562170"
  },
  {
    "text": "So an autoregressive\nprocess with phi between 0 on 1 corresponds to a\nmean-reverting process.",
    "start": "3562170",
    "end": "3569490"
  },
  {
    "text": "This process is\nactually one that has been used theoretically\nfor interest rate models and a lot of theoretical\nwork in finance.",
    "start": "3569490",
    "end": "3576920"
  },
  {
    "text": "The Vasicek model is\nactually an example of the Ornstein-Uhlenbeck\nprocess,",
    "start": "3576920",
    "end": "3582300"
  },
  {
    "text": "which is basically a\nmean-reverting Brownian motion.",
    "start": "3582300",
    "end": "3587840"
  },
  {
    "text": "And any variables\nthat exhibit or could",
    "start": "3587840",
    "end": "3593070"
  },
  {
    "text": "be thought of as\nexhibiting mean reversion,",
    "start": "3593070",
    "end": "3599950"
  },
  {
    "text": "this model can be\napplied to those processes, such as interest rate\nspreads or real exchange rates,",
    "start": "3599950",
    "end": "3607470"
  },
  {
    "text": "variables where one can\nexpect that things never get too large or too small.",
    "start": "3607470",
    "end": "3612790"
  },
  {
    "text": "They come back to some mean. Now, the challenge\nis, that usually may be true over\nshort periods of time.",
    "start": "3612790",
    "end": "3618930"
  },
  {
    "text": "But over very long\nperiods of time, the point to which you're\nreverting to changes. So these models tend to\nnot have broad application",
    "start": "3618930",
    "end": "3626640"
  },
  {
    "text": "over long time ranges. You need to adapt. Anyway, with the AR\nprocess, we can also",
    "start": "3626640",
    "end": "3632220"
  },
  {
    "text": "have negative\nvalues of phi, which results in exponential mean\nreversion that's oscillating",
    "start": "3632220",
    "end": "3638460"
  },
  {
    "text": "in time, because the\nautoregressive coefficient",
    "start": "3638460",
    "end": "3644190"
  },
  {
    "text": "basically is a negative value. And for phi equal to 1, the Wold\ndecomposition doesn't exist.",
    "start": "3644190",
    "end": "3654510"
  },
  {
    "text": "And the process is the\nsimple random walk. So basically, if\nphi is equal to 1,",
    "start": "3654510",
    "end": "3660340"
  },
  {
    "text": "that means that basically just\nchanges in value of the process are independent and identically\ndistributed white noise.",
    "start": "3660340",
    "end": "3668859"
  },
  {
    "text": "And that's the\nrandom walk process. And that process, as was\ncovered in earlier lectures,",
    "start": "3668860",
    "end": "3675839"
  },
  {
    "text": "is non-stationary. If phi is greater than 1, then\nyou have an explosive process,",
    "start": "3675840",
    "end": "3682790"
  },
  {
    "text": "because basically the\nvalues are scaling up every time increment.",
    "start": "3682790",
    "end": "3691000"
  },
  {
    "text": "So those are features\nof the AR(1) model. For a general autoregressive\nprocess of order p,",
    "start": "3691000",
    "end": "3702110"
  },
  {
    "text": "there's a method-- well, we\ncan look at the second order moments of that process, which\nhave a very nice structure,",
    "start": "3702110",
    "end": "3709590"
  },
  {
    "text": "and then use those to\nsolve for estimates of the ARMA parameters, or\nautoregressive parameters.",
    "start": "3709590",
    "end": "3716630"
  },
  {
    "text": "And those happen to be\nspecified by what are called",
    "start": "3716630",
    "end": "3721819"
  },
  {
    "text": "the Yule-Walker equations. So the Yule-Walker equations\nis a standard topic",
    "start": "3721820",
    "end": "3727270"
  },
  {
    "text": "in time series analysis. What is it? What does it correspond to?",
    "start": "3727270",
    "end": "3733030"
  },
  {
    "text": "Well, we take our original\nautoregressive process of order p. And we write out the\nformulas for the covariance",
    "start": "3733030",
    "end": "3744400"
  },
  {
    "text": "at lag j between\ntwo observations. So what's the covariance\nbetween X_t and X_(t-j)?",
    "start": "3744400",
    "end": "3751789"
  },
  {
    "text": "And that expression is\ngiven by this equation.",
    "start": "3751790",
    "end": "3759820"
  },
  {
    "text": "And so this equation for gamma\nof j is determined simply by evaluating the expectations\nwhere we're taking",
    "start": "3759820",
    "end": "3768700"
  },
  {
    "text": "the expectation of X_t in the\nautoregressive process times the fix X_(t-j) minus mu.",
    "start": "3768700",
    "end": "3776110"
  },
  {
    "text": "So just evaluating\nthose terms, you can validate that\nthis is the equation.",
    "start": "3776110",
    "end": "3782880"
  },
  {
    "text": "If we look at the equations\ncorresponding to j equals 1--",
    "start": "3782880",
    "end": "3788619"
  },
  {
    "text": "so lag 1 up through\nlag p-- this is what those equations look like.",
    "start": "3788620",
    "end": "3796070"
  },
  {
    "text": "Basically, the left-hand side\nis gamma_1 through gamma_p. The covariance to\nlag 1 up to lag p",
    "start": "3796070",
    "end": "3803090"
  },
  {
    "text": "is equal to basically\nlinear functions given by the phi of\nthe other covariances.",
    "start": "3803090",
    "end": "3809980"
  },
  {
    "text": " Who can tell me what the\nstructure is of this matrix?",
    "start": "3809980",
    "end": "3817410"
  },
  {
    "text": "It's not a diagonal matrix? What kind of matrix is this? Math trivia question here.",
    "start": "3817410",
    "end": "3822900"
  },
  {
    "start": "3822900",
    "end": "3828849"
  },
  {
    "text": "It has a special name.  Anyone?",
    "start": "3828850",
    "end": "3834599"
  },
  {
    "text": "It's a Toeplitz matrix. The off diagonals are\nall the same value.",
    "start": "3834600",
    "end": "3840839"
  },
  {
    "text": "And in fact, because of the\nsymmetry of the covariance,",
    "start": "3840840",
    "end": "3846680"
  },
  {
    "text": "basically the gamma of 1 is\nequal to gamma of minus 1. Gamma of minus 2 is\nequal to gamma plus 2.",
    "start": "3846680",
    "end": "3852680"
  },
  {
    "text": "Because of the\ncovariant stationarity, it's actually also symmetric. So these equations allow\nus to solve for the phis",
    "start": "3852680",
    "end": "3862630"
  },
  {
    "text": "so long as we have estimates\nof these covariances. So if we have a\nsystem of estimates,",
    "start": "3862630",
    "end": "3870510"
  },
  {
    "text": "we can plug these in in\nan attempt to solve this. If they're consistent\nestimates of the covariances,",
    "start": "3870510",
    "end": "3876770"
  },
  {
    "text": "then there will be a solution. And then the 0th\nequation, which was not",
    "start": "3876770",
    "end": "3881980"
  },
  {
    "text": "part of the series\nof equations-- if you go back and look\nat the 0th equation, that allows you to get an estimate\nfor the sigma squared.",
    "start": "3881980",
    "end": "3887920"
  },
  {
    "text": "So these Yule-Walker\nequations are the way in which many ARMA\nmodels are specified",
    "start": "3887920",
    "end": "3894510"
  },
  {
    "text": "in different statistics packages\nand in terms of what principles",
    "start": "3894510",
    "end": "3903650"
  },
  {
    "text": "are being applied. Well, if we're using unbiased\nestimates of these parameters,",
    "start": "3903650",
    "end": "3909700"
  },
  {
    "text": "then this is applying\nwhat's called the method of moments principle\nfor statistical estimation.",
    "start": "3909700",
    "end": "3916250"
  },
  {
    "text": "And with complicated models,\nwhere sometimes the likelihood functions are very hard\nto specify and compute,",
    "start": "3916250",
    "end": "3925900"
  },
  {
    "text": "and then to do optimization\nover those is even harder. It can turn out that\nthere are relationships",
    "start": "3925900",
    "end": "3932780"
  },
  {
    "text": "between the moments of the\nrandom variables, which are functions of the\nunknown parameters.",
    "start": "3932780",
    "end": "3938340"
  },
  {
    "text": "And you can solve for basically\nthe sample moments equalling the theoretical moments\nand you apply the method",
    "start": "3938340",
    "end": "3945940"
  },
  {
    "text": "of moments estimation method. Econometrics is rich with many\napplications of that principle.",
    "start": "3945940",
    "end": "3954670"
  },
  {
    "text": " The next section goes through\nthe moving average model.",
    "start": "3954670",
    "end": "3962110"
  },
  {
    "text": " Let me highlight this.",
    "start": "3962110",
    "end": "3972340"
  },
  {
    "text": "So with an order\nq moving average, we basically have a polynomial\nin the lag operator L,",
    "start": "3972340",
    "end": "3979560"
  },
  {
    "text": "which is operated\nupon the eta_t's. And if you write out\nthe expectations of X_t,",
    "start": "3979560",
    "end": "3985700"
  },
  {
    "text": "you get mu. The variance of X_t,\nwhich is gamma 0, is sigma squared times 1 plus\nthe squares of the coefficients",
    "start": "3985700",
    "end": "3994470"
  },
  {
    "text": "in the polynomial. And so this feature,\nthis property here is due",
    "start": "3994470",
    "end": "3999920"
  },
  {
    "text": "to the fact that we have\nuncorrelated innovations in the eta_t's.",
    "start": "3999920",
    "end": "4007060"
  },
  {
    "text": "The eta t's are white noise. So the only thing that comes\nthrough in the square of X_t",
    "start": "4007060",
    "end": "4012830"
  },
  {
    "text": "and the expectation of\nthat is the squared powers of the etas, which\nhave coefficients",
    "start": "4012830",
    "end": "4021900"
  },
  {
    "text": "given by the theta_i squared. So these properties are left--\nI'll leave you just to verify,",
    "start": "4021900",
    "end": "4029170"
  },
  {
    "text": "very straightforward. But let's now turn to the\nfinal minutes of the lecture",
    "start": "4029170",
    "end": "4034430"
  },
  {
    "text": "today to accommodating\nnon-stationary behavior",
    "start": "4034430",
    "end": "4040170"
  },
  {
    "text": "in time series. The original approaches\nwith time series",
    "start": "4040170",
    "end": "4047990"
  },
  {
    "text": "was to focus on\nestimation methodologies for covariance\nstationary process.",
    "start": "4047990",
    "end": "4054940"
  },
  {
    "text": "So if the series is not\ncovariance stationary, then we would want to\ndo some transformation",
    "start": "4054940",
    "end": "4062410"
  },
  {
    "text": "of the data, of the\nseries, into a stationary",
    "start": "4062410",
    "end": "4068660"
  },
  {
    "text": "so that the resulting\nprocess is stationary. And with the\ndifferencing operators,",
    "start": "4068660",
    "end": "4075990"
  },
  {
    "text": "delta, Box and Jenkins\nadvocated moving non-stationary trending\nbehavior, which",
    "start": "4075990",
    "end": "4083420"
  },
  {
    "text": "is exhibited often in\neconomic time series, by using a first difference,\nmaybe a second difference,",
    "start": "4083420",
    "end": "4089960"
  },
  {
    "text": "or a k-th order difference. So these operators are\ndefined in this way.",
    "start": "4089960",
    "end": "4100229"
  },
  {
    "text": "Basically with the\nk-th order operator having this\nexpression here, this is the binomial expansion\nof a k-th power,",
    "start": "4100229",
    "end": "4111189"
  },
  {
    "text": "which can be useful. It comes up all the time\nin probability theory.",
    "start": "4111189",
    "end": "4120609"
  },
  {
    "text": "And if a process has\na linear time trend, then delta X_t is going to\nhave no time trend at all,",
    "start": "4120609",
    "end": "4128389"
  },
  {
    "text": "because you're\nbasically taking out that linear component by\ntaking successive differences.",
    "start": "4128390",
    "end": "4134430"
  },
  {
    "text": "Sometimes, if you\nhave a real series and you look at the difference,\nit appears non-stationary, you look at first differences,\nthat can still not",
    "start": "4134430",
    "end": "4142810"
  },
  {
    "text": "appear to be growing\nover time, in which case sometimes the second\ndifference will result",
    "start": "4142810",
    "end": "4148810"
  },
  {
    "text": "in a process with no trend. So these are sort of\nconvenient tricks,",
    "start": "4148810",
    "end": "4154170"
  },
  {
    "text": "techniques to render\nthe series stationary. And let's see.",
    "start": "4154170",
    "end": "4161220"
  },
  {
    "text": "There's examples here of\nlinear trend reversion models",
    "start": "4161220",
    "end": "4166960"
  },
  {
    "text": "which are rendered\ncovariance stationary",
    "start": "4166960",
    "end": "4172318"
  },
  {
    "text": "under first differencing. In this case, this is an\nexample where you have",
    "start": "4172319",
    "end": "4178689"
  },
  {
    "text": "a deterministic time trend. But then you have reversion\nto the time trend over time.",
    "start": "4178689",
    "end": "4186040"
  },
  {
    "text": "So we basically have\neta_t, the error about the deterministic trend,\nis a first order autoregressive",
    "start": "4186040",
    "end": "4193830"
  },
  {
    "text": "process. And the moments here\ncan be derived this way.",
    "start": "4193830",
    "end": "4200307"
  },
  {
    "text": "Leave that as an exercise.  One could also consider\nthe pure integrated process",
    "start": "4200307",
    "end": "4209510"
  },
  {
    "text": "and talk about\nstochastic trends.",
    "start": "4209510",
    "end": "4216329"
  },
  {
    "text": "And basically,\nrandom walk processes are often referred\nto in econometrics",
    "start": "4216330",
    "end": "4222740"
  },
  {
    "text": "as stochastic trends. And you may want to try and\nremove those from the data,",
    "start": "4222740",
    "end": "4231610"
  },
  {
    "text": "or accommodate them. And so the stochastic\ntrend process is basically",
    "start": "4231610",
    "end": "4240929"
  },
  {
    "text": "given by the first difference\nX_t is just equal to eta_t.",
    "start": "4240930",
    "end": "4249630"
  },
  {
    "text": "And so we have essentially\nthis random walk from a given starting point.",
    "start": "4249630",
    "end": "4255830"
  },
  {
    "text": "And it's easy to verify it if\nyou knew the 0th point, then the variance of the t-th time\npoint would be t sigma squared,",
    "start": "4255830",
    "end": "4264770"
  },
  {
    "text": "because we're summing t\nindependent innovations. And the covariance between\nt and lag t minus j",
    "start": "4264770",
    "end": "4274475"
  },
  {
    "text": "is simply t minus\nj sigma squared. And the correlation between\nthose has this form.",
    "start": "4274475",
    "end": "4280860"
  },
  {
    "text": "What you can see is that this\ndefinitely depends on time. So it's not a\nstationary process.",
    "start": "4280860",
    "end": "4286660"
  },
  {
    "text": "So this first differencing\nresults in stationarity.",
    "start": "4286660",
    "end": "4293880"
  },
  {
    "text": "And the end difference\nprocess has those features. ",
    "start": "4293880",
    "end": "4306847"
  },
  {
    "text": "Let's see where we are. ",
    "start": "4306847",
    "end": "4312730"
  },
  {
    "text": "Final topic for\ntoday is just how you incorporate non-stationary\nprocess into ARMA processes.",
    "start": "4312730",
    "end": "4324630"
  },
  {
    "text": "Well, if you take\nfirst differences or second differences\nand the resulting process",
    "start": "4324630",
    "end": "4330340"
  },
  {
    "text": "is covariance\nstationary, then we can just incorporate that\ndifferencing into the model",
    "start": "4330340",
    "end": "4335460"
  },
  {
    "text": "specification itself, and define\nARIMA models, Autoregressive",
    "start": "4335460",
    "end": "4340489"
  },
  {
    "text": "Integrated Moving\nAverage Processes. And so to specify\nthese models, we",
    "start": "4340490",
    "end": "4346000"
  },
  {
    "text": "need to determine the order\nof the differencing required to move trends,\ndeterministic or stochastic,",
    "start": "4346000",
    "end": "4352990"
  },
  {
    "text": "and then estimating\nthe unknown parameters, and then applying model\nselection criteria.",
    "start": "4352990",
    "end": "4358940"
  },
  {
    "text": "So let me go very\nquickly through this and come back to it the\nbeginning of next time.",
    "start": "4358940",
    "end": "4368600"
  },
  {
    "text": "But in specifying the\nparameters of these models, we can apply maximum\nlikelihood, again,",
    "start": "4368600",
    "end": "4374410"
  },
  {
    "text": "if we assume normality of\nthese innovations eta_t. And we can express\nthe ARMA model",
    "start": "4374410",
    "end": "4382260"
  },
  {
    "text": "in state space\nform, which results in a form for the\nlikelihood function, which",
    "start": "4382260",
    "end": "4387880"
  },
  {
    "text": "we'll see a few lectures ahead. But then we can apply limited\ninformation maximum likelihood,",
    "start": "4387880",
    "end": "4395969"
  },
  {
    "text": "where we just condition on the\nfirst observations of the data and maximize the likelihood.",
    "start": "4395970",
    "end": "4402550"
  },
  {
    "text": "Or not condition on the first\nfew observations, but also use their information as well,\nand look at their density",
    "start": "4402550",
    "end": "4413700"
  },
  {
    "text": "functions, incorporating\nthose into the likelihood relative to the stationary\ndistribution for their values.",
    "start": "4413700",
    "end": "4421160"
  },
  {
    "text": "And then the issue\nbecomes, how do we choose amongst different models? Now, last time we talked about\nlinear regression models,",
    "start": "4421160",
    "end": "4428480"
  },
  {
    "text": "how you'd specify a\ngiven model, here, we're talking about autoregressive,\nmoving average, and even integrated\nmoving average processes",
    "start": "4428480",
    "end": "4435000"
  },
  {
    "text": "and how do we specify\nthose, well, with the method of maximum likelihood,\nthere are procedures",
    "start": "4435000",
    "end": "4446469"
  },
  {
    "text": "which-- there are measures of\nhow effectively a fitted model",
    "start": "4446470",
    "end": "4452440"
  },
  {
    "text": "is, given by an\ninformation criterion that you would want to minimize\nfor a given fitted model.",
    "start": "4452440",
    "end": "4461250"
  },
  {
    "text": "So we can consider\ndifferent sets of models, different numbers of\nexplanatory variables,",
    "start": "4461250",
    "end": "4466510"
  },
  {
    "text": "different orders of\nautoregressive parameters, moving average parameters,\nand compute, say,",
    "start": "4466510",
    "end": "4473100"
  },
  {
    "text": "the Akaike information criterion\nor the Bayes information criterion or the\nHannan-Quinn criterion",
    "start": "4473100",
    "end": "4479989"
  },
  {
    "text": "as different ways of judging\nhow good different models are. And let me just finish\ntoday by pointing out",
    "start": "4479990",
    "end": "4487960"
  },
  {
    "text": "that what these\ninformation criteria are is basically a function of the\nlog likelihood function, which",
    "start": "4487960",
    "end": "4498560"
  },
  {
    "text": "is something we're\ntrying to maximize with maximum\nlikelihood estimates. ",
    "start": "4498560",
    "end": "4504869"
  },
  {
    "text": "And then adding some penalty\nfor how many parameters we're estimating.",
    "start": "4504870",
    "end": "4510742"
  },
  {
    "text": "And so what I'd like you to\nthink about for next time is what kind of a penalty\nis appropriate for adding",
    "start": "4510742",
    "end": "4518600"
  },
  {
    "text": "an extra parameter. Like, what evidence is\nrequired to incorporate",
    "start": "4518600",
    "end": "4523639"
  },
  {
    "text": "extra parameters, extra\nvariables, in the model. Would it be t statistics\nthat exceeds some threshold",
    "start": "4523640",
    "end": "4531180"
  },
  {
    "text": "or some other criteria. Turns out that these are\nall related to those issues. And it's very interesting\nhow those play out.",
    "start": "4531180",
    "end": "4539500"
  },
  {
    "text": "And I'll say that for those\nof you who have actually",
    "start": "4539500",
    "end": "4545180"
  },
  {
    "text": "seen these before, the\nBayes information criterion corresponds to an\nassumption that there",
    "start": "4545180",
    "end": "4550400"
  },
  {
    "text": "is some finite number of\nvariables in the model. And you know what those are.",
    "start": "4550400",
    "end": "4557010"
  },
  {
    "text": "The Hannan-Quinn criterion\nsays maybe there's an infinite number of\nvariables in the model,",
    "start": "4557010",
    "end": "4563760"
  },
  {
    "text": "but you want to be\nable to identify those.",
    "start": "4563760",
    "end": "4568809"
  },
  {
    "text": "And so anyway, it's a\nvery challenging problem with model selection. And these criteria can\nbe used to specify those.",
    "start": "4568810",
    "end": "4576900"
  },
  {
    "text": "So we'll go through\nthat next time.",
    "start": "4576900",
    "end": "4579050"
  }
]