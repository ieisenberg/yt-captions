[
  {
    "text": "[SQUEAKING] [RUSTLING] [CLICKING]",
    "start": "0",
    "end": "7365"
  },
  {
    "start": "7365",
    "end": "16501"
  },
  {
    "text": "SARA ELLISON: OK. So last time, we finished up\nwith the example of the auction",
    "start": "16502",
    "end": "24650"
  },
  {
    "text": "example, the sort of\nextended auction example, and that was a little bit\nof a sort of side trip",
    "start": "24650",
    "end": "30350"
  },
  {
    "text": "into auction theory. But we'll go back\nnow to probability and pick up where we left off.",
    "start": "30350",
    "end": "36900"
  },
  {
    "text": "And we were talking about\nmoments of distributions and about expectation\nin particular.",
    "start": "36900",
    "end": "44149"
  },
  {
    "text": "So what if instead\nof wanting to know a certain feature of a\ndistribution of x, say,",
    "start": "44150",
    "end": "51790"
  },
  {
    "text": "the expectation of\nx, we instead are interested in that feature,\nsay, the expectation",
    "start": "51790",
    "end": "57550"
  },
  {
    "text": "of some function of x. So for instance, we know what\nthe distribution of x is.",
    "start": "57550",
    "end": "66400"
  },
  {
    "text": "We really care about y, this\nother random variable y, which is equal to g of x.",
    "start": "66400",
    "end": "72640"
  },
  {
    "text": "And maybe we don't care about\nthe entire distribution of y, we just care about some feature\nof it, like the expectation.",
    "start": "72640",
    "end": "79390"
  },
  {
    "text": "How can we find the\nexpectation of y? Well, we know one sort\nof surefire way to do it.",
    "start": "79390",
    "end": "87710"
  },
  {
    "text": "We can figure out\nhow y is distributed. We know how to do that, right? So if we know how\nx is distributed,",
    "start": "87710",
    "end": "94160"
  },
  {
    "text": "and we know y is a\nfunction of x, then we just use our knowledge of\nfunctions of random variables",
    "start": "94160",
    "end": "99850"
  },
  {
    "text": "to figure that out. OK? And then we can\ncompute the expectation of that new\ndistribution no problem.",
    "start": "99850",
    "end": "107330"
  },
  {
    "text": "But there might\nbe an easier way. In a lot of cases, this\ncan be mathematically",
    "start": "107330",
    "end": "116180"
  },
  {
    "text": "messy or difficult,\nand maybe we don't care about the whole distribution. Maybe we only care\nabout the expectation,",
    "start": "116180",
    "end": "123630"
  },
  {
    "text": "so we don't really\nneed the PDF of y. All we care about is\nthe expectation of y.",
    "start": "123630",
    "end": "129860"
  },
  {
    "text": "And so in cases like that,\nit might be just easier",
    "start": "129860",
    "end": "134870"
  },
  {
    "text": "to use this formula. So what this tells us is that\nwe can compute the expectation",
    "start": "134870",
    "end": "142040"
  },
  {
    "text": "of y where y is a function\nof another random variable x in this fashion here.",
    "start": "142040",
    "end": "149099"
  },
  {
    "text": "So this is the sort old way\nthat we knew how to compute it, but it might be cumbersome\nor too much trouble.",
    "start": "149100",
    "end": "156480"
  },
  {
    "text": "But in fact, all we need to\ndo is use this formula, OK? So we just integrate\nover the support of x,",
    "start": "156480",
    "end": "164819"
  },
  {
    "text": "g of x, and then the PDF of x. OK?",
    "start": "164820",
    "end": "171742"
  },
  {
    "text": "So now let me do an\nexample where we're going to do that exact calculation.",
    "start": "171742",
    "end": "179500"
  },
  {
    "text": "OK. So this example is called\nthe Saint Petersburg paradox. It is a classic example/paradox\nin probability theory been",
    "start": "179500",
    "end": "189360"
  },
  {
    "text": "sort of taught to\nstudents of probability theory for centuries, probably.",
    "start": "189360",
    "end": "195000"
  },
  {
    "text": "And this example is first\ndiscussed by 18th century Swiss mathematician\nNicolas Bernoulli",
    "start": "195000",
    "end": "201750"
  },
  {
    "text": "and published in the\nSaint Petersburg Academy proceedings in 1738. That's where it got its name.",
    "start": "201750",
    "end": "207780"
  },
  {
    "text": "OK? ",
    "start": "207780",
    "end": "213870"
  },
  {
    "text": "The name Bernoulli. Does that sound familiar? Yeah?",
    "start": "213870",
    "end": "219330"
  },
  {
    "text": "I can't remember if I defined\na Bernoulli distribution in this class or not, but\na Bernoulli distribution",
    "start": "219330",
    "end": "226800"
  },
  {
    "text": "is just a special case of\nthe binomial distribution. So remember,\nbinomial distribution we can think of as\nn coin flips where",
    "start": "226800",
    "end": "235030"
  },
  {
    "text": "the probability of a success\non each coin flip is p and they're independent, where\nBernoulli distribution is",
    "start": "235030",
    "end": "241120"
  },
  {
    "text": "just one coin flip. So it's just a binomial\nwhere n equals 1. OK?",
    "start": "241120",
    "end": "246430"
  },
  {
    "text": "That's a Bernoulli distribution. OK.",
    "start": "246430",
    "end": "251450"
  },
  {
    "text": "So here's the game. I'm going to propose\na game to you, and I'm going to ask\nyou how much you'd",
    "start": "251450",
    "end": "257420"
  },
  {
    "text": "be willing to pay me\nto play this game. OK? I flip a fair coin\nuntil it comes up heads.",
    "start": "257420",
    "end": "264690"
  },
  {
    "text": "And if the number of\nflips necessary is x, I pay you 2 to the x dollars.",
    "start": "264690",
    "end": "271970"
  },
  {
    "text": "OK? So how much would you be willing\nto pay me to play this game?",
    "start": "271970",
    "end": "278120"
  },
  {
    "text": "Does anyone want to venture? I'm not going to hold\nyou to it, by the way.",
    "start": "278120",
    "end": "283340"
  },
  {
    "text": "You're not committing to\nplay this game for me. But how much do\nyou think you might",
    "start": "283340",
    "end": "288657"
  },
  {
    "text": "be willing to pay\nme to play the game? Yeah? AUDIENCE: I would find\nthe expectation of x. SARA ELLISON: Oh, no, without\ndoing any calculations.",
    "start": "288657",
    "end": "295610"
  },
  {
    "text": "We'll do the\ncalculations in a second. Yeah? AUDIENCE: I would pay\n$1 because there's",
    "start": "295610",
    "end": "301630"
  },
  {
    "text": "a 50/50 chance that I\ncould double [INAUDIBLE].. SARA ELLISON: Yeah. There's also a small chance\nyou could get $1,000,",
    "start": "301630",
    "end": "309290"
  },
  {
    "text": "though, right? Or more than $1,000, you know? But OK, fine. You'd be willing to pay $1.",
    "start": "309290",
    "end": "314990"
  },
  {
    "text": "Yeah. Any other? AUDIENCE: [INAUDIBLE]\nnumbers first? SARA ELLISON: No, you're\ngiving away the punch line.",
    "start": "314990",
    "end": "322720"
  },
  {
    "text": "But you're absolutely right. So we'll come back to\nyour answer in a second.",
    "start": "322720",
    "end": "328370"
  },
  {
    "text": " To be honest, to be\nperfectly honest, if I said,",
    "start": "328370",
    "end": "337510"
  },
  {
    "text": "how much would you be willing\nto pay me to play this game, you'd give me a number much less\nthan infinity to play the game,",
    "start": "337510",
    "end": "345639"
  },
  {
    "text": "right? OK? So maybe I get someone\nwho's willing to pay me $1. Maybe I get someone\nwho's willing to pay me",
    "start": "345640",
    "end": "351970"
  },
  {
    "text": "$5, something like that. OK? So let's actually compute\nwhat the expected winnings",
    "start": "351970",
    "end": "361330"
  },
  {
    "text": "of this game are. OK? Oh, and by the way,\nwhat's this distribution? I can't remember.",
    "start": "361330",
    "end": "367000"
  },
  {
    "text": "So you saw it on a\nproblem set, right? I can't remember\nwhether the problem set",
    "start": "367000",
    "end": "372400"
  },
  {
    "text": "or whether in any other\ntime I've told you what the name of it is. It's called the geometric\ndistribution, OK?",
    "start": "372400",
    "end": "377590"
  },
  {
    "text": "So it's just a coin flip\nuntil you get 1 success. OK? And how many coin\nflips are there",
    "start": "377590",
    "end": "384919"
  },
  {
    "text": "until you get to that success? OK. So it makes sense--",
    "start": "384920",
    "end": "391069"
  },
  {
    "text": "I think, at least I'll\nmake that claim initially.",
    "start": "391070",
    "end": "396740"
  },
  {
    "text": "It makes sense\nthat you should be willing to pay your expected\nwinnings for this game. OK?",
    "start": "396740",
    "end": "401960"
  },
  {
    "text": "So let's calculate\nthe expected winnings and see what you\nguys are willing see if you'd be willing to pay\nme that to play this game.",
    "start": "401960",
    "end": "410780"
  },
  {
    "text": "So let x be the number\nof flips required, and note that x has this\ngeometric distribution",
    "start": "410780",
    "end": "420139"
  },
  {
    "text": "with probability 0.5. I can look up in some\ntable of distributions",
    "start": "420140",
    "end": "427039"
  },
  {
    "text": "what the expectation of x\nis, and it's equal to 2. I can also calculate it.",
    "start": "427040",
    "end": "433100"
  },
  {
    "text": "The calculation that I would\ngo through is a little fussy, so I won't bother doing it. But but you you can calculate\nwhat this expectation is.",
    "start": "433100",
    "end": "441200"
  },
  {
    "text": "OK? Or you could just look\nit up in a table of PDFs.",
    "start": "441200",
    "end": "447490"
  },
  {
    "text": "OK? And we'll need that\nfigure in a second. OK? And then we define y, a\nnew random variable y,",
    "start": "447490",
    "end": "455100"
  },
  {
    "text": "to be equal to the\nwinnings in this game. OK? And so that's just equal\nto 2 raised to the x power.",
    "start": "455100",
    "end": "461950"
  },
  {
    "text": "OK? So we've got two\nrandom variables x. This one has a\ngeometric distribution. And y-- we don't know exactly\nwhat that distribution of y is,",
    "start": "461950",
    "end": "471030"
  },
  {
    "text": "but we can calculate\nits expectation with the formula I just put up.",
    "start": "471030",
    "end": "477870"
  },
  {
    "text": "OK? So in particular, this\nis the discrete analog",
    "start": "477870",
    "end": "483840"
  },
  {
    "text": "of the formula we just had\nup a couple of slides ago. If we're interested in\nthe expectation of y",
    "start": "483840",
    "end": "490360"
  },
  {
    "text": "where y is a function\nof random variable x, and we have the PDF of\nthe random variable x,",
    "start": "490360",
    "end": "497580"
  },
  {
    "text": "and we don't have the PDF\nof the random variable y, we can just use this formula. OK?",
    "start": "497580",
    "end": "502980"
  },
  {
    "text": "So that's what we'll do.  And so let's plug in.",
    "start": "502980",
    "end": "507990"
  },
  {
    "text": "This is the PDF of x. It's just one half\nraised to the x power.",
    "start": "507990",
    "end": "513059"
  },
  {
    "text": "Right? On your problem set, that's\nwhat you got when you did a problem similar to this.",
    "start": "513059",
    "end": "519279"
  },
  {
    "text": "And then we multiply\nit by the function r of x, which is 2 to\nthe x, and we sum up",
    "start": "519280",
    "end": "527280"
  },
  {
    "text": "over all possible values of x. So all possible values of x--",
    "start": "527280",
    "end": "535019"
  },
  {
    "text": "I mean, we have to sum-- this is an infinite sum because\nthere is some tiny probability",
    "start": "535020",
    "end": "543810"
  },
  {
    "text": "that we could just be flipping\nthis coin forever before we get a head, right?",
    "start": "543810",
    "end": "549000"
  },
  {
    "text": "And so we're adding this up\nfrom x equals 1 to infinity.",
    "start": "549000",
    "end": "555120"
  },
  {
    "text": "And when we multiply this out,\nwe just get that that's 1.",
    "start": "555120",
    "end": "560279"
  },
  {
    "text": "We're adding up 1. Infinite number of\ntimes, we get infinity. OK?",
    "start": "560280",
    "end": "566020"
  },
  {
    "text": "So this is maybe kind\nof a surprising result. And certainly, if\nyou buy my argument",
    "start": "566020",
    "end": "573317"
  },
  {
    "text": "that you should be willing\nto pay your expected winnings to play this game,\nthis result doesn't sound right at all\nbecause no one's",
    "start": "573317",
    "end": "579850"
  },
  {
    "text": "willing to pay me anything close\nto an infinite amount of money to play this game.",
    "start": "579850",
    "end": "585115"
  },
  {
    "text": " OK. So that's the paradox.",
    "start": "585115",
    "end": "591400"
  },
  {
    "text": "But is it really a paradox? And the answer is economists--\nnot to economists, OK?",
    "start": "591400",
    "end": "600070"
  },
  {
    "text": "So economists know that\npeople have diminishing marginal utility of money. This is equivalent\nto being risk averse.",
    "start": "600070",
    "end": "608380"
  },
  {
    "text": "So this is another way of\ndescribing a utility function with risk aversion.",
    "start": "608380",
    "end": "614270"
  },
  {
    "text": "So economists know\nthat people have a diminishing marginal\nutility of money. So in other words,\ntheir valuation",
    "start": "614270",
    "end": "621100"
  },
  {
    "text": "of additional money decreases\nas the amount of money they have increases. So if I win $50,000\nin a lottery,",
    "start": "621100",
    "end": "629740"
  },
  {
    "text": "that's going to mean\na lot more to me than it would to\nMichael Bloomberg. I mean, it's just a tiny, little\nrounding error in his wealth.",
    "start": "629740",
    "end": "641170"
  },
  {
    "text": "And so this is a well-known,\nwell-documented facet",
    "start": "641170",
    "end": "647529"
  },
  {
    "text": "of people's utility functions\nor valuation of money. They care less about\nan increment of money",
    "start": "647530",
    "end": "655700"
  },
  {
    "text": "the more money they have. So what we really\nshould have been doing is instead of\ncalculating expected winnings",
    "start": "655700",
    "end": "664090"
  },
  {
    "text": "for this game, we should have\nbeen calculating our valuation of the expected winnings.",
    "start": "664090",
    "end": "669550"
  },
  {
    "text": " So basically, what diminishing\nmarginal utility of money",
    "start": "669550",
    "end": "679240"
  },
  {
    "text": "implies is that your\nutility function,",
    "start": "679240",
    "end": "687580"
  },
  {
    "text": "as a function of money-- the amount of\nmoney you have-- is going to be increasing,\nbut at a decreasing rate.",
    "start": "687580",
    "end": "695170"
  },
  {
    "text": "So that's what diminishing\nmarginal utility is going to imply. So let's just come up with\nsome arbitrary functional form",
    "start": "695170",
    "end": "702670"
  },
  {
    "text": "that looks kind of like this. And that, we'll say,\nis our valuation",
    "start": "702670",
    "end": "707950"
  },
  {
    "text": "of the winnings of this game. And so what I came up\nwith was a log of y.",
    "start": "707950",
    "end": "715070"
  },
  {
    "text": "And you could use many other\ndifferent kind of functions that have this general shape.",
    "start": "715070",
    "end": "721500"
  },
  {
    "text": "So now, we have a third\nrandom variable, z. And z is going to be the\nvaluation of the winnings.",
    "start": "721500",
    "end": "728550"
  },
  {
    "text": "And just we're\nassuming it's log of y. And then if I plug in x here,\nit's just log of 2 to the x.",
    "start": "728550",
    "end": "736790"
  },
  {
    "text": "And so now, let's calculate. Let's use the formula again and\ncalculate the expectation of z",
    "start": "736790",
    "end": "744589"
  },
  {
    "text": "and see what we get. So expectation of z is\njust equal to the sum",
    "start": "744590",
    "end": "751680"
  },
  {
    "text": "from x equals 1 to infinity\nof log of 2 to the x--",
    "start": "751680",
    "end": "757950"
  },
  {
    "text": "that's the function that\nwe've defined as z-- times the PDF of x, which\nis 1/2 to the x still.",
    "start": "757950",
    "end": "768180"
  },
  {
    "text": "And we can rewrite this\nas log of 2 times the sum",
    "start": "768180",
    "end": "773490"
  },
  {
    "text": "of x to the-- times 1/2 to the x. And then if we just use this\nformula for an infinite series,",
    "start": "773490",
    "end": "782760"
  },
  {
    "text": "we get that that is equal\nto 2 times log of 2. And that's a lot\nless than infinity.",
    "start": "782760",
    "end": "789720"
  },
  {
    "text": " So that's the Saint\nPetersburg Paradox.",
    "start": "789720",
    "end": "795910"
  },
  {
    "text": "But just keep in mind,\nit's only a paradox unless a little\nbit of economics.",
    "start": "795910",
    "end": "801070"
  },
  {
    "text": "And then it makes perfect sense. OK. Questions?",
    "start": "801070",
    "end": "807988"
  },
  {
    "text": "No? OK. So now, back to expectation.",
    "start": "807988",
    "end": "813210"
  },
  {
    "text": " So we've seen we've seen the\ndefinition of expectation.",
    "start": "813210",
    "end": "820700"
  },
  {
    "text": "And we have talked about\nhow to compute expectations.",
    "start": "820700",
    "end": "826385"
  },
  {
    "text": "We've talked about\nhow to-- we just saw an example how to\ncompute expectation of a function of\na random variable.",
    "start": "826385",
    "end": "831700"
  },
  {
    "text": "And it's going to\nbe useful for us to list a whole bunch of\nproperties of expectation",
    "start": "831700",
    "end": "839500"
  },
  {
    "text": "that will make expectation-- computing expectations\neasier and, in particular,",
    "start": "839500",
    "end": "845890"
  },
  {
    "text": "is going to make computing\nexpectations of functions of certain random\nvariables a lot easier.",
    "start": "845890",
    "end": "852340"
  },
  {
    "text": "So let's go through the\nlist of expectations, properties of expectation. So the first one is\njust that-- and this",
    "start": "852340",
    "end": "860529"
  },
  {
    "text": "may seem pretty obvious. The expectation of a constant\nas opposed to a random variable",
    "start": "860530",
    "end": "866350"
  },
  {
    "text": "is just equal to that constant.  And I mean, that\nseems not very useful.",
    "start": "866350",
    "end": "873579"
  },
  {
    "text": "And it seems obvious. But in fact, we'll use this\nfact implicitly all the time.",
    "start": "873580",
    "end": "878620"
  },
  {
    "text": "I'll remind you, oh,\nthis thing is a constant. So it can come outside\nthe expectation. ",
    "start": "878620",
    "end": "886940"
  },
  {
    "text": "Let's see. Or its expectation\nis equal to itself. The second property is that, if\nwe have a linear transformation",
    "start": "886940",
    "end": "896029"
  },
  {
    "text": "of the random variable x-- so y is equal to ax plus b-- then the expectation\nof y is just",
    "start": "896030",
    "end": "903200"
  },
  {
    "text": "equal to that same\nlinear function of the expectation of x. ",
    "start": "903200",
    "end": "914111"
  },
  {
    "text": "Number 3-- suppose we have\na bunch of random variables,",
    "start": "914111",
    "end": "919960"
  },
  {
    "text": "x1 through xn, and y\nis equal to the sum of those random variables.",
    "start": "919960",
    "end": "927199"
  },
  {
    "text": "Then the expectation of\ny is equal to the sum of the expectations.",
    "start": "927200",
    "end": "932710"
  },
  {
    "text": "So this is also going\nto be super useful. We'll use it many times.",
    "start": "932710",
    "end": "938980"
  },
  {
    "text": "And I want to point out\nsomething very important about this property, which is that\nI haven't said anything about",
    "start": "938980",
    "end": "945760"
  },
  {
    "text": "the x's. So in particular, I\nhaven't said that the x's needed to be independent.",
    "start": "945760",
    "end": "951340"
  },
  {
    "text": "And in fact, they don't\nneed to be independent. So this is true with\nany x1 through xn.",
    "start": "951340",
    "end": "958300"
  },
  {
    "text": " The fourth property is\nkind of a combination",
    "start": "958300",
    "end": "966340"
  },
  {
    "text": "of number 2 and number 3. And that's just\nsaying, let's say you have a linear combination,\nan arbitrary linear combination",
    "start": "966340",
    "end": "974800"
  },
  {
    "text": "of a bunch of x-- a bunch of x random variables. Then the expectation of\nthat linear combination",
    "start": "974800",
    "end": "981640"
  },
  {
    "text": "is the linear combination\nof the expectations. ",
    "start": "981640",
    "end": "990820"
  },
  {
    "text": "Number 5-- if x and\ny are independent,",
    "start": "990820",
    "end": "996560"
  },
  {
    "text": "then we have that expectation\nof xy, the product of x and y,",
    "start": "996560",
    "end": "1002490"
  },
  {
    "text": "is equal to the product\nof the expectations. ",
    "start": "1002490",
    "end": "1012583"
  },
  {
    "text": "So we've been talking\nabout expectation. Expectation will be the\nmost important moment",
    "start": "1012583",
    "end": "1018060"
  },
  {
    "text": "of a distribution that we are\ngoing to be concerned with. But it's not the only\nmoment of a distribution",
    "start": "1018060",
    "end": "1023220"
  },
  {
    "text": "that we care about. So in addition to\ndescribing the location or center of a\ndistribution, we often",
    "start": "1023220",
    "end": "1030930"
  },
  {
    "text": "would like to describe\nhow spread out it is. And there's a moment for that. And it's called variance.",
    "start": "1030930",
    "end": "1037170"
  },
  {
    "text": "So here is the definition\nof the variance of the random variable x.",
    "start": "1037170",
    "end": "1042510"
  },
  {
    "text": "So it's just equal\nto the expectation of x minus mu quantity squared.",
    "start": "1042510",
    "end": "1049480"
  },
  {
    "text": "And here, mu-- I'm just using mu to stand\nfor the expectation of x. ",
    "start": "1049480",
    "end": "1059247"
  },
  {
    "text": "So we're creating this\nnew random variable, which is just equal to the\nsquared deviation between x",
    "start": "1059247",
    "end": "1070030"
  },
  {
    "text": "and its expectation. And then we're taking\nthe expectation of that new random variable.",
    "start": "1070030",
    "end": "1076010"
  },
  {
    "text": "And that's the variance. I'll give you some\nmore examples that--",
    "start": "1076010",
    "end": "1082570"
  },
  {
    "text": "we'll deal with\nvariance in a variety of ways, where I think you'll\nget a good intuitive sense",
    "start": "1082570",
    "end": "1090430"
  },
  {
    "text": "of what it is.  A note about terminology\nand notation--",
    "start": "1090430",
    "end": "1098700"
  },
  {
    "text": "we often denote the variance of\nx with the Greek symbol sigma",
    "start": "1098700",
    "end": "1104139"
  },
  {
    "text": "squared. ",
    "start": "1104140",
    "end": "1109650"
  },
  {
    "text": "And note also that\nvariance is an expectation. So many of the\nproperties of variance",
    "start": "1109650",
    "end": "1116910"
  },
  {
    "text": "will follow from the fact\nthat it's an expectation. ",
    "start": "1116910",
    "end": "1124200"
  },
  {
    "text": "So just like we went through\nthe properties of expectation, we'll go through a series of\nproperties of variance as well.",
    "start": "1124200",
    "end": "1132345"
  },
  {
    "text": " So the first one is that\nvariance of any random variable",
    "start": "1132345",
    "end": "1138690"
  },
  {
    "text": "is less than or equal to 0. So why is that? Just look at the\ndefinition of variance",
    "start": "1138690",
    "end": "1145680"
  },
  {
    "text": "and see that the thing\nthat's inside the expectation",
    "start": "1145680",
    "end": "1151290"
  },
  {
    "text": "is always going to be non-zero.  So you're taking the\nexpectation of something",
    "start": "1151290",
    "end": "1158760"
  },
  {
    "text": "that's never negative. Then it's not going\nto be negative either. ",
    "start": "1158760",
    "end": "1167549"
  },
  {
    "text": "The variance of a, where a is\na constant, is equal to 0l.",
    "start": "1167550",
    "end": "1173400"
  },
  {
    "text": "Again, you just think back\nto the formula for variance. And we remember that\nexpectation of a constant",
    "start": "1173400",
    "end": "1180150"
  },
  {
    "text": "is equal to that constant. So then you just plug\nin that constant. And you get that the\nvariance of a constant",
    "start": "1180150",
    "end": "1186777"
  },
  {
    "text": "is going to be equal to 0. ",
    "start": "1186777",
    "end": "1194860"
  },
  {
    "text": "So remember that if we\ntook a random variable x and we transformed it linearly,\nthen the expectation of that",
    "start": "1194860",
    "end": "1203890"
  },
  {
    "text": "transformed random variable\nwas the linear transformation of the expectation?",
    "start": "1203890",
    "end": "1210190"
  },
  {
    "text": "Well, something different\nhappens with the variance of that random variable. So in particular, if we\ntake a random variable",
    "start": "1210190",
    "end": "1217390"
  },
  {
    "text": "and we transform it\nlinearly, the variance is just multiplied by the\nsquare of the coefficient on x.",
    "start": "1217390",
    "end": "1227620"
  },
  {
    "text": "And it's not affected at all\nby this additive constant, b.",
    "start": "1227620",
    "end": "1235360"
  },
  {
    "text": "So what's going on here? Well, basically, the\nvariance-- remember,",
    "start": "1235360",
    "end": "1240670"
  },
  {
    "text": "think back to the interpretation\nI gave you for variance or the motivation I\ngave you for variance a second ago, which is\nthat it measured how",
    "start": "1240670",
    "end": "1248350"
  },
  {
    "text": "spread out a distribution is. And so if you have a measure of\nhow spread out a distribution",
    "start": "1248350",
    "end": "1254900"
  },
  {
    "text": "is and you just shift the\ndistribution by a constant, you would hope that that\nmeasure doesn't change.",
    "start": "1254900",
    "end": "1260840"
  },
  {
    "text": "And it doesn't. But if you multiply\nthat distribution,",
    "start": "1260840",
    "end": "1266000"
  },
  {
    "text": "or multiply that random\nvariable by a constant-- a, in this case--",
    "start": "1266000",
    "end": "1271280"
  },
  {
    "text": "then, in fact, the measure\nof how spread out it is does change because when you\nmultiply a random variable",
    "start": "1271280",
    "end": "1278420"
  },
  {
    "text": "by a constant and it spreads\nit out or shrinks it down. And in fact, for\nvariance, that's",
    "start": "1278420",
    "end": "1285020"
  },
  {
    "text": "the factor by which it spreads\nit out or shrinks it down. It's the square of the\nmultiplicative constant.",
    "start": "1285020",
    "end": "1292680"
  },
  {
    "text": "Does that make sense? ",
    "start": "1292680",
    "end": "1298180"
  },
  {
    "text": "Oh, I guess I just said this. In other words, shift the\ndistribution and its variance doesn't change.",
    "start": "1298180",
    "end": "1304190"
  },
  {
    "text": "So shifting corresponds to\nadding a constant to it.",
    "start": "1304190",
    "end": "1309250"
  },
  {
    "text": "You shift the distribution,\nits variance doesn't change. You shrink or spread\nout a distribution and its variance\nchanges by the square",
    "start": "1309250",
    "end": "1316060"
  },
  {
    "text": "of the multiplicative factor. ",
    "start": "1316060",
    "end": "1322139"
  },
  {
    "text": "Property number 4-- so\nlet's suppose we have",
    "start": "1322140",
    "end": "1327640"
  },
  {
    "text": "a bunch of random variables, x. And we add them up to create\na new random variable, y.",
    "start": "1327640",
    "end": "1333400"
  },
  {
    "text": "The variance of y is equal\nto the sum of the variances of the x's.",
    "start": "1333400",
    "end": "1339280"
  },
  {
    "text": "But that's only true if\nthe x's are independent. So remember when we had\na very similar property",
    "start": "1339280",
    "end": "1346990"
  },
  {
    "text": "for expectation? We did not require\nindependence there. We require independence here.",
    "start": "1346990",
    "end": "1353665"
  },
  {
    "text": "AUDIENCE: Why is that? SARA ELLISON: Why is that? I'm not sure if I\ncan give you a good-- I mean, I can show you\nmathematically why that's true.",
    "start": "1353666",
    "end": "1360960"
  },
  {
    "text": "But I mean, I'm not sure I\ncould give you a good intuition. Yeah.",
    "start": "1360960",
    "end": "1366150"
  },
  {
    "text": "Actually, there probably\nis a geometric intuition I could give you. But I might have to think about\nit for a couple of minutes,",
    "start": "1366150",
    "end": "1374135"
  },
  {
    "text": "so it would be coherent.  OK. ",
    "start": "1374135",
    "end": "1381600"
  },
  {
    "text": "And then property\nnumber 5 is, again, a combination of 3 and 4.",
    "start": "1381600",
    "end": "1388020"
  },
  {
    "text": "We have an arbitrary linear\ntransformation of the x's.",
    "start": "1388020",
    "end": "1393990"
  },
  {
    "text": "And the variance of the\nresulting random variable is just going to be equal\nto the sum of the variances",
    "start": "1393990",
    "end": "1402660"
  },
  {
    "text": "where each variance\nis multiplied by the square of the\nmultiplicative factor in the linear combination.",
    "start": "1402660",
    "end": "1412020"
  },
  {
    "text": "And again here, the x's have\nto be independent for this to be true. ",
    "start": "1412020",
    "end": "1419720"
  },
  {
    "text": "And then finally,\nthe sixth property is that the variance of x is\nequal to the expectation of x",
    "start": "1419720",
    "end": "1425380"
  },
  {
    "text": "squared minus the expectation\nof x quantity squared. And this is pretty easy\nto prove if you just",
    "start": "1425380",
    "end": "1430900"
  },
  {
    "text": "start from the\ndefinition of variance. It's not too difficult\nto show this.",
    "start": "1430900",
    "end": "1436510"
  },
  {
    "text": "And this can be a\npretty useful property",
    "start": "1436510",
    "end": "1442330"
  },
  {
    "text": "when we're computing--\nif you need to compute the variance\nof a random variable. Sometimes, computing\nthe expectation--",
    "start": "1442330",
    "end": "1451690"
  },
  {
    "text": "sorry, the expectation squared\nand the expectation of x--",
    "start": "1451690",
    "end": "1458440"
  },
  {
    "text": "the expectation of x\nsquared and the expectation of x quantity squared\ncan be an easier thing",
    "start": "1458440",
    "end": "1463540"
  },
  {
    "text": "to do than computing\nthe variance. So anyhow, this can be a\nuseful formula for that reason.",
    "start": "1463540",
    "end": "1468945"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]\ncalculating expectation of x squared we\njust use the previous formula [INAUDIBLE]? SARA ELLISON: Yep, yep, yep.",
    "start": "1468945",
    "end": "1475390"
  },
  {
    "text": "You can do it in\na variety of ways. One is you can figure out what\nthe distribution of x squared",
    "start": "1475390",
    "end": "1480740"
  },
  {
    "text": "is and compute its expectation. Or we can use the\nformula that we saw at the beginning\nof the lecture.",
    "start": "1480740",
    "end": "1486440"
  },
  {
    "text": "Yep. OK. Questions about\nproperties of variance?",
    "start": "1486440",
    "end": "1492685"
  },
  {
    "text": " OK.",
    "start": "1492685",
    "end": "1499750"
  },
  {
    "text": "And then I also want to\nintroduce standard deviation. So standard deviation\nwe're not going",
    "start": "1499750",
    "end": "1505360"
  },
  {
    "text": "to use as much as variance. But sometimes, it's convenient\nfor our measure of dispersion",
    "start": "1505360",
    "end": "1511540"
  },
  {
    "text": "of a distribution to\nhave the same units as the random variable itself.",
    "start": "1511540",
    "end": "1517299"
  },
  {
    "text": "And so for this reason-- so variance, the\nunits of variance are the units of the\nrandom variable squared.",
    "start": "1517300",
    "end": "1525800"
  },
  {
    "text": "And here, what we do is we just\ndefine the standard deviation",
    "start": "1525800",
    "end": "1531100"
  },
  {
    "text": "to be the square\nroot of the variance. And that's going to\nhave the same units",
    "start": "1531100",
    "end": "1537399"
  },
  {
    "text": "as the random variable itself. So in a lot of ways, standard\ndeviation and variance are--",
    "start": "1537400",
    "end": "1546370"
  },
  {
    "text": "I mean, obviously, one's\na function of the other. But they're essentially\nequivalent ways",
    "start": "1546370",
    "end": "1551980"
  },
  {
    "text": "of measuring the dispersion\nof a distribution. And sometimes, it's easier\nto use standard deviation.",
    "start": "1551980",
    "end": "1558500"
  },
  {
    "text": "And sometimes, it's\neasier to use variance. OK. So just so you know\nthe definition.",
    "start": "1558500",
    "end": "1565250"
  },
  {
    "text": " OK. Now, we can do\nsomething similar.",
    "start": "1565250",
    "end": "1573010"
  },
  {
    "text": "So at the very beginning\nof the lecture, I gave you the formula\nfor how to calculate the expectation of a function\nof a random variable.",
    "start": "1573010",
    "end": "1580890"
  },
  {
    "text": "And I said-- well,\nI said, sometimes that's a lot easier to do\nthan actually figuring out",
    "start": "1580890",
    "end": "1589650"
  },
  {
    "text": "what the PDF of that function\nthe random variable is, and then using that to\ncalculate the expectation.",
    "start": "1589650",
    "end": "1595440"
  },
  {
    "text": "Well, sometimes, we\nmight want a shortcut for calculating\nvariance as well.",
    "start": "1595440",
    "end": "1600960"
  },
  {
    "text": "And we can basically\njust apply the results",
    "start": "1600960",
    "end": "1607110"
  },
  {
    "text": "of expectation of a function of\na random variable and the fact that the variance is,\nin fact, an expectation",
    "start": "1607110",
    "end": "1614730"
  },
  {
    "text": "to get a formula for the\nvariance of a function of a random variable.",
    "start": "1614730",
    "end": "1621269"
  },
  {
    "text": "So variance of y is just\nequal to the expectation",
    "start": "1621270",
    "end": "1626640"
  },
  {
    "text": "of y squared minus\nthe expectation of y quantity squared. And that's one of the\nproperties I just told you.",
    "start": "1626640",
    "end": "1633220"
  },
  {
    "text": "But let's suppose we\ndon't want to calculate the expectation-- we\ndon't want to calculate",
    "start": "1633220",
    "end": "1639850"
  },
  {
    "text": "this distribution, this PDF. And so what we\nwant to do instead",
    "start": "1639850",
    "end": "1645070"
  },
  {
    "text": "is just plug in the formula,\nr of x, and calculate",
    "start": "1645070",
    "end": "1652690"
  },
  {
    "text": "the expectation of that instead. And so using the\nformula for expectation",
    "start": "1652690",
    "end": "1659890"
  },
  {
    "text": "of a function of\na random variable, we get that the variance is\nequal to this minus this.",
    "start": "1659890",
    "end": "1667240"
  },
  {
    "text": "So this is just combining two-- combining the\ndefinition of variance",
    "start": "1667240",
    "end": "1673240"
  },
  {
    "text": "and this property of\nvariance I just showed you with the expectation of a\nfunction of a random variable,",
    "start": "1673240",
    "end": "1680649"
  },
  {
    "text": "combining all those things\ntogether to get this formula. So now, let me define\nanother quantity that,",
    "start": "1680650",
    "end": "1689150"
  },
  {
    "text": "especially as we move\ninto linear regression",
    "start": "1689150",
    "end": "1696790"
  },
  {
    "text": "and other kinds of models, other\nkinds of models that include",
    "start": "1696790",
    "end": "1704110"
  },
  {
    "text": "multiple random variables. This is a concept that's going\nto be very useful for us.",
    "start": "1704110",
    "end": "1710289"
  },
  {
    "text": "And the concept is\nconditional expectation. So what is a\nconditional expectation?",
    "start": "1710290",
    "end": "1715690"
  },
  {
    "text": "A conditional expectation is\njust simply the expectation of a conditional distribution.",
    "start": "1715690",
    "end": "1720980"
  },
  {
    "text": "So remember a couple of\nlectures ago, we figured out we had a joint distribution\nand we talked about computing",
    "start": "1720980",
    "end": "1727120"
  },
  {
    "text": "a conditional distribution? It was just taking a slice and\nthen blowing it up, normalizing",
    "start": "1727120",
    "end": "1733360"
  },
  {
    "text": "it so that it integrated to 1. So that's a conditional\ndistribution.",
    "start": "1733360",
    "end": "1738880"
  },
  {
    "text": "And the notion of a\nconditional expectation",
    "start": "1738880",
    "end": "1745450"
  },
  {
    "text": "is going to be\nuseful going forward. And that is just\nthe expectation of that conditional distribution.",
    "start": "1745450",
    "end": "1751540"
  },
  {
    "text": "So in some sense, it isn't-- it's not quite a new concept.",
    "start": "1751540",
    "end": "1757158"
  },
  {
    "text": "It's just the expectation\nof a distribution we knew already existed. ",
    "start": "1757158",
    "end": "1763420"
  },
  {
    "text": "But if we think of the\nconditional distribution",
    "start": "1763420",
    "end": "1768610"
  },
  {
    "text": "more broadly as a function of\nthe conditioning variable, then we can also think of the\nconditional expectation",
    "start": "1768610",
    "end": "1776770"
  },
  {
    "text": "as a function of this\nconditioning variable. And that's going to be\na useful thing for us",
    "start": "1776770",
    "end": "1782410"
  },
  {
    "text": "when we're building\nthese linear models. So this is the definition.",
    "start": "1782410",
    "end": "1788590"
  },
  {
    "text": "It's just expectation of y given\nx is equal to the integral of y",
    "start": "1788590",
    "end": "1794620"
  },
  {
    "text": "times the conditional\ndistribution of y given x-- so nothing\nparticularly new there.",
    "start": "1794620",
    "end": "1801820"
  },
  {
    "text": "The thing that's\nnew about this is how I want you to think\nabout this quantity.",
    "start": "1801820",
    "end": "1808150"
  },
  {
    "text": "So note that\nexpectation of y given x is a function of\nthe random variable x.",
    "start": "1808150",
    "end": "1816789"
  },
  {
    "text": "If I plug in a specific\nrealization for x, it's no longer a function of x.",
    "start": "1816790",
    "end": "1823720"
  },
  {
    "text": "If we leave it in this\ngeneral form, then x--",
    "start": "1823720",
    "end": "1829419"
  },
  {
    "text": "it's a function of\nthe random variable x. And what do we know about\nfunctions of random variables?",
    "start": "1829420",
    "end": "1836800"
  },
  {
    "text": "They're also random variables. So x is a random variable.",
    "start": "1836800",
    "end": "1842620"
  },
  {
    "text": "That means the expectation\nof y conditional on x is a random variable.",
    "start": "1842620",
    "end": "1847675"
  },
  {
    "text": " And then, as I said,\nif we just plug",
    "start": "1847675",
    "end": "1854290"
  },
  {
    "text": "in a particular realization\nfor x, then it's just a number.",
    "start": "1854290",
    "end": "1860020"
  },
  {
    "text": "Then it's just an\nexpectation, just a number. So this distinction might\nseem a little odd to you.",
    "start": "1860020",
    "end": "1868395"
  },
  {
    "text": "What I'm going to\ndo is I'm going to give you a couple\nof laws involving",
    "start": "1868395",
    "end": "1875020"
  },
  {
    "text": "conditional expectation. And then we'll do an example. And I hope the\nexample will not only",
    "start": "1875020",
    "end": "1881260"
  },
  {
    "text": "make this distinction\na little clearer, but also give you\nsome idea of why",
    "start": "1881260",
    "end": "1886810"
  },
  {
    "text": "this might be a useful concept. ",
    "start": "1886810",
    "end": "1893400"
  },
  {
    "text": "So the first law that\nI'm going to tell you is the law of\niterated expectations.",
    "start": "1893400",
    "end": "1899950"
  },
  {
    "text": " So think now of the\nexpectation of y",
    "start": "1899950",
    "end": "1907860"
  },
  {
    "text": "conditional on x being a\nrandom variable because it's a function of random variables.",
    "start": "1907860",
    "end": "1913030"
  },
  {
    "text": "And so we can talk\nabout its expectation because it itself is\na random variable.",
    "start": "1913030",
    "end": "1918610"
  },
  {
    "text": "So the expectation of\nthe expectation of y given x is equal to\nthe expectation of y.",
    "start": "1918610",
    "end": "1926040"
  },
  {
    "text": "That's the result here. ",
    "start": "1926040",
    "end": "1931950"
  },
  {
    "text": "Does this seem mysterious? Yeah? Yeah, it is a little mysterious. The proof is actually\npretty straightforward.",
    "start": "1931950",
    "end": "1939820"
  },
  {
    "text": "But I don't think it's\nnecessary for me to show it. But the proof is\nnot the hard part.",
    "start": "1939820",
    "end": "1949030"
  },
  {
    "text": "The hard part is wrapping\nyour mind around the fact that we're treating this\nconditional expectation",
    "start": "1949030",
    "end": "1956730"
  },
  {
    "text": "as a random variable. It's a function of\na random variable. So it is a random variable.",
    "start": "1956730",
    "end": "1962980"
  },
  {
    "text": "And so we can also take\nthe expectation of it. So it's a little--",
    "start": "1962980",
    "end": "1969218"
  },
  {
    "text": "I don't know. It's a little subtle, I guess. But like I said, I'll do an\nexample in a couple of minutes.",
    "start": "1969218",
    "end": "1974230"
  },
  {
    "text": "And I hope that will help. ",
    "start": "1974230",
    "end": "1980360"
  },
  {
    "text": "OK. And well, secondly,\nthe definition of conditional variance\nfollows from that",
    "start": "1980360",
    "end": "1985809"
  },
  {
    "text": "of variance and\nconditional expectation. And then the second law,\nthe law of total variance--",
    "start": "1985810",
    "end": "1993160"
  },
  {
    "text": "the second law I'm\ngoing to tell you is that the variance of the\nexpectation of y given x",
    "start": "1993160",
    "end": "1998740"
  },
  {
    "text": "is equal to the expectation\nof the variance of y given x-- or sorry, plus the expectation\nof the variance of y given",
    "start": "1998740",
    "end": "2005640"
  },
  {
    "text": "x is equal to the variance of\ny, the unconditional variance of y.",
    "start": "2005640",
    "end": "2010830"
  },
  {
    "text": "So now, we have two\nlaws that tell us how to compute unconditional\nexpectation and variance of y",
    "start": "2010830",
    "end": "2018510"
  },
  {
    "text": "when we just have\nconditional moments. ",
    "start": "2018510",
    "end": "2027760"
  },
  {
    "text": "Here are the two laws. So just keep those in\nthe back of your mind.",
    "start": "2027760",
    "end": "2033240"
  },
  {
    "text": "You don't have to memorize them. But keep those in\nthe back of your mind as we go through\nthis next example.",
    "start": "2033240",
    "end": "2038865"
  },
  {
    "start": "2038865",
    "end": "2043940"
  },
  {
    "text": "OK. So I have a former student. And he moved to New York City\nafter graduating from MIT.",
    "start": "2043940",
    "end": "2050980"
  },
  {
    "text": "And he started an\ninnovation incubator. So suppose he's been doing\nthis for a few years.",
    "start": "2050980",
    "end": "2057399"
  },
  {
    "text": "In reality, I think\nhe's only been doing it for about a year and a half. But suppose he's been\ndoing it for a few years and has kept track of the\nnumber of patents produced",
    "start": "2057400",
    "end": "2066129"
  },
  {
    "text": "every year in his incubator. And he knows that the\nexpectation of N-- here,",
    "start": "2066130",
    "end": "2073569"
  },
  {
    "text": "N is the number of patents. The expectation of\nN is equal to 2. And the variance of\nN is also equal to 2.",
    "start": "2073570",
    "end": "2081610"
  },
  {
    "text": "So he doesn't know the\ndistribution of patents. He just knows what the\nexpectation and the variance",
    "start": "2081610",
    "end": "2087070"
  },
  {
    "text": "is.  And then let's also\nsuppose that each patent is",
    "start": "2087070",
    "end": "2095349"
  },
  {
    "text": "a commercial success\nwith probability 0.2.",
    "start": "2095350",
    "end": "2101260"
  },
  {
    "text": "We'll assume independence\nacross patents. ",
    "start": "2101260",
    "end": "2108320"
  },
  {
    "text": "Now, suppose there are\nfive patents this year coming out of his incubator.",
    "start": "2108320",
    "end": "2113750"
  },
  {
    "text": "What is the probability that\nthree are commercial successes?",
    "start": "2113750",
    "end": "2119090"
  },
  {
    "text": "So how would we even think\nabout approaching this problem? ",
    "start": "2119090",
    "end": "2126180"
  },
  {
    "text": "So all I told you is\na couple of moments of the distribution with which\nthe patents are generated.",
    "start": "2126180",
    "end": "2136910"
  },
  {
    "text": "And then I also\ntold you this piece, which is going to be crucial. ",
    "start": "2136910",
    "end": "2144070"
  },
  {
    "text": "What's the probability that\nthree out of the five patents produced this year are going\nto be commercial successes?",
    "start": "2144070",
    "end": "2151869"
  },
  {
    "text": "Any guesses on how\nwe might proceed? AUDIENCE: [INAUDIBLE]",
    "start": "2151870",
    "end": "2174140"
  },
  {
    "text": "SARA ELLISON: So your answer\nwas almost entirely correct. But it was more detailed\nthan I was looking for.",
    "start": "2174140",
    "end": "2179990"
  },
  {
    "text": "But that's fine. We'll get to that\ndetail in a second. The insight that you\nhad that let you come up",
    "start": "2179990",
    "end": "2186950"
  },
  {
    "text": "with that calculation\nwas that you figured out what the conditional\ndistribution of successes",
    "start": "2186950",
    "end": "2195470"
  },
  {
    "text": "conditional on N was. You might not have even realized\nthat's what you were doing, but that's what you did.",
    "start": "2195470",
    "end": "2202010"
  },
  {
    "text": "So this was the\nimportant insight,",
    "start": "2202010",
    "end": "2208190"
  },
  {
    "text": "that S conditional on N\nbeing equal to some little n",
    "start": "2208190",
    "end": "2213470"
  },
  {
    "text": "is just equal to-- is binomial\nwith parameters n and 0.2.",
    "start": "2213470",
    "end": "2224040"
  },
  {
    "text": "Where did that come from? Let's go back. So each patent is a commercial\nsuccess with probability 0.2.",
    "start": "2224040",
    "end": "2231390"
  },
  {
    "text": "And we can assume independence. So each patent is\nlike a coin flip",
    "start": "2231390",
    "end": "2236430"
  },
  {
    "text": "where the probability\nof a success is 0.2. We're assuming independence. What is that? That's a binomial distribution.",
    "start": "2236430",
    "end": "2242940"
  },
  {
    "text": "So basically, just\nbased on this, based on my verbal description,\nyou had to make the leap, the--",
    "start": "2242940",
    "end": "2253500"
  },
  {
    "text": "I don't know-- intellectual\nleap that what I was saying is that successes conditional\non number of patents",
    "start": "2253500",
    "end": "2261630"
  },
  {
    "text": "has a binomial distribution\nwith parameters n and 0.2.",
    "start": "2261630",
    "end": "2268799"
  },
  {
    "text": "So in this case, what I'm saying\nis the question is asking, what's the probability that\nthere are three successes given",
    "start": "2268800",
    "end": "2276180"
  },
  {
    "text": "that there are five patents? And that's just the\nprobability that",
    "start": "2276180",
    "end": "2282120"
  },
  {
    "text": "a binomial random variable,\nwith parameters n and 0.2--",
    "start": "2282120",
    "end": "2287180"
  },
  {
    "text": "or 5 and 0.2, I guess-- is equal to 3. So the probability that S is\nequal to 3 given that N is",
    "start": "2287180",
    "end": "2294070"
  },
  {
    "text": "equal to 5 is just equal to-- I'm just plugging into\nthe binomial formula here.",
    "start": "2294070",
    "end": "2299350"
  },
  {
    "start": "2299350",
    "end": "2304420"
  },
  {
    "text": "Make sense? Sort of? OK.",
    "start": "2304420",
    "end": "2309490"
  },
  {
    "text": "So now, we can answer. So now that we've\nhad this insight, we've figured out what the\nconditional distribution",
    "start": "2309490",
    "end": "2316690"
  },
  {
    "text": "of successes conditional\non patents is. We've had that insight. And now, we can do a\nlot more with that.",
    "start": "2316690",
    "end": "2324490"
  },
  {
    "text": "Oh, and by the way,\nthat's equal to 5%. Suppose there are five\npatents this year.",
    "start": "2324490",
    "end": "2330789"
  },
  {
    "text": "What's the expected number\nof commercial successes? ",
    "start": "2330790",
    "end": "2337050"
  },
  {
    "text": "AUDIENCE: The same thing for\nevery number of successes from [INAUDIBLE].",
    "start": "2337050",
    "end": "2343937"
  },
  {
    "text": "SARA ELLISON: Yes, that's right. So there are a couple\nof different ways you could do this. So you could actually just\ngo through and figure out",
    "start": "2343937",
    "end": "2350100"
  },
  {
    "text": "the probability for 0 successes,\n1, success, 2 successes, and then use the expectation\nformula and compute it.",
    "start": "2350100",
    "end": "2357750"
  },
  {
    "text": "That's not exactly how I did it. But that would work\nperfectly fine. So what I did instead is I\nknew, off the top of my head,",
    "start": "2357750",
    "end": "2368730"
  },
  {
    "text": "that the expectation of a\nbinomial random variable was equal to np, where the\nparameters the two parameters",
    "start": "2368730",
    "end": "2376470"
  },
  {
    "text": "are n and p. I happen to know that. Proving that is actually, again,\nsort of a fussy calculation.",
    "start": "2376470",
    "end": "2385859"
  },
  {
    "text": "But you can-- actually,\nI'll just go ahead. But you can compute\nthe expectation",
    "start": "2385860",
    "end": "2392790"
  },
  {
    "text": "in general of a Bernoulli\nrandom variable, and then add it up n times. So the expectation of one coin\nflip being a success is 0.2.",
    "start": "2392790",
    "end": "2404110"
  },
  {
    "text": "That's the Bernoulli. And then since there\nare five of them, you just add that\nup five times using",
    "start": "2404110",
    "end": "2409480"
  },
  {
    "text": "one of the properties of\nexpectation that we saw before. So this gives you a different\nway to get the expectation.",
    "start": "2409480",
    "end": "2417789"
  },
  {
    "text": "And the third way you\ncould get the expectation is you just know\nthis is binomial and you look up the expectation\nfor binomial in a book.",
    "start": "2417790",
    "end": "2425170"
  },
  {
    "text": "That's a third\nway you can do it. And so you get np. And then, in this\ncase, n is equal to 5",
    "start": "2425170",
    "end": "2431770"
  },
  {
    "text": "and p is equal to 0.2. You multiply them and you get 1. OK?",
    "start": "2431770",
    "end": "2437620"
  },
  {
    "text": "Makes sense?  Now, what's the\nunconditional expected number",
    "start": "2437620",
    "end": "2445090"
  },
  {
    "text": "of commercial successes? So all the calculations\nwe've done so far are conditional on\nn being equal to 5.",
    "start": "2445090",
    "end": "2452110"
  },
  {
    "text": "But we want to\nknow-- next year, we don't know how many patents\nare going to be produced. And what we want to know is--",
    "start": "2452110",
    "end": "2458530"
  },
  {
    "text": "maybe my student's\nputting together his budget for the incubator. And he's trying to figure\nout how much to charge.",
    "start": "2458530",
    "end": "2466540"
  },
  {
    "text": "And he gets a certain percentage\nof commercial successes and so forth. So he has to be able to compute\nthe unconditional expected",
    "start": "2466540",
    "end": "2473650"
  },
  {
    "text": "number of commercial successes\nbecause he doesn't know what n next year is going to be.",
    "start": "2473650",
    "end": "2480050"
  },
  {
    "text": "OK. Yes? AUDIENCE: He knows the\nexpected number of patents is [INAUDIBLE].",
    "start": "2480050",
    "end": "2486790"
  },
  {
    "text": "SARA ELLISON: Yes, exactly. That's exactly right. He will use that. But how does he use that?",
    "start": "2486790",
    "end": "2493349"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]?  SARA ELLISON: More\nor less, yeah.",
    "start": "2493350",
    "end": "2498930"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. SARA ELLISON: Yes. But how did you guys\ncome up with that?",
    "start": "2498930",
    "end": "2505140"
  },
  {
    "text": "AUDIENCE: The expected number\nof patents [INAUDIBLE]..  SARA ELLISON: I\nthink what you're",
    "start": "2505140",
    "end": "2510960"
  },
  {
    "text": "trying to say is the law\nof iterated expectations. So yes, exactly.",
    "start": "2510960",
    "end": "2517980"
  },
  {
    "text": "OK.  So how do we use the law\nof iterated-- so now,",
    "start": "2517980",
    "end": "2525720"
  },
  {
    "text": "does this now make the law\nof iterated expectations feel a little bit more-- make a little bit more sense?",
    "start": "2525720",
    "end": "2531840"
  },
  {
    "text": "Because that's the calculation\nyou did in your head without even realizing\nyou were using the law of iterated expectations.",
    "start": "2531840",
    "end": "2538240"
  },
  {
    "text": "So let's use the law of\niterated expectations. The unconditional\nexpectation of S is equal to the expectation\nof the expectation of S",
    "start": "2538240",
    "end": "2546990"
  },
  {
    "text": "conditional on N. N is\na random variable here. We don't know what\nthe realization is.",
    "start": "2546990",
    "end": "2553410"
  },
  {
    "text": "We're treating it as\na random variable. So that is just\nequal to-- well, we plug in the formula for the\nexpectation of a binomial.",
    "start": "2553410",
    "end": "2560700"
  },
  {
    "text": "It's just equal to Np. But here, I'm using capital N\nto emphasize the fact that this is still a random variable.",
    "start": "2560700",
    "end": "2566369"
  },
  {
    "text": "Yep? AUDIENCE: Does this theorem\nhold for joint distributions",
    "start": "2566370",
    "end": "2572500"
  },
  {
    "text": "or distributions that has\nvariablest that are so [INAUDIBLE]?",
    "start": "2572500",
    "end": "2578208"
  },
  {
    "text": "SARA ELLISON: Yeah. I mean, this whole-- yes. I mean, this holds\nfor-- all you need is the conditional\ndistribution, yeah.",
    "start": "2578208",
    "end": "2587040"
  },
  {
    "text": "Yeah, it's general. You don't have to have random\nvariables that are independent. Well, yeah, I mean, in\nthis particular example,",
    "start": "2587040",
    "end": "2593970"
  },
  {
    "text": "I'm using independence\nto get the binomial. But that's not relevant to the\nlaw of iterated expectations.",
    "start": "2593970",
    "end": "2601670"
  },
  {
    "text": "So here, I'm still\nusing capital N because we're still thinking of\ncapital N as a random variable.",
    "start": "2601670",
    "end": "2609740"
  },
  {
    "text": "So the expectation of capital\nN times p is just equal",
    "start": "2609740",
    "end": "2616070"
  },
  {
    "text": "to-- well, we saw some\nproperties of expectation that tell us if we have a constant p\nand we're multiplying a random",
    "start": "2616070",
    "end": "2625580"
  },
  {
    "text": "variable by that constant, well,\nthe expectation of that new random variable is just--",
    "start": "2625580",
    "end": "2630740"
  },
  {
    "text": "we can bring the constant\noutside the expectation.",
    "start": "2630740",
    "end": "2636680"
  },
  {
    "text": "What property was that? I don't know. Property 3 of\nexpectations or something. And so we do that.",
    "start": "2636680",
    "end": "2643350"
  },
  {
    "text": "So p, we just plug\nin p is equal to 0.2. And that comes outside\nthe expectation",
    "start": "2643350",
    "end": "2649760"
  },
  {
    "text": "because it's just a constant. We're allowed to do that. And then we have\nexpectation of n. We were told at the\nbeginning of the problem",
    "start": "2649760",
    "end": "2655760"
  },
  {
    "text": "that my student knows\nthe expectation of n. He can plug that in. We're done.",
    "start": "2655760",
    "end": "2662970"
  },
  {
    "text": "OK? Makes sense? ",
    "start": "2662970",
    "end": "2668260"
  },
  {
    "text": "He'd also like to know something\nabout how volatile his income",
    "start": "2668260",
    "end": "2673360"
  },
  {
    "text": "is going to be next year. So he cares about\nthe expectation. But he also cares maybe\nabout the probability",
    "start": "2673360",
    "end": "2679503"
  },
  {
    "text": "that he's going to get no\nincome or probability he's going to get a ton of income. And the variance is going to\ntell us something about that.",
    "start": "2679503",
    "end": "2685569"
  },
  {
    "text": "It tells us how spread out\nthe distribution of his income for next year is going to be.",
    "start": "2685570",
    "end": "2692140"
  },
  {
    "text": "And so how do we calculate\nthe unconditional variance of number of\ncommercial successes?",
    "start": "2692140",
    "end": "2697180"
  },
  {
    "text": " AUDIENCE: The law\nof total variance.",
    "start": "2697180",
    "end": "2703410"
  },
  {
    "text": "SARA ELLISON: The law of\ntotal variance, exactly. OK. There we go.",
    "start": "2703410",
    "end": "2709450"
  },
  {
    "text": "So the variance of S is\njust equal to the variance of the expectation\nof S conditional on N",
    "start": "2709450",
    "end": "2716130"
  },
  {
    "text": "plus the expectation\nof the variance of S conditional on N. Somehow,\nthat sounded funny.",
    "start": "2716130",
    "end": "2722370"
  },
  {
    "text": "But I think I said it correctly. So now, we're going\nto do something similar to what we did before.",
    "start": "2722370",
    "end": "2730170"
  },
  {
    "text": "Actually, in this case,\nI put in a little piece that you probably don't\nknow off the top--",
    "start": "2730170",
    "end": "2736320"
  },
  {
    "text": "I knew it off the top of\nmy head because I've been teaching probability for years. You probably don't know\noff the top of your head",
    "start": "2736320",
    "end": "2742260"
  },
  {
    "text": "that the variance of a\nbinomial random variable is N times p times 1 minus p.",
    "start": "2742260",
    "end": "2748290"
  },
  {
    "text": "So here, we plug\nin the expectation",
    "start": "2748290",
    "end": "2753360"
  },
  {
    "text": "of the binomial random\nvariable N times p, again keeping N as\na capital to emphasize",
    "start": "2753360",
    "end": "2761680"
  },
  {
    "text": "that it's a random variable. And then we plug in the\nvariance of a binomial. So capital N times\np times 1 minus p.",
    "start": "2761680",
    "end": "2769840"
  },
  {
    "text": "And then we take the\nvariance of that first thing, the expectation of\nthe second thing. We use now properties of\nvariance and expectation",
    "start": "2769840",
    "end": "2777730"
  },
  {
    "text": "to pull things outside of the\nvariance and the expectation.",
    "start": "2777730",
    "end": "2783430"
  },
  {
    "text": "So here, we pull out the 0.2. The p is equal to 0.2. We pull it out the front.",
    "start": "2783430",
    "end": "2789550"
  },
  {
    "text": "But we've got to square it. Remember that? So we've got 0.2 squared\ntimes the variance of N plus--",
    "start": "2789550",
    "end": "2797020"
  },
  {
    "text": "and then we just\nbring out p times 1 minus p out of the expectation\nbecause it's just a constant.",
    "start": "2797020",
    "end": "2803600"
  },
  {
    "text": "We can do that. So we get 0.2 times 1 minus\n0.2 times expectation of N.",
    "start": "2803600",
    "end": "2808720"
  },
  {
    "text": "And then you plug in what the\nunconditional variance of N",
    "start": "2808720",
    "end": "2813820"
  },
  {
    "text": "and the unconditional\nexpectation of N is into the formula. And you get 0.4.",
    "start": "2813820",
    "end": "2823329"
  },
  {
    "text": "Does this seem like magic? AUDIENCE: Could\nyou say something about the intuition behind\nthe law of total variance?",
    "start": "2823330",
    "end": "2831355"
  },
  {
    "text": "SARA ELLISON: Probably not.  The intuition behind the\nlaw of total variance?",
    "start": "2831355",
    "end": "2839589"
  },
  {
    "text": "AUDIENCE: Why does it make sense\nthat it has those two parts? ",
    "start": "2839590",
    "end": "2846280"
  },
  {
    "text": "SARA ELLISON: No. I don't think I can. Does anyone have any intuition\nfor the law of total variance?",
    "start": "2846280",
    "end": "2852520"
  },
  {
    "start": "2852520",
    "end": "2858170"
  },
  {
    "text": "Maybe I'll ponder that. I mean, it's a very\nreasonable question.",
    "start": "2858170",
    "end": "2863579"
  },
  {
    "text": "But it's not something I've\never formed an intuition about. So I just think,\noh, it's useful.",
    "start": "2863580",
    "end": "2869569"
  },
  {
    "text": "And I can plug in and use it. Yeah? AUDIENCE: I think\nthat the expectation of the variance itself is kind\nof like a mean kind of thing.",
    "start": "2869570",
    "end": "2878329"
  },
  {
    "text": "And the variance of\nexpectation is how much it's deviating from the mean. And if you add\nthose two together--",
    "start": "2878330",
    "end": "2884911"
  },
  {
    "text": "SARA ELLISON: So\nyou're saying it's some kind of a decomposition? Yeah.",
    "start": "2884912",
    "end": "2890330"
  },
  {
    "text": "I mean, that's got\nto be the case. It's some specific kind of\ndecomposition of the variance coming from different sources.",
    "start": "2890330",
    "end": "2898520"
  },
  {
    "text": "Yeah, yeah, that's\nprobably right. Yeah. And whether I can say\nanything more specific now,",
    "start": "2898520",
    "end": "2907670"
  },
  {
    "text": "I'm not sure.  OK?",
    "start": "2907670",
    "end": "2913170"
  },
  {
    "start": "2913170",
    "end": "2919109"
  },
  {
    "text": "So we've been talking. We've talked a fair amount\nabout moments of single--",
    "start": "2919110",
    "end": "2925230"
  },
  {
    "text": "of univariate distributions. So we talked about expectation.",
    "start": "2925230",
    "end": "2930870"
  },
  {
    "text": "And we talked about variance and\nmentioned standard deviation.",
    "start": "2930870",
    "end": "2935950"
  },
  {
    "text": "But we often are interested\nin the relationship between random variables. That's what we do a\nlot of in econometrics.",
    "start": "2935950",
    "end": "2943980"
  },
  {
    "text": "That's what happens in\nmultivariate statistics, et cetera. And we have an important\nmoment of joint distributions",
    "start": "2943980",
    "end": "2953849"
  },
  {
    "text": "to describe one aspect\nof the relationship between random variables.",
    "start": "2953850",
    "end": "2958950"
  },
  {
    "text": "And that's covariance. And basically,\ncovariance is a way to describe how closely\nassociated two random variables",
    "start": "2958950",
    "end": "2967770"
  },
  {
    "text": "are. When we get to properties,\nthat will give you a little bit more\ninformation about how",
    "start": "2967770",
    "end": "2973380"
  },
  {
    "text": "to interpret covariance. But the way to think about\ncovariance, I suppose,",
    "start": "2973380",
    "end": "2979780"
  },
  {
    "text": "is just that if two random\nvariables are independent--",
    "start": "2979780",
    "end": "2986287"
  },
  {
    "text": "so if two random variables\nare independent-- we'll see in a second-- their covariance is equal to 0.",
    "start": "2986287",
    "end": "2992020"
  },
  {
    "text": "And if two random variables\nare very closely related, then they're going to\nhave a high covariance.",
    "start": "2992020",
    "end": "3001200"
  },
  {
    "text": "So how do we define covariance? Well, we define it as the\nexpectation of x minus mu sub",
    "start": "3001200",
    "end": "3007470"
  },
  {
    "text": "x times y minus mu sub y-- so that function,\nthe expectation",
    "start": "3007470",
    "end": "3012750"
  },
  {
    "text": "of that function of\nrandom variables. And we often denote it\nwith a sigma sub xy.",
    "start": "3012750",
    "end": "3021440"
  },
  {
    "text": " I'm not sure-- I mean-- well, that's how we--",
    "start": "3021440",
    "end": "3027560"
  },
  {
    "text": "I should just say that's\nhow we often denote it. You might think it might\nmake sense to denote it sigma",
    "start": "3027560",
    "end": "3032570"
  },
  {
    "text": "squared sub xy. But this is how we\noften denote it. ",
    "start": "3032570",
    "end": "3038210"
  },
  {
    "text": "And then we also have\na standardized version of this called correlation. So correlation-- we'll often\nuse rho to denote correlation.",
    "start": "3038210",
    "end": "3049050"
  },
  {
    "text": "So either rho of xy-- or sometimes, rho sub xy, we\nuse that notation as well.",
    "start": "3049050",
    "end": "3055130"
  },
  {
    "text": "And that's just equal to\nthe covariance divided by the square root\nof the variance of x",
    "start": "3055130",
    "end": "3063200"
  },
  {
    "text": "times the square root\nof the variance of y. So we'll see some things about\nproperties of correlation",
    "start": "3063200",
    "end": "3069829"
  },
  {
    "text": "and its relationship with\ncovariance in a second. Well, I guess, here's\nthe first bit of that.",
    "start": "3069830",
    "end": "3076650"
  },
  {
    "text": " This is just terminology.",
    "start": "3076650",
    "end": "3082589"
  },
  {
    "text": "We say that the random\nvariables x and y are positively correlated\nif rho is greater than 0.",
    "start": "3082590",
    "end": "3088770"
  },
  {
    "text": "And we say they're\nnegatively correlated if rho is less than 0. And we say that\nthey're uncorrelated",
    "start": "3088770",
    "end": "3095580"
  },
  {
    "text": "if rho is equal to 0.  And for some reason, we don't\nhave similar terminology",
    "start": "3095580",
    "end": "3103140"
  },
  {
    "text": "with covariance. But it actually doesn't matter. This is sort of\nequivalent to what--",
    "start": "3103140",
    "end": "3108930"
  },
  {
    "text": "we don't call something\nuncovariated or something like that. We just call it uncorrelated.",
    "start": "3108930",
    "end": "3114030"
  },
  {
    "text": " OK. So let's go through some\nproperties of covariance",
    "start": "3114030",
    "end": "3119730"
  },
  {
    "text": "and correlation as well. So first of all, you\nmight have noticed",
    "start": "3119730",
    "end": "3124770"
  },
  {
    "text": "from the definition\nof covariance that it was very similar to\nthe definition of variance,",
    "start": "3124770",
    "end": "3130920"
  },
  {
    "text": "but it involved two variables. And in fact, the covariance,\nif you plug in x and itself",
    "start": "3130920",
    "end": "3137730"
  },
  {
    "text": "into the covariance formula,\nyou just get the variance of x. ",
    "start": "3137730",
    "end": "3146028"
  },
  {
    "text": "You might also notice\nfrom the definition that you can switch\nthe places of x and y",
    "start": "3146028",
    "end": "3154400"
  },
  {
    "text": "and you're still going\nto get the same value. ",
    "start": "3154400",
    "end": "3161480"
  },
  {
    "text": "So when we saw\nproperties of variance, there was a property of\nvariance at the very end that I said is sometimes\nuseful for calculating variance",
    "start": "3161480",
    "end": "3169190"
  },
  {
    "text": "and it's pretty easy to prove. Well, this is sort of the\ncounterpart for covariance. So you can show--",
    "start": "3169190",
    "end": "3176620"
  },
  {
    "text": "it's not too\ndifficult. You can show that the covariance\nof x and y is equal to the\nexpectation of x times y",
    "start": "3176620",
    "end": "3185000"
  },
  {
    "text": "minus the expectation of x\ntimes the expectation of y. So remember, the counterpart\nfor that for variance",
    "start": "3185000",
    "end": "3192380"
  },
  {
    "text": "was that the variance of x was\nequal to the expectation of x squared minus the expectation\nof x quantity squared.",
    "start": "3192380",
    "end": "3200165"
  },
  {
    "start": "3200165",
    "end": "3207370"
  },
  {
    "text": "If we have two random variables,\nx, and they're independent, that implies that their\ncovariance is equal to 0.",
    "start": "3207370",
    "end": "3214660"
  },
  {
    "text": "So if you have two random\nvariables whose covariance is equal to 0, you\nthat does not, in fact,",
    "start": "3214660",
    "end": "3221560"
  },
  {
    "text": "imply that they're independent. Although, in most\ncases that we're",
    "start": "3221560",
    "end": "3227020"
  },
  {
    "text": "going to see in this\nclass, random variables with covariance 0\nwill be independent.",
    "start": "3227020",
    "end": "3233260"
  },
  {
    "text": "But the implication definitely\ndoes not go the other way. ",
    "start": "3233260",
    "end": "3244820"
  },
  {
    "text": "So this is a property having to\ndo with linear transformations",
    "start": "3244820",
    "end": "3250310"
  },
  {
    "text": "of random variables. So you take x and transform\nit, linearly transform it,",
    "start": "3250310",
    "end": "3260720"
  },
  {
    "text": "and take y and\nlinearly transform it using different constants. And the covariance of\nx and y gets multiplied",
    "start": "3260720",
    "end": "3268940"
  },
  {
    "text": "by the coefficients on x and y. ",
    "start": "3268940",
    "end": "3276680"
  },
  {
    "text": "The additive constants, b\nand d, don't do anything to change the covariance.",
    "start": "3276680",
    "end": "3282260"
  },
  {
    "start": "3282260",
    "end": "3290440"
  },
  {
    "text": "So remember, when we saw\nproperties of variance, we said that if x--",
    "start": "3290440",
    "end": "3297630"
  },
  {
    "text": "well, we actually saw this\nin a more general form for x1",
    "start": "3297630",
    "end": "3303119"
  },
  {
    "text": "up through xn. But if we only had\ntwo x's, we saw a property that said if\nx1 and x2 are independent,",
    "start": "3303120",
    "end": "3312070"
  },
  {
    "text": "then the variance of the-- sorry, the variance\nof the sum of the x's",
    "start": "3312070",
    "end": "3320339"
  },
  {
    "text": "is equal to the sum\nof the variances, but only if they're independent.",
    "start": "3320340",
    "end": "3325740"
  },
  {
    "text": "Here, this gives\nus a formula for-- at least in the case of\ntwo random variables,",
    "start": "3325740",
    "end": "3332130"
  },
  {
    "text": "for calculating the\nvariance of the sum when they're not independent. ",
    "start": "3332130",
    "end": "3339330"
  },
  {
    "text": "The seventh property\nI want to mention is that rho is always\nless than or equal to 1",
    "start": "3339330",
    "end": "3345839"
  },
  {
    "text": "in absolute value. So rho goes from\nnegative 1 to positive 1.",
    "start": "3345840",
    "end": "3351309"
  },
  {
    "text": "And actually, it can\nbe very handy to have. So covariance can be greater\nthan 1 or less than negative 1.",
    "start": "3351310",
    "end": "3360490"
  },
  {
    "text": "And sometimes, it's handy to\nhave this units-free moment",
    "start": "3360490",
    "end": "3366340"
  },
  {
    "text": "that describes how closely\nassociated two random variables are.",
    "start": "3366340",
    "end": "3371619"
  },
  {
    "text": "And in particular, if the\nabsolute value of rho is equal",
    "start": "3371620",
    "end": "3376780"
  },
  {
    "text": "to 1-- so it's either\nequal to 1 or negative 1-- then that implies that y is\na linear transformation of x.",
    "start": "3376780",
    "end": "3386740"
  },
  {
    "text": "And actually, the implication\ngoes the other way as well. So basically, if you\nhave two random variables",
    "start": "3386740",
    "end": "3394210"
  },
  {
    "text": "and they're\nperfectly correlated, either with correlation\ncoefficient 1 or negative 1,",
    "start": "3394210",
    "end": "3401500"
  },
  {
    "text": "then you know that there\nis a linear relationship between those two\nrandom variables.",
    "start": "3401500",
    "end": "3407500"
  },
  {
    "start": "3407500",
    "end": "3415700"
  },
  {
    "text": "OK. So now, with all of these\ndefinitions, properties, tools,",
    "start": "3415700",
    "end": "3423680"
  },
  {
    "text": "and things like\nthat under our belt, I'm going to give you a\nlittle preview of regression.",
    "start": "3423680",
    "end": "3429150"
  },
  {
    "text": "So still, we're\ngoing to talk about-- in the next couple\nof minutes, we're going to talk about\nlinear regression. It's still going to be in\nthe context of probability.",
    "start": "3429150",
    "end": "3437040"
  },
  {
    "text": "So we're not we're\nnot actually talking about estimating the parameters\nof a linear regression yet.",
    "start": "3437040",
    "end": "3442440"
  },
  {
    "text": "We'll get there. We're going to talk\nabout linear regression in the context of probability.",
    "start": "3442440",
    "end": "3447500"
  },
  {
    "text": "But I hope this is\ngoing to give you a little bit of a sense for\nwhat's coming up in this class,",
    "start": "3447500",
    "end": "3457070"
  },
  {
    "text": "what we're going to be\ncovering in the weeks to come.",
    "start": "3457070",
    "end": "3462200"
  },
  {
    "text": "So let's suppose we have two\nrandom variables, x and y. ",
    "start": "3462200",
    "end": "3469190"
  },
  {
    "text": "We're going to denote the\nexpectation of x as mu sub x and the expectation\nof y is mu sub y.",
    "start": "3469190",
    "end": "3475290"
  },
  {
    "text": "And likewise, we're going to\ndenote the variances of x and y in this sort of standard way.",
    "start": "3475290",
    "end": "3482550"
  },
  {
    "text": "And rho sub xy,\nthe correlation, is",
    "start": "3482550",
    "end": "3487830"
  },
  {
    "text": "just equal to the\nstandard definition.",
    "start": "3487830",
    "end": "3493350"
  },
  {
    "text": " Well, I just told you\nthat if the correlation",
    "start": "3493350",
    "end": "3499740"
  },
  {
    "text": "was equal to 1-- actually, I didn't tell\nyou this, but this is true. I told you something\nclose to this.",
    "start": "3499740",
    "end": "3504960"
  },
  {
    "text": "If the correlation\nis equal to 1, then y is equal to a\nplus bx with b positive.",
    "start": "3504960",
    "end": "3512760"
  },
  {
    "text": "And if rho is equal\nto negative 1, then y is equal to a\nplus bx b negative.",
    "start": "3512760",
    "end": "3520160"
  },
  {
    "text": "So I told you that there\nis a linear relationship if the absolute value\nof this is equal to 1.",
    "start": "3520160",
    "end": "3526160"
  },
  {
    "text": "So this just gives you a\nlittle more information. ",
    "start": "3526160",
    "end": "3531310"
  },
  {
    "text": "If rho is strictly less\nthan 1 in absolute value,",
    "start": "3531310",
    "end": "3536710"
  },
  {
    "text": "then we can't write y and\nx as a linear combination",
    "start": "3536710",
    "end": "3543190"
  },
  {
    "text": "of each other. But what we can do\nis we can write y",
    "start": "3543190",
    "end": "3550269"
  },
  {
    "text": "as a linear function of x\nplus another random variable.",
    "start": "3550270",
    "end": "3555890"
  },
  {
    "text": "And we'll call that\nother random variable u. ",
    "start": "3555890",
    "end": "3562410"
  },
  {
    "text": "So here, we've got\na random variable u that we tacked on here. We'll talk about the\ninterpretation of it",
    "start": "3562410",
    "end": "3569070"
  },
  {
    "text": "in a second. But what can we say about it? Can we say anything\nabout how u behaves,",
    "start": "3569070",
    "end": "3576840"
  },
  {
    "text": "or what its properties\nare, or anything like that? Well, it depends on how\nwe define alpha and beta.",
    "start": "3576840",
    "end": "3584940"
  },
  {
    "text": "So let's suppose that just\nfalling out of the sky we're told that we should\ndefine beta as rho sub xy times",
    "start": "3584940",
    "end": "3594869"
  },
  {
    "text": "sigma sub y over sigma sub x. You don't have to\nworry about where this comes from at this point.",
    "start": "3594870",
    "end": "3600780"
  },
  {
    "text": "And let's say we were told that\nwe should define alpha as mu sub y minus beta mu sub x.",
    "start": "3600780",
    "end": "3608625"
  },
  {
    "text": " Then u has the\nfollowing properties.",
    "start": "3608625",
    "end": "3617980"
  },
  {
    "text": " Sorry, u defined\nas y minus alpha--",
    "start": "3617980",
    "end": "3625349"
  },
  {
    "text": "oh, I think that's\nsupposed to be minus beta. Yeah, so u is equal\nto y minus alpha",
    "start": "3625350",
    "end": "3632170"
  },
  {
    "text": "minus beta x has the\nfollowing properties. The expectation of\nu is equal to 0.",
    "start": "3632170",
    "end": "3638170"
  },
  {
    "text": "And the covariance between\nx and u is also equal to 0.",
    "start": "3638170",
    "end": "3643299"
  },
  {
    "text": " So you don't have to\ntake my word for it.",
    "start": "3643300",
    "end": "3649230"
  },
  {
    "text": "You can show these two\nthings pretty easily using properties of\nexpectation variance",
    "start": "3649230",
    "end": "3655850"
  },
  {
    "text": "and covariance that we've seen. And so maybe that\nwill be on the problem set that I need to put\ntogether this afternoon.",
    "start": "3655850",
    "end": "3663320"
  },
  {
    "text": "So you may see that soon, OK? But basically, if we define\nalpha and beta this way,",
    "start": "3663320",
    "end": "3670460"
  },
  {
    "text": "then u has those properties. So how do we think\nabout x, y, u?",
    "start": "3670460",
    "end": "3677750"
  },
  {
    "text": "How do we think about\ntheir relationship in a case like this? Yeah? AUDIENCE: What is the\ncovariance between x",
    "start": "3677750",
    "end": "3684270"
  },
  {
    "text": "and the function of x? ",
    "start": "3684270",
    "end": "3689930"
  },
  {
    "text": "SARA ELLISON: So the covariance\nbetween x and a function of x? It just depends on the function.",
    "start": "3689930",
    "end": "3695435"
  },
  {
    "text": "There's nothing I can\nsay generally about it. Yeah, OK. So then in the case that\nI have discussed here,",
    "start": "3695435",
    "end": "3708850"
  },
  {
    "text": "the alpha and beta\nhave a particular name. They're called the\nregression coefficients",
    "start": "3708850",
    "end": "3713950"
  },
  {
    "text": "in a bivariate regression. ",
    "start": "3713950",
    "end": "3719619"
  },
  {
    "text": "The way that we think\nabout the relationship among these random\nvariables is that we think of alpha plus\nbeta x as the part of y.",
    "start": "3719620",
    "end": "3728920"
  },
  {
    "text": "We're decomposing the\nvariation that we see in y. And we think of alpha plus\nbeta x as the part of y",
    "start": "3728920",
    "end": "3736870"
  },
  {
    "text": "that's explained by\nx and u as the part that's unexplained by x.",
    "start": "3736870",
    "end": "3742705"
  },
  {
    "text": " How do we get this\ninterpretation? Well, in particular, notice\nthat we've chosen alpha and beta",
    "start": "3742705",
    "end": "3751220"
  },
  {
    "text": "so that covariance of\nx and u is equal to 0. ",
    "start": "3751220",
    "end": "3756460"
  },
  {
    "text": "The x and u are-- they have covariance 0.",
    "start": "3756460",
    "end": "3762710"
  },
  {
    "text": "It means that they're\ncompletely uncorrelated. They don't have--\nyeah, I don't know",
    "start": "3762710",
    "end": "3768230"
  },
  {
    "text": "how else to explain it exactly. And so the part of\nthe variation that we",
    "start": "3768230",
    "end": "3778430"
  },
  {
    "text": "see in y that is explained\nby this linear function of x",
    "start": "3778430",
    "end": "3784670"
  },
  {
    "text": "is the--  I don't know.",
    "start": "3784670",
    "end": "3791060"
  },
  {
    "text": "Well, how do I say this? We're decomposing\nthe variation we see in y into the\npart that's explained",
    "start": "3791060",
    "end": "3796700"
  },
  {
    "text": "by this linear function of x\nand this uncorrelated part. And we typically think of\nu, in a regression context,",
    "start": "3796700",
    "end": "3805700"
  },
  {
    "text": "as being the error term. So anyhow, you don't have\nto understand or completely",
    "start": "3805700",
    "end": "3813020"
  },
  {
    "text": "have a clear intuition\nof what's going on here. But this is one way to think\nabout linear regression.",
    "start": "3813020",
    "end": "3819560"
  },
  {
    "text": "And we'll see other ways\nlater on in the semester. OK?",
    "start": "3819560",
    "end": "3825119"
  },
  {
    "text": "Questions? Nope? OK.",
    "start": "3825120",
    "end": "3830400"
  },
  {
    "text": "So I have two other quick\nthings to go through.",
    "start": "3830400",
    "end": "3835880"
  },
  {
    "text": "And these are two\ninequalities involving",
    "start": "3835880",
    "end": "3841579"
  },
  {
    "text": "probabilities and distributions\nof random variables",
    "start": "3841580",
    "end": "3847550"
  },
  {
    "text": "that do come in handy\nfrom time to time. The first one is called\nthe Markov inequality.",
    "start": "3847550",
    "end": "3855150"
  },
  {
    "text": "So let's suppose you\nhave a random variable x. And it's always non-negative.",
    "start": "3855150",
    "end": "3861350"
  },
  {
    "text": "Then for any t, any\nconstant t that's positive,",
    "start": "3861350",
    "end": "3867290"
  },
  {
    "text": "the probability\nthat x is greater than or equal to that\nconstant t is less than",
    "start": "3867290",
    "end": "3874339"
  },
  {
    "text": "or equal to the\nexpectation of x over t. So basically, how much\nprobability is out",
    "start": "3874340",
    "end": "3882110"
  },
  {
    "text": "in the right tail of\nthis random variable, x, is bounded by the\nexpectation-- some function",
    "start": "3882110",
    "end": "3890039"
  },
  {
    "text": "of the expectation of x, which\nI guess makes sense, right? I mean, the expectation\nis a function",
    "start": "3890040",
    "end": "3898589"
  },
  {
    "text": "of the probability density\nin all parts of the support.",
    "start": "3898590",
    "end": "3904780"
  },
  {
    "text": "And so if you have\na lot of probability out in the right\ntail, then that's",
    "start": "3904780",
    "end": "3910920"
  },
  {
    "text": "going to pull the\nexpectation out. So it's not\nsurprising that there is this relationship between the\nprobability in the right tail",
    "start": "3910920",
    "end": "3917220"
  },
  {
    "text": "and the expectation. So let me draw just\na couple of pictures. So what the Markov\ninequality tells",
    "start": "3917220",
    "end": "3924180"
  },
  {
    "text": "us-- let's suppose we have\na uniform distribution over on the left. And the uniform distribution--",
    "start": "3924180",
    "end": "3932760"
  },
  {
    "text": "let's see. The Markov inequality tells\nus the probability that the uniform distribution-- or\na random variable with that",
    "start": "3932760",
    "end": "3941730"
  },
  {
    "text": "uniform distribution is going to\nbe greater than t is bounded--",
    "start": "3941730",
    "end": "3946950"
  },
  {
    "text": "that slice of the\ndistribution is bounded above by the expectation\nof that distribution divided",
    "start": "3946950",
    "end": "3954750"
  },
  {
    "text": "by t. And the same thing\nis true for any shape",
    "start": "3954750",
    "end": "3960300"
  },
  {
    "text": "of non-negative random variable. So do keep in mind that the\nMarkov inequality is only",
    "start": "3960300",
    "end": "3966780"
  },
  {
    "text": "for random variables that\nare always non-negative. But this is going to be true\nfor any non-negative random",
    "start": "3966780",
    "end": "3974910"
  },
  {
    "text": "variable for which\nexpectation exists. ",
    "start": "3974910",
    "end": "3980720"
  },
  {
    "text": "The second inequality is\nthe Chebyshev inequality. So for Chebyshev, we need\nthat x is a random variable",
    "start": "3980720",
    "end": "3988370"
  },
  {
    "text": "whose variance exists. But it doesn't need to be a\nnon-negative random variable.",
    "start": "3988370",
    "end": "3995180"
  },
  {
    "text": "Then for any constant\nt greater than 0, we have that the probability\nthat x minus the expectation",
    "start": "3995180",
    "end": "4005380"
  },
  {
    "text": "absolute value is greater than\nor equal to t is less than or equal to the variance\nof x over t squared.",
    "start": "4005380",
    "end": "4014560"
  },
  {
    "text": "So basically, what the\nChebyshev inequality is doing is it's putting bounds on both\ntails of the distribution.",
    "start": "4014560",
    "end": "4022330"
  },
  {
    "text": "And I have pictures\nfor that as well. So it's putting bounds on both\ntails of the distribution.",
    "start": "4022330",
    "end": "4030310"
  },
  {
    "text": "And those bounds are a\nfunction of the variance. So basically, if you\nhave a random variable",
    "start": "4030310",
    "end": "4041270"
  },
  {
    "text": "with an expectation, whose\nexpectation and variance exists-- basically, if you\nlook more than t",
    "start": "4041270",
    "end": "4049820"
  },
  {
    "text": "away from the\nexpectation in both tails and you add up that\nprobability that's more than t",
    "start": "4049820",
    "end": "4057530"
  },
  {
    "text": "away from the expectation\nin both tails, that probability is going to be\nbounded above by the variance",
    "start": "4057530",
    "end": "4062720"
  },
  {
    "text": "of x over t squared. AUDIENCE: Can you use\ntwo Markov inequalities",
    "start": "4062720",
    "end": "4068260"
  },
  {
    "text": "to define an\nasymmetric Chebyshev? SARA ELLISON: I\nbelieve that is true.",
    "start": "4068260",
    "end": "4074210"
  },
  {
    "text": "So the Chebyshev\ninequality can be derived from the Markov inequality. So my guess is you could derive\nother flavors of the Chebyshev",
    "start": "4074210",
    "end": "4082600"
  },
  {
    "text": "inequality as well, yeah. Yeah. OK, so that's it.",
    "start": "4082600",
    "end": "4087849"
  },
  {
    "text": "We'll call it a day. And we'll start talking about\nthe sample mean next time.",
    "start": "4087850",
    "end": "4096690"
  },
  {
    "start": "4096690",
    "end": "4107000"
  }
]