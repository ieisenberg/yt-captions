[
  {
    "start": "0",
    "end": "121000"
  },
  {
    "text": "The modern world has an insatiable appetite\nfor computation, so system architects are",
    "start": "740",
    "end": "5790"
  },
  {
    "text": "always thinking about ways to make programs\nrun faster.",
    "start": "5790",
    "end": "9620"
  },
  {
    "text": "The running time of a program is the product\nof three terms:",
    "start": "9620",
    "end": "13389"
  },
  {
    "text": "The number of instructions in the program,\nmultiplied by the average number of processor",
    "start": "13389",
    "end": "18240"
  },
  {
    "text": "cycles required to execute each instruction\n(CPI), multiplied by the time required for",
    "start": "18240",
    "end": "23790"
  },
  {
    "text": "each processor cycle (t_CLK).",
    "start": "23790",
    "end": "27440"
  },
  {
    "text": "To decrease the running time we need to decrease\none or more of these terms.",
    "start": "27440",
    "end": "32070"
  },
  {
    "text": "The number of instructions per program is\ndetermined by the ISA and by the compiler",
    "start": "32070",
    "end": "36250"
  },
  {
    "text": "that produced the sequence of assembly language\ninstructions to be executed.",
    "start": "36250",
    "end": "40030"
  },
  {
    "text": "Both are fair game, but for this discussion,\nlet's work on reducing the other two terms.",
    "start": "40030",
    "end": "46699"
  },
  {
    "text": "As we've seen, pipelining reduces t_CLK by\ndividing instruction execution into a sequence",
    "start": "46700",
    "end": "51460"
  },
  {
    "text": "of steps, each of which can complete its task\nin a shorter t_CLK.",
    "start": "51460",
    "end": "56329"
  },
  {
    "text": "What about reducing CPI?",
    "start": "56330",
    "end": "59420"
  },
  {
    "text": "In our 5-stage pipelined implementation of\nthe Beta, we designed the hardware to complete",
    "start": "59420",
    "end": "64280"
  },
  {
    "text": "the execution of one instruction every clock\ncycle, so CPI_ideal is 1.",
    "start": "64280",
    "end": "70399"
  },
  {
    "text": "But sometimes the hardware has to introduce\n\"NOP bubbles\" into the pipeline to delay execution",
    "start": "70399",
    "end": "75619"
  },
  {
    "text": "of a pipeline stage if the required operation\ncouldn't (yet) be completed.",
    "start": "75619",
    "end": "81659"
  },
  {
    "text": "This happens on taken branch instructions,\nwhen attempting to immediately use a value",
    "start": "81659",
    "end": "86450"
  },
  {
    "text": "loaded from memory by the LD instruction,\nand when waiting for a cache miss to be satisfied",
    "start": "86450",
    "end": "91710"
  },
  {
    "text": "from main memory.",
    "start": "91710",
    "end": "93759"
  },
  {
    "text": "CPI_stall accounts for the cycles lost to\nthe NOPs introduced into the pipeline.",
    "start": "93759",
    "end": "99770"
  },
  {
    "text": "Its value depends on the frequency of taken\nbranches and immediate use of LD results.",
    "start": "99770",
    "end": "105020"
  },
  {
    "text": "Typically it's some fraction of a cycle.",
    "start": "105020",
    "end": "107468"
  },
  {
    "text": "For example, if a 6-instruction loop with\na LD takes 8 cycles to complete, CPI_stall",
    "start": "107469",
    "end": "113909"
  },
  {
    "text": "for the loop would be 2/6, i.e., 2 extra cycles\nfor every 6 instructions.",
    "start": "113909",
    "end": "120659"
  },
  {
    "text": "Our classic 5-stage pipeline is an effective\ncompromise that allows for a substantial reduction",
    "start": "120659",
    "end": "125920"
  },
  {
    "start": "121000",
    "end": "121000"
  },
  {
    "text": "of t_CLK while keeping CPI_stall to a reasonably\nmodest value.",
    "start": "125920",
    "end": "131140"
  },
  {
    "text": "There is room for improvement.",
    "start": "131140",
    "end": "132730"
  },
  {
    "text": "Since each stage is working on one instruction\nat a time, CPI_ideal is 1.",
    "start": "132730",
    "end": "139690"
  },
  {
    "text": "Slow operations - e.g, completing a multiply\nin the ALU stage, or accessing a large cache",
    "start": "139690",
    "end": "145800"
  },
  {
    "text": "in the IF or MEM stages - force t_CLK to be\nlarge to accommodate all the work that has",
    "start": "145800",
    "end": "151450"
  },
  {
    "text": "to be done in one cycle.",
    "start": "151450",
    "end": "154510"
  },
  {
    "text": "The order of the instructions in the pipeline\nis fixed.",
    "start": "154510",
    "end": "157870"
  },
  {
    "text": "If, say, a LD instruction is delayed in the\nMEM stage because of a cache miss, all the",
    "start": "157870",
    "end": "163750"
  },
  {
    "text": "instructions in earlier stages are also delayed\neven though their execution may not depend",
    "start": "163750",
    "end": "168590"
  },
  {
    "text": "on the value produced by the LD.",
    "start": "168590",
    "end": "171160"
  },
  {
    "text": "The order of instructions in the pipeline\nalways reflects the order in which they were",
    "start": "171160",
    "end": "174960"
  },
  {
    "text": "fetched by the IF stage.",
    "start": "174960",
    "end": "177300"
  },
  {
    "text": "Let's look into what it would take to relax\nthese constraints and hopefully improve program",
    "start": "177300",
    "end": "182150"
  },
  {
    "text": "runtimes.",
    "start": "182150",
    "end": "184180"
  },
  {
    "text": "Increasing the number of pipeline stages should\nallow us to decrease the clock cycle time.",
    "start": "184180",
    "end": "188859"
  },
  {
    "text": "We'd add stages to break up performance bottlenecks,\ne.g., adding additional pipeline stages (MEM1",
    "start": "188860",
    "end": "195350"
  },
  {
    "text": "and MEM2) to allow a longer time for memory\noperations to complete.",
    "start": "195350",
    "end": "201240"
  },
  {
    "text": "This comes at cost to CPI_stall since each\nadditional MEM stage means that more NOP bubbles",
    "start": "201240",
    "end": "206710"
  },
  {
    "text": "have to be introduced when there's a LD data\nhazard.",
    "start": "206710",
    "end": "210600"
  },
  {
    "text": "Deeper pipelines mean that the processor will\nbe executing more instructions in parallel.",
    "start": "210600",
    "end": "216200"
  },
  {
    "start": "216000",
    "end": "216000"
  },
  {
    "text": "Let's interrupt enumerating our performance\nshopping list to think about limits to pipeline",
    "start": "216200",
    "end": "220930"
  },
  {
    "text": "depth.",
    "start": "220930",
    "end": "222120"
  },
  {
    "text": "Each additional pipeline stage includes some\nadditional overhead costs to the time budget.",
    "start": "222120",
    "end": "227159"
  },
  {
    "text": "We have to account for the propagation, setup,\nand hold times for the pipeline registers.",
    "start": "227160",
    "end": "232380"
  },
  {
    "text": "And we usually have to allow a bit of extra\ntime to account for clock skew, i.e., the",
    "start": "232380",
    "end": "237440"
  },
  {
    "text": "difference in arrival time of the clock edge\nat each register.",
    "start": "237440",
    "end": "240940"
  },
  {
    "text": "And, finally, since we can't always divide\nthe work exactly evenly between the pipeline",
    "start": "240940",
    "end": "246270"
  },
  {
    "text": "stages, there will be some wasted time in\nthe stages that have less work.",
    "start": "246270",
    "end": "251020"
  },
  {
    "text": "We'll capture all of these effects as an additional\nper-stage time overhead of O.",
    "start": "251020",
    "end": "256789"
  },
  {
    "text": "If the original clock period was T, then with\nN pipeline stages, the clock period will be",
    "start": "256789",
    "end": "262270"
  },
  {
    "text": "T/N + O.",
    "start": "262270",
    "end": "264729"
  },
  {
    "text": "At the limit, as N becomes large, the speedup\napproaches T/O.",
    "start": "264729",
    "end": "271110"
  },
  {
    "text": "In other words, the overhead starts to dominate\nas the time spent on work in each stage becomes",
    "start": "271110",
    "end": "276240"
  },
  {
    "text": "smaller and smaller.",
    "start": "276240",
    "end": "278520"
  },
  {
    "text": "At some point adding additional pipeline stages\nhas almost no impact on the clock period.",
    "start": "278520",
    "end": "283610"
  },
  {
    "text": "As a data point, the Intel Core-2 x86 chips\n(nicknamed \"Nehalem\") have a 14-stage execution",
    "start": "283610",
    "end": "291508"
  },
  {
    "text": "pipeline.",
    "start": "291509",
    "end": "292509"
  },
  {
    "start": "292000",
    "end": "292000"
  },
  {
    "text": "Okay, back to our performance shopping listâ€¦",
    "start": "292509",
    "end": "295990"
  },
  {
    "text": "There may be times we can arrange to execute\ntwo or more instructions in parallel, assuming",
    "start": "295990",
    "end": "301150"
  },
  {
    "text": "that their executions are independent from\neach other.",
    "start": "301150",
    "end": "304240"
  },
  {
    "text": "This would increase CPI_ideal at the cost\nof increasing the complexity of each pipeline",
    "start": "304240",
    "end": "308960"
  },
  {
    "text": "stage to deal with concurrent execution of\nmultiple instructions.",
    "start": "308960",
    "end": "314710"
  },
  {
    "text": "If there's an instruction stalled in the pipeline\nby a data hazard, there may be following instructions",
    "start": "314710",
    "end": "319930"
  },
  {
    "text": "whose execution could still proceed.",
    "start": "319930",
    "end": "323080"
  },
  {
    "text": "Allowing instructions to pass each other in\nthe pipeline is called out-of-order execution.",
    "start": "323080",
    "end": "328668"
  },
  {
    "text": "We'd have to be careful to ensure that changing\nthe execution order didn't affect the values",
    "start": "328669",
    "end": "333159"
  },
  {
    "text": "produced by the program.",
    "start": "333159",
    "end": "336379"
  },
  {
    "text": "More pipeline stages and wider pipeline stages\nincrease the amount of work that has to be",
    "start": "336379",
    "end": "340979"
  },
  {
    "text": "discarded on control hazards, potentially\nincreasing CPI_stall.",
    "start": "340979",
    "end": "346240"
  },
  {
    "text": "So it's important to minimize the number of\ncontrol hazards by predicting the results",
    "start": "346240",
    "end": "351009"
  },
  {
    "text": "of a branch (i.e., taken or not taken)\nso that we increase the chances that the instructions",
    "start": "351010",
    "end": "357319"
  },
  {
    "text": "in the pipeline are the ones we'll want to\nexecute.",
    "start": "357319",
    "end": "360930"
  },
  {
    "text": "Our ability to exploit wider pipelines and\nout-of-order execution depends on finding",
    "start": "360930",
    "end": "365949"
  },
  {
    "text": "instructions that can be executed in parallel\nor in different orders.",
    "start": "365949",
    "end": "370960"
  },
  {
    "text": "Collectively these properties are called \"instruction-level\nparallelism\" (ILP).",
    "start": "370960",
    "end": "375249"
  },
  {
    "start": "375000",
    "end": "375000"
  },
  {
    "text": "Here's an example that will let us explore\nthe amount of ILP that might be available.",
    "start": "375249",
    "end": "381780"
  },
  {
    "text": "On the left is an unoptimized loop that computes\nthe product of the first N integers.",
    "start": "381780",
    "end": "387009"
  },
  {
    "text": "On the right, we've rewritten the code, placing\ninstructions that could be executed concurrently",
    "start": "387009",
    "end": "391909"
  },
  {
    "text": "on the same line.",
    "start": "391909",
    "end": "394150"
  },
  {
    "text": "First notice the red line following the BF\ninstruction.",
    "start": "394150",
    "end": "398360"
  },
  {
    "text": "Instructions below the line should only be\nexecuted if the BF is *not* taken.",
    "start": "398360",
    "end": "403168"
  },
  {
    "text": "That doesn't mean we couldn't start executing\nthem before the results of the branch are",
    "start": "403169",
    "end": "406840"
  },
  {
    "text": "known,\nbut if we executed them before the branch,",
    "start": "406840",
    "end": "409520"
  },
  {
    "text": "we would have to be prepared to throw away\ntheir results if the branch was taken.",
    "start": "409520",
    "end": "414800"
  },
  {
    "text": "The possible execution order is constrained\nby the read-after-write (RAW) dependencies",
    "start": "414800",
    "end": "418810"
  },
  {
    "text": "shown by the red arrows.",
    "start": "418810",
    "end": "420580"
  },
  {
    "text": "We recognize these as the potential data hazards\nthat occur when an operand value for one instruction",
    "start": "420580",
    "end": "426589"
  },
  {
    "text": "depends on the result of an earlier instruction.",
    "start": "426589",
    "end": "430110"
  },
  {
    "text": "In our 5-stage pipeline, we were able to resolve\nmany of these hazards by bypassing values",
    "start": "430110",
    "end": "434990"
  },
  {
    "text": "from the ALU, MEM, and WB stages back to the\nRF stage where operand values are determined.",
    "start": "434990",
    "end": "442020"
  },
  {
    "text": "Of course, bypassing will only work when the\ninstruction has been executed so its result",
    "start": "442020",
    "end": "447789"
  },
  {
    "text": "is available for bypassing!",
    "start": "447789",
    "end": "449570"
  },
  {
    "text": "So, in this case, the arrows are showing us\nthe constraints on execution order that guarantee",
    "start": "449570",
    "end": "455069"
  },
  {
    "text": "bypassing will be possible.",
    "start": "455069",
    "end": "458680"
  },
  {
    "text": "There are other constraints on execution order.",
    "start": "458680",
    "end": "461469"
  },
  {
    "text": "The green arrow identifies a write-after-write\n(WAW) constraint between two instructions",
    "start": "461469",
    "end": "466089"
  },
  {
    "text": "with the same destination register.",
    "start": "466089",
    "end": "469120"
  },
  {
    "text": "In order to ensure the correct value is in\nR2 at the end of the loop, the LD(r,R2) instruction",
    "start": "469120",
    "end": "475241"
  },
  {
    "text": "has to store its result into the register\nfile after the result of the CMPLT instruction",
    "start": "475241",
    "end": "481001"
  },
  {
    "text": "is stored into the register file.",
    "start": "481001",
    "end": "483330"
  },
  {
    "text": "Similarly, the blue arrow shows a write-after-read\n(WAR) constraint that ensures that the correct",
    "start": "483330",
    "end": "489309"
  },
  {
    "text": "values are used when accessing a register.",
    "start": "489309",
    "end": "492168"
  },
  {
    "text": "In this case, LD(r,R2) must store into R2\nafter the Ra operand for the BF has been read",
    "start": "492169",
    "end": "498839"
  },
  {
    "text": "from R2.",
    "start": "498839",
    "end": "500819"
  },
  {
    "text": "As it turns out, WAW and WAR constraints can\nbe eliminated if we give each instruction",
    "start": "500819",
    "end": "506610"
  },
  {
    "text": "result a unique register name.",
    "start": "506610",
    "end": "509638"
  },
  {
    "text": "This can actually be done relatively easily\nby the hardware by using a generous supply",
    "start": "509639",
    "end": "514399"
  },
  {
    "text": "of temporary registers, but we won't go into\nthe details of renaming here.",
    "start": "514399",
    "end": "519310"
  },
  {
    "text": "The use of temporary registers also makes\nit easy to discard results of instructions",
    "start": "519310",
    "end": "523849"
  },
  {
    "text": "executed before we know the outcomes of branches.",
    "start": "523849",
    "end": "527480"
  },
  {
    "text": "In this example, we discovered that the potential\nconcurrency was actually pretty good for the",
    "start": "527480",
    "end": "532690"
  },
  {
    "text": "instructions following the BF.",
    "start": "532690",
    "end": "534930"
  },
  {
    "text": "To take advantage of this potential concurrency,\nwe'll need to modify the pipeline to execute",
    "start": "534930",
    "end": "540050"
  },
  {
    "start": "535000",
    "end": "535000"
  },
  {
    "text": "some number N of instructions in parallel.",
    "start": "540050",
    "end": "543160"
  },
  {
    "text": "If we can sustain that rate of execution,\nCPI_ideal would then be 1/N since we'd complete",
    "start": "543160",
    "end": "549089"
  },
  {
    "text": "the execution of N instructions in each clock\ncycle as they exited the final pipeline stage.",
    "start": "549089",
    "end": "555910"
  },
  {
    "text": "So what value should we choose for N?",
    "start": "555910",
    "end": "559190"
  },
  {
    "text": "Instructions that are executed by different\nALU hardware are easy to execute in parallel,",
    "start": "559190",
    "end": "563660"
  },
  {
    "text": "e.g., ADDs and SHIFTs, or integer and floating-point\noperations.",
    "start": "563660",
    "end": "568449"
  },
  {
    "text": "Of course, if we provided multiple adders,\nwe could execute multiple integer arithmetic",
    "start": "568449",
    "end": "573170"
  },
  {
    "text": "instructions concurrently.",
    "start": "573170",
    "end": "576130"
  },
  {
    "text": "Having separate hardware for address arithmetic\n(called LD/ST units) would support concurrent",
    "start": "576130",
    "end": "581660"
  },
  {
    "text": "execution of LD/ST instructions and integer\narithmetic instructions.",
    "start": "581660",
    "end": "586730"
  },
  {
    "text": "This set of lecture slides from Duke gives\na nice overview of techniques used in each",
    "start": "586730",
    "end": "591089"
  },
  {
    "text": "pipeline stage to support concurrent execution.",
    "start": "591090",
    "end": "595260"
  },
  {
    "text": "Basically by increasing the number of functional\nunits in the ALU and the number of memory",
    "start": "595260",
    "end": "600170"
  },
  {
    "text": "ports on the register file and main memory,\nwe would have what it takes to support concurrent",
    "start": "600170",
    "end": "604990"
  },
  {
    "text": "execution of multiple instructions.",
    "start": "604990",
    "end": "607560"
  },
  {
    "text": "So, what's the right tradeoff between increased\ncircuit costs and increased concurrency?",
    "start": "607560",
    "end": "614639"
  },
  {
    "text": "As a data point, the Intel Nehelam core can\ncomplete up to 4 micro-operations per cycle,",
    "start": "614639",
    "end": "620110"
  },
  {
    "text": "where each micro-operation corresponds to\none of our simple RISC instructions.",
    "start": "620110",
    "end": "625339"
  },
  {
    "start": "625000",
    "end": "625000"
  },
  {
    "text": "Here's a simplified diagram of a modern out-of-order\nsuperscalar processor.",
    "start": "625340",
    "end": "631630"
  },
  {
    "text": "Instruction fetch and decode handles, say,\n4 instructions at a time.",
    "start": "631630",
    "end": "635380"
  },
  {
    "text": "The ability to sustain this execution rate\ndepends heavily on the ability to predict",
    "start": "635380",
    "end": "639870"
  },
  {
    "text": "the outcome of branch instructions,\nensuring that the wide pipeline will be mostly",
    "start": "639870",
    "end": "644490"
  },
  {
    "text": "filled with instructions we actually want\nto execute.",
    "start": "644490",
    "end": "648220"
  },
  {
    "text": "Good branch prediction requires the use of\nthe history from previous branches and there's",
    "start": "648220",
    "end": "652680"
  },
  {
    "text": "been a lot of cleverness devoted to getting\ngood predictions from the least amount of",
    "start": "652680",
    "end": "656910"
  },
  {
    "text": "hardware!",
    "start": "656910",
    "end": "658589"
  },
  {
    "text": "If you're interested in the details, search\nfor \"branch predictor\" on Wikipedia.",
    "start": "658589",
    "end": "663930"
  },
  {
    "text": "The register renaming happens during instruction\ndecode, after which the instructions are ready",
    "start": "663930",
    "end": "668819"
  },
  {
    "text": "to be dispatched to the functional units.",
    "start": "668819",
    "end": "672939"
  },
  {
    "text": "If an instruction needs the result of an earlier\ninstruction as an operand, the dispatcher",
    "start": "672940",
    "end": "677540"
  },
  {
    "text": "has identified which functional unit will\nbe producing the result.",
    "start": "677540",
    "end": "682290"
  },
  {
    "text": "The instruction waits in a queue until the\nindicated functional unit produces the result",
    "start": "682290",
    "end": "686870"
  },
  {
    "text": "and when all the operand values are known,\nthe instruction is finally taken from the",
    "start": "686870",
    "end": "691290"
  },
  {
    "text": "queue and executed.",
    "start": "691290",
    "end": "693720"
  },
  {
    "text": "Since the instructions are executed by different\nfunctional units as soon as their operands",
    "start": "693720",
    "end": "697850"
  },
  {
    "text": "are available, the order of execution may\nnot be the same as in the original program.",
    "start": "697850",
    "end": "704319"
  },
  {
    "text": "After execution, the functional units broadcast\ntheir results so that waiting instructions",
    "start": "704320",
    "end": "709170"
  },
  {
    "text": "know when to proceed.",
    "start": "709170",
    "end": "711339"
  },
  {
    "text": "The results are also collected in a large\nreorder buffer so that that they can be retired",
    "start": "711340",
    "end": "715589"
  },
  {
    "text": "(i.e., write their results in the register\nfile) in the correct order.",
    "start": "715589",
    "end": "720949"
  },
  {
    "text": "Whew!",
    "start": "720949",
    "end": "722199"
  },
  {
    "text": "There's a lot of circuitry involved in keeping\nthe functional units fed with instructions,",
    "start": "722199",
    "end": "726759"
  },
  {
    "text": "knowing when instructions have all their operands,\nand organizing the execution results into",
    "start": "726760",
    "end": "731529"
  },
  {
    "text": "the correct order.",
    "start": "731529",
    "end": "733269"
  },
  {
    "text": "So how much speed up should we expect from\nall this machinery?",
    "start": "733269",
    "end": "738420"
  },
  {
    "text": "The effective CPI is very program-specific,\ndepending as it does on cache hit rates, successful",
    "start": "738420",
    "end": "744259"
  },
  {
    "text": "branch prediction, available ILP, and so on.",
    "start": "744259",
    "end": "749440"
  },
  {
    "text": "Given the architecture described here the\nbest speed up we could hope for is a factor",
    "start": "749440",
    "end": "753620"
  },
  {
    "text": "of 4.",
    "start": "753620",
    "end": "754700"
  },
  {
    "text": "Googling around, it seems that the reality\nis an average speed-up of 2, maybe slightly",
    "start": "754700",
    "end": "760490"
  },
  {
    "text": "less, over what would be achieved by an in-order,\nsingle-issue processor.",
    "start": "760490",
    "end": "766089"
  },
  {
    "start": "766000",
    "end": "766000"
  },
  {
    "text": "What can we expect for future performance\nimprovements in out-of-order, superscalar",
    "start": "766089",
    "end": "769889"
  },
  {
    "text": "pipelines?",
    "start": "769889",
    "end": "772378"
  },
  {
    "text": "Increases in pipeline depth can cause CPI_stall\nand timing overheads to rise.",
    "start": "772379",
    "end": "777670"
  },
  {
    "text": "At the current pipeline depths the increase\nin CPI_stall is larger than the gains from",
    "start": "777670",
    "end": "782480"
  },
  {
    "text": "decreased t_CLK and so further increases in\ndepth are unlikely.",
    "start": "782480",
    "end": "786810"
  },
  {
    "text": "A similar tradeoff exists between using more\nout-of-order execution to increase ILP and",
    "start": "786810",
    "end": "793430"
  },
  {
    "text": "the increase in CPI_stall caused by the impact\nof mis-predicted branches and the inability",
    "start": "793430",
    "end": "799080"
  },
  {
    "text": "to run main memories any faster.",
    "start": "799080",
    "end": "802860"
  },
  {
    "text": "Power consumption increases more quickly than\nthe performance gains from lower t_CLK and",
    "start": "802860",
    "end": "807920"
  },
  {
    "text": "additional out-of-order execution logic.",
    "start": "807920",
    "end": "810649"
  },
  {
    "text": "The additional complexity required to enable\nfurther improvements in branch prediction",
    "start": "810649",
    "end": "814790"
  },
  {
    "text": "and concurrent execution seems very daunting.",
    "start": "814790",
    "end": "817560"
  },
  {
    "text": "All of these factors suggest that is unlikely\nthat we'll see substantial future improvements",
    "start": "817560",
    "end": "823269"
  },
  {
    "text": "in the performance of out-of-order superscalar\npipelined processors.",
    "start": "823269",
    "end": "828990"
  },
  {
    "text": "So system architects have turned their attention\nto exploiting data-level parallelism (DLP)",
    "start": "828990",
    "end": "833509"
  },
  {
    "text": "and thread-level parallelism (TLP).",
    "start": "833509",
    "end": "835709"
  },
  {
    "text": "These are our next two topics.",
    "start": "835709",
    "end": "837109"
  }
]