[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5580"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5580",
    "end": "12270"
  },
  {
    "text": "To make a donation or\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "12270",
    "end": "18830"
  },
  {
    "text": "at ocw.mit.edu. CARLO CILIBERTO: Good morning. So today, there is\na bit if a overview",
    "start": "18830",
    "end": "26330"
  },
  {
    "text": "on the iCub Robot, and it\nwill be about one hour, one",
    "start": "26330",
    "end": "33680"
  },
  {
    "text": "hour and half. And we organized the\nschedule for this time",
    "start": "33680",
    "end": "39120"
  },
  {
    "text": "in series of four small\ntalks and then a demo",
    "start": "39120",
    "end": "44660"
  },
  {
    "text": "from the iCub, live demo. So I will give you an\noverview of the kind of fields",
    "start": "44660",
    "end": "49760"
  },
  {
    "text": "and capabilities that the\niCub has developed so far, while Alessandro, Rafaello,\nand Giulia will show you",
    "start": "49760",
    "end": "56810"
  },
  {
    "text": "what's going on right\nnow on the iCub, part of what's going on the robot. So they are going to talk\nabout their recent work.",
    "start": "56810",
    "end": "65939"
  },
  {
    "text": "So let's start with\nthe presentation. This is the iCub, which\nis a child humanoid robot.",
    "start": "65940",
    "end": "72140"
  },
  {
    "text": "This size. And the project of the iCub\nbegan in 2004, and the iCub--",
    "start": "72140",
    "end": "81660"
  },
  {
    "text": "actually, the iCubs, because\nthere are many of them-- they were built in Genoa\nat the Italian Institute",
    "start": "81660",
    "end": "87260"
  },
  {
    "text": "of Technology. And the main motivation behind\nthe creation and the design",
    "start": "87260",
    "end": "93575"
  },
  {
    "text": "of this platform was\nto actually have a way",
    "start": "93575",
    "end": "99619"
  },
  {
    "text": "to have a platform in order\nto study how intelligence, how cognition emerges in\nartificial embodied systems.",
    "start": "99620",
    "end": "107500"
  },
  {
    "text": "So Giulio Sandini and Giorgio\nMetta, that you can see there, are the actual original\nfounders of iCub world,",
    "start": "107500",
    "end": "118090"
  },
  {
    "text": "so they are both\ndirectors at the IIT. And this is a bit of a\ntimeline that I drew.",
    "start": "118090",
    "end": "125690"
  },
  {
    "text": "Actually, there are many\nother things going on during all these 11 years.",
    "start": "125690",
    "end": "131570"
  },
  {
    "text": "This is a video celebrating\nthe actual first 10 years of the project. And actually, you can\nsee many more things",
    "start": "131570",
    "end": "140146"
  },
  {
    "text": "that the iCub will\nbe able to do. But I selected\nthis part because I think they can be useful, also,\nif you are interested in doing",
    "start": "140146",
    "end": "146810"
  },
  {
    "text": "projects with the robot, to have\nan idea of the kind of skills and the kind of feedback that\nthe robot can provide you,",
    "start": "146810",
    "end": "153340"
  },
  {
    "text": "to do an experiment. So as I told you, iCub\nwas built with the idea",
    "start": "153340",
    "end": "159900"
  },
  {
    "text": "of replicating an artificial\nembodied system that could explore the environment\nand learn from it.",
    "start": "159900",
    "end": "166410"
  },
  {
    "text": "So it has many\ndifferent sensors. These are some of them. So in the head, we have one\naccelerometer and the gyroscope",
    "start": "166410",
    "end": "174650"
  },
  {
    "text": "in order to provide the\ninertial feedback to the system, and two Dragonfly cameras\nthat provide medium resolution",
    "start": "174650",
    "end": "182750"
  },
  {
    "text": "images. And as you can see\nthere, it's all",
    "start": "182750",
    "end": "188180"
  },
  {
    "text": "about one meter, one\nmeter and something, and it's pretty light-- 55 kilograms-- and has a\nlot of degrees of freedom.",
    "start": "188180",
    "end": "196209"
  },
  {
    "text": "So 53 degrees of\nfreedom, and they allow the robot to perform\nmany complicated actions.",
    "start": "196210",
    "end": "203870"
  },
  {
    "text": "It's provided with\ntorque and force sensors, and I will go over\nthese in a minute.",
    "start": "203870",
    "end": "211370"
  },
  {
    "text": "Its whole body, or at least\nthe covered part of the robot-- the black part that\nyou can see there--",
    "start": "211370",
    "end": "217610"
  },
  {
    "text": "are all covered in\nartificial skin. So they provide\nfeedback about contact with the external world.",
    "start": "217610",
    "end": "224420"
  },
  {
    "text": "And it has also microphones\nmounted on the head, but probably for sound\nand speech recognition",
    "start": "224420",
    "end": "229810"
  },
  {
    "text": "is better to use direct\nmicrofeedback at the moment, because, of course,\nnoise-canceling problems",
    "start": "229810",
    "end": "236570"
  },
  {
    "text": "and so on. If you're interested in\nspeech and found feedback,",
    "start": "236570",
    "end": "243470"
  },
  {
    "text": "we are going to use\nother kind of microphone. So during these\n11 years, iCub has",
    "start": "243470",
    "end": "249739"
  },
  {
    "text": "been involved in\nmany, many projects, and indeed, part of what\nI'm going to show you is the result of the joint\neffort of many labs, mainly",
    "start": "249740",
    "end": "258018"
  },
  {
    "text": "in Europe. These are mostly\nEuropean projects. But iCub is also an\ninternational partner",
    "start": "258019",
    "end": "264439"
  },
  {
    "text": "of the CBMM Project. ",
    "start": "264440",
    "end": "269912"
  },
  {
    "text": "So regarding\nforce/torque sensors, they are these sensors\nthat you can see there.",
    "start": "269912",
    "end": "275997"
  },
  {
    "text": "So they provide [INAUDIBLE]\nand also [INAUDIBLE]..",
    "start": "275997",
    "end": "284520"
  },
  {
    "text": " And they're mounted\nin each of the limbs",
    "start": "284520",
    "end": "291070"
  },
  {
    "text": "of the robot and the torso. And they allow the robot\nto [INAUDIBLE] interaction",
    "start": "291070",
    "end": "299065"
  },
  {
    "text": "with the world. And indeed, with\nthis kind of object, it can do many different things. For instance in this video,\nI'm showing an example",
    "start": "299065",
    "end": "306480"
  },
  {
    "text": "of how the feedback provided\nby the force/torque sensor can be used to guide the\nrobot and teach it to learn",
    "start": "306480",
    "end": "313539"
  },
  {
    "text": "different kind of action. Like, in this case, a\npouring action and then repeat it and maybe\ntry to generalize it.",
    "start": "313540",
    "end": "321419"
  },
  {
    "text": "So force/torque sensors\nprovide the robot feedback",
    "start": "321420",
    "end": "326860"
  },
  {
    "text": "about the intensity\nof the interaction with the external world.",
    "start": "326860",
    "end": "333160"
  },
  {
    "text": "But they're not allowed\nto have the robot have an idea of where this kind\nof interaction is occurring.",
    "start": "333160",
    "end": "339490"
  },
  {
    "text": "So, for this, we have artificial\nskin covering the robot, as I told you.",
    "start": "339490",
    "end": "346340"
  },
  {
    "text": "And the technology used\nfor the kind of thing, that you can see here, for the\npalm of the hand of the robot,",
    "start": "346340",
    "end": "352990"
  },
  {
    "text": "is capacitive and it's\nsimilar to the technology used",
    "start": "352990",
    "end": "359319"
  },
  {
    "text": "for smart phones. ",
    "start": "359320",
    "end": "367629"
  },
  {
    "text": "It's using, if you can\nsee these yellow dots. They are all electrodes that,\ntogether with another arm,",
    "start": "367630",
    "end": "376375"
  },
  {
    "text": "they form a capacitor. ",
    "start": "376375",
    "end": "383949"
  },
  {
    "text": "The way the arm works\nof this capacitor are formed when there\nis an interaction",
    "start": "383950",
    "end": "389170"
  },
  {
    "text": "with the environment\nallowed to provide the feedback of the kind\nof intensity [INAUDIBLE]",
    "start": "389170",
    "end": "396780"
  },
  {
    "text": "the location itself. It's providing\ninformation about where this interaction is occurring.",
    "start": "396780",
    "end": "402250"
  },
  {
    "text": "And the artifical\nskin is actually really useful for\nan embodied agent.",
    "start": "402250",
    "end": "408410"
  },
  {
    "text": "And for reasons like\nwe see in this video, without using this feedback, if\nyou have a very light object,",
    "start": "408410",
    "end": "417700"
  },
  {
    "text": "the robot is not able to detect\nthe process is interrupting with something. It's just closing the end and\nit doesn't have any feedback.",
    "start": "417700",
    "end": "425229"
  },
  {
    "text": "It's crushing the object. By using the sensors on\nthe fingertips of the hand,",
    "start": "425230",
    "end": "432884"
  },
  {
    "text": "the robot is able to\ndetect that it's actually touching something,\nand therefore, it's stopping the action without\ncrushing the object.",
    "start": "432884",
    "end": "442120"
  },
  {
    "text": "So other useful things that can\nbe done with artificial skin.",
    "start": "442120",
    "end": "448139"
  },
  {
    "text": "This is an example of\ncombining the information from the force/torque sensor\nand the artificial skin.",
    "start": "448140",
    "end": "454419"
  },
  {
    "text": "So the artificial skin\nallows to detect where and the direction\nof the kind of force",
    "start": "454420",
    "end": "460660"
  },
  {
    "text": "that is applied to the robot.",
    "start": "460660",
    "end": "466750"
  },
  {
    "text": "And in that case, the\nrobot is counterbalancing.",
    "start": "466750",
    "end": "472225"
  },
  {
    "text": "It's negating the effect of\ngravity and of internal forces. So it's basically having\narm floating around like",
    "start": "472225",
    "end": "478759"
  },
  {
    "text": "if it was in space. And there is no friction. So by touching the\narm of the robot,",
    "start": "478760",
    "end": "487360"
  },
  {
    "text": "we have the arm drifting\nin the direction opposite to where the force is\napplied, or the torque.",
    "start": "487360",
    "end": "494870"
  },
  {
    "text": "You can see that,\nthe arm is turning. But as you can see, like it was\nin space without any friction,",
    "start": "494870",
    "end": "501159"
  },
  {
    "text": "because it's actually\nnegating both gravity and internal forces.",
    "start": "501160",
    "end": "506389"
  },
  {
    "text": "And finally, again about\nthe artificial skin, there's some work by\nAlessandro, but he's",
    "start": "506390",
    "end": "511840"
  },
  {
    "text": "going to talk about\nsomething else, but I found is particularly\ninteresting to show him.",
    "start": "511840",
    "end": "518099"
  },
  {
    "text": "This is an example of the\nrobot self-calibrating",
    "start": "518100",
    "end": "525709"
  },
  {
    "text": "the model of its own body,\nwith respect to its own hand.",
    "start": "525710",
    "end": "531100"
  },
  {
    "text": "The idea is to have the robot\nuse the tactile feedback from the fingertip and the skin\nof its forearm, for instance,",
    "start": "531100",
    "end": "539800"
  },
  {
    "text": "to learn the position between\nthe fingertip and the arm.",
    "start": "539800",
    "end": "548500"
  },
  {
    "text": "And therefore, it's able,\nfirst by touching itself, to learn the\ncorrespondence and then",
    "start": "548500",
    "end": "554829"
  },
  {
    "text": "to actually show\nthat it has learned this kind of correlation by\nreaching the point when someone",
    "start": "554830",
    "end": "562750"
  },
  {
    "text": "else touches the thing point. And tries to reach it. And therefore, this can be seen\nas a way of self-calibrating",
    "start": "562750",
    "end": "569019"
  },
  {
    "text": "without the need of a model of\nthe kinematics of the robot. The robot would just be able\nto explore itself and learn",
    "start": "569020",
    "end": "574839"
  },
  {
    "text": "how different part of it's\nbody relate one to the other. And again, related to\nself-calibration sometimes,",
    "start": "574840",
    "end": "585029"
  },
  {
    "text": "but this is more of a\ncalibration between vision and motor activity. This is a work appeared in\n2014 in which, basically,",
    "start": "585030",
    "end": "598510"
  },
  {
    "text": "the correlation between the kind\nof actions that the robot is able to perform is calibrated\nwith respect to its ability",
    "start": "598510",
    "end": "607910"
  },
  {
    "text": "to perceive the work. So, in this kind of\nvideo I'm going to show, the robot is trying\nto reach for an object",
    "start": "607910",
    "end": "616519"
  },
  {
    "text": "and failing in its action\nbecause the actual model",
    "start": "616520",
    "end": "624800"
  },
  {
    "text": "of the word that you use\nto perform the reaching is not aligned with the 3-D\nmodel provided by vision.",
    "start": "624800",
    "end": "632350"
  },
  {
    "text": "And this can happen\ndue to smaller errors in the kinematics\nor in the vision",
    "start": "632350",
    "end": "639470"
  },
  {
    "text": "and therefore even\njust small errors cause a complete\nfailure of the system.",
    "start": "639470",
    "end": "645680"
  },
  {
    "text": "And therefore, by trying to\ncorrelate that information,",
    "start": "645680",
    "end": "650795"
  },
  {
    "text": "the one from the kinematics,\nin this case the robot is using it on the fingertip\nto the see where it actually",
    "start": "650795",
    "end": "661370"
  },
  {
    "text": "is in the image. And when the kinematic model\npredicts the hand should be.",
    "start": "661370",
    "end": "668150"
  },
  {
    "text": "So the green dot is the point,\npredicted by kinematics, of where the system expects\nthe fingertip to be,",
    "start": "668150",
    "end": "674480"
  },
  {
    "text": "and where actually\nthe fingertip is. Of course, by learning\nthis relation, the robot is able to cope with\nthis kind of misalignment.",
    "start": "674480",
    "end": "682940"
  },
  {
    "text": "And therefore, after a\nfirst calibration phase, it's able to perform the\nreaching action, successfully,",
    "start": "682940",
    "end": "691948"
  },
  {
    "text": "as you will see in a moment. Also, this kind of\nability of calibrating",
    "start": "691948",
    "end": "698580"
  },
  {
    "text": "would be pretty useful in case\nof the situations in which the robot is damaged, and\ntherefore, it's actual model",
    "start": "698580",
    "end": "705500"
  },
  {
    "text": "changes completely. As you can see,\nnow it's reaching",
    "start": "705500",
    "end": "710860"
  },
  {
    "text": "and it's performing\nthe grasp correctly. ",
    "start": "710860",
    "end": "721190"
  },
  {
    "text": "Finally, before going\nwith the actual talk,",
    "start": "721190",
    "end": "727010"
  },
  {
    "text": "I'm going to show a final\nvideo about balancing. Some of you have asked\nif the robot works,",
    "start": "727010",
    "end": "733460"
  },
  {
    "text": "and the robot is currently\nnot able to work, but this is a video\nfrom the people from the group that\nis actually in charge",
    "start": "733460",
    "end": "739970"
  },
  {
    "text": "of making the robot work. The first step, of\ncourse, will be balancing. And this is an example of it.",
    "start": "739970",
    "end": "746950"
  },
  {
    "text": "It's actually one\nfoot balancing where multiple components of what\nI've shown you about the iCub",
    "start": "746950",
    "end": "755930"
  },
  {
    "text": "so far. Torque sensing, initial\nsensing are combined together to have the robot\nstand on one foot.",
    "start": "755930",
    "end": "763070"
  },
  {
    "text": "And also be able to cope\nwith the interaction with internal forces that\ncould try to have it fall.",
    "start": "763070",
    "end": "773503"
  },
  {
    "start": "773503",
    "end": "780420"
  },
  {
    "text": "They are applying\nforces to the robot and it's able to\ndetect the force",
    "start": "780420",
    "end": "786500"
  },
  {
    "text": "and to cope a bit with the\nforces but to stay stable.",
    "start": "786500",
    "end": "791554"
  },
  {
    "text": "OK so, this was just\na brief overview of some things that can\nbe done with the iCub.",
    "start": "791554",
    "end": "799279"
  },
  {
    "text": "And actually, the next\ntalk will be a bit more about what's going on with it.",
    "start": "799280",
    "end": "804430"
  },
  {
    "text": " ALESSANDRO RONCONE: I want\nto talk to you about part",
    "start": "804430",
    "end": "811910"
  },
  {
    "text": "of my PhD project that was\nabout tackling the perception",
    "start": "811910",
    "end": "818595"
  },
  {
    "text": "problem. ",
    "start": "818595",
    "end": "824509"
  },
  {
    "text": "Tackling the perception\nproblem through the use of multisensor integration.",
    "start": "824510",
    "end": "830810"
  },
  {
    "text": "And specifically, I narrowed\ndown this big problem",
    "start": "830810",
    "end": "835920"
  },
  {
    "text": "by implementing a model of\nPeriPersonal Space on the iCub. That is, biology\ninspired approach.",
    "start": "835920",
    "end": "844130"
  },
  {
    "text": "PeriPersonal Space\nis a concept that has been known in neuroscience\nand psychology for years.",
    "start": "844130",
    "end": "850040"
  },
  {
    "text": "And so, let me start with\nwhat PeriPersonal Space is, and why it is so important\nfor humans and animals.",
    "start": "850040",
    "end": "858050"
  },
  {
    "text": "It is defined as\nthe space around us, within which objects can\nbe grasped and manipulated.",
    "start": "858050",
    "end": "863610"
  },
  {
    "text": "It's an interface,\nbasically, between our body and the external world. And for this reason,\nit benefits from",
    "start": "863610",
    "end": "870080"
  },
  {
    "text": "a multimodal integrated\nrepresentation that merges information\nbetween different modalities.",
    "start": "870080",
    "end": "876319"
  },
  {
    "text": "And historically, these\nhave been the vision system, the tactile system, the\nperception, auditory system,",
    "start": "876320",
    "end": "882980"
  },
  {
    "text": "and even the motor system. Historically, it\nhas been studied by two different fields.",
    "start": "882980",
    "end": "889070"
  },
  {
    "text": "The neurophysiology on\none side, and all that could be related to psychology\nand developmental psychology",
    "start": "889070",
    "end": "897439"
  },
  {
    "text": "on the other. They basically follow the\ntwo different approaches, the former being bottom up,\nthe latter being top down.",
    "start": "897440",
    "end": "904399"
  },
  {
    "text": "And they came out with\ndifferent outcomes. And the former emphasizes\nthe role of the perception",
    "start": "904400",
    "end": "911420"
  },
  {
    "text": "and it interplays\nwith the motor system in the control of movement,\nwhereas the latter was focusing",
    "start": "911420",
    "end": "918980"
  },
  {
    "text": "mainly on the\nmultisensory aspect, that is, how different modalities\nwere combined together",
    "start": "918980",
    "end": "925339"
  },
  {
    "text": "in order to form a\ncoherent view of the body and the nearby space.",
    "start": "925340",
    "end": "931010"
  },
  {
    "text": "Luckily, in recent\nyears, they decided to converge to a common ground,\nand a shared interpretation,",
    "start": "931010",
    "end": "937190"
  },
  {
    "text": "and for the purposes\nof my work I would like to highlight\nthe main aspect.",
    "start": "937190",
    "end": "942200"
  },
  {
    "text": "Firstly, and this one might be\nof interest from an engineering perspective. PeriPersonal Space is made\nof different reference",
    "start": "942200",
    "end": "951650"
  },
  {
    "text": "points that are located in\ndifferent regions of the brain. And there might be\na way for the brain",
    "start": "951650",
    "end": "957350"
  },
  {
    "text": "to switch from one to another,\naccording to different contact and goal.",
    "start": "957350",
    "end": "962630"
  },
  {
    "text": "And secondly, as I was\nsaying, PeriPersonal Space benefits from a\nmultisensory integration",
    "start": "962630",
    "end": "969500"
  },
  {
    "text": "in order to form a coherent\nview of the body understanding space. In this experiment made\nby Fogassi in 1996,",
    "start": "969500",
    "end": "975900"
  },
  {
    "text": "they basically found a number\nof so-called visual tactile neurons that are set up neurons\nthat fire both stimulated",
    "start": "975900",
    "end": "984850"
  },
  {
    "text": "in a specific skin part\nand if an object is presented in surrounding space.",
    "start": "984850",
    "end": "991140"
  },
  {
    "text": "So this means that\nthese neurons code both the visual information\nand the tactile information.",
    "start": "991140",
    "end": "997079"
  },
  {
    "text": "But they also have some\nproprioceptive information, because they are basically\nattached to the body part",
    "start": "997080",
    "end": "1002260"
  },
  {
    "text": "that they belong to. Lastly, one of the main\nproperties of this presentation",
    "start": "1002260",
    "end": "1008140"
  },
  {
    "text": "is in basic plasticity. And for example,\nin this experiment, made by Iriki ten years ago,\nthe extension of this receptive",
    "start": "1008140",
    "end": "1018490"
  },
  {
    "text": "field in the visual space,\nin the surrounding space, after training with\na rake, have been",
    "start": "1018490",
    "end": "1024849"
  },
  {
    "text": "shown to go up to\nenclose the tool as if this tool becomes\npart of the body.",
    "start": "1024849",
    "end": "1033000"
  },
  {
    "text": "So through experience\nand through tool use, the monkey was able to\ngrow this receptive field.",
    "start": "1033000",
    "end": "1041369"
  },
  {
    "text": " Those are properties\nthat are very nice,",
    "start": "1041369",
    "end": "1047680"
  },
  {
    "text": "and we would like them to\nbe available for the robot. And, in general\nrobotics, the work",
    "start": "1047680",
    "end": "1052690"
  },
  {
    "text": "related to PeriPersonal Space\ncan be divided into two groups. On the one side, the model,\nand the simulation basically.",
    "start": "1052690",
    "end": "1060910"
  },
  {
    "text": "The closest one to\nmy work was the one from Fuke, a colleague that\nare from [INAUDIBLE] lab,",
    "start": "1060910",
    "end": "1066799"
  },
  {
    "text": "in which they used a\nstimulated robot in order to model the mechanisms\nthat are leading",
    "start": "1066800",
    "end": "1075340"
  },
  {
    "text": "to this visual-tactile\npresentation. On the other side, there are\nthe engineering approaches",
    "start": "1075340",
    "end": "1080650"
  },
  {
    "text": "that are few. The closest one is this\none by Mittendorfer from Gordon Cheng's\nlab, in which they first",
    "start": "1080650",
    "end": "1088220"
  },
  {
    "text": "developed the multimodal skin. So they developed the hardware\nto be able to do that. And then they use the to trigger\nlocal avoidance responses,",
    "start": "1088220",
    "end": "1099770"
  },
  {
    "text": "reflexes to incoming objects. We are trying to position\nourself in the middle.",
    "start": "1099770",
    "end": "1104900"
  },
  {
    "text": "Let's say, we are\nnot trying to create a perfect model of\nPeriPersonal Space from a biological perspective.",
    "start": "1104900",
    "end": "1112690"
  },
  {
    "text": "But on the other\nside, we would like to have something that is\nalso working, and useful",
    "start": "1112690",
    "end": "1118750"
  },
  {
    "text": "for our proposals. So from now on, I will divide\nthe presentation in two parts.",
    "start": "1118750",
    "end": "1124679"
  },
  {
    "text": "The first will be\nabout the model. So what we think will be useful\nfor tackling the problem,",
    "start": "1124680",
    "end": "1130990"
  },
  {
    "text": "on the other side I show you an\napplication of this model, that is basically using the\nlow-color presentation in order",
    "start": "1130990",
    "end": "1138470"
  },
  {
    "text": "to trigger avoidance responses\nor reaching responses distributed throughout the body.",
    "start": "1138470",
    "end": "1146140"
  },
  {
    "text": "So let me start\nwith the proposed model of PeriPersonal Space. Loosely inspired by the\nneurophysiological findings",
    "start": "1146140",
    "end": "1154070"
  },
  {
    "text": "like we discussed before, we\ndeveloped this PeriPersonal",
    "start": "1154070",
    "end": "1159190"
  },
  {
    "text": "Space presentation\nby means of access of facial receptive field\nthat we are going out",
    "start": "1159190",
    "end": "1165760"
  },
  {
    "text": "from the robot's skin. So basically, they were\nextending the tactile domain",
    "start": "1165760",
    "end": "1170779"
  },
  {
    "text": "into nearby space. Each tactile, that is, each pair\nthe iCub skin is composed of,",
    "start": "1170780",
    "end": "1180029"
  },
  {
    "text": "will experience a set\nof multisensory events. So basically, you\nare letting the robot",
    "start": "1180030",
    "end": "1187450"
  },
  {
    "text": "learn this visual-tactile\nsensations by taking an object and making contact\non the skin part.",
    "start": "1187450",
    "end": "1195890"
  },
  {
    "text": "We learn it by\ntactile experience we learn a sort of probability\nof being touched prior",
    "start": "1195890",
    "end": "1202840"
  },
  {
    "text": "to contact activation when\nthe new incoming object is presented.",
    "start": "1202840",
    "end": "1208510"
  },
  {
    "text": "And we basically created this\ncone shape receptive field that is going from\neach of the taxels.",
    "start": "1208510",
    "end": "1215350"
  },
  {
    "text": "And for any object that is\nentering this receptive field, we created we called a buffer,\nof the path so basically,",
    "start": "1215350",
    "end": "1225210"
  },
  {
    "text": "the idea is that the orbit\nhas some information from what was going on before they\ntouch, the actual contact.",
    "start": "1225210",
    "end": "1230980"
  },
  {
    "text": "And if the object eventually\nends up touching the tactile,",
    "start": "1230980",
    "end": "1236540"
  },
  {
    "text": "it will be labeled as\na positive event that",
    "start": "1236540",
    "end": "1241900"
  },
  {
    "text": "will enforce the\nprobability of the event the ending of\ntouching the taxel.",
    "start": "1241900",
    "end": "1247960"
  },
  {
    "text": "If not, for example, it might\nbe that the object enters this receptive field,\nand in the end, ends up touching another taxel.",
    "start": "1247960",
    "end": "1256030"
  },
  {
    "text": "This will be labeled\nas a negative. So at the end, we will have a\nset of positive and negative",
    "start": "1256030",
    "end": "1261399"
  },
  {
    "text": "events a taxel can learn from. This is three dimensional\nspace because the distance",
    "start": "1261400",
    "end": "1267269"
  },
  {
    "text": "is three dimensional. And we narrowed it down to\na one dimensional domain,",
    "start": "1267270",
    "end": "1273370"
  },
  {
    "text": "by basically, taking the\nnorm of the distance, but also the relative position\nof the object and the taxel.",
    "start": "1273370",
    "end": "1279460"
  },
  {
    "text": "In order for us to be able\nto cope with the calibration errors that were amounting\nup to a couple of centimeters",
    "start": "1279460",
    "end": "1286420"
  },
  {
    "text": "that were significant.  One dimensional\nvariable has been",
    "start": "1286420",
    "end": "1294210"
  },
  {
    "text": "discretized into a set of bins. And for each bin, we\ncomputed the probability",
    "start": "1294210",
    "end": "1300280"
  },
  {
    "text": "of an event belonging to\nthat, of being touched. So the idea is that,\nat 20 centimeters,",
    "start": "1300280",
    "end": "1307990"
  },
  {
    "text": "the probability of being\ntouched would be lower than a zero centimeter. This is the intuitive idea. ",
    "start": "1307990",
    "end": "1316410"
  },
  {
    "text": "Over this one dimensional\nvisualization, we used a partial window\ninterpolation technique",
    "start": "1316410",
    "end": "1321850"
  },
  {
    "text": "in order to provide us with\nthe two dimensional function that, at the end, we\ngive up a inactivation",
    "start": "1321850",
    "end": "1328450"
  },
  {
    "text": "value that is proportional with\nthe distance of the object.",
    "start": "1328450",
    "end": "1334399"
  },
  {
    "text": "So as soon as the new object\nwill enter the receptive field, I will have the taxel fire\nbefore being contacted.",
    "start": "1334400",
    "end": "1342700"
  },
  {
    "text": " We did, basically,\ntwo experiments.",
    "start": "1342700",
    "end": "1348070"
  },
  {
    "text": "Initially, we did a simulation\nin a mock lab in order to assess the convergence\non the long term",
    "start": "1348070",
    "end": "1355090"
  },
  {
    "text": "learning, one-shot\nlearning behavior, to assess if our model was\nable to cope with noise,",
    "start": "1355090",
    "end": "1363880"
  },
  {
    "text": "with the current\ncalibration errors. And then, we went\non the real robot. We presented them with\ndifferent objects.",
    "start": "1363880",
    "end": "1370540"
  },
  {
    "text": "And we were basically\ntouching the robot 100 times in order to make it learn\nthese presentations.",
    "start": "1370540",
    "end": "1377780"
  },
  {
    "text": "So, trust me, I don't\nwant to bother you with this kind of\ntechnicalities,",
    "start": "1377780",
    "end": "1383259"
  },
  {
    "text": "but we did a lot of work. This is, basically,\nthe math of the result.",
    "start": "1383260",
    "end": "1389169"
  },
  {
    "text": "So, let me go on the second\npart, in which the main problem",
    "start": "1389170",
    "end": "1394300"
  },
  {
    "text": "was for the robot to\ndetect the object visually. In order for us to do that,\nwe developed a 3D tracking",
    "start": "1394300",
    "end": "1401140"
  },
  {
    "text": "algorithm, that was able to\ntrack a [INAUDIBLE] object, basically. To design, we used\nsome software that",
    "start": "1401140",
    "end": "1409240"
  },
  {
    "text": "was only available in the\niCub software repository. The engine provides you\nwith some basic algorithms",
    "start": "1409240",
    "end": "1416350"
  },
  {
    "text": "that you can play with. And namely, we used a two\ndimensional optical flow",
    "start": "1416350",
    "end": "1422500"
  },
  {
    "text": "made by Carlo and a\n2D particle filter and a 3D stereo\nvision algorithm,",
    "start": "1422500",
    "end": "1427750"
  },
  {
    "text": "that is basically the same\nas I was showing before during the recognition game.",
    "start": "1427750",
    "end": "1433659"
  },
  {
    "text": "And this basically was\nfeeding a 3-D camera",
    "start": "1433660",
    "end": "1440470"
  },
  {
    "text": "to provide the robot estimation\nof the position of the object. So, the idea is that the motion\ndetector from the optical flow",
    "start": "1440470",
    "end": "1450120"
  },
  {
    "text": "act as a trigger for the\nsubsequent pipeline, in which,",
    "start": "1450120",
    "end": "1455550"
  },
  {
    "text": "basically, after a\nconsistent enough motion in this optical flow module,\nthis would be a template",
    "start": "1455550",
    "end": "1467799"
  },
  {
    "text": "to be taught in the visual in\nthe 2D visual world by this. Then, that this information\nis sent to the 3D depth map",
    "start": "1467800",
    "end": "1475960"
  },
  {
    "text": "and this would be feeding\nthe camera feature in order to provide us with the table\nrepresentation because,",
    "start": "1475960",
    "end": "1484000"
  },
  {
    "text": "obviously, the stereo system\ndoesn't work that good in our context.",
    "start": "1484000",
    "end": "1489980"
  },
  {
    "text": "And this, if it works-- no.",
    "start": "1489980",
    "end": "1495730"
  },
  {
    "text": "OK. On my laptop, it works here. OK. Now it works. OK.",
    "start": "1495730",
    "end": "1501181"
  },
  {
    "text": "This is the idea. So I was basically\nwaving, moving the object in the beginning.",
    "start": "1501181",
    "end": "1506340"
  },
  {
    "text": "OK. Then when it is detected,\nthis pattern starts. And you can see\nhere the tracking.",
    "start": "1506340",
    "end": "1513237"
  },
  {
    "text": "This is the stereo vision. This the final outcome. This was used for the learning.",
    "start": "1513237",
    "end": "1522370"
  },
  {
    "text": "We did a lot of iterations\nof these objects that are approaching the\nskin on different body parts.",
    "start": "1522370",
    "end": "1528130"
  },
  {
    "text": "This is the graph. I don't want to talk about that. So let me start with the video.",
    "start": "1528130",
    "end": "1534400"
  },
  {
    "text": "This is basically the skin. And this is the part that\nit was trained before.",
    "start": "1534400",
    "end": "1539919"
  },
  {
    "text": "When there is a contact,\nthere is activation here. You can see here the activation. And soon after, this thing\nworked also with one example.",
    "start": "1539920",
    "end": "1549970"
  },
  {
    "text": "The taxel starts firing\nbefore the contact. And obviously, this is\nimproved over the time.",
    "start": "1549970",
    "end": "1557080"
  },
  {
    "text": "And it depends on the\nbody part that is touched. For example, if I touch here,\nI'm coming from the top.",
    "start": "1557080",
    "end": "1562750"
  },
  {
    "text": "So the representation\nstarts firing mainly here.",
    "start": "1562750",
    "end": "1568430"
  },
  {
    "text": "And this, obviously, depends\non the specific body part. Now, I think that I'm\ngoing to touch the hand.",
    "start": "1568430",
    "end": "1574570"
  },
  {
    "text": "And so after a while, you will\nhave an activation on the hand.",
    "start": "1574570",
    "end": "1580091"
  },
  {
    "text": "Obviously, I will have also\nsome activation in the forearm, because I was getting\ncloser to the forearm.",
    "start": "1580092",
    "end": "1586090"
  },
  {
    "text": "And as an application\nof this, this one is simply a presentation. So it's not that usable.",
    "start": "1586090",
    "end": "1591940"
  },
  {
    "text": "We basically exploited it in\norder to develop an avoidance--",
    "start": "1591940",
    "end": "1598179"
  },
  {
    "text": "a margin of safety\naround the body. Let's say if the\ntaxel is firing, I would like it to go\naway from the object,",
    "start": "1598180",
    "end": "1607090"
  },
  {
    "text": "assuming that this can be a\npotentially harmful object. And on the other\nway, I would like",
    "start": "1607090",
    "end": "1612130"
  },
  {
    "text": "it to be able to reach\nwith any body part the object under consideration.",
    "start": "1612130",
    "end": "1617640"
  },
  {
    "text": "So to this end, we developed\nthe avoidance and catching controller that was able to\nleverage on this distributed",
    "start": "1617640",
    "end": "1625299"
  },
  {
    "text": "information and perform\na sensor-based guidance",
    "start": "1625300",
    "end": "1630640"
  },
  {
    "text": "of the model actions by\nmeans of this visual tactile associations. And this is basically\nhow it works.",
    "start": "1630640",
    "end": "1637340"
  },
  {
    "text": "So this is at the testing stage. So I already learned\nthe representation.",
    "start": "1637340",
    "end": "1642700"
  },
  {
    "text": "As soon as I get\ncloser, the taxel starts firing, because of the\nprobabilities I was learning.",
    "start": "1642700",
    "end": "1648980"
  },
  {
    "text": "And the arm goes away. Obviously, the movement\ndepends on the specific skin",
    "start": "1648980",
    "end": "1654290"
  },
  {
    "text": "part that has been touched. If I'm touching here, the\nobject will go away from here.",
    "start": "1654290",
    "end": "1659920"
  },
  {
    "text": "If I'm coming from the top-- I think this one was\ndoing from the top, yes--",
    "start": "1659920",
    "end": "1666160"
  },
  {
    "text": "the object will go\naway from the back. The object will be going a\nway from another direction.",
    "start": "1666160",
    "end": "1672500"
  },
  {
    "text": "And the idea here is not to,\nbasically, tackle the problem from a classical\nrobotics approach.",
    "start": "1672500",
    "end": "1681334"
  },
  {
    "text": "But the basic\nidea-- this behavior emerges from the learning. And the idea was very simple.",
    "start": "1681334",
    "end": "1687970"
  },
  {
    "text": "We were basically looking at\nthe taxel that we were firing. If they were firing\nenough, then we",
    "start": "1687970",
    "end": "1694030"
  },
  {
    "text": "were recording their position. And we were doing,\nbasically, a population coding that is a weighted\naverage according",
    "start": "1694030",
    "end": "1700450"
  },
  {
    "text": "to the activation\nand the prediction. We did that to both\nfor the position of the taxel and the normal.",
    "start": "1700450",
    "end": "1706150"
  },
  {
    "text": "So at the end if you have\na bunch of tactiles here, we will end up with one\npoint to go away from.",
    "start": "1706150",
    "end": "1713590"
  },
  {
    "text": "And on the other side, the\ncatching, the reaching, was basically the same, but\nin the opposite direction.",
    "start": "1713590",
    "end": "1720400"
  },
  {
    "text": "So if I want to\navoid, I do this. If I want to catch, I do this. Obviously, if you\ndo it in the hand,",
    "start": "1720400",
    "end": "1727250"
  },
  {
    "text": "this would be a standard\nrobotic reaching. But this actually\ncan be triggered also",
    "start": "1727250",
    "end": "1734260"
  },
  {
    "text": "in different body parts. As you can see here, I get a\nvirtual activation, and then the physical contact.",
    "start": "1734260",
    "end": "1740990"
  },
  {
    "text": "And yes, basically, our design\nwas to use the same controller",
    "start": "1740990",
    "end": "1749320"
  },
  {
    "text": "for both of the behaviors. ",
    "start": "1749320",
    "end": "1755910"
  },
  {
    "text": "OK. This is also some technicalities\nthat I don't want to show you. So in conclusion, the\ndetector presented here",
    "start": "1755910",
    "end": "1762760"
  },
  {
    "text": "is, to our knowledge,\nthe first attempt at creating a decentralized,\nmultisensory visual tactile",
    "start": "1762760",
    "end": "1768610"
  },
  {
    "text": "representation for a\nrobot and its nearby space by means of the distributed\nskin and interaction",
    "start": "1768610",
    "end": "1776770"
  },
  {
    "text": "with the environment. One of the assets of\nour representation is that learning is fast. As you were seeing,\nit can learn, also,",
    "start": "1776770",
    "end": "1784750"
  },
  {
    "text": "from one single example. It's in parallel for the\nwhole body in the sense",
    "start": "1784750",
    "end": "1790090"
  },
  {
    "text": "that every tactile\nlearns independently. Its own representation\nis incremental in a sense",
    "start": "1790090",
    "end": "1795450"
  },
  {
    "text": "that it converges toward\na stable representation over the time. And importantly, it is\nadapted from experience.",
    "start": "1795450",
    "end": "1802440"
  },
  {
    "text": "So basically, it\ncan automatically compensate for errors\nin the model that, for humanoid robots, is\none of the main problems",
    "start": "1802440",
    "end": "1809580"
  },
  {
    "text": "when merging different\nmodalities OK. Thank you. If you have any question,\nfeel free to ask.",
    "start": "1809580",
    "end": "1816281"
  },
  {
    "text": "RAFFAELLO CAMORIANO:\nI am Raffaello. And today, I'll talk to\nyou about a little bit of my work on machine\nlearning and robotics,",
    "start": "1816281",
    "end": "1823980"
  },
  {
    "text": "in particular some subsets\nof machine learning",
    "start": "1823980",
    "end": "1829380"
  },
  {
    "text": "which are the large\nscale learning and incremental learning. But what do we expect\nfrom our modern robot?",
    "start": "1829380",
    "end": "1838220"
  },
  {
    "text": "And how can machine\nlearning help out with this? Well, we expect\nmodern robots to work",
    "start": "1838220",
    "end": "1846045"
  },
  {
    "text": "in, particularly,\nunstructured environments which they have\nnever seen before",
    "start": "1846045",
    "end": "1852270"
  },
  {
    "text": "and to learn new\ntasks on the fly depending on the\nparticular needs throughout the operation\nof the robot itself",
    "start": "1852270",
    "end": "1860670"
  },
  {
    "text": "and across different modalities. For instance--\nvision, of course, but also tactile sensing which\nis available on the iCub also",
    "start": "1860670",
    "end": "1869830"
  },
  {
    "text": "proprioceptive\nsensing, including force sensing, [INAUDIBLE]\nand so on and so forth.",
    "start": "1869830",
    "end": "1876220"
  },
  {
    "text": "And we want to do all of this\nthroughout a very long time span potentially. Because we expect\nrobots to be companions",
    "start": "1876220",
    "end": "1884250"
  },
  {
    "text": "of humans in the real\nworld operating for maybe years or more.",
    "start": "1884250",
    "end": "1890370"
  },
  {
    "text": "And this poses a\nlot of challenges, especially from the\ncomputational point of view.",
    "start": "1890370",
    "end": "1895650"
  },
  {
    "text": "And machine learning\ncan actually help with this tackling\nthese challenges.",
    "start": "1895650",
    "end": "1903600"
  },
  {
    "text": "For instance, there\nare large scale learning methods, which are\nalgorithms which can work",
    "start": "1903600",
    "end": "1912820"
  },
  {
    "text": "with very large scale datasets. For instance, if we have\nmillions of points gathered",
    "start": "1912820",
    "end": "1920460"
  },
  {
    "text": "by the robot cameras\nthroughout 10 days and we want to\nprocess them, well,",
    "start": "1920460",
    "end": "1926970"
  },
  {
    "text": "if we use standard\nmachine learning methods, that will be a very\ndifficult problem",
    "start": "1926970",
    "end": "1932670"
  },
  {
    "text": "to solve if we don't use, for\ninstance, randomizing methods",
    "start": "1932670",
    "end": "1940410"
  },
  {
    "text": "and so on and so forth. Machine learning also has\nincremental algorithms,",
    "start": "1940410",
    "end": "1950890"
  },
  {
    "text": "which can allow the\nlearned model to be updated",
    "start": "1950890",
    "end": "1957030"
  },
  {
    "text": "as new previously unseen\nfeatures are presented",
    "start": "1957030",
    "end": "1965790"
  },
  {
    "text": "to the agent. And also, there is a\nsubfield of transfer learning",
    "start": "1965790",
    "end": "1973140"
  },
  {
    "text": "which allows knowledge\nlearned for a particular task",
    "start": "1973140",
    "end": "1978840"
  },
  {
    "text": "to be used for serving another\nrelated task without the need",
    "start": "1978840",
    "end": "1984179"
  },
  {
    "text": "for seeing many new\nexamples for the new task.",
    "start": "1984180",
    "end": "1990630"
  },
  {
    "text": "So my main research focuses\nare in machine learning.",
    "start": "1990630",
    "end": "1995920"
  },
  {
    "text": "I work especially in large\nscale learning methods, incremental learning,\nand in the design",
    "start": "1995920",
    "end": "2003860"
  },
  {
    "text": "of algorithms which allow for\ncomputational and accuracy",
    "start": "2003860",
    "end": "2011760"
  },
  {
    "text": "trade-offs. I will explain this\na bit more later.",
    "start": "2011760",
    "end": "2019049"
  },
  {
    "text": "And as concerns\nrobotic applications, I work with Guilia,\nCarlo and others",
    "start": "2019050",
    "end": "2029120"
  },
  {
    "text": "on incremental\nobject recognition, so in a setting in which the\nrobot is presented new objects",
    "start": "2029120",
    "end": "2036950"
  },
  {
    "text": "throughout a long time span. And it has to learn\nthem on the fly.",
    "start": "2036950",
    "end": "2042560"
  },
  {
    "text": "And also, I'm working in a\nsystem identification setting,",
    "start": "2042560",
    "end": "2052730"
  },
  {
    "text": "which I will explain\nlater, related to the motion of the robot.",
    "start": "2052730",
    "end": "2058620"
  },
  {
    "text": "So this is one of\nthe works which",
    "start": "2058620",
    "end": "2063919"
  },
  {
    "text": "has occupied my last year. And it is related to\nlarge scale learning.",
    "start": "2063920",
    "end": "2071210"
  },
  {
    "text": "So if we consider that we may\nhave a very large n, which",
    "start": "2071210",
    "end": "2078199"
  },
  {
    "text": "is a number of examples\nwe have access to, in the setting of\nkernel methods,",
    "start": "2078199",
    "end": "2084710"
  },
  {
    "text": "we may have to store a huge\nmatrix, the matrix K, which is n by n, which could be\nsimply impossible to store.",
    "start": "2084710",
    "end": "2094888"
  },
  {
    "text": "So there are randomized methods,\nlike the Nystrom method, which",
    "start": "2094889",
    "end": "2101339"
  },
  {
    "text": "enable to compute a low rank\napproximation of the kernel",
    "start": "2101340",
    "end": "2109190"
  },
  {
    "text": "metrics simply by throwing\na few points m at random,",
    "start": "2109190",
    "end": "2123960"
  },
  {
    "text": "a few samples at random, and\nbuilding the metrics K and m,",
    "start": "2123960",
    "end": "2130966"
  },
  {
    "text": "which is just much smaller. Because m is much\nsmaller than n. And this is a well-known\nmethod in machine learning.",
    "start": "2130966",
    "end": "2141060"
  },
  {
    "text": "But we tried to see it from\na different point of view",
    "start": "2141060",
    "end": "2147470"
  },
  {
    "text": "than usual. Usually, this is seen just from\na computational point of view in order to fit a difficult\nproblem inside computers",
    "start": "2147470",
    "end": "2161569"
  },
  {
    "text": "with limited capabilities\nwhile we proposed",
    "start": "2161570",
    "end": "2169860"
  },
  {
    "text": "to see the Nystrom\napproximation as regularization",
    "start": "2169860",
    "end": "2174880"
  },
  {
    "text": "of operation itself. So if you can see this, the\nusual way in which the Nystrom",
    "start": "2174880",
    "end": "2187250"
  },
  {
    "text": "method is applied, for instance,\nwith kernel regularized least squares.",
    "start": "2187250",
    "end": "2194160"
  },
  {
    "text": "The parameter m, so\nthe number of examples we are taking at\nrandom, is usually taken",
    "start": "2194160",
    "end": "2203340"
  },
  {
    "text": "as large as possible\nin order just to fit in the memory of\nthe available machines.",
    "start": "2203340",
    "end": "2211850"
  },
  {
    "text": "While, actually, after\nchoosing a large m,",
    "start": "2211850",
    "end": "2217200"
  },
  {
    "text": "it is often necessary\nto regularize with deep neural\nregularization, for instance.",
    "start": "2217200",
    "end": "2223520"
  },
  {
    "text": "And this sounds a bit like\na waste of time and memory.",
    "start": "2223520",
    "end": "2229290"
  },
  {
    "text": "Because, actually, what\nregularization, roughly",
    "start": "2229290",
    "end": "2234780"
  },
  {
    "text": "speaking, does is to discard\nthe irrelevant eigen components",
    "start": "2234780",
    "end": "2240270"
  },
  {
    "text": "of the kernel metrics. So we observe that\nwe can do this by just less random examples,\nso having a smaller model which",
    "start": "2240270",
    "end": "2254359"
  },
  {
    "text": "can be computed more\nefficiently and without having to regularize again later.",
    "start": "2254360",
    "end": "2262579"
  },
  {
    "text": "So m, the number of\nexamples which are used, controls both the\nregularization and",
    "start": "2262580",
    "end": "2270450"
  },
  {
    "text": "the computational\ncomplexity of our algorithm. This is very useful in a\nrobotic setting in which we",
    "start": "2270450",
    "end": "2277650"
  },
  {
    "text": "have to deal with lots of data. As regards to the incremental\nobjects recognition task,",
    "start": "2277650",
    "end": "2288340"
  },
  {
    "text": "this is another\nproject I'm working on. And imagine that the\nrobot has to work",
    "start": "2288340",
    "end": "2296760"
  },
  {
    "text": "in an unknown environment, and\nit is presented novel objects on the fly.",
    "start": "2296760",
    "end": "2301785"
  },
  {
    "text": "And it has to update its\nobject recognition model",
    "start": "2301785",
    "end": "2310770"
  },
  {
    "text": "in an efficient way\nwithout retraining from scratch every time\na new object arrives.",
    "start": "2310770",
    "end": "2316650"
  },
  {
    "text": "So this can be done easily\nby a slight modification",
    "start": "2316650",
    "end": "2321750"
  },
  {
    "text": "of the regularized\nleast squares algorithm and proper reweighting. An open question is how to\nchange the regularization",
    "start": "2321750",
    "end": "2328940"
  },
  {
    "text": "as n grows. Because we didn't find\nyet a way to efficiently",
    "start": "2328940",
    "end": "2334230"
  },
  {
    "text": "update regularization\nparameter in this case. So we are still working on this.",
    "start": "2334230",
    "end": "2341250"
  },
  {
    "text": "The last project I'll talk about\nis more related to, let's see,",
    "start": "2341250",
    "end": "2347295"
  },
  {
    "text": "physics and motion. So we have an arbitrary\nlimb of the robot,",
    "start": "2347295",
    "end": "2357530"
  },
  {
    "text": "for instance, the arm. And our task is to learn\na model which can provide",
    "start": "2357530",
    "end": "2370380"
  },
  {
    "text": "an interesting dynamics model. So it can predict the\ninner forces of the arm",
    "start": "2370380",
    "end": "2376220"
  },
  {
    "text": "during motion. This is useful, for instance,\nin a contact detection setting.",
    "start": "2376220",
    "end": "2384010"
  },
  {
    "text": "So when the sensor readings\nare different from the group predicted one, that means\nthat there may be a contact.",
    "start": "2384010",
    "end": "2391310"
  },
  {
    "text": "Or for external force\nestimation or, for example,",
    "start": "2391310",
    "end": "2399570"
  },
  {
    "text": "for the identification of the\nmass of a manipulated object.",
    "start": "2399570",
    "end": "2404770"
  },
  {
    "text": "So we have some challenged\nfor this project.",
    "start": "2404770",
    "end": "2411700"
  },
  {
    "text": "We have to devise a model\nwhich could be interpretable, so in which the rigid body\ndynamic parameter would",
    "start": "2411700",
    "end": "2420970"
  },
  {
    "text": "be understandable\nand intelligible for controlled purposes.",
    "start": "2420970",
    "end": "2427780"
  },
  {
    "text": "And we wanted this\nmodel to be more accurate than standard multibody\ndynamics, rigid body dynamics",
    "start": "2427780",
    "end": "2434790"
  },
  {
    "text": "model. And also, we want to adapt\nto changing conditions",
    "start": "2434790",
    "end": "2442220"
  },
  {
    "text": "throughout time. For instance, during the\noperation of the robot,",
    "start": "2442220",
    "end": "2449230"
  },
  {
    "text": "after one hour, the\nchanges in temperature",
    "start": "2449230",
    "end": "2456040"
  },
  {
    "text": "determine a change also\nin the dynamic properties of the mechanical\nproperties of the arm.",
    "start": "2456040",
    "end": "2464030"
  },
  {
    "text": "And we want to accommodate for\nthis in an incremental way.",
    "start": "2464030",
    "end": "2472910"
  },
  {
    "text": "So this is what we did. We implemented a\nsemi-parametric model",
    "start": "2472910",
    "end": "2477950"
  },
  {
    "text": "which the first part\nwhich has priority is a simple incremental\nparametric model.",
    "start": "2477950",
    "end": "2485710"
  },
  {
    "text": "And then we used random\nfeatures for building",
    "start": "2485710",
    "end": "2491661"
  },
  {
    "text": "non-parametric incremental\nmodel which can be updated in an efficient way.",
    "start": "2491662",
    "end": "2498120"
  },
  {
    "text": "And we shown with\nthis real experiment that the semi-parametric\nmodel worked",
    "start": "2498120",
    "end": "2504720"
  },
  {
    "text": "as well as the\nnon-parametric one. But it's faster to\nconverge, because it",
    "start": "2504720",
    "end": "2509970"
  },
  {
    "text": "has an initial knowledge\nabout the physics of the arm.",
    "start": "2509970",
    "end": "2515230"
  },
  {
    "text": "And it is also better than\nthe fully parametric one,",
    "start": "2515230",
    "end": "2520450"
  },
  {
    "text": "because it also\nmodels, for example, dynamical effect due to\ndeflectability of the body.",
    "start": "2520450",
    "end": "2528680"
  },
  {
    "text": "And dynamic deflectors\nare usually not modeled",
    "start": "2528680",
    "end": "2535640"
  },
  {
    "text": "by rigid body dynamic models. OK.",
    "start": "2535640",
    "end": "2541000"
  },
  {
    "text": "Another thing I'm doing is\nmaintaining the Grand Unified Regularized Least\nSquares library,",
    "start": "2541000",
    "end": "2547859"
  },
  {
    "text": "which is a library for\nregularized least squares, of course.",
    "start": "2547860",
    "end": "2554400"
  },
  {
    "text": "It supports a large\nscale dataset. This was developed in joint\nexchange between MIT and IIT",
    "start": "2554400",
    "end": "2560349"
  },
  {
    "text": "some years ago by\nothers, not by me. And it has a MATLAB\nand a C++ interface.",
    "start": "2560350",
    "end": "2569296"
  },
  {
    "text": "If you want to have a look\nat how these methods work,",
    "start": "2569296",
    "end": "2574670"
  },
  {
    "text": "I suggest you to\ntry out tutorials which are available on GitHub.",
    "start": "2574670",
    "end": "2579740"
  },
  {
    "text": "GUILIA PASQUALE: I'm Guilia. And I work on the iCub\nrobot with my colleagues,",
    "start": "2579740",
    "end": "2585310"
  },
  {
    "text": "especially on vision\nand, in particular, on visual recognition. I work under the supervision\nof Lorenzo Natale and Lorenzo",
    "start": "2585310",
    "end": "2593650"
  },
  {
    "text": "Rosasco. Both will be here for a few\ndays in the following weeks.",
    "start": "2593650",
    "end": "2599210"
  },
  {
    "text": "And the work that\nI'm going to present has been done in\ncollaboration with Carlo and also Francesca Odone\nfrom the University of Genoa.",
    "start": "2599210",
    "end": "2609130"
  },
  {
    "text": "So in the last couple of\nyears, computer vision methods based on deep convolution\non neural networks",
    "start": "2609130",
    "end": "2617310"
  },
  {
    "text": "have achieved a\nremarkable performance in tasks such as\nlarge scale image",
    "start": "2617310",
    "end": "2623910"
  },
  {
    "text": "classification and retrieval. And the extreme success\nof these methods",
    "start": "2623910",
    "end": "2629550"
  },
  {
    "text": "is mainly due to the\nincreasing availability of all these larger datasets.",
    "start": "2629550",
    "end": "2635490"
  },
  {
    "text": "And in particular, I'm referring\nto the ImageNet one, which is composed by millions of\nexamples labeled into thousands",
    "start": "2635490",
    "end": "2645090"
  },
  {
    "text": "of categories through\ncrowd sourcing methods such as the Amazon Turk.",
    "start": "2645090",
    "end": "2650970"
  },
  {
    "text": "And in particular, the\nincreased data availability to gather with the increased\ncomputational power",
    "start": "2650970",
    "end": "2658560"
  },
  {
    "text": "has allowed to train deep\nnetworks characterized by millions of parameters\nin a supervised way",
    "start": "2658560",
    "end": "2667079"
  },
  {
    "text": "from the image up\nto the final label through the back\npropagation algorithm.",
    "start": "2667080",
    "end": "2674770"
  },
  {
    "text": "And this has allowed to mark a\nbreakthrough-- in particular,",
    "start": "2674770",
    "end": "2680830"
  },
  {
    "text": "in 2012 when Alex\nKrizhevsky proposed",
    "start": "2680830",
    "end": "2686220"
  },
  {
    "text": "for the first time of a\nnetwork of this kind trained on ImageNet dataset\nand definitely won",
    "start": "2686220",
    "end": "2693780"
  },
  {
    "text": "the ImageNet large\nscale user recognition challenge in this way.",
    "start": "2693780",
    "end": "2699000"
  },
  {
    "text": "And the trend has been confirmed\nin the following years. So that nowadays problems\nsuch as large scale image",
    "start": "2699000",
    "end": "2706980"
  },
  {
    "text": "classification or\ndetection are usually tackled following this\ndeep learning approach.",
    "start": "2706980",
    "end": "2715349"
  },
  {
    "text": "And not only, it has been\nalso demonstrated at least empirically.",
    "start": "2715350",
    "end": "2721752"
  },
  {
    "text": "Oh, I'm sorry. Maybe this is not\nparticularly clear. But this is the\nKrizhevsky Network.",
    "start": "2721752",
    "end": "2731920"
  },
  {
    "text": "Models of networks of this\nkind trained on large datasets, such as the ImageNet\none, do provide also",
    "start": "2731920",
    "end": "2742140"
  },
  {
    "text": "very good general\nand powerful image descriptors to be applied also\non other tasks and datasets.",
    "start": "2742140",
    "end": "2751500"
  },
  {
    "text": "In particular, it\nis possible to use a convolutional neural network\ntrained on ImageNet dataset,",
    "start": "2751500",
    "end": "2760000"
  },
  {
    "text": "feed it with\nimages, and using it as a black box extracting\nthe vectorial representation",
    "start": "2760000",
    "end": "2768660"
  },
  {
    "text": "of the incoming images\nas the output of one of the intermediate layers.",
    "start": "2768660",
    "end": "2774600"
  },
  {
    "text": "Or even better, it is possible\nto start from a network model trained on\nthe ImageNet dataset",
    "start": "2774600",
    "end": "2781410"
  },
  {
    "text": "and fine tune its parameters\non a new dataset for a new task",
    "start": "2781410",
    "end": "2787289"
  },
  {
    "text": "and achieving and surpassing\nthe state of the art-- for example, also in the Pascal\ndataset and other tasks--",
    "start": "2787290",
    "end": "2797400"
  },
  {
    "text": "following this approach.  So it is natural to\nask at this point, why?",
    "start": "2797400",
    "end": "2808240"
  },
  {
    "text": "Instead, in robotics,\nproviding robots",
    "start": "2808240",
    "end": "2813630"
  },
  {
    "text": "with robust and accurate\nvisual recognition capabilities in the real world is still one\nof the greatest challenge that",
    "start": "2813630",
    "end": "2823110"
  },
  {
    "text": "prevents the use of\nautonomous agents for concrete applications.",
    "start": "2823110",
    "end": "2830030"
  },
  {
    "text": "An actually, this is a problem\nthat is not only related to the iCub platform, but\nit is also a limiting factor",
    "start": "2830030",
    "end": "2840190"
  },
  {
    "text": "that the performance\nof the latest robotics platforms,\nsuch as the ones that",
    "start": "2840190",
    "end": "2847050"
  },
  {
    "text": "have been participating, for\nexample, to the DARPA robotics challenge.",
    "start": "2847050",
    "end": "2852980"
  },
  {
    "text": "Indeed, as you can\nsee here, robots",
    "start": "2852980",
    "end": "2860700"
  },
  {
    "text": "are still either highly\ntele-operated or complex",
    "start": "2860700",
    "end": "2868500"
  },
  {
    "text": "methods. To, for example, map the 3D\nstructure on the environment",
    "start": "2868500",
    "end": "2875520"
  },
  {
    "text": "and label it a priori must\nbe implemented in order to enable autonomous\nagents to act in very",
    "start": "2875520",
    "end": "2883080"
  },
  {
    "text": "controlled environments. So we decided to focus\non very simple settings",
    "start": "2883080",
    "end": "2893460"
  },
  {
    "text": "where, in principle,\ncomputer vision methods as the ones that\nI've been describing you",
    "start": "2893460",
    "end": "2900030"
  },
  {
    "text": "should be at least-- well, should provide\nvery good performances.",
    "start": "2900030",
    "end": "2905550"
  },
  {
    "text": "Because here the setting\nis pretty simple. And we tried to\nevaluate the performance",
    "start": "2905550",
    "end": "2911309"
  },
  {
    "text": "of these deep learning\nmethods in these settings. Here you can see\nthe robot, that one,",
    "start": "2911310",
    "end": "2917260"
  },
  {
    "text": "standing in front of a table. There is a human which\ngives verbal instruction",
    "start": "2917260",
    "end": "2923610"
  },
  {
    "text": "to the robot and also,\nfor example in this case, the label of the object to be\neither learned or recognized.",
    "start": "2923610",
    "end": "2934470"
  },
  {
    "text": "And the robot can focus his\nattention on potential objects",
    "start": "2934470",
    "end": "2940830"
  },
  {
    "text": "through bottom up segmentation\ntechniques-- for example, in this case, color or the other\nsaliency-based segmentation",
    "start": "2940830",
    "end": "2948829"
  },
  {
    "text": "methods. I'm not going into the\ndetail of this setting, because you would see a\ndemo after my talk of this.",
    "start": "2948830",
    "end": "2956760"
  },
  {
    "text": "Another setting that\nwe are considering is similar to the previous one.",
    "start": "2956760",
    "end": "2962130"
  },
  {
    "text": "But this time, there is a human\nstanding in front of the robot. And there is no table.",
    "start": "2962130",
    "end": "2967380"
  },
  {
    "text": "And the human, he's holding\nthe objects in his hands and is showing one\nobject after the other",
    "start": "2967380",
    "end": "2975150"
  },
  {
    "text": "to the robot providing\nthe verbal annotation for that object.",
    "start": "2975150",
    "end": "2980800"
  },
  {
    "text": "The robot in this\nway, for example here, can exploit motion\ndetection techniques",
    "start": "2980800",
    "end": "2989250"
  },
  {
    "text": "in order to localize the\nobject in the visual field and focus on it.",
    "start": "2989250",
    "end": "2995130"
  },
  {
    "text": "The robot tracks the\nobject continuously, acquiring in this way\ncropped the frames",
    "start": "2995130",
    "end": "3002270"
  },
  {
    "text": "around the object that are\nthe training examples that will be used to learn\nthe object's appearance.",
    "start": "3002270",
    "end": "3010470"
  },
  {
    "text": "So in general, this is\nthe recognition pipeline",
    "start": "3010470",
    "end": "3017060"
  },
  {
    "text": "that is implemented to\nperform both the two behaviors",
    "start": "3017060",
    "end": "3022610"
  },
  {
    "text": "that I've been showing you. As you can see, the\ninput is the image, the stream of images from\none of the two cameras.",
    "start": "3022610",
    "end": "3031220"
  },
  {
    "text": "Then there is the verbal\nsupervision of the teacher. Then there are\nsegmentation techniques",
    "start": "3031220",
    "end": "3038030"
  },
  {
    "text": "in order to crop\nregion of interest from the incoming frame\nand feed this crop",
    "start": "3038030",
    "end": "3046550"
  },
  {
    "text": "to a convolutional\nneural network. In this case, we are using\nthe famous Krizhevsky model.",
    "start": "3046550",
    "end": "3054619"
  },
  {
    "text": "Then we encode\neach incoming crop in a vector as the output\nof one of the latest",
    "start": "3054620",
    "end": "3061940"
  },
  {
    "text": "layers of the network. And we feed all these vectors\nto a linear classifier,",
    "start": "3061940",
    "end": "3067485"
  },
  {
    "text": "which is linear because, in\nprinciple, the representation that we are extracting is good\nenough for the discrimination",
    "start": "3067485",
    "end": "3074360"
  },
  {
    "text": "that we want to perform. And so the classifier uses\nthese incoming vectors either as examples for\nthe training sector",
    "start": "3074360",
    "end": "3084800"
  },
  {
    "text": "or assigns to each vector\nthe prediction label.",
    "start": "3084800",
    "end": "3093950"
  },
  {
    "text": "And the output is an histogram\nwith the probabilities of all the classes.",
    "start": "3093950",
    "end": "3100370"
  },
  {
    "text": "And the final outcome is the one\nwith the highest probability.",
    "start": "3100370",
    "end": "3105890"
  },
  {
    "text": "And the histogram is\nupdated in real time. So this pipeline\ncan be used either",
    "start": "3105890",
    "end": "3112280"
  },
  {
    "text": "for one or the other settings\nthat have been described you.",
    "start": "3112280",
    "end": "3118280"
  },
  {
    "text": "So in particular, we\nstarted from trying",
    "start": "3118280",
    "end": "3125330"
  },
  {
    "text": "to list some requirements\nthat according to us",
    "start": "3125330",
    "end": "3133490"
  },
  {
    "text": "are fundamental in\norder to implement a sort of ideal robotic\nvisual recognition system.",
    "start": "3133490",
    "end": "3142730"
  },
  {
    "text": "And these requirements\nare usually not considered",
    "start": "3142730",
    "end": "3148430"
  },
  {
    "text": "by typical computer vision\nmethods as the ones that have been described you,\nbut are the same fundamental",
    "start": "3148430",
    "end": "3156859"
  },
  {
    "text": "if we want to\nachieve human level performances in the settings\nthat I've been showing you.",
    "start": "3156860",
    "end": "3164000"
  },
  {
    "text": "For example, first\nof all, the system should be, as you\nhave seen, as much as possible self-supervised,\nmeaning that there must",
    "start": "3164000",
    "end": "3171250"
  },
  {
    "text": "be techniques in order\nto focus that robot's attention on the\nobject of interest and isolate them from\nthe visual field.",
    "start": "3171250",
    "end": "3179140"
  },
  {
    "text": "Then hopefully, we\nwould like to come out with a system that is\nreliable and robust",
    "start": "3179140",
    "end": "3184210"
  },
  {
    "text": "to the variations\nin the environment and also in the\nobject's appearance.",
    "start": "3184210",
    "end": "3190829"
  },
  {
    "text": "Then also, as we are\nin the real world, we would like a\nsystem able to exploit",
    "start": "3190830",
    "end": "3198309"
  },
  {
    "text": "the contextual information\nthat is available. For example-- the fact\nthat we are actually dealing with videos.",
    "start": "3198310",
    "end": "3204190"
  },
  {
    "text": "So the frames are\ntemporarily correlated. And we are not dealing\nwith images in the wild,",
    "start": "3204190",
    "end": "3209740"
  },
  {
    "text": "as the ImageNet case. And finally, as\nRaffaello was mentioning, we would like to\nhave a system that",
    "start": "3209740",
    "end": "3215710"
  },
  {
    "text": "is able to learn\nincrementally to build always richer models of the\nobject through time.",
    "start": "3215710",
    "end": "3223310"
  },
  {
    "text": "So we decided to evaluate\nthis recognition pipeline according to the criteria\nthat have been described you.",
    "start": "3223310",
    "end": "3231059"
  },
  {
    "text": " And in order to provide\nreproducibility to our study,",
    "start": "3231060",
    "end": "3240290"
  },
  {
    "text": "we decided to acquire\na dataset on which to perform our analysis.",
    "start": "3240290",
    "end": "3246080"
  },
  {
    "text": "However, we would like\nalso to be confident enough that the result that we\nobtain on our benchmark",
    "start": "3246080",
    "end": "3253960"
  },
  {
    "text": "will hold also in the\nreal usage of our system. And this is the\nreason why we decided",
    "start": "3253960",
    "end": "3260049"
  },
  {
    "text": "to acquire our dataset in the\nsame application setting where the robot usually operates.",
    "start": "3260050",
    "end": "3267190"
  },
  {
    "text": "So this is the\niCubWork28 dataset that I acquired last year.",
    "start": "3267190",
    "end": "3272780"
  },
  {
    "text": "As you can see, it's\ncomposed by 28 objects divided into seven\ncategories and four",
    "start": "3272780",
    "end": "3278970"
  },
  {
    "text": "instances per category. And I acquired it for\nfour different days in order to test also\nincremental learning",
    "start": "3278970",
    "end": "3286720"
  },
  {
    "text": "capabilities. The dataset is available\non the IIT website.",
    "start": "3286720",
    "end": "3292810"
  },
  {
    "text": "And you can also\nuse it, for example, for the project of trust\nfive if you are interested.",
    "start": "3292810",
    "end": "3298547"
  },
  {
    "text": " And this is an example\nof the kind of videos",
    "start": "3298547",
    "end": "3303820"
  },
  {
    "text": "that I acquired considering\none of the 28 objects.",
    "start": "3303820",
    "end": "3309190"
  },
  {
    "text": "There are four videos for\nthe train, four for the test, acquired in four\ndifferent conditions.",
    "start": "3309190",
    "end": "3316960"
  },
  {
    "text": "The object is undergoing random\ntransformations, mainly limited to 3D rotations.",
    "start": "3316960",
    "end": "3323290"
  },
  {
    "text": "And as you can see, the\ndifference between the days is mainly limited to the\nfact that we are just",
    "start": "3323290",
    "end": "3330130"
  },
  {
    "text": "changing the conditions\nin the environment-- for example, the background\nor the lighting conditions.",
    "start": "3330130",
    "end": "3339700"
  },
  {
    "text": "And we acquired eight videos\nfor each of the 28 objects that I show you.",
    "start": "3339700",
    "end": "3345520"
  },
  {
    "text": "So first of all, we\ntried to find a measure, as I was saying\nbefore, to quantify",
    "start": "3345520",
    "end": "3352960"
  },
  {
    "text": "the confidence with\nwhich we can expect that the results and the\nperformance that we observe",
    "start": "3352960",
    "end": "3359589"
  },
  {
    "text": "on these benchmarks\nwill also hold in the real usage of the system. And to do this, first\nof all, we focused only",
    "start": "3359590",
    "end": "3368470"
  },
  {
    "text": "on object identification\nfor the moment. So the task is to discriminate\nthe specific instances",
    "start": "3368470",
    "end": "3374650"
  },
  {
    "text": "of objects among the pool of 28. And we decided to estimate for\nan increasing number of objects",
    "start": "3374650",
    "end": "3388240"
  },
  {
    "text": "to be discriminated from 2 to\n28 the empirical probability",
    "start": "3388240",
    "end": "3394630"
  },
  {
    "text": "distribution of the\nidentification accuracy",
    "start": "3394630",
    "end": "3400869"
  },
  {
    "text": "that we can observe\nstatistically for a fixed number of objects.",
    "start": "3400870",
    "end": "3406510"
  },
  {
    "text": "That is depicted here in\nthe form of box plots. ",
    "start": "3406510",
    "end": "3411740"
  },
  {
    "text": "And also, we estimated for\neach fixed number of objects to be discriminated\nthe minimum accuracy",
    "start": "3411740",
    "end": "3420800"
  },
  {
    "text": "that we can expect to achieve\nwith increasing confidence levels.",
    "start": "3420800",
    "end": "3426920"
  },
  {
    "text": "And this is a sort\nof data sheet. The idea is to provide an\nidea to an hypothetical user",
    "start": "3426920",
    "end": "3435079"
  },
  {
    "text": "of the robot of the\nidentification accuracy that can be expected given\na certain pool of objects",
    "start": "3435080",
    "end": "3442940"
  },
  {
    "text": "to be discriminated. So the second point that\nI'll briefly describe you",
    "start": "3442940",
    "end": "3448109"
  },
  {
    "text": "is the fact that we\ninvestigated the effect of having a more or less precise\nsegmentation in the image.",
    "start": "3448110",
    "end": "3458190"
  },
  {
    "text": "So we evaluated the task\nof identifying the 28 objects with different\nlevels of segmentation",
    "start": "3458190",
    "end": "3467089"
  },
  {
    "text": "starting from the whole image\nup to a very precise amount of segmentation of the objects. It can be seen\nthat, indeed, even",
    "start": "3467090",
    "end": "3476000"
  },
  {
    "text": "if in principle these\nconvolutional networks are trained to classify\nobjects in the world image",
    "start": "3476000",
    "end": "3483230"
  },
  {
    "text": "as it is in the\nImageNet dataset, it is still true\nthat in our case",
    "start": "3483230",
    "end": "3488370"
  },
  {
    "text": "we observed that there is still\na large benefit from having a fine-grained segmentation.",
    "start": "3488370",
    "end": "3495950"
  },
  {
    "text": "So probably the network is\nnot able to completely discard the new relevant information\nthat is in the background.",
    "start": "3495950",
    "end": "3503220"
  },
  {
    "text": "So this is a possible\ninteresting direction of research. And finally, the last point\nthat I decided to tell you--",
    "start": "3503220",
    "end": "3514460"
  },
  {
    "text": "I will skip on the\nincremental part, because it's an ongoing work\nthat I'm doing with Raffaello--",
    "start": "3514460",
    "end": "3520670"
  },
  {
    "text": "is about the exploitation\nof the temporal contextual information.",
    "start": "3520670",
    "end": "3526850"
  },
  {
    "text": "Here, you can see\nthe same kind of plot that I showed you before.",
    "start": "3526850",
    "end": "3533640"
  },
  {
    "text": "So the task is object\nidentification, increasing number of objects. And the dot black line\nrepresent the accuracy",
    "start": "3533640",
    "end": "3542660"
  },
  {
    "text": "that you obtain if\nyou consider, as you were asking before, the\nclassification of each",
    "start": "3542660",
    "end": "3549800"
  },
  {
    "text": "frame independently. So you can see that in this\ncase the accuracy that you get",
    "start": "3549800",
    "end": "3554870"
  },
  {
    "text": "is pretty low, considering\nthat we have to discriminate between only 28 objects.",
    "start": "3554870",
    "end": "3561390"
  },
  {
    "text": "However, it is also\ntrue that as soon as you start considering instead\nof the prediction given looking",
    "start": "3561390",
    "end": "3574100"
  },
  {
    "text": "only at the current frame,\nthe most frequent prediction",
    "start": "3574100",
    "end": "3579320"
  },
  {
    "text": "occurred in a temporal\nwindow, so in the previous, let's say, 50 frames.",
    "start": "3579320",
    "end": "3586190"
  },
  {
    "text": "You can boost your\ncondition accuracy a lot. As you can see here,\nfrom green to red,",
    "start": "3586190",
    "end": "3594049"
  },
  {
    "text": "increasing the length\nof the temporal window increases the recognition\naccuracy that you get.",
    "start": "3594050",
    "end": "3599480"
  },
  {
    "text": "This is a very simple approach. But it is showing that actually\nit is relevant in the fact",
    "start": "3599480",
    "end": "3605359"
  },
  {
    "text": "that you are actually dealing\nwith videos instead of images in the wild. And it is another\ndirection of research.",
    "start": "3605360",
    "end": "3613460"
  },
  {
    "text": "So finally, in the\nlast part of my talk, I would like to tell\nyou about the work",
    "start": "3613460",
    "end": "3618470"
  },
  {
    "text": "that I'm actually doing\nnow, which is concerning",
    "start": "3618470",
    "end": "3624050"
  },
  {
    "text": "about most object\ncategorization tasks instead of identification.",
    "start": "3624050",
    "end": "3630859"
  },
  {
    "text": "And this is the\nreason why we decided to acquire a new\ndataset, which is",
    "start": "3630860",
    "end": "3637160"
  },
  {
    "text": "larger than the previous ones. Because it is composed not\nonly by more categories, but,",
    "start": "3637160",
    "end": "3649520"
  },
  {
    "text": "in particular, by more\ninstances per category in order to be able to perform\ncategorization experiments as I",
    "start": "3649520",
    "end": "3657530"
  },
  {
    "text": "told you. Here, you can see the categories\nwith which we are starting",
    "start": "3657530",
    "end": "3662660"
  },
  {
    "text": "are 28 divided into seven\nmacro categories, let's say.",
    "start": "3662660",
    "end": "3668849"
  },
  {
    "text": "But the idea of this dataset\nis to have a continuously expandable in time dataset.",
    "start": "3668850",
    "end": "3676310"
  },
  {
    "text": "So there is an\napplication that we used to acquire these datasets. And the idea is to perform\nperiodical acquisitions",
    "start": "3676310",
    "end": "3683660"
  },
  {
    "text": "in order to incrementally enrich\nthe knowledge of the robot",
    "start": "3683660",
    "end": "3689119"
  },
  {
    "text": "about the objects in the scene. ",
    "start": "3689120",
    "end": "3694609"
  },
  {
    "text": "Also, another important\nfactor regarding this dataset is that differently\nfrom the previous one",
    "start": "3694610",
    "end": "3702290"
  },
  {
    "text": "this would be divided and\ntagged by nuisance factors. And in particular,\nfor each object,",
    "start": "3702290",
    "end": "3710029"
  },
  {
    "text": "we are acquiring\ndifferent videos where we isolate the\ndifferent transformations",
    "start": "3710030",
    "end": "3717319"
  },
  {
    "text": "that the object is undergoing. So we have a video\nwhere the object is just",
    "start": "3717320",
    "end": "3722349"
  },
  {
    "text": "at different scales. Then it is rotating\non the plane. Outside the plane,\nit is translating.",
    "start": "3722350",
    "end": "3728709"
  },
  {
    "text": "And then there is\na final video where all of this transformation\noccurs simultaneously.",
    "start": "3728709",
    "end": "3734490"
  },
  {
    "text": "And finally, to\nacquire this dataset we decided to use the depth\ninformation, so that in the end",
    "start": "3734490",
    "end": "3744290"
  },
  {
    "text": "we acquired both the left\nand the right to cameras. And in principle,\nthis information",
    "start": "3744290",
    "end": "3749390"
  },
  {
    "text": "could be used to obtain the\n3D structure of the objects. And this is the idea\nthat we used in order",
    "start": "3749390",
    "end": "3760849"
  },
  {
    "text": "to make the robot focusing\non the object of interest using disparity.",
    "start": "3760850",
    "end": "3767080"
  },
  {
    "text": "Disparity is very\nuseful in this case, because it allows to detect\nunknown objects just given",
    "start": "3767080",
    "end": "3777410"
  },
  {
    "text": "the fact that we know that\nwe want the robot focused",
    "start": "3777410",
    "end": "3782720"
  },
  {
    "text": "on the closest\nobjects in the scene. So it is a very\npowerful method in order",
    "start": "3782720",
    "end": "3789470"
  },
  {
    "text": "to have the robot\ntracking a known object with all different\nlighting conditions and so on.",
    "start": "3789470",
    "end": "3796720"
  },
  {
    "text": "Yeah. And here, you can see\nthis is the left camera.",
    "start": "3796720",
    "end": "3802070"
  },
  {
    "text": "This is the disparity map. This is its segmentation, which\nprovides an approximate region",
    "start": "3802070",
    "end": "3807680"
  },
  {
    "text": "of interest around the object. And this is the final output. ",
    "start": "3807680",
    "end": "3814010"
  },
  {
    "text": "So I started\nacquiring the first-- well, it should be red, but\nit's not very clear I mean.",
    "start": "3814010",
    "end": "3821329"
  },
  {
    "text": "I started acquiring\nthe first categories among these 21\nlisted here, which",
    "start": "3821330",
    "end": "3828470"
  },
  {
    "text": "are the squeezer, sprayer,\nthe cream, the oven",
    "start": "3828470",
    "end": "3833570"
  },
  {
    "text": "glove, and the bottle. For each row, you see the tiny\ninstances that I collected.",
    "start": "3833570",
    "end": "3839119"
  },
  {
    "text": "And the idea is to\ncontinue acquiring them when I come back in Genoa. And so here, you can see an\nexample of the five videos.",
    "start": "3839120",
    "end": "3849720"
  },
  {
    "text": "Actually, I acquired\n10 videos per object, five for the training set\nand five for the test set.",
    "start": "3849720",
    "end": "3856400"
  },
  {
    "text": "And you can see that\nin the different videos the object is undergoing\ndifferent transformations.",
    "start": "3856400",
    "end": "3863030"
  },
  {
    "text": "And this is the final one while\nthese transformation are mixed.",
    "start": "3863030",
    "end": "3868090"
  },
  {
    "text": "Oh, the images here\nare not segmented yet. So you can see the whole image.",
    "start": "3868090",
    "end": "3873920"
  },
  {
    "text": "But in end, also the information\nabout the segmentation and disparity and so\non will be available.",
    "start": "3873920",
    "end": "3881280"
  },
  {
    "text": "And this dataset\nregarding the 50 object that I acquired together\nwith the application",
    "start": "3881280",
    "end": "3887779"
  },
  {
    "text": "that I'm using to\nacquire the dataset are available if you\nare willing to use them for the projects in trust\nfive, for example, in order",
    "start": "3887780",
    "end": "3895730"
  },
  {
    "text": "to investigate the\ninvariant properties of the different\nrepresentations.",
    "start": "3895730",
    "end": "3901170"
  },
  {
    "text": "And so that's it. ",
    "start": "3901170",
    "end": "3935270"
  }
]