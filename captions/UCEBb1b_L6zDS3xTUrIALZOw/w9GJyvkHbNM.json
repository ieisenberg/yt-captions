[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5340"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5340",
    "end": "11640"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11640",
    "end": "18110"
  },
  {
    "text": "at ocw.mit.edu. ",
    "start": "18110",
    "end": "23390"
  },
  {
    "text": "JAMES W. SWAN: So this is\ngoing to be our last lecture on linear algebra. The first three\nlectures covered basics.",
    "start": "23390",
    "end": "30417"
  },
  {
    "text": "The next three lectures, we\ntalked about different sorts of transformations of matrices. This final lecture is\nthe last of those three.",
    "start": "30417",
    "end": "36742"
  },
  {
    "text": "We're going to talk about in\nanother sort of transformation called the singular\nvalue decomposition.",
    "start": "36742",
    "end": "42820"
  },
  {
    "text": "OK, before we jump in, I'd like\nto do the usual recap business. I think it's always hopeful\nto recap or look at things",
    "start": "42820",
    "end": "49010"
  },
  {
    "text": "from a different perspective. Early on, I told you that the\ninfinite dimensional equivalent",
    "start": "49010",
    "end": "55129"
  },
  {
    "text": "of vectors would be something\nlike a function, which is a map, a unique map maybe\nfrom a point to x to some value",
    "start": "55130",
    "end": "62510"
  },
  {
    "text": "f of x. And there is an\nequivalent representation of the eigenvalue eigenvector\nproblem in function space.",
    "start": "62510",
    "end": "68780"
  },
  {
    "text": "We call these eigenvalues\nand eigenfunctions. Here's a classic one where\nthe function is y of x, OK?",
    "start": "68780",
    "end": "75074"
  },
  {
    "text": "This is the equivalent of\nthe vector, and equivalent of the transformation\nor the matrix that's this differential\noperator this time,",
    "start": "75074",
    "end": "81229"
  },
  {
    "text": "the second derivative. So I take the second derivative\nof this particular function,",
    "start": "81230",
    "end": "86340"
  },
  {
    "text": "and the function is stretched. It's multiplied by some\nfixed value at all points. And it becomes lambda times y.",
    "start": "86340",
    "end": "93830"
  },
  {
    "text": "And that operator has to be\nclosed with some boundary conditions as well. We have to say\nwhat the value of y",
    "start": "93830",
    "end": "99830"
  },
  {
    "text": "is at the edges\nof some boundary. So there's a one-to-one\ncorrespondence",
    "start": "99830",
    "end": "105590"
  },
  {
    "text": "between these things. What is the eigenfunction here,\nor what are the eigenfunctions?",
    "start": "105590",
    "end": "114730"
  },
  {
    "text": "And what are the\neigenvalues associated with this transformation\nor this operator? Can you work those\nout really quickly?",
    "start": "114730",
    "end": "121667"
  },
  {
    "text": "You learned this at\nsome point, right? Somebody taught you\ndifferential equations and you calculated these things.",
    "start": "121667",
    "end": "127360"
  },
  {
    "text": "Take about 90 seconds. Work with the people around you. See if you can come to\na conclusion about what the eigenfunction\nand eigenvalues are.",
    "start": "127360",
    "end": "134980"
  },
  {
    "start": "134980",
    "end": "146698"
  },
  {
    "text": "That's enough time. You can work on this\non your own later if you've run out of time. Don't worry about it. Does somebody want\nto volunteer a guess",
    "start": "146699",
    "end": "152599"
  },
  {
    "text": "for what the eigenfunctions\nare in this case? What are they?",
    "start": "152600",
    "end": "158349"
  },
  {
    "text": "Yeah? AUDIENCE: [INAUDIBLE] JAMES W. SWAN: OK, so\nyou chose exponentials.",
    "start": "158350",
    "end": "164719"
  },
  {
    "text": "That's an interesting choice. That's one possible\nchoice you can make. OK, so we could say-- this is sort of a\nclassical one that you",
    "start": "164720",
    "end": "170660"
  },
  {
    "text": "think about when you first\nlearn differential equation. They say, an\nequation of this sort",
    "start": "170660",
    "end": "176690"
  },
  {
    "text": "has solutions that look like\nexponentials, and that's true. There's another\nrepresentation for this,",
    "start": "176690",
    "end": "183210"
  },
  {
    "text": "which is as trigonometric\nfunctions instead, right? ",
    "start": "183210",
    "end": "189110"
  },
  {
    "text": "Either of those is acceptable. [INAUDIBLE] the\ntrigonometric functions, that representation is a\nlittle more useful for us here.",
    "start": "189110",
    "end": "197417"
  },
  {
    "text": "We know that the boundary\nconditions tell us that y of 0 is supposed to be 0.",
    "start": "197417",
    "end": "202430"
  },
  {
    "text": "That means that the C1 has to\nbe 0, because cosine of 0 is 1. So C1 has 0 in this case.",
    "start": "202430",
    "end": "209550"
  },
  {
    "text": "So that fixes one of\nthese coefficients. And now we're left\nwith a problem, right?",
    "start": "209550",
    "end": "215730"
  },
  {
    "text": "Our solutions, our\neigenfunctions, cannot be unique. So we don't get to\nspecify C2, right?",
    "start": "215730",
    "end": "221760"
  },
  {
    "text": "Any function that's a\nmultiple of this sine should also be an eigenfunction. So instead the other\nboundary condition,",
    "start": "221760",
    "end": "227640"
  },
  {
    "text": "this y of l equals 0, needs\nto be used to pin down with the eigenvalue is.",
    "start": "227640",
    "end": "232920"
  },
  {
    "text": "So the second\nequation, y of l equals 0, which implies that the\nsquare root of minus lambda",
    "start": "232920",
    "end": "239560"
  },
  {
    "text": "has to be equal\nto 2 pi over l, it has to be all the\nnodes of the sine where the sine is equal to 0.",
    "start": "239560",
    "end": "245439"
  },
  {
    "text": "That's the equivalent of\nour secular characteristic polynomial that prescribes with\nthe eigenvalues are associated",
    "start": "245440",
    "end": "250990"
  },
  {
    "text": "with each of the eigenfunctions. So now we know what\nthe eigenvalues are. The eigenvalues are\nthe set of numbers",
    "start": "250990",
    "end": "257320"
  },
  {
    "text": "minus 2 pi n over l squared. There's an infinite\nnumber of eigenvalues.",
    "start": "257320",
    "end": "263490"
  },
  {
    "text": "It's an infinite dimensional\nspace that we're in, so it's not a big surprise\nthat it works out that way.",
    "start": "263490",
    "end": "269230"
  },
  {
    "text": "And the eigenvectors then are\ndifferent scalar multiples of sine of the eigenvalues,\nsquare root of the eigenvalues,",
    "start": "269230",
    "end": "276220"
  },
  {
    "text": "minus x. There's a one-to-one\ncorrespondence between all the linear\nalgebra we've done and linear\ndifferential equations",
    "start": "276220",
    "end": "282669"
  },
  {
    "text": "or linear partial\ndifferential equations. You can think about these\nthings in exactly the same way. I'm sure in 1050, you started to\ntalk about orthogonal functions",
    "start": "282670",
    "end": "293080"
  },
  {
    "text": "to represent solutions of\ndifferential equations. Or if you haven't, you're\ngoing to very soon. This is a part of\nthe course you get",
    "start": "293080",
    "end": "298690"
  },
  {
    "text": "to look at the analytical side\nof some of these things as opposed to the numerical side. But there's a\none-to-one relationship",
    "start": "298690",
    "end": "303771"
  },
  {
    "text": "between those things. So if you understand one,\nyou understand the other, and you can come at them\nfrom either perspective.",
    "start": "303771",
    "end": "310770"
  },
  {
    "text": "This sort of stuff is useful. Actually, the classical\nchemical engineering example comes from quantum\nmechanics where",
    "start": "310770",
    "end": "315920"
  },
  {
    "text": "you think about wave functions\nand different energy levels corresponding to eigenvalues.",
    "start": "315920",
    "end": "321870"
  },
  {
    "text": "That's cool. Sometimes, I like to think\nabout a mechanical analog to that, which is the\nbuckling of an elastic column.",
    "start": "321870",
    "end": "328419"
  },
  {
    "text": "So you should do this at home. You should go get a\npiece of spaghetti and push on the ends of\nthe piece of the spaghetti.",
    "start": "328420",
    "end": "334620"
  },
  {
    "text": "And the spaghetti will buckle. Eventually it'll break,\nbut it'll buckle first. It'll bend. And how does it bend?",
    "start": "334620",
    "end": "341069"
  },
  {
    "text": "Well, a balance of linear\nmomentum on this bar would tell you\nthat the deflection",
    "start": "341070",
    "end": "348780"
  },
  {
    "text": "in the bar at different\npoints x along the bar multiplied by the pressure has\nto balance the bending moment",
    "start": "348780",
    "end": "357600"
  },
  {
    "text": "in the bar itself. So this e is some\nelastic constant. I has a moment of inertia.",
    "start": "357600",
    "end": "362639"
  },
  {
    "text": "And D squared y dx\nsquared is something like the curvature of the bar. So it's the bending\nmoments of the bar that balances the\npressure that's",
    "start": "362640",
    "end": "369510"
  },
  {
    "text": "being exerted on the bar. And sure enough,\nthis bar will buckle when the pressure\napplied exceeds",
    "start": "369510",
    "end": "376860"
  },
  {
    "text": "the first eigenvalue associated\nwith this differential equation. We just worked that\neigenvalue out.",
    "start": "376860",
    "end": "384389"
  },
  {
    "text": "We said that that eigenvalue\nhad to be the square root of 2",
    "start": "384390",
    "end": "389950"
  },
  {
    "text": "pi over l squared. And so when the pressure\nexceeds square root of 2 pi over l squared\ntimes the elastic modulus,",
    "start": "389950",
    "end": "396810"
  },
  {
    "text": "this column will bend\nand deform continuously until it eventually\nbreaks, right?",
    "start": "396810",
    "end": "401879"
  },
  {
    "text": "It will undergo this\nlinear elastic deformation, then plastic deformation\nlater, and it will break.",
    "start": "401880",
    "end": "408560"
  },
  {
    "text": "The Eiffel Tower,\nactually, is one of the first\nstructures in the world to utilize this\nprinciple, right?",
    "start": "408560",
    "end": "414270"
  },
  {
    "text": "It's got very\nnarrow beams in it. The beams are engineered so\nthat their elastic modulus",
    "start": "414270",
    "end": "419614"
  },
  {
    "text": "is strong enough that\nthey won't buckle. Gustave Eiffel is one of the\nfirst applied physicists, somebody who took the\nphysics of elastic bars",
    "start": "419614",
    "end": "427700"
  },
  {
    "text": "and applied them to\nbuilding structures that weren't big and blocky, but used\na minimal amount of material.",
    "start": "427700",
    "end": "434569"
  },
  {
    "text": "Cool, right? OK, so that's recap. ",
    "start": "434570",
    "end": "441580"
  },
  {
    "text": "Any questions about that? You've seen these things before. You understood them\nwell before too maybe?",
    "start": "441580",
    "end": "449530"
  },
  {
    "text": "Give some thought to this, OK? We talked about\neigendecomposition last time",
    "start": "449530",
    "end": "457090"
  },
  {
    "text": "that, associated with\nthe square matrix, was a particular eigenvalue or\nparticular set of eigenvalues,",
    "start": "457090",
    "end": "464050"
  },
  {
    "text": "stretches and corresponding\neigenvectors directions. These were special solutions to\nthe system of linear equations",
    "start": "464050",
    "end": "471490"
  },
  {
    "text": "based on a matrix. It was a square matrix. And you might ask,\nwell, what happens if the matrix isn't square?",
    "start": "471490",
    "end": "477850"
  },
  {
    "text": "What if A is in the space of\nreal matrices that are n by m,",
    "start": "477850",
    "end": "483010"
  },
  {
    "text": "where n and m maybe\naren't the same? Maybe they are the same,\nbut maybe they're not. And there is an\nequivalent decomposition.",
    "start": "483010",
    "end": "490046"
  },
  {
    "text": "It's called the singular\nvalue decomposition. It's like an eigendecomposition\nfor non-square matrices.",
    "start": "490046",
    "end": "496850"
  },
  {
    "text": "So rather than writing our\nmatrix as some w lambda w inverse, we're going to\nwrite it as some product",
    "start": "496850",
    "end": "504720"
  },
  {
    "text": "U times sigma times\nV with this dagger. The dagger here is\nconjugate transpose.",
    "start": "504720",
    "end": "511940"
  },
  {
    "text": "Transpose the matrix, and\ntake the complex conjugate of all the elements, OK? I mentioned last time that\neigenvalues and eigenvectors",
    "start": "511940",
    "end": "519630"
  },
  {
    "text": "could be complex,\npotentially, right? So whenever we have that case\nwhere things can be complex,",
    "start": "519630",
    "end": "525960"
  },
  {
    "text": "usually the\ntransposition operation is replaced with the\nconjugate transpose. ",
    "start": "525960",
    "end": "532224"
  },
  {
    "text": "What are these\ndifferent matrices. Well, let me tell you. U is a complex matrix.",
    "start": "532224",
    "end": "538140"
  },
  {
    "text": "It maps from the\nspace N to R N to R N, so it's an n by n square matrix.",
    "start": "538140",
    "end": "544350"
  },
  {
    "text": "Sigma is a real valued matrix,\nand it lives in the space of n by n matrices.",
    "start": "544350",
    "end": "550830"
  },
  {
    "text": "V is a square matrix again,\nbut it has dimensions m by m.",
    "start": "550830",
    "end": "556520"
  },
  {
    "text": "Remember, A maps\nfrom R M to R N, so that's what the\nsequence of products says.",
    "start": "556520",
    "end": "562110"
  },
  {
    "text": "B maps from m to m. Sigma maps from m to n.",
    "start": "562110",
    "end": "567320"
  },
  {
    "text": "U maps from n to n. So this match from\nm to n as well. ",
    "start": "567320",
    "end": "574060"
  },
  {
    "text": "Sigma is like\nlambda from before. It's a diagonal matrix. It only has diagonal elements.",
    "start": "574060",
    "end": "580040"
  },
  {
    "text": "It's just not\nsquare, but it only has diagonal elements, all\nof which will be positive.",
    "start": "580040",
    "end": "585540"
  },
  {
    "text": " And then U and V are called\nthe left and right singular",
    "start": "585540",
    "end": "591730"
  },
  {
    "text": "vectors. And they have special\nproperties associated with them, which I'll show you right now. Any questions about\nhow this decomposition",
    "start": "591730",
    "end": "599600"
  },
  {
    "text": "is composed or made up? It looks just like the\neigendecomposition,",
    "start": "599600",
    "end": "604790"
  },
  {
    "text": "but it can be applied\nto any matrix. Yes? AUDIENCE: Quick question. JAMES W. SWAN: Sure. AUDIENCE: Do all\nmatrices have this thing,",
    "start": "604790",
    "end": "609920"
  },
  {
    "text": "or is it like the eigenvalues\nwhere some do and some don't. JAMES W. SWAN: This\nis a great question. So all matrices are going\nto have a singular value",
    "start": "609920",
    "end": "615253"
  },
  {
    "text": "decomposition. We saw with the\neigenvalue decomposition that there could be a case\nwhere the eigenvectors are",
    "start": "615253",
    "end": "622519"
  },
  {
    "text": "degenerate, and we can't\nwrite that full decomposition. All matrices are going to\nhave this decomposition.",
    "start": "622520",
    "end": "628214"
  },
  {
    "start": "628214",
    "end": "634380"
  },
  {
    "text": "So for some properties\nof this decomposition,",
    "start": "634380",
    "end": "641690"
  },
  {
    "text": "U and V are what we\ncall unitary matrices. I talked about these before. Unitary matrices are ones for\nwhom, if they're real valued,",
    "start": "641690",
    "end": "649430"
  },
  {
    "text": "their transpose is\nalso their inverse. If they're complex\nvalued, and they're",
    "start": "649430",
    "end": "654490"
  },
  {
    "text": "conjugate transpose is the\nequivalent of their inverse. So U times U conjugate\ntranspose will be identity.",
    "start": "654490",
    "end": "661930"
  },
  {
    "text": "V times V conjugate\ntranspose will be identity. Unitary matrices also\nhave the property",
    "start": "661930",
    "end": "668510"
  },
  {
    "text": "that they impart no\nstretch to a matrix-- or to vectors.",
    "start": "668510",
    "end": "673820"
  },
  {
    "text": "So their maps don't stretch. They're kind of like\nrotational matrices, right?",
    "start": "673820",
    "end": "679104"
  },
  {
    "text": "They change directions, but\nthey don't stretch things out. ",
    "start": "679104",
    "end": "685190"
  },
  {
    "text": "If I were to take A conjugate\ntranspose and multiply it by A,",
    "start": "685190",
    "end": "690890"
  },
  {
    "text": "that would be the same as taking\nU sigma V conjugate transpose,",
    "start": "690890",
    "end": "696680"
  },
  {
    "text": "and multiplying it\nby U sigma V. If I use the properties of\nmatrix multiplications",
    "start": "696680",
    "end": "703010"
  },
  {
    "text": "and complex\nconjugate transposes, and work out what\nthis expression is,",
    "start": "703010",
    "end": "708500"
  },
  {
    "text": "I'll find out that it's\nequivalent to V sigma conjugate transpose sigma V\nconjugate transpose.",
    "start": "708500",
    "end": "716660"
  },
  {
    "text": "Well this has exactly the same\nform as an eigendecomposition.",
    "start": "716660",
    "end": "723050"
  },
  {
    "text": "An eigendecomposition\nof A times A instead of an eigendecomposition\nof A. So V",
    "start": "723050",
    "end": "731600"
  },
  {
    "text": "is the set of eigenvectors\nof A conjugate transpose A, and sigma squared\nare the eigenvalues",
    "start": "731600",
    "end": "739699"
  },
  {
    "text": "of A conjugate\ntranspose times A. And if I reverse the order\nof this multiplication--",
    "start": "739700",
    "end": "746610"
  },
  {
    "text": "so I do A times A conjugate\ntranspose-- and work it out, that would be U sigma\nsigma U. And so U",
    "start": "746610",
    "end": "754010"
  },
  {
    "text": "are the eigenvectors of\nA A conjugate transpose, and sigma squared are still the\neigenvalues of A A conjugate",
    "start": "754010",
    "end": "762320"
  },
  {
    "text": "transpose. So what are these\nthings U and V?",
    "start": "762320",
    "end": "767830"
  },
  {
    "text": "They relate to the\neigenvectors of the product of A with itself, this\nparticular product of A",
    "start": "767830",
    "end": "774550"
  },
  {
    "text": "with itself, or this particular\nproduct of A with itself. Sigma are the singular values.",
    "start": "774550",
    "end": "781310"
  },
  {
    "text": "And all matrices possess\nthis sort of a decomposition. They all have a set of singular\nvalues and singular vectors.",
    "start": "781310",
    "end": "786530"
  },
  {
    "text": "These sigmas are called the\nsingular values of the A. They have a particular name. I'm going to show you how you\ncan use this decomposition",
    "start": "786530",
    "end": "792110"
  },
  {
    "text": "to do something you\nalready know how to do, but how to do it formally. ",
    "start": "792110",
    "end": "798200"
  },
  {
    "text": "What are some properties of the\nsingular value decomposition? ",
    "start": "798200",
    "end": "804810"
  },
  {
    "text": "So if we take a matrix A and\nwe compute it's singular value decomposition, this is\nhow you do it in Matlab. ",
    "start": "804810",
    "end": "813029"
  },
  {
    "text": "We'll find out, for this\nmatrix, U is identity. Sigma is identity with an\nextra column pasted on it.",
    "start": "813030",
    "end": "819720"
  },
  {
    "text": "And B is also identity. I mean, this is the simplest\npossible four by three matrix I can write down.",
    "start": "819720",
    "end": "825517"
  },
  {
    "text": "You don't have to know how\nto compute the singular value decomposition, you just\nneed to know that it can be computed in this way.",
    "start": "825517",
    "end": "831420"
  },
  {
    "text": "You might be able to\nguess how to compute it based on what we did\nwith eigenvalues earlier and eigenvectors.",
    "start": "831420",
    "end": "837240"
  },
  {
    "text": "It'll turn out some of\nthe columns of sigma will be non-zero right? There are three non-zero\ncolumns of sigma.",
    "start": "837240",
    "end": "844390"
  },
  {
    "text": "And the columns of\nV, they correspond to those columns of sigma,\nspanned the null space",
    "start": "844390",
    "end": "851800"
  },
  {
    "text": "of the matrix A. So the first three\ncolumns here are non-zero,",
    "start": "851800",
    "end": "860220"
  },
  {
    "text": "the first three columns\nof V. I'm sorry, the first three columns\nhere are non-zero.",
    "start": "860220",
    "end": "865970"
  },
  {
    "text": "The last column is 0. The columns of sigma\nwhich are 0 correspond to a particular column in V,\nthis last column here, which",
    "start": "865970",
    "end": "873920"
  },
  {
    "text": "lives in the null\nspace of A. So you can see, if I take\nA and I multiply it by any vector that's\nproportional to 0, 0, 0, 1,",
    "start": "873920",
    "end": "881970"
  },
  {
    "text": "I'll get back 0. So the null space\nof A is spanned by all these vectors\ncorresponding",
    "start": "881970",
    "end": "888260"
  },
  {
    "text": "to the 0 columns of sigma.  Some of the columns\nof sigma are non-zero.",
    "start": "888260",
    "end": "894807"
  },
  {
    "text": "These first three columns. And the rows of U corresponding\nto those three columns",
    "start": "894807",
    "end": "902000"
  },
  {
    "text": "span the range of A. So\nif I do the singular value decomposition of a matrix,\nand I look at U, V, and sigma",
    "start": "902000",
    "end": "909410"
  },
  {
    "text": "and what they're composed of-- where sigma is 0 and non-zero,\nand the corresponding columns",
    "start": "909410",
    "end": "915050"
  },
  {
    "text": "or rows of U and V--\nthen I can figure out what vectors span the range\nand null space of the matrix A.",
    "start": "915050",
    "end": "925110"
  },
  {
    "text": "Here's another example. So here I have A.\nNow instead of being three rows by four columns,\nit's four rows by three columns.",
    "start": "925110",
    "end": "932850"
  },
  {
    "text": "And here's the singular\nvalue decomposition that comes out of Matlab. There are no vectors that\nlive in the null space of A,",
    "start": "932850",
    "end": "941580"
  },
  {
    "text": "and there are no 0\ncolumns in sigma. There's no corresponding\ncolumns in V.",
    "start": "941580",
    "end": "946975"
  },
  {
    "text": "There are no vectors\nin the null space of A. The range of A is spanned\nby the rows corresponding",
    "start": "946975",
    "end": "955480"
  },
  {
    "text": "to the non-zero-- the\nrows of U corresponding to the non-zero\ncolumns of sigma. So it's these three columns\nin the first three rows.",
    "start": "955480",
    "end": "962890"
  },
  {
    "text": "And these first three\nrows, clearly they span-- they describe the same range\nas the three columns in A.",
    "start": "962890",
    "end": "972040"
  },
  {
    "text": "So the singular\nvalue decomposition gives us direct access\nto the null space",
    "start": "972040",
    "end": "977290"
  },
  {
    "text": "and the range of a matrix. That's handy. And it can be used\nin various ways.",
    "start": "977290",
    "end": "984170"
  },
  {
    "text": "So here's one example\nwhere it can be used. Here I have a fingerprint.",
    "start": "984170",
    "end": "989830"
  },
  {
    "text": "It's a bitmap. It's a square bit of\ndata, like a matrix, and each of the\nelements of the matrix",
    "start": "989830",
    "end": "995350"
  },
  {
    "text": "takes on a value describing\nhow dark or light that pixel. Let's say it's grayscale, and\nit's value's between 0 and 255.",
    "start": "995350",
    "end": "1003839"
  },
  {
    "text": "That's pretty typical. So I have this matrix, and\neach element to the matrix corresponds to a pixel.",
    "start": "1003840",
    "end": "1010410"
  },
  {
    "text": "And I do a singular\nvalue decomposition. Some of the singular\nvalues, the values of sigma,",
    "start": "1010410",
    "end": "1015750"
  },
  {
    "text": "are bigger than others. They're all positive, but\nsome are bigger than others.",
    "start": "1015750",
    "end": "1020770"
  },
  {
    "text": "The ones that are\nbiggest in magnitude carry the most information\ncontent about the matrix.",
    "start": "1020770",
    "end": "1026770"
  },
  {
    "text": "So we can do data compression by\nneglecting singular values that are smaller than some\nthreshold, and also neglecting",
    "start": "1026770",
    "end": "1034980"
  },
  {
    "text": "the corresponding\nsingular vectors. And that's what I've done here. So here's the original\nbitmap of the fingerprint.",
    "start": "1034980",
    "end": "1041400"
  },
  {
    "text": "I did the singular\nvalue decomposition, and then I retained only the 50\nbiggest singular values and I",
    "start": "1041400",
    "end": "1047819"
  },
  {
    "text": "left all the other\nsingular values out. This bitmap was something\nlike, I don't know, 300 pixels by 300\npixels, so there's",
    "start": "1047819",
    "end": "1054450"
  },
  {
    "text": "like 300 singular\nvalues, but I got rid of 5/6 of the\ninformation content.",
    "start": "1054450",
    "end": "1060480"
  },
  {
    "text": "I dropped 5/6 of the\nsingular vectors, and then I\nreconstructed the matrix from the singular values\nand those singular vectors,",
    "start": "1060480",
    "end": "1067320"
  },
  {
    "text": "and you get a faithful\nrepresentation of the original fingerprint. So the singular\nvalue decomposition",
    "start": "1067320",
    "end": "1072600"
  },
  {
    "text": "says something about\nthe information content in the transformation\nthat is the matrix, right? There are some\ntransformations that",
    "start": "1072600",
    "end": "1078720"
  },
  {
    "text": "are of lower power or\nimportance than others. And the magnitude\nof these singular",
    "start": "1078720",
    "end": "1084630"
  },
  {
    "text": "values tell you what they are. Does that makes sense?",
    "start": "1084630",
    "end": "1089680"
  },
  {
    "text": "How else can it be used? Well, one way it can be\nused is finding the least square solution\nto the equation Ax",
    "start": "1089680",
    "end": "1097820"
  },
  {
    "text": "equals b, where A is no\nlonger a square matrix, OK? ",
    "start": "1097820",
    "end": "1106410"
  },
  {
    "text": "You've done this\nin other contexts before where the equations\nare overspecified.",
    "start": "1106410",
    "end": "1112860"
  },
  {
    "text": "We have more equations than\nunknowns, like data fitting. You form the normal equations,\nyou multiply both sides of Ax",
    "start": "1112860",
    "end": "1119820"
  },
  {
    "text": "equals b by A transpose, and\nthen invert A transpose A.",
    "start": "1119820",
    "end": "1125909"
  },
  {
    "text": "You might not be\ntoo surprised, then, to think that singular value\ndecomposition could be useful here too.",
    "start": "1125910",
    "end": "1130920"
  },
  {
    "text": "Since we already saw the data in\na singular value decomposition corresponds to eigenvectors and\neigenvalues of this A transpose",
    "start": "1130920",
    "end": "1138630"
  },
  {
    "text": "A, right? But there's a way to use\nthis sort of decomposition formally to solve\nproblems that are",
    "start": "1138630",
    "end": "1145139"
  },
  {
    "text": "both overspecified\nand underspecified. Least squares means find\nthe vector of solutions",
    "start": "1145140",
    "end": "1154620"
  },
  {
    "text": "x that minimizes\nthis function phi.",
    "start": "1154620",
    "end": "1161860"
  },
  {
    "text": "Phi is the length of the\nvector given by the difference between Ax and b. It's one measure of how far\nan error our solution x is.",
    "start": "1161860",
    "end": "1169899"
  },
  {
    "text": "So let's define the value\nx which is least in error. This is one definition\nof least squares.",
    "start": "1169900",
    "end": "1176065"
  },
  {
    "text": " And I know the singular value\ndecomposition of A. So A",
    "start": "1176065",
    "end": "1183950"
  },
  {
    "text": "is U sigma times V. So I\nhave U sigma V times x. I can factor out U, and I've\ngot a factor of U transpose,",
    "start": "1183950",
    "end": "1192140"
  },
  {
    "text": "or U conjugate transpose\nmultiplying by b. So Ax minus b is the same as\nU times the quantity sigma V",
    "start": "1192140",
    "end": "1200000"
  },
  {
    "text": "conjugate transpose x minus\nU conjugate transpose b.",
    "start": "1200000",
    "end": "1205240"
  },
  {
    "text": "We want to know the x\nthat minimizes this phi. It's an optimization problem. We'll talk in great detail about\nthese sorts of problems later.",
    "start": "1205240",
    "end": "1212990"
  },
  {
    "text": "This one is so easy to do,\nwe can just work it out in a couple lines of text.",
    "start": "1212990",
    "end": "1218152"
  },
  {
    "text": "We'll define a new\nset of unknowns, y, which is V transpose times\nx, and a new right-hand side",
    "start": "1218152",
    "end": "1225130"
  },
  {
    "text": "for a system of equations p,\nwhich is U transpose times b. And then we can rewrite\nour function phi",
    "start": "1225130",
    "end": "1231220"
  },
  {
    "text": "that we're trying to minimize. So phi then becomes\nU sigma y minus p.",
    "start": "1231220",
    "end": "1237720"
  },
  {
    "text": "U is a unitary vector. It imparts no stretch in the two\nnorms, so this sigma y minus p",
    "start": "1237720",
    "end": "1245440"
  },
  {
    "text": "doesn't get elongated by\nmultiplication with U. So it's length,\nthe length of this,",
    "start": "1245440",
    "end": "1252280"
  },
  {
    "text": "is the same as the length\nof sigma y minus p. You can prove this. It's not very difficult\nto show at all.",
    "start": "1252280",
    "end": "1258540"
  },
  {
    "text": "You use the definition of\nthe two norm to prove it. So phi is minimized by y's,\nwhich makes this norm smallest,",
    "start": "1258540",
    "end": "1270150"
  },
  {
    "text": "make it closest to 0.  Let r be the number\nof non-zero singular",
    "start": "1270150",
    "end": "1277059"
  },
  {
    "text": "values, the number\nof those sigmas which are not equal to 0.",
    "start": "1277060",
    "end": "1282440"
  },
  {
    "text": "That's also the rank of A. Then I can rewrite\nphi as the sum from i",
    "start": "1282440",
    "end": "1289330"
  },
  {
    "text": "equals 1 to r of sigma i i\ntime y i minus p i squared.",
    "start": "1289330",
    "end": "1296059"
  },
  {
    "text": "That's parts of this length,\nthis Euclidean length, for which sigma is non-zero.",
    "start": "1296060",
    "end": "1302320"
  },
  {
    "text": "Plus the sum from r\nplus 1 to n, the sum over the rest of\nthe values of p,",
    "start": "1302320",
    "end": "1310240"
  },
  {
    "text": "for which the\ncorresponding sigmas are 0. ",
    "start": "1310240",
    "end": "1319710"
  },
  {
    "text": "I want to minimize\nphi, and the only thing that I can change to\nminimize it is what?",
    "start": "1319710",
    "end": "1325336"
  },
  {
    "text": " What am I free to\npick in this equation",
    "start": "1325337",
    "end": "1330530"
  },
  {
    "text": "in order to make phi\nas small as possible? Yeah? AUDIENCE: y. JAMES W. SWAN: y,\nso I need to choose",
    "start": "1330530",
    "end": "1337289"
  },
  {
    "text": "the y's that make this\nphi as small as possible. What value should I\nchoose for the y's?",
    "start": "1337290",
    "end": "1342955"
  },
  {
    "text": " What do you think? ",
    "start": "1342955",
    "end": "1350630"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] JAMES W. SWAN: Perfect, right? Choose y equals p\ni over sigma i i.",
    "start": "1350630",
    "end": "1360000"
  },
  {
    "text": "Right, y i is p\ni over sigma i i. Then all of these terms is 0.",
    "start": "1360000",
    "end": "1366150"
  },
  {
    "text": "I can't make this sum\nany smaller than that. That fixes the value\nof y i up to r.",
    "start": "1366150",
    "end": "1373980"
  },
  {
    "text": "I can't do anything about\nthis left over bit here. There's no choice of\ny that's going to make",
    "start": "1373980",
    "end": "1380340"
  },
  {
    "text": "this part and the smaller. It's just left over. It's some remainder that\nwe can't make any smaller or minimize an smaller.",
    "start": "1380340",
    "end": "1385410"
  },
  {
    "text": "There isn't an exact solution\nto this problem, in many cases. ",
    "start": "1385410",
    "end": "1390690"
  },
  {
    "text": "But one way this could be\n0 is if r is equal to n.",
    "start": "1390690",
    "end": "1397190"
  },
  {
    "text": "Then there are left\nover unspecified terms, and then this y i\nequals p i over sigma i",
    "start": "1397190",
    "end": "1402375"
  },
  {
    "text": "is the exact solution\nto the problem. ",
    "start": "1402375",
    "end": "1408280"
  },
  {
    "text": "So this is what you told me. Choose y i is p i over sigma i i\nfor i bigger than 1 and smaller",
    "start": "1408280",
    "end": "1416230"
  },
  {
    "text": "than r. There are going to\nbe values of y i that go between r plus 1 and\nm, because A was a vector that",
    "start": "1416230",
    "end": "1426070"
  },
  {
    "text": "mapped from m to n, right? So I have extra values of y that\ncould be specified potentially.",
    "start": "1426070",
    "end": "1432160"
  },
  {
    "text": "If that's true, if r\nplus 1 is smaller than m, then there's some components\nof y that I don't get to--",
    "start": "1432160",
    "end": "1438570"
  },
  {
    "text": "I can't specify, right? My system of equations is\nsomehow underdetermined. I need some external\ninformation to show me what",
    "start": "1438570",
    "end": "1445540"
  },
  {
    "text": "values to pick for those y i. I don't know. I can't use them. Sometimes people just\nset y i equal to 0.",
    "start": "1445540",
    "end": "1453420"
  },
  {
    "text": "That's sort of silly,\nbut that's what's done. It's called the minimum\nnorm least square solution.",
    "start": "1453420",
    "end": "1460150"
  },
  {
    "text": "y has minimum length, when you\nset all these other components to 0. But the truth is, we can't\nspecify those components,",
    "start": "1460150",
    "end": "1467140"
  },
  {
    "text": "right? We need some\nexternal information in order to specify them. Once we know y, we\ncan find x going back",
    "start": "1467140",
    "end": "1474679"
  },
  {
    "text": "to our definition of what y is. So I multiply this equation\nby V on both sides, and I'll get V y equals x.",
    "start": "1474680",
    "end": "1482350"
  },
  {
    "text": "So I can find my\nleast square solution to the problem from the\nsingular value decomposition.",
    "start": "1482350",
    "end": "1487596"
  },
  {
    "text": "So I can find the\nleast square solution to both overdetermined and\nunderdetermined problems using",
    "start": "1487596",
    "end": "1492850"
  },
  {
    "text": "singular value decomposition. It inherits all\nthe properties you know of solving the\nnormal equations,",
    "start": "1492850",
    "end": "1498670"
  },
  {
    "text": "multiplying by A transpose\nthe entire equation, and solving for a least\nsquare solution that way.",
    "start": "1498670",
    "end": "1504722"
  },
  {
    "text": "But that's only good for\noverdetermined systems of equations. This can work for\nunderdetermined equations as well.",
    "start": "1504722",
    "end": "1509770"
  },
  {
    "text": "And maybe we do have\nextraneous information that lets us specify these\nother components somehow.",
    "start": "1509770",
    "end": "1515080"
  },
  {
    "text": "Maybe we do a\nseparate optimization that chooses from all\npossible solutions",
    "start": "1515080",
    "end": "1520110"
  },
  {
    "text": "where these y i's are free,\nand picks the best one subject to some other constraint.",
    "start": "1520110",
    "end": "1525889"
  },
  {
    "text": "Does it makes sense? OK, that's the\nlast decomposition we're going to talk about.",
    "start": "1525890",
    "end": "1532070"
  },
  {
    "text": "It's as expensive to compute\nthe singular value decomposition as it is to solve a\nsystem of equations. You might have\nguessed that it's got",
    "start": "1532070",
    "end": "1538310"
  },
  {
    "text": "an order N cubed flavor to it. It's kind of inescapable\nthat we run up against those\ncomputational difficulties,",
    "start": "1538310",
    "end": "1545920"
  },
  {
    "text": "order N cubed\ncomputational complexity. And there are many problems\nof practical interest, particularly solutions of\nPDEs, for which that's not",
    "start": "1545920",
    "end": "1554179"
  },
  {
    "text": "going to cut it. Where you couldn't solve\nthe problem with that sort",
    "start": "1554180",
    "end": "1559970"
  },
  {
    "text": "of scaling in time. You couldn't compute the\nGaussian elimination, or the singular\nvalue decomposition,",
    "start": "1559970",
    "end": "1566539"
  },
  {
    "text": "or an eigenvalue decomposition. It won't work. And in those cases, we appeal\nto not exact solution methods,",
    "start": "1566540",
    "end": "1574520"
  },
  {
    "text": "but approximate\nsolution methods. So instead of trying to\nget an exact solution,",
    "start": "1574520",
    "end": "1580120"
  },
  {
    "text": "we'll try to formulate\none that's good enough. We already know the computer\nintroduces numerical error anyways.",
    "start": "1580120",
    "end": "1586070"
  },
  {
    "text": "Maybe we don't need machine\nprecision in our solution or something close to machine\nprecision in our solution. Maybe we're solving\nengineering problem,",
    "start": "1586070",
    "end": "1592266"
  },
  {
    "text": "and we're willing to\naccept relative errors on the order of 10 to the\nminus 3 or 10 to the minus 5,",
    "start": "1592266",
    "end": "1597770"
  },
  {
    "text": "some specified tolerance\nthat we apply to the problem. And in those circumstances,\nwe use iterative methods",
    "start": "1597770",
    "end": "1604250"
  },
  {
    "text": "to solve systems of\nequations instead of exact methods,\nelimination methods, or metrics\ndecomposition methods.",
    "start": "1604250",
    "end": "1610640"
  },
  {
    "text": " These algorithms are all\nbased on iterative refinement",
    "start": "1610640",
    "end": "1616559"
  },
  {
    "text": "of an initial guess. So if we have some\nsystem of equations we're trying to\nsolve, Ax equals b,",
    "start": "1616560",
    "end": "1622350"
  },
  {
    "text": "we'll formulate some\nlinear map, right? xi plus 1 will be some\nmatrix C times x i",
    "start": "1622350",
    "end": "1629910"
  },
  {
    "text": "plus some little vector c\nwhere x i is my last best guess for the solution to\nthis problem, and x i plus 1",
    "start": "1629910",
    "end": "1637080"
  },
  {
    "text": "is my next best guess for\nthe solution to this problem. And I'm hoping, as I apply\nthis map more and more times,",
    "start": "1637080",
    "end": "1644700"
  },
  {
    "text": "I'm creeping closer\nto the exact solution to the original\nsystem of equations. The map will converge\nwhen x i plus 1",
    "start": "1644700",
    "end": "1653610"
  },
  {
    "text": "approaches x i, when the\nmap isn't making any changes to the vector anymore.",
    "start": "1653610",
    "end": "1659250"
  },
  {
    "text": "And the converged value will\nbe a solution when x i--",
    "start": "1659250",
    "end": "1668490"
  },
  {
    "text": "which is equal to i\nminus c inverse times c, if I replace x i was\n1 with x i appear,",
    "start": "1668490",
    "end": "1674370"
  },
  {
    "text": "so I say that my map has\nconverged-- when this value is equivalent to A\ninverse times B, when",
    "start": "1674370",
    "end": "1680160"
  },
  {
    "text": "it's a solution to the\noriginal problem, right? So my map may converge.",
    "start": "1680160",
    "end": "1685480"
  },
  {
    "text": "It may not converge to a\nsolution of the problem I like, but if it satisfies\nthis condition, then has converged to be\na solution of the problem",
    "start": "1685480",
    "end": "1692730"
  },
  {
    "text": "that I like as well. And so it's all about using this\nC here and this little c here",
    "start": "1692730",
    "end": "1698400"
  },
  {
    "text": "so that this map converges\nto solution of the problem I'm after. And there are lots of\nschemes for doing this.",
    "start": "1698400",
    "end": "1705000"
  },
  {
    "text": "Some of them are kind of ad hoc. I'm going to show\nyou one right now. And then when we\ndo optimization,",
    "start": "1705000",
    "end": "1710490"
  },
  {
    "text": "we'll talk about\na more formal way of doing this for\nwhich you can guarantee very rapid convergence\nto a solution.",
    "start": "1710490",
    "end": "1717937"
  },
  {
    "text": "So here's a system of\nequations I'd like to solve. It's not a very big one. It doesn't really make sense\nto solve this one iteratively,",
    "start": "1717937",
    "end": "1724050"
  },
  {
    "text": "but it's a nice illustration. One way to go about\nformulating this map",
    "start": "1724050",
    "end": "1729570"
  },
  {
    "text": "is to split this\nmatrix into two parts. So I'll split it into a diagonal\npart and an off diagonal part.",
    "start": "1729570",
    "end": "1737160"
  },
  {
    "text": "So I haven't changed the\nproblem at all by doing that. And then I'm going to\nrename this x x i plus 1,",
    "start": "1737160",
    "end": "1743740"
  },
  {
    "text": "and I'm going to\nrename this x x i. And then move this\nmatrix vector product",
    "start": "1743740",
    "end": "1749454"
  },
  {
    "text": "to the other side\nof the equation. And here's my map. Of course, this\nmatrix multiplied doesn't make any-- it's\nnot useful to write it out",
    "start": "1749454",
    "end": "1756210"
  },
  {
    "text": "explicitly. This is just identity. So I can drop this entirely. This is just x i plus one.",
    "start": "1756210",
    "end": "1762090"
  },
  {
    "text": "So here's my map. Take an initial guess,\nmultiply it by this matrix,",
    "start": "1762090",
    "end": "1767429"
  },
  {
    "text": "add the vector 1, 0, and repeat\nover and over and over again. Hopefully-- we\ndon't really know--",
    "start": "1767430",
    "end": "1773130"
  },
  {
    "text": "but hopefully, it's\ngoing to converge to a solution of the\noriginal linear equations.",
    "start": "1773130",
    "end": "1778502"
  },
  {
    "text": "I didn't make up that method. That's a method called\nJacobi Iteration. And the strategy is to split\nthe matrix A into two parts--",
    "start": "1778502",
    "end": "1786280"
  },
  {
    "text": "a sum of its diagonal\nelements, and it's off diagonal elements-- and rewrite the original\nequations as an iterative map.",
    "start": "1786280",
    "end": "1794410"
  },
  {
    "text": "So D times x i plus 1 is equal\nto minus r times x i plus b.",
    "start": "1794410",
    "end": "1800650"
  },
  {
    "text": "Or x i plus 1 is D inverse\ntimes minus r x i plus b.",
    "start": "1800650",
    "end": "1807600"
  },
  {
    "text": "If the equations converge,\nthen D plus r times x i has to be equal to b, we\nwill have found a solution.",
    "start": "1807600",
    "end": "1813780"
  },
  {
    "text": "If it converges, right? If these iterations\napproach a steady value. If they don't change from\niteration to iteration.",
    "start": "1813780",
    "end": "1820770"
  },
  {
    "text": "Is The nice thing about\nthe Jacobi method is it turns the hard\nproblem, the order N cubed problem of\ncomputing A inverse B,",
    "start": "1820770",
    "end": "1829920"
  },
  {
    "text": "into a succession\nof easy problems, D inverse times some vector C.\nHow many calculations does it",
    "start": "1829920",
    "end": "1838710"
  },
  {
    "text": "take to compute that D inverse? ",
    "start": "1838710",
    "end": "1844710"
  },
  {
    "text": "N, that's right, order N. It's just a diagonal matrix. I invert each of its diagonal\nelements, and I'm done.",
    "start": "1844710",
    "end": "1849880"
  },
  {
    "text": "So I went from order N cubed,\nwhich was going to be hard, into a succession\nof order N problems.",
    "start": "1849880",
    "end": "1857320"
  },
  {
    "text": "So as long as it doesn't\ntake me order N squared iterations to get to the\nsolution that I want,",
    "start": "1857320",
    "end": "1862879"
  },
  {
    "text": "I'm going to be OK. This is going to be\na viable way to solve this problem faster than\nfinding the exact solution. ",
    "start": "1862879",
    "end": "1873240"
  },
  {
    "text": "How do you know\nthat it converges? That's the question. Is this thing actually\ngoing to converge or not,",
    "start": "1873240",
    "end": "1880290"
  },
  {
    "text": "or are these iterations just\ngoing to run on and on forever? Well, one way to check whether\nit will converge or not",
    "start": "1880290",
    "end": "1886620"
  },
  {
    "text": "is to go back up to this\nequation here, and substitute b equals Ax, where x is the\nexact solution to the problem.",
    "start": "1886620",
    "end": "1894890"
  },
  {
    "text": " And you can transform,\nthen, this equation into one",
    "start": "1894890",
    "end": "1900810"
  },
  {
    "text": "that looks like x i plus\n1 minus x equal to minus D inverse times r x i minus x.",
    "start": "1900810",
    "end": "1909930"
  },
  {
    "text": "And if I take the\nnorm of both sides and I apply our\nnormal equality--",
    "start": "1909930",
    "end": "1915270"
  },
  {
    "text": "where the norm of a\nmatrix vector product is smaller than the product\nof the norms of the matrices",
    "start": "1915270",
    "end": "1921460"
  },
  {
    "text": "of the vectors-- then I can get a\nratio like this. That the absolute error\nin iteration I plus 1",
    "start": "1921460",
    "end": "1930059"
  },
  {
    "text": "divided by the absolute\nerror in iteration i is smaller than the\nnorm of this matrix.",
    "start": "1930060",
    "end": "1937409"
  },
  {
    "text": "So if I'm converging,\nthen what I expect is this ratio should\nbe smaller than 1.",
    "start": "1937410",
    "end": "1945320"
  },
  {
    "text": "The error in my\nnext approximation should be smaller than the error\nin my current approximation. That makes sense?",
    "start": "1945320",
    "end": "1951450"
  },
  {
    "text": "So that means that I would hope\nthat the norm of this matrix is also smaller than 1.",
    "start": "1951450",
    "end": "1956690"
  },
  {
    "text": "If it is, then I'm going to\nbe guaranteed to converge. So for a particular\ncoefficient matrix,",
    "start": "1956690",
    "end": "1962912"
  },
  {
    "text": "for a system of linear\nequations I'm trying to solve, I may be able to find-- I may find that this is true.",
    "start": "1962912",
    "end": "1970430"
  },
  {
    "text": "And then I can\napply this method, and I'll converge to a solution. We call this sort of\nconvergence linear.",
    "start": "1970430",
    "end": "1978100"
  },
  {
    "text": "Whatever this number\nis, it tells me the fraction by which the\nerror is reduced from iteration",
    "start": "1978100",
    "end": "1983890"
  },
  {
    "text": "to iteration. So suppose this is 1/10. Then the absolute error is going\nto be reduced by a factor of 10",
    "start": "1983890",
    "end": "1991510"
  },
  {
    "text": "in each iteration. It's not going to\nbe 1/10 usually. It's going to be\nsomething that's",
    "start": "1991510",
    "end": "1997095"
  },
  {
    "text": "a little bit bigger than that\ntypically, but that's the idea. You can show-- I would encourage\nyou to try to work this out",
    "start": "1997095",
    "end": "2002700"
  },
  {
    "text": "on your own-- but you can show\nthat the infinity norm of this product-- ",
    "start": "2002700",
    "end": "2009060"
  },
  {
    "text": "infinity norm of this\nproduct is equal to this. And if I ask that the\ninfinity norm of this product",
    "start": "2009060",
    "end": "2016040"
  },
  {
    "text": "be smaller than 1,\nthat's guaranteed when the diagonal values of\nthe matrix and absolute value",
    "start": "2016040",
    "end": "2021450"
  },
  {
    "text": "are bigger than\nthe sum of the off diagonal values in a particular\nrow or a particular column.",
    "start": "2021450",
    "end": "2026669"
  },
  {
    "text": "And that kind of matrix we\ncall diagonally dominant. The diagonal values are bigger\nthan the sum and absolute value",
    "start": "2026670",
    "end": "2031770"
  },
  {
    "text": "of the off diagonal pieces. So diagonally dominant matrices,\nwhich come up quite often,",
    "start": "2031770",
    "end": "2037289"
  },
  {
    "text": "can be-- those linear\nequations based on those matrices can be solved\nreasonable efficiency using",
    "start": "2037290",
    "end": "2042660"
  },
  {
    "text": "the Jacobi method. There are better\nmethods to choose. I'll show you one in a second. But you can guarantee\nthat this is",
    "start": "2042660",
    "end": "2048899"
  },
  {
    "text": "going to converge to a solution,\nand that the solution will be the right solution to\nthe linear equations you were trying to solve.",
    "start": "2048900",
    "end": "2054094"
  },
  {
    "start": "2054094",
    "end": "2059440"
  },
  {
    "text": "So if the goal is just to\nturn hard problems into easier to solve problems, then\nthere are other natural ways",
    "start": "2059440",
    "end": "2065500"
  },
  {
    "text": "to want to split a matrix. So maybe you want to split into\nA lower triangular part which",
    "start": "2065500",
    "end": "2072100"
  },
  {
    "text": "contains the diagonal\nelements of A, and an upper\ntriangular part which has no diagonal elements of A.\nWe just split this thing apart.",
    "start": "2072100",
    "end": "2081129"
  },
  {
    "text": "And then we could rewrite\nour system of equations is an iterative map\nlike this, L times x i plus 1 is minus U\ntimes x i plus b.",
    "start": "2081130",
    "end": "2090429"
  },
  {
    "text": "All I have to do is invert\nl to find my next iteration. And how expensive\ncomputationally",
    "start": "2090429",
    "end": "2095590"
  },
  {
    "text": "is it to solve a system of\nequations which is triangular? This is a process we\ncall back substitution.",
    "start": "2095590",
    "end": "2102630"
  },
  {
    "text": "Its order-- AUDIENCE: N squared. JAMES W. SWAN: --N squared. So we still beat N cubed.",
    "start": "2102630",
    "end": "2108160"
  },
  {
    "text": "One would hope that it doesn't\nrequire too many iterations to do this. But in principle, we can do\nthis order N squared operations",
    "start": "2108160",
    "end": "2115160"
  },
  {
    "text": "many times. And it'll turn out\nthat this sort of a map converges to the solution\nthat we're after.",
    "start": "2115160",
    "end": "2122080"
  },
  {
    "text": "It converges when matrices\nare either diagonally dominant as before, or they're symmetric\nand they're positive definite.",
    "start": "2122080",
    "end": "2128470"
  },
  {
    "text": "Positive definite means all\nthe eigenvalues of the matrix are bigger than 0. ",
    "start": "2128470",
    "end": "2140680"
  },
  {
    "text": "So try the iterative method\nsolving some equations and see how we convert. Yes? AUDIENCE: How do you justify\nignoring the diagonal elements",
    "start": "2140680",
    "end": "2147460"
  },
  {
    "text": "in that method?  JAMES W. SWAN: So\nthe question was, how do you justify ignoring\nthe diagonal elements",
    "start": "2147460",
    "end": "2154960"
  },
  {
    "text": "in this method. Maybe I was going too\nfast or I misspoke. So I'm going to split A into\na lower triangular matrix that",
    "start": "2154960",
    "end": "2161980"
  },
  {
    "text": "has all the diagonal\nelements, and U is the upper parts with none of\nthose diagonal elements on it.",
    "start": "2161980",
    "end": "2168125"
  },
  {
    "text": "Does that make sense? AUDIENCE: Yeah. JAMES W. SWAN: Thank you\nfor asking that question. I hope that's clear. l holds onto the diagonal\npieces and U takes those away.",
    "start": "2168125",
    "end": "2176660"
  },
  {
    "text": " So let's try it. On a matrix like this,\nthe exact solution",
    "start": "2176660",
    "end": "2183310"
  },
  {
    "text": "to this system of equations\nis 3/4, 1/2, and 1/4. All right, we'll\ntry Jacobi, we'll",
    "start": "2183310",
    "end": "2188980"
  },
  {
    "text": "have to give it some initial\nguess for the solution, right? We're talking about\nplaces where you can derive those initial guesses\nfrom later on in the course,",
    "start": "2188980",
    "end": "2197160"
  },
  {
    "text": "but we have to start\nthe iterative process with some guess\nat the solutions. So here's an initial guess.",
    "start": "2197160",
    "end": "2203380"
  },
  {
    "text": "We'll apply this map. Here's Gauss-Seidel with\nthe same initial guess, and we'll apply this map.",
    "start": "2203380",
    "end": "2208670"
  },
  {
    "text": "They're both\nlinearly convergent, so the relative\nerror will go down",
    "start": "2208670",
    "end": "2213700"
  },
  {
    "text": "by a fixed factor\nafter each iteration. Iteration one, the relative\nerror in Jacobi will be 38%.",
    "start": "2213700",
    "end": "2220470"
  },
  {
    "text": "In Gauss-Seidel, it'll be 40%. If we apply this all the\nway down to 10 iterations, the relative error Jacobi will\nbe 1.7%, and the relative error",
    "start": "2220470",
    "end": "2229020"
  },
  {
    "text": "in Gauss-Seidel 0.08%. And we can go on and on\nwith these iterations if we want until we get\nsufficiently converged, we",
    "start": "2229020",
    "end": "2236740"
  },
  {
    "text": "get to a point where the\nrelative error is small enough that we're happy to accept\nthis answer as a solution",
    "start": "2236740",
    "end": "2242590"
  },
  {
    "text": "to our system of equations. So we traded the burden of doing\nall these calculations to do",
    "start": "2242590",
    "end": "2248450"
  },
  {
    "text": "elimination for a faster,\nless computationally complex",
    "start": "2248450",
    "end": "2254079"
  },
  {
    "text": "methodology. But the trade off was we don't\nget an exact solution anymore. We're going to have finite\nprecision in the result,",
    "start": "2254080",
    "end": "2260890"
  },
  {
    "text": "and we have to\nspecify the tolerance that we want to converge to. We're going to see now-- this\nis the hook into the next part",
    "start": "2260890",
    "end": "2267040"
  },
  {
    "text": "of that class-- we're going to talk about\nsolutions of nonlinear equations next for\nwhich there are",
    "start": "2267040",
    "end": "2272470"
  },
  {
    "text": "almost no non-linear equations\nthat we can solve exactly. They all have to be solved\nusing these iterative methods.",
    "start": "2272470",
    "end": "2278944"
  },
  {
    "text": "You can use these iterative\nmethods for linear equations. It's very common\nto do it this way. In my group, we\nsolve lots of systems",
    "start": "2278945",
    "end": "2284620"
  },
  {
    "text": "of linear equations associated\nwith hydrodynamic problems. These come up when\nyou're talking about,",
    "start": "2284620",
    "end": "2290670"
  },
  {
    "text": "say, low Reynolds\nnumber flows, which are linear sorts of\nfluid flow problems. They're big.",
    "start": "2290670",
    "end": "2295990"
  },
  {
    "text": "It's really hard to do\nGaussian elimination, so you apply different\niterative methods. You can do Gauss-Seidel. You can do Jacobi.",
    "start": "2295990",
    "end": "2302119"
  },
  {
    "text": "We'll learn about\nmore advanced ones like PCG, which you're\napplying on your homework now, and you should be seeing that\nit converges relatively quickly",
    "start": "2302120",
    "end": "2309970"
  },
  {
    "text": "in cases where exact\nelimination doesn't work. We'll learn, actually,\nhow to do that method. That's one that we\napply in my own group.",
    "start": "2309970",
    "end": "2315758"
  },
  {
    "text": "It's pretty common\nto use out there. Yes? AUDIENCE: One question, is\nthat that Gauss, [INAUDIBLE]",
    "start": "2315758",
    "end": "2323440"
  },
  {
    "text": "JAMES W. SWAN: Order N squared. AUDIENCE: Yeah,\nthat's what I meant. So now we've got an [INAUDIBLE].",
    "start": "2323440",
    "end": "2330148"
  },
  {
    "text": "So we basically have\n[INAUDIBLE] iterations, right? JAMES W. SWAN: This is\na wonderful question.",
    "start": "2330148",
    "end": "2336240"
  },
  {
    "text": "So this is a pathological\nproblem in the sense that it requires a\nlot of calculations",
    "start": "2336240",
    "end": "2343300"
  },
  {
    "text": "to get an iterative\nsolution here. We haven't gotten\nto an end that's big enough that the\ncomputational complexities",
    "start": "2343300",
    "end": "2350290"
  },
  {
    "text": "crossover. So for small Ns, probably\nthe factor in front of N--",
    "start": "2350290",
    "end": "2355810"
  },
  {
    "text": "whatever number that is-- and maybe even the\nsmaller factors, order N squared\nfactors on that order",
    "start": "2355810",
    "end": "2360940"
  },
  {
    "text": "N cubed, play a big\nrole in how long it takes to actually\ncomplete this thing. But modern problems are so\nbig that we almost always",
    "start": "2360940",
    "end": "2368140"
  },
  {
    "text": "are running out to Ns\nthat are large enough that we see a crossover. You'll see this in your\nhomework this week.",
    "start": "2368140",
    "end": "2374110"
  },
  {
    "text": "You won't see this\ncrossover at N equals 3. You're going to see\nit out at N equals 500 or 1,200, big problems.",
    "start": "2374110",
    "end": "2381336"
  },
  {
    "text": "Then we're going to\nencounter this crossover. That's a wonderful question. So first small system\nsizes, iterative methods",
    "start": "2381336",
    "end": "2389109"
  },
  {
    "text": "maybe don't buy you much. I suppose it depends on the\napplication though, right? If you're doing something\nthat involves solving problems",
    "start": "2389110",
    "end": "2396430"
  },
  {
    "text": "on embedded hardware, in some\nsort of sensor or control valve, there may be\nvery limited memory",
    "start": "2396430",
    "end": "2404230"
  },
  {
    "text": "or computational capacity\navailable to you. And you may actually\napply an iterative method like this to a problem\nthat that controller",
    "start": "2404230",
    "end": "2412119"
  },
  {
    "text": "needs to solve, for example. It just may not\nhave the capability",
    "start": "2412120",
    "end": "2417370"
  },
  {
    "text": "of storing and inverting what\nwe would consider, today, a relatively small matrix\nbecause the hardware doesn't",
    "start": "2417370",
    "end": "2425349"
  },
  {
    "text": "have that sort of capability. So there could be\ncases where you might choose something\nthat's slower but feasible,",
    "start": "2425350",
    "end": "2432250"
  },
  {
    "text": "versus something that's\nfaster and exact, because there are\nother constraints. They do exist, but modern\ncomputers are pretty efficient.",
    "start": "2432250",
    "end": "2440560"
  },
  {
    "text": "Your cell phone is faster\nthan the fastest computers in the world 20 years ago.",
    "start": "2440560",
    "end": "2446410"
  },
  {
    "text": "We're doing OK. So we've got to get out to\nbig system sizes, big problem sizes, before this\nstarts to pay off.",
    "start": "2446410",
    "end": "2452059"
  },
  {
    "text": "But it does for many\npractical problems. OK I'll close with this, because\nthis is the hook into solving",
    "start": "2452060",
    "end": "2459410"
  },
  {
    "text": "nonlinear equations.  So I showed you these\ntwo iterative methods,",
    "start": "2459410",
    "end": "2465370"
  },
  {
    "text": "and they kind of had\nstringent requirements for when they were actually\ngoing to converge, right?",
    "start": "2465370",
    "end": "2470770"
  },
  {
    "text": "I had to have a diagonally\ndominant system of equations for Jacobi to converge. I had to have diagonal dominance\nor symmetric positive definite",
    "start": "2470770",
    "end": "2478720"
  },
  {
    "text": "matrices. These things exist\nand they come up in lots of physical\nproblems, but I had to have it in order for\nGauss-Seidel to converge.",
    "start": "2478720",
    "end": "2485050"
  },
  {
    "text": "What if I have a\nsystem of equations that doesn't work that way? Or what if I have\nan iterative map that I like for some reason, but\nit doesn't appear to converge?",
    "start": "2485050",
    "end": "2493900"
  },
  {
    "text": "Maybe it converges under some\ncircumstances, but not others. Well, there's a way to modify\nthese iterative maps, called",
    "start": "2493900",
    "end": "2500140"
  },
  {
    "text": "successive\nover-relaxation, which can help promote convergence.",
    "start": "2500140",
    "end": "2505976"
  },
  {
    "text": "So suppose we have an\niterative map like this, x i plus 1 is some function of\nthe previous iteration value.",
    "start": "2505977",
    "end": "2511897"
  },
  {
    "text": "Doesn't matter what it is. It could be linear,\ncould be non-linear. We don't actually care.",
    "start": "2511897",
    "end": "2517760"
  },
  {
    "text": "The sought after solution\nis found when x i plus 1 is equal to x i. So this map is one\nthe convergence",
    "start": "2517760",
    "end": "2523539"
  },
  {
    "text": "to the exact solution of\nthe problem that we want. We've somehow guaranteed\nthat that's the case,",
    "start": "2523540",
    "end": "2528650"
  },
  {
    "text": "but it has to converge. One way to modify that\nmap is to say x i plus 1",
    "start": "2528650",
    "end": "2534890"
  },
  {
    "text": "is 1 minus some\nscalar value omega times x i plus omega times f.",
    "start": "2534890",
    "end": "2543019"
  },
  {
    "text": "You can confirm that if you\nsubstitute x i plus 1 equals x i into this equation,\nyou'll come up",
    "start": "2543020",
    "end": "2548900"
  },
  {
    "text": "with the same fixed points\nof this iterative map",
    "start": "2548900",
    "end": "2553940"
  },
  {
    "text": "x i is equal to f of x i. So you haven't changed what\nvalue will converge here,",
    "start": "2553940",
    "end": "2560210"
  },
  {
    "text": "but you've affected the\nrate at which it converges. Here you're saying x i\nplus 1 is some fraction",
    "start": "2560210",
    "end": "2565640"
  },
  {
    "text": "of my previous solution plus\nsome fraction of this f. And I get to control how big\nthose different fractions.",
    "start": "2565640",
    "end": "2573890"
  },
  {
    "text": "So if things aren't converging\nwell for a map like this, then I could try\nsuccessive over-relaxation,",
    "start": "2573890",
    "end": "2580700"
  },
  {
    "text": "and I could adjust this\nrelaxation parameter to be some fraction, some\nnumber between 0 and 1,",
    "start": "2580700",
    "end": "2587890"
  },
  {
    "text": "until I start to\nobserve convergence. And there are some\nrules one can use to try to promote\nconvergence with this kind",
    "start": "2587890",
    "end": "2593383"
  },
  {
    "text": "of successive over-relaxation. This is a very generic\ntechnique that one can apply. If you have any iterative\nmap you're trying to apply,",
    "start": "2593383",
    "end": "2600700"
  },
  {
    "text": "it should go to the\nsolution you want but it doesn't converge\nfor some reason, then you can use this\nrelaxation technique to promote",
    "start": "2600700",
    "end": "2607630"
  },
  {
    "text": "convergence to the solution. You may slow the\nconvergence way down. It may be very slow to\nconverge, but it will converge.",
    "start": "2607630",
    "end": "2614680"
  },
  {
    "text": "And after all, an\nanswer is better than no answer, no matter\nhow long it takes to get it. So sometimes you've\ngot to get these things",
    "start": "2614680",
    "end": "2620740"
  },
  {
    "text": "by hook or by crook. So for example, you can\napply this to Jacobi. This was the\noriginal Jacobi map.",
    "start": "2620740",
    "end": "2628514"
  },
  {
    "text": "And we just take that. We add 1 minus omega\ntimes x i plus omega",
    "start": "2628514",
    "end": "2633640"
  },
  {
    "text": "times this factor over here. And now we can choose omega so\nthat this solution converges.",
    "start": "2633640",
    "end": "2639300"
  },
  {
    "text": "We always make omega\nsmall enough so that the diagonal\nvalues of our matrix",
    "start": "2639300",
    "end": "2644550"
  },
  {
    "text": "appear big enough\nthat the matrix looks like it's diagonally dominated.",
    "start": "2644550",
    "end": "2649770"
  },
  {
    "text": "You could go back to that\nsame convergence analysis that I showed you before\nand try to apply it to this over-relaxation\nform of Jacobi and see that,",
    "start": "2649770",
    "end": "2659252"
  },
  {
    "text": "while there's always going to\nbe some value of omega that's small enough, that this\nthing will converge.",
    "start": "2659252",
    "end": "2665420"
  },
  {
    "text": "It will look effectively\ndiagonally dominant, because omega inverse\ntimes D will be big enough,",
    "start": "2665420",
    "end": "2672150"
  },
  {
    "text": "or omega times D inverse\nwill be small enough. Does that make sense? You can apply the same sort of\ndamping method to Gauss-Seidel",
    "start": "2672150",
    "end": "2679160"
  },
  {
    "text": "as well. It's very common to do this. The relaxation parameter acts\nlike an effective increase",
    "start": "2679160",
    "end": "2685560"
  },
  {
    "text": "in the eigenvalues\nof the matrix. So you can think about L. That's\na lower triangular matrix. It's diagonal values\nare its eigenvalues.",
    "start": "2685560",
    "end": "2694309"
  },
  {
    "text": "The diagonal values\nof L inverse-- well, 1 over those\ndiagonal values are the eigenvalues\nof L inverse.",
    "start": "2694310",
    "end": "2700820"
  },
  {
    "text": "And so if we make\nomega very small, then we make the eigenvalues\nof L inverse very small,",
    "start": "2700820",
    "end": "2706630"
  },
  {
    "text": "or the eigenvalues\nor L very big. And again, the matrix starts\nto look diagonally dominated.",
    "start": "2706630",
    "end": "2712170"
  },
  {
    "text": "And you can promote\nconvergence in this way. So even though this\nmay be slow, you",
    "start": "2712170",
    "end": "2717960"
  },
  {
    "text": "can use it to\nguarantee convergence of some iterative procedures,\nnot just for linear equations, but for non-linear\nequations as well.",
    "start": "2717960",
    "end": "2723840"
  },
  {
    "text": "And we'll see,\nthere are good ways of choosing omega\nfor certain classes of non-linear equations.",
    "start": "2723840",
    "end": "2729010"
  },
  {
    "text": "We'll apply\nNewton-Raphson method, and then will damp it using\nexactly the sort of procedure. And I'll show you\nhow you can choose",
    "start": "2729010",
    "end": "2737190"
  },
  {
    "text": "a nearly optimal value for\nomega to promote convergence to the solution.",
    "start": "2737190",
    "end": "2743610"
  },
  {
    "text": "Any questions? No, let me address one\nmore thing before you go.",
    "start": "2743610",
    "end": "2750140"
  },
  {
    "text": "We've scheduled times\nfor the quizzes. They are going to\nbe in the evenings",
    "start": "2750140",
    "end": "2755180"
  },
  {
    "text": "on the dates that are\nspecified on the syllabus. We wanted to do them\nduring the daytime.",
    "start": "2755180",
    "end": "2761059"
  },
  {
    "text": "It was really\ndifficult to schedule a room that was big\nenough for this class, so they have to be from 7:00\nto 9:00 in the gymnasium.",
    "start": "2761060",
    "end": "2767990"
  },
  {
    "text": "I apologize for that. We spent several days\nlooking around trying to find a place where\nwe could put everybody",
    "start": "2767990",
    "end": "2774050"
  },
  {
    "text": "so you would all get the\nsame experience in the quiz. I know that the November\nquiz comes back to back",
    "start": "2774050",
    "end": "2780589"
  },
  {
    "text": "with the thermodynamics\nexam as well. That's frustrating.",
    "start": "2780590",
    "end": "2786029"
  },
  {
    "text": "Thermodynamics is the next day. That week is tricky. That's AICHE, so most of\nthe faculty have to travel.",
    "start": "2786030",
    "end": "2793004"
  },
  {
    "text": "We won't be able\nto teach, but you won't have classes\none of those days so you have extra time to study.",
    "start": "2793004",
    "end": "2799910"
  },
  {
    "text": "And Columbus Day also\nfalls in that week, so there's no way to put\nthree exams in four days without having them\ncome right back to back.",
    "start": "2799910",
    "end": "2806830"
  },
  {
    "text": "Believe me, we\nthought about this and tried to get things\nscheduled as efficiently as we could for you, but\nsometimes there",
    "start": "2806830",
    "end": "2812420"
  },
  {
    "text": "are constraints that are\noutside of our control. But the quiz times are set. There's going to be done in\nOctober and one in November.",
    "start": "2812420",
    "end": "2818260"
  },
  {
    "text": "They'll be in the evening, and\nthey'll be in the gymnasium. I'll give you directions\nto it before the exam, just",
    "start": "2818260",
    "end": "2823790"
  },
  {
    "text": "say you know exactly\nwhere to go, OK? Thank you, guys. ",
    "start": "2823790",
    "end": "2831765"
  }
]