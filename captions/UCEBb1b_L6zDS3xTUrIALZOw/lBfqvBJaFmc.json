[
  {
    "text": "[SQUEAKING] [RUSTLING] [CLICKING]",
    "start": "0",
    "end": "5976"
  },
  {
    "text": " STEVEN G. JOHNSON: OK.",
    "start": "5976",
    "end": "11450"
  },
  {
    "text": "So I think we can get started. So the next part, I'm going to\nshow some slides, so talking about some applications.",
    "start": "11450",
    "end": "17450"
  },
  {
    "text": "As usual, these slides\nwill be posted later today to the GitHub page. And also the handwritten\nnotes also from the first part",
    "start": "17450",
    "end": "26359"
  },
  {
    "text": "will also be posted. They're probably already posted. So now I want to\ntalk about some--",
    "start": "26360",
    "end": "36085"
  },
  {
    "text": "remind you of why\nwe're computing derivatives in the first place. And so we talked\nabout some things at a high level in lecture one.",
    "start": "36085",
    "end": "43039"
  },
  {
    "text": "I want to just drill\ndown a little bit and then also talk more about\ncomputation of derivatives.",
    "start": "43040",
    "end": "50450"
  },
  {
    "text": " So one basic use\nof derivatives is",
    "start": "50450",
    "end": "57800"
  },
  {
    "text": "to solve a nonlinear equations. And you all learned this\nprobably in first semester calculus if you have a--",
    "start": "57800",
    "end": "64390"
  },
  {
    "text": "a scalar function f of x that\ntakes a scalar in and gives a scalar out. And that's some nonlinear--",
    "start": "64390",
    "end": "71760"
  },
  {
    "text": "if it's linear obviously,\nyou can solve it. If it's a quadratic, you\nhave a quadratic formula. But if it's some\narbitrary function",
    "start": "71760",
    "end": "77760"
  },
  {
    "text": "like sine of cosine\nof x or something like that, like some\ncomplicated thing,",
    "start": "77760",
    "end": "83640"
  },
  {
    "text": "you might not be able to\nwrite a closed-form solution. But there's a nice way to get\nthe solution approximately",
    "start": "83640",
    "end": "90990"
  },
  {
    "text": "to any accuracy you\nwant, as many digits as you want very\nquickly if you know at least roughly where the root\nis, and that's Newton's method.",
    "start": "90990",
    "end": "100050"
  },
  {
    "text": "And what it really is it's\nreally a linear algebra technique. It's really taking the function,\napproximating it by a line,",
    "start": "100050",
    "end": "109500"
  },
  {
    "text": "by linear function. And then once it's linear,\nit's easy to find the root, and that gives you\nan approximate root.",
    "start": "109500",
    "end": "115049"
  },
  {
    "text": "And you use that as a guess. So that's the three steps here. So if you're solving\nf of x equals 0--",
    "start": "115050",
    "end": "121530"
  },
  {
    "text": "so f of x is this blue\ncurve on the right-- then you linearize\nwith your derivatives.",
    "start": "121530",
    "end": "126540"
  },
  {
    "text": "So f of x plus dx is\napproximate-- for delta x,",
    "start": "126540",
    "end": "132410"
  },
  {
    "text": "and it is not infinitesimal--\nis approximately f of x plus f prime of x delta x.",
    "start": "132410",
    "end": "138860"
  },
  {
    "text": "That's what the derivative is. And then if we\ntreat the equation",
    "start": "138860",
    "end": "144069"
  },
  {
    "text": "on the right, the linear\nproblem as an approximation for a real problem, that\none's easy to solve.",
    "start": "144070",
    "end": "150140"
  },
  {
    "text": "So we can set that\nlinear thing equal to 0. Solve for delta x, and it's\njust minus f over f prime.",
    "start": "150140",
    "end": "159160"
  },
  {
    "text": "And so that gives us a new x. So that's our guess\nfor the solution is x plus delta x, which\nis the same thing as x",
    "start": "159160",
    "end": "166570"
  },
  {
    "text": "minus f divided by f prime. Of course, it's\nnot an exact route because this linear was\nonly a linear approximation.",
    "start": "166570",
    "end": "176220"
  },
  {
    "text": "But it gets us\ncloser to the root. So on the right is a nice\nanimation from Wikipedia",
    "start": "176220",
    "end": "181410"
  },
  {
    "text": "that shows this process,\nstarting from a few x's where you start with an x.",
    "start": "181410",
    "end": "186630"
  },
  {
    "text": "What you're doing is\napproximating a function by its tangent. That's the red line, and\nthat gives you the next x.",
    "start": "186630",
    "end": "191970"
  },
  {
    "text": "And so you start with this x. And the x1 on the\nright gives you the x2. And then you linearize it.",
    "start": "191970",
    "end": "197849"
  },
  {
    "text": "You get another x, x3,\nand then another x, x4.",
    "start": "197850",
    "end": "203940"
  },
  {
    "text": "And then very, very quickly,\nonce you start getting close, this converges extremely fast.",
    "start": "203940",
    "end": "211210"
  },
  {
    "text": "If you're far away, it can\nbe a little unpredictable. But once you're close, it turns\nout-- it's actually amazing.",
    "start": "211210",
    "end": "217030"
  },
  {
    "text": "It doubles the number of\ndecimal places in every step. So if you have it to one\ndigit, on the next step,",
    "start": "217030",
    "end": "222290"
  },
  {
    "text": "you'll have it to two\ndigits, then the next step four, then 8, and\nthen 16, and then you run out of digits on\nyour computer very quickly.",
    "start": "222290",
    "end": "230840"
  },
  {
    "text": "So it's really, really\na great algorithm. It goes back thousands of\nyears, the Newton's algorithm.",
    "start": "230840",
    "end": "238570"
  },
  {
    "text": "But you can see that as soon\nas we write it this way, then we should be able to\nsee that the same thing works",
    "start": "238570",
    "end": "246069"
  },
  {
    "text": "for multi-dimensional functions. So if we have a function\nthat takes a vector in and a vector out--",
    "start": "246070",
    "end": "251209"
  },
  {
    "text": "so suppose x is a vector in rn. I don't know why that comes\nout in that weird font here.",
    "start": "251210",
    "end": "256810"
  },
  {
    "text": "So you have n equations\nand n variables. So that's important.",
    "start": "256810",
    "end": "262100"
  },
  {
    "text": "You have to have the same number\nof equations and variables. But now they're nonlinear\nfor some arbitrary nonlinear function, and you're\ntrying to find a root.",
    "start": "262100",
    "end": "269750"
  },
  {
    "text": "And then you do\nexactly the same thing. You approximate f of by a\nlinear function. f of x plus x",
    "start": "269750",
    "end": "277819"
  },
  {
    "text": "is approximately f of x\nplus f prime of x delta x. And that's what we've been\nseeing over and over again.",
    "start": "277820",
    "end": "283940"
  },
  {
    "text": "This is the\ndefinition of f prime. Is that linearization? Oops, and this\nalways happens when",
    "start": "283940",
    "end": "291440"
  },
  {
    "text": "I display it on the computer. Let me see if I can--",
    "start": "291440",
    "end": "297800"
  },
  {
    "text": "and for some\nreason, this display gets screwed up a little bit. Hold on, how do I\nget out of this?",
    "start": "297800",
    "end": "302930"
  },
  {
    "text": " Yeah. So it looks fine here in\nnon-presentation mode.",
    "start": "302930",
    "end": "309740"
  },
  {
    "text": " So what do we do?",
    "start": "309740",
    "end": "315754"
  },
  {
    "text": "We approximate f\nby a linear thing, and now we know how to do\nthat for any vector space.",
    "start": "315755",
    "end": "322169"
  },
  {
    "text": "And then, for example,\nx is a column vector; then f prime is a\nJacobian matrix.",
    "start": "322170",
    "end": "328700"
  },
  {
    "text": "You set this linear\napproximation equal to 0, and you solve it for\ndelta x, and that's",
    "start": "328700",
    "end": "334509"
  },
  {
    "text": "just a linear equation. So that's just a 1806 thing,\na linear algebra thing. So now delta x is--",
    "start": "334510",
    "end": "340595"
  },
  {
    "text": "you move f to the other side,\nand you multiply both sides by f prime inverse. Delta x is minus f prime\ninverse times f of x.",
    "start": "340595",
    "end": "347800"
  },
  {
    "text": "So it's just the inverse of\nthe Jacobian matrix times f with the minus sign. And so that's our new x.",
    "start": "347800",
    "end": "354100"
  },
  {
    "text": "Our new x is this x-- the original x\nplus this delta x, which should give us a\nroot if it were linear.",
    "start": "354100",
    "end": "360970"
  },
  {
    "text": "So it's just f-- the new x is x minus\ninverse Jacobian times f.",
    "start": "360970",
    "end": "366970"
  },
  {
    "text": "So you solve one linear\nequation with a Jacobian, and you get your new x. And of course, this is\nan approximate root,",
    "start": "366970",
    "end": "373380"
  },
  {
    "text": "but you just keep\nrepeating the process. It's much harder to picture\nthis in higher dimensions,",
    "start": "373380",
    "end": "380160"
  },
  {
    "text": "compared to the 1D case. But it works amazingly fast.",
    "start": "380160",
    "end": "385230"
  },
  {
    "text": "It says the same thing. You solve a linear\nsystem in each step. It doubles the number\nof digits in each step.",
    "start": "385230",
    "end": "390635"
  },
  {
    "text": "And with the caveat\nthat you need a starting guess close\nenough to the root. If you start very far\naway from the root,",
    "start": "390635",
    "end": "395879"
  },
  {
    "text": "it can be kind of\nunpredictable what this does. In fact, it can even make\nthese amazing fractal patterns.",
    "start": "395880",
    "end": "402409"
  },
  {
    "text": "So if you Google\nNewton fractal, there's a nice Wikipedia\npage on this that has lots of beautiful images\nof what Newton's method does",
    "start": "402410",
    "end": "410670"
  },
  {
    "text": "if you start far away from\nthe root in higher dimensions. It has this weird\nfractal pattern.",
    "start": "410670",
    "end": "416010"
  },
  {
    "text": "So you have to have some rough\nidea where the solution is, but then you can polish\nit off super fast.",
    "start": "416010",
    "end": "424759"
  },
  {
    "text": "So that's Newton's methond. So this is a great\napplication for Jacobians and for derivatives in general\nof equations that take vectors",
    "start": "424760",
    "end": "433880"
  },
  {
    "text": "in and give vectors out. And the other even more\nfamous application I would say",
    "start": "433880",
    "end": "440510"
  },
  {
    "text": "is optimization. So optimization, you\nhave a scalar function.",
    "start": "440510",
    "end": "446220"
  },
  {
    "text": "So you have a vector\nin and a scalar out, and you're trying to minimize\nit, for example, or maximize",
    "start": "446220",
    "end": "453919"
  },
  {
    "text": "it. Maximizing, minimizing is\njust flipping the sign.",
    "start": "453920",
    "end": "459050"
  },
  {
    "text": "So for example, machine\nlearning, you have a fx. You have a big neural network.",
    "start": "459050",
    "end": "464310"
  },
  {
    "text": "x Is a million parameters\nof your neural network. And the thing you're\ntrying to minimize is the loss function\nthat basically you",
    "start": "464310",
    "end": "471500"
  },
  {
    "text": "evaluate your neural network,\ncompare its output to what you want its output to be.",
    "start": "471500",
    "end": "476600"
  },
  {
    "text": "And you take some norm of the\ndifference, and that's your f. It's the error in the neural\nnetwork for its function",
    "start": "476600",
    "end": "482160"
  },
  {
    "text": "of the parameters. And you want to go downhill\nand make that error as small as possible. And so the key point is if you\ncan take the gradient of this--",
    "start": "482160",
    "end": "490310"
  },
  {
    "text": "and now we have a general\ndefinition-- for any vector space, any scalar function,\nif we have a dot product,",
    "start": "490310",
    "end": "497490"
  },
  {
    "text": "then we can define a gradient. And that gradient then points\nin the uphill direction,",
    "start": "497490",
    "end": "504430"
  },
  {
    "text": "or minus gradient points\nin the downhill direction, which is called the\nsteepest descent direction. And the most basic idea--",
    "start": "504430",
    "end": "514030"
  },
  {
    "text": "and there's many\ncomplications on top of this. But the most basic idea\nis you just go downhill.",
    "start": "514030",
    "end": "519070"
  },
  {
    "text": "Imagine you're in the\nmountains in a fog. You can't see anything. But if you want to find\nthe bottom of a valley,",
    "start": "519070",
    "end": "526120"
  },
  {
    "text": "you just go downhill. You might not find\nthe deepest valley. You're going to find\nthe closest valley.",
    "start": "526120",
    "end": "532900"
  },
  {
    "text": "There might be a deeper\nvalley someplace else, but you'll find a local minimum. And even if you have\na million parameters,",
    "start": "532900",
    "end": "540880"
  },
  {
    "text": "as you have very commonly\nin neural networks, this allows you to evolve\nthem all simultaneously",
    "start": "540880",
    "end": "546670"
  },
  {
    "text": "in the downhill direction right. You want to update all-- in your million\ndimensional space,",
    "start": "546670",
    "end": "551930"
  },
  {
    "text": "you want to go in\nthis direction. You don't want to optimize\neach variable one at a time",
    "start": "551930",
    "end": "557290"
  },
  {
    "text": "because that'll take forever if\nthere are a million variables. And as we're going to talk\nabout over and over again,",
    "start": "557290",
    "end": "563230"
  },
  {
    "text": "that it turns out that\nwhen you have a scalar function of vector\ninputs, you can compute all million derivatives\nor all n derivatives,",
    "start": "563230",
    "end": "572440"
  },
  {
    "text": "the gradient, with about\nthe same cost as evaluating the function once. So evaluating the function,\nsay, is a big neural network.",
    "start": "572440",
    "end": "579190"
  },
  {
    "text": "You have to evaluate that once\nto get your loss function. It turns out you can\nget the derivative of that loss with respect\nto all million inputs",
    "start": "579190",
    "end": "585370"
  },
  {
    "text": "with essentially the equivalent\nof one additional function evaluation.",
    "start": "585370",
    "end": "590390"
  },
  {
    "text": "So for neural networks, this\nis called backpropagation. In automatic\ndifferentiation, this",
    "start": "590390",
    "end": "595570"
  },
  {
    "text": "is called reverse\nmode differentiation. This is also called\nadjoint methods. And as I alluded\nto the other day,",
    "start": "595570",
    "end": "601899"
  },
  {
    "text": "it's equivalent to just\nevaluating the chain rule and multiplying left to\nright so that you always",
    "start": "601900",
    "end": "607330"
  },
  {
    "text": "multiply vectors times matrices\nand never matrices times matrices. And this is what makes\nlarge-scale optimization",
    "start": "607330",
    "end": "613930"
  },
  {
    "text": "practical. So when you're optimizing\nover a million parameters in a neural network,\nor you're optimizing the shape of an airplane\nwing characterized",
    "start": "613930",
    "end": "621170"
  },
  {
    "text": "by zillions of parameters and\nso forth, portfolio optimization and in finance and so forth.",
    "start": "621170",
    "end": "626750"
  },
  {
    "text": "So there's a lot of\ncomplications in this that I'm not going\nto go through at all, but I just want to mention\nsome of the complications",
    "start": "626750",
    "end": "633440"
  },
  {
    "text": "that if you want to\ndo this for real, that you would cover,\nto some extent, in a nonlinear\noptimization class.",
    "start": "633440",
    "end": "640320"
  },
  {
    "text": "So the gradient points uphill\nminus gradient points downhill. So, great, that tells\nyou what direction to go,",
    "start": "640320",
    "end": "647960"
  },
  {
    "text": "but how far do you\ngo in that direction? And usually, you want to\ntake kind of as big a step",
    "start": "647960",
    "end": "653380"
  },
  {
    "text": "as you can to go downhill\nas far as possible. But if you take\ntoo big of a step,",
    "start": "653380",
    "end": "659410"
  },
  {
    "text": "the gradient is like a\nlinear approximation. So that's only going to be valid\nfor relatively small steps.",
    "start": "659410",
    "end": "665377"
  },
  {
    "text": "If you take too big of\na steps, the gradient is not going to be an indication\nof the downhill direction.",
    "start": "665377",
    "end": "675800"
  },
  {
    "text": "So there's a lot of\ndifferent strategies. One is you do a line search. You do 1D minimization\nin this direction.",
    "start": "675800",
    "end": "682930"
  },
  {
    "text": "And you take a step. And the simpler thing\nis you take a big step. And if the function\ngets better, great.",
    "start": "682930",
    "end": "688870"
  },
  {
    "text": "If the function\ndoes not get better, then you kind of backtrack\nalong that direction. So you'd divide your step by\ntwo or some geometric factor,",
    "start": "688870",
    "end": "696250"
  },
  {
    "text": "and you keep doing that until\nyou find you're going downhill. Another thing is you have\nsomething called a trust",
    "start": "696250",
    "end": "702589"
  },
  {
    "text": "region. You have some notion of how big\na step you're allowed to take. And you take you--",
    "start": "702590",
    "end": "708920"
  },
  {
    "text": "you basically optimize\nit within this region using the linear approximation. And then the tricky\nparts are like, how",
    "start": "708921",
    "end": "715880"
  },
  {
    "text": "do you decide how big the trust\nregion is as you go along? You might want to\ngrow it or shrink it.",
    "start": "715880",
    "end": "720913"
  },
  {
    "text": "And it turns out this is\nreally tricky to get right. It's really easy to write\ndown an algorithm that",
    "start": "720913",
    "end": "726050"
  },
  {
    "text": "looks reasonable, but\nactually kind of goes in circles forever and\never and never converges.",
    "start": "726050",
    "end": "731540"
  },
  {
    "text": "So you have to-- and the history of\nnonlinear optimization is full of such algorithms\nthat look reasonable,",
    "start": "731540",
    "end": "737790"
  },
  {
    "text": "but it turns out, don't\nconverge in general. They might converge sometimes,\nbut then sometimes they",
    "start": "737790",
    "end": "743060"
  },
  {
    "text": "might just loop around\nforever; or worse, they might seem to converge,\nbut they're actually stopping at a point\nthat's not a minimum.",
    "start": "743060",
    "end": "751160"
  },
  {
    "text": "But there's lots of\nalgorithms for this. Usually, most people doing\nnonlinear optimization",
    "start": "751160",
    "end": "756470"
  },
  {
    "text": "do not write their\nown algorithms. They take one off the shelf. There's a lot of trade out--\nand you try different ones.",
    "start": "756470",
    "end": "762980"
  },
  {
    "text": "Each one, you\nprovide a function, you provide the\ngradient, and you let it somehow decide what to\ndo, and you try different ones",
    "start": "762980",
    "end": "770930"
  },
  {
    "text": "and see which one works\nwell for your problem. Another complication is\nthat a lot of real problems, you're not just\nminimizing function.",
    "start": "770930",
    "end": "776780"
  },
  {
    "text": "You also have constraints. So you have often a\nset of constraints. And the general form\nis you're minimizing",
    "start": "776780",
    "end": "782300"
  },
  {
    "text": "a function f of x, subject\nto some set of constraints, say, of some functions gk that\nyou want to stay negative;",
    "start": "782300",
    "end": "789590"
  },
  {
    "text": "or feasible, is the terminology. So it's more complicated,\nbut you still",
    "start": "789590",
    "end": "795190"
  },
  {
    "text": "need the gradient of f to\nfind the downhill direction. And turns out you still need\nthe gradient of your constraint functions.",
    "start": "795190",
    "end": "800930"
  },
  {
    "text": "To approximate the\nbilinear things that are easy to understand\nand easy to take steps.",
    "start": "800930",
    "end": "806820"
  },
  {
    "text": "Another complication is that if\nyou just go straight downhill,",
    "start": "806820",
    "end": "812180"
  },
  {
    "text": "if you have a function that has\nkind of a long narrow valley, things tend to zig-zag a lot.",
    "start": "812180",
    "end": "818210"
  },
  {
    "text": "So you might converge\nvery, very slowly where you zig-zag back and\nforth and with steepest descent.",
    "start": "818210",
    "end": "824579"
  },
  {
    "text": "So people have lots of\ncorrections to try and improve this using momentum terms-- you might have heard\nthat-- conjugate gradients,",
    "start": "824580",
    "end": "831470"
  },
  {
    "text": "other forms of memory\nto try and remember the previous steps\nand not zig-zag,",
    "start": "831470",
    "end": "837709"
  },
  {
    "text": "backtrack in a direction\nyou just optimized. Another fancy\nthing you can do is you can try and estimate\nsecond derivatives",
    "start": "837710",
    "end": "844490"
  },
  {
    "text": "or even calculate second\nderivatives-- are called Hessian matrices-- and then use\nthat information to help you.",
    "start": "844490",
    "end": "850710"
  },
  {
    "text": "And there's the\nmost important class are called BFGS algorithms. So I don't expect you to\nunderstand all these terms.",
    "start": "850710",
    "end": "856550"
  },
  {
    "text": "I just want to throw\nout some culture. So if you hear, oh, use a\nmomentum term or use BFGS,",
    "start": "856550",
    "end": "863089"
  },
  {
    "text": "you're trying to\navoid zig-zagging by kind of remembering\nwhat you did before.",
    "start": "863090",
    "end": "868330"
  },
  {
    "text": "And it's one of\nthose things where there is no one best algorithm.",
    "start": "868330",
    "end": "873910"
  },
  {
    "text": "In certain fields,\nthere's algorithms that are very popular. Like, in machine\nlearning, there's an algorithm called atom\nthat's very popular.",
    "start": "873910",
    "end": "880570"
  },
  {
    "text": "But in general, there are a\nlot of algorithms out there. There's not one that's\nkind of one that's",
    "start": "880570",
    "end": "886930"
  },
  {
    "text": "become the best algorithm. So in general, people-- you have to do a little\nbit of trial and error.",
    "start": "886930",
    "end": "893860"
  },
  {
    "text": "For a given problem, you\nexperiment a little bit, see what other people\non similar problems use,",
    "start": "893860",
    "end": "899050"
  },
  {
    "text": "or just try different\nthings and see which one works best for your problem. And so and some\nimparting advice on this",
    "start": "899050",
    "end": "909170"
  },
  {
    "text": "is that when you're doing\nlarge scale optimization, the main trick is not\noften the detail--",
    "start": "909170",
    "end": "914590"
  },
  {
    "text": "it's often not the details of\nthese optimization algorithms. You're usually using\noff-the-shelf algorithms. So most of the practitioners\ndoing optimization",
    "start": "914590",
    "end": "923060"
  },
  {
    "text": "in practice, they spend\nmost of their time not choosing the algorithm,\nbut on the problem formulation,",
    "start": "923060",
    "end": "928250"
  },
  {
    "text": "really thinking about\nwhat exact function you're trying to optimize,\nwhat constraints, how do you write down\nthe parameters in order",
    "start": "928250",
    "end": "935150"
  },
  {
    "text": "to enable you to use\nthe best algorithms and have things\nbehave most nicely?",
    "start": "935150",
    "end": "941960"
  },
  {
    "text": "Because very often, there\nare multiple mathematically equivalent formulations\nof the same problem,",
    "start": "941960",
    "end": "949040"
  },
  {
    "text": "but that have very\ndifferent characteristics. And very often, someone if\nyou're optimizing something,",
    "start": "949040",
    "end": "955430"
  },
  {
    "text": "someone doesn't hand you\na mathematical function that they want to optimize. They describe in words, \"I want\nto make a stronger airplane",
    "start": "955430",
    "end": "961430"
  },
  {
    "text": "wing,\" or whatever it is. And there are a lot of\nqualitatively similar ways",
    "start": "961430",
    "end": "966870"
  },
  {
    "text": "to write down that\nproblem mathematically. And so it really\nmatters for optimization",
    "start": "966870",
    "end": "972030"
  },
  {
    "text": "to choose one that well-match\nto optimization algorithms, and choose one where you\ncan compute where it's",
    "start": "972030",
    "end": "979050"
  },
  {
    "text": "differentiable, where you can\ncompute derivatives efficiently and so forth. And as I said earlier, if\nyou have lots of parameters,",
    "start": "979050",
    "end": "985440"
  },
  {
    "text": "you really have to compute\nderivatives analytically. If you find\ndifferences, you'd have",
    "start": "985440",
    "end": "991010"
  },
  {
    "text": "to take evaluate your\nfunction once per parameter. So in particular, I want\nto talk a little bit more",
    "start": "991010",
    "end": "997810"
  },
  {
    "text": "about not machine learning,\nbut something that's near and dear to my heart,\nwhich is engineering or physics",
    "start": "997810",
    "end": "1003300"
  },
  {
    "text": "optimization, where,\nfor example, that you're trying to make an airplane wing\nthat's as strong as possible.",
    "start": "1003300",
    "end": "1009560"
  },
  {
    "text": "So let me try and write down\nthe general process of this.",
    "start": "1009560",
    "end": "1015040"
  },
  {
    "text": "So you have some\nengineering problem you're trying to optimize. What does it look like? So first of all, you have\nsome design parameters.",
    "start": "1015040",
    "end": "1022072"
  },
  {
    "text": "Let's call those p. And those may be the materials. It may be the geometry\nof the shape of the wing.",
    "start": "1022072",
    "end": "1028869"
  },
  {
    "text": "So it might be the forces. Some way you're going to-- what your input's in the\nsystem are going to be,",
    "start": "1028869",
    "end": "1034060"
  },
  {
    "text": "some other kind\nof unknowns these are the things you can\ncontrol as an engineer. And then you might\nhave a lot of them.",
    "start": "1034060",
    "end": "1041880"
  },
  {
    "text": "To describe the shape\nof an airplane wing, you might need a\nlot of parameters. And then you're going to\ntake those parameters,",
    "start": "1041880",
    "end": "1048240"
  },
  {
    "text": "and you're going to stick\nthem into some physical model, the physical model. If it's strength, it\nmight be solid mechanics.",
    "start": "1048240",
    "end": "1055020"
  },
  {
    "text": "It might be chemical reactions. It might be heat transport. It might be\nelectromagnetic waves.",
    "start": "1055020",
    "end": "1060180"
  },
  {
    "text": "It might be sound waves. It might be fluid flow\nproblems and so forth.",
    "start": "1060180",
    "end": "1065399"
  },
  {
    "text": "And you're going to solve that\nmodel to find some physics, like the stresses or\nsomething like that,",
    "start": "1065400",
    "end": "1071220"
  },
  {
    "text": "or the electromagnetic waves. So the simplest example\nwould be a linear model. So you're solving ax equals b.",
    "start": "1071220",
    "end": "1077840"
  },
  {
    "text": "And your matrix and\nyour right hand side depend on your parameters. Those are the things\nyou can control.",
    "start": "1077840",
    "end": "1084180"
  },
  {
    "text": "And then you find some solution,\nx, which is the physics. Say, the heat in your system\nor the electromagnetic fields",
    "start": "1084180",
    "end": "1091380"
  },
  {
    "text": "in your system or the\nstresses in your system. And given that solution\nso that those X's now",
    "start": "1091380",
    "end": "1098400"
  },
  {
    "text": "depend on the parameters\nthrough the matrix and through the and through\nthe right hand sides. So these could be\nforces, displacements,",
    "start": "1098400",
    "end": "1104700"
  },
  {
    "text": "and so forth, magnetic\nfield, pressures, velocities. Then, you have something you're\ntrying to do, some objective.",
    "start": "1104700",
    "end": "1111390"
  },
  {
    "text": "You're trying to make\nit as strong as possible or as fast as possible or as\nefficient as possible in power,",
    "start": "1111390",
    "end": "1117930"
  },
  {
    "text": "or you're trying to match\nsomething to experiment. Like, if you have some\nunknowns, an unknown physics,",
    "start": "1117930",
    "end": "1125243"
  },
  {
    "text": "and you're trying\nto match your it to your measurement\nof chemical reactions. So that's something that\ntakes the physical solution",
    "start": "1125243",
    "end": "1131909"
  },
  {
    "text": "and gives you a number that's\nyour objective function. Your parameters go\ninto the physics. The physics goes\ninto a solution.",
    "start": "1131910",
    "end": "1137850"
  },
  {
    "text": "The solution goes into\na design objective function, a number, a scalar. And then you want\nto optimize it.",
    "start": "1137850",
    "end": "1144492"
  },
  {
    "text": "You want to update\nthe design parameters to make this function better\nto say maximize or minimize it.",
    "start": "1144493",
    "end": "1149710"
  },
  {
    "text": "And so you have to\ncompute the gradient. You need to compute\nthe derivative of f, not with respect to f--",
    "start": "1149710",
    "end": "1155560"
  },
  {
    "text": "I'm sorry, not\nwith respect to x, but with respect to p,\nyour design parameters. So you have to follow\nthe chain rue all the way",
    "start": "1155560",
    "end": "1162430"
  },
  {
    "text": "through this process from\nthe design parameters to the objective to get\nall these derivatives.",
    "start": "1162430",
    "end": "1168280"
  },
  {
    "text": "And then you can go update\nthe design parameters in the downhill or\nuphill direction to make your function better.",
    "start": "1168280",
    "end": "1175179"
  },
  {
    "text": "So this is kind of\nthe general cartoon. And an example of that,\nI wanted to show-- there's lots and\nlots of examples,",
    "start": "1175180",
    "end": "1180760"
  },
  {
    "text": "but this is kind of a\nfun function example. So this was found online.",
    "start": "1180760",
    "end": "1186490"
  },
  {
    "text": "So they're trying to do what's\ncalled topology optimization of a chair.",
    "start": "1186490",
    "end": "1192429"
  },
  {
    "text": "So what you're\ntrying to do here is you're trying to make\na chair as strong as possible with a minimal\namount of material.",
    "start": "1192430",
    "end": "1201190"
  },
  {
    "text": "And it has a seat and\nit has a seat back. Those are fixed. ALAN EDELMAN: I want\nto buy one of these.",
    "start": "1201190",
    "end": "1206350"
  },
  {
    "text": "Where do I buy it? STEVEN G. JOHNSON: [CHUCKLES]\nBut the shape of the chair is just completely free.",
    "start": "1206350",
    "end": "1212140"
  },
  {
    "text": "So if you imagine,\nyou're going to start with just a block of metal. And every voxel-- you're going\nto divide it into 3D pixels,",
    "start": "1212140",
    "end": "1220540"
  },
  {
    "text": "voxels. Every voxel, you can\neither put metal or air. This is called\ntopology optimization.",
    "start": "1220540",
    "end": "1226690"
  },
  {
    "text": "Topology in math is how\nthings are connected. Are their hole-- is it one\nbig thing or are there holes?",
    "start": "1226690",
    "end": "1233770"
  },
  {
    "text": "How many holes are\nthere and so forth. And we're going to leave\nthis completely free. So the computer is going to\ndecide where to put the holes",
    "start": "1233770",
    "end": "1239980"
  },
  {
    "text": "or if there are\nholes and so forth. And I think what\nthey're doing is",
    "start": "1239980",
    "end": "1245170"
  },
  {
    "text": "they're maximizing the\nweight with a given amount of material.",
    "start": "1245170",
    "end": "1250370"
  },
  {
    "text": "Either that or they're fixing\nthe weight that it has to hold and minimizing. Probably, they're minimizing\nthe amount of material subject",
    "start": "1250370",
    "end": "1257960"
  },
  {
    "text": "to a constraint of a\ncertain amount of weight. So there's zillions\nof parameters here. And this is a movie.",
    "start": "1257960",
    "end": "1265460"
  },
  {
    "text": "Let's see, does this work? Present on this device. ",
    "start": "1265460",
    "end": "1271200"
  },
  {
    "text": "OK. So this is a movie of that\noptimization you can see. So it starts with just a--",
    "start": "1271200",
    "end": "1278090"
  },
  {
    "text": "their starting thing is\njust a solid block of metal. So that certainly will\nsupport your weight,",
    "start": "1278090",
    "end": "1283610"
  },
  {
    "text": "but it's very, very heavy. And so now they're going\nto try and optimize",
    "start": "1283610",
    "end": "1288818"
  },
  {
    "text": "it to minimize that\namount of weight, subject to the\nconstraint that it still has to support a person right.",
    "start": "1288818",
    "end": "1294799"
  },
  {
    "text": "And this thing-- even though\nthere's a zillion parameters, going in the downhill\ndirection allows",
    "start": "1294800",
    "end": "1302330"
  },
  {
    "text": "you to continuously evolve\nthat into this structure. And so it continuously\nshaves away",
    "start": "1302330",
    "end": "1312710"
  },
  {
    "text": "metal, varying all million\nparameters at once. And it decides to put these\nweird this arrange of holes",
    "start": "1312710",
    "end": "1321890"
  },
  {
    "text": "here to make this chair. And so this is in\nsome art exhibit. I think they said they then\nuse some kind of 3D printing",
    "start": "1321890",
    "end": "1330410"
  },
  {
    "text": "somehow to make life-size chair\nout of metal with this design, and it works.",
    "start": "1330410",
    "end": "1335450"
  },
  {
    "text": "And they displayed it. More practically, I think,\nin the first lecture, Alan showed an example\nof an airplane wing",
    "start": "1335450",
    "end": "1340850"
  },
  {
    "text": "that was designed by\nthis kind of thing. And there's lots of\nexamples of this. So the key thing in this\nthat makes this practical",
    "start": "1340850",
    "end": "1349770"
  },
  {
    "text": "is your ability to\ncalculate derivatives. You need to be able to\ncalculate the derivative of the strength of this or\nsome function of a solution",
    "start": "1349770",
    "end": "1358350"
  },
  {
    "text": "with respect to all\nmillion parameters of every voxel, the density of\nthe material at every voxel.",
    "start": "1358350",
    "end": "1367470"
  },
  {
    "text": "So as I said, this is sometimes\ncalled backpropagation in neural networks.",
    "start": "1367470",
    "end": "1372510"
  },
  {
    "text": "It really is just doing the\nchain rule left or right. And so I want to talk\nabout that in this kind",
    "start": "1372510",
    "end": "1377670"
  },
  {
    "text": "of specific circumstance. You can really see how it\nmakes a huge difference. So suppose we're\ncomputing a function.",
    "start": "1377670",
    "end": "1384179"
  },
  {
    "text": "We have some scalar\nfunction f of x. But our x we get by solving\na system of equations, ax",
    "start": "1384180",
    "end": "1392520"
  },
  {
    "text": "equals b, where the matrix\ndepends on some parameters. And I want to compute the\nderivative of f with respect",
    "start": "1392520",
    "end": "1398730"
  },
  {
    "text": "to the parameters in the matrix. So x is a inverse b. So I'm really computing\nf of A inverse",
    "start": "1398730",
    "end": "1404820"
  },
  {
    "text": "b, where the parameters\nare inside the matrix, and I want to apply\nthe chain rule.",
    "start": "1404820",
    "end": "1410600"
  },
  {
    "text": "So we can just do the-- so this is-- yeah.",
    "start": "1410600",
    "end": "1418309"
  },
  {
    "text": "So we have f of x\nand then x of p, and I want to propagate\nmy chain rules to that. So I to ask what's\ndf, the change in f,",
    "start": "1418310",
    "end": "1425300"
  },
  {
    "text": "for a small change\nin the parameters p? So let's just do chain rule.",
    "start": "1425300",
    "end": "1430410"
  },
  {
    "text": "So first of all, we\nhave f prime of x. So this is just the derivative\nof f with respect to x.",
    "start": "1430410",
    "end": "1435470"
  },
  {
    "text": "So that's just a row vector. So x is a column vector. f is a scalar.",
    "start": "1435470",
    "end": "1440630"
  },
  {
    "text": "So f prime is a row vector. It's just the transpose\nof our gradient of f. But we're not done yet because\nthat gets multiplied by dx.",
    "start": "1440630",
    "end": "1448050"
  },
  {
    "text": "But that's the change\nin the solution. And that's not what we want.",
    "start": "1448050",
    "end": "1453150"
  },
  {
    "text": "We want is what happens when\nyou change the parameters or when you change the matrix. So we do the chain rule.",
    "start": "1453150",
    "end": "1459660"
  },
  {
    "text": "And we have to take the\nderivative-- x is A inverse b, so we need to take dA.",
    "start": "1459660",
    "end": "1465440"
  },
  {
    "text": "And b depend on the parameters. That's just a constant. So we just need dA inverse b. ",
    "start": "1465440",
    "end": "1472430"
  },
  {
    "text": "Then we saw I think last\nlecture that the d of A inverse",
    "start": "1472430",
    "end": "1478850"
  },
  {
    "text": "is equal to minus A\ninverse dA inverse. Do you remember that?",
    "start": "1478850",
    "end": "1484220"
  },
  {
    "text": "We derived that last term\njust by taking the product rule AA inverse\nequals I. d of that,",
    "start": "1484220",
    "end": "1490220"
  },
  {
    "text": "we derive this rule--\nthe D of A inverse is minus A inverse\nDa A inverse-- really useful formula.",
    "start": "1490220",
    "end": "1496970"
  },
  {
    "text": "And so now if we group this,\nlet's look at this expression. You get A inverse\nb, which is just x.",
    "start": "1496970",
    "end": "1504870"
  },
  {
    "text": "So you're really\ncomputing f prime, which is a row vec, times A inverse--",
    "start": "1504870",
    "end": "1511640"
  },
  {
    "text": "so inverse of our\nsystem matrix-- times the change in the\nmatrix to the parameters",
    "start": "1511640",
    "end": "1519760"
  },
  {
    "text": "times x times A inverse b.",
    "start": "1519760",
    "end": "1526060"
  },
  {
    "text": "So the whole trick,\nthe whole stupid trick, which becomes obvious\nonce you see it,",
    "start": "1526060",
    "end": "1532660"
  },
  {
    "text": "is just to take this product\nand multiply it left to right. So that means you want\nto put parentheses",
    "start": "1532660",
    "end": "1539820"
  },
  {
    "text": "around the F prime A inverse. Why is that? Because that's\nindependent of dA.",
    "start": "1539820",
    "end": "1549320"
  },
  {
    "text": "That's one vector. That's solving one\nsystem of equations. If you do a row vector\ntimes a matrix on the left,",
    "start": "1549320",
    "end": "1557870"
  },
  {
    "text": "that's one matrix\nvector multiply. So that's the same thing as\nA inverse transpose times F.",
    "start": "1557870",
    "end": "1564380"
  },
  {
    "text": "Or equivalently, if you call\nthis-- this is a row vector. If you call this\nv transpose, it's the same thing as\nsolving A transpose V",
    "start": "1564380",
    "end": "1571820"
  },
  {
    "text": "equals f prime transpose. So that means this is\ncalled the adjoint equation.",
    "start": "1571820",
    "end": "1578490"
  },
  {
    "text": "So you solved--\nthink what you do. You first solve one\nequation Ax equals to get x.",
    "start": "1578490",
    "end": "1584480"
  },
  {
    "text": "That might be really expensive\nbecause this is a big physics simulation. A is a huge matrix.",
    "start": "1584480",
    "end": "1591530"
  },
  {
    "text": "But then you solve another\nequation with the same matrix, just transposed, to get v.\nAnd then once you have that,",
    "start": "1591530",
    "end": "1602710"
  },
  {
    "text": "then the derivative with\nrespect to any parameter is just v transpose dAx, which\nmeans if you have a parameter,",
    "start": "1602710",
    "end": "1610620"
  },
  {
    "text": "this should be a pk out\nof that little box there. It's a font problem. It should be a p usb k.",
    "start": "1610620",
    "end": "1616273"
  },
  {
    "text": "If you have a whole\nbunch of parameters, if you want the derivative\nof f with respect to pk,",
    "start": "1616273",
    "end": "1622529"
  },
  {
    "text": "that's just dividing\nboth sides by tpk. That's partial f partial pk is\njust df dpk is v transpose I",
    "start": "1622530",
    "end": "1636687"
  },
  {
    "text": "guess with the minus sign. I should have put\na minus sign there. ",
    "start": "1636687",
    "end": "1642870"
  },
  {
    "text": "How do I get back? And I tried this tablet\npresentation mode.",
    "start": "1642870",
    "end": "1648840"
  },
  {
    "text": "Let me just-- I'm going to switch\nto my computer. ",
    "start": "1648840",
    "end": "1655930"
  },
  {
    "text": "That's just because\nthis is annoying. ALAN EDELMAN: Did\nyou develop on a Mac and go to a PC or vice versa?",
    "start": "1655930",
    "end": "1662309"
  },
  {
    "text": "STEVEN G. JOHNSON: No. I developed it on Google\nSlides, but then I'm using the Google Slides\napp on my tablet, which is",
    "start": "1662310",
    "end": "1673760"
  },
  {
    "text": "a little bit flaky, apparently. So let me just share my screen. ALAN EDELMAN: Yeah,\nsometimes the trick",
    "start": "1673760",
    "end": "1680360"
  },
  {
    "text": "is to just do the PDF. STEVEN G. JOHNSON:\nWell, I wanted to show movies and things. So I can just share it on my--",
    "start": "1680360",
    "end": "1689680"
  },
  {
    "text": "on my laptop, it\nshould work better. Do you see that\nthe fonts come out?",
    "start": "1689680",
    "end": "1695570"
  },
  {
    "text": "Yes, good. So there should be\na minus sign here. ",
    "start": "1695570",
    "end": "1704620"
  },
  {
    "text": "We take this product. We're going to multiply\nit left to right, which means I multiply row\nvector-- the first thing I do",
    "start": "1704620",
    "end": "1711040"
  },
  {
    "text": "is multiply row vector times\na matrix, which gives you another row vector.",
    "start": "1711040",
    "end": "1716800"
  },
  {
    "text": "But that's multiplying\non the left by-- A inverse on the left\nis equivalent to solving",
    "start": "1716800",
    "end": "1722559"
  },
  {
    "text": "a system of equations with A\ntranspose for this row vector, where f prime transpose\nis on the right.",
    "start": "1722560",
    "end": "1731650"
  },
  {
    "text": "That's why it's called\nan adjoint equation because an adjoint\nis another name for a transpose of a real\nvector of a real matrix.",
    "start": "1731650",
    "end": "1739610"
  },
  {
    "text": "So you solved one\nsystem of equations to get your x, another\nsystem of equations with almost the same matrix\njust transposed to get v.",
    "start": "1739610",
    "end": "1746120"
  },
  {
    "text": "So it's equally easy or hard. And then your df is just--",
    "start": "1746120",
    "end": "1753860"
  },
  {
    "text": "there should be a\nminus sign here. Or I can just put the\nminus sign inside.",
    "start": "1753860",
    "end": "1760290"
  },
  {
    "text": "Well, it doesn't matter. ",
    "start": "1760290",
    "end": "1765720"
  },
  {
    "text": "df is just minus\nv transpose dAx. So now you have a whole\nbunch of parameters, pk.",
    "start": "1765720",
    "end": "1772650"
  },
  {
    "text": "So if you want df/dpk or just\ndivide both sides by dpk.",
    "start": "1772650",
    "end": "1778210"
  },
  {
    "text": "So df/dpk is the same\nthing as minus v transpose partial a partial plx.",
    "start": "1778210",
    "end": "1786389"
  },
  {
    "text": "So I just take for each-- if I have a million\nparameters, I just take a million dot products.",
    "start": "1786390",
    "end": "1792320"
  },
  {
    "text": "I only solve two\nsystems of equations, but this is the expensive part. This is solving my big\nstructural mechanics",
    "start": "1792320",
    "end": "1798830"
  },
  {
    "text": "equation or whatever it is. I solve it twice,\nand then I just take a bunch of dot products.",
    "start": "1798830",
    "end": "1804200"
  },
  {
    "text": "And in fact, usually,\nthis matrix is mostly 0. It's very sparse because usually\neach often each parameter only",
    "start": "1804200",
    "end": "1810590"
  },
  {
    "text": "affects a few entries\nof the matrix. So this dot product is\nextremely cheap and because you",
    "start": "1810590",
    "end": "1815630"
  },
  {
    "text": "don't need to multiply by 0's. So this is what allows you\nto do this efficiently.",
    "start": "1815630",
    "end": "1821120"
  },
  {
    "text": "You only have to solve\nthe equations twice, your huge systems of equations,\nyour physics equations, twice.",
    "start": "1821120",
    "end": "1827810"
  },
  {
    "text": "And then you can get both\nthe f and its gradient with respect to\nall the parameters.",
    "start": "1827810",
    "end": "1834850"
  },
  {
    "text": "So this is the adjoint method. It's exactly the same\nidea as back propagation in neural networks.",
    "start": "1834850",
    "end": "1840100"
  },
  {
    "text": "It's exactly what people in\nautomatic differentiation call reverse mode, but\nit's really just doing",
    "start": "1840100",
    "end": "1845710"
  },
  {
    "text": "chain rule left to right.  And I want to make\nsure you understand",
    "start": "1845710",
    "end": "1851100"
  },
  {
    "text": "why this is so important\nto do it left or right. So suppose we did\nit right to left.",
    "start": "1851100",
    "end": "1856960"
  },
  {
    "text": "So if I did right to left, and\nI wanted partial f partial pk,",
    "start": "1856960",
    "end": "1862260"
  },
  {
    "text": "then I have minus f prime,\na inverse, partial a,",
    "start": "1862260",
    "end": "1867340"
  },
  {
    "text": "partial pkx. It's the same equation, but\nit's going to put parentheses in a different place.",
    "start": "1867340",
    "end": "1872740"
  },
  {
    "text": "Suppose I did that. So for each parameter,\nthis is a vector.",
    "start": "1872740",
    "end": "1879600"
  },
  {
    "text": "And then multiplying by\nA inverse is a solve. It's a solving a huge\nsystem of equations.",
    "start": "1879600",
    "end": "1886420"
  },
  {
    "text": "So this is-- and that's\nexpensive part of the physics is usually solving this\nhuge system of equations.",
    "start": "1886420",
    "end": "1892620"
  },
  {
    "text": "So you have to--\nbut you have one of these vectors\nfor every parameter.",
    "start": "1892620",
    "end": "1897700"
  },
  {
    "text": "So if you just did\nit right to left, which is called forward mode\ndifferentiation, it's terrible.",
    "start": "1897700",
    "end": "1903252"
  },
  {
    "text": "You have to solve-- if you\nhave a million parameters, you have to solve a\nmillion mechanics problems",
    "start": "1903252",
    "end": "1910559"
  },
  {
    "text": "or whatever your physics\nproblem is for this. So this is called forward mode.",
    "start": "1910560",
    "end": "1917000"
  },
  {
    "text": "And this is very\nbad basically when you have lots of parameters.",
    "start": "1917000",
    "end": "1922040"
  },
  {
    "text": "And so it turns out this\nreverse mode differentiation is really good. That's what you should\nuse when you have lots",
    "start": "1922040",
    "end": "1927890"
  },
  {
    "text": "of inputs and only one output. Forward mode is\ngoing to turn out to be good when you\nhave lots of outputs",
    "start": "1927890",
    "end": "1933679"
  },
  {
    "text": "but only one input or\na small number input. So yeah, actually,\nI wrote this down.",
    "start": "1933680",
    "end": "1939680"
  },
  {
    "text": "Right-to-left forward\nmode is better when you have one\ninput and many outputs. Left-to-right, which is\nalso called backward mode,",
    "start": "1939680",
    "end": "1945082"
  },
  {
    "text": "also called adjoint methods. It's also called\nbackpropagation. It's a sort of\nthing that's been-- it has a lot of\nnames because it's",
    "start": "1945083",
    "end": "1950337"
  },
  {
    "text": "been I think rediscovered\nindependently multiple times. It's just left to\nright chain rule.",
    "start": "1950337",
    "end": "1956642"
  },
  {
    "text": "It's better when you have\none output or a few outputs and lots of inputs. And so if you're using automatic\ndifferentiation, most of them",
    "start": "1956642",
    "end": "1964460"
  },
  {
    "text": "will use the term reverse\nmode or forward mode. So in Jax, for example,\nwhich is the Python thing.",
    "start": "1964460",
    "end": "1971940"
  },
  {
    "text": "They'll call Jack forward. In Julia, there's a\npackage called forward diff, which stands for\nforward mode differentiation.",
    "start": "1971940",
    "end": "1978424"
  },
  {
    "text": "We're going to talk pretty\nsoon about dual numbers. That's forward\nmode, and so forth. Another thing you\nmight come to mind",
    "start": "1978425",
    "end": "1984680"
  },
  {
    "text": "is to use finite differences. So finite differences, if\nyou have lots of parameters,",
    "start": "1984680",
    "end": "1994470"
  },
  {
    "text": "it's also just as bad.\nright so if imagine you're using finite differences. So you're going to approximate\nyour partial derivative",
    "start": "1994470",
    "end": "2000260"
  },
  {
    "text": "with respect to each parameter. What do you do? You take each parameter--\nyou take f and you perturb",
    "start": "2000260",
    "end": "2007310"
  },
  {
    "text": "each parameter by epsilon. So you think of ek,\nhere is the unit vector in the direction of pk.",
    "start": "2007310",
    "end": "2014790"
  },
  {
    "text": "So you take each parameter. You perturb it by epsilon. In the case direction\nis [INAUDIBLE] fp",
    "start": "2014790",
    "end": "2022370"
  },
  {
    "text": "divided by epsilon. That gives you one\npartial derivative. But now if you have\na million parameters,",
    "start": "2022370",
    "end": "2027590"
  },
  {
    "text": "you need to do this\na million times, each one is a separate solve. You need to solve for--",
    "start": "2027590",
    "end": "2034250"
  },
  {
    "text": "you're perturbing the\nparameters one by one and, then you have to\nsolve it one by one. So again, it's a disaster.",
    "start": "2034250",
    "end": "2040309"
  },
  {
    "text": "If you had to use\nfinite differences, you would not be able to do\nneural networks or topology",
    "start": "2040310",
    "end": "2047150"
  },
  {
    "text": "optimization of chairs\nor airplane wings or things like that. And you can generalize\nit in other ways.",
    "start": "2047150",
    "end": "2053280"
  },
  {
    "text": "So that was adjoint methods\nfor linear equations.",
    "start": "2053280",
    "end": "2058860"
  },
  {
    "text": "So this is-- so you're\nsolving f of x, where x solves a linear equation, but\nyou can do the same thing where",
    "start": "2058860",
    "end": "2065780"
  },
  {
    "text": "f solves a nonlinear equation. Suppose you're solving--\nyou're computing",
    "start": "2065780",
    "end": "2071658"
  },
  {
    "text": "f of x, where x is your\nphysics, p is your parameters, like your shape of\nyour airplane wing.",
    "start": "2071659",
    "end": "2078379"
  },
  {
    "text": "But now to solve for your\nstresses or whatever, your physical equations\nare not linear.",
    "start": "2078380",
    "end": "2085710"
  },
  {
    "text": "So what do nonlinear\nequations look like? In general, nonlinear\nequations, you can write them as a\nroot-finding problem. You have n parameters.",
    "start": "2085710",
    "end": "2093289"
  },
  {
    "text": "You have n physics unknowns, and\nyou have n nonlinear equations.",
    "start": "2093289",
    "end": "2098680"
  },
  {
    "text": "So you have some\nnonlinear equations. It's called g that depend\non the parameters, that depend on the\nphysics, and you're",
    "start": "2098680",
    "end": "2104700"
  },
  {
    "text": "trying to set these equal to 0. So these are-- g is\nnot a scalar here.",
    "start": "2104700",
    "end": "2110160"
  },
  {
    "text": "Its n component vector. So it's exactly like\nwhat we talked about with Newton's method.",
    "start": "2110160",
    "end": "2116100"
  },
  {
    "text": "So how would you solve\nthis in the first place? You compute the\nJacobian of this thing, and you do a sequence of\nlinear solves to converge",
    "start": "2116100",
    "end": "2127070"
  },
  {
    "text": "towards the solution. But now how do we take the\nderivative of f with respect",
    "start": "2127070",
    "end": "2133380"
  },
  {
    "text": "to p? So now, again, we want\nto do the chain rule. So df is f prime of x dx.",
    "start": "2133380",
    "end": "2142369"
  },
  {
    "text": "That's the derivative of f of x. So f prime is\nstill a row vector.",
    "start": "2142370",
    "end": "2148440"
  },
  {
    "text": "What is dx? So now we need to think\nabout the chain rule. So we need to go back and\nthink about the derivative",
    "start": "2148440",
    "end": "2154950"
  },
  {
    "text": "of this equation. So this is our\nequation that we're going to use to solve for x.",
    "start": "2154950",
    "end": "2161660"
  },
  {
    "text": "So g equals 0. That means dg is also 0.",
    "start": "2161660",
    "end": "2169030"
  },
  {
    "text": "The solution g is\nalways a constant. So d of 0 is 0. But then if we do--",
    "start": "2169030",
    "end": "2176391"
  },
  {
    "text": "you can think of\nthis as two terms. There's dg, partial\ng partial p times dp.",
    "start": "2176392",
    "end": "2183610"
  },
  {
    "text": "So I'm sneaking back\nin some 18.02 notation. But think of this as like\na Jacobian matrix now.",
    "start": "2183610",
    "end": "2189910"
  },
  {
    "text": "It's not necessarily\na scalar derivative. So this is the\nderivative aspect of dp,",
    "start": "2189910",
    "end": "2197192"
  },
  {
    "text": "the change in the parameters. There's also the derivative\nwith respectto x, which, again-- think of this as an n\nby n Jacobian matrix",
    "start": "2197192",
    "end": "2204280"
  },
  {
    "text": "of the changes in\nall n components of g with all n components of x dx.",
    "start": "2204280",
    "end": "2210160"
  },
  {
    "text": "So we're just linearizing\nthis nonlinear function in two variables, which we haven't-- two parameters, which\nwe haven't done so far.",
    "start": "2210160",
    "end": "2216740"
  },
  {
    "text": "So I'm just kind of\nintroducing this notation. So if we write this down,\nand we just solve it for dx,",
    "start": "2216740",
    "end": "2225805"
  },
  {
    "text": "I move this to the\nright hand side, I get a minus this on\nthe right hand side,",
    "start": "2225805",
    "end": "2232690"
  },
  {
    "text": "and I get a Jacobian\ninverse times dp. Does everyone see this?",
    "start": "2232690",
    "end": "2237780"
  },
  {
    "text": "So I took this. I linearized it with two\nJacobian matrices with respect",
    "start": "2237780",
    "end": "2243400"
  },
  {
    "text": "to the parameters,\nrespects to the x's. And then I solve for\ndx in terms of dp.",
    "start": "2243400",
    "end": "2249430"
  },
  {
    "text": "And you can see\nit's this equation. And then we plug that in for dx.",
    "start": "2249430",
    "end": "2255700"
  },
  {
    "text": "And so what you get\nis you get f prime of x here with a\nminus sign from here",
    "start": "2255700",
    "end": "2264319"
  },
  {
    "text": "times this inverse\nJacobian matrix times this other Jacobian matrix\nof the parameters times dp.",
    "start": "2264320",
    "end": "2276700"
  },
  {
    "text": "And notice that this\nJacobian matrix is exactly the same Jacobian matrix\nyou would use in the Newton solver for x So if you're\nsolving nonlinear equations,",
    "start": "2276700",
    "end": "2284290"
  },
  {
    "text": "you already have this Jacobian. And you're already solving\nsystems with this--",
    "start": "2284290",
    "end": "2290381"
  },
  {
    "text": "you're inverting this\nJacobian or solving a system of equations. And so, again, you\ndo the same trick.",
    "start": "2290382",
    "end": "2296859"
  },
  {
    "text": "The whole trick is, to\ncompute df, you just multiply this left to right.",
    "start": "2296860",
    "end": "2302910"
  },
  {
    "text": "So this is a row vector. This is an inverse Jacobian.",
    "start": "2302910",
    "end": "2308280"
  },
  {
    "text": "You call this-- row vector\ntimes matrix is a row vector. Call that v transpose.",
    "start": "2308280",
    "end": "2314530"
  },
  {
    "text": "So multiplying\nthis by the inverse is equivalent to solving\na system of equations",
    "start": "2314530",
    "end": "2320250"
  },
  {
    "text": "with the transpose of\nthe Jacobian matrix. So again, we call this\nan adjoint equation",
    "start": "2320250",
    "end": "2325349"
  },
  {
    "text": "or a transpose equation\nfor the adjoint solution v. And on the right hand side\nis the transpose of this row",
    "start": "2325350",
    "end": "2332940"
  },
  {
    "text": "vector f prime. ",
    "start": "2332940",
    "end": "2339390"
  },
  {
    "text": "So we solve-- first, we solve\na nonlinear system of equations",
    "start": "2339390",
    "end": "2344430"
  },
  {
    "text": "to get x. And we do that by a\nsequence of linear solves probably, using this\ninverse Jacobian matrix.",
    "start": "2344430",
    "end": "2351710"
  },
  {
    "text": "We solve with many times. And then we solve one\nmore linear solve just",
    "start": "2351710",
    "end": "2358410"
  },
  {
    "text": "with the transposed\nJacobian matrix. And then once you\nhave that, then you",
    "start": "2358410",
    "end": "2364550"
  },
  {
    "text": "can get the derivatives\nbecause this is just-- if you want df/dp,\nit's just a dot product",
    "start": "2364550",
    "end": "2373630"
  },
  {
    "text": "of this transpose\nwith dg/dp times-- ",
    "start": "2373630",
    "end": "2380440"
  },
  {
    "text": "the dp cancels. That's it, so it's just\na bunch of dot products.",
    "start": "2380440",
    "end": "2385910"
  },
  {
    "text": "So this is called-- by the way, this formula where\nyou linearize the nonlinear",
    "start": "2385910",
    "end": "2391670"
  },
  {
    "text": "equations is also called the\nimplicit function theorem. And so these are--",
    "start": "2391670",
    "end": "2398360"
  },
  {
    "text": "this is all the rage now. You have what's called a\nbilevel optimization problem or problems with\noptimizing problems",
    "start": "2398360",
    "end": "2405080"
  },
  {
    "text": "with implicit equations. And it's really, really\npowerful to be able to do this.",
    "start": "2405080",
    "end": "2412790"
  },
  {
    "text": "And actually, the automatic\ndifferentiation systems usually have to be told\nthat you're doing this. They don't know-- if you\nsolve this nonlinear system",
    "start": "2412790",
    "end": "2423320"
  },
  {
    "text": "by a sequence of Newton's steps,\nthe automatic differentiation doesn't know that\nwhat you're solving",
    "start": "2423320",
    "end": "2429980"
  },
  {
    "text": "is g equals 0 by\nNewton's method. It thinks, oh, you're just\ndoing a bunch of steps, and it tries to do the chain\nrule, propagate it through all",
    "start": "2429980",
    "end": "2437390"
  },
  {
    "text": "those Newton's steps. So it's basically trying\nto exactly differentiate the error in your\nNewton's solver.",
    "start": "2437390",
    "end": "2445060"
  },
  {
    "text": "If you tell it, if you know\nthat you're solving a nonlinear systems, you know that this\nis what you should be doing.",
    "start": "2445060",
    "end": "2452390"
  },
  {
    "text": "You should really only be\nfor the adjoint problem, for the back propagation,\nor for reverse mode,",
    "start": "2452390",
    "end": "2457450"
  },
  {
    "text": "you should really only do\neffectively one Newton step, one linear solve.",
    "start": "2457450",
    "end": "2462829"
  },
  {
    "text": "So it's really important to\nbe able to do this basically to know that you have to tell\nthe automatic differentiation",
    "start": "2462830",
    "end": "2470680"
  },
  {
    "text": "to do this either. You either supply a manual\nderivative of this part, or there are now ways\nto basically tell",
    "start": "2470680",
    "end": "2478390"
  },
  {
    "text": "the automatic differentiation\nsolver that one step of it,",
    "start": "2478390",
    "end": "2484036"
  },
  {
    "text": "is solving a nonlinear system. And it should do this for you. ",
    "start": "2484036",
    "end": "2490910"
  },
  {
    "text": "Yes, that's my last slide here. So even if you're using\nautomatic differentiation, and the computer is taking\nall your derivatives for you,",
    "start": "2490910",
    "end": "2498080"
  },
  {
    "text": "doing all your chain\nrules and so forth, it's important to have some idea of\nwhat's going on under the hood.",
    "start": "2498080",
    "end": "2503570"
  },
  {
    "text": "Understand adjoint methods. Understand reverse mode\nversus forward mode. Because, first of all, you need\nto know what is forward mode?",
    "start": "2503570",
    "end": "2511160"
  },
  {
    "text": "What is reverse mode? When should you use forward? When should you use reverse?",
    "start": "2511160",
    "end": "2516800"
  },
  {
    "text": "Another problem\nis that if you're doing you know physics,\nespecially, a lot of physics methods, you're calling large\nsoftware packages written",
    "start": "2516800",
    "end": "2524150"
  },
  {
    "text": "over decades in\nvarious languages that can't be differentiated\nby automatic differentiation.",
    "start": "2524150",
    "end": "2531230"
  },
  {
    "text": "Jax can understand\nPython programs, but it doesn't really\nunderstand Python programs that call libraries\nwritten in Fortran.",
    "start": "2531230",
    "end": "2537140"
  },
  {
    "text": "So, often, you need-- some parts\nof it, it just can't handle. And so but it turns\nout you only need",
    "start": "2537140",
    "end": "2543110"
  },
  {
    "text": "to write down the derivative for\nthat part or a vector Jacobian product, what's\ncalled, for that part.",
    "start": "2543110",
    "end": "2549840"
  },
  {
    "text": "And then it can do the rest\nof the chain rule for you. But you really need to know\nthese adjoint methods in order",
    "start": "2549840",
    "end": "2555390"
  },
  {
    "text": "to do that in a reasonable way. And then even if\nyou have Ad, even if it works, if you\nhave a model that",
    "start": "2555390",
    "end": "2562369"
  },
  {
    "text": "involves an approximate\ncalculation, like a Newton solve or something like that,\nautomatic differentiation",
    "start": "2562370",
    "end": "2567837"
  },
  {
    "text": "typically doesn't\nknow this and spends a lot of effort trying to\ndifferentiate exactly the error in your approximation.",
    "start": "2567837",
    "end": "2573680"
  },
  {
    "text": "It wastes a lot of time. So it's really, really\nuseful to know that",
    "start": "2573680",
    "end": "2579260"
  },
  {
    "text": "and to know, oh, I need to\ntell it something to help it. So examples when you're\nsolving nonlinear systems",
    "start": "2579260",
    "end": "2586340"
  },
  {
    "text": "by iterative methods\nlike Newton equations, or even if you're approximating\nthings like by discretizing",
    "start": "2586340",
    "end": "2593359"
  },
  {
    "text": "physics, like finite\nelement methods, often you want to differentiate\nthe physics before you",
    "start": "2593360",
    "end": "2598549"
  },
  {
    "text": "discretize, rather\nthan have it try to do automatic differentiation\non the discretization itself.",
    "start": "2598550",
    "end": "2605210"
  },
  {
    "text": "Anyway, so I'll stop there. And we'll be spending more\ntime on the nuts and bolts",
    "start": "2605210",
    "end": "2613060"
  },
  {
    "text": "of automatic differentiation. And I'll also give\nsome more links to further reading on adjoint\nmethods and backward mode",
    "start": "2613060",
    "end": "2620110"
  },
  {
    "text": "and vector Jacobian products\nand these kinds of-- these kinds of systems I've--",
    "start": "2620110",
    "end": "2627640"
  },
  {
    "text": "there's lots of notes\nwritten in web pages on these kinds of things. ALAN EDELMAN: And I'll\nbe back live on Friday.",
    "start": "2627640",
    "end": "2632980"
  },
  {
    "text": "STEVEN G. JOHNSON: Yes. And so maybe Friday,\nI don't know if you're",
    "start": "2632980",
    "end": "2638120"
  },
  {
    "text": "going to do your dual numbers. ALAN EDELMAN: I\ncould do dual numbers and even reverse\nand forward mode the graph theory if you want.",
    "start": "2638120",
    "end": "2644859"
  },
  {
    "text": "STEVEN G. JOHNSON: Yeah. So maybe I think\nmaybe it's time to do some automatic differentiation.",
    "start": "2644860",
    "end": "2650680"
  },
  {
    "text": "ALAN EDELMAN: OK, sounds good. All right, see you then. ",
    "start": "2650680",
    "end": "2666000"
  }
]