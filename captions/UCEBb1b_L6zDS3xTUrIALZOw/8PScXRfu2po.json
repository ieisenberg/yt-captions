[
  {
    "text": "SPEAKER: The following content\nis provided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6600"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation, or to view\nadditional material from",
    "start": "6600",
    "end": "12815"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu. ",
    "start": "12815",
    "end": "22369"
  },
  {
    "text": "PROFESSOR: OK I want to review\na little bit what we said about detection at the end of\nlast hour, last hour and a",
    "start": "22370",
    "end": "29550"
  },
  {
    "text": "half, because we were going\nthrough it relatively quickly.",
    "start": "29550",
    "end": "35620"
  },
  {
    "text": "Detection is a very\nfunny subject. Particularly the way that we\ndo it here because we go",
    "start": "35620",
    "end": "43850"
  },
  {
    "text": "through a bunch of very,\nvery simple steps. Everything looks trivial-- I hope it looks trivial-- as\nwe go, at least after you",
    "start": "43850",
    "end": "51920"
  },
  {
    "text": "think about it for awhile, and\nyou work with it for awhile, you will come back and look at\nit and you will say, \"yes, in",
    "start": "51920",
    "end": "58010"
  },
  {
    "text": "fact that is trivial.\" Nothing\nwhen you look at it for the first time is trivial.",
    "start": "58010",
    "end": "64170"
  },
  {
    "text": "And the kind of detection\nproblems that we're interested in is--",
    "start": "64170",
    "end": "71460"
  },
  {
    "text": "to start out with-- we want to\nlook just at binary detection. We're sending a binary signal,\nit's going to go through some",
    "start": "71460",
    "end": "79490"
  },
  {
    "text": "signal encoder, which is the\nkind of channel encoder we've been thinking about.",
    "start": "79490",
    "end": "86270"
  },
  {
    "text": "It's going to go through a\nbaseband modulator, a baseband",
    "start": "86270",
    "end": "92460"
  },
  {
    "text": "to passband modulator. It's going to have white\nGaussian noise, or some kind of noise, added to it.",
    "start": "92460",
    "end": "99230"
  },
  {
    "text": "It's going to come out\nthe other end. We come back from passband\nto baseband.",
    "start": "99230",
    "end": "104899"
  },
  {
    "text": "We then go through a baseband\ndemodulator. We then sample at that point.",
    "start": "104900",
    "end": "110330"
  },
  {
    "text": "And, the point is, when you're\nall done with all of that, what you've done is you've\nstarted out sending either",
    "start": "110330",
    "end": "120869"
  },
  {
    "text": "plus a or minus a as a one-\ndimensional numerical signal.",
    "start": "120870",
    "end": "127220"
  },
  {
    "text": "And when you're all through,\nthere's some one- dimensional number that comes out, v. And\non the basis of that one-",
    "start": "127220",
    "end": "135319"
  },
  {
    "text": "dimensional number, v, you're\nsupposed to guess whether the output is zero or one.",
    "start": "135320",
    "end": "141250"
  },
  {
    "text": "Now, one of the things that\nwe're doing right now is we're simplifying the problem in the\nsense that we're not looking",
    "start": "141250",
    "end": "148660"
  },
  {
    "text": "at a sequence of inputs coming\nin, and we're not looking at a sequence of outputs\ncoming out.",
    "start": "148660",
    "end": "154560"
  },
  {
    "text": "We're only looking at a single\ninput coming in. In other words, you build this\npiece of communication",
    "start": "154560",
    "end": "161190"
  },
  {
    "text": "equipment, you get it all tuned\nup, you get it into steady state. You send one bit.",
    "start": "161190",
    "end": "167910"
  },
  {
    "text": "You receive something. You try to guess at the\nreceiver, what was sent. And at that point you tear the\nwhole thing down and you wait",
    "start": "167910",
    "end": "175710"
  },
  {
    "text": "a year until you've set\nit up perfectly again. You send another bit. And we're not going to worry at\nall about what happens with",
    "start": "175710",
    "end": "183250"
  },
  {
    "text": "the sequence, we're only\ngoing to worry about this one shot problem. You sort of have some kind of\nclue that if you send the",
    "start": "183250",
    "end": "194950"
  },
  {
    "text": "whole sequence of bits, in a\nsystem like this, and you don't have intersymbol\ninterference, and the noise is",
    "start": "194950",
    "end": "201950"
  },
  {
    "text": "white, so it's sort of\nindependent from time to time. You sort of have a clue that\nyou're going to get the same",
    "start": "201950",
    "end": "209210"
  },
  {
    "text": "answer whether you send the\nsequence of data or whether you just send a single bit.",
    "start": "209210",
    "end": "215290"
  },
  {
    "text": "And we're going to show later\nthat that, in fact, is true. But for the time being we want\nto understand what's going on,",
    "start": "215290",
    "end": "222810"
  },
  {
    "text": "and to understand what's going\non we take this simplest possible case where there's\nonly one bit that's being",
    "start": "222810",
    "end": "229350"
  },
  {
    "text": "transmitted.  It's the question, \"Are we going\nto destroy ourselves in",
    "start": "229350",
    "end": "236120"
  },
  {
    "text": "the next five years or not?\" And\nthis question is important to most of us, and at the output\nwe find out, in fact,",
    "start": "236120",
    "end": "245340"
  },
  {
    "text": "whether we're going to destroy\nourselves or not. So it's one bit, but it's\none important bit.",
    "start": "245340",
    "end": "250600"
  },
  {
    "text": "OK, why are we doing\nthings this way? Want to tell you a little story\nabout the first time I",
    "start": "250600",
    "end": "258329"
  },
  {
    "text": "really talked to\nClaude Shannon. I was a young member of the\nfaculty at that time, and I",
    "start": "258330",
    "end": "265169"
  },
  {
    "text": "was working on a problem\nwhich I thought was really a neat problem. It was interesting\ntheoretically.",
    "start": "265170",
    "end": "271830"
  },
  {
    "text": "It was important practically. And I thought, \"gee, I finally\nhave something I can go to",
    "start": "271830",
    "end": "277949"
  },
  {
    "text": "this great man and talk to him\nabout.\" So I screwed up my courage for about two days.",
    "start": "277950",
    "end": "283440"
  },
  {
    "text": "Finally I saw his door open and\nhim sitting there, so I went in and started to tell\nhim about this problem.",
    "start": "283440",
    "end": "290639"
  },
  {
    "text": "He's a very kind person and he\nlistened very patiently. And after about 15 minutes\nhe said, \"My god!",
    "start": "290640",
    "end": "297510"
  },
  {
    "text": "I'm just sort of lost\nwith all of this. There's so much stuff going\non in this problem. Can't we simplify it a little\nbit by throwing out this kind",
    "start": "297510",
    "end": "305949"
  },
  {
    "text": "of practical constraint you've\nput on it?\" I said, \"yeah, I guess so.\" So we threw that\nout and the we went on for",
    "start": "305950",
    "end": "313040"
  },
  {
    "text": "awhile later, and then he\nsaid, \"My god, I'm still terribly confused about\nthis whole thing.",
    "start": "313040",
    "end": "318240"
  },
  {
    "text": "Why don't we simplify it\nin some other way?\" And this went on for\nabout an hour. As I say, he was a\nvery patient guy.",
    "start": "318240",
    "end": "326050"
  },
  {
    "text": "And at the end of an hour I was\ngetting really depressed. Because here was this beautiful\nproblem that I",
    "start": "326050",
    "end": "331920"
  },
  {
    "text": "thought was going to make me\nfamous, give me tenure, do all these neat things.",
    "start": "331920",
    "end": "337140"
  },
  {
    "text": "And here he'd reduced\nthe thing to a totally trivial toy problem. And we looked at it.",
    "start": "337140",
    "end": "342930"
  },
  {
    "text": "And we said, yes this is\na trivial toy problem. This is the answer. The problem is solved.",
    "start": "342930",
    "end": "348840"
  },
  {
    "text": "But so what? And then he suggested putting\nsome of those",
    "start": "348840",
    "end": "353919"
  },
  {
    "text": "constraints back in again. And as we started putting the\nconstraints back in, one- by-",
    "start": "353920",
    "end": "359370"
  },
  {
    "text": "one, we saw that each time we\nput a new constraint in-- since we understood the problem\nand its simplest",
    "start": "359370",
    "end": "366160"
  },
  {
    "text": "form-- putting the constraint\nin, it was still simple. And by the time we built the\nwhole thing back up again, it",
    "start": "366160",
    "end": "373060"
  },
  {
    "text": "was clear what the answer was. OK, in other words, what theory\nmeans is really solving",
    "start": "373060",
    "end": "382410"
  },
  {
    "text": "these toy problems. And solving the toy\nproblems first. And in terms of practice, some\npeople think the most",
    "start": "382410",
    "end": "389110"
  },
  {
    "text": "practical thing is\nto be practical. And the whole point of this\ncourse, and this particular",
    "start": "389110",
    "end": "398100"
  },
  {
    "text": "subject of detection is a\nwonderful example of this, the most practical thing is\nto be theoretical.",
    "start": "398100",
    "end": "407080"
  },
  {
    "text": "I mean, you need to add practice\nto the theory, but the way you do things is you\nstart with a theory-- which",
    "start": "407080",
    "end": "414260"
  },
  {
    "text": "means you start with the toy\nproblems, you build up from those toy problems, and after\nyou build up for awhile,",
    "start": "414260",
    "end": "420630"
  },
  {
    "text": "understanding what the practical\nproblem is also-- you then understand\nhow to deal with the practical problem.",
    "start": "420630",
    "end": "427199"
  },
  {
    "text": "And the practical engineer who\ndoesn't have any of that fundamental knowledge about\nhow to deal with these",
    "start": "427200",
    "end": "432970"
  },
  {
    "text": "problems is always submerged\nin a sea of complexity. Always doing simulations of\nsomething that he doesn't, he",
    "start": "432970",
    "end": "441080"
  },
  {
    "text": "or she doesn't understand. Always trying to interpret\nsomething from it, but with just too many things going on\nto have any idea of what it",
    "start": "441080",
    "end": "449440"
  },
  {
    "text": "really means. Ok, so that's why we're making\nthis trivial assumption here.",
    "start": "449440",
    "end": "454599"
  },
  {
    "text": "We're only putting one bit in. We're ignoring what\nhappens all the way through the system.",
    "start": "454600",
    "end": "461090"
  },
  {
    "text": "We only get one number out. We're going to assume that this\none number here is either",
    "start": "461090",
    "end": "466880"
  },
  {
    "text": "plus or minus a, plus\nthe Gaussian random noise variable. And we're not quite sure why\nit's going to be plus or minus",
    "start": "466880",
    "end": "475090"
  },
  {
    "text": "a, plus a Gaussian noise random\nvariable, but we're going to assume that\nfor the time being. OK?",
    "start": "475090",
    "end": "480630"
  },
  {
    "text": "So the detector observes the\nsample values of the random",
    "start": "480630",
    "end": "486610"
  },
  {
    "text": "variable for which this is the\nsample value, and then guesses",
    "start": "486610",
    "end": "491750"
  },
  {
    "text": "what the value of this random\nvariable, h, which is what we call the input now.",
    "start": "491750",
    "end": "496780"
  },
  {
    "text": "Because we view it from the\nstandpoint of the detector-- the detector has two possible\nhypotheses--",
    "start": "496780",
    "end": "504080"
  },
  {
    "text": "one is that a zero was\nsent, and the other that a one was sent. And on the basis of this\nobservation, you take first",
    "start": "504080",
    "end": "513690"
  },
  {
    "text": "the hypothesis zero and you\nsay, \"Is this a reasonable hypothesis?\" Then you look at\nthe hypothesis one, say, \"Is",
    "start": "513690",
    "end": "519839"
  },
  {
    "text": "this a reasonable hypothesis?\"\nAnd then you guess whether you",
    "start": "519840",
    "end": "526480"
  },
  {
    "text": "think zero was more likely or\none is more likely, given this observation that you've had.",
    "start": "526480",
    "end": "532070"
  },
  {
    "text": "So what they detector has,\nat this point, is a full statistical characterization\nof the entire problem.",
    "start": "532070",
    "end": "541180"
  },
  {
    "text": "Mainly you have a model\nof the problem. You understand every probability\nin the universe",
    "start": "541180",
    "end": "546340"
  },
  {
    "text": "that might have any\neffect on this. And what might have any\neffect on this--",
    "start": "546340",
    "end": "553240"
  },
  {
    "text": "as far as the way we've\nset up the problem-- is only the question of what\nare the probabilities that",
    "start": "553240",
    "end": "560060"
  },
  {
    "text": "you're going to send\none or the other of these signals here?",
    "start": "560060",
    "end": "565540"
  },
  {
    "text": "And conditional on each\nof these, what are the probabilities of this\nrandom variable",
    "start": "565540",
    "end": "570779"
  },
  {
    "text": "appearing at the output? Because you have to base your\ndecision only on this. So all of the probabilities\nonly give you",
    "start": "570780",
    "end": "577040"
  },
  {
    "text": "this one simple thing. Hypothesis testing, decision\nmaking, decoding, all mean the",
    "start": "577040",
    "end": "583310"
  },
  {
    "text": "same thing. They mean exactly\nthe same thing. ",
    "start": "583310",
    "end": "591970"
  },
  {
    "text": "And they're just done\nby different people. OK, so what that says is we're\nassuming the detector uses a",
    "start": "591970",
    "end": "598140"
  },
  {
    "text": "known probability model. And in designing the detector,\nyou know what that",
    "start": "598140",
    "end": "603200"
  },
  {
    "text": "probability model is. It might not be the right\nprobability model, and one of",
    "start": "603200",
    "end": "609050"
  },
  {
    "text": "the things that many people\ninterested in detection study is the question of when you\nthink the probability model is",
    "start": "609050",
    "end": "617920"
  },
  {
    "text": "one thing and it's actually\nsomething else, how well does the detection work? It's a little like the\nLempel-Ziv algorithm that we",
    "start": "617920",
    "end": "625690"
  },
  {
    "text": "studied earlier for doing\nsource coding.",
    "start": "625690",
    "end": "631040"
  },
  {
    "text": "Which is, how do you do source\ncoding when you don't know what the probabilities are? And we found the best way to\nstudy that, of course, was to",
    "start": "631040",
    "end": "638450"
  },
  {
    "text": "first find out how to do source\nencoding when you did know what the probabilities\nwere.",
    "start": "638450",
    "end": "643459"
  },
  {
    "text": "So we're doing the\nsame thing here. We assume the detector is\ndesigned to maximize the",
    "start": "643460",
    "end": "649090"
  },
  {
    "text": "probability of guessing\ncorrectly. In other words, it's trying\nto minimize the",
    "start": "649090",
    "end": "654240"
  },
  {
    "text": "probability of error. We call that a MAP detector--\nmaximum a posteriori",
    "start": "654240",
    "end": "659940"
  },
  {
    "text": "probability decoding. You can try to do\nother things.",
    "start": "659940",
    "end": "666830"
  },
  {
    "text": "You can say that there's a cost\nof one kind of error, and there's another cost of\nanother kind of error.",
    "start": "666830",
    "end": "673420"
  },
  {
    "text": "I mean, if you're doing medical\ntesting or something. If you guess wrong in one way,\nyou tell the patient there's",
    "start": "673420",
    "end": "680350"
  },
  {
    "text": "nothing wrong with them, the\npatient goes out, drops dead the next day. And you don't care about that,\nof course, but you care about",
    "start": "680350",
    "end": "687430"
  },
  {
    "text": "the fact that the patient is\ngoing to sue the hospital for 100 million dollars and you're\ngoing to lose your",
    "start": "687430",
    "end": "692600"
  },
  {
    "text": "job because of it. So there's a big cost to\nguessing wrong in that way.",
    "start": "692600",
    "end": "697860"
  },
  {
    "text": " But for now, we're not going\nto bother about the costs.",
    "start": "697860",
    "end": "705960"
  },
  {
    "text": "One of the things that you'll\nsee when we get all done is that putting in cost\ndoesn't make the",
    "start": "705960",
    "end": "711639"
  },
  {
    "text": "problem any harder, really. You really wind up with the\nsame kind of problem.",
    "start": "711640",
    "end": "717310"
  },
  {
    "text": "OK, so h is the random\nvariable that will be detected, and v is the\nrandom variable that's going to be observed.",
    "start": "717310",
    "end": "723590"
  },
  {
    "text": "The experiment is performed. Some sample value of v is\nobserved, and some sample",
    "start": "723590",
    "end": "730140"
  },
  {
    "text": "value of the hypothesis\nhas actually happened. In other words, what has\nhappened is you prepared the",
    "start": "730140",
    "end": "736320"
  },
  {
    "text": "whole system. Then at the input end to the\nwhole system, the input to the",
    "start": "736320",
    "end": "744350"
  },
  {
    "text": "channel, somebody has chosen\na one or a zero without the knowledge of the receiver.",
    "start": "744350",
    "end": "750920"
  },
  {
    "text": "That one or zero has been sent\nthrough this whole system, the receiver has observed some\noutput, v, so in fact we're",
    "start": "750920",
    "end": "758330"
  },
  {
    "text": "now dealing with the\nsample values of two different things. The sample value of the input,\nwhich is h, the sample value",
    "start": "758330",
    "end": "765509"
  },
  {
    "text": "of the output, which is v, and\nin terms of the sample value of the output, we're trying to\nguess what the sample value of",
    "start": "765510",
    "end": "772440"
  },
  {
    "text": "the input is. OK, an error then occurs if,\nafter the output chooses a",
    "start": "772440",
    "end": "780630"
  },
  {
    "text": "particular hypothesis as its\nguess, and that hypothesis,",
    "start": "780630",
    "end": "786030"
  },
  {
    "text": "then, is a function of\nwhat it receives. In other words, after you\nreceive something, what the",
    "start": "786030",
    "end": "792410"
  },
  {
    "text": "detector has to do is somehow\nmap what gets received, which",
    "start": "792410",
    "end": "797529"
  },
  {
    "text": "is some number, into\neither zero or one. It's like what a\nQuantizer does.",
    "start": "797530",
    "end": "804250"
  },
  {
    "text": "Mainly, it maps the whole\nregion into two different sub- regions.",
    "start": "804250",
    "end": "810360"
  },
  {
    "text": "Some things are mapped into\nzero, Some things are mapped into one.",
    "start": "810360",
    "end": "816020"
  },
  {
    "text": "This H bar then becomes a random\nvariable, but is a random variable that is a\nfunction of what's received.",
    "start": "816020",
    "end": "823399"
  },
  {
    "text": "So we have one random variable,\nH, which is what actually happened. There's another random variable,\nH hat, which is what",
    "start": "823400",
    "end": "830720"
  },
  {
    "text": "the detector guesses\nwill happen. This is an unusual random\nvariable, because it's not",
    "start": "830720",
    "end": "838030"
  },
  {
    "text": "determined ahead of time. It's determined only in terms\nof what you decide your",
    "start": "838030",
    "end": "844170"
  },
  {
    "text": "detection rule is going to be. This is a random variable that\nyou have some control over.",
    "start": "844170",
    "end": "849190"
  },
  {
    "text": "These other random variables\nyou have no control over at all.",
    "start": "849190",
    "end": "854540"
  },
  {
    "text": "So that's the random variable\nwe're going to choose. And, in fact, what we're going\nto do is we're going to say",
    "start": "854540",
    "end": "860329"
  },
  {
    "text": "what we want to do is this MAP\ndecoding, maximum a posteriori probability decoding, where\nwe're trying to minimize the",
    "start": "860330",
    "end": "867710"
  },
  {
    "text": "probability of screwing up. And we don't care whether we\nmake an error of one kind or",
    "start": "867710",
    "end": "875470"
  },
  {
    "text": "make an error of\nthe other kind. OK, is that formulation of the\nproblem crystal clear?",
    "start": "875470",
    "end": "884840"
  },
  {
    "text": "Anybody have any questions\nabout it? I mean, the easiest way to get\nscrewed up with detection is",
    "start": "884840",
    "end": "894730"
  },
  {
    "text": "at a certain point to be going\nthrough, studying a detection problem, and then you\nsuddenly realize you",
    "start": "894730",
    "end": "900700"
  },
  {
    "text": "don't understand what-- you don't understand what the\nwhole problem is about.",
    "start": "900700",
    "end": "906589"
  },
  {
    "text": "OK, let's assume we do know\nwhat the problem is, then.",
    "start": "906590",
    "end": "912650"
  },
  {
    "text": "In principle it's simple. Given a particular observed\nvalue, what we're going to do",
    "start": "912650",
    "end": "919190"
  },
  {
    "text": "is we're going to calculate\nthe-- what we call the a posteriori probability, the\nprobability given that",
    "start": "919190",
    "end": "925850"
  },
  {
    "text": "particular sample value\nof the observation-- we're going to calculate the\nprobability that what went",
    "start": "925850",
    "end": "932120"
  },
  {
    "text": "into the system is a zero\nand what went into the system is a one.",
    "start": "932120",
    "end": "938790"
  },
  {
    "text": "OK? This is the probability that\nj is the sample value of H,",
    "start": "938790",
    "end": "944930"
  },
  {
    "text": "conditional on what\nwe observed.",
    "start": "944930",
    "end": "950040"
  },
  {
    "text": "OK, if you can calculate this\nquantity, it tells you if I decide that--",
    "start": "950040",
    "end": "956030"
  },
  {
    "text": " if I guess that H is equal to\nj-- this in fact tells me that",
    "start": "956030",
    "end": "962870"
  },
  {
    "text": "this is the probability\nthat guess is correct. And if this is the probability\nthat guess is correct, and I",
    "start": "962870",
    "end": "970180"
  },
  {
    "text": "want to maximize my probability\nof guessing correctly, what do I do?",
    "start": "970180",
    "end": "975350"
  },
  {
    "text": "Well, what I do is my MAP\nrule is arg max of this",
    "start": "975350",
    "end": "982500"
  },
  {
    "text": "probability. And \"arg max\" means instead\nof trying to maximize this",
    "start": "982500",
    "end": "990830"
  },
  {
    "text": "instead of trying to maximize\nthis quantity over something, what we're doing is trying to\nfind the value of j, which",
    "start": "990830",
    "end": "997870"
  },
  {
    "text": "maximizes this. In other words, we calculate\nthis for each value of j, and then we picked the j for which\nthis quantity is largest.",
    "start": "997870",
    "end": "1006330"
  },
  {
    "text": "In other words, we maximize this\nbut we're not interested in the maximum value of\nit at this point.",
    "start": "1006330",
    "end": "1012860"
  },
  {
    "text": "We're interested in it later\nbecause that's the probability that we're choosing correctly. What we're interested in, for\nnow, is what is the hypothesis",
    "start": "1012860",
    "end": "1021100"
  },
  {
    "text": "that we're going to guess. OK? So this probability of being\ncorrect is going to the this",
    "start": "1021100",
    "end": "1028569"
  },
  {
    "text": "probability for this\nmaximal j. And when we average over v we\nget the overall probability of",
    "start": "1028570",
    "end": "1036125"
  },
  {
    "text": "being correct. There's a theorem which is\nstated in the notes, which is",
    "start": "1036125",
    "end": "1041839"
  },
  {
    "text": "one of the more trivial theorems\nyou can think of, which says that if you do the\nbest thing for every sample",
    "start": "1041840",
    "end": "1048240"
  },
  {
    "text": "point you, in fact, have done\nthe best thing on average. I think that's pretty clear.",
    "start": "1048240",
    "end": "1055050"
  },
  {
    "text": "If you, well I mean you can read\nit if you want to form a proof, but if you do the best\nthing all the time, then it's",
    "start": "1055050",
    "end": "1064650"
  },
  {
    "text": "the overall best thing. OK, so that's the general\nidea of detection.",
    "start": "1064650",
    "end": "1077570"
  },
  {
    "text": "And in doing this we have to\nbe able to calculate these probabilities, so that's\nthe only constraint.",
    "start": "1077570",
    "end": "1084820"
  },
  {
    "text": "These are probabilities, which\nmeans that this set of hypothesis is discrete.",
    "start": "1084820",
    "end": "1092080"
  },
  {
    "text": "If you have an uncountable\ninfinite number of hypotheses, at that point you're dealing\nwith an estimation problem.",
    "start": "1092080",
    "end": "1099070"
  },
  {
    "text": "Because you don't have any\nchance in hell of getting the right answer, exactly\nthe right answer.",
    "start": "1099070",
    "end": "1105700"
  },
  {
    "text": "And therefore you have to\nhave some criterion for how close you are. And that's what's important\nin estimation.",
    "start": "1105700",
    "end": "1112280"
  },
  {
    "text": "And here what's important is\nreally, do we guess right or don't we guess right.",
    "start": "1112280",
    "end": "1117299"
  },
  {
    "text": "And we don't, we don't care. There aren't any near\nmisses here. You either get it on the\nnose or you don't.",
    "start": "1117300",
    "end": "1125330"
  },
  {
    "text": "OK, so we want to study\nbinary detection now to start off with. We want to trivialize the\nproblem because even that",
    "start": "1125330",
    "end": "1131419"
  },
  {
    "text": "problem we just stated\nis too hard. So we're going to trivialize\nit in two ways. We're going to assume that there\nare only two hypotheses.",
    "start": "1131420",
    "end": "1138960"
  },
  {
    "text": "That it's a binary detection\nproblem. And we're also going to assume\nthat it's Gaussian noise.",
    "start": "1138960",
    "end": "1145370"
  },
  {
    "text": "And that will make it sort of\ntransparent what's happening.",
    "start": "1145370",
    "end": "1150380"
  },
  {
    "text": "So H takes the values\nzero or one. And we'll call the probabilities\nwith which it",
    "start": "1150380",
    "end": "1155880"
  },
  {
    "text": "takes those values\nP zero and P one. These are called a priori\nprobabilities. In other words, these are the\nprobabilities that the",
    "start": "1155880",
    "end": "1163610"
  },
  {
    "text": "hypothesis takes to value zero\nor one before seeing any observation.",
    "start": "1163610",
    "end": "1169680"
  },
  {
    "text": "And the probabilities after you\nsee the observation are called a posteriori\nprobabilities.",
    "start": "1169680",
    "end": "1175940"
  },
  {
    "text": "In other words, probabilities\nafter the observation and the probabilities before\nthe observation.",
    "start": "1175940",
    "end": "1181420"
  },
  {
    "text": "Up until about 1950\nstatisticians used to argue terribly about whether it was\nvalid to assume a priori",
    "start": "1181420",
    "end": "1190150"
  },
  {
    "text": "probabilities. And as you can see by thinking\nabout it a little bit, the",
    "start": "1190150",
    "end": "1196150"
  },
  {
    "text": "problem they were facing was\nthey couldn't separate the problem of choosing a\nmathematical model and",
    "start": "1196150",
    "end": "1202680"
  },
  {
    "text": "analyzing it from the problem\nof figuring out whether the",
    "start": "1202680",
    "end": "1207730"
  },
  {
    "text": "model was valid or not. And at that point people\nstudying in that area had not",
    "start": "1207730",
    "end": "1215090"
  },
  {
    "text": "gotten to the point where they\ncould say, \"Well, maybe I ought to analyze the problem for\ndifferent models, and then",
    "start": "1215090",
    "end": "1221660"
  },
  {
    "text": "after I understand what happens\nfor different models I then ought to go back because\nI'll know what's important to",
    "start": "1221660",
    "end": "1227470"
  },
  {
    "text": "find out in the real problem.\"\nBut up until that time, there",
    "start": "1227470",
    "end": "1233140"
  },
  {
    "text": "was just fighting\namong everyone. Bayes was the person who decided\nyou really ought to",
    "start": "1233140",
    "end": "1239309"
  },
  {
    "text": "assume that there's a\nmodel to start with. And he developed most\nof detection theory at an early time.",
    "start": "1239310",
    "end": "1246810"
  },
  {
    "text": "And people used to think that\nBayes was a terrible fraud. Because in fact he was using\nmodels of the problem rather",
    "start": "1246810",
    "end": "1253529"
  },
  {
    "text": "than nothing. But anyway, that's\nwhere we were.",
    "start": "1253530",
    "end": "1259429"
  },
  {
    "text": " We're also going to assume that,\nafter we get all through",
    "start": "1259430",
    "end": "1266910"
  },
  {
    "text": "with modulation and\ndemodulation, and we really want to look at a general\nproblem here. There's one discrete random\nvariable, H, one analog random",
    "start": "1266910",
    "end": "1276950"
  },
  {
    "text": "variable, which has a\nprobability density, v, what we want to assume is that\nthere's a probability density",
    "start": "1276950",
    "end": "1285200"
  },
  {
    "text": "that we know, which is the\nprobability density of the observation conditional\non the hypothesis.",
    "start": "1285200",
    "end": "1294050"
  },
  {
    "text": "We're assuming that we know\nthis, and we know this-- we call these things\nlikelihoods--",
    "start": "1294050",
    "end": "1301160"
  },
  {
    "text": "and in most communication\nproblems anyway, it's far",
    "start": "1301160",
    "end": "1306430"
  },
  {
    "text": "easier to get your hand on these\nlikelihoods than it is to get your hand the a\nposteriori probabilities,",
    "start": "1306430",
    "end": "1313260"
  },
  {
    "text": "which you're really\ninterested in. So we find these likelihoods,\nwe can find the marginal",
    "start": "1313260",
    "end": "1321390"
  },
  {
    "text": "density of the observation. Which is just the\nweighted sum.",
    "start": "1321390",
    "end": "1330720"
  },
  {
    "text": "The probability that the\nhypothesis is zero times a conditional probability of the\nobservation and so forth.",
    "start": "1330720",
    "end": "1338200"
  },
  {
    "text": " So we're going to assume that\nthose densities exist.",
    "start": "1338200",
    "end": "1344570"
  },
  {
    "text": "We're going to assume\nthat we know them.  And then with a great feat of\nprobability theory, we say the",
    "start": "1344570",
    "end": "1355840"
  },
  {
    "text": "a posteriori probability is\nequal to the a priori",
    "start": "1355840",
    "end": "1364289"
  },
  {
    "text": "probability times the likelihood\ndivided by the",
    "start": "1364290",
    "end": "1370570"
  },
  {
    "text": "marginal probability of v. OK? What was the first thing you\nlearn in probability when you",
    "start": "1370570",
    "end": "1375899"
  },
  {
    "text": "started studying random\nvariables? It was probably this formula,\nwhich says when you have a",
    "start": "1375900",
    "end": "1382679"
  },
  {
    "text": "joint density of two random\nvariables you can write it in two ways.",
    "start": "1382680",
    "end": "1388370"
  },
  {
    "text": "You can either write, \"density\nof one times density of two conditional and one is equal\nto the density of two times",
    "start": "1388370",
    "end": "1396420"
  },
  {
    "text": "density of one conditional in\ntwo.\" And then you think about it a little bit and you say,\n\"A ha!\" It doesn't matter",
    "start": "1396420",
    "end": "1403900"
  },
  {
    "text": "whether the first one is the\ndensity or whether it's the probability. You can deal with it\nin the same way.",
    "start": "1403900",
    "end": "1409640"
  },
  {
    "text": "And you get this formula,\nwhich I hope is not unusual to you.",
    "start": "1409640",
    "end": "1416340"
  },
  {
    "text": "OK, so our MAP decision rule\nthen, our MAP decision rule, remember, is to pick the more,\nis to find the a posteriori",
    "start": "1416340",
    "end": "1425720"
  },
  {
    "text": "probability which is\nmost probable. Because that is the probability\nof being correct.",
    "start": "1425720",
    "end": "1432540"
  },
  {
    "text": "So, in fact, if this probability\nis bigger than this probability, this is the a\nposteriori probability that",
    "start": "1432540",
    "end": "1441260"
  },
  {
    "text": "H is equal to zero. This is the a posteriori\nprobability that",
    "start": "1441260",
    "end": "1446809"
  },
  {
    "text": "H is equal to one. So we're just going to compare\nthose two and pick the larger. And if this one is larger than\nthis one, we pick our choice",
    "start": "1446810",
    "end": "1458419"
  },
  {
    "text": "equal zero. And if it's smaller, we pick\nour choice equal to one. So this is what MAP\ndetection is.",
    "start": "1458420",
    "end": "1467240"
  },
  {
    "text": "Why did I make this greater\nthan or equal and this less than?",
    "start": "1467240",
    "end": "1473000"
  },
  {
    "text": "Well, if we have densities,\nit doesn't often make any difference. Strangely enough, it does\nsometimes make a difference.",
    "start": "1473000",
    "end": "1480710"
  },
  {
    "text": "Because sometimes you can have\na density, and the densities are the same for both of\nthese likelihoods.",
    "start": "1480710",
    "end": "1487890"
  },
  {
    "text": "And you can find situations\nwhere it's important. But when the two probabilities\nare the same, the probability",
    "start": "1487890",
    "end": "1495500"
  },
  {
    "text": "of being correct is the same in\nboth cases, so it doesn't make any difference what\nyou do when you",
    "start": "1495500",
    "end": "1501750"
  },
  {
    "text": "have a quality here. And therefore we've just\nmade a decision. We've said, OK, what we're going\nto do is whenever this",
    "start": "1501750",
    "end": "1509610"
  },
  {
    "text": "is equal to this, we're\ngoing to choose zero. If you prefer choosing\none, be my guest.",
    "start": "1509610",
    "end": "1518019"
  },
  {
    "text": "All of your MAP error\nprobabilities will be exactly the same. Nothing will change.",
    "start": "1518020",
    "end": "1523580"
  },
  {
    "text": "It just is easier to do the\nsame thing all the time. OK, well then we look at this\nformula, and we say, \"Well, I",
    "start": "1523580",
    "end": "1529700"
  },
  {
    "text": "can simplify this a little\nbit.\" If I take this likelihood and move it over to\nthis side, and if I take this",
    "start": "1529700",
    "end": "1539700"
  },
  {
    "text": "marginal density and move it\nover to this side, and if I take p zero and move it over\nto this side, then the",
    "start": "1539700",
    "end": "1545970"
  },
  {
    "text": "marginal densities cancel out. They had nothing to do\nwith the problem. And I wind up with a ratio\nof the likelihoods.",
    "start": "1545970",
    "end": "1554080"
  },
  {
    "text": "And what do you think\nthe ratio of the likelihoods is called? Somebody got the smart\nidea of calling that",
    "start": "1554080",
    "end": "1559414"
  },
  {
    "text": "a likelihood ratio. Somehow the people in statistics\nwere much better at",
    "start": "1559415",
    "end": "1565810"
  },
  {
    "text": "generating notation than the\npeople in communication theory who have done just an abominable\njob of choosing",
    "start": "1565810",
    "end": "1572360"
  },
  {
    "text": "notation for things. But anyway, they call this\na likelihood ratio.",
    "start": "1572360",
    "end": "1577500"
  },
  {
    "text": "And the rule then becomes: if\nthe likelihood ratio is greater than or equal to\nthe ratio of p1 to",
    "start": "1577500",
    "end": "1584179"
  },
  {
    "text": "p0, we choose zero. And if it's less,\nwe choose one. And we call this ratio\nthe threshold.",
    "start": "1584180",
    "end": "1592220"
  },
  {
    "text": "So in fact what this says is\nbinary MAP tests are always",
    "start": "1592220",
    "end": "1598240"
  },
  {
    "text": "threshold tests. And by a threshold test I mean\nfinds the likelihood ratio,",
    "start": "1598240",
    "end": "1604610"
  },
  {
    "text": "compare the likelihood ratio\nwith the threshold-- the threshold in fact is this\nratio of a priori",
    "start": "1604610",
    "end": "1611010"
  },
  {
    "text": "probabilities-- and at that point you\nhave actually achieved the MAP test.",
    "start": "1611010",
    "end": "1617980"
  },
  {
    "text": "In other words, you have done\nsomething which actually, for real, minimizes the probability\nof error.",
    "start": "1617980",
    "end": "1626950"
  },
  {
    "text": "Maximizes the probability\nof being correct. Well because of that, this thing\nhere, this likelihood",
    "start": "1626950",
    "end": "1635730"
  },
  {
    "text": "ratio, is called a sufficient\nstatistic. And it's called a sufficient\nstatistic because you can do",
    "start": "1635730",
    "end": "1642830"
  },
  {
    "text": "math decoding just by\nknowing this number. OK? In other words, it says you can\nyou can calculate these",
    "start": "1642830",
    "end": "1650920"
  },
  {
    "text": "likelihoods. You can find the ratio of\nthem-- which is this",
    "start": "1650920",
    "end": "1656410"
  },
  {
    "text": "likelihood ratio-- and after you\nknow the likelihood ratio, you don't have to worry about\nthese likelihoods anymore.",
    "start": "1656410",
    "end": "1662809"
  },
  {
    "text": "This is the only thing relevant\nto the problem. Now this doesn't seem to be a\nhuge saving, because here",
    "start": "1662810",
    "end": "1670420"
  },
  {
    "text": "we're dealing with two real\nnumbers-- well here we've reduced it to one real number--\nwhich is something.",
    "start": "1670420",
    "end": "1675820"
  },
  {
    "text": "When we start dealing with\nvectors, when we start dealing with wave forms, this is\nreally a big thing.",
    "start": "1675820",
    "end": "1682830"
  },
  {
    "text": "Because what you're\ndoing is reducing the vectors to numbers.",
    "start": "1682830",
    "end": "1688269"
  },
  {
    "text": "And when you reduce accountibly\ninfinite dimensional vector to a number,",
    "start": "1688270",
    "end": "1693960"
  },
  {
    "text": "that's a big advantage. It also, in terms of the\ncommunication problems we're",
    "start": "1693960",
    "end": "1699870"
  },
  {
    "text": "facing, breaks up a detector\ninto two pieces in an interesting way.",
    "start": "1699870",
    "end": "1705900"
  },
  {
    "text": "Mainly it says there are things\nyou do with the wave form in order to calculate what\nthis likelihood ratio is,",
    "start": "1705900",
    "end": "1713400"
  },
  {
    "text": "and then after you find the\nlikelihood ratio you just forget about what the wave\nform was and you",
    "start": "1713400",
    "end": "1719150"
  },
  {
    "text": "deal only with that. What we're going to find out\nis in this problem we were looking at here--",
    "start": "1719150",
    "end": "1724640"
  },
  {
    "text": " we're going to find out\nlater when we look at the vector problem--",
    "start": "1724640",
    "end": "1732060"
  },
  {
    "text": "in fact this thing here is in\nfact the likelihood ratio if",
    "start": "1732060",
    "end": "1739010"
  },
  {
    "text": "you make an observation out\nat this point here. In other words, right at the\nfront end of the receiver,",
    "start": "1739010",
    "end": "1746090"
  },
  {
    "text": "that's where you have all\nthe information you can possibly have. If you calculate likelihood\nratios at that point what",
    "start": "1746090",
    "end": "1756680"
  },
  {
    "text": "you're going to do is to find\nthe likelihood ratio you're going to go through all this\nstuff right here and wind up",
    "start": "1756680",
    "end": "1764530"
  },
  {
    "text": "with that which is work which\nis proportional to the likelihood ratio right here.",
    "start": "1764530",
    "end": "1770760"
  },
  {
    "text": "OK, so one of the things we're\ndoing right now is we're not looking at that problem. We're only looking at the\nsimpler problem, assuming a",
    "start": "1770760",
    "end": "1778070"
  },
  {
    "text": "one dimensional problem. But the reason we're looking\nat it is that later we're going to show that this is, in\nfact, the solution to the more",
    "start": "1778070",
    "end": "1785970"
  },
  {
    "text": "general problem. Which was Shannon's idea\nin the first place.",
    "start": "1785970",
    "end": "1793040"
  },
  {
    "text": "Of, how do you solve the trivial\nproblem first and then see what the complicated\nproblem is.",
    "start": "1793040",
    "end": "1802520"
  },
  {
    "text": "OK, so that's what we're trying\nto do, summarized here",
    "start": "1802520",
    "end": "1810340"
  },
  {
    "text": "for any binary detection problem\nwhere the observation has a sample value of\na random something.",
    "start": "1810340",
    "end": "1818520"
  },
  {
    "text": "Namely, a random vector, a\nrandom process, a random variable, a complex variable,\na complex anything.",
    "start": "1818520",
    "end": "1824730"
  },
  {
    "text": "Anything whatsoever, so long\nas you can assign a probability density to it.",
    "start": "1824730",
    "end": "1830950"
  },
  {
    "text": "You calculate the likelihood\nratio, which is this ratio here, so long as you have then\ncities even talk about.",
    "start": "1830950",
    "end": "1838250"
  },
  {
    "text": "The MAP rule is to compare this\nlikelihood ratio with the threshold data-- which is just\nthe ratio of the a priori",
    "start": "1838250",
    "end": "1845460"
  },
  {
    "text": "probabilities-- if this is greater\nthan or equal to that, you choose zero.",
    "start": "1845460",
    "end": "1851780"
  },
  {
    "text": "Otherwise you choose one. The MAP rule, as I said before,\npartitions this",
    "start": "1851780",
    "end": "1858000"
  },
  {
    "text": "observation space\ninto two pieces. Into two segments.",
    "start": "1858000",
    "end": "1865669"
  },
  {
    "text": "And one of those pieces\ngets mapped into zero. One of the pieces gets\nmapped into one.",
    "start": "1865670",
    "end": "1871850"
  },
  {
    "text": "It's exactly like a\nbinary quanitizer. Except the rule you use to\nchoose the the quantization",
    "start": "1871850",
    "end": "1879440"
  },
  {
    "text": "regions is different. But a quanitizer maps a space\ninto a finite set of regions.",
    "start": "1879440",
    "end": "1885429"
  },
  {
    "text": "And this detection rule does\nexactly the same thing. And since the beginning of\ninformation theory people have",
    "start": "1885430",
    "end": "1892269"
  },
  {
    "text": "been puzzling over how to make\nuse of the correspondence between quanitization on one\nhand and detection on the",
    "start": "1892270",
    "end": "1901020"
  },
  {
    "text": "other hand. And there are some\ncorrespondences but they aren't all that good\nmost of the time.",
    "start": "1901020",
    "end": "1907830"
  },
  {
    "text": "OK, so you get an error when\nthe actual hypothesis that occurred, namely the bit that\ngot sent was i and if the",
    "start": "1907830",
    "end": "1917290"
  },
  {
    "text": "observation landed in\nthe other subset. We know that the MAP rule\nminimizes the error",
    "start": "1917290",
    "end": "1923960"
  },
  {
    "text": "probability. So you have a rule which you\ncan use for all binary detection problems so long\nas you have the density.",
    "start": "1923960",
    "end": "1932390"
  },
  {
    "text": "And if you don't have a density\nyou can generalize it without too much trouble.",
    "start": "1932390",
    "end": "1938560"
  },
  {
    "text": "OK, so we want to look at the\nproblem in Gaussian noise.",
    "start": "1938560",
    "end": "1944330"
  },
  {
    "text": "In particular we want to\nlook at it for 2PAM. In other words for a standard\nPAM system, where zero gets",
    "start": "1944330",
    "end": "1952720"
  },
  {
    "text": "mapped into plus a and one\ngets mapped into minus a. This is often called antipodal\nsignaling because you're",
    "start": "1952720",
    "end": "1960850"
  },
  {
    "text": "sending a plus something\nand a minus something. They are at opposite ends\nof the spectrum.",
    "start": "1960850",
    "end": "1966600"
  },
  {
    "text": "You push them as far away as you\ncan, because as you push them further and further\naway it requires more and more energy.",
    "start": "1966600",
    "end": "1972930"
  },
  {
    "text": "So you use the energy\nyou have. You get them as far apart as you\ncan, and you hope that's going to help you.",
    "start": "1972930",
    "end": "1978940"
  },
  {
    "text": "And we'll see that\nit does help you. OK, so what you receive then,\nwe'll assume, is either plus",
    "start": "1978940",
    "end": "1987110"
  },
  {
    "text": "or minus a-- depending on which hypothesis\noccured-- plus a Gaussian random\nvariable.",
    "start": "1987110",
    "end": "1994750"
  },
  {
    "text": "And here's where the notation\nof communication theorists rears its ugly head.",
    "start": "1994750",
    "end": "2002030"
  },
  {
    "text": "We call the variance of this\nrandom variable n 0 over 2.",
    "start": "2002030",
    "end": "2007450"
  },
  {
    "text": "I would prefer to call\nit sigma squared but unfortunately you can't\nfight city hall on",
    "start": "2007450",
    "end": "2014470"
  },
  {
    "text": "something like this. And everybody talks about\nn 0 and n 0 over 2.",
    "start": "2014470",
    "end": "2020510"
  },
  {
    "text": "And you got to get used to it. So here's where we're starting\nto get used to it. So that's the variance of this\nnoise random variable.",
    "start": "2020510",
    "end": "2030510"
  },
  {
    "text": "OK, we're only going to send one\nbinary digit, H, so this is the only, this is the sole\nproblem we have to deal with.",
    "start": "2030510",
    "end": "2037850"
  },
  {
    "text": "We've made a binary choice. Added one Gaussian random\nvariable to it.",
    "start": "2037850",
    "end": "2044110"
  },
  {
    "text": "You observe the sum,\nand you guess. So what are these likelihoods\nin this case?",
    "start": "2044110",
    "end": "2052330"
  },
  {
    "text": "Well, the likelihood if H is\nequal zero, in other words if you're sending a plus a, the\nlikelihood is just a Gaussian",
    "start": "2052330",
    "end": "2060269"
  },
  {
    "text": "density shifted over by a. And if you're sending, on\nthe other hand, a one--",
    "start": "2060270",
    "end": "2067620"
  },
  {
    "text": "which means you're sending minus\na-- you have a Gaussian density shifted over\nthe other way.",
    "start": "2067620",
    "end": "2073419"
  },
  {
    "text": "Let me show you a\npicture of that. ",
    "start": "2073420",
    "end": "2080530"
  },
  {
    "text": "We'll come back to analyze more\nthings about the picture in a little bit, so don't\nworry about most of the",
    "start": "2080530",
    "end": "2086080"
  },
  {
    "text": "picture at this point.  OK, this is the likelihood\nprobability density of the",
    "start": "2086080",
    "end": "2095030"
  },
  {
    "text": "output given that\nyou sent a zero. Mainly that you sent plus a.",
    "start": "2095030",
    "end": "2100380"
  },
  {
    "text": "So we have a Gaussian density--\nthis bell shaped curve-- centered\naround plus a.",
    "start": "2100380",
    "end": "2105619"
  },
  {
    "text": "If you sent a one, you're\nsending minus a-- one gets mapped into minus a-- and you\nhave the same bell shaped",
    "start": "2105620",
    "end": "2113230"
  },
  {
    "text": "curve centered around minus a. If you receive any particular\nvalue of v, mainly suppose you",
    "start": "2113230",
    "end": "2121560"
  },
  {
    "text": "receive the value of v here,\nyou calculate these two likelihoods. One of them is this.",
    "start": "2121560",
    "end": "2127490"
  },
  {
    "text": "One of them is that. You compare them, the ratio with\nthe threshold, and you",
    "start": "2127490",
    "end": "2133200"
  },
  {
    "text": "make your choice. OK, so let's go back to do it.",
    "start": "2133200",
    "end": "2139910"
  },
  {
    "text": "To do the arithmetic. Here are the two likelihoods.",
    "start": "2139910",
    "end": "2145080"
  },
  {
    "text": "You take the ratio of\nthese two things. When you take the ratio\nof them, what happens?",
    "start": "2145080",
    "end": "2151180"
  },
  {
    "text": "And this sort of always\nhappens in these Gaussian problems. These terms cancel out.",
    "start": "2151180",
    "end": "2157320"
  },
  {
    "text": "Well it always happens in these additive Gaussian problems. These terms cancel out.",
    "start": "2157320",
    "end": "2162809"
  },
  {
    "text": "You take a ratio of\ntwo exponents. You just get the difference.",
    "start": "2162810",
    "end": "2168520"
  },
  {
    "text": "So the likelihood ratio--\nthis divided by this-- is then e to the minus v minus\na squared over n0.",
    "start": "2168520",
    "end": "2178140"
  },
  {
    "text": "and v plus a squared over n0. OK? Because normally the Gaussian\ndensity is something divided",
    "start": "2178140",
    "end": "2186940"
  },
  {
    "text": "by two sigma squared, and sigma\nsquared here is n0 over 2, so the 2's cancel out.",
    "start": "2186940",
    "end": "2192580"
  },
  {
    "text": "One nice thing about the\nnotation anyway, you get rid of one factor of two in it. Well so you have this\nminus this.",
    "start": "2192580",
    "end": "2199760"
  },
  {
    "text": "When you take the difference\nof these two things the v squareds cancel out.",
    "start": "2199760",
    "end": "2206480"
  },
  {
    "text": "Because one of these things is\nin the numerator, the other one was in the denominator.",
    "start": "2206480",
    "end": "2211970"
  },
  {
    "text": "So you have this term\ncomes through as is. This is-- you're dividing by this--",
    "start": "2211970",
    "end": "2218829"
  },
  {
    "text": "so when you multiply this\nturns into a plus sign. So the v squared here cancels\nout with the v squared here.",
    "start": "2218830",
    "end": "2225930"
  },
  {
    "text": "The a squared here cancels out\nwith the a squared here. And it's only the inner product\nterm that survives",
    "start": "2225930",
    "end": "2232809"
  },
  {
    "text": "this whole thing. And here you have plus 2va. Here you have plus 2va.",
    "start": "2232810",
    "end": "2238810"
  },
  {
    "text": "So you wind up with e to\nthe 4av divided by n0.",
    "start": "2238810",
    "end": "2244920"
  },
  {
    "text": "Which is very nice, because\nwhat it says is this likelihood, which is what\ndetermines everything in the",
    "start": "2244920",
    "end": "2251140"
  },
  {
    "text": "world, is just a scalar in\nmultiple of the observation. ",
    "start": "2251140",
    "end": "2257740"
  },
  {
    "text": "And that that's going to\nsimplify things a fair amount. It's why that picture comes\nout as simply is it does.",
    "start": "2257740",
    "end": "2263170"
  },
  {
    "text": " OK, so to do a little more\nof the arithmetic.",
    "start": "2263170",
    "end": "2272760"
  },
  {
    "text": "This is the likelihood here,\ne to the 4av over n0. So our rule is you compare\nthis likelihood to the",
    "start": "2272760",
    "end": "2282340"
  },
  {
    "text": "threshold-- which is p1 over\np0, which we call eta-- and you look at that for awhile\nand you say, \"Gee, this",
    "start": "2282340",
    "end": "2289589"
  },
  {
    "text": "is going to be much easier\nto deal with. Instead of looking at the\nlikelihood ratio, I look at",
    "start": "2289590",
    "end": "2295630"
  },
  {
    "text": "the log likelihood ratio.\" And people who deal with\nGaussian problems a lot, you",
    "start": "2295630",
    "end": "2301950"
  },
  {
    "text": "never hear them talk about\nlikelihood ratios, you always hear them talk about log\nlikelihood ratios.",
    "start": "2301950",
    "end": "2308890"
  },
  {
    "text": "And you can find one from the\nother, so either one is equally good.",
    "start": "2308890",
    "end": "2314109"
  },
  {
    "text": "In other words, the log\nlikelihood ratio is a sufficient statistic, because\nyou can calculate the",
    "start": "2314110",
    "end": "2320110"
  },
  {
    "text": "likelihood ratio from it. So this is a sufficient\nstatistic.",
    "start": "2320110",
    "end": "2326040"
  },
  {
    "text": "It's equal to 4av over n0. And when this is greater than\nor equal the log of the",
    "start": "2326040",
    "end": "2332200"
  },
  {
    "text": "threshold, you go this way. When it's less than,\nyou go this way. So when you then multiply by n0\nover 4a, your decision rule",
    "start": "2332200",
    "end": "2345170"
  },
  {
    "text": "is you just look at\nthe observation. You compare it with n0 times\nlog of eta divided by 4a.",
    "start": "2345170",
    "end": "2353710"
  },
  {
    "text": "And at this point we can go back\nto this picture and sort of sort out what all\nof it means.",
    "start": "2353710",
    "end": "2362770"
  },
  {
    "text": "Because this point here\nis now the threshold. It's n0 times log of\neta divided 4eta.",
    "start": "2362770",
    "end": "2369640"
  },
  {
    "text": " By 4a. That's what we said the\nthreshold had to be.",
    "start": "2369640",
    "end": "2377090"
  },
  {
    "text": "So we have these two Gaussian\ncurves now. Why do we have to go back and\nlook at these Gaussian curves?",
    "start": "2377090",
    "end": "2383440"
  },
  {
    "text": "I told you that once we\ncalculated the likelihood",
    "start": "2383440",
    "end": "2388640"
  },
  {
    "text": "ratio we could forget\nabout the curves. So why do I want to put\nthe curves back in?",
    "start": "2388640",
    "end": "2395480"
  },
  {
    "text": "Well because I want to calculate\nthe probability of error at this point.",
    "start": "2395480",
    "end": "2401150"
  },
  {
    "text": "OK? And it's easier to calculate the\nprobability of error if, in fact, I draw curve for\nmyself and I look at",
    "start": "2401150",
    "end": "2408150"
  },
  {
    "text": "what's going on. So here's the threshold. Here's the density when H equals\none is the correct",
    "start": "2408150",
    "end": "2418220"
  },
  {
    "text": "hypothesis. The probability of error is the\nprobability that when I",
    "start": "2418220",
    "end": "2423250"
  },
  {
    "text": "send one, the observation is\ngoing to be a random variable",
    "start": "2423250",
    "end": "2429970"
  },
  {
    "text": "with this probability density\nand if it's a wild case, and I",
    "start": "2429970",
    "end": "2436460"
  },
  {
    "text": "got an enormous value of noise,\npositive noise, the noise is going to push me over\nthat threshold there and I'm",
    "start": "2436460",
    "end": "2444310"
  },
  {
    "text": "going to make a mistake. So, in fact, the probability\nof error-- conditional on sending one--",
    "start": "2444310",
    "end": "2450570"
  },
  {
    "text": "is just this probability of that\nlittle space in there. OK? Which is the probability that\nI'm going to say that zero",
    "start": "2450570",
    "end": "2459360"
  },
  {
    "text": "occurred when, in fact,\none occurred. So that's my probability of\nerror when one occurs.",
    "start": "2459360",
    "end": "2466830"
  },
  {
    "text": "What's the probability of\nerror when zero occurs? Well it's the same analysis. When zero occurs--",
    "start": "2466830",
    "end": "2473520"
  },
  {
    "text": "mainly when the correct\nhypothesis is zero-- the output, v, follows this\nprobability density here.",
    "start": "2473520",
    "end": "2481850"
  },
  {
    "text": "And I'm going to screw up if\nthe noise carries me beyond",
    "start": "2481850",
    "end": "2487410"
  },
  {
    "text": "this point here. So you can see what the\nthreshold is doing now.",
    "start": "2487410",
    "end": "2493680"
  },
  {
    "text": "I mean, when you choose a\nthreshold which is positive it makes it much harder to screw\nup when you send a minus a.",
    "start": "2493680",
    "end": "2503559"
  },
  {
    "text": "It makes it much easier\nto screw up when you send a plus a. But you see that's what we\nwanted to do, because the",
    "start": "2503560",
    "end": "2510690"
  },
  {
    "text": "threshold was positive in this\ncase, because p1 was so much",
    "start": "2510690",
    "end": "2518230"
  },
  {
    "text": "larger than p0. And because p1 is so much\nlarger than p0--",
    "start": "2518230",
    "end": "2523250"
  },
  {
    "text": "p1 happens almost\nall the time-- and therefore you would normally\nalmost choose p1",
    "start": "2523250",
    "end": "2530320"
  },
  {
    "text": "without looking at v. Which says\nyou want to push it over that way a little bit.",
    "start": "2530320",
    "end": "2536730"
  },
  {
    "text": "OK, when you calculate this\nprobability of error, it's the probability of the tail of a\nGaussian random variable.",
    "start": "2536730",
    "end": "2546650"
  },
  {
    "text": "So you define this tail\nvariable, q of x, is the",
    "start": "2546650",
    "end": "2553789"
  },
  {
    "text": "complimentary distribution\nfunction of a normal random variable.",
    "start": "2553790",
    "end": "2558900"
  },
  {
    "text": "It's the integral from x to\ninfinity of one over the square root of 2pi, e to the\nminus c squared over 2.",
    "start": "2558900",
    "end": "2568750"
  },
  {
    "text": "I guess this would make better\nsense if this were a z-- one and--",
    "start": "2568750",
    "end": "2574510"
  },
  {
    "text": "oh, no. No, I did it right\nthe first time. That's an x, because x is the\nlimit in there, you see.",
    "start": "2574510",
    "end": "2581380"
  },
  {
    "text": "So I'm calculating all of the\nprobability density that's off to the right of x.",
    "start": "2581380",
    "end": "2588550"
  },
  {
    "text": "And the probability of error\nwhen H is equal to one is this",
    "start": "2588550",
    "end": "2593900"
  },
  {
    "text": "probability-- which looks like\nit's the tail on the negative side but if you think about\nit a little bit, since the",
    "start": "2593900",
    "end": "2600119"
  },
  {
    "text": "Gaussian curve is symmetric, you\ncan also look at it as a q function where now when you have\nthis is equal to zero,",
    "start": "2600120",
    "end": "2612070"
  },
  {
    "text": "this corresponds to changing\nthis plus to a minus here and",
    "start": "2612070",
    "end": "2617720"
  },
  {
    "text": "that's the only change. OK, so this looks a\nlittle ugly and it",
    "start": "2617720",
    "end": "2626080"
  },
  {
    "text": "looks a little strange. I mean you can sort\nof interpret",
    "start": "2626080",
    "end": "2632000"
  },
  {
    "text": "this part of it here-- I can interpret this\npart if I'm using",
    "start": "2632000",
    "end": "2637780"
  },
  {
    "text": "maximum likelihood decoding-- maximum likelihood is mapped\ndecoding where the threshold",
    "start": "2637780",
    "end": "2644390"
  },
  {
    "text": "is equal to one. In other words, it's where\nyou're assuming that the hypothesis is equally likely to\nbe zero or one-- a priori--",
    "start": "2644390",
    "end": "2653340"
  },
  {
    "text": "which is a good assumption\nalmost always in communication because we work very hard in\ndoing source coding to make",
    "start": "2653340",
    "end": "2662680"
  },
  {
    "text": "those binary digits equally\nlikely zero or one. And there are other reasons\nfor choosing maximum",
    "start": "2662680",
    "end": "2669420"
  },
  {
    "text": "likelihood. If you don't know anything about\nthe probabilities it's a good assumption in a sort\nof a max/min sense.",
    "start": "2669420",
    "end": "2677910"
  },
  {
    "text": "It sort of limits how much you\ncan screw up by having the wrong probability, so it's a\nvery robust choice also.",
    "start": "2677910",
    "end": "2687390"
  },
  {
    "text": "OK, but now this, we're taking\nthe ratio of a with the square",
    "start": "2687390",
    "end": "2692690"
  },
  {
    "text": "root of n0. Well the square root of n0 over\n2 is really the standard",
    "start": "2692690",
    "end": "2699110"
  },
  {
    "text": "deviation of the noise. So what we're doing is comparing\nthe amount of input we've put in with the standard\ndeviation of the noise.",
    "start": "2699110",
    "end": "2710000"
  },
  {
    "text": "Now does that make any sense? The probability of error\ndepends on that ratio? Well yeah, it makes a\nwhole lot of sense.",
    "start": "2710000",
    "end": "2716599"
  },
  {
    "text": "Because if, for example, I\nwanted to look at this problem in a different scaling\nsystem--",
    "start": "2716600",
    "end": "2722450"
  },
  {
    "text": "if this is volts and I want to\nlook at it in milli-volts-- I'm going to divide a by, I'm\ngoing to multiply a by 1000.",
    "start": "2722450",
    "end": "2732020"
  },
  {
    "text": "I'm going to multiply the\nstandard deviation of the noise by 1000.",
    "start": "2732020",
    "end": "2737430"
  },
  {
    "text": "Because one of the things we\nalways do here-- the way we choose n0 over 2-- n0 over 2 is sort of a\nmeaningless quantity.",
    "start": "2737430",
    "end": "2746210"
  },
  {
    "text": "It's the noise energy in one\ndegree of freedom in the",
    "start": "2746210",
    "end": "2752990"
  },
  {
    "text": "scaling reference that we're\nusing for the data. OK? And that's the only definition\nyou can come up with that",
    "start": "2752990",
    "end": "2760190"
  },
  {
    "text": "makes any sense. I mean you scale the data in\nwhatever way you please, and",
    "start": "2760190",
    "end": "2766420"
  },
  {
    "text": "when we've gone from baseband\nto passband, we in fact have",
    "start": "2766420",
    "end": "2772579"
  },
  {
    "text": "multiplied the energy in the\ninput by a factor of two.",
    "start": "2772580",
    "end": "2779140"
  },
  {
    "text": "And therefore, because of\nthat, we're going to-- n0 at passband is going to be a\nsquare root of 2 bigger than",
    "start": "2779140",
    "end": "2786190"
  },
  {
    "text": "it is at baseband. If you don't like that,\nlive with it. That's the way it is.",
    "start": "2786190",
    "end": "2793140"
  },
  {
    "text": "Nobody will change n0,\nno matter who wants them to change it. OK, so this term make sense.",
    "start": "2793140",
    "end": "2800869"
  },
  {
    "text": "It's the ratio of the signal\namplitude to the standard",
    "start": "2800870",
    "end": "2808080"
  },
  {
    "text": "deviation of the noise. And that should be the only way\nthat the signal amplitude",
    "start": "2808080",
    "end": "2813720"
  },
  {
    "text": "or the standard deviation of the\nnoise enters in, because it's really the ratio that\nhas to be important.",
    "start": "2813720",
    "end": "2820250"
  },
  {
    "text": "Why this crazy term? Well if you look at the curve\nyou can sort of see why it is.",
    "start": "2820250",
    "end": "2829410"
  },
  {
    "text": "The threshold test is comparing\nthe likelihood ratio of this curve with\nthe likelihood",
    "start": "2829410",
    "end": "2835250"
  },
  {
    "text": "ratio of this curve. What's going to happen as\na gets very, very large?",
    "start": "2835250",
    "end": "2847150"
  },
  {
    "text": "You move a out, and the thing\nthat's happening then is this",
    "start": "2847150",
    "end": "2852240"
  },
  {
    "text": "curve-- which is now coming down\nin a modest way here-- if you move a way out here, you're\ngoing to have almost",
    "start": "2852240",
    "end": "2859369"
  },
  {
    "text": "nothing there. And it's going to be going\ndown very fast.",
    "start": "2859370",
    "end": "2864750"
  },
  {
    "text": "It's going to be going\ndown very fast relative to its magnitude.",
    "start": "2864750",
    "end": "2870470"
  },
  {
    "text": "In other words the bigger\na gets, the bigger this difference is going to be\nfor any given threshold.",
    "start": "2870470",
    "end": "2877849"
  },
  {
    "text": "And that's why you get a over\nsquare root of n0 here.",
    "start": "2877850",
    "end": "2883370"
  },
  {
    "text": "And here you get exactly\nthe opposite thing. That's because for a given\nthreshold, as this signal to",
    "start": "2883370",
    "end": "2889400"
  },
  {
    "text": "noise ratio gets bigger, this\nthreshold term becomes almost",
    "start": "2889400",
    "end": "2897730"
  },
  {
    "text": "totally unimportant. I mean you get so much\ninformation out of the reading",
    "start": "2897730",
    "end": "2903270"
  },
  {
    "text": "you're making, because it's\nso reliable, that having a",
    "start": "2903270",
    "end": "2910470"
  },
  {
    "text": "threshold is almost completely\nirrelevant. And therefore you can sort\nof forget about it.",
    "start": "2910470",
    "end": "2915990"
  },
  {
    "text": "If a is very large, this\nterm is zilch. OK?",
    "start": "2915990",
    "end": "2921350"
  },
  {
    "text": " So if you want to have reliable\ncommunication and you use a large amount of single\nnoise ratio to get it, that's",
    "start": "2921350",
    "end": "2929000"
  },
  {
    "text": "another reason for forgetting\nabout whether the threshold is",
    "start": "2929000",
    "end": "2935320"
  },
  {
    "text": "one or something else. And we would certainly like to\ndeal with problems where the",
    "start": "2935320",
    "end": "2942520"
  },
  {
    "text": "threshold is equal to one\nbecause most people who remember q of a signal\nto noise ratio.",
    "start": "2942520",
    "end": "2950820"
  },
  {
    "text": "I don't know anybody who can\nremember this formula. I'm sure there's some people,\nbut I don't think anybody who",
    "start": "2950820",
    "end": "2958330"
  },
  {
    "text": "works in the communication\nfield ever thinks about this at all. Except the first time they\nderive it and the say, \"Oh,",
    "start": "2958330",
    "end": "2965019"
  },
  {
    "text": "that's very nice.\" And then they\npromptly forget about it. The only reason I think\nabout it more is I",
    "start": "2965020",
    "end": "2971420"
  },
  {
    "text": "teach the course sometimes. Otherwise I would forget\nabout it, too. ",
    "start": "2971420",
    "end": "2981730"
  },
  {
    "text": "OK, which is what this says. For communication we assume\np0 is equal to p1.",
    "start": "2981730",
    "end": "2988390"
  },
  {
    "text": "So we assume that eta\nis equal to one. So the probability of error,\nwhich is also the probability",
    "start": "2988390",
    "end": "2994329"
  },
  {
    "text": "of error when H is equal to\none-- in other words when a one actually enters the\ncommunication system-- is",
    "start": "2994330",
    "end": "3001230"
  },
  {
    "text": "equal to the probability of\nerror when H is equal to 0. In other words these two tails\nhere, when the threshold is",
    "start": "3001230",
    "end": "3008980"
  },
  {
    "text": "equal to one you set the\nthreshold right there. The probability of this tail\nis clearly equal to the",
    "start": "3008980",
    "end": "3015540"
  },
  {
    "text": "probability of this tail. Just by symmetry. So these two error probabilities\nare the same.",
    "start": "3015540",
    "end": "3022279"
  },
  {
    "text": "And in fact they are just\nq of a over the square root of n0 over 2.",
    "start": "3022280",
    "end": "3029660"
  },
  {
    "text": "It's nice to put this\nin terms of energy. We said before that energy is\nsort of important in the communication field.",
    "start": "3029660",
    "end": "3036799"
  },
  {
    "text": "So we call e sub b the energy\nper bit that we're spending to send data.",
    "start": "3036800",
    "end": "3042900"
  },
  {
    "text": "I mean don't worry about the\nfact that we're only sending one bit and then we're tearing\nthe communication system down.",
    "start": "3042900",
    "end": "3048150"
  },
  {
    "text": "Because pretty soon we're going\nto send multiple bits. But the amount of energy we're\nspending sending this one bit",
    "start": "3048150",
    "end": "3055830"
  },
  {
    "text": "is a squared. At least back in this frame of\nreference that we're looking",
    "start": "3055830",
    "end": "3061270"
  },
  {
    "text": "at now, where we're just looking\nat this discrete signal and single\nvariable noise.",
    "start": "3061270",
    "end": "3068980"
  },
  {
    "text": "And n0 over 2 is the noise\nvariance of this particular",
    "start": "3068980",
    "end": "3074010"
  },
  {
    "text": "random variable, z, so when we\nwrite this out in terms of Eb,",
    "start": "3074010",
    "end": "3080960"
  },
  {
    "text": "which is a squared, it looks\nlike this-- it's 2eb over n0.",
    "start": "3080960",
    "end": "3087010"
  },
  {
    "text": "So the probability of error for\nthis binary communication problem is just the square\nroot of 2Eb over n0.",
    "start": "3087010",
    "end": "3096210"
  },
  {
    "text": "Which is a formula that\nyou want to remember. It's the error probability for\nbinary detection when n0 over",
    "start": "3096210",
    "end": "3107850"
  },
  {
    "text": "2 is the noise energy on this\none degree of freedom and Eb",
    "start": "3107850",
    "end": "3115520"
  },
  {
    "text": "is the amount of energy you're\nspending on this one degree of freedom.",
    "start": "3115520",
    "end": "3120660"
  },
  {
    "text": "You will see about\n50 variations of this as we go on.",
    "start": "3120660",
    "end": "3126089"
  },
  {
    "text": "If you try to remember this\nfundamental definition, it'll",
    "start": "3126090",
    "end": "3131220"
  },
  {
    "text": "save you a lot of agony. Even so, everybody I know who\ndeals with this kind of thing",
    "start": "3131220",
    "end": "3140170"
  },
  {
    "text": "always screws up the\nfactors of twos. And finally when they get all\ndone, they try to figure out",
    "start": "3140170",
    "end": "3147200"
  },
  {
    "text": "from common sense or from\nsomething else, what the factors of two ought to be.",
    "start": "3147200",
    "end": "3152660"
  },
  {
    "text": "And they reduce their\nprobability of error to about a quarter after they're all done\nwith doing all of that.",
    "start": "3152660",
    "end": "3158790"
  },
  {
    "text": " OK, so we spent a lot\nof time analyzing",
    "start": "3158790",
    "end": "3166490"
  },
  {
    "text": "binary antipodal signals. What about the binary non\nantipodal signals?",
    "start": "3166490",
    "end": "3173430"
  },
  {
    "text": "This is beautiful example of\nShannon's idea of studying the simplest cases first.",
    "start": "3173430",
    "end": "3178910"
  },
  {
    "text": " If you have two signals, one of\nwhich is b and one of which",
    "start": "3178910",
    "end": "3185500"
  },
  {
    "text": "is b prime, and they can be put\nanywhere on the real line.",
    "start": "3185500",
    "end": "3194330"
  },
  {
    "text": "And what I've done, because I\ndidn't want to plot this whole picture again, is I just took\nthe zero out, and I replaced",
    "start": "3194330",
    "end": "3201440"
  },
  {
    "text": "the zero by the point halfway\nbetween these two points which is b plus b prime over 2.",
    "start": "3201440",
    "end": "3208529"
  },
  {
    "text": "And then we look at it and we\nsay what happens if you have an arbitrary set of two points\nanywhere on the real line?",
    "start": "3208530",
    "end": "3218500"
  },
  {
    "text": "Well when I send this point the\nlikelihood, conditional on",
    "start": "3218500",
    "end": "3224900"
  },
  {
    "text": "this point being sent, is\nthis Gaussian curve centered on b prime.",
    "start": "3224900",
    "end": "3230470"
  },
  {
    "text": "When I send b the likelihood\nis a Gaussian curve centered on b.",
    "start": "3230470",
    "end": "3237309"
  },
  {
    "text": "And it is in fact the same curve\nthat we drew before. If in fact I replaced zero with\na center point between",
    "start": "3237310",
    "end": "3246869"
  },
  {
    "text": "these two, which is b\nplus b prime over 2. And if I then define a as this\ndistance in here, the",
    "start": "3246870",
    "end": "3268500"
  },
  {
    "text": "probability of error that I\ncalculated before is the same as it was before.",
    "start": "3268500",
    "end": "3274279"
  },
  {
    "text": "Now I would suggest to all of\nyou that you try to find the",
    "start": "3274280",
    "end": "3280050"
  },
  {
    "text": "probability of error for this\nsystem here, just not using",
    "start": "3280050",
    "end": "3286700"
  },
  {
    "text": "what we've already done, and\njust writing out the likelihood ratios as a function\nof an arbitrary b",
    "start": "3286700",
    "end": "3293160"
  },
  {
    "text": "prime and an arbitrary b and\nfinding a likelihood ratio,",
    "start": "3293160",
    "end": "3298369"
  },
  {
    "text": "and calculating through\nall of that. And most of you are capable of\ncalculating through all of it.",
    "start": "3298370",
    "end": "3304760"
  },
  {
    "text": "But when you do so, you will\nget a god awful looking formula, which just looks\ntotally messy.",
    "start": "3304760",
    "end": "3312530"
  },
  {
    "text": "And by looking at the formula\nyou are not going to be able to realize that what's going\non is what we see here from",
    "start": "3312530",
    "end": "3320540"
  },
  {
    "text": "the picture. And the only reason we figured\nout what was going like the picture is we already\nsolved the problem",
    "start": "3320540",
    "end": "3327609"
  },
  {
    "text": "for the Simpson case. OK so it never makes any sense\nin this problem to look at",
    "start": "3327610",
    "end": "3335400"
  },
  {
    "text": "this general case. You always want to say the\ngeneral case is just a special case of a special case.",
    "start": "3335400",
    "end": "3343360"
  },
  {
    "text": "Where you just have to define\nthings slightly differently. OK, so we're going to do the\ncenter point, then, I mean it",
    "start": "3343360",
    "end": "3351650"
  },
  {
    "text": "might be a pilot tone. It might be any other non\ninformation bearing signal.",
    "start": "3351650",
    "end": "3358600"
  },
  {
    "text": "In other words were sending\nthe one bit. Sometimes for some reason or\nother, you need to get synchronization.",
    "start": "3358600",
    "end": "3364470"
  },
  {
    "text": "You need to get other things\nin a communication system. And for that reason, you\nsend other things.",
    "start": "3364470",
    "end": "3369500"
  },
  {
    "text": "We'll talk about a lot\nof those later. But they don't change the error\nprobability at all.",
    "start": "3369500",
    "end": "3375500"
  },
  {
    "text": "The error probability is\ndetermined solely by the distance between these two\npoints which we call 2a.",
    "start": "3375500",
    "end": "3381910"
  },
  {
    "text": " So probability of error remains\nthe same in terms of",
    "start": "3381910",
    "end": "3388660"
  },
  {
    "text": "this distance. The energy per bit\nnow changes. The energy per bit is\nthe energy here",
    "start": "3388660",
    "end": "3396080"
  },
  {
    "text": "plus the energy here. Which in fact is the energy\nin the center point plus a squared.",
    "start": "3396080",
    "end": "3403990"
  },
  {
    "text": "I mean we've done that in a\nnumber of contexts, the way to find the energy in a binary\nrandom variable is to take the",
    "start": "3403990",
    "end": "3412990"
  },
  {
    "text": "energy in the center point\nplus the energy in the difference.",
    "start": "3412990",
    "end": "3418069"
  },
  {
    "text": "It's the same as finding the\nfluctuation plus the square of the mean. It's that same underlying\nidea.",
    "start": "3418070",
    "end": "3425529"
  },
  {
    "text": "So any time you use non\nantipodal and you shift things off the mean, you can\nsee what's going",
    "start": "3425530",
    "end": "3433840"
  },
  {
    "text": "on very, very easily. You waste energy. I mean it might not be wasted,\nyou might have to waste it for",
    "start": "3433840",
    "end": "3441170"
  },
  {
    "text": "some reason. But as far as a communication\nis concerned you're simply wasting it.",
    "start": "3441170",
    "end": "3446830"
  },
  {
    "text": "So your energy per bit changes,\nbut your probability of error remains the same.",
    "start": "3446830",
    "end": "3453310"
  },
  {
    "text": "Because of that, you get a very\nclear cut idea of what",
    "start": "3453310",
    "end": "3458450"
  },
  {
    "text": "it's costing you to send\nthat pilot tone. Because in fact what you've done\nis to just increase this",
    "start": "3458450",
    "end": "3465990"
  },
  {
    "text": "energy, which we talk about\nin terms of db. If c is equal to a in this case\nwhich, as will see, is a",
    "start": "3465990",
    "end": "3474340"
  },
  {
    "text": "common thing that happens in a\nlot of systems, what you've lost is a factor three db.",
    "start": "3474340",
    "end": "3479589"
  },
  {
    "text": "Because you're using twice as\nmuch energy which is three db more energy, than you have\nto use for the pure",
    "start": "3479590",
    "end": "3485849"
  },
  {
    "text": "communication. So it's costing you three db to\ndo whatever silly thing you",
    "start": "3485850",
    "end": "3492040"
  },
  {
    "text": "want to do for synchronization\nor something else. Which is why people work very\nhard to try to send signals",
    "start": "3492040",
    "end": "3498560"
  },
  {
    "text": "which carry their own\nsynchronization information in them. And we will talk about that more\nwhen we get to wireless",
    "start": "3498560",
    "end": "3505030"
  },
  {
    "text": "and things like that. ",
    "start": "3505030",
    "end": "3511100"
  },
  {
    "text": "OK. Let's go on to real antipodal\nvectors in white Gaussian noise.",
    "start": "3511100",
    "end": "3519030"
  },
  {
    "text": "And again, let me point out to\nyou again that one of the remarkable things about\ndetection theory is once you",
    "start": "3519030",
    "end": "3525500"
  },
  {
    "text": "understand detection for\nantipodal binary signals and",
    "start": "3525500",
    "end": "3531940"
  },
  {
    "text": "Gaussian noise, everything\nelse just follows along. ",
    "start": "3531940",
    "end": "3538260"
  },
  {
    "text": "OK, so here what we're going to\ndo is to assume that under the hypothesis H equals zero--\nin other words conditional on",
    "start": "3538260",
    "end": "3545839"
  },
  {
    "text": "a zero entering the\ncommunication system-- what we're going to send is not\na single degree, is not",
    "start": "3545840",
    "end": "3554700"
  },
  {
    "text": "something in a single\ndegree of freedom. But we're actually going\nto send a vector. And you can think of that if you\nwant to as sending a way",
    "start": "3554700",
    "end": "3563520"
  },
  {
    "text": "form and breaking up the\nwaveform into an orthonormal expansion and a1 to\nak as being the",
    "start": "3563520",
    "end": "3571440"
  },
  {
    "text": "coefficients in that expansion. So we're going to use\nseveral degrees of freedom to send one signal.",
    "start": "3571440",
    "end": "3577870"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE] PROFESSOR: I'm only\nsending one bit.",
    "start": "3577870",
    "end": "3586270"
  },
  {
    "text": "And on Wednesday I'm going to\ntalk about what happens when we want to send multiple bits\nor when we want to send a",
    "start": "3586270",
    "end": "3594400"
  },
  {
    "text": "large number of signals in one\ndegree of freedom, or all those cases, multiple\nhypotheses.",
    "start": "3594400",
    "end": "3601040"
  },
  {
    "text": "You know what's going\nto happen? It's going to turn out to be\na trivial problem again.",
    "start": "3601040",
    "end": "3606750"
  },
  {
    "text": "Multiple hypotheses are no\nharder than just binary hypotheses.",
    "start": "3606750",
    "end": "3612560"
  },
  {
    "text": "So again, once you understand\nthe simplest case, all Gaussian problems turn\nout to be solved.",
    "start": "3612560",
    "end": "3618660"
  },
  {
    "text": "Just with minor variations\nand minor tweaks.",
    "start": "3618660",
    "end": "3624240"
  },
  {
    "text": "OK, so I have antipodal\nvectors. One vector is a1 to a sub k.",
    "start": "3624240",
    "end": "3630020"
  },
  {
    "text": "Under the other hypothesis we're\ngoing to send minus a, which is the opposite vector.",
    "start": "3630020",
    "end": "3636550"
  },
  {
    "text": "So if we're dealing with two\ndimensional space with coordinates here and here,\nif I send this I'm",
    "start": "3636550",
    "end": "3643140"
  },
  {
    "text": "going to send this. If I send that I'm going to\nsend that, and so forth. As the opposite alternative.",
    "start": "3643140",
    "end": "3651030"
  },
  {
    "text": "The likelihood ratio is then,\nthe probability of vector v",
    "start": "3651030",
    "end": "3658650"
  },
  {
    "text": "conditional on sending this. I'm assuming here that the noise\nis IID and each noise",
    "start": "3658650",
    "end": "3666779"
  },
  {
    "text": "variable has mean zero and\nvariance n0 over 2.",
    "start": "3666780",
    "end": "3672650"
  },
  {
    "text": "Namely, we're pretending we're\ncommunication people here, using and n0 over 2 here.",
    "start": "3672650",
    "end": "3678430"
  },
  {
    "text": "So the conditional density-- the likelihood of this output\nvector given zero-- is just",
    "start": "3678430",
    "end": "3687090"
  },
  {
    "text": "this density of z shifted\nover by a.",
    "start": "3687090",
    "end": "3692350"
  },
  {
    "text": "So it's what we've talked about\nas the Gaussian density.",
    "start": "3692350",
    "end": "3697580"
  },
  {
    "text": "Just this, which is just the\nenergy in v minus a is what",
    "start": "3697580",
    "end": "3706540"
  },
  {
    "text": "that turns out to be. So the log likelihood ratio is\nthe ratio of this quantity to",
    "start": "3706540",
    "end": "3715460"
  },
  {
    "text": "the density where H\nis equal to one. And when H is equal to one, what\nhappens is the same thing",
    "start": "3715460",
    "end": "3726180"
  },
  {
    "text": "as happened before. One makes this sign turn\ninto a plus sign.",
    "start": "3726180",
    "end": "3734039"
  },
  {
    "text": "So when I look at the log\nlikelihood ratio, I want to take the ratio of this quantity\nto the same quantity",
    "start": "3734040",
    "end": "3743480"
  },
  {
    "text": "with a plus put into it. And when I take the log of that,\nwhat happens is I get",
    "start": "3743480",
    "end": "3749250"
  },
  {
    "text": "this term minus the opposite\nterm of the opposite side. So I have minus the norm squared\nof v minus a plus the",
    "start": "3749250",
    "end": "3758930"
  },
  {
    "text": "norm squared of v\nplus a over n0. And again, if you multiply\nthis out, the v squareds",
    "start": "3758930",
    "end": "3766079"
  },
  {
    "text": "cancel out. The a squareds cancel out. And you just get the inner\nproduct terms.",
    "start": "3766080",
    "end": "3771460"
  },
  {
    "text": "And strangely enough you get the\nsame formula that you got before, almost, except here you\nhave the inner product of",
    "start": "3771460",
    "end": "3778130"
  },
  {
    "text": "v with a instead of just the\nproduct of v times a. So in fact we just have a slight\ngeneralization of the",
    "start": "3778130",
    "end": "3786440"
  },
  {
    "text": "thing that we did before. In other words, the\nscalar product is",
    "start": "3786440",
    "end": "3801980"
  },
  {
    "text": "a sufficient statistic. Now what does that tell you?",
    "start": "3801980",
    "end": "3807850"
  },
  {
    "text": "It tells you how to\nbuild a detector. OK? It tells you when you have a\nvector detection problem, the",
    "start": "3807850",
    "end": "3814770"
  },
  {
    "text": "thing that you want to do is to\ntake this vector, v, that",
    "start": "3814770",
    "end": "3820440"
  },
  {
    "text": "you have and form the inner\nproduct of v times a.",
    "start": "3820440",
    "end": "3827260"
  },
  {
    "text": "If in fact v is a waveform\nand a is a waveform, what do you do then?",
    "start": "3827260",
    "end": "3832600"
  },
  {
    "text": " Well the first thing you\ndo is to think of",
    "start": "3832600",
    "end": "3838549"
  },
  {
    "text": "v as being a vector-- where it's the vector of\ncoefficients-- in the",
    "start": "3838550",
    "end": "3844150"
  },
  {
    "text": "expansion for that waveform\nand a in the same way.",
    "start": "3844150",
    "end": "3850470"
  },
  {
    "text": "You look at what the inner\nproduct is then, and then you say, \"well what does that\ncorrespond to when I deal with",
    "start": "3850470",
    "end": "3856859"
  },
  {
    "text": "L2 waveforms?\" What's the inner product for L2 waveforms? ",
    "start": "3856860",
    "end": "3866619"
  },
  {
    "text": "It's the integral of the product\nof the waveforms.  And how do you form the\nintegral of the",
    "start": "3866620",
    "end": "3872930"
  },
  {
    "text": "product of the waveforms? You take this waveform here.",
    "start": "3872930",
    "end": "3878520"
  },
  {
    "text": "You turn it around and you call\nit a matched filter to a.",
    "start": "3878520",
    "end": "3884770"
  },
  {
    "text": "And you take the received\nwaveform. You pass it through the max\nfilter for a, and you look at",
    "start": "3884770",
    "end": "3890279"
  },
  {
    "text": "the output for it.  Now, let's go back and look at\nwhat all of this was doing.",
    "start": "3890280",
    "end": "3901820"
  },
  {
    "text": "And for now let's forget\nabout the baseband to the passband business. Let's just look at this part\nhere because it's a little",
    "start": "3901820",
    "end": "3910070"
  },
  {
    "text": "easier to see this first. ",
    "start": "3910070",
    "end": "3921040"
  },
  {
    "text": "So this comes in here. Now remember what\nwe were saying when we studied Nyquist.",
    "start": "3921040",
    "end": "3926530"
  },
  {
    "text": "We said a neat thing to do was\nto use a square root of the",
    "start": "3926530",
    "end": "3932095"
  },
  {
    "text": "Nyquist pulse at the\ntransmitter. When you use a square root of\nthe Nyquist pulse at the transmitter, what you have is\northogonality between the",
    "start": "3932095",
    "end": "3941920"
  },
  {
    "text": "pulse and all of its shifts. Well now we don't much care\nabout the orthogonality between the pulse and all of the\nshifts because we're only",
    "start": "3941920",
    "end": "3949820"
  },
  {
    "text": "sending this one bit anyway. But it sort of looks like we're\ngoing to be able to put",
    "start": "3949820",
    "end": "3954990"
  },
  {
    "text": "that back in in a nice\nconvenient way. So we're sending this one pulse,\np of t, city and what",
    "start": "3954990",
    "end": "3962000"
  },
  {
    "text": "did we do in this baseband\ndemodulator?  We passed this through another\nfilter, q of t, which was the",
    "start": "3962000",
    "end": "3971049"
  },
  {
    "text": "matched filter to p of t. What's our optimal detector\nfor maximum likelihood?",
    "start": "3971050",
    "end": "3979220"
  },
  {
    "text": "It's to take whatever this\nwaveform was, pass it through the matched filter. In other words, to calculate\nthat inner product we just",
    "start": "3979220",
    "end": "3986120"
  },
  {
    "text": "talked about. OK? So in fact when we were looking\nat the Nyquist problem",
    "start": "3986120",
    "end": "3994090"
  },
  {
    "text": "and worrying about inner symbol\ninterference, in fact what we were doing was also\ndoing the first part of an",
    "start": "3994090",
    "end": "4001590"
  },
  {
    "text": "optimal MAP detector. And at this point what comes\nout of here is a single",
    "start": "4001590",
    "end": "4008760"
  },
  {
    "text": "number, v, which in fact now is\nthe inner product of this",
    "start": "4008760",
    "end": "4014740"
  },
  {
    "text": "waveform at this point. With the waveform,\na, that we sent.",
    "start": "4014740",
    "end": "4021450"
  },
  {
    "text": "OK? In other words, we started out\nby saying, \"let's suppose that what we have here is a number.",
    "start": "4021450",
    "end": "4028280"
  },
  {
    "text": "What's the optimal detector to\nbuild?\" And then we go on and say, \"OK, let's suppose we\nlook at the problem here.",
    "start": "4028280",
    "end": "4035849"
  },
  {
    "text": "What's the optimal detector to\nbuild now?\" And the optimal detector to build now at this\npoint is this matched filter",
    "start": "4035850",
    "end": "4043850"
  },
  {
    "text": "to this input waveform.  Followed by the inner product\nhere-- which is what the match",
    "start": "4043850",
    "end": "4050770"
  },
  {
    "text": "filter does for us-- followed\nby our binary antipodal detector again.",
    "start": "4050770",
    "end": "4057620"
  },
  {
    "text": "OK? So by studying the problem at\nthis point, we now understand",
    "start": "4057620",
    "end": "4064770"
  },
  {
    "text": "what happens at this point. ",
    "start": "4064770",
    "end": "4071050"
  },
  {
    "text": "And do I have time to show you\nwhat happens at this point? I don't know. Let me--",
    "start": "4071050",
    "end": "4076630"
  },
  {
    "start": "4076630",
    "end": "4083269"
  },
  {
    "text": "let's not do that at\nleast right now-- let's look at the picture of\nthis that we get when we just",
    "start": "4083270",
    "end": "4090520"
  },
  {
    "text": "look at the problem when\nwe have two dimensions. So we're either going to\ntransmit a vector, a, or we're",
    "start": "4090520",
    "end": "4098940"
  },
  {
    "text": "going to transmit a\nvector, minus a. And think of this in\ntwo dimensions. When we transmit the\nvector, a, we have",
    "start": "4098940",
    "end": "4106049"
  },
  {
    "text": "two dimensional noise. We've already pointed out that\ntwo dimensional Gaussian noise",
    "start": "4106050",
    "end": "4111490"
  },
  {
    "text": "has circular symmetry. Spherical symmetry in an\narbitrary number of dimension. So what happens is you get\nthese equal probability",
    "start": "4111490",
    "end": "4118739"
  },
  {
    "text": "regions which are spreading out\nlike when you drop a rock into a pool of water.",
    "start": "4118740",
    "end": "4125359"
  },
  {
    "text": "You see all of these things\nspreading out in circles. And you then say, \"OK, what's\nthis inner product going to",
    "start": "4125360",
    "end": "4136120"
  },
  {
    "text": "correspond to?\" Finding the\ninner product and comparing it",
    "start": "4136120",
    "end": "4141330"
  },
  {
    "text": "with a threshold. Well you can see geometrically\nwhat's going to happen here.",
    "start": "4141330",
    "end": "4146940"
  },
  {
    "text": "You're trying to do maximum\nlikelihood. And we already know we're\nsupposed to calculate the",
    "start": "4146940",
    "end": "4153230"
  },
  {
    "text": "inner product, so what the inner\nproduct is going to do is take whatever v that we\nreceive-- it's going to",
    "start": "4153230",
    "end": "4159250"
  },
  {
    "text": "project it on to this line\nbetween 0 and a.",
    "start": "4159250",
    "end": "4165920"
  },
  {
    "text": "So if I got a v here I'm going\nto project it down to here. And then what I'm going to do\nis I'm going to compare the",
    "start": "4165920",
    "end": "4172210"
  },
  {
    "text": "distance from here to there\nwith the distance from here to there. Which says first project, then\ndo the old decision in a one",
    "start": "4172210",
    "end": "4181160"
  },
  {
    "text": "dimensional way. Now geometrically, this distance\nsquared is equal to",
    "start": "4181160",
    "end": "4187909"
  },
  {
    "text": "this distance squared plus\nthis distance squared. And this distance squared is\nequal to the same distance",
    "start": "4187910",
    "end": "4193880"
  },
  {
    "text": "squared plus this distance\nsquared. So whatever you decide to do in\nterms of these distances,",
    "start": "4193880",
    "end": "4200980"
  },
  {
    "text": "you will also decide to do in\nterms of these distances. Which also means that the\nmaximum likelihood regions",
    "start": "4200980",
    "end": "4207940"
  },
  {
    "text": "that you're going to develop,\nor in fact the maximum a posteriori probability regions\nare simply planes.",
    "start": "4207940",
    "end": "4215480"
  },
  {
    "text": "Which are perpendicular\nto the line between minus a and plus a.",
    "start": "4215480",
    "end": "4221330"
  },
  {
    "text": "OK? So if you're doing maximum\nlikelihood you just form a plane halfway between\nthese two points.",
    "start": "4221330",
    "end": "4227330"
  },
  {
    "text": "Yeah? AUDIENCE: [UNINTELLIGIBLE]",
    "start": "4227330",
    "end": "4242100"
  },
  {
    "text": "PROFESSOR: We got the error\nprobability just by first doing the projection and then\nturning it into this scale or",
    "start": "4242100",
    "end": "4248820"
  },
  {
    "text": "problem again. So in fact the error\nprobability-- What? AUDIENCE: [UNINTELLIGIBLE]",
    "start": "4248820",
    "end": "4255870"
  },
  {
    "text": "PROFESSOR: The probability of\nerror is just the probability of error in the projection. Did I write it down someplace?",
    "start": "4255870",
    "end": "4262860"
  },
  {
    "text": "Oh yeah, I did write it down. ",
    "start": "4262860",
    "end": "4268100"
  },
  {
    "text": "But I wrote it down, well\nI sort of cheated.",
    "start": "4268100",
    "end": "4274000"
  },
  {
    "text": "It's in the notes. I mean the likelihood ratio is\njust a center product here",
    "start": "4274000",
    "end": "4281330"
  },
  {
    "text": "which is a number. And when you find the error\nprobability, you just use the same q formula that\nwe used before.",
    "start": "4281330",
    "end": "4290030"
  },
  {
    "text": "And in place of a\nyou substitute-- ",
    "start": "4290030",
    "end": "4296699"
  },
  {
    "text": "in place of a you substitute the\ninner product of v with a.",
    "start": "4296700",
    "end": "4302800"
  },
  {
    "text": "Which is the corresponding\nquantity. So it's q of 4va\ndivided by n0.",
    "start": "4302800",
    "end": "4321080"
  },
  {
    "text": "OK? So that's the maximum likelihood\nerror probability.",
    "start": "4321080",
    "end": "4327780"
  },
  {
    "text": "OK? In other words, nothing\nnew has happened here. You just go through the match\nfilter and then you do this",
    "start": "4327780",
    "end": "4335900"
  },
  {
    "text": "same one dimensional problem\nthat we've already figured out how to do.",
    "start": "4335900",
    "end": "4341620"
  },
  {
    "text": "I think I'm going to stop there\nand we'll do the complex case which really corresponds to\nwhat happens after baseband",
    "start": "4341620",
    "end": "4353409"
  },
  {
    "text": "to passband then passband\nto baseband. ",
    "start": "4353410",
    "end": "4358825"
  }
]