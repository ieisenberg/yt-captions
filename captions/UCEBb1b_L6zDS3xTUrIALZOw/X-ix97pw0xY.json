[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare and ocw.mit.edu.",
    "start": "12720",
    "end": "19914"
  },
  {
    "text": "PHILIPPE RIGOLLET: The\nchapter is a natural capstone chapter for this entire course. We'll see some of\nthe things we've",
    "start": "19914",
    "end": "26760"
  },
  {
    "text": "seen during maximum likelihood\nand some of the things we've seen during linear\nregression, some of the things",
    "start": "26760",
    "end": "34080"
  },
  {
    "text": "we've seen in terms of the basic\nmodeling that we've had before. We're not going to go back\nto much inference questions.",
    "start": "34080",
    "end": "39655"
  },
  {
    "text": "It's really going to\nbe about modeling. And in a way, generalized\nlinear models, as the word says, are just a generalization\nof linear models.",
    "start": "39655",
    "end": "47010"
  },
  {
    "text": "And they're actually\nextremely useful. They're often forgotten\nabout and people just jump onto machine learning\nand sophisticated techniques.",
    "start": "47010",
    "end": "54720"
  },
  {
    "text": "But those things do\nthe job quite well. So let's see in what sense\nthey are a generalization of the linear models.",
    "start": "54720",
    "end": "62250"
  },
  {
    "text": "So remember, the linear\nmodel looked like this. We said that y was equal to x\ntranspose beta plus epsilon,",
    "start": "62250",
    "end": "73030"
  },
  {
    "text": "right? That was our linear\nregression model. And it's-- another way\nto say this is that if--",
    "start": "73030",
    "end": "79330"
  },
  {
    "text": "and let's assume\nthat those were, say, Gaussian with mean 0 and\nidentity covariance matrix.",
    "start": "79330",
    "end": "85230"
  },
  {
    "text": "Then another way\nto say this is that the conditional distribution\nof y given x is equal to--",
    "start": "85230",
    "end": "92700"
  },
  {
    "text": "sorry, I a Gaussian with mean\nx transpose beta and variance--",
    "start": "92700",
    "end": "99689"
  },
  {
    "text": "well, we had a sigma squared,\nwhich I will forget as usual-- x transpose beta and\nthen sigma squared.",
    "start": "99690",
    "end": "106080"
  },
  {
    "text": "OK, so here, we just assumed\nthat-- so what is regression is just saying I'm trying to\nexplain why as a function of x.",
    "start": "106080",
    "end": "114630"
  },
  {
    "text": "Given x, I'm assuming a\ndistribution for the y. And this x is just\ngoing to be here to help me model what the mean\nof this Gaussian is, right?",
    "start": "114630",
    "end": "125430"
  },
  {
    "text": "I mean, I could have\nsomething crazy. I could have something\nthat looks like y given",
    "start": "125430",
    "end": "133570"
  },
  {
    "text": "x is n0 x transpose beta. And then this could\nbe some other thing",
    "start": "133570",
    "end": "139660"
  },
  {
    "text": "which looks like, I don't\nknow, some x transpose gamma squared\ntimes, I don't know,",
    "start": "139660",
    "end": "146950"
  },
  {
    "text": "x, x transpose plus identity-- some crazy thing that\ndepends on x here, right?",
    "start": "146950",
    "end": "153250"
  },
  {
    "text": "And we deliberately assumed that\nall the thing that depends on x shows up in the mean, OK?",
    "start": "153250",
    "end": "159820"
  },
  {
    "text": "And so what I have\nhere is that y given x is a Gaussian\nwith a mean that",
    "start": "159820",
    "end": "165640"
  },
  {
    "text": "depends on x and covariance\nmatrix sigma square identity.",
    "start": "165640",
    "end": "171240"
  },
  {
    "text": "Now the linear model\nassumed a very specific form for the mean. It said I want the\nmean to be equal to x",
    "start": "171240",
    "end": "179190"
  },
  {
    "text": "transpose beta\nwhich, remember, was the sum from, say, j equals\n1 to p of beta j xj, right?",
    "start": "179190",
    "end": "190270"
  },
  {
    "text": "It's where the xj's are\nthe coordinates of x. But I could do something\nalso more complicated, right?",
    "start": "190270",
    "end": "196050"
  },
  {
    "text": "I could have something\nthat looks like instead , replace this by, I don't know,\nsum of beta j log of x to the j",
    "start": "196050",
    "end": "208989"
  },
  {
    "text": "divided by x to the j squared\nor something like this, right?",
    "start": "208990",
    "end": "214450"
  },
  {
    "text": "I could do this as well. So there's two things\nthat we have assumed.",
    "start": "214450",
    "end": "219630"
  },
  {
    "text": "The first one is\nthat when I look at the conditional\ndistribution of y given x, x affects only the mean.",
    "start": "219630",
    "end": "225570"
  },
  {
    "text": "I also assume that\nit was Gaussian and that it affects\nonly the mean. And the mean is affected\nin a very specific way,",
    "start": "225570",
    "end": "231130"
  },
  {
    "text": "which is linear in x, right? So this is\nessentially the things",
    "start": "231130",
    "end": "236270"
  },
  {
    "text": "we're going to try to relax. So the first thing\nthat we assume, the fact that y was Gaussian and\nhad only its mean [INAUDIBLE]",
    "start": "236270",
    "end": "243300"
  },
  {
    "text": "dependant no x is what's\ncalled the random component. It just says that the\nresponse variables, you know,",
    "start": "243300",
    "end": "249435"
  },
  {
    "text": "it sort of makes sense to\nassume that they're Gaussian. And everything was\nessentially captured, right?",
    "start": "249435",
    "end": "257220"
  },
  {
    "text": "So there's this\nproperty of Gaussians that if you tell me-- if\nthe variance is known, all you need to tell\nme to understand",
    "start": "257220",
    "end": "263610"
  },
  {
    "text": "exactly what the distribution\nof a Gaussian is, all you need to tell me\nis its expected value.",
    "start": "263610",
    "end": "269110"
  },
  {
    "text": "All right, so\nthat's this mu of x. And the second thing is that\nwe have this link that says,",
    "start": "269110",
    "end": "275570"
  },
  {
    "text": "well, I need to find a way\nto use my x's to explain this mu you and the\nlink was exactly mu of x was equal\nto x transpose beta.",
    "start": "275570",
    "end": "282389"
  },
  {
    "text": " Now we are talking about\ngeneralized linear models.",
    "start": "282390",
    "end": "291139"
  },
  {
    "text": "So this part here where mu\nof x is of the form-- the way",
    "start": "291140",
    "end": "296150"
  },
  {
    "text": "I want my beta, my x,\nto show up is linear, this will never be a question.",
    "start": "296150",
    "end": "303380"
  },
  {
    "text": "In principle, I could\nadd a third point, which is just question this\npart, the fact that mu of x",
    "start": "303380",
    "end": "310250"
  },
  {
    "text": "is x transpose beta. I could have some more\ncomplicated, nonlinear function of x. And then we'll never do\nthat because we're talking",
    "start": "310250",
    "end": "315740"
  },
  {
    "text": "about generalized linear model. The only thing with generalize\nare the random component, the conditional\ndistribution of y given x,",
    "start": "315740",
    "end": "323330"
  },
  {
    "text": "and the link that just says,\nwell, once you actually tell me that the only thing I need\nto figure out is the mean,",
    "start": "323330",
    "end": "329540"
  },
  {
    "text": "I'm just going to slap it\nexactly these x transpose beta thing without any transformation\nof x transpose beta.",
    "start": "329540",
    "end": "336520"
  },
  {
    "text": "So those are the two things.  It will become\nclear what I mean.",
    "start": "336520",
    "end": "342259"
  },
  {
    "text": "This sounds like a\ntautology, but let's just see how we could extend that. So what we're going to do in\ngeneralized linear models--",
    "start": "342260",
    "end": "350140"
  },
  {
    "text": "right, so when I\ntalk about GLNs,",
    "start": "350140",
    "end": "355482"
  },
  {
    "text": "the first thing I'm\ngoing to do with my x is turn it into some\nx transpose beta. And that's just\nthe l part, right?",
    "start": "355482",
    "end": "362371"
  },
  {
    "text": "I'm not going to\nbe able to change. That's the way it works. I'm not going to do\nanything non-linear.",
    "start": "362372",
    "end": "367530"
  },
  {
    "text": "But the two things\nI'm going to change is this random\ncomponent, which is",
    "start": "367530",
    "end": "376430"
  },
  {
    "text": "that y, which used to be some\nGaussian with mean mu of x here in sigma squared--",
    "start": "376430",
    "end": "384200"
  },
  {
    "text": "so y given x, sorry-- this is going to become y given\nx follows some distribution.",
    "start": "384200",
    "end": "395770"
  },
  {
    "text": "And I'm not going to\nallow any distribution. I want something that comes\nfrom the exponential family.",
    "start": "395770",
    "end": "400900"
  },
  {
    "start": "400900",
    "end": "409910"
  },
  {
    "text": "Who knows what the exponential\nfamily of distribution is? This is not the same thing as\nthe exponential distribution.",
    "start": "409910",
    "end": "415970"
  },
  {
    "text": "It's a family of distributions. All right, so we'll see that. It's-- wow.",
    "start": "415970",
    "end": "421770"
  },
  {
    "text": " What can that be? Oh yeah, that's\nactually [INAUDIBLE]..",
    "start": "421770",
    "end": "428194"
  },
  {
    "text": " So-- I'm sorry?",
    "start": "428194",
    "end": "437050"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: I'm\nin presentation mode. That should not happen.",
    "start": "437050",
    "end": "443650"
  },
  {
    "text": "OK, so hopefully, this is muted. ",
    "start": "443650",
    "end": "449390"
  },
  {
    "text": "So essentially, this is going\nto be a family of distributions. And what makes them\nexponential typically",
    "start": "449390",
    "end": "454442"
  },
  {
    "text": "is that there's an\nexponential that shows up in the definition\nof the density, all right? We'll see that the\nGaussian belongs",
    "start": "454442",
    "end": "461000"
  },
  {
    "text": "to the exponential family. But they're slightly\nless expected ones because there's this crazy\nthing that a to the x",
    "start": "461000",
    "end": "468569"
  },
  {
    "text": "is exponential x log a, which\nmakes the potential show up without being there. So if there's an\nexponential of some power,",
    "start": "468570",
    "end": "474910"
  },
  {
    "text": "it's going to show up. But it's more than that. So we'll actually come\nto this particular family of distribution. Why this particular family?",
    "start": "474910",
    "end": "480990"
  },
  {
    "text": "Because in a way,\neverything we've done for the linear\nmodel with Gaussian is going to extend fairly\nnaturally to this family.",
    "start": "480990",
    "end": "488610"
  },
  {
    "text": "All right, and it actually\nalso, because it encompasses pretty much everything,\nall the distributions we've discussed before.",
    "start": "488610",
    "end": "495950"
  },
  {
    "text": "All right, so the second thing\nthat I want to question-- right, so before,\nwe just said, well,",
    "start": "495950",
    "end": "502260"
  },
  {
    "text": "mu of x was directly\nequal to this thing.",
    "start": "502260",
    "end": "508560"
  },
  {
    "text": " Mu of x was directly\nx transpose beta.",
    "start": "508560",
    "end": "514260"
  },
  {
    "text": "So I knew I was going to\nhave an x transpose beta and I said, well, I could do\nsomething with this x transpose beta before I used it to\nexplain the expected value.",
    "start": "514260",
    "end": "522750"
  },
  {
    "text": "But I'm actually\ntaking it like that. Here, we're going to say, let's\nextend this to some function",
    "start": "522750",
    "end": "532200"
  },
  {
    "text": "is equal to this thing. Now admittedly, this is\nnot the most natural way to think about it.",
    "start": "532200",
    "end": "537600"
  },
  {
    "text": "What you would probably\nfeel more comfortable doing is write something like\nmu of x is a function.",
    "start": "537600",
    "end": "543870"
  },
  {
    "text": "Let's call it f of\nx transpose beta. But here, I decide\nto call f g inverse.",
    "start": "543870",
    "end": "552850"
  },
  {
    "text": "OK, let's just my g inverse. Yes. AUDIENCE: Is this different\nthen just [INAUDIBLE]",
    "start": "552850",
    "end": "558430"
  },
  {
    "text": "PHILIPPE RIGOLLET: Yeah.  I mean, what transformation\nyou want to put on your x's?",
    "start": "558430",
    "end": "566855"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]",
    "start": "566855",
    "end": "575120"
  },
  {
    "text": "PHILIPPE RIGOLLET: Oh\nno, certainly not, right? I mean, if I give you-- if I\nforce you to work with x1 plus",
    "start": "575120",
    "end": "580820"
  },
  {
    "text": "x2, you cannot work with\nany function of x1 plus any function of x2, right?",
    "start": "580820",
    "end": "586050"
  },
  {
    "text": "So this is different. ",
    "start": "586050",
    "end": "591900"
  },
  {
    "text": "All right, so-- yeah. The transformation would\nbe just the simple part",
    "start": "591900",
    "end": "597899"
  },
  {
    "text": "of your linear\nregression problem where you would take your\nexes, transform them, and then just apply\nanother linear regression.",
    "start": "597900",
    "end": "603959"
  },
  {
    "text": "This is genuinely new.  Any other question? ",
    "start": "603960",
    "end": "611040"
  },
  {
    "text": "All right, so this\nfunction g and the reason why I sort of have to, like,\nstick to this slightly less",
    "start": "611040",
    "end": "616830"
  },
  {
    "text": "natural way of defining\nit is because that's g that gets a name, not g\ninverse that gets a name. And the name of g is\nthe link function.",
    "start": "616830",
    "end": "623329"
  },
  {
    "start": "623330",
    "end": "629950"
  },
  {
    "text": "So if I want to give you a\ngeneralized linear model, I need to give you\ntwo ingredients.",
    "start": "629950",
    "end": "635250"
  },
  {
    "text": "The first one is the\nrandom component, which is the distribution\nof y given x. And it can be anything in what's\ncalled the exponential family",
    "start": "635250",
    "end": "644519"
  },
  {
    "text": "of distributions. So for example, I\ncould say, y given x is Gaussian with mean\nmu x sigma identity.",
    "start": "644520",
    "end": "650910"
  },
  {
    "text": "But I can also\ntell you y given x is gamma with shared parameter\nequal to alpha of x, OK?",
    "start": "650910",
    "end": "657580"
  },
  {
    "text": "I could do some weird\nthings like this. And the second thing is I need\nto give you a link function.",
    "start": "657580",
    "end": "663930"
  },
  {
    "text": "And the link function is\ngoing to become very clear how you pick a link function.",
    "start": "663930",
    "end": "669860"
  },
  {
    "text": "And the only reason that you\nactually pick a link function is because of compatibility.",
    "start": "669860",
    "end": "675009"
  },
  {
    "text": "This mu of x, I call\nit mu because mu of x is always the conditional\nexpectation of y given x,",
    "start": "675010",
    "end": "681949"
  },
  {
    "text": "always, which means\nthat let's think of y as being a Bernoulli\nrandom variable.",
    "start": "681950",
    "end": "687660"
  },
  {
    "text": " Where does mu of x live? ",
    "start": "687660",
    "end": "697430"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: 0, 1, right? That's the expectation\nof a Bernoulli. It's just the probability\nthat my coin flip gives me 1.",
    "start": "697430",
    "end": "703630"
  },
  {
    "text": "So it's a number\nbetween 0 and 1. But this guy right here, if\nmy x's are anything, right--",
    "start": "703630",
    "end": "709960"
  },
  {
    "text": "think of any body\nmeasurements plus [INAUDIBLE] linear combinations with\narbitrarily large coefficients.",
    "start": "709960",
    "end": "715519"
  },
  {
    "text": "This thing can be\nany real number. So the link function, what\nit's effectively going to do",
    "start": "715520",
    "end": "721180"
  },
  {
    "text": "is make those two\nthings compatible. It's going to take\nmy number which, for example, is constrained\nto be between 0 and 1",
    "start": "721180",
    "end": "727270"
  },
  {
    "text": "and map it into the\nentire real line. If I have mu which is forced\nto be positive, for example,",
    "start": "727270",
    "end": "733380"
  },
  {
    "text": "in an exponential distribution,\nthe mean is positive, right? That's the, say, don't\nknow, inter-arrival time",
    "start": "733380",
    "end": "740850"
  },
  {
    "text": "for Poisson process. This thing is known to be\npositive for an exponential. I need to map something\nthat's exponential",
    "start": "740850",
    "end": "747060"
  },
  {
    "text": "to the entire real line. I need a function that\ntakes something positive and [INAUDIBLE] everywhere. So we'll see.",
    "start": "747060",
    "end": "752520"
  },
  {
    "text": "By the end of this\nchapter, you will have 100 ways of doing this, but\nthere are some more traditional ones [INAUDIBLE].",
    "start": "752520",
    "end": "758560"
  },
  {
    "text": "So before we go any further,\nI gave you the example of a Bernoulli random variable.",
    "start": "758560",
    "end": "766808"
  },
  {
    "text": "Let's see a few examples\nthat actually fit there. Yes.  AUDIENCE: Will it come up\nlater [INAUDIBLE] already know",
    "start": "766809",
    "end": "773509"
  },
  {
    "text": "why do we need the\ntransformer [INAUDIBLE] why don't [INAUDIBLE]",
    "start": "773509",
    "end": "779300"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nWell actually, this will not come up later. It should be very\nclear from here",
    "start": "779300",
    "end": "784510"
  },
  {
    "text": "because if I actually\nhave a model, I just want it to\nbe plausible, right? I mean, what happens if I\nsuddenly decide that my--",
    "start": "784510",
    "end": "791040"
  },
  {
    "text": "so this is what's\ngoing to happen. You're going to have only\ndata to fit this model. Let's say you actually\nforget about this thing here.",
    "start": "791040",
    "end": "797530"
  },
  {
    "text": "You can always do this, right? You can always say I'm\ngoing to pretend my y's just",
    "start": "797530",
    "end": "803974"
  },
  {
    "text": "happen to be the realizations\nof said Gaussians that happen to be 0 or 1 only. You can always, like, stuff that\nin some linear model, right?",
    "start": "803974",
    "end": "812020"
  },
  {
    "text": "You will have some least\nsquares estimated for beta. And it's going to be fine. For all the points\nthat you see, it",
    "start": "812020",
    "end": "818630"
  },
  {
    "text": "will definitely put\nsome number that's actually between 0 and 1. So this is what your picture\nis going to look like.",
    "start": "818630",
    "end": "824140"
  },
  {
    "text": "You're going to have a\nbunch of values for x. This is your y.",
    "start": "824140",
    "end": "830169"
  },
  {
    "text": "And for different-- so\nthese are the values of x that you will get. And for a y, you will see\neither a 0 or a 1, right?",
    "start": "830169",
    "end": "835920"
  },
  {
    "text": " Right, that's what your\nBernoulli dataset would look",
    "start": "835920",
    "end": "842990"
  },
  {
    "text": "like with a one dimensional x. Now if you do least squares\non this, you will find this.",
    "start": "842990",
    "end": "849680"
  },
  {
    "text": "And for this guy,\nthis line certainly takes values between 0 and 1. But let's say now\nyou get an x here.",
    "start": "849680",
    "end": "856242"
  },
  {
    "text": "You're going to actually\nstart pretending that the probability it spits\nout one conditionally in x is like 1.2, and that's\ngoing to be weird.",
    "start": "856242",
    "end": "862910"
  },
  {
    "start": "862910",
    "end": "868310"
  },
  {
    "text": "Any other questions? All right, so let's\nstart with some examples.",
    "start": "868310",
    "end": "874700"
  },
  {
    "text": "Right, I mean, you get so used\nto them through this course. So the first one is--",
    "start": "874700",
    "end": "881250"
  },
  {
    "text": "so all these things are taken. So there's a few\nbooks on generalizing, your models, generalize\n[INAUDIBLE] models. And there's tons of\napplications that you can see.",
    "start": "881250",
    "end": "888920"
  },
  {
    "text": "Those are extremely\nversatile, and as soon as you want to do modeling\nto explain some y given x, you sort of need to\ndo that if you want",
    "start": "888920",
    "end": "895400"
  },
  {
    "text": "to go beyond linear models. So this was in the\ndisease occurring rate.",
    "start": "895400",
    "end": "900610"
  },
  {
    "text": "So you have a disease\nepidemic and you want to basically model\nthe expected number",
    "start": "900610",
    "end": "908390"
  },
  {
    "text": "of new cases given-- at a certain time, OK? So you have time that progresses\nfor each of your reservation.",
    "start": "908390",
    "end": "916190"
  },
  {
    "text": "Each of your reservation\nis a time stamp-- say, I don't know, 20th day.",
    "start": "916190",
    "end": "921410"
  },
  {
    "text": "And your response is\nthe number of new cases. And you're going to actually\nput your model directly",
    "start": "921410",
    "end": "928520"
  },
  {
    "text": "on mu, right? When I looked at\nthis, everything here was on mu itself, on\nthe expected, right?",
    "start": "928520",
    "end": "934459"
  },
  {
    "text": "Mu of x is always the expected-- ",
    "start": "934460",
    "end": "939608"
  },
  {
    "text": "the conditional\nexpectation of y given x. ",
    "start": "939609",
    "end": "945279"
  },
  {
    "text": "right? So all I need to model\nis this expected value.",
    "start": "945280",
    "end": "951750"
  },
  {
    "text": "So this mu I'm going\nto actually say-- so I look at some parameters,\nand it says, well,",
    "start": "951750",
    "end": "957620"
  },
  {
    "text": "it increases exponentially. So I want to say I have some\nsort of exponential trend.",
    "start": "957620",
    "end": "962780"
  },
  {
    "text": "I can parametrize\nthat in several ways. And the two parameters\nI want to slap in is, like, some sort of gamma,\nwhich is just the coefficient.",
    "start": "962780",
    "end": "970190"
  },
  {
    "text": "And then there's some rate\ndelta that's in the exponential. So if I tell you\nit's exponential,",
    "start": "970190",
    "end": "975649"
  },
  {
    "text": "that's a nice family\nof functions you might want to think about, OK? So here, mu of x, if I want\nto keep the notation, x",
    "start": "975650",
    "end": "984520"
  },
  {
    "text": "is gamma exponential\ndelta x, right?",
    "start": "984520",
    "end": "990650"
  },
  {
    "text": "Except that here, my x\nare t1, t2, t3, et cetera. And I want to find what the\nparameters gamma and delta are",
    "start": "990650",
    "end": "997339"
  },
  {
    "text": "because I want to be\nable to maybe compare different epidemics and see if\nthey have the same parameter",
    "start": "997340",
    "end": "1002980"
  },
  {
    "text": "or maybe just do some\nprediction based on the data that I have without-- to\nextrapolate in the future.",
    "start": "1002980",
    "end": "1009070"
  },
  {
    "text": " So here, clearly mu of\nx is not of the form",
    "start": "1009070",
    "end": "1018280"
  },
  {
    "text": "x transpose beta, right? That's not x\ntranspose beta at all.",
    "start": "1018280",
    "end": "1024409"
  },
  {
    "text": "And it's actually not even a\nfunction of x transpose data, right? There's two parameters,\ngamma and delta,",
    "start": "1024410",
    "end": "1029899"
  },
  {
    "text": "and it's not of the form. So here we have x,\nwhich is 1 and x, right? I have two parameters.",
    "start": "1029900",
    "end": "1036199"
  },
  {
    "text": "So what I do here\nis that I say, well, first, let me transform\nmu in such a way that I can hope to see\nsomething that's linear.",
    "start": "1036200",
    "end": "1043119"
  },
  {
    "text": "So if I transform mu, I'm\ngoing to have log of mu, which is log of this thing, right? So log of mu of\nx is equal, well,",
    "start": "1043119",
    "end": "1053770"
  },
  {
    "text": "to log of gamma plus\nlog of exponential delta x, which is delta x.",
    "start": "1053770",
    "end": "1059350"
  },
  {
    "text": " And now this thing is\nactually linear in x.",
    "start": "1059350",
    "end": "1066190"
  },
  {
    "text": "So I have that this\nguy is my first beta 1. And so that's beta 1 finds 1. And this guy is beta 2--",
    "start": "1066190",
    "end": "1073320"
  },
  {
    "text": "times, sorry that said beta\n0-- times 1, and this guy is beta 1 times x. OK, so that looks\nlike a linear model.",
    "start": "1073320",
    "end": "1080200"
  },
  {
    "text": "I just have to change\nmy parameters-- my parameters beta 1 becomes\nthe log of gamma and beta 2",
    "start": "1080200",
    "end": "1085840"
  },
  {
    "text": "becomes delta itself. And the reason why we do this\nis because, well, the way",
    "start": "1085840",
    "end": "1091210"
  },
  {
    "text": "we put those gamma and those\ndelta was just so that we have some parametrization. It just so happens that if\nwe want this to be linear,",
    "start": "1091210",
    "end": "1097299"
  },
  {
    "text": "we need to just change the\nparametrization itself. This is going to\nhave some effects. We know that it's going\nto have some effect",
    "start": "1097300",
    "end": "1103301"
  },
  {
    "text": "in the fissure information. It's going to have a bunch of\neffect to change those things. But that's what needs\nto be done to have",
    "start": "1103301",
    "end": "1109510"
  },
  {
    "text": "a generalized linear model. Now here, the\nfunction that I took",
    "start": "1109510",
    "end": "1115460"
  },
  {
    "text": "to turn it into something\nthat's linear is simple. It came directly from some\nnatural thing I would do here,",
    "start": "1115460",
    "end": "1121000"
  },
  {
    "text": "which is taking the log. And so the function g,\nthe link that I take, is called the log\nlink very creatively.",
    "start": "1121000",
    "end": "1127530"
  },
  {
    "text": "And it's just the\nfunction that I apply to mu so that I see\nsomething that's linear and that looks like this.",
    "start": "1127530",
    "end": "1133260"
  },
  {
    "start": "1133260",
    "end": "1139580"
  },
  {
    "text": "So now this only tells me how\nto deal with the link function. But I still have\nto deal with 0.1.",
    "start": "1139580",
    "end": "1146380"
  },
  {
    "text": "And this, again, is\njust some modeling. Given some data,\nsome random data, what distribution do you choose\nto explain the randomness?",
    "start": "1146380",
    "end": "1154630"
  },
  {
    "text": "And this-- I mean,\nunless there's no choice, you know, it's just a\nmatter of practice, right?",
    "start": "1154630",
    "end": "1159820"
  },
  {
    "text": "I mean, why would it be\nGaussian and not, you know, doubly exponential? This is-- there's matters\nof convenience that",
    "start": "1159820",
    "end": "1165472"
  },
  {
    "text": "come into this, and there's\njust matter of experience that come into this. You know, I remember when\nyou chat with engineers,",
    "start": "1165472",
    "end": "1172660"
  },
  {
    "text": "they have a very\ngood notion of what the distribution should be. They have y bold distributions.",
    "start": "1172660",
    "end": "1177970"
  },
  {
    "text": "You know, they do optics\nand things like this. So there's some distributions\nthat just come up but sometimes just have to work.",
    "start": "1177970",
    "end": "1183639"
  },
  {
    "text": "Now here what do we have? The thing we're\ntrying to measure, y-- as we said, so mu\nis the expectation,",
    "start": "1183640",
    "end": "1189789"
  },
  {
    "text": "the conditional\nexpectation, of y given x. But y is the number\nof new cases, right?",
    "start": "1189790",
    "end": "1196090"
  },
  {
    "text": "Well it's a number of. And the first thing\nyou should think of when you think\nabout number of, if it were bounded above, you\nwould think binomial, baby.",
    "start": "1196090",
    "end": "1203620"
  },
  {
    "text": "But here, it's just a number. So you think Poisson. That's how insurers think.",
    "start": "1203620",
    "end": "1208750"
  },
  {
    "text": "I have a number of, you\nknow, claims per year. This is a Poisson distribution.",
    "start": "1208750",
    "end": "1215570"
  },
  {
    "text": "And hopefully they can model\nthe conditional distribution of the number of claims given\neverything that they actually ask you in the\nsurveys that I hear",
    "start": "1215570",
    "end": "1224940"
  },
  {
    "text": "you now fail in 15 minutes. All right, so now you have\nthis Poisson distribution.",
    "start": "1224940",
    "end": "1231924"
  },
  {
    "text": "And that's just the\nmodeling assumption. There's no particular\nreason why you should do this except\nthat, you know, that might be a good idea.",
    "start": "1231924",
    "end": "1238049"
  },
  {
    "text": "And the expected\nvalue of your Poisson has to be this mu i, OK? At time i.",
    "start": "1238050",
    "end": "1246330"
  },
  {
    "text": "Any question about this slide? OK, so let's switch\nto another example.",
    "start": "1246330",
    "end": "1251660"
  },
  {
    "text": "Another example is the\nso-called pray capture rate. So here, what\nyou're interested in",
    "start": "1251660",
    "end": "1258010"
  },
  {
    "text": "is the rate capture of\npreys yi for a given prey.",
    "start": "1258010",
    "end": "1265330"
  },
  {
    "text": "And you have xy, which\nis your explanation.",
    "start": "1265330",
    "end": "1270730"
  },
  {
    "text": "And this is just\nthe density of pray. So you're trying to explain the\nrate of captures of preys given",
    "start": "1270730",
    "end": "1277030"
  },
  {
    "text": "the density of the prey, OK? And so you need to find\nsome sort of relationship",
    "start": "1277030",
    "end": "1282964"
  },
  {
    "text": "between the two. And here again,\nyou talk to experts and what they tell you\nis that, well, it's going to be increasing, right?",
    "start": "1282964",
    "end": "1288820"
  },
  {
    "text": "I mean, animals like predators\nare going to just eat more if there's more preys.",
    "start": "1288820",
    "end": "1294238"
  },
  {
    "text": "But at some point,\nthey're just going to level off because they're\ngoing to be [INAUDIBLE] full and they're going to stop\ncapturing those prays.",
    "start": "1294239",
    "end": "1302380"
  },
  {
    "text": "And you're just going to\nhave some phenomenon that looks like this. So here is a curve that\nsort of makes sense, right?",
    "start": "1302380",
    "end": "1307870"
  },
  {
    "text": "As your capture rate goes from\n0 to 1, you're increasing, and then you see you have\nthis like [INAUDIBLE] function",
    "start": "1307870",
    "end": "1314530"
  },
  {
    "text": "that says, you know, at\nsome point it levels up. OK, so here, one way I could-- I mean, there's again\nmany ways I could just",
    "start": "1314530",
    "end": "1321590"
  },
  {
    "text": "model a function\nthat looks like this. But a simple one that\nhas only two parameters is this one, where mu i is\nthis a function of xi where",
    "start": "1321590",
    "end": "1329930"
  },
  {
    "text": "I have some parameter alpha\nhere and some parameter h here. OK, so there's clearly--",
    "start": "1329930",
    "end": "1335820"
  },
  {
    "text": "so this function, there's one\nthat essentially tells you--",
    "start": "1335820",
    "end": "1341240"
  },
  {
    "text": "so this thing starts\nat 0 for sure. And essentially,\nalpha tells you how sharp this thing\nis, and h tells you",
    "start": "1341240",
    "end": "1348170"
  },
  {
    "text": "at which points you end here. Well, it's not exactly what\nthose values are equal to, but that tells you this.",
    "start": "1348170",
    "end": "1355380"
  },
  {
    "text": "OK, so, you know-- simple, and--",
    "start": "1355380",
    "end": "1361329"
  },
  {
    "text": "well, no, OK. Sorry, that's actually alpha,\nwhich is the maximum capture. The rate and h represent\nthe pre-density",
    "start": "1361329",
    "end": "1366450"
  },
  {
    "text": "at which the capture weight is. So that's the half time. OK, so there's actual\nvalue [INAUDIBLE]..",
    "start": "1366450",
    "end": "1372600"
  },
  {
    "text": "All right, so now I\nhave this function. It's certainly not a function. There's no-- I don't see\nit as a function of x.",
    "start": "1372600",
    "end": "1379330"
  },
  {
    "text": "So I need to find something that\nlooks like a function of x, OK?",
    "start": "1379330",
    "end": "1386390"
  },
  {
    "text": "So then here, there's no log. There's no-- well, I could\nactually take a log here.",
    "start": "1386390",
    "end": "1393570"
  },
  {
    "text": "But I would have log of\nx and log of x plus h. So that would be weird. So what we propose to\ndo here is to look,",
    "start": "1393570",
    "end": "1399990"
  },
  {
    "text": "rather than looking at mu\ni, we look 1 over mu i. Right, and so\nsince your function was mu i, when you\ntake 1 over mu i,",
    "start": "1399990",
    "end": "1417450"
  },
  {
    "text": "you get h plus xi divided\nby alpha xi, which",
    "start": "1417450",
    "end": "1422580"
  },
  {
    "text": "is h over alpha times one\nover xi plus 1 over alpha.",
    "start": "1422580",
    "end": "1429690"
  },
  {
    "text": "And now if I'm willing to\nmake this transformation of variables and say,\nactually, I don't-- my x, whether it's\nthe density of prey",
    "start": "1429690",
    "end": "1437899"
  },
  {
    "text": "or the inverse density of\nprey, it really doesn't matter. I can always make\nthis transformation when the data comes.",
    "start": "1437900",
    "end": "1443750"
  },
  {
    "text": "Then I'm actually just\ngoing to think of this as being some linear\nfunction beta 0 plus beta 1,",
    "start": "1443750",
    "end": "1451399"
  },
  {
    "text": "which is this guy,\ntimes 1 over xi.",
    "start": "1451400",
    "end": "1457345"
  },
  {
    "text": "And now my new variable\nbecomes 1 over xi. And now it's linear. And the transformation\nI had to take",
    "start": "1457345",
    "end": "1463350"
  },
  {
    "text": "was this 1 over x, which is\ncalled the reciprocal link, OK?",
    "start": "1463350",
    "end": "1474240"
  },
  {
    "text": "You can probably guess what the\nexponential link is going to be and things like this, all right? So we'll talk about other\nlinks that have slightly less",
    "start": "1474240",
    "end": "1481380"
  },
  {
    "text": "obvious names. Now again, modeling, right? So this was the\nrandom component.",
    "start": "1481380",
    "end": "1486580"
  },
  {
    "text": "This was the easy part. Now I need to just poor\nin some domain knowledge about how do I think this\nfunction, this y, which",
    "start": "1486580",
    "end": "1495900"
  },
  {
    "text": "is which is the rate\nof capture of praise,",
    "start": "1495900",
    "end": "1501810"
  },
  {
    "text": "I want to understand how\nthis thing is actually changing what is the randomness\nof the thing around its mean.",
    "start": "1501810",
    "end": "1509429"
  },
  {
    "text": "And you know, something\nthat-- so that comes from this textbook. The standing deviation\nof capture rate might be approximately\nproportional to the mean rate.",
    "start": "1509430",
    "end": "1516750"
  },
  {
    "text": "You need to find a\ndistribution that actually has this property. And it turns out\nthat this happens for gamma distributions, right?",
    "start": "1516750",
    "end": "1523950"
  },
  {
    "text": "In gamma distributions,\njust like say, for Poisson distribution, the--",
    "start": "1523950",
    "end": "1529740"
  },
  {
    "text": "well, for Poisson, the variance\nand mean are of the same order. Here is the standard\ndeviation that's of the same order as the\n[INAUDIBLE] for gammas.",
    "start": "1529740",
    "end": "1539540"
  },
  {
    "text": "And it's a positive\ndistribution as well. So here is a candidate. Now since we're\nsort of constrained",
    "start": "1539540",
    "end": "1545260"
  },
  {
    "text": "to work under the exponential\nfamily of distributions, then you can just\ngo through your list",
    "start": "1545260",
    "end": "1550360"
  },
  {
    "text": "and just decide which\none works best for you.  All right, third example--",
    "start": "1550360",
    "end": "1556940"
  },
  {
    "text": "so here we have binary response. Here, essentially the\nbinary response variable indicates the\npresence or absence",
    "start": "1556940",
    "end": "1562960"
  },
  {
    "text": "of postoperative deforming\nfor kyphosis on children. And here, rather than having\none covariance which was before,",
    "start": "1562960",
    "end": "1570309"
  },
  {
    "text": "in the first example, was\ntime, in the second example was the density, here\nthere's three ways that you measure on children.",
    "start": "1570310",
    "end": "1577030"
  },
  {
    "text": "The first one is\nage of the child and the second one is\nthe number of vertebrae involved in the operation.",
    "start": "1577030",
    "end": "1583039"
  },
  {
    "text": "And the third one is\nthe start of the range, right-- so where\nit is on the spine.",
    "start": "1583040",
    "end": "1589660"
  },
  {
    "text": "OK, so the response\nvariable here is, you know,",
    "start": "1589660",
    "end": "1595105"
  },
  {
    "text": "did it work or not, right? I mean, that's very simple. And so here, it's nice\nbecause the random component",
    "start": "1595105",
    "end": "1601859"
  },
  {
    "text": "is the easiest one. As I said, any random variable\nthat takes only two outcomes must be a Bernoulli, right?",
    "start": "1601859",
    "end": "1609020"
  },
  {
    "text": "So that's nice there's no\nmodeling going on here. So you know that y given x\nis going to be Bernoulli,",
    "start": "1609020",
    "end": "1614170"
  },
  {
    "text": "but of course, all\nyour efforts are going to try to understand\nwhat the conditional mean of your Bernoulli, what\nthe conditional probability",
    "start": "1614170",
    "end": "1620315"
  },
  {
    "text": "of being 1 is going to be, OK? And so in particular--\nso I'm just-- here,",
    "start": "1620315",
    "end": "1625960"
  },
  {
    "text": "I'm spelling it out before\nwe close those examples. I cannot say that mu of x is x\ntranspose data for exactly this",
    "start": "1625960",
    "end": "1632559"
  },
  {
    "text": "picture that I drew\nfor you here, right? There's just no\nway here-- the goal of doing this is certainly\nto be able to extrapolate",
    "start": "1632560",
    "end": "1640050"
  },
  {
    "text": "for yet unseen children\nwhether this is something that we should be doing. And maybe the range\nof x is actually",
    "start": "1640050",
    "end": "1647340"
  },
  {
    "text": "going to be slightly out. And so, OK I don't\nwant to see that have a negative probability of\noutcome or a positive one--",
    "start": "1647340",
    "end": "1654770"
  },
  {
    "text": "sorry, or one that's\nlower than one. So I need to make\nthis transformation.",
    "start": "1654770",
    "end": "1660970"
  },
  {
    "text": "So what I need to do is\nto transform mu, which is, we know only a number. All we know is a\nnumber between 0 and 1.",
    "start": "1660970",
    "end": "1666880"
  },
  {
    "text": "And we need to transform\nit in such a way that it maps the\nentire real line or reciprocally to say that--",
    "start": "1666880",
    "end": "1677270"
  },
  {
    "text": "or inversely, I should say-- that f of x\ntranspose beta should be a number between 0 and 1.",
    "start": "1677270",
    "end": "1682410"
  },
  {
    "text": "I need to find a function\nthat takes any real number and maps it into 0 and 1. And we'll see that\nagain, but you",
    "start": "1682410",
    "end": "1690460"
  },
  {
    "text": "have an army of functions\nthat do that for you. What are those functions? ",
    "start": "1690460",
    "end": "1696707"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: I'm sorry? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Trait? AUDIENCE: [INAUDIBLE]",
    "start": "1696707",
    "end": "1702664"
  },
  {
    "text": "PHILIPPE RIGOLLET: Oh. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Yeah, I want\nthem to be invertible, right?",
    "start": "1702665",
    "end": "1708059"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]",
    "start": "1708059",
    "end": "1714074"
  },
  {
    "text": "PHILIPPE RIGOLLET: I\nhave an army of function. I'm not asking for one\nsoldier in this army.",
    "start": "1714074",
    "end": "1719100"
  },
  {
    "text": "I want the name of this army. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Well, they're\nnot really invertible either,",
    "start": "1719100",
    "end": "1726640"
  },
  {
    "text": "right? So they're actually in\n[INAUDIBLE] textbook.",
    "start": "1726640",
    "end": "1733730"
  },
  {
    "text": "Because remember,\nstatisticians don't know how to integrate\nfunctions, but they know how to turn a function\ninto a Gaussian integral.",
    "start": "1733730",
    "end": "1739250"
  },
  {
    "text": "So we know it integrates\nto 1 and things like this. Same thing here--\nwe don't know how to build functions that\nare invertible and map",
    "start": "1739250",
    "end": "1746692"
  },
  {
    "text": "the entire real line\nto 0, 1, but there's all the cumulative distribution\nfunctions that do that for us. So I can you any of\nthose guys, and that's",
    "start": "1746692",
    "end": "1753190"
  },
  {
    "text": "what I'm going to\nbe doing, actually. All right, so just\nto recap what I just",
    "start": "1753190",
    "end": "1759730"
  },
  {
    "text": "said as we were speaking, so\nnormal linear model is not appropriate for these examples\nif only because the response",
    "start": "1759730",
    "end": "1770470"
  },
  {
    "text": "variable is not\nnecessarily Gaussian and also because the\nlinear model has to be--",
    "start": "1770470",
    "end": "1777429"
  },
  {
    "text": "the mean has to be transformed\nbefore I can actually apply a linear model for all\nthese plausible nonlinear models that I\nactually came up with.",
    "start": "1777430",
    "end": "1784890"
  },
  {
    "text": "OK, so the family\nwe're going to go for is the exponential\nfamily of distributions.",
    "start": "1784890",
    "end": "1790779"
  },
  {
    "text": "And we're going to\nbe able to show-- so one of the nice\npart of this is",
    "start": "1790780",
    "end": "1796120"
  },
  {
    "text": "to actually compute\nmaximum likelihood estimaters for those right? In the linear model,\nmaximum-- like, in the Gauss",
    "start": "1796120",
    "end": "1802389"
  },
  {
    "text": "linear model, maximum likelihood\nwas as nice as it gets, right? This actually was the\nleast squares estimator.",
    "start": "1802390",
    "end": "1808809"
  },
  {
    "text": "We had a close form. x transpose x inverse\nx transpose y, and that was it, OK?",
    "start": "1808810",
    "end": "1814120"
  },
  {
    "text": "We had to just take\none derivative. Here, we're going to have a\ngenerally concave likelihood.",
    "start": "1814120",
    "end": "1819580"
  },
  {
    "text": "We're not going to\nbe able to actually solve this thing\ndirectly in close form unless it's Gaussian,\nbut we will have--",
    "start": "1819580",
    "end": "1826610"
  },
  {
    "text": "we'll see actually\nhow this is not just a black box optimization\nof a concave function.",
    "start": "1826610",
    "end": "1832769"
  },
  {
    "text": "We have a lot of properties\nof this concave function, and we will be able to show\nsome iterative algorithms.",
    "start": "1832770",
    "end": "1838500"
  },
  {
    "text": "We'll basically see how, when\nyou opened the box of convex optimization, you will actually\nbe able to see how things work",
    "start": "1838500",
    "end": "1846270"
  },
  {
    "text": "and actually implement\nit using least squares. So each iteration of\nthis iterative algorithm will essentially\nbe a least squares,",
    "start": "1846270",
    "end": "1852760"
  },
  {
    "text": "and that's actually\nquite [INAUDIBLE].. So, very demonstrative\nof statisticians being pretty\ningenious so that they",
    "start": "1852760",
    "end": "1859769"
  },
  {
    "text": "don't have to call in\nsome statistical software but just can repeatedly\ncall their least squares",
    "start": "1859770",
    "end": "1866040"
  },
  {
    "text": "Oracle within a\nstatistical software. OK, so what is the\nexponential family, right?",
    "start": "1866040",
    "end": "1872169"
  },
  {
    "text": "I promised to do the\nexponential family. Before we go into\nthis, let me just",
    "start": "1872170",
    "end": "1877540"
  },
  {
    "text": "tell you something about\nexponential families, and what's the only\nthing to differentiate an exponential family from\nall possible distributions?",
    "start": "1877540",
    "end": "1885870"
  },
  {
    "text": "An exponential family has\ntwo parameters, right? And those are not\nreally parameters, but there's this theta parameter\nof my distribution, OK?",
    "start": "1885870",
    "end": "1893530"
  },
  {
    "text": "So it's going to be\nindexed by some parameter. Here, I'm only talking\nabout the distribution of, say, some random variable\nor some random vector, OK?",
    "start": "1893530",
    "end": "1900549"
  },
  {
    "text": "So here in this slide, you see\nthat the parameter theta that indexed those distribution\nis k dimensional",
    "start": "1900550",
    "end": "1908760"
  },
  {
    "text": "and the space of the x's\nthat I'm looking at-- so",
    "start": "1908760",
    "end": "1913840"
  },
  {
    "text": "that should really be y, right? What I'm going to\nplug in here is the conditional distribution\nof y given x and theta is",
    "start": "1913840",
    "end": "1919570"
  },
  {
    "text": "going to depend on x. But this really is the y. That's their distribution\nof the response variable.",
    "start": "1919570",
    "end": "1924770"
  },
  {
    "text": "And so this is on q, right? So I'm going to\nassume that y takes-- q dimensional--\nis q dimensional.",
    "start": "1924770",
    "end": "1932200"
  },
  {
    "text": "Clearly soon, q is\ngoing to be equal to 1, but I can define those\nthings generally. OK, so I have this.",
    "start": "1932200",
    "end": "1937750"
  },
  {
    "text": "I have to tell you\nwhat this looks like. And let's assume that this is\na probability density function.",
    "start": "1937750",
    "end": "1943309"
  },
  {
    "text": "So this, right this notation,\nthe fact that I just put my theta in\nsubscript, is just",
    "start": "1943310",
    "end": "1948490"
  },
  {
    "text": "for me to remember that\nthis is the variable that indicates the random variable,\nand this is just the parameter.",
    "start": "1948490",
    "end": "1954160"
  },
  {
    "text": "But I could just write it as a\nfunction of theta and x, right? This is just going to be--\nright, if you were in calc,",
    "start": "1954160",
    "end": "1959650"
  },
  {
    "text": "in multivariable\ncalc, you would have two parameter of theta\nand x and you would need to give me a function.",
    "start": "1959650",
    "end": "1965320"
  },
  {
    "text": "Now think of all-- think of x and theta as being\none dimensional at this point.",
    "start": "1965320",
    "end": "1970360"
  },
  {
    "text": "Think of all the\nfunctions that can be depending on theta and x. There's many of them.",
    "start": "1970360",
    "end": "1976660"
  },
  {
    "text": "And in particular, there's many\nways theta and x can interact.",
    "start": "1976660",
    "end": "1981810"
  },
  {
    "text": "What the exponential\nfamily does for you is that it restricts\nthe way these things can actually interact\nwith each other.",
    "start": "1981810",
    "end": "1987877"
  },
  {
    "text": "It's essentially\nsaying the following. It's saying this is going to\nbe of the form exponential--",
    "start": "1987877",
    "end": "1995700"
  },
  {
    "text": "so this exponential is\nreally not much because I could put a log next to it. But what I want is that\nthe way theta and x",
    "start": "1995700",
    "end": "2004940"
  },
  {
    "text": "interact has to be of\nthe form theta times x",
    "start": "2004940",
    "end": "2010309"
  },
  {
    "text": "in an exponential, OK? So that's the\nsimplest-- that's one of the ways you can think of\nthem interacting is you just",
    "start": "2010310",
    "end": "2016585"
  },
  {
    "text": "the product of the two. Now clearly, this is\nnot a very rich family. So what I'm allowing\nmyself is to just slap",
    "start": "2016585",
    "end": "2023090"
  },
  {
    "text": "on some terms that depend only\non theta and depend only on x. So let's just call this thing, I\ndon't know, f of x, g of theta.",
    "start": "2023090",
    "end": "2032630"
  },
  {
    "text": "OK, so here, I've restricted the\nway theta and x can interact. So I have something\nthat depends only",
    "start": "2032630",
    "end": "2038190"
  },
  {
    "text": "on x, something that\ndepends only on theta. And here, I have this\nvery specific interaction. And that's all that exponential\nfamilies are doing for you, OK?",
    "start": "2038190",
    "end": "2046310"
  },
  {
    "text": "So if we go back to this slide,\nthis is much more general, right? if I want to go from\ntheta and x in r to theta",
    "start": "2046310",
    "end": "2054770"
  },
  {
    "text": "and x theta in r--  to theta in r k and x in rq,\nI cannot take the product",
    "start": "2054770",
    "end": "2066658"
  },
  {
    "text": "of theta and x. I cannot even take the inner\nproduct between theta and x because they're not even\nof compatible dimensions.",
    "start": "2066659",
    "end": "2072030"
  },
  {
    "text": "But what I can do is to first\nmap my theta into something",
    "start": "2072030",
    "end": "2077460"
  },
  {
    "text": "and map my x into something\nso that I actually end up having the same dimensions. And then I can take\nthe inner product.",
    "start": "2077460",
    "end": "2083550"
  },
  {
    "text": "That's the natural\ngeneralization of this simple product. ",
    "start": "2083550",
    "end": "2099799"
  },
  {
    "text": "OK, so what I have is-- right, so if I want\nto go from theta",
    "start": "2099800",
    "end": "2105230"
  },
  {
    "text": "to x, when I'm going to first\ndo is I'm going to take theta,",
    "start": "2105230",
    "end": "2110510"
  },
  {
    "text": "eta of theta-- so let's say eta1 of\ntheta to eta k of theta.",
    "start": "2110510",
    "end": "2116589"
  },
  {
    "text": " And then I'm going\nto actually take",
    "start": "2116590",
    "end": "2122220"
  },
  {
    "text": "x becomes t1 of x all\nthe way to tk of x.",
    "start": "2122220",
    "end": "2129994"
  },
  {
    "text": "And what I'm going to do\nis take the inner product-- so let's call this eta\nand let's call this t.",
    "start": "2129994",
    "end": "2135539"
  },
  {
    "text": "And I'm going to take the inner\nproduct of eta and t, which is just the sum from j equal\n1 to k of eta j of theta times",
    "start": "2135540",
    "end": "2149550"
  },
  {
    "text": "tj of x. OK, so that's just a way to say\nI want this simple interaction",
    "start": "2149550",
    "end": "2157690"
  },
  {
    "text": "but in higher dimension. The simplest way I can actually\nmake those things happen is just by taking inner product. ",
    "start": "2157690",
    "end": "2165490"
  },
  {
    "text": "OK, and so now what\nit's telling me is that the distribution-- so\nI want the exponential times something that depends only\non theta and something that",
    "start": "2165490",
    "end": "2171921"
  },
  {
    "text": "depends only on x. And so what it tells\nme is that when I'm going to take\np of theta x, it's just going to be something\nwhich is exponential",
    "start": "2171921",
    "end": "2179640"
  },
  {
    "text": "times the sum from j equal 1\nto k of eta j theta tj of x.",
    "start": "2179640",
    "end": "2190224"
  },
  {
    "text": "And then I'm going to have a\nfunction that depends only-- so let me read it for now\nlike c of theta and then",
    "start": "2190225",
    "end": "2196040"
  },
  {
    "text": "a function that\ndepends only on x. Let me call it h of x. And for convenience,\nthere's no particular reason",
    "start": "2196040",
    "end": "2202340"
  },
  {
    "text": "why I do that. I'm taking this\nfunction c of theta and I'm just actually\npushing it in there. So I can write c of theta as\nexponential minus log of 1",
    "start": "2202340",
    "end": "2217182"
  },
  {
    "text": "over c of theta, right?  And now I have exponential\ntimes exponential.",
    "start": "2217182",
    "end": "2223324"
  },
  {
    "text": "So I push it in, and\nthis thing actually looks like exponential sum\nfrom j equal 1 to k of eta",
    "start": "2223324",
    "end": "2230320"
  },
  {
    "text": "j theta tj of x minus log 1\nover c of theta times h of x.",
    "start": "2230320",
    "end": "2242120"
  },
  {
    "text": "And this thing here, log 1 over\nc of theta, I call actually b of theta Because\nc, I called it c.",
    "start": "2242120",
    "end": "2252060"
  },
  {
    "text": "But I can actually\ndirectly call this guy b, and I don't actually\ncare about c itself.",
    "start": "2252060",
    "end": "2258160"
  },
  {
    "text": "Now why don't I put back\nalso h of x in there?",
    "start": "2258160",
    "end": "2263900"
  },
  {
    "text": "Because h of x is\nreally here to just--",
    "start": "2263900",
    "end": "2268949"
  },
  {
    "text": "how to put it-- ",
    "start": "2268949",
    "end": "2274262"
  },
  {
    "text": "OK, h of x and b of theta\ndon't play the same role.",
    "start": "2274262",
    "end": "2280160"
  },
  {
    "text": "B of theta in many ways is a\nnormalizing constant, right? I want this density\nto integrate to 1.",
    "start": "2280160",
    "end": "2286820"
  },
  {
    "text": "If I did not have\nthis guy, I'm not guaranteed that this\nthing integrates to 1.",
    "start": "2286820",
    "end": "2291950"
  },
  {
    "text": "But by tweaking this function\nb of theta or c of theta-- they're equivalent-- I can actually ensure that\nthis thing integrates to 1.",
    "start": "2291950",
    "end": "2298350"
  },
  {
    "text": "So b of theta is just\na normalizing constant. H of x is something that's\ngoing to be funny for us.",
    "start": "2298350",
    "end": "2305000"
  },
  {
    "text": "It's going to be\nsomething that allows us to be able to treat both\ndiscrete and continuous variables within the framework\nof exponential families.",
    "start": "2305000",
    "end": "2318140"
  },
  {
    "text": "So for those that are\nfamiliar with this, this is essentially\nsaying that that h of x is really just a\nchange of measure.",
    "start": "2318140",
    "end": "2324119"
  },
  {
    "text": "When I actually look at\nthe density of p of theta-- this is with respect\nto some measure--",
    "start": "2324120",
    "end": "2330320"
  },
  {
    "text": "the fact that I just multiplied\nby a function of x just means that I'm not looking-- that this guy here\nwithout h of theta",
    "start": "2330320",
    "end": "2336420"
  },
  {
    "text": "is not the density with respect\nto the original measure, but it's the density with\nrespect to the distribution",
    "start": "2336420",
    "end": "2341660"
  },
  {
    "text": "that has h as a density. That's all I'm saying, right? So I can first transform my\nx's and then take the density",
    "start": "2341660",
    "end": "2348650"
  },
  {
    "text": "with respect to that. If you don't want to think\nabout densities or measures, you don't have to. This is just the way--",
    "start": "2348650",
    "end": "2354790"
  },
  {
    "text": "this is just the definition. Is there any question\nabout this definition? All right, so it\nlooks complicated,",
    "start": "2354790",
    "end": "2361290"
  },
  {
    "text": "but it's actually\nessentially the simplest way you could think about it. You want to be able to\nhave x and theta interact",
    "start": "2361290",
    "end": "2369004"
  },
  {
    "text": "and you just say, I\nwant the interaction to be of the form\nexponential x times theta.",
    "start": "2369004",
    "end": "2374126"
  },
  {
    "text": "And if they're\nhigher dimensions, I'm going to take\nthe exponential of the function\nof x inner product with a function of theta.",
    "start": "2374126",
    "end": "2379244"
  },
  {
    "text": " All right, so I claimed\nsince the beginning",
    "start": "2379244",
    "end": "2385540"
  },
  {
    "text": "that the Gaussian\nwas such an example. So let's just do it. So is the Gaussian of the-- is\nthe interaction between theta",
    "start": "2385540",
    "end": "2391330"
  },
  {
    "text": "and x in a Gaussian of\nthe form in the product? And the answer is yes.",
    "start": "2391330",
    "end": "2398680"
  },
  {
    "text": "Actually, whether I know or\nnot what the variance is, OK? So let's start for the case\nwhere I actually do not",
    "start": "2398680",
    "end": "2406747"
  },
  {
    "text": "know what the variance is. So here, I have x is\nn mu sigma squared.",
    "start": "2406747",
    "end": "2413500"
  },
  {
    "text": "This is all one dimensional. And here, I'm going to assume\nthat my parameter is both mu and sigma square.",
    "start": "2413500",
    "end": "2419440"
  },
  {
    "text": "OK, so what I need to do is\nto have some function of mu, some function of stigma square,\nand take an inner product",
    "start": "2419440",
    "end": "2424510"
  },
  {
    "text": "of some function of x and\nsome other function of x. So I want to show that-- so p theta of x is what?",
    "start": "2424510",
    "end": "2432350"
  },
  {
    "text": "Well, it's one over\nsquare root sigma 2 pi exponential minus x minus mu\nsquared over 2 sigma squared,",
    "start": "2432350",
    "end": "2442279"
  },
  {
    "text": "right? So that's just my\nGaussian density. And I want to say that\nthis thing here-- so",
    "start": "2442280",
    "end": "2449410"
  },
  {
    "text": "clearly, the exponential\nshows up already. I want to show that this\nis something that looks like, you know, eta 1 of--",
    "start": "2449410",
    "end": "2461619"
  },
  {
    "text": "sorry, so that was-- yeah, eta\n1 of, say, mu sigma squared.",
    "start": "2461620",
    "end": "2468395"
  },
  {
    "text": "So I have only\ntwo of those guys, so I'm going to need\nonly two etas, right? So I want it to be eta 1\nof mu and sigma times t1",
    "start": "2468395",
    "end": "2476030"
  },
  {
    "text": "of x plus eta 2 mu 1 mu sigma\nsquared times t2 of x, right?",
    "start": "2476030",
    "end": "2482940"
  },
  {
    "text": "So I want to have something\nlike that that shows up, and the only things\nthat are left, I want them to depend either\nonly on theta or only on x.",
    "start": "2482940",
    "end": "2492250"
  },
  {
    "text": "So to find that out,\nwe just need to expand.",
    "start": "2492250",
    "end": "2497500"
  },
  {
    "text": "OK, so I'm going to first put\neverything into my exponential and expand this guy.",
    "start": "2497500",
    "end": "2503650"
  },
  {
    "text": "So the first term here\nis going to be minus x squared over 2 sigma square. The second term is\ngoing to be minus mu",
    "start": "2503650",
    "end": "2509500"
  },
  {
    "text": "squared over two sigma squared. And then the cross term is\ngoing to be plus x mu divided",
    "start": "2509500",
    "end": "2515650"
  },
  {
    "text": "by sigma squared. And then I'm going\nto put this guy here. So I have a minus log\nsigma over 2 pi, OK?",
    "start": "2515650",
    "end": "2525036"
  },
  {
    "text": " OK, is this-- so this term\nhere contains an interaction",
    "start": "2525037",
    "end": "2533740"
  },
  {
    "text": "between X and the parameters. This term here\ncontains an interaction between X and the parameters. So let me try to write\nthem in a way that I want.",
    "start": "2533740",
    "end": "2541240"
  },
  {
    "text": "This guy only depends\non the parameters, this guy only depends\non the parameter. So I'm going to\nrearrange things.",
    "start": "2541240",
    "end": "2548390"
  },
  {
    "text": "And so I claim that this\nis of the form x squared.",
    "start": "2548390",
    "end": "2554079"
  },
  {
    "text": "Well, let's say-- do-- ",
    "start": "2554080",
    "end": "2563769"
  },
  {
    "text": "who's getting the minus? Eta, OK. So it's x squared times\nminus 1 over 2 sigma",
    "start": "2563770",
    "end": "2572960"
  },
  {
    "text": "squared plus x times mu\nover sigma squared, right?",
    "start": "2572960",
    "end": "2578450"
  },
  {
    "text": "So that's this term here. That's this term here. Now I need to get this guy\nhere, and that's minus.",
    "start": "2578450",
    "end": "2584129"
  },
  {
    "text": "So I'm going to write\nit like this-- minus, and now I have mu\nsquared over 2 sigma",
    "start": "2584129",
    "end": "2589950"
  },
  {
    "text": "squared plus log sigma\nsquare root 2 pi.",
    "start": "2589950",
    "end": "2595648"
  },
  {
    "start": "2595648",
    "end": "2602210"
  },
  {
    "text": "And now this thing is definitely\nof the form t of x times--",
    "start": "2602210",
    "end": "2611430"
  },
  {
    "text": "did I call them the\nright way or not? Of course not.",
    "start": "2611430",
    "end": "2616490"
  },
  {
    "text": "OK, so that's going to\nbe t2 of x times eta 2 of x eta 2 of theta.",
    "start": "2616490",
    "end": "2621820"
  },
  {
    "text": "This guy is going to be t1\nof x times eta 1 of theta.",
    "start": "2621820",
    "end": "2628230"
  },
  {
    "text": "All right, so just a function\nof theta times a function of x-- just a function of theta\ntimes a function of x. And the way combined is\njust by sending them.",
    "start": "2628230",
    "end": "2635680"
  },
  {
    "text": "And this is going\nto be my d of theta. ",
    "start": "2635680",
    "end": "2641710"
  },
  {
    "text": "What is h of x? AUDIENCE: 1. PHILIPPE RIGOLLET: 1.",
    "start": "2641710",
    "end": "2647020"
  },
  {
    "text": "There's one thing I\ncan actually play with, and this is something you're\ngoing to have some three",
    "start": "2647020",
    "end": "2653040"
  },
  {
    "text": "choices, right? This is not actually completely\ndetermined here is that--",
    "start": "2653040",
    "end": "2659850"
  },
  {
    "text": "for example, so when I write\nthe log sigma square root 2 pi,",
    "start": "2659850",
    "end": "2667220"
  },
  {
    "text": "this is just log of sigma\nplus log square root 2 pi.",
    "start": "2667220",
    "end": "2672660"
  },
  {
    "text": "So I have two choices here. Either my b becomes\nthis guy, or--",
    "start": "2672660",
    "end": "2677670"
  },
  {
    "text": "so either I have\nb of theta, which is mu squared over 2 sigma\nsquared plus log sigma",
    "start": "2677670",
    "end": "2685319"
  },
  {
    "text": "square root 2 pi and h of\nx is equal to 1, or I have",
    "start": "2685320",
    "end": "2691920"
  },
  {
    "text": "that b of theta is mu\nsquare over 2 sigma squared plus log sigma.",
    "start": "2691920",
    "end": "2698120"
  },
  {
    "text": "And h of x is equal to what? ",
    "start": "2698120",
    "end": "2708400"
  },
  {
    "text": "Well, I can just push\nthis guy out, right? I can push it out\nof the exponential. And so it's just square\nroot of 2 pi, which is",
    "start": "2708400",
    "end": "2715369"
  },
  {
    "text": "a function of x, technically. I mean, it's a constant function\nof x, but it's a function. So you can see that it's\nnot completely clear",
    "start": "2715370",
    "end": "2722420"
  },
  {
    "text": "how you're going to do\nthe trade off, right? So the constant terms can\ngo either in b or in h.",
    "start": "2722420",
    "end": "2728840"
  },
  {
    "text": "But you know, why bother with\ntracking down b and h when you can actually stuff\neverything into one",
    "start": "2728840",
    "end": "2735410"
  },
  {
    "text": "and just call h one\nand call it a day? Right, so you can\njust forget about h.",
    "start": "2735410",
    "end": "2740770"
  },
  {
    "text": "You know it's one and\nthink about the right. H won't matter actually for\nestimation purposes or anything",
    "start": "2740770",
    "end": "2746410"
  },
  {
    "text": "like this. All right, so that's basically\neverything that's written. When stigma square\nis known, what's",
    "start": "2746410",
    "end": "2755040"
  },
  {
    "text": "happening is that this\nguy here is no longer a function of theta, right?",
    "start": "2755040",
    "end": "2763640"
  },
  {
    "text": "Agreed? This is no longer a parameter. When sigma square is known,\nthen theta is equal to mu only.",
    "start": "2763640",
    "end": "2774990"
  },
  {
    "text": "There's no sigma\nsquare going on. So this-- everything\ndepends on sigma square can be thought of as a constant.",
    "start": "2774990",
    "end": "2780990"
  },
  {
    "text": "Think one. So in particular, this\nterm here does not",
    "start": "2780990",
    "end": "2786910"
  },
  {
    "text": "belong in the interaction\nbetween x and theta. It belongs to h, right?",
    "start": "2786910",
    "end": "2797150"
  },
  {
    "text": "So if sigma is known, then this\nguy is only a function of h--",
    "start": "2797150",
    "end": "2809119"
  },
  {
    "text": "of x. So h of x becomes exponential\nx squared minus x squared",
    "start": "2809120",
    "end": "2821420"
  },
  {
    "text": "over 2 sigma squared, right? That's just a function of x.",
    "start": "2821420",
    "end": "2826840"
  },
  {
    "text": " Is that clear? ",
    "start": "2826840",
    "end": "2836099"
  },
  {
    "text": "So if you complete this\ncomputation, what you're going to get is that your new\none parameter thing is that p",
    "start": "2836100",
    "end": "2848401"
  },
  {
    "text": "theta x is not equal to\nexponential x times mu",
    "start": "2848402",
    "end": "2855760"
  },
  {
    "text": "over sigma squared minus-- well, it's still the same thing. ",
    "start": "2855760",
    "end": "2869300"
  },
  {
    "text": "And then you have your\nh of x that comes out--  x squared over 2 sigma squared.",
    "start": "2869300",
    "end": "2878369"
  },
  {
    "text": "OK, so that's my h of x. That's still my b of theta.",
    "start": "2878370",
    "end": "2885960"
  },
  {
    "text": "And this is my t1 of x.",
    "start": "2885960",
    "end": "2891260"
  },
  {
    "text": "And this is my eta one of theta. And remember, theta is just\nequal to mu in this case.",
    "start": "2891260",
    "end": "2898060"
  },
  {
    "text": " So if I ask you prove that\nthis distribution belongs",
    "start": "2898060",
    "end": "2906609"
  },
  {
    "text": "to an exponential family,\nyou just have to work it out. Typically, it's expanding what's\nin the exponential and see",
    "start": "2906610",
    "end": "2912480"
  },
  {
    "text": "what's-- and just write it in\nthis term and identify all the components, right? So here, notice those guys\ndon't even get an index anymore",
    "start": "2912480",
    "end": "2919576"
  },
  {
    "text": "because there's\njust one of them. So I wrote eta 1 and t1, but\nit's really just eta and t.",
    "start": "2919576",
    "end": "2925629"
  },
  {
    "text": " Oh sorry, this guy also goes.",
    "start": "2925629",
    "end": "2934410"
  },
  {
    "text": "This is also a constant, right? So it can actually\njust put sigma divided",
    "start": "2934410",
    "end": "2941240"
  },
  {
    "text": "by sigma square root 2 pi. So h of x is what, actually? ",
    "start": "2941240",
    "end": "2948718"
  },
  {
    "text": "Is it the density of-- AUDIENCE: Standard [INAUDIBLE]. PHILIPPE RIGOLLET:\nIt's not standard.",
    "start": "2948718",
    "end": "2954349"
  },
  {
    "text": "It's centered. It has mean 0. But it variance\nsigma squared, right? But it's the density\nof a Gaussian.",
    "start": "2954350",
    "end": "2961060"
  },
  {
    "text": "And this is what I\nmeant when I said h of x is really just telling\nyou with respect to which",
    "start": "2961060",
    "end": "2967280"
  },
  {
    "text": "distribution, which measure\nyou're taking the density. And so this thing here\nis really telling you",
    "start": "2967280",
    "end": "2973309"
  },
  {
    "text": "the density of my\nGaussian with mean mu is equal to-- is this with\nrespect to a centered Gaussian",
    "start": "2973310",
    "end": "2981710"
  },
  {
    "text": "is this guy, right? That's what it means. If this thing ends\nup being a density, it just means that now you\njust have a new measure, which",
    "start": "2981710",
    "end": "2989370"
  },
  {
    "text": "is this density. So it's just saying\nthat the density of the Gaussian with\nmean mu with respect",
    "start": "2989370",
    "end": "2997560"
  },
  {
    "text": "to the Gaussian with mean 0\nis just this [INAUDIBLE] here. ",
    "start": "2997560",
    "end": "3005140"
  },
  {
    "text": "All right, so let's move on. So here, as I said,\nyou could actually",
    "start": "3005140",
    "end": "3011050"
  },
  {
    "text": "do all these computations\nand forget about the fact that x is continuous.",
    "start": "3011050",
    "end": "3016430"
  },
  {
    "text": "You can actually do it with PMFs\nand do it for x is discrete. This actually also tells\nyou if you can actually",
    "start": "3016430",
    "end": "3023540"
  },
  {
    "text": "get the same form for\nyour density, which is of the form exponential\ntimes the product",
    "start": "3023540",
    "end": "3029000"
  },
  {
    "text": "of the the interaction\nbetween theta and x is just\ntaking this product,",
    "start": "3029000",
    "end": "3034010"
  },
  {
    "text": "then a function only of theta\nand of function only of x, for the PMF, it also works.",
    "start": "3034010",
    "end": "3040130"
  },
  {
    "text": "OK, so I claim\nthat the Bernoulli belongs to this family. So the PMF of a Bernoulli--",
    "start": "3040130",
    "end": "3049380"
  },
  {
    "text": "we say parameter p is p to the\nx 1 minus p to the 1 minus x,",
    "start": "3049380",
    "end": "3054589"
  },
  {
    "text": "right? Because we know so that's\nonly for x equals 0 or 1.",
    "start": "3054590",
    "end": "3060440"
  },
  {
    "text": "And the reason is because\nwhen x is equal to 0, this is 1 minus p. When x is equal to\n1, this is minus 0.",
    "start": "3060440",
    "end": "3066627"
  },
  {
    "text": "OK, we've seen that\nwhen we're looking at likelihoods for Bernoullis.",
    "start": "3066627",
    "end": "3071730"
  },
  {
    "text": "OK, this is not clear this is\ngoing to look like this at all. But let's do it.",
    "start": "3071730",
    "end": "3079609"
  },
  {
    "text": "OK, so what does\nthis thing look like? Well, the first\nthing I want to do is to make an\nexponential show up.",
    "start": "3079610",
    "end": "3084710"
  },
  {
    "text": "So what I'm going\nto write is I'm going to write p to the x as\nexponential x log p, right?",
    "start": "3084710",
    "end": "3091190"
  },
  {
    "text": " And so I'm going to do\nthat for the other one. So this thing here--",
    "start": "3091190",
    "end": "3097690"
  },
  {
    "text": "so I'm going to get\nexponential x log",
    "start": "3097690",
    "end": "3103089"
  },
  {
    "text": "p plus 1 minus x log 1 minus p. ",
    "start": "3103090",
    "end": "3111250"
  },
  {
    "text": "So what I need to do is\nto collect my terms in x and my terms in whatever\nparameters I have,",
    "start": "3111250",
    "end": "3116750"
  },
  {
    "text": "see here if theta is equal to p. ",
    "start": "3116750",
    "end": "3123180"
  },
  {
    "text": "So if I do this,\nwhat I end up having is equal to exponential--",
    "start": "3123180",
    "end": "3128440"
  },
  {
    "text": "so determine x is log\np minus log 1 minus p. So that's x times\nlog p over 1 minus p.",
    "start": "3128440",
    "end": "3138140"
  },
  {
    "text": "And then the term\nthat rest is just-- that stays is just 1\ntimes log 1 minus p.",
    "start": "3138140",
    "end": "3143276"
  },
  {
    "text": "But I want to see this as\na minus something, right? It was minus b of theta. So I'm going to\nwrite it as minus--",
    "start": "3143276",
    "end": "3148525"
  },
  {
    "text": " well, I can just keep the\nplus, and I'm going to do--",
    "start": "3148525",
    "end": "3155150"
  },
  {
    "start": "3155150",
    "end": "3161770"
  },
  {
    "text": "and that's all [INAUDIBLE]. A-ha! Well, this is of the\nform exponential--",
    "start": "3161770",
    "end": "3168060"
  },
  {
    "text": "something that depends only on\nx times something that depends only on theta-- minus a function that\ndepends only on theta.",
    "start": "3168060",
    "end": "3176000"
  },
  {
    "text": "And then h of x is\nequal to 1 again. OK, so let's see. So I have t1 of x is equal to x.",
    "start": "3176000",
    "end": "3183410"
  },
  {
    "text": "That's this guy. Eta 1 of theta is equal\nto log p1 minus p.",
    "start": "3183410",
    "end": "3191000"
  },
  {
    "text": "And b of theta is equal to\nlog 1 over 1 minus p, OK?",
    "start": "3191000",
    "end": "3200930"
  },
  {
    "text": "And h of x is equal\nto 1, all right?",
    "start": "3200930",
    "end": "3206470"
  },
  {
    "text": " You guys want to do\nPoisson, or do you want to have any homework?",
    "start": "3206470",
    "end": "3212313"
  },
  {
    "text": " It's a dilemma because that's\nan easy homework versus",
    "start": "3212313",
    "end": "3217670"
  },
  {
    "text": "no homework at all but maybe\nsomething more difficult. OK, who wants to do it now?",
    "start": "3217670",
    "end": "3223680"
  },
  {
    "text": "Who does not want to\nraise their hand now? Who wants to raise\ntheir hand now? All right, so let's move on.",
    "start": "3223680",
    "end": "3237116"
  },
  {
    "text": "I'll just do-- do you want\nto do the gammas instead in the homework? That's going to be fun.",
    "start": "3237116",
    "end": "3242150"
  },
  {
    "text": "I'm not even going to\npropose to do the gammas. And so this is the\ngamma distribution.",
    "start": "3242150",
    "end": "3248570"
  },
  {
    "text": "It's brilliantly\ncalled gamma because it has the gamma function just\nlike the beta distribution had",
    "start": "3248570",
    "end": "3254480"
  },
  {
    "text": "the beta function in there. They look very similar. One is defined over r plus,\nthe positive real line.",
    "start": "3254480",
    "end": "3260960"
  },
  {
    "text": "And remember, the beta was\ndefined over the interval 0, 1. And it's of the form x to\nsome power times exponential",
    "start": "3260960",
    "end": "3268640"
  },
  {
    "text": "of minus x to some-- times something, right? So there's a function of\npolynomial [INAUDIBLE]",
    "start": "3268640",
    "end": "3274298"
  },
  {
    "text": "x where the exponent\ndepends on the parameter. And then there's the exponential\nminus x times something depends",
    "start": "3274298",
    "end": "3280670"
  },
  {
    "text": "on the parameters. So this is going to also look\nlike some function of x--",
    "start": "3280670",
    "end": "3287921"
  },
  {
    "text": "sorry, like some\nexponential distribution. Can somebody guess what\nis going to be t2 of x? ",
    "start": "3287921",
    "end": "3298338"
  },
  {
    "text": "Oh, those are the functions of\nx that show up in this product, right? Remember when we have this--",
    "start": "3298338",
    "end": "3303462"
  },
  {
    "text": "we just need to take\nsome transformations of x so it looks linear in those\nthings and not in x itself.",
    "start": "3303462",
    "end": "3308869"
  },
  {
    "text": "Remember, we had x squared\nand x, for example, in the Gaussian case. I don't know if\nit's still there.",
    "start": "3308870",
    "end": "3314471"
  },
  {
    "text": "Yeah, it's still there, right? t2 was x squared. What do you think x is\ngoing-- t2 of x here.",
    "start": "3314471",
    "end": "3320540"
  },
  {
    "text": "So here's a hint.\nt1 is going to be x. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:\nYeah, [INAUDIBLE],, what is going to be t1?",
    "start": "3320540",
    "end": "3326438"
  },
  {
    "text": "Yeah, you can--\nthis one is taken. This one is taken.  What?",
    "start": "3326438",
    "end": "3332580"
  },
  {
    "text": "Log x, right? Because this x to\nthe a minus 1, I'm going to write that as\nexponential a minus 1 log x.",
    "start": "3332580",
    "end": "3339380"
  },
  {
    "text": "So basically, eta 1 is\ngoing to be a minus 1. Eta 2 is going to\nbe minus 1 over b--",
    "start": "3339380",
    "end": "3347560"
  },
  {
    "text": "well, actually the opposite. And then you're going to have-- but this is actually\nnot too complicated.",
    "start": "3347560",
    "end": "3352630"
  },
  {
    "text": "All right, then those\nparameters get names. a is the shape parameter,\nb is the scale parameter.",
    "start": "3352630",
    "end": "3358480"
  },
  {
    "text": "It doesn't really matter. You have other things that\nare called the inverse gamma distribution, which\nhas this form.",
    "start": "3358480",
    "end": "3365849"
  },
  {
    "text": "The difference is that\nthe parameter alpha shows negatively there and\nthen the inverse Gaussian",
    "start": "3365850",
    "end": "3374700"
  },
  {
    "text": "distribution.  You know, just densities\nyou can come up with",
    "start": "3374700",
    "end": "3380220"
  },
  {
    "text": "and they just happened\nto fall in this family. And there's other ones that\nyou can actually put in there",
    "start": "3380220",
    "end": "3385680"
  },
  {
    "text": "that we've seen before. The chi-square is actually\npart of this family. The beta distribution\nis part of this family. The binomial distribution\nis part of this family.",
    "start": "3385680",
    "end": "3392611"
  },
  {
    "text": "Well, that's easy because\nthe Bernoulli was. The negative binomial, which\nis some stopping time--",
    "start": "3392611",
    "end": "3399390"
  },
  {
    "text": "the first time you hit a\ncertain number of successes when you flip some\nBernoulli coins.",
    "start": "3399390",
    "end": "3406120"
  },
  {
    "text": "So you can check\nfor all of those, and you will see that you can\nactually write them as part of the exponential family.",
    "start": "3406120",
    "end": "3411510"
  },
  {
    "text": "So the main goal\nof this slide is to convince you that\nthis is actually a pretty broad range\nof distributions because it basically includes\neverything we've seen",
    "start": "3411510",
    "end": "3420359"
  },
  {
    "text": "but not anything there-- sorry, plus more, OK?",
    "start": "3420360",
    "end": "3426540"
  },
  {
    "text": "Yeah. AUDIENCE: Is there any\nexample of a distribution that comes up\npretty often that's not in the exponential family?",
    "start": "3426540",
    "end": "3431801"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nYeah, like uniform. AUDIENCE: Oh, OK, so maybe\na bit more complicated than [INAUDIBLE].",
    "start": "3431801",
    "end": "3437702"
  },
  {
    "text": "Anything Anything that\nhas a support that depends on the parameter\nis not going to fall-- is not going to fit in there.",
    "start": "3437702",
    "end": "3444410"
  },
  {
    "text": "Right, and you can\nactually convince yourself why anything that\nhas the support that",
    "start": "3444410",
    "end": "3451910"
  },
  {
    "text": "does not-- that depends\non the parameter is not going to be\npart of this guy. It's kind of a hard thing to--",
    "start": "3451910",
    "end": "3457460"
  },
  {
    "text": "in fact, you proved that it's\nnot and you prove this rule. That's kind of a\nlittle difficult,",
    "start": "3457460",
    "end": "3463850"
  },
  {
    "text": "but the way you can convince\nyourself is that remember, the only interaction between\nx and theta that I allowed",
    "start": "3463850",
    "end": "3469910"
  },
  {
    "text": "was taking the\nproduct of those guys and then the exponential, right? If you have something that\ndepends on some parameter--",
    "start": "3469910",
    "end": "3476660"
  },
  {
    "text": "let's say you're going to see\nsomething that looks like this. Right, for uniform,\nit looks like this. ",
    "start": "3476660",
    "end": "3484720"
  },
  {
    "text": "Well, this is not of the form\nexponential x times theta. There's an interaction\nbetween x and theta here,",
    "start": "3484720",
    "end": "3490990"
  },
  {
    "text": "but it's actually\ncertainly not of the form x exponential x times theta. So this is definitely\nnot going to be",
    "start": "3490990",
    "end": "3496680"
  },
  {
    "text": "part of the exponential family. And every time you start\ndoing things like that, it's just not going to happen.",
    "start": "3496680",
    "end": "3501930"
  },
  {
    "text": " Actually, to be fair,\nI'm not even sure",
    "start": "3501930",
    "end": "3508370"
  },
  {
    "text": "that all these\nguys, when you allow them to have all\ntheir parameters free, are actually going\nto be part of this.",
    "start": "3508370",
    "end": "3514810"
  },
  {
    "text": "For example-- the\nbeta probably is, but I'm not actually\nentirely convinced. ",
    "start": "3514810",
    "end": "3523140"
  },
  {
    "text": "There's books on\nexperiential families. All right, so let's go back.",
    "start": "3523140",
    "end": "3528970"
  },
  {
    "text": "So here, we've put a lot\nof effort understanding how big, how much wider than\nthe Gaussian distribution",
    "start": "3528970",
    "end": "3537160"
  },
  {
    "text": "can we think of for the\nconditional distribution of our response y given x.",
    "start": "3537160",
    "end": "3544029"
  },
  {
    "text": "So let's go back to the\ngeneralized linear models, right? So [INAUDIBLE] said, OK,\nthe random component?",
    "start": "3544030",
    "end": "3549870"
  },
  {
    "text": "y has to be part of\nsome exponential family distribution-- check. We know what this means. So now I have to\nunderstand two things.",
    "start": "3549870",
    "end": "3556350"
  },
  {
    "text": "I have to understand what\nis the expectation, right? Because that's actually\nwhat I model, right?",
    "start": "3556350",
    "end": "3561960"
  },
  {
    "text": "I take the expectation, the\nconditional expectation, of y given x. So I need to understand\ngiven this guy,",
    "start": "3561960",
    "end": "3567100"
  },
  {
    "text": "it would be nice if you had some\nsimple rules that would tell me exactly what the expectation\nis rather than having to do it",
    "start": "3567100",
    "end": "3572950"
  },
  {
    "text": "over and over again, right? If I told you,\nhere's a Gaussian, compute the\nexpectation, every time you had to use that would\nbe slightly painful.",
    "start": "3572950",
    "end": "3580750"
  },
  {
    "text": "So hopefully, this thing\nbeing simple enough-- we've actually\nselected a class that's",
    "start": "3580750",
    "end": "3585870"
  },
  {
    "text": "simple enough so that\nwe can have rules. Whereas as soon as they give you\nthose parameters t1, t2, eta 1,",
    "start": "3585870",
    "end": "3592360"
  },
  {
    "text": "eta 2, b and h, you can\nactually have some simple rules to compute the mean and\nvariance and all those things.",
    "start": "3592360",
    "end": "3600370"
  },
  {
    "text": "And so in particular, I'm\ninterested in the mean, and I'm going to have to\nactually say, well, you know,",
    "start": "3600370",
    "end": "3605950"
  },
  {
    "text": "this mean has to be mapped\ninto the whole real line. So I can actually talk\nabout modeling this function",
    "start": "3605950",
    "end": "3612040"
  },
  {
    "text": "of the mean as x transpose beta. And we saw that for\nthe [INAUDIBLE] dataset",
    "start": "3612040",
    "end": "3617380"
  },
  {
    "text": "or whatever other data sets. You actually can-- you can\nactually do this using the log",
    "start": "3617380",
    "end": "3624250"
  },
  {
    "text": "of the reciprocal or for the-- oh, actually, we didn't\ndo it for the Bernoulli.",
    "start": "3624250",
    "end": "3630050"
  },
  {
    "text": "We'll come to this. This is the most important\none, and that's called a logit it or a logistic link. ",
    "start": "3630050",
    "end": "3637090"
  },
  {
    "text": "But before we go there,\nthis was actually a very broad family, right?",
    "start": "3637090",
    "end": "3642320"
  },
  {
    "text": "When I wrote this thing on the\nbottom board-- it's gone now, but when I wrote it\nin the first place, the only thing that I wrote\nis I wanted x times theta.",
    "start": "3642320",
    "end": "3648870"
  },
  {
    "text": "Wouldn't it be nice if you\nhave some distribution that was just x times theta,\nnot some function of x times some function of theta?",
    "start": "3648870",
    "end": "3654660"
  },
  {
    "text": "The functions seem to be\nhere so that they actually make things a little--",
    "start": "3654660",
    "end": "3662610"
  },
  {
    "text": "so the functions were here\nso that I can actually put a lot of functions there. But first of all,\nif I actually decide",
    "start": "3662610",
    "end": "3668430"
  },
  {
    "text": "to re-parametrize my\nproblem, I can always assume-- if I'm\none dimensional, I can always assume\nthat eta 1 of theta",
    "start": "3668430",
    "end": "3674970"
  },
  {
    "text": "becomes my new theta, right? So this thing--\nhere for example,",
    "start": "3674970",
    "end": "3680772"
  },
  {
    "text": "I could say, well,\nthis is actually the parameter of my Bernoulli. Let me call this\nguy theta, right?",
    "start": "3680772",
    "end": "3685950"
  },
  {
    "text": "I could do that. Then I could say, well, here\nI have x that shows up here.",
    "start": "3685950",
    "end": "3691230"
  },
  {
    "text": "And here since I'm talking\nabout the response, I cannot really make\nany transformations. So here, I'm going to actually\ntalk about a specific family",
    "start": "3691230",
    "end": "3698240"
  },
  {
    "text": "for which this guy is not x\nsquare or square root of x or log of x or anything I want.",
    "start": "3698240",
    "end": "3703349"
  },
  {
    "text": "I'm just going to actually\nlook at distributions for which this is x. This exponential\nfamilies are called",
    "start": "3703350",
    "end": "3708484"
  },
  {
    "text": "a canonical exponential family. So in the canonical\nexponential family, what I have",
    "start": "3708485",
    "end": "3715010"
  },
  {
    "text": "is that I have my x times theta. I'm going to allow myself\nsome normalization factor phi, and we'll see, for\nexample, that it's",
    "start": "3715010",
    "end": "3721500"
  },
  {
    "text": "very convenient when I talk\nabout the Gaussian, right? Because even if I know--",
    "start": "3721500",
    "end": "3727830"
  },
  {
    "text": " yeah, even if I know this guy,\nwhich I actually pull into my--",
    "start": "3727830",
    "end": "3735134"
  },
  {
    "text": "oh, that's over here, right? ",
    "start": "3735134",
    "end": "3740970"
  },
  {
    "text": "Right, I know sigma squared. But I don't want to\nchange my parameter to be mu over sigma squared.",
    "start": "3740970",
    "end": "3746290"
  },
  {
    "text": "It's kind of painful. So I just take mu, and\nI'm going to keep this guy as being this phi over there.",
    "start": "3746290",
    "end": "3751980"
  },
  {
    "text": "And it's called the\ndispersion parameter from a clear analogy\nwith the Gaussian, right?",
    "start": "3751980",
    "end": "3758010"
  },
  {
    "text": "That's the variance and\nthat's measuring dispersion. OK, so here, what\nI want is I'm going",
    "start": "3758010",
    "end": "3765540"
  },
  {
    "text": "to think throughout this class--\nso phi may be known or not. And depending--\nwhen it's not known,",
    "start": "3765540",
    "end": "3771390"
  },
  {
    "text": "this actually might turn\ninto some exponential family or it might not. And the main reason is because\nthis b of theta over phi",
    "start": "3771390",
    "end": "3781380"
  },
  {
    "text": "is not necessarily a function\nof theta over phi, right? If I actually have phi\nunknown, then y theta over phi",
    "start": "3781380",
    "end": "3789660"
  },
  {
    "text": "has to be-- this guy has to be\nmy new parameter. And b might not be a function\nof this new parameter.",
    "start": "3789660",
    "end": "3797930"
  },
  {
    "text": "OK, so in a way,\nit may or may not, but this is not really a\nconcern that we're going to have",
    "start": "3797930",
    "end": "3804710"
  },
  {
    "text": "because throughout\nthis class, we're going to assume that\nphi is known, OK? Phi is going to be known all the\ntime, which means that this is",
    "start": "3804710",
    "end": "3811820"
  },
  {
    "text": "always an exponential family. And it's just the\nsimplest one you could think of-- one\ndimensional parameter, one",
    "start": "3811820",
    "end": "3818270"
  },
  {
    "text": "dimensional response, and I just\nhave-- the product is just y times or, we used to call it x.",
    "start": "3818270",
    "end": "3825050"
  },
  {
    "text": "Now I've switched to y, but y\ntimes theta divided by phi, OK? ",
    "start": "3825050",
    "end": "3832549"
  },
  {
    "text": "Should I write this or this is\nclear to everyone what this is? Let me write it somewhere so\nwe actually keep track of it",
    "start": "3832550",
    "end": "3838665"
  },
  {
    "text": "toward the [INAUDIBLE]. ",
    "start": "3838665",
    "end": "3845800"
  },
  {
    "text": "OK, so this is-- remember, we had all\nthe distributions.",
    "start": "3845800",
    "end": "3851619"
  },
  {
    "text": "And then here we had\nthe exponential family. And now we have the\ncanonical exponential family.",
    "start": "3851620",
    "end": "3858609"
  },
  {
    "text": " It's actually\nmuch, much smaller.",
    "start": "3858610",
    "end": "3864200"
  },
  {
    "text": "Well, actually, it's probably\nsort of a good picture. And what I have is that\nmy density or my PMF",
    "start": "3864200",
    "end": "3872619"
  },
  {
    "text": "is just exponential\ny times theta minus b of theta divided by phi.",
    "start": "3872620",
    "end": "3881020"
  },
  {
    "text": "And I have plus phi of--",
    "start": "3881020",
    "end": "3886480"
  },
  {
    "text": "oh, yeah, plus phi\nof y phi, which",
    "start": "3886480",
    "end": "3893820"
  },
  {
    "text": "means that this is really--\nif phi is known, h of y is just exponential\nc of y phi, agreed?",
    "start": "3893820",
    "end": "3905742"
  },
  {
    "text": "Actually, this is the reason\nwhy it's not necessarily a canonical family. It might not be that\nthis depends only on y.",
    "start": "3905742",
    "end": "3912990"
  },
  {
    "text": "It could depend on y and\nphi in some annoying way and I may not be\nable to break it.",
    "start": "3912990",
    "end": "3918950"
  },
  {
    "text": "OK, but if phi is known,\nthis is just a function that depends on y, agreed? ",
    "start": "3918950",
    "end": "3928290"
  },
  {
    "text": "In particular, I\nthink you need-- I hope you can convince\nyourself that this is just a subcase of everything\nwe've seen before.",
    "start": "3928290",
    "end": "3933750"
  },
  {
    "start": "3933750",
    "end": "3941990"
  },
  {
    "text": "So for example, the Gaussian\nwhen the variance is known is indeed of this form, right?",
    "start": "3941990",
    "end": "3947010"
  },
  {
    "text": "So we still have\nit on the board. So here is my y, right? So then let me write\nthis as f theta of y.",
    "start": "3947010",
    "end": "3953950"
  },
  {
    "text": "So every x is replaceable\nwith y, blah, blah, blah.",
    "start": "3953950",
    "end": "3959030"
  },
  {
    "text": "This is this guy. And now what I have is that\nthis is going to be my phi.",
    "start": "3959030",
    "end": "3967120"
  },
  {
    "text": "This is my parameter of theta. So I'm definitely of the form\ny times theta divided by phi.",
    "start": "3967120",
    "end": "3974319"
  },
  {
    "text": "And then here I\nhave a function b that depends only on\ntheta over phi again.",
    "start": "3974320",
    "end": "3980890"
  },
  {
    "text": "So b of theta is mu\nsquared divided by 2.",
    "start": "3980890",
    "end": "3987039"
  },
  {
    "text": " OK, then it's divided\nby 6 sigma square.",
    "start": "3987040",
    "end": "3993890"
  },
  {
    "text": "And then I have\nthis extra stuff. But I really don't care\nwhat it is for now. It's just something that depends\nonly on y and known stuff.",
    "start": "3993890",
    "end": "4002140"
  },
  {
    "text": "So it was just a function\nof y just like my h. I stuff everything in there.",
    "start": "4002140",
    "end": "4007180"
  },
  {
    "text": "The b, though, this\nthing here, this is actually what's\nimportant because",
    "start": "4007180",
    "end": "4012229"
  },
  {
    "text": "in the canonical\nfamily, if you think about it, when you know phi-- sorry-- right, this\nis just y times theta",
    "start": "4012229",
    "end": "4023270"
  },
  {
    "text": "scaled by a known\nconstant-- sorry, y times theta scaled by a known\nconstant is the first term. The second term is b of theta\nscaled by some known constant.",
    "start": "4023270",
    "end": "4032000"
  },
  {
    "text": "But b of theta is\nwhat's going to make the difference between the\nGaussian and Bernoullis",
    "start": "4032000",
    "end": "4037580"
  },
  {
    "text": "and gammas and betas-- this is all in this b\nof theta. b of theta contains everything\nthat's idiosyncratic to",
    "start": "4037580",
    "end": "4045050"
  },
  {
    "text": "this particular distribution. And so this is going\nto be important. And we will see that b of theta\nis going to capture information",
    "start": "4045050",
    "end": "4052120"
  },
  {
    "text": "about the mean,\nabout the variance, about likelihood,\nabout everything.",
    "start": "4052120",
    "end": "4057133"
  },
  {
    "start": "4057133",
    "end": "4064710"
  },
  {
    "text": "Should I go through\nthis computation? I mean, it's the same. We've just done it, right? So maybe it's probably better\nif you can redo it on your own.",
    "start": "4064710",
    "end": "4073750"
  },
  {
    "text": "All right, so the canonical\nexponential family also has other distributions, right? So there's the Gaussian\nand there's the Poisson",
    "start": "4073750",
    "end": "4080890"
  },
  {
    "text": "and there's the Bernoulli. But the other ones may not\nbe part of this, right? In particular, think about\nthe gamma distribution.",
    "start": "4080890",
    "end": "4087810"
  },
  {
    "text": "We had this-- log x was one\nof the things that showed up.",
    "start": "4087810",
    "end": "4093600"
  },
  {
    "text": "I mean, I cannot get\nrid of this log x. I mean, that's part of it\nexcept if a is equal to 1",
    "start": "4093600",
    "end": "4098729"
  },
  {
    "text": "and I know it for sure, right? So if a is equal to 1, then\nI'm going to have a minus 1,",
    "start": "4098729",
    "end": "4103979"
  },
  {
    "text": "which is equal to 0. So I'm going to have\na minus 1 times log x, which is going to be just 0. So log x is going\nto vanish from here.",
    "start": "4103979",
    "end": "4110560"
  },
  {
    "text": "But if a is equal to 1,\nthen this distribution is actually much nicer, and\nit actually does not even",
    "start": "4110560",
    "end": "4116250"
  },
  {
    "text": "deserve the name gamma. What is it if a is equal to 1? ",
    "start": "4116250",
    "end": "4122443"
  },
  {
    "text": "It's an exponential, right? Gamma 1 is equal to 1. x to\nthe a minus 1 is equal to 1.",
    "start": "4122444",
    "end": "4127778"
  },
  {
    "text": "b-- so I have exponential\nx over b divided by b. So 1 over b-- call it lambda.",
    "start": "4127779",
    "end": "4133520"
  },
  {
    "text": "And this is just an\nexponential distribution. And so every time you're\ngoing to see something--",
    "start": "4133520",
    "end": "4138799"
  },
  {
    "text": "so all these guys that\ndon't make it to this table, they could be part of those\nguys, but they're just more--",
    "start": "4138800",
    "end": "4146093"
  },
  {
    "text": "they're just to-- they just have another\nname in this thing. All right, so you could\ncompute the value of theta",
    "start": "4146094",
    "end": "4153969"
  },
  {
    "text": "for different values, right? So again, you still have some\ncontinuous or discrete ones. This is my b of theta.",
    "start": "4153970",
    "end": "4159630"
  },
  {
    "text": "And I said this is actually\nreally what captures my theta. This b is actually called\ncumulant generating function,",
    "start": "4159630",
    "end": "4166450"
  },
  {
    "text": "OK? I don't have time. I could write five\nslides to explain to you, but it would just only\ntell you why it's called",
    "start": "4166450",
    "end": "4172729"
  },
  {
    "text": "cumulant generating function. It's also known as the log of\nthe moment generating function.",
    "start": "4172729",
    "end": "4178089"
  },
  {
    "text": "And the way it's called\ncumulant generating function is because if I start taking\nsuccessive derivatives",
    "start": "4178090",
    "end": "4184319"
  },
  {
    "text": "and evaluating them at 0, I\nget the successive cumulance of this distribution, which\nare some transformation",
    "start": "4184320",
    "end": "4190859"
  },
  {
    "text": "of the moments. AUDIENCE: What are you\ntalking about again? PHILIPPE RIGOLLET:\nThe function b. AUDIENCE: [INAUDIBLE]",
    "start": "4190859",
    "end": "4195945"
  },
  {
    "text": "PHILIPPE RIGOLLET: So this\nis just normalization. So this is just to tell\nyou I can compute this, but I really don't care.",
    "start": "4195945",
    "end": "4201640"
  },
  {
    "text": "And obviously I don't care\nabout stuff that's complicated. This is actually cute, and this\nis what completes everything.",
    "start": "4201640",
    "end": "4207316"
  },
  {
    "text": "And the rest is just like\nsome general description. You only need to tell\nyou that the range of y is 0 to infinity, right?",
    "start": "4207316",
    "end": "4214090"
  },
  {
    "text": "And that is\nessentially telling me this is going to give me some\nhints as to which link function I should be using, right?",
    "start": "4214090",
    "end": "4220179"
  },
  {
    "text": "Because the range\nof y tells me what the range of expectation\nof y is going to be. All right, so here, it\ntells me that the range of y",
    "start": "4220180",
    "end": "4225970"
  },
  {
    "text": "is between 0 and 1. OK, so what I want\nto show you is that this captures a\nvariety of different ranges",
    "start": "4225970",
    "end": "4233134"
  },
  {
    "text": "that you can have. ",
    "start": "4233134",
    "end": "4240300"
  },
  {
    "text": "OK, so I'm going to want\nto go into the likelihood.",
    "start": "4240300",
    "end": "4246570"
  },
  {
    "text": "And the likelihood\nI'm actually going to use to compute\nthe expectations. But since I actually\ndon't have time",
    "start": "4246570",
    "end": "4252840"
  },
  {
    "text": "to do this now, let's just\ngo quickly through this and give you spoiler alert to\nmake sure that you all wake up",
    "start": "4252840",
    "end": "4259770"
  },
  {
    "text": "on Thursday and\nreally, really want to think about coming\nhere immediately. All right, so the thing\nI'm going to want to do,",
    "start": "4259770",
    "end": "4265470"
  },
  {
    "text": "as I said, is it would\nbe nice if, at least for this canonical\nfamily, when I give you b,",
    "start": "4265470",
    "end": "4271433"
  },
  {
    "text": "you would be able\nto say, oh, here is a simple computation of b\nthat would actually give me the mean and the variance.",
    "start": "4271434",
    "end": "4277530"
  },
  {
    "text": "The mean and the variance\nare also known as moments. b is called cumulant\ngenerating function.",
    "start": "4277530",
    "end": "4282969"
  },
  {
    "text": "So it sounds like\nmoments being related to cumulance, I might have a\npath to finding those, right?",
    "start": "4282970",
    "end": "4288060"
  },
  {
    "text": "And it might involve taking\nderivatives of b, as we'll see. The way we're\ngoing to prove this",
    "start": "4288060",
    "end": "4293330"
  },
  {
    "text": "by using this thing that\nwe've used several times. So this property we use\nwhen we're computing,",
    "start": "4293330",
    "end": "4299354"
  },
  {
    "text": "remember, the fisher\ninformation, right? We had two formulas for\nthe fisher information. One was the expectation of the\nsecond derivative of the log",
    "start": "4299354",
    "end": "4309210"
  },
  {
    "text": "likelihood, and one was negative\nexpectation of the square-- sorry, expectation of the\nsquare, and the other one",
    "start": "4309210",
    "end": "4315150"
  },
  {
    "text": "was negative the expectation of\nthe second derivative, right? The log likelihood is concave,\nso this number is negative,",
    "start": "4315150",
    "end": "4320850"
  },
  {
    "text": "this number is positive. And the way we did this is by\njust permuting some derivative and integral here.",
    "start": "4320850",
    "end": "4326004"
  },
  {
    "text": "And there was just-- we\nused the fact that something that looked like this, right? The log likelihood\nis log of f theta.",
    "start": "4326004",
    "end": "4333780"
  },
  {
    "text": "And when I take the derivative\nof this guy with respect",
    "start": "4333780",
    "end": "4340500"
  },
  {
    "text": "to theta, then I\nhave something that looks like the derivative\ndivided by f theta.",
    "start": "4340500",
    "end": "4350460"
  },
  {
    "text": "And if I start taking the\nintegral against f theta of this thing, so the\nexpectation of this thing,",
    "start": "4350460",
    "end": "4359270"
  },
  {
    "text": "those things would cancel. And then I had just the\nintegral of a derivative, which",
    "start": "4359270",
    "end": "4365739"
  },
  {
    "text": "I would make a leap of faith\nand say that it's actually the derivative of the integral. ",
    "start": "4365739",
    "end": "4373770"
  },
  {
    "text": "But this was equal to 1. So this derivative was\nactually equal to 0. And so that's how you\ngot that the expectation",
    "start": "4373770",
    "end": "4380320"
  },
  {
    "text": "of the derivative of the log\nlikelihood is equal to 0. And you do it once again\nand you get this guy. It's just some nice\nthings that happen",
    "start": "4380320",
    "end": "4386350"
  },
  {
    "text": "with the [INAUDIBLE] taking\nderivative of the log. We've done that,\nwe'll do that again. But once you do this, you\ncan actually apply it.",
    "start": "4386350",
    "end": "4393660"
  },
  {
    "text": "And-- missing a\nparenthesis over there. So when you write\nthe log likelihood,",
    "start": "4393660",
    "end": "4399610"
  },
  {
    "text": "it's just log of an exponential. Huh, that's actually\npretty nice. Just like the least squares\ncame naturally, the least",
    "start": "4399610",
    "end": "4405020"
  },
  {
    "text": "squares [INAUDIBLE]\ncame naturally when we took the log\nlikelihood of the Gaussians, we're going to have the\nsame thing that happens when",
    "start": "4405020",
    "end": "4411370"
  },
  {
    "text": "I take the log of the density. The exponential is\ngoing to go away, and then I'm going\nto use this formula.",
    "start": "4411370",
    "end": "4416989"
  },
  {
    "text": "But this formula is\ngoing to actually give me an equation directly--\noh, that's where it was.",
    "start": "4416990",
    "end": "4423026"
  },
  {
    "text": "So that's the one\nthat's missing up there. And so the expectation\nminus this thing",
    "start": "4423026",
    "end": "4429010"
  },
  {
    "text": "is going to be equal\nto 0, which tells me that the expectation\nis just the derivative. Right, so it's still\na function of theta,",
    "start": "4429010",
    "end": "4435190"
  },
  {
    "text": "but it's just a derivative of b. And the variance\nis just going to be the second derivative of b.",
    "start": "4435190",
    "end": "4441280"
  },
  {
    "text": "But remember, this was some\nsort of a scaling, right? It's called the\ndispersion parameter. So if I had a Gaussian and\nthe variance of the Gaussian",
    "start": "4441280",
    "end": "4449410"
  },
  {
    "text": "did not depend on\nthe sigma squared which I stuffed in this phi,\nthat would be certainly weird.",
    "start": "4449410",
    "end": "4455260"
  },
  {
    "text": "And it cannot depend only\non mu, and so this will-- for the Gaussian, this is\ndefinitely going to be equal to 1.",
    "start": "4455260",
    "end": "4460960"
  },
  {
    "text": "And this is just going to\nbe equal to my variance. So this is just by taking\nthe second derivative.",
    "start": "4460960",
    "end": "4468460"
  },
  {
    "text": "So basically, the take-home\nmessage is that this function b captures--",
    "start": "4468460",
    "end": "4475170"
  },
  {
    "text": "by taking one derivative\nof the expectation and by taking two derivatives\ncaptures the variance. Another thing\nthat's actually cool",
    "start": "4475170",
    "end": "4481199"
  },
  {
    "text": "and we'll come\nback to this and I want to think about is if\nthis second derivative is the variance, what can\nI say about this thing?",
    "start": "4481200",
    "end": "4489190"
  },
  {
    "text": " What do I know about a variance? AUDIENCE: [INAUDIBLE]",
    "start": "4489190",
    "end": "4494950"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nYeah, that's positive. So I know that this is positive. So what does that tell me?",
    "start": "4494950",
    "end": "4500600"
  },
  {
    "text": "Positive? That's convex, right? A function that has positive\nsecond derivative is convex.",
    "start": "4500600",
    "end": "4507050"
  },
  {
    "text": "So we're going to use\nthat as well, all right? So yeah, I'll see\nyou on Thursday.",
    "start": "4507050",
    "end": "4512530"
  },
  {
    "text": "I have your homework.",
    "start": "4512530",
    "end": "4514380"
  }
]