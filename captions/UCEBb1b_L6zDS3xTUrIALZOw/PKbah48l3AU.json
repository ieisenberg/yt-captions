[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5300"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high-quality\neducational resources for free.",
    "start": "5300",
    "end": "11600"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "11600",
    "end": "18100"
  },
  {
    "text": "at ocw.mit.edu. ",
    "start": "18100",
    "end": "24145"
  },
  {
    "text": "JAMES SWAN: OK. Well, everyone's\nquieted down, so that means we have to get started. So let me say something here.",
    "start": "24145",
    "end": "31350"
  },
  {
    "text": "This will be our\nlast conversation about optimization.",
    "start": "31350",
    "end": "36359"
  },
  {
    "text": "So we've discussed\nunconstrained optimization. And now we're going to discuss\na slightly more complicated",
    "start": "36360",
    "end": "42149"
  },
  {
    "text": "problem-- but\nyou're going to see it's really not that\nmuch more complicated-- constrained optimization.",
    "start": "42150",
    "end": "47170"
  },
  {
    "text": " These are the things\nwe discussed before. I don't want to spend\nmuch time recapping",
    "start": "47170",
    "end": "53520"
  },
  {
    "text": "because I want to take a minute\nand talk about the midterm exam. So we have a quiz.",
    "start": "53520",
    "end": "59250"
  },
  {
    "text": "It's next Wednesday. Here's where it's\ngoing to be located. Here's 66. Head down Ames.",
    "start": "59250",
    "end": "64949"
  },
  {
    "text": "You're looking for Walker\nMemorial on the third floor. Unfortunately, the time for\nthe quiz is 7:00 to 9:00 PM.",
    "start": "64950",
    "end": "72030"
  },
  {
    "text": "We really did try hard\nto get the scheduling office to give us\nsomething better, but the only way to get a room\nthat would fit everybody in",
    "start": "72030",
    "end": "80100"
  },
  {
    "text": "was to do it at\nthis time in Walker. I really don't understand,\nbecause I actually requested locations for\nthe quizzes back in April.",
    "start": "80100",
    "end": "87480"
  },
  {
    "text": "And somehow I was\ntoo early, maybe, and got buried under a pile.",
    "start": "87480",
    "end": "93992"
  },
  {
    "text": "Maybe not important\nenough, I don't know. But it's got to be from\nseven to nine next Wednesday.",
    "start": "93992",
    "end": "100146"
  },
  {
    "text": "Third floor. There's not going\nto be any class next Wednesday because\nyou have a quiz instead. So you get a little extra time\nto relax or study, prepare,",
    "start": "100146",
    "end": "109500"
  },
  {
    "text": "calm yourself before\nyou go into the exam. There's no homework this week. So you can just use this time\nto focus on the material we've",
    "start": "109500",
    "end": "118950"
  },
  {
    "text": "discussed. There's a practice\nexam from last year posted on the Steller site,\nwhich you can utilize and study",
    "start": "118950",
    "end": "127950"
  },
  {
    "text": "from. I'll tell you this. ",
    "start": "127950",
    "end": "133506"
  },
  {
    "text": "That practice exam is\nskewed a little more towards some chemical\nengineering problems",
    "start": "133506",
    "end": "139410"
  },
  {
    "text": "that motivate the numerics. I've found in the past that\nwhen problems like that",
    "start": "139410",
    "end": "145050"
  },
  {
    "text": "are given on the exam,\nsometimes there's a lot of reading that goes into\nunderstanding the engineering",
    "start": "145050",
    "end": "150580"
  },
  {
    "text": "problem. And that tends to set\nback the problem-solving. So I'll tell you that the quiz\nthat you'll take on Wednesday",
    "start": "150580",
    "end": "157110"
  },
  {
    "text": "will have less of the\nengineering associated with it, and focus more on the numerical\nor computational science.",
    "start": "157110",
    "end": "166080"
  },
  {
    "text": "The underlying\nsorts of questions, the way the questions are\nasked, the kinds of responses",
    "start": "166080",
    "end": "171450"
  },
  {
    "text": "you're expected to give\nI'd say are very similar. But we've tried to tune\nthe exam so that it'll",
    "start": "171450",
    "end": "177450"
  },
  {
    "text": "be less of a burden\nto understand the structure of the\nproblem before describing how you'd solve it. So I think that's good.",
    "start": "177450",
    "end": "183920"
  },
  {
    "text": "It's comprehensive up to today. So linear algebra, systems\nof nonlinear equations",
    "start": "183920",
    "end": "190860"
  },
  {
    "text": "and optimization\nare the quiz topics. We're going to switch on\nFriday to ordinary differential",
    "start": "190860",
    "end": "198660"
  },
  {
    "text": "equations and initial\nvalue problems. So you have two\nlectures on that, but you won't have\ndone any homework.",
    "start": "198660",
    "end": "204540"
  },
  {
    "text": "You probably don't know enough\nor aren't practiced enough to answer any questions\nintelligently on the quiz.",
    "start": "204540",
    "end": "210796"
  },
  {
    "text": "So don't expect that\nmaterial to be on there. It's not. It's going to be\nthese three topics.",
    "start": "210796",
    "end": "216672"
  },
  {
    "text": "Are there any questions\nabout this that I can answer? ",
    "start": "216672",
    "end": "222864"
  },
  {
    "text": "Kristin has a question. AUDIENCE: [INAUDIBLE]. ",
    "start": "222864",
    "end": "236535"
  },
  {
    "text": "JAMES SWAN: OK. So yeah, come prepared. It might be cold. It might be hot.",
    "start": "236535",
    "end": "242440"
  },
  {
    "text": "It leaks when it\nrains a little bit. Yeah, it's not\nthe greatest spot.",
    "start": "242440",
    "end": "247480"
  },
  {
    "text": "So come prepared. That's true. Other questions? Things you want to know?",
    "start": "247480",
    "end": "253292"
  },
  {
    "text": "AUDIENCE: What can\nwe take to the exam? JAMES SWAN: Ooh, good question. So you can bring the book\nrecommended for the course.",
    "start": "253292",
    "end": "260058"
  },
  {
    "text": "You can bring your notes. You can bring a calculator. You need to bring some pencils. We'll provide blue books for\nyou to write your solutions",
    "start": "260059",
    "end": "268210"
  },
  {
    "text": "to the exam in. So those are the materials. Good. What else?",
    "start": "268210",
    "end": "273370"
  },
  {
    "text": "Same question. OK. Other questions? ",
    "start": "273370",
    "end": "280856"
  },
  {
    "text": "No? OK.  So then let's jump into\nthe topic of the day, which",
    "start": "280856",
    "end": "287880"
  },
  {
    "text": "is constrained optimization. So these are\nproblems of the sort. Minimize an objective function\nf of x subject to the constraint",
    "start": "287880",
    "end": "297870"
  },
  {
    "text": "that x belongs to some set\nD, or find the argument x that minimizes this function.",
    "start": "297870",
    "end": "304007"
  },
  {
    "text": "These are equivalent\nsorts of problem. Sometimes, we want to know\none or the other or both. That's not a problem.",
    "start": "304007",
    "end": "310790"
  },
  {
    "text": "And graphically,\nit looks like this. Here's f, our\nobjective function. It's a nice convex\nbowl-shaped function here.",
    "start": "310790",
    "end": "316470"
  },
  {
    "text": "And we want to know the\nvalues of x1 and x2, let's say, that\nminimize this function",
    "start": "316470",
    "end": "321870"
  },
  {
    "text": "subject to some constraint. That constraint could\nbe that x1 and x2 live",
    "start": "321870",
    "end": "327900"
  },
  {
    "text": "inside this little blue circle. It could be D. It could\nbe that x1 and x2 live on the surface of\nthis circle, right,",
    "start": "327900",
    "end": "334680"
  },
  {
    "text": "on the circumference\nof this circle. That could be the constraint.",
    "start": "334680",
    "end": "340670"
  },
  {
    "text": "So these are the sorts of\nproblems we want to solve. D is called the\nfeasible set, and can",
    "start": "340670",
    "end": "345980"
  },
  {
    "text": "be described in terms of really\ntwo types of constraints. One is what we call\nequality constraints.",
    "start": "345980",
    "end": "351199"
  },
  {
    "text": "So D can be the set\nof values x such that some nonlinear function\nc of x is equal to zero.",
    "start": "351200",
    "end": "360440"
  },
  {
    "text": "So it's the set of points\nthat satisfy this nonlinear equation. And among those\npoints, we want to know",
    "start": "360440",
    "end": "366080"
  },
  {
    "text": "which one produces the minimum\nin the objective function. Or it could be an\ninequality constraint. So D could be the set of\npoints such that some nonlinear",
    "start": "366080",
    "end": "374420"
  },
  {
    "text": "function h of x is, by\nconvention, positive.",
    "start": "374420",
    "end": "380240"
  },
  {
    "text": "So h of x could\nrepresent, for example, the interior of a\ncircle, and c of x could represent the\ncircumference of a circle.",
    "start": "380240",
    "end": "387189"
  },
  {
    "text": "And we would have\nnonlinear equations that reflect those\nvalues of x that satisfy",
    "start": "387189",
    "end": "393560"
  },
  {
    "text": "those sorts of geometries. ",
    "start": "393560",
    "end": "398770"
  },
  {
    "text": "So equality constrained,\npoints that lie on this circle,",
    "start": "398770",
    "end": "403930"
  },
  {
    "text": "inequality constrained, points\nthat lie within this circle. The shape of the feasible set\nis constrained by the problem",
    "start": "403930",
    "end": "410560"
  },
  {
    "text": "that you're actually\ninterested in. So it's easy for me to\ndraw circles in the plane because that's a shape\nyou're familiar with.",
    "start": "410560",
    "end": "416293"
  },
  {
    "text": "But actually, it'll\ncome from some sort of physical constraint on the\nengineering problem you're looking at, like mole fractions\nneed to be bigger than zero",
    "start": "416293",
    "end": "423940"
  },
  {
    "text": "and smaller than one, and\ntemperatures in absolute value have to be bigger than zero\nand smaller than some value",
    "start": "423940",
    "end": "429220"
  },
  {
    "text": "because that's a safety\nfactor on the process. So these set up the\nconstraints on various sorts",
    "start": "429220",
    "end": "436960"
  },
  {
    "text": "of optimization problems\nthat we're interested in. It could also be true that\nwe're interested in, say,",
    "start": "436960",
    "end": "442030"
  },
  {
    "text": "optimization in the domain\noutside of this circle, too. It could be on the inside,\ncould be on the outside.",
    "start": "442030",
    "end": "448030"
  },
  {
    "text": "That's also an inequality\nconstrained sort of problem. You know some of these already.",
    "start": "448030",
    "end": "454730"
  },
  {
    "text": "They're familiar to you. So here's a classic\none from mechanics. Here's the total energy in a\nsystem for, say, a pendulum.",
    "start": "454730",
    "end": "463120"
  },
  {
    "text": "So x is like the position of\nthe tip of this pendulum and v is the velocity\nthat it moves with.",
    "start": "463120",
    "end": "468909"
  },
  {
    "text": "This is the kinetic energy. This is the potential energy. And we know the pendulum\nwill come to rest in a place",
    "start": "468910",
    "end": "474128"
  },
  {
    "text": "where the energy is minimized. Well, the energy can\nonly be minimized when the velocity here is zero,\nbecause any non-zero velocity",
    "start": "474128",
    "end": "482590"
  },
  {
    "text": "will always push the\nenergy content up. So it comes to rest. It doesn't move.",
    "start": "482590",
    "end": "487690"
  },
  {
    "text": "And then there's some\nvalue of x at which the energy is minimized. If there is no constraint\nthat says that the pendulum is",
    "start": "487690",
    "end": "494680"
  },
  {
    "text": "attached to some\ncentral axis, then I can always make\nthe energy smaller by making x more\nand more negative.",
    "start": "494680",
    "end": "501430"
  },
  {
    "text": "It just keeps falling. There is no stopping point. But there's a constraint. The distance between\nthe tip of the pendulum",
    "start": "501430",
    "end": "507340"
  },
  {
    "text": "and this central point is\nsome fixed distance out. So this is an equality\nconstrained sort of problem,",
    "start": "507340",
    "end": "514089"
  },
  {
    "text": "and we have to choose\nfrom the set of v and x the values subject\nto this constraint that minimize the total energy.",
    "start": "514090",
    "end": "519969"
  },
  {
    "text": "And that's this configuration\nof the pendulum here. So you know these sorts\nof problems already.",
    "start": "519970",
    "end": "526420"
  },
  {
    "text": "We talked about this one,\nlinear sorts of programs. These are optimization problems\nwhere the objective function is",
    "start": "526420",
    "end": "535480"
  },
  {
    "text": "linear in the design variables. So it's just the dot product\nbetween x and some vector",
    "start": "535480",
    "end": "541300"
  },
  {
    "text": "c that weights the\ndifferent design options against each other. So we talked about ice cream.",
    "start": "541300",
    "end": "547060"
  },
  {
    "text": "Yes, this is all\npremium ice cream because it comes in\nthe small containers, subject to different\nconstraints.",
    "start": "547060",
    "end": "552810"
  },
  {
    "text": "So those constraints\ncan be things like, oh, x has to be\npositive because we can't make negative amounts of ice cream.",
    "start": "552810",
    "end": "558220"
  },
  {
    "text": "And maybe we've\ndone market research that tells us that\nthe market can only tolerate certain ratios of\ndifferent types of ice cream.",
    "start": "558220",
    "end": "566110"
  },
  {
    "text": "And that may be some\nset of linear equations that describe that market\nresearch that sort of bound",
    "start": "566110",
    "end": "571569"
  },
  {
    "text": "the upper values of\nhow much ice cream we can put out on the market. And then we try to choose the\noptimal blend of pina colada",
    "start": "571570",
    "end": "578680"
  },
  {
    "text": "and strawberry to sell.  So those are linear programs.",
    "start": "578680",
    "end": "586040"
  },
  {
    "text": "This is an inequality\nconstrained optimization. ",
    "start": "586040",
    "end": "593660"
  },
  {
    "text": "In general, we might write\nthese problems like this. We might say minimize f of\nx subject to the constraint",
    "start": "593660",
    "end": "599900"
  },
  {
    "text": "that c of x is 0 and\nh of x is positive. So minimize it over\nthe values of x that",
    "start": "599900",
    "end": "606200"
  },
  {
    "text": "satisfy these two constraints. There's an old approach that's\ndiscussed in the literature.",
    "start": "606200",
    "end": "612060"
  },
  {
    "text": "And it's not used. I'm going to describe\nit to you, and then I want you to try to figure\nout why it's not used. And it's called\nthe penalty method.",
    "start": "612060",
    "end": "619210"
  },
  {
    "text": "And the penalty\nmethod works this way. It says define a new\nobjective function, which is our old objective\nfunction plus some penalty",
    "start": "619210",
    "end": "628690"
  },
  {
    "text": "for violating the constraints. How does that penalty work? So we know that we want\nvalues of x for which c of x",
    "start": "628690",
    "end": "635590"
  },
  {
    "text": "is equal to 0. So if we add to our objective\nfunction the norm of c of x--",
    "start": "635590",
    "end": "641200"
  },
  {
    "text": "this is a positive quantity-- this is a positive\nquantity-- whenever",
    "start": "641200",
    "end": "646699"
  },
  {
    "text": "x doesn't satisfy\nthe constraint, this positive\nquantity will give us a bigger value for this\nobjective function f",
    "start": "646700",
    "end": "654420"
  },
  {
    "text": "than if c of x was equal to 0. So we penalize points which\ndon't satisfy the constraint.",
    "start": "654420",
    "end": "661820"
  },
  {
    "text": "And in the limit that this\npenalty factor mu here goes to zero, the penalties\nget large, so large",
    "start": "661820",
    "end": "670130"
  },
  {
    "text": "that our solution\nwill have to prefer satisfying the constraints. There's another penalty\nfactor over here,",
    "start": "670130",
    "end": "676670"
  },
  {
    "text": "which is identical to this\none but for the inequality constraint. It says take a\nheaviside step function",
    "start": "676670",
    "end": "686769"
  },
  {
    "text": "for which is equal to 1 when\nthe value of its argument is positive, and\nit's equal to zero",
    "start": "686770",
    "end": "692960"
  },
  {
    "text": "when the value of its\nargument is negative. So whenever I violate each\nof my inequality constraints,",
    "start": "692960",
    "end": "700760"
  },
  {
    "text": "Hi of x, turn on this\nheaviside step function, make it equal to 1,\nand then multiply it",
    "start": "700760",
    "end": "706610"
  },
  {
    "text": "by the value of the constraint\nsquared, a positive number. So this is the inequality\nconstraint penalty,",
    "start": "706610",
    "end": "712430"
  },
  {
    "text": "and this is the equality\nconstraint penalty. People don't use this, though.",
    "start": "712430",
    "end": "717480"
  },
  {
    "text": "It makes sense. I take the limit\nthat mu goes to zero. I'm going to have\nto prefer solutions",
    "start": "717480",
    "end": "723810"
  },
  {
    "text": "that satisfy these constraints. Otherwise, if I don't\nsatisfy these constraints, I could always move\ncloser to a solution that",
    "start": "723810",
    "end": "730620"
  },
  {
    "text": "satisfies the\nconstraint, and I'll bring down the value of\nthe objective function. I'll make it lower.",
    "start": "730620",
    "end": "735857"
  },
  {
    "text": "So I'll always prefer these\nlower value solutions. But can you guys take a second\nand sort of talk to each other?",
    "start": "735857",
    "end": "741180"
  },
  {
    "text": "See if you can figure out why\none doesn't use this method. Why is this method a problem?",
    "start": "741180",
    "end": "746355"
  },
  {
    "start": "746355",
    "end": "908980"
  },
  {
    "text": "OK, I heard the volume go\nup at some point, which means either you\nswitched topics and felt more comfortable\ntalking about that",
    "start": "908980",
    "end": "915410"
  },
  {
    "text": "than this, or maybe\nyou guys were coming to some conclusions, or\nhad some ideas about why this might be a bad idea.",
    "start": "915410",
    "end": "921358"
  },
  {
    "text": "Do you want to volunteer some\nof what you were talking about? Yeah, Hersh. AUDIENCE: Could it\nbe that [INAUDIBLE]??",
    "start": "921359",
    "end": "928300"
  },
  {
    "start": "928301",
    "end": "941040"
  },
  {
    "text": "JAMES SWAN: Well, that's\nan interesting idea. So yeah, if we have a\nnon-convex optimization problem, there could be some issues\nwith f of x, and maybe f",
    "start": "941040",
    "end": "948710"
  },
  {
    "text": "of x runs away so\nfast that I can never make the penalty big enough\nto enforce the constraint. That's actually a\nreally interesting idea.",
    "start": "948710",
    "end": "954830"
  },
  {
    "text": "And I like the idea of comparing\nthe magnitude of these two terms. I think that's on\nthe right track. Were there some\nother ideas about why",
    "start": "954830",
    "end": "961560"
  },
  {
    "text": "you might not do this? Different ideas? Yeah. AUDIENCE: [INAUDIBLE]. ",
    "start": "961560",
    "end": "969980"
  },
  {
    "text": "JAMES SWAN: Well, you know,\nthat that's an interesting idea, but actually the two\nterms in the parentheses here are both positive.",
    "start": "969980",
    "end": "977150"
  },
  {
    "text": "So they're only\ngoing to be minimized when I satisfy the constraints. So the local minima of\nthe terms in parentheses",
    "start": "977150",
    "end": "984500"
  },
  {
    "text": "sit on or within the\nboundaries of the feasible set",
    "start": "984500",
    "end": "990260"
  },
  {
    "text": "that we're looking at. So by construction,\nactually, we're going to be able to satisfy\nthem because the local minima",
    "start": "990260",
    "end": "995660"
  },
  {
    "text": "of these points sits\non these boundaries. These terms are minimized by\nsatisfying the constraints.",
    "start": "995660",
    "end": "1003230"
  },
  {
    "text": "Other ideas? Yeah. AUDIENCE: Do your iterates\nhave to be feasible? JAMES SWAN: What's that? AUDIENCE: Your iterates\ndon't have to be feasible?",
    "start": "1003230",
    "end": "1008380"
  },
  {
    "text": "JAMES SWAN: Ooh,\nthis is a good point. The iterates-- this is an\nunconstrained optimization problem. I'm just going to minimize\nthis objective function.",
    "start": "1008380",
    "end": "1015600"
  },
  {
    "text": "It's like what\nHersh said, I can go anywhere I want in the domain. I'm going to minimize\nthis objective function,",
    "start": "1015600",
    "end": "1020680"
  },
  {
    "text": "and then I'm going\nto try to take the limit as mu goes to zero. The iterates don't\nhave to be feasible. Maybe I can't even evaluate\nf of x if the iterates aren't",
    "start": "1020680",
    "end": "1026609"
  },
  {
    "text": "feasible. That's an excellent point. That could be an issue. Anything else?",
    "start": "1026609",
    "end": "1033829"
  },
  {
    "text": "Are there some other ideas? Sure. AUDIENCE: [INAUDIBLE].",
    "start": "1033829",
    "end": "1039078"
  },
  {
    "start": "1039078",
    "end": "1048050"
  },
  {
    "text": "JAMES SWAN: I think\nthat's a good point. AUDIENCE: --boundary\nfrom outside without knowing what's inside.",
    "start": "1048050",
    "end": "1053210"
  },
  {
    "text": "JAMES SWAN: Short. So you'll see, actually,\nthe right way to do this is to use what's called\ninterior point methods, which",
    "start": "1053210",
    "end": "1058370"
  },
  {
    "text": "live inside the domain. This is an excellent point. There's another issue\nwith this that's",
    "start": "1058370",
    "end": "1063890"
  },
  {
    "text": "I think actually less subtle\nthan some of these ideas, which they're all correct, actually. These can be problems with\nthis sort of penalty method.",
    "start": "1063890",
    "end": "1070220"
  },
  {
    "text": "As I take the limit\nthat mu goes to zero, the penalty function\nbecomes large for all points",
    "start": "1070220",
    "end": "1077390"
  },
  {
    "text": "outside the domain. They can become larger\nthan f for those points. And so there are\nsome practical issues",
    "start": "1077390",
    "end": "1083930"
  },
  {
    "text": "about comparing these two\nterms against each other. I may not have sufficient\naccuracy, sufficient number",
    "start": "1083930",
    "end": "1091010"
  },
  {
    "text": "of digits to accurately add\nthese two terms together. So I may prefer\nto find some point",
    "start": "1091010",
    "end": "1097661"
  },
  {
    "text": "that lives on the boundary of\nthe domain as mu goes to zero. But I can't\nguarantee that it was a minima of f on that domain,\nor within that feasible set.",
    "start": "1097661",
    "end": "1107600"
  },
  {
    "text": "So a lot of practical\nissues that suggest this is a bad idea. This is an old idea.",
    "start": "1107600",
    "end": "1113230"
  },
  {
    "text": "People knew this was\nbad for a long time. It seems natural, though. It seems like a good\nway to transform from these constrained\noptimization problems",
    "start": "1113230",
    "end": "1121154"
  },
  {
    "text": "to something we\nknow how to solve, an unconstrained optimization. But actually, it turns out\nnot to be such a great way",
    "start": "1121154",
    "end": "1126200"
  },
  {
    "text": "to do it.  So let's talk about\nseparating out",
    "start": "1126200",
    "end": "1132080"
  },
  {
    "text": "these two different\nmethods from each other, or these two different problems. Let's talk first about\nequality constraints,",
    "start": "1132080",
    "end": "1137840"
  },
  {
    "text": "and then we'll talk about\ninequality constraints. So equality constrained\noptimization problems",
    "start": "1137840",
    "end": "1144040"
  },
  {
    "text": "look like this. Minimize f of x subject\nto c of x equals zero. And let's make it even easier.",
    "start": "1144040",
    "end": "1149710"
  },
  {
    "text": "Rather than having some vector\nof equality constraints, let's just have\na single equation",
    "start": "1149710",
    "end": "1155722"
  },
  {
    "text": "that we have to satisfy for\nthat equality constraint, like the equation for a circle. Solutions have to sit on the\ncircumference of a circle.",
    "start": "1155722",
    "end": "1162970"
  },
  {
    "text": "So one equation that\nwe have to satisfy. You might ask again, what\nare the necessary conditions",
    "start": "1162970",
    "end": "1168640"
  },
  {
    "text": "for defining a minimum? That's what we used\nwhen we had equality-- or when we had\nunconstrained optimization.",
    "start": "1168640",
    "end": "1175270"
  },
  {
    "text": "First we had to define\nwhat a minimum was, and we found that minima\nwere critical points, places",
    "start": "1175270",
    "end": "1180940"
  },
  {
    "text": "where the gradient of the\nobjective function was zero. That doesn't have\nto be true anymore.",
    "start": "1180940",
    "end": "1186880"
  },
  {
    "text": "Now, the minima has to live on\nthis boundary of some domain.",
    "start": "1186880",
    "end": "1192670"
  },
  {
    "text": "It has to live in this set\nof points c of x equals zero. And the gradient of\nf is not necessarily",
    "start": "1192670",
    "end": "1198100"
  },
  {
    "text": "zero at that minimal point. But you might guess that\nTaylor expansions are the way",
    "start": "1198100",
    "end": "1204580"
  },
  {
    "text": "to figure out what the\nappropriate conditions",
    "start": "1204580",
    "end": "1214179"
  },
  {
    "text": "for a minima are. So let's take f of x, and\nlet's expand it, do a Taylor expansion in some direction, d.",
    "start": "1214180",
    "end": "1220630"
  },
  {
    "text": "So we'll take a step away\nfrom x, which is small, in some direction, d. So f of x plus d is\nf of x plus g dot",
    "start": "1220630",
    "end": "1228549"
  },
  {
    "text": "d, the dot product between\nthe gradient of f and d.",
    "start": "1228550",
    "end": "1234580"
  },
  {
    "text": "And at a minimum,\neither the gradient is zero or the gradient is\nperpendicular to this direction",
    "start": "1234580",
    "end": "1243460"
  },
  {
    "text": "we moved in, d. We know that because this\nterm is going to increase--",
    "start": "1243460",
    "end": "1253380"
  },
  {
    "text": "well, will change\nthe value of f of x. It will either make\nit bigger or smaller depending on whether it's\npositive or negative.",
    "start": "1253380",
    "end": "1259460"
  },
  {
    "text": "In either case, it\nwill say that this point x can't be a minimum\nunless this term is exactly",
    "start": "1259460",
    "end": "1264770"
  },
  {
    "text": "equal to zero in the limit\nthat d becomes small. So either the gradient\nis zero or the gradient",
    "start": "1264770",
    "end": "1269870"
  },
  {
    "text": "is orthogonal to this\ndirection d we stepped in. And d was arbitrary.",
    "start": "1269870",
    "end": "1276290"
  },
  {
    "text": "We just said take a\nstep in a direction, d. ",
    "start": "1276290",
    "end": "1282350"
  },
  {
    "text": "Lets take our\nequality constraint and do the same sort of\nTaylor expansion, because we",
    "start": "1282350",
    "end": "1288140"
  },
  {
    "text": "know if we're searching for\na minima along this curve c of x better be equal to zero.",
    "start": "1288140",
    "end": "1295310"
  },
  {
    "text": "It better satisfy\nthe constraint. And also, c of x plus d, that\nlittle step in the direction d,",
    "start": "1295310",
    "end": "1300440"
  },
  {
    "text": "should also satisfy\nthe constraint. We want to study only the\nfeasible set of values.",
    "start": "1300440",
    "end": "1306980"
  },
  {
    "text": "So actually, d\nwasn't arbitrary. d had to satisfy this\nconstraint that, when I took this little step, c of x\nplus d had to be equal to zero.",
    "start": "1306980",
    "end": "1313700"
  },
  {
    "text": "So again, we'll take\nnow a Taylor expansion of c of x plus d, which\nis c of x plus grad",
    "start": "1313700",
    "end": "1319310"
  },
  {
    "text": "of c of x dotted with d. And that implies that d must be\nperpendicular to the gradient",
    "start": "1319310",
    "end": "1327730"
  },
  {
    "text": "of c of x, because c of\nx plus d has to be zero and c of x has to be zero. So the gradient of c of x\ndot d-- it's a leading order",
    "start": "1327730",
    "end": "1336496"
  },
  {
    "text": "has also got to\nbe equal to zero. So d and the gradient\nin c are perpendicular, and d and the gradient\nin g have to be",
    "start": "1336496",
    "end": "1343780"
  },
  {
    "text": "perpendicular at a minimum. That's going to define the\nminimum on this equality",
    "start": "1343780",
    "end": "1349900"
  },
  {
    "text": "constrained set. Does that make sense? c satisfies the\nconstraint, c plus d",
    "start": "1349900",
    "end": "1357260"
  },
  {
    "text": "satisfies the constraint. If this is true, d has to be\nperpendicular to the gradient of c, g has to be perpendicular\nto the gradient of d.",
    "start": "1357260",
    "end": "1366430"
  },
  {
    "text": "d is, in some sense,\narbitrary still. d has to satisfy\ncondition that it's",
    "start": "1366430",
    "end": "1372080"
  },
  {
    "text": "perpendicular to\nthe gradient of c, but who knows,\nthere could be lots of vectors that are\nperpendicular to the gradient",
    "start": "1372080",
    "end": "1377990"
  },
  {
    "text": "of c. So the only generic\nrelationship between these two we can formulate is g must be\nparallel to the gradient of c.",
    "start": "1377990",
    "end": "1386720"
  },
  {
    "text": "g is perpendicular\nto d, gradient of c is perpendicular to d. In the most generic\nway, g and gradient of c",
    "start": "1386720",
    "end": "1392985"
  },
  {
    "text": "should be parallel\nto each other, because d I can\nselect arbitrarily from all the vectors of\nthe same dimension as x.",
    "start": "1392985",
    "end": "1401400"
  },
  {
    "text": " If g is parallel to\nthe gradient of c, then I can write that g\nminus some scalar multiplied",
    "start": "1401400",
    "end": "1411080"
  },
  {
    "text": "by the gradient of\nc is equal to zero. That's an equivalent\nstatement, that g is parallel to the gradient of c.",
    "start": "1411080",
    "end": "1417230"
  },
  {
    "text": "So that's a condition\nassociated with points x that solve this equality\nconstrained problem.",
    "start": "1417230",
    "end": "1425660"
  },
  {
    "text": "The other condition\nis that point x still has to satisfy the\nequality constraint.",
    "start": "1425660",
    "end": "1432170"
  },
  {
    "text": "But I introduced a new\nunknown, this lambda, which is called the\nLagrange multiplier.",
    "start": "1432170",
    "end": "1438170"
  },
  {
    "text": "So now I have one extra unknown,\nbut I have one extra equation. ",
    "start": "1438170",
    "end": "1446094"
  },
  {
    "text": "Let me give you a graphical\ndepiction of this, and then I'll write down\nthe formal equations again.",
    "start": "1446094",
    "end": "1451510"
  },
  {
    "text": "So let's suppose\nwe want to minimize this parabolic function\nsubject to the constraint",
    "start": "1451510",
    "end": "1457750"
  },
  {
    "text": "that the solution\nlives on the line. So here's the contours\nof the function, and the solution has\nto live on this line.",
    "start": "1457750",
    "end": "1464590"
  },
  {
    "text": " So I get to stand\non this line, and I",
    "start": "1464590",
    "end": "1470050"
  },
  {
    "text": "get to walk and walk and walk\nuntil I can't walk downhill anymore. and I've got to\nturn and walk uphill again.",
    "start": "1470050",
    "end": "1476650"
  },
  {
    "text": "And you can see the point where\nI can't walk downhill anymore is the place where this\nconstraint is parallel",
    "start": "1476650",
    "end": "1485170"
  },
  {
    "text": "to the contour, or\nwhere the gradient of the objective\nfunction is parallel",
    "start": "1485170",
    "end": "1492519"
  },
  {
    "text": "to the gradient\nof the constraint. So you can actually\nfind this point by imagining yourself\nmoving along this landscape.",
    "start": "1492520",
    "end": "1500140"
  },
  {
    "text": "After I get to this point,\nI start going uphill again. ",
    "start": "1500140",
    "end": "1505529"
  },
  {
    "text": "So that's the method of\nLagrange multipliers. Minimize f of x subject\nto this constraint.",
    "start": "1505530",
    "end": "1511799"
  },
  {
    "text": "The solution is\ngiven by the point x at which the gradient is\nparallel to the gradient of c,",
    "start": "1511800",
    "end": "1520510"
  },
  {
    "text": "and at which c is equal to zero. And you solve this system\nof nonlinear equations",
    "start": "1520510",
    "end": "1525970"
  },
  {
    "text": "for two unknowns. One is x, and the other\nis this unknown lambda.",
    "start": "1525970",
    "end": "1531790"
  },
  {
    "text": "How far stretched\nis the gradient in f relative to\nthe gradient in c?",
    "start": "1531790",
    "end": "1539192"
  },
  {
    "text": "So again, we've turned\nthe minimization problem into a system of\nnonlinear equations. In order to satisfy the\nequality constraint,",
    "start": "1539192",
    "end": "1546100"
  },
  {
    "text": "we've had to introduce\nanother unknown, the Lagrange multiplier. It turns out this solution\nset, x and lambda,",
    "start": "1546100",
    "end": "1554980"
  },
  {
    "text": "is a critical point of\nsomething called the Lagrangian. It's a function f of x\nminus lambda times c.",
    "start": "1554980",
    "end": "1563950"
  },
  {
    "text": "It's a critical point in x\nand lambda of this nonlinear function called the Lagrangian.",
    "start": "1563950",
    "end": "1570820"
  },
  {
    "text": "It's not a minimum of this\nfunction, unfortunately. It's a saddle point of the\nLagrangian, it turns out.",
    "start": "1570820",
    "end": "1577240"
  },
  {
    "text": "So we're trying to find a\nsaddle point of the Lagrangian. Does this make sense?",
    "start": "1577240",
    "end": "1583990"
  },
  {
    "text": "Yes? OK. We've got to be\ncareful, of course. Just like with\nunconstrained optimization,",
    "start": "1583990",
    "end": "1589630"
  },
  {
    "text": "we actually have to check that\nour solution is a minimum. We can't take for\ngranted, we can't",
    "start": "1589630",
    "end": "1596200"
  },
  {
    "text": "suppose that our nonlinear\nsolver found a minimum when it solved this equation.",
    "start": "1596200",
    "end": "1601322"
  },
  {
    "text": "Other critical points can\nsatisfy this equation, too. So we've got to go back and try\nto check robustly whether it's",
    "start": "1601322",
    "end": "1607169"
  },
  {
    "text": "actually a minimum. But this is the method. Introduce an additional unknown,\nthe Lagrange multiplier,",
    "start": "1607169",
    "end": "1613059"
  },
  {
    "text": "because you can\nshow geometrically that the gradient of\nthe objective function should be parallel to the\ngradient of the constraint",
    "start": "1613060",
    "end": "1620140"
  },
  {
    "text": "at the minimum. Does that make sense? Does this picture make sense? OK.",
    "start": "1620140",
    "end": "1626020"
  },
  {
    "text": "So you know how to solve\nsystems of nonlinear equations, you know how to solve\nconstrained optimization problems. ",
    "start": "1626020",
    "end": "1635299"
  },
  {
    "text": "So here's f. Here's c. We can actually write out\nwhat these equations are.",
    "start": "1635300",
    "end": "1643170"
  },
  {
    "text": "So you can show that the\ngradient of x minus lambda gradient of c, that's a vector,\n2x1 minus lambda and 20x2",
    "start": "1643170",
    "end": "1651660"
  },
  {
    "text": "plus lambda. And c is the equation\nfor this line down here, so x1\nminus x2 minus 3.",
    "start": "1651660",
    "end": "1657480"
  },
  {
    "text": "And that's all got\nto be equal to zero. In this case, this is just a\nsystem of linear equations. So you can actually\nsolve directly",
    "start": "1657480",
    "end": "1664080"
  },
  {
    "text": "for x1, x2, and lambda. And it's not too difficult to\nfind the solution for all three",
    "start": "1664080",
    "end": "1670170"
  },
  {
    "text": "of these things by hand. But in general, these\nconstraints can be nonlinear.",
    "start": "1670170",
    "end": "1675750"
  },
  {
    "text": "The objective function\ndoesn't have to be quadratic. Those are the easiest\ncases to look at. And the same\nmethodology applies.",
    "start": "1675750",
    "end": "1682809"
  },
  {
    "text": "And so you should check\nthat you're able to do this. This is the simplest possible\nequality constraint problem.",
    "start": "1682810",
    "end": "1688700"
  },
  {
    "text": "You could do it by hand. You should check that you're\nactually able to do it, that you understand the steps\nthat go into writing out",
    "start": "1688700",
    "end": "1693840"
  },
  {
    "text": "these equations.  Let's just take one\nstep forward and look",
    "start": "1693840",
    "end": "1699290"
  },
  {
    "text": "at a less generic\ncase, one in which we have a vector valued\nfunction that gives the equality",
    "start": "1699290",
    "end": "1708220"
  },
  {
    "text": "constraints instead. So rather than one equation\nwe have to satisfy, there may be many. ",
    "start": "1708220",
    "end": "1714610"
  },
  {
    "text": "It's possible that the\nfeasible set doesn't have any solutions in it.",
    "start": "1714610",
    "end": "1721240"
  },
  {
    "text": "It's possible that\nthere is no x that satisfies all of these\nconstraints simultaneously.",
    "start": "1721240",
    "end": "1726559"
  },
  {
    "text": "That's a bad problem to have. You wouldn't like to have\nthat problem very much.",
    "start": "1726560",
    "end": "1732100"
  },
  {
    "text": "But it's possible\nthat that's the case. But let's assume that there are\nsolutions for the time being.",
    "start": "1732100",
    "end": "1737600"
  },
  {
    "text": "So there are x's that satisfy\nthe equality constraint. Let's see if we can\nfigure out again what the necessary conditions\nfor defining a minima are.",
    "start": "1737600",
    "end": "1744640"
  },
  {
    "text": "So same as before,\nlet's Taylor expand f of x going in\nsome direction, d.",
    "start": "1744640",
    "end": "1750340"
  },
  {
    "text": "And let's make d\na nice small step so we can just treat\nthe f of x plus d as a linearized function.",
    "start": "1750340",
    "end": "1756820"
  },
  {
    "text": "So we can see again\nthat g has to be perpendicular to this\ndirection, d, if we're going to have a minima.",
    "start": "1756820",
    "end": "1762790"
  },
  {
    "text": "Otherwise, I could step\nin some direction, d, and I'll find either a\nsmaller value of f of x plus d",
    "start": "1762790",
    "end": "1767920"
  },
  {
    "text": "or a bigger value\nof f of x plus d. So g has to be\nperpendicular to d.",
    "start": "1767920",
    "end": "1773626"
  },
  {
    "text": "And for the equality\nconstraints, again, they all have to satisfy\nthis equality constraint",
    "start": "1773626",
    "end": "1779901"
  },
  {
    "text": "up there. So c of x has to be equal\nto zero, and c of x plus d also has to be equal to zero.",
    "start": "1779901",
    "end": "1785309"
  },
  {
    "text": " And so if we take\na Taylor expansion",
    "start": "1785310",
    "end": "1791710"
  },
  {
    "text": "of c of x plus d,\nabout x, you'll get c of x plus d\nplus the Jacobian",
    "start": "1791710",
    "end": "1798730"
  },
  {
    "text": "of c, all the partial\nderivatives of c with respect to x, multiplied by d.",
    "start": "1798730",
    "end": "1804970"
  },
  {
    "text": " We know that c of x plus d\nis zero, and c of x is zero,",
    "start": "1804970",
    "end": "1811710"
  },
  {
    "text": "so the directions, d, belong\nto what set of vectors?",
    "start": "1811710",
    "end": "1816830"
  },
  {
    "text": "The null space. So these directions have\nto live in the null space of the Jacobian of c.",
    "start": "1816830",
    "end": "1825620"
  },
  {
    "text": "So I can't step\nin any direction, I have to step in\ndirections that are in the null space of c.",
    "start": "1825620",
    "end": "1833464"
  },
  {
    "text": "g is perpendicular\nto d, as well. And d belongs to\nthe null space of c.",
    "start": "1833464",
    "end": "1840180"
  },
  {
    "text": "In fact, you know that d\nis perpendicular to each",
    "start": "1840180",
    "end": "1845940"
  },
  {
    "text": "of the rows of the Jacobian. Right? You know that?",
    "start": "1845940",
    "end": "1851120"
  },
  {
    "text": "I just do the matrix\nvector product, right? And so each element of\nthis matrix vector product",
    "start": "1851120",
    "end": "1856190"
  },
  {
    "text": "is the dot product of d with a\ndifferent row of the Jacobian.",
    "start": "1856190",
    "end": "1861919"
  },
  {
    "text": "So those rows are\na set of vectors. Those rows describe the\nrange of J transpose,",
    "start": "1861920",
    "end": "1872660"
  },
  {
    "text": "or the row space of J. Remember\nwe talked about the four fundamental\nsubspaces, and I said we almost never use\nthose other ones,",
    "start": "1872660",
    "end": "1879001"
  },
  {
    "text": "but this is one\ntime when we will. So those rows belong to\nthe range of J transpose,",
    "start": "1879001",
    "end": "1885670"
  },
  {
    "text": "or they belong to the\nleft null space of J. I need to find a g,\na gradient, which",
    "start": "1885670",
    "end": "1893420"
  },
  {
    "text": "is always perpendicular to d. And I know d is always\nperpendicular to the rows of J.",
    "start": "1893420",
    "end": "1899690"
  },
  {
    "text": "So I can write g as a linear\nsuperposition of the rows of J. As long as g is a linear\nsuperposition of the rows,",
    "start": "1899690",
    "end": "1906440"
  },
  {
    "text": "it'll always be\nperpendicular to d. Vectors from the null\nspace of a matrix",
    "start": "1906440",
    "end": "1913080"
  },
  {
    "text": "are orthogonal to vectors from\nthe row space of that matrix, it turns out.",
    "start": "1913080",
    "end": "1919326"
  },
  {
    "text": "And they're orthogonal\nfor this reason. ",
    "start": "1919326",
    "end": "1926350"
  },
  {
    "text": "So it tells us, if\nJd is zero, then",
    "start": "1926350",
    "end": "1931750"
  },
  {
    "text": "d belongs to the null space. g is perpendicular to d. That means I could write g\nas a linear superposition",
    "start": "1931750",
    "end": "1938350"
  },
  {
    "text": "of the rows of J. So g belongs\nto the range of J transpose,",
    "start": "1938350",
    "end": "1944530"
  },
  {
    "text": "or it belongs to\nthe row space of J. Those are equivalent statements. And therefore, I should\nbe able to write g",
    "start": "1944530",
    "end": "1950980"
  },
  {
    "text": "as a linear superposition\nof the rows of J. And one way to say\nthat is I should be able to write\ng as J transpose",
    "start": "1950980",
    "end": "1957160"
  },
  {
    "text": "times some other vector lambda. That's an equivalent\nway of saying",
    "start": "1957160",
    "end": "1963790"
  },
  {
    "text": "that g is a linear\nsuperposition of the rows of J. I don't know the\nvalues of lambda.",
    "start": "1963790",
    "end": "1969420"
  },
  {
    "text": " So I introduced a\nnew set of unknowns,",
    "start": "1969420",
    "end": "1975590"
  },
  {
    "text": "a set of Lagrange multipliers. My minima is going\nto be found when",
    "start": "1975590",
    "end": "1980600"
  },
  {
    "text": "I satisfy this equation,\njust like before, and when I'm able to satisfy\nall of the equality constraints.",
    "start": "1980600",
    "end": "1988280"
  },
  {
    "start": "1988280",
    "end": "1994160"
  },
  {
    "text": "How many Lagrange\nmultipliers do I have here?",
    "start": "1994160",
    "end": "1999960"
  },
  {
    "text": "Can you figure that out? You can talk with your\nneighbors if you want. Take a couple minutes. Tell me how many Lagrange\nmultipliers, how many elements",
    "start": "1999960",
    "end": "2006600"
  },
  {
    "text": "are in this vector lambda. ",
    "start": "2006600",
    "end": "2059379"
  },
  {
    "text": "How many elements are in lambda? Can you tell me? ",
    "start": "2059380",
    "end": "2066383"
  },
  {
    "text": "Sam. AUDIENCE: Same as the number\nof equality constraints. JAMES SWAN: Yes. It's the same as the number\nof equality constraints.",
    "start": "2066383",
    "end": "2073469"
  },
  {
    "text": "J came from the gradient of c. It's the Jacobian of c.",
    "start": "2073469",
    "end": "2081369"
  },
  {
    "text": "So it has a number of columns\nequal to the number of elements in x, because I'm taking\npartial derivatives with respect",
    "start": "2081370",
    "end": "2088888"
  },
  {
    "text": "to each element of\nx, and has a number of rows equal to the\nnumber of elements of c.",
    "start": "2088889",
    "end": "2094960"
  },
  {
    "text": "So J transpose, I just\ntranspose those dimensions. And lambda must have the\nsame number of elements",
    "start": "2094960",
    "end": "2102700"
  },
  {
    "text": "as c does in order to make\nthis product make sense. So I introduce a new\nnumber of unknowns.",
    "start": "2102700",
    "end": "2108250"
  },
  {
    "text": "It's equal to exactly the\nnumber of equality constraints that I had, which\nis good, because I'm",
    "start": "2108250",
    "end": "2114610"
  },
  {
    "text": "going to make a system\nof equations that says g of x minus J\ntranspose lambda equals 0",
    "start": "2114610",
    "end": "2120790"
  },
  {
    "text": "and c of x equals 0. And the number of equations\nhere is the number",
    "start": "2120790",
    "end": "2126970"
  },
  {
    "text": "of elements in x\nfor this gradient, and the number of\nelements in c for c. And the number of\nunknowns is the number",
    "start": "2126970",
    "end": "2133540"
  },
  {
    "text": "of elements in x, and the number\nof elements in c associated with the Lagrange multiplier. So I have enough equations\nand unknowns to determine",
    "start": "2133540",
    "end": "2140800"
  },
  {
    "text": "all of these things. So whether I have one equality\nconstraint or a million",
    "start": "2140800",
    "end": "2147440"
  },
  {
    "text": "equality constraints,\nthe problem is identical. We use the method of\nLagrange multipliers.",
    "start": "2147440",
    "end": "2152660"
  },
  {
    "text": "We have to solve an\naugmented system of equations for x and this projection on the\nrow space of J, which tells us",
    "start": "2152660",
    "end": "2162800"
  },
  {
    "text": "how the gradient is\nstretched or made up, composed of elements\nof the row space of J.",
    "start": "2162800",
    "end": "2168800"
  },
  {
    "text": "These are the\nconditions associated with a minima in our\nobjective function on this boundary dictated\nby the equality constraint.",
    "start": "2168800",
    "end": "2177566"
  },
  {
    "text": "And of course, the solution\nset is a critical point of a Lagrangian, which is\nf of x minus c dot lambda.",
    "start": "2177566",
    "end": "2185800"
  },
  {
    "text": "And it's not a minimum of\nit, it's a critical point. It's a saddle point, it turns\nout, of this Lagrangian.",
    "start": "2185800",
    "end": "2193309"
  },
  {
    "text": "So we've got to check,\ndid we find a saddle point or not when we find a solution\nto this equation here.",
    "start": "2193309",
    "end": "2199974"
  },
  {
    "text": "But it's just a system\nof nonlinear equations. If we have some good initial\nguess, what do we apply? Newton-Raphson, converge\nrate towards the solution.",
    "start": "2199974",
    "end": "2208670"
  },
  {
    "text": "If we don't have a\ngood initial guess, we've discussed lots of methods\nwe could employ, like homotopy",
    "start": "2208670",
    "end": "2215270"
  },
  {
    "text": "or continuation\nto try to develop good initial guesses for\nwhat the solution should be.",
    "start": "2215270",
    "end": "2220709"
  },
  {
    "text": "Are there any\nquestions about this?  Good.",
    "start": "2220709",
    "end": "2227930"
  },
  {
    "text": "OK. So you go to Matlab\nand you call fmincon,",
    "start": "2227930",
    "end": "2234369"
  },
  {
    "text": "do a minimization problem, and\nyou give it some constraints. Linear constraints,\nnonlinear constraints, it doesn't matter actually.",
    "start": "2234370",
    "end": "2240502"
  },
  {
    "text": "The problem is the\nsame for both of them. It's just a little bit easier\nif I have linear constraints. If this constraining function\nis a linear function, then",
    "start": "2240502",
    "end": "2248650"
  },
  {
    "text": "the Jacobian I know. It's the coefficient matrix\nof this linear problem.",
    "start": "2248650",
    "end": "2254594"
  },
  {
    "text": "Now I only have to solve\nlinear equations down here. So the problem is a little\nbit simpler to solve.",
    "start": "2254594",
    "end": "2259730"
  },
  {
    "text": "So Matlab sort of\nbreaks these apart so it can use different\ntechniques depending on which",
    "start": "2259730",
    "end": "2265420"
  },
  {
    "text": "sort of problem is posed. But the solution\nmethod is the same. It does the method of\nLagrange multipliers to find the solution.",
    "start": "2265420",
    "end": "2271394"
  },
  {
    "text": "OK?  Inequality constraints.",
    "start": "2271394",
    "end": "2278270"
  },
  {
    "text": "So interior point\nmethods were mentioned. And it turns out this\nis really the best",
    "start": "2278270",
    "end": "2284750"
  },
  {
    "text": "way to go about solving\ngeneric inequality constrained problems.",
    "start": "2284750",
    "end": "2289790"
  },
  {
    "text": "So the problems of\nthe sort minimize f of x subject to\nh of x is positive,",
    "start": "2289790",
    "end": "2295340"
  },
  {
    "text": "or at least not negative. This is some\nnonlinear inequality that describes some domain\nand its boundary in which",
    "start": "2295340",
    "end": "2303350"
  },
  {
    "text": "the solution has to live. And what's done is to rewrite\nas an unconstrained optimization",
    "start": "2303350",
    "end": "2309530"
  },
  {
    "text": "problem with a barrier\nthat's incorporated. This looks a lot like\nthe penalty method,",
    "start": "2309530",
    "end": "2316310"
  },
  {
    "text": "but it's very different. And I'll explain how. So instead, we want to\nminimize this f of x minus mu",
    "start": "2316310",
    "end": "2323630"
  },
  {
    "text": "times the sum of log of h,\neach of these constraints.",
    "start": "2323630",
    "end": "2329089"
  },
  {
    "text": " If h is negative, we'll take the\nlog of the negative argument.",
    "start": "2329090",
    "end": "2338110"
  },
  {
    "text": "That's a problem\ncomputationally. So the best we could do\nis approach the boundary where h is equal to zero.",
    "start": "2338110",
    "end": "2345400"
  },
  {
    "text": "And as h goes to zero, the\nlog goes to minus infinity. So this term tends to blow\nup because I've got a minus",
    "start": "2345400",
    "end": "2351490"
  },
  {
    "text": "sign in front of it. So this is sort\nof like a penalty,",
    "start": "2351490",
    "end": "2356920"
  },
  {
    "text": "but it's a little different\nbecause the factor in front I'm actually going to take\nthe limit as mu goes to zero.",
    "start": "2356920",
    "end": "2363670"
  },
  {
    "text": "I'm going to take the limit\nas this factor gets small, rather than gets big. The log will always\nget big as I approach",
    "start": "2363670",
    "end": "2371170"
  },
  {
    "text": "the boundary of the domain. It'll blow up. So that's not a problem. But I can take the limit that\nmu gets smaller and smaller.",
    "start": "2371170",
    "end": "2379180"
  },
  {
    "text": "And this quantity here\nwill have less and less of an impact on the shape of\nthis new objective function",
    "start": "2379180",
    "end": "2386920"
  },
  {
    "text": "and mu gets smaller and smaller. The impact will only be\nnearest the boundary. Does that make sense?",
    "start": "2386920",
    "end": "2393100"
  },
  {
    "text": "So you take the limit\nthat mu approaches zero. It's got to approach it\nfrom the positive side, not the negative side, so\neverything behaves well.",
    "start": "2393100",
    "end": "2401386"
  },
  {
    "text": "And this is called an\ninterior point method. So we have to determine the\nminimum of this new objective",
    "start": "2401386",
    "end": "2407950"
  },
  {
    "text": "function for progressively\nweaker barriers. So we might start\nwith some value of mu, and we might reduce\nmu progressively",
    "start": "2407950",
    "end": "2415780"
  },
  {
    "text": "until we get mu\ndown small enough that we think we've\nconverged to a solution. So how do you do that reliably?",
    "start": "2415780",
    "end": "2420800"
  },
  {
    "text": " What's the procedure one uses\nto solve a problem successively",
    "start": "2420800",
    "end": "2429140"
  },
  {
    "text": "for different parameter values?  AUDIENCE: [INAUDIBLE]. JAMES SWAN: Yeah, it's\na homotopy, right?",
    "start": "2429140",
    "end": "2435538"
  },
  {
    "text": "You're just going to change\nthe value of this barrier parameter. And you're going\nto find a minima.",
    "start": "2435538",
    "end": "2440780"
  },
  {
    "text": "And if you make a small change\nin the barrier parameter, that's going to serve as\nan excellent initial guess for the next value.",
    "start": "2440780",
    "end": "2446220"
  },
  {
    "text": "And so you're just going\nto take these small steps. And the optimization\nroutine is going to carry you towards\nthe minimum in the limit",
    "start": "2446220",
    "end": "2453180"
  },
  {
    "text": "that mu goes to zero. So you do this with homotopy. ",
    "start": "2453180",
    "end": "2458340"
  },
  {
    "text": "Here's an example of this\nsort of interior point method, a trivial example. Minimize x subject\nto x being positive.",
    "start": "2458340",
    "end": "2466180"
  },
  {
    "text": "So we know the solution\nlives where x equals zero. But let's write this as\nunconstrained optimization",
    "start": "2466180",
    "end": "2473040"
  },
  {
    "text": "using a barrier. So minimize x minus\nmu times log x.",
    "start": "2473040",
    "end": "2478850"
  },
  {
    "text": "Here's x minus mu times log x. So out here, where x is\nbig, x wins over log x,",
    "start": "2478850",
    "end": "2486069"
  },
  {
    "text": "so everything starts\nto look linear. But as x become\nsmaller and smaller, log x gets very negative, so\nminus log x gets very positive.",
    "start": "2486070",
    "end": "2494010"
  },
  {
    "text": "And here's the log\ncreeping back up. And as I decrease mu\nsmaller and smaller, you can see the minima\nof this function",
    "start": "2494010",
    "end": "2499830"
  },
  {
    "text": "is moving closer and\ncloser and closer to zero.",
    "start": "2499830",
    "end": "2504840"
  },
  {
    "text": "So if I take the limit\nthat mu decreases from some positive\nnumber towards zero, eventually this minimum\nis going to converge",
    "start": "2504840",
    "end": "2512410"
  },
  {
    "text": "to the minimum of the\nconstrained inequality, constrained\noptimization problem.",
    "start": "2512410",
    "end": "2517580"
  },
  {
    "text": "Make sense? OK. OK. So we want to do this.",
    "start": "2517580",
    "end": "2524740"
  },
  {
    "text": "You can use any barrier\nfunction you want. Any thoughts on why a\nlogarithmic barrier is used? ",
    "start": "2524740",
    "end": "2538710"
  },
  {
    "text": "No. OK, that's OK. So minus log is\ngoing to be convex.",
    "start": "2538710",
    "end": "2543910"
  },
  {
    "text": "Log isn't convex, but minus\nlog is going to be convex. So that's good. If this function's convex,\nthen their combination's",
    "start": "2543910",
    "end": "2549920"
  },
  {
    "text": "going to be convex,\nand we'll be OK. But the gradient of the\nlog is easy to compute. Grad log h is 1 over h grad h.",
    "start": "2549920",
    "end": "2557690"
  },
  {
    "text": "So if I know h, I know\ngrad h, it's easy for me to compute the\ngradient of log h. We know we're going to solve\nthis unconstrained optimization",
    "start": "2557690",
    "end": "2566000"
  },
  {
    "text": "problem where we need to take\ngrad of this objective function equal zero. So the calculations are easy. The log makes it easy like that.",
    "start": "2566000",
    "end": "2572320"
  },
  {
    "text": "The log is also like\nthe most weakly singular function available to us.",
    "start": "2572320",
    "end": "2577520"
  },
  {
    "text": "Out of all the tool box of\nall problems we can reach to, the log has the mildest\nsort of singularities.",
    "start": "2577520",
    "end": "2582835"
  },
  {
    "text": "Singularities at both ends,\nwhich is sort of funny, but the mildest sort\nof singularities you have to cope with.",
    "start": "2582835",
    "end": "2588782"
  },
  {
    "text": "So we want to find the\nminimum of these unconstrained optimization problems where the\ngradient of f minus mu sum 1",
    "start": "2588782",
    "end": "2595310"
  },
  {
    "text": "over h, grad h,\nis equal to zero. And we just do that for\nprogressively smaller values",
    "start": "2595310",
    "end": "2600700"
  },
  {
    "text": "of mu, and we'll\nconverge to a solution. That's the interior\npoint method.",
    "start": "2600700",
    "end": "2605720"
  },
  {
    "text": "You use homotopy to study a\nsequence of barrier parameters,",
    "start": "2605720",
    "end": "2611359"
  },
  {
    "text": "or continuation to study a\nsequence of barrier parameters.",
    "start": "2611360",
    "end": "2616760"
  },
  {
    "text": "You stop the homotopy or\ncontinuation when what? How are you going to stop?",
    "start": "2616760",
    "end": "2622806"
  },
  {
    "text": " I've got to make\nmu small, right?",
    "start": "2622806",
    "end": "2629234"
  },
  {
    "text": "I want to go towards the\nlimit mu equals zero. I can't actually get\nto mu equals zero, I've just got to approach it.",
    "start": "2629234",
    "end": "2634450"
  },
  {
    "text": "So how small do I need\nto make mu before I quit? It's an interesting question. What do you think?",
    "start": "2634450",
    "end": "2639910"
  },
  {
    "text": " I'll take this answer first. AUDIENCE: So it doesn't\naffect the limitation.",
    "start": "2639910",
    "end": "2646512"
  },
  {
    "text": "JAMES SWAN: Good. So we might look at\nthe solution and see is the solution\nbecoming less and less sensitive to the choice of mu.",
    "start": "2646512",
    "end": "2652430"
  },
  {
    "text": "Did you have another suggestion? AUDIENCE: [INAUDIBLE]. ",
    "start": "2652430",
    "end": "2657930"
  },
  {
    "text": "JAMES SWAN: Set the tolerance. Right, OK. AUDIENCE: [INAUDIBLE].",
    "start": "2657930",
    "end": "2663484"
  },
  {
    "text": "JAMES SWAN: Mhm. Right, right, right, right. So you-- AUDIENCE: [INAUDIBLE].",
    "start": "2663484",
    "end": "2668842"
  },
  {
    "text": "JAMES SWAN: Good. So there were two\nsuggestions here. One is along the lines\nof a step-norm criteria,",
    "start": "2668842",
    "end": "2674099"
  },
  {
    "text": "like I check my\nsolution as I change mu, and I ask when does\nmy solution seem",
    "start": "2674100",
    "end": "2679700"
  },
  {
    "text": "relatively insensitive to mu. When the changes in these\nsteps relative to mu",
    "start": "2679700",
    "end": "2685369"
  },
  {
    "text": "get sufficiently\nsmall, I might be willing to accept\nthese solutions as reasonable solutions for\nthe constrained optimization.",
    "start": "2685370",
    "end": "2693230"
  },
  {
    "text": "I can also go back\nand I can check sort of function norm criteria.",
    "start": "2693230",
    "end": "2698240"
  },
  {
    "text": "I can take the value of\nx I found as the minimum, and I can ask how\ngood a job does it do satisfying the\noriginal equations.",
    "start": "2698240",
    "end": "2708140"
  },
  {
    "text": "How far away am I from\nsatisfying the inequality constraint? How close am I to actually\nminimizing the function",
    "start": "2708140",
    "end": "2714740"
  },
  {
    "text": "within that domain? ",
    "start": "2714740",
    "end": "2720245"
  },
  {
    "text": "OK. So we're running\nout of time here. Let me provide you\nwith an example.",
    "start": "2720245",
    "end": "2726300"
  },
  {
    "text": "So let's minimize again-- I always pick this function\nbecause it's easy to visualize, a nice parabolic function\nthat opens upwards.",
    "start": "2726300",
    "end": "2732770"
  },
  {
    "text": "And let's minimize it\nsubject to the constraint that h of x1 and x2\nis equal to 1 minus--",
    "start": "2732770",
    "end": "2742620"
  },
  {
    "text": "well, the equation for a circle\nof radius 1, essentially. The interior of that circle.",
    "start": "2742620",
    "end": "2749240"
  },
  {
    "text": "So here's the contours\nof the function, and this red domain\nis the constraint. And we want to know\nthe smallest value",
    "start": "2749240",
    "end": "2754839"
  },
  {
    "text": "of f that lives in this domain. So here's a Matlab code. You can try it out.",
    "start": "2754840",
    "end": "2760800"
  },
  {
    "text": "And make a function, the\nobjective function, f, it's x squared plus 10x--",
    "start": "2760800",
    "end": "2766650"
  },
  {
    "text": "x1 squared plus 10x2 squared. Here's the gradient. Here's the Hessian.",
    "start": "2766650",
    "end": "2773210"
  },
  {
    "text": "Here, I calculate h. Here's the gradient in h. Here's the Hessian in h.",
    "start": "2773210",
    "end": "2779810"
  },
  {
    "text": "I've got to define a new\nobjective function, phi, which is f minus mu log h.",
    "start": "2779810",
    "end": "2786470"
  },
  {
    "text": "This is the gradient in phi\nand this is the Hessian of phi. Oh, man, what a mess. But actually, not such\na mess, because the log",
    "start": "2786470",
    "end": "2793310"
  },
  {
    "text": "makes it really easy to\ntake these derivatives. So it's just a lot of\ndifferential sort of calculus",
    "start": "2793310",
    "end": "2800809"
  },
  {
    "text": "involved in working this out,\nbut this is the Hessian of phi. And then I need\nsome initial guess.",
    "start": "2800810",
    "end": "2806270"
  },
  {
    "text": "So I pick the\ncenter of my circle as an initial guess\nfor the solution. And I'm going to loop\nover values of mu that",
    "start": "2806270",
    "end": "2812570"
  },
  {
    "text": "get progressively smaller. I'll just go down\nto 10 to the minus 2 and stop for illustration\npurposes here.",
    "start": "2812570",
    "end": "2817719"
  },
  {
    "text": "But really, we should be\nchecking the solution as we go and deciding what values\nwe want to stop with.",
    "start": "2817719",
    "end": "2824360"
  },
  {
    "text": "And then this loop\nhere, what's this do? ",
    "start": "2824360",
    "end": "2829880"
  },
  {
    "text": "What's it do? Can you tell?",
    "start": "2829880",
    "end": "2835030"
  },
  {
    "text": "AUDIENCE: Is it Newton? JAMES SWAN: What's that? AUDIENCE: Newton? JAMES SWAN: Yeah, it's\nNewton-Raphson, right? x is x minus Hessian inverse\ntimes grad phi, right?",
    "start": "2835030",
    "end": "2845050"
  },
  {
    "text": "So I just do Newton-Raphson. I take my initial\nguess and I loop around with Newton-Raphson, and\nwhen this loop finishes,",
    "start": "2845050",
    "end": "2850630"
  },
  {
    "text": "I reduce mu, and it'll\njust use my previous guess as the initial guess for\nthe next value of the loop, until mu is sufficiently small.",
    "start": "2850630",
    "end": "2858187"
  },
  {
    "text": "OK? Interior point method. Here's what that\nsolution path looks like.",
    "start": "2858187",
    "end": "2863370"
  },
  {
    "text": "So mu started at 1, and\nthe barrier was here. It was close to the edge of the\ncircle, but not quite on it.",
    "start": "2863370",
    "end": "2869370"
  },
  {
    "text": "But as I reduced mu\nfurther and further and further, you\ncan see the path, the solution path,\nthat was followed",
    "start": "2869370",
    "end": "2874530"
  },
  {
    "text": "works its way closer to\nthe boundary of the circle. And the minimum is\nfound right here. So it turns out the\nminimum of this function",
    "start": "2874530",
    "end": "2880860"
  },
  {
    "text": "doesn't live in the\ndomain, it lives on the boundary of the domain. Recall that this point\nshould be a point where",
    "start": "2880860",
    "end": "2888830"
  },
  {
    "text": "the boundary of the\ndomain is parallel to the contours of the function,\nsince actually we didn't need",
    "start": "2888830",
    "end": "2895699"
  },
  {
    "text": "the inequality constraint here. We could have used the\nequality constraint. The equality constrained\nproblem has the same solution",
    "start": "2895699",
    "end": "2900839"
  },
  {
    "text": "as the inequality\nconstrained problem. And look, that\nactually happened. Here's the contours\nof the function. The contour of the function\nruns right along here,",
    "start": "2900840",
    "end": "2907760"
  },
  {
    "text": "and you can see\nit looks like it's going to be tangent to\nthe circle at this point. So the interpoint\nmethod actually solved",
    "start": "2907760",
    "end": "2914450"
  },
  {
    "text": "an equality constrained problem\nin addition to an inequality constrained problem, which is--\nthat's sort of cool that you",
    "start": "2914450",
    "end": "2920588"
  },
  {
    "text": "can do it that way.  How about if I want to do\na combination of equality",
    "start": "2920589",
    "end": "2926620"
  },
  {
    "text": "and inequality constraints? Then what do I do? ",
    "start": "2926620",
    "end": "2937285"
  },
  {
    "text": "Yeah. AUDIENCE: [INAUDIBLE]. JAMES SWAN: Perfect.",
    "start": "2937285",
    "end": "2943020"
  },
  {
    "text": "Convert the equality\nconstraint into unknowns, Lagrange multipliers, instead.",
    "start": "2943020",
    "end": "2949909"
  },
  {
    "text": "And then do the\ninterior point method on the Lagrange\nmultiplier problem. Now you've got a combination\nof equality and inequality",
    "start": "2949909",
    "end": "2955740"
  },
  {
    "text": "constrained. This is exactly\nwhat Matlab does. So it converts\nequality constraints",
    "start": "2955740",
    "end": "2960840"
  },
  {
    "text": "into Lagrange multipliers. Inequality constraints\nit actually solves using interior point methods.",
    "start": "2960840",
    "end": "2966630"
  },
  {
    "text": "Buried in that\ninterior point method is some form of Newton-Raphson\nand steepest descent combined",
    "start": "2966630",
    "end": "2972450"
  },
  {
    "text": "together, like dog\nleg we talked about for unconstrained problems. And it's going to\ndo a continuation.",
    "start": "2972450",
    "end": "2978120"
  },
  {
    "text": "As it reduces the\nvalues of mu, it'll have some heuristic\nfor how it does that. It's going to use its previous\nsolutions as initial guesses",
    "start": "2978120",
    "end": "2986010"
  },
  {
    "text": "for the next iteration. So these are very\ncomplicated problems, but if you understand how to\nsolve systems of nonlinear",
    "start": "2986010",
    "end": "2992310"
  },
  {
    "text": "equations, and you\nthink carefully about how to control numerical\nerror in your algorithm,",
    "start": "2992310",
    "end": "2997350"
  },
  {
    "text": "you come to a\nconclusion like this, that you can do these sorts of\nLagrange multiplier interior",
    "start": "2997350",
    "end": "3004100"
  },
  {
    "text": "point methods to solve a\nwide variety of problems with reasonable reliability.",
    "start": "3004100",
    "end": "3009820"
  },
  {
    "text": "OK? Any more questions? ",
    "start": "3009820",
    "end": "3015030"
  },
  {
    "text": "No? Good. Well, thank you, and\nwe'll see you on Friday.",
    "start": "3015030",
    "end": "3020829"
  },
  {
    "start": "3020830",
    "end": "3021913"
  }
]