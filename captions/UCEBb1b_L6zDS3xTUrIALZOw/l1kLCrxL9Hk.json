[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13339"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13340",
    "end": "21490"
  },
  {
    "text": "PROFESSOR: Today's topic\nis regression analysis. And this subject\nis one that we're",
    "start": "21490",
    "end": "28950"
  },
  {
    "text": "going to cover it today\ncovering the mathematical and statistical foundations\nof regression",
    "start": "28950",
    "end": "35620"
  },
  {
    "text": "and focus particularly\non linear regression. This methodology is perhaps\nthe most powerful method",
    "start": "35620",
    "end": "44840"
  },
  {
    "text": "in statistical modeling. And the foundations\nof it, I think, are very, very important\nto understand and master,",
    "start": "44840",
    "end": "52739"
  },
  {
    "text": "and they'll help you in any\nkind of statistical modeling exercise you might entertain\nduring or after this course.",
    "start": "52740",
    "end": "62870"
  },
  {
    "text": "And its popularity in\nfinance is very, very high, but it's also a very\npopular methodology",
    "start": "62870",
    "end": "70540"
  },
  {
    "text": "in all other disciplines\nthat do applied statistics.",
    "start": "70540",
    "end": "75630"
  },
  {
    "text": "So let's begin with setting up\nthe multiple linear regression",
    "start": "75630",
    "end": "83409"
  },
  {
    "text": "problem. So we begin with a data set that\nconsists of data observations",
    "start": "83410",
    "end": "91759"
  },
  {
    "text": "on different cases,\na number of cases. So we have n cases indexed by i.",
    "start": "91760",
    "end": "100110"
  },
  {
    "text": "And there's a single variable,\na dependent variable or response variable, which is\nthe variable of focus.",
    "start": "100110",
    "end": "107970"
  },
  {
    "text": "And we'll denote that y sub i. And together with that,\nfor each of the cases,",
    "start": "107970",
    "end": "115550"
  },
  {
    "text": "there are explanatory variables\nthat we might observe. So the y_i's, the\ndependent variables,",
    "start": "115550",
    "end": "124330"
  },
  {
    "text": "could be returns on stocks. The explanatory variables could\nbe underlying characteristics",
    "start": "124330",
    "end": "132160"
  },
  {
    "text": "of those stocks\nover a given period. The dependent variable\ncould be the change",
    "start": "132160",
    "end": "141090"
  },
  {
    "text": "in value of an index, the S&P\n500 index or the yield rate,",
    "start": "141090",
    "end": "148360"
  },
  {
    "text": "and the explanatory\nvariables can be various macroeconomic\nfactors or other factors that",
    "start": "148360",
    "end": "153390"
  },
  {
    "text": "might be used to explain how\nthe response variable changes",
    "start": "153390",
    "end": "159430"
  },
  {
    "text": "and takes on its value. Let's go through various\ngoals of regression analysis.",
    "start": "159430",
    "end": "164879"
  },
  {
    "text": "OK, first it can be\nto extract or exploit the relationship between\nthe dependent variable and the independent variable.",
    "start": "164880",
    "end": "171719"
  },
  {
    "text": "And examples of\nthis are prediction. Indeed, in finance\nthat's where I've",
    "start": "171720",
    "end": "177830"
  },
  {
    "text": "used regression analysis most. We want to predict what's going\nto happen and take actions",
    "start": "177830",
    "end": "182965"
  },
  {
    "text": "to take advantage of that. One can also use\nregression analysis to talk about causal inference.",
    "start": "182965",
    "end": "190940"
  },
  {
    "text": "What factors are really\ndriving a dependent variable?",
    "start": "190940",
    "end": "196240"
  },
  {
    "text": "And so one can actually\ntest hypotheses about what are true\ncausal factors underlying",
    "start": "196240",
    "end": "203930"
  },
  {
    "text": "the relationships\nbetween the variables. Another application is for\njust simple approximation.",
    "start": "203930",
    "end": "212430"
  },
  {
    "text": "As mathematicians,\nyou're all very familiar with how\nsmooth functions can",
    "start": "212430",
    "end": "218970"
  },
  {
    "text": "be-- that are smooth\nin the sense of being differentiable and bounded. Those can be approximated\nwell by a Taylor series",
    "start": "218970",
    "end": "226565"
  },
  {
    "text": "if you have a function of\na single variable or even a multivariable function.",
    "start": "226565",
    "end": "233240"
  },
  {
    "text": "So one can use\nregression analysis to actually approximate\nfunctions nicely.",
    "start": "233240",
    "end": "240459"
  },
  {
    "text": "And one can also use\nregression analysis to uncover functional\nrelationships",
    "start": "240460",
    "end": "248599"
  },
  {
    "text": "and validate functional\nrelationships amongst the variables. ",
    "start": "248600",
    "end": "254270"
  },
  {
    "text": "So let's set up the\ngeneral linear model from a mathematical\nstandpoint to begin with.",
    "start": "254270",
    "end": "259570"
  },
  {
    "text": "In this lecture, OK,\nwe're going to start off with discussing ordinary\nleast squares, which",
    "start": "259570",
    "end": "268050"
  },
  {
    "text": "is a purely mathematical\ncriterion for how you specify regression models.",
    "start": "268050",
    "end": "273580"
  },
  {
    "text": "And then we're going to turn to\nthe Gauss-Markov theorem which incorporates some statistical\nmodeling principles there.",
    "start": "273580",
    "end": "282265"
  },
  {
    "text": "They're essentially\nweak principles. And then we will\nturn to formal models",
    "start": "282265",
    "end": "289570"
  },
  {
    "text": "with normal linear\nregression models, and then consider extensions\nof those to broader classes.",
    "start": "289570",
    "end": "295700"
  },
  {
    "text": " Now we're in the\nmathematical context. And a linear model is\nbasically attempting",
    "start": "295700",
    "end": "305180"
  },
  {
    "text": "to model the conditional\ndistribution of the response variable y_i given the\nindependent variables x_i.",
    "start": "305180",
    "end": "314580"
  },
  {
    "text": "And the conditional distribution\nof the response variable",
    "start": "314580",
    "end": "321050"
  },
  {
    "text": "is modeled simply\nas a linear function of the independent variables.",
    "start": "321050",
    "end": "327229"
  },
  {
    "text": "So the x_i's, x_(i,1)\nthrough x_(i,p),",
    "start": "327230",
    "end": "332880"
  },
  {
    "text": "are the key explanatory\nvariables that relate to the response\nvariables, possibly.",
    "start": "332880",
    "end": "338340"
  },
  {
    "text": "And the beta_1, beta_2,\nbeta_i, or beta_p,",
    "start": "338340",
    "end": "345850"
  },
  {
    "text": "are the regression\nparameters which would be used in defining\nthat linear relationship.",
    "start": "345850",
    "end": "353190"
  },
  {
    "text": "So this relationship has\nresiduals, epsilon_i,",
    "start": "353190",
    "end": "363680"
  },
  {
    "text": "basically where there's\nuncertainty in the data-- whether it's either due to a\nmeasurement error or modeling",
    "start": "363680",
    "end": "371100"
  },
  {
    "text": "error or underlying\nstochastic processes that are driving the error. This epsilon_i is a\nresidual error variable",
    "start": "371100",
    "end": "379340"
  },
  {
    "text": "that will indicate how this\nlinear relationship varies",
    "start": "379340",
    "end": "384440"
  },
  {
    "text": "across the different n cases. ",
    "start": "384440",
    "end": "390590"
  },
  {
    "text": "So OK, how broad are the models? Well, the models\nreally are very broad.",
    "start": "390590",
    "end": "398080"
  },
  {
    "text": "First of all,\npolynomial approximation is indicated here. It corresponds, essentially,\nto a truncated Taylor",
    "start": "398080",
    "end": "405630"
  },
  {
    "text": "series approximation\nto a functional form. ",
    "start": "405630",
    "end": "411730"
  },
  {
    "text": "With variables that\nexhibit cyclical behavior, Fourier series can be applied\nin a linear regression context.",
    "start": "411730",
    "end": "423199"
  },
  {
    "text": "How many people in here are\nfamiliar with Fourier series? ",
    "start": "423200",
    "end": "428580"
  },
  {
    "text": "Almost everybody. So Fourier series\nbasically provide a set of basis functions\nthat allow you to closely",
    "start": "428580",
    "end": "437650"
  },
  {
    "text": "approximate most functions. And certainly with\nbounded functions that possibly have a\ncyclical structure to them,",
    "start": "437650",
    "end": "444510"
  },
  {
    "text": "it provides a\ncomplete description. So we could apply\nFourier series here.",
    "start": "444510",
    "end": "450940"
  },
  {
    "text": "Finally, time series regressions\nwhere the cases i one through n",
    "start": "450940",
    "end": "458640"
  },
  {
    "text": "are really indexes of different\ntime points can be applied. And so the independent\nvariables can",
    "start": "458640",
    "end": "466650"
  },
  {
    "text": "be variables that are\nobservable at a given time point or known at a given time.",
    "start": "466650",
    "end": "471910"
  },
  {
    "text": "So those can include lags\nof the response variables. So we'll see actually when\nwe talk about time series",
    "start": "471910",
    "end": "480530"
  },
  {
    "text": "that there's\nautoregressive time series models that can be specified.",
    "start": "480530",
    "end": "485870"
  },
  {
    "text": "And those are very broadly\napplied in finance.",
    "start": "485870",
    "end": "491410"
  },
  {
    "start": "491410",
    "end": "498900"
  },
  {
    "text": "All right, so let's go through\nwhat the steps are for fitting a regression model.",
    "start": "498900",
    "end": "504840"
  },
  {
    "text": " First, one wants\nto propose a model",
    "start": "504840",
    "end": "511759"
  },
  {
    "text": "in terms of what\nis it that we have to identify or be interested in\na particular response variable.",
    "start": "511760",
    "end": "518727"
  },
  {
    "text": "And critical here is\nspecifying the scale of that response variable.",
    "start": "518727",
    "end": "527010"
  },
  {
    "text": "Choongbum was discussing\nproblems of modeling stock prices.",
    "start": "527010",
    "end": "532450"
  },
  {
    "text": "If, say, y is the stock price? Well, it may be that it's\nmore appropriate to consider",
    "start": "532450",
    "end": "540090"
  },
  {
    "text": "modeling it on a logarithmic\nscale than on a linear scale.",
    "start": "540090",
    "end": "546320"
  },
  {
    "text": "Who can tell me why that\nwould be a good idea? AUDIENCE: Because\nthe changes might",
    "start": "546320",
    "end": "551740"
  },
  {
    "text": "become more percent\nchanges in price rather than absolute\nchanges in price. PROFESSOR: Very good, yeah.",
    "start": "551740",
    "end": "557490"
  },
  {
    "text": "So price changes basically\non the percentage scale, which log changes would be,\nmay be much better predicted",
    "start": "557490",
    "end": "565680"
  },
  {
    "text": "by knowing factors than\nthe absolute price level. OK, and so we have\nto have a collection",
    "start": "565680",
    "end": "575940"
  },
  {
    "text": "of independent variables,\nwhich to include in the model. And it's important\nto think about how",
    "start": "575940",
    "end": "582970"
  },
  {
    "text": "general this set up is. I mean, the\nindependent variables can be functions, lag values\nof the response variable.",
    "start": "582970",
    "end": "590140"
  },
  {
    "text": "They can be different\nfunctional forms of other independent variables. So the fact that we're talking\nabout a linear regression model",
    "start": "590140",
    "end": "598720"
  },
  {
    "text": "here is it's not so limiting\nin terms of the linearity. We can really capture\nlot of nonlinear behavior",
    "start": "598720",
    "end": "606050"
  },
  {
    "text": "in this framework. So then third, we need to\naddress the assumptions",
    "start": "606050",
    "end": "611290"
  },
  {
    "text": "about the distribution of\nthe residuals, epsilon, over the cases.",
    "start": "611290",
    "end": "616820"
  },
  {
    "text": "So that has to be specified. Once we've set up\nthe model in terms",
    "start": "616820",
    "end": "623610"
  },
  {
    "text": "of identifying the response\nof the explanatory variables and the assumptions\nunderlying the distribution",
    "start": "623610",
    "end": "629699"
  },
  {
    "text": "of the residuals, we need to\nspecify a criterion for judging",
    "start": "629700",
    "end": "635080"
  },
  {
    "text": "different estimators. So given a particular\nsetup, what we want to do",
    "start": "635080",
    "end": "640839"
  },
  {
    "text": "is be able to define a\nmethodology for specifying",
    "start": "640840",
    "end": "648000"
  },
  {
    "text": "the regression parameters\nso that we can then use this regression\nmodel for prediction",
    "start": "648000",
    "end": "654010"
  },
  {
    "text": "or whatever our purpose is. So the second\nthing we want to do",
    "start": "654010",
    "end": "660700"
  },
  {
    "text": "is define a criterion\nfor how we might judge different estimators of\nthe progression parameters.",
    "start": "660700",
    "end": "669090"
  },
  {
    "text": "We're going to go\nthrough several of those. And you'll see those-- least\nsquares is the first one,",
    "start": "669090",
    "end": "675200"
  },
  {
    "text": "but there are actually\nmore general ones. In fact, the last\nsection of this lecture on generalized estimators\nwill cover those as well.",
    "start": "675200",
    "end": "682930"
  },
  {
    "text": "Third, we need to characterize\nthe best estimator and apply it to the given data. So once we choose a\ncriterion for how good",
    "start": "682930",
    "end": "691401"
  },
  {
    "text": "an estimate of\nregression parameters is, then we have to have\na technology for solving",
    "start": "691401",
    "end": "697080"
  },
  {
    "text": "for that. And then fourth, we need\nto check our assumptions.",
    "start": "697080",
    "end": "703029"
  },
  {
    "text": "Now, it's very often the case\nthat at this fourth step, where",
    "start": "703030",
    "end": "708770"
  },
  {
    "text": "you're checking the\nassumptions that you've made, you'll discover features\nof your data or the process",
    "start": "708770",
    "end": "715370"
  },
  {
    "text": "that it's modeling\nthat make you want to expand upon your assumptions\nor change your assumptions.",
    "start": "715370",
    "end": "721519"
  },
  {
    "text": "And so checking the\nassumptions is a critical part of any modeling process.",
    "start": "721520",
    "end": "728170"
  },
  {
    "text": "And then if necessary, modify\nthe model and assumptions and repeat this process.",
    "start": "728170",
    "end": "735260"
  },
  {
    "text": "What I can tell you\nis that this sort of protocol for\nhow you fit models",
    "start": "735260",
    "end": "741279"
  },
  {
    "text": "is what I've applied\nmany, many times.",
    "start": "741280",
    "end": "746680"
  },
  {
    "text": "And if you are lucky in a\nparticular problem area,",
    "start": "746680",
    "end": "751870"
  },
  {
    "text": "the very simple\nmodels will work well with small changes\nin assumptions.",
    "start": "751870",
    "end": "757339"
  },
  {
    "text": "But when you get\nchallenging problems, then this item five\nof modify the model",
    "start": "757340",
    "end": "763900"
  },
  {
    "text": "and/or assumptions is critical. And in statistical\nmodeling, my philosophy",
    "start": "763900",
    "end": "770790"
  },
  {
    "text": "is you really want to,\nas much as possible, tailor the model to the\nprocess you're modeling. You don't want to fit a\nsquare peg in a round hole",
    "start": "770790",
    "end": "779070"
  },
  {
    "text": "and just apply, say,\nsimple linear regression to everything. You want to apply it when\nthe assumptions are valid.",
    "start": "779070",
    "end": "786191"
  },
  {
    "text": "If the assumptions\naren't valid, maybe you can change the\nspecification of the problem so a linear model is still\napplicable in a changed",
    "start": "786191",
    "end": "794890"
  },
  {
    "text": "framework. But if not, then\nyou'll want to extend to other kinds of models.",
    "start": "794890",
    "end": "800250"
  },
  {
    "text": "But what we'll be\ndoing-- or what you will be doing if you do\nthat-- is basically applying all the same principles\nthat are developed",
    "start": "800250",
    "end": "808120"
  },
  {
    "text": "in the linear\nmodeling framework. ",
    "start": "808120",
    "end": "815750"
  },
  {
    "text": "OK, now let's see. I wanted to make\nsome comments here about specifying assumptions\nfor the residual distribution.",
    "start": "815750",
    "end": "823975"
  },
  {
    "text": " What kind of assumptions\nmight we make?",
    "start": "823975",
    "end": "829620"
  },
  {
    "text": "OK, would anyone like to\nsuggest some assumptions you might make in\na linear regression",
    "start": "829620",
    "end": "835640"
  },
  {
    "text": "model for the residuals? Yes? What's your name, by the way? AUDIENCE: My name is Will.",
    "start": "835640",
    "end": "840699"
  },
  {
    "text": "PROFESSOR: Will, OK. Will what? [? AUDIENCE: Ossler. ?] PROFESSOR: [? Ossler, ?] great. OK, thank you, Will. AUDIENCE: It might\nbe-- or we might",
    "start": "840700",
    "end": "846480"
  },
  {
    "text": "want to say that the residual\nmight be normally distributed and it might not depend too\nmuch on what value of the input",
    "start": "846480",
    "end": "857598"
  },
  {
    "text": "variable we'd use. PROFESSOR: OK. Anyone else? ",
    "start": "857598",
    "end": "863810"
  },
  {
    "text": "OK. Well, that certainly\nis an excellent place to start in terms of starting\nwith a distribution that's",
    "start": "863810",
    "end": "873510"
  },
  {
    "text": "familiar. Familiar is always good. Although it's not something\nthat should be necessary,",
    "start": "873510",
    "end": "878598"
  },
  {
    "text": "but we know from some of\nChoongbum's lecture areas",
    "start": "878598",
    "end": "884210"
  },
  {
    "text": "that Gaussian and\nnormal distributions arise in many\nsettings where we're",
    "start": "884210",
    "end": "889670"
  },
  {
    "text": "taking basically sums of\nindependent, random variables. And so it may be that these\nresiduals are like that.",
    "start": "889670",
    "end": "898180"
  },
  {
    "text": "Anyway, a slightly simpler\nor weaker condition",
    "start": "898180",
    "end": "904910"
  },
  {
    "text": "is to use the Gauss-- what\nare called in statistics",
    "start": "904910",
    "end": "910110"
  },
  {
    "text": "the Gauss-Markov assumptions. And these are assumptions\nwhere we're only",
    "start": "910110",
    "end": "915340"
  },
  {
    "text": "concerned with the means\nor averages, statistically,",
    "start": "915340",
    "end": "920390"
  },
  {
    "text": "and the variances\nof the residuals. And so we assume that\nthere's zero mean.",
    "start": "920390",
    "end": "926160"
  },
  {
    "text": "So on average, they're not\nadding a bias up or down to the dependent variable.",
    "start": "926160",
    "end": "932579"
  },
  {
    "text": "And those have a\nconstant variance. So the level of\nuncertainty in our model",
    "start": "932580",
    "end": "940829"
  },
  {
    "text": "doesn't depend on the case. And so indeed, if errors\non the percentage scale",
    "start": "940830",
    "end": "947660"
  },
  {
    "text": "are more appropriate, then\none could look at, say, a time series of prices\nthat you're trying to model.",
    "start": "947660",
    "end": "953970"
  },
  {
    "text": "And it may be that\non the log scale, that constant variance\nlooks much more appropriate",
    "start": "953970",
    "end": "959160"
  },
  {
    "text": "than on the original\nscale, which would have-- And then a third attribute of\nthe Gauss-Markov assumptions",
    "start": "959160",
    "end": "966660"
  },
  {
    "text": "is that the residuals\nare uncorrelated. So now uncorrelated does\nnot mean independent",
    "start": "966660",
    "end": "976770"
  },
  {
    "text": "or statistically independent. So this is a somewhat weak\ncondition, or weaker condition, than independence\nof the residuals.",
    "start": "976770",
    "end": "984326"
  },
  {
    "text": "But in the Gauss-Markov\nsetting, we're just setting up\nbasically a reduced set",
    "start": "984326",
    "end": "989650"
  },
  {
    "text": "of assumptions that we might\napply to fit the model. If we extend upon\nthat, we can then",
    "start": "989650",
    "end": "996620"
  },
  {
    "text": "consider normal linear\nregression models, which Will just suggested.",
    "start": "996620",
    "end": "1004050"
  },
  {
    "text": "And in this case, those could\nbe assumed to be independent and identically\ndistributed-- IID",
    "start": "1004050",
    "end": "1010440"
  },
  {
    "text": "is that notation for that-- with\nGaussian or normal with mean 0",
    "start": "1010440",
    "end": "1016380"
  },
  {
    "text": "and variance sigma squared.  We can extend upon\nthat to consider",
    "start": "1016380",
    "end": "1023050"
  },
  {
    "text": "generalized Gauss-Markov\nassumptions where we maintain still the zero mean\nfor the residuals,",
    "start": "1023050",
    "end": "1029750"
  },
  {
    "text": "but the general-- we might\nhave a covariance matrix which",
    "start": "1029750",
    "end": "1035990"
  },
  {
    "text": "does not correspond to\nindependent and identically distributed random variables. Now, let's see.",
    "start": "1035990",
    "end": "1041760"
  },
  {
    "text": "In the discussion of\nprobability theory, we really haven't talked yet\nabout matrix-valued random",
    "start": "1041760",
    "end": "1049400"
  },
  {
    "text": "variables, right? But how many people\nin the class have covered matrix-value or\nvector-valued random variables",
    "start": "1049400",
    "end": "1057380"
  },
  {
    "text": "before? OK, just a handful. Well, a vector-valued\nrandom variable,",
    "start": "1057380",
    "end": "1067290"
  },
  {
    "text": "we think of the\nvalues of these n cases for the dependent variable\nto be an n-valued, an n-vector",
    "start": "1067290",
    "end": "1077200"
  },
  {
    "text": "of random variables. And so we can\ngeneralize the variance",
    "start": "1077200",
    "end": "1085600"
  },
  {
    "text": "of individual random variables\nto the variance covariance matrix of the collection.",
    "start": "1085600",
    "end": "1092870"
  },
  {
    "text": "And so you have a covariance\nmatrix characterizing",
    "start": "1092870",
    "end": "1098490"
  },
  {
    "text": "the variance of the n-vector\nwhich gives us the-- the (i, j)",
    "start": "1098490",
    "end": "1105929"
  },
  {
    "text": "element gives us the\nvalue of the covariance.",
    "start": "1105930",
    "end": "1111640"
  },
  {
    "text": "All right, let me put\nthe screen up and just",
    "start": "1111640",
    "end": "1118130"
  },
  {
    "text": "write that on the board so\nthat you're familiar with that. ",
    "start": "1118130",
    "end": "1129820"
  },
  {
    "text": "All right, so we have\ny_1, y_2, down to y_n,",
    "start": "1129820",
    "end": "1138269"
  },
  {
    "text": "our n values of our\nresponse variable. And we can basically talk\nabout the expectation",
    "start": "1138270",
    "end": "1150755"
  },
  {
    "text": "of that being equal to\nmu_1, mu_2, down to mu_n.",
    "start": "1150755",
    "end": "1158303"
  },
  {
    "text": " And the covariance matrix\nof y_1, y_2, down to y_n",
    "start": "1158303",
    "end": "1178270"
  },
  {
    "text": "is equal to a matrix\nwith the variance of y_1",
    "start": "1178270",
    "end": "1186060"
  },
  {
    "text": "in the upper 1, 1 element, and\nthe variance of y_2 in the 2,",
    "start": "1186060",
    "end": "1193280"
  },
  {
    "text": "2 element, and the variance of\ny_n in the nth column and nth",
    "start": "1193280",
    "end": "1204760"
  },
  {
    "text": "row. And in the (i,j)-th row, (i, j),\nwe have the covariance between",
    "start": "1204760",
    "end": "1215080"
  },
  {
    "text": "y_i and y_j. So we're going to use matrices\nto represent covariances.",
    "start": "1215080",
    "end": "1224070"
  },
  {
    "text": "And that's something\nwhich I want everyone to get very familiar\nwith because we're going to assume that we\nare comfortable with those,",
    "start": "1224070",
    "end": "1231750"
  },
  {
    "text": "and apply matrix algebra with\nthese kinds of constructs.",
    "start": "1231750",
    "end": "1238470"
  },
  {
    "text": "So the generalized\nGauss-Markov theorem assumes a general\ncovariance matrix",
    "start": "1238470",
    "end": "1243960"
  },
  {
    "text": "where you can have\nnonzero covariances",
    "start": "1243960",
    "end": "1250399"
  },
  {
    "text": "between the\nindependent variables or the dependent variables\nand the residuals. And those can be correlated.",
    "start": "1250400",
    "end": "1258450"
  },
  {
    "text": "Now, who can come\nup with an example of why the residuals might\nbe correlated in a regression",
    "start": "1258450",
    "end": "1271471"
  },
  {
    "text": "model?  Dan?",
    "start": "1271471",
    "end": "1276490"
  },
  {
    "text": "OK. That's a really good example\nbecause it's nonlinear. If you imagine sort of\na simple nonlinear curve",
    "start": "1276490",
    "end": "1284720"
  },
  {
    "text": "and you try to fit a\nstraight line to it, then the residuals\nfrom that linear fit",
    "start": "1284720",
    "end": "1292080"
  },
  {
    "text": "are going to be consistently\nabove or below the line depending on where you are\nin the nonlinearity, how",
    "start": "1292080",
    "end": "1297630"
  },
  {
    "text": "it might be fitting. So that's one example\nwhere that could arise. Any other possibilities?",
    "start": "1297630",
    "end": "1303415"
  },
  {
    "text": " Well, next week we'll be talking\nabout some time series models.",
    "start": "1303415",
    "end": "1310380"
  },
  {
    "text": "And there can be time\ndependence amongst variables where there are some\nunderlying factors maybe",
    "start": "1310380",
    "end": "1317170"
  },
  {
    "text": "that are driving the process. And those ongoing\nfactors can persist in making the\nlinear relationship",
    "start": "1317170",
    "end": "1325480"
  },
  {
    "text": "over or under gauge\nthe dependent variable.",
    "start": "1325480",
    "end": "1331299"
  },
  {
    "text": "So that can happen as well. All right, yes?",
    "start": "1331300",
    "end": "1337410"
  },
  {
    "text": "AUDIENCE: The Gauss-Markov\nis just the diagonal case? PROFESSOR: Yes, the Gauss-Markov\nis simply the diagonal case.",
    "start": "1337410",
    "end": "1342490"
  },
  {
    "text": "And explicitly if we replace\ny's here by the residuals, epsilon_1 through\nepsilon_n, then",
    "start": "1342490",
    "end": "1350779"
  },
  {
    "text": "that diagonal matrix\nwith a constant diagonal",
    "start": "1350780",
    "end": "1356930"
  },
  {
    "text": "is the simple Gauss-Markov\nassumption, yeah.",
    "start": "1356930",
    "end": "1361948"
  },
  {
    "text": " Now, I'm sure it\ncomes as no surprise",
    "start": "1361948",
    "end": "1368920"
  },
  {
    "text": "that Gaussian distributions\ndon't always fit everything. And so one needs to get\nclever with extending",
    "start": "1368920",
    "end": "1375350"
  },
  {
    "text": "the models to other cases.",
    "start": "1375350",
    "end": "1381559"
  },
  {
    "text": "And there are-- I know--\nLaplace distributions, Pareto",
    "start": "1381560",
    "end": "1387280"
  },
  {
    "text": "distributions, contaminated\nnormal distributions, which can be used to\nfit regression models.",
    "start": "1387280",
    "end": "1394000"
  },
  {
    "text": "And these general cases really\nextend the applicability",
    "start": "1394000",
    "end": "1402330"
  },
  {
    "text": "of regression models to\nmany interesting settings.",
    "start": "1402330",
    "end": "1408610"
  },
  {
    "text": "So let's turn to specifying\nthe estimator criterion in two.",
    "start": "1408610",
    "end": "1415530"
  },
  {
    "text": "So how do we judge what's a\ngood estimate of the regression parameters?",
    "start": "1415530",
    "end": "1420580"
  },
  {
    "text": "Well, we're going to cover least\nsquares, maximum likelihood, robust methods, which are\ncontamination resistant.",
    "start": "1420580",
    "end": "1430990"
  },
  {
    "text": "And other methods exist\nthat we will mention but not",
    "start": "1430990",
    "end": "1436370"
  },
  {
    "text": "get into really in\nthe lectures, are Bayes methods and accommodating\nincomplete or missing data.",
    "start": "1436370",
    "end": "1443390"
  },
  {
    "text": "Essentially, as your approach\nto modeling a problem gets more and more\nrealistic, you",
    "start": "1443390",
    "end": "1450120"
  },
  {
    "text": "start adding more and more\ncomplexity as it's needed. And certainly issues\nof-- well, robust methods",
    "start": "1450120",
    "end": "1464450"
  },
  {
    "text": "is where you assume\nmost of the data arrives under normal\nconditions, but once in a while there may be some\nproblem with the data.",
    "start": "1464450",
    "end": "1471710"
  },
  {
    "text": "And you don't want\nyour methodology just to break down if there happens\nto be some outliers in the data",
    "start": "1471710",
    "end": "1479635"
  },
  {
    "text": "or contamination. Bayes methodologies\nare the technology",
    "start": "1479635",
    "end": "1487200"
  },
  {
    "text": "for incorporating\nsubjective beliefs into statistical models.",
    "start": "1487200",
    "end": "1494310"
  },
  {
    "text": "And I think it's fair\nto say that probably all statistical modeling\nis essentially subjective.",
    "start": "1494310",
    "end": "1501720"
  },
  {
    "text": "And so if you're going to be\ngood at statistical modeling, you want to be sure that you're\neffectively incorporating",
    "start": "1501720",
    "end": "1508778"
  },
  {
    "text": "subjective information in that. And so Bayes methodologies\nare very, very useful, and indeed pretty much\nrequired to engage",
    "start": "1508779",
    "end": "1516460"
  },
  {
    "text": "in appropriate modeling. And then finally, accommodate\nincomplete or missing data.",
    "start": "1516460",
    "end": "1522700"
  },
  {
    "text": "The world is always sort\nof cruel in terms of you often are missing what you\nthink is critical information",
    "start": "1522700",
    "end": "1530900"
  },
  {
    "text": "to do your analysis. And so how do you\ndeal with situations where you have some\nholes in your data?",
    "start": "1530900",
    "end": "1539680"
  },
  {
    "text": "Statistical models provide\ngood methods and tools",
    "start": "1539680",
    "end": "1544720"
  },
  {
    "text": "for dealing with that situation.  OK. Then let's see.",
    "start": "1544720",
    "end": "1550960"
  },
  {
    "text": "In case analyses for\nchecking assumptions, let me go through this.",
    "start": "1550960",
    "end": "1557540"
  },
  {
    "text": " Basically when you fit\na regression model,",
    "start": "1557540",
    "end": "1563790"
  },
  {
    "text": "you check assumptions by\nlooking at the residuals, which are the basically estimates of\nthe epsilons, the deviations",
    "start": "1563790",
    "end": "1576330"
  },
  {
    "text": "of the dependent variable\nfrom their predictions.",
    "start": "1576330",
    "end": "1582359"
  },
  {
    "text": "And what one wants\nto do is analyze these to determine whether our\nassumptions are appropriate.",
    "start": "1582359",
    "end": "1589660"
  },
  {
    "text": "OK, but the Gauss-Markov\nassumptions would be, do these appear to\nhave constant variance?",
    "start": "1589660",
    "end": "1596169"
  },
  {
    "text": "And it may be that their\nvariance depends on time, if the i is indexing time.",
    "start": "1596170",
    "end": "1602555"
  },
  {
    "text": " Residuals might depend on\nthe other variables as well,",
    "start": "1602555",
    "end": "1608190"
  },
  {
    "text": "and one wants to determine\nthat that isn't the case.",
    "start": "1608190",
    "end": "1613610"
  },
  {
    "text": "There are also influence\ndiagnostics identifying cases which are highly influential.",
    "start": "1613610",
    "end": "1620330"
  },
  {
    "text": "It turns out that when you\nare building a regression",
    "start": "1620330",
    "end": "1625620"
  },
  {
    "text": "model with data, you\ntreat all the cases as if they're equally important.",
    "start": "1625620",
    "end": "1632830"
  },
  {
    "text": "Well, it may be\nthat certain cases are really critical to\nestimated certain factors.",
    "start": "1632830",
    "end": "1639540"
  },
  {
    "text": "And it may be that much of the\ninference about how important",
    "start": "1639540",
    "end": "1645650"
  },
  {
    "text": "a certain factor\nis is determined by very small number of points. So even though you\nhave a massive data set",
    "start": "1645650",
    "end": "1652875"
  },
  {
    "text": "that you're using\nto fit a model, it could be that\nsome of the structure is driven by a very\nsmall number of cases.",
    "start": "1652875",
    "end": "1659600"
  },
  {
    "text": "So influence diagnostics give\nyou a way of analyzing that.",
    "start": "1659600",
    "end": "1665100"
  },
  {
    "text": "In the problem set\nfor this lecture,",
    "start": "1665100",
    "end": "1670730"
  },
  {
    "text": "you'll be deriving some\ninfluence diagnostics for linear regression\nmodels and seeing how they're mathematically defined.",
    "start": "1670730",
    "end": "1677790"
  },
  {
    "text": "And I'll be distributing\na case study which illustrates fitting\nlinear regression",
    "start": "1677790",
    "end": "1684290"
  },
  {
    "text": "models for asset prices. And you can see\nhow those play out",
    "start": "1684290",
    "end": "1690502"
  },
  {
    "text": "with some practical examples. ",
    "start": "1690502",
    "end": "1696169"
  },
  {
    "text": "OK, finally there's\noutlier detection. ",
    "start": "1696170",
    "end": "1701210"
  },
  {
    "text": "With outliers, it's interesting. The exceptions in data are\noften the most interesting.",
    "start": "1701210",
    "end": "1713100"
  },
  {
    "text": "It's important in\nmodeling to understand whether certain\ncases are unusual.",
    "start": "1713100",
    "end": "1720700"
  },
  {
    "text": "And sometimes their\ndegree of idiosyncrasy",
    "start": "1720700",
    "end": "1727600"
  },
  {
    "text": "can be explained away\nso that one essentially discards those outliers. But other times,\nthose idiosyncrasies",
    "start": "1727600",
    "end": "1734789"
  },
  {
    "text": "lead to extensions of the model. And so outlier detection can be\nvery important for validating",
    "start": "1734790",
    "end": "1744345"
  },
  {
    "text": "a model. OK, so with that introduction to\nregression, linear regression,",
    "start": "1744345",
    "end": "1750970"
  },
  {
    "text": "let's talk about\nordinary least squares. Ah. ",
    "start": "1750970",
    "end": "1759075"
  },
  {
    "text": "OK, the least squares criterion\nis for a given a regression",
    "start": "1759075",
    "end": "1764360"
  },
  {
    "text": "parameter, beta,\nwhich is considered to be a column vector-- so I'm\ntaking the transpose of a row",
    "start": "1764360",
    "end": "1772160"
  },
  {
    "text": "vector.  The least squares criterion\nis to basically take",
    "start": "1772160",
    "end": "1779610"
  },
  {
    "text": "the sum of square deviations\nfrom the actual value of the response variable\nfrom its linear prediction.",
    "start": "1779610",
    "end": "1786160"
  },
  {
    "text": "So y_i minus y hat i,\nwe're just plugging in for y hat i the\nlinear function of the independent variables\nand the squaring that.",
    "start": "1786160",
    "end": "1795470"
  },
  {
    "text": " And the ordinary least\nsquares estimate, beta hat,",
    "start": "1795470",
    "end": "1801570"
  },
  {
    "text": "minimizes this function. So in order to solve for this,\nwe're going to use matrices.",
    "start": "1801570",
    "end": "1811520"
  },
  {
    "text": "And so we're going to take\nthe y vector, the vector of n",
    "start": "1811520",
    "end": "1816627"
  },
  {
    "text": "values of the\ndependent variable, or the response variable,\nand X, the matrix",
    "start": "1816627",
    "end": "1821710"
  },
  {
    "text": "of values of the\nindependent variable. It's important in this\nset up to keep straight",
    "start": "1821710",
    "end": "1828799"
  },
  {
    "text": "that cases go by\nrows and columns go by values of the\nindependent variable.",
    "start": "1828800",
    "end": "1835585"
  },
  {
    "start": "1835585",
    "end": "1841409"
  },
  {
    "text": "Boy, this thing is\nultra sensitive. Excuse me. ",
    "start": "1841410",
    "end": "1849316"
  },
  {
    "text": "Do I turn off the touchpad here? OK. So we can now define\nour fitted value, y hat,",
    "start": "1849317",
    "end": "1861990"
  },
  {
    "text": "to be equal to the\nmatrix x times beta. And with matrix multiplication,\nthat results in the y hat 1",
    "start": "1861990",
    "end": "1870940"
  },
  {
    "text": "through y hat n. And Q of beta can basically\nbe written as y minus X beta",
    "start": "1870940",
    "end": "1880799"
  },
  {
    "text": "transpose y minus X beta. So this term here is an\nn-vector minus the product",
    "start": "1880800",
    "end": "1887580"
  },
  {
    "text": "of the X matrix times beta,\nwhich is another n-vector. And we're just taking the\ncross product of that.",
    "start": "1887580",
    "end": "1893380"
  },
  {
    "text": " And the ordinary least\nsquares estimate for beta",
    "start": "1893380",
    "end": "1901460"
  },
  {
    "text": "solves the derivative of\nthis criterion equaling 0.",
    "start": "1901460",
    "end": "1907649"
  },
  {
    "text": "Now, that's in\nfact true, but who",
    "start": "1907650",
    "end": "1913990"
  },
  {
    "text": "can tell me why that's true? ",
    "start": "1913990",
    "end": "1920360"
  },
  {
    "text": "Say again? AUDIENCE: Is that minimum? PROFESSOR: OK. So your name? AUDIENCE: Seth.",
    "start": "1920360",
    "end": "1925610"
  },
  {
    "text": "PROFESSOR: Seth? Seth. Very good, Seth. Thanks, Seth. So if we want to\nfind a minimum of Q,",
    "start": "1925610",
    "end": "1933200"
  },
  {
    "text": "then that minimum will have,\nif it's a smooth function, will have a minimum\nat slope equals 0.",
    "start": "1933200",
    "end": "1940950"
  },
  {
    "text": "Now, how do we know whether\nit's a minimum or not? It could be a maximum. AUDIENCE: [INAUDIBLE]?",
    "start": "1940950",
    "end": "1946340"
  },
  {
    "text": " PROFESSOR: OK, right. So in fact, this\nis a-- Q of beta",
    "start": "1946340",
    "end": "1954050"
  },
  {
    "text": "is a convex function of beta.",
    "start": "1954050",
    "end": "1959160"
  },
  {
    "text": "And so its second\nderivative is positive.",
    "start": "1959160",
    "end": "1964770"
  },
  {
    "text": "And if you basically think\nabout the set-- basically,",
    "start": "1964770",
    "end": "1972140"
  },
  {
    "text": "this is the first\nderivative of Q with respect to beta equaling 0. If you were to solve for\nthe second derivative of Q",
    "start": "1972140",
    "end": "1978420"
  },
  {
    "text": "with respect to beta,\nwell, beta is a p-vector. So the second\nderivative is actually",
    "start": "1978420",
    "end": "1984560"
  },
  {
    "text": "a second derivative\nmatrix, and that matrix,",
    "start": "1984560",
    "end": "1991240"
  },
  {
    "text": "you can solve for it. It will be X\ntranspose X, which is a positive definite or\nsemi-definite matrix.",
    "start": "1991240",
    "end": "1998730"
  },
  {
    "text": "So it basically had a\npositive derivative there.",
    "start": "1998730",
    "end": "2003950"
  },
  {
    "text": "So anyway, this ordinary\nleast squares estimates will solve this d Q of\nbeta by d beta equals 0.",
    "start": "2003950",
    "end": "2012370"
  },
  {
    "text": "What does d Q beta by d beta_j? Well, you just take the\nderivative of this sum.",
    "start": "2012370",
    "end": "2020980"
  },
  {
    "text": " So we're taking the sum\nof all these elements.",
    "start": "2020980",
    "end": "2027390"
  },
  {
    "text": " And if you take the\nderivative-- well,",
    "start": "2027390",
    "end": "2033640"
  },
  {
    "text": "OK, the derivative\nis a linear operator. So the derivative of a sum is\nthe sum of the derivatives.",
    "start": "2033640",
    "end": "2040450"
  },
  {
    "text": "So we take the summation out and\nwe take the derivative of each term, so we get 2 minus x_(i,j),\nthen the thing in square",
    "start": "2040450",
    "end": "2049929"
  },
  {
    "text": "brackets, y_i minus that. ",
    "start": "2049929",
    "end": "2055239"
  },
  {
    "text": "And what is that? Well, in matrix\nnotation, if we let this sort of bold X\nsub square j denote",
    "start": "2055239",
    "end": "2062469"
  },
  {
    "text": "the j-th column of the\nindependent variables, then this is minus 2.",
    "start": "2062469",
    "end": "2068219"
  },
  {
    "text": "Basically, the j-th column of X\ntranspose times y minus X beta.",
    "start": "2068219",
    "end": "2075350"
  },
  {
    "text": "So this j-th equation for\nordinary least squares",
    "start": "2075350",
    "end": "2082940"
  },
  {
    "text": "has that representation in\nterms-- in matrix notation. Now if we put that all\ntogether, we basically",
    "start": "2082940",
    "end": "2091859"
  },
  {
    "text": "can define this derivative\nof Q with respect to the different\nregression parameters",
    "start": "2091860",
    "end": "2097740"
  },
  {
    "text": "as basically the minus twice\nthe j-th column stacked times y",
    "start": "2097740",
    "end": "2105640"
  },
  {
    "text": "minus X beta, which is simply\nminus 2 X transpose, y minus X beta.",
    "start": "2105640",
    "end": "2111040"
  },
  {
    "text": "And this has to equal 0. And if we just simplify,\ntaking out the two,",
    "start": "2111040",
    "end": "2119550"
  },
  {
    "text": "we get this set of equations. It must be satisfied by\nthe ordinary least squares",
    "start": "2119550",
    "end": "2125559"
  },
  {
    "text": "estimate, beta. And that's called the\nnormal equations in books",
    "start": "2125560",
    "end": "2131960"
  },
  {
    "text": "on regression modeling. So let's consider\nhow we solve that.",
    "start": "2131960",
    "end": "2137310"
  },
  {
    "text": "Well, we can re-express that\nby multiplying through the X",
    "start": "2137310",
    "end": "2143100"
  },
  {
    "text": "transpose on each of the terms. And then beta hat basically\nsolves this equation.",
    "start": "2143100",
    "end": "2154770"
  },
  {
    "text": "And if X transpose\nX inverse exists, we get beta hat is equal\nto X transpose X inverse X",
    "start": "2154770",
    "end": "2162385"
  },
  {
    "text": "transpose y. So with matrix algebra, we\ncan actually solve this.",
    "start": "2162385",
    "end": "2171030"
  },
  {
    "text": "And matrix algebra\nis going to be very important to this\nlecture and other lectures.",
    "start": "2171030",
    "end": "2176910"
  },
  {
    "text": "So if this stuff is-- if\nyou're a bit rusty on this, do brush up.",
    "start": "2176910",
    "end": "2182210"
  },
  {
    "text": " This particular\nsolution for beta hat",
    "start": "2182210",
    "end": "2189610"
  },
  {
    "text": "assumes that X transpose\nX inverse exists.",
    "start": "2189610",
    "end": "2197980"
  },
  {
    "text": "Who can tell me\nwhat assumptions do we need to make for X\ntranspose X to have an inverse?",
    "start": "2197980",
    "end": "2207290"
  },
  {
    "text": " I'll call you in a second\nif no one else does.",
    "start": "2207290",
    "end": "2215910"
  },
  {
    "text": "Somebody just said something. Someone else.",
    "start": "2215910",
    "end": "2221370"
  },
  {
    "text": "No? All right. OK, Will. AUDIENCE: So X\ntranspose X inverse needs to have full\nrank, which means that each of the submatrices\nneeds to have [INAUDIBLE]",
    "start": "2221370",
    "end": "2230268"
  },
  {
    "text": "smaller dimension. PROFESSOR: OK, so Will said,\nbasically, the matrix X",
    "start": "2230269",
    "end": "2235305"
  },
  {
    "text": "needs to have full rank. And so if X has full rank,\nthen-- well, let's see.",
    "start": "2235305",
    "end": "2243990"
  },
  {
    "text": "If X has full rank, then the\nsingular value decomposition",
    "start": "2243990",
    "end": "2249340"
  },
  {
    "text": "which was in the very\nfirst class can exist.",
    "start": "2249340",
    "end": "2255990"
  },
  {
    "text": "And you have basically\np singular values that are all non-zero.",
    "start": "2255990",
    "end": "2262730"
  },
  {
    "text": "And X transpose X\ncan be expressed as sort of a, from the\nsingular value decomposition,",
    "start": "2262730",
    "end": "2270610"
  },
  {
    "text": "as one of the orthogonal\nmatrices times the square of the singular values times\nthat same matrix transpose,",
    "start": "2270610",
    "end": "2277145"
  },
  {
    "text": "if you recall that definition. So that actually\nis-- it basically",
    "start": "2277145",
    "end": "2282420"
  },
  {
    "text": "provides a solution for X\ntranspose X inverse, indeed, from the singular value\ndecomposition of X.",
    "start": "2282420",
    "end": "2288810"
  },
  {
    "text": "But what's required is that\nyou have a full rank in X. And what that means\nis that you can't have",
    "start": "2288810",
    "end": "2294450"
  },
  {
    "text": "independent variables\nthat are explained",
    "start": "2294450",
    "end": "2300119"
  },
  {
    "text": "by other independent variables. So different columns of\nX have to be linear--",
    "start": "2300120",
    "end": "2309010"
  },
  {
    "text": "or they can't linearly depend\non any other columns of X. Otherwise, you would\nhave reduced rank.",
    "start": "2309010",
    "end": "2314670"
  },
  {
    "text": " So now if beta hat\ndoesn't have full rank,",
    "start": "2314670",
    "end": "2324570"
  },
  {
    "text": "then our least squares estimate\nof beta might be non-unique. And in fact, it is\nthe case that if you",
    "start": "2324570",
    "end": "2333279"
  },
  {
    "text": "are really interested\nin just predicting values of a dependent\nvariable, then",
    "start": "2333280",
    "end": "2339180"
  },
  {
    "text": "having non-unique\nleast squares estimates isn't as much of a\nproblem, because you still",
    "start": "2339180",
    "end": "2345530"
  },
  {
    "text": "get estimates out of that. But for now, we want to assume\nthat there's full column rank",
    "start": "2345530",
    "end": "2351302"
  },
  {
    "text": "in the independent variables.  All right.",
    "start": "2351302",
    "end": "2357510"
  },
  {
    "text": "Now, if we plug in the value\nof the solution for the least",
    "start": "2357510",
    "end": "2370100"
  },
  {
    "text": "squares estimate,\nwe get fitted values for the response variable, which\nare simply the matrix X times",
    "start": "2370100",
    "end": "2381960"
  },
  {
    "text": "beta hat. And this expression\nfor the fitted values",
    "start": "2381960",
    "end": "2392070"
  },
  {
    "text": "is basically X times X transpose\nX inverse X transpose y,",
    "start": "2392070",
    "end": "2398570"
  },
  {
    "text": "which we can represent as Hy. Basically, this H matrix in\nlinear models and statistics",
    "start": "2398570",
    "end": "2408160"
  },
  {
    "text": "is called the hat matrix. It's basically a\nprojection matrix that takes the linear vector,\nor the vector of values",
    "start": "2408160",
    "end": "2419120"
  },
  {
    "text": "of the response variable,\ninto the fitted values.",
    "start": "2419120",
    "end": "2424290"
  },
  {
    "text": "So this hat matrix\nis quite important.",
    "start": "2424290",
    "end": "2430713"
  },
  {
    "text": " The problem set's going\nto cover some features,",
    "start": "2430713",
    "end": "2437744"
  },
  {
    "text": "go into some properties\nof the hat matrix. ",
    "start": "2437745",
    "end": "2442790"
  },
  {
    "text": "Does anyone want to make any\ncomments about this hat matrix?",
    "start": "2442790",
    "end": "2449180"
  },
  {
    "text": "It's actually a very\nspecial type of matrix. Does anyone want to point out\nwhat that special type is?",
    "start": "2449180",
    "end": "2456230"
  },
  {
    "text": " It's a projection matrix, OK.",
    "start": "2456230",
    "end": "2462811"
  },
  {
    "text": "Yeah. And in linear algebra,\nprojection matrices",
    "start": "2462811",
    "end": "2468490"
  },
  {
    "text": "have some very\nspecial properties. And it's actually an\northogonal projection matrix.",
    "start": "2468490",
    "end": "2477400"
  },
  {
    "text": "And so if you're\ninterested in that feature,",
    "start": "2477400",
    "end": "2484029"
  },
  {
    "text": "you should look into that. But it's really a very rich\nset of properties associated",
    "start": "2484030",
    "end": "2490635"
  },
  {
    "text": "with this hat matrix. It's an orthogonal projection,\nand it's-- let's see.",
    "start": "2490635",
    "end": "2496970"
  },
  {
    "text": "What's it projecting? It's projecting from\nn-space into what? ",
    "start": "2496970",
    "end": "2504570"
  },
  {
    "text": "Go ahead. What's your name? AUDIENCE: Ethan. PROFESSOR: Ethan, OK. AUDIENCE: Into space [INAUDIBLE] ",
    "start": "2504570",
    "end": "2511250"
  },
  {
    "text": "PROFESSOR: Basically, yeah. It's projecting into\nthe column space of X. So that's what linear\nregression is doing.",
    "start": "2511250",
    "end": "2520730"
  },
  {
    "text": "So in focusing and\nunderstanding linear regression, you can think of, how do we\nget estimates of this p-vector?",
    "start": "2520730",
    "end": "2528619"
  },
  {
    "text": "That's all very good and useful,\nand we'll do a lot of that. But you can also\nthink of it as, what's",
    "start": "2528620",
    "end": "2533880"
  },
  {
    "text": "happening in the\nn-dimensional space? So you basically\nare representing this n-dimensional vector\ny by its projection",
    "start": "2533880",
    "end": "2541960"
  },
  {
    "text": "onto the column space. ",
    "start": "2541960",
    "end": "2549730"
  },
  {
    "text": "Now, the residuals are\nbasically the difference between the response value\nand the fitted value.",
    "start": "2549730",
    "end": "2558319"
  },
  {
    "text": "And this can be expressed\nas y minus y hat,",
    "start": "2558320",
    "end": "2563960"
  },
  {
    "text": "or I_n minus H times y. And it turns out that I_n minus\nH is also a projection matrix,",
    "start": "2563960",
    "end": "2578700"
  },
  {
    "text": "and it's projecting the data\nonto the space orthogonal",
    "start": "2578700",
    "end": "2583950"
  },
  {
    "text": "to the column space of x. And to show that that's\ntrue, if we consider",
    "start": "2583950",
    "end": "2591980"
  },
  {
    "text": "the normal equations, which\nare X transpose y minus X beta hat equaling 0, that basically\nis X transpose epsilon hat",
    "start": "2591980",
    "end": "2600890"
  },
  {
    "text": "equals 0. And so from the\nnormal equations, we can see that\nwhat they mean is",
    "start": "2600890",
    "end": "2607609"
  },
  {
    "text": "they mean that the residual\nvector epsilon hat is",
    "start": "2607610",
    "end": "2613380"
  },
  {
    "text": "orthogonal to each\nof the columns of X. You can take any column\nin X, multiply that",
    "start": "2613380",
    "end": "2618570"
  },
  {
    "text": "by the residual vector,\nand get 0 coming out. So that's a feature\nof the residuals",
    "start": "2618570",
    "end": "2627760"
  },
  {
    "text": "as they relate to the\nindependent variables. OK, all right.",
    "start": "2627760",
    "end": "2633940"
  },
  {
    "text": "So at this point, we've gone\nthrough really not talking",
    "start": "2633940",
    "end": "2640520"
  },
  {
    "text": "about any statistical\nproperties to specify the betas. All we've done is talked-- we've\nintroduced the least squares",
    "start": "2640520",
    "end": "2646210"
  },
  {
    "text": "criterion and said, what\nvalue of the beta vector minimizes that least\nsquares criterion?",
    "start": "2646210",
    "end": "2653260"
  },
  {
    "text": "Let's turn to the\nGauss-Markov theorem and start introducing some\nstatistical properties,",
    "start": "2653260",
    "end": "2662240"
  },
  {
    "text": "probability properties. So with our data, y and X-- yes?",
    "start": "2662240",
    "end": "2668944"
  },
  {
    "text": "Yes. AUDIENCE: [INAUDIBLE]? ",
    "start": "2668945",
    "end": "2676110"
  },
  {
    "text": "PROFESSOR: That epsilon-- AUDIENCE: [INAUDIBLE]? ",
    "start": "2676110",
    "end": "2681480"
  },
  {
    "text": "PROFESSOR: OK. Let me go back to that. ",
    "start": "2681480",
    "end": "2687830"
  },
  {
    "text": "It's that X, the columns\nof X, and the column",
    "start": "2687830",
    "end": "2693500"
  },
  {
    "text": "vector of the residual are\northogonal to each other.",
    "start": "2693500",
    "end": "2699660"
  },
  {
    "text": "So we're not doing a\nprojection onto a null space. This is just a statement that\nthose values, or those column",
    "start": "2699660",
    "end": "2711440"
  },
  {
    "text": "vectors, are orthogonal\nto each other. And just to recap, the\nepsilon is a projection of y",
    "start": "2711440",
    "end": "2722010"
  },
  {
    "text": "onto the space orthogonal\nto the column space.",
    "start": "2722010",
    "end": "2727060"
  },
  {
    "text": "And y hat is a projection\nonto the column space of y.",
    "start": "2727060",
    "end": "2732788"
  },
  {
    "text": "And these projections are\nall orthogonal projections, and so they happen to result in\nthe projected value epsilon hat",
    "start": "2732788",
    "end": "2745740"
  },
  {
    "text": "must be orthogonal to\nthe column space of X, if you project it out.",
    "start": "2745740",
    "end": "2753080"
  },
  {
    "text": "OK? All right. So the Gauss-Markov theorem,\nwe have data y and X again.",
    "start": "2753080",
    "end": "2762240"
  },
  {
    "text": "And now we're going to\nthink of the observed data, little y_1 through\ny_n, is actually",
    "start": "2762240",
    "end": "2768640"
  },
  {
    "text": "an observation of the\nrandom vector capital Y, composed of random\nvariables Y_1 up to Y_n.",
    "start": "2768640",
    "end": "2779610"
  },
  {
    "text": "And the expectation\nof this vector",
    "start": "2779610",
    "end": "2785120"
  },
  {
    "text": "conditional on the values\nof the independent variables and their regression\nparameters given by X,",
    "start": "2785120",
    "end": "2790680"
  },
  {
    "text": "beta-- so the dependent\nvariable vector",
    "start": "2790680",
    "end": "2796490"
  },
  {
    "text": "has expectation\ngiven by the product of the independent variables\nmatrix times the regression",
    "start": "2796490",
    "end": "2803339"
  },
  {
    "text": "parameters. And the covariance matrix\nof Y given X and beta",
    "start": "2803340",
    "end": "2808425"
  },
  {
    "text": "is sigma squared\ntimes the identity matrix, the n-dimensional\nidentity matrix.",
    "start": "2808425",
    "end": "2814329"
  },
  {
    "text": "So the identity matrix has\n1's along the diagonal, n-dimensional, and\n0's off the diagonal.",
    "start": "2814330",
    "end": "2820480"
  },
  {
    "text": "So the variances of the Y's\nare the diagonal entries,",
    "start": "2820480",
    "end": "2826980"
  },
  {
    "text": "those are all the\nsame, sigma squared. And the covariance between\nany two are equal to 0",
    "start": "2826980",
    "end": "2833323"
  },
  {
    "text": "conditionally. ",
    "start": "2833323",
    "end": "2841530"
  },
  {
    "text": "OK, now the\nGauss-Markov theorem. This is a terrific result\nin linear models theory.",
    "start": "2841530",
    "end": "2851790"
  },
  {
    "text": "And it's terrific in terms of\nthe mathematical content of it.",
    "start": "2851790",
    "end": "2857790"
  },
  {
    "text": "I think it's-- for a math class,\nit's really a nice theorem",
    "start": "2857790",
    "end": "2863850"
  },
  {
    "text": "to introduce you to and\nhighlight the power of, I",
    "start": "2863850",
    "end": "2871110"
  },
  {
    "text": "guess, results that can arise\nfrom applying the theory. And so to set this\ntheorem up, we",
    "start": "2871110",
    "end": "2880120"
  },
  {
    "text": "want to think about trying\nto estimate some function",
    "start": "2880120",
    "end": "2885710"
  },
  {
    "text": "of the regression parameters. And so OK, our problem is\nwith ordinary least squares--",
    "start": "2885710",
    "end": "2894060"
  },
  {
    "text": "it was, how do we specify\nthe regression parameters beta_1 through beta_p? Let's consider a general\ntarget of interest,",
    "start": "2894060",
    "end": "2903510"
  },
  {
    "text": "which is a linear\ncombination of the betas. So we want to predict\na parameter theta which",
    "start": "2903510",
    "end": "2912089"
  },
  {
    "text": "is some linear combination\nof the regression parameters.",
    "start": "2912090",
    "end": "2917530"
  },
  {
    "text": "And because that linear\ncombination of the regression parameters corresponds to the\nexpectation of the response",
    "start": "2917530",
    "end": "2929920"
  },
  {
    "text": "variable corresponding\nto a given row of the independent\nvariables matrix, this is just a\ngeneralization of trying",
    "start": "2929920",
    "end": "2935690"
  },
  {
    "text": "to estimate the means\nof the regression model at different points\nin the space,",
    "start": "2935690",
    "end": "2941410"
  },
  {
    "text": "or to be estimating other\nquantities that might arise.",
    "start": "2941410",
    "end": "2946720"
  },
  {
    "text": "So this is really a very\ngeneral kind of thing to want to estimate. It certainly is appropriate\nfor predictions.",
    "start": "2946720",
    "end": "2953560"
  },
  {
    "text": "And if we consider the\nleast squares estimate by just plugging in beta hat\none through beta hat p, solved",
    "start": "2953560",
    "end": "2962760"
  },
  {
    "text": "by the least squares,\nwell, it turns out",
    "start": "2962760",
    "end": "2968990"
  },
  {
    "text": "that those are an unbiased\nestimator of the parameter",
    "start": "2968990",
    "end": "2976900"
  },
  {
    "text": "theta. So if we're trying to\nestimate this combination of these unknown parameters,\nyou plug in the least squares",
    "start": "2976900",
    "end": "2982580"
  },
  {
    "text": "estimate, you're going to get\nan estimator that's unbiased. Who can tell me\nwhat unbiased is?",
    "start": "2982580",
    "end": "2989330"
  },
  {
    "text": "It's probably going to be a new\nconcept for some people here.",
    "start": "2989330",
    "end": "2994770"
  },
  {
    "text": "Anyone? OK, well it's a basic\nproperty of estimators",
    "start": "2994770",
    "end": "2999810"
  },
  {
    "text": "in statistics where the\nexpectation of this statistic is the true parameter.",
    "start": "2999810",
    "end": "3006720"
  },
  {
    "text": "So it doesn't, on average,\nprobabilistically, it doesn't over- or\nunderestimate the value.",
    "start": "3006720",
    "end": "3012859"
  },
  {
    "text": "So that's what unbiased means. Now, it's also a\nlinear estimator of theta in terms\nof this theta hat",
    "start": "3012860",
    "end": "3021270"
  },
  {
    "text": "being a particular\nlinear combination of the dependent variables.",
    "start": "3021270",
    "end": "3026750"
  },
  {
    "text": "So with our original\nresponse variable y, in the case of y_1 through\ny_n, this theta hat is simply",
    "start": "3026750",
    "end": "3037510"
  },
  {
    "text": "a linear combination\nof all the y's. And now why is that true?",
    "start": "3037510",
    "end": "3042560"
  },
  {
    "text": "Well, we know that beta hat,\nfrom the normal equations,",
    "start": "3042560",
    "end": "3049246"
  },
  {
    "text": "is solved by X transpose\nX inverse X transpose y. So it's a linear\ntransform of the y vector.",
    "start": "3049246",
    "end": "3056200"
  },
  {
    "text": "So if we take a\nlinear combination of those components, it's also\nanother linear combination of the y vector.",
    "start": "3056200",
    "end": "3061490"
  },
  {
    "text": "So this is a linear\nfunction of the underlying-- of the response variables.",
    "start": "3061490",
    "end": "3068830"
  },
  {
    "text": "Now, the Gauss-Markov\ntheorem says that, if the Gauss-Markov\nassumptions apply,",
    "start": "3068830",
    "end": "3076349"
  },
  {
    "text": "then the estimator theta\nhas the smallest variance amongst all linear unbiased\nestimators of theta.",
    "start": "3076350",
    "end": "3085750"
  },
  {
    "text": "So it actually is\nlike the optimal one, as long as this is our criteria.",
    "start": "3085750",
    "end": "3092069"
  },
  {
    "text": "And this is really a\nvery powerful result. And to prove it, it's very easy.",
    "start": "3092070",
    "end": "3102482"
  },
  {
    "text": "Let's see. Actually, these notes are\ngoing to be distributed.",
    "start": "3102482",
    "end": "3107750"
  },
  {
    "text": "So I'm going to go through\nthis very, very quickly",
    "start": "3107750",
    "end": "3113890"
  },
  {
    "text": "and come back to it later\nif we have more time. But you basically-- the\nargument for the proof here",
    "start": "3113890",
    "end": "3121670"
  },
  {
    "text": "is you consider another\nlinear estimate which is also an unbiased estimate.",
    "start": "3121670",
    "end": "3128410"
  },
  {
    "text": "So let's consider a competitor\nto the least squares value",
    "start": "3128410",
    "end": "3133539"
  },
  {
    "text": "and then look at the difference\nbetween that estimator and theta hat.",
    "start": "3133540",
    "end": "3140960"
  },
  {
    "text": "And so that can be characterized\nas basically this vector,",
    "start": "3140960",
    "end": "3147880"
  },
  {
    "text": "f transpose y.  And this difference\nin the estimates",
    "start": "3147880",
    "end": "3155970"
  },
  {
    "text": "must have expectation 0. So basically, if we look at--\nif theta tilde is unbiased,",
    "start": "3155970",
    "end": "3162790"
  },
  {
    "text": "then this expression\nhere is going to be equal to zero,\nwhich means that f--",
    "start": "3162790",
    "end": "3170579"
  },
  {
    "text": "the difference in\nthese two estimators, f",
    "start": "3170580",
    "end": "3177100"
  },
  {
    "text": "defines the difference\nin the two estimators-- has to be orthogonal to\nthe column space of x.",
    "start": "3177100",
    "end": "3182119"
  },
  {
    "text": "And with this\nresult, one then uses",
    "start": "3182120",
    "end": "3193350"
  },
  {
    "text": "this orthogonality of\nf and d to evaluate the variance of theta tilde.",
    "start": "3193350",
    "end": "3200040"
  },
  {
    "text": "And in this proof, the\nmathematical argument here is really something--\nI should put some asterisks",
    "start": "3200040",
    "end": "3209535"
  },
  {
    "text": "on a few lines here. This expression here is\nactually very important.",
    "start": "3209535",
    "end": "3215849"
  },
  {
    "text": "We're basically looking\nat the decomposition of the variance to\nbe the variance of b",
    "start": "3215850",
    "end": "3223250"
  },
  {
    "text": "transpose y, which is\nthe variance of the sum of these two random variables.",
    "start": "3223250",
    "end": "3229349"
  },
  {
    "text": "So the page before\nbasically defined d and f",
    "start": "3229350",
    "end": "3235880"
  },
  {
    "text": "such that this is true. Now when you consider\nthe variance of a sum,",
    "start": "3235880",
    "end": "3242370"
  },
  {
    "text": "it's not the sum\nof the variances. It's the sum of the\nvariances plus twice",
    "start": "3242370",
    "end": "3249730"
  },
  {
    "text": "the sum of the covariances. And so when you are\ncalculating variances",
    "start": "3249730",
    "end": "3258795"
  },
  {
    "text": "of sums of random variables,\nyou have to really keep track of the covariance terms.",
    "start": "3258795",
    "end": "3264180"
  },
  {
    "text": "In this case, this\nargument shows that the covariance\nterms are, in fact, 0,",
    "start": "3264180",
    "end": "3269320"
  },
  {
    "text": "and you get the\nresult popping out. But that's really a-- in\nan econometrics class,",
    "start": "3269320",
    "end": "3278890"
  },
  {
    "text": "they'll talk about BLUE\nestimates of regression, or the BLUE property of the\nleast squares estimates.",
    "start": "3278890",
    "end": "3285100"
  },
  {
    "text": "That's where that comes from. All right, so let's now consider\ngeneralizing from Gauss-Markov",
    "start": "3285100",
    "end": "3293090"
  },
  {
    "text": "to allow for unequal variances\nand possibly correlated",
    "start": "3293090",
    "end": "3304660"
  },
  {
    "text": "nonzero covariances\nbetween the components. And in this case,\nthe regression model",
    "start": "3304660",
    "end": "3313150"
  },
  {
    "text": "has the same linear set up. The only difference\nis the expectation of the residual\nvector is still 0.",
    "start": "3313150",
    "end": "3319730"
  },
  {
    "text": "But the covariance matrix\nof the residual vector is sigma squared,\na single parameter,",
    "start": "3319730",
    "end": "3326329"
  },
  {
    "text": "times let's say capital sigma. And we'll assume here\nthat this capital sigma",
    "start": "3326330",
    "end": "3333530"
  },
  {
    "text": "matrix is a known n by n\npositive definite matrix",
    "start": "3333530",
    "end": "3339000"
  },
  {
    "text": "specifying relative\nvariances and correlations between the observations. ",
    "start": "3339000",
    "end": "3347570"
  },
  {
    "text": "OK.  Well, in order to solve\nfor regression estimates",
    "start": "3347570",
    "end": "3359392"
  },
  {
    "text": "under these generalized\nGauss-Markov assumptions,",
    "start": "3359392",
    "end": "3364400"
  },
  {
    "text": "we can transform the\ndata Y, X to Y star equals sigma to the\nminus 1/2 y and X",
    "start": "3364400",
    "end": "3373170"
  },
  {
    "text": "to X star, which is\nsigma to the minus 1/2 x. And this model then becomes\na model, a linear regression",
    "start": "3373170",
    "end": "3384470"
  },
  {
    "text": "model, in terms of\nY star and X star.",
    "start": "3384470",
    "end": "3389640"
  },
  {
    "text": "We're basically multiplying\nthis regression model by sigma to the minus 1/2 across.",
    "start": "3389640",
    "end": "3396320"
  },
  {
    "text": "And epsilon star actually\nhas a covariance matrix",
    "start": "3396320",
    "end": "3403700"
  },
  {
    "text": "equal to sigma squared\ntimes the identity. So if we just take a\nlinear transformation",
    "start": "3403700",
    "end": "3410380"
  },
  {
    "text": "of the original data,\nwe get a representation",
    "start": "3410380",
    "end": "3416259"
  },
  {
    "text": "of the regression\nmodel that satisfies the original\nGauss-Markov assumptions. And what we had to\ndo was basically",
    "start": "3416260",
    "end": "3423410"
  },
  {
    "text": "do a linear transformation\nthat makes the response variables all have constant\nvariance and be uncorrelated.",
    "start": "3423410",
    "end": "3430500"
  },
  {
    "text": " So with that, we then have the\nleast squares estimate of beta",
    "start": "3430500",
    "end": "3439740"
  },
  {
    "text": "is the least squares, the\nordinary least squares, in terms of Y star and X star.",
    "start": "3439740",
    "end": "3446250"
  },
  {
    "text": "And so plugging that in, we then\nhave X star transpose X star",
    "start": "3446250",
    "end": "3451910"
  },
  {
    "text": "inverse X star transpose Y star. And if you multiply through,\nthat's how the formula changes.",
    "start": "3451910",
    "end": "3457130"
  },
  {
    "text": " So this formula characterizing\nthe least squares estimate",
    "start": "3457130",
    "end": "3465600"
  },
  {
    "text": "under this generalized\nset of assumptions highlights what you\nneed to do to be",
    "start": "3465600",
    "end": "3475010"
  },
  {
    "text": "able to apply that theorem. So with response values that\nhave very large variances,",
    "start": "3475010",
    "end": "3483140"
  },
  {
    "text": "you basically want to discount\nthose by the sigma inverse. ",
    "start": "3483140",
    "end": "3490275"
  },
  {
    "text": "And that's part of the way in\nwhich these generalized least squares work.",
    "start": "3490275",
    "end": "3495940"
  },
  {
    "text": "All right. So now let's turn to\ndistribution theory for normal regression models.",
    "start": "3495940",
    "end": "3501682"
  },
  {
    "text": " Let's assume that\nthe residuals are",
    "start": "3501683",
    "end": "3508820"
  },
  {
    "text": "normals with mean 0 and\nvariance sigma squared. ",
    "start": "3508820",
    "end": "3517880"
  },
  {
    "text": "OK, conditioning on the values\nof the independent variable, the Y's, the response\nvariables, are",
    "start": "3517880",
    "end": "3524240"
  },
  {
    "text": "going to be independent\nover the index i.",
    "start": "3524240",
    "end": "3529932"
  },
  {
    "text": "They're not going to be\nidentically distributed because they have\ndifferent means, mu_i for the dependent\nvariable Y_i, but they",
    "start": "3529932",
    "end": "3538819"
  },
  {
    "text": "will have a constant variance. And what we can do is\nbasically condition on X, beta,",
    "start": "3538820",
    "end": "3550930"
  },
  {
    "text": "and sigma squared\nand then represent this model in terms of the\ndistribution of the epsilons.",
    "start": "3550930",
    "end": "3560280"
  },
  {
    "text": "So if we're conditioning\non x and beta, this X beta is a constant,\nknown, we've conditioned on it.",
    "start": "3560280",
    "end": "3567950"
  },
  {
    "text": "And the remaining uncertainty\nis in the residual vector, which is assumed to\nbe all independent",
    "start": "3567950",
    "end": "3576590"
  },
  {
    "text": "and identically distributed\nnormal random variables. Now, this is the\nfirst time you'll see this notation, capital N sub\nlittle n, for a random vector.",
    "start": "3576590",
    "end": "3588400"
  },
  {
    "text": "It's a multivariate\nnormal random variable where you consider an n-vector\nwhere each component is",
    "start": "3588400",
    "end": "3597540"
  },
  {
    "text": "normally distributed,\nwith mean given by some corresponding\nmean vector,",
    "start": "3597540",
    "end": "3604090"
  },
  {
    "text": "and a covariance matrix\ngiven by a covariance matrix.",
    "start": "3604090",
    "end": "3610200"
  },
  {
    "text": "In terms of independent and\nidentically distributed values,",
    "start": "3610200",
    "end": "3616150"
  },
  {
    "text": "the probability structure\nhere is totally well-defined.",
    "start": "3616150",
    "end": "3621250"
  },
  {
    "text": "Anyone here who's taken a\nbeginning probability class knows what the\ndensity function is",
    "start": "3621250",
    "end": "3626420"
  },
  {
    "text": "for this multivariate\nnormal distribution because it's the product\nof the independent density",
    "start": "3626420",
    "end": "3632180"
  },
  {
    "text": "functions for the\nindependent components, because they're all\nindependent random variables. So this multivariate\nnormal random vector",
    "start": "3632180",
    "end": "3640190"
  },
  {
    "text": "has a density function\nwhich you can write down, given your first\nprobability class.",
    "start": "3640190",
    "end": "3647266"
  },
  {
    "text": " OK, here I'm just\nhighlighting or defining",
    "start": "3647266",
    "end": "3654960"
  },
  {
    "text": "the mu vector for the means\nof the cases of the data.",
    "start": "3654960",
    "end": "3661670"
  },
  {
    "text": "And the covariance matrix\nsigma is this diagonal matrix. ",
    "start": "3661670",
    "end": "3668940"
  },
  {
    "text": "And so basically sigma_(i,j)\nis equal to sigma squared times",
    "start": "3668940",
    "end": "3679450"
  },
  {
    "text": "the Kronecker delta\nfor the (i,j) element. Now what we want to do\nis, under the assumptions",
    "start": "3679450",
    "end": "3688940"
  },
  {
    "text": "of normally\ndistributed residuals, to solve for the distribution\nof the least squares estimators.",
    "start": "3688940",
    "end": "3698700"
  },
  {
    "text": "We want to know, basically,\nwhat kind of distribution does it have? Because what we want\nto be able to do",
    "start": "3698700",
    "end": "3704360"
  },
  {
    "text": "is to determine\nwhether estimates are particularly large or not. And maybe there's\nno structure at all",
    "start": "3704360",
    "end": "3710850"
  },
  {
    "text": "and the regression\nparameters are 0 so that there's no dependence\non a given factor.",
    "start": "3710850",
    "end": "3717510"
  },
  {
    "text": "And we need to be able to\njudge how significant that is. So we need to know what\nthe distribution is",
    "start": "3717510",
    "end": "3723289"
  },
  {
    "text": "of our least squares estimate. So what we're going to do\nis apply moment generating",
    "start": "3723290",
    "end": "3729160"
  },
  {
    "text": "functions to derive the\njoint distribution of y and the joint\ndistribution of beta hat. ",
    "start": "3729160",
    "end": "3737060"
  },
  {
    "text": "And so Choongbum introduced\nthe moment generating function",
    "start": "3737060",
    "end": "3742560"
  },
  {
    "text": "for individual random variables\nfor single-variate random variables. For n-variate\nrandom variables, we",
    "start": "3742560",
    "end": "3750100"
  },
  {
    "text": "can define the moment generating\nfunction of the Y vector",
    "start": "3750100",
    "end": "3755190"
  },
  {
    "text": "to be the expectation of\ne to the t transpose Y. So t is an argument of the\nmoment generating function.",
    "start": "3755190",
    "end": "3761840"
  },
  {
    "text": "It's another n-vector. And it's equal to the\nexpectation of e to the t_1 Y_1 plus t_2 Y_2 up to t_n Y_n.",
    "start": "3761840",
    "end": "3768839"
  },
  {
    "text": "So this is a very\nsimple definition.",
    "start": "3768840",
    "end": "3773930"
  },
  {
    "text": "Because of independence,\nthe expectation of the products, or\nthis exponential sum",
    "start": "3773930",
    "end": "3782380"
  },
  {
    "text": "is the product of\nthe exponentials. And so this moment\ngenerating function is simply",
    "start": "3782380",
    "end": "3788840"
  },
  {
    "text": "the product of the moment\ngenerating functions for Y_1 up through Y_n.",
    "start": "3788840",
    "end": "3795190"
  },
  {
    "text": "And I think-- I don't know if\nit was in the first problem set or in the first lecture, but e\nto the t_i mu_i plus a half t_i",
    "start": "3795190",
    "end": "3802651"
  },
  {
    "text": "squared sigma squared\nwas the moment generating function for the\nsingle univariate",
    "start": "3802652",
    "end": "3808310"
  },
  {
    "text": "normal random variable,\nmean mu_i and variance sigma squared. And so if we have n of\nthese, we take their product.",
    "start": "3808310",
    "end": "3816780"
  },
  {
    "text": "And the moment\ngenerating function for y is simply e to the\nt transpose mu plus 1/2",
    "start": "3816780",
    "end": "3824700"
  },
  {
    "text": "t transpose sigma t. And so for this multivariate\nnormal distribution,",
    "start": "3824700",
    "end": "3833020"
  },
  {
    "text": "this is its moment\ngenerating function. And this happens to be--\nthe distribution of y",
    "start": "3833020",
    "end": "3846069"
  },
  {
    "text": "is a multivariate normal with\nmean mu and covariance matrix sigma.",
    "start": "3846070",
    "end": "3851940"
  },
  {
    "text": "So a fact that\nwe're going to use is that if we're working with\nmultivariate normal random",
    "start": "3851940",
    "end": "3859970"
  },
  {
    "text": "variables, this is the structure\nof their moment generating functions. And so if we solve for\nthe moment generation",
    "start": "3859970",
    "end": "3866870"
  },
  {
    "text": "function of some\nother item of interest and recognize that\nit has the same form, we can conclude that it's also\na multivariate normal random",
    "start": "3866870",
    "end": "3875237"
  },
  {
    "text": "variable.  So let's do that.",
    "start": "3875237",
    "end": "3881329"
  },
  {
    "text": "Let's solve for the\nmoment generation function of the least\nsquares estimate, beta hat.",
    "start": "3881330",
    "end": "3888119"
  },
  {
    "text": "Now rather than dealing\nwith an n-vector, we're dealing with a p-vector\nof the betas, beta hats.",
    "start": "3888120",
    "end": "3895212"
  },
  {
    "text": "And this is simply the\ndefinition of the moment generating function.",
    "start": "3895212",
    "end": "3900240"
  },
  {
    "text": "If we plug in for basically\nwhat the functional form is",
    "start": "3900240",
    "end": "3906380"
  },
  {
    "text": "for the ordinary least\nsquares estimates and how they depend on\nthe underlying Y, then we",
    "start": "3906380",
    "end": "3913960"
  },
  {
    "text": "basically-- OK, we have\nA equal to, essentially,",
    "start": "3913960",
    "end": "3920480"
  },
  {
    "text": "the linear projection of Y.\nThat gives us the least squares estimate. And then we can say that\nthis moment generating",
    "start": "3920480",
    "end": "3928800"
  },
  {
    "text": "function for beta hat is\nequal to the expectation of e",
    "start": "3928800",
    "end": "3934723"
  },
  {
    "text": "to the t transpose Y, where\nlittle t is A transpose tau.",
    "start": "3934723",
    "end": "3940650"
  },
  {
    "text": "Well, we know what this is. This is the moment\ngenerating function of X-- sorry, of Y-- evaluated\nat the vector little t.",
    "start": "3940650",
    "end": "3950080"
  },
  {
    "text": "So we just need to plug in\nlittle t, that expression A transpose tau.",
    "start": "3950080",
    "end": "3956280"
  },
  {
    "text": "So let's do that. And you do that and it turns\nout to be e to the t transpose",
    "start": "3956280",
    "end": "3963580"
  },
  {
    "text": "mu plus that. And we go through a\nnumber of calculations.",
    "start": "3963580",
    "end": "3973450"
  },
  {
    "text": "And at the end of the day, we\nget that the moment generating function is just e to the tau\ntranspose beta plus a 1/2 tau",
    "start": "3973450",
    "end": "3980580"
  },
  {
    "text": "transpose this matrix tau. And that is the moment\ngeneration function",
    "start": "3980580",
    "end": "3986050"
  },
  {
    "text": "of a multivariate normal. So these few lines that you\ncan go through after class",
    "start": "3986050",
    "end": "3992280"
  },
  {
    "text": "basically solve for\nthe moment generating function of beta hat. And because we can\nrecognize this as the MGF",
    "start": "3992280",
    "end": "3999456"
  },
  {
    "text": "of a multivariate normal, we\nknow that that's-- beta hat is",
    "start": "3999456",
    "end": "4004960"
  },
  {
    "text": "a multivariate normal,\nwith mean the true beta, and covariance matrix given by\nthe object in square brackets",
    "start": "4004960",
    "end": "4012609"
  },
  {
    "text": "there. OK, so this is\nessentially the conclusion",
    "start": "4012610",
    "end": "4021589"
  },
  {
    "text": "of that previous analysis. The marginal distribution\nof each of the beta hats",
    "start": "4021590",
    "end": "4028410"
  },
  {
    "text": "is given by beta hat-- by a\nunivariate normal distribution",
    "start": "4028410",
    "end": "4033490"
  },
  {
    "text": "with mean beta_j and variance\nequal to the diagonal. Now at this point, saying\nthat is like an assertion.",
    "start": "4033490",
    "end": "4045190"
  },
  {
    "text": "But one can actually\nprove that very easily, given this sequence of argument.",
    "start": "4045190",
    "end": "4053290"
  },
  {
    "text": "And can anyone tell\nme why this is true? ",
    "start": "4053290",
    "end": "4063080"
  },
  {
    "text": "Let me tell you. If you consider plugging in\nthe moment generating function, the value tau, where only\nthe j-th entry is non-zero,",
    "start": "4063080",
    "end": "4073990"
  },
  {
    "text": "then you have the moment\ngenerating function of the j-th component\nof beta hat.",
    "start": "4073990",
    "end": "4079310"
  },
  {
    "text": "And that's a Gaussian\nmoment generating function. So the marginal distribution of\nthe j-th component is normal.",
    "start": "4079310",
    "end": "4088190"
  },
  {
    "text": "So you get that\nalmost for free from this multivariate analysis.",
    "start": "4088190",
    "end": "4093680"
  },
  {
    "text": "And so there's no hand waving\ngoing on in having that result. This actually follows\ndirectly from the moment",
    "start": "4093680",
    "end": "4100229"
  },
  {
    "text": "generating functions. OK, let's now turn\nto another topic.",
    "start": "4100229",
    "end": "4106869"
  },
  {
    "text": "Related, but it's the\nQR decomposition of X.",
    "start": "4106870",
    "end": "4112200"
  },
  {
    "text": "So we have-- with our\nindependent variables X, we want to express\nthis as a product",
    "start": "4112200",
    "end": "4121630"
  },
  {
    "text": "of an orthonormal matrix\nQ which is n by p, and an upper\ntriangular matrix R.",
    "start": "4121630",
    "end": "4131684"
  },
  {
    "text": "So it turns out that any\nmatrix, n by p matrix,",
    "start": "4131684",
    "end": "4139750"
  },
  {
    "text": "can be expressed in this form. And we'll quickly show you\nhow that can be accomplished.",
    "start": "4139750",
    "end": "4147259"
  },
  {
    "text": "We can accomplish\nthat by conducting a Gram-Schmidt\northonormalization",
    "start": "4147260",
    "end": "4153479"
  },
  {
    "text": "of the independent\nvariables matrix X. And let's see.",
    "start": "4153479",
    "end": "4161540"
  },
  {
    "text": "So if we define R, the upper\ntriangular matrix in the QR",
    "start": "4161540",
    "end": "4166689"
  },
  {
    "text": "decomposition, to have\n0's off the diagonal below and then possibly nonzero\nvalue along the diagonal",
    "start": "4166689",
    "end": "4175710"
  },
  {
    "text": "into the right, we're just\ngoing to solve for Q and R",
    "start": "4175710",
    "end": "4181500"
  },
  {
    "text": "through this\nGram-Schmidt process. So the first column of X is\nequal to the first column",
    "start": "4181500",
    "end": "4190439"
  },
  {
    "text": "of Q times the first\nelement, the top left corner",
    "start": "4190439",
    "end": "4197430"
  },
  {
    "text": "of the matrix R. And if we take the cross product\nof that vector with itself,",
    "start": "4197430",
    "end": "4208130"
  },
  {
    "text": "then we get this expression\nfor r_(1,1) squared--",
    "start": "4208130",
    "end": "4214600"
  },
  {
    "text": "we can basically solve for\nr_(1,1) as the square root of this dot product. And Q_Q_[1] is simply the first\ncolumn of X divided by that",
    "start": "4214600",
    "end": "4222920"
  },
  {
    "text": "square root. So this first element\nof the Q matrix and the first element r, this\ncan be solved for right away.",
    "start": "4222920",
    "end": "4232210"
  },
  {
    "text": "Then let's solve for\nthe second column of Q",
    "start": "4232210",
    "end": "4238100"
  },
  {
    "text": "and the second column\nof the R matrix.",
    "start": "4238100",
    "end": "4243310"
  },
  {
    "text": "Well, X_X_[2], the second\ncolumn of the X matrix, is the first column\nof Q times r_(1,2),",
    "start": "4243310",
    "end": "4256090"
  },
  {
    "text": "plus the second column\nof Q times r_(2,2). ",
    "start": "4256090",
    "end": "4262250"
  },
  {
    "text": "And if we multiply this\nexpression by Q_Q_[1] transpose,",
    "start": "4262250",
    "end": "4269300"
  },
  {
    "text": "then we basically get this\nexpression for r_(1,2).",
    "start": "4269300",
    "end": "4275719"
  },
  {
    "text": " So we actually have\njust solved for r_(1,2).",
    "start": "4275720",
    "end": "4284239"
  },
  {
    "text": "And Q_Q_[2] is solved for by\nthe arguments given here.",
    "start": "4284240",
    "end": "4295910"
  },
  {
    "text": "So basically, we successively\nare orthogonalizing",
    "start": "4295910",
    "end": "4301070"
  },
  {
    "text": "columns of X to the\nprevious columns of X through this\nGram-Schmidt process. And it basically can be repeated\nthrough all the columns.",
    "start": "4301070",
    "end": "4308460"
  },
  {
    "text": " Now with this QR\ndecomposition, what we get",
    "start": "4308460",
    "end": "4315409"
  },
  {
    "text": "is a really nice form for\nthe least squares estimate.",
    "start": "4315410",
    "end": "4321470"
  },
  {
    "text": "Basically, it simplifies to the\ninverse of R times Q transpose",
    "start": "4321470",
    "end": "4327080"
  },
  {
    "text": "y. And this basically\nmeans that you",
    "start": "4327080",
    "end": "4335490"
  },
  {
    "text": "can solve for least squares\nestimates by calculating the QR decomposition, which is a\nvery simple linear algebra",
    "start": "4335490",
    "end": "4342150"
  },
  {
    "text": "operation, and then just do\na couple of matrix products to get the-- well, you do have\nto do a matrix inverse with R",
    "start": "4342150",
    "end": "4351310"
  },
  {
    "text": "to get that out. And the covariance\nmatrix of beta hat",
    "start": "4351310",
    "end": "4357810"
  },
  {
    "text": "is equal to sigma squared\nX transpose X inverse.",
    "start": "4357810",
    "end": "4364480"
  },
  {
    "text": "And in terms of the covariance\nmatrix, what is implicit here",
    "start": "4364480",
    "end": "4373330"
  },
  {
    "text": "but you should make\nexplicit in your study, is if you consider taking a\nmatrix, R inverse Q transpose",
    "start": "4373330",
    "end": "4386190"
  },
  {
    "text": "times y, the only thing that's\nrandom there is that y vector, OK?",
    "start": "4386190",
    "end": "4391660"
  },
  {
    "text": "The covariance of a matrix\ntimes a random vector",
    "start": "4391660",
    "end": "4396720"
  },
  {
    "text": "is that matrix\ntimes the covariance of the vector times the\ntranspose of the matrix.",
    "start": "4396720",
    "end": "4402960"
  },
  {
    "text": "So if you take a\nmatrix transformation of a random vector,\nthen the covariance",
    "start": "4402960",
    "end": "4410510"
  },
  {
    "text": "of that transformation\nhas that form. So that's where this covariance\nmatrix is coming into play.",
    "start": "4410510",
    "end": "4421110"
  },
  {
    "text": "And from the MGF, the\nmoment generating function, for the least squares\nestimate, this basically",
    "start": "4421110",
    "end": "4428660"
  },
  {
    "text": "comes out of the moment\ngenerating function definition as well. And if we take X\ntranspose X, plug",
    "start": "4428660",
    "end": "4436480"
  },
  {
    "text": "in the QR decomposition,\nonly the R's play out,",
    "start": "4436480",
    "end": "4441864"
  },
  {
    "text": "giving you that. Now, this also gives\nus a very nice form for the hat matrix,\nwhich turns out",
    "start": "4441864",
    "end": "4450450"
  },
  {
    "text": "to just be Q times Q transpose. So that's a very simple form.",
    "start": "4450450",
    "end": "4460370"
  },
  {
    "text": " So now with the\ndistribution theory,",
    "start": "4460370",
    "end": "4468160"
  },
  {
    "text": "this next section is\ngoing to actually prove",
    "start": "4468160",
    "end": "4475190"
  },
  {
    "text": "what's really a\nfundamental result about normal linear\nregression models.",
    "start": "4475190",
    "end": "4480380"
  },
  {
    "text": "And I'm going to go through\nthis somewhat quickly just so that we cover what the\nmain ideas are of the theorem.",
    "start": "4480380",
    "end": "4489290"
  },
  {
    "text": "But the details, I think,\nare very straightforward. And these course notes\nthat will be posted online",
    "start": "4489290",
    "end": "4497250"
  },
  {
    "text": "will go through the various\nsteps of the analysis. ",
    "start": "4497250",
    "end": "4503390"
  },
  {
    "text": "OK, so there's an\nimportant theorem here which is for any\nmatrix A, m by n,",
    "start": "4503390",
    "end": "4511660"
  },
  {
    "text": "you consider transforming\nthe random vector y by this matrix A. It is\nalso a random normal vector.",
    "start": "4511660",
    "end": "4523600"
  },
  {
    "text": "And its distribution\nis going to have a mean and covariance\nmatrix given",
    "start": "4523600",
    "end": "4530090"
  },
  {
    "text": "by mu_z and sigma_z, which have\nthis simple expression in terms",
    "start": "4530090",
    "end": "4535389"
  },
  {
    "text": "of the matrix A and\nthe underlying means and covariances of y.",
    "start": "4535390",
    "end": "4540995"
  },
  {
    "text": " OK, earlier we actually\napplied this theorem",
    "start": "4540995",
    "end": "4548620"
  },
  {
    "text": "with A corresponding to the\nmatrix that generates the least squares estimates.",
    "start": "4548620",
    "end": "4554920"
  },
  {
    "text": "So with A equal to X\ntranspose X inverse, we actually previously went\nthrough the solution for what's",
    "start": "4554920",
    "end": "4561060"
  },
  {
    "text": "the distribution of beta hat. And with any other\nmatrix A, we can",
    "start": "4561060",
    "end": "4566960"
  },
  {
    "text": "go through the same analysis\nand get the distribution. ",
    "start": "4566960",
    "end": "4573820"
  },
  {
    "text": "So if we do that here,\nwell, we can actually",
    "start": "4573820",
    "end": "4580690"
  },
  {
    "text": "prove this important\ntheorem, which says that with least\nsquares estimates",
    "start": "4580690",
    "end": "4587335"
  },
  {
    "text": "of normal linear regression\nmodels, our least",
    "start": "4587335",
    "end": "4593670"
  },
  {
    "text": "squares estimate beta hat and\nour residual vector epsilon hat",
    "start": "4593670",
    "end": "4600050"
  },
  {
    "text": "are independent\nrandom variables. So when we construct\nthese statistics,",
    "start": "4600050",
    "end": "4608520"
  },
  {
    "text": "they are statistically\nindependent of each other. And the distribution of beta\nhat is multivariate normal.",
    "start": "4608520",
    "end": "4617580"
  },
  {
    "text": "The sum of the squared\nresiduals is, in fact,",
    "start": "4617580",
    "end": "4624730"
  },
  {
    "text": "a multiple of a chi-squared\nrandom variable. Now who in here can tell me what\na chi-squared random variable",
    "start": "4624730",
    "end": "4636010"
  },
  {
    "text": "is? Anyone? AUDIENCE: [INAUDIBLE]? ",
    "start": "4636010",
    "end": "4641590"
  },
  {
    "text": "PROFESSOR: Yes, that's right. So a chi-squared random variable\nwith one degree of freedom is a squared normal zero\none random variable.",
    "start": "4641590",
    "end": "4649652"
  },
  {
    "text": "A chi-squared with\ntwo degrees of freedom is the sum of two independent\nnormals, zero one, squared.",
    "start": "4649652",
    "end": "4656420"
  },
  {
    "text": "And so the sum of n squared\nresiduals is, in fact,",
    "start": "4656420",
    "end": "4663000"
  },
  {
    "text": "an n minus p chi-squared random\nvariable scale it by sigma",
    "start": "4663000",
    "end": "4668182"
  },
  {
    "text": "squared. And for each component\nj, if we take",
    "start": "4668182",
    "end": "4675870"
  },
  {
    "text": "the difference between the least\nsquares estimate beta hat j",
    "start": "4675870",
    "end": "4681080"
  },
  {
    "text": "and beta_j and divide\nthrough by this estimate of the standard\ndeviation of that, then",
    "start": "4681080",
    "end": "4692320"
  },
  {
    "text": "that will, in fact, have a\nt distribution on n minus p degrees of freedom. And let's see, a t distribution\nin probability theory",
    "start": "4692320",
    "end": "4705039"
  },
  {
    "text": "is the ratio of a normal random\nvariable to an independent chi",
    "start": "4705040",
    "end": "4715684"
  },
  {
    "text": "squared random variable, or\nthe root of an independent chi squared random variable. So basically these\nproperties characterize",
    "start": "4715684",
    "end": "4725780"
  },
  {
    "text": "our regression parameter\nestimates and t statistics",
    "start": "4725780",
    "end": "4730969"
  },
  {
    "text": "for those estimates. Now, OK, in the\ncourse notes, there's",
    "start": "4730970",
    "end": "4739489"
  },
  {
    "text": "a moderately long proof. But all the details\nare given, and I'll",
    "start": "4739490",
    "end": "4745320"
  },
  {
    "text": "be happy to go through any\nof those details with people during office hours.",
    "start": "4745320",
    "end": "4751050"
  },
  {
    "text": "Let me just push\non to-- let's see.",
    "start": "4751050",
    "end": "4759500"
  },
  {
    "text": "We have maybe two minutes\nleft in the class. ",
    "start": "4759500",
    "end": "4765860"
  },
  {
    "text": "Let me just talk about\nmaximum likelihood estimation.",
    "start": "4765860",
    "end": "4772440"
  },
  {
    "text": "And in fitting models\nand statistics,",
    "start": "4772440",
    "end": "4777850"
  },
  {
    "text": "maximum likelihood estimation\ncomes up again and again. And with normal linear\nregression models,",
    "start": "4777850",
    "end": "4785295"
  },
  {
    "text": "it turns out that ordinary\nleast squares estimate are, in fact, our maximum\nlikelihood estimates.",
    "start": "4785295",
    "end": "4791690"
  },
  {
    "text": "And what we want to do\nwith a maximum likelihood",
    "start": "4791690",
    "end": "4797570"
  },
  {
    "text": "is to maximize. We want to define the\nlikelihood function, which",
    "start": "4797570",
    "end": "4805640"
  },
  {
    "text": "is the density function\nfor the data given the unknown parameters.",
    "start": "4805640",
    "end": "4812110"
  },
  {
    "text": "And this density\nfunction is simply the density function for a\nmultivariate normal random",
    "start": "4812110",
    "end": "4818720"
  },
  {
    "text": "variable. And the maximum\nlikelihood estimates",
    "start": "4818720",
    "end": "4824630"
  },
  {
    "text": "are the estimates of the\nunderlying parameters that basically maximize\nthe density function.",
    "start": "4824630",
    "end": "4831650"
  },
  {
    "text": "So it's the values of\nthe underlying parameters that make the data that was\nobserved the most likely.",
    "start": "4831650",
    "end": "4837130"
  },
  {
    "text": " And if you plug in the values\nof the density function,",
    "start": "4837130",
    "end": "4849110"
  },
  {
    "text": "basically we have these\nindependent random variables, Y_i, whose product\nis the joint density.",
    "start": "4849110",
    "end": "4860540"
  },
  {
    "text": "The likelihood\nfunction turns out to be basically a function of\nthe least squares criterion.",
    "start": "4860540",
    "end": "4871260"
  },
  {
    "text": "So if you fit models\nby least squares, you're consistent with doing\nsomething decent in at least",
    "start": "4871260",
    "end": "4878472"
  },
  {
    "text": "applying the maximum\nlikelihood principle if you had a normal\nlinear regression model.",
    "start": "4878472",
    "end": "4883720"
  },
  {
    "text": "And it's useful to know when\nyour statistical estimation",
    "start": "4883720",
    "end": "4891730"
  },
  {
    "text": "algorithms are consistent\nwith certain principles",
    "start": "4891730",
    "end": "4897230"
  },
  {
    "text": "like maximum likelihood\nestimation or others. So let me, I guess,\nfinish there.",
    "start": "4897230",
    "end": "4903930"
  },
  {
    "text": "And next time, I will\njust talk a little bit",
    "start": "4903930",
    "end": "4909100"
  },
  {
    "text": "about generalized M estimators. Those provide a\nclass of estimators",
    "start": "4909100",
    "end": "4915159"
  },
  {
    "text": "that can be used for\nfinding robust estimates",
    "start": "4915160",
    "end": "4924420"
  },
  {
    "text": "and also quantile estimates\nof regression parameters which are very interesting.",
    "start": "4924420",
    "end": "4932320"
  }
]