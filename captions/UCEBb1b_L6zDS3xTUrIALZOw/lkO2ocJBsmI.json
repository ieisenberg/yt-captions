[
  {
    "start": "0",
    "end": "14825"
  },
  {
    "text": "PETER SZOLOVITS: All right. Let's get started. Good afternoon.",
    "start": "14825",
    "end": "20119"
  },
  {
    "text": "So last time, I started\ntalking about the use of natural language processing\nto process clinical data.",
    "start": "20120",
    "end": "29770"
  },
  {
    "text": "And things went a\nlittle bit slowly. And so we didn't get through\na lot of the material.",
    "start": "29770",
    "end": "36010"
  },
  {
    "text": "I'm going to try to\nrush a bit more today. And as a result, I have\na lot of stuff to cover.",
    "start": "36010",
    "end": "44690"
  },
  {
    "text": "So if you remember,\nlast time, I started by saying that a\nlot of the NLP work",
    "start": "44690",
    "end": "54010"
  },
  {
    "text": "involves coming up with\nphrases that one might be interested in to help\nidentify the kinds of data",
    "start": "54010",
    "end": "61900"
  },
  {
    "text": "that you want, and then just\nlooking for those in text. So that's a very simple method.",
    "start": "61900",
    "end": "67810"
  },
  {
    "text": "But it's one that\nworks reasonably well. And then Kat Liao was\nhere to talk about some",
    "start": "67810",
    "end": "73150"
  },
  {
    "text": "of the applications\nof that kind of work in what she's been doing\nin cohort selection.",
    "start": "73150",
    "end": "80409"
  },
  {
    "text": "So what I want to\ntalk about today is more sophisticated\nversions of that, and then move on to more\ncontemporary approaches",
    "start": "80410",
    "end": "89110"
  },
  {
    "text": "to natural language processing. So this is a paper\nthat was given",
    "start": "89110",
    "end": "95860"
  },
  {
    "text": "to you as one of the\noptional readings last time. And it's work from\nDavid Sontag's lab,",
    "start": "95860",
    "end": "102250"
  },
  {
    "text": "where they said, well, how do\nwe make this more sophisticated? So they start the same way.",
    "start": "102250",
    "end": "107650"
  },
  {
    "text": "They say, OK, Dr.\nLiao, let's say, give me terms that are very\ngood indicators that I have",
    "start": "107650",
    "end": "116829"
  },
  {
    "text": "the right kind of\npatient, if I find them in the patient's notes.",
    "start": "116830",
    "end": "121880"
  },
  {
    "text": "So these are things with\nhigh predictive value. So you don't want to use a term\nlike sick, because that's going",
    "start": "121880",
    "end": "129990"
  },
  {
    "text": "to find way too many people. But you want to\nfind something that is very specific but that\nhas a high predictive value",
    "start": "129990",
    "end": "137980"
  },
  {
    "text": "that you are going to\nfind the right person. And then what they\ndid is they built",
    "start": "137980",
    "end": "144120"
  },
  {
    "text": "a model that tries to\npredict the presence of that word in the\ntext from everything",
    "start": "144120",
    "end": "153640"
  },
  {
    "text": "else in the medical record. So now, this is an example of a\nsilver-standard way of training",
    "start": "153640",
    "end": "162080"
  },
  {
    "text": "a model that says, well, I don't\nhave the energy or the time",
    "start": "162080",
    "end": "167420"
  },
  {
    "text": "to get doctors to\nlook through thousands and thousands of records. But if I select these\nanchors well enough,",
    "start": "167420",
    "end": "175350"
  },
  {
    "text": "then I'm going to get a high\nyield of correct responses from those.",
    "start": "175350",
    "end": "180410"
  },
  {
    "text": "And then I train\na machine learning model that learns to\nidentify those same terms,",
    "start": "180410",
    "end": "191000"
  },
  {
    "text": "or those same records that\nhave those terms in them. And by the way, from\nthat, we're going",
    "start": "191000",
    "end": "196430"
  },
  {
    "text": "to learn a whole\nbunch of other terms that are proxies for the\nones that we started with.",
    "start": "196430",
    "end": "203040"
  },
  {
    "text": "So this is a way of\nenlarging that set of terms automatically.",
    "start": "203040",
    "end": "210140"
  },
  {
    "text": "And so there are a bunch\nof technical details that you can find out\nabout by reading the paper.",
    "start": "210140",
    "end": "218209"
  },
  {
    "text": "They used a relatively\nsimple representation, which is essentially a\nbag-of-words representation.",
    "start": "218210",
    "end": "225140"
  },
  {
    "text": "They then sort of\nmasked the three words around the word that\nactually is the one they're",
    "start": "225140",
    "end": "232760"
  },
  {
    "text": "trying to predict\njust to get rid of short-term\nsyntactic correlations.",
    "start": "232760",
    "end": "239090"
  },
  {
    "text": "And then they built an\nL2-regularized logistic regression model that said, what\nare the features that predict",
    "start": "239090",
    "end": "246890"
  },
  {
    "text": "the occurrence of this word? And then they expanded\nthe search vocabulary",
    "start": "246890",
    "end": "252379"
  },
  {
    "text": "to include those\nfeatures as well. And again, there\nare tons of details about how to discretize\ncontinuous values",
    "start": "252380",
    "end": "261078"
  },
  {
    "text": "and things like that that\nyou can find out about. So you build a\nphenotype estimator from the anchors and\nthe chosen predictors.",
    "start": "261079",
    "end": "269330"
  },
  {
    "text": "They calculated a\ncalibration score for each of these\nother predictors that",
    "start": "269330",
    "end": "274639"
  },
  {
    "text": "told you how well it predicted. And then you can build\na joint estimator",
    "start": "274640",
    "end": "281360"
  },
  {
    "text": "that uses all of these. And the bottom line is\nthat they did very well. So in order to\nevaluate this, they",
    "start": "281360",
    "end": "291440"
  },
  {
    "text": "looked at eight different\nphenotypes for which they had human judgment data.",
    "start": "291440",
    "end": "298970"
  },
  {
    "text": "And so this tells\nyou that they're getting AUCs of\nbetween 0.83 and 0.95",
    "start": "298970",
    "end": "307130"
  },
  {
    "text": "for these different phenotypes. So that's quite good.",
    "start": "307130",
    "end": "313170"
  },
  {
    "text": "They, in fact, were estimating\nnot only these eight phenotypes but 40-something.",
    "start": "313170",
    "end": "319040"
  },
  {
    "text": "I don't remember the exact\nnumber, much larger number. But they didn't\nhave validated data",
    "start": "319040",
    "end": "325400"
  },
  {
    "text": "against which to\ntest the others. But the expectation is that\nif it does well on these,",
    "start": "325400",
    "end": "330470"
  },
  {
    "text": "it probably does well\non the others as well. So this was a very nice idea.",
    "start": "330470",
    "end": "335810"
  },
  {
    "text": "And just to illustrate, if\nyou start with something like diabetes as a\nphenotype and you say,",
    "start": "335810",
    "end": "341840"
  },
  {
    "text": "well, I'm going to\nlook for anchors that are a code of\n250 diabetes mellitus,",
    "start": "341840",
    "end": "348710"
  },
  {
    "text": "or I'm going to\nlook at medication history for diabetic therapy--",
    "start": "348710",
    "end": "355110"
  },
  {
    "text": "so those are the silver-standard\ngoals that I'm looking at.",
    "start": "355110",
    "end": "360169"
  },
  {
    "text": "And those, in fact, have a high\npredictive value for somebody being in the cohort.",
    "start": "360170",
    "end": "365430"
  },
  {
    "text": "And then they identify\nall these other features that predict those,\nand therefore, in turn,",
    "start": "365430",
    "end": "372229"
  },
  {
    "text": "predict appropriate\nselectors for the phenotype that they're interested in. And if you look at the\npaper again, what you see",
    "start": "372230",
    "end": "379910"
  },
  {
    "text": "is that this\noutperforms, over time, the standard supervised baseline\nthat they're comparing against,",
    "start": "379910",
    "end": "388970"
  },
  {
    "text": "where you're getting much\nhigher accuracy early in a patient's visit to\nbe able to identify them",
    "start": "388970",
    "end": "395509"
  },
  {
    "text": "as belonging to this cohort. I'm going to come back later to\nlook at another similar attempt",
    "start": "395510",
    "end": "405110"
  },
  {
    "text": "to generalize from a core using\na different set of techniques.",
    "start": "405110",
    "end": "410280"
  },
  {
    "text": "So you should see that in\nabout 45 minutes, I hope. ",
    "start": "410280",
    "end": "417380"
  },
  {
    "text": "Well, context is important. So if you look at a sentence\nlike Mr. Huntington was treated",
    "start": "417380",
    "end": "422810"
  },
  {
    "text": "for Huntington's disease at\nHuntington Hospital, located on Huntington Avenue, each\nof those mentions of the word",
    "start": "422810",
    "end": "430670"
  },
  {
    "text": "Huntington is different. And for example, if you're\ninterested in eliminating",
    "start": "430670",
    "end": "436850"
  },
  {
    "text": "personally identifiable\nhealth information from a record like\nthis, then certainly you",
    "start": "436850",
    "end": "443270"
  },
  {
    "text": "want to get rid of the\nMr. Huntington part. You don't want to get rid\nof Huntington's disease,",
    "start": "443270",
    "end": "449150"
  },
  {
    "text": "because that's a\nmedically relevant fact. And you probably do want to\nget rid of Huntington Hospital",
    "start": "449150",
    "end": "457940"
  },
  {
    "text": "and its location on\nHuntington Avenue, although those are not\nnecessarily something",
    "start": "457940",
    "end": "464390"
  },
  {
    "text": "that you're prohibited\nfrom retaining. So for example, if you're\ntrying to do quality studies",
    "start": "464390",
    "end": "470599"
  },
  {
    "text": "among different\nhospitals, then it would make sense to retain the\nname of the hospital, which",
    "start": "470600",
    "end": "476480"
  },
  {
    "text": "is not considered identifying\nof the individual. So we, in fact, did a study\nback in the mid 2000s,",
    "start": "476480",
    "end": "485420"
  },
  {
    "text": "where we were trying to build\nan improved de-identifier. ",
    "start": "485420",
    "end": "492580"
  },
  {
    "text": "And here's the way\nwe went about it. This is a kind of kitchen\nsink approach that says,",
    "start": "492580",
    "end": "497990"
  },
  {
    "text": "OK, take the text, tokenize it.",
    "start": "497990",
    "end": "503330"
  },
  {
    "text": "Look at every single token. And derive things from it. So the words that\nmake up the token,",
    "start": "503330",
    "end": "510350"
  },
  {
    "text": "the part of speech,\nhow it's capitalized, whether there's\npunctuation around it,",
    "start": "510350",
    "end": "516169"
  },
  {
    "text": "which document\nsection is it in-- many databases have sort\nof conventional document",
    "start": "516169",
    "end": "524059"
  },
  {
    "text": "structure. If you've looked at the\nmimic discharge summaries, for example, there's a\nkind of prototypical way",
    "start": "524059",
    "end": "531530"
  },
  {
    "text": "in which that flows\nfrom beginning to end. And you can use that\nstructural information.",
    "start": "531530",
    "end": "537920"
  },
  {
    "text": "We then identified a bunch of\npatterns and thesaurus terms. So we looked up, in the\nUMLS, words and phrases",
    "start": "537920",
    "end": "546650"
  },
  {
    "text": "to see if they matched some\nclinically meaningful term. We had patterns that\nidentified things",
    "start": "546650",
    "end": "553010"
  },
  {
    "text": "like phone numbers and social\nsecurity numbers and addresses and so on.",
    "start": "553010",
    "end": "559140"
  },
  {
    "text": "And then we did\nparsing of the text. So in those days,\nwe used something",
    "start": "559140",
    "end": "564740"
  },
  {
    "text": "called the Link\nGrammar Parser, which, doesn't make a whole lot\nof difference what parser.",
    "start": "564740",
    "end": "570470"
  },
  {
    "text": "But you get either a constituent\nor constituency or dependency parse, which gives you\nrelationships among the words.",
    "start": "570470",
    "end": "579420"
  },
  {
    "text": "And so it allows you to\ninclude, as features, the way in which a word\nthat you're looking at",
    "start": "579420",
    "end": "587180"
  },
  {
    "text": "relates to other\nwords around it. And so what we did is we\nsaid, OK, the lexical context",
    "start": "587180",
    "end": "594110"
  },
  {
    "text": "includes all of the\nabove kind of information for all of the words that\nare either literally adjacent",
    "start": "594110",
    "end": "602720"
  },
  {
    "text": "or within n words of the\noriginal word that you're focusing on, or that are\nlinked by within k links",
    "start": "602720",
    "end": "610730"
  },
  {
    "text": "through the parse to that word. So this gives you a very\nlarge set of features.",
    "start": "610730",
    "end": "617900"
  },
  {
    "text": "And of course, parsing\nis not a solved problem.",
    "start": "617900",
    "end": "623070"
  },
  {
    "text": "And so this is an\nexample from that story that I showed you last time.",
    "start": "623070",
    "end": "629779"
  },
  {
    "text": "And if you see, it comes\nup with 24 ambiguous parses",
    "start": "629780",
    "end": "636530"
  },
  {
    "text": "of this sentence. So there are technical problems\nabout how to deal with that.",
    "start": "636530",
    "end": "644960"
  },
  {
    "text": "Today, you could use\na different parser. The Stanford\nParser, for example, probably does a better\njob than the one",
    "start": "644960",
    "end": "651650"
  },
  {
    "text": "we were using 14 years\nago and gives you at least",
    "start": "651650",
    "end": "658010"
  },
  {
    "text": "more definitive answers. And so you could\nuse that instead. And so if you look\nat what we did,",
    "start": "658010",
    "end": "664460"
  },
  {
    "text": "we said, well, here\nis the text \"Mr.\" And here are all the ways that\nyou can look it up in the UMLS.",
    "start": "664460",
    "end": "675079"
  },
  {
    "text": "And it turns out to\nbe very ambiguous. So M-R stands not\nonly for mister,",
    "start": "675080",
    "end": "682960"
  },
  {
    "text": "but it also stands for\nMagnetic Resonance. And it stands for a whole\nbunch of other things.",
    "start": "682960",
    "end": "688740"
  },
  {
    "text": "And so you get huge\namounts of ambiguity. \"Blind\" turns out also to\ngive you various ambiguities.",
    "start": "688740",
    "end": "696410"
  },
  {
    "text": "So it maps here to four\ndifferent concept-unique identifiers.",
    "start": "696410",
    "end": "703250"
  },
  {
    "text": "\"Is\" is OK. \"79-year-old\" is OK.",
    "start": "703250",
    "end": "709250"
  },
  {
    "text": "And then \"male,\" again, maps to\nfive different concept-unique",
    "start": "709250",
    "end": "716300"
  },
  {
    "text": "identifiers. So there are all these\nproblems of over-generation from this database.",
    "start": "716300",
    "end": "722580"
  },
  {
    "text": "And here's some more, but\nI'm going to skip over that. And then the learning\nmodel, in our case, was a support vector machine for\nthis project, in which we just",
    "start": "722580",
    "end": "731240"
  },
  {
    "text": "said, well, throw in all the-- you know, it's\nthe kill them all, and God will sort them\nout kind of approach.",
    "start": "731240",
    "end": "739370"
  },
  {
    "text": "So we just threw in\nall these features and said, oh, support\nvector machines are really good at picking\nout exactly what are the best",
    "start": "739370",
    "end": "746990"
  },
  {
    "text": "features. And so we just relied on that. And sure enough, so you wind\nup with literally millions",
    "start": "746990",
    "end": "754580"
  },
  {
    "text": "of features. But sure enough, it\nworked pretty well. And so Stat De-ID\nwas our program.",
    "start": "754580",
    "end": "761550"
  },
  {
    "text": "And you see that on real\ndischarge summaries, we're getting precision\nand recall on PHI",
    "start": "761550",
    "end": "769370"
  },
  {
    "text": "up around 98 and\n1/2%, 95 and 1/4%, which was much better than\nthe previous state of the art,",
    "start": "769370",
    "end": "776480"
  },
  {
    "text": "which had been based on\nrules and dictionaries as a way of\nde-identifying things.",
    "start": "776480",
    "end": "783560"
  },
  {
    "text": "So this was a successful\nexample of that approach. And of course, this is usable\nnot only for de-identification.",
    "start": "783560",
    "end": "793160"
  },
  {
    "text": "But it's also usable\nfor entity recognition. Because instead of\nselecting entities",
    "start": "793160",
    "end": "799399"
  },
  {
    "text": "that are personally\nidentifiable health information, you could train it to select\nentities that are diseases",
    "start": "799400",
    "end": "806829"
  },
  {
    "text": "or that are medications or\nthat are various other things. And so this was, in the\n2000s, a pretty typical way",
    "start": "806830",
    "end": "815470"
  },
  {
    "text": "for people to approach\nthese kinds of problems. And it's still used today. There are tools around\nthat let you do this.",
    "start": "815470",
    "end": "823060"
  },
  {
    "text": "And they work\nreasonably effectively. They're not state of\nthe art at the moment, but they're simpler than\nmany of today's state",
    "start": "823060",
    "end": "830680"
  },
  {
    "text": "of the art methods. So here's another approach.",
    "start": "830680",
    "end": "836690"
  },
  {
    "text": "This was something we published\na few years ago, where",
    "start": "836690",
    "end": "841760"
  },
  {
    "text": "we started working with\nsome psychiatrists and said, could we predict\n30-day readmission",
    "start": "841760",
    "end": "849560"
  },
  {
    "text": "for a psychiatric patient with\nany degree of reliability?",
    "start": "849560",
    "end": "854779"
  },
  {
    "text": "That's a hard prediction. Willie is currently\nrunning an experiment where we're asking\npsychiatrists to predict that.",
    "start": "854780",
    "end": "863030"
  },
  {
    "text": "And it turns out, they're\nbarely better than chance at that prediction. So it's not an easy task.",
    "start": "863030",
    "end": "870800"
  },
  {
    "text": "And what we did is we said,\nwell, let's use topic modeling.",
    "start": "870800",
    "end": "875959"
  },
  {
    "text": "And so we had this\ncohort of patients, close to 5,000 patients.",
    "start": "875960",
    "end": "882320"
  },
  {
    "text": "About 10% of them\nwere readmitted with a psych diagnosis.",
    "start": "882320",
    "end": "887390"
  },
  {
    "text": "And almost 3,000 of\nthem were readmitted with other diagnoses.",
    "start": "887390",
    "end": "892790"
  },
  {
    "text": "So one thing this\ntells you right away is that if you're dealing\nwith psychiatric patients,",
    "start": "892790",
    "end": "898464"
  },
  {
    "text": "they come and go to the\nhospital frequently. And this is not good for the\nhospital's bottom line because",
    "start": "898465",
    "end": "905150"
  },
  {
    "text": "of reimbursement policies of\ninsurance companies and so on.",
    "start": "905150",
    "end": "910710"
  },
  {
    "text": "So of the 4,700, only 1,240 were\nnot readmitted within 30 days.",
    "start": "910710",
    "end": "917820"
  },
  {
    "text": "So there's very\nfrequent bounce-back. So we said, well, let's try\nbuilding a baseline model using",
    "start": "917820",
    "end": "927560"
  },
  {
    "text": "a support vector machine from\nbaseline clinical features like age, gender,\npublic health insurance",
    "start": "927560",
    "end": "934430"
  },
  {
    "text": "as a proxy for\nsocioeconomic status. So if you're on Medicaid,\nyou're probably poor.",
    "start": "934430",
    "end": "941210"
  },
  {
    "text": "And if you have\nprivate insurance, then you're probably an MIT\nemployee and/or better off.",
    "start": "941210",
    "end": "948829"
  },
  {
    "text": "So that's a frequently used\nproxy, a comorbidity index that tells you sort\nof how sick you",
    "start": "948830",
    "end": "955640"
  },
  {
    "text": "are from things other than\nyour psychiatric problems. And then we said,\nwell, what if we",
    "start": "955640",
    "end": "961160"
  },
  {
    "text": "add to that model\ncommon words from notes. So we said, let's do\na TF-IDF calculation.",
    "start": "961160",
    "end": "970200"
  },
  {
    "text": "So this is term frequency\ndivided by log of the document frequency.",
    "start": "970200",
    "end": "975510"
  },
  {
    "text": "So it's sort of, how\nspecific is a term to identify a particular\nkind of condition?",
    "start": "975510",
    "end": "982100"
  },
  {
    "text": "And we take the 1,000 most\ninformative words, and so there",
    "start": "982100",
    "end": "988579"
  },
  {
    "text": "are a lot of these. So if you use 1,000\nmost informative words from these nearly\n5,000 patients,",
    "start": "988580",
    "end": "997399"
  },
  {
    "text": "you wind up with something like\n66,000 words, unique words,",
    "start": "997400",
    "end": "1002860"
  },
  {
    "text": "that are informative\nfor some patient. But if you limit\nyourself to the top 10,",
    "start": "1002860",
    "end": "1010250"
  },
  {
    "text": "then it only uses 18,000 words. And if you limit\nyourself to the top one,",
    "start": "1010250",
    "end": "1015490"
  },
  {
    "text": "then it uses about 3,000 words. And then we said, well, instead\nof doing individual words,",
    "start": "1015490",
    "end": "1021550"
  },
  {
    "text": "let's do a latent\nDirichlet allocation. So topic modeling on all of\nthe words, as a bag of words--",
    "start": "1021550",
    "end": "1029140"
  },
  {
    "text": "so no sequence information,\njust the collection of words. And so we calculated\n75 topics from using",
    "start": "1029140",
    "end": "1039640"
  },
  {
    "text": "LDA on all these notes. So just to remind\nyou, the LDA process",
    "start": "1039640",
    "end": "1046839"
  },
  {
    "text": "is a model that says\nevery document consists of a certain mixture of topics,\nand each of those topics",
    "start": "1046839",
    "end": "1054670"
  },
  {
    "text": "probabilistically\ngenerates certain words. And so you can build\na model like this,",
    "start": "1054670",
    "end": "1062680"
  },
  {
    "text": "and then solve it using\ncomplicated techniques. And you'd wind up with topics,\nin this study, as follows.",
    "start": "1062680",
    "end": "1072218"
  },
  {
    "text": "I don't know. Can you read these? They may be too small. So these are\nunsupervised topics.",
    "start": "1072218",
    "end": "1081550"
  },
  {
    "text": "And if you look\nat the first one, it says patient, alcohol,\nwithdrawal, depression,",
    "start": "1081550",
    "end": "1086650"
  },
  {
    "text": "drinking, and Ativan,\nETOH, drinks, medications, clinic inpatient, diagnosis,\ndays, hospital, substance,",
    "start": "1086650",
    "end": "1096730"
  },
  {
    "text": "use treatment program, name. That's a de-identified\nuse/abuse problem number.",
    "start": "1096730",
    "end": "1104110"
  },
  {
    "text": "And we had our experts\nlook at these topics. And they said, oh,\nwell, that topic is related to alcohol abuse,\nwhich seems reasonable.",
    "start": "1104110",
    "end": "1113380"
  },
  {
    "text": "And then you see, on\nthe bottom, psychosis, thought features, paranoid\npsychosis, paranoia symptoms,",
    "start": "1113380",
    "end": "1121090"
  },
  {
    "text": "psychiatric, et cetera. And they said, OK,\nthat's a psychosis topic. So in retrospect, you can\nassign meaning to these topics.",
    "start": "1121090",
    "end": "1129760"
  },
  {
    "text": "But in fact, they're generated\nwithout any a priori notion of what they ought to be.",
    "start": "1129760",
    "end": "1135010"
  },
  {
    "text": "They're just a\nstatistical summarization of the common co-occurrences\nof words in these documents.",
    "start": "1135010",
    "end": "1143980"
  },
  {
    "text": "But what you find is that if you\nuse the baseline model, which",
    "start": "1143980",
    "end": "1151390"
  },
  {
    "text": "used just the demographic\nand clinical variables, and you say, what's the\ndifference in survival,",
    "start": "1151390",
    "end": "1159220"
  },
  {
    "text": "in this case, in\ntime to readmission between one set and\nanother in this cohort,",
    "start": "1159220",
    "end": "1168370"
  },
  {
    "text": "and the answer is\nthey're pretty similar. Whereas, if you use\na model that predicts",
    "start": "1168370",
    "end": "1174130"
  },
  {
    "text": "based on the baseline\nand 75 topics, the 75 topics that\nwe identified,",
    "start": "1174130",
    "end": "1180010"
  },
  {
    "text": "you get a much\nbigger separation. And of course, this is\nstatistically significant. And it tells you\nthat this technique",
    "start": "1180010",
    "end": "1187150"
  },
  {
    "text": "is useful for being\nable to improve the prediction of\na cohort that's",
    "start": "1187150",
    "end": "1194290"
  },
  {
    "text": "more likely to be readmitted\nfrom a cohort that's less likely to be readmitted.",
    "start": "1194290",
    "end": "1199320"
  },
  {
    "text": "It's not a terrific prediction. So the AUC for this model\nwas only on the order of 0.7.",
    "start": "1199320",
    "end": "1206960"
  },
  {
    "text": "So you know, it's not like 0.99. But nevertheless, it\nprovides useful information.",
    "start": "1206960",
    "end": "1216040"
  },
  {
    "text": "The same group of psychiatrists\nthat we worked with also did a study with a much larger\ncohort but much less rich data.",
    "start": "1216040",
    "end": "1225370"
  },
  {
    "text": "So they got all\nof the discharges from two medical centers\nover a period of 12 years.",
    "start": "1225370",
    "end": "1233120"
  },
  {
    "text": "So they had 845,000\ndischarges from 458,000",
    "start": "1233120",
    "end": "1238960"
  },
  {
    "text": "unique individuals. And they were looking for\nsuicide or other causes",
    "start": "1238960",
    "end": "1244480"
  },
  {
    "text": "of death in these\npatients to see if they could predict\nwhether somebody",
    "start": "1244480",
    "end": "1249910"
  },
  {
    "text": "is likely to try\nto harm themselves, or whether they're likely\nto die accidentally, which sometimes can't be\ndistinguished from suicide.",
    "start": "1249910",
    "end": "1259880"
  },
  {
    "text": "So the censoring problems\nthat David talked about are very much present in this.",
    "start": "1259880",
    "end": "1265190"
  },
  {
    "text": "Because you lose\ntrack of people. It's a highly\nimbalanced data set. Because out of the\n845,000 patients, only 235",
    "start": "1265190",
    "end": "1275990"
  },
  {
    "text": "committed suicide, which is, of\ncourse, probably a good thing from a societal point\nof view but makes",
    "start": "1275990",
    "end": "1282409"
  },
  {
    "text": "the data analysis hard. On the other hand, all-cause\nmortality was about 18%",
    "start": "1282410",
    "end": "1288230"
  },
  {
    "text": "during nine years\nof a follow-up. So that's not so imbalanced. And then what they\ndid is they curated",
    "start": "1288230",
    "end": "1295340"
  },
  {
    "text": "a list of 3,000 terms\nthat correspond to what, in the psychiatric literature,\nis called positive valence.",
    "start": "1295340",
    "end": "1303080"
  },
  {
    "text": "So this is concepts like joy\nand happiness and good stuff, as opposed to negative valence,\nlike depression and sorrow",
    "start": "1303080",
    "end": "1311720"
  },
  {
    "text": "and all that stuff. And they said, well, we can\nuse these types of terms",
    "start": "1311720",
    "end": "1318740"
  },
  {
    "text": "in order to help distinguish\namong these patients. And what they found is that, if\nyou plot the Kaplan-Meier curve",
    "start": "1318740",
    "end": "1327650"
  },
  {
    "text": "for different quartiles of\nrisk for these patients,",
    "start": "1327650",
    "end": "1334280"
  },
  {
    "text": "you see that there's a\npretty big difference between the different quartiles. And you can certainly\nidentify the people",
    "start": "1334280",
    "end": "1343460"
  },
  {
    "text": "who are more likely to commit\nsuicide from the people who are less likely to do so.",
    "start": "1343460",
    "end": "1349280"
  },
  {
    "text": "This curve is for suicide\nor accidental death. So this is a much\nlarger data set,",
    "start": "1349280",
    "end": "1356660"
  },
  {
    "text": "and therefore the\nerror bars are smaller. But you see the same\nkind of separation here.",
    "start": "1356660",
    "end": "1363059"
  },
  {
    "text": "So these are all\nuseful techniques. Now I'll to another approach.",
    "start": "1363060",
    "end": "1368930"
  },
  {
    "text": "This was work by one of\nmy students, Yuon Wo, who was working with some\nlymphoma pathologists",
    "start": "1368930",
    "end": "1376070"
  },
  {
    "text": "at Mass General. And so the approach\nthey took was to say, well, if you read a\npathology report about somebody",
    "start": "1376070",
    "end": "1386590"
  },
  {
    "text": "with lymphoma, can we\ntell what type of lymphoma they had from the\npathology report",
    "start": "1386590",
    "end": "1393190"
  },
  {
    "text": "if we blank out the part of\nthe pathology report that says, \"I, the pathologist, think\nthis person has non-Hodgkin's",
    "start": "1393190",
    "end": "1402340"
  },
  {
    "text": "lymphoma,\" or something? So from the rest of the context,\ncan we make that prediction?",
    "start": "1402340",
    "end": "1408880"
  },
  {
    "text": "Now, Yuon took a kind of\ninteresting, slightly odd approach to it,\nwhich is to treat",
    "start": "1408880",
    "end": "1415900"
  },
  {
    "text": "this as an unsupervised\nlearning problem rather than as a supervised\nlearning problem.",
    "start": "1415900",
    "end": "1421220"
  },
  {
    "text": "So he literally\nmasked the real answer and said, if we just treat\neverything except what",
    "start": "1421220",
    "end": "1428590"
  },
  {
    "text": "gives away the\nanswer as just data, can we essentially cluster that\ndata in some interesting way",
    "start": "1428590",
    "end": "1437030"
  },
  {
    "text": "so that we re-identify the\ndifferent types of lymphoma?",
    "start": "1437030",
    "end": "1442540"
  },
  {
    "text": "Now, the reason this\nturns out to be important is because lymphoma\npathologists keep",
    "start": "1442540",
    "end": "1447580"
  },
  {
    "text": "arguing about how to\nclassify lymphomas. And every few years, they\nrevise the classification rules.",
    "start": "1447580",
    "end": "1455920"
  },
  {
    "text": "And so part of his\nobjective was to say, let's try to provide an\nunbiased, data-driven method",
    "start": "1455920",
    "end": "1464320"
  },
  {
    "text": "that may help identify\nappropriate characteristics by which to classify\nthese different lymphomas.",
    "start": "1464320",
    "end": "1472570"
  },
  {
    "text": "So his approach was a tensor\nfactorization approach.",
    "start": "1472570",
    "end": "1477759"
  },
  {
    "text": " You often see data\nsets like this that's, say, patient\nby a characteristic.",
    "start": "1477760",
    "end": "1487180"
  },
  {
    "text": "So in this case,\nlaboratory measurements-- so systolic/diastolic blood\npressure, sodium, potassium,",
    "start": "1487180",
    "end": "1493179"
  },
  {
    "text": "et cetera. That's a very vanilla\nmatrix encoding of data. And then if you add a\nthird dimension to it,",
    "start": "1493180",
    "end": "1500350"
  },
  {
    "text": "like this is at the\ntime of admission, 30 minutes later, 60 minutes\nlater, 90 minutes later,",
    "start": "1500350",
    "end": "1506890"
  },
  {
    "text": "now you have a\nthree-dimensional tensor. And so just like you can\ndo matrix factorization, as",
    "start": "1506890",
    "end": "1514180"
  },
  {
    "text": "in the picture above, where\nwe say, my matrix of data,",
    "start": "1514180",
    "end": "1519400"
  },
  {
    "text": "I'm going to assume is generated\nby a product of two matrices,",
    "start": "1519400",
    "end": "1526130"
  },
  {
    "text": "which are smaller in dimension. And you can train\nthis by saying,",
    "start": "1526130",
    "end": "1531610"
  },
  {
    "text": "I want entries in\nthese two matrices that minimize the\nreconstruction error.",
    "start": "1531610",
    "end": "1537860"
  },
  {
    "text": "So if I multiply these\nmatrices together, then I get back my\noriginal matrix plus error.",
    "start": "1537860",
    "end": "1546350"
  },
  {
    "text": "And I want to\nminimize that error, usually root mean square, or\nmean square error, or something",
    "start": "1546350",
    "end": "1551860"
  },
  {
    "text": "like that. Well, you can play the\nsame game for a tensor",
    "start": "1551860",
    "end": "1557230"
  },
  {
    "text": "by having a so-called core\ntensor, which identifies",
    "start": "1557230",
    "end": "1562900"
  },
  {
    "text": "the subset of characteristics\nthat subdivide",
    "start": "1562900",
    "end": "1574660"
  },
  {
    "text": "that dimension of your data. And then what you\ndo is the same game.",
    "start": "1574660",
    "end": "1580630"
  },
  {
    "text": "You have matrices corresponding\nto each of the dimensions.",
    "start": "1580630",
    "end": "1586810"
  },
  {
    "text": "And if you multiply\nthis core tensor by each of these\nmatrices, you reconstruct",
    "start": "1586810",
    "end": "1592240"
  },
  {
    "text": "the original tensor. And you can train it again to\nminimize the reconstruction",
    "start": "1592240",
    "end": "1597730"
  },
  {
    "text": "loss. So there are, again,\na few more tricks.",
    "start": "1597730",
    "end": "1603130"
  },
  {
    "text": "Because this is\ndealing with language. And so this is a typical report\nfrom one of these lymphoma",
    "start": "1603130",
    "end": "1610660"
  },
  {
    "text": "pathologists that says\nimmunohistochemical stains show that the follicles-- blah,\nblah, blah, blah, blah--",
    "start": "1610660",
    "end": "1618850"
  },
  {
    "text": "so lots and lots of details. And so he needed a\nrepresentation that",
    "start": "1618850",
    "end": "1625000"
  },
  {
    "text": "could be put into\nthis matrix tensor, this tensor factorization form.",
    "start": "1625000",
    "end": "1633610"
  },
  {
    "text": "And what he did is to\nsay, well, let's see. If we look at a\nstatement like this,",
    "start": "1633610",
    "end": "1638770"
  },
  {
    "text": "immuno stains show that\nlarge atypical cells are strongly positive for\nCD30, negative for these other",
    "start": "1638770",
    "end": "1648520"
  },
  {
    "text": "surface expressions. So the sentence tells us\nrelationships among procedures,",
    "start": "1648520",
    "end": "1655480"
  },
  {
    "text": "types of cells, and\nimmunologic factors. And for feature choice,\nwe can use words.",
    "start": "1655480",
    "end": "1663010"
  },
  {
    "text": "Or we can use UMLS concepts. Or we can find various\nkinds of mappings.",
    "start": "1663010",
    "end": "1668590"
  },
  {
    "text": "But he decided that\nin order to retain",
    "start": "1668590",
    "end": "1673900"
  },
  {
    "text": "the syntactic relationships\nhere, what he would do is to use a graphical\nrepresentation that",
    "start": "1673900",
    "end": "1681760"
  },
  {
    "text": "came out of, again, parsing\nall of these sentences. And so what you get is that\nthis creates one graph that",
    "start": "1681760",
    "end": "1691780"
  },
  {
    "text": "talks about the strongly\npositive for CD30,",
    "start": "1691780",
    "end": "1697750"
  },
  {
    "text": "large atypical cells, et cetera. And then you can factor\nthis into subgraphs.",
    "start": "1697750",
    "end": "1704470"
  },
  {
    "text": "And then you also have\nto identify frequently occurring subgraphs.",
    "start": "1704470",
    "end": "1709570"
  },
  {
    "text": "So for example,\nlarge atypical cells appears here, and also appears\nthere, and of course will",
    "start": "1709570",
    "end": "1716380"
  },
  {
    "text": "appear in many other places. Yeah? AUDIENCE: Is this parsing\ndomain in language diagnostics?",
    "start": "1716380",
    "end": "1722595"
  },
  {
    "text": "For example, did\nthey incorporate some sort of medical\ninformation here or some sort of linguistic-- PETER SZOLOVITS: So in\nthis particular study,",
    "start": "1722595",
    "end": "1729390"
  },
  {
    "text": "he was using the Stanford\nParser with some tricks. So the Stanford\nParser doesn't know",
    "start": "1729390",
    "end": "1735780"
  },
  {
    "text": "a lot of the medical words. And so he basically marked\nthese things as noun phrases.",
    "start": "1735780",
    "end": "1743640"
  },
  {
    "text": "And then the\nStanford Parser also doesn't do well with\nlong lists like the set",
    "start": "1743640",
    "end": "1750200"
  },
  {
    "text": "of immune features. And so he would recognize\nthose as a pattern,",
    "start": "1750200",
    "end": "1758100"
  },
  {
    "text": "substitute a single\nmade-up word for them, and that made the parser\nwork much better on it.",
    "start": "1758100",
    "end": "1764850"
  },
  {
    "text": "So there were a whole\nbunch of little tricks like that in order to adapt it. But it was not a model\ntrained specifically on this.",
    "start": "1764850",
    "end": "1773160"
  },
  {
    "text": "I think it's trained on\nWall Street Journal corpus or something like that. So it's general English.",
    "start": "1773160",
    "end": "1779667"
  },
  {
    "text": "AUDIENCE: Those are things that\nhe did manually as opposed to, say, [INAUDIBLE]? PETER SZOLOVITS: No.",
    "start": "1779667",
    "end": "1785280"
  },
  {
    "text": "He did it algorithmically,\nbut he didn't learn which algorithms to use. He made them up by hand.",
    "start": "1785280",
    "end": "1792299"
  },
  {
    "text": "But then, of course,\nit's a big corpus. And he ran these\nprograms over it that did those transformations.",
    "start": "1792300",
    "end": "1798809"
  },
  {
    "text": "So he calls it\ntwo-phase parsing. There's a reference to his\npaper on the first slide",
    "start": "1798810",
    "end": "1805950"
  },
  {
    "text": "in this section if you're\ninterested in the details. It's described there.",
    "start": "1805950",
    "end": "1811470"
  },
  {
    "text": "So what he wound\nup with is a tensor that has patients on one\naxis, the words appearing",
    "start": "1811470",
    "end": "1820890"
  },
  {
    "text": "in the text on another axis. So he's still using a\nbag-of-words representation.",
    "start": "1820890",
    "end": "1827730"
  },
  {
    "text": "But the third axis is\nthese language concept subgraphs that we\nwere talking about.",
    "start": "1827730",
    "end": "1833650"
  },
  {
    "text": "And then he does tensor\nfactorization on this. And what's interesting\nis that it works",
    "start": "1833650",
    "end": "1840360"
  },
  {
    "text": "much better than I expected. So if you look at his technique,\nwhich he called SANTF,",
    "start": "1840360",
    "end": "1849540"
  },
  {
    "text": "the precision and recall\nare about 0.72 and 0.854",
    "start": "1849540",
    "end": "1855450"
  },
  {
    "text": "macro-average and\n0.754 micro-average, which is much better than\nthe non-negative matrix",
    "start": "1855450",
    "end": "1864510"
  },
  {
    "text": "factorization results, which\nonly use patient by word or patient by subgraph, or,\nin fact, one where you simply",
    "start": "1864510",
    "end": "1874860"
  },
  {
    "text": "do patient and concatenate\nthe subgraphs and the words in one dimension.",
    "start": "1874860",
    "end": "1880740"
  },
  {
    "text": "So that means that this is\nactually taking advantage of the three-way relationship.",
    "start": "1880740",
    "end": "1887090"
  },
  {
    "text": "If you read papers from\nabout 15, 20 years ago, people got very excited about\nthe idea of bi-clustering,",
    "start": "1887090",
    "end": "1895680"
  },
  {
    "text": "which is, in modern terms,\nthe equivalent of matrix factorization.",
    "start": "1895680",
    "end": "1901590"
  },
  {
    "text": "So it says given two\ndimensions of data, and I want to\ncluster things, but I",
    "start": "1901590",
    "end": "1908570"
  },
  {
    "text": "want to cluster\nthem in such a way that the clustering\nof one dimension helps the clustering\nof the other dimension.",
    "start": "1908570",
    "end": "1916370"
  },
  {
    "text": "So this is a formal way of doing\nthat relatively efficiently. And tensor factorization is\nessentially tri-clustering.",
    "start": "1916370",
    "end": "1924170"
  },
  {
    "text": " So now I'm going to turn to\nthe last of today's big topics,",
    "start": "1924170",
    "end": "1933320"
  },
  {
    "text": "which is language modeling. And this is really where\nthe action is nowadays in natural language\nprocessing in general.",
    "start": "1933320",
    "end": "1941210"
  },
  {
    "text": "I would say that the\nnatural language processing on clinical data is\nsomewhat behind the state",
    "start": "1941210",
    "end": "1948010"
  },
  {
    "text": "of the art in natural\nlanguage processing overall. There are fewer corpora\nthat are available.",
    "start": "1948010",
    "end": "1954520"
  },
  {
    "text": "There are fewer\npeople working on it. And so we're catching up.",
    "start": "1954520",
    "end": "1960460"
  },
  {
    "text": "But I'm going to lead\ninto this somewhat gently. So what does it mean\nto model a language?",
    "start": "1960460",
    "end": "1967660"
  },
  {
    "text": "I mean, you could imagine\nsaying it's coming up with a set of parsing rules that\ndefine the syntactic structure",
    "start": "1967660",
    "end": "1975550"
  },
  {
    "text": "of the language. Or you could imagine\nsaying, as we suggested last time, coming up\nwith a corresponding set",
    "start": "1975550",
    "end": "1983350"
  },
  {
    "text": "of semantic rules\nthat say a concept or terms in the language\ncorrespond to certain concepts",
    "start": "1983350",
    "end": "1990970"
  },
  {
    "text": "and that they are\na combinatorially, functionally combined\nas the syntax directs,",
    "start": "1990970",
    "end": "1997420"
  },
  {
    "text": "in order to give us a\nsemantic representation. So we don't know how to do\neither of those very well.",
    "start": "1997420",
    "end": "2004090"
  },
  {
    "text": "And so the current,\nthe contemporary idea about language\nmodeling is to say,",
    "start": "2004090",
    "end": "2010200"
  },
  {
    "text": "given a sequence of tokens,\npredict the next token. ",
    "start": "2010200",
    "end": "2015750"
  },
  {
    "text": "If you could do that\nperfectly, presumably you would have a good\nlanguage model.",
    "start": "2015750",
    "end": "2021480"
  },
  {
    "text": "So obviously, you\ncan't do it perfectly. Because we don't always\nsay the same word",
    "start": "2021480",
    "end": "2027910"
  },
  {
    "text": "after some sequence of\nprevious words when we speak. But probabilistically,\nyou can get close to that.",
    "start": "2027910",
    "end": "2036220"
  },
  {
    "text": "And there's usually some\nkind of Markov assumption that says that the probability\nof emitting a token",
    "start": "2036220",
    "end": "2044380"
  },
  {
    "text": "given the stuff that came before\nit is ordinarily dependent",
    "start": "2044380",
    "end": "2050230"
  },
  {
    "text": "only on n previous words\nrather than on all of history,",
    "start": "2050230",
    "end": "2058600"
  },
  {
    "text": "on everything you've ever\nsaid before in your life. ",
    "start": "2058600",
    "end": "2064480"
  },
  {
    "text": "And there's a measure\ncalled perplexity,",
    "start": "2064480",
    "end": "2070570"
  },
  {
    "text": "which is the entropy of the\nprobability distribution over the predicted words.",
    "start": "2070570",
    "end": "2076570"
  },
  {
    "text": "And roughly speaking, it's\nthe number of likely ways that you could continue the\ntext if all of the possibilities",
    "start": "2076570",
    "end": "2087609"
  },
  {
    "text": "were equally likely. So perplexity is often used, for\nexample, in speech processing.",
    "start": "2087610",
    "end": "2094989"
  },
  {
    "text": " We did a study\nwhere we were trying to build a speech system that\nunderstood a conversation",
    "start": "2094989",
    "end": "2103280"
  },
  {
    "text": "between a doctor and a patient. And we ran into real\nproblems, because we were using software that had\nbeen developed to interpret",
    "start": "2103280",
    "end": "2112280"
  },
  {
    "text": "dictation by doctors. And that was very well trained. But it turned out-- we didn't\nknow this when we started--",
    "start": "2112280",
    "end": "2119990"
  },
  {
    "text": "that the language that doctors\nuse in dictating medical notes is pretty straightforward,\npretty simple.",
    "start": "2119990",
    "end": "2127490"
  },
  {
    "text": "And so it's perplexity\nis about nine,",
    "start": "2127490",
    "end": "2132730"
  },
  {
    "text": "whereas conversations are much\nmore free flowing and cover many more topics.",
    "start": "2132730",
    "end": "2138700"
  },
  {
    "text": "And so its perplexity\nis about 73. And so the model that works\nwell for perplexity nine",
    "start": "2138700",
    "end": "2146230"
  },
  {
    "text": "doesn't work as well\nfor perplexity 73. And so what this tells you about\nthe difficulty of accurately",
    "start": "2146230",
    "end": "2154840"
  },
  {
    "text": "transcribing speech\nis that it's hard. It's much harder. And that's still not\na solved problem.",
    "start": "2154840",
    "end": "2162930"
  },
  {
    "text": "Now, you probably all\nknow about Zipf's law. So if you empirically\njust take all the words",
    "start": "2162930",
    "end": "2170790"
  },
  {
    "text": "in all the literature of, let's\nsay, English, what you discover is that the n-th word\nis about one over n",
    "start": "2170790",
    "end": "2180990"
  },
  {
    "text": "as probable as the first word. So there is a\nlong-tailed distribution.",
    "start": "2180990",
    "end": "2188000"
  },
  {
    "text": "One thing you should\nrealize, of course, is if you integrate one over\nn from zero to infinity,",
    "start": "2188000",
    "end": "2193280"
  },
  {
    "text": "it's infinite. And that may not be an\ninaccurate representation",
    "start": "2193280",
    "end": "2199700"
  },
  {
    "text": "of language, because language\nis productive and changes. And people make up new words\nall the time and so on.",
    "start": "2199700",
    "end": "2207859"
  },
  {
    "text": "So it may actually be infinite. But roughly speaking, there is\na kind of decline like this.",
    "start": "2207860",
    "end": "2214260"
  },
  {
    "text": "And interestingly,\nin the brown corpus, the top 10 words make\nup almost a quarter",
    "start": "2214260",
    "end": "2221540"
  },
  {
    "text": "of the size of the corpus. So you write a lot of thes,\nofs, ands, a's, twos, ins,",
    "start": "2221540",
    "end": "2227500"
  },
  {
    "text": "et cetera, and much less\nhematemesis, obviously.",
    "start": "2227500",
    "end": "2234470"
  },
  {
    "text": " So what about n-gram models?",
    "start": "2234470",
    "end": "2239589"
  },
  {
    "text": "Well, remember, if we make\nthis Markov assumption, then all we have to\ndo is pay attention",
    "start": "2239590",
    "end": "2245530"
  },
  {
    "text": "to the last n tokens\nbefore the one that we're interested\nin predicting. And so people have generated\nthese large corpora n-grams.",
    "start": "2245530",
    "end": "2254800"
  },
  {
    "text": "So for example, somebody,\na couple of decades ago, took all of\nShakespeare's writings--",
    "start": "2254800",
    "end": "2261700"
  },
  {
    "text": "I think they were\ntrying to decide whether he had\nwritten all his works or whether the earl\nof somebody or other",
    "start": "2261700",
    "end": "2269230"
  },
  {
    "text": "was actually the guy\nwho wrote Shakespeare. You know about this controversy?",
    "start": "2269230",
    "end": "2274810"
  },
  {
    "text": "Yeah. So that's why they\nwere doing it. But anyway, they\ncreated this corpus.",
    "start": "2274810",
    "end": "2280890"
  },
  {
    "text": "And they said--\nso Shakespeare had a vocabulary of about\n30,000 words and about",
    "start": "2280890",
    "end": "2287790"
  },
  {
    "text": "300,000 bigrams, and out of\n844 million possible bigrams.",
    "start": "2287790",
    "end": "2297480"
  },
  {
    "text": "So 99.96% of bigrams\nwere never seen.",
    "start": "2297480",
    "end": "2302970"
  },
  {
    "text": "So there's a certain regularity\nto his production of language. Now, Google, of course,\ndid Shakespeare one better.",
    "start": "2302970",
    "end": "2310309"
  },
  {
    "text": "And they said, hmm, we can\ntake a terabyte corpus-- this was in 2006.",
    "start": "2310310",
    "end": "2316230"
  },
  {
    "text": "I wouldn't be surprised if\nit's a petabyte corpus today. And they published this.",
    "start": "2316230",
    "end": "2321539"
  },
  {
    "text": "They just made it available. So there were 13.6\nmillion unique words that occurred at least 200\ntimes in this tera-word corpus.",
    "start": "2321540",
    "end": "2331290"
  },
  {
    "text": "And there were 1.2 billion\nfive-word sequences that occurred at least 40 times.",
    "start": "2331290",
    "end": "2337500"
  },
  {
    "text": "So these are the statistics. And if you're interested,\nthere's a URL. And here's a very tiny\npart of their database.",
    "start": "2337500",
    "end": "2345240"
  },
  {
    "text": "So ceramics, collectibles,\ncollectibles--",
    "start": "2345240",
    "end": "2351210"
  },
  {
    "text": "I don't know-- occurred 55\ntimes in a terabyte of text.",
    "start": "2351210",
    "end": "2356550"
  },
  {
    "text": "Ceramics collectibles fine,\nceramics collectibles by, pottery, cooking, comma, period,\nend of sentence, and, at,",
    "start": "2356550",
    "end": "2365140"
  },
  {
    "text": "is, et cetera-- different number of times. Ceramics comes from\noccurred 660 times,",
    "start": "2365140",
    "end": "2372640"
  },
  {
    "text": "which is reasonably large\nnumber compared to some of its competitors here.",
    "start": "2372640",
    "end": "2377920"
  },
  {
    "text": "If you look at\nfour-grams, you see things like serve as the\nincoming, blah, blah,",
    "start": "2377920",
    "end": "2384069"
  },
  {
    "text": "blah, 92 times; serve\nas the index, 223 times;",
    "start": "2384070",
    "end": "2389500"
  },
  {
    "text": "serve as the\ninitial, 5,300 times. So you've got all\nthese statistics.",
    "start": "2389500",
    "end": "2396730"
  },
  {
    "text": "And now, given those statistics,\nwe can then build a generator.",
    "start": "2396730",
    "end": "2402430"
  },
  {
    "text": "So we can say, all right. Suppose I start with\nthe token, which",
    "start": "2402430",
    "end": "2408700"
  },
  {
    "text": "is the beginning of a\nsentence, or the separator between sentences. And I say sample a\nrandom bigram starting",
    "start": "2408700",
    "end": "2416380"
  },
  {
    "text": "with the beginning of\na sentence and a word, according to its probability,\nand then sample the next bigram",
    "start": "2416380",
    "end": "2424240"
  },
  {
    "text": "from that word and\nall the other words, according to its\nprobability, and keep",
    "start": "2424240",
    "end": "2430630"
  },
  {
    "text": "doing that until you hit\nthe end of sentence marker. So for example, here I'm\ngenerating the sentence,",
    "start": "2430630",
    "end": "2440020"
  },
  {
    "text": "I, starts with I,\nthen followed by want, followed by two, followed\nby get, followed by Chinese,",
    "start": "2440020",
    "end": "2447730"
  },
  {
    "text": "followed by food, followed\nby end of sentence. So I've just generated,\n\"I want to get",
    "start": "2447730",
    "end": "2453100"
  },
  {
    "text": "Chinese food,\" which sounds\nlike a perfectly good sentence. So here's what's interesting.",
    "start": "2453100",
    "end": "2459170"
  },
  {
    "text": "If you look back again\nat the Shakespeare corpus and saying, if we generated\nShakespeare from unigrams,",
    "start": "2459170",
    "end": "2467220"
  },
  {
    "text": "you get stuff like\nat the top, \"To him swallowed confess here both. Which.",
    "start": "2467220",
    "end": "2473540"
  },
  {
    "text": "Of save on trail for are ay\ndevice and rote life have.\"",
    "start": "2473540",
    "end": "2479100"
  },
  {
    "text": "It doesn't sound terribly good. It's not very grammatical. It doesn't have that sort of\nShakespearean English flavor.",
    "start": "2479100",
    "end": "2490140"
  },
  {
    "text": "Although, you do have words\nlike nave and ay and so on that are vaguely reminiscent.",
    "start": "2490140",
    "end": "2496680"
  },
  {
    "text": "Now, if you go to\nbigrams, it starts to sound a little better. \"What means, sir. I confess she?",
    "start": "2496680",
    "end": "2503040"
  },
  {
    "text": "Then all sorts, he\nis trim, captain.\" ",
    "start": "2503040",
    "end": "2509400"
  },
  {
    "text": "That doesn't make any sense. But it starts to\nsound a little better. And with trigrams, we get,\n\"Sweet prince, Falstaff",
    "start": "2509400",
    "end": "2516869"
  },
  {
    "text": "shall die. Harry of Monmouth,\" et cetera. So this is beginning to\nsound a little Shakespearean.",
    "start": "2516870",
    "end": "2525540"
  },
  {
    "text": "And if you go to quadrigrams,\nyou get, \"King Henry. What? I will go seek the\ntraitor Gloucester.",
    "start": "2525540",
    "end": "2531180"
  },
  {
    "text": "Exeunt some of the watch. A great banquet\nserv'd in,\" et cetera.",
    "start": "2531180",
    "end": "2537960"
  },
  {
    "text": "I mean, when I first saw this,\nlike 20 years ago or something,",
    "start": "2537960",
    "end": "2543089"
  },
  {
    "text": "I was stunned. This is actually\ngenerating stuff that sounds vaguely\nShakespearean and vaguely",
    "start": "2543090",
    "end": "2550170"
  },
  {
    "text": "English-like. Here's an example of generating\nthe Wall Street Journal.",
    "start": "2550170",
    "end": "2557070"
  },
  {
    "text": "So from unigrams, \"Months\nthe my and issue of year",
    "start": "2557070",
    "end": "2562410"
  },
  {
    "text": "foreign new exchanges\nSeptember were recession.\" It's word salad.",
    "start": "2562410",
    "end": "2567600"
  },
  {
    "text": "But if you go to trigrams,\n\"They also point to ninety nine point six billion\nfrom two hundred four",
    "start": "2567600",
    "end": "2574020"
  },
  {
    "text": "oh six three percent of the\nrates of interest stores as Mexico and Brazil.\"",
    "start": "2574020",
    "end": "2580050"
  },
  {
    "text": "So you could imagine that this\nis some Wall Street Journal writer on acid\nwriting this text.",
    "start": "2580050",
    "end": "2589080"
  },
  {
    "text": "Because it has a little bit\nof the right kind of flavor. So more recently,\npeople said, well,",
    "start": "2589080",
    "end": "2597570"
  },
  {
    "text": "we ought to be able to make use\nof this in some systematic way to help us with our\nlanguage analysis tasks.",
    "start": "2597570",
    "end": "2605730"
  },
  {
    "text": "So to me, the first\neffort in this direction",
    "start": "2605730",
    "end": "2611790"
  },
  {
    "text": "was Word2Vec, which\nwas Mikolov's approach to doing this. And he developed two models.",
    "start": "2611790",
    "end": "2618540"
  },
  {
    "text": "He said, let's build a\ncontinuous bag-of-words model",
    "start": "2618540",
    "end": "2625260"
  },
  {
    "text": "that says what\nwe're going to use is co-occurrence data on a\nseries of tokens in the text",
    "start": "2625260",
    "end": "2634849"
  },
  {
    "text": "that we're trying to model. And we're going to\nuse a neural network model to predict the word\nfrom the words around it.",
    "start": "2634850",
    "end": "2645250"
  },
  {
    "text": "And in that process,\nwe're going to use the parameters of that neural\nnetwork model as a vector.",
    "start": "2645250",
    "end": "2653910"
  },
  {
    "text": "And that vector will be the\nrepresentation of that word. ",
    "start": "2653910",
    "end": "2659590"
  },
  {
    "text": "And so what we're\ngoing to find is that words that tend to\nappear in the same context",
    "start": "2659590",
    "end": "2666580"
  },
  {
    "text": "will have similar\nrepresentations in this high-dimensional vector.",
    "start": "2666580",
    "end": "2671835"
  },
  {
    "text": "And by the way,\nhigh-dimensional, people typically use like 300\nor 500 dimensional vectors.",
    "start": "2671835",
    "end": "2677670"
  },
  {
    "text": "So there's a lot of-- it's a big space. And the words are\nscattered throughout this.",
    "start": "2677670",
    "end": "2683860"
  },
  {
    "text": "But you get this\nkind of cohesion, where words that are\nused in the same context",
    "start": "2683860",
    "end": "2693470"
  },
  {
    "text": "appear close to each other. And the extrapolation\nof that is that if words are used in the\nsame context, maybe",
    "start": "2693470",
    "end": "2700500"
  },
  {
    "text": "they share something\nabout meaning. So the other model\nis a skip-gram model,",
    "start": "2700500",
    "end": "2706404"
  },
  {
    "text": "where you're doing\nthe prediction in the other direction. From a word, you're predicting\nthe words that are around it.",
    "start": "2706405",
    "end": "2713299"
  },
  {
    "text": "And again, you are using\na neural network model to do that. And you use the\nparameters of that model",
    "start": "2713300",
    "end": "2720800"
  },
  {
    "text": "in order to represent the\nword that you're focused on.",
    "start": "2720800",
    "end": "2727050"
  },
  {
    "text": "So what came as a surprise\nto me is this claim that's in his original paper, which\nis that not only do you",
    "start": "2727050",
    "end": "2735830"
  },
  {
    "text": "get this effect of locality\nas corresponding meaning",
    "start": "2735830",
    "end": "2743030"
  },
  {
    "text": "but that you get relationships\nthat are geometrically represented in the space\nof these embeddings.",
    "start": "2743030",
    "end": "2750769"
  },
  {
    "text": "And so what you\nsee is that if you take the encoding of the\nword man and the word woman",
    "start": "2750770",
    "end": "2758510"
  },
  {
    "text": "and look at the vector\ndifference between them, and then apply that same\nvector difference to king,",
    "start": "2758510",
    "end": "2765530"
  },
  {
    "text": "you get close to queen. And if you apply it uncle,\nyou get close to aunt.",
    "start": "2765530",
    "end": "2771410"
  },
  {
    "text": "And so they showed a\nnumber of examples. And then people\nhave studied this. It doesn't hold\nit perfectly well.",
    "start": "2771410",
    "end": "2777500"
  },
  {
    "text": "I mean, it's not like we've\nsolved the semantics problem. But it is a genuine\nrelationship.",
    "start": "2777500",
    "end": "2784040"
  },
  {
    "text": "The place where it\ndoesn't work well is when some of these things are\nmuch more frequent than others.",
    "start": "2784040",
    "end": "2790460"
  },
  {
    "text": "And so one of the examples\nthat's often cited is if you go, London is to\nEngland as Paris is to France,",
    "start": "2790460",
    "end": "2801420"
  },
  {
    "text": "and that one works. But then you say as Kuala\nLumpur is to Malaysia,",
    "start": "2801420",
    "end": "2807950"
  },
  {
    "text": "and that one doesn't\nwork so well. And then you go, as\nJuba or something",
    "start": "2807950",
    "end": "2817310"
  },
  {
    "text": "is to whatever country\nit's the capital of. And since we don't write about\nAfrica in our newspapers,",
    "start": "2817310",
    "end": "2825140"
  },
  {
    "text": "there's very little\ndata on that. And so that doesn't\nwork so well.",
    "start": "2825140",
    "end": "2830420"
  },
  {
    "text": "So there was this\nother paper later from van der Maaten\nand Geoff Hinton,",
    "start": "2830420",
    "end": "2836960"
  },
  {
    "text": "where they came up with\na visualization method to take these\nhigh-dimensional vectors",
    "start": "2836960",
    "end": "2842180"
  },
  {
    "text": "and visualize them\nin two dimensions. And what you see is that if\nyou take a bunch of concepts",
    "start": "2842180",
    "end": "2848750"
  },
  {
    "text": "that are count concepts-- so 1/2, 30, 15, 5, 4, 2,\n3, several, some, many,",
    "start": "2848750",
    "end": "2856490"
  },
  {
    "text": "et cetera-- there is a geometric\nrelationship between them. So they, in fact, do map to\nthe same part of the space.",
    "start": "2856490",
    "end": "2865380"
  },
  {
    "text": "Similarly, minister, leader,\npresident, chairman, director, spokesman, chief,\nhead, et cetera",
    "start": "2865380",
    "end": "2871580"
  },
  {
    "text": "form a kind of\ncluster in the space. So there's definitely\nsomething to this.",
    "start": "2871580",
    "end": "2878540"
  },
  {
    "text": "I promised you that I would\nget back to a different attempt",
    "start": "2878540",
    "end": "2884120"
  },
  {
    "text": "to try to take a\ncore of concepts that you want to use\nfor term-spotting",
    "start": "2884120",
    "end": "2889640"
  },
  {
    "text": "and develop an automated way of\nenlarging that set of concepts in order to give you a\nricher vocabulary by which",
    "start": "2889640",
    "end": "2897080"
  },
  {
    "text": "to try to identify cases\nthat you're interested in. So this was by some\nof my colleagues,",
    "start": "2897080",
    "end": "2903480"
  },
  {
    "text": "including Kat, who\nyou saw on Tuesday. And they said,\nwell, what we'd like",
    "start": "2903480",
    "end": "2912800"
  },
  {
    "text": "is the fully automated and\nrobust, unsupervised feature selection method that\nleverages only publicly",
    "start": "2912800",
    "end": "2918860"
  },
  {
    "text": "available medical knowledge\nsources instead of VHR data. So the method that David's\ngroup had developed,",
    "start": "2918860",
    "end": "2926690"
  },
  {
    "text": "which we talked about\nearlier, uses data from electronic\nhealth records, which",
    "start": "2926690",
    "end": "2931790"
  },
  {
    "text": "means that you move\nto different hospitals and there may be\ndifferent conventions. And you might\nimagine that you have",
    "start": "2931790",
    "end": "2938390"
  },
  {
    "text": "to retrain that sort of method,\nwhereas here the idea is",
    "start": "2938390",
    "end": "2943880"
  },
  {
    "text": "to derive these surrogate\nfeatures from knowledge sources. So unlike that earlier model,\nhere they built a Word2Vec",
    "start": "2943880",
    "end": "2953330"
  },
  {
    "text": "skip-gram model from about 5\nmillion Springer articles-- so these are published\nmedical articles--",
    "start": "2953330",
    "end": "2961610"
  },
  {
    "text": "to yield 500 dimensional\nvectors for each word. And then what they did is\nthey took the concept names",
    "start": "2961610",
    "end": "2969799"
  },
  {
    "text": "that they were interested\nin and their definitions from the UMLS, and\nthen they summoned",
    "start": "2969800",
    "end": "2978579"
  },
  {
    "text": "the word vectors for each\nof these words, weighted by inverse document frequency.",
    "start": "2978580",
    "end": "2984650"
  },
  {
    "text": "So it's sort of a\nTF-IDF-like approach to weight different words.",
    "start": "2984650",
    "end": "2991240"
  },
  {
    "text": "And then they went\nout and they said, OK, for every disease that's\nmentioned in Wikipedia,",
    "start": "2991240",
    "end": "2996609"
  },
  {
    "text": "Medscape, eMedicine, the\nMerck Manuals Professional Edition, the Mayo Clinic\nDiseases and Conditions,",
    "start": "2996610",
    "end": "3003390"
  },
  {
    "text": "MedlinePlus Medical\nEncyclopedia, they used named entity\nrecognition techniques",
    "start": "3003390",
    "end": "3009330"
  },
  {
    "text": "to find all the concepts that\nare related to this phenotype.",
    "start": "3009330",
    "end": "3015550"
  },
  {
    "text": "So then they said, well,\nthere's a lot of randomness in these sources, and maybe\nin our extraction techniques.",
    "start": "3015550",
    "end": "3022840"
  },
  {
    "text": "But if we insist that\nsome concept appear in at least three of\nthese five sources,",
    "start": "3022840",
    "end": "3028810"
  },
  {
    "text": "then we can be pretty confident\nthat it's a relevant concept. And so they said,\nOK, we'll do that.",
    "start": "3028810",
    "end": "3034480"
  },
  {
    "text": "Then they chose\nthe top k concepts whose embedding vectors are\nclosest by cosine distance",
    "start": "3034480",
    "end": "3041190"
  },
  {
    "text": "to the embedding\nof this phenotype that they've calculated. And they say, OK,\nthe phenotype is",
    "start": "3041190",
    "end": "3047280"
  },
  {
    "text": "going to be a linear combination\nof all these related concepts. So again, this is a bit\nsimilar to what we saw before.",
    "start": "3047280",
    "end": "3055840"
  },
  {
    "text": "But here, instead of\nextracting the data from electronic medical\nrecords, they're",
    "start": "3055840",
    "end": "3061110"
  },
  {
    "text": "extracting it from published\nliterature and these web sources.",
    "start": "3061110",
    "end": "3067260"
  },
  {
    "text": "And again, what you see is that\nthe expert-curated features",
    "start": "3067260",
    "end": "3076230"
  },
  {
    "text": "for these five phenotypes,\nwhich are coronary artery",
    "start": "3076230",
    "end": "3082050"
  },
  {
    "text": "disease, rheumatoid\narthritis, Crohn's disease, ulcerative colitis,\nand pediatric pulmonary arterial",
    "start": "3082050",
    "end": "3089069"
  },
  {
    "text": "hypertension, they started\nwith 20 to 50 curated features.",
    "start": "3089070",
    "end": "3097260"
  },
  {
    "text": "So these were the\nones that the doctors said, OK, these are the\nanchors in David's terminology.",
    "start": "3097260",
    "end": "3104609"
  },
  {
    "text": "And then they expanded\nthese to a larger set",
    "start": "3104610",
    "end": "3111090"
  },
  {
    "text": "using the technique that I just\ndescribed, and then selected",
    "start": "3111090",
    "end": "3116850"
  },
  {
    "text": "down to the top n that\nwere effective in finding",
    "start": "3116850",
    "end": "3124515"
  },
  {
    "text": "relevant phenotypes. And this is a terrible graph\nthat summarizes the results.",
    "start": "3124515",
    "end": "3133140"
  },
  {
    "text": "But what you're seeing is that\nthe orange lines are based",
    "start": "3133140",
    "end": "3139589"
  },
  {
    "text": "on the expert-curated features. This is based on an earlier\nversion of trying to do this.",
    "start": "3139590",
    "end": "3148920"
  },
  {
    "text": "And SEDFE is the technique\nthat I've just described. And what you see is that\nthe automatic techniques",
    "start": "3148920",
    "end": "3157410"
  },
  {
    "text": "for many of these phenotypes\nare just about as good as the manually curated ones.",
    "start": "3157410",
    "end": "3164760"
  },
  {
    "text": "And of course, they require\nmuch less manual curation. Because they're using this\nautomatic learning approach.",
    "start": "3164760",
    "end": "3172980"
  },
  {
    "text": "Another interesting\nexample to return to the theme of\nde-identification",
    "start": "3172980",
    "end": "3178770"
  },
  {
    "text": "is a couple of my\nstudents, a few years ago, built a new de-identifier\nthat has this rather",
    "start": "3178770",
    "end": "3186150"
  },
  {
    "text": "complicated architecture. So it starts with a\nbi-directional recursive neural",
    "start": "3186150",
    "end": "3193680"
  },
  {
    "text": "network model that\nis implemented over the character sequences\nof words in the medical text.",
    "start": "3193680",
    "end": "3203280"
  },
  {
    "text": "So why character sequences? Why might those be important? ",
    "start": "3203280",
    "end": "3213140"
  },
  {
    "text": "Well, consider a misspelled\nword, for example. Most of the character\nsequence is correct.",
    "start": "3213140",
    "end": "3221120"
  },
  {
    "text": "There will be a bug in\nit at the misspelling. Or consider that a\nlot of medical terms",
    "start": "3221120",
    "end": "3227540"
  },
  {
    "text": "are these compound\nterms, where they're made up of lots of\npieces that correspond",
    "start": "3227540",
    "end": "3233120"
  },
  {
    "text": "to Greek or Latin roots. So learning those can\nactually be very helpful.",
    "start": "3233120",
    "end": "3240440"
  },
  {
    "text": "So you start with that model. You then could\nconcatenate the results",
    "start": "3240440",
    "end": "3246110"
  },
  {
    "text": "from both the left-running and\nthe right-running recursive neural network.",
    "start": "3246110",
    "end": "3252140"
  },
  {
    "text": "And concatenate that with\nthe Word2Vec embedding",
    "start": "3252140",
    "end": "3258095"
  },
  {
    "text": "of the whole word. And you feed that into another\nbi-directional RNN layer.",
    "start": "3258095",
    "end": "3266490"
  },
  {
    "text": "And then for each word, you\ntake the output of those RNNs,",
    "start": "3266490",
    "end": "3273050"
  },
  {
    "text": "run them through a feed-forward\nneural network in order to estimate the prob--",
    "start": "3273050",
    "end": "3278940"
  },
  {
    "text": "it's like a soft max. And you estimate the probability\nof this word belonging",
    "start": "3278940",
    "end": "3284900"
  },
  {
    "text": "to a particular category of\npersonally identifiable health information.",
    "start": "3284900",
    "end": "3290300"
  },
  {
    "text": "So is it a name? Is it an address? Is it a phone number? Is it or whatever?",
    "start": "3290300",
    "end": "3296150"
  },
  {
    "text": "And then the top layer is a\nkind of conditional random field-like layer that imposes\na sequential probability",
    "start": "3296150",
    "end": "3304970"
  },
  {
    "text": "distribution that says, OK,\nif you've seen a name, then",
    "start": "3304970",
    "end": "3310490"
  },
  {
    "text": "what's the next most likely\nthing that you're going to see? And so you combine that with\nthe probability distributions",
    "start": "3310490",
    "end": "3319220"
  },
  {
    "text": "for each word in order to\nidentify the category of PHI",
    "start": "3319220",
    "end": "3324920"
  },
  {
    "text": "or non-PHI for that word. And this did insanely well.",
    "start": "3324920",
    "end": "3331400"
  },
  {
    "text": "So optimized by F1 score, we're\nup at a precision of 99.2%,",
    "start": "3331400",
    "end": "3341000"
  },
  {
    "text": "recall of 99.3%. Optimized by recall,\nwe're up at about 98%, 99%",
    "start": "3341000",
    "end": "3351290"
  },
  {
    "text": "for each of them. So this is doing quite well. Now, there is a non-machine\nlearning comment to make,",
    "start": "3351290",
    "end": "3360030"
  },
  {
    "text": "which is that if you read\nthe HIPAA law, the HIPAA regulations, they\ndon't say that you",
    "start": "3360030",
    "end": "3365660"
  },
  {
    "text": "must get rid of 99%\nof the personally identifying information in\norder to be able to share",
    "start": "3365660",
    "end": "3373760"
  },
  {
    "text": "this data for research. It says you have to\nget rid of all of it.",
    "start": "3373760",
    "end": "3378761"
  },
  {
    "text": "So no technique we\nknow is 100% perfect.",
    "start": "3378761",
    "end": "3383770"
  },
  {
    "text": "And so there's a kind of\npractical understanding among people who\nwork on this stuff",
    "start": "3383770",
    "end": "3390240"
  },
  {
    "text": "that nothing's\ngoing to be perfect. And therefore, that you can\nget away with a little bit.",
    "start": "3390240",
    "end": "3396990"
  },
  {
    "text": "But legally, you're on thin ice.",
    "start": "3396990",
    "end": "3402300"
  },
  {
    "text": "So I remember many years ago,\nmy wife was in law school. And I asked her at one point,\nso what can people sue you for?",
    "start": "3402300",
    "end": "3411600"
  },
  {
    "text": "And she said,\nabsolutely anything. They may not win.",
    "start": "3411600",
    "end": "3417430"
  },
  {
    "text": "But they can be a\nreal pain if you have to go defend yourself in court.",
    "start": "3417430",
    "end": "3422460"
  },
  {
    "text": "And so this hasn't\nplayed out yet. We don't know if a\nde-identifier that",
    "start": "3422460",
    "end": "3428910"
  },
  {
    "text": "is 99% sensitive\nand 99% specific will pass muster with people\nwho agree to release data sets.",
    "start": "3428910",
    "end": "3437730"
  },
  {
    "text": "Because they're worried,\ntoo, about winding up in the newspaper or\nwinding up getting sued.",
    "start": "3437730",
    "end": "3443700"
  },
  {
    "text": " Last topic for today--",
    "start": "3443700",
    "end": "3448809"
  },
  {
    "text": "so if you read this interesting\nblog, which, by the way,",
    "start": "3448810",
    "end": "3454980"
  },
  {
    "text": "has a very good\ntutorial on BERT, he says, \"The year 2018 has been\nan inflection point for machine",
    "start": "3454980",
    "end": "3463290"
  },
  {
    "text": "learning models handling\ntext, or more accurately, NLP. Our conceptual\nunderstanding of how",
    "start": "3463290",
    "end": "3469680"
  },
  {
    "text": "best to represent words\nand sentences in a way that best captures underlying\nmeanings and relationships",
    "start": "3469680",
    "end": "3475710"
  },
  {
    "text": "is rapidly evolving.\" And so there are a\nwhole bunch of new ideas that have come about in about\nthe last year or two years,",
    "start": "3475710",
    "end": "3485530"
  },
  {
    "text": "including ELMo, which learns\ncontext-specific embeddings, the Transformer architecture,\nthis BERT approach.",
    "start": "3485530",
    "end": "3493920"
  },
  {
    "text": "And then I'll end with just\nshowing you this gigantic GPT",
    "start": "3493920",
    "end": "3499470"
  },
  {
    "text": "model that was developed\nby the OpenAI people, which does remarkably better\nthan the stuff I showed you",
    "start": "3499470",
    "end": "3507360"
  },
  {
    "text": "before in generating language. All right.",
    "start": "3507360",
    "end": "3513160"
  },
  {
    "text": "If you look inside\nGoogle Translate, at least as of not\nlong ago, what you find",
    "start": "3513160",
    "end": "3520180"
  },
  {
    "text": "is a model like this. So it's essentially an LSTM\nmodel that takes input words",
    "start": "3520180",
    "end": "3529470"
  },
  {
    "text": "and munges them together\ninto some representation, a high-dimensional vector\nrepresentation, that summarizes",
    "start": "3529470",
    "end": "3538980"
  },
  {
    "text": "everything that the model\nknows about that sentence that you've just fed it.",
    "start": "3538980",
    "end": "3546330"
  },
  {
    "text": "Obviously, it has to be\na pretty high-dimensional representation, because your\nsentence could be about almost",
    "start": "3546330",
    "end": "3552120"
  },
  {
    "text": "anything. And so it's important to\nbe able to capture all",
    "start": "3552120",
    "end": "3557520"
  },
  {
    "text": "that in this representation. But basically, at\nthis point, you start generating the output.",
    "start": "3557520",
    "end": "3564339"
  },
  {
    "text": "So if you're translating\nEnglish to French, these are English\nwords coming in, and these are French words\ngoing out, in sort of the way",
    "start": "3564340",
    "end": "3572670"
  },
  {
    "text": "I showed you, where we're\ngenerating Shakespeare or we're generating Wall\nStreet Journal text.",
    "start": "3572670",
    "end": "3579030"
  },
  {
    "text": " But the critical feature here\nis that in the initial version",
    "start": "3579030",
    "end": "3585780"
  },
  {
    "text": "of this, everything\nthat you learned about this English sentence\nhad to be encoded in this one",
    "start": "3585780",
    "end": "3591869"
  },
  {
    "text": "vector that got passed from\nthe encoder into the decoder,",
    "start": "3591870",
    "end": "3598150"
  },
  {
    "text": "or from the source language into\nthe target language generator.",
    "start": "3598150",
    "end": "3603720"
  },
  {
    "text": "So then someone came\nalong and said, hmm-- someone, namely these\nguys, came along and said,",
    "start": "3603720",
    "end": "3611470"
  },
  {
    "text": "wouldn't it be nice\nif we could provide some auxiliary information\nto the generator that said,",
    "start": "3611470",
    "end": "3617430"
  },
  {
    "text": "hey, which part of\nthe input sentence should you pay attention to?",
    "start": "3617430",
    "end": "3623119"
  },
  {
    "text": "And of course, there's\nno fixed answer to that. I mean, if I'm translating\nan arbitrary English sentence",
    "start": "3623120",
    "end": "3629180"
  },
  {
    "text": "into an arbitrary French\nsentence, I can't say, in general, look at the third\nword in the English sentence",
    "start": "3629180",
    "end": "3636770"
  },
  {
    "text": "when you're generating the third\nword in the French sentence. Because that may or may\nnot be true, depending",
    "start": "3636770",
    "end": "3643040"
  },
  {
    "text": "on the particular sentence. But on the other\nhand, the intuition is that there is such\na positional dependence",
    "start": "3643040",
    "end": "3650060"
  },
  {
    "text": "and a dependence on what the\nparticular English word was",
    "start": "3650060",
    "end": "3656030"
  },
  {
    "text": "that is an important component\nof generating the French word. And so they created this\nidea that in addition",
    "start": "3656030",
    "end": "3664190"
  },
  {
    "text": "to passing along\nthe this vector that",
    "start": "3664190",
    "end": "3670339"
  },
  {
    "text": "encodes the meaning\nof the entire input and the previous word that you\nhad generated in the output,",
    "start": "3670340",
    "end": "3678680"
  },
  {
    "text": "in addition, we pass along this\nother information that says,",
    "start": "3678680",
    "end": "3683730"
  },
  {
    "text": "which of the input words\nshould we pay attention to? And how much attention\nshould we pay to them?",
    "start": "3683730",
    "end": "3690110"
  },
  {
    "text": "And of course, in the\nstyle of these embeddings, these are all represented\nby high-dimensional vectors,",
    "start": "3690110",
    "end": "3697520"
  },
  {
    "text": "high-dimensional real\nnumber vectors that get combined with\nthe other vectors",
    "start": "3697520",
    "end": "3704030"
  },
  {
    "text": "in order to produce the output. Now, a classical linguist\nwould look at this and retch.",
    "start": "3704030",
    "end": "3713660"
  },
  {
    "text": "Because this looks nothing\nlike classical linguistics. It's just numerology that gets\ntrained by stochastic gradient",
    "start": "3713660",
    "end": "3724160"
  },
  {
    "text": "descent methods in order\nto optimize the output. But from an engineering point\nof view, it works quite well.",
    "start": "3724160",
    "end": "3732990"
  },
  {
    "text": "So then for a while, that\nwas the state of the art. And then last year, these\nguys, Vaswani et al.",
    "start": "3732990",
    "end": "3742640"
  },
  {
    "text": "came along and said,\nyou know, we now",
    "start": "3742640",
    "end": "3747920"
  },
  {
    "text": "have this complicated\narchitecture, where we are doing the\nold-style translation where",
    "start": "3747920",
    "end": "3754490"
  },
  {
    "text": "we summarize everything\ninto one vector, and then use that to generate\na sequence of outputs.",
    "start": "3754490",
    "end": "3761690"
  },
  {
    "text": "And we have this\nattention mechanism that tells us how\nmuch of various inputs",
    "start": "3761690",
    "end": "3767450"
  },
  {
    "text": "to use in generating each\nelement of the output. Is the first of those\nactually necessary?",
    "start": "3767450",
    "end": "3775050"
  },
  {
    "text": "And so they published this\nlovely paper saying attention is all you need,\nthat says, hey, you",
    "start": "3775050",
    "end": "3780740"
  },
  {
    "text": "know that thing that you guys\nhave added to this translation model. Not only is it a\nuseful addition,",
    "start": "3780740",
    "end": "3787790"
  },
  {
    "text": "but in fact, it can take the\nplace of the original model. And so the Transformer\nis an architecture that",
    "start": "3787790",
    "end": "3796340"
  },
  {
    "text": "is the hottest thing\nsince sliced bread at the moment, that says,\nOK, here's what we do.",
    "start": "3796340",
    "end": "3803940"
  },
  {
    "text": "We take the inputs. We calculate some\nembedding for them.",
    "start": "3803940",
    "end": "3809400"
  },
  {
    "text": "We then want to\nretain the position, because of course, the sequence\nin which the words appear,",
    "start": "3809400",
    "end": "3815380"
  },
  {
    "text": "it matters. And the positional encoding\nis this weird thing where it encodes using\nsine waves so that--",
    "start": "3815380",
    "end": "3824230"
  },
  {
    "text": "it's an orthogonal basis. And so it has nice\ncharacteristics.",
    "start": "3824230",
    "end": "3829460"
  },
  {
    "text": "And then we run it\ninto an attention model that is essentially\ncomputing self-attention.",
    "start": "3829460",
    "end": "3834890"
  },
  {
    "text": "So it's saying what-- it's like Word2Vec, except\nin a more sophisticated way.",
    "start": "3834890",
    "end": "3842870"
  },
  {
    "text": "So it's looking at all\nthe words in the sentence and saying, which words is\nthis word most related to?",
    "start": "3842870",
    "end": "3851270"
  },
  {
    "text": " And then, in order to\ncomplicate it some more,",
    "start": "3851270",
    "end": "3857579"
  },
  {
    "text": "they say, well, we don't\nwant just a single notion of attention. We want multiple\nnotions of attention.",
    "start": "3857580",
    "end": "3865210"
  },
  {
    "text": "So what does that sound like? Well, to me, it\nsounds a bit like what",
    "start": "3865210",
    "end": "3870510"
  },
  {
    "text": "you see in convolutional\nneural networks, where often when you're\nprocessing an image with a CNN,",
    "start": "3870510",
    "end": "3879270"
  },
  {
    "text": "you're not only applying\none filter to the image but you're applying a whole\nbunch of different filters.",
    "start": "3879270",
    "end": "3885540"
  },
  {
    "text": "And because you\ninitialize them randomly, you hope that they\nwill converge to things that actually detect different\ninteresting properties",
    "start": "3885540",
    "end": "3895370"
  },
  {
    "text": "of the image. So the same idea here-- that what they're\ndoing is they're starting with a bunch of these\nattention matrices and saying,",
    "start": "3895370",
    "end": "3906330"
  },
  {
    "text": "we initialize them randomly. They will evolve\ninto something that is most useful for helping us\ndeal with the overall problem.",
    "start": "3906330",
    "end": "3914860"
  },
  {
    "text": "So then they run\nthis through a series of, I think, in Vaswani's paper,\nsomething like six layers that",
    "start": "3914860",
    "end": "3922289"
  },
  {
    "text": "are just replicated. And there are additional things\nlike feeding forward the input",
    "start": "3922290",
    "end": "3930510"
  },
  {
    "text": "signal in order to add it to\nthe output signal of the stage,",
    "start": "3930510",
    "end": "3936240"
  },
  {
    "text": "and then normalizing,\nand then rerunning it, and then running it through\na feed-forward network that",
    "start": "3936240",
    "end": "3942900"
  },
  {
    "text": "also has a bypass that combines\nthe input with the output of the feed-forward network.",
    "start": "3942900",
    "end": "3949500"
  },
  {
    "text": "And then you do this\nsix times, or n times. And that then feeds\ninto the generator.",
    "start": "3949500",
    "end": "3957260"
  },
  {
    "text": "And the generator then uses\na very similar architecture",
    "start": "3957260",
    "end": "3962390"
  },
  {
    "text": "to calculate output\nprobabilities, And then it samples from those\nin order to generate the text.",
    "start": "3962390",
    "end": "3969330"
  },
  {
    "text": "So this is sort of\nthe contemporary way that one can do translation,\nusing this approach.",
    "start": "3969330",
    "end": "3976190"
  },
  {
    "text": "Obviously, I don't have time to\ngo into all the details of how all this is done.",
    "start": "3976190",
    "end": "3981440"
  },
  {
    "text": "And I'd probably\ndo it wrong anyway. But you can look at the paper,\nwhich gives a good explanation.",
    "start": "3981440",
    "end": "3987710"
  },
  {
    "text": "And that blog that I\npointed to also has a pointer to another\nblog post by the same guy",
    "start": "3987710",
    "end": "3994670"
  },
  {
    "text": "that does a pretty good job\nof explaining the Transformer",
    "start": "3994670",
    "end": "3999799"
  },
  {
    "text": "architecture. It's complicated. So what you get out of the\nmulti-head attention mechanism",
    "start": "3999800",
    "end": "4008200"
  },
  {
    "text": "is that-- here is one attention machine.",
    "start": "4008200",
    "end": "4013700"
  },
  {
    "text": "And for example, the colors\nhere indicate the degree to which the encoding\nof the word \"it\"",
    "start": "4013700",
    "end": "4021849"
  },
  {
    "text": "depends on the other\nwords in the sentence. And you see that it's focused on\nthe animal, which makes sense.",
    "start": "4021850",
    "end": "4029860"
  },
  {
    "text": "Because \"it,\" in\nfact, is referring to the animal in this sentence.",
    "start": "4029860",
    "end": "4037210"
  },
  {
    "text": "Here they introduce\nanother encoding. And this one focuses on \"was\ntoo tired,\" which is also good.",
    "start": "4037210",
    "end": "4046210"
  },
  {
    "text": "Because \"it,\" again, refers to\nthe thing that was too tired.",
    "start": "4046210",
    "end": "4052490"
  },
  {
    "text": "And of course, by\nmulti-headed, they mean that it's doing\nthis many times. And so you're\nidentifying all kinds",
    "start": "4052490",
    "end": "4060200"
  },
  {
    "text": "of different relationships\nin the input sentence.",
    "start": "4060200",
    "end": "4065930"
  },
  {
    "text": "Well, along the same lines\nis this encoding called ELMo.",
    "start": "4065930",
    "end": "4072380"
  },
  {
    "text": "People seem to like\nSesame Street characters. So ELMo is based on a\nbi-directional LSTM.",
    "start": "4072380",
    "end": "4080090"
  },
  {
    "text": "So it's an older technology. But what it does\nis, unlike Word2Vec,",
    "start": "4080090",
    "end": "4086200"
  },
  {
    "text": "which built an embedding\nfor each type--",
    "start": "4086200",
    "end": "4092000"
  },
  {
    "text": "so every time the\nword \"junk\" appears,",
    "start": "4092000",
    "end": "4097060"
  },
  {
    "text": "it gets the same embedding. Here what they're saying is,\nhey, take context seriously.",
    "start": "4097060",
    "end": "4103509"
  },
  {
    "text": "And we're going to calculate\na different embedding for each occurrence\nin context of a token.",
    "start": "4103510",
    "end": "4112710"
  },
  {
    "text": "And this turns out\nto be very good. Because it goes part\nof the way to solving",
    "start": "4112710",
    "end": "4118199"
  },
  {
    "text": "the word-sense\ndisambiguation problem. So this is just an example.",
    "start": "4118200",
    "end": "4123580"
  },
  {
    "text": "If you look at the word\n\"play\" in GloVe, which is a slightly more\nsophisticated variant",
    "start": "4123580",
    "end": "4129330"
  },
  {
    "text": "of the Word2Vec approach,\nyou get playing, game, games, played, players, plays, player,\nplay, football, multiplayer.",
    "start": "4129330",
    "end": "4137520"
  },
  {
    "text": "This all seems to\nbe about games. Because probably,\nfrom the literature",
    "start": "4137520",
    "end": "4142740"
  },
  {
    "text": "that they got this from,\nthat's the most common usage of the word \"play.\"",
    "start": "4142740",
    "end": "4148350"
  },
  {
    "text": "Whereas, using this\nbi-directional language model, they can separate\nout something like,",
    "start": "4148350",
    "end": "4156330"
  },
  {
    "text": "\"Kieffer, the only\njunior in the group, was commended for his ability\nto hit in the clutch, as well as",
    "start": "4156330",
    "end": "4162549"
  },
  {
    "text": "his all-around excellent play.\" So this is presumably\nthe baseball player.",
    "start": "4162550",
    "end": "4167969"
  },
  {
    "text": "And here is, \"They\nwere actors who had been handed fat roles\nin a successful play.\"",
    "start": "4167970",
    "end": "4173100"
  },
  {
    "text": "So this is a different\nmeaning of the word play. And so this embedding also\nhas made really important",
    "start": "4173100",
    "end": "4180540"
  },
  {
    "text": "contributions to improving the\nquality of natural language processing by being able\nto deal with the fact",
    "start": "4180540",
    "end": "4187139"
  },
  {
    "text": "that single words have multiple\nmeanings not only in English but in other languages.",
    "start": "4187140",
    "end": "4193710"
  },
  {
    "text": "So after ELMo comes BERT, which\nis this Bidirectional Encoder",
    "start": "4193710",
    "end": "4200120"
  },
  {
    "text": "Representations\nfrom Transformers. So rather than using the LSTM\nkind of model that ELMo used,",
    "start": "4200120",
    "end": "4207380"
  },
  {
    "text": "these guys say, well,\nlet's hop on the bandwagon, use the Transformer-based\narchitecture.",
    "start": "4207380",
    "end": "4214790"
  },
  {
    "text": "And then they introduced\nsome interesting tricks. So one of the problems\nwith Transformers",
    "start": "4214790",
    "end": "4221510"
  },
  {
    "text": "is if you stack them on\ntop of each other there are many paths from\nany of the inputs",
    "start": "4221510",
    "end": "4227929"
  },
  {
    "text": "to any of the intermediate\nnodes and the outputs. And so if you're\ndoing self-attention,",
    "start": "4227930",
    "end": "4233930"
  },
  {
    "text": "you're trying to figure\nout where the output should pay attention to the input,\nthe answer, of course,",
    "start": "4233930",
    "end": "4242210"
  },
  {
    "text": "is like, if you're trying\nto reconstruct the input, if the input is present in\nyour model, what you will learn",
    "start": "4242210",
    "end": "4250700"
  },
  {
    "text": "is that the\ncorresponding word is the right word for your output.",
    "start": "4250700",
    "end": "4255950"
  },
  {
    "text": "So they have to prevent\nthat from happening. And so the way they do\nit is by masking off,",
    "start": "4255950",
    "end": "4262610"
  },
  {
    "text": "at each level, some fraction\nof the words or of the inputs at that level.",
    "start": "4262610",
    "end": "4269460"
  },
  {
    "text": "So what this is doing\nis it's a little bit like the skip-gram model\nin Word2Vec, where it's",
    "start": "4269460",
    "end": "4275810"
  },
  {
    "text": "trying to predict the\nlikelihood of some word, except it doesn't know\nwhat a significant fraction",
    "start": "4275810",
    "end": "4283100"
  },
  {
    "text": "of the words are. And so it can't overfit in the\nway that I was just suggesting.",
    "start": "4283100",
    "end": "4289910"
  },
  {
    "text": "So this turned out\nto be a good idea. It's more complicated. Again, for the details,\nyou have to read the paper.",
    "start": "4289910",
    "end": "4297440"
  },
  {
    "text": "I gave both the Transformer\npaper and the BERT paper as optional readings for today.",
    "start": "4297440",
    "end": "4304010"
  },
  {
    "text": "I meant to give them\nas required readings, but I didn't do it in time. So they're optional.",
    "start": "4304010",
    "end": "4310220"
  },
  {
    "text": "But there are a whole\nbunch of other tricks. So instead of using words,\nthey actually used word pieces.",
    "start": "4310220",
    "end": "4317240"
  },
  {
    "text": "So think about syllables and\ndon't becomes do and apostrophe",
    "start": "4317240",
    "end": "4323690"
  },
  {
    "text": "t, and so on. And then they discovered\nthat about 15% of the tokens",
    "start": "4323690",
    "end": "4331130"
  },
  {
    "text": "to be masked seems to work\nbetter than other percentages. So those are the hidden tokens\nthat prevent overfitting.",
    "start": "4331130",
    "end": "4341720"
  },
  {
    "text": "And then they do some\nother weird stuff. Like, instead of\nmasking a token,",
    "start": "4341720",
    "end": "4348860"
  },
  {
    "text": "they will inject random other\nwords from the vocabulary into its place, again,\nto prevent overfitting.",
    "start": "4348860",
    "end": "4356810"
  },
  {
    "text": "And then they look at\ndifferent tasks like, can I predict the next\nsentence in a corpus?",
    "start": "4356810",
    "end": "4363020"
  },
  {
    "text": "So I read a sentence. And the translation is\nnot into another language.",
    "start": "4363020",
    "end": "4368330"
  },
  {
    "text": "But it's predicting what the\nnext sentence is going to be. So they trained it on 800\nmillion words from something",
    "start": "4368330",
    "end": "4376880"
  },
  {
    "text": "called the Books corpus\nand about 2 and 1/2",
    "start": "4376880",
    "end": "4382429"
  },
  {
    "text": "million-word Wikipedia corpus. And what they found\nwas that there",
    "start": "4382430",
    "end": "4387640"
  },
  {
    "text": "is an enormous improvement\non a lot of classical tasks. So this is a listing of\nsome of the standard tasks",
    "start": "4387640",
    "end": "4395989"
  },
  {
    "text": "for natural language processing,\nmostly not in the medical world but in the general NLP domain.",
    "start": "4395990",
    "end": "4404450"
  },
  {
    "text": "And you see that you get things\nlike an improvement from 80%.",
    "start": "4404450",
    "end": "4412280"
  },
  {
    "text": "Or even the GPT model\nthat I'll talk about in a minute is at 82%.",
    "start": "4412280",
    "end": "4419060"
  },
  {
    "text": "They're up to about 86%. So a 4% improvement in\nthis domain is really huge.",
    "start": "4419060",
    "end": "4427469"
  },
  {
    "text": "I mean, very often\npeople publish papers showing a 1% improvement.",
    "start": "4427470",
    "end": "4433110"
  },
  {
    "text": "And if their corpus\nis big enough, then it's statistically\nsignificant, and therefore publishable.",
    "start": "4433110",
    "end": "4439020"
  },
  {
    "text": "But it's not significant in the\nordinary meaning of the term significant, if you're\ndoing 1% better.",
    "start": "4439020",
    "end": "4445889"
  },
  {
    "text": "But doing 4% better\nis pretty good. Here we're going\nfrom like 66% to 72%",
    "start": "4445890",
    "end": "4455369"
  },
  {
    "text": "from the earlier\nstate of the art-- 82 to 91; 93 to 94; 35 to\n60 in the CoLA task corpus",
    "start": "4455370",
    "end": "4466410"
  },
  {
    "text": "of linguistic acceptability. So this is asking, I\nthink, Mechanical Turk",
    "start": "4466410",
    "end": "4472110"
  },
  {
    "text": "people, for generated\nsentences, is this sentence a valid sentence of English?",
    "start": "4472110",
    "end": "4479000"
  },
  {
    "text": "And so it's an\ninteresting benchmark. So it's producing really\nsignificant improvements",
    "start": "4479000",
    "end": "4487650"
  },
  {
    "text": "all over the place. They trained two models of it. The base model is\nthe smaller one.",
    "start": "4487650",
    "end": "4492750"
  },
  {
    "text": "The large model is just\ntrained on larger data sets. Enormous amount of computation\nin doing this training--",
    "start": "4492750",
    "end": "4501050"
  },
  {
    "text": "so I've forgotten, it\ntook them like a month on some gigantic\ncluster of GPU machines.",
    "start": "4501050",
    "end": "4508270"
  },
  {
    "text": "And so it's daunting,\nbecause you can't just crank this up on your\nlaptop and expect",
    "start": "4508270",
    "end": "4514000"
  },
  {
    "text": "it to finish in your lifetime. ",
    "start": "4514000",
    "end": "4520210"
  },
  {
    "text": "The last thing I want to\ntell you about is this GPT-2. So this is from the\nOpenAI Institute,",
    "start": "4520210",
    "end": "4526780"
  },
  {
    "text": "which is one of these\nphilanthropically funded-- I think, this one,\nby Elon Musk--",
    "start": "4526780",
    "end": "4533320"
  },
  {
    "text": "research institute\nto advance AI. And what they said is, well,\nthis is all cool, but--",
    "start": "4533320",
    "end": "4542900"
  },
  {
    "text": "so they were not using BERT. They were using the\nTransformer architecture",
    "start": "4542900",
    "end": "4549519"
  },
  {
    "text": "but without the same\ntraining style as BERT. And they said, the\nsecret is going",
    "start": "4549520",
    "end": "4556780"
  },
  {
    "text": "to be that we're going to apply\nthis not only to one problem",
    "start": "4556780",
    "end": "4562929"
  },
  {
    "text": "but to a whole\nbunch of problems. So it's a multi-task\nlearning approach that says,",
    "start": "4562930",
    "end": "4568690"
  },
  {
    "text": "we're going to\nbuild a better model by trying to solve a bunch of\ndifferent tasks simultaneously.",
    "start": "4568690",
    "end": "4576000"
  },
  {
    "text": "And so they built\nenormous models. By the way, the task itself is\ngiven as a sequence of tokens.",
    "start": "4576000",
    "end": "4584180"
  },
  {
    "text": "So for example, they\nmight have a task that says translate to French,\nEnglish text, French text.",
    "start": "4584180",
    "end": "4591889"
  },
  {
    "text": "Or answer the question,\ndocument, question, answer. And so the system\nnot only learns",
    "start": "4591890",
    "end": "4603400"
  },
  {
    "text": "how to do whatever\nit's supposed to do. But it even learns\nsomething about the tasks that it's being asked to work\non by encoding these and using",
    "start": "4603400",
    "end": "4612670"
  },
  {
    "text": "them as part of its model. So they built four\ndifferent models.",
    "start": "4612670",
    "end": "4618070"
  },
  {
    "text": "Take a look at the bottom one. 1.5 billion parameters--\nthis is a large model.",
    "start": "4618070",
    "end": "4629120"
  },
  {
    "text": "This is a very large model.  And so it's a byte-level model.",
    "start": "4629120",
    "end": "4636610"
  },
  {
    "text": "So they just said forget\nwords, because we're trying to do this multilingually.",
    "start": "4636610",
    "end": "4641890"
  },
  {
    "text": "And so for Chinese,\nyou want characters. And for English, you might\nas well take characters also.",
    "start": "4641890",
    "end": "4649330"
  },
  {
    "text": "And the system will, in\nits 1.5 billion parameters, learn all about the sequences of\ncharacters that make up words.",
    "start": "4649330",
    "end": "4657520"
  },
  {
    "text": "And it'll be cool. And so then they look at a whole\nbunch of different challenges.",
    "start": "4657520",
    "end": "4664540"
  },
  {
    "text": "And what you see is that the\nstate of the art before they did this on, for example,\nthe Lambada data set",
    "start": "4664540",
    "end": "4674010"
  },
  {
    "text": "was that the perplexity of\nits predictions was a hundred.",
    "start": "4674010",
    "end": "4680130"
  },
  {
    "text": "And with this large model, the\nperplexity of its predictions is about nine.",
    "start": "4680130",
    "end": "4686500"
  },
  {
    "text": "So that means that it's\nreduced the uncertainty of what to predict next\nridiculously much--",
    "start": "4686500",
    "end": "4693699"
  },
  {
    "text": "I mean, by more than\nan order of magnitude. And you get similar\ngains, accuracy going",
    "start": "4693700",
    "end": "4698920"
  },
  {
    "text": "from 59% to 63% accuracy on a--",
    "start": "4698920",
    "end": "4705699"
  },
  {
    "text": "this is the children's\nsomething-or-other challenge-- from 85% to 93%--",
    "start": "4705700",
    "end": "4711640"
  },
  {
    "text": "so dramatic improvements\nalmost across the board,",
    "start": "4711640",
    "end": "4717100"
  },
  {
    "text": "except for this\nparticular data set, where they did not do well.",
    "start": "4717100",
    "end": "4722720"
  },
  {
    "text": "And what really blew\nme away is here's",
    "start": "4722720",
    "end": "4727880"
  },
  {
    "text": "an application of this\n1.5 billion-word model that they built. So they\nsaid, OK, I give you a prompt,",
    "start": "4727880",
    "end": "4736730"
  },
  {
    "text": "like the opening paragraph\nof a Wall Street Journal article or a Wikipedia article.",
    "start": "4736730",
    "end": "4742010"
  },
  {
    "text": "And you complete the article\nby using that generator idea",
    "start": "4742010",
    "end": "4747230"
  },
  {
    "text": "that I showed you before, that\njust uses the language model and picks the most\nlikely word to come next",
    "start": "4747230",
    "end": "4754520"
  },
  {
    "text": "and emits that as the next word. So here is a prompt that says,\n\"A train carriage containing",
    "start": "4754520",
    "end": "4760490"
  },
  {
    "text": "controlled nuclear materials\nwas stolen in Cincinnati today. Its whereabouts are unknown.\"",
    "start": "4760490",
    "end": "4766070"
  },
  {
    "text": "By the way, this is made up. I mean, this is not\na real news article.",
    "start": "4766070",
    "end": "4771230"
  },
  {
    "text": "And the system comes\nback with a completion that says, \"The incident\noccurred on the downtown train",
    "start": "4771230",
    "end": "4776900"
  },
  {
    "text": "line, which runs from\nCovington and Ashland stations. In an email to\nOhio news outlets,",
    "start": "4776900",
    "end": "4782300"
  },
  {
    "text": "the US Department\nof Energy said it's working with the Federal\nRailroad Administration to find the thief,\" et cetera.",
    "start": "4782300",
    "end": "4790400"
  },
  {
    "text": "This looks astoundingly good. Now, the paper from\nwhich this comes--",
    "start": "4790400",
    "end": "4796650"
  },
  {
    "text": "this is actually from a\nblog, but they've also published a paper about it-- claims that these examples\nare not even cherry-picked.",
    "start": "4796650",
    "end": "4804590"
  },
  {
    "text": "If you go to that page and\npick sample 1, 2, 3, 4, 5, 6, et cetera, you get\ndifferent examples",
    "start": "4804590",
    "end": "4812810"
  },
  {
    "text": "that they claim are\nnot cherry-picked. And every one of\nthem is really good.",
    "start": "4812810",
    "end": "4817880"
  },
  {
    "text": "I mean, you could imagine\nthis being an actual article about this actual event.",
    "start": "4817880",
    "end": "4824090"
  },
  {
    "text": "So somehow or other,\nin this enormous model, and with this\nTransformer technology,",
    "start": "4824090",
    "end": "4830600"
  },
  {
    "text": "and with the multi-task\ntraining that they've done, they have managed\nto capture so much",
    "start": "4830600",
    "end": "4837300"
  },
  {
    "text": "of the regularity of\nthe English language that they can generate these\nfake news articles based",
    "start": "4837300",
    "end": "4843840"
  },
  {
    "text": "on a prompt and make them\nlook unbelievably realistic.",
    "start": "4843840",
    "end": "4848909"
  },
  {
    "text": "Now, interestingly,\nthey have chosen not to release that trained model.",
    "start": "4848910",
    "end": "4854400"
  },
  {
    "text": "Because they're worried that\npeople will, in fact, do this, and that they will generate\nfake news articles all the time.",
    "start": "4854400",
    "end": "4862260"
  },
  {
    "text": "They've released a\nmuch smaller model that is not nearly as good\nin terms of its realism.",
    "start": "4862260",
    "end": "4869010"
  },
  {
    "text": "So that's the state of the\nart in language modeling at the moment. And as I say, the general domain\nis ahead of the medical domain.",
    "start": "4869010",
    "end": "4878520"
  },
  {
    "text": "But you can bet\nthat there are tons of people who are sitting\naround looking at exactly",
    "start": "4878520",
    "end": "4884039"
  },
  {
    "text": "these results and\nsaying, well, we ought to be able to\ntake advantage of this",
    "start": "4884040",
    "end": "4889590"
  },
  {
    "text": "to build much better language\nmodels for the medical domain and to exploit them in order\nto do phenotyping, in order",
    "start": "4889590",
    "end": "4896670"
  },
  {
    "text": "to do entity recognition,\nin order to do inference, in order to do\nquestion answering,",
    "start": "4896670",
    "end": "4903420"
  },
  {
    "text": "in order to do any of\nthese kinds of topics. And I was talking to\nPatrick Winston, who",
    "start": "4903420",
    "end": "4911030"
  },
  {
    "text": "is one of the good\nold-fashioned AI people, as he characterizes himself.",
    "start": "4911030",
    "end": "4916970"
  },
  {
    "text": "And the thing that's a\nlittle troublesome about this is that this technology\nhas virtually nothing",
    "start": "4916970",
    "end": "4924770"
  },
  {
    "text": "to do with anything\nthat we understand about language or about\ninference or about question",
    "start": "4924770",
    "end": "4931670"
  },
  {
    "text": "answering or about anything. And so one is left with\nthis queasy feeling that,",
    "start": "4931670",
    "end": "4939139"
  },
  {
    "text": "here is a wonderful engineering\nsolution to a whole set of problems, but\nit's unclear how",
    "start": "4939140",
    "end": "4944869"
  },
  {
    "text": "it relates to the original goal\nof artificial intelligence, which is to understand something\nabout human intelligence",
    "start": "4944870",
    "end": "4951830"
  },
  {
    "text": "by simulating it in a computer. Maybe our BCS\nfriends will discover",
    "start": "4951830",
    "end": "4958410"
  },
  {
    "text": "that there are, in fact,\ntransformer mechanisms deeply buried in our brain.",
    "start": "4958410",
    "end": "4964670"
  },
  {
    "text": "But I would be surprised\nif that turned out to be exactly the case. But perhaps there is\nsomething like that going on.",
    "start": "4964670",
    "end": "4972480"
  },
  {
    "text": "And so this leaves an\ninteresting scientific conundrum of,\nexactly what have we learned from this type of very,\nvery successful model building?",
    "start": "4972480",
    "end": "4982040"
  },
  {
    "text": "OK. Thank you. [APPLAUSE] ",
    "start": "4982040",
    "end": "5003000"
  }
]