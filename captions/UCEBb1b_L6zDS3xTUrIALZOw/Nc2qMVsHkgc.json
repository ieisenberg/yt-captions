[
  {
    "start": "0",
    "end": "128000"
  },
  {
    "start": "0",
    "end": "4870"
  },
  {
    "text": "[MUSIC PLAYING]",
    "start": "4870",
    "end": "8278"
  },
  {
    "start": "8279",
    "end": "10720"
  },
  {
    "text": "AUDACE NAKESHIMANA: Hi, my\nname is Audace Nakeshimana.",
    "start": "10720",
    "end": "13680"
  },
  {
    "text": "I am an undergraduate student\nand researcher at MIT.",
    "start": "13680",
    "end": "16900"
  },
  {
    "text": "In this video, we'll\ncontinue exploring fairness",
    "start": "16900",
    "end": "19180"
  },
  {
    "text": "in machine learning by looking\nat techniques for mitigating",
    "start": "19180",
    "end": "22270"
  },
  {
    "text": "bias.",
    "start": "22270",
    "end": "23380"
  },
  {
    "text": "Throughout the course, we'll\nstart by illustrating bias",
    "start": "23380",
    "end": "25900"
  },
  {
    "text": "in machine learning.",
    "start": "25900",
    "end": "27189"
  },
  {
    "text": "Then we'll look at techniques\nfor mitigating bias,",
    "start": "27190",
    "end": "29720"
  },
  {
    "text": "specifically we'll explore\ntwo types of techniques--",
    "start": "29720",
    "end": "32860"
  },
  {
    "text": "the database\ntechniques, where we'll",
    "start": "32860",
    "end": "35110"
  },
  {
    "text": "look at how to\ncalibrate and augment",
    "start": "35110",
    "end": "37360"
  },
  {
    "text": "our data set to mitigate\nbias in machine learning.",
    "start": "37360",
    "end": "40453"
  },
  {
    "text": "And then we'll look at\nmodel-based techniques,",
    "start": "40453",
    "end": "42370"
  },
  {
    "text": "in which you explore\ndifferent model types",
    "start": "42370",
    "end": "44600"
  },
  {
    "text": "and architectures that help us\nto get to a less biased model.",
    "start": "44600",
    "end": "49149"
  },
  {
    "text": "And we'll do this by\napplying these techniques",
    "start": "49150",
    "end": "51430"
  },
  {
    "text": "on the UCI Adult Data Set.",
    "start": "51430",
    "end": "55050"
  },
  {
    "text": "In this module, we'll explore\ndifferent steps and principles",
    "start": "55050",
    "end": "58170"
  },
  {
    "text": "involved in building less biased\nmachine learning applications.",
    "start": "58170",
    "end": "61890"
  },
  {
    "text": "We look at two main classes\nof techniques, specifically",
    "start": "61890",
    "end": "64830"
  },
  {
    "text": "data and model-based\ntechniques, for mitigating bias",
    "start": "64830",
    "end": "67710"
  },
  {
    "text": "in machine learning.",
    "start": "67710",
    "end": "68920"
  },
  {
    "text": "We will be applying these\ntechniques on the UCI Adult",
    "start": "68920",
    "end": "71509"
  },
  {
    "text": "Data Set with the purpose\nof mitigating gender bias",
    "start": "71510",
    "end": "74490"
  },
  {
    "text": "in predicting income category.",
    "start": "74490",
    "end": "77130"
  },
  {
    "text": "This module is comprised\nof seven main parts.",
    "start": "77130",
    "end": "80369"
  },
  {
    "text": "In part 1, we're going\nto look at an overview",
    "start": "80370",
    "end": "82830"
  },
  {
    "text": "of algorithmic bias.",
    "start": "82830",
    "end": "84330"
  },
  {
    "text": "In the second part, we will\nexplore the UCI Adult Data Set.",
    "start": "84330",
    "end": "88520"
  },
  {
    "text": "In the third part, we look\nat different data preparation",
    "start": "88520",
    "end": "91469"
  },
  {
    "text": "steps for machine learning.",
    "start": "91470",
    "end": "93330"
  },
  {
    "text": "In part 4, we're going to look\nat an example of gender bias.",
    "start": "93330",
    "end": "97560"
  },
  {
    "text": "And in part 5,\nwe're going to look",
    "start": "97560",
    "end": "99420"
  },
  {
    "text": "at different\ndata-based approaches",
    "start": "99420",
    "end": "101580"
  },
  {
    "text": "for mitigating gender bias.",
    "start": "101580",
    "end": "103650"
  },
  {
    "text": "And in part 6,\nwe're going to look",
    "start": "103650",
    "end": "105240"
  },
  {
    "text": "at different\nmodel-based approaches.",
    "start": "105240",
    "end": "107460"
  },
  {
    "text": "And in the last\npart, we'll conclude",
    "start": "107460",
    "end": "109080"
  },
  {
    "text": "by looking at the\npossible next steps.",
    "start": "109080",
    "end": "112140"
  },
  {
    "text": "Recommended prerequisites\nfor this module",
    "start": "112140",
    "end": "114409"
  },
  {
    "text": "are familiarity with the fields\nof data science, statistics,",
    "start": "114410",
    "end": "117720"
  },
  {
    "text": "or machine learning\nand familiarity",
    "start": "117720",
    "end": "119940"
  },
  {
    "text": "with the programming\ntools that we'll be using.",
    "start": "119940",
    "end": "122370"
  },
  {
    "text": "These are Python, Pandas,\nand the Scikit-Learn Library.",
    "start": "122370",
    "end": "127710"
  },
  {
    "text": "In part 1 of this\nmodule, we will",
    "start": "127710",
    "end": "129630"
  },
  {
    "start": "128000",
    "end": "128000"
  },
  {
    "text": "start by understanding\nalgorithmic bias.",
    "start": "129630",
    "end": "132180"
  },
  {
    "text": "We will define it and look at\nits sources and implications.",
    "start": "132180",
    "end": "137599"
  },
  {
    "text": "Throughout the module, we\nwill use the term bias,",
    "start": "137600",
    "end": "140540"
  },
  {
    "text": "algorithmic bias, or model bias\nto describe systematic errors",
    "start": "140540",
    "end": "144590"
  },
  {
    "text": "in algorithms or\nmodels that could lead",
    "start": "144590",
    "end": "147170"
  },
  {
    "text": "to potentially unfair outcomes.",
    "start": "147170",
    "end": "149569"
  },
  {
    "text": "We will identify\nbias qualitatively",
    "start": "149570",
    "end": "151880"
  },
  {
    "text": "and quantitatively by looking\nat model errors, disparities",
    "start": "151880",
    "end": "155390"
  },
  {
    "text": "across different\ngender demographics.",
    "start": "155390",
    "end": "157970"
  },
  {
    "text": "And notice that,\nthroughout the module,",
    "start": "157970",
    "end": "160010"
  },
  {
    "text": "we will use gender to describe\nbiological sex at birth.",
    "start": "160010",
    "end": "163080"
  },
  {
    "start": "163080",
    "end": "165910"
  },
  {
    "text": "So what are some potential\nsources of algorithmic bias?",
    "start": "165910",
    "end": "169140"
  },
  {
    "text": "First, bias can come\nduring direct collection.",
    "start": "169140",
    "end": "172230"
  },
  {
    "text": "And this could happen when the\ndata that you collect already",
    "start": "172230",
    "end": "174840"
  },
  {
    "text": "contains some systematic\nbiases or stereotypes",
    "start": "174840",
    "end": "177629"
  },
  {
    "text": "about some demographics.",
    "start": "177630",
    "end": "179550"
  },
  {
    "text": "This could also happen\nif different demographics",
    "start": "179550",
    "end": "182130"
  },
  {
    "text": "in our data set are not\nequally represented.",
    "start": "182130",
    "end": "185010"
  },
  {
    "text": "The second example of how bias\ncould come in machine learning",
    "start": "185010",
    "end": "188700"
  },
  {
    "text": "is in the training process,\nwhen our models are not",
    "start": "188700",
    "end": "191550"
  },
  {
    "text": "penalized for being biased.",
    "start": "191550",
    "end": "193020"
  },
  {
    "start": "193020",
    "end": "195580"
  },
  {
    "text": "Algorithmic bias is a problem\nbecause of different reasons.",
    "start": "195580",
    "end": "199130"
  },
  {
    "text": "It leads to unfair outcomes\ntoward some individuals",
    "start": "199130",
    "end": "201610"
  },
  {
    "text": "or demographics and it leads\nto further bias propagation,",
    "start": "201610",
    "end": "204970"
  },
  {
    "text": "creating a feedback\ncycle of bias.",
    "start": "204970",
    "end": "209040"
  },
  {
    "text": "In the second part\nof this module,",
    "start": "209040",
    "end": "211180"
  },
  {
    "text": "we'll explore the\nUCI Adult Data Set",
    "start": "211180",
    "end": "213629"
  },
  {
    "text": "by establishing familiarity\nwith the data set",
    "start": "213630",
    "end": "216030"
  },
  {
    "text": "and looking at different\ndistributions in the data set.",
    "start": "216030",
    "end": "219569"
  },
  {
    "text": "The UCI Adult Data Set is one\nof the most popular machine",
    "start": "219570",
    "end": "223050"
  },
  {
    "text": "learning data sets.",
    "start": "223050",
    "end": "224520"
  },
  {
    "text": "It is available on the internet\non the UCI Machine Learning",
    "start": "224520",
    "end": "227280"
  },
  {
    "text": "Repository.",
    "start": "227280",
    "end": "228540"
  },
  {
    "text": "The data set is comprised\nof more than 48,000 data",
    "start": "228540",
    "end": "231810"
  },
  {
    "text": "points that were extracted\nfrom the 1994 Census database",
    "start": "231810",
    "end": "235950"
  },
  {
    "text": "in the United States.",
    "start": "235950",
    "end": "239160"
  },
  {
    "text": "Each data point in the data set\nit is comprised of 15 features.",
    "start": "239160",
    "end": "243030"
  },
  {
    "text": "These include age, work class,\neducation, relationship, race,",
    "start": "243030",
    "end": "247170"
  },
  {
    "text": "sex, salary, and others.",
    "start": "247170",
    "end": "250319"
  },
  {
    "text": "If you look at the gender\ndistribution in the data set,",
    "start": "250320",
    "end": "253650"
  },
  {
    "text": "you can see that about 16,000\nindividuals identify as female.",
    "start": "253650",
    "end": "258018"
  },
  {
    "text": "And about 32,000 individuals\nidentify as male.",
    "start": "258019",
    "end": "261930"
  },
  {
    "text": "If you look at the\nrace distribution,",
    "start": "261930",
    "end": "264210"
  },
  {
    "text": "you can see that slightly\nmore than 40,000 individuals",
    "start": "264210",
    "end": "266850"
  },
  {
    "text": "identify as white.",
    "start": "266850",
    "end": "268890"
  },
  {
    "text": "And about 4,000 to 5,000\nindividuals identify as black.",
    "start": "268890",
    "end": "273300"
  },
  {
    "text": "The rest is other minorities.",
    "start": "273300",
    "end": "277539"
  },
  {
    "text": "If we look at the distribution\nof income category",
    "start": "277540",
    "end": "280300"
  },
  {
    "text": "in the general\npopulation, we can",
    "start": "280300",
    "end": "282310"
  },
  {
    "text": "see that about\n37,000 individuals",
    "start": "282310",
    "end": "285400"
  },
  {
    "text": "earn less or equal to $50,000.",
    "start": "285400",
    "end": "288220"
  },
  {
    "text": "And only about between\n12,000 and 13,000 individuals",
    "start": "288220",
    "end": "292630"
  },
  {
    "text": "earn more than $50,000.",
    "start": "292630",
    "end": "296910"
  },
  {
    "text": "By looking at the income\nlevel distribution",
    "start": "296910",
    "end": "299020"
  },
  {
    "text": "across different\nlevels, we can see",
    "start": "299020",
    "end": "301057"
  },
  {
    "text": "that the ratio of male\nindividuals that make more",
    "start": "301058",
    "end": "303100"
  },
  {
    "text": "than $50,000 is about a third.",
    "start": "303100",
    "end": "306220"
  },
  {
    "text": "But for the female demographic,\nthis ratio drops to about 20%.",
    "start": "306220",
    "end": "310030"
  },
  {
    "start": "310030",
    "end": "313389"
  },
  {
    "text": "An important observation\nfrom what we've seen so far",
    "start": "313390",
    "end": "316510"
  },
  {
    "text": "is that the number of data\npoints in the male population",
    "start": "316510",
    "end": "319210"
  },
  {
    "text": "is significantly higher than\nthe number of data points",
    "start": "319210",
    "end": "321460"
  },
  {
    "text": "in the female\npopulation, exceeding it",
    "start": "321460",
    "end": "323650"
  },
  {
    "text": "by more than three times in\nthe higher income category.",
    "start": "323650",
    "end": "326680"
  },
  {
    "text": "Therefore, it is very\nimportant to think",
    "start": "326680",
    "end": "328570"
  },
  {
    "text": "about how this representation\ndisparity might",
    "start": "328570",
    "end": "331510"
  },
  {
    "text": "affect predictions of a\nmodel trained from this data.",
    "start": "331510",
    "end": "334750"
  },
  {
    "start": "334750",
    "end": "337290"
  },
  {
    "text": "In the third part\nof this module,",
    "start": "337290",
    "end": "339280"
  },
  {
    "start": "338000",
    "end": "338000"
  },
  {
    "text": "we are going to explore\ndifferent steps involved",
    "start": "339280",
    "end": "341430"
  },
  {
    "text": "in transforming our data\nfrom raw representation",
    "start": "341430",
    "end": "344039"
  },
  {
    "text": "to appropriate numerical or\ncategorical representation",
    "start": "344040",
    "end": "347360"
  },
  {
    "text": "in order to be able to perform\nmachine learning tasks.",
    "start": "347360",
    "end": "350699"
  },
  {
    "text": "An example of\ntransformation to be made",
    "start": "350700",
    "end": "353220"
  },
  {
    "text": "is the conversion of native\ncountry from raw representation",
    "start": "353220",
    "end": "356310"
  },
  {
    "text": "to binary.",
    "start": "356310",
    "end": "357570"
  },
  {
    "text": "In this example, we decided\nto assign a binary label",
    "start": "357570",
    "end": "361140"
  },
  {
    "text": "to individuals who\ncome from the United",
    "start": "361140",
    "end": "363300"
  },
  {
    "text": "States and another binary label\nto individuals who come outside",
    "start": "363300",
    "end": "367349"
  },
  {
    "text": "of the United States.",
    "start": "367350",
    "end": "370340"
  },
  {
    "text": "We applied the\nsame transformation",
    "start": "370340",
    "end": "371990"
  },
  {
    "text": "to the sex and salary\nattribute, since each one",
    "start": "371990",
    "end": "375020"
  },
  {
    "text": "of these attributes has to\npossible values in the data",
    "start": "375020",
    "end": "378020"
  },
  {
    "text": "set, therefore making binary\nrepresentation appropriate.",
    "start": "378020",
    "end": "383389"
  },
  {
    "text": "There are more than\ntwo possible values",
    "start": "383390",
    "end": "385160"
  },
  {
    "text": "that the relationship\nattribute can take.",
    "start": "385160",
    "end": "387920"
  },
  {
    "text": "Therefore, for\nthis attribute, we",
    "start": "387920",
    "end": "390110"
  },
  {
    "text": "use one-hot encoding, which\nis more powerful than binary",
    "start": "390110",
    "end": "393620"
  },
  {
    "text": "encoding because it can encode\nan alphabet of any size.",
    "start": "393620",
    "end": "396830"
  },
  {
    "start": "396830",
    "end": "399409"
  },
  {
    "text": "We applied the\nsame transformation",
    "start": "399410",
    "end": "400940"
  },
  {
    "text": "from raw representation\nto binary or one-hot",
    "start": "400940",
    "end": "403400"
  },
  {
    "text": "to all other\ncategorical attributes.",
    "start": "403400",
    "end": "405740"
  },
  {
    "text": "In most cases, we chose binary\nencoding for simplicity.",
    "start": "405740",
    "end": "408919"
  },
  {
    "text": "But this is often\na decision that",
    "start": "408920",
    "end": "410780"
  },
  {
    "text": "has to be made on a\ncase-by-case basis,",
    "start": "410780",
    "end": "412920"
  },
  {
    "text": "depending on the application.",
    "start": "412920",
    "end": "414722"
  },
  {
    "text": "It is also important to note\nthat converting features",
    "start": "414722",
    "end": "416930"
  },
  {
    "text": "like work class to\nbinary can be problematic",
    "start": "416930",
    "end": "419509"
  },
  {
    "text": "if individuals from\ndifferent categories",
    "start": "419510",
    "end": "421250"
  },
  {
    "text": "have systematically\ndifferent levels of income.",
    "start": "421250",
    "end": "423800"
  },
  {
    "text": "On the other hand,\nnot doing this",
    "start": "423800",
    "end": "425569"
  },
  {
    "text": "might be a problem\nif one category",
    "start": "425570",
    "end": "427280"
  },
  {
    "text": "has very few people in it\nthat we can generalize from.",
    "start": "427280",
    "end": "431000"
  },
  {
    "start": "431000",
    "end": "431000"
  },
  {
    "text": "In the fourth part\nof this module,",
    "start": "431000",
    "end": "433440"
  },
  {
    "text": "we are going to\nillustrate gender bias.",
    "start": "433440",
    "end": "435200"
  },
  {
    "text": "We will apply the standard\nmachine learning approach",
    "start": "435200",
    "end": "437630"
  },
  {
    "text": "to our data and then\nevaluate the bias in the task",
    "start": "437630",
    "end": "440540"
  },
  {
    "text": "of predicting income category.",
    "start": "440540",
    "end": "444050"
  },
  {
    "text": "We start by splitting the data\nset into the training and test",
    "start": "444050",
    "end": "447139"
  },
  {
    "text": "data.",
    "start": "447140",
    "end": "448340"
  },
  {
    "text": "We then feed MLPClassifier\non training data,",
    "start": "448340",
    "end": "451430"
  },
  {
    "text": "then use the model to make\nprediction on the test data.",
    "start": "451430",
    "end": "455810"
  },
  {
    "text": "In case you're not familiar\nwith the multi-layer perceptron",
    "start": "455810",
    "end": "458370"
  },
  {
    "text": "or MLPClassifier, this\nmodel belongs to the class",
    "start": "458370",
    "end": "461630"
  },
  {
    "text": "of feedforward neural networks.",
    "start": "461630",
    "end": "463740"
  },
  {
    "text": "Each node uses a non-linear\nactivation function,",
    "start": "463740",
    "end": "466370"
  },
  {
    "text": "giving the model ability to\nseparate non-linear data.",
    "start": "466370",
    "end": "469160"
  },
  {
    "text": "The model is trained using\nbackpropagation technique.",
    "start": "469160",
    "end": "472700"
  },
  {
    "text": "However, a few downsides is that\nthe model suffers overfitting,",
    "start": "472700",
    "end": "476630"
  },
  {
    "text": "and it is not easily\ninterpretable.",
    "start": "476630",
    "end": "480200"
  },
  {
    "text": "Before we evaluate\nour model, let's",
    "start": "480200",
    "end": "482480"
  },
  {
    "text": "start by establishing\nsome terminology.",
    "start": "482480",
    "end": "485420"
  },
  {
    "text": "Throughout the\nrest of the module,",
    "start": "485420",
    "end": "487210"
  },
  {
    "text": "we will refer to the\npositive category",
    "start": "487210",
    "end": "489199"
  },
  {
    "text": "as the group of individuals that\nearn more than $50,000 a year,",
    "start": "489200",
    "end": "492940"
  },
  {
    "text": "or the high-income category.",
    "start": "492940",
    "end": "494990"
  },
  {
    "text": "And we will refer to\nthe negative category",
    "start": "494990",
    "end": "497330"
  },
  {
    "text": "as the group of individuals that\nearn $50,000 a year or less.",
    "start": "497330",
    "end": "501169"
  },
  {
    "text": "We will also refer to it\nas the low-income category.",
    "start": "501170",
    "end": "506470"
  },
  {
    "text": "Now that we've established\nimportant terminology,",
    "start": "506470",
    "end": "508870"
  },
  {
    "text": "let's look at different error\nrate metrics for the model",
    "start": "508870",
    "end": "511419"
  },
  {
    "text": "that we trained previously\nacross different gender",
    "start": "511420",
    "end": "513700"
  },
  {
    "text": "demographics.",
    "start": "513700",
    "end": "515080"
  },
  {
    "text": "If you look at\naccuracy, you can see",
    "start": "515080",
    "end": "516929"
  },
  {
    "text": "that the accuracy for the\nmale demographic is about 0.8,",
    "start": "516929",
    "end": "520495"
  },
  {
    "text": "while the accuracy for\nthe female demographic",
    "start": "520495",
    "end": "522370"
  },
  {
    "text": "is about 90%, or 0.9.",
    "start": "522370",
    "end": "525190"
  },
  {
    "text": "If you look at the positive\nrate and the true positive rate,",
    "start": "525190",
    "end": "527770"
  },
  {
    "text": "you can see that both of these\nmetrics are higher for the male",
    "start": "527770",
    "end": "530353"
  },
  {
    "text": "demographic than for\nthe female demographic.",
    "start": "530353",
    "end": "532810"
  },
  {
    "text": "However, if you look\nat the negative rate",
    "start": "532810",
    "end": "535000"
  },
  {
    "text": "and the true negative\nrate, you can",
    "start": "535000",
    "end": "536650"
  },
  {
    "text": "see that these metrics\nare higher for the female",
    "start": "536650",
    "end": "538750"
  },
  {
    "text": "demographic than for the\nmale demographic instead.",
    "start": "538750",
    "end": "541120"
  },
  {
    "start": "541120",
    "end": "544220"
  },
  {
    "text": "The metrics that we just saw\nindicate consistent disparity",
    "start": "544220",
    "end": "546995"
  },
  {
    "text": "in error rate between the male\nand the female demographic.",
    "start": "546995",
    "end": "550460"
  },
  {
    "text": "This is what will\ndefine as gender bias.",
    "start": "550460",
    "end": "552980"
  },
  {
    "text": "Mitigating gender\nbias, in this case,",
    "start": "552980",
    "end": "554810"
  },
  {
    "text": "is equivalent to using\ndifferent techniques",
    "start": "554810",
    "end": "556670"
  },
  {
    "text": "to minimize this disparity.",
    "start": "556670",
    "end": "558362"
  },
  {
    "text": "And this will be the focus\nof the rest of the module.",
    "start": "558362",
    "end": "560570"
  },
  {
    "start": "560570",
    "end": "563320"
  },
  {
    "start": "563000",
    "end": "563000"
  },
  {
    "text": "In part 5 of our\nmodule, we are going",
    "start": "563320",
    "end": "565750"
  },
  {
    "text": "to explore different database\ndebiasing techniques.",
    "start": "565750",
    "end": "569140"
  },
  {
    "text": "More specifically, we will\nlook at different ways",
    "start": "569140",
    "end": "571660"
  },
  {
    "text": "we can recalibrate or augment\nour data set in a way that",
    "start": "571660",
    "end": "574779"
  },
  {
    "text": "makes predictions less biased.",
    "start": "574780",
    "end": "578680"
  },
  {
    "text": "The motivation behind this\nis from our hypothesis",
    "start": "578680",
    "end": "581360"
  },
  {
    "text": "that the gender bias could come\nfrom unequal representation",
    "start": "581360",
    "end": "584390"
  },
  {
    "text": "of male and female\ndemographics in our data set.",
    "start": "584390",
    "end": "587480"
  },
  {
    "text": "We therefore make an attempt\nto recalibrate or augment",
    "start": "587480",
    "end": "590240"
  },
  {
    "text": "the data set with\nthe aim of equalizing",
    "start": "590240",
    "end": "592399"
  },
  {
    "text": "gender representation\nin our training data.",
    "start": "592400",
    "end": "596330"
  },
  {
    "text": "The first database technique\nthat you're going to explore",
    "start": "596330",
    "end": "598850"
  },
  {
    "text": "is called debiasing\nby unawareness.",
    "start": "598850",
    "end": "601160"
  },
  {
    "text": "And in this technique,\nwe mitigate gender bias",
    "start": "601160",
    "end": "603529"
  },
  {
    "text": "by removing gender from the\nattributes that we train on.",
    "start": "603530",
    "end": "606530"
  },
  {
    "text": "The code snippet shows\nour implementation.",
    "start": "606530",
    "end": "609860"
  },
  {
    "text": "By looking at the\nresults for our debiasing",
    "start": "609860",
    "end": "611720"
  },
  {
    "text": "by unawareness\ntechnique, we can see",
    "start": "611720",
    "end": "613699"
  },
  {
    "text": "that, although there was\nnot significant improvement",
    "start": "613700",
    "end": "616070"
  },
  {
    "text": "in reducing the\ngap for accuracy,",
    "start": "616070",
    "end": "618318"
  },
  {
    "text": "we were able to reduce\nthe gap for other metrics,",
    "start": "618318",
    "end": "620360"
  },
  {
    "text": "like the positive rate,\nthe negative rate,",
    "start": "620360",
    "end": "622360"
  },
  {
    "text": "the true positive rate,\nand the true negative rate.",
    "start": "622360",
    "end": "625160"
  },
  {
    "text": "And this is an example of how\na debasing technique might not",
    "start": "625160",
    "end": "628040"
  },
  {
    "text": "be able to achieve an\nimprovement in all the metrics,",
    "start": "628040",
    "end": "630860"
  },
  {
    "text": "although it might see\na significant reduction",
    "start": "630860",
    "end": "632930"
  },
  {
    "text": "in the gap for other\nmetrics of interest.",
    "start": "632930",
    "end": "636680"
  },
  {
    "text": "Debiasing by unawareness\ncan be one approach",
    "start": "636680",
    "end": "638779"
  },
  {
    "text": "to mitigate gender\nbias to some extent,",
    "start": "638780",
    "end": "640790"
  },
  {
    "text": "as we saw in our results.",
    "start": "640790",
    "end": "642620"
  },
  {
    "text": "However, studies have shown that\nthis method can be ineffective,",
    "start": "642620",
    "end": "646520"
  },
  {
    "text": "especially if there are other\nfeatures in the data set",
    "start": "646520",
    "end": "648770"
  },
  {
    "text": "that have some correlation\nwith the protected attributes",
    "start": "648770",
    "end": "651620"
  },
  {
    "text": "that we are dropping.",
    "start": "651620",
    "end": "653300"
  },
  {
    "text": "These type of attributes are\nreferred to as proxy variables.",
    "start": "653300",
    "end": "658800"
  },
  {
    "text": "The second database technique\nthat you're going to explore",
    "start": "658800",
    "end": "661339"
  },
  {
    "text": "is equalizing the\nnumber of data points.",
    "start": "661340",
    "end": "663940"
  },
  {
    "text": "And in this approach,\nwe will attempt",
    "start": "663940",
    "end": "666020"
  },
  {
    "text": "to equalize representation\nby using equal number",
    "start": "666020",
    "end": "669320"
  },
  {
    "text": "or equal ratio of male\nand female individuals",
    "start": "669320",
    "end": "672220"
  },
  {
    "text": "in our data set or within\neach income category.",
    "start": "672220",
    "end": "675050"
  },
  {
    "start": "675050",
    "end": "678180"
  },
  {
    "text": "We will start by attempting\nto equalize the number of data",
    "start": "678180",
    "end": "680980"
  },
  {
    "text": "points per gender category.",
    "start": "680980",
    "end": "682704"
  },
  {
    "text": "And in this\napproach, we're going",
    "start": "682705",
    "end": "684080"
  },
  {
    "text": "to draw a sample in which\nthere is equal number of data",
    "start": "684080",
    "end": "686940"
  },
  {
    "text": "points from the\nmale demographic and",
    "start": "686940",
    "end": "688830"
  },
  {
    "text": "from the female demographic.",
    "start": "688830",
    "end": "690810"
  },
  {
    "text": "Feel free to pause the video to\nunderstand the implementation.",
    "start": "690810",
    "end": "693870"
  },
  {
    "start": "693870",
    "end": "696520"
  },
  {
    "text": "These are the\nresults that we got",
    "start": "696520",
    "end": "698050"
  },
  {
    "text": "from training an\nMLPClassifier on a data set",
    "start": "698050",
    "end": "701170"
  },
  {
    "text": "in which there is an\nequal number of data",
    "start": "701170",
    "end": "702899"
  },
  {
    "text": "points per gender category.",
    "start": "702900",
    "end": "705120"
  },
  {
    "text": "Feel free to pause the\nvideo to understand",
    "start": "705120",
    "end": "707410"
  },
  {
    "text": "the results in more detail.",
    "start": "707410",
    "end": "710730"
  },
  {
    "text": "The next attempt\nwill be to equalize",
    "start": "710730",
    "end": "713010"
  },
  {
    "text": "the number of data\npoints per income level",
    "start": "713010",
    "end": "715080"
  },
  {
    "text": "in each gender category.",
    "start": "715080",
    "end": "716790"
  },
  {
    "text": "And what this means\nis that the number",
    "start": "716790",
    "end": "718649"
  },
  {
    "text": "of high-income and\nlow-income earners",
    "start": "718650",
    "end": "720990"
  },
  {
    "text": "is the same in the\nmale and the female",
    "start": "720990",
    "end": "723060"
  },
  {
    "text": "demographic for the sample that\nyou're going to use to train.",
    "start": "723060",
    "end": "726270"
  },
  {
    "start": "726270",
    "end": "729920"
  },
  {
    "text": "Here are the key\nmetrics from a model",
    "start": "729920",
    "end": "731660"
  },
  {
    "text": "that was trained on a sample\nof the data set in which there",
    "start": "731660",
    "end": "734569"
  },
  {
    "text": "is an equal number of\ndata points per income",
    "start": "734570",
    "end": "736550"
  },
  {
    "text": "level in each gender category.",
    "start": "736550",
    "end": "738440"
  },
  {
    "start": "738440",
    "end": "741060"
  },
  {
    "text": "One downside to this\nmethodology of equalizing",
    "start": "741060",
    "end": "743850"
  },
  {
    "text": "the number of data\npoints per demographic",
    "start": "743850",
    "end": "746040"
  },
  {
    "text": "is that the size of\nthe resulting data set",
    "start": "746040",
    "end": "748199"
  },
  {
    "text": "depends on the size of\nthe smallest demographic.",
    "start": "748200",
    "end": "751560"
  },
  {
    "text": "Therefore, if the\nsmallest demographic",
    "start": "751560",
    "end": "753510"
  },
  {
    "text": "has a very small\nnumber of data points,",
    "start": "753510",
    "end": "756270"
  },
  {
    "text": "you're going to end up with\na very small training set.",
    "start": "756270",
    "end": "759180"
  },
  {
    "text": "Therefore, in some\ncases, you might",
    "start": "759180",
    "end": "760860"
  },
  {
    "text": "find that equalizing the ratio\ninstead of the number of data",
    "start": "760860",
    "end": "763829"
  },
  {
    "text": "points by demographic can lead\nto a higher resulting sample",
    "start": "763830",
    "end": "766950"
  },
  {
    "text": "size.",
    "start": "766950",
    "end": "767920"
  },
  {
    "text": "And that's what we're going to\nlook at in the next approach.",
    "start": "767920",
    "end": "772470"
  },
  {
    "text": "In this methodology\nof equalizing",
    "start": "772470",
    "end": "774209"
  },
  {
    "text": "the ratio of the number of\ndata points per income level",
    "start": "774210",
    "end": "776970"
  },
  {
    "text": "in each category, we\nequalize the ratio",
    "start": "776970",
    "end": "779220"
  },
  {
    "text": "of male individuals\nwith high income",
    "start": "779220",
    "end": "781110"
  },
  {
    "text": "to male individuals\nwith low income.",
    "start": "781110",
    "end": "783510"
  },
  {
    "text": "And we do this for the\nfemale demographic, as well.",
    "start": "783510",
    "end": "786150"
  },
  {
    "text": "This results into a\nhigher sample size.",
    "start": "786150",
    "end": "788400"
  },
  {
    "text": "And to see how\nthis is the case, I",
    "start": "788400",
    "end": "790080"
  },
  {
    "text": "encourage you to look at\nthe notebook for this work.",
    "start": "790080",
    "end": "792870"
  },
  {
    "start": "792870",
    "end": "796040"
  },
  {
    "text": "This is the plot for the\nresults of this methodology.",
    "start": "796040",
    "end": "799940"
  },
  {
    "text": "And you can see\nthat, although there",
    "start": "799940",
    "end": "801520"
  },
  {
    "text": "is some gap for the accuracy\nand true positive rate,",
    "start": "801520",
    "end": "805580"
  },
  {
    "text": "the gap is way smaller for\npositive rate and negative rate",
    "start": "805580",
    "end": "808600"
  },
  {
    "text": "or true negative rate.",
    "start": "808600",
    "end": "809860"
  },
  {
    "start": "809860",
    "end": "814170"
  },
  {
    "text": "The next technique that\nyou're going to look at",
    "start": "814170",
    "end": "816240"
  },
  {
    "text": "is counterfactual augmentation.",
    "start": "816240",
    "end": "818510"
  },
  {
    "text": "And in this approach, for each\ndata point Xi with a given",
    "start": "818510",
    "end": "821910"
  },
  {
    "text": "gender, we generate\na new data point",
    "start": "821910",
    "end": "824069"
  },
  {
    "text": "Yi that differs with Xi only\nat the gender attribute.",
    "start": "824070",
    "end": "828060"
  },
  {
    "text": "And we add Yi to our\ntraining data set.",
    "start": "828060",
    "end": "830430"
  },
  {
    "start": "830430",
    "end": "833589"
  },
  {
    "text": "I encourage you to\npause a little bit",
    "start": "833590",
    "end": "835270"
  },
  {
    "text": "and convince yourself that\nthe resulting data set",
    "start": "835270",
    "end": "837520"
  },
  {
    "text": "from counterfactual\naugmentation will",
    "start": "837520",
    "end": "839530"
  },
  {
    "text": "satisfy all the following\nconstraints shown here.",
    "start": "839530",
    "end": "842230"
  },
  {
    "start": "842230",
    "end": "846420"
  },
  {
    "text": "The code snippet shown here\nshows our implementation",
    "start": "846420",
    "end": "849180"
  },
  {
    "text": "of counterfactual\naugmentation on the data set.",
    "start": "849180",
    "end": "851730"
  },
  {
    "start": "851730",
    "end": "855130"
  },
  {
    "text": "By looking at the results of\ncounterfactual augmentation",
    "start": "855130",
    "end": "857860"
  },
  {
    "text": "on our data set, you can\nsee that the gap for all",
    "start": "857860",
    "end": "860230"
  },
  {
    "text": "the metrics that you're\nlooking at for the male",
    "start": "860230",
    "end": "862300"
  },
  {
    "text": "and female demographic\nis pretty much gone.",
    "start": "862300",
    "end": "864830"
  },
  {
    "text": "And this is what\nwe want and expect",
    "start": "864830",
    "end": "866800"
  },
  {
    "text": "from a fair machine\nlearning model.",
    "start": "866800",
    "end": "868810"
  },
  {
    "start": "868810",
    "end": "872070"
  },
  {
    "text": "So let's compare all\nthe metrics of interest",
    "start": "872070",
    "end": "874700"
  },
  {
    "text": "on all the approaches that\nwe've carried out so far.",
    "start": "874700",
    "end": "877240"
  },
  {
    "start": "877240",
    "end": "881270"
  },
  {
    "text": "By looking at the metric\nof overall accuracy,",
    "start": "881270",
    "end": "883605"
  },
  {
    "text": "you can see that\nsome techniques,",
    "start": "883605",
    "end": "884980"
  },
  {
    "text": "like equal number of data points\nper gender or counterfactual",
    "start": "884980",
    "end": "887829"
  },
  {
    "text": "augmentation, leads to\nhigher degrees of accuracy.",
    "start": "887830",
    "end": "891550"
  },
  {
    "text": "But you can see that\nsome techniques,",
    "start": "891550",
    "end": "893230"
  },
  {
    "text": "like gender unawareness,\ndo not always",
    "start": "893230",
    "end": "895540"
  },
  {
    "text": "guarantee higher accuracy.",
    "start": "895540",
    "end": "899519"
  },
  {
    "text": "By looking at accuracy\nacross gender,",
    "start": "899520",
    "end": "901770"
  },
  {
    "text": "you can see that the\ncounterfactual augmentation",
    "start": "901770",
    "end": "903960"
  },
  {
    "text": "technique still has the smallest\ngap between male and female.",
    "start": "903960",
    "end": "907370"
  },
  {
    "text": "But you can see that some\ntechniques like gender",
    "start": "907370",
    "end": "909370"
  },
  {
    "text": "unawareness still have a\nsignificantly higher gap",
    "start": "909370",
    "end": "912150"
  },
  {
    "text": "between the accuracy in male\nversus female demographics.",
    "start": "912150",
    "end": "918740"
  },
  {
    "text": "The plots shown\nshowed the comparison",
    "start": "918740",
    "end": "920940"
  },
  {
    "text": "between the positive\nrates across gender",
    "start": "920940",
    "end": "923250"
  },
  {
    "text": "and the comparison between the\nnegative rates across gender.",
    "start": "923250",
    "end": "926130"
  },
  {
    "start": "926130",
    "end": "930640"
  },
  {
    "text": "The plots shown here\nshow the comparison",
    "start": "930640",
    "end": "932800"
  },
  {
    "text": "between true positive\nrate across gender",
    "start": "932800",
    "end": "935230"
  },
  {
    "text": "and the comparison between\ntrue negative rates",
    "start": "935230",
    "end": "937420"
  },
  {
    "text": "across gender for\nall the techniques",
    "start": "937420",
    "end": "939430"
  },
  {
    "text": "that we covered so far.",
    "start": "939430",
    "end": "940839"
  },
  {
    "start": "940840",
    "end": "944380"
  },
  {
    "start": "944000",
    "end": "944000"
  },
  {
    "text": "In this part, we are going to\nexplore model-based debiasing",
    "start": "944380",
    "end": "947320"
  },
  {
    "text": "techniques.",
    "start": "947320",
    "end": "948400"
  },
  {
    "text": "And specifically, we will\nlook at different model types",
    "start": "948400",
    "end": "951320"
  },
  {
    "text": "and architectures and\nexamine how each one of them",
    "start": "951320",
    "end": "954550"
  },
  {
    "text": "performs for the male\nversus female demographic.",
    "start": "954550",
    "end": "957760"
  },
  {
    "start": "957760",
    "end": "960430"
  },
  {
    "text": "The motivation for\nthis is that we",
    "start": "960430",
    "end": "962020"
  },
  {
    "text": "should expect different\nmodels to have",
    "start": "962020",
    "end": "963760"
  },
  {
    "text": "different degrees of bias.",
    "start": "963760",
    "end": "965680"
  },
  {
    "text": "Therefore, by changing the\nmodel type or architecture,",
    "start": "965680",
    "end": "968589"
  },
  {
    "text": "we can observe which ones tend\nto be inherently less biased.",
    "start": "968590",
    "end": "971813"
  },
  {
    "text": "And these are the\nones that we are going",
    "start": "971813",
    "end": "973480"
  },
  {
    "text": "to choose in our application.",
    "start": "973480",
    "end": "977230"
  },
  {
    "text": "We start by examining\nsingle-model architectures.",
    "start": "977230",
    "end": "980260"
  },
  {
    "text": "And for each of the model\nfamilies shown here,",
    "start": "980260",
    "end": "982510"
  },
  {
    "text": "we picked one model and trained\nit on the data that we have.",
    "start": "982510",
    "end": "985720"
  },
  {
    "start": "985720",
    "end": "989170"
  },
  {
    "text": "This code snippet shows\ndifferent models and parameters",
    "start": "989170",
    "end": "992110"
  },
  {
    "text": "that we used.",
    "start": "992110",
    "end": "993459"
  },
  {
    "text": "For simplicity, we used\ndifferent parameters",
    "start": "993460",
    "end": "995590"
  },
  {
    "text": "for different models.",
    "start": "995590",
    "end": "997240"
  },
  {
    "text": "But in a practical\nsetting, we would",
    "start": "997240",
    "end": "999040"
  },
  {
    "text": "have to use a technique\nlike cross-validation",
    "start": "999040",
    "end": "1001829"
  },
  {
    "text": "or hyperparameter search\nto find the best parameter",
    "start": "1001830",
    "end": "1004560"
  },
  {
    "text": "to use for each model.",
    "start": "1004560",
    "end": "1005750"
  },
  {
    "start": "1005750",
    "end": "1008450"
  },
  {
    "text": "We also examined\nmulti-model architectures.",
    "start": "1008450",
    "end": "1011240"
  },
  {
    "text": "In this approach,\nwe trained a group",
    "start": "1011240",
    "end": "1012800"
  },
  {
    "text": "of different models\non the same data",
    "start": "1012800",
    "end": "1015080"
  },
  {
    "text": "and then make a final\nprediction based on consensus.",
    "start": "1015080",
    "end": "1018470"
  },
  {
    "text": "We compared two\ntypes of consensus.",
    "start": "1018470",
    "end": "1021040"
  },
  {
    "text": "The hard voting\nconsensus is the one",
    "start": "1021040",
    "end": "1023209"
  },
  {
    "text": "in which the final\nprediction is the majority",
    "start": "1023210",
    "end": "1025430"
  },
  {
    "text": "prediction among all models.",
    "start": "1025430",
    "end": "1027949"
  },
  {
    "text": "And in the soft\nvoting consensus,",
    "start": "1027950",
    "end": "1030079"
  },
  {
    "text": "the final prediction is\nthe average prediction",
    "start": "1030079",
    "end": "1032449"
  },
  {
    "text": "across all models\nin consideration.",
    "start": "1032450",
    "end": "1034639"
  },
  {
    "start": "1034640",
    "end": "1037240"
  },
  {
    "text": "We use Scikit-Learn\nVotingClassifier",
    "start": "1037240",
    "end": "1039429"
  },
  {
    "text": "to combine single models\nand train them all at once.",
    "start": "1039430",
    "end": "1042510"
  },
  {
    "text": "The code snippet shown here\nshows the models that we used",
    "start": "1042510",
    "end": "1045670"
  },
  {
    "text": "and how we trained the\nVotingClassifiers on our data.",
    "start": "1045670",
    "end": "1048840"
  },
  {
    "start": "1048840",
    "end": "1052090"
  },
  {
    "text": "Let us now evaluate and\ncompare the metrics of interest",
    "start": "1052090",
    "end": "1055010"
  },
  {
    "text": "on all model types\nand architectures",
    "start": "1055010",
    "end": "1056860"
  },
  {
    "text": "that we've trained on so far.",
    "start": "1056860",
    "end": "1058549"
  },
  {
    "start": "1058550",
    "end": "1061660"
  },
  {
    "text": "This plot shows the results\nfor overall accuracy.",
    "start": "1061660",
    "end": "1064833"
  },
  {
    "text": "You can see that\nthe random forest",
    "start": "1064833",
    "end": "1066250"
  },
  {
    "text": "classifier has the highest\ndegree of accuracy,",
    "start": "1066250",
    "end": "1069050"
  },
  {
    "text": "which is about 94%.",
    "start": "1069050",
    "end": "1070990"
  },
  {
    "text": "You can also see that the\nGaussian Naive Bayes model",
    "start": "1070990",
    "end": "1073630"
  },
  {
    "text": "has about 72% of accuracy.",
    "start": "1073630",
    "end": "1076540"
  },
  {
    "text": "And you can also see\nthat all the other models",
    "start": "1076540",
    "end": "1078490"
  },
  {
    "text": "fall in between.",
    "start": "1078490",
    "end": "1081530"
  },
  {
    "text": "This plot shows\naccuracy across gender.",
    "start": "1081530",
    "end": "1084372"
  },
  {
    "text": "And you can see that\nthere are different levels",
    "start": "1084372",
    "end": "1086330"
  },
  {
    "text": "of the gap between the male\nand female demographic,",
    "start": "1086330",
    "end": "1089299"
  },
  {
    "text": "depending on the model type.",
    "start": "1089300",
    "end": "1091070"
  },
  {
    "text": "And this is an\nexample that indicates",
    "start": "1091070",
    "end": "1093470"
  },
  {
    "text": "that different models inherently\nhave different levels of bias.",
    "start": "1093470",
    "end": "1096710"
  },
  {
    "start": "1096710",
    "end": "1099840"
  },
  {
    "text": "These plots show the\npositive and negative rates",
    "start": "1099840",
    "end": "1102090"
  },
  {
    "text": "across gender.",
    "start": "1102090",
    "end": "1103470"
  },
  {
    "text": "If you look at the plot\nfor the positive rate,",
    "start": "1103470",
    "end": "1105720"
  },
  {
    "text": "you observe that\nthe positive rate",
    "start": "1105720",
    "end": "1107190"
  },
  {
    "text": "is always higher for\nthe male demographic",
    "start": "1107190",
    "end": "1109049"
  },
  {
    "text": "than the female demographic\nfor all the models",
    "start": "1109050",
    "end": "1111660"
  },
  {
    "text": "that we've looked that.",
    "start": "1111660",
    "end": "1113090"
  },
  {
    "text": "And this can be\nproblematic if we",
    "start": "1113090",
    "end": "1114600"
  },
  {
    "text": "deploy any of these\nmodels in the real world,",
    "start": "1114600",
    "end": "1117277"
  },
  {
    "text": "because you end up in a\nscenario in which the model just",
    "start": "1117277",
    "end": "1119610"
  },
  {
    "text": "systematically predicts\nmore favorable outcome",
    "start": "1119610",
    "end": "1121920"
  },
  {
    "text": "for the male demographic\nthan the female demographic.",
    "start": "1121920",
    "end": "1124620"
  },
  {
    "start": "1124620",
    "end": "1127290"
  },
  {
    "text": "These plots show the\ntrue positive and true",
    "start": "1127290",
    "end": "1129330"
  },
  {
    "text": "negative rates across gender.",
    "start": "1129330",
    "end": "1131549"
  },
  {
    "text": "If you look at the plot\nfor the true positive rate,",
    "start": "1131550",
    "end": "1134010"
  },
  {
    "text": "you observe that the true\npositive rate is always",
    "start": "1134010",
    "end": "1136080"
  },
  {
    "text": "higher for the male individuals\nthan female individuals.",
    "start": "1136080",
    "end": "1139860"
  },
  {
    "text": "And if you look at the plot\nfor the true negative rate,",
    "start": "1139860",
    "end": "1142450"
  },
  {
    "text": "it's the other way around.",
    "start": "1142450",
    "end": "1143700"
  },
  {
    "text": "The true negative rate is\nalways higher for the female",
    "start": "1143700",
    "end": "1145950"
  },
  {
    "text": "demographic than for\nthe male demographic.",
    "start": "1145950",
    "end": "1148380"
  },
  {
    "text": "And this can especially\nbe problematic,",
    "start": "1148380",
    "end": "1150330"
  },
  {
    "text": "because it shows that our models\nhave learned how to better",
    "start": "1150330",
    "end": "1152940"
  },
  {
    "text": "classify high-income\nmale earners",
    "start": "1152940",
    "end": "1155220"
  },
  {
    "text": "than high-income female\nearners and to classify",
    "start": "1155220",
    "end": "1158130"
  },
  {
    "text": "low-income female earners than\nlow-income male earners, which",
    "start": "1158130",
    "end": "1161760"
  },
  {
    "text": "means they could be widening\nthe gap between the male earners",
    "start": "1161760",
    "end": "1165240"
  },
  {
    "text": "and the female earners.",
    "start": "1165240",
    "end": "1167530"
  },
  {
    "text": "To account for randomness, we\nran the previous experiment",
    "start": "1167530",
    "end": "1170690"
  },
  {
    "text": "five more times in order\nto get a better idea",
    "start": "1170690",
    "end": "1173059"
  },
  {
    "text": "of the average model behavior.",
    "start": "1173060",
    "end": "1175900"
  },
  {
    "text": "To compare the metrics across\nmultiple training sessions,",
    "start": "1175900",
    "end": "1179120"
  },
  {
    "text": "we created five instances\nof each model type.",
    "start": "1179120",
    "end": "1181830"
  },
  {
    "text": "And we trained each one\nof them on the data.",
    "start": "1181830",
    "end": "1184309"
  },
  {
    "text": "Then, for each one\nof these instances,",
    "start": "1184310",
    "end": "1186440"
  },
  {
    "text": "we evaluated the absolute\nvalue of the difference",
    "start": "1186440",
    "end": "1188750"
  },
  {
    "text": "in each metric of\ninterest between the male",
    "start": "1188750",
    "end": "1190790"
  },
  {
    "text": "and the female demographics\nfrom the test data.",
    "start": "1190790",
    "end": "1193220"
  },
  {
    "start": "1193220",
    "end": "1196830"
  },
  {
    "text": "If you look at the plot for the\naccuracy disparity comparison,",
    "start": "1196830",
    "end": "1200090"
  },
  {
    "text": "you can see that models\nlike logistic regression",
    "start": "1200090",
    "end": "1202400"
  },
  {
    "text": "or hard voting or SVC have a\nsignificantly lower accuracy",
    "start": "1202400",
    "end": "1206450"
  },
  {
    "text": "disparity than Gaussian\nNaive Bayes or random forest.",
    "start": "1206450",
    "end": "1209465"
  },
  {
    "start": "1209465",
    "end": "1212029"
  },
  {
    "text": "We see a similar\ntrend by looking",
    "start": "1212030",
    "end": "1213500"
  },
  {
    "text": "at the positive and\nnegative rate disparity.",
    "start": "1213500",
    "end": "1216350"
  },
  {
    "text": "If you look at models like\nlogistic regression or SVC",
    "start": "1216350",
    "end": "1219679"
  },
  {
    "text": "or had voting, you can see that\nthey have significantly lower",
    "start": "1219680",
    "end": "1223010"
  },
  {
    "text": "disparity than GNB.",
    "start": "1223010",
    "end": "1226700"
  },
  {
    "text": "Surprisingly, we see a\nsignificantly different result",
    "start": "1226700",
    "end": "1229549"
  },
  {
    "text": "by looking at the true positive\nand negative rate disparity.",
    "start": "1229550",
    "end": "1233090"
  },
  {
    "text": "If you look at the true\npositive rate disparity,",
    "start": "1233090",
    "end": "1235309"
  },
  {
    "text": "you can see that models like\nlogistic regression or SVC",
    "start": "1235310",
    "end": "1238760"
  },
  {
    "text": "now have higher disparity\nand higher variability",
    "start": "1238760",
    "end": "1241280"
  },
  {
    "text": "than models like GNB.",
    "start": "1241280",
    "end": "1243440"
  },
  {
    "text": "But if you look at the plot\nfor the true negative rate,",
    "start": "1243440",
    "end": "1246019"
  },
  {
    "text": "you can see that it tends to\nfollow the previous trend,",
    "start": "1246020",
    "end": "1248830"
  },
  {
    "text": "where logistic regression and\nSVC have lower variability",
    "start": "1248830",
    "end": "1252559"
  },
  {
    "text": "and lower disparity than GNB.",
    "start": "1252560",
    "end": "1254870"
  },
  {
    "text": "It is therefore very\nimportant to see",
    "start": "1254870",
    "end": "1257090"
  },
  {
    "text": "that these models have different\ninherent behaviors when",
    "start": "1257090",
    "end": "1260779"
  },
  {
    "text": "it comes to bias.",
    "start": "1260780",
    "end": "1264160"
  },
  {
    "start": "1264000",
    "end": "1264000"
  },
  {
    "text": "In the last part, we\nconclude by looking",
    "start": "1264160",
    "end": "1266260"
  },
  {
    "text": "at the possible next steps that\nwill allow us to strengthen",
    "start": "1266260",
    "end": "1269050"
  },
  {
    "text": "our understanding and\napplication of ethics",
    "start": "1269050",
    "end": "1271780"
  },
  {
    "text": "in machine learning from\nthe technical perspective.",
    "start": "1271780",
    "end": "1276010"
  },
  {
    "text": "Now that you've gone\nthrough the entire module,",
    "start": "1276010",
    "end": "1278290"
  },
  {
    "text": "we invite you to check\nout our GitHub repository.",
    "start": "1278290",
    "end": "1281530"
  },
  {
    "text": "This will help you\ndeepen your understanding",
    "start": "1281530",
    "end": "1283450"
  },
  {
    "text": "of the work that's being done.",
    "start": "1283450",
    "end": "1285580"
  },
  {
    "text": "In addition, we\nalso encourage you",
    "start": "1285580",
    "end": "1287409"
  },
  {
    "text": "to explore more advanced\ndebiasing techniques.",
    "start": "1287410",
    "end": "1290530"
  },
  {
    "text": "And we also recommend\nsharing and discussing these",
    "start": "1290530",
    "end": "1293080"
  },
  {
    "text": "across your team,\norganization, or community.",
    "start": "1293080",
    "end": "1296260"
  },
  {
    "text": "And of course, we all\nneed to take action",
    "start": "1296260",
    "end": "1298720"
  },
  {
    "text": "by applying what we learned\nin what we do every day.",
    "start": "1298720",
    "end": "1304010"
  },
  {
    "text": "Finally, here are the\nreferences to the materials",
    "start": "1304010",
    "end": "1306700"
  },
  {
    "text": "that we consulted while\nmaking the module.",
    "start": "1306700",
    "end": "1311220"
  },
  {
    "text": "Thank you for following this\ncourse on mitigating bias",
    "start": "1311220",
    "end": "1313770"
  },
  {
    "text": "in machine learning.",
    "start": "1313770",
    "end": "1314910"
  },
  {
    "text": "And I hope that this helps\nyou build less biased machine",
    "start": "1314910",
    "end": "1317390"
  },
  {
    "text": "learning applications\nin the future.",
    "start": "1317390",
    "end": "1320330"
  },
  {
    "text": "[MUSIC PLAYING]",
    "start": "1320330",
    "end": "1323679"
  },
  {
    "start": "1323680",
    "end": "1335000"
  }
]