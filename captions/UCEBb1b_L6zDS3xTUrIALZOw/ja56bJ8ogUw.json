[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6330"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6330",
    "end": "13320"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13320",
    "end": "20962"
  },
  {
    "text": "RUSS TEDRAKE: OK. So welcome back. We talked last time about the\nproblem of policy evaluation,",
    "start": "20962",
    "end": "45830"
  },
  {
    "text": "which was, given I'm\nexecuting some policy pi,",
    "start": "45830",
    "end": "53270"
  },
  {
    "text": "estimate the cost to go, right?",
    "start": "53270",
    "end": "60110"
  },
  {
    "text": "And we showed that it\nwas sort of trivial to do if you have a model. And if you don't\nhave a model, you",
    "start": "60110",
    "end": "68750"
  },
  {
    "text": "can still do it with the\ntemporal difference learning",
    "start": "68750",
    "end": "73970"
  },
  {
    "text": "class of algorithms, which\nis TD, which is in the title",
    "start": "73970",
    "end": "87380"
  },
  {
    "text": "there, OK? And the temporal\ndifference learning, the TD, lambda, in\nparticular, was nice",
    "start": "87380",
    "end": "94000"
  },
  {
    "text": "because it encapsulated\nall the algorithms we talked about last time. TD, if lambda is\n0 was essentially",
    "start": "94000",
    "end": "102799"
  },
  {
    "text": "the one-step\nbootstrapping idea, we said, where you use your\ncurrent cost plus your expected",
    "start": "102800",
    "end": "119930"
  },
  {
    "text": "value of all future costs as\nyour new estimate of this.",
    "start": "119930",
    "end": "126350"
  },
  {
    "text": "And the other\nlimiting case was TD1, which resulted in just\nMonte Carlo evaluation.",
    "start": "126350",
    "end": "133340"
  },
  {
    "start": "133340",
    "end": "141650"
  },
  {
    "text": "And what I said quickly at\nthe end of class is that-- we spent all our time last\ntime on doing temporal distance",
    "start": "141650",
    "end": "150650"
  },
  {
    "text": "learning just on the\nrepresentation of solving a Markov chain, right?",
    "start": "150650",
    "end": "156080"
  },
  {
    "text": " We did it for discrete state,\ndiscrete action, discrete time.",
    "start": "156080",
    "end": "162785"
  },
  {
    "start": "162785",
    "end": "172070"
  },
  {
    "text": "And it's known that\nthese algorithms converge to the correct\nestimate if you run",
    "start": "172070",
    "end": "177740"
  },
  {
    "text": "enough times and your\nMarkov chain is ergodic, if you visit all states\nat all times, OK?",
    "start": "177740",
    "end": "184739"
  },
  {
    "text": " But that's pretty\nlimiting because in order",
    "start": "184740",
    "end": "190819"
  },
  {
    "text": "to do a Markov chain analysis\nof even an acrobot or something, you have to democratize four\nstate variables-- theta,",
    "start": "190820",
    "end": "198620"
  },
  {
    "text": "theta dot, theta 1, theta\n2, theta 1 dot, theta 2 dot. So you do 25--",
    "start": "198620",
    "end": "203840"
  },
  {
    "text": "I mean, things get big fast. It'd be the 25 to\nthe 4th power if you put 25 bins in each dimension,\nand that's not very many.",
    "start": "203840",
    "end": "214890"
  },
  {
    "text": "So today, we want to make\nthe tools more relevant, more powerful, by breaking\nthis assumption that we're",
    "start": "214890",
    "end": "222349"
  },
  {
    "text": "in a Markov chain, where\nwe've discretized everything, and try to do more general-- try to do temporal\ndifference learning",
    "start": "222350",
    "end": "229940"
  },
  {
    "text": "on a more general class\nof functions that are more natively continuous, OK?",
    "start": "229940",
    "end": "235129"
  },
  {
    "text": "So we're going to do it\nwith function approximation. Now, to get there--",
    "start": "235130",
    "end": "240140"
  },
  {
    "text": "John did a little bit of\nfunction approximation in the reinforce lectures, so\nI want to basically pick up",
    "start": "240140",
    "end": "245330"
  },
  {
    "text": "on the kind of examples\nhe was showing you, and do a quick crash\ncourse on least",
    "start": "245330",
    "end": "250640"
  },
  {
    "text": "squares function approximation,\njust to make sure",
    "start": "250640",
    "end": "255717"
  },
  {
    "text": "that people are\ncomfortable with that, and we'll build on that quickly.",
    "start": "255717",
    "end": "261049"
  },
  {
    "text": "OK, so let's talk a\nlittle bit about least",
    "start": "261050",
    "end": "273000"
  },
  {
    "text": "squares function approximation. ",
    "start": "273000",
    "end": "289110"
  },
  {
    "text": "OK, so the canonical example,\nwhich is the same thing",
    "start": "289110",
    "end": "294139"
  },
  {
    "text": "that John showed you in class,\nis we want to approximate, or we want to estimate, some\nunknown function that takes",
    "start": "294140",
    "end": "314690"
  },
  {
    "text": "input x and spits out output y. ",
    "start": "314690",
    "end": "320572"
  },
  {
    "text": "And you're given\ninput/output data, which",
    "start": "320572",
    "end": "336889"
  },
  {
    "text": "we can write something like-- you're given pairs\nof data, samples",
    "start": "336890",
    "end": "343220"
  },
  {
    "text": "of the input and output. So you're allowed to\nquery the function. Given this input, what\noutput should you do?",
    "start": "343220",
    "end": "350419"
  },
  {
    "text": "In the basic case, you're\ndoing this passively. Someone just gives\nyou a data set,",
    "start": "350420",
    "end": "356150"
  },
  {
    "text": "and you're supposed\nto then do your best job at reconstructing F, OK? There are interesting cases\nthat people look at where,",
    "start": "356150",
    "end": "363965"
  },
  {
    "text": "if you're allowed\nto choose this, then how do you actively\ninterrogate the system? How do you pick the x's to get\nthe most information output?",
    "start": "363965",
    "end": "370969"
  },
  {
    "text": "In the simple case,\nlet's just say you're given a collection\nof input/output data, and you want to estimate F.",
    "start": "370970",
    "end": "380419"
  },
  {
    "text": "So the standard way to do\nthis is to write down--",
    "start": "380420",
    "end": "389900"
  },
  {
    "text": "there are many cost\nfunctions you could use, but the one most people\nuse is to write down",
    "start": "389900",
    "end": "395810"
  },
  {
    "text": "a least squares\ncost function, where we're going to try to find a\nmodel where y hat is some F of,",
    "start": "395810",
    "end": "416073"
  },
  {
    "text": "let's say, it\ndepends on parameter vector alpha, hat of x.",
    "start": "416073",
    "end": "422694"
  },
  {
    "text": "Just like in the policy\ngradient kind of case, I'm going to write\ndown a set of functions",
    "start": "422695",
    "end": "429790"
  },
  {
    "text": "where the actual function\ndepends on both alpha and x, but we can think of\nit as taking x's input",
    "start": "429790",
    "end": "435640"
  },
  {
    "text": "and generating an\nestimate y hat, OK? And you can formulate the\nproblem with the least squares",
    "start": "435640",
    "end": "442570"
  },
  {
    "text": "metric, where you're\ngoing to minimize,",
    "start": "442570",
    "end": "453430"
  },
  {
    "text": "over alpha, the squared\nerror over my data",
    "start": "453430",
    "end": "460960"
  },
  {
    "text": "minus my estimator.  I actually have\nthe estimator here.",
    "start": "460960",
    "end": "466660"
  },
  {
    "text": "No big deal, but just to keep\nit consistent with my chicken scratch notes.",
    "start": "466660",
    "end": "473460"
  },
  {
    "text": "OK?  So if we can write down\na class of functions",
    "start": "473460",
    "end": "480448"
  },
  {
    "text": "we want to search over,\nthen we can turn it into the standard optimization\nwe've been doing throughout, where we just try to find\nthe parameter vector alpha,",
    "start": "480448",
    "end": "488580"
  },
  {
    "text": "which makes our estimates\nas close as possible, in the least squares sense,\nto the actual data, yeah?",
    "start": "488580",
    "end": "494741"
  },
  {
    "text": "Or y hat here is F hat alpha xi. ",
    "start": "494741",
    "end": "506770"
  },
  {
    "text": "OK, so why do we care\nabout doing that? The methods used-- you already\nknow optimization methods.",
    "start": "506770",
    "end": "513080"
  },
  {
    "text": "We know a lot of ways\nthat we could potentially try to solve this. ",
    "start": "513080",
    "end": "519440"
  },
  {
    "text": "So for instance, we\ncould take the gradient of this with respect to alpha\nand do gradient descent, which",
    "start": "519440",
    "end": "526180"
  },
  {
    "text": "is exactly what John was\ndoing in the reinforce case, except the gradient was\nnot calculated explicitly.",
    "start": "526180",
    "end": "531220"
  },
  {
    "text": "It was estimated by sampling\nin the reinforce case, right?",
    "start": "531220",
    "end": "537230"
  },
  {
    "text": "And in general, there\nare some cases-- there are some classes of\nfunctions that we can choose,",
    "start": "537230",
    "end": "542860"
  },
  {
    "text": "and I'll make this\nmore graphical and explicit in a second,\nwhere you can just solve it analytically, right?",
    "start": "542860",
    "end": "548529"
  },
  {
    "text": "Same way in the optimal\ncontrol derivations, there were some u's\nthat we could just",
    "start": "548530",
    "end": "553600"
  },
  {
    "text": "derive analytically\nand some that we had to do gradient descent for. ",
    "start": "553600",
    "end": "559840"
  },
  {
    "text": "So let's dig in now\nand get more specific and say, what kind\nof models do we",
    "start": "559840",
    "end": "566650"
  },
  {
    "text": "want to use as candidates\nfor these functions we're trying to fit? How would we write\ndown an F hat, which",
    "start": "566650",
    "end": "573699"
  },
  {
    "text": "depends on both alpha and x? The literature is just filled\nwith people's different models",
    "start": "573700",
    "end": "580420"
  },
  {
    "text": "that they do this. One of the most popular ones\nof, let's say, 20 years ago, which actually are\nstill popular today,",
    "start": "580420",
    "end": "590042"
  },
  {
    "text": "and we call them\nfunction classes. ",
    "start": "590042",
    "end": "595118"
  },
  {
    "text": "One of them is neural networks. ",
    "start": "595118",
    "end": "604100"
  },
  {
    "text": "So a lot of people\nbelieve that a good way to write down an arbitrary\nfunction is to take your inputs",
    "start": "604100",
    "end": "617830"
  },
  {
    "text": "and try to do something\nthat models a neuron. It adds things up.",
    "start": "617830",
    "end": "623290"
  },
  {
    "text": "Adds up the different\ninputs, and then goes through a\nsigmoidal function",
    "start": "623290",
    "end": "629920"
  },
  {
    "text": "and gives you an output y. ",
    "start": "629920",
    "end": "636520"
  },
  {
    "text": "Maybe just because that's what\nthe brain sort of does, maybe because there's a lot of\nsuccess stories from it,",
    "start": "636520",
    "end": "644570"
  },
  {
    "text": "but a lot of people\ndo this, right? And moreover, this is sort of\nthe single layer neural network",
    "start": "644570",
    "end": "660620"
  },
  {
    "text": "if you have lots of\ndifferent input functions.",
    "start": "660620",
    "end": "666400"
  },
  {
    "text": " And they potentially add\nup to y, for instance.",
    "start": "666400",
    "end": "675805"
  },
  {
    "text": " And the parameters now are the\nweights of this connection.",
    "start": "675805",
    "end": "684640"
  },
  {
    "start": "684640",
    "end": "693050"
  },
  {
    "text": "So your function might be\nsome squashing function,",
    "start": "693050",
    "end": "700450"
  },
  {
    "text": "this nonlinear function,\ntimes a weighted combination",
    "start": "700450",
    "end": "705700"
  },
  {
    "text": "of the inputs, potentially\nplus some baseline or something like this, where this\nis tanh or some function",
    "start": "705700",
    "end": "715550"
  },
  {
    "text": "like that that does a sigmoid. ",
    "start": "715550",
    "end": "726570"
  },
  {
    "text": "OK. And if you do this, then you\ncan stack things up and make",
    "start": "726570",
    "end": "732073"
  },
  {
    "text": "multiple-layer neural networks. ",
    "start": "732073",
    "end": "738270"
  },
  {
    "text": "And people believe, and\npeople certainly in the '80s very strongly believed, that\nthis is a representative class",
    "start": "738270",
    "end": "747390"
  },
  {
    "text": "of functions that maybe\nlooked a little bit like what was going on in the brain.",
    "start": "747390",
    "end": "754770"
  },
  {
    "text": "And people know that if I have-- it's a general\nfunction approximator.",
    "start": "754770",
    "end": "760176"
  },
  {
    "start": "760176",
    "end": "769410"
  },
  {
    "text": "If I have enough of these\nneurons and enough layers,",
    "start": "769410",
    "end": "790120"
  },
  {
    "text": "then I can represent any\nfunction arbitrarily finally. ",
    "start": "790120",
    "end": "808332"
  },
  {
    "text": "So it's kind of a\ncool result state. Using this thing that\nlooks like the brain,",
    "start": "808332",
    "end": "814050"
  },
  {
    "text": "if I stack up these\nelements, then I can represent any function\nwith enough neurons.",
    "start": "814050",
    "end": "820149"
  },
  {
    "text": "And if I want to solve a\nleast squares optimization problem to make the input and\noutput of this neural network",
    "start": "820150",
    "end": "825937"
  },
  {
    "text": "work the same, then I can just\nsolve this optimization problem with gradient descent,\nand that's roughly",
    "start": "825937",
    "end": "832800"
  },
  {
    "text": "what everybody did in the '80s. I guess that's not\nquite fair, but-- right? ",
    "start": "832800",
    "end": "839430"
  },
  {
    "text": "And it works, actually. People still use these today\nto do face recognizers, right?",
    "start": "839430",
    "end": "848030"
  },
  {
    "text": "They use it to--  Yann LeCun down at NYU has\ngot this text recognizer",
    "start": "848030",
    "end": "856290"
  },
  {
    "text": "that they use actually to scan-- to read checks at\nthe bank, right? That's still neural\nnetwork-based, OK?",
    "start": "856290",
    "end": "863352"
  },
  {
    "text": "A lot of people\ngot a lot of things to work with these\nneural networks. Gerry Tesaro, in the\noptimal control sense, got the TD gammon--",
    "start": "863352",
    "end": "870185"
  },
  {
    "text": "the reinforcement\nlearning system to play gammon, where the\nboard configurations were",
    "start": "870185",
    "end": "875550"
  },
  {
    "text": "put in as inputs to\nthe neural network, and the output was what\nmove you should take, and he got that to work, right?",
    "start": "875550",
    "end": "881070"
  },
  {
    "text": "And actually, there was a\nvalue function that came out. He would even explicitly\nestimate the value function.",
    "start": "881070",
    "end": "887545"
  },
  {
    "text": "But today, this is\na strawman because I don't like neural networks. They're not a particularly\nnice least squares thing",
    "start": "887545",
    "end": "896920"
  },
  {
    "text": "to optimize. We can do better these days.",
    "start": "896920",
    "end": "901932"
  },
  {
    "text": "I just wanted to\nput it up there is an important element\nof the class,",
    "start": "901932",
    "end": "907440"
  },
  {
    "text": "but not one we'll\ntend to use, OK? ",
    "start": "907440",
    "end": "914840"
  },
  {
    "text": "If you care about least\nsquares function approximation, there are a lot of\nother choices you can have for your function f,\nwhich you can parameterized",
    "start": "914840",
    "end": "923990"
  },
  {
    "text": "by alpha, which maps x to y. A lot of choices, OK? The ones that people in\nreinforcement learning",
    "start": "923990",
    "end": "931370"
  },
  {
    "text": "tend to use most effectively\nare the linear function approximators.",
    "start": "931370",
    "end": "936645"
  },
  {
    "start": "936645",
    "end": "953125"
  },
  {
    "text": "Linear here,\nmeaning that they're linear in the parameters,\nbut not necessarily linear in the outputs. So I want yi hat to be\nsome linear combination",
    "start": "953125",
    "end": "969740"
  },
  {
    "text": "of potentially non-linear\nbasis functions of xi. ",
    "start": "969740",
    "end": "992510"
  },
  {
    "text": "OK. So the neural networks\nhad the problem that they were rich because they\nhave nonlinearities in them,",
    "start": "992510",
    "end": "1000050"
  },
  {
    "text": "and if you cascade\nnonlinearities, you can get arbitrarily\nrich things. But the parameters that\nchange the function",
    "start": "1000050",
    "end": "1007470"
  },
  {
    "text": "are buried deep inside\nthe nonlinearities, OK? It turns out, if you come\nup with policy and function",
    "start": "1007470",
    "end": "1013890"
  },
  {
    "text": "classes that can represent\nyour problem nicely, where the parameters\nare all at the output,",
    "start": "1013890",
    "end": "1021450"
  },
  {
    "text": "then life gets a lot better, OK? Why does life get a lot better?",
    "start": "1021450",
    "end": "1026470"
  },
  {
    "text": "Well, because now I can take\nthis least squares metric",
    "start": "1026470",
    "end": "1031949"
  },
  {
    "text": "and solve it explicitly\nfor alpha, OK? ",
    "start": "1031950",
    "end": "1044258"
  },
  {
    "text": "There's two things. I can represent arbitrary\nnon-linear functions",
    "start": "1044258",
    "end": "1051350"
  },
  {
    "text": "if I have the right basis set. ",
    "start": "1051350",
    "end": "1065810"
  },
  {
    "text": "Also, compute alpha explicitly.",
    "start": "1065810",
    "end": "1071660"
  },
  {
    "text": " Don't have to do\ngradient descent. I can just find the optimum.",
    "start": "1071660",
    "end": "1076757"
  },
  {
    "start": "1076757",
    "end": "1083195"
  },
  {
    "text": "Just to see how that goes-- you can probably see\nit, but it's so simple, we'll write it out here.",
    "start": "1083196",
    "end": "1088640"
  },
  {
    "start": "1088640",
    "end": "1096670"
  },
  {
    "text": "If I want to minimize\nthat squared error, I minimize over alpha, sum over\ni of yi minus alpha transpose.",
    "start": "1096670",
    "end": "1112570"
  },
  {
    "text": "I'll write the vector form\nof that v of xi squared.",
    "start": "1112570",
    "end": "1120549"
  },
  {
    "start": "1120550",
    "end": "1128700"
  },
  {
    "text": "I could write this\neven more vector form if I choose big Y to be y--",
    "start": "1128700",
    "end": "1134820"
  },
  {
    "text": " if I put these things\ninto a big vector,",
    "start": "1134820",
    "end": "1141720"
  },
  {
    "text": "and big phi is in\nthe other direction,",
    "start": "1141720",
    "end": "1155190"
  },
  {
    "text": "then I can write this guy as min\nover alpha y minus phi alpha.",
    "start": "1155190",
    "end": "1163320"
  },
  {
    "start": "1163320",
    "end": "1173000"
  },
  {
    "text": "If you take the gradient\nwith respect to alpha, what do you get?",
    "start": "1173000",
    "end": "1178170"
  },
  {
    "text": "You get negative phi transpose\ny minus phi alpha equals 0.",
    "start": "1178170",
    "end": "1185800"
  },
  {
    "text": " Assuming that this\nthing is well-behaved,",
    "start": "1185800",
    "end": "1192990"
  },
  {
    "text": "then you can just say alpha is-- ",
    "start": "1192990",
    "end": "1235550"
  },
  {
    "text": "very straightforward\nleast squares estimation with\nlinear approximators, but let me now convince\nyou just how rich",
    "start": "1235550",
    "end": "1243590"
  },
  {
    "text": "that class of functions is, OK? ",
    "start": "1243590",
    "end": "1292120"
  },
  {
    "text": "OK. Here's the game. There's some\nfunction that exists.",
    "start": "1292120",
    "end": "1299890"
  },
  {
    "text": "This is my actual f, OK? I don't know it. My algorithm that I'm about\nto do doesn't know it,",
    "start": "1299890",
    "end": "1305440"
  },
  {
    "text": "but that's the original f\nI'm trying to estimate, OK? And let's say I don't\nget to experience f,",
    "start": "1305440",
    "end": "1311400"
  },
  {
    "text": "but I get to sample from f,\nand every time I sample from f, I get some noise, right?",
    "start": "1311400",
    "end": "1318420"
  },
  {
    "text": "Then maybe my input/output data\nmight look something like this. This is just--",
    "start": "1318420",
    "end": "1324059"
  },
  {
    "text": "I took a bunch of\nrandom x's, I evaluated what that function was,\nand I added a little bit of random noise to it, OK?",
    "start": "1324060",
    "end": "1331350"
  },
  {
    "text": "Now the question is, if I\njust have those red data points as input/output\ndata, can I",
    "start": "1331350",
    "end": "1337110"
  },
  {
    "text": "come up with an\nestimate in f hat, which reproduces the original f?",
    "start": "1337110",
    "end": "1343380"
  },
  {
    "text": "And that's not many\ndata points, right? OK. I could do it with\na neural network, but without telling\nyou all the details,",
    "start": "1343380",
    "end": "1350547"
  },
  {
    "text": "it's not quite as elegant. We did it with reinforce. It took a little\nbit of convergence. If we do it with a linear\nfunction approximator,",
    "start": "1350547",
    "end": "1357330"
  },
  {
    "text": "we can do it just in one\nshot, just like that. The first trick,\nthough, is we have to pick our basis set, phi, OK?",
    "start": "1357330",
    "end": "1365940"
  },
  {
    "text": "You've got lots of\nchoices with phi. Some of the common ones--",
    "start": "1365940",
    "end": "1372690"
  },
  {
    "text": "let me do-- one common one is\nradial basis functions, where",
    "start": "1372690",
    "end": "1399840"
  },
  {
    "text": "you assume phi of x is\njust some Gaussian, right? ",
    "start": "1399840",
    "end": "1410429"
  },
  {
    "text": "phi i of x is just\na Gaussian function. The normalization\ndoesn't matter.",
    "start": "1410430",
    "end": "1417480"
  },
  {
    "text": "x minus mi squared to sigma i.",
    "start": "1417480",
    "end": "1423450"
  },
  {
    "text": " It's just a collection\nof Gaussians where the means are different\nfor every basis function, OK?",
    "start": "1423450",
    "end": "1431970"
  },
  {
    "text": "The variances could\nbe different too. That doesn't matter. ",
    "start": "1431970",
    "end": "1437940"
  },
  {
    "text": "So that might look like this. If I made 10 different basis\nfunctions for this problem,",
    "start": "1437940",
    "end": "1444460"
  },
  {
    "text": "and I centered them\nevenly across the space that I sampled for,\nthen that would",
    "start": "1444460",
    "end": "1449830"
  },
  {
    "text": "look like a reasonable basis\nset for function approximation. OK.",
    "start": "1449830",
    "end": "1455280"
  },
  {
    "text": "And then the question is, can\nI take a linear combination of that 10 Gaussians and turn\nit into a pretty good estimate",
    "start": "1455280",
    "end": "1461250"
  },
  {
    "text": "of the original function just\nfrom looking at the red points? OK?",
    "start": "1461250",
    "end": "1466530"
  },
  {
    "text": "If I plug this\nequation in, then I",
    "start": "1466530",
    "end": "1472680"
  },
  {
    "text": "can get a weighting on each\nof those individual phis. Alpha i is the weight\non phi i, right?",
    "start": "1472680",
    "end": "1480230"
  },
  {
    "text": " I'll do that with\nanother click here.",
    "start": "1480230",
    "end": "1487350"
  },
  {
    "text": "And it turns out\nit said, in order to represent this function, that\none that was centered around 0",
    "start": "1487350",
    "end": "1492650"
  },
  {
    "text": "had better have\nsome negative value. The one that was\ncentered around 1 has got a pretty\nbig positive value. This one's got a pretty big\npositive value and so on.",
    "start": "1492650",
    "end": "1499860"
  },
  {
    "text": "So you can see all the\nsame Gaussians are there. They're just weighted by\na different amount, OK?",
    "start": "1499860",
    "end": "1505669"
  },
  {
    "text": "And if you sum all\nthose up to estimate y, then what do you get?",
    "start": "1505670",
    "end": "1511430"
  },
  {
    "text": "Pretty good, right? It's pretty sparse\nset of data points. Pretty sparse set\nof basis functions",
    "start": "1511430",
    "end": "1517520"
  },
  {
    "text": "gets a really nice\naccurate-- in one shot. No gradient descent or anything. This is consistent with\nwhat John and I have",
    "start": "1517520",
    "end": "1523640"
  },
  {
    "text": "been saying, don't do\nreinforce unless you have to. Because if you know\nthe function and you can sample from the function,\nyou can just explicitly get it.",
    "start": "1523640",
    "end": "1531780"
  },
  {
    "text": "OK. The barycentric interpolators\nthat we talked about before--",
    "start": "1531780",
    "end": "1537170"
  },
  {
    "text": "remember, we interpolated\nbetween elements of the grid with barycentric interpolators.",
    "start": "1537170",
    "end": "1543740"
  },
  {
    "text": "Those are linear\nfunction approximators,",
    "start": "1543740",
    "end": "1555720"
  },
  {
    "text": "where it turns out,\nyou can think of that as having nonlinear\nbasis functions, which",
    "start": "1555720",
    "end": "1564890"
  },
  {
    "text": "have something like this,\nthe whole thing rectified.",
    "start": "1564890",
    "end": "1579530"
  },
  {
    "text": " Essentially, if I plot\nit in 2D here, then--",
    "start": "1579530",
    "end": "1589220"
  },
  {
    "start": "1589220",
    "end": "1597450"
  },
  {
    "text": "if I turn my radial basis\nfunctions off and do my barycentric interpolators,\nand I run that same demo,",
    "start": "1597450",
    "end": "1609650"
  },
  {
    "text": "the barycentric\ninterpolators are going to look like that, OK? If I want to linearly\ninterpolate between all",
    "start": "1609650",
    "end": "1616700"
  },
  {
    "text": "these neighboring\npoints, one way to view it is I take my\ndistance between the points, I interpolate.",
    "start": "1616700",
    "end": "1622460"
  },
  {
    "text": "Another way to view\nthat is actually, those are basis functions\nthat look like tents.",
    "start": "1622460",
    "end": "1627490"
  },
  {
    "text": "And the same thing is true\nin 2D, they're 2D tents, OK?",
    "start": "1627490",
    "end": "1632983"
  },
  {
    "text": "It's the opposite way\nto think about it, but those barycentric\ninterpolators we've been using the\nwhole time are exactly",
    "start": "1632983",
    "end": "1638150"
  },
  {
    "text": "linear basis functions. And again, I can\nsum these guys up, and I can make an approximation\nof the original non-linear",
    "start": "1638150",
    "end": "1646520"
  },
  {
    "text": "function. This one, of course, has\ngot to be piecewise linear, but that's OK. It did a pretty good\njob, considering",
    "start": "1646520",
    "end": "1652160"
  },
  {
    "text": "it's piecewise linear and it's\nnot a piecewise linear thing it's estimating.",
    "start": "1652160",
    "end": "1657890"
  },
  {
    "text": "OK, you could do basis\nfunctions based on Fourier",
    "start": "1657890",
    "end": "1665930"
  },
  {
    "text": "decompositions, for instance. ",
    "start": "1665930",
    "end": "1687710"
  },
  {
    "text": "You could do polynomials basis\nfunctions, where phi i of x",
    "start": "1687710",
    "end": "1697870"
  },
  {
    "text": "is x to the i\nminus 1, let's say.",
    "start": "1697870",
    "end": "1703059"
  },
  {
    "text": "Something like that. All these things work. ",
    "start": "1703060",
    "end": "1712850"
  },
  {
    "text": "I'll do the Fourier one here.  Same function, noisy data.",
    "start": "1712850",
    "end": "1720080"
  },
  {
    "text": "There's a Fourier basis over-- spatial Fourier basis.",
    "start": "1720080",
    "end": "1725690"
  },
  {
    "text": "I can add those up. I get very large coefficients. These tend to cancel\nthemselves out a lot,",
    "start": "1725690",
    "end": "1731630"
  },
  {
    "text": "but still, I get a pretty\nnice representation. OK. So this idea of using linear\nfunction approximators",
    "start": "1731630",
    "end": "1742659"
  },
  {
    "text": "and then linear least square--\nexact least squares solution is a very powerful idea. You can represent very\ncomplicated functions",
    "start": "1742660",
    "end": "1749450"
  },
  {
    "text": "potentially with this. This was not the\nway people tended to do things 15, 20 years ago.",
    "start": "1749450",
    "end": "1754700"
  },
  {
    "text": "It really tends to be the\nway people do things now. In fact, machine\nlearning got on this kick",
    "start": "1754700",
    "end": "1761210"
  },
  {
    "text": "in statistical learning theory. People talk about\nkernel methods, and you might know these. I mean, the essential idea\nis there's two problems.",
    "start": "1761210",
    "end": "1771890"
  },
  {
    "text": "I didn't say it,\nbut sometimes people are trying to estimate scalar--",
    "start": "1771890",
    "end": "1779360"
  },
  {
    "text": "continuous-valued outputs. Sometimes, people are trying\nto do Boolean outputs. They would call that a\nclassification problem.",
    "start": "1779360",
    "end": "1785150"
  },
  {
    "text": "There's different\nforms of this problem. But in machine learning,\npeople realized that you can often take a\nfairly low-dimensional problem,",
    "start": "1785150",
    "end": "1792770"
  },
  {
    "text": "blow it up into a very\nhigh-dimensional space using lots of basis functions,\nfor instance, lots of kernel",
    "start": "1792770",
    "end": "1799040"
  },
  {
    "text": "functions, and then\ndo linear least squares in the\nhigh-dimensional space,",
    "start": "1799040",
    "end": "1804289"
  },
  {
    "text": "and that works a lot better than\ndoing some sort of nonlinear gradient descent-based\nestimation in the low-dimensional\nspace, OK?",
    "start": "1804290",
    "end": "1810740"
  },
  {
    "text": "So that's really a trend that\nhappened in machine learning. In reinforcement learning, it's\nthe norm because essentially,",
    "start": "1810740",
    "end": "1818608"
  },
  {
    "text": "when we have linear\nfunction approximators, we have some proofs\nthat our optimal control algorithms are going to converge\nwhen we have almost nothing.",
    "start": "1818608",
    "end": "1825990"
  },
  {
    "text": "In fact, in the more nonlinear\nfunction approximator case, we have lots of\nexamples where things",
    "start": "1825990",
    "end": "1831470"
  },
  {
    "text": "like TD on a\nfunction approximator just don't converge. There are simple\nexamples where they do exactly the wrong thing.",
    "start": "1831470",
    "end": "1836555"
  },
  {
    "text": " Good. So just to convince you of one\nmore basis function that is",
    "start": "1836555",
    "end": "1845440"
  },
  {
    "text": "more relevant to this class--\nthat was just the crash course for people who haven't seen\nfunction approximators--",
    "start": "1845440",
    "end": "1852799"
  },
  {
    "text": "let me give you another example\nfrom system identification. ",
    "start": "1852800",
    "end": "1865180"
  },
  {
    "text": "Let's do nonlinear\nsystem identification",
    "start": "1865180",
    "end": "1880000"
  },
  {
    "text": "as a least squares problem with\nlinear function approximation.",
    "start": "1880000",
    "end": "1887540"
  },
  {
    "text": "So let's say we've got\nour equations of motion, which come from this. It's been a little while\nsince I wrote these equations.",
    "start": "1887540",
    "end": "1893320"
  },
  {
    "text": "Jeez. ",
    "start": "1893320",
    "end": "1905870"
  },
  {
    "text": "Let's say it's for the\npendulum or for the acrobot, you name it. And let's say we know\nthe form of the equation,",
    "start": "1905870",
    "end": "1911930"
  },
  {
    "text": "but we don't know\nthe parameters. We don't know how\nmuch the masses are, how long the link lengths are. Normally you can measure those.",
    "start": "1911930",
    "end": "1918200"
  },
  {
    "text": "But inertias, things\nlike that, these can be harder to get right. ",
    "start": "1918200",
    "end": "1923450"
  },
  {
    "text": "So we can, for instance,\nrun a bunch of trials",
    "start": "1923450",
    "end": "1928490"
  },
  {
    "text": "with some dumb open\nloop controller. Just pick you\nrandomly, let's say, and make the acrobot\nflail around a little bit",
    "start": "1928490",
    "end": "1938000"
  },
  {
    "text": "and collect data that\nlooked like this, right?",
    "start": "1938000",
    "end": "1945800"
  },
  {
    "text": "q, q dot, even q double dot,\nand u at every instant in time.",
    "start": "1945800",
    "end": "1953150"
  },
  {
    "start": "1953150",
    "end": "1959100"
  },
  {
    "text": "And this is going to be exactly\nlike our input/output data in our least squares\nformulation, OK?",
    "start": "1959100",
    "end": "1966540"
  },
  {
    "start": "1966540",
    "end": "1972490"
  },
  {
    "text": "And here's the very\namazing observation. This one really surprised\nme when I first got it.",
    "start": "1972490",
    "end": "1980560"
  },
  {
    "text": "The manipulator equations\nfor random robot arms, they're actually linear\nin their parameters.",
    "start": "1980560",
    "end": "1988270"
  },
  {
    "text": "Very non-linear functions. They do very\ncomplicated dynamics. But they tend to be, for\nmost robot manipulators--",
    "start": "1988270",
    "end": "1998277"
  },
  {
    "text": "robotic arms on the factory\nfloor, walking robots, things like that-- those equations\nactually turn out",
    "start": "1998277",
    "end": "2003720"
  },
  {
    "text": "to be linear in the parameters. They're not linear\nin q, but they're linear in the parameters.",
    "start": "2003720",
    "end": "2010002"
  },
  {
    "start": "2010003",
    "end": "2041060"
  },
  {
    "text": "All right. Take the simple pendulum. ",
    "start": "2041060",
    "end": "2067719"
  },
  {
    "text": "I can rewrite this dynamics\nas alpha transpose times",
    "start": "2067719",
    "end": "2076780"
  },
  {
    "text": "theta double dot, theta\ndot, sine of theta equals u,",
    "start": "2076780",
    "end": "2086560"
  },
  {
    "text": "where alpha is i, b, and mgl.",
    "start": "2086560",
    "end": "2093429"
  },
  {
    "start": "2093429",
    "end": "2106829"
  },
  {
    "text": "OK, after you think\nabout it a little bit, it turns out to be not\nall that surprising. In all our robot manipulators,\nyou see sine thetas everywhere.",
    "start": "2106830",
    "end": "2114400"
  },
  {
    "text": "You see sine squared thetas. You see sine cosines. You see all these things. You never see sine l theta\nor something like that, OK?",
    "start": "2114400",
    "end": "2123580"
  },
  {
    "text": "It turns out, in these problems,\nthat the parameters don't end up inside your nonlinearities.",
    "start": "2123580",
    "end": "2131380"
  },
  {
    "text": "Yeah. AUDIENCE: Isn't\nit still nonlinear because the parameters\nare multiplied together? RUSS TEDRAKE: OK, good.",
    "start": "2131380",
    "end": "2136450"
  },
  {
    "text": "So it's not linear in\nevery individual parameter, but I can get--",
    "start": "2136450",
    "end": "2142089"
  },
  {
    "text": "I can re-estimate this\nas a linear optimization. So that's exactly right. So sometimes, you have to\nsettle for groups of parameters.",
    "start": "2142090",
    "end": "2169860"
  },
  {
    "text": "But those groups of\nparameters are always enough to rewrite your\ninitial dynamics, OK?",
    "start": "2169860",
    "end": "2174990"
  },
  {
    "text": " OK, so that actually makes\nit sound like sys ID is easy.",
    "start": "2174990",
    "end": "2182940"
  },
  {
    "text": "If we have a complicated robot-- yeah, says Michael, who's been\ndoing it for the last three months, six months, maybe.",
    "start": "2182940",
    "end": "2188380"
  },
  {
    "text": "I don't know. (LAUGHS) You\njust shot daggers at me there.",
    "start": "2188380",
    "end": "2195930"
  },
  {
    "text": "Yeah, so sys ID should\nbe easy for robots that are well-behaved. It turns out, if\nyou have saturations",
    "start": "2195930",
    "end": "2201300"
  },
  {
    "text": "in your motor and stuff like\nthat, it gets more complicated. Michael could tell\nyou all about it.",
    "start": "2201300",
    "end": "2206860"
  },
  {
    "text": "But if I have a\nsimulation of a pendulum, let's say, then it should be\ntrivial, and it is trivial.",
    "start": "2206860",
    "end": "2213220"
  },
  {
    "text": "So let me just show you that. ",
    "start": "2213220",
    "end": "2222040"
  },
  {
    "text": "It turns out, it's exactly\nthe same linear function approximation. This is my basis\nfunctions, right? These are my phi's.",
    "start": "2222040",
    "end": "2228280"
  },
  {
    "text": "I've got three basis functions. One is theta double dot, one is\ntheta dot, one is sine theta. These are my\ncoefficients, alpha.",
    "start": "2228280",
    "end": "2235839"
  },
  {
    "text": "And how am I going\nto do it here? ",
    "start": "2235840",
    "end": "2241240"
  },
  {
    "text": "Where is it? ",
    "start": "2241240",
    "end": "2248000"
  },
  {
    "text": "Sys ID, yeah. Just a few lines there\nof the Matlab code,",
    "start": "2248000",
    "end": "2255570"
  },
  {
    "text": "and that includes\nrunning the tests, OK? ",
    "start": "2255570",
    "end": "2261240"
  },
  {
    "text": "So most of this is just\nsetting up my simulator. I'm going to pick some small\nrandom actions, a random tape",
    "start": "2261240",
    "end": "2272060"
  },
  {
    "text": "of torques to play out. I'm going to pick some\nrandom initial condition. I'm going to run\nmy control system",
    "start": "2272060",
    "end": "2277820"
  },
  {
    "text": "with just making that tape--",
    "start": "2277820",
    "end": "2283310"
  },
  {
    "text": "a zero-order hold\non that tape, OK? And I'm going to collect\nthe time x torque",
    "start": "2283310",
    "end": "2291620"
  },
  {
    "text": "and x dot that came\nout of that simulation, and do exactly the math\nI showed you over here,",
    "start": "2291620",
    "end": "2298070"
  },
  {
    "text": "where alpha is i, b, mgl-- in this case, it's the location\nof the center of mass--",
    "start": "2298070",
    "end": "2303590"
  },
  {
    "text": "and do my optimization like\nthat and watch what happens.",
    "start": "2303590",
    "end": "2308930"
  },
  {
    "start": "2308930",
    "end": "2321222"
  },
  {
    "text": "What's it called? ",
    "start": "2321222",
    "end": "2327119"
  },
  {
    "text": "OK, so I started from\nrandom initial conditions. I play a small random tape\nfor 10 seconds out, OK?",
    "start": "2327120",
    "end": "2334350"
  },
  {
    "text": "The actual parameters that\nI started with were this. The ones that are\nestimated after 10",
    "start": "2334350",
    "end": "2339540"
  },
  {
    "text": "seconds of running my simulation\nand doing least squares are that. It's pretty good, right?",
    "start": "2339540",
    "end": "2344730"
  },
  {
    "text": "And that was corrupted by noise. Not a lot of noise,\nI guess, but noise. ",
    "start": "2344730",
    "end": "2352740"
  },
  {
    "text": "It's easy. System identification's\neasy, right? It's actually a very,\nvery powerful observation",
    "start": "2352740",
    "end": "2358349"
  },
  {
    "text": "that you can do system\nidentification for these really hard systems just as a linear\nfunction approximation task.",
    "start": "2358350",
    "end": "2364560"
  },
  {
    "text": "One shot. That's what makes\nadaptive control tick on a lot of these\nmanipulators, right? It's a very fundamental\nobservation.",
    "start": "2364560",
    "end": "2370771"
  },
  {
    "text": "AUDIENCE: What would happen\nin the case you assigned theta to a theta?",
    "start": "2370771",
    "end": "2376090"
  },
  {
    "text": "RUSS TEDRAKE: OK, it's not going\nto work as well, but let's-- ",
    "start": "2376090",
    "end": "2389056"
  },
  {
    "text": "it's running some random\ninitial tape here. ",
    "start": "2389056",
    "end": "2394830"
  },
  {
    "text": "It's not catastrophically\nbad, actually. AUDIENCE: We have\nthis small angle.",
    "start": "2394830",
    "end": "2400700"
  },
  {
    "text": "RUSS TEDRAKE: Exactly. That one happened to be\na small angle, right? Let's see if I get-- come on.",
    "start": "2400700",
    "end": "2407337"
  },
  {
    "text": "That's a bigger one. ",
    "start": "2407337",
    "end": "2412500"
  },
  {
    "text": "That's worse, yeah? There's another way I can break\nit, just to anticipate here.",
    "start": "2412500",
    "end": "2419579"
  },
  {
    "text": " Let me put it back\nbefore I forget.",
    "start": "2419580",
    "end": "2425460"
  },
  {
    "text": "This was sine here, right?  What if I don't put enough\ncontrol torque in, OK?",
    "start": "2425460",
    "end": "2433770"
  },
  {
    "text": "I put a note to myself,\nif I make a 0.1 here, then suddenly, I'm not putting\nin very much control torque.",
    "start": "2433770",
    "end": "2440940"
  },
  {
    "text": "And why could that be a problem? AUDIENCE: Unprotected. RUSS TEDRAKE: What's that? AUDIENCE: You're unprotected.",
    "start": "2440940",
    "end": "2447350"
  },
  {
    "text": "[INAUDIBLE] RUSS TEDRAKE: Yeah, I'm\nsort of not-- exactly. I'm not simulating the\nsystem beyond the noise that I've added, and\nthat can break things.",
    "start": "2447350",
    "end": "2455000"
  },
  {
    "text": "Let's see. So now it's pretty much\njust falling almost as a passive pendulum, and\nthat breaks things more.",
    "start": "2455000",
    "end": "2463515"
  },
  {
    "text": "Although this is a\npretty easy problem. That doesn't horribly\nbreak anything. OK, that same code could have\nrun for the acrobot, right?",
    "start": "2463515",
    "end": "2471920"
  },
  {
    "text": "It couldn't have run\nfor one of our airplanes because the aerodynamics\ntends to not be",
    "start": "2471920",
    "end": "2479120"
  },
  {
    "text": "linear in the parameters. But rigid body dynamics tend\nto be linear in the parameters, right?",
    "start": "2479120",
    "end": "2485404"
  },
  {
    "text": "Doing it on the acrobot's\na little bit harder because you have to be\nmore clever in stimulating",
    "start": "2485405",
    "end": "2490490"
  },
  {
    "text": "all the dynamics with\nyour one actuator. So there are a lot\nof good problems left in system identification.",
    "start": "2490490",
    "end": "2496579"
  },
  {
    "text": "Designing sufficiently\nrich inputs to stimulate all your dynamics\nis one of the big ones. ",
    "start": "2496580",
    "end": "2505050"
  },
  {
    "text": "But function approximation\nand least squares is basically what you need to\ndo to do system identification.",
    "start": "2505050",
    "end": "2515464"
  },
  {
    "text": "AUDIENCE: So if you have,\nfor example, [INAUDIBLE]---- RUSS TEDRAKE: Yeah. AUDIENCE: And then\npresumably, you",
    "start": "2515465",
    "end": "2521410"
  },
  {
    "text": "have sine theta minus as one\nof the parts in your equations.",
    "start": "2521410",
    "end": "2528260"
  },
  {
    "text": "So if you want to get\nsomething good, you, should put that as one\nof the features, right? You have sine alpha minus gamma\nthere, and then [INAUDIBLE]..",
    "start": "2528260",
    "end": "2535912"
  },
  {
    "text": "RUSS TEDRAKE:\nThere's always a step where you have to\nlook at your equations and pull out the proper\nbasis function to describe",
    "start": "2535912",
    "end": "2542350"
  },
  {
    "text": "that class of systems. Absolutely. So if there's a sine\ntheta 1 minus theta 2",
    "start": "2542350",
    "end": "2547581"
  },
  {
    "text": "floating around in\nthere, it should be in one of your basis-- as one of your basis elements. AUDIENCE: If you want to\ndo this with this system",
    "start": "2547582",
    "end": "2554172"
  },
  {
    "text": "but you don't have\nthe knowledge, you think that it's\nlinear, but you don't know the equation for it.",
    "start": "2554172",
    "end": "2559630"
  },
  {
    "text": "RUSS TEDRAKE: Good. AUDIENCE: Then? RUSS TEDRAKE: Then\nmaybe you should do radial basis functions or\npolynomials or some other basis",
    "start": "2559630",
    "end": "2565519"
  },
  {
    "text": "set. I think a more common\nthing would be, imagine there's something\nI'm not modeling well",
    "start": "2565520",
    "end": "2570805"
  },
  {
    "text": "in my pendulum. Let's say there's some\nnonlinear friction in the joints or something like that. A common thing that people would\ndo would be to add in here some",
    "start": "2570805",
    "end": "2579109"
  },
  {
    "text": "slop terms-- let's say radial\nbasis functions or something,",
    "start": "2579110",
    "end": "2587480"
  },
  {
    "text": "just in my standard linear\nfunction approximator-- and do that, OK?",
    "start": "2587480",
    "end": "2593930"
  },
  {
    "text": "And then, now you've just added\nmore representational power to this-- you've given the basis functions\nwhich you believe to be true,",
    "start": "2593930",
    "end": "2600860"
  },
  {
    "text": "but you also add in\nsome slop terms, right? And this is-- I mean, so Slotine\ncertainly teaches this and does",
    "start": "2600860",
    "end": "2607130"
  },
  {
    "text": "this in his robots, right? For his adaptive\ncontroller, he puts those in",
    "start": "2607130",
    "end": "2612170"
  },
  {
    "text": "to capture the terms that\nhe didn't expect to show up. Yeah. AUDIENCE: How well does\nthis work for things",
    "start": "2612170",
    "end": "2617960"
  },
  {
    "text": "that aren't really smooth. Sometimes stick slope can-- it seems like if you\nplot it, it's not really",
    "start": "2617960",
    "end": "2625714"
  },
  {
    "text": "a continuous function. RUSS TEDRAKE: Continuous\ndoesn't matter, right?",
    "start": "2625715",
    "end": "2632240"
  },
  {
    "text": "If you were trying to fit\na continuous basis set to a discontinuous\nfunction, then it'll only do as well as it can\nin the least square sense.",
    "start": "2632240",
    "end": "2639300"
  },
  {
    "text": "But you can represent\narbitrary static functions of-- if it's a function of x,\nthen it should be right.",
    "start": "2639300",
    "end": "2647487"
  },
  {
    "text": "I think the more\ncommon problem, maybe in a stick slope kind of model,\nis that there's hidden state-- AUDIENCE: OK, yeah. RUSS TEDRAKE: --for\ninstance, right?",
    "start": "2647487",
    "end": "2653090"
  },
  {
    "text": "Maybe you don't know exactly\nwhat the state of the system is because there's some other--",
    "start": "2653090",
    "end": "2658180"
  },
  {
    "text": "and then you'd have to\nestimate that to put it into your basis set.",
    "start": "2658180",
    "end": "2664750"
  },
  {
    "text": "OK, people feel OK with\nleast squares estimation? Yeah? ",
    "start": "2664750",
    "end": "2670460"
  },
  {
    "text": "Good. Now let's see if we\ncan put it to use to do what we promised at\nthe beginning, which is temporal difference learning.",
    "start": "2670460",
    "end": "2678319"
  },
  {
    "text": "It's gone, isn't it? OK. ",
    "start": "2678320",
    "end": "2687320"
  },
  {
    "text": "So if you remember, and I hope\nthe impression came across when I was talking about temporal\ndifference learning,",
    "start": "2687320",
    "end": "2693560"
  },
  {
    "text": "that we made a pretty\ncomplicated update, which",
    "start": "2693560",
    "end": "2704790"
  },
  {
    "text": "was this weighted sum of one\nstep, two step, three step,",
    "start": "2704790",
    "end": "2710160"
  },
  {
    "text": "end step returns through\nsome algebraic trick,",
    "start": "2710160",
    "end": "2715292"
  },
  {
    "text": "and that's really probably the\nright way to think about it. Through some\nalgebraic trick, you could do it with a\nvery simple algorithm.",
    "start": "2715292",
    "end": "2721080"
  },
  {
    "text": "So let me just remind you\nthat that algorithm was-- ",
    "start": "2721080",
    "end": "2800790"
  },
  {
    "text": "OK. So if you remember, the\npicture we had last time was that I've got some Markov\nchain that I'm moving around.",
    "start": "2800790",
    "end": "2816020"
  },
  {
    "start": "2816020",
    "end": "2825800"
  },
  {
    "text": "As I'm walking around\nthis Markov chain,",
    "start": "2825800",
    "end": "2830870"
  },
  {
    "text": "I'm trying to-- every\ntime I visit a state, I want to update my\nestimate of the costs",
    "start": "2830870",
    "end": "2836930"
  },
  {
    "text": "to go from being in\nthat state, right? And I can do it with this\nvery simple algorithm",
    "start": "2836930",
    "end": "2845269"
  },
  {
    "text": "which keeps a decaying\neligibility trace on each node.",
    "start": "2845270",
    "end": "2852440"
  },
  {
    "text": "Every time I visit\nthis node, it goes up. And then it starts decaying\nuntil the next time I visit and it goes up. ",
    "start": "2852440",
    "end": "2860359"
  },
  {
    "text": "The rate at which\nit decays is given by the discount factor in\nmy optimal control problem",
    "start": "2860360",
    "end": "2867140"
  },
  {
    "text": "and the lambda, which\nis the amount that I",
    "start": "2867140",
    "end": "2872510"
  },
  {
    "text": "want to weight Monte Carlo-- long-term evaluations-- versus\nshort-term evaluations, right?",
    "start": "2872510",
    "end": "2878930"
  },
  {
    "text": "If lambda is 1, I'm going to let\nthat settle much more slowly, and it's going to be making\nvery long-term updates,",
    "start": "2878930",
    "end": "2885559"
  },
  {
    "text": "and if lambda is\n0, then it's just going to make an update based\non the one-step prediction, OK?",
    "start": "2885560",
    "end": "2892220"
  },
  {
    "text": "If I carry around this\neligibility trace, this would be ei as a\nfunction of time for every i,",
    "start": "2892220",
    "end": "2900710"
  },
  {
    "text": "and I do this very\nsimple update,",
    "start": "2900710",
    "end": "2906080"
  },
  {
    "text": "then there's this interpretation\nthat j hat is doing something",
    "start": "2906080",
    "end": "2911630"
  },
  {
    "text": "between Monte Carlo and\none-step bootstrapping, depending on what\nlambda you pick.",
    "start": "2911630",
    "end": "2917000"
  },
  {
    "text": " Right? ",
    "start": "2917000",
    "end": "2925500"
  },
  {
    "text": "OK, so let's say we don't have\na discrete state and action system, but we now\nhave, let's say",
    "start": "2925500",
    "end": "2934905"
  },
  {
    "text": "our barycentric interpolator\nis on a pendulum or something like that, right? ",
    "start": "2934905",
    "end": "2946830"
  },
  {
    "text": "And let's say I take a\ntrajectory through here, which I-- that was a bad\ntrajectory for a phase space,",
    "start": "2946830",
    "end": "2953800"
  },
  {
    "text": "but I take some trajectory\nthrough my state space, OK?",
    "start": "2953800",
    "end": "2960120"
  },
  {
    "text": " Let's say I'm willing to\ndiscretize the system in time",
    "start": "2960120",
    "end": "2965700"
  },
  {
    "text": "still. ",
    "start": "2965700",
    "end": "2976180"
  },
  {
    "text": "And let's say my value\nestimate is a linear function approximator here, which is\nthis barycentric grid, OK?",
    "start": "2976180",
    "end": "2984280"
  },
  {
    "text": "So at every point\nhere, I'm going to say j hat is just a weighted\ncombination of the four",
    "start": "2984280",
    "end": "2991720"
  },
  {
    "text": "neighboring points weighted\nby the distance, right? Like I said, that\nactually is exactly",
    "start": "2991720",
    "end": "3001230"
  },
  {
    "text": "of the form where I've\ngot a scalar output, and I've just got-- you\ncan think of this as being",
    "start": "3001230",
    "end": "3007440"
  },
  {
    "text": "a tent, this being a\nbasis function which has sort of a tent of region\nof applicability right here,",
    "start": "3007440",
    "end": "3016559"
  },
  {
    "text": "and added with this one that has\na tent of applicability here. And at every point, there's\na-- at every cross-hair,",
    "start": "3016560",
    "end": "3022800"
  },
  {
    "text": "there's a basis function\nthat looks like a tent. Did you get that out\nof the previous one--",
    "start": "3022800",
    "end": "3028450"
  },
  {
    "text": "the previous picture? Linearly interpolating\nbetween those four points",
    "start": "3028450",
    "end": "3033540"
  },
  {
    "text": "is the same as saying I've\ngot four basis functions that are active, and each\nof them contributes",
    "start": "3033540",
    "end": "3038790"
  },
  {
    "text": "in a way that diminishes\nfrom their point of attack. ",
    "start": "3038790",
    "end": "3046420"
  },
  {
    "text": "OK. So now I've got a linear\nfunction approximator trying to estimate j hat.",
    "start": "3046420",
    "end": "3052030"
  },
  {
    "text": "So how could I possibly\ndo temporal distance learning-- yeah, John.",
    "start": "3052030",
    "end": "3057490"
  },
  {
    "text": "AUDIENCE: Is that\nvolumetric interpolation? Barycentric has smaller-- RUSS TEDRAKE: OK, good.",
    "start": "3057490",
    "end": "3062680"
  },
  {
    "text": "So you can do barycentric\nin three or four, actually. AUDIENCE: And here, you\nhave-- this would be-- you'd have three points, though,\nright, in this problem?",
    "start": "3062680",
    "end": "3069922"
  },
  {
    "text": "RUSS TEDRAKE: This would\nstill be called barycentric, but it's true the barycentric\nwe did before was just,",
    "start": "3069922",
    "end": "3076000"
  },
  {
    "text": "you take the neighboring\nthree points. It's true. So actually, you can do\nbarycentric with the three",
    "start": "3076000",
    "end": "3082627"
  },
  {
    "text": "neighboring points or the four\nneighboring points or whatever. The volumetric is\nactually also called-- is also a barycentric\ninterpolator.",
    "start": "3082627",
    "end": "3089470"
  },
  {
    "text": "But you're right. I should have been more careful. The way we did\nbarycentric before is we took the three closest\npoints, not the four, right?",
    "start": "3089470",
    "end": "3099127"
  },
  {
    "text": "Taking the four is also\na good interpolator. It also is called a barycentric\ninterpolator, actually, right? And the question is just\nhow it grows with the state.",
    "start": "3099127",
    "end": "3105940"
  },
  {
    "text": "Most people use the\nthree closest points because then, in\nhigher dimensions, it's always n plus 1 points\ninstead of the powers of n.",
    "start": "3105940",
    "end": "3114490"
  },
  {
    "text": " Good, thank you. OK.",
    "start": "3114490",
    "end": "3121260"
  },
  {
    "text": "So I guess, then, the\ndomain of attraction is more like that\nor something, right? ",
    "start": "3121260",
    "end": "3127925"
  },
  {
    "text": "For every cross-hair,\nthere's a-- if you're doing the\ntriangle interpolation, then it's more like that.",
    "start": "3127925",
    "end": "3134870"
  },
  {
    "text": "Yeah? So what do you think? So how can we make an\nanalogy between going through discrete states\nand increasing eligibility?",
    "start": "3134870",
    "end": "3144740"
  },
  {
    "text": "This eligibility is really-- I just need to remember\nthat that state was relevant in my history of costs.",
    "start": "3144740",
    "end": "3150800"
  },
  {
    "text": " Can you think of\nan analogy of how",
    "start": "3150800",
    "end": "3156320"
  },
  {
    "text": "this function approximator\nmight play into those equations? ",
    "start": "3156320",
    "end": "3161630"
  },
  {
    "text": "What I want to get to with\nfunction approximation,",
    "start": "3161630",
    "end": "3166940"
  },
  {
    "text": "I want to get an\nupdate for alpha",
    "start": "3166940",
    "end": "3179240"
  },
  {
    "text": "that has the same sort of form. This j, remember, was-- in the Markov chain\ncase, that's a vector j,",
    "start": "3179240",
    "end": "3193099"
  },
  {
    "text": "where each element of the vector\nwas the cost to go estimate",
    "start": "3193100",
    "end": "3198470"
  },
  {
    "text": "for the i-th node, right? Now my function\napproximator is-- again,",
    "start": "3198470",
    "end": "3204619"
  },
  {
    "text": "I'm trying to estimate\na vector alpha,",
    "start": "3204620",
    "end": "3211600"
  },
  {
    "text": "but each alpha could\npotentially affect my estimate in multiple states. The power of function\napproximators",
    "start": "3211600",
    "end": "3217740"
  },
  {
    "text": "is you don't have to consider\nevery point individually. You get some generalization.",
    "start": "3217740",
    "end": "3223810"
  },
  {
    "text": "One parameter affects\nmultiple outputs. OK?",
    "start": "3223810",
    "end": "3229240"
  },
  {
    "text": "So what could possibly\nmake this tick? How would you do temporal\ndifference learning",
    "start": "3229240",
    "end": "3234460"
  },
  {
    "text": "with function approximators? ",
    "start": "3234460",
    "end": "3252460"
  },
  {
    "text": "Yeah. AUDIENCE: [INAUDIBLE] Before, we\nwere basically doing it through basis functions that\nwere deltas at every--",
    "start": "3252460",
    "end": "3258819"
  },
  {
    "text": "RUSS TEDRAKE: Good. AUDIENCE: --point. So you can break\nj hat up into be something that is actually based\non these other basis functions.",
    "start": "3258820",
    "end": "3268452"
  },
  {
    "text": "RUSS TEDRAKE: So this should\nbe-- whatever we come up with, this should hopefully\nbe the limiting case where,",
    "start": "3268452",
    "end": "3273869"
  },
  {
    "text": "if my basis function\nwas delta functions, you'd like to get back to that? I think that's,\nunfortunately, going to be a--",
    "start": "3273870",
    "end": "3283110"
  },
  {
    "text": "AUDIENCE: OK. RUSS TEDRAKE: No, no. Well, it just happens I'm going\nto be taking gradients, so-- but yeah, that's very-- that's\na very nice observation,",
    "start": "3283110",
    "end": "3290740"
  },
  {
    "text": "actually.  What's that? AUDIENCE: I think you can't\nerase because that [INAUDIBLE]",
    "start": "3290740",
    "end": "3297440"
  },
  {
    "text": "Kronecker delta, so it's-- RUSS TEDRAKE: Exactly, yeah. AUDIENCE: By delta function,\nhe meant infinity or 1?",
    "start": "3297440",
    "end": "3306079"
  },
  {
    "text": "RUSS TEDRAKE: That's what\nJohn was pointing out. So he thinks it's going to be\na Kronecker delta because it's",
    "start": "3306080",
    "end": "3312410"
  },
  {
    "text": "going to have-- I guess it could be 1, yeah? AUDIENCE: So if it\nis 1, it's actually-- it would be mapping from\na Tableau representation",
    "start": "3312410",
    "end": "3319224"
  },
  {
    "text": "to the actual function\napproximation. RUSS TEDRAKE: Yeah. AUDIENCE: But having features\nthat are just 1, and you say--",
    "start": "3319224",
    "end": "3324523"
  },
  {
    "text": "RUSS TEDRAKE: No,\nI think that's-- I think it's a very\nnice observation. If we think of each feature as\nhaving height 1 here and domain",
    "start": "3324523",
    "end": "3334430"
  },
  {
    "text": "nothing, right, then that\nshould be the limiting case where we get our Markov chain. I think that's right.",
    "start": "3334430",
    "end": "3341060"
  },
  {
    "text": "AUDIENCE: So if you just\nchanged to taking, at each step, a weighted sum of nodes--",
    "start": "3341060",
    "end": "3347198"
  },
  {
    "text": "RUSS TEDRAKE: Yeah. AUDIENCE: --then mapping that\nweighted sum, [INAUDIBLE].. ",
    "start": "3347198",
    "end": "3355100"
  },
  {
    "text": "RUSS TEDRAKE: Yeah, I mean,\nthat's effectively right. So the way that you\ncan do it turns out",
    "start": "3355100",
    "end": "3362960"
  },
  {
    "text": "to be a little bit-- again, a\nlittle bit nice and magical, OK? So it turns out-- so there's a couple of\nways to think about this.",
    "start": "3362960",
    "end": "3369538"
  },
  {
    "text": "One of them is when I'm\ngoing through the Markov chain, every time\nI get here, I'm",
    "start": "3369538",
    "end": "3375260"
  },
  {
    "text": "going to think forward about-- I'm going to do a\none-step prediction, I'm going to do a\ntwo-step prediction, I'm going to do a\nthree-step prediction.",
    "start": "3375260",
    "end": "3380960"
  },
  {
    "text": "And what happens is that these\neligibility traces are just this trick which\nmakes all that work.",
    "start": "3380960",
    "end": "3386900"
  },
  {
    "text": "If I just remember\nwhere I've been, then I can make an update, which\nis the same as looking forward. Instead, I'm\nlooking back in time",
    "start": "3386900",
    "end": "3393350"
  },
  {
    "text": "with my eligibility traces. In the function\napproximator case, doing exactly what\nyou said turns out",
    "start": "3393350",
    "end": "3398930"
  },
  {
    "text": "to be equivalent\nto remembering how important that parameter alpha\ni was in your recent history.",
    "start": "3398930",
    "end": "3408100"
  },
  {
    "text": "OK?  Does that make sense?",
    "start": "3408100",
    "end": "3413830"
  },
  {
    "text": "If I remember the\ncontribution that alpha made-- let's say one of the elements\nof alpha, alpha i, alpha j--",
    "start": "3413830",
    "end": "3421510"
  },
  {
    "text": "made in my recent\nhistory, then I can update alpha in\nthe same sort of trick",
    "start": "3421510",
    "end": "3427809"
  },
  {
    "text": "that this eligibility\ntrace worked on for. AUDIENCE: Would you do some\nkind of decaying exponential? RUSS TEDRAKE: Yeah, yeah.",
    "start": "3427810",
    "end": "3433972"
  },
  {
    "text": "AUDIENCE: That's kind of\nwhat it's doing there. RUSS TEDRAKE: This is\nexactly what it's doing here.",
    "start": "3433972",
    "end": "3439030"
  },
  {
    "text": "And here, we had\nthe special case where when I visited\nthe state, I got a 1, and when I didn't\nvisit it, I get here.",
    "start": "3439030",
    "end": "3446299"
  },
  {
    "text": "I got nothing. It just decayed. And what we're going to get\nnow is an eligibility trace",
    "start": "3446300",
    "end": "3452260"
  },
  {
    "text": "on alpha. Do you have it? Yeah? AUDIENCE: Well, I mean,\ndoes this thing in brackets",
    "start": "3452260",
    "end": "3458832"
  },
  {
    "text": "need to be changed at all? RUSS TEDRAKE: Excellent. AUDIENCE: It seems like delta-- RUSS TEDRAKE: The\neligibility trace changes.",
    "start": "3458832",
    "end": "3465143"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] ",
    "start": "3465143",
    "end": "3475812"
  },
  {
    "text": "RUSS TEDRAKE: Perfect, right? ",
    "start": "3475812",
    "end": "3486620"
  },
  {
    "text": "This is gamma. It's going to do a\ndecaying exponential still.",
    "start": "3486620",
    "end": "3492869"
  },
  {
    "text": "You want to forget things. This thing is now-- ",
    "start": "3492870",
    "end": "3506540"
  },
  {
    "text": "the new eligibility trace here\nis the same size as alpha, not the number of\nnodes in the system.",
    "start": "3506540",
    "end": "3512650"
  },
  {
    "text": "And the amount that I'm going\nto credit each alpha with",
    "start": "3512650",
    "end": "3518950"
  },
  {
    "text": "is the gradient of my estimate. ",
    "start": "3518950",
    "end": "3537540"
  },
  {
    "text": "Does that sort of make sense? Yeah? In the case of linear\nfunction approximators,",
    "start": "3537540",
    "end": "3543810"
  },
  {
    "text": "the gradient of this\nis just phi of x. ",
    "start": "3543810",
    "end": "3551859"
  },
  {
    "text": "Partial j, partial alpha. Just gets rid of that alpha. ",
    "start": "3551860",
    "end": "3558299"
  },
  {
    "text": "OK? So I remember the\nrelative contribution of each of my alphas\nin the recent past,",
    "start": "3558300",
    "end": "3563760"
  },
  {
    "text": "and then, based on this\ne, I make the same update that I did before. Just copy this down.",
    "start": "3563760",
    "end": "3570090"
  },
  {
    "text": "But my e is now the\neligibility on alpha.",
    "start": "3570090",
    "end": "3575340"
  },
  {
    "start": "3575340",
    "end": "3582880"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] based on\nthat one to the one that gets visited and zero to the others.",
    "start": "3582880",
    "end": "3591123"
  },
  {
    "text": "RUSS TEDRAKE: Which is what? What's that? Yeah? ",
    "start": "3591123",
    "end": "3603160"
  },
  {
    "text": "OK. So that is an intuitively\ncorrect algorithm, I think.",
    "start": "3603160",
    "end": "3608850"
  },
  {
    "text": "So it seems pretty natural,\nusing the eligibility argument, that this could work.",
    "start": "3608850",
    "end": "3615400"
  },
  {
    "text": "Proving that it works\nturns out to be a pain. It's not actually an update\nlike we would normally have.",
    "start": "3615400",
    "end": "3624009"
  },
  {
    "text": "There's a special case. So in one case, when\nlambda equals 1,",
    "start": "3624010",
    "end": "3634780"
  },
  {
    "text": "then you can actually\nshow that this TD update with linear\nfunction approximation",
    "start": "3634780",
    "end": "3648550"
  },
  {
    "text": "is doing gradient descent on\nthe difference between my j",
    "start": "3648550",
    "end": "3665860"
  },
  {
    "text": "lambda-- what I call j lambda-- and my other j squared.",
    "start": "3665860",
    "end": "3679020"
  },
  {
    "text": "OK? So in the lambda versus\n1 case, it actually is doing gradient descent\non the Monte Carlo error.",
    "start": "3679020",
    "end": "3689329"
  },
  {
    "text": "In every other\nsetting of lambda, the algorithm is not a\nstandard optimization framework",
    "start": "3689330",
    "end": "3695180"
  },
  {
    "text": "of going down in some steepest\ndescent kind of approach. But it tends-- in\nsome cases, it works",
    "start": "3695180",
    "end": "3700819"
  },
  {
    "text": "faster because it uses previous\ninformation in a heuristic sort of way, but it does\nit very effectively.",
    "start": "3700820",
    "end": "3707540"
  },
  {
    "text": "And people have done the work. In fact, the work\nwas done upstairs by Tsitsiklis and Van Roy in\n'97 or something like this",
    "start": "3707540",
    "end": "3714710"
  },
  {
    "text": "to prove that, for all\nlambdas between 0 and 1 in linear function\napproximators,",
    "start": "3714710",
    "end": "3721910"
  },
  {
    "text": "that this update will go\nto the true value estimate",
    "start": "3721910",
    "end": "3727670"
  },
  {
    "text": "as your system runs.  OK?",
    "start": "3727670",
    "end": "3734060"
  },
  {
    "text": "AUDIENCE: Did they mention\nthat the [INAUDIBLE]?? ",
    "start": "3734060",
    "end": "3740300"
  },
  {
    "text": "RUSS TEDRAKE: They should. AUDIENCE: [INAUDIBLE] I guess\ndifferent algorithms can reach",
    "start": "3740300",
    "end": "3746435"
  },
  {
    "text": "a different result,\nand lambda equals 1 only converges to the actual\nthing that you're looking for.",
    "start": "3746435",
    "end": "3754890"
  },
  {
    "text": "As you start varying\nlambda, you converge toward different\nthings, but it's still converging [INAUDIBLE].",
    "start": "3754890",
    "end": "3761086"
  },
  {
    "start": "3761086",
    "end": "3766589"
  },
  {
    "text": "RUSS TEDRAKE: That's\npossible, but-- AUDIENCE: Different lambdas? RUSS TEDRAKE: I mean-- AUDIENCE: Once\nyour [INAUDIBLE] is",
    "start": "3766590",
    "end": "3772360"
  },
  {
    "text": "correct, doesn't lambda\nnot matter anymore? Because it's going to\nproject to the future-- RUSS TEDRAKE: Yeah, the\nonly stationary point",
    "start": "3772360",
    "end": "3778095"
  },
  {
    "text": "should be the true\ncost to go function. So if it converges--",
    "start": "3778095",
    "end": "3784707"
  },
  {
    "text": "I'd be surprised if that's true. AUDIENCE: Your\nlambda [INAUDIBLE]---- RUSS TEDRAKE: I mean,\nAlborz has done the stuff,",
    "start": "3784707",
    "end": "3790137"
  },
  {
    "text": "so you might know\nit better than I do. But I was under the impression\nit converges to the true value",
    "start": "3790137",
    "end": "3796510"
  },
  {
    "text": "estimate. I mean, it's all based on\nthese contraction mappings,",
    "start": "3796510",
    "end": "3802480"
  },
  {
    "text": "and I think the stationary\npoint of the contraction is the true value. ",
    "start": "3802480",
    "end": "3810075"
  },
  {
    "text": "If you find out\ndifferently, then tell us next class, definitely. ",
    "start": "3810075",
    "end": "3816319"
  },
  {
    "text": "OK. So what just happened? So I made a trivial\nchange to the algorithm.",
    "start": "3816320",
    "end": "3823099"
  },
  {
    "text": "In fact, in some ways,\nit looks almost easier. It's one less line. And it now suddenly works with\nlinear function approximators.",
    "start": "3823100",
    "end": "3831640"
  },
  {
    "text": "So I don't have to feel like\nI discretized my state space. I can cover my state space\nwith radial basis functions.",
    "start": "3831640",
    "end": "3837370"
  },
  {
    "text": "That might be as\npainful, by the way, as discretizing the\nstate space if you have to put a lot of radial\nbasis functions down.",
    "start": "3837370",
    "end": "3842650"
  },
  {
    "text": "But I could also do it\nwith more complicated-- I could do it with\npolynomials, I could do it with Fourier bases,\nand potentially,",
    "start": "3842650",
    "end": "3851980"
  },
  {
    "text": "things that work with\nless basis functions over a very large domain.",
    "start": "3851980",
    "end": "3858640"
  },
  {
    "text": "And now suddenly,\nthe tools scale up to little bit higher\ndimensional systems,",
    "start": "3858640",
    "end": "3863740"
  },
  {
    "text": "as high as you can imagine. As creative as your basis\nset allows you to be. ",
    "start": "3863740",
    "end": "3871510"
  },
  {
    "text": "To complain about\nthese algorithms is that they're inefficient\nin their use of data.",
    "start": "3871510",
    "end": "3876970"
  },
  {
    "text": " So if you think-- certainly when we're shooting\nplanes and they break,",
    "start": "3876970",
    "end": "3886600"
  },
  {
    "text": "or if we're using a walking\nrobot and it falls down a lot, then every piece of data should\nbe treated with reverence,",
    "start": "3886600",
    "end": "3896020"
  },
  {
    "text": "right? This is hard-earned data. These algorithms, as written\nlike this, basically,",
    "start": "3896020",
    "end": "3903010"
  },
  {
    "text": "every time they\nvisit the data, they make some small\nincremental update,",
    "start": "3903010",
    "end": "3910060"
  },
  {
    "text": "and a throw it away\nand keep moving. And so, by no means are\nthey efficient in data.",
    "start": "3910060",
    "end": "3916330"
  },
  {
    "text": "They are efficient\nonly in the sense that they use previous\nestimates of j hat to bootstrap,",
    "start": "3916330",
    "end": "3923710"
  },
  {
    "text": "but there's no sense of\nremembering all your data and reusing it. So some people have thought\nabout replaying trajectories.",
    "start": "3923710",
    "end": "3932750"
  },
  {
    "text": "So if you store all your\ntrajectories-- let's say I ran my plane\n10 times, well, I'll remember all that\ndata and I'll just",
    "start": "3932750",
    "end": "3938860"
  },
  {
    "text": "run the TD update over and\nover again over those same 10 trajectories. That's a reasonable thing to do.",
    "start": "3938860",
    "end": "3944743"
  },
  {
    "text": "But again, with linear\nfunction approximators, you can do better, right? You can do LSTD, least squares\ntemporal difference learning.",
    "start": "3944743",
    "end": "3950710"
  },
  {
    "start": "3950710",
    "end": "3977210"
  },
  {
    "text": "This is least squares temporal\ndifference learning, yeah? ",
    "start": "3977210",
    "end": "3999630"
  },
  {
    "text": "The argument basically\ngoes like this. ",
    "start": "3999630",
    "end": "4005387"
  },
  {
    "text": "So in learning, there's\nalways a difference-- people like to\ndistinguish between online",
    "start": "4005387",
    "end": "4011810"
  },
  {
    "text": "versus batch updates. So the online-- this\nis an online update. I took my piece of data in. I immediately changed alpha\nand then I spat it out, right?",
    "start": "4011810",
    "end": "4021970"
  },
  {
    "text": "You could imagine a \"batch\"\nupdate, which collected a bunch of these trajectories. Let's say it ran a whole\ntrajectory with the plane.",
    "start": "4021970",
    "end": "4029090"
  },
  {
    "text": "Stop. Now process that trajectory,\nmake a change in alpha, and then repeat.",
    "start": "4029090",
    "end": "4034830"
  },
  {
    "text": "So instead of doing it at\nevery single time step, let's collect a little bit of\ndata and then make an update,",
    "start": "4034830",
    "end": "4040310"
  },
  {
    "text": "OK? So let's just write down\nwhat a batch update-- ",
    "start": "4040310",
    "end": "4050240"
  },
  {
    "text": "the batch update for this\nsystem, if I ran a trajectory",
    "start": "4050240",
    "end": "4056630"
  },
  {
    "text": "and then made an\nupdate, that update",
    "start": "4056630",
    "end": "4063829"
  },
  {
    "text": "would look like this,\njust by applying that rule a bunch of times\nbut without changing alpha",
    "start": "4063830",
    "end": "4069950"
  },
  {
    "text": "in between, right? So another way to say it is\nI'm going to hold alpha fixed,",
    "start": "4069950",
    "end": "4076220"
  },
  {
    "text": "collect up all\nthe changes I want to make at alpha, and then,\nat the end of the run, make one change to alpha, OK?",
    "start": "4076220",
    "end": "4082819"
  },
  {
    "text": "Well, then that change, if\nyou do it in the batch mode, is just going to be a\nsum of all these guys.",
    "start": "4082820",
    "end": "4089690"
  },
  {
    "start": "4089690",
    "end": "4096219"
  },
  {
    "text": "That's a scalar times a vector. So I can reorder it\nwithout any pain.",
    "start": "4096220",
    "end": "4102729"
  },
  {
    "text": "Oops. I got to put that inside. ",
    "start": "4102729",
    "end": "4126475"
  },
  {
    "text": "Let me write out the\nintermediate step here. j alpha ik plus 1\nminus j alpha ik.",
    "start": "4126475",
    "end": "4137085"
  },
  {
    "text": " Agree with that?",
    "start": "4137085",
    "end": "4142880"
  },
  {
    "text": "That's the batch update of that. I just collect them all\nup, I sum them over k here,",
    "start": "4142880",
    "end": "4149960"
  },
  {
    "text": "all my time steps. That's what my update's\ngoing to look like. ",
    "start": "4149960",
    "end": "4172089"
  },
  {
    "text": "If I write this a\nlittle bit-- if I break into my function\napproximator there,",
    "start": "4172090",
    "end": "4179000"
  },
  {
    "text": "I can write it like this. hm over k. Oh, boy, I forgot\nmy ek over here.",
    "start": "4179000",
    "end": "4187339"
  },
  {
    "text": "Sorry about that. There's an ek there too, right? ",
    "start": "4187340",
    "end": "4231070"
  },
  {
    "text": "j is just phi times alpha, so\nI'm going to break into that. ",
    "start": "4231070",
    "end": "4242340"
  },
  {
    "text": "If I collect those terms,\nthen I get something",
    "start": "4242340",
    "end": "4248068"
  },
  {
    "text": "we can think about again.  Sorry, a times alpha, where this\nis b and this guy here is a.",
    "start": "4248068",
    "end": "4268390"
  },
  {
    "start": "4268390",
    "end": "4281550"
  },
  {
    "text": "In other words, the long-term\nbehavior of this system, if I collect the updates and\nthen make them like this,",
    "start": "4281550",
    "end": "4290400"
  },
  {
    "text": "well, this looks like\na low-pass filter, really, that's going\nto this solution. Yeah. ",
    "start": "4290400",
    "end": "4296985"
  },
  {
    "text": "The steady state is alpha\nequals a inverse phi,",
    "start": "4296985",
    "end": "4306210"
  },
  {
    "text": "where you do that\ninverse carefully. svd or something. ",
    "start": "4306210",
    "end": "4320960"
  },
  {
    "text": "OK.  So that observation led\nto this least squares",
    "start": "4320960",
    "end": "4328830"
  },
  {
    "text": "temporal difference\nlearning algorithm, which said instead of chewing\nup every piece of-- every data",
    "start": "4328830",
    "end": "4334170"
  },
  {
    "text": "point and spitting it\nout, remember everything that you have\nexperienced in the past.",
    "start": "4334170",
    "end": "4341310"
  },
  {
    "text": "And instead of doing this\nsort of incremental step",
    "start": "4341310",
    "end": "4346620"
  },
  {
    "text": "that goes epsilon\ntowards the steady state, you can solve for\nthat steady state",
    "start": "4346620",
    "end": "4351900"
  },
  {
    "text": "every time you have--\nif you collect b and a, you can just collect that\nwith a bunch of data,",
    "start": "4351900",
    "end": "4356912"
  },
  {
    "text": "go ahead and jump\nright to the solution. ",
    "start": "4356913",
    "end": "4361930"
  },
  {
    "text": "So LSTD, what I\nthink a lot of people would consider the\nstate of the art",
    "start": "4361930",
    "end": "4368170"
  },
  {
    "text": "if you just want to\ndo policy evaluation,",
    "start": "4368170",
    "end": "4375630"
  },
  {
    "text": "build up b and a as you run. ",
    "start": "4375630",
    "end": "4386520"
  },
  {
    "text": "Compute alpha is a inverse\nb whenever you need",
    "start": "4386520",
    "end": "4393030"
  },
  {
    "text": "a new estimate of alpha, OK? ",
    "start": "4393030",
    "end": "4399645"
  },
  {
    "text": "Why do you want to do that? ",
    "start": "4399645",
    "end": "4406850"
  },
  {
    "text": "It's more efficient\nwith your data. You remember and replay\nyour last data seamlessly.",
    "start": "4406850",
    "end": "4417340"
  },
  {
    "text": "You don't have to guess\nsome silly learning rate. ",
    "start": "4417340",
    "end": "4423712"
  },
  {
    "text": "And it doesn't even depend\non your initial conditions anymore, your initial\nguess at alpha.",
    "start": "4423712",
    "end": "4436699"
  },
  {
    "start": "4436700",
    "end": "4442966"
  },
  {
    "text": "OK? So if I went to go and do--\nif I were given a robot",
    "start": "4442966",
    "end": "4449690"
  },
  {
    "text": "that's currently performing\nsome feedback controller. Let's say it's stochastic.",
    "start": "4449690",
    "end": "4454880"
  },
  {
    "text": "It's a stochastic system. There's noise and\neverything like that. And I wanted to just evaluate\nhow well it performed",
    "start": "4454880",
    "end": "4461840"
  },
  {
    "text": "on this cost function, some cost\nfunction that someone gave me. If [INAUDIBLE] says\nI got this robot,",
    "start": "4461840",
    "end": "4469028"
  },
  {
    "text": "I wanted it to optimize\nthis cost function. How is it doing? Tell me how it's doing. I would look at that--",
    "start": "4469028",
    "end": "4475477"
  },
  {
    "text": "I would look at the state space. I'd try to come up with a\nlinear tiling of radial basis",
    "start": "4475477",
    "end": "4480530"
  },
  {
    "text": "functions, or some\nlinear function approximator over\nthat state space, and I'd start running trials\nand I'd keep those trials",
    "start": "4480530",
    "end": "4489679"
  },
  {
    "text": "and store up these matrices,\nand do a least squares temporal difference update.",
    "start": "4489680",
    "end": "4495889"
  },
  {
    "text": "And this result\nbasically tells me that it's going to do\nthe same thing as the TD.",
    "start": "4495890",
    "end": "4501290"
  },
  {
    "text": "It's going to do it potentially\nmore effectively because it's more efficient\nwith data, and it's",
    "start": "4501290",
    "end": "4506719"
  },
  {
    "text": "going to do it without having\nto guess initial vector",
    "start": "4506720",
    "end": "4512300"
  },
  {
    "text": "or having some learning rate. AUDIENCE: Do you\nactually have to define the length of an\nepisode, for example,",
    "start": "4512300",
    "end": "4517440"
  },
  {
    "text": "if it's a periodic system\nlike a walking system? [INAUDIBLE]",
    "start": "4517440",
    "end": "4526160"
  },
  {
    "text": "RUSS TEDRAKE:\nThat's really good. So Alborz here has\nactually written a paper on incremental LSTD.",
    "start": "4526160",
    "end": "4533390"
  },
  {
    "text": "So you might think that doing\nthat update is expensive, and it can be if you\njust naively do that.",
    "start": "4533390",
    "end": "4540560"
  },
  {
    "text": "But you can do\nrecursively squares, and you can sort of make\na cheaper online update",
    "start": "4540560",
    "end": "4545840"
  },
  {
    "text": "to try to do this, to make this\na constant update of alpha. If you choose to update\nalpha at every step, which",
    "start": "4545840",
    "end": "4551960"
  },
  {
    "text": "you could choose to do,\nand maybe you would choose to do on a non-episodic\ntask, or maybe in walking, you do it once per\nstep or whatever,",
    "start": "4551960",
    "end": "4557905"
  },
  {
    "text": "then you can do it nicely\nwith an incremental LSTD. And you can look\nat Alborz's paper to figure out how\nto do that, which is an approximation\nof the true LSTD,",
    "start": "4557905",
    "end": "4564590"
  },
  {
    "text": "but a pretty good one in\nyour experiments, right? And much more efficient, right?",
    "start": "4564590",
    "end": "4570660"
  },
  {
    "text": " Yeah, or there's a lot of time-- I mean, I think, in\nwalking, it turns out",
    "start": "4570660",
    "end": "4576650"
  },
  {
    "text": "I would do it probably\nonce per step or something. There's natural discretizations. But there's nothing\nto say, if you have the computational\nhorsepower, that you couldn't",
    "start": "4576650",
    "end": "4583617"
  },
  {
    "text": "just do this every step too. It's just more expensive than\ndoing an incremental version. ",
    "start": "4583617",
    "end": "4592500"
  },
  {
    "text": "OK? So function approximation's\nvery powerful. This is what's going to\ntake our tabular based ideas",
    "start": "4592500",
    "end": "4597780"
  },
  {
    "text": "and our Markov\nchain ideas and make them scale to the real world. This is the first year I did\nfunction approximation first",
    "start": "4597780",
    "end": "4605940"
  },
  {
    "text": "in the temporal\ndifferent learning case, but of course it's relevant\nfor the policy gradient world",
    "start": "4605940",
    "end": "4611850"
  },
  {
    "text": "too, right? John showed you different\nfunction approximators that were doing reinforce.",
    "start": "4611850",
    "end": "4618140"
  },
  {
    "text": "Instead of parameterizing my\nvalue function as a function approximator, I could have also\njust directly parameterized",
    "start": "4618140",
    "end": "4625199"
  },
  {
    "text": "my feedback controller\nas a value function, and done gradient\ndescent if I had a model,",
    "start": "4625200",
    "end": "4630720"
  },
  {
    "text": "or reinforce if I\ndidn't have a model. Function approximation\nis supposed to be the savior of\nreinforcement learning.",
    "start": "4630720",
    "end": "4637800"
  },
  {
    "text": "The problem is there's\nlimited results. I mean, the linear\nfunction approximation is really the only case we\nhave strong results for most",
    "start": "4637800",
    "end": "4644220"
  },
  {
    "text": "of these cases. So I'm going to talk about doing\nthe policy stuff with function",
    "start": "4644220",
    "end": "4650039"
  },
  {
    "text": "approximation on\nThursday, and then it culminates in actor-critic,\nwhere you do both function approximation in the\npolicy and the value",
    "start": "4650040",
    "end": "4657270"
  },
  {
    "text": "function simultaneously. That'll happen\nsometime next week. Excellent.",
    "start": "4657270",
    "end": "4662310"
  },
  {
    "text": "I'll see you next week. ",
    "start": "4662310",
    "end": "4665000"
  }
]