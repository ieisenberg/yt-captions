[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "7410"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "7410",
    "end": "13960"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "13960",
    "end": "19140"
  },
  {
    "start": "19140",
    "end": "24220"
  },
  {
    "text": "PROFESSOR: OK. Today we're going to finish\nup with Markov chains.",
    "start": "24220",
    "end": "30230"
  },
  {
    "text": "And the last topic will be\ndynamic programming. I'm not going to say an awful\nlot about dynamic programming.",
    "start": "30230",
    "end": "39900"
  },
  {
    "text": "It's a topic that was enormously\nimportant in research for probably 20\nyears from 1960 until",
    "start": "39900",
    "end": "49600"
  },
  {
    "text": "about 1980, or 1990. And it seemed as if half the\nPh.D. theses done in the",
    "start": "49600",
    "end": "60300"
  },
  {
    "text": "control area and the\noperations research were in this area.",
    "start": "60300",
    "end": "67630"
  },
  {
    "text": "Suddenly, everything seemed\nto be done, could be done. And strangely enough, not\nmany people seem to",
    "start": "67630",
    "end": "75310"
  },
  {
    "text": "know about it anymore. It's an enormously useful\nalgorithm for solving an awful",
    "start": "75310",
    "end": "80760"
  },
  {
    "text": "lot of different problems. It's quite a simple algorithm. You don't need the full power\nof Markov chains in order to",
    "start": "80760",
    "end": "88780"
  },
  {
    "text": "understand it. So I do want to at least talk\nabout it a little bit.",
    "start": "88780",
    "end": "94250"
  },
  {
    "text": "And we will use what we've done\nso far with Markov chains in order to understand it.",
    "start": "94250",
    "end": "100940"
  },
  {
    "text": "I want to start out today by\nreviewing a little bit of what we did last time about\neigenvalues and eigenvectors.",
    "start": "100940",
    "end": "109039"
  },
  {
    "text": "This was a somewhat awkward\ntopic to talk about, because",
    "start": "109040",
    "end": "116320"
  },
  {
    "text": "you people have very different\nbackgrounds in linear algebra. Some of you have a very strong\nbackground, some of you have",
    "start": "116320",
    "end": "123450"
  },
  {
    "text": "almost no background. So it was a lot of material for\nthose of you who know very",
    "start": "123450",
    "end": "130509"
  },
  {
    "text": "little about linear algebra. And probably somewhat boring\nfor those of you",
    "start": "130509",
    "end": "136620"
  },
  {
    "text": "use it all the time. At any rate, if you don't know\nanything about it, linear",
    "start": "136620",
    "end": "142670"
  },
  {
    "text": "algebra is a topic that you\nought to understand for almost",
    "start": "142670",
    "end": "148819"
  },
  {
    "text": "anything you do. If you've gotten to this point\nwithout having to study it,",
    "start": "148820",
    "end": "155230"
  },
  {
    "text": "it's very strange. So you should probably take\nsome extra time out, not",
    "start": "155230",
    "end": "161720"
  },
  {
    "text": "because you need it so\nmuch for this course. We won't use it enormously\nin many of the things we do later.",
    "start": "161720",
    "end": "168500"
  },
  {
    "text": "But you will use it so many\ntimes in the future that you ought to just sit down, not to\nlearn abstract linear algebra,",
    "start": "168500",
    "end": "176870"
  },
  {
    "text": "which is very useful also, but\njust to learn how to use the topic of solving linear\nequations.",
    "start": "176870",
    "end": "183280"
  },
  {
    "text": "Being able to express them\nin terms of matrices. Being able to use the\neigenvalues and eigenvectors,",
    "start": "183280",
    "end": "189310"
  },
  {
    "text": "and matrices as a way of\nunderstanding these things. So I want to say a little more\nabout that today, which is why",
    "start": "189310",
    "end": "196440"
  },
  {
    "text": "I've called this a review\nplus of eigenvalues and eigenvectors. It's a review of the topics\nwe did last time, but it's",
    "start": "196440",
    "end": "205930"
  },
  {
    "text": "looking at it in a somewhat\ndifferent way. So let's proceed with that.",
    "start": "205930",
    "end": "212150"
  },
  {
    "text": "We said that the determinant of\nan M by M matrix is given by this strange formula.",
    "start": "212150",
    "end": "218530"
  },
  {
    "text": "The determinant of a is the sum\nover all permutations of",
    "start": "218530",
    "end": "224340"
  },
  {
    "text": "the integers 1 to M of the\nproduct from i equals 1 to M",
    "start": "224340",
    "end": "231260"
  },
  {
    "text": "of the matrix element\na sub i mu of i. Mu of i is the permutation of\nthe number i. i is between one",
    "start": "231260",
    "end": "241670"
  },
  {
    "text": "and M, and mu of i is a\npermutation of that. Now if you look at the matrix,\nwhich has the form, which is",
    "start": "241670",
    "end": "257528"
  },
  {
    "text": "block upper diagonal. In other words, there's a matrix\nhere, a square matrix a",
    "start": "257529",
    "end": "262990"
  },
  {
    "text": "sub t, which is a transient\nmatrix. There's a recurrent matrix here,\nand there's some way of",
    "start": "262990",
    "end": "271610"
  },
  {
    "text": "getting from the transient\nstates to the recurring states.",
    "start": "271610",
    "end": "276729"
  },
  {
    "text": "And this is the general form\nthat a unit chain has to have. There are a bunch of transient\nstates, there are a bunch of",
    "start": "276730",
    "end": "284970"
  },
  {
    "text": "recurring states. And the interesting thing here\nis that the determinant of a",
    "start": "284970",
    "end": "292630"
  },
  {
    "text": "is exactly the determinant\nof a sub t times the determinant a sub r.",
    "start": "292630",
    "end": "299410"
  },
  {
    "text": "I'm calling this a instead of\nthe transition matrix p because I want to replace a by\np minus lambda i, so I can",
    "start": "299410",
    "end": "308840"
  },
  {
    "text": "talk about the eigenvalues\nof p. So when I do that replacement\nhere, if I know that the",
    "start": "308840",
    "end": "315690"
  },
  {
    "text": "determinant of a is this product\nof determinants, then the determinant of p minus\nlambda i is the determinant of",
    "start": "315690",
    "end": "324130"
  },
  {
    "text": "pt minus lambda it, where it is\njust a crazy way of saying",
    "start": "324130",
    "end": "332160"
  },
  {
    "text": "a diagonal matrix. A diagonal t by t matrix,\nbecause this is a t by t",
    "start": "332160",
    "end": "340070"
  },
  {
    "text": "matrix, also. i sub r is an r by r matrix,\nwhere this is a square r by r",
    "start": "340070",
    "end": "348580"
  },
  {
    "text": "matrix also. Now, why is it that this\ndeterminant is equal to this",
    "start": "348580",
    "end": "353970"
  },
  {
    "text": "product of determinants here? Well, before explaining why this\nis true, why do you care?",
    "start": "353970",
    "end": "362010"
  },
  {
    "text": "Well, because we know that if\nwe have a recurring matrix",
    "start": "362010",
    "end": "368180"
  },
  {
    "text": "here, we know that it has-- I mean, we know a great\ndeal about it.",
    "start": "368180",
    "end": "373790"
  },
  {
    "text": "We know that any square matrix,\nr by r matrix has r",
    "start": "373790",
    "end": "381150"
  },
  {
    "text": "different eigenvalues. Some of them might be repeated,\nbut they're always r",
    "start": "381150",
    "end": "386330"
  },
  {
    "text": "eigenvalues. This matrix here has\nt eigenvalues.",
    "start": "386330",
    "end": "391420"
  },
  {
    "text": "OK. This matrix here, we know has\nr plus t eigenvalues.",
    "start": "391420",
    "end": "397729"
  },
  {
    "text": "You look at this formula here\nand you say aha, I can take all the eigenvalues here, add\nthem to all the eigenvalues",
    "start": "397730",
    "end": "406669"
  },
  {
    "text": "here, and I have every one\nof the eigenvalues here. In other words, if I want to\nfind all of the eigenvalues of",
    "start": "406670",
    "end": "414780"
  },
  {
    "text": "p, all I have to do is define\nthe eigenvalues of p sub t, add them to the eigenvalues of\np sub r, and I'm all done.",
    "start": "414780",
    "end": "424710"
  },
  {
    "text": "So that really has simplified\nthings a good deal. And it also really says\nexplicitly that if you",
    "start": "424710",
    "end": "434270"
  },
  {
    "text": "understand how to deal with\nrecurrent Markov chains, you",
    "start": "434270",
    "end": "440060"
  },
  {
    "text": "really know everything. Well, you also have to know how\nto deal with a transient",
    "start": "440060",
    "end": "445840"
  },
  {
    "text": "chain, but the main part of it\nis dealing with this chain. This has little r different\neigenvalues, and all of those",
    "start": "445840",
    "end": "454870"
  },
  {
    "text": "are eigenvalues, excuse me,\np sub r has little r",
    "start": "454870",
    "end": "461860"
  },
  {
    "text": "eigenvalues. They're given by the roots\nof this determinant here. And all of those\nare roots here.",
    "start": "461860",
    "end": "469530"
  },
  {
    "text": "OK, so why is this true? Well, the reason for it is that\nthis product up here,",
    "start": "469530",
    "end": "477990"
  },
  {
    "text": "look at this. We're taking the sum over\nall permutations. But which one of those\npermutations can be non-zero?",
    "start": "477990",
    "end": "485315"
  },
  {
    "start": "485315",
    "end": "492940"
  },
  {
    "text": "If I start out by saying that a\nsub t is t by t, then I know",
    "start": "492940",
    "end": "498740"
  },
  {
    "text": "that this might be anything. These have to be zeroes here.",
    "start": "498740",
    "end": "504050"
  },
  {
    "text": "If I choose some permutation of\ndown here, of sum i, which",
    "start": "504050",
    "end": "510449"
  },
  {
    "text": "is greater than t. In other words, if I choose\nmu o i to be some element over here.",
    "start": "510450",
    "end": "516130"
  },
  {
    "text": "If I choose mu of i to be less\nthan our equal to t, and i to",
    "start": "516130",
    "end": "522309"
  },
  {
    "text": "be greater than t,\nwhat happens? I get a term which\nis equal to zero.",
    "start": "522309",
    "end": "527790"
  },
  {
    "text": "That term in this\nproduct is zero. And none of those products\ncan be zero.",
    "start": "527790",
    "end": "535670"
  },
  {
    "text": "So the only way I can get non\nzeros here is when I'm dealing",
    "start": "535670",
    "end": "540829"
  },
  {
    "text": "with an i which is less\nthan or equal to t. Namely an i here.",
    "start": "540830",
    "end": "546100"
  },
  {
    "text": "I have to choose a mu of\ni, a column which is less than t, also. If I'm dealing with an i which\nis greater than t, namely and",
    "start": "546100",
    "end": "557540"
  },
  {
    "text": "i up here, then, well, it\nlooks like I can choose",
    "start": "557540",
    "end": "563410"
  },
  {
    "text": "anything there. But look. I've already used up all of\nthese columns here by the five",
    "start": "563410",
    "end": "571180"
  },
  {
    "text": "by the non-zero terms here. So I can't do anything\nbut use a smaller i,",
    "start": "571180",
    "end": "577360"
  },
  {
    "text": "smaller than t up here. So when I look at the\npermutations that are non",
    "start": "577360",
    "end": "584703"
  },
  {
    "text": "zero, the only permutations that\nare non zero are those where mu of i is less than t if\ni less than t, and mu of i",
    "start": "584703",
    "end": "595610"
  },
  {
    "text": "is less than or equal to t if i\nis less than or equal to t.",
    "start": "595610",
    "end": "601959"
  },
  {
    "text": "And mu of i is greater than\nt if i is greater than t. Now, how does that show that\nthis is equal here?",
    "start": "601960",
    "end": "611579"
  },
  {
    "text": "Well, let's look at\nthat a little bit. I didn't even try to do it on\nthe slide because the notation",
    "start": "611580",
    "end": "619740"
  },
  {
    "text": "is kind of horrifying. But let's try to write this\nthe following way.",
    "start": "619740",
    "end": "624850"
  },
  {
    "text": "Determinant of a is equal to the\nsum, and now I'll write it",
    "start": "624850",
    "end": "636910"
  },
  {
    "text": "as a sum over mu of 1 up to t.",
    "start": "636910",
    "end": "648040"
  },
  {
    "text": "And the sum over mu of t\nplus 1 up to, well, t",
    "start": "648040",
    "end": "659690"
  },
  {
    "text": "plus r, let's say. OK, so here I have all of\nthe permutations of the",
    "start": "659690",
    "end": "666210"
  },
  {
    "text": "numbers 1 to t. And here I have all the\npermutations of the",
    "start": "666210",
    "end": "671350"
  },
  {
    "text": "numbers t plus 1 up. And for all of those,\nI'm going to",
    "start": "671350",
    "end": "676760"
  },
  {
    "text": "ignore this plus minus. You can sort that out\nfor yourselves. And then I have a product\nfrom i equals 1 to t.",
    "start": "676760",
    "end": "687620"
  },
  {
    "text": "And then a product from i\nequals t plus 1 up to m.",
    "start": "687620",
    "end": "697950"
  },
  {
    "text": "Excuse me.  i sub i, mu of i times\nproduct of a of i.",
    "start": "697950",
    "end": "713000"
  },
  {
    "text": "Mu of i for i equals t plus\n1 up to t plus r.",
    "start": "713000",
    "end": "727130"
  },
  {
    "text": "OK? So I'm separating this product\nhere into a product first of",
    "start": "727130",
    "end": "734740"
  },
  {
    "text": "the terms i less than or equal\nto t, and then for the terms i greater than t.",
    "start": "734740",
    "end": "740180"
  },
  {
    "text": "For every permutation I choose\nusing the i's that are less than or equal to t, I can choose\nany of the permutation",
    "start": "740180",
    "end": "749089"
  },
  {
    "text": "using mu of i greater than\nt that I choose to use. So this breaks up in this way.",
    "start": "749090",
    "end": "755570"
  },
  {
    "text": "I have this sum, I\nhave this sum. I have these two products, so\nI can break this up as a sum",
    "start": "755570",
    "end": "763120"
  },
  {
    "text": "over mu of 1 to t of plus minus\nproduct from i equals 1",
    "start": "763120",
    "end": "775270"
  },
  {
    "text": "to t of ai, mu of i times the\nsum over mu of t plus 1 up to",
    "start": "775270",
    "end": "788752"
  },
  {
    "text": "t plus r ai mu of i.",
    "start": "788752",
    "end": "795072"
  },
  {
    "start": "795072",
    "end": "800160"
  },
  {
    "text": "Product. OK. So I've separated that into\ntwo different terms.",
    "start": "800160",
    "end": "806029"
  },
  {
    "text": "STUDENT: T equals [INAUDIBLE]. PROFESSOR: What? STUDENT: T plus r\nequals big m? PROFESSOR: T plus\nr is big m, yes.",
    "start": "806030",
    "end": "813230"
  },
  {
    "text": "Because I have t terms here,\nand I have r terms here.",
    "start": "813230",
    "end": "820060"
  },
  {
    "text": "OK, so the interesting thing\nhere is having this non-zero term here doesn't make\nany difference here.",
    "start": "820060",
    "end": "828400"
  },
  {
    "text": "I mean, this is more\nstraightforward if you have a block diagonal matrix.",
    "start": "828400",
    "end": "834020"
  },
  {
    "text": "It's clear that the eigenvalues\nof a block diagonal matrix are going to be\nthe eigenvalues of 1 plus",
    "start": "834020",
    "end": "843700"
  },
  {
    "text": "the eigenvalues of the other. Here we have the eigenvalues\nof this, and the",
    "start": "843700",
    "end": "849980"
  },
  {
    "text": "eigenvalues is this. And what's surprising is that as\nfar as the eigenvalues are concerned, this has nothing\nwhatsoever to do with it.",
    "start": "849980",
    "end": "859949"
  },
  {
    "text": "OK. The only thing that this has\nto do with it is it says something about the sums of this\nmatrix here, because the",
    "start": "859950",
    "end": "868779"
  },
  {
    "text": "sums of these rows are\nnow less than 1. They all have to be, some of\nthem, at least, have to be",
    "start": "868780",
    "end": "874660"
  },
  {
    "text": "less than or equal to 1. Because you do have this way of\ngetting from the transient",
    "start": "874660",
    "end": "880090"
  },
  {
    "text": "elements to the non transient\nelements. But it's very surprising that\nthese elements, which are",
    "start": "880090",
    "end": "888060"
  },
  {
    "text": "critically important, because\nthose are the things that get you from the transition states\nto the recurrent states have",
    "start": "888060",
    "end": "895800"
  },
  {
    "text": "nothing to do in the eigenvalues\nwhatsoever. I don't know why. I can't give you any insights\nabout that, but",
    "start": "895800",
    "end": "904310"
  },
  {
    "text": "that's the way it is. That's an interesting thing,\nbecause if you take this",
    "start": "904310",
    "end": "912029"
  },
  {
    "text": "transition matrix, and you keep\nat and a sub r fixed, and",
    "start": "912030",
    "end": "919930"
  },
  {
    "text": "you play any kind of funny game\nyou want to with those terms going from the transient\nstates to the non transient",
    "start": "919930",
    "end": "928780"
  },
  {
    "text": "states, it won't change\nany eigenvalues. Don't know why it doesn't.",
    "start": "928780",
    "end": "935490"
  },
  {
    "text": "OK, so where do we\ngo with that? Well, that's what it says.",
    "start": "935490",
    "end": "945440"
  },
  {
    "text": "The eigenvalues of p, or the t\neigenvalues of pt, and the r",
    "start": "945440",
    "end": "950580"
  },
  {
    "text": "eigenvalues of PR. It also tells you something\nabout simple eigenvalues, and",
    "start": "950580",
    "end": "956180"
  },
  {
    "text": "these crazy eigenvalues, which\ndon't have enough eigenvectors to go along with them.",
    "start": "956180",
    "end": "961230"
  },
  {
    "text": "Because it tells you that a\npiece of r has all of its",
    "start": "961230",
    "end": "966420"
  },
  {
    "text": "eigenvectors, and a piece of t\nhas all of its eigenvectors.",
    "start": "966420",
    "end": "971880"
  },
  {
    "text": "Then you don't have any of this\ncrazy [INAUDIBLE] form thing, or anything. OK If pi is a left eigenvector\nof this recurrent matrix, then",
    "start": "971880",
    "end": "989670"
  },
  {
    "text": "if you look at the vector,\nstarting was zeros, and then I",
    "start": "989670",
    "end": "995550"
  },
  {
    "text": "guess I should really say, well,\nif pi sub 1 up to pi sub",
    "start": "995550",
    "end": "1002390"
  },
  {
    "text": "r as a left eigenvalue of this r\nby r matrix, then if I start",
    "start": "1002390",
    "end": "1007910"
  },
  {
    "text": "out with t zeroes, and then\nput in pi 1 to pi r, this vector here has to be a left\neigenvector of all of p.",
    "start": "1007910",
    "end": "1017310"
  },
  {
    "text": "Why is that? Well, if I look at a vector,\nwhich starts out with zeroes, and then has this eigenvector\npi, and I multiply that vector",
    "start": "1017310",
    "end": "1026900"
  },
  {
    "text": "by this matrix here, I'm\ntaking these terms, multiplying them by the columns\nof this matrix, these",
    "start": "1026900",
    "end": "1036260"
  },
  {
    "text": "zeros knock out all of\nthese elements here.",
    "start": "1036260",
    "end": "1042310"
  },
  {
    "text": "These zeroes knock out all\nof these elements. So I start out with zeroes\neverywhere here.",
    "start": "1042310",
    "end": "1048409"
  },
  {
    "text": "That's what this says. And then when I'm dealing with\nthis part of the matrix, the",
    "start": "1048410",
    "end": "1054660"
  },
  {
    "text": "zeros knock out all of this, and\nI just have pi multiplying",
    "start": "1054660",
    "end": "1059750"
  },
  {
    "text": "piece of r. So if I have an eigenvalue\nlambda, it says I have the",
    "start": "1059750",
    "end": "1065220"
  },
  {
    "text": "eigenvalue lambda times a\nvector zero times pi. It says that if I have an\neigenvector, a left",
    "start": "1065220",
    "end": "1074760"
  },
  {
    "text": "eigenvector of this recurrent\nmatrix, then that turns into,",
    "start": "1074760",
    "end": "1081010"
  },
  {
    "text": "if you put some zeroes up in\nfront of, it turns into an eigenvector of the\nwhole matrix.",
    "start": "1081010",
    "end": "1087790"
  },
  {
    "text": "If we look at the eigenvalue 1,\nwhich is the most important thing this, is the thing that\ngives you the steady state",
    "start": "1087790",
    "end": "1094350"
  },
  {
    "text": "factor, this is sort\nof obvious. Because the steady state\nvector is where you go",
    "start": "1094350",
    "end": "1099630"
  },
  {
    "text": "eventually, and eventually where\nyou go is you have to be in one of these recurrent\nstates, eventually.",
    "start": "1099630",
    "end": "1107289"
  },
  {
    "text": "And the probabilities within\nthe recurrent set of states are the same as the\nprobabilities if you didn't",
    "start": "1107290",
    "end": "1113400"
  },
  {
    "text": "have this transient\nstates at all. so this is all sort of obvious,\nas far as the steady",
    "start": "1113400",
    "end": "1120490"
  },
  {
    "text": "state factor pi. But it's a little less obvious\nas far as the other vectors.",
    "start": "1120490",
    "end": "1127480"
  },
  {
    "text": "The left eigenvectors,\na piece of t, I don't understand them at all.",
    "start": "1127480",
    "end": "1133610"
  },
  {
    "text": "They aren't the same as the left\neigenvectors of, well,",
    "start": "1133610",
    "end": "1139660"
  },
  {
    "text": "the left eigenvectors of the\neigenvalues of p sub t.",
    "start": "1139660",
    "end": "1144670"
  },
  {
    "text": " I didn't say this right here.",
    "start": "1144670",
    "end": "1150270"
  },
  {
    "text": "The left eigenvectors of p\ncorresponding to the left",
    "start": "1150270",
    "end": "1155870"
  },
  {
    "text": "eigenvectors of p sub t. I don't understand how they\nwork, and I don't understand",
    "start": "1155870",
    "end": "1162010"
  },
  {
    "text": "anything you can derive\nfrom them. They're just kind of crazy\nthings, which are what they happen to be.",
    "start": "1162010",
    "end": "1167780"
  },
  {
    "text": "And I don't care about them. I don't know anything\nto do with them. But these other eigenvectors\nare very useful.",
    "start": "1167780",
    "end": "1175200"
  },
  {
    "text": "OK. We can extend this to as many\ndifferent recurrent sets of",
    "start": "1175200",
    "end": "1185040"
  },
  {
    "text": "states as you choose. Here I'm doing it with a Markov\nchain, which has two",
    "start": "1185040",
    "end": "1193100"
  },
  {
    "text": "different sets of recurrent\nstates. They might be periodic, they\nmight be ergodic, it doesn't",
    "start": "1193100",
    "end": "1200010"
  },
  {
    "text": "make any difference. So the matrix p has these\ntransient states up here.",
    "start": "1200010",
    "end": "1207730"
  },
  {
    "text": "Here we have those transition\nstates would just go to each other, where the transition\nprobabilities starting with",
    "start": "1207730",
    "end": "1216320"
  },
  {
    "text": "the transient state and going\nto a transition state. Here we have the transitions,\nwhich go from transient states",
    "start": "1216320",
    "end": "1224090"
  },
  {
    "text": "to this first set of\nrecurrent states. Here we have the transitions,\nwhich go from a transient",
    "start": "1224090",
    "end": "1230810"
  },
  {
    "text": "state to the second state\nof recurrent states. OK.",
    "start": "1230810",
    "end": "1236179"
  },
  {
    "text": "The same way as before, the\ndeterminant of this whole thing here, and this\ndeterminant, the roots of that",
    "start": "1236180",
    "end": "1244789"
  },
  {
    "text": "are in fact the eigenvalues of\np, are the product of the determinant of pt minus lambda\nit times the product of this,",
    "start": "1244790",
    "end": "1254930"
  },
  {
    "text": "times this determinant here. This has little t eigenvalues.",
    "start": "1254930",
    "end": "1262180"
  },
  {
    "text": "This has little r eigenvalues. This has little r prime\neigenvalues, and if you add up",
    "start": "1262180",
    "end": "1268690"
  },
  {
    "text": "t plus little r plus little\nr prime, what do you get? You get jM, excuse me, capital\nM, which is the total number",
    "start": "1268690",
    "end": "1277790"
  },
  {
    "text": "of states in the Markov chain. So the eigenvalues here are\nexactly the eigenvalues here",
    "start": "1277790",
    "end": "1287110"
  },
  {
    "text": "plus the eigenvalues here, plus\nthe eigenvalues here.",
    "start": "1287110",
    "end": "1293299"
  },
  {
    "text": "And you can find the\neigenvectors, the left eigenvectors for these\nstates in exactly",
    "start": "1293300",
    "end": "1300809"
  },
  {
    "text": "the same way as before. OK. Yeah? STUDENT: So again, the\neigenvalues can be repeated",
    "start": "1300810",
    "end": "1308627"
  },
  {
    "text": "both within t, r, r prime,\nand in between the-- PROFESSOR: Yes. STUDENT: There's nothing\nthat says [INAUDIBLE].",
    "start": "1308628",
    "end": "1314340"
  },
  {
    "text": "PROFESSOR: No. There's nothing that says they\ncan't, except you can always find the left eigenvectors,\nanyway, of this are, in fact,",
    "start": "1314340",
    "end": "1325980"
  },
  {
    "text": "these things in the form. If pi is a left eigenvector of p\nsub r, then zero followed by",
    "start": "1325980",
    "end": "1335840"
  },
  {
    "text": "pi followed by zero. In other words, little t zeros\nfollowed by r, followed by the",
    "start": "1335840",
    "end": "1346480"
  },
  {
    "text": "eigenvector pi, followed by\nlittle r prime zeroes here,",
    "start": "1346480",
    "end": "1352059"
  },
  {
    "text": "this has to be a left\neigenvector of t. So this tells you something\nabout whether you're going to",
    "start": "1352060",
    "end": "1357280"
  },
  {
    "text": "have a Jordan form or not,\none of these really ugly things in it. And it tells you that\nin many cases, you",
    "start": "1357280",
    "end": "1364590"
  },
  {
    "text": "just can't have them. If you have them, they're\nusually tied up with this matrix here.",
    "start": "1364590",
    "end": "1370730"
  },
  {
    "text": "OK, so that, I don't know. Was this useful? Does this clarify anything? Or if it doesn't,\nit's too bad.",
    "start": "1370730",
    "end": "1378830"
  },
  {
    "text": " OK. So now we want to start\ntalking about rewards.",
    "start": "1378830",
    "end": "1385080"
  },
  {
    "text": " Some people call these costs. If you're an optimist,\nyou call it rewards.",
    "start": "1385080",
    "end": "1391230"
  },
  {
    "text": "If you're a pessimist,\nyou call it costs. They're both the same thing. If you're dealing with rewards,\nyou maximize them.",
    "start": "1391230",
    "end": "1398180"
  },
  {
    "text": "If you're dealing with costs,\nyou minimize them. So mathematically, who cares?",
    "start": "1398180",
    "end": "1404799"
  },
  {
    "text": "OK, so suppose that each state\ni of a Markov chain is",
    "start": "1404800",
    "end": "1410590"
  },
  {
    "text": "associated with a given\nreward, or a sub i. In other words, you think of\nthis Markov chain, which is",
    "start": "1410590",
    "end": "1416350"
  },
  {
    "text": "running along. You go from one state to\nanother over time. And while this is happening,\nyou're pocketing some reward",
    "start": "1416350",
    "end": "1425929"
  },
  {
    "text": "all the time. OK. You invest in a stock. Strangely enough, these\nparticular stocks we're",
    "start": "1425930",
    "end": "1433470"
  },
  {
    "text": "thinking about here I this\nMarkov property. Stocks really don't have a\nMarkov property, but we'll",
    "start": "1433470",
    "end": "1439970"
  },
  {
    "text": "assume they do. And since they have this Markov\nproperty, you win for a",
    "start": "1439970",
    "end": "1446200"
  },
  {
    "text": "while, and you lose\nfor a while. You win for a while, you\nlose for a while. But we have something\nextra, other than",
    "start": "1446200",
    "end": "1452770"
  },
  {
    "text": "just the Markov chains. We can analyze this whole\nsituation, knowing how Markov",
    "start": "1452770",
    "end": "1458830"
  },
  {
    "text": "chains behave. There's not much left besides\nthat, but there are an",
    "start": "1458830",
    "end": "1464980"
  },
  {
    "text": "extraordinary number of\napplications of this idea, and dynamic programming\nis one of them.",
    "start": "1464980",
    "end": "1471900"
  },
  {
    "text": "Because that's just one\nadded extension beyond this idea of rewards.",
    "start": "1471900",
    "end": "1477880"
  },
  {
    "text": "OK. The random variable x of n. That's a random quantity.",
    "start": "1477880",
    "end": "1483240"
  },
  {
    "text": "It's the state at time n. And the random reward of time n\nis then the random variable",
    "start": "1483240",
    "end": "1490010"
  },
  {
    "text": "r of xn that maps xn equals\ni into ri for each i.",
    "start": "1490010",
    "end": "1495680"
  },
  {
    "text": "This is the same idea of taking\none random variable, which is a function of another\nrandom variable.",
    "start": "1495680",
    "end": "1502030"
  },
  {
    "text": "The one random variable takes\non the values one up to capital M.",
    "start": "1502030",
    "end": "1507740"
  },
  {
    "text": "And then the other random\nvariable takes on a value which is determined by the state\nthat you happen to be",
    "start": "1507740",
    "end": "1514679"
  },
  {
    "text": "in, which is this\nrandom states. So specifying our sub i\nspecifies what the set of",
    "start": "1514680",
    "end": "1521700"
  },
  {
    "text": "rewards are, what the reward\nis in each given state. Again, we have this awful\nproblem, which I wish we could",
    "start": "1521700",
    "end": "1528520"
  },
  {
    "text": "avoid in Markov chains, of using\nthe same word state to talk about the set of\ndifferent states.",
    "start": "1528520",
    "end": "1535900"
  },
  {
    "text": "And also to talk about\nthe random state at any given time. But hopefully by now you're\nused to that.",
    "start": "1535900",
    "end": "1543560"
  },
  {
    "text": "In our discussion here, the only\nthing we're going to talk about are expected rewards.",
    "start": "1543560",
    "end": "1550669"
  },
  {
    "text": "Now, you know that expected\nrewards, or expectations are a",
    "start": "1550670",
    "end": "1555810"
  },
  {
    "text": "little more generally than you\nwould think they would be, because you're going to take the\nexpected value of any sort",
    "start": "1555810",
    "end": "1562060"
  },
  {
    "text": "of crazy thing. If you want to talk about any\nevent, you can take the",
    "start": "1562060",
    "end": "1567870"
  },
  {
    "text": "indicator function of that\nevent, and find the expected value of that indicator\nfunction.",
    "start": "1567870",
    "end": "1573890"
  },
  {
    "text": "And that's just the probability\nof that event. So by understanding how to deal\nwith expectations, you",
    "start": "1573890",
    "end": "1582660"
  },
  {
    "text": "really have the capability\nof finding distribution functions, or anything else\nyou want to find.",
    "start": "1582660",
    "end": "1588480"
  },
  {
    "text": "OK. But anyway, since we're\ninterested only in expected rewards, the expected reward at\ntime n, given that x zero",
    "start": "1588480",
    "end": "1597554"
  },
  {
    "text": "is i is the expected value of r\nof xn given x zero equals i,",
    "start": "1597555",
    "end": "1604950"
  },
  {
    "text": "which is the sum over j of the\nreward you get if you're in state j at time n times p sub\nij, super n, which we've",
    "start": "1604950",
    "end": "1615700"
  },
  {
    "text": "talked about ad nauseum for the\nlast four lectures now.",
    "start": "1615700",
    "end": "1620850"
  },
  {
    "text": "And this is the probability that\nthe state at time n is j,",
    "start": "1620850",
    "end": "1626900"
  },
  {
    "text": "given that the state\nat time zero is i. So you can just automatically\nfind the expected",
    "start": "1626900",
    "end": "1633650"
  },
  {
    "text": "value of r of xn. And it's by that formula.",
    "start": "1633650",
    "end": "1640610"
  },
  {
    "text": "Now, recall that this quantity\nhere is not all that simple. This is the ij element of the\nproduct of the matrix, of the",
    "start": "1640610",
    "end": "1648679"
  },
  {
    "text": "nth product of the matrix p. But, so what? We can at least write a nice\nformula for it now.",
    "start": "1648680",
    "end": "1656130"
  },
  {
    "text": "The expected aggregate reward\nover the n steps from m to m plus n minus 1.",
    "start": "1656130",
    "end": "1663080"
  },
  {
    "text": "What is m doing in here? It's just reminding us that\nMarkov chains are",
    "start": "1663080",
    "end": "1668970"
  },
  {
    "text": "homogeneous over time. So, when I talk about the\naggregate reward from time m",
    "start": "1668970",
    "end": "1676370"
  },
  {
    "text": "the m plus n minus 1, it's the\nsame as the aggregate reward from time 0 up to\ntime n minus 1.",
    "start": "1676370",
    "end": "1684500"
  },
  {
    "text": "The expected values\nare the same. The actual sample functions\nare different.",
    "start": "1684500",
    "end": "1689550"
  },
  {
    "text": "OK, so if I try to calculate\nthis aggregate reward conditional on xm equals i,\nmainly conditional on starting",
    "start": "1689550",
    "end": "1698880"
  },
  {
    "text": "in state i, then this expected\naggregate reward, I use that as a symbol for it, is the\nexpected value of r of xm,",
    "start": "1698880",
    "end": "1708610"
  },
  {
    "text": "given xm equals i. What is that? Well, that's ri. I mean, given that xm\nis equal to i, this",
    "start": "1708610",
    "end": "1715220"
  },
  {
    "text": "isn't random anymore. It's just the source sub i. Plus the expected value of r of\nxm plus 1, which is the sum",
    "start": "1715220",
    "end": "1725350"
  },
  {
    "text": "over j, of pij times r sub j. That's the time m plus 1 given\nthat you're in state i at time",
    "start": "1725350",
    "end": "1734304"
  },
  {
    "text": "m, and so forth, up until time\nn minus 1, where the expected",
    "start": "1734305",
    "end": "1740370"
  },
  {
    "text": "reward, then, is\na piece of ij. ",
    "start": "1740370",
    "end": "1746179"
  },
  {
    "text": "Probability of being in state j\nat time n minus 1 given that you started off in state i\nat time 0 times r sub j.",
    "start": "1746180",
    "end": "1756190"
  },
  {
    "text": "And since expectations add, we\nhave this nice, convenient formula here.",
    "start": "1756190",
    "end": "1762040"
  },
  {
    "text": " We're doing something I normally\nhate doing, which is",
    "start": "1762040",
    "end": "1770580"
  },
  {
    "text": "building up a lot of notation,\nand then using that notation to write extremely complicated\nformulas in a way that looks",
    "start": "1770580",
    "end": "1780470"
  },
  {
    "text": "very simple. And therefore you will get some\nsense of what we're doing is very simple.",
    "start": "1780470",
    "end": "1785840"
  },
  {
    "text": "These quantities in\nhere, again, are not all that simple. But at least we can write\nit in a simple way.",
    "start": "1785840",
    "end": "1792550"
  },
  {
    "text": "And since we can write it in a\nsimple way, it turns out we can do some nice\nthings with it.",
    "start": "1792550",
    "end": "1799160"
  },
  {
    "text": "OK. So where do we go from\nall of this? ",
    "start": "1799160",
    "end": "1804860"
  },
  {
    "text": "We have just said that the\nexpected reward we get,",
    "start": "1804860",
    "end": "1812280"
  },
  {
    "text": "expected aggregate reward over n\nsteps, namely from m up to m",
    "start": "1812280",
    "end": "1818550"
  },
  {
    "text": "plus n minus 1. We're assuming that if we start\nat time m, we pick up a",
    "start": "1818550",
    "end": "1825659"
  },
  {
    "text": "reward at time n. I mean, that's just an\narbitrary decision. We might as well do that,\nbecause otherwise we just have",
    "start": "1825660",
    "end": "1833960"
  },
  {
    "text": "one more transition matrix\nsitting here. OK, so we start at time m. We pick up a reward, which\nis conditional on the",
    "start": "1833960",
    "end": "1842640"
  },
  {
    "text": "state we start in. And then we look at the expected\nreward for time m and",
    "start": "1842640",
    "end": "1853040"
  },
  {
    "text": "time m plus 1, m plus 2,\nup to m plus n minus 1.",
    "start": "1853040",
    "end": "1858420"
  },
  {
    "text": "Since we started at\nm, we're picking up n different rewards. We have to stop at time\nm plus n minus 1.",
    "start": "1858420",
    "end": "1867490"
  },
  {
    "text": "OK, so that's this expected\naggregate reward.",
    "start": "1867490",
    "end": "1874040"
  },
  {
    "text": "Why do I care about expected\naggregate reward? Because the rewards at any time\nn are sort of trivial.",
    "start": "1874040",
    "end": "1882220"
  },
  {
    "text": "What we're are interested\nin is how does this build up over time?",
    "start": "1882220",
    "end": "1887320"
  },
  {
    "text": "You start to invest\nin a stock. You don't much care what\nit's worth at time 10.",
    "start": "1887320",
    "end": "1894480"
  },
  {
    "text": "You care how it grows.  You care about its value when\nyou want to sell it, and you",
    "start": "1894480",
    "end": "1901040"
  },
  {
    "text": "don't know when you're going to\nsell it, most of the time. So you're really interested\nin these aggregate",
    "start": "1901040",
    "end": "1908150"
  },
  {
    "text": "rewards that you.  You'll see when we get to\ndynamic programming what",
    "start": "1908150",
    "end": "1914590"
  },
  {
    "text": "you're interested\nin that, also. OK. If the Markov chain is an\nergotic unit chain, then",
    "start": "1914590",
    "end": "1921340"
  },
  {
    "text": "successive terms of this\nexpression tend to a steady state gain per step.",
    "start": "1921340",
    "end": "1926450"
  },
  {
    "text": "In other words, these terms here\n, when n gets very large,",
    "start": "1926450",
    "end": "1931519"
  },
  {
    "text": "if I run this process for very\nlong time, what happens to p",
    "start": "1931520",
    "end": "1937070"
  },
  {
    "text": "sub ij to n minus 1? This tends towards the steady\nstate vector pi sub j.",
    "start": "1937070",
    "end": "1947920"
  },
  {
    "text": "And it doesn't matter\nwhere we started. The only thing of importance\nis where we end up.",
    "start": "1947920",
    "end": "1954690"
  },
  {
    "text": "It doesn't matter how\nhigh this is. So we have a sum over j, of\npi sub j times r sub j.",
    "start": "1954690",
    "end": "1962670"
  },
  {
    "text": "After a very long time, the\nexpected gain per step is just",
    "start": "1962670",
    "end": "1968745"
  },
  {
    "text": "a sum of pi j times\nour r sub j. That's what's important\nafter a long time.",
    "start": "1968745",
    "end": "1976000"
  },
  {
    "text": "And that's independent of\nthe starting state. So what we have here is a big,\nmessy transient, which is a",
    "start": "1976000",
    "end": "1982670"
  },
  {
    "text": "sum of a whole bunch\nof things. And then eventually it just\nsettles down, and every extra",
    "start": "1982670",
    "end": "1988090"
  },
  {
    "text": "step you do, you just pick up\nan extra factor of g as an",
    "start": "1988090",
    "end": "1995190"
  },
  {
    "text": "extra reward. The reward might, of course, be\nnegative, like in the stock market over the last 10 years,\nor up until the last year or",
    "start": "1995190",
    "end": "2005100"
  },
  {
    "text": "so, who was negative\nfor a long time. But that doesn't make\nany difference.",
    "start": "2005100",
    "end": "2010799"
  },
  {
    "text": "This is just a number, and\nthis is independent of starting state.",
    "start": "2010800",
    "end": "2016590"
  },
  {
    "text": "And p sub in can be viewed a\ntransient ni, which is all",
    "start": "2016590",
    "end": "2021740"
  },
  {
    "text": "this stuff at the beginning. The sum of all these terms at\nthe beginning plus something",
    "start": "2021740",
    "end": "2027010"
  },
  {
    "text": "that settles down over a\nlong period of time. How to calculate that transient,\nhow to combine it",
    "start": "2027010",
    "end": "2034200"
  },
  {
    "text": "with the steady state gain. Then those talk a great\ndeal about that.",
    "start": "2034200",
    "end": "2039919"
  },
  {
    "text": "What we're trying to do today\nis to talk about dynamic programming without going into\nall of this terrible mess",
    "start": "2039920",
    "end": "2049080"
  },
  {
    "text": "about dealing rewards\nwords in a very systematic and simple way.",
    "start": "2049080",
    "end": "2054239"
  },
  {
    "text": "You can read about that later. What we're aiming at is to talk\nabout dynamic programming",
    "start": "2054239",
    "end": "2059609"
  },
  {
    "text": "a little bit, and then get\noff to other things. OK. So anyway, we have a transient,\nplus we have a",
    "start": "2059610",
    "end": "2067239"
  },
  {
    "text": "steady state gain. The transient is important. And it's particularly important\nif g equals zero.",
    "start": "2067239",
    "end": "2074520"
  },
  {
    "text": "Namely if your average gain per\nstep is nothing, then what",
    "start": "2074520",
    "end": "2080090"
  },
  {
    "text": "you're primarily interested in\nis how valuable is it to start",
    "start": "2080090",
    "end": "2087980"
  },
  {
    "text": "in a particular state? If you start in one state versus\nanother state, you",
    "start": "2087980",
    "end": "2093000"
  },
  {
    "text": "might get a great deal of reward\nin this one state, whereas you make a loss\nin some other state.",
    "start": "2093000",
    "end": "2099120"
  },
  {
    "text": "So it's important to know which\nstate is worth being in. So that's the next thing\nwe try to look at.",
    "start": "2099120",
    "end": "2107960"
  },
  {
    "text": "How does the state\naffect things? This brings us to one example\nwhich is particularly useful.",
    "start": "2107960",
    "end": "2117760"
  },
  {
    "text": "And along with being a useful\nexample, well, it's a nice illustration of Markov\nrewards.",
    "start": "2117760",
    "end": "2125840"
  },
  {
    "text": "It's also something which\nyou often want to find.",
    "start": "2125840",
    "end": "2130980"
  },
  {
    "text": "And when we start talking about\nrenewal processes, you will find that this idea here\nis a nice connection between",
    "start": "2130980",
    "end": "2140890"
  },
  {
    "text": "Markov chains and\nrenewal series. So it's important for a whole\nbunch of different reasons.",
    "start": "2140890",
    "end": "2147240"
  },
  {
    "text": "OK. Suppose for some arbitrary\nunit chain, namely we're",
    "start": "2147240",
    "end": "2152470"
  },
  {
    "text": "saying one set of recurring\nstates. We want to find the expected\nnumber of steps, starting from",
    "start": "2152470",
    "end": "2159710"
  },
  {
    "text": "a given state i, until\nsome particular state 1 is first entered.",
    "start": "2159710",
    "end": "2166560"
  },
  {
    "text": "So you start at one state. There's this other state\nway over here.",
    "start": "2166560",
    "end": "2172090"
  },
  {
    "text": "This state is recurrent, so\npresumably, eventually you're going to enter it.",
    "start": "2172090",
    "end": "2177579"
  },
  {
    "text": "And you want to find out, what's\nthe expected time that it takes to get to that\nparticular state?",
    "start": "2177580",
    "end": "2183809"
  },
  {
    "text": "OK? If you're a Ph.D. student, you\nhave this Markov chain of",
    "start": "2183810",
    "end": "2190160"
  },
  {
    "text": "doing your research. And at some point, you're going\nto get a Ph.D. So we can",
    "start": "2190160",
    "end": "2196180"
  },
  {
    "text": "think of this as the first pass\neach time to your first Ph.D. I mean, if you want to\nget more Ph.D.'s, fine, but",
    "start": "2196180",
    "end": "2204500"
  },
  {
    "text": "that's probably a different\nMarkov chain. OK. So anyway, that's the problem\nwe're trying to solve here.",
    "start": "2204500",
    "end": "2213110"
  },
  {
    "text": "We can view this problem\nas a reward problem. We have to go through a number\nof steps if we want to view it",
    "start": "2213110",
    "end": "2219750"
  },
  {
    "text": "as a reward problem. The first one, first step is to\nassign one unit of reward",
    "start": "2219750",
    "end": "2227390"
  },
  {
    "text": "to each successive state until\nyou enter state 1. So you're bombing through this\nMarkov chain, a frog jumping",
    "start": "2227390",
    "end": "2235040"
  },
  {
    "text": "from lily pad to lily pad. And finally, the frog\ngets to the lily pad with the food on it.",
    "start": "2235040",
    "end": "2241500"
  },
  {
    "text": "And the frog wants to know, is\nit going to start before he gets to this lily pad\nwith the food on it?",
    "start": "2241500",
    "end": "2248830"
  },
  {
    "text": "So, if we're trying to find\nthe expected time to get there, here what we're really\ninterested in is a cost,",
    "start": "2248830",
    "end": "2255849"
  },
  {
    "text": "because the frog is in\ndanger of starving. Or on the other hand, there\nmight be a snake lying under",
    "start": "2255850",
    "end": "2262220"
  },
  {
    "text": "this one lily pad. And then he's getting a reward\nfor staying alive.",
    "start": "2262220",
    "end": "2267770"
  },
  {
    "text": "You can look at these things\nwhichever way you want to. OK. We're going to assign one unit\nof reward to successive state",
    "start": "2267770",
    "end": "2275019"
  },
  {
    "text": "until state 1 is entered. 1 is just an arbitrary state\nthat we've selected.",
    "start": "2275020",
    "end": "2281430"
  },
  {
    "text": "That's where the snake is\nunderneath a lily pad, or that's where the food is,\nor what have you.",
    "start": "2281430",
    "end": "2288130"
  },
  {
    "text": "Now, there's something\nelse we have to do. Because if we're starting out at\nsome arbitrary state i, and",
    "start": "2288130",
    "end": "2297010"
  },
  {
    "text": "we're trying to look for the\nfirst time that we enter state 1, what do you do after\nyou enter state 1?",
    "start": "2297010",
    "end": "2303694"
  },
  {
    "text": " Well eventually, normally you're\ngoing to go away from",
    "start": "2303695",
    "end": "2312400"
  },
  {
    "text": "state 1, and you're\ngoing to start picking up rewards again. You don't want that to happen.",
    "start": "2312400",
    "end": "2318990"
  },
  {
    "text": "So you do something we do all\nthe time when we're dealing with Markov chains, which is\nwe start with one Markov",
    "start": "2318990",
    "end": "2325510"
  },
  {
    "text": "chain, and we say, to solve this\nproblem I'm interested in, I've got to change\nthe Markov chain.",
    "start": "2325510",
    "end": "2332110"
  },
  {
    "text": "So how are we going\nto change it? We're going to change it to say,\nonce we get in state 1,",
    "start": "2332110",
    "end": "2338160"
  },
  {
    "text": "we're going to stay\nthere forever.  Or in other words, the frog gets\neaten by the snake, and",
    "start": "2338160",
    "end": "2344600"
  },
  {
    "text": "therefore its remains always\nstay at that one lily pad.",
    "start": "2344600",
    "end": "2349650"
  },
  {
    "text": "So we change the Markov\nchain again. The frog can't jump anymore. And the way we change it is\nto change the transition",
    "start": "2349650",
    "end": "2358290"
  },
  {
    "text": "probabilities out of state 1\nto p sub 1, 1, namely the",
    "start": "2358290",
    "end": "2363910"
  },
  {
    "text": "probability given you're in\nstate 1, of going back to state 1 in the next transition\nis equal to 1.",
    "start": "2363910",
    "end": "2370320"
  },
  {
    "text": "So whenever you get\nto state 1, you just stay there forever. We're going to say r1 equal to\nzero, namely the reward you",
    "start": "2370320",
    "end": "2379210"
  },
  {
    "text": "get in state 1 will be zero. So you keep getting rewards\nuntil you go to state 1.",
    "start": "2379210",
    "end": "2386070"
  },
  {
    "text": "And then when you go to state\n1, you don't get any reward. You don't get any reward from\nany time after that.",
    "start": "2386070",
    "end": "2394150"
  },
  {
    "text": "So in fact, we've converted\nthe problem. We've converted the Markov chain\nto be able to solve the",
    "start": "2394150",
    "end": "2399970"
  },
  {
    "text": "problem that we want to solve. Now, how do we know that we\nhaven't changed the problem in",
    "start": "2399970",
    "end": "2407660"
  },
  {
    "text": "some awful way? I mean, any time you start out\nwith a Markov chain and you",
    "start": "2407660",
    "end": "2413710"
  },
  {
    "text": "modify it, and you solve a\nproblem for the modified chain, you have to really think\nthrough whether you",
    "start": "2413710",
    "end": "2420410"
  },
  {
    "text": "changed the problem that\nyou started to solve. Well, think of any sample path\nwhich starts in some state i,",
    "start": "2420410",
    "end": "2427790"
  },
  {
    "text": "which is not equal to 1. Think of the sample path\nas going forever.",
    "start": "2427790",
    "end": "2433930"
  },
  {
    "text": "In the original Markov chain,\nthat sample path at some point, presumably, is going\nto get to state 1.",
    "start": "2433930",
    "end": "2443050"
  },
  {
    "text": "After it gets to state 1, we\ndon't care what happens, because we then know how long\nit's taken to get to state 1.",
    "start": "2443050",
    "end": "2451520"
  },
  {
    "text": "And after it gets to state\n1, the transition probabilities change. We don't care about that.",
    "start": "2451520",
    "end": "2458410"
  },
  {
    "text": "So for every sample path, the\ntime that it takes the first",
    "start": "2458410",
    "end": "2463569"
  },
  {
    "text": "pass each time to state 1 is the\nsame in the modify chain as it is in the actual chain.",
    "start": "2463570",
    "end": "2470920"
  },
  {
    "text": "The transition probabilities are\nthe same up until the time when you first get to state 1.",
    "start": "2470920",
    "end": "2477770"
  },
  {
    "text": "So for first pass each time\nproblems, it doesn't make any difference what you do after\nyou get to state 1.",
    "start": "2477770",
    "end": "2486550"
  },
  {
    "text": "So to make the problem easy,\nwe're going to set these transition probabilities in\nstate 1 to 1, and we're going",
    "start": "2486550",
    "end": "2494450"
  },
  {
    "text": "to set the reward\nequal to zero. What do you call a state which\nhas p sub i, i equal to 1?",
    "start": "2494450",
    "end": "2506710"
  },
  {
    "text": "You call it a trapping state. It's a trapping state because\nonce you get there, you can't get out.",
    "start": "2506710",
    "end": "2512330"
  },
  {
    "text": " And since we started out with\na unit chain, and since",
    "start": "2512330",
    "end": "2519710"
  },
  {
    "text": "presumably state 1 is a\nrecurrent state in that unit chain, eventually you're going\nto get to state 1.",
    "start": "2519710",
    "end": "2526500"
  },
  {
    "text": "But once you get there,\nyou can't get out. So what you've done is you've\nturned the unit chain into",
    "start": "2526500",
    "end": "2531690"
  },
  {
    "text": "another unit chain where the\nrecurrent set of states has only this one state,\nstate 1 in it.",
    "start": "2531690",
    "end": "2537900"
  },
  {
    "text": "So it's a trapping state. Everything eventually\nleads to state 1.",
    "start": "2537900",
    "end": "2543920"
  },
  {
    "text": "All roads lead to Rome, but it's\nnot obvious that they're leading to Rome. And all of these states\neventually lead to state 1,",
    "start": "2543920",
    "end": "2551480"
  },
  {
    "text": "but not for quite a\nwhile sometimes. OK. So the probability of an initial\nsegment until 1 is",
    "start": "2551480",
    "end": "2557710"
  },
  {
    "text": "entered is unchanged, and\nexpected first pass each time is unchanged.",
    "start": "2557710",
    "end": "2563210"
  },
  {
    "text": " OK. A modified Markov chain is now\nan ergotic unit chain.",
    "start": "2563210",
    "end": "2570430"
  },
  {
    "text": "It has a single recurrent\nstate. State 1 is a trapping\nstate, we call it.",
    "start": "2570430",
    "end": "2577150"
  },
  {
    "text": "ri is equal to 1 for i unequal\nto 1, and r1 is equal to zero.",
    "start": "2577150",
    "end": "2583730"
  },
  {
    "text": "This says that a state 1 is\nfirst entered at time l, and the aggregate reward from 0 to\nn is l for all m greater than",
    "start": "2583730",
    "end": "2593770"
  },
  {
    "text": "or equal to l. In other words, after you get\nto the trapping state, you stay there, and you don't\npick up any more",
    "start": "2593770",
    "end": "2599410"
  },
  {
    "text": "reward from then on. One of the things that's\nmaddening about problems like this, at least that's maddening\nfor me, because I",
    "start": "2599410",
    "end": "2606720"
  },
  {
    "text": "can't keep those things\nstraight, is the difference between n and n plus 1,\nor n and n minus 1.",
    "start": "2606720",
    "end": "2614290"
  },
  {
    "text": "There's always that strange\nthing, we've started at time m, we get reward at time m.",
    "start": "2614290",
    "end": "2620270"
  },
  {
    "text": "So if we're looking at m\ntransitions, as we go from m the m plus n minus 1.",
    "start": "2620270",
    "end": "2626860"
  },
  {
    "text": "And that's just life. If you try to do it in a\ndifferent way, you wind up",
    "start": "2626860",
    "end": "2632910"
  },
  {
    "text": "with a similar problem. You can't avoid it. OK, so what we're trying to find\nis the expected value of",
    "start": "2632910",
    "end": "2642130"
  },
  {
    "text": "v sub i of n, and the limit as n\ngoes to infinity, we'll just call that v sub i without\nthe n on it.",
    "start": "2642130",
    "end": "2650640"
  },
  {
    "text": "And what we want to do is to\ncalculate this expected time until we first enter\nstate one.",
    "start": "2650640",
    "end": "2658040"
  },
  {
    "text": "We want to calculate that for\nall of the other states i. Well fortunately, there's a\nsneaky way to calculate this.",
    "start": "2658040",
    "end": "2666980"
  },
  {
    "text": "For most of these problems,\nthere's a sneaky way to calculate these limits. And you don't have to worry\nabout the limit.",
    "start": "2666980",
    "end": "2674640"
  },
  {
    "text": "So the next thing I'm going to\ndo is to explain what this sneaky way is.",
    "start": "2674640",
    "end": "2679760"
  },
  {
    "text": "You will see the same sneaky\nmethod done about 100 times from now on until the\nend of course.",
    "start": "2679760",
    "end": "2686460"
  },
  {
    "text": "We use it all the time. And each time we do it, we'll\nget a better sense of what it",
    "start": "2686460",
    "end": "2692250"
  },
  {
    "text": "really amounts to. So for each state unequal to\nthe trapping state, let's",
    "start": "2692250",
    "end": "2699150"
  },
  {
    "text": "start out by assuming that\nwe start at time zero, and state i.",
    "start": "2699150",
    "end": "2704470"
  },
  {
    "text": "In other words, what this means\nis first we're going to assume that x sub 0 equals\ni for some given i.",
    "start": "2704470",
    "end": "2712490"
  },
  {
    "text": "We're going to go through\nwhatever we're going to go through, then we'll go back\nand assume that x sub 0 is",
    "start": "2712490",
    "end": "2717620"
  },
  {
    "text": "some other i. And we don't have to worry about\nthat, because i is just a generic state.",
    "start": "2717620",
    "end": "2722900"
  },
  {
    "text": "So we'll do it for everything\nat once. There's a unit reward\nat time 0.",
    "start": "2722900",
    "end": "2730630"
  },
  {
    "text": "r sub i is equal to 1. So we start out at time\nzero and state i.",
    "start": "2730630",
    "end": "2737270"
  },
  {
    "text": "We pick up our reward of 1, and\nthen we go on from there to see how much longer it\ntakes to get to state 1.",
    "start": "2737270",
    "end": "2746370"
  },
  {
    "text": "In addition to this unit reward\nat time zero, which",
    "start": "2746370",
    "end": "2753170"
  },
  {
    "text": "means it's already taken us one\nunit of time to get the state 1, given that x sub 1\nequals j, namely, given that",
    "start": "2753170",
    "end": "2762119"
  },
  {
    "text": "we go from state i to state j,\nthe remaining expected reward",
    "start": "2762120",
    "end": "2767910"
  },
  {
    "text": "is v sub j. In other words, if it's times\n0, I'm in some state i.",
    "start": "2767910",
    "end": "2775829"
  },
  {
    "text": "Given that I go to some stage\nj, the next unit of time,",
    "start": "2775830",
    "end": "2781110"
  },
  {
    "text": "what's the remaining accepted\nexpected time they get to state 1?",
    "start": "2781110",
    "end": "2787559"
  },
  {
    "text": "The remaining expected time is\njust v sub j, because that's",
    "start": "2787560",
    "end": "2792830"
  },
  {
    "text": "the expected time. I mean, if v sub j is something\nwhere it's very hard to get to state 1, then\nwe really lost out.",
    "start": "2792830",
    "end": "2801560"
  },
  {
    "text": "If it's something which is\ncloser to state 1 in some sense, then we've gained. But what we wind up with is the\nexpected time to get to",
    "start": "2801560",
    "end": "2811180"
  },
  {
    "text": "state 1 from state i is one. That's the instant reward that\nwe get, or the instant cost",
    "start": "2811180",
    "end": "2819450"
  },
  {
    "text": "that we pay, plus each of\nthe possible states",
    "start": "2819450",
    "end": "2824880"
  },
  {
    "text": "we might get to. There's a cost to go, or\nreward to go from that",
    "start": "2824880",
    "end": "2831290"
  },
  {
    "text": "particular j. So this is the formula\nwe have to solve. What's this mean? It means we have to solve\nthis formula for all i.",
    "start": "2831290",
    "end": "2840280"
  },
  {
    "text": "If I solve it for all i, and\nI've solved this for all i, then that's the linear equation\nin the variables v",
    "start": "2840280",
    "end": "2848910"
  },
  {
    "text": "sub 1 up to v linear equations\nin i equals 2, up to m.",
    "start": "2848910",
    "end": "2860010"
  },
  {
    "text": "We also have decided that\nv sub 1 is equal to 0. In other words, if we start out\nin state 1, you expect the",
    "start": "2860010",
    "end": "2868349"
  },
  {
    "text": "time to get to state 1 is 0. We're already there. OK.",
    "start": "2868350",
    "end": "2873730"
  },
  {
    "text": "So we have to solve these\nlinear equations. And if your philosophy on\nsolving linear equations is",
    "start": "2873730",
    "end": "2883130"
  },
  {
    "text": "that of, I shouldn't say a\ncomputer scientist because I",
    "start": "2883130",
    "end": "2888930"
  },
  {
    "text": "don't want to indicate that they\nare any different from any of the rest of us, but for\nmany people, your philosophy",
    "start": "2888930",
    "end": "2896960"
  },
  {
    "text": "of solving linear equations\nis to try to solve it. If you can't solve it, it\ndoesn't have any solution.",
    "start": "2896960",
    "end": "2904440"
  },
  {
    "text": "And if you're happy with\ndoing that, fine. Some people would rather spend\n10 hours asking whether in",
    "start": "2904440",
    "end": "2913480"
  },
  {
    "text": "general it has any solution,\nrather than spending five minutes solving it.",
    "start": "2913480",
    "end": "2918806"
  },
  {
    "text": "So either way, this expected\nfirst passage time, we've just",
    "start": "2918806",
    "end": "2928420"
  },
  {
    "text": "stated what it is. Starting in state i, it's 1 plus\nthe time to go for any",
    "start": "2928420",
    "end": "2937710"
  },
  {
    "text": "other state you happen\nto go to. If we put this in vector form,\nyou put things in vector form",
    "start": "2937710",
    "end": "2943910"
  },
  {
    "text": "because you want to spend two\nhours finding the general solution, rather than five\nminutes solving the problem.",
    "start": "2943910",
    "end": "2949684"
  },
  {
    "text": " If you have 1,000 states, then\nit works the other way.",
    "start": "2949685",
    "end": "2958660"
  },
  {
    "text": "It takes you multiple hours to\nwork it out by hand, and it takes you five minutes by\nlooking at the equation.",
    "start": "2958660",
    "end": "2965430"
  },
  {
    "text": "So sometimes you win, and\nsometimes you lose by looking at the general solution.",
    "start": "2965430",
    "end": "2970780"
  },
  {
    "text": "If you look at this as a vector\nsolution, the vector v",
    "start": "2970780",
    "end": "2977360"
  },
  {
    "text": "where v1 is equal to zero, and\nthe other v's are unknowns, is",
    "start": "2977360",
    "end": "2983080"
  },
  {
    "text": "the vector r, the\nvector r is 0. 0 reward in state 1.",
    "start": "2983080",
    "end": "2990030"
  },
  {
    "text": "Unit reward in all other states,\nbecause we're trying to get to this end.",
    "start": "2990030",
    "end": "2995860"
  },
  {
    "text": "And then we have the matrix\nhere, t times v. So we want to solve this set of\nlinear equations, and what",
    "start": "2995860",
    "end": "3004780"
  },
  {
    "text": "do we know about this set\nof linear equations? We have an ergotic unit chain.",
    "start": "3004780",
    "end": "3011890"
  },
  {
    "text": "We know that p has\nan eigenvalue, which is equal to 1.",
    "start": "3011890",
    "end": "3018700"
  },
  {
    "text": "We know that's a simple\neigenvalue. So that in fact, when we write\nv equals r plus pv as zero",
    "start": "3018700",
    "end": "3037130"
  },
  {
    "text": "equals r plus p minus\ni times v.",
    "start": "3037130",
    "end": "3050069"
  },
  {
    "text": "And we try to ask whether\nv has any solution, what's the answer? Well, this matrix here has\nan eigenvalue of 1.",
    "start": "3050070",
    "end": "3059140"
  },
  {
    "text": "Since it has an eigenvalue of\none, and since it's a simple eigenvalue, there's a space of\nsolutions to this equation.",
    "start": "3059140",
    "end": "3066160"
  },
  {
    "text": "The space of solutions is the\nvector of all ones and the",
    "start": "3066160",
    "end": "3071329"
  },
  {
    "text": "vector of all anything else. In other words, it's a vector of\nv times any constant alpha.",
    "start": "3071330",
    "end": "3077650"
  },
  {
    "text": "Now we've stuck this in here,\nso now we want to find out what's the set of\nsolutions now.",
    "start": "3077650",
    "end": "3085200"
  },
  {
    "text": "We observe v plus alpha e also\nsatisfies this equation if we",
    "start": "3085200",
    "end": "3091730"
  },
  {
    "text": "found another solution. So if we found a solution, we\nhave a one dimensional family",
    "start": "3091730",
    "end": "3097450"
  },
  {
    "text": "of solutions. Well, since this eigenvalue is a\nsimple eigenvalue, the space",
    "start": "3097450",
    "end": "3107520"
  },
  {
    "text": "of vectors for which r is equal\nto p minus i times v as",
    "start": "3107520",
    "end": "3116040"
  },
  {
    "text": "a one dimensional space, and\ntherefore there has to be a unique solution to\nthis question.",
    "start": "3116040",
    "end": "3122349"
  },
  {
    "text": "OK. So in fact, in only 15 minutes,\nwe've solved the",
    "start": "3122350",
    "end": "3127460"
  },
  {
    "text": "problem in general, so that you\ncan deal with matrices of",
    "start": "3127460",
    "end": "3133710"
  },
  {
    "text": "1,000 states, as opposed\nto two states. And you still have\nthe same answer.",
    "start": "3133710",
    "end": "3140170"
  },
  {
    "text": "OK. So this equation has a simple\nsolution, which says that you",
    "start": "3140170",
    "end": "3146970"
  },
  {
    "text": "can program your computer to\nsolve this set of linear equations, and you're bound\nto get an answer.",
    "start": "3146970",
    "end": "3153270"
  },
  {
    "text": "And the answer will tell you\nhow long it takes to get to this particular state.",
    "start": "3153270",
    "end": "3159958"
  },
  {
    "text": "OK. Let's go one to aggregate\nrewards with a final reward.",
    "start": "3159958",
    "end": "3166704"
  },
  {
    "text": " Starting to sound like-- yes?",
    "start": "3166705",
    "end": "3173559"
  },
  {
    "text": "STUDENT: I'm sorry, for the\nlast example, how are we guaranteed that it's ergotic? Like, I possible you enter a\nloop somewhere that can never",
    "start": "3173560",
    "end": "3181369"
  },
  {
    "text": "go to your trapping\nstate, right? PROFESSOR: But I can't do that\nbecause there always has to be",
    "start": "3181370",
    "end": "3189750"
  },
  {
    "text": "a way of getting to the trapping\nstate, because there's only one recurrent\nstate.",
    "start": "3189750",
    "end": "3194770"
  },
  {
    "text": "All these other states\nare transient now. STUDENT: No, but I mean--",
    "start": "3194770",
    "end": "3199920"
  },
  {
    "text": "OK, like, let's say you\nstart off with a general Markov chain. PROFESSOR: Oh, I start off with\na general Markov chain? You're absolutely right.",
    "start": "3199920",
    "end": "3207060"
  },
  {
    "text": "Then there might be no way of\ngetting from some starting state to state 1, and therefore,\nthe amount of time",
    "start": "3207060",
    "end": "3214610"
  },
  {
    "text": "that it takes you to get from\nthat state to the starting state is going to be infinite. You can't get there.",
    "start": "3214610",
    "end": "3220250"
  },
  {
    "text": "So in fact, what you have to do\nwith a problem like this is to look at it first, and say,\nare you in fact dealing with a",
    "start": "3220250",
    "end": "3228730"
  },
  {
    "text": "unit chain? Or do you have multiple\nrecurrent sets? If you have multiple recurrent\nsets, then the expected time",
    "start": "3228730",
    "end": "3237100"
  },
  {
    "text": "to get into one of the recurrent\nstates, starting from either a transient state,\nor from some other recurrent",
    "start": "3237100",
    "end": "3244840"
  },
  {
    "text": "set is infinite. I mean, just like this business\nwe were going through",
    "start": "3244840",
    "end": "3251820"
  },
  {
    "text": "at the beginning. What you would like to do is not\nhave to go through a lot of calculation when you have, or\na lot of thinking when you",
    "start": "3251820",
    "end": "3260750"
  },
  {
    "text": "have multiple recurrent\nsets of states. You just know what\nhappens there.",
    "start": "3260750",
    "end": "3265980"
  },
  {
    "text": "There's no way to get from this\nrecurrent set to this recurrent set. So that's the end of it.",
    "start": "3265980",
    "end": "3271440"
  },
  {
    "text": "STUDENT: OK. So like it works when you have\nthe unit chain, and then you choose your trapping state to\nbe one instance [INAUDIBLE].",
    "start": "3271440",
    "end": "3276585"
  },
  {
    "text": "PROFESSOR: Yes.  OK. Good. ",
    "start": "3276585",
    "end": "3284220"
  },
  {
    "text": "Now, yes? STUDENT: The previous equation\nis true for any reward.",
    "start": "3284220",
    "end": "3290410"
  },
  {
    "text": "But it's not necessary-- PROFESSOR: Yeah, it is true for\nany set of rewards, yes. ",
    "start": "3290410",
    "end": "3299720"
  },
  {
    "text": "Although what the interpretation\nwould be of any set of rewards is if you\nhave to sort that out.",
    "start": "3299720",
    "end": "3305900"
  },
  {
    "text": "But yes. For any r that you choose,\nthere's going to be one unique solution, so long as one is\nactually a trapping state, and",
    "start": "3305900",
    "end": "3315530"
  },
  {
    "text": "everything else leads to one. ",
    "start": "3315530",
    "end": "3320599"
  },
  {
    "text": "OK, so why do I want to\nput a-- ah, good.",
    "start": "3320600",
    "end": "3325875"
  },
  {
    "text": "STUDENT: I feel like there's a\nlot of the rewards that are designed for it, designed with\nrespect to being in a particular state.",
    "start": "3325875",
    "end": "3331575"
  },
  {
    "text": "PROFESSOR: Yes. STUDENT: But if the rewards are\nactually in transition, so for example, if you go from i to\nj, there are going to be a",
    "start": "3331575",
    "end": "3338012"
  },
  {
    "text": "different number from j to j. How do you deal with that? PROFESSOR: How do I\ndeal with that? Well, then let's talk\nabout that.",
    "start": "3338012",
    "end": "3345000"
  },
  {
    "text": "And in fact, it's fairly simple\nso long as you're only talking about expected\nrewards.",
    "start": "3345000",
    "end": "3350750"
  },
  {
    "text": "Because if I have a reward\nassociated with-- ",
    "start": "3350750",
    "end": "3357096"
  },
  {
    "text": "if I have a reward rij, which is\nthe reward for transition i",
    "start": "3357096",
    "end": "3378574"
  },
  {
    "text": "to j, then if I take the sum of\nrij times p summed over j,",
    "start": "3378574",
    "end": "3396599"
  },
  {
    "text": "what this gives me is the\nexpected reward associated",
    "start": "3396600",
    "end": "3411768"
  },
  {
    "text": "with state j, with state i. ",
    "start": "3411768",
    "end": "3419750"
  },
  {
    "text": "Now, you have to be a little bit\ncareful with this because before we've been picking up\nthis reward as soon as we get",
    "start": "3419750",
    "end": "3426309"
  },
  {
    "text": "to state i, and here suddenly\nwe have a slightly different situation where you have a\nreward associated with state i",
    "start": "3426310",
    "end": "3434560"
  },
  {
    "text": "but you don't pick it up\nuntil the next set. So this is where this problem\nof i or i plus 1 comes in.",
    "start": "3434560",
    "end": "3443780"
  },
  {
    "text": "And you guys can do that much\nbetter than I can, because at",
    "start": "3443780",
    "end": "3449070"
  },
  {
    "text": "my age I start out with an age\nof 60 and an age of 61 is the",
    "start": "3449070",
    "end": "3456480"
  },
  {
    "text": "same thing. I mean, these are-- OK.",
    "start": "3456480",
    "end": "3462130"
  },
  {
    "text": " So anyway, the point of it is,\nif you have rewards associated",
    "start": "3462130",
    "end": "3468660"
  },
  {
    "text": "with transitions you can always\nconvert that to rewards associated with states. ",
    "start": "3468660",
    "end": "3478320"
  },
  {
    "text": "Oh, I didn't really\nget to this. What I've been trying to say\nnow for a while is that",
    "start": "3478320",
    "end": "3486150"
  },
  {
    "text": "sometimes, for some reason or\nother, after you go through",
    "start": "3486150",
    "end": "3493119"
  },
  {
    "text": "and end steps of this Markov\nchain, when you get to the end, you want to consider some\nparticularly large reward for",
    "start": "3493120",
    "end": "3501340"
  },
  {
    "text": "having gotten to the end, or\nsome particularly large cost of getting to the end, or\nsomething which depends on the",
    "start": "3501340",
    "end": "3507950"
  },
  {
    "text": "state that you happen\nto be in. So we will assign some final\nreward which in general can be",
    "start": "3507950",
    "end": "3514630"
  },
  {
    "text": "different from the reward that\nwe're picking up at each of the other states. We're going to do this\nin a particular way.",
    "start": "3514630",
    "end": "3521105"
  },
  {
    "start": "3521105",
    "end": "3527740"
  },
  {
    "text": "You would think that what we\nwould want to do is, if we went through in steps, we would\nassociate this final",
    "start": "3527740",
    "end": "3535210"
  },
  {
    "text": "reward with the n-th step. We're going to do it\na different way. We're going to go through n\nsteps, and then the final",
    "start": "3535210",
    "end": "3542180"
  },
  {
    "text": "reward is what happens on\nthe state after that. So we're really turning the\nproblem of looking at n steps",
    "start": "3542180",
    "end": "3549480"
  },
  {
    "text": "into a problem of looking\nat n plus 1 steps. Why do we do that?",
    "start": "3549480",
    "end": "3554490"
  },
  {
    "text": "Completely arbitrary. It turns out to be convenient\nwhen we talk about dynamic programming, and you'll see\nwhy in just a minute.",
    "start": "3554490",
    "end": "3564720"
  },
  {
    "text": "So this extra final state is\njust an arbitrary thing that",
    "start": "3564720",
    "end": "3569770"
  },
  {
    "text": "you add, and we'll see\nthe main purpose for it in just a minute.",
    "start": "3569770",
    "end": "3575780"
  },
  {
    "text": " OK. So we're going to now look at\nwhat in principle is a much",
    "start": "3575780",
    "end": "3585910"
  },
  {
    "text": "more complicated situation than\nwhat we were looking at before, but you still have this\nbasic mark off condition",
    "start": "3585910",
    "end": "3593180"
  },
  {
    "text": "which is making things\nsimple for you. So the idea is, you're looking\nat a discrete time situation.",
    "start": "3593180",
    "end": "3600990"
  },
  {
    "text": "Things happen in steps. There's a finite set of states\nwhich don't change over time.",
    "start": "3600990",
    "end": "3607655"
  },
  {
    "text": " At each unit of time, you're\ngoing to be in one of the set",
    "start": "3607655",
    "end": "3613690"
  },
  {
    "text": "of m states, and at each time l,\nthere's some decision maker",
    "start": "3613690",
    "end": "3620420"
  },
  {
    "text": "sitting around who looks\nat the state that you're in at time l.",
    "start": "3620420",
    "end": "3626530"
  },
  {
    "text": "And the decision maker says I\nhave a choice between what",
    "start": "3626530",
    "end": "3631970"
  },
  {
    "text": "reward I'm going to pick up\nat this time and what the",
    "start": "3631970",
    "end": "3638570"
  },
  {
    "text": "transition probabilities are for\ngoing to the next state. OK, so it's kind of a\ncomplicated thing.",
    "start": "3638570",
    "end": "3646110"
  },
  {
    "text": "It's the same thing that\nyou face all the time.",
    "start": "3646110",
    "end": "3651440"
  },
  {
    "text": "I mean, in the stock market for\nexample, you see that one stock is doing poorly,\nso you have a choice.",
    "start": "3651440",
    "end": "3657010"
  },
  {
    "text": "Should I sell it, eat my losses,\nor should I keep on",
    "start": "3657010",
    "end": "3663620"
  },
  {
    "text": "going and hope it'll\nturnaround? If you're doing a thesis, you\nhave the even worse problem.",
    "start": "3663620",
    "end": "3669980"
  },
  {
    "text": "You go for three months without\ngetting the result that you need, and you say,\nwell, I don't have a thesis.",
    "start": "3669980",
    "end": "3679120"
  },
  {
    "text": "I can't say something\nabout this. Should I go on for one more\nmonth, or should I can it and",
    "start": "3679120",
    "end": "3685280"
  },
  {
    "text": "go on to another topic? OK, it's exactly the\nsame situation.",
    "start": "3685280",
    "end": "3690460"
  },
  {
    "text": "So this is really a very broad\nset of situations. The only thing that makes it\nreally different from real",
    "start": "3690460",
    "end": "3697858"
  },
  {
    "text": "life is this Markov property\nsitting there and the fact that you actually understand\nwhat the rewards are and you",
    "start": "3697858",
    "end": "3706190"
  },
  {
    "text": "can predict them in advance. You can't predict what state\nyou're going to be in, but you",
    "start": "3706190",
    "end": "3711990"
  },
  {
    "text": "know that if you're in a\nparticular state, you know what your choices are in the\nfuture as well as now, and all",
    "start": "3711990",
    "end": "3718560"
  },
  {
    "text": "you have to do at each unit of\ntime is to make this choice between various different\nthings.",
    "start": "3718560",
    "end": "3725860"
  },
  {
    "text": "You see an interesting\nexample of that here. ",
    "start": "3725860",
    "end": "3733890"
  },
  {
    "text": "If you look at this Markov chain\nhere, it's a two state Markov chain. ",
    "start": "3733890",
    "end": "3741770"
  },
  {
    "text": "And what's the steady\nstate probability of being in state one? ",
    "start": "3741770",
    "end": "3752420"
  },
  {
    "text": "Anybody?  It's a half, yes. Why is it a half, and\nwhy don't you have",
    "start": "3752420",
    "end": "3760480"
  },
  {
    "text": "to solve for this? Why can you look at it\nand say it's a half? Because it's completely\nsymmetric.",
    "start": "3760480",
    "end": "3766740"
  },
  {
    "text": "0.99 here, 0.99 here, 0.01\nhere, 0.01 here.",
    "start": "3766740",
    "end": "3773930"
  },
  {
    "text": "These rewards here had nothing\nto do with the Markov chain itself. The Markov chain is symmetric\nbetween states one and two,",
    "start": "3773930",
    "end": "3782210"
  },
  {
    "text": "and therefore, the steady state\nprobabilities have to be one half each. So here's something where, if\nyou happen to be in state two,",
    "start": "3782210",
    "end": "3793410"
  },
  {
    "text": "you're going to stay\nthere typically for a very long time. And while you're studying there\nfor a very long time,",
    "start": "3793410",
    "end": "3800080"
  },
  {
    "text": "you're going to be picking up\nrewards one unit of reward every unit of time.",
    "start": "3800080",
    "end": "3806930"
  },
  {
    "text": "You work for some very stable\nemployer who pays you very little, and that's a\nsituation you have.",
    "start": "3806930",
    "end": "3813540"
  },
  {
    "text": "You're sitting here, you have\na job but you're not making much, but still you're making\nsomething, and you have a lot",
    "start": "3813540",
    "end": "3822710"
  },
  {
    "text": "of job security. Now, we have a different choice\nwhen we're sitting here",
    "start": "3822710",
    "end": "3829760"
  },
  {
    "text": "with a job in state two, we can,\nfor example, you can go",
    "start": "3829760",
    "end": "3837390"
  },
  {
    "text": "to the cash register and take\nall the money out of it and disappear from the company. ",
    "start": "3837390",
    "end": "3843920"
  },
  {
    "text": "I don't advocate doing\nthat, except, it's one of your choices.",
    "start": "3843920",
    "end": "3849190"
  },
  {
    "text": "So you pick up a big reward of\n50, and then for a long period of time you go back to this\nstate over here and you make",
    "start": "3849190",
    "end": "3858820"
  },
  {
    "text": "nothing in reward for a\nlong period of time while you're in jail. And then eventually you pop back\nhere, and if we assume",
    "start": "3858820",
    "end": "3868650"
  },
  {
    "text": "the judicial system is such\nthat it has no memory, [INAUDIBLE]",
    "start": "3868650",
    "end": "3873670"
  },
  {
    "text": "you can cut into the cash\nregister, and, well, OK.",
    "start": "3873670",
    "end": "3880020"
  },
  {
    "text": "So anyway, this decision two,\nyou're looking for instant gratification here.",
    "start": "3880020",
    "end": "3885410"
  },
  {
    "text": "You're getting a big reward all\nat once, but by getting a big reward with probability\none, you're going back to",
    "start": "3885410",
    "end": "3893040"
  },
  {
    "text": "state zero. From state zero, it takes a long\ntime to get back to the point where you can get a big\nreward again, so you wonder,",
    "start": "3893040",
    "end": "3902940"
  },
  {
    "text": "is it better to use this policy\nor is it better to use this policy?",
    "start": "3902940",
    "end": "3908270"
  },
  {
    "text": " Now, there are two basic ways\nto look at this problem.",
    "start": "3908270",
    "end": "3914160"
  },
  {
    "text": "I think it's important to\nunderstand what they are before we go further. One of the ways is to say, OK,\nlet's suppose that I work out",
    "start": "3914160",
    "end": "3924660"
  },
  {
    "text": "which is the best policy\nand I use it forever.",
    "start": "3924660",
    "end": "3930440"
  },
  {
    "text": "Namely, I use this policy\nforever or I use this policy forever.",
    "start": "3930440",
    "end": "3936910"
  },
  {
    "text": "And if I use this policy\nforever, I can pretty easily work out what the steady state\nprobabilities of these two",
    "start": "3936910",
    "end": "3943470"
  },
  {
    "text": "states are. I can then work out what my\nexpected gain is per unit time",
    "start": "3943470",
    "end": "3950040"
  },
  {
    "text": "and I can compare\nthis with that. ",
    "start": "3950040",
    "end": "3955260"
  },
  {
    "text": "And who thinks that this is\ngoing to be better than that and who thinks that this is\ngoing to be better than that?",
    "start": "3955260",
    "end": "3961369"
  },
  {
    "text": "Well, you can work\nit out easily. It's kind of interesting because\nthe steady state gain",
    "start": "3961370",
    "end": "3967080"
  },
  {
    "text": "here and here are very\nclose to the same.",
    "start": "3967080",
    "end": "3972940"
  },
  {
    "text": "It turns out that this is just\na smidgen better than this, only by a very small amount.",
    "start": "3972940",
    "end": "3979609"
  },
  {
    "text": "OK. See, what happens here is that\nhere, you tend to go for about",
    "start": "3979610",
    "end": "3985619"
  },
  {
    "text": "100 steps here. So you pick up every reward of\nabout 100 if you use this very",
    "start": "3985620",
    "end": "3993090"
  },
  {
    "text": "simple minded analysis. Then for 100 steps, you're\nsitting here, you're getting no reward, so you think we ought\nto get every reward of",
    "start": "3993090",
    "end": "4003020"
  },
  {
    "text": "one half on the average,\nand that's exactly what you do get here.",
    "start": "4003020",
    "end": "4008090"
  },
  {
    "text": "And here, you get this big\nreward of 50, but then you go over here and you spend 100\nunits of time in purgatory and",
    "start": "4008090",
    "end": "4017560"
  },
  {
    "text": "then you get back again, you get\nanother reward of 50 and then spend hundreds units\nof time in purgatory.",
    "start": "4017560",
    "end": "4023830"
  },
  {
    "text": "So again, you're getting pretty\nclose to a half of a unit of reward, but it turns\nout, when you work it out,",
    "start": "4023830",
    "end": "4030690"
  },
  {
    "text": "that here is just a smidgen. It's 1% less than a half, so\nthis is not as good as that.",
    "start": "4030690",
    "end": "4038190"
  },
  {
    "text": "But suppose that you have\na shorter time horizon.",
    "start": "4038190",
    "end": "4044900"
  },
  {
    "text": "Suppose you don't want to wait\nfor 1,000 steps to see what's going on, so you don't want\nto look at the average.",
    "start": "4044900",
    "end": "4052279"
  },
  {
    "text": "Suppose this was a\ngambling game. You have your choice of these\ntwo gambling options, and",
    "start": "4052280",
    "end": "4058230"
  },
  {
    "text": "suppose you're only going to be\nplaying for a short time. Suppose you're going\nto be only playing for one unit of time.",
    "start": "4058230",
    "end": "4064829"
  },
  {
    "text": "You can only play for one unit\nof time and then you have to stop, you have to go home, you\nhave to go back to work, or",
    "start": "4064830",
    "end": "4070780"
  },
  {
    "text": "something else. And you happen to be sitting\nin state two. What do you want to do\nif you only have one",
    "start": "4070780",
    "end": "4077180"
  },
  {
    "text": "unit of time to play. Well, obviously, you want to get\nthe reward of 50, because",
    "start": "4077180",
    "end": "4083630"
  },
  {
    "text": "delayed gratification doesn't\nwork here, because you don't get any opportunity for that\ngratification later.",
    "start": "4083630",
    "end": "4091330"
  },
  {
    "text": "So you pick up the big\nreward at first. So when you have this problem of\nplaying for a finite amount",
    "start": "4091330",
    "end": "4098630"
  },
  {
    "text": "of time, whatever kind of\nsituation you're in, what you",
    "start": "4098630",
    "end": "4104649"
  },
  {
    "text": "would like to do is say, for\nthis finite amount of time that I'm going to play, what's\nmy best strategy then?",
    "start": "4104649",
    "end": "4114290"
  },
  {
    "text": "Dynamic programming is the\nproblem, which is the",
    "start": "4114290",
    "end": "4119850"
  },
  {
    "text": "algorithm which finds out what\nthe best thing to do is dynamically.",
    "start": "4119850",
    "end": "4125000"
  },
  {
    "text": "Namely, if you're going to stop\nin 10 steps, stop in 100 steps, stop in one step, it\ntells you what to do under all",
    "start": "4125000",
    "end": "4132710"
  },
  {
    "text": "of those circumstances. And the stationary policy tells\nyou what to do if you're",
    "start": "4132710",
    "end": "4139339"
  },
  {
    "text": "going to play forever. But in a situation like this\nwhere things happen rather",
    "start": "4139340",
    "end": "4145759"
  },
  {
    "text": "slowly, it might not be the\nrelevant thing to deal with. A lot of the notes deal with\ncomparing the stationary",
    "start": "4145760",
    "end": "4153170"
  },
  {
    "text": "policy with this\ndynamic policy. And I'm not going to do that\nhere because, well, we have",
    "start": "4153170",
    "end": "4161399"
  },
  {
    "text": "too many other interesting\nthings that we want to deal with. So we're just going to skip\nall of that stuff about",
    "start": "4161399",
    "end": "4166939"
  },
  {
    "text": "stationary policies. You don't have to bother to\nread it unless you're interested in it.",
    "start": "4166939",
    "end": "4172580"
  },
  {
    "text": "I mean, if you're interested in\nit, by all means, read it. It's a very interesting topic.",
    "start": "4172580",
    "end": "4178949"
  },
  {
    "text": "It's not all that interesting\nto find out what the best stationary policy is. That's kind of simple.",
    "start": "4178950",
    "end": "4185210"
  },
  {
    "text": "What's the interesting topic\nis what's the comparison between the dynamic policy and\nthe stationary policy.",
    "start": "4185210",
    "end": "4193100"
  },
  {
    "text": "But all we're going to do\nis worry about what the dynamic policy is.",
    "start": "4193100",
    "end": "4198160"
  },
  {
    "text": "That seems like a hard problem,\nand someone by the",
    "start": "4198160",
    "end": "4203460"
  },
  {
    "text": "name of Bellman figured out what\nthe optimal solution to",
    "start": "4203460",
    "end": "4209719"
  },
  {
    "text": "that dynamic policy was. And it turned out to be a\ntrivially simple algorithm,",
    "start": "4209720",
    "end": "4216900"
  },
  {
    "text": "and Bellman became\nfamous forever. One of the things I want to\npoint out to you, again, I",
    "start": "4216900",
    "end": "4223079"
  },
  {
    "text": "keep coming back to this because\nyou people are just starting a research career.",
    "start": "4223080",
    "end": "4229970"
  },
  {
    "text": "Everyone in this class, given\nthe formulation of this dynamic programming problem,\ncould develop and would",
    "start": "4229970",
    "end": "4238670"
  },
  {
    "text": "develop, I'm pretty sure, the\ndynamic programming algorithm. Developing the algorithm,\nunderstanding what the problem",
    "start": "4238670",
    "end": "4247020"
  },
  {
    "text": "is is a trivial matter. Why is Bellman famous?",
    "start": "4247020",
    "end": "4252390"
  },
  {
    "text": "Because he formulated\nthe problem. He said, aha, this dynamic\nproblem is interesting.",
    "start": "4252390",
    "end": "4261010"
  },
  {
    "text": "I don't have to go through\nthe stationary problem. And in fact, my sense from\nreading his book and from",
    "start": "4261010",
    "end": "4268430"
  },
  {
    "text": "reading things he's written is\nthat he couldn't have solved the stationary problem because\nhe didn't understand",
    "start": "4268430",
    "end": "4274240"
  },
  {
    "text": "probability that well. But he did understand how to\nformulate what this really",
    "start": "4274240",
    "end": "4280600"
  },
  {
    "text": "important problem was\nand he solved it. So, all the more credit to him,\nbut when you're doing",
    "start": "4280600",
    "end": "4287880"
  },
  {
    "text": "research, the time you spend\non formulating the right problem is far more important\nthan the time you spend",
    "start": "4287880",
    "end": "4297430"
  },
  {
    "text": "solving it. If you start out with the right\nproblem, the solution is trivial and you're all done.",
    "start": "4297430",
    "end": "4305650"
  },
  {
    "text": "It's hard to formulate the right\nproblem, and you learn to formulate the problem not\nby playing all of this",
    "start": "4305650",
    "end": "4317810"
  },
  {
    "text": "calculating things, but by\nsetting back and thinking about the problem and trying\nto look at things in a more",
    "start": "4317810",
    "end": "4324480"
  },
  {
    "text": "general way. So just another plug. I've been saying this, I will\nprobably say it every three or",
    "start": "4324480",
    "end": "4330440"
  },
  {
    "text": "four lectures throughout\nthe term. OK. So let's go back and look\nat what the problem is.",
    "start": "4330440",
    "end": "4338449"
  },
  {
    "text": "We haven't quite formulated\nit yet. We're going to assume this\nprocess of random transitions",
    "start": "4338450",
    "end": "4344940"
  },
  {
    "text": "combined with decisions based\non the current state. In other words, in this decision\nmaker, the decision",
    "start": "4344940",
    "end": "4350380"
  },
  {
    "text": "maker at each unit of time sees\nwhat state you're in at this unit of time.",
    "start": "4350380",
    "end": "4357040"
  },
  {
    "text": "And seeing what state you're in\nat this given unit of time, the decision maker has a choice\nbetween how much reward",
    "start": "4357040",
    "end": "4365020"
  },
  {
    "text": "is to be taken and along with\nhow much reward is to be",
    "start": "4365020",
    "end": "4371740"
  },
  {
    "text": "taken, what the transition\nprobabilities are for the next state. If you rob the cash register,\nyour transition probabilities",
    "start": "4371740",
    "end": "4380150"
  },
  {
    "text": "are going to be very different\nthan if you don't rob the cash register. By robbing the cash register,\nyour transition probabilities",
    "start": "4380150",
    "end": "4388190"
  },
  {
    "text": "go into a rather high transition\nprobability that you're going to be caught. OK, so you don't want that.",
    "start": "4388190",
    "end": "4396050"
  },
  {
    "text": "So you can't avoid the problem\nof having the rewards at a given time locked into what the\ntransition probabilities",
    "start": "4396050",
    "end": "4404290"
  },
  {
    "text": "are for going to the next state,\nand that's the essence of this problem.",
    "start": "4404290",
    "end": "4409890"
  },
  {
    "text": "OK. So, the decision maker observers\nthe state and chooses one of a finite\nset of alternatives.",
    "start": "4409890",
    "end": "4416530"
  },
  {
    "text": "Each alternative consists of\nrecurrent reward which we'll call r sub j of k, the\nalternative is k, and a set of",
    "start": "4416530",
    "end": "4424030"
  },
  {
    "text": "transition probabilities. pjl of k, one less than or\nequal to a l less than or",
    "start": "4424030",
    "end": "4430250"
  },
  {
    "text": "equal to m for going\nto the next state. OK, the notation here is\nhorrifying, but the idea is",
    "start": "4430250",
    "end": "4436450"
  },
  {
    "text": "very simple. I mean, once you get used to the\nnotation, there's nothing complicated here at all.",
    "start": "4436450",
    "end": "4444880"
  },
  {
    "text": "OK, so in this example here,\nwell, we already talked about that.",
    "start": "4444880",
    "end": "4450190"
  },
  {
    "text": " We're going to start\nout at time m. ",
    "start": "4450190",
    "end": "4457960"
  },
  {
    "text": "We're going to make a decision\nat time m, pick up the associated reward for that\ndecision, and pick the",
    "start": "4457960",
    "end": "4468090"
  },
  {
    "text": "transition probabilities that\nwe're going to use at that time m, and then go on\nto the next state.",
    "start": "4468090",
    "end": "4473460"
  },
  {
    "text": "We're going to continue doing\nthis until time m plus n minus 1. Mainly, we're going to do this\nfor n steps of time.",
    "start": "4473460",
    "end": "4481450"
  },
  {
    "text": "After the n-th decision-- you make the n-th decision\nat m plus n minus t--",
    "start": "4481450",
    "end": "4487140"
  },
  {
    "text": "there's a final transition\nbased on that decision.",
    "start": "4487140",
    "end": "4492270"
  },
  {
    "text": "The final transition is based\non that decision, but the final reward is fixed\nahead of time.",
    "start": "4492270",
    "end": "4498345"
  },
  {
    "text": "You know what the final reward\nis going to be, which happens at time m plus n.",
    "start": "4498345",
    "end": "4503480"
  },
  {
    "text": "So the things which are variable\nis how much reward do you get at each of these first\nn time units, and what",
    "start": "4503480",
    "end": "4514070"
  },
  {
    "text": "probabilities you choose for\ngoing through the next state. Is this still a Markov chain?",
    "start": "4514070",
    "end": "4520170"
  },
  {
    "text": "Is this still Markov?  You can talk about this\nfor a long time.",
    "start": "4520170",
    "end": "4526560"
  },
  {
    "text": "You can think about it for\na long time because this decision maker might or\nmight not be Markov.",
    "start": "4526560",
    "end": "4534770"
  },
  {
    "text": "What is Markov is the transition\nprobabilities that are taking place in\neach unit of time.",
    "start": "4534770",
    "end": "4541380"
  },
  {
    "text": "After I make a decision, the\ntransition probabilities are",
    "start": "4541380",
    "end": "4546409"
  },
  {
    "text": "fixed for that decision and\nthat initial state and had nothing to do with the decisions\nthat had been made",
    "start": "4546410",
    "end": "4554650"
  },
  {
    "text": "before that or the states you've\nbeen in before that. The Markov condition says that\nwhat happens in the next unit",
    "start": "4554650",
    "end": "4562219"
  },
  {
    "text": "of time is a function simply\nof those transition probabilities that\nhad been chosen.",
    "start": "4562220",
    "end": "4570370"
  },
  {
    "text": "We will see that when we look at\nthe algorithm, and then you can sort out for yourselves\nwhether there's something",
    "start": "4570370",
    "end": "4576670"
  },
  {
    "text": "dishonest here or not. Turns out there isn't, but to\nBellman's credit he did sort",
    "start": "4576670",
    "end": "4585480"
  },
  {
    "text": "out correctly that this worked,\nand many people for a long time did not\nthink it worked.",
    "start": "4585480",
    "end": "4590520"
  },
  {
    "text": " So the objective of dynamic\nprogramming is both to",
    "start": "4590520",
    "end": "4597150"
  },
  {
    "text": "determine the optimal decision\nat each time and to determine the expected reward for each\nstarting state and for each",
    "start": "4597150",
    "end": "4605040"
  },
  {
    "text": "number and steps. As one might suspect, now here's\nthe first thing that",
    "start": "4605040",
    "end": "4611090"
  },
  {
    "text": "Bellman did. He said, here, I have\nthis problem. I want to find out what happens\nafter 1,000 steps.",
    "start": "4611090",
    "end": "4617880"
  },
  {
    "text": "How do I solve the problem? Well, anybody with any sense\nwill tell you don't solve the",
    "start": "4617880",
    "end": "4624330"
  },
  {
    "text": "problem with 1,000\nsteps first. Solve the problem with one step\nfirst, and then see if",
    "start": "4624330",
    "end": "4630220"
  },
  {
    "text": "you find out anything from it\nand then maybe you can solve the problem with two steps and\nthen maybe something nice will",
    "start": "4630220",
    "end": "4637030"
  },
  {
    "text": "happen, or maybe it won't. When we do this, it'll turn\nout that what we're really",
    "start": "4637030",
    "end": "4645320"
  },
  {
    "text": "doing is we're starting at the\nend and working our way back,",
    "start": "4645320",
    "end": "4650820"
  },
  {
    "text": "and this algorithm is due to\nRichard Bellman, as I said. And he was the one who sorted\nout how it worked.",
    "start": "4650820",
    "end": "4658400"
  },
  {
    "text": "So what is the algorithm? We're going to start out making\na decision at time 1.",
    "start": "4658400",
    "end": "4665250"
  },
  {
    "text": "So we're going to\nstart at time n.",
    "start": "4665250",
    "end": "4670500"
  },
  {
    "text": "We're going to start\nin a given state i. You make a decision, decision\nk at time m.",
    "start": "4670500",
    "end": "4678580"
  },
  {
    "text": "This provides a reward at time\nm, and the selected transition probabilities lead to a\nfinal expected reward.",
    "start": "4678580",
    "end": "4686240"
  },
  {
    "text": "These are these final rewards\nwhich occur at time n plus 1.",
    "start": "4686240",
    "end": "4691380"
  },
  {
    "text": "It's nice to have that n because\nit's what let's us generalize the problem. So this was another clever\nthing that went on here.",
    "start": "4691380",
    "end": "4698460"
  },
  {
    "text": "So the expected optimal\naggregate reward for a one",
    "start": "4698460",
    "end": "4704710"
  },
  {
    "text": "step problem is the sum of the\nreward that you get at time m",
    "start": "4704710",
    "end": "4712230"
  },
  {
    "text": "plus this final reward you get\nat time n plus 1, and you're",
    "start": "4712230",
    "end": "4717260"
  },
  {
    "text": "maximizing over the different\npolicies you have available to you. So it looks like a trivial\nproblem, but the optimal",
    "start": "4717260",
    "end": "4724970"
  },
  {
    "text": "reward with a one step\nproblem is just this. ",
    "start": "4724970",
    "end": "4731170"
  },
  {
    "text": "OK, next you want to consider\nthe two step problem. What's the maximum expected\nreward starting at xm equals i",
    "start": "4731170",
    "end": "4738900"
  },
  {
    "text": "with decisions at times\nm and n plus 1. You make two decisions.",
    "start": "4738900",
    "end": "4745400"
  },
  {
    "text": "Now, before, we just made\none decision at time m. Now we make a decision at time\nm and at time n plus 1, and",
    "start": "4745400",
    "end": "4753000"
  },
  {
    "text": "finally we pick up a final\nreward at time n plus 2. Knowing what that final reward\nis going to be is going to",
    "start": "4753000",
    "end": "4760540"
  },
  {
    "text": "affect the decision you make at\ntime n plus 1, but it's a",
    "start": "4760540",
    "end": "4766230"
  },
  {
    "text": "fixed reward which is a\nfunction of the state. You can adjust the transition\nprobabilities of getting to",
    "start": "4766230",
    "end": "4772720"
  },
  {
    "text": "those different rewards. The key to dynamic programming\nis an optimal decision at time",
    "start": "4772720",
    "end": "4778420"
  },
  {
    "text": "n plus 1 can be selected based\nonly on the state j at time n plus 1.",
    "start": "4778420",
    "end": "4785060"
  },
  {
    "text": "This decision, given that you're\nin state j at time n plus 1, is optimal independent\nof what you did before that,",
    "start": "4785060",
    "end": "4793600"
  },
  {
    "text": "which is why we're starting\nout looking at what we're going to do with time n plus 1\nbefore we even worry about",
    "start": "4793600",
    "end": "4799240"
  },
  {
    "text": "what we're going to\ndo with time n. So, whatever decision you made\nat time n, you observe what",
    "start": "4799240",
    "end": "4806340"
  },
  {
    "text": "state you're at time n plus\n1 and the maximal expected reward over times n plus 1 and\nn plus 2, given that you",
    "start": "4806340",
    "end": "4815510"
  },
  {
    "text": "happen to be in state j is just\nmaximal over k as the",
    "start": "4815510",
    "end": "4820610"
  },
  {
    "text": "reward you're going to get by\nchoosing policy k and the",
    "start": "4820610",
    "end": "4826429"
  },
  {
    "text": "expected value of the final\nreward you get if you're using this policy k.",
    "start": "4826430",
    "end": "4832480"
  },
  {
    "text": "This is just dj* of 1 and\nu as you just found. In other words, you have the\nsame situation at time n plus",
    "start": "4832480",
    "end": "4840070"
  },
  {
    "text": "1 as you have at time n.  Well, surprisingly, you've just\nsolved the whole problem.",
    "start": "4840070",
    "end": "4849785"
  },
  {
    "text": " So we've seen that what we\nshould do at time n plus 1 is",
    "start": "4849785",
    "end": "4858410"
  },
  {
    "text": "do this maximization. So the optimal reward, aggregate\nreward over times m,",
    "start": "4858410",
    "end": "4865670"
  },
  {
    "text": "n plus 1, and n plus 2 is what\nwe get maximizing over our",
    "start": "4865670",
    "end": "4871815"
  },
  {
    "text": "choice at time m of the reward\nwe get at time m plus the",
    "start": "4871815",
    "end": "4878110"
  },
  {
    "text": "decision plus the transition\nprobabilities which we've decided on which get us to this\nreward at time n plus 1",
    "start": "4878110",
    "end": "4887340"
  },
  {
    "text": "and n plus 2. We found out what the reward\nis for times n plus 1 and n",
    "start": "4887340",
    "end": "4893070"
  },
  {
    "text": "plus 2 together. That's the reward to go, And\nwe know what that is, so we have this same formula\nwe used before.",
    "start": "4893070",
    "end": "4900210"
  },
  {
    "text": "Why do we want to look at\nthese final rewards now?",
    "start": "4900210",
    "end": "4906965"
  },
  {
    "text": "Well, you can view this as a\nfinal reward in state m. It's the final reward which\ntells you what you get both",
    "start": "4906965",
    "end": "4914220"
  },
  {
    "text": "from state n plus\n1 and n plus 2. And, going quickly, if we look\nat playing this game for three",
    "start": "4914220",
    "end": "4924790"
  },
  {
    "text": "steps, the optimal reward for\nthe three step game is the",
    "start": "4924790",
    "end": "4931280"
  },
  {
    "text": "immediate reward optimized over\nk plus the rewards at n",
    "start": "4931280",
    "end": "4936599"
  },
  {
    "text": "plus 1, n plus 2, and n plus 3,\nwhich we've already found.",
    "start": "4936600",
    "end": "4942900"
  },
  {
    "text": "And in general, the optimal\nreward at time n--",
    "start": "4942900",
    "end": "4948449"
  },
  {
    "text": "when you play the game for n\nsteps, the optimal reward is",
    "start": "4948450",
    "end": "4953900"
  },
  {
    "text": "maximum here. So, all you do in the algorithm\nis, for each value",
    "start": "4953900",
    "end": "4959980"
  },
  {
    "text": "of n when you start with n\nequal to 1, you solve the problem for all states and you\nmaximize over all policies you",
    "start": "4959980",
    "end": "4968950"
  },
  {
    "text": "have a choice over, and then you\ngo on to the next larger value of n, you solve the\nproblem for all states and you",
    "start": "4968950",
    "end": "4976140"
  },
  {
    "text": "keep on going. If you don't have many\nstates, it's easy. If you have 100,000 states, it's\nkind of tedious to run",
    "start": "4976140",
    "end": "4985099"
  },
  {
    "text": "the algorithm. Today it's not bad, but today\nwe look at problems with millions and millions of states\nor billions of states,",
    "start": "4985100",
    "end": "4991770"
  },
  {
    "text": "and no matter how fast\ncomputation gets, the",
    "start": "4991770",
    "end": "4998100"
  },
  {
    "text": "ingenuity people to invent\nharder problems always makes it hard to solve\nthese problems.",
    "start": "4998100",
    "end": "5004630"
  },
  {
    "text": "So anyway, that's the dynamic\nprogramming algorithm. And next time, we're going to\nstart on renewal processes.",
    "start": "5004630",
    "end": "5011320"
  },
  {
    "start": "5011320",
    "end": "5015552"
  }
]