[
  {
    "start": "0",
    "end": "54000"
  },
  {
    "text": "[MUSIC PLAYING]",
    "start": "0",
    "end": "3952"
  },
  {
    "start": "3952",
    "end": "5762"
  },
  {
    "text": "MIKE TEODORESCU:\nHello, and welcome",
    "start": "5762",
    "end": "7220"
  },
  {
    "text": "to this module on choices\nof fairness criteria.",
    "start": "7220",
    "end": "9280"
  },
  {
    "text": "My name is Mike Teodorescu.",
    "start": "9280",
    "end": "10210"
  },
  {
    "text": "I'm an assistant professor\nof information systems",
    "start": "10210",
    "end": "11860"
  },
  {
    "text": "at Boston College, as was a\nvisiting scholar at MIT D-Lab.",
    "start": "11860",
    "end": "14276"
  },
  {
    "text": "What we'll cover\nin this module will",
    "start": "14277",
    "end": "15777"
  },
  {
    "text": "be the concept of\nconfusion matrix,",
    "start": "15777",
    "end": "17290"
  },
  {
    "text": "as well as three popular\nexamples of fairness criteria--",
    "start": "17290",
    "end": "20470"
  },
  {
    "text": "demographic parity,\nequalized odds,",
    "start": "20470",
    "end": "22599"
  },
  {
    "text": "and equalized opportunity.",
    "start": "22600",
    "end": "25418"
  },
  {
    "text": "Some countries have\nlaws that protect",
    "start": "25418",
    "end": "26960"
  },
  {
    "text": "specific groups of people\nfrom discrimination",
    "start": "26960",
    "end": "29269"
  },
  {
    "text": "based on certain attributes.",
    "start": "29270",
    "end": "31490"
  },
  {
    "text": "As we review in\nthe previous video,",
    "start": "31490",
    "end": "33480"
  },
  {
    "text": "these are called\nprotected attributes.",
    "start": "33480",
    "end": "35630"
  },
  {
    "text": "Some examples are on this slide.",
    "start": "35630",
    "end": "38523"
  },
  {
    "text": "Regardless of the\nlegal landscape,",
    "start": "38523",
    "end": "39940"
  },
  {
    "text": "machine learning\nhas the potential",
    "start": "39940",
    "end": "41357"
  },
  {
    "text": "to produce unfair outcomes\nfor certain groups of people.",
    "start": "41357",
    "end": "45019"
  },
  {
    "text": "As an algorithm designer,\none should make clear choices",
    "start": "45020",
    "end": "47540"
  },
  {
    "text": "about fairness criteria.",
    "start": "47540",
    "end": "49520"
  },
  {
    "text": "Some criteria will be reviewed\nin the next few slides.",
    "start": "49520",
    "end": "51980"
  },
  {
    "start": "51980",
    "end": "54535"
  },
  {
    "start": "54000",
    "end": "54000"
  },
  {
    "text": "In the previous video, we also\ndiscussed case of fairness",
    "start": "54535",
    "end": "56910"
  },
  {
    "text": "through unawareness,\nwhich refers",
    "start": "56910",
    "end": "58910"
  },
  {
    "text": "to leaving out the protected\nattributes out of your model.",
    "start": "58910",
    "end": "62190"
  },
  {
    "text": "And we also explained why\nthis is not a good choice.",
    "start": "62190",
    "end": "64448"
  },
  {
    "text": "Specifically, you may end up\nwith other attributes that",
    "start": "64448",
    "end": "66740"
  },
  {
    "text": "correlate with\nprotected attributes,",
    "start": "66740",
    "end": "68630"
  },
  {
    "text": "and you may end up\ndiscriminating inadvertently",
    "start": "68630",
    "end": "71299"
  },
  {
    "text": "nonetheless.",
    "start": "71300",
    "end": "73928"
  },
  {
    "text": "In order to go into\nadditional fairness criteria,",
    "start": "73928",
    "end": "75970"
  },
  {
    "start": "74000",
    "end": "74000"
  },
  {
    "text": "we need to discuss some\nadditional concepts.",
    "start": "75970",
    "end": "78750"
  },
  {
    "text": "Consider you have a\nbinary classifier.",
    "start": "78750",
    "end": "81750"
  },
  {
    "text": "For example, you're\nlooking at decision of hire",
    "start": "81750",
    "end": "84630"
  },
  {
    "text": "or not hired or to lend\ncredit and not to lend credit.",
    "start": "84630",
    "end": "88380"
  },
  {
    "text": "If we want to look at the\npredictions for a model that",
    "start": "88380",
    "end": "91957"
  },
  {
    "text": "would do such a\nbinary classification,",
    "start": "91957",
    "end": "93540"
  },
  {
    "text": "we would look at the\npredicted values.",
    "start": "93540",
    "end": "95420"
  },
  {
    "text": "We could bucket them in four\ncategories-- true positives,",
    "start": "95420",
    "end": "98292"
  },
  {
    "text": "which would be\ncorrectly classified",
    "start": "98292",
    "end": "99750"
  },
  {
    "text": "as positive, true negative,\ncorrectly classified",
    "start": "99750",
    "end": "102300"
  },
  {
    "text": "as negative, false positives,\nwhich would be values that",
    "start": "102300",
    "end": "105645"
  },
  {
    "text": "were incorrectly classified\nas positive by the algorithm,",
    "start": "105645",
    "end": "108338"
  },
  {
    "text": "false negatives, which would be\nvalues incorrectly classified",
    "start": "108338",
    "end": "110880"
  },
  {
    "text": "as negative.",
    "start": "110880",
    "end": "112240"
  },
  {
    "text": "And, if we were to add\nthe the true positives",
    "start": "112240",
    "end": "114284"
  },
  {
    "text": "to the true negatives and\ndivide that by all four,",
    "start": "114285",
    "end": "118270"
  },
  {
    "text": "we would end up with the value\nof the accuracy of the model.",
    "start": "118270",
    "end": "121530"
  },
  {
    "text": "In this example where\naccuracy is this fraction,",
    "start": "121530",
    "end": "124650"
  },
  {
    "text": "an accuracy of 0.5 is the same\nas a random classification.",
    "start": "124650",
    "end": "128970"
  },
  {
    "text": "Now we should look\nat accuracy carefully",
    "start": "128970",
    "end": "131980"
  },
  {
    "text": "and see that it doesn't tell us\nanything about the prediction",
    "start": "131980",
    "end": "134760"
  },
  {
    "text": "of negatives.",
    "start": "134760",
    "end": "135360"
  },
  {
    "text": "It could mislead us if two\nclasses were imbalanced,",
    "start": "135360",
    "end": "137790"
  },
  {
    "text": "for example, if 90% of\nthe sample is positives,",
    "start": "137790",
    "end": "141060"
  },
  {
    "text": "and 10% is negatives.",
    "start": "141060",
    "end": "142620"
  },
  {
    "text": "For that, we have other\nadditional criteria",
    "start": "142620",
    "end": "144989"
  },
  {
    "text": "we could go into deeper, such\nas MCC score and AUC score.",
    "start": "144990",
    "end": "149130"
  },
  {
    "start": "149130",
    "end": "151940"
  },
  {
    "text": "The true positives, true\nnegatives, false positives,",
    "start": "151940",
    "end": "154130"
  },
  {
    "text": "and false negatives are,\noftentimes, represented in a 2",
    "start": "154130",
    "end": "157410"
  },
  {
    "text": "by 2 matrix form called\na confusion matrix.",
    "start": "157410",
    "end": "161160"
  },
  {
    "text": "This is simply an easier\npresentation for us",
    "start": "161160",
    "end": "163560"
  },
  {
    "text": "to see the behavior\nof the classifier.",
    "start": "163560",
    "end": "167870"
  },
  {
    "start": "167000",
    "end": "167000"
  },
  {
    "text": "The first additional\nfairness criteria",
    "start": "167870",
    "end": "169670"
  },
  {
    "text": "is called demographic parity.",
    "start": "169670",
    "end": "171900"
  },
  {
    "text": "It's a criterion for what's\ncalled group-level fairness.",
    "start": "171900",
    "end": "175560"
  },
  {
    "text": "This criterion specify\nthat the outcome, which",
    "start": "175560",
    "end": "178260"
  },
  {
    "text": "here is denoted by a\ny hat, is independent",
    "start": "178260",
    "end": "180659"
  },
  {
    "text": "of the protected attribute A.\nFor example, the probability",
    "start": "180660",
    "end": "185310"
  },
  {
    "text": "of being hired is\nindependent of the gender.",
    "start": "185310",
    "end": "189610"
  },
  {
    "text": "There are multiple problems\nwith demographic parity.",
    "start": "189610",
    "end": "192130"
  },
  {
    "text": "One would be the\ndefinition that we just",
    "start": "192130",
    "end": "193810"
  },
  {
    "text": "discussed would\nnot hold if we had",
    "start": "193810",
    "end": "196030"
  },
  {
    "text": "individuals who would be members\nof multiple protected groups.",
    "start": "196030",
    "end": "199300"
  },
  {
    "text": "By enforcing group-level\nfairness for one attribute,",
    "start": "199300",
    "end": "201642"
  },
  {
    "text": "we would still violate\nthe group fairness",
    "start": "201642",
    "end": "203349"
  },
  {
    "text": "for other attributes or\ncombinations of attributes,",
    "start": "203350",
    "end": "205750"
  },
  {
    "text": "such as subgroups\nof the population.",
    "start": "205750",
    "end": "209310"
  },
  {
    "text": "Furthermore, while enforcing\ngroup-level fairness,",
    "start": "209310",
    "end": "211560"
  },
  {
    "text": "for example, the same hiring\ngrade for females and males,",
    "start": "211560",
    "end": "214620"
  },
  {
    "text": "that could still be\nunfair to individuals.",
    "start": "214620",
    "end": "216737"
  },
  {
    "text": "It could force the algorithm\nto drop otherwise qualified",
    "start": "216737",
    "end": "219069"
  },
  {
    "text": "individuals just to\nachieve independence",
    "start": "219070",
    "end": "220800"
  },
  {
    "text": "of outcome of the attribute.",
    "start": "220800",
    "end": "224700"
  },
  {
    "start": "224000",
    "end": "224000"
  },
  {
    "text": "Fairness at the group\nlevel could, potentially,",
    "start": "224700",
    "end": "226770"
  },
  {
    "text": "be unfair at the\nindividual level.",
    "start": "226770",
    "end": "229300"
  },
  {
    "text": "For example, if we have a\nhigh rate of false positives,",
    "start": "229300",
    "end": "232800"
  },
  {
    "text": "we could end up-- and a low\nrate of false negatives,",
    "start": "232800",
    "end": "234990"
  },
  {
    "text": "it could still end up\nbeing unfair to individuals",
    "start": "234990",
    "end": "237120"
  },
  {
    "text": "in that we could hire\npeople who are without merit",
    "start": "237120",
    "end": "240420"
  },
  {
    "text": "at the disadvantage\nof other individuals",
    "start": "240420",
    "end": "243030"
  },
  {
    "text": "who could be qualified\nand should be hired,",
    "start": "243030",
    "end": "245790"
  },
  {
    "text": "but, due to the group-level\nfairness criterion,",
    "start": "245790",
    "end": "248549"
  },
  {
    "text": "we have to hire some\nwho are not qualified",
    "start": "248550",
    "end": "251160"
  },
  {
    "text": "from one of the groups.",
    "start": "251160",
    "end": "253820"
  },
  {
    "text": "The sweet spot would\nbe low false negatives",
    "start": "253820",
    "end": "255710"
  },
  {
    "text": "and low false positives, which\nwould be fair, potentially,",
    "start": "255710",
    "end": "258440"
  },
  {
    "text": "to both the individual\nand the group level.",
    "start": "258440",
    "end": "261019"
  },
  {
    "text": "We could also end up in\nthe top right corner, which",
    "start": "261019",
    "end": "263240"
  },
  {
    "text": "would be the worst-case\nscenario, low accuracy,",
    "start": "263240",
    "end": "266060"
  },
  {
    "text": "unfair to individuals, but\npotentially fair for the group",
    "start": "266060",
    "end": "268550"
  },
  {
    "text": "where we have high\nfalse negatives",
    "start": "268550",
    "end": "270409"
  },
  {
    "text": "and high false positives.",
    "start": "270410",
    "end": "274210"
  },
  {
    "start": "274000",
    "end": "274000"
  },
  {
    "text": "A stricter criterion\nis equalized odds,",
    "start": "274210",
    "end": "276460"
  },
  {
    "text": "which means matching both\nof true positive rate",
    "start": "276460",
    "end": "278770"
  },
  {
    "text": "and the false positive\nrate for different values",
    "start": "278770",
    "end": "281199"
  },
  {
    "text": "of the protected attribute.",
    "start": "281200",
    "end": "283150"
  },
  {
    "text": "This is much harder to achieve\nthan demographic parity,",
    "start": "283150",
    "end": "285759"
  },
  {
    "text": "but it is one of the higher\nlevels of algorithmic fairness.",
    "start": "285760",
    "end": "289150"
  },
  {
    "text": "In this case, rather\nthan predicting",
    "start": "289150",
    "end": "290860"
  },
  {
    "text": "the same average\nrisk across subgroups",
    "start": "290860",
    "end": "292689"
  },
  {
    "text": "of protected social attributes,\nlike in demographic parity,",
    "start": "292690",
    "end": "296460"
  },
  {
    "text": "equalized odds is stricter\nin that it forces equality",
    "start": "296460",
    "end": "299080"
  },
  {
    "text": "only among individuals who\nreach similar outcomes.",
    "start": "299080",
    "end": "302259"
  },
  {
    "text": "In the hiring example\nthat we've discussed",
    "start": "302260",
    "end": "304360"
  },
  {
    "text": "in the previous\nvideo, this implies",
    "start": "304360",
    "end": "306639"
  },
  {
    "text": "that the probability of a\nqualified applicant to be hired",
    "start": "306640",
    "end": "309120"
  },
  {
    "text": "and the probability of\nan unqualified applicant",
    "start": "309120",
    "end": "311139"
  },
  {
    "text": "to be incorrectly hired should\nbe the same across genders.",
    "start": "311140",
    "end": "313930"
  },
  {
    "start": "313930",
    "end": "317030"
  },
  {
    "text": "A milder version\nof equalized odds",
    "start": "317030",
    "end": "318740"
  },
  {
    "text": "is equalized opportunity\nwhere we're only",
    "start": "318740",
    "end": "320509"
  },
  {
    "text": "concerned with treating fairly\nthose who are determined",
    "start": "320510",
    "end": "323450"
  },
  {
    "text": "to be worthy of acceptance,\ni.e. dependent variable",
    "start": "323450",
    "end": "326540"
  },
  {
    "text": "is equal to 1, or\nthey're worthy of being",
    "start": "326540",
    "end": "329030"
  },
  {
    "text": "hired, worthy of being\nadmitted, and so on.",
    "start": "329030",
    "end": "332350"
  },
  {
    "text": "Equalized opportunity is not\nconcerned with rejecting people",
    "start": "332350",
    "end": "334850"
  },
  {
    "text": "fairly across protected groups.",
    "start": "334850",
    "end": "337140"
  },
  {
    "text": "So to speak, the false positive\nrates and the true positive",
    "start": "337140",
    "end": "339660"
  },
  {
    "text": "rates do not both need to be\nequal across the protected",
    "start": "339660",
    "end": "341952"
  },
  {
    "text": "categories.",
    "start": "341952",
    "end": "342830"
  },
  {
    "text": "We only need the\ntrue positive rate",
    "start": "342830",
    "end": "344300"
  },
  {
    "text": "to be equal across\nprotected categories.",
    "start": "344300",
    "end": "347409"
  },
  {
    "text": "In a way, equalized\nopportunity is",
    "start": "347410",
    "end": "349060"
  },
  {
    "text": "less interventionist\nthan equalized odds,",
    "start": "349060",
    "end": "351280"
  },
  {
    "text": "and it may be more\nachievable, depending",
    "start": "351280",
    "end": "353710"
  },
  {
    "text": "on your individual situation\nand implementation challenges.",
    "start": "353710",
    "end": "358720"
  },
  {
    "text": "In the example of\nhiring, we only",
    "start": "358720",
    "end": "361330"
  },
  {
    "text": "are concerned with individuals\nwho are worthy of being hired,",
    "start": "361330",
    "end": "364629"
  },
  {
    "text": "i.e. the actual\nqualified applicants.",
    "start": "364630",
    "end": "367000"
  },
  {
    "text": "And, out of the\nrejected applicants,",
    "start": "367000",
    "end": "368530"
  },
  {
    "text": "we may be sometimes\nrejecting unfairly.",
    "start": "368530",
    "end": "370155"
  },
  {
    "start": "370155",
    "end": "372610"
  },
  {
    "start": "372000",
    "end": "372000"
  },
  {
    "text": "Here's some review questions\nfor the last two videos.",
    "start": "372610",
    "end": "375949"
  },
  {
    "text": "What is demographic parity?",
    "start": "375950",
    "end": "377960"
  },
  {
    "text": "What is fairness\nthrough unawareness?",
    "start": "377960",
    "end": "380580"
  },
  {
    "text": "Is fairness at the group\nlevel always the best?",
    "start": "380580",
    "end": "383490"
  },
  {
    "text": "What is a confusion matrix?",
    "start": "383490",
    "end": "385914"
  },
  {
    "text": "What is the equality\nof odds criterion?",
    "start": "385915",
    "end": "387540"
  },
  {
    "text": "And when might you\nwant to use it?",
    "start": "387540",
    "end": "388957"
  },
  {
    "start": "388957",
    "end": "391150"
  },
  {
    "text": "This course is sponsored by the\nUSAID grant through MIT D-Lab.",
    "start": "391150",
    "end": "395520"
  },
  {
    "text": "And this is joint work with my\nfaculty colleagues Lily Morse",
    "start": "395520",
    "end": "398160"
  },
  {
    "text": "and Gerald Kane from Boston\ncollege and research assistant",
    "start": "398160",
    "end": "401400"
  },
  {
    "text": "Yazeed Awwad from MIT D-Lab.",
    "start": "401400",
    "end": "402669"
  },
  {
    "start": "402670",
    "end": "405158"
  },
  {
    "start": "405000",
    "end": "405000"
  },
  {
    "text": "If you would like to\nlearn more about this,",
    "start": "405158",
    "end": "406949"
  },
  {
    "text": "please consult the\nfollowing references.",
    "start": "406950",
    "end": "410040"
  },
  {
    "text": "Thank you so much for\nwatching this video.",
    "start": "410040",
    "end": "411960"
  },
  {
    "text": "We hope you find it\nuseful and you'll continue",
    "start": "411960",
    "end": "413876"
  },
  {
    "text": "watching the rest of the class.",
    "start": "413877",
    "end": "415880"
  },
  {
    "text": "[MUSIC PLAYING]",
    "start": "415880",
    "end": "419830"
  },
  {
    "start": "419830",
    "end": "426000"
  }
]