[
  {
    "start": "0",
    "end": "240000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5310"
  },
  {
    "text": "Your support will help\nMIT Open Courseware continue to offer high quality\neducational resources for free.",
    "start": "5310",
    "end": "11610"
  },
  {
    "text": "To make a donation or to\nview additional materials from hundreds of\nMIT courses, visit",
    "start": "11610",
    "end": "16670"
  },
  {
    "text": "MITopencourseware@ocw.MIT.edu. ",
    "start": "16670",
    "end": "24170"
  },
  {
    "text": "GILBERT STRANG: So I'm going to\ntalk about the gradient descent today to get to that\ncentral algorithm",
    "start": "24170",
    "end": "32579"
  },
  {
    "text": "of neural net deep\nlearning, machine learning,",
    "start": "32580",
    "end": "38190"
  },
  {
    "text": "and optimization in general. So I'm trying to\nminimize a function.",
    "start": "38190",
    "end": "43230"
  },
  {
    "text": "And that's the way you do it if\nthere are many, many variables,",
    "start": "43230",
    "end": "50400"
  },
  {
    "text": "too many to take\nsecond derivatives, then we settle for first\nderivatives of the function.",
    "start": "50400",
    "end": "56880"
  },
  {
    "text": "So I introduced,\nand you've already met the idea of gradient. But let me just be sure\nto make some comments",
    "start": "56880",
    "end": "64470"
  },
  {
    "text": "about the gradient\nand the Hessian and the role of convexity before\nwe see the big crucial example.",
    "start": "64470",
    "end": "75610"
  },
  {
    "text": "So I've kind of prepared over\nhere for this crucial example. ",
    "start": "75610",
    "end": "82010"
  },
  {
    "text": "The function is a pure\nquadratic, two unknowns, x and y, pure quadratic.",
    "start": "82010",
    "end": "90240"
  },
  {
    "text": "So every pure quadratic\nI can write in terms of a symmetric matrix s.",
    "start": "90240",
    "end": "97160"
  },
  {
    "text": "And in this case, x1 squared\nwas bx2 squared, the symmetric,",
    "start": "97160",
    "end": "102890"
  },
  {
    "text": "the matrix is just 2 by 2. It's diagonal. It's got eigenvalues 1 and\nb sitting on the diagonal.",
    "start": "102890",
    "end": "112440"
  },
  {
    "text": "I'm thinking of b as\nbeing the smaller one. So the condition\nnumber, which we'll see,",
    "start": "112440",
    "end": "120719"
  },
  {
    "text": "is all important in the question\nof the speed of convergence",
    "start": "120720",
    "end": "127230"
  },
  {
    "text": "is the ratio of the\nlargest to the smallest.",
    "start": "127230",
    "end": "133260"
  },
  {
    "text": "In this case, the largest\nis 1 the smallest is b. So that's 1 over b.",
    "start": "133260",
    "end": "139260"
  },
  {
    "text": "And when 1 over b\nis a big number, when b is a very small\nnumber, then that's",
    "start": "139260",
    "end": "146129"
  },
  {
    "text": "when we're in trouble. ",
    "start": "146130",
    "end": "151560"
  },
  {
    "text": "When the matrix is symmetric,\nthat condition number is lambda max over lambda min.",
    "start": "151560",
    "end": "157620"
  },
  {
    "text": "If I had an\nunsymmetric matrix, I would probably use sigma max\nover sigma min, of course.",
    "start": "157620",
    "end": "164360"
  },
  {
    "text": "But here, matrices\nare symmetric. We're going to\nsee something neat",
    "start": "164360",
    "end": "172170"
  },
  {
    "text": "is that we can actually take\nthe steps of steepest descent,",
    "start": "172170",
    "end": "178260"
  },
  {
    "text": "write down what\neach step gives us, and see how quickly they\nconverge to the answer.",
    "start": "178260",
    "end": "185310"
  },
  {
    "text": "And what is the answer? So I haven't put in\nany linear term here.",
    "start": "185310",
    "end": "191370"
  },
  {
    "text": "So I just have a bowl\nsitting on the origin. So of course, the minimum\npoint is x equal 0, y equals 0.",
    "start": "191370",
    "end": "198989"
  },
  {
    "text": "So the minimum point x\nstar, is 0, 0, of course.",
    "start": "198990",
    "end": "206050"
  },
  {
    "text": "So the question will be how\nquickly do we get to that one. And you will say pretty\nsmall example, not typical.",
    "start": "206050",
    "end": "213450"
  },
  {
    "text": "But the terrific\nthing is that we see everything for this example.",
    "start": "213450",
    "end": "218890"
  },
  {
    "text": "We can see the actual\nsteps of steepest descent. We can see how\nquickly they converge",
    "start": "218890",
    "end": "225600"
  },
  {
    "text": "to the x star, the\nanswer, the place",
    "start": "225600",
    "end": "230730"
  },
  {
    "text": "where this thing is a minimum. And we can begin to think\nwhat to do if it's too slow.",
    "start": "230730",
    "end": "241440"
  },
  {
    "start": "240000",
    "end": "512000"
  },
  {
    "text": "So I'll come to that example\nafter some general thoughts",
    "start": "241440",
    "end": "246930"
  },
  {
    "text": "about gradients, Hessians. So what does the\ngradient tell us?",
    "start": "246930",
    "end": "252300"
  },
  {
    "text": "So let me just take an\nexample of the gradient. ",
    "start": "252300",
    "end": "257859"
  },
  {
    "text": "Let me take a linear function,\nf of xy equals say, 2x plus 5y.",
    "start": "257860",
    "end": "263979"
  },
  {
    "text": " I just think we ought to get\ntotally familiar with these.",
    "start": "263980",
    "end": "271540"
  },
  {
    "text": "We're doing something. We're jumping into\nan important topic.",
    "start": "271540",
    "end": "278800"
  },
  {
    "text": "When I ask you\nwhat's the gradient, that's a freshman question. But let's just be sure we know\nhow to interpret the gradient,",
    "start": "278800",
    "end": "288460"
  },
  {
    "text": "how to compute\nit, what it means, how to see it geometrically.",
    "start": "288460",
    "end": "294200"
  },
  {
    "text": "So what's the gradient\nof that function? It's a function\nof two variables. So the gradient is a\nvector with two components.",
    "start": "294200",
    "end": "302110"
  },
  {
    "text": "And they are? ",
    "start": "302110",
    "end": "307540"
  },
  {
    "text": "The derivative of\nthis factor x, which is 2 and the derivative of\nthis factor y, which is 5.",
    "start": "307540",
    "end": "313320"
  },
  {
    "text": "So in this case, the\ngradient is constant. And the Hessian, which I\noften call H after Hessian,",
    "start": "313320",
    "end": "322650"
  },
  {
    "text": "or del squared F\nwould tell us we're taking the second\nderivatives, that",
    "start": "322650",
    "end": "327990"
  },
  {
    "text": "will be the second derivatives\nobviously 0 in this case.",
    "start": "327990",
    "end": "333150"
  },
  {
    "text": "So what shape is H here?",
    "start": "333150",
    "end": "338229"
  },
  {
    "text": "It's 2 by 2. Everybody recognizes 2 by\n2 is H would have the--",
    "start": "338230",
    "end": "345212"
  },
  {
    "text": "I'll take a second\nderivative of that-- sorry, the first derivative\nof that with respect to x,",
    "start": "345212",
    "end": "352090"
  },
  {
    "text": "obviously 0, the first\nderivative with respect to y, the first derivative\nof that with respect to x y.",
    "start": "352090",
    "end": "360620"
  },
  {
    "text": "Anyway, Hessian 0 for sure. So let me draw the surface.",
    "start": "360620",
    "end": "368080"
  },
  {
    "text": "So x, y, and the surface, if\nI graph F in this direction,",
    "start": "368080",
    "end": "373539"
  },
  {
    "text": "then obviously, I have a plane. And I'm at a typical point\non the plane let's say.",
    "start": "373540",
    "end": "380840"
  },
  {
    "text": "Yeah, yeah. So I'm at a point\nx, y, I should say. I'm at a point x, y. And let me put the\nplane through it.",
    "start": "380840",
    "end": "388340"
  },
  {
    "text": "So how do I interpret\nthe gradient at that particular point x, y? ",
    "start": "388340",
    "end": "395629"
  },
  {
    "text": "What does 2x plus 5y tell me? Or rather what does grad\nF tell me about movement",
    "start": "395630",
    "end": "406400"
  },
  {
    "text": "from that point x, y? Of course, the\ngradient is constant.",
    "start": "406400",
    "end": "412030"
  },
  {
    "text": "So it really didn't matter\nwhat point I'm moving from. But taking a point here.",
    "start": "412030",
    "end": "417680"
  },
  {
    "text": "So what's the deal if I move? What's the fastest way\nto go up the surface?",
    "start": "417680",
    "end": "424010"
  },
  {
    "text": "If I took the plane that\nwent through that point x, y,",
    "start": "424010",
    "end": "429110"
  },
  {
    "text": "what's the fastest way\nto climb the plane? What direction goes up fastest?",
    "start": "429110",
    "end": "434630"
  },
  {
    "text": "The gradient direction, right? The gradient direction\nis the way up. How am I going to put\nit in this picture?",
    "start": "434630",
    "end": "442699"
  },
  {
    "text": "I guess I'm thinking\nof this plane as-- so what plane? You could well ask what\nplane have I drawn?",
    "start": "442700",
    "end": "450230"
  },
  {
    "text": "Suppose I've drawn the plane\n2x plus 5y equals 0 even?",
    "start": "450230",
    "end": "459350"
  },
  {
    "text": "So I'll make it go\nthrough the arc. And I've taken a typical\npoint on that plane.",
    "start": "459350",
    "end": "464540"
  },
  {
    "text": "Now if I want to\nincrease that function, I go perpendicular to the plane.",
    "start": "464540",
    "end": "472700"
  },
  {
    "text": "If I want to stay level\nwith the function, if I wanted to stay at\n0, I stay in the plane.",
    "start": "472700",
    "end": "478620"
  },
  {
    "text": "So there are two key directions. Everybody knows this. I'm just repeating. This is the direction\nof the gradient of F out",
    "start": "478620",
    "end": "488030"
  },
  {
    "text": "of the plane, steepest upwards. This is the downwards\ndirection minus gradient",
    "start": "488030",
    "end": "493190"
  },
  {
    "text": "of F, perpendicular to\nthe plane downwards. And that line is in the plane.",
    "start": "493190",
    "end": "501800"
  },
  {
    "text": "That's part of the level set. 2x plus 5y equals 0\nwould be a level set.",
    "start": "501800",
    "end": "508070"
  },
  {
    "text": "That's my pretty\namateur picture. Just all I want to remember is\nthese words level and steepest,",
    "start": "508070",
    "end": "525130"
  },
  {
    "start": "512000",
    "end": "629000"
  },
  {
    "text": "up or down. Down with a minus sign that\nwe see in steepest descent.",
    "start": "525130",
    "end": "534610"
  },
  {
    "text": "So where in steepest descent. ",
    "start": "534610",
    "end": "543019"
  },
  {
    "text": "And what's the Hessian\ntelling me about the surface",
    "start": "543020",
    "end": "548900"
  },
  {
    "text": "if I take the matrix\nof second derivatives? So I have this surface.",
    "start": "548900",
    "end": "554680"
  },
  {
    "text": "So I have a surface\nF equal constant. ",
    "start": "554680",
    "end": "562990"
  },
  {
    "text": "That's the sort\nof level surface. So if I stay in that surface,\nthe gradient of F is 0.",
    "start": "562990",
    "end": "569529"
  },
  {
    "text": "Gradient of F is 0 in-- ",
    "start": "569530",
    "end": "576960"
  },
  {
    "text": "on-- on is a better word-- on the surface. ",
    "start": "576960",
    "end": "583330"
  },
  {
    "text": "The gradient of F\npoints perpendicular. But what about the Hessian,\nthe second derivative?",
    "start": "583330",
    "end": "598100"
  },
  {
    "text": "What is that telling\nme about that surface",
    "start": "598100",
    "end": "603430"
  },
  {
    "text": "in particular when the Hessian\nis 0 or other surfaces? What does the Hessian\ntell me about--",
    "start": "603430",
    "end": "610395"
  },
  {
    "text": " I'm thinking of the Hessian\nat a particular point.",
    "start": "610395",
    "end": "616990"
  },
  {
    "text": "So I'm getting 0 for the Hessian\nbecause the surface is flat.",
    "start": "616990",
    "end": "625580"
  },
  {
    "text": "If the surface was\nconvex upwards from--",
    "start": "625580",
    "end": "634180"
  },
  {
    "start": "629000",
    "end": "761000"
  },
  {
    "text": "if it was a convex or a graph\nof F, the Hessian would be--",
    "start": "634180",
    "end": "641774"
  },
  {
    "text": " so I just want to make\nthat connection now.",
    "start": "641775",
    "end": "648810"
  },
  {
    "text": "What's the connection between\nthe Hessian and convexity",
    "start": "648810",
    "end": "654990"
  },
  {
    "text": "of the-- the Hessian of the function\nand convexity of the function?",
    "start": "654990",
    "end": "660660"
  },
  {
    "text": "So the point is that convexity--",
    "start": "660660",
    "end": "666550"
  },
  {
    "text": "the Hessian tells me whether\nor not the surface is convex. And what is the test? AUDIENCE: [INAUDIBLE].",
    "start": "666550",
    "end": "672600"
  },
  {
    "text": "GILBERT STRANG: Positive\ndefinite or semi definite. I'm just looking for\nan excuse to write down",
    "start": "672600",
    "end": "680339"
  },
  {
    "text": "convexity and strong.",
    "start": "680340",
    "end": "686910"
  },
  {
    "text": "Do I say strict or\nstrong convexity? I've forgotten. Strict, I think.",
    "start": "686910",
    "end": "692150"
  },
  {
    "text": "Strictly convex. ",
    "start": "692150",
    "end": "698230"
  },
  {
    "text": "So convexity, the Hessian\nis positive semi-definite,",
    "start": "698230",
    "end": "705100"
  },
  {
    "text": "or which includes-- I better say that right here-- includes positive definite.",
    "start": "705100",
    "end": "712074"
  },
  {
    "start": "712074",
    "end": "718380"
  },
  {
    "text": "If I'm looking for\na strict convexity, then I must require\npositive definite. H is positive definite.",
    "start": "718380",
    "end": "725863"
  },
  {
    "text": " Semi-definite won't do.",
    "start": "725863",
    "end": "732300"
  },
  {
    "text": "So semi-definite for convex. So that in fact,\nthe linear function",
    "start": "732300",
    "end": "738540"
  },
  {
    "text": "is convex, but not\nstrictly convex. Strictly means it\nreally bends upwards.",
    "start": "738540",
    "end": "745160"
  },
  {
    "text": "The Hessian is\npositive definite. The curvatures are positive.",
    "start": "745160",
    "end": "751120"
  },
  {
    "text": "So this would include\nlinear functions, and that would not\ninclude linear function.",
    "start": "751120",
    "end": "757459"
  },
  {
    "text": "They're not strictly convex. Good, good, good.",
    "start": "757460",
    "end": "762510"
  },
  {
    "start": "761000",
    "end": "1073000"
  },
  {
    "text": "Some examples-- OK, the\nnumber one example, of course, is the one we're\ntalking about over here.",
    "start": "762510",
    "end": "769410"
  },
  {
    "text": "So examples f of x equal\n1/2 x transpose Sx.",
    "start": "769410",
    "end": "779839"
  },
  {
    "text": " And of course, I could\nhave linear terms",
    "start": "779840",
    "end": "785660"
  },
  {
    "text": "minus a transpose\nx, a linear term. And I could have a constant.",
    "start": "785660",
    "end": "792770"
  },
  {
    "text": "OK. ",
    "start": "792770",
    "end": "798790"
  },
  {
    "text": "So this function\nis strictly convex when S is positive\ndefinite, because H is now",
    "start": "798790",
    "end": "808130"
  },
  {
    "text": "S for that function,\nfor that function",
    "start": "808130",
    "end": "813800"
  },
  {
    "text": "H. Usually H, the Hessian is\nvarying from point to point.",
    "start": "813800",
    "end": "819170"
  },
  {
    "text": "The nice thing about a pure\nquadratic is its constant. It's the same S at all points.",
    "start": "819170",
    "end": "826550"
  },
  {
    "text": "Let me just ask you-- so that's a convex function.",
    "start": "826550",
    "end": "833370"
  },
  {
    "text": "And what's its minimum? What's the gradient,\nfirst of all? What's the gradient of that?",
    "start": "833370",
    "end": "839050"
  },
  {
    "text": " I'm asking really\nfor differentiating",
    "start": "839050",
    "end": "849570"
  },
  {
    "text": "thinking in vector, doing all\nn derivatives at once here. I'm asking for the whole\nvector of first derivatives.",
    "start": "849570",
    "end": "859840"
  },
  {
    "text": "Because here I'm giving\nyou the whole function with x for vector x.",
    "start": "859840",
    "end": "868149"
  },
  {
    "text": "Of course, we could\ntake n to be 1. And then we would\nsee that if n was 1,",
    "start": "868150",
    "end": "873760"
  },
  {
    "text": "this would just be Sx\nsquared, half Sx squared.",
    "start": "873760",
    "end": "879880"
  },
  {
    "text": "And the derivative of\na half Sx squared-- let me just put that\nover here so we're",
    "start": "879880",
    "end": "886029"
  },
  {
    "text": "sure to get it right--\nhalf of Sx squared. This is in the n equal 1 case.",
    "start": "886030",
    "end": "891490"
  },
  {
    "text": "And the derivative\nis obviously Sx. And that's what it is here, Sx. ",
    "start": "891490",
    "end": "906490"
  },
  {
    "text": "It's obviously\nsimple, but if you haven't thought\nabout that line, it's",
    "start": "906490",
    "end": "914190"
  },
  {
    "text": "asking for all the\nfirst derivatives of that quadratic function.",
    "start": "914190",
    "end": "920850"
  },
  {
    "text": "Oh! It's not-- What do I\nhave to include now here?",
    "start": "920850",
    "end": "927940"
  },
  {
    "text": "That's not right as it stands\nfor the function that's written above it. What's the right gradient?",
    "start": "927940",
    "end": "933600"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. GILBERT STRANG: Minus a, thanks. Because the linear function,\nits partial derivatives",
    "start": "933600",
    "end": "941439"
  },
  {
    "text": "are obviously just\nthe components of a. And the Hessian H is S,\nderivatives of that guy.",
    "start": "941440",
    "end": "956029"
  },
  {
    "text": "OK. Good. Good, good, good. And the minimum value-- we\nmight as well-- oh yeah!",
    "start": "956030",
    "end": "962519"
  },
  {
    "text": "What's the right words\nfor a minimum value?",
    "start": "962520",
    "end": "967820"
  },
  {
    "text": "No, I'm sorry. The right word is\nminimum value like f min.",
    "start": "967820",
    "end": "974430"
  },
  {
    "text": "So I want to compute f min. Well, first I have to figure out\nwhere is that minimum reached?",
    "start": "974430",
    "end": "983930"
  },
  {
    "text": "And what's the answer to that? We're putting everything on\nthe board for this simple case.",
    "start": "983930",
    "end": "990840"
  },
  {
    "text": "The minimum of f\nof f of f of x--",
    "start": "990840",
    "end": "998990"
  },
  {
    "text": "remember, it's x is--\nwe're in n dimensions-- is at x equal what?",
    "start": "998990",
    "end": "1009910"
  },
  {
    "text": "Well, the minimum is\nwhere the gradient is 0. ",
    "start": "1009910",
    "end": "1015460"
  },
  {
    "text": "So what's the minimizing x? S inverse a, thanks.",
    "start": "1015460",
    "end": "1021115"
  },
  {
    "start": "1021115",
    "end": "1028179"
  },
  {
    "text": "Sorry. That's not right. It's here that I\nmeant to write it.",
    "start": "1028180",
    "end": "1034020"
  },
  {
    "text": " Really, my whole point\nfor this little moment",
    "start": "1034020",
    "end": "1040550"
  },
  {
    "text": "is to be sure that\nwe keep straight what I mean by the place where\nthe minimum is reached",
    "start": "1040550",
    "end": "1047780"
  },
  {
    "text": "and the minimum value. Those are two different things. ",
    "start": "1047780",
    "end": "1054330"
  },
  {
    "text": "So the minimum is\nreached at S inverse a, because that's obviously\nwhere the gradient is 0.",
    "start": "1054330",
    "end": "1060270"
  },
  {
    "text": "It's the solution to Sx equal a. And what I was going to ask\nyou is what's the right word--",
    "start": "1060270",
    "end": "1068970"
  },
  {
    "text": "well, sort of word, made up\nword-- for this point x star",
    "start": "1068970",
    "end": "1076440"
  },
  {
    "start": "1073000",
    "end": "1156000"
  },
  {
    "text": "where the minimum is reached? So it's not the minimum value. It's the point\nwhere it's reached.",
    "start": "1076440",
    "end": "1081720"
  },
  {
    "text": "And that's called-- the\nnotation for that point is AUDIENCE: Arg min.",
    "start": "1081720",
    "end": "1086991"
  },
  {
    "text": "GILBERT STRANG: Arg min, thanks. Arg min of my function.",
    "start": "1086991",
    "end": "1096620"
  },
  {
    "text": "And that means the place-- the point where f equals f min.",
    "start": "1096620",
    "end": "1104918"
  },
  {
    "text": " I haven't said yet what\nthe minimum value is.",
    "start": "1104918",
    "end": "1110600"
  },
  {
    "text": "This tells us the point. And that's usually what\nwe're interested in. We're, to tell the\ntruth, not that",
    "start": "1110600",
    "end": "1116540"
  },
  {
    "text": "interested in a typical example\nand what the minimum value is as much as where is it?",
    "start": "1116540",
    "end": "1123740"
  },
  {
    "text": "Where do we reach that thing? And of course, so this is x min.",
    "start": "1123740",
    "end": "1130490"
  },
  {
    "text": "This is then arg min\nof my function f.",
    "start": "1130490",
    "end": "1140010"
  },
  {
    "text": "That's the point. And it happens to\nbe in this case, the minimum value is actually 0.",
    "start": "1140010",
    "end": "1146520"
  },
  {
    "text": " Because there's no linear\nterm a transpose x.",
    "start": "1146520",
    "end": "1155190"
  },
  {
    "text": " Why am I talking about arg\nmin when you've all seen it?",
    "start": "1155190",
    "end": "1166269"
  },
  {
    "start": "1156000",
    "end": "1420000"
  },
  {
    "text": "I guess I think that\nsomebody could just be reading this stuff,\nfor example, learning",
    "start": "1166270",
    "end": "1174750"
  },
  {
    "text": "about neural net, and run\ninto this expression arg min",
    "start": "1174750",
    "end": "1180740"
  },
  {
    "text": "and think what's that? So it's maybe a right\ntime to say what it is.",
    "start": "1180740",
    "end": "1187620"
  },
  {
    "text": "It's the point where\nthe minimum is reached. ",
    "start": "1187620",
    "end": "1192929"
  },
  {
    "text": "Why those words, by the way? Well, arg isn't much of a word. It sounds like you're\ngetting strangled.",
    "start": "1192930",
    "end": "1200160"
  },
  {
    "text": "But it's sort of short. I assume it's short.",
    "start": "1200160",
    "end": "1205440"
  },
  {
    "text": "Nobody ever told me this. I assume it's\nshort for argument. The word argument is a kind of\nlong word for the value of x.",
    "start": "1205440",
    "end": "1215159"
  },
  {
    "text": "If I have a function\nf of x, f, I call it function and x is the\nargument of that function.",
    "start": "1215160",
    "end": "1223770"
  },
  {
    "text": "You might more often\nsee the word variable. But argument-- and I'm assuming\nthat's what that refers to,",
    "start": "1223770",
    "end": "1231240"
  },
  {
    "text": "it's the argument that\nminimizes the function. OK, good.",
    "start": "1231240",
    "end": "1237180"
  },
  {
    "text": "And here it is, S inverse a. Now but just by the\nway, what is f min?",
    "start": "1237180",
    "end": "1243179"
  },
  {
    "text": "Do you know the\nminimum of a quadratic? I mean, this is the fundamental\nminimization question,",
    "start": "1243180",
    "end": "1249750"
  },
  {
    "text": "to minimize a quadratic. Electrical engineering, a\nquadratic regulator problem",
    "start": "1249750",
    "end": "1256410"
  },
  {
    "text": "is the simplest problem there. There could be constraints. And we'll see it with\nconstraints included.",
    "start": "1256410",
    "end": "1263070"
  },
  {
    "text": "But right now, no\nconstraints at all. We're just looking at\nthe function f of x.",
    "start": "1263070",
    "end": "1268560"
  },
  {
    "text": " Let me to remove the\nb, because that just",
    "start": "1268560",
    "end": "1275040"
  },
  {
    "text": "shifts the function by b. If I erase that, just\nto say it didn't matter.",
    "start": "1275040",
    "end": "1282710"
  },
  {
    "text": "It's really that function. So that function\nactually goes through 0.",
    "start": "1282710",
    "end": "1288030"
  },
  {
    "text": "As it is, when x is\n0, we obviously get 0. But it's still on its\nway down, so to speak.",
    "start": "1288030",
    "end": "1295950"
  },
  {
    "text": "It's on its way down to\nthis point, S inverse a. That's where it bottoms out.",
    "start": "1295950",
    "end": "1302490"
  },
  {
    "text": "And when it bottoms out,\nwhat do you get for f? One thing I know, it's\ngoing to be negative",
    "start": "1302490",
    "end": "1309660"
  },
  {
    "text": "because it passed through 0,\nand it was on its way below 0. So let's just figure\nout what that f min is.",
    "start": "1309660",
    "end": "1317220"
  },
  {
    "text": "So I have a half. I'm just going to plug in S\ninverse a, the bottom point",
    "start": "1317220",
    "end": "1325560"
  },
  {
    "text": "into the function, and see\nwhere the surface bottoms out",
    "start": "1325560",
    "end": "1331860"
  },
  {
    "text": "and at what level\nit bottoms out. So I have a half.",
    "start": "1331860",
    "end": "1337200"
  },
  {
    "text": "So that's S inverse a is\na transpose S inverse.",
    "start": "1337200",
    "end": "1343320"
  },
  {
    "text": "S symmetric, so I'll just\nwrite this inverse transpose. S, S inverse a from\nthe quadratic term,",
    "start": "1343320",
    "end": "1353519"
  },
  {
    "text": "minus a transpose. And x is S inverse a.",
    "start": "1353520",
    "end": "1360030"
  },
  {
    "text": "Have you done this calculation? It just doesn't\nhurt to repeat it.",
    "start": "1360030",
    "end": "1366240"
  },
  {
    "text": "So I've plugged in S inverse\na there, there, and there.",
    "start": "1366240",
    "end": "1373530"
  },
  {
    "text": "OK, what have I got? Well, S inverse\ncancels S. So I have",
    "start": "1373530",
    "end": "1378630"
  },
  {
    "text": "a half of a transpose\nS inverse a minus 1 of a transpose inverse a.",
    "start": "1378630",
    "end": "1384150"
  },
  {
    "text": "So I get finally\nnegative a half. Half of it minus one of it\nof a transpose S inverse a.",
    "start": "1384150",
    "end": "1395850"
  },
  {
    "text": "Sorry, that's not brilliant\nuse of the blackboard to squeeze that in there.",
    "start": "1395850",
    "end": "1401370"
  },
  {
    "text": "But that's easily repeatable.",
    "start": "1401370",
    "end": "1406380"
  },
  {
    "text": "OK, good. So that's what a quadratic bowl,\na perfect quadratic problem",
    "start": "1406380",
    "end": "1414560"
  },
  {
    "text": "minimizes to that's\nits lowest level.",
    "start": "1414560",
    "end": "1420390"
  },
  {
    "start": "1420000",
    "end": "1696000"
  },
  {
    "text": "Ooh, I wanted to mention\none other function, because I'm going to speak\nmostly about quadratics,",
    "start": "1420390",
    "end": "1428480"
  },
  {
    "text": "but obviously,\nthe whole point is that it's the convexity that's\nreally making things work.",
    "start": "1428480",
    "end": "1436520"
  },
  {
    "text": "So here, let me just put here,\na remarkable convex function.",
    "start": "1436520",
    "end": "1447190"
  },
  {
    "text": " And the notes tell what's the\ngradient of this function.",
    "start": "1447190",
    "end": "1460690"
  },
  {
    "text": "They don't actually go\nas far as the Hessian. Proving that this function I'm\ngoing to write down is convex,",
    "start": "1460690",
    "end": "1472780"
  },
  {
    "text": "it takes a little thinking. But it's a fantastic function.",
    "start": "1472780",
    "end": "1477809"
  },
  {
    "text": "You would never\nsort of imagine it if you didn't see it sometime.",
    "start": "1477810",
    "end": "1484110"
  },
  {
    "text": "So it's going to be a function\nof a matrix, a function of-- those are n squared\nvariables, x, i, j.",
    "start": "1484110",
    "end": "1498630"
  },
  {
    "text": "So it's a function\nof many variables. And here is this function. It's you take the\ndeterminant of the matrix.",
    "start": "1498630",
    "end": "1507299"
  },
  {
    "text": "That's clearly a function of\nall the n squared variables. Then you take the log\nof the determinant",
    "start": "1507300",
    "end": "1515810"
  },
  {
    "text": "and put in a minus sign\nbecause we want convex.",
    "start": "1515810",
    "end": "1521840"
  },
  {
    "text": "That turns out to be\na convex function. And even to just check that\nfor 2 by 2 well, for 2 by 2",
    "start": "1521840",
    "end": "1529250"
  },
  {
    "text": "you have four variables,\nbecause it's a 2 by 2 matrix. We could maybe check it\nfor a symmetric matrix.",
    "start": "1529250",
    "end": "1535159"
  },
  {
    "text": "I move it down to\nthree variables. But I'd be glad anybody\nwho's ambitious to see",
    "start": "1535160",
    "end": "1545540"
  },
  {
    "text": "why that log determinant\nis a remarkable function.",
    "start": "1545540",
    "end": "1551450"
  },
  {
    "text": "And let me see.  So the gradient of that\nthing is also amazing.",
    "start": "1551450",
    "end": "1561860"
  },
  {
    "text": "The gradient of that function-- I'm going to peek so I don't\nwrite the wrong fact here.",
    "start": "1561860",
    "end": "1571610"
  },
  {
    "text": " So the partial derivative\nof that function",
    "start": "1571610",
    "end": "1579800"
  },
  {
    "text": "are the entries of-- these are the entries\nof a, a inverse.",
    "start": "1579800",
    "end": "1586220"
  },
  {
    "text": "That's the-- of x inverse. ",
    "start": "1586220",
    "end": "1598360"
  },
  {
    "text": "That's like, wow. Where did that come from? It might be minus the\nentries, of course.",
    "start": "1598360",
    "end": "1605410"
  },
  {
    "text": "Yeah, yeah, yeah. So we've got n\nsquared function--",
    "start": "1605410",
    "end": "1613240"
  },
  {
    "text": "what is a typical\nentry in x inverse? What does a typical\nx inverse i, j?",
    "start": "1613240",
    "end": "1622090"
  },
  {
    "text": "Just to remember\nthat bit of pretty old fashioned linear\nalgebra, the entry",
    "start": "1622090",
    "end": "1629910"
  },
  {
    "text": "is of the inverse matrix,\nI'm sure to divide by what?",
    "start": "1629910",
    "end": "1634980"
  },
  {
    "text": "The determinant, that's\nthe one thing we know. ",
    "start": "1634980",
    "end": "1641720"
  },
  {
    "text": "And that's the reason\nwe take the log, because when you take\nderivatives of a log,",
    "start": "1641720",
    "end": "1647840"
  },
  {
    "text": "that will put determinant\nof x in the denominator. And then the numerator\nwill be the derivatives",
    "start": "1647840",
    "end": "1653990"
  },
  {
    "text": "of the determinant of x. Oh! Can we get any idea what are the\nderivatives of the determinant?",
    "start": "1653990",
    "end": "1661640"
  },
  {
    "text": "Oh my god. How did I never get into this? So are you with me so far?",
    "start": "1661640",
    "end": "1670090"
  },
  {
    "text": "This is going to be\nderivatives of determinant, the strength of all\nthese variables divided",
    "start": "1670090",
    "end": "1678020"
  },
  {
    "text": "by the determinant, because\nthat's what the log achieved. So when I take the derivative\nof the log of something,",
    "start": "1678020",
    "end": "1684560"
  },
  {
    "text": "that chain rule says take the\nderivative of that something",
    "start": "1684560",
    "end": "1692060"
  },
  {
    "text": "divide by the function\ndeterminant of x. So what's the derivative of\nthe determinant of a matrix",
    "start": "1692060",
    "end": "1700710"
  },
  {
    "start": "1696000",
    "end": "2046000"
  },
  {
    "text": "with respect to its 1, 1 entry? Yeah, sure. This is crazy. But it's crazy to be doing this.",
    "start": "1700710",
    "end": "1706490"
  },
  {
    "text": "But it's healthy. OK. ",
    "start": "1706490",
    "end": "1711960"
  },
  {
    "text": "So I have a matrix x, da,\nda, da, x, x, 1, 1, x, 1n,",
    "start": "1711960",
    "end": "1718111"
  },
  {
    "text": "et cetera, xn, 1, x, n, n.",
    "start": "1718111",
    "end": "1723399"
  },
  {
    "text": "OK. And what am I looking for? I'm looking for that for\nthe derivatives of the--",
    "start": "1723400",
    "end": "1732159"
  },
  {
    "text": "do I want the derivatives\nof the determinant? Yes.",
    "start": "1732160",
    "end": "1737550"
  },
  {
    "text": "So what's the derivative of x\nof the determinant with respect",
    "start": "1737550",
    "end": "1745470"
  },
  {
    "text": "to the first equals what? ",
    "start": "1745470",
    "end": "1753780"
  },
  {
    "text": "How can I figure out? So what's this asking me to do? It's asking me to change x,\n1, 1 by delta x and see what's",
    "start": "1753780",
    "end": "1762790"
  },
  {
    "text": "the change in the determinant. That's what derivatives are.",
    "start": "1762790",
    "end": "1768220"
  },
  {
    "text": "Change x, 1, 1 a little bit. How much did the\ndeterminant change? ",
    "start": "1768220",
    "end": "1776150"
  },
  {
    "text": "What has the determinant\nof the whole matrix got to do with x, 1, 1?",
    "start": "1776150",
    "end": "1782850"
  },
  {
    "text": "You remember that there is\na formula for determinants. So I need that fact.",
    "start": "1782850",
    "end": "1789160"
  },
  {
    "text": "The determinant of x is\nx, 1, 1 times something.",
    "start": "1789160",
    "end": "1795600"
  },
  {
    "text": "Is that something that\nI really want to know? Plus x, 1, 2 times\nother something plus",
    "start": "1795600",
    "end": "1801870"
  },
  {
    "text": "say, along the first row\ntimes another something. ",
    "start": "1801870",
    "end": "1809340"
  },
  {
    "text": "What are these\nfactors that multiply",
    "start": "1809340",
    "end": "1815970"
  },
  {
    "text": "the x's to give the determinant? What [INAUDIBLE] a\nlinear combination",
    "start": "1815970",
    "end": "1822520"
  },
  {
    "text": "of the first row time certain\nfactors gives the determinant? And how do I know that\nthere will be such factors,",
    "start": "1822520",
    "end": "1830520"
  },
  {
    "text": "because the fundamental\nproperty of the determinant is that it's linear in row 1 if\nI don't mess with other rows.",
    "start": "1830520",
    "end": "1839280"
  },
  {
    "text": "It's a linear function of row 1. So it has a form x,\n1, 1 times something.",
    "start": "1839280",
    "end": "1846510"
  },
  {
    "text": "And what is something? AUDIENCE: [INAUDIBLE]. GILBERT STRANG: The\ndeterminant of this.",
    "start": "1846510",
    "end": "1852300"
  },
  {
    "text": "So what does x, 1, 1 multiply\nwhen you compute determinants? X, 1, 1 will not multiply\nany other guys in its row,",
    "start": "1852300",
    "end": "1860280"
  },
  {
    "text": "because you're never\nmultiplying two x's in the same row\nor the same column.",
    "start": "1860280",
    "end": "1866280"
  },
  {
    "text": "What x, 1, 1 is\nmultiplying all these guys. And in fact, it turns out\nto be is the determinant.",
    "start": "1866280",
    "end": "1875039"
  },
  {
    "text": "And what is this called? That one smaller determinant\nthat I get by throwing away",
    "start": "1875040",
    "end": "1882930"
  },
  {
    "text": "the first row and first column? It's called a-- Minor is good.",
    "start": "1882930",
    "end": "1888880"
  },
  {
    "text": "Yes, minor is good. I was saying there are two\nwords that can be used, minor and co-factor.",
    "start": "1888880",
    "end": "1896890"
  },
  {
    "start": "1896890",
    "end": "1902860"
  },
  {
    "text": "Yeah. And what is it? I mean, how do I compute it? What is the number? This is a number.",
    "start": "1902860",
    "end": "1908075"
  },
  {
    "text": " It's just a number. ",
    "start": "1908075",
    "end": "1916880"
  },
  {
    "text": "Maybe I think of the minor\nas this determinant-- Ah! Let me cancel that.",
    "start": "1916880",
    "end": "1923480"
  },
  {
    "text": "Maybe I think of the\nminor as this smaller matrix, and the\nco-factor, which is",
    "start": "1923480",
    "end": "1928789"
  },
  {
    "text": "the determinant of the minor. ",
    "start": "1928790",
    "end": "1935180"
  },
  {
    "text": "And there is a plus or minus. Everything about\ndeterminants, there's",
    "start": "1935180",
    "end": "1940250"
  },
  {
    "text": "a there's a plus or\nminus choice to be made. And we're not going\nto worry about that.",
    "start": "1940250",
    "end": "1947600"
  },
  {
    "text": "But so anyway, so\nit's the co-factor.",
    "start": "1947600",
    "end": "1953325"
  },
  {
    "text": "Let me call it C, 1, 1.  And so that's the formula\nfor a determinant.",
    "start": "1953325",
    "end": "1962690"
  },
  {
    "text": "That's the co-factor\nexpansion of a determinant. ",
    "start": "1962690",
    "end": "1974230"
  },
  {
    "text": "OK. And that will connect\nback to this amazing fact",
    "start": "1974230",
    "end": "1979400"
  },
  {
    "text": "that the gradient is the\nentries of x inverse, because the inverse is the ratio\nof co-factor to determinant.",
    "start": "1979400",
    "end": "1987720"
  },
  {
    "text": "So x inverse 1, 1 is that\nco-factor over the determinant.",
    "start": "1987720",
    "end": "1995772"
  },
  {
    "text": " Yeah. So that's where\nthis all comes from.",
    "start": "1995772",
    "end": "2002529"
  },
  {
    "text": "Anyway, I'm just mentioning that\nas a very interesting example",
    "start": "2002530",
    "end": "2012670"
  },
  {
    "text": "of a convex function. OK. I'll leave that.",
    "start": "2012670",
    "end": "2017950"
  },
  {
    "text": "That's just for like, education. OK.",
    "start": "2017950",
    "end": "2023080"
  },
  {
    "text": "Now I'm ready to go to\nwork on gradient descent.",
    "start": "2023080",
    "end": "2028510"
  },
  {
    "text": "So actually, the rest of\nthis class and Friday's class about gradient descent are very\nfundamental parts of 18.065.",
    "start": "2028510",
    "end": "2039310"
  },
  {
    "text": "And that will be\none of our examples. And then the general case here.",
    "start": "2039310",
    "end": "2046650"
  },
  {
    "start": "2046000",
    "end": "2968000"
  },
  {
    "text": "So I'm using this. It would be interesting\nto minimize that thing,",
    "start": "2046650",
    "end": "2053669"
  },
  {
    "text": "but we're not going there. Let's hide it, so we\ndon't see it again.",
    "start": "2053670",
    "end": "2060480"
  },
  {
    "text": "And I'll work with that example. ",
    "start": "2060480",
    "end": "2066429"
  },
  {
    "text": "So here's gradient descent. ",
    "start": "2066429",
    "end": "2077770"
  },
  {
    "text": "Is xk plus 1 is xk\nminus Sk the step size",
    "start": "2077770",
    "end": "2085030"
  },
  {
    "text": "times the gradient of f at xk. ",
    "start": "2085030",
    "end": "2092922"
  },
  {
    "text": "So the only thing\nleft that requires us to input some decision making\nis a step size, the learning",
    "start": "2092922",
    "end": "2101570"
  },
  {
    "text": "rate. We can take it as constant. If we take too big\na learning rate,",
    "start": "2101570",
    "end": "2109170"
  },
  {
    "text": "the thing will oscillate\nall over the place and it's a disaster.",
    "start": "2109170",
    "end": "2116130"
  },
  {
    "text": "If we take too small a\nlearning rate, too small steps, what's the matter with that?",
    "start": "2116130",
    "end": "2122600"
  },
  {
    "text": "Takes too long. Takes too long. So the problem is to\nget it just right.",
    "start": "2122600",
    "end": "2130400"
  },
  {
    "text": "And one way that you\ncould say get it right would be to think of optimize.",
    "start": "2130400",
    "end": "2137030"
  },
  {
    "text": "Choose the optimal Sk. Of course, that takes longer\nthan just deciding an Sk",
    "start": "2137030",
    "end": "2143450"
  },
  {
    "text": "in advance, which\nis what people do. So I'll tell you what people\ndo is on really big problems is",
    "start": "2143450",
    "end": "2151760"
  },
  {
    "text": "take an Sk-- estimate a suitable Sk, and\nthen go with it for a while.",
    "start": "2151760",
    "end": "2157520"
  },
  {
    "text": "And then look back to\nsee if it was too big,",
    "start": "2157520",
    "end": "2162830"
  },
  {
    "text": "they'll see oscillations. It'll be bouncing\nall over the place.",
    "start": "2162830",
    "end": "2169220"
  },
  {
    "text": "Or of course, an\nexact line search-- ",
    "start": "2169220",
    "end": "2176730"
  },
  {
    "text": "so you see that this\nexpression often. The exact line search choose\nSk to make my function",
    "start": "2176730",
    "end": "2190810"
  },
  {
    "text": "f at xk plus 1 a minimum on\nthe line, on the search line,",
    "start": "2190810",
    "end": "2204020"
  },
  {
    "text": "a minimum in the\nsearch direction. ",
    "start": "2204020",
    "end": "2214175"
  },
  {
    "text": "The search direction is\ngiven by the gradient. That's the direction\nwe're moving.",
    "start": "2214175",
    "end": "2219770"
  },
  {
    "text": "This is the distance\nwe're moving, or measure of the\ndistance we're moving.",
    "start": "2219770",
    "end": "2225440"
  },
  {
    "text": "And an exact search would\nbe to go along there. If I have a convex function,\nthen as I move along this line,",
    "start": "2225440",
    "end": "2234110"
  },
  {
    "text": "as I increase Sk, I'll see\nthe function start down,",
    "start": "2234110",
    "end": "2239350"
  },
  {
    "text": "because the gradient,\nnegative gradient means down.",
    "start": "2239350",
    "end": "2245380"
  },
  {
    "text": "But at some point\nit'll turn up again. And an exact line search would\nfind that point and stop there.",
    "start": "2245380",
    "end": "2253220"
  },
  {
    "text": " That doesn't mean we would--",
    "start": "2253220",
    "end": "2258860"
  },
  {
    "text": "we will see in\nthis example where we will do exact line searches\nthat for a small value of b,",
    "start": "2258860",
    "end": "2266960"
  },
  {
    "text": "it's extremely slow, that\nthe condition number controls the speed.",
    "start": "2266960",
    "end": "2272660"
  },
  {
    "text": "That's really what\nmy message will be just in these last\nminutes and next time",
    "start": "2272660",
    "end": "2279050"
  },
  {
    "text": "the sort of key lecture\non gradient descent. So an exact line\nsearch would be that.",
    "start": "2279050",
    "end": "2286670"
  },
  {
    "text": "So what a backtracking\nline search-- ",
    "start": "2286670",
    "end": "2295880"
  },
  {
    "text": "backtracking would be\ntake a fixed S like one.",
    "start": "2295880",
    "end": "2304670"
  },
  {
    "text": "And then be prepared\nto come backwards.",
    "start": "2304670",
    "end": "2312290"
  },
  {
    "text": "Cut back by half. See what you get at that point. Cut back by half of that to a\nquarter of the original step.",
    "start": "2312290",
    "end": "2320180"
  },
  {
    "text": "See what that is.  So the full step might\nhave taken you back",
    "start": "2320180",
    "end": "2328970"
  },
  {
    "text": "to the upward sweep. Halfway forward it might\nstill be on the upward sweep.",
    "start": "2328970",
    "end": "2335420"
  },
  {
    "text": "Might be too much, but so\nbacktracking cuts the step size",
    "start": "2335420",
    "end": "2340760"
  },
  {
    "text": "in pieces and checks until it-- ",
    "start": "2340760",
    "end": "2348440"
  },
  {
    "text": "So S0, half of\nS0, quarter of S0, or obviously, a different\nparameter, aS0, a squared S0,",
    "start": "2348440",
    "end": "2358250"
  },
  {
    "text": "and so on until you're\nsatisfied with that step.",
    "start": "2358250",
    "end": "2365720"
  },
  {
    "text": "And there are of course,\nmany, many refinements. We're talking about\nthe big algorithm",
    "start": "2365720",
    "end": "2371810"
  },
  {
    "text": "here that everybody has,\ndepending on their function,",
    "start": "2371810",
    "end": "2380260"
  },
  {
    "text": "has different experiences with. So here's my\nfundamental question.",
    "start": "2380260",
    "end": "2386670"
  },
  {
    "text": " Let's think of an\nexact line search.",
    "start": "2386670",
    "end": "2393610"
  },
  {
    "text": "How much does that\nreduce the function? How much does that\nreduce the function?",
    "start": "2393610",
    "end": "2400400"
  },
  {
    "text": "So that's really what the\nbounds that I want are. How much does that\nreduce the function?",
    "start": "2400400",
    "end": "2408440"
  },
  {
    "text": "And we'll see that the reduction\ninvolves the condition number,",
    "start": "2408440",
    "end": "2424319"
  },
  {
    "text": "m over M. So why don't I\nturn to the example first?",
    "start": "2424320",
    "end": "2432730"
  },
  {
    "text": "And then where we\nknow exact answers. That gives us a\nbasis for comparison.",
    "start": "2432730",
    "end": "2439980"
  },
  {
    "text": "And then our math\ngoal is prove--",
    "start": "2439980",
    "end": "2446150"
  },
  {
    "text": "get S dead bounds\non the size of f that match what we see\nexactly in that example",
    "start": "2446150",
    "end": "2455330"
  },
  {
    "text": "where we know everything. We know the gradient.",
    "start": "2455330",
    "end": "2461510"
  },
  {
    "text": "We know the Hessian. It's that matrix. We know the condition number. So what happens if\nI start at a point",
    "start": "2461510",
    "end": "2468440"
  },
  {
    "text": "x0 y0 that's on my surface?",
    "start": "2468440",
    "end": "2475105"
  },
  {
    "text": " Sorry.",
    "start": "2475105",
    "end": "2480230"
  },
  {
    "text": "What do I want to do here? Yeah. I take a point, x0\ny0 and I iterate.",
    "start": "2480230",
    "end": "2491080"
  },
  {
    "text": " So the new xy k plus\n1 is xyk minus the S,",
    "start": "2491080",
    "end": "2514040"
  },
  {
    "text": "which I can compute\ntimes the gradient of f. So I'm going to\nput in gradient f. What is the gradient here?",
    "start": "2514040",
    "end": "2520030"
  },
  {
    "text": " The derivative is\nwe expect to x.",
    "start": "2520030",
    "end": "2525790"
  },
  {
    "text": "So I have a 2xk and 2by.",
    "start": "2525790",
    "end": "2531970"
  },
  {
    "text": " And this is the step size.",
    "start": "2531970",
    "end": "2538244"
  },
  {
    "text": " And for this small\nproblem where we're",
    "start": "2538244",
    "end": "2545450"
  },
  {
    "text": "going to get such\na revealing answer, I'm going to choose\nexact line search. I'm going to choose the best xk.",
    "start": "2545450",
    "end": "2551240"
  },
  {
    "text": " And what's the answer? So I just want to tell you\nwhat the iterations are",
    "start": "2551240",
    "end": "2559500"
  },
  {
    "text": "for that particular\nfunction starting at x0 y0. ",
    "start": "2559500",
    "end": "2566080"
  },
  {
    "text": "So let me put start x0 y0.",
    "start": "2566080",
    "end": "2571460"
  },
  {
    "text": " And I haven't done this\ncalculation myself.",
    "start": "2571460",
    "end": "2576790"
  },
  {
    "text": "It's taken from the book by\nSteven Boyd and Vandenberghe called Convex Optimization.",
    "start": "2576790",
    "end": "2583240"
  },
  {
    "text": "Of course, they weren't the\nfirst to do this either. But I'm happy to mention that\nbook Convex Optimization.",
    "start": "2583240",
    "end": "2591580"
  },
  {
    "text": "And Steven Boyd will be\non campus this spring actually, in April\nfor three lectures.",
    "start": "2591580",
    "end": "2598180"
  },
  {
    "text": "This is April, maybe. Yeah, OK. So it's this month in\ntwo or three weeks.",
    "start": "2598180",
    "end": "2604400"
  },
  {
    "text": "And I'll tell you about that. So here are the xk's and the\nyk's and the f and the function",
    "start": "2604400",
    "end": "2614820"
  },
  {
    "text": "values. ",
    "start": "2614820",
    "end": "2620190"
  },
  {
    "text": "So where am I going to start?  Yeah.",
    "start": "2620190",
    "end": "2625440"
  },
  {
    "text": "So I'm starting from the\npoint x0 y0 equal b1.",
    "start": "2625440",
    "end": "2630480"
  },
  {
    "text": "Turns out that will make our\nformulas very convenient, x0 y0 equals b1.",
    "start": "2630480",
    "end": "2637500"
  },
  {
    "text": "Good. So OK. So xk is b times the key\nratio b minus 1 over b plus 1",
    "start": "2637500",
    "end": "2649260"
  },
  {
    "text": "to the kth power. And yk happens to be--",
    "start": "2649260",
    "end": "2655335"
  },
  {
    "text": " it has this same ratio.",
    "start": "2655335",
    "end": "2664020"
  },
  {
    "text": "And my function f has\nthe same ratio too.",
    "start": "2664020",
    "end": "2669600"
  },
  {
    "text": "This is fk. It has that same\nratio 1 minus b over 1 plus b to the kth times f0.",
    "start": "2669600",
    "end": "2679710"
  },
  {
    "text": "That's the beautiful\nformula that we're",
    "start": "2679710",
    "end": "2691160"
  },
  {
    "text": "going to take as the\nbest example possible. Let's just see. If k equals 0, I have xk equal\nb yk equal 1 b starting at b1.",
    "start": "2691160",
    "end": "2704799"
  },
  {
    "text": "And that tells me the rate\nof decrease of the function. It's this same ratio.",
    "start": "2704800",
    "end": "2711680"
  },
  {
    "text": "So what am I learning\nfrom this example? What's jumping out is that this\nratio 1 minus b over 1 plus b",
    "start": "2711680",
    "end": "2720365"
  },
  {
    "text": "is crucial. ",
    "start": "2720365",
    "end": "2725920"
  },
  {
    "text": "If b is near 1,\nthat ratio is small. If b is near 1,\nthat's near 0 over 2.",
    "start": "2725920",
    "end": "2732870"
  },
  {
    "text": "And I converge quickly,\nno problem at all. But if b is near 0, if my\ncondition number is bad--",
    "start": "2732870",
    "end": "2742490"
  },
  {
    "text": "so the bad case, the\nhard case is small b.",
    "start": "2742490",
    "end": "2751430"
  },
  {
    "text": " Of course, when b is small,\nthat ratio is very near 1.",
    "start": "2751430",
    "end": "2761300"
  },
  {
    "text": "It's below 1. The ratio is below 1, so\nI'm getting convergence. I do get convergence.",
    "start": "2761300",
    "end": "2767359"
  },
  {
    "text": "I do go downhill. But what happens is I don't\ngo downhill very far until I'm",
    "start": "2767360",
    "end": "2773810"
  },
  {
    "text": "headed back uphill again. So the picture to\ndraw for this--",
    "start": "2773810",
    "end": "2780720"
  },
  {
    "text": "let me change that picture\nto a picture in the xy",
    "start": "2780720",
    "end": "2786070"
  },
  {
    "text": "plane of the level sets. So the picture really to\nsee is in the xy plane.",
    "start": "2786070",
    "end": "2793869"
  },
  {
    "text": "The level sets f equal constant. That's what a level set is.",
    "start": "2793870",
    "end": "2798940"
  },
  {
    "text": "It's a set of points, x and\ny where f has the same value. And what do those look like?",
    "start": "2798940",
    "end": "2806510"
  },
  {
    "text": "Oh, let's see.  I think-- what do you think?",
    "start": "2806510",
    "end": "2813680"
  },
  {
    "text": "What do the level sets look like\nfor this particular function?",
    "start": "2813680",
    "end": "2819859"
  },
  {
    "text": "If I look at the curve x\nsquared plus b y squared equal a constant, that's\nwhat the level set is.",
    "start": "2819860",
    "end": "2827240"
  },
  {
    "text": "This is x squared plus by\nsquared equal a constant.",
    "start": "2827240",
    "end": "2833619"
  },
  {
    "text": "What kind of a curve is that? AUDIENCE: [INAUDIBLE]. GILBERT STRANG:\nThat's an ellipse.",
    "start": "2833620",
    "end": "2839470"
  },
  {
    "text": "And what's up with that ellipse? What's the shape of it?",
    "start": "2839470",
    "end": "2844750"
  },
  {
    "text": "Because there is no\nxy term, that ellipse is like, well lined\nup with the axes.",
    "start": "2844750",
    "end": "2853180"
  },
  {
    "text": "The major axes of the ellipse\nare in the x and y directions, because there is\nno cross term here.",
    "start": "2853180",
    "end": "2862150"
  },
  {
    "text": "We could always have\ndiagonalized our matrix if it wasn't diagonal.",
    "start": "2862150",
    "end": "2867623"
  },
  {
    "text": "And that wouldn't\nhave changed anything. So it's just\nrotating this space.",
    "start": "2867623",
    "end": "2872740"
  },
  {
    "text": "And we've done that.  What do the levels\nset look like?",
    "start": "2872740",
    "end": "2879130"
  },
  {
    "text": "They're ellipses. And suppose b is a small number,\nthen what's with the ellipses?",
    "start": "2879130",
    "end": "2886690"
  },
  {
    "text": "If b is small, I\nhave to go pretty-- I have to take a pretty\nlarge y to match a--",
    "start": "2886690",
    "end": "2894070"
  },
  {
    "text": "change an x. I think maybe they're\nellipses of that sort. Are they? ",
    "start": "2894070",
    "end": "2904220"
  },
  {
    "text": "They're lined up for the axes. And I hope I'm drawing\nin the right direction.",
    "start": "2904220",
    "end": "2910610"
  },
  {
    "text": "They're long and thin. Is that right? Because I would have\nto take a pretty big y",
    "start": "2910610",
    "end": "2916880"
  },
  {
    "text": "to make up for a small b. OK. So what happens\nwhen I'm descending?",
    "start": "2916880",
    "end": "2924140"
  },
  {
    "text": "This is a narrow valley then. Think of it as a valley\nwhich comes down steeply",
    "start": "2924140",
    "end": "2932240"
  },
  {
    "text": "in the y direction,\nbut in the x direction I'm crossing the valley slow--",
    "start": "2932240",
    "end": "2937560"
  },
  {
    "text": "Oh, is that right? So what happens if I\ntake a point there?",
    "start": "2937560",
    "end": "2944300"
  },
  {
    "text": "Oh yeah, I remember what to do. So let's start at that\npoint on that ellipse.",
    "start": "2944300",
    "end": "2950849"
  },
  {
    "text": " And those were the levels\nsets f equal constant.",
    "start": "2950850",
    "end": "2957490"
  },
  {
    "text": "So what's the first\nsearch direction? What direction do\nI move from x0 y0?",
    "start": "2957490",
    "end": "2963320"
  },
  {
    "start": "2963320",
    "end": "2968510"
  },
  {
    "text": "Do I move along the ellipse? Absolutely not, because along\nthe ellipse f is constant.",
    "start": "2968510",
    "end": "2975490"
  },
  {
    "text": "The gradient direction is\nperpendicular to the ellipse. So I move perpendicular\nto the ellipse.",
    "start": "2975490",
    "end": "2982280"
  },
  {
    "text": "And when do I stop?  Pretty soon, because very\nsoon I'm going back up again.",
    "start": "2982280",
    "end": "2990930"
  },
  {
    "start": "2990930",
    "end": "3002410"
  },
  {
    "text": "I haven't practiced\nwith this curve. But I know-- and time\nis up, thank God.",
    "start": "3002410",
    "end": "3008400"
  },
  {
    "text": "So what do I know\nis going to happen? And by Friday we'll\nmake it happen?",
    "start": "3008400",
    "end": "3013780"
  },
  {
    "text": "So what do we see for the\ncurve, the track of the--",
    "start": "3013780",
    "end": "3022840"
  },
  {
    "text": "it's say it? AUDIENCE: Zigzag. GILBERT STRANG:\nIt's a zigzag, yeah.",
    "start": "3022840",
    "end": "3028110"
  },
  {
    "text": "We would like to get here, but\nwe're not aimed here at all. So we zig, zig, zig zag,\nand very slowly approach",
    "start": "3028110",
    "end": "3036000"
  },
  {
    "text": "that point.  And how slowly?",
    "start": "3036000",
    "end": "3041910"
  },
  {
    "text": "With that multiplier, 1\nminus b over 1 plus b.",
    "start": "3041910",
    "end": "3048990"
  },
  {
    "text": "That's what I'm learning\nfrom this example, that that's a key number. And then you could ask, well,\nwhat about general examples?",
    "start": "3048990",
    "end": "3056760"
  },
  {
    "text": "This was one specially chose\nan example with exact solution. Well, we'll see at the\nbeginning of next time",
    "start": "3056760",
    "end": "3064530"
  },
  {
    "text": "that for a convex\nfunction this is typical. This is 1 minus b is the\ncritical quantity, or 1 over b,",
    "start": "3064530",
    "end": "3074549"
  },
  {
    "text": "or the how small\nis b compared to 1? So that will be the\ncritical quantity.",
    "start": "3074550",
    "end": "3080110"
  },
  {
    "text": "And we see it in this ratio\n1 minus b over 1 plus b. So if b is 100, this\nis 0.99 over 1.01.",
    "start": "3080110",
    "end": "3090210"
  },
  {
    "text": "It's virtually 1. OK. So next time is a\nsort of a key lecture",
    "start": "3090210",
    "end": "3096780"
  },
  {
    "text": "to see what I've just\nsaid, that this controls",
    "start": "3096780",
    "end": "3103380"
  },
  {
    "text": "the convergence of\nsteepest descent, and then to see an\nidea that speeds it up.",
    "start": "3103380",
    "end": "3111130"
  },
  {
    "text": "That idea is called\nmomentum or heavy ball. So the physical idea is if you\nhad a heavy ball right there",
    "start": "3111130",
    "end": "3122819"
  },
  {
    "text": "and wanted to get it down\nthe valley toward the bottom, you wouldn't go perpendicular\nto the level sets.",
    "start": "3122820",
    "end": "3130650"
  },
  {
    "text": "Not at all. You'd let the momentum\nof the ball take over and let it roll down.",
    "start": "3130650",
    "end": "3136990"
  },
  {
    "text": "So the idea of momentum is\nto model the possibility of letting that heavy ball\nroll instead of directing it",
    "start": "3136990",
    "end": "3146240"
  },
  {
    "text": "by the steepest\ndescent at every point. So there's an extra term in\nsteepest descent, the momentum",
    "start": "3146240",
    "end": "3154279"
  },
  {
    "text": "term that accelerates. OK. So Friday is the day.",
    "start": "3154280",
    "end": "3159530"
  },
  {
    "text": "Good. See you then. ",
    "start": "3159530",
    "end": "3163113"
  }
]