[
  {
    "start": "0",
    "end": "43000"
  },
  {
    "start": "0",
    "end": "5280"
  },
  {
    "text": "[MUSIC PLAYING]",
    "start": "5280",
    "end": "8640"
  },
  {
    "start": "8640",
    "end": "10360"
  },
  {
    "text": "AUDACE NAKESHIMANA: In our\nwork on fairness and AI,",
    "start": "10360",
    "end": "12530"
  },
  {
    "text": "we present a case study on\nnatural language processing",
    "start": "12530",
    "end": "14990"
  },
  {
    "text": "titled \"Identifying\nand Mitigating",
    "start": "14990",
    "end": "17029"
  },
  {
    "text": "Unintended Demographic\nBias in Machine Learning.\"",
    "start": "17030",
    "end": "20140"
  },
  {
    "text": "We will break down what each\npart of the title means.",
    "start": "20140",
    "end": "23029"
  },
  {
    "text": "This is the work\nthat was done jointly",
    "start": "23030",
    "end": "24710"
  },
  {
    "text": "by Chris Sweeney\nand Maryam Najafian.",
    "start": "24710",
    "end": "27439"
  },
  {
    "text": "My name is Audace Nakeshimana.",
    "start": "27440",
    "end": "29330"
  },
  {
    "text": "I am a Researcher at MIT, and\nI'll be presenting their work.",
    "start": "29330",
    "end": "32930"
  },
  {
    "text": "The content of the slides\npresents a high-level overview",
    "start": "32930",
    "end": "35630"
  },
  {
    "text": "of a thesis project that\nwas done throughout a course",
    "start": "35630",
    "end": "37970"
  },
  {
    "text": "of the year.",
    "start": "37970",
    "end": "38680"
  },
  {
    "text": "It will be released\nsoon on MIT DSpace.",
    "start": "38680",
    "end": "40860"
  },
  {
    "start": "40860",
    "end": "43910"
  },
  {
    "start": "43000",
    "end": "43000"
  },
  {
    "text": "AI has the power\nto impact society",
    "start": "43910",
    "end": "46360"
  },
  {
    "text": "in a vast amount of ways.",
    "start": "46360",
    "end": "48260"
  },
  {
    "text": "For example, in the\nbanking industry,",
    "start": "48260",
    "end": "50710"
  },
  {
    "text": "many companies are trying to use\nmachine learning to figure out",
    "start": "50710",
    "end": "53530"
  },
  {
    "text": "if someone will default on a\nloan given the data about them.",
    "start": "53530",
    "end": "57280"
  },
  {
    "text": "Now, because machine\nlearning is used",
    "start": "57280",
    "end": "59590"
  },
  {
    "text": "in the high-stakes\napplications, errors",
    "start": "59590",
    "end": "61960"
  },
  {
    "text": "that cause it to be unfair\ncould cause discrimination,",
    "start": "61960",
    "end": "64900"
  },
  {
    "text": "preventing certain demographic\ngroups from gaining access",
    "start": "64900",
    "end": "67750"
  },
  {
    "text": "to fair loans.",
    "start": "67750",
    "end": "69160"
  },
  {
    "text": "This problem is\nespecially important",
    "start": "69160",
    "end": "70990"
  },
  {
    "text": "to address in\ndeveloping nations where",
    "start": "70990",
    "end": "73270"
  },
  {
    "text": "there may not be existing\nsophisticated credit systems.",
    "start": "73270",
    "end": "77079"
  },
  {
    "text": "Those nations will have to\nrely on machine learning models",
    "start": "77080",
    "end": "80200"
  },
  {
    "text": "to make these high-stakes\ndecisions, such as alternative",
    "start": "80200",
    "end": "83409"
  },
  {
    "text": "credit scoring mechanisms\nthat are possibly going to be",
    "start": "83410",
    "end": "86350"
  },
  {
    "text": "involving AI more and more.",
    "start": "86350",
    "end": "90440"
  },
  {
    "start": "89000",
    "end": "89000"
  },
  {
    "text": "This work focuses\non applications",
    "start": "90440",
    "end": "92240"
  },
  {
    "text": "of machine learning in\nnatural language processing.",
    "start": "92240",
    "end": "95119"
  },
  {
    "text": "NLP is important to\nstudying fairness",
    "start": "95120",
    "end": "96950"
  },
  {
    "text": "in AI because it is used\nin many different domains,",
    "start": "96950",
    "end": "99799"
  },
  {
    "text": "from education to marketing.",
    "start": "99800",
    "end": "102080"
  },
  {
    "text": "Furthermore, there\nare many sources",
    "start": "102080",
    "end": "103910"
  },
  {
    "text": "of unintended demographic\nbias in the standard natural",
    "start": "103910",
    "end": "106520"
  },
  {
    "text": "language processing pipeline.",
    "start": "106520",
    "end": "108259"
  },
  {
    "text": "Here we define the NLP\npipeline as a combination",
    "start": "108260",
    "end": "111410"
  },
  {
    "text": "of steps involved, from\ncollecting natural language",
    "start": "111410",
    "end": "113720"
  },
  {
    "text": "data to making decisions\nbased on the NLP models trend",
    "start": "113720",
    "end": "117200"
  },
  {
    "text": "and resulting data.",
    "start": "117200",
    "end": "118950"
  },
  {
    "text": "Lastly, therefore, natural\nlanguage processing systems",
    "start": "118950",
    "end": "121700"
  },
  {
    "text": "is easier to get.",
    "start": "121700",
    "end": "123110"
  },
  {
    "text": "Unlike tabular systems from\nbanking or health care,",
    "start": "123110",
    "end": "125820"
  },
  {
    "text": "where companies may be\nreluctant to release data",
    "start": "125820",
    "end": "128280"
  },
  {
    "text": "due to privacy\nconcerns, NLP data,",
    "start": "128280",
    "end": "130940"
  },
  {
    "text": "especially in widely spoken\nlanguages like French",
    "start": "130940",
    "end": "133610"
  },
  {
    "text": "or English, is available from\ndifferent sources, including",
    "start": "133610",
    "end": "136610"
  },
  {
    "text": "social media and different\nforms of formal and informal",
    "start": "136610",
    "end": "139580"
  },
  {
    "text": "publications, making it more\neffective to use in research",
    "start": "139580",
    "end": "143300"
  },
  {
    "text": "on how to make NLP\nsystems more fair.",
    "start": "143300",
    "end": "147580"
  },
  {
    "start": "147000",
    "end": "147000"
  },
  {
    "text": "We now break down what\nunintended demographic bias",
    "start": "147580",
    "end": "150250"
  },
  {
    "text": "means.",
    "start": "150250",
    "end": "151190"
  },
  {
    "text": "The unintended part\nmeans that this bias",
    "start": "151190",
    "end": "153100"
  },
  {
    "text": "comes as an adverse side\neffect, not deliberately learned",
    "start": "153100",
    "end": "156310"
  },
  {
    "text": "in a machine learning model.",
    "start": "156310",
    "end": "157870"
  },
  {
    "text": "The demographic part means\nthat the bias translates",
    "start": "157870",
    "end": "160390"
  },
  {
    "text": "into some sort of inequality\nbetween demographic groups",
    "start": "160390",
    "end": "163347"
  },
  {
    "text": "that could cause discrimination\nin a downstream machine",
    "start": "163348",
    "end": "165640"
  },
  {
    "text": "learning model.",
    "start": "165640",
    "end": "167000"
  },
  {
    "text": "And finally, bias is an artifact\nof natural language processing",
    "start": "167000",
    "end": "170230"
  },
  {
    "text": "pipeline that causes\nthis unfairness.",
    "start": "170230",
    "end": "173349"
  },
  {
    "text": "Bias is [INAUDIBLE] term.",
    "start": "173350",
    "end": "175420"
  },
  {
    "text": "Therefore, it is\nimportant that we",
    "start": "175420",
    "end": "177280"
  },
  {
    "text": "center on a specific\nform of bias that",
    "start": "177280",
    "end": "179140"
  },
  {
    "text": "causes unfairness in typical\nmachine learning applications.",
    "start": "179140",
    "end": "182380"
  },
  {
    "text": "In gender-based\ndemographic bias,",
    "start": "182380",
    "end": "183970"
  },
  {
    "text": "for example, machine\nlearning model",
    "start": "183970",
    "end": "185860"
  },
  {
    "text": "might associate specific types\nof jobs to specific gender",
    "start": "185860",
    "end": "189520"
  },
  {
    "text": "just because it's the way\nit is in the data used",
    "start": "189520",
    "end": "191800"
  },
  {
    "text": "to train the model.",
    "start": "191800",
    "end": "192730"
  },
  {
    "start": "192730",
    "end": "196470"
  },
  {
    "start": "195000",
    "end": "195000"
  },
  {
    "text": "Within unintended\ndemographic bias,",
    "start": "196470",
    "end": "199110"
  },
  {
    "text": "there are two\ndifferent types of bias",
    "start": "199110",
    "end": "200730"
  },
  {
    "text": "that will focus on in\nnatural language processing",
    "start": "200730",
    "end": "202920"
  },
  {
    "text": "applications.",
    "start": "202920",
    "end": "204209"
  },
  {
    "text": "These are bias in\nsentiment analysis systems",
    "start": "204210",
    "end": "206940"
  },
  {
    "text": "that analyze positive or\nnegative feelings associated",
    "start": "206940",
    "end": "209550"
  },
  {
    "text": "with words or phrases\nand toxicity analysis",
    "start": "209550",
    "end": "212340"
  },
  {
    "text": "systems designed to detect\nderogatory or offensive terms",
    "start": "212340",
    "end": "215819"
  },
  {
    "text": "in words or phrases.",
    "start": "215820",
    "end": "216810"
  },
  {
    "start": "216810",
    "end": "220020"
  },
  {
    "text": "Sentiment bias\nrefers to an artifact",
    "start": "220020",
    "end": "222120"
  },
  {
    "text": "of the machine learning\npipeline that causes unfairness",
    "start": "222120",
    "end": "225000"
  },
  {
    "text": "in sentiment analysis systems.",
    "start": "225000",
    "end": "227130"
  },
  {
    "text": "And toxicity bias is an\nartifact of the pipeline that",
    "start": "227130",
    "end": "230040"
  },
  {
    "text": "causes unfairness in systems\nthat tries to predict toxicity",
    "start": "230040",
    "end": "233040"
  },
  {
    "text": "from text.",
    "start": "233040",
    "end": "234329"
  },
  {
    "text": "In either sentiment analysis\nor toxicity prediction,",
    "start": "234330",
    "end": "237202"
  },
  {
    "text": "it is important that our\nmachine learning model",
    "start": "237202",
    "end": "239159"
  },
  {
    "text": "doesn't use sensitive\nattributes describing",
    "start": "239160",
    "end": "241740"
  },
  {
    "text": "someone's demographic to inform\nthem whether a sentence should",
    "start": "241740",
    "end": "244650"
  },
  {
    "text": "be positive or negative\nsentiment or toxic",
    "start": "244650",
    "end": "247019"
  },
  {
    "text": "or less toxic.",
    "start": "247020",
    "end": "249890"
  },
  {
    "text": "Toxicity classification\nis used in a wide variety",
    "start": "249890",
    "end": "252940"
  },
  {
    "text": "of applications.",
    "start": "252940",
    "end": "254390"
  },
  {
    "text": "For example, it can be used\nto censor online comments that",
    "start": "254390",
    "end": "257560"
  },
  {
    "text": "are too toxic or offensive.",
    "start": "257560",
    "end": "259600"
  },
  {
    "text": "Unfortunately, these\nalgorithms can be very unfair.",
    "start": "259600",
    "end": "263260"
  },
  {
    "text": "For example, the decision\nof whether sentence",
    "start": "263260",
    "end": "265650"
  },
  {
    "text": "is toxic or non-toxic\ncan depend solely",
    "start": "265650",
    "end": "268630"
  },
  {
    "text": "on the demographic\nidentity term,",
    "start": "268630",
    "end": "270310"
  },
  {
    "text": "such as American or Mexican,\nthat appears in the sentence.",
    "start": "270310",
    "end": "274419"
  },
  {
    "text": "This unfairness can be caused\nby many different artifacts",
    "start": "274420",
    "end": "276820"
  },
  {
    "text": "of the natural language\nprocessing pipeline.",
    "start": "276820",
    "end": "279430"
  },
  {
    "text": "For instance, certain\nnationalities and ethnic groups",
    "start": "279430",
    "end": "282550"
  },
  {
    "text": "are specifically more\nfrequently marginalized.",
    "start": "282550",
    "end": "285190"
  },
  {
    "text": "And this is reflected in the\nlanguage usually associated",
    "start": "285190",
    "end": "287830"
  },
  {
    "text": "with them.",
    "start": "287830",
    "end": "288759"
  },
  {
    "text": "Therefore, training NLP\nalgorithms and resulting data",
    "start": "288760",
    "end": "292050"
  },
  {
    "text": "sets could result\nin a certain form",
    "start": "292050",
    "end": "294039"
  },
  {
    "text": "of unintended demographic bias.",
    "start": "294040",
    "end": "295705"
  },
  {
    "start": "295705",
    "end": "298990"
  },
  {
    "start": "298000",
    "end": "298000"
  },
  {
    "text": "We want to drive home the point\nof unintended demographic bias",
    "start": "298990",
    "end": "302090"
  },
  {
    "text": "versus unfairness.",
    "start": "302090",
    "end": "303850"
  },
  {
    "text": "Unintended demographic bias\ncan enter a typical machine",
    "start": "303850",
    "end": "306190"
  },
  {
    "text": "learning pipeline from a\nwide variety of sources,",
    "start": "306190",
    "end": "308990"
  },
  {
    "text": "from the word corpus\nto the word embedding,",
    "start": "308990",
    "end": "311340"
  },
  {
    "text": "the data sent to the algorithm,\nand finally from the thresholds",
    "start": "311340",
    "end": "314330"
  },
  {
    "text": "used to make decisions.",
    "start": "314330",
    "end": "315710"
  },
  {
    "text": "The possible unfairness\nor the discrimination",
    "start": "315710",
    "end": "318533"
  },
  {
    "text": "comes at the point where\nthis machine learning",
    "start": "318533",
    "end": "320449"
  },
  {
    "text": "model meets society and\nactually causes harm.",
    "start": "320450",
    "end": "323090"
  },
  {
    "text": "This work addresses\nmitigating and identifying",
    "start": "323090",
    "end": "325610"
  },
  {
    "text": "unintended demographic\nbias at each stage",
    "start": "325610",
    "end": "328400"
  },
  {
    "text": "in the natural language\nprocessing pipeline,",
    "start": "328400",
    "end": "330290"
  },
  {
    "text": "from the words corpus\nto the decision level.",
    "start": "330290",
    "end": "332330"
  },
  {
    "start": "332330",
    "end": "335169"
  },
  {
    "start": "334000",
    "end": "334000"
  },
  {
    "text": "Our big goal here\nis to find ways",
    "start": "335170",
    "end": "336880"
  },
  {
    "text": "to mitigate the bias\nthat we might inherently",
    "start": "336880",
    "end": "338860"
  },
  {
    "text": "find in the text corpora\nor other types of data",
    "start": "338860",
    "end": "341319"
  },
  {
    "text": "representation that are used\nto build NLP applications.",
    "start": "341320",
    "end": "345340"
  },
  {
    "text": "For this module,\nwe cover measuring",
    "start": "345340",
    "end": "347340"
  },
  {
    "text": "unintended demographic\nbias in word embeddings",
    "start": "347340",
    "end": "349780"
  },
  {
    "text": "and using adversarial learning\nto mitigate word embedding",
    "start": "349780",
    "end": "352450"
  },
  {
    "text": "bias.",
    "start": "352450",
    "end": "353870"
  },
  {
    "text": "The corresponding\nthesis goes further,",
    "start": "353870",
    "end": "355810"
  },
  {
    "text": "and it covers techniques for\nidentifying and mitigating",
    "start": "355810",
    "end": "358360"
  },
  {
    "text": "unintended demographic bias\nat other stages of the NLP",
    "start": "358360",
    "end": "361360"
  },
  {
    "text": "pipeline.",
    "start": "361360",
    "end": "363569"
  },
  {
    "text": "We now cover the work as\nmeasuring word embedding bias.",
    "start": "363570",
    "end": "366375"
  },
  {
    "start": "366375",
    "end": "369480"
  },
  {
    "start": "368000",
    "end": "368000"
  },
  {
    "text": "Word embeddings encode\ntext into vector spaces",
    "start": "369480",
    "end": "372150"
  },
  {
    "text": "where distances\nbetween words describe",
    "start": "372150",
    "end": "373800"
  },
  {
    "text": "a certain semantic meaning.",
    "start": "373800",
    "end": "376080"
  },
  {
    "text": "This allows one to\ncomplete the analogy of man",
    "start": "376080",
    "end": "378379"
  },
  {
    "text": "is to woman as king is to queen.",
    "start": "378380",
    "end": "380710"
  },
  {
    "text": "Unfortunately, researchers\nTolga Balukbasi and others",
    "start": "380710",
    "end": "384360"
  },
  {
    "text": "found that even for word\nembeddings trained from Google",
    "start": "384360",
    "end": "386879"
  },
  {
    "text": "News articles, there exists bias\nin word embedding space, where",
    "start": "386880",
    "end": "390910"
  },
  {
    "text": "the analogy becomes man is to\nwoman as computer programmer is",
    "start": "390910",
    "end": "394500"
  },
  {
    "text": "to homemaker, another\nword for a housewife.",
    "start": "394500",
    "end": "397410"
  },
  {
    "text": "This is concerning given\nthat word embeddings could",
    "start": "397410",
    "end": "400170"
  },
  {
    "text": "be used in natural language\nprocessing applications devoted",
    "start": "400170",
    "end": "402690"
  },
  {
    "text": "to predicting whether someone\nshould get a certain job.",
    "start": "402690",
    "end": "405570"
  },
  {
    "text": "However, it is difficult to\nquantify the bias just based",
    "start": "405570",
    "end": "408870"
  },
  {
    "text": "on the vector space analogies.",
    "start": "408870",
    "end": "412229"
  },
  {
    "start": "411000",
    "end": "411000"
  },
  {
    "text": "In this work, researchers\nSweeney and Najafian",
    "start": "412230",
    "end": "415740"
  },
  {
    "text": "develop a system to\nmeasure sentiment bias",
    "start": "415740",
    "end": "418090"
  },
  {
    "text": "in word embeddings\nto a specific number.",
    "start": "418090",
    "end": "420810"
  },
  {
    "text": "The way they do this is they\ntake the bias toward embeddings",
    "start": "420810",
    "end": "424230"
  },
  {
    "text": "and use them to\ninitialize an unbiased",
    "start": "424230",
    "end": "426210"
  },
  {
    "text": "labeled word\nsentiments data set.",
    "start": "426210",
    "end": "428215"
  },
  {
    "text": "They train a logistic regression\nclassifier on this data set,",
    "start": "428215",
    "end": "432300"
  },
  {
    "text": "and they predict\nnegative sentiment",
    "start": "432300",
    "end": "434099"
  },
  {
    "text": "for a set of identity terms.",
    "start": "434100",
    "end": "436500"
  },
  {
    "text": "For example, in\nthis case, this is",
    "start": "436500",
    "end": "439160"
  },
  {
    "text": "a set of identity terms\ndescribing demographics",
    "start": "439160",
    "end": "441690"
  },
  {
    "text": "from different national origins.",
    "start": "441690",
    "end": "443920"
  },
  {
    "text": "They analyzed the\nnegative sentiment",
    "start": "443920",
    "end": "445530"
  },
  {
    "text": "for each identity\nterm and predict",
    "start": "445530",
    "end": "447720"
  },
  {
    "text": "a score that describes the\nbias in word embeddings.",
    "start": "447720",
    "end": "453460"
  },
  {
    "text": "This score is the divergence\nbetween the [INAUDIBLE]",
    "start": "453460",
    "end": "455949"
  },
  {
    "text": "for abilities, for\nnegative sentiment,",
    "start": "455950",
    "end": "457810"
  },
  {
    "text": "for national origin,\nidentity terms,",
    "start": "457810",
    "end": "460010"
  },
  {
    "text": "and the uniform distribution.",
    "start": "460010",
    "end": "461640"
  },
  {
    "text": "The uniform distribution\ndescribes a perfectly fair",
    "start": "461640",
    "end": "464320"
  },
  {
    "text": "case, wherein a\ndemographic is receiving",
    "start": "464320",
    "end": "466690"
  },
  {
    "text": "an equal amount of sentiment\nin the word embedding model.",
    "start": "466690",
    "end": "469360"
  },
  {
    "start": "469360",
    "end": "472020"
  },
  {
    "text": "Now that we have a grasp\non the word embedding bias,",
    "start": "472020",
    "end": "474990"
  },
  {
    "text": "we can start to figure out how\nto mitigate some of this bias.",
    "start": "474990",
    "end": "480030"
  },
  {
    "start": "479000",
    "end": "479000"
  },
  {
    "text": "In the thesis,\nSweeney and Nafajian",
    "start": "480030",
    "end": "482820"
  },
  {
    "text": "describe how they use\nadversarial learning",
    "start": "482820",
    "end": "484800"
  },
  {
    "text": "to debias word embeddings.",
    "start": "484800",
    "end": "486780"
  },
  {
    "text": "Different identity terms can\nbe more or less correlated",
    "start": "486780",
    "end": "489570"
  },
  {
    "text": "with positive or\nnegative sentiment.",
    "start": "489570",
    "end": "491880"
  },
  {
    "text": "For example, words like\nAmerican, Mexican, and German",
    "start": "491880",
    "end": "495570"
  },
  {
    "text": "can have more correlations with\nnegative sentiment subspaces",
    "start": "495570",
    "end": "498720"
  },
  {
    "text": "and positive\nsentiment subspaces,",
    "start": "498720",
    "end": "500400"
  },
  {
    "text": "because in the\ndata sets used, it",
    "start": "500400",
    "end": "502514"
  },
  {
    "text": "might appear to be more\nfrequently associated",
    "start": "502515",
    "end": "504390"
  },
  {
    "text": "with negative or\npositive sentiments.",
    "start": "504390",
    "end": "506700"
  },
  {
    "text": "This is concerning.",
    "start": "506700",
    "end": "507690"
  },
  {
    "text": "Even downstream\nmachine learning model",
    "start": "507690",
    "end": "509273"
  },
  {
    "text": "picks up on these correlations.",
    "start": "509273",
    "end": "510990"
  },
  {
    "text": "Ideally, you want to have\neach of those identity terms",
    "start": "510990",
    "end": "514229"
  },
  {
    "text": "to a neutral point between\nnegative and positive sentiment",
    "start": "514230",
    "end": "516719"
  },
  {
    "text": "subspaces without distorting\ntheir meaning within the vector",
    "start": "516720",
    "end": "519928"
  },
  {
    "text": "space so that the word embedding\nmodel can still be useful.",
    "start": "519929",
    "end": "524790"
  },
  {
    "text": "They use an adversarial learning\nalgorithm to achieve this.",
    "start": "524790",
    "end": "528240"
  },
  {
    "text": "More details of this\nalgorithm are described",
    "start": "528240",
    "end": "530610"
  },
  {
    "text": "in the corresponding phases.",
    "start": "530610",
    "end": "531779"
  },
  {
    "start": "531780",
    "end": "534860"
  },
  {
    "start": "534000",
    "end": "534000"
  },
  {
    "text": "I now present some of\ntheir work in evaluating",
    "start": "534860",
    "end": "537350"
  },
  {
    "text": "how adversarial\nlearning algorithms can",
    "start": "537350",
    "end": "539750"
  },
  {
    "text": "debias word embeddings and make\nthe resulting natural language",
    "start": "539750",
    "end": "542420"
  },
  {
    "text": "processing system more fair.",
    "start": "542420",
    "end": "544899"
  },
  {
    "text": "We focus on realistic systems\nin both sentiment analysis",
    "start": "544900",
    "end": "548490"
  },
  {
    "text": "and toxicity prediction.",
    "start": "548490",
    "end": "550410"
  },
  {
    "text": "For each application,\nSweeney and Najafian",
    "start": "550410",
    "end": "553199"
  },
  {
    "text": "define fairness\nmetrics to let us",
    "start": "553200",
    "end": "554880"
  },
  {
    "text": "know whether the debiased\nword embeddings are actually",
    "start": "554880",
    "end": "557370"
  },
  {
    "text": "helping.",
    "start": "557370",
    "end": "559690"
  },
  {
    "start": "559000",
    "end": "559000"
  },
  {
    "text": "These fairness metrics\noften come in the form",
    "start": "559690",
    "end": "562180"
  },
  {
    "text": "of a templates data set.",
    "start": "562180",
    "end": "564040"
  },
  {
    "text": "Researchers have created these\ndata sets to somewhat tease out",
    "start": "564040",
    "end": "567070"
  },
  {
    "text": "different biases with respect\nto different demographic groups.",
    "start": "567070",
    "end": "570640"
  },
  {
    "text": "For example, this set\nis meant to tease out",
    "start": "570640",
    "end": "573550"
  },
  {
    "text": "biases between African-American\nnames and European-American",
    "start": "573550",
    "end": "576640"
  },
  {
    "text": "names when substituting each\nname out in the same sentence.",
    "start": "576640",
    "end": "581590"
  },
  {
    "text": "Similar template\ndata sets have been",
    "start": "581590",
    "end": "583390"
  },
  {
    "text": "created for toxicity\nclassification algorithms,",
    "start": "583390",
    "end": "586090"
  },
  {
    "text": "where you sub out different\ndemographic identity",
    "start": "586090",
    "end": "588520"
  },
  {
    "text": "terms within a sentence\nand compare differences",
    "start": "588520",
    "end": "591010"
  },
  {
    "text": "in the overall\ntoxicity predictions.",
    "start": "591010",
    "end": "593100"
  },
  {
    "start": "593100",
    "end": "596139"
  },
  {
    "start": "595000",
    "end": "595000"
  },
  {
    "text": "Sweeney and Najafian\nused these templates data",
    "start": "596140",
    "end": "598590"
  },
  {
    "text": "sets to compute fairness for a\nreal-world toxicity classifier.",
    "start": "598590",
    "end": "602290"
  },
  {
    "text": "This graph shows per-term\nAUC distributions",
    "start": "602290",
    "end": "604899"
  },
  {
    "text": "for CNN convolution\nneural network",
    "start": "604900",
    "end": "607750"
  },
  {
    "text": "that was trained on a toxicity\nclassification data set.",
    "start": "607750",
    "end": "610960"
  },
  {
    "text": "The x-axis represents\neach demographic group,",
    "start": "610960",
    "end": "613390"
  },
  {
    "text": "where the templates data set\nhas that identify term subbed",
    "start": "613390",
    "end": "616170"
  },
  {
    "text": "in for each sentence.",
    "start": "616170",
    "end": "617980"
  },
  {
    "text": "Each dot describes a particular\ntraining run of the CNN.",
    "start": "617980",
    "end": "622329"
  },
  {
    "text": "The y-axis describes the\narea under the curve accuracy",
    "start": "622330",
    "end": "625390"
  },
  {
    "text": "for this template's data set.",
    "start": "625390",
    "end": "627430"
  },
  {
    "text": "One can see that there\nis a lot of disparity",
    "start": "627430",
    "end": "629710"
  },
  {
    "text": "between the accuracies for\ndifferent demographic groups.",
    "start": "629710",
    "end": "633400"
  },
  {
    "text": "Ideally, you would want the\nvariance in different training",
    "start": "633400",
    "end": "636400"
  },
  {
    "text": "runs to be compressed as\nwell as the differences",
    "start": "636400",
    "end": "638980"
  },
  {
    "text": "between each demographic\ngroup in the AUC scores",
    "start": "638980",
    "end": "641440"
  },
  {
    "text": "to be smaller.",
    "start": "641440",
    "end": "642670"
  },
  {
    "text": "Sweeney and Najafian show\na toxicity classification",
    "start": "642670",
    "end": "645970"
  },
  {
    "text": "algorithm that uses the\ndebias towards embeddings",
    "start": "645970",
    "end": "648819"
  },
  {
    "text": "creates better results.",
    "start": "648820",
    "end": "652060"
  },
  {
    "text": "This slide shows results for\nper-term AUC distributions",
    "start": "652060",
    "end": "655290"
  },
  {
    "text": "for the CNN with different\ndebias treatments.",
    "start": "655290",
    "end": "658870"
  },
  {
    "text": "Sweeney and Najafian measure how\ntheir word embedding debiasing",
    "start": "658870",
    "end": "662680"
  },
  {
    "text": "compares to other\nstate-of-the-art techniques.",
    "start": "662680",
    "end": "665510"
  },
  {
    "text": "Further discussion and\nevaluation of these graphs",
    "start": "665510",
    "end": "668500"
  },
  {
    "text": "are presented in the\ncorresponding thesis.",
    "start": "668500",
    "end": "672540"
  },
  {
    "start": "671000",
    "end": "671000"
  },
  {
    "text": "To wrap up, we describe some\nkey takeaways from this project.",
    "start": "672540",
    "end": "676649"
  },
  {
    "text": "First, there is\nno silver bullet.",
    "start": "676650",
    "end": "678963"
  },
  {
    "text": "There are many different\ntypes of applications",
    "start": "678963",
    "end": "680880"
  },
  {
    "text": "and various types of bias\nto correct for when trying",
    "start": "680880",
    "end": "683370"
  },
  {
    "text": "to make NLP systems more fair.",
    "start": "683370",
    "end": "685860"
  },
  {
    "text": "Second, bias can emanate\nfrom any stage of the machine",
    "start": "685860",
    "end": "689160"
  },
  {
    "text": "learning pipeline.",
    "start": "689160",
    "end": "690509"
  },
  {
    "text": "Therefore, having to also\nidentify and mitigate bias",
    "start": "690510",
    "end": "693810"
  },
  {
    "text": "at all stages of the machine\nlearning pipeline is essential.",
    "start": "693810",
    "end": "697170"
  },
  {
    "text": "Finally, we focus on solving\nthis problem within an academic",
    "start": "697170",
    "end": "700800"
  },
  {
    "text": "context for natural language\nprocessing pipeline,",
    "start": "700800",
    "end": "703470"
  },
  {
    "text": "but this cannot all\nbe solved in academia.",
    "start": "703470",
    "end": "708100"
  },
  {
    "text": "For example, much of the\nunintended bias in the data",
    "start": "708100",
    "end": "711029"
  },
  {
    "text": "set, like the text corpus,\ncould come from decisions made",
    "start": "711030",
    "end": "714610"
  },
  {
    "text": "upstream in direct collection.",
    "start": "714610",
    "end": "716709"
  },
  {
    "text": "Furthermore,\nunintended bias could",
    "start": "716710",
    "end": "718720"
  },
  {
    "text": "come from decisions\nmade when deploying",
    "start": "718720",
    "end": "720610"
  },
  {
    "text": "the model into society.",
    "start": "720610",
    "end": "722430"
  },
  {
    "text": "When the model is used\nin a way that does not",
    "start": "722430",
    "end": "724390"
  },
  {
    "text": "resonate with how the data was\ncollected in the first place,",
    "start": "724390",
    "end": "727730"
  },
  {
    "text": "this could cause discrimination.",
    "start": "727730",
    "end": "729730"
  },
  {
    "text": "An example of this is\nwhen the data collected",
    "start": "729730",
    "end": "731860"
  },
  {
    "text": "from a specific\ndemographic population",
    "start": "731860",
    "end": "734110"
  },
  {
    "text": "is used to make predictions that\naffect other demographics that",
    "start": "734110",
    "end": "737260"
  },
  {
    "text": "were not taken into account\nduring data collection.",
    "start": "737260",
    "end": "740050"
  },
  {
    "text": "Finally, it is important to have\nefficient channels of feedback",
    "start": "740050",
    "end": "743178"
  },
  {
    "text": "for these machine\nlearning models.",
    "start": "743178",
    "end": "744595"
  },
  {
    "start": "744595",
    "end": "747399"
  },
  {
    "text": "The work presented\nin this module",
    "start": "747400",
    "end": "749410"
  },
  {
    "text": "highlights why fairness is\na very important concept.",
    "start": "749410",
    "end": "752620"
  },
  {
    "text": "It is therefore critical for\ndata scientists and engineers",
    "start": "752620",
    "end": "755650"
  },
  {
    "text": "to measure and understand\nperformance of their models",
    "start": "755650",
    "end": "758350"
  },
  {
    "text": "not just through accuracy,\nbut also through fairness.",
    "start": "758350",
    "end": "762490"
  },
  {
    "text": "[MUSIC PLAYING]",
    "start": "762490",
    "end": "765839"
  },
  {
    "start": "765840",
    "end": "782000"
  }
]