[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6910"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6910",
    "end": "13460"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at ocw.mit.edu.",
    "start": "13460",
    "end": "18640"
  },
  {
    "text": " JOHN TSITSIKLIS: We're going\nto start today a new unit.",
    "start": "18640",
    "end": "25130"
  },
  {
    "text": "so we will be talking about\nlimit theorems. So just to introduce the topic,\nlet's think of the",
    "start": "25130",
    "end": "33580"
  },
  {
    "text": "following situation. There's a population\nof penguins down at the South Pole.",
    "start": "33580",
    "end": "38970"
  },
  {
    "text": "And if you were to pick a\npenguin at random and measure their height, the expected value\nof their height would be",
    "start": "38970",
    "end": "46930"
  },
  {
    "text": "the average of the heights of\nthe different penguins in the population. So suppose when you\npick one, every",
    "start": "46930",
    "end": "53430"
  },
  {
    "text": "penguin is equally likely. Then the expected value is just\nthe average of all the penguins out there.",
    "start": "53430",
    "end": "59340"
  },
  {
    "text": "So your boss asks you to\nfind out what that the expected value is. One way would be to\ngo and measure",
    "start": "59340",
    "end": "64980"
  },
  {
    "text": "each and every penguin. That might be a little\ntime consuming. So alternatively, what you can\ndo is to go and pick penguins",
    "start": "64980",
    "end": "73119"
  },
  {
    "text": "at random, pick a few of them,\nlet's say a number n of them. So you measure the height\nof each one.",
    "start": "73120",
    "end": "80420"
  },
  {
    "text": "And then you calculate the\naverage of the heights of",
    "start": "80420",
    "end": "85920"
  },
  {
    "text": "those penguins that you\nhave collected. So this is your estimate\nof the expected value.",
    "start": "85920",
    "end": "93100"
  },
  {
    "text": "Now, we called this the sample\nmean, which is the mean value,",
    "start": "93100",
    "end": "101009"
  },
  {
    "text": "but within the sample that\nyou have collected. This is something that's sort\nof feels the same as the",
    "start": "101010",
    "end": "108090"
  },
  {
    "text": "expected value, which\nis again, the mean. But the expected value's a\ndifferent kind of mean.",
    "start": "108090",
    "end": "114400"
  },
  {
    "text": "The expected value is the mean\nover the entire population, whereas the sample mean is the\naverage over the smaller",
    "start": "114400",
    "end": "121680"
  },
  {
    "text": "sample that you have measured. The expected value\nis a number. The sample mean is a\nrandom variable.",
    "start": "121680",
    "end": "129220"
  },
  {
    "text": "It's a random variable because\nthe sample you have collected is random.",
    "start": "129220",
    "end": "135010"
  },
  {
    "text": "Now, we think that this is a\nreasonable way of estimating the expectation. So in the limit as n goes to\ninfinity, it's plausible that",
    "start": "135010",
    "end": "145709"
  },
  {
    "text": "the sample mean, the estimate\nthat we are constructing, should somehow get close\nto the expected value.",
    "start": "145710",
    "end": "153790"
  },
  {
    "text": "What does this mean? What does it mean\nto get close? In what sense? And is this statement true?",
    "start": "153790",
    "end": "159440"
  },
  {
    "text": "This is the kind of statement\nthat we deal with when dealing with limit theorems.",
    "start": "159440",
    "end": "165710"
  },
  {
    "text": "That's the subject of limit\ntheorems, when what happens if you're dealing with lots and\nlots of random variables, and",
    "start": "165710",
    "end": "172020"
  },
  {
    "text": "perhaps take averages\nand so on. So why do we bother\nabout this?",
    "start": "172020",
    "end": "177280"
  },
  {
    "text": "Well, if you're in the sampling\nbusiness, it would be reassuring to know that this\nparticular way of estimating",
    "start": "177280",
    "end": "184870"
  },
  {
    "text": "the expected value\nactually gets you close to the true answer. There's also a higher level\nreason, which is a little more",
    "start": "184870",
    "end": "191890"
  },
  {
    "text": "abstract and mathematical. So probability problems are easy\nto deal with if you're",
    "start": "191890",
    "end": "197110"
  },
  {
    "text": "having in your hands one or\ntwo random variables. You can write down their mass\nfunctions, joints density",
    "start": "197110",
    "end": "203519"
  },
  {
    "text": "functions, and so on. You can calculate on paper\nor on a computer, you can get the answers.",
    "start": "203520",
    "end": "209430"
  },
  {
    "text": "Probability problems become\ncomputationally intractable if you're dealing, let's say, with\n100 random variables and",
    "start": "209430",
    "end": "216760"
  },
  {
    "text": "you're trying to get the exact\nanswers for anything. So in principle, the same\nformulas that we have, they",
    "start": "216760",
    "end": "223050"
  },
  {
    "text": "still apply. But they involve summations\nover large ranges of combinations of indices.",
    "start": "223050",
    "end": "228830"
  },
  {
    "text": "And that makes life extremely\ndifficult. But when you push the envelope\nand you go to a situation",
    "start": "228830",
    "end": "235099"
  },
  {
    "text": "where you're dealing with a\nvery, very large number of variables, then you can\nstart taking limits.",
    "start": "235100",
    "end": "242129"
  },
  {
    "text": "And when you take limits,\nwonderful things happen. Many formulas start simplifying,\nand you can",
    "start": "242130",
    "end": "248030"
  },
  {
    "text": "actually get useful answers by\nconsidering those limits. And that's sort of the big\nreason why looking at limit",
    "start": "248030",
    "end": "255450"
  },
  {
    "text": "theorems is a useful\nthing to do. So what we're going to do today,\nfirst we're going to",
    "start": "255450",
    "end": "260989"
  },
  {
    "text": "start with a useful, simple tool\nthat allows us to relates",
    "start": "260990",
    "end": "267110"
  },
  {
    "text": "probabilities with\nexpected values. The Markov inequality is the\nfirst inequality we're going",
    "start": "267110",
    "end": "273230"
  },
  {
    "text": "to write down. And then using that, we're going\nto get the Chebyshev's inequality, a related\ninequality.",
    "start": "273230",
    "end": "279759"
  },
  {
    "text": "Then we need to define what do\nwe mean by convergence when we talk about random variables.",
    "start": "279760",
    "end": "285270"
  },
  {
    "text": "It's a notion that's a\ngeneralization of the notion of the usual convergence\nof limits of",
    "start": "285270",
    "end": "291000"
  },
  {
    "text": "a sequence of numbers. And once we have our notion of\nconvergence, we're going to see that, indeed, the sample\nmean converges to the true",
    "start": "291000",
    "end": "300860"
  },
  {
    "text": "mean, converges to the expected\nvalue of the X's. And this statement is called the\nweak law of large numbers.",
    "start": "300860",
    "end": "308840"
  },
  {
    "text": "The reason it's called the weak\nlaw is because there's also a strong law, which is\na statement with the same",
    "start": "308840",
    "end": "314639"
  },
  {
    "text": "flavor, but with a somewhat\ndifferent mathematical content. But it's a little more abstract,\nand we will not be",
    "start": "314640",
    "end": "320790"
  },
  {
    "text": "getting into this. So the weak law is all that\nyou're going to get.",
    "start": "320790",
    "end": "326070"
  },
  {
    "text": "All right. So now we start our\ndigression. And our first tool will be the\nso-called Markov inequality.",
    "start": "326070",
    "end": "338220"
  },
  {
    "start": "338220",
    "end": "345050"
  },
  {
    "text": "So let's take a random variable\nthat's always non-negative. No matter what, it gets\nno negative values.",
    "start": "345050",
    "end": "351790"
  },
  {
    "text": "To keep things simple,\nlet's assume it's a discrete random variable. So the expected value is the sum\nover all possible values",
    "start": "351790",
    "end": "359770"
  },
  {
    "text": "that a random variable\ncan take.  The values of the random\nvariables that can take",
    "start": "359770",
    "end": "366600"
  },
  {
    "text": "weighted according to their\ncorresponding probabilities. Now, this is a sum\nover all x's.",
    "start": "366600",
    "end": "373700"
  },
  {
    "text": "But x takes non-negative\nvalues. And the PMF is also\nnon-negative.",
    "start": "373700",
    "end": "379780"
  },
  {
    "text": "So if I take a sum over fewer\nthings, I'm going to get a smaller value.",
    "start": "379780",
    "end": "385550"
  },
  {
    "text": "So the sum when I add over\neverything is less than or equal to the sum that I will get\nif I only add those terms",
    "start": "385550",
    "end": "393255"
  },
  {
    "text": "that are bigger than\na certain constant. ",
    "start": "393255",
    "end": "398600"
  },
  {
    "text": "Now, if I'm adding over x's that\nare bigger than a, the x",
    "start": "398600",
    "end": "405140"
  },
  {
    "text": "that shows up up there\nwill always be larger than or equal to a.",
    "start": "405140",
    "end": "410490"
  },
  {
    "text": "So we get this inequality. ",
    "start": "410490",
    "end": "418169"
  },
  {
    "text": "And now, a is a constant. I can pull it outside\nthe summation. And then I'm left with the\nprobabilities of all the x's",
    "start": "418170",
    "end": "425320"
  },
  {
    "text": "that are bigger than a. And that's just the\nprobability of being bigger than a. ",
    "start": "425320",
    "end": "435540"
  },
  {
    "text": "OK, so that's the Markov\ninequality. Basically tells us that the\nexpected value is larger than",
    "start": "435540",
    "end": "443800"
  },
  {
    "text": "or equal to this number. It relates expected values\nto probabilities.",
    "start": "443800",
    "end": "450259"
  },
  {
    "text": "It tells us that if the expected\nvalue is small, then the probability that x is big\nis also going to be small.",
    "start": "450260",
    "end": "459250"
  },
  {
    "text": "So it's translates a statement\nabout smallness of expected values to a statement about\nsmallness of probabilities.",
    "start": "459250",
    "end": "466205"
  },
  {
    "text": " OK. What we actually need is a\nsomewhat different version of",
    "start": "466205",
    "end": "474210"
  },
  {
    "text": "this same statement. And what we're going to do is to\napply this inequality to a",
    "start": "474210",
    "end": "483009"
  },
  {
    "text": "non-negative random variable\nof a special type.",
    "start": "483010",
    "end": "488150"
  },
  {
    "text": "And you can think of applying\nthis same calculation to a",
    "start": "488150",
    "end": "493330"
  },
  {
    "text": "random variable of this form, (X\nminus mu)-squared, where mu",
    "start": "493330",
    "end": "498800"
  },
  {
    "text": "is the expected value of X. Now, this is a non-negative\nrandom variable.",
    "start": "498800",
    "end": "504074"
  },
  {
    "start": "504075",
    "end": "515419"
  },
  {
    "text": "So, the expected value of this\nrandom variable, which is the variance, by following the same\nthinking as we had in",
    "start": "515419",
    "end": "522219"
  },
  {
    "text": "that derivation up to there, is\nbigger than the probability",
    "start": "522220",
    "end": "532879"
  },
  {
    "text": "that this random variable\nis bigger than some--",
    "start": "532880",
    "end": "538210"
  },
  {
    "text": "let me use a-squared\ninstead of an a",
    "start": "538210",
    "end": "544760"
  },
  {
    "text": "times the value a-squared. ",
    "start": "544760",
    "end": "552420"
  },
  {
    "text": "So now of course, this\nprobability is the same as the probability that the absolute\nvalue of X minus mu is bigger",
    "start": "552420",
    "end": "563440"
  },
  {
    "text": "than a times a-squared. And this side is equal to the\nvariance of X. So this relates",
    "start": "563440",
    "end": "574860"
  },
  {
    "text": "the variance of X to the\nprobability that our random",
    "start": "574860",
    "end": "580890"
  },
  {
    "text": "variable is far away\nfrom its mean. If the variance is small, then\nit means that the probability",
    "start": "580890",
    "end": "590590"
  },
  {
    "text": "of being far away from the\nmean is also small. ",
    "start": "590590",
    "end": "597240"
  },
  {
    "text": "So I derived this by applying\nthe Markov inequality to this particular non-negative\nrandom variable.",
    "start": "597240",
    "end": "604949"
  },
  {
    "text": "Or just to reinforce, perhaps,\nthe message, and increase your confidence in this inequality,\nlet's just look at the",
    "start": "604950",
    "end": "613450"
  },
  {
    "text": "derivation once more, where I'm\ngoing, here, to start from first principles, but use the\nsame idea as the one that was",
    "start": "613450",
    "end": "620889"
  },
  {
    "text": "used in the proof out here. Ok. So just for variety, now let's\nthink of X as being a",
    "start": "620890",
    "end": "626920"
  },
  {
    "text": "continuous random variable. The derivation is the same\nwhether it's discrete or continuous.",
    "start": "626920",
    "end": "632510"
  },
  {
    "text": "So by definition, the variance\nis the integral, is this particular integral.",
    "start": "632510",
    "end": "638130"
  },
  {
    "text": "Now, the integral is going to\nbecome smaller if I integrate,",
    "start": "638130",
    "end": "643920"
  },
  {
    "text": "instead of integrating over\nthe full range, I only integrate over x's that are\nfar away from the mean.",
    "start": "643920",
    "end": "651070"
  },
  {
    "text": "So mu is the mean. Think of c as some big number. ",
    "start": "651070",
    "end": "659670"
  },
  {
    "text": "These are x's that are far\naway from the mean to the left, from minus infinity\nto mu minus c.",
    "start": "659670",
    "end": "665410"
  },
  {
    "text": "And these are the x's that are\nfar away from the mean on the positive side.",
    "start": "665410",
    "end": "671210"
  },
  {
    "text": "So by integrating over\nfewer stuff, I'm getting a smaller integral. Now, for any x in this range,\nthis distance, x minus mu, is",
    "start": "671210",
    "end": "681970"
  },
  {
    "text": "at least c. So that squared is at\nleast c squared. So this term over this\nrange of integration",
    "start": "681970",
    "end": "688910"
  },
  {
    "text": "is at least c squared. So I can take it outside\nthe integral. And I'm left just with the\nintegral of the density.",
    "start": "688910",
    "end": "696400"
  },
  {
    "text": "Same thing on the other side. And so what factors out is\nthis term c squared.",
    "start": "696400",
    "end": "701769"
  },
  {
    "text": "And inside, we're left with the\nprobability of being to the left of mu minus c, and then\nthe probability of being",
    "start": "701770",
    "end": "709060"
  },
  {
    "text": "to the right of mu plus c,\nwhich is the same as the probability that the absolute\nvalue of the distance from the",
    "start": "709060",
    "end": "715370"
  },
  {
    "text": "mean is larger than\nor equal to c. So that's the same inequality\nthat we proved there, except",
    "start": "715370",
    "end": "724820"
  },
  {
    "text": "that here I'm using c. There I used a, but it's\nexactly the same one.",
    "start": "724820",
    "end": "730529"
  },
  {
    "text": "This inequality was maybe better\nto understand if you take that term and send it\nto the other side and",
    "start": "730530",
    "end": "736790"
  },
  {
    "text": "write it this form. What does it tell us? It tells us that if c is a big\nnumber, it tells us that the",
    "start": "736790",
    "end": "745750"
  },
  {
    "text": "probability of being more than\nc away from the mean is going to be a small number.",
    "start": "745750",
    "end": "752330"
  },
  {
    "text": "When c is big, this is small. Now, this is intuitive. The variance is a measure\nof the spread of the",
    "start": "752330",
    "end": "758290"
  },
  {
    "text": "distribution, how wide it is. It tells us that if the\nvariance is small, the",
    "start": "758290",
    "end": "763960"
  },
  {
    "text": "distribution is not very wide. And mathematically, this\ntranslates to this statement",
    "start": "763960",
    "end": "769020"
  },
  {
    "text": "that when the variance is small,\nthe probability of being far away is going\nto be small.",
    "start": "769020",
    "end": "774880"
  },
  {
    "text": "And the further away you're\nlooking, that is, if c is a bigger number, that probability",
    "start": "774880",
    "end": "780330"
  },
  {
    "text": "also becomes small.  Maybe an even more intuitive way\nto think about the content",
    "start": "780330",
    "end": "787880"
  },
  {
    "text": "of this inequality is to,\ninstead of c, use the number",
    "start": "787880",
    "end": "793230"
  },
  {
    "text": "k, where k is positive\nand sigma is the standard deviation.",
    "start": "793230",
    "end": "798529"
  },
  {
    "text": "So let's just plug k sigma\nin the place of c. So this becomes k\nsigma squared.",
    "start": "798530",
    "end": "805300"
  },
  {
    "text": "These sigma squared's cancel. We're left with 1\nover k-square. Now, what is this?",
    "start": "805300",
    "end": "811690"
  },
  {
    "text": "This is the event that you are\nk standard deviations away from the mean.",
    "start": "811690",
    "end": "817770"
  },
  {
    "text": "So for example, this statement\nhere tells you that if you look at the test scores from a\nquiz, what fraction of the",
    "start": "817770",
    "end": "824899"
  },
  {
    "text": "class are 3 standard deviations\naway from the mean? It's possible, but it's not\ngoing to be a lot of people.",
    "start": "824900",
    "end": "833000"
  },
  {
    "text": "It's going to be at most, 1/9\nof the class that can be 3 standard deviations or more\naway from the mean.",
    "start": "833000",
    "end": "842190"
  },
  {
    "text": "So the Chebyshev inequality\nis a really useful one. ",
    "start": "842190",
    "end": "847860"
  },
  {
    "text": "It comes in handy whenever you\nwant to relate probabilities and expected values. So if you know that your\nexpected values or, in",
    "start": "847860",
    "end": "856390"
  },
  {
    "text": "particular, that your variance\nis small, this tells you something about tailed\nprobabilities.",
    "start": "856390",
    "end": "863080"
  },
  {
    "text": "So this is the end of our\nfirst digression. We have this inequality\nin our hands.",
    "start": "863080",
    "end": "868320"
  },
  {
    "text": "Our second digression is\ntalk about limits. ",
    "start": "868320",
    "end": "874680"
  },
  {
    "text": "We want to eventually talk\nabout limits of random variables, but as a warm up,\nwe're going to start with",
    "start": "874680",
    "end": "879750"
  },
  {
    "text": "limits of sequences. So you're given a sequence\nof numbers, a1,",
    "start": "879750",
    "end": "887670"
  },
  {
    "text": "a2, a3, and so on. And we want to define the\nnotion that a sequence",
    "start": "887670",
    "end": "894160"
  },
  {
    "text": "converges to a number. You sort of know what this\nmeans, but let's just go",
    "start": "894160",
    "end": "904709"
  },
  {
    "text": "through it some more. So here's a.",
    "start": "904710",
    "end": "909890"
  },
  {
    "text": "We have our sequence of\nvalues as n increases.",
    "start": "909890",
    "end": "916200"
  },
  {
    "text": "What do we mean by the sequence\nconverging to a is that when you look at those\nvalues, they get closer and",
    "start": "916200",
    "end": "923550"
  },
  {
    "text": "closer to a. So this value here is your\ntypical a sub n.",
    "start": "923550",
    "end": "929570"
  },
  {
    "text": "They get closer and closer to\na, and they stay closer. So let's try to make\nthat more precise.",
    "start": "929570",
    "end": "936860"
  },
  {
    "text": "What it means is let's\nfix a sense of what it means to be close.",
    "start": "936860",
    "end": "942250"
  },
  {
    "text": "Let me look at an interval that\ngoes from a - epsilon to",
    "start": "942250",
    "end": "947540"
  },
  {
    "text": "a + epsilon. Then if my sequence converges\nto a, this means that as n",
    "start": "947540",
    "end": "957280"
  },
  {
    "text": "increases, eventually the values\nof the sequence that I",
    "start": "957280",
    "end": "962810"
  },
  {
    "text": "get stay inside this band. Since they converge to a, this\nmeans that eventually they",
    "start": "962810",
    "end": "970430"
  },
  {
    "text": "will be smaller than\na + epsilon and bigger than a - epsilon.",
    "start": "970430",
    "end": "976310"
  },
  {
    "text": "So convergence means that\ngiven a band of positive",
    "start": "976310",
    "end": "981320"
  },
  {
    "text": "length around the number a,\nthe values of the sequence that you get eventually\nget inside and",
    "start": "981320",
    "end": "988720"
  },
  {
    "text": "stay inside that band. So that's sort of the picture\ndefinition of",
    "start": "988720",
    "end": "994060"
  },
  {
    "text": "what convergence means. So now let's translate this into\na mathematical statement.",
    "start": "994060",
    "end": "1000460"
  },
  {
    "text": "Given a band of positive length,\nno matter how wide",
    "start": "1000460",
    "end": "1005610"
  },
  {
    "text": "that band is or how narrow it\nis, so for every epsilon",
    "start": "1005610",
    "end": "1010690"
  },
  {
    "text": "positive, eventually the\nsequence gets inside the band.",
    "start": "1010690",
    "end": "1016500"
  },
  {
    "text": "What does eventually mean? There exists a time,\nso that after that time something happens.",
    "start": "1016500",
    "end": "1023509"
  },
  {
    "text": "And the something that happens\nis that after that time, we are inside that band.",
    "start": "1023510",
    "end": "1029520"
  },
  {
    "text": "So this is a formal mathematical\ndefinition, which actually translates what I was\ntelling in the wordy way",
    "start": "1029520",
    "end": "1037250"
  },
  {
    "text": "before, and showing in\nterms of the picture. Given a certain band, even if\nit's narrow, eventually, after",
    "start": "1037250",
    "end": "1045140"
  },
  {
    "text": "a certain time n0, the values\nof the sequence are going to stay inside this band.",
    "start": "1045140",
    "end": "1050240"
  },
  {
    "text": "Now, if I were to take epsilon\nto be very small, this thing",
    "start": "1050240",
    "end": "1055770"
  },
  {
    "text": "would still be true that\neventually I'm going to get inside of the band, except that\nI may have to wait longer",
    "start": "1055770",
    "end": "1062400"
  },
  {
    "text": "for the values to\nget inside here. All right, that's what it means\nfor a deterministic",
    "start": "1062400",
    "end": "1068400"
  },
  {
    "text": "sequence to converge\nto something. Now, how about random\nvariables.",
    "start": "1068400",
    "end": "1074150"
  },
  {
    "text": "What does it mean for a sequence\nof random variables to converge to a number?",
    "start": "1074150",
    "end": "1080279"
  },
  {
    "text": "We're just going to twist\na little bit of the word definition. For numbers, we said that\neventually the numbers get",
    "start": "1080280",
    "end": "1088390"
  },
  {
    "text": "inside that band. But if instead of numbers we\nhave random variables with a certain distribution, so here\ninstead of a_n we're dealing",
    "start": "1088390",
    "end": "1098080"
  },
  {
    "text": "with a random variable that has\na distribution, let's say, of this kind, what we want is\nthat this distribution gets",
    "start": "1098080",
    "end": "1106650"
  },
  {
    "text": "inside this band, so it gets\nconcentrated inside here. What does it means that\nthe distribution",
    "start": "1106650",
    "end": "1113149"
  },
  {
    "text": "gets inside this band? I mean a random variable\nhas a distribution. It may have some tails, so\nmaybe not the entire",
    "start": "1113150",
    "end": "1120130"
  },
  {
    "text": "distribution gets concentrated\ninside of the band. But we want that more and more\nof this distribution is",
    "start": "1120130",
    "end": "1128660"
  },
  {
    "text": "concentrated in this band. So that -- in a sense that -- the probability of falling\noutside the band converges to",
    "start": "1128660",
    "end": "1137070"
  },
  {
    "text": "0 -- becomes smaller\nand smaller. So in words, we're going to say\nthat the sequence random",
    "start": "1137070",
    "end": "1145660"
  },
  {
    "text": "variables or a sequence of\nprobability distributions, that would be the same,\nconverges to a particular",
    "start": "1145660",
    "end": "1152059"
  },
  {
    "text": "number a if the following\nis true. If I consider a small band\naround a, then the probability",
    "start": "1152060",
    "end": "1162320"
  },
  {
    "text": "that my random variable falls\noutside this band, which is the area under this curve,\nthis probability becomes",
    "start": "1162320",
    "end": "1169530"
  },
  {
    "text": "smaller and smaller as\nn goes to infinity. The probability of being\noutside this band",
    "start": "1169530",
    "end": "1175370"
  },
  {
    "text": "converges to 0. So that's the intuitive idea.",
    "start": "1175370",
    "end": "1180620"
  },
  {
    "text": "So in the beginning, maybe our\ndistribution is sitting everywhere.",
    "start": "1180620",
    "end": "1186590"
  },
  {
    "text": "As n increases, the distribution\nstarts to get concentrating inside the band. When a is even bigger, our\ndistribution is even more",
    "start": "1186590",
    "end": "1197300"
  },
  {
    "text": "inside that band, so that these\noutside probabilities become smaller and smaller.",
    "start": "1197300",
    "end": "1202460"
  },
  {
    "text": "So the corresponding\nmathematical statement is the following. I fix a band around\na, a +/- epsilon.",
    "start": "1202460",
    "end": "1213730"
  },
  {
    "text": "Given that band, the probability\nof falling outside this band, this probability\nconverges to 0.",
    "start": "1213730",
    "end": "1221350"
  },
  {
    "text": "Or another way to say it is\nthat the limit of this probability is equal to 0.",
    "start": "1221350",
    "end": "1226559"
  },
  {
    "text": "If you were to translate this\ninto a complete mathematical statement, you would have\nto write down the",
    "start": "1226560",
    "end": "1231799"
  },
  {
    "text": "following messy thing. For every epsilon positive --",
    "start": "1231800",
    "end": "1237220"
  },
  {
    "text": "that's this statement -- the limit is 0. What does it mean that the\nlimit of something is 0?",
    "start": "1237220",
    "end": "1244610"
  },
  {
    "text": "We flip back to the\nprevious slide. Why? Because a probability\nis a number.",
    "start": "1244610",
    "end": "1251429"
  },
  {
    "text": "So here we're talking about\na sequence of numbers convergent to 0. What does it mean for a\nsequence of numbers to",
    "start": "1251430",
    "end": "1258190"
  },
  {
    "text": "converge to 0? It means that for any epsilon\nprime positive, there exists",
    "start": "1258190",
    "end": "1265320"
  },
  {
    "text": "some n0 such that for every\nn bigger than n0 the",
    "start": "1265320",
    "end": "1271230"
  },
  {
    "text": "following is true -- that this probability\nis less than or",
    "start": "1271230",
    "end": "1276450"
  },
  {
    "text": "equal to epsilon prime.  So the mathematical statement\nis a little hard to parse.",
    "start": "1276450",
    "end": "1287660"
  },
  {
    "text": "For every size of that band,\nand then you take the definition of what it means for\nthe limit of a sequence of",
    "start": "1287660",
    "end": "1294990"
  },
  {
    "text": "numbers to converge to 0. But it's a lot easier to\ndescribe this in words and,",
    "start": "1294990",
    "end": "1302340"
  },
  {
    "text": "basically, think in terms\nof this picture. That as n increases, the\nprobability of falling outside",
    "start": "1302340",
    "end": "1308690"
  },
  {
    "text": "those bands just become\nsmaller and smaller. So the statement is that our\ndistribution gets concentrated",
    "start": "1308690",
    "end": "1316590"
  },
  {
    "text": "in arbitrarily narrow little\nbands around that particular number a.",
    "start": "1316590",
    "end": "1325049"
  },
  {
    "text": "OK. So let's look at an example. Suppose a random variable Yn has\na discrete distribution of",
    "start": "1325050",
    "end": "1331660"
  },
  {
    "text": "this particular type. Does it converge to something?",
    "start": "1331660",
    "end": "1337150"
  },
  {
    "text": "Well, the probability\ndistribution of this random variable gets concentrated\nat 0 --",
    "start": "1337150",
    "end": "1342370"
  },
  {
    "text": "there's more and more\nprobability of being at 0. If I fix a band around 0 --",
    "start": "1342370",
    "end": "1349710"
  },
  {
    "text": "so if I take the band from minus\nepsilon to epsilon and",
    "start": "1349710",
    "end": "1354850"
  },
  {
    "text": "look at that band-- the probability of falling\noutside this band is 1/n.",
    "start": "1354850",
    "end": "1362350"
  },
  {
    "text": "As n goes to infinity, that\nprobability goes to 0. So in this case, we do\nhave convergence.",
    "start": "1362350",
    "end": "1370550"
  },
  {
    "text": "And Yn converges in probability\nto the number 0.",
    "start": "1370550",
    "end": "1376780"
  },
  {
    "text": "So this just captures the\nfacts obvious from this picture, that more and more of\nour probability distribution",
    "start": "1376780",
    "end": "1383680"
  },
  {
    "text": "gets concentrated around 0,\nas n goes to infinity. Now, an interesting thing to\nnotice is the following, that",
    "start": "1383680",
    "end": "1390330"
  },
  {
    "text": "even though Yn converges to 0,\nif you were to write down the",
    "start": "1390330",
    "end": "1395390"
  },
  {
    "text": "expected value for Yn,\nwhat would it be?",
    "start": "1395390",
    "end": "1400440"
  },
  {
    "text": "It's going to be n times the\nprobability of this value, which is 1/n.",
    "start": "1400440",
    "end": "1406240"
  },
  {
    "text": "So the expected value\nturns out to be 1. And if you were to look at the\nexpected value of Yn-squared,",
    "start": "1406240",
    "end": "1414299"
  },
  {
    "text": "this would be 0. times this probability, and\nthen n-squared times this",
    "start": "1414300",
    "end": "1421769"
  },
  {
    "text": "probability, which\nis equal to n. And this actually goes\nto infinity.",
    "start": "1421770",
    "end": "1429850"
  },
  {
    "text": "So we have this, perhaps,\nstrange situation where a random variable goes to 0, but\nthe expected value of this",
    "start": "1429850",
    "end": "1438029"
  },
  {
    "text": "random variable does\nnot go to 0. And the second moment of that\nrandom variable actually goes",
    "start": "1438030",
    "end": "1444570"
  },
  {
    "text": "to infinity. So this tells us that\nconvergence in probability tells you something,\nbut it doesn't tell",
    "start": "1444570",
    "end": "1451380"
  },
  {
    "text": "you the whole story. Convergence to 0 of a random\nvariable doesn't imply",
    "start": "1451380",
    "end": "1457260"
  },
  {
    "text": "anything about convergence\nof expected values or of variances and so on.",
    "start": "1457260",
    "end": "1463420"
  },
  {
    "text": "So the reason is that\nconvergence in probability tells you that this\ntail probability",
    "start": "1463420",
    "end": "1468470"
  },
  {
    "text": "here is very small. But it doesn't tell you how\nfar does this tail go.",
    "start": "1468470",
    "end": "1474440"
  },
  {
    "text": "As in this example, the tail\nprobability is small, but that tail acts far away, so it\ngives a disproportionate",
    "start": "1474440",
    "end": "1483410"
  },
  {
    "text": "contribution to the expected\nvalue or the expected value squared. ",
    "start": "1483410",
    "end": "1493340"
  },
  {
    "text": "OK. So now we've got everything that\nwe need to go back to the",
    "start": "1493340",
    "end": "1499000"
  },
  {
    "text": "sample mean and study\nits properties. So the sad thing is\nthat we have a",
    "start": "1499000",
    "end": "1505460"
  },
  {
    "text": "sequence of random variables. They're independent. They have the same\ndistribution. And we assume that they\nhave a finite mean",
    "start": "1505460",
    "end": "1512789"
  },
  {
    "text": "and a finite variance. We're looking at the\nsample mean.",
    "start": "1512790",
    "end": "1518429"
  },
  {
    "text": "Now in principle, you can\ncalculate the probability distribution of the sample mean,\nbecause we know how to",
    "start": "1518430",
    "end": "1525090"
  },
  {
    "text": "find the distributions\nof sums of independent random variables. You use the convolution\nformula over and over.",
    "start": "1525090",
    "end": "1531029"
  },
  {
    "text": "But this is pretty\ncomplicated, so let's not look at that. Let's just look at expected\nvalues, variances, and the",
    "start": "1531030",
    "end": "1538919"
  },
  {
    "text": "probabilities that the sample\nmean is far away from the true mean.",
    "start": "1538920",
    "end": "1544309"
  },
  {
    "text": "So what is the expected value\nof this random variable? The expected value of a sum of\nrandom variables is the sum of",
    "start": "1544310",
    "end": "1551260"
  },
  {
    "text": "the expected values. ",
    "start": "1551260",
    "end": "1556320"
  },
  {
    "text": "And then we have this factor\nof n in the denominator. Each one of these expected\nvalues is mu, so we get mu.",
    "start": "1556320",
    "end": "1567039"
  },
  {
    "text": "So the sample mean, the average\nvalue of this Mn in",
    "start": "1567040",
    "end": "1573960"
  },
  {
    "text": "expectation is the same as\nthe true mean inside our population.",
    "start": "1573960",
    "end": "1580620"
  },
  {
    "text": "Now here, this is a fine\nconceptual point, there's two",
    "start": "1580620",
    "end": "1586559"
  },
  {
    "text": "kinds of averages involved\nwhen you write down this expression. We understand that\nexpectations are",
    "start": "1586560",
    "end": "1593310"
  },
  {
    "text": "some kind of average. The sample mean is also an\naverage over the values that",
    "start": "1593310",
    "end": "1600250"
  },
  {
    "text": "we have observed. But it's two different\nkinds of averages. The sample mean is the average\nof the heights of the penguins",
    "start": "1600250",
    "end": "1610460"
  },
  {
    "text": "that we collected over\na single expedition. The expected value is to be\nthought of as follows, my",
    "start": "1610460",
    "end": "1619600"
  },
  {
    "text": "probabilistic experiment\nis one expedition to the South Pole. Expected value here means\nthinking on the average over a",
    "start": "1619600",
    "end": "1629760"
  },
  {
    "text": "huge number of expeditions. So my expedition is a random\nexperiment, I collect random",
    "start": "1629760",
    "end": "1636270"
  },
  {
    "text": "samples, and they record Mn.  The average result of an\nexpedition is what we would",
    "start": "1636270",
    "end": "1647169"
  },
  {
    "text": "get if we were to carry out\na zillion expeditions and average the averages that we\nget at each particular",
    "start": "1647170",
    "end": "1655050"
  },
  {
    "text": "expedition. So this Mn is the average during\na single expedition. This expectation is the average\nover an imagined",
    "start": "1655050",
    "end": "1664090"
  },
  {
    "text": "infinite sequence\nof expeditions. ",
    "start": "1664090",
    "end": "1669760"
  },
  {
    "text": "And of course, the other thing\nto always keep in mind is that expectations give you numbers,\nwhereas the sample mean is",
    "start": "1669760",
    "end": "1676909"
  },
  {
    "text": "actually a random variable. All right. So this random variable,\nhow random is it?",
    "start": "1676910",
    "end": "1683309"
  },
  {
    "text": "How big is its variance? So the variance of a sum of\nrandom variables is the sum of",
    "start": "1683310",
    "end": "1690040"
  },
  {
    "text": "the variances. But since we're dividing by n,\nwhen you calculate variances",
    "start": "1690040",
    "end": "1696610"
  },
  {
    "text": "this brings in a factor\nof n-squared. So the variance is sigma-squared\nover n. ",
    "start": "1696610",
    "end": "1704340"
  },
  {
    "text": "And in particular, the variance\nof the sample mean becomes smaller and smaller. It means that when you estimate\nthat average height",
    "start": "1704340",
    "end": "1711170"
  },
  {
    "text": "of penguins, if you take a\nlarge sample, then your estimate is not going\nto be too random.",
    "start": "1711170",
    "end": "1717529"
  },
  {
    "text": "The randomness in your estimates\nbecome small if you have a large sample size.",
    "start": "1717530",
    "end": "1723250"
  },
  {
    "text": "Having a large sample size kind\nof removes the randomness from your experiment. Now let's apply the Chebyshev\ninequality to say something",
    "start": "1723250",
    "end": "1732690"
  },
  {
    "text": "about tail probabilities\nfor the sample mean. The probability that you are\nmore than epsilon away from",
    "start": "1732690",
    "end": "1739610"
  },
  {
    "text": "the true mean is less than or\nequal to the variance of this quantity divided by this\nnumber squared.",
    "start": "1739610",
    "end": "1747030"
  },
  {
    "text": "So that's just the translation\nof the Chebyshev inequality to the particular context\nwe've got here.",
    "start": "1747030",
    "end": "1752320"
  },
  {
    "text": "We found the variance. It's sigma-squared over n. So we end up with\nthis expression.",
    "start": "1752320",
    "end": "1758340"
  },
  {
    "text": "So what does this\nexpression do? ",
    "start": "1758340",
    "end": "1765570"
  },
  {
    "text": "For any given epsilon, if\nI fix epsilon, then this",
    "start": "1765570",
    "end": "1772370"
  },
  {
    "text": "probability, which is less\nthan sigma-squared over n epsilon-squared, converges to\n0 as n goes to infinity.",
    "start": "1772370",
    "end": "1780550"
  },
  {
    "text": " And this is just the definition\nof convergence in",
    "start": "1780550",
    "end": "1788049"
  },
  {
    "text": "probability. If this happens, that the\nprobability of being more than",
    "start": "1788050",
    "end": "1794309"
  },
  {
    "text": "epsilon away from the mean, that\nprobability goes to 0, and this is true no matter how\nI choose my epsilon, then by",
    "start": "1794310",
    "end": "1801510"
  },
  {
    "text": "definition we have convergence\nin probability. So we have proved that the\nsample mean converges in",
    "start": "1801510",
    "end": "1808050"
  },
  {
    "text": "probability to the true mean. And this is what the weak law\nof large numbers tells us.",
    "start": "1808050",
    "end": "1816210"
  },
  {
    "text": "So in some vague sense, it\ntells us that the sample means, when you take the\naverage of many, many",
    "start": "1816210",
    "end": "1824350"
  },
  {
    "text": "measurements in your sample,\nthen the sample mean is a good estimate of the true mean in the\nsense that it approaches",
    "start": "1824350",
    "end": "1831870"
  },
  {
    "text": "the true mean as your sample\nsize increases. It approaches the true mean,\nbut of course in a very",
    "start": "1831870",
    "end": "1839220"
  },
  {
    "text": "specific sense, in probability,\naccording to this notion of convergence\nthat we have used.",
    "start": "1839220",
    "end": "1846550"
  },
  {
    "text": "So since we're talking about\nsampling, let's go over an example, which is the typical\nsituation faced by someone",
    "start": "1846550",
    "end": "1856150"
  },
  {
    "text": "who's constructing a poll. So you're interested in some\nproperty of the population.",
    "start": "1856150",
    "end": "1862679"
  },
  {
    "text": "So what fraction of\nthe population prefers Coke to Pepsi?",
    "start": "1862680",
    "end": "1868380"
  },
  {
    "text": "So there's a number f, which\nis that fraction of the population. And so this is an\nexact number.",
    "start": "1868380",
    "end": "1876260"
  },
  {
    "text": "So out of a population of 100\nmillion, 20 million prefer Coke, then f would be 0.2.",
    "start": "1876260",
    "end": "1885590"
  },
  {
    "text": "We want to find out what\nthat fraction is. We cannot ask everyone. What we're going to do is to\ntake a random sample of people",
    "start": "1885590",
    "end": "1894250"
  },
  {
    "text": "and ask them for their\npreferences. So the ith person either says\nyes for Coke or no.",
    "start": "1894250",
    "end": "1902690"
  },
  {
    "text": "And we record that by putting\na 1 each time that we get a yes answer.",
    "start": "1902690",
    "end": "1909160"
  },
  {
    "text": "And then we form the average\nof these x's. What is this average? It's the number of 1's that\nwe got divided by n.",
    "start": "1909160",
    "end": "1917000"
  },
  {
    "text": "So this is a fraction, but\ncalculated only on the basis",
    "start": "1917000",
    "end": "1922590"
  },
  {
    "text": "of the sample that we have. So you can think of this as\nbeing an estimate, f_hat,",
    "start": "1922590",
    "end": "1930260"
  },
  {
    "text": "based on the sample\nthat we have. Now, even though we used the\nlower case letter here, this",
    "start": "1930260",
    "end": "1937155"
  },
  {
    "text": "f_hat is, of course,\na random variable. f is a number.",
    "start": "1937155",
    "end": "1943299"
  },
  {
    "text": "This is the true fraction in\nthe overall population. f_hat is the estimate\nthat we get by using",
    "start": "1943300",
    "end": "1950380"
  },
  {
    "text": "our particular sample. Ok. So your boss told you, I need to\nknow what f is, but go and",
    "start": "1950380",
    "end": "1958760"
  },
  {
    "text": "do some sampling. What are you going to respond? Unless I ask everyone in the\nwhole population, there's no",
    "start": "1958760",
    "end": "1966360"
  },
  {
    "text": "way for me to know f exactly. Right?",
    "start": "1966360",
    "end": "1971890"
  },
  {
    "text": "There's no way. OK, so the boss tells you, well\nOK, then that'll me f",
    "start": "1971890",
    "end": "1979040"
  },
  {
    "text": "within an accuracy. I want an answer from you,\nthat's your answer, which is",
    "start": "1979040",
    "end": "1990910"
  },
  {
    "text": "close to the correct answer\nwithin 1 % point. So if the true f is 0.4, your\nanswer should be somewhere",
    "start": "1990910",
    "end": "2000260"
  },
  {
    "text": "between 0.39 and 0.41. I want a really accurate\nanswer.",
    "start": "2000260",
    "end": "2005520"
  },
  {
    "text": "What are you going to say? Well, there's no guarantee\nthat my answer",
    "start": "2005520",
    "end": "2011360"
  },
  {
    "text": "will be within 1 %. Maybe I'm unlucky and I just\nhappen to sample the wrong set",
    "start": "2011360",
    "end": "2017320"
  },
  {
    "text": "of people and my answer\ncomes out to be wrong. So I cannot give you a hard\nguarantee that this inequality",
    "start": "2017320",
    "end": "2025799"
  },
  {
    "text": "will be satisfied. But perhaps, I can give you a\nguarantee that this inequality",
    "start": "2025800",
    "end": "2031990"
  },
  {
    "text": "will be satisfied, this accuracy\nrequirement will be satisfied, with high\nconfidence.",
    "start": "2031990",
    "end": "2039340"
  },
  {
    "text": "That is, there's going to be\na smaller probability that things go wrong, that\nI'm unlikely",
    "start": "2039340",
    "end": "2044420"
  },
  {
    "text": "and I use a bad sample. But leaving aside that smaller\nprobability of being unlucky,",
    "start": "2044420",
    "end": "2050750"
  },
  {
    "text": "my answer will be accurate\nwithin the accuracy requirement that you have.",
    "start": "2050750",
    "end": "2056100"
  },
  {
    "text": "So these two numbers are the\nusual specs that one has when designing polls.",
    "start": "2056100",
    "end": "2062010"
  },
  {
    "text": "So this number is the accuracy\nthat we want.",
    "start": "2062010",
    "end": "2067370"
  },
  {
    "text": "It's the desired accuracy. And this number has to do with\nthe confidence that we want.",
    "start": "2067370",
    "end": "2075239"
  },
  {
    "text": "So 1 minus that number, we could\ncall it the confidence that we want out\nof our sample.",
    "start": "2075239",
    "end": "2083500"
  },
  {
    "text": "So this is really 1\nminus confidence. So now your job is to figure out\nhow large an n, how large",
    "start": "2083500",
    "end": "2091830"
  },
  {
    "text": "a sample should you be using, in\norder to satisfy the specs that your boss gave you.",
    "start": "2091830",
    "end": "2099060"
  },
  {
    "text": "All you know at this stage is\nthe Chebyshev inequality. So you just try to use it.",
    "start": "2099060",
    "end": "2105210"
  },
  {
    "text": "The probability of getting an\nanswer that's more than 0.01 away from the true answer is, by\nChebyshev's inequality, the",
    "start": "2105210",
    "end": "2114780"
  },
  {
    "text": "variance of this random variable\ndivided by this",
    "start": "2114780",
    "end": "2120170"
  },
  {
    "text": "number squared. The variance, as we argued\na little earlier, is the",
    "start": "2120170",
    "end": "2125870"
  },
  {
    "text": "variance of the x's\ndivided by n. So we get this expression.",
    "start": "2125870",
    "end": "2131829"
  },
  {
    "text": "So we would like this\nnumber to be less than or equal to 0.05.",
    "start": "2131830",
    "end": "2138330"
  },
  {
    "text": "OK, here we hit a little\nbit off a difficulty. The variance, (sigma_x)-squared,\nwhat is it?",
    "start": "2138330",
    "end": "2149040"
  },
  {
    "text": "(Sigma_x)-squared is, if you\nremember the variance of a Bernoulli random variable,\nis this quantity.",
    "start": "2149040",
    "end": "2158010"
  },
  {
    "text": "But we don't know it. f is what we're trying to\nestimate in the first place. So the variance is not known,\nso I cannot plug in a number",
    "start": "2158010",
    "end": "2166789"
  },
  {
    "text": "inside here. What I can do is to be\nconservative and use an upper",
    "start": "2166790",
    "end": "2172340"
  },
  {
    "text": "bound of the variance. How large can this number get? Well, you can plot\nf times (1-f).",
    "start": "2172340",
    "end": "2180090"
  },
  {
    "start": "2180090",
    "end": "2185950"
  },
  {
    "text": "It's a parabola. It has a root at 0 and at 1. So the maximum value is going to\nbe, by symmetry, at 1/2 and",
    "start": "2185950",
    "end": "2194450"
  },
  {
    "text": "when f is 1/2, then this\nvariance becomes 1/4. So I don't know\n(sigma_x)-squared, but I'm",
    "start": "2194450",
    "end": "2202340"
  },
  {
    "text": "going to use the worst case\nvalue for (sigma_x)-squared, which is 4.",
    "start": "2202340",
    "end": "2208480"
  },
  {
    "text": "And this is now an inequality\nthat I know to be always true. I've got my specs, and my specs\ntell me that I want this",
    "start": "2208480",
    "end": "2216910"
  },
  {
    "text": "number to be less than 0.05. And given what I know, the best\nthing I can do is to say,",
    "start": "2216910",
    "end": "2224980"
  },
  {
    "text": "OK, I'm going to take\nthis number and make it less than 0.05.",
    "start": "2224980",
    "end": "2234070"
  },
  {
    "text": "If I choose my n so that this\nis less than 0.05, then I'm",
    "start": "2234070",
    "end": "2240860"
  },
  {
    "text": "certain that this probability\nis also less than 0.05. What does it take for this\ninequality to be true?",
    "start": "2240860",
    "end": "2248720"
  },
  {
    "text": "You can solve for n here, and\nyou find that to satisfy this",
    "start": "2248720",
    "end": "2256369"
  },
  {
    "text": "inequality, n should be larger\nthan or equal to 50,000. So you can just let n\nbe equal to 50,000.",
    "start": "2256370",
    "end": "2264250"
  },
  {
    "text": "So the Chebyshev inequality\ntells us that if you take n equal to 50,000, then by the\nChebyshev inequality, we're",
    "start": "2264250",
    "end": "2271940"
  },
  {
    "text": "guaranteed to satisfy the specs\nthat we were given.",
    "start": "2271940",
    "end": "2277849"
  },
  {
    "text": "Ok. Now, 50,000 is a bit of\na large sample size.",
    "start": "2277850",
    "end": "2283950"
  },
  {
    "text": "Right? If you read anything in the\nnewspapers where they say so",
    "start": "2283950",
    "end": "2289490"
  },
  {
    "text": "much of the voters think this\nand that, this was determined on the basis of a sample of\n1,200 likely voters or so.",
    "start": "2289490",
    "end": "2299830"
  },
  {
    "text": "So the numbers that you will\ntypically see in these news items about polling, they\nusually involve sample sizes",
    "start": "2299830",
    "end": "2307589"
  },
  {
    "text": "about the 1,000 or so. You will never see a sample\nsize of 50,000.",
    "start": "2307590",
    "end": "2315250"
  },
  {
    "text": "That's too much. So where can we cut\nsome corners?",
    "start": "2315250",
    "end": "2321670"
  },
  {
    "text": "Well, we can cut corners\nbasically in three places. This requirement is a\nlittle too tight.",
    "start": "2321670",
    "end": "2329950"
  },
  {
    "text": "Newspaper stories will usually\ntell you, we have an accuracy of +/- 3 % points, instead\nof 1 % point.",
    "start": "2329950",
    "end": "2338800"
  },
  {
    "text": "And because this number comes up\nas a square, by making it 3 % points instead of 1, saves\nyou a factor of 10.",
    "start": "2338800",
    "end": "2349000"
  },
  {
    "text": "Then, the five percent\nconfidence, I guess that's usually OK.",
    "start": "2349000",
    "end": "2355180"
  },
  {
    "text": "If we use that factor of 10,\nthen we make our sample that we gain from here, then we get\na sample size of 10,000.",
    "start": "2355180",
    "end": "2363730"
  },
  {
    "text": "And that's, again,\na little too big. So where can we fix things? Well, it turns out that this\ninequality that we're using",
    "start": "2363730",
    "end": "2371140"
  },
  {
    "text": "here, Chebyshev's inequality,\nis just an inequality. It's not that tight.",
    "start": "2371140",
    "end": "2376890"
  },
  {
    "text": "It's not very accurate. Maybe there's a better way of\ncalculating or estimating this",
    "start": "2376890",
    "end": "2382800"
  },
  {
    "text": "quantity, which is smaller\nthan this. And using a more accurate\ninequality or a more accurate",
    "start": "2382800",
    "end": "2389770"
  },
  {
    "text": "bound, then we can convince\nourselves that we can settle",
    "start": "2389770",
    "end": "2395320"
  },
  {
    "text": "with a smaller sample size. This more accurate kind of\ninequality comes out of a",
    "start": "2395320",
    "end": "2401770"
  },
  {
    "text": "difference limit theorem,\nwhich is the next limit theorem we're going\nto consider. We're going to start the\ndiscussion today, but we're",
    "start": "2401770",
    "end": "2408310"
  },
  {
    "text": "going to continue with\nit next week. Before I tell you exactly what\nthat other limit theorem says,",
    "start": "2408310",
    "end": "2418750"
  },
  {
    "text": "let me give you the\nbig picture of what's involved here.",
    "start": "2418750",
    "end": "2424760"
  },
  {
    "text": "We're dealing with sums of\ni.i.d random variables. Each X has a distribution\nof its own.",
    "start": "2424760",
    "end": "2432299"
  },
  {
    "text": " So suppose that X has a\ndistribution which is",
    "start": "2432300",
    "end": "2441190"
  },
  {
    "text": "something like this. This is the density of X. If I\nadd lots of X's together, what",
    "start": "2441190",
    "end": "2448560"
  },
  {
    "text": "kind of distribution\ndo I expect? The mean is going to be\nn times the mean of an",
    "start": "2448560",
    "end": "2455170"
  },
  {
    "text": "individual X. So if this is mu,\nI'm going to get a mean of",
    "start": "2455170",
    "end": "2460559"
  },
  {
    "text": "n times mu. But my variance will\nalso increase.",
    "start": "2460560",
    "end": "2466619"
  },
  {
    "text": "When I add the random\nvariables, I'm adding the variances. So since the variance increases,\nwe're going to get",
    "start": "2466620",
    "end": "2473369"
  },
  {
    "text": "a distribution that's\npretty wide. So this is the density of X1\nplus all the way to Xn.",
    "start": "2473370",
    "end": "2483240"
  },
  {
    "text": "So as n increases, my\ndistribution shifts, because the mean is positive.",
    "start": "2483240",
    "end": "2488769"
  },
  {
    "text": "So I keep adding things. And also, my distribution\nbecomes wider and wider.",
    "start": "2488770",
    "end": "2493870"
  },
  {
    "text": "The variance increases. Well, we started a different\nscaling.",
    "start": "2493870",
    "end": "2499260"
  },
  {
    "text": "We started a scaled version of\nthis quantity when we looked at the weak law of\nlarge numbers.",
    "start": "2499260",
    "end": "2506180"
  },
  {
    "text": "In the weak law of large\nnumbers, we take this random variable and divide it by n.",
    "start": "2506180",
    "end": "2512140"
  },
  {
    "text": "And what the weak law tells us\nis that we're going to get a distribution that's very highly\nconcentrated around the",
    "start": "2512140",
    "end": "2521050"
  },
  {
    "text": "true mean, which is mu. So this here would be the\ndensity of X1 plus",
    "start": "2521050",
    "end": "2527520"
  },
  {
    "text": "Xn divided by n.",
    "start": "2527520",
    "end": "2532630"
  },
  {
    "text": "Because I've divided by n, the\nmean has become the original mean, which is mu.",
    "start": "2532630",
    "end": "2539410"
  },
  {
    "text": "But the weak law of large\nnumbers tells us that the distribution of this random\nvariable is very concentrated",
    "start": "2539410",
    "end": "2546650"
  },
  {
    "text": "around the mean. So we get a distribution\nthat's very narrow in this kind. In the limit, this distribution\nbecomes one",
    "start": "2546650",
    "end": "2554230"
  },
  {
    "text": "that's just concentrated\non top of mu. So it's sort of a degenerate\ndistribution.",
    "start": "2554230",
    "end": "2560930"
  },
  {
    "text": "So these are two extremes, no\nscaling for the sum, a scaling",
    "start": "2560930",
    "end": "2566069"
  },
  {
    "text": "where we divide by n. In this extreme, we get the\ntrivial case of a distribution that flattens out completely.",
    "start": "2566070",
    "end": "2572859"
  },
  {
    "text": "In this scaling, we get a\ndistribution that gets concentrated around\na single point.",
    "start": "2572860",
    "end": "2579150"
  },
  {
    "text": "Again, we look at some\nintermediate scaling that makes things more interesting. Things do become interesting\nif we scale by dividing the",
    "start": "2579150",
    "end": "2589700"
  },
  {
    "text": "sum by square root of n instead\nof dividing by n. What effect does this have?",
    "start": "2589700",
    "end": "2597210"
  },
  {
    "text": "When we scale by dividing by\nsquare root of n, the variance",
    "start": "2597210",
    "end": "2602510"
  },
  {
    "text": "of Sn over square root of n is\ngoing to be the variance of Sn",
    "start": "2602510",
    "end": "2608050"
  },
  {
    "text": "over sum divided by n. That's how variances behave. The variance of Sn is n\nsigma-squared, divide by n,",
    "start": "2608050",
    "end": "2617370"
  },
  {
    "text": "which is sigma squared, which\nmeans that when we scale in this particular way,\nas n changes, the",
    "start": "2617370",
    "end": "2625940"
  },
  {
    "text": "variance doesn't change. So the width of our\ndistribution will be sort of constant.",
    "start": "2625940",
    "end": "2632190"
  },
  {
    "text": "The distribution changes shape,\nbut it doesn't become narrower as was the case here.",
    "start": "2632190",
    "end": "2639910"
  },
  {
    "text": "It doesn't become wider, kind\nof keeps the same width. So perhaps in the limit, this\ndistribution is going to take",
    "start": "2639910",
    "end": "2649260"
  },
  {
    "text": "an interesting shape. And that's indeed the case. So let's do what\nwe did before.",
    "start": "2649260",
    "end": "2659799"
  },
  {
    "text": "So we're looking at the sum, and\nwe want to divide the sum",
    "start": "2659800",
    "end": "2665110"
  },
  {
    "text": "by something that goes like\nsquare root of n. So the variance of Sn\nis n sigma squared.",
    "start": "2665110",
    "end": "2673140"
  },
  {
    "text": "The variance of the sigma Sn\nis the square root of that.",
    "start": "2673140",
    "end": "2678240"
  },
  {
    "text": "It's this number. So effectively, we're scaling\nby order of square root n.",
    "start": "2678240",
    "end": "2683930"
  },
  {
    "text": "Now, I'm doing another\nthing here. If my random variable has a\npositive mean, then this",
    "start": "2683930",
    "end": "2692350"
  },
  {
    "text": "quantity is going to\nhave a mean that's positive and growing. It's going to be shifting\nto the right.",
    "start": "2692350",
    "end": "2699450"
  },
  {
    "text": "Why is that? Sn has a mean that's\nproportional to n. When I divide by square root n,\nthen it means that the mean",
    "start": "2699450",
    "end": "2709510"
  },
  {
    "text": "scales like square root of n. So my distribution would\nstill keep shifting",
    "start": "2709510",
    "end": "2714740"
  },
  {
    "text": "after I do this division. I want to keep my distribution\nin place, so I subtract out",
    "start": "2714740",
    "end": "2720859"
  },
  {
    "text": "the mean of Sn. So what we're doing here is\na standard technique or",
    "start": "2720860",
    "end": "2729579"
  },
  {
    "text": "transformation where you take\na random variable and you so-called standardize it.",
    "start": "2729580",
    "end": "2734890"
  },
  {
    "text": "I remove the mean of that random\nvariable and I divide by the standard deviation.",
    "start": "2734890",
    "end": "2740100"
  },
  {
    "text": "This results in a random\nvariable that has 0 mean and unit variance. What Zn measures is the\nfollowing, Zn tells me how",
    "start": "2740100",
    "end": "2749880"
  },
  {
    "text": "many standard deviations am\nI away from the mean.",
    "start": "2749880",
    "end": "2755519"
  },
  {
    "text": "Sn minus (n times expected value\nof X) tells me how much is Sn away from the\nmean value of Sn.",
    "start": "2755520",
    "end": "2762980"
  },
  {
    "text": "And by dividing by the standard\ndeviation of Sn -- this tells me how many standard\ndeviations away from",
    "start": "2762980",
    "end": "2769830"
  },
  {
    "text": "the mean am I. So we're going to look at this\nrandom variable, which is just",
    "start": "2769830",
    "end": "2775360"
  },
  {
    "text": "a transformation Zn. It's a linear transformation\nof Sn.",
    "start": "2775360",
    "end": "2780840"
  },
  {
    "text": "S And we're going to compare\nthis random variable to a standard normal random\nvariable.",
    "start": "2780840",
    "end": "2787230"
  },
  {
    "text": "So a standard normal is the\nrandom variable that you are familiar with, given by the\nusual formula, and for which",
    "start": "2787230",
    "end": "2795200"
  },
  {
    "text": "we have tables for it. This Zn has 0 mean and\nunit variance.",
    "start": "2795200",
    "end": "2800400"
  },
  {
    "text": "So in that respect, it has the\nsame statistics as the standard normal.",
    "start": "2800400",
    "end": "2805655"
  },
  {
    "text": "The distribution of Zn\ncould be anything -- can be pretty messy.",
    "start": "2805655",
    "end": "2810770"
  },
  {
    "text": "But there is this amazing\ntheorem called the central limit theorem that tells us that\nthe distribution of Zn",
    "start": "2810770",
    "end": "2818250"
  },
  {
    "text": "approaches the distribution of\nthe standard normal in the following sense, that\nprobability is that you can",
    "start": "2818250",
    "end": "2826270"
  },
  {
    "text": "calculate -- of this type -- that you can calculate\nfor Zn -- is the limit becomes the same as\nthe probabilities that you",
    "start": "2826270",
    "end": "2833330"
  },
  {
    "text": "would get from the standard\nnormal tables for Z. It's a statement about\nthe cumulative",
    "start": "2833330",
    "end": "2839750"
  },
  {
    "text": "distribution functions. This quantity, as a function\nof c, is the cumulative",
    "start": "2839750",
    "end": "2845060"
  },
  {
    "text": "distribution function of\nthe random variable Zn. This is the cumulative\ndistribution function of the",
    "start": "2845060",
    "end": "2850859"
  },
  {
    "text": "standard normal. The central limit theorem tells\nus that the cumulative distribution function of the\nsum of a number of random",
    "start": "2850860",
    "end": "2859340"
  },
  {
    "text": "variables, after they're\nappropriately standardized, approaches the cumulative\ndistribution function over the",
    "start": "2859340",
    "end": "2866480"
  },
  {
    "text": "standard normal distribution. In particular, this tells\nus that we can calculate",
    "start": "2866480",
    "end": "2873620"
  },
  {
    "text": "probabilities for Zn when n is\nlarge by calculating instead",
    "start": "2873620",
    "end": "2879480"
  },
  {
    "text": "probabilities for Z. And that's\ngoing to be a good approximation. Probabilities for Z are easy to\ncalculate because they're",
    "start": "2879480",
    "end": "2887670"
  },
  {
    "text": "well tabulated. So we get a very nice shortcut\nfor calculating",
    "start": "2887670",
    "end": "2892820"
  },
  {
    "text": "probabilities for Zn. Now, it's not Zn that you're\ninterested in.",
    "start": "2892820",
    "end": "2897990"
  },
  {
    "text": "What you're interested\nin is Sn. And Sn --",
    "start": "2897990",
    "end": "2903819"
  },
  {
    "text": "inverting this relation\nhere --",
    "start": "2903820",
    "end": "2909080"
  },
  {
    "text": "Sn is square root n sigma\nZn plus n expected",
    "start": "2909080",
    "end": "2918330"
  },
  {
    "text": "value of X. All right. Now, if you can calculate\nprobabilities for Zn, even",
    "start": "2918330",
    "end": "2926620"
  },
  {
    "text": "approximately, then you can\ncertainly calculate probabilities for Sn, because\none is a linear",
    "start": "2926620",
    "end": "2933289"
  },
  {
    "text": "function of the other. And we're going to do a little\nbit of that next time.",
    "start": "2933290",
    "end": "2938710"
  },
  {
    "text": "You're going to get, also, some\npractice in recitation. At a more vague level, you could\ndescribe the central",
    "start": "2938710",
    "end": "2944974"
  },
  {
    "text": "limit theorem as saying the\nfollowing, when n is large, you can pretend that Zn is\na standard normal random",
    "start": "2944975",
    "end": "2952160"
  },
  {
    "text": "variable and do the calculations\nas if Zn was standard normal. Now, pretending that Zn is\nnormal is the same as",
    "start": "2952160",
    "end": "2961530"
  },
  {
    "text": "pretending that Sn is normal,\nbecause Sn is a linear function of Zn.",
    "start": "2961530",
    "end": "2967700"
  },
  {
    "text": "And we know that linear\nfunctions of normal random variables are normal. So the central limit theorem\nessentially tells us that we",
    "start": "2967700",
    "end": "2976289"
  },
  {
    "text": "can pretend that Sn is a normal\nrandom variable and do the calculations just as if it\nwere a normal random variable.",
    "start": "2976290",
    "end": "2984760"
  },
  {
    "text": "Mathematically speaking though,\nthe central limit theorem does not talk about\nthe distribution of Sn,",
    "start": "2984760",
    "end": "2990480"
  },
  {
    "text": "because the distribution of Sn\nbecomes degenerate in the limit, just a very flat\nand long thing.",
    "start": "2990480",
    "end": "2997650"
  },
  {
    "text": "So strictly speaking\nmathematically, it's a statement about cumulative\ndistributions of Zn's.",
    "start": "2997650",
    "end": "3003060"
  },
  {
    "text": "Practically, the way you use it\nis by just pretending that Sn is normal.",
    "start": "3003060",
    "end": "3008415"
  },
  {
    "text": "Very good. Enjoy the Thanksgiving Holiday. ",
    "start": "3008415",
    "end": "3012330"
  }
]