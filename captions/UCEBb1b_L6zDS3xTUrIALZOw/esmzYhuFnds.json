[
  {
    "start": "0",
    "end": "463000"
  },
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6760"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation, or to\nview additional materials",
    "start": "6760",
    "end": "13390"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "13390",
    "end": "18570"
  },
  {
    "text": " JOHN GUTTAG: I'm\na little reluctant to say good afternoon,\ngiven the weather,",
    "start": "18570",
    "end": "25880"
  },
  {
    "text": "but I'll say it anyway. I guess now we all do know\nthat we live in Boston.",
    "start": "25880",
    "end": "32900"
  },
  {
    "text": "And I should say,\nI hope none of you were affected too much by the\nfire yesterday in Cambridge,",
    "start": "32900",
    "end": "39740"
  },
  {
    "text": "but that seems to have been\na pretty disastrous event for some. Anyway, here's the reading.",
    "start": "39740",
    "end": "45740"
  },
  {
    "text": "This is a chapter in\nthe book on clustering, a topic that Professor\nGrimson introduced last week.",
    "start": "45740",
    "end": "52610"
  },
  {
    "text": "And I'm going to try and finish\nup with respect to this course today, though not with\nrespect to everything",
    "start": "52610",
    "end": "60080"
  },
  {
    "text": "there is to know\nabout clustering. Quickly just reviewing\nwhere we were.",
    "start": "60080",
    "end": "67700"
  },
  {
    "text": "We're in the unit of a\ncourse on machine learning, and we always follow\nthe same paradigm.",
    "start": "67700",
    "end": "73190"
  },
  {
    "text": "We observe some set\nof examples, which we call the training data.",
    "start": "73190",
    "end": "78440"
  },
  {
    "text": "We try and infer something\nabout the process that created those examples.",
    "start": "78440",
    "end": "85450"
  },
  {
    "text": "And then we use inference\ntechniques, different kinds of techniques, to\nmake predictions",
    "start": "85450",
    "end": "90760"
  },
  {
    "text": "about previously unseen data. We call that the test data.",
    "start": "90760",
    "end": "96830"
  },
  {
    "text": "As Professor Grimson said, you\ncan think of two broad classes. Supervised, where we have a\nset of examples and some label",
    "start": "96830",
    "end": "104450"
  },
  {
    "text": "associated with the example-- Democrat, Republican,\nsmart, dumb,",
    "start": "104450",
    "end": "110600"
  },
  {
    "text": "whatever you want to\nassociate with them-- and then we try and\ninfer the labels.",
    "start": "110600",
    "end": "117920"
  },
  {
    "text": "Or unsupervised, where we're\ngiven a set of feature vectors without labels, and\nthen we attempt to group",
    "start": "117920",
    "end": "125659"
  },
  {
    "text": "them into natural clusters. That's going to be\ntoday's topic, clustering.",
    "start": "125660",
    "end": "133470"
  },
  {
    "text": "So clustering is an\noptimization problem. As we'll see later,\nsupervised machine learning",
    "start": "133470",
    "end": "140780"
  },
  {
    "text": "is also an optimization problem. Clustering's a\nrather simple one.",
    "start": "140780",
    "end": "146660"
  },
  {
    "text": "We're going to start first\nwith the notion of variability. So this little c is\na single cluster,",
    "start": "146660",
    "end": "154939"
  },
  {
    "text": "and we're going to talk about\nthe variability in that cluster of the sum of the distance\nbetween the mean of the cluster",
    "start": "154940",
    "end": "165440"
  },
  {
    "text": "and each example in the cluster. And then we square it.",
    "start": "165440",
    "end": "170920"
  },
  {
    "text": "OK? Pretty straightforward. For the moment,\nwe can just assume",
    "start": "170920",
    "end": "176510"
  },
  {
    "text": "that we're using Euclidean\ndistance as our distance metric. Minkowski with p equals two.",
    "start": "176510",
    "end": "184080"
  },
  {
    "text": "So variability should look\npretty similar to something",
    "start": "184080",
    "end": "190030"
  },
  {
    "text": "we've seen before, right? It's not quite variance,\nright, but it's very close.",
    "start": "190030",
    "end": "196099"
  },
  {
    "text": "In a minute, we'll look\nat why it's different. And then we can look\nat the dissimilarity",
    "start": "196100",
    "end": "203159"
  },
  {
    "text": "of a set of clusters, a group\nof clusters, which I'm writing as capital C, and\nthat's just the sum",
    "start": "203160",
    "end": "210600"
  },
  {
    "text": "of all the variabilities.  Now, if I had\ndivided variability",
    "start": "210600",
    "end": "220150"
  },
  {
    "text": "by the size of the\ncluster, what would I have?",
    "start": "220150",
    "end": "225514"
  },
  {
    "text": "Something we've seen before. What would that be? Somebody?",
    "start": "225514",
    "end": "231890"
  },
  {
    "text": "Isn't that just the variance? So the question is, why\nam I not doing that?",
    "start": "231890",
    "end": "237910"
  },
  {
    "text": "If up til now, we always\nwanted to talk about variance, why suddenly am I not doing it?",
    "start": "237910",
    "end": "245310"
  },
  {
    "text": "Why do I define this\nnotion of variability instead of good old variance?",
    "start": "245310",
    "end": "250750"
  },
  {
    "text": "Any thoughts?  What am I accomplishing\nby not dividing",
    "start": "250750",
    "end": "258299"
  },
  {
    "text": "by the size of the cluster? Or what would happen\nif I did divide by the size of the cluster?",
    "start": "258300",
    "end": "264420"
  },
  {
    "text": "Yes. AUDIENCE: You normalize it? JOHN GUTTAG: Absolutely. I'd normalize it.",
    "start": "264420",
    "end": "269720"
  },
  {
    "text": "That's exactly what\nit would be doing. And what might be good or\nbad about normalizing it?",
    "start": "269720",
    "end": "276380"
  },
  {
    "text": " What does it essentially\nmean to normalize?",
    "start": "276380",
    "end": "284280"
  },
  {
    "text": "It means that the\npenalty for a big cluster with a lot of variance\nin it is no higher",
    "start": "284280",
    "end": "291540"
  },
  {
    "text": "than the penalty of\na tiny little cluster with a lot of variance in it.",
    "start": "291540",
    "end": "296720"
  },
  {
    "text": "By not normalizing,\nwhat I'm saying is I want to penalize big,\nhighly-diverse clusters",
    "start": "296720",
    "end": "305510"
  },
  {
    "text": "more than small,\nhighly-diverse clusters. OK? And if you think about it,\nthat probably makes sense.",
    "start": "305510",
    "end": "312990"
  },
  {
    "text": " Big and bad is worse\nthan small and bad.",
    "start": "312990",
    "end": "318470"
  },
  {
    "text": " All right, so now we define\nthe objective function.",
    "start": "318470",
    "end": "326110"
  },
  {
    "text": "And can we say that the\noptimization problem we want to solve by clustering\nis simply finding a capital",
    "start": "326110",
    "end": "334470"
  },
  {
    "text": "C that minimizes dissimilarity? ",
    "start": "334470",
    "end": "341500"
  },
  {
    "text": "Is that a reasonable definition? ",
    "start": "341500",
    "end": "346742"
  },
  {
    "text": "Well, hint-- no. What foolish thing could\nwe do that would optimize",
    "start": "346743",
    "end": "354680"
  },
  {
    "text": "that objective function? Yeah. AUDIENCE: You could\nhave the same number of clusters as points?",
    "start": "354680",
    "end": "359720"
  },
  {
    "text": "JOHN GUTTAG: Yeah. I can have the same\nnumber of clusters as points, assign each point\nto its own cluster, whoops.",
    "start": "359720",
    "end": "367700"
  },
  {
    "text": "Ooh, almost a relay. The dissimilarity of\neach cluster would be 0.",
    "start": "367700",
    "end": "374520"
  },
  {
    "text": "The variability would be 0, so\nthe dissimilarity would be 0, and I just solved the problem.",
    "start": "374520",
    "end": "379629"
  },
  {
    "text": "Well, that's clearly not\na very useful thing to do. So, well, what do you think\nwe do to get around that?",
    "start": "379630",
    "end": "388870"
  },
  {
    "text": "Yeah. AUDIENCE: We apply a constraint? JOHN GUTTAG: We\napply a constraint. Exactly. ",
    "start": "388870",
    "end": "395830"
  },
  {
    "text": "And so we have to\npick some constraint. ",
    "start": "395830",
    "end": "402970"
  },
  {
    "text": "What would be a suitable\nconstraint, for example?",
    "start": "402970",
    "end": "408020"
  },
  {
    "text": "Well, maybe we'd\nsay, OK, the clusters have to have some minimum\ndistance between them.",
    "start": "408020",
    "end": "413449"
  },
  {
    "text": " Or-- and this is the constraint\nwe'll be using today--",
    "start": "413450",
    "end": "419580"
  },
  {
    "text": "we could constrain the\nnumber of clusters. Say, all right, I only want\nto have at most five clusters.",
    "start": "419580",
    "end": "427160"
  },
  {
    "text": "Do the best you can to\nminimize dissimilarity, but you're not allowed to\nuse more than five clusters.",
    "start": "427160",
    "end": "434630"
  },
  {
    "text": "That's the most\ncommon constraint that gets placed in the problem.",
    "start": "434630",
    "end": "440550"
  },
  {
    "text": "All right, we're going to\nlook at two algorithms. Maybe I should say two\nmethods, because there are multiple implementations\nof these methods.",
    "start": "440550",
    "end": "448780"
  },
  {
    "text": "The first is called\nhierarchical clustering, and the second is\ncalled k-means. There should be an S\non the word mean there.",
    "start": "448780",
    "end": "456460"
  },
  {
    "text": "Sorry about that. All right, let's look at\nhierarchical clustering first. ",
    "start": "456460",
    "end": "464330"
  },
  {
    "start": "463000",
    "end": "589000"
  },
  {
    "text": "It's a strange algorithm. We start by assigning\neach item, each example,",
    "start": "464330",
    "end": "471870"
  },
  {
    "text": "to its own cluster. So this is the trivial solution\nwe talked about before.",
    "start": "471870",
    "end": "477610"
  },
  {
    "text": "So if you have N items,\nyou now have N clusters, each containing just one item. ",
    "start": "477610",
    "end": "487050"
  },
  {
    "text": "In the next step, we find\nthe two most similar clusters",
    "start": "487050",
    "end": "492870"
  },
  {
    "text": "we have and merge them\ninto a single cluster, so that now instead\nof N clusters,",
    "start": "492870",
    "end": "499300"
  },
  {
    "text": "we have N minus 1 clusters. ",
    "start": "499300",
    "end": "506210"
  },
  {
    "text": "And we continue this\nprocess until all items are clustered into a\nsingle cluster of size N.",
    "start": "506210",
    "end": "514010"
  },
  {
    "text": "Now of course,\nthat's kind of silly, because if all I\nwanted to put them all it in is in\na single cluster,",
    "start": "514010",
    "end": "519830"
  },
  {
    "text": "I don't need to iterate. I just go wham, right? But what's interesting about\nhierarchical clustering",
    "start": "519830",
    "end": "526010"
  },
  {
    "text": "is you stop it, typically,\nsomewhere along the way. You produce something\ncalled a [? dendogram. ?]",
    "start": "526010",
    "end": "533960"
  },
  {
    "text": "Let me write that down. ",
    "start": "533960",
    "end": "542959"
  },
  {
    "text": "At each step here, it shows you\nwhat you've merged thus far.",
    "start": "542960",
    "end": "548920"
  },
  {
    "text": "We'll see an example\nof that shortly. And then you can have\nsome stopping criteria.",
    "start": "548920",
    "end": "554170"
  },
  {
    "text": "We'll talk about that. This is called\nagglomerative hierarchical",
    "start": "554170",
    "end": "559820"
  },
  {
    "text": "clustering because we start\nwith a bunch of things and we agglomerate them. That is to say, we\nput them together.",
    "start": "559820",
    "end": "568050"
  },
  {
    "text": "All right? Make sense? Well, there's a catch.",
    "start": "568050",
    "end": "574060"
  },
  {
    "text": "What do we mean by distance? And there are multiple plausible\ndefinitions of distance,",
    "start": "574060",
    "end": "582160"
  },
  {
    "text": "and you would get a\ndifferent answer depending upon which measure you used. ",
    "start": "582160",
    "end": "590410"
  },
  {
    "start": "589000",
    "end": "701000"
  },
  {
    "text": "These are called\nlinkage metrics. The most common one used\nis probably single-linkage,",
    "start": "590410",
    "end": "598040"
  },
  {
    "text": "and that says the distance\nbetween a pair of clusters is equal to the shortest\ndistance from any member of one",
    "start": "598040",
    "end": "606130"
  },
  {
    "text": "cluster to any member\nof the other cluster. ",
    "start": "606130",
    "end": "612100"
  },
  {
    "text": "So if I have two\nclusters, here and here,",
    "start": "612100",
    "end": "617579"
  },
  {
    "text": "and they have bunches\nof points in them, single-linkage distance\nwould say, well,",
    "start": "617580",
    "end": "623510"
  },
  {
    "text": "let's use these two points\nwhich are the closest, and the distance\nbetween these two",
    "start": "623510",
    "end": "629780"
  },
  {
    "text": "is the distance\nbetween the clusters. ",
    "start": "629780",
    "end": "637090"
  },
  {
    "text": "You can also use\ncomplete-linkage,",
    "start": "637090",
    "end": "643990"
  },
  {
    "text": "and that says the distance\nbetween any two clusters is equal to the greatest\ndistance from any member",
    "start": "643990",
    "end": "650170"
  },
  {
    "text": "to any other member. OK? So if we had the same\npicture we had before--",
    "start": "650170",
    "end": "656150"
  },
  {
    "start": "656150",
    "end": "661860"
  },
  {
    "text": "probably not the same\npicture, but it's a picture. Whoops.",
    "start": "661860",
    "end": "667450"
  },
  {
    "text": "Then we would say, well, I guess\ncomplete-linkage is probably the distance, maybe,\nbetween those two.",
    "start": "667450",
    "end": "672760"
  },
  {
    "start": "672760",
    "end": "679078"
  },
  {
    "text": "And finally, not\nsurprisingly, you",
    "start": "679078",
    "end": "684550"
  },
  {
    "text": "can take the average distance. These are all plausible metrics.",
    "start": "684550",
    "end": "691050"
  },
  {
    "text": "They're all used and practiced\nfor different kinds of results",
    "start": "691050",
    "end": "696450"
  },
  {
    "text": "depending upon the\napplication of the clustering. ",
    "start": "696450",
    "end": "702740"
  },
  {
    "start": "701000",
    "end": "1021000"
  },
  {
    "text": "All right, let's\nlook at an example. So what I have here\nis the air distance",
    "start": "702740",
    "end": "709070"
  },
  {
    "text": "between six different cities,\nBoston, New York, Chicago,",
    "start": "709070",
    "end": "715200"
  },
  {
    "text": "Denver, San Francisco,\nand Seattle. And now let's say we're-- want\nto cluster these airports just",
    "start": "715200",
    "end": "724910"
  },
  {
    "text": "based upon their distance. So we start. The first piece of our\n[? dendogram ?] says,",
    "start": "724910",
    "end": "732860"
  },
  {
    "text": "well, all right,\nI have six cities, I have six clusters,\neach containing one city. ",
    "start": "732860",
    "end": "742777"
  },
  {
    "text": "All right, what happens next?  What's the next level\ngoing to look like?",
    "start": "742777",
    "end": "750550"
  },
  {
    "text": "Yeah? AUDIENCE: You're going\nfrom Boston [INAUDIBLE] JOHN GUTTAG: I'm going to\njoin Boston and New York, as",
    "start": "750550",
    "end": "755620"
  },
  {
    "text": "improbable as that sounds. All right, so that's\nthe next level.",
    "start": "755620",
    "end": "762130"
  },
  {
    "text": "And if for some reason I only\nwanted to have five clusters, well, I could stop here.",
    "start": "762130",
    "end": "768890"
  },
  {
    "text": "Next, what happens?  Well, I look at it,\nI say well, I'll",
    "start": "768890",
    "end": "776100"
  },
  {
    "text": "join up Chicago with\nBoston and New York. ",
    "start": "776100",
    "end": "784320"
  },
  {
    "text": "All right. What do I get at the next level? Somebody? Yeah. AUDIENCE: Seattle [INAUDIBLE]",
    "start": "784320",
    "end": "792150"
  },
  {
    "text": "JOHN GUTTAG: Doesn't\nlook like it to me. If you look at San Francisco\nand Seattle, they are 808 miles,",
    "start": "792150",
    "end": "801130"
  },
  {
    "text": "and Denver and San\nFrancisco is 1,235.",
    "start": "801130",
    "end": "807140"
  },
  {
    "text": "So I'd end up, in fact, joining\nSan Francisco and Seattle. AUDIENCE: That's what I said.",
    "start": "807140",
    "end": "814130"
  },
  {
    "text": "JOHN GUTTAG: Well, that explains\nwhy I need my hearing fixed. [LAUGHTER]",
    "start": "814130",
    "end": "819380"
  },
  {
    "text": "All right. So I combine San\nFrancisco and Seattle,",
    "start": "819380",
    "end": "824480"
  },
  {
    "text": "and now it gets interesting. I have two choices with Denver.",
    "start": "824480",
    "end": "830230"
  },
  {
    "text": "Obviously, there are\nonly two choices,",
    "start": "830230",
    "end": "837519"
  },
  {
    "text": "and which I choose depends upon\nwhich linkage criterion I use.",
    "start": "837520",
    "end": "843280"
  },
  {
    "text": "If I'm using single-linkage,\nwell, then Denver gets joined with Boston,\nNew York, and Chicago,",
    "start": "843280",
    "end": "849910"
  },
  {
    "text": "because it's closer to Chicago\nthan it is to either San Francisco or Seattle. ",
    "start": "849910",
    "end": "857420"
  },
  {
    "text": "But if I use\ncomplete-linkage, it gets joined up with San\nFrancisco and Seattle,",
    "start": "857420",
    "end": "863949"
  },
  {
    "text": "because it is further from\nBoston than it is from,",
    "start": "863950",
    "end": "871060"
  },
  {
    "text": "I guess it's San\nFrancisco or Seattle. Whichever it is, right? So this is a place\nwhere you see what",
    "start": "871060",
    "end": "877920"
  },
  {
    "text": "answer I get depends upon\nthe linkage criteria. And then if I want, I can\nconsider to the next step",
    "start": "877920",
    "end": "884100"
  },
  {
    "text": "and just join them all. All right? That's hierarchical clustering.",
    "start": "884100",
    "end": "890670"
  },
  {
    "text": "So it's good because you get\nthis whole history of the",
    "start": "890670",
    "end": "896110"
  },
  {
    "text": "[? dendograms, ?] and\nyou get to look at it, say, well, all right,\nthat looks pretty good.",
    "start": "896110",
    "end": "902600"
  },
  {
    "text": "I'll stick with this clustering. It's deterministic.",
    "start": "902600",
    "end": "909600"
  },
  {
    "text": "Given a linkage criterion, you\nalways get the same answer. There's nothing random here.",
    "start": "909600",
    "end": "914899"
  },
  {
    "text": " Notice, by the way,\nthe answer might not",
    "start": "914900",
    "end": "920500"
  },
  {
    "text": "be optimal with regards\nto that linkage criteria. Why not?",
    "start": "920500",
    "end": "926480"
  },
  {
    "text": "What kind of algorithm is this? AUDIENCE: Greedy. JOHN GUTTAG: It's a\ngreedy algorithm, exactly.",
    "start": "926480",
    "end": "932420"
  },
  {
    "text": "And so I'm making\nlocally optimal decisions at each point which may or\nmay not be globally optimal.",
    "start": "932420",
    "end": "938510"
  },
  {
    "text": " It's flexible.",
    "start": "938510",
    "end": "944449"
  },
  {
    "text": "Choosing different\nlinkage criteria, I get different results. But it's also potentially\nreally, really slow.",
    "start": "944450",
    "end": "953660"
  },
  {
    "text": "This is not something you want\nto do on a million examples. The naive algorithm, the one\nI just sort of showed you,",
    "start": "953660",
    "end": "962570"
  },
  {
    "text": "is N cubed. N cubed is typically\nimpractical.",
    "start": "962570",
    "end": "970120"
  },
  {
    "text": "For some linkage criteria, for\nexample, single-linkage, there exists very clever N\nsquared algorithms.",
    "start": "970120",
    "end": "978680"
  },
  {
    "text": "For others, you\ncan't beat N cubed. But even N squared is\nreally not very good.",
    "start": "978680",
    "end": "987420"
  },
  {
    "text": "Which gets me to a much\nfaster greedy algorithm called k-means. ",
    "start": "987420",
    "end": "993740"
  },
  {
    "text": "Now, the k in k-means is the\nnumber of clusters you want.",
    "start": "993740",
    "end": "1000350"
  },
  {
    "text": "So the catch with\nk-means is if you don't have any idea how\nmany clusters you want,",
    "start": "1000350",
    "end": "1006050"
  },
  {
    "text": "it's problematical,\nwhereas hierarchical, you get to inspect it and\nsee what you're getting.",
    "start": "1006050",
    "end": "1013640"
  },
  {
    "text": "If you know how many you\nwant, it's a good choice because it's much faster.",
    "start": "1013640",
    "end": "1019010"
  },
  {
    "text": " All right, the algorithm,\nagain, is very simple.",
    "start": "1019010",
    "end": "1027319"
  },
  {
    "start": "1021000",
    "end": "1193000"
  },
  {
    "text": "This is the one that Professor\nGrimson briefly discussed. You randomly choose k examples\nas your initial centroids.",
    "start": "1027319",
    "end": "1036349"
  },
  {
    "text": "Doesn't matter which of\nthe examples you choose. Then you create k clusters\nby assigning each example",
    "start": "1036349",
    "end": "1044020"
  },
  {
    "text": "to the closest centroid,\ncompute k new centroids",
    "start": "1044020",
    "end": "1051440"
  },
  {
    "text": "by averaging the\nexamples in each cluster. So in the first iteration,\nthe centroids are all examples",
    "start": "1051440",
    "end": "1060950"
  },
  {
    "text": "that you started with. But after that, they're\nprobably not examples,",
    "start": "1060950",
    "end": "1066409"
  },
  {
    "text": "because you're now taking the\naverage of two examples, which may not correspond to\nany example you have.",
    "start": "1066410",
    "end": "1073070"
  },
  {
    "text": "Actually the average\nof N examples. And then you just\nkeep doing this",
    "start": "1073070",
    "end": "1079120"
  },
  {
    "text": "until the centroids don't move. Right? Once you go through\none iteration",
    "start": "1079120",
    "end": "1084875"
  },
  {
    "text": "where they don't\nmove, there's no point in recomputing them again\nand again and again,",
    "start": "1084875",
    "end": "1090100"
  },
  {
    "text": "so it is converged. ",
    "start": "1090100",
    "end": "1096610"
  },
  {
    "text": "So let's look at the complexity. Well, at the moment,\nwe can't tell you",
    "start": "1096610",
    "end": "1103809"
  },
  {
    "text": "how many iterations\nyou're going to have, but what's the complexity\nof one iteration? ",
    "start": "1103810",
    "end": "1114640"
  },
  {
    "text": "Well, let's think about\nwhat you're doing here. You've got k centroids.",
    "start": "1114640",
    "end": "1123240"
  },
  {
    "text": "Now I have to take each\nexample and compare it to each-- in a naively, at\nleast-- to each centroid",
    "start": "1123240",
    "end": "1130019"
  },
  {
    "text": "to see which it's closest to. Right? So that's k comparisons\nper example.",
    "start": "1130020",
    "end": "1141510"
  },
  {
    "text": "So that's k times\nn times d, where",
    "start": "1141510",
    "end": "1147480"
  },
  {
    "text": "how much time each of\nthese comparison takes, which is likely to depend\nupon the dimensionality",
    "start": "1147480",
    "end": "1152910"
  },
  {
    "text": "of the features, right? Just the Euclidean\ndistance, for example. ",
    "start": "1152910",
    "end": "1160150"
  },
  {
    "text": "But this is a way small number\nthan N squared, typically.",
    "start": "1160150",
    "end": "1165600"
  },
  {
    "text": "So each iteration\nis pretty quick, and in practice, as\nwe'll see, this typically",
    "start": "1165600",
    "end": "1171330"
  },
  {
    "text": "converges quite\nquickly, so you usually need a very small\nnumber of iterations.",
    "start": "1171330",
    "end": "1179120"
  },
  {
    "text": "So it is quite\nefficient, and then there are various ways\nyou can optimize it to make it even more efficient.",
    "start": "1179120",
    "end": "1185899"
  },
  {
    "text": "This is the most commonly-used\nclustering algorithm because it works really fast.",
    "start": "1185900",
    "end": "1193200"
  },
  {
    "start": "1193000",
    "end": "1737000"
  },
  {
    "text": "Let's look at an example. So I've got a bunch\nof blue points here,",
    "start": "1193200",
    "end": "1198880"
  },
  {
    "text": "and I actually wrote\nthe code to do this. I'm not going to\nshow you the code. And I chose four centroids\nat random, colored stars.",
    "start": "1198880",
    "end": "1213020"
  },
  {
    "text": "A green one, a fuchsia-colored\none, a red one, and a blue one.",
    "start": "1213020",
    "end": "1218390"
  },
  {
    "text": " So maybe they're not the\nones you would have chosen,",
    "start": "1218390",
    "end": "1224480"
  },
  {
    "text": "but there they are.  And I then, having chosen\nthem, assign each point",
    "start": "1224480",
    "end": "1233630"
  },
  {
    "text": "to one of those centroids,\nwhichever one it's closest to. All right?",
    "start": "1233630",
    "end": "1240660"
  },
  {
    "text": "Step one. ",
    "start": "1240660",
    "end": "1245680"
  },
  {
    "text": "And then I recompute\nthe centroid. So let's go back.",
    "start": "1245680",
    "end": "1251260"
  },
  {
    "text": " So we're here, and these\nare the initial centroids.",
    "start": "1251260",
    "end": "1259020"
  },
  {
    "text": "Now, when I find\nthe new centroids, if we look at where\nthe red one is,",
    "start": "1259020",
    "end": "1266130"
  },
  {
    "text": "the red one is this point,\nthis point, and this point. Clearly, the new centroid\nis going to move, right?",
    "start": "1266130",
    "end": "1274170"
  },
  {
    "text": "It's going to move somewhere\nalong in here or something like that, right?",
    "start": "1274170",
    "end": "1279950"
  },
  {
    "text": "So we'll get those\nnew centroids. There it is.",
    "start": "1279950",
    "end": "1286460"
  },
  {
    "text": "And now we'll re-assign points.",
    "start": "1286460",
    "end": "1291870"
  },
  {
    "text": "And what we'll see is this point\nis now closer to the red star",
    "start": "1291870",
    "end": "1298190"
  },
  {
    "text": "than it is to the fuchsia\nstar, because we've moved the red star.",
    "start": "1298190",
    "end": "1303919"
  },
  {
    "text": "Whoops. That one. Said the wrong thing. They were red to start with. This one is now suddenly\ncloser to the purple, so--",
    "start": "1303920",
    "end": "1313490"
  },
  {
    "text": "and to the red. It will get recolored. We compute the new centroids. ",
    "start": "1313490",
    "end": "1319970"
  },
  {
    "text": "We're going to move\nsomething again. We continue. Points will move around.",
    "start": "1319970",
    "end": "1325289"
  },
  {
    "text": "This time we move two points. Here we go again. Notice, again, the\ncentroids don't",
    "start": "1325290",
    "end": "1331980"
  },
  {
    "text": "correspond to actual examples. This one is close, but it's\nnot really one of them. ",
    "start": "1331980",
    "end": "1339210"
  },
  {
    "text": "Move two more. Recompute centroids,\nand we're done. So here we've converged, and I\nthink it was five iterations,",
    "start": "1339210",
    "end": "1349300"
  },
  {
    "text": "and nothing will move again. All right? Does that make\nsense to everybody?",
    "start": "1349300",
    "end": "1354354"
  },
  {
    "text": "So it's pretty simple.  What are the downsides?",
    "start": "1354354",
    "end": "1359770"
  },
  {
    "text": "Well, choosing k foolishly\ncan lead to strange results.",
    "start": "1359770",
    "end": "1365170"
  },
  {
    "text": "So if I chose k\nequal to 3, looking at this particular\narrangement of points,",
    "start": "1365170",
    "end": "1371470"
  },
  {
    "text": "it's not obvious what \"the\nright answer\" is, right? Maybe it's making all\nof this one cluster.",
    "start": "1371470",
    "end": "1378130"
  },
  {
    "text": "I don't know. But there are weird\nk's and if you choose a k that is nonsensical\nwith respect to your data,",
    "start": "1378130",
    "end": "1388050"
  },
  {
    "text": "then your clustering\nwill be nonsensical. So that's one problem\nwe have think about.",
    "start": "1388050",
    "end": "1393240"
  },
  {
    "text": "How do we choose k? Another problem, and this is\none somebody raised last time,",
    "start": "1393240",
    "end": "1400120"
  },
  {
    "text": "is that the results can depend\nupon the initial centroids. Unlike hierarchical clustering,\nk-means is non-deterministic.",
    "start": "1400120",
    "end": "1409330"
  },
  {
    "text": "Depending upon what\nrandom examples we choose,",
    "start": "1409330",
    "end": "1414460"
  },
  {
    "text": "we can get a different\nnumber of iterations. If we choose them poorly, it\ncould take longer to converge.",
    "start": "1414460",
    "end": "1420190"
  },
  {
    "text": "More worrisome, you\nget a different answer. You're running this\ngreedy algorithm,",
    "start": "1420190",
    "end": "1425670"
  },
  {
    "text": "and you might actually\nget to a different place, depending upon which\ncentroids you chose. ",
    "start": "1425670",
    "end": "1432390"
  },
  {
    "text": "So these are the\ntwo issues we have to think about dealing with. So let's first think\nabout choosing k.",
    "start": "1432390",
    "end": "1440980"
  },
  {
    "text": "What often happens\nis people choose k using a priori knowledge\nabout the application.",
    "start": "1440980",
    "end": "1447820"
  },
  {
    "text": " If I'm in medicine,\nI actually know",
    "start": "1447820",
    "end": "1453070"
  },
  {
    "text": "that there are only\nfive different kinds of bacteria in the world. That's true.",
    "start": "1453070",
    "end": "1459110"
  },
  {
    "text": "I mean, there are subspecies,\nbut five large categories. And if I had a bunch of\nbacterium I wanted to cluster,",
    "start": "1459110",
    "end": "1465980"
  },
  {
    "text": "may just set k equal to 5. Maybe I believe there are\nonly two kinds of people",
    "start": "1465980",
    "end": "1472390"
  },
  {
    "text": "in the world, those who are\nat MIT and those who are not. And so I'll choose k equal to 2.",
    "start": "1472390",
    "end": "1477549"
  },
  {
    "text": " Often, we know enough about the\napplication, we can choose k.",
    "start": "1477550",
    "end": "1485059"
  },
  {
    "text": "As we'll see later, often we\ncan think we do, and we don't. ",
    "start": "1485060",
    "end": "1491940"
  },
  {
    "text": "A better approach is\nto search for a good k. ",
    "start": "1491940",
    "end": "1501050"
  },
  {
    "text": "So you can try\ndifferent values of k and evaluate the\nquality of the result.",
    "start": "1501050",
    "end": "1508050"
  },
  {
    "text": "Assume you have some\nmetric, as to say yeah, I like this clustering, I\ndon't like this clustering.",
    "start": "1508050",
    "end": "1513290"
  },
  {
    "text": "And we'll talk about\ndo that in detail. Or you can run hierarchical\nclustering on a subset of data.",
    "start": "1513290",
    "end": "1522260"
  },
  {
    "text": "I've got a million points. All right, what I'm going to\ndo is take a subset of 1,000 of them or 10,000.",
    "start": "1522260",
    "end": "1528630"
  },
  {
    "text": "Run hierarchical clustering. From that, get a sense of the\nstructure underlying the data.",
    "start": "1528630",
    "end": "1536750"
  },
  {
    "text": "Decide k should be 6, and then\nrun k-means with k equals 6. People often do this.",
    "start": "1536750",
    "end": "1542940"
  },
  {
    "text": "They run hierarchical clustering\non a small subset of the data and then choose k.",
    "start": "1542940",
    "end": "1548570"
  },
  {
    "text": " And we'll look-- but one we're\ngoing to look at is that one.",
    "start": "1548570",
    "end": "1557830"
  },
  {
    "text": "What about unlucky centroids? So here I got the same\npoints we started with.",
    "start": "1557830",
    "end": "1565640"
  },
  {
    "text": "Different initial centroids. I've got a fuchsia\none, a black one,",
    "start": "1565640",
    "end": "1571310"
  },
  {
    "text": "and then I've got red\nand blue down here, which I happened to accidentally\nchoose close to one another.",
    "start": "1571310",
    "end": "1581780"
  },
  {
    "text": "Well, if I start\nwith these centroids, certainly you\nwould expect things",
    "start": "1581780",
    "end": "1587299"
  },
  {
    "text": "to take longer to converge. But in fact, what\nhappens is this-- ",
    "start": "1587300",
    "end": "1594450"
  },
  {
    "text": "I get this assignment of\nblue, this assignment of red,",
    "start": "1594450",
    "end": "1600059"
  },
  {
    "text": "and I'm done. It converges on this,\nwhich probably is not",
    "start": "1600060",
    "end": "1608980"
  },
  {
    "text": "what we wanted out of this. Maybe it is, but the\nfact that I converged",
    "start": "1608980",
    "end": "1614350"
  },
  {
    "text": "on some very\ndifferent place shows that it's a real weakness\nof the algorithm,",
    "start": "1614350",
    "end": "1619480"
  },
  {
    "text": "that it's sensitive to the\nrandomly-chosen initial conditions.",
    "start": "1619480",
    "end": "1625738"
  },
  {
    "text": "Well, couple of things\nyou can do about that.",
    "start": "1625738",
    "end": "1631000"
  },
  {
    "text": "You could be clever and try and\nselect good initial centroids.",
    "start": "1631000",
    "end": "1637180"
  },
  {
    "text": "So people often will do that,\nand what they'll do is try and just make sure that they're\ndistributed over the space.",
    "start": "1637180",
    "end": "1644740"
  },
  {
    "text": "So they would look at\nsome picture like this and say, well, let's just put\nmy centroids at the corners",
    "start": "1644740",
    "end": "1651940"
  },
  {
    "text": "or something like that so\nthat they're far apart. ",
    "start": "1651940",
    "end": "1659759"
  },
  {
    "text": "Another approach is\nto try multiple sets of randomly-chosen\ncentroids, and then",
    "start": "1659760",
    "end": "1666279"
  },
  {
    "text": "just select the best results.  And that's what this little\nalgorithm on the screen does.",
    "start": "1666280",
    "end": "1675980"
  },
  {
    "text": "So I'll say best is equal\nto k-means of the points themselves, or\nsomething, then for t",
    "start": "1675980",
    "end": "1685350"
  },
  {
    "text": "in range number of trials, I'll\nsay C equals k-means of points,",
    "start": "1685350",
    "end": "1690630"
  },
  {
    "text": "and I'll just keep track and\nchoose the one with the least dissimilarity. The thing I'm\ntrying to minimize.",
    "start": "1690630",
    "end": "1696780"
  },
  {
    "text": "OK?  The first one is got all\nthe points in one cluster.",
    "start": "1696780",
    "end": "1704909"
  },
  {
    "text": "So it's very dissimilar. And then I'll just\nkeep generating for different k's\nand I'll choose",
    "start": "1704910",
    "end": "1711210"
  },
  {
    "text": "the k that seems to\nbe the best, that does the best job of minimizing\nmy objective function.",
    "start": "1711210",
    "end": "1719740"
  },
  {
    "text": "And this is a very common\nsolution, by the way, for any randomized\ngreedy algorithm.",
    "start": "1719740",
    "end": "1726010"
  },
  {
    "text": "And there are a lot of\nrandomized greedy algorithms that you just choose\nmultiple initial conditions,",
    "start": "1726010",
    "end": "1733270"
  },
  {
    "text": "try them all out\nand pick the best. ",
    "start": "1733270",
    "end": "1739450"
  },
  {
    "start": "1737000",
    "end": "1882000"
  },
  {
    "text": "All right, now I\nwant to show you a slightly more real example.",
    "start": "1739450",
    "end": "1744585"
  },
  {
    "text": " So this is a file we've\ngot with medical patients,",
    "start": "1744585",
    "end": "1753470"
  },
  {
    "text": "and we're going to try\nand cluster them and see whether the clusters\ntell us anything",
    "start": "1753470",
    "end": "1759170"
  },
  {
    "text": "about the probability\nof them dying of a heart attack in, say,\nthe next year or some period",
    "start": "1759170",
    "end": "1766340"
  },
  {
    "text": "of time. So to simplify things,\nand this is something I have done with research,\nbut we're looking",
    "start": "1766340",
    "end": "1773059"
  },
  {
    "text": "at only four features here-- the heart rate in\nbeats per minute,",
    "start": "1773060",
    "end": "1779570"
  },
  {
    "text": "the number of previous heart\nattacks, the age, and something",
    "start": "1779570",
    "end": "1786250"
  },
  {
    "text": "called ST elevation,\na binary attribute. So the first three are obvious.",
    "start": "1786250",
    "end": "1792700"
  },
  {
    "text": "If you take an ECG of somebody's\nheart, it looks like this. This is a normal one.",
    "start": "1792700",
    "end": "1799900"
  },
  {
    "text": "They have the S, the\nT, and then there's this region between the\nS wave and the T wave.",
    "start": "1799900",
    "end": "1806480"
  },
  {
    "text": "And if it's higher, hence\nelevated, that's a bad thing.",
    "start": "1806480",
    "end": "1811950"
  },
  {
    "text": "And so this is about\nthe first thing that they measure if someone\nis having cardiac problems.",
    "start": "1811950",
    "end": "1817550"
  },
  {
    "text": "Do they have ST elevation?  And then with each\npatient, we're",
    "start": "1817550",
    "end": "1824290"
  },
  {
    "text": "going to have an outcome,\nwhether they died, and it's related\nto the features,",
    "start": "1824290",
    "end": "1831390"
  },
  {
    "text": "but it's probabilistic\nnot deterministic. So for example, an older person\nwith multiple heart attacks",
    "start": "1831390",
    "end": "1839919"
  },
  {
    "text": "is at higher risk than\na young person who's never had a heart attack. That doesn't mean,\nthough, that the older",
    "start": "1839920",
    "end": "1846400"
  },
  {
    "text": "person will die first. It's just more probable. ",
    "start": "1846400",
    "end": "1854289"
  },
  {
    "text": "We're going to take this data,\nwe're going to cluster it, and then we're going\nto look at what's called the purity\nof the clusters",
    "start": "1854290",
    "end": "1862970"
  },
  {
    "text": "relative to the outcomes. So is the cluster, say,\nenriched by people who died?",
    "start": "1862970",
    "end": "1871380"
  },
  {
    "text": "If you have one cluster\nand everyone in it died, then the clustering is\nclearly finding some structure",
    "start": "1871380",
    "end": "1877410"
  },
  {
    "text": "related to the outcome. ",
    "start": "1877410",
    "end": "1883990"
  },
  {
    "start": "1882000",
    "end": "1918000"
  },
  {
    "text": "So the file is in the\nzip file I uploaded. It looks more or less like this.",
    "start": "1883990",
    "end": "1890235"
  },
  {
    "text": "Right? So it's very straightforward. The outcomes are binary. 1 is a positive outcome.",
    "start": "1890235",
    "end": "1896940"
  },
  {
    "text": "Strangely enough in\nthe medical jargon, a death is a positive outcome.",
    "start": "1896940",
    "end": "1902220"
  },
  {
    "text": "I guess maybe if you're\nresponsible for the medical bills, it's positive. If you're the patient, it's hard\nto think of it as a good thing.",
    "start": "1902220",
    "end": "1910410"
  },
  {
    "text": "Nevertheless, that's\nthe way that they talk. And the others are\nall there, right?",
    "start": "1910410",
    "end": "1915450"
  },
  {
    "text": "Heart rate, other things. All right, let's\nlook at some code.",
    "start": "1915450",
    "end": "1921480"
  },
  {
    "start": "1918000",
    "end": "2128000"
  },
  {
    "text": " So I've extracted some code. I'm not going to\nshow you all of it.",
    "start": "1921480",
    "end": "1926980"
  },
  {
    "text": "There's quite a lot\nof it, as you'll see. So we'll start-- one\nof the files you've got",
    "start": "1926980",
    "end": "1934450"
  },
  {
    "text": "is called cluster dot pi. I decided there\nwas enough code, I didn't want to put\nit all in one file.",
    "start": "1934450",
    "end": "1941020"
  },
  {
    "text": "I was getting confused. So I said, let me\ncreate a file that has some of the code\nand a different file",
    "start": "1941020",
    "end": "1947950"
  },
  {
    "text": "that will then\nimport it and use it. Cluster has things\nthat are pretty much",
    "start": "1947950",
    "end": "1953500"
  },
  {
    "text": "unrelated to this example, but\njust useful for clustering.",
    "start": "1953500",
    "end": "1958700"
  },
  {
    "text": "So an example here has\nname, features, and label.",
    "start": "1958700",
    "end": "1964970"
  },
  {
    "text": "And really, the only\ninteresting thing in it-- and it's not that\ninteresting-- is distance.",
    "start": "1964970",
    "end": "1970880"
  },
  {
    "text": "And the fact that I'm\nusing Minkowski with 2 says we're using\nEuclidean distance.",
    "start": "1970880",
    "end": "1976760"
  },
  {
    "start": "1976760",
    "end": "1982290"
  },
  {
    "text": "Class cluster. It's a lot more\ncode to that one.",
    "start": "1982290",
    "end": "1988410"
  },
  {
    "text": "So we start with a\nnon-empty list of examples. That's what init does. You can imagine what\nthe code looks like,",
    "start": "1988410",
    "end": "1994380"
  },
  {
    "text": "or you can look at it. Update is interesting in that it\ntakes the cluster and examples",
    "start": "1994380",
    "end": "2005580"
  },
  {
    "text": "and puts them in-- if you\nthink of k-means in the cluster",
    "start": "2005580",
    "end": "2015549"
  },
  {
    "text": "closest to the\nprevious centroids and then returns the amount\nthe centroid has changed.",
    "start": "2015550",
    "end": "2023500"
  },
  {
    "text": "So if the centroid\nhas changed by 0, then you don't have\nanything, right? Creates the new cluster.",
    "start": "2023500",
    "end": "2030270"
  },
  {
    "text": "And the most interesting\nthing is computeCentroid. And if you look\nat this code, you",
    "start": "2030270",
    "end": "2035430"
  },
  {
    "text": "can see that I'm a slightly\nunreconstructed Python 2 programmers. I just noticed this.",
    "start": "2035430",
    "end": "2041910"
  },
  {
    "text": "I really shouldn't\nhave written 0.0. I should have just written\n0, but in Python 2,",
    "start": "2041910",
    "end": "2048419"
  },
  {
    "text": "you had to write that 0.0. Sorry about that. Thought I'd fixed these.",
    "start": "2048420",
    "end": "2055449"
  },
  {
    "text": "Anyway, so how do we\ncompute the centroid? We start by creating\nan array of all 0s.",
    "start": "2055449",
    "end": "2065750"
  },
  {
    "text": "The dimensionality is the number\nof features in the example. It's one of the methods from--",
    "start": "2065750",
    "end": "2074100"
  },
  {
    "text": "I didn't put up\non the PowerPoint. And then for e in\nexamples, I'm going",
    "start": "2074100",
    "end": "2080309"
  },
  {
    "text": "to add to vals\ne.getFeatures, and then I'm",
    "start": "2080310",
    "end": "2087790"
  },
  {
    "text": "just going to divide vals by\nthe length of self.examples,",
    "start": "2087790",
    "end": "2092860"
  },
  {
    "text": "the number of examples. So now you see why I made it a\npylab array, or a numpy array",
    "start": "2092860",
    "end": "2099480"
  },
  {
    "text": "rather than a\nlist, so I could do nice things like divide the\nwhole thing in one expression.",
    "start": "2099480",
    "end": "2107890"
  },
  {
    "text": "As you do math, any\nkind of math things, you'll find these arrays\nare incredibly convenient.",
    "start": "2107890",
    "end": "2114010"
  },
  {
    "text": "Rather than having to\nwrite recursive functions or do bunches of\niterations, the fact",
    "start": "2114010",
    "end": "2119140"
  },
  {
    "text": "that you can do it in one\nkeystroke is incredibly nice. And then I'm going to\nreturn the centroid.",
    "start": "2119140",
    "end": "2125570"
  },
  {
    "text": " Variability is exactly\nwhat we saw in the formula.",
    "start": "2125570",
    "end": "2133565"
  },
  {
    "start": "2128000",
    "end": "2197000"
  },
  {
    "text": " And then just for fun,\nso you could see this,",
    "start": "2133565",
    "end": "2139690"
  },
  {
    "text": "I used an iterator here. I don't know that\nany of you have used the yield statement in Python.",
    "start": "2139690",
    "end": "2147339"
  },
  {
    "text": "I recommend it. It's very convenient. One of the nice\nthings about Python",
    "start": "2147340",
    "end": "2152740"
  },
  {
    "text": "is almost anything\nthat's built in, you can make your\nown version of it.",
    "start": "2152740",
    "end": "2158540"
  },
  {
    "text": "And so once I've done\nthis, if c is a cluster,",
    "start": "2158540",
    "end": "2164470"
  },
  {
    "text": "I can now write something\nlike for c in big C,",
    "start": "2164470",
    "end": "2171320"
  },
  {
    "text": "and this will make it work just\nlike iterating over a list.",
    "start": "2171320",
    "end": "2177740"
  },
  {
    "text": "Right, so this makes it\npossible to iterate over it. If you haven't read\nabout yield, you probably",
    "start": "2177740",
    "end": "2184360"
  },
  {
    "text": "should read the probably\nabout two paragraphs in the textbook\nexplaining how it works,",
    "start": "2184360",
    "end": "2190339"
  },
  {
    "text": "but it's very convenient. Dissimilarity\nwe've already seen.",
    "start": "2190340",
    "end": "2195530"
  },
  {
    "text": " All right, now we\nget to patients.",
    "start": "2195530",
    "end": "2201870"
  },
  {
    "start": "2197000",
    "end": "2340000"
  },
  {
    "text": "This is in the file lec\n12, lecture 12 dot py.",
    "start": "2201870",
    "end": "2208300"
  },
  {
    "text": "In addition to importing\nthe usual suspects of pylab and numpy, and probably it\nshould import random too,",
    "start": "2208300",
    "end": "2217260"
  },
  {
    "text": "it imports cluster, the\none we just looked at. ",
    "start": "2217260",
    "end": "2224160"
  },
  {
    "text": "And so patient is a\nsub-type of cluster.Example.",
    "start": "2224160",
    "end": "2231589"
  },
  {
    "text": "Then I'm going to define\nthis interesting thing called scale attributes.",
    "start": "2231590",
    "end": "2238330"
  },
  {
    "text": "So you might remember,\nin the last lecture when Professor Grimson was\nlooking at these reptiles,",
    "start": "2238330",
    "end": "2245680"
  },
  {
    "text": "he ran into this\nproblem about alligators looking like chickens\nbecause they each have",
    "start": "2245680",
    "end": "2251200"
  },
  {
    "text": "a large number of legs. And he said, well, what can\nwe do to get around this?",
    "start": "2251200",
    "end": "2257330"
  },
  {
    "text": "Well, we can represent the\nfeature as a binary number. Has legs, doesn't have legs.",
    "start": "2257330",
    "end": "2263214"
  },
  {
    "text": "0 or 1. And the problem he\nwas dealing with is that when you\nhave a feature vector",
    "start": "2263215",
    "end": "2271859"
  },
  {
    "text": "and the dynamic range\nof some features is much greater than\nthe others, they",
    "start": "2271860",
    "end": "2279210"
  },
  {
    "text": "tend to dominate because the\ndistances just look bigger when you get Euclidean distance.",
    "start": "2279210",
    "end": "2286190"
  },
  {
    "text": "So for example, if we\nwanted to cluster the people in this room, and I\nhad one feature that",
    "start": "2286190",
    "end": "2293980"
  },
  {
    "text": "was, say, 1 for male\nand 0 for female, and another feature that\nwas 1 for wears glasses,",
    "start": "2293980",
    "end": "2301810"
  },
  {
    "text": "0 for doesn't wear glasses,\nand then a third feature which was weight, and\nI clustered them,",
    "start": "2301810",
    "end": "2311260"
  },
  {
    "text": "well, weight would\nalways completely dominate the Euclidean\ndistance, right?",
    "start": "2311260",
    "end": "2316690"
  },
  {
    "text": "Because the dynamic range\nof the weights in this room is much higher than\nthe dynamic range of 0 to 1.",
    "start": "2316690",
    "end": "2325450"
  },
  {
    "text": "And so for the reptiles,\nhe said, well, OK, we'll",
    "start": "2325450",
    "end": "2331119"
  },
  {
    "text": "just make it a binary variable. But maybe we don't\nwant to make weight a binary variable, because\nmaybe it is something",
    "start": "2331120",
    "end": "2338170"
  },
  {
    "text": "we want to take into account. So what we do is we scale it.",
    "start": "2338170",
    "end": "2344349"
  },
  {
    "start": "2340000",
    "end": "2451000"
  },
  {
    "text": "So this is a method\ncalled z-scaling. More general than just\nmaking things 0 or 1.",
    "start": "2344350",
    "end": "2354280"
  },
  {
    "text": "It's a simple code. It takes in all of the\nvalues of a specific feature",
    "start": "2354280",
    "end": "2362240"
  },
  {
    "text": "and then performs some\nsimple calculations, and when it's done, the\nresulting array it returns",
    "start": "2362240",
    "end": "2374970"
  },
  {
    "text": "has a known mean and a\nknown standard deviation.",
    "start": "2374970",
    "end": "2380320"
  },
  {
    "text": "So what's the mean going to be? It's always going to be\nthe same thing, independent of the initial values. ",
    "start": "2380320",
    "end": "2387660"
  },
  {
    "text": "Take a look at the code. Try and see if you\ncan figure it out. ",
    "start": "2387660",
    "end": "2395190"
  },
  {
    "text": "Anybody want to\ntake a guess at it? 0. Right? So the mean will always be 0.",
    "start": "2395190",
    "end": "2404160"
  },
  {
    "text": "And the standard deviation,\na little harder to figure, but it will always be 1. ",
    "start": "2404160",
    "end": "2413320"
  },
  {
    "text": "OK? So it's done this scaling. This is a very common kind\nof scaling called z-scaling.",
    "start": "2413320",
    "end": "2422160"
  },
  {
    "text": "The other way people\nscale is interpolate. They take the smallest value and\ncall it 0, the biggest value,",
    "start": "2422160",
    "end": "2429440"
  },
  {
    "text": "they call it 1, and then they\ndo a linear interpolation of all the values\nbetween 0 and 1.",
    "start": "2429440",
    "end": "2436230"
  },
  {
    "text": "So the range is 0 to 1. That's also very common.",
    "start": "2436230",
    "end": "2443230"
  },
  {
    "text": "So this is a general\nway to get all of the features sort\nof in the same ballpark",
    "start": "2443230",
    "end": "2448836"
  },
  {
    "text": "so that we can compare them.  And we'll look at what\nhappens when we scale",
    "start": "2448836",
    "end": "2455140"
  },
  {
    "start": "2451000",
    "end": "2535000"
  },
  {
    "text": "and when we don't scale. And that's why my getData\nfunction has this parameter",
    "start": "2455140",
    "end": "2461200"
  },
  {
    "text": "to scale. It either creates a set of\nexamples with the attributes as initially or scaled.",
    "start": "2461200",
    "end": "2470089"
  },
  {
    "text": "And then there's k-means. It's exactly the\nalgorithm I showed you with one little wrinkle,\nwhich is this part.",
    "start": "2470090",
    "end": "2480200"
  },
  {
    "text": "You don't want to end\nup with empty clusters. If I tell you I\nwant four clusters,",
    "start": "2480200",
    "end": "2486170"
  },
  {
    "text": "I don't mean I want\nthree with examples and one that's empty, right? Because then I really\ndon't have four clusters.",
    "start": "2486170",
    "end": "2494049"
  },
  {
    "text": "And so this is one\nof multiple ways to avoid having empty clusters.",
    "start": "2494050",
    "end": "2499510"
  },
  {
    "text": "Basically what I\ndid here is say, well, I'm going to try a lot of\ndifferent initial conditions.",
    "start": "2499510",
    "end": "2504640"
  },
  {
    "text": "If one of them is so unlucky\nto give me an empty cluster, I'm just going to skip it\nand go on to the next one",
    "start": "2504640",
    "end": "2511549"
  },
  {
    "text": "by raising a value\nerror, empty cluster. And if you look at\nthe code, you'll",
    "start": "2511550",
    "end": "2517350"
  },
  {
    "text": "see how this value\nerror is used. And then try k-means.",
    "start": "2517350",
    "end": "2522690"
  },
  {
    "text": "We'll call k-means numTrial\ntimes, each one getting a different set of\ninitial centroids,",
    "start": "2522690",
    "end": "2531060"
  },
  {
    "text": "and return the result with\nthe lowest dissimilarity. ",
    "start": "2531060",
    "end": "2536820"
  },
  {
    "start": "2535000",
    "end": "2599000"
  },
  {
    "text": "Then I have various ways\nto examine the results.",
    "start": "2536820",
    "end": "2543090"
  },
  {
    "text": "Nothing very\ninteresting, and here's the key place where we're\ngoing to run the whole thing.",
    "start": "2543090",
    "end": "2548190"
  },
  {
    "text": "We'll get the data,\ninitially not scaling it, because remember,\nit defaults to true.",
    "start": "2548190",
    "end": "2554200"
  },
  {
    "text": "Then initially, I'm only going\nto try one k. k equals 2. And we'll call testClustering\nwith the patients.",
    "start": "2554200",
    "end": "2567950"
  },
  {
    "text": "The number of clusters, k. I put in seed as\na parameter here",
    "start": "2567950",
    "end": "2573770"
  },
  {
    "text": "because I wanted to be\nable to play with it and make sure I got different\nthings for 0 and 1 and 2",
    "start": "2573770",
    "end": "2579710"
  },
  {
    "text": "just as a testing thing. And five trials\nit's defaulting to.",
    "start": "2579710",
    "end": "2586230"
  },
  {
    "text": "And then we'll look\nat testClustering",
    "start": "2586230",
    "end": "2592480"
  },
  {
    "text": "is returning the fraction\nof positive examples for each cluster.",
    "start": "2592480",
    "end": "2599780"
  },
  {
    "start": "2599000",
    "end": "2712000"
  },
  {
    "text": "OK? So let's see what\nhappens when we run it. ",
    "start": "2599780",
    "end": "2619690"
  },
  {
    "text": "All right. So we got two clusters. Cluster of size 118 with\n.3305, and a cluster",
    "start": "2619690",
    "end": "2629589"
  },
  {
    "text": "of size 132 with a positive\nfraction of point quadruple 3.",
    "start": "2629590",
    "end": "2635010"
  },
  {
    "text": " Should we be happy?",
    "start": "2635010",
    "end": "2643230"
  },
  {
    "text": "Does our clustering tell\nus anything, somehow correspond to the expected\noutcome for patients here?",
    "start": "2643230",
    "end": "2653220"
  },
  {
    "text": "Probably not, right? Those numbers are pretty\nmuch indistinguishable",
    "start": "2653220",
    "end": "2658599"
  },
  {
    "text": "statistically. And you'd have to guess that\nthe fraction of positives in the whole population\nis around .33, right?",
    "start": "2658600",
    "end": "2666544"
  },
  {
    "text": "That about a third\nof these people died of their heart attack. And I might as well have\nsigned them randomly",
    "start": "2666544",
    "end": "2675040"
  },
  {
    "text": "to the two clusters, right? There's not much\ndifference between this and what you would get\nwith the random result.",
    "start": "2675040",
    "end": "2682480"
  },
  {
    "text": "Well, why do we\nthink that's true?  Because I didn't scale, right?",
    "start": "2682480",
    "end": "2689550"
  },
  {
    "text": "And so one of the issues\nwe had to deal with is, well, age had a\nbig dynamic range,",
    "start": "2689550",
    "end": "2696760"
  },
  {
    "text": "and, say, ST elevation, which I\ntold you was highly diagnostic,",
    "start": "2696760",
    "end": "2702300"
  },
  {
    "text": "was either 0 or 1. And so probably\neverything is getting swamped by age or\nsomething else, right?",
    "start": "2702300",
    "end": "2712820"
  },
  {
    "start": "2712000",
    "end": "2867000"
  },
  {
    "text": "All right, so we have\nan easy way to fix that. We'll just scale the data.",
    "start": "2712820",
    "end": "2720440"
  },
  {
    "text": "Now let's see what we get. ",
    "start": "2720440",
    "end": "2726660"
  },
  {
    "text": "All right. That's interesting. With casting rule?",
    "start": "2726660",
    "end": "2733089"
  },
  {
    "text": "Good grief. That caught me by surprise. ",
    "start": "2733090",
    "end": "2748150"
  },
  {
    "text": "Good thing I have the answers\nin PowerPoint to show you, because the code doesn't\nseem to be working.",
    "start": "2748150",
    "end": "2753236"
  },
  {
    "start": "2753236",
    "end": "2760190"
  },
  {
    "text": "Try it once more. ",
    "start": "2760190",
    "end": "2765309"
  },
  {
    "text": "No. All right, well, in\nthe interest of getting through this\nlecture on schedule,",
    "start": "2765310",
    "end": "2771630"
  },
  {
    "text": "we'll go look at the\nresults that we get-- I got last time I ran it. ",
    "start": "2771630",
    "end": "2780281"
  },
  {
    "text": "All right.  When I scaled, what we see here\nis that now there is a pretty",
    "start": "2780281",
    "end": "2792109"
  },
  {
    "text": "dramatic difference, right? One of the clusters has\na much higher fraction",
    "start": "2792110",
    "end": "2797170"
  },
  {
    "text": "of positive patients\nthan others,",
    "start": "2797170",
    "end": "2803030"
  },
  {
    "text": "but it's still a\nbit problematic. So this has pretty\ngood specificity,",
    "start": "2803030",
    "end": "2812670"
  },
  {
    "text": "or positive predictive value,\nbut its sensitivity is lousy. ",
    "start": "2812670",
    "end": "2822170"
  },
  {
    "text": "Remember, a third of our\ninitial population more or less, was positive.",
    "start": "2822170",
    "end": "2828260"
  },
  {
    "text": "26 is way less than a\nthird, so in fact I've",
    "start": "2828260",
    "end": "2833320"
  },
  {
    "text": "got a class, a cluster,\nthat is strongly enriched,",
    "start": "2833320",
    "end": "2838690"
  },
  {
    "text": "but I'm still lumping most\nof the positive patients into the other cluster.",
    "start": "2838690",
    "end": "2844350"
  },
  {
    "text": " And in fact, there\nare 83 positives.",
    "start": "2844350",
    "end": "2851790"
  },
  {
    "text": "Wrote some code to do that. And so we see that\nof the 83 positives,",
    "start": "2851790",
    "end": "2857870"
  },
  {
    "text": "only this class,\nwhich is 70% positive, only has 26 in it\nto start with it.",
    "start": "2857870",
    "end": "2864710"
  },
  {
    "text": "So I'm clearly missing\nmost of the positives. So why?",
    "start": "2864710",
    "end": "2871130"
  },
  {
    "text": "Well, my hypothesis was\nthat different subgroups of positive patients have\ndifferent characteristics.",
    "start": "2871130",
    "end": "2878851"
  },
  {
    "text": " And so we could test this\nby trying other values of k",
    "start": "2878852",
    "end": "2889080"
  },
  {
    "text": "to see with-- we would\nget more clusters. So here, I said, let's\ntry k equals 2, 4, and 6.",
    "start": "2889080",
    "end": "2894540"
  },
  {
    "text": " And here's what I\ngot when I ran that.",
    "start": "2894540",
    "end": "2899740"
  },
  {
    "text": " So what you'll notice here, as\nwe get to, say, 4, that I have",
    "start": "2899740",
    "end": "2912010"
  },
  {
    "text": "two clusters, this\none and this one,",
    "start": "2912010",
    "end": "2919030"
  },
  {
    "text": "which are heavily enriched\nwith positive patients. 26 as before in the first\none, but 76 patients",
    "start": "2919030",
    "end": "2929530"
  },
  {
    "text": "in the third one. So I'm now getting a much\nhigher fraction of patients",
    "start": "2929530",
    "end": "2935560"
  },
  {
    "text": "in one of the \"risky\" clusters.",
    "start": "2935560",
    "end": "2940930"
  },
  {
    "text": "And I can continue to do that,\nbut if I look at k equals 6,",
    "start": "2940930",
    "end": "2948930"
  },
  {
    "text": "we now look at the\npositive clusters. There were three of them\nsignificantly positive.",
    "start": "2948930",
    "end": "2955559"
  },
  {
    "text": "But I'm not really getting\na lot more patients total, so maybe 4 is the right answer.",
    "start": "2955560",
    "end": "2962260"
  },
  {
    "text": " So what you see here is that\nwe have at least two parameters",
    "start": "2962260",
    "end": "2969470"
  },
  {
    "text": "to play with, scaling and k. Even though I was only\nwanted a structure",
    "start": "2969470",
    "end": "2975200"
  },
  {
    "text": "that would separate the risk-- high-risk patients\nfrom the lower-risk, which is why I started\nwith 2, I later",
    "start": "2975200",
    "end": "2985140"
  },
  {
    "text": "discovered that, in fact,\nthere are multiple reasons for being high-risk.",
    "start": "2985140",
    "end": "2990390"
  },
  {
    "text": "And so maybe one\nof these clusters is heavily enriched\nby old people. Maybe another one\nis heavily enriched",
    "start": "2990390",
    "end": "2996420"
  },
  {
    "text": "by people who have had three\nheart attacks in the past, or ST elevation or\nsome combination.",
    "start": "2996420",
    "end": "3003990"
  },
  {
    "text": "And when I had\nonly two clusters, I couldn't get that\nfine gradation. So this is what data\nscientists spend",
    "start": "3003990",
    "end": "3011519"
  },
  {
    "text": "their time doing when\nthey're doing clustering, is they actually have\nmultiple parameters.",
    "start": "3011520",
    "end": "3017970"
  },
  {
    "text": "They try different things out. They look at the\nresults, and that's why you actually have to think\nto manipulate data rather",
    "start": "3017970",
    "end": "3026040"
  },
  {
    "text": "than just push a button\nand wait for the answer. All right. More of this general\ntopic on Wednesday",
    "start": "3026040",
    "end": "3034350"
  },
  {
    "text": "when we're going to talk\nabout classification. Thank you. ",
    "start": "3034350",
    "end": "3039328"
  }
]