[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "12720",
    "end": "19865"
  },
  {
    "text": "PHILIPPE RIGOLLET: [INAUDIBLE]\nminus xi transpose t.",
    "start": "19865",
    "end": "26880"
  },
  {
    "text": "I just pick whatever notation\nI want from a variable. And let's say it's t.",
    "start": "26880",
    "end": "33850"
  },
  {
    "text": "So that's the least\nsquares estimator. And it turns out that,\nas I said last time, it's going to be\nconvenient to think",
    "start": "33850",
    "end": "39850"
  },
  {
    "text": "of those things as matrices. So here, I already have vectors. I've already gone from one\ndimension, just real valued",
    "start": "39850",
    "end": "47050"
  },
  {
    "text": "random variables through\nrandom vectors when I think of each xi, but if I\nstart stacking them together,",
    "start": "47050",
    "end": "52720"
  },
  {
    "text": "I'm going to have vectors\nand matrices that show up. So the first vector\nI'm getting is y, which is just a vector\nwhere I have y1 to yn.",
    "start": "52720",
    "end": "64030"
  },
  {
    "text": "Then I have-- so that's\na boldface vector. Then I have x, which is\na matrix where I have--",
    "start": "64030",
    "end": "72940"
  },
  {
    "text": "well, the first\ncoordinate is always 1. So I have 1, and then x1 xp\nminus 1, and that's-- sorry,",
    "start": "72940",
    "end": "84760"
  },
  {
    "text": "x1 xp minus 1, and\nthat's for observation 1. And then I have the same\nthing all the way down",
    "start": "84760",
    "end": "91630"
  },
  {
    "text": "for observation n. ",
    "start": "91630",
    "end": "100390"
  },
  {
    "text": "OK, everybody\nunderstands what this is? So I'm just basically\nstacking up all the xi's.",
    "start": "100390",
    "end": "107920"
  },
  {
    "text": "So this i-th row\nis xi transpose.",
    "start": "107920",
    "end": "115420"
  },
  {
    "text": "I am just stacking them up. And so if I want to write\nall these things to be true for each of\nthem, all I need to do",
    "start": "115420",
    "end": "123130"
  },
  {
    "text": "is to write a vector\nepsilon, which is epsilon 1 to epsilon n.",
    "start": "123130",
    "end": "128679"
  },
  {
    "text": "And what I'm going to have is\nthat y, the boldface vector, now is equal to the\nmatrix x times the vector",
    "start": "128680",
    "end": "134260"
  },
  {
    "text": "beta plus the vector epsilon. And it's really\njust exactly saying",
    "start": "134260",
    "end": "140540"
  },
  {
    "text": "what's there, because for 2--\nso this is a vector, right? This is a vector.",
    "start": "140540",
    "end": "145780"
  },
  {
    "text": "And what is the\ndimension of this vector? ",
    "start": "145780",
    "end": "152660"
  },
  {
    "text": "n, so this is n observations. And for all these-- for\ntwo vectors to be equal,",
    "start": "152660",
    "end": "159770"
  },
  {
    "text": "I need to have all the\ncoordinates to be equal, and that's exactly the same\nthing as saying that this holds for i equal 1 to n.",
    "start": "159770",
    "end": "166290"
  },
  {
    "text": " But now, when I have\nthis, I can actually",
    "start": "166290",
    "end": "171400"
  },
  {
    "text": "rewrite the sum for t equals-- sorry, for i equals 1 to\nn of yi minus xi transpose",
    "start": "171400",
    "end": "183310"
  },
  {
    "text": "beta squared, this\nturns out to be equal to the Euclidean norm of\nthe vector y minus the matrix x",
    "start": "183310",
    "end": "192340"
  },
  {
    "text": "times beta squared. And I'm going to\nput a 2 here so we know we're talking about\nthe Euclidean norm.",
    "start": "192340",
    "end": "199079"
  },
  {
    "text": "This just means this\nis the Euclidean norm. ",
    "start": "199079",
    "end": "207259"
  },
  {
    "text": "That's the one we've\nseen before when we talked about chi squared-- that's the square\nnorm is the sum of the square of\nthe coefficients,",
    "start": "207259",
    "end": "212940"
  },
  {
    "text": "and then I take a\nsquare root, but here I have an extra square. So it's really just the sum of\nthe square of the coefficients,",
    "start": "212940",
    "end": "218049"
  },
  {
    "text": "which is this. And here are the coefficients. ",
    "start": "218050",
    "end": "223370"
  },
  {
    "text": "So then, that I write this thing\nlike that, then minimizing--",
    "start": "223370",
    "end": "229530"
  },
  {
    "text": "so my goal here, now, is\ngoing to solve minimum over t in our p of y minus\nx times t2 squared.",
    "start": "229530",
    "end": "245299"
  },
  {
    "text": "And just like we did\nfor one dimension, we can actually write\noptimality conditions for this.",
    "start": "245300",
    "end": "252200"
  },
  {
    "text": "I mean, this is a function. So this is a function\nfrom rp to r.",
    "start": "252200",
    "end": "263064"
  },
  {
    "text": "And if I want to minimize\nit, all I have to do is to take its gradient\nand set it equal to 0.",
    "start": "263064",
    "end": "268820"
  },
  {
    "text": "So minimum, set gradient to 0.",
    "start": "268820",
    "end": "282010"
  },
  {
    "text": "So that's where it becomes\na little complicated. Now I'm going to have to take\nthe gradient of this norm.",
    "start": "282010",
    "end": "289210"
  },
  {
    "text": "It might be a little\nannoying to do. But actually, what's\nnice about those things-- I mean, I remember that it\nwas a bit annoying to learn.",
    "start": "289210",
    "end": "296720"
  },
  {
    "text": "I mean, it's just\nbasically rules of calculus that you don't use that much. But essentially, you can\nactually expend this norm.",
    "start": "296720",
    "end": "305690"
  },
  {
    "text": "And you will see that\nthe rules are basically the same as in one\ndimension, you just have to be careful about the\nfact that matrices do not",
    "start": "305690",
    "end": "311889"
  },
  {
    "text": "commute. So let's expand this thing. y minus xt squared--",
    "start": "311890",
    "end": "319550"
  },
  {
    "text": "well, this is equal\nto the norm of y squared plus the norm\nof x squared plus 2",
    "start": "319550",
    "end": "330580"
  },
  {
    "text": "times y transpose xt. ",
    "start": "330580",
    "end": "336730"
  },
  {
    "text": "That's just expanding the\nsquare in more dimensions. And this, I'm actually going\nto write as y squared plus--",
    "start": "336730",
    "end": "347710"
  },
  {
    "text": "so here, the norm\nsquared of this guy, I always have that\nthe norm of x squared",
    "start": "347710",
    "end": "353140"
  },
  {
    "text": "is equal to x transpose x. So I'm going to write\nthis as x transpose",
    "start": "353140",
    "end": "358479"
  },
  {
    "text": "x, so it's t transpose\nx transpose xt",
    "start": "358480",
    "end": "364540"
  },
  {
    "text": "plus 2 times y transpose xt.",
    "start": "364540",
    "end": "370540"
  },
  {
    "text": "So now, if I'm going to take\nthe gradient with respect to t, I have basically three\nterms, and each of them",
    "start": "370540",
    "end": "376210"
  },
  {
    "text": "has some sort of a\ndifferent nature. This term is linear\nin t, and it's",
    "start": "376210",
    "end": "381610"
  },
  {
    "text": "going to differentiate\nthe same way that I differentiate a times x. I'm just going to keep the a.",
    "start": "381610",
    "end": "388210"
  },
  {
    "text": "This guy is quadratic. t appears twice. And this guy, I'm\ngoing to pick up a 2,",
    "start": "388210",
    "end": "394140"
  },
  {
    "text": "and it's going to differentiate\njust like when I differentiate a times x squared. It's 2 times ax.",
    "start": "394140",
    "end": "401199"
  },
  {
    "text": "And this guy is a constant\nwith respect to t, so it's going to\ndifferentiate to 0.",
    "start": "401200",
    "end": "407379"
  },
  {
    "text": "So when I compute the gradient-- ",
    "start": "407380",
    "end": "413680"
  },
  {
    "text": "now, of course, all of these\nrules that I give you you can check by looking at the\npartial derivative with respect",
    "start": "413680",
    "end": "418810"
  },
  {
    "text": "to each coordinate. But arguably, it's\nmuch faster to know the rules of differentiability.",
    "start": "418810",
    "end": "424780"
  },
  {
    "text": "It's like if I gave you\nthe function exponential x and I said, what\nis the derivative, and you started\nwriting, well, I'm",
    "start": "424780",
    "end": "429796"
  },
  {
    "text": "going to write exponential x\nplus h minus exponential ax divided by h and let h go to 0.",
    "start": "429796",
    "end": "435670"
  },
  {
    "text": "That's a bit painful. AUDIENCE: Why did\nyou transpose your-- why does x have\nto be [INAUDIBLE]??",
    "start": "435670",
    "end": "443755"
  },
  {
    "text": "PHILIPPE RIGOLLET: I'm sorry? AUDIENCE: I was\nwondering why you times t times the [INAUDIBLE]?",
    "start": "443755",
    "end": "449080"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nThe transpose of 2ab is b transpose a transpose.",
    "start": "449080",
    "end": "455490"
  },
  {
    "text": " If you're not sure\nabout this, just",
    "start": "455490",
    "end": "460880"
  },
  {
    "text": "make a and b have\ndifferent size, and then you will see that\nthere's some incompatibility.",
    "start": "460880",
    "end": "466069"
  },
  {
    "text": "I mean, there's basically\nonly one way to not screw that one up, so that's\neasy to remember.",
    "start": "466070",
    "end": "471230"
  },
  {
    "text": "So if I take the gradient, then\nit's going to be equal to what? It's going to be 0\nplus-- we said here,",
    "start": "471230",
    "end": "478130"
  },
  {
    "text": "this is going to\ndifferentiate like-- so think a times x squared.",
    "start": "478130",
    "end": "485130"
  },
  {
    "text": "So I'm going to have 2ax. So here, basically, this guy is\ngoing to go to x transpose xt.",
    "start": "485130",
    "end": "493840"
  },
  {
    "text": "Now, I could have\nmade this one go away, but that's the same thing as\nsaying that my gradient is--",
    "start": "493840",
    "end": "500050"
  },
  {
    "text": "I can think of my\ngradient as being either a horizontal vector\nor a vertical vector. So if I remove this guy,\nI'm thinking of my gradient",
    "start": "500050",
    "end": "506530"
  },
  {
    "text": "as being horizontal. If I remove that guy, I'm\nthinking of my gradient as being vertical. And that's what I want\nto think of, typically--",
    "start": "506530",
    "end": "513258"
  },
  {
    "text": "vertical vectors,\ncolumn vectors. And then this guy, well,\nit's like these guys just",
    "start": "513258",
    "end": "519159"
  },
  {
    "text": "think a times x. So the derivative is\njust a, so I'm going",
    "start": "519159",
    "end": "524560"
  },
  {
    "text": "to keep only that part here. Sorry, I forgot a minus\nsomewhere-- yeah, here.",
    "start": "524560",
    "end": "529670"
  },
  {
    "start": "529670",
    "end": "535200"
  },
  {
    "text": "Minus 2y transpose x. And what I want is this\nthing to be equal to 0.",
    "start": "535200",
    "end": "542160"
  },
  {
    "text": " So t, the optimal t, is called\nbeta hat and satisfies--",
    "start": "542160",
    "end": "560529"
  },
  {
    "text": " well, I can cancel the\n2's and put the minus",
    "start": "560530",
    "end": "567786"
  },
  {
    "text": "on the other side,\nand what I get is that x transpose xt is\nequal to y transpose x.",
    "start": "567786",
    "end": "576970"
  },
  {
    "start": "576970",
    "end": "584720"
  },
  {
    "text": "Yeah, that's not working for me. Yeah, that's because when\nI took the derivative,",
    "start": "584720",
    "end": "590240"
  },
  {
    "text": "I still need to make sure-- so it's the same question\nof whether I want things to be columns or rows.",
    "start": "590240",
    "end": "595610"
  },
  {
    "text": "So this is not a column. If I remove that guy,\ny transpose t is a row.",
    "start": "595610",
    "end": "601464"
  },
  {
    "text": "So I'm just going to take\nthe transpose of this guy to make things work, and this is\njust going to be x transpose y.",
    "start": "601465",
    "end": "607890"
  },
  {
    "text": " And this guy is x transpose\ny so that I have columns.",
    "start": "607890",
    "end": "614310"
  },
  {
    "text": " So this is just the\nlinear equation in t.",
    "start": "614310",
    "end": "623910"
  },
  {
    "text": "And I have to solve it, so it's\nof the form some matrix times t is equal to another vector.",
    "start": "623910",
    "end": "629890"
  },
  {
    "text": "And so that's basically\nin your system. And the way to solve\nit, at least formally, is to just take the inverse\nof the matrix on the left.",
    "start": "629890",
    "end": "636449"
  },
  {
    "text": "So if x transpose x is\ninvertible, then-- sorry,",
    "start": "636450",
    "end": "645830"
  },
  {
    "text": "that's beta hat is the t I want. I get that beta hat is\nequal to x transpose",
    "start": "645830",
    "end": "651410"
  },
  {
    "text": "x inverse x transpose y. ",
    "start": "651410",
    "end": "657860"
  },
  {
    "text": "And that's the least\nsquares estimator. ",
    "start": "657860",
    "end": "672670"
  },
  {
    "text": "So here, I use this condition. I want it to be invertible so I\ncan actually write its inverse.",
    "start": "672670",
    "end": "681579"
  },
  {
    "text": "Here, I wrote, rank\nof x is equal to p. What is the difference?",
    "start": "681580",
    "end": "686948"
  },
  {
    "start": "686948",
    "end": "696910"
  },
  {
    "text": "Well, there's basically\nno difference. Basically, here,\nI have to assume--",
    "start": "696910",
    "end": "705100"
  },
  {
    "text": "what is the size of the\nmatrix x transpose x? ",
    "start": "705100",
    "end": "712509"
  },
  {
    "text": "[INTERPOSING VOICES] PHILIPPE RIGOLLET: Yeah,\nso what is the size? AUDIENCE: p by p. PHILIPPE RIGOLLET: p by p.",
    "start": "712509",
    "end": "718000"
  },
  {
    "text": "So this matrix is\ninvertible if it's a rank p, if you know what rank means. If you don't, that just rank\np means that it's invertible.",
    "start": "718000",
    "end": "725740"
  },
  {
    "text": "So it's full rank\nand it's invertible. And the rank of x\ntranspose x is actually just the rank of x because this\nis the same matrix that you",
    "start": "725740",
    "end": "733540"
  },
  {
    "text": "apply twice. And that's all it's saying. So if you're not comfortable\nwith the notion of rank that you see here, just\nthink of this condition",
    "start": "733540",
    "end": "741069"
  },
  {
    "text": "just being the condition that\nx transpose x is invertible. And that's all it says.",
    "start": "741070",
    "end": "746650"
  },
  {
    "text": "What it means for it to be\ninvertible-- this was true. We made no assumption\nup to this point.",
    "start": "746650",
    "end": "752529"
  },
  {
    "text": "If x is not invertible,\nit means that there might be multiple\nsolutions to this equation.",
    "start": "752530",
    "end": "758840"
  },
  {
    "text": "In particular, for a matrix\nto not be invertible, it means that there's\nsome vector v.",
    "start": "758840",
    "end": "765890"
  },
  {
    "text": "So if x transpose x\nis not invertible,",
    "start": "765890",
    "end": "775800"
  },
  {
    "text": "then this is equivalent\nto there exists a vector v, which is not 0, and such that\nx transpose xv is equal to 0.",
    "start": "775800",
    "end": "787910"
  },
  {
    "text": "That's what it means\nto not be invertible. So in particular, if\nbeta hat is a solution--",
    "start": "787910",
    "end": "793730"
  },
  {
    "start": "793730",
    "end": "802089"
  },
  {
    "text": "so this equation is sometimes\ncalled score equations, because the gradient\nis called the score,",
    "start": "802090",
    "end": "808279"
  },
  {
    "text": "and so you're just checking\nif the gradient is equal to 0. So if beta hat\nsatisfies star, then so",
    "start": "808280",
    "end": "813730"
  },
  {
    "text": "does beta hat plus lambda v for\nall lambda in the real line.",
    "start": "813730",
    "end": "826820"
  },
  {
    "start": "826820",
    "end": "831840"
  },
  {
    "text": "And the reason is because,\nwell, if I start looking at-- what is x transpose x times\nbeta hat plus lambda v?",
    "start": "831840",
    "end": "842399"
  },
  {
    "text": "Well, by linearity, this\nis just x transpose x",
    "start": "842400",
    "end": "848000"
  },
  {
    "text": "beta hat plus lambda\nx transpose x times v.",
    "start": "848000",
    "end": "856510"
  },
  {
    "text": "But this guy is what? ",
    "start": "856510",
    "end": "862420"
  },
  {
    "text": "It's 0, just because\nthat's what we assumed.",
    "start": "862420",
    "end": "867860"
  },
  {
    "text": "We assumed that x transpose\nxv was equal to 0, so we're left only with this\npart, which, by star, is just",
    "start": "867860",
    "end": "874130"
  },
  {
    "text": "x transpose y. ",
    "start": "874130",
    "end": "880040"
  },
  {
    "text": "So that means that x transpose\nx beta hat plus lambda v is actually equal to x transpose\ny, which means that there's",
    "start": "880040",
    "end": "888080"
  },
  {
    "text": "another solution, which\nis not just beta hat, but any move of beta hat along\nthis direction v by any size.",
    "start": "888080",
    "end": "896784"
  },
  {
    "text": "So that's going to be\nan issue, because you're looking for one estimator. And there's not just one,\nin this case, there's many.",
    "start": "896784",
    "end": "903350"
  },
  {
    "text": "And so this is not\ngoing to be well-defined and you're going to\nhave some issues. So if you want to talk about\nthe least squares estimator,",
    "start": "903350",
    "end": "909560"
  },
  {
    "text": "you have to make\nthis assumption. What does it imply\nin terms of, can I",
    "start": "909560",
    "end": "915310"
  },
  {
    "text": "think of p being too n,\nfor example, in this case? What happens if\np is equal to 2n?",
    "start": "915310",
    "end": "920350"
  },
  {
    "start": "920350",
    "end": "927527"
  },
  {
    "text": "AUDIENCE: Well, then the rank\nof your matrix is only p/2. PHILIPPE RIGOLLET: So the rank\nof your matrix is only p/2,",
    "start": "927528",
    "end": "933500"
  },
  {
    "text": "so that means that this is\nactually not going to happen. I mean, it's not only\np/2, it's at most p/2.",
    "start": "933500",
    "end": "939529"
  },
  {
    "text": "It's at most the smallest of the\ntwo dimensions of your matrix. So if your matrix\nis n times 2n, it's",
    "start": "939530",
    "end": "944600"
  },
  {
    "text": "at most n, which means that\nit's not going to be full rank, so it's not going\nto be invertible. So every time the dimension p\nis larger than the sample size,",
    "start": "944600",
    "end": "953600"
  },
  {
    "text": "your matrix is not invertible,\nand you cannot talk about the least squares estimator. So that's something\nto keep in mind.",
    "start": "953600",
    "end": "959930"
  },
  {
    "text": "And it's actually a\nvery simple thing. It's essentially saying,\nwell, if p is lower than n,",
    "start": "959930",
    "end": "965750"
  },
  {
    "text": "it means that you\nhave more parameters to estimate than you have\nequations to estimate it.",
    "start": "965750",
    "end": "971000"
  },
  {
    "text": "So you have this linear system. There's one equation\nper observation.",
    "start": "971000",
    "end": "977240"
  },
  {
    "text": "Each row, which was\neach observation, was giving me one equation. But then the number of unknowns\nin this linear system is p,",
    "start": "977240",
    "end": "984959"
  },
  {
    "text": "and so I cannot solve linear\nsystems that have more unknowns than they have equations.",
    "start": "984960",
    "end": "990350"
  },
  {
    "text": "And so that's basically\nwhat's happening. Now, in practice, if\nyou think about what data sets look like\nthese days, for example,",
    "start": "990350",
    "end": "996330"
  },
  {
    "text": "people are trying to\nexpress some phenotype. So phenotype is something you\ncan measure on people-- maybe",
    "start": "996330",
    "end": "1001570"
  },
  {
    "text": "the color of your\neyes, or your height, or whether you have diabetes\nor not, things like this,",
    "start": "1001570",
    "end": "1007690"
  },
  {
    "text": "so things that are macroscopic. And then they want to use\nthe genotype to do that.",
    "start": "1007690",
    "end": "1013429"
  },
  {
    "text": "They want to measure your-- they\nwant to sequence your genome and try to use this to\npredict whether you're going",
    "start": "1013429",
    "end": "1018939"
  },
  {
    "text": "to be responsive to a drug\nor whether your r's are going to be blue, or\nsomething like this. Now, the data sets\nthat you can have--",
    "start": "1018940",
    "end": "1025059"
  },
  {
    "text": "people, maybe, for a given study\nabout some sort of disease. Maybe you will sequence the\ngenome of maybe 100 people.",
    "start": "1025060",
    "end": "1035260"
  },
  {
    "text": "n is equal to 100. p is basically the number\nof genes they're sequencing.",
    "start": "1035260",
    "end": "1041030"
  },
  {
    "text": "This is of the order of 100,000. So you can imagine that this\nis a case where n is much,",
    "start": "1041030",
    "end": "1046287"
  },
  {
    "text": "much smaller than p, and you\ncannot talk about the least squares estimator. There's plenty of them. There's not just\none line like that,",
    "start": "1046287",
    "end": "1053630"
  },
  {
    "text": "lambda times v that\nyou can move away. There's basically an entire\nspace in which you can move,",
    "start": "1053630",
    "end": "1060320"
  },
  {
    "text": "and so it's not well-defined. So at the end of this\nclass, I will give you a short introduction\non how you do this.",
    "start": "1060320",
    "end": "1066740"
  },
  {
    "text": "This actually represents\nmore and more. It becomes a more and more\npreponderant part of the data sets you have to deal\nwith, because people just",
    "start": "1066740",
    "end": "1073610"
  },
  {
    "text": "collect data. When I do the\nsequencing, the machine allows me to sequence\n100,000 genes.",
    "start": "1073610",
    "end": "1079730"
  },
  {
    "text": "I'm not going to stop at 100\nbecause doctors are never going to have cohorts of\nmore than 100 patients.",
    "start": "1079730",
    "end": "1086510"
  },
  {
    "text": "So you just collect\neverything you can collect. And this is true for everything. Cars have sensors\nall over the place,",
    "start": "1086510",
    "end": "1093372"
  },
  {
    "text": "much more than they\nactually gather data. There's data, there's--\nwe're creating, we're recording\neverything we can.",
    "start": "1093372",
    "end": "1098840"
  },
  {
    "text": "And so we need some new\ntechniques for that, and that's what high-dimensional\nstatistics is trying to answer. So this is way beyond\nthe scope of this class,",
    "start": "1098840",
    "end": "1105530"
  },
  {
    "text": "but towards the\nend, I will give you some hints about what can\nbe done in this framework because, well, this is the new\nreality we have to deal with.",
    "start": "1105530",
    "end": "1114809"
  },
  {
    "text": "So here, we're in a case\nwhere p's less than n and typically much\nsmaller than n. So the kind of orders of\nmagnitude you want to have",
    "start": "1114810",
    "end": "1120680"
  },
  {
    "text": "is maybe p's of order 10 and\nn's of order 100, something",
    "start": "1120680",
    "end": "1126135"
  },
  {
    "text": "like this. So you can scale that,\nbut maybe 10 times larger. So maybe you cannot solve this\nguy b for b hat, but actually,",
    "start": "1126135",
    "end": "1137810"
  },
  {
    "text": "you can talk about\nx times b hat, even if p is larger than n. And the reason is\nthat x times b hat",
    "start": "1137810",
    "end": "1146880"
  },
  {
    "text": "is actually something\nthat's very well-defined. So what is x times b hat? Remember, I started\nwith the model.",
    "start": "1146880",
    "end": "1156810"
  },
  {
    "text": "So if I look at this\ndefinition, essentially, what I had as the original\nthing was that the vector",
    "start": "1156810",
    "end": "1164360"
  },
  {
    "text": "y was equal to x times beta\nplus the vector epsilon.",
    "start": "1164360",
    "end": "1169910"
  },
  {
    "text": "That was my model. So beta is actually\ngiving me something.",
    "start": "1169910",
    "end": "1176399"
  },
  {
    "text": "Beta is actually some\nparameter, some coefficients that are interesting. But a good estimator\nfor-- so here, it",
    "start": "1176400",
    "end": "1183610"
  },
  {
    "text": "means that the\nobservations that I have are of the form x times\nbeta plus some noise.",
    "start": "1183610",
    "end": "1188870"
  },
  {
    "text": "So if I want to adjust the\nnoise, remove the noise, a good candidate to do\nnoise is x times beta hat.",
    "start": "1188870",
    "end": "1197110"
  },
  {
    "text": "x times beta hat is something\nthat should actually be useful to me, which should\nbe close to x times beta.",
    "start": "1197110",
    "end": "1210139"
  },
  {
    "text": "So in the one-dimensional case,\nwhat it means is that if I have-- let's say this\nis the true line,",
    "start": "1210140",
    "end": "1216530"
  },
  {
    "text": "and these are my\nx's, so I have-- these are the true\npoints on the real line,",
    "start": "1216530",
    "end": "1222050"
  },
  {
    "text": "and then I have\nmy little epsilon that just give me\nmy observations that move around this line.",
    "start": "1222050",
    "end": "1228560"
  },
  {
    "text": "So this is one of\nepsilons, say epsilon i.",
    "start": "1228560",
    "end": "1234860"
  },
  {
    "text": "Then I can actually\neither talk-- to say that I\nrecovered the line, I can actually talk about\nrecovering the right intercept",
    "start": "1234860",
    "end": "1242270"
  },
  {
    "text": "or recovering the right\nslope for this line. Those are the two parameters\nthat I need to recover. But I can also say\nthat I've actually",
    "start": "1242270",
    "end": "1248900"
  },
  {
    "text": "found a set of\npoints that's closer to being on the line that are\ncloser to this set of points",
    "start": "1248900",
    "end": "1256250"
  },
  {
    "text": "right here than the original\ncrosses that I observed. So if we go back to\nthe picture here,",
    "start": "1256250",
    "end": "1263870"
  },
  {
    "text": "for example, what I could do\nis say, well, for this point here--",
    "start": "1263870",
    "end": "1269750"
  },
  {
    "text": "there was an x here-- rather than looking at this\ndot, which was my observation, I can say, well, now that\nI've estimated the red line,",
    "start": "1269750",
    "end": "1277732"
  },
  {
    "text": "I can actually just\nsay, well, this point should really be here. And actually, I can\nmove all these dots",
    "start": "1277732",
    "end": "1283760"
  },
  {
    "text": "so that they're actually\non the red line. And this should be a\nbetter value, something that has less noise than\nthe original y value",
    "start": "1283760",
    "end": "1290840"
  },
  {
    "text": "that I should see. It should be close\nto the true value that I should be seeing\nwithout the extra noise.",
    "start": "1290840",
    "end": "1297240"
  },
  {
    "text": "So that's definitely something\nthat could be of interest. ",
    "start": "1297240",
    "end": "1303409"
  },
  {
    "text": "For example, in\nimaging, you're not trying to understand--\nso when you do imaging,",
    "start": "1303410",
    "end": "1308690"
  },
  {
    "text": "y is basically an image. So think of a pixel\nimage, and you just stack it into one long vector.",
    "start": "1308690",
    "end": "1315644"
  },
  {
    "text": "And what you see\nis something that should look like some linear\ncombination of some feature vectors, maybe.",
    "start": "1315644",
    "end": "1321440"
  },
  {
    "text": "So there's people created\na bunch of features. They're called, for example,\nGabor frames or wavelet",
    "start": "1321440",
    "end": "1329290"
  },
  {
    "text": "transforms-- so just well-known\nlibraries of variables x such",
    "start": "1329290",
    "end": "1334820"
  },
  {
    "text": "that when you take linear\ncombinations of those guys, this should looks like\na bunch of images. And what you want\nfor your image--",
    "start": "1334820",
    "end": "1342049"
  },
  {
    "text": "you don't care what the\ncoefficients of the image are in these bases\nthat you came up with. What you care about is\nthe noise in the image.",
    "start": "1342049",
    "end": "1348690"
  },
  {
    "text": "And so you really\nwant to get x beta. So if you want to\nestimate x beta,",
    "start": "1348690",
    "end": "1354389"
  },
  {
    "text": "well, you can use x beta hat. What is x beta hat? Well, since beta hat is x\ntranspose x inverse x transpose",
    "start": "1354390",
    "end": "1362039"
  },
  {
    "text": "y, this is x transpose. ",
    "start": "1362040",
    "end": "1368800"
  },
  {
    "text": "That's my estimator for x beta. ",
    "start": "1368800",
    "end": "1374059"
  },
  {
    "text": "Now, this thing,\nactually, I can define",
    "start": "1374060",
    "end": "1379170"
  },
  {
    "text": "even if I'm not low rank. So why is this\nthing interesting? Well, there's a formula\nfor this estimator,",
    "start": "1379170",
    "end": "1388120"
  },
  {
    "text": "but actually, I can\nvisualize what this thing is. ",
    "start": "1388120",
    "end": "1398792"
  },
  {
    "text": "So let's assume, for the\nsake of illustration, that n is equal to 3.",
    "start": "1398792",
    "end": "1406200"
  },
  {
    "text": " So that means that y lives\nin a three-dimensional space.",
    "start": "1406200",
    "end": "1413700"
  },
  {
    "text": "And so let's say it's here. And so I have my,\nlet's say, y's here.",
    "start": "1413700",
    "end": "1423970"
  },
  {
    "text": "And I also have a\nplane that's given by the vectors x1 transpose\nx2 transpose, which",
    "start": "1423970",
    "end": "1435450"
  },
  {
    "text": "is, by the way, 1-- sorry, that's not\nwhat I want to do.",
    "start": "1435450",
    "end": "1441290"
  },
  {
    "text": " I'm going to say that n is equal\nto 3 and that p is equal to 2.",
    "start": "1441290",
    "end": "1450600"
  },
  {
    "text": "So I basically have two\nvectors, 1, 1 and another one,",
    "start": "1450600",
    "end": "1458460"
  },
  {
    "text": "let's assume that\nit's, for example, abc.",
    "start": "1458460",
    "end": "1465669"
  },
  {
    "text": "So those are my two vectors. This is x1, and this is x2.",
    "start": "1465670",
    "end": "1473430"
  },
  {
    "text": " And those are my three\nobservations for this guy.",
    "start": "1473430",
    "end": "1479190"
  },
  {
    "text": "So what I want when\nI minimize this,",
    "start": "1479190",
    "end": "1488940"
  },
  {
    "text": "I'm looking at the\npoint which can be formed as the linear\ncombination of the columns of x, and I'm trying to find\nthe guy that's the closest to y.",
    "start": "1488940",
    "end": "1497887"
  },
  {
    "text": "So what does it look like? Well, the two points, 1, 1,\n1 is going to be, say, here. That's the point 1, 1, 1.",
    "start": "1497887",
    "end": "1504360"
  },
  {
    "text": "And let's say that\nabc is this point. ",
    "start": "1504360",
    "end": "1514890"
  },
  {
    "text": "So now I have a line that\ngoes through those two guys. ",
    "start": "1514890",
    "end": "1520620"
  },
  {
    "text": "That's not really--\nlet's say it's going through those two guys. And this is the line which\ncan be formed by looking only",
    "start": "1520620",
    "end": "1527800"
  },
  {
    "text": "at linear combination. So this is the line of\nx times t for t in r2.",
    "start": "1527800",
    "end": "1536330"
  },
  {
    "text": "That's this entire\nline that you can get. Why is it-- yeah, sorry,\nit's not just a line,",
    "start": "1536330",
    "end": "1542870"
  },
  {
    "text": "I also have to have\nt, all the 0's thing. So that actually\ncreates an entire plane,",
    "start": "1542870",
    "end": "1548860"
  },
  {
    "text": "which is going to be really\nhard for me to represent.",
    "start": "1548860",
    "end": "1554160"
  },
  {
    "text": "I don't know. I mean, maybe I shouldn't\ndo it in these dimensions.",
    "start": "1554160",
    "end": "1560044"
  },
  {
    "start": "1560044",
    "end": "1565130"
  },
  {
    "text": "So I'm going to do it like that. ",
    "start": "1565130",
    "end": "1571240"
  },
  {
    "text": "So this plane here is the\nset of xt for t and r2. ",
    "start": "1571240",
    "end": "1577770"
  },
  {
    "text": "So that's a two-dimensional\nplane, definitely goes to 0, and those are all these things.",
    "start": "1577770",
    "end": "1583760"
  },
  {
    "text": "So think of a sheet of\npaper in three dimensions. Those are the things I can get. So now, what I'm\ngoing to have as y",
    "start": "1583760",
    "end": "1589600"
  },
  {
    "text": "is not necessarily\nin this plane. y is actually something\nin this plane, x beta",
    "start": "1589600",
    "end": "1599440"
  },
  {
    "text": "plus some epsilon. ",
    "start": "1599440",
    "end": "1604810"
  },
  {
    "text": "y is x beta plus epsilon.",
    "start": "1604810",
    "end": "1610091"
  },
  {
    "text": "So I start from\nthis plane, and then I have this epsilon\nthat pushes me, maybe, outside of this plane. And what least squares\nis doing is saying,",
    "start": "1610091",
    "end": "1616370"
  },
  {
    "text": "well, I know that epsilon\nshould be fairly small, so the only thing I'm going to\nbe doing that actually makes",
    "start": "1616370",
    "end": "1621670"
  },
  {
    "text": "sense is to take y and\nfind the point that's on this plane that's\nthe closest to it. And that corresponds to doing\nan orthogonal projection of y",
    "start": "1621670",
    "end": "1630010"
  },
  {
    "text": "onto this thing, and that's\nactually exactly x beta hat. ",
    "start": "1630010",
    "end": "1638840"
  },
  {
    "text": "So in one dimension, just\nbecause this is actually a little hard-- in one dimension, so\nthat's if p is equal to 1.",
    "start": "1638840",
    "end": "1654140"
  },
  {
    "text": "So let's say this is my point. And then I have y, which\nis in two dimensions, so this is all on the plane.",
    "start": "1654140",
    "end": "1660020"
  },
  {
    "text": " What it does, this is my-- the point that's right here\nis actually x beta hat.",
    "start": "1660020",
    "end": "1668579"
  },
  {
    "text": "That's how you find x beta hat. You take your point\ny and you project it on the linear span\nof the columns of x.",
    "start": "1668579",
    "end": "1674490"
  },
  {
    "text": "And that's x beta hat. This does not tell you\nexactly what beta should be. And if you know a little\nbit of linear algebra,",
    "start": "1674490",
    "end": "1680990"
  },
  {
    "text": "it's pretty clear, because\nif you want to find beta hat, that means that you\nshould be able to find",
    "start": "1680990",
    "end": "1686330"
  },
  {
    "text": "the coordinates of a point in\nthe system of columns of x.",
    "start": "1686330",
    "end": "1692283"
  },
  {
    "text": "And if those guys are\nredundant, there's not going to be unique\ncoordinates for these guys,",
    "start": "1692284",
    "end": "1697429"
  },
  {
    "text": "so that's why it's\nactually not easy to find. But x beta hat is\nuniquely defined. It's a projection. Yeah?",
    "start": "1697430",
    "end": "1702744"
  },
  {
    "text": "AUDIENCE: And epsilon\nis the distance between the y and the-- PHILIPPE RIGOLLET: No, epsilon\nis the vector that goes from--",
    "start": "1702744",
    "end": "1709630"
  },
  {
    "text": "so there's a true x beta. That's the true one.",
    "start": "1709630",
    "end": "1716245"
  },
  {
    "text": "It's not clear. I mean, x beta hat is unlikely\nto be exactly equal to x beta.",
    "start": "1716245",
    "end": "1721940"
  },
  {
    "text": "And then the epsilon is the\none that starts from this line. It's the vector that\npushes you away. So really, this is this vector.",
    "start": "1721940",
    "end": "1728240"
  },
  {
    "text": "That's epsilon. So it's not a length. The lengths of epsilon\nis the distance,",
    "start": "1728240",
    "end": "1734245"
  },
  {
    "text": "but epsilon is just the\nactual vector that takes you from one to the other. ",
    "start": "1734245",
    "end": "1741600"
  },
  {
    "text": "So this is all in\ntwo dimensions, and it's probably much\nclearer than what's here. ",
    "start": "1741600",
    "end": "1749080"
  },
  {
    "text": "And so here, I claim\nthat this x beta hat-- so from this\npicture, I implicitly",
    "start": "1749080",
    "end": "1755110"
  },
  {
    "text": "claim that forming this\noperator that ticks y",
    "start": "1755110",
    "end": "1762980"
  },
  {
    "text": "and maps it into this vector\nx times x transpose y, blah, blah, blah, this should actually\nbe equal to the projection of y",
    "start": "1762980",
    "end": "1773570"
  },
  {
    "text": "onto the linear span\nof the columns of x.",
    "start": "1773570",
    "end": "1784990"
  },
  {
    "text": "That's what I just drew for you. And what it means\nis that this matrix must be the projection matrix. ",
    "start": "1784990",
    "end": "1794350"
  },
  {
    "text": "So of course, anybody-- who knows linear algebra here?",
    "start": "1794350",
    "end": "1799400"
  },
  {
    "text": "OK, wow. So what are the conditions\nthat a projection matrix",
    "start": "1799400",
    "end": "1804559"
  },
  {
    "text": "should be satisfying? AUDIENCE: Squares\nthrough itself. PHILIPPE RIGOLLET: Squares\nthrough itself, right. If I project twice,\nI'm not moving.",
    "start": "1804560",
    "end": "1811429"
  },
  {
    "text": "If I keep on\niterating projection, once I'm in the space\nI'm projecting onto, I'm not moving.",
    "start": "1811430",
    "end": "1816854"
  },
  {
    "text": "What else? ",
    "start": "1816854",
    "end": "1824970"
  },
  {
    "text": "Do they have to be\nsymmetric, maybe? AUDIENCE: If it's an\northogonal projection. PHILIPPE RIGOLLET: Yeah, so this\nis an orthogonal projection.",
    "start": "1824970",
    "end": "1832501"
  },
  {
    "text": "It has to be symmetric. And that's pretty much it. So from those things,\nyou can actually",
    "start": "1832501",
    "end": "1838520"
  },
  {
    "text": "get quite a bit of things. But what's interesting\nis that if you actually look at the eigenvalues\nof this matrix,",
    "start": "1838520",
    "end": "1844549"
  },
  {
    "text": "they should be either\n0 or 1, essentially. And they are 1 if the\neigenvector associated",
    "start": "1844550",
    "end": "1852320"
  },
  {
    "text": "is within this space,\nand 0 otherwise. And so that's basically\nwhat you can check. This is not an exercise\nin linear algebra,",
    "start": "1852320",
    "end": "1858630"
  },
  {
    "text": "so I'm not going to go too\nmuch into those details. But this is essentially what\nyou want to keep in mind. What's associated to\northogonal projections",
    "start": "1858630",
    "end": "1865460"
  },
  {
    "text": "is Pythagoras theorem. And that's something that's\ngoing to be useful for us. What it's essentially\ntelling is that if I",
    "start": "1865460",
    "end": "1872150"
  },
  {
    "text": "look at this norm squared, it's\nequal to this norm squared-- sorry, this norm squared\nplus this norm squared",
    "start": "1872150",
    "end": "1878300"
  },
  {
    "text": "is equal to this norm squared. And that's something\nthe norm of y squared. So Pythagoras tells me\nthat the norm of y squared",
    "start": "1878300",
    "end": "1892040"
  },
  {
    "text": "is equal to the norm of x beta\nhat squared plus the norm of y",
    "start": "1892040",
    "end": "1900090"
  },
  {
    "text": "minus x beta hat squared. ",
    "start": "1900090",
    "end": "1906120"
  },
  {
    "text": "Agreed? It's just because I have\na straight angle here.",
    "start": "1906120",
    "end": "1911700"
  },
  {
    "text": "So that's this plus\nthis is equal to this. ",
    "start": "1911700",
    "end": "1918840"
  },
  {
    "text": "So now, to define this,\nI made no assumption. Epsilon could be as wild.",
    "start": "1918840",
    "end": "1924630"
  },
  {
    "text": "I was just crossing my fingers\nthat epsilon was actually small enough that\nit would make sense",
    "start": "1924630",
    "end": "1929910"
  },
  {
    "text": "to project onto the linear\nspan, because I implicitly assumed that epsilon did not\ntake me all the way there,",
    "start": "1929910",
    "end": "1936639"
  },
  {
    "text": "so that actually, it makes\nsense to project back. And so for that, I need to\nsomehow make assumptions",
    "start": "1936640",
    "end": "1942240"
  },
  {
    "text": "that epsilon is\nwell-behaved and that it's completely wild, that\nit's moving uniformly",
    "start": "1942240",
    "end": "1951330"
  },
  {
    "text": "in all directions of the space. There's no privileged\ndirection where it's always going,\notherwise, I'm going to make a\nsystematic error.",
    "start": "1951330",
    "end": "1957900"
  },
  {
    "text": "And I need that those epsilons\nare going to average somehow. So here are the\nassumptions we're",
    "start": "1957900",
    "end": "1964641"
  },
  {
    "text": "going to be making so\nthat we can actually do some statistical inference. The first one is that the\ndesign matrix is deterministic.",
    "start": "1964641",
    "end": "1973350"
  },
  {
    "text": "So I started by saying the x-- I have xi, yi, and maybe\nthey're independent.",
    "start": "1973350",
    "end": "1978570"
  },
  {
    "text": "Here, they are, but the xi's, I\nwant to think as deterministic. If they're not deterministic,\nit can condition on them,",
    "start": "1978570",
    "end": "1986399"
  },
  {
    "text": "but otherwise,\nit's very difficult to think about this thing\nif I think of those entries",
    "start": "1986400",
    "end": "1991770"
  },
  {
    "text": "as being random,\nbecause then I have the inverse of a random matrix,\nand things become very, very",
    "start": "1991770",
    "end": "1997470"
  },
  {
    "text": "complicated. So we're to think of those\nguys as being deterministic. We're going to think of the\nmodel as being homoscedastic.",
    "start": "1997470",
    "end": "2007400"
  },
  {
    "text": "And actually, let me come\nback to this in a second. Homoscedastic-- well,\nI mean, if you're trying to find the\netymology of this word,",
    "start": "2007400",
    "end": "2014330"
  },
  {
    "text": "\"homo\" means the same,\n\"scedastic\" means scaling. So what I want to say\nis that the epsilons",
    "start": "2014330",
    "end": "2020090"
  },
  {
    "text": "have the same scaling. And since my third assumption is\nthat epsilon is Gaussian, then",
    "start": "2020090",
    "end": "2026914"
  },
  {
    "text": "essentially, what I'm going\nto want is that they all share the same sigma squared.",
    "start": "2026914",
    "end": "2032899"
  },
  {
    "text": "They're independent, so this\nis definitely in the identity covariance matrix. And I want them to\nbe centered, as well.",
    "start": "2032900",
    "end": "2038450"
  },
  {
    "text": "That means that\nthere's no direction that I'm always privileging when\nI'm moving away from my plane",
    "start": "2038450",
    "end": "2044240"
  },
  {
    "text": "there. So these are\nimportant conditions.",
    "start": "2044240",
    "end": "2049969"
  },
  {
    "text": "It depends on how much\ninference you want to do. If you want to write t-tests,\nyou need all these assumptions.",
    "start": "2049969",
    "end": "2056310"
  },
  {
    "text": "But if you only want to\nwrite, for example, the fact that your least squares\nestimator is consistent,",
    "start": "2056310",
    "end": "2063230"
  },
  {
    "text": "you really just need\nthe fact that epsilon has variance sigma squared. The fact that it's\nGaussian won't matter, just",
    "start": "2063230",
    "end": "2069849"
  },
  {
    "text": "like Gaussianity doesn't\nmatter for a large number. Yeah? AUDIENCE: So the\nfirst assumption",
    "start": "2069850",
    "end": "2075480"
  },
  {
    "text": "that x has to be\ndeterministic, but I just made up this x1, x2-- PHILIPPE RIGOLLET:\nx is the matrix.",
    "start": "2075480",
    "end": "2081784"
  },
  {
    "text": "AUDIENCE: Yeah. So most are random\nvariables, right? PHILIPPE RIGOLLET: No,\nthat's the assumption.",
    "start": "2081785",
    "end": "2087075"
  },
  {
    "text": "AUDIENCE: OK. So I mean, once we collect the\ndata and put it in the matrix,",
    "start": "2087075",
    "end": "2092595"
  },
  {
    "text": "it becomes deterministic. So maybe I'm missing something. PHILIPPE RIGOLLET: Yeah. So this is for the\npurpose of the analysis.",
    "start": "2092595",
    "end": "2100510"
  },
  {
    "text": "I can actually assume that-- I look at my data,\nand I think of this. So what is the difference\nbetween thinking",
    "start": "2100510",
    "end": "2106210"
  },
  {
    "text": "of data as deterministic or\nthinking of it as random? When I talked about random\ndata, the only assumptions that I made were about\nthe distribution.",
    "start": "2106210",
    "end": "2112706"
  },
  {
    "text": "I said, well, if my x\nis a random variable, I want it to have this\nvariance and I want it to have, maybe, this distribution,\nthings like this.",
    "start": "2112706",
    "end": "2119250"
  },
  {
    "text": "Here, I'm actually making\nan assumption on the values",
    "start": "2119250",
    "end": "2125050"
  },
  {
    "text": "that I see. I'm seeing that the value\nthat you give me is--",
    "start": "2125050",
    "end": "2130119"
  },
  {
    "text": "the matrix is\nactually invertible. x transpose x will\nbe invertible. So I've never done\nthat before, assuming",
    "start": "2130120",
    "end": "2136690"
  },
  {
    "text": "that some random variable-- assuming that some Gaussian\nrandom variable was positive,",
    "start": "2136690",
    "end": "2141740"
  },
  {
    "text": "for example. We don't do that, because\nthere's always some probability that things don't happen if\nyou make things at random.",
    "start": "2141740",
    "end": "2149110"
  },
  {
    "text": "And so here, I'm just going\nto say, OK, forget about-- here, it's basically\na little stronger.",
    "start": "2149110",
    "end": "2154990"
  },
  {
    "text": "I start my assumption by saying,\nthe data that's given to me will actually satisfy\nthose assumptions.",
    "start": "2154990",
    "end": "2160630"
  },
  {
    "text": "And that means that\nI don't actually need to make some modeling\nassumption on this thing, because I'm actually\nputting directly",
    "start": "2160630",
    "end": "2166820"
  },
  {
    "text": "the assumption I want to see. ",
    "start": "2166820",
    "end": "2172650"
  },
  {
    "text": "So here, either I\nknow sigma squared or I don't know sigma squared. So is that clear? So essentially, I'm assuming\nthat I have this model, where",
    "start": "2172650",
    "end": "2181880"
  },
  {
    "text": "this guy, now, is\ndeterministic, and this",
    "start": "2181880",
    "end": "2186950"
  },
  {
    "text": "is some multivariate\nGaussian with mean 0 and covariance matrix\nidentity of rn.",
    "start": "2186950",
    "end": "2193500"
  },
  {
    "text": "That's the model I'm assuming. And I'm observing this, and\nI'm given this matrix x.",
    "start": "2193500",
    "end": "2200810"
  },
  {
    "text": "Where does this make sense? You could say, well, if I think\nof my rows as being people and I'm collecting genes,\nit's a little intense",
    "start": "2200810",
    "end": "2208084"
  },
  {
    "text": "to assume that I actually\nknow, ahead of time, what I'm going to be seeing,\nand that those things are deterministic. That's true, but it still\ndoes not prevent the analysis",
    "start": "2208084",
    "end": "2215630"
  },
  {
    "text": "to go through, for one. And second, a better example\nmight be this imaging example",
    "start": "2215630",
    "end": "2220970"
  },
  {
    "text": "that I described, where those\nx's are actually libraries. Those are libraries of\npatterns that people",
    "start": "2220970",
    "end": "2227570"
  },
  {
    "text": "have created, maybe\nfrom deep learning nets, or something like this. But they've created\npatterns, and they say that all images should\nbe representable as a linear",
    "start": "2227570",
    "end": "2234829"
  },
  {
    "text": "combination of those patterns. And those patterns are\nsomewhere in books, so they're certainly\ndeterministic. Everything that's actually\nwritten down in a book",
    "start": "2234830",
    "end": "2241190"
  },
  {
    "text": "is as deterministic as it gets. ",
    "start": "2241190",
    "end": "2249027"
  },
  {
    "text": "Any questions about\nthose assumptions? Those are the things we're\ngoing to be working with. There's only three of them. One is about x.",
    "start": "2249027",
    "end": "2255130"
  },
  {
    "text": "Actually, there's\nreally two of them. I mean, this guy\nalready appears here.",
    "start": "2255130",
    "end": "2261625"
  },
  {
    "text": "So there's two-- one on\nthe noise, one on the x's. That's it. ",
    "start": "2261625",
    "end": "2268480"
  },
  {
    "text": "Those things allow\nus to do quite a bit. They will allow us to-- ",
    "start": "2268480",
    "end": "2275410"
  },
  {
    "text": "well, that's\nactually-- they allow",
    "start": "2275410",
    "end": "2282980"
  },
  {
    "text": "me to write the distribution\nof beta hat, which is great,",
    "start": "2282980",
    "end": "2289100"
  },
  {
    "text": "because when I know the\ndistribution of my estimator, I know it's fluctuations.",
    "start": "2289100",
    "end": "2294250"
  },
  {
    "text": "If it's centered around\nthe true parameter, I know that it's going\nto be fluctuating around the true parameter.",
    "start": "2294250",
    "end": "2300440"
  },
  {
    "text": "And it should tell me\nwhat kind of distribution the fluctuations are. I actually know how to\nbuild confidence intervals.",
    "start": "2300440",
    "end": "2306260"
  },
  {
    "text": "I know how to build tests. I know how to build everything. It's just like when I told\nyou that asymptotically,",
    "start": "2306260",
    "end": "2311660"
  },
  {
    "text": "the empirical\nvariance was Gaussian with mean theta and standard\ndeviation that depended on n,",
    "start": "2311660",
    "end": "2319470"
  },
  {
    "text": "et cetera, that's basically\nthe only thing I needed. And this is what I'm\nactually getting here.",
    "start": "2319470",
    "end": "2324840"
  },
  {
    "text": "So let me start\nwith this statement. So remember, beta\nhat satisfied this,",
    "start": "2324840",
    "end": "2332087"
  },
  {
    "text": "so I'm going to rewrite it here. ",
    "start": "2332087",
    "end": "2337940"
  },
  {
    "text": "So beta hat was\nequal to x transpose x inverse x transpose y.",
    "start": "2337940",
    "end": "2347440"
  },
  {
    "text": "That was the definition\nthat we found. And now, I also know that y was\nequal to x beta plus epsilon.",
    "start": "2347440",
    "end": "2357450"
  },
  {
    "text": "So let me just replace y by\nx beta plus epsilon here. Yeah? AUDIENCE: Isn't it x transpose\nx inverse x transpose y?",
    "start": "2357450",
    "end": "2365185"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nYes, x transpose. Thank you. ",
    "start": "2365185",
    "end": "2371890"
  },
  {
    "text": "So I'm going to replace\ny by x beta plus epsilon. So that's-- and here\ncomes the magic.",
    "start": "2371890",
    "end": "2398560"
  },
  {
    "text": "I have an inverse of\na matrix, and then I have the true matrix, I\nhave the original matrix.",
    "start": "2398560",
    "end": "2403810"
  },
  {
    "text": "So this is actually the\nidentity times beta. And now this guy, well,\nthis is a Gaussian,",
    "start": "2403810",
    "end": "2411609"
  },
  {
    "text": "because this is a\nGaussian random vector, and I just multiply it by\na deterministic matrix.",
    "start": "2411610",
    "end": "2418540"
  },
  {
    "text": "So we're going to use the rule\nthat if I have, say, epsilon, which is n0 sigma, then\nb times epsilon is n0--",
    "start": "2418540",
    "end": "2429780"
  },
  {
    "text": "can somebody tell me what the\ncovariance matrix of b epsilon is? ",
    "start": "2429780",
    "end": "2435302"
  },
  {
    "text": "AUDIENCE: What is\ncapital B in this case? PHILIPPE RIGOLLET:\nIt's just a matrix. ",
    "start": "2435302",
    "end": "2442410"
  },
  {
    "text": "And for any matrix, I mean any\nmatrix that I can premultiply-- that I can postmultiply\nwith epsilon.",
    "start": "2442410",
    "end": "2448359"
  },
  {
    "text": "Yeah? AUDIENCE: b transpose b. PHILIPPE RIGOLLET: b transpose? AUDIENCE: Times b.",
    "start": "2448360",
    "end": "2453503"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nAnd sigma is gone. AUDIENCE: Oh,\ntimes sigma, sorry. PHILIPPE RIGOLLET:\nThat's the matrix, right?",
    "start": "2453503",
    "end": "2459010"
  },
  {
    "text": "AUDIENCE: b transpose sigma b. PHILIPPE RIGOLLET: Almost. ",
    "start": "2459010",
    "end": "2464255"
  },
  {
    "text": "Anybody wants to take a\nguess at the last one? I think we've removed\nall other possibilities.",
    "start": "2464255",
    "end": "2472790"
  },
  {
    "text": "It's b sigma b transpose. ",
    "start": "2472790",
    "end": "2480880"
  },
  {
    "text": "So if you ever answered\nto the question, do you know Gaussian\nrandom vectors,",
    "start": "2480880",
    "end": "2486589"
  },
  {
    "text": "but you did not know that,\nthere's a gap in your knowledge that you need to fill,\nbecause that's probably the most important property\nof Gaussian vectors.",
    "start": "2486590",
    "end": "2493880"
  },
  {
    "text": "When you multiply\nthem by matrices, you have a simple rule on how\nto update the covariance matrix.",
    "start": "2493880",
    "end": "2503390"
  },
  {
    "text": "So here, sigma is the identity.",
    "start": "2503390",
    "end": "2509250"
  },
  {
    "text": "And here, this is the\nmatrix b that I had here. So what this is is, basically,\nn, some multivariate n,",
    "start": "2509250",
    "end": "2518480"
  },
  {
    "text": "of course. Then I'm going to have 0. And so what I need to do is\nb times the identity times b",
    "start": "2518480",
    "end": "2524140"
  },
  {
    "text": "transpose, which is\njust b b transpose. And what is it going to tell me? It's x transpose x--",
    "start": "2524140",
    "end": "2532850"
  },
  {
    "text": "sorry, that's inverse--\ninverse x transpose, and then the transpose of this\nguy, which is x x",
    "start": "2532850",
    "end": "2541760"
  },
  {
    "text": "transpose x inverse transpose. But this matrix is\nsymmetric, so I'm actually",
    "start": "2541760",
    "end": "2547130"
  },
  {
    "text": "not going to make the\ntranspose of this guy. And again, magic shows up.",
    "start": "2547130",
    "end": "2554089"
  },
  {
    "text": "Inverse times the\nmatrix of those two guys cancel, and so this is\nactually equal to beta plus some n0 x\ntranspose x inverse.",
    "start": "2554090",
    "end": "2563990"
  },
  {
    "text": " Yeah? AUDIENCE: I'm a little\nlost on the [INAUDIBLE]..",
    "start": "2563990",
    "end": "2569454"
  },
  {
    "text": "So you define that as the\nb matrix, and what happens? PHILIPPE RIGOLLET: So I\njust apply this rule, right?",
    "start": "2569454",
    "end": "2574954"
  },
  {
    "text": "AUDIENCE: Yeah. PHILIPPE RIGOLLET: So\nif I multiply a matrix by a Gaussian, then let's\nsay this Gaussian had",
    "start": "2574954",
    "end": "2581840"
  },
  {
    "text": "mean 0, which is the\ncase of epsilon here, then the covariance\nmatrix that I get",
    "start": "2581840",
    "end": "2587960"
  },
  {
    "text": "is b times the original\ncovariance matrix times b transpose. So all I did is write this\nmatrix times the identity",
    "start": "2587960",
    "end": "2595290"
  },
  {
    "text": "times this matrix transpose. And the identity, of course,\ndoesn't play any role,",
    "start": "2595290",
    "end": "2600320"
  },
  {
    "text": "so I can remove it. It's just this matrix,\nthen the matrix transpose. And what happened?",
    "start": "2600320",
    "end": "2605370"
  },
  {
    "text": "So what is the transpose\nof this matrix? So I used the fact that if I\nlook at x transpose x inverse x",
    "start": "2605370",
    "end": "2612710"
  },
  {
    "text": "transpose, and now I look at the\nwhole transpose of this thing,",
    "start": "2612710",
    "end": "2619160"
  },
  {
    "text": "that's actually equal 2. And I use the rule that ab\ntranspose is b transpose a transpose-- let me finish--",
    "start": "2619160",
    "end": "2626030"
  },
  {
    "text": "and it's x x\ntranspose x inverse.",
    "start": "2626030",
    "end": "2631925"
  },
  {
    "text": " Yes? AUDIENCE: I thought the--",
    "start": "2631925",
    "end": "2638020"
  },
  {
    "text": "for epsilon, it\nwas sigma squared. PHILIPPE RIGOLLET:\nOh, thank you. There's a sigma\nsquared somewhere.",
    "start": "2638020",
    "end": "2643609"
  },
  {
    "text": "So this was sigma squared times\nthe identity, so I can just pick up a sigma\nsquared anywhere.",
    "start": "2643610",
    "end": "2650566"
  },
  {
    "text": " So here, in our case, so\nfor epsilon, this is sigma.",
    "start": "2650566",
    "end": "2668560"
  },
  {
    "text": "Sigma squared\ntimes the identity, that's my covariance matrix. ",
    "start": "2668560",
    "end": "2673920"
  },
  {
    "text": "You seem perplexed. AUDIENCE: It's just\na new idea for me to think of a maximum likelihood\nestimator as a random variable.",
    "start": "2673920",
    "end": "2681754"
  },
  {
    "text": "PHILIPPE RIGOLLET:\nOh, it should not be. Any estimator is\na random variable. AUDIENCE: Oh, yeah,\nthat's a good point.",
    "start": "2681754",
    "end": "2688131"
  },
  {
    "text": "PHILIPPE RIGOLLET:\n[LAUGHS] And I have not told you that this was\nthe maximum likelihood",
    "start": "2688132",
    "end": "2694110"
  },
  {
    "text": "estimator just yet. The estimator is\na random variable. There's a word-- some\npeople use estimate just",
    "start": "2694110",
    "end": "2700890"
  },
  {
    "text": "to differentiate the\nestimator while you're doing the analysis with random\nvariables and the values when you plug in the\nnumbers in there.",
    "start": "2700890",
    "end": "2709477"
  },
  {
    "text": "But then, of course, people use\nestimate because it's shorter, so then it's confusing.",
    "start": "2709477",
    "end": "2714660"
  },
  {
    "text": "So any questions about\nthis computation? Did I forget any other\nGreek letter along the way?",
    "start": "2714660",
    "end": "2720810"
  },
  {
    "text": "All right, I think we're good. So one thing that it\nsays-- and actually,",
    "start": "2720810",
    "end": "2726225"
  },
  {
    "text": "thank you for\npointing this out-- I said there's actually a\nlittle hidden statement there. By the way, this\nanswers this question.",
    "start": "2726225",
    "end": "2733130"
  },
  {
    "text": "Beta hat is of the form beta\nplus something that's centered, so it's indeed of the form\nGaussian with mean beta",
    "start": "2733130",
    "end": "2739484"
  },
  {
    "text": "and covariance matrix sigma\nsquared x transpose x inverse. ",
    "start": "2739484",
    "end": "2745520"
  },
  {
    "text": "So that's very nice. As long as x transpose\nx is not huge,",
    "start": "2745520",
    "end": "2750829"
  },
  {
    "text": "I'm going to have something\nthat is close to what I want.",
    "start": "2750830",
    "end": "2755900"
  },
  {
    "text": "Oh, sorry, x transpose\nx inverse is not huge. ",
    "start": "2755900",
    "end": "2761800"
  },
  {
    "text": "So there's a hidden\nclaim in there, which is that least\nsquares estimator",
    "start": "2761800",
    "end": "2768640"
  },
  {
    "text": "is equal to the maximum\nlikelihood estimator. ",
    "start": "2768640",
    "end": "2775500"
  },
  {
    "text": "Why does the maximum\nlikelihood estimator just enter the picture now? We've been talking about\nregression for the past 18",
    "start": "2775500",
    "end": "2783280"
  },
  {
    "text": "slides. And we've been talking\nabout estimators. And I just dumped on you\nthe least squares estimator,",
    "start": "2783280",
    "end": "2789069"
  },
  {
    "text": "but I never really came back\nto this thing that we know-- maybe the method of moments,\nor maybe the maximum likelihood",
    "start": "2789070",
    "end": "2795100"
  },
  {
    "text": "estimator. It turns out that those\ntwo things are the same. But if I want to talk about a\nmaximum likelihood estimator,",
    "start": "2795100",
    "end": "2801880"
  },
  {
    "text": "I need to have a likelihood. In particular, I need\nto have a density. And so if I want\na density, I have",
    "start": "2801880",
    "end": "2807599"
  },
  {
    "text": "to make those assumptions,\nsuch as the epsilons have",
    "start": "2807600",
    "end": "2813210"
  },
  {
    "text": "this Gaussian distribution. So why is this the maximum\nlikelihood estimator?",
    "start": "2813210",
    "end": "2818580"
  },
  {
    "text": "Well, remember, y is x\ntranspose beta plus epsilon.",
    "start": "2818580",
    "end": "2824740"
  },
  {
    "text": "So I actually have\na bunch of data. So what is my model here?",
    "start": "2824740",
    "end": "2834390"
  },
  {
    "text": "Well, its the\nfamily of Gaussians on n observations with\nmean x beta, variance sigma",
    "start": "2834390",
    "end": "2842460"
  },
  {
    "text": "squared identity,\nand beta lives in rp.",
    "start": "2842460",
    "end": "2851380"
  },
  {
    "text": "Here's my family\nof distributions. That's the possible\ndistributions for y.",
    "start": "2851380",
    "end": "2858160"
  },
  {
    "text": "And so in particular, I\ncan write the density of y. ",
    "start": "2858160",
    "end": "2867980"
  },
  {
    "text": "Well, what is it? It's something that\nlooks like p of x-- well, p of y, let's say,\nis equal to 1 over--",
    "start": "2867980",
    "end": "2878359"
  },
  {
    "text": "so now its going to be a\nlittle more complicated, but its sigma squared times 2\npi to the p/2 exponential minus",
    "start": "2878359",
    "end": "2897740"
  },
  {
    "text": "norm of y minus x beta squared\ndivided by 2 sigma squared.",
    "start": "2897740",
    "end": "2906840"
  },
  {
    "text": "So that's just the\nmultivariate Gaussian density. I just wrote it. That's the density of\na multivariate Gaussian",
    "start": "2906840",
    "end": "2913529"
  },
  {
    "text": "with mean x beta and\ncovariance matrix sigma squared times the identity. That's what it is.",
    "start": "2913530",
    "end": "2920410"
  },
  {
    "text": "So you don't have to\nlearn this by heart, but if you are familiar with\nthe case where p is equal to 1,",
    "start": "2920410",
    "end": "2927100"
  },
  {
    "text": "you can check that you recover\nwhat you're familiar with, and this makes sense\nas an extension.",
    "start": "2927100",
    "end": "2934811"
  },
  {
    "text": " So now, I can actually\nwrite my log likelihood.",
    "start": "2934811",
    "end": "2948560"
  },
  {
    "text": "How many observations do\nI have of this vector y?",
    "start": "2948560",
    "end": "2954880"
  },
  {
    "start": "2954880",
    "end": "2963710"
  },
  {
    "text": "Do I have n observations of y? ",
    "start": "2963710",
    "end": "2970580"
  },
  {
    "text": "I have just one, right? Oh, sorry, I shouldn't\nhave said p, this is n.",
    "start": "2970580",
    "end": "2976830"
  },
  {
    "text": "Everything is in dimension n. So I can think of either having\nn independent observations",
    "start": "2976830",
    "end": "2982700"
  },
  {
    "text": "of each coordinate,\nor I can think of having just one\nobservation of the vector y. So when I write\nmy log likelihood,",
    "start": "2982700",
    "end": "2990050"
  },
  {
    "text": "it's just the log\nof the density at y. ",
    "start": "2990050",
    "end": "3009090"
  },
  {
    "text": "And that's the\nvector y, which I can write as minus n/2\nlog sigma squared",
    "start": "3009090",
    "end": "3018990"
  },
  {
    "text": "2 pi minus 1 over 2 sigma\nsquared norm of y minus x beta.",
    "start": "3018990",
    "end": "3028690"
  },
  {
    "text": "And that's, again,\nmy boldface y. ",
    "start": "3028690",
    "end": "3036710"
  },
  {
    "text": "And what is my maximum\nlikelihood estimator? ",
    "start": "3036710",
    "end": "3044470"
  },
  {
    "text": "Well, this guy does\nnot depend on beta. And this is just a constant\nfactor in front of this guy.",
    "start": "3044470",
    "end": "3050849"
  },
  {
    "text": "So it's the same thing\nas just minimizing, because I have a minus\nsign, over all beta and rp.",
    "start": "3050850",
    "end": "3057230"
  },
  {
    "start": "3057230",
    "end": "3063140"
  },
  {
    "text": "y minus x beta squared,\nand that's my least squares estimator. ",
    "start": "3063140",
    "end": "3075312"
  },
  {
    "text": "Is there anything that's\nunclear on this board? Any question? ",
    "start": "3075312",
    "end": "3080549"
  },
  {
    "text": "So all I used was-- so I\nwrote my log likelihood, which is just the log\nof this expression",
    "start": "3080550",
    "end": "3085859"
  },
  {
    "text": "where y is my observation. And that's indeed the\nobservation that I have here.",
    "start": "3085860",
    "end": "3092430"
  },
  {
    "text": "And that was just some constant\nminus some constant times this quantity that\ndepends on beta.",
    "start": "3092430",
    "end": "3097960"
  },
  {
    "text": "So maximizing this whole\nthing is the same thing as minimizing only this thing. The minimizers are the same.",
    "start": "3097960",
    "end": "3104620"
  },
  {
    "text": "And so that tells me\nthat I actually just have to minimize\nthe squared norm to get my maximum\nlikelihood estimator.",
    "start": "3104620",
    "end": "3111710"
  },
  {
    "text": "But this used, heavily, the\nfact that I could actually write exactly what\nmy density was,",
    "start": "3111710",
    "end": "3123450"
  },
  {
    "text": "and that when I took\nthe log of this thing, I had exactly the square\nnorm that showed up.",
    "start": "3123450",
    "end": "3129660"
  },
  {
    "text": "If I had a different\ndensity, if, for example, I assumed that my coordinates\nof epsilons were, say, iid",
    "start": "3129660",
    "end": "3137039"
  },
  {
    "text": "double exponential\nrandom variables. So it's just half\nof an exponential. And the plus is half of an\nexponential on the negatives.",
    "start": "3137040",
    "end": "3144280"
  },
  {
    "text": "So if I said that,\nthen this would not have the square\nnorm that shows up. This is really\nidiosyncratic to Gaussians.",
    "start": "3144280",
    "end": "3151057"
  },
  {
    "text": "If I had something\nelse, I would have, maybe, a different norm\nhere, or something different measures the difference\nbetween y and x beta.",
    "start": "3151057",
    "end": "3159420"
  },
  {
    "text": "And that's how you come up\nwith other maximum likelihood estimators that leads\nto other estimators that are not the least squares--",
    "start": "3159420",
    "end": "3165420"
  },
  {
    "text": "maybe the least\nabsolute deviation, for example, or this\nfourth movement, for example, that you\nsuggested last time.",
    "start": "3165420",
    "end": "3172890"
  },
  {
    "text": "So I can come up with a\nbunch of different things, and they might be tied-- maybe I can come up from them\nfrom the same perspective",
    "start": "3172890",
    "end": "3179715"
  },
  {
    "text": "that I came from the\nleast squares estimator. I said, let's just\ndo something smart and check, then, that it's\nindeed the maximum likelihood",
    "start": "3179716",
    "end": "3186349"
  },
  {
    "text": "estimator. Or I could just start\nwith the modeling on-- and check, then, what happens--",
    "start": "3186350",
    "end": "3193260"
  },
  {
    "text": "what was the implicit assumption\nthat I put on my noise. Or I could start with the\nassumption of the noise, compute the maximum\nlikelihood estimator",
    "start": "3193260",
    "end": "3199830"
  },
  {
    "text": "and see what it turns into.  So that was the first thing.",
    "start": "3199830",
    "end": "3206760"
  },
  {
    "text": "I've just proved to\nyou the first line. And from there, you\ncan get what you want.",
    "start": "3206760",
    "end": "3211950"
  },
  {
    "text": "So all the other lines\nare going to follow. So what is beta hat-- so\nfor example, let's look",
    "start": "3211950",
    "end": "3219569"
  },
  {
    "text": "at the second line,\nthe quadratic risk. ",
    "start": "3219570",
    "end": "3226180"
  },
  {
    "text": "Beta hat minus beta,\nfrom this formula, has a distribution,\nwhich is n n0,",
    "start": "3226180",
    "end": "3233780"
  },
  {
    "text": "and then I have x\ntranspose x inverse. AUDIENCE: Wouldn't the\ndimension be p on the board?",
    "start": "3233780",
    "end": "3243298"
  },
  {
    "text": " PHILIPPE RIGOLLET: Sorry,\nthe dimension of what?",
    "start": "3243299",
    "end": "3250308"
  },
  {
    "text": "AUDIENCE: Oh beta\nhat minus beta. Isn't beta only a p dimensional? PHILIPPE RIGOLLET: Oh, yeah,\nyou're right, you're right.",
    "start": "3250308",
    "end": "3255620"
  },
  {
    "text": "That was all p\ndimensional there. ",
    "start": "3255620",
    "end": "3262170"
  },
  {
    "text": "Yeah. So if b here, the matrix\nthat I'm actually applying,",
    "start": "3262170",
    "end": "3268220"
  },
  {
    "text": "has dimension p times n-- so even if epsilon was an n\ndimensional Gaussian vector,",
    "start": "3268220",
    "end": "3274710"
  },
  {
    "text": "then b times epsilon is a p\ndimensional Gaussian vector now.",
    "start": "3274710",
    "end": "3279980"
  },
  {
    "text": "So that's how I\nswitch from p to n-- from n to p. Thank you.",
    "start": "3279980",
    "end": "3285120"
  },
  {
    "text": "So you're right, this is beta\nhat minus beta is this guy.",
    "start": "3285120",
    "end": "3290430"
  },
  {
    "text": "And so in particular, if\nI look at the expectation of the norm of beta hat minus\nbeta squared, what is it?",
    "start": "3290430",
    "end": "3301160"
  },
  {
    "text": "It's the expectation of the\nnorm of some Gaussian vector.",
    "start": "3301160",
    "end": "3308140"
  },
  {
    "text": " And so it turns out--\nso maybe we don't have--",
    "start": "3308140",
    "end": "3315530"
  },
  {
    "text": "well, that's just also a\nproperty of a Gaussian vector. So if epsilon is n0 sigma,\nthen the expectation",
    "start": "3315530",
    "end": "3326839"
  },
  {
    "text": "of the norm of epsilon squared\nis just the trace of sigma.",
    "start": "3326840",
    "end": "3334576"
  },
  {
    "text": " Actually, we can\nprobably check this",
    "start": "3334576",
    "end": "3341030"
  },
  {
    "text": "by saying that this is\nthe sum from j equal 1 to p of the expectation of beta\nhat j minus beta j squared.",
    "start": "3341030",
    "end": "3351128"
  },
  {
    "text": " Since beta j squared is\nthe expectation-- beta j",
    "start": "3351128",
    "end": "3357879"
  },
  {
    "text": "is the expectation of beta hat. This is actually equal\nto the sum from j equal 1 to p of the variance\nof beta hat j,",
    "start": "3357879",
    "end": "3368110"
  },
  {
    "text": "just because this is the\nexpectation of beta hat. And how do I read the variances\nin a covariance matrix?",
    "start": "3368110",
    "end": "3375589"
  },
  {
    "text": "There are just the\ndiagonal elements. So that's really just sigma jj.",
    "start": "3375590",
    "end": "3385390"
  },
  {
    "text": "And so that's really equal to-- so that's the sum of\nthe diagonal elements of this matrix.",
    "start": "3385390",
    "end": "3390790"
  },
  {
    "text": "Let's call it sigma. So that's equal to the trace\nof x transpose x inverse.",
    "start": "3390790",
    "end": "3400020"
  },
  {
    "text": " The trace is the sum of the\ndiagonal elements of a matrix.",
    "start": "3400020",
    "end": "3405364"
  },
  {
    "text": " And I still had something else. I'm sorry, this\nwas sigma squared.",
    "start": "3405364",
    "end": "3412070"
  },
  {
    "text": "I forget it all the time. So the sigma squared comes out. It's there.",
    "start": "3412070",
    "end": "3418760"
  },
  {
    "text": "And so the sigma\nsquared comes out because the trace is\na linear operator. If I multiply all the entries\nof my matrix by the same number,",
    "start": "3418760",
    "end": "3426275"
  },
  {
    "text": "then all the diagonal\nelements are multiplied by the same number,\nso when I sum them, the sum is multiplied\nby the same number.",
    "start": "3426275",
    "end": "3433930"
  },
  {
    "text": "So that's for the\nquadratic risk of beta hat. And now I need to tell\nyou about x beta hat.",
    "start": "3433930",
    "end": "3441580"
  },
  {
    "text": "x beta hat was something\nthat was actually telling me",
    "start": "3441580",
    "end": "3447250"
  },
  {
    "text": "that that was the point that\nI reported on the red line that I estimated. That was my x beta hat.",
    "start": "3447250",
    "end": "3452800"
  },
  {
    "text": "That was my y minus the noise.",
    "start": "3452800",
    "end": "3460310"
  },
  {
    "text": "Now, this thing here-- so remember, we had this line,\nand I had my observation.",
    "start": "3460310",
    "end": "3467099"
  },
  {
    "text": "And here, I'm really trying to\nmeasure this distance squared. This distance is actually\nquite important for me",
    "start": "3467100",
    "end": "3473470"
  },
  {
    "text": "because it actually shows up\nin the Pythagoras theorem.",
    "start": "3473470",
    "end": "3478920"
  },
  {
    "text": "And so you could actually\ntry to estimate this thing. So what is the prediction error? ",
    "start": "3478920",
    "end": "3492900"
  },
  {
    "text": "So we said we have y minus\nx beta hat, so that's",
    "start": "3492900",
    "end": "3498839"
  },
  {
    "text": "the norm of this thing\nwe're trying to compute. But let's write this for\nwhat it is for one second.",
    "start": "3498840",
    "end": "3505349"
  },
  {
    "text": "So we said that beta\nhat was x transpose x inverse extra transpose\ny, and we know that y is",
    "start": "3505350",
    "end": "3511710"
  },
  {
    "text": "x transpose beta plus epsilon. So let's write this--",
    "start": "3511710",
    "end": "3517410"
  },
  {
    "text": " x beta plus epsilon plus x.",
    "start": "3517410",
    "end": "3523800"
  },
  {
    "start": "3523800",
    "end": "3537000"
  },
  {
    "text": "And actually, maybe I\nshould not write it. Let me keep the y\nfor what it is now.",
    "start": "3537000",
    "end": "3542722"
  },
  {
    "text": " So that means that\nI have, essentially,",
    "start": "3542722",
    "end": "3548960"
  },
  {
    "text": "the identity of rn times y\nminus this matrix times y. So I can factor\ny out, and that's",
    "start": "3548960",
    "end": "3555510"
  },
  {
    "text": "the identity of rn\nminus x x transpose x inverse x transpose,\nthe whole thing times y.",
    "start": "3555510",
    "end": "3567280"
  },
  {
    "start": "3567280",
    "end": "3572760"
  },
  {
    "text": "We call this matrix p because\nthis was the projection matrix",
    "start": "3572760",
    "end": "3577980"
  },
  {
    "text": "onto the linear span of the x's. So that means that if I take a\npoint x and I apply p times x,",
    "start": "3577980",
    "end": "3586120"
  },
  {
    "text": "I'm projecting onto the linear\nspan of the columns of x. What happens if I do\ni minus p times x?",
    "start": "3586120",
    "end": "3597400"
  },
  {
    "text": "It's x minus px.  So if I look at the\npoint on which--",
    "start": "3597400",
    "end": "3604690"
  },
  {
    "text": "so this is the point\non which I project. This is x. I project orthogonally\nto get p times x.",
    "start": "3604690",
    "end": "3613260"
  },
  {
    "text": "And so what it means\nis that this operator i minus px is actually giving me\nthis guy, this vector here--",
    "start": "3613260",
    "end": "3621809"
  },
  {
    "text": "x minus p times x. ",
    "start": "3621810",
    "end": "3630790"
  },
  {
    "text": "Let's say this is 0. This means that this\nvector, I can put it here.",
    "start": "3630790",
    "end": "3636460"
  },
  {
    "text": "It's this vector here. And that's actually the\northogonal projection of x onto the orthogonal\ncomplement of the span",
    "start": "3636460",
    "end": "3643870"
  },
  {
    "text": "of the columns of x. So if I project x, or if I\nlook of x minus its projection,",
    "start": "3643870",
    "end": "3651000"
  },
  {
    "text": "I'm basically projecting\nonto two orthogonal spaces. What I'm trying to say\nhere is that this here",
    "start": "3651000",
    "end": "3659520"
  },
  {
    "text": "is another projection\nmatrix p prime.  That is just the projection\nmatrix onto the orthogonal--",
    "start": "3659520",
    "end": "3670310"
  },
  {
    "text": "projection onto orthogonal\nof column span of x.",
    "start": "3670310",
    "end": "3689560"
  },
  {
    "text": "Orthogonal means\nthe set of vectors that's orthogonal to everyone\nin this linear space. ",
    "start": "3689560",
    "end": "3697050"
  },
  {
    "text": "So now, when I'm doing\nthis, this is exactly what-- I mean, in a way, this is\nillustrating this Pythagoras",
    "start": "3697050",
    "end": "3702600"
  },
  {
    "text": "theorem. And so when I want to compute\nthe norm of this guy, the norm squared of this guy,\nI'm really computing--",
    "start": "3702600",
    "end": "3709560"
  },
  {
    "text": "if this is my y now,\nthis is px of y, I'm really controlling the\nnorm squared of this thing.",
    "start": "3709560",
    "end": "3715738"
  },
  {
    "start": "3715738",
    "end": "3726720"
  },
  {
    "text": "So if I want to compute\nthe norm squared-- ",
    "start": "3726720",
    "end": "3762540"
  },
  {
    "text": "so I'm almost there.",
    "start": "3762540",
    "end": "3768020"
  },
  {
    "text": "So what am I projecting here\nonto the orthogonal projector? So here, y, now,\nI know that y is",
    "start": "3768020",
    "end": "3775340"
  },
  {
    "text": "equal to x beta plus epsilon.",
    "start": "3775340",
    "end": "3780480"
  },
  {
    "text": "So when I look at this\nmatrix p prime times y,",
    "start": "3780480",
    "end": "3786480"
  },
  {
    "text": "It's actually p prime times\nx beta plus p prime times epsilon.",
    "start": "3786480",
    "end": "3791604"
  },
  {
    "text": " What's happening to\np prime times x beta?",
    "start": "3791604",
    "end": "3798400"
  },
  {
    "text": "Let's look at this picture.  So we know that p prime takes\nany point here and projects it",
    "start": "3798400",
    "end": "3806609"
  },
  {
    "text": "orthogonally on this guy. But x beta is actually\na point that lives here.",
    "start": "3806610",
    "end": "3813960"
  },
  {
    "text": "It's something that's\non the linear span. So where do all the points\nthat are on this line",
    "start": "3813960",
    "end": "3819660"
  },
  {
    "text": "get projected to? AUDIENCE: The origin. PHILIPPE RIGOLLET:\nThe origin, to 0.",
    "start": "3819660",
    "end": "3825920"
  },
  {
    "text": "They all get projected to 0. And that's because I'm\nbasically projecting something that's on the column\nspan of x onto its orthogonal.",
    "start": "3825920",
    "end": "3834872"
  },
  {
    "text": "So that's always 0\nthat I'm getting here. ",
    "start": "3834872",
    "end": "3842410"
  },
  {
    "text": "So when I apply\np prime to y, I'm really just applying\np prime to epsilon.",
    "start": "3842410",
    "end": "3848609"
  },
  {
    "text": "So I know that now,\nthis, actually, is equal to the norm of\nsome multivariate Gaussian.",
    "start": "3848610",
    "end": "3858480"
  },
  {
    "text": "What is the size\nof this Gaussian?  What is the size of this matrix?",
    "start": "3858480",
    "end": "3864570"
  },
  {
    "text": "Well, I actually had it there. It's i n, so it's n dimensional. So it's some n\ndimensional with mean 0.",
    "start": "3864570",
    "end": "3871235"
  },
  {
    "text": "And what is the\ncovariance matrix of p prime times epsilon? ",
    "start": "3871236",
    "end": "3879109"
  },
  {
    "text": "AUDIENCE: p p transpose. PHILIPPE RIGOLLET: Yeah,\np prime p prime transpose, which we just said p\nprime transpose is p,",
    "start": "3879109",
    "end": "3888500"
  },
  {
    "text": "so that's p squared. And we see that when\nwe project twice, it's as if we\nprojected only once.",
    "start": "3888500",
    "end": "3894540"
  },
  {
    "text": "So here, this is n0 p\nprime p prime transpose.",
    "start": "3894540",
    "end": "3900090"
  },
  {
    "text": "That's the formula for\nthe covariance matrix.",
    "start": "3900090",
    "end": "3905150"
  },
  {
    "text": "But this guy is actually equal\nto p prime times p prime, which is equal to p prime.",
    "start": "3905150",
    "end": "3913580"
  },
  {
    "text": "So now, what I'm looking for is\nthe norm squared of the trace. So that means that\nthis whole thing here",
    "start": "3913580",
    "end": "3920049"
  },
  {
    "text": "is actually equal to the trace. Oh, did I forget\nagain a sigma squared? Yeah, I forgot it only\nhere, which is good news.",
    "start": "3920050",
    "end": "3928160"
  },
  {
    "text": "So I should assume that\nsigma squared is equal to 1. So sigma squared's here.",
    "start": "3928160",
    "end": "3934270"
  },
  {
    "text": "And then what I'm left\nwith is sigma squared times the trace of p prime.",
    "start": "3934270",
    "end": "3939920"
  },
  {
    "start": "3939920",
    "end": "3945780"
  },
  {
    "text": "At some point, I mentioned that\nthe eigenvalues of a projection",
    "start": "3945780",
    "end": "3951240"
  },
  {
    "text": "matrix were actually 0 or 1. The trace is the sum\nof the eigenvalues.",
    "start": "3951240",
    "end": "3956689"
  },
  {
    "text": "So that means that\nthe trace is going to be an integer number as the\nnumber of non-0 eigenvalues.",
    "start": "3956689",
    "end": "3963720"
  },
  {
    "text": "And the non-0\neigenvalues are just the dimension of the space\nonto which I'm projecting. ",
    "start": "3963720",
    "end": "3970490"
  },
  {
    "text": "Now, I'm projecting from\nsomething of dimension n onto the orthogonal of\na space of dimension p.",
    "start": "3970490",
    "end": "3979520"
  },
  {
    "text": "What is the dimension\nof the orthogonal of a space of dimension\np when thought of space in dimension n?",
    "start": "3979520",
    "end": "3986546"
  },
  {
    "text": "AUDIENCE: [? 1. ?] PHILIPPE RIGOLLET: N minus p-- that's the so-called rank\ntheorem, I guess, as a name.",
    "start": "3986546",
    "end": "3992980"
  },
  {
    "text": "And so that's how I get\nthis n minus p here. This is really just\nequal to n minus p.",
    "start": "3992980",
    "end": "4000071"
  },
  {
    "text": "Yeah? AUDIENCE: Here, we're taking the\nexpectation of the whole thing. PHILIPPE RIGOLLET:\nYes, you're right. So that's actually\nthe expectation",
    "start": "4000071",
    "end": "4008780"
  },
  {
    "text": "of this thing that's\nequal to that. Absolutely. But I actually have much better.",
    "start": "4008780",
    "end": "4015150"
  },
  {
    "text": "I know, even, that the\nnorm that I'm looking at, I know it's going\nto be this thing. What is going to be the\ndistribution of this guy?",
    "start": "4015150",
    "end": "4020911"
  },
  {
    "text": " Norm squared of a\nGaussian, chi squared.",
    "start": "4020911",
    "end": "4026830"
  },
  {
    "text": "So there's going to be some\nchi squared that shows up. And the number of\ndegrees of freedom is actually going to\nbe also n minus p.",
    "start": "4026830",
    "end": "4032940"
  },
  {
    "text": "And maybe it's\nactually somewhere-- yeah, right here-- n\nminus p times sigma hat",
    "start": "4032940",
    "end": "4040560"
  },
  {
    "text": "squared over sigma squared. This is my sigma hat squared. If I multiply n minus p, I'm\nleft only with this thing,",
    "start": "4040560",
    "end": "4048200"
  },
  {
    "text": "and so that means that I\nget sigma squared times-- because they always\nforget my sigma squared-- I get sigma squared\ntimes this thing.",
    "start": "4048200",
    "end": "4054870"
  },
  {
    "text": "And it turns out that the\nsquare norm of this guy is actually exactly chi\nsquared with n minus b degrees of freedom.",
    "start": "4054870",
    "end": "4060226"
  },
  {
    "text": " So in particular, so we\nknow that the expectation",
    "start": "4060226",
    "end": "4067900"
  },
  {
    "text": "of this thing is equal to\nsigma squared times n minus p. So if I divide both\nsides by n minus p,",
    "start": "4067900",
    "end": "4073341"
  },
  {
    "text": "I'm going to have that\nsomething whose expectation is sigma squared. And this something, I\ncan actually compute.",
    "start": "4073342",
    "end": "4079140"
  },
  {
    "text": "It depends on y,\nand x that I know, and beta hat that\nI've just estimated. I know what n is.",
    "start": "4079140",
    "end": "4085000"
  },
  {
    "text": "And pr's are the\ndimensions of my matrix x. So I'm actually given an\nestimator whose expectation",
    "start": "4085000",
    "end": "4091120"
  },
  {
    "text": "is sigma squared. And so now, I actually\nhave an unbiased estimator of sigma squared.",
    "start": "4091120",
    "end": "4097429"
  },
  {
    "text": "That's this guy right here. And it's actually super useful. ",
    "start": "4097430",
    "end": "4103470"
  },
  {
    "text": "So those are called the-- this is the normalized\nsum of square residuals. These are called the residuals.",
    "start": "4103470",
    "end": "4109339"
  },
  {
    "text": "Those are whatever\nis residual when I project my points onto the\nline that I've estimated.",
    "start": "4109340",
    "end": "4116580"
  },
  {
    "text": "And so in a way, those guys--\nif you go back to this picture, this was yi and this was\nxi transpose beta hat.",
    "start": "4116580",
    "end": "4127109"
  },
  {
    "text": "So if beta hat is close\nto beta, the difference between yi and xi\ntranspose beta should",
    "start": "4127109",
    "end": "4132810"
  },
  {
    "text": "be close to my epsilon i. It's some sort of epsilon i hat. ",
    "start": "4132810",
    "end": "4140318"
  },
  {
    "text": "Agreed? And so that means\nthat if I think of those as being\nepsilon i hat, they",
    "start": "4140319",
    "end": "4147509"
  },
  {
    "text": "should be close to epsilon\ni, and so their norm should be giving me something\nthat looks like sigma squared.",
    "start": "4147510",
    "end": "4154390"
  },
  {
    "text": "And so that's why it\nactually makes sense. It's just magical that\neverything works out together, because I'm not projecting\non the right line,",
    "start": "4154390",
    "end": "4161130"
  },
  {
    "text": "I'm actually projecting\non the wrong line. But in the end, things\nactually work out pretty well.",
    "start": "4161130",
    "end": "4167310"
  },
  {
    "text": "There's one thing--\nso here, the theorem is that this thing not only\nhas the right expectation, but also has a chi\nsquared distribution.",
    "start": "4167310",
    "end": "4173449"
  },
  {
    "text": "That's what we just discussed. So here, I'm just\ntelling you this. But it's not too\nhard to believe, because it's actually\nthe norm of some vector.",
    "start": "4173450",
    "end": "4180299"
  },
  {
    "text": "You could make this\nobvious, but again, I didn't want to bring in\ntoo much linear algebra. So to prove this,\nyou actually have",
    "start": "4180300",
    "end": "4186359"
  },
  {
    "text": "to diagonalize the matrix p. So you have to invoke the\neigenvalue decomposition",
    "start": "4186359",
    "end": "4193890"
  },
  {
    "text": "and the fact that the norm\nis invariant by rotation. So for those who are\nfamiliar with, what I can do",
    "start": "4193890",
    "end": "4199440"
  },
  {
    "text": "is just look at the\ndecomposition of p prime into ud u transpose where\nthis is an orthogonal matrix,",
    "start": "4199440",
    "end": "4208199"
  },
  {
    "text": "and this is a diagonal\nmatrix of eigenvalues. And when I look at the\nnorm squared of this thing,",
    "start": "4208200",
    "end": "4213312"
  },
  {
    "text": "I mean, I have,\nbasically, the norm squared of p prime\ntimes some epsilon.",
    "start": "4213312",
    "end": "4220200"
  },
  {
    "text": "It's the norm of ud u\ntranspose epsilon squared.",
    "start": "4220200",
    "end": "4226300"
  },
  {
    "text": "The norm of a\nrotation of a vector is the same as the norm of the\nvector, so this guy goes away.",
    "start": "4226300",
    "end": "4232280"
  },
  {
    "text": "This is not actually-- I mean, you don't have\nto care about this if you don't understand what I'm\nsaying, so don't freak out.",
    "start": "4232280",
    "end": "4237880"
  },
  {
    "text": "This is really for\nthose who follow. What is the distribution\nof u transpose epsilon? ",
    "start": "4237880",
    "end": "4245899"
  },
  {
    "text": "I take a Gaussian vector that\nhas covariance matrix sigma squared times the\n[? identity, ?] and I basically",
    "start": "4245899",
    "end": "4252560"
  },
  {
    "text": "rotate it. What is its distribution?",
    "start": "4252560",
    "end": "4257965"
  },
  {
    "text": "Yeah? AUDIENCE: The same. PHILIPPE RIGOLLET:\nIt's the same. It's completely invariant,\nbecause the Gaussian think of all directions\nas being the same.",
    "start": "4257965",
    "end": "4264700"
  },
  {
    "text": "So it doesn't really matter if\nI take a Gaussian or a rotated Gaussian. So this is also a\nGaussian, so I'm",
    "start": "4264700",
    "end": "4270190"
  },
  {
    "text": "going to call it epsilon prime. And I am left with just\nthe norm of epsilon primes. So this is the sum of the\ndj's squared times epsilon",
    "start": "4270190",
    "end": "4283730"
  },
  {
    "text": "j squared.  And we just said that\nthe eigenvalues of p",
    "start": "4283730",
    "end": "4290059"
  },
  {
    "text": "are either 0 or 1,\nbecause it's a projector. And so here, I'm going\nto get only 0's and 1's.",
    "start": "4290060",
    "end": "4296090"
  },
  {
    "text": "So I'm really just\nsumming a certain number of epsilon i squared.",
    "start": "4296090",
    "end": "4302050"
  },
  {
    "text": "So square root of\nstandard Gaussians-- sorry, with a sigma\nsquared somewhere.",
    "start": "4302050",
    "end": "4308210"
  },
  {
    "text": "And basically, how\nmany am I summing? Well, the n minus p, the\nnumber of non-0 eigenvalues",
    "start": "4308210",
    "end": "4315530"
  },
  {
    "text": "of p prime. So that's how it shows up. When you see this, what\ntheorem am I using here?",
    "start": "4315530",
    "end": "4325820"
  },
  {
    "text": "Cochran's theorem. This is this magic book. I'm actually going to dump\neverything that I'm not going to prove to you and say, oh,\nthis is actually Cochran's.",
    "start": "4325820",
    "end": "4331160"
  },
  {
    "text": "No, Cochran's theorem\nis really just telling me something about\northogonality of things, and therefore,\nindependence of things.",
    "start": "4331160",
    "end": "4337712"
  },
  {
    "text": "And Cochran's\ntheorem was something that I used when I\nwanted to use what?",
    "start": "4337712",
    "end": "4343271"
  },
  {
    "text": "That's something I used\njust one slide before. Student t-test, right?",
    "start": "4343271",
    "end": "4348886"
  },
  {
    "text": "I used Cochran's theorem\nto see that the numerator and the denominator of\nthe student statistic were independent of each other.",
    "start": "4348887",
    "end": "4355414"
  },
  {
    "text": "And this is exactly what\nI'm going to do here.  I'm going to actually write\na test to test, maybe,",
    "start": "4355414",
    "end": "4362429"
  },
  {
    "text": "if the beta j's are equal to 0. I'm going to form a numerator,\nwhich is beta hat minus beta.",
    "start": "4362430",
    "end": "4369110"
  },
  {
    "text": "This is normal. And we know that beta hat\nhas a Gaussian distribution. I'm going to\nstandardized by something",
    "start": "4369110",
    "end": "4374870"
  },
  {
    "text": "that makes sense to me. And I'm not going\nto go into details, because we're out of time. But there's the sigma\nhat that shows up. And then there's a gamma\nj, which takes into account",
    "start": "4374870",
    "end": "4383240"
  },
  {
    "text": "the fact that my x's-- if I look at the distribution of\nbeta, which is gone, I think--",
    "start": "4383240",
    "end": "4392465"
  },
  {
    "text": "yeah, beta is gone. Oh, yeah, that's where it is. The covariance matrix depends\non this matrix x transpose x.",
    "start": "4392465",
    "end": "4400040"
  },
  {
    "text": "So this will show\nup in the variance. In particular, diagonal elements\nare going to play a role here. And so that's what\nmy gammas are.",
    "start": "4400040",
    "end": "4406850"
  },
  {
    "text": "The gammas is the j's diagonal\nelement of this matrix. So we'll resume\nthat on Tuesday, so",
    "start": "4406850",
    "end": "4415010"
  },
  {
    "text": "don't worry too much if\nthis is going too fast. I'm not supposed to\ncover it, but just so you",
    "start": "4415010",
    "end": "4420350"
  },
  {
    "text": "get a hint of why Cochran's\ntheorem actually was useful. So I don't know if we\nactually ended up recording.",
    "start": "4420350",
    "end": "4431690"
  },
  {
    "text": "I have your homework. And as usual, I will\ngive it to you outside. ",
    "start": "4431690",
    "end": "4438400"
  }
]