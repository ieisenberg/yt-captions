[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "text": " The following\ncontent is provided by MIT OpenCourseWare under\na Creative Commons license.",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "Additional information\nabout our license and MIT OpenCourseWare\nin general, is available at ocw.mit.edu.",
    "start": "6090",
    "end": "11930"
  },
  {
    "start": "11930",
    "end": "17100"
  },
  {
    "text": "PROFESSOR: Up on the web\nnow are lots of sections, including this\nconjugate gradients,",
    "start": "17100",
    "end": "22279"
  },
  {
    "text": "so today is the end of the\nconjugate gradients discussion,",
    "start": "22280",
    "end": "27960"
  },
  {
    "text": "and it's kind of a\ndifficult algorithm.",
    "start": "27960",
    "end": "33000"
  },
  {
    "text": "It's beautiful and\nslightly tricky, but if I just see what the\npoint is of conjugate gradients,",
    "start": "33000",
    "end": "42610"
  },
  {
    "text": "we don't have to check that the\nexact formulas that I've listed here -- so that's one step\nof conjugate gradients:",
    "start": "42610",
    "end": "52440"
  },
  {
    "text": "going from k minus 1 to k, and\nwe'll look at each of the five",
    "start": "52440",
    "end": "60480"
  },
  {
    "text": "steps in the cycle, without\npatiently checking all those",
    "start": "60480",
    "end": "66650"
  },
  {
    "text": "formulas. We'll just, sort of, see\nhow much work is involved. But let me begin where we were\nlast time, which was Arnoldi.",
    "start": "66650",
    "end": "77040"
  },
  {
    "text": "So Arnoldi was a\nGram-Schmidt-like algorithm, not exactly Gram-Schmidt, that\nproduced an orthogonal basis,",
    "start": "77040",
    "end": "86350"
  },
  {
    "text": "and that basis is the basis\nfor the Krylov subspace. So each new basis vector\nin the Krylov subspace",
    "start": "86350",
    "end": "98620"
  },
  {
    "text": "is found by multiplying\nby another power of A, but that's a terrible basis.",
    "start": "98620",
    "end": "104619"
  },
  {
    "text": "So the basis that we get from\nthese guys has to be improved,",
    "start": "104620",
    "end": "110920"
  },
  {
    "text": "orthogonalized, and\nthat's what Arnoldi does. And if we look to see\nwhat it does matrix-wise,",
    "start": "110920",
    "end": "118790"
  },
  {
    "text": "it turns out to be exactly\n-- have that nice equation, that we're creating a matrix\nQ with the orthogonal vectors,",
    "start": "118790",
    "end": "128490"
  },
  {
    "text": "and a matrix H that tells\nus how they were connected",
    "start": "128490",
    "end": "134320"
  },
  {
    "text": "to the matrix A. OK.",
    "start": "134320",
    "end": "139880"
  },
  {
    "text": "So that's the central equation,\nand the properties of H",
    "start": "139880",
    "end": "153270"
  },
  {
    "text": "are important. So we saw that the key property\nwas, in general, we've got --",
    "start": "153270",
    "end": "162900"
  },
  {
    "text": "it's full up in\nthat upper corner, just the way\nGram-Schmidt would be.",
    "start": "162900",
    "end": "169319"
  },
  {
    "text": "But in case A is a\nsymmetric matrix, then we can check that H will\nbe symmetric from this formula.",
    "start": "169320",
    "end": "178180"
  },
  {
    "text": "And the fact that\nit's symmetric means that those are zeros\nup there, because we",
    "start": "178180",
    "end": "183290"
  },
  {
    "text": "know there are zeros down here. We know there's\nonly one diagonal below the main diagonal.",
    "start": "183290",
    "end": "189570"
  },
  {
    "text": " And if it's symmetric,\nthen there's only one diagonal above,\nand that's the great case,",
    "start": "189570",
    "end": "199740"
  },
  {
    "text": "the symmetric case. So I'm going to take\ntwo seconds just to say,",
    "start": "199740",
    "end": "209120"
  },
  {
    "text": "here is the best way\nto find eigenvalues. We haven't spoken one word\nabout computing eigenvalues,",
    "start": "209120",
    "end": "217210"
  },
  {
    "text": "but this is the moment\nto say that word. If I have a symmetric matrix,\neigenvalues are way, way easier",
    "start": "217210",
    "end": "225709"
  },
  {
    "text": "for symmetric matrices. Linear systems are also\neasier, but eigenvalues",
    "start": "225710",
    "end": "231940"
  },
  {
    "text": "are an order of magnitude\neasier for symmetric matrices. And here is a really good\nway to find the eigenvalues",
    "start": "231940",
    "end": "240300"
  },
  {
    "text": "of large matrices. I assume that you don't\nwant all the eigenvalues.",
    "start": "240300",
    "end": "245440"
  },
  {
    "text": "I mean, that is very unusual\nto need many, many eigenvalues",
    "start": "245440",
    "end": "252160"
  },
  {
    "text": "of a big matrix. You're looking -- probably those\nhigh eigenvalues are not really",
    "start": "252160",
    "end": "260500"
  },
  {
    "text": "physically close, good\nrepresentations of the actual eigenvalues of the\ncontinuous problem.",
    "start": "260500",
    "end": "268330"
  },
  {
    "text": "So, we have continuous\nand discrete that are close for\nthe low eigenvalues,",
    "start": "268330",
    "end": "274650"
  },
  {
    "text": "because that's where\nthe error is smooth.",
    "start": "274650",
    "end": "280810"
  },
  {
    "text": "High eigenvalues\nare not so reliable. So for the low eigenvalues,\nhere is a good way to do them.",
    "start": "280810",
    "end": "291450"
  },
  {
    "text": "Do Arnoldi, and it gets named\nalso, now, after Lanczos,",
    "start": "291450",
    "end": "299540"
  },
  {
    "text": "because he had this\neigenvalue idea. Run that for a while.",
    "start": "299540",
    "end": "307560"
  },
  {
    "text": "Say if you want 10 eigenvalues,\nmaybe go up to j equal to 20.",
    "start": "307560",
    "end": "313430"
  },
  {
    "text": "So 20 Arnoldi steps,\nquite reasonable. And then look at the 20\nby 20 submatrix of H,",
    "start": "313430",
    "end": "322150"
  },
  {
    "text": "so you're not going to\ngo to the end, to H_n. You're going to\nstop at some point,",
    "start": "322150",
    "end": "328600"
  },
  {
    "text": "and you have this nice matrix. ",
    "start": "328600",
    "end": "334770"
  },
  {
    "text": "It'll be tri-diagonal,\nsymmetric tri-diagonal, and that's the kind of\nmatrix we know how to find",
    "start": "334770",
    "end": "341500"
  },
  {
    "text": "the eigenvalues of; so this\nis symmetric and tri-diagonal,",
    "start": "341500",
    "end": "346920"
  },
  {
    "text": "and eigenvalues of that class\nof matrices are very nice,",
    "start": "346920",
    "end": "356050"
  },
  {
    "text": "and they're very meaningful,\nand they're very close to -- in many cases; I won't try\nto give a theory here --",
    "start": "356050",
    "end": "364800"
  },
  {
    "text": "in many cases, you get\nvery good approximations. The eigenvalues\nof this submatrix",
    "start": "364800",
    "end": "370030"
  },
  {
    "text": "are sometimes\ncalled Ritz values, and they use the eigenvalues\nof this submatrix,",
    "start": "370030",
    "end": "378550"
  },
  {
    "text": "and those are often\ncalled Ritz values.",
    "start": "378550",
    "end": "384430"
  },
  {
    "text": "And so the first 10 eigenvalues\nof that 20 by 20 matrix, would be in many, many\nproblems very fast, very",
    "start": "384430",
    "end": "395990"
  },
  {
    "text": "efficient approximations\nto the low eigenvalues of the big matrix A\nthat you start with.",
    "start": "395990",
    "end": "404270"
  },
  {
    "text": "OK, the reason is\nthat eigenvalues -- do you all recognize\nthat eigenvalues --",
    "start": "404270",
    "end": "411380"
  },
  {
    "text": "that this is the operation, when\nI bring Q inverse over here, that I would call these\nmatrices similar --",
    "start": "411380",
    "end": "420890"
  },
  {
    "text": "two matrices are similar if\nthere's a Q that connects them",
    "start": "420890",
    "end": "428200"
  },
  {
    "text": "this way. And similar matrices\nhave the same lambdas,",
    "start": "428200",
    "end": "434860"
  },
  {
    "text": "the same eigenvalues.  So the full matrix Q, if we\nwent all the way to the end,",
    "start": "434860",
    "end": "442789"
  },
  {
    "text": "we could find its eigenvalues,\nbut the whole point of all this, of this\nmonth, is algorithms",
    "start": "442790",
    "end": "450760"
  },
  {
    "text": "that you can stop\npartway, and you still have something really good. OK, so that's -- I think\nwe can't do everything,",
    "start": "450760",
    "end": "462560"
  },
  {
    "text": "so I won't try to do more with\neigenvalues except to mention that. Someday you may want the\neigenvalues of a large matrix,",
    "start": "462560",
    "end": "470370"
  },
  {
    "text": "and I guess ARPACK would\nbe the Arnoldi pack, that",
    "start": "470370",
    "end": "480340"
  },
  {
    "text": "would be the package to go\nto for the Arnoldi iteration,",
    "start": "480340",
    "end": "486030"
  },
  {
    "text": "and then to use for\nthe eigenvalues. OK, so that's a side point,\njust since linear algebra is --",
    "start": "486030",
    "end": "497930"
  },
  {
    "text": "half of it's linear systems\nand half of it's eigenvalue problems, I didn't think I\ncould miss the chance to do that",
    "start": "497930",
    "end": "506490"
  },
  {
    "text": "second half, the\neigenvalue part. OK, now I'm going to\ntackle conjugate gradients,",
    "start": "506490",
    "end": "513750"
  },
  {
    "text": "to get the idea. OK, so the idea,\nwell, of course, A is symmetric positive definite.",
    "start": "513750",
    "end": "522899"
  },
  {
    "text": "If it's not, you could\ntry these formulas, but you'd be taking\na pretty big chance.",
    "start": "522900",
    "end": "532780"
  },
  {
    "text": "If it is, these are just right. OK, so what's the\nmain point about --",
    "start": "532780",
    "end": "539850"
  },
  {
    "text": "here's the rule that\ndecides what x_k will be.",
    "start": "539850",
    "end": "548300"
  },
  {
    "text": "x_k will be the vector in\nthe Krylov -- of course,",
    "start": "548300",
    "end": "555490"
  },
  {
    "text": "x_k is in this Krylov space K_k.",
    "start": "555490",
    "end": "562440"
  },
  {
    "text": " So it'll be -- we can\ncreate x_k recursively,",
    "start": "562440",
    "end": "572490"
  },
  {
    "text": "and we should need,\njust, if we do it right, should need just one\nmultiplication by A at every",
    "start": "572490",
    "end": "578709"
  },
  {
    "text": "step, and then\nsome inner product. OK, so now, what do\nwe know from this?",
    "start": "578710",
    "end": "587139"
  },
  {
    "text": "This vector, since it has\nthis, it starts with --",
    "start": "587140",
    "end": "593090"
  },
  {
    "text": "it has an x_k that's in\nthe k-th Krylov subspace, but now it's multiplied by A, so\nthat's up to the k plus first.",
    "start": "593090",
    "end": "601950"
  },
  {
    "text": "And b is in K_1, it's in\nall the Krylov subspaces,",
    "start": "601950",
    "end": "608520"
  },
  {
    "text": "so this is in -- and therefore\nr_k is in -- K_(k+1), right?",
    "start": "608520",
    "end": "618980"
  },
  {
    "text": "This is in the next space\nup, because there's a multiplication by\nA. And therefore,",
    "start": "618980",
    "end": "625660"
  },
  {
    "text": "if we choose to determine\nit by this rule, we learn that this\nvector -- I mean,",
    "start": "625660",
    "end": "636120"
  },
  {
    "text": "we already know from Arnoldi\nthat q_(k+1) is orthogonal",
    "start": "636120",
    "end": "644339"
  },
  {
    "text": "to this space. This is the new k plus first\nvector that's in K_(k+1),",
    "start": "644340",
    "end": "653680"
  },
  {
    "text": "so this is just where\nthis is to be found. So this vector r_k is found\nin the next Krylov subspace;",
    "start": "653680",
    "end": "662160"
  },
  {
    "text": "this one is in the\nnext Krylov subspace. They're both orthogonal to all\nthe previous Krylov subspaces,",
    "start": "662160",
    "end": "669060"
  },
  {
    "text": "so one's a multiple\nof the other. ",
    "start": "669060",
    "end": "674180"
  },
  {
    "text": "So, that will mean\nthat automatically, that the r's have the\nsame property that Arnoldi",
    "start": "674180",
    "end": "682430"
  },
  {
    "text": "created for the q's:\nthey're orthogonal. So the residuals are orthogonal.",
    "start": "682430",
    "end": "689920"
  },
  {
    "text": "So, orthogonal residuals. Orthogonal residuals, good.",
    "start": "689920",
    "end": "699850"
  },
  {
    "text": " I better say, by the way, when\nwe do conjugate gradients,",
    "start": "699850",
    "end": "708050"
  },
  {
    "text": "we don't also do Arnoldi, so\nArnoldi's finding these q's; we're leaving him\nbehind for a while,",
    "start": "708050",
    "end": "715740"
  },
  {
    "text": "because we're going\nto go directly to the x's and r's here. So, I won't know the q's but\nI know a lot about the q,",
    "start": "715740",
    "end": "725420"
  },
  {
    "text": "so it's natural to write this\nsimple fact: that each residual",
    "start": "725420",
    "end": "730720"
  },
  {
    "text": "is a multiple of the new\nq, and therefore, it's orthogonal to all the old ones.",
    "start": "730720",
    "end": "736770"
  },
  {
    "text": "And now I want to -- this is the\nother property that determines",
    "start": "736770",
    "end": "743410"
  },
  {
    "text": "the new x. So the new x should be\n-- what do I see here,",
    "start": "743410",
    "end": "749670"
  },
  {
    "text": "I see a sort of delta x, the\nchange in x, the correction --",
    "start": "749670",
    "end": "756620"
  },
  {
    "text": "which is what I want to\ncompute -- at step k, that's what I have to find.",
    "start": "756620",
    "end": "762750"
  },
  {
    "text": "And I could look at the\ncorrections at the old steps, and notice there's\nan A in the middle.",
    "start": "762750",
    "end": "770010"
  },
  {
    "text": "So the corrections\nto the x's are orthogonal in the\nA inner products,",
    "start": "770010",
    "end": "778410"
  },
  {
    "text": "and that's where the name\nconjugate comes from. So this is conjugate directions.",
    "start": "778410",
    "end": "791190"
  },
  {
    "text": "Why do I say directions,\nand why do I say conjugate? Conjugate gradients,\neven, I could say.",
    "start": "791190",
    "end": "797650"
  },
  {
    "text": "Why did I --\nconjugate directions, I maybe shouldn't\nhave erased that, but I'll also say\nconjugate gradients.",
    "start": "797650",
    "end": "808390"
  },
  {
    "text": "OK. So conjugate, all I\nmean by conjugate,",
    "start": "808390",
    "end": "814920"
  },
  {
    "text": "is orthogonal in the natural\ninner product that goes with",
    "start": "814920",
    "end": "822060"
  },
  {
    "text": "the problem, and\nthat's given by A. So the inner product given\nby the problem is the inner",
    "start": "822060",
    "end": "827900"
  },
  {
    "text": "product of x and y,\nis x transposed A*y.",
    "start": "827900",
    "end": "834070"
  },
  {
    "text": "And that's a good inner product,\nand it's the one that comes",
    "start": "834070",
    "end": "840080"
  },
  {
    "text": "with the problem and\nso, natural to look at orthogonality in that\nsense, and that's what we have.",
    "start": "840080",
    "end": "849570"
  },
  {
    "text": "So these are orthogonal\nin the usual sense, because they have an A\nalready built into them,",
    "start": "849570",
    "end": "855580"
  },
  {
    "text": "and these are orthogonal\nin the A inner product. OK.",
    "start": "855580",
    "end": "861509"
  },
  {
    "text": "So I wrote down two\nrequirements on the new x.",
    "start": "861510",
    "end": "866720"
  },
  {
    "text": "Well, actually you might\nsay, I wrote down a whole lot of requirements, because I'm\nsaying that this should hold",
    "start": "866720",
    "end": "872010"
  },
  {
    "text": "for all the previous i's. And this should hold for\nall the previous i's.",
    "start": "872010",
    "end": "877420"
  },
  {
    "text": "And you see indices\nare getting in here, and I'm asking\nfor your patience.",
    "start": "877420",
    "end": "885310"
  },
  {
    "start": "885000",
    "end": "1540000"
  },
  {
    "text": "What's the point that\nI want to make here? It's this short recurrence\npoint, this point that we had,",
    "start": "885310",
    "end": "892470"
  },
  {
    "text": "over here, when h was\njust tri-diagonal, so we only needed\n-- at the new step,",
    "start": "892470",
    "end": "899220"
  },
  {
    "text": "the only things we had\nto find were the --",
    "start": "899220",
    "end": "906079"
  },
  {
    "text": "say at the second step, we just\nhad to find that h and that h, because this one was\nthe same as that.",
    "start": "906080",
    "end": "914200"
  },
  {
    "text": "And then the next one, we --\nmaybe I didn't say that right.",
    "start": "914200",
    "end": "921410"
  },
  {
    "text": "I guess when we --\nlet me start again. At the first step I need\nto find two numbers.",
    "start": "921410",
    "end": "929540"
  },
  {
    "text": "Now because H is\nsymmetric, that's already the same as that. So at the next step I\nneed these two numbers,",
    "start": "929540",
    "end": "935560"
  },
  {
    "text": "and then that number\nis already that. So, two numbers at every\nstep and big zeros up here.",
    "start": "935560",
    "end": "942560"
  },
  {
    "text": "That's that's the key\npoint when A is symmetric. So the same will\nbe true over here.",
    "start": "942560",
    "end": "949480"
  },
  {
    "text": "We have two numbers, and\nthey're called alpha and beta,",
    "start": "949480",
    "end": "958190"
  },
  {
    "text": "so they're a little\ndifferent numbers, because we're working with the\nx's now and not with the q's,",
    "start": "958190",
    "end": "964020"
  },
  {
    "text": "so you don't see any q's written\ndown for conjugate gradients.",
    "start": "964020",
    "end": "969330"
  },
  {
    "text": "OK. All right, let me just look\nat the cycle of five steps.",
    "start": "969330",
    "end": "975940"
  },
  {
    "text": " So I have to give it a start,\nso I start with d_0 as b and x_0",
    "start": "975940",
    "end": "988170"
  },
  {
    "text": "as 0, and I guess, r_0 which,\nof course, is b minus A*x_0.",
    "start": "988170",
    "end": "996940"
  },
  {
    "text": "If that's 0, it's also b. So those are the starts. ",
    "start": "996940",
    "end": "1004630"
  },
  {
    "text": "So I'm ready to take a step. I know the ones\nwith subscript zero,",
    "start": "1004630",
    "end": "1011570"
  },
  {
    "text": "and I'm ready for the next step. I know the ones with\nsubscript k minus 1, and I'm ready for this\nstep, this cycle, k.",
    "start": "1011570",
    "end": "1021050"
  },
  {
    "text": " So, here's a number.",
    "start": "1021050",
    "end": "1026370"
  },
  {
    "text": " So I'm coming into this -- well\nlet me say maybe how I should",
    "start": "1026370",
    "end": "1032900"
  },
  {
    "text": "say it. I'm coming into that cycle\nwith a new d, now what is a d?",
    "start": "1032900",
    "end": "1038459"
  },
  {
    "text": "It's a search direction. It's the way I'm going to go. It's the direction in which\nthis delta x is going to go.",
    "start": "1038460",
    "end": "1046259"
  },
  {
    "text": "This delta x is going to be,\nprobably I'll see it here. Yeah, there is delta x,\nis a multiple of d, right?",
    "start": "1046260",
    "end": "1054950"
  },
  {
    "text": "If I put this on this\nside, that's delta x, the change in x is\na multiple of d,",
    "start": "1054950",
    "end": "1062520"
  },
  {
    "text": "so d is the direction\nin n-dimensional space that I'm looking\nfor the correction,",
    "start": "1062520",
    "end": "1069840"
  },
  {
    "text": "and alpha is the\nnumber, is how far I go along d for the best,\nfor the right, delta x.",
    "start": "1069840",
    "end": "1078649"
  },
  {
    "text": "So alpha will be determined\nby, probably by that.",
    "start": "1078650",
    "end": "1086190"
  },
  {
    "text": "Probably by that, yeah, it\ninvolves an inner product,",
    "start": "1086190",
    "end": "1091740"
  },
  {
    "text": "picking off a component. OK? So the first step is\nto find the number;",
    "start": "1091740",
    "end": "1098000"
  },
  {
    "text": "the next step is to take\nthat update, delta x is the right multiple of\nthe search direction",
    "start": "1098000",
    "end": "1104740"
  },
  {
    "text": "that I'm coming in with. Then correcting r is easy;\nI'll come back to that;",
    "start": "1104740",
    "end": "1110020"
  },
  {
    "text": "if I know the\nchange in x, I know the change in r right away.",
    "start": "1110020",
    "end": "1115390"
  },
  {
    "text": "So that's given me,\nI'm now updated. But now I have to look ahead.",
    "start": "1115390",
    "end": "1123039"
  },
  {
    "text": "What direction am I\ngoing to travel next? So the next steps are to find a\nnumber beta and then the search",
    "start": "1123040",
    "end": "1134390"
  },
  {
    "text": "direction, which isn't r; it's r\nagain with just one correction.",
    "start": "1134390",
    "end": "1141620"
  },
  {
    "text": "So it's a beautiful set\nof formulas, beautiful. And I could write -- you know,\nbecause I know x's and r's, I",
    "start": "1141620",
    "end": "1149610"
  },
  {
    "text": "could write the formulas --\nthere are different expressions",
    "start": "1149610",
    "end": "1156250"
  },
  {
    "text": "for the same alpha and beta,\nbut this is a very satisfactory one.",
    "start": "1156250",
    "end": "1161550"
  },
  {
    "text": "OK. Have a look at that\ncycle just to see what",
    "start": "1161550",
    "end": "1166820"
  },
  {
    "text": "how much work is involved. How much work is\ninvolved in that cycle? OK, well, there's\na multiplication",
    "start": "1166820",
    "end": "1175490"
  },
  {
    "text": "by A, no surprise. It's that\nmultiplication by A that takes us to the\nnext Krylov space,",
    "start": "1175490",
    "end": "1183480"
  },
  {
    "text": "takes us to the next update. So I see a multiplication by\nA, and also here, but it's",
    "start": "1183480",
    "end": "1190030"
  },
  {
    "text": "the same multiplication. So I see one multiplication\nby A. So, easy to list.",
    "start": "1190030",
    "end": "1199090"
  },
  {
    "text": "So each cycle, there's\none multiplication A",
    "start": "1199090",
    "end": "1205500"
  },
  {
    "text": "times the search direction, and\nbecause A is a sparse matrix,",
    "start": "1205500",
    "end": "1212460"
  },
  {
    "text": "the cost there is the number\nof non-zeros in the matrix.",
    "start": "1212460",
    "end": "1218450"
  },
  {
    "text": "OK. Then, what other computations\ndo you have to do?",
    "start": "1218450",
    "end": "1224520"
  },
  {
    "text": "I see an inner product there,\nand I'm going to use it again.",
    "start": "1224520",
    "end": "1232020"
  },
  {
    "text": "I guess, and here it\nis at the next step, so I guess per cycle, I have\nto do one inner product of r",
    "start": "1232020",
    "end": "1242040"
  },
  {
    "text": "with itself, and I have to do\none of these inner products. So I see two inner products,\nand of course, these",
    "start": "1242040",
    "end": "1253270"
  },
  {
    "text": "are vectors of length n. so that's 2n multiplications,\n2n additions.",
    "start": "1253270",
    "end": "1262549"
  },
  {
    "text": "OK, this was a multiple of\nn, depending how sparse A is,",
    "start": "1262550",
    "end": "1268930"
  },
  {
    "text": "this is 2n adds and 2n\nmultiplies for these two",
    "start": "1268930",
    "end": "1274326"
  },
  {
    "text": "inner products. OK. Yeah. I guess you could\nsay take them there.",
    "start": "1274326",
    "end": "1283450"
  },
  {
    "text": "And then what else\ndo we have to do? Oh, we have these\nupdates, so I have to multiply a vector\nby that scalar and add.",
    "start": "1283450",
    "end": "1292230"
  },
  {
    "text": "And again here, I multiply\nthat vector by that scalar and subtract.",
    "start": "1292230",
    "end": "1297610"
  },
  {
    "text": "Oh, and do I have\nto do it again here? Have I got three? Somehow, I thought I\nmight only have two.",
    "start": "1297610",
    "end": "1304560"
  },
  {
    "text": "Oh, yeah, let me say two\nor three vector updates.",
    "start": "1304560",
    "end": "1311670"
  },
  {
    "start": "1311670",
    "end": "1318410"
  },
  {
    "text": "People are much better than I\nam at spotting the right way",
    "start": "1318410",
    "end": "1327120"
  },
  {
    "text": "to organize those calculations. But that's really a very\nsmall price to pay per cycle,",
    "start": "1327120",
    "end": "1337470"
  },
  {
    "text": "and so if it converges\nquickly, as it normally does,",
    "start": "1337470",
    "end": "1344980"
  },
  {
    "text": "this is going to be\na good algorithm. Maybe I'll -- maybe having\nintroduced the question of how",
    "start": "1344980",
    "end": "1350020"
  },
  {
    "text": "quickly does it converge, I\nshould maybe say something about that. So what's the error,\nafter k steps?",
    "start": "1350020",
    "end": "1360519"
  },
  {
    "text": "How is it related to\nthe error at the start? ",
    "start": "1360520",
    "end": "1366830"
  },
  {
    "text": "So, I have to first say, what\ndo I mean by this measure of error, and then I\nhave to say what's the --",
    "start": "1366830",
    "end": "1377150"
  },
  {
    "text": "I hope I see a factor, less\nthan 1, to the k power. Yeah, so I'm hoping some factor\nto the k power will be there,",
    "start": "1377150",
    "end": "1385780"
  },
  {
    "text": "and I sure hope\nit's less than 1. Let me say what it is. I think -- so this is maybe\nnot the very best estimate,",
    "start": "1385780",
    "end": "1395080"
  },
  {
    "text": "but it shows the main point. It's the square\nroot of lambda_max",
    "start": "1395080",
    "end": "1401059"
  },
  {
    "text": "minus the square\nroot of lambda_min, divided by the\nsquare root of that",
    "start": "1401060",
    "end": "1406980"
  },
  {
    "text": "plus the square root of that. And maybe there's a\nfactor 2 somewhere.",
    "start": "1406980",
    "end": "1414260"
  },
  {
    "text": "So this is one estimate\nfor the convergence factor, and you see what it depends\non, I could divide everything",
    "start": "1414260",
    "end": "1424060"
  },
  {
    "text": "by lambda_min there, so that I\ncould write that as the square root of the condition number --\nI'll divide by that -- minus 1,",
    "start": "1424060",
    "end": "1434120"
  },
  {
    "text": "divided by the square root of\nthe condition number plus 1, to the k power.",
    "start": "1434120",
    "end": "1439150"
  },
  {
    "text": " So the condition number has a\npart in this error estimate,",
    "start": "1439150",
    "end": "1448659"
  },
  {
    "text": "and I won't derive that,\neven in the notes, I won't.",
    "start": "1448660",
    "end": "1454620"
  },
  {
    "text": "And other people work out\nother estimates for the error, it's an interesting problem.",
    "start": "1454620",
    "end": "1460590"
  },
  {
    "text": "And I do have to say\nthat this norm is the natural inner\nproduct, e_k squared",
    "start": "1460590",
    "end": "1473380"
  },
  {
    "text": "is the inner product\nof e_k with itself, but again with A in the middle.",
    "start": "1473380",
    "end": "1479970"
  },
  {
    "text": "I suppose that a little\npart of the message here, and on that middle board, is\nthat it's just natural to see",
    "start": "1479970",
    "end": "1491250"
  },
  {
    "text": "the matrix A playing\na part in the -- it just comes in naturally.",
    "start": "1491250",
    "end": "1497370"
  },
  {
    "text": "Conjugate gradients\nbrings it in naturally. These formulas are just\ncreated so that this will work.",
    "start": "1497370",
    "end": "1507650"
  },
  {
    "text": "OK. So I suppose I'm\nthinking there's",
    "start": "1507650",
    "end": "1513150"
  },
  {
    "text": "one word I haven't\nreally justified, and that's the word gradient.",
    "start": "1513150",
    "end": "1518690"
  },
  {
    "text": "Why do we call it\nconjugate gradient? Here I erased --\n\"conjugate directions\"",
    "start": "1518690",
    "end": "1525480"
  },
  {
    "text": "I was quite happy\nwith, the directions or the d's, because this is\nthe direction in which we moved",
    "start": "1525480",
    "end": "1538450"
  },
  {
    "text": "from x, so conjugate\ndirections would be just fine, and those words come into this\npart of numerical mathematics,",
    "start": "1538450",
    "end": "1549830"
  },
  {
    "start": "1540000",
    "end": "1985000"
  },
  {
    "text": "but where do gradients come in? OK. So, let me say,\ngradient of what?",
    "start": "1549830",
    "end": "1554880"
  },
  {
    "text": " Well, what is --\nso first of all,",
    "start": "1554880",
    "end": "1562570"
  },
  {
    "text": "gradient of what is in these\nlinear problems where A*x equal",
    "start": "1562570",
    "end": "1575580"
  },
  {
    "text": "b? Those come from the\ngradient of the energy. So can I say E of\nx for the energy?",
    "start": "1575580",
    "end": "1582470"
  },
  {
    "text": "As 1/2 x transpose A*x\nminus b transpose x?",
    "start": "1582470",
    "end": "1587940"
  },
  {
    "start": "1587940",
    "end": "1595029"
  },
  {
    "text": "You might say, where\ndid that come from? ",
    "start": "1595030",
    "end": "1601240"
  },
  {
    "text": "Normally, we've been talking\nabout linear systems, everything's been in terms of\nA*x equal b, and now suddenly,",
    "start": "1601240",
    "end": "1607530"
  },
  {
    "text": "I'm changing to a different\npart of mathematics. I'm looking at minimizing\n-- so optimization,",
    "start": "1607530",
    "end": "1616520"
  },
  {
    "text": "the part that's coming in\nApril, is minimizing energy,",
    "start": "1616520",
    "end": "1622440"
  },
  {
    "text": "minimizing function,\nand what's the link? Well, if I minimize that,\nthe gradient of this is --",
    "start": "1622440",
    "end": "1633120"
  },
  {
    "text": "how shall I write? grad\nE, the gradient of E,",
    "start": "1633120",
    "end": "1638559"
  },
  {
    "text": "the list of the derivatives\nof E with respect to the x_i,",
    "start": "1638560",
    "end": "1648030"
  },
  {
    "text": "and if we work that out -- well,\nwhy not pretend it's scalar",
    "start": "1648030",
    "end": "1653650"
  },
  {
    "text": "for the moment, so\nthese are just numbers. Then this ordinary derivative\nof 1/2 A x squared is A*x,",
    "start": "1653650",
    "end": "1663250"
  },
  {
    "text": "and the derivative of b*x is\nb, and that rule continues",
    "start": "1663250",
    "end": "1671950"
  },
  {
    "text": "into the vector case. So what have we got here?",
    "start": "1671950",
    "end": "1677910"
  },
  {
    "text": "We've computed, we've\nintroduced the natural energy, so this is a natural\nenergy for the problem,",
    "start": "1677910",
    "end": "1685649"
  },
  {
    "text": "and the reason it's natural\nis when we minimize it,",
    "start": "1685650",
    "end": "1692470"
  },
  {
    "text": "set the derivatives to zero, we\ngot the equation A*x equal b,",
    "start": "1692470",
    "end": "1699240"
  },
  {
    "text": "so that would be\nzero at a minimum. This would equal zero at a min,\nso what I'm doing right now",
    "start": "1699240",
    "end": "1707190"
  },
  {
    "text": "is pretty serious. On the other hand, I'll repeat\nit again in April when we do",
    "start": "1707190",
    "end": "1715860"
  },
  {
    "text": "minimizations,\nbut this is the -- this quadratic energy\nhas a linear gradient,",
    "start": "1715860",
    "end": "1725160"
  },
  {
    "text": "and its minimum is\nexactly A*x equal b. The minimum point\nis A*x equal b.",
    "start": "1725160",
    "end": "1732340"
  },
  {
    "text": "In other words, solving\nthe linear equation and minimizing the energy\nare one and the same, one",
    "start": "1732340",
    "end": "1739259"
  },
  {
    "text": "and the same. And I'm allowed to\nuse the word min,",
    "start": "1739260",
    "end": "1744940"
  },
  {
    "text": "because A is positive definite,\nso the positive definiteness",
    "start": "1744940",
    "end": "1751419"
  },
  {
    "text": "of A is what means that\nthis is really a minimum and not a maximum\nor a saddle point.",
    "start": "1751420",
    "end": "1758790"
  },
  {
    "text": "OK. So I was just -- I wanted\njust to think about how do you",
    "start": "1758790",
    "end": "1765280"
  },
  {
    "text": "minimize a function?  I guess any sensible\nperson, given a function",
    "start": "1765280",
    "end": "1774230"
  },
  {
    "text": "and looking for the minimum,\nwould figure out the gradient",
    "start": "1774230",
    "end": "1779309"
  },
  {
    "text": "and move that way, move,\nwell, not up the gradient, move down the gradient, so\nI imagine this minimization",
    "start": "1779310",
    "end": "1787090"
  },
  {
    "text": "problem, I imagine here I'm\nin x, this is the x_1, x_2,",
    "start": "1787090",
    "end": "1794169"
  },
  {
    "text": "the x's; here's the E, E of\nx is something like that,",
    "start": "1794170",
    "end": "1800920"
  },
  {
    "text": "maybe its minimum is there. This is the graph of E of x, and\nI'm trying to find that point,",
    "start": "1800920",
    "end": "1809410"
  },
  {
    "text": "and I make as first\nguess, somewhere there.",
    "start": "1809410",
    "end": "1814600"
  },
  {
    "text": "Somewhere on that surface. Maybe, so to help your\neye, this is like a bowl,",
    "start": "1814600",
    "end": "1825580"
  },
  {
    "text": "and maybe there's\na point, so that's on the surface of the bowl.",
    "start": "1825580",
    "end": "1831480"
  },
  {
    "text": "That's x_0, let's say. I'm trying to get to the bottom\nof the bowl, and as I said,",
    "start": "1831480",
    "end": "1841640"
  },
  {
    "text": "any sensible person\nwould figure out, well, what's the steepest way down. The steepest way down is the\ndirection of the gradient.",
    "start": "1841640",
    "end": "1848960"
  },
  {
    "text": " So I could call this gradient\ng, if I wanted, and then I",
    "start": "1848960",
    "end": "1859470"
  },
  {
    "text": "would go in the\ndirection of minus g, because I want to\ngo down and not up. I mean, we're climbing,\nI shouldn't say climbing,",
    "start": "1859470",
    "end": "1867310"
  },
  {
    "text": "we're descending a mountain,\nand trying to get to the --",
    "start": "1867310",
    "end": "1874150"
  },
  {
    "text": "or descending a valley, sorry. We're descending into\na valley and trying to get to the bottom.",
    "start": "1874150",
    "end": "1880400"
  },
  {
    "text": "And I'm assuming\nthat this valley is this nice shape given by\njust a second-degree polynomial.",
    "start": "1880400",
    "end": "1893310"
  },
  {
    "text": "Of course, when we\nget to optimization, this will be the simplest\nproblem, but not the only one.",
    "start": "1893310",
    "end": "1900669"
  },
  {
    "text": "Here it's our problem. Now, of course -- so\nwhat happens if I move",
    "start": "1900670",
    "end": "1907400"
  },
  {
    "text": "in the direction of -- the\ndirection water would flow.",
    "start": "1907400",
    "end": "1914570"
  },
  {
    "text": "If I tip a flask of\nwater on the ground, it's going to flow in\nthe steepest direction,",
    "start": "1914570",
    "end": "1921730"
  },
  {
    "text": "and that would be the\nnatural direction to go. ",
    "start": "1921730",
    "end": "1929490"
  },
  {
    "text": "But it's not the best direction. Maybe the first step is, but\n-- so I'm talking now about",
    "start": "1929490",
    "end": "1938960"
  },
  {
    "text": "conjugate gradient seen\nas a minimization problem, because that's where the\nword gradient is justified.",
    "start": "1938960",
    "end": "1946779"
  },
  {
    "text": "This is the gradient,\nand of course, this is just minus the\nresidual, so this is",
    "start": "1946780",
    "end": "1955890"
  },
  {
    "text": "the direction of the residual. So that's the\nquestion, should I just go in the direction of\nthe residual all the time?",
    "start": "1955890",
    "end": "1962980"
  },
  {
    "text": "At every step -- and this is\nwhat the method of steepest descent will do, so let\nme make the contrast.",
    "start": "1962980",
    "end": "1970900"
  },
  {
    "text": "Steepest descent\nis the first thing you would think\nof, direction is r.",
    "start": "1970900",
    "end": "1981300"
  },
  {
    "text": " That's the gradient direction,\nor the negative gradient",
    "start": "1981300",
    "end": "1988820"
  },
  {
    "start": "1985000",
    "end": "2015000"
  },
  {
    "text": "direction. Follow r until, in that\ndirection, you've hit bottom.",
    "start": "1988820",
    "end": "1996130"
  },
  {
    "text": "So you'll go down this,\nyou see what happens, you'll be going down one\nside, and you'll hit bottom,",
    "start": "1996130",
    "end": "2002920"
  },
  {
    "text": "and then you will come up\nagain on a plane that's",
    "start": "2002920",
    "end": "2008040"
  },
  {
    "text": "cutting through the surface. But you're not going to\nhit that, first shot.",
    "start": "2008040",
    "end": "2014000"
  },
  {
    "text": "When you start here,\nsee water would -- the actual steepest direction,\nthe gradient direction,",
    "start": "2014000",
    "end": "2020710"
  },
  {
    "start": "2015000",
    "end": "2210000"
  },
  {
    "text": "changes as the water\nflows down to the bottom, but here we've chosen\na fixed direction;",
    "start": "2020710",
    "end": "2028610"
  },
  {
    "text": "we're following it as\nlong as we're going down, but then it'll start up again.",
    "start": "2028610",
    "end": "2034060"
  },
  {
    "text": "So we stop there and find\nthe gradient again there.",
    "start": "2034060",
    "end": "2042920"
  },
  {
    "text": "Follow that down,\nuntil it goes up again; follow that down,\nuntil it goes up again, and the trouble is\nconvergence isn't great.",
    "start": "2042920",
    "end": "2054260"
  },
  {
    "text": "The trouble is that\nwe end up -- even if we're in\n100-dimensional space,",
    "start": "2054260",
    "end": "2059399"
  },
  {
    "text": "we end up sort of just -- our\nrepeated gradients are not",
    "start": "2059400",
    "end": "2069079"
  },
  {
    "text": "really in good, new directions. We find ourselves doing a lot of\nwork to make a little progress,",
    "start": "2069080",
    "end": "2078110"
  },
  {
    "text": "and the reason is that these\nr's don't have the property",
    "start": "2078110",
    "end": "2086669"
  },
  {
    "text": "that you're always looking for\nin computations, orthogonality.",
    "start": "2086670",
    "end": "2092299"
  },
  {
    "text": "Somehow orthogonality tells\nyou that you're really getting something new, if it's\northogonal to all the old ones.",
    "start": "2092300",
    "end": "2101450"
  },
  {
    "text": "OK. Now, so this, the\ndirection r is not great.",
    "start": "2101450",
    "end": "2109570"
  },
  {
    "start": "2109570",
    "end": "2114870"
  },
  {
    "text": "The better one is to\nhave some orthogonality. Take a direction d that\n-- well, here's the point.",
    "start": "2114870",
    "end": "2131410"
  },
  {
    "text": " This is the right direction;\nthis is the direction",
    "start": "2131410",
    "end": "2136810"
  },
  {
    "text": "conjugate gradients choose. This is where --\nso it's a gradient, but then it removes the\ncomponent in the direction we",
    "start": "2136810",
    "end": "2147340"
  },
  {
    "text": "just took. That's what that beta will be. When we compute that beta,\nand I'm a little surprised",
    "start": "2147340",
    "end": "2158170"
  },
  {
    "text": "that it isn't a minus sign. No, it's OK.",
    "start": "2158170",
    "end": "2164360"
  },
  {
    "text": "It turns out that\nway, let me just be sure that I wrote\nthe formula down right,",
    "start": "2164360",
    "end": "2170860"
  },
  {
    "text": "at least wrote it down\nas I have it here, yep.",
    "start": "2170860",
    "end": "2176600"
  },
  {
    "text": "Again, I'm not\nchecking the formulas, but just describing the ideas.",
    "start": "2176600",
    "end": "2182910"
  },
  {
    "text": "So the idea is that\nthis formula gives us a new direction, which has this\nproperty two that we're after.",
    "start": "2182910",
    "end": "2193400"
  },
  {
    "text": "That the direction, it's\nnot actually orthogonal, it's A-orthogonal to\nthe previous direction.",
    "start": "2193400",
    "end": "2200520"
  },
  {
    "text": "OK, I'm going to draw one\npicture of what -- whoops, not on that -- of\nthese directions,",
    "start": "2200520",
    "end": "2207740"
  },
  {
    "text": "and then that's my best I can\ndo with conjugate gradients.",
    "start": "2207740",
    "end": "2214170"
  },
  {
    "start": "2210000",
    "end": "2435000"
  },
  {
    "text": "I want to try to\ndraw this search",
    "start": "2214170",
    "end": "2220940"
  },
  {
    "text": "for the bottom of the valley. So I'm going to draw\nthat search by drawing",
    "start": "2220940",
    "end": "2228000"
  },
  {
    "text": "the level sets of the function. I'm going to -- yeah, somehow\nthe contours of my function",
    "start": "2228000",
    "end": "2237450"
  },
  {
    "text": "are, since my function here is\nquadratic, all the contours,",
    "start": "2237450",
    "end": "2246320"
  },
  {
    "text": "the level sets will be ellipses. They'll all be, sort of,\nlike similar ellipses, and there is the one I want.",
    "start": "2246320",
    "end": "2253360"
  },
  {
    "text": "That's the bottom of the valley. So this is the contour\nwhere energy three, this is the contour energy two,\nthis is the contour energy one,",
    "start": "2253360",
    "end": "2262119"
  },
  {
    "text": "and there's the point where\nthe energy is a minimum. Well, it might be\nnegative, whatever,",
    "start": "2262120",
    "end": "2270279"
  },
  {
    "text": "but it's the smallest. OK. So what does -- can I compare\nsteepest descent with conjugate",
    "start": "2270280",
    "end": "2279850"
  },
  {
    "text": "gradient? Let me draw one more contour,\njust so I have enough to make this point.",
    "start": "2279850",
    "end": "2286880"
  },
  {
    "text": "OK, so steepest descent starts\nsomewhere, OK, starts there,",
    "start": "2286880",
    "end": "2294000"
  },
  {
    "text": "let's say. Suppose that's my\nstarting point. Steepest descent goes,\nif I draw it right,",
    "start": "2294000",
    "end": "2302230"
  },
  {
    "text": "it'll go perpendicular, right? It goes perpendicular\nto the level contour.",
    "start": "2302230",
    "end": "2310349"
  },
  {
    "text": "You carry along until, as long\nas the contours are going down, and then if I continue here,\nI'm climbing back up the valley.",
    "start": "2310350",
    "end": "2320510"
  },
  {
    "text": "So I'll stop here. Let's suppose that that happened\nto be tangent right there, OK.",
    "start": "2320510",
    "end": "2327200"
  },
  {
    "text": "Now, that was the first\nstep of steepest descent.",
    "start": "2327200",
    "end": "2332800"
  },
  {
    "text": "Now I'm at this point,\nI look at this contour, and well, if I've drawn\nit reasonably well,",
    "start": "2332800",
    "end": "2340130"
  },
  {
    "text": "I'm following maybe\nperpendicular, and now again I'm perpendicular,\nsteepest descent says",
    "start": "2340130",
    "end": "2346599"
  },
  {
    "text": "go perpendicular to the contour,\nuntil you've got to the bottom",
    "start": "2346600",
    "end": "2351650"
  },
  {
    "text": "there. Then down, then up, then do\nyou see this frustrating crawl",
    "start": "2351650",
    "end": "2363109"
  },
  {
    "text": "across the valley. It wouldn't happen if these\ncontours were circles.",
    "start": "2363110",
    "end": "2369380"
  },
  {
    "text": "If these contours -- if A\nwas the identity matrix, if A was the identity matrix,\nthese would be perfect circles,",
    "start": "2369380",
    "end": "2377000"
  },
  {
    "text": "and you'd go to the\ncenter right away. Well that only says that A*x\nequal b is easy to solve when A",
    "start": "2377000",
    "end": "2384800"
  },
  {
    "text": "is the identity matrix. So this is steepest descent,\nand its rate of convergence",
    "start": "2384800",
    "end": "2395070"
  },
  {
    "text": "is slow, because the longer\nand thinner the valley,",
    "start": "2395070",
    "end": "2400800"
  },
  {
    "text": "the worse it is here, OK.",
    "start": "2400800",
    "end": "2406260"
  },
  {
    "text": "What about conjugate gradients? Well, I'm just going\nto draw magic --",
    "start": "2406260",
    "end": "2418800"
  },
  {
    "text": "I'm going to make\nit look like magic. I'm going to start at the\nsame place, the first step,",
    "start": "2418800",
    "end": "2425640"
  },
  {
    "text": "because I'm starting with\nzero, the best thing I can do is follow the\ngradients to there,",
    "start": "2425640",
    "end": "2433180"
  },
  {
    "text": "and now, so that was the\nfirst step, and now what?",
    "start": "2433180",
    "end": "2439809"
  },
  {
    "start": "2435000",
    "end": "2690000"
  },
  {
    "text": "Well, the next direction\n-- so these are the search",
    "start": "2439810",
    "end": "2446230"
  },
  {
    "text": "directions. That's the search direction\nd_1, d_2, d_3, d_4.",
    "start": "2446230",
    "end": "2451790"
  },
  {
    "text": "Here is d_1, and what\ndoes d_2 look like? May I cheat and go\nstraight to the center?",
    "start": "2451790",
    "end": "2459580"
  },
  {
    "text": "It's a little bit of a cheat,\nI'll go near the center. The next direction is\nmuch more like that",
    "start": "2459580",
    "end": "2469280"
  },
  {
    "text": "and then goes to\nthe bottom, which would be somewhere like\nthere inside that level set,",
    "start": "2469280",
    "end": "2476250"
  },
  {
    "text": "and then the next direction\nwould probably be very close. So this is not 90 degrees unless\nyou're in the A inner product.",
    "start": "2476250",
    "end": "2487520"
  },
  {
    "text": "So this is a 90 degree angle\nin the A inner product.",
    "start": "2487520",
    "end": "2497350"
  },
  {
    "text": "And what does the\nA inner product do? In the A inner product,\nin which level sets",
    "start": "2497350",
    "end": "2509080"
  },
  {
    "text": "are circles, or spheres or\nwhatever, in whatever dimension",
    "start": "2509080",
    "end": "2518040"
  },
  {
    "text": "we want. Well, I'm not going\nto be, I don't want to be held to everything here.",
    "start": "2518040",
    "end": "2526099"
  },
  {
    "text": "If you see this\npicture, then we've",
    "start": "2526100",
    "end": "2531700"
  },
  {
    "text": "not only made headway with\nthe conjugate gradient method, which is a big deal\nfor solving linear systems,",
    "start": "2531700",
    "end": "2539150"
  },
  {
    "text": "but also we've made headway with\nthe conjugate gradient method for minimizing functions.",
    "start": "2539150",
    "end": "2545290"
  },
  {
    "text": "And if the function\nwasn't quadratic, and our equations\nweren't linear,",
    "start": "2545290",
    "end": "2551330"
  },
  {
    "text": "the conjugate gradient idea\nwould still be available. Non-linear conjugate\ngradients would somehow",
    "start": "2551330",
    "end": "2559759"
  },
  {
    "text": "follow the same idea of\nA-orthogonal search directions.",
    "start": "2559760",
    "end": "2568310"
  },
  {
    "text": "So these are not the gradients,\nthey're the search directions here. OK.",
    "start": "2568310",
    "end": "2575940"
  },
  {
    "text": "That's sort of my speech about\nthe conjugate gradient method.",
    "start": "2575940",
    "end": "2583530"
  },
  {
    "text": "Without having checked all\nthe formulas in the cycle, we know how much work is\ninvolved, we know what we get,",
    "start": "2583530",
    "end": "2592020"
  },
  {
    "text": "and we see a bit\nof the geometry. OK.",
    "start": "2592020",
    "end": "2597130"
  },
  {
    "text": "And we see it as a\nlinear systems solver and also as an energy\nminimizer, and those",
    "start": "2597130",
    "end": "2606660"
  },
  {
    "text": "are both important ways to\nthink of conjugate gradients. OK.",
    "start": "2606660",
    "end": "2612660"
  },
  {
    "text": "I'll just take a few final\nminutes to say a word about",
    "start": "2612660",
    "end": "2620339"
  },
  {
    "text": "other -- what if our problem\nis not symmetric positive definite?",
    "start": "2620340",
    "end": "2626460"
  },
  {
    "text": "What if we have an\nun-symmetric problem, or a symmetric, but not\npositive definite problem?",
    "start": "2626460",
    "end": "2633460"
  },
  {
    "text": "Well, in that case, these\ndenominators could be zero.",
    "start": "2633460",
    "end": "2641869"
  },
  {
    "text": "I mean, if we don't know that\nA is positive definite, that could be a zero, it\ncould be anything,",
    "start": "2641870",
    "end": "2648040"
  },
  {
    "text": "and that method is broken. So I want to suggest a safer,\nbut more expensive method now",
    "start": "2648040",
    "end": "2658030"
  },
  {
    "text": "for -- so this would be\nMINRES, minimum residual.",
    "start": "2658030",
    "end": "2664850"
  },
  {
    "text": "So the idea is\nminimize, choose x_k to minimize the norm of\nr_k, that's the residual.",
    "start": "2664850",
    "end": "2675080"
  },
  {
    "text": "So minimize the norm\nof b minus A*x_k.",
    "start": "2675080",
    "end": "2684230"
  },
  {
    "text": "So that's my choice of\nx_k, and the question is, how quickly can I do it?",
    "start": "2684230",
    "end": "2691829"
  },
  {
    "start": "2690000",
    "end": "3140000"
  },
  {
    "text": "So this is MINRES, OK. And there are other\nrelated methods,",
    "start": "2691830",
    "end": "2698690"
  },
  {
    "text": "there's a whole\nfamily of methods. What do I want to\nsay about MINRES?",
    "start": "2698690",
    "end": "2703750"
  },
  {
    "text": "Let me just find MINRES. Well, yeah.",
    "start": "2703750",
    "end": "2710440"
  },
  {
    "text": " I guess, for this I\nwill start with Arnoldi.",
    "start": "2710440",
    "end": "2718839"
  },
  {
    "start": "2718840",
    "end": "2728980"
  },
  {
    "text": "So for this different algorithm,\nwhich comes up with a different",
    "start": "2728980",
    "end": "2734180"
  },
  {
    "text": "x_k, I'm going to\n-- and of course, the x_k is going to be\nin the Krylov space K_k,",
    "start": "2734180",
    "end": "2743600"
  },
  {
    "text": "and I'm going to\nuse this good basis. This gives me the\ngreat basis of q's.",
    "start": "2743600",
    "end": "2752890"
  },
  {
    "text": "So I turn my problem,\nI convert the problem. I convert this minimization --\nI look for x as a combination",
    "start": "2752890",
    "end": "2770790"
  },
  {
    "text": "of the of the q's, right? Is that how you\nsee x equals Q*y?",
    "start": "2770790",
    "end": "2778980"
  },
  {
    "text": "So instead of looking\nfor the vector x, I'm looking for\nthe vector y, which",
    "start": "2778980",
    "end": "2784099"
  },
  {
    "text": "tells me what combination of\nthese columns, the q's, x is.",
    "start": "2784100",
    "end": "2790110"
  },
  {
    "text": "In other words, I'm working\nin the q system, the q basis. So I'm working with y.",
    "start": "2790110",
    "end": "2796330"
  },
  {
    "text": "So I would, naturally, write\nthis as a minimum of b minus",
    "start": "2796330",
    "end": "2803890"
  },
  {
    "text": "A*Q*y_k.  Same problem.",
    "start": "2803890",
    "end": "2809120"
  },
  {
    "text": "Same problem, I'm\njust deciding, OK, I'm going to find this x as\na combination of the q's,",
    "start": "2809120",
    "end": "2815930"
  },
  {
    "text": "because those q's\nare orthogonal, and that will make this\ncalculation a lot nicer.",
    "start": "2815930",
    "end": "2822810"
  },
  {
    "text": "OK, what's going to happen here? You know that I'm going to use\nthe property of the q's that",
    "start": "2822810",
    "end": "2830680"
  },
  {
    "text": "A*Q is Q*H.",
    "start": "2830680",
    "end": "2835730"
  },
  {
    "text": "That was the key point\nthat Arnoldi achieved. And the net result is\ngoing to be, of course,",
    "start": "2835730",
    "end": "2844070"
  },
  {
    "text": "I have to do this at level\nk, so I'm chopping it off, and I have to\npatiently, and the notes",
    "start": "2844070",
    "end": "2851380"
  },
  {
    "text": "do that, patiently\nsee, OK, what's happening when we chop it\noff, chop off these matrices.",
    "start": "2851380",
    "end": "2858579"
  },
  {
    "text": "We have to watch where\nthe non-zeros appear. In the end, I get to a\nminimization problem for y,",
    "start": "2858580",
    "end": "2865930"
  },
  {
    "text": "so we get to a minimum\nproblem for the matrix",
    "start": "2865930",
    "end": "2872309"
  },
  {
    "text": "H that leads me to to\nthe y that I'm after.",
    "start": "2872310",
    "end": "2881920"
  },
  {
    "text": "So it's a minimum problem\nfor a piece, sorry, I should really say H_k, some\npiece of it, like this piece.",
    "start": "2881920",
    "end": "2895400"
  },
  {
    "text": "In fact, exactly that piece.",
    "start": "2895400",
    "end": "2902770"
  },
  {
    "text": "So if I ask -- all right,\nsuppose I have a least squares",
    "start": "2902770",
    "end": "2907960"
  },
  {
    "text": "problem. This is least squares because\nmy norm is just the square,",
    "start": "2907960",
    "end": "2914200"
  },
  {
    "text": "the norm squared. I'm just working with l^2 norm.",
    "start": "2914200",
    "end": "2919420"
  },
  {
    "text": "And here's my matrix. ",
    "start": "2919420",
    "end": "2925820"
  },
  {
    "text": "It's 3 by 2, so I'm looking\nat least squares problems,",
    "start": "2925820",
    "end": "2932210"
  },
  {
    "text": "in that -- matrices\nof that type,",
    "start": "2932210",
    "end": "2937900"
  },
  {
    "text": "and they're easy to solve. The notes will give the\nalgorithm that solves the least",
    "start": "2937900",
    "end": "2944539"
  },
  {
    "text": "squares problem. I guess what I'm interested in\nhere is just is it expensive or not?",
    "start": "2944540",
    "end": "2950780"
  },
  {
    "text": "Well, the least squares\nproblem will not be expensive;",
    "start": "2950780",
    "end": "2955910"
  },
  {
    "text": "it's only got one extra\nrow, compared to the number of columns, that's not bad. What's expensive is the fact\nthat our H is not tri-diagonal",
    "start": "2955910",
    "end": "2967690"
  },
  {
    "text": "anymore, these are\nnot zeros anymore. I have to compute all\nthese H's, so the hard work",
    "start": "2967690",
    "end": "2977050"
  },
  {
    "text": "is back in Arnoldi. So it's Arnoldi\nleading to MINRES,",
    "start": "2977050",
    "end": "2986650"
  },
  {
    "text": "and we have serious\nwork already in Arnoldi. We don't have short recurrences,\nwe've got all these h's.",
    "start": "2986650",
    "end": "2994170"
  },
  {
    "text": "OK, enough. Because ARPACK would be a good\ncode to call to do MINRES.",
    "start": "2994170",
    "end": "3008200"
  },
  {
    "text": "OK that's my shot\nat the key Krylov",
    "start": "3008200",
    "end": "3015140"
  },
  {
    "text": "algorithms of numerical\nlinear algebra, and you see that they're\nmore subtle than Jacobi.",
    "start": "3015140",
    "end": "3024270"
  },
  {
    "text": "Jacobi and\nGauss-Seidel were just straight simple\niterations; these have recurrence that\nproduces orthogonality,",
    "start": "3024270",
    "end": "3035500"
  },
  {
    "text": "and you pay very little,\nand you get so much. OK, so that's Krylov methods,\nand that section of notes",
    "start": "3035500",
    "end": "3045570"
  },
  {
    "text": "is up on the web, and of\ncourse, at some point,",
    "start": "3045570",
    "end": "3051640"
  },
  {
    "text": "numerical comparisons of how\nfast is conjugate gradients,",
    "start": "3051640",
    "end": "3058329"
  },
  {
    "text": "and is it faster than\ndirect elimination? You know, because that's a\nbig choice you have to make.",
    "start": "3058330",
    "end": "3066000"
  },
  {
    "text": "Suppose you do have a symmetric\npositive definite matrix. Do you do Gauss elimination with\nre-ordering, minimum degree,",
    "start": "3066000",
    "end": "3077150"
  },
  {
    "text": "or do you make the\ncompletely different choice of iterative methods\nlike conjugate gradients?",
    "start": "3077150",
    "end": "3084599"
  },
  {
    "text": "And that's a decision you\nhave to make when you're faced with a large\nmatrix problem,",
    "start": "3084600",
    "end": "3091540"
  },
  {
    "text": "and only experiments can compare\ntwo such widely different",
    "start": "3091540",
    "end": "3099510"
  },
  {
    "text": "directions to take. So I like to end up by\nemphasizing that there's still",
    "start": "3099510",
    "end": "3110890"
  },
  {
    "text": "a big question. Do this or do minimum degree?",
    "start": "3110890",
    "end": "3118010"
  },
  {
    "text": "And only some experiments\nwill show which is the better.",
    "start": "3118010",
    "end": "3124860"
  },
  {
    "text": "OK, so that's good for\ntoday, and next time will be direct\nsolution, using the FFT.",
    "start": "3124860",
    "end": "3133620"
  },
  {
    "text": "Good, thanks. ",
    "start": "3133620",
    "end": "3140941"
  }
]