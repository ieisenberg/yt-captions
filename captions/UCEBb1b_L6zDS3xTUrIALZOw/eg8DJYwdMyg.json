[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6760"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation, or to\nview additional materials",
    "start": "6760",
    "end": "13390"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13390",
    "end": "28431"
  },
  {
    "text": "PROFESSOR: Hello, everybody. Before we start the material,\na couple of announcements.",
    "start": "28431",
    "end": "35059"
  },
  {
    "text": "As usual, there's some\nreading assignments, and you might be surprised\nto see something from Chapter",
    "start": "35060",
    "end": "40940"
  },
  {
    "text": "5 suddenly popping up. But this is my\nrelentless attempt to introduce more Python.",
    "start": "40940",
    "end": "47050"
  },
  {
    "text": "We'll see one new concept later\ntoday, list comprehension. Today we're going to\nlook at classification.",
    "start": "47050",
    "end": "55649"
  },
  {
    "text": "And you remember\nlast, on Monday, we looked at\nunsupervised learning.",
    "start": "55650",
    "end": "61640"
  },
  {
    "text": "Today we're looking at\nsupervised learning. It can usually be divided\ninto two categories.",
    "start": "61640",
    "end": "68659"
  },
  {
    "text": "Regression, where\nyou try and predict some real number associated\nwith the feature vector,",
    "start": "68660",
    "end": "75420"
  },
  {
    "text": "and this is something\nwe've already done really, back when we looked at curve\nfitting, linear regression",
    "start": "75420",
    "end": "82980"
  },
  {
    "text": "in particular. It was exactly building a model\nthat, given some features,",
    "start": "82980",
    "end": "88080"
  },
  {
    "text": "would predict a point. In this case, it\nwas pretty simple. It was given x predict y.",
    "start": "88080",
    "end": "93810"
  },
  {
    "text": "You can imagine generalizing\nthat to multi dimensions. Today I'm going to talk\nabout classification,",
    "start": "93810",
    "end": "102659"
  },
  {
    "text": "which is very common,\nin many ways more common than regression for--",
    "start": "102660",
    "end": "108390"
  },
  {
    "text": "in the machine learning world. And here the goal is to predict\na discrete value, often called",
    "start": "108390",
    "end": "115170"
  },
  {
    "text": "a label, associated with\nsome feature vector.",
    "start": "115170",
    "end": "120420"
  },
  {
    "text": "So this is the sort of thing\nwhere you try and, for example, predict whether a\nperson will have",
    "start": "120420",
    "end": "127550"
  },
  {
    "text": "an adverse reaction to a drug. You're not looking\nfor a real number, you're looking for will they get\nsick, will they not get sick.",
    "start": "127550",
    "end": "137719"
  },
  {
    "text": "Maybe you're trying to predict\nthe grade in a course A, B, C, D, and other grades\nwe won't mention.",
    "start": "137720",
    "end": "145349"
  },
  {
    "text": "Again, those are\nlabels, so it doesn't have to be a binary label but\nit's a finite number of labels.",
    "start": "145350",
    "end": "152860"
  },
  {
    "text": "So here's an example\nto start with. We won't linger on it too long. This is basically\nsomething you saw",
    "start": "152860",
    "end": "160660"
  },
  {
    "text": "in an earlier lecture, where\nwe had a bunch of animals and a bunch of properties,\nand a label identifying",
    "start": "160660",
    "end": "168070"
  },
  {
    "text": "whether or not they\nwere a reptile. ",
    "start": "168070",
    "end": "175810"
  },
  {
    "text": "So we start by building\na distance matrix.",
    "start": "175810",
    "end": "181640"
  },
  {
    "text": "How far apart they are,\nan in fact, in this case,",
    "start": "181640",
    "end": "187270"
  },
  {
    "text": "I'm not using the\nrepresentation you just saw. I'm going to use the\nbinary representation,",
    "start": "187270",
    "end": "195010"
  },
  {
    "text": "As Professor Grimson showed\nyou, and for the reasons he showed you. ",
    "start": "195010",
    "end": "201240"
  },
  {
    "text": "If you're interested, I didn't\nproduce this table by hand, I wrote some Python\ncode to produce it,",
    "start": "201240",
    "end": "208500"
  },
  {
    "text": "not only to compute\nthe distances, but more delicately to\nproduce the actual table.",
    "start": "208500",
    "end": "216030"
  },
  {
    "text": "And you'll probably find it\ninstructive at some point to at least remember\nthat that code is there,",
    "start": "216030",
    "end": "221700"
  },
  {
    "text": "in case you need to ever\nproduce a table for some paper. In general, you probably noticed\nI spent relatively little time",
    "start": "221700",
    "end": "231100"
  },
  {
    "text": "going over the actual\nvast amounts of codes we've been posting. That doesn't mean you\nshouldn't look at it.",
    "start": "231100",
    "end": "238930"
  },
  {
    "text": "In part, a lot of\nit's there because I'm hoping at some point in\nthe future it will be handy",
    "start": "238930",
    "end": "244510"
  },
  {
    "text": "for you to have a model\non how to do something. All right.",
    "start": "244510",
    "end": "249609"
  },
  {
    "text": "So we have all these distances. And we can tell how far apart\none animal is from another.",
    "start": "249610",
    "end": "258070"
  },
  {
    "text": "Now how do we use those\nto classify animals? And the simplest approach\nto classification,",
    "start": "258070",
    "end": "265020"
  },
  {
    "text": "and it's actually one that's\nused a fair amount in practice is called nearest neighbor.",
    "start": "265020",
    "end": "271750"
  },
  {
    "text": "So the learning part is trivial. We don't actually learn anything\nother than we just remember.",
    "start": "271750",
    "end": "279010"
  },
  {
    "text": "So we remember\nthe training data. And when we want to predict\nthe label of a new example,",
    "start": "279010",
    "end": "285640"
  },
  {
    "text": "we find the nearest example\nin the training data, and just choose the label\nassociated with that example.",
    "start": "285640",
    "end": "293030"
  },
  {
    "text": "So here I've just\ndrawing a cloud of red dots and black dots.",
    "start": "293030",
    "end": "299060"
  },
  {
    "text": "I have a fuschia\ncolored X. And if I want to classify\nX as black or red,",
    "start": "299060",
    "end": "305230"
  },
  {
    "text": "I'd say well its\nnearest neighbor is red. So we'll call X red. ",
    "start": "305230",
    "end": "312650"
  },
  {
    "text": "Doesn't get much\nsimpler than that. ",
    "start": "312650",
    "end": "318781"
  },
  {
    "text": "All right. Let's try and do it\nnow for our animals. I've blocked out this\nlower right hand corner,",
    "start": "318781",
    "end": "326250"
  },
  {
    "text": "because I want to classify these\nthree animals that are in gray. So my training data, very\nsmall, are these animals.",
    "start": "326250",
    "end": "334310"
  },
  {
    "text": "And these are my test set here. So let's first try and\nclassify the zebra.",
    "start": "334310",
    "end": "340980"
  },
  {
    "text": "We look at the zebra's\nnearest neighbor. Well it's either a\nguppy or a dart frog.",
    "start": "340980",
    "end": "348229"
  },
  {
    "text": "Well, let's just choose one. Let's choose the guppy. And if we look at the\nguppy, it's not a reptile,",
    "start": "348230",
    "end": "354650"
  },
  {
    "text": "so we say the zebra\nis not a reptile. So got one right. ",
    "start": "354650",
    "end": "362759"
  },
  {
    "text": "Look at the python, choose\nits nearest neighbor, say it's a cobra. The label associated\nwith cobra is reptile,",
    "start": "362760",
    "end": "369990"
  },
  {
    "text": "so we win again on the python. ",
    "start": "369990",
    "end": "376030"
  },
  {
    "text": "Alligator, it's nearest\nneighbor is clearly a chicken.",
    "start": "376030",
    "end": "382170"
  },
  {
    "text": "And so we classify the\nalligator as not a reptile.",
    "start": "382170",
    "end": "387770"
  },
  {
    "text": " Oh, dear.",
    "start": "387770",
    "end": "393400"
  },
  {
    "text": "Clearly the wrong answer. ",
    "start": "393400",
    "end": "398720"
  },
  {
    "text": "All right. What might have gone wrong? Well, the problem with\nK nearest neighbors,",
    "start": "398720",
    "end": "409040"
  },
  {
    "text": "we can illustrate it by\nlooking at this example. So one of the things people do\nwith classifiers these days is",
    "start": "409040",
    "end": "415310"
  },
  {
    "text": "handwriting recognition. So I just copied from a\nwebsite a bunch of numbers,",
    "start": "415310",
    "end": "421800"
  },
  {
    "text": "then I wrote the number 40 in\nmy own inimitable handwriting.",
    "start": "421800",
    "end": "426810"
  },
  {
    "text": "So if we go and we look for,\nsay, the nearest neighbor of four-- or sorry, of whatever\nthat digit is.",
    "start": "426810",
    "end": "433020"
  },
  {
    "text": " It is, I believe, this one.",
    "start": "433020",
    "end": "440080"
  },
  {
    "text": "And sure enough that's\nthe row of fours. We're OK on this. ",
    "start": "440080",
    "end": "447570"
  },
  {
    "text": "Now if we want to\nclassify my zero,",
    "start": "447570",
    "end": "452910"
  },
  {
    "text": "the actual nearest\nneighbor, in terms of the bitmaps if you will,\nturns out to be this guy.",
    "start": "452910",
    "end": "459770"
  },
  {
    "text": "A very poorly written nine. I didn't make up this nine,\nit was it was already there.",
    "start": "459770",
    "end": "465930"
  },
  {
    "text": "And the problem we see here\nwhen we use nearest neighbor is if something is noisy, if you\nhave one noisy piece of data,",
    "start": "465930",
    "end": "475540"
  },
  {
    "text": "in this case, it's rather\nugly looking version of nine, you can get the wrong\nanswer because you match it.",
    "start": "475540",
    "end": "481170"
  },
  {
    "text": " And indeed, in this case, you\nwould get the wrong answer.",
    "start": "481170",
    "end": "487490"
  },
  {
    "text": "What is usually done to\navoid that is something called K nearest neighbors.",
    "start": "487490",
    "end": "492940"
  },
  {
    "text": " And the basic idea here\nis that we don't just",
    "start": "492940",
    "end": "499930"
  },
  {
    "text": "take the nearest\nneighbors, we take some number of nearest\nneighbors, usually",
    "start": "499930",
    "end": "506440"
  },
  {
    "text": "an odd number, and we\njust let them vote. So now if we want to\nclassify this fuchsia X,",
    "start": "506440",
    "end": "516900"
  },
  {
    "text": "and we said K equal to\nthree, we say well these are it's three\nnearest neighbors.",
    "start": "516900",
    "end": "522599"
  },
  {
    "text": "One is red, two\nare black, so we're going to call X black\nis our better guess.",
    "start": "522600",
    "end": "529540"
  },
  {
    "text": "And maybe that actually\nis a better guess, because it looks like this\nred point here is really an outlier, and we don't want\nto let the outliers dominate",
    "start": "529540",
    "end": "539320"
  },
  {
    "text": "our classification. And this is why people almost\nalways use K nearest neighbors",
    "start": "539320",
    "end": "545560"
  },
  {
    "text": "rather than just\nnearest neighbor. Now if we look at this, and\nwe use K nearest neighbors,",
    "start": "545560",
    "end": "554519"
  },
  {
    "text": "those are the three nearest\nto the first numeral, and they are all fours.",
    "start": "554520",
    "end": "561132"
  },
  {
    "text": "And if we look at the\nK nearest neighbors for the second numeral,\nwe still have this nine but now we have two zeros.",
    "start": "561132",
    "end": "568600"
  },
  {
    "text": "And so we vote and we\ndecide it's a zero. Is it infallible? No.",
    "start": "568600",
    "end": "574130"
  },
  {
    "text": "But it's typically\nmuch more reliable than just nearest neighbors,\nhence used much more often.",
    "start": "574130",
    "end": "581620"
  },
  {
    "text": " And that was our problem, by\nthe way, with the alligator.",
    "start": "581620",
    "end": "589120"
  },
  {
    "text": "The nearest neighbor\nwas the chicken, but if we went back\nand looked at it--",
    "start": "589120",
    "end": "594170"
  },
  {
    "text": "maybe we should go do that. ",
    "start": "594170",
    "end": "601949"
  },
  {
    "text": "And we take the alligator's\nthree nearest neighbors, it would be the chicken, a\ncobra, and the rattlesnake--",
    "start": "601950",
    "end": "609870"
  },
  {
    "text": "or the boa, we\ndon't care, and we would end up correctly\nclassifying it now",
    "start": "609870",
    "end": "615180"
  },
  {
    "text": "as a reptile. Yes? AUDIENCE: Is there like a\nlimit to how many [INAUDIBLE]?",
    "start": "615180",
    "end": "622222"
  },
  {
    "text": "PROFESSOR: The\nquestion is is there a limit to how many nearest\nneighbors you'd want? Absolutely.",
    "start": "622222",
    "end": "629560"
  },
  {
    "text": "Most obviously, there's no point\nin setting K equal to-- whoops. Ooh, on the rebound--",
    "start": "629560",
    "end": "636089"
  },
  {
    "text": "to the size of the training set. So one of the problems\nwith K nearest neighbors",
    "start": "636090",
    "end": "642940"
  },
  {
    "text": "is efficiency. If you're trying to\ndefine K nearest neighbors and K is bigger,\nit takes longer.",
    "start": "642940",
    "end": "651529"
  },
  {
    "text": "So we worry about\nhow big K should be. And if we make it too big--",
    "start": "651530",
    "end": "658400"
  },
  {
    "text": "and this is a crucial thing-- we end up getting dominated\nby the size of the class.",
    "start": "658400",
    "end": "667240"
  },
  {
    "text": "So let's look at this\npicture we had before. It happens to be more\nred dots than black dots.",
    "start": "667240",
    "end": "674650"
  },
  {
    "text": "If I make K 10 or 15, I'm going\nto classify a lot of things",
    "start": "674650",
    "end": "680440"
  },
  {
    "text": "as red, just because red is so\nmuch more prevalent than black.",
    "start": "680440",
    "end": "686230"
  },
  {
    "text": "And so when you have an\nimbalance, which you usually do, you have to be very careful\nabout K. Does that make sense?",
    "start": "686230",
    "end": "694250"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] choose K? PROFESSOR: So how\ndo you choose K? Remember back on Monday when we\ntalked about choosing K for K",
    "start": "694250",
    "end": "703780"
  },
  {
    "text": "means clustering? We typically do a very\nsimilar kind of thing.",
    "start": "703780",
    "end": "709740"
  },
  {
    "text": "We take our training data and\nwe split it into two parts.",
    "start": "709740",
    "end": "716230"
  },
  {
    "text": "So we have training\nand testing, but now we just take the training,\nand we split that",
    "start": "716230",
    "end": "721260"
  },
  {
    "text": "into training and\ntesting multiple times. And we experiment with\ndifferent K's, and we",
    "start": "721260",
    "end": "728540"
  },
  {
    "text": "see which K's gives us the best\nresult on the training data. And then that becomes our K.\nAnd that's a very common method.",
    "start": "728540",
    "end": "739190"
  },
  {
    "text": "It's called\ncross-validation, and it's-- for almost all of machine\nlearning, the algorithms",
    "start": "739190",
    "end": "746760"
  },
  {
    "text": "have parameters in this case,\nit's just one parameter, K. And the way we typically\nchoose the parameter values",
    "start": "746760",
    "end": "754170"
  },
  {
    "text": "is by searching\nthrough the space using this cross-validation\nin the training data.",
    "start": "754170",
    "end": "760720"
  },
  {
    "text": "Does that makes\nsense to everybody? Great question. And there was someone\nelse had a question,",
    "start": "760720",
    "end": "766230"
  },
  {
    "text": "but maybe it was the same. Do you still have a question? AUDIENCE: Well, just that\nyou were using like K nearest",
    "start": "766230",
    "end": "772310"
  },
  {
    "text": "and you get, like\nif my K is three and I get three different\nclusters for the K [INAUDIBLE] PROFESSOR: Three\ndifferent clusters?",
    "start": "772310",
    "end": "778183"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] PROFESSOR: Well, right. So if K is 3, and I had\nred, black, and purple",
    "start": "778183",
    "end": "785250"
  },
  {
    "text": "and I get one of each,\nthen what do I do? And then I'm kind of stuck. So you need to typically\nchoose K in such a way",
    "start": "785250",
    "end": "793260"
  },
  {
    "text": "that when you vote\nyou get a winner. Nice. So if there's two, any\nodd number will do.",
    "start": "793260",
    "end": "799880"
  },
  {
    "text": "If it's three, well then\nyou need another number so that there's some-- so\nthere's always a majority.",
    "start": "799880",
    "end": "805410"
  },
  {
    "text": "Right? You want to make sure\nthat there is a winner.",
    "start": "805410",
    "end": "810920"
  },
  {
    "text": "Also a good question. ",
    "start": "810920",
    "end": "816900"
  },
  {
    "text": "Let's see if I get\nthis to you directly.  I'm much better at\nthrowing overhand, I guess.",
    "start": "816900",
    "end": "825560"
  },
  {
    "text": "Wow. Finally got applause\nfor something. All right, advantages\nand disadvantages KNN?",
    "start": "825560",
    "end": "832770"
  },
  {
    "text": "The learning is\nreally fast, right? I just remember everything. No math is required.",
    "start": "832770",
    "end": "839472"
  },
  {
    "text": "Didn't have to show\nyou any theory. Was obviously an idea. It's easy to explain the method\nto somebody, and the results.",
    "start": "839472",
    "end": "846899"
  },
  {
    "text": "Why did I label it black? Because that's who\nit was closest to.",
    "start": "846900",
    "end": "852210"
  },
  {
    "text": "The disadvantages is\nit's memory intensive. If I've got a million examples,\nI have to store them all.",
    "start": "852210",
    "end": "859740"
  },
  {
    "text": "And the predictions\ncan take a long time. If I have an example and I\nwant to find its K nearest",
    "start": "859740",
    "end": "867650"
  },
  {
    "text": "neighbors, I'm doing\na lot of comparisons. Right? If I have a million\ntank training points",
    "start": "867650",
    "end": "873709"
  },
  {
    "text": "I have to compare my\nexample to all a million. So I have no real\npre-processing overhead.",
    "start": "873710",
    "end": "881100"
  },
  {
    "text": "But each time I need\nto do a classification, it takes a long time. Now there are better\nalgorithms and brute force",
    "start": "881100",
    "end": "888700"
  },
  {
    "text": "that give you approximate\nK nearest neighbors. But on the whole,\nit's still not fast.",
    "start": "888700",
    "end": "896760"
  },
  {
    "text": "And we're not getting any\ninformation about what process",
    "start": "896760",
    "end": "902920"
  },
  {
    "text": "might have generated the data. We don't have a model of the\ndata in the way we say when",
    "start": "902920",
    "end": "910290"
  },
  {
    "text": "we did our linear regression\nfor curve fitting, we had a model for the data that\nsort of described the pattern.",
    "start": "910290",
    "end": "918279"
  },
  {
    "text": "We don't get that out\nof k nearest neighbors. I'm going to show you a\ndifferent approach where",
    "start": "918280",
    "end": "925339"
  },
  {
    "text": "we do get that. And I'm going to do it on\na more interesting example than reptiles.",
    "start": "925340",
    "end": "932029"
  },
  {
    "text": "I apologize to those of\nyou who are reptologists. So you probably all\nheard of the Titanic.",
    "start": "932030",
    "end": "940160"
  },
  {
    "text": "There was a movie\nabout it, I'm told. It was one of the great\nsea disasters of all time,",
    "start": "940160",
    "end": "947610"
  },
  {
    "text": "a so-called unsinkable ship-- they had advertised\nit as unsinkable--",
    "start": "947610",
    "end": "953060"
  },
  {
    "text": "hit an iceberg and went down. Of the 1,300\npassengers, 812 died.",
    "start": "953060",
    "end": "958760"
  },
  {
    "text": "The crew did way worse. So at least it looks as\nif the curve was actually pretty heroic.",
    "start": "958760",
    "end": "964070"
  },
  {
    "text": "They had a higher death rate. So we're going to\nuse machine learning to see if we can predict\nwhich passengers survived.",
    "start": "964070",
    "end": "972529"
  },
  {
    "text": " There's an online\ndatabase I'm using.",
    "start": "972530",
    "end": "977960"
  },
  {
    "text": "It doesn't have all\n1,200 passengers, but it has information\nabout 1,046 of them.",
    "start": "977960",
    "end": "984790"
  },
  {
    "text": "Some of them they couldn't\nget the information. Says what cabin class they\nwere in first, second,",
    "start": "984790",
    "end": "989829"
  },
  {
    "text": "or third, how old they\nwere, and their gender. Also has their\nname and their home",
    "start": "989830",
    "end": "996100"
  },
  {
    "text": "address and things,\nwhich I'm not using. We want to use these\nfeatures to see",
    "start": "996100",
    "end": "1002990"
  },
  {
    "text": "if we can predict\nwhich passengers were going to survive the disaster.",
    "start": "1002990",
    "end": "1010030"
  },
  {
    "text": "Well, the first\nquestion is something that Professor Grimson\nalluded to is, is it OK,",
    "start": "1010030",
    "end": "1017529"
  },
  {
    "text": "just to look at accuracy? How are we going to evaluate\nour machine learning?",
    "start": "1017530",
    "end": "1023560"
  },
  {
    "text": "And it's not. If we just predict died\nfor everybody, well then we'll be 62% accurate for the\npassengers and 76% accurate",
    "start": "1023560",
    "end": "1034319"
  },
  {
    "text": "for the crew members. Usually machine\nlearning, if you're 76% you say that's not bad.",
    "start": "1034319",
    "end": "1040709"
  },
  {
    "text": "Well, here I can get that\njust by predicting died. So whenever you have a class\nimbalance that much more of one",
    "start": "1040710",
    "end": "1050490"
  },
  {
    "text": "than the other, accuracy isn't\na particularly meaningful measure. ",
    "start": "1050490",
    "end": "1057340"
  },
  {
    "text": "I discovered this early on\nin my work and medical area. There are a lot of\ndiseases that rarely occur,",
    "start": "1057340",
    "end": "1063549"
  },
  {
    "text": "they occur in say 0.1%\nof the population. And I can build a great\nmodel for predicting it",
    "start": "1063550",
    "end": "1069280"
  },
  {
    "text": "by just saying,\nno, you don't have it, which will be 0.999%\naccurate, but totally useless.",
    "start": "1069280",
    "end": "1077170"
  },
  {
    "text": " Unfortunately, you do see\npeople doing that sort",
    "start": "1077170",
    "end": "1082810"
  },
  {
    "text": "of thing in the literature.  You saw these in an earlier\nlecture, just to remind you,",
    "start": "1082810",
    "end": "1090710"
  },
  {
    "text": "we're going to be\nlooking at other metrics. Sensitivity, think\nof that as how good",
    "start": "1090710",
    "end": "1098870"
  },
  {
    "text": "is it at identifying\nthe positive cases. In this case, positive\nis going to be dead.",
    "start": "1098870",
    "end": "1106980"
  },
  {
    "text": "How specific is it, and the\npositive predictive value.",
    "start": "1106980",
    "end": "1113110"
  },
  {
    "text": "If we say somebody died,\nwhat's the probability is that they really did?",
    "start": "1113110",
    "end": "1118172"
  },
  {
    "text": "And then there's the\nnegative predictive value. If we say they\ndidn't die, what's the probability they didn't die?",
    "start": "1118172",
    "end": "1123430"
  },
  {
    "text": " So these are four\nvery common metrics.",
    "start": "1123430",
    "end": "1130040"
  },
  {
    "text": "There is something called an\nF score that combines them, but I'm not going to be\nshowing you that today.",
    "start": "1130040",
    "end": "1138500"
  },
  {
    "text": "I will mention that\nin the literature, people often use the word\nrecall to mean sensitivity",
    "start": "1138500",
    "end": "1144170"
  },
  {
    "text": "or sensitivity I mean recall,\nand specificity and precision",
    "start": "1144170",
    "end": "1149480"
  },
  {
    "text": "are used pretty much\ninterchangeably. So you might see various\ncombinations of these words.",
    "start": "1149480",
    "end": "1156080"
  },
  {
    "text": "Typically, people talk\nabout recall n precision or sensitivity and specificity.",
    "start": "1156080",
    "end": "1162730"
  },
  {
    "text": "Does that makes\nsense, why we want to look at the measures\nother than accuracy? We will look at accuracy,\ntoo, and how they all tell us",
    "start": "1162730",
    "end": "1171330"
  },
  {
    "text": "kind of different\nthings, and how you might choose a different balance.",
    "start": "1171330",
    "end": "1177840"
  },
  {
    "text": "For example, if I'm running\na screening test, say for breast cancer, a\nmammogram, and trying",
    "start": "1177840",
    "end": "1187600"
  },
  {
    "text": "to find the people\nwho should get on for a more extensive\nexamination, what do I want to\nemphasize here?",
    "start": "1187600",
    "end": "1195990"
  },
  {
    "text": "Which of these is likely\nto be the most important? ",
    "start": "1195990",
    "end": "1202049"
  },
  {
    "text": "Or what would you\ncare about most? ",
    "start": "1202050",
    "end": "1208190"
  },
  {
    "text": "Well, maybe I want sensitivity. Since I'm going to send this\nperson on for future tests,",
    "start": "1208190",
    "end": "1215390"
  },
  {
    "text": "I really don't want to miss\nsomebody who has cancer, and so I might\nthink sensitivity is",
    "start": "1215390",
    "end": "1222580"
  },
  {
    "text": "more important than specificity\nin that particular case. On the other hand,\nif I'm deciding",
    "start": "1222580",
    "end": "1230720"
  },
  {
    "text": "who is so sick I should do\nopen heart surgery on them,",
    "start": "1230720",
    "end": "1236710"
  },
  {
    "text": "maybe I want to be\npretty specific. Because the risk of the\nsurgery itself are very high.",
    "start": "1236710",
    "end": "1243190"
  },
  {
    "text": "I don't want to do it on\npeople who don't need it. So we end up having to choose\na balance between these things,",
    "start": "1243190",
    "end": "1251530"
  },
  {
    "text": "depending upon our application. ",
    "start": "1251530",
    "end": "1257159"
  },
  {
    "text": "The other thing I want to talk\nabout before actually building a classifier is how we\ntest our classifier,",
    "start": "1257160",
    "end": "1267760"
  },
  {
    "text": "because this is very important. I'm going to talk about\ntwo different methods,",
    "start": "1267760",
    "end": "1273190"
  },
  {
    "text": "leave one out class of\ntesting and repeated random subsampling.",
    "start": "1273190",
    "end": "1281730"
  },
  {
    "text": "For leave one out,\nit's typically used when you have a\nsmall number of examples,",
    "start": "1281730",
    "end": "1291140"
  },
  {
    "text": "so you want as much\ntraining data as possible as you build your model.",
    "start": "1291140",
    "end": "1296730"
  },
  {
    "text": "So you take all of your n\nexamples, remove one of them, train on n minus\n1, test on the 1.",
    "start": "1296730",
    "end": "1305850"
  },
  {
    "text": "Then you put that 1 back\nand remove another 1. Train on n minus 1, test on 1.",
    "start": "1305850",
    "end": "1313110"
  },
  {
    "text": "And you do this for each\nelement of the data, and then you average\nyour results. ",
    "start": "1313110",
    "end": "1322669"
  },
  {
    "text": "Repeated random\nsubsampling is done when you have a larger set of\ndata, and there you might say",
    "start": "1322670",
    "end": "1330860"
  },
  {
    "text": "split your data 80/20. Take 80% of the data to\ntrain on, test it on 20.",
    "start": "1330860",
    "end": "1340130"
  },
  {
    "text": "So this is very similar to\nwhat I talked about earlier, and answered the\nquestion about how",
    "start": "1340130",
    "end": "1346310"
  },
  {
    "text": "to choose K. I haven't\nseen the future examples,",
    "start": "1346310",
    "end": "1352340"
  },
  {
    "text": "but in order to\nbelieve in my model and say my parameter\nsettings, I do this repeated",
    "start": "1352340",
    "end": "1358930"
  },
  {
    "text": "random subsampling or\nleave one out, either one.",
    "start": "1358930",
    "end": "1364090"
  },
  {
    "text": "There's the code\nfor leave one out.  Absolutely nothing\ninteresting about it,",
    "start": "1364090",
    "end": "1371340"
  },
  {
    "text": "so I'm not going to waste\nyour time looking at it. ",
    "start": "1371340",
    "end": "1377430"
  },
  {
    "text": "Repeated random subsampling\nis a little more interesting.",
    "start": "1377430",
    "end": "1384110"
  },
  {
    "text": "What I've done here\nis I first sample--",
    "start": "1384110",
    "end": "1390640"
  },
  {
    "text": "this one is just\nto splitted 80/20. It's not doing\nanything repeated,",
    "start": "1390640",
    "end": "1395809"
  },
  {
    "text": "and I start by sampling 20% of\nthe indices, not the samples.",
    "start": "1395810",
    "end": "1407445"
  },
  {
    "text": " And I want to do that at random. I don't want to say\nget consecutive ones.",
    "start": "1407445",
    "end": "1413654"
  },
  {
    "text": " So we do that, and then\nonce I've got the indices,",
    "start": "1413655",
    "end": "1422049"
  },
  {
    "text": "I just go through and\nassign each example, to either test or training,\nand then return the two sets.",
    "start": "1422050",
    "end": "1430560"
  },
  {
    "text": "But if I just sort\nof sampled one, then I'd have to do a\nmore complicated thing",
    "start": "1430560",
    "end": "1436480"
  },
  {
    "text": "to subtract it from the other. This is just efficiency. And then here's the--",
    "start": "1436480",
    "end": "1442370"
  },
  {
    "text": "sorry about the yellow there-- the random splits. ",
    "start": "1442370",
    "end": "1449110"
  },
  {
    "text": "Obviously, I was\nsearching for results when I did my screen capture. ",
    "start": "1449110",
    "end": "1455579"
  },
  {
    "text": "I'm just going to for\nrange and number of splits, I'm going to split it 80/20. ",
    "start": "1455579",
    "end": "1462240"
  },
  {
    "text": "It takes a parameter method,\nand that's interesting, and we'll see the\nramifications of that later.",
    "start": "1462240",
    "end": "1469800"
  },
  {
    "text": "That's going to be the\nmachine learning method. We're going to compare KNN\nto another method called",
    "start": "1469800",
    "end": "1475850"
  },
  {
    "text": "logistic regression. I didn't want to\nhave to do this code",
    "start": "1475850",
    "end": "1481160"
  },
  {
    "text": "twice, so I made the\nmethod itself a parameter. We'll see that introduces\na slight complication,",
    "start": "1481160",
    "end": "1487870"
  },
  {
    "text": "but we'll get to it\nwhen we get to it. So I split it, I apply\nwhatever that method is",
    "start": "1487870",
    "end": "1494090"
  },
  {
    "text": "the training the test\nset, I get the results,",
    "start": "1494090",
    "end": "1501039"
  },
  {
    "text": "true positive false positive,\ntrue negative false negatives. And then I call this\nthing get stats,",
    "start": "1501040",
    "end": "1508210"
  },
  {
    "text": "but I'm dividing it by\nthe number of splits, so that will give me\nthe average number",
    "start": "1508210",
    "end": "1513580"
  },
  {
    "text": "of true positives, the average\nnumber of false positives, etc. And then I'm just going\nto return the average.",
    "start": "1513580",
    "end": "1522340"
  },
  {
    "text": "Get stats actually just prints\na bunch of statistics for us.",
    "start": "1522340",
    "end": "1527770"
  },
  {
    "text": "Any questions about\nthe two methods, leave one out versus\nrepeated random sampling? ",
    "start": "1527770",
    "end": "1538690"
  },
  {
    "text": "Let's try it for\nKNN on the Titanic. ",
    "start": "1538690",
    "end": "1545120"
  },
  {
    "text": "So I'm not going to show you\nthe code for K nearest classify.",
    "start": "1545120",
    "end": "1550400"
  },
  {
    "text": "It's in the code we uploaded. It takes four arguments\nthe training set,",
    "start": "1550400",
    "end": "1556520"
  },
  {
    "text": "the test set, the label that\nwe're trying to classify.",
    "start": "1556520",
    "end": "1561620"
  },
  {
    "text": "Are we looking for\nthe people who died? Or the people who didn't die? Are we looking for\nreptiles or not reptiles?",
    "start": "1561620",
    "end": "1567410"
  },
  {
    "text": "Or if case there\nwere six labels, which one are we\ntrying to detect? And K as in how many\nnearest neighbors?",
    "start": "1567410",
    "end": "1576470"
  },
  {
    "text": "And then it returns the true\npositives, the false positives, the true negatives, and\nthe false negatives. ",
    "start": "1576470",
    "end": "1586440"
  },
  {
    "text": "Then you'll recall we'd\nalready looked at lambda in a different context.",
    "start": "1586440",
    "end": "1592950"
  },
  {
    "text": "The issue here is K nearest\nclassify takes four arguments,",
    "start": "1592950",
    "end": "1601250"
  },
  {
    "text": "yet if we go back here, for\nexample, to random splits,",
    "start": "1601250",
    "end": "1607180"
  },
  {
    "text": "what we're seeing is I'm\ncalling the method with only two arguments.",
    "start": "1607180",
    "end": "1613639"
  },
  {
    "text": "Because after all, if I'm not\ndoing K nearest neighbors, maybe I don't need to pass\nin K. I'm sure I don't.",
    "start": "1613640",
    "end": "1622120"
  },
  {
    "text": "Different methods will\ntake different numbers of parameters, and yet I want\nto use the same function here",
    "start": "1622120",
    "end": "1629919"
  },
  {
    "text": "method. So the trick I use\nto get around that-- and this is a very common\nprogramming trick--",
    "start": "1629920",
    "end": "1637900"
  },
  {
    "text": "in math. It's called currying, after\nthe mathematician Curry, not the Indian dish.",
    "start": "1637900",
    "end": "1645990"
  },
  {
    "text": "I'm creating a function a\nnew function called KNN. This will be a function of\ntwo arguments, the training",
    "start": "1645990",
    "end": "1653580"
  },
  {
    "text": "set and the test\nset, and it will be K nearest classifier\nwith training set and test",
    "start": "1653580",
    "end": "1660240"
  },
  {
    "text": "set as variables, and\ntwo constants, survived--",
    "start": "1660240",
    "end": "1666970"
  },
  {
    "text": "so I'm going to\npredict who survived-- and 3, the K.",
    "start": "1666970",
    "end": "1673420"
  },
  {
    "text": "I've been able to turn a\nfunction of four arguments, K nearest classify, into a\nfunction of two arguments",
    "start": "1673420",
    "end": "1680139"
  },
  {
    "text": "KNN by using lambda abstraction.",
    "start": "1680140",
    "end": "1685570"
  },
  {
    "text": "This is something that\npeople do fairly frequently, because it lets you build much\nmore general programs when",
    "start": "1685570",
    "end": "1692690"
  },
  {
    "text": "you don't have to worry about\nthe number of arguments. So it's a good trick to\nkeeping your bag of tricks.",
    "start": "1692690",
    "end": "1699500"
  },
  {
    "text": "Again, it's a trick\nwe've used before. Then I've just chosen 10\nfor the number of splits,",
    "start": "1699500",
    "end": "1706740"
  },
  {
    "text": "and we'll try it, and we'll try\nit for both methods of testing.",
    "start": "1706740",
    "end": "1716850"
  },
  {
    "text": "Any questions before\nI run this code? ",
    "start": "1716850",
    "end": "1732720"
  },
  {
    "text": "So here it is. We'll run it. ",
    "start": "1732720",
    "end": "1739470"
  },
  {
    "text": "Well, I should learn how to\nspell finished, shouldn't I? But that's OK. ",
    "start": "1739470",
    "end": "1751220"
  },
  {
    "text": "Here we have the\nresults, and they're--",
    "start": "1751220",
    "end": "1756679"
  },
  {
    "text": "well, what can we\nsay about them? They're not much\ndifferent to start with,",
    "start": "1756680",
    "end": "1761750"
  },
  {
    "text": "so it doesn't appear that\nour testing methodology had much of a difference on\nhow well the KNN worked,",
    "start": "1761750",
    "end": "1769640"
  },
  {
    "text": "and that's actually\nkind of comforting. The accurate-- none of\nthe evaluation criteria",
    "start": "1769640",
    "end": "1776480"
  },
  {
    "text": "are radically different,\nso that's kind of good. We hoped that was true.",
    "start": "1776480",
    "end": "1782880"
  },
  {
    "text": "The other thing to notice\nis that we're actually doing considerably better than\njust always predicting, say,",
    "start": "1782880",
    "end": "1790040"
  },
  {
    "text": "didn't survive. ",
    "start": "1790040",
    "end": "1796070"
  },
  {
    "text": "We're doing better than\na random prediction. Let's go back now\nto the Power Point.",
    "start": "1796070",
    "end": "1801617"
  },
  {
    "start": "1801617",
    "end": "1808075"
  },
  {
    "text": "Here are the results. We don't need to\nstudy them anymore. ",
    "start": "1808075",
    "end": "1814020"
  },
  {
    "text": "Better than 62% accuracy,\nbut not much difference between the experiments.",
    "start": "1814020",
    "end": "1821490"
  },
  {
    "text": "So that's one method. Now let's look at\na different method, and this is probably\nthe most common method",
    "start": "1821490",
    "end": "1828340"
  },
  {
    "text": "used in machine learning. It's called logistic regression.",
    "start": "1828340",
    "end": "1834830"
  },
  {
    "text": "It's, in some ways, if\nyou look at it, similar to a linear regression,\nbut different",
    "start": "1834830",
    "end": "1840200"
  },
  {
    "text": "in some important ways.  Linear regression, you\nwill I'm sure recall,",
    "start": "1840200",
    "end": "1849900"
  },
  {
    "text": "is designed to\npredict a real number. ",
    "start": "1849900",
    "end": "1854920"
  },
  {
    "text": "Now what we want here\nis a probability, so",
    "start": "1854920",
    "end": "1862220"
  },
  {
    "text": "the probability of some event. We know that the dependent\nvariable can only take on a finite set of values,\nso we want to predict survived",
    "start": "1862220",
    "end": "1877020"
  },
  {
    "text": "or didn't survive. It's no good to say we predict\nthis person half survived,",
    "start": "1877020",
    "end": "1883309"
  },
  {
    "text": "you know survived, but is\nbrain dead or something. I don't know. That's not what\nwe're trying to do.",
    "start": "1883310",
    "end": "1889500"
  },
  {
    "text": "The problem with just using\nregular linear regression is a lot of time you get\nnonsense predictions.",
    "start": "1889500",
    "end": "1897240"
  },
  {
    "text": "Now you can claim,\nOK 0.5 is there, and it means has a half\nprobability of dying,",
    "start": "1897240",
    "end": "1904860"
  },
  {
    "text": "not that half died. But in fact, if you\nlook at what goes on,",
    "start": "1904860",
    "end": "1909899"
  },
  {
    "text": "you could get more\nthan one or less than 0 out of linear\nregression, and that's",
    "start": "1909900",
    "end": "1917670"
  },
  {
    "text": "nonsense when we're talking\nabout probabilities. So we need a different method,\nand that's logistic regression.",
    "start": "1917670",
    "end": "1926520"
  },
  {
    "text": "What logistic\nregression does is it finds what are called the\nweights for each feature.",
    "start": "1926520",
    "end": "1934330"
  },
  {
    "text": "You may recall I complained when\nProfessor [? Grimson ?] used the word weights to mean\nsomething somewhat different.",
    "start": "1934330",
    "end": "1942450"
  },
  {
    "text": "We take each feature, for\nexample the gender, the cabin",
    "start": "1942450",
    "end": "1947639"
  },
  {
    "text": "class, the age, and\ncompute for that weight",
    "start": "1947640",
    "end": "1957114"
  },
  {
    "text": "that we're going to use\nin making predictions. So think of the weights\nas corresponding",
    "start": "1957114",
    "end": "1962380"
  },
  {
    "text": "to the coefficients we get\nwhen we do a linear regression. So we have now a coefficient\nassociated with each variable.",
    "start": "1962380",
    "end": "1971400"
  },
  {
    "text": "We're going to take\nthose coefficients, add them up, multiply\nthem by something,",
    "start": "1971400",
    "end": "1976710"
  },
  {
    "text": "and make a prediction. A positive weight implies--",
    "start": "1976710",
    "end": "1982820"
  },
  {
    "text": "and I'll come back\nto this later-- it almost implies that\nthe variable is positively",
    "start": "1982820",
    "end": "1988530"
  },
  {
    "text": "correlated with the outcome. So we would, for\nexample, say the",
    "start": "1988530",
    "end": "1998029"
  },
  {
    "text": "have scales is\npositively correlated with being a reptile.",
    "start": "1998030",
    "end": "2004100"
  },
  {
    "text": "A negative weight implies that\nthe variable is negatively correlated with the\noutcome, so number of legs",
    "start": "2004100",
    "end": "2012649"
  },
  {
    "text": "might have a negative weight. The more legs an animal\nhas, the less likely it is to be a reptile.",
    "start": "2012650",
    "end": "2020149"
  },
  {
    "text": "It's not absolute, it's\njust a correlation.",
    "start": "2020150",
    "end": "2027020"
  },
  {
    "text": "The absolute\nmagnitude is related to the strength of\nthe correlation,",
    "start": "2027020",
    "end": "2032230"
  },
  {
    "text": "so if it's being\npositive it means it's a really strong indicator. If it's big negative,\nit's a really strong",
    "start": "2032230",
    "end": "2038460"
  },
  {
    "text": "negative indicator. ",
    "start": "2038460",
    "end": "2044149"
  },
  {
    "text": "And then we use an\noptimization process to compute these weights\nfrom the training data.",
    "start": "2044150",
    "end": "2051949"
  },
  {
    "text": "It's a little bit complex. It's key is the way it uses\nthe log function, hence",
    "start": "2051949",
    "end": "2057109"
  },
  {
    "text": "the name logistic, but I'm not\ngoing to make you look at it. ",
    "start": "2057110",
    "end": "2064270"
  },
  {
    "text": "But I will show\nyou how to use it. You start by importing something\ncalled sklearn.linear_model.",
    "start": "2064270",
    "end": "2071804"
  },
  {
    "text": " Sklearn is a Python library,\nand in that is a class",
    "start": "2071805",
    "end": "2082299"
  },
  {
    "text": "called logistic regression. It's the name of a\nclass, and here are",
    "start": "2082300",
    "end": "2087329"
  },
  {
    "text": "three methods of that class. Fit, which takes a\nsequence of feature vectors",
    "start": "2087330",
    "end": "2096610"
  },
  {
    "text": "and a sequence of\nlabels and returns an object of type\nlogistic regression.",
    "start": "2096610",
    "end": "2105180"
  },
  {
    "text": "So this is the place where\nthe optimization is done. Now all the examples\nI'm going to show you,",
    "start": "2105180",
    "end": "2113230"
  },
  {
    "text": "these two sequences will be-- well all right. So think of this as the\nsequence of feature vectors,",
    "start": "2113230",
    "end": "2120859"
  },
  {
    "text": "one per passenger, and the\nlabels associated with those.",
    "start": "2120860",
    "end": "2125870"
  },
  {
    "text": "So this and this have\nto be the same length. ",
    "start": "2125870",
    "end": "2133280"
  },
  {
    "text": "That produces an\nobject of this type, and then I can ask for\nthe coefficients, which",
    "start": "2133280",
    "end": "2141990"
  },
  {
    "text": "will return the weight of\neach variable, each feature.",
    "start": "2141990",
    "end": "2147350"
  },
  {
    "text": "And then I can\nmake a prediction, given a feature vector\nreturned the probabilities",
    "start": "2147350",
    "end": "2155040"
  },
  {
    "text": "of different labels. Let's look at it as an example.",
    "start": "2155040",
    "end": "2162550"
  },
  {
    "text": "So first let's build the model.  To build the model, we'll take\nthe examples, the training",
    "start": "2162550",
    "end": "2169690"
  },
  {
    "text": "data, and I just said whether\nwe're going to print something. You'll notice from\nthis slide I've",
    "start": "2169690",
    "end": "2175599"
  },
  {
    "text": "elighted the printed stuff. We'll come back in a later slide\nand look at what's in there.",
    "start": "2175600",
    "end": "2182090"
  },
  {
    "text": "But for now I want to focus on\nactually building the model. ",
    "start": "2182090",
    "end": "2188160"
  },
  {
    "text": "I need to create two vectors,\ntwo lists in this case, the feature vectors\nand the labels.",
    "start": "2188160",
    "end": "2194940"
  },
  {
    "text": "For e in examples,\nfeaturevectors.a ppend(e.getfeatures\ne.getfeatures e.getlabel.",
    "start": "2194940",
    "end": "2200870"
  },
  {
    "text": "Couldn't be much\nsimpler than that. Then, just because it wouldn't\nfit on a line on my slide,",
    "start": "2200870",
    "end": "2210360"
  },
  {
    "text": "I've created this\nidentifier called logistic regression,\nwhich is sklearn.linearmo",
    "start": "2210360",
    "end": "2216494"
  },
  {
    "text": "del.logisticregression. So this is the thing I\nimported, and this is a class,",
    "start": "2216495",
    "end": "2224340"
  },
  {
    "text": "and now I'll get\na model by first creating an instance of the\nclass, logistic regression.",
    "start": "2224340",
    "end": "2230670"
  },
  {
    "text": "Here I'm getting an\ninstance, and then I'll call dot fit with\nthat instance, passing",
    "start": "2230670",
    "end": "2236730"
  },
  {
    "text": "it feature vecs and labels. I now have built a\nlogistic regression model, which is simply\na set of weights",
    "start": "2236730",
    "end": "2245260"
  },
  {
    "text": "for each of the variables. This makes sense? ",
    "start": "2245260",
    "end": "2252770"
  },
  {
    "text": "Now we're going to\napply the model, and I think this is the\nlast piece of Python",
    "start": "2252770",
    "end": "2259040"
  },
  {
    "text": "I'm going to introduce this\nsemester, in case you're tired of learning about Python.",
    "start": "2259040",
    "end": "2264619"
  },
  {
    "text": "And this is at least\nlist comprehension. This is how I'm going to build\nmy set of test feature vectors.",
    "start": "2264620",
    "end": "2273140"
  },
  {
    "text": "So before we go and\nlook at the code, let's look at how list\ncomprehension works.",
    "start": "2273140",
    "end": "2280690"
  },
  {
    "text": "In its simplest form,\nsays some expression for some identifier\nin some list,",
    "start": "2280690",
    "end": "2286839"
  },
  {
    "text": "L. It creates a new list by\nevaluating this expression Len",
    "start": "2286840",
    "end": "2294235"
  },
  {
    "text": "(L) times with the ID in\nthe expression replaced",
    "start": "2294235",
    "end": "2299860"
  },
  {
    "text": "by each element of\nthe list L. So let's look at a simple example.",
    "start": "2299860",
    "end": "2305500"
  },
  {
    "text": "Here I'm saying L equals x\ntimes x for x in range 10.",
    "start": "2305500",
    "end": "2312150"
  },
  {
    "text": "What's that going to do? It's going to,\nessentially, create a list.",
    "start": "2312150",
    "end": "2317654"
  },
  {
    "text": "Think of it as a\nlist, or at least a sequence of values, a range\ntype actually in Python 3--",
    "start": "2317654",
    "end": "2323620"
  },
  {
    "text": "of values 0 to 9. It will then create a\nlist of length 10, where",
    "start": "2323620",
    "end": "2331079"
  },
  {
    "text": "the first element is\ngoing to be 0 times 0. The second element\n1 times 1, etc.",
    "start": "2331080",
    "end": "2338630"
  },
  {
    "text": "OK? So it's a simple\nway for me to create a list that looks like that.",
    "start": "2338630",
    "end": "2345030"
  },
  {
    "text": "I can be fancier and say for x\ntimes L equals x times x for x",
    "start": "2345030",
    "end": "2352800"
  },
  {
    "text": "in range 10, and I add and if. If x mod 2 is equal to 0.",
    "start": "2352800",
    "end": "2360080"
  },
  {
    "text": "Now instead of returning all-- building a list using\neach value in range 10,",
    "start": "2360080",
    "end": "2365880"
  },
  {
    "text": "it will use only those values\nthat satisfy that test. ",
    "start": "2365880",
    "end": "2374880"
  },
  {
    "text": "We can go look at what\nhappens when we run that code. ",
    "start": "2374880",
    "end": "2391700"
  },
  {
    "text": "You can see the first\nlist is 1 times 1, 2 times 2, et cetera, and\nthe second list",
    "start": "2391700",
    "end": "2397100"
  },
  {
    "text": "is much shorter, because I'm\nonly squaring even numbers. ",
    "start": "2397100",
    "end": "2407060"
  },
  {
    "text": "Well, you can see that\nlist comprehension gives us a convenient compact way to\ndo certain kinds of things.",
    "start": "2407060",
    "end": "2413940"
  },
  {
    "text": "Like lambda expressions,\nthey're easy to misuse.",
    "start": "2413940",
    "end": "2419460"
  },
  {
    "text": "I hate reading code where I\nhave list comprehensions that go over multiple lines on\nmy screen, for example.",
    "start": "2419460",
    "end": "2426060"
  },
  {
    "text": "So I use it quite a lot\nfor small things like this. If it's very large, I\nfind another way to do it.",
    "start": "2426060",
    "end": "2433110"
  },
  {
    "start": "2433110",
    "end": "2448410"
  },
  {
    "text": "Now we can move forward. ",
    "start": "2448410",
    "end": "2458790"
  },
  {
    "text": "In applying the model, I\nfirst build my testing feature of x, my e.getfeatures\nfor e in test set,",
    "start": "2458790",
    "end": "2467160"
  },
  {
    "text": "so that will give me\nthe features associated with each element\nin the test set. I could obviously have written\na for loop to do the same thing,",
    "start": "2467160",
    "end": "2474930"
  },
  {
    "text": "but this was just\na little cooler. Then we get model.predict\nfor each of these.",
    "start": "2474930",
    "end": "2482690"
  },
  {
    "text": "Model.predict_proba is nice in\nthat I don't have to predict it",
    "start": "2482690",
    "end": "2488119"
  },
  {
    "text": "for one example at a time. I can pass it as set of\nexamples, and what I get back",
    "start": "2488120",
    "end": "2493880"
  },
  {
    "text": "is a list of predictions,\nso that's just convenient.",
    "start": "2493880",
    "end": "2502890"
  },
  {
    "text": "And then setting these to 0,\nand for I in range len of probs,",
    "start": "2502890",
    "end": "2510420"
  },
  {
    "text": "here a probability of 0.5. What's that's saying is what I\nget out of logistic regression",
    "start": "2510420",
    "end": "2520200"
  },
  {
    "text": "is a probability of\nsomething having a label. I then have to build a\nclassifier, give a threshold.",
    "start": "2520200",
    "end": "2528950"
  },
  {
    "text": "And here what I've said, if the\nprobability of it being true is over a 0.5, call it true.",
    "start": "2528950",
    "end": "2534890"
  },
  {
    "text": "So if the probability\nof survival is over 0.5, call it survived. If it's below, call\nit not survived.",
    "start": "2534890",
    "end": "2542600"
  },
  {
    "text": "We'll later see that, again,\nsetting that probability is itself an interesting thing,\nbut the default in most systems",
    "start": "2542600",
    "end": "2551630"
  },
  {
    "text": "is half, for obvious reasons. ",
    "start": "2551630",
    "end": "2558279"
  },
  {
    "text": "I get my probabilities\nfor each feature vector, and then for I in ranged\nlens of probabilities,",
    "start": "2558280",
    "end": "2564820"
  },
  {
    "text": "I'm just testing whether\nthe predicted label is the same as the actual label,\nand updating true positives,",
    "start": "2564820",
    "end": "2574000"
  },
  {
    "text": "false positives, true\nnegatives, and false negatives accordingly.",
    "start": "2574000",
    "end": "2579517"
  },
  {
    "text": "So far, so good? ",
    "start": "2579518",
    "end": "2585859"
  },
  {
    "text": "All right, let's\nput it all together. I'm defining something called\nLR, for logistic regression.",
    "start": "2585860",
    "end": "2593224"
  },
  {
    "text": "It takes the training data,\nthe test data, the probability, it builds a model, and\nthen it gets the results",
    "start": "2593225",
    "end": "2601810"
  },
  {
    "text": "by calling apply model\nwith the label survived and whatever this prob was.",
    "start": "2601810",
    "end": "2607840"
  },
  {
    "text": "Again, we'll do it\nfor both leave one out and random splits, and\nagain for 10 random splits.",
    "start": "2607840",
    "end": "2614950"
  },
  {
    "start": "2614950",
    "end": "2643790"
  },
  {
    "text": "You'll notice it actually runs-- maybe you won't notice, but\nit does run faster than KNN.",
    "start": "2643790",
    "end": "2650700"
  },
  {
    "text": "One of the nice things\nabout logistic regression is building the\nmodel takes a while,",
    "start": "2650700",
    "end": "2656010"
  },
  {
    "text": "but once you've got\nthe model, applying it to a large number of variables--\nfeature vectors is fast.",
    "start": "2656010",
    "end": "2663660"
  },
  {
    "text": "It's independent of the\nnumber of training examples, because we've got our weights.",
    "start": "2663660",
    "end": "2669000"
  },
  {
    "text": "So solving the optimization\nproblem, getting the weights, depends upon the number\nof training examples.",
    "start": "2669000",
    "end": "2675180"
  },
  {
    "text": "Once we've got the weights, it's\njust evaluating a polynomial. It's very fast, so\nthat's a nice advantage.",
    "start": "2675180",
    "end": "2682986"
  },
  {
    "text": " If we look at those-- ",
    "start": "2682986",
    "end": "2695170"
  },
  {
    "text": "and we should probably compare\nthem to our earlier KNN results, so KNN on the\nleft, logistic regression",
    "start": "2695170",
    "end": "2704560"
  },
  {
    "text": "on the right. And I guess if I look at it, it\nlooks like logistic regression",
    "start": "2704560",
    "end": "2712000"
  },
  {
    "text": "did a little bit better. ",
    "start": "2712000",
    "end": "2718100"
  },
  {
    "text": "That's not guaranteed,\nbut it often does outperform because it's\nmore subtle in what it does,",
    "start": "2718100",
    "end": "2725172"
  },
  {
    "text": "in being able to assign\ndifferent weights to different variables.",
    "start": "2725172",
    "end": "2730329"
  },
  {
    "text": "It's a little bit better. That's probably a good\nthing, but there's",
    "start": "2730330",
    "end": "2736799"
  },
  {
    "text": "another reason that's really\nimportant that people prefer logistic regression,\nis it provides",
    "start": "2736800",
    "end": "2742680"
  },
  {
    "text": "insights about the variables. We can look at the\nfeature weights.",
    "start": "2742680",
    "end": "2748245"
  },
  {
    "text": " This code does that, so remember\nwe looked at build model",
    "start": "2748245",
    "end": "2756130"
  },
  {
    "text": "and I left out the printing? Well here I'm leaving out\neverything except the printing.",
    "start": "2756130",
    "end": "2761630"
  },
  {
    "text": "Same function, but leaving out\neverything except the printing. ",
    "start": "2761630",
    "end": "2767410"
  },
  {
    "text": "We can do model\nunderbar classes, so model.classes underbar\ngives you the classes.",
    "start": "2767410",
    "end": "2776110"
  },
  {
    "text": "In this case, the classes\nare survived, didn't survive. I forget what I called it. We'll see.",
    "start": "2776110",
    "end": "2782200"
  },
  {
    "text": "So I can see what the\nclasses it's using are, and then for I in range\nlen model dot cof underbar,",
    "start": "2782200",
    "end": "2790510"
  },
  {
    "text": "these are giving the\nweights of each variable. The coefficients, I can\nprint what they are.",
    "start": "2790510",
    "end": "2796656"
  },
  {
    "text": " So let's run that\nand see what we get. ",
    "start": "2796656",
    "end": "2807890"
  },
  {
    "text": "We get a syntax error\nbecause I turned a comment into a line of code. ",
    "start": "2807890",
    "end": "2823319"
  },
  {
    "text": "Our model classes are\ndied and survived,",
    "start": "2823320",
    "end": "2828650"
  },
  {
    "text": "and for label survived-- what I've done, by the\nway, in the representation",
    "start": "2828650",
    "end": "2835099"
  },
  {
    "text": "is I represented the cabin\nclass as a binary variable. It's either 0 or 1, because\nit doesn't make sense",
    "start": "2835100",
    "end": "2842600"
  },
  {
    "text": "to treat them as if they were\nreally numbers because we don't know, for example,\nthe difference",
    "start": "2842600",
    "end": "2848270"
  },
  {
    "text": "between first and second is\nthe same as the difference between second and third. If we treated the class,\nwe just said cabin class",
    "start": "2848270",
    "end": "2855570"
  },
  {
    "text": "and used an integer, implicitly\nthe learning algorithm is going to assume that the\ndifference between 1 and 2",
    "start": "2855570",
    "end": "2862250"
  },
  {
    "text": "is the same as between 2 and 3. If you, for example, look at\nthe prices of these cabins,",
    "start": "2862250",
    "end": "2867320"
  },
  {
    "text": "you'll see that that's not true. The difference in an\nairplane between economy plus",
    "start": "2867320",
    "end": "2873119"
  },
  {
    "text": "and economy is way smaller than\nbetween economy plus him first. Same thing on the Titanic.",
    "start": "2873120",
    "end": "2880840"
  },
  {
    "text": "But what we see here is\nthat for the label survived,",
    "start": "2880840",
    "end": "2886060"
  },
  {
    "text": "pretty good sized\npositive weight for being in first class cabin. ",
    "start": "2886060",
    "end": "2893000"
  },
  {
    "text": "Moderate for being\nin the second, and if you're in the third\nclass well, tough luck.",
    "start": "2893000",
    "end": "2898130"
  },
  {
    "text": "So what we see here is\nthat rich people did better than the poor people. Shocking.",
    "start": "2898130",
    "end": "2905135"
  },
  {
    "text": "If We look at age, we'll see\nit's negatively correlated. What does this mean?",
    "start": "2905135",
    "end": "2912010"
  },
  {
    "text": "It's not a huge weight,\nbut it basically says that if you're older,\nthe bigger your age,",
    "start": "2912010",
    "end": "2919780"
  },
  {
    "text": "the less likely you are to\nhave survived the disaster. And finally, it\nsays it's really bad",
    "start": "2919780",
    "end": "2927860"
  },
  {
    "text": "to be a male, that the men-- being a male was very negatively\ncorrelated with surviving.",
    "start": "2927860",
    "end": "2937040"
  },
  {
    "text": "We see a nice thing here is\nwe get these labels, which we can make sense of.",
    "start": "2937040",
    "end": "2943040"
  },
  {
    "text": "One more slide\nand then I'm done. ",
    "start": "2943040",
    "end": "2949890"
  },
  {
    "text": "These values are\nslightly different, because different randomization,\ndifferent example,",
    "start": "2949890",
    "end": "2955270"
  },
  {
    "text": "but the main point\nI want to say is you have to be a little\nbit wary of reading too much into these weights.",
    "start": "2955270",
    "end": "2962290"
  },
  {
    "text": "Because not in this example,\nbut other examples-- well, also in these features\nare often correlated,",
    "start": "2962290",
    "end": "2970580"
  },
  {
    "text": "and if they're\ncorrelated, you run--",
    "start": "2970580",
    "end": "2976210"
  },
  {
    "text": "actually it's 3:56. I'm going to explain the\nproblem with this on Monday when I have time\nto do it properly.",
    "start": "2976210",
    "end": "2982900"
  },
  {
    "text": "So I'll see you then. ",
    "start": "2982900",
    "end": "2993027"
  }
]