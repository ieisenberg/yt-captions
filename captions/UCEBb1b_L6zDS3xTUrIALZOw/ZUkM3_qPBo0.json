[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6029"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6030",
    "end": "12690"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "12690",
    "end": "17904"
  },
  {
    "text": " DUANE BONING: OK,\nso last time we",
    "start": "17904",
    "end": "25810"
  },
  {
    "text": "continued with our discussion\nof design of experiments and especially looking at\nfractional factorial designs,",
    "start": "25810",
    "end": "32830"
  },
  {
    "text": "some of the aliasing\npatterns that come up, and how that interplays\nwith model construction,",
    "start": "32830",
    "end": "39309"
  },
  {
    "text": "in particular, what terms\nof a model you can include, what you can't\ninclude, as well as",
    "start": "39310",
    "end": "44770"
  },
  {
    "text": "a few ideas on different\nkinds of patterns, things like the central\ncomposite pattern as well as",
    "start": "44770",
    "end": "51790"
  },
  {
    "text": "fractional or full factorial. What I want to do today is\npick up a little bit more on response surface\nmodeling, or RSM.",
    "start": "51790",
    "end": "60130"
  },
  {
    "text": "We've already touched\non some of these, but there's a couple of\nthings I've alluded to, but we haven't really\nshown you, things",
    "start": "60130",
    "end": "66310"
  },
  {
    "text": "like how one gets confidence\nintervals on the estimates of coefficients in the model.",
    "start": "66310",
    "end": "72400"
  },
  {
    "text": "Just like when we were\ndoing some estimation of statistical\ndistributions, we would say we want more than just\nan estimate of the mean",
    "start": "72400",
    "end": "80979"
  },
  {
    "text": "or an estimate of the\nvariance of a process. We would like to know what range\nwe might say 90% of the time",
    "start": "80980",
    "end": "87640"
  },
  {
    "text": "with 90% confidence, we think\nthe true mean or true variance lies. Similarly, when we're fitting\nmodels and model coefficients,",
    "start": "87640",
    "end": "96040"
  },
  {
    "text": "we'd like some\nnotion of what range we think the true model\ncoefficients likely lie",
    "start": "96040",
    "end": "102160"
  },
  {
    "text": "based on the data that we have. So I want to go over\nthat a little bit. And then we'll start talking\nabout using these models",
    "start": "102160",
    "end": "109180"
  },
  {
    "text": "for process optimization,\nso combining a little bit of the\nresponse surface methodology",
    "start": "109180",
    "end": "116979"
  },
  {
    "text": "with design of experiments\nboth in sequential fashion and in iterative fashion,\nwhere one might adapt",
    "start": "116980",
    "end": "124659"
  },
  {
    "text": "the model on the fly or based\non the additional experiments in order to drive the process\nor try to seek out and find",
    "start": "124660",
    "end": "133360"
  },
  {
    "text": "an optimum in the process. So that's the plan. I've noted here a\nreading assignment.",
    "start": "133360",
    "end": "141430"
  },
  {
    "text": "You can read all of chapter 8. It's actually interesting,\nbut what I'm mostly focused on",
    "start": "141430",
    "end": "146890"
  },
  {
    "text": "are the first three\nsections in May and Spanos, which talks about\nprocess modeling.",
    "start": "146890",
    "end": "152240"
  },
  {
    "text": "So it's covering a\nlot of the material here on response surface models,\nmodel fitting, a little bit",
    "start": "152240",
    "end": "158980"
  },
  {
    "text": "of regression, and\nthen also using these things for optimization. So a couple of chapters\nthat have a little bit more",
    "start": "158980",
    "end": "165340"
  },
  {
    "text": "advanced material on\nprincipal component analysis, which we may come back\nto a little bit later.",
    "start": "165340",
    "end": "172849"
  },
  {
    "text": "OK, so that's the plan. ",
    "start": "172850",
    "end": "178380"
  },
  {
    "text": "Here's a list of some of the\nfundamentals of regression. When we were talking\nabout fractional factorial",
    "start": "178380",
    "end": "184685"
  },
  {
    "text": "and factorial design, especially\nthose formed out of contrast,",
    "start": "184685",
    "end": "191180"
  },
  {
    "text": "that simplified method\nusing differences in different\ncollections of the data,",
    "start": "191180",
    "end": "198659"
  },
  {
    "text": "we found that those\nwere very useful, quick ways to be able to\nestimate model effects,",
    "start": "198660",
    "end": "205189"
  },
  {
    "text": "to fill those into ANOVA tables\nand decide if those effects are significant, and then\nalso the relationship",
    "start": "205190",
    "end": "212210"
  },
  {
    "text": "of those for model\ncoefficient estimation. I want to talk a\nlittle bit about",
    "start": "212210",
    "end": "220349"
  },
  {
    "text": "the alternative\nperspective, which is regression as a way for\nfitting those coefficients.",
    "start": "220350",
    "end": "227130"
  },
  {
    "text": "And we've already\ndone some of that. What I'm going to\nillustrate here is our basic assumption\nand what falls out",
    "start": "227130",
    "end": "235860"
  },
  {
    "text": "of using minimization of\nleast square or squared error",
    "start": "235860",
    "end": "241050"
  },
  {
    "text": "estimates in order to\nfit the coefficients or estimate the\ncoefficients in a model.",
    "start": "241050",
    "end": "247260"
  },
  {
    "text": "And I want to talk a little\nbit more about estimation.",
    "start": "247260",
    "end": "252569"
  },
  {
    "text": "We've already\ntouched on estimation using the normal equations. But especially I want to talk\nabout the variance again,",
    "start": "252570",
    "end": "259500"
  },
  {
    "text": "in these coefficients, things\nlike the confidence intervals for fitting of coefficients.",
    "start": "259500",
    "end": "266919"
  },
  {
    "text": "I'm going to do this here mostly\nin the context of a simplified",
    "start": "266920",
    "end": "271930"
  },
  {
    "text": "perspective, a one\nparameter model. I just have one\ninput and one output.",
    "start": "271930",
    "end": "277240"
  },
  {
    "text": "And we'll do the simplest model. We'll build it up to\na simple linear model, but all of these ideas\nalso carry through",
    "start": "277240",
    "end": "284410"
  },
  {
    "text": "for polynomial regression\nwhen I've got multiple inputs. But I think it's a\nlittle bit easier",
    "start": "284410",
    "end": "290860"
  },
  {
    "text": "to see and discuss in the\ncontext of a simplified-- a simplified model.",
    "start": "290860",
    "end": "296660"
  },
  {
    "text": "And we also talked\nlast time a bit-- the last couple of\ntimes about lack of fit.",
    "start": "296660",
    "end": "301780"
  },
  {
    "text": "And I have a little\nexample that carries us through the development of a\nmodel looking for lack of fit",
    "start": "301780",
    "end": "308440"
  },
  {
    "text": "or seeing lack of fit\nand extending the model. So it's got a small\nexample embedded in here.",
    "start": "308440",
    "end": "316639"
  },
  {
    "text": "In fact, that\nsmall example might look familiar to those of\nyou that saw or took 2853.",
    "start": "316640",
    "end": "324790"
  },
  {
    "text": "It's actually the\nsame model that I described in a very condensed\nlecture there on regression.",
    "start": "324790",
    "end": "332350"
  },
  {
    "text": " It's also important,\nI think, for us to get",
    "start": "332350",
    "end": "339640"
  },
  {
    "text": "a little bit of terminology. You've probably\nrun into measures of moral goodness, an overall\nsummary measure of R-squared",
    "start": "339640",
    "end": "350470"
  },
  {
    "text": "that is an attempt to\ncapture how good the model is in describing what's\ngoing on with your data.",
    "start": "350470",
    "end": "357980"
  },
  {
    "text": "So once one is done, the ANOVA\nanalysis it's actually quite easy to calculate both the\ngoodness of fit R-squared",
    "start": "357980",
    "end": "366849"
  },
  {
    "text": "and the adjusted R-squared as\nshown here because they both depend on--",
    "start": "366850",
    "end": "372850"
  },
  {
    "text": "both of these R-squared\nmeasures and the ANOVA look at the amount of\nvariation in your data",
    "start": "372850",
    "end": "382270"
  },
  {
    "text": "and the amount of variation\nexpressed in your model and use those to summarize\nhow good the model is.",
    "start": "382270",
    "end": "390070"
  },
  {
    "text": "So the first measure,\nthis R-squared, is basically just\nlooking and saying",
    "start": "390070",
    "end": "395320"
  },
  {
    "text": "if I were to simply model\nmy output as the mean, how",
    "start": "395320",
    "end": "403170"
  },
  {
    "text": "much better does a model that\nhas more than the mean in it do in explaining the data?",
    "start": "403170",
    "end": "410169"
  },
  {
    "text": "So essentially\nwhat we do is look at the sum of squared\ndeviations around the mean.",
    "start": "410170",
    "end": "417260"
  },
  {
    "text": "So this is total summit squared\ndeviations around the mean. And then we say OK,\nhow much sum of squared",
    "start": "417260",
    "end": "425700"
  },
  {
    "text": "deviations based on the\nmodel, so the amount explained",
    "start": "425700",
    "end": "437860"
  },
  {
    "text": "in the model, compared\nto the total deviations around the mean? What fraction of those\nis captured in the model?",
    "start": "437860",
    "end": "448780"
  },
  {
    "text": "So in other words,\nif there's really nothing going on except a\nflat dependency, that is,",
    "start": "448780",
    "end": "456940"
  },
  {
    "text": "there is no slope with x. As I vary x, nothing\nchanges in the model,",
    "start": "456940",
    "end": "462060"
  },
  {
    "text": "then this simplified\nnotion of R-squared is basically saying there\nis no dependency on x.",
    "start": "462060",
    "end": "469570"
  },
  {
    "text": "And therefore, I'm\ngoing to explain with the model\nessentially nothing.",
    "start": "469570",
    "end": "474850"
  },
  {
    "text": "Now it's funny because\nwe are ignoring the fact that you might also be\nfitting the mean value.",
    "start": "474850",
    "end": "481420"
  },
  {
    "text": "But the notion captured in\nthe R-squared is dependence on the input, dependence on x.",
    "start": "481420",
    "end": "488650"
  },
  {
    "text": "Now the big gotcha with\nthis simple measure is I can always add\nmodel coefficients",
    "start": "488650",
    "end": "496450"
  },
  {
    "text": "and fit more of my\ndata, or at least I can do that ignoring\nreplication in the model.",
    "start": "496450",
    "end": "505460"
  },
  {
    "text": "For Example, we saw with a,\nsay a two input, one output",
    "start": "505460",
    "end": "510979"
  },
  {
    "text": "model and a full factorial, If\nI just have those four corner points, I can fit up\nto a second order model",
    "start": "510980",
    "end": "519169"
  },
  {
    "text": "with the interaction terms. If I have four data\npoints, I could fit the mean, first order,\nfirst order, and interaction",
    "start": "519169",
    "end": "525350"
  },
  {
    "text": "with exactly four coefficients. And in that case, what\nwould the R-squared",
    "start": "525350",
    "end": "530509"
  },
  {
    "text": "be if I put my data with\nall four coefficients?",
    "start": "530510",
    "end": "535920"
  },
  {
    "text": "1. I would fit the data perfectly. Again, this is\nwithout replication. And I would fit\nthe data perfectly.",
    "start": "535920",
    "end": "542790"
  },
  {
    "text": " And therefore I'd have\nan R-squared of 1.",
    "start": "542790",
    "end": "547940"
  },
  {
    "text": "Now is that really\na perfect model? Well, kind of, but\nwhat you've done",
    "start": "547940",
    "end": "555550"
  },
  {
    "text": "is you've used all of\nthe degrees of freedom in the data to actually fit\nor use them to fit the model.",
    "start": "555550",
    "end": "562060"
  },
  {
    "text": "We also don't have any notion of\nreplication, which isn't really",
    "start": "562060",
    "end": "568029"
  },
  {
    "text": "completely captured. So one way of\npenalizing ourselves",
    "start": "568030",
    "end": "573399"
  },
  {
    "text": "for the use of these\nadditional model terms is to essentially have a\ndifferent perspective referred",
    "start": "573400",
    "end": "582339"
  },
  {
    "text": "to as the adjusted\nR-squared, which essentially looks at the residual data.",
    "start": "582340",
    "end": "588580"
  },
  {
    "text": "Rather than the deviations\ncaptured by the model, it's looking at OK,\nwhat deviations are not",
    "start": "588580",
    "end": "595209"
  },
  {
    "text": "captured by the model? What residual data\nwould I have, which also has a side effect of\nessentially penalizing us",
    "start": "595210",
    "end": "603010"
  },
  {
    "text": "for the use of additional\nmodel coefficients because we use up\ndegrees of freedom",
    "start": "603010",
    "end": "609910"
  },
  {
    "text": "in the model when we\nadd model coefficients.",
    "start": "609910",
    "end": "615550"
  },
  {
    "text": "So very often people talk\nabout the adjusted R square as this fair comparison\nbetween models, especially",
    "start": "615550",
    "end": "621640"
  },
  {
    "text": "between models where I may have\na simplified model with fewer coefficients and a more\ncomplicated model with more",
    "start": "621640",
    "end": "627820"
  },
  {
    "text": "coefficients. And essentially what\nwe do is form it as the ratio of the mean\nsquare error of the residuals",
    "start": "627820",
    "end": "637480"
  },
  {
    "text": "over the total mean square\nvariance, if you will, captured by deviations\naround the mean",
    "start": "637480",
    "end": "645250"
  },
  {
    "text": "and then subtract\nthat all from 1. And the way I like to think\nabout it is essentially,",
    "start": "645250",
    "end": "650770"
  },
  {
    "text": "I start with the perfect model. And then any\nresidual error, which",
    "start": "650770",
    "end": "657550"
  },
  {
    "text": "could include both replication\nerror and lack of fit error,",
    "start": "657550",
    "end": "663250"
  },
  {
    "text": "whatever percentage error that\nI don't capture in the data--",
    "start": "663250",
    "end": "668320"
  },
  {
    "text": "the sum of squared\ndeviations divided by the degrees of freedom,\nthat's my mean square error",
    "start": "668320",
    "end": "675700"
  },
  {
    "text": "estimate or my estimate\nof the true total variance around the mean.",
    "start": "675700",
    "end": "682380"
  },
  {
    "text": "Whatever fraction of that\nthat is in the residual, that's what I'm not modeling.",
    "start": "682380",
    "end": "689310"
  },
  {
    "text": "So essentially what\nwe're doing is simply looking at what's not\nexpressed in the model.",
    "start": "689310",
    "end": "695520"
  },
  {
    "text": "And the model can never\ncapture pure replication error, so it's got that variance, but\nit might also have lack of it",
    "start": "695520",
    "end": "701970"
  },
  {
    "text": "in it.  So most statistical\npackages will report out",
    "start": "701970",
    "end": "709089"
  },
  {
    "text": "both of these numbers. You can also calculate them. But it's generally\nI like the R-squared",
    "start": "709090",
    "end": "717910"
  },
  {
    "text": "adjusted as a better measure. In part because it feels\nto me a little bit more",
    "start": "717910",
    "end": "724360"
  },
  {
    "text": "conceptual and comprehensive,\nin terms of telling me what's not captured\nin the model,",
    "start": "724360",
    "end": "730300"
  },
  {
    "text": "how much pure variation\ngoing on in the data",
    "start": "730300",
    "end": "735670"
  },
  {
    "text": "is not in the model. However, you have to be really\ncareful interpreting what",
    "start": "735670",
    "end": "741370"
  },
  {
    "text": "that R-squared is telling you. It's not necessarily telling you\nthat your model is good or bad.",
    "start": "741370",
    "end": "748870"
  },
  {
    "text": "You might have a perfect model\ngiven variant noise factors",
    "start": "748870",
    "end": "755160"
  },
  {
    "text": "in the model. So for example, if\nunderlying everything, I've got a true\nsystematic dependency,",
    "start": "755160",
    "end": "762640"
  },
  {
    "text": "but I also have pure\nreplication variance, that's going to limit how good\nyour R-squared can possibly",
    "start": "762640",
    "end": "769170"
  },
  {
    "text": "be even if your model were\nperfect in terms of capturing the systematic dependency.",
    "start": "769170",
    "end": "777430"
  },
  {
    "text": "I think there was a question\nlurking there in Singapore. AUDIENCE: Yes, so for\nR-squared and just R-squared",
    "start": "777430",
    "end": "787190"
  },
  {
    "text": "the closer those values are to\n1, the better of the model is?",
    "start": "787190",
    "end": "794220"
  },
  {
    "text": "DUANE BONING: Yes, definitely. AUDIENCE: OK. DUANE BONING:\nDefinitely 1 is better.",
    "start": "794220",
    "end": "799442"
  },
  {
    "text": "But you have to be\na little careful in interpreting because even-- AUDIENCE: What you just--",
    "start": "799442",
    "end": "806390"
  },
  {
    "text": "no but what, Professor,\nyou just said is the R-squared increase both\nthe error of the model, also",
    "start": "806390",
    "end": "813139"
  },
  {
    "text": "the error of the noise. So you can't really\ndifferentiate between these two. DUANE BONING: That's\nright, that's right.",
    "start": "813140",
    "end": "819860"
  },
  {
    "text": "And that's where a lack of\nfit analysis-- and we'll go in and do one of\nthose as well --is also still important for being\nable to try to differentiate",
    "start": "819860",
    "end": "827180"
  },
  {
    "text": "between those two sources of\nimperfection in the model. Yeah?",
    "start": "827180",
    "end": "833390"
  },
  {
    "text": "AUDIENCE: Also you mentioned\nthe second R-squared also being",
    "start": "833390",
    "end": "838910"
  },
  {
    "text": "[INAUDIBLE]. DUANE BONING: Right. AUDIENCE: Your\nmain concern is fit and having more\ncoefficients is cheap,",
    "start": "838910",
    "end": "847220"
  },
  {
    "text": "would you prefer R-squared\nor adjusted R-squared?",
    "start": "847220",
    "end": "852279"
  },
  {
    "text": "DUANE BONING: So the\nquestion is what would I prefer if the number of-- if\nfitting additional coefficients",
    "start": "852280",
    "end": "857829"
  },
  {
    "text": "is cheap. AUDIENCE: And fit\nis more important DUANE BONING: And fit\nis more important.",
    "start": "857830",
    "end": "863380"
  },
  {
    "text": "I think I would still\nessentially think",
    "start": "863380",
    "end": "869160"
  },
  {
    "text": "of R-squared as somewhat of a\nmore representative description of the trade off between adding\ncoefficients, improving my fit.",
    "start": "869160",
    "end": "881850"
  },
  {
    "text": "But also my R-squared\ndoesn't get as much batter. And in fact, if I\nstart overfitting,",
    "start": "881850",
    "end": "889690"
  },
  {
    "text": "it will tend to degrade\nslightly my R-squared. However, what I think\na better mechanism",
    "start": "889690",
    "end": "895930"
  },
  {
    "text": "for actually making the decision\nabout whether to include coefficients or not is\nan analysis of variance",
    "start": "895930",
    "end": "903280"
  },
  {
    "text": "and looking at the\nsignificance of those model coefficients, both the\nsignificance and the magnitude.",
    "start": "903280",
    "end": "911370"
  },
  {
    "text": "So I would tend to do more the\nregression analysis together with the ANOVA.",
    "start": "911370",
    "end": "917000"
  },
  {
    "text": "And the R-squared is a\nnice aggregate measure, but it's not the thing that\ndrives my decision-making so",
    "start": "917000",
    "end": "924279"
  },
  {
    "text": "much, so I hope that helps. So we'll see some examples\nof some R-squared that",
    "start": "924280",
    "end": "929350"
  },
  {
    "text": "come out of some analysis. Now we said that regression\nis at least as almost--",
    "start": "929350",
    "end": "938170"
  },
  {
    "text": "most commonly used is driven\nby minimization of a least--",
    "start": "938170",
    "end": "944060"
  },
  {
    "text": "or minimization of a\nsquared error measure. And this is just\ntrying to illustrate what I'm talking\nabout here with where",
    "start": "944060",
    "end": "951790"
  },
  {
    "text": "the residuals, the differences\nbetween my model and my data,",
    "start": "951790",
    "end": "957730"
  },
  {
    "text": "may come from in\nthe simple 1D case. We've already talked\na bit about this, but I'm using a very,\nvery simple model here,",
    "start": "957730",
    "end": "965180"
  },
  {
    "text": "which has only one term. It doesn't even have\na constant offset.",
    "start": "965180",
    "end": "970540"
  },
  {
    "text": "It's simply got a linear, direct\nlinear dependence of the output on the input.",
    "start": "970540",
    "end": "976209"
  },
  {
    "text": "And I'm saying\nthat the true model does have some noise in it,\nwhich is normally distributed.",
    "start": "976210",
    "end": "983600"
  },
  {
    "text": "And I'm fitting that\nor estimating that with some coefficient,\na little b.",
    "start": "983600",
    "end": "989500"
  },
  {
    "text": "And so this is my fit\nthrough my data minimizing",
    "start": "989500",
    "end": "995370"
  },
  {
    "text": "the squared\ndeviations, or I'd like to minimize the\nsquared deviations. And again, we're saying that any\ndifferences between the model",
    "start": "995370",
    "end": "1002360"
  },
  {
    "text": "prediction, essentially the\ny hat sub i minus the y sub i",
    "start": "1002360",
    "end": "1008360"
  },
  {
    "text": "for that data point,\nthat's a residual. That's an error.",
    "start": "1008360",
    "end": "1014089"
  },
  {
    "text": "And it can come from two factors\nagain, either lack of fit in the model or because of the\nunderlying noise in the data.",
    "start": "1014090",
    "end": "1024770"
  },
  {
    "text": "Now last time, or\nmaybe even 2 times ago, we talked about the use\nof regression numerically,",
    "start": "1024770",
    "end": "1031760"
  },
  {
    "text": "if you will or\nalgebraically, to estimate this beta with the best\nbe based on a minimization",
    "start": "1031760",
    "end": "1040459"
  },
  {
    "text": "of the sum of squared errors. So we take each one of\nthose residuals, square it,",
    "start": "1040460",
    "end": "1047839"
  },
  {
    "text": "and then we sum that\nover all of our data. And it turns out\nwhat we're trying to do is find the\nbeta hat, the b that",
    "start": "1047839",
    "end": "1055549"
  },
  {
    "text": "estimates the beta\nhat that minimizes that sum of squared deviations. And what's nice\nwith linear models",
    "start": "1055550",
    "end": "1063390"
  },
  {
    "text": "is there's an algebraic\nway to find actually what b",
    "start": "1063390",
    "end": "1068400"
  },
  {
    "text": "does that minimization for us. But I also want\nto just remind you",
    "start": "1068400",
    "end": "1073830"
  },
  {
    "text": "that lurking inside\nof that minimization is an estimate of the total\nsum of squared residuals, SSR,",
    "start": "1073830",
    "end": "1086880"
  },
  {
    "text": "what's lurking back there in\nthat R and R-squared adjusted.",
    "start": "1086880",
    "end": "1092280"
  },
  {
    "text": "And then if I divide\nthat out again by the degrees of\nfreedom, mu sub R,",
    "start": "1092280",
    "end": "1100559"
  },
  {
    "text": "then I've got also my estimate\nof variance in the underlying",
    "start": "1100560",
    "end": "1106370"
  },
  {
    "text": "model assuming no lack of fit. ",
    "start": "1106370",
    "end": "1112320"
  },
  {
    "text": "So we said with least\nsquares estimation, I can form the set\nof linear equations.",
    "start": "1112320",
    "end": "1119140"
  },
  {
    "text": "And assuming that the residuals\nare all normal or orthogonal to each other, then the sum\nof the product of our residual",
    "start": "1119140",
    "end": "1130679"
  },
  {
    "text": "and the input should sum to 0. And when you carry through\nthe algebra for that, out plops the formula for\nthe slope coefficient given",
    "start": "1130680",
    "end": "1141179"
  },
  {
    "text": "our data, simply the sum\nof the product of my x sub i times y sub i over the sum of\nmy x sub i squared, it's funky.",
    "start": "1141180",
    "end": "1150090"
  },
  {
    "text": "And as I said,\nhere's our estimate of the underlying variance.",
    "start": "1150090",
    "end": "1156030"
  },
  {
    "text": "That's our best estimate,\nunbiased, best estimate of the process variance.",
    "start": "1156030",
    "end": "1161820"
  },
  {
    "text": "And in this case, we're only\nfitting one model coefficient. So I've got my total\nnumber set of data",
    "start": "1161820",
    "end": "1168270"
  },
  {
    "text": "and then I've just\ngot n minus 1, since I've only got\none model coefficient.",
    "start": "1168270",
    "end": "1175750"
  },
  {
    "text": "Now the interesting\nthing that I've alluded to in a previous\nlecture but haven't",
    "start": "1175750",
    "end": "1180789"
  },
  {
    "text": "shown you is I want more than\njust the best estimate of b.",
    "start": "1180790",
    "end": "1186640"
  },
  {
    "text": "I'd like to have a\nconfidence interval on b. Given the spread in the data\nand an underlying normal noise",
    "start": "1186640",
    "end": "1195060"
  },
  {
    "text": "model or noise assumption,\nwhat do I think the range, say a 95% confidence interval,\nmight be on my estimation of b?",
    "start": "1195060",
    "end": "1204300"
  },
  {
    "text": "And we can do that very simply\nby taking the formula for b and simply doing our variance of\nb calculation on that formula.",
    "start": "1204300",
    "end": "1216990"
  },
  {
    "text": "It's just variance math. And that's what's broken\nout here in the y. If I expand out the b\nsummation into a some",
    "start": "1216990",
    "end": "1228480"
  },
  {
    "text": "of those individual terms, I can\nthen apply my normal variance math here.",
    "start": "1228480",
    "end": "1235270"
  },
  {
    "text": "And what I've got for the\nvariance of that some-- just thinking of each\nof these elements",
    "start": "1235270",
    "end": "1241500"
  },
  {
    "text": "has some constant, then that the\nvariance of that sum of terms",
    "start": "1241500",
    "end": "1246750"
  },
  {
    "text": "is the variance of\nthe constant squared-- or excuse me, the value of\nthe constant squared times",
    "start": "1246750",
    "end": "1253080"
  },
  {
    "text": "the variance of each of\nthose underlying variables. And when you go and do\nthat, what you've got",
    "start": "1253080",
    "end": "1258840"
  },
  {
    "text": "is another formula down\nhere for the variance",
    "start": "1258840",
    "end": "1264570"
  },
  {
    "text": "in that coefficient b based\non the data that you've got.",
    "start": "1264570",
    "end": "1273110"
  },
  {
    "text": "So once I've got\nthat up here, I've got my estimate\nfor the variance.",
    "start": "1273110",
    "end": "1279270"
  },
  {
    "text": "Now we've got an estimate of\nwhat 1 standard deviation would be in the variance.",
    "start": "1279270",
    "end": "1285390"
  },
  {
    "text": "And then you can express that\nbased on whatever confidence interval you want. So I might write that\ntypically as b plus or minus",
    "start": "1285390",
    "end": "1294000"
  },
  {
    "text": "1 standard error, 1\nstandard deviation in b.",
    "start": "1294000",
    "end": "1299070"
  },
  {
    "text": "1 standard deviation-- I can't remember, what that\ncorrespond to, the typical?",
    "start": "1299070",
    "end": "1304700"
  },
  {
    "text": "Got about a 90%\nconfidence interval? Plus or minus 1\nstandard deviation?",
    "start": "1304700",
    "end": "1312090"
  },
  {
    "text": "67%, thank you. The one I always remember\nis two standard errors.",
    "start": "1312090",
    "end": "1319030"
  },
  {
    "text": "That's about 95% confidence. So if you wanted to 95%\nconfidence interval, now",
    "start": "1319030",
    "end": "1326680"
  },
  {
    "text": "you know how to formulate that. It might be 1.96\nor whatever it is. ",
    "start": "1326680",
    "end": "1335690"
  },
  {
    "text": "So there you have\nnicely falling out of the basic mathematical\nformulation for minimizing",
    "start": "1335690",
    "end": "1342860"
  },
  {
    "text": "the sum of squares\nboth the best estimate for your slope and a confidence\ninterval to the slope.",
    "start": "1342860",
    "end": "1348920"
  },
  {
    "text": " By the way, if you're based that\non a relatively small number",
    "start": "1348920",
    "end": "1357529"
  },
  {
    "text": "of data points,\nyou should probably use a t distribution rather\nthan a normal distribution.",
    "start": "1357530",
    "end": "1365070"
  },
  {
    "text": "So it might change my 1.964\nfor a 95% confidence interval,",
    "start": "1365070",
    "end": "1371389"
  },
  {
    "text": "as we're used to. So this also lets us\nnow go back and do--",
    "start": "1371390",
    "end": "1377990"
  },
  {
    "text": "think again about\nanother perspective on analysis of variance.",
    "start": "1377990",
    "end": "1383231"
  },
  {
    "text": "In fact, you guys played\nwith this a little bit or saw this in a slightly\ndifferent form on the quiz.",
    "start": "1383232",
    "end": "1390919"
  },
  {
    "text": "There's two ways of thinking\nabout the significance",
    "start": "1390920",
    "end": "1397250"
  },
  {
    "text": "whether some slope coefficient\nor model coefficient should",
    "start": "1397250",
    "end": "1402740"
  },
  {
    "text": "be included in the model. The basic hypothesis\nis are we saying do I have enough evidence to\nsuggest that that slope term is",
    "start": "1402740",
    "end": "1410510"
  },
  {
    "text": "non-zero? If it might be 0 to some\ndegree of confidence,",
    "start": "1410510",
    "end": "1416990"
  },
  {
    "text": "then I shouldn't include it. So one way of doing\nit is the ANOVA",
    "start": "1416990",
    "end": "1422180"
  },
  {
    "text": "with the ratio of\nvariances in the F test. The other way is basically\nlooking at the confidence",
    "start": "1422180",
    "end": "1429440"
  },
  {
    "text": "interval for beta, say the\n95% confidence interval, and if that intersects 0,\nthat says that more than 5%",
    "start": "1429440",
    "end": "1438580"
  },
  {
    "text": "of the time based on just\nrandom variation in the data, I might have a 0\ncoefficient there,",
    "start": "1438580",
    "end": "1445490"
  },
  {
    "text": "in which case I cannot say that\nit is significantly different than 0.",
    "start": "1445490",
    "end": "1450820"
  },
  {
    "text": "So you can make\nthat determination about whether you should include\nthe model coefficient based on your confidence interval for\neach individual term as well.",
    "start": "1450820",
    "end": "1459870"
  },
  {
    "text": " So that's just alluding\nback to what we already know but just trying to make\nsure you see the connection",
    "start": "1459870",
    "end": "1468220"
  },
  {
    "text": "or alternative ways of looking\nat it either in the ANOVA table, or if you want to look\nat individual coefficient",
    "start": "1468220",
    "end": "1474880"
  },
  {
    "text": "terms, the confidence\nintervals on those individual coefficients. OK, let's do an example.",
    "start": "1474880",
    "end": "1482930"
  },
  {
    "text": "Here's a very\nsimple set of data. We've got some input, some\nx value, call that \"age\".",
    "start": "1482930",
    "end": "1490870"
  },
  {
    "text": "And some y values. Call that \"income\".",
    "start": "1490870",
    "end": "1496350"
  },
  {
    "text": "And if I just plot the data,\nlet me get the data up here-- ",
    "start": "1496350",
    "end": "1503460"
  },
  {
    "text": "actually, what I've\ndone here is used JUMP. I don't know how many of\nyou have played with JUMP,",
    "start": "1503460",
    "end": "1508770"
  },
  {
    "text": "but I love JUMP because\nit's nice and interactive. It does a lot of\nregression analysis,",
    "start": "1508770",
    "end": "1514860"
  },
  {
    "text": "lets me explore the data\nfairly interactively, I like it a lot better\nthan Excel for doing",
    "start": "1514860",
    "end": "1521549"
  },
  {
    "text": "some of these analysis. I think in an\nearlier problem set, we did give you a pointer\nto where you could run that",
    "start": "1521550",
    "end": "1527700"
  },
  {
    "text": "on Athena and so on. And what this is doing is\nbasically looking and doing",
    "start": "1527700",
    "end": "1534750"
  },
  {
    "text": "my analysis of variance for\na very simple linear model",
    "start": "1534750",
    "end": "1539820"
  },
  {
    "text": "without a constant term. So I've just got one\nmodel coefficient, looks",
    "start": "1539820",
    "end": "1545490"
  },
  {
    "text": "at the sum of squares,\nthe mean square, looks at the residual with\nthe remaining data point",
    "start": "1545490",
    "end": "1551550"
  },
  {
    "text": "forms and F. That F ratio is huge. It's 1,000, and the probability\nof observing that large of an F",
    "start": "1551550",
    "end": "1559799"
  },
  {
    "text": "is minuscule. So I have great confidence\nthat in fact there is a slope.",
    "start": "1559800",
    "end": "1566850"
  },
  {
    "text": "And if I look down here\nat my income leverage residual versus\nthe age parameter,",
    "start": "1566850",
    "end": "1574620"
  },
  {
    "text": "I can see this is basically\njust y sub i x sub i.",
    "start": "1574620",
    "end": "1581510"
  },
  {
    "text": "I see a definite trend there. Now what this nice plot\nhas done is the solid line",
    "start": "1581510",
    "end": "1590040"
  },
  {
    "text": "is my best fit, but it\nis also plotted for us with the dashed line the\nconfidence on the output.",
    "start": "1590040",
    "end": "1599810"
  },
  {
    "text": "I think it's a 95% confidence\ninterval on the output as well.",
    "start": "1599810",
    "end": "1606150"
  },
  {
    "text": "Now I told you how to get an\nestimate on the confidence interval for our b term.",
    "start": "1606150",
    "end": "1612050"
  },
  {
    "text": "How do we get a confidence\ninterval on the output term? ",
    "start": "1612050",
    "end": "1617570"
  },
  {
    "text": "Well, what we're\ngoing to need to do is also do the variance\ncalculations on our y formula",
    "start": "1617570",
    "end": "1623420"
  },
  {
    "text": "and see how\nuncertainty in our data also propagates through to\nuncertainty in our output.",
    "start": "1623420",
    "end": "1632570"
  },
  {
    "text": "But before we do\nthat, we can also see here in the\nJUMP output things like the parameter estimates\nfor our age dependence.",
    "start": "1632570",
    "end": "1643940"
  },
  {
    "text": "So here's our best guess\nfor the age dependence is a simple 0.5 estimate.",
    "start": "1643940",
    "end": "1651140"
  },
  {
    "text": "And it is also showing us\nthings like the standard error in these typical ANOVA tables,\nwhich we've ignored in the past",
    "start": "1651140",
    "end": "1658010"
  },
  {
    "text": "if you've been looking at these. But that can also be\nused them directly, as we talked about, to give\nme a confidence interval,",
    "start": "1658010",
    "end": "1664669"
  },
  {
    "text": "depending on what level of\nerror whatever level of alpha",
    "start": "1664670",
    "end": "1670010"
  },
  {
    "text": "I want to be able to\nestimate those things. And it's also looking\nat an individual t ratio",
    "start": "1670010",
    "end": "1676190"
  },
  {
    "text": "for each of the coefficients. I've only got one here,\nbut it's basically doing a one by one assessment\nof each of my model coefficients",
    "start": "1676190",
    "end": "1684620"
  },
  {
    "text": "to see if it's significant. And in fact, it's\nsignificant since it's",
    "start": "1684620",
    "end": "1691830"
  },
  {
    "text": "exactly ends up being the same\nprobability not really shown here.",
    "start": "1691830",
    "end": "1697539"
  },
  {
    "text": "Essentially, the t\ntest and the F test are identical in\nthis simple example.",
    "start": "1697540",
    "end": "1705125"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] think\nof some subset of data,",
    "start": "1705125",
    "end": "1712550"
  },
  {
    "text": "wouldn't it make sense to have\n[INAUDIBLE] part of the data then use some for testing like\nthe model and seeing if it",
    "start": "1712550",
    "end": "1721010"
  },
  {
    "text": "actually has a prediction\nbecause if you use that entire data set then essentially--",
    "start": "1721010",
    "end": "1728890"
  },
  {
    "text": "DUANE BONING: That's\nan interesting point. So what you're saying\nis how about the idea if you have a fair amount\nof data of holding out",
    "start": "1728890",
    "end": "1736180"
  },
  {
    "text": "some of the data, fitting\nthe data some portion of it, and the held back data to\nsort of test the model.",
    "start": "1736180",
    "end": "1745110"
  },
  {
    "text": "And I think, especially when\nyou do nonlinear models--",
    "start": "1745110",
    "end": "1756260"
  },
  {
    "text": "and I don't mean\njust polynomial, but I mean some other\nnonlinear dependence --that cross validation is\nextremely common and very",
    "start": "1756260",
    "end": "1764360"
  },
  {
    "text": "useful. Here, you could do that. And essentially what\nI think that's doing",
    "start": "1764360",
    "end": "1771440"
  },
  {
    "text": "is allowing you to do a lack\nof fit versus noise estimate.",
    "start": "1771440",
    "end": "1779240"
  },
  {
    "text": "In other words,\nwhat you're doing, I think conceptually,\nthere is saying here's what my model would\nhave predicted.",
    "start": "1779240",
    "end": "1785900"
  },
  {
    "text": "Here's my data point. There's a residual that I'm\ngoing to attribute maybe--",
    "start": "1785900",
    "end": "1792380"
  },
  {
    "text": "again, it's to a mix of\nrandom noise underlying but also model lack of fidelity.",
    "start": "1792380",
    "end": "1800059"
  },
  {
    "text": " I think it's more common to go\nahead and use all of your data",
    "start": "1800060",
    "end": "1809090"
  },
  {
    "text": "because then you've got\nyour aggregate measures and can run all of your tests\nwith the highest resolution",
    "start": "1809090",
    "end": "1816740"
  },
  {
    "text": "possible. But I suspect there's\nactually a relationship that's",
    "start": "1816740",
    "end": "1822440"
  },
  {
    "text": "very close in there. I think it's a little better\nto use all of the data because the more data you\nhave, the better your estimates",
    "start": "1822440",
    "end": "1830330"
  },
  {
    "text": "of underlying\nprocess variance are so you can better differentiate\nlack of fit from noise.",
    "start": "1830330",
    "end": "1837260"
  },
  {
    "text": "But I haven't thought\nabout that very much, especially of the\nsimple linear cases.",
    "start": "1837260",
    "end": "1842900"
  },
  {
    "text": "It's an interesting approach. ",
    "start": "1842900",
    "end": "1851490"
  },
  {
    "text": "So I want to come back\nto this lack of fit versus the pure error\nbecause we talked",
    "start": "1851490",
    "end": "1858660"
  },
  {
    "text": "about often being able to do\nmultiple runs at the same x values.",
    "start": "1858660",
    "end": "1863669"
  },
  {
    "text": "In this data here\nthat I've shown you, we actually have a\ndifficulty in distinguishing",
    "start": "1863670",
    "end": "1870929"
  },
  {
    "text": "between model lack of fit\nand underlying variance. I had to basically\nmake an assumption",
    "start": "1870930",
    "end": "1878130"
  },
  {
    "text": "that my underlying\nmodel was truly linear. And then I'm basically\nassuming, if I",
    "start": "1878130",
    "end": "1884940"
  },
  {
    "text": "go back even further here-- where did my data go? ",
    "start": "1884940",
    "end": "1890730"
  },
  {
    "text": "--I'm basically assuming a y\nsub i is equal to beat sub--",
    "start": "1890730",
    "end": "1896309"
  },
  {
    "text": "beta x sub i plus\nepsilon sub i model. Why not-- I have really nothing\nexcept ideas of parsimony,",
    "start": "1896310",
    "end": "1910080"
  },
  {
    "text": "simple models in general\nand perhaps prior knowledge",
    "start": "1910080",
    "end": "1916169"
  },
  {
    "text": "of the physics of the\nprocess to really say this is the form of the model. If you look at my data, why\ncouldn't my model be that?",
    "start": "1916170",
    "end": "1929280"
  },
  {
    "text": "It may well be. It might have a very\ncomplicated structure. That might be true.",
    "start": "1929280",
    "end": "1935980"
  },
  {
    "text": "The problem is I don't have-- in this random data, I\ndon't have any replicates",
    "start": "1935980",
    "end": "1944080"
  },
  {
    "text": "to be able to give me\nan independent notion of underlying repeated\nvariance noise from model form.",
    "start": "1944080",
    "end": "1954310"
  },
  {
    "text": "And so that goes\nback to what we said is if we have multiple runs at\nthe same x values, especially",
    "start": "1954310",
    "end": "1960190"
  },
  {
    "text": "if we design an experiment\nso that we do that, and we aren't using this\nsort of happenstance data,",
    "start": "1960190",
    "end": "1967630"
  },
  {
    "text": "then we can decompose\nthe total residual error into that lack of fit and\npure replicate error and start",
    "start": "1967630",
    "end": "1975460"
  },
  {
    "text": "to be able to distinguish\nbetween model structure",
    "start": "1975460",
    "end": "1980529"
  },
  {
    "text": "and and pure replication error.",
    "start": "1980530",
    "end": "1985600"
  },
  {
    "text": "And so we talked\npreviously about being able to form the F\ntest, the of variance",
    "start": "1985600",
    "end": "1991950"
  },
  {
    "text": "explained by deviations\nfrom model prediction in the replicate\ndata over total error",
    "start": "1991950",
    "end": "2001190"
  },
  {
    "text": "and then seeing how likely it\nwould be to observe that ratio and use the F test in\nthe ANOVA test for that.",
    "start": "2001190",
    "end": "2010228"
  },
  {
    "text": "And we'll come back to that\na little bit in an example. ",
    "start": "2010228",
    "end": "2016510"
  },
  {
    "text": "This is a quick one. I showed you an example here\nwhere the previous example",
    "start": "2016510",
    "end": "2021789"
  },
  {
    "text": "was a pure linear term without\neven a constant offset. We can also do models\nthat have both a slope",
    "start": "2021790",
    "end": "2030820"
  },
  {
    "text": "term and a constant term. And this is simply formulated\nhere as a means centered model.",
    "start": "2030820",
    "end": "2037540"
  },
  {
    "text": "If I were to take my data in\nand say when x was added mean,",
    "start": "2037540",
    "end": "2044170"
  },
  {
    "text": "this term would be 0. So this is not\nreally an intercept. This is saying my a coefficient\nis when x is added to mean.",
    "start": "2044170",
    "end": "2053320"
  },
  {
    "text": "I could similarly formulate\nit so that the coefficient would be when x was 0.",
    "start": "2053320",
    "end": "2061199"
  },
  {
    "text": "The point being that\nthe same approach for estimating both a linear\nterm and a constant offset term",
    "start": "2061199",
    "end": "2069360"
  },
  {
    "text": "can apply. And the same notion of not\nonly getting estimates but also",
    "start": "2069360",
    "end": "2079500"
  },
  {
    "text": "getting confidence\nintervals based on variances",
    "start": "2079500",
    "end": "2084719"
  },
  {
    "text": "in those coefficients applies. So we can also use this to\nget confidence intervals,",
    "start": "2084719",
    "end": "2093059"
  },
  {
    "text": "not only on the slope term but\nalso on the variance term-- I mean the offset term.",
    "start": "2093060",
    "end": "2098820"
  },
  {
    "text": " Now we can also, what's nice is,\ndo the same math now and look",
    "start": "2098820",
    "end": "2110369"
  },
  {
    "text": "at a variance in our\nprediction of the output. I already alluded to that with\nthese confidence intervals",
    "start": "2110370",
    "end": "2117000"
  },
  {
    "text": "on that plot of y versus\nx in that one set of data.",
    "start": "2117000",
    "end": "2123430"
  },
  {
    "text": "And if I basically am saying,\nOK, this is my best estimate-- this was my-- this is equal to the a\ncoefficient --this is my best",
    "start": "2123430",
    "end": "2131280"
  },
  {
    "text": "estimate of the\nunderlying linear model with an offset term, and I just\ndo my variance math on this,",
    "start": "2131280",
    "end": "2140080"
  },
  {
    "text": "I've got a variance of\nsome of these terms. And if you carry\nthrough that math,",
    "start": "2140080",
    "end": "2145650"
  },
  {
    "text": "this is just a constant\nat each x sub i. Since x bar is a constant,\nx sub i is a constant.",
    "start": "2145650",
    "end": "2152560"
  },
  {
    "text": "So in the variance\nmath, when I look at the variance\nof this term, it's the variance of this times\nthe variance of this.",
    "start": "2152560",
    "end": "2159405"
  },
  {
    "text": "This is a constant\nterm, so I've got that constant squared out in\nfront of the variance of my b.",
    "start": "2159405",
    "end": "2165750"
  },
  {
    "text": "We already calculated\nwhat the variance of the b a and the variance\nof the B term were. I can plug those in and\nget an overall estimate",
    "start": "2165750",
    "end": "2174000"
  },
  {
    "text": "of the variance of each of\nmy y sub i terms in my model.",
    "start": "2174000",
    "end": "2180870"
  },
  {
    "text": "And based on--\nonce I've got that for the single standard error,\nmy single standard deviation,",
    "start": "2180870",
    "end": "2186420"
  },
  {
    "text": "I can use the t or the normal\nto get a confidence interval on the output. ",
    "start": "2186420",
    "end": "2195110"
  },
  {
    "text": "So it's the same thing we\ndid on the coefficients. I can also do it to tell me what\nkind of spread, what confidence",
    "start": "2195110",
    "end": "2201940"
  },
  {
    "text": "do I have in where the true\noutput should lie when I'm predicting for\nany x value, where",
    "start": "2201940",
    "end": "2209579"
  },
  {
    "text": "I think the actual true\noutput y would lie. Now there's an\ninteresting aspect",
    "start": "2209580",
    "end": "2215220"
  },
  {
    "text": "to this, which is if I look\nfor any given x sub i input",
    "start": "2215220",
    "end": "2223869"
  },
  {
    "text": "particular x input value,\nnotice OK, that's right here.",
    "start": "2223870",
    "end": "2230240"
  },
  {
    "text": "I plug-in for my\nparticular i of interest. Notice that the denominator here\nwas a sum over all of my data.",
    "start": "2230240",
    "end": "2237359"
  },
  {
    "text": "So that ends up being\njust a constant. It doesn't change. But depending on what\nx I'm looking at,",
    "start": "2237360",
    "end": "2244160"
  },
  {
    "text": "where I am on the x, the\nsize of this changes. ",
    "start": "2244160",
    "end": "2252010"
  },
  {
    "text": "So for example, if\nI look at my mean, if I look where my x\nsub i is equal to x bar,",
    "start": "2252010",
    "end": "2259630"
  },
  {
    "text": "that numerator term goes to 0. And essentially what\nI've got in that case",
    "start": "2259630",
    "end": "2265690"
  },
  {
    "text": "is at the mean of my\ndata, my estimation is basically-- my variance\nin my output estimate",
    "start": "2265690",
    "end": "2273490"
  },
  {
    "text": "is basically just related to\nthe random noise in the data. But then as I get further\nand further from the mean,",
    "start": "2273490",
    "end": "2282609"
  },
  {
    "text": "my confidence interval\nin my output spreads. ",
    "start": "2282610",
    "end": "2287990"
  },
  {
    "text": "So what you will\noften see on data-- this was x data and\nthis is my y --is",
    "start": "2287990",
    "end": "2296360"
  },
  {
    "text": "near the center of\nyour data, you've got the narrowest\nconfidence intervals.",
    "start": "2296360",
    "end": "2302660"
  },
  {
    "text": "And as I get further\nand further away, if I were to use the dash for\na 95% confidence on the output,",
    "start": "2302660",
    "end": "2310609"
  },
  {
    "text": "the further away that I\nget in x from my x bar, the wider my prediction\nerror becomes.",
    "start": "2310610",
    "end": "2318575"
  },
  {
    "text": " Even though I'm still\nmay be interpolating over",
    "start": "2318575",
    "end": "2323840"
  },
  {
    "text": "the data I've got, my\nvariance does spread as I get further and further\naway, just an interesting fact.",
    "start": "2323840",
    "end": "2337240"
  },
  {
    "text": "All right, we're almost ready\nto do a polynomial example. I just want to point out we\ntalked about this previously.",
    "start": "2337240",
    "end": "2345130"
  },
  {
    "text": "We can also do not only\na constant term but also a linear term.",
    "start": "2345130",
    "end": "2351310"
  },
  {
    "text": "We can do terms that include\nthis square polynomial, for example, include\ncurvature in the x squared.",
    "start": "2351310",
    "end": "2358720"
  },
  {
    "text": "One important fact\nis this is still linear data in the coefficients.",
    "start": "2358720",
    "end": "2365500"
  },
  {
    "text": " And what this means is the\nleast squares approach--",
    "start": "2365500",
    "end": "2372960"
  },
  {
    "text": "least squares minimization,\nstill applies. So you can still\ndo least squares minimization to estimate\nyour beta coefficients.",
    "start": "2372960",
    "end": "2380670"
  },
  {
    "text": "And essentially what\nyou do mechanically, say in something\nlike Excel, is create",
    "start": "2380670",
    "end": "2386400"
  },
  {
    "text": "that additional fake column\nof data, just taking your x.",
    "start": "2386400",
    "end": "2391440"
  },
  {
    "text": "You can almost think of this\nas equating that with an x2, think of this as an x1, and\nbuilding your data column,",
    "start": "2391440",
    "end": "2399780"
  },
  {
    "text": "taking each of your x\ncoefficients, squaring it, and that becomes a\nnew x sub 2 input.",
    "start": "2399780",
    "end": "2406680"
  },
  {
    "text": "And then all you're\ndoing is just a linear fit now in these\nmultiple coefficients.",
    "start": "2406680",
    "end": "2412000"
  },
  {
    "text": "So it looks exactly\nthe same like we did for multiple inputs, even if\nwe have additional higher order",
    "start": "2412000",
    "end": "2419940"
  },
  {
    "text": "terms in the x squared. ",
    "start": "2419940",
    "end": "2425630"
  },
  {
    "text": "So let's look at a\nsimple example here. Pull these threads together,\nlook at confidence,",
    "start": "2425630",
    "end": "2431990"
  },
  {
    "text": "but also look at\nit in the case when I've got some replicate data so\nwe can get a little experience",
    "start": "2431990",
    "end": "2438859"
  },
  {
    "text": "with this lack of fit idea. And so in this case, we've\ngot importantly here cases",
    "start": "2438860",
    "end": "2447020"
  },
  {
    "text": "where I've replicated\nmy x values. So I've got two runs with 20\ngrams of some kind of growth",
    "start": "2447020",
    "end": "2455960"
  },
  {
    "text": "supplement. And so I've got two different\noutput values at that point. And I've got another\npoint where I've",
    "start": "2455960",
    "end": "2461810"
  },
  {
    "text": "got three replicates, triply\nreplicated set of data.",
    "start": "2461810",
    "end": "2469570"
  },
  {
    "text": "And what I'd like to do\nis try to fit a model and hear what we've\ngot in the picture is",
    "start": "2469570",
    "end": "2476500"
  },
  {
    "text": "an inkling or a foreshadowing\nof some of the kinds of models we might consider and some of\nthe issues we might consider.",
    "start": "2476500",
    "end": "2483580"
  },
  {
    "text": "If we look-- I think you can see it here\n--the basic data here in black,",
    "start": "2483580",
    "end": "2488690"
  },
  {
    "text": "these are the data points. So this is just my output. There's my triply\nreplicated data.",
    "start": "2488690",
    "end": "2494920"
  },
  {
    "text": "There Is my x data. First off, I could try\nto fit that with a mean. That's just the red line.",
    "start": "2494920",
    "end": "2500360"
  },
  {
    "text": "That's the pure just\nmean of my data. The green line here\nis a first order",
    "start": "2500360",
    "end": "2505930"
  },
  {
    "text": "fit to just a slope\ncoefficient and the mean, so two model terms.",
    "start": "2505930",
    "end": "2511720"
  },
  {
    "text": "And you can see\nalready that's not going to be a very good model. And what we've got is enough\ndata here with the replicates",
    "start": "2511720",
    "end": "2518920"
  },
  {
    "text": "to perhaps be able\nto detect that using our machinery of ANOVA, and\nthen perhaps then build that",
    "start": "2518920",
    "end": "2525340"
  },
  {
    "text": "into a second order model that\nwe can already get a sense is",
    "start": "2525340",
    "end": "2530860"
  },
  {
    "text": "going to be a quadratic\nmodel that fits the data lot that a lot better. Now, If I were to just try it--",
    "start": "2530860",
    "end": "2537610"
  },
  {
    "text": "let's say I didn't already-- first off you should always\nplot your actual data so you have a feel for\nwhat kind of a model",
    "start": "2537610",
    "end": "2544870"
  },
  {
    "text": "is going to be needed. So if you were to actually plot\nthat data, you would already you probably needed\na quadratic model.",
    "start": "2544870",
    "end": "2552670"
  },
  {
    "text": "So you might go ahead and\nup front, include that term. But let's say we had\nnot done that, we'd just",
    "start": "2552670",
    "end": "2560740"
  },
  {
    "text": "tried to fit it with\na very simple model, a simple linear model. And if we go through\nand do the ANOVA,",
    "start": "2560740",
    "end": "2566890"
  },
  {
    "text": "now because we do have\nrepeated residual, I can split my overall residual\nsum of squared deviations",
    "start": "2566890",
    "end": "2575140"
  },
  {
    "text": "into a lack of fit term. That's a sum of\nsquared deviations just from my replicated--",
    "start": "2575140",
    "end": "2582430"
  },
  {
    "text": "or my total deviation from my\nmodel from my replicated data. And I can formulate then a\nratio of those two things.",
    "start": "2582430",
    "end": "2590890"
  },
  {
    "text": "And what I've got is\ndeviations from my model that are much larger.",
    "start": "2590890",
    "end": "2597140"
  },
  {
    "text": "So this is a deviation. It's not a good one.",
    "start": "2597140",
    "end": "2602430"
  },
  {
    "text": "Actually right, there the\ndeviation from the model is quite small.",
    "start": "2602430",
    "end": "2607740"
  },
  {
    "text": "If I were to look right\nhere, for example, this is my deviation\nfrom the model.",
    "start": "2607740",
    "end": "2614070"
  },
  {
    "text": "I don't have any\nreplicate data there. Right here, I've got deviation\nfrom the linear model.",
    "start": "2614070",
    "end": "2619770"
  },
  {
    "text": "And then I've got\npure replicate error. And you can start to see that\nthe deviations from my best",
    "start": "2619770",
    "end": "2630380"
  },
  {
    "text": "estimate prediction at the\nmodel is much, much larger. And that's what shows up in\nthis ratio of the two variances.",
    "start": "2630380",
    "end": "2637940"
  },
  {
    "text": "If you do that and follow\nthrough with the F, that's highly unlikely--\nthat big of a ratio",
    "start": "2637940",
    "end": "2643910"
  },
  {
    "text": "is highly unlikely to occur by\nchance given the noise spread.",
    "start": "2643910",
    "end": "2649579"
  },
  {
    "text": "So if you actually go in and\ndo the lack of fit analysis, it's already setting\nup big red flags.",
    "start": "2649580",
    "end": "2656369"
  },
  {
    "text": "Here's my red flag saying,\nlook out, look out. You've got a lot of\nevidence of a lack of fit.",
    "start": "2656370",
    "end": "2663530"
  },
  {
    "text": "What's interesting\nin this example is if I were to just look\nat the significance",
    "start": "2663530",
    "end": "2670010"
  },
  {
    "text": "of the individual model\nterms, this pops out in fact",
    "start": "2670010",
    "end": "2676980"
  },
  {
    "text": "that the mean is\nhighly significant but the slope term is not.",
    "start": "2676980",
    "end": "2682025"
  },
  {
    "text": " So this would say--",
    "start": "2682025",
    "end": "2687280"
  },
  {
    "text": "if I weren't looking\nat lack of fit and paying attention\nto that red flag, I might be tempted to\nsay a very wrong thing.",
    "start": "2687280",
    "end": "2697270"
  },
  {
    "text": "I might be tempted to say\nthere is a significant estimate of the mean that's non-zero,\nbut given the spread in my data,",
    "start": "2697270",
    "end": "2707710"
  },
  {
    "text": "I cannot conclude that\nthere is a linear dependence on my input. My linear dependence\non x could be 0.",
    "start": "2707710",
    "end": "2718089"
  },
  {
    "text": "In other words,\nwith that green line right here, that's a\nsmall slope that given",
    "start": "2718090",
    "end": "2725820"
  },
  {
    "text": "the spread in my data is not\njustified to actually estimate as anything other than 0.",
    "start": "2725820",
    "end": "2732060"
  },
  {
    "text": " Interesting, huh? ",
    "start": "2732060",
    "end": "2739349"
  },
  {
    "text": "So you really need\nto look at both. I'd have to be very\ncareful because the extra explanatory\npower of the linear term",
    "start": "2739350",
    "end": "2746910"
  },
  {
    "text": "is very, very minimal here. So I might think\nOK, so I've really got no dependence at\nall, which what I really",
    "start": "2746910",
    "end": "2753210"
  },
  {
    "text": "got his lack of fit.  That making sense?",
    "start": "2753210",
    "end": "2758620"
  },
  {
    "text": " So what I might then do is\nsay, OK, I am paying attention",
    "start": "2758620",
    "end": "2764780"
  },
  {
    "text": "to that big red flag. I've got lack of fit. Maybe I better add a\nquadratic term, refit my data.",
    "start": "2764780",
    "end": "2773460"
  },
  {
    "text": "So now if I look at\nthe S for my model",
    "start": "2773460",
    "end": "2779300"
  },
  {
    "text": "with the mean with a term\nfor the linear coefficient and one for the quadratic,\nnow what do I get?",
    "start": "2779300",
    "end": "2785960"
  },
  {
    "text": "And return to breaking\napart my residual and now looking and seeing\nhow much deviation is there",
    "start": "2785960",
    "end": "2794330"
  },
  {
    "text": "due to lack of fit compared to\nunderlying replicate variance. And now that ratio\nis very small.",
    "start": "2794330",
    "end": "2800880"
  },
  {
    "text": "So now I don't have\nany longer any evidence of lack of fit, that's good.",
    "start": "2800880",
    "end": "2807770"
  },
  {
    "text": "And now I can return\nto deciding about whether individual\nterms are significant.",
    "start": "2807770",
    "end": "2814820"
  },
  {
    "text": "And we don't see the full F\ntest, it's an incomplete ANOVA. But what we would\nbasically find here",
    "start": "2814820",
    "end": "2821750"
  },
  {
    "text": "is the mean term is\nsignificant, the quadratic term is significant.",
    "start": "2821750",
    "end": "2828197"
  },
  {
    "text": "How about the linear term?  It's still not significant.",
    "start": "2828197",
    "end": "2834180"
  },
  {
    "text": "So in fact, we've got a\na mean and a square term but no dependence\non the linear term.",
    "start": "2834180",
    "end": "2840590"
  },
  {
    "text": "You will typically see that. In fact, these-- if these\nterms are truly orthogonal,",
    "start": "2840590",
    "end": "2846770"
  },
  {
    "text": "if I add the terms, it should\nnot change my estimates for the other terms. That's not quite true if you\nthrow those missing terms",
    "start": "2846770",
    "end": "2854570"
  },
  {
    "text": "into noise factors. But the basic point here is\nI've now actually captured",
    "start": "2854570",
    "end": "2860750"
  },
  {
    "text": "that the dependence on x\nwith this quadratic term.",
    "start": "2860750",
    "end": "2867610"
  },
  {
    "text": "So you can do exactly\nthe same thing. This is the same\ndata using Excel.",
    "start": "2867610",
    "end": "2874210"
  },
  {
    "text": "And you get the\nsame kind of a table here with an x term\nand x squared term.",
    "start": "2874210",
    "end": "2879970"
  },
  {
    "text": "And what's interesting\nhere is you can also go in and look at estimates\nof the coefficients,",
    "start": "2879970",
    "end": "2886779"
  },
  {
    "text": "the standard error, 95%\nconfidence intervals. And I guess actually if you were\nto look at that 95% confidence",
    "start": "2886780",
    "end": "2895630"
  },
  {
    "text": "interval for that x term,\nlooks like it actually is likely to be non-zero.",
    "start": "2895630",
    "end": "2902170"
  },
  {
    "text": "So I did get that right. ",
    "start": "2902170",
    "end": "2908040"
  },
  {
    "text": "So actually you probably\nshould include that term, even though the ratio\nis a little bit smaller.",
    "start": "2908040",
    "end": "2914530"
  },
  {
    "text": "It is still significant. Now I also put this one\nup because it's also got estimates of your R-squared\nand adjusted R-squared.",
    "start": "2914530",
    "end": "2923880"
  },
  {
    "text": "where it's giving\nyou a nice feel.",
    "start": "2923880",
    "end": "2929369"
  },
  {
    "text": "R-squared of around 0.9, 0.95,\nyou start to feel pretty good about--",
    "start": "2929370",
    "end": "2934462"
  },
  {
    "text": "pretty good about your model.  So I don't know if you\nplayed around with Excel.",
    "start": "2934462",
    "end": "2940880"
  },
  {
    "text": "So again, I encourage JUMP, but\nif you do need to use Excel,",
    "start": "2940880",
    "end": "2946849"
  },
  {
    "text": "there is-- under the data analysis\ntool if you pull that down,",
    "start": "2946850",
    "end": "2952400"
  },
  {
    "text": "you will also see the\nregression analysis. And it will let you indicate\nwhat your output problems are",
    "start": "2952400",
    "end": "2959240"
  },
  {
    "text": "and what your input columns are. And it does just the least\nsquares regression, pops out your ANOVA table for you.",
    "start": "2959240",
    "end": "2966920"
  },
  {
    "text": "In that case, you\nactually have to construct by hand your wide\nsquare or your x",
    "start": "2966920",
    "end": "2972799"
  },
  {
    "text": "squared data if you\nwant to polynomial fit. And that's what I've\njust illustrated here.",
    "start": "2972800",
    "end": "2977870"
  },
  {
    "text": "You can't simply, unfortunately,\nat least in the version of Excel I have, say I want\nto try a polynomial model up",
    "start": "2977870",
    "end": "2985040"
  },
  {
    "text": "to some order and have\nit just know to do that on the polynomial input data.",
    "start": "2985040",
    "end": "2991400"
  },
  {
    "text": "You actually have\nto create columns for each of the\nmodel coefficients that you want to estimate.",
    "start": "2991400",
    "end": "2996478"
  },
  {
    "text": " Here's the same polynomial\nregression using the JUMP",
    "start": "2996478",
    "end": "3003779"
  },
  {
    "text": "package, again, with\nall of the lack of fit versus pure error, the\nx and x squared terms,",
    "start": "3003780",
    "end": "3012359"
  },
  {
    "text": "t ratios, all of that, but\nbasically the same analysis with the second order included.",
    "start": "3012360",
    "end": "3020849"
  },
  {
    "text": "OK so with that, I'm going\nto-- about to move on to process optimization. But I'd like to take any\nquestions on regression,",
    "start": "3020850",
    "end": "3030000"
  },
  {
    "text": "confidence intervals, confidence\nintervals and input, confidence intervals and outputs. Is that all?",
    "start": "3030000",
    "end": "3035430"
  },
  {
    "text": "It's starting to feel-- are you confident in\nyour understanding",
    "start": "3035430",
    "end": "3040950"
  },
  {
    "text": "of confidence intervals? Yeah, question? AUDIENCE: Definitely\ndon't know what do you do if your inputs\nthat are correlated?",
    "start": "3040950",
    "end": "3049220"
  },
  {
    "text": "DUANE BONING: OK so the\nquestion was, what do you do if your inputs\nare correlated. ",
    "start": "3049220",
    "end": "3055890"
  },
  {
    "text": "So what is assumed\nin all of these fits",
    "start": "3055890",
    "end": "3062539"
  },
  {
    "text": "is essentially you've\ngot orthogonality. If we go back to the\ntables we were forming",
    "start": "3062540",
    "end": "3067800"
  },
  {
    "text": "with full factorial\nand so on, we're assuming that each of your\ncolumns are orthogonal,",
    "start": "3067800",
    "end": "3073950"
  },
  {
    "text": "which is to say we're assuming\neach of your coefficients in each of your different terms\nare uncorrelated or orthogonal.",
    "start": "3073950",
    "end": "3082580"
  },
  {
    "text": "If they are orthogonal, and you\ndo a least squares regression--",
    "start": "3082580",
    "end": "3087860"
  },
  {
    "text": "or if they are not orthogonal,\nthere they are correlated, what happens?",
    "start": "3087860",
    "end": "3093480"
  },
  {
    "text": "Well, what happens is you've\ngot to model coefficients both trying to explain some\namount of the same data.",
    "start": "3093480",
    "end": "3101120"
  },
  {
    "text": "And they fight\nagainst each other. And it's almost\nrandom how the effect",
    "start": "3101120",
    "end": "3108980"
  },
  {
    "text": "that-- that true underlying\neffect gets apportioned between say a beta 1 and a beta to term.",
    "start": "3108980",
    "end": "3114390"
  },
  {
    "text": "In fact very, very tiny\nlittle perturbations, and you can get a different\nmix of beta 1 and beta 2.",
    "start": "3114390",
    "end": "3120170"
  },
  {
    "text": "And it turns out\nyou might still be OK in terms of\npredicting an output",
    "start": "3120170",
    "end": "3125300"
  },
  {
    "text": "because at least your model\nhas both of them in there. But it really screws up\nyour ability to decide",
    "start": "3125300",
    "end": "3131390"
  },
  {
    "text": "is that model term\nsignificant or not.",
    "start": "3131390",
    "end": "3137980"
  },
  {
    "text": "What you need to do\nis transform your data to get it into an\northogonal form",
    "start": "3137980",
    "end": "3145060"
  },
  {
    "text": "to get rid of the correlation\nto basically create do model coefficients and\nnew explanatory values",
    "start": "3145060",
    "end": "3152980"
  },
  {
    "text": "to fake x values that don't\nhave the correlation in them.",
    "start": "3152980",
    "end": "3159190"
  },
  {
    "text": "And the classic\ntool for doing that is principal component\nanalysis or some transformation",
    "start": "3159190",
    "end": "3169390"
  },
  {
    "text": "of the data to a different basis\nthan your original x1, x2, x3",
    "start": "3169390",
    "end": "3175900"
  },
  {
    "text": "coefficients. We might talk a little bit\nabout multivariable things.",
    "start": "3175900",
    "end": "3182580"
  },
  {
    "text": "I think we did a little bit with\nmultivariable statistical and T",
    "start": "3182580",
    "end": "3188100"
  },
  {
    "text": "charts and so on,\nbut essentially a principal components or some\nother kind of transformation",
    "start": "3188100",
    "end": "3195180"
  },
  {
    "text": "is needed on the\ndata in order to then have individual\ncoefficients that",
    "start": "3195180",
    "end": "3200640"
  },
  {
    "text": "are not duplicating each other. If you look, I think it's\nchapter section 8 point--",
    "start": "3200640",
    "end": "3207060"
  },
  {
    "text": "maybe 8.4. The next one after what I\nassigned as a reading, that talks about principal\ncomponent analysis",
    "start": "3207060",
    "end": "3213660"
  },
  {
    "text": "and how you do that\nand process modeling. So you can read that section. It's actually very\ngood, very interesting.",
    "start": "3213660",
    "end": "3222510"
  },
  {
    "text": "Other questions, progression?",
    "start": "3222510",
    "end": "3227990"
  },
  {
    "text": "Yeah? AUDIENCE: If there\nis a big difference between R-squared and\nadjusted R-squared, what is that telling us?",
    "start": "3227990",
    "end": "3234010"
  },
  {
    "text": "In this case, it's essentially\n[INAUDIBLE] 0.9 and 0.8, or 0.7 [INAUDIBLE].",
    "start": "3234010",
    "end": "3241192"
  },
  {
    "text": " DUANE BONING: Yes,\nso the question is what if you have\nbig differences",
    "start": "3241192",
    "end": "3246330"
  },
  {
    "text": "between R-squared and\nadjusted R-squared. I think it's\nessentially telling you",
    "start": "3246330",
    "end": "3253710"
  },
  {
    "text": "that the influence of\nadditional model coefficients is really important, both--",
    "start": "3253710",
    "end": "3264349"
  },
  {
    "text": "this very qualitative. But essentially,\nit's telling you there's more than going on\nthan just the mean response.",
    "start": "3264350",
    "end": "3271850"
  },
  {
    "text": "So you're seeing a little\nbit of a mix of both-- the penalty of adding\nmore model coefficients,",
    "start": "3271850",
    "end": "3277369"
  },
  {
    "text": "but it's also\ntelling you there's likely additional structure\nthat you needed in order",
    "start": "3277370",
    "end": "3285530"
  },
  {
    "text": "to use that. But that's pretty qualitative. I think basically it's\nsignaling that there's",
    "start": "3285530",
    "end": "3290780"
  },
  {
    "text": "more than just mean-- mean deviations going on. ",
    "start": "3290780",
    "end": "3297480"
  },
  {
    "text": "It sounded like there\nwas a microphone question in Singapore?",
    "start": "3297480",
    "end": "3303054"
  },
  {
    "text": "AUDIENCE: Question on slide 50. ",
    "start": "3303054",
    "end": "3309650"
  },
  {
    "text": "You mentioned we should\nonly see the mean which also focused on the lack\nof fit and the pure error.",
    "start": "3309650",
    "end": "3317900"
  },
  {
    "text": "So why do you say that\nwe only see the mean, we may say it's a good model. Can you explain that again?",
    "start": "3317900",
    "end": "3323990"
  },
  {
    "text": "DUANE BONING:\nYeah, actually what I was saying in this\nexample is that if I only looked at the mean, I might be\nhesitant to include any model",
    "start": "3323990",
    "end": "3334710"
  },
  {
    "text": "terms beyond the mean. So I might not actually think\nit's a good model at all.",
    "start": "3334710",
    "end": "3342100"
  },
  {
    "text": "So that part of your question,\nI'm not sure I quite understood or quite agreed with.",
    "start": "3342100",
    "end": "3349260"
  },
  {
    "text": "But I do-- I guess maybe I'm\njust repeating myself, I think it is really critical\nto look for lack of fit",
    "start": "3349260",
    "end": "3357270"
  },
  {
    "text": "because you need\nboth perspectives. You need to look not only at\nmodel coefficients in terms",
    "start": "3357270",
    "end": "3365790"
  },
  {
    "text": "and whether they should\nbe included in the model, but you also have to be\nalert am I missing terms.",
    "start": "3365790",
    "end": "3372869"
  },
  {
    "text": " That's what the lack of\nfit enables you to do.",
    "start": "3372870",
    "end": "3379040"
  },
  {
    "text": "This is basically saying\nthe terms that are there, are they significant? ",
    "start": "3379040",
    "end": "3386630"
  },
  {
    "text": "So in some, sense this one\nis basically just leading you to throw away coefficients\nand throw away model terms.",
    "start": "3386630",
    "end": "3392920"
  },
  {
    "text": "And this number two, the\nlack of fit, is telling you, hey wait a second, there's\nstuff going on in the model",
    "start": "3392920",
    "end": "3398650"
  },
  {
    "text": "that you're not\nexplaining that's different than random\nnoise, so maybe you",
    "start": "3398650",
    "end": "3403840"
  },
  {
    "text": "should add model terms. And so you need\nboth perspectives. ",
    "start": "3403840",
    "end": "3412380"
  },
  {
    "text": "OK so I think we're ready to\nmove on and look a little bit",
    "start": "3412380",
    "end": "3418309"
  },
  {
    "text": "at process optimization. I want to touch on the most\nnatural use of these sorts",
    "start": "3418310",
    "end": "3423710"
  },
  {
    "text": "of models, which is we define\nan experimental design, we go gather the data,\nwe build a model,",
    "start": "3423710",
    "end": "3430279"
  },
  {
    "text": "and then we start\nplaying with the model. I think of that is\noffline use of the model,",
    "start": "3430280",
    "end": "3435620"
  },
  {
    "text": "using it to try to\nidentify an optimal point. But it's not purely\noffline because I",
    "start": "3435620",
    "end": "3442880"
  },
  {
    "text": "want to make the point that if\nyou're predicting an optimum, you probably want to go back and\nrun some confirming experiments",
    "start": "3442880",
    "end": "3450770"
  },
  {
    "text": "and use those back with\nyour physical process to check your model and maybe\neven iterate and improve",
    "start": "3450770",
    "end": "3457940"
  },
  {
    "text": "your model. So that's one natural approach. And the other is-- that should be online use.",
    "start": "3457940",
    "end": "3467070"
  },
  {
    "text": "So another clever approach\nis actually build simplified",
    "start": "3467070",
    "end": "3472440"
  },
  {
    "text": "models in a little part of\nthe space, use that to tell me what direction to move in\nexploring my overall process",
    "start": "3472440",
    "end": "3480300"
  },
  {
    "text": "space, and then dynamically\nbuild and improve my model.",
    "start": "3480300",
    "end": "3485340"
  },
  {
    "text": "In the case when my real goal\nis getting to an optimum, not having the perfect model\ncovering all of my space",
    "start": "3485340",
    "end": "3492840"
  },
  {
    "text": "but rather get to\nan optimum point. So I want to touch on\nboth of these ideas, ways of using these sort of\nsimplified response surface",
    "start": "3492840",
    "end": "3500640"
  },
  {
    "text": "models. And part of the point\nhere is one important use",
    "start": "3500640",
    "end": "3507250"
  },
  {
    "text": "of these models really is trying\nto find an optimal process output or find the inputs that\ngive me an optimal process",
    "start": "3507250",
    "end": "3515230"
  },
  {
    "text": "output. And that optimal\nprocess output may have multiple\ncharacteristics about it",
    "start": "3515230",
    "end": "3521170"
  },
  {
    "text": "that are important for us. One is I want to be\nclose to a target value.",
    "start": "3521170",
    "end": "3528220"
  },
  {
    "text": "But the other is we may\nalso want small sensitivity,",
    "start": "3528220",
    "end": "3533530"
  },
  {
    "text": "small deviations in my output. And if we go back to\nour variation equation, that may mean I want small\ndeviations around noise factors",
    "start": "3533530",
    "end": "3542770"
  },
  {
    "text": "that I'm not controlling. And I may also want\nrelatively small sensitivity",
    "start": "3542770",
    "end": "3551670"
  },
  {
    "text": "even to some of my\ninput parameters because I'm going to\nfix them in my process. And I'm not dynamically or in\na feedback loop changing them.",
    "start": "3551670",
    "end": "3560500"
  },
  {
    "text": "So in some cases, I want\nthis to also be small. So we'll talk a\nlittle bit about ways",
    "start": "3560500",
    "end": "3567000"
  },
  {
    "text": "to mix in these and\nother objectives. For right now, I'm\ngoing to mostly focus",
    "start": "3567000",
    "end": "3573060"
  },
  {
    "text": "on say trying to meet some\nset of target mean values.",
    "start": "3573060",
    "end": "3580710"
  },
  {
    "text": "But I can make the\npoint you can generalize what I'm going to be talking\nabout here by thinking",
    "start": "3580710",
    "end": "3587730"
  },
  {
    "text": "of some objective function,\nor some cost function, or some goodness function that\nactually mixes in together",
    "start": "3587730",
    "end": "3595619"
  },
  {
    "text": "multiple objectives. So some of the objectives,\nyou might have a cost function that penalizes for\ndeviations from the target",
    "start": "3595620",
    "end": "3606119"
  },
  {
    "text": "or maybe sum of\nsquared deviations if I have multiple\noutputs from the target. It may also penalize me\nfor larger x's because--",
    "start": "3606120",
    "end": "3615450"
  },
  {
    "text": "larger input because\nthere's more cost associated with using more\ngas if I have a higher gas",
    "start": "3615450",
    "end": "3624000"
  },
  {
    "text": "flow in some process. And then I can also\ninclude other things like terms that penalize for\nsensitivity, these delta y's,",
    "start": "3624000",
    "end": "3634410"
  },
  {
    "text": "sensitivity to the output. And I can keep throwing\nadditional things in.",
    "start": "3634410",
    "end": "3640890"
  },
  {
    "text": "So if I've got in general some\ncomplicated objective function,",
    "start": "3640890",
    "end": "3646769"
  },
  {
    "text": "if I can formulate\nthat and actually model either empirically or\nanalytically that cost",
    "start": "3646770",
    "end": "3657120"
  },
  {
    "text": "function as a\nfunction of my input or as a function utilizing the\nmodels that I already have,",
    "start": "3657120",
    "end": "3664650"
  },
  {
    "text": "I can then formulate an\noptimization function or an optimization\nproblem where I might be trying to\nminimize that cost",
    "start": "3664650",
    "end": "3671130"
  },
  {
    "text": "or minimize that objective. Or maybe I'm trying\nto maximize it",
    "start": "3671130",
    "end": "3676980"
  },
  {
    "text": "because I think of it as really\na goodness function rather than a penalty function. But overall, I've got some\ncomplicated form for J",
    "start": "3676980",
    "end": "3686460"
  },
  {
    "text": "as a function of my factors. Or my factors might\nbe my actual input,",
    "start": "3686460",
    "end": "3692820"
  },
  {
    "text": "but they may also be noise\nfactors, other factors that I",
    "start": "3692820",
    "end": "3698010"
  },
  {
    "text": "haven't explicitly modeled. And we'll talk about robustness\nnext week, or not next week,",
    "start": "3698010",
    "end": "3708150"
  },
  {
    "text": "on Thursday. But right now, I\njust want to talk about adjusting or searching\nfor good input factors",
    "start": "3708150",
    "end": "3717690"
  },
  {
    "text": "to minimize or maximize some\ncost function with constraints.",
    "start": "3717690",
    "end": "3723430"
  },
  {
    "text": "So in general, you can think\nabout different approaches for this. If I've got a full\nexpression for y",
    "start": "3723430",
    "end": "3730020"
  },
  {
    "text": "as some function of x and\nmaybe J is some function of y,",
    "start": "3730020",
    "end": "3736530"
  },
  {
    "text": "I have overall got some\noverall function for my cost",
    "start": "3736530",
    "end": "3741570"
  },
  {
    "text": "as a function of my inputs. Then I can go in\nand try to minimize,",
    "start": "3741570",
    "end": "3747690"
  },
  {
    "text": "really dj dx and find--",
    "start": "3747690",
    "end": "3752910"
  },
  {
    "text": "with some assumptions\nof monotonicity, I can find an overall minimum\nor at least a local minimum",
    "start": "3752910",
    "end": "3758430"
  },
  {
    "text": "or maximum to that function. So that's if I've got\na full expression. And we'll explore\nthat a little bit.",
    "start": "3758430",
    "end": "3766230"
  },
  {
    "text": "Another approach is more\nof an incremental approach. Rather than having\nthe full expression and leaping right\nto the optimum point",
    "start": "3766230",
    "end": "3774420"
  },
  {
    "text": "based on a local minimum\nor local maximum, I may have to search for it.",
    "start": "3774420",
    "end": "3780540"
  },
  {
    "text": "I may have to iteratively\nexplore the space. And we'll talk a\nlittle bit about these",
    "start": "3780540",
    "end": "3785880"
  },
  {
    "text": "with hill climbing or steepest\nascent and descent kinds of problems. And I've already\nmentioned a little bit",
    "start": "3785880",
    "end": "3792480"
  },
  {
    "text": "of this online versus offline. So here's the simplest\npicture for one of these optimization problems.",
    "start": "3792480",
    "end": "3799230"
  },
  {
    "text": "I've got my input x, and\nI've got my output y. And what I'm looking for is\na maximum for my output y.",
    "start": "3799230",
    "end": "3811470"
  },
  {
    "text": "And maybe here simply\nmy cost function is simply J or J is equal\nto y, something like that.",
    "start": "3811470",
    "end": "3819850"
  },
  {
    "text": "So I'm not differentiating\nhere too much between y and J. I'm just simply saying\nwhat I'm looking",
    "start": "3819850",
    "end": "3825450"
  },
  {
    "text": "for is the overall\nmaximum for this output.",
    "start": "3825450",
    "end": "3830520"
  },
  {
    "text": "And one knows from basic\ngeometry, basic algebra",
    "start": "3830520",
    "end": "3835650"
  },
  {
    "text": "that the maximum will occur-- unless I hit some\nconstraints or some boundary",
    "start": "3835650",
    "end": "3842309"
  },
  {
    "text": "cases --will occur\nwhen I've got zero curvature in that function.",
    "start": "3842310",
    "end": "3849780"
  },
  {
    "text": "So how do I find it? Well, one approach is, again,\nthis analytic approach.",
    "start": "3849780",
    "end": "3855970"
  },
  {
    "text": "If I have a full\nexpression, I can simply recognize that\nthat minimum occurs where there is zero curvature,\nsolve for the y such",
    "start": "3855970",
    "end": "3866319"
  },
  {
    "text": "that that curvature is 0, and\nI directly get to the answer.",
    "start": "3866320",
    "end": "3872440"
  },
  {
    "text": "But in order to do that, I\nneed a full analytic model. To do that, I needed\nperhaps relatively small",
    "start": "3872440",
    "end": "3880329"
  },
  {
    "text": "or good accurate increments\nand x or assumptions on the model form.",
    "start": "3880330",
    "end": "3885730"
  },
  {
    "text": "And especially if I have\nrelatively sparse data points, if I had say just\nthese data points,",
    "start": "3885730",
    "end": "3894190"
  },
  {
    "text": "it's quite easy to miss\nthe true optimum because of noise or imperfections\nin my model fit.",
    "start": "3894190",
    "end": "3906370"
  },
  {
    "text": "So it can actually be a little\nbit tricky with small amounts of data to find that if I\nfit an overall analytic model",
    "start": "3906370",
    "end": "3914430"
  },
  {
    "text": "to a very small\nnumber of data points.  An alternative is a little bit\nof an iterative or a search",
    "start": "3914430",
    "end": "3923940"
  },
  {
    "text": "process where we might actually\nadd data or explore or model,",
    "start": "3923940",
    "end": "3931020"
  },
  {
    "text": "either explore experiments or\nexplore a model in a smaller",
    "start": "3931020",
    "end": "3937770"
  },
  {
    "text": "space in each case and sort of\nseek to find the optimum point.",
    "start": "3937770",
    "end": "3943770"
  },
  {
    "text": "And here are a simple\nconceptual idea here is in some\nregions of my space,",
    "start": "3943770",
    "end": "3950340"
  },
  {
    "text": "I may have very good\nmodel fits less so than with much less\nerror than trying",
    "start": "3950340",
    "end": "3956730"
  },
  {
    "text": "to fit this overall quadratic to\na small number of data points. I may have relatively\ngood model fit",
    "start": "3956730",
    "end": "3962310"
  },
  {
    "text": "in smaller regions of the space.  Remember that confidence\ninterval on the output?",
    "start": "3962310",
    "end": "3969110"
  },
  {
    "text": "I said as we get further\nand further away from say the central moments of our\ndata, my confidence interval",
    "start": "3969110",
    "end": "3978440"
  },
  {
    "text": "on my output prediction\ngets wider and wider. If I shrink my space,\nI get better estimates",
    "start": "3978440",
    "end": "3984859"
  },
  {
    "text": "of my model in a local space. And so one approach\nhere is to say, I'm going to look\nin a local space",
    "start": "3984860",
    "end": "3990900"
  },
  {
    "text": "get a good estimate\nof what the slope is. Maybe it's a reduced order\nmodel that's only linear.",
    "start": "3990900",
    "end": "3999270"
  },
  {
    "text": "So I'm not even trying to\nfit additional curvature. And then use that\nto say my output y",
    "start": "3999270",
    "end": "4006410"
  },
  {
    "text": "is increasing in this\ndirection with x increasing. And use that to project\nforward a small amount",
    "start": "4006410",
    "end": "4015200"
  },
  {
    "text": "and suggest a new\nx value to try. So it's projecting and\nadditional steps to explore.",
    "start": "4015200",
    "end": "4026660"
  },
  {
    "text": "If I then do that and build\nan additional linear model-- whoa --build an additional\nlinear model here,",
    "start": "4026660",
    "end": "4036220"
  },
  {
    "text": "it might suggest\nanother small step. And as my linear model starts to\nhave a slope turn that shrinks,",
    "start": "4036220",
    "end": "4043540"
  },
  {
    "text": "that's telling me I'm\ngetting something closer to an optimum point or at\nleast a local optimum point.",
    "start": "4043540",
    "end": "4051369"
  },
  {
    "text": "And at that point that's\nsignaling me that if I really want improved accuracy\nat that point in space,",
    "start": "4051370",
    "end": "4059260"
  },
  {
    "text": "to really zero in on the\nmaximum, I can do two things. One is still constrained\nmy search space.",
    "start": "4059260",
    "end": "4067760"
  },
  {
    "text": "But also in this region,\nit's quite likely that my-- ",
    "start": "4067760",
    "end": "4075580"
  },
  {
    "text": "it's quite likely-- I don't want this. I don't know what that was.",
    "start": "4075580",
    "end": "4080900"
  },
  {
    "text": "Oh, wow, something\nfunky happened. In this space, it's just like\nwith that curvature model",
    "start": "4080900",
    "end": "4090540"
  },
  {
    "text": "that I showed you\nearlier, the linear term is probably no longer\nvery significant. I really need the\nquadratic term.",
    "start": "4090540",
    "end": "4097470"
  },
  {
    "text": "So I might fit locally\na quadratic model just near the optimum which allows\nme in a restricted space",
    "start": "4097470",
    "end": "4104068"
  },
  {
    "text": "to get an accurate model\nthat really lets me zero in on the optimum point.",
    "start": "4104069",
    "end": "4111180"
  },
  {
    "text": "So out here, a linear\nmodel might be good enough up in here. I may need a beta\n0 plus a beta 2 x",
    "start": "4111180",
    "end": "4117839"
  },
  {
    "text": "squared term, maybe still\nalso with a linear term here as well.",
    "start": "4117840",
    "end": "4124509"
  },
  {
    "text": "But I can basically\nbuild dynamically the model getting an accurate\nmodel near the optimum point.",
    "start": "4124510",
    "end": "4130189"
  },
  {
    "text": " Now, I showed you this\nin 1D, the point 1D,",
    "start": "4130189",
    "end": "4135689"
  },
  {
    "text": "but you can also do\nthis with two inputs, where I've got a 3D model if\nthis is an x1, this is an x2,",
    "start": "4135689",
    "end": "4142759"
  },
  {
    "text": "and this is a y. But you can essentially\nthink the same thing. If I start out here in this\nspace, locally it's linear.",
    "start": "4142760",
    "end": "4153770"
  },
  {
    "text": "I can use that to\nsuggest the next step to take using a simplified\nlinear model in this region.",
    "start": "4153770",
    "end": "4162790"
  },
  {
    "text": "And then as I hill climb up,\nas I get close to the optimum,",
    "start": "4162790",
    "end": "4169390"
  },
  {
    "text": "then again now near\nthe optimum, I need-- as my x1 and x2, I may\nneed a quadratic model",
    "start": "4169390",
    "end": "4177189"
  },
  {
    "text": "in those two coefficients. But I can extend the same\nidea to hill climbing",
    "start": "4177189",
    "end": "4182560"
  },
  {
    "text": "not only in one input, but\ntwo inputs, three inputs, multiple inputs in order\nto get to an optimum point.",
    "start": "4182560",
    "end": "4191219"
  },
  {
    "text": "So essentially what\nwe're doing here is, again, linear\ngradient modeling, it is useful often to include\nstill an interaction term.",
    "start": "4191220",
    "end": "4199590"
  },
  {
    "text": "But essentially we're doing\nexactly that same thing. And if my model\nitself is linear,",
    "start": "4199590",
    "end": "4205840"
  },
  {
    "text": "an interesting thing happened.  Where is my overall optimum?",
    "start": "4205840",
    "end": "4212400"
  },
  {
    "text": "If I'm trying to\nget to maximized y, where's my maximum\ny going to occur?",
    "start": "4212400",
    "end": "4219900"
  },
  {
    "text": "It will always occur on a\nboundary when I hit a limit of my input and x's.",
    "start": "4219900",
    "end": "4227080"
  },
  {
    "text": "So an important thing that\nI haven't talked much about is also the notion of\nadditional constraints.",
    "start": "4227080",
    "end": "4234560"
  },
  {
    "text": "We may be driving to an interior\npoint like in this model, but it's also\npossible that we may",
    "start": "4234560",
    "end": "4241460"
  },
  {
    "text": "be driving to either a corner\npoint or some other boundary point because of a constraint\non my allowable ranges",
    "start": "4241460",
    "end": "4252020"
  },
  {
    "text": "for my x inputs.  There is another\npiece of terminology",
    "start": "4252020",
    "end": "4258110"
  },
  {
    "text": "that's sometimes used for\nthese kinds of searches, either steepest descent\nor steepest descent,",
    "start": "4258110",
    "end": "4264800"
  },
  {
    "text": "whether you're climbing or\nlooking for a local minima. And the basic point is when\nI've got that simplified",
    "start": "4264800",
    "end": "4270680"
  },
  {
    "text": "linear model perhaps with\nthe linear interaction term as well, you can think about\nthe local gradient with respect",
    "start": "4270680",
    "end": "4279340"
  },
  {
    "text": "to x1 or the local gradient\nwith respect to x2.",
    "start": "4279340",
    "end": "4284650"
  },
  {
    "text": "And now when you make your\nstep, what you often want to do is make the step in the overall\nsteepest descent direction,",
    "start": "4284650",
    "end": "4294460"
  },
  {
    "text": "changing both your x1 and x2\nparameter at the same time. So this is simply\nshowing when I move",
    "start": "4294460",
    "end": "4304830"
  },
  {
    "text": "and hill climb, I may change\nx1 and x2 proportionally depending on the relative slope\nin those two coefficients.",
    "start": "4304830",
    "end": "4313320"
  },
  {
    "text": "And it's relatively\neasy once I've got that model to\ndecide what direction is the overall steepest descent.",
    "start": "4313320",
    "end": "4320880"
  },
  {
    "text": "Another point here is\nthat with quadratic terms, you can have complicated\nfunctions where your minima may",
    "start": "4320880",
    "end": "4329230"
  },
  {
    "text": "occur in the interior of\nthe space or your maxima in the interior of the space.",
    "start": "4329230",
    "end": "4335020"
  },
  {
    "text": "But you can also have\nhyperbolic or inverse polynomial",
    "start": "4335020",
    "end": "4340450"
  },
  {
    "text": "kinds of relationships\nwhere, again, you may have local minima or maxima\nwith respect to one variable",
    "start": "4340450",
    "end": "4348970"
  },
  {
    "text": "depending on what you're\ndoing with the other variable. Or you may also have places\nwhere you end up with a maxima",
    "start": "4348970",
    "end": "4354820"
  },
  {
    "text": "again at your constraint points. So in your search, you've\ngot to account for both.",
    "start": "4354820",
    "end": "4362660"
  },
  {
    "text": "So I can summarize\nwhat we've done here with a combined procedure\nfor design of experiments",
    "start": "4362660",
    "end": "4370030"
  },
  {
    "text": "and optimization in either\nthe iterative fashion or at the end, I'll allude to\nevolutionary or incremental",
    "start": "4370030",
    "end": "4382450"
  },
  {
    "text": "kind of version. So this is a summary of the last\ntwo or three lectures boiled",
    "start": "4382450",
    "end": "4388540"
  },
  {
    "text": "down into a reminder-- a\nsummary of the basic process or procedure for doing\nDOE and optimization.",
    "start": "4388540",
    "end": "4396610"
  },
  {
    "text": "We said originally\nour goal here is to build a model, to do\na design of experiments.",
    "start": "4396610",
    "end": "4402070"
  },
  {
    "text": "I do want to\nemphasize that depends on some knowledge of the\nprocess, a little bit of knowledge either\nexperience based",
    "start": "4402070",
    "end": "4408790"
  },
  {
    "text": "or in the physics\nof the process. Because you need that\nin order to do things decide what the important\ninputs are likely to be.",
    "start": "4408790",
    "end": "4419280"
  },
  {
    "text": "Now there are things you can\ndo with the DOE to confirm that or to expand your knowledge,\nlike factor screening",
    "start": "4419280",
    "end": "4426630"
  },
  {
    "text": "experiments. We talked about\nfractional factorial with large numbers of\ncoefficients where you're just",
    "start": "4426630",
    "end": "4432390"
  },
  {
    "text": "trying to decide is there\na main effect associated with that factor.",
    "start": "4432390",
    "end": "4437550"
  },
  {
    "text": "But up front, defining the\ninputs is very important. We also need to define\nlimits on the inputs.",
    "start": "4437550",
    "end": "4445890"
  },
  {
    "text": "What space do we want\nto explore and build a model over in our\ndesign of experiments?",
    "start": "4445890",
    "end": "4451380"
  },
  {
    "text": " So overall, we're going to\nneed to first build our--",
    "start": "4451380",
    "end": "4457350"
  },
  {
    "text": "decide on a DOE. We'd go and run our experiments. And then we're going to\nconstruct our response service",
    "start": "4457350",
    "end": "4463950"
  },
  {
    "text": "model. And if we're using it\nfor the optimization, I also want to make\nthe point that you need to think early on about\nwhat your overall optimization",
    "start": "4463950",
    "end": "4472469"
  },
  {
    "text": "or penalty function is\nbecause that may strongly influence your DOE and maybe\neven your factor selection.",
    "start": "4472470",
    "end": "4482130"
  },
  {
    "text": "So for example, if you\nbelieve that you're really going to need an optimization\nthat folds in things like noise",
    "start": "4482130",
    "end": "4493650"
  },
  {
    "text": "in addition to just\ntrying to get to a target, that can have a profound effect\non the DOE that you explore.",
    "start": "4493650",
    "end": "4502500"
  },
  {
    "text": "And we'll talk about\nthat on Thursday, where you might do\nadditional small experiments",
    "start": "4502500",
    "end": "4507929"
  },
  {
    "text": "at each point in\nthe DOE in order to build a sensitivity\nmodel of that delta y",
    "start": "4507930",
    "end": "4515040"
  },
  {
    "text": "as a function of some\nadditional noise factors. So depending on\nwhat it is you're",
    "start": "4515040",
    "end": "4522210"
  },
  {
    "text": "trying to achieve with your\nmodel, that can of course, I guess it's obvious,\nthat can affect",
    "start": "4522210",
    "end": "4529920"
  },
  {
    "text": "the structure of your model\nand the design of experiments that you want to do. So we've already talked\nabout a lot of this.",
    "start": "4529920",
    "end": "4536880"
  },
  {
    "text": "Again in summary,\nyour DOE includes decisions about what\nlikely terms you think",
    "start": "4536880",
    "end": "4543600"
  },
  {
    "text": "might be in there based on\nyour knowledge of the physics. Is it going to be mostly linear? Might there be quadratic terms?",
    "start": "4543600",
    "end": "4550680"
  },
  {
    "text": "That can influence\nagain the selection of the high-low center points.",
    "start": "4550680",
    "end": "4555810"
  },
  {
    "text": "Do you need center points,\ndo you need three levels for all factors, and so on.",
    "start": "4555810",
    "end": "4561610"
  },
  {
    "text": "And you also need to think about\nthings like the noise factors. We talked about these\nnuisance factors, if you will,",
    "start": "4561610",
    "end": "4568020"
  },
  {
    "text": "or additional noise factors. So that you might randomize\nor block against those.",
    "start": "4568020",
    "end": "4573420"
  },
  {
    "text": "If they're not going to be\nexplicitly in the model, you don't want them\naliasing with or confounding",
    "start": "4573420",
    "end": "4579000"
  },
  {
    "text": "with the terms you actually had. The response service modeling\nis actually a pretty easy piece,",
    "start": "4579000",
    "end": "4585860"
  },
  {
    "text": "especially if you use\nthings like the regression and the ANOVA approach.",
    "start": "4585860",
    "end": "4591950"
  },
  {
    "text": "Again, you can use\ncontrast, if you've got a highly structured\ndesign and experiment",
    "start": "4591950",
    "end": "4598010"
  },
  {
    "text": "for very rapid estimation\nof those terms. But overall, the\nemphasis here is",
    "start": "4598010",
    "end": "4603590"
  },
  {
    "text": "you're trying to determine if\nthere's significant variation",
    "start": "4603590",
    "end": "4609260"
  },
  {
    "text": "in your data, are individual\nterms significant, are you missing terms. So that lack of fit is\nextremely important.",
    "start": "4609260",
    "end": "4617180"
  },
  {
    "text": "And there's often a very\ninteresting interplay with the regression modeling.",
    "start": "4617180",
    "end": "4625010"
  },
  {
    "text": "In fact, an approach we\nhaven't talked about much, but it's essentially\ninherent in what",
    "start": "4625010",
    "end": "4631280"
  },
  {
    "text": "we've been talking about\nhere is also referred to as--",
    "start": "4631280",
    "end": "4636679"
  },
  {
    "text": "I think it's-- not\npiece-wise, step-wise, step-wise regression.",
    "start": "4636680",
    "end": "4642680"
  },
  {
    "text": "And some of the\ninteractive tools like JUMP actually explicitly\nsupport this,",
    "start": "4642680",
    "end": "4648440"
  },
  {
    "text": "where one factor at a\ntime, you look and say, I would like to\nadd a term or drop",
    "start": "4648440",
    "end": "4654590"
  },
  {
    "text": "a term based on cut off decision\npoints, on significance, and so on. So you can build up an\nappropriate regression model",
    "start": "4654590",
    "end": "4663500"
  },
  {
    "text": "by dropping or adding\nterms as needed. And we talked about this at a\nfairly high order or high level",
    "start": "4663500",
    "end": "4672380"
  },
  {
    "text": "about the optimization\nprocedure and again, just ideas of defining\nyour penalty function",
    "start": "4672380",
    "end": "4678500"
  },
  {
    "text": "and then searching\nfor your optimization either piece-wise\nor analytically.",
    "start": "4678500",
    "end": "4684270"
  },
  {
    "text": "I'll come back to\nthis in just a second. But I do want to\nemphasize that once you've come to some expected\noptimum point,",
    "start": "4684270",
    "end": "4698940"
  },
  {
    "text": "you really should check\nthat and confirm that often",
    "start": "4698940",
    "end": "4704370"
  },
  {
    "text": "because you're building\nyour estimate of your model",
    "start": "4704370",
    "end": "4710100"
  },
  {
    "text": "based on relatively\nlimited data, especially in the\nfactorial models perhaps with only one interior\npoint or center point based",
    "start": "4710100",
    "end": "4717690"
  },
  {
    "text": "on mostly extreme old data. And especially if you've\ndriven your optimum",
    "start": "4717690",
    "end": "4723599"
  },
  {
    "text": "to some interior point\nusing say the analytic model",
    "start": "4723600",
    "end": "4729940"
  },
  {
    "text": "of the response service\nmodel rather than iteratively or incrementally, you're\nmaking a lot of big assumptions",
    "start": "4729940",
    "end": "4737530"
  },
  {
    "text": "about the shape of the model\nright near your optimum, like it's convex right\nat that optimum point.",
    "start": "4737530",
    "end": "4744790"
  },
  {
    "text": "So you really ought to go in\nand do a confirming experiment right at or right\nnear your optimum",
    "start": "4744790",
    "end": "4753960"
  },
  {
    "text": "in order to really\ntest the model and consider model error\nright at that point.",
    "start": "4753960",
    "end": "4761739"
  },
  {
    "text": "And that might actually drive\nyou to improving the model or exploring slightly different\nspace right near that optimum.",
    "start": "4761740",
    "end": "4771080"
  },
  {
    "text": "Now the one last\nthing I just want to allude to is an\nalternative approach",
    "start": "4771080",
    "end": "4777340"
  },
  {
    "text": "here is often starting with\nsome data point in a small space",
    "start": "4777340",
    "end": "4783770"
  },
  {
    "text": "and building your model\niteratively or adaptively. And next week, at\nthe end of next week,",
    "start": "4783770",
    "end": "4790230"
  },
  {
    "text": "we'll have a guest\nlecturer, Dan fry, who has actually studied\none factor at a time",
    "start": "4790230",
    "end": "4796520"
  },
  {
    "text": "incremental exploration\nand model building for the purpose of\noptimization a great deal.",
    "start": "4796520",
    "end": "4804260"
  },
  {
    "text": "So he's going to lead us\nthrough an alternative approach of actually doing\nfull factorial models",
    "start": "4804260",
    "end": "4811280"
  },
  {
    "text": "but trying to find the optimum\nby not defining up front the whole DOE and\nrunning the whole thing,",
    "start": "4811280",
    "end": "4819150"
  },
  {
    "text": "but rather just walking\naround your multifactor space in order to try to\nfind the optimum point.",
    "start": "4819150",
    "end": "4827600"
  },
  {
    "text": "And that has some relationship\nto another approach that",
    "start": "4827600",
    "end": "4833990"
  },
  {
    "text": "is also in May and Spanos in\nchapter 8.5 which I've just mentioned to you but not\nexpect that you actually",
    "start": "4833990",
    "end": "4842000"
  },
  {
    "text": "have to know a lot about, which\nis evolutionary optimization. Which would say build a\nlocal model use that again",
    "start": "4842000",
    "end": "4849830"
  },
  {
    "text": "and a hill climbing fashion\nto suggest where you want to go for your next point.",
    "start": "4849830",
    "end": "4856170"
  },
  {
    "text": "Maybe in fact you simply\npick one of those corners. And then you build a\ndo model around that.",
    "start": "4856170",
    "end": "4861800"
  },
  {
    "text": "And it might suggest\nyou move your process to another corner, in which\ncase you build another model",
    "start": "4861800",
    "end": "4868370"
  },
  {
    "text": "and so on, so that you\ncan walk or evolutionarily",
    "start": "4868370",
    "end": "4873710"
  },
  {
    "text": "arrive at an optimum\npoint in your process, building local\nmodels along the way.",
    "start": "4873710",
    "end": "4881200"
  },
  {
    "text": "OK so next time, the\none additional topic I want to mention in this space\nof optimization and process",
    "start": "4881200",
    "end": "4888340"
  },
  {
    "text": "optimization and DOE is\nthis notion of robustness. I'll allude to actually\nbuilding models",
    "start": "4888340",
    "end": "4894640"
  },
  {
    "text": "that include the\nvariance in them and not just the overall output.",
    "start": "4894640",
    "end": "4900440"
  },
  {
    "text": "So we'll come back to that\non Thursday and enjoy. In the meantime, I think\nyou've got the problem that",
    "start": "4900440",
    "end": "4907750"
  },
  {
    "text": "is due on Thursday. And it's going to let\nyou explore a little bit more some of these DOE and\nresponse service model kinds",
    "start": "4907750",
    "end": "4913840"
  },
  {
    "text": "of things. So we'll see you on Thursday. ",
    "start": "4913840",
    "end": "4917000"
  }
]