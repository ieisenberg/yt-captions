[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help MIT\nOpenCourseWare continue to",
    "start": "0",
    "end": "6910"
  },
  {
    "text": "offer high quality educational\nresources for free. To make a donation or view\nadditional materials from",
    "start": "6910",
    "end": "13460"
  },
  {
    "text": "hundreds of MIT courses, visit\nMIT OpenCourseWare at",
    "start": "13460",
    "end": "19290"
  },
  {
    "text": "ocw.mit.edu PROFESSOR: So we're going to\nfinish today our discussion of",
    "start": "19290",
    "end": "25430"
  },
  {
    "text": "Bayesian Inference, which\nwe started last time. As you probably saw there's\nnot a huge lot of concepts",
    "start": "25430",
    "end": "32960"
  },
  {
    "text": "that we're introducing at this\npoint in terms of specific skills of calculating\nprobabilities.",
    "start": "32960",
    "end": "39770"
  },
  {
    "text": "But, rather, it's more of an\ninterpretation and setting up the framework.",
    "start": "39770",
    "end": "45460"
  },
  {
    "text": "So the framework in Bayesian\nestimation is that there is some parameter which is not\nknown, but we have a prior",
    "start": "45460",
    "end": "52500"
  },
  {
    "text": "distribution on it. These are beliefs about what\nthis variable might be, and",
    "start": "52500",
    "end": "60040"
  },
  {
    "text": "then we'll obtain some\nmeasurements. And the measurements are\naffected by the value of that",
    "start": "60040",
    "end": "65409"
  },
  {
    "text": "parameter that we don't know. And this effect, the fact that\nX is affected by Theta, is",
    "start": "65410",
    "end": "72490"
  },
  {
    "text": "captured by introducing a\nconditional probability distribution-- the distribution of X\ndepends on Theta.",
    "start": "72490",
    "end": "79590"
  },
  {
    "text": "It's a conditional probability\ndistribution. So we have formulas for these\ntwo densities, the prior",
    "start": "79590",
    "end": "86280"
  },
  {
    "text": "density and the conditional\ndensity. And given that we have these,\nif we multiply them we can also get the joint density\nof X and Theta.",
    "start": "86280",
    "end": "94000"
  },
  {
    "text": "So we have everything\nthat's there is to know in this second. And now we observe the random\nvariable X. Given this random",
    "start": "94000",
    "end": "101649"
  },
  {
    "text": "variable what can we\nsay about Theta? Well, what we can do is we\ncan always calculate the",
    "start": "101650",
    "end": "108380"
  },
  {
    "text": "conditional distribution of\ntheta given X. And now that we have the specific value of\nX we can plot this as",
    "start": "108380",
    "end": "115990"
  },
  {
    "text": "a function of Theta. OK. And this is the complete\nanswer to a",
    "start": "115990",
    "end": "121380"
  },
  {
    "text": "Bayesian Inference problem. This posterior distribution\ncaptures everything there is to say about Theta, that's\nwhat we know about Theta.",
    "start": "121380",
    "end": "130239"
  },
  {
    "text": "Given the X that we have\nobserved Theta is still random, it's still unknown. And it might be here, there,\nor there with several",
    "start": "130240",
    "end": "138270"
  },
  {
    "text": "probabilities. On the other hand, if you want\nto report a single value for Theta then you do\nsome extra work.",
    "start": "138270",
    "end": "147590"
  },
  {
    "text": "You continue from here, and you\ndo some data processing on X. Doing data processing means\nthat you apply a certain",
    "start": "147590",
    "end": "155359"
  },
  {
    "text": "function on the data,\nand this function is something that you design.",
    "start": "155360",
    "end": "160650"
  },
  {
    "text": "It's the so-called estimator. And once that function is\napplied it outputs an estimate",
    "start": "160650",
    "end": "166459"
  },
  {
    "text": "of Theta, which we\ncall Theta hat. So this is sort of the big\npicture of what's happening.",
    "start": "166460",
    "end": "173489"
  },
  {
    "text": "Now one thing to keep in mind\nis that even though I'm writing single letters here, in\ngeneral Theta or X could be",
    "start": "173490",
    "end": "180450"
  },
  {
    "text": "vector random variables. So think of this-- it could be a collection\nTheta1, Theta2, Theta3.",
    "start": "180450",
    "end": "188170"
  },
  {
    "text": "And maybe we obtained several\nmeasurements, so this X is really a vector X1,\nX2, up to Xn.",
    "start": "188170",
    "end": "195630"
  },
  {
    "text": "All right, so now how do we\nchoose a Theta to report? There are various ways\nof doing it.",
    "start": "195630",
    "end": "201960"
  },
  {
    "text": "One is to look at the posterior\ndistribution and report the value of Theta, at\nwhich the density or the PMF",
    "start": "201960",
    "end": "209940"
  },
  {
    "text": "is highest. This is called the maximum\na posteriori estimate.",
    "start": "209940",
    "end": "215570"
  },
  {
    "text": "So we pick a value of theta for\nwhich the posteriori is maximum, and we report it.",
    "start": "215570",
    "end": "220990"
  },
  {
    "text": "An alternative way is to try to\nbe optimal with respects to",
    "start": "220990",
    "end": "226030"
  },
  {
    "text": "a mean squared error. So what is this? If we have a specific estimator,\ng, this is the",
    "start": "226030",
    "end": "233260"
  },
  {
    "text": "estimate it's going\nto produce. This is the true value of\nTheta, so this is our",
    "start": "233260",
    "end": "238300"
  },
  {
    "text": "estimation error. We look at the square of the\nestimation error, and look at the average value.",
    "start": "238300",
    "end": "244180"
  },
  {
    "text": "We would like this squared\nestimation error to be as small as possible. How can we design our estimator\ng to make that error",
    "start": "244180",
    "end": "252470"
  },
  {
    "text": "as small as possible? It turns out that the answer is\nto produce, as an estimate,",
    "start": "252470",
    "end": "259489"
  },
  {
    "text": "the conditional expectation\nof Theta given X. So the conditional expectation is the\nbest estimate that you could",
    "start": "259490",
    "end": "266600"
  },
  {
    "text": "produce if your objective is to\nkeep the mean squared error as small as possible.",
    "start": "266600",
    "end": "272720"
  },
  {
    "text": "So this statement here is a\nstatement of what happens on the average over all Theta's and\nall X's that may happen in",
    "start": "272720",
    "end": "279950"
  },
  {
    "text": "our experiment. The conditional expectation as\nan estimator has an even",
    "start": "279950",
    "end": "285160"
  },
  {
    "text": "stronger property. Not only it's optimal on the\naverage, but it's also optimal",
    "start": "285160",
    "end": "291490"
  },
  {
    "text": "given that you have made a\nspecific observation, no matter what you observe.",
    "start": "291490",
    "end": "297840"
  },
  {
    "text": "Let's say you observe the\nspecific value for the random variable X. After that point if\nyou're asked to produce a",
    "start": "297840",
    "end": "305560"
  },
  {
    "text": "best estimate Theta hat that\nminimizes this mean squared",
    "start": "305560",
    "end": "311190"
  },
  {
    "text": "error, your best estimate\nwould be the conditional expectation given the specific\nvalue that you have observed.",
    "start": "311190",
    "end": "318939"
  },
  {
    "text": "These two statements say almost\nthe same thing, but this one is a bit stronger.",
    "start": "318940",
    "end": "325650"
  },
  {
    "text": "This one tells you no matter\nwhat specific X happens the",
    "start": "325650",
    "end": "330830"
  },
  {
    "text": "conditional expectation\nis the best estimate. This one tells you on the\naverage, over all X's may",
    "start": "330830",
    "end": "336870"
  },
  {
    "text": "happen, the conditional expectation is the best estimator.",
    "start": "336870",
    "end": "342650"
  },
  {
    "text": "Now this is really a consequence\nof this. If the conditional expectation\nis best for any specific X,",
    "start": "342650",
    "end": "348510"
  },
  {
    "text": "then it's the best one even when\nX is left random and you are averaging your error\nover all possible X's.",
    "start": "348510",
    "end": "358200"
  },
  {
    "text": "OK so now that we know what is\nthe optimal way of producing an estimate let's do a\nsimple example to see",
    "start": "358200",
    "end": "365510"
  },
  {
    "text": "how things work out. So we have started with an\nunknown random variable, Theta, which is uniformly\ndistributed between 4 and 10.",
    "start": "365510",
    "end": "375080"
  },
  {
    "text": "And then we have an observation\nmodel that tells us that given the value of\nTheta, X is going to be a",
    "start": "375080",
    "end": "382430"
  },
  {
    "text": "random variable that ranges\nbetween Theta - 1, and Theta + 1. So think of X as a noisy\nmeasurement of Theta, plus",
    "start": "382430",
    "end": "392550"
  },
  {
    "text": "some noise, which is\nbetween -1, and +1.",
    "start": "392550",
    "end": "397599"
  },
  {
    "text": "So really the model that we are\nusing here is that X is equal to Theta plus U --",
    "start": "397600",
    "end": "404430"
  },
  {
    "text": "where U is uniform\non -1, and +1.",
    "start": "404430",
    "end": "410500"
  },
  {
    "text": "one, and plus one. So we have the true value of\nTheta, but X could be Theta -",
    "start": "410500",
    "end": "415946"
  },
  {
    "text": "1, or it could be all the\nway up to Theta + 1. And the X is uniformly\ndistributed on that interval.",
    "start": "415946",
    "end": "423770"
  },
  {
    "text": "That's the same as saying that\nU is uniformly distributed over this interval.",
    "start": "423770",
    "end": "429820"
  },
  {
    "text": "So now we have all the\ninformation that we need, we can construct the\njoint density.",
    "start": "429820",
    "end": "435270"
  },
  {
    "text": "And the joint density is, of\ncourse, the prior density times the conditional density.",
    "start": "435270",
    "end": "441849"
  },
  {
    "text": "We go both of these. Both of these are constants, so\nthe joint density is also",
    "start": "441850",
    "end": "448880"
  },
  {
    "text": "going to be a constant. 1/6 times 1/2, this\nis one over 12.",
    "start": "448880",
    "end": "454419"
  },
  {
    "text": "But it is a constant,\nnot everywhere. Only on the range of possible\nx's and thetas.",
    "start": "454420",
    "end": "461280"
  },
  {
    "text": "So theta can take any value\nbetween four and ten, so these are the values of theta.",
    "start": "461280",
    "end": "467430"
  },
  {
    "text": "And for any given value of theta\nx can take values from theta minus one, up\nto theta plus one.",
    "start": "467430",
    "end": "475690"
  },
  {
    "text": "So here, if you can imagine, a\nline that goes with slope one, and then x can take that value\nof theta plus or minus one.",
    "start": "475690",
    "end": "488530"
  },
  {
    "text": "So this object here, this is\nthe set of possible x and",
    "start": "488530",
    "end": "494720"
  },
  {
    "text": "theta pairs. So the density is equal to one\nover 12 over this set, and",
    "start": "494720",
    "end": "501490"
  },
  {
    "text": "it's zero everywhere else. So outside here the density is\nzero, the density only applies",
    "start": "501490",
    "end": "508035"
  },
  {
    "text": "at that point. All right, so now we're\nasked to estimate",
    "start": "508035",
    "end": "513110"
  },
  {
    "text": "theta in terms of x. So we want to build an estimator\nwhich is going to be a function from the\nx's to the thetas.",
    "start": "513110",
    "end": "520000"
  },
  {
    "text": "That's why I chose the axis\nthis way-- x to be on this axis, theta on that axis-- Because the estimator we're\nbuilding is a function of x.",
    "start": "520000",
    "end": "528019"
  },
  {
    "text": "Based on the observation that\nwe obtained, we want to estimate theta. So we know that the optimal\nestimator is the conditional",
    "start": "528020",
    "end": "535680"
  },
  {
    "text": "expectation, given\nthe value of x. So what is the conditional\nexpectation?",
    "start": "535680",
    "end": "542160"
  },
  {
    "text": "If you fix a particular value of\nx, let's say in this range.",
    "start": "542160",
    "end": "547889"
  },
  {
    "text": "So this is our x, then what\ndo we know about theta?",
    "start": "547890",
    "end": "553240"
  },
  {
    "text": "We know that theta lies\nin this range. Theta can only be sampled\nbetween those two values.",
    "start": "553240",
    "end": "561670"
  },
  {
    "text": "And what kind of distribution\ndoes theta have? What is the conditional\ndistribution of theta given x?",
    "start": "561670",
    "end": "568980"
  },
  {
    "text": "Well, remember how we built\nconditional distributions from joint distributions? The conditional distribution is\njust a section of the joint",
    "start": "568980",
    "end": "578899"
  },
  {
    "text": "distribution applied to\nthe place where we're conditioning. So the joint is constant.",
    "start": "578900",
    "end": "585800"
  },
  {
    "text": "So the conditional is also going\nto be a constant density over this interval. So the posterior distribution\nof theta is",
    "start": "585800",
    "end": "593560"
  },
  {
    "text": "uniform over this interval. So if the posterior of theta is\nuniform over that interval,",
    "start": "593560",
    "end": "601110"
  },
  {
    "text": "the expected value of theta is\ngoing to be the meet point of that interval. So the estimate which\nyou report--",
    "start": "601110",
    "end": "608880"
  },
  {
    "text": "if you observe that theta-- is going to be this particular\npoint here, it's the midpoint.",
    "start": "608880",
    "end": "615750"
  },
  {
    "text": "The same argument goes through\neven if you obtain an x somewhere here.",
    "start": "615750",
    "end": "622570"
  },
  {
    "text": "Given this x, theta\ncan take a value",
    "start": "622570",
    "end": "629540"
  },
  {
    "text": "between these two values. Theta is going to have a uniform\ndistribution over this",
    "start": "629540",
    "end": "635990"
  },
  {
    "text": "interval, and the conditional\nexpectation of theta given x is going to be the midpoint\nof that interval.",
    "start": "635990",
    "end": "643840"
  },
  {
    "text": "So now if we plot our estimator\nby tracing midpoints",
    "start": "643840",
    "end": "650790"
  },
  {
    "text": "in this diagram what you're\ngoing to obtain is a curve",
    "start": "650790",
    "end": "656300"
  },
  {
    "text": "that starts like this, then\nit changes slope.",
    "start": "656300",
    "end": "661795"
  },
  {
    "text": " So that it keeps track of the\nmidpoint, and then it goes",
    "start": "661795",
    "end": "667279"
  },
  {
    "text": "like that again. So this blue curve here is\nour g of x, which is the",
    "start": "667280",
    "end": "673760"
  },
  {
    "text": "conditional expectation of\ntheta given that x is equal to little x.",
    "start": "673760",
    "end": "680480"
  },
  {
    "text": "So it's a curve, in our example\nit consists of three",
    "start": "680480",
    "end": "686610"
  },
  {
    "text": "straight segments. But overall it's non-linear. It's not a single line\nthrough this diagram.",
    "start": "686610",
    "end": "693440"
  },
  {
    "text": "And that's how things\nare in general. g of x, our optimal estimate has\nno reason to be a linear",
    "start": "693440",
    "end": "699300"
  },
  {
    "text": "function of x. In general it's going to be\nsome complicated curve. ",
    "start": "699300",
    "end": "707350"
  },
  {
    "text": "So how good is our estimate? I mean you reported your x, your\nestimate of theta based",
    "start": "707350",
    "end": "715700"
  },
  {
    "text": "on x, and your boss asks you\nwhat kind of error do you expect to get?",
    "start": "715700",
    "end": "723350"
  },
  {
    "text": "Having observed the particular\nvalue of x, what you can report to your boss is what you\nthink is the mean squared",
    "start": "723350",
    "end": "731140"
  },
  {
    "text": "error is going to be. We observe the particular\nvalue of x. So we're conditioning, and we're\nliving in this universe.",
    "start": "731140",
    "end": "739650"
  },
  {
    "text": "Given that we have made this\nobservation, this is the true value of theta, this is the\nestimate that we have",
    "start": "739650",
    "end": "745840"
  },
  {
    "text": "produced, this is the expected\nsquared error, given that we",
    "start": "745840",
    "end": "752220"
  },
  {
    "text": "have made the particular\nobservation. Now in this conditional universe\nthis is the expected",
    "start": "752220",
    "end": "759700"
  },
  {
    "text": "value of theta given x. So this is the expected value of\nthis random variable inside",
    "start": "759700",
    "end": "766240"
  },
  {
    "text": "the conditional universe. So when you take the mean\nsquared of a random variable minus the expected value, this\nis the same thing as the",
    "start": "766240",
    "end": "773780"
  },
  {
    "text": "variance of that random\nvariable. Except that it's the\nvariance inside the conditional universe.",
    "start": "773780",
    "end": "780940"
  },
  {
    "text": "Having observed x, theta is\nstill a random variable.",
    "start": "780940",
    "end": "786230"
  },
  {
    "text": "It's distributed according to\nthe posterior distribution. Since it's a random variable,\nit has a variance.",
    "start": "786230",
    "end": "792220"
  },
  {
    "text": "And that variance is our\nmean squared error. So this is the variance of the\nposterior distribution of",
    "start": "792220",
    "end": "800279"
  },
  {
    "text": "Theta given the observation\nthat we have made. ",
    "start": "800280",
    "end": "806688"
  },
  {
    "text": "OK, so what is the variance\nin our example? If X happens to be here, then\nTheta is uniform over this",
    "start": "806688",
    "end": "816270"
  },
  {
    "text": "interval, and this interval\nhas length 2.",
    "start": "816270",
    "end": "821990"
  },
  {
    "text": "Theta is uniformly distributed\nover an interval of length 2. This is the posterior\ndistribution of Theta.",
    "start": "821990",
    "end": "829900"
  },
  {
    "text": "What is the variance? Then you remember the formula\nfor the variance of a uniform random variable, it is the\nlength of the interval squared",
    "start": "829900",
    "end": "839520"
  },
  {
    "text": "divided by 12, so this is 1/3. So the variance of Theta --",
    "start": "839520",
    "end": "846060"
  },
  {
    "text": "the mean squared error-- is\ngoing to be 1/3 whenever this kind of picture applies.",
    "start": "846060",
    "end": "852430"
  },
  {
    "text": "This picture applies when\nX is between 5 and 9. If X is less than 5, then the\npicture is a little different,",
    "start": "852430",
    "end": "860100"
  },
  {
    "text": "and Theta is going\nto be uniform over a smaller interval. And so the variance of\ntheta is going to",
    "start": "860100",
    "end": "866930"
  },
  {
    "text": "be smaller as well. So let's start plotting our\nmean squared error. Between 5 and 9 the variance\nof Theta --",
    "start": "866930",
    "end": "875930"
  },
  {
    "text": "the posterior variance-- is 1/3. Now when the X falls in here\nTheta is uniformly distributed",
    "start": "875930",
    "end": "886100"
  },
  {
    "text": "over a smaller interval. The size of this interval\nchanges linearly over that range.",
    "start": "886100",
    "end": "892800"
  },
  {
    "text": "And so when we take the square\nsize of that interval we get a",
    "start": "892800",
    "end": "899260"
  },
  {
    "text": "quadratic function of\nhow much we have moved from that corner. So at that corner what is\nthe variance of Theta?",
    "start": "899260",
    "end": "907140"
  },
  {
    "text": "Well if I observe an X that's\nequal to 3 then I know with certainty that Theta\nis equal to 4.",
    "start": "907140",
    "end": "914810"
  },
  {
    "text": "Then I'm in very good shape, I\nknow exactly what Theta is going to be. So the variance, in this\ncase, is going to be 0.",
    "start": "914810",
    "end": "922890"
  },
  {
    "text": "If I observe an X that's a\nlittle larger than Theta is now random, takes values on\na little interval, and the",
    "start": "922890",
    "end": "931130"
  },
  {
    "text": "variance of Theta is going to be\nproportional to the square of the length of that\nlittle interval.",
    "start": "931130",
    "end": "937910"
  },
  {
    "text": "So we get a curve that\nstarts rising quadratically from here. It goes up forward 1/3.",
    "start": "937910",
    "end": "945390"
  },
  {
    "text": "At the other end of the picture\nthe same is true. If you observe an X which is\n11 then Theta can only be",
    "start": "945390",
    "end": "954500"
  },
  {
    "text": "equal to 10. And so the error in Theta\nis equal to 0,",
    "start": "954500",
    "end": "960720"
  },
  {
    "text": "there's 0 error variance. But as we obtain X's that are\nslightly less than 11 then the",
    "start": "960720",
    "end": "967360"
  },
  {
    "text": "mean squared error again\nrises quadratically. So we end up with a\nplot like this.",
    "start": "967360",
    "end": "973450"
  },
  {
    "text": "What this plot tells us is that\ncertain measurements are better than others.",
    "start": "973450",
    "end": "978920"
  },
  {
    "text": "If you're lucky, and you see X\nequal to 3 then you're lucky,",
    "start": "978920",
    "end": "985269"
  },
  {
    "text": "because you know Theta\nexactly what it is. If you see an X which is equal\nto 6 then you're sort of",
    "start": "985270",
    "end": "993830"
  },
  {
    "text": "unlikely, because it\ndoesn't tell you Theta with great precision. Theta could be anywhere\non that interval.",
    "start": "993830",
    "end": "1000560"
  },
  {
    "text": "And so the variance\nof Theta -- even after you have\nobserved X -- is a certain number,\n1/3 in our case.",
    "start": "1000560",
    "end": "1008470"
  },
  {
    "text": "So the moral to keep out\nof that story is that the error variance--",
    "start": "1008470",
    "end": "1016970"
  },
  {
    "text": "or the mean squared error-- depends on what particular\nobservation",
    "start": "1016970",
    "end": "1023350"
  },
  {
    "text": "you happen to obtain. Some observations may be very\ninformative, and once you see",
    "start": "1023350",
    "end": "1030240"
  },
  {
    "text": "a specific number than you know\nexactly what Theta is. Some observations might\nbe less informative.",
    "start": "1030240",
    "end": "1035760"
  },
  {
    "text": "You observe your X, but it could\nstill leave a lot of uncertainty about Theta. ",
    "start": "1035760",
    "end": "1043839"
  },
  {
    "text": "So conditional expectations are\nreally the cornerstone of Bayesian estimation.",
    "start": "1043839",
    "end": "1048890"
  },
  {
    "text": "They're particularly\npopular, especially in engineering contexts.",
    "start": "1048890",
    "end": "1053950"
  },
  {
    "text": "There used a lot in signal\nprocessing, communications, control theory, so on.",
    "start": "1053950",
    "end": "1060940"
  },
  {
    "text": "So that makes it worth playing\na little bit with their theoretical properties, and get\nsome appreciation of a few",
    "start": "1060940",
    "end": "1070450"
  },
  {
    "text": "subtleties involved here. No new math in reality, in what\nwe're going to do here.",
    "start": "1070450",
    "end": "1077990"
  },
  {
    "text": "But it's going to be a good\nopportunity to practice manipulation of conditional\nexpectations.",
    "start": "1077990",
    "end": "1085310"
  },
  {
    "text": "So let's look at the expected\nvalue of the estimation error",
    "start": "1085310",
    "end": "1093150"
  },
  {
    "text": "that we obtained. So Theta hat is our estimator,\nis the conditional",
    "start": "1093150",
    "end": "1098539"
  },
  {
    "text": "expectation. Theta hat minus Theta is what\nkind of error do we have?",
    "start": "1098540",
    "end": "1105690"
  },
  {
    "text": "If Theta hat, is bigger than\nTheta then we have made the positive error.",
    "start": "1105690",
    "end": "1111510"
  },
  {
    "text": "If not, if it's on the other\nside, we have made the negative error. Then it turns out that on the\naverage the errors cancel each",
    "start": "1111510",
    "end": "1119110"
  },
  {
    "text": "other out, on the average. So let's do this calculation. Let's calculate the expected\nvalue of the error given X.",
    "start": "1119110",
    "end": "1130010"
  },
  {
    "text": "Now by definition the error is\nexpected value of Theta hat minus Theta given X.",
    "start": "1130010",
    "end": "1137850"
  },
  {
    "text": "We use linearity of expectations\nto break it up as expected value of Theta hat\ngiven X minus expected value",
    "start": "1137850",
    "end": "1144850"
  },
  {
    "text": "of Theta given X.\nAnd now what?",
    "start": "1144850",
    "end": "1151090"
  },
  {
    "text": "Our estimate is made on the\nbasis of the data of the X's.",
    "start": "1151090",
    "end": "1158679"
  },
  {
    "text": "If I tell you X then you\nknow what Theta hat is. Remember that the conditional\nexpectation is a random",
    "start": "1158680",
    "end": "1166490"
  },
  {
    "text": "variable which is a function\nof the random variable, on which you're conditioning on.",
    "start": "1166490",
    "end": "1171560"
  },
  {
    "text": "If you know X then you know the\nconditional expectation given X, you know what Theta\nhat is going to be.",
    "start": "1171560",
    "end": "1178390"
  },
  {
    "text": "So Theta hat is a function of\nX. If it's a function of X then once I tell you X\nyou know what Theta",
    "start": "1178390",
    "end": "1185910"
  },
  {
    "text": "hat is going to be. So this conditional expectation\nis going to be Theta hat itself.",
    "start": "1185910",
    "end": "1191860"
  },
  {
    "text": "Here this is-- just\nby definition-- Theta hat, and so we\nget equality to 0.",
    "start": "1191860",
    "end": "1199580"
  },
  {
    "text": "So what we have proved is that\nno matter what I have observed, and given that I have\nobserved something on the",
    "start": "1199580",
    "end": "1208970"
  },
  {
    "text": "average my error is\ngoing to be 0.",
    "start": "1208970",
    "end": "1214049"
  },
  {
    "text": "This is a statement involving\nequality of random variables.",
    "start": "1214050",
    "end": "1219960"
  },
  {
    "text": "Remember that conditional\nexpectations are random variables because they depend\non the thing you're",
    "start": "1219960",
    "end": "1226970"
  },
  {
    "text": "conditioning on. 0 is sort of a trivial\nrandom variable. This tells you that this random\nvariable is identically",
    "start": "1226970",
    "end": "1234080"
  },
  {
    "text": "equal to the 0 random\nvariable. More specifically it tells you\nthat no matter what value for",
    "start": "1234080",
    "end": "1240720"
  },
  {
    "text": "X you observe, the conditional\nexpectation of the error is going to be 0.",
    "start": "1240720",
    "end": "1246410"
  },
  {
    "text": "And this takes us to this\nstatement here, which is inequality between numbers.",
    "start": "1246410",
    "end": "1251830"
  },
  {
    "text": "No matter what specific value\nfor capital X you have observed, your error, on\nthe average, is going",
    "start": "1251830",
    "end": "1260440"
  },
  {
    "text": "to be equal to 0. So this is a less abstract\nversion of these statements.",
    "start": "1260440",
    "end": "1266730"
  },
  {
    "text": "This is inequality between\ntwo numbers. It's true for every value of\nX, so it's true in terms of",
    "start": "1266730",
    "end": "1275080"
  },
  {
    "text": "these random variables being\nequal to that random variable. Because remember according to\nour definition this random",
    "start": "1275080",
    "end": "1281170"
  },
  {
    "text": "variable is the random variable\nthat takes this specific value when capital\nX happens to be",
    "start": "1281170",
    "end": "1287410"
  },
  {
    "text": "equal to little x. Now this doesn't mean that your\nerror is 0, it only means",
    "start": "1287410",
    "end": "1293480"
  },
  {
    "text": "that your error is as likely, in\nsome sense, to fall on the positive side, as to fall\non the negative side.",
    "start": "1293480",
    "end": "1300040"
  },
  {
    "text": "So sometimes your error will be positive, sometimes negative. And on the average these\nthings cancel out and",
    "start": "1300040",
    "end": "1306360"
  },
  {
    "text": "give you a 0 --. on the average. So this is a property that's\nsometimes giving the name we",
    "start": "1306360",
    "end": "1313620"
  },
  {
    "text": "say that Theta hat\nis unbiased.",
    "start": "1313620",
    "end": "1319040"
  },
  {
    "text": "So Theta hat, our estimate, does\nnot have a tendency to be on the high side.",
    "start": "1319040",
    "end": "1324179"
  },
  {
    "text": "It does not have a tendency\nto be on the low side. On the average it's\njust right.",
    "start": "1324180",
    "end": "1330580"
  },
  {
    "text": " So let's do a little\nmore playing here.",
    "start": "1330580",
    "end": "1338389"
  },
  {
    "text": " Let's see how our error is\nrelated to an arbitrary",
    "start": "1338390",
    "end": "1347690"
  },
  {
    "text": "function of the data. Let's do this in a conditional\nuniverse and",
    "start": "1347690",
    "end": "1356960"
  },
  {
    "text": "look at this quantity. ",
    "start": "1356960",
    "end": "1365210"
  },
  {
    "text": "In a conditional universe\nwhere X is known then h of X is known.",
    "start": "1365210",
    "end": "1371060"
  },
  {
    "text": "And so you can pull it outside\nthe expectation. In the conditional universe\nwhere the value of X is given",
    "start": "1371060",
    "end": "1378010"
  },
  {
    "text": "this quantity becomes\njust a constant. There's nothing random\nabout it.",
    "start": "1378010",
    "end": "1383250"
  },
  {
    "text": "So you can pull it out,\nthe expectation, and write things this way.",
    "start": "1383250",
    "end": "1389840"
  },
  {
    "text": "And we have just calculated\nthat this quantity is 0. So this number turns out\nto be 0 as well.",
    "start": "1389840",
    "end": "1397390"
  },
  {
    "text": " Now having done this,\nwe can take",
    "start": "1397390",
    "end": "1403830"
  },
  {
    "text": "expectations of both sides. And now let's use the law of\niterated expectations.",
    "start": "1403830",
    "end": "1409530"
  },
  {
    "text": "Expectation of a conditional\nexpectation gives us the unconditional expectation, and\nthis is also going to be 0.",
    "start": "1409530",
    "end": "1422200"
  },
  {
    "text": "So here we use the law of\niterated expectations.",
    "start": "1422200",
    "end": "1427455"
  },
  {
    "start": "1427455",
    "end": "1434460"
  },
  {
    "text": "OK. ",
    "start": "1434460",
    "end": "1444510"
  },
  {
    "text": "OK, why are we doing this? We're doing this because I would\nlike to calculate the",
    "start": "1444510",
    "end": "1449990"
  },
  {
    "text": "covariance between Theta\ntilde and Theta hat. Theta hat is, ask the question\n-- is there a systematic",
    "start": "1449990",
    "end": "1456490"
  },
  {
    "text": "relation between the error\nand the estimate? So to calculate the covariance\nwe use the property that we",
    "start": "1456490",
    "end": "1470830"
  },
  {
    "text": "can calculate the covariances\nby calculating the expected value of the product minus\nthe product of",
    "start": "1470830",
    "end": "1479520"
  },
  {
    "text": "the expected values. ",
    "start": "1479520",
    "end": "1488440"
  },
  {
    "text": "And what do we get? This is 0, because of\nwhat we just proved.",
    "start": "1488440",
    "end": "1496080"
  },
  {
    "text": " And this is 0, because of\nwhat we proved earlier.",
    "start": "1496080",
    "end": "1506160"
  },
  {
    "text": "That the expected value of\nthe error is equal to 0. ",
    "start": "1506160",
    "end": "1512900"
  },
  {
    "text": "So the covariance between the\nerror and any function of X is",
    "start": "1512900",
    "end": "1527800"
  },
  {
    "text": "equal to 0. Let's use that to the case where\nthe function of X we're",
    "start": "1527800",
    "end": "1533059"
  },
  {
    "text": "considering is Theta\nhat itself.",
    "start": "1533060",
    "end": "1538620"
  },
  {
    "text": "Theta hat is our estimate, it's\na function of X. So this 0 result would still apply,\nand we get that this",
    "start": "1538620",
    "end": "1546845"
  },
  {
    "text": "covariance is equal to 0. OK, so that's what we proved.",
    "start": "1546845",
    "end": "1559100"
  },
  {
    "text": "Let's see, what are the morals\nto take out of all this? First is you should be very\ncomfortable with this type of",
    "start": "1559100",
    "end": "1567640"
  },
  {
    "text": "calculation involving\nconditional expectations. The main two things that we're\nusing are that when you",
    "start": "1567640",
    "end": "1574100"
  },
  {
    "text": "condition on a random variable\nany function of that random variable becomes a constant,\nand can be pulled out the",
    "start": "1574100",
    "end": "1581020"
  },
  {
    "text": "conditional expectation. The other thing that we are\nusing is the law of iterated expectations, so these are\nthe skills involved.",
    "start": "1581020",
    "end": "1589450"
  },
  {
    "text": "Now on the substance, why is\nthis result interesting? This tells us that the error is",
    "start": "1589450",
    "end": "1595390"
  },
  {
    "text": "uncorrelated with the estimate.  What's a hypothetical situation\nwhere these would",
    "start": "1595390",
    "end": "1602530"
  },
  {
    "text": "not happen? Whenever Theta hat is positive\nmy error tends to be negative.",
    "start": "1602530",
    "end": "1612720"
  },
  {
    "text": "Suppose that whenever Theta hat\nis big then you say oh my estimate is too big, maybe the\ntrue Theta is on the lower",
    "start": "1612720",
    "end": "1620610"
  },
  {
    "text": "side, so I expect my error\nto be negative. That would be a situation that\nwould violate this condition.",
    "start": "1620610",
    "end": "1629230"
  },
  {
    "text": "This condition tells you that\nno matter what Theta hat is, you don't expect your error to\nbe on the positive side or on",
    "start": "1629230",
    "end": "1637110"
  },
  {
    "text": "the negative side. Your error will still\nbe 0 on the average. So if you obtain a very high\nestimate this is no reason for",
    "start": "1637110",
    "end": "1645780"
  },
  {
    "text": "you to suspect that\nthe true Theta is lower than your estimate.",
    "start": "1645780",
    "end": "1650890"
  },
  {
    "text": "If you suspected that the true\nTheta was lower than your estimate you should have\nchanged your Theta hat.",
    "start": "1650890",
    "end": "1658830"
  },
  {
    "text": "If you make an estimate and\nafter obtaining that estimate you say I think my estimate\nis too big, and so",
    "start": "1658830",
    "end": "1666269"
  },
  {
    "text": "the error is negative. If you thought that way then\nthat means that your estimate is not the optimal one, that\nyour estimate should have been",
    "start": "1666270",
    "end": "1673690"
  },
  {
    "text": "corrected to be smaller. And that would mean that there's\na better estimate than",
    "start": "1673690",
    "end": "1680029"
  },
  {
    "text": "the one you used, but the\nestimate that we are using here is the optimal one in terms\nof mean squared error,",
    "start": "1680030",
    "end": "1686060"
  },
  {
    "text": "there's no way of\nimproving it. And this is really captured\nin that statement.",
    "start": "1686060",
    "end": "1691640"
  },
  {
    "text": "That is knowing Theta hat\ndoesn't give you a lot of information about the error, and\ngives you, therefore, no",
    "start": "1691640",
    "end": "1698290"
  },
  {
    "text": "reason to adjust your estimate\nfrom what it was.",
    "start": "1698290",
    "end": "1704430"
  },
  {
    "text": "Finally, a consequence\nof all this. This is the definition\nof the error.",
    "start": "1704430",
    "end": "1711909"
  },
  {
    "text": "Send Theta to this side, send\nTheta tilde to that side, you get this relation. The true parameter is composed\nof two quantities.",
    "start": "1711910",
    "end": "1721010"
  },
  {
    "text": "The estimate, and the\nerror that they got with a minus sign.",
    "start": "1721010",
    "end": "1726460"
  },
  {
    "text": "These two quantities are\nuncorrelated with each other. Their covariance is 0, and\ntherefore, the variance of",
    "start": "1726460",
    "end": "1733350"
  },
  {
    "text": "this is the sum of the variances\nof these two quantities. ",
    "start": "1733350",
    "end": "1740470"
  },
  {
    "text": "So what's an interpretation\nof this equality?",
    "start": "1740470",
    "end": "1747520"
  },
  {
    "text": "There is some inherent\nrandomness in the random variable theta that we're\ntrying to estimate.",
    "start": "1747520",
    "end": "1754540"
  },
  {
    "text": "Theta hat tries to estimate it,\ntries to get close to it. And if Theta hat always stays\nclose to Theta, since Theta is",
    "start": "1754540",
    "end": "1765500"
  },
  {
    "text": "random Theta hat must also be\nquite random, so it has uncertainty in it.",
    "start": "1765500",
    "end": "1771170"
  },
  {
    "text": "And the more uncertain Theta\nhat is the more it moves together with Theta.",
    "start": "1771170",
    "end": "1776640"
  },
  {
    "text": "So the more uncertainty\nit removes from Theta. And this is the remaining\nuncertainty in Theta.",
    "start": "1776640",
    "end": "1783900"
  },
  {
    "text": "The uncertainty that's left\nafter we've done our estimation. So ideally, to have a small\nerror we want this",
    "start": "1783900",
    "end": "1792330"
  },
  {
    "text": "quantity to be small. Which is the same as\nsaying that this quantity should be big.",
    "start": "1792330",
    "end": "1797740"
  },
  {
    "text": "In the ideal case Theta hat\nis the same as Theta. That's the best we\ncould hope for.",
    "start": "1797740",
    "end": "1804820"
  },
  {
    "text": "That corresponds to 0 error,\nand all the uncertainly in Theta is absorbed by the\nuncertainty in Theta hat.",
    "start": "1804820",
    "end": "1814230"
  },
  {
    "text": "Interestingly, this relation\nhere is just another variation of the law of total variance\nthat we have seen at some",
    "start": "1814230",
    "end": "1821630"
  },
  {
    "text": "point in the past. I will skip that derivation, but\nit's an interesting fact,",
    "start": "1821630",
    "end": "1828570"
  },
  {
    "text": "and it can give you an\nalternative interpretation of the law of total variance. ",
    "start": "1828570",
    "end": "1836840"
  },
  {
    "text": "OK, so now let's return\nto our example. In our example we obtained the\noptimal estimator, and we saw",
    "start": "1836840",
    "end": "1845630"
  },
  {
    "text": "that it was a nonlinear curve,\nsomething like this.",
    "start": "1845630",
    "end": "1851220"
  },
  {
    "text": "I'm exaggerating the corner\nof a little bit to show that it's nonlinear. This is the optimal estimator.",
    "start": "1851220",
    "end": "1857400"
  },
  {
    "text": "It's a nonlinear function\nof X -- nonlinear generally\nmeans complicated.",
    "start": "1857400",
    "end": "1865200"
  },
  {
    "text": "Sometimes the conditional\nexpectation is really hard to compute, because whenever you\nhave to compute expectations",
    "start": "1865200",
    "end": "1872320"
  },
  {
    "text": "you need to do some integrals. And if you have many random\nvariables involved it might",
    "start": "1872320",
    "end": "1879880"
  },
  {
    "text": "correspond to a\nmulti-dimensional integration. We don't like this. Can we come up, maybe,\nwith a simpler way",
    "start": "1879880",
    "end": "1887370"
  },
  {
    "text": "of estimating Theta? Of coming up with a point\nestimate which still has some",
    "start": "1887370",
    "end": "1892580"
  },
  {
    "text": "nice properties, it\nhas some good motivation, but is simpler. What does simpler mean?",
    "start": "1892580",
    "end": "1898630"
  },
  {
    "text": "Perhaps linear. Let's put ourselves in a\nstraitjacket and restrict",
    "start": "1898630",
    "end": "1905570"
  },
  {
    "text": "ourselves to estimators that's\nare of these forms. My estimate is constrained\nto be a linear",
    "start": "1905570",
    "end": "1913280"
  },
  {
    "text": "function of the X's. So my estimator is going to be\na curve, a linear curve.",
    "start": "1913280",
    "end": "1919320"
  },
  {
    "text": "It could be this, it could be\nthat, maybe it would want to be something like this.",
    "start": "1919320",
    "end": "1926350"
  },
  {
    "text": "I want to choose the best\npossible linear function. What does that mean?",
    "start": "1926350",
    "end": "1931490"
  },
  {
    "text": "It means that I write my\nTheta hat in this form. If I fix a certain a and b I\nhave fixed the functional form",
    "start": "1931490",
    "end": "1940750"
  },
  {
    "text": "of my estimator, and this\nis the corresponding mean squared error. That's the error between the\ntrue parameter and the",
    "start": "1940750",
    "end": "1948210"
  },
  {
    "text": "estimate of that parameter, we\ntake the square of this. ",
    "start": "1948210",
    "end": "1953730"
  },
  {
    "text": "And now the optimal linear\nestimator is defined as one for which these mean squared\nerror is smallest possible",
    "start": "1953730",
    "end": "1962210"
  },
  {
    "text": "over all choices of a and b. So we want to minimize\nthis expression",
    "start": "1962210",
    "end": "1968260"
  },
  {
    "text": "over all a's and b's. How do we do this\nminimization?",
    "start": "1968260",
    "end": "1975650"
  },
  {
    "text": "Well this is a square,\nyou can expand it. Write down all the terms in the\nexpansion of the square.",
    "start": "1975650",
    "end": "1982040"
  },
  {
    "text": "So you're going to get\nthe term expected value of Theta squared. You're going to get\nanother term--",
    "start": "1982040",
    "end": "1987380"
  },
  {
    "text": "a squared expected value of X\nsquared, another term which is b squared, and then you're\ngoing to get to",
    "start": "1987380",
    "end": "1993340"
  },
  {
    "text": "various cross terms. What you have here is really a\nquadratic function of a and b.",
    "start": "1993340",
    "end": "2002050"
  },
  {
    "text": "So think of this quantity that\nwe're minimizing as some function h of a and b, and it\nhappens to be quadratic.",
    "start": "2002050",
    "end": "2008920"
  },
  {
    "text": " How do we minimize a\nquadratic function?",
    "start": "2008920",
    "end": "2015280"
  },
  {
    "text": "We set the derivative of this\nfunction with respect to a and b to 0, and then\ndo the algebra.",
    "start": "2015280",
    "end": "2022940"
  },
  {
    "text": "After you do the algebra you\nfind that the best choice for",
    "start": "2022940",
    "end": "2028000"
  },
  {
    "text": "a is this 1, so this is the\ncoefficient next to X. This is",
    "start": "2028000",
    "end": "2034380"
  },
  {
    "text": "the optimal a. ",
    "start": "2034380",
    "end": "2039560"
  },
  {
    "text": "And the optimal b corresponds\nof the constant terms. So this term and this times that\ntogether are the optimal",
    "start": "2039560",
    "end": "2048770"
  },
  {
    "text": "choices of b. So the algebra itself is\nnot very interesting.",
    "start": "2048770",
    "end": "2055590"
  },
  {
    "text": "What is really interesting is\nthe nature of the result that we get here.",
    "start": "2055590",
    "end": "2061179"
  },
  {
    "text": "If we were to plot the result on\nthis particular example you",
    "start": "2061179",
    "end": "2066260"
  },
  {
    "text": "would get the curve that's\nsomething like this.",
    "start": "2066260",
    "end": "2072280"
  },
  {
    "text": " It goes through the middle\nof this diagram",
    "start": "2072280",
    "end": "2080710"
  },
  {
    "text": "and is a little slanted. In this example, X and Theta\nare positively correlated.",
    "start": "2080710",
    "end": "2088638"
  },
  {
    "text": "Bigger values of X generally\ncorrespond to bigger values of Theta. So in this example the\ncovariance between X and Theta",
    "start": "2088639",
    "end": "2096309"
  },
  {
    "text": "is positive, and so our estimate\ncan be interpreted in",
    "start": "2096310",
    "end": "2105530"
  },
  {
    "text": "the following way: The expected\nvalue of Theta is the estimate that you would come up\nwith if you didn't have any",
    "start": "2105530",
    "end": "2113130"
  },
  {
    "text": "information about Theta. If you don't make any\nobservations this is the best",
    "start": "2113130",
    "end": "2119589"
  },
  {
    "text": "way of estimating Theta. But I have made an observation,\nX, and I need to",
    "start": "2119590",
    "end": "2126190"
  },
  {
    "text": "take it into account. I look at this difference, which\nis the piece of news",
    "start": "2126190",
    "end": "2132359"
  },
  {
    "text": "contained in X? That's what X should\nbe on the average.",
    "start": "2132360",
    "end": "2137869"
  },
  {
    "text": "If I observe an X which is\nbigger than what I expected it to be, and since X and Theta\nare positively correlated,",
    "start": "2137870",
    "end": "2146829"
  },
  {
    "text": "this tells me that Theta should\nalso be bigger than its average value.",
    "start": "2146830",
    "end": "2152690"
  },
  {
    "text": "Whenever I see an X that's\nlarger than its average value this gives me an indication\nthat theta should also",
    "start": "2152690",
    "end": "2160230"
  },
  {
    "text": "probably be larger than\nits average value. And so I'm taking that\ndifference and multiplying it",
    "start": "2160230",
    "end": "2168040"
  },
  {
    "text": "by a positive coefficient. And that's what gives\nme a curve here that has a positive slope.",
    "start": "2168040",
    "end": "2174880"
  },
  {
    "text": "So this increment-- the new information contained\nin X as compared to the",
    "start": "2174880",
    "end": "2181750"
  },
  {
    "text": "average value we expected\napriori, that increment allows us to make a correction to our\nprior estimate of Theta, and",
    "start": "2181750",
    "end": "2190780"
  },
  {
    "text": "the amount of that correction is\nguided by the covariance of X with Theta.",
    "start": "2190780",
    "end": "2196260"
  },
  {
    "text": "If the covariance of X with\nTheta were 0, that would mean there's no systematic relation\nbetween the two, and in that",
    "start": "2196260",
    "end": "2203049"
  },
  {
    "text": "case obtaining some information\nfrom X doesn't give us a guide as to how to\nchange the estimates of Theta.",
    "start": "2203050",
    "end": "2211009"
  },
  {
    "text": "If that were 0, we would\njust stay with this particular estimate. We're not able to make\na correction.",
    "start": "2211010",
    "end": "2217090"
  },
  {
    "text": "But when there's a non zero\ncovariance between X and Theta that covariance works as a\nguide for us to obtain a",
    "start": "2217090",
    "end": "2224620"
  },
  {
    "text": "better estimate of Theta. ",
    "start": "2224620",
    "end": "2232269"
  },
  {
    "text": "How about the resulting\nmean squared error? In this context turns out that\nthere's a very nice formula",
    "start": "2232270",
    "end": "2238690"
  },
  {
    "text": "for the mean squared\nerror obtained from the best linear estimate.",
    "start": "2238690",
    "end": "2244780"
  },
  {
    "text": "What's the story here? The mean squared error that we\nhave has something to do with",
    "start": "2244780",
    "end": "2251210"
  },
  {
    "text": "the variance of the original\nrandom variable. The more uncertain our original\nrandom variable is,",
    "start": "2251210",
    "end": "2258710"
  },
  {
    "text": "the more error we're\ngoing to make. On the other hand, when the two\nvariables are correlated",
    "start": "2258710",
    "end": "2265589"
  },
  {
    "text": "we explored that correlation\nto improve our estimate. ",
    "start": "2265590",
    "end": "2272100"
  },
  {
    "text": "This row here is the correlation\ncoefficient between the two random\nvariables. When this correlation\ncoefficient is larger this",
    "start": "2272100",
    "end": "2279720"
  },
  {
    "text": "factor here becomes smaller. And our mean squared error\nbecome smaller. So think of the two\nextreme cases.",
    "start": "2279720",
    "end": "2287559"
  },
  {
    "text": "One extreme case is when\nrho equal to 1 -- so X and Theta are perfectly\ncorrelated.",
    "start": "2287560",
    "end": "2294200"
  },
  {
    "text": "When they're perfectly\ncorrelated once I know X then I also know Theta.",
    "start": "2294200",
    "end": "2300309"
  },
  {
    "text": "And the two random variables\nare linearly related. In that case, my estimate is\nright on the target, and the",
    "start": "2300310",
    "end": "2307080"
  },
  {
    "text": "mean squared error\nis going to be 0. The other extreme case is\nif rho is equal to 0.",
    "start": "2307080",
    "end": "2314810"
  },
  {
    "text": "The two random variables\nare uncorrelated. In that case the measurement\ndoes not help me estimate",
    "start": "2314810",
    "end": "2321740"
  },
  {
    "text": "Theta, and the uncertainty\nthat's left-- the mean squared error--",
    "start": "2321740",
    "end": "2326970"
  },
  {
    "text": "is just the original\nvariance of Theta. So the uncertainty in Theta\ndoes not get reduced.",
    "start": "2326970",
    "end": "2333750"
  },
  {
    "text": "So moral-- the estimation error is a\nreduced version of the",
    "start": "2333750",
    "end": "2339710"
  },
  {
    "text": "original amount of uncertainty\nin the random variable Theta, and the larger the correlation\nbetween those two random",
    "start": "2339710",
    "end": "2348280"
  },
  {
    "text": "variables, the better we can\nremove uncertainty from the original random variable.",
    "start": "2348280",
    "end": "2353970"
  },
  {
    "text": " I didn't derive this formula,\nbut it's just a matter of",
    "start": "2353970",
    "end": "2361200"
  },
  {
    "text": "algebraic manipulations. We have a formula for\nTheta hat, subtract Theta from that formula.",
    "start": "2361200",
    "end": "2367620"
  },
  {
    "text": "Take square, take expectations,\nand do a few lines of algebra that you can\nread in the text, and you end",
    "start": "2367620",
    "end": "2373750"
  },
  {
    "text": "up with this really neat\nand clean formula.  Now I mentioned in the beginning\nof the lecture that",
    "start": "2373750",
    "end": "2382359"
  },
  {
    "text": "we can do inference with Theta's\nand X's not just being single numbers, but they could\nbe vector random variables.",
    "start": "2382360",
    "end": "2388970"
  },
  {
    "text": "So for example we might have\nmultiple data that gives us information about X.",
    "start": "2388970",
    "end": "2396710"
  },
  {
    "text": "There are no vectors here, so\nthis discussion was for the case where Theta and X were just\nscalar, one-dimensional",
    "start": "2396710",
    "end": "2404460"
  },
  {
    "text": "quantities. What do we do if we have\nmultiple data? Suppose that Theta is still a\nscalar, it's one dimensional,",
    "start": "2404460",
    "end": "2411990"
  },
  {
    "text": "but we make several\nobservations. And on the basis of these\nobservations we want to",
    "start": "2411990",
    "end": "2417050"
  },
  {
    "text": "estimate Theta. The optimal least mean squares\nestimator would be again the",
    "start": "2417050",
    "end": "2424650"
  },
  {
    "text": "conditional expectation of\nTheta given X. That's the optimal one.",
    "start": "2424650",
    "end": "2430130"
  },
  {
    "text": "And in this case X is a\nvector, so the general",
    "start": "2430130",
    "end": "2436329"
  },
  {
    "text": "estimator we would use\nwould be this one. But if we want to keep things\nsimple and we want our",
    "start": "2436330",
    "end": "2444049"
  },
  {
    "text": "estimator to have a simple\nfunctional form we might restrict to estimator that are\nlinear functions of the data.",
    "start": "2444050",
    "end": "2451870"
  },
  {
    "text": "And then the story is\nexactly the same as we discussed before.",
    "start": "2451870",
    "end": "2457010"
  },
  {
    "text": "I constrained myself to\nestimating Theta using a linear function of the data,\nso my signal processing box",
    "start": "2457010",
    "end": "2465880"
  },
  {
    "text": "just applies a linear\nfunction. And I'm looking for the best\ncoefficients, the coefficients",
    "start": "2465880",
    "end": "2471145"
  },
  {
    "text": "that are going to result\nin the least possible squared error. This is my squared error, this\nis (my estimate minus the",
    "start": "2471145",
    "end": "2479780"
  },
  {
    "text": "thing I'm trying to estimate)\nsquared, and then taking the average. How do we do this?",
    "start": "2479780",
    "end": "2485330"
  },
  {
    "text": "Same story as before.  The X's and the Theta's get\naveraged out because we have",
    "start": "2485330",
    "end": "2492500"
  },
  {
    "text": "an expectation. Whatever is left is just a\nfunction of the coefficients of the a's and of b's.",
    "start": "2492500",
    "end": "2498760"
  },
  {
    "text": "As before it turns out to\nbe a quadratic function. Then we set the derivatives of\nthis function of a's and b's",
    "start": "2498760",
    "end": "2506580"
  },
  {
    "text": "with respect to the\ncoefficients, we set it to 0. And this gives us a system\nof linear equations.",
    "start": "2506580",
    "end": "2514340"
  },
  {
    "text": "It's a system of linear\nequations that's satisfied by those coefficients. It's a linear system because\nthis is a quadratic function",
    "start": "2514340",
    "end": "2520859"
  },
  {
    "text": "of those coefficients. So to get closed-form formulas\nin this particular case one",
    "start": "2520860",
    "end": "2530410"
  },
  {
    "text": "would need to introduce vectors,\nand matrices, and metrics inverses and so on. The particular formulas are not\nso much what interests us",
    "start": "2530410",
    "end": "2538570"
  },
  {
    "text": "here, rather, the interesting\nthing is that this is simply done just using straightforward\nsolvers of",
    "start": "2538570",
    "end": "2547119"
  },
  {
    "text": "linear equations. The only thing you need to do\nis to write down the correct",
    "start": "2547120",
    "end": "2552470"
  },
  {
    "text": "coefficients of those non-linear\nequations. And the typical coefficient\nthat you would get would be what?",
    "start": "2552470",
    "end": "2559240"
  },
  {
    "text": "Let say a typical quick\nequations would be -- let's take a typical\nterm of this quadratic one you expanded.",
    "start": "2559240",
    "end": "2565680"
  },
  {
    "text": "You're going to get the terms\nsuch as a1x1 times a2x2.",
    "start": "2565680",
    "end": "2571470"
  },
  {
    "text": "When you take expectations\nyou're left with a1a2 times expected value of x1x2.",
    "start": "2571470",
    "end": "2578210"
  },
  {
    "text": " So this would involve terms such\nas a1 squared expected",
    "start": "2578210",
    "end": "2586700"
  },
  {
    "text": "value of x1 squared. You would get terms such as\na1a2, expected value of x1x2,",
    "start": "2586700",
    "end": "2594760"
  },
  {
    "text": "and a lot of other terms\nhere should have a too.",
    "start": "2594760",
    "end": "2600120"
  },
  {
    "text": "So you get something that's\nquadratic in your coefficients. And the constants that show up\nin your system of equations",
    "start": "2600120",
    "end": "2610490"
  },
  {
    "text": "are things that have to do with\nthe expected values of squares of your random\nvariables, or products of your",
    "start": "2610490",
    "end": "2617070"
  },
  {
    "text": "random variables. To write down the numerical\nvalues for these the only",
    "start": "2617070",
    "end": "2623059"
  },
  {
    "text": "thing you need to know are the\nmeans and variances of your random variables. If you know the mean and\nvariance then you know what",
    "start": "2623060",
    "end": "2630360"
  },
  {
    "text": "this thing is. And if you know the covariances\nas well then you know what this thing is.",
    "start": "2630360",
    "end": "2637250"
  },
  {
    "text": "So in order to find the optimal\nlinear estimator in the case of multiple data you do\nnot need to know the entire",
    "start": "2637250",
    "end": "2646870"
  },
  {
    "text": "probability distribution\nof the random variables that are involved. You only need to know your\nmeans and covariances.",
    "start": "2646870",
    "end": "2654690"
  },
  {
    "text": "These are the only quantities\nthat affect the construction of your optimal estimator.",
    "start": "2654690",
    "end": "2660569"
  },
  {
    "text": "We could see this already\nin this formula. The form of my optimal estimator\nis completely",
    "start": "2660570",
    "end": "2669650"
  },
  {
    "text": "determined once I know the\nmeans, variance, and covariance of the random\nvariables in my model.",
    "start": "2669650",
    "end": "2677970"
  },
  {
    "text": "I do not need to know how the\ndetails distribution of the",
    "start": "2677970",
    "end": "2684410"
  },
  {
    "text": "random variables that\nare involved here. ",
    "start": "2684410",
    "end": "2691690"
  },
  {
    "text": "So as I said in general, you\nfind the form of the optimal estimator by using a linear\nequation solver.",
    "start": "2691690",
    "end": "2699550"
  },
  {
    "text": "There are special examples\nin which you can get closed-form solutions.",
    "start": "2699550",
    "end": "2705210"
  },
  {
    "text": "The nicest simplest estimation\nproblem one can think of is the following--",
    "start": "2705210",
    "end": "2711120"
  },
  {
    "text": "you have some uncertain\nparameter, and you make multiple measurements\nof that parameter in",
    "start": "2711120",
    "end": "2717790"
  },
  {
    "text": "the presence of noise. So the Wi's are noises. I corresponds to your\ni-th experiment.",
    "start": "2717790",
    "end": "2725130"
  },
  {
    "text": "So this is the most common\nsituation that you encounter in the lab. If you are dealing with some\nprocess, you're trying to",
    "start": "2725130",
    "end": "2731240"
  },
  {
    "text": "measure something you measure\nit over and over. Each time your measurement\nhas some random error.",
    "start": "2731240",
    "end": "2737030"
  },
  {
    "text": "And then you need to take all\nyour measurements together and come up with a single\nestimate.",
    "start": "2737030",
    "end": "2743550"
  },
  {
    "text": "So the noises are assumed to be\nindependent of each other, and also to be independent\nfrom the",
    "start": "2743550",
    "end": "2750010"
  },
  {
    "text": "value of the true parameter. Without loss of generality we\ncan assume that the noises have 0 mean and they have\nsome variances that we",
    "start": "2750010",
    "end": "2758890"
  },
  {
    "text": "assume to be known. Theta itself has a prior\ndistribution with a certain mean and the certain variance.",
    "start": "2758890",
    "end": "2765670"
  },
  {
    "text": "So the form of the\noptimal linear estimator is really nice.",
    "start": "2765670",
    "end": "2770940"
  },
  {
    "text": "Well maybe you cannot see it\nright away because this looks messy, but what is it really?",
    "start": "2770940",
    "end": "2778579"
  },
  {
    "text": "It's a linear combination of\nthe X's and the prior mean.",
    "start": "2778580",
    "end": "2784590"
  },
  {
    "text": "And it's actually a weighted\naverage of the X's and the prior mean.",
    "start": "2784590",
    "end": "2790250"
  },
  {
    "text": "Here we collect all of\nthe coefficients that we have at the top.",
    "start": "2790250",
    "end": "2795920"
  },
  {
    "text": "So the whole thing is basically\na weighted average.",
    "start": "2795920",
    "end": "2802059"
  },
  {
    "text": " 1/(sigma_i-squared) is the\nweight that we give to Xi, and",
    "start": "2802060",
    "end": "2811109"
  },
  {
    "text": "in the denominator we have the\nsum of all of the weights. So in the end we're dealing\nwith a weighted average.",
    "start": "2811110",
    "end": "2819260"
  },
  {
    "text": "If mu was equal to 1, and all\nthe Xi's were equal to 1 then our estimate would also\nbe equal to 1.",
    "start": "2819260",
    "end": "2826790"
  },
  {
    "text": "Now the form of the weights that\nwe have is interesting. Any given data point is\nweighted inversely",
    "start": "2826790",
    "end": "2836049"
  },
  {
    "text": "proportional to the variance. What does that say? If my i-th data point has a lot\nof variance, if Wi is very",
    "start": "2836050",
    "end": "2846920"
  },
  {
    "text": "noisy then Xi is not very\nuseful, is not very reliable.",
    "start": "2846920",
    "end": "2852900"
  },
  {
    "text": "So I'm giving it\na small weight. Large variance, a lot of error\nin my Xi means that I should",
    "start": "2852900",
    "end": "2861870"
  },
  {
    "text": "give it a smaller weight. If two data points have the\nsame variance, they're of",
    "start": "2861870",
    "end": "2867920"
  },
  {
    "text": "comparable quality,\nthen I'm going to give them equal weight. The other interesting thing is\nthat the prior mean is treated",
    "start": "2867920",
    "end": "2876200"
  },
  {
    "text": "the same way as the X's. So it's treated as an additional\nobservation.",
    "start": "2876200",
    "end": "2883050"
  },
  {
    "text": "So we're taking a weighted\naverage of the prior mean and of the measurements that\nwe are making.",
    "start": "2883050",
    "end": "2889850"
  },
  {
    "text": "The formula looks as if the\nprior mean was just another data point. So that's the way of thinking\nabout Bayesian estimation.",
    "start": "2889850",
    "end": "2897440"
  },
  {
    "text": "You have your real data points,\nthe X's that you observe, you also had some\nprior information.",
    "start": "2897440",
    "end": "2903430"
  },
  {
    "text": "This plays a role similar\nto a data point. Interesting note that if all\nrandom variables are normal in",
    "start": "2903430",
    "end": "2911580"
  },
  {
    "text": "this model these optimal linear\nestimator happens to be also the conditional\nexpectation.",
    "start": "2911580",
    "end": "2916950"
  },
  {
    "text": "That's the nice thing about\nnormal random variables that conditional expectations\nturn out to be linear.",
    "start": "2916950",
    "end": "2922770"
  },
  {
    "text": "So the optimal estimate and the\noptimal linear estimate turn out to be the same.",
    "start": "2922770",
    "end": "2928559"
  },
  {
    "text": "And that gives us another\ninterpretation of linear estimation. Linear estimation is essentially\nthe same as",
    "start": "2928560",
    "end": "2934660"
  },
  {
    "text": "pretending that all random\nvariables are normal. So that's a side point.",
    "start": "2934660",
    "end": "2942040"
  },
  {
    "text": "Now I'd like to close\nwith a comment. ",
    "start": "2942040",
    "end": "2948369"
  },
  {
    "text": "You do your measurements and\nyou estimate Theta on the basis of X. Suppose that instead\nyou have a measuring",
    "start": "2948370",
    "end": "2957040"
  },
  {
    "text": "device that's measures X-cubed\ninstead of measuring X, and you want to estimate Theta.",
    "start": "2957040",
    "end": "2963349"
  },
  {
    "text": "Are you going to get to\ndifferent a estimate? Well X and X-cubed contained\nthe same information.",
    "start": "2963350",
    "end": "2971790"
  },
  {
    "text": "Telling you X is the\nsame as telling you the value of X-cubed. So the posterior distribution\nof Theta given X is the same",
    "start": "2971790",
    "end": "2980660"
  },
  {
    "text": "as the posterior distribution\nof Theta given X-cubed. And so the means of these\nposterior distributions are",
    "start": "2980660",
    "end": "2987450"
  },
  {
    "text": "going to be the same. So doing transformations through\nyour data does not",
    "start": "2987450",
    "end": "2992849"
  },
  {
    "text": "matter if you're doing optimal\nleast squares estimation. On the other hand, if you\nrestrict yourself to doing",
    "start": "2992850",
    "end": "3000100"
  },
  {
    "text": "linear estimation then using a\nlinear function of X is not",
    "start": "3000100",
    "end": "3005540"
  },
  {
    "text": "the same as using a linear\nfunction of X-cubed. So this is a linear estimator,\nbut where the data are the",
    "start": "3005540",
    "end": "3014720"
  },
  {
    "text": "X-cube's, and we have a linear\nfunction of the data. So this means that when you're\nusing linear estimation you",
    "start": "3014720",
    "end": "3023690"
  },
  {
    "text": "have some choices to make\nlinear on what? Sometimes you want to plot your\ndata on a not ordinary",
    "start": "3023690",
    "end": "3032290"
  },
  {
    "text": "scale and try to plot\na line through them. Sometimes you plot your data\non a logarithmic scale, and",
    "start": "3032290",
    "end": "3038360"
  },
  {
    "text": "try to plot a line\nthrough them. Which scale is the\nappropriate one? Here it would be\na cubic scale.",
    "start": "3038360",
    "end": "3044510"
  },
  {
    "text": "And you have to think about\nyour particular model to decide which version would be\na more appropriate one.",
    "start": "3044510",
    "end": "3051180"
  },
  {
    "text": "Finally when we have multiple\ndata sometimes these multiple data might contain the\nsame information.",
    "start": "3051180",
    "end": "3059910"
  },
  {
    "text": "So X is one data point,\nX-squared is another data point, X-cubed is another\ndata point.",
    "start": "3059910",
    "end": "3065610"
  },
  {
    "text": "The three of them contain the\nsame information, but you can try to form a linear\nfunction of them.",
    "start": "3065610",
    "end": "3071480"
  },
  {
    "text": "And then you obtain a linear\nestimator that has a more general form as a\nfunction of X.",
    "start": "3071480",
    "end": "3076930"
  },
  {
    "text": "So if you want to estimate your\nTheta as a cubic function",
    "start": "3076930",
    "end": "3082130"
  },
  {
    "text": "of X, for example, you can set\nup a linear estimation model of this particular form and find\nthe optimal coefficients,",
    "start": "3082130",
    "end": "3089480"
  },
  {
    "text": "the a's and the b's. All right, so the last slide\njust gives you the big picture",
    "start": "3089480",
    "end": "3095700"
  },
  {
    "text": "of what's happening in Bayesian\nInference, it's for you to ponder. Basically we talked about three",
    "start": "3095700",
    "end": "3101930"
  },
  {
    "text": "possible estimation methods. Maximum posteriori, mean squared\nerror estimation, and",
    "start": "3101930",
    "end": "3108300"
  },
  {
    "text": "linear mean squared error\nestimation, or least squares estimation. And there's a number of standard\nexamples that you",
    "start": "3108300",
    "end": "3114410"
  },
  {
    "text": "will be seeing over and over in\nthe recitations, tutorial, homework, and so on, perhaps\non exams even.",
    "start": "3114410",
    "end": "3120950"
  },
  {
    "text": "Where we take some nice priors\non some unknown parameter, we take some nice models for the\nnoise or the observations, and",
    "start": "3120950",
    "end": "3129410"
  },
  {
    "text": "then you need to work out\nposterior distributions in the various estimates and\ncompare them. ",
    "start": "3129410",
    "end": "3135040"
  }
]