[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13339"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13340",
    "end": "21519"
  },
  {
    "text": "PROFESSOR: OK, so\ngood afternoon. Today, we will review\nprobability theory.",
    "start": "21520",
    "end": "30820"
  },
  {
    "text": "So I will mostly focus on-- I'll\ngive you some distributions.",
    "start": "30820",
    "end": "36090"
  },
  {
    "text": "So probabilistic distributions,\nthat will be of interest to us throughout the course. And I will talk about\nmoment-generating function",
    "start": "36090",
    "end": "44610"
  },
  {
    "text": "a little bit. Afterwards, I will talk\nabout law of large numbers",
    "start": "44610",
    "end": "50660"
  },
  {
    "text": "and central limit theorem. ",
    "start": "50660",
    "end": "56309"
  },
  {
    "text": "Who has heard of all\nof these topics before? OK.",
    "start": "56310",
    "end": "62150"
  },
  {
    "text": "That's good. Then I'll try to focus\nmore on a little bit more of the advanced stuff.",
    "start": "62150",
    "end": "67540"
  },
  {
    "text": " Then a big part of it\nwill be review for you.",
    "start": "67540",
    "end": "73830"
  },
  {
    "text": "So first of all, just to\nagree on terminology, let's review some definitions.",
    "start": "73830",
    "end": "81490"
  },
  {
    "text": "So a random variable\nX-- we will talk",
    "start": "81490",
    "end": "92670"
  },
  {
    "text": "about discrete and\ncontinuous random variables.",
    "start": "92670",
    "end": "98900"
  },
  {
    "text": " Just to set up the notation,\nI will write discrete as X",
    "start": "98900",
    "end": "107240"
  },
  {
    "text": "and continuous random\nvariable as Y for now. So they are given by its\nprobability distribution--",
    "start": "107240",
    "end": "112820"
  },
  {
    "text": "discrete random variable is\ngiven by its probability mass function, f sub\nX, I will denote.",
    "start": "112820",
    "end": "122490"
  },
  {
    "text": "And continuous is given by\nprobability distribution function. ",
    "start": "122490",
    "end": "131530"
  },
  {
    "text": "I will denote by f\nsub Y. So pmf and pdf.",
    "start": "131530",
    "end": "137745"
  },
  {
    "text": " Here, I just use a\nsubscript because I",
    "start": "137745",
    "end": "143930"
  },
  {
    "text": "wanted to distinguish\nf sub x and f sub y. But when it's clear which random\nvariable we're talking about,",
    "start": "143930",
    "end": "149140"
  },
  {
    "text": "I'll just say f. So what is this? A probability mass function is\na function from the sample space",
    "start": "149140",
    "end": "162980"
  },
  {
    "text": "to non-negative reals such\nthat the sum over all points",
    "start": "162980",
    "end": "170290"
  },
  {
    "text": "in the domain equals 1. The probability distribution\nis very similar.",
    "start": "170290",
    "end": "177110"
  },
  {
    "text": " The function from the\nsample space non-negative",
    "start": "177110",
    "end": "182890"
  },
  {
    "text": "reals, but now the\nintegration over the domain. ",
    "start": "182890",
    "end": "191780"
  },
  {
    "text": "So it's pretty much safe to\nconsider our sample space to be the real numbers for\ncontinuous random variables.",
    "start": "191780",
    "end": "200569"
  },
  {
    "text": "Later in the course, you\nwill see some examples where it's not the real numbers. But for now, just consider\nit as real numbers.",
    "start": "200570",
    "end": "209217"
  },
  {
    "start": "209217",
    "end": "214840"
  },
  {
    "text": "For example, probability\nmass function. If X takes 1 with\nprobability 1/3,",
    "start": "214840",
    "end": "226810"
  },
  {
    "text": "minus 1 with probability 1/3,\nand 0 with probability 1/3.",
    "start": "226810",
    "end": "233010"
  },
  {
    "text": " Then our probability mass\nfunction is f_x(1) equals",
    "start": "233010",
    "end": "241464"
  },
  {
    "text": "f_x(-1), 1/3, just like that.",
    "start": "241464",
    "end": "248370"
  },
  {
    "text": "An example of a\ncontinuous random variable is if-- let's say, for\nexample, if f sub Y is",
    "start": "248370",
    "end": "257470"
  },
  {
    "text": "equal to 1 for all\ny in [0,1], then",
    "start": "257470",
    "end": "265420"
  },
  {
    "text": "this is pdf of uniform\nrandom variable",
    "start": "265420",
    "end": "276305"
  },
  {
    "text": "where the space is [0,1]. So this random variable\njust picks one out",
    "start": "276305",
    "end": "281849"
  },
  {
    "text": "of the three numbers\nwith equal probability. This picks one out of this,\nall the real numbers between 0",
    "start": "281850",
    "end": "287449"
  },
  {
    "text": "and 1, with equal probability. These are just some basic stuff.",
    "start": "287450",
    "end": "294956"
  },
  {
    "text": "You should be\nfamiliar with this, but I wrote it down just so\nthat we agree on the notation.",
    "start": "294956",
    "end": "300934"
  },
  {
    "text": "OK. Both of the boards don't slide. That's good.",
    "start": "300934",
    "end": "306311"
  },
  {
    "text": "A few more stuff. Expectation-- probability first.",
    "start": "306311",
    "end": "314530"
  },
  {
    "text": "Probability of an event can be\ncomputed as probability of A",
    "start": "314530",
    "end": "322092"
  },
  {
    "text": "is equal to either sum of all\npoints in A-- this probability",
    "start": "322092",
    "end": "328200"
  },
  {
    "text": "mass function-- or\nintegral over the set A",
    "start": "328200",
    "end": "336700"
  },
  {
    "text": "depending on what you're using. And expectation, or mean\nis-- expectation of X",
    "start": "336700",
    "end": "350050"
  },
  {
    "text": "is equal to the sum over\nall x, x times that.",
    "start": "350050",
    "end": "355409"
  },
  {
    "text": "And expectation of Y is\nthe integral over omega.",
    "start": "355410",
    "end": "361110"
  },
  {
    "text": "Oh, sorry. Space. y times. ",
    "start": "361110",
    "end": "371016"
  },
  {
    "text": "OK. And one more basic\nconcept I'd like to review",
    "start": "371016",
    "end": "376849"
  },
  {
    "text": "is two random variables X_1, X_2\nare independent if probability",
    "start": "376850",
    "end": "392150"
  },
  {
    "text": "that X_1 is in A and\nX_2 is in B equals",
    "start": "392150",
    "end": "398220"
  },
  {
    "text": "the product of the\nprobabilities, for all events A",
    "start": "398220",
    "end": "408898"
  },
  {
    "text": "and B. OK.",
    "start": "408898",
    "end": "414222"
  },
  {
    "text": " All agreed?",
    "start": "414222",
    "end": "419569"
  },
  {
    "text": "So for independence, I will\ntalk about independence of several random\nvariables as well. There are two concepts\nof independence--",
    "start": "419570",
    "end": "429290"
  },
  {
    "text": "not two, but several. The two most popular are\nmutually independent events",
    "start": "429290",
    "end": "437220"
  },
  {
    "text": "and pairwise independent events. ",
    "start": "437220",
    "end": "443582"
  },
  {
    "text": "Can somebody tell me the\ndifference between these two for several variables?",
    "start": "443583",
    "end": "448865"
  },
  {
    "text": " Yes?",
    "start": "448865",
    "end": "454200"
  },
  {
    "text": "AUDIENCE: So\nusually, independent means all the random\nvariables are independent, like X_1 is independent\nwith every others.",
    "start": "454200",
    "end": "462550"
  },
  {
    "text": "But pairwise means X_1\nand X_2 are independent, but X_1, X_2, and x_3, they\nmay not be independent.",
    "start": "462550",
    "end": "471676"
  },
  {
    "text": "PROFESSOR: OK. Maybe-- yeah. So that's good.",
    "start": "471677",
    "end": "477020"
  },
  {
    "text": "So let's see-- for the example\nof three random variables,",
    "start": "477020",
    "end": "484419"
  },
  {
    "text": "it might be the case that\neach pair are independent. X_1 and X_2 X_1 is\nindependent with X_2,",
    "start": "484420",
    "end": "490110"
  },
  {
    "text": "X_1 is independent with\nX_3, X_2 is with X_3. But altogether, it's\nnot independent.",
    "start": "490110",
    "end": "495290"
  },
  {
    "text": "What that means is, this type\nof statement is not true.",
    "start": "495290",
    "end": "500780"
  },
  {
    "text": "So there are say A_1, A_2, A_3\nfor which this does not hold. But that's just some\ntechnical detail.",
    "start": "500780",
    "end": "508150"
  },
  {
    "text": "We will mostly just consider\nmutually independent events. So when we say that several\nrandom variables are independent, it just means\nwhatever collection you take,",
    "start": "508150",
    "end": "516630"
  },
  {
    "text": "they're all independent. ",
    "start": "516630",
    "end": "523995"
  },
  {
    "text": "OK. So a little bit more fun\nstuff [? in this ?] overview. ",
    "start": "523995",
    "end": "530640"
  },
  {
    "text": "So we defined random variables. And one of the most\nuniversal random variable,",
    "start": "530640",
    "end": "539060"
  },
  {
    "text": "or distribution, is a\nnormal distribution. ",
    "start": "539060",
    "end": "550920"
  },
  {
    "text": "It's a continuous\nrandom variable. Our continuous random variable\nhas normal distribution,",
    "start": "550920",
    "end": "561160"
  },
  {
    "text": "is said to have normal\ndistribution, if-- N(mu,",
    "start": "561160",
    "end": "569834"
  },
  {
    "text": "sigma)-- if the probability\ndistribution function is given",
    "start": "569835",
    "end": "580380"
  },
  {
    "text": "as 1 over sigma\nsquare root 2 pi,",
    "start": "580380",
    "end": "586820"
  },
  {
    "text": "e to the minus x\nminus mu squared. ",
    "start": "586820",
    "end": "597270"
  },
  {
    "text": "For all reals. OK?",
    "start": "597270",
    "end": "604146"
  },
  {
    "text": "So mu mean over--\nthat's one of the most",
    "start": "604146",
    "end": "612500"
  },
  {
    "text": "universal random variables--\ndistributions, the most important one as well.",
    "start": "612500",
    "end": "618100"
  },
  {
    "start": "618100",
    "end": "628990"
  },
  {
    "text": "OK. So this distribution, how\nit looks like-- I'm sure you saw this bell curve before.",
    "start": "628990",
    "end": "636043"
  },
  {
    "text": "It looks like this if\nit's N(0,1), let's say.",
    "start": "636043",
    "end": "642351"
  },
  {
    "text": "And that's your y. So it's centered\naround the origin,",
    "start": "642351",
    "end": "648360"
  },
  {
    "text": "and it's symmetrical\non the origin. So now let's look\nat our purpose.",
    "start": "648360",
    "end": "655290"
  },
  {
    "text": "Let's think about our purpose. We want to model a financial\nproduct or a stock,",
    "start": "655290",
    "end": "661940"
  },
  {
    "text": "the price of the stock,\nusing some random variable. The first thing you can try\nis to use normal distribution.",
    "start": "661940",
    "end": "669065"
  },
  {
    "text": "Normal distribution\ndoesn't make sense, but we can say the price at\nday n minus the price at day n",
    "start": "669065",
    "end": "679586"
  },
  {
    "text": "minus 1 is normal distribution. ",
    "start": "679586",
    "end": "685575"
  },
  {
    "text": "Is this a sensible definition? That's not really.",
    "start": "685575",
    "end": "690637"
  },
  {
    "text": "So it's not a good choice. You can model it like this,\nbut it's not a good choice.",
    "start": "690637",
    "end": "695810"
  },
  {
    "text": "There may be several\nreasons, but one reason is that it doesn't take into\naccount the order of magnitude",
    "start": "695810",
    "end": "700860"
  },
  {
    "text": "of the price itself. So the stock-- let's say\nyou have a stock price that",
    "start": "700860",
    "end": "709487"
  },
  {
    "text": "goes something like that. And say it was $10\nhere, and $50 here.",
    "start": "709487",
    "end": "718620"
  },
  {
    "text": "Regardless of where\nyour position is at, it says that the increment,\nthe absolute value of increment",
    "start": "718620",
    "end": "725900"
  },
  {
    "text": "is identically distributed at\nthis point and at this point.",
    "start": "725900",
    "end": "731080"
  },
  {
    "text": "But if you observed\nhow it works, usually that's not\nnormally distributed.",
    "start": "731080",
    "end": "738040"
  },
  {
    "text": "What's normally distributed\nis the percentage of how much it changes daily.",
    "start": "738040",
    "end": "744610"
  },
  {
    "text": "So this is not a sensible\nmodel, not a good model.",
    "start": "744610",
    "end": "752125"
  },
  {
    "text": " But still, we can use\nnormal distribution",
    "start": "752125",
    "end": "761199"
  },
  {
    "text": "to come up with a\npretty good model. ",
    "start": "761200",
    "end": "769170"
  },
  {
    "text": "So instead, what we want\nis a relative difference",
    "start": "769170",
    "end": "786130"
  },
  {
    "text": "to be normally distributed. ",
    "start": "786130",
    "end": "795680"
  },
  {
    "text": "That is the percent. ",
    "start": "795680",
    "end": "806759"
  },
  {
    "text": "The question is, what is\nthe distribution of price?",
    "start": "806760",
    "end": "813150"
  },
  {
    "text": "What does the\ndistribution of price? ",
    "start": "813150",
    "end": "825750"
  },
  {
    "text": "So it's not a very\ngood explanation. Because I'm giving just\ndiscrete increments while",
    "start": "825750",
    "end": "832860"
  },
  {
    "text": "these are continuous\nrandom variables and so on. But what I'm trying to say here\nis that normal distribution",
    "start": "832860",
    "end": "839030"
  },
  {
    "text": "is not good enough. Instead, we want the\npercentage change to be normally distributed.",
    "start": "839030",
    "end": "845450"
  },
  {
    "text": "And if that is the case,\nwhat will be the distribution",
    "start": "845450",
    "end": "851300"
  },
  {
    "text": "of the random variable? In this case, what will be\nthe distribution of the price? ",
    "start": "851300",
    "end": "867420"
  },
  {
    "text": "One thing I should\nmention is, in this case, if each discrement is\nnormally distributed,",
    "start": "867420",
    "end": "874230"
  },
  {
    "text": "then the price at\nday n will still",
    "start": "874230",
    "end": "879529"
  },
  {
    "text": "be a normal random variable\ndistributed like that. ",
    "start": "879530",
    "end": "887440"
  },
  {
    "text": "So if there's no tendency-- if\nthe average daily increment is",
    "start": "887440",
    "end": "893900"
  },
  {
    "text": "0, then no matter\nhow far you go, your random variable will\nbe normally distributed.",
    "start": "893900",
    "end": "898915"
  },
  {
    "text": " But here, that will\nnot be the case.",
    "start": "898915",
    "end": "906110"
  },
  {
    "text": "So we want to see what\nthe distribution of P_n will be in this case.",
    "start": "906110",
    "end": "911980"
  },
  {
    "text": "OK. ",
    "start": "911981",
    "end": "917820"
  },
  {
    "text": "To do that-- let me formally\nwrite down what I want to say.",
    "start": "917820",
    "end": "929300"
  },
  {
    "text": "What I want to say is this. I want to define a\nlog-normal distribution Y,",
    "start": "929300",
    "end": "946029"
  },
  {
    "text": "or log-normal random variable\nY, such that log of Y",
    "start": "946030",
    "end": "967274"
  },
  {
    "text": "is normally distributed. ",
    "start": "967274",
    "end": "984170"
  },
  {
    "text": "So to derive the probability\ndistribution of this from the normal\ndistribution, we can use the change of\nvariable formula, which",
    "start": "984170",
    "end": "1000009"
  },
  {
    "text": "says the following:\nsuppose X and Y",
    "start": "1000010",
    "end": "1007340"
  },
  {
    "text": "are random variables such\nthat-- probability of X",
    "start": "1007340",
    "end": "1036781"
  },
  {
    "text": "minus x-- for all x.",
    "start": "1036781",
    "end": "1046261"
  },
  {
    "start": "1046262",
    "end": "1052250"
  },
  {
    "text": "Then F of Y of y--\nthe first-- of f",
    "start": "1052250",
    "end": "1068218"
  },
  {
    "text": "sub X is equal to f sub Y of y. ",
    "start": "1068218",
    "end": "1078198"
  },
  {
    "text": "h of x. ",
    "start": "1078198",
    "end": "1087200"
  },
  {
    "text": "So let's try to fit\ninto this story. We want to have a\nrandom variable Y such",
    "start": "1087200",
    "end": "1094920"
  },
  {
    "text": "that log Y is\nnormally distributed. Here-- so you can\nput log of x here.",
    "start": "1094920",
    "end": "1106430"
  },
  {
    "text": "If Y is normally distributed,\nX will be the distribution that we're interested in.",
    "start": "1106430",
    "end": "1112890"
  },
  {
    "text": "So using this formula, we can\nfind probability distribution function of the log-normal\ndistribution using",
    "start": "1112890",
    "end": "1120650"
  },
  {
    "text": "the probability\ndistribution of normal. So let's do that. ",
    "start": "1120650",
    "end": "1145669"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE], right? PROFESSOR: Yes.",
    "start": "1145669",
    "end": "1152910"
  },
  {
    "text": "So it's not a good choice. Locally, it might\nbe good choice. But if it's taken\nover a long time,",
    "start": "1152910",
    "end": "1160357"
  },
  {
    "text": "it won't be a good choice. Because it will also take\nnegative values, for example. ",
    "start": "1160357",
    "end": "1168517"
  },
  {
    "text": "So if you just take\nthis model, what's going to happen over\na long period of time is it's going to hit\nthis square root of n,",
    "start": "1168517",
    "end": "1175730"
  },
  {
    "text": "negative square root of\nn line infinitely often. ",
    "start": "1175730",
    "end": "1182049"
  },
  {
    "text": "And then it can\ngo up to infinity, or it can go down to\ninfinity eventually.",
    "start": "1182050",
    "end": "1187470"
  },
  {
    "text": "So it will take negative\nvalues and positive values. ",
    "start": "1187470",
    "end": "1193309"
  },
  {
    "text": "That's one reason, but\nthere are several reasons why that's not a good choice. If you look at a\nvery small scale,",
    "start": "1193310",
    "end": "1199440"
  },
  {
    "text": "it might be OK, because the base\nprice doesn't change that much. So if you model\nin terms of ratio,",
    "start": "1199440",
    "end": "1205490"
  },
  {
    "text": "our if you model it\nin an absolute way, it doesn't matter that much. But if you want to do it a\nlittle bit more large scale,",
    "start": "1205490",
    "end": "1213850"
  },
  {
    "text": "then that's not a\nvery good choice. Other questions?",
    "start": "1213850",
    "end": "1220120"
  },
  {
    "text": "Do you want me to\nadd some explanation? ",
    "start": "1220120",
    "end": "1225322"
  },
  {
    "text": "OK.  So let me get this right.",
    "start": "1225322",
    "end": "1232720"
  },
  {
    "text": " Y. I want X to be-- yes.",
    "start": "1232720",
    "end": "1245440"
  },
  {
    "text": "I want X to be the log\nnormal distribution. ",
    "start": "1245440",
    "end": "1256950"
  },
  {
    "text": "And I want Y to be\nnormal distribution",
    "start": "1256950",
    "end": "1264580"
  },
  {
    "text": "or a normal random variable. Then the probability\nthat X is at most x",
    "start": "1264580",
    "end": "1272571"
  },
  {
    "text": "equals the probability\nthat Y is at most-- sigma.",
    "start": "1272572",
    "end": "1284500"
  },
  {
    "text": "Y is at most log x. That's the definition of\nlog-normal distribution.",
    "start": "1284500",
    "end": "1293159"
  },
  {
    "text": "Then by using this change\nof variable formula,",
    "start": "1293160",
    "end": "1299130"
  },
  {
    "text": "probability density\nfunction of X is equal to probability\ndensity function of Y at log",
    "start": "1299130",
    "end": "1306980"
  },
  {
    "text": "x times the differentiation\nof log x which is 1 over x.",
    "start": "1306980",
    "end": "1314440"
  },
  {
    "text": "So it becomes 1 over\nx sigma square root",
    "start": "1314440",
    "end": "1320460"
  },
  {
    "text": "2 pi, e to the minus\nlog x minus mu squared.",
    "start": "1320460",
    "end": "1327704"
  },
  {
    "text": " So log-normal\ndistribution can also",
    "start": "1327704",
    "end": "1333429"
  },
  {
    "text": "be defined as the\ndistribution which has probability mass function this. ",
    "start": "1333430",
    "end": "1342650"
  },
  {
    "text": "You can use either definition. Let me just make sure that I\ndidn't mess up in the middle.",
    "start": "1342650",
    "end": "1349391"
  },
  {
    "text": " Yes. And that only works\nfor x greater than 0.",
    "start": "1349391",
    "end": "1359187"
  },
  {
    "text": "Yes? AUDIENCE: [INAUDIBLE]? PROFESSOR: Yeah. So all logs are natural log. It should be ln.",
    "start": "1359187",
    "end": "1366170"
  },
  {
    "text": "Yeah. Thank you. OK. So question-- what's the mean\nof this distribution here?",
    "start": "1366171",
    "end": "1378370"
  },
  {
    "text": "Yeah? AUDIENCE: 1? PROFESSOR: Not 1. It might be mu.",
    "start": "1378370",
    "end": "1384820"
  },
  {
    "text": "Is it mu? Oh, sorry. It might be e to the mu.",
    "start": "1384820",
    "end": "1389850"
  },
  {
    "text": "Because log X, the normal\ndistribution had mean mu.",
    "start": "1389850",
    "end": "1395470"
  },
  {
    "text": "log x equals mu\nmight be the center. If that's the case, x is e\nto the mu will be the mean.",
    "start": "1395470",
    "end": "1400850"
  },
  {
    "text": "Is that the case? Yes? AUDIENCE: Can you get\nthe mu minus [INAUDIBLE]?",
    "start": "1400850",
    "end": "1407890"
  },
  {
    "text": "PROFESSOR: Probably right. I don't remember what's there. There is a correcting factor. I don't remember\nexactly what that is,",
    "start": "1407890",
    "end": "1414292"
  },
  {
    "text": "but I think you're right. So one very important\nthing to remember",
    "start": "1414292",
    "end": "1419770"
  },
  {
    "text": "is log-normal\ndistribution are referred to in terms of the\nparameters mu and sigma,",
    "start": "1419770",
    "end": "1428150"
  },
  {
    "text": "because that's the mu and\nsigma up here and here coming from the normal distribution. But those are not the\nmean and variance anymore,",
    "start": "1428150",
    "end": "1437580"
  },
  {
    "text": "because you skew\nthe distribution. It's no longer centered at mu.",
    "start": "1437580",
    "end": "1443700"
  },
  {
    "text": "log X is centered at mu, but\nwhen it takes exponential, it becomes skewed. And we take the average,\nyou'll see that the mean",
    "start": "1443700",
    "end": "1452630"
  },
  {
    "text": "is no longer e to the mu. So that doesn't give the mean. That doesn't imply that\nthe mean is e to the sigma.",
    "start": "1452630",
    "end": "1458490"
  },
  {
    "text": "That doesn't imply\nthat the variance is something like e to the sigma. That's just totally nonsense.",
    "start": "1458490",
    "end": "1467039"
  },
  {
    "text": "Just remember-- these are just\nparameters, some parameters. It's no longer mean or variance.",
    "start": "1467040",
    "end": "1472450"
  },
  {
    "text": " And in your homework,\none exercise,",
    "start": "1472450",
    "end": "1479794"
  },
  {
    "text": "we'll ask you to compute\nthe mean and variance of the random variable. But really, just try to\nhave it stick in your mind",
    "start": "1479794",
    "end": "1488559"
  },
  {
    "text": "that mu and sigma is no\nlonger mean and variance. That's only the case for\nnormal random variables.",
    "start": "1488560",
    "end": "1496230"
  },
  {
    "text": "And the reason we are\nstill using mu and sigma is because of this derivation. And it's easy to\ndescribe it in those.",
    "start": "1496230",
    "end": "1502390"
  },
  {
    "text": " OK.",
    "start": "1502390",
    "end": "1507940"
  },
  {
    "text": "So the normal distribution\nand log-normal distribution will probably be\nthe distributions",
    "start": "1507940",
    "end": "1513720"
  },
  {
    "text": "that you'll see the most\nthroughout the course. But there are some\nother distributions that you'll also see. ",
    "start": "1513720",
    "end": "1523460"
  },
  {
    "text": "I need this. ",
    "start": "1523460",
    "end": "1532884"
  },
  {
    "text": "I will not talk\nabout it in detail. It will be some\nexercise questions.",
    "start": "1532884",
    "end": "1538540"
  },
  {
    "text": "For example, you have Poisson\ndistribution or exponential",
    "start": "1538540",
    "end": "1544939"
  },
  {
    "text": "distributions. ",
    "start": "1544939",
    "end": "1552130"
  },
  {
    "text": "These are some other\ndistributions that you'll see. And all of these-- normal,\nlog-normal, Poisson,",
    "start": "1552130",
    "end": "1559060"
  },
  {
    "text": "and exponential,\nand a lot more can be grouped into a\nfamily of distributions",
    "start": "1559060",
    "end": "1564400"
  },
  {
    "text": "called exponential family. ",
    "start": "1564400",
    "end": "1578490"
  },
  {
    "text": "So a distribution is called to\nbe in an exponential family--",
    "start": "1578490",
    "end": "1584026"
  },
  {
    "text": "A distribution belongs\nto exponential family",
    "start": "1584026",
    "end": "1596590"
  },
  {
    "text": "if there exists a theta,\na vector that parametrizes",
    "start": "1596590",
    "end": "1610890"
  },
  {
    "text": "the distribution such that\nthe probability density",
    "start": "1610890",
    "end": "1625520"
  },
  {
    "text": "function for this choice\nof parameter theta",
    "start": "1625520",
    "end": "1630670"
  },
  {
    "text": "can be written as h\nof x times c of theta",
    "start": "1630670",
    "end": "1636480"
  },
  {
    "text": "times the exponent of\nsum from i equal 1 to k--",
    "start": "1636480",
    "end": "1642497"
  },
  {
    "start": "1642498",
    "end": "1655446"
  },
  {
    "text": "Yes. So here, when I write\nonly x, h should only depend on x, not on theta.",
    "start": "1655446",
    "end": "1663400"
  },
  {
    "text": "When I write some\nfunction of theta, it should only depend\non theta, not on x. So h(x), t_i(x) depends only\non x and c(theta) on my value",
    "start": "1663400",
    "end": "1681070"
  },
  {
    "text": "theta, depends only on theta. That's an abstract thing. It's not clear why\nthis is so useful,",
    "start": "1681070",
    "end": "1687830"
  },
  {
    "text": "at least from the definition. But you're going to talk\nabout some distribution",
    "start": "1687830",
    "end": "1694955"
  },
  {
    "text": "for an exponential\nfamily, right? Yeah. So you will see\nsomething about this. But one good thing\nis, they exhibit",
    "start": "1694955",
    "end": "1701770"
  },
  {
    "text": "some good statistical\nbehavior, the things-- when you group them into--\nall distributions",
    "start": "1701770",
    "end": "1708330"
  },
  {
    "text": "in the exponential family\nhave some nice statistical properties, which makes it good.",
    "start": "1708330",
    "end": "1715590"
  },
  {
    "text": "That's too abstract. Let's see how log-normal\ndistribution actually falls",
    "start": "1715590",
    "end": "1722140"
  },
  {
    "text": "into the exponential family. ",
    "start": "1722140",
    "end": "1727606"
  },
  {
    "text": "AUDIENCE: So, let\nme just comment. PROFESSOR: Yeah, sure. AUDIENCE: The notion of\nindependent random variables,",
    "start": "1727607",
    "end": "1733976"
  },
  {
    "text": "you went over how the--\nwell, the probability density functions of collections\nof random variables",
    "start": "1733976",
    "end": "1740520"
  },
  {
    "text": "if they're mutually\nindependent is the product of the\nprobability densities",
    "start": "1740520",
    "end": "1745640"
  },
  {
    "text": "of the individual variables. And so with this\nexponential family, if you have random variables\nfrom the same exponential",
    "start": "1745640",
    "end": "1752684"
  },
  {
    "text": "family, products of this\ndensity function factor out",
    "start": "1752685",
    "end": "1758380"
  },
  {
    "text": "into a very simple form. It doesn't get more\ncomplicated as you look at the joint density\nof many variables,",
    "start": "1758380",
    "end": "1764429"
  },
  {
    "text": "and in fact simplifies to\nthe same exponential family. So that's where that\nbecomes very useful.",
    "start": "1764430",
    "end": "1770210"
  },
  {
    "text": "PROFESSOR: So it's designed\nso that it factors out when it's multiplied. It factors out well. ",
    "start": "1770210",
    "end": "1777990"
  },
  {
    "text": "OK. So-- sorry about that.",
    "start": "1777990",
    "end": "1783000"
  },
  {
    "text": "Yeah, log-normal distribution. So take h(x), 1 over x.",
    "start": "1783000",
    "end": "1789970"
  },
  {
    "text": "Before that, let's just rewrite\nthat in a different way. So 1 over x sigma square\nroot 2 pi, e to the minus log",
    "start": "1789970",
    "end": "1798804"
  },
  {
    "text": "x [INAUDIBLE] squared. Square.",
    "start": "1798804",
    "end": "1804530"
  },
  {
    "text": "Can be rewritten as 1\nover x, times 1 over sigma",
    "start": "1804530",
    "end": "1810546"
  },
  {
    "text": "squared 2 pi, e to\nthe minus log x square",
    "start": "1810546",
    "end": "1818215"
  },
  {
    "text": "over 2 sigma square plus\nmu log x over sigma square",
    "start": "1818215",
    "end": "1830590"
  },
  {
    "text": "minus mu square. ",
    "start": "1830590",
    "end": "1837049"
  },
  {
    "text": "Let's write it like that. Set up h(x) equals 1 over x.",
    "start": "1837050",
    "end": "1842464"
  },
  {
    "text": "c of theta-- sorry,\ntheta equals mu sigma.",
    "start": "1842464",
    "end": "1851422"
  },
  {
    "text": "c(theta) is equal to 1 over\nsigma square root 2 pi, e to the minus mu square.",
    "start": "1851422",
    "end": "1857163"
  },
  {
    "text": " So you will\nparametrize this family",
    "start": "1857163",
    "end": "1863920"
  },
  {
    "text": "in terms of mu and sigma. Your h of x here\nwill be 1 over x.",
    "start": "1863920",
    "end": "1869490"
  },
  {
    "text": "Your c(theta) will be this\nterm and the last term here, because this\ndoesn't depend on x.",
    "start": "1869490",
    "end": "1876960"
  },
  {
    "text": "And then you have to\nfigure out what w and t is. You can let w_1 of\nx be log x square.",
    "start": "1876960",
    "end": "1884970"
  },
  {
    "text": " t_1-- no, t_1 of x be log x\nsquare, w_1 of theta be minus 1",
    "start": "1884970",
    "end": "1898940"
  },
  {
    "text": "over 2 sigma square. And similarly, you\ncan let t_2 equals log",
    "start": "1898940",
    "end": "1904080"
  },
  {
    "text": "x and w_2 equals mu over sigma.",
    "start": "1904080",
    "end": "1911403"
  },
  {
    "text": " It's just some technicality,\nbut at least you",
    "start": "1911404",
    "end": "1916570"
  },
  {
    "text": "can see it really fits in. ",
    "start": "1916570",
    "end": "1922690"
  },
  {
    "text": "OK. So that's all\nabout distributions that I want to talk about.",
    "start": "1922690",
    "end": "1930080"
  },
  {
    "text": "And then let's talk\na little bit more about more interesting\nstuff, in my opinion.",
    "start": "1930080",
    "end": "1935340"
  },
  {
    "text": "I like this stuff better.  There are two main things\nthat we're interested in.",
    "start": "1935340",
    "end": "1943340"
  },
  {
    "text": "When we have a random variable,\nat least for our purpose, what",
    "start": "1943340",
    "end": "1950650"
  },
  {
    "text": "we want to study is given\na random variable, first,",
    "start": "1950650",
    "end": "1962766"
  },
  {
    "text": "we want to study a statistics. ",
    "start": "1962766",
    "end": "1970710"
  },
  {
    "text": "So we want to study this\nstatistics, whatever that means.",
    "start": "1970710",
    "end": "1975798"
  },
  {
    "text": " And that will be represented\nby the k-th moments",
    "start": "1975798",
    "end": "1982566"
  },
  {
    "text": "of the random variable. ",
    "start": "1982567",
    "end": "1990340"
  },
  {
    "text": "Where k-th moment is defined\nas expectation of X to the k.",
    "start": "1990340",
    "end": "1995370"
  },
  {
    "start": "1995370",
    "end": "2000600"
  },
  {
    "text": "And a good way to study\nall the moments together in one function is a\nmoment-generating function.",
    "start": "2000600",
    "end": "2006855"
  },
  {
    "start": "2006855",
    "end": "2014299"
  },
  {
    "text": "So this moment-generating\nfunction encodes all the k-th moments\nof a random variable.",
    "start": "2014300",
    "end": "2020340"
  },
  {
    "text": "So it contains all the\nstatistical information of a random variable. That's why\nmoment-generating function",
    "start": "2020340",
    "end": "2026880"
  },
  {
    "text": "will be interesting to us. Because when you\nwant to study it, you don't have to consider\neach moment separately.",
    "start": "2026880",
    "end": "2032760"
  },
  {
    "text": "It gives a unified way. It gives a very good\nfeeling about your function.",
    "start": "2032760",
    "end": "2038049"
  },
  {
    "text": "That will be our first topic. Our second topic will\nbe we want to study its long-term or\nlarge-scale behavior.",
    "start": "2038050",
    "end": "2050139"
  },
  {
    "start": "2050140",
    "end": "2058190"
  },
  {
    "text": "So for example, assume that you\nhave a normal distribution-- one random variable with\nnormal distribution.",
    "start": "2058190",
    "end": "2064449"
  },
  {
    "text": "If we just have a\nsingle random variable, you really have no control.",
    "start": "2064449",
    "end": "2070760"
  },
  {
    "text": "It can be anywhere. The outcome can be anything\naccording to that distribution.",
    "start": "2070760",
    "end": "2079260"
  },
  {
    "text": "But if you have several\nindependent random variables with the exact\nsame distribution,",
    "start": "2079260",
    "end": "2084540"
  },
  {
    "text": "if the number is super large--\nlet's say 100 million-- and you plot how many random\nvariables fall into each point",
    "start": "2084540",
    "end": "2095320"
  },
  {
    "text": "into a graph,\nyou'll know that it has to look very\nclose to this curve.",
    "start": "2095320",
    "end": "2101672"
  },
  {
    "text": "It will be more dense\nhere, sparser there, and sparser there.",
    "start": "2101672",
    "end": "2106720"
  },
  {
    "text": "So you don't have\nindividual control on each of the random variables. But when you look\nat large scale,",
    "start": "2106720",
    "end": "2112184"
  },
  {
    "text": "you know, at least with\nvery high probability, it has to look like this curve.",
    "start": "2112185",
    "end": "2119990"
  },
  {
    "text": "Those kind of things are\nwhat we want to study. When we look at this long-term\nbehavior or large scale",
    "start": "2119990",
    "end": "2125720"
  },
  {
    "text": "behavior, what can we say? What kind of events\nare guaranteed to happen with probability,\nlet's say, 99.9%?",
    "start": "2125720",
    "end": "2135109"
  },
  {
    "text": "And actually, some interesting\nthings are happening. As you might already know, two\ntypical theorems of this type",
    "start": "2135110",
    "end": "2144800"
  },
  {
    "text": "will be, in this\ntopic will be law of large numbers and\ncentral limit theorem.",
    "start": "2144800",
    "end": "2153282"
  },
  {
    "start": "2153282",
    "end": "2162520"
  },
  {
    "text": "So let's start with\nour first topic-- the moment-generating function. ",
    "start": "2162520",
    "end": "2186310"
  },
  {
    "text": "The moment-generating\nfunction of a random variable is defined as-- I\nwrite it as m sub",
    "start": "2186310",
    "end": "2191540"
  },
  {
    "text": "X. It's defined as expectation\nof e to the t times x",
    "start": "2191540",
    "end": "2199330"
  },
  {
    "text": "where t is some parameter. t can be any real. ",
    "start": "2199330",
    "end": "2207372"
  },
  {
    "text": "You have to be careful. It doesn't always converge. So remark: does not\nnecessarily exist.",
    "start": "2207372",
    "end": "2218359"
  },
  {
    "start": "2218360",
    "end": "2229900"
  },
  {
    "text": "So for example, one of the\ndistributions you already saw does not have\nmoment-generating function.",
    "start": "2229900",
    "end": "2235010"
  },
  {
    "text": "The log-normal\ndistribution does not",
    "start": "2235010",
    "end": "2242101"
  },
  {
    "text": "have any moment-generating\nfunction. ",
    "start": "2242101",
    "end": "2250650"
  },
  {
    "text": "And that's one thing\nyou have to be careful. It's not just some\ntheoretical thing.",
    "start": "2250650",
    "end": "2255869"
  },
  {
    "text": " The statement is not\nsomething theoretical. It actually happens for\nsome random variables",
    "start": "2255870",
    "end": "2262670"
  },
  {
    "text": "that you encounter in your life. So be careful.",
    "start": "2262670",
    "end": "2268190"
  },
  {
    "text": "And that will actually show\nsome very interesting thing",
    "start": "2268190",
    "end": "2274460"
  },
  {
    "text": "I will later explain. Some very interesting\nfacts arise from this fact.",
    "start": "2274460",
    "end": "2279796"
  },
  {
    "text": " Before going into\nthat, first of all,",
    "start": "2279796",
    "end": "2286276"
  },
  {
    "text": "why is it called\nmoment-generating function? It's because if you\ntake the k-th derivative",
    "start": "2286277",
    "end": "2294540"
  },
  {
    "text": "of this function,\nthen it actually",
    "start": "2294540",
    "end": "2306280"
  },
  {
    "text": "gives the k-th moment\nof your random variable.",
    "start": "2306280",
    "end": "2313131"
  },
  {
    "text": "That's where the\nname comes from. ",
    "start": "2313131",
    "end": "2323234"
  },
  {
    "text": "It's for all integers. ",
    "start": "2323235",
    "end": "2338320"
  },
  {
    "text": "And that gives a\ndifferent way of writing a moment-generating function. ",
    "start": "2338320",
    "end": "2351230"
  },
  {
    "text": "Because of that, we may write\nthe moment-generating function",
    "start": "2351230",
    "end": "2358090"
  },
  {
    "text": "as the sum from k equals\n0 to infinity, t to the k,",
    "start": "2358090",
    "end": "2364992"
  },
  {
    "text": "k factorial, times\na k-th moment. ",
    "start": "2364992",
    "end": "2377790"
  },
  {
    "text": "That's like the\nTaylor expansion. Because you know\nall the derivatives, you know what the\nfunctions would be.",
    "start": "2377790",
    "end": "2383551"
  },
  {
    "text": "Of course, only if it exists. This might not converge. ",
    "start": "2383551",
    "end": "2395080"
  },
  {
    "text": "So if moment-generating\nfunction exists, they pretty much classify\nyour random variables.",
    "start": "2395080",
    "end": "2401120"
  },
  {
    "text": " So if two random\nvariables, X, Y,",
    "start": "2401120",
    "end": "2409019"
  },
  {
    "text": "have the same\nmoment-generating function,",
    "start": "2409020",
    "end": "2416120"
  },
  {
    "text": "then X and Y have the\nsame distribution.",
    "start": "2416120",
    "end": "2424835"
  },
  {
    "start": "2424835",
    "end": "2430020"
  },
  {
    "text": "I will not prove this theorem. But it says that\nmoment-generating function,",
    "start": "2430020",
    "end": "2435080"
  },
  {
    "text": "if it exists, encodes\nreally all the information about your random variables.",
    "start": "2435080",
    "end": "2441516"
  },
  {
    "text": "You're not losing anything.  However, be very careful when\nyou're applying this theorem.",
    "start": "2441516",
    "end": "2450540"
  },
  {
    "text": "Because remark,\nit does not imply",
    "start": "2450540",
    "end": "2459920"
  },
  {
    "text": "that all random variables\nwith identical k-th moments",
    "start": "2459920",
    "end": "2480740"
  },
  {
    "text": "for all k has the\nsame distribution.",
    "start": "2480740",
    "end": "2486790"
  },
  {
    "start": "2486790",
    "end": "2497418"
  },
  {
    "text": "Do you see it? If X and Y have a\nmoment-generating function,",
    "start": "2497418",
    "end": "2503330"
  },
  {
    "text": "and they're the same, then they\nhave the same distribution.",
    "start": "2503330",
    "end": "2509210"
  },
  {
    "text": "This looks a little bit\ncontroversial to this theorem. It says that it's not\nnecessarily the case",
    "start": "2509210",
    "end": "2516890"
  },
  {
    "text": "that two random variables, which\nhave identical moments-- so all k-th moments are the\nsame for two variables--",
    "start": "2516890",
    "end": "2524750"
  },
  {
    "text": "even if that's the case,\nthey don't necessarily have to have the\nsame distribution.",
    "start": "2524750",
    "end": "2530060"
  },
  {
    "text": "Which seems like it\ndoesn't make sense if you look at this theorem. Because moment-generating\nfunction is defined in terms\nof the moments.",
    "start": "2530060",
    "end": "2536650"
  },
  {
    "text": "If two random variables\nhave the same moment, we have the same\nmoment-generating function. If they have the same\nmoment-generating function,",
    "start": "2536650",
    "end": "2542616"
  },
  {
    "text": "they have the same distribution. There is a hole\nin this argument.",
    "start": "2542616",
    "end": "2548450"
  },
  {
    "text": "Even if they have\nthe same moments, it doesn't necessarily\nimply that they",
    "start": "2548450",
    "end": "2553792"
  },
  {
    "text": "have the same\nmoment-generating function. They might both not have\nmoment-generating functions.",
    "start": "2553792",
    "end": "2559520"
  },
  {
    "text": "That's the glitch. Be careful. So just remember that even if\nthey have the same moments,",
    "start": "2559520",
    "end": "2567587"
  },
  {
    "text": "they don't necessarily\nhave the same distribution. And the reason is\nbecause-- one reason is because the moment-generating\nfunction might not exist.",
    "start": "2567587",
    "end": "2576110"
  },
  {
    "text": "And if you look in\nto Wikipedia, you'll see an example of\nwhen it happens, of two random variables\nwhere this happens.",
    "start": "2576110",
    "end": "2583345"
  },
  {
    "start": "2583345",
    "end": "2590310"
  },
  {
    "text": "So that's one thing\nwe will use later. Another thing that\nwe will use later,",
    "start": "2590310",
    "end": "2597660"
  },
  {
    "text": "it's a statement\nvery similar to that, but it says something about a\nsequence of random variables.",
    "start": "2597660",
    "end": "2605820"
  },
  {
    "text": "So if X_1, X_2, up to X_n is\na sequence of random variables",
    "start": "2605820",
    "end": "2619406"
  },
  {
    "text": "such that the moment-generating\nfunction exists,",
    "start": "2619406",
    "end": "2628470"
  },
  {
    "text": "and it converges-- ah,\nit goes to infinity. ",
    "start": "2628470",
    "end": "2637542"
  },
  {
    "text": "Tends to the\nmoment-generating function",
    "start": "2637542",
    "end": "2643250"
  },
  {
    "text": "of some random variable t. X. For some random\nvariable X for all t.",
    "start": "2643250",
    "end": "2653090"
  },
  {
    "text": " Here, we're assuming that all\nmoment-generating function",
    "start": "2653091",
    "end": "2658970"
  },
  {
    "text": "exists. So again, the\nsituation is, you have a sequence of random variables.",
    "start": "2658970",
    "end": "2664900"
  },
  {
    "text": "Their moment-generating\nfunction exists. And in each point\nt, it converges",
    "start": "2664900",
    "end": "2671790"
  },
  {
    "text": "to the value of the\nmoment-generating function of some other random variable x. ",
    "start": "2671790",
    "end": "2678270"
  },
  {
    "text": "And what should happen? In light of this theorem,\nit should be the case",
    "start": "2678270",
    "end": "2683880"
  },
  {
    "text": "that the distribution\nof this sequence gets closer and closer\nto the distribution",
    "start": "2683880",
    "end": "2689240"
  },
  {
    "text": "of this random variable x. And to make it formal, to make\nthat information formal, what",
    "start": "2689240",
    "end": "2700220"
  },
  {
    "text": "we can conclude is, for\nall x, the probability",
    "start": "2700220",
    "end": "2709760"
  },
  {
    "text": "X_i is less than or equal to\nx tends to the probability",
    "start": "2709760",
    "end": "2715440"
  },
  {
    "text": "that at x.  So in this sense,\nthe distributions",
    "start": "2715440",
    "end": "2722990"
  },
  {
    "text": "of these random variables\nconverges to the distribution of that random variable. ",
    "start": "2722990",
    "end": "2730090"
  },
  {
    "text": "So it's just a technical issue. You can just think of it as\nthese random variables converge",
    "start": "2730090",
    "end": "2738890"
  },
  {
    "text": "to that random variable. If you take some graduate\nprobability course, you'll see that there's\nseveral possible ways",
    "start": "2738890",
    "end": "2747100"
  },
  {
    "text": "to define convergence. But that's just\nsome technicality. And the spirit\nhere is just really",
    "start": "2747100",
    "end": "2753397"
  },
  {
    "text": "the sequence converges if its\nmoment-generating function converges. ",
    "start": "2753397",
    "end": "2759790"
  },
  {
    "text": "So as you can see from\nthese two theorems, moment-generating\nfunction, if it exists, is a really powerful\ntool that allows you",
    "start": "2759790",
    "end": "2768270"
  },
  {
    "text": "to control the distribution.  You'll see some applications\nlater in central limit theorem.",
    "start": "2768270",
    "end": "2776407"
  },
  {
    "text": "Any questions? ",
    "start": "2776407",
    "end": "2781530"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]? ",
    "start": "2781530",
    "end": "2788557"
  },
  {
    "text": "PROFESSOR: This one?  Why?",
    "start": "2788557",
    "end": "2794154"
  },
  {
    "text": "AUDIENCE: Because\nit starts with t, and the right-hand side\nhas nothing general. ",
    "start": "2794154",
    "end": "2800777"
  },
  {
    "text": "PROFESSOR: Ah.  Thank you.",
    "start": "2800777",
    "end": "2807180"
  },
  {
    "text": "We evaluated at zero. ",
    "start": "2807180",
    "end": "2813230"
  },
  {
    "text": "Other questions? Other corrections? AUDIENCE: When you say the\nmoment-generating function",
    "start": "2813230",
    "end": "2819086"
  },
  {
    "text": "doesn't exist, do you mean\nthat it isn't analytic or it doesn't converge? PROFESSOR: It\nmight not converge.",
    "start": "2819086",
    "end": "2824580"
  },
  {
    "text": "So log-normal distribution,\nit does not converge. So for all non-zero\nt, it does not",
    "start": "2824580",
    "end": "2830411"
  },
  {
    "text": "converge, for\nlog-normal distribution. AUDIENCE: [INAUDIBLE]? ",
    "start": "2830412",
    "end": "2836349"
  },
  {
    "text": "PROFESSOR: Here? Yes. Pointwise convergence implies\npointwise convergence. ",
    "start": "2836350",
    "end": "2842420"
  },
  {
    "text": "No, no.  Because it's pointwise, this\nconclusion is also rather weak.",
    "start": "2842420",
    "end": "2850474"
  },
  {
    "text": "It's almost the weakest\nconvergence in distribution. ",
    "start": "2850474",
    "end": "2881024"
  },
  {
    "text": "OK. The law of large numbers.",
    "start": "2881024",
    "end": "2892480"
  },
  {
    "start": "2892480",
    "end": "2944100"
  },
  {
    "text": "So now we're talking about\nlarge-scale behavior. Let X_1 up to X_n be\nindependent random variables",
    "start": "2944100",
    "end": "2949630"
  },
  {
    "text": "with identical distribution. We don't really know\nwhat the distribution is, but we know that\nthey're all the same.",
    "start": "2949630",
    "end": "2955270"
  },
  {
    "text": "In short, I'll just refer\nto this condition as i.i.d. random variables later.",
    "start": "2955270",
    "end": "2961990"
  },
  {
    "text": "Independent, identically\ndistributed random variables. ",
    "start": "2961990",
    "end": "2969040"
  },
  {
    "text": "And let mean be mu,\nvariance be sigma square.",
    "start": "2969040",
    "end": "2976530"
  },
  {
    "start": "2976530",
    "end": "2984470"
  },
  {
    "text": "Let's also define X as the\naverage of n random variables.",
    "start": "2984470",
    "end": "2990740"
  },
  {
    "text": " Then the probability that--\nX-- for all-- all positive",
    "start": "2990740",
    "end": "3022986"
  },
  {
    "text": "[INAUDIBLE]. ",
    "start": "3022986",
    "end": "3031590"
  },
  {
    "text": "So whenever you have identical\nindependent distributions, when you take their average, if\nyou take a large enough number",
    "start": "3031590",
    "end": "3039050"
  },
  {
    "text": "of samples, they will be\nvery close to the mean, which makes sense.",
    "start": "3039050",
    "end": "3044144"
  },
  {
    "start": "3044144",
    "end": "3064420"
  },
  {
    "text": "So what's an example of this? Before proving it, example\nof this theorem in practice",
    "start": "3064420",
    "end": "3074010"
  },
  {
    "text": "can be seen in the casino. ",
    "start": "3074010",
    "end": "3082530"
  },
  {
    "text": "So for example, if\nyou're playing blackjack in a casino, when you're\nplaying against the casino,",
    "start": "3082530",
    "end": "3098890"
  },
  {
    "text": "you have a very\nsmall disadvantage. If you're playing at\nthe optimal strategy,",
    "start": "3098890",
    "end": "3112500"
  },
  {
    "text": "you have-- does anybody\nknow the probability? It's about 48%, 49%.",
    "start": "3112500",
    "end": "3120460"
  },
  {
    "text": "About 48% chance of winning. ",
    "start": "3120460",
    "end": "3129160"
  },
  {
    "text": "That means if you bet $1 at\nthe beginning of each round,",
    "start": "3129160",
    "end": "3134339"
  },
  {
    "text": "the expected amount\nyou'll win is $0.48.",
    "start": "3134340",
    "end": "3142605"
  },
  {
    "text": "The expected amount that the\ncasino will win is $0.52.",
    "start": "3142605",
    "end": "3148060"
  },
  {
    "text": "But it's designed so\nthat the variance is so big that this expectation\nis hidden, the mean is hidden.",
    "start": "3148060",
    "end": "3157030"
  },
  {
    "text": "From the player's\npoint of view, you only have a very small sample. So it looks like the\nmean doesn't matter,",
    "start": "3157030",
    "end": "3164960"
  },
  {
    "text": "because the variance takes\nover in a very short scale. But from the casino's\npoint of view,",
    "start": "3164960",
    "end": "3170730"
  },
  {
    "text": "they're taking a\nvery large n there. So for each round, let's\nsay from the casino's",
    "start": "3170730",
    "end": "3182720"
  },
  {
    "text": "point of view, it's\nlike taking, they",
    "start": "3182720",
    "end": "3193500"
  },
  {
    "text": "are taking enormous value of n.",
    "start": "3193500",
    "end": "3200520"
  },
  {
    "start": "3200520",
    "end": "3206640"
  },
  {
    "text": "n here. And that means as long as they\nhave the slightest advantage,",
    "start": "3206640",
    "end": "3212380"
  },
  {
    "text": "they'll be winning money,\nand a huge amount of money. ",
    "start": "3212380",
    "end": "3218240"
  },
  {
    "text": "And most games played in the\ncasinos are designed like this. It looks like the mean\nis really close to 50%,",
    "start": "3218240",
    "end": "3225730"
  },
  {
    "text": "but it's hidden,\nbecause they designed it so the variance is big.",
    "start": "3225730",
    "end": "3231000"
  },
  {
    "text": "But from the casino's\npoint of view, they have enough\nplayers to play the game so that the law of large\nnumbers just makes them money.",
    "start": "3231000",
    "end": "3242120"
  },
  {
    "start": "3242120",
    "end": "3247770"
  },
  {
    "text": "The moral is, don't\nplay blackjack.  Play poker.",
    "start": "3247770",
    "end": "3255360"
  },
  {
    "text": "The reason that the rule\nof law of large numbers doesn't apply, at least\nin this sense, to poker--",
    "start": "3255360",
    "end": "3263010"
  },
  {
    "text": "can anybody explain why?  It's because poker, you're\nplaying against other players.",
    "start": "3263010",
    "end": "3272000"
  },
  {
    "text": "If you have an advantage, if\nyour skill-- if you believe that there is skill in poker--\nif your skill is better",
    "start": "3272000",
    "end": "3278980"
  },
  {
    "text": "than the other\nplayer by, let's say, 5% chance, then you have\nan edge over that player.",
    "start": "3278980",
    "end": "3287010"
  },
  {
    "text": "So you can win money. The only problem is that\nbecause-- poker, you're",
    "start": "3287010",
    "end": "3293870"
  },
  {
    "text": "not playing against the casino. ",
    "start": "3293870",
    "end": "3300390"
  },
  {
    "text": "Don't play against casino. But they still\nhave to make money.",
    "start": "3300390",
    "end": "3306530"
  },
  {
    "text": "So what they do instead\nis they take rake. So for each round\nthat the players play,",
    "start": "3306530",
    "end": "3312350"
  },
  {
    "text": "they pay some fee to the casino. And how the casino makes\nmoney at the poker table",
    "start": "3312350",
    "end": "3319920"
  },
  {
    "text": "is by accumulating those fees. They're not taking\nchances there.",
    "start": "3319920",
    "end": "3325291"
  },
  {
    "text": "But from the player's\npoint of view, if you're better than the other\nplayer, and the amount of edge",
    "start": "3325291",
    "end": "3332405"
  },
  {
    "text": "you have over the other\nplayer is larger than the fee that the casino\ncharges to you, then",
    "start": "3332405",
    "end": "3338000"
  },
  {
    "text": "now you can apply law of large\nnumbers to yourself and win. ",
    "start": "3338000",
    "end": "3345420"
  },
  {
    "text": "And if you take an\nexample as poker, it looks like-- OK, I'm\nnot going to play poker.",
    "start": "3345420",
    "end": "3354372"
  },
  {
    "text": "But if it's a hedge\nfund, or if you're doing high-frequency trading,\nthat's the moral behind it.",
    "start": "3354372",
    "end": "3364850"
  },
  {
    "text": "So that's the belief\nyou should have. You have to believe\nthat you have an edge.",
    "start": "3364850",
    "end": "3370760"
  },
  {
    "text": "Even if you have a\ntiny edge, if you can have enough\nnumber of trials,",
    "start": "3370760",
    "end": "3376400"
  },
  {
    "text": "if you can trade enough of times\nusing some strategy that you believe is winning over time,\nthen law of large numbers",
    "start": "3376400",
    "end": "3386580"
  },
  {
    "text": "will take it from there and\nwill bring you money, profit. ",
    "start": "3386580",
    "end": "3394920"
  },
  {
    "text": "Of course, the problem is,\nwhen the variance is big,",
    "start": "3394920",
    "end": "3401770"
  },
  {
    "text": "your belief starts to fall. At least, that was the case for\nme when I was playing poker.",
    "start": "3401770",
    "end": "3408660"
  },
  {
    "text": "Because I believed\nthat I had an edge, but when there is\nreally swing, it",
    "start": "3408660",
    "end": "3415520"
  },
  {
    "text": "looks like your\nexpectation is negative. And that's when you have\nto believe in yourself.",
    "start": "3415520",
    "end": "3421885"
  },
  {
    "text": " Yeah.",
    "start": "3421885",
    "end": "3427690"
  },
  {
    "text": "That's when your\nfaith in mathematics is being challenged. It really happened.",
    "start": "3427690",
    "end": "3432720"
  },
  {
    "text": " I hope it doesn't happen to you. Anyway, that's proof\nlaw of large numbers.",
    "start": "3432720",
    "end": "3442730"
  },
  {
    "text": "How do you prove it? The proof is quite easy. ",
    "start": "3442730",
    "end": "3447839"
  },
  {
    "text": "First of all, one observation--\nexpectation of X is just",
    "start": "3447840",
    "end": "3452940"
  },
  {
    "text": "expectation of 1 over\nn times sum of X_i's. ",
    "start": "3452940",
    "end": "3461400"
  },
  {
    "text": "And that, by linearity,\njust becomes the sum of--",
    "start": "3461400",
    "end": "3472471"
  },
  {
    "text": "and that's mu. OK. That's good.",
    "start": "3472471",
    "end": "3479317"
  },
  {
    "text": "And then the variance,\nwhat's the variance of X? ",
    "start": "3479317",
    "end": "3484430"
  },
  {
    "text": "That's the expectation\nof X minus mu",
    "start": "3484430",
    "end": "3489750"
  },
  {
    "text": "square, which is the expectation\nsum over all i's, minus mu",
    "start": "3489750",
    "end": "3500976"
  },
  {
    "text": "square.  I'll group them.",
    "start": "3500976",
    "end": "3506260"
  },
  {
    "text": "That's the expectation of 1 over\nn sum of X_i minus mu square.",
    "start": "3506260",
    "end": "3513584"
  },
  {
    "text": "i is from 1 to n. ",
    "start": "3513584",
    "end": "3523570"
  },
  {
    "text": "What did I do wrong? 1 over n is inside the square. So I can take it out\nand square, n square.",
    "start": "3523570",
    "end": "3530720"
  },
  {
    "text": "And then you're summing\nn terms of sigma square. So that is equal to\nsigma square over n.",
    "start": "3530720",
    "end": "3537145"
  },
  {
    "start": "3537145",
    "end": "3542450"
  },
  {
    "text": "That means the\neffect of averaging n terms does not\naffect your average,",
    "start": "3542450",
    "end": "3548600"
  },
  {
    "text": "but it affects your variance.  It divides your variance by n.",
    "start": "3548600",
    "end": "3555802"
  },
  {
    "text": "If you take larger and\nlarger n, your variance gets smaller and smaller. ",
    "start": "3555802",
    "end": "3562590"
  },
  {
    "text": "And using that, we can\nprove this statement. There's only one thing\nyou have to notice--",
    "start": "3562590",
    "end": "3567840"
  },
  {
    "text": "that the probability\nthat x minus mu is greater than epsilon. When you multiply this\nby epsilon square.",
    "start": "3567840",
    "end": "3575840"
  },
  {
    "text": "This will be less than or\nequal to the variance of x.",
    "start": "3575840",
    "end": "3581230"
  },
  {
    "text": "The reason this\ninequality holds is because variance X is defined\nas the expectation of X minus mu",
    "start": "3581230",
    "end": "3586290"
  },
  {
    "text": "square. For all the events when you have\nX minus mu at least epsilon,",
    "start": "3586290",
    "end": "3592339"
  },
  {
    "text": "your multiplying\nfactor X square will be at least epsilon square. This term will be at\nleast epsilon square",
    "start": "3592340",
    "end": "3600350"
  },
  {
    "text": "when you fall into this event. So your variance has\nto be at least that.",
    "start": "3600350",
    "end": "3607100"
  },
  {
    "text": "And this is known to\nbe sigma square over n. So probability that\nx minus mu is greater",
    "start": "3607100",
    "end": "3615703"
  },
  {
    "text": "than epsilon is at most sigma\nsquare over n epsilon squared.",
    "start": "3615704",
    "end": "3621980"
  },
  {
    "text": "That means if you take n to go\nto infinity, that goes to zero. So the probability that\nyou deviate from the mean",
    "start": "3621980",
    "end": "3629589"
  },
  {
    "text": "by more than epsilon goes to 0. You can actually read out a\nlittle bit more from the proof.",
    "start": "3629590",
    "end": "3635645"
  },
  {
    "text": " It also tells a little bit\nabout the speed of convergence.",
    "start": "3635645",
    "end": "3641635"
  },
  {
    "text": " So let's say you have a random\nvariable X. Your mean is 50.",
    "start": "3641635",
    "end": "3650230"
  },
  {
    "text": "You epsilon is 0.1. So you want to know\nthe probability",
    "start": "3650230",
    "end": "3655829"
  },
  {
    "text": "that you deviate from your\nmean by more than 0.1. Let's say you want\nto be 99% sure.",
    "start": "3655830",
    "end": "3666010"
  },
  {
    "text": "Want to be 99% sure that X\nminus mu is less than 0.1,",
    "start": "3666010",
    "end": "3674812"
  },
  {
    "text": "or X minus 50 is less than 0.1. In that case, what you can do\nis-- you want this to be 0.01.",
    "start": "3674812",
    "end": "3683060"
  },
  {
    "text": "It has to be 0.01. So plug in that, plug in your\nvariance, plug in your epsilon.",
    "start": "3683060",
    "end": "3689800"
  },
  {
    "text": "That will give you\nsome bound on n. If you have more than\nthat number of trials, you can be 99% sure that you\ndon't deviate from your mean",
    "start": "3689800",
    "end": "3698113"
  },
  {
    "text": "by more than epsilon. So that does give\nsome estimate, but I should mention that this\nis a very bad estimate.",
    "start": "3698113",
    "end": "3706150"
  },
  {
    "text": "There are much more\npowerful estimates that can be done here. That will give the order of\nmagnitude-- I didn't really calculate here, but it looks\nlike it's close to millions.",
    "start": "3706150",
    "end": "3713440"
  },
  {
    "text": "It has to be close to millions. But in practice, if you use\na lot more powerful tool",
    "start": "3713440",
    "end": "3720359"
  },
  {
    "text": "of estimating it, it should\nonly be hundreds or at most thousands.",
    "start": "3720360",
    "end": "3725508"
  },
  {
    "start": "3725508",
    "end": "3733460"
  },
  {
    "text": "So the tool you'll use there\nis moment-generating functions, something similar to\nmoment-generating functions. But I will not go into it.",
    "start": "3733460",
    "end": "3740412"
  },
  {
    "text": "Any questions?  OK. For those who already saw\nlaw of large numbers before,",
    "start": "3740412",
    "end": "3748552"
  },
  {
    "text": "the name suggests there's\nalso something called strong law of large numbers. ",
    "start": "3748552",
    "end": "3755982"
  },
  {
    "text": "In that theorem, your\nconclusion is stronger.",
    "start": "3755982",
    "end": "3761380"
  },
  {
    "text": "So the convergence is stronger\nthan this type of convergence. ",
    "start": "3761380",
    "end": "3767809"
  },
  {
    "text": "And also, the\ncondition I gave here is a very strong condition.",
    "start": "3767810",
    "end": "3773580"
  },
  {
    "text": "The same conclusion\nis true even if you weaken some of the conditions.",
    "start": "3773580",
    "end": "3778840"
  },
  {
    "text": "So for example, the variance\ndoes not have to exist. It can be replaced by some\nother condition, and so on.",
    "start": "3778840",
    "end": "3786480"
  },
  {
    "text": "But here, I just want\nit to be a simple form so that it's easy to prove. And you at least get the\nspirit of what's happening.",
    "start": "3786480",
    "end": "3794274"
  },
  {
    "start": "3794274",
    "end": "3800480"
  },
  {
    "text": "Now let's move on to the next\ntopic-- central limit theorem.",
    "start": "3800480",
    "end": "3806140"
  },
  {
    "start": "3806140",
    "end": "3851240"
  },
  {
    "text": "So weak law of\nlarge numbers says",
    "start": "3851240",
    "end": "3856880"
  },
  {
    "text": "that if you have IID random\nvariables, 1 over n times",
    "start": "3856880",
    "end": "3862210"
  },
  {
    "text": "sum over X_i's converges to mu,\nthe mean, in some weak sense.",
    "start": "3862210",
    "end": "3867400"
  },
  {
    "text": " And the reason it happened\nwas because this had",
    "start": "3867400",
    "end": "3873730"
  },
  {
    "text": "mean mu and variance\nsigma square over n.",
    "start": "3873730",
    "end": "3879157"
  },
  {
    "text": " We've exploited the fact that\nvariance vanishes to get this.",
    "start": "3879157",
    "end": "3889730"
  },
  {
    "text": "So the question is, what\nhappens if you replace 1 over n by 1 over square root n?",
    "start": "3889730",
    "end": "3894903"
  },
  {
    "text": " What happens if-- for\nthe random variable",
    "start": "3894903",
    "end": "3904590"
  },
  {
    "text": "1 over square root n times X_i? ",
    "start": "3904590",
    "end": "3914180"
  },
  {
    "text": "The reason I'm making this\nchoice of 1 over square root n is because if you\nmake this choice,",
    "start": "3914180",
    "end": "3919310"
  },
  {
    "text": "now the average has mean mu\nand variance sigma square just",
    "start": "3919310",
    "end": "3926330"
  },
  {
    "text": "as in X_i's. So this is the same as X_i.",
    "start": "3926330",
    "end": "3934981"
  },
  {
    "start": "3934981",
    "end": "3940910"
  },
  {
    "text": "Then what should it look like? If the random variable is the\nsame mean and same variance",
    "start": "3940910",
    "end": "3946730"
  },
  {
    "text": "as your original random\nvariable, the distribution",
    "start": "3946730",
    "end": "3952119"
  },
  {
    "text": "of this, should it look like\nthe distribution of X_i? ",
    "start": "3952120",
    "end": "3960530"
  },
  {
    "text": "If mean is mu. Thank you very much. The case when mean is 0.",
    "start": "3960530",
    "end": "3965535"
  },
  {
    "start": "3965535",
    "end": "3973160"
  },
  {
    "text": "OK. For this special case,\nwill it look like X_i, or will it not look like X_i?",
    "start": "3973160",
    "end": "3980820"
  },
  {
    "text": "If it doesn't look like X_i,\ncan we say anything interesting about the distribution of this?",
    "start": "3980820",
    "end": "3987590"
  },
  {
    "text": "And central limit theorem\nanswers this question. When I first saw it, I thought\nit was really interesting.",
    "start": "3987590",
    "end": "3994980"
  },
  {
    "text": "Because normal\ndistribution comes up here. ",
    "start": "3994980",
    "end": "4000250"
  },
  {
    "text": "And that's probably\none of the reasons that normal distribution\nis so universal. Because when you take\nmany independent events",
    "start": "4000250",
    "end": "4010310"
  },
  {
    "text": "and take the average\nin this sense, their distribution converges\nto a normal distribution.",
    "start": "4010310",
    "end": "4016765"
  },
  {
    "text": "Yes? AUDIENCE: How did you get\nmean equals [INAUDIBLE]? PROFESSOR: I didn't get it. I assumed it if X-- yeah.",
    "start": "4016765",
    "end": "4022678"
  },
  {
    "start": "4022678",
    "end": "4049600"
  },
  {
    "text": "So theorem: let\nX_1, X_2, to X_n be",
    "start": "4049600",
    "end": "4061480"
  },
  {
    "text": "IID random variables with mean,\nthis time, mu and variance",
    "start": "4061480",
    "end": "4071960"
  },
  {
    "text": "sigma squared. And let X-- or Y_n.",
    "start": "4071960",
    "end": "4079308"
  },
  {
    "text": " Y_n be square root n times\n1 over n, of X_i minus mu.",
    "start": "4079308",
    "end": "4090023"
  },
  {
    "start": "4090023",
    "end": "4104813"
  },
  {
    "text": "Then the distribution\nof Y_n converges",
    "start": "4104813",
    "end": "4121080"
  },
  {
    "text": "to that of normal distribution\nwith mean 0 and variance sigma.",
    "start": "4121080",
    "end": "4130056"
  },
  {
    "text": " What this means-- I'll\nwrite it down again--",
    "start": "4130056",
    "end": "4137349"
  },
  {
    "text": "it means for all x,\nprobability that Y_n is less than or\nequal to x converges",
    "start": "4137350",
    "end": "4143790"
  },
  {
    "text": "the probability that normal\ndistribution is less than or equal to x.",
    "start": "4143790",
    "end": "4148910"
  },
  {
    "start": "4148910",
    "end": "4154140"
  },
  {
    "text": "What's really\ninteresting here is, no matter what distribution\nyou had in the beginning,",
    "start": "4154140",
    "end": "4160339"
  },
  {
    "text": "if we average it\nout in this sense, then you converge to\nthe normal distribution.",
    "start": "4160340",
    "end": "4165964"
  },
  {
    "start": "4165965",
    "end": "4175428"
  },
  {
    "text": "Any questions about this\nstatement, or any corrections? ",
    "start": "4175429",
    "end": "4180490"
  },
  {
    "text": "Any mistakes that I made? OK.",
    "start": "4180490",
    "end": "4186015"
  },
  {
    "text": "Here's the proof.  I will prove it when the\nmoment-generating function",
    "start": "4186015",
    "end": "4194400"
  },
  {
    "text": "exists. So assume that the\nmoment-generating functions exists. So proof assuming\nm of X_i exists.",
    "start": "4194400",
    "end": "4204963"
  },
  {
    "start": "4204963",
    "end": "4216810"
  },
  {
    "text": "So remember that theorem. Try to recall that\ntheorem where if you",
    "start": "4216810",
    "end": "4222159"
  },
  {
    "text": "know that the moment-generating\nfunction of Y_n's converges to the moment-generating\nfunction of the normal, then",
    "start": "4222160",
    "end": "4229250"
  },
  {
    "text": "we have the statement. The distribution converges. So that's the statement\nwe're going to use.",
    "start": "4229250",
    "end": "4234328"
  },
  {
    "text": "That means our goal is to prove\nthat the moment-generating function of these Y_n's converge\nto the moment-generating",
    "start": "4234328",
    "end": "4243020"
  },
  {
    "text": "function of the normal for\nall t, pointwise convergence.",
    "start": "4243020",
    "end": "4251088"
  },
  {
    "start": "4251088",
    "end": "4256360"
  },
  {
    "text": "And this part is well known. I'll just write it down.",
    "start": "4256360",
    "end": "4261455"
  },
  {
    "text": "It's known to be e to the t\nsquare sigma square over 2. ",
    "start": "4261455",
    "end": "4268818"
  },
  {
    "text": "That just can be computed. ",
    "start": "4268818",
    "end": "4278610"
  },
  {
    "text": "So we want to somehow show that\nthe moment-generating function of this Y_n converges to that.",
    "start": "4278610",
    "end": "4285738"
  },
  {
    "text": "The moment-generating\nfunction of Y_n is equal to expectation\nof e to t Y_n.",
    "start": "4285738",
    "end": "4296102"
  },
  {
    "start": "4296102",
    "end": "4302544"
  },
  {
    "text": "e to the t, 1 over square\nroot n, sum of X_i minus mu.",
    "start": "4302544",
    "end": "4310496"
  },
  {
    "text": " And then because each of\nthe X_i's are independent,",
    "start": "4310496",
    "end": "4317680"
  },
  {
    "text": "this sum will split\ninto products.  Product of-- let\nme split it better.",
    "start": "4317680",
    "end": "4334059"
  },
  {
    "text": "Meets the expectation-- we\ndidn't use independent yet.",
    "start": "4334059",
    "end": "4339240"
  },
  {
    "text": "Sum becomes products of e to\nthe t, 1 over square root n, X_i",
    "start": "4339240",
    "end": "4346504"
  },
  {
    "text": "minus mu. ",
    "start": "4346504",
    "end": "4354650"
  },
  {
    "text": "And then because\nthey're independent, this product can go out. ",
    "start": "4354650",
    "end": "4360925"
  },
  {
    "text": "Equal to the product from 1 to\nn expectation e to the t times",
    "start": "4360925",
    "end": "4369995"
  },
  {
    "text": "square root n-- ",
    "start": "4369996",
    "end": "4376159"
  },
  {
    "text": "OK. Now they're identically\ndistributed, so you just have to take\nthe n-th power of that. That's equal to the\nexpectation of e",
    "start": "4376160",
    "end": "4383923"
  },
  {
    "text": "to the t over square root n,\nX_i minus mu, to the n-th power.",
    "start": "4383923",
    "end": "4391920"
  },
  {
    "text": "Now we'll do some estimation. So use the Taylor\nexpansion of this.",
    "start": "4391920",
    "end": "4399449"
  },
  {
    "text": "What we get is expectation of 1\nplus that, t over square root n",
    "start": "4399450",
    "end": "4410002"
  },
  {
    "text": "xi minus mu, plus 1 over\n2 factorial, that squared,",
    "start": "4410002",
    "end": "4416989"
  },
  {
    "text": "t over square root n,\nxi minus mu squared,",
    "start": "4416990",
    "end": "4423760"
  },
  {
    "text": "plus 1 over 3 factorial,\nthat cubed plus so on. ",
    "start": "4423760",
    "end": "4435050"
  },
  {
    "text": "Then that's equal to 1--\nAh, to the n-th power. ",
    "start": "4435050",
    "end": "4442920"
  },
  {
    "text": "The linearity of\nexpectation, 1 comes out. Second term is 0,\nbecause X_i have mean mu.",
    "start": "4442920",
    "end": "4452830"
  },
  {
    "text": "So that disappears. This term-- we have 1 over 2,\nt squared over n, X_i minus mu",
    "start": "4452830",
    "end": "4466930"
  },
  {
    "text": "square. X_i minus mu square, when\nyou take expectation, that will be sigma square.",
    "start": "4466930",
    "end": "4475550"
  },
  {
    "text": "And then the terms after\nthat, because we're only interested in\nproving that for fixed t,",
    "start": "4475550",
    "end": "4482850"
  },
  {
    "text": "this converges-- so we're only\nproving pointwise convergence. You may consider t\nas a fixed number.",
    "start": "4482850",
    "end": "4489030"
  },
  {
    "text": "So as n goes to infinity--\nif n is really, really large, all these terms will be\nsmaller order of magnitude",
    "start": "4489030",
    "end": "4496730"
  },
  {
    "text": "than n, 1 over n. Something like that happens.",
    "start": "4496730",
    "end": "4502270"
  },
  {
    "start": "4502270",
    "end": "4508530"
  },
  {
    "text": "And that's happening\nbecause we're fixed. For fixed t, we\nhave to prove it.",
    "start": "4508530",
    "end": "4514260"
  },
  {
    "text": "So if we're saying\nsomething uniformly about t, that's no longer true. Now we go back to\nthe exponential form.",
    "start": "4514260",
    "end": "4521060"
  },
  {
    "text": "So this is pretty much\njust e to that term,",
    "start": "4521060",
    "end": "4526540"
  },
  {
    "text": "1 over 2 t square\nsigma square over n plus little o of 1 over\nn to the n-th power.",
    "start": "4526540",
    "end": "4537370"
  },
  {
    "text": "Now, that n can be\nmultiplied to cancel out.",
    "start": "4537370",
    "end": "4542980"
  },
  {
    "text": "And we see that it's e to t\nsquare sigma square over 2 plus the little o of 1.",
    "start": "4542980",
    "end": "4548342"
  },
  {
    "text": "So if you take n\nto go to infinity, that term disappears,\nand we prove",
    "start": "4548342",
    "end": "4555840"
  },
  {
    "text": "that it converges to that.  And then by the theorem that I\nstated before, if we have this,",
    "start": "4555840",
    "end": "4564516"
  },
  {
    "text": "we know that the\ndistribution converges. ",
    "start": "4564516",
    "end": "4569880"
  },
  {
    "text": "Any questions?  OK. I'll make one final remark.",
    "start": "4569880",
    "end": "4575514"
  },
  {
    "start": "4575515",
    "end": "4589009"
  },
  {
    "text": "So suppose there is a random\nvariable x whose mean we do not",
    "start": "4589009",
    "end": "4602639"
  },
  {
    "text": "know, whose mean is unknown. ",
    "start": "4602640",
    "end": "4613670"
  },
  {
    "text": "Our goal is to\nestimate the mean. ",
    "start": "4613670",
    "end": "4618969"
  },
  {
    "text": "And one way to do that is by\ntaking many independent trials of this random variable.",
    "start": "4618970",
    "end": "4625220"
  },
  {
    "text": "So take independent trials X_1,\nX_2, to X_n, and use 1 over--",
    "start": "4625220",
    "end": "4641680"
  },
  {
    "text": "X_1 plus... X_n as our estimator. ",
    "start": "4641680",
    "end": "4652960"
  },
  {
    "text": "Then the law of large\nnumbers says that this will be very close to the mean. So if you take n\nto be large enough,",
    "start": "4652960",
    "end": "4659840"
  },
  {
    "text": "you will more than likely\nhave some value which is very close to the mean. And then the central\nlimit theorem",
    "start": "4659840",
    "end": "4667050"
  },
  {
    "text": "tells you how the\ndistribution of this variable",
    "start": "4667050",
    "end": "4673530"
  },
  {
    "text": "is around the mean. So we don't know what\nthe real value is, but we know that\nthe distribution",
    "start": "4673530",
    "end": "4680620"
  },
  {
    "text": "of the value that\nwe will obtain here is something like\nthat around the mean. ",
    "start": "4680620",
    "end": "4689340"
  },
  {
    "text": "And because normal distribution\nhave very small tails,",
    "start": "4689340",
    "end": "4697080"
  },
  {
    "text": "the tail distributions\nis really small, we will get really\nclose really fast.",
    "start": "4697080",
    "end": "4703949"
  },
  {
    "text": " And this is known as the maximum\nlikelihood estimator, is it?",
    "start": "4703950",
    "end": "4714387"
  },
  {
    "text": " OK, yeah. For some distributions,\nit's better",
    "start": "4714387",
    "end": "4719980"
  },
  {
    "text": "to take some other estimator. Which is quite interesting.",
    "start": "4719980",
    "end": "4727280"
  },
  {
    "text": "At least my intuition is to\ntake this for every single case, looks like that will\nbe a good choice.",
    "start": "4727280",
    "end": "4732890"
  },
  {
    "text": "But it turns out that\nthat's not the case; for some distributions there's\na better choice than this.",
    "start": "4732890",
    "end": "4739492"
  },
  {
    "text": "And Peter will\nlater talk about it. ",
    "start": "4739492",
    "end": "4746340"
  },
  {
    "text": "If you're interested\nin, come back. And that's it for\ntoday, any questions?",
    "start": "4746340",
    "end": "4753960"
  },
  {
    "text": "So next Tuesday we will\nhave an outside speaker, and it will be on bonds.",
    "start": "4753960",
    "end": "4761256"
  },
  {
    "text": "and I don't think anything from\nlinear algebra will be here. ",
    "start": "4761256",
    "end": "4765383"
  }
]