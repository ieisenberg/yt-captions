[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6360"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6360",
    "end": "13339"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "13340",
    "end": "22239"
  },
  {
    "text": "PROFESSOR: Today's topic\nis factor modeling, and the subject here basically\nexploits multivariate analysis",
    "start": "22240",
    "end": "32419"
  },
  {
    "text": "in statistics to financial\nmarkets where our concern is",
    "start": "32420",
    "end": "37909"
  },
  {
    "text": "using factors to model\nreturns and variances, covariances, correlations.",
    "start": "37910",
    "end": "44900"
  },
  {
    "text": "And with these models,\nthere are two basic cases. There's one where the\nfactors are observable.",
    "start": "44900",
    "end": "52150"
  },
  {
    "text": "Those can be\nmacroeconomic factors. They can be fundamental\nfactors on assets or securities",
    "start": "52150",
    "end": "59690"
  },
  {
    "text": "that might explain\nreturns and covariances. A second class of models\nis where these factors",
    "start": "59690",
    "end": "66490"
  },
  {
    "text": "are hidden or latent. And statistical\nfactor models are then",
    "start": "66490",
    "end": "71850"
  },
  {
    "text": "used to specify these models. In particular, there\nare two methodologies.",
    "start": "71850",
    "end": "77110"
  },
  {
    "text": "There's factor analysis and\nprincipal components analysis, which we'll get into some\ndetail during the lecture.",
    "start": "77110",
    "end": "84930"
  },
  {
    "text": "So let's proceed to talk about\nthe setup for a linear factor",
    "start": "84930",
    "end": "91200"
  },
  {
    "text": "model. We have m assets, or\ninstruments, or indexes",
    "start": "91200",
    "end": "98540"
  },
  {
    "text": "whose values correspond to a\nmultivariate stochastic process we're modeling.",
    "start": "98540",
    "end": "104030"
  },
  {
    "text": "And we have n time periods t. And with the factor model\nwe model the t-th value",
    "start": "104030",
    "end": "112840"
  },
  {
    "text": "for the i-th object-- whether\nit's a stock price, futures",
    "start": "112840",
    "end": "118140"
  },
  {
    "text": "price, currency-- as a\nlinear function of factors",
    "start": "118140",
    "end": "124750"
  },
  {
    "text": "f_1 through f_k. So there's basically\nlike a state-space model",
    "start": "124750",
    "end": "130690"
  },
  {
    "text": "for the value of the\nstochastic process, as it depends on these\nunderlying factors.",
    "start": "130690",
    "end": "136020"
  },
  {
    "text": "And the dependence is given\nby coefficients beta_1 through beta_k, which are\ndepending upon i, the asset.",
    "start": "136020",
    "end": "147600"
  },
  {
    "text": "So we allow each asset, say\nif we're thinking of stocks, to depend on factors\nin different ways.",
    "start": "147600",
    "end": "154769"
  },
  {
    "text": "If a certain underlying\nfactor changes in value, the beta corresponds to the\nimpact of that underlying",
    "start": "154770",
    "end": "164340"
  },
  {
    "text": "factor. So we have common factors.",
    "start": "164340",
    "end": "169440"
  },
  {
    "text": " Now these common factors\nf, this is really going to be a nice model if the\nnumber of factors that we're",
    "start": "169440",
    "end": "178150"
  },
  {
    "text": "using is relatively small. So the number k\nof common factors",
    "start": "178150",
    "end": "185360"
  },
  {
    "text": "is generally very, very\nsmall relative to m. And if you think about modeling,\nsay asset-- equity asset",
    "start": "185360",
    "end": "193010"
  },
  {
    "text": "returns in a market, there\ncan be hundreds and thousands of securities. And so in terms of modeling\nthose returns and covariances,",
    "start": "193010",
    "end": "202530"
  },
  {
    "text": "what we're trying to\ndo is characterize those in terms of a modest\nnumber of underlying factors",
    "start": "202530",
    "end": "208230"
  },
  {
    "text": "which simplifies\nthe problem greatly. ",
    "start": "208230",
    "end": "213450"
  },
  {
    "text": "The vectors beta_i are\ntermed the factor loadings of an asset.",
    "start": "213450",
    "end": "218610"
  },
  {
    "text": "And the epsilon_(i,t)'s are\na specific factor of asset i,",
    "start": "218610",
    "end": "223680"
  },
  {
    "text": "period t. So in factor modeling,\nwe talk about there being common factors affecting\nthe dynamics of the system,",
    "start": "223680",
    "end": "233340"
  },
  {
    "text": "and the factor associated\nwith particular cases",
    "start": "233340",
    "end": "239209"
  },
  {
    "text": "are the specific factors. So this setup is\nreally very familiar.",
    "start": "239210",
    "end": "245120"
  },
  {
    "text": "It just looks like a standard\nsort of regression type model that we've seen before.",
    "start": "245120",
    "end": "251239"
  },
  {
    "text": "And so let's see how\nthis can be set up as a set of cross-sectional\nregressions.",
    "start": "251240",
    "end": "258100"
  },
  {
    "text": "So now we're going to fix\nthe period t, the time t,",
    "start": "258100",
    "end": "265870"
  },
  {
    "text": "and consider the\nm-variate x variable",
    "start": "265870",
    "end": "271040"
  },
  {
    "text": "as satisfying a regression model\nwith intercept given by alphas.",
    "start": "271040",
    "end": "278460"
  },
  {
    "text": "And then the independent\nvariables matrix is B, given by the coefficients\nof the factor loadings.",
    "start": "278460",
    "end": "288710"
  },
  {
    "text": "And then we have the residuals\nepsilon_t for the m assets.",
    "start": "288710",
    "end": "294210"
  },
  {
    "text": "So the cross-sectional\nterminology means we're fixing time\nand looking across all",
    "start": "294210",
    "end": "300700"
  },
  {
    "text": "the assets for one fixed time. And we're trying to explain\nhow, say, the returns of assets",
    "start": "300700",
    "end": "309310"
  },
  {
    "text": "are varying depending upon\nthe underlying factors. And so the-- well OK, what's\nrandom in this process?",
    "start": "309310",
    "end": "319990"
  },
  {
    "text": "Well certainly the residual\nterm is considered to be random. That's basically\ngoing to be assumed",
    "start": "319990",
    "end": "326410"
  },
  {
    "text": "to be white noise with mean 0. There's going to be possibly\na covariance matrix psi.",
    "start": "326410",
    "end": "335490"
  },
  {
    "text": "And it's going to\nbe uncorrelated across different\ntime cross sections.",
    "start": "335490",
    "end": "341860"
  },
  {
    "text": "Let's see if I can move the\nmouse, if this is what's causing the problem down here. So in this model we have the\nrealizations on the underlying",
    "start": "341860",
    "end": "354450"
  },
  {
    "text": "factors being random variables. The returns on the assets depend\non the underlying factors.",
    "start": "354450",
    "end": "359710"
  },
  {
    "text": "Those are going to be assumed\nto have some mean, mu_f, and some covariance matrix.",
    "start": "359710",
    "end": "367010"
  },
  {
    "text": "And basically the\ndimension of that covariance matrix omega_f\nis going to be k by k.",
    "start": "367010",
    "end": "373250"
  },
  {
    "text": "So in terms of modeling this\nproblem, we go from an m by m system of\ncovariances, correlations,",
    "start": "373250",
    "end": "382130"
  },
  {
    "text": "to focusing initially on an a\nk by k system of covariances",
    "start": "382130",
    "end": "387360"
  },
  {
    "text": "and correlations between\nthe underlying factors. Psi is a diagonal matrix\nwith the specific variances",
    "start": "387360",
    "end": "398380"
  },
  {
    "text": "of the underlying assets. So we have basically epsilon--\nthe covariance matrix",
    "start": "398380",
    "end": "410270"
  },
  {
    "text": "of the epsilons is\na diagonal matrix, and the covariance matrix of\nf is given by this omega_f.",
    "start": "410270",
    "end": "419500"
  },
  {
    "text": "Well, with those\nspecifications we can get the covariance\nfor the overall vector",
    "start": "419500",
    "end": "429070"
  },
  {
    "text": "of the m-variate\nstochastic process. And we have this model here\nfor the conditional moments.",
    "start": "429070",
    "end": "439880"
  },
  {
    "text": "Basically, the\nconditional expectation of the process given\nthe underlying factors",
    "start": "439880",
    "end": "445810"
  },
  {
    "text": "is this linear model in terms\nof the underlying factors f. And the covariance matrix is the\npsi matrix, which is diagonal.",
    "start": "445810",
    "end": "454025"
  },
  {
    "text": " And the unconditional\nmoments are",
    "start": "454025",
    "end": "462840"
  },
  {
    "text": "obtained by just taking\nthe expectations of these. Well actually, the unconditional\nexpectation of x is this.",
    "start": "462840",
    "end": "470130"
  },
  {
    "text": "The unconditional\ncovariance of x is actually equal\nto the expectation",
    "start": "470130",
    "end": "476340"
  },
  {
    "text": "of this plus the variance of\nthe conditional expectation,",
    "start": "476340",
    "end": "482129"
  },
  {
    "text": "or the covariance of the\nconditional expectation. So one of the formulas that's\nimportant to realize here",
    "start": "482129",
    "end": "488690"
  },
  {
    "text": "is that if we're considering\nthe covariance of x_t, which is equal to covariance of B\nf_t plus epsilon_t, that's",
    "start": "488690",
    "end": "501530"
  },
  {
    "text": "equal to the covariance of\nB f_t plus the covariance",
    "start": "501530",
    "end": "507715"
  },
  {
    "text": "of epsilon_t plus\ntwice the covariance",
    "start": "507715",
    "end": "515099"
  },
  {
    "text": "between this term and this,\nbut those are uncorrelated. And so this is equal to B\ncovariance of f_t B transpose",
    "start": "515100",
    "end": "527519"
  },
  {
    "text": "plus psi. ",
    "start": "527520",
    "end": "534700"
  },
  {
    "text": "With m assets, how\nmany parameters are in the covariance\nmatrix if there's",
    "start": "534700",
    "end": "539890"
  },
  {
    "text": "no constraints on the\ncovariance matrix? AUDIENCE: [INAUDIBLE]. ",
    "start": "539890",
    "end": "547340"
  },
  {
    "text": "PROFESSOR: How many parameters? Right. So this is sigma. So the number of\nparameters in sigma.",
    "start": "547340",
    "end": "555214"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE].  PROFESSOR: m plus what?",
    "start": "555214",
    "end": "561595"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. PROFESSOR: OK, this is\na square matrix, m by m.",
    "start": "561595",
    "end": "569250"
  },
  {
    "text": "So there's possibly m\nsquared, but it's symmetric. So we're double-counting\noff the diagonal.",
    "start": "569250",
    "end": "576660"
  },
  {
    "text": "So it's m times m plus 1 over 2. ",
    "start": "576660",
    "end": "583490"
  },
  {
    "text": "How many parameters do we\nhave with the factor model? ",
    "start": "583490",
    "end": "592149"
  },
  {
    "text": "So if we think of a--\nlet's call this sigma star.",
    "start": "592150",
    "end": "597210"
  },
  {
    "text": "The number of parameters\nin sigma star is what? ",
    "start": "597210",
    "end": "605810"
  },
  {
    "text": "Well, B is an m by k matrix.",
    "start": "605810",
    "end": "610986"
  },
  {
    "text": " This is m by k, so we have\npossibly m times k values.",
    "start": "610986",
    "end": "622315"
  },
  {
    "text": " The f_x is-- or the\ncovariance of f_t",
    "start": "622315",
    "end": "639029"
  },
  {
    "text": "is the number of elements in\nthe covariance matrix of f,",
    "start": "639030",
    "end": "645460"
  },
  {
    "text": "which is k by k. And then we have psi, which\nis a diagonal of dimension m.",
    "start": "645460",
    "end": "658470"
  },
  {
    "text": "So depending on how\nwe structure things, we can have many, many fewer\nparameters in this factor model",
    "start": "658470",
    "end": "663970"
  },
  {
    "text": "than in the unconstrained case. And we're going to see\nthat we can actually reduce this number in the\ncovariance matrix of f",
    "start": "663970",
    "end": "672630"
  },
  {
    "text": "dramatically because\nof flexibility in choosing those factors. ",
    "start": "672630",
    "end": "681939"
  },
  {
    "text": "Well let's also look at the\ninterpretation of the factor",
    "start": "681940",
    "end": "687990"
  },
  {
    "text": "model as a series of\ntime series regressions. You remember when we talked\nabout multivariate regression",
    "start": "687990",
    "end": "695410"
  },
  {
    "text": "a few lectures ago, we\ntalked about cross-sectional regressions and time\nseries regressions,",
    "start": "695410",
    "end": "701760"
  },
  {
    "text": "and looking at the collection\nof all the regressions in a multivariate\nregression setting.",
    "start": "701760",
    "end": "707770"
  },
  {
    "text": "Here we can do the same thing. In contrast to the\ncross-sectional regression where we're fixing time and\nlooking at all the assets,",
    "start": "707770",
    "end": "715680"
  },
  {
    "text": "here we're looking at fixing\nthe asset i and the regression",
    "start": "715680",
    "end": "721570"
  },
  {
    "text": "over time for that single asset. So the values of x_i,\nranging from time 1",
    "start": "721570",
    "end": "729980"
  },
  {
    "text": "up to time capital T, basically\nfollows a regression model",
    "start": "729980",
    "end": "736130"
  },
  {
    "text": "that's equal to the intercept\nalpha_i plus this matrix F",
    "start": "736130",
    "end": "742890"
  },
  {
    "text": "times beta_i, where beta_i\ncorresponds to the regression",
    "start": "742890",
    "end": "750055"
  },
  {
    "text": "parameters in this\nregression, but they are the factor corresponding to\nan asset i on the different k",
    "start": "750055",
    "end": "755985"
  },
  {
    "text": "factors.  In this setting, the covariance\nmatrix of the epsilon_i vector",
    "start": "755985",
    "end": "765470"
  },
  {
    "text": "is the diagonal matrix sigma\nsquared times the identity.",
    "start": "765470",
    "end": "770639"
  },
  {
    "text": "And so this is the classic\nGauss-Markov assumptions for a single linear\nregression model.",
    "start": "770640",
    "end": "778180"
  },
  {
    "start": "778180",
    "end": "784529"
  },
  {
    "text": "Well, as we did previously,\nwe can group together",
    "start": "784530",
    "end": "789600"
  },
  {
    "text": "all of these time series\nregressions for all the m assets together by simply\nputting them all together.",
    "start": "789600",
    "end": "799220"
  },
  {
    "text": "So we start off with x_i\nequal to basically F beta_i",
    "start": "799220",
    "end": "808620"
  },
  {
    "text": "plus epsilon_i. And we can basically\nconsider x_1, x_2, up to x_n.",
    "start": "808620",
    "end": "819980"
  },
  {
    "text": "So we have a T by m\nmatrix for the m assets.",
    "start": "819980",
    "end": "826260"
  },
  {
    "text": "And that's equal to a regression\nmodel given by basically",
    "start": "826260",
    "end": "836230"
  },
  {
    "text": "what's on the slides here. So basically, we're able to\ngroup everything together",
    "start": "836230",
    "end": "841370"
  },
  {
    "text": "and deal with everything all\nat once, which computationally is applied in fitting these.",
    "start": "841370",
    "end": "848530"
  },
  {
    "start": "848530",
    "end": "856630"
  },
  {
    "text": "Let's look at the simplest\nexample of a factor model.",
    "start": "856630",
    "end": "861780"
  },
  {
    "text": "This is the single-factor\nmodel of Sharpe. We went through the capital\nasset pricing model,",
    "start": "861780",
    "end": "867640"
  },
  {
    "text": "how returns on assets and\nstocks are basically--",
    "start": "867640",
    "end": "873382"
  },
  {
    "text": "the excess return on\nstock can be modeled in terms as a linear\nregression on the excess return",
    "start": "873382",
    "end": "879360"
  },
  {
    "text": "of the market. And the regression\nparameter beta_i corresponds to the level of\nrisk associated with the asset.",
    "start": "879360",
    "end": "888760"
  },
  {
    "text": "And all we're doing\nin this model is,",
    "start": "888760",
    "end": "894110"
  },
  {
    "text": "by choosing different\nassets we're choosing assets with different levels of\nrisk scaled by the beta_i.",
    "start": "894110",
    "end": "901800"
  },
  {
    "text": "And they may have\nreturns that vary across assets given by alpha_i.",
    "start": "901800",
    "end": "908760"
  },
  {
    "text": "The covariance\nmatrix of the assets",
    "start": "908760",
    "end": "916380"
  },
  {
    "text": "has-- the unconditional\ncovariance matrix has this structure. It's basically equal to the\nvariance of the market times",
    "start": "916380",
    "end": "925190"
  },
  {
    "text": "beta beta prime plus psi. And so that equation\nis really very simple.",
    "start": "925190",
    "end": "933780"
  },
  {
    "text": " It's really self-evident from\nwhat we've discussed, but let",
    "start": "933780",
    "end": "941270"
  },
  {
    "text": "me just highlight what it is. Sigma squared beta beta\ntransposed plus psi.",
    "start": "941270",
    "end": "953276"
  },
  {
    "text": "And that's equal\nto sigma squared times basically a column vector\nof all the betas, beta_1 down",
    "start": "953276",
    "end": "958720"
  },
  {
    "text": "to beta_m times its transpose\nplus a diagonal matrix",
    "start": "958720",
    "end": "968459"
  },
  {
    "text": "with the psi. So this is really a very,\nvery simple structure for the covariance.",
    "start": "968460",
    "end": "974790"
  },
  {
    "text": "And if you had wanted to\napply this model to thousands of securities, it's\nbasically no problem.",
    "start": "974790",
    "end": "980850"
  },
  {
    "text": "You can construct a\ncovariance matrix. And if this were appropriate\nfor a large collection",
    "start": "980850",
    "end": "986510"
  },
  {
    "text": "of securities, then the\namount-- the reduction in terms of what you're\nestimating is enormous.",
    "start": "986510",
    "end": "995810"
  },
  {
    "text": "Rather than estimating\neach cross-correlation and covariance of\nall the assets,",
    "start": "995810",
    "end": "1004190"
  },
  {
    "text": "the factor model tells us what\nthose cross covariances are. So that's really where the\npower of the model comes in.",
    "start": "1004190",
    "end": "1014141"
  },
  {
    "text": "And in terms of why\nis this so useful, well in portfolio management\none of the key drivers",
    "start": "1014141",
    "end": "1023980"
  },
  {
    "text": "of asset allocation is\nthe covariance matrix of the assets. So if you have an\neffective model",
    "start": "1023980",
    "end": "1029909"
  },
  {
    "text": "for modeling the\ncovariance, then that simplifies the portfolio\nallocation problem",
    "start": "1029910",
    "end": "1034919"
  },
  {
    "text": "because you can specify\na covariance matrix that you are confident with.",
    "start": "1034920",
    "end": "1040510"
  },
  {
    "text": "And also in risk\nmanagement, effective models",
    "start": "1040510",
    "end": "1048089"
  },
  {
    "text": "of risk management\ndeal with, how do we anticipate what would\nhappen if different scenarios",
    "start": "1048089",
    "end": "1057820"
  },
  {
    "text": "occur in the market? Well, the different\nscenarios that can occur can be associated with what's\nhappening to underlying factors",
    "start": "1057820",
    "end": "1065899"
  },
  {
    "text": "that affect the system. And so we can consider\nrisk management approaches",
    "start": "1065900",
    "end": "1071580"
  },
  {
    "text": "that vary these underlying\nfactors, and look at how that has an impact\non the covariance matrix",
    "start": "1071580",
    "end": "1077172"
  },
  {
    "text": "very directly. ",
    "start": "1077172",
    "end": "1084640"
  },
  {
    "text": "Estimation of Sharpe's\nsingle index model. We went through\nbefore how we estimate",
    "start": "1084640",
    "end": "1091460"
  },
  {
    "text": "the alphas and the betas. In terms of estimating\nthe sigmas--",
    "start": "1091460",
    "end": "1097440"
  },
  {
    "text": "the specific\nvariances-- basically, that comes from the\nsimple regression as well.",
    "start": "1097440",
    "end": "1103640"
  },
  {
    "text": "Basically, the sum of the\nsquared estimated residuals divided by t minus 2.",
    "start": "1103640",
    "end": "1108840"
  },
  {
    "text": "Here we're assuming unbiasedness\nbecause we have two parameters estimated per model.",
    "start": "1108840",
    "end": "1114220"
  },
  {
    "text": "Then for the market\nportfolio, that basically",
    "start": "1114220",
    "end": "1120700"
  },
  {
    "text": "has a simple estimate as well. The psi hat matrix would\njust be the diagonal",
    "start": "1120700",
    "end": "1126470"
  },
  {
    "text": "of that-- the diagonal of\nthe specific variances.",
    "start": "1126470",
    "end": "1133679"
  },
  {
    "text": "And then the unconditional\ncovariance matrix is estimated by simply plugging\nin these parameter estimates.",
    "start": "1133680",
    "end": "1140940"
  },
  {
    "text": "So, very simple and effective\nif that single-factor",
    "start": "1140940",
    "end": "1148490"
  },
  {
    "text": "model is appropriate. Now needless to say,\na single-factor model",
    "start": "1148490",
    "end": "1153660"
  },
  {
    "text": "doesn't characterize the\nstructure of the covariances",
    "start": "1153660",
    "end": "1158860"
  },
  {
    "text": "and/or the returns typically. And so we want to consider\nmore general models,",
    "start": "1158860",
    "end": "1166860"
  },
  {
    "text": "multi-factor models. And the first set of models\nwe're going to talk about are common factor variables\nthat can actually be observed.",
    "start": "1166860",
    "end": "1176640"
  },
  {
    "text": " Basically any factor\nthat you can observe",
    "start": "1176640",
    "end": "1184430"
  },
  {
    "text": "is a potential candidate\nfor being a relevant factor",
    "start": "1184430",
    "end": "1189480"
  },
  {
    "text": "in a linear factor model. The effectiveness of\na potential factor is determined by\nfitting the model",
    "start": "1189480",
    "end": "1196490"
  },
  {
    "text": "and seeing how much\ncontribution that factor makes to the explanation\nof the returns",
    "start": "1196490",
    "end": "1203567"
  },
  {
    "text": "and the covariance structure.  Chen, Ross, and Roll wrote\na classic paper in 1986.",
    "start": "1203567",
    "end": "1212970"
  },
  {
    "text": "Now Ross is actually\nhere at MIT. And with their paper,\nthey looked at modeling--",
    "start": "1212970",
    "end": "1230580"
  },
  {
    "text": "rather than looking\nat these factors directly, including\nthose in the model, they looked at\ntransforming these factors",
    "start": "1230580",
    "end": "1239940"
  },
  {
    "text": "into surprise factors. So rather than having\ninterest rates just",
    "start": "1239940",
    "end": "1247549"
  },
  {
    "text": "as a simple factor directly\nplugged into the model, it would be the change\nin interest rates.",
    "start": "1247550",
    "end": "1254100"
  },
  {
    "text": "And additionally, not only just\nthe change in interest rate, but the unanticipated\nchange in interest",
    "start": "1254100",
    "end": "1259180"
  },
  {
    "text": "rates given market information. So their paper goes\nthrough modeling different",
    "start": "1259180",
    "end": "1267480"
  },
  {
    "text": "macroeconomic variables with\nvector autoregression models, and then using those to\nspecify unanticipated changes",
    "start": "1267480",
    "end": "1277270"
  },
  {
    "text": "in these underlying factors. And so that's where\nthe power comes in.",
    "start": "1277270",
    "end": "1282680"
  },
  {
    "text": "And that highlights how when\nyou're applying these models, it does involve some\ncreativity to get the most bang",
    "start": "1282680",
    "end": "1290960"
  },
  {
    "text": "for the buck with your models. And the idea they had of\nincorporating unanticipated",
    "start": "1290960",
    "end": "1296779"
  },
  {
    "text": "changes was really\na very good one and is applied quite widely now. ",
    "start": "1296780",
    "end": "1315050"
  },
  {
    "text": "Now with this setup,\none basically--",
    "start": "1315050",
    "end": "1323380"
  },
  {
    "text": "if one has empirical data over\ntimes 1 through capital T,",
    "start": "1323380",
    "end": "1330040"
  },
  {
    "text": "then if one wants to\nspecify these models, one has their observations\non the x_i process.",
    "start": "1330040",
    "end": "1337740"
  },
  {
    "text": "You basically have observed\nall the returns historically. We also, because the\nfactors are observable,",
    "start": "1337740",
    "end": "1344940"
  },
  {
    "text": "have the F matrix as a\nset of observed variables. So we can basically use those to\nestimate the beta_i's and also",
    "start": "1344940",
    "end": "1356300"
  },
  {
    "text": "estimate the variances\nof the residual terms with simple regression methods.",
    "start": "1356300",
    "end": "1364309"
  },
  {
    "text": "So implementing these\nis quite feasible,",
    "start": "1364310",
    "end": "1369970"
  },
  {
    "text": "and basically applies methods\nthat we have from before. So what this slide now discusses\nis how we basically estimate",
    "start": "1369970",
    "end": "1381110"
  },
  {
    "text": "the underlying parameters. We need to be a little bit\ncareful about the Gauss-Markov",
    "start": "1381110",
    "end": "1386990"
  },
  {
    "text": "assumptions. You'll remember that if we\nhave a regression model where",
    "start": "1386990",
    "end": "1395100"
  },
  {
    "text": "the residual terms are\nuncorrelated and constant variance, then the\nsimple linear regression",
    "start": "1395100",
    "end": "1402560"
  },
  {
    "text": "estimates are the best ones. If there is unequal\nvariances of the residuals,",
    "start": "1402560",
    "end": "1412740"
  },
  {
    "text": "and maybe even\ncovariances, then we need to use generalized\nleast squares.",
    "start": "1412740",
    "end": "1420250"
  },
  {
    "text": "So the notes go through those\ncomputations and the formulas,",
    "start": "1420250",
    "end": "1426850"
  },
  {
    "text": "which are just simple extensions\nof our regression model theory that we had in\nprevious lectures.",
    "start": "1426850",
    "end": "1433755"
  },
  {
    "start": "1433755",
    "end": "1444240"
  },
  {
    "text": "Let me go through an example.",
    "start": "1444240",
    "end": "1451433"
  },
  {
    "start": "1451433",
    "end": "1457720"
  },
  {
    "text": "With common factor\nvariables that are using either fundamental\nor asset-specific attributes,",
    "start": "1457720",
    "end": "1465560"
  },
  {
    "text": "there's the approach of-- well,\nit's called the BARRA Approach. This is from Barr Rosenberg. ",
    "start": "1465560",
    "end": "1473470"
  },
  {
    "text": "Actually, I have to say, he\nwas one of the inspirations to me for going into statistical\nmodeling and finance.",
    "start": "1473470",
    "end": "1481039"
  },
  {
    "text": "He was a professor at UC\nBerkeley who left academics very early to basically\napply models in trade money.",
    "start": "1481040",
    "end": "1491770"
  },
  {
    "text": "As an anecdote, his\ncurrent situation is a little different.",
    "start": "1491770",
    "end": "1497210"
  },
  {
    "text": "I'll let you look that up. But anyway, this\napproach basically",
    "start": "1497210",
    "end": "1504170"
  },
  {
    "text": "provided the BARRA Approach\nfor factor modeling and risk",
    "start": "1504170",
    "end": "1509260"
  },
  {
    "text": "analysis, which is still\nused extensively today. ",
    "start": "1509260",
    "end": "1515360"
  },
  {
    "text": "So with common factor\nvariables using",
    "start": "1515360",
    "end": "1523710"
  },
  {
    "text": "asset-specific\nattributes-- in fact,",
    "start": "1523710",
    "end": "1528740"
  },
  {
    "text": "the factor realizations\nare unobserved",
    "start": "1528740",
    "end": "1533890"
  },
  {
    "text": "but are estimated in the\napplication of these models.",
    "start": "1533890",
    "end": "1538960"
  },
  {
    "text": "So let's see how that goes. Oh, OK, this slide talks about\nthe Fama-French approach, which",
    "start": "1538960",
    "end": "1550410"
  },
  {
    "text": "concerns-- OK, Fama and\nFrench, Fama of course we talked about him\nin the last lecture.",
    "start": "1550410",
    "end": "1556780"
  },
  {
    "text": "He got the Nobel\nPrize for his work in modeling asset price\nreturns and market efficiency.",
    "start": "1556780",
    "end": "1565220"
  },
  {
    "text": "Fama and French\nfound that there were some very important\nfactors affecting",
    "start": "1565220",
    "end": "1571860"
  },
  {
    "text": "asset returns in equities. Basically, returns\ntended to vary depending",
    "start": "1571860",
    "end": "1578299"
  },
  {
    "text": "upon the size of firms. So if you consider small\nfirms versus large firms,",
    "start": "1578300",
    "end": "1585680"
  },
  {
    "text": "small firms tended to\nhave returns that were more similar to each other. Large firms tended to\nhave returns that were more similar to each other.",
    "start": "1585680",
    "end": "1592110"
  },
  {
    "text": "So there's basically a\nbig versus small factor that is operating in the market.",
    "start": "1592110",
    "end": "1598500"
  },
  {
    "text": "Sometimes the market\nprefers small stocks, sometimes it prefers\nlarge stocks. And similarly,\nthere's another factor",
    "start": "1598500",
    "end": "1608580"
  },
  {
    "text": "which is value versus growth. ",
    "start": "1608580",
    "end": "1614029"
  },
  {
    "text": "Basically, stocks that\nare considered good values are stocks which are cheap,\nbasically, for what they have.",
    "start": "1614030",
    "end": "1622914"
  },
  {
    "text": "So you're basically getting\na stock at a discount if you're getting a good value.",
    "start": "1622914",
    "end": "1628390"
  },
  {
    "text": "And value stocks can be\nmeasured by looking at the price to book equity. If that's low, then\nthe price you're",
    "start": "1628390",
    "end": "1635940"
  },
  {
    "text": "paying for that equity in the\nfirm is low, and it's cheap. And that compares\nwith stocks for which",
    "start": "1635940",
    "end": "1644150"
  },
  {
    "text": "the price relative to the\nbook value is very, very high. Why are people willing\nto pay a lot for stocks?",
    "start": "1644150",
    "end": "1652240"
  },
  {
    "text": "In that case, well it's\nbecause the growth prospects of those stocks is high,\nand there's an anticipation",
    "start": "1652240",
    "end": "1659030"
  },
  {
    "text": "basically that the\ncurrent price is just reflecting a projection of how\nmuch growth potential there is.",
    "start": "1659030",
    "end": "1667610"
  },
  {
    "text": "Now the Fama French approach\nis for each of these factors to basically rank order all\nthe stocks by this factor",
    "start": "1667610",
    "end": "1677080"
  },
  {
    "text": "and divide them\nup into quintiles. So say this is market cap.",
    "start": "1677080",
    "end": "1686550"
  },
  {
    "text": "We can divide up all the\nstocks in-- basically",
    "start": "1686550",
    "end": "1692030"
  },
  {
    "text": "consider a histogram,\nor whatever, of all the market caps of all\nthe stocks in our universe.",
    "start": "1692030",
    "end": "1698230"
  },
  {
    "text": "And then divide it up into\nbasically the bottom fifth,",
    "start": "1698230",
    "end": "1703500"
  },
  {
    "text": "the next fifth, and\nthen-- it probably needs to go up-- the top fifth.",
    "start": "1703500",
    "end": "1709830"
  },
  {
    "text": " And the Fama-French\napproach says, well,",
    "start": "1709830",
    "end": "1715960"
  },
  {
    "text": "let's look at an equal-weighted\naverage of the top fifth.",
    "start": "1715960",
    "end": "1721080"
  },
  {
    "text": "And basically, buy that\nand sell the bottom fifth.",
    "start": "1721080",
    "end": "1730919"
  },
  {
    "text": "And so that would be the\nbig versus small market factor of Fama and French.",
    "start": "1730920",
    "end": "1737640"
  },
  {
    "text": "Now, if you look at\ntheir work, they actually do the bottom minus the\ntop, because the value",
    "start": "1737640",
    "end": "1743080"
  },
  {
    "text": "tends to outperform the other. So they have a factor\nwhose more positive values and associated\nmore generally",
    "start": "1743080",
    "end": "1748510"
  },
  {
    "text": "with positive returns. But that factor can\nbe applied and used",
    "start": "1748510",
    "end": "1754780"
  },
  {
    "text": "to correlate with individual\nasset returns as well.",
    "start": "1754780",
    "end": "1760400"
  },
  {
    "start": "1760400",
    "end": "1766590"
  },
  {
    "text": "Now, with the BARRA\nIndustry Factor-- this is just getting back\nto the BARRA Approach--",
    "start": "1766590",
    "end": "1775960"
  },
  {
    "text": "the simplest case\nof understanding the BARRA industry\nfactor models is to consider looking\nat dividing stocks up",
    "start": "1775960",
    "end": "1782820"
  },
  {
    "text": "into different industry groups. So we might expect that,\nsay oil stocks will",
    "start": "1782820",
    "end": "1796610"
  },
  {
    "text": "tend to move together and\nhave greater variability",
    "start": "1796610",
    "end": "1802100"
  },
  {
    "text": "or common variability. And that could be very different\nfrom utility stocks, which",
    "start": "1802100",
    "end": "1810640"
  },
  {
    "text": "tend to actually be\nquite low-risk stocks. ",
    "start": "1810640",
    "end": "1817749"
  },
  {
    "text": "Utility companies\nare companies which are very highly regulated. And the profitability\nof those firms",
    "start": "1817749",
    "end": "1826360"
  },
  {
    "text": "is basically overlooked\nby the regulators. They don't want the\nutilities to gouge consumers",
    "start": "1826360",
    "end": "1837110"
  },
  {
    "text": "and make too much profit from\ndelivering power to customers. So utilities tend to have\nfairly low volatility",
    "start": "1837110",
    "end": "1846555"
  },
  {
    "text": "but very consistent\nreturns, which are based on reasonable,\nfrom a regulatory standpoint,",
    "start": "1846555",
    "end": "1855529"
  },
  {
    "text": "levels of profitability\nfor those companies. Well with an industry\nfactor model, what we can do",
    "start": "1855530",
    "end": "1863270"
  },
  {
    "text": "is associate factor\nloadings, which basically",
    "start": "1863270",
    "end": "1868710"
  },
  {
    "text": "are loading each asset in\nterms of which industry group it's in.",
    "start": "1868710",
    "end": "1874519"
  },
  {
    "text": "So we actually know the beta\nvalues for these stocks,",
    "start": "1874520",
    "end": "1880140"
  },
  {
    "text": "but we don't know\nthe underlying factor realizations for these stocks.",
    "start": "1880140",
    "end": "1886399"
  },
  {
    "text": "But in terms of the\nbetas, with these factors we can basically get a well\ndefined beta vectors and B",
    "start": "1886400",
    "end": "1894641"
  },
  {
    "text": "matrix for all the stocks. And the problem\nthen is, how do we",
    "start": "1894641",
    "end": "1900650"
  },
  {
    "text": "specify the realization\nof the underlying factors? Well the realization of\nthe underlying factors",
    "start": "1900650",
    "end": "1911000"
  },
  {
    "text": "basically is just estimated\nwith a regression model.",
    "start": "1911000",
    "end": "1916190"
  },
  {
    "text": "And so if we have all of our\nassets x_i for different times",
    "start": "1916190",
    "end": "1926299"
  },
  {
    "text": "t, those would have a model\ngiven by factor realizations",
    "start": "1926300",
    "end": "1933700"
  },
  {
    "text": "corresponding to the k industry\ngroups with known beta_(i,j)",
    "start": "1933700",
    "end": "1941380"
  },
  {
    "text": "values. ",
    "start": "1941380",
    "end": "1949010"
  },
  {
    "text": "And the estimation of\nthese, we basically",
    "start": "1949010",
    "end": "1954030"
  },
  {
    "text": "have a simple\nregression model where the realizations of\nthe factor returns f_t",
    "start": "1954030",
    "end": "1963059"
  },
  {
    "text": "are given by essentially\na regression coefficient in this regression, where\nwe have the asset returns",
    "start": "1963060",
    "end": "1970270"
  },
  {
    "text": "x_t, the known\nfactor loadings B, the unknown factor\nrealizations f_t.",
    "start": "1970270",
    "end": "1978520"
  },
  {
    "text": "And just plugging\ninto the regression, if we do it very simply\nwe get this expression",
    "start": "1978520",
    "end": "1985500"
  },
  {
    "text": "for f hat t, which is the\nsimple linear regression model",
    "start": "1985500",
    "end": "1990710"
  },
  {
    "text": "estimating those realizations. Now this particular estimate\nof the factor realizations",
    "start": "1990710",
    "end": "2000660"
  },
  {
    "text": "is assuming that the variability\nof the components of x",
    "start": "2000660",
    "end": "2009020"
  },
  {
    "text": "have the same variance. This is like the\nlinear regression estimates under normal\nGauss-Markov assumptions.",
    "start": "2009020",
    "end": "2016940"
  },
  {
    "text": "But basically the\nepsilon_i's will",
    "start": "2016940",
    "end": "2022970"
  },
  {
    "text": "vary across the\ndifferent assets. The different assets will\nhave different variabilities, different specific variances.",
    "start": "2022970",
    "end": "2028500"
  },
  {
    "text": "So that's actually going\nto be heteroscedasticity",
    "start": "2028500",
    "end": "2033630"
  },
  {
    "text": "in these models. So this particular vector\nof industry averages should actually be extended\nto accommodate for that.",
    "start": "2033630",
    "end": "2046669"
  },
  {
    "text": "So we have the estimation\nof the covariance matrix of the factors can\nthen be estimated",
    "start": "2046670",
    "end": "2054899"
  },
  {
    "text": "using these estimates\nof the realizations. And our estimation of the\nresidual covariance matrix",
    "start": "2054900",
    "end": "2061598"
  },
  {
    "text": "can then be estimated.  So I guess an initial estimate\nof the covariance matrix sigma",
    "start": "2061599",
    "end": "2069638"
  },
  {
    "text": "hat is given by this known\nmatrix B times our sample estimate of the factor\nrealizations plus the diagonal",
    "start": "2069639",
    "end": "2079339"
  },
  {
    "text": "matrix C hat. And a second step\nin this process",
    "start": "2079340",
    "end": "2086310"
  },
  {
    "text": "can incorporate\ninformation about there being heteroscedasticity along\nthe diagonal of the psi's",
    "start": "2086310",
    "end": "2093658"
  },
  {
    "text": "to adjust the\nregression estimates. So we basically get a\nrefinement of the estimates",
    "start": "2093659",
    "end": "2100950"
  },
  {
    "text": "that does account for the\nnon-constant variability. Now this property of\nheteroscedasticity verses",
    "start": "2100950",
    "end": "2113750"
  },
  {
    "text": "homoscedasticity in estimating\nthe regression parameters,",
    "start": "2113750",
    "end": "2120270"
  },
  {
    "text": "it may seem like\nthis is a nicety of the statistical theory that\nwe just have to try and check,",
    "start": "2120270",
    "end": "2127550"
  },
  {
    "text": "but it's not too big a deal. But let me highlight where this\nissue comes up again and again.",
    "start": "2127550",
    "end": "2136819"
  },
  {
    "text": "With portfolio optimization,\nwe went through last time--",
    "start": "2136820",
    "end": "2143880"
  },
  {
    "text": "for mean-variance\noptimization, we want to consider a weighting of\nassets that basically weights",
    "start": "2143880",
    "end": "2150550"
  },
  {
    "text": "the assets by the expected\nreturns, pre-multiplied",
    "start": "2150550",
    "end": "2156640"
  },
  {
    "text": "by the inverse of the\ncovariance matrix. And so we basically in\nportfolio allocation",
    "start": "2156640",
    "end": "2164150"
  },
  {
    "text": "want to allocate to\nassets with high return, but we're going to penalize\nthose with high variance.",
    "start": "2164150",
    "end": "2170970"
  },
  {
    "text": "And so the impact of discounting\nvalues with high variability",
    "start": "2170970",
    "end": "2181450"
  },
  {
    "text": "arises in asset allocation. And then of course arises\nin statistical estimation.",
    "start": "2181450",
    "end": "2187480"
  },
  {
    "text": "Basically with signals\nwith high noise, you want to normalize\nby the level of noise",
    "start": "2187480",
    "end": "2193069"
  },
  {
    "text": "before you incorporate the\nimpact of that variable on the particular model.",
    "start": "2193070",
    "end": "2198900"
  },
  {
    "start": "2198900",
    "end": "2205400"
  },
  {
    "text": "So here are just some notes\nabout the inefficiency of estimates due to\nheteroscedasticity. We can apply generalized\nleast squares.",
    "start": "2205400",
    "end": "2213032"
  },
  {
    "text": "A second bullet here is\nthat factor realizations can be scaled to represent\nfactor mimicking portfolios.",
    "start": "2213032",
    "end": "2220063"
  },
  {
    "text": " Now with the\nFama-French factors,",
    "start": "2220063",
    "end": "2226359"
  },
  {
    "text": "where we have say big\nversus small stocks or value versus growth\nstocks, it would be",
    "start": "2226360",
    "end": "2231410"
  },
  {
    "text": "nice to know, well what's\nthe real value of trading that factor?",
    "start": "2231410",
    "end": "2237390"
  },
  {
    "text": "If you were to have unit\nweight to trading that factor, would you make money or not?",
    "start": "2237390",
    "end": "2242740"
  },
  {
    "text": "Or under what periods\nwould you make money? And the notion of factor\nmimicking portfolios",
    "start": "2242740",
    "end": "2251010"
  },
  {
    "text": "is important. Let's go back to the\nspecification of the factor",
    "start": "2251010",
    "end": "2258060"
  },
  {
    "text": "realizations here. f hat t, the t-th realization\nof the factors, their k factors,",
    "start": "2258060",
    "end": "2268210"
  },
  {
    "text": "is given by essentially\nthe regression estimate of those factors from the\nrealizations of the asset",
    "start": "2268210",
    "end": "2274400"
  },
  {
    "text": "returns. And if we're doing\nthis in the proper way, we'd be correcting for\nthe heteroscedasticity.",
    "start": "2274400",
    "end": "2281369"
  },
  {
    "text": "Well this realization\nof the factor returns",
    "start": "2281370",
    "end": "2286810"
  },
  {
    "text": "is a weighted average or\na weighted sum of the x_t.",
    "start": "2286810",
    "end": "2297430"
  },
  {
    "text": "So we have basically f_t\nis equal to a matrix times",
    "start": "2297430",
    "end": "2309099"
  },
  {
    "text": "x_t, where this is B B prime\ntoe the minus one, B prime.",
    "start": "2309100",
    "end": "2319409"
  },
  {
    "text": " So our k-dimensional\nrealizations-- let's see,",
    "start": "2319409",
    "end": "2329216"
  },
  {
    "text": "this is basically k by 1. ",
    "start": "2329216",
    "end": "2337470"
  },
  {
    "text": "Each of these k factors is\na weighted sum of these x's.",
    "start": "2337470",
    "end": "2344099"
  },
  {
    "text": "Now the x's, if these are\nreturns on the underlying assets, then we can consider\nnormalizing these factors.",
    "start": "2344100",
    "end": "2353770"
  },
  {
    "text": "Or basically normalizing\nthis matrix here so that the row weights sum to\n1, say, for a unit of capital.",
    "start": "2353770",
    "end": "2363880"
  },
  {
    "text": "So if we were to invest\na net unit of one capital unit in these assets, then\nthis factor realization",
    "start": "2363880",
    "end": "2372549"
  },
  {
    "text": "would give us the return on\na portfolio of the assets",
    "start": "2372550",
    "end": "2378510"
  },
  {
    "text": "that is perfectly correlated\nwith the factor realization. So factor mimicking portfolios\ncan be defined that way.",
    "start": "2378510",
    "end": "2389820"
  },
  {
    "text": "And they have a\ngood interpretation in terms of the realization\nof potential investments.",
    "start": "2389820",
    "end": "2397080"
  },
  {
    "start": "2397080",
    "end": "2402980"
  },
  {
    "text": "So let's go back. ",
    "start": "2402980",
    "end": "2418000"
  },
  {
    "text": "The next subject is\nstatistical factor models. This is the case where\nwe begin the analysis",
    "start": "2418000",
    "end": "2426540"
  },
  {
    "text": "with just our collection of\noutcomes for the process x_t.",
    "start": "2426540",
    "end": "2431740"
  },
  {
    "text": "So basically our\ntime series of asset returns for m assets\nover T time units.",
    "start": "2431740",
    "end": "2440099"
  },
  {
    "text": "And we have no clue initially\nwhat the underlying factors are, but we hypothesize\nthat there are factors that",
    "start": "2440100",
    "end": "2447360"
  },
  {
    "text": "do characterize the returns. And factor analysis and\nprincipal components analysis provide ways of uncovering\nthose underlying factors",
    "start": "2447360",
    "end": "2457890"
  },
  {
    "text": "and defining them in terms\nof the data themselves. ",
    "start": "2457890",
    "end": "2475540"
  },
  {
    "text": "So we'll first talk\nabout factor analysis. Then we'll turn to principal\ncomponents analysis.",
    "start": "2475540",
    "end": "2481290"
  },
  {
    "text": "Both of these\nmethods are efforts to model the covariance matrix.",
    "start": "2481290",
    "end": "2489360"
  },
  {
    "text": "And the underlying covariance\nmatrix for the assets x",
    "start": "2489360",
    "end": "2497710"
  },
  {
    "text": "can be estimated with sample\ndata in terms of the sample covariance matrix. So here I've just written\nout in matrix form",
    "start": "2497710",
    "end": "2505090"
  },
  {
    "text": "how that would be computed. And so with this m by T\nmatrix x, we basically",
    "start": "2505090",
    "end": "2517260"
  },
  {
    "text": "take that matrix, take\nout the means by computing",
    "start": "2517260",
    "end": "2523280"
  },
  {
    "text": "the means with multiplying\nby this matrix, and then take the\nsum of deviations",
    "start": "2523280",
    "end": "2530480"
  },
  {
    "text": "about the means for\nall the m assets individually and\nacross each other,",
    "start": "2530480",
    "end": "2537470"
  },
  {
    "text": "and divide that by capital T.",
    "start": "2537470",
    "end": "2560170"
  },
  {
    "text": "Now, the setup for\nstatistical factor models is exactly the same as\nbefore, except the only thing",
    "start": "2560170",
    "end": "2568810"
  },
  {
    "text": "that we observe is x_t. So we're hypothesizing a\nmodel where alpha is basically",
    "start": "2568810",
    "end": "2579490"
  },
  {
    "text": "a vector that is basically\nthe vector of mean returns",
    "start": "2579490",
    "end": "2586580"
  },
  {
    "text": "of the individual assets. B is a matrix of factor\nloadings on k factors f_t.",
    "start": "2586580",
    "end": "2594500"
  },
  {
    "text": "And epsilon_t is white\nnoise with mean 0, covariance matrix\ngiven by the diagonal.",
    "start": "2594500",
    "end": "2600910"
  },
  {
    "text": "So the setup here is the\nsame basic setup as before, but we don't have the\nmatrix B or the vector f_t.",
    "start": "2600910",
    "end": "2613099"
  },
  {
    "text": " Or, of course, alpha. ",
    "start": "2613100",
    "end": "2619920"
  },
  {
    "text": "Now in this setup,\nit's important that there is an\nindeterminacy of this model,",
    "start": "2619920",
    "end": "2629920"
  },
  {
    "text": "because for any given\nspecification of the matrix B",
    "start": "2629920",
    "end": "2637450"
  },
  {
    "text": "or the factors f, we can\nactually transform those",
    "start": "2637450",
    "end": "2644540"
  },
  {
    "text": "by a k by k invertible\nmatrix H. So for a given",
    "start": "2644540",
    "end": "2649840"
  },
  {
    "text": "specification of this model,\nif we transform the underlying factor realizations f by the\nmatrix H, which is k by k,",
    "start": "2649840",
    "end": "2659230"
  },
  {
    "text": "then if we transform the\nfactor loadings B by H inverse,",
    "start": "2659230",
    "end": "2665890"
  },
  {
    "text": "we get the same model. So there is an indeterminacy\nhere, or a-- OK,",
    "start": "2665890",
    "end": "2671800"
  },
  {
    "text": "there's an indeterminacy of\nthese particular variables,",
    "start": "2671800",
    "end": "2677940"
  },
  {
    "text": "but there's basically\nflexibility in how we define the factor model.",
    "start": "2677940",
    "end": "2684630"
  },
  {
    "text": "So in trying to uncover a factor\nmodel with statistical factor analysis, there is\nsome flexibility",
    "start": "2684630",
    "end": "2690990"
  },
  {
    "text": "in defining our factors. We can arbitrarily\ntransform the factors",
    "start": "2690990",
    "end": "2697029"
  },
  {
    "text": "by an invertible\ntransformation in the k space. ",
    "start": "2697030",
    "end": "2715050"
  },
  {
    "text": "And I guess it's important\nto note that what changes when we do that transformation?",
    "start": "2715050",
    "end": "2722550"
  },
  {
    "text": "Well the linear\nfunction stays the same in terms of the covariance\nmatrix of the underlying",
    "start": "2722550",
    "end": "2728040"
  },
  {
    "text": "factors. Well, if we have a covariance\nmatrix for those underlying factors, we need to accommodate\nthe matrix transformation",
    "start": "2728040",
    "end": "2736349"
  },
  {
    "text": "H in that. So that has an impact there. But one of the\nthings we can do is",
    "start": "2736350",
    "end": "2744029"
  },
  {
    "text": "consider trying to\ndefine a matrix H, that",
    "start": "2744030",
    "end": "2749040"
  },
  {
    "text": "diagonalizes the factors. So in some settings, it's useful\nto consider factor models where you have uncorrelated\nfactor components.",
    "start": "2749040",
    "end": "2760260"
  },
  {
    "text": "And it's possible to\ndefine linear factor models with uncorrelated factor\ncomponents by a choice of H.",
    "start": "2760260",
    "end": "2769289"
  },
  {
    "text": "So with any linear\nfactor model in fact, we can have uncorrelated factor\ncomponents if that's useful.",
    "start": "2769290",
    "end": "2777571"
  },
  {
    "text": " So this first bullet\nhighlights that point",
    "start": "2777571",
    "end": "2786300"
  },
  {
    "text": "that we can get\northonormal factors. ",
    "start": "2786300",
    "end": "2792930"
  },
  {
    "text": "And we can also have\nzero mean factors by adjusting the\ndata to incorporate",
    "start": "2792930",
    "end": "2801530"
  },
  {
    "text": "the mean of these factors.  And if we make these\nparticular assumptions,",
    "start": "2801530",
    "end": "2813290"
  },
  {
    "text": "then the model does\nsimplify to just being the covariance matrix\nsigma_x is the factor",
    "start": "2813290",
    "end": "2822400"
  },
  {
    "text": "loadings B times its transpose\nplus a diagonal matrix.",
    "start": "2822400",
    "end": "2828020"
  },
  {
    "text": "And just to reiterate,\nthe power of this is basically no matter how\nlarge m is, as m increases",
    "start": "2828020",
    "end": "2839130"
  },
  {
    "text": "the B matrix just increases\nby k for every increment in m.",
    "start": "2839130",
    "end": "2848004"
  },
  {
    "text": "And we also have an additional\ndiagonal entry on the psi. So as we add more and more\nassets to our modeling,",
    "start": "2848004",
    "end": "2859320"
  },
  {
    "text": "the complexity basically\ndoesn't increase very much. ",
    "start": "2859320",
    "end": "2871720"
  },
  {
    "text": "With all of our statistical\nmodels, one of the questions is how do we specify the\nparticular parameters?",
    "start": "2871720",
    "end": "2879850"
  },
  {
    "text": "Maximum likelihood estimation is\nthe first thing to go through,",
    "start": "2879850",
    "end": "2885930"
  },
  {
    "text": "and with normal\nlinear factor models",
    "start": "2885930",
    "end": "2892050"
  },
  {
    "text": "we have normal\ndistributions for all the underlying random variables. So the residuals\nepsilon_t are independent",
    "start": "2892050",
    "end": "2899755"
  },
  {
    "text": "and identically distributed,\nmultivariate normal dimension m with diagonal matrix psi given\nby the individual elements'",
    "start": "2899755",
    "end": "2910760"
  },
  {
    "text": "variances. f_t, the realization\nof the factors,",
    "start": "2910760",
    "end": "2916950"
  },
  {
    "text": "the k-dimensional\nfactors can have mean 0, and just to have the\nidentity covariance",
    "start": "2916950",
    "end": "2923970"
  },
  {
    "text": "we can scale them and\nmake them uncorrelated. And then x_t will be\nnormally distributed",
    "start": "2923970",
    "end": "2933050"
  },
  {
    "text": "with mean alpha and\ncovariance matrix sigma_x given by the formulas\nin the previous slide.",
    "start": "2933050",
    "end": "2939210"
  },
  {
    "text": " With these assumptions,\nwe can write down",
    "start": "2939210",
    "end": "2945370"
  },
  {
    "text": "the model likelihood. The model likelihood\nis the joint density",
    "start": "2945370",
    "end": "2950440"
  },
  {
    "text": "of our data given the\nunknown parameters. ",
    "start": "2950440",
    "end": "2962670"
  },
  {
    "text": "And the standard setup actually\nfor statistical factor modeling",
    "start": "2962670",
    "end": "2968720"
  },
  {
    "text": "is to assume\nindependence over time. Now we know that there can\nbe time series dependence.",
    "start": "2968720",
    "end": "2974900"
  },
  {
    "text": "We won't deal with\nthat at this point. Let's just assume that they\nare independent across time.",
    "start": "2974900",
    "end": "2981340"
  },
  {
    "text": "Then we can consider this\nas simply the product of the conditional density\nof x_t given alpha and sigma,",
    "start": "2981340",
    "end": "2991570"
  },
  {
    "text": "which has this form. This form for the density\nfunction of a multivariate",
    "start": "2991570",
    "end": "3000120"
  },
  {
    "text": "normal should be very\nfamiliar to you at this point.",
    "start": "3000120",
    "end": "3005210"
  },
  {
    "text": "It's basically the extension\nof the univariate normal distribution to m-variate. So we have 1 over the square\nroot of pi to the m power.",
    "start": "3005210",
    "end": "3014370"
  },
  {
    "text": "There are m components. And then we divide by the square\nroot of the individual variance",
    "start": "3014370",
    "end": "3023170"
  },
  {
    "text": "or the determinant of\nthe covariance matrix. And then the exponential\nof this term here,",
    "start": "3023170",
    "end": "3031970"
  },
  {
    "text": "which for the t-th case is\na quadratic form in the x's.",
    "start": "3031970",
    "end": "3041369"
  },
  {
    "text": "So this multivariate normal\nx, we take off its mean and look at the quadratic\nform of that with the inverse",
    "start": "3041370",
    "end": "3048759"
  },
  {
    "text": "of its covariance matrix. ",
    "start": "3048759",
    "end": "3057650"
  },
  {
    "text": "So there's the\nlog-likelihood function. It reduces to this form here.",
    "start": "3057650",
    "end": "3066400"
  },
  {
    "text": "And maximum likelihood\nestimation methods can be applied to specify all\nthe parameters of B and psi.",
    "start": "3066400",
    "end": "3076549"
  },
  {
    "text": "And there's an EM algorithm,\nwhich is applied in this case.",
    "start": "3076550",
    "end": "3083620"
  },
  {
    "text": "I think I may have\nhighlighted it before, but the EM algorithm is a very\npowerful estimation methodology",
    "start": "3083620",
    "end": "3090000"
  },
  {
    "text": "for maximum likelihood\nin statistics. When one has very\ncomplicated models which",
    "start": "3090000",
    "end": "3100520"
  },
  {
    "text": "can be simplified-- well,\nmodels that are complicated by the fact that we have\nhidden variables-- basically",
    "start": "3100520",
    "end": "3107830"
  },
  {
    "text": "the hidden variables lead\nto very complex likelihood functions.",
    "start": "3107830",
    "end": "3114550"
  },
  {
    "text": "A simplification\nof the EM algorithm is that if we could observe\nsome of the hidden variables,",
    "start": "3114550",
    "end": "3120450"
  },
  {
    "text": "then our likelihood\nfunctions are very simple and can be computed directly. And the EM algorithm\nalternates estimating",
    "start": "3120450",
    "end": "3130820"
  },
  {
    "text": "the hidden variables, assuming\nthe hidden variables are known doing the simple estimates with\nthe observed hidden variables,",
    "start": "3130820",
    "end": "3138362"
  },
  {
    "text": "and then estimating the\nhidden variables again, and just iterating that\nprocess again and again. And it converges.",
    "start": "3138362",
    "end": "3144100"
  },
  {
    "text": "And their paper\ndemonstrates that this applies in many, many\ndifferent application settings.",
    "start": "3144100",
    "end": "3149980"
  },
  {
    "text": "And it's just a very, very\npowerful estimation methodology that is applied here with\nstatistical factor analysis.",
    "start": "3149980",
    "end": "3159900"
  },
  {
    "text": "I indicated that for now we\ncould just assume independence",
    "start": "3159900",
    "end": "3165460"
  },
  {
    "text": "over time of the data points\nin computing its likelihood. You recall our discussion\na couple of lectures back",
    "start": "3165460",
    "end": "3173060"
  },
  {
    "text": "about the state-space models,\nlinear state-space models. Essentially, that linear\nstate-space model framework",
    "start": "3173060",
    "end": "3180710"
  },
  {
    "text": "can be applied here as\nwell to incorporate time dependence in the data as well.",
    "start": "3180710",
    "end": "3186840"
  },
  {
    "text": " So that simplification is not\nbinding in terms of holding us",
    "start": "3186840",
    "end": "3196190"
  },
  {
    "text": "up in estimating these models. ",
    "start": "3196190",
    "end": "3205555"
  },
  {
    "text": "Let me go back here, OK. So the maximum likelihood\nestimation process",
    "start": "3205555",
    "end": "3212160"
  },
  {
    "text": "will give us estimates of the\nB matrix and the psi matrix.",
    "start": "3212160",
    "end": "3217920"
  },
  {
    "text": "So applying this EM\nalgorithm, a good computer",
    "start": "3217920",
    "end": "3223630"
  },
  {
    "text": "can actually get estimates of\nB and psi and the underlying",
    "start": "3223630",
    "end": "3231880"
  },
  {
    "text": "alpha, of course. Now from these we can estimate\nthe factor realizations f_t.",
    "start": "3231880",
    "end": "3243660"
  },
  {
    "text": "And these can be estimated by\nsimply this regression formula,",
    "start": "3243660",
    "end": "3251559"
  },
  {
    "text": "using our estimates for\nthe factor loadings B hat, our estimates of\nalpha, we can actually",
    "start": "3251560",
    "end": "3257720"
  },
  {
    "text": "estimate the factor\nrealizations. So with statistical\nfactor analysis,",
    "start": "3257720",
    "end": "3264390"
  },
  {
    "text": "we use the EM algorithm to\nestimate the covariance matrix parameters. Then the next step, we\ncan estimate the factor",
    "start": "3264390",
    "end": "3272455"
  },
  {
    "text": "realizations.  So as the output\nfrom factor analysis,",
    "start": "3272455",
    "end": "3281310"
  },
  {
    "text": "we can work with these\nfactor realizations. And those realizations\nor those estimates",
    "start": "3281310",
    "end": "3290610"
  },
  {
    "text": "of the realizations\nof the factors can then be used basically\nfor risk modeling as well.",
    "start": "3290610",
    "end": "3300570"
  },
  {
    "text": "So we could do a statistical\nfactor analysis of returns",
    "start": "3300570",
    "end": "3310150"
  },
  {
    "text": "in, say, the\ncommodities markets.",
    "start": "3310150",
    "end": "3315980"
  },
  {
    "text": "And identify what factors are\ndriving returns and covariances",
    "start": "3315980",
    "end": "3321609"
  },
  {
    "text": "in commodity markets. We can then get estimates\nof those underlying factors from the methodology.",
    "start": "3321610",
    "end": "3329570"
  },
  {
    "text": "We could then use those\nas inputs to other models. Certain stocks may depend\non significant factors",
    "start": "3329570",
    "end": "3335210"
  },
  {
    "text": "in the commodity markets. And what they depend on, well\nwe can use statistical modeling",
    "start": "3335210",
    "end": "3341309"
  },
  {
    "text": "to identify where\nthe dependencies are. So getting these realizations\nof the statistical factors",
    "start": "3341310",
    "end": "3349529"
  },
  {
    "text": "is very useful, not\nonly to understand what happened in the\npast with the process",
    "start": "3349530",
    "end": "3355330"
  },
  {
    "text": "and how these\nunderlying factors vary, but you can also use those\nas inputs to other models. ",
    "start": "3355330",
    "end": "3371770"
  },
  {
    "text": "Finally, let's see,\nthere was a lot",
    "start": "3371770",
    "end": "3376950"
  },
  {
    "text": "of interest with\nstatistical factor analysis on the interpretation\nof the underlying factors.",
    "start": "3376950",
    "end": "3383030"
  },
  {
    "text": "Of course, in terms\nof using any model,",
    "start": "3383030",
    "end": "3388320"
  },
  {
    "text": "it's once confidence\nrises when you have highly interpretable results.",
    "start": "3388320",
    "end": "3394580"
  },
  {
    "text": "One of the initial applications\nof statistical factor analysis was in measuring IQ.",
    "start": "3394580",
    "end": "3400309"
  },
  {
    "text": "And how many people here\nhave taken an IQ test? Probably everybody\nor almost everybody?",
    "start": "3400310",
    "end": "3407580"
  },
  {
    "text": "Well actually if you want to\nwork for some hedge funds, you'll have to\ntake some IQ tests.",
    "start": "3407580",
    "end": "3414690"
  },
  {
    "text": "But basically in an IQ test\nthere are 20, 30, 40 questions.",
    "start": "3414690",
    "end": "3420401"
  },
  {
    "text": "And they're trying to\nmeasure different aspects of your ability. And statistical\nfactor analysis has",
    "start": "3420402",
    "end": "3429819"
  },
  {
    "text": "been used to try and understand\nwhat are the underlying dimensions of intelligence.",
    "start": "3429820",
    "end": "3434930"
  },
  {
    "text": "And one has the\nflexibility of considering",
    "start": "3434930",
    "end": "3441930"
  },
  {
    "text": "different transformations\nof any given set of estimated factors by this\nH matrix for transformation.",
    "start": "3441930",
    "end": "3450290"
  },
  {
    "text": "And so there has been work in\nstatistical factor analysis to find rotations of\nthe factor loadings",
    "start": "3450290",
    "end": "3458520"
  },
  {
    "text": "that make the factors\nmore interpretable. So I just raise that as there's\npotential to get insight",
    "start": "3458520",
    "end": "3468650"
  },
  {
    "text": "into these underlying factors\nif that's appropriate. In the IQ setting, the\neffort was actually",
    "start": "3468650",
    "end": "3474099"
  },
  {
    "text": "to try and find what\nare interpretations of different dimensions\nof intelligence?",
    "start": "3474100",
    "end": "3479645"
  },
  {
    "start": "3479645",
    "end": "3487400"
  },
  {
    "text": "We previously talked about\nfactor mimicking portfolios. The same thing applies.",
    "start": "3487400",
    "end": "3493280"
  },
  {
    "text": "One final thing is with\nlikelihood ratio tests,",
    "start": "3493280",
    "end": "3498460"
  },
  {
    "text": "one can test for whether\nthe linear factor model is",
    "start": "3498460",
    "end": "3503700"
  },
  {
    "text": "a good description of the data. And so with likelihood\nratio tests,",
    "start": "3503700",
    "end": "3509950"
  },
  {
    "text": "we compare the\nlikelihood of the data where we fit our unknown\nparameters, the mean vector",
    "start": "3509950",
    "end": "3516650"
  },
  {
    "text": "alpha and covariance matrix\nsigma, without any constraints. And then we compare that\nto the likelihood function",
    "start": "3516650",
    "end": "3525849"
  },
  {
    "text": "under the factor model\nwith, say, k factors. And the likelihood\nratio tests are",
    "start": "3525850",
    "end": "3536930"
  },
  {
    "text": "computed by looking at twice the\ndifference in log likelihoods. If you take an advanced\ncourse in statistics,",
    "start": "3536930",
    "end": "3544710"
  },
  {
    "text": "you'll see that basically this\ndifference in the likelihood functions under many conditions\nis approximately a chi",
    "start": "3544710",
    "end": "3553280"
  },
  {
    "text": "squared random\nvariable with degrees of freedom equal to the\ndifference in parameters under the two models.",
    "start": "3553280",
    "end": "3560070"
  },
  {
    "text": "So that's why it's\nspecified this way.",
    "start": "3560070",
    "end": "3565230"
  },
  {
    "text": "But anyway, one can test for\nthe dimensionality of the factor model. ",
    "start": "3565230",
    "end": "3573940"
  },
  {
    "text": "Before going into an\nexample of factor modeling, I want to cover principal\ncomponents analysis.",
    "start": "3573940",
    "end": "3579890"
  },
  {
    "text": " Actually, principal\ncomponents analysis comes up in factor\nmodeling, but it's also",
    "start": "3579890",
    "end": "3586990"
  },
  {
    "text": "a methodology that's appropriate\nfor modeling multivariate data",
    "start": "3586990",
    "end": "3592700"
  },
  {
    "text": "and considering\ndimensionality reduction. You're dealing with data\nin very many dimensions.",
    "start": "3592700",
    "end": "3599750"
  },
  {
    "text": "You're wondering is there\na simple characterization",
    "start": "3599750",
    "end": "3605680"
  },
  {
    "text": "of the multivariate\nstructure that lies in a smaller dimensional space?",
    "start": "3605680",
    "end": "3610770"
  },
  {
    "text": "And principle components\nanalysis gives us that. The theoretical framework for\nprincipal components analysis",
    "start": "3610770",
    "end": "3618319"
  },
  {
    "text": "is to consider an\nm-variate random variable. So this is like a single\nrealization of asset returns",
    "start": "3618320",
    "end": "3627650"
  },
  {
    "text": "in a given time, which has\nsome mean and covariance matrix sigma. ",
    "start": "3627650",
    "end": "3634876"
  },
  {
    "text": "The principal\ncomponents analysis is going to exploit the\neigenvalues and eigenvectors",
    "start": "3634876",
    "end": "3641190"
  },
  {
    "text": "of the covariance matrix.  Choongbum went through\neigenvalues and singular value",
    "start": "3641190",
    "end": "3650320"
  },
  {
    "text": "decompositions. So here we basically have\nthe eigenvalue/eigenvector",
    "start": "3650320",
    "end": "3655640"
  },
  {
    "text": "decomposition of our\ncovariance matrix sigma, which is the scalar eigenvalues\ntimes the eigenvector",
    "start": "3655640",
    "end": "3664700"
  },
  {
    "text": "gamma_i times its transpose. So we actually are able to\ndecompose our covariance matrix",
    "start": "3664700",
    "end": "3672900"
  },
  {
    "text": "with eigenvalues, eigenvectors. The principal\ncomponent variables",
    "start": "3672900",
    "end": "3680980"
  },
  {
    "text": "are to consider taking away the\nmean from the random vector x,",
    "start": "3680980",
    "end": "3688190"
  },
  {
    "text": "alpha. And then consider the weighted\naverage of those de-meaned x's",
    "start": "3688190",
    "end": "3695799"
  },
  {
    "text": "given by the i-th eigenvector. So these are going to be\ncalled the principal component",
    "start": "3695800",
    "end": "3702210"
  },
  {
    "text": "variables, where gamma_1 is\nthe first one corresponding to the largest eigenvalue.",
    "start": "3702210",
    "end": "3708450"
  },
  {
    "text": "Gamma m is going to be the\nm-th, or last, corresponding to the smallest. ",
    "start": "3708450",
    "end": "3719690"
  },
  {
    "text": "The properties of these\nprincipal component variables",
    "start": "3719690",
    "end": "3727650"
  },
  {
    "text": "is that they have mean 0,\nand their covariance matrix",
    "start": "3727650",
    "end": "3734029"
  },
  {
    "text": "is given by the diagonal\nmatrix of eigenvalues. So the principal\ncomponent variables",
    "start": "3734030",
    "end": "3741670"
  },
  {
    "text": "are a very simple sort\nof affine transformation of the original variable x.",
    "start": "3741670",
    "end": "3749760"
  },
  {
    "text": "We translate x to a new origin,\nbasically to the 0 origin,",
    "start": "3749760",
    "end": "3758450"
  },
  {
    "text": "by subtracting the means off it. And then we multiply\nthat de-meaned x value",
    "start": "3758450",
    "end": "3766710"
  },
  {
    "text": "by an orthogonal\nmatrix gamma prime. ",
    "start": "3766710",
    "end": "3774004"
  },
  {
    "text": "And what does that do? That simply rotates\nthe coordinate axes.",
    "start": "3774004",
    "end": "3779450"
  },
  {
    "text": "So what we're doing is creating\na new coordinate system for our data, which\nhasn't changed",
    "start": "3779450",
    "end": "3787860"
  },
  {
    "text": "the relative position of the\ndata or the random variable at all in the space.",
    "start": "3787860",
    "end": "3794599"
  },
  {
    "text": "Basically, it just is using\na new coordinate system with no change in the\noverall variability of what",
    "start": "3794600",
    "end": "3802389"
  },
  {
    "text": "we're working with. ",
    "start": "3802389",
    "end": "3818170"
  },
  {
    "text": "In matrix form, we can express\nthis principal component",
    "start": "3818170",
    "end": "3826349"
  },
  {
    "text": "variables p. ",
    "start": "3826350",
    "end": "3831539"
  },
  {
    "text": "Let's consider partitioning\np into the first k elements and the last\nm minus k elements p_2.",
    "start": "3831540",
    "end": "3839750"
  },
  {
    "text": "Then our original random vector\nx has this decomposition.",
    "start": "3839750",
    "end": "3845463"
  },
  {
    "text": " And we can think of this\nas being approximately",
    "start": "3845463",
    "end": "3853530"
  },
  {
    "text": "a linear factor model. ",
    "start": "3853530",
    "end": "3859790"
  },
  {
    "text": "We can consider from\nprincipal components analysis essentially if p_1, the\nprinciple component variables,",
    "start": "3859790",
    "end": "3866720"
  },
  {
    "text": "correspond to our factors,\nthen our linear factor model",
    "start": "3866720",
    "end": "3872900"
  },
  {
    "text": "would have B as given by\ngamma_1, F as given by p_1.",
    "start": "3872900",
    "end": "3877940"
  },
  {
    "text": "And our epsilon vector would\nbe given by gamma_2 p_2. So the principal\ncomponents decomposition",
    "start": "3877940",
    "end": "3885109"
  },
  {
    "text": "is almost a linear factor model. The only issue is this\ngamma_2 p_2 is an m-vector,",
    "start": "3885110",
    "end": "3899910"
  },
  {
    "text": "but it may not have a\ndiagonal covariance matrix.",
    "start": "3899910",
    "end": "3906339"
  },
  {
    "text": "Under the linear factor model\nwith a given set of factors k less than m, we\nalways are assuming",
    "start": "3906340",
    "end": "3912630"
  },
  {
    "text": "that the residual vector\nhas covariance matrix",
    "start": "3912630",
    "end": "3917809"
  },
  {
    "text": "equal to a diagonal. With a principal\ncomponents analysis, that may or may not be true.",
    "start": "3917810",
    "end": "3925810"
  },
  {
    "text": "So this is like an\napproximate factor model, but that's why this is called\nprincipal components analysis.",
    "start": "3925810",
    "end": "3932540"
  },
  {
    "text": "It's not called principal\nfactor analysis yet. ",
    "start": "3932540",
    "end": "3945130"
  },
  {
    "text": "The empirical principal\ncomponents analysis now. We've gone through\njust a description",
    "start": "3945130",
    "end": "3951870"
  },
  {
    "text": "of theoretical principal\ncomponents, where if we have a mean vector alpha,\ncovariance matrix sigma, how",
    "start": "3951870",
    "end": "3958453"
  },
  {
    "text": "we would define these\nprinciple component variables. If we just have sample\ndata, then this slide",
    "start": "3958454",
    "end": "3965400"
  },
  {
    "text": "goes through the computations\nof the empirical principal components results. So all we're doing is\nsubstituting in estimates",
    "start": "3965400",
    "end": "3974119"
  },
  {
    "text": "of the means and\ncovariance matrix, and computing the\neigenvalue/eigenvector decomposition of that.",
    "start": "3974120",
    "end": "3981060"
  },
  {
    "text": "And we get sample principal\ncomponent variables which are-- we basically\ncompute x, the de-meaned vector,",
    "start": "3981060",
    "end": "3991720"
  },
  {
    "text": "or matrix of realizations and\npre-multiply that by gamma hat",
    "start": "3991720",
    "end": "3998780"
  },
  {
    "text": "prime, which is the\nmatrix of eigenvectors",
    "start": "3998780",
    "end": "4004470"
  },
  {
    "text": "corresponding to the\neigenvalue/eigenvector decomposition of the\nsample covariance matrix. ",
    "start": "4004470",
    "end": "4014790"
  },
  {
    "text": "This slide goes through the\nsingular value decomposition.",
    "start": "4014790",
    "end": "4019960"
  },
  {
    "text": "You don't have to go through and\ncompute variances, covariances to derive estimates of the\nprincipal component variables.",
    "start": "4019960",
    "end": "4028340"
  },
  {
    "text": "You can work simply with the\nsingular value decomposition. I'll let you go through\nthat on your own.",
    "start": "4028340",
    "end": "4035804"
  },
  {
    "text": "There's an alternate definition\nof the principal component variable though\nthat's very important. ",
    "start": "4035804",
    "end": "4047270"
  },
  {
    "text": "If we consider a\nlinear combination",
    "start": "4047270",
    "end": "4052470"
  },
  {
    "text": "of the components of x, x_1\nthrough x_m, given by w,",
    "start": "4052470",
    "end": "4060849"
  },
  {
    "text": "if we consider a linear\ncombination of that which maximizes the variability\nof that linear combination",
    "start": "4060850",
    "end": "4068390"
  },
  {
    "text": "subject to the norm of the\ncoefficients w equals 1,",
    "start": "4068390",
    "end": "4076039"
  },
  {
    "text": "then this is the first\nprincipal component variable. So if we have in two\ndimensions the x_1 and x_2,",
    "start": "4076040",
    "end": "4088250"
  },
  {
    "text": "if we have points that look like\nan ellipsoidal distribution,",
    "start": "4088250",
    "end": "4101540"
  },
  {
    "text": "this would correspond to having\nalpha 1 there, alpha 2 there,",
    "start": "4101540",
    "end": "4108850"
  },
  {
    "text": "a sort of degree of variability. The principal\ncomponents analysis",
    "start": "4108850",
    "end": "4115770"
  },
  {
    "text": "says, let's shift to the origin\nbeing at (alpha_1, alpha_2).",
    "start": "4115770",
    "end": "4122620"
  },
  {
    "text": "And then let's rotate the axes\nto align with the eigenvectors.",
    "start": "4122620",
    "end": "4130370"
  },
  {
    "text": "Well the first principal\ncomponent variable finds the dimension at which\nthe coordinate axis at which",
    "start": "4130370",
    "end": "4142549"
  },
  {
    "text": "the variability is a maximum. And basically along\nthis dimension here, this is where\nthe variability",
    "start": "4142550",
    "end": "4151790"
  },
  {
    "text": "would be the maximum. And that's the first\nprincipal component variable. So this principal\ncomponents analysis",
    "start": "4151790",
    "end": "4158659"
  },
  {
    "text": "is identifying\nessentially where's there the most\nvariability in the data, where it's the most variability\nwithout doing any change",
    "start": "4158660",
    "end": "4168390"
  },
  {
    "text": "in the scaling of the data? All we're doing is\nshifting and rotating.",
    "start": "4168390",
    "end": "4173816"
  },
  {
    "text": "Then the second principal\ncomponent variable is basically the\ndirection which is orthogonal to the first, which\nhas the maximum variance.",
    "start": "4173816",
    "end": "4182528"
  },
  {
    "text": "And continuing that\nprocess to define all m principal component variables.",
    "start": "4182529",
    "end": "4188159"
  },
  {
    "start": "4188160",
    "end": "4196780"
  },
  {
    "text": "In principal\ncomponents analysis, there's discussions of the\ntotal variability of the data and how well that's explained\nby principal components.",
    "start": "4196780",
    "end": "4205460"
  },
  {
    "text": "If we have a covariance\nmatrix sigma, the total variance\ncan be defined",
    "start": "4205460",
    "end": "4213389"
  },
  {
    "text": "and is defined as the sum\nof the diagonal entries. So it's the trace of\na covariance matrix.",
    "start": "4213390",
    "end": "4221040"
  },
  {
    "text": "We'll call that the total\nvariance of this multivariate x.",
    "start": "4221040",
    "end": "4227160"
  },
  {
    "text": "That is equal to the sum\nof the eigenvalues as well. So we have a decomposition\nof the total variability",
    "start": "4227160",
    "end": "4236520"
  },
  {
    "text": "into the variability of\ndifferent principal component variables.",
    "start": "4236520",
    "end": "4242070"
  },
  {
    "text": "And the principal\ncomponent variables themselves are uncorrelated.",
    "start": "4242070",
    "end": "4248459"
  },
  {
    "text": "You remember the\ncovariance matrix of the principal\ncomponent variables was the lambda, the diagonal\nmatrix of eigenvalues.",
    "start": "4248459",
    "end": "4256750"
  },
  {
    "text": "So the off-diagonal\nentries are 0. So the principal\ncomponent variables are uncorrelated, and\nhave variability lambda_k,",
    "start": "4256750",
    "end": "4265099"
  },
  {
    "text": "and basically decompose\nthe variability. So principal components\nanalysis provides this very nice\ndecomposition of the data",
    "start": "4265100",
    "end": "4274139"
  },
  {
    "text": "into different\ndimensions, with highest to lowest information content\nas given by the eigenvalues.",
    "start": "4274140",
    "end": "4282450"
  },
  {
    "start": "4282450",
    "end": "4288690"
  },
  {
    "text": "I want to go\nthrough a case study",
    "start": "4288690",
    "end": "4294140"
  },
  {
    "text": "here of doing factor modeling\nwith the U.S. Treasury yields.",
    "start": "4294140",
    "end": "4301295"
  },
  {
    "text": " I loaded in data into R, which\nranged from the beginning",
    "start": "4301295",
    "end": "4309040"
  },
  {
    "text": "of 2000 to the end of May 2013.",
    "start": "4309040",
    "end": "4314280"
  },
  {
    "text": "And here are the yields on\nconstant maturity U.S. Treasury securities ranging from\n3 months, 6 months,",
    "start": "4314280",
    "end": "4321100"
  },
  {
    "text": "up to 20 years. So this is essentially\nthe term structure of US Government [INAUDIBLE]\nof varying levels of risk.",
    "start": "4321100",
    "end": "4332858"
  },
  {
    "start": "4332858",
    "end": "4338170"
  },
  {
    "text": "Here's a plot of [INAUDIBLE]\nover that period.",
    "start": "4338170",
    "end": "4345292"
  },
  {
    "start": "4345292",
    "end": "4353148"
  },
  {
    "text": "So starting in the\n[INAUDIBLE], we can see this, the rather\ndramatic evolution of the term",
    "start": "4353148",
    "end": "4361570"
  },
  {
    "text": "structure over\nthis entire period. If we wanted to have\nset any [INAUDIBLE].",
    "start": "4361570",
    "end": "4367320"
  },
  {
    "start": "4367320",
    "end": "4372732"
  },
  {
    "text": "If we wanted to do a principal\ncomponents analysis of this, well if we did the\nentire period we'd",
    "start": "4372732",
    "end": "4377900"
  },
  {
    "text": "be measuring variability\nof all kinds of things. When things go down, up, down.",
    "start": "4377900",
    "end": "4383580"
  },
  {
    "text": "What I've done in this\nnote is just initially to look at the period\nfrom 2001 up through 2005.",
    "start": "4383580",
    "end": "4395750"
  },
  {
    "text": "So we have five years of data\non basically the early part of this period that I\nwant to focus on and do",
    "start": "4395750",
    "end": "4403380"
  },
  {
    "text": "a principal components analysis\nof the yields on this data.",
    "start": "4403380",
    "end": "4412340"
  },
  {
    "text": "So here's basically the series\nover that five year period.",
    "start": "4412340",
    "end": "4420844"
  },
  {
    "text": " Beginning of this\nanalysis, this analysis",
    "start": "4420845",
    "end": "4427110"
  },
  {
    "text": "is on the actual yield changes. So just as we might be modeling\nsay asset prices over time",
    "start": "4427110",
    "end": "4433940"
  },
  {
    "text": "and then doing an analysis\nof the changes, the returns, here we're looking\nat yield changes.",
    "start": "4433940",
    "end": "4440015"
  },
  {
    "start": "4440015",
    "end": "4447080"
  },
  {
    "text": "So first, you can see\nthere's-- basically, the average daily value for the\ndifferent yield tenors ranging",
    "start": "4447080",
    "end": "4457170"
  },
  {
    "text": "from 3 months up to 20, those\nare actually all negative. That corresponds to the time\nseries over that five year",
    "start": "4457170",
    "end": "4464360"
  },
  {
    "text": "period. Basically the time series\nwere all at lower levels",
    "start": "4464360",
    "end": "4469480"
  },
  {
    "text": "from beginning to\nend on average. The daily volatility is the\ndaily standard deviation.",
    "start": "4469480",
    "end": "4476400"
  },
  {
    "text": "Those vary from\n0.0384 up to .0698",
    "start": "4476400",
    "end": "4482590"
  },
  {
    "text": "for-- is that the three year? And this is the standard\ndeviation of daily yield",
    "start": "4482590",
    "end": "4489920"
  },
  {
    "text": "changes where 1 is like 1%.",
    "start": "4489920",
    "end": "4495650"
  },
  {
    "text": "And so basically it's\nbetween three and six basis",
    "start": "4495650",
    "end": "4502310"
  },
  {
    "text": "points a day are the variation\nin the yield changes. So that's something\nthat's reasonable. When you look at the\nnews or a newspaper",
    "start": "4502310",
    "end": "4510720"
  },
  {
    "text": "and see how interest\nrates change from one day to the next, it's generally\na few basis points",
    "start": "4510720",
    "end": "4515780"
  },
  {
    "text": "from one day to the next.  This next matrix is\nthe correlation matrix",
    "start": "4515780",
    "end": "4526560"
  },
  {
    "text": "of the yield changes.  If you look at\nthis closely, which",
    "start": "4526560",
    "end": "4532650"
  },
  {
    "text": "you can when you\ndownload these results,",
    "start": "4532650",
    "end": "4538480"
  },
  {
    "text": "you'll see that\nnear the diagonal the values are very high, like\nabove 90% for correlation.",
    "start": "4538480",
    "end": "4547870"
  },
  {
    "text": "And as you move across,\naway from the diagonal, the correlations\nget lower and lower.",
    "start": "4547870",
    "end": "4553918"
  },
  {
    "text": " Mathematically that\nis what is happening.",
    "start": "4553918",
    "end": "4562870"
  },
  {
    "text": "We can look at these things\ngraphically, which I always like to do. Here is just a graph, a bar\nchart of the yield changes",
    "start": "4562870",
    "end": "4571840"
  },
  {
    "text": "and the standard\ndeviations of the yield changes, daily volatilities\nranging from very short yields",
    "start": "4571840",
    "end": "4578910"
  },
  {
    "text": "to long-tenor yields,\nup to 20 years. ",
    "start": "4578910",
    "end": "4585956"
  },
  {
    "text": "So there's variability there. ",
    "start": "4585956",
    "end": "4595840"
  },
  {
    "text": "Here is a pairs\nplot of the data. So what I've done is\njust looked at basically",
    "start": "4595840",
    "end": "4605460"
  },
  {
    "text": "for every single tenor, this\nis say the 5 year, 7 year, 10 year, 20 year.",
    "start": "4605460",
    "end": "4613015"
  },
  {
    "text": "I basically plotted\nthe yield changes of each of those\nagainst each other. We could do this with basically\nall nine different tenors,",
    "start": "4613015",
    "end": "4621244"
  },
  {
    "text": "and we'd have a very dense\npage of a pairs plot.",
    "start": "4621245",
    "end": "4628690"
  },
  {
    "text": "So I split it up\ninto looking just at the top and bottom\nblock diagonals.",
    "start": "4628690",
    "end": "4634890"
  },
  {
    "text": "But you can see basically\nhow the correlation between these yield\nchanges is very tight",
    "start": "4634890",
    "end": "4643190"
  },
  {
    "text": "and then gets less tight\nas you move further away. With the long\ntenors-- let's see,",
    "start": "4643190",
    "end": "4653250"
  },
  {
    "text": "the short tenors--\none, one more.",
    "start": "4653250",
    "end": "4659030"
  },
  {
    "text": "Here the short tenors, ranging\nfrom 3 year, 2 year, 1 year,",
    "start": "4659030",
    "end": "4664070"
  },
  {
    "text": "6 month, and so forth. So here you can see how it\ngets less and less correlated as you move away\nfrom a given tenor.",
    "start": "4664070",
    "end": "4670950"
  },
  {
    "text": " Well the principal\ncomponents analysis",
    "start": "4670950",
    "end": "4678100"
  },
  {
    "text": "gives us-- if you conduct\na principal components,",
    "start": "4678100",
    "end": "4691700"
  },
  {
    "text": "basically the standard\noutput is first a table of how the\nvariability of the series",
    "start": "4691700",
    "end": "4698610"
  },
  {
    "text": "is broken down across the\ndifferent component variables. And so there's\nbasically the importance",
    "start": "4698610",
    "end": "4704989"
  },
  {
    "text": "of components for each of\nthe nine component variables where it's measured in terms\nof the relative squared",
    "start": "4704990",
    "end": "4716260"
  },
  {
    "text": "standard deviations of these\nvariables relative to the sum. And the proportion\nof variance explained",
    "start": "4716260",
    "end": "4723400"
  },
  {
    "text": "by the first component\nvariable is 0.849. So basically 85% of\nthe total variability",
    "start": "4723400",
    "end": "4730300"
  },
  {
    "text": "is explained by the first\nprincipal component variable. Looking at the second\nrow, second in, 0.0919,",
    "start": "4730300",
    "end": "4737990"
  },
  {
    "text": "that's the percentage\nof total variability explained by the second\nprincipal component variable.",
    "start": "4737990",
    "end": "4744250"
  },
  {
    "text": "So 9%. And then for third\nit's around 3%. And it just goes\ndown closer to 0,",
    "start": "4744250",
    "end": "4760940"
  },
  {
    "text": "There's a scree plot for\nprincipal components analysis, which is just a plot\nof the variability",
    "start": "4760940",
    "end": "4766351"
  },
  {
    "text": "of the different principal\ncomponent variables. So you can see whether\nthe principal components",
    "start": "4766352",
    "end": "4774510"
  },
  {
    "text": "is explaining much variability\nin the first few components or not. Here there's a huge\namount of variability",
    "start": "4774510",
    "end": "4781280"
  },
  {
    "text": "explained by the first\nprincipal component variable. I've plotted here the\nstandard deviations",
    "start": "4781280",
    "end": "4787930"
  },
  {
    "text": "of the original yield\nchanges in green, versus the standard deviations\nof the principal component",
    "start": "4787930",
    "end": "4792990"
  },
  {
    "text": "variables in blue. So we basically are modeling\nwith principal component",
    "start": "4792990",
    "end": "4802090"
  },
  {
    "text": "variables most of\nthe variability in the first few\nprincipal components. Now let's look at\nthe interpretation",
    "start": "4802090",
    "end": "4808489"
  },
  {
    "text": "of the principal\ncomponent variables. There's the loadings\nmatrix, which is the gamma matrix for the\nprincipal components variables.",
    "start": "4808489",
    "end": "4816280"
  },
  {
    "text": " Looking at numbers is\nless informative for me",
    "start": "4816280",
    "end": "4825199"
  },
  {
    "text": "than looking at graphs. Here's a plot of the loadings\non the different yield",
    "start": "4825200",
    "end": "4830620"
  },
  {
    "text": "changes for the first\nprincipal component variable. So the first principal\ncomponent variable",
    "start": "4830620",
    "end": "4836119"
  },
  {
    "text": "is a weighted average of\nall the yield changes, giving greatest weight\nto the five year.",
    "start": "4836120",
    "end": "4844690"
  },
  {
    "text": "What's that? Well that's just a measure of a\nlevel shift in the yield curve.",
    "start": "4844690",
    "end": "4851219"
  },
  {
    "text": "It's like, what's\nthe average yield change across the whole range? So that's what the first\nprincipal component",
    "start": "4851220",
    "end": "4857580"
  },
  {
    "text": "variable is measuring. The second principal component\nvariable gives positive weight",
    "start": "4857580",
    "end": "4863440"
  },
  {
    "text": "to the long tenors, negative\nweight to the short tenors. So it's looking at the\ndifference between the yield",
    "start": "4863440",
    "end": "4871540"
  },
  {
    "text": "changes on the long\ntenors verses the yield change on the short tenors. So that's looking at how the\nspread in yields is changing.",
    "start": "4871540",
    "end": "4879774"
  },
  {
    "start": "4879774",
    "end": "4887090"
  },
  {
    "text": "Then the third principal\ncomponent variable",
    "start": "4887090",
    "end": "4892250"
  },
  {
    "text": "has this structure. And this structure\nfor the weights",
    "start": "4892250",
    "end": "4898780"
  },
  {
    "text": "is like a double difference. It's looking at the difference\nbetween the long tenor",
    "start": "4898780",
    "end": "4904570"
  },
  {
    "text": "and medium tenor,\nminus the medium tenor, minus the short tenor.",
    "start": "4904570",
    "end": "4910710"
  },
  {
    "text": "So that's giving us a measure\nof the curvature of the term structure and how that's\nchanging over time.",
    "start": "4910710",
    "end": "4917440"
  },
  {
    "text": "So these principal\ncomponent variables are measuring the level\nshift for the first, the spread for the second, and\nthe curvature for the third.",
    "start": "4917440",
    "end": "4924710"
  },
  {
    "text": " With principal\ncomponents analysis, many times I think\npeople focus just",
    "start": "4924710",
    "end": "4931879"
  },
  {
    "text": "on the first few principal\ncomponent variables and then say they're done. The last principle component\nvariable, and the last few,",
    "start": "4931879",
    "end": "4939137"
  },
  {
    "text": "can be very, very\ninteresting as well, because these are the variables\nof the original scales,",
    "start": "4939137",
    "end": "4947640"
  },
  {
    "text": "the linear combinations which\nhave the least variability.",
    "start": "4947640",
    "end": "4953420"
  },
  {
    "text": "And if you look at the\nninth principle component variable-- there were\nnine yield changes here-- it's basically looking\nat a weighted average of the 5",
    "start": "4953420",
    "end": "4963810"
  },
  {
    "text": "and 10 year minus the 7 year. So this is like the hedge of the\n7 year yield with the 5 and 10",
    "start": "4963810",
    "end": "4973240"
  },
  {
    "text": "year.  So that difference\nin yield change",
    "start": "4973240",
    "end": "4980600"
  },
  {
    "text": "is-- that combination\nof yield change is going to have the\nleast variability. ",
    "start": "4980600",
    "end": "4987310"
  },
  {
    "text": "The principal\ncomponent variables have zero correlation. Here's just a pairs plot of the\nfirst three principal component",
    "start": "4987310",
    "end": "4994840"
  },
  {
    "text": "variables and the ninth. And you can see\nthat those have been transformed to have zero\ncorrelations with each other.",
    "start": "4994840",
    "end": "5002239"
  },
  {
    "text": " One can plot the cumulative\nprincipal component variables",
    "start": "5002240",
    "end": "5011540"
  },
  {
    "text": "over time to see how the\nevolution of these underlying factors has changed\nover the time period.",
    "start": "5011540",
    "end": "5018820"
  },
  {
    "text": "And you'll recall that\nwe talked about the first being the level shift. Basically from 2001 to 2005, the\noverall level of interest rates",
    "start": "5018820",
    "end": "5029750"
  },
  {
    "text": "went down and then went up. And this is captured by this\nfirst principal component variable accumulating from 0\ndown to minus 8, back up to 0.",
    "start": "5029750",
    "end": "5040770"
  },
  {
    "start": "5040770",
    "end": "5046170"
  },
  {
    "text": "And the scale of this\nchange from 0 to minus 8",
    "start": "5046170",
    "end": "5051920"
  },
  {
    "text": "is the amount of\ngreatest variability. The second principal\ncomponent variable",
    "start": "5051920",
    "end": "5059340"
  },
  {
    "text": "accumulates from 0 up to\nless than 6, back down to 0. So this is a measure\nof the spread",
    "start": "5059340",
    "end": "5067092"
  },
  {
    "text": "between long and short rates. So the spread\nincreased, and then it decreased over the period.",
    "start": "5067092",
    "end": "5073805"
  },
  {
    "start": "5073805",
    "end": "5079700"
  },
  {
    "text": "And then the curvature, it\nvaries from 0 down to minus 1.5",
    "start": "5079700",
    "end": "5086560"
  },
  {
    "text": "back up to 0. So how the curvature changed\nover this entire period was much, much less, which\nis perhaps as it should be.",
    "start": "5086560",
    "end": "5097260"
  },
  {
    "text": "But these graphs\nindicate basically how these underlying factors\nevolved over the time period.",
    "start": "5097260",
    "end": "5103710"
  },
  {
    "text": "In the case note I go through\nand fit a statistical factor",
    "start": "5103710",
    "end": "5110410"
  },
  {
    "text": "analysis model to\nthese same data and look at identifying\nthe number of factors.",
    "start": "5110410",
    "end": "5116980"
  },
  {
    "text": "And also comparing the results\nover this five year period with the period\nfrom 2009 to 2013,",
    "start": "5116980",
    "end": "5124030"
  },
  {
    "text": "and comparing those\ndifferent results. They are different,\nand so it really",
    "start": "5124030",
    "end": "5129970"
  },
  {
    "text": "matters over what period one\nspecifies these models to. And fitting these models is\nreally just a starting point",
    "start": "5129970",
    "end": "5137620"
  },
  {
    "text": "where you want to\nultimately model the dynamics in these\nfactors and their structural",
    "start": "5137620",
    "end": "5144450"
  },
  {
    "text": "relationships. So we'll finish there.",
    "start": "5144450",
    "end": "5149000"
  }
]