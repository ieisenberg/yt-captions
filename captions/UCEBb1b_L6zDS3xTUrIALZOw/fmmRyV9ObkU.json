[
  {
    "text": " The following content is\nprovided under a Creative Commons license.",
    "start": "0",
    "end": "5580"
  },
  {
    "text": "Your support will help\nMIT OpenCourseWare continue to offer high quality\neducational resources for free.",
    "start": "5580",
    "end": "12270"
  },
  {
    "text": "To make a donation or\nview additional materials from hundreds of MIT courses,\nvisit MIT OpenCourseWare",
    "start": "12270",
    "end": "18830"
  },
  {
    "text": "at ocw.mit.edu. LORENZO ROSASCO: So\nwhat we want to do now is to move away\nfrom local methods",
    "start": "18830",
    "end": "25040"
  },
  {
    "text": "and start to do some form of\nglobal regularization method. The word regularization I'm\ngoing to use broadly as a term",
    "start": "25040",
    "end": "33470"
  },
  {
    "text": "to define procedures,\nstatistical procedures and computational\nprocedure that do",
    "start": "33470",
    "end": "38540"
  },
  {
    "text": "have some parameters that\nallow to do from complex model to simple model in\na very broad sense.",
    "start": "38540",
    "end": "43879"
  },
  {
    "text": "What I mean by complex is\nsomething that is potentially going closer to overfitting and\nby simple model something that",
    "start": "43880",
    "end": "49910"
  },
  {
    "text": "is giving me something, which\nis stable with respect to data. So we're going to consider\nthe following algorithm.",
    "start": "49910",
    "end": "61800"
  },
  {
    "text": "I imagine a lot of you\nhave seen it before. This is called-- it has a\nbunch of different names--",
    "start": "61800",
    "end": "67890"
  },
  {
    "text": "probably the most famous one\nis Tikhonov regularization. A bunch of people at the\nbeginning of the '60s",
    "start": "67890",
    "end": "74520"
  },
  {
    "text": "thought about something\nsimilar either in the context of statistics or solving\nlinear equations.",
    "start": "74520",
    "end": "80974"
  },
  {
    "text": "So Tikhonov is the only one for\nwhich I can find the picture. The other one was\nPhilips, and then there is Hoerl and other people.",
    "start": "80974",
    "end": "88020"
  },
  {
    "text": "They basically thought all\nabout this same procedure. ",
    "start": "88020",
    "end": "94750"
  },
  {
    "text": "The procedure basically\nis based on a functional that you want to minimize\nbased on two terms.",
    "start": "94750",
    "end": "101380"
  },
  {
    "text": "So there are several\ningredients going on here. First of all, this is f of x. We assume the\nfunctional form of-- we",
    "start": "101380",
    "end": "107760"
  },
  {
    "text": "try to estimate the\nfunction, and we do assume a parametric form\nof this function, which in this case, just linear.",
    "start": "107760",
    "end": "114720"
  },
  {
    "text": "And for the time being,\nbecause you can really, you can put it back in, I\ndon't look at the offset.",
    "start": "114720",
    "end": "121229"
  },
  {
    "text": "So I just take lines\npassing through the origin. And this is just because\nyou can prove in one line that you can put back in\nthe offset at zero cost.",
    "start": "121230",
    "end": "129664"
  },
  {
    "text": "So for the time\nbeing, just think that data are actually standard. ",
    "start": "129665",
    "end": "134700"
  },
  {
    "text": "The way you try to estimate\nthis parameter is, on one hand, try to make the empirical error\nsmall, and on the other hand,",
    "start": "134700",
    "end": "142420"
  },
  {
    "text": "you put a budget on the weights.",
    "start": "142420",
    "end": "147844"
  },
  {
    "text": "The reason why you do this-- there are a bunch of\nway to explain this. Andrei yesterday talked about\nmargin, and different lines,",
    "start": "147844",
    "end": "154110"
  },
  {
    "text": "and so on. Another way to think about\nit is that you can convince yourself-- and we were\ngoing to see later--",
    "start": "154110",
    "end": "159720"
  },
  {
    "text": "that if you're in low dimension,\na line is a very poor model. Because basically if you\nhave more than a few points--",
    "start": "159720",
    "end": "167587"
  },
  {
    "text": "and they're not\nstanding on the line-- you will not be able\nto make zero error. But if the number\nof points is lower",
    "start": "167587",
    "end": "173099"
  },
  {
    "text": "than the number\nof dimension, you can show that the line actually\ncan give you zero error.",
    "start": "173100",
    "end": "178200"
  },
  {
    "text": "It's just a matter of\ndegrees of freedom. You have fewer equations\nthan the actual variables.",
    "start": "178200",
    "end": "184570"
  },
  {
    "text": "So what you do is\nthat you actually add a regularization theorem. It's basically a theorem that\nmakes the problem well-posed.",
    "start": "184570",
    "end": "191820"
  },
  {
    "text": "We're going to see\nthis in a minute from a different perspective. The easiest one is\ngoing to be numerical.",
    "start": "191820",
    "end": "198390"
  },
  {
    "text": "We stick to least squares\nfor-- and there is-- so there is an extra\nparenthesis that I forgot,",
    "start": "198390",
    "end": "206430"
  },
  {
    "text": "but before I tell you why\nwe use least squares also let me tell you that--\nas somebody pointed out-- there is a mistake here,\nbecause this is a minus.",
    "start": "206430",
    "end": "212611"
  },
  {
    "text": " It should just be a minus.",
    "start": "212611",
    "end": "218396"
  },
  {
    "text": "I'll fix this. So back, why do you\nuse least squares?",
    "start": "218396",
    "end": "223760"
  },
  {
    "text": "OK, so least squares\non the one hand, if you're in low\ndimension especially, you can think of least\nsquares as its way is basic,",
    "start": "223760",
    "end": "231340"
  },
  {
    "text": "but it's not a very robust\nway to measure error, because you squared them. And so just one error\ncan count a lot.",
    "start": "231340",
    "end": "240750"
  },
  {
    "text": "So typically, there\nis a whole literature on robust statistics, where\nyou want to replace least square with something\nlike an absolute value",
    "start": "240750",
    "end": "246122"
  },
  {
    "text": "or something like that. It turns out that at\nleast in our experience and when you have high\ndimensional problem,",
    "start": "246123",
    "end": "251432"
  },
  {
    "text": "it's not completely clear how\nmuch this kind of instability will occur and will not\nbe cured by just adding",
    "start": "251432",
    "end": "256890"
  },
  {
    "text": "some regularization term. And the computation\nunderlying this algorithm",
    "start": "256890",
    "end": "262710"
  },
  {
    "text": "are extremely, extremely simple. So that's why we're\nsticking to this, because it",
    "start": "262710",
    "end": "268229"
  },
  {
    "text": "works pretty well in practice. We actually developed\nin the last few years some toolbox that you can use.",
    "start": "268230",
    "end": "274169"
  },
  {
    "text": "They're pretty\nmuch plug and play. And because the algorithm\nis easy to understand in simpler terms.",
    "start": "274170",
    "end": "280419"
  },
  {
    "text": "Yesterday, Andrei was\ntalking about SVM. SVM is very similar\nin principle. Basically the only\ndifference is that you change",
    "start": "280420",
    "end": "286650"
  },
  {
    "text": "the way you measure cost here. This algorithm you can use\nboth for classification",
    "start": "286650",
    "end": "291690"
  },
  {
    "text": "and regression, whereas SVM-- the one which was\ntalked about yesterday-- is just for classification.",
    "start": "291690",
    "end": "297670"
  },
  {
    "text": "And because the cost function\nturns out to be non-smooth--",
    "start": "297670",
    "end": "302790"
  },
  {
    "text": "and non-smooth is basically\nnon-differentiable-- and so the whole math is\nmuch more complicated,",
    "start": "302790",
    "end": "308321"
  },
  {
    "text": "because you have to learn\nhow to minimize things that are not differentiable. So in this case, you can\nstick to elementary stuff.",
    "start": "308321",
    "end": "315720"
  },
  {
    "text": "And I think I did somewhere,\nthat also because Legendre 200 years ago said that least\nsquares are really great.",
    "start": "315720",
    "end": "322740"
  },
  {
    "text": "There is this old story-- who between Gauss and Legendre\ninvented least squares first.",
    "start": "322740",
    "end": "328570"
  },
  {
    "text": "And there are actually\nlong articles about this. But anyway, it's\naround that time. It's around the end of the--",
    "start": "328570",
    "end": "334436"
  },
  {
    "text": "this is when he was\nborn-- it's around the end of the 18th century. So the algorithm is pretty old.",
    "start": "334437",
    "end": "342430"
  },
  {
    "text": "So what's the idea? So back to the case we\nhad before, you're going to take a linear function.",
    "start": "342430",
    "end": "350110"
  },
  {
    "text": "So one thing is-- just to be careful--\nthink about it once. Because if you've never\nthought about it before,",
    "start": "350110",
    "end": "355500"
  },
  {
    "text": "it's good to focus. When you do this drawing,\nthis is not f of x.",
    "start": "355500",
    "end": "364560"
  },
  {
    "text": "This line is not f of x. It's f of x equals zero.",
    "start": "364560",
    "end": "370919"
  },
  {
    "text": "So I think I made enough\ntime to have a 3D plot. So f of x is actually a plane\nthat cuts through the slide.",
    "start": "370920",
    "end": "382380"
  },
  {
    "text": "It's positive, when\nit's not dotted-- because this points are\npositive-- and then becomes",
    "start": "382380",
    "end": "387930"
  },
  {
    "text": "negative. And this line is\nwhere it changes sign. So the decision boundary\nis not f of x itself,",
    "start": "387930",
    "end": "394615"
  },
  {
    "text": "but it's the level\nset that corresponds to f of x equals zero. Whereas f of x itself\nis this one line.",
    "start": "394615",
    "end": "401460"
  },
  {
    "text": "If you think in one\ndimension, the points are just standing on a line.",
    "start": "401460",
    "end": "407220"
  },
  {
    "text": "Some here are plus 1. Some here are minus 1. So what is f of x? It's just a line.",
    "start": "407220",
    "end": "414660"
  },
  {
    "text": "What is the decision\nboundary in this case? It will just be one point\nin this case actually,",
    "start": "414660",
    "end": "420150"
  },
  {
    "text": "because it's just\none line that cuts the input line in one point.",
    "start": "420150",
    "end": "426530"
  },
  {
    "text": "And that's it. If you were to take a more\ncomplicated nonlinear line, it would be more than one point.",
    "start": "426530",
    "end": "432240"
  },
  {
    "text": "In two dimension,\nit becomes one line. In three dimension, it becomes\na plane, and so on and so forth. But the important piece-- just\na remember, at least once--",
    "start": "432240",
    "end": "439310"
  },
  {
    "text": "then we look at this plot. This is not f of\nx, but only the set",
    "start": "439310",
    "end": "446449"
  },
  {
    "text": "f of x equals zero, which\nis where you change sign. And that's how you're\ngoing to make prediction.",
    "start": "446450",
    "end": "452030"
  },
  {
    "text": "You take real valued\nfunctions, so you would like-- in principle,\nin classification, you would allow this function\njust to be binary.",
    "start": "452030",
    "end": "459620"
  },
  {
    "text": "But optimization with binary\nfunctions is very hard. So what you typically\ndo to relax this? You just allow it to be\na real valued function,",
    "start": "459620",
    "end": "467789"
  },
  {
    "text": "and then you take a sign. When it's positive,\nyou take plus 1. If it's negative,\nyou say minus 1. If it's a regression\nproblem, you just",
    "start": "467789",
    "end": "473510"
  },
  {
    "text": "keep it for what it is. ",
    "start": "473510",
    "end": "481130"
  },
  {
    "text": "And how many free parameters\nhas this algorithm? Well, one. It's lambda for now and w.",
    "start": "481130",
    "end": "488040"
  },
  {
    "text": "But w we're going to solve\nby solving this optimization problem. How about lambda? Well, whatever we\ndiscussed before for k.",
    "start": "488040",
    "end": "495960"
  },
  {
    "text": "We would try to sit\ndown and do some bias variance of the composition,\nsee what it depends on,",
    "start": "495960",
    "end": "501410"
  },
  {
    "text": "try to see if we can\nget a grasp on what the theory of this algorithm is. And then we try to see if\nwe can use cross-validation.",
    "start": "501410",
    "end": "508162"
  },
  {
    "text": "You can do all these\nthings, so we're not going to discuss much how you\nchoose lambda, but most of you",
    "start": "508162",
    "end": "516059"
  },
  {
    "text": "are going to discuss how you can\ncompute the minimizer of this. And this is not a problem,\nbecause this is smooth.",
    "start": "516059",
    "end": "525439"
  },
  {
    "text": "So you can take the\nretrospect to w and also this. So what you can do is just to\ntake the derivative of this,",
    "start": "525440",
    "end": "532129"
  },
  {
    "text": "set it equal to zero,\nand check what happens. ",
    "start": "532130",
    "end": "539089"
  },
  {
    "text": "So it's useful to\ndo this to other-- just some vectorial notation.",
    "start": "539090",
    "end": "544820"
  },
  {
    "text": "We've already seen it before. So you take all the x's and you\nstack it as rows of the data",
    "start": "544820",
    "end": "550700"
  },
  {
    "text": "matrix x of n. So this ny, you just stack\nthem as entries of a vector.",
    "start": "550700",
    "end": "557060"
  },
  {
    "text": "You call it yn. Then you can rewrite\nthis term just in this way, as this vector\nminus this vector here, which",
    "start": "557060",
    "end": "564440"
  },
  {
    "text": "you obtain by multiplying\nthe matrix with w. So this norm is the norm in Rn. ",
    "start": "564440",
    "end": "571270"
  },
  {
    "text": "So this is just\nsimple rewriting. It's useful just\nbecause if you now",
    "start": "571270",
    "end": "578120"
  },
  {
    "text": "take the derivative\nof this with respect to w, set it equal to\nzero, you get this. This is the gradient.",
    "start": "578120",
    "end": "583690"
  },
  {
    "text": "So I haven't set it to zero yet. This is the gradient of\nthe least square part.",
    "start": "583690",
    "end": "589190"
  },
  {
    "text": "This is the gradient\nof the second term. It is still\nmultiplied by lambda.",
    "start": "589190",
    "end": "596150"
  },
  {
    "text": "If you set them equal to\nzero, what you get is this. You take everything with x,\nso the 2 and the 2 goes away.",
    "start": "596150",
    "end": "603339"
  },
  {
    "text": "You took everything with\nx, and you put it here. There's still the\none here with lambda. You put it here.",
    "start": "603340",
    "end": "610200"
  },
  {
    "text": "You take this term\nin x transpose y, and you put it on the\nother side of the equality. So you take everything with\nw on one side and everything",
    "start": "610200",
    "end": "617810"
  },
  {
    "text": "without w on the other side. And then here, I remove\nn by multiplying.",
    "start": "617810",
    "end": "623420"
  },
  {
    "text": " And so what you get\nis a linear system.",
    "start": "623420",
    "end": "628797"
  },
  {
    "text": "It's just a linear system. So that's the beauty\nof least squares. Whether you\nregularize it or not-- in this case for this simple\nsquared loss regularization,",
    "start": "628797",
    "end": "636620"
  },
  {
    "text": "all you get is a linear system. And this is the first way\nto think about the effect",
    "start": "636620",
    "end": "642680"
  },
  {
    "text": "of adding this term. So what is this doing?",
    "start": "642680",
    "end": "647900"
  },
  {
    "text": "So just quickly for you a\nquick linear system recap.",
    "start": "647900",
    "end": "653180"
  },
  {
    "text": "You're solving a linear system. I changed notation. This is just a parenthesis,\njust a little bit.",
    "start": "653180",
    "end": "659120"
  },
  {
    "text": "The simplest case\nyou can think of is the case where m is diagonal. Suppose it's just a diagonal\nmatrix, a square diagonal",
    "start": "659120",
    "end": "666140"
  },
  {
    "text": "matrix.  How do you solve this problem? You have to invert the matrix m.",
    "start": "666140",
    "end": "673422"
  },
  {
    "text": "What is the inverse\nof a diagonal matrix?  So it's just another\ndiagonal matrix.",
    "start": "673422",
    "end": "680470"
  },
  {
    "text": "On the entries, instead of, say,\nsigma, you have 1 over sigma or whatever it is. ",
    "start": "680470",
    "end": "687250"
  },
  {
    "text": "So what you see is that if\nm-- you just consider m-- and m is diagonal\nlike this-- this",
    "start": "687250",
    "end": "694030"
  },
  {
    "text": "is what you're going to get. Suppose that now\nsome of these numbers are actually small, then\nwhen you take 1 over,",
    "start": "694030",
    "end": "702070"
  },
  {
    "text": "this is going to blow up. When you apply this matrix\nto b, what you might have is",
    "start": "702070",
    "end": "707649"
  },
  {
    "text": "that if you change the\nsigmas or the b slightly, you can have an explosion.",
    "start": "707650",
    "end": "714670"
  },
  {
    "text": "And if you want, this is\none way to understand why adding the lambda would help.",
    "start": "714670",
    "end": "719980"
  },
  {
    "text": "And it's another way to look\nat overfitting, if you want, from a numerical point of view. You take the data. You change them slightly, and\nyou have numerical instability",
    "start": "719980",
    "end": "727480"
  },
  {
    "text": "right away. What is the effect\nof adding this term? ",
    "start": "727480",
    "end": "734020"
  },
  {
    "text": "Well, what you see\nis that instead of just doing m minus 1, you're\ndoing m plus lambda I minus 1.",
    "start": "734020",
    "end": "743710"
  },
  {
    "text": "And this is the simple\ncase, where it's diagonal. But what you see is that on\nthe diagonal instead of 1 over sigma 1, you take 1\nover sigma 1 plus lambda.",
    "start": "743710",
    "end": "752990"
  },
  {
    "text": "If sigma 1 is big, adding\nthis lambda won't matter. If sigma-- for example, sigma\nd, now think there are order.",
    "start": "752990",
    "end": "760509"
  },
  {
    "text": "I'm thinking they are\norder, and sigma d is small. If this is small, at some point\nlambda is going to jump in,",
    "start": "760510",
    "end": "767320"
  },
  {
    "text": "make the problem\nstable at the price of ignoring the\ninformation in that sigma, that you basically\nconsider it to be",
    "start": "767320",
    "end": "773410"
  },
  {
    "text": "at the same size of the\nnoise or the perturbation or the sample in your data. Does this make sense?",
    "start": "773410",
    "end": "779050"
  },
  {
    "text": "So this is what the\nalgorithm is doing. And it's a numerical way\nto look at stability.",
    "start": "779050",
    "end": "784881"
  },
  {
    "text": "But you can imagine that this\nis an immediate statistical consequence. You change the data\nslightly, you'll have a big change in your\nsolution and the other way",
    "start": "784881",
    "end": "791230"
  },
  {
    "text": "around. And lambda governs this\nby basically telling you how much this is invertible. So it's a connection between\nstatistical and numerical",
    "start": "791230",
    "end": "798310"
  },
  {
    "text": "stability. Now of course, you can say,\nthis is oversimplistic, because this is just\na diagonal matrix.",
    "start": "798310",
    "end": "806110"
  },
  {
    "text": "But basically, if\nyou now take matrices",
    "start": "806110",
    "end": "811300"
  },
  {
    "text": "that you can diagonalize,\nconceptually nothing would change. Because basically\nyou would have that",
    "start": "811300",
    "end": "817570"
  },
  {
    "text": "if you have a matrix-- so\nthere is a mistake here. There should be no minus 1. If you have an m that you can--",
    "start": "817570",
    "end": "823360"
  },
  {
    "text": "this is just sigma, not minus 1. You can just diagonalize it. And now every operation you\nwant to do on the matrix you",
    "start": "823360",
    "end": "829390"
  },
  {
    "text": "can just do on the diagonal. So all the reasoning\nhere will work the same.",
    "start": "829390",
    "end": "834845"
  },
  {
    "text": "Only now you have\nto remember that you have to squeeze the diagonal\nmatrix in between v and v transpose.",
    "start": "834845",
    "end": "840631"
  },
  {
    "text": "I'm not saying that this is\nwhat you want to do numerically. But I'm just saying that the\nconceptual reasoning here-- that we tell it that this\nwas the effect of lambda--",
    "start": "840631",
    "end": "847610"
  },
  {
    "text": "is going to hold\njust the same here. This is m, which you can\nwrite like this-- m minus 1",
    "start": "847610",
    "end": "853210"
  },
  {
    "text": "you can write like this. And so this is just going to\nbe the same diagonal terms",
    "start": "853210",
    "end": "858220"
  },
  {
    "text": "inverted. And now you see the\neffect of lambda. It's just the same. So once you grasp\nthis conceptually,",
    "start": "858220",
    "end": "864290"
  },
  {
    "text": "for any matrix you can make\ndiagonal, it's the same. And the point is\nthat as long as you",
    "start": "864290",
    "end": "869736"
  },
  {
    "text": "have a symmetric positive\ndefinite matrix, the reason you can diagonalize it, you just\nhave the same thing squeezed",
    "start": "869736",
    "end": "875370"
  },
  {
    "text": "in between v and v transpose. And that's what we have,\nbecause instead of-- ",
    "start": "875370",
    "end": "884680"
  },
  {
    "text": "because what we have is\nexactly this matrix here. So instead of-- and you see\nhere that basically this",
    "start": "884680",
    "end": "890269"
  },
  {
    "text": "depends a lot on the\ndimensionality of the data. If the number of points is much\nbigger than the dimensionality,",
    "start": "890270",
    "end": "897220"
  },
  {
    "text": "this matrix in\nprinciple could be-- it's easier that is invertible. But if the number\nof points is smaller",
    "start": "897220",
    "end": "903100"
  },
  {
    "text": "than the dimensionality-- how big is this matrix? So xn is-- you remember\nhow big was xn?",
    "start": "903100",
    "end": "910079"
  },
  {
    "text": "It was the rows were the\npoints, and the columns were the variable. So how big is this? And we call this d.",
    "start": "910080",
    "end": "915730"
  },
  {
    "text": "We called the length n. So this is-- AUDIENCE: [INAUDIBLE] LORENZO ROSASCO: --n by d.",
    "start": "915730",
    "end": "922470"
  },
  {
    "text": "So this matrix here is how big? Just d by d, and\nthe number of points",
    "start": "922470",
    "end": "928320"
  },
  {
    "text": "is smaller than the\nnumber dimension. The rank of this-- this is going to\nbe rank-deficient.",
    "start": "928320",
    "end": "934440"
  },
  {
    "text": "So it's not invertible. So if the number of points is\nmore, if you're in a high-- so called\nhigh-dimensional scenario,",
    "start": "934440",
    "end": "939870"
  },
  {
    "text": "where the number of\npoints is more than the number of\ndimension, for sure you won't be able to invert this. Ordinary least\nsquares will not work.",
    "start": "939870",
    "end": "946949"
  },
  {
    "text": "It will be unstable. And then you will\nhave to regularize to get anything reasonable.",
    "start": "946950",
    "end": "952279"
  },
  {
    "text": "So in the case of least squares,\njust by setting rank to zero and looking in this computation\nto get a grasp of both. What kind of computation\nyou have to do,",
    "start": "952279",
    "end": "958529"
  },
  {
    "text": "and what they mean both\nfrom the statistical and the numerical point of view. And that's why that's one of\nthe beauty of least squares.",
    "start": "958530",
    "end": "963750"
  },
  {
    "text": " We could stick to a whole\nderivation of this--",
    "start": "963750",
    "end": "971400"
  },
  {
    "text": "so this is more the\nlinear system perspective. There is a whole\nliterature trying to justify more from a\nstatistical point of view what",
    "start": "971400",
    "end": "978570"
  },
  {
    "text": "I'm saying. You can talk about the\nmaximum likelihood, then you can talk about\nmaximum a posteriori.",
    "start": "978570",
    "end": "984300"
  },
  {
    "text": "You can talk about variance\nreduction and so-called Stein effect. And you can make a much bigger\nstory trying, for example,",
    "start": "984300",
    "end": "991170"
  },
  {
    "text": "to develop the whole theory of\nshrinkage estimators, the bias variance tradeoff of this. But we're not going\nto talk about that.",
    "start": "991170",
    "end": "997380"
  },
  {
    "text": "So this simple\nnumerical stability, statistical stability\nintuition is going to be my main motivation\nfor considering these schemes.",
    "start": "997380",
    "end": "1004430"
  },
  {
    "text": " So let me skip these.",
    "start": "1004430",
    "end": "1009889"
  },
  {
    "text": "I wanted to show the demo, but-- it's very simple. It's going to be very\nstable, because you're just",
    "start": "1009890",
    "end": "1015470"
  },
  {
    "text": "drawing a one-dimensional line. Then you move on just\na bit, because we didn't cover as much as\nI want in the first part.",
    "start": "1015470",
    "end": "1024290"
  },
  {
    "text": "So first of all, so far so good? Are you all with me about this?",
    "start": "1024290",
    "end": "1032230"
  },
  {
    "text": "So again, the basic\nthing if you want-- all the interesting--\nso this is the one line,",
    "start": "1032230",
    "end": "1040359"
  },
  {
    "text": "where there is something\nconceptual happening. This is the one line, where we\nmake it a bit more complicated",
    "start": "1040359",
    "end": "1046530"
  },
  {
    "text": "mathematically. And then all you have\nto do is to match this with what we just wrote before.",
    "start": "1046530",
    "end": "1051760"
  },
  {
    "text": "That's all. These are the main three\nthings we want to do. And think a bit\nabout dimensionality.",
    "start": "1051760",
    "end": "1057900"
  },
  {
    "text": "Now if you look at a problem\neven like this, as I said,",
    "start": "1057900",
    "end": "1064029"
  },
  {
    "text": "this might be misleading--\na low dimension. And in fact, what we\ntypically do in high dimension is that, first of all, you\nstart with the linear model",
    "start": "1064029",
    "end": "1069540"
  },
  {
    "text": "and you see how far\nyou can go with that. And typically, you go a bit\nfurther that you might imagine.",
    "start": "1069540",
    "end": "1075969"
  },
  {
    "text": "But still, you can\nthink, why should I just stick to linear decision rule? This won't give me\nmuch of a flexibility.",
    "start": "1075969",
    "end": "1083080"
  },
  {
    "text": "So in this case,\nobviously, it looks like something that\nwould be better, some kind of quadric\ndecision boundary.",
    "start": "1083080",
    "end": "1089850"
  },
  {
    "text": "So how can you do this? How can you go--\nsuppose that I give you the code of least squares.",
    "start": "1089850",
    "end": "1096126"
  },
  {
    "text": "And you're the\nlaziest programmer in the world, which in\nmy case is actually not that hard to imagine.",
    "start": "1096126",
    "end": "1102360"
  },
  {
    "text": "How can you recycle\nthe code to fit, to create a solution\nlike this, instead",
    "start": "1102360",
    "end": "1108420"
  },
  {
    "text": "of a solution like this? You see the question? I give you the code\nto solve this problem,",
    "start": "1108420",
    "end": "1114035"
  },
  {
    "text": "the one I showed you before-- the linear system for\ndifferent lambdas. But you want to go from this\nsolution to the solution.",
    "start": "1114035",
    "end": "1120270"
  },
  {
    "text": "How could you do that? So one way you can do\nit in this simple case is-- this is the example.",
    "start": "1120270",
    "end": "1126155"
  },
  {
    "text": "So the idea is-- you remember the matrix? I'm going to invent new\nentries of the matrix,",
    "start": "1126155",
    "end": "1131334"
  },
  {
    "text": "not of the points, because\nyou cannot invent points, but of the variables. So what you're going to do,\ninstead of just-- they can say,",
    "start": "1131334",
    "end": "1137198"
  },
  {
    "text": "in this case I call them x1, x2. I'm just in two dimension. These are my data.",
    "start": "1137198",
    "end": "1142250"
  },
  {
    "text": "This is just another\nexample of this. So these are my data--\nsorry these are-- let's see what they are.",
    "start": "1142250",
    "end": "1147779"
  },
  {
    "text": "This is one point. X1 and x2 here\nare just the entry of the point x, so\nthe first coordinate",
    "start": "1147780",
    "end": "1155970"
  },
  {
    "text": "and the second coordinate. So what you said is\nexactly one way to do this.",
    "start": "1155970",
    "end": "1161070"
  },
  {
    "text": "And it is-- I'm going to now build a\nnew vector representation of the same points. So it's going to\nbe the same point,",
    "start": "1161070",
    "end": "1166470"
  },
  {
    "text": "but instead of two\ncoordinates I now use three, which are going to be\nthe first coordinate square,",
    "start": "1166470",
    "end": "1172410"
  },
  {
    "text": "the second coordinate square,\nand the product of the two coordinates. ",
    "start": "1172410",
    "end": "1179559"
  },
  {
    "text": "Once I've done this, I\nforget about how I got this, and I just treat it\nas new variables.",
    "start": "1179560",
    "end": "1184800"
  },
  {
    "text": "And I take a linear model\nwith that variables. It's a linear model with\nthese new variables,",
    "start": "1184800",
    "end": "1191340"
  },
  {
    "text": "but it's a new linear model\nwith the original variables. And that's what you see here. So x tilde is this stuff.",
    "start": "1191340",
    "end": "1200120"
  },
  {
    "text": "It's just a new\nvector representation. And now I'm linear with\nrespect to this new vector",
    "start": "1200120",
    "end": "1205500"
  },
  {
    "text": "representation. But when you write\nx tilde explicitly, it's some kind of\nnon-linear function",
    "start": "1205500",
    "end": "1211410"
  },
  {
    "text": "of the original variable. So this function\nhere is non-linear in the original variable.",
    "start": "1211410",
    "end": "1216870"
  },
  {
    "text": "It's harder to say\nthan probably to see. Does it make sense?",
    "start": "1216870",
    "end": "1222310"
  },
  {
    "text": "So if you do this,\nyou're completely recycling the beauty\nof the linearity from a computational\npoint of view while",
    "start": "1222310",
    "end": "1229530"
  },
  {
    "text": "augmenting the\npower of your model from linear to non-linear. It's still parametric in the\nsense that in this case--",
    "start": "1229530",
    "end": "1237351"
  },
  {
    "text": "what I mean by parametric\nis that we still fix a priori the\nnumber of degrees of freedom of our problem.",
    "start": "1237351",
    "end": "1242850"
  },
  {
    "text": "It was true now I make it three. More general I could\nmake it p, but the number of numbers I have to\nfind is fixed a priori.",
    "start": "1242850",
    "end": "1249280"
  },
  {
    "text": "It doesn't depend on my\ndata, and it's fixed. But I can definitely go\nfrom linear to non-linear.",
    "start": "1249280",
    "end": "1258150"
  },
  {
    "text": "So let's keep on going. So from the simple linear model\nwe already went quite far, because we basically know\nthat with the same computation",
    "start": "1258150",
    "end": "1264780"
  },
  {
    "text": "we can now solve\nstuff like this. Let's take a couple\nof steps further. So one is-- appreciate\nthat really the code",
    "start": "1264780",
    "end": "1273330"
  },
  {
    "text": "is just the same. Instead of x, I have\nto do a pre-processing to replace x with this\nnew matrix x tilde, which",
    "start": "1273330",
    "end": "1279630"
  },
  {
    "text": "is the one which instead of\nbeing n by d, is now n by p where p is this new number\nof variables that I invented.",
    "start": "1279630",
    "end": "1284830"
  },
  {
    "text": " Now it's useful to just\nget the feeling of what is",
    "start": "1284830",
    "end": "1291870"
  },
  {
    "text": "the complexity of this method. And this is a very\nquick complexity recap.",
    "start": "1291870",
    "end": "1298399"
  },
  {
    "text": "Here basically, the\nproduct of two numbers is going to count one. And then when you take product\nof vectors of matrices,",
    "start": "1298400",
    "end": "1304340"
  },
  {
    "text": "you just count on any real\nnumber multiplication you do. And this is a quick recap. If I multiply two vectors\nof size p, the cost p,",
    "start": "1304340",
    "end": "1312630"
  },
  {
    "text": "matrix vector is going to be np. Matrix matrix is going\nto be n square p.",
    "start": "1312630",
    "end": "1318600"
  },
  {
    "text": "You have n vectors. And one-to-one, other n vectors. And they are size p, so each\ntime you have-- it costs you p.",
    "start": "1318600",
    "end": "1325880"
  },
  {
    "text": "And you have to do n against n. So it's going to be n square p. And the last one\nis-- this is a much--",
    "start": "1325880",
    "end": "1333674"
  },
  {
    "text": "less clear to just\nlook at it like this. But roughly speaking,\nthe inversion of a matrix costs roughly speaking n\ncube in the worst case.",
    "start": "1333674",
    "end": "1341460"
  },
  {
    "text": "It's just to give you a feeling\nof what the complexity are. So it makes sense?",
    "start": "1341460",
    "end": "1347740"
  },
  {
    "text": "It's a bit quick,\nbut it's simple. If you know it, OK. Otherwise, you just\ntake this on the side, when you think about this.",
    "start": "1347740",
    "end": "1353520"
  },
  {
    "text": "So what is the\ncomplexity of this? Well, the matrix-- you have\nto multiply this times this,",
    "start": "1353520",
    "end": "1361390"
  },
  {
    "text": "and this is going to\ncost you nd or np. You have to build this matrix.",
    "start": "1361390",
    "end": "1366820"
  },
  {
    "text": "This is going to cost you\nn square d or n square p. And then you have to invert.",
    "start": "1366820",
    "end": "1372350"
  },
  {
    "text": "These are going to be n cube. So-- sorry, p cubed,\nbecause with this matrix is",
    "start": "1372350",
    "end": "1378990"
  },
  {
    "text": "going to be-- or d cube,\nbecause this matrix is d by d. So this is, roughly\nspeaking, the cost.",
    "start": "1378990",
    "end": "1386559"
  },
  {
    "text": "So now look at this. This is-- I take this. In this case, p is the\nnew variable, otherwise d.",
    "start": "1386560",
    "end": "1393750"
  },
  {
    "text": "So in this case, I have p cube,\nand then I have p square n.",
    "start": "1393750",
    "end": "1401050"
  },
  {
    "text": "But one question is\nwhat if n is much-- and that's a fact-- what if\nn is much smaller than p?",
    "start": "1401050",
    "end": "1408390"
  },
  {
    "text": "If n is a 10, do I\nreally have to pay quadratic or even cubic\nin the number of dimension",
    "start": "1408390",
    "end": "1416430"
  },
  {
    "text": "to solve this problem? Because in some sense, it looks\nI'm overshooting things a bit. Because I'm inverting a\nmatrix, yes, but this matrix",
    "start": "1416430",
    "end": "1422820"
  },
  {
    "text": "is really a rank n. It only has n rows that are\nlinearly independent at most.",
    "start": "1422820",
    "end": "1428010"
  },
  {
    "text": "It might be less,\nbut at most it has n. So can I break the\ncomplexity of this?",
    "start": "1428010",
    "end": "1433920"
  },
  {
    "text": "Linear system have\nto solve, you just use the table I\nshowed you before. Check the computation. These are the computation\nyou have to do.",
    "start": "1433920",
    "end": "1440190"
  },
  {
    "text": "And one observation\nhere is you pay really a lot in the dimension,\nthe number of variables",
    "start": "1440190",
    "end": "1446370"
  },
  {
    "text": "or the number of\nfeatures you invented. And this might be OK,\nwhen p is smaller than n.",
    "start": "1446370",
    "end": "1452370"
  },
  {
    "text": "But one thing-- this\nseems wrong intuitively, when n is much smaller than p. Because the complexity\nof the problem,",
    "start": "1452370",
    "end": "1458850"
  },
  {
    "text": "the rank of the\nproblem is just n. The matrix here has n\nrows and d or p columns",
    "start": "1458850",
    "end": "1465510"
  },
  {
    "text": "depending on which\nrepresentation you take. And so the rank of the\nwhole thing is at most n, if n is much smaller.",
    "start": "1465510",
    "end": "1476159"
  },
  {
    "text": "So now the red dot appears. And what you can do is\nproving this one line.",
    "start": "1476160",
    "end": "1484484"
  },
  {
    "text": "So let's see what\nthey do, and then I'll tell you how you can prove it. And it's an exercise. So you see here if\nyou invert this,",
    "start": "1484484",
    "end": "1492460"
  },
  {
    "text": "then you have to\nmultiply x transpose y times the inverse\nof this matrix, which",
    "start": "1492460",
    "end": "1497940"
  },
  {
    "text": "is what's written in here. So I claim that this\nequality stands.",
    "start": "1497940",
    "end": "1503010"
  },
  {
    "text": "Look what it does. I take this x transpose. I move it in front.",
    "start": "1503010",
    "end": "1509050"
  },
  {
    "text": "But then if I do\nthis, you clearly see that I'm messing\naround with dimensions. So what you do is that you have\nto switch the order of the two",
    "start": "1509050",
    "end": "1516122"
  },
  {
    "text": "matrices in the middle.  Now from a dimensionality\npoint of view, at least,",
    "start": "1516122",
    "end": "1522360"
  },
  {
    "text": "I still see that this\nmatrix and this matrix have the same dimension.",
    "start": "1522360",
    "end": "1527520"
  },
  {
    "text": "How do you prove this? Well, you basically\njust need to do SVD. You take the singular-value\ndecomposition of the matrix Xn.",
    "start": "1527520",
    "end": "1534059"
  },
  {
    "text": "You plug it in, and you\njust compute things. And you check that this\nside of the equality is the same of this\nside of the equality.",
    "start": "1534060",
    "end": "1541590"
  },
  {
    "text": "So there's nothing\nmore than this, but we're going to skip this. So you just take this as a fact.",
    "start": "1541590",
    "end": "1547320"
  },
  {
    "text": "It's a little trick. Why do I want to do this trick? Because look, now what I\nsay is that my w is going",
    "start": "1547320",
    "end": "1552870"
  },
  {
    "text": "to be x transpose of something. ",
    "start": "1552870",
    "end": "1559330"
  },
  {
    "text": "What is this something? So w is going to be X\ntranspose of this thing here.",
    "start": "1559330",
    "end": "1566130"
  },
  {
    "text": "How big is this vector? So how big is this\nmatrix first of all? So remember, Xn was how big?",
    "start": "1566130",
    "end": "1573495"
  },
  {
    "text": "AUDIENCE: N by d. LORENZO ROSASCO: N by d or p. How big is this? AUDIENCE: N by n.",
    "start": "1573495",
    "end": "1579059"
  },
  {
    "text": "LORENZO ROSASCO: N by n. So how big is this vector? It's n by 1.",
    "start": "1579060",
    "end": "1585130"
  },
  {
    "text": "So now I have to-- I found out that\nmy w can always be written as x transpose\nc, where c is just",
    "start": "1585130",
    "end": "1594320"
  },
  {
    "text": "an n-dimensional vector. I rewrote it like\nthis, if you want.",
    "start": "1594320",
    "end": "1600350"
  },
  {
    "text": "So what is the\ncost of doing this? ",
    "start": "1600350",
    "end": "1607800"
  },
  {
    "text": "Well, this was the\ncost of doing this? But now you just have to do--",
    "start": "1607800",
    "end": "1614260"
  },
  {
    "text": "so let's say what is the\ncost of doing this thing here above the bracket? Well, if this one was\np cube p square n,",
    "start": "1614260",
    "end": "1621710"
  },
  {
    "text": "this one will be how much? I have that this\nmatrix will say p by p,",
    "start": "1621710",
    "end": "1627720"
  },
  {
    "text": "and then this vector was p by 1. Whereas here, my matrix is n by\nn, and the victory is n by 1.",
    "start": "1627720",
    "end": "1634679"
  },
  {
    "text": " So you basically have that\nthese two numbers swap.",
    "start": "1634680",
    "end": "1640699"
  },
  {
    "text": "Instead of having\nthis complexity, now you have a complexity,\nwhich is n cube.",
    "start": "1640699",
    "end": "1645780"
  },
  {
    "text": "And then you have n square\np, which sounds about right.",
    "start": "1645780",
    "end": "1650910"
  },
  {
    "text": "It's linear in p. You cannot avoid that. You have to look at\nthe data at least once. But then it's polynomial only in\nthe small quantity of the two.",
    "start": "1650910",
    "end": "1660034"
  },
  {
    "text": "So in some sense,\nwhat you see is that, depending on the\nsize of n, of course, you still have to do\nthis multiplication.",
    "start": "1660035",
    "end": "1666110"
  },
  {
    "text": "But this multiplication\nis just n, nd, or np. So let's just recap\nwhat I'm telling you.",
    "start": "1666110",
    "end": "1672169"
  },
  {
    "text": "This is a lot more\nmathematical fact I put. I have a warning here. The first thing is the\nquestion should be clear.",
    "start": "1672170",
    "end": "1679860"
  },
  {
    "text": "Can I break the complexity\nof this in the case when n is smaller than p or d?",
    "start": "1679860",
    "end": "1685549"
  },
  {
    "text": "This is relevant because the\nquestion came out a second ago, which was should\nI always explode",
    "start": "1685550",
    "end": "1690870"
  },
  {
    "text": "the dimension of my features? And here what you see is that-- well, at least for now we\nsee that even if you do,",
    "start": "1690870",
    "end": "1696960"
  },
  {
    "text": "you don't pay more\nthan linearly in that. And the way you\nprove it is A, you",
    "start": "1696960",
    "end": "1702690"
  },
  {
    "text": "observe this factor,\nwhich, again, I measured if you're curious,\nto show how you do it, but it's a one line.",
    "start": "1702690",
    "end": "1708720"
  },
  {
    "text": "And 2, you observe that\nonce you have this, if you just rewrite w, you can\nwrite w as a x transpose c.",
    "start": "1708720",
    "end": "1714840"
  },
  {
    "text": "And to find a c-- which is now\nyou basically re-parametrize-- and to find the new c\nis going to cost you",
    "start": "1714840",
    "end": "1719970"
  },
  {
    "text": "only n cube n square p. So you do exactly\nwhat you wanted to do.",
    "start": "1719970",
    "end": "1726294"
  },
  {
    "text": "And basically,\nwhat you see now is that whenever you\ndo least squares, you can check the\nnumber of dimensions, the number of points, and always\nre-parametrize the problem",
    "start": "1726294",
    "end": "1734610"
  },
  {
    "text": "in such a way that complexity\nis depending linearly on the bigger of the\ntwo and polynomially",
    "start": "1734610",
    "end": "1740940"
  },
  {
    "text": "on the smaller of the two. So that's good news. ",
    "start": "1740940",
    "end": "1751782"
  },
  {
    "text": "Oh, I wrote it. ",
    "start": "1751782",
    "end": "1757264"
  },
  {
    "text": "So this is where\nwe are right now.  So if we're lost now, you're\ngoing to become completely lost",
    "start": "1757264",
    "end": "1763870"
  },
  {
    "text": "in one second. Because this is\nwhat we want to do. We want to introduce kernel\nin the simplest possible way,",
    "start": "1763870",
    "end": "1769910"
  },
  {
    "text": "which is the following.  So look at-- this\nis what we find out.",
    "start": "1769910",
    "end": "1777150"
  },
  {
    "text": "We discovered, we\nactually proved a theorem. And the theorem\nsays that the w's",
    "start": "1777150",
    "end": "1784350"
  },
  {
    "text": "that are output by the least\nsquares algorithm are not any possible\nd-dimensional vectors,",
    "start": "1784350",
    "end": "1790740"
  },
  {
    "text": "but they're always\nvectors that I can write as the combination\nof the training set vectors.",
    "start": "1790740",
    "end": "1797100"
  },
  {
    "text": "So xi is long d or p,\nand I've summed them up with these weights.",
    "start": "1797100",
    "end": "1802429"
  },
  {
    "text": "And the w's that are going\nto come out of least squares are always of that form.",
    "start": "1802429",
    "end": "1807500"
  },
  {
    "text": "They cannot be of\nany other form. This is called the\nrepresenter theorem.",
    "start": "1807500",
    "end": "1813140"
  },
  {
    "text": "It's the basic theorem of\nso-called kernel methods. It shows you that the\nsolution you're looking for",
    "start": "1813140",
    "end": "1819960"
  },
  {
    "text": "can be written as a linear\nsuperposition of these terms. ",
    "start": "1819960",
    "end": "1827309"
  },
  {
    "text": "If you now write-- this is just the w. Let's just write down f of x. F of x is going to be\nx transpose w, just",
    "start": "1827310",
    "end": "1833670"
  },
  {
    "text": "the linear function. And now you can-- if you write\nit down, you just get this.",
    "start": "1833670",
    "end": "1839400"
  },
  {
    "text": "By linearity you can-- so w is written like this. You multiply by x transpose. This is a finite sum.",
    "start": "1839400",
    "end": "1845190"
  },
  {
    "text": "So you can let x\ntranspose inside the sum. This is what you get. Are you OK?",
    "start": "1845190",
    "end": "1850320"
  },
  {
    "text": "So you have x\ntranspose times a sum. This is the sum of x transpose\nmultiplied by the rest.",
    "start": "1850320",
    "end": "1857800"
  },
  {
    "text": "Why do we care about this? Because basically the\nidea of kernel methods-- in this very basic\nform-- is what",
    "start": "1857800",
    "end": "1865570"
  },
  {
    "text": "if I replace this\ninner product, which is a way to measure similarity\nbetween my functions,",
    "start": "1865570",
    "end": "1871720"
  },
  {
    "text": "with another similarity. So instead of mapping\neach x into a very high",
    "start": "1871720",
    "end": "1877417"
  },
  {
    "text": "dimensional vector and\nthen taking product-- which is itself, if you\nwant another way, as I said,",
    "start": "1877417",
    "end": "1882430"
  },
  {
    "text": "of measuring similarity\nin your product, distances between vectors--\nwhat if I just define it,",
    "start": "1882430",
    "end": "1887650"
  },
  {
    "text": "instead of by an\nexplicit mapping, by redefining the inner product.",
    "start": "1887650",
    "end": "1892810"
  },
  {
    "text": "So this k here is the\nk similar to the one we had in the previous--\nin the very first slide.",
    "start": "1892810",
    "end": "1899730"
  },
  {
    "text": "And it's-- re-parametrize\nthe inner product. Change the inner\nproduct, and then I",
    "start": "1899730",
    "end": "1905140"
  },
  {
    "text": "want to use everything else. So we need to question-- we\nneed to answer two question.",
    "start": "1905140",
    "end": "1910520"
  },
  {
    "text": "The first one is if I\ngive you now a procedure that whenever you would\nwant to do x transpose",
    "start": "1910520",
    "end": "1916330"
  },
  {
    "text": "x does something else\ncalled ax comma x prime. How do you change\nthe computations?",
    "start": "1916330",
    "end": "1923410"
  },
  {
    "text": "This is going to be very easy. But also what are you doing\nfrom a modeling perspective? ",
    "start": "1923410",
    "end": "1934000"
  },
  {
    "text": "So from the computational\npoint of view, it's very easy, because you see\nhere you always had",
    "start": "1934000",
    "end": "1941519"
  },
  {
    "text": "that you have to build a\nmatrix whose entries were xi transpose xj. ",
    "start": "1941520",
    "end": "1949919"
  },
  {
    "text": "So it was always a\nproduct of two vectors. And what you do now is\nthat you do the same.",
    "start": "1949920",
    "end": "1956070"
  },
  {
    "text": "So you build the matrix\nkn, which is not just xn, xn transpose but is a new matrix\nwhose entries are just this.",
    "start": "1956070",
    "end": "1963705"
  },
  {
    "text": "This is just a generalization. If I put the linear\nkernel, I just get back in what we had before.",
    "start": "1963705",
    "end": "1969760"
  },
  {
    "text": "If you put another kernel,\nyou just get something else. So from a computational\npoint of view, you're done for this\ncomputation of c.",
    "start": "1969760",
    "end": "1978164"
  },
  {
    "text": "You have to do nothing else. You just replace this matrix\nwith these general matrix. And if you want\nto now compute s--",
    "start": "1978164",
    "end": "1985350"
  },
  {
    "text": "so w you cannot compute\nanymore, because you don't know what's an x by itself. But if you want to\ncompute f of x, you can,",
    "start": "1985350",
    "end": "1993090"
  },
  {
    "text": "because you've just to plug-in-- So you know how\nto compute the c. And you know how to compute this\nquantity, because you have just",
    "start": "1993090",
    "end": "2000190"
  },
  {
    "text": "to put the kernel there. So the magic here\nis that you never ever point x in isolation. You always have a point x\nmultiplied by another point x.",
    "start": "2000190",
    "end": "2007880"
  },
  {
    "text": "And this allows you to\nreplace vectors by-- in some sense, this is\nan implicit remapping",
    "start": "2007880",
    "end": "2014000"
  },
  {
    "text": "of the points by just\nredefining the inner product. So what you should\nsee for now is",
    "start": "2014000",
    "end": "2019089"
  },
  {
    "text": "just that the\ncomputation that you've done to compute f of x in\nthe linear case you can redo,",
    "start": "2019089",
    "end": "2025100"
  },
  {
    "text": "if you replace the inner\nproduct with this new function. Because A, you can compute c\nby just using this new matrix",
    "start": "2025100",
    "end": "2032990"
  },
  {
    "text": "in place of this. And B, you can replace f\nof x, because all you need is to replace this inner\nproduct with this one",
    "start": "2032990",
    "end": "2039635"
  },
  {
    "text": "and put the right weights,\nwhich you know how to compute. From a modeling\nperspective what you",
    "start": "2039635",
    "end": "2045919"
  },
  {
    "text": "can check is that, for\nexample, if you choose here this polynomial kernel--",
    "start": "2045920",
    "end": "2051259"
  },
  {
    "text": "which is just x\ntranspose x prime plus 1 elevated to the d-- if you take, for\nexample, d equal 2,",
    "start": "2051260",
    "end": "2058399"
  },
  {
    "text": "this is equivalent to the\nmapping I showed you before, the one with explicit\nmonomials as entries.",
    "start": "2058400",
    "end": "2065989"
  },
  {
    "text": "This is just doing\nit implicitly. If you're in low-dimensional,\nif you're low-dimensional,",
    "start": "2065989",
    "end": "2071435"
  },
  {
    "text": "if n is very big, and the\ndimensions are very small, the first way might be better.",
    "start": "2071435",
    "end": "2077179"
  },
  {
    "text": "But if n is much bigger,\nthis way would be better.",
    "start": "2077179",
    "end": "2082399"
  },
  {
    "text": "But also you can use stuff like\nthis, like a Gaussian kernel. And in that case, you cannot\nreally write down explicitly",
    "start": "2082400",
    "end": "2087860"
  },
  {
    "text": "the explicit map,\nbecause it turns out that it's infinite-dimensional. The vectors you would\nneed to write down,",
    "start": "2087860",
    "end": "2093559"
  },
  {
    "text": "to write down the explicit\nvariable version of-- embedding version of this\nis infinite-dimensional.",
    "start": "2093560",
    "end": "2100840"
  },
  {
    "text": "So this is a-- if you use this, you get the\ntruly non-parametric model. ",
    "start": "2100840",
    "end": "2107480"
  },
  {
    "text": "If you think of what is\nthe effect of using this, it's quite clear if\nyou plug them here. Because what you have\nis that in one case",
    "start": "2107480",
    "end": "2113000"
  },
  {
    "text": "you have a superposition\nof linear stuff, a superposition of\npolynomial stuff, or a superposition of Gaussians.",
    "start": "2113000",
    "end": "2120099"
  },
  {
    "text": " So same game as before. ",
    "start": "2120100",
    "end": "2128490"
  },
  {
    "text": "So same dataset we train. I take kernel least squares-- which is what I\njust showed you--",
    "start": "2128490",
    "end": "2134650"
  },
  {
    "text": "compute the c\ninverting that matrix, use the Gaussian kernel--\nthe last of the example--",
    "start": "2134650",
    "end": "2140089"
  },
  {
    "text": "and then compute f of x. And then we just\nwant to plot it. ",
    "start": "2140090",
    "end": "2147231"
  },
  {
    "text": "So this is the solution.  The algorithm depends\non two parameters.",
    "start": "2147231",
    "end": "2153510"
  },
  {
    "text": "What are they?  AUDIENCE: Lambda.",
    "start": "2153510",
    "end": "2158619"
  },
  {
    "text": "LORENZO ROSASCO: Lambda, the\nregularization parameter, the one that appeared\nalready in the linear case--",
    "start": "2158620",
    "end": "2163700"
  },
  {
    "text": "and then-- AUDIENCE: Whatever parameter\nyou've chosen [INAUDIBLE].. LORENZO ROSASCO: Exactly. Whatever parameters\nthere is in your kernel.",
    "start": "2163700",
    "end": "2169220"
  },
  {
    "text": "In this case, it's\nthe Gaussian, so it will depend on this width. ",
    "start": "2169220",
    "end": "2177370"
  },
  {
    "text": "Now suppose that\nI take gamma big. I don't know what big is.",
    "start": "2177370",
    "end": "2184120"
  },
  {
    "text": "I just do it by hand here,\nso we see what happens. ",
    "start": "2184120",
    "end": "2192620"
  },
  {
    "text": "If you take gamma--\nsorry gamma, sigma big, you start to get\nsomething very simple.",
    "start": "2192620",
    "end": "2199450"
  },
  {
    "text": "And if I make it\na bit bigger, it will probably start to look very\nmuch like a linear solution.",
    "start": "2199450",
    "end": "2207550"
  },
  {
    "start": "2207550",
    "end": "2214530"
  },
  {
    "text": "If I make it small-- ",
    "start": "2214530",
    "end": "2219805"
  },
  {
    "text": "and again, I don't\nknow what small is, so I'm just going to try. ",
    "start": "2219805",
    "end": "2227725"
  },
  {
    "text": "I's very small. ",
    "start": "2227725",
    "end": "2235660"
  },
  {
    "text": "You start to see\nwhat's going on. And if you go in\nbetween, you really start to see that you can\ncircle out individual examples.",
    "start": "2235660",
    "end": "2243224"
  },
  {
    "text": "So let's think a second\nwhat we're doing here. ",
    "start": "2243224",
    "end": "2249369"
  },
  {
    "text": "It is going to be again other\nhand-waving explanation. Look at this equation.",
    "start": "2249370",
    "end": "2257190"
  },
  {
    "text": "Let's read out what it says. In the case of Gaussians,\nit says, I take a Gaussian--",
    "start": "2257190",
    "end": "2262710"
  },
  {
    "text": "just a usual Gaussian-- I center it over a\ntraining set point,",
    "start": "2262710",
    "end": "2267840"
  },
  {
    "text": "then by choosing the ci\nI'm choosing whether it is going to be a peak or a valley.",
    "start": "2267840",
    "end": "2274309"
  },
  {
    "text": "It can go up, or it can go down\nin the two-dimensional case. And by choosing\nthe width, I decide",
    "start": "2274310",
    "end": "2280520"
  },
  {
    "text": "how large it's going to be. If I do f of x, then I sum up\nall this stuff, which basically",
    "start": "2280520",
    "end": "2288059"
  },
  {
    "text": "means that I'm going to have\nthese peaks and these valleys and I connect them in some way.",
    "start": "2288060",
    "end": "2297000"
  },
  {
    "text": "Now you remember before\nthat I pointed out within the\ntwo-dimensional case what we draw is not f of x,\nbut f of x equal to zero.",
    "start": "2297000",
    "end": "2304920"
  },
  {
    "text": "So what you should really think\nis that f of x in this case",
    "start": "2304920",
    "end": "2311525"
  },
  {
    "text": "is no longer an upper plane,\nbut it's this surface. It goes up, and it goes down.",
    "start": "2311525",
    "end": "2317545"
  },
  {
    "text": "And it goes up,\nand it goes down. So in the blue part, it goes\nup, and in the orange part, it goes down into valley.",
    "start": "2317545",
    "end": "2325260"
  },
  {
    "text": "So what you do is\nthat right now you're taking all these\nsmall Gaussians,",
    "start": "2325260",
    "end": "2330270"
  },
  {
    "text": "and you put them in around\nblue and orange point, and then you\nconnect their peaks.",
    "start": "2330270",
    "end": "2336702"
  },
  {
    "text": "And by making them\nsmall, you allow them to create a very\ncomplicated surface. ",
    "start": "2336702",
    "end": "2343310"
  },
  {
    "text": "So what did we put before? ",
    "start": "2343310",
    "end": "2351240"
  },
  {
    "text": "So they're small. They're getting smaller,\nand smaller, and smaller. And they go out,\nand you see the--",
    "start": "2351240",
    "end": "2356640"
  },
  {
    "text": "there is a point here,\nso they circle it out here by putting basically\nGaussian right there for that individual point.",
    "start": "2356640",
    "end": "2364920"
  },
  {
    "text": "Imagine what happens\nif my points-- I have two points here\nand two points here-- and now I put a huge\nGaussian around each point.",
    "start": "2364920",
    "end": "2373020"
  },
  {
    "text": "Basically, the peaks are almost\ngoing to touch each other. So what you're\nimagine is that you get something, where basically\nthe decision boundary has",
    "start": "2373020",
    "end": "2380670"
  },
  {
    "text": "to look like a line,\nbecause you get something which is so smooth. It doesn't go up and\ndown all the time. It's going to be--",
    "start": "2380670",
    "end": "2387539"
  },
  {
    "text": "And that's what we\nsaw before, right? And again, I don't\nremember what I put here. ",
    "start": "2387539",
    "end": "2394718"
  },
  {
    "text": "So this is starting\nto look good. So you really see that somewhat\nsomething nice happens.",
    "start": "2394718",
    "end": "2400720"
  },
  {
    "text": "Maybe if I put-- five is\nwhat we put before maybe. ",
    "start": "2400720",
    "end": "2409010"
  },
  {
    "text": "So basically what\nyou're basically doing is that you're computing\nthe center of mass of one class in the\nsense of the Gaussians.",
    "start": "2409010",
    "end": "2416794"
  },
  {
    "text": "So you're doing a\nGaussian mixture on one side, a Gaussian\nmixture on the other side, you're basically computing\nthe center of masses, and then you just\nfind the line that",
    "start": "2416794",
    "end": "2422890"
  },
  {
    "text": "separates the center of masses. That's what you're\ndoing here, and you just find this one big line here. ",
    "start": "2422890",
    "end": "2430600"
  },
  {
    "text": "So again, so we're\nnot playing around with the number of points.",
    "start": "2430600",
    "end": "2438040"
  },
  {
    "text": "We're not play\naround with lambda. But because this is basically\nwhat we already saw before. All I want to show you right\nnow is the effect of the kernel.",
    "start": "2438040",
    "end": "2444691"
  },
  {
    "text": "And here I'm using the Gaussian\nkernel, but-- let's see--",
    "start": "2444691",
    "end": "2451609"
  },
  {
    "text": "but you can also use\nthe linear kernel. This is the linear kernel. This is using the\nlinear least squares.",
    "start": "2451610",
    "end": "2456885"
  },
  {
    "text": "If you now use the\nGaussian kernel, you give yourself the\nextra possibility. Essentially, what you\nsee is that if you put the Gaussian which is\nvery big, in some sense",
    "start": "2456885",
    "end": "2463691"
  },
  {
    "text": "you get back the linear kernel. But if you put the Gaussian\nwhich is very small, you allow yourself to\nthis extra complexity.",
    "start": "2463691",
    "end": "2472050"
  },
  {
    "text": "And so that's what we gain\nwith this little trick that we did of replacing\nthe inner product",
    "start": "2472050",
    "end": "2480500"
  },
  {
    "text": "with this new kernel. We went from the simple\nlinear estimators",
    "start": "2480500",
    "end": "2486380"
  },
  {
    "text": "to something, which is-- It's the same\nthing-- if you want-- that we did by\nbuilding explicitly these monomials of higher\npower, but here you're",
    "start": "2486380",
    "end": "2494180"
  },
  {
    "text": "doing it implicitly. And it turns out\nthat it's actually-- there is no explicit\nversion that you can--",
    "start": "2494180",
    "end": "2500510"
  },
  {
    "text": "You can do it mathematically,\nbut the feature representation, the variable representation\nof this kernel",
    "start": "2500510",
    "end": "2505910"
  },
  {
    "text": "would be an infinitely\nlong vector. The space of function\nthat is built as a combination of Gaussians\nis not finite-dimensional.",
    "start": "2505910",
    "end": "2513130"
  },
  {
    "text": "For polynomials, you can check\nthat the space of function, it basically is a\npolynomial in d.",
    "start": "2513130",
    "end": "2519680"
  },
  {
    "text": "If I ask you how\nbig is the function space that you can build using\nthis-- well, this is easy.",
    "start": "2519680",
    "end": "2524780"
  },
  {
    "text": "It's just d-dimensional. With this, well, this is\na bit more complicated, but you can compute.",
    "start": "2524780",
    "end": "2531250"
  },
  {
    "text": "For this, it's not easy to\ncompute, because it's infinite.",
    "start": "2531250",
    "end": "2536270"
  },
  {
    "text": "So it in some sense is\na non-parametric model. What does it mean?",
    "start": "2536270",
    "end": "2541280"
  },
  {
    "text": "Of course, you still\nhave a finite number of parameters in practice. And that's the good news. But there is no fixed number\nof parameters a priori.",
    "start": "2541280",
    "end": "2548440"
  },
  {
    "text": "If I give you a hundred points,\nyou get a hundred parameters. If I give you 2 million points,\nyou get 2 million parameters.",
    "start": "2548440",
    "end": "2553670"
  },
  {
    "text": "If I give you 5 million points,\nyou get 5 million parameters. But you never hit a\nboundary of complexity,",
    "start": "2553670",
    "end": "2560299"
  },
  {
    "text": "because these are in some sense\nas an infinite-dimensional parameter space.",
    "start": "2560300",
    "end": "2565640"
  },
  {
    "text": " So of course, I\nsee that here there",
    "start": "2565640",
    "end": "2572210"
  },
  {
    "text": "are some of the part that I'm\nexplaining are complicated, especially if this is the\nfirst time you see them.",
    "start": "2572210",
    "end": "2577370"
  },
  {
    "text": "But the take-home message\nshould be essentially from least squares,\nI can understand what's going on from a\nnumerical point of view",
    "start": "2577370",
    "end": "2583936"
  },
  {
    "text": "and bridge numerics\nand statistics. Then by just simple\nlinear algebra, I can understand\nthe complexity--",
    "start": "2583936",
    "end": "2590089"
  },
  {
    "text": "how I can get\ncomplexly-- which is linear in the\nnumber of dimension or the number of points.",
    "start": "2590090",
    "end": "2596240"
  },
  {
    "text": "And then by following up,\nI can do a a little magic and go from the linear model\nto something non-linear.",
    "start": "2596240",
    "end": "2603480"
  },
  {
    "text": "The deep reason why this is\npossible are complicated. But as a take-home\nmessage, A, the computation",
    "start": "2603480",
    "end": "2609099"
  },
  {
    "text": "you can check easy. It remained the same. B, you can check that\nwhat you're doing is now allowing yourself to take\na more complicated model,",
    "start": "2609099",
    "end": "2615920"
  },
  {
    "text": "it's combination of\nthe kernel functions. And then even just by playing\nwith these simple demos,",
    "start": "2615920",
    "end": "2626480"
  },
  {
    "text": "you can understand a\nbit what is the effect. And that's what you\nintuitively would expect. So I hope that it would\nget you close enough",
    "start": "2626480",
    "end": "2632600"
  },
  {
    "text": "to have some awareness,\nwhen you use this. And of course, you can put--",
    "start": "2632600",
    "end": "2637849"
  },
  {
    "text": "when you abstract from the\nspecificity of this algorithm, you build an algorithm with\none or two parameters--",
    "start": "2637850",
    "end": "2643990"
  },
  {
    "text": "lambda and sigma. And so as soon as you ask\nme how you choose those, well, we go back to the\nfirst part of the lecture--",
    "start": "2643990",
    "end": "2649685"
  },
  {
    "text": "bias-variance, tradeoffs,\ncross-validation, and so on and so forth. So you just have to\nput them together.",
    "start": "2649685",
    "end": "2656750"
  },
  {
    "text": "There is a lot of stuff\nI've not talked about. And it's a step away\nfrom what we discussed, so you've just seen the\ntake-home message part,",
    "start": "2656750",
    "end": "2663901"
  },
  {
    "text": "but we could talk\nabout reproducing kernel hybrid spaces,\nthe functional analysis behind everything I said.",
    "start": "2663902",
    "end": "2669559"
  },
  {
    "text": "We can talk about Gaussian\nprocesses, which is basically the probabilistic version of\nwhat I just showed you now.",
    "start": "2669560",
    "end": "2674780"
  },
  {
    "text": "Then we can all see the\nconnection with a bunch of math like integral\nequations and PDEs. There is a whole connection\nwith the sampling",
    "start": "2674780",
    "end": "2681620"
  },
  {
    "text": "theory a la Shannon,\ninverse problems and so on. And there is a\nbunch of extension,",
    "start": "2681620",
    "end": "2687230"
  },
  {
    "text": "which are almost for free. You change the loss function. You can make the logistic,\nand you take kernel logistic regression.",
    "start": "2687230",
    "end": "2693109"
  },
  {
    "text": "You can take SVM, and\nyou get kernel SVM. Then you can also take more\ncomplicated output spaces.",
    "start": "2693110",
    "end": "2700430"
  },
  {
    "text": "And you can do multiclass,\nmultivariate regression. You can do regression. You can do multilabel,\nand you can",
    "start": "2700430",
    "end": "2706220"
  },
  {
    "text": "do a bunch of different things. And these are\nreally a step away. These are minor\nmodification of the code.",
    "start": "2706220",
    "end": "2712387"
  },
  {
    "text": "And you can do a bunch of stuff. So the good thing of this\nis that with really, really, really minor effort,\nyou can actually",
    "start": "2712387",
    "end": "2717770"
  },
  {
    "text": "solve a bunch of problem. I'm not saying that it's going\nto be the best algorithm ever, but definitely it\ngets you quite far.",
    "start": "2717770",
    "end": "2726290"
  },
  {
    "text": "So again we spent quite\na bit of time thinking about bias-variance and\nwhat it means and used least squares and just\nbasically warming up",
    "start": "2726290",
    "end": "2733250"
  },
  {
    "text": "a bit with this setting. And then in the last hour or\nso, we discussed least squares,",
    "start": "2733250",
    "end": "2739022"
  },
  {
    "text": "because it allows to just think\nin terms of linear algebra, which is something that-- one way or another--\nyou've seen in your life.",
    "start": "2739022",
    "end": "2745130"
  },
  {
    "text": "And then from there, you can\ngo from linear to non-linear. And that's a bit of magic,\nbut a couple of parts--",
    "start": "2745130",
    "end": "2751099"
  },
  {
    "text": "which are how you use\nit both numerically and just from a\npractical perspective",
    "start": "2751100",
    "end": "2756300"
  },
  {
    "text": "to go from complex models to\nsimple models and vice versa-- should be-- is the part that\nI hope you keep in your mind.",
    "start": "2756300",
    "end": "2763579"
  },
  {
    "text": "For now, our concern has just\nbeen to make predictions. If you hear\nclassification, you want to have good clarification. If you hear regression, you\nwant to do good regression.",
    "start": "2763579",
    "end": "2770869"
  },
  {
    "text": "But you didn't talk about-- we\ndidn't talk about understanding how did you do good regression?",
    "start": "2770870",
    "end": "2777200"
  },
  {
    "text": "So a typical example is\nthe example in biology. This is, perhaps, a bit old.",
    "start": "2777200",
    "end": "2783110"
  },
  {
    "text": "This is micro-arrays. But the idea is the datasets\nyou have is a bunch of patients.",
    "start": "2783110",
    "end": "2791860"
  },
  {
    "text": "For each patient, you\nhave measurements, and the measurements correspond\nto some gene expression level or some other\nbiological process.",
    "start": "2791860",
    "end": "2798265"
  },
  {
    "text": " The patients are divided in\ntwo groups, say, disease type",
    "start": "2798265",
    "end": "2805839"
  },
  {
    "text": "A and disease type B. And\nbased on the good prediction of whether a patient\nis disease type A or B,",
    "start": "2805840",
    "end": "2810880"
  },
  {
    "text": "you can change the way you cure\nit or you address the disease. So of course, you want to\nhave a good prediction.",
    "start": "2810880",
    "end": "2817130"
  },
  {
    "text": "You want to be able-- when\na new patient arrive-- to say whether it's going\nto-- this is type A or type B.",
    "start": "2817130",
    "end": "2824440"
  },
  {
    "text": "But oftentimes,\nwhat you want to do is that you want to use\nthis not as the final tool,",
    "start": "2824440",
    "end": "2829900"
  },
  {
    "text": "because unless deep\nlearning can solve this, you might go back and study a\nbit more the biological process",
    "start": "2829900",
    "end": "2838450"
  },
  {
    "text": "to understand a bit more. So you use this as\nmore statistical tools",
    "start": "2838450",
    "end": "2843589"
  },
  {
    "text": "like measurements, like the\nway you can use a microscope",
    "start": "2843590",
    "end": "2848840"
  },
  {
    "text": "or something to look into\nyour data and get information. And in that sense\nsometimes, it's interesting to--\ninstead of just saying",
    "start": "2848840",
    "end": "2854335"
  },
  {
    "text": "is this patient going to be\nmore likely to be disease type A or B, it's to\ngo in and say, ah,",
    "start": "2854335",
    "end": "2860300"
  },
  {
    "text": "but when you make\nthe prediction, what are the process that\nmatters for this prediction? Is this gene number 33 or 34,\nso that I can go in and say,",
    "start": "2860300",
    "end": "2869032"
  },
  {
    "text": "oh, these genes make sense,\nbecause they're in fact related to these other processes,\nwhich are known to be related,",
    "start": "2869032",
    "end": "2874210"
  },
  {
    "text": "involved in this disease. And doing that, you use\njust as a little tool, then you use other\nones to get a picture.",
    "start": "2874210",
    "end": "2880219"
  },
  {
    "text": "And then you put them together. And then it's mostly on the\ndoctor, or the clinician, or the biostatistician to try\nto develop better understanding.",
    "start": "2880219",
    "end": "2888320"
  },
  {
    "text": "But you do use these as\ntools to understand and look into the data. And in that perspective,\nthe word interpretability",
    "start": "2888320",
    "end": "2894650"
  },
  {
    "text": "plays a big role. And here by\ninterpretability I mean I not only want to\nmake predictions, but I want to know how I make\npredictions and tell you, come",
    "start": "2894650",
    "end": "2902299"
  },
  {
    "text": "afterwards with an\nexplanation of how I picked the information that\nwere contained in my data.",
    "start": "2902300",
    "end": "2909410"
  },
  {
    "text": "So so far it's hard to see how\nto do it with the tools we had.",
    "start": "2909410",
    "end": "2915079"
  },
  {
    "text": "So this is basically the\nfield of variable selection.",
    "start": "2915080",
    "end": "2921940"
  },
  {
    "text": "And in this basic\nform, the setting where we do understand\nwhat's going on is the setting of linear models.",
    "start": "2921940",
    "end": "2929210"
  },
  {
    "text": "So in this setting\nbasically, I just rewrite what we've seen before. You have x is a vector, and you\ncan think of it, for example,",
    "start": "2929210",
    "end": "2937510"
  },
  {
    "text": "as a patient. And xj are measurements\nthat you have done describing this patient.",
    "start": "2937510",
    "end": "2944140"
  },
  {
    "text": "When you do a linear\nmodel, you basically have that by putting a\nweight on each variables,",
    "start": "2944140",
    "end": "2950630"
  },
  {
    "text": "you're putting a weight\non each measurement. If a measurement\ndoesn't matter, you",
    "start": "2950630",
    "end": "2955839"
  },
  {
    "text": "think you might put here a zero. And it will disappear\nfrom the sum. If the measurement\nmatters a lot,",
    "start": "2955840",
    "end": "2961420"
  },
  {
    "text": "then here you might\nget a big weight. So one way to try to get the\nfeeling of which measurements",
    "start": "2961420",
    "end": "2967840"
  },
  {
    "text": "are important and which\nare not and to try to estimate and model, a linear\nmodel, where you get the w,",
    "start": "2967840",
    "end": "2973980"
  },
  {
    "text": "but ideally we would like to\nget the w, which has many zeros. You don't want to fumble with\nwhat's small and what's not.",
    "start": "2973980",
    "end": "2980189"
  },
  {
    "text": "So if you do least squares\nthe way I showed you before, you would get a w. Then you would get-- most of them you can check\nthat it will not be zero.",
    "start": "2980189",
    "end": "2987819"
  },
  {
    "text": "In fact, none of them\nwill be zero in general. And so now you have to decide\nwhat's small and what's big, and that might not be easy.",
    "start": "2987820",
    "end": "2993794"
  },
  {
    "text": " Oops, what happened here? ",
    "start": "2993794",
    "end": "3005590"
  },
  {
    "text": "So funny enough, this is\nthe name I found on how--",
    "start": "3005590",
    "end": "3010797"
  },
  {
    "text": "I don't remember the\nname of the book. It's the name that\nwas used to describe the process of variable\nselection, which",
    "start": "3010797",
    "end": "3016759"
  },
  {
    "text": "is a much harder\nproblem, because you don't want to make predictions. But you want to\ngo back and check how you make the prediction.",
    "start": "3016759",
    "end": "3022640"
  },
  {
    "text": "And so it's very easy to start\nto get overfitting and start",
    "start": "3022640",
    "end": "3028230"
  },
  {
    "text": "to try to squeeze the data\nuntil you get some information. So it's good to have\na procedure that will give you somewhat a\nclean procedure to extract",
    "start": "3028230",
    "end": "3034950"
  },
  {
    "text": "the important variables. Again, you can think of\nthis as a-- basically, I want to build an\nf, but I also want",
    "start": "3034950",
    "end": "3040500"
  },
  {
    "text": "to come up with a list or even\nbetter weights that tell me which variables are important.",
    "start": "3040500",
    "end": "3045599"
  },
  {
    "text": "And often this will\nbe just a list, which is much smaller than\nd, so that I can go back and say, oh, measurement 33,\n34, and 50-- what are they?",
    "start": "3045600",
    "end": "3052740"
  },
  {
    "text": "I could go in and look at it. Notice that there is also\na computational reason why this would be interesting.",
    "start": "3052740",
    "end": "3058743"
  },
  {
    "text": "Because of course,\nif d here is 50,000-- and what I see is\nthat, in fact, I can throw away most of these\nmeasurements and just keep 10--",
    "start": "3058743",
    "end": "3067710"
  },
  {
    "text": "then it means that\nI can hopefully reduce the complexity\nof my computation, but also the storage of\nthe data, for example.",
    "start": "3067710",
    "end": "3072905"
  },
  {
    "text": "If I have to send you\nthe datasets after I've done this thing,\nI've just to send you this teeny tiny matrix. ",
    "start": "3072905",
    "end": "3080160"
  },
  {
    "text": "So interpretability\nis one reason, but the computational\naspect could be another one.",
    "start": "3080160",
    "end": "3087690"
  },
  {
    "text": " Another reason that I don't\nwant to talk too much is also--",
    "start": "3087690",
    "end": "3093510"
  },
  {
    "text": "remember that we\nhad this idea, where we said we could document\nthe complexity of a model",
    "start": "3093510",
    "end": "3099600"
  },
  {
    "text": "by inventing\nfeatures, and he said do I always have to pay\nthe price of making it big?",
    "start": "3099600",
    "end": "3107100"
  },
  {
    "text": "Well, I basically--\nif you what-- I was pointing at-- I said, no, not always, because\nI was thinking of kernels.",
    "start": "3107100",
    "end": "3112566"
  },
  {
    "text": "These, if you want give\nyou another way potentially to go around in which what\nyou do is that, first of all, you explode the\nnumber of features.",
    "start": "3112566",
    "end": "3119252"
  },
  {
    "text": "You take many, many,\nmany, many, and then you use this as a\npreliminary step to shrink them down to a\nmore reasonable number.",
    "start": "3119252",
    "end": "3127050"
  },
  {
    "text": "Because it's quite\nlikely that among these many, many\nmeasurements, some of them would just be very\ncorrelated, or uninteresting,",
    "start": "3127050",
    "end": "3133170"
  },
  {
    "text": "or so on and so forth. So this dimensionality\nreduction or",
    "start": "3133170",
    "end": "3138480"
  },
  {
    "text": "computational or interpretable\nmodel perspective is what stands behind the desire\nto do something like this.",
    "start": "3138480",
    "end": "3145860"
  },
  {
    "text": "So let's say one more\nthing and then we'll stop. ",
    "start": "3145860",
    "end": "3151570"
  },
  {
    "text": "So suppose that you have an\ninfinite computational power. So the computation\nare not your concern,",
    "start": "3151570",
    "end": "3159450"
  },
  {
    "text": "and you want to\nsolve this problem. How will you do it? ",
    "start": "3159450",
    "end": "3166358"
  },
  {
    "text": "Suppose that you have the\ncode for least squares. And you can run it as\nmany times as you want.",
    "start": "3166358",
    "end": "3172450"
  },
  {
    "text": "How would you go and\ntry to estimate which variables are more important? AUDIENCE: [INAUDIBLE]\npossibility of computations.",
    "start": "3172450",
    "end": "3178864"
  },
  {
    "text": "LORENZO ROSASCO:\nThat's one possibility. What you do is that you have-- you start and look at\nall single variables.",
    "start": "3178864",
    "end": "3185270"
  },
  {
    "text": "And you solve least squares\nfor all single variables. Then you take all\ncouples of variables.",
    "start": "3185270",
    "end": "3192260"
  },
  {
    "text": "Then you get all\ntriplets of variables. And then you find\nwhich one is best.",
    "start": "3192260",
    "end": "3197797"
  },
  {
    "text": "From a statistical\npoint of view there is absolutely nothing\nwrong with this, because you're\ntrying everything.",
    "start": "3197797",
    "end": "3203330"
  },
  {
    "text": "And at some point, you\nfind what's the best. The problem is that\nit's combinatorial.",
    "start": "3203330",
    "end": "3208910"
  },
  {
    "text": "And you see that when you're\nin dimension a few more then-- very few, it's huge.",
    "start": "3208910",
    "end": "3216950"
  },
  {
    "text": "So it's exponential. So it turns out that\ndoing what you just",
    "start": "3216950",
    "end": "3223670"
  },
  {
    "text": "told me to do, which is what\nI asked you to tell me to do, which is this brute\nforce approach is equivalent to do\nsomething like this is again",
    "start": "3223670",
    "end": "3232400"
  },
  {
    "text": "a regularization approach. Here I put what is\ncalled the zero norm. The zero norm is\nactually not a norm.",
    "start": "3232400",
    "end": "3239730"
  },
  {
    "text": "And it is just functional. It's a thing that does\nthe following thing. If I give you a vector,\nyou've to return",
    "start": "3239730",
    "end": "3246320"
  },
  {
    "text": "the number of components\ndifferent from zero, only that. So you go inside and\nlook at each entry,",
    "start": "3246320",
    "end": "3251930"
  },
  {
    "text": "and you tell if they\nare different from zero. This is absolutely not convex.",
    "start": "3251930",
    "end": "3257799"
  },
  {
    "text": "And so this is the reason why\nthis problem is equivalent-- it becomes a computation\nnot feasible.",
    "start": "3257800",
    "end": "3264530"
  },
  {
    "text": "So perhaps, we can stop here. And what I want to show\nyou next is essentially--",
    "start": "3264530",
    "end": "3269810"
  },
  {
    "text": "if you have this-- and you know that in some sense,\nthis is what you would like to do, if you could\ndo it computationally,",
    "start": "3269810",
    "end": "3275809"
  },
  {
    "text": "but you cannot-- so how can you find\napproximate version of this",
    "start": "3275810",
    "end": "3281330"
  },
  {
    "text": "that you can\ncompute in practice? And we're going to discuss\ntwo ways of doing it. One is greedy methods and\none is convex relaxations.",
    "start": "3281330",
    "end": "3288820"
  },
  {
    "start": "3288820",
    "end": "3303734"
  }
]