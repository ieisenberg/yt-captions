[
  {
    "text": "[SQUEAKING] [RUSTLING] [CLICKING]",
    "start": "0",
    "end": "5335"
  },
  {
    "start": "5335",
    "end": "11620"
  },
  {
    "text": "STEVEN JOHNSON: So Alan\nmentioned fine differences as something that automatic\ndifferentiation is not doing.",
    "start": "11620",
    "end": "17150"
  },
  {
    "text": "But I do want to talk\nabout fine differences because they do\ncome up a lot when",
    "start": "17150",
    "end": "24560"
  },
  {
    "text": "you're talking about computing\nderivatives on a computer.  And as he mentioned,\nwe're going to spend",
    "start": "24560",
    "end": "31630"
  },
  {
    "text": "a lot of time talking about\nautomatic differentiation, which is this amazing\ntechnology where you can take a program\nthat computes a function.",
    "start": "31630",
    "end": "39129"
  },
  {
    "text": "And it will basically compute\nthe derivative analytically in some sense for you. But how it does it takes a\nlittle bit of explanation.",
    "start": "39130",
    "end": "47469"
  },
  {
    "text": "And this is great. If you're hand-computing\nderivatives, even though you might\nthink the rules are simple,",
    "start": "47470",
    "end": "54430"
  },
  {
    "text": "as you see from the\nhomework, when you get to more complicated functions,\nespecially involving vectors and matrices, it's\nquite error-prone.",
    "start": "54430",
    "end": "61780"
  },
  {
    "text": "And it's a common\nsource of bugs if you're doing numerical optimization\nroute finding sensitivity",
    "start": "61780",
    "end": "68230"
  },
  {
    "text": "analysis-- all of these\nthings on computers where you might want to\ndifferentiate a really complicated calculation.",
    "start": "68230",
    "end": "74370"
  },
  {
    "text": "So AD is a great alternative. It's extremely reliable.",
    "start": "74370",
    "end": "79549"
  },
  {
    "text": "If it gives an answer,\nit's probably correct.",
    "start": "79550",
    "end": "84620"
  },
  {
    "text": "Unfortunately, there's\nstill a lot of cases where automatic\ndifferentiation doesn't work,",
    "start": "84620",
    "end": "90800"
  },
  {
    "text": "where if, for example,\nit's code that calls external libraries\nand other languages that the automatic\ndifferentiation can't handle.",
    "start": "90800",
    "end": "97670"
  },
  {
    "text": "Or even in a given\nlanguage, usually they can only handle a\nsubset of the language.",
    "start": "97670",
    "end": "102710"
  },
  {
    "text": "And there's other\ncases where, even if automatic\ndifferentiation could work,",
    "start": "102710",
    "end": "107900"
  },
  {
    "text": "you need to really give it a\nlittle help in order to use it",
    "start": "107900",
    "end": "113030"
  },
  {
    "text": "effectively-- basically any kind\nof problem where you're computing an\nanswer approximately.",
    "start": "113030",
    "end": "120229"
  },
  {
    "text": "Like, for example, you're\nsolving a nonlinear equation by Newton's method. If you try to use\nautomatic differentiation,",
    "start": "120230",
    "end": "127250"
  },
  {
    "text": "even if it works, it\nwastes a lot of effort basically trying to\nnot only differentiate",
    "start": "127250",
    "end": "133040"
  },
  {
    "text": "your function you're\ncomputing but trying to exactly differentiate the\nerror in your approximation. So it tries to propagate the\nderivatives through every step",
    "start": "133040",
    "end": "140359"
  },
  {
    "text": "of the Newton's method. And so it turns out you\ncan usually do much,",
    "start": "140360",
    "end": "146099"
  },
  {
    "text": "much better if you know what\nfunction you're actually trying to compute. And we'll talk about\nthat a little bit.",
    "start": "146100",
    "end": "153959"
  },
  {
    "text": "And so in the cases where\nautomatic differentiation fails",
    "start": "153960",
    "end": "159050"
  },
  {
    "text": "completely or where automatic\ndifferentiation works but it's really\nsuboptimal-- like,",
    "start": "159050",
    "end": "165319"
  },
  {
    "text": "you'd like to replace portions\nof it with a more specialized method--",
    "start": "165320",
    "end": "171110"
  },
  {
    "text": "often you only need to give\nit a little bit of help. So what you do is\nyou take the one piece of your program,\nthe one function,",
    "start": "171110",
    "end": "177500"
  },
  {
    "text": "one subroutine that\nit fails on or where it's doing a poor job on.",
    "start": "177500",
    "end": "183680"
  },
  {
    "text": "And you do a manual\ndifferentiation of that. But then you let it handle\nthe rest of it, your program.",
    "start": "183680",
    "end": "189260"
  },
  {
    "text": "And AD systems are very\ngood at the chain rule. They're very good at\nbasically-- if they",
    "start": "189260",
    "end": "195063"
  },
  {
    "text": "know the derivative\nof each piece, they can propagate\nthe derivative through the whole thing. So on Julia, there's a package\ncalled ChainRules for this.",
    "start": "195063",
    "end": "202290"
  },
  {
    "text": "If you're using Python\nwith autograd or Jax, you do it by defining a custom\nJacobian vector product,",
    "start": "202290",
    "end": "209500"
  },
  {
    "text": "or vector Jacobian product. And we'll talk\nabout these as well. But the end result\nis you often find",
    "start": "209500",
    "end": "216130"
  },
  {
    "text": "yourself doing at least\nsome manual derivatives of complicated things. And as I said, it's\nvery error-prone.",
    "start": "216130",
    "end": "222489"
  },
  {
    "text": "And so you really\nneed to check it. If you have your program\nto compute the derivative",
    "start": "222490",
    "end": "228340"
  },
  {
    "text": "or you're doing\nhomework and you think you've differentiated some\ncomplicated expression that we've given you, it's a\nreally good idea to check it.",
    "start": "228340",
    "end": "237040"
  },
  {
    "text": "And then the usual\nway to check it is using finite\ndifference approximations.",
    "start": "237040",
    "end": "243400"
  },
  {
    "text": "And so remember, when we\nlook at the derivatives, right, you should think\nof it-- this is really",
    "start": "243400",
    "end": "251220"
  },
  {
    "text": "a way of computing the\nsmall change in the output df for a small change\nin the input dx.",
    "start": "251220",
    "end": "260639"
  },
  {
    "text": "And mathematically,\nwe like these d's to be infinitesimal things. But they're really--\nyou can think",
    "start": "260640",
    "end": "267330"
  },
  {
    "text": "of these as an\napproximation in the case when you have a finite delta x.",
    "start": "267330",
    "end": "272837"
  },
  {
    "text": "And we actually started\nin the opposite direction. We started with imagining a\nfinite small change delta x.",
    "start": "272837",
    "end": "281430"
  },
  {
    "text": "And that gives a finite\nsmall change delta f, right?",
    "start": "281430",
    "end": "286639"
  },
  {
    "text": "And in both cases, the linear\napproximation is f primed. So it's f-- if it's an\ninfinitesimal change",
    "start": "286640",
    "end": "293990"
  },
  {
    "text": "in the input, then this\nhas exactly f primed the x, or linearization. If you have a finite\nsmall change dx,",
    "start": "293990",
    "end": "302330"
  },
  {
    "text": "then the change in the\noutput is approximately the derivative times this.",
    "start": "302330",
    "end": "309530"
  },
  {
    "text": "Right? And so if you just look\nat the left-hand side,",
    "start": "309530",
    "end": "315060"
  },
  {
    "text": "this is easy, right? If you have a function\nthat computes f of x, you can just pick a\nvery small delta x,",
    "start": "315060",
    "end": "322050"
  },
  {
    "text": "compute f of x plus\ndelta x minus f of x, take a difference. And it should be approximately\nequal to your derivative times",
    "start": "322050",
    "end": "330840"
  },
  {
    "text": "that or operating on\nthat change, delta x, plus higher-order terms which\nare negligible if you can",
    "start": "330840",
    "end": "337349"
  },
  {
    "text": "make this delta x small enough. And we've actually--\nAlan has done a whole bunch of examples of\nthis sort in the notebooks",
    "start": "337350",
    "end": "344669"
  },
  {
    "text": "so far. But I want to dig into this in\na little more detail, right? So this is a finite difference\napproximation, right,",
    "start": "344670",
    "end": "353189"
  },
  {
    "text": "for the change in the output.",
    "start": "353190",
    "end": "358920"
  },
  {
    "text": "Very conventionally,\nif delta x is a scalar,",
    "start": "358920",
    "end": "364470"
  },
  {
    "text": "people often put the\ndelta x on the bottom there and to think of it as a--",
    "start": "364470",
    "end": "372169"
  },
  {
    "text": "did I write that in? Yeah. I should write that in. But so if you put the delta\nx in the bottom there, then--",
    "start": "372170",
    "end": "381117"
  },
  {
    "text": "because if it's a\nscalar, you can just say f prime of x is\napproximately f of x plus delta",
    "start": "381117",
    "end": "391610"
  },
  {
    "text": "x minus f of x\ndivided by delta x.",
    "start": "391610",
    "end": "400020"
  },
  {
    "text": "So this is probably sort of\nthe more conventional form to see this kind\nof approximation",
    "start": "400020",
    "end": "405449"
  },
  {
    "text": "in as an approximation\nfor the derivative rather than the approximation\nfor the differential, right,",
    "start": "405450",
    "end": "413130"
  },
  {
    "text": "the df. But we try to push\nthis format here",
    "start": "413130",
    "end": "418710"
  },
  {
    "text": "because this one works even\nif delta x is not a scalar. We can't divide by it, right? You have a vector, for example,\nor a matrix, or something",
    "start": "418710",
    "end": "426180"
  },
  {
    "text": "like that. And this is a linear\noperator. f prime is a linear operator\nacting on delta x, right? So this is still\naccurate, right?",
    "start": "426180",
    "end": "433260"
  },
  {
    "text": "So if you have a\nprogram that computes f, and then you can easily\ncompute the change in f.",
    "start": "433260",
    "end": "441330"
  },
  {
    "text": "Oops, I computed the-- the change in f for a\nsmall change in the output.",
    "start": "441330",
    "end": "448060"
  },
  {
    "text": "And then you can\ncompare that to what you think is your derivative\nthat you worked out laboriously by hand maybe.",
    "start": "448060",
    "end": "453300"
  },
  {
    "text": "Or if you're lucky, you have\nan automatic differentiation. You operate that on delta x. You should get approximately\nthe same thing.",
    "start": "453300",
    "end": "460410"
  },
  {
    "text": "So this formula here,\nas it says here-- this is called a\nforward difference. It's called\n\"forward\" because you",
    "start": "460410",
    "end": "466230"
  },
  {
    "text": "kind of perturb x in\nthe plus direction if delta x is positive.",
    "start": "466230",
    "end": "472350"
  },
  {
    "text": "Right? You can also do a\nbackwards difference-- f of x minus f of\nx minus delta x,",
    "start": "472350",
    "end": "479410"
  },
  {
    "text": "which is also approximately\nf prime of x dx. It doesn't really matter\nwhich one you use.",
    "start": "479410",
    "end": "486530"
  },
  {
    "text": "So that-- but if I tell\nyou \"forward difference,\" the immediate question is like,\nwhat's the backward difference?",
    "start": "486530",
    "end": "491580"
  },
  {
    "text": "It's just-- ALAN EDELMAN: Can\nI just point out that the backwards\ndifference is not the same sense of \"backwards\"\nas in backward mode",
    "start": "491580",
    "end": "497490"
  },
  {
    "text": "differentiation,\nor reverse mode? STEVEN JOHNSON: Yes, yes. We're going to talk about--\nforward and backward mode differentiation--\nthose are totally",
    "start": "497490",
    "end": "504090"
  },
  {
    "text": "unrelated to forward and\nbackward differences. So yeah.",
    "start": "504090",
    "end": "510040"
  },
  {
    "text": "And as I said, there's\na lot of forms. And we'll talk about maybe some\nmore sophisticated forms of it than forward and\nbackward differences.",
    "start": "510040",
    "end": "517599"
  },
  {
    "text": "You can do them-- the\nones that give you higher accuracy for a given delta x.",
    "start": "517600",
    "end": "523690"
  },
  {
    "text": "But they're generally\na last resort as a computational scheme. But they're a first resort\nas a quick check, right?",
    "start": "523690",
    "end": "530830"
  },
  {
    "text": "They're really,\nreally easy to do-- really kind of braindead. And they're hard\nto get too wrong.",
    "start": "530830",
    "end": "539080"
  },
  {
    "text": "So if this is a good match\nfor your f prime of x dx,",
    "start": "539080",
    "end": "544680"
  },
  {
    "text": "that's a really good sign. Right? So let's just start\nout with an example.",
    "start": "544680",
    "end": "551095"
  },
  {
    "text": "And this is an example\nwe've seen a few times in the class, which\nis f of A. So A is now going to be a square matrix.",
    "start": "551095",
    "end": "557889"
  },
  {
    "text": "And f of A is just A-squared OK? And we saw from the\nproduct rule several times",
    "start": "557890",
    "end": "564020"
  },
  {
    "text": "that df is A dA plus\ndA A, which is not",
    "start": "564020",
    "end": "570940"
  },
  {
    "text": "the same thing as 2AdA,\nthe first-year calculus rule for scalars.",
    "start": "570940",
    "end": "576790"
  },
  {
    "text": "Because A and delta\nA don't commute. We've seen this many times.",
    "start": "576790",
    "end": "582000"
  },
  {
    "text": "Right? So f prime here is now-- the derivative is really a\nlinear operator that gives",
    "start": "582000",
    "end": "588370"
  },
  {
    "text": "a small change in the input,\ngives you-- to first order, to the linear part of the\nchange in the output--",
    "start": "588370",
    "end": "596560"
  },
  {
    "text": "A delta A plus delta A A. Right? And what we can do is just--",
    "start": "596560",
    "end": "603430"
  },
  {
    "text": "suppose I've\nderived this formula and I want to check that\nI didn't screw it up. This one's pretty simple.",
    "start": "603430",
    "end": "608510"
  },
  {
    "text": "I'm pretty sure I\ndidn't screw it up, but we want to check this. So what you do is-- so let's\ndefine this function, f of A",
    "start": "608510",
    "end": "616710"
  },
  {
    "text": "equals A-squared. Right? We'll try it for a random input\nand a random small perturbation",
    "start": "616710",
    "end": "624170"
  },
  {
    "text": "delta A. So I'll\ndefine that function, define a random 4-by-4 matrix.",
    "start": "624170",
    "end": "630410"
  },
  {
    "text": "randn is Gaussian\nrandom numbers. They can be plus, or minus, or\nkind of bell curve-distributed. So some of them are positive.",
    "start": "630410",
    "end": "636630"
  },
  {
    "text": "Some of them are negative. But they're all of order one. All right? And then delta A, or d--",
    "start": "636630",
    "end": "643893"
  },
  {
    "text": "I'll just call it,\nright, dA here. But it's really a delta\nA. It's a finite change. It's also going to be a\ndifferent random matrix,",
    "start": "643893",
    "end": "651028"
  },
  {
    "text": "but I'm going to make it small. And I'm going to multiply\nit by a 10 to the minus 8th. And we'll talk\nvery soon about how",
    "start": "651028",
    "end": "657820"
  },
  {
    "text": "small do you want to make it. Do we want to make it\n10 to the minus 100th? Do we want to make\nit 10 to the minus 1? Where does this-- how do\nwe choose this, right?",
    "start": "657820",
    "end": "666110"
  },
  {
    "text": "So this is going to\nbe a small change. So it's random numbers of order\none times 10 to the minus 8.",
    "start": "666110",
    "end": "672519"
  },
  {
    "text": "So they're all kind of ordered\n10 to minus 8, 10 to minus 9. It's different random\nnumbers than in A.",
    "start": "672520",
    "end": "678460"
  },
  {
    "text": "So these will not commute. Should we check that, actually? Let's do A times dA\nminus dA times A. Right?",
    "start": "678460",
    "end": "692510"
  },
  {
    "text": "And you can see\nthat this is not 0. And you can say, oh, these\nare kind of small numbers but remember that the dA was\na small number of order 10",
    "start": "692510",
    "end": "701885"
  },
  {
    "text": "to the minus 8. And so these are\nalso small numbers. But they're small the\nsame order as dA, right?",
    "start": "701885",
    "end": "707270"
  },
  {
    "text": "So these are not actually\nsmall compared to dA. So this is not roundoff\nerrors or something else.",
    "start": "707270",
    "end": "716890"
  },
  {
    "text": "OK? So now our finite\ndifference approximation-- we just said it's just going to\nbe f of A plus dA minus f of A.",
    "start": "716890",
    "end": "723360"
  },
  {
    "text": "It's just the most simple,\nstupid thing directly from the definition of\nthe derivative, but plugging in instead of--",
    "start": "723360",
    "end": "731640"
  },
  {
    "text": "we're plugging in a\nnon-infinitesimal change in dA-- pretty small but\nnon-infinitesimal.",
    "start": "731640",
    "end": "737370"
  },
  {
    "text": "So we get this\napproximate change. It's small, but it's of the\nsame order as delta A, right?",
    "start": "737370",
    "end": "744570"
  },
  {
    "text": "So this-- and so the\nexact derivative, again,",
    "start": "744570",
    "end": "749910"
  },
  {
    "text": "is going to be A times dA\nplus dA times A. And again, this is small, but\nit's of the same--",
    "start": "749910",
    "end": "757350"
  },
  {
    "text": "it's small because\ndelta A is small. This is the df, right? This is not the derivative. This is, I should say, maybe\nthe directional differential.",
    "start": "757350",
    "end": "768720"
  },
  {
    "text": " Right? Right? So it's not the derivative.",
    "start": "768720",
    "end": "775160"
  },
  {
    "text": "It's the derivative operator\nacting on dA, right? And you can already see, just to\nthe eye, this kind of matches,",
    "start": "775160",
    "end": "782329"
  },
  {
    "text": "right? It looks like it's\npretty good, right? There's the number\nand so forth, right?",
    "start": "782330",
    "end": "790280"
  },
  {
    "text": "If we do 2A dA-- just a little bit smaller\nso I can see this.",
    "start": "790280",
    "end": "798069"
  },
  {
    "text": "OK? If I did 2A dA-- if I pretended they\ncommuted, right,",
    "start": "798070",
    "end": "803870"
  },
  {
    "text": "and I wrote A times dA\nplus dA times A as 2A dA,",
    "start": "803870",
    "end": "809390"
  },
  {
    "text": "you get answers that are\ncompletely different. And in fact, these look\nexactly the same as these.",
    "start": "809390",
    "end": "816970"
  },
  {
    "text": "But that's because\nJulia's only printing out six digits of this. And actually, there are more\ndigits in these numbers.",
    "start": "816970",
    "end": "823450"
  },
  {
    "text": "It's just not showing\nthem by default. So it might be useful to look\nat the exact answer minus--",
    "start": "823450",
    "end": "836260"
  },
  {
    "text": "or let's see, the approximate\nanswer minus the exact answer. Right? So the approximate is\nthe finite difference.",
    "start": "836260",
    "end": "844529"
  },
  {
    "text": "The exact is this\nformula A dA plus dA A that we derive from our\nderivative operator, right?",
    "start": "844530",
    "end": "853450"
  },
  {
    "text": "And the errors are really small. They're 10 to the minus 16th. So they're small compared\nto these numbers,",
    "start": "853450",
    "end": "861720"
  },
  {
    "text": "10 to the minus 8th. So this brings us\ninto to a key question",
    "start": "861720",
    "end": "871889"
  },
  {
    "text": "when you're talking\nabout errors. And just in general in\nmathematics, or science,",
    "start": "871890",
    "end": "877110"
  },
  {
    "text": "or engineering, whenever someone\ntells you something is small or something is large,\nyou should always",
    "start": "877110",
    "end": "884400"
  },
  {
    "text": "ask, compared to what, right? It's a meaningless question\nto say, for example,",
    "start": "884400",
    "end": "890160"
  },
  {
    "text": "is 1 meter-- is that\na small distance? Well, not compared to the\nradius of a hydrogen atom.",
    "start": "890160",
    "end": "900690"
  },
  {
    "text": "That's a really long distance\nfor an electron to move away from a hydrogen atom.",
    "start": "900690",
    "end": "906420"
  },
  {
    "text": "But 1 meter is a tiny\ndistance compared to the radius of the galaxy.",
    "start": "906420",
    "end": "911520"
  },
  {
    "text": "It might as well be 0 if\nyou're an astronomer, right? And so it really matters\nwhat you compare it to. We need a yardstick.",
    "start": "911520",
    "end": "918000"
  },
  {
    "text": "And a typical yardstick is,\nwe want to have some measure-- so I'm going to\ntalk about a norm.",
    "start": "918000",
    "end": "924320"
  },
  {
    "text": "But if these were vectors,\nyou'd know what the norm is. It's the length of the vector. So the distance from\napproximate to exact",
    "start": "924320",
    "end": "932760"
  },
  {
    "text": "is the size of this error. But I need to compare\nit to something",
    "start": "932760",
    "end": "938367"
  },
  {
    "text": "to decide whether\nit's large or small. And a natural thing\nto compare it to is the size of the exact answer.",
    "start": "938367",
    "end": "943490"
  },
  {
    "text": "Because the exact answer is\nthat f prime of A dA, right?",
    "start": "943490",
    "end": "948740"
  },
  {
    "text": "It's a small number\nbecause dA is small. So you really want to know\nwhether this error is small",
    "start": "948740",
    "end": "955130"
  },
  {
    "text": "compared to that. So for example-- and we can\nsee by eye they are, right? These errors are on the order\nof 10 to the minus 16th.",
    "start": "955130",
    "end": "962030"
  },
  {
    "text": "And the exact answer is on the\norder of 10 to the minus 8th. So this is good, right?",
    "start": "962030",
    "end": "967370"
  },
  {
    "text": "But to make it quantitative,\nwe define this ratio. And we call it the\nrelative error, right?",
    "start": "967370",
    "end": "972800"
  },
  {
    "text": "So it's the difference between\nwhat we want and what--",
    "start": "972800",
    "end": "978380"
  },
  {
    "text": "the difference\nbetween what we want, which is the exact\nanswer, and what we get,",
    "start": "978380",
    "end": "984670"
  },
  {
    "text": "which is the approximate\nanswer, and divided by the size of the exact thing.",
    "start": "984670",
    "end": "990870"
  },
  {
    "text": "OK? So this norm is defined in\nJulia in a package called",
    "start": "990870",
    "end": "996680"
  },
  {
    "text": "the LinearAlgebra package. And I can define a\nfunction relative_error that takes approx, exact,\nand computes exactly this--",
    "start": "996680",
    "end": "1003279"
  },
  {
    "text": "the norm of the\ndifference divided by the norm of the exact answer. And if I just\ncompare it here, we",
    "start": "1003280",
    "end": "1010000"
  },
  {
    "text": "see actually it's\nwhat we expect. The difference, we said,\nwas on the order of 10",
    "start": "1010000",
    "end": "1015050"
  },
  {
    "text": "to the minus 16th. The exact was on the order\nof the 10 to the minus 8th. The ratio was on the order\nof 10 to the minus 8th.",
    "start": "1015050",
    "end": "1020620"
  },
  {
    "text": "And indeed, what\nwe expected by eye is what we get here-- that it's\nabout 9 times e to the minus 9.",
    "start": "1020620",
    "end": "1028347"
  },
  {
    "text": "It's about 10 to the minus 8th-- so about eight\nsignificant digits, right?",
    "start": "1028347",
    "end": "1033959"
  },
  {
    "text": "So as I said, if we do the--",
    "start": "1033960",
    "end": "1039399"
  },
  {
    "text": "and let's do also the\nrelative error of our stupid--",
    "start": "1039400",
    "end": "1044859"
  },
  {
    "text": "of our incorrect way. If I do 2A dA-- if I use that as\nmy exact formula,",
    "start": "1044859",
    "end": "1054205"
  },
  {
    "text": "and if I had done my\nderivative incorrectly, right-- suppose I thought\nmy exact formula was 2 A dA.",
    "start": "1054205",
    "end": "1059650"
  },
  {
    "text": "Then I'd get a relative\nerror of 0.6 of order one. So I really would have caught\nit and said, oh, this--",
    "start": "1059650",
    "end": "1066340"
  },
  {
    "text": "you notice immediately that\nthis relative error is huge. Even though the 2A dA is\nsmall because dA is small,",
    "start": "1066340",
    "end": "1076790"
  },
  {
    "text": "the difference is not small\ncompared to the correct answer. So you immediately notice\nsomething is wrong here--",
    "start": "1076790",
    "end": "1084075"
  },
  {
    "text": "either have a bug in my finite\ndifference approximation-- but that's really\nhard to get wrong. It's just f of A plus\ndA minus f of A, right?",
    "start": "1084075",
    "end": "1092450"
  },
  {
    "text": "It's usually dead simple. So you usually get it right. Or I have a bug in my\nderivative, which is",
    "start": "1092450",
    "end": "1098540"
  },
  {
    "text": "really, really common, right? So this is good.",
    "start": "1098540",
    "end": "1103880"
  },
  {
    "text": "So you get a good match. This isn't a proof that\nsomething is correct, but it's a pretty big hint\nthat you got something",
    "start": "1103880",
    "end": "1113970"
  },
  {
    "text": "basically fundamentally right. It's pretty unlikely, if\nyou had a wrong formula,",
    "start": "1113970",
    "end": "1121530"
  },
  {
    "text": "if you picked a random input\nand a random displacement, that's going to give you the\ncorrect answer to eight digits,",
    "start": "1121530",
    "end": "1127650"
  },
  {
    "text": "right? And so it's a really good way\nto catch, really, screw ups.",
    "start": "1127650",
    "end": "1134040"
  },
  {
    "text": "Oh, I actually wrote this\nrelative error down there. Let me just-- yeah.",
    "start": "1134040",
    "end": "1139299"
  },
  {
    "text": "So here's my-- so if I really\nscrewed up, I'm actually--",
    "start": "1139300",
    "end": "1146300"
  },
  {
    "text": "I would be checking it against\nthe approximate answer. But yeah, if I\nreally screwed up,",
    "start": "1146300",
    "end": "1152000"
  },
  {
    "text": "you'd notice immediately, right? So it's over 50%, right?",
    "start": "1152000",
    "end": "1157830"
  },
  {
    "text": "Right? And now I need to go back\nhere and explain something that I kind of glossed over. So you all know what these\nvertical bars are for a vector,",
    "start": "1157830",
    "end": "1169190"
  },
  {
    "text": "right? And so the norm of\na vector is just the square root of the sum of\nthe squares of the entries. It's the length of the vector.",
    "start": "1169190",
    "end": "1175310"
  },
  {
    "text": "And that's one of the\nfirst things you learned about a vector, is\nthat it has a direction and then has a length.",
    "start": "1175310",
    "end": "1181160"
  },
  {
    "text": "That's probably the first\ndefinition of a vector that you heard. But now, in linear algebra,\nwe generalized our notion",
    "start": "1181160",
    "end": "1189310"
  },
  {
    "text": "of a vector to a\nvector space-- things you can add, and subtract,\nand multiply by scalars.",
    "start": "1189310",
    "end": "1194679"
  },
  {
    "text": "And so for example,\nin this example, we're using a matrix\nas a kind of vector.",
    "start": "1194680",
    "end": "1201590"
  },
  {
    "text": "So we're thinking the inputs\nare matrices, which we can add, subtract, multiply by scalars.",
    "start": "1201590",
    "end": "1206950"
  },
  {
    "text": "And you really need to not\nonly be able to add, subtract,",
    "start": "1206950",
    "end": "1213990"
  },
  {
    "text": "and multiply by scalars to talk\nabout errors, and derivatives, and magnitudes. You really need to also--\na notion of a length.",
    "start": "1213990",
    "end": "1221300"
  },
  {
    "text": "Or the fancy word in\nlinear algebra for that is a \"norm\" of a vector. And so we need to be able to--\nwe need a norm of our matrix.",
    "start": "1221300",
    "end": "1230340"
  },
  {
    "text": "And so what would that mean? What's the norm-- we know\nwhat the norm of a vector is.",
    "start": "1230340",
    "end": "1235789"
  },
  {
    "text": "You probably at least\nlearned the Euclidean norm. So if you have a matrix--",
    "start": "1235790",
    "end": "1241470"
  },
  {
    "text": "say, a 3-by-3 matrix-- or is this 4-by-4? These are 4-by-4 matrix, right?",
    "start": "1241470",
    "end": "1248400"
  },
  {
    "text": "What does that norm mean? And it turns out there\nare multiple choices.",
    "start": "1248400",
    "end": "1255953"
  },
  {
    "text": "And it turns out\nit doesn't really matter too much for our\npurposes which one we choose. So I'm going to focus on\nthe simplest one and kind",
    "start": "1255953",
    "end": "1265965"
  },
  {
    "text": "of the most familiar\none, which is basically-- this is the same as your\nEuclidean norm of vectors,",
    "start": "1265965",
    "end": "1271030"
  },
  {
    "text": "right? So for a Euclidean norm of\na, right, column vector, what do you do?",
    "start": "1271030",
    "end": "1276590"
  },
  {
    "text": "You take the square\nroot of the sum of the squares of the entries.",
    "start": "1276590",
    "end": "1281980"
  },
  {
    "text": "And this is also called\nan L2 norm, right? Or in linear algebra\nnotation, I could write it",
    "start": "1281980",
    "end": "1287080"
  },
  {
    "text": "as x transpose x is my\ndot product with itself.",
    "start": "1287080",
    "end": "1292340"
  },
  {
    "text": "And then I take the square root. So for a matrix-- at the simplest level,\nyou could think of it",
    "start": "1292340",
    "end": "1299149"
  },
  {
    "text": "as just a big bag full\nof numbers, right? The most obvious\nchoice of a norm is you do exactly the same\nprocess to the matrix entries.",
    "start": "1299150",
    "end": "1307700"
  },
  {
    "text": "You think of the matrix\nentries as the components of the matrix. And you take the square root\nof the sum of the squares",
    "start": "1307700",
    "end": "1313730"
  },
  {
    "text": "of those entries. And this is, in fact, called a\nmatrix form, a very famous one",
    "start": "1313730",
    "end": "1318800"
  },
  {
    "text": "called the Frobenius norm. And in Julia, you call-- oops-- norm(A).",
    "start": "1318800",
    "end": "1324210"
  },
  {
    "text": " You call the norm\nfunction on a matrix. That's what it does by default.",
    "start": "1324210",
    "end": "1329820"
  },
  {
    "text": "And we can use the Symbolics\npackage to just see what it's doing symbolically. So that's what Alan\nhas been using there.",
    "start": "1329820",
    "end": "1336330"
  },
  {
    "text": "So if I define a matrix\nfull of variables--",
    "start": "1336330",
    "end": "1344250"
  },
  {
    "text": "and maybe I should use koala\nbears or something like that. But this works.",
    "start": "1344250",
    "end": "1351899"
  },
  {
    "text": "I think actually I can\ndo an even nicer one. I can do m 1 to 2, 1 to 3.",
    "start": "1351900",
    "end": "1359670"
  },
  {
    "text": "And M equals collect(m). I think that works.",
    "start": "1359670",
    "end": "1364680"
  },
  {
    "text": "Yes. OK. So now I can have subscripts\nif you like subscripts better.",
    "start": "1364680",
    "end": "1370200"
  },
  {
    "text": "And then I can-- to do the norm. And it will do\nthat symbolically. And it's exactly\nwhat I just said. So the norm is the\nsquare root of the sum",
    "start": "1370200",
    "end": "1377640"
  },
  {
    "text": "of the squares of the entries. And it puts absolute\nvalue signs here. Because if they're complex\nnumbers, then you need those.",
    "start": "1377640",
    "end": "1383220"
  },
  {
    "text": "But we're dealing mostly with\nreal numbers in this class. OK? ",
    "start": "1383220",
    "end": "1389918"
  },
  {
    "text": "Oh, it used to have\nan abs2 function, but it doesn't anymore. So let me just\ndelete that comment. Right?",
    "start": "1389918",
    "end": "1395300"
  },
  {
    "text": "So one of the-- so this Frobenius norm way of\nmeasuring the size of a matrix",
    "start": "1395300",
    "end": "1402530"
  },
  {
    "text": "is really intuitive. Because like I said,\nit's the direct analog of the first and\nmaybe the only norm",
    "start": "1402530",
    "end": "1411470"
  },
  {
    "text": "you learned for column\nvectors, the Euclidean norm. But it's also nice in\nlinear algebra because--",
    "start": "1411470",
    "end": "1418655"
  },
  {
    "text": "so this Euclidean norm is\nreally useful in linear algebra because you can\nexpress it in terms",
    "start": "1418655",
    "end": "1425330"
  },
  {
    "text": "of these simple linear\nalgebra operations, as the x transpose x. And then that's a number.",
    "start": "1425330",
    "end": "1430730"
  },
  {
    "text": "And you take the square root. So this Frobenius norm of\nmatrices-- it turns out we can also write it in terms\nof matrix operations.",
    "start": "1430730",
    "end": "1443179"
  },
  {
    "text": "It turns out it's\nexactly the trace of A transpose A square root.",
    "start": "1443180",
    "end": "1448610"
  },
  {
    "text": "So A transpose A--\nyou take the trace. And you work out what it is. And it turns out that's\nexactly the sum of the squares",
    "start": "1448610",
    "end": "1454640"
  },
  {
    "text": "of the entries of the matrix. And then you take\nthe square root. And we call that the\nFrobenius norm, or AF.",
    "start": "1454640",
    "end": "1460640"
  },
  {
    "text": "And later on-- it says \"recall,\"\nbut I think we haven't done this yet-- we're going to generalize\nthis to think about a dot",
    "start": "1460640",
    "end": "1467320"
  },
  {
    "text": "product of matrices. We'll think of B\ndot A as a trace--",
    "start": "1467320",
    "end": "1477510"
  },
  {
    "text": "B transpose A. So this is\nlike the dot product of A with itself. So this is going to become\nimportant in defining",
    "start": "1477510",
    "end": "1483419"
  },
  {
    "text": "the gradient of matrix functions\nlike determinants, and traces, and so forth.",
    "start": "1483420",
    "end": "1489580"
  },
  {
    "text": "OK. So this is-- and anyway, so\nthis is kind of an aside. We're going to spend some\nmore time on this pretty soon.",
    "start": "1489580",
    "end": "1495720"
  },
  {
    "text": "But yes, you can absolutely\ndefine the norm for a matrix.",
    "start": "1495720",
    "end": "1501580"
  },
  {
    "text": "And you can-- and it\nturns out the simplest one is kind of what\nyou would expect,",
    "start": "1501580",
    "end": "1507480"
  },
  {
    "text": "the square root of the sum of\nthe squares of the entries. But it has this\nreally nice formula that lets you do linear algebra\nwith this norm, which we're not",
    "start": "1507480",
    "end": "1513682"
  },
  {
    "text": "going to do too\nmuch in this class. But that's one of the reasons\nwhy this is so useful.",
    "start": "1513682",
    "end": "1519420"
  },
  {
    "text": "I can compute it in\nmultiple ways in Julia. But I can take the square root\nof the trace of A transpose A.",
    "start": "1519420",
    "end": "1525600"
  },
  {
    "text": "I can take norm of A. The\nsquare root of the sum of abs2 is the absolute value\nsquared of the entries.",
    "start": "1525600",
    "end": "1532110"
  },
  {
    "text": "Or I can take this kind of\nmore complicated expression, square root of the sum\nof A i, j squared for--",
    "start": "1532110",
    "end": "1538529"
  },
  {
    "text": "i is 1 to 4. j is 1 to 4. They all give the same answers. OK? And in fact, if we\ngo back, this is",
    "start": "1538530",
    "end": "1550200"
  },
  {
    "text": "something I kind of swept\nunder the rug earlier. But in the whole\ndefinition of a derivative,",
    "start": "1550200",
    "end": "1555510"
  },
  {
    "text": "right, when we\ndefine the derivative as the linearization\nof the difference,",
    "start": "1555510",
    "end": "1562000"
  },
  {
    "text": "right-- so that\nbasically the derivative is what gives you the\ndifference to first order",
    "start": "1562000",
    "end": "1568290"
  },
  {
    "text": "such that every other\nterm is higher order. In order to define what it\nmeans to be higher order,",
    "start": "1568290",
    "end": "1575880"
  },
  {
    "text": "you have to have some\nmeasure of the length, of the size of a vector. And so you actually need\na norm of any vector space",
    "start": "1575880",
    "end": "1584370"
  },
  {
    "text": "if you want to define a\nderivative in this way. But fortunately, norms are\nvery, very easy to define.",
    "start": "1584370",
    "end": "1590460"
  },
  {
    "text": "And they're available for\njust about any vector space you're likely to think of.",
    "start": "1590460",
    "end": "1597850"
  },
  {
    "text": "OK. So now we have a norm of-- we know what it means to\nmeasure the norm of a matrix.",
    "start": "1597850",
    "end": "1604330"
  },
  {
    "text": "We have-- oops,\nlet me scroll down. We have a measure of the error.",
    "start": "1604330",
    "end": "1614750"
  },
  {
    "text": "So this is the relevant error. We want the difference\nbetween the approximation",
    "start": "1614750",
    "end": "1619810"
  },
  {
    "text": "and the exact answer, or a\nderivative formula divided",
    "start": "1619810",
    "end": "1625450"
  },
  {
    "text": "by the magnitude of\nthe exact formula. And so now I want to go back\nand talk about this issue",
    "start": "1625450",
    "end": "1634260"
  },
  {
    "text": "of how big dA should be. So we have a small perturbation.",
    "start": "1634260",
    "end": "1641100"
  },
  {
    "text": "Here, I chose them of\norder 10 to the minus 8th. So why did I choose\n10 to the minus 8? And does it matter?",
    "start": "1641100",
    "end": "1646590"
  },
  {
    "text": "Like, what happens if I\nmake it 10 to the minus 4th, or 10 to the minus\n1, and so forth?",
    "start": "1646590",
    "end": "1652720"
  },
  {
    "text": "So before I do any\ncalculations, let's think about what you\nexpect might happen.",
    "start": "1652720",
    "end": "1658740"
  },
  {
    "text": "So this formula, right--",
    "start": "1658740",
    "end": "1665090"
  },
  {
    "text": "this finite difference\nformula is an approximation",
    "start": "1665090",
    "end": "1670529"
  },
  {
    "text": "for the derivative\ntimes delta x. But it's dropping\nhigher-order terms, right?",
    "start": "1670530",
    "end": "1676920"
  },
  {
    "text": "This only becomes exact in the\nlimit as delta x goes to 0.",
    "start": "1676920",
    "end": "1684060"
  },
  {
    "text": "Right? That's the limit\nin which we get-- or strictly speaking, I should\nmaybe say, the norm of that",
    "start": "1684060",
    "end": "1689840"
  },
  {
    "text": "goes to 0, right? That's the limit in which\nwe get a derivative out",
    "start": "1689840",
    "end": "1696470"
  },
  {
    "text": "of a difference, right? So you might expect\nthat, as delta x gets smaller, and\nsmaller, and smaller,",
    "start": "1696470",
    "end": "1703650"
  },
  {
    "text": "this difference formula should\nget more and more accurate, right? It should match-- these\nterms that you're neglecting",
    "start": "1703650",
    "end": "1711690"
  },
  {
    "text": "should get smaller and smaller. And this difference should match\nour derivative more and more",
    "start": "1711690",
    "end": "1718169"
  },
  {
    "text": "closely. So what I'm going\nto do in a second is I'm going to plot\nthe relative error",
    "start": "1718170",
    "end": "1726430"
  },
  {
    "text": "as a function of\nthe norm of delta x-- so in this case,\ndelta A, right?",
    "start": "1726430",
    "end": "1734820"
  },
  {
    "text": "And you might expect that\nthe error is going to go down with delta A.\nAnd in fact, it--",
    "start": "1734820",
    "end": "1744480"
  },
  {
    "text": "and if we think\nabout it, these are going to be small magnitudes. So it's always nice to plot\nthem on a log-log scale.",
    "start": "1744480",
    "end": "1752039"
  },
  {
    "text": "A log-log scale has\nthe nice property that it's going to turn power\nlaws into straight lines. So usually in all\nof these things,",
    "start": "1752040",
    "end": "1758250"
  },
  {
    "text": "we're going to see\npower law dependencies. It could be\nproportional to delta A, delta A-squared, delta A-cubed.",
    "start": "1758250",
    "end": "1764220"
  },
  {
    "text": "On a log-log scale, that\nturns into a straight line. And this is something that I\ntry and drill into my students. Like, anytime you're plotting\nsomething, if at all possible,",
    "start": "1764220",
    "end": "1772900"
  },
  {
    "text": "you should choose\nyour coordinates-- either logs or some\nchange of variables-- to make the thing you're\nplotting a straight line.",
    "start": "1772900",
    "end": "1779130"
  },
  {
    "text": "It just makes it-- or at least, you expect\nit to be a straight line. That makes it much easier to\nunderstand what's going on.",
    "start": "1779130",
    "end": "1785620"
  },
  {
    "text": "So if we expect it to be a power\nlaw, we expect a straight line. If it's not a straight\nline, you should be able to immediately see that.",
    "start": "1785620",
    "end": "1791250"
  },
  {
    "text": "Whereas, if you plot\nit on a linear scale and it goes on a\nlinear scale, the error",
    "start": "1791250",
    "end": "1798910"
  },
  {
    "text": "might go like this, right? It's really hard to see what\nkind of dependence that is.",
    "start": "1798910",
    "end": "1806125"
  },
  {
    "text": "And it's also really\nhard to see what happens when things get really small. OK. So let's do that.",
    "start": "1806125",
    "end": "1812200"
  },
  {
    "text": "So I wrote down here some\nJulia code that is going to--",
    "start": "1812200",
    "end": "1818429"
  },
  {
    "text": "because I'm going to use\nthe plotting package. And I'm going to make\nsome nice labels.",
    "start": "1818430",
    "end": "1823845"
  },
  {
    "text": "There's multiple plotting\npackages for Julia. I'm going to use\none called Plots. And I'm going to compute\nthe relative error of my f",
    "start": "1823845",
    "end": "1834279"
  },
  {
    "text": "of A plus dA minus\nf of A. But I'm going to scale it dA\nby different factors.",
    "start": "1834280",
    "end": "1841409"
  },
  {
    "text": "OK? And I'm going to compare\nthat to my exact answer which",
    "start": "1841410",
    "end": "1847200"
  },
  {
    "text": "I had above, my A dA plus dA\nA. But if I scale dA by s--",
    "start": "1847200",
    "end": "1854284"
  },
  {
    "text": "that scale factor,\nwhich is going to go from 10 to the minus\n8th to 10 to the 8th-- ",
    "start": "1854285",
    "end": "1861620"
  },
  {
    "text": "the exact formula is linear. It's a linear operator. So scaling dA by s will\njust scale that by s.",
    "start": "1861620",
    "end": "1867680"
  },
  {
    "text": "So this is just going to be-- so I'm going to keep\nthe same random dA, but I'm just going to scale it\nup or down in magnitude by--",
    "start": "1867680",
    "end": "1876170"
  },
  {
    "text": "this gives me a range of\npoints from minus 8 to 8. And then I'm going\nto take 10 to that. So it's going to go from--",
    "start": "1876170",
    "end": "1882380"
  },
  {
    "text": "this is 50 points from 10 to\nthe minus 8th to 10 to the 8th.",
    "start": "1882380",
    "end": "1896870"
  },
  {
    "text": "OK? And I'm going to plot them. And I'm going to plot also a\nstraight line for reference.",
    "start": "1896870",
    "end": "1903070"
  },
  {
    "text": "And this is what we get. So this is a plot of\nthe relative error--",
    "start": "1903070",
    "end": "1909310"
  },
  {
    "text": "so the finite difference\nminus the derivative formula divided by the derivative\nformula, magnitude,",
    "start": "1909310",
    "end": "1916960"
  },
  {
    "text": "versus the norm of A-- so\nfor the same direction, but scaling the size.",
    "start": "1916960",
    "end": "1923230"
  },
  {
    "text": "And the direction\nwas chosen randomly. And the dots are these errors.",
    "start": "1923230",
    "end": "1928269"
  },
  {
    "text": "And the line here is\njust a line proportional to delta A-- just\nfor comparison.",
    "start": "1928270",
    "end": "1935400"
  },
  {
    "text": "OK? So we see-- stare at\nthis for a second. And just a bunch of things\nshould jump out at you.",
    "start": "1935400",
    "end": "1942210"
  },
  {
    "text": "So if delta A is-- so suppose we start out\nwith a pretty big delta A of order one, right?",
    "start": "1942210",
    "end": "1950340"
  },
  {
    "text": "Then you expect the\nfinite difference is not going to be very accurate\nif delta A is big. And it's not.",
    "start": "1950340",
    "end": "1955620"
  },
  {
    "text": "The error is, like, 100%. And then as you decrease\ndelta A in magnitude,",
    "start": "1955620",
    "end": "1962220"
  },
  {
    "text": "the error gets smaller\njust like we expect, right? The error-- the fine\ndifference formula",
    "start": "1962220",
    "end": "1968190"
  },
  {
    "text": "should get more\naccurate mathematically as delta A gets\ncloser to a dA, it",
    "start": "1968190",
    "end": "1974850"
  },
  {
    "text": "goes closer to an infinitesimal. So you can drop those\nhigher-order terms. And it actually goes to 0.",
    "start": "1974850",
    "end": "1980010"
  },
  {
    "text": "It gets smaller\nlinearly, proportional to a straight line,\nproportional to delta A.",
    "start": "1980010",
    "end": "1986040"
  },
  {
    "text": "But at some point, when the\nrelative error gets about 10",
    "start": "1986040",
    "end": "1991300"
  },
  {
    "text": "to the minus-- I don't know. This is about 10 to the minus\n10th here, it looks like.",
    "start": "1991300",
    "end": "1996390"
  },
  {
    "text": "It stops. The error stops getting smaller. And then as you make\ndelta A even smaller,",
    "start": "1996390",
    "end": "2002090"
  },
  {
    "text": "the error gets bigger. So it seems like\nmath is broken here.",
    "start": "2002090",
    "end": "2007590"
  },
  {
    "text": "So does anyone--\nany guesses on why the error is getting\nbigger when we make delta",
    "start": "2007590",
    "end": "2015200"
  },
  {
    "text": "A really small, like 10\nto the minus 10th, 10 to the minus 12th? Like, why isn't the\nerror just getting better, and better, and better?",
    "start": "2015200",
    "end": "2021202"
  },
  {
    "text": "That's what the\nequations tell us. AUDIENCE: Does it\nhave this over there? AUDIENCE: It's a roundoff error. STEVEN JOHNSON:\nIt's roundoff error.",
    "start": "2021202",
    "end": "2026733"
  },
  {
    "text": "Yes. So what's happening-- and so\nthere's two main features.",
    "start": "2026733",
    "end": "2032030"
  },
  {
    "text": "So first of all,\nthe relative error decreases linearly with delta. That's what we expect.",
    "start": "2032030",
    "end": "2037330"
  },
  {
    "text": "This is called\nfirst-order accuracy, by the way-- this\nlinear decrease. But then it increases.",
    "start": "2037330",
    "end": "2042640"
  },
  {
    "text": "And it increases because\nof roundoff errors. Because the computer\nis only going",
    "start": "2042640",
    "end": "2049000"
  },
  {
    "text": "to keep a certain number of\ndigits when it does arithmetic. And so when delta A gets\nreally, really small,",
    "start": "2049000",
    "end": "2057760"
  },
  {
    "text": "it turns out it's going to cause\nthe answers to be completely wrong.",
    "start": "2057760",
    "end": "2063199"
  },
  {
    "text": "So I'm going to talk about\nboth of these things. So first of all,\nI'm going to talk about the right portion\nof the plot, where it's",
    "start": "2063199",
    "end": "2069310"
  },
  {
    "text": "kind of doing what we expect. The error is decreasing\nas you make dA smaller.",
    "start": "2069310",
    "end": "2074589"
  },
  {
    "text": "But I want to drill\nin a little deeper. Like, why is it decreasing\nlinearly with dA? Why are we getting first-order\naccuracy, as it's called--",
    "start": "2074590",
    "end": "2083469"
  },
  {
    "text": "so error that goes dA to\nthe first power, right?",
    "start": "2083469",
    "end": "2088510"
  },
  {
    "text": "And then I'm going to talk about\nthe second portion of the plot, this left portion--",
    "start": "2088510",
    "end": "2094908"
  },
  {
    "text": "that when dA gets really\nsmall, the error actually",
    "start": "2094909",
    "end": "2100010"
  },
  {
    "text": "not only stops decreasing. It actually gets worse. And as someone\nsaid, it has to do",
    "start": "2100010",
    "end": "2105620"
  },
  {
    "text": "with roundoff errors because\nof the finite precision of computer arithmetic. But I want to drill down into\nthat a little bit more as well.",
    "start": "2105620",
    "end": "2113970"
  },
  {
    "text": "So let's start with the\nright portion of the plot. This is easier to\nunderstand, I think,",
    "start": "2113970",
    "end": "2119060"
  },
  {
    "text": "because it's not about\ncomputer arithmetic. You can understand it\njust from the equations,",
    "start": "2119060",
    "end": "2125300"
  },
  {
    "text": "treating everything as real\nnumbers, as exact arithmetic. So we're computing--\nso basically,",
    "start": "2125300",
    "end": "2135170"
  },
  {
    "text": "the way to understand this is,\nyou can use a Taylor series.",
    "start": "2135170",
    "end": "2140369"
  },
  {
    "text": "So if you can imagine\ntaking f of plus dx",
    "start": "2140370",
    "end": "2147830"
  },
  {
    "text": "and thinking of this not just as\nthe definition of a derivative, but thinking of it\nas a Taylor series.",
    "start": "2147830",
    "end": "2153610"
  },
  {
    "text": "So if you take the Taylor\nseries, you get f of x plus dx. What's the first term?",
    "start": "2153610",
    "end": "2158950"
  },
  {
    "text": "It's f of x. What's the second term? It's f prime of x dx. That's our linear term.",
    "start": "2158950",
    "end": "2167050"
  },
  {
    "text": "And this still works even when\ndx is a vector, or a matrix, or in some vector space.",
    "start": "2167050",
    "end": "2173350"
  },
  {
    "text": "And think about, again, 18.01. What's the next term\nin the Taylor series?",
    "start": "2173350",
    "end": "2179319"
  },
  {
    "text": "The next term is the\nsecond derivative term, the f double prime term. ",
    "start": "2179320",
    "end": "2186640"
  },
  {
    "text": "And that would give you terms\nthat go like dx-squared. And it turns out this still\nworks for arbitrary vector",
    "start": "2186640",
    "end": "2192579"
  },
  {
    "text": "spaces. This second derivative is\ngoing to be something called a Hessian or quadratic form.",
    "start": "2192580",
    "end": "2197700"
  },
  {
    "text": "We'll talk about that later. But the basic idea is\nthis is proportional to a second derivative. And it has terms that go like\nthe square of the change.",
    "start": "2197700",
    "end": "2206250"
  },
  {
    "text": "And then there are even\nhigher-order terms, which I can use this notation\nin computer science-- this little o of delta\nx-squared-- terms",
    "start": "2206250",
    "end": "2213810"
  },
  {
    "text": "that go even smaller--\ndelta x-cubed, delta x to the 4th from the\nthird derivative,",
    "start": "2213810",
    "end": "2219263"
  },
  {
    "text": "and the fourth\nderivative, and so forth. So if the function-- and\nit turns out this is true. It doesn't even require\na Taylor series.",
    "start": "2219263",
    "end": "2225390"
  },
  {
    "text": "This is true any time\nyou have a function that has a second derivative. It may not even be 3\ntimes differentiable.",
    "start": "2225390",
    "end": "2232980"
  },
  {
    "text": "But if it has a\nsecond derivative, then you can write these\nfirst three terms and then",
    "start": "2232980",
    "end": "2239670"
  },
  {
    "text": "the higher-order term. So imagine what does this\nmean for our difference approximation, right?",
    "start": "2239670",
    "end": "2244760"
  },
  {
    "text": "So what are we computing here? We're plotting this\nrelative error. We're plotting f of x\nplus dx minus f of x.",
    "start": "2244760",
    "end": "2253570"
  },
  {
    "text": "That's our finite difference. And we're subtracting our exact\nderivative f prime of x dx.",
    "start": "2253570",
    "end": "2261869"
  },
  {
    "text": "Right? But then we're dividing by\nthe exact derivative norm. But what happens when we plug\nthis Taylor series into here?",
    "start": "2261870",
    "end": "2274140"
  },
  {
    "text": "So you-- well, first of\nall, the first two terms",
    "start": "2274140",
    "end": "2281279"
  },
  {
    "text": "here are just these\nfirst two terms here. It's just f of x. If we move the f of x\nto the left-hand side,",
    "start": "2281280",
    "end": "2290244"
  },
  {
    "text": "it's gone from there. That's this. OK? So what's left? Well, there's this term.",
    "start": "2290245",
    "end": "2296609"
  },
  {
    "text": "But this cancels the\nthing we're subtracting, the exact derivative.",
    "start": "2296610",
    "end": "2303090"
  },
  {
    "text": "So what's left over? These terms. OK?",
    "start": "2303090",
    "end": "2308510"
  },
  {
    "text": "It is the terms\nproportional to delta x-squared plus\nhigher-order terms.",
    "start": "2308510",
    "end": "2315625"
  },
  {
    "text": " Right? So does everyone follow that? So I know, doing math\nlike this from slides,",
    "start": "2315625",
    "end": "2325610"
  },
  {
    "text": "I need to pause and make\nsure everyone has a chance to stare at the formula. Because it's easy\nto kind of scroll",
    "start": "2325610",
    "end": "2330968"
  },
  {
    "text": "faster than you\ncan read compared to when I write it on the\nblackboard or on a notebook, right? So all I'm doing is\nplugging-- and you",
    "start": "2330968",
    "end": "2338067"
  },
  {
    "text": "can think of this\nas a Taylor series-- into my finite\ndifference formula and thinking about my error, my\nfinite difference minus exact",
    "start": "2338067",
    "end": "2346820"
  },
  {
    "text": "over exact. And this finite\ndifference minus exact-- the first two terms cancel.",
    "start": "2346820",
    "end": "2354640"
  },
  {
    "text": "And what's left\nover are these terms that go, like, delta x-squared\nand higher-order terms.",
    "start": "2354640",
    "end": "2360970"
  },
  {
    "text": "But then we're dividing\nit by a denominator that's",
    "start": "2360970",
    "end": "2366170"
  },
  {
    "text": "f of x dx is\nproportional to delta x. So if I take a term that's\nproportional to delta",
    "start": "2366170",
    "end": "2371359"
  },
  {
    "text": "x-squared plus\nhigher-order terms and divide it by a term that's\nproportional to delta x, what",
    "start": "2371360",
    "end": "2378260"
  },
  {
    "text": "you get are terms that\nare proportional to delta x from the first term\nplus higher-order terms.",
    "start": "2378260",
    "end": "2386150"
  },
  {
    "text": "Right? So what we expect\nis exactly this.",
    "start": "2386150",
    "end": "2396440"
  },
  {
    "text": "We expect our relative error\nto be approximately linear",
    "start": "2396440",
    "end": "2403980"
  },
  {
    "text": "in delta x plus terms that\nare higher-order that vanish as delta x-- that are negligible\nas delta X gets small.",
    "start": "2403980",
    "end": "2413339"
  },
  {
    "text": "Does that make sense? So this is kind of the inherent\nerror in the finite difference",
    "start": "2413340",
    "end": "2420569"
  },
  {
    "text": "approximation. We expect this forward\ndifference formula to have errors that decrease\nlinearly with delta x.",
    "start": "2420570",
    "end": "2427080"
  },
  {
    "text": "This is called the\ntruncation error, the error that you get because\nyou truncated the Taylor",
    "start": "2427080",
    "end": "2435520"
  },
  {
    "text": "series. Because delta x is\nnot infinitesimal. And that's exactly\nwhat we're seeing",
    "start": "2435520",
    "end": "2440730"
  },
  {
    "text": "in this plot in the\nright-hand side. We're seeing it's actually\nalmost perfectly linear as we decrease delta A. So\nthat's what the math predicts.",
    "start": "2440730",
    "end": "2453450"
  },
  {
    "text": "So this deviation here is\nnot predicted by that math.",
    "start": "2453450",
    "end": "2459530"
  },
  {
    "text": "And it would not happen if we\nwere doing exact arithmetic with real numbers.",
    "start": "2459530",
    "end": "2464940"
  },
  {
    "text": "So what we're-- this\nincrease over here is due to our roundoff errors.",
    "start": "2464940",
    "end": "2473060"
  },
  {
    "text": "As I said, it's due to\nthe fact that we're not doing exact math, at least exact\nreal number math on a computer.",
    "start": "2473060",
    "end": "2481130"
  },
  {
    "text": "We're doing only approximate\nreal number mathematics. And that's called\nfloating-point arithmetic.",
    "start": "2481130",
    "end": "2489560"
  },
  {
    "text": "And so the basic thing is--\nyou can think of it this way. So a computer, right--\nit cannot store--",
    "start": "2489560",
    "end": "2496388"
  },
  {
    "text": "if you have an\narbitrary real number, you would need infinitely many\ndigits to store it, right?",
    "start": "2496388",
    "end": "2501770"
  },
  {
    "text": "A computer doesn't\nhave infinite memory. And even working with\na huge number of digits would be very expensive, right?",
    "start": "2501770",
    "end": "2508760"
  },
  {
    "text": "And so a computer-- when\nit's working with numbers, by default, it keeps only\na relatively small number",
    "start": "2508760",
    "end": "2514730"
  },
  {
    "text": "of significant digits-- I mean relatively small\ncompared to infinity. It keeps about 15 decimal\ndigits, which is a lot, right?",
    "start": "2514730",
    "end": "2521309"
  },
  {
    "text": "That's a lot more than you would\ncarry in a hand calculation, right?",
    "start": "2521310",
    "end": "2526530"
  },
  {
    "text": "And with 15 decimal\ndigits, it can do arithmetic very, very quickly. It's built into the CPU.",
    "start": "2526530",
    "end": "2531750"
  },
  {
    "text": "That's why everyone uses\nthis same number of digits. Whether you're in Matlab, or\nin Python, or in Julia, or C,",
    "start": "2531750",
    "end": "2538560"
  },
  {
    "text": "or Fortran, this is\nkind of the default. It's called double-precision\nfloating-point arithmetic.",
    "start": "2538560",
    "end": "2543839"
  },
  {
    "text": "And so everyone uses\nthe same thing most of the time, at least.",
    "start": "2543840",
    "end": "2548950"
  },
  {
    "text": "And so this is the\nsource of that increase. It's these things\ncalled roundoff errors",
    "start": "2548950",
    "end": "2554850"
  },
  {
    "text": "from this finite number\nof significant digits. And so let's think in a\nlittle bit more detail of how",
    "start": "2554850",
    "end": "2562319"
  },
  {
    "text": "that happens. So what happens if\ndelta x is too small?",
    "start": "2562320",
    "end": "2568590"
  },
  {
    "text": "So then f of x plus\ndelta x and f of x",
    "start": "2568590",
    "end": "2573720"
  },
  {
    "text": "are almost the\nsame number, right? They're almost the same.",
    "start": "2573720",
    "end": "2579310"
  },
  {
    "text": "And when you subtract\nthem, right, then most of the significant digits\ncancel one another.",
    "start": "2579310",
    "end": "2587150"
  },
  {
    "text": "OK? And in fact, even when\nyou take x plus dx, if x is really small-- if\nx is only storing 15 digits",
    "start": "2587150",
    "end": "2595810"
  },
  {
    "text": "and you add a delta x that's 10\nto the minus 100th, x plus dx will just give you x.",
    "start": "2595810",
    "end": "2602680"
  },
  {
    "text": "Because it'll just round\noff x plus dx back to x. It won't be able to store that\n1 in the 100th decimal place.",
    "start": "2602680",
    "end": "2610340"
  },
  {
    "text": "Right? So in general, this is called\na catastrophic cancellation-- when you take two\nnearly equal numbers",
    "start": "2610340",
    "end": "2617500"
  },
  {
    "text": "and you subtract them\nto finite, but they each have finite accuracy.",
    "start": "2617500",
    "end": "2622835"
  },
  {
    "text": "You subtract them. Most of the significant\ndigits cancel. You're left with only a few\nsignificant digits or maybe",
    "start": "2622835",
    "end": "2629240"
  },
  {
    "text": "even no significant digits. Maybe if they're small\nenough, this difference might just be 0. So it's easier to see with\na simpler function like--",
    "start": "2629240",
    "end": "2637340"
  },
  {
    "text": "not a matrix function, but\njust a scalar function. So that's just sine of x. OK?",
    "start": "2637340",
    "end": "2642690"
  },
  {
    "text": "At x equals 1, all right,\nis it the exact derivative of just cosine of 1, right?",
    "start": "2642690",
    "end": "2648920"
  },
  {
    "text": "And so we can imagine our finite\ndifference approximation is just f of x plus dx. f is just\nsine minus fx over delta x",
    "start": "2648920",
    "end": "2657500"
  },
  {
    "text": "plus first-order terms, right? That's what happens\nwhen I take this formula and just divide it\nthrough by delta x, right?",
    "start": "2657500",
    "end": "2666079"
  },
  {
    "text": "So if dx equals 1e to the\nminus 5, I get 0.54 something.",
    "start": "2666080",
    "end": "2673010"
  },
  {
    "text": "The cosine of 1-- the exact\nanswer is 0.54 something. But you can see they differ\nin the third digit already.",
    "start": "2673010",
    "end": "2681470"
  },
  {
    "text": "The relative error is\nactually 10 to the minus 6. So it's actually\npretty good here.",
    "start": "2681470",
    "end": "2687085"
  },
  {
    "text": "Why is it 10 to the minus--\noh, because it's really not the third digit. It's .540298.",
    "start": "2687085",
    "end": "2692420"
  },
  {
    "text": "And this is \"302.\" So they really differ in almost,\nlike, the sixth digit here",
    "start": "2692420",
    "end": "2698120"
  },
  {
    "text": "versus there. It's-- right? And that's differing by 4\nin the sixth digit, right?",
    "start": "2698120",
    "end": "2704780"
  },
  {
    "text": "And so now what happens if\nyou do delta x equals 10 to the minus 100th, right?",
    "start": "2704780",
    "end": "2710150"
  },
  {
    "text": "Mathematically, that should\ngive you a much better answer. But actually, it\njust gives you 0.",
    "start": "2710150",
    "end": "2715880"
  },
  {
    "text": "And the computer is perfectly\ncapable of representing 10 to the minus 100th. So it's basically\nfloating-point.",
    "start": "2715880",
    "end": "2721550"
  },
  {
    "text": "It stores a fixed number of\ndigits and then an exponent.",
    "start": "2721550",
    "end": "2726780"
  },
  {
    "text": "You can think of it as\nlike scientific notation on the computer. So it can store really\nbig, really small numbers",
    "start": "2726780",
    "end": "2733070"
  },
  {
    "text": "just with a finite number of\nsignificant digits, right? But if you do 1 plus 10 to\nthe minus 100th, you get 1.",
    "start": "2733070",
    "end": "2741380"
  },
  {
    "text": "Because what would\nthe correct answer be? The correct answer would be\n1 plus 10 to the minus 100th is 1, a decimal point, 99\n0's, and then a 1, right?",
    "start": "2741380",
    "end": "2752119"
  },
  {
    "text": "And if the computer is\nonly storing 15 digits, it cannot store 1, 99\n0's, and then a 1, right?",
    "start": "2752120",
    "end": "2759710"
  },
  {
    "text": "And what it does is\nit takes this result. And effectively, it rounds\nit to the closest number",
    "start": "2759710",
    "end": "2767960"
  },
  {
    "text": "that it can represent\nwith 15 digits. And the closest number is 1.",
    "start": "2767960",
    "end": "2774020"
  },
  {
    "text": "So that's what it gives you. So even if you do something\nless extreme-- so if you do--",
    "start": "2774020",
    "end": "2779695"
  },
  {
    "text": "it's storing 15 digits. So if you do 10 to the\nminus 13th for your dx, then 1 plus dx it\ncan represent, right?",
    "start": "2779695",
    "end": "2788270"
  },
  {
    "text": "So then the computer can\nstore this number, right? But if you take sine of\n1 plus dx and sine of 1,",
    "start": "2788270",
    "end": "2797140"
  },
  {
    "text": "right, you can see\nthat they're now-- if dx is 10 to the\nminus 13th, they're",
    "start": "2797140",
    "end": "2802180"
  },
  {
    "text": "almost the same number-- this and this. See?",
    "start": "2802180",
    "end": "2807310"
  },
  {
    "text": "They only differ here, in the\nlast four significant digits.",
    "start": "2807310",
    "end": "2813370"
  },
  {
    "text": "So when you subtract them in a\nfinite difference approximation and take this minus this, almost\nall the significant digits",
    "start": "2813370",
    "end": "2820520"
  },
  {
    "text": "cancel.  You have 16 digits here--",
    "start": "2820520",
    "end": "2825800"
  },
  {
    "text": "15 digits. But 11 of them cancel. And you're just left with\nonly four digits of accuracy.",
    "start": "2825800",
    "end": "2835512"
  },
  {
    "text": "And so if you compute\nthe relative error, you get 10 to the minus 3 from\nthose four digits of accuracy.",
    "start": "2835512",
    "end": "2844260"
  },
  {
    "text": "So this is the general\nproblem with finite difference is that they're not\ncompletely bulletproof.",
    "start": "2844260",
    "end": "2851490"
  },
  {
    "text": "You need to make delta x small\nenough that the truncation error is small but not so small\nthat the roundoff errors kill",
    "start": "2851490",
    "end": "2859500"
  },
  {
    "text": "you, right? And kind of a rule\nof thumb is that it",
    "start": "2859500",
    "end": "2865470"
  },
  {
    "text": "works-- it's not perfect,\nlike any rule of thumb. No rule of thumb is\nperfectly reliable.",
    "start": "2865470",
    "end": "2870660"
  },
  {
    "text": "A good rule of thumb is to\nuse about half the digits for your difference. So more precisely, the\nnumber of significant digits",
    "start": "2870660",
    "end": "2877470"
  },
  {
    "text": "on the computer is\nrepresented by this quantity called the machine epsilon. So basically,\nepsilon x is the size",
    "start": "2877470",
    "end": "2885120"
  },
  {
    "text": "of the last significant digit. Or the next\nfloating-point number",
    "start": "2885120",
    "end": "2890380"
  },
  {
    "text": "out of x-- after x is\nx times 1 plus epsilon. So in Julia, there's a function\ncalled eps that returns this.",
    "start": "2890380",
    "end": "2897350"
  },
  {
    "text": "It's about 10 to the minus 16th. So as I said, it's about\n15 or 16 decimal digits.",
    "start": "2897350",
    "end": "2904450"
  },
  {
    "text": "The reason this is kind of\na weird-looking number-- it's not 1e to the minus 16th\nor 1e to the minus 15th--",
    "start": "2904450",
    "end": "2912220"
  },
  {
    "text": "is because the computer\nis actually not storing decimal digits. It's storing binary digits. So it's actually storing 1.000--",
    "start": "2912220",
    "end": "2920755"
  },
  {
    "text": "a whole bunch of digits\ntimes an exponent, which is a power of 2. And the digits are\nactually binary digits.",
    "start": "2920755",
    "end": "2927010"
  },
  {
    "text": "So this is not actually\n10 to the minus 15th. It's 2 to the minus 52.",
    "start": "2927010",
    "end": "2933520"
  },
  {
    "text": "But it's a crude approximation. You can think of it as\n15 decimal digits, right?",
    "start": "2933520",
    "end": "2939109"
  },
  {
    "text": "And so a crude rule of thumb\nfor finding differences-- it's kind of a good median--",
    "start": "2939110",
    "end": "2945650"
  },
  {
    "text": "for a small truncation error\nbut not big roundoff errors is to choose your delta\nx to be about half",
    "start": "2945650",
    "end": "2951050"
  },
  {
    "text": "the significant digits of x. So basically, the\nmagnitude of delta x is on the order of square\nroot of epsilon times x.",
    "start": "2951050",
    "end": "2957710"
  },
  {
    "text": "So square root of epsilon is\nabout 10 to the minus 8th. So that's why I chose\n10 to the minus 8th.",
    "start": "2957710",
    "end": "2964220"
  },
  {
    "text": "My matrix A was random\nnumbers on order one. So my matrix dA, I made them\nrandom numbers on order 10",
    "start": "2964220",
    "end": "2970700"
  },
  {
    "text": "to minus 8th. And that was-- if you go back\nup here, 10 to the minus 8th",
    "start": "2970700",
    "end": "2976790"
  },
  {
    "text": "is about where we see this\ncrossover point happening. It's about the point where\nthe truncation error is",
    "start": "2976790",
    "end": "2982760"
  },
  {
    "text": "balancing the roundoff error. And so these aren't perfect\nrules of thumb, but it's--",
    "start": "2982760",
    "end": "2988400"
  },
  {
    "text": "and if you have to pick a\ndelta x quickly, that's--",
    "start": "2988400",
    "end": "2995119"
  },
  {
    "text": "let's see. That's a good way to\nchoose that, right?",
    "start": "2995120",
    "end": "3002089"
  },
  {
    "text": "So-- ALAN EDELMAN: Steven, let me\npoint out that we're just about out of time. STEVEN JOHNSON: Yep. And I'll show this,\ntoo-- just the same thing for the sine plot.",
    "start": "3002090",
    "end": "3007730"
  },
  {
    "text": "So this is the same\nthing for difference. The dots are the\nrelative error compared to the exact derivative,\nwhich is cosine of 1.",
    "start": "3007730",
    "end": "3016010"
  },
  {
    "text": "And the line is just a\nstraight line for reference. You see exactly the same thing. It's, as delta x gets\nsmaller, initially",
    "start": "3016010",
    "end": "3023119"
  },
  {
    "text": "the error gets smaller because\nthe finite difference is getting better. And it goes down\nlinearly because this is",
    "start": "3023120",
    "end": "3029270"
  },
  {
    "text": "a first-order-accurate formula. But eventually, when\ndelta x gets too small,",
    "start": "3029270",
    "end": "3036740"
  },
  {
    "text": "the cancellation error, the\nroundoff error kills you. And the error gets worse. And the crossover point--\nagain, that rule of thumb--",
    "start": "3036740",
    "end": "3043070"
  },
  {
    "text": "works pretty well. It's about 10 to the minus 8th\nin here for this, since x is 1.",
    "start": "3043070",
    "end": "3048980"
  },
  {
    "text": "So I'll stop there. And we'll see you\nagain on Wednesday.",
    "start": "3048980",
    "end": "3056450"
  },
  {
    "text": "And I'll post this notebook. And we'll post-- I'll get Alan's updated\nnotebook from him.",
    "start": "3056450",
    "end": "3061640"
  },
  {
    "text": "And we'll post that on\nthe website as well. ALAN EDELMAN: Sounds good.",
    "start": "3061640",
    "end": "3066670"
  },
  {
    "start": "3066670",
    "end": "3070000"
  }
]