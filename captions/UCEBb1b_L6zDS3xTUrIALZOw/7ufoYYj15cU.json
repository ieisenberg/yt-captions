[
  {
    "start": "0",
    "end": "1069"
  },
  {
    "text": "In the next section we're going to start our\ndiscussion on how to actually engineer the",
    "start": "1069",
    "end": "5050"
  },
  {
    "text": "bit encodings we'll use in our circuitry,\nbut first we'll need a way to evaluate the",
    "start": "5050",
    "end": "9660"
  },
  {
    "text": "efficacy of an encoding.",
    "start": "9660",
    "end": "12120"
  },
  {
    "text": "The entropy of a random variable is average\namount of information received when learning",
    "start": "12120",
    "end": "16980"
  },
  {
    "text": "the value of the random variable.",
    "start": "16980",
    "end": "19500"
  },
  {
    "text": "The mathematician's name for \"average\" is\n\"expected value\"; that's what the capital",
    "start": "19500",
    "end": "23439"
  },
  {
    "text": "E means.",
    "start": "23439",
    "end": "25050"
  },
  {
    "text": "We compute the average in the obvious way:\nwe take the weighted sum, where the amount",
    "start": "25050",
    "end": "30560"
  },
  {
    "text": "of information received when learning of particular\nchoice i -- that's the log-base-2 of 1/p_i",
    "start": "30560",
    "end": "37329"
  },
  {
    "text": "-- is weighted by the probability of that\nchoice actually happening.",
    "start": "37329",
    "end": "41160"
  },
  {
    "text": "Here's an example.",
    "start": "41160",
    "end": "43489"
  },
  {
    "text": "We have a random variable that can take on\none of four values: A, B, C or D.",
    "start": "43489",
    "end": "49149"
  },
  {
    "text": "The probabilities of each choice are shown\nin the table, along with the associated information",
    "start": "49149",
    "end": "53739"
  },
  {
    "text": "content.",
    "start": "53739",
    "end": "56280"
  },
  {
    "text": "Now we'll compute the entropy using the probabilities\nand information content.",
    "start": "56280",
    "end": "60948"
  },
  {
    "text": "So we have the probability of A (1/3) times\nits associated information content (1.58 bits),",
    "start": "60949",
    "end": "66770"
  },
  {
    "text": "plus the probability of B times its associated\ninformation content, and so on.",
    "start": "66770",
    "end": "73360"
  },
  {
    "text": "The result is 1.626 bits.",
    "start": "73360",
    "end": "77079"
  },
  {
    "text": "This is telling us that a clever encoding\nscheme should be able to do better than simply",
    "start": "77079",
    "end": "81330"
  },
  {
    "text": "encoding each symbol using 2 bits to represent\nwhich of the four possible values is next.",
    "start": "81330",
    "end": "87100"
  },
  {
    "text": "Food for thought!",
    "start": "87100",
    "end": "89020"
  },
  {
    "text": "We'll discuss this further in the third section\nof this chapter.",
    "start": "89020",
    "end": "93398"
  },
  {
    "text": "So, what is the entropy telling us?",
    "start": "93399",
    "end": "97820"
  },
  {
    "text": "Suppose we have a sequence of data describing\na sequence of values of the random variable",
    "start": "97820",
    "end": "102490"
  },
  {
    "text": "X.",
    "start": "102490",
    "end": "103490"
  },
  {
    "text": "If, on the average, we use less than H(X)\nbits to transmit each piece of information",
    "start": "103490",
    "end": "109860"
  },
  {
    "text": "in the sequence, we will not be sending enough\ninformation to resolve the uncertainty about",
    "start": "109860",
    "end": "114340"
  },
  {
    "text": "the values.",
    "start": "114340",
    "end": "115840"
  },
  {
    "text": "In other words, the entropy is a lower bound\non the number of bits we need to transmit.",
    "start": "115840",
    "end": "121648"
  },
  {
    "text": "Getting less than this number of bits wouldn't\nbe good if the goal was to unambiguously describe",
    "start": "121649",
    "end": "126579"
  },
  {
    "text": "the sequence of values -- we'd have failed\nat our job!",
    "start": "126579",
    "end": "130970"
  },
  {
    "text": "On the other hand, if we send, on the average,\nmore than H(X) bits to describe the sequence",
    "start": "130970",
    "end": "136140"
  },
  {
    "text": "of values, we will not be making the most\neffective use of our resources, since the",
    "start": "136140",
    "end": "140970"
  },
  {
    "text": "same information might have been able to be\nrepresented with fewer bits.",
    "start": "140970",
    "end": "145050"
  },
  {
    "text": "This is okay, but perhaps with some insights\nwe could do better.",
    "start": "145050",
    "end": "149560"
  },
  {
    "text": "Finally, if we send, on the average, exactly\nH(X) bits, then we'd have the perfect encoding.",
    "start": "149560",
    "end": "155970"
  },
  {
    "text": "Alas, perfection is, as always, a tough goal,\nso most of the time we'll have to settle for",
    "start": "155970",
    "end": "160660"
  },
  {
    "text": "getting close.",
    "start": "160660",
    "end": "162040"
  },
  {
    "text": "In the final set of exercises for this section,\ntry computing the entropy for various scenarios.",
    "start": "162040",
    "end": "168299"
  }
]