[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6029"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6030",
    "end": "12680"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu. ",
    "start": "12680",
    "end": "30885"
  },
  {
    "text": "[MUSIC PLAYING] ",
    "start": "30885",
    "end": "37780"
  },
  {
    "text": "PATRICK H. WINSTON: Well,\nwhat we're going to do today is climb a pretty big\nmountain because we're going to go from a\nneural net with two",
    "start": "37780",
    "end": "43920"
  },
  {
    "text": "parameters to discussing\nthe kind of neural nets in which people end up dealing\nwith 60 million parameters.",
    "start": "43920",
    "end": "56589"
  },
  {
    "text": "So it's going to be\na pretty big jump. Along the way are\na couple things I wanted to underscore from\nour previous discussion.",
    "start": "56590",
    "end": "65750"
  },
  {
    "text": "Last time, I tried to\ndevelop some intuition for the kinds of formulas\nthat you use to actually do",
    "start": "65750",
    "end": "71600"
  },
  {
    "text": "the calculations in a\nsmall neural net about how the weights are going to change. And the main thing\nI tried to emphasize",
    "start": "71600",
    "end": "78230"
  },
  {
    "text": "is that when you have a\nneural net like this one,",
    "start": "78230",
    "end": "90420"
  },
  {
    "text": "everything is sort of\ndivided in each column.",
    "start": "90420",
    "end": "96610"
  },
  {
    "text": "You can't have the performance\nbased on this output",
    "start": "96610",
    "end": "102360"
  },
  {
    "text": "affect some weight\nchange back here without going through this\nfinite number of output",
    "start": "102360",
    "end": "109150"
  },
  {
    "text": "variables, the y1s. And by the way, there's no y2\nand y4-- there's no y2 and y3.",
    "start": "109150",
    "end": "117310"
  },
  {
    "text": "Dealing with this is really\na notational nightmare, and I spent a lot\nof time yesterday",
    "start": "117310",
    "end": "123890"
  },
  {
    "text": "trying to clean it\nup a little bit. But basically, what\nI'm trying to say has nothing to do with\nthe notation I have used",
    "start": "123890",
    "end": "129959"
  },
  {
    "text": "but rather with the\nfact that there's a limited number of ways in\nwhich that can influence this,",
    "start": "129959",
    "end": "135124"
  },
  {
    "text": "even though the number of\npaths through this network can be growing exponential. So those equations\nunderneath are",
    "start": "135124",
    "end": "142840"
  },
  {
    "text": "equations that derive\nfrom trying to figure out how the output performance\ndepends on some",
    "start": "142840",
    "end": "151690"
  },
  {
    "text": "of these weights back here. And what I've calculated\nis I've calculated the dependence of\nthe performance on w1",
    "start": "151690",
    "end": "161000"
  },
  {
    "text": "going that way, and\nI've also calculated the dependence of performance\non w1 going that way.",
    "start": "161000",
    "end": "172420"
  },
  {
    "text": "So that's one of the\nequations I've got down there. And another one\ndeals with w3, and it",
    "start": "172420",
    "end": "178830"
  },
  {
    "text": "involves going both\nthis way and this way.",
    "start": "178830",
    "end": "187060"
  },
  {
    "text": "And all I've done in both\ncases, in all four cases, is just take the partial\nderivative of performance",
    "start": "187060",
    "end": "193350"
  },
  {
    "text": "with respect to those weights\nand use the chain rule to expand it. And when I do that,\nthis is the stuff I get.",
    "start": "193350",
    "end": "205732"
  },
  {
    "text": "And that's just a whole\nbunch of partial derivatives. But if you look at it and let\nit sing a little bit to you, what you see is that\nthere's a lot of redundancy",
    "start": "205732",
    "end": "212508"
  },
  {
    "text": "in the computation. So for example, this\nguy here, partial",
    "start": "212509",
    "end": "218990"
  },
  {
    "text": "of performance\nwith respect to w1, depends on both\npaths, of course.",
    "start": "218990",
    "end": "225350"
  },
  {
    "text": "But look at the first elements\nhere, these guys right here.",
    "start": "225350",
    "end": "231150"
  },
  {
    "text": "And look at the first\nelements in the expression for calculating the partial\nderivative of performance",
    "start": "231150",
    "end": "236210"
  },
  {
    "text": "with respect to w3, these guys. ",
    "start": "236210",
    "end": "244612"
  },
  {
    "text": "They're the same.  And not only that, if you\nlook inside these expressions",
    "start": "244612",
    "end": "252310"
  },
  {
    "text": "and look at this\nparticular piece here, you see that that is\nan expression that",
    "start": "252310",
    "end": "258810"
  },
  {
    "text": "was needed in order\nto calculate one of the downstream weights,\nthe changes in one",
    "start": "258810",
    "end": "264710"
  },
  {
    "text": "of the downstream weights. But it happens to be the same\nthing as you see over here.",
    "start": "264710",
    "end": "270070"
  },
  {
    "text": " And likewise, this piece is the\nsame thing you see over here.",
    "start": "270070",
    "end": "281532"
  },
  {
    "text": " So each time you move\nfurther and further back",
    "start": "281532",
    "end": "287180"
  },
  {
    "text": "from the outputs\ntoward the inputs, you're reusing a\nlot of computation that you've already done.",
    "start": "287180",
    "end": "294180"
  },
  {
    "text": "So I'm trying to find a\nway to sloganize this, and what I've come up with is\nwhat's done is done and cannot",
    "start": "294180",
    "end": "302280"
  },
  {
    "text": "be-- no, no. That's not quite right, is it? It's what's computed is computed\nand need not be recomputed.",
    "start": "302280",
    "end": "310090"
  },
  {
    "text": "OK? So that's what's going on here. And that's why this is\na calculation that's",
    "start": "310090",
    "end": "316180"
  },
  {
    "text": "linear in the depths of the\nneural net, not exponential.",
    "start": "316180",
    "end": "322074"
  },
  {
    "text": "There's another thing I wanted\nto point out in connection with these neural nets.",
    "start": "322075",
    "end": "328900"
  },
  {
    "text": "And that has to do\nwith what happens when we look at a single neuron\nand note that what we've got",
    "start": "328900",
    "end": "334870"
  },
  {
    "text": "is we've got a bunch of\nweights that you multiply times a bunch of inputs like so. ",
    "start": "334870",
    "end": "346960"
  },
  {
    "text": "And then those are all\nsummed up in a summing box before they enter some kind\nof non-linearity, in our case",
    "start": "346960",
    "end": "357920"
  },
  {
    "text": "a sigmoid function. But if I ask you to write down\nthe expression for the value",
    "start": "357920",
    "end": "365890"
  },
  {
    "text": "we've got there, what is it? Well, it's just the sum\nof the w's times the x's.",
    "start": "365890",
    "end": "373072"
  },
  {
    "text": " What's that? ",
    "start": "373072",
    "end": "380590"
  },
  {
    "text": "That's the dot product. Remember a few lectures\nago I said that some of us believe that the dot product is\na fundamental calculation that",
    "start": "380590",
    "end": "388690"
  },
  {
    "text": "takes place in our heads? So this is why we think so.",
    "start": "388690",
    "end": "393790"
  },
  {
    "text": "If neural nets are doing\nanything like this, then there's a dot product\nbetween some weights",
    "start": "393790",
    "end": "399200"
  },
  {
    "text": "and some input values. Now, it's a funny\nkind of dot product because in the models\nthat we've been using,",
    "start": "399200",
    "end": "407410"
  },
  {
    "text": "these input variables are\nall or none, or 0 or 1. But that's OK.",
    "start": "407410",
    "end": "412500"
  },
  {
    "text": "I have it on good\nauthority that there are neurons in our head\nfor which the values that",
    "start": "412500",
    "end": "418090"
  },
  {
    "text": "are produced are not\nexactly all or none but rather have a kind of\nproportionality to them.",
    "start": "418090",
    "end": "423950"
  },
  {
    "text": "So you get a real dot product\ntype of operation out of that. So that's by way of\na couple of asides",
    "start": "423950",
    "end": "429510"
  },
  {
    "text": "that I wanted to\nunderscore before we get into the center\nof today's discussion,",
    "start": "429510",
    "end": "436360"
  },
  {
    "text": "which will be to talk about\nthe so-called deep nets. Now, let's see,\nwhat's a deep net do?",
    "start": "436360",
    "end": "443889"
  },
  {
    "text": "Well, from last time, you\nknow that a deep net does",
    "start": "443890",
    "end": "449820"
  },
  {
    "text": "that sort of thing, and\nit's interesting to look at some of the offerings here.",
    "start": "449820",
    "end": "456470"
  },
  {
    "text": "By the way, how good was\nthis performance in 2012? Well, it turned out\nthat the fraction",
    "start": "456470",
    "end": "464630"
  },
  {
    "text": "of the time that the\nsystem had the right answer in its top five\nchoices was about 15%.",
    "start": "464630",
    "end": "472690"
  },
  {
    "text": "And the fraction of the time\nthat it got exactly the right answer as its top pick\nwas about 37%-- error,",
    "start": "472690",
    "end": "479900"
  },
  {
    "text": "15% error if you count it as\nan error if it's-- what am I",
    "start": "479900",
    "end": "485740"
  },
  {
    "text": "saying? You got it right if you\ngot it in the top five. An error rate on that\ncalculation, about 15%.",
    "start": "485740",
    "end": "492660"
  },
  {
    "text": "If you say you only get it right\nif it was your top choice, then the error rate was about 37%.",
    "start": "492660",
    "end": "498830"
  },
  {
    "text": "So pretty good, especially\nsince some of these things are highly ambiguous even to us.",
    "start": "498830",
    "end": "504602"
  },
  {
    "text": "And what kind of\na system did that? Well, it wasn't one\nthat looked exactly",
    "start": "504602",
    "end": "510610"
  },
  {
    "text": "like that, although that\nis the essence of it. The system actually\nlooked like that.",
    "start": "510610",
    "end": "516447"
  },
  {
    "text": "There's quite a lot\nof stuff in there. And what I'm going to talk about\nis not exactly this system, but I'm going to talk about the\nstuff of which such systems are",
    "start": "516447",
    "end": "524530"
  },
  {
    "text": "made because there's\nnothing particularly special about this. It just happens to be\na particular assembly",
    "start": "524530",
    "end": "530990"
  },
  {
    "text": "of components that tend to\nreappear when anyone does this sort of neural net stuff.",
    "start": "530990",
    "end": "536910"
  },
  {
    "text": "So let me explain that this way. First thing I need to talk\nabout is the concept of-- well,",
    "start": "536910",
    "end": "545110"
  },
  {
    "text": "I don't like the term. It's called convolution. I don't like the term because\nin the second-best course",
    "start": "545110",
    "end": "551290"
  },
  {
    "text": "at the Institute,\nSignals and Systems, you learn about impulse\nresponses and convolution integrals and stuff like that.",
    "start": "551290",
    "end": "556810"
  },
  {
    "text": "And this hints at that,\nbut it's not the same thing because there's no memory\ninvolved in what's going on",
    "start": "556810",
    "end": "562930"
  },
  {
    "text": "as these signals are processed. But they call it convolutional\nneural nets anyway. So here you are.",
    "start": "562930",
    "end": "568140"
  },
  {
    "text": "You got some kind of image.  And even with lots of computing\npower and GPUs and all",
    "start": "568140",
    "end": "577620"
  },
  {
    "text": "that sort of stuff, we're\nnot talking about images with 4 million pixels.",
    "start": "577620",
    "end": "582629"
  },
  {
    "text": "We're talking about images\nthat might be 256 on a side. ",
    "start": "582630",
    "end": "593284"
  },
  {
    "text": "As I say, we're not\ntalking about images that are 1,000 by 1,000 or 4,000\nby 4,000 or anything like that. They tend to be\nkind of compressed",
    "start": "593284",
    "end": "600940"
  },
  {
    "text": "into a 256-by-256 image. And now what we do\nis we run over this",
    "start": "600940",
    "end": "609180"
  },
  {
    "text": "with a neuron that\nis looking only at a 10-by-10 square like so,\nand that produces an output.",
    "start": "609180",
    "end": "622980"
  },
  {
    "text": "And next, we went\nover that again having shifted this neuron\na little bit like so.",
    "start": "622980",
    "end": "632270"
  },
  {
    "text": "And then the next thing we do\nis we shift it again, so we get that output right there.",
    "start": "632270",
    "end": "639870"
  },
  {
    "text": "So each of those deployments\nof a neuron produces an output,",
    "start": "639870",
    "end": "646190"
  },
  {
    "text": "and that output is associated\nwith a particular place in the image. This is the process that\nis called convolution",
    "start": "646190",
    "end": "658060"
  },
  {
    "text": "as a term of art. Now, this guy, or this\nconvolution operation,",
    "start": "658060",
    "end": "664160"
  },
  {
    "text": "results in a bunch\nof points over here. ",
    "start": "664160",
    "end": "672384"
  },
  {
    "text": "And the next thing that\nwe do with those points is we look in\nlocal neighborhoods",
    "start": "672384",
    "end": "679209"
  },
  {
    "text": "and see what the\nmaximum value is. And then we take\nthat maximum value",
    "start": "679210",
    "end": "684650"
  },
  {
    "text": "and construct yet another\nmapping of the image over here using\nthat maximum value.",
    "start": "684650",
    "end": "691030"
  },
  {
    "text": "Then we slide that over like so,\nand we produce another value.",
    "start": "691030",
    "end": "696470"
  },
  {
    "text": "And then we slide\nthat over one more time with a different\ncolor, and now we've",
    "start": "696470",
    "end": "704400"
  },
  {
    "text": "got yet another value. So this process\nis called pooling. ",
    "start": "704400",
    "end": "714969"
  },
  {
    "text": "And because we're\ntaking the maximum, this particular kind of\npooling is called max pooling.",
    "start": "714969",
    "end": "720960"
  },
  {
    "text": "So now let's see what's next. This is taking a\nparticular neuron and running it across the image.",
    "start": "720960",
    "end": "728870"
  },
  {
    "text": "We call that a kernel, again\nsucking some terminology out of Signals and Systems.",
    "start": "728870",
    "end": "734136"
  },
  {
    "text": "But now what we're\ngoing to do is we're going to say we could\nuse a whole bunch of kernels.",
    "start": "734136",
    "end": "739140"
  },
  {
    "text": "So the thing that I\nproduce with one kernel can now be repeated\nmany times like so.",
    "start": "739140",
    "end": "747430"
  },
  {
    "text": "In fact, a typical\nnumber is 100 times. So now what we've got is\nwe've got a 256-by-256 image.",
    "start": "747430",
    "end": "754440"
  },
  {
    "text": "We've gone over it\nwith a 10-by-10 kernel. We have taken the\nmaximum values that",
    "start": "754440",
    "end": "761760"
  },
  {
    "text": "are in the vicinity\nof each other, and then we repeated\nthat 100 times.",
    "start": "761760",
    "end": "768270"
  },
  {
    "text": "So now we can take that, and\nwe can feed all those results",
    "start": "768270",
    "end": "773320"
  },
  {
    "text": "into some kind of neural net. And then we can, through\nperhaps a fully-connected job",
    "start": "773320",
    "end": "779209"
  },
  {
    "text": "on the final layers of this, and\nthen in the ultimate output we get some sort of\nindication of how likely it",
    "start": "779210",
    "end": "787770"
  },
  {
    "text": "is that the thing that's\nbeing seen is, say, a mite. ",
    "start": "787770",
    "end": "795300"
  },
  {
    "text": "So that's roughly how\nthese things work.",
    "start": "795300",
    "end": "800850"
  },
  {
    "text": "So what have we\ntalked about so far? We've talked about pooling, and\nwe've talked about convolution.",
    "start": "800850",
    "end": "807780"
  },
  {
    "text": "And now we can talk about\nsome of the good stuff. But before I get into that,\nthis is what we can do now,",
    "start": "807780",
    "end": "815175"
  },
  {
    "text": "and you can compare this with\nwhat was done in the old days. ",
    "start": "815175",
    "end": "822630"
  },
  {
    "text": "It was done in the old\ndays before massive amounts of computing became available\nis a kind of neural net activity",
    "start": "822630",
    "end": "836070"
  },
  {
    "text": "that's a little easier to see. You might, in the old days,\nonly have enough computing power",
    "start": "836070",
    "end": "843089"
  },
  {
    "text": "to deal with a small\ngrid of picture elements, or so-called pixels. And then each of these might be\na value that is fed as an input",
    "start": "843090",
    "end": "852889"
  },
  {
    "text": "into some kind of neuron. And so you might have a column\nof neurons that are looking",
    "start": "852890",
    "end": "859110"
  },
  {
    "text": "at these pixels in your image. And then there might be\na small number of columns",
    "start": "859110",
    "end": "866410"
  },
  {
    "text": "that follow from that. And finally, something\nthat says this neuron is looking for things that are\na number 1, that is to say,",
    "start": "866410",
    "end": "875820"
  },
  {
    "text": "something that looks like\na number 1 in the image.",
    "start": "875820",
    "end": "883330"
  },
  {
    "text": "So this stuff up\nhere is what you can do when you have a\nmassive amount of computation",
    "start": "883330",
    "end": "888597"
  },
  {
    "text": "relative to the\nkind of thing you used to see in the old days. ",
    "start": "888597",
    "end": "895399"
  },
  {
    "text": "So what's different? Well, what's\ndifferent is instead of a few hundred parameters,\nwe've got a lot more.",
    "start": "895400",
    "end": "902730"
  },
  {
    "text": "Instead of 10 digits,\nwe have 1,000 classes. Instead of a few\nhundred samples,",
    "start": "902730",
    "end": "909130"
  },
  {
    "text": "we have maybe 1,000\nexamples of each class. So that makes a million samples.",
    "start": "909130",
    "end": "916270"
  },
  {
    "text": "And we got 60 million\nparameters to play with. And the surprising thing\nis that the net result",
    "start": "916270",
    "end": "923060"
  },
  {
    "text": "is we've got a function\napproximator that astonishes everybody.",
    "start": "923060",
    "end": "928380"
  },
  {
    "text": "And no one quite\nknows why it works, except that when you throw an\nimmense amount of computation",
    "start": "928380",
    "end": "934230"
  },
  {
    "text": "into this kind of\narrangement, it's possible to get a performance\nthat no one expected would",
    "start": "934230",
    "end": "942769"
  },
  {
    "text": "be possible. So that's sort of\nthe bottom line. But now there are a couple of\nideas beyond that that I think",
    "start": "942770",
    "end": "951240"
  },
  {
    "text": "are especially interesting,\nand I want to talk about those. First idea that's\nespecially interesting",
    "start": "951240",
    "end": "958690"
  },
  {
    "text": "is the idea of\nautocoding, and here's how the idea of\nautocoding works. ",
    "start": "958690",
    "end": "969190"
  },
  {
    "text": "I'm going to run\nout of board space, so I think I'll\ndo it right here.",
    "start": "969190",
    "end": "974230"
  },
  {
    "text": "You have some input values. ",
    "start": "974230",
    "end": "984860"
  },
  {
    "text": "They go into a layer of\nneurons, the input layer. Then there is a so-called hidden\nlayer that's much smaller.",
    "start": "984860",
    "end": "993181"
  },
  {
    "text": " So maybe in the example,\nthere will be 10 neurons here",
    "start": "993181",
    "end": "999830"
  },
  {
    "text": "and just a couple here. And then these expand to\nan output layer like so.",
    "start": "999830",
    "end": "1007750"
  },
  {
    "text": "Now we can take the output\nlayer, z1 through zn,",
    "start": "1007750",
    "end": "1013180"
  },
  {
    "text": "and compare it with the\ndesired values, d1 through dn.",
    "start": "1013180",
    "end": "1019029"
  },
  {
    "text": " You following me so far? Now, the trick is to say, well,\nwhat are the desired values?",
    "start": "1019030",
    "end": "1029020"
  },
  {
    "text": "Let's let the desired\nvalues be the input values. ",
    "start": "1029020",
    "end": "1037254"
  },
  {
    "text": "So what we're going\nto do is we're going to train this net\nup so that the output's the same as the input.",
    "start": "1037254",
    "end": "1043600"
  },
  {
    "text": "What's the good of that? Well, we're going to\nforce it down through this [? neck-down ?]\npiece of network.",
    "start": "1043600",
    "end": "1050270"
  },
  {
    "text": "So if this network\nis going to succeed in taking all the possibilities\nhere and cramming them",
    "start": "1050270",
    "end": "1057160"
  },
  {
    "text": "into this smaller inner layer,\nthe so-called hidden layer,",
    "start": "1057160",
    "end": "1062720"
  },
  {
    "text": "such that it can reproduce\nthe input [? at ?] the output, it must be doing some\nkind of generalization",
    "start": "1062720",
    "end": "1068300"
  },
  {
    "text": "of the kinds of things\nit sees on its input. And that's a very clever idea,\nand it's seen in various forms",
    "start": "1068300",
    "end": "1076830"
  },
  {
    "text": "in a large fraction\nof the papers that appear on deep neural nets.",
    "start": "1076830",
    "end": "1083340"
  },
  {
    "text": "But now I want to\ntalk about an example so I can show you\na demonstration. OK? So we don't have GPUs, and\nwe don't have three days",
    "start": "1083340",
    "end": "1091410"
  },
  {
    "text": "to do this. So I'm going to make up a\nvery simple example that's",
    "start": "1091410",
    "end": "1097520"
  },
  {
    "text": "reminiscent of what goes\non here but involves hardly any computation.",
    "start": "1097520",
    "end": "1102974"
  },
  {
    "text": "What I'm going to\nimagine is we're trying to recognize\nanimals from how tall they",
    "start": "1102974",
    "end": "1111260"
  },
  {
    "text": "are from the shadows\nthat they cast. So we're going to recognize\nthree animals, a cheetah,",
    "start": "1111260",
    "end": "1123940"
  },
  {
    "text": "a zebra, and a giraffe, and\nthey will each cast a shadow",
    "start": "1123940",
    "end": "1131059"
  },
  {
    "text": "on the blackboard like me. No vampire involved here.",
    "start": "1131060",
    "end": "1137012"
  },
  {
    "text": "And what we're\ngoing to do is we're going to use the shadow as\nan input to a neural net.",
    "start": "1137012",
    "end": "1143289"
  },
  {
    "text": "All right? So let's see how\nthat would work. ",
    "start": "1143290",
    "end": "1159350"
  },
  {
    "text": "So there is our network. And if I just clicked into\none of these test samples,",
    "start": "1159350",
    "end": "1166480"
  },
  {
    "text": "that's the height of the shadow\nthat a cheetah casts on a wall.",
    "start": "1166480",
    "end": "1172010"
  },
  {
    "text": "And there are 10 input\nneurons corresponding to each level of the shadow.",
    "start": "1172010",
    "end": "1177500"
  },
  {
    "text": "They're rammed through\nthree inner layer neurons, and from that it spreads out and\nbecomes the outer layer values.",
    "start": "1177500",
    "end": "1186167"
  },
  {
    "text": "And we're going to\ncompare those outer layer values to the desired values,\nbut the desired values are the same as\nthe input values.",
    "start": "1186167",
    "end": "1192640"
  },
  {
    "text": "So this column is a\ncolumn of input values. On the far right, we have\nour column of desired values.",
    "start": "1192640",
    "end": "1198419"
  },
  {
    "text": "And we haven't trained\nthis neural net yet. All we've got is\nrandom values in there. So if we run the test samples\nthrough, we get that and that.",
    "start": "1198420",
    "end": "1208140"
  },
  {
    "text": "Yeah, cheetahs are short,\nzebras are medium height, and giraffes are tall. But our output is just pretty\nmuch 0.5 for all of them,",
    "start": "1208140",
    "end": "1219630"
  },
  {
    "text": "for all of those shadow\nheights, all right, [? with ?] no training so far. So let's run this thing.",
    "start": "1219630",
    "end": "1225270"
  },
  {
    "text": "We're just using simple\n[? backdrop, ?] just like on our world's simplest neural net.",
    "start": "1225270",
    "end": "1230570"
  },
  {
    "text": "And it's interesting\nto see what happens.",
    "start": "1230570",
    "end": "1236700"
  },
  {
    "text": "You see all those\nvalues changing? Now, I need to mention that\nwhen you see a green connection,",
    "start": "1236700",
    "end": "1242010"
  },
  {
    "text": "that means it's a\npositive weight, and the density of the green\nindicates how positive it is.",
    "start": "1242010",
    "end": "1249870"
  },
  {
    "text": "And the red ones are\nnegative weights, and the intensity of the\nred indicates how red it is.",
    "start": "1249870",
    "end": "1255000"
  },
  {
    "text": "So here you can\nsee that we still have from our random\ninputs a variety of red and green values.",
    "start": "1255000",
    "end": "1260950"
  },
  {
    "text": "We haven't really\ndone much training, so everything correctly\nlooks pretty much random.",
    "start": "1260950",
    "end": "1266840"
  },
  {
    "text": "So let's run this thing. And after only 1,000 iterations\ngoing through these examples",
    "start": "1266840",
    "end": "1277244"
  },
  {
    "text": "and trying to make the\noutput the same as the input, we reached a point where\nthe error rate has dropped. In fact, it's\ndropped so much it's",
    "start": "1277244",
    "end": "1283590"
  },
  {
    "text": "interesting to relook\nat the test cases. So here's a test case\nwhere we have a cheetah.",
    "start": "1283590",
    "end": "1289150"
  },
  {
    "text": "And now the output\nvalue is, in fact, very close to the desired value\nin all the output neurons.",
    "start": "1289150",
    "end": "1297740"
  },
  {
    "text": "So if we look at\nanother one, once again, there's a correspondence\nin the right two columns.",
    "start": "1297740",
    "end": "1303780"
  },
  {
    "text": "And if we look at the\nfinal one, yeah, there's a correspondence in\nthe right two columns. ",
    "start": "1303780",
    "end": "1310980"
  },
  {
    "text": "Now, you back up from\nthis and say, well, what's going on here? It turns out that you're\nnot training this thing",
    "start": "1310980",
    "end": "1320320"
  },
  {
    "text": "to classify animals. You're training it to understand\nthe nature of the things",
    "start": "1320320",
    "end": "1325590"
  },
  {
    "text": "that it sees in the\nenvironment because all it sees is the height of a shadow. It doesn't know anything\nabout the classifications",
    "start": "1325590",
    "end": "1332613"
  },
  {
    "text": "you're going to try\nto get out of that. All it sees is that there's\na kind of consistency in the kind of data that it\nsees on the input values.",
    "start": "1332613",
    "end": "1341630"
  },
  {
    "text": "Right? Now, you might say,\nOK, oh, that's cool, because what must\nbe happening is that that hidden layer,\nbecause everything is forced",
    "start": "1341630",
    "end": "1349730"
  },
  {
    "text": "through that narrow\npipe, must be doing some kind of generalization.",
    "start": "1349730",
    "end": "1355229"
  },
  {
    "text": "So it ought to be the\ncase that if we click on each of those\nneurons, we ought to see it specialize\nto a particular height,",
    "start": "1355229",
    "end": "1360820"
  },
  {
    "text": "because that's the sort of stuff\nthat's presented on the input.",
    "start": "1360820",
    "end": "1366289"
  },
  {
    "text": "Well, let's go see\nwhat, in fact, is the maximum\nstimulation to be seen",
    "start": "1366290",
    "end": "1372980"
  },
  {
    "text": "on the neurons in\nthat hidden layer. So when I click on these\nguys, what we're going to see",
    "start": "1372980",
    "end": "1379860"
  },
  {
    "text": "is the input values\nthat maximally stimulate that neuron.",
    "start": "1379860",
    "end": "1385174"
  },
  {
    "text": "And by the way, I\nhave no idea how this is going to turn out\nbecause the initialization's all random.",
    "start": "1385174",
    "end": "1391580"
  },
  {
    "text": "Well, that's good. That one looks like\nit's generalized the notion of short.",
    "start": "1391580",
    "end": "1396920"
  },
  {
    "text": "Ugh, that doesn't\nlook like medium. And in fact, the\nmaximum stimulation",
    "start": "1396920",
    "end": "1404669"
  },
  {
    "text": "doesn't involve any stimulation\nfrom that lower neuron. Here, look at this one.",
    "start": "1404670",
    "end": "1411169"
  },
  {
    "text": "That doesn't look like tall. So we got one that looks\nlike short and two that just look completely random.",
    "start": "1411170",
    "end": "1417505"
  },
  {
    "text": " So in fact, maybe we\nbetter back off the idea",
    "start": "1417505",
    "end": "1422509"
  },
  {
    "text": "that what's going on\nin that hidden layer is generalization\nand say that what",
    "start": "1422510",
    "end": "1428850"
  },
  {
    "text": "is going on in there\nis maybe the encoding of a generalization. It doesn't look like\nan encoding we can see,",
    "start": "1428850",
    "end": "1436190"
  },
  {
    "text": "but there is a generalization\nthat's-- let me start that over.",
    "start": "1436190",
    "end": "1441820"
  },
  {
    "text": "We don't see the generalization\nin the stimulating values.",
    "start": "1441820",
    "end": "1448311"
  },
  {
    "text": "What we have instead\nis we have some kind of encoded generalization. And because we got\nthis stuff encoded,",
    "start": "1448312",
    "end": "1454150"
  },
  {
    "text": "it's what makes these neural\nnets so extraordinarily difficult to understand. We don't understand\nwhat they're doing.",
    "start": "1454150",
    "end": "1460610"
  },
  {
    "text": "We don't understand why they\ncan recognize a cheetah. We don't understand why\nit can recognize a school bus in some cases,\nbut not in others,",
    "start": "1460610",
    "end": "1467070"
  },
  {
    "text": "because we don't\nreally understand what these neurons\nare responding to.",
    "start": "1467070",
    "end": "1472530"
  },
  {
    "text": "Well, that's not quite true. There's been a lot\nof work recently on trying to sort that\nout, but it's still",
    "start": "1472530",
    "end": "1478690"
  },
  {
    "text": "a lot of mystery in this world. In any event, that's\nthe autocoding idea.",
    "start": "1478690",
    "end": "1484700"
  },
  {
    "text": "It comes in various guises. Sometimes people talk about\nBoltzmann machines and things of that sort. But it's basically all\nthe same sort of idea.",
    "start": "1484700",
    "end": "1491230"
  },
  {
    "text": "And so what you can\ndo is layer by layer. Once you've trained\nthe input layer, then you can use that layer\nto train the next layer,",
    "start": "1491230",
    "end": "1497810"
  },
  {
    "text": "and then that can train\nthe next layer after that. And it's only at the very, very\nend that you say to yourself,",
    "start": "1497810",
    "end": "1504360"
  },
  {
    "text": "well, now I've accumulated\na lot of knowledge about the environment and what\ncan be seen in the environment.",
    "start": "1504360",
    "end": "1510220"
  },
  {
    "text": "Maybe it's time to\nget around to using some samples of particular\nclasses and train on classes.",
    "start": "1510220",
    "end": "1517770"
  },
  {
    "text": "So that's the story\non autocoding. ",
    "start": "1517770",
    "end": "1522779"
  },
  {
    "text": "Now, the next thing to talk\nabout is that final layer. ",
    "start": "1522780",
    "end": "1529660"
  },
  {
    "text": "So let's see what the final\nlayer might look like. ",
    "start": "1529660",
    "end": "1535110"
  },
  {
    "text": "Let's see, it might\nlook like this. There's a [? summer. ?]\nThere's a minus 1 up here.",
    "start": "1535110",
    "end": "1544936"
  },
  {
    "text": "No. Let's see, there's a\nminus 1 up-- [INAUDIBLE]. ",
    "start": "1544937",
    "end": "1550710"
  },
  {
    "text": "There's a minus 1 up there. There's a multiplier here. And there's a\nthreshold value there.",
    "start": "1550710",
    "end": "1558091"
  },
  {
    "text": "Now, likewise, there's some\nother input values here. Let me call this one x, and it\ngets multiplied by some weight.",
    "start": "1558091",
    "end": "1567690"
  },
  {
    "text": "And then that goes into\nthe [? summer ?] as well. And that, in turn, goes into\na sigmoid that looks like so.",
    "start": "1567690",
    "end": "1579540"
  },
  {
    "text": "And finally, you get an\noutput, which we'll z.",
    "start": "1579540",
    "end": "1585180"
  },
  {
    "text": "So it's clear that if you\njust write out the value of z as it depends on those inputs\nusing the formula that we",
    "start": "1585180",
    "end": "1596020"
  },
  {
    "text": "worked with last\ntime, then what you see is that z is\nequal to 1 over 1",
    "start": "1596020",
    "end": "1608280"
  },
  {
    "text": "plus e to the minus w times\nx minus T-- plus T, I guess.",
    "start": "1608280",
    "end": "1621218"
  },
  {
    "text": "Right?  So that's a sigmoid\nfunction that",
    "start": "1621218",
    "end": "1628190"
  },
  {
    "text": "depends on the\nvalue of that weight and on the value\nof that threshold.",
    "start": "1628190",
    "end": "1633970"
  },
  {
    "text": "So let's look at how those\nvalues might change things.",
    "start": "1633970",
    "end": "1640510"
  },
  {
    "text": "So here we have an\nordinary sigmoid. ",
    "start": "1640510",
    "end": "1646549"
  },
  {
    "text": "And what happens if we shift\nit with a threshold value? If we change that\nthreshold value,",
    "start": "1646550",
    "end": "1654520"
  },
  {
    "text": "then it's going\nto shift the place where that sigmoid comes down.",
    "start": "1654520",
    "end": "1661948"
  },
  {
    "text": " So a change in T\ncould cause this thing",
    "start": "1661948",
    "end": "1667860"
  },
  {
    "text": "to shift over that way. And if we change\nthe value of w, that",
    "start": "1667860",
    "end": "1673010"
  },
  {
    "text": "could change how\nsteep this guy is. ",
    "start": "1673010",
    "end": "1678460"
  },
  {
    "text": "So we might think that the\nperformance, since it depends on w and T, should be\nadjusted in such a way",
    "start": "1678460",
    "end": "1688820"
  },
  {
    "text": "as to make the classification\ndo the right thing.",
    "start": "1688820",
    "end": "1694470"
  },
  {
    "text": "But what's the right thing? Well, that depends on the\nsamples that we've seen. ",
    "start": "1694470",
    "end": "1704490"
  },
  {
    "text": "Suppose, for example, that\nthis is our sigmoid function. ",
    "start": "1704490",
    "end": "1711330"
  },
  {
    "text": "And we see some examples of a\nclass, some positive examples",
    "start": "1711330",
    "end": "1717380"
  },
  {
    "text": "of a class, that\nhave values that lie at that point and\nthat point and that point.",
    "start": "1717380",
    "end": "1726800"
  },
  {
    "text": "And we have some values that\ncorrespond to situations where",
    "start": "1726800",
    "end": "1734180"
  },
  {
    "text": "the class is not one of the\nthings that are associated with this neuron. And in that case, what\nwe see is examples that",
    "start": "1734180",
    "end": "1741929"
  },
  {
    "text": "are over in this vicinity here.  So the probability that we\nwould see this particular guy",
    "start": "1741930",
    "end": "1750710"
  },
  {
    "text": "in this world is associated with\nthe value on the sigmoid curve. So you could think of\nthis as the probability",
    "start": "1750710",
    "end": "1757420"
  },
  {
    "text": "of that positive\nexample, and this is the probability of\nthat positive example, and this is the probability\nof that positive example.",
    "start": "1757420",
    "end": "1765020"
  },
  {
    "text": "What's the probability\nof this negative example? Well, it's 1 minus the\nvalue on that curve.",
    "start": "1765020",
    "end": "1772480"
  },
  {
    "text": "And this one's 1 minus\nthe value on that curve. So we could go through\nthe calculations.",
    "start": "1772480",
    "end": "1779510"
  },
  {
    "text": "And what we would determine\nis that to maximize the probability of seeing this\ndata, this particular stuff",
    "start": "1779510",
    "end": "1786790"
  },
  {
    "text": "in a set of experiments, to\nmaximize that probability, we would have to adjust T and\nw so as to get this curve doing",
    "start": "1786790",
    "end": "1795860"
  },
  {
    "text": "the optimal thing. And there's nothing\nmysterious about it. It's just more\npartial derivatives",
    "start": "1795860",
    "end": "1801050"
  },
  {
    "text": "and that sort of thing. But the bottom line is that the\nprobability of seeing this data",
    "start": "1801050",
    "end": "1809720"
  },
  {
    "text": "is dependent on the\nshape of this curve, and the shape of this curve is\ndependent on those parameters.",
    "start": "1809720",
    "end": "1816539"
  },
  {
    "text": "And if we wanted to maximize\nthe probability that we've seen this data, then we have\nto adjust those parameters",
    "start": "1816540",
    "end": "1823058"
  },
  {
    "text": "accordingly.  Let's have a look\nat a demonstration. ",
    "start": "1823058",
    "end": "1839770"
  },
  {
    "text": "OK. So there's an ordinary\nsigmoid curve. Here are a couple of\npositive examples.",
    "start": "1839770",
    "end": "1846850"
  },
  {
    "text": "Here's a negative example. ",
    "start": "1846850",
    "end": "1853500"
  },
  {
    "text": "Let's put in some more\npositive examples over here. And now let's run the good,\nold gradient ascent algorithm",
    "start": "1853500",
    "end": "1864670"
  },
  {
    "text": "on that. And this is what happens. You've seen how the\nprobability, as we adjust",
    "start": "1864670",
    "end": "1871640"
  },
  {
    "text": "the shape of the curve,\nthe probability of seeing those examples of\nthe class goes up,",
    "start": "1871640",
    "end": "1878059"
  },
  {
    "text": "and the probability of seeing\nthe non-example goes down. So what if we put\nsome more examples in?",
    "start": "1878060",
    "end": "1886110"
  },
  {
    "text": "If we put a negative\nexample there, not much is going to happen. What would happen if we put a\npositive example right there?",
    "start": "1886110",
    "end": "1893762"
  },
  {
    "text": "Then we're going to start\nseeing some dramatic shifts in the shape of the curve. ",
    "start": "1893762",
    "end": "1908000"
  },
  {
    "text": "So that's probably\na noise point. But we can put some more\nnegative examples in there",
    "start": "1908000",
    "end": "1914750"
  },
  {
    "text": "and see how that\nadjusts the curve.  All right.",
    "start": "1914750",
    "end": "1920010"
  },
  {
    "text": "So that's what we're doing. We're viewing this\noutput value as something that's related to the\nprobability of seeing a class.",
    "start": "1920010",
    "end": "1927250"
  },
  {
    "text": "And we're adjusting the\nparameters on that output layer so as to maximize the\nprobability of the sample data",
    "start": "1927250",
    "end": "1932620"
  },
  {
    "text": "that we've got at hand. Right? ",
    "start": "1932620",
    "end": "1937930"
  },
  {
    "text": "Now, there's one more thing. Because see what\nwe've got here is we've got the basic idea\nof back propagation, which",
    "start": "1937930",
    "end": "1944880"
  },
  {
    "text": "has layers and layers\nof additional--",
    "start": "1944880",
    "end": "1950200"
  },
  {
    "text": "let me be flattering and call\nthem ideas layered on top. So here's the next idea\nthat's layered on top.",
    "start": "1950200",
    "end": "1958820"
  },
  {
    "text": "So we've got an\noutput value here. ",
    "start": "1958820",
    "end": "1965740"
  },
  {
    "text": "And it's a function after\nall, and it's got a value. And if we have\n1,000 classes, we're",
    "start": "1965740",
    "end": "1974120"
  },
  {
    "text": "going to have 1,000\noutput neurons, and each is going to be\nproducing some kind of value. And we can think of that\nvalue as a probability.",
    "start": "1974120",
    "end": "1982326"
  },
  {
    "text": " But I didn't want to\nwrite a probability yet. I just want to say\nthat what we've",
    "start": "1982326",
    "end": "1987996"
  },
  {
    "text": "got for this output neuron\nis a function of class 1.",
    "start": "1987996",
    "end": "1993185"
  },
  {
    "text": "And then there will be\nanother output neuron, which is a function of class 2.",
    "start": "1993186",
    "end": "1998200"
  },
  {
    "text": "And these values will\nbe presumably higher-- this will be higher if we are,\nin fact, looking at class 1.",
    "start": "1998200",
    "end": "2004550"
  },
  {
    "text": "And this one down here\nwill be, in fact, higher if we're looking at class m. ",
    "start": "2004550",
    "end": "2011820"
  },
  {
    "text": "So what we would like to do\nis we'd like to not just pick one of these outputs\nand say, well, you've",
    "start": "2011820",
    "end": "2017950"
  },
  {
    "text": "got the highest\nvalue, so you win. What we want to do\ninstead is we want to associate some\nkind of probability",
    "start": "2017950",
    "end": "2025130"
  },
  {
    "text": "with each of the classes. Because, after all,\nwe want to do things like find the most\nprobable five.",
    "start": "2025130",
    "end": "2032602"
  },
  {
    "text": "So what we do is\nwe say, all right, so the actual\nprobability of class 1",
    "start": "2032602",
    "end": "2039950"
  },
  {
    "text": "is equal to the output of\nthat sigmoid function divided",
    "start": "2039950",
    "end": "2047990"
  },
  {
    "text": "by the sum over all functions. ",
    "start": "2047990",
    "end": "2054908"
  },
  {
    "text": "So that takes all of\nthat entire output vector and converts each output\nvalue into a probability.",
    "start": "2054909",
    "end": "2060919"
  },
  {
    "text": "So when we used that\nsigmoid function, we did it with the\nview toward thinking",
    "start": "2060920",
    "end": "2066100"
  },
  {
    "text": "about that as a probability. And in fact, we assumed\nit was a probability when we made this argument.",
    "start": "2066100",
    "end": "2072429"
  },
  {
    "text": "But in the end,\nthere's an output for each of those classes. And so what we get is, in the\nend, not exactly a probability",
    "start": "2072429",
    "end": "2079000"
  },
  {
    "text": "until we divide by a\nnormalizing factor. So this, by the way, is called--\nnot on my list of things,",
    "start": "2079000",
    "end": "2089500"
  },
  {
    "text": "but it soon will be. ",
    "start": "2089500",
    "end": "2094579"
  },
  {
    "text": "Since we're not talking\nabout taking the maximum",
    "start": "2094580",
    "end": "2099640"
  },
  {
    "text": "and using that to classify the\npicture, what we're going to do is we're going to use\nwhat's called softmax.",
    "start": "2099640",
    "end": "2105290"
  },
  {
    "text": " So we're going to give a\nrange of classifications,",
    "start": "2105290",
    "end": "2111730"
  },
  {
    "text": "and we're going to associate\na probability with each. And that's what you saw\nin all of those samples.",
    "start": "2111730",
    "end": "2118609"
  },
  {
    "text": "You saw, yes, this is\n[? containership, ?] but maybe it's also this,\nthat, or a third, or fourth,",
    "start": "2118610",
    "end": "2124070"
  },
  {
    "text": "and fifth thing. So that is a pretty good\nsummary of the kinds",
    "start": "2124070",
    "end": "2131924"
  },
  {
    "text": "of things that are involved. But now we've got one more\nstep, because what we can do now is we can take this output\nlayer idea, this softmax idea,",
    "start": "2131924",
    "end": "2141089"
  },
  {
    "text": "and we can put them together\nwith the autocoding idea. ",
    "start": "2141090",
    "end": "2147770"
  },
  {
    "text": "So we've trained\njust a layer up. And now we're going to detach\nit from the output layer",
    "start": "2147770",
    "end": "2153700"
  },
  {
    "text": "but retain those\nweights that connect the input to the hidden layer. And when we do that,\nwhat we're going to see",
    "start": "2153700",
    "end": "2160559"
  },
  {
    "text": "is something that\nlooks like this. And now we've got a\ntrained first layer but an untrained output layer.",
    "start": "2160560",
    "end": "2167849"
  },
  {
    "text": "We're going to freeze\nthe input layer and train the output layer\nusing the sigmoid curve",
    "start": "2167850",
    "end": "2176590"
  },
  {
    "text": "and see what happens\nwhen we do that. Oh, by the way, let's run\nour test samples through.",
    "start": "2176590",
    "end": "2181724"
  },
  {
    "text": "You can see it's\nnot doing anything, and the output is half\nfor each of the categories",
    "start": "2181725",
    "end": "2187180"
  },
  {
    "text": "even though we've got\na trained middle layer. So we have to train\nthe outer layer. Let's see how long it takes.",
    "start": "2187180",
    "end": "2192826"
  },
  {
    "text": "Whoa, that was pretty fast.  Now there's an extraordinarily\ngood match between the outputs",
    "start": "2192826",
    "end": "2200210"
  },
  {
    "text": "and the desired outputs. So that's the combination\nof the autocoding idea and the softmax idea.",
    "start": "2200210",
    "end": "2205541"
  },
  {
    "text": " [? There's ?] just one more\nidea that's worthy of mention,",
    "start": "2205541",
    "end": "2215539"
  },
  {
    "text": "and that's the idea of dropout.  The plague of any neural\nnet is that it gets stuck",
    "start": "2215540",
    "end": "2222880"
  },
  {
    "text": "in some kind of local maximum. So it was discovered\nthat these things train",
    "start": "2222880",
    "end": "2228740"
  },
  {
    "text": "better if, on every\niteration, you",
    "start": "2228740",
    "end": "2236500"
  },
  {
    "text": "flip a coin for each neuron. And if the coin\nends up tails, you",
    "start": "2236500",
    "end": "2242260"
  },
  {
    "text": "assume it's just died and has\nno influence on the output. It's called dropping\nout those neurons.",
    "start": "2242260",
    "end": "2249930"
  },
  {
    "text": "And in our next iteration,\nyou drop out a different set. So what this seems\nto do is it seems",
    "start": "2249930",
    "end": "2255660"
  },
  {
    "text": "to prevent this thing from going\ninto a frozen local maximum",
    "start": "2255660",
    "end": "2261021"
  },
  {
    "text": "state.  So that's deep nets.",
    "start": "2261021",
    "end": "2266230"
  },
  {
    "text": "They should be called, by the\nway, wide nets because they tend to be enormously\nwide but rarely",
    "start": "2266230",
    "end": "2273020"
  },
  {
    "text": "more than 10 columns deep.",
    "start": "2273020",
    "end": "2281250"
  },
  {
    "text": "Now, let's see, where\nto go from here? Maybe what we should do is talk\nabout the awesome curiosity",
    "start": "2281250",
    "end": "2290900"
  },
  {
    "text": "in the current state of the art. And that is that\nall of [? this ?]",
    "start": "2290900",
    "end": "2297910"
  },
  {
    "text": "sophistication with output\nlayers that are probabilities and training using autocoding\nor Boltzmann machines,",
    "start": "2297910",
    "end": "2308580"
  },
  {
    "text": "it doesn't seem to help much\nrelative to plain, old back propagation.",
    "start": "2308580",
    "end": "2315640"
  },
  {
    "text": "So back propagation\nwith a convolutional net seems to do just about\nas good as anything.",
    "start": "2315640",
    "end": "2321630"
  },
  {
    "text": "And while we're on the subject\nof an ordinary deep net, I'd like to examine\na situation here",
    "start": "2321630",
    "end": "2330070"
  },
  {
    "text": "where we have a deep net--\nwell, it's a classroom deep net.",
    "start": "2330070",
    "end": "2339175"
  },
  {
    "text": "And we'll will put\nfive layers in there, and its job is still\nto do the same thing.",
    "start": "2339175",
    "end": "2344930"
  },
  {
    "text": "It's to classify an animal as a\ncheetah, a zebra, or a giraffe based on the height of\nthe shadow it casts.",
    "start": "2344930",
    "end": "2353630"
  },
  {
    "text": "And as before, if it's\ngreen, that means positive. If it's red, that\nmeans negative.",
    "start": "2353630",
    "end": "2359300"
  },
  {
    "text": "And right at the moment,\nwe have no training. So if we run our\ntest samples through,",
    "start": "2359300",
    "end": "2364420"
  },
  {
    "text": "the output is always a 1/2\nno matter what the animal is. All right?",
    "start": "2364420",
    "end": "2369975"
  },
  {
    "text": "So what we're\ngoing to do is just going to use ordinary back\nprop on this, same thing as in that sample that's\nunderneath the blackboard.",
    "start": "2369975",
    "end": "2381970"
  },
  {
    "text": "Only now we've got a\nlot more parameters. We've got five columns,\nand each one of them has 9 or 10 neurons in it.",
    "start": "2381970",
    "end": "2390320"
  },
  {
    "text": "So let's let this one run. ",
    "start": "2390320",
    "end": "2396160"
  },
  {
    "text": "Now, look at that\nstuff on the right. It's all turned red. At first I thought this\nwas a bug in my program.",
    "start": "2396160",
    "end": "2403270"
  },
  {
    "text": "But that makes absolute sense. If you don't know what the\nactual animal is going to be and there are a whole\nbunch of possibilities,",
    "start": "2403270",
    "end": "2408865"
  },
  {
    "text": "you better just say\nno for everybody. It's like when a biologist\nsays, we don't know. It's the most probable answer.",
    "start": "2408865",
    "end": "2416380"
  },
  {
    "text": "Well, but eventually, after\nabout 160,000 iterations, it seems to have got it.",
    "start": "2416380",
    "end": "2421400"
  },
  {
    "text": "Let's run the test\nsamples through. ",
    "start": "2421400",
    "end": "2427044"
  },
  {
    "text": "Now it's doing great. Let's do it again just to\nsee if this is a fluke. ",
    "start": "2427044",
    "end": "2437130"
  },
  {
    "text": "And all red on the right\nside, and finally, you",
    "start": "2437131",
    "end": "2452590"
  },
  {
    "text": "start seeing some changes go\nin the final layers there. And if you look at the error\nrate down at the bottom,",
    "start": "2452590",
    "end": "2459599"
  },
  {
    "text": "you'll see that it kind\nof falls off a cliff. So nothing happens\nfor a real long time,",
    "start": "2459600",
    "end": "2464806"
  },
  {
    "text": "and then it falls off a cliff.  Now, what would happen if\nthis neural net were not",
    "start": "2464806",
    "end": "2473620"
  },
  {
    "text": "quite so wide? Good question. But before we get to that\nquestion, what I'm going to do",
    "start": "2473620",
    "end": "2479093"
  },
  {
    "text": "is I'm going to do a\nfunny kind of variation on the theme of dropout. What I'm going to\ndo is I'm going",
    "start": "2479093",
    "end": "2485050"
  },
  {
    "text": "to kill off one\nneuron in each column, and then see if I can\nretrain the network",
    "start": "2485050",
    "end": "2490590"
  },
  {
    "text": "to do the right thing. So I'm going to reassign\nthose to some other purpose.",
    "start": "2490590",
    "end": "2497210"
  },
  {
    "text": "So now there's one fewer\nneuron in the network. If we rerun that, we see that\nit trains itself up very fast.",
    "start": "2497210",
    "end": "2504622"
  },
  {
    "text": "So we seem to be\nstill close enough to a solution we\ncan do without one",
    "start": "2504622",
    "end": "2510470"
  },
  {
    "text": "of the neurons in each column. Let's do it again.  Now it goes up a little\nbit, but it quickly",
    "start": "2510470",
    "end": "2517060"
  },
  {
    "text": "falls down to a solution. Try again. Quickly falls down\nto a solution.",
    "start": "2517060",
    "end": "2525620"
  },
  {
    "text": "Oh, my god, how much of\nthis am I going to do? Each time I knock\nsomething out and retrain,",
    "start": "2525620",
    "end": "2530970"
  },
  {
    "text": "it finds its solution very fast. ",
    "start": "2530970",
    "end": "2550480"
  },
  {
    "text": "Whoa, I got it all the way down\nto two neurons in each column, and it still has a solution.",
    "start": "2550480",
    "end": "2557290"
  },
  {
    "text": "It's interesting,\ndon't you think? But let's repeat the\nexperiment, but this time we're",
    "start": "2557290",
    "end": "2563119"
  },
  {
    "text": "going to do it a\nlittle differently. We're going to take\nour five layers, and before we do\nany training I'm",
    "start": "2563120",
    "end": "2569610"
  },
  {
    "text": "going to knock out all but\ntwo neurons in each column.",
    "start": "2569610",
    "end": "2577390"
  },
  {
    "text": "Now, I know that with two\nneurons in each column, I've got a solution. I just showed it. I just showed one.",
    "start": "2577390",
    "end": "2583345"
  },
  {
    "text": "But let's run it this way. ",
    "start": "2583345",
    "end": "2599060"
  },
  {
    "text": "It looks like\nincreasingly bad news. What's happened is that\nthis sucker's got itself",
    "start": "2599060",
    "end": "2605810"
  },
  {
    "text": "into a local maximum. So now you can see\nwhy there's been",
    "start": "2605810",
    "end": "2611550"
  },
  {
    "text": "a breakthrough in this\nneural net learning stuff. And it's because when\nyou widen the net,",
    "start": "2611550",
    "end": "2619300"
  },
  {
    "text": "you turn local maxima\ninto saddle points. So now it's got a way\nof crawling its way",
    "start": "2619300",
    "end": "2625560"
  },
  {
    "text": "through this vast\nspace without getting stuck on a local maximum,\nas suggested by this.",
    "start": "2625560",
    "end": "2633650"
  },
  {
    "text": "All right. So those are some, I\nthink, interesting things to look at by way of\nthese demonstrations.",
    "start": "2633650",
    "end": "2641809"
  },
  {
    "text": "But now I'd like to go\nback to my slide set and show you some\nexamples that will address",
    "start": "2641810",
    "end": "2646860"
  },
  {
    "text": "the question of whether these\nthings are seeing like we see. ",
    "start": "2646860",
    "end": "2660609"
  },
  {
    "text": "So you can try these\nexamples online. There are a variety\nof websites that allow you to put in your own picture.",
    "start": "2660610",
    "end": "2667950"
  },
  {
    "text": "And there's a cottage industry\nof producing papers in journals",
    "start": "2667950",
    "end": "2673510"
  },
  {
    "text": "that fool neural nets. So in this case, a very\nsmall number of pixels",
    "start": "2673510",
    "end": "2678600"
  },
  {
    "text": "have been changed. You don't see the\ndifference, but it's enough to take this\nparticular neural net",
    "start": "2678600",
    "end": "2684289"
  },
  {
    "text": "from a high confidence that\nit's looking at a school bus to thinking that it's\nnot a school bus.",
    "start": "2684290",
    "end": "2691777"
  },
  {
    "text": "Those are some things that\nit thinks are a school bus. ",
    "start": "2691777",
    "end": "2696779"
  },
  {
    "text": "So it appears to be\nthe case that what is triggering this\nschool bus result is that it's seeing enough\nlocal evidence that this is not",
    "start": "2696780",
    "end": "2704340"
  },
  {
    "text": "one of the other 999 classes\nand enough positive evidence",
    "start": "2704340",
    "end": "2710080"
  },
  {
    "text": "from these local\nlooks to conclude that it's a school bus. ",
    "start": "2710080",
    "end": "2718020"
  },
  {
    "text": "So do you see any\nof those things? I don't. ",
    "start": "2718020",
    "end": "2724494"
  },
  {
    "text": "And here you can say, OK, well,\nlook at that baseball one. Yeah, that looks like it's got\na little bit of baseball texture",
    "start": "2724494",
    "end": "2731500"
  },
  {
    "text": "in it. So maybe what it's doing\nis looking at texture. ",
    "start": "2731500",
    "end": "2739130"
  },
  {
    "text": "These are some examples from\na recent and very famous paper by Google using\nessentially the same ideas",
    "start": "2739130",
    "end": "2747380"
  },
  {
    "text": "to put captions on pictures. So this, by the way,\nis what has stimulated",
    "start": "2747380",
    "end": "2753790"
  },
  {
    "text": "all this enormous concern\nabout artificial intelligence. Because a naive viewer looks\nat that picture and says,",
    "start": "2753790",
    "end": "2758870"
  },
  {
    "text": "oh, my god, this\nthing knows what it's like to play, or be young,\nor move, or what a Frisbee is.",
    "start": "2758870",
    "end": "2766260"
  },
  {
    "text": "And of course, it\nknows none of that. It just knows how to\nlabel this picture. And to the credit of the\npeople who wrote this paper,",
    "start": "2766260",
    "end": "2774079"
  },
  {
    "text": "they show examples\nthat don't do so well. So yeah, it's a cat,\nbut it's not lying.",
    "start": "2774080",
    "end": "2781000"
  },
  {
    "text": "Oh, it's a little girl, but\nshe's not blowing bubbles. What about this one? [LAUGHTER]",
    "start": "2781000",
    "end": "2788848"
  },
  {
    "text": " So we've been doing our\nown work in my laboratory",
    "start": "2788848",
    "end": "2794770"
  },
  {
    "text": "on some of this. And the way the following set of\npictures was produced was this.",
    "start": "2794770",
    "end": "2799900"
  },
  {
    "text": "You take an image,\nand you separate it into a bunch of slices,\neach representing a particular frequency band.",
    "start": "2799900",
    "end": "2806760"
  },
  {
    "text": "And then you go into one\nof those frequency bands and you knock out a\nrectangle from the picture, and then you\nreassemble the thing.",
    "start": "2806760",
    "end": "2814730"
  },
  {
    "text": "And if you hadn't\nknocked that piece out, when you reassemble it,\nit would look exactly like it did when you started.",
    "start": "2814730",
    "end": "2820760"
  },
  {
    "text": "So what we're doing is we\nknock out as much as we can and still retain the\nneural net's impression",
    "start": "2820760",
    "end": "2825827"
  },
  {
    "text": "that it's the thing that it\nstarted out thinking it was. So what do you think this is? ",
    "start": "2825827",
    "end": "2833640"
  },
  {
    "text": "It's identified by a neural\nnet as a railroad car because this is the image\nthat it started with.",
    "start": "2833640",
    "end": "2841589"
  },
  {
    "text": "How about this one? That's easy, right? That's a guitar. We weren't able to mutilate that\none very much and still retain",
    "start": "2841589",
    "end": "2848090"
  },
  {
    "text": "the guitar-ness of it. How about this one? AUDIENCE: A lamp? PATRICK H. WINSTON: What's that?",
    "start": "2848090",
    "end": "2854361"
  },
  {
    "text": "AUDIENCE: Lamp. PATRICK H. WINSTON: What? AUDIENCE: Lamp. PATRICK H. WINSTON: A lamp. Any other ideas? AUDIENCE: [INAUDIBLE]. AUDIENCE: [INAUDIBLE].",
    "start": "2854361",
    "end": "2860280"
  },
  {
    "text": "PATRICK H. WINSTON: Ken,\nwhat do you think it is? AUDIENCE: A toilet. PATRICK H. WINSTON: See, he's\nan expert on this subject.",
    "start": "2860280",
    "end": "2865490"
  },
  {
    "text": "[LAUGHTER] It was identified as a barbell. What's that?",
    "start": "2865490",
    "end": "2871290"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. PATRICK H. WINSTON: A what? AUDIENCE: Cello. PATRICK H. WINSTON: Cello. You didn't see the little\ngirl or the instructor.",
    "start": "2871290",
    "end": "2879361"
  },
  {
    "text": "How about this one? AUDIENCE: [INAUDIBLE]. PATRICK H. WINSTON: What? AUDIENCE: [INAUDIBLE]. PATRICK H. WINSTON: No. ",
    "start": "2879361",
    "end": "2887205"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. PATRICK H. WINSTON:\nIt's a grasshopper. What's this? AUDIENCE: A wolf.",
    "start": "2887205",
    "end": "2892329"
  },
  {
    "text": "PATRICK H. WINSTON:\nWow, you're good.  It's actually not\na two-headed wolf.",
    "start": "2892330",
    "end": "2897693"
  },
  {
    "text": "[LAUGHTER] It's two wolves that\nare close together.",
    "start": "2897693",
    "end": "2903438"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]. PATRICK H. WINSTON:\nThat's a bird, right? AUDIENCE: [INAUDIBLE]. PATRICK H. WINSTON:\nGood for you.",
    "start": "2903438",
    "end": "2909150"
  },
  {
    "text": "It's a rabbit. [LAUGHTER] How about that? [? AUDIENCE: Giraffe. ?] ",
    "start": "2909150",
    "end": "2916040"
  },
  {
    "text": "PATRICK H. WINSTON:\nRussian wolfhound. AUDIENCE: [INAUDIBLE]. ",
    "start": "2916040",
    "end": "2926415"
  },
  {
    "text": "PATRICK H. WINSTON: If\nyou've been to Venice, you recognize this. AUDIENCE: [INAUDIBLE].",
    "start": "2926415",
    "end": "2931920"
  },
  {
    "text": "PATRICK H. WINSTON:\nSo bottom line is that these things\nare an engineering marvel and do great things,\nbut they don't see like we see.",
    "start": "2931920",
    "end": "2940536"
  },
  {
    "start": "2940536",
    "end": "2945264"
  }
]