[
  {
    "text": "The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6330"
  },
  {
    "text": "continue to offer high-quality\neducational resources for free. To make a donation or\nview additional materials",
    "start": "6330",
    "end": "13320"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "13320",
    "end": "18370"
  },
  {
    "text": " RUSS TEDRAKE: OK, so\nevery once in a while,",
    "start": "18370",
    "end": "23870"
  },
  {
    "text": "I stop and try to do a\nlittle bit of reflection, since we've-- have so many methods flying\nthrough this semester",
    "start": "23870",
    "end": "30700"
  },
  {
    "text": "that I want to just-- once again, let's\nsay where we've been, where we're going, what\nwe have, what we can do,",
    "start": "30700",
    "end": "37210"
  },
  {
    "text": "what we can't do, and why\nwe're going to do something different today-- ",
    "start": "37210",
    "end": "43995"
  },
  {
    "text": "so just a little\nreflection here. ",
    "start": "43995",
    "end": "52449"
  },
  {
    "text": "We've been talking, obviously,\nabout optimal control. There's two major approaches\nto optimal control",
    "start": "52450",
    "end": "58030"
  },
  {
    "text": "that we've focused on-- well, three, I guess. In some cases, we've done\nanalytical optimal control.",
    "start": "58030",
    "end": "63745"
  },
  {
    "start": "63745",
    "end": "71750"
  },
  {
    "text": "And I think, by now,\nyou appreciate that, although it was often-- only works in special cases--",
    "start": "71750",
    "end": "77890"
  },
  {
    "text": "linear quadratic regulators\nand things like that-- the lessons we\nlearned from that help",
    "start": "77890",
    "end": "84159"
  },
  {
    "text": "us design better algorithms. And things like LQR can fit\nright into more complicated",
    "start": "84160",
    "end": "89200"
  },
  {
    "text": "non-linear algorithms to\nmake them click, so I think, absolutely, it's essential to\nunderstand the things we can do",
    "start": "89200",
    "end": "95680"
  },
  {
    "text": "analytically in\noptimal control-- even though they\ncrap out pretty early in the scale of complexity\nthat we care about.",
    "start": "95680",
    "end": "102852"
  },
  {
    "text": "So mostly, that's good\nfor linear systems and even restricted\nthere, linear systems",
    "start": "102852",
    "end": "111610"
  },
  {
    "text": "with quadratic costs\nand things like that. And then we talked about\nmajor direction number",
    "start": "111610",
    "end": "119229"
  },
  {
    "text": "two was the dynamic programming\nand value iteration approach.",
    "start": "119230",
    "end": "132600"
  },
  {
    "text": "And the big idea there right\nwas that, because we've",
    "start": "132600",
    "end": "138500"
  },
  {
    "text": "written our cost functions\nover time to be additive, the big idea really was\nthat we're going to learn--",
    "start": "138500",
    "end": "145760"
  },
  {
    "text": "we're going to figure out\nthe cost-to-go function-- the value function,\nthe value iteration--",
    "start": "145760",
    "end": "151580"
  },
  {
    "text": "and from there, we\ncan just extract that. That captures all of\nthe long-term reasoning we have to do about the system.",
    "start": "151580",
    "end": "156890"
  },
  {
    "text": "From that, we can extract the\noptimal control decisions. And it's actually\nvery efficient.",
    "start": "156890",
    "end": "164120"
  },
  {
    "text": "I hope, by now, you agree with\nme that it's very efficient,",
    "start": "164120",
    "end": "170900"
  },
  {
    "text": "because if you think about it,\nit's solving for an entire-- solving for the optimal policy\nfor every possible initial",
    "start": "170900",
    "end": "176780"
  },
  {
    "text": "condition in times that\nare comparable to what we're doing for single initial\nconditions in the loop cases.",
    "start": "176780",
    "end": "184519"
  },
  {
    "text": "But it only works\nin low dimensions, and it has some\ndiscretization issues. ",
    "start": "184520",
    "end": "196959"
  },
  {
    "text": "And then the third\nmajor approach-- I called it policy search.",
    "start": "196960",
    "end": "202555"
  },
  {
    "text": " And we focused mostly,\nin the policy search,",
    "start": "202555",
    "end": "213610"
  },
  {
    "text": "on loop trajectory optimization. ",
    "start": "213610",
    "end": "224650"
  },
  {
    "text": "But I tried to make\nthe point early-- and I'm going to make the point\nagain in a lecture or two--",
    "start": "224650",
    "end": "230110"
  },
  {
    "text": "that it's really not\nrestricted to thinking about loop trajectories.",
    "start": "230110",
    "end": "235402"
  },
  {
    "text": "So when I first\nsaid policy search, I said we could be\nlooking for parameters of a feedback controller of a-- the linear gain matrix\nof a linear feedback.",
    "start": "235402",
    "end": "244300"
  },
  {
    "text": "We could do a lot of\nthings, but we quickly started being specific\nin our algorithms in trying to optimize some loop\ntape with direct colocation",
    "start": "244300",
    "end": "252670"
  },
  {
    "text": "with shooting. But the ideas really are\nmore general than that, and I'm going to-- we're\ngoing to have a lecture soon",
    "start": "252670",
    "end": "258338"
  },
  {
    "text": "about how to do\nthese kind of things with function approximation,\nand do more general feedback controllers.",
    "start": "258339",
    "end": "265090"
  },
  {
    "text": "These worked in higher\ndimensional systems,",
    "start": "265090",
    "end": "270580"
  },
  {
    "text": "had local minima-- all the problems\nyou know by now. ",
    "start": "270580",
    "end": "283080"
  },
  {
    "text": "OK, good-- so for\nour model systems,",
    "start": "283080",
    "end": "288210"
  },
  {
    "text": "we got pretty far with that. In the cases where\nwe knew the model, we assumed that the\nmodel was deterministic",
    "start": "288210",
    "end": "295950"
  },
  {
    "text": "and sensing was clean-- everything like that. We could make our simulations\ndo pretty much what we wanted",
    "start": "295950",
    "end": "302340"
  },
  {
    "text": "with those bag of tricks. Then I threw in the stochastic\noptimal control case.",
    "start": "302340",
    "end": "309660"
  },
  {
    "start": "309660",
    "end": "321370"
  },
  {
    "text": "We said, what happens if the\nmodels aren't deterministic?",
    "start": "321370",
    "end": "327270"
  },
  {
    "text": "Analytical optimal control-- I didn't really talk about\nit, but there are still some cases where you can do\nanalytical optimal control.",
    "start": "327270",
    "end": "333102"
  },
  {
    "text": "The linear quadratic\nGaussian systems are the clear example of that.",
    "start": "333102",
    "end": "340410"
  },
  {
    "text": "We said that value\niteration for this-- although I was quickly\nchallenged on it,",
    "start": "340410",
    "end": "347280"
  },
  {
    "text": "I said, basically,\nit was no harder to do value iteration\nstochastic optimization, where",
    "start": "347280",
    "end": "355470"
  },
  {
    "text": "now our goal is to minimize\nsome expected value of a long-term cost.",
    "start": "355470",
    "end": "361190"
  },
  {
    "text": " Value iteration we\nbasically said and almost",
    "start": "361190",
    "end": "367740"
  },
  {
    "text": "no harder to do the case with\ntransition probabilities flying around. ",
    "start": "367740",
    "end": "379379"
  },
  {
    "text": "And in fact, the\nbarycentric grids",
    "start": "379380",
    "end": "386740"
  },
  {
    "text": "that we used in the value\nduration way back there I told you actually has a\nmore clean interpretation",
    "start": "386740",
    "end": "393550"
  },
  {
    "text": "as taking a continuous--\nyou can think of it as being a continuous\ndeterministic system,",
    "start": "393550",
    "end": "404020"
  },
  {
    "text": "and converting it\ninto a discrete state stochastic system. ",
    "start": "404020",
    "end": "416129"
  },
  {
    "text": "Remember, the interpolation\nthat you do in the barycentric",
    "start": "416130",
    "end": "421350"
  },
  {
    "text": "actually takes\nexactly the same form as some transition probabilities\nwhen you're going from-- ",
    "start": "421350",
    "end": "429060"
  },
  {
    "text": "you've got some\ngrid, and you want to know where you're going\nto go from simulating forward from this with some\naction for some dt,",
    "start": "429060",
    "end": "435840"
  },
  {
    "text": "you can approximate that\nas being some fraction here, some fraction\nhere, some fraction here.",
    "start": "435840",
    "end": "441580"
  },
  {
    "text": "And it turns out to be\nexactly equivalent to saying that there's some\nprobability I get here, some probability I get here,\nsome probability I get here,",
    "start": "441580",
    "end": "447815"
  },
  {
    "text": "and the like. So value iteration\nreally, in that sense,",
    "start": "447815",
    "end": "453210"
  },
  {
    "text": "can solve stochastic\nproblems nicely. The other major approach,\nthe policy search,",
    "start": "453210",
    "end": "465080"
  },
  {
    "text": "can still work for\nstochastic problems.",
    "start": "465080",
    "end": "470389"
  },
  {
    "text": "In some cases, you can\ncompute the gradient",
    "start": "470390",
    "end": "478400"
  },
  {
    "text": "of the expected\nvalue with respect to your parameters analytically,\nwith a [INAUDIBLE] update.",
    "start": "478400",
    "end": "485330"
  },
  {
    "text": "In other cases, you\ncan do sampling based,",
    "start": "485330",
    "end": "490460"
  },
  {
    "text": "Monte Carlo based\nestimates, and I'm going to get more into that. ",
    "start": "490460",
    "end": "506560"
  },
  {
    "text": "We're going to talk\nmore about that, but the takeaway messages,\nwhen things get stochastic,",
    "start": "506560",
    "end": "512130"
  },
  {
    "text": "both of these\nmethods still work. And they work in\nslightly different ways,",
    "start": "512130",
    "end": "517260"
  },
  {
    "text": "but you can make\nboth of those work And then, last week, John threw\na major wrench into things.",
    "start": "517260",
    "end": "526930"
  },
  {
    "text": "Sorry. That wasn't supposed to\nbe a statement about John. John made your life\nbetter by telling you",
    "start": "526930",
    "end": "535170"
  },
  {
    "text": "that some of these statements--\nsome of these algorithms work even if you\ndon't know the model. ",
    "start": "535170",
    "end": "554730"
  },
  {
    "text": "And he talked about doing\npolicy search without a model.",
    "start": "554730",
    "end": "560639"
  },
  {
    "start": "560640",
    "end": "567420"
  },
  {
    "text": "And the big idea there was that\nactually fairly simple looking",
    "start": "567420",
    "end": "575940"
  },
  {
    "text": "algorithms, which just perturb\nthe-- which try different parameters--",
    "start": "575940",
    "end": "581220"
  },
  {
    "text": "run a trial, try\ndifferent parameters-- ",
    "start": "581220",
    "end": "586230"
  },
  {
    "text": "simple sampling algorithms\ncan estimate the same thing",
    "start": "586230",
    "end": "598010"
  },
  {
    "text": "we would do with our policy\ngradient, the gradient of the expected reward with\nrespect to the parameters.",
    "start": "598010",
    "end": "607820"
  },
  {
    "text": "Even the simplest\nthing is let's change my parameters a little\nbit, see what happened. That gives me a sample\nfrom this gradient.",
    "start": "607820",
    "end": "617515"
  },
  {
    "text": "And if I do it enough times,\nI pull enough samples, and I can-- I get a sample of\nthe expected returns.",
    "start": "617515",
    "end": "622805"
  },
  {
    "text": " If you think back,\nthat's why we try",
    "start": "622805",
    "end": "629750"
  },
  {
    "text": "to stick in stochastic\noptimal control before we got to that,\nbecause John also told you the nice interpretation\nof these algorithms",
    "start": "629750",
    "end": "636110"
  },
  {
    "text": "in the stochastic-- ",
    "start": "636110",
    "end": "646350"
  },
  {
    "text": "that, even if the plant\nthat you're measuring from",
    "start": "646350",
    "end": "652310"
  },
  {
    "text": "is stochastic or if the\nsensors are-- there's noise, then actually, still, these\nsame sampling algorithms",
    "start": "652310",
    "end": "658310"
  },
  {
    "text": "can estimate these\ngradients for you nicely. ",
    "start": "658310",
    "end": "667449"
  },
  {
    "text": "I think John also\nmade the point-- and I want to make it again--",
    "start": "667450",
    "end": "673067"
  },
  {
    "text": "you would never use these\nalgorithms if you had a model. ",
    "start": "673067",
    "end": "678760"
  },
  {
    "text": "They're beautiful, but\nprobably, if you have a model-- ",
    "start": "678760",
    "end": "699370"
  },
  {
    "text": "maybe, if you have\na model and you're a very patient\nperson, but very lazy, then you might try to use\nthis, because you can type it",
    "start": "699370",
    "end": "705640"
  },
  {
    "text": "in in a few minutes,\nbut it's going to take a lot longer to run. And the reason for\nthat is it's going",
    "start": "705640",
    "end": "712630"
  },
  {
    "text": "to require many [INAUDIBLE]\nrequires many more simulations.",
    "start": "712630",
    "end": "718260"
  },
  {
    "start": "718260",
    "end": "725260"
  },
  {
    "text": "In fact, it just\nrequires many simulations to estimate a single gradient-- ",
    "start": "725260",
    "end": "736280"
  },
  {
    "text": "policy gradient. ",
    "start": "736280",
    "end": "746510"
  },
  {
    "text": "Now, the next thing I'm\ngoing to say is a little more controversial, but most people\nwould say that the limiting",
    "start": "746510",
    "end": "755410"
  },
  {
    "text": "case, the best you could\npossibly do with these reinforce type algorithms-- are\nyou raising your hand or just-- AUDIENCE: [INAUDIBLE]",
    "start": "755410",
    "end": "761220"
  },
  {
    "text": "RUSS TEDRAKE: No-- sorry. The best thing you can do with\nthese reinforced algorithms, if you really--",
    "start": "761220",
    "end": "766810"
  },
  {
    "text": "the best performance you could\nexpect is a shooting algorithm.",
    "start": "766810",
    "end": "773363"
  },
  {
    "start": "773363",
    "end": "786699"
  },
  {
    "text": "And it's really, I should say,\na first-order shooting method.",
    "start": "786700",
    "end": "793090"
  },
  {
    "text": "It's really just doing gradient\ndescent by doing trials. And when we talked\nabout shooting methods,",
    "start": "793090",
    "end": "799600"
  },
  {
    "text": "I actually said never do\nfirst-order shooting methods. I made a big point. I said never do\nthis, never do this--",
    "start": "799600",
    "end": "805269"
  },
  {
    "text": "because if you go to the\nsecond-order methods, things converge faster. You don't have to\npick learning rates. You can handle constraints.",
    "start": "805270",
    "end": "812649"
  },
  {
    "text": "So there are people that do a\nbit of more second-order policy",
    "start": "812650",
    "end": "818572"
  },
  {
    "text": "gradient algorithms, but\nthat's not the standard yet. ",
    "start": "818572",
    "end": "825095"
  },
  {
    "text": "So you should really think of\nthose as cool systems that, if you-- cool algorithms that, if\nyou don't have a model,",
    "start": "825095",
    "end": "833140"
  },
  {
    "text": "you can almost do\na shooting method.  Why do I say that's a\ncontroversial statement?",
    "start": "833140",
    "end": "839990"
  },
  {
    "start": "839990",
    "end": "848290"
  },
  {
    "text": "Could you imagine somebody\nstanding up and saying, this is actually better\nthan doing gradient descent? ",
    "start": "848290",
    "end": "855829"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] RUSS TEDRAKE: Yeah. So the one advantage is that\nit's doing stochastic gradient",
    "start": "855830",
    "end": "863760"
  },
  {
    "text": "descent. ",
    "start": "863760",
    "end": "874372"
  },
  {
    "text": "And there are people\nout there that really believe stochastic\ngradient descent can outperform",
    "start": "874372",
    "end": "879430"
  },
  {
    "text": "even higher order\nmethods in certain cases, just because of their ability\nto, by virtue of being random--",
    "start": "879430",
    "end": "887180"
  },
  {
    "text": "this is not some magical\nproperty we've endowed. This is [? Because ?] the\nalgorithm is a little crazy. It bounces out of local minima.",
    "start": "887180",
    "end": "893946"
  },
  {
    "text": " So for that reason, it does\nhave all the strong optimization",
    "start": "893946",
    "end": "905320"
  },
  {
    "text": "claims that a stochastic\ngradient descent algorithm has. ",
    "start": "905320",
    "end": "916160"
  },
  {
    "text": "There's another point\nto make, though, and I think John made this too. The performance of this--",
    "start": "916160",
    "end": "921589"
  },
  {
    "text": "and John's done nice-- written nice paper on this-- the performance\nyou'd expect, meaning",
    "start": "921590",
    "end": "927770"
  },
  {
    "text": "the number of trials it would\ntake to learn to optimize your cost function-- the performance of these\nreinforced type algorithms--",
    "start": "927770",
    "end": "935343"
  },
  {
    "start": "935343",
    "end": "942250"
  },
  {
    "text": "it degrades with a\nnumber of parameters you're trying to tune. ",
    "start": "942250",
    "end": "969910"
  },
  {
    "text": "So remember, the\nfundamental idea was-- and the way I like\nto think of it is, imagine you're at a mixer\nstation in a sound recording",
    "start": "969910",
    "end": "978834"
  },
  {
    "text": "studio, and you're\nlooking through the glass, and you've got a\nrobot over there. You've got all your\nknobs set in some place,",
    "start": "978835",
    "end": "985379"
  },
  {
    "text": "and your robot\ndoes its behavior, and then you give it a score. You turn your\nknobs a little bit,",
    "start": "985380",
    "end": "991090"
  },
  {
    "text": "you see how the robot acts. You turn it off a\nlittle bit more. And your job is to just twist\nthese knobs in a way that",
    "start": "991090",
    "end": "996310"
  },
  {
    "text": "finds the way down the gradient,\nand gets your robot doing what you want to do. ",
    "start": "996310",
    "end": "1003450"
  },
  {
    "text": "That maybe is a\ndemystifying way to think about this everything, which\nis mathematically beautiful, but really, it's\njust turning knobs.",
    "start": "1003450",
    "end": "1011010"
  },
  {
    "text": "If you have a model and you\ncan compute the gradient, then you don't have to guess\nthe way you turn knobs. You should always use that\nmodel to turn the knobs",
    "start": "1011010",
    "end": "1017323"
  },
  {
    "text": "in the right direction. And also, if you think about\nthat analogy, the number of--",
    "start": "1017323",
    "end": "1023321"
  },
  {
    "text": "the length of time it's\ngoing to take you to optimize your function is\ngoing to depend on how many knobs you have to turn.",
    "start": "1023322",
    "end": "1029459"
  },
  {
    "text": "If I have 100 knobs\nin front of me and I change them\nall a little bit-- I see how my robot\nacted-- then it's",
    "start": "1029460",
    "end": "1034724"
  },
  {
    "text": "going to be hard for me\nto figure out exactly which knob to assign credit to. The fewer knobs\nI have to change,",
    "start": "1034724",
    "end": "1040560"
  },
  {
    "text": "the faster I can estimate\nwhich knobs were important, and climb down a gradient. ",
    "start": "1040560",
    "end": "1052060"
  },
  {
    "text": "I still say, when\nyou have a model, you should always\nuse it, because you can estimate the gradients. You can turn the knobs\nin the right way.",
    "start": "1052060",
    "end": "1057970"
  },
  {
    "text": "But in the case where you don't\nhave a model, it's actually--",
    "start": "1057970",
    "end": "1063580"
  },
  {
    "text": "they're very nice\nclasses of algorithms. This knob tuning thing\nsounds ridiculous. Maybe, if I have\neven an [INAUDIBLE],,",
    "start": "1063580",
    "end": "1071185"
  },
  {
    "text": "if you have a good model\nof the [INAUDIBLE],, then maybe you shouldn't-- you\nshould definitely be using it.",
    "start": "1071185",
    "end": "1076539"
  },
  {
    "text": "But if you have a very\ncomplicated system, and the performance only depends\non the number of parameters,",
    "start": "1076540",
    "end": "1082120"
  },
  {
    "text": "then it-- I just want to make\nthe point that it's-- they're actually pretty powerful\nfor some control problems.",
    "start": "1082120",
    "end": "1093445"
  },
  {
    "start": "1093445",
    "end": "1098862"
  },
  {
    "text": "And the ones that we're\nworking on in my group are fluid dynamics\ncontrol problems,",
    "start": "1098863",
    "end": "1104230"
  },
  {
    "text": "but specifically if you have\nproblems where you can get away",
    "start": "1104230",
    "end": "1109360"
  },
  {
    "text": "with the small\nnumber of parameters,",
    "start": "1109360",
    "end": "1123549"
  },
  {
    "text": "but you have a very\ncomplicated unknown dynamics.",
    "start": "1123550",
    "end": "1131260"
  },
  {
    "text": " And actually, those\nalgorithms are really--",
    "start": "1131260",
    "end": "1136830"
  },
  {
    "text": "make a lot of sense to me. So the performance of these\nrandomized policy search",
    "start": "1136830",
    "end": "1144000"
  },
  {
    "text": "algorithms-- it goes with\nthe number of parameters you're trying to tune. I could be sitting in\nthis mixing station,",
    "start": "1144000",
    "end": "1150090"
  },
  {
    "text": "and I could be twittering\nfour parameters and having a simple\npendulum do its thing,",
    "start": "1150090",
    "end": "1155850"
  },
  {
    "text": "or I could be sitting in\nthere turning these four knobs and having a\nNavier-Stokes simulation, with some very complicated\nfluid doing something,",
    "start": "1155850",
    "end": "1164547"
  },
  {
    "text": "and the amount of time it takes\nme to twiddle those parameters is the same. One of the strongest\nproperties of these algorithms",
    "start": "1164547",
    "end": "1170850"
  },
  {
    "text": "is that, by virtue of\nignoring the model, they're actually insensitive\nto the model complexity.",
    "start": "1170850",
    "end": "1177130"
  },
  {
    "text": "So in my group, we're\nreally trying to push-- in some problems where the\ndynamics are unknown and very complicated and a\nlot of the community",
    "start": "1177130",
    "end": "1184740"
  },
  {
    "text": "is trying to build\nbetter models of this, we're trying to say, well, maybe\nbefore you have perfect models,",
    "start": "1184740",
    "end": "1189750"
  },
  {
    "text": "we can do some of these\nmodel-free search algorithms to build good controllers\nwithout perfect models.",
    "start": "1189750",
    "end": "1195210"
  },
  {
    "text": " Are people OK with that\narray of techniques?",
    "start": "1195210",
    "end": "1203530"
  },
  {
    "text": "Yeah? You have a good\narsenal of tools?",
    "start": "1203530",
    "end": "1210961"
  },
  {
    "text": "Can you see the\nobvious place where I'm trying to go next, now\nthat I've set it up like this? ",
    "start": "1210962",
    "end": "1221960"
  },
  {
    "text": "We did value methods and\npolicy search methods for the simple case, then we did\nvalue methods and policy search",
    "start": "1221960",
    "end": "1227570"
  },
  {
    "text": "methods for the\nstochastic case, then we did policy methods\nfor the model-free case.",
    "start": "1227570",
    "end": "1233490"
  },
  {
    "text": "So how about we do model-free\nvalue methods today? ",
    "start": "1233490",
    "end": "1257460"
  },
  {
    "text": "But I know it's a complicated\nweb of algorithms, so I want to make sure that I\nstop and say that kind of stuff every once in a while.",
    "start": "1257460",
    "end": "1262877"
  },
  {
    "start": "1262877",
    "end": "1268250"
  },
  {
    "text": "So what's the difference between\na policy method and a value method? So value duration-- like I\nsaid, it's very, very efficient.",
    "start": "1268250",
    "end": "1279270"
  },
  {
    "text": "The way we represented\nvalue iteration with a grid,",
    "start": "1279270",
    "end": "1285360"
  },
  {
    "text": "and having to solve\nevery possible state at every possible time, is\nthe extreme form of value--",
    "start": "1285360",
    "end": "1292260"
  },
  {
    "text": "of the value methods. In general, we can try to build\napproximate value methods--",
    "start": "1292260",
    "end": "1298770"
  },
  {
    "text": "estimates of our value\nfunction that don't require the big discretization.",
    "start": "1298770",
    "end": "1304423"
  },
  {
    "text": "So actually, last\nweek, at the meeting-- one of the meetings I was\nat, I met Gerry Tesauro. And Gerry Tesauro is the\nguy who did TD-Gammon.",
    "start": "1304423",
    "end": "1311790"
  },
  {
    "text": "Anybody heard of TD-Gammon? Yeah? [INAUDIBLE] knows TD-Gammon. ",
    "start": "1311790",
    "end": "1319680"
  },
  {
    "text": "I don't know what year it was. It was 20 years ago now. One of the big success stories\nfor reinforcement learning",
    "start": "1319680",
    "end": "1326610"
  },
  {
    "text": "was that they\nbuilt a game player based on reinforcement\nlearning that could play backgammon\nwith the experts",
    "start": "1326610",
    "end": "1333390"
  },
  {
    "text": "and beat the experts\nat backgammon. Now, backgammon's actually\nnot a trivial game. It's got a huge state space--",
    "start": "1333390",
    "end": "1339600"
  },
  {
    "text": "huge state space. I don't play\nbackgammon, but I know there's a lot of bits\ngoing around there. It's stochastic, because\nyou roll a die every once",
    "start": "1339600",
    "end": "1346230"
  },
  {
    "text": "in a while. So it's actually not\nsome complicated--",
    "start": "1346230",
    "end": "1352290"
  },
  {
    "text": "not some simple game. In some ways, it's\nsurprising that it was solved before\ncheckers and these others.",
    "start": "1352290",
    "end": "1361173"
  },
  {
    "text": "Maybe it's just because not\nenough people play backgammon, so you can beat\nthe experts easier. I don't know.",
    "start": "1361173",
    "end": "1367650"
  },
  {
    "text": "But we were playing\ncompetition style-- beat the best humans\nat backgammon-- well before checkers and chess,\nbecause of a value-based--",
    "start": "1367650",
    "end": "1378900"
  },
  {
    "text": "model-free, value-based\nmethod for backgammon.",
    "start": "1378900",
    "end": "1384150"
  },
  {
    "text": "So Gerry Tesauro actually\nuse neural networks, and he learned, from\nwatching the game,",
    "start": "1384150",
    "end": "1389370"
  },
  {
    "text": "a value function for the game. What does that mean? So what do you do when\nyou play backgammon--",
    "start": "1389370",
    "end": "1396180"
  },
  {
    "text": "or whatever game you play? I'm not trying to\ndump on that game. I just haven't played it myself. So if you look at a go\nboard or a chess board,",
    "start": "1396180",
    "end": "1406230"
  },
  {
    "text": "you don't think about every\nsingle state that's possibly in there, but you're able\nto quickly look at the board",
    "start": "1406230",
    "end": "1413310"
  },
  {
    "text": "and get a sense of if\nyou're winning or losing. If you were to make this move,\nmy life should get better.",
    "start": "1413310",
    "end": "1421565"
  },
  {
    "text": "And there are\nserious people that think that the natural\nrepresentation for very complicated physical\ncontrol processes",
    "start": "1421565",
    "end": "1427590"
  },
  {
    "text": "or very complicated\ngame playing scenarios is to not learn actually\nthe policy directly,",
    "start": "1427590",
    "end": "1433740"
  },
  {
    "text": "but to just learn a\nsense of what's good and what's bad directly, learn\na value function directly.",
    "start": "1433740",
    "end": "1441480"
  },
  {
    "text": "And then we [INAUDIBLE]\nfrom value iteration. That captures all\nthat's hard about--",
    "start": "1441480",
    "end": "1446640"
  },
  {
    "text": "that captures the\nentire long-term look ahead in the optimal\ncontrol problem. Once I have a value\nfunction, if I",
    "start": "1446640",
    "end": "1453690"
  },
  {
    "text": "have a value function I believe,\nif I want to make an action, all I have to do is think about,\nwell, if I made this action,",
    "start": "1453690",
    "end": "1458730"
  },
  {
    "text": "my value would get\nbetter by this much. If I made this action, my value\nwould get better by this much. And I just pick the action that\nmaximizes my expected value.",
    "start": "1458730",
    "end": "1466390"
  },
  {
    "text": " Now, the good thing\nabout value-based methods",
    "start": "1466390",
    "end": "1473190"
  },
  {
    "text": "is that they tend to\nbe very efficient. ",
    "start": "1473190",
    "end": "1478200"
  },
  {
    "text": "You can simultaneously think\nabout lots of different states at a time. Just like value duration,\nit's very efficient",
    "start": "1478200",
    "end": "1485309"
  },
  {
    "text": "to learn value methods. And historically, in the\nreinforcement learning world,",
    "start": "1485310",
    "end": "1492120"
  },
  {
    "text": "nobody ever really did\npolicy search methods until the early '90s. There was at least\n15 years where",
    "start": "1492120",
    "end": "1499410"
  },
  {
    "text": "people were doing cool things\nwith robots, and game playing, and things like that, where\nalmost everybody, every paper",
    "start": "1499410",
    "end": "1505260"
  },
  {
    "text": "was talking about, how do\nyou learn a value function? How do you learn\na value function if you have to put in a\nfunction approximator?",
    "start": "1505260",
    "end": "1510637"
  },
  {
    "text": "Or how do you do a value\nfunction if this, if this? So really, even though\nI did it second,",
    "start": "1510637",
    "end": "1515670"
  },
  {
    "text": "this was actually the\ncore of reinforcement learning for a long time. How do you learn\na value function? How do you estimate\nthe cost-to-go--",
    "start": "1515670",
    "end": "1522780"
  },
  {
    "text": "ideally, the\noptimal cost-to-go-- given trial and error\nexperience with the robot?",
    "start": "1522780",
    "end": "1531807"
  },
  {
    "text": "So that's today's problem. ",
    "start": "1531807",
    "end": "1541620"
  },
  {
    "text": "Good-- so we can make\nit easier by thinking about a sub-problem first. ",
    "start": "1541620",
    "end": "1550200"
  },
  {
    "text": "And that's really\npolicy evaluation,",
    "start": "1550200",
    "end": "1561409"
  },
  {
    "text": "which is the problem\nof, given I have-- ",
    "start": "1561410",
    "end": "1567070"
  },
  {
    "text": "I have my dynamics, of\ncourse, and some policy pi,",
    "start": "1567070",
    "end": "1580460"
  },
  {
    "text": "I want to estimate\nor compute J of pi,",
    "start": "1580460",
    "end": "1590399"
  },
  {
    "text": "the long-term potentially\nexpected reward of executing that\nfeedback policy",
    "start": "1590400",
    "end": "1597090"
  },
  {
    "text": "on that robot, potentially\nfrom all states at all times.",
    "start": "1597090",
    "end": "1603330"
  },
  {
    "text": " So this is maybe equivalent to\nwhat I just said about chess.",
    "start": "1603330",
    "end": "1609750"
  },
  {
    "text": "So my value function for\nchess might look different than somebody who knows\nhow to play chess.",
    "start": "1609750",
    "end": "1617030"
  },
  {
    "text": "I look at the board, and\nmost of the time, I'm losing, and my actions are going\nto be chosen differently,",
    "start": "1617030",
    "end": "1624152"
  },
  {
    "text": "because I wouldn't even know\nwhat to do if my rook ended up over there. And the optimal value function,\ngiven I was acting optimally,",
    "start": "1624152",
    "end": "1633720"
  },
  {
    "text": "might look very different. But for me, the first problem is\njust estimate what's my cost--",
    "start": "1633720",
    "end": "1642060"
  },
  {
    "text": "the cost of executing\nmy current game playing strategy, my current\ncontrol-- feedback controller on this robot, or this game?",
    "start": "1642060",
    "end": "1651510"
  },
  {
    "text": "Now, there is\nsomething culturally different from the\nreinforcement learning",
    "start": "1651510",
    "end": "1658050"
  },
  {
    "text": "value-based communities,\nand I'm going to go ahead and make\nthat switch now. ",
    "start": "1658050",
    "end": "1667680"
  },
  {
    "text": "Most of the time, these\nthings are infinite horizon discounted problems.",
    "start": "1667680",
    "end": "1672900"
  },
  {
    "text": " I'll say it's discrete\ntime just to keep it clean,",
    "start": "1672900",
    "end": "1679049"
  },
  {
    "text": "because then it's easy to\nwrite [INAUDIBLE] equals t",
    "start": "1679050",
    "end": "1684360"
  },
  {
    "text": "to infinity here, gamma to the-- let me just do it like this.",
    "start": "1684360",
    "end": "1690220"
  },
  {
    "text": "Let's assume that it's\ncompletely feedback. That'll just keep me\nwriting less symbols for the rest of\nthe lecture here.",
    "start": "1690220",
    "end": "1696179"
  },
  {
    "text": "0 to infinity, gamma to the\nn, xn, and then pi of xn,",
    "start": "1696180",
    "end": "1712130"
  },
  {
    "text": "where my action is always\npulled directly from pi-- ",
    "start": "1712130",
    "end": "1723711"
  },
  {
    "text": "I mentioned it once\nbefore, but why do people do discounted things?",
    "start": "1723712",
    "end": "1729250"
  },
  {
    "text": "Lots of reasons why people\ndo discounted things-- first of all, if you have\ninfinite horizon rewards,",
    "start": "1729250",
    "end": "1734828"
  },
  {
    "text": "there's just a practical issue. If you're not careful,\ninfinite horizon rewards will blow up on you.",
    "start": "1734828",
    "end": "1741360"
  },
  {
    "text": "So if you put some sort\nof decaying factor gammas,",
    "start": "1741360",
    "end": "1749500"
  },
  {
    "text": "typically, it's constrained\nto be less than 1 just so you don't have to\nworry about things blowing up",
    "start": "1749500",
    "end": "1754630"
  },
  {
    "text": "in the long term. But you can make it\n1, and then you just",
    "start": "1754630",
    "end": "1760522"
  },
  {
    "text": "have to be more careful that\nyou get to a fixed point [INAUDIBLE] cost,\nor whatever it is. ",
    "start": "1760522",
    "end": "1766750"
  },
  {
    "text": "Let's just put some decaying\ncost on future experiences.",
    "start": "1766750",
    "end": "1773702"
  },
  {
    "start": "1773702",
    "end": "1779500"
  },
  {
    "text": "Philosophically, some\npeople really like this. So a lot of the problems\nwe've talked about are very episodic in nature.",
    "start": "1779500",
    "end": "1786820"
  },
  {
    "text": "We talked about designing\ntrajectories from time 0 to time final. What's the optimal thing? What's the optimal thing?",
    "start": "1786820",
    "end": "1793659"
  },
  {
    "text": "If you just want\nto live your life-- presumably, you don't\nknow exactly when",
    "start": "1793660",
    "end": "1799810"
  },
  {
    "text": "you're going to die. You're going to maximize\nsome long-term reward. You'd like it to be infinite,\nbut realistically, the things",
    "start": "1799810",
    "end": "1806080"
  },
  {
    "text": "that are going to\nhappen to me tomorrow are more important to\nme that the things that are happening in the\nvery far, distant future.",
    "start": "1806080",
    "end": "1811140"
  },
  {
    "text": "So some people, philosophically,\njust like having this as a cost function\nfor a robot that's",
    "start": "1811140",
    "end": "1818860"
  },
  {
    "text": "alive executing\nan online policy, worrying about short-term\nthings a little bit more, but thinking about\ninto the future.",
    "start": "1818860",
    "end": "1825558"
  },
  {
    "text": "And that knob is\ncontrolled by gamma. ",
    "start": "1825558",
    "end": "1831280"
  },
  {
    "text": "Almost all of the\nRL tools can be",
    "start": "1831280",
    "end": "1836290"
  },
  {
    "text": "made compatible with the\nepisodic non-discounted cases, but culturally, like I said,\nthey're almost always written",
    "start": "1836290",
    "end": "1842507"
  },
  {
    "text": "in this form, so I\nthought it'd makes sense to switch to that\nform for a little bit. ",
    "start": "1842508",
    "end": "1856090"
  },
  {
    "text": "So how do we estimate J pi of\nx, given that kind of a setup?",
    "start": "1856090",
    "end": "1861760"
  },
  {
    "text": " Let's do the model-based\ncase, just as a first case.",
    "start": "1861760",
    "end": "1874690"
  },
  {
    "text": "Let's say I have a good model. ",
    "start": "1874690",
    "end": "1883770"
  },
  {
    "text": "I made it look\ndeterministic here, but we can, in general, do\nthis for stochastic things.",
    "start": "1883770",
    "end": "1888900"
  },
  {
    "text": " Let me do the model-based\nMarkov chain version first.",
    "start": "1888900",
    "end": "1897250"
  },
  {
    "start": "1897250",
    "end": "1910808"
  },
  {
    "text": "So you remember, in general, we\nsaid that the optimal control problem for discrete\nstates, discrete actions,",
    "start": "1910808",
    "end": "1918539"
  },
  {
    "text": "stochastic transitions looked\nlike a Markov decision process,",
    "start": "1918540",
    "end": "1923850"
  },
  {
    "text": "where we have some\ndiscrete state space,",
    "start": "1923850",
    "end": "1936049"
  },
  {
    "text": "we have a probability\ntransition matrix,",
    "start": "1936050",
    "end": "1951560"
  },
  {
    "text": "where T-I-J is probability\nof transitioning from I to J.",
    "start": "1951560",
    "end": "1975640"
  },
  {
    "text": "And we have some cost. And in the graph sense,\nI tend to write--",
    "start": "1975640",
    "end": "1980830"
  },
  {
    "text": "we tend to write the cost as-- instead of being\n[INAUDIBLE] action,",
    "start": "1980830",
    "end": "1986020"
  },
  {
    "text": "we can just write it\nas the probability of--",
    "start": "1986020",
    "end": "1991060"
  },
  {
    "text": "the cost of transitioning from\nstate I to state J. Good--",
    "start": "1991060",
    "end": "2015280"
  },
  {
    "text": "now, in the Markov\ndecision processes that we talked about before,\nthe transition matrix",
    "start": "2015280",
    "end": "2022570"
  },
  {
    "text": "was a function of\nthe action you chose. Your goal was to choose\nthe action, which",
    "start": "2022570",
    "end": "2030100"
  },
  {
    "text": "made your transition\nmatrices have the optimal-- choose the best transition\nmatrices for your problem.",
    "start": "2030100",
    "end": "2037390"
  },
  {
    "text": "In policy evaluation,\nwhere we're saying, we're trying to figure out the\nprobability of the cost-to-go",
    "start": "2037390",
    "end": "2043299"
  },
  {
    "text": "of running this policy,\nthen the actions are-- the parameterization by\naction disappears again.",
    "start": "2043300",
    "end": "2049582"
  },
  {
    "text": "It's not a Markov\ndecision process. It falls back right into\nbeing a Markov chain. So it's a simple picture now.",
    "start": "2049582",
    "end": "2056230"
  },
  {
    "text": "We have a graph there's some\nprobabilities of transitioning",
    "start": "2056230",
    "end": "2065580"
  },
  {
    "text": "from each state to each\naction, from each state,",
    "start": "2065580",
    "end": "2072330"
  },
  {
    "text": "because my actions\nare predetermined. If I'm in some state, I'm\ngoing to take this action based on pi.",
    "start": "2072330",
    "end": "2077469"
  },
  {
    "text": " And each transition\nincurs some cost,",
    "start": "2077469",
    "end": "2084030"
  },
  {
    "text": "and my goal is to\nmove around the graph",
    "start": "2084030",
    "end": "2089250"
  },
  {
    "text": "in a way that incurs minimal\nlong-term cost and expected value.",
    "start": "2089250",
    "end": "2094379"
  },
  {
    "start": "2094380",
    "end": "2101970"
  },
  {
    "text": "So that's a good way\nto start figuring out how to do policy evaluation. ",
    "start": "2101970",
    "end": "2110490"
  },
  {
    "text": "So now, in this discrete state\ntransition matrix [INAUDIBLE]",
    "start": "2110490",
    "end": "2115890"
  },
  {
    "text": "this form, I'm going to rewrite\nJ pi as being a function of i,",
    "start": "2115890",
    "end": "2124559"
  },
  {
    "text": "where i is from-- i is drawn from S, some--",
    "start": "2124560",
    "end": "2130230"
  },
  {
    "text": "it's one discrete state. And it's the expected value of\nG-I-N to I [INAUDIBLE] plus 1.",
    "start": "2130230",
    "end": "2144750"
  },
  {
    "start": "2144750",
    "end": "2174310"
  },
  {
    "text": "I should say another funny\nexample I just remembered. So I gave an analogy\nof playing a game.",
    "start": "2174310",
    "end": "2180089"
  },
  {
    "text": "You might look at the\nboard and figure out what's the value of\nbeing in certain states. People think it's relevant\nin your brains too.",
    "start": "2180090",
    "end": "2187270"
  },
  {
    "text": "So there's actually a lot of\nwork in neuroscience these days which probes activity of\ncertain neurons in your brain,",
    "start": "2187270",
    "end": "2194170"
  },
  {
    "text": "and finds neurons that basically\nrespond with the expected value",
    "start": "2194170",
    "end": "2199329"
  },
  {
    "text": "of your cost-to-go function. They have monkeys\ndoing these tasks,",
    "start": "2199330",
    "end": "2205120"
  },
  {
    "text": "where they pull levers or\nblink at the right time, and get certain rewards.",
    "start": "2205120",
    "end": "2210160"
  },
  {
    "text": "And there's neurons\nthat fire correlated with their expected\nreward in ways that are--",
    "start": "2210160",
    "end": "2215410"
  },
  {
    "text": "they design an experiment so\nit doesn't look like something that's correlated with the\naction they're going to choose, but it does look\nlike it's correlated",
    "start": "2215410",
    "end": "2221932"
  },
  {
    "text": "with expected reward. And interestingly,\nwhen they learn--",
    "start": "2221932",
    "end": "2227140"
  },
  {
    "text": "as the monkeys learn\nduring the task, you can actually\nsee that they start making predictions\naccurately when",
    "start": "2227140",
    "end": "2233080"
  },
  {
    "text": "they're close to the reward. They're about to get juice, and\nthen, a few minutes later, they can predict when they're a\nminute away from getting juice.",
    "start": "2233080",
    "end": "2240320"
  },
  {
    "text": "And then, if you look at\nit a couple of days in, they're able to\npredict when they're a half hour from getting\njuice or something like this.",
    "start": "2240320",
    "end": "2245890"
  },
  {
    "text": " I think the structure of trying\nto learn the value function",
    "start": "2245890",
    "end": "2251080"
  },
  {
    "text": "is very real, especially if\nyou're a juice-deprived monkey.",
    "start": "2251080",
    "end": "2257230"
  },
  {
    "text": "So let's continue on here. ",
    "start": "2257230",
    "end": "2266470"
  },
  {
    "text": "How do you compute J\npi, given this equation? ",
    "start": "2266470",
    "end": "2273069"
  },
  {
    "text": "J is a vector now. Well, first of all the dynamic\nprogramming recursion let's",
    "start": "2273070",
    "end": "2278393"
  },
  {
    "text": "us write it like this.  J of ik is the expected\nvalue of taking one step--",
    "start": "2278393",
    "end": "2287950"
  },
  {
    "start": "2287950",
    "end": "2303280"
  },
  {
    "text": "the one step cost plus\nthe discount factor times the future.",
    "start": "2303280",
    "end": "2309670"
  },
  {
    "text": "The reason people choose this\nform for the discount factor is that the Bellman recursion\njust looks like that.",
    "start": "2309670",
    "end": "2316827"
  },
  {
    "text": "You just put a gamma\nin front of everything. ",
    "start": "2316827",
    "end": "2326349"
  },
  {
    "text": "We can take the expected value\nof this with our Markov chain notation and say\nit's the sum over i k",
    "start": "2326350",
    "end": "2333320"
  },
  {
    "text": "plus 1's of Tik ik plus\n1 times g ik, ik plus 1.",
    "start": "2333320",
    "end": "2343800"
  },
  {
    "start": "2343800",
    "end": "2348960"
  },
  {
    "text": "Keep putting pi everywhere\nso we remember that. Pi k plus 1.",
    "start": "2348960",
    "end": "2354839"
  },
  {
    "start": "2354840",
    "end": "2364000"
  },
  {
    "text": "The expected value just is a sum\nover probabilities of getting each of the outcomes.",
    "start": "2364000",
    "end": "2369015"
  },
  {
    "text": " So you can use that\ntransition matrix. And in vector form, since\nI have a finite number",
    "start": "2369015",
    "end": "2380290"
  },
  {
    "text": "of discrete states,\nI can just write that as J is g plus gamma TJ,\nwhere the i-th element of g",
    "start": "2380290",
    "end": "2397600"
  },
  {
    "text": "is Keep my pi's everywhere.",
    "start": "2397600",
    "end": "2420032"
  },
  {
    "text": " Everybody agree\nwith those steps?",
    "start": "2420032",
    "end": "2425500"
  },
  {
    "text": " OK. So what's J-- J pi?",
    "start": "2425500",
    "end": "2432406"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] RUSS TEDRAKE: Mm-hmm.",
    "start": "2432406",
    "end": "2438010"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE]",
    "start": "2438010",
    "end": "2444117"
  },
  {
    "text": "RUSS TEDRAKE: I have to\ngo to a vector form for J, so I just put it over here. I'm saying that the i-th\nelement of the vector g--",
    "start": "2444117",
    "end": "2452140"
  },
  {
    "text": "this is my vector g now-- and the i-th element of my\nvector g has that T in there-- absolutely. ",
    "start": "2452140",
    "end": "2464170"
  },
  {
    "text": "Yep. So it's the expected\nvalue of g there. OK, so what's J pi?",
    "start": "2464170",
    "end": "2469780"
  },
  {
    "start": "2469780",
    "end": "2511660"
  },
  {
    "text": "So lo and behold,\npolicy evaluation",
    "start": "2511660",
    "end": "2519460"
  },
  {
    "text": "on a Markov chain with known\nprobabilities is trivial.",
    "start": "2519460",
    "end": "2526849"
  },
  {
    "text": "It's this. It's almost free to compute. I could tell you exactly\nwhat my long-term cost",
    "start": "2526850",
    "end": "2535570"
  },
  {
    "text": "is going to be just by\nknowing my transition matrix. That's something I think\nwe forget, because we're",
    "start": "2535570",
    "end": "2542200"
  },
  {
    "text": "going to get into\nmodels that look more complicated than\nthat, but remember, if the transition matrix,\nit's trivial to compute",
    "start": "2542200",
    "end": "2549220"
  },
  {
    "text": "the long-term cost\nfor a Markov chain. So let me just show you why\nthat's relevant, for instance.",
    "start": "2549220",
    "end": "2555970"
  },
  {
    "text": " All right, so I told you about\nthis the day the clock stopped.",
    "start": "2555970",
    "end": "2563640"
  },
  {
    "text": "I kept telling you about it\n[INAUDIBLE] And for the record, do you know what\nhappened that day?",
    "start": "2563640",
    "end": "2568920"
  },
  {
    "text": "The clock physically stopped. Michael debugged it. There was a little\npiece of paint",
    "start": "2568920",
    "end": "2574619"
  },
  {
    "text": "that blocked it at\nexactly 3:05 the day I was giving that lecture. That was a hard one\nto catch, to be fair.",
    "start": "2574620",
    "end": "2583120"
  },
  {
    "text": "So one of my favorite models\nof stochastic processes",
    "start": "2583120",
    "end": "2588270"
  },
  {
    "text": "in discrete time,\nfor instance, is taking our rimless wheels,\nour passive walking models,",
    "start": "2588270",
    "end": "2594030"
  },
  {
    "text": "and putting them\non rough terrain. So this is the rimless wheel,\nwhere now, every time it",
    "start": "2594030",
    "end": "2601530"
  },
  {
    "text": "takes a step, the ramp angle is\ndrawn from some distribution.",
    "start": "2601530",
    "end": "2607560"
  },
  {
    "text": "Now, in real life, maybe you\ndon't roll rimless wheels on that kind of slope, but\nthe contention in that paper",
    "start": "2607560",
    "end": "2617400"
  },
  {
    "text": "was that actually, every\nfloor is rough terrain, and you actually have to worry\nabout the stochastic dynamics",
    "start": "2617400",
    "end": "2622440"
  },
  {
    "text": "all the time. And if you want to take-- you can take your\ncompass-gait model and put it on rough\nterrain, and you",
    "start": "2622440",
    "end": "2629940"
  },
  {
    "text": "could take the [? kneed ?] model\nand put it on rough terrain. These are the passive\nthings, so they can't walk on very much rough\nterrain before they fall down.",
    "start": "2629940",
    "end": "2638910"
  },
  {
    "text": "But they can. They can walk on rough terrain.  And then you want to ask\ncomplicated questions",
    "start": "2638910",
    "end": "2645630"
  },
  {
    "text": "about this, maybe. You want to say,\ngiven my terrain was drawn from\nsome distribution,",
    "start": "2645630",
    "end": "2650940"
  },
  {
    "text": "how far should I expect my robot\nto walk before it falls down?",
    "start": "2650940",
    "end": "2656158"
  },
  {
    "text": "That sounds like a hard\nquestion to answer.  It's trivial to\nanswer, actually.",
    "start": "2656158",
    "end": "2663690"
  },
  {
    "text": "So this equation is exactly\nwhat drove that work. We built the transition matrix\non the [INAUDIBLE] map, saying,",
    "start": "2663690",
    "end": "2671850"
  },
  {
    "text": "given it's passive-- there's no actions\nto choose from-- given it's passive, what's\nthe probability of being",
    "start": "2671850",
    "end": "2677286"
  },
  {
    "text": "in this new state,\ngiven the terrain's drawn from some distribution\ngiven it's at a current state. The cost function was\n1 if it keeps taking",
    "start": "2677287",
    "end": "2684600"
  },
  {
    "text": "a step, 0 if it fell over. And you compute this, and it--",
    "start": "2684600",
    "end": "2690632"
  },
  {
    "text": "what does it tells you? It tells you the expected number\nof steps until you fall down-- period.",
    "start": "2690632",
    "end": "2695730"
  },
  {
    "text": "On shot. Simple calculation. It's so simple. The bad part is you have\nto discretize your state",
    "start": "2695730",
    "end": "2701912"
  },
  {
    "text": "space to do it. But if you're willing to\ndiscretize your state space, then you can make very long-term\npredictions about your model",
    "start": "2701912",
    "end": "2708780"
  },
  {
    "text": "with-- just like that,\nto the point where we are trying to say that people\nwho talk about stability--",
    "start": "2708780",
    "end": "2716482"
  },
  {
    "text": "people are coming up with\nmetrics for stability and walking systems. They say, why not just do this? Why not actually\ncompute, given some model",
    "start": "2716482",
    "end": "2723330"
  },
  {
    "text": "of the terrain, how\nmany steps you'd expect to take until it falls down? That's what you'd\nlike to compute, and it's not hard to compute,\nso you should do that.",
    "start": "2723330",
    "end": "2731279"
  },
  {
    "text": "So that's a clear place where\npolicy evaluation by itself-- there's lots of cases where\nyou have a robot that's",
    "start": "2731280",
    "end": "2738210"
  },
  {
    "text": "doing something, it's\ngot a control system, and you just want to\nverify how well it works. If you're trying to verify it\nan expected value, it's easy.",
    "start": "2738210",
    "end": "2745619"
  },
  {
    "text": "Just do the Monte Carlo-- or\nsorry-- the Markov chain thing. ",
    "start": "2745620",
    "end": "2755401"
  },
  {
    "text": "But what happens if\nI don't have a model? That's what we're supposed\nto be talking about today. Can we do the same thing\nif we don't have a model?",
    "start": "2755402",
    "end": "2764400"
  },
  {
    "text": "I had to know T. I had to\nknow the-- all the transition probabilities in order\nto make that calculation.",
    "start": "2764400",
    "end": "2770325"
  },
  {
    "text": "What happens if we don't have a\nmodel-- we just have a robot we can run a bunch of times? How do you do it? ",
    "start": "2770325",
    "end": "2835470"
  },
  {
    "text": "What would you do,\nif I asked you-- I say, I like your robot. I want to know how long it\ntends to run before it fails.",
    "start": "2835470",
    "end": "2842337"
  },
  {
    "text": "How would you do it? ",
    "start": "2842337",
    "end": "2848226"
  },
  {
    "text": "How would you do it?  There's an easy answer.",
    "start": "2848227",
    "end": "2854780"
  },
  {
    "text": "You could run it a bunch of\ntimes and take an average. ",
    "start": "2854780",
    "end": "2861330"
  },
  {
    "text": "We know that these value\nfunctions are state-dependent, so it's a little more\npainful than that.",
    "start": "2861330",
    "end": "2866630"
  },
  {
    "text": "Technically, you're\ngoing to have to run it a bunch of times from\nevery single initial condition,",
    "start": "2866630",
    "end": "2871773"
  },
  {
    "text": "but you could do that. And actually, that's\nnot totally crazy. ",
    "start": "2871773",
    "end": "2895820"
  },
  {
    "text": "So I want to know\nhow much cost I'm going to incur-- in the case\nof the walking robot, how many steps it's going to take\non average before it falls down.",
    "start": "2895820",
    "end": "2903049"
  },
  {
    "text": "First thing to try-- don't have to know the\ntransition matrices-- just run it a bunch of times.",
    "start": "2903050",
    "end": "2908420"
  },
  {
    "text": "So if I say Jn i, the n-th\ntime I run my robot, I incur--",
    "start": "2908420",
    "end": "2924920"
  },
  {
    "text": "I just keep track of the cost. I keep track of how\nmany steps it took. I keep track of how\nmuch gold it found--",
    "start": "2924920",
    "end": "2931007"
  },
  {
    "text": "whatever your cost function is. ",
    "start": "2931008",
    "end": "2949430"
  },
  {
    "text": "The thing I'm trying to\nestimate is the expected value of that long-term cost.",
    "start": "2949430",
    "end": "2955530"
  },
  {
    "text": "But any one trial-- I get this thing out\nas a random variable. ",
    "start": "2955530",
    "end": "2963620"
  },
  {
    "text": "I could take the expected\nvalue of the random variable. I can make a nice\nestimate of J pi i",
    "start": "2963620",
    "end": "2970760"
  },
  {
    "text": "by just running it a bunch\nof times and taking--",
    "start": "2970760",
    "end": "2975830"
  },
  {
    "start": "2975830",
    "end": "2984700"
  },
  {
    "text": "doesn't sound very\nelegant, but it works. ",
    "start": "2984700",
    "end": "2992284"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] RUSS TEDRAKE: What? ",
    "start": "2992284",
    "end": "2998490"
  },
  {
    "text": "Sum over k. Good. Thank you, thank you. Good. Sum over k.",
    "start": "2998490",
    "end": "3004730"
  },
  {
    "text": "[INAUDIBLE] you've corrected\nboth simultaneously.",
    "start": "3004730",
    "end": "3011395"
  },
  {
    "start": "3011395",
    "end": "3023790"
  },
  {
    "text": "OK, so a couple\nof nuances here-- so first of all, I have an\ninfinite horizon cost function.",
    "start": "3023790",
    "end": "3032309"
  },
  {
    "text": "So this is only going\nto be an approximation, because I'm not going to\nrun this forever 10 times. I'm going to run it for some\nfinite duration 10 times.",
    "start": "3032310",
    "end": "3040050"
  },
  {
    "text": "So in practice,\nI'm actually going to run something\nthat's big number.",
    "start": "3040050",
    "end": "3046550"
  },
  {
    "start": "3046550",
    "end": "3051600"
  },
  {
    "text": "But that's OK because\nthis discount factor means that a finite trial\napproximation should",
    "start": "3051600",
    "end": "3057698"
  },
  {
    "text": "be a pretty good estimate\nof the long-term. ",
    "start": "3057698",
    "end": "3063330"
  },
  {
    "text": "And if I run it from initial\ncondition i long enough, then I should be able\nto take an average",
    "start": "3063330",
    "end": "3070200"
  },
  {
    "text": "and get the expected\n[INAUDIBLE].. ",
    "start": "3070200",
    "end": "3081770"
  },
  {
    "text": "There's lots of ways you\ncan do that kind of thing",
    "start": "3081770",
    "end": "3089210"
  },
  {
    "text": "if you don't want to do all\nthe bookkeeping of remembering where you've been.",
    "start": "3089210",
    "end": "3095690"
  },
  {
    "text": "You don't have to remember\nall of these things. You can do an online\nversion, incremental.",
    "start": "3095690",
    "end": "3101089"
  },
  {
    "text": "You can say that\nmy J hat is just-- my J hat pi is just J hat pi--",
    "start": "3101090",
    "end": "3109505"
  },
  {
    "start": "3109505",
    "end": "3123829"
  },
  {
    "text": "I can guess an initial\nJ hat, and then, every time I get\na new trial, I'll",
    "start": "3123830",
    "end": "3130609"
  },
  {
    "text": "just move my estimate\ntowards that trial.",
    "start": "3130610",
    "end": "3135680"
  },
  {
    "text": "And this is actually\nan online version that approximates that in batch.",
    "start": "3135680",
    "end": "3141980"
  },
  {
    "text": "This is just a\nstandard [INAUDIBLE] that you can do\nit more carefully.",
    "start": "3141980",
    "end": "3147109"
  },
  {
    "text": " I could choose these to\nbe a perfect weighting",
    "start": "3147110",
    "end": "3153570"
  },
  {
    "text": "but in general, this is actually\na pretty good approximation, as the number of trials goes\nup, to this sum without keeping",
    "start": "3153570",
    "end": "3160610"
  },
  {
    "text": "track of every J. Every time, I'm just\ngoing to do moving average towards the new point, and\nby changing a small amount,",
    "start": "3160610",
    "end": "3166550"
  },
  {
    "text": "it will converge. This is a low-pass filter. That's another way to say it. It's a low-pass filter\nthat tries to get me to--",
    "start": "3166550",
    "end": "3173180"
  },
  {
    "text": "the mean of the J\nsamples I'm getting in.",
    "start": "3173180",
    "end": "3178400"
  },
  {
    "start": "3178400",
    "end": "3185680"
  },
  {
    "text": "So that gets rid of a\nlittle bit of bookkeeping. There's other things you can do. Now, here's a really cool one. ",
    "start": "3185680",
    "end": "3195130"
  },
  {
    "text": "Think about this, and tell me\nif you think it's possible. I'm going to tell you\nin a minute how that--",
    "start": "3195130",
    "end": "3202240"
  },
  {
    "text": "if I have two policies, I can--",
    "start": "3202240",
    "end": "3212100"
  },
  {
    "text": "say, pi 1 and pi 2-- ",
    "start": "3212100",
    "end": "3234189"
  },
  {
    "text": "Do you believe that?  It's going to take a\nlittle bit more machinery,",
    "start": "3234189",
    "end": "3240570"
  },
  {
    "text": "but just to see where we go. Say I have two control systems.",
    "start": "3240570",
    "end": "3246330"
  },
  {
    "text": "I have the one that is\nrisky, and I ran it once, and the thing fell down.",
    "start": "3246330",
    "end": "3253072"
  },
  {
    "text": "So I don't actually want\nto run that 100 times. I might break my robot. Let's say I've got\na different policy",
    "start": "3253072",
    "end": "3258660"
  },
  {
    "text": "that I like a little better. It's a little safer\nto do evaluations on. Can you imagine running\nthe safe policy, let's say,",
    "start": "3258660",
    "end": "3265410"
  },
  {
    "text": "to learn about the risky policy? ",
    "start": "3265410",
    "end": "3270480"
  },
  {
    "text": "That's pretty cool idea, right?  What is wrong with this?",
    "start": "3270480",
    "end": "3276270"
  },
  {
    "start": "3276270",
    "end": "3288390"
  },
  {
    "text": "Typically done\nwith a q function. I'll show you how to\nsay that in a second. ",
    "start": "3288390",
    "end": "3296283"
  },
  {
    "text": "So there's lots of\nways you can do that. You can run trials,\nyou can keep averages,",
    "start": "3296283",
    "end": "3302322"
  },
  {
    "text": "you can try to learn about one\ntrial by learning the other. What the fundamental\nidea here is is that it requires\nstochasticity.",
    "start": "3302322",
    "end": "3309910"
  },
  {
    "text": "You need that, in\npolicy, pi 1 and pi 2 have to change--\ntake the same actions",
    "start": "3309910",
    "end": "3317140"
  },
  {
    "text": "with some non-zero probability. Pi 2 might be my risky policy,\nand every once in a while,",
    "start": "3317140",
    "end": "3323230"
  },
  {
    "text": "with some small probability, it\ntakes a safe action, let's say. And pi 1 is my safe policy,\nbut every once in a while, it",
    "start": "3323230",
    "end": "3331080"
  },
  {
    "text": "takes a risky action. As long as these things\nhave some non-zero overlap in probability space,\nthen I can actually",
    "start": "3331080",
    "end": "3338230"
  },
  {
    "text": "learn about what\nit would have been to do the more risky\nthing by taking the more conservative thing.",
    "start": "3338230",
    "end": "3345060"
  },
  {
    "text": "So policy evaluation's\na really nice tool.  But this feels slow.",
    "start": "3345060",
    "end": "3351100"
  },
  {
    "text": "The Monte Carlo\nthing feels slow-- feels like I got to\nrun a lot of trials from a lot of different\ninitial conditions.",
    "start": "3351100",
    "end": "3357280"
  },
  {
    "text": "And now you tell me\nwhat the cost-to-go is from this initial\ncondition, and let's say I try this initial condition. What do I do?",
    "start": "3357280",
    "end": "3362810"
  },
  {
    "text": "Do I just have to start over\nand run trials from the get-go again? Well, that doesn't\nseem very satisfying.",
    "start": "3362810",
    "end": "3368480"
  },
  {
    "text": " Approach number two\nis bootstrapping.",
    "start": "3368480",
    "end": "3374358"
  },
  {
    "start": "3374358",
    "end": "3391470"
  },
  {
    "text": "I call it bootstrapping. If I learned about the cost\nof being in this state,",
    "start": "3391470",
    "end": "3399026"
  },
  {
    "text": "and I spent a long time learning\nabout the cost-to-go of being in this state, and then\nI go back and ask what's the cost of being in\nthis state, if this one",
    "start": "3399027",
    "end": "3407400"
  },
  {
    "text": "transitions into\nthis one, then I should be able to reuse what\nI learned about the state to make it faster to\nlearn about that state.",
    "start": "3407400",
    "end": "3414390"
  },
  {
    "text": "I didn't really plan to do it\nwith the steps on the floor, but I hope that makes sense.",
    "start": "3414390",
    "end": "3419428"
  },
  {
    "text": "Maybe I could do it on a graph. That's better, yeah? Let's say I figured out\nwhat J pi of this state is--",
    "start": "3419428",
    "end": "3430440"
  },
  {
    "text": "because I went from\nhere, and I went around, and I did my stuff,\nand I learned pretty much what there is to\nlearn about here [INAUDIBLE]",
    "start": "3430440",
    "end": "3436295"
  },
  {
    "text": "policy. And now I want to\nknow about this state.",
    "start": "3436295",
    "end": "3441853"
  },
  {
    "text": "Well, I should be\nable to reuse the fact that I've learned about\nthat to help me learn this more quickly--",
    "start": "3441853",
    "end": "3449730"
  },
  {
    "text": "reasonable idea.  Using your estimate to inform\nyour future estimates is",
    "start": "3449730",
    "end": "3457800"
  },
  {
    "text": "an idea about\nbootstrapping, reusing-- building on your current guess\nto build a better future guess.",
    "start": "3457800",
    "end": "3465060"
  },
  {
    "text": "And here's how it could look\nin the optimal control policy evaluation sense.",
    "start": "3465060",
    "end": "3470670"
  },
  {
    "start": "3470670",
    "end": "3485000"
  },
  {
    "text": "What if I said my\nonline rule used",
    "start": "3485000",
    "end": "3491330"
  },
  {
    "text": "to be this, where I've got\nsome estimate J pi hat?",
    "start": "3491330",
    "end": "3498380"
  },
  {
    "text": "I'm going to run from 0\nto some very large number to estimate this, and\nthen make the update.",
    "start": "3498380",
    "end": "3504260"
  },
  {
    "text": "What if, instead, I\njust took a single step and I did this update? ",
    "start": "3504260",
    "end": "3548192"
  },
  {
    "text": "Does that make sense to you? ",
    "start": "3548193",
    "end": "3560730"
  },
  {
    "text": "Let's say I ask you to guess\nthe long-term cost here. Instead of running all the\nway to the end, what if I just",
    "start": "3560730",
    "end": "3568710"
  },
  {
    "text": "run a single step and\nthen use as my cost my estimate for this,\nthe cost of going here",
    "start": "3568710",
    "end": "3577500"
  },
  {
    "text": "plus the gamma times the\ncost of doing all that? It's just using this one-step\ncost as an estimate for when I",
    "start": "3577500",
    "end": "3595440"
  },
  {
    "text": "was going J-N of ik plus 1-- or sorry, J-N of ik.",
    "start": "3595440",
    "end": "3600600"
  },
  {
    "text": " Does that makes sense?",
    "start": "3600600",
    "end": "3607430"
  },
  {
    "text": "If I find myself in a lot of\ndifferent initial conditions, I could take one step\nand then use my guess for the cost-to-go from that\nstep to the rest of the time.",
    "start": "3607430",
    "end": "3616290"
  },
  {
    "text": "Now, this starts feeling\na lot more appealing, actually, because now\nI don't have to think--",
    "start": "3616290",
    "end": "3621660"
  },
  {
    "text": "this actually got rid of\nthat whole episodic problem. I don't have to go in\nand run some fixed length",
    "start": "3621660",
    "end": "3627750"
  },
  {
    "text": "trial to approximate\nthe long-term thing. I just take a single step,\nuse this as my estimate,",
    "start": "3627750",
    "end": "3636390"
  },
  {
    "text": "and I can just keep moving\nthrough my Markov chain. I don't have to ever reset. And potentially, if I\nvisit states often enough--",
    "start": "3636390",
    "end": "3644798"
  },
  {
    "text": "I won't get into all\nthe details-- roughly, it involves that\nMarkov chain being-- having ergodicity.",
    "start": "3644798",
    "end": "3651630"
  },
  {
    "text": "you have to be able to\nvisit all the states with some non-zero\nprobability as you go along. But if you visit the states--\neach state infinitely often",
    "start": "3651630",
    "end": "3658740"
  },
  {
    "text": "is roughly the thing--\nthen this actually will converge to J pi of ik.",
    "start": "3658740",
    "end": "3674619"
  },
  {
    "start": "3674620",
    "end": "3705540"
  },
  {
    "text": "So the ergodicity is actually\nbad news for my walking robot, because if my walking\nrobot falls down,",
    "start": "3705540",
    "end": "3710897"
  },
  {
    "text": "I'm going have to\npick it back up if I want to get ergodicity back. There are robots that\ndon't visit every state",
    "start": "3710897",
    "end": "3715930"
  },
  {
    "text": "every arbitrarily often. But in the Markov\nchain sense, that doesn't seem like\nsuch a bad assumption.",
    "start": "3715930",
    "end": "3722320"
  },
  {
    "text": "And if I'm willing to take\nmy robot when it falls down and pick it back up-- which, by the way,\nis about how I spent the last year of my PhD--",
    "start": "3722320",
    "end": "3728410"
  },
  {
    "text": " then actually, I can\nget ergodicity back. ",
    "start": "3728410",
    "end": "3737590"
  },
  {
    "text": "OK, cool-- so that\nmakes sense, right? I'm going to use my existing\nestimate of the cost-to-go",
    "start": "3737590",
    "end": "3744460"
  },
  {
    "text": "to bootstrap my algorithm for\nestimating the cost-to-go. Yeah? AUDIENCE: Does the\ntransition [INAUDIBLE] come into play at all?",
    "start": "3744460",
    "end": "3750760"
  },
  {
    "text": "RUSS TEDRAKE: It\ndoes, because I'm getting this from sampled data. So this is actually drawn.",
    "start": "3750760",
    "end": "3756628"
  },
  {
    "text": "The expected value of this\nupdate does the right thing.  So this update doesn't\nhave it, because this",
    "start": "3756628",
    "end": "3763440"
  },
  {
    "text": "is from a real trials. But you should think\nabout this as a sample from the real distribution.",
    "start": "3763440",
    "end": "3770535"
  },
  {
    "text": "Now, that's actually a\nreally good way for me to lead into the next step. These algorithms tend to be\na lot faster in practice than",
    "start": "3770535",
    "end": "3779049"
  },
  {
    "text": "those algorithms-- not only are\nthey a little bit more elegant, because you don't have to reset\nand run finite-length trials--",
    "start": "3779050",
    "end": "3784630"
  },
  {
    "text": "they tend to be a lot faster. And the reason for that is\nthis here is really the--",
    "start": "3784630",
    "end": "3792280"
  },
  {
    "text": "it has the expected value of\nfuture costs built into it. ",
    "start": "3792280",
    "end": "3798029"
  },
  {
    "text": "Let me say that in the pictures. There's two ways, I\ncould estimate this.",
    "start": "3798030",
    "end": "3803520"
  },
  {
    "text": "I could get here and then\nI could take a single path. Well, this one is not\nrich enough for me",
    "start": "3803520",
    "end": "3808590"
  },
  {
    "text": "to make my point\nhere, but OK-- so I could take a single\npath through here and get a single sample\nestimating the long-term cost.",
    "start": "3808590",
    "end": "3818460"
  },
  {
    "text": "But if I instead use J pi,\nJ pi is the expected value of going around\nand living in this.",
    "start": "3818460",
    "end": "3825330"
  },
  {
    "text": "So by using this\nupdate to bootstrap, or if I just take\none step from here, I get for free the expected\nvalue of living over here",
    "start": "3825330",
    "end": "3834000"
  },
  {
    "text": "for a long time. Does that make sense?",
    "start": "3834000",
    "end": "3839369"
  },
  {
    "text": "So J is building up a map\nof the expected value, because it's visiting things\noften and it's-- drew this online algorithm with\nthis low-pass filter.",
    "start": "3839370",
    "end": "3847410"
  },
  {
    "text": "He's basically doing an\nexpected value calculation. By using my low-pass\nfiltered [INAUDIBLE] in here,",
    "start": "3847410",
    "end": "3854550"
  },
  {
    "text": "it's also-- it's getting the reward of-- maybe you could just say\nit's filtering faster. That's actually not a bad\nway to think about it.",
    "start": "3854550",
    "end": "3860640"
  },
  {
    "text": "I've already got\nthis thing filtered, so this one filters faster. That's a pretty reasonable way\nto think about it, actually.",
    "start": "3860640",
    "end": "3868510"
  },
  {
    "text": "OK.  So this quantity\nhere in the brackets,",
    "start": "3868510",
    "end": "3875020"
  },
  {
    "text": "this whole guy right here,\nit's a very important quantity,",
    "start": "3875020",
    "end": "3884660"
  },
  {
    "text": "it comes up a lot. It's called the temporal\ndifference error. ",
    "start": "3884660",
    "end": "3898770"
  },
  {
    "text": "It's the difference that\nI get from executing my policy for one step and then\nusing the long-term estimate",
    "start": "3898770",
    "end": "3906750"
  },
  {
    "text": "compared to what I have\nis my long-term estimate, temporal difference error.",
    "start": "3906750",
    "end": "3913530"
  },
  {
    "text": "Now, if the system\nwas deterministic and I had already converged,\nthen that temporal difference there should be 0, because\nthis thing should be exactly--",
    "start": "3913530",
    "end": "3923670"
  },
  {
    "text": "predict the long-term thing. If the system's stochastic, then\nthis temporal difference error",
    "start": "3923670",
    "end": "3929130"
  },
  {
    "text": "should be 0, on average. ",
    "start": "3929130",
    "end": "3935080"
  },
  {
    "text": "It's comparing my\ncost-to-go from ik, given my 1 step plus the\ncost-to-go from ik plus 1.",
    "start": "3935080",
    "end": "3942130"
  },
  {
    "text": "So those things-- you want\nthose to match, right? You want that my 1 step\nplus long-term prediction",
    "start": "3942130",
    "end": "3950020"
  },
  {
    "text": "should match my\nlong-term prediction, if things are right. They should match\nan expected value.",
    "start": "3950020",
    "end": "3956090"
  },
  {
    "text": "So that thing's called the\ntemporal difference error, and it's an important quantity\nin reinforcement learning. ",
    "start": "3956090",
    "end": "3974010"
  },
  {
    "text": "It makes sense to\nwrite that down. That's a reasonable\nestimate for J.",
    "start": "3974010",
    "end": "3980124"
  },
  {
    "text": "n would be to take one step\nand then do the other one, but there's--",
    "start": "3980124",
    "end": "3985800"
  },
  {
    "text": "that seems a little\nbit arbitrary. Why don't I just do one\nstep and then use my lookup?",
    "start": "3985800",
    "end": "3990952"
  },
  {
    "text": "Thi sis the way Rich says it. Why not do two steps and\nthen use my value function",
    "start": "3990952",
    "end": "3996390"
  },
  {
    "text": "to look up? Or three steps-- why not\ntake, similarly, three steps, and then use that\nto look it up--",
    "start": "3996390",
    "end": "4001970"
  },
  {
    "text": "or 14 steps or\nsomething like that. Why should I arbitrarily\npick this one real data",
    "start": "4001970",
    "end": "4009710"
  },
  {
    "text": "and then look ahead, instead\nof two real pieces of data and my look ahead? ",
    "start": "4009710",
    "end": "4016732"
  },
  {
    "text": "Well, there's no reason\nthat you actually have to pick like that. ",
    "start": "4016732",
    "end": "4029020"
  },
  {
    "text": "Can I just write the\ninside part here? I could have estimated\nJn ik as gik ik",
    "start": "4029020",
    "end": "4040720"
  },
  {
    "text": "plus 1 plus gamma gik plus 1\nik plus 2 plus gamma squared",
    "start": "4040720",
    "end": "4052540"
  },
  {
    "text": "J pi at ik plus 2.",
    "start": "4052540",
    "end": "4058480"
  },
  {
    "text": "That's a perfectly\ngood approximation too. I could have done three-step. I could have done four-step.",
    "start": "4058480",
    "end": "4063922"
  },
  {
    "text": "AUDIENCE: [INAUDIBLE] RUSS TEDRAKE: Say it again. AUDIENCE: [INAUDIBLE] RUSS TEDRAKE: Yeah I. I\nhaven't been writing it",
    "start": "4063923",
    "end": "4070600"
  },
  {
    "text": "that way because-- yeah. I would have been fine\nwriting it that way.",
    "start": "4070600",
    "end": "4076840"
  },
  {
    "text": "At some point, I decided\nto not write pi there, and I'll just stay consistent\nby not writing that. ",
    "start": "4076840",
    "end": "4086880"
  },
  {
    "text": "OK, so Rich [INAUDIBLE] came\nup with a clever algorithm",
    "start": "4086880",
    "end": "4092400"
  },
  {
    "text": "that's-- basically allows you to\nseamlessly pick between the one",
    "start": "4092400",
    "end": "4097409"
  },
  {
    "text": "step, two step, three step,\nn-step look ahead with a single knob.",
    "start": "4097410",
    "end": "4102839"
  },
  {
    "text": "And it works. It's called the TD\nlambda algorithm.",
    "start": "4102840",
    "end": "4109829"
  },
  {
    "start": "4109830",
    "end": "4120410"
  },
  {
    "text": "And the basic idea\nis that you want to combine a lot of\nthese different updates",
    "start": "4120410",
    "end": "4133180"
  },
  {
    "text": "into a single update. It sounds really\nbizarre [INAUDIBLE] so let me just say it. Let's say I call my\nestimate J of Jn,",
    "start": "4133180",
    "end": "4147068"
  },
  {
    "text": "with M step look ahead of ik. ",
    "start": "4147069",
    "end": "4155528"
  },
  {
    "text": "M equals 0 to M gamma of M\ngik plus M ik plus M plus 1.",
    "start": "4155529",
    "end": "4166809"
  },
  {
    "start": "4166810",
    "end": "4179939"
  },
  {
    "text": "Big M. This is a big M. Big M.\nEverything else is little m's.",
    "start": "4179939",
    "end": "4188580"
  },
  {
    "text": " That was the one step.",
    "start": "4188580",
    "end": "4193890"
  },
  {
    "text": "This is the two step. In general, this is\nthe M step look ahead. ",
    "start": "4193890",
    "end": "4200780"
  },
  {
    "text": "So it turns out there's\nactually an efficient way to compute this. ",
    "start": "4200780",
    "end": "4223239"
  },
  {
    "text": "Let's call it something else-- p, p, p. ",
    "start": "4223240",
    "end": "4235290"
  },
  {
    "text": "This one takes a\nlittle time to digest. But it turns out it's pretty\nefficient to calculate",
    "start": "4235290",
    "end": "4245910"
  },
  {
    "text": "a weighted sum of the one\nstep, two step, three step--",
    "start": "4245910",
    "end": "4252930"
  },
  {
    "text": "onto forever--\nsum of look-aheads",
    "start": "4252930",
    "end": "4258840"
  },
  {
    "text": "parameterized by another\nparameter, lambda. ",
    "start": "4258840",
    "end": "4265020"
  },
  {
    "text": "So when lambda's 1,\nthis thing turns out to be basically\ndoing Monte Carlo.",
    "start": "4265020",
    "end": "4272969"
  },
  {
    "text": "And when lambda's 0,\nthis thing basically is doing just the\none-step look ahead.",
    "start": "4272970",
    "end": "4278430"
  },
  {
    "text": "And when lambda is\nsomewhere in between, it's doing some look\nahead using a few steps.",
    "start": "4278430",
    "end": "4287610"
  },
  {
    "text": "Does that makes sense at all? It's a lot of terms\nflying around here. ",
    "start": "4287610",
    "end": "4293880"
  },
  {
    "text": "Even if you don't\ncompletely love that, just think of my estimate,\nJ of lambda being awaited,",
    "start": "4293880",
    "end": "4304710"
  },
  {
    "text": "basically something that,\nwhere if lambda is 1, it's going to be the very\nlong-term look ahead.",
    "start": "4304710",
    "end": "4310470"
  },
  {
    "text": "If lambda is 0, it's going to be\nthe very short-term look ahead. And there's a\ncontinuum in between, a continuous knob I can\nturn to say how far I'm",
    "start": "4310470",
    "end": "4317100"
  },
  {
    "text": "going to look ahead into\nthe future as my estimate that I'm going to\nuse in that TD error. ",
    "start": "4317100",
    "end": "4342100"
  },
  {
    "text": "And there's a whole\ngamut in between.  AUDIENCE: [INAUDIBLE]",
    "start": "4342100",
    "end": "4348945"
  },
  {
    "text": "RUSS TEDRAKE: Can\nI say it again? AUDIENCE: [INAUDIBLE] RUSS TEDRAKE: Or can I read it?",
    "start": "4348945",
    "end": "4354230"
  },
  {
    "text": "1 minus lambda i just a\nthe normalization factor. p equals 1 to infinity.",
    "start": "4354230",
    "end": "4360620"
  },
  {
    "text": "Lambda the p minus 1 J p--",
    "start": "4360620",
    "end": "4367045"
  },
  {
    "text": "where this is the\np step look ahead. ",
    "start": "4367045",
    "end": "4377460"
  },
  {
    "text": "So this is a very\nfamous algorithm-- the TD lambda algorithm-- which allows you to\ndo policy evaluation",
    "start": "4377460",
    "end": "4387630"
  },
  {
    "text": "without knowing the\ntransition matrix, doing bootstrapping\nor Monte Carlo",
    "start": "4387630",
    "end": "4394110"
  },
  {
    "text": "in a simple single framework\nwith just a parameter lambda to evaluate.",
    "start": "4394110",
    "end": "4400580"
  },
  {
    "text": "So it's a tweak. And it turns out it uses\nan eligibility trace,",
    "start": "4400580",
    "end": "4408619"
  },
  {
    "text": "just like in reinforce. Did you get the\neligibility traces, John? AUDIENCE: [INAUDIBLE]",
    "start": "4408620",
    "end": "4415190"
  },
  {
    "text": "RUSS TEDRAKE: OK. Well, that's fine. So it turns out to have\na really simple form.",
    "start": "4415190",
    "end": "4421730"
  },
  {
    "text": " I'll write it, because\nit's so simple,",
    "start": "4421730",
    "end": "4427420"
  },
  {
    "text": "but it'll also be\nin the notes, if you want to spend more\ntime with it here. ",
    "start": "4427420",
    "end": "4501820"
  },
  {
    "text": "OK, two observations--\nfirst of all, this looks no harder [INAUDIBLE]\nthan the original version I had,",
    "start": "4501820",
    "end": "4507190"
  },
  {
    "text": "pretty much. It just requires one\nextra variable, which is this eligibility trace.",
    "start": "4507190",
    "end": "4514120"
  },
  {
    "text": "What does the eligibility\ntrace look like? ",
    "start": "4514120",
    "end": "4528110"
  },
  {
    "text": "OK. It starts off at 0. There's an element for\nevery note in the graph.",
    "start": "4528110",
    "end": "4536240"
  },
  {
    "text": "Every time I visit the graph--\nthat node in the graph, I--",
    "start": "4536240",
    "end": "4541550"
  },
  {
    "text": "it goes up by 1,\nand then it starts",
    "start": "4541550",
    "end": "4546929"
  },
  {
    "text": "forgetting based on\ngamma and lambda,",
    "start": "4546930",
    "end": "4553110"
  },
  {
    "text": "as this discount factor. And then, the next time I\nvisit it, it goes up by 1. If I visit it a lot, it\ncan build up like this.",
    "start": "4553110",
    "end": "4559830"
  },
  {
    "text": "It's just a trace of memory\nof when I visited this cell.",
    "start": "4559830",
    "end": "4566180"
  },
  {
    "text": "Does that makes sense,\nthis dynamics here? Every time I visit the\ncell, it goes up by 1, and always, it's going\ndown exponentially.",
    "start": "4566180",
    "end": "4575670"
  },
  {
    "text": "It turns out, if you\njust remember that, the way that you've\nvisited cells in the past,",
    "start": "4575670",
    "end": "4582150"
  },
  {
    "text": "decade by this lambda-- as well\nas gamma-- but this lambda-- which is the new term--",
    "start": "4582150",
    "end": "4588210"
  },
  {
    "text": "then it's enough to [INAUDIBLE]\nthis trivial update here, scaled by the--",
    "start": "4588210",
    "end": "4594360"
  },
  {
    "text": "how often I visited\nthat cell recently. Is it enough to accomplish this\nseemingly bizarre combination",
    "start": "4594360",
    "end": "4601500"
  },
  {
    "text": "of short and\nlong-term look-aheads. ",
    "start": "4601500",
    "end": "4606739"
  },
  {
    "text": "So it's a really simple,\nreally beautiful algorithm. Just remember how-- when\nI visited these cells,",
    "start": "4606740",
    "end": "4613489"
  },
  {
    "text": "and then make this TD error\nupdate scaled by that,",
    "start": "4613490",
    "end": "4619790"
  },
  {
    "text": "and I've got the TD\nlambda algorithm. ",
    "start": "4619790",
    "end": "4626550"
  },
  {
    "text": "And what people can do is they-- people can prove that TD lambda\nconverges to the TD lambda",
    "start": "4626550",
    "end": "4638870"
  },
  {
    "text": "update that J hat will go to J\npi from any initial conditions.",
    "start": "4638870",
    "end": "4646260"
  },
  {
    "text": "So you can just guess J\nrandomly to begin with. And if I run it, as I visit all\nthese states arbitrarily often,",
    "start": "4646260",
    "end": "4652730"
  },
  {
    "text": "it still makes that\nergodicity assumption. Then I'll get my\npolicy evaluation out.",
    "start": "4652730",
    "end": "4659480"
  },
  {
    "text": "That's really cool--\nsimple algorithm. Now, what people also realize\nis that, when you start out,",
    "start": "4659480",
    "end": "4667310"
  },
  {
    "text": "and J is randomly initialized,\nthen it makes a lot of sense to set lambda close to 1,\nbecause bootstrapping has less",
    "start": "4667310",
    "end": "4676863"
  },
  {
    "text": "value when I just start out. My estimate is\nbad everywhere, so why should I use my bad\nestimate as my predictor?",
    "start": "4676863",
    "end": "4684940"
  },
  {
    "text": "So you start off-- you\nkeep lambda close to 1. It does very long-term. It does more Monte\nCarlo style updates.",
    "start": "4684940",
    "end": "4690750"
  },
  {
    "text": "And as this estimate\nstarts converging to the good estimate, you\nstart turning lambda down.",
    "start": "4690750",
    "end": "4696780"
  },
  {
    "text": "And with a cleverly\ntuned timing of lambda, you can get very fast\nconvergence compared",
    "start": "4696780",
    "end": "4702329"
  },
  {
    "text": "to the Monte Carlo algorithms. You more and more bootstrap.",
    "start": "4702330",
    "end": "4708115"
  },
  {
    "text": " Excellent. Well, time's up.",
    "start": "4708115",
    "end": "4713159"
  },
  {
    "text": "The clock is still moving\ntoday, so I have to stop. So the really cool thing--",
    "start": "4713160",
    "end": "4718590"
  },
  {
    "text": " we only talked about\npolicy evaluation today.",
    "start": "4718590",
    "end": "4724480"
  },
  {
    "text": "The next step is, how\ndo you do these value methods to improve your policy? And it turns, out in many cases,\nif you make a current estimate",
    "start": "4724480",
    "end": "4732730"
  },
  {
    "text": "of your value function\nand then, on every step, you try to do the greedy\npolicy, epsilon greedy policy,",
    "start": "4732730",
    "end": "4739840"
  },
  {
    "text": "you basically-- you mostly\nexploit your current estimate of the value function, then\nyou can still prove that these",
    "start": "4739840",
    "end": "4745000"
  },
  {
    "text": "things-- at least on the\ngrid, the Markov chain case-- can get to their optimal-- the optimal value function\nand the optimal policy.",
    "start": "4745000",
    "end": "4752985"
  },
  {
    "text": "So we'll finish\nthat up next time and get into the more\ninteresting-- get rid of these Markov chains to try\nto get back to the real world.",
    "start": "4752985",
    "end": "4759290"
  },
  {
    "text": "Good. OK, see you Tuesday. ",
    "start": "4759290",
    "end": "4764000"
  }
]