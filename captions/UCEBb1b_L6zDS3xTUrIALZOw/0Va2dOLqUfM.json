[
  {
    "text": " The following content is\nprovided under a Creative Commons license. Your support will help\nMIT OpenCourseWare",
    "start": "0",
    "end": "6090"
  },
  {
    "text": "continue to offer high quality\neducational resources for free. To make a donation or to\nview additional materials",
    "start": "6090",
    "end": "12720"
  },
  {
    "text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare at ocw.mit.edu.",
    "start": "12720",
    "end": "17880"
  },
  {
    "text": " PROFESSOR: So I'm using\na few things here, right?",
    "start": "17880",
    "end": "24770"
  },
  {
    "text": "I'm using the fact that\nKL is non-negative. But KL is equal to 0 when I\ntake twice the same argument.",
    "start": "24770",
    "end": "31130"
  },
  {
    "text": "So I know that this function\nis always non-negative. ",
    "start": "31130",
    "end": "38650"
  },
  {
    "text": "So that's theta and that's\nKL P theta star P theta.",
    "start": "38650",
    "end": "45580"
  },
  {
    "text": "And I know that at theta\nstar, it's equal to 0.",
    "start": "45580",
    "end": "51170"
  },
  {
    "text": "OK? I could be in the case\nwhere I have this happening.",
    "start": "51170",
    "end": "56450"
  },
  {
    "text": "I have two-- let's call\nit theta star prime.",
    "start": "56450",
    "end": "61460"
  },
  {
    "text": "I have two minimizers. That could be the case, right? I'm not saying\nthat-- so K of L--",
    "start": "61460",
    "end": "67460"
  },
  {
    "text": "KL is 0 at the minimum. That doesn't mean that I\nhave a unit minimum, right?",
    "start": "67460",
    "end": "74659"
  },
  {
    "text": "But it does, actually. What do I need to\nuse to make sure that I have only one minimum? ",
    "start": "74660",
    "end": "82210"
  },
  {
    "text": "So the definiteness\nis guaranteeing to me that there's a unique P\ntheta star that minimizes it.",
    "start": "82210",
    "end": "88412"
  },
  {
    "text": "But then I need to\nmake sure that there's a unique-- from this\nunique P theta star, I need to make sure there's\na unique theta star that",
    "start": "88412",
    "end": "95380"
  },
  {
    "text": "defines this P theta star.  Exactly.",
    "start": "95380",
    "end": "100550"
  },
  {
    "text": "All right, so I\ncombine definiteness and identifiability to make\nsure that there is a unique",
    "start": "100550",
    "end": "107409"
  },
  {
    "text": "minimizer, in this\ncase cannot exist. OK, so basically, let me\nwrite what I just said.",
    "start": "107410",
    "end": "115630"
  },
  {
    "text": "So definiteness, that\nimplies that P theta star",
    "start": "115630",
    "end": "126539"
  },
  {
    "text": "is the unique minimizer of P\ntheta maps to KL P theta star P",
    "start": "126540",
    "end": "143120"
  },
  {
    "text": "theta. So definiteness only\nguarantees that the probability distribution is\nuniquely identified.",
    "start": "143120",
    "end": "149430"
  },
  {
    "text": "And identifiability\nimplies that theta star",
    "start": "149430",
    "end": "162260"
  },
  {
    "text": "is the unique minimizer of\ntheta maps to KL P theta star P",
    "start": "162260",
    "end": "176937"
  },
  {
    "text": "theta, OK? So I'm basically\ndoing the composition",
    "start": "176937",
    "end": "182480"
  },
  {
    "text": "of two injective functions. The first one is the one that\nmaps, say, theta to P theta.",
    "start": "182480",
    "end": "187970"
  },
  {
    "text": "And the second one is\nthe one that maps P theta to the set of minimizers, OK?",
    "start": "187970",
    "end": "194482"
  },
  {
    "start": "194482",
    "end": "200769"
  },
  {
    "text": "So at least morally, you\nshould agree that theta star",
    "start": "200770",
    "end": "207560"
  },
  {
    "text": "is the minimizer of this thing. Whether it's unique\nor not, you should agree that it's a good one.",
    "start": "207560",
    "end": "213110"
  },
  {
    "text": "So maybe you can think\na little longer on this. So thinking about this\nbeing the minimizer,",
    "start": "213110",
    "end": "218720"
  },
  {
    "text": "then it says,\nwell, if I actually had a good estimate\nfor this function, I would use the strategy\nthat I described",
    "start": "218720",
    "end": "224050"
  },
  {
    "text": "for the total\nvariation, which is, well, I don't know what\nthis function looks like. It depends on theta star.",
    "start": "224050",
    "end": "229700"
  },
  {
    "text": "But maybe I can\nfind an estimator of this function that\nfluctuates around this function,",
    "start": "229700",
    "end": "235170"
  },
  {
    "text": "and such that when I minimize\nthis estimator of the function, I'm actually not too far, OK?",
    "start": "235170",
    "end": "241380"
  },
  {
    "text": "And this is exactly what\ndrives me to do this, because I can actually\nconstruct an estimator.",
    "start": "241380",
    "end": "247379"
  },
  {
    "text": "I can actually construct\nan estimator such that this estimator\nis actually--",
    "start": "247380",
    "end": "252540"
  },
  {
    "text": "of the KL is actually\nclose to the KL, all right? So I define KL hat.",
    "start": "252540",
    "end": "258708"
  },
  {
    "text": "So all we did is just replacing\nexpectation with respect to theta star by averages.",
    "start": "258709",
    "end": "267400"
  },
  {
    "text": " That's what we did.",
    "start": "267400",
    "end": "273850"
  },
  {
    "text": "So if you're a little puzzled by\nthis error, that's all it says. Replace this guy by this guy.",
    "start": "273850",
    "end": "279539"
  },
  {
    "text": "It has no mathematical meaning. It just means just\nreplace it by. And now that actually tells\nme how to get my estimator.",
    "start": "279540",
    "end": "286190"
  },
  {
    "text": "It just says, well,\nmy estimator, KL hat,",
    "start": "286190",
    "end": "291660"
  },
  {
    "text": "is equal to some constant\nwhich I don't know. I mean, it certainly\ndepends on theta star,",
    "start": "291660",
    "end": "296759"
  },
  {
    "text": "but I won't care about it\nwhen I'm trying to minimize-- minus 1/n sum from i from\n1 to n log f theta of x.",
    "start": "296760",
    "end": "309610"
  },
  {
    "text": "So here I'm reading\nit with the density. You have it with the\nPMF on the slides, and so you have the two\nversions in front of you, OK?",
    "start": "309610",
    "end": "318560"
  },
  {
    "text": "Oh sorry, I forgot the xi. Now clearly, this function\nI know how to compute.",
    "start": "318560",
    "end": "325430"
  },
  {
    "text": "If you give me a theta, since\nI know the form of the density",
    "start": "325430",
    "end": "330710"
  },
  {
    "text": "f theta, for each\ntheta that you give me, I can actually compute\nthis quantity, right?",
    "start": "330710",
    "end": "338070"
  },
  {
    "text": "This I don't know,\nbut I don't care. Because I'm just shifting\nthe value of the function I'm trying to minimize.",
    "start": "338070",
    "end": "343590"
  },
  {
    "text": "The set of minimizers\nis not going to change. So now, this is my\nestimation strategy.",
    "start": "343590",
    "end": "350419"
  },
  {
    "text": "Minimize in theta KL hat\nP theta star P theta, OK?",
    "start": "350420",
    "end": "361560"
  },
  {
    "text": "So now let's just make sure\nthat we all agree that-- so what we want is the\nargument of the minimum,",
    "start": "361560",
    "end": "367960"
  },
  {
    "text": "right? arg min means the\ntheta that minimizes this guy, rather than finding\nthe value of the min.",
    "start": "367960",
    "end": "373534"
  },
  {
    "text": "OK, so I'm trying to find\nthe arg min of this thing. Well, this is equivalent\nto finding the arg",
    "start": "373534",
    "end": "378900"
  },
  {
    "text": "min of, say, a constant minus\n1/n sum from i from 1 to n",
    "start": "378900",
    "end": "388020"
  },
  {
    "text": "of log f theta of xi. ",
    "start": "388020",
    "end": "393864"
  },
  {
    "text": "So that's just--  I don't think it likes me.",
    "start": "393864",
    "end": "401026"
  },
  {
    "text": "No. OK, so thus minimizing\nthis average, right?",
    "start": "401026",
    "end": "406110"
  },
  {
    "text": "I just plugged in the\ndefinition of KL hat. Now, I claim that\ntaking the arg min of a constant plus a function\nor the arg min of the function",
    "start": "406110",
    "end": "413509"
  },
  {
    "text": "is the same thing. Is anybody not comfortable\nwith this idea?",
    "start": "413510",
    "end": "420650"
  },
  {
    "text": "OK, so this is the same. ",
    "start": "420650",
    "end": "433757"
  },
  {
    "text": "By the way, this\nI should probably switch to the next slide,\nbecause I'm writing the same thing, but better.",
    "start": "433757",
    "end": "442830"
  },
  {
    "text": "And it's with PMF\nrather than as PF.",
    "start": "442830",
    "end": "449360"
  },
  {
    "text": "OK, now, arg min of the minimum\nis the same of arg max-- sorry, arg min of\nthe negative thing",
    "start": "449360",
    "end": "455595"
  },
  {
    "text": "is the same as arg max\nwithout the negative, right?  arg max over theta of 1/n from\ni equal equal 1 to n log f",
    "start": "455595",
    "end": "469010"
  },
  {
    "text": "theta of xi.  Taking the arg\nmin of the average",
    "start": "469010",
    "end": "474980"
  },
  {
    "text": "or the arg min of\nthe sum, again, it's not going to make\nmuch difference. Just adding constants OR\nmultiplying by constants",
    "start": "474980",
    "end": "481310"
  },
  {
    "text": "does not change the\narg min or the arg max. Now, I have the\nsum of logs, which",
    "start": "481310",
    "end": "487677"
  },
  {
    "text": "is the log of the product. ",
    "start": "487677",
    "end": "503310"
  },
  {
    "text": "OK? It's the arg max of the\nlog of f theta of x1 times f theta of x2, f theta of xn.",
    "start": "503310",
    "end": "510190"
  },
  {
    "text": "But the log is a function\nthat's increasing, so maximizing",
    "start": "510190",
    "end": "517440"
  },
  {
    "text": "log of a function or\nmaximizing the function itself is the same thing.",
    "start": "517440",
    "end": "522860"
  },
  {
    "text": "The value is going to\nchange, but the arg max is not going to change. Everybody agrees with this? ",
    "start": "522860",
    "end": "530339"
  },
  {
    "text": "So this is equivalent to arg\nmax over theta of pi from 1 to n",
    "start": "530340",
    "end": "539990"
  },
  {
    "text": "of f theta xi. And that's because x maps\nto log x is increasing.",
    "start": "539990",
    "end": "550515"
  },
  {
    "text": " So now I've gone from\nminimizing the KL",
    "start": "550515",
    "end": "557140"
  },
  {
    "text": "to minimizing the\nestimate of the KL to maximizing this product.",
    "start": "557140",
    "end": "563520"
  },
  {
    "text": "Well, this chapter is called\nmaximum likelihood estimation. The maximum comes from the\nfact that our original idea",
    "start": "563520",
    "end": "570370"
  },
  {
    "text": "was to minimize the\nnegative of a function. So that's why it's\nmaximum likelihood. And this function here\nis called the likelihood.",
    "start": "570370",
    "end": "582770"
  },
  {
    "text": "This function is really\njust telling me-- they call it\nlikelihood because it's some measure of how\nlikely it is that theta",
    "start": "582770",
    "end": "589920"
  },
  {
    "text": "was the parameter that\ngenerated the data. OK, so let's go to the--",
    "start": "589920",
    "end": "595444"
  },
  {
    "text": "well, we'll go to the formal\ndefinition in a second. But actually, let\nme just give you intuition as to why this is\nthe distribution of the data.",
    "start": "595444",
    "end": "605592"
  },
  {
    "text": "Why this is the\nlikelihood-- sorry. Why is this making sense\nas a measure of likelihood?",
    "start": "605592",
    "end": "611940"
  },
  {
    "text": "Let's now think for simplicity\nof the following model. So I have-- I'm on the real line\nand I look at n, say,",
    "start": "611940",
    "end": "619550"
  },
  {
    "text": "theta 1 for theta in the\nreal-- do you see that?",
    "start": "619550",
    "end": "625540"
  },
  {
    "text": "OK. Probably you don't. Not that you care. OK, so-- ",
    "start": "625540",
    "end": "641120"
  },
  {
    "text": "OK, let's look at\na simple example.  So here's the model.",
    "start": "641120",
    "end": "648910"
  },
  {
    "text": "As I said, we're looking at\nobservations on the real line. And they're distributed\naccording to some n theta 1.",
    "start": "648910",
    "end": "657032"
  },
  {
    "text": "So I don't care\nabout the variance. I know it's 1. And it's indexed by\ntheta in the real line.",
    "start": "657032",
    "end": "663170"
  },
  {
    "text": "OK, so this is-- the only\nthing I need to figure out is, what is the mean\nof those guys, OK?",
    "start": "663170",
    "end": "669260"
  },
  {
    "text": "Now, I have this n observations. And if you actually remember\nfrom your probability class,",
    "start": "669260",
    "end": "675920"
  },
  {
    "text": "are you familiar with the\nconcept of joint density? You have multivariate\nobservations. The joint density of\nindependent random variables",
    "start": "675920",
    "end": "683750"
  },
  {
    "text": "is just a product of their\nindividual densities. So really, when I look\nat the product from i",
    "start": "683750",
    "end": "690950"
  },
  {
    "text": "equal 1 to n of f\ntheta of xi, this is really the joint\ndensity of the vector--",
    "start": "690950",
    "end": "704800"
  },
  {
    "text": " well, let me not use\nthe word vector--",
    "start": "704800",
    "end": "711209"
  },
  {
    "text": "of x1 xn, OK? So if I take the product of\ndensity, is it still a density?",
    "start": "711210",
    "end": "718930"
  },
  {
    "text": "And it's actually-- but\nthis time on the r to the n.",
    "start": "718930",
    "end": "724136"
  },
  {
    "text": "And so now what this\nthing is telling me-- so think of it in r2, right? So this is the joint\ndensity of two Gaussians.",
    "start": "724136",
    "end": "730630"
  },
  {
    "text": "So it's something that looks\nlike some bell-shaped curve in two dimensions.",
    "start": "730630",
    "end": "735940"
  },
  {
    "text": "And it's centered at\nthe value theta theta. OK, they both have\nthe mean theta.",
    "start": "735940",
    "end": "742389"
  },
  {
    "text": "So let's assume for\none second-- it's going to be hard for me to\nmake pictures in n dimensions.",
    "start": "742390",
    "end": "748000"
  },
  {
    "text": "Actually, already\nin two dimensions, I can promise you that\nit's not very easy. So I'm actually\njust going to assume",
    "start": "748000",
    "end": "754300"
  },
  {
    "text": "that n is equal to 1 for\nthe sake of illustration. OK, so now I have this data.",
    "start": "754300",
    "end": "760900"
  },
  {
    "text": "And now I have one\nobservation, OK? And I know that the f\ntheta looks like this.",
    "start": "760900",
    "end": "767524"
  },
  {
    "text": "And what I'm doing\nis I'm actually looking at the value of x\ntheta as my observation. ",
    "start": "767524",
    "end": "774787"
  },
  {
    "text": "Let's call it x1. Now, my principal tells me,\njust find the theta that",
    "start": "774787",
    "end": "780589"
  },
  {
    "text": "makes this guy the most likely. What is the likelihood of my x1? Well, it's just the\nvalue of the function.",
    "start": "780590",
    "end": "787640"
  },
  {
    "text": "That this value here. And if I wanted to find the most\nlikely theta that had generated",
    "start": "787640",
    "end": "793670"
  },
  {
    "text": "this x1, what I would need\nto do is to shift this thing and put it here.",
    "start": "793670",
    "end": "799290"
  },
  {
    "text": "And so my estimate, my\nmaximum likelihood estimator here would be theta\nis equal to x1, OK?",
    "start": "799290",
    "end": "808720"
  },
  {
    "text": "That would be just\nthe observation. Because if I have\nonly one observation, what else am I going to do? OK, and so it sort\nof makes sense.",
    "start": "808720",
    "end": "814870"
  },
  {
    "text": "And if you have\nmore observations, you can think of it this way,\nas if you had more observations.",
    "start": "814870",
    "end": "820540"
  },
  {
    "text": "So now I have, say,\nK observations, or n observations. And what I do is that I\nlook at the value for each",
    "start": "820540",
    "end": "826240"
  },
  {
    "text": "of these guys. So this value, this value,\nthis value, this value.",
    "start": "826240",
    "end": "832240"
  },
  {
    "text": "I take their product and\nI make this thing large. OK, why do I take the product?",
    "start": "832240",
    "end": "837250"
  },
  {
    "text": "Well, because I'm trying\nto maximize their value all together, and I need to\njust turn it into one number",
    "start": "837250",
    "end": "842829"
  },
  {
    "text": "that I can maximize. And taking the product\nis the natural way of doing it, either\nby motivating it",
    "start": "842830",
    "end": "848470"
  },
  {
    "text": "by the KL principle\nor motivating it by maximizing the joint density,\nrather than just maximizing",
    "start": "848470",
    "end": "854800"
  },
  {
    "text": "anything. OK, so that's why, visually,\nthis is the maximum likelihood.",
    "start": "854800",
    "end": "860199"
  },
  {
    "text": "It just says that if my\nobservations are here, then this guy, this mean theta,\nis more likely than this guy.",
    "start": "860200",
    "end": "869210"
  },
  {
    "text": "Because now if I\nlook at the value of the function\nfor this guy-- if I look at theta being\nthis thing, then this",
    "start": "869210",
    "end": "875740"
  },
  {
    "text": "is a very small value. Very small value, very small\nvalue, very small value. Everything gets a super\nsmall value, right?",
    "start": "875740",
    "end": "881983"
  },
  {
    "text": "That's just the value\nthat it gets in the tail here, which is very close to 0. But as soon as I start\ncovering all my points",
    "start": "881984",
    "end": "887980"
  },
  {
    "text": "with my bell-shaped curve,\nthen all the values go up.",
    "start": "887980",
    "end": "893260"
  },
  {
    "text": "All right, so I just want\nto make a short break",
    "start": "893260",
    "end": "898720"
  },
  {
    "text": "into statistics,\nand just make sure that the maximum likelihood\nprinciple involves",
    "start": "898720",
    "end": "904120"
  },
  {
    "text": "maximizing a function. So I just want to\nmake sure that we're all on par about how do\nwe maximize functions.",
    "start": "904120",
    "end": "911200"
  },
  {
    "text": "In most instances, it's going to\nbe a one-dimensional function, because theta is going to be\na one-dimensional parameter.",
    "start": "911200",
    "end": "916690"
  },
  {
    "text": "Like here it's the real line. So it's going to be easy. In some cases, it may be\na multivariate function",
    "start": "916690",
    "end": "922130"
  },
  {
    "text": "and it might be\nmore complicated. OK, so let's just\nmake this interlude. So the first thing\nI want you to notice",
    "start": "922130",
    "end": "928450"
  },
  {
    "text": "is that if you open any book\non what's called optimization, which basically is the science\nbehind optimizing functions,",
    "start": "928450",
    "end": "935110"
  },
  {
    "text": "you will talk mostly-- I mean, I'd say\n99.9% of the cases",
    "start": "935110",
    "end": "940170"
  },
  {
    "text": "will talk about\nminimizing functions. But it doesn't matter, because\nyou can just flip the function and you just put a minus\nsign, and minimizing h",
    "start": "940170",
    "end": "947710"
  },
  {
    "text": "is the same as maximizing\nminus h and the opposite, OK? So for this class,\nsince we're only",
    "start": "947710",
    "end": "953547"
  },
  {
    "text": "going to talk about maximum\nlikelihood estimation, we will talk about\nmaximizing functions. But don't be lost if\nyou decide suddenly",
    "start": "953547",
    "end": "959320"
  },
  {
    "text": "to open a book on optimization\nand find only something about minimizing functions. OK, so maximizing an arbitrary\nfunction can actually be fairly",
    "start": "959320",
    "end": "968279"
  },
  {
    "text": "difficult. If I give you a\nfunction that has this weird shape, right-- let's think of\nthis polynomial for example--",
    "start": "968279",
    "end": "973740"
  },
  {
    "text": "and I wanted to find the\nmaximum, how would we do it? ",
    "start": "973740",
    "end": "980350"
  },
  {
    "text": "So what is the thing you've\nlearned in calculus on how to maximize the function?",
    "start": "980350",
    "end": "986200"
  },
  {
    "text": "Set the derivative equal to 0. Maybe you want to check\nthe second derivative to make sure it's a\nmaximum and not a minimum.",
    "start": "986200",
    "end": "991647"
  },
  {
    "text": "But the thing is, this is only\nguaranteeing to you that you have a local one, right? So if I do it for this function,\nfor example, then this guy",
    "start": "991647",
    "end": "998412"
  },
  {
    "text": "is going to satisfy\nthis criterion, this guy is going to\nsatisfy this criterion, this guy is going to satisfy\nthis criterion, this guy here,",
    "start": "998412",
    "end": "1003529"
  },
  {
    "text": "and this guy satisfies\nthe criterion, but not the second derivative one. So I have a lot of candidates.",
    "start": "1003530",
    "end": "1010160"
  },
  {
    "text": "And if my function can\nbe really anything, it's going to be\ndifficult, whether it's analytically by taking\nderivatives and setting them",
    "start": "1010160",
    "end": "1016820"
  },
  {
    "text": "to 0, or trying to find\nsome algorithms to do this. Because if my function\nis very jittery,",
    "start": "1016820",
    "end": "1022399"
  },
  {
    "text": "then my algorithm basically\nhas to check all candidates. And if there's a lot of them,\nit might take forever, OK?",
    "start": "1022400",
    "end": "1028520"
  },
  {
    "text": "So this is-- I have only\none, two, three, four, five candidates to check. But in practice, you might have\na million of them to check.",
    "start": "1028520",
    "end": "1035900"
  },
  {
    "text": "And that might take forever. OK, so what's nice about\nstatistical models, and one",
    "start": "1035900",
    "end": "1041179"
  },
  {
    "text": "of the things that makes all\nthese models particularly robust, and that we\nstill talk about them 100",
    "start": "1041180",
    "end": "1047792"
  },
  {
    "text": "years after they've\nbeen introduced is that the functions\nthat-- the likelihoods that they lead\nfor us to maximize",
    "start": "1047792",
    "end": "1053450"
  },
  {
    "text": "are actually very simple. And they all share\na nice property, which is that of being concave.",
    "start": "1053450",
    "end": "1060350"
  },
  {
    "text": "All right, so what is\na concave function? Well, by definition, it's\njust a function for which-- let's think of it as being\ntwice differentiable.",
    "start": "1060350",
    "end": "1067760"
  },
  {
    "text": "You can define\nfunctions that are not differentiable as being concave,\nbut let's think about it as having a second derivative.",
    "start": "1067760",
    "end": "1073034"
  },
  {
    "text": "And so if you look\nat the function that has a second derivative,\nconcave are the functions that have their second\nderivative that's",
    "start": "1073035",
    "end": "1079340"
  },
  {
    "text": "negative everywhere. Not just at the\nmaximum, everywhere, OK?",
    "start": "1079340",
    "end": "1086430"
  },
  {
    "text": "And so if it's strictly\nconcave, this second derivative is actually strictly\nless than zero.",
    "start": "1086430",
    "end": "1092279"
  },
  {
    "text": "And particularly if I\nthink of a linear function, y is equal to x,\nthen this function",
    "start": "1092280",
    "end": "1099480"
  },
  {
    "text": "has its second derivative\nwhich is equal to zero, OK? So it is concave.",
    "start": "1099480",
    "end": "1106020"
  },
  {
    "text": "But it's not\nstrictly concave, OK? If I look at the function\nwhich is negative x squared,",
    "start": "1106020",
    "end": "1111570"
  },
  {
    "text": "what is its second derivative?  Minus 2.",
    "start": "1111570",
    "end": "1116809"
  },
  {
    "text": "So it's strictly\nnegative everywhere, OK? So actually, this is a\npretty canonical example",
    "start": "1116810",
    "end": "1123767"
  },
  {
    "text": "strictly concave function. If you want to think of a\npicture of a strictly concave function, think of\nnegative x squared. So parabola pointing downwards.",
    "start": "1123767",
    "end": "1132770"
  },
  {
    "text": "OK, so we can talk about\nstrictly convex functions. So convex is just happening when\nthe negative of the function",
    "start": "1132770",
    "end": "1139940"
  },
  {
    "text": "is concave. So that translates into having\na second derivative which is either non-negative\nor positive, depending",
    "start": "1139940",
    "end": "1145909"
  },
  {
    "text": "on whether you're talking about\nconvexity or strict convexity. But again, those\nconvex functions",
    "start": "1145910",
    "end": "1151850"
  },
  {
    "text": "are convenient when you're\ntrying to minimize something. And since we're trying\nto maximize the function, we're looking for concave.",
    "start": "1151850",
    "end": "1158710"
  },
  {
    "text": "So here are some examples. Let's just go\nthrough them quickly. ",
    "start": "1158710",
    "end": "1179059"
  },
  {
    "text": "OK, so the first one is-- so here I made my\nlife a little uneasy",
    "start": "1179060",
    "end": "1186539"
  },
  {
    "text": "by talking about the\nfunctions in theta, right? I'm talking about\nlikelihoods, right? So I'm thinking of functions\nwhere the parameter is theta.",
    "start": "1186540",
    "end": "1194460"
  },
  {
    "text": "So I have h of theta. And so if I start\nwith theta squared, negative theta squared,\nthen as we said,",
    "start": "1194460",
    "end": "1202170"
  },
  {
    "text": "h prime prime of theta, the\nsecond derivative is minus 2,",
    "start": "1202170",
    "end": "1209490"
  },
  {
    "text": "which is strictly negative,\nso this function is strictly concave. ",
    "start": "1209490",
    "end": "1219210"
  },
  {
    "text": "OK, another function is\nh of theta, which is--",
    "start": "1219210",
    "end": "1224399"
  },
  {
    "text": "what did we pick-- square root of theta. What is the first derivative?",
    "start": "1224400",
    "end": "1230018"
  },
  {
    "start": "1230018",
    "end": "1235759"
  },
  {
    "text": "1/2 square root of theta. What is the second derivative?",
    "start": "1235760",
    "end": "1241332"
  },
  {
    "start": "1241332",
    "end": "1248220"
  },
  {
    "text": "So that's theta to\nthe negative 1/2. So I'm just picking up\nanother negative 1/2,",
    "start": "1248220",
    "end": "1253450"
  },
  {
    "text": "so I get negative 1/4. And then I get theta to\nthe 3/4 downstairs, OK?",
    "start": "1253450",
    "end": "1262420"
  },
  {
    "text": "Sorry, 3/2. ",
    "start": "1262420",
    "end": "1269429"
  },
  {
    "text": "And that's strictly negative\nfor theta, say, larger than 0.",
    "start": "1269430",
    "end": "1276820"
  },
  {
    "text": "And I really need to have\nthis thing larger than 0 so that it's well-defined. But strictly larger than 0 is\nso that this thing does not",
    "start": "1276820",
    "end": "1284320"
  },
  {
    "text": "blow up to infinity. And it's true. If you think about this\nfunction, it looks like this.",
    "start": "1284320",
    "end": "1290740"
  },
  {
    "text": "And already, the first\nderivative to infinity at 0. And it's a concave function, OK?",
    "start": "1290740",
    "end": "1297519"
  },
  {
    "text": "Another one is the\nlog, of course. ",
    "start": "1297520",
    "end": "1304070"
  },
  {
    "text": "What is the\nderivative of the log? That's 1 over theta, where h\nprime of theta is 1 over theta.",
    "start": "1304070",
    "end": "1312710"
  },
  {
    "text": "And the second derivative\nnegative 1 over theta squared,",
    "start": "1312710",
    "end": "1321080"
  },
  {
    "text": "which again, is negative if\ntheta is strictly positive.",
    "start": "1321080",
    "end": "1326210"
  },
  {
    "text": "Here I define it as-- I don't need to define it to\nbe strictly positive here, but I need it for the log.",
    "start": "1326210",
    "end": "1333110"
  },
  {
    "text": "And sine. OK, so let's just do one more. So h of theta is sine of theta.",
    "start": "1333110",
    "end": "1342555"
  },
  {
    "text": "But here I take it\nonly on an interval, because you want to\nthink of this function as pointing always downwards.",
    "start": "1342555",
    "end": "1349112"
  },
  {
    "text": "And in particular, you\ndon't want this function to have an inflection point. You don't want it to\ngo down and then up and then down and then up,\nbecause this is not concave.",
    "start": "1349112",
    "end": "1357200"
  },
  {
    "text": "And so sine is certainly\ngoing up and down, right? So what we do is we restrict\nit to an interval where sine",
    "start": "1357200",
    "end": "1363529"
  },
  {
    "text": "is actually-- so what does\nthe sine function looks like at 0, 0? And it's going up.",
    "start": "1363530",
    "end": "1368600"
  },
  {
    "text": "Where is the first\nmaximum of the sine? STUDENT: [INAUDIBLE]",
    "start": "1368600",
    "end": "1374341"
  },
  {
    "text": "PROFESSOR: I'm sorry. STUDENT: Pi over 2. PROFESSOR: Pi over 2,\nwhere it takes value 1. And then it goes down again.",
    "start": "1374341",
    "end": "1381700"
  },
  {
    "text": "And then that's at pi. And then I go down again. And here you see I actually\nstart changing my inflection.",
    "start": "1381700",
    "end": "1388360"
  },
  {
    "text": "So what we do is\nwe stop it at pi. And we look at this\nfunction, it certainly looks like a parabola\npointing downwards.",
    "start": "1388360",
    "end": "1394403"
  },
  {
    "text": "And so if you look at the--\nyou can check that it actually works with the derivatives. So the derivative\nof sine is cosine.",
    "start": "1394404",
    "end": "1402530"
  },
  {
    "text": " And the derivative of\ncosine is negative sine.",
    "start": "1402530",
    "end": "1411850"
  },
  {
    "text": " OK, and this thing between 0\nand pi is actually positive.",
    "start": "1411850",
    "end": "1418170"
  },
  {
    "text": "So this entire thing is\ngoing to be negative. OK? And you know, I can come\nup with a lot of examples,",
    "start": "1418170",
    "end": "1425160"
  },
  {
    "text": "but let's just stick to those. There's a linear\nfunction, of course. And the find function\nis going to be concave,",
    "start": "1425160",
    "end": "1431196"
  },
  {
    "text": "but it's actually going to\nbe convex as well, which means that it's\ncertainly not going to be strictly concave or convex, OK?",
    "start": "1431196",
    "end": "1438779"
  },
  {
    "text": "So here's your standard picture. And here, if you look\nat the dotted line, what",
    "start": "1438780",
    "end": "1444630"
  },
  {
    "text": "it tells me is that\na concave function, and the property we're\ngoing to be using is that if a strictly concave\nfunction has a maximum, which",
    "start": "1444630",
    "end": "1452910"
  },
  {
    "text": "is not always the case,\nbut if it has a maximum, then it actually must be--\nsorry, a local maximum,",
    "start": "1452910",
    "end": "1458770"
  },
  {
    "text": "it must be a global maximum. OK, so just the fact that\nit goes up and down and not",
    "start": "1458770",
    "end": "1463870"
  },
  {
    "text": "again means that there's only\nglobal maximum that can exist. Now if you looked, for example,\nat the square root function,",
    "start": "1463870",
    "end": "1472480"
  },
  {
    "text": "look at the entire\npositive real line, then this thing is never\ngoing to attain a maximum. It's just going to infinity\nas x goes to infinity.",
    "start": "1472480",
    "end": "1479362"
  },
  {
    "text": "So if I wanted to\nfind the maximum, I would have to stop\nsomewhere and say that the maximum is attained\nat the right-hand side.",
    "start": "1479362",
    "end": "1486200"
  },
  {
    "text": "OK, so that's the beauty about\nconvex functions or concave functions, is that\nessentially, these functions",
    "start": "1486200",
    "end": "1493880"
  },
  {
    "text": "are easy to maximize. And if I tell you a\nfunction is concave, you take the first\nderivative, set it equal to 0.",
    "start": "1493880",
    "end": "1500164"
  },
  {
    "text": "If you find a point\nthat satisfies this, then it must be a\nglobal maximum, OK?",
    "start": "1500164",
    "end": "1507559"
  },
  {
    "text": "STUDENT: What if\nyour set theta was [INAUDIBLE] then couldn't\nyou have a function that,",
    "start": "1507560",
    "end": "1513695"
  },
  {
    "text": "by the definition, is concave,\nwith two upside down parabolas at two disjoint intervals, but\nyet it has two global maximums?",
    "start": "1513695",
    "end": "1522909"
  },
  {
    "text": " PROFESSOR: So you\nwon't get them--",
    "start": "1522910",
    "end": "1528120"
  },
  {
    "text": "so you want the function\nto be concave on what? On the convex cell\nof the intervals?",
    "start": "1528120",
    "end": "1534430"
  },
  {
    "text": "Or you want it to be-- STUDENT: [INAUDIBLE] just\nsaid that any subset. PROFESSOR: OK, OK.",
    "start": "1534430",
    "end": "1540029"
  },
  {
    "text": "You're right. So maybe the\ndefinition-- so you're pointing to a weakness\nin the definition.",
    "start": "1540029",
    "end": "1545810"
  },
  {
    "text": "Let's just say that\ntheta is a convex set and then you're good, OK? So you're right.",
    "start": "1545810",
    "end": "1551450"
  },
  {
    "text": " Since I actually just said that\nthis is true only for theta,",
    "start": "1551450",
    "end": "1556789"
  },
  {
    "text": "I can just take pieces of\nconcave functions, right? I can do this, and\nthen the next one I can do this, on the\nnext one I can do this.",
    "start": "1556790",
    "end": "1563330"
  },
  {
    "text": "And then I would\nhave a bunch of them. But what I want is think\nof it as a global function",
    "start": "1563330",
    "end": "1570620"
  },
  {
    "text": "on some convex set. You're right. So think of theta\nas being convex for this guy, an interval,\nif it's a real line.",
    "start": "1570620",
    "end": "1577560"
  },
  {
    "text": " OK, so as I said, for\nmore generally-- so",
    "start": "1577560",
    "end": "1585689"
  },
  {
    "text": "we can actually define concave\nfunctions more generally in higher dimensions. And that will be useful\nif theta is not just",
    "start": "1585689",
    "end": "1592690"
  },
  {
    "text": "one parameter but\nseveral parameters. And for that, you need to\nremind yourself of Calculus II,",
    "start": "1592690",
    "end": "1599050"
  },
  {
    "text": "and you have generalization of\nthe notion of derivative, which is called a gradient, which\nis basically a vector where",
    "start": "1599050",
    "end": "1606129"
  },
  {
    "text": "each coordinate is just the\npartial derivative with respect to each coordinate of theta.",
    "start": "1606130",
    "end": "1611170"
  },
  {
    "text": "And the Hessian is\nthe matrix, which is essentially a generalization\nof the second derivative.",
    "start": "1611170",
    "end": "1618020"
  },
  {
    "text": "I denote it by nabla\nsquared, but you can write it the way you want. And so this matrix\nhere is taking as entry",
    "start": "1618020",
    "end": "1627296"
  },
  {
    "text": "the second partial\nderivatives of h with respect to theta i and theta j.",
    "start": "1627296",
    "end": "1632919"
  },
  {
    "text": "And so that's the ij-th entry. Who has never seen that? ",
    "start": "1632920",
    "end": "1639400"
  },
  {
    "text": "OK. So now, being concave here\nis essentially generalizing,",
    "start": "1639400",
    "end": "1647200"
  },
  {
    "text": "saying that a vector\nis equal to zero. Well, that's just setting\nthe vector-- sorry. The first order condition\nto say that it's a maximum",
    "start": "1647200",
    "end": "1653700"
  },
  {
    "text": "is going to be the same. Saying that a function has\na gradient equal to zero",
    "start": "1653700",
    "end": "1658929"
  },
  {
    "text": "is the same as saying that\neach of its coordinates are equal to zero.",
    "start": "1658930",
    "end": "1664730"
  },
  {
    "text": "And that's actually\ngoing to be a condition for a global maximum here. So to check convexity, we need\nto see that a matrix itself",
    "start": "1664730",
    "end": "1672190"
  },
  {
    "text": "is negative. Sorry, to check\nconcavity, we need to check that a\nmatrix is negative. And there is a\nnotion among matrices",
    "start": "1672190",
    "end": "1679840"
  },
  {
    "text": "that compare matrix to zero,\nand that's exactly this notion. You pre- and post-multiply\nby the same x.",
    "start": "1679840",
    "end": "1686169"
  },
  {
    "text": "So that works for\nsymmetric matrices, which is the case here. And so you pre-multiply by x,\npost-multiply by the same x.",
    "start": "1686170",
    "end": "1693940"
  },
  {
    "text": "So you have your matrix,\nyour Hessian here. ",
    "start": "1693940",
    "end": "1700630"
  },
  {
    "text": "It's a d by d matrix if you\nhave a d-dimensional matrix. So let's call it--",
    "start": "1700630",
    "end": "1706900"
  },
  {
    "text": "OK. And then here I\npre-multiply by x transpose. I post-multiply by x.",
    "start": "1706900",
    "end": "1714330"
  },
  {
    "text": "And this has to be non-positive\nif I want it to be concave, and strictly negative if I\nwant it to be strictly concave.",
    "start": "1714330",
    "end": "1722850"
  },
  {
    "text": "OK, that's just a\nreal generalization. You can check for yourself\nthat this is the same thing. If I were in dimension 1,\nthis would be the same thing.",
    "start": "1722850",
    "end": "1729760"
  },
  {
    "text": "Why? Because in dimension 1, pre-\nand post-multiplying by x is the same as\nmultiplying by x squared.",
    "start": "1729760",
    "end": "1735840"
  },
  {
    "text": "Because in dimension 1, I can\njust move my x's around, right? And so that would just\nmean the first condition",
    "start": "1735840",
    "end": "1741179"
  },
  {
    "text": "would mean in dimension 1 that\nthe second derivative times x squared has to be less\nthan or equal to zero.",
    "start": "1741180",
    "end": "1751110"
  },
  {
    "text": "So here I need this for\nall x's that are not zero, because I can take x to be zero\nand make this equal to zero,",
    "start": "1751110",
    "end": "1756870"
  },
  {
    "text": "right? So this is for x's that\nare not equal to zero, OK? And so some examples.",
    "start": "1756870",
    "end": "1765720"
  },
  {
    "text": "Just look at this function. So now I have functions that\ndepend on two parameters, theta1 and theta2.",
    "start": "1765720",
    "end": "1771600"
  },
  {
    "text": "So the first one is-- so if I take theta\nto be equal to-- now I need two\nparameters, r squared.",
    "start": "1771600",
    "end": "1779010"
  },
  {
    "text": "And I look at the function,\nwhich is h of theta. Can somebody tell me\nwhat h of theta is?",
    "start": "1779010",
    "end": "1785266"
  },
  {
    "text": "STUDENT: [INAUDIBLE] PROFESSOR: Minus\n2 theta2 squared?",
    "start": "1785266",
    "end": "1792040"
  },
  {
    "text": "OK, so let's compute the\ngradient of h of theta.",
    "start": "1792040",
    "end": "1800920"
  },
  {
    "text": "So it's going to be something\nthat has two coordinates. To get the first\ncoordinate, what do I do?",
    "start": "1800920",
    "end": "1806064"
  },
  {
    "text": "Well, I take the\nderivative with respect to theta1, thinking of\ntheta2 as being a constant. So this thing is\ngoing to go away.",
    "start": "1806064",
    "end": "1811750"
  },
  {
    "text": "And so I get negative 2 theta1. And when I take the\nderivative with respect to the second part, thinking\nof this part as being constant,",
    "start": "1811750",
    "end": "1818620"
  },
  {
    "text": "I get minus 4 theta2. ",
    "start": "1818620",
    "end": "1824559"
  },
  {
    "text": "That clear for everyone? That's just the definition\nof partial derivatives. ",
    "start": "1824560",
    "end": "1832429"
  },
  {
    "text": "And then if I want\nto do the Hessian,",
    "start": "1832430",
    "end": "1840880"
  },
  {
    "text": "so now I'm going to\nget a 2 by 2 matrix.  The first guy here, I take\nthe first-- so this guy",
    "start": "1840880",
    "end": "1848649"
  },
  {
    "text": "I get by taking the derivative\nof this guy with respect to theta1. So that's easy. So that's just minus 2.",
    "start": "1848650",
    "end": "1855152"
  },
  {
    "text": "This guy I get by\ntaking derivative of this guy with\nrespect to theta2. So I get what?",
    "start": "1855152",
    "end": "1860341"
  },
  {
    "text": "Zero. I treat this guy as\nbeing a constant. This guy is also\ngoing to be zero, because I take the derivative\nof this guy with respect",
    "start": "1860341",
    "end": "1866990"
  },
  {
    "text": "to theta1. And then I take the derivative\nof this guy with respect to theta2, so I get minus 4.",
    "start": "1866990",
    "end": "1874310"
  },
  {
    "text": "OK, so now I want to check\nthat this matrix satisfies x transpose--",
    "start": "1874310",
    "end": "1881210"
  },
  {
    "text": "this matrix x is negative. So what I do is-- so what is x transpose x?",
    "start": "1881210",
    "end": "1887360"
  },
  {
    "text": "So if I do x transpose delta\nsquared h theta x, what I get",
    "start": "1887360",
    "end": "1893809"
  },
  {
    "text": "is minus 2 x1 squared\nminus 4 x2 squared.",
    "start": "1893810",
    "end": "1902570"
  },
  {
    "text": "Because this matrix is diagonal,\nso all it does is just weights the square of the x's.",
    "start": "1902570",
    "end": "1907920"
  },
  {
    "text": "So this guy is\ndefinitely negative. This guy is negative.",
    "start": "1907920",
    "end": "1913580"
  },
  {
    "text": "And actually, if one\nof the two is non-zero, which means that x is\nnon-zero, then this thing is actually strictly negative.",
    "start": "1913580",
    "end": "1920240"
  },
  {
    "text": "So this function is\nactually strictly concave. ",
    "start": "1920240",
    "end": "1925380"
  },
  {
    "text": "And it looks like a\nparabola that's slightly distorted in one direction. ",
    "start": "1925380",
    "end": "1935730"
  },
  {
    "text": "So well, I know this might\nhave been some time ago.",
    "start": "1935730",
    "end": "1941257"
  },
  {
    "text": "Maybe for some of you might\nhave been since high school. So just remind yourself of doing\nsecond derivatives and Hessians",
    "start": "1941257",
    "end": "1947360"
  },
  {
    "text": "and things like this. Here's another one\nas an exercise.",
    "start": "1947360",
    "end": "1952919"
  },
  {
    "text": "h is minus theta1\nminus theta2 squared. So this one is going to\nactually not be diagonal.",
    "start": "1952920",
    "end": "1964100"
  },
  {
    "text": "The Hessian is not\ngoing to be diagonal. Who would like to do\nthis now in class?",
    "start": "1964100",
    "end": "1970660"
  },
  {
    "text": "OK, thank you. This is not a calculus class. So you can just do it\nas a calculus exercise.",
    "start": "1970660",
    "end": "1976090"
  },
  {
    "text": "And you can do it\nfor log as well. Now, there is a nice\nrecipe for concavity",
    "start": "1976090",
    "end": "1981100"
  },
  {
    "text": "that works for the second\none and the third one. And the thing is, if you look\nat those particular functions,",
    "start": "1981100",
    "end": "1987610"
  },
  {
    "text": "what I'm doing is taking, first\nof all, a linear combination of my arguments.",
    "start": "1987610",
    "end": "1993040"
  },
  {
    "text": "And then I take a concave\nfunction of this guy. And this is always\ngoing to work.",
    "start": "1993040",
    "end": "1998350"
  },
  {
    "text": "This is always going to\ngive me a complete function. So the computations\nthat I just made, I actually never made\nthem when I prepared those",
    "start": "1998350",
    "end": "2004840"
  },
  {
    "text": "slides because I don't have to. I know that if I take a linear\ncombination of those things and then I take a concave\nfunction of this guy,",
    "start": "2004840",
    "end": "2010650"
  },
  {
    "text": "I'm always going to\nget a concave function. OK, so that's an easy way to\ncheck this, or at least as",
    "start": "2010650",
    "end": "2019410"
  },
  {
    "text": "a sanity check. All right, and so as I said,\nfinding maximizers of concave",
    "start": "2019410",
    "end": "2028250"
  },
  {
    "text": "or strictly concave\nfunction is the same as it was in the\none-dimensional case. What I do-- sorry, in\nthe one-dimensional case,",
    "start": "2028250",
    "end": "2035052"
  },
  {
    "text": "we just agreed that we\njust take the derivative and set it to zero. In the high dimensional\ncase, we take the gradient",
    "start": "2035052",
    "end": "2040160"
  },
  {
    "text": "and set it equal to zero. Again, that's\ncalculus, all right? So it turns out that\nso this is going",
    "start": "2040160",
    "end": "2047930"
  },
  {
    "text": "to give me equations, right? The first one is an\nequation in theta. The second one is an equation\nin theta1, theta2, theta3,",
    "start": "2047930",
    "end": "2055040"
  },
  {
    "text": "all the way to theta d. And it doesn't mean that because\nI can write this equation that I can actually solve it.",
    "start": "2055040",
    "end": "2061129"
  },
  {
    "text": "This equation might\nbe super nasty. It might be like some polynomial\nand exponentials and logs equal",
    "start": "2061130",
    "end": "2068929"
  },
  {
    "text": "zero, or some crazy thing. And so there's actually,\nfor a concave function,",
    "start": "2068929",
    "end": "2076619"
  },
  {
    "text": "since we know there's\na unique maximizer, there's this theory of convex\noptimization, which really,",
    "start": "2076620",
    "end": "2082780"
  },
  {
    "text": "since those books are\ntalking about minimizing, you had to find some\nsort of direction. But you can think of it as the\ntheory of concave maximization.",
    "start": "2082780",
    "end": "2090280"
  },
  {
    "text": "And they allow you to\nfind algorithms to solve this numerically and\nfairly efficiently.",
    "start": "2090280",
    "end": "2097670"
  },
  {
    "text": "OK, that means fast. Even if d is of\nsize 10,000, you're going to wait for\none second and it's going to tell you\nwhat the maximum is.",
    "start": "2097670",
    "end": "2105130"
  },
  {
    "text": "And that's what machine\nlearning is about. If you've taken any class\non machine learning, there's a lot of optimization,\nbecause they have really,",
    "start": "2105130",
    "end": "2111163"
  },
  {
    "text": "really big problems to solve. Often in this\nclass, since this is more introductory statistics,\nwe will have a close form.",
    "start": "2111163",
    "end": "2119460"
  },
  {
    "text": "For the maximum\nlikelihood estimator will be saying theta hat\nequals, and say x bar,",
    "start": "2119460",
    "end": "2125240"
  },
  {
    "text": "and that will be the maximum\nlikelihood estimator. So just why-- so has anybody\nseen convex optimization",
    "start": "2125240",
    "end": "2134309"
  },
  {
    "text": "before? So let me just give\nyou an intuition why those functions are easy\nto maximize or to minimize.",
    "start": "2134310",
    "end": "2143690"
  },
  {
    "text": "In one dimension, it's actually\nvery easy for you to see that. ",
    "start": "2143690",
    "end": "2150540"
  },
  {
    "text": "And the reason is this. If I want to maximize the\nconcave function, what",
    "start": "2150540",
    "end": "2157110"
  },
  {
    "text": "I need to do is to be\nable to query a point and get as an answer the\nderivative of this function,",
    "start": "2157110",
    "end": "2164080"
  },
  {
    "text": "OK? So now I said this is the\nfunction I want to optimize, and I've been running my\nalgorithm for 5/10 of a second.",
    "start": "2164080",
    "end": "2173410"
  },
  {
    "text": "And it's at this point here. OK, that's the candidate. Now, what I can ask is,\nwhat is the derivative",
    "start": "2173410",
    "end": "2179130"
  },
  {
    "text": "of my function here? Well, it's going\nto give me a value. And this value is going to\nbe either negative, positive,",
    "start": "2179130",
    "end": "2186600"
  },
  {
    "text": "or zero. Well, if it's\nzero, that's great. That means I'm here\nand I can just go home. I've solved my problem.",
    "start": "2186600",
    "end": "2191679"
  },
  {
    "text": "I know there's a unique\nmaximum, and that's what I wanted to find. If it's positive,\nit actually tells me",
    "start": "2191679",
    "end": "2197340"
  },
  {
    "text": "that I'm on the left\nof the optimizer. And on the left of\nthe optimal value.",
    "start": "2197340",
    "end": "2203520"
  },
  {
    "text": "And if it's negative,\nit means that I'm at the right of the\nvalue I'm looking for.",
    "start": "2203520",
    "end": "2210370"
  },
  {
    "text": "And so most of the convex\noptimization methods basically tell you, well,\nif you query the derivative",
    "start": "2210370",
    "end": "2216780"
  },
  {
    "text": "and it's actually positive,\nmove to the right. And if it's negative,\nmove to the left.",
    "start": "2216780",
    "end": "2222430"
  },
  {
    "text": "Now, by how much you\nmove is basically, well, why people write books.",
    "start": "2222430",
    "end": "2229020"
  },
  {
    "text": "And in higher dimension, it's\na little more complicated, because in higher dimension,\nthinks about two dimensions,",
    "start": "2229020",
    "end": "2236260"
  },
  {
    "text": "then I'm only being\nable to get in a vector.",
    "start": "2236260",
    "end": "2241940"
  },
  {
    "text": "And the vector is only\ntelling me, well, here is half of the space\nin which you can move. Now here, if you tell\nme move to the right,",
    "start": "2241940",
    "end": "2248370"
  },
  {
    "text": "I know exactly which direction\nI'm going to have to move. But in two dimension,\nyou're going to basically tell me, well,\nmove in this global direction.",
    "start": "2248370",
    "end": "2257160"
  },
  {
    "text": "And so, of course, I know\nthere's a line on the floor I cannot move behind. But even if you tell me,\ndraw a line on the floor",
    "start": "2257160",
    "end": "2265349"
  },
  {
    "text": "and move only to that\nside of the line, then there's many directions\nin that line that I can go to.",
    "start": "2265350",
    "end": "2270840"
  },
  {
    "text": "And that's also why\nthere's lots of things you can do in optimization.",
    "start": "2270840",
    "end": "2275870"
  },
  {
    "text": "OK, but still, putting this\nline on the floor is telling me,",
    "start": "2275870",
    "end": "2280990"
  },
  {
    "text": "do not go backwards. And that's very important. It's just telling\nyou which direction I should be going always, OK?",
    "start": "2280990",
    "end": "2287470"
  },
  {
    "text": "All right, so that's\nwhat's behind this notion of gradient descent\nalgorithm, steepest descent.",
    "start": "2287470",
    "end": "2294490"
  },
  {
    "text": "Or steepest descent, actually,\nif we're trying to maximize. OK, so let's move on.",
    "start": "2294490",
    "end": "2302150"
  },
  {
    "text": "So this course is not about\noptimization, all right? So as I said, the\nlikelihood was this guy.",
    "start": "2302150",
    "end": "2310690"
  },
  {
    "text": "The product of f of the xi's. And one way you\ncan do this is just basically the joint distribution\nof my data at the point theta.",
    "start": "2310690",
    "end": "2319060"
  },
  {
    "text": "So now the likelihood,\nformerly-- so here I am giving myself\nthe model e theta.",
    "start": "2319060",
    "end": "2324760"
  },
  {
    "text": "And here I'm going to\nassume that e is discrete so that I can talk about PMFs. But everything\nyou're doing, just",
    "start": "2324760",
    "end": "2331840"
  },
  {
    "text": "redo for the sake of yourself\nby replacing PMFs by PDFs, and everything's\ngoing to be fine. We'll do it in a second.",
    "start": "2331840",
    "end": "2338260"
  },
  {
    "text": "All right, so the\nlikelihood of the model. So here I'm not looking at\nthe likelihood of a parameter.",
    "start": "2338260",
    "end": "2345552"
  },
  {
    "text": "I'm looking at the\nlikelihood of a model. So it's actually a\nfunction of the parameter. And actually, I'm\ngoing to make it",
    "start": "2345552",
    "end": "2350650"
  },
  {
    "text": "even a function of\nthe points x1 to xn. All right, so I have a function.",
    "start": "2350650",
    "end": "2355759"
  },
  {
    "text": "And what it takes as\ninput is all the points x1 to xn and a candidate\nparameter theta.",
    "start": "2355760",
    "end": "2362062"
  },
  {
    "text": "Not the true one. A candidate. And what I'm going\nto do is I'm going to look at the probability\nthat my random variables",
    "start": "2362062",
    "end": "2368530"
  },
  {
    "text": "under this\ndistribution, p theta, take these exact\nvalues, px1, px2, pxn.",
    "start": "2368530",
    "end": "2374630"
  },
  {
    "text": "Now remember, if my\ndata was independent,",
    "start": "2374630",
    "end": "2380289"
  },
  {
    "text": "then I could actually just\nsay that the probability of this intersection is just a\nproduct of the probabilities.",
    "start": "2380290",
    "end": "2385960"
  },
  {
    "text": "And it would look\nsomething like this. But I can define likelihood\neven if I don't have independent random variables.",
    "start": "2385960",
    "end": "2392865"
  },
  {
    "text": "But think of them as\nbeing independent, because that's all we're going\nto encounter in this class, OK? I just want you to be aware that\nif I had dependent variables,",
    "start": "2392865",
    "end": "2400380"
  },
  {
    "text": "I could still define\nthe likelihood. I would have to understand how\nto compute these probabilities there to be able to compute it.",
    "start": "2400380",
    "end": "2408269"
  },
  {
    "text": "OK, so think of\nBernoullis, for example. So here is my example\nof a Bernoulli. ",
    "start": "2408270",
    "end": "2416570"
  },
  {
    "text": "So my parameter is-- so my model is 0,1 Bernoulli p.",
    "start": "2416570",
    "end": "2425211"
  },
  {
    "text": "p is in the interval 0,1.",
    "start": "2425211",
    "end": "2431790"
  },
  {
    "text": "The probability, just\nas a side remark, I'm just going to use the\nfact that I can actually",
    "start": "2431790",
    "end": "2438000"
  },
  {
    "text": "write the PMF of a Bernoulli\nin a very concise form, right? If I ask you what the\nPMF of a Bernoulli is,",
    "start": "2438000",
    "end": "2443970"
  },
  {
    "text": "you could tell me, well,\nthe probability that x-- so under p, the probability that\nx is equal to 0 is 1 minus p.",
    "start": "2443970",
    "end": "2450720"
  },
  {
    "text": "The probability under p that\nx is equal to 1 is equal to p.",
    "start": "2450720",
    "end": "2457230"
  },
  {
    "text": "But I can be a bit smart and\nsay that for any X that's either 0 or 1, the\nprobability under p",
    "start": "2457230",
    "end": "2464910"
  },
  {
    "text": "that X is equal to\nlittle x, I can write it in a compact form as p to the\nX, 1 minus p to the 1 minus x.",
    "start": "2464910",
    "end": "2474150"
  },
  {
    "text": "And you can check that this is\nthe right form because, well, you have to check it only\nfor two values of X, 0 and 1.",
    "start": "2474150",
    "end": "2480910"
  },
  {
    "text": "And if you plug in 1,\nyou only keep the p. If you plug in 0, you\nonly keep the 1 minus p.",
    "start": "2480910",
    "end": "2487840"
  },
  {
    "text": "And that's just a trick, OK? I could have gone\nwith many other ways.",
    "start": "2487840",
    "end": "2494349"
  },
  {
    "text": "Agreed? I could have said,\nactually, something like-- another one would be-- which\nwe are not going to use,",
    "start": "2494350",
    "end": "2501550"
  },
  {
    "text": "but we could say, well, it's\nxp plus and minus x 1 minus",
    "start": "2501550",
    "end": "2507340"
  },
  {
    "text": "p, right?  That's another one.",
    "start": "2507340",
    "end": "2513160"
  },
  {
    "text": "But this one is going\nto be convenient. So forget about this\nguy for a second. ",
    "start": "2513160",
    "end": "2522750"
  },
  {
    "text": "So now, I said that\nthe likelihood is just this function that's computing\nthe probability that X1",
    "start": "2522750",
    "end": "2532380"
  },
  {
    "text": "is equal to little x1. So likelihood is L of X1, Xn.",
    "start": "2532380",
    "end": "2547950"
  },
  {
    "text": "So let me try to make\nthose calligraphic so you know that I'm talking about\nsmaller values, right?",
    "start": "2547950",
    "end": "2553140"
  },
  {
    "text": "Small x's. x1, xn, and then of course p.",
    "start": "2553140",
    "end": "2558840"
  },
  {
    "text": "Sometimes we even put-- I didn't do it, but\nsometimes you can actually put a semicolon here, semicolon\nso you know that those two",
    "start": "2558840",
    "end": "2566640"
  },
  {
    "text": "things are treated differently. And so now you have this\nthing is equal to what? Well, it's just the\nprobability under p",
    "start": "2566640",
    "end": "2574440"
  },
  {
    "text": "that X1 is little x1 all\nthe way to Xn is little xn.",
    "start": "2574440",
    "end": "2579990"
  },
  {
    "text": "OK, that's just the definition. ",
    "start": "2579990",
    "end": "2586910"
  },
  {
    "text": "All right, so now\nlet's start working. So we write the\ndefinition, and then we",
    "start": "2586910",
    "end": "2593240"
  },
  {
    "text": "want to make it look like\nsomething we would potentially be able to maximize if I were-- like if I take the derivative\nof this with respect to p,",
    "start": "2593240",
    "end": "2600235"
  },
  {
    "text": "it's not very helpful\nbecause I just don't know. Just want the algebraic\nfunction of p.",
    "start": "2600235",
    "end": "2606770"
  },
  {
    "text": "So this thing is going\nto be equal to what? Well, what is the first\nthing I want to use? ",
    "start": "2606770",
    "end": "2612740"
  },
  {
    "text": "I have a probability of\nan intersection of events, so it's just the product\nof the probabilities.",
    "start": "2612740",
    "end": "2619630"
  },
  {
    "text": "So this is the product from\ni equal 1 to n of P, small p, Xi is equal to little xi.",
    "start": "2619630",
    "end": "2627970"
  },
  {
    "text": "That's independence. ",
    "start": "2627970",
    "end": "2634069"
  },
  {
    "text": "OK, now, I'm starting to mean\nbusiness, because for each P, we have a closed form, right?",
    "start": "2634070",
    "end": "2640369"
  },
  {
    "text": "I wrote this as this\nsupposedly convenient form. I still have to reveal to\nyou why it's convenient.",
    "start": "2640370",
    "end": "2646470"
  },
  {
    "text": "So this thing is equal to-- well, we said that that\nwas p xi for a little xi.",
    "start": "2646470",
    "end": "2655089"
  },
  {
    "text": "1 minus p to the 1 minus xi, OK?",
    "start": "2655090",
    "end": "2660240"
  },
  {
    "text": " So that was just what I wrote\nover there as the probability",
    "start": "2660240",
    "end": "2666650"
  },
  {
    "text": "that Xi is equal to little xi. And since they all have\nthe same parameter p, just",
    "start": "2666650",
    "end": "2672780"
  },
  {
    "text": "have this p that shows up here. ",
    "start": "2672780",
    "end": "2678140"
  },
  {
    "text": "And so now I'm just taking\nthe products of something to the xi, so it's this\nthing to the sum of the xi's.",
    "start": "2678140",
    "end": "2685160"
  },
  {
    "text": "Everybody agrees with this? So this is equal to p\nsum of the xi, 1 minus p",
    "start": "2685160",
    "end": "2696360"
  },
  {
    "text": "to the n minus sum of the xi. ",
    "start": "2696360",
    "end": "2710180"
  },
  {
    "text": "If you don't feel comfortable\nwith writing it directly, you can observe\nthat this thing here",
    "start": "2710180",
    "end": "2715520"
  },
  {
    "text": "is actually equal to p over\n1 minus p to the xi times 1",
    "start": "2715520",
    "end": "2722170"
  },
  {
    "text": "minus p, OK? So now when I take\nthe product, I'm",
    "start": "2722170",
    "end": "2727480"
  },
  {
    "text": "getting the products\nof those guys. So it's just this guy\nto the power of sum and this guy to the power n.",
    "start": "2727480",
    "end": "2733569"
  },
  {
    "text": "And then I can rewrite\nit like this if I want to",
    "start": "2733570",
    "end": "2739670"
  },
  {
    "text": "And so now-- well,\nthat's what we have here. And now I am in business\nbecause I can still",
    "start": "2739670",
    "end": "2745750"
  },
  {
    "text": "hope to maximize this function. And how to maximize\nthis function? All I have to do is to\ntake the derivative.",
    "start": "2745750",
    "end": "2752470"
  },
  {
    "text": "Do you want to do it? Let's just take\nthe derivative, OK? Sorry, I didn't tell you that,\nwell, the maximum likelihood",
    "start": "2752470",
    "end": "2758960"
  },
  {
    "text": "principle is to just maxim-- the\nidea is to maximize this thing, OK? But I'm not going to\nget there right now.",
    "start": "2758960",
    "end": "2764310"
  },
  {
    "text": "OK, so let's do it maybe for\nthe Poisson model for a second. So if you want to do it\nfor the Poisson model,",
    "start": "2764310",
    "end": "2776910"
  },
  {
    "text": "let's write the likelihood. So right now I'm\nnot doing anything. I'm not maximizing. I'm just computing the\nlikelihood function.",
    "start": "2776910",
    "end": "2784040"
  },
  {
    "start": "2784040",
    "end": "2789640"
  },
  {
    "text": "OK, so the likelihood\nfunction for Poisson. So now I know-- what is my\nsample space for Poisson?",
    "start": "2789640",
    "end": "2796710"
  },
  {
    "text": "STUDENT: Positives. PROFESSOR: Positive integers. And well, let me\nwrite it like this.",
    "start": "2796710",
    "end": "2805220"
  },
  {
    "text": "Poisson lambda, and I'm going\nto take lambda to be positive.",
    "start": "2805220",
    "end": "2811170"
  },
  {
    "text": "And so that means that the\nprobability under lambda that X is equal to little\nx in the sample space",
    "start": "2811170",
    "end": "2817920"
  },
  {
    "text": "is lambda to the\nX over factorial x e to the minus lambda.",
    "start": "2817920",
    "end": "2823130"
  },
  {
    "text": "So that's basically the\nsame as the compact form that I wrote over there. It's just now a different one.",
    "start": "2823130",
    "end": "2828860"
  },
  {
    "text": "And so when I want to\nwrite my likelihood, again, we said little x's. ",
    "start": "2828860",
    "end": "2837050"
  },
  {
    "text": "This is equal to what? Well, it's equal to the\nprobability under lambda",
    "start": "2837050",
    "end": "2843690"
  },
  {
    "text": "that X1 is little\nx1, Xn is little xn,",
    "start": "2843690",
    "end": "2851796"
  },
  {
    "text": "which is equal to the product. ",
    "start": "2851796",
    "end": "2860950"
  },
  {
    "text": "OK? Just by independence. And now I can write those\nguys as being-- each",
    "start": "2860950",
    "end": "2867640"
  },
  {
    "text": "of them being i equal 1 to n. So this guy is just this\nthing where a plug in Xi.",
    "start": "2867640",
    "end": "2876100"
  },
  {
    "text": "So I get lambda to the Xi\ndivided by factorial xi times e",
    "start": "2876100",
    "end": "2885540"
  },
  {
    "text": "to the minus lambda, OK?",
    "start": "2885540",
    "end": "2890660"
  },
  {
    "text": "And now, I mean, this\nguy is going to be nice. This guy is not\ngoing to be too nice. But let's write it.",
    "start": "2890660",
    "end": "2896570"
  },
  {
    "text": "When I'm going to take the\nproduct of those guys here, I'm going to pick up lambda\nto the sum of the xi's.",
    "start": "2896570",
    "end": "2901910"
  },
  {
    "text": "Here I'm going to\npick up exponential minus n times lambda. And here I'm going to\npick up just the product",
    "start": "2901910",
    "end": "2907250"
  },
  {
    "text": "of the factorials. So x1 factorial all the\nway to xn factorial.",
    "start": "2907250",
    "end": "2915900"
  },
  {
    "text": "Then I get lambda,\nthe sum of the xi.",
    "start": "2915900",
    "end": "2921130"
  },
  {
    "text": "Those are little xi's. e to the minus xn lambda.",
    "start": "2921130",
    "end": "2926581"
  },
  {
    "text": "OK? ",
    "start": "2926581",
    "end": "2931900"
  },
  {
    "text": "So that might be freaky at\nthis point, but remember, this is a function we\nwill be maximizing.",
    "start": "2931900",
    "end": "2938100"
  },
  {
    "text": "And the denominator here\ndoes not depend on lambda. So we knew that maximizing this\nfunction with this denominator,",
    "start": "2938100",
    "end": "2944859"
  },
  {
    "text": "or any other\ndenominator, including 1, will give me the same arg max.",
    "start": "2944860",
    "end": "2949930"
  },
  {
    "text": "So it won't be a problem for me. As long as it does\nnot depend on lambda, this thing is going to go away.",
    "start": "2949930",
    "end": "2955640"
  },
  {
    "text": " OK, so in the continuous case,\nthe likelihood I cannot--",
    "start": "2955640",
    "end": "2964720"
  },
  {
    "text": "right? So if I would write\nthe likelihood like this in the\ncontinuous case, this one would be equal to what?",
    "start": "2964720",
    "end": "2972240"
  },
  {
    "text": "Zero, right? So it's not very helpful. And so what we do is we\ndefine the likelihood as the product of\nthe f of theta xi.",
    "start": "2972240",
    "end": "2979859"
  },
  {
    "text": "Now that would be a\njump if I told you, well, just define it\nlike that and go home",
    "start": "2979860",
    "end": "2985230"
  },
  {
    "text": "and don't discuss it. But we know that this is\nexactly what's coming from the--",
    "start": "2985230",
    "end": "2992011"
  },
  {
    "text": "well, actually, I\nthink I erased it. It was just behind. So this was exactly what\nwas coming from the KL",
    "start": "2992011",
    "end": "2998280"
  },
  {
    "text": "divergence estimated, right? The thing that I\nshowed you, if we want to follow this\nstrategy, which consists in estimating the KL\ndivergence and minimizing it,",
    "start": "2998280",
    "end": "3006829"
  },
  {
    "text": "is exactly doing this. ",
    "start": "3006830",
    "end": "3012190"
  },
  {
    "text": "So in the Gaussian case-- well, let's write it.",
    "start": "3012190",
    "end": "3017835"
  },
  {
    "text": "So in the Gaussian\ncase, let's see what the likelihood looks like. ",
    "start": "3017835",
    "end": "3027599"
  },
  {
    "text": "OK, so if I have a\nGaussian experiment here-- did I actually write it?",
    "start": "3027600",
    "end": "3033430"
  },
  {
    "text": " OK, so I'm going to take mu and\nsigma as being two parameters.",
    "start": "3033430",
    "end": "3040589"
  },
  {
    "text": "So that means that my sample\nspace is going to be what? ",
    "start": "3040590",
    "end": "3047330"
  },
  {
    "text": "Well, my sample\nspace is still R. Those are just my observations. But then I'm going to\nhave a N mu sigma squared.",
    "start": "3047330",
    "end": "3056840"
  },
  {
    "text": "And the parameters\nof interest are mu and R. And sigma squared\nand say 0 infinity.",
    "start": "3056840",
    "end": "3064291"
  },
  {
    "text": "OK, so that's my Gaussian model. Yes. STUDENT: [INAUDIBLE]",
    "start": "3064291",
    "end": "3077455"
  },
  {
    "text": "PROFESSOR: No, there's no-- I mean, there's no difference. STUDENT: [INAUDIBLE] PROFESSOR: Yeah. I think the all the slides\nI put the curly bracket,",
    "start": "3077455",
    "end": "3084880"
  },
  {
    "text": "then I'm just being lazy. I just like those\nconcave parenthesis.",
    "start": "3084880",
    "end": "3091540"
  },
  {
    "text": "All right, so let's write it. So the definition, L xi, xn.",
    "start": "3091540",
    "end": "3099670"
  },
  {
    "text": "And now I have two parameters,\nmu and sigma squared. We said, by definition,\nis the product from i",
    "start": "3099670",
    "end": "3108035"
  },
  {
    "text": "equal 1 to n of f\ntheta of little xi.",
    "start": "3108035",
    "end": "3115539"
  },
  {
    "text": "Now, think about it. Here we always had\nan extra line, right?",
    "start": "3115540",
    "end": "3120790"
  },
  {
    "text": "The line was to say that the\ndefinition was the probability that they were all\nequal to each other. That was the joint probability.",
    "start": "3120790",
    "end": "3128230"
  },
  {
    "text": "And here it could actually have\na line that says it's the joint probability distribution\nof the xi's.",
    "start": "3128230",
    "end": "3134146"
  },
  {
    "text": "And if it's not\nindependent, it's not going to be the product. But again, since\nwe're only dealing with independent observations\nin the scope of this class,",
    "start": "3134146",
    "end": "3141020"
  },
  {
    "text": "this is the only definition\nwe're going to be using. OK, and actually,\nfrom here on, I",
    "start": "3141020",
    "end": "3146710"
  },
  {
    "text": "will literally skip this step\nwhen I talk about discrete ones as well, because they\nare also independent.",
    "start": "3146710",
    "end": "3153270"
  },
  {
    "text": "Agreed? So we start with\nthis, which we agreed was the definition for\nthis particular case.",
    "start": "3153270",
    "end": "3159590"
  },
  {
    "text": "And so now all of you know by\nheart what the density of a-- sorry, that's not theta.",
    "start": "3159590",
    "end": "3165599"
  },
  {
    "text": "I should write it\nmu sigma squared. And so you need to\nunderstand what this density.",
    "start": "3165600",
    "end": "3170650"
  },
  {
    "text": "And it's product of 1 over\nsigma square root 2 pi times",
    "start": "3170650",
    "end": "3181069"
  },
  {
    "text": "exponential minus\nxi minus mu squared",
    "start": "3181070",
    "end": "3187350"
  },
  {
    "text": "divided by 2 sigma squared. OK, that's the Gaussian density\nwith parameters mu and sigma",
    "start": "3187350",
    "end": "3193750"
  },
  {
    "text": "squared. I just plugged in this thing\nwhich I don't give you, so you just have to trust me.",
    "start": "3193750",
    "end": "3200630"
  },
  {
    "text": "It's all over any book. Certainly, I mean,\nyou can find it. I will give it to you.",
    "start": "3200630",
    "end": "3206250"
  },
  {
    "text": "And again, you're not\nexpected to know it by heart. Though, if you do your homework\nevery week without wanting to,",
    "start": "3206250",
    "end": "3214290"
  },
  {
    "text": "you will definitely\nuse some of your brain to remember that thing. OK, and so now, well, I\nhave this constant in front.",
    "start": "3214290",
    "end": "3222680"
  },
  {
    "text": "1 over sigma square root\n2 pi that I can pull out. So I get 1 over sigma square\nroot 2 pi to the power n.",
    "start": "3222680",
    "end": "3230474"
  },
  {
    "text": "And then I have the product\nof exponentials, which we know is the exponential of the sum. So this is equal to\nexponential minus.",
    "start": "3230474",
    "end": "3238710"
  },
  {
    "text": "And here I'm going to put\nthe 1 over 2 sigma squared outside the sum. ",
    "start": "3238710",
    "end": "3255740"
  },
  {
    "text": "And so that's how\nthis guy shows up. Just the product of the density\nis evaluated at, respectively,",
    "start": "3255740",
    "end": "3263550"
  },
  {
    "text": "x1 to xn. ",
    "start": "3263550",
    "end": "3268850"
  },
  {
    "text": "OK, any questions about\ncomputing those likelihoods? Yes.",
    "start": "3268850",
    "end": "3274556"
  },
  {
    "text": "STUDENT: Why [INAUDIBLE]",
    "start": "3274556",
    "end": "3281460"
  },
  {
    "text": "PROFESSOR: Oh, that's a typo. Thank you. Because I just took it from\nprobably the previous thing.",
    "start": "3281460",
    "end": "3287040"
  },
  {
    "text": "So those are\nactually-- should be-- OK, thank you for\nnoting that one. So this line should say for\nany x1 to xn in R to the n.",
    "start": "3287040",
    "end": "3300180"
  },
  {
    "text": "Thank you, good catch. ",
    "start": "3300180",
    "end": "3306940"
  },
  {
    "text": "All right, so that's\nreally e to the n, right? My sample space always.",
    "start": "3306940",
    "end": "3312490"
  },
  {
    "text": " OK, so what is maximum\nlikelihood estimation?",
    "start": "3312490",
    "end": "3319800"
  },
  {
    "text": "Well again, if you go\nback to the estimate that we got, the estimation\nstrategy, which consisted",
    "start": "3319800",
    "end": "3327770"
  },
  {
    "text": "in replacing expectation\nwith respect to theta star by average of the data\nin the KL divergence,",
    "start": "3327770",
    "end": "3335539"
  },
  {
    "text": "we would try to maximize\nnot this guy, but this guy.",
    "start": "3335540",
    "end": "3341810"
  },
  {
    "text": " The thing that we actually\nplugged in were not any small",
    "start": "3341810",
    "end": "3348260"
  },
  {
    "text": "xi's. Were actually-- the random\nvariable is capital Xi. So the maximum\nlikelihood estimator",
    "start": "3348260",
    "end": "3354190"
  },
  {
    "text": "is actually taking\nthe likelihood, which is a function\nof little x's, and now",
    "start": "3354190",
    "end": "3359570"
  },
  {
    "text": "the values at which it\nestimates, if you look at it, is actually-- the capital X is my data.",
    "start": "3359570",
    "end": "3365870"
  },
  {
    "text": "So it looks at the\nfunction, at the data, and at the parameter theta.",
    "start": "3365870",
    "end": "3371900"
  },
  {
    "text": "That's what the-- so\nthat's the first thing. And then the maximum\nlikelihood estimator is maximizing this, OK?",
    "start": "3371900",
    "end": "3379930"
  },
  {
    "text": "So in a way, what it does is\nit's a function that couples together the data,\ncapital X1 to capital Xn,",
    "start": "3379930",
    "end": "3387810"
  },
  {
    "text": "with the parameter theta and\njust now tries to maximize it. So if this is just a\nlittle hard for you to get,",
    "start": "3387810",
    "end": "3400120"
  },
  {
    "text": "the likelihood is\nformally defined as a function of x, right? Like when I write f of x.",
    "start": "3400120",
    "end": "3406105"
  },
  {
    "text": "f of little x, I\ndefine it like that. But really, the only\nx arguments we're",
    "start": "3406105",
    "end": "3412990"
  },
  {
    "text": "going to evaluate\nthis function at are always the random\nvariable, which is the data. So if you want,\nyou can think of it",
    "start": "3412990",
    "end": "3419440"
  },
  {
    "text": "as those guys being not\nparameters of this function, but really, random variables\nthemselves directly.",
    "start": "3419440",
    "end": "3424810"
  },
  {
    "text": " Is there any question?",
    "start": "3424810",
    "end": "3430682"
  },
  {
    "text": "STUDENT: [INAUDIBLE] those\nrandom variables [INAUDIBLE]?? PROFESSOR: So those are going\nto be known once you have--",
    "start": "3430683",
    "end": "3437890"
  },
  {
    "text": "so it's always the\nsame thing in stats. You first design your\nestimator as a function",
    "start": "3437890",
    "end": "3444040"
  },
  {
    "text": "of random variables. And then once you get\ndata, you just plug it in. But we want to think of them\nas being random variables",
    "start": "3444040",
    "end": "3449920"
  },
  {
    "text": "because we want to understand\nwhat the fluctuations are. So we're going to keep them as\nrandom variables for as long as we can.",
    "start": "3449920",
    "end": "3455685"
  },
  {
    "text": "We're going to spit out\nthe estimator as a function of the random variables. And then when we want\nto compute it from data, we're just going to plug it in.",
    "start": "3455685",
    "end": "3461351"
  },
  {
    "text": " So keep the random variables\nfor as long as you can.",
    "start": "3461351",
    "end": "3466630"
  },
  {
    "text": "Unless I give you\nnumbers, actual numbers, just those are random variables. OK, so there might\nbe some confusion",
    "start": "3466630",
    "end": "3473548"
  },
  {
    "text": "if you've seen any stats\nclass, sometimes there's a notation which says,\noh, the realization of the random variables\nare lower case versions",
    "start": "3473549",
    "end": "3481240"
  },
  {
    "text": "of the original\nrandom variables. So lowercase x should be\nthought as the realization of the upper case X. This\nis not the case here.",
    "start": "3481240",
    "end": "3489609"
  },
  {
    "text": "When I write this,\nit's the same way as I write f of x is\nequal to x squared, right?",
    "start": "3489610",
    "end": "3496630"
  },
  {
    "text": "It's just an argument of a\nfunction that I want to define. So those are just generic x.",
    "start": "3496630",
    "end": "3502150"
  },
  {
    "text": "So if you correct\nthe typo that I have, this should say that this\nshould be for any x and xn. I'm just describing a function.",
    "start": "3502150",
    "end": "3508990"
  },
  {
    "text": "And now the only\nplace at which I'm interested in evaluating\nthat function, at least for those first n\narguments, is at the capital",
    "start": "3508990",
    "end": "3515477"
  },
  {
    "text": "N observations random\nvariables that I have. ",
    "start": "3515477",
    "end": "3521110"
  },
  {
    "text": "So there's actually\ntexts, there's actually people doing research on when\ndoes the maximum likelihood",
    "start": "3521110",
    "end": "3528070"
  },
  {
    "text": "estimator exist? And that happens when you\nhave infinite sets, thetas.",
    "start": "3528070",
    "end": "3536890"
  },
  {
    "text": "And this thing can diverge. There is no global maximum. There's crazy things\nthat might happen.",
    "start": "3536890",
    "end": "3541990"
  },
  {
    "text": "And so we're actually\nalways going to be in a case where this maximum\nlikelihood estimator exists.",
    "start": "3541990",
    "end": "3547450"
  },
  {
    "text": "And if it doesn't, then\nit means that you actually need to restrict your\nparameter space, capital Theta,",
    "start": "3547450",
    "end": "3553840"
  },
  {
    "text": "to something smaller. Otherwise it won't exist. OK, so another thing is the\nlog likelihood estimator.",
    "start": "3553840",
    "end": "3561910"
  },
  {
    "text": "So it is still the\nlikelihood estimator. We solved before that\nmaximizing a function or maximizing log\nof this function",
    "start": "3561910",
    "end": "3567819"
  },
  {
    "text": "is the same thing, because the\nlog function is increasing. So the same thing is\nmaximizing a function or maximizing, I don't know,\nexponential of this function.",
    "start": "3567820",
    "end": "3575352"
  },
  {
    "text": "Every time I take an\nincreasing function, it's actually the same thing. Maximizing a function\nor maximizing 10 times",
    "start": "3575352",
    "end": "3580359"
  },
  {
    "text": "this function is the same thing. So the function x maps to\n10 times x is increasing.",
    "start": "3580360",
    "end": "3585730"
  },
  {
    "text": "And so why do we talk about\nlog likelihood rather than likelihood? So the log of likelihood\nis really just--",
    "start": "3585730",
    "end": "3592590"
  },
  {
    "text": "I mean the log likelihood is\nthe log of the likelihood. And the reason is exactly\nfor this kind of reasons.",
    "start": "3592590",
    "end": "3599420"
  },
  {
    "text": "Remember, that was\nmy likelihood, right? And I want to maximize it. And it turns out that\nin stats, there's",
    "start": "3599420",
    "end": "3605940"
  },
  {
    "text": "a lot of distributions that look\nlike exponential of something. So I might as well just\nremove the exponential",
    "start": "3605940",
    "end": "3612930"
  },
  {
    "text": "by taking the log. So once I have this\nguy, I can take the log. This is something to\na power of something.",
    "start": "3612930",
    "end": "3619080"
  },
  {
    "text": "If I take the log, it's\ngoing to look better for me. I have this thing-- well, I have another\none somewhere, I think,",
    "start": "3619080",
    "end": "3625650"
  },
  {
    "text": "where I had the Poisson. Where was the Poisson? The Poisson's gone.",
    "start": "3625650",
    "end": "3631890"
  },
  {
    "text": "So the Poisson was\nthe same thing. If I took the log,\nbecause it had a power, that would make my life easier.",
    "start": "3631890",
    "end": "3637210"
  },
  {
    "text": "So the log doesn't have any\nparticular intrinsic notion,",
    "start": "3637210",
    "end": "3643800"
  },
  {
    "text": "except that it's\njust more convenient. Now, that being\nsaid, if you think",
    "start": "3643800",
    "end": "3649500"
  },
  {
    "text": "about maximizing the KL,\nthe original formulation, we actually remove the log.",
    "start": "3649500",
    "end": "3655590"
  },
  {
    "text": "If we come back\nto the KL thing-- ",
    "start": "3655590",
    "end": "3660700"
  },
  {
    "text": "where is my KL? Sorry. That was maximizing the sum\nof the logs of the pi's.",
    "start": "3660700",
    "end": "3668630"
  },
  {
    "text": "And so then we worked at it by\nsaying that the sum of the logs was-- maximizing the sum of\nthe logs was the same",
    "start": "3668630",
    "end": "3674330"
  },
  {
    "text": "as maximizing the product. But here, we're\nbasically-- log likelihood is just going backwards in\nthis chain of equivalences.",
    "start": "3674330",
    "end": "3681570"
  },
  {
    "text": "And that's just because\nthe original formulation was already convenient.",
    "start": "3681571",
    "end": "3687180"
  },
  {
    "text": "So we went to find\nthe likelihood and then coming back to our\noriginal estimation strategy.",
    "start": "3687180",
    "end": "3692619"
  },
  {
    "text": "So look at the Poisson. I want to take log here to\nmake my sum of xi's go down.",
    "start": "3692620",
    "end": "3699210"
  },
  {
    "text": "OK, so this is my estimator.",
    "start": "3699210",
    "end": "3707510"
  },
  {
    "text": "So the log of L-- so one thing that\nyou want to notice is that the log of L of\nx1, xn theta, as we said,",
    "start": "3707510",
    "end": "3719960"
  },
  {
    "text": "is equal to the\nsum from i equal 1 to n of the log of either\np theta of xi, or--",
    "start": "3719960",
    "end": "3729950"
  },
  {
    "text": "so that's in the discrete case. And in the continuous\ncase is the sum of the log of f theta of xi.",
    "start": "3729950",
    "end": "3736627"
  },
  {
    "text": " The beauty of this is that you\ndon't have to really understand",
    "start": "3736627",
    "end": "3741860"
  },
  {
    "text": "the difference between\nprobability mass function and probability\ndistribution function to implement this. Whatever you get,\nthat's what you plug in.",
    "start": "3741860",
    "end": "3749517"
  },
  {
    "text": " Any questions so far? ",
    "start": "3749518",
    "end": "3756550"
  },
  {
    "text": "All right, so shall we\ndo some computations and check that, actually, we've\nintroduced all this stuff--",
    "start": "3756550",
    "end": "3764720"
  },
  {
    "text": "complicate functions,\nmaximizing, KL divergence, lot of things-- so that we\ncan spit out, again, averages?",
    "start": "3764720",
    "end": "3770589"
  },
  {
    "text": "All right? That's great. We're going to able\nto sleep at night and know that there's a really\npowerful mechanism called maximum likelihood\nestimator that was actually",
    "start": "3770590",
    "end": "3777369"
  },
  {
    "text": "driving our intuition\nwithout us knowing. OK, so let's do this so.",
    "start": "3777370",
    "end": "3784730"
  },
  {
    "text": "Bernoulli trials. I still have it over there. ",
    "start": "3784730",
    "end": "3795920"
  },
  {
    "text": "OK, so actually, I\ndon't know what-- well, let me write it like that.",
    "start": "3795920",
    "end": "3801260"
  },
  {
    "text": "So it's P over 1 minus P xi-- sorry, sum of the xi's\ntimes 1 minus P is to the n.",
    "start": "3801260",
    "end": "3812650"
  },
  {
    "text": "So now I want to maximize\nthis as a function of P.",
    "start": "3812650",
    "end": "3817960"
  },
  {
    "text": "Well, the first thing\nwe would want to do is to check that this\nfunction is concave. And I'm just going to ask\nyou to trust me on this.",
    "start": "3817960",
    "end": "3825220"
  },
  {
    "text": "So I don't want--\nsorry, sum of the xi's. I only want to take the\nderivative and just go home.",
    "start": "3825220",
    "end": "3832520"
  },
  {
    "text": "So let's just take the\nderivative of this with respect to P. Actually, no. This one was more convenient.",
    "start": "3832520",
    "end": "3837540"
  },
  {
    "text": "I'm sorry.  This one was slightly\nmore convenient, OK?",
    "start": "3837540",
    "end": "3843100"
  },
  {
    "text": "So now we have-- so now let me take the log.",
    "start": "3843100",
    "end": "3849130"
  },
  {
    "text": "So if I take the log, what I get\nis sum of the xi's times log p",
    "start": "3849130",
    "end": "3856960"
  },
  {
    "text": "plus n minus some of the\nxi's times log 1 minus p.",
    "start": "3856960",
    "end": "3864703"
  },
  {
    "text": " Now I take the\nderivative with respect to p and set it equal to zero.",
    "start": "3864704",
    "end": "3875837"
  },
  {
    "text": "So what does that give me? It tells me that sum of the\nxi's divided by p minus n",
    "start": "3875837",
    "end": "3883710"
  },
  {
    "text": "sum of the xi's divided by\n1 minus p is equal to 0.",
    "start": "3883710",
    "end": "3890130"
  },
  {
    "start": "3890130",
    "end": "3896359"
  },
  {
    "text": "So now I need to solve for p. So let's just do it. So what we get is that 1 minus p\nsum of the xi's is equal to p n",
    "start": "3896360",
    "end": "3906500"
  },
  {
    "text": "minus sum of the xi's. So that's p times n minus sum of\nthe xi's plus sum of the xi's.",
    "start": "3906500",
    "end": "3917300"
  },
  {
    "text": "So let me put it on the right. So that's p times n is\nequal to sum of the xi's.",
    "start": "3917300",
    "end": "3924410"
  },
  {
    "text": "And that's equivalent to p-- actually, I should start\nby putting p hat from here",
    "start": "3924410",
    "end": "3930020"
  },
  {
    "text": "on, because I'm already\nsolving an equation, right? And so p hat is equal\nto syn of the xi's",
    "start": "3930020",
    "end": "3936880"
  },
  {
    "text": "divided by n,\nwhich is my xn bar. ",
    "start": "3936880",
    "end": "3944050"
  },
  {
    "text": "Poisson model, as I\nsaid, Poisson is gone.",
    "start": "3944050",
    "end": "3950280"
  },
  {
    "text": "So let me rewrite it quickly. ",
    "start": "3950280",
    "end": "3960850"
  },
  {
    "text": "So Poisson, the likelihood\nin X1, Xn, and lambda",
    "start": "3960850",
    "end": "3967975"
  },
  {
    "text": "was equal to lambda to\nthe sum of the xi's e",
    "start": "3967975",
    "end": "3973270"
  },
  {
    "text": "to the minus n lambda\ndivided by X1 factorial, all the way to Xn factorial.",
    "start": "3973270",
    "end": "3980920"
  },
  {
    "text": "So let me take the\nlog likelihood. That's going to\nbe equal to what?",
    "start": "3980920",
    "end": "3986490"
  },
  {
    "text": "It's going to tell me. It's going to be-- well, let me get rid\nof this guy first. Minus log of X1 factorial\nall the way to Xn factorial.",
    "start": "3986490",
    "end": "3996780"
  },
  {
    "text": "That's a constant with\nrespect to lambda. So when I'm going to take the\nderivative, it's going to go.",
    "start": "3996780",
    "end": "4003180"
  },
  {
    "text": "Then I'm going to have plus sum\nof the xi's times log lambda.",
    "start": "4003180",
    "end": "4009410"
  },
  {
    "text": "And then I'm going to\nhave minus n lambda.  So now then, you\ntake the derivative",
    "start": "4009410",
    "end": "4015890"
  },
  {
    "text": "and set it equal to zero. So log L-- well, partial with\nrespect to lambda of log L,",
    "start": "4015890",
    "end": "4024859"
  },
  {
    "text": "say lambda, equals zero. This is equivalent\nto, so this guy goes.",
    "start": "4024860",
    "end": "4031160"
  },
  {
    "text": "This guy gives me sum of the\nxi's divided by lambda hat",
    "start": "4031160",
    "end": "4036440"
  },
  {
    "text": "equals n. ",
    "start": "4036440",
    "end": "4042470"
  },
  {
    "text": "And so that's\nequivalent to lambda hat is equal to sum of the xi's\ndivided by n, which is Xn bar.",
    "start": "4042470",
    "end": "4051092"
  },
  {
    "text": " Take derivative, set it equal\nto zero, and just solve.",
    "start": "4051092",
    "end": "4058785"
  },
  {
    "text": "It's a very satisfying\nexercise, especially when you get the average in the end.",
    "start": "4058785",
    "end": "4065150"
  },
  {
    "text": "You don't have to\nthink about it forever. OK, the Gaussian model I'm going\nto leave to you as an exercise.",
    "start": "4065150",
    "end": "4074360"
  },
  {
    "text": "Take the log to get rid\nof the pesky exponential, and then take the derivative\nand you should be fine.",
    "start": "4074360",
    "end": "4080690"
  },
  {
    "text": "It's a bit more-- it might be one more\nline than those guys.",
    "start": "4080690",
    "end": "4085960"
  },
  {
    "text": "OK, so-- well actually,\nyou need to take",
    "start": "4085960",
    "end": "4092760"
  },
  {
    "text": "the gradient in this case. Don't check the second\nderivative right now. You don't have to\nreally think about it. ",
    "start": "4092760",
    "end": "4101430"
  },
  {
    "text": "What did I want to add? I think there was\nsomething I wanted to say. Yes.",
    "start": "4101430",
    "end": "4107318"
  },
  {
    "text": "When I have a function that's\nconcave and I'm on, like, some infinite\ninterval, then it's",
    "start": "4107319",
    "end": "4113671"
  },
  {
    "text": "true that taking the derivative\nand setting it equal to zero will give me the maximum. But again, I might have a\nfunction that looks like this.",
    "start": "4113671",
    "end": "4122329"
  },
  {
    "text": "Now, if I'm on some finite\ninterval-- let me go elsewhere. So if I'm on some\nfinite interval",
    "start": "4122330",
    "end": "4135549"
  },
  {
    "text": "and my function looks like\nthis as a function of theta--",
    "start": "4135550",
    "end": "4140979"
  },
  {
    "text": "let's say this is\nmy log likelihood as a function of theta--",
    "start": "4140979",
    "end": "4146410"
  },
  {
    "text": "then, OK, there's no\nplace in this interval--",
    "start": "4146410",
    "end": "4153200"
  },
  {
    "text": "let's say this is\nbetween 0 and 1-- there's no place in this interval where\nthe derivative is equal to 0.",
    "start": "4153200",
    "end": "4159870"
  },
  {
    "text": "And if you actually\ntry to solve this, you won't find a solution which\nis not in the interval 0, 1.",
    "start": "4159870",
    "end": "4166187"
  },
  {
    "text": "And that's actually how\nyou know that you probably should not take the\nderivative equal to zero. So don't panic if you\nget something that says,",
    "start": "4166187",
    "end": "4172720"
  },
  {
    "text": "well, the solution is\nat infinity, right? If this function\nkeeps going, you will find that\nthe solution-- you won't be able to find a\nsolution apart from infinity.",
    "start": "4172720",
    "end": "4180490"
  },
  {
    "text": "You are going to see something\nlike 1 over theta hat is equal to 0, or\nsomething like this.",
    "start": "4180490",
    "end": "4186359"
  },
  {
    "text": "So you know that when you've\nfound this kind of solution, you've probably made a\nmistake at some point.",
    "start": "4186359",
    "end": "4191370"
  },
  {
    "text": "And the reason is because the\nfunctions that are like this, you don't find the maximum by\nsetting the derivative equal",
    "start": "4191370",
    "end": "4198150"
  },
  {
    "text": "to zero. You actually just find\nthe maximum by saying, well, it's an increasing\nfunction on the interval 0, 1,",
    "start": "4198150",
    "end": "4203450"
  },
  {
    "text": "so the maximum must\nbe attained at 1.  So here in this\ncase, that would mean",
    "start": "4203450",
    "end": "4208750"
  },
  {
    "text": "that my maximum would be 1. My estimator would be\n1, which would be weird.",
    "start": "4208750",
    "end": "4214540"
  },
  {
    "text": "So typically here, you have\na function of the xi's. So one example that you will see\nmany times is when this guy is",
    "start": "4214540",
    "end": "4219940"
  },
  {
    "text": "the maximum of the xi's. And in which case, the\nmaximum is attained here,",
    "start": "4219940",
    "end": "4227210"
  },
  {
    "text": "which is the maximum of this. OK, so just keep in mind-- what I would recommend\nis every time",
    "start": "4227210",
    "end": "4233840"
  },
  {
    "text": "you're trying to take the\nmaximum of a function, just try to plot the\nfunction in your head.",
    "start": "4233840",
    "end": "4239320"
  },
  {
    "text": "It's not too complicated. Those things are usually\nsquares, or square roots,",
    "start": "4239320",
    "end": "4244790"
  },
  {
    "text": "or logs. You know what those\nfunctions look like. Just plug them in your\nmind and make sure",
    "start": "4244790",
    "end": "4250040"
  },
  {
    "text": "that you will find a\nmaximum which really goes up and then down again. If you don't, then\nthat means your maximum",
    "start": "4250040",
    "end": "4256400"
  },
  {
    "text": "is achieved at the\nboundary and you have to think differently to get it.",
    "start": "4256400",
    "end": "4261950"
  },
  {
    "text": "So the machinery that consists\nin setting the derivative equal to zero works 80% of the time. But o you have to be careful.",
    "start": "4261950",
    "end": "4268880"
  },
  {
    "text": "And from the context,\nit will be clear that you had to be careful,\nbecause you will find",
    "start": "4268880",
    "end": "4274460"
  },
  {
    "text": "some crazy stuff, such\nas solve 1 over theta hat is equal to zero. ",
    "start": "4274460",
    "end": "4283139"
  },
  {
    "text": "All right, so\nbefore we conclude, I just wanted to give you\nsome intuition about how does the maximum likelihood perform?",
    "start": "4283140",
    "end": "4290619"
  },
  {
    "text": "So there's something called\nthe Fisher information that essentially controls\nhow this thing performs.",
    "start": "4290620",
    "end": "4295980"
  },
  {
    "text": "And the Fisher information\nis, essentially, a second derivative\nor a Hessian. So if I'm in a one-dimensional\nparameter case, it's a number,",
    "start": "4295980",
    "end": "4304980"
  },
  {
    "text": "it's a second derivative. If I'm in a multidimensional\ncase, it's actually a Hessian,",
    "start": "4304980",
    "end": "4311000"
  },
  {
    "text": "it's a matrix. So I'm going to actually take\nin notation little curly L",
    "start": "4311000",
    "end": "4317800"
  },
  {
    "text": "of theta to be the\nlog likelihood, OK? And that's the log likelihood\nfor one observation.",
    "start": "4317800",
    "end": "4322910"
  },
  {
    "text": "So let's call it x generically,\nbut think of it as being x1, for example. And I don't care\nof, like, summing,",
    "start": "4322910",
    "end": "4329250"
  },
  {
    "text": "because I'm actually going to\ntake expectation of this thing. So it's not going to be\na data driven quantity I'm going to play with.",
    "start": "4329250",
    "end": "4334389"
  },
  {
    "text": "So now I'm going\nto assume that it is twice differentiable,\nalmost surely, because it's a random function.",
    "start": "4334390",
    "end": "4341350"
  },
  {
    "text": "And so now I'm going to\njust sweep under the rug some technical conditions\nwhen these things hold.",
    "start": "4341350",
    "end": "4347700"
  },
  {
    "text": "So typically, when can I\npermute integral and derivatives and this kind of stuff that\nyou don't want to think about?",
    "start": "4347700",
    "end": "4355159"
  },
  {
    "text": "OK, the rule of\nthumb is it always works until it\ndoesn't, in which case, that probably means\nyou're actually solving",
    "start": "4355160",
    "end": "4361380"
  },
  {
    "text": "some sort of calculus problem. Because in practice,\nit just doesn't happen.",
    "start": "4361380",
    "end": "4367390"
  },
  {
    "text": "So the Fisher information\nis the expectation of the--",
    "start": "4367390",
    "end": "4376010"
  },
  {
    "text": "that's called the outer product. So that's the product\nof this gradient",
    "start": "4376010",
    "end": "4381240"
  },
  {
    "text": "and the gradient transpose. So that forms a matrix, right? That's a matrix minus the outer\nproduct of the expectations.",
    "start": "4381240",
    "end": "4389830"
  },
  {
    "text": "So that's really what's\ncalled the covariance matrix of this vector, nabla\nof L theta, which",
    "start": "4389830",
    "end": "4396285"
  },
  {
    "text": "is a random vector. So I'm forming the covariance\nmatrix of this thing. And the technical conditions\ntells me that, actually,",
    "start": "4396285",
    "end": "4403250"
  },
  {
    "text": "this guy, which depends\nonly on the Hessian, is actually equal to negative\nexpectation of the-- sorry.",
    "start": "4403250",
    "end": "4411115"
  },
  {
    "text": "It depends on the gradient. Is actually negative\nexpectation of the Hessian.",
    "start": "4411115",
    "end": "4416140"
  },
  {
    "text": "So I can actually\nget a quantity that depends on the second\nderivatives only using first derivatives.",
    "start": "4416140",
    "end": "4421739"
  },
  {
    "text": "But the expectation is\ngoing to play a role here. And the fact that it's a log. And lots of things\nactually show up here.",
    "start": "4421740",
    "end": "4428180"
  },
  {
    "text": "And so in this case,\nwhat I get is that-- so in the one-dimensional\ncase, then this",
    "start": "4428180",
    "end": "4433510"
  },
  {
    "text": "is just the covariance matrix of\na one-dimensional thing, which is just a variance of itself. So the variance\nof the derivative",
    "start": "4433510",
    "end": "4440050"
  },
  {
    "text": "is actually equal to\nnegative the expectation of the second derivative.",
    "start": "4440050",
    "end": "4447080"
  },
  {
    "text": "OK, so we'll see that next time. But what I wanted to emphasize\nwith this is that why do",
    "start": "4447080",
    "end": "4452599"
  },
  {
    "text": "we care about this quantity? That's called the\nFisher information. Fisher is the founding\nfather of modern statistics.",
    "start": "4452600",
    "end": "4459770"
  },
  {
    "text": "Why do we give this\nquantity his name? Well, it's because this quantity\nis actually very critical.",
    "start": "4459770",
    "end": "4465546"
  },
  {
    "text": "What does the second\nderivative of a function tell me at the maximum? Well, it's telling me\nhow curved it is, right?",
    "start": "4465546",
    "end": "4474350"
  },
  {
    "text": "If I have a zero second\nderivative, I'm basically flat. And if I have a very high second\nderivative, I'm very curvy.",
    "start": "4474350",
    "end": "4481136"
  },
  {
    "text": "And when I'm very\ncurvy, what it means is that I'm very robust\nto the estimation error. Remember our\nestimation strategy,",
    "start": "4481137",
    "end": "4487159"
  },
  {
    "text": "which consisted in replacing\nexpectation by averages? If I'm extremely curvy,\nI can move a little bit.",
    "start": "4487160",
    "end": "4492830"
  },
  {
    "text": "This thing, the maximum,\nis not going to move much. And this formula here-- so forget about the matrix\nversion for a second--",
    "start": "4492830",
    "end": "4500090"
  },
  {
    "text": "is actually telling me exactly-- it's telling me the curvature\nis basically the variance",
    "start": "4500090",
    "end": "4506000"
  },
  {
    "text": "of the first derivative. And so the more the first\nderivative fluctuates, the more your maximum is\nactually-- your org max",
    "start": "4506000",
    "end": "4512929"
  },
  {
    "text": "is going to move\nall over the place. So this is really\ncontrolling how flat your likelihood, your log\nlikelihood, is at its maximum.",
    "start": "4512930",
    "end": "4520280"
  },
  {
    "text": "The flatter it is, the more\nsensitive to fluctuation the arg max is going to be. The curvier it is, the\nless sensitive it is.",
    "start": "4520280",
    "end": "4527060"
  },
  {
    "text": "And so what we're\nhoping-- a good model is going to be one that\nhas a large or small value for the Fisher information.",
    "start": "4527060",
    "end": "4534350"
  },
  {
    "text": "I want this to be-- small? I want it to be large.",
    "start": "4534350",
    "end": "4540070"
  },
  {
    "text": "Because this is the\ncurvature, right? This number is\nnegative, it's concave. So if I take a\nnegative sign, it's",
    "start": "4540070",
    "end": "4545830"
  },
  {
    "text": "going to be something\nthat's positive. And the larger this thing,\nthe more curvy it is.",
    "start": "4545830",
    "end": "4551230"
  },
  {
    "text": "Oh, yeah, because\nit's the variance. Again, sorry. This is what-- OK. ",
    "start": "4551230",
    "end": "4559480"
  },
  {
    "text": "Yeah, maybe I should not\ngo into those details because I'm actually\nout of time. But just spoiler alert,\nthe asymptotic variance",
    "start": "4559480",
    "end": "4566889"
  },
  {
    "text": "of your-- the variance,\nbasically, as n goes to infinity of the\nmaximum likelihood estimator is going to be 1 over this guy.",
    "start": "4566890",
    "end": "4572830"
  },
  {
    "text": "So we want it to be large,\nbecause the asymptotic variance is going to be very small. All right, so we're out of time.",
    "start": "4572830",
    "end": "4578650"
  },
  {
    "text": "We'll see that next week. And I have your\nhomework with me. And I will actually turn it in.",
    "start": "4578650",
    "end": "4585052"
  },
  {
    "text": "I will give it to\nyou outside so we can let the other room come in. OK, I'll just leave you the--",
    "start": "4585052",
    "end": "4591630"
  }
]